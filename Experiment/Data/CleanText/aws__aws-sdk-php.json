{
    "mtdowling": "Thanks for submitting the first PR!\n. You can use both SDK1 and SDK2 in the same application without any conflicts. SDK1 does not use namespaces, so the namespaces in SDK2 will never cause a conflict. We have a detailed guide that can help walk you through installing both SDKs and using them side by side in the same application: http://docs.amazonwebservices.com/awssdkdocsphp2/latest/sidebysideguide/sdk-php2-welcome-side-by-side-guide.html. This should help you take advantage of the features in the new SDK while still allowing you to use the old version of the SDK until you are able to migrate your application.\n. Client objects have factory methods for each HTTP method. You don't have to use a RequestFactory directly; calling things like $client->get(), $client->put(), $client->post(), etc will use the RequestFactory object owned by the client to create and return a RequestInterface object.\nHere's an example of using the client factory methods to create a signed URL (I'm also using a time helper):\necho $s3->createPresignedUrl($s3->get(BUCKET . '/testing123.jpg'), '+5 minutes');\nHope that helps.\n. Here are a couple examples from recent forum posts:\nhttps://forums.aws.amazon.com/thread.jspa?threadID=109840&tstart=0\nhttps://forums.aws.amazon.com/message.jspa?messageID=399919#399919\nWe'll work on putting a strategy in place to provide better high-level documentation.\n. Can you reopen this with a clean pull request against a feature branch? Looks like other commits are being pulled into this PR.\n. You should also try increasing the apc.shm_size even more than 64MB. The size this setting needs to be depends on the other libraries you're using as well. As an example, many Magento (a large framework) users recommend an apc.shm_size of 256M.\nOn Dec 7, 2012, at 8:42 AM, Jeremy Lindblom notifications@github.com wrote:\n\nNope, APC just needs to be enabled in your INI so that opcode caching is in effect. Is there any other information that would be useful to help us find out why it appears to be slower on your system? What kind of environment are you on? How are you benchmarking the two session handlers?\nI was comparing them yesterday, and with APC enabled, version 2 was always faster than version 1. Here are samples of two scripts I was using to test.\nFor SDK version 1:\n<?php\n$time = microtime(true);\nrequire 'aws-sdk-for-php/sdk.class.php';\nrequire 'config-sdk1.php';\n$db = new AmazonDynamoDB();\n$db->register_session_handler(array(\n    'session_locking' => false,\n    'table_name'      => 'sessions',\n    'hash_key'        => 'id',\n));\nsession_start();\n$_SESSION['random_value'] = rand(0, 100);\nsession_commit();\necho microtime(true) - $time;\nFor SDK version 2:\n<?php\n$time = microtime(true);\nrequire 'vendor/autoload.php';\n$aws = Aws\\Common\\Aws::factory('config-sdk2.php');\n$db = $aws->get('dynamodb');\n$db->registerSessionHandler(array(\n    'locking_strategy' => null,\n    'table_name'       => 'sessions',\n    'hash_key'         => 'id',\n));\nsession_start();\n$_SESSION['random_value'] = rand(0, 100);\nsession_commit();\necho microtime(true) - $time;\nThese scripts include the time to bootstrap the SDK, instantiate the DynamoDB client, register the session handler, read from the session, and write to the session. I ran them hundreds and hundreds of times, took averages, etc. I also has some other scripts that measured each part of these scripts in more finer grained steps.\n\u2014\nReply to this email directly or view it on GitHub.\n. Thanks for pointing this out. It looks like our service description is missing PartNumber and UploadId parameters. We'll get those added as soon as possible.\n. Thanks. We'll take a look at this and see if we can make the error message more informative.\n. One of the ways we're working on addressing this is in the form of end-user documentation. You can see our current progress on the docs branch: https://github.com/aws/aws-sdk-php/tree/docs/docs. Do docs like that help?\n. Yeah, we'll generate these docs and host them somewhere. The intent of using reStructuredText and Sphinx is that it will be easy to contribute to them.\n\nWe'll see about how to best link between docs. Thanks for your feedback.\n. Our user guide documentation is now hosted at http://docs.aws.amazon.com/aws-sdk-php-2/guide/latest/index.html. This documentation includes various examples for common tasks. We will continue to update these guides and work to improve our API documentation.\nPlease note that our documentation is open source, and we are happy to accept pull requests that help to improve them: https://github.com/aws/aws-sdk-php/tree/master/docs\n. I was unable to reproduce this error, and I added an integration test to the Amazon S3 client to ensure that spaces are able to be used in pre-signed URLs.\nCan you provide more information on what is failing? How are you downloading the objects with the pre-signed URL? What's the name of the key?\n. I'm going to go ahead and close this issue. Please feel free to reopen it if you can provide a test case to reproduce. Thanks.\n. @ralph-tice Thanks for your feedback.\n\nI don't think you should default to the php.ini gc probability if it's recommended to run it on a cron instead, and I think it's pretty important to run it on a cron instead of the default PHP way so you don't have throughput problems, but instead of changing functionality you could just say set this:\nsession.gc_probability = 0\n\nThis is a tough one. While it isn't optimal, defaulting to these settings also allows the session handler to work in more restrictive PHP environments like shared hosting where users do not have access to the shell or cron.\n\nauto_prepend_file = /var/www/dynamodb_session_handler/register_sessionhandler.php seem warranted to go over, unless there's a reason not to use auto_prepend.\n\nI'm not a big fan of auto_prepend_file. It often leads to confusion because it fragments the bootstrapping process of an application. Another downside: what if you don't want all of your PHP scripts to start a session (e.g. command line scripts)?\n. Thanks, Clay!\n. This has been fixed in the latest release of the documentation.\n. What's the size of the file you're uploading?\nDoes this error happen consistently for particular files on multiple systems?\nOn Jan 12, 2013, at 1:40 AM, Johannes notifications@github.com wrote:\n\nOn some system, I get the above error when trying to upload a file to Amazon S3.\nTriggering Code:\nUploadBuilder::newInstance()\n    ->setClient($this->s3)\n    ->setSource($url)\n    ->setBucket($this->bucketName)\n    ->setKey($key)\n    ->build()\n    ->upload()\n;\nThe error can be prevented by switching the source to be in memory:\nUploadBuilder::newInstance()\n    // ...\n    // Pass the content instead of just the URL\n    ->setSource(EntityBody::factory($content))\n    // ...\n;\nMaybe you have seen this before, and have an idea how to fix this automatically?\nComplete Stack Trace:\nAws\\Common\\Exception\\MultipartUploadException: An error was encountered while performing a multipart upload: Your socket connection to the server was not read from or written to within the timeout period. Idle connections will be closed.\nvendor/aws/aws-sdk-php/src/Aws/Common/Model/MultipartUpload/AbstractTransfer.php:177\nvendor/aws/aws-sdk-php/src/Aws/Common/Exception/NamespaceExceptionFactory.php:75\nvendor/aws/aws-sdk-php/src/Aws/Common/Exception/ExceptionListener.php:55\nvendor/symfony/event-dispatcher/Symfony/Component/EventDispatcher/EventDispatcher.php:164\nvendor/symfony/event-dispatcher/Symfony/Component/EventDispatcher/EventDispatcher.php:53\nvendor/guzzle/guzzle/src/Guzzle/Http/Message/Request.php:757\nvendor/guzzle/guzzle/src/Guzzle/Http/Message/Request.php:466\nvendor/guzzle/guzzle/src/Guzzle/Http/Message/EntityEnclosingRequest.php:66\nvendor/guzzle/guzzle/src/Guzzle/Http/Curl/CurlMulti.php:499\nvendor/guzzle/guzzle/src/Guzzle/Http/Curl/CurlMulti.php:426\nvendor/guzzle/guzzle/src/Guzzle/Http/Curl/CurlMulti.php:387\nvendor/guzzle/guzzle/src/Guzzle/Http/Curl/CurlMulti.php:278\nvendor/guzzle/guzzle/src/Guzzle/Http/Client.php:363\nvendor/guzzle/guzzle/src/Guzzle/Service/Client.php:223\nvendor/guzzle/guzzle/src/Guzzle/Service/Command/AbstractCommand.php:167\nvendor/guzzle/guzzle/src/Guzzle/Service/Command/AbstractCommand.php:206\nvendor/aws/aws-sdk-php/src/Aws/S3/Model/MultipartUpload/SerialTransfer.php:73\nvendor/aws/aws-sdk-php/src/Aws/Common/Model/MultipartUpload/AbstractTransfer.php:167\nCaused by\nAws\\S3\\Exception\\RequestTimeoutException: Your socket connection to the server was not read from or written to within the timeout period. Idle connections will be closed.\nvendor/aws/aws-sdk-php/src/Aws/Common/Exception/NamespaceExceptionFactory.php:89\nvendor/aws/aws-sdk-php/src/Aws/Common/Exception/NamespaceExceptionFactory.php:75\nvendor/aws/aws-sdk-php/src/Aws/Common/Exception/ExceptionListener.php:55\nvendor/symfony/event-dispatcher/Symfony/Component/EventDispatcher/EventDispatcher.php:164\nvendor/symfony/event-dispatcher/Symfony/Component/EventDispatcher/EventDispatcher.php:53\nvendor/guzzle/guzzle/src/Guzzle/Http/Message/Request.php:757\nvendor/guzzle/guzzle/src/Guzzle/Http/Message/Request.php:466\nvendor/guzzle/guzzle/src/Guzzle/Http/Message/EntityEnclosingRequest.php:66\nvendor/guzzle/guzzle/src/Guzzle/Http/Curl/CurlMulti.php:499\nvendor/guzzle/guzzle/src/Guzzle/Http/Curl/CurlMulti.php:426\nvendor/guzzle/guzzle/src/Guzzle/Http/Curl/CurlMulti.php:387\nvendor/guzzle/guzzle/src/Guzzle/Http/Curl/CurlMulti.php:278\nvendor/guzzle/guzzle/src/Guzzle/Http/Client.php:363\nvendor/guzzle/guzzle/src/Guzzle/Service/Client.php:223\nvendor/guzzle/guzzle/src/Guzzle/Service/Command/AbstractCommand.php:167\nvendor/guzzle/guzzle/src/Guzzle/Service/Command/AbstractCommand.php:206\nvendor/aws/aws-sdk-php/src/Aws/S3/Model/MultipartUpload/SerialTransfer.php:73\nvendor/aws/aws-sdk-php/src/Aws/Common/Model/MultipartUpload/AbstractTransfer.php:167\n\u2014\nReply to this email directly or view it on GitHub.\n. Ah interesting. The UploadBuilder is used for multipart uploads. The size of a multipart upload file for Amazon S3 has to be greater than or equal to 5MB (see http://docs.amazonwebservices.com/AmazonS3/latest/dev/qfacts.html). If this is in fact the error, we will see about adding a check to ensure the size of the file meets the minimum size of the uploader.\n. What does the variable $url contain? A string that contains a URL (e.g. 'http://www.example.com'), a resource returned by fopen('http://example.com', 'r'), an EntityBody object, or the path to a file? If it's the path to a file, is it possible that the file is frequently being changed and fstat cache isn't picking up size changes in the file, thus reporting an erroneous filesize()?\n. The reason this is happening is because cURL is being told that it is going to send more data than it actually has. I'm not sure where the erroneous length is being specified, but clearing fstat cache should help to narrow down the possibilities. Does this still fail for you after the change I made to clear the fstat cache?\n. @RamyTalal What version of PHP are you using, what version of cURL, and what arch is your system (i386, x86_64)?\n\nbash\nuname -a\nphp --version\ncurl --version\n. It seems that this error occurs intermittently when uploading to Amazon S3. I've updated the Amazon S3 client to automatically retry these specific failures using exponential backoff.\nIf you continue to see this issue, please ensure that you are not sending an incorrect Content-Length header in your requests.\n. I've commented on this issue on the forums. Please let us know if my response on the forums does not resolve the issue.\n. The Type enum can be used for full-names like this.\n. We generate enums based on service descriptions.\n. Support was added in 2.1.0\n. Thanks for the information, @tblanchard. This has now been fixed in the master branch and will go out in the next release.\n. Sorry to hear that you are having trouble with this. Thanks for trying to dig in with some details on the failure cases. I'll take a look at this and post back when I have more information.\n. This issue is now fixed in master. It looks like there was an error the documentation on how a signed URL is created. I'll work with the documentation team to have this corrected.\n. This has not yet been released. This will change will be present in the phar and PEAR package after the next release. We don't have a definitive timeframe for the next release, but it should be soon. In the meantime, you could grab the latest version of the SDK from master using Composer.\n. The value you should pass is actually Filters not Filter. Does that fix the issue for you?\n. Thanks for the information, ralph-tice. The problem is that, for some reason, the request is being resent internally inside of cURL. This could be caused by some sort of intermittent networking issue. When cURL needs to resend data that is previously sent, it attempts to rewind the data stream. Unfortunately, PHP does not allow a custom seek or ioctl function to be passed to cURL, so when cURL attempts to rewind our custom stream, it fails. Here's the related issue on PHP.net-- I would be grateful if you upvoted this issue: https://bugs.php.net/bug.php?id=47204\nI've disabled the use of streaming requests when interacting with JSON services like Amazon DynamoDB. This will prevent cURL error 65 from occurring because cURL is able to rewind a string of POST data internally. This fix will be present in the next release.\n. This should be much easier to do using the master branch. One of the changes that will go out in the next release is the addition of the getInstanceProfileCredentials() method to Aws\\Common\\InstanceMetadata\\InstanceMetadataClient: https://github.com/aws/aws-sdk-php/blob/d18023be217df1e350e5caf545bc22b71de89c42/src/Aws/Common/InstanceMetadata/InstanceMetadataClient.php#L73\nIt would be much easier to mock this method to return a special credentials object. Does that help, or would you still want help mocking the current implementation?\n. > I'm attempting to do this with PHPUnit but for some reason it is expecting that I send a parameter to send() which is strange since the real object is not expecting this.\nThe send method you're mocking requires a Guzzle\\Http\\Message\\RequestInterface object. You'll need to ensure that a request is passed to the method in order to mock it correctly.\n\nOK, so lets say I mock getInstanceProfileCredentials(), how do I get from\nthere to the user-data? Will this solution work offline? (just curious\nabout the offline thing, not that I need it, but I DO need to work with the\nuser-data!)\n\nMocking that method wont actually help you retrieve user data.\nHow you mock the various behavior of retrieving things from the instance metadata server is up to you. You can add an abstraction layer over the client and mock your abstraction layer. Alternatively, you could add a Guzzle\\Plugin\\Mock\\MockPlugin to the client and queue up an array of HTTP responses with the plugin: http://guzzlephp.org/guide/plugins.html#mock-plugin. We actually use this technique in some of our tests of the client. Here are a mocked HTTP responses: https://github.com/aws/aws-sdk-php/tree/master/tests/mock/metadata. Our unit test for the metadata client gives a good example of how to use the mock plugin: https://github.com/aws/aws-sdk-php/blob/master/tests/Aws/Tests/Common/InstanceMetadata/InstanceMetadataClientTest.php.\n. Thanks. I had to make a few other changes in order for this to work. This will work with Guzzle's built-in URI template expansion, but will not currently work with the PECL URI template extension. I have an open issue filed with them to look into it: https://github.com/ioseb/uri-template/issues/3\nThe best way to avoid any edge cases related to this is to let the SDK handle all URL encoding for you.\n. I actually found a couple of other edge cases (these will be merged into master soon). In the above case, it seems like you wish for a literal to be placed into the key of the object, so it should be encoded over the wire as \"%2520\" and not \"%20\".\nThis will work in all cases except you will need to rawurlencode() the key before creating a pre-signed URL. This is because the URI template expansion does not encode already encoded literals (i.e. won't convert %20 to %2520), but it will urlencode non-encoded special characters (i.e. \" \" -> \"%20\"). Because of this, it's impossible to know how after creating a request object which %20 should be double encoded and which should not.\n. I doubt that there will be anything we can do here other than add documentation, but can you give us more information about your environment?\n- What platform are you using (OS, which version, what web server, PHP version, etc)?\n- What version of the SDK are you using?\n- Are you running in Apache and seeing this problem?\n- Did this occur after recently updating?\n- Did you try manually clearing the APC cache?\n. Thanks for digging into this. Actually though, APC does not support phar files. If you're worried about performance, you should probably use Composer with an optimized autoloader: http://getcomposer.org/doc/03-cli.md#install. I'll try to reproduce this behavior and update our documentation to reflect this.\nWe do actually have downloadable distributions of the SDK available if you'd like to use them instead of installing through Composer. Take a look at http://pear.amazonwebservices.com. Each version listed provides a tarball containing the phar as well as the raw source code. You'll need to register your own autoloader if you use a tarball. We follow PSR-0, so this should fit in well with whatever autloaders you are currently using.\n. Yeah, phars work, but according to the author of the phar extension, APC doesn't cache them at all. We need to communicate this better in our documentation.\nWe currently support three distribution formats: Composer, PEAR, and a phar file. Have you considered using Composer?\n. If we were to create another distribution of our SDK, then what sort of package layout are you looking for? The SDK has a dependency on Guzzle and the Symfony2 EventDispatcher. How would we add all of that to an archive in a way that would work for all project structures?\n. This is now fixed in https://github.com/aws/aws-sdk-php/commit/ec5f32c7ef55c192a28587b82ba45f8d89f1f31d\n. :shipit: \n. This would not be something that we could add to the SDK before it is added as a feature in Amazon S3 (because the SDK is client-side and this would require being enforced server side).\n. Thanks for the report. I've made the necessary changes to the SDK. These changes can be found in the master branch and will be present in the next release.\n. I was unable to reproduce this error, but we've made a few changes in the latest release of the SDK that may address this issue. Can you update your SDK installation and version of Guzzle to see if this issue still persists?\n. Removing files from this repository and replacing them with git submodules would be a major breaking change for users who are installing the SDK using git and would add a great deal of overhead to maintaining the SDK. I recommend using Composer to manage your dependencies and bring the SDK into your projects as needed.\nWe offer a number of alternative ways to install the SDK: Composer, PEAR, GitHub, Phar.\n. Calling S3Client::factory() will always return a new instance of an S3Client class.\nIf all you're looking for is network connectivity, have you thought about just doing a ping or telnet from the command line?\n. If that's what you're looking for, then yes, the doesBucketExist() method should serve this purpose for all cases.\nCan you please post to the developer forums if you'd like to discuss this further? It will be easier to track there: https://forums.aws.amazon.com/forum.jspa?forumID=80\n. The SDK throws exceptions that you need to catch in order for your script to not exit early.\nYou can catch exceptions to check for any specific errors. For example, let's say you just call $s3client->headBucket() to send a HEAD request to a bucket. You could catch Guzzle\\Http\\Exception\\CurlException to catch network level exceptions. Catch Aws\\S3\\Exception\\NoSuchBucketException to check if the bucket does not exist. Catch Aws\\S3\\Exception\\SignatureDoesNotMatchException to check if your credentials are invalid.\nHere is a direct link to post on the forums: https://forums.aws.amazon.com/post!default.jspa?forumID=80\n. :ship: \n. :shipit: \n. Thanks for the thorough error report. This has been fixed in master and will be available in the next tagged release.\n. Hi. There's no need to add any additional features for any AWS client to support the AsyncPlugin. You can just use the async plugin with the SimpleDbClient object as you normally would.\nphp\nuse Guzzle\\Plugin\\Async\\AsyncPlugin;\n$client = $aws->get('SimpleDb');\n$client->addSubscriber(new AsyncPlugin());\nEdit: This is an example of how to use it, though I highly discourage using it.\n. The AsyncPlugin is not guaranteed to work with anything. I highly discourage using it as it changes the way the SDK works, there's no promise that it actually will be \"async\", and probably will cause an exception when used with the SDK.\n. I just downloaded the phar and was unable to reproduce this error. What operating system are you using? What version of PHP?\n. APCs and phar files don't get along very well, especially when running under the CLI SAPI. Does running PHP through php5-fpm run PHP through the CLI SAPI? If so, then you'll likely not want to use the phar. I'd recommend installing the SDK using Composer: http://docs.aws.amazon.com/aws-sdk-php-2/guide/latest/installation.html#installing-via-composer.\n\nBtw, are there any sample codes with full implementation?\n\nI'm not sure what you want to see a full implementation of, but here is a link to our user-guide that contains plenty of sample code on using various clients and information to help you get started: http://docs.aws.amazon.com/aws-sdk-php-2/guide/latest/index.html\n. It's definitely possible to send a request that uses an HTTP stream.\nThe problem here is that the SDK could not determine the Content-Length of the request because you passed in a stream. Because there is no Content-Length header and the HTTP protocol being used is HTTP/1.1, the SDK (specifically Guzzle) sent the request with the Transfer-Encoding: chunked header. Because Amazon S3 does not support chunked uploads, you received a 501 response.\nYou should specify a \"ContentLength\" parameter in your $s3->putObject() call to tell the SDK the size of the file. Try something like this:\nphp\n $s3->putObject(array(\n    'Bucket' => 'my-bucket',\n    'Key'    => 'my-object',\n    'ContentLength' => filesize($url),\n    'Body'   => fopen( $url, 'r' ),\n    'ACL'    => Aws\\S3\\Enum\\CannedAcl::PUBLIC_READ\n));\n. This issue is now resolved and the fix will be available in the next tagged release. In the meantime, if you use the master branch of the SDK and the master branch of Guzzle, then you can use non-seekable HTTP streams in your PutObject requests immediately. Here's the related commit in Guzzle: https://github.com/guzzle/guzzle/commit/487250f2c62783a5e57451810116deb46ff8ecc0\n. I wasn't able to reproduce this error. Which version of the SDK are you using? Can you provide a minimal sample we can use to reproduce this?\n. I'm not sure why this error is triggered on your system and not one mine, but my guess is that you have a custom autoloader that sits in front of the SDK's autoloader. Is this the case? Does the backtrace related to this warning indicate that it's caused by an autoloader?\nThe only reference to \"PutObjectCopy\" I could find was an incorrect command alias in the S3Client class. While I wasn't able to reproduce this error, I'd be curious if this has any effect on you. Can you please pull from master of the SDK and see if the warning is still triggered?\n. Another question: how are you installing the SDK and what version are you using?\n. Thanks for letting us know the issue is now resolved.\n. I saw this on StackOverflow earlier: http://stackoverflow.com/questions/16153960/php-aws-sdk-throwing-unknown-error.\nIt's strange that you'd get this error. CURLE_COULDNT_RESOLVE_HOST is a valid cURL constant that has been around since well before PHP 5.3 (looks like it's been in cURL since 2004). Are you building PHP and curl from source for some reason? Are you able to use other cURL error codes like CURLE_COULDNT_CONNECT correctly?\nDoes a script that just calls curl_version(); complete successfully?\n. All requests sent to Amazon EC2 from the SDK should be POST requests. Are you seeing GET requests? We'll look into the charset=UTF-8 addition.\n. Thanks for the fix! I made a change in Guzzle that addresses this issue. You can update to the master branch of Guzzle or wait for the next tagged release of Guzzle.\nhttps://github.com/guzzle/guzzle/commit/026a68286dee6ce6c7e5f76baa4543acc25d1573\n. I'm going to refactor this a bit.\n. We need to add this to our documentation, but there's a simple way to delete a large number of objects from a bucket using the results of a ListObjects operation. I don't we will implement a wildcard option for the deleteObject operation because Amazon S3 objects can actually contain \"*\" characters in the keys.\nInstead of trying to pass a large array of keys to the deleteObject operation, you could use the ClearBucket object  and provide a custom iterator that only returns the keys from a bucket you wish to delete.\n``` php\nuse Aws\\S3\\Model\\ClearBucket;\n$iterator = $s3Client->getIterator('ListObjects', array(\n    'Bucket' => 'test_bucket',\n    'Prefix' => '/subfolderprefix/'\n));\n$clear = new ClearBucket($s3Client, 'test_bucket');\n// Be sure to set the custom iterator to ensure that you only delete keys with the prefix\n$clear->setIterator($iterator);\n// Clear out the matching objects using batches in parallel\n$clear->clear();\n```\n. Can you try just looking at the results of what the iterator returns? That way you can make sure the iterator settings are correct. Try something like this:\n``` php\n$iterator = $s3Client->getIterator('ListObjects', array(\n    'Bucket' => AWS_FS_BUCKET,\n    'Prefix' => /myprefix/' // top level pseudo \"folder\" in bucket\n));\nforeach ($iterator as $object) {\n    var_export($object);\n}\n```\n. Sorry about that error. We'll make a fix to check if the key is present. In the meantime, you can use a 'ListObjectVersions' iterator instead.\n. I updated the ClearBucket abstraction to work with a ListObjects iterator. This change will be available in the master branch and present in the next release. Thanks for the help.\n. How would you make it easier to find this particular code in the API docs? What about the current layout isn't working?\nIt's possible to contribute to our user guide content, but it is not possible to contribute to our API documentation generation. It requires quite a bit of customization to ApiGen and plugins, so we haven't publicly exposed that process.\n. Oh ok. The API docs are generated from PHPDoc comments that are above methods. If you modify the docblock with additional information, then that will be reflected when we build the next release of the SDK.\n. I did some refactoring to Guzzle (the underlying framework of the SDK), and I've corrected this issue. The problem was that there was a circular reference of a request to a response that was causing the request to and it's associated objects to have a refcount > 0, so they only get cleaned up when the garbage collection runs (cleaning up circular references like this). You can update to the dev-master version of Guzzle to get this change.\nIn the meantime, I suggest that you close the file yourself when you're done with it to ensure that memory and resources are managed in a way suitable to your use case.\n``` php\n// Open the file handle manually\n$handle = fopen($payloadFile, 'r');\n$s3response = $s3->putObject(array(\n    'Bucket'        => self::S3_BUCKETNAME,\n    'Key'           => $targetKey,\n    'Body'          => $handle,\n    'Metadata'      => array(\n        'job_name'  => $this->getAttribute('job_name'),\n        'job_start' => strval($this->getAttribute('job_start'))\n    )\n));\n// Explicitly close the file handle when done\nfclose($handle);\n```\n. Which version of the SDK are you using? I am seeing that the signature in a pre-signed URL is already URL encoded. Adding this additional URL encoding would break the existing implementation.\n``` php\n$client = Aws\\S3\\S3Client::factory(array(\n    'key'    => '',\n    'secret' => '',\n));\n$bucket = 'my_bucket';\n$client->putObject(array(\n    'Bucket' => $bucket,\n    'Key'    => 'data.txt',\n    'Body'   => 'test'\n));\n$command = $client->getCommand('GetObject', array(\n    'Bucket' => $bucket,\n    'Key'    => 'data.txt'\n));\n$signedUrl = $command->createPresignedUrl('+10 minutes');\necho $signedUrl . \"\\n\\n\";\necho file_get_contents($signedUrl);\n```\nThe output for my test was something like this (notice that the trailing equal sign of the base64 encoding was url encoded to %3D):\nhttps://s3.amazonaws.com/t1234/data.txt?AWSAccessKeyId=AKIA*******&Expires=1521212898&Signature=*****drsTSY%3D\ntest\n. Thanks!\n. You can catch the CurlException separately or loosen your catch statement to catch a more generic \\Exception class.\nFor example:\nphp\ntry {\n    $result = $client->putObject(array(\n        'Bucket' => $this->bucket,\n        'Key' => 'teste/data.txt',\n        'Body' => 'Hello!'\n    ));\n    return $result;\n}  catch (\\Exception $e) {\n    // handle error\n}\nAlternatively:\nphp\ntry {\n    $result = $client->putObject(array(\n        'Bucket' => $this->bucket,\n        'Key' => 'teste/data.txt',\n        'Body' => 'Hello!'\n    ));\n    return $result;\n}  catch (S3Exception $e) {\n    // handle error\n}  catch (CurlException $e) {\n    // handle error\n}\nDoes that help?\n. Thanks!\n. I think the answer to this question is much like the answer for any other library. Send us a message on the forums or via email and let us know your ideas. If it sounds like something general purpose enough for a significant group of customers and the benefit outweighs the cost of the addition (e.g. size, complexity, additional dependencies, etc), then we will consider adding it to the core of the SDK. If it is beneficial but is too costly of an addition to be added to the core, then we will consider adding it as a suggested dependency and adding a link to the documentation.\n. PR is merged. Closing.\n. You're absolutely correctly about the events. We'll take a look ASAP.\n. Thanks!\n. Are you attempting to run tests on every single PHP file in the repository? The phar-stub.php file should not be tested alone; it is only used as the stub to the phar file that we generate. Try changing your PHPDepend settings to only examine files under src/.\n. No problem. Thanks.\n. Which version of the SDK are you using? There was an issue in an older version of the SDK (really Guzzle) that caused file handles to remain open due to the zval ref count of the internally created EntityBody objects to always be greater than 0 due to circular references. Newer versions of the SDK don't have this issue. If you're using an older version, try running gc_collect_cycles() in your loop and see if it fixes the problem.\nHow many files are you trying to send in parallel? If it's a lot, you should use a batch abstraction:\n``` php\nuse Guzzle\\Batch\\BatchBuilder;\n$batch = BatchBuilder::factory()\n    ->transferCommands(10)\n    ->autoFlushAt(10)\n    ->build();\nforeach ($files as $file) {\n    $batch->add($s3->getCommand('PutObject', $opts));\n}\n$commands = $batch->flush();\n```\n. Good points, Jeremy.\nThere's no magic number for transferring things in parallel. It depends on the size of the files being transferred and how much bandwidth you have available. I think that if it's a lot of smaller files, then you can send more of them in parallel. Lots of larger files, and I think you should lower the number of parallel requests. You definitely should experiment to see what's right for your use case.\n. transferCommand(X) specifies that you will transfer X requests in parallel when the batch is flushed.\nautoFlushAt(Y) will automatically flush the batch when the size of the queue reaches Y items. Most workflows warrant that you should set the autoFlushAt value Y to be same as the transferCommand() value X.\nphp\n$batch = BatchBuilder::factory()\n    ->transferCommands(10)\n    ->autoFlushAt(40)\n    ->build();\nDoing that will cause the batch to flush when 40 commands are in the queue. When flushed, the batch will send 4 different batches of 10 requests in parallel.\nYou need to grab the exceptions of the BatchTransferException and inspect each one to see why there was an error. \nphp\n$previous = $exception->getPrevious();\nThat previous exception is probably a MultiTransferException: http://guzzlephp.org/api/class-Guzzle.Http.Exception.MultiTransferException.html.\n. @skyzyx I pushed a change up to the master branch of Guzzle that will make these exception messages more informative. If you get a chance, could you try updating to dev-master of Guzzle and running your script again?\nChange: https://github.com/guzzle/guzzle/commit/babaad5325e80dff0bf15f024345f9ac609772d5\nThis will make exception collection print something like:\n(Exception) Test\n(Guzzle\\Common\\Exception\\ExceptionCollection)\n    Meta description!\n    (Exception) Test 2\n. Uploading from a local file should not use a PHP temp stream. If you are uploading from a local file, you can get the URI of the body being sent by the command using:\nphp\n$command['Body']->getUri();\n// or using the command's request\n$command->getRequest()->getBody()->getUri();\n. This error is likely occurring because no Content-Length header is being sent with your request. Amazon S3 requires a content-length for every operation. I believe using the input stream would work, but you will need to add a \"ContentLength\" parameter to the operation.\nOn Jun 13, 2013, at 7:31 PM, Cory Kilger notifications@github.com wrote:\n\nI'm writing a web service where I want to be able to take a POST, do some user authentication, and then put the whole request content to my S3 bucket. I'm trying to use the input stream itself (under the perhaps naive hope that it might be faster than file_get_contents() if it can upload to S3 while sill receiving data?). Issue is that I always get this exception:\nPHP Fatal error:  Uncaught Aws\\S3\\Exception\\NotImplementedException: AWS Error Code: NotImplemented, Status Code: 501, AWS Request ID: 1D5F10665EDBF4B4, AWS Error Type: server, AWS Error Message: A header you provided implies functionality that is not implemented, User-Agent: aws-sdk-php2/2.3.4 Guzzle/3.6.0 curl/7.28.1 PHP/5.4.10\n  thrown in /Users/cmkilger/Dropbox/Development/Cocoa/Native/NativeWeb/NativeServer/vendor/aws/aws-sdk-php/src/Aws/Common/Exception/NamespaceExceptionFactory.php on line 91\nHere's what I'm doing:\n$client = S3Client::factory(array(\n    'key'    => $key,\n    'secret' => $secret,\n    'region' => $region,\n));\n$result = $client->putObject(array(\n    'ACL'    => 'public-read',\n    'Bucket' => $bucket,\n    'Key'    => $filename,\n    'ContentType' => $contentType,\n    'Body'   => fopen('php://input', 'r'),\n));\n\u2014\nReply to this email directly or view it on GitHub.\n. I'm going to go ahead and close this issue. Please feel free to reopen it if you continue to have problems after adding a ContentLength parameter.\n. The only thing I can think of is that the glob pattern is not matching anything. Can you try giving an absolute path followed by the pattern?\n. You wouldn't use a KeyPrefix there, but rather you'd use a custom baseDir. Setting a base dir will strip the base directory from the key that is generated.\n\nFilename => /foo/bar/bar.jpg\nBase dir => /foo/bar\nKey = bar.jpg\n. What makes you think it is not supported? The parts of the SDK that are mentioned in the documentation uses fields that are currently supported (like setting ReturnPath on SendEmail for example).\n. Thanks for the heads up. I've pushed a fix to ensure that the key exists before iterating over \"Messages\".\n. This is an issue with your client configuration settings. You are attempting to access a bucket in a different region than the region setting configured with your client. If you will be working with buckets across multiple regions using the same client, then you should either omit a region setting or use \"us-east-1\" as you discovered.\n. Are you saying that you are getting this error when using the us-west-2 region accessing a bucket in the us-west-2 region? Can you confirm that the bucket is located in us-west-2? What's the name of your bucket?\n. That's what I was saying when I said you can either omit the region setting or use us-east-1 to use the global region. It sounds like you might be referencing the bucket using a client that is configured to send requests to a region that is not the same region as the bucket. Judging from your bucket name, I'd guess it's in eu-west-1.\n. Thanks for checking.\nIt's actually sometimes required to use a region specific endpoint when accessing new buckets before the DNS records have propagated across all regions.\n. Closing per our conversation\n. Hm. That line can probably be removed anyways.\nOn Jul 13, 2013, at 8:59 PM, Jeremy Lindblom notifications@github.com wrote:\n\nCool. Seems to cover everything I can think of off of the top of my head. Looks easy enough to add more later if we want. One of the lines says date.timezone in is currently set to '' but should be set to true., but I think the suggestion to set it to true is not right.\n\u2014\nReply to this email directly or view it on GitHub.\n. Merged.\n. Hi. Sorry for the delay. This took a while to track down, but the order in which you specify options in the parameters array will effect how the body is serialized. For each Grant please specify the Type attribute before the URI attribute so that the XML is serialized correctly.\n\nphp\n$_defaultGranteeURI = 'http://acs.amazonaws.com/groups/s3/LogDelivery';\n$this->_s3Client->PutBucketLogging(array('Bucket' => 'some.bucket.test', 'LoggingEnabled' => array(\n    'TargetBucket' =>  'some.bucket.test',\n    'TargetPrefix' => 'log/',\n    'TargetGrants' => array(\n        array(\n            'Grantee' => array('Type' => 'Group', 'URI' => $_defaultGranteeURI) ,\n            'Permission' => 'WRITE'\n        ),\n        array(\n            'Grantee' => array('Type' => 'Group', 'URI' => $_defaultGranteeURI) ,\n            'Permission' => 'READ_ACP'\n        )\n    ))\n));\nThe same approach should be taken when using SetAccessControlPolicy.\nBecause the XML body is built recursively using XMLWriter, I don't currently have a good solution for making the ordering of arguments that effect XML namespaces of a particular element insignificant. We will look and see if there is something we can do to address this, but that change would need to be made in Guzzle rather than the SDK.\n. I think that attempting to SSH into any EC2 instance can be a slippery slope. What if they want to SSH in using a different user? What if they want to SSH in with a password instead of SSH keys? What if their SSH keys have a passphrase?\nWhat do you think about us just offering a waiter that polls a server on a particular port until it is accessible? I think that once a server starts accepting connections on port 22, then SSH is available. This would also allow us to wait until HTTP is available by polling port 80, HTTPS 443, etc. I think it would also be significantly less code. This can be achieved quite easily using fsockopen with a long timeout setting. Thoughts?\n. We discussed this, and we agreed that a better approach would be polling an instance until it responds to a particular port. This would address waiting for SSH, HTTP, or any other service to become available. I will add an issue to our issue tracker, but also feel free to submit the polling waiter if you beat us to it.\n. Thanks for the contribution. Could you also fix the failing test please?\n. Yeah you did-- sorry. I'm doing a lot of things at once and missed that. I'll take a look at the failing test.\n. Merged. Thanks!\n. We can look into this to see if there's something we can do, but I worry that changing this now would be a breaking change.\nInstead of catching these separately, you can catch their common ancestor: \\RuntimeException:\nphp\ntry {\n    $s3->upload(...);\n    return true;\n} catch (\\RuntimeException $e) {\n    return false;\n}\nIf you're writing a function that returns true on success or false on failure, then wouldn't you rather catch \\Exception to catch all exceptions that could occur during the transfer?\n. I think I should explain the key differences between S3Exception and Guzzle's cURL exception.\nAws\\S3\\Exception\\S3Exception (which extends from Aws\\Common\\Exception\\ServiceResponseException) assumes that the web service returned an invalid response. It parses the error response and populates the exception object with helpful properties to help you better handle the error case.\nGuzzle\\Http\\Exception\\CurlException is thrown when a response is not received due to a network error. This exception has no response, no AWS error code, etc. You are much more limited with what you can do with a CurlException than you are with an S3Exception.\nIf I were to wrap CurlException with S3Exception, then I'd create an exception object that cannot be used like all of the other S3Exception objects because it wouldn't have a response or AWS specific error information.\nIf CurlException were to be caught and thrown in a different exception, then I think Aws\\Common\\Exception\\RuntimeException would be more appropriate. What do you think would be the best approach?\n. I'll ask the Amazon S3 team to get their feedback on this question. In the meantime, I believe that using a cors enabled bucket could be a good solution: http://docs.aws.amazon.com/AmazonS3/latest/API/RESTOPTIONSobject.html.\n. What is your use case for needing to know up front if you have write permissions?\nIf you just want to check before attempting to write data, then I think it would actually be better and a more general purpose approach to just attempt to write the data and catch any exceptions that might be thrown from attempting to write. The reason I say this is because there are a lot of factors that determine whether or not you can write to an object:\n1. Check the object's ACL for a specific grantee's write permission. This requires that you have READ_ACP permissions, so the request to get an object's ACL could fail as well. This also requires that you can disambiguate the various grantee types (email, account ID etc) and that you have a grantee before checking the ACL.\n2. You could try the OPTIONs request approach, but this requires that a bucket has a cors configuration. Not all buckets have this, so attempting to download a bucket's cors configuration could fail as well.\n3. You could try checking the bucket's policy, but this requires that you are the bucket owner and that the bucket has a policy.\n\nMy app uses CakePHP 2.4 and I am only integrating a few S3 functions in the SDK. Once I have a more stable module, how do I contribute to that?\n\nWhile we do not place third party integrations into the SDK itself, we would be happy to help promote third party integrations that make it easier to use the AWS SDK for PHP with other frameworks. Just let us know via the forums when it's done and we can take a look.\n. We actually do need a region value for services that use regions because we still need to know how to properly sign the request. You can, however, override the default endpoint for a particular region by providing a base_url. The documentation is actually wrong in stating that a region is not required if you pass a base_url. I'll make sure this gets addressed.\n. Thanks for reporting this issue. Client factory docblocks and the generated API documentation will be cleaned up with the next release of the SDK.\n. @jeremeamia Ping\n. Good suggestion. We could make that change without an issue.\nNote though that we already catch 5xx and 4xx responses automatically and convert them into exceptions specific to the web service (e.g. S3Exception, NoSuchKeyException, etc), and the only service that I know that redirects is Amazon S3 (and it only has a single redirect).\n. Actually, now that I look at it more in depth, we already intercept any possibility of a RequestException being thrown, so there's no need for a change.\n. Thanks for pointing this out. I think this would be a good change to make as it's not currently working the same way as other stream wrappers. Could you send a PR and then we can further discuss the possibility of making this a non-breaking change?\n. Merged. Thanks for the PR.\nI made a couple of other similar changes to ensure that the stat caching used with things like crawling through opendir or RecursiveDirectoryIterator are still giving cache hits.\n. I just submitted a pull request that increases the connection timeout of the InstanceMetadataClient from 1 to 5 seconds (I rebased a couple times, so it is showing up multiple times in this issue). I also noticed that if you happen to have something listening to http://169.254.169.254 but not responding, then the connection will hang forever. Because of this, I added a timeout setting of 10 seconds for the entire transaction. Does a 5 second connection timeout and 10 second total timeout provide enough of a window for your use case?\nWhen credentials aren't provided to a client, the client automatically attempts to retrieve credentials using the InstanceMetadataClient. I'm hesitant to have the client block for much longer in the event that a user not on Amazon EC2 forgets to enter their credentials (it would appear as if the script is doing nothing for several seconds).\n\nOffer a non-APC caching solution (dropfile on disk, etc) that can be shared across processes. Ideally it'd be a fallback to APC.\n\nIf you know that you are going to use IAM role credentials, then you can build up a credentials object that uses the exact client timeout and caching behavior of your choice. For example, here we are creating a custom credentials object that uses an extended timeout setting and a custom cache storage.\n``` php\n<?php\nrequire 'vendor/autoload.php';\nuse Aws\\Common\\Enum\\ClientOptions as Options;\nuse Aws\\Common\\Credentials\\Credentials;\nuse Aws\\Common\\InstanceMetadata\\InstanceMetadataClient;\nuse Aws\\S3\\S3Client;\nuse Doctrine\\Common\\Cache\\FilesystemCache;\nuse Guzzle\\Cache\\DoctrineCacheAdapter;\n// Create an instance profile client with custom timeouts\n$instanceClient = InstanceMetadataClient::factory(array(\n    'request.options' => array(\n        'connect_timeout' => 20,\n        'timeout'         => 120\n    )\n));\n// Create a cache adapter that stores data on the filesystem\n$cacheAdapter = new DoctrineCacheAdapter(new FilesystemCache('/tmp/cache'));\n// Create a credentials object using the factory method\n$credentials = Credentials::factory(array(\n    Options::CREDENTIALS_CACHE     => $cacheAdapter,\n    Options::CREDENTIALS_CLIENT    => $instanceClient,\n    Options::CREDENTIALS_CACHE_KEY => 'shared_iam_credentials'\n));\n$s3 = S3Client::factory(array('credentials' => $credentials));\n$s3->listBuckets();\n```\nYou can also create credentials without using the factory method of the Credentials object to gain more control (or perhaps insight into what is exactly being created):\n``` php\n<?php\nrequire 'vendor/autoload.php';\nuse Aws\\Common\\Credentials\\Credentials;\nuse Aws\\Common\\Credentials\\CacheableCredentials;\nuse Aws\\Common\\Credentials\\RefreshableInstanceProfileCredentials;\nuse Aws\\Common\\InstanceMetadata\\InstanceMetadataClient;\nuse Aws\\S3\\S3Client;\nuse Doctrine\\Common\\Cache\\FilesystemCache;\nuse Guzzle\\Cache\\DoctrineCacheAdapter;\n// Create an instance profile client with custom timeouts\n$instanceClient = InstanceMetadataClient::factory(array(\n    'request.options' => array(\n        'connect_timeout' => 5,\n        'timeout' => 20\n    )\n));\n// Create an expired credentials object\n$expired = new Credentials(null, null, null, -1);\n// Create a refreshable instance profile credentials object that will\n// refresh credentials using the custom client\n$credentials = new RefreshableInstanceProfileCredentials($expired, $instanceClient);\n// Create a cache adapter that stores data on the filesystem\n$cacheAdapter = new DoctrineCacheAdapter(new FilesystemCache('/tmp/cache'));\n// Create a decorated credentials object that stores the cached credentials\n// in the same file each time it is created/loaded\n$credentials = new CacheableCredentials($credentials, $cacheAdapter, 'shared_iam_credentials');\n// Create a client object using the customized credential provider\n$s3 = S3Client::factory(['credentials' => $credentials]);\n// Trigger the credentials to load\n$s3->listBuckets();\n```\nIf you are using a service builder to create clients, then you can create the custom credentials as described above and share them with each client created by the service builder by modifying the service builder and specifying default credentials for each client.\n``` php\n<?php\n// Continuing from the above code example\nuse Aws\\Common\\Aws;\n// Share the customized credentials with every client\n$aws = Aws::factory(array(\n    'credentials' => $credentials\n));\n// Get an S3 client with the custom credentials\n$s3 = $aws->get('s3');\n// Force the credentials to load\n$s3->listBuckets();\n```\nDoes this help? Is there anything else we can do to make this fit your use case?\n. I'll go ahead and resolve this issue. Please feel free to reopen if you run into any further issues.\n. Right\n. We intentionally avoided creating empty pseudo-folders in the stream wrapper. Allowing empty pseudo-folders forces a lot of application layer code in the abstraction. The typical way this is achieved is by uploading a file with some special name (e.g., \"$bucketname$\").  Implementing this would add additional requests for various operations (does a file exist by this key?, do objects exist under this prefix?, does the place holder exist?), requires a large number of changes that would complicate the stream wrapper even further, and causes the problem of having an arbitrary place holder file that is relevant only to the abstraction that created it.\nThe Amazon S3 stream wrapper provides a different interface that PHP developers can use to interact with Amazon S3. We feel that the mkdir() functionality currently implemented is the most appropriate abstraction to offer and it works in the way Amazon S3 normally behaves.\n. This has been addressed in e4b670a\nYou can supply a manual Content-Length header by add a ContentLength parameter to the operation.\nHere's the information I added:\n\nYou must supply a \"ContentLength\" parameter to an operation if the steam does not respond to fstat() or if the fstat() of stream does not provide a valid the 'size' attribute. For example, the \"http\" stream wrapper will require a ContentLength parameter because it does not respond to fstat().\n. > Would a pull request be accepted that added an event when the multipart upload is created that could be subscribed to?\n\nSure. I think adding an event after creating an UploadBuilder but before calling build would be great (https://github.com/aws/aws-sdk-php/blob/master/src/Aws/S3/Sync/UploadSync.php#L67).\n. @timmow How does that PR look? Will that help?\n. This is an SDK specific issue. This issue will likely be fixed and packaged into a release early next week.\n. Thanks for reminding us that this needs to be documented.\nI don't think there is any way to workaround the ini limit enforced by precision. Even if Amazon DynamoDB did not serialize number values in strings and allowed JSON primitives, using json_encode directly with a float can lose precision:\nphp\nini_set('precision', 3);\nvar_export(json_encode(['foo' => 2.12345]));\nOutputs: '{\"foo\":2.12}'\nSo I think there are a two options:\n1. Increase the precision ini value\n2. Build the request without using the formatAttributes() method and pass the float value in as a string\n. Actually, the SDK guesses the mime-type of a file based on file extensions. I suspect that the issue here is that the file extensions are in uppercase and the checks are not case-insensitive.\n. This has now been addressed in Guzzle: https://github.com/guzzle/guzzle/commit/647cd469ce63efc8908a604e866ee322e82a693c. I'll tag a release of Guzzle in the next week or so, but you could run off Guzzle's master branch in the meantime if needed.\n. You would not be able to use the latest master version of Guzzle. If you use the phar, then this update will only be available to you in the next tagged release of the SDK.\n. :ship: \n. Are you being redirected when you try to upload the object? If it's possible to reproduce, can you add curl debug output to your client and provide the output of a failing request to this issue?\nphp\n$client->getConfig()->set('curl.options', array(CURLOPT_VERBOSE => true));\nThis will show us if the request is being redirected, maybe a 100-continue status is tripping things up, maybe the connection is being interrupted, etc.\n. Thanks. If you are sending small payloads that are not being redirected, then you could try using the body_as_string cURL option to send the body payload as a string with cURL rather than using a CURLOPT_READFUNCTION (which is the default). cURL sometimes has issues when using a read function because it doesn't know how to rewind a function. However, when giving cURL a string, it manages rewinding the stream internally.\nphp\n$client->getConfig()->set('curl.options', array('body_as_string' => true));\nI'll go ahead and resolve. Please feel free to reopen the issue if you can provide a wire log of the issue.\n. The different in performance should not be significant or measurable. From the tests I've done, it's basically the same speed and can go either way.\n. Yeah, this would be a cool feature. We will add this to our backlog.\n. I'm moving this feature request to our team's internal backlog where we can track it and prioritize it more effectively.\n. Can you provide a code sample that illustrates what you mean? Fell free to use a fake distribution, keys, etc.\n. Thanks for reporting this issue. This was an issue with the Guzzle library and how unit timestamps were serialized as strings. This has been fixed in this commit: https://github.com/guzzle/guzzle/commit/03ebadab3ccbbbaf7e8fbb0e891a463e4c6eb572. This change will be present in the next tagged release of the SDK. In the meantime, you can use the master-dev version of Guzzle with the SDK if you are installing the SDK through Composer.\n. Can you provide verbose cURL output for one of these requests (be sure to strip any identifying information). I wonder if for some reason cURL is not adding a Content-Length header once the POST body exceeds a certain threshold.\nSo you'd add this:\nphp\n$this->_client->getConfig()->set('curl.options', array(CURLOPT_VERBOSE => true));\n. Thanks for the verbose output. I'll forward this information along to the Amazon CloudSearch team to help me investigate.\n(p.s. I removed your AccessKeyId and computed signatures from the issue).\n. Does this issue still occur if you use the master branch of the SDK? I've been told that this problem can sometimes occur when using signature v2. I've updated the SDK to now use signature version 4 with Amazon CloudSearch, so I think this should no longer be an issue after upgrading.\n. Thanks for letting us know. I'll talk more with the Amazon CloudSearch team to see what's going on.\n. @banasiak are you still having issues as well?\n. Thanks for the clarification. From an SDK point of view, this isn't an issue with the client. I've spoken with the Amazon CloudSearch team, and they would prefer it if you posted to their forum with a timestamp of when you sent the request using signature version 4 that received the 505 error.\nhttps://forums.aws.amazon.com/forum.jspa?forumID=137\n. I've submitted a ticket with our documentation team to look into this issue. We pull our SDK docs directly from the service teams' documentation, so using the \"Tell us about it...\" link at the top of an API documentation page is the fastest way to report a service specific documentation issue.\n. Here's the equivalent feature in the PHP SDK: https://github.com/aws/aws-sdk-php/blob/master/src/Aws/S3/Model/PostObject.php\n. Thanks for the feedback. We'll try to make this more discoverable. Let us know if you run into any issues.\n. I have a bunch of questions for you :)\n1. What platform are you running this on?\n2. What version of PHP?\n3. Are you using APC?\n4. Is this run from the command line?\nYou can grab the SDK version number at runtime using echo Aws\\Common\\Aws::VERSION;\n. Can you provide a minimal code example that shows your use case?\n. Thanks for providing the example.\nCreating a pre-signed URL using a completely custom request will not invoke the bucket style listener. Using a custom request, the SDK will sign the request as-is. This is the expected behavior of the SDK. Did you find documentation that stated otherwise?\n. Creating a pre-signed URL using a custom request object will sign the request literally as it is given to the method. If you provide a valid URL in the request you create, then the pre signed URL will work correctly. This provides a much lower-level interface to the SDK that I think is important to keep. For example, this allows you to use a completely custom URL through a CNAME and still successfully sign the request.\nYou'll need to use the URL-presigning functionality of the Command objects or the getObjectUrl() method of the client in order to utilize the BucketStyleListener.\nLooking over the docs, I see that we do call out that using createPresignedUrl() uses a completely custom request object.\nIf you need more flexibility in creating your pre-signed URL, then you can create a pre-signed URL for a completely custom Guzzle\\Http\\Message\\RequestInterface object. You can use the get(), post(), head(), put(), and delete() methods of a client object to easily create a Guzzle request object.\nThe documentation is hosted on GitHub. Here's a direct link to the Amazon S3 docs: https://github.com/aws/aws-sdk-php/blob/master/docs/service-s3.rst. We love contributions, especially to our documentation. Feel free to make edits as you see fit and send a pull request, or just let us know how you think the docs can be improved.\n. The API documentation is generated from the docblocks of the source code. You can find this particular docblock at https://github.com/aws/aws-sdk-php/blob/master/src/Aws/S3/S3Client.php#L281.\nI'll go ahead and resolve this issue. Please let us know if you have any questions on contributing to the docblock.\n. Thanks for letting us know about this discrepancy. I was able to confirm that multiple header values should not be sorted in the canonicalized request. I'll look at your PR and get this corrected. I'm also following up with the team that owns the Signature Version 4 test suite to see about getting those two test cases corrected.\n. This is now corrected. Thanks! I'll take a look at your other PR for adding additional test cases.\n. I think it makes sense to retry curl error 18 (partial file). I've made a commit to Guzzle to add this error code to the default retried curl errors of the exponential backoff plugin: https://github.com/guzzle/guzzle/commit/d5bf6d70565da39b408d970006dec25088923e3c. This will be present in the next tagged release of Guzzle (and bundled in the next tagged release of the SDK).\n. Are you seeing that the request is retried over and over until it finally fails with cURL error 18, or is it immediately failing without retrying?\n. @matisoffn Can you provide verbose curl output of the failure?\nphp\n$s3->getConfig()->set('curl.options' => [CURLOPT_VERBOSE => true]);\n. @matisoffn Any update here?\n. > Using latest code from github, calling receiveMessage with five second timeout, max messages of 10 and requesting all AttributeNames sometimes doesn't return from receiveMessage.\nWhat happens? Do you get an exception? Do you get an empty result with no messages?\n. Thanks for the PR. I tried running these changes from my Mac, and got the following error:\n```\nPHP Parse error:  syntax error, unexpected '=' in Command line code on line 1                                                                         \nException occurred:\n  File \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/subprocess.py\", line 537, in check_output\n    raise CalledProcessError(retcode, cmd, output=output)\nCalledProcessError: Command 'php -r \"$c = include '/Users/dowling/projects/aws-sdk-php/src/Aws/Common/Resources/aws-config.php'; echo json_encode($c);\"' returned non-zero exit status 254\n```\nIt looks like the variables are being incorrectly expanded because of the double quoting you added.\n. Thanks!\n. Merged. Thanks for digging into the issue and providing a fix.\n. I believe you'll need to create a queue policy in order to specify things like \"Everybody ()\". When I created a queue in the console and enabled the \"\" option, it created a policy similar to the following:\njson\n{\n  \"Version\": \"2008-10-17\",\n  \"Id\": \"arn:aws:sqs:us-east-1:2234...:test/SQSDefaultPolicy\",\n  \"Statement\": [\n    {\n      \"Sid\": \"Sid1234...\",\n      \"Effect\": \"Allow\",\n      \"Principal\": {\n        \"AWS\": \"*\"\n      },\n      \"Action\": \"SQS:*\",\n      \"Resource\": \"arn:aws:sqs:us-east-1:1234...:test\"\n    }\n  ]\n}\nYou can modify your queue to use a different policy using the SetQueueAttributes operation. The documentation for this operation can be found here: http://docs.aws.amazon.com/AWSSimpleQueueService/latest/APIReference/Query_QuerySetQueueAttributes.html. You would pass in a policy in a JSON formatted string similar to the policy I posted above. You can find out more about creating policies in the Amazon SQS documentation: http://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/AccessPolicyLanguage_HowToWritePolicies.html\nI recommend posting specific questions about configuring queue permissions to the Amazon SQS forums where Amazon SQS engineers will be able to quickly and definitively answer your questions.\n. Unless copying and pasting into GitHub stripped some special characters, the example you called out should work because it doesn't have any special characters:\n\nposts/attachments/000/001/133/large_thumbnail/2013_2_17_18_03_19_0_LA\n\nHowever, I see in your stack trace that you have keys that contain the \">\" character. While Amazon S3 allows keys to be stored using reserved XML characters, attempting to parse and XML document that contains these characters results in a parsing error. This is a known issue with the Amazon S3 service and not something that can be fixed by the client (the AWS SDK for PHP). Customer feedback is used to help prioritize development schedules, so it would be great if you could let the Amazon S3 team know that this is important to you by posting to their forum: https://forums.aws.amazon.com/forum.jspa?forumID=24\n. Thanks for clarifying. Those characters didn't show up in the stack trace for some reason.\nAn XML parsing exception due to unescaped XML characters in the XML response is a server side issue that cannot be fixed by the SDK. This error can occur when attempting to list objects in a bucket where an object contains a reserved XML character.\nDeleting an object with special characters would not cause an XML parsing error. What specific error do you receive when attempting to delete a key that contains a special character?\n. When you pass a stream resource into an API operation like getObject, the resource is actually being converted into a Guzzle EntityBody object. When an EntityBody is used with an existing resource, that resource's lifecycle is now managed by the EntityBody object. For example, when the __destruct() method of the EntityBody object is called, the underlying resource is closed.\nWhen the refcount (the number of variables the reference something) of the EntityBody object reaches 0, then the destructor of the EntityBody object is triggered which causes the stream resource wrapped by the object to be closed. The stream was not being closed in older versions due to an unnecessary circular reference. Circular references can cause problems with reference counting memory systems because the number of references for an object will always be greater than 0. In order to work around this limitation of reference counting, PHP has a garbage collection mechanism that detects cycles and cleans them up.\nWhen the unnecessary circular reference was removed, it caused the underlying resources utilized by an operation to be cleaned immediately after the number of references reached 0. This is a really good thing because it helps to prevent memory leaks in an application without forcing a developer to constantly manually trigger gc_collect_cycles when in a tight loop.\nIf you want the EntityBody to be kept around after the command completes, then you'll need to pass in an EntityBody manually:\n``` php\n<?php\nuse Aws\\S3\\S3Client;\n$bucket = '...';\n$keyname = '...';\n// Instantiate the client.\n$s3 = S3Client::factory(array(\n    'key'    => '...',\n    'secret' => '...'\n));\n$r = fopen('php://temp/', 'wb');\n$e = \\Guzzle\\Http\\EntityBody::factory($r);\n// Save object to a file.\n$result = $s3->getObject(array(\n    'Bucket' => $bucket,\n    'Key'    => $keyname,\n    'SaveAs' => $e\n));\n```\nNow the stream resource will not be closed until the EntityBody is destructed. You can detach the stream resource from the EntityBody and take back control of the resource using the detach() method of the EntityBody.\nphp\n<?php\n$e->detach();\n. We definitely wont split the SDK into subtree splits, but it's possible that we could break the package apart into discrete packages at some point (though I doubt it would happen in the 2.x series of the SDK).\nAs of right now, splitting the package up would add a huge maintenance burden for very little benefit. For example, when using autoloading, the number of classes provided by a library is irrelevant if your application doesn't actually use each and every class of a library.\n\nIf every project had a subsplit (like symfony does for it's packages) the repositories can be versioned with the service not as a whole which makes versioning more reliable and understandable.\n\nSymfony used subtree splits because of a limitation of the available packaging tools for the PHP community. I created Guzzle (the library that powers the SDK), and I too implemented subtree splits. So, speaking from experience, subtree splits are not a good solution for distributing portions of a project. Symfony, as well as Guzzle, both share tags over all of the splits, so they aren't actually versioned independently. Guzzle has 15-20 subtree splits and it takes close to an hour to replicate changes from the main repo to all of the splits. With 31 packages, it would take about an hour just to propagate code to the subtree splits.\nA better solution would be an improved Composer that can install from tarballs or zip files that contain independent parts of a larger project. For example, we could bundle up 31 different packages as zips and deploy them to a public Satis repository that Composer and Packagist could pull and proxy from. There are some changes that need to be made in Composer over time that will gradually support this sort of model.\n\nThe independent parts of the AWS services will go through iterations, but I highly doubt the entire platform is versioned (right?)\n\nThe SDK is actually versioned globally as well as per API. There's a lot of shared code under the Common namespace that is periodically updated in order to support new or different APIs.\n. The regular expression, \"preg_match('/^[a-z0-9][a-z0-9-.]*[a-z0-9]?$/', $bucket)\", prevents bucket names containing uppercase characters, so I don't think this change is necessary. I also think that this function is named poorly (and that's my fault). I'll update the description of the function to reflect the correct behavior of the function and add a test case to ensure that uppercase characters will result in a false return value.\n. If this is happening on every request, then I wonder if your installation of cURL is broken. Can you use cURL successfully with other sites or APIs?\nCan you turn on verbose cURL output so that we can better see where this error occurs?\nphp\n$cmd = $s3->getCommand('PutObject', array(\n    'Bucket'     => 'bucket_name',\n    'Key'        => 'random_key_name',\n    'SourceFile' => 'file_name'\n));\n$request = $cmd->prepare();\n$request->getCurlOptions()->set(CURLOPT_VERBOSE, true);\n$cmd->execute();\n. I wasn't able to reproduce this error. Which version of the SDK are you using?\nHere's my test script that worked correctly:\n``` php\n<?php\nrequire 'vendor/autoload.php';\n$s3 = Aws\\S3\\S3Client::factory(['key' => '', 'secret' => '']);\n$s3->uploadDirectory('/tmp/foo', 'mybucket', null, [\n    'params' => ['ACL' => 'public-read']\n]);\n```\nIt didn't make a difference as to whether or not I got the error you received, but you should pass the \"ACL\" param in all caps to ensure the ACL is set correctly on the objects you upload.\n. I'll go ahead and close this issue. Feel free to reopen if you can get more information and a reproducible script we can use to troubleshoot.\n. The SDK provides two ways to ensure an environment is configured correctly: Composer and compatibility_test.php.\n\nOf course, the issue of newly-installed PHP modules being available only on the CLI remains.\n\nThe compatibility_test.php can be run from Apache as well. It's also fairly common enough knowledge for developers to restart apache after installing new modules, that I don't think the effort required to check which operating system is running, which Linux distro is running, which modules are installed, and whether or not Apache has them running in the current process is a good path to go down.\n\nActually, grepping the code for CURLE_COULDNT_RESOLVE_HOST I see that this is an upstream Guzzle issue, not specific to AWS SDK.\n\nThere is a check in Guzzle that will warn a user if they do not have cURL installed, but it's only placed at the most common entry point to the library. Having this check sprinkled throughout the library in every point that uses cURL would get quite messy.\n. > Basically I have Symfony's Finder component, that returns an iterator to files in a directory. Then I put that iterator to UploadSyncBuilder and try to transfer all that to S3.\nAre you sure that the finder is yielding files?\n\nThis doesn't work, because ChangedFilesIterator thinks, that nothing is changed in my directory and doesn't upload anything. I surely know, that I've added a new file.\n\nNew files should get uploaded every time, even with the ChangedFilesIterator.\nYou can force and upload and not use the ChangedFilesIterator by calling force(true) on the builder. Does forcing the upload actually transfer files?\n\nAlso it seems (looking at code), that UploadSyncBuilder doesn't delete files/folders on S3, that are no longer present locally, since it does iterate through local files only and not in both directions.\n\nThat is correct. We don't currently have plans to add support for deleting files that weren't found in the sync. This issue is a good example of why we made that decision: if we were to delete files that didn't match your iterator, we would have deleted your entire bucket. It's possible to implement this yourself, but you'd need to pass in a custom iterator, call force(true) to disable the default handling of ChangedFilesIterator, and provide a custom iterator wrapped by a ChangedFilesIterator. The ChangedFilesIterator provides an array of unmatched files which you could then use to delete files: https://github.com/aws/aws-sdk-php/blob/master/src/Aws/S3/Sync/ChangedFilesIterator.php#L75.\n. > I know, but the point was exactly to do a clever upload of only missing files. That's why I specifically omitted force option.\nI'm not saying that you should run it like that forever, but I was hoping you could give a shot and let me know if it still doesn't upload anything. I'm just trying to eliminate variables so that this can be debugged.\n\nNot necessarily. This is currently implemented in s3cmd (see http://s3tools.org/s3cmd-sync). It does it somehow, but I wanted to use PHP-based solution to make my all my application parts use same library for interacting with AWS. Like rsync for S3.\n\nYes, but the matching algorithm they use is much simpler than the possibilities in the PHP SDK. The matching of local files to remote files would have to be based on the provided iterator. This isn't something we're considering adding to the PHP SDK, but it's possible for you to add given the steps I've provided.\nIf you'd like an alternative official AWS mechanism for syncing with Amazon S3, then I recommend you check out the CLI: https://github.com/aws/aws-cli.\n. Interesting. Thanks for giving that a shot. Maybe there is an issue interacting with the Symfony Finder component. I'll take a look.\nHave you tried just using the uploadDirectory method of the S3Client class? This works for me every time and should cover most used cases.\n\nFatal Error: Uncaught exception 'Guzzle\\Service\\Exception\\CommandTransferException' with message 'Errors during multi transfer (Aws\\S3\\Exception\\RequestTimeoutException) Your socket connection to the server was not read from or written to within the timeout period. Idle connections will be closed.\n\nThis error happens randomly when sending files to Amazon S3, typically when using a poor internet connection or when saturating your bandwidth. These errors are retried with exponential backoff, which can cause long delays. It might be worth trying to reduce the number of files you send in parallel.\n\nDoes AWS CLI provide more intelligent rsync algorithm for s3, then AWS PHP SDK?\n\nIf your question is if the CLI deletes remote files, then yes. As far as how flexible it is, you have much more flexibility and extensibility with the PHP SDK. If you don't need this flexibility and you are able to install the CLI, then I think it's a great option.\n. I identified an issue with how we are decoration iterators. Using a Symfony Finder iterator now works correctly when uploading files to Amazon S3 (see the above commit).\nI'll clarify a few more things before wrapping up this issue:\n\n[...] This makes me thinking, that empty sub-folders might not be uploading at all using uploadDirectory method.\n\nCorrect. We intentionally do not upload a place holder for empty folders. You'll need to put a place holder file in your sub directories if you want a place holder uploaded to Amazon S3.\n\nSo is that AWS CLI package a library of it's own or it's just an example of usage of AWS PHP SDK in CLI?\n\nHere's a link to the AWS CLI (it's a completely separate code base that runs on Python): https://github.com/aws/aws-cl.\n\nAnd why AWS CLI understands how to delete files during rsync, but AWS PHP SDK doesn't? Maybe it's possible to add that \"delete process code\" to AWS PHP SDK as well?\n\nThe CLI uses a much simpler process for syncing files. The PHP SDK has a more complex process for uploading and downloading files, filtering, changing filenames, etc.\n\n[...] It seems, that if we'd had 2 iterators (one from s3 and other from local fs), then processing them using logic from that class might actually do the trick.\n\nThe PHP SDK uses two iterators as well inside of the ChangedFilesIterator. Like I said, if you give it a bogus iterator on the left and a valid iterator on the right, then all of the files on the right would be deleted. As an example of how easy it would be to delete files accidentally: this would have happened to you.\n\nAlso comments in that file states, that file lists on both fs (s3 and local) needs to be specifically sorted to allow correct processing. I'm not sure that s3 does sort files returned by ls or similar command.\n\nThis isn't necessary for the PHP SDK.\n. Based on the reasons I've stated so far, I don't see us adding deletion any time soon. However, we use customer feedback as a metric to determine what features we work on, so we'll keep this in mind as a feature request and track it.\nIn the meantime, the AWS CLI has support for deletion from Amazon S3 based on files that don't exist locally.\n. I'm not sure I follow how deletion would benefit you in that scenario. I don't think you'll get an error if you attempt to upload a file at the same time as another process uploading a file to Amazon S3 (the latest upload wins). Deleting a file would not make a difference in this situation (mainly because: it's not necessary to delete a file if it already exists because you will overwrite it, and if you are uploading in parallel and neither have completed, then no file will be found and no delete would occur). Deletion is only useful for cleaning up files remotely that aren't present locally.\nAgain, the CLI can be used to do a traditional rsync style deletion.\nI'll go ahead and close this issue as you should be able to upload files using the Symfony Finder component now. This modification will be available in the next tagged release.\n. What are you doing that causes the \"method not compatible\" error? Are you using some kind of mocking framework or something?\n. You can get a .tgz file that contains the source of just the SDK by downloading from our PEAR channel: http://pear.amazonwebservices.com/. For example, http://pear.amazonwebservices.com/get/sdk-2.4.10.tgz.\nThere isn't currently a method to install older versions of the generated phar or zip files. You could however, check out a specific version of the Git repository and run the phar or zip build tasks through Phing (i.e., phing phar or phing zip). Running these tasks will perform the same build process we use to generate these files and places them in the build/ directory.\n. Thanks for bringing this to our attention. We've now removed enum validation from the client so that client side validation of enums will no longer be a problem after a new instance type or other similar attribute is updated server side. The client should never prevent a user from sending a valid request.\nWe've removed enums from the service descriptions, but they will still appear in our API docs. This change can be picked up in the master branch and will be available in the next tagged release of the SDK.\n. The only way to check if credentials are valid is to attempt to send a request to one of our web services.\n. The S3Client::upload() method accepts a $body argument as a Guzzle\\Http\\EntityBodyInterface, stream resource, or string of data to upload.\nhttp://docs.aws.amazon.com/aws-sdk-php/latest/class-Aws.S3.S3Client.html#_upload\n. Good catch. I was able to confirm that you can send DeleteAttributes requests without specifying a value. I'll work on getting this corrected.\n. Looks like there's a failing test that needs to be fixed.\n\nShould it be LogRecordIterator or LogRecordsIterator?\n\nI like \"LogRecordIterator\"\n\nShould it be accessible via the CloudTrail client?\n\nI lean towards \"no\". While we've done this with other abstractions, it is a bit muddier with this abstraction because it uses multiple clients.\n\nShould the options be proper case (like operation parameters) or snake case (like client config options)?\n\nDo the options map to a parameter in the service description or are they arbitrarily created for this iterator. If the former, then I say ProperCase, if the latter, then snake_case.\n. Thanks!\n. Adding arbitrary seeking isn't something that is likely to be added to the built-in Amazon S3 stream wrapper, but it seems like something that could be added in through an extension.\n\nAs an experimental solution, I extended the Guzzle\\Http\\CachingEntityBody class and overode the seek() function. Now, instead of throwing a runtime exception when the caller attempts to seek past the end of the cache, it reads in enough of the remote file to let the seek work.\nSo my question is: is that a good way to do this? Are there drawbacks that I haven't taken into account? I'm sure you guys are much more expert in the ways of filesystem-level programming than myself, so I imagine that you'd be able to come up with an even better solution.\n\nThat seems like a decent solution to me. Supporting arbitrary seeking is difficult, but possible.\n\nAfter discussing this with my boss, I decided to add an addition check to the seek operation. Now, attempting to seek more than 50 megabytes into a file will throw an exception, since that would require reading 50+ megs into RAM. Since Drupal is already quite heavy, an additional 50 megs seems like a good limit.\n\nThe CachingEntityBody uses a PHP temp stream, so only up to 2mb is loaded into memory before PHP automatically switches it to a file based cache. You limitation of 50mb is still valid-- that's a lot to cache on disk too. I just wanted to make you aware of how it works by default with a PHP temp stream.\n. Awesome. Nice work!\n. This type of error is almost always a transient error that is safe to retry. The 3.8.0 release of Guzzle made a change that automatically retries cURL error code 18 with exponential backoff. I just pushed a change to the AWS SDK for PHP to make it safe to use Guzzle 3.7.0 through 3.8.0. After pulling down this change and updating Guzzle, this should no longer be an issue.\nTo answer your question about iterators... Unfortunately PHP iteration errors cannot be caught when using a foreach statement. You'll need to use the current, valid, and next methods to catch errors during iteration. This is a limitation of PHP and not something that we can fix in the SDK. However, there might be some sort of clever iterator decorator that could catch exceptions and continue iteration (we'll look into that).\nTo add error handling, one might code something like this:\n``` php\n$c = \\Aws\\S3\\S3Client::factory();\n$i = $c->getIterator('ListObjects', array('Bucket' => 'testing'));\nwhile ($i->valid()) {\n    $object = $i->current();\n    var_export($object);\n    try {\n        $i->next();\n    } catch (\\Exception $e) {\n        $lastKey = $object['Key'];\n        printf(\"\\n\\n====\\nRequest failed at %s: %s\\n====\\n\\n\", $e->getMessage(), $lastKey);\n        $i = $c->getIterator('ListObjects', array('Bucket' => 'testing', 'Marker' => $lastKey));\n    }\n}\n```\nYou could wrap this up a bit more generically using generators with something like this:\n``` php\nfunction iter($client, $operation, $params, $errorHandler)\n{\n    $i = $client->getIterator($operation, $params);\nwhile ($i->valid()) {\n    $current = $i->current();\n    yield $current;\n    try {\n        $i->next();\n    } catch (\\Exception $e) {\n        call_user_func($errorHandler, $current, $e);\n        $i = $client->getIterator($operation, $params + array('Marker' => $current['Key']));\n    }\n}\n\n}\n$errorHandler = function ($current, \\Exception $e) {\n    printf(\"\\n\\n====\\nRequest failed at %s: %s\\n====\\n\\n\", $e->getMessage(), $current['Key']);\n};\nforeach (iter($c, 'ListObjects', array('Bucket' => 'testing'), $errorHandler) as $object) {\n    var_export($object);\n}\n```\n. You're right. The BackoffPlugin doesn't work well in situations where a request is not idempotent. This shouldn't be an issue with any AWS services, but it could potentially be an issue with other web services using Guzzle.\nThe BackoffPlugin already sort of assumes that anything can be retried and is idempotent: it uses several other curl errors to determine if a request should be retried: https://github.com/guzzle/guzzle/blob/master/src/Guzzle/Plugin/Backoff/CurlBackoffStrategy.php#L17\nI think we could potentially add a constructor argument to the CurlBackoffStrategy to tell it to only retry response errors when the request is idempotent.\n. You're right. Some operations aren't idempotent. I asked around what other SDKs are doing, and they actually have a very similar behavior that just retries all networking issues.\n\nI'm sure this will require the SDK to know for every single operation type whether it can be considered idempotent or not.\n\nYeah, this is the root of the issue. We don't currently have an automated way to know which operations are idempotent and which are not. This becomes a really big problem for our RPC type services that send POST requests for idempotent and non-idempotent requests. It might even be more complicated than that: for example, a request might become non-idempotent if a particular parameter is specified. This is something we hope to address in the future.\nThe Amazon EC2 token approach would be a great way to handle idempotency across all AWS web services, but that sort of support in every API would be a long ways off if it is indeed a possibility because of the sheer scope and breadth of services.\nFor the time being, if you know you are issuing a request that cannot be retried, you can remove the BackoffPlugin from the request explicitly:\nphp\n$command = $client->getCommand('NonIdempotentOperation');\n$request = $command->prepare();\n$plugin = $client->getConfig('client.backoff');\n$request->getEventDispatcher()->removeSubscriber($plugin);\nI know this isn't the greatest solution, but it is a workaround for now. We'll look more into how idempotency can become a factor in handling retry logic (it's a much larger discussion than just the PHP SDK, so it may take some time).\n. Tracking in #203 \n. I'm moving this feature request to our team's internal backlog where we can track it and prioritize it more effectively.\n. I'm looking into this issue. It looks like we are trying to make paths absolute when we don't need to be.\n. I've submitted this PR to address the issue: https://github.com/aws/aws-sdk-php/pull/207. Does this fix the problem for you?\n. Thanks for sending me the screenshots. What are your KeyConverter settings for $baseDir, $delimiter, and $prefix?\n. Unfortunately, that doesn't help much. If I could see a quick example that demonstrates the problem, then that would be extremely helpful. Thanks for you help so far.\n. Curl error 35 is described as follows:\n```\nCURLE_SSL_CONNECT_ERROR (35)\nA problem occurred somewhere in the SSL/TLS handshake. You really want the error buffer and read the message there as it pinpoints the problem slightly more. Could be certificates (file formats, paths, permissions), passwords, and others.\n```\nThis error could really be anything, so you'll need to turn on verbose cURL output in order to see what's happening. This can be done using the following snippet:\nphp\n$s3 = S3Client::factory([\n    'key' => '****',\n    'secret' => '****',\n    'curl.options' => ['CURLOPT_VERBOSE' => true]\n]);\nThe PHP manual claims that you can only set CURLOPT_SSLVERSION to 2 or 3. It looks like setting this value to 1 is forcing TLSv1 (or CURL_SSLVERSION_TLSv1).\nThe SSL version (2 or 3) to use. By default PHP will try to determine this itself, although in some cases this must be set manually.\nThe cURL docs further describe CURLOPT_SSLVERSION:\n- CURL_SSLVERSION_DEFAULT: The default action. This will attempt to figure out the remote SSL protocol version, i.e. either SSLv3 or TLSv1 (but not SSLv2, which became disabled by default with 7.18.1).\n- CURL_SSLVERSION_TLSv1: Force TLSv1.x\n- CURL_SSLVERSION_SSLv2: Force SSLv2\n- CURL_SSLVERSION_SSLv3: Force SSLv3\n- CURL_SSLVERSION_TLSv1_0: Force TLSv1.0 (Added in 7.34.0)\n- CURL_SSLVERSION_TLSv1_1: Force TLSv1.1 (Added in 7.34.0)\n- CURL_SSLVERSION_TLSv1_2: Force TLSv1.2 (Added in 7.34.0)\nThe automatic determination made when deciding which SSL version to use is probably pretty accurate, but it looks like you're in the minority that will need to manually specify this setting.\n. > Found an article suggesting that fix is to recompile CURL against NSS instead of openssl.\nI think you read that wrong. The article suggests that you compile cURL against OpenSSL instead of NSS.\nI'm not sure if that is a valid solution, but it seems likely that it would work. This is a cURL and platform specific issue that I've never seen. You might also try just yum updating nss.\n. Whoops! Didn't mean to close this issue.\nThanks for keeping us posted. Let us know if you get some concrete information that we can help others with.\n. I'm to go ahead and resolve this issue. Please let us know if you come up with anything that you think might be useful to document in our user guide.\n. I'm moving this issue to our team's internal backlog tool where we are tracking the issue.\n. Hi James. Thanks for letting us know about this cool functionality you've created. I think this (and perhaps an Amazon DynamoDB integration as well) would be really helpful for PHP developers. I do think it's too specific of a feature to be added to the SDK itself, but we would be happy to provide a link to the feature in our user guide when it is completed. Keep us posted!\n. Nice work. LGTM.\n. Thanks!\n. Looks good!\n. Thanks for the PR. It looks like the build failed when running HHVM (see: https://travis-ci.org/aws/aws-sdk-php/jobs/17181829). Any ideas?\n$ sh -c 'if [ $(php -r \"echo PHP_MINOR_VERSION;\") -le 4 ]; then echo \"extension = apc.so\" >> ~/.phpenv/versions/$(phpenv version-name)/etc/php.ini; fi;'\nsh: 1: cannot create /home/travis/.phpenv/versions/hhvm/etc/php.ini: Directory nonexistent\nThe command \"sh -c 'if [ $(php -r \"echo PHP_MINOR_VERSION;\") -le 4 ]; then echo \"extension = apc.so\" >> ~/.phpenv/versions/$(phpenv version-name)/etc/php.ini; fi;'\" failed and exited with 2 during before_script.\n. Can you provide some more context on this issue?\nFor example:\n1. When did you encounter this error?\n2. Did you add any plugins or event listeners that could have modified the request?\n3. Are you making requests from a __destruct method of a class?\n4. How often does this error occur?\n5. Can you reproduce the error?\n. I'm going to go ahead and close the issue. Feel free to reopen if you have more information that can provide a reproducible test case.\n. The doesObjectExist() method is poorly named; it should really have been called asTheCurrentUserCanThisObjectBeAccessed() :).\ndoesObjectExist() is a convenience method that invokes HEAD on an object. It will tell you whether or not you have access to HEAD the object based on your current credentials. It's tricky to try to determine under each and every circumstance whether or not an object actually exists because of ACLs and bucket policies.\nIf you need more control over the operation, then I suggest using the headObject operation directly (it will throw exceptions on error, including 404's). Changing this to throw an exception is a really big BC break that we probably shouldn't make to the doesObjectExist method. I'll make sure that we make a note that the documentation for this method is improved.\n. I noticed that you are using the phar file. The common solution to random characters like this when using the phar file is to disable detect_unicode. Enabling detect_unicode may cause errors when using phar files. See https://bugs.php.net/bug.php?id=42396. Can you check to see if this setting is enabled on your Windows machine?\n. There isn't any way to interact with Amazon S3 over JSON.\nCan you provide a TCP dump or wiretrace of one of the responses that contains the extra characters? I'd like to know if this is an issue with the server or the client.\n. Probably. What version of PHP, cURL, and libxml are you running?\n. Based on the nature of this issue, I would guess that this is an environment specific issue where one of the components on your system is corrupting the data. The fact that an extra, random, character is appended to the body makes me wonder if something is reading one too many characters (an off-by-one error), causing a buffer overflow, and ultimately corrupting the data. Due to the fact that you aren't receiving the same error on the Ubuntu system, then it does not appear to be evidence to support this being an issue with the service.\nDoes this error occur consistently every time you make the call? Have you tried retrying these requests to see if that corrects the issue? Can you provide verbose cURL output of one of these failed requests and compare the content-length reported by curl against the content length of the actual body you are receiving?\nSomething like the following will add verbose cURL output to your requests:\nphp\n$s3Client = Aws\\S3\\S3Client::factory(array('curl.options' => array(CURLOPT_VERBOSE => true)));\nI would recommend updating PHP, but it looks like you might be using WampServer 2.4 64-bit edition, and I don't see a newer release than this. Unfortunately, I'm not sure there is a way to easily update PHP or cURL.\n. This seems to be an environment specific issue with one of the dependencies on your system. I'm going to close for now, but feel free to reopen if you can provide evidence that this issue is internal to the SDK.\n. There was a brief window in which we had an issue with the SDK and a recent update to the underlying Guzzle library where some Amazon S3 signatures were being signed with this guzzle_blank value. Updating to the latest version of the SDK should fix this issue.\n. This can happen if you don't specify a region setting in your client when referencing DNS style buckets outside of the global Amazon S3 endpoint. Is the bucket you are referencing outside of the default, us-east-1 region? If so, try updating the region on your client to use the corresponding  region of the bucket (e.g., eu-west-1):\nphp\n$s3 = S3Client::factory(array(\n    'region' => 'eu-west-1',\n    /** other settings **/\n));\n. Amazon S3 has a restriction on how buckets created using a location constraint can be accessed. You must use DNS style access when accessing a bucket with a location constraint. If you use path style requests, then you receive a 301 redirect with no Location header, meaning there's no way to automatically follow the redirect.\nTo further complicate the issue, bucket names with dots in them always fail SSL certificate verification because Amazon S3's wildcard SSL certificate only works with buckets that have no periods. When the SDK send a SSL request to a bucket that contains periods, it automatically moves to address the bucket using path style access. I think the only way to access a DNS style bucket with periods over SSL is using path style access and a region specific endpoint (or you could disable SSL verification).\nWith all that said, I don't think we \"just make this work\" client-side. We should definitely improve the documentation to better describe how to work around this issue.\n. The fix to this issue is to use a specific region. There isn't anything we can do in the SDK, but we are tracking this in our internal backlog to document the error and any workarounds.\n. We provide automatic switching between path style and virtual style hosting of buckets when necessary. The problem is that this logic isn't used when building the POST request using the PostObject class. We'll need to add the same logic used elsewhere in the client to the PostObject class (specifically these two lines: https://github.com/aws/aws-sdk-php/blob/master/src/Aws/S3/Model/PostObject.php#L123-L124)\n. Interesting. What do you mean by \"a basic version?\" Can you provide a code sample of that and the corresponding debug output? I wonder if it is generating different filenames for some reason which causes the file to key comparison to be different.\n. Thanks for the additional information. We'll take a look to see if we can figure out why there is an inconsistency here based on how the sync is created.\n. I don't see any PHPUnit releases marked as 3.8: https://github.com/sebastianbergmann/phpunit/releases. I guess this is a development branch or something?\n\n\"ReflectionException: Class Mock_SplFileInfo_xxxxxxxx is an internal class that cannot be instantiated without invoking its constructor\"\n\nWeird. This must be a change in PHPUnit that wasn't an issue with 3.7.\n. Thanks for the PR. We're interested in being able to use the AWS CLI configuration format, but we're still trying to determine to what extent we will support the format.\nWe're less interested in being able to use the old config format or the Boto config files due to the additional service area, complexity, and implied commitment to supporting these formats now and in the future (i.e., if they change the format). Would you mind removing everything except for the AWS CLI configuration parsing? After that's done, we can more thoroughly review your code.\n. Just want to give you a heads up that we'll likely support the same credentials file as the CLI, but the credentials file that is shared by the CLI and the SDK may change. When integrating this, we'd likely need to make other significant changes (like possibly supporting something equivalent to the CLI's \"profiles\"). Because this is a larger scoped issued that needs to be resolved first internally, I'm going to close this PR.\n. Based on the error message from Amazon EC2, I think you need to remove the outer parameter: \"SubnetId\" => \"subnet-xxxxxx\" but keep the one that you are using inside of the \"NetworkInterfaces\" setting.\n. I'll go ahead and resolve this issue. Feel free to reopen if you are still experiencing issues.\n. Thanks for the PR. I think this is headed in the right direction. This change would probably need to make sure that the bucket exists before returning true. With that said, I've actually been thinking that the SDK should mimic the AWS Management Console when it comes to directories and Amazon S3.\nIt might make sense for the stream wrapper to create an empty object that uses a key suffixed with \"/\" when creating a directory. Then when calling is_dir() or similar directory checking functions, the stream wrapper would call ListObjects with an object key containing a trailing \"/\" and a maxKeys of 1. This would tell us if the directory was created using mkdir() or if the user uploaded files to a nested path under the given key.\n. We're going to look into how we can implement this in a way that is consistent with how the AWS Management Console creates empty directories using a 0-length file that has a key ending in \"/\". I'm moving to our backlog to track.\n. Do you know if you are being redirected or throttled on any of these uploads?\n. > do I need to do something special to make cURL/Guzzle report that?\nYou can set the client.backoff.logger setting of your client to \"debug\" so that retries will trigger a E_USER_NOTICE error. The reason I'm curious is because retrying failed requests after several timeout issues can sometimes cause signature does not match errors.\nOne interesting this to not is that the upload() method will automatically switch to multi-part uploads when the file size is greater than 5MB. There was briefly an issue with the 2.5.2 release that had a broken signature calculation for some Amazon S3 operations. I wonder if upgrading to the dev-master version of the SDK would fix this issue.\n. Ok, thanks for giving that a shot.\nYou'll need to set that option in the factory method of the client. Setting it after the client is created will have no effect.\n. client.backoff.logger is the correct key. For example:\nphp\n$client = Aws\\S3\\S3Client::factory([\n  'client.backoff.logger' => 'debug'\n]);\n. Thanks for the update. I'm curious: why are you using \"body_as_string\"? Does omitting that parameter cause it to work correctly?\nWould you mind providing verbose cURL output for one of the failed transfers? This output could tell us interesting information like if you are being redirected, if you are sending the \"Expect: 100-Continue\" header, etc..\nphp\n$client = S3Client::factory(array(\n    'key' => '***',\n    'secret' => '***',\n    'curl.options' => [CURLOPT_VERBOSE => true]\n));\n. Thanks for the additional information. Are you both using the dev-master version of Guzzle? I just pushed a fix to the SDK that, if you are using Guzzle dev-master, should resolve this issue.\n. Great. Thanks for letting us know that the issue is resolved. I'll go ahead and resolve the issue. Feel free to reopen if needed.\n. Does the key actually have an unencoded space? I noticed a space between MASTER and (1). I haven't been able to reproduce an error using \"+\" symbols and \"()\" symbols, but I'm not sure what to do with the unencoded space.\n. Digging into this further, I see that you'll need to URL encode the CopySource parameter of the copyObject operation so that it references the same key on Amazon S3 (using either RFC 3986 or RFC 1738).\nFor example, here I've used PHP's urlencode function:\nphp\n$client->copyObject(array(\n    'Bucket'                => $targetBucket,\n    'Key'                   => $targetKeyname,\n    'CopySource'            => urlencode(\"{$sourceBucket}/{$sourceKeyname}\"),\n    'ServerSideEncryption'  => ServerSideEncryption::AES256,\n));\nHere's more information from the API docs: http://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectCOPY.html\nFeel free to reopen if you can't get this working.\n. I think there's a misunderstanding somewhere with regards to what a CurlException actually means. A CurlException is caused by a networking error, and almost always means that you don't receive an HTTP response. If you don't receive a response, then there's no possible way to retrieve a requestId.\nWhat is the exception?\n. If this is a service issue, then they'd need a request-id to dig through logs and try to debug. It appears that this a networking issue somewhere between you and DynamoDB. You could use things like a TCP dump, traceroute, and other networking diagnostic tools to debug this issue.\n. As a follow up, the SDK will retry this error using truncated exponential backoff. This means that you were unable to reconnect to Amazon DynamoDB after 11 retries (their recommended max number of retries). Based on this information, there's some sort of networking issue going on with your host/instance and this does not seem to be an issue with the SDK itself.\n. I see that you are concatenating some text the the $signedUrl variable. This is going to change the actual signature and cause the error because signed URLs have to be in a very specific format. Does removing the concatenation fix the issue?\n. I debugged the issue a bit and I see that a ContentMD5 is being added for the body that is specified in the command. This is not the desired behavior for your use case of creating a pre-signed PUT URL, so you'll need to disable the automatic addition of the Content-MD5 header by setting the ContentMD5 option to false.\nYou also don't need to include '&Content-Type=image%2Fjpeg' in the URL.\nphp\n$command = $client->getCommand('PutObject', array(\n    'Bucket'      => 'myBucket',\n    'Key'         => 'testing/signed_'.time(),\n    'ContentType' => 'image/jpeg',\n    'Body'        => '',\n    'ContentMD5'  => false\n));\nAdding this parameter tells the SDK to not generate a Content-MD5 header for the command. Does this resolve the issue?\n. Please note: You no longer need to set ContentMD5 to false.\n@fousheezy Can you share the command you're using to send the request to the pre-signed URL? Is it through PHP, cURL, something else?\n@LukasRos Is this still an issue for you?\n. Do you know what the HTTP request looks like when it's sent over the wire from POSTman? Is it adding the appropriate content-type?\n. @fousheezy Are you using the latest version of the SDK? What does the presigned URL look like (without the actual signature value please)? I ask because we have various integration tests that test pre-signed URLs, and they're working. There may be something funny in your URL that we didn't think about when implementing the pre-signed URL feature.\n. @BCJFinlayson Did you try passing a Content-Length header to your Guzzle PUT request?\n. Doh! I meant to write Content-Type. Guzzle will automatically insert a Content-Length. Sorry, still early for me :)\n. Ok, glad to hear it's working now.\n. Please note that when creating pre-signed URLs, the headers that are present when a request is signed must also be present when using the pre-signed URL. This means that creating things like pre-signed PUT requests with a content-type require that the actual PUT you send use the same content-type. This is important to keep in mind when using tools like curl, Postman, etc.\n. :+1: \n. The Aws class is namespaced under Aws\\Common\\Aws. You'll need to either \"use\" the namespace or reference it with the fully qualified class name.\n\nOn Feb 23, 2014, at 6:13 PM, Justin Keller notifications@github.com wrote:\nI downloaded the .zip of the Amazon SDK and extracted it into lib/aws and then doing:\nrequire_once(dirname(DIR) . \"/lib/aws/aws-autoloader.php\");\n$aws = Aws::factory(array(\n    'key'    => getenv('ec2_key'),\n    'secret' => getenv('ec2_secret')\n));\nHowever, I am getting Class 'Aws' not found. It is requiring aws-autoloader.php, but does not seem to be requiring the rest.\nAny ideas?\n\u2014\nReply to this email directly or view it on GitHub.\n. Please feel free to reopen if you still run into issues after using the namespaced class name Aws\\Common\\Aws::factory( /* ... */ )\n. Maybe there's something else going on, but this appears to be a permission issue where you're able to read files but not delete them. Are you able to delete files from this directory (without using any of the features of the SDK)?\n. Yeah, if the file handle was still open, then it could prevent the file from being deleted. If that's the case, then the lifecycle of the underlying handle needs to be managed explicitly rather than allowing the abstraction to create the fopen resource for you. In that case, you would need to pass an EntityBody object to the UploadBuilder::setSource() method. When the transfer is complete, you could then close the handle. As of now, the handle is only closed in the __destruct method of the EntityBody object object that is automatically created.\n\n``` php\n$body = Guzzle\\Http\\EntityBody::factory(fopen('/path/to/file', 'r'));\n$transfer = UploadBuilder::newInstance()\n  ->setClient($s3)\n  ->setSource($body)\n  ->setBucket(\"my_bucket_name\")\n  ->setKey(\"my_key\")\n  ->setMinPartSize(5 * Size::MB)\n  ->setConcurrency(4)\n  ->build();\ntry {\n  $transfer->upload();\n} catch (MultipartUploadException $e) {\n  error_log(\"failed to upload - \".$e->getMessage());\n  $transfer->abort();\n}\n$body->close();\nunlink('/path/to/file');\n```\n. As you've discovered, the same timestamp is used for each request sent in parallel because they were all created at the same time then sent over the network. The type of 400 level response you received is typically returned when a network is saturated or there is a poor internet connection. When this specific 400 error is encountered, the request is retried with exponential backoff. The window between when a request is signed to when the signature expires 15 minutes (I'm fairly sure), so this means requests are either very large or being retried several times.\nYou'll need to reduce the number of requests sent in parallel to work around the bandwidth constraints.\n. I don't have enough expertise on the subject to give concrete guidelines, but I do know that the bandwidth available to instances varies by instance type. I would suggest asking this question on the Amazon EC2 forum to get a more accurate response.\n. After speaking about this, we've determined that this feature would be outside the scope of the SDK, and we will bring it up with the Amazon DynamoDB team as a feature request. In the meantime, the Python project you linked to seems like it could fill this need for you.\n. How are you providing credentials to the SDK?\nCan you provide a stack trace?\nAre you seeing this error from the CLI or are you trying to access a web page that you modified to include the autoloader? If it is the latter, then please provide the relevant section of your error log.\n. I'll go ahead and close this issue. Feel free to reopen if you can provide more details.\n. Based on the API documentation of the service, I don't think there's a way to update an application name because it's used as the identifier for the operation: http://docs.aws.amazon.com/elasticbeanstalk/latest/api/API_UpdateApplication.html. It doesn't look like an environment name can be changed either. You might want to ask on the AWS Elastic Beanstalk forum for a definitive answer or to file a feature request: https://forums.aws.amazon.com/forum.jspa?forumID=86\n. So you're creating a pre-signed URL for a PUT request and trying to include an ACL? Can you share with me the code you're using to send the actual PUT request?\n. This issue it now resolved. I've also updated the SDK so that setting ContentMD5 to false is no longer required.\n. Are you getting any kind of error when you try to stream the private content?\n. Iterators provided by the SDK will retrieve all results of an operation until no more results can be retrieved. Because you specified a limit parameter in the operation, the iterator that wraps the operation will retrieve every result from the query one page after the other in chunks of 10 per page. Instead of providing the limit argument to the operation, you can provide the \"limit\" option to the the iterator and allow the iterator to control how many items to retrieve per page.\nHere's an in-depth guide on how iterators work and how you can provide a \"limit\" option to the iterator (rather than the operation): http://docs.aws.amazon.com/aws-sdk-php/guide/latest/feature-iterators.html#basic-configuration\n. Thanks! I put some work into addressing some related issues last week. I'll get this merged in (possibly with minor tweaks) this week.\n. I made some tweaks and have merged this into master. Thanks again.\n. What do you mean?\n\nOn Mar 18, 2014, at 9:50 PM, Martin Mikl\u00f3s notifications@github.com wrote:\nThanks\nBut @ has no effect now.\n\u2014\nReply to this email directly or view it on GitHub.\n. I was unable to reproduce this behavior. Are you still seeing this problem on newer versions of the SDK?\n. Awesome. Thanks for letting us know.\n. Fixed in #265\n. Thanks for sending this (related to https://github.com/aws/aws-sdk-php/issues/264).\n\nrmdir() in PHP requires that the directory is empty before it can be deleted. This is also the case for deleting Amazon S3 buckets, so that is handled by the try/catch block without the need for custom code in the stream wrapper.\nHowever, when a key is present, then we're attempting to delete a pseudo-directory. There is no check performed to see if the pseudo-directory is empty (meaning there are files that have that directory as their key prefix) before attempting to deleting the pseudo-directory.  Perhaps we should perform a list objects operation with a key prefix and a max-keys of 1 see if the pseudo-directory is empty? If there are keys beneath the pseudo-key, then the directory is not empty and it will fail. If there are not keys, then attempt to delete the pseudo-key.\n. I just pushed a fix for this that should address this issue and other related issues. This fix allows the Amazon S3 stream wrapper to better emulate the behavior of other stream wrappers with regards to when it raises errors or returns false. I'm also removing the \"throw_exceptions\" option from the stream wrapper as that completely breaks compatibility with other stream wrappers.\n. Looks good\n. I can't reproduce this error. What version of the SDK and Guzzle are you using?\nphp\n<?php\nrequire 'vendor/autoload.php';\n$s3 = Aws\\S3\\S3Client::factory();\n$s3->registerStreamWrapper();\nvar_export(mkdir('s3://t1234/isnotreal'));\n// true\n. There have been various fixes in the stream wrapper over the last two releases. Can you try upgrading and then let us know if you're still having issues?\n. Guzzle can send requests to a path that contains nothing but a \".\" character.\n``` php\n<?php\nrequire 'vendor/autoload.php';\n$c = new Guzzle\\Http\\Client();\n$c->getConfig()->set('curl.options', [CURLOPT_VERBOSE => true]);\n$c->get('http://www.google.com/.')->send();\n```\n```\n About to connect() to www.google.com port 80 (#0)\n   Trying 74.125.28.106... * Connected to www.google.com (74.125.28.106) port 80 (#0)\n\nGET /. HTTP/1.1\nHost: www.google.com\nUser-Agent: Guzzle/3.9.1 curl/7.21.4 PHP/5.5.8\n\n< HTTP/1.1 302 Found\n< Cache-Control: private\n< Content-Type: text/html; charset=UTF-8\n< Location: http://www.google.com/\n< Date: Wed, 07 May 2014 18:32:16 GMT\n< Server: GFE/2.0\n< Content-Length: 219\n< Via: 1.1 sea3-proxy-2.amazon.com:80 (Cisco-IronPort-WSA/7.5.2-351)\n< Connection: keep-alive\n< \n Connection #0 to host www.google.com left intact\n About to connect() to www.google.com port 80 (#0)\n*   Trying 74.125.28.106... * Connected to www.google.com (74.125.28.106) port 80 (#0)\n\nGET / HTTP/1.1\nHost: www.google.com\nUser-Agent: Guzzle/3.9.1 curl/7.21.4 PHP/5.5.8\n\n< HTTP/1.1 200 OK\n< Date: Wed, 07 May 2014 18:32:16 GMT\n< Expires: -1\n< Cache-Control: private, max-age=0\n< Content-Type: text/html; charset=ISO-8859-1\n< Set-Cookie: PREF=ID=98112ee9d6576c9e:FF=0:TM=1399487536:LM=1399487536:S=4Ct-lifW2-QRrYlh; expires=Fri, 06-May-2016 18:32:16 GMT; path=/; domain=.google.com\n< Set-Cookie: NID=67=wLanyCzXD0xDDkWhCa0QC_3wkCHYi8SB1LB9gmYidy42yOU3h5CFqNwkf8oyhqTFKcGCiUZWZaX_lVDBSYnkyXBGvA-Mxe6TVzEi5x8nWRwivuTuI-BJGTlUsMpHVFbB; expires=Thu, 06-Nov-2014 18:32:16 GMT; path=/; domain=.google.com; HttpOnly\n< P3P: CP=\"This is not a P3P policy! See http://www.google.com/support/accounts/bin/answer.py?hl=en&answer=151657 for more info.\"\n< Server: gws\n< X-XSS-Protection: 1; mode=block\n< X-Frame-Options: SAMEORIGIN\n< Alternate-Protocol: 80:quic\n< Transfer-Encoding: chunked\n< Via: 1.1 sea3-proxy-2.amazon.com:80 (Cisco-IronPort-WSA/7.5.2-351)\n< Connection: keep-alive\n< \n* Connection #0 to host www.google.com left intact\n``\n. This is not something that is going to be supported by the SDK. Using \".\" and \"..\" in URLs has special meaning due to what is called dot segments. For example, attempting to upload a file tohttp://test.com/.is equivalent tohttp://test.com/` (see RFC 3986 section 5.2.4).\nEven if the SDK attempted to work around this, we still wouldn't be able to send a request to the URL because cURL automatically resolves \"/.\" paths to \"/\". See:\n```\n$ curl -v 'http://t1234.s3.amazonaws.com/.'\n Rebuilt URL to: http://t1234.s3.amazonaws.com/\n Hostname was NOT found in DNS cache\n   Trying 10.230.128.2...\n Connected to t1234.s3.amazonaws.com (10.230.128.2) port 80 (#0)\n\nGET / HTTP/1.1\nUser-Agent: curl/7.37.0\nHost: t1234.s3.amazonaws.com\nAccept: /\n```\n\nYou'll need to filter out the uploading of \".\" and \"..\" files when putting object into Amazon S3.\n. Closing. Feel free to reopen if you find that this is an issue with the SDK.\n. > I am trying to catch exceptions when making requests to the S3 service for bucket contents.\nThe Amazon S3 stream wrapper does not actually throw exceptions. You'll need to use error handling to work with the stream wrapper.\n\nThe \"/key\" directory does not exist.\n\nThis error message doesn't come from the SDK, but perhaps it comes from the Finder component. I wonder if the Finder component is dealing with errors in the stream wrapper and then causing a different error message or exception.\n. Can you try testing against the latest version of the SDK? There were a number of bug fixes for the stream wrapper in the latest release.\n. Any update here or should this issue be closed?\n. Thanks!\n. This is relevant: http://fabien.potencier.org/article/72/the-rise-of-composer-and-the-fall-of-pear. Pirum (the PEAR channel management tool we use) was deprecated and the repo is no longer taking new issues. If we did want to continue to support PEAR, we'd be on our own if we run into issues while working with Pirum.\nConsidering that other large PEAR vendors are retiring/retired their PEAR channels (PHPUnit, Symfony, Doctrine, Guzzle, etc), it doesn't make much sense for us to continue to provide a PEAR channel for the SDK.\n. Thanks for providing the test file. I'm able to reproduce this issue, and I'm researching why it's happening.\n. I found a bug in Guzzle that was causing this strange issue. For some reason, when reading the first chunk of data from the file stream, it always returns '0'. Because '0' is falsey, a checksum was being calculated based only on the first character of the file. Here's the Guzzle fix for reference: https://github.com/guzzle/guzzle3/commit/42ea3b0447817eaf203ceb62eb10c96eb9fab2a2\nYou can get the fix for this by using the dev-master version of Guzzle, or by pulling down the phar. This fix will be present in the next release.\n. I see you are using Amazon S3 and uploading a file to the eu-west-1 region. Is that the correct region you need for accessing this bucket? If it is not the actual location constraint region of the bucket, then you might be redirected. If you are being redirected then it could explain the error. This is because the SDK streams data to cURL so that the entire payload does not need to be loaded into memory.\n. Setting the body_as_string cURL option will correct this issue. @pulkit-clowdy can you share a code sample that shows this not working?\n. The answer to use \"body_as_string\" is correct.\nI've been thinking though that perhaps there could be a retry strategy for Amazon S3 that retries failed requests that have a payload less than [some amount like 2 MB?] using the body_as_string setting. This would work for a lot of case, but it would have a built-in guard to ensure that large payloads are not loaded into memory.\nDoes that seem like a good solution?\n. The solution I proposed will be implemented in the next major version of the SDK. In the mean time, just use \"body_as_string\" to work around the issue.\n. Hm. I wonder if we can just use getenv() by itself?\n\nOn May 3, 2014, at 2:20 PM, Jeremy Lindblom notifications@github.com wrote:\n@mtdowling Was working on something and ran into an issue where it was not finding my HOME dir with $_SERVER, but it was finding it with getenv(). I think we need to always do isset($_SERVER[...]) ? $_SERVER[...] : getenv(...) in order to have max compatibility.\nYou can merge this Pull Request by running\ngit pull https://github.com/aws/aws-sdk-php credential-profiles-check\nOr view, comment on, or merge it at:\nhttps://github.com/aws/aws-sdk-php/pull/285\nCommit Summary\nFixing some environment variable reading logic.\nFile Changes\nM src/Aws/Common/Credentials/Credentials.php (44)\nM tests/Aws/Tests/Common/Credentials/CredentialsTest.php (3)\nPatch Links:\nhttps://github.com/aws/aws-sdk-php/pull/285.patch\nhttps://github.com/aws/aws-sdk-php/pull/285.diff\n\u2014\nReply to this email directly or view it on GitHub.\n. This change seems fine and is self-contained enough that we can easily change it later if we get more information.\n. I don't think we can make this change as it could break existing PEAR consumers who rely on the current directory structure. That said, we are also very likely going to stop updating our PEAR channel at some point this year (see https://github.com/aws/aws-sdk-php/issues/280). Is it possible for Fedora to bundle the SDK without relying on PEAR?\n. We really appreciate your work.\n\nWe are going to deprecate our PEAR channel at some, point, so it's just a matter of time. I think you'll find that this is happening in various other PHP projects, including PHPUnit and Symfony (http://fabien.potencier.org/article/72/the-rise-of-composer-and-the-fall-of-pear).\n. The SDK creates presigned URLs based on the provided input. We, however, do not send a request to the URL to ensure that the URL is correct (e.g., the region is correct, the bucket exists, key exists, you have permission to view the resource, etc.). This is something that the SDK is not going to support by default, but it is something that you could do in your application after creating the URL if desired.\n. This is a regression due to a change that needs to be deployed in Guzzle. I am tagging a new release of Guzzle 3.9.1 right now to fix this issue, and I'll redeploy the AWS phar and zip when it is complete.\n. Yes, that change and this change are intertwined. This change to ReadLimitEntityBody should resolve the issue for you (I'm pretty confident it will because multipart uploads use the ReadLimitEntityBody).\n. Guzzle has now been tagged at 3.9.1, so Composer users should be fine. I've also uploaded new versions of the phar and zip to https://github.com/aws/aws-sdk-php/releases/tag/2.6.2.\n. A region is not required for Amazon S3 if you are connecting to the classic (us-east-1) region. If a bucket is created with a LocationConstraint and you connect to a region that is not that same location, then you might get a 307 redirect which would cause a failure. Here's more information on redirects from the Amazon S3 developer guide: http://docs.aws.amazon.com/AmazonS3/latest/dev/Redirects.html.\n. Those badges appear to be broken. Any reason why we need to make this change?\n. They are working now. I see that you're the main person behind the project that produces these images, so I'll take your word that we should upgrade. Thanks for the PR.\n. The SDK always sends a Content-Length header when sending requests to Amazon S3 (as that's required by S3). You can view what's being sent over the wire using verbose cURL output: http://docs.aws.amazon.com/aws-sdk-php/guide/latest/faq.html#how-can-i-see-what-data-is-sent-over-the-wire. Maybe that will show you something interesting (perhaps a redirect that is causing issues).\n. I'm unable to reproduce this and it sounds like it works for you now based on your comment on May 26 . It's possible that you updated the SDK when testing on the new host, which is possibly the reason it works now. I'm going to resolve this issue, but feel free to reopen if you can provide verbose curl output for a failing case.\n. Does this error occur all the time or intermittently? Which region are you connecting to? Are you able to connect to other hosts without issue? Is DNS working correctly?\n. Can you provide a code snippet that reproduces this issue? Can you also provide the debug output from curl? See: http://docs.aws.amazon.com/aws-sdk-php/guide/latest/faq.html#how-can-i-see-what-data-is-sent-over-the-wire\n. It sounds like this is a DNS issue between your host and the service. I'll go ahead and resolve this issue, but feel free to reopen if you can provide more information.\n. cURL error code 56 (connection reset by peer) is retried automatically by the SDK 3 times before failing. This error can occur for various reasons, and my usual suspicion is that there's too many requests being handled by a single instance. Try lowering your concurrency level to see if you can reduce the number of these errors, or using a larger instance (I'm not sure what the size of the instance you're using is).\n\nThe $this->s3_client is super boring... only custom option is 'curl.options' => array('CURLOPT_HTTP_VERSION'=>'CURL_HTTP_VERSION_1_0') to address errors I was having with chunked transfer (see #173). \n\nStrange. The SDK never sends chunked transfer-encoding requests as that is not supported with Amazon S3.\n. Are you executing multiple multi-part uploads from the same instance? If so, then the concurrency of other active transactions could interfere with the creation of a new transaction in a different process.\n\nIs the exponential backoff strategy definitely in place for the service request that creates the multipart upload ID?\n\nYes, exponential backoff should be utilized on every request sent through the SDK.\nCould you provide the verbose cURL output of a failed transfer (similar to what's described in #173)? It might provide some additional details.\n. Thanks for providing the verbose cURL output.\nI noticed that you're sending HTTP/1.0 requests but S3 is responding with HTTP/1.1 responses. I wonder if this in combination with your version of cURL and NSS is causing issues. Can you try sending requests over HTTP/1.1 (the default) instead to see if that fixes the issue?\nI think it's weird that Amazon Linux using NSS rather than OpenSSL; I've seen other users have issues with NSS and moved over the OpenSSL.\n. Apologies, I've been traveling all week.\nI noticed that you are using a fairly old version of Guzzle, and you're a few versions behind in a SDK versions. There are some fixes in guzzle between 3.7 and 3.9 that could possibly be relevant.\nCould you try updating to the latest version if the SDK and Guzzle (3.9) to see if it fixes the issue? If it does not, then I think the output of a tcpdump would be necessary as this is troubleshooting a network issue that might not be related to the client.\n\nOn Jun 4, 2014, at 9:08 AM, Sasha notifications@github.com wrote:\n@mtdowling, any chance you had a chance to look at this?\n\u2014\nReply to this email directly or view it on GitHub.\n. This error means that the server received a request using an unsupported protocol version. It's strange that you're receiving this error 50% of the time. Is there a proxy between you and Amazon EC2 that could be modifying your requests? Can you provide a snippet of code that causes this error?\n\nCan you provide a link to where Boto had this same issue?\n. I don't think that Boto issue is related to this issue. Can you provide debug output of the call?\nhttp://docs.aws.amazon.com/aws-sdk-php/guide/latest/faq.html#how-can-i-see-what-data-is-sent-over-the-wire\n. Thanks. Is there any way you could provide the debug output? It's probable that this is not an issue the SDK team can fix, and this issue is probably better suited for the CloudFormation forums where their service team will have more insight into the issue.\nhttps://forums.aws.amazon.com/forum.jspa?forumID=92\n(edited to use the right service name).\n. Because it's a completely different service (Autoscaling vs CloudFormation) I don't think they're related. Also, you mentioned your issue was region specific, which makes me think this is a service issue and not an SDK issue.\nHere's documentation on adding verbose curl output: http://docs.aws.amazon.com/aws-sdk-php/guide/latest/faq.html#how-can-i-see-what-data-is-sent-over-the-wire. Let me know if you need more help on this.\n. Ah, I just noticed you shared a GitHub issue as well as a link to a forum post regarding Autoscaling (maybe in an edit to the issue?).\nThe GitHub issue you linked to is the same issue occurring in a different SDK. After looking at the issue, I see that the service team has already looked into this and recommends uploading your policy to S3 and using a URL rather than a policy body: https://github.com/boto/boto/issues/1673#issuecomment-24615273\nSo instead of passing a \"TemplayBody\", provide a \"TemplateURL\" parameter with a link to your publicly accessible template:\n\nLocation of file containing the template body. The URL must point to a template (max size: 307,200 bytes) located in an S3 bucket in the same region as the stack. For more information, go to the Template Anatomy in the AWS CloudFormation User Guide.\n\nAs this isn't an SDK issue but rather an issue with the service, I suggest posting to their forum to let the service team know that you are affected by this issue.\n. From what I've seen, this is not a client side issue (meaning this cannot be fixed in this repository). You'll need to open a thread on the AWS CloudFormation forum to report the issue to the proper team. If there is a related forum post that says the issue is resolved, then you could reply to it with a wiretrace showing that it clearly is still happening. The only thing the SDK team (the owners of this repo) could help with is gathering the data needed to report the issue.\nhttps://forums.aws.amazon.com/forum.jspa?forumID=92\n. > instantiating the s3 client produces the error that the cURL constants are not defined.\nWhere are you seeing cURL extensions being utilized when using the stream wrapper HTTP handler?\n. I see where it was using cURL constants. I've updated v3 to use a different retry strategy that abstracts away cURL. This should remove all usage of cURL constants: https://github.com/aws/aws-sdk-php/commit/4d03c193872e4f00e6d4523848623c9faf7dc820.\n. I don't know that this is actually related to clock skew. Are you sure you're using the right keys?\n. Let us know if you are still running into trouble after ensuring you are using the correct keys, and if you are still having trouble, showing us exactly what's being sent over the wire would help: http://docs.aws.amazon.com/aws-sdk-php/guide/latest/faq.html#how-can-i-see-what-data-is-sent-over-the-wire.\n. :sheep: :ship: :goat: \n. @maknz Glad to hear that. One of the cool things about Guzzle 4 is that it can be used the same project as Guzzle 3 because it utilizes a different namespace.\n. @stof We do know that this would be a BC break and that it would require a new major version of the SDK. We've talked about decoupling from Guzzle, but it wouldn't be worth it considering we would have to reinvent things that Guzzle was designed to provide: an event system, HTTP messages, HTTP clients, web service clients, event subscribers (and much more). We are planning on moving a few things a bit further from Guzzle though: the return value of operations would be an AWS object, there's an AwsClientInterface, we would use AWS service descriptions directly rather than Guzzle service descriptions, we'd use our own configuration logic rather than Guzzle's, etc.\n\nThis would mean that most users of the SDK would not need to alter their code the next time a Guzzle major version happens.\n\nI'm the author of Guzzle, and I can tell you that I have no plans for a new major version in Guzzle anytime in the next few years (maybe even ever). If there ever was a major new version, it would not be a rewrite but rather a few small breaking changes that were needed to solve a bug or security issue. If that did happen, it would likely have no impact on the SDK.\n. You can access the XML data of the exception response using the following code:\nphp\n$xml = $e->getResponse()->xml();\n// Access a property:\necho $xml->ArgumentValue;\n. It looks like cURL is keeping connections open longer than it should (each connection requires a file descriptor). What version of PHP and cURL are you using? I wonder if there could be a version specific bug that is causing this.\n. Another possible culprit could be circular references. Does adding a call to gc_collect_cycles() after this line correct the issue?\n. I just ran some tests on OS X 10.9.4, PHP 5.5.13, curl 7.37.0, Guzzle 3.9.1, and the master branch of the SDK. I was able to successfully upload 14,632 files with a concurrency level of 5 without noticing any memory leaks or increasing number of open IPv4 file descriptors. I then tried with 20, and was still able to upload the 14,632 files without any memory or file descriptor leaks.\nAre you using the same version of the SDK and Guzzle? Is there any way you could try updating curl to see if that is the issue? I'm not sure how many could apply to this issue, but there are 12 memory leaks fixed in cURL between 7.32 and 7.37.\n. It seems like this is no longer an issue for you. I'll do more testing and determine if we need to update the documentation. Please feel free to reopen if you continue to have an issue.\n. That's strange because this error (Error completing request) occurs if no response was received by the client yet cURL has finished transferring the request. Perhaps this is due to an empty reply from the server.\nDoes this error happen all the time or intermittently? If it does occur again, can you provide some verbose cURL output of the error?\n. Can you provide verbose cURL output and the debug output of the the sync uploader? That will help to see if there are any issues with the connection itself.\n. I just noticed there was a bug related to trying to sync between two S3 buckets. This has been corrected and pushed to master. You can now successfully transfer between two buckets using the following code (just add in the corresponding variables in the method call):\nphp\nuse Aws\\S3\\S3Client;\n$s3 = S3Client::factory(['region'=> 'us-east-1']);\n$s3->registerStreamWrapper();\n$s3->uploadDirectory($fromBucket, $toBucket, $keyPrefix, ['debug' => true]);\n. I would love to add HHVM to Travis. We just need to first get the test suite able to run on Travis. This would require only conditionally installing APC and maybe a few other minor tweaks.\n. Thanks!\n. Thanks for reporting. I've pushed a fix that is available in the master branch.\n. This is the default behavior of the PHP SDK for syncing files to Amazon S3. I'll go ahead and close. If you are having an issue with the sync behavior, please reopen the issue with more information.\n. Does this happen when you use rmdir with no trailing slash in the bucket name (e.g., \"foo\" vs \"foo/\")?\n\nOn Jul 24, 2014, at 9:47 AM, Craig Bartholomew notifications@github.com wrote:\nI'm having problems deleting a directory, even if it is empty. In commit 2a4355c (line 416 of src/Aws/S3/StreamWrapper.php) The 'Prefix' value of the array passed to $client->listObjects was changed. Previously it was:\n'Prefix' => $path,\nShouldn't the refactored line now be:\n'Prefix' => rtrim($path, '/') . '/',\nUsing the latest version (2.6.12) it's impossible to delete a directory using rmdir() even if it empty. This is because in the $result there is always a 'Contents' key, even if the directory is empty. Here's an example of the result when I try and delete an empty directory:\nGuzzle\\Service\\Resource\\Model Object\n(\n    [structure:protected] => \n    [data:protected] => Array\n        (\n            [Name] => nlcontent-static\n            [Prefix] => bundles/framework/images/\n            [Marker] => \n            [MaxKeys] => 1\n            [IsTruncated] => \n            [Contents] => Array\n                (\n                    [0] => Array\n                        (\n                            [Key] => bundles/framework/images/\n                            [LastModified] => 2014-07-24T11:19:13.000Z\n                            [ETag] => \"d31d8cd98f00b204e9800998ecf8427e\"\n                            [Size] => 0\n                            [Owner] => Array\n                                (\n                                    [ID] => 8d8fac9e83f71a47530b5631b20eee2b2c62dcfad9b424e840f97b1794c2e411\n                                    [DisplayName] => me\n                                )\n                            [StorageClass] => STANDARD\n                        )\n                )\n[RequestId] => 0CB75A893FDG3569\n    )\n)\nIf I change your the offending line to my suggestion, here is the result:\nGuzzle\\Service\\Resource\\Model Object\n(\n    [structure:protected] => \n    [data:protected] => Array\n        (\n            [Name] => nameloop-static\n            [Prefix] => s3://nlcontent-static/bundles/framework/images/\n            [Marker] => \n            [MaxKeys] => 1\n            [IsTruncated] => \n            [RequestId] => 3E23FA1A13106F03\n        )\n)\nSince there is no 'Content' in the result, the directory is deleted correctly.\n\u2014\nReply to this email directly or view it on GitHub.\n. Thanks for the bug report. I've pushed a fix that will be available in the next release of the SDK.\n. Based on the fact that the timeout setting is configurable, curl errors are retried by default, and you can provide a custom retry plugin to handle retries, I think this issue can be resolved. Please feel free to reopen if you need more information.\n. Thanks for the PR. Sorry for the delay. This has been merged after rebasing.\n. The \"saveAs\" parameter validates and opens a file for writing before the download begins. When the file is opened for writing, it is truncated. This behavior is performed at the HTTP layer of the SDK and has no knowledge of the semantics of the operation being performed.\n\nWe are planning on addressing this behavior in the next major version of the SDK. In the meantime, you can do a HEAD request (headObject) first to see if the object has been updated.\n. We are trying to keep the core of the SDK light by moving things like Hive and Pig helpers into separate packages. Looking at the previous helpers classes, I can see that they were relatively simple, so they could probably be easily be ported to a new package. As Jeremy mentioned, you could likey use the old classes as-is if you'd like. If you or someone else ends up porting these, please let us know; we'd be happy to tweet/blog/promote it.\n. That's interesting. I don't know why it would fail intermittently. The only guess that comes to mind would be not having enough memory available to create the stream context.\n. I'm going to go ahead and close this issue as it seems like it is either environment specific or a PHP bug that we cannot fix. Please feel free to reopen it if you can get more information.\n. One idea to speed up creating pre-signed URLs would be to turn off validation: http://docs.aws.amazon.com/aws-sdk-php/guide/latest/performance.html#turn-off-parameter-validation.\nThe URL style used when creating pre-signed URLs is dependent on the location of your bucket and the name of the bucket. Older buckets in S3 that are not DNS compatible must use the \"path style\" URL, whereas newer buckets should use the virtual hosted bucket style. When calling createPresignedUrl and providing a request object, the function uses the request object provided, verbatim, and returns a signed URL. This is a lower-level operation on the SDK that requires that the caller format the URL and request correctly.\n. You should be able to create a client that has a custom base_url and a region set to the appropriate region in which the CNAME bucket points to (e.g., if you created a bucket in us-west-1, then your region should be us-west-1).\n\nI'm having trouble getting that to work either.\n\nHow is it failing?\n. What did you pass as the base_url? When provided a full URL (e.g., https://foo.mysite.com), you should be able to create presigned URLs.\n\nTrying to set the key to test, and the base_url to http://s3.yourdomain.com without a bucket I get:\n\nCan you show your code? What operation are you sending?\n. Can you provide a code example of how you are using the client after it's been created? Based on the error message, it looks like you're trying to send an operation, but providing a string as the options to the command rather than an array.\n. delete_all_objects is not a valid operation on the SDK. Because of this, the SDK is using the magic __call method to attempt to find an operation by the provided name and execute it. Because the operation does not exist, you're seeing this error. You should instead use the deleteMatchingObjects function to delete objects under a specific \"folder\" (or key prefix): https://github.com/aws/aws-sdk-php/blob/master/src/Aws/S3/S3Client.php#L626\n. Good catch. Thanks.\n. I don't see how it would be possible for the error message you provided to ever be triggered. The function parseHeaders() in src/Aws/S3/Exception/Parser/S3ExceptionParser.php requires that a RequestInterface object is provided via a typehint. The error message you posted says that the provided request object is not an object, which is impossible... This makes me think there's something weird going on here.\nCan you provide verbose curl output of a failing headObject request? You provide example bucket and key values I can test?\nYou can enable verbose cURL output using something like this:\nphp\n$s3 = Aws\\S3\\S3Client::factory([\n    'curl.options' => [CURLOPT_VERBOSE => true]\n]);\n. I'll go ahead and close this issue. It sounds like you'll be using a Laravel specific wrapper anyway. Please feel free to reopen it if you can provide more information on how this is possible.\n. You'll need to either configure Apache so that it has the HOME environment variable which references a directory containing your ini file, deploy instance profile credentials to your EC2 instance (when running on EC2), create credentials using the fromIni method of a credentials object, or create credentials manually in some other way.\n. Wow! That was fast!\n. I think it should be recommended but not required.\n. Which region are you using? Are you able to contact other services?\nPlease try setting ssl.certificate_authority to \"system\" when creating a client. This will use the CA bundle on your system.\n. I've also seen this report that indicates the issue might be fixed by simply restarting apache, php-fpm, or whatever web server you're running.\nhttp://bugs.centos.org/view.php?id=7647\n. Which of my two recommendations did you try?\n. Thanks for letting us know. Hopefully this will help others if they encounter the same issue.\n. Can you provide a stack trace? The only place I can see the UriParser being used is in the error parser used by the S3Client, and we can update that bit of code to fail gracefully when using a non-standard S3 location. The stack trace would help us to understand where this is happening.\n. Whoops. I accidentally hit close instead of comment.\n. I've updated the v3 branch to add support for custom S3 endpoints:\n1. Specify an endpoint parameter in the factory method.\n2. Set force_path_style to true to prevent the SDK from using bucket-style addressing.\nphp\n$s3 = Aws\\S3\\S3Client::factory([\n    'endpoint'         => 'http://127.0.0.1:8125',\n    'region'           => 'us-east-1',\n    'version'          => 'latest',\n    'force_path_style' => true\n]);\n. You're right, thanks for letting me know. I pushed a fix that addresses this and adds a test. https://github.com/aws/aws-sdk-php/commit/620d3a8cc2790e016ef5d256da48a4a2321b2513\n. Yeah, we removed all instances of newInstance() as PHP 5.4+ now allows you to do this:\nphp\n(new UploadBuilder)->setClient($s3)->....\n. @iwai That looks like it's likely a bug in the React RingPHP adapter. I suggest opening a small reproducible test case on that repository: https://github.com/WyriHaximus/ReactGuzzleRing\n. Thanks for all the feedback we've received so far.\nI'm going to close this issue, but please feel free to continue to provide v3 feedback on this issue.\n. Can this issue be resolved or are you still having problems?\n. I'll go ahead and resolve. Feel free to reopen or comment further if you still have questions.\n. It's weird that you'd get this intermittently. I'm not sure if you're using a webserver, but other users have reported that they needed to restart their web server (e.g., Apache) to correct this issue after a recent NSS update. See: https://github.com/aws/aws-sdk-php/issues/363#issuecomment-59261137\nIf that doesn't fix the issue for you, can you provide verbose curl output?\n. By the way, you can get verbose curl output using the following:\nphp\n$s3 = Aws\\S3\\S3Client::factory([\n    'curl.options' => [CURLOPT_VERBOSE => true]\n]);\nIt would also help to know if you're running this from Amazon Linux, which version of cURL you're using, and which TLS backed you're using (e.g., NSS, OpenSSL). This can be retrieved using:\nphp\nvar_dump(curl_version());\n. Thanks for the additional information.\nAre you running these tests from a web server or at the command line? If you are running from a web server, then the verbose curl output I asked for is sent to STDERR, which should be visible in you webserver's error log.\n\nAny clue?\n\nNo :disappointed:. It seems like this is a cURL error of some kind, but I'm not sure why it's happening. I'll launch a beanstalk environment and test this out as well.\n. I just launched a beanstalk environment in us-west-2 running PHP 5.5 and sent 100,000 GetItem requests to a DynamoDB table with 50 read capacity units in the same region. I was unable to reproduce this issue. I also launched an environment in eu-west-1 (to ensure this isn't region specific) and was unable to reproduce this (or any) error.\nCan you be more specific about your stress test? What operations are you executing? Are you hitting a DynamoDB table in the same region? What's your read capacity units? Can you look in your Apache error logs for clues? For example, maybe you're running out of memory which makes it so that cURL cannot load and parse the CA bundle. The error log for Apache is located at /var/log/httpd/error_log.\n. It's going to depend a great deal on your application. You should determine how many processes your server can reasonably handle and tune your Apache configs. Using a larger instance type with more memory can help as well. You should check to ensure that you don't have a memory leak in your application. If there is a memory leak that you are unable to find, then you can lower the MaxRequestsPerChild Apache directive: http://httpd.apache.org/docs/2.2/mod/mpm_common.html#maxrequestsperchild.\n. Our minimum version is 5.4.0. 5.4.1 is recommended but not required.\n. Can you provide a reproducible test case? I can't reproduce this:\n``` php\nrequire 'vendor/autoload.php';\n$s3 = Aws\\S3\\S3Client::factory([\n    'region'       => 'us-east-1',\n    'curl.options' => [CURLOPT_VERBOSE => true] // Show what's sent over the wire\n]);\n// Create a test object. The SDK will encode the key over the wire as \"foo%20bar\".\n$s3->putObject(['Bucket' => 't1234', 'Key' => 'foo bar', 'Body' => 'test']);\n// Use the CopySource operation. Pass the CopySource operation exactly as it's stored in S3.\n$s3->copyObject(['Bucket' => 't1234', 'Key' => 'copy', 'CopySource' => 't1234/foo%20bar']);\n```\nI tried other variations of CopySource: I used \" \" instead of \"%20\" and I tried \"+\" instead of %20. They all successfully copied the object.\n. Ah, I see. Thanks for the example. This is the result of how the key is URL encoded by the SDK and how Amazon S3 is interpreting the CopySource parameter.\nTake the following example:\n``` php\n<?php\n$s3->putObject([\n    'Bucket' => 't1234',\n    'Key'    => 'foo(+7).bar',\n    'Body'   => 'hi'\n]);\n```\nThis is sent over the wire as:\n```\nPUT /foo%28%2B7%29.bar HTTP/1.1\nHost: t1234.s3.amazonaws.com\nUser-Agent: aws-sdk-php2/2.7.2 Guzzle/3.9.2 curl/7.37.0 PHP/5.5.13\nContent-MD5: SfaKXIST7CwL9ImCHCH8Ow==\nDate: Wed, 29 Oct 2014 18:33:48 +0000\nAuthorization: AWS ...\nContent-Length: 2\nhi\n```\nWhen sending a request with the CopySource parameter, you must URL encode the key exactly as it was sent to Amazon S3. Different tools can use different encodings, and the SDK must be able to work with any data stored on Amazon S3. Because of this, we cannot automatically encode the provided key. You can encode the key using the same encoding strategy used by the SDK using the encodeKey() method of the SDK:\n``` php\n<?php\n$s3->copyObject([\n    'Bucket'     => 't1234',\n    'Key'        => 'copy',\n    'CopySource' => $s3::encodeKey('t1234/foo(+7).bar')\n]);\n```\nThis will correctly send the following HTTP request:\n```\nPUT /copy HTTP/1.1\nHost: t1234.s3.amazonaws.com\nUser-Agent: aws-sdk-php2/2.7.2 Guzzle/3.9.2 curl/7.37.0 PHP/5.5.13\nx-amz-copy-source: t1234/foo%28%2B7%29.bar\nDate: Wed, 29 Oct 2014 18:36:14 +0000\nAuthorization: AWS ...\nContent-Length: 0\n```\nNotice that the CopySource parameter is serialized correctly as x-amz-copy-source: t1234/foo%28%2B7%29.bar so that Amazon S3 is able to identify the object.\n. Thanks to this PR, https://github.com/aws/aws-sdk-php/issues/381, the constant is now available.\nThat said, I suggest migrating your application away from relying on these enums. They have been deprecated in the SDK and will not be part of the next major version of the SDK. See: https://github.com/aws/aws-sdk-php/issues/368#issuecomment-59449270\n. Thanks.\n. We haven't made any changes to how we use Composer.\nAre you sure you're using the right path to the autoload.php file? You're using a relative path there, so you should check the script's current working directory to ensure the path resolves correctly.\n. Which version of the SDK are you using?\n. I've identified the issue and will push a fix as soon as possible.\n. I noticed this today as. It's due to a regression in Guzzle that was literally fixed a few minutes ago :). Composer update, get the latest version of Guzzle, and everything will work.\n. I'll merge this, but please update your code to instead use the endpoint provider: https://github.com/aws/aws-sdk-php/blob/master/src/Aws/Common/RulesEndpointProvider.php\nphp\n$provider = Aws\\Common\\RulesEndpointProvider::fromDefaults();\n$result = $provider(['service' => 'my-service-name', 'region' => 'my-region-value']);\necho $result['endpoint'];\n. We do not maintain the packages for ubuntu or any distribution other than Amazon Linus.\n. That is correct. Thanks, @skyzyx.\n. Yes I tested it as well with actual numeric values: using the following results in a server side error:\n``` php\n<?php\nrequire 'vendor/autoload.php';\n$ddb = Aws\\DynamoDb\\DynamoDbClient::factory([\n    'version' => 'latest',\n    'region'  => 'us-east-1'\n]);\n$c = $ddb->getHttpClient();\n$r = $c->createRequest('POST', 'https://dynamodb.us-east-1.amazonaws.com/', [\n    'headers' => [\n        'Host' => 'dynamodb.us-east-1.amazonaws.com',\n        'X-Amz-Target' => 'DynamoDB_20120810.PutItem',\n        'Content-Type' => 'application/x-amz-json-1.0'\n    ],\n]);\n$r->setBody(GuzzleHttp\\Stream\\Stream::factory('{\"TableName\":\"32095378929_put_item_perf\",\"Item\":{\"foo\":{\"S\":\"bar\"},\"bar\":{\"N\":\"10\"},\"baz\":{\"M\":{0:{\"S\":\"1\"},\"hi\":{\"S\":\"there\"}}}}}'));\n$c->send($r);\n```\nThe reason your payload is valid is because the key is a string \"0\" and not a numeric 0. When providing a numeric 0 as a key in the JSON payload (as a string), you get an error. If PHP's json_encode coerces this automatically, then that's cool, but I think it's an edge case that we don't need to account for considering mixing numeric and string keys in PHP is commonly accepted as a bad practice except in very specific circumstances. Based on this, I don't think we  need to modify the existing checks we have in place because they cover the normal use case and do not conflate \"0\" and 0 keys.\n(edited for clarity and formatting)\n. Which version of the SDK and Guzzle are you using? If you are using the latest version of the SDK and Guzzle,  then we will need to see verbose curl output of a failure.\nCan you also provide a stack trace please?\n\nOn Nov 9, 2014, at 7:48 AM, jehord notifications@github.com wrote:\nI try upload multiple images on cloud, in process uploading i have next message error.\nGuzzle\\Http\\Exception\\RequestException\nError completing request\npublic function parseArticleData($url, $xpathQuery)\n{\n$dir = Yii::getAlias('@frontend/web/funny_images/');\n```\n$content = '';\n$nodes = $this->createDomXPath($url)->query($xpathQuery);\nif ($nodes->length === 0) {\n    return $this;\n}\nfor($i = 0; $i < $nodes->length; $i++){\n    $fileExtansion = Yii::$app->yadisk->getImageType($nodes->item($i)->nodeValue); \n    $fileContent = file_get_contents($this->checkDomain($url, $nodes->item($i)->nodeValue)); \n    $fileName = md5(time() . $nodes->item($i)->nodeValue) . \".\" . $fileExtansion; \n    file_put_contents($dir . $fileName, $fileContent, LOCK_EX); \n    Yii::$app->yadisk->disk->uploadFile(\n        '/funny_images/',\n        array(\n            'path' => $dir . $fileName,\n            //'size' => filesize($fileName),\n            'name' => $fileName\n        )\n    );\n        //Yii::$app->yadisk->disk->showImage();\n        $content .= BaseUrl::toRoute(['cloud/images', 'path' => '/funny_images/'. $fileName]);\n\n}\nreturn $content;\n```\n}\n\u2014\nReply to this email directly or view it on GitHub.\n. Where did I say the latest version wouldn't work with it?\nOn Nov 9, 2014, at 12:37 PM, jehord notifications@github.com wrote:\nI used yandex disk api. Not the latest version of the SDK, since you said that the latest version will not work with yndex disk api.\nFull stack trace i'll post later\n\u2014\nReply to this email directly or view it on GitHub.\n. Please upgrade to the latest 2.x version of the SDK and 3.x version of Guzzle. If that does not fix the issue then we will need to see verbose curl output of a failed request.\nOn Nov 10, 2014, at 11:23 AM, jehord notifications@github.com wrote:\nin D:\\Programs\\OpenServerMini\\OpenServer\\domains\\funsite-lc.ru\\vendor\\guzzle\\http\\Guzzle\\Http\\Message\\Request.php at line 569\n560561562563564565566567568569570571572573574575576577578 * Process a received response\n\n- @param array $context Contextual information\n- @throws RequestException|BadResponseException on unsuccessful responses\n  /\n  protected function processResponse(array $context = array())\n  {\n  if (!$this->response) {\n  // If no response, then processResponse shouldn't have been called\n  $e = new RequestException('Error completing request');\n  $e->setRequest($this);\n  throw $e;\n  }\n$this->state = self::STATE_COMPLETE;\n// A request was sent, but we don't know if we'll send more or if the final response will be successful\n  $this->dispatch('request.sent', $this->getEventArray() + $context);\n  in D:\\Programs\\OpenServerMini\\OpenServer\\domains\\funsite-lc.ru\\vendor\\guzzle\\http\\Guzzle\\Http\\Message\\Request.php \u2013 Guzzle\\Http\\Message\\Request::processResponse(['handle' => Guzzle\\Http\\Curl\\CurlHandle]) at line 378 372373374375376377378379380381382383384 } $this->dispatch('request.before_send', array('request' => $this)); } break; case self::STATE_COMPLETE: if ($oldState !== $state) { $this->processResponse($context); $this->responseBody = null; } break; case self::STATE_ERROR: if (isset($context['exception'])) { $this->dispatch('request.exception', array(\n  in D:\\Programs\\OpenServerMini\\OpenServer\\domains\\funsite-lc.ru\\vendor\\guzzle\\http\\Guzzle\\Http\\Message\\EntityEnclosingRequest.php \u2013 Guzzle\\Http\\Message\\Request::setState('complete', ['handle' => Guzzle\\Http\\Curl\\CurlHandle]) at line 49\n  43444546474849505152535455 \n  return parent::__toString() . $this->body;\n  }\npublic function setState($state, array $context = array())\n{\nparent::setState($state, $context);\nif ($state == self::STATE_TRANSFER && !$this->body && !count($this->postFields) && !count($this->postFiles)) {\n$this->setHeader('Content-Length', 0)->removeHeader('Transfer-Encoding');\n}\nreturn $this->state;\n}\nin D:\\Programs\\OpenServerMini\\OpenServer\\domains\\funsite-lc.ru\\vendor\\guzzle\\http\\Guzzle\\Http\\Curl\\CurlMulti.php \u2013 Guzzle\\Http\\Message\\EntityEnclosingRequest::setState('complete', ['handle' => Guzzle\\Http\\Curl\\CurlHandle]) at line 319 313314315316317318319320321322323324325 protected function processResponse(RequestInterface $request, CurlHandle $handle, array $curl) { // Set the transfer stats on the response $handle->updateRequestFromTransfer($request); // Check if a cURL exception occurred, and if so, notify things $state = $request->setState(RequestInterface::STATE_COMPLETE, array('handle' => $handle)); // Only remove the request if it wasn't resent as a result of the state change if ($state != RequestInterface::STATE_TRANSFER) { $this->remove($request); } } /*\nin D:\\Programs\\OpenServerMini\\OpenServer\\domains\\funsite-lc.ru\\vendor\\guzzle\\http\\Guzzle\\Http\\Curl\\CurlMulti.php \u2013 Guzzle\\Http\\Curl\\CurlMulti::processResponse(Guzzle\\Http\\Message\\EntityEnclosingRequest, Guzzle\\Http\\Curl\\CurlHandle, ['msg' => 1, 'result' => 56, 'handle' => Resource id #936]) at line 244 238239240241242243244245246247248249250 / private function processMessages() { while ($done = curl_multi_info_read($this->multiHandle)) { $request = $this->resourceHash[(int) $done['handle']]; try { $this->processResponse($request, $this->handles[$request], $done); $this->successful[] = $request; } catch (\\Exception $e) { $this->removeErroredRequest($request, $e); } } }\nin D:\\Programs\\OpenServerMini\\OpenServer\\domains\\funsite-lc.ru\\vendor\\guzzle\\http\\Guzzle\\Http\\Curl\\CurlMulti.php \u2013 Guzzle\\Http\\Curl\\CurlMulti::processMessages() at line 227 221222223224225226227228229230231232233 // The first curl_multi_select often times out no matter what, but is usually required for fast transfers $selectTimeout = 0.001; $active = false; do { while (($mrc = curl_multi_exec($this->multiHandle, $active)) == CURLM_CALL_MULTI_PERFORM); $this->checkCurlResult($mrc); $this->processMessages(); if ($active && curl_multi_select($this->multiHandle, $selectTimeout) === -1) { // Perform a usleep if a select returns -1: https://bugs.php.net/bug.php?id=61141 usleep(150); } $selectTimeout = 1; } while ($active);\nin D:\\Programs\\OpenServerMini\\OpenServer\\domains\\funsite-lc.ru\\vendor\\guzzle\\http\\Guzzle\\Http\\Curl\\CurlMulti.php \u2013 Guzzle\\Http\\Curl\\CurlMulti::executeHandles() at line 211\n205206207208209210211212213214215216217 }\n}\nif ($blocking == $total) {\n// Sleep to prevent eating CPU because no requests are actually pending a select call\nusleep(500);\n} else {\n$this->executeHandles();\n}\n}\n}\n/**\nExecute and select curl handles\nin D:\\Programs\\OpenServerMini\\OpenServer\\domains\\funsite-lc.ru\\vendor\\guzzle\\http\\Guzzle\\Http\\Curl\\CurlMulti.php \u2013 Guzzle\\Http\\Curl\\CurlMulti::perform() at line 105\n99100101102103104105106107108109110111 $this->handles = new \\SplObjectStorage();\n$this->requests = $this->resourceHash = $this->exceptions = $this->successful = array();\n}\npublic function send()\n{\n$this->perform();\n$exceptions = $this->exceptions;\n$successful = $this->successful;\n$this->reset();\nif ($exceptions) {\n    $this->throwMultiException($exceptions, $successful);\nin D:\\Programs\\OpenServerMini\\OpenServer\\domains\\funsite-lc.ru\\vendor\\guzzle\\http\\Guzzle\\Http\\Curl\\CurlMultiProxy.php \u2013 Guzzle\\Http\\Curl\\CurlMulti::send() at line 91 85868788899091929394959697 // Add this handle to a list of handles than is claimed $this->groups[] = $group; while ($request = array_shift($this->queued)) { $group->add($request); } try { $group->send(); array_pop($this->groups); $this->cleanupHandles(); } catch (\\Exception $e) { // Remove the group and cleanup if an exception was encountered and no more requests in group if (!$group->count()) { array_pop($this->groups);\nin D:\\Programs\\OpenServerMini\\OpenServer\\domains\\funsite-lc.ru\\vendor\\guzzle\\http\\Guzzle\\Http\\Client.php \u2013 Guzzle\\Http\\Curl\\CurlMultiProxy::send() at line 282\n276277278279280281282283284285286287288 if (!($requests instanceof RequestInterface)) {\nreturn $this->sendMultiple($requests);\n}\ntry {\n    /* @var $requests RequestInterface  /\n    $this->getCurlMulti()->add($requests)->send();\n    return $requests->getResponse();\n} catch (ExceptionCollection $e) {\n    throw $e->getFirst();\n}\n}\nin D:\\Programs\\OpenServerMini\\OpenServer\\domains\\funsite-lc.ru\\vendor\\guzzle\\http\\Guzzle\\Http\\Message\\Request.php \u2013 Guzzle\\Http\\Client::send(Guzzle\\Http\\Message\\EntityEnclosingRequest) at line 198\n192193194195196197198199200201202203204 public function send()\n{\nif (!$this->client) {\nthrow new RuntimeException('A client must be set on the request');\n}\nreturn $this->client->send($this);\n}\npublic function getResponse()\n{\nreturn $this->response;\n}\nin D:\\Programs\\OpenServerMini\\OpenServer\\domains\\funsite-lc.ru\\vendor\\nixsolutions\\yandex-php-library\\src\\Yandex\\Disk\\DiskClient.php \u2013 Guzzle\\Http\\Message\\Request::send() at line 100\n949596979899100101102103104105106 * @return Response\n*/\nprotected function sendRequest(RequestInterface $request)\n{\ntry {\n$request = $this->prepareRequest($request);\n$response = $request->send();\n} catch (ClientErrorResponseException $ex) {\n$result = $request->getResponse();\n$code = $result->getStatusCode();\n$message = $result->getReasonPhrase();\nin D:\\Programs\\OpenServerMini\\OpenServer\\domains\\funsite-lc.ru\\vendor\\nixsolutions\\yandex-php-library\\src\\Yandex\\Disk\\DiskClient.php \u2013 Yandex\\Disk\\DiskClient::sendRequest(Guzzle\\Http\\Message\\EntityEnclosingRequest) at line 352\n346347348349350351352353354355356357358 $request = $client->createRequest(\n'PUT',\n$path . $file['name'],\n$headers,\nfile_get_contents($file['path'])\n);\n$this->sendRequest($request);\n}\n}\n/**\nin D:\\Programs\\OpenServerMini\\OpenServer\\domains\\funsite-lc.ru\\common\\components\\RobotComponent.php \u2013 Yandex\\Disk\\DiskClient::uploadFile('/funny_images/', ['path' => 'D:\\Programs\\OpenServerMini\\OpenS...', 'name' => '433c0eab64d945fbcdd6fb1c8001b073...']) at line 298\n292293294295296297298299300301302303304 '/funny_images/',\narray(\n'path' => $dir . $fileName,\n//'size' => filesize($fileName),\n'name' => $fileName\n)\n);\n//Yii::$app->yadisk->disk->showImage();\n$content .= BaseUrl::toRoute(['cloud/images', 'path' => '/funny_images/'. $fileName]);\n}\nreturn $content;\n}\nin D:\\Programs\\OpenServerMini\\OpenServer\\domains\\funsite-lc.ru\\common\\components\\RobotComponent.php \u2013 common\\components\\RobotComponent::parseArticleData('http://trinixy.ru/108097-prikoln...', '//*[@id=\"dle-content\"]/div[@clas...') at line 321\n315316317318319320321322323324325326327 $modelParsUrlData->site_id=$site->id;\n$modelParsUrlData->url_param_id=$param->id;\n$modelParsUrlData->url=$pars_url;\n$modelParsUrlData->save();\n$url_articles = $modelParsUrlData::find()->where(array('url_param_id' => $param->id))->all();\nforeach($url_articles as $url_article){\n$article = Yii::$app->robot->parseArticleData($url_article->url, $param->xpath_article);\n//VarDumper::dump($article, 10, true);\n            if(!empty($article)){//\u0415\u0441\u043b\u0438 \u043f\u043e\u043b\u0443\u0447\u0438\u043b\u0438 \u043a\u0430\u043a\u0438\u0435 \u0442\u043e \u0434\u0430\u043d\u043d\u044b\u0435 \u0432\u043d\u0443\u0442\u0440\u0438 \u0437\u0430\u043f\u0438\u0441\u0438\n                $modelParsArticles->site_id = $site->id;\n                $modelParsArticles->url_data_id = $url_article->id;\n                $modelParsArticles->article = $article;\nin D:\\Programs\\OpenServerMini\\OpenServer\\domains\\funsite-lc.ru\\frontend\\controllers\\RobotController.php \u2013 common\\components\\RobotComponent::parse(common\\models\\ParsSites) at line 26 202122232425262728293031 * @param string $username */ public function actionStart() { $sites = ParsSites::find()->all(); foreach($sites as $site){ Yii::$app->robot->parse($site);     } }\n}\n17. frontend\\controllers\\RobotController::actionStart()\n18. in D:\\Programs\\OpenServerMini\\OpenServer\\domains\\funsite-lc.ru\\vendor\\yiisoft\\yii2\\base\\InlineAction.php \u2013 call_user_func_array([frontend\\controllers\\RobotController, 'actionStart'], []) at line 55\n495051525354555657 $args = $this->controller->bindActionParams($this, $params);\nYii::trace('Running action: ' . get_class($this->controller) . '::' . $this->actionMethod . '()', METHOD);\nif (Yii::$app->requestedParams === null) {\nYii::$app->requestedParams = $args;\n}\nreturn call_user_func_array([$this->controller, $this->actionMethod], $args);\n}\n}\n\u2014\nReply to this email directly or view it on GitHub.\n. Does this error persist on the latest version of the SDK and Guzzle? If so, can you provide verbose curl output?\n. Feel free to reopen with more info if needed.\n. Thanks for the PR. These exception classes have been deprecated in the SDK. We are keeping the exception classes that existed previously, however we are no longer creating new specific exception classes. These types of exceptions are going to be removed in v3. For more information, see https://github.com/aws/aws-sdk-php/issues/368#issuecomment-59549399\n\nIf there is a fatal error due to loading a missing exception, then we will need to fix that separately.\n. The last exception looks like a v2 SDK exception.\n. This method was just a wrapper method around ClearBucket. You can still use the ClearBucket class and provide a custom iterator that does filtering based on key prefix, regex, etc. See: https://github.com/aws/aws-sdk-php/blob/v3/src/S3/ClearBucket.php#L48.\nDo you think we still need this method if ClearBucket is available?\n. I think it makes sense to keep this function around if only for backwards compatibility. The only minor break I made is that the \"before_delete\" option was renamed to \"before\" so that it can be passed without modification to the ClearBucket constructor.\nThis has been added back in https://github.com/aws/aws-sdk-php/commit/a2b1a8022e7338c9ce307fef1f094f1e7e6405df\n. > it might be good to update the docs on this one\nWhich docs are you referring to?\n. Gotcha. We are still working on fine tuning the docs. This is definitely good feedback.\n. Older S3 endpoints use s3-<region>.amazonaws.com, while all of the newer regions (and from what I understand all regions going forward) will use the s3.<region>.amazonaws.com pattern.\n. I've added this PR to address this: https://github.com/aws/aws-sdk-php/pull/401. What's happening is that the bucket is being placed in the host. Moving the bucket to the host is not necessary for GetBucketLocation, and can result in this error when dealing with buckets in different regions than what was configured for your client.\n. Merged now for v2 and committed to v3: cf77940137d42a4f42e9855c19c95afb1be7832f. Let us know if you are still having issues.\n. The AWS SDK for PHP does not support Amazon Payments or Amazon FPS. For questions regarding these services, please post to their forum at https://forums.aws.amazon.com/forum.jspa?forumID=35\n. I have a couple suggestions about calling document JSON, but other than that, :shipit: \n. v3 will use the system CA bundle by default. V2 uses a bundled CA bundle. We have considered changing v2 to use the system CA bundle by default, but that would probably be too much of a breaking change for users that are relying on the bundled cert.\nIn v2, you can use the system CA bundle by setting ssl.certificate_authority to \"system\" when creating a client.\n. It will not be released this year. We hope to release it in the next few months if possible.\n. Which SDK version are you using? What settings are you passing when creating a client?\nThe SDK switches between virtual hosted and path style endpoints based on whether or not a bucket name is DNS compatible. In order for third-parties to fully emulate S3, they'd need to support these style URLs as well. Otherwise, you might try setting PathStyle to true when sending a command with the S3Client. This will ensure that the bucket name is not moved to the host name.\nThe SDK also needs to use different endpoints for different regions (as you noticed in one of the issues you linked to). There's nothing we can do here client-side.\n. Yes, the Frankfurt endpoint uses a different URL pattern. Why do you need to set the region to Frankfurt if you aren't actually hitting Frankfurt? Setting the region to eu-central-1 also updates the SDK to use signature version 4 instead of the classic S3 signing method. This could potentially cause problems as well.\n\nSo, if I pass the PathStyle to the factory should it get me covered when sending commands or should I be passing it like here aws/aws-sdk-php-laravel#36 (comment) as well?\n\nYes, that should make it work. By passing it into the client, it's a default option so that you don't need to set it on each command.\n. Sorry about that. This is now fixed in the master branch and will be present in the next release.\n. It does need to be updated. I'll remove hardcoded as that doesn't make as much sense as I thought. I'm looking at making a few other changes (like supporting credential providers in a client factory) and will update the PR.\n. Ok, updated with more changes and cleanup. Take a look at the commit message for changes and motivation.\n. > It seems like maybe we could consolidate that by removing Credentials::factory and refactoring that logic into the Provider and/or ClientFactory. This is not really a big problem though, just a thought.\nThe Credentials::factory() method remains for backwards compatibility with v2: https://github.com/aws/aws-sdk-php/blob/master/src/Aws/Common/Credentials/Credentials.php#L81. I'm fine with moving this logic to the client factory, but I want to point out that this would be a breaking change.\n. Pushed another commit that removes Credentials::factory() and moves the consts from Credentials to Provider.\n. Merged :tada: :cake:\n. Can you confirm that the user executing the script has an exported HOME environment variable available to PHP that contains a credentials file? For example, when running as apache or root, these users have a different HOME environment variable or none at all.\n. Using a custom path is not currently possible, but I just pushed a commit to a feature branch that will be merged soon that adds support for using a custom path. In the meantime, you can use putenv:\nphp\nputenv('HOME=/path/to/dir');\n. It looks like this is a networking issue specific to your server. I do not believe there is anything we can do from the client side.\n. This won't be something added to the SDK itself as it would be a large project, but we'll forward this feature request along to the SWF team.\n. Sorry for the delay.\nIt looks like the DescribeInstances operation could be missing some configuration values to make it iterable. It's currently pageable, but has no iterable configs which causes it to fail. In the meantime, you could paginate over the result using getPaginator() and use the search() function of the paginator to perform a custom JMESPath search over the aggregated results.\n. Closed by https://github.com/aws/aws-sdk-php/commit/fbaa02d06ccea42fc177df5bda9e2ee27f321240\n. This information is sourced from an internal build tool. Because this is an automated process, we'll push the relevant changes through our internal build process and then deploy any code changes as a result of that build step. \nWe won't be able to change the result_key  because it loses the association of an instance and its reservation. Other SDKs also use the current result_key setting, so changing this upstream would be a breaking change and would make the PHP SDK inconsistent with the other AWS SDKs.\n. We use a custom error handler here to remove any side-effects that do not follow the described interface (here we're turning warnings into exceptions). I acknowledge that there is some information loss here, so we'll look into this and see if there's a better way to transfer this information into the exception.\n. I just pushed a fix to Guzzle 3 to address this issue. I'll try to release the next version of Guzzle 3 this week or next week. In the meantime, you could update your composer.json to use the dev-master version if this is a blocker.\nhttps://github.com/guzzle/guzzle3/commit/9563ee1feb74e289ef608a983e88d696d3ce18b2\n. Are you using version 3 of the SDK via Composer with 3.x@dev?\n. We are working on putting another beta together that will address this, but in the meantime, you need to use 3.x@dev.\n. I'm hoping to get a new beta release tagged this week.\n. Does it cause an error? Can you send a PR?\n. I wouldn't consider it a breaking change as the interface is in the stream wrapper functionality not in the class.\n. You'll need to use either PHP 5.6+ or specify the path to a CA bundle on disk. You can get a CA bundle from http://curl.haxx.se/docs/caextract.html or from somewhere else you trust. You can then wire up the SDK to use this CA bundle by providing a ssl.certificate_authority value to a client factory method:\nphp\n$s3 = Aws\\S3\\S3Client::factory([\n    'region' => 'us-west-2',\n    'ssl.certificate_authority' => '/path/to/ca-bundle.crt'\n]);\nYou can also specify the openssl.cafile PHP ini setting to set a default location for a CA bundle: https://wiki.php.net/rfc/tls-peer-verification#phpini_defaults\n. Looks like GAE doesn't really provide parity with the PHP stream wrapper context options: https://cloud.google.com/appengine/docs/php/urlfetch/. So setting a CA bundle probably won't work.\nIt seems like setting of the CA bundle needs to be disabled so that GAE can validate the CA certs using their proxy based validation. This will need to be added to Guzzle's Ring library. It would also be really great if you could raise this issue with Google App Engine as they don't support cURL and they don't support SSL options on the PHP stream wrapper.\n. We appreciate it. Here's to hoping they add better client support (either through cURL or adding parity with PHP's stream wrapper options).\n. Across our services, you can't always disable SSL (actually you can only rarely disable it). You can however disable SSL verification in v3 using client_defaults and verify:\nphp\n$client = Aws\\S3\\S3Client::factory([\n    'region'          => 'us-west-2',\n    'version'         => 'latest',\n    'client_defaults' => ['verify' => false]\n]);\nThe v2 option of setting ssl.certificate_authority to false will also work:\nphp\n$client = Aws\\S3\\S3Client::factory([\n    'region'  => 'us-west-2',\n    'version' => 'latest',\n    'ssl.certificate_authority' => false\n]);\n. Doh. I removed client_defaults previously because I didn't like how it was an exclusive option with client in ClientFactory. I meant to add it back in and apply the defaults after the client is created in the factory, but forgot.\nI've added back client_defaults here: https://github.com/aws/aws-sdk-php/commit/8f92cbafe9c78e6de6c910acc0cfa5d422a83200. This will make the examples I posted work for you.\nPlease also composer update your dependencies.\n. That's good news, and yes, info on that would be great. Hopefully GAE will add curl support and ssl verification, but your info could help others in the meantime.\n\nOn Dec 19, 2014, at 1:50 PM, ChrisTerBeke notifications@github.com wrote:\nWorked! Is it cool if I send you some info on all the steps I needed to take to get it working on app engine? Maybe you can somehow incorporate that in your documentation?\n\u2014\nReply to this email directly or view it on GitHub.\n. Thanks!\n. Which version of the SDK are you using?\n. I'm not able to reproduce this with the following code:\n\n``` php\n<?php\nrequire 'vendor/autoload.php';\nuse Aws\\S3\\S3Client;\n$s3 = S3Client::factory([\n    'region' => 'us-east-1',\n    'curl.options' => [CURLOPT_VERBOSE => true]\n]);\n$s3->uploadDirectory(DIR . '/src', 'abcba', '', ['debug' => true]);\n```\nNote: I also tried passing null to the $keyPrefix argument of uploadDirectory().\nThis calls a single GET request for abcba.s3.amazonaws.com, then starts uploading files. The way this function works is that it iterates over the files in the bucket and compares them against the local files to see which files have changed. As the files in the bucket are iterated, you should see more ListObjects related requests for buckets that contain more than 1000 files.\n. Ah, I see that in the output of sudo dtruss -f -t lstat64 php test.php. I'll see if I can figure out why that's happening.\nIt's not causing any extraneous HTTP requests though, so I'm not seeing any performance impact from this.\nEdit: Just as a datapoint, I checked v3 of the PHP SDK, and it no longer has this problem.\n. Just curious: how are you measuring the performance impact? Does strace show you the time it takes to make each call?\n. I found the issue: we were calling getRealPath() on S3 SplFileInfo objects. This isn't needed and was causing the issue. Can you try out this change (on master) and see if it fixes the performance issue?\n. I guess the lstat calls weren't causing a significant performance impact (but I think I know what is).\n\nLooking at your fix, I'm surprised it wasn't in the Wrapper as Calling getRealPath() is still \"broken\" for anyone who calls it.\n\nI'm not sure there's anything I can do about that. Simply calling getRealPath() on an SplFileObject using a custom stream wrapper will trigger the erroneous call. I believe this is because PHP is calling real_path on the custom protocol filename. real_path doesn't think it's an absolute path, so it prepends the current working directory. That's what I can gather from a quick scan of the source (https://github.com/php/php-src/blob/59593ba66c00220f0161a721c80550b2a0305c31/ext/spl/spl_directory.c#L1287 -> https://github.com/php/php-src/blob/fc33f52d8c25997dd0711de3e07d0dc260a18c11/Zend/zend_virtual_cwd.c#L1429).\nThe system calls to gettimeofday are due to getting the MTime of each file to check if the file needs to be uploaded again... Speaking of which...\nIf you know your bucket has a ton of files and that you aren't uploading many files, then you could \"force\" the upload, meaning you upload all of the files regardless of if they changed remotely or not.\nphp\n$s3->uploadDirectory('/path/to/files', 'mybucket', '', ['force' => true]);\nAttempting to determine whether or not a file should be uploaded to the bucket requires that the files in the bucket be iterated over as the local files are iterated. When the file is present locally but not remotely, it is uploaded unconditionally. When they are both present and the local file is newer or has a different size, then the local file is uploaded. This is likely the cause of the slowness and CPU utilization you are experiencing.\nThe technique we use to for syncing is not foolproof as it can provide false positives. The AWS CLI is the recommended way to do any type of syncing with S3 (that is, if you do not have to use PHP). We actually removed the syncing aspects from v3 of the PHP SDK in favor of simply uploading every file unconditionally (the abstraction accepts an iterator that yields filenames as strings, so this could be extended in the future to add back in syncing, but we would wait on S3 to provide more API hooks to better allow S3 to be treated like a filesystem).\n. I'll ahead and close this. Please let me know if you have any further questions.\n. Good idea. I'll add this suggestion to our backlog.\n. Can you provide more information please? How about a small code snippet that reproduces the problem?\n. Based on the API documentation for Amazon RDS, I think this is how the API is intended to work: http://docs.aws.amazon.com/AmazonRDS/latest/APIReference/API_CreateDBInstance.html. \nYour commented out example shows using a 'standard' StorageType with 100 IOPS resulting in a magnetic storage. If I'm understanding that correctly, then this is as expected.\nThe second example shows using gp2 with no provision IOPs setting. Based on the docs, it looks like you have to set, at a minimum, Iops to 1000 in order to actually use gp2 (see: http://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/CHAP_Storage.html#Concepts.Storage.GeneralSSD).\nBecause this is an issue with the API and not the PHP SDK, I suggest posting to the Amazon RDS forums to have them take a look: https://forums.aws.amazon.com/forum.jspa?forumID=60\n. Can you provide a full stack trace of this exception? What version of the SDK are you using? What version of Guzzle are you using? Newer versions of the SDK and Guzzle will retry this type of networking error multiple times before failing. From what we've seen previously (and assuming this is the same thing), these types of errors typically occur on poor internet connections. The typical workaround is to reduce the number of files you are uploading concurrently.\n. I see you got an SSL read error: [curl] 56: SSL read: error. This error is automatically retried several times before failing. I suggest lowering the number of simultaneous requests you are sending to work around this problem. Updating your version of cURL or OpenSSL/NSS could potentially help, but I'm not sure if it will.\n. One potential way to try to figure out why it's happening would be to evaluate a tcpdump that shows the error occurring.\n. I'll go ahead and close this for now. Feel free to reopen with a network capture if necessary. Thanks!\n. Thanks for taking a stab at this! I've now created a generated client using our internal build system. This has been added in https://github.com/aws/aws-sdk-php/commit/c62d4ac30c1bfbcbb1ed3d086ec8021830b5d426.\n. This isn't a necessary change.\n. Can you show us the code you used to attach the logger? We need to see the XML body of the request in order to see why/if it was malformed.\n. Doh: the docs you referenced was actually an incorrect example (I'll update that today). The \"result\" wrapper shown in these incorrect docs is still removed (meaning \"DescribeCacheClustersResult\" is stripped from the result).\nThe wrapper that I meant to write about that is no longer removed is a different kind of wrapper. It's a wrapper that is a shape reference that has a \"wrapper\" value set to true. We previously would look at the shape reference of an output shape and see that if the reference contained a \"wrapper\" attribute then we would merge the referenced shape up one level. Here's an example from RDS's DescribeEngineDefaultParameters:\nThe operation has an output shape reference with a wrapper set to true (note that this is different than having a resultWrapper): https://github.com/aws/aws-sdk-php/blob/v3/src/Common/Resources/api/rds-2014-09-01.api.php#L1345. This output shape references the DescribeEngineDefaultParametersResult shape. The DescribeEngineDefaultParametersResult shape is a structure that has a single key, EngineDefaults, in which the key value is a reference to a EngineDefaults shape. In v2, we got the value of \"EngineDefaults\" and merged it up to the root level of the result, while in v3 we are no longer doing this. This is to be consistent with the other SDKs and CLI, and to prevent any breaking changes if a service team ever adds a new shape to an output structure that has a wrapper attribute.\nThere are 3 services that utilize this attribute:\n- elasticache\n- rds\n- redshift\n. Here's the updated upgrade guide with a complete list of affected operation results: https://github.com/aws/aws-sdk-php/commit/790bc4d06809df907de6008f99adb853d08dd912\n. Yes it's still supported and the call in the integration test should not fail.\nCan you ensure that garbage collection is enabled? If you increase the maximum allowed memory size, does the example operation complete? What's the amount of memory consumed by your process before invoking the operation?\n\nOn Jan 2, 2015, at 11:05 AM, Ivete Tecedor notifications@github.com wrote:\nI am trying to automate the addition of ALIAS records for certain subdomains for our app. I can instantiate the Route53 client and it responds to a getServerTime() request just fine. However any time I try to make changes using changeResourceRecordSets, it appears to go into an infinite loop as I get a memory error.\nI've tried adjusting the array in various ways, including using the XML version of the payload (found here: http://docs.aws.amazon.com/Route53/latest/APIReference/API_ChangeResourceRecordSets_Requests.html#API_ChangeResourceRecordSets_RequestAliasSyntax) but have not gotten anywhere. I even tried using the change that is in the Route 53 Integration test BasicOperationsTest.php and THAT threw the memory error:\nException: Error 1: Allowed memory size of 134217728 bytes exhausted (tried to allocate 131334144 bytes) \nCan someone please confirm that the operation is still supported and that the format of the changeRequest array still conforms to the documentation? Documentation link: http://docs.aws.amazon.com/aws-sdk-php/latest/class-Aws.Route53.Route53Client.html#_changeResourceRecordSets\nThank you.\n\u2014\nReply to this email directly or view it on GitHub.\n. Not a problem. Increasing the memory should work. Some of these responses can get rather large as they are kept in memory as both parsed and un-parsed data. This is something that is different in the WIP v3 SDK (we do not hang on to the HTTP response when you are provided with a result object).\n. Have you tried placing the \"NextToken\" parameter inside of the parameters of the call? The code you've provided looks incorrect because you are passing in two parameter arrays. Try something like this:\n\nphp\n$params = array('SelectExpression' => \"select * from mydomain WHERE City like 'Dallas%' LIMIT 2\");\n$params['NextToken'] = $resultb['NextToken'];\n$result = $client->select($params);\n. Can you run these commands again, but include over the wire HTTP output?\nphp\nuse Guzzle\\Plugin\\Log\\LogPlugin;\n$client = Aws\\Rds\\RdsClient::factory();\n$client->addSubscriber(LogPlugin::getDebugPlugin());\n$client->createDBInstance($params);\nbash\naws rds create-db-instance --debug \\ # other params ...\n. Thanks for the verbose output.\nYou need to update the version of the PHP SDK you are using. You are using version 2.6.16. StorageType wasn't added to RdsClient until 2.7.1: https://github.com/aws/aws-sdk-php/commit/937a39ca3cee98d31a7410a17db24e0496c41494. The current version of the SDK is 2.7.12.\n. Thanks for the report. This has now been fixed in the v3 branch.\n. LGTM\n. I just pushed up all the docs models and waiters2 models that are currently published to the v3 branch. This PR will need to be reapplied to the v3 branch. I used the models published to the Ruby SDK as the basis for what is currently published. Closing until then.\n. I got the build passing after fixing the @covers annotations and making a goto related recursion change to guzzlehttp/command: https://github.com/guzzle/command/commit/3303240d33b53c4ced7d94695dd2faa1712fdc58\n. It depends on the stream that you use. If it has a known content-length because there was a content-length header in the HTTP response, then we will automatically know the length of the stream (assuming you're using a PHP stream wrapper for the input stream).\nFor chunked streams or streams, we do not attempt to calculate the length because it could be anywhere from 0 to an infinite number of bytes. If the stream is not rewindable (which network streams usually aren't), then we'd have to cache all of that data in memory.\n. Cool, I'll fix that.\n. I like them at the root namespace because the namespace is shorter to type, there's no namespace collision possibility thanks to the \"Client\" suffix, and every client that we support will have a Client class. Not every client we support will have a custom factory, and end users will never need to interact with factories, so it makes sense to put them in a specific namespace.\n. Fixed by #458. We were trying to utilize a shallower namespace structure for clients and tried to change the namespaces in a way that allowed old code to still work via aliases at runtime. We ran into some edge cases (one you've run into here), and I've partially reverted those changes. This is now fixed in the v3 branch.\n. Thanks for letting us know about the issue with the policy simulator. I'll pass this along to the relevant IAM team.\nThe documentation page you linked to provides all of the information you need for constructing ARNs, but it would be nice if the service would return this information in the error message so that it could be automatically extracted by the client. I'll pass this along as a feature request to the IAM team.\n. This has the following error message for a missing version:\n\nA 'version' configuration value is required when creating an API client. For example, when using Amazon S3, you can lock your API version to '2006-03-01' to ensure that your code will be unaffected by a change in the web service.\nYour build of the SDK has the following versions of 's3': \n- 2006-03-01\nYou may provide 'latest' to the 'version' configuration value to utilize the most recent available API version that your client's API provider can find (the default API provider will scan the /Users/dowling/projects/sdk3/src/data directory for .api.php and .api.json files). Using 'latest' in a production application is not recommended.\nA list of available API versions can be found on each client's API documentation page: http://docs.aws.amazon.com/aws-sdk-php/v3/api/index.html. If you are unable to load a specific API version, then you may need to  update your copy of the SDK.\n\nAnd the following for a missing region:\n\nA \"region\" configuration value is required when connecting to the dynamodb service (e.g., us-west-2). A list of available public regions and endpoints can be found at http://docs.aws.amazon.com/general/latest/gr/rande.html\n\n. Fixed the @covers annotation to fix the tests.\n. Great changes! I had a couple points of feedback, but otherwise :shipit: \n. No, there's no other way to do it. Creating a relative URI is not supported by the SDK because it is meant to return an absolute URI.\n. Hm. Can you provide a stack trace? We attempt to load specific exception classes by name by checking if a class exists. This triggers autoloaders to try to load the given class name. If the class is not found, then the autoloaders should not raise an error, and we will throw the generic service specific exception.\nAre you using other autoloaders that are causing this error?\n. That's enough info. Thanks. This sounds like a bug in Magento's autoloader. When an autoloader cannot find a class, it should not raise an error, but rather let other autoloaders in the chain attempt to load the class. You can see that they're just calling include in the Simple autoloader: http://svn.magentocommerce.com/source/branches/1.9/lib/Magento/Autoload/Simple.php.\nGlad you got it working.\n. Should there also be a Number wrapper that ensures a value is serialized as a number, but the actual value can be internally stored as a string so as to not lose precision?\n. :shipit: \n. @jeremeamia You happy with this? Good to merge?\n. TTL is conditionally required, which means we cannot flag it as a required parameter (it's not required for Alias resource record sets). The Route53 documentation provides more information on TTL here: http://docs.aws.amazon.com/Route53/latest/APIReference/API_ChangeResourceRecordSets_Requests.html. The PHP SDK generates API documentation using a service description that contains all of the inputs, outputs, and API documentation. Unfortunately, the information that's described in the link I provided is not present in the service description. I'll pass this information along to the Route53 team.\n. You need to make sure that you're catching the exception properly by using the fully-qualified class name. You have used the wrong class name of the exception.\nuse Aws\\Ses\\SesClient\\SesException; -> replace with -> use Aws\\Ses\\Exception\\SesException;\n. Thanks for the thorough report. I think this might be related to another issue we had with empty list serialization in these query protocol services. I'll check on how we can address this and if the previous solution (IIRC) can be applied more generically.\n. Fixed in https://github.com/aws/aws-sdk-php/commit/b358d67b0e75195914799f5755f78d2bb37fdb69\n. Ok, all done. This PR is a huge refactor of the client factory system such that it is now gone. The goal of this PR is to allow you to call new S3Client($args) instead of having to use the factory method (note: the factory method is still available for BC).\nClient constructor arguments are declaratively specified per/client using a static getArguments() method. Custom factories are now gone in favor of simply overriding the client constructor and/or modifying the return value of getArguments() (for example, you can make \"endpoint\" required for CloudSearchDomain and provide a custom default \"region\" value using a closure).\nConstructor error messages are much more verbose and helpful now. For example, if you provide an invalid type to a specific setting, it'll tell you:\nInvalid configuration value provided for \"region\". Expected string, but got int(-2)'\nWhen required arguments are missing, it will describe the argument so that the customer doesn't need to look anything up, but rather can just make the necessary changes using the error message:\n```\nFatal error: Uncaught exception 'InvalidArgumentException' with message ' in /Users/dowling/projects/sdk3/src/ClientResolver.php on line 449\nInvalidArgumentException: Missing required client configuration options: \nregion: (string)\nA \"region\" configuration value is required for the \"dynamodb\" service\n  (e.g., \"us-west-2\"). A list of available public regions and endpoints can be\n  found at http://docs.aws.amazon.com/general/latest/gr/rande.html.\nversion: (string)\nA \"version\" configuration value is required. Specifying a version constraint\n  ensures that your code will not be affected by a breaking change made to the\n  service. For example, when using Amazon S3, you can lock your API version to\n  \"2006-03-01\".\nYour build of the SDK has the following version(s) of \"dynamodb\":\n  * \"2012-08-10\"\nYou may provide \"latest\" to the \"version\" configuration value to utilize the\n  most recent available API version that your client's API provider can find\n  (the default api_provider will scan the /Users/dowling/projects/sdk3/src/data\n  directory for .api.php and .api.json files). Note: Using 'latest' in a\n  production application is not recommended.\nA list of available API versions can be found on each client's API documentation\n  page: http://docs.aws.amazon.com/aws-sdk-php/v3/api/index.html. If you are\n  unable to load a specific API version, then you may need to update your copy of\n  the SDK. in /Users/dowling/projects/sdk3/src/ClientResolver.php on line 449\n```\nI know it's hard to review huge diffs like this... sorry. @jeremeamia  @GrahamCampbell: does this look OK? Note: we'll be updating the docs/ directory soon in a different PR.\n. I'm going to go ahead and merge this. We can fix any issues that need to be addressed in the v3 branch before we push out an RC.\n. This is a known issue with CloudFront that unfortunately cannot be fixed by the SDK (client), but has to be fixed at the service level. Here's a related forum post that shows that the CloudFront team is aware of the issue, and they'll post to this thread when it has been addressed: https://forums.aws.amazon.com/thread.jspa?messageID=580347\n. Hi Ben. base_url has been deprecated in favor of endpoint.  V2 supports both endpoint and base_url, while v3 only supports endpoint.\nI'm currently working on the documentation of v3, so hopefully we'll be able to clear up any confusion about configuration settings.\n. Thanks for the report. This will be fixed in the next release and build of the SDK.\n. I like the changes. Is there a reason why there isn't something like a ManifestProvider and a separate FilesystemProvider?\n. I want us to look into possibly not having the build step of creating .php files for the models and instead lazily create .php files in a cache directory if an API provider is wrapped in a caching function. With the caveat that going that route might require us to refactor some of this (and assuming it's performant enough): LGTM\n. Setting base_url is likely causing problems with the signature. Can you remove that option? Instead, you can just specify a region if needed.\n\nOn Feb 13, 2015, at 9:15 PM, ryanmc2033 notifications@github.com wrote:\nI am just getting started with AWS and I am trying to upload some images using this sdk. I have been following the examples and I am now getting the error listed in the title.\nI have checked my aws_access_key_id and aws_secret_access_key and they are both correct. I have stepped through the code and made sure the request contained the correct keys and data.\nI have tried both listing buckets and uploading an image but they both return the same message.\n$client = S3Client::factory(array(\n            'profile' => 'default',\n            'base_url' => xxx.s3.amazonaws.com/'\n        ));\n```\n    $result = $client->listBuckets();\ntry {\n    $resource = fopen($path, 'r');\n    $client->upload($bucket, $name, $resource, 'public-read');\n} catch (S3Exception $e) {\n    echo \"There was an error uploading the file.\\n\";\n}\n\n```\nI have also Search online for answers and followed a couple ideas, but still have not found a solution.\nIf anyone has a tip I would appreciate it.\nThanks\n\u2014\nReply to this email directly or view it on GitHub.\n. It's interesting that the Content-Length: 0 header was removed and a Transfer-Encoding: chunked header was added.\n\nI tried reproducing this with the following test case, but was unable:\n``` php\n<?php\nrequire 'vendor/autoload.php';\n$client = Aws\\S3\\S3Client::factory([\n    'region' => 'us-east-1'\n]);\n$history = new \\Guzzle\\Plugin\\History\\HistoryPlugin();\n$mock = new Guzzle\\Plugin\\Mock\\MockPlugin([\n    new \\Guzzle\\Http\\Message\\Response(500),\n    new \\Guzzle\\Http\\Message\\Response(200),\n]);\n$client->getEventDispatcher()->addSubscriber($mock);\n$client->getEventDispatcher()->addSubscriber($history);\n$client->putObject([\n    'Key'    => 'foo',\n    'Bucket' => 'bar',\n    'Body'   => ''\n]);\necho $history;\n```\nDoes running the above code example include a Content-Length header on the retry request?\n. Thanks for the additional information. I was able to reproduce this and found that the cause of the error was in Guzzle. I've pushed a fix for Guzzle (v3.9.3), and we will have this version bundled in the next release of the SDK's phar and zip files.\n. The amount of latency for a request greatly depends on your connection and the region you're connecting to. Most of the request times you posted don't seem out of the norm to me (e.g., 208 ms). The longer CloudFront times could be related to the geographical distance between your client and the server. You'd need to analyze your connection to the CloudFront endpoint to see why there is latency (e.g., traceroute, tcpdump, etc.).\nOne thing to keep in mind when contacting services is to use a region that's closest to your servers so that the distance that data needs to travel is reduced (thereby reducing latency).\nAs @jeremeamia  stated, you can also reduce the total amount of time needed to send these requests by sending them concurrently.\n. Interesting. When you call the copy function with a wire logger attached, what HTTP request is sent over the wire? Does it have the ACL parameter set as a header?\n. I'm not able to reproduce this problem. When I provide a context to copy, I'm able to set an ACL.\nHere's my script:\n``` php\n<?php\nrequire 'vendor/autoload.php';\n$s3 = Aws\\S3\\S3Client::factory([\n    'region'  => 'us-east-1',\n    'version' => 'latest',\n    'curl.options' => [CURLOPT_VERBOSE => true]\n]);\n\\Aws\\S3\\StreamWrapper::register($s3);\n$ctx = stream_context_create([\n    's3' => ['ACL' => 'foo']\n]);\ncopy('s3://t1234/channel-copy.xml', 's3://t1234/channel.xml', $ctx);\n```\nAs you can see, I've set the ACL to \"foo\", which should trigger a failure.\nHere's the output:\n```\n Hostname was NOT found in DNS cache\n   Trying 54.231.8.121...\n Connected to t1234.s3.amazonaws.com (54.231.8.121) port 443 (#0)\n TLS 1.2 connection using TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA\n Server certificate: .s3.amazonaws.com\n Server certificate: VeriSign Class 3 Secure Server CA - G3\n Server certificate: VeriSign Class 3 Public Primary Certification Authority - G5\n* Server certificate: Class 3 Public Primary Certification Authority\n\nHEAD /channel-copy.xml HTTP/1.1\nHost: t1234.s3.amazonaws.com\nUser-Agent: aws-sdk-php2/2.8.1 Guzzle/3.9.3 curl/7.39.0 PHP/5.6.3\nDate: Wed, 22 Apr 2015 05:13:00 +0000\nAuthorization: AWS :\n\n< HTTP/1.1 200 OK\n< x-amz-id-2: SpPpizdeq0XI7rvMuIB7TdEZ4o5G6V1SOmpC8Iu0tJh/IETMTf3vizzHVXi39kPh\n< x-amz-request-id: 0C0C0D7198865344\n< Date: Wed, 22 Apr 2015 05:13:02 GMT\n< Last-Modified: Wed, 22 Apr 2015 05:08:18 GMT\n< ETag: \"4c6820d314fd005212fcd46192302de7\"\n< Accept-Ranges: bytes\n< Content-Type: application/xml\n< Content-Length: 867\n< Server: AmazonS3\n< \n Connection #0 to host t1234.s3.amazonaws.com left intact\n Found bundle for host t1234.s3.amazonaws.com: 0x7f9e629a4510\n Re-using existing connection! (#0) with host t1234.s3.amazonaws.com\n Connected to t1234.s3.amazonaws.com (54.231.8.121) port 443 (#0)\n\nHEAD /channel.xml HTTP/1.1\nHost: t1234.s3.amazonaws.com\nUser-Agent: aws-sdk-php2/2.8.1 Guzzle/3.9.3 curl/7.39.0 PHP/5.6.3\nDate: Wed, 22 Apr 2015 05:13:01 +0000\nAuthorization: AWS :\n\n< HTTP/1.1 404 Not Found\n< x-amz-request-id: 5A06B1F44758F0BB\n< x-amz-id-2: Ul6zZG8lTt6B9sO6LSiiPxte5lgO8a4fOns7tk6jZQxfDT1kgiEIJzK4riDxHRpA\n< Content-Type: application/xml\n< Transfer-Encoding: chunked\n< Date: Wed, 22 Apr 2015 05:13:00 GMT\n< Server: AmazonS3\n< \n Connection #0 to host t1234.s3.amazonaws.com left intact\n Rebuilt URL to: https://t1234.s3.amazonaws.com/?max-keys=1&prefix=channel.xml%2F\n Found bundle for host t1234.s3.amazonaws.com: 0x7f9e629a4510\n Re-using existing connection! (#0) with host t1234.s3.amazonaws.com\n* Connected to t1234.s3.amazonaws.com (54.231.8.121) port 443 (#0)\n\nGET /?max-keys=1&prefix=channel.xml%2F HTTP/1.1\nHost: t1234.s3.amazonaws.com\nUser-Agent: aws-sdk-php2/2.8.1 Guzzle/3.9.3 curl/7.39.0 PHP/5.6.3\nDate: Wed, 22 Apr 2015 05:13:01 +0000\nAuthorization: AWS :\n\n< HTTP/1.1 200 OK\n< x-amz-id-2: WoNwmeqhER3YNpDR54s1fSiHc5CnKrXWrOdJWZTBjEDEwDlH1POv078MJMsLMVVH\n< x-amz-request-id: D5BB659199609875\n< Date: Wed, 22 Apr 2015 05:13:03 GMT\n< Content-Type: application/xml\n< Transfer-Encoding: chunked\n< Server: AmazonS3\n< \n Connection #0 to host t1234.s3.amazonaws.com left intact\n Found bundle for host t1234.s3.amazonaws.com: 0x7f9e629a4510\n Re-using existing connection! (#0) with host t1234.s3.amazonaws.com\n Connected to t1234.s3.amazonaws.com (54.231.8.121) port 443 (#0)\n\nPUT /channel.xml HTTP/1.1\nHost: t1234.s3.amazonaws.com\nUser-Agent: aws-sdk-php2/2.8.1 Guzzle/3.9.3 curl/7.39.0 PHP/5.6.3\nx-amz-acl: foo\nContent-Type: application/xml\nContent-MD5: TGgg0xT9AFIS/NRhkjAt5w==\nDate: Wed, 22 Apr 2015 05:13:02 +0000\nAuthorization: AWS :\nContent-Length: 867\n\n\nWe are completely uploaded and fine\n< HTTP/1.1 400 Bad Request\n< x-amz-request-id: 01ABB7C3A85742B1\n< x-amz-id-2: 5XstUWat+GQwRr2WOL59dcyI2TRjpE4RlUiSiPC1vhGr12oo5aKSmV8ljc4yoWj3\n< Content-Type: application/xml\n< Transfer-Encoding: chunked\n< Date: Wed, 22 Apr 2015 05:13:02 GMT\n< Connection: close\n< Server: AmazonS3\n< \nClosing connection 0\n```\n\nAs you can see, it tried to send a PutRequest with my bogus ACL and triggered a 400 response. You can also see that using the stream wrapper to perform a copy is far less efficient that using CopyObject directly, so I definitely recommend that over using the stream wrapper.\nI'm going to go ahead and close this issue. If you can provide more information on why this is not working for you, then I'd be happy to help.\n. This looks good.\nI liked having a getWaiter() and a waitUntil() method though. It would work well with your changes such that getWaiter() would return the waiter object and not initiate anything.  You'd then call promise() to kick off the async waiting and could then off of it. waitUntil() would call getWaiter(...)->promise()->wait();.\n. I'll go ahead and merge this. We can discuss getWaiter() or waitUntil() later.\n. :ship: \n. The \"limit\" attribute stands for the limit of the number of outstanding promises. It's based on async.js: https://github.com/caolan/async#eachLimit. The abstraction used in the SDK is a light abstraction on top of the Guzzle promises each abstraction which also uses \"limit\". I like limit personally, but if we were to change it, what would you suggest?\n. This is a known issue with Windows builds of PHP. I don't think there's a simple fix that we can make to the SDK to support 64-bit integers on Windows or any other 32-bit build.\nEven trying to read from a file on a Windows build that's bigger than PHP_INT_MAX is unpredictable. For example, calling ftell on a stream resource after seeking past PHP_INT_MAX will return unpredictable results. In your example,  I don't think $file->getSize() will return the right result once you've gone past PHP_INT_MAX.\n. I think there are experimental 64-bit builds of PHP for Windows. But, yeah, if you have to use Windows are using files > than the 32-bit cutoff, then you may need to use a different language's SDK.\n. Can you provide an example of when you encountered this error? Which version of the SDK are you using?\n. Thanks. I'll get in contact with the CloudFront team and see if there is an issue.\n. The CloudFront API had an issue related to the originPath parameter. They have a fix that will be deployed in their next release. They will update this forum issue when it is deployed: https://forums.aws.amazon.com/thread.jspa?threadID=172288\nIn the meantime, you can pass originPath as '' to work around the issue.\n. Thanks for pointing this out. Consumed capacity and various other information in the putItem response are only returned conditionally if the client asks for the information. You can get the ConsumedCapacity data by passing the following into the operation:\nphp\n$result = $client->putItem(array(\n    'ReturnConsumedCapacity' => 'TOTAL',\n    'TableName' => 'errors',\n    'Item' => array(\n        'id'      => array('N' => '1201'),\n        'time'    => array('N' => $time),\n        'error'   => array('S' => 'Executive overflow'),\n        'message' => array('S' => 'no vacant areas')\n    )\n));\nI'll push an update to the documentation shortly.\n. What's the \"^\" mean?\n. LGTM\n. Can you provide verbose cURL output?\nAlmost every case of this that I've seen is the result of cURL not receiving a response or error. This is likely cause by a bad network connection. Verbose cURL output might be able to provide supporting evidence.\nI suggest wrapping your call in a try/catch and retrying to the request if it fails with this particular error.\n. Sure, just supply this configuration to your client:\nphp\n$client = Aws\\S3\\S3Client::factory([\n    // other options...,\n    'curl.options' => [CURLOPT_VERBOSE => true],\n]);\n. Yes, this will give debug information from curl and will hopefully tell us what is going wrong in curl.\nRetrying 3 times sounds good. Retrying failed requests is mostly base on application specific thresholds (i.e., number of retries, delays between retries, etc.).\n\nOn Mar 30, 2015, at 6:30 PM, ducnguyenhuy notifications@github.com wrote:\nSo this will help to output more details about error, right?\nI have tried to use a loop and try/catch to retry calling. Max of retrying is 3 times. Do you think it's good?\nThanks\n\u2014\nReply to this email directly or view it on GitHub.\n. > Adding retry works and try/catch to show error in log.\n\nGreat!\n\nBut the error's still \"Error completing request\". Does that mean the server takes so long to response?\n\nThat's possible. I suspect that there's a bug in cURL or the PHP libcurl bindings. One possible way to determine that would be to see the verbose curl output of a failed request.\n. Just wanted to quickly follow up on this. Did you ever get the verbose curl output?\n. By adding the retry from https://github.com/aws/aws-sdk-php/issues/520#issuecomment-89995392, you will probably not see this error again. However, if you do, please provide us with the verbose curl output so that we can investigate further. Closing until then.\n. Which signature version is being utilized (v4 or s3)? If you don't know that, then what region are you connecting to?\n. Lookin good! I really like that there isn't an upload builder any more. I felt like that added a lot of complexity and state that wasn't necessary.\n. :shipit: \n. The diff times out when viewing in the changes tab. You can see the full diff here: https://github.com/aws/aws-sdk-php/pull/526.diff or browse the source here: https://github.com/aws/aws-sdk-php/tree/json-directly\n@jeremeamia Things of note:\n- Rewrote how the manifest is built: https://github.com/aws/aws-sdk-php/blob/json-directly/src/Api/ApiProvider.php#L224\n- Added a JsonCompiler class: https://github.com/aws/aws-sdk-php/blob/json-directly/src/JsonCompiler.php\n@trevorrowe I cc'd you so that you could take a look and ensure the directory structure looks right: https://github.com/aws/aws-sdk-php/tree/json-directly/src/data\n. Trevor reviewed this and said the model structure looks good. I'm going to merge this. We can iterate on the feature if needed. It's pretty much unreviewable due to the model updates.\n. We are blocked on PSR-7 acceptance. We will ship shortly thereafter.\n\nOn Apr 7, 2015, at 3:55 AM, Graham Campbell notifications@github.com wrote:\nAs I understand, the plan was to have it ready in May - it's nothing to do with me though. :)\nPing @jeremeamia.\n\u2014\nReply to this email directly or view it on GitHub.\n. Thanks for reporting. I've pushed a fix that now allows for an empty XML body for this operation. It is now sent over the wire successfully like this (sans signature):\n\n```\nPUT /?notification HTTP/1.1\nHost: my-bucket.s3.amazonaws.com\nUser-Agent: aws-sdk-php2/2.7.27 Guzzle/3.9.3 curl/7.39.0 PHP/5.6.3\nContent-Type: application/xml\nContent-Length: 99\nxml version=\"1.0\"?\n\n```\n. Hi. If you are running the code from your server, then the files will be downloaded to your server. You will need to run the code from your computer in order for the files to be downloaded to your computer. Alternatively, you could set up NFS or some other kind of shared disk to connect your server to your computer.\n\nPlease let me know if I misunderstood the question.\n. I'll go ahead and close this issue. Let me know if you still need assistance.\n. Based on the API documentation, I think the 'location' argument is just part of the raw JSON payload: http://docs.aws.amazon.com/lambda/latest/dg/API_Invoke.html. You need to provide a raw JSON document as a string parameter here.\n. Ah yes, you're right. I'll fix this up and have it ready to go for our next release.\n. I suppose that is possible. We'll add this as a feature request.\n. Support for using a custom protocol will be added in v3 of the PHP SDK which will likely ship in the next few weeks.\n. This looks awesome.\n. :tada: :balloon: \n. Just to make sure we're on the same page: the class you're talking about is https://github.com/aws/aws-sdk-php/blob/master/src/Aws/Sns/MessageValidator/Message.php, which is used to validate POST requests sent from SNS to your server. Correct?\n\nUnfortunately, Amazon SNS POST requests have a Content-Type of text/plain, which makes it impossible for the json_decode call to return anything other than null.\n\nThe fromRawPostData() function doesn't check the content-type of the HTTP request that it received. It just takes the raw body of the HTTP request that was received and tries to JSON parse it. The content-type sent from the service shouldn't matter.\n\n\nChange the SNS POST request Content-Type to 'application/json' or some other acceptable Content-Type\n\n\nThe POST comes from the service, so we can't change it from the client.\n\n\nAdd a private POST body-parsing function to the Message class\n\n\nAre you saying that the POST body is not actually JSON data?\n. Do you have any more information on this?\n. Reading from php://input should just include the body of the received request: http://php.net/manual/en/wrappers.php.php#wrappers.php.input. Are you seeing a different behavior?\n. What environment are you running in?\n. I just tested this on Amazon Linux, and the contents of php://input does not contain the headers.\nphp\n<?php\necho file_get_contents('php://input');\nOutput of this running under Apache with PHP 5.6.8\n```\ncurl -v --data 'testing' http://127.0.0.1/test.php\n   Trying 127.0.0.1...\n Connected to 127.0.0.1 (127.0.0.1) port 80 (#0)\n\nPOST /test.php HTTP/1.1\nUser-Agent: curl/7.40.0\nHost: 127.0.0.1\nAccept: /\nContent-Length: 7\nContent-Type: application/x-www-form-urlencoded\n\nupload completely sent off: 7 out of 7 bytes\n< HTTP/1.1 200 OK\n< Date: Wed, 22 Apr 2015 19:50:26 GMT\n< Server: Apache/2.4.12 (Amazon) PHP/5.6.8\n< X-Powered-By: PHP/5.6.8\n< Content-Length: 7\n< Content-Type: text/html; charset=UTF-8\n< \nConnection #0 to host 127.0.0.1 left intact\n\n\ntesting\n```\nI got the same result for PHP 5.3.29 and PHP 5.4.40.\nYou said that it \"seems\" to return the full request including headers. Can you write up a script to prove that and share the results with us? I want to make sure that this is the case even on Windows before we change anything.\n. @gummoe Were you able to confirm that php://input includes request headers? Based on my tests, the PHP documentation, and tests in the PHP git repo, it should not contain request headers.\nIf you look at some of the acceptance tests in php-src, you can see that the headers are not included:\n- https://github.com/php/php-src/blob/128eda843f7dff487fff529a384fee3c5494e0f6/tests/basic/enable_post_data_reading_05.phpt\n- https://github.com/php/php-src/blob/128eda843f7dff487fff529a384fee3c5494e0f6/tests/basic/enable_post_data_reading_01.phpt\n. I'm going to close this issue. If you are receiving headers in the contents of php://input, then it is likely an environment specific issue. If that is not the case, it would be helpful to have a reproducible test case that you can provide.\n. Sorry to hear that. I'll take a look and see if I can reproduce the issue.\n. Fixed in https://github.com/aws/aws-sdk-php/pull/538\n. Looks good. As Graham suggested, we don't need to parens. If it's too much of a hassle, then it doesn't really matter.\n. Hahaha :) thanks for the PR!\n. > I'm in the process of upgrading the amazons3 module to the 2.x SDK.\nJust to give you a heads up: we are actively working on a v3 of the PHP SDK that should be GA in the next month or so: https://github.com/aws/aws-sdk-php/tree/abstract-http. This branch will be merged into the v3 branch shortly, which will be merged into the master branch and tagged as 3.0.0 soon. It might be better to update the module to use our V3 instead as v2 will eventually be deprecated.\n\nOne of the features the module has is to cache stat calls.\n\nThis is one of those things that I'd prefer just be added to our V3. We actually already added an LRU cache to our v3 stat calls: https://github.com/aws/aws-sdk-php/blob/abstract-http/src/S3/StreamWrapper.php.\nWhen using our v3, the following code only sends a single request:\n``` php\n<?php\nrequire 'vendor/autoload.php';\n$c = new Aws\\S3\\S3Client([\n    'region'  => 'us-east-1',\n    'version' => 'latest',\n]);\n$c->getHandlerList()->append('sign', Aws\\Middleware::tap(function () {\n    echo \"Sending request\\n\";\n}));\n$c->registerStreamWrapper();\nvar_export(is_file('s3://t1234/test'));\nvar_export(is_file('s3://t1234/test'));\nvar_export(is_file('s3://t1234/test'));\n```\nIn summary, we'd prefer to not add major features to v2 now that v3 is about to be released. Given that v3 supports caching stat calls by default, is it acceptable for you to use our v3 in this Drupal module?\n. > What would you think about changing the static array to Doctrine's ArrayCache(), so it can be swapped out?\nIt needs to be a static value because you need to be able to persist the cache between created instances of the stream wrapper. As for using Doctrine's array cache: what's the use case for this? I don't really want to expose the caching behavior publicly or add a dependency on a cache until there is a need.\n\nI've got to have a beta out of the Drupal module next week, and need to be production ready in 3-4 weeks, which puts me before the GA release. Do you know of anyone already using the v3 branch in production?\n\nWe are hoping to ship v3 shortly after PSR-7 is accepted. I can't commit to a hard deadline (because I don't know how PSR-7 will play out), but I'd estimate sometime between May 11 through the 15th.\n\nDo you know of anyone already using the v3 branch in production?\n\nI'm not aware of anyone using it in production yet, but I think by mid-next week we will have the abstract http branch merged into the v3 branch and it will be relatively stable before our GA.\n. I'm adding support for an injectable cache to our v3. ~~Will send a PR tomorrow.~~\nHere's the PR that adds caching support to v3: https://github.com/aws/aws-sdk-php/pull/548\n. Support for cache stat's has now been added into our WIP v3: https://github.com/aws/aws-sdk-php/pull/548. Closing this in favor of this being a new feature in v3.\n. Can you check to ensure that your machine's clock is relatively in sync with the service's clock?\n. If this is caused by clock skew, then using something like NTP to keep your clock in sync would be the solution. If this is not clock skew related, then seeing some example code on how error was triggered may help.\n. Thanks. I've taken your suggested fix, merged it, added tests, and added a case for changing the method to GET if the request is a application/x-www-form-urlencoded POST request.\n. The SDK requires that other autoloaders will play nicely and not error out if they cannot find the script to autoload. This allows you to stack autoloaders so that multiple autoloaders can handle different libraries and requirements. If the framework you are using does not allow multiple autoloaders, then you will need to open an issue on their project. Taking a look at the \"huge\" framework, it looks like they now use PSR-4 autoloading, so perhaps you just need to update that dependency.\n. So the answer to this question is that you need to use the SDK with a library that works with other autoloaders. If the library you are using does not allow this, then you may need to disable their built-in autoload() function and instead add an autoloader based on PHP's spl_autoload_register(): http://php.net/manual/en/function.spl-autoload-register.php\n. Some stream filters require buffering the data as it passes through. In order for remaining characters in the buffer to pass through the stream, you need to flush the buffer. For example, this can be done by removing the stream filter with http://php.net/manual/en/function.stream-filter-remove.php. The example in the linked documentation shows how to store the ID of the filter and remove the filter from the stream.\n. Did you try calling fpassthru() again after removing the filter?\n. I tested it out and confirmed that my suggestion is correct: you will need to remove the filter and then call fpassthru again in order to actually consume and emit the data that was flushed from the base64 filter's buffer.\nBy removing the filter from the stream, you flush the buffer of the filter and place it into the buffer of the stream such that a subsequent call to read from the stream will read the previously buffered data.\n. Ah, true. Good idea.\n. I added more information about sigv4 only regions to the error message.\n. This parameter is not currently part of the API, so we can't do anything here client-side: http://docs.aws.amazon.com/AWSEC2/latest/APIReference/API_CreateSnapshot.html.\nWe'll pass the feedback along, but the best place to currently leave feedback for a specific service is on the service's forum: https://forums.aws.amazon.com/forum.jspa?forumID=30.\n. By counting the dots, I think it's segfaulting on test 90, which I think is Crc32ValidatingParserTest::testNothingWhenValidChecksum.\n. I'm not sure what you're asking. Can you elaborate?\n. Using glob with the Amazon S3 stream wrapper is not currently possible due to limitations of glob (and I believe the glob stream wrapper): http://php.net/manual/en/function.glob.php\nFrom the docs:\n\nNote: This function will not work on remote files as the file to be examined must be accessible via the server's filesystem.\n. Is your Yii autoloader not checking if the file exists before attempting to include it? If it is not doing this, then it will not work correctly with the SDK because the autoloader is not allowing other autoloaders to load the class.\n. I'm having a hard time conceptualizing the problem. Can you provide an example of the problem?\n. Ok, so you are using a custom command factory that add parameters to the command when it's created.\nFor example, a developer might alter a command based on the bucket and key to set a response-cache-control parameter. However, that will break unless the caller to getObjectUrl() is already signing the request, which we can't know or change from within the command object.\n\nWhy would your command factory or the getObjectUrl() method need to know about each other? If you are just adding custom parameters to the command (i.e., ResponseCacheControl), then those custom parameters should be signed when calling getObjectUrl().\n\nAdd a 'ObjectUrlExpires' key to the command arguments, defaulting to whatever $expires is.\n\nI don't think we'll do this because ObjectUrlExpires is not related to an API operation, but rather to a specific serialization format for a command (e.g., a signed URL). The expiration of a signed URL is provided using an argument to getObjectUrl().\n\nIn getObjectUrl(), check $args for any arguments that require a presigned request, and automatically set $expires to something safe like 30 days.\n\ngetObjectUrl does not always sign the URL it creates. It only signs the URL if you provide an $expires argument. Otherwise, it just serializes the request.\n. Ok, I see what you are saying now. If we start signing getObjectUrl calls in certain cases, then it doesn't adhere to the same interface that is described in the docblock:\nReturns the URL to an object identified by its bucket and key. If an expiration time is provided, the URL will be signed and set to expire at the provided time.\nIf we were to make a change, then I would rather throw an exception when no $expires is provided and an option is set that would require the URL to be signed.\n. With v3, we've actually separated the SDK from Guzzle such that Guzzle is just one of (and the default) HTTP adapters you can plug into the SDK. The extension mechanism in v3 is the middleware system (e.g., https://github.com/aws/aws-sdk-php/blob/v3/src/Middleware.php). I'll hopefully get to write some docs for the middleware system in the next few days.\n. The fix here will be documentation added to the getObjectUrl() method. I've added information that the URL may not be valid in all cases, and that if you need to use parameters that require a signature, then you need to either provide an expires argument or sign the URL in some other manner (e.g., passing it to a signature object). If we started throwing an exception now, then it could potentially be a breaking change for users who might be signing the URL outside of this method.\n. I just spun up an EC2 instance, and tried to reproduce this issue with Apache using the simplest possible test case.\nI ran the following commands and ensured that I installed the OPcache.\nyum install php56 php56-opcache -y\ncd /var/www/html\nwget https://github.com/aws/aws-sdk-php/releases/download/2.8.2/aws.phar\nservice httpd start\nPlace the following in test.php:\n``` php\n<?php\nrequire DIR . '/aws.phar';\n$client = Aws\\S3\\S3Client::factory([\n    'region'  => 'us-east-1',\n    'version' => 'latest'\n]);\necho $client->listBuckets();\n```\nNow hit the endpoint with cURL:\ncurl http://127.0.0.1/test.php\nI ran this several times and did not reproduce this issue.\nIt is interesting that we switched to a different build tool to generate our phars starting around version 2.6.16, which is when you see this problem start to occur. However, I'm not able to reproduce this issue and we haven't seen any other complaints about this in the last 7+ months. This makes me think that there might be some weird autoloading issue going on that is either an edge case that we did not cover or an application specific error.\n. The phar is not signed, so maybe your IDE just needs to be restarted?\n. @NobleUplift are you thinking that this might be an application specific error? Any further information we can act on?\n. Are you able to use any phar with the opcache, or is it just the AWS phar that doesn't work? Which distro are you using? Is the suhosin patch enabled?\n\nOn May 26, 2015, at 7:37 AM, Richard Baker notifications@github.com wrote:\nI have had the same issue with exact same error. From my limited testing it appears to be caused by Zend Opcache. Disabling the opcache (not recommended) or adding the aws.phar path to the list of ignored files fixes this.\n\u2014\nReply to this email directly or view it on GitHub.\n. I'm not able to reproduce this error. Let me show you what I did and see if you can tell me if I'm missing something, and if not, perhaps show me how I can reproduce the issue exactly as you have.\n1. I launched ubuntu-trusty-14.04-amd64-server-20150325 (ami-5189a661) (this is the Ubuntu image listed in the console).\n2. I ran apt-get update && apt-get install php5, giving me the following PHP installation:\n\nubuntu@ip-172-30-0-236:~$ php -v\n   PHP 5.5.9-1ubuntu4.9 (cli) (built: Apr 17 2015 11:44:57) \n   Copyright (c) 1997-2014 The PHP Group\n   Zend Engine v2.5.0, Copyright (c) 1998-2014 Zend Technologies\n   with Zend OPcache v7.0.3, Copyright (c) 1999-2014, by Zend Technologies\n3. I downloaded the latest phar into /var/www/html: wget https://github.com/aws/aws-sdk-php/releases/download/2.8.8/aws.phar\n4. I created the following test script in test.php:\nphp\n   <?php\n   require 'aws.phar';\n   $c = \\Aws\\Common\\Aws::factory();\n   echo 'OK!';\n5. I contacted the local Apache server using cURL: curl http://127.0.0.1/test.php (give \"OK!\"). I tried this multiple times and each time worked correctly.\n6. I created the following script in info.php to ensure that the OPCache is working:\nphp\n   <?php\n   var_dump(opcache_get_status());\n7. I accessed info.php using curl and got the following:\narray(8) {\n         [\"opcache_enabled\"]=>\n         bool(true)\n         [\"cache_full\"]=>\n         bool(false)\n         [\"restart_pending\"]=>\n         bool(false)\n         [\"restart_in_progress\"]=>\n         bool(false)\n         [\"memory_usage\"]=>\n         array(4) {\n           [\"used_memory\"]=>\n           int(5745440)\n           [\"free_memory\"]=>\n           int(61363424)\n           [\"wasted_memory\"]=>\n           int(0)\n           [\"current_wasted_percentage\"]=>\n           float(0)\n         }\n         [\"interned_strings_usage\"]=>\n         array(4) {\n           [\"buffer_size\"]=>\n           int(4194304)\n           [\"used_memory\"]=>\n           int(885056)\n           [\"free_memory\"]=>\n           int(3309248)\n           [\"number_of_strings\"]=>\n           int(8361)\n         }\n         [\"opcache_statistics\"]=>\n         array(13) {\n           [\"num_cached_scripts\"]=>\n           int(13)\n           [\"num_cached_keys\"]=>\n           int(15)\n           [\"max_cached_keys\"]=>\n           int(3907)\n           [\"hits\"]=>\n           int(25)\n           [\"start_time\"]=>\n           int(1432928309)\n           [\"last_restart_time\"]=>\n           int(0)\n           [\"oom_restarts\"]=>\n           int(0)\n           [\"hash_restarts\"]=>\n           int(0)\n           [\"manual_restarts\"]=>\n           int(0)\n           [\"misses\"]=>\n           int(13)\n           [\"blacklist_misses\"]=>\n           int(0)\n           [\"blacklist_miss_ratio\"]=>\n           float(0)\n           [\"opcache_hit_rate\"]=>\n           float(65.789473684211)\n         }\n         [\"scripts\"]=>\n         array(13) {\n           [\"phar:///var/www/html/aws.phar/Guzzle/Service/AbstractConfigLoader.php\"]=>\n           array(6) {\n             [\"full_path\"]=>\n             string(69) \"phar:///var/www/html/aws.phar/Guzzle/Service/AbstractConfigLoader.php\"\n             [\"hits\"]=>\n             int(2)\n             [\"memory_consumption\"]=>\n             int(22848)\n             [\"last_used\"]=>\n             string(24) \"Fri May 29 19:38:34 2015\"\n             [\"last_used_timestamp\"]=>\n             int(1432928314)\n             [\"timestamp\"]=>\n             int(1432859894)\n           }\n           [\"phar:///var/www/html/aws.phar/Aws/Common/Resources/aws-config.php\"]=>\n           array(6) {\n             [\"full_path\"]=>\n             string(65) \"phar:///var/www/html/aws.phar/Aws/Common/Resources/aws-config.php\"\n             [\"hits\"]=>\n             int(2)\n             [\"memory_consumption\"]=>\n             int(17384)\n             [\"last_used\"]=>\n             string(24) \"Fri May 29 19:38:34 2015\"\n             [\"last_used_timestamp\"]=>\n             int(1432928314)\n             [\"timestamp\"]=>\n             int(1432859894)\n           }\n           [\"phar:///var/www/html/aws.phar/Guzzle/Common/AbstractHasDispatcher.php\"]=>\n           array(6) {\n             [\"full_path\"]=>\n             string(69) \"phar:///var/www/html/aws.phar/Guzzle/Common/AbstractHasDispatcher.php\"\n             [\"hits\"]=>\n             int(2)\n             [\"memory_consumption\"]=>\n             int(6520)\n             [\"last_used\"]=>\n             string(24) \"Fri May 29 19:38:34 2015\"\n             [\"last_used_timestamp\"]=>\n             int(1432928314)\n             [\"timestamp\"]=>\n             int(1432859894)\n           }\n           [\"phar:///var/www/html/aws.phar/aws-autoloader.php\"]=>\n           array(6) {\n             [\"full_path\"]=>\n             string(48) \"phar:///var/www/html/aws.phar/aws-autoloader.php\"\n             [\"hits\"]=>\n             int(2)\n             [\"memory_consumption\"]=>\n             int(173944)\n             [\"last_used\"]=>\n             string(24) \"Fri May 29 19:38:34 2015\"\n             [\"last_used_timestamp\"]=>\n             int(1432928314)\n             [\"timestamp\"]=>\n             int(1432859894)\n           }\n           [\"phar:///var/www/html/aws.phar/Guzzle/Service/Builder/ServiceBuilderLoader.php\"]=>\n           array(6) {\n             [\"full_path\"]=>\n             string(77) \"phar:///var/www/html/aws.phar/Guzzle/Service/Builder/ServiceBuilderLoader.php\"\n             [\"hits\"]=>\n             int(2)\n             [\"memory_consumption\"]=>\n             int(10584)\n             [\"last_used\"]=>\n             string(24) \"Fri May 29 19:38:34 2015\"\n             [\"last_used_timestamp\"]=>\n             int(1432928314)\n             [\"timestamp\"]=>\n             int(1432859894)\n           }\n           [\"phar:///var/www/html/aws.phar/Guzzle/Service/ConfigLoaderInterface.php\"]=>\n           array(6) {\n             [\"full_path\"]=>\n             string(70) \"phar:///var/www/html/aws.phar/Guzzle/Service/ConfigLoaderInterface.php\"\n             [\"hits\"]=>\n             int(2)\n             [\"memory_consumption\"]=>\n             int(2936)\n             [\"last_used\"]=>\n             string(24) \"Fri May 29 19:38:34 2015\"\n             [\"last_used_timestamp\"]=>\n             int(1432928314)\n             [\"timestamp\"]=>\n             int(1432859894)\n           }\n           [\"phar:///var/www/html/aws.phar/Guzzle/Common/HasDispatcherInterface.php\"]=>\n           array(6) {\n             [\"full_path\"]=>\n             string(70) \"phar:///var/www/html/aws.phar/Guzzle/Common/HasDispatcherInterface.php\"\n             [\"hits\"]=>\n             int(2)\n             [\"memory_consumption\"]=>\n             int(5328)\n             [\"last_used\"]=>\n             string(24) \"Fri May 29 19:38:34 2015\"\n             [\"last_used_timestamp\"]=>\n             int(1432928314)\n             [\"timestamp\"]=>\n             int(1432859894)\n           }\n           [\"/var/www/html/aws.phar\"]=>\n           array(6) {\n             [\"full_path\"]=>\n             string(22) \"/var/www/html/aws.phar\"\n             [\"hits\"]=>\n             int(2)\n             [\"memory_consumption\"]=>\n             int(1024)\n             [\"last_used\"]=>\n             string(24) \"Fri May 29 19:38:34 2015\"\n             [\"last_used_timestamp\"]=>\n             int(1432928314)\n             [\"timestamp\"]=>\n             int(1432928039)\n           }\n           [\"phar:///var/www/html/aws.phar/Guzzle/Service/Builder/ServiceBuilderInterface.php\"]=>\n           array(6) {\n             [\"full_path\"]=>\n             string(80) \"phar:///var/www/html/aws.phar/Guzzle/Service/Builder/ServiceBuilderInterface.php\"\n             [\"hits\"]=>\n             int(2)\n             [\"memory_consumption\"]=>\n             int(4776)\n             [\"last_used\"]=>\n             string(24) \"Fri May 29 19:38:34 2015\"\n             [\"last_used_timestamp\"]=>\n             int(1432928314)\n             [\"timestamp\"]=>\n             int(1432859894)\n           }\n           [\"phar:///var/www/html/aws.phar/Guzzle/Service/Builder/ServiceBuilder.php\"]=>\n           array(6) {\n             [\"full_path\"]=>\n             string(71) \"phar:///var/www/html/aws.phar/Guzzle/Service/Builder/ServiceBuilder.php\"\n             [\"hits\"]=>\n             int(2)\n             [\"memory_consumption\"]=>\n             int(24280)\n             [\"last_used\"]=>\n             string(24) \"Fri May 29 19:38:34 2015\"\n             [\"last_used_timestamp\"]=>\n             int(1432928314)\n             [\"timestamp\"]=>\n             int(1432859894)\n           }\n           [\"/var/www/html/info.php\"]=>\n           array(6) {\n             [\"full_path\"]=>\n             string(22) \"/var/www/html/info.php\"\n             [\"hits\"]=>\n             int(1)\n             [\"memory_consumption\"]=>\n             int(880)\n             [\"last_used\"]=>\n             string(24) \"Fri May 29 19:45:21 2015\"\n             [\"last_used_timestamp\"]=>\n             int(1432928721)\n             [\"timestamp\"]=>\n             int(1432928386)\n           }\n           [\"/var/www/html/test.php\"]=>\n           array(6) {\n             [\"full_path\"]=>\n             string(22) \"/var/www/html/test.php\"\n             [\"hits\"]=>\n             int(2)\n             [\"memory_consumption\"]=>\n             int(1160)\n             [\"last_used\"]=>\n             string(24) \"Fri May 29 19:38:34 2015\"\n             [\"last_used_timestamp\"]=>\n             int(1432928314)\n             [\"timestamp\"]=>\n             int(1432928083)\n           }\n           [\"phar:///var/www/html/aws.phar/Aws/Common/Aws.php\"]=>\n           array(6) {\n             [\"full_path\"]=>\n             string(48) \"phar:///var/www/html/aws.phar/Aws/Common/Aws.php\"\n             [\"hits\"]=>\n             int(2)\n             [\"memory_consumption\"]=>\n             int(8680)\n             [\"last_used\"]=>\n             string(24) \"Fri May 29 19:38:34 2015\"\n             [\"last_used_timestamp\"]=>\n             int(1432928314)\n             [\"timestamp\"]=>\n             int(1432859894)\n           }\n         }\n       }\n8. I then upgraded PHP to PHP 5.6 using the following instructions: http://devdocs.magento.com/guides/v1.0/install-gde/prereq/php-ubuntu.html#instgde-prereq-php56-install-ubuntu\n9. I restarted apache: service apache2 restart.\n10. I contacted test.php using curl multiple times and it continued to work correctly.\n11. I contacted info.php and it outputs similar OPcache data.\nThis shows me that the phar we use is working correctly on Ubuntu using both PHP 5.5 and 5.6. What steps do I need to take to make this fail in a similar manner to what you are observing?\n. Thanks for sharing, but that's not a good solution as it disables the opcache.\nDid you see my walkthrough and how I was not able to reproduce the issue? Does walking through the steps I provided cause an issue for you? What about your application is different than the steps I provided?\n\nOn May 29, 2015, at 5:24 PM, Joel Kuntz notifications@github.com wrote:\nI had a similar problem and wanted to share to help any other googlers\nthe environment\nUbuntu 15.04\nPHP 5.6.4-4ubuntu6 (cli) (built: Apr 17 2015 15:47:51) \nServer version: Apache/2.4.10 (Ubuntu)\nServer built: Mar 9 2015 11:53:48\nthe error\nPHP Warning:  require(phar://aws.phar/aws-autoloader.php): failed to open stream: phar error: invalid url or non-existent phar \"phar://aws.phar/aws-autoloader.php&quot\nthe solution\nchmod 755 /path/to/aws.phar\nadd the following to /etc/php5/apache2/php.ini opcache.enable=0\nrestart apache service apache2 restart\n\u2014\nReply to this email directly or view it on GitHub.\n. I added an alias to the phar to see if it would help. I've linked to a built phar that has the alias. Could one of you test this phar out to see if it fixes the issue?\n\nhttps://s3.amazonaws.com/s3dowling/aws.phar\n. That's great. I'll wait on confirmation from one more reporter on this issue before merging the change and closing this issue. @Frozenfire92 / @NobleUplift \n. Are you sure that's the same issue? It looks like your system can't find the phar at all. Previously the error was related to the phar attempting to include another file in the phar.\n. @NobleUplift Yes, the phar is for version 3.x of the SDK. It looks like it's working as far as the phar is concerned though, so that's good news. The second issue looks like a coding issue. Can you show the code you used that triggered that error?\n. Now that https://github.com/aws/aws-sdk-php/pull/662 has been merged and two users have shown that it fixes the issue, I'll resolve this issue. We'll have new phars packaged up with an alias in the next release.\n. > They found so far 7 objects, out of ~1 million uploads, are truncated.\nIs there any information that you can share about the 7 files that you observed as truncated? What file size was uploaded and what was the expected size? Is there a common pattern regarding the sizes (e.g., they are all X size but were expected to be Y size)?\n\nThey use setConcurrency(300) and the default values for other parameters.\n\nThat's a very large number. I think you'll get better throughput by reducing this number significantly. At 300 concurrent requests, you're probably at 100% CPU and possibly saturating your network connection.\n. Closing based on conversation in #560 \n. Looks good. Are the test failures related?\n. LGTM\n. Is this good to merge? Is there something else that needs to be done?\n. Cool thanks. I think this can be added to the double-dispatch though by creating a new method called format_boolean that matches the other double dispatch methods.\n. This is a great change. Ship it.\n\nOn May 8, 2015, at 4:40 PM, Jeremy Lindblom notifications@github.com wrote:\nUpdated the SignatureInterface to change createPresignedUrl to presign. Changed S3's createPresignedUrl to createPresignedRequest.\nConsumers can get the presigned URL (and other information like headers) from the presigned request object. Formerly, only the URLs were returned, and getting the header values needed to actually make requests to the presigned URLs was not possible with custom middleware.\nYou can view, comment on, or merge this pull request online at:\nhttps://github.com/aws/aws-sdk-php/pull/569\nCommit Summary\nUpdated the SignatureInterface to change createPresignedUrl to presign. Changed S3's createPresignedUrl to createPresignedRequest.\nFile Changes\nM src/Ec2/CopySnapshotMiddleware.php (4)\nM src/S3/S3Client.php (14)\nM src/Signature/AnonymousSignature.php (4)\nM src/Signature/S3Signature.php     (4)\nM src/Signature/SignatureInterface.php (8)\nM src/Signature/SignatureV2.php     (2)\nM src/Signature/SignatureV4.php     (4)\nM tests/S3/S3ClientTest.php (6)\nM tests/Signature/AnonymousSignatureTest.php (4)\nM tests/Signature/S3SignatureTest.php (12)\nM tests/Signature/S3SignatureV4Test.php (20)\nM tests/Signature/SignatureV2Test.php (2)\nM tests/Signature/SignatureV4Test.php (4)\nPatch Links:\nhttps://github.com/aws/aws-sdk-php/pull/569.patch\nhttps://github.com/aws/aws-sdk-php/pull/569.diff\n\u2014\nReply to this email directly or view it on GitHub.\n. I've fixed the CompleteMultipartUpload issue with the switch statement.\n. Is this class being required but is not present on disk? I assume that's the problem, and the cause would be that you are using an additional autoloader that is not properly allowing other autoloaders a chance to autoload files that it does not know how to autoload. In this case, your autoloader is likely requiring classes rather than checking fist if they exist. We use class_exists to determine if a special case exception exists for a service. class_exists triggers an autoloader to check if a file exists, which should normally not cause a failure. The fix to this problem is to update your autoloaders to first check if a file exists rather than to just require it. This will allows your autoloader to work correctly with class_exists and other autoloaders.\n\nI'll go ahead and close this issue as we haven't header back from you, but do let us know if I've mis-diagnosed the issue here.\n. LGTM\n. :ship: \n. Can you provide a snippet of code that we can use to reproduce this issue?\n. Ah, yes that makes sense. The SDK uses JSON for DynamoDB. The json_encode function requires that the data provided is UTF-8 encoded, so it makes sense to UTF-8 encode strings when necessary.\n. :+1: \n. Can you provide the output of composer.phar show -i? It looks like you're using a really old version of SDK v3.\n. @GrahamCampbell Do you know how to install the SDK v3 using the latest commit on GitHub? using 3.*@dev is always pulling in the 3.0-beta.1 tag.\n@benmadin In the meantime, you will need to set \"minimum-stability\": \"dev\" in your composer.json. That said, we are shipping a stable v3 release next week.\n. I'm going to close this as there is a workaround, and we're shipping a stable release next week.\nSorry for the inconvenience.\n. I may have cleared up other issues earlier today. I'm tagging the psr7 package tomorrow which may also make the min stability setting unnecessary.\n\nOn May 18, 2015, at 10:15 PM, benmadin notifications@github.com wrote:\nThanks Michael,\nConfusing... If I set it on the entire composer, I have three other packages that won't load. but if I put the git commit on the sdk only, it won't pull it.\nI'll try a git pull instead.\ncheers\nBen\n\u2014\nReply to this email directly or view it on GitHub.\n. Yeah, this is a known issue with a 32-bit build of PHP (which is the only build available on Windows, and  I'm assuming you're using Windows based on the mention of NTFS). I agree that trying to work around this using something like the functions you mentioned would be invasive and most likely a massive loss in performance when calculating the size of a large file.\n\nThe Windows PHP site mentions this issue in the left column: http://windows.php.net/.\n\nx86_64 Builds\nThe x64 builds of PHP for Windows should be considered experimental, and do not yet provide 64-bit\ninteger or large file support. Please see this post for work ongoing to improve these builds.\n\nIt looks like even if you tried using a 64-bit version of PHP on Windows, it still would not support \"large\" files.\nThere really isn't anything we can do about this. This issue is documented in the FAQ: http://docs.aws.amazon.com/aws-sdk-php/v2/guide/faq.html#why-can-t-i-upload-or-download-files-greater-than-2gb\n. As this is a known issue in PHP and is documented as a limitation in the SDK documentation, I'll go ahead and close this issue.\n. I'm not able to reproduce this. Here's my code:\n``` php\n<?php\nrequire 'vendor/autoload.php';\n$s3 = Aws\\S3\\S3Client::factory([\n    'version' => 'latest',\n    'region'  => 'us-east-1'\n]);\n$s3->getEventDispatcher()->addSubscriber(\n    \\Guzzle\\Plugin\\Log\\LogPlugin::getDebugPlugin()\n);\n$bucket = 'change-me';\n$keyname = \"2b/e1/3a/bf/33c663ff370496f2965c0d702a023e59\";\n$objects = array(\n    array('Key' => $keyname.\".m4a\"),\n    array('Key' => $keyname.\".ogg\"),\n    array('Key' => $keyname.\".mp3\"),\n);\necho $result = $s3->deleteObjects(array(\n    'Bucket'  => $bucket,\n    'Objects' => $objects\n));\n```\nThis sends and receives the following (notice that everything is correct here):\n```\nRequest:\nPOST /?delete HTTP/1.1\nHost: .s3.amazonaws.com\nUser-Agent: aws-sdk-php2/2.8.3 Guzzle/3.9.3 curl/7.39.0 PHP/5.6.7\nContent-Type: application/xml\nContent-MD5: o4oidt8MeCU+x/CxqkTNZw==\nDate: Mon, 18 May 2015 17:48:18 +0000\nAuthorization: AWS *:/*=\nContent-Length: 316\nxml version=\"1.0\"?\n2b/e1/3a/bf/33c663ff370496f2965c0d702a023e59.m4a2b/e1/3a/bf/33c663ff370496f2965c0d702a023e59.ogg2b/e1/3a/bf/33c663ff370496f2965c0d702a023e59.mp3\nResponse:\nHTTP/1.1 200 OK\nx-amz-id-2: Y/1zHiB014R0u/CW5RJtL07oAWZAlZ3htA0DdjXmBd++FJC8fpYjjlMt0gZ92mMK\nx-amz-request-id: 37B90389391131EC\nDate: Mon, 18 May 2015 17:48:12 GMT\nConnection: close\nContent-Type: application/xml\nTransfer-Encoding: chunked\nServer: AmazonS3\nxml version=\"1.0\" encoding=\"UTF-8\"?\n2b/e1/3a/bf/33c663ff370496f2965c0d702a023e59.m4a2b/e1/3a/bf/33c663ff370496f2965c0d702a023e59.ogg2b/e1/3a/bf/33c663ff370496f2965c0d702a023e59.mp3\n```\n1. Can you attach the debug log subscriber I've attached and provide the output?\n2. Which version of the SDK are you using?\n3. Which operating system are you using?\n. Looks like this is an issue similar to one we've had before with Lambda in that the \"ZipFile\" needs a location of \"body\". We'll take a look at our service description generator to see why this is happening to fix this at the source.\n. I like it, but can you tell me how `$bucketEndpoint` is implemented?\n. Ah ok. If that's all it does, what would you think about renaming it to `BuckeEndpointMiddleware`? Other than that, :ship:\n. :ship:\n. You need to set a region when creating the client. If you do this, then the SDK will automatically use the correct signature version:\n\nphp\n$client = S3Client::factory(array(\n    'key'    => 'key',\n    'secret' => 'secret',\n    'region' => 'eu-central-1'\n));\nSetting the region on the client after it is created will not change the signature method used by the client.\n. Amazon S3 regions have recently become \"region aware\", meaning you actually need to know the region you are contacting before you send requests. This limitation was introduced because some regions require signature version 4. In the past, you could send a request a \"s3.amazonaws.com\" and it would mostly \"just work\" thanks to CNAMEs or redirects taking you to the correct regionalized bucket. However, with the addition of sigv4 only regions (e.g., \"eu-central-1\"), it is now not possible to rely on this behavior as sigv4 regions must be signed explicitly for the region you are contacting (for canonicalizing the signature). \nThe good news is that you can call getBucketLocation against any region endpoint and it will give you the actual region a bucket is stored in. You then need to use a client that has been configured to use that region when sending requests to those buckets. There are various ways you could achieve this; one thing that comes to mind is decorating S3Client, getting the region of a bucket before sending requests, using a client that is scoped for the given region, and returning the result. You could cache previously created clients to avoid having to create clients over and over.\n. That's a perfectly valid way to do this. My idea of decorating S3Client would wrap this up a bit more nicely and allow you to reuse previously created client instances, but it's essentially the same strategy.\n. No problem.\n. From what I understand, session handler implementations are not supposed to throw exceptions on error, but rather return a boolean true or false value. We'll look into seeing if that's a hard requirement, and if not, see if throwing an exception here is possible. If it emits a warning and throws an exception (due to internal PHP issues), then that might still be a better experience than currently swallowing the exception.\n. Thanks for opening this issue. We will add this to our backlog for both v2 and v3.\n. > The function takes a URI, so we can't pass in any stream context options. The extension code is assuming that seeking is available, but the StreamWrapper docs explicitly state that seeking is not required to be supported.\nThere's nothing we can do here in the SDK, but you could use the default stream options features from PHP to work around this: http://php.net/manual/en/function.stream-context-set-default.php\n\nIf we force seeking to be enabled in the stream wrapper, getimagesize() still fails because it seeks without first reading the stream. CachingEntityBody documents that it can only seek on read bytes, but this is an implementation detail we can't expect calling code to be aware of.\n\nThe only solution I can think of for this would be to load the entire response body into memory if \"seekable\" is set to true. That would be a breaking change IMO for the v2 of the SDK, but it could be a change that we make before shipping the v3 that is going out next week.\n. I've pushed a fix for CachingStream that allows forward seeking. This should take care of your second issue when working with V3. As the other issue is something that can only be fixed in PHP itself, I'll close this issue. See https://github.com/guzzle/psr7/pull/23\n. This could be related to this issue in cURL that I believe was fixed in 7.28: https://github.com/bagder/curl/commit/ee3551e45e60856eb0b779aa6cd34d77f16208a5\nHave you tried updating your version of cURL during the travis setup to be >= 7.28? I noticed the output in your travis build was 7.22: https://travis-ci.org/shadiakiki1986/just-want-to-pass-dynamodb-travisci/builds/63951116#L316\n\nupgrading curl from 1.22 to 1.36 using the package whitelist, but I think the fact that the VMs run Ubuntu 12.04 left curl at 1.22\n\nIs this some kind of version indirection? cURL is at 7.42.1.\n. I wasn't sure based on your last response, but just to be clear: have you tried updating cURL to >= 7.28 on Travis? The reason I ask is because it looks like the commit I linked to specifically fixes the problem you are encountering with gnutls and cURL.\n\nI really think that this is a matter of php compiled with gnutls (doesn't work) versus openssl (works).\n\nI think it will work if you upgrade to cURL >= 7.28. I'll try this out myself as well.\n. Fixed in the caching stream. See https://github.com/guzzle/psr7/pull/23\n. :birthday: :balloon: :tada: :haircut: \n. I appreciate that. Unfortunately we need to make this change for now.\nI was hoping you could inform me on this: do you know if the syntax of | and || are equivalent?\n. Kind of, but it's really fragile. For example, what if we only wanted 6.1 and up because of a new required feature? Or 6.0.1 and up for a bug fix?\n\nOn May 27, 2015, at 6:05 PM, Jeremy Lindblom notifications@github.com wrote:\nCould we do >=5.3,<7?\n\u2014\nReply to this email directly or view it on GitHub.\n. As per @GrahamCampbell's suggestion, I've update the version constraint to >=5.3|~6.0.1|~6.1\n. Thanks for the issue report. The $transfer object just needs to be renamed to $manager. We'll make the change.\n. Which version of the SDK are you using? Have you tried percent encoding special characters like ' ' with '%20'?\n. Which version of the SDK are you using? Can you provide the equivalent CLI command you used that worked?\nOn May 29, 2015, at 5:39 PM, PSiAU notifications@github.com wrote:\nWhen attempting to launch an instance in a VPC with a public IP address, the following error is thrown:\n\"Network interfaces and an instance-level security groups may not be specified on the same request\"\nThe code i'm launching it with:\n$response = $ec2Client->runInstances([\n    'ImageId' => 'ami-fd9cecc7',\n    'MinCount' => '1',\n    'MaxCount' => '1',\n    'InstanceType' => 'c4.xlarge',\n    'KeyName' => 'mykey',\n    'NetworkInterfaces' => [[\n        'DeviceIndex' => 0,\n        'SubnetId' => 'subnet-xxxxxxxx',\n        'AssociatePublicIpAddress' => true\n    ]],\n    'SecurityGroupIds' => array('sg-xxxxxxxx','sg-xxxxxxxx'),\n]);\nI'm able to do the same through the AWS CLI with no problems.\n\u2014\nReply to this email directly or view it on GitHub.\n. :ship: \n. When executing commands with the SDK (v3), you can use the debug option to get a much more detailed view into what's going on. To enable debug mode, pass the debug option in to the client's constructor:\n\nphp\n$s3 = new Aws\\S3\\S3Client([\n    'region' => 'my-region',\n    'version' => '2006-03-01',\n    'debug' => true\n]);\nCan you please run your test with debug mode enabled and post the debug output?\n. I made several enhancements to the SDK based on this issue. Here's the related pull request: https://github.com/aws/aws-sdk-php/pull/605.\nTo solve your immediate issue, you will need to create a seekable stream with the PHP stream wrapper. This can be done through a stream context that sets seekable to true.\nHere's a working example of how this would work (I also forced the use of the PHP stream wrapper as well in case you are using the stream wrapper as your client. Note that you could omit the http_handler option altogether and this will still work).\n``` php\n<?php\nrequire 'vendor/autoload.php';\n$streamHandler = new \\GuzzleHttp\\Handler\\StreamHandler();\n$s3 = new \\Aws\\S3\\S3Client([\n    'region'  => 'us-east-1',\n    'version' => 'latest',\n    'debug'   => true,\n    'http_handler' => $streamHandler\n]);\n$s3->registerStreamWrapper();\n$context = stream_context_create(['s3' => ['seekable' => true]]);\n$resource = fopen('s3://t1234/test', 'rb', null, $context);\nif (!$resource) {\n    die('Could not open s3 resource');\n}\n$s3->putObject([\n    'Bucket' => 't1234',\n    'Key'    => 'stream-test',\n    'Body'   => $resource\n]);\n```\n. :shipit: \n. Have you tried updating your copy of the SDK to version 2.8?\n. Ah ok. Thanks for letting us know.\n. Fixed in https://github.com/aws/aws-sdk-php/issues/657\n. It's only throwing because PHPUnit is turning warnings into exceptions. It just emits a warning in practice and returns false.\n\nOn Jun 25, 2015, at 11:47 PM, Ulrich Eckhardt notifications@github.com wrote:\nI just looked over the changes and the test surprises me. file_get_contents() there seems to be expected to throw an exception, is that right? Reason I ask is that that's a breaking interface change when compared to PHP's implementation, which returns false in case the files isn't present or not readable.\n\u2014\nReply to this email directly or view it on GitHub.\n. The example you gave shows the same path for both the input and output. Can you provide an actual file path input that failed to load, what the compiler tried to load, and what it should have loaded? (This should all show up in a stack trace or warning)\nOn Jun 4, 2015, at 5:00 AM, smagnaschi notifications@github.com wrote:\nHello all,\nI think I've found a problem in the normalize function of JsonCompiler.php.\nWhenever the path given to normalize is in the form of '\\xxx\\sd' for example in a VirtualBox Windows Server + Apache + network shared folder, the normalize functions spits out a resolved pattern in the form of '\\xxx\\sd' which is wrong and this raises an InvalidArgumentException since PHP cannot locate the right files (manifest.json / endpoints.json) and so on.\n\u2014\nReply to this email directly or view it on GitHub.\n. Ah I see thanks. This seems to be a very specific case -- I wonder if PHP's built-in realpath would support this type of path. I'll reopen the issue, and we'll see if there's a solution.\n. I've pushed changes that will eventually make the travis builds succeed, but it seems that Travis is not receiving any of my pushes to trigger new builds. If you review and give this a :shipit:, I'll merge.\n. Glad it's working now.\n. Web services that the SDK connects to are also versioned, and these APIs may introduce breaking changes between versions. V2 automatically utilized the latest version, which has the negative effect of potential breakage in your application when taking a minor update of the SDK.\n\nV3 of the SDK is more resilient against this type of breakage by explicitly requiring a service specific API version number when creating a client. We do allow you to set the \"version\" number to \"latest\", however, keep in mind that you may inadvertently begin to use a newer version of an API that may cause your application to break at runtime. Using \"latest\" as the version is a way to opt into convenience of not choosing a version number, but it come with additional risk.\n\nOn Jun 8, 2015, at 8:19 PM, NoahTwine notifications@github.com wrote:\nWe are using the AWS PHP SDK with composer. Previously, with version 2 we declared the version in the composer.json file. We then did not have to declare it when we connected to the API in our code.\nAfter upgrading to version 3 we receive an error saying that we must declare a version number.\nThis means we have to add in the versioning at dozens of places within our code.\nHow do we go back to the version 2 method?\n\u2014\nReply to this email directly or view it on GitHub.\n. Credential loading should work the same way. Can you confirm that your user has permission to access the file, that the HOME directory is exported to this user, and that credentials file is valid? If you are running as the root user, then you may need to manually export a HOME directory as this is sometimes not declared.\nOn Jun 8, 2015, at 8:48 PM, NoahTwine notifications@github.com wrote:\nThanks for the super-quick answer! So I guess we will have to change every instance were we call the SDK.\nOne other question: After declaring the version number at an instance of the SDK, we get another error saying that the AWS credentials under .aws/credentials could not be accessed.\nSpecifically:\nUncaught exception 'Aws\\Exception\\CredentialsException' with message 'Error retrieving credentials from the instance profile metadata server. (cURL error 28: Connection timed out after 1005 milliseconds (see http://curl.haxx.se/libcurl/c/libcurl-errors.html))' in /Users/MYUSER/DIRECTORY-OF-PROJECT/lib/vendor/aws/aws-sdk-php/src/Credentials/InstanceProfileProvider.php\nCurrently these credentials are in the root under ~.aws/credentials. Does version 3 of the SDK expect these credentials to have a different level or permissions or be placed in a different location?\nThat would be help to know. Thanks.\n\u2014\nReply to this email directly or view it on GitHub.\n. Do you have any more information on this issue or is it ok to close?\n. Version 3 of the SDK uses \"new\". V3 still supports the factory method for backwards compatibility, but using \"new\" is now recommended.\nOn Jun 9, 2015, at 1:19 AM, Jonny Schmid notifications@github.com wrote:\nYou can view, comment on, or merge this pull request online at:\nhttps://github.com/aws/aws-sdk-php/pull/618\nCommit Summary\nFixes code example in README\nFile Changes\nM README.md (2)\nPatch Links:\nhttps://github.com/aws/aws-sdk-php/pull/618.patch\nhttps://github.com/aws/aws-sdk-php/pull/618.diff\n\u2014\nReply to this email directly or view it on GitHub.\n. No worries. The Github releases layout definitely needs some work around how they present repos with multiple major versions.\nOn Jun 9, 2015, at 8:58 AM, Jonny Schmid notifications@github.com wrote:\nMy bad. Got going with 2.8.9 as it's tagged \"Latest release\" and I didn't check further.\n\u2014\nReply to this email directly or view it on GitHub.\n. I think there are some really good questions in this thread. I think I see three separate questions:\n1. You're observing a sort of batching behavior and would like help.\n2. You want to know how to send async requests with the SDK that don't require a call to wait.\n3. You want to use the SDK with React.\n\nSo...\n1. You're observing a sort of batching behavior and would like help\nYour example would be blocking between calls because you are not queuing up multiple requests-- you're only queuing more requests after a request completes. Think of the CommandPool and EachPromise abstractions as a pipeline to transfer requests (or commands). It needs an iterator that yields promises and will ensure that N number of promises are in flight at any given time.\nLet's make some modifications to your example to make it send the requests concurrently...\n``` php\n<?php\nrequire 'vendor/autoload.php';\nuse Aws\\Sdk;\nuse GuzzleHttp\\Promise;\nuse GuzzleHttp\\Handler\\CurlMultiHandler;\nuse GuzzleHttp\\HandlerStack;\n$sdk = new Sdk(['region' => 'us-east-1', 'version' => 'latest']);\n$s3Client = $sdk->createS3();\n$bucket = 'my-bucket';\n$promiseGenerator = function ($total) use ($s3Client, $bucket) {\n    for ($i = 0; $i < $total; $i++) {\n        yield $s3Client->getObjectAsync([\n            'Key'    => sprintf('test%s.text', $i),\n            'Bucket' => $bucket,\n        ]);\n    }\n};\n$fulfilled = function($result) {\n    echo 'Got result: ' . var_export($result->toArray(), true) . \"\\n\\n\";\n};\n$rejected = function($reason) {\n    echo 'Rejected: ' . $reason . \"\\n\\n\";\n};\n// Create the generator that yields 1000 total promises.\n$promises = $promiseGenerator(1000);\n// Create a promise that sends 50 promises concurrently by reading from\n// a queue of promises.\n$each = Promise\\each_limit($promises, 50, $fulfilled, $rejected);\n// Trigger a wait. Note that if you use an event loop then this is not\n// necessary.\n$each->wait();\n```\nNote that there is also a CommandPool and several examples on sending concurrent requests: http://docs.aws.amazon.com/aws-sdk-php/v3/guide/guide/commands.html#commandpool\n2. You want to know how to send async requests with the SDK that don't require a call to wait.\nThe SDK is designed to be able to work with any sort of HTTP client. Some clients might use an event loop that you can tick externally, while others might require you to call wait.\nThe SDK will use cURL or the PHP stream wrapper by default if you do not configure a custom HTTP handler for the SDK. This is accomplished by using Guzzle by default. However, keep in mind that you can use any HTTP client with the SDK (more on that later).\nIf you are using the PHP stream wrapper, then there's no way to send the requests other than to call wait. This is because the PHP stream does not allow concurrent requests.\nWhen using cURL, you would need to tick the cURL loop in order to asynchronously progress the transfers. Most non-blocking event loops require that they are ticked to progress the transfers. Using a cURL handler with the SDK is no different. Here's how you could use a Guzzle handler that is coupled to cURL to manually tick the cURL event loop:\n``` php\n<?php\nrequire 'vendor/autoload.php';\nuse Aws\\Sdk;\nuse GuzzleHttp\\Promise;\nuse GuzzleHttp\\Handler\\CurlMultiHandler;\nuse GuzzleHttp\\HandlerStack;\n$curl = new CurlMultiHandler();\n$handler = GuzzleHttp\\HandlerStack::create($curl);\n$sdk = new Sdk([\n    'http_handler' => $handler,\n    'region' => 'us-west-2',\n    'version' => 'latest',\n]);\n$client = $sdk->createS3();\n$p1 = $client->listBucketsAsync()->then(function () { echo '-done 1-'; });\n$p2 = $client->listBucketsAsync()->then(function () { echo '-done 2-'; });\n$aggregate = Promise\\all([$p1, $p2]);\n// Tick the curl loop manually.\nwhile (!Promise\\is_settled($aggregate)) {\n    $curl->tick();\n}\n```\nIf you write HTTP handlers for other clients that use an event loop that is ticked automatically (because you are calling run or something on an event loop), then manually ticking an event loop or calling wait is unnecessary.\nHere are a couple examples of creating custom HTTP handlers for the SDK: https://github.com/aws/aws-sdk-php/tree/master/src/Handler. You could create a custom handler to bind the SDK to an event loop of your choice. Here is more information on SDK handlers: http://docs.aws.amazon.com/aws-sdk-php/v3/guide/guide/handlers-and-middleware.html#creating-custom-handlers\n3. You want to use the SDK with React.\n\nI've also tried adding them to the guzzle promise queue and calling run on a periodic timer, as per Guzzle's docs, but that also seems to be blocking, which kind of defeats the whole purpose!\n\nYes, adding a Guzzle client that uses cURL into a React event loop would be blocking. This is because you are using two different event loops that do not cooperate with one another. You will need to use a React HTTP handler with Guzzle in order to send non-blocking requests when injecting Guzzle into the React event loop. There is a promising start to a Guzzle handler here: https://github.com/WyriHaximus/react-guzzle-psr7 (@WyriHaximus is doing a fantastic job).\nWe haven't checked to see if this handler is in a state that it will work with the SDK, but eventually the goal is that it will. In order to configure Guzzle to use this adapter and then to configure the SDK to use a specific Guzzle client, you would essentially do what I showed in the above cURL example, but instead you would use the React handler instead of cURL.\n. Oh, interesting. Can you show the code you used to determine inflight requests vs inflight promises?\n\nOn Jun 12, 2015, at 2:40 AM, Milos Colakovic notifications@github.com wrote:\nHey @mtdowling, thanks for the great explanation!\nThere is one unexpected behavior that I noticed in the first case - the promises are resolved only after a whole batch finishes. There is indeed an N number of promises in flight; but not requests. I'll try to illustrate with Promise\\each_limit($promises, 3, $success, $fail) (with [==] representing request duration):\nPipe 1: [===]            $success() [==============] $success() [==...\nPipe 2: [=====]          $success() [======]         $success() [==...\nPipe 3: [==============] $success() [========]       $success() [==...\nWhat I expected to get is:\nPipe 1: [===] $success() [==============] $success() [==...\nPipe 2: [=====] $success() [======] $success() [==...\nPipe 3: [==============] $success() [========] $success() [==...\n\u2014\nReply to this email directly or view it on GitHub.\n. Glad to hear that fixed it. Guzzle 5 is architected quite differently and would probably require non-trivial changes to make it work identically to v6. I don't think there's a big need to update v5 to match v6 here considering v6 is available. We'll tag the related libraries and make sure they get pulled into the next release.\n\n@adamlc awesome! We'll try to do more testing and make sure everything works as expected. As soon as we are sure, we'll start promoting that integration point.\n. LGTM\n. You should definitely not create a new client in each iteration of the loop. Move client creation out of the loop for performance and to mitigate and circular references that would only be cleaned up from a periodic call to gc_collect_cycles.\n. Do the integration tests all pass?\nCan you add an integration for this for S3 and for Cognito?\nAfter those are added: :ship: \n. Other than the test failure: looks good\n. Looks good, but should this replace the old test or add a new test?\n\nOn Jun 23, 2015, at 5:03 PM, Jeremy Lindblom notifications@github.com wrote:\nFixes how empty lists are serialized in some cases.\n/cc @mtdowling\nYou can view, comment on, or merge this pull request online at:\nhttps://github.com/aws/aws-sdk-php/pull/647\nCommit Summary\nFixes #479. Affects empty list serialization for specific ELB and CFN operations.\nFile Changes\nM src/Aws/Common/Command/AwsQueryVisitor.php (3)\nM tests/Aws/Tests/Common/Command/AwsQueryVisitorTest.php (3)\nPatch Links:\nhttps://github.com/aws/aws-sdk-php/pull/647.patch\nhttps://github.com/aws/aws-sdk-php/pull/647.diff\n\u2014\nReply to this email directly or view it on GitHub.\n. Ok. Maybe add another field with a sentAs that is consumed by the conditional if (substr($prefix, -7) === '.member') {? Just to prevent regressions. Other than that: :ship: \n. The failing integration test now passes\n. Minor changes needed in the compliance test, but otherwise :ship: \n. Trying to see if this addresses #563 \n. Can you provide motivation for this? Does this significantly increase perf? What about memory utilization for large data structures (like recursive DynamoDB)?\n. Awesome. LGTM.\n. :ship:\n. :ship:\n. We utilize Composer's autoload/files feature to automatically require the necessary PHP files when the Composer autoloader is required: https://github.com/aws/aws-sdk-php/blob/master/composer.json#L43. If Composer is working correctly and you are including the autoloader correctly, then it should work normally and include the files. If it is not working, then there may be something weird going on, possibly Composer or environment specific.\n\nHere are a few questions I have:\n1. What OS are you running?\n2. Are you running any \"global\" composer installs of anything?\n3. Can you show us the code you are using to require the composer autoloader?\n4. Can you try setting up a basic test to see if you can create an empty project that uses the SDK?\n. Can you please provide more information and documentation about this PR?\nWe have our own cache interface, so I think this should be updated to use it instead of Doctrine: https://github.com/aws/aws-sdk-php/blob/master/src/CacheInterface.php\nI'm not sure that we need a hard dependency on Doctrine. In fact, I'd rather we just have a single implementation that writes credentials to disk instead of using Doctrine at all. If someone wants Doctrine cache integration, they would just need to write an adapter.\n. > I did try downgrading to the 2.8 version of the api with composer with no success.\nThis tells me that it's unlikely that this is an issue with the PHP SDK.\nThe only thing I can think of off the top of my head is that you could monitor the memory usage of the process to see if it's climbing. You could also step through using http://phpdbg.com/ to understand why it is segfaulting.\nWe haven't heard from you, so I'll go ahead and close this issue.\n. shipit\n. I'm ok with this change. It's what Doctrine/cache does when creating cache directories. https://github.com/doctrine/cache/blob/master/lib/Doctrine/Common/Cache/FileCache.php#L169-L173\n. The intent of the \"bucket_endpoint\" option and BucketEndpointMiddleware is to allow you to create a client that is bound to a specific bucket endpoint. The only intended usecase for this option and middleware is to create presigned URLs for a CNAME bucket. You need to create a client per bucket endpoint. You do not need to use CNAME urls for normal API requests.\n\nIf the intent of \"BucketEndpointMiddleware\" is to convert from path-style URLs to virtual-hosted style URLs, and for the conversion to happen via configuration in v3 instead of automatically as before, I'm happy to create a pull request.\n\nNo that is not the intent. Because we require a region and use sig-v4, we do not use virtual hosted buckets in v3. The reason we used them in v2 was to allow redirects to work when using the old \"s3\" signer.\n. You need to ensure that the handler you use with the SDK is throwing an exception when a non-200/300 response is received. This can be done when creating the Guzzle handler.\n``` php\nuse Aws\\Handler\\GuzzleV6\\GuzzleHandler;\nuse GuzzleHttp\\HandlerStack;\nuse GuzzleHttp\\Handler\\StreamHandler;\n$handler = HandlerStack::create(new StreamHandler);\n$wrappedHandler = GuzzleHandler($handler);\n$client = new \\Aws\\S3\\Client(['http_handler' => $wrappedHandler]);\n```\n. :ship: \n. LGTM\n. :shipit: \n. > Though, the version of php 7 travis use isn't the beta release, so it still might break since it's just built from the master...\nYeah, that is a bit of a risk. It doesn't matter that much though as we don't have any process that hangs off of Travis builds. I think this is fine to merge.\n. :ship: \n. LGTM\n. :ship:\n. This needs to be documented before merging, but otherwise LGTM\n. I think that we could update the MultipartUploader to accept Bucket/bucket, Key/key, ACL/acl.\nMetadata and other options are meant to be applied using the callback configuration options. You can observe operations before they are sent and mutate them as needed. For example, before_initiate is a function that is called with the CreateMultipartUpload command that is about to be executed.\nphp\n$uploader = new MultipartUploader($s3Client, $source, [\n    'bucket' => 'your-bucket',\n    'key'    => 'my-file.zip',\n    'before_initiate' => function (Aws\\Command $command) {\n        $command['Metadata']['xyz'] = '123';\n        echo 'Initiating';\n    }\n]);\nNote: when providing a state that is already initiated, the before_initiate callback is not invoked.\nThese callbacks are used to modify specific operations sent by a multipart uploader because different options can be applied at different steps in the upload process (i.e., CreateMultipartUpload, UploadPart, CompleteMultipartUpload). We also wanted users to be able to observe and modify operations before they are executed, which also allows them to implement custom things like progress bars (when the size of the upload is known). Because callbacks allow custom parameters per/operation, allows you to observe the state of an upload as it progresses, and you can provide different parameters per operation, we decided to make the API of customizing uploads be exposed through the callbacks. Note that we made acl an abstracted top-level parameter (an intentional inconsistency with the rest of the parameters) because it's extremely important that users always know what ACL they are applying to their objects.\nI think we could add more content to the multipart uploader user guide to make this more clear.\n\nThere's some oddness around Content-Type, too, where it looks like it's hard-coding it based on filename extension\n\nThe ContentType parameter is set by default on multipart uploads using the same logic that is used in other parts of the SDK (this is pretty typical in HTTP clients and high level abstractions). You can modify what content-type is applied using the before_initiate callback.\n\nAre there plans to bring MultiPartUploader to parity with S3Client::putObject()'s arguments?\n\nWhile it supports them using callbacks, you might be interested in a more abstracted uploader that exposes a consistent API between PutObject and MultipartUploader: https://github.com/aws/aws-sdk-php/blob/master/src/S3/S3Client.php#L278.  This helper function is also great because you can call it indifferently with different sized files, and the abstraction will choose whether or not to use a single put object or a multipart upload based on the size (all of which are configurable thresholds).\n. Great :smile: \nJust so we can track it, here are the action items we'll take away from this issue:\n1. Make the bucket, key, and acl parameters take both API parameter form and the existing lowercase form.\n2. Update the multipart upload documentation to show how to customize operations and how to use the callback functions.\n. One minor suggestion, but LGTM\n. :ship: \n. LGTM\n. `` diff\ndiff --git a/src/functions.php b/src/functions.php\nindex 0d3dd9d..234c12a 100644\n--- a/src/functions.php\n+++ b/src/functions.php\n@@ -132,14 +132,7 @@ function or_chain()\n  * Loads a compiled JSON file from a PHP file.\n  *\n  * If the JSON file has not been cached to disk as a PHP file, it will be loaded\n- * from the JSON source file, written to disk as a PHP file, and returned. This\n- * allows subsequent access of the JSON file to be read from a compiled PHP\n- * script which is added to PHP's in-memory opcode cache.\n- *\n- * The default directory used to save compiled PHP scripts is the \"aws-cache\"\n- * sub-directory of PHP temp directory (return value of sys_get_temp_dir()).\n- * You can customize where the cache files are stored by specifying the\n- *AWS_PHP_CACHE_DIR` environment variable.\n+ * from the JSON source file and returned.\n  \n  * @param string $path Path to the JSON file on disk\n  \n@@ -150,6 +143,10 @@ function load_compiled_json($path)\n {\n     static $loader;\n\nif ($compiled = @include(\"$path.php\")) {\nreturn $compiled;\n}\n+\n```\n\nAny reason to not use flle_exists here instead or using the silence operator?\n. diff\n /**\n- * Loads JSON files and compiles them into PHP files so that they are loaded\n- * from PHP's opcode cache.\n+ * Loads JSON files and compiles them into PHP arrays\n  *\n- * @internal Please use Aws\\load_compiled_json() instead.\n+ * @internal Please use json_decode instead.\n  */\n class JsonCompiler\n {\n-    const CACHE_ENV = 'AWS_PHP_CACHE_DIR';\nWe might want to leave the constant here if someone is actually using it in their code.\n``` diff\n-    /\n-     * Loads a JSON file.\n-     \n-     * @param string $path Provided path.\n-     * @param string $real Normalized path.\n-     \n-     * @return array\n-     * @throw \\InvalidArgumentException if file does not exist.\n-     */\n-    private function loadJsonFromFile($path, $real)\n-    {\n-        if (!file_exists($real)) {\n+        if (!file_exists($path)) {\n             throw new \\InvalidArgumentException(\n-                sprintf(\"File not found: %s, realpath: %s\", $path, $real)\n+                sprintf(\"File not found: %s\", $path)\n             );\n         }\n\nreturn json_decode(file_get_contents($real), true);\nreturn json_decode(file_get_contents($path), true);\n     }\n```\n\nPerhaps we should emit a warning here?\n. Can you describe what the git commit hook you added does?\n. Just using include sounds reasonable to me. When using the SDK out of the box, calling include should be successful every time, so there's no real concern for normal SDK usage.\n. :ship: \n. Looks good other than renaming it to @retries. We might also need to update the docs.\n. :shipit: \n. LGTM.\nMaybe we could make the scrubbing pattern stuff more generic as well in that it could be a hash of pattern to replacement?\n. :ship:\n. LGTM\n. :ship:\n. LGTM\n. one minor piece of feedback, but LGTM\n. Couple minor points of feedback but looks good\n. Version 3 of the SDK is far more dynamic that version 2-- you can now more easily change the version of an API you are interacting with at runtime, and you could even use multiple API versions of the same client in the same process (as different instances of the client). When services bump their API versions, they sometimes add new operations or remove old operations. Because clients are not associated with a specific API version but rather just a service in general, generating a static list of @method annotations could lead to clients having inaccurate autocompletions when you use different API versions of a service.\nWe're open to suggestions here if it's possible to provide more context on the operations that would be listed in the @method list.\nAnother idea we've talked about is creating a PHPStorm plugin that could autocomplete a client based on the API version. There could be other nice possibilities for the plugin if we could use the model data of the version being used by the client in the plugin. This might require that when you use a variable you have to hint to the IDE the service and API version being utilized, but we're not sure yet.\n. Looks good. I think there should be new unit tests for the new classes, but after that: :shipit: \n. :shipit: \n. :ship: \n. How does this affect documentation generation?\n. I see some magic happening here with using the opcode cache to lint vs linting a script using php -l. What's the reasoning for this? If it's for performance, can you provide the numbers on how much this influences the build time for compiling JSON scripts and generating docblocks?\n. Should we also add the async magic methods for sending operations (e.g., listBucketsAsync)?\n. The return value of an async method is a Guzzle promise, not a result object. After that is changed, :ship: \n. This looks good. The only thing that concerns me is making a bunch of stuff protected, but if we mark that class as @internal, I think this can be :ship:'d\n. :ship:\n. Octal permissions don't cleanly map to Amazon S3 ACL permissions, and I don't think we can map octal permissions to ACLs in a way that will make sense. I think that coming up with different ways to map octal to ACL may make sense for some users, it will likely confuse many more. The current behavior is to return false, telling the user that the stream wrapper is unable to chmod the file. I think that is the appropriate behavior.\nFor the use case mentioned here, the best way to set an ACL is using stream context options when creating the file.\n. :ship:\n. I'm -1 on this idea for now.\nIf/when it becomes a standard then maybe we would use it, but it would have to be backwards compatible with our current implementation, not add any complexity, and provide some added value.\n. Couple pieces of feedback, but otherwise, :shipit: \n. I think the bucket_endpoint option of the S3 constructor would work here, but you would not be able to use the client to contact multiple buckets. It's basically a fixed bucket endpoint: http://docs.aws.amazon.com/aws-sdk-php/v3/api/class-Aws.S3.S3Client.html#___construct\n. I'm not sure that the SDK should do things like this in the core SDK package. It's possible though that you could create a middleware for the init step that modifies the table name using prefixes. If you thought it was generic enough, it could be open-sourced as a package that depends on the SDK.\nSee http://docs.aws.amazon.com/aws-sdk-php/v3/guide/guide/handlers-and-middleware.html#middleware\n. I gave this a ship on Friday via email, but it isn't shown here for some reason. Let me try again: :ship:\n. :ship:\n. :ship:\n. :ship:\n. It looks like the same type of logic applied in this PR can be done so without needing a hash of session IDs to session data. I think we could just track the expected session ID against the actual session ID and get the same bug fix.\n. :ship:\n. :ship:\n. Nice! You'll need to update the Makefile: https://github.com/aws/aws-sdk-php/blob/master/Makefile#L48\n. :ship:\n. Is there any way we can add tests to the example builder?\nGiven that we represent date/time values as DateTime objects in responses, do you want to represent that somehow in a response? What about blob types in a response? If you do want to do that, then you'll need to walk the model at the same time as walking the example data.\n. :ship:\n. :ship: \n. \"unparseable\" meaning we can't even extract an HTTP error code?\n. :ship: \n. Looks good, just had a comment on how we'll maintain the policy class.\n. :ship:\n. :ship:\n. I'm :-1: on adding this option as wrapping sets is a requirement for roundtripping DynamoDB types in PHP (given the ambiguity between an empty array and an empty set). We also wouldn't know how to serialize the set after loading it then trying to save it back as we would only see an array of values rather than a set.\n. Sure, why not? :shipit: \n. Sounds reasonable. I am hesitant to add a fully-fleshed out set data type to the SDK that is specific to DynamoDB, because I don't think we need things like union and difference. Maybe implementing some of the methods of ArrayAccess would be a good start.\n. One question about generators, but otherwise, :shipit: \n. :ship: \n. :ship:\n. :ship: \n. :shipit: \n. \u00af(\u30c4)/\u00af  as long as this was requested by customers, then :shipit: \n. :shipit: \n. Nice :ship: \n. It would be nice to have a similar commit made to v2, or to at least disable \"max\" validation in v2.\n. Fixed the PHP 5.5 syntax error and rebased.\n. AWESOME\nHigh level feedback before looking at the code:\n\nA region must be provided when creating a multi-region client; this serves as the default region when none is specified in an operation's parameter.\n\nWhat would you think about making the region optional and then just having a runtime check to ensure a region was resolved for a particular operation? I think a valid use case is wanting to create a client and not know which regions is will connect to because they would only know at runtime.\n\nAll clients in the pool share a handler.\n\n+1\n\nThe S3 multi-region client is able to recover from sending requests to the wrong region and resends them through the appropriate regionalized client.\n\nNice.\n\nThe multi-region functionality is implemented as a trait rather than an alternate implementation of Aws\\AwsClientInterface.\n\nThis seems like a less composable approach to me. You cannot wrap a client at runtime, it has to be a compile time \"copy and paste\" of the trait implementation.\n\nThis is done so that it can be used to create clients that are subclasses of service clients.\n\nWhat is the advantage of that over just being an instance of ClientInterface?\n\nThis means that the S3 multi-region client can be used with the stream wrapper and multipart upload objects, which take an Aws\\S3\\S3Client in their constructor.\n\nMaybe we should change that class to instead check the provided client at runtime to see if it uses the \"s3\" service?\n. Let me know when this is reviewable again (with context on what changed), and I'll take a look.\n. :shipit: \n. I don't think this PR is necessary now and would complicate the API as it currently stands. I don't see the need to have the ability to swap out what stream is used to provide the exact same functionality of caching+seeking. This should, IMO, be an implementation detail of the stream wrapper that users do not have to worry about.\n. +1 on looking at adding the number of retries to the result.\nI'd prefer that we wait on adding an on_retry. We may be looking to add some kind of idempotency token to requests that would allow you to inject middleware that can group requests together as a single transaction, so on_retry might be unnecessary.\n. I could see that as a separate, more integrated feature of the SDK rather than a one-off addition. I would prefer to table that particular aspect for now and evaluate if there is a better way to more cohesively provide statistics about a transfer to a user.\n. Can you also add documentation to the configuration docs so that we can better understand how this will work for end-users?\n. I had a request for a code comment and adding of missing tests. After that, :shipit: !\n. :shipit: \n. I think you're right that the fact that PSR7 streams take ownership of a resource should be made more clear in the documentation. @jeskew  provided a good pattern for how you can give ownership of the stream temporarily to the SDK, and then regain ownership once the transfer is complete.\nI don't think this behavior is something we can (or should IMO) change at this point. I think a good takeaway is that this should be documented clearly.\n. \ud83d\udc4d  \ud83c\udfc8 \n. We talked about this PR in person. Let me know when this has changed and is ready for another review.\n. Looking good! Have you tested out the new endpoint resolution against the old pattern based system to make sure you're resolving them in the same way? I did that in Botocore by finding every permutation endpoints from the new system and compared it against what the old system would have resolved.\n. There's one last test case to write to validate the endpoints against a list of known endpoints. Then \ud83d\udea2 \n. \ud83d\udea2 \n. :shipit: \n. :ship:\n. I'd prefer we not remove the caching from readdir. Removing it makes iterating over the contents of a bucket extremely slow.\n. :shipit: \n. Should we set this option to be disabled by default? Decoding by default was an oversight on my part and was not intentional in the SDK. This would be a sort of breaking change, but I wonder if anyone is relying on it to decode the response body.\n. If the test was run when the change was made originally, then there's either something wrong with the test or the test is inadequate to detect the failure. How could this test have worked both before this change and after this change?\n. Do these tests hit the actual service or are they mocked? We should be integration testing here to ensure it actually works.\n. I can't think of a way that we could support this with S3: http://php.net/manual/en/streamwrapper.stream-metadata.php\nSee also https://github.com/aws/aws-sdk-php/pull/776\n. Why is this change not being made to the endpoint configuration file? The list of endpoints in each service is deprecated even in v2.\nChange this file instead: https://github.com/aws/aws-sdk-php/blob/2.8/src/Aws/Common/Resources/public-endpoints.php\n. :shipit: \n. :shipit: \n. What's the motivation for this feature?\n. \ud83d\udc4d \n. LGTM\n. You will need to migrate to the AWS SDK for PHP version 3 in order to use a maintained version of Guzzle. You can find out more information here: http://docs.aws.amazon.com/aws-sdk-php/v3/guide/guide/migration.html\n. This truncation could be cased by the log_errors_max_len PHP ini setting: http://php.net/manual/en/errorfunc.configuration.php#ini.log-errors-max-len. Try to increase this setting to see if you can see more of the message. And, as @cjyclaire stated, you could also try/catch the exception and print out the message.\n. Interestingly, it appears that ::class works even on undefined classes...\n:shipit: \n. Correct.\nI think the docs are a bit confusing right now as-is. They should really make it more clear that those errors listed are just values that are returned from getAwsErrorCode() and not concrete classes. We will take an action item to make this more clear.\n. This should have been fixed in a recent version. Have you updated to the latest version of the SDK?\n\nOn Aug 6, 2016, at 7:24 AM, Samed Ceylan notifications@github.com wrote:\ninclude(Throwable.php): failed to open stream: No such file or directory \nvar/www/html/organizetedarik/vendor/aws/aws-sdk-php/src/RetryMiddleware.php(212): class_exists()\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub, or mute the thread.\n. :shipit: \n. Can you provide some example outputs of this?\nthis automated release note script doesn't count new service release\n\nAre you going to add support for this before this is merged?\n. The changelog, in this proposed format, doesn't reveal much information about the change. Couldn't we automate detecting what operations are added, service version bumps, addition of new fields, deprecation of fields, new services, etc? All of that could be automated in a way that would be close to what we are currently providing to customers, but still providing about as much information as our current changelog.\n. Is this is a command line script? If so, then the max_execution_time should default to 0 i.e., indefinite.\nIf this is running inside a webserver like Apache, then you'll need to modify the max_execution_time. With a server like Apache, you have to block somewhere to let the PHP process get released back into Apache's pool of workers. Perhaps you could send the work off into some other pool of workers using something like Gearman, but that would be external to the Apache process.\nOr perhaps you're looking for an event loop integrated into your webserver. You'd need to use something like React for that and likely use it directly as your webserver, bypassing Apache altogether. Guzzle and the SDK can be used with React via https://github.com/WyriHaximus/react-guzzle-psr7.\n. Sending a request as async in the SDK doesn't magically make it faster unless you are sending other requests concurrently.\nYou either need to architect your application to utilize an event loop and hook the SDK up through that, queue the request somehow using a faster mechanism like Gearman to do the actual request in a job queue, or you need to block until the request has finished sending.\n. \ud83d\udea2 \n. Would it be possible to buffer in memory by changing the memory to disk cutoff in the temp stream? Perhaps we should just update the amount of data cached in memory to 5 MB (or even allow it to be configurable).\nhttp://php.net/manual/en/wrappers.php.php\n\nThe memory limit of php://temp can be controlled by appending /maxmemory:NN, where NN is the maximum amount of data to keep in memory before using a temporary file, in bytes.\n. One quick thing to point out: the SDK will always return a Guzzle promise regardless of the handler you use because that's what is specified in the SDK interfaces. You will need to convert the Guzzle promise to a React promise manually.\n. Agreed with @cjyclaire. This is too large of a change to be able to maintain in the SDK. Perhaps a separate repository meant for specifically building DynamoDB queries would be a more appropriate place for this.\n. :shipit: . :shipit: from me.\n\nCan you squash this to cleanup the history?\nCan you also get a shipit from @jeskew before merging?. It looks like from the referenced issue, the customer wants to know which file that was being uploaded failed to upload. It's nice to also provide the bucket and key that we were attempting to upload to, but we should also try to capture the filename of the upload. You might be able to get this from stream metadata.. :shipit: . That looks like you were able to connect to localhost, but got an error from the local web service. Perhaps it's failing due to a signature error. Can you try passing in a region of \"local\" instead of overriding the endpoint?. I tried responding to this via email, but it didn't work for some reason... Anyways, I think we can take this PR (assuming it's been reviewed by @cjyclaire), and fix the other instances of this problem as a followup to this PR. That we can unblock @Sazpaimon now, and then fix other instances of this problem.. This is the issue tracker for the AWS SDK for PHP. You are looking for the AWS SDK for JS: https://github.com/aws/aws-sdk-js. Please open an issue there and the JavaScript team can help you.. Is the expectation here that we lock the S3 object somehow? How would that be possible?. Aren't they both valid? They would have to be for Composer to even work.\nWhat version of Composer are you using? I thought \"|\" was added after Composer 1.0 launched? I'm asking for clarification here, because for us to change this, we'll need to communicate a minimum version requirement of Composer to our customers (both external and AWS internal). We'll also need to ensure this wont affect Amazon Elastic Beanstalk as they have Composer integration.. Can you add a test case please?. What's the difference / advantages between this and coveralls?. :ship:. @imshashank this error is occurring because @Tietew does not have the SimpleXML extension installed. When you tried to reproduce this, did you ensure that you did not have that installed? The proposed solution to the error seems appropriate to me:\n\nwe should add \"ext-simplexml\" as a require in our composer.json.\nWe cannot catch Throwable because it doesn't exist pre-PHP 7. I seem to remember there being a hacky way to make this work though.... trigger_error is the standard way used to represent errors when creating PHP stream wrappers. When working with production PHP applications, you should disable error_reporting to prevent exposing error information in HTML: http://php.net/manual/en/security.errors.php\n\n\nOne way of catching this issue ahead of time is to make use of PHP's own error_reporting(), to help you secure your code and find variable usage that may be dangerous. By testing your code, prior to deployment, with E_ALL, you can quickly find areas where your variables may be open to poisoning or modification in other ways. Once you are ready for deployment, you should either disable error reporting completely by setting error_reporting() to 0, or turn off the error display using the php.ini option display_errors, to insulate your code from probing. If you choose to do the latter, you should also define the path to your log file using the error_log ini directive, and turn log_errors on.. > It would allow the AWS SDK library to be abstracted from a specific HTTP client implementation. Users would be able to choose any HTTP client they want to use with the SDK.\n\nThis is already supported. The AWS SDK has a pluggable HTTP client system. For example, it supports both Guzzle 5 and 6. You can configure the SDK to use HTTPlug today if you wanted.\nhttps://github.com/aws/aws-sdk-php/tree/master/src/Handler\nWith regards to HTTPlug, Guzzle actually has the same goal of being transport agnostic, and it supports pluggable HTTP client implementations. For example, Guzzle could technically support HTTPlug as a handler. See http://docs.guzzlephp.org/en/stable/handlers-and-middleware.html. Thanks for your feedback.\nGuzzle is used in the SDK in three ways:\n\nAs a handler implementation; Guzzle just works on most systems as it supports curl and a fallback to PHP streams. This is GuzzleHttp/Guzzle.\nAs a PSR-7 implementation; Guzzle provides implementations of PSR-7 messages. This is GuzzleHttp/Psr7.\nFor higher level PSR-7 stream operations / decorators; Guzzle provides things like LimitStream that are super useful. This is also in This is GuzzleHttp/Psr7.\n\nYou're right that choosing Guzzle is both a technical and business decision. We chose to use Guzzle because it is widely used, has the features we need for the SDK, it is a project that I created and am familiar with (and I work on the SDK team), and because several AWS employees have commit access so we can fix critical issues quickly with little impediments.\n\nWith PHPPlug developers will have the business decision about the implementation in their hands: a Zend project may use Diactoros, a micro-framework project may use Buzz, any other framework may use Guzzle and so on.\n\nSo this is where things are a bit nuanced -- Guzzle both has a client and implements PSR-7 messages. Perhaps you care more about the HTTP message implementation? One thing to keep in mind is that all of the public interfaces in the SDK typehint on PSR7 messages, not on Guzzle concretions. While we do create messages (i.e., Guzzle implementations of PSR-7 messages), we expose these in interfaces and typehints as PSR-7 messages, and downstream consumers shouldn't care about the implementation of the message. A library that expects a PSR-7 message doesn't care if it's implemented by Diactoros or Guzzle.\nI understand your point that using Guzzle in the SDK for PSR-7 messages means we are pulling in a Guzzle dependency that might be yet another implementation of PSR-7 messages in your dependency closure. Can you expand on why this is an issue for you? Are you deploying to something like Lamdba and need a smaller application size?\n\nHTTPlug, which is going to become a PSR standard.\n\nI don't think HTTPPlug is going to become a standard -- it might implement a standard. To be fair, Guzzle could also provide an implementation of a PSR client standard.\nIf there were a PSR HTTP client standard, then it's possible the SDK could be updated at some point to rely on that as the \"handler\" abstraction. Until then, I don't see a strong benefit in breaking our existing customers.\n\nThere are some benefits for the AWS SDK developers. HTTPlug is a very stable project because it is an interface, that's why it will not have as many major versions as Guzzle has, so you won't need to update the AWS SDK for a new HTTP client version so often. And you won't need to support several versions of HTTP clients simultaneously like you do with Guzzle 5 and 6.\n\nThe client in the SDK can be swapped out today for any implementation you want, so the versioning of Guzzle isn't really a problem here. I should also point out that Guzzle hasn't had a breaking change in about 2.5 years. If I recall correctly, the PSR-7 package provided by Guzzle is still stable at 1.x.\nIn summary, I think we should circle back to this if/when a PSR comes out for HTTP clients. If that PSR meets the requirements of the AWS SDK for PHP, then we could look into how it can either be better supported by the SDK without a breaking change or supported in the next major version of the SDK.. Should this be moved before the ThrottlingErrorChecker?\n. Same here, right?\n. Should this be:\nphp\n} elseif ($global && (!$region || ($region && $description->getData('namespace') !== 'S3'))) {\n. Makes sense\n. > Removing the section on not using the Phar because it seems that it is version dependent on whether or not it is supported by APC. PHP 5.5's OPcache also supports Phars.\nI don't think this recommendation has enough evidence to keep this section.. There are contradicting statements as to whether phars are cached by APC.\n- http://pecl.php.net/package/phar\n- http://www.reddit.com/r/PHP/comments/13uwgk/phar_performance/c77kmjb\n. I think the options should be documented in this docblock.\n. Should have a better description\n. I think that having to pass a CloudTrailClient and S3Client into the iterator would be preferred rather than creating one.\n. Could just use += here.\n. Can you add a comment that states why you are doing this and adding the Accept-Encoding header (e.g., \"In order for cURL to automatically inflate response data...\")\n. What's the value in defining all of these constants? Do we expect developers to actually use these constants given that the difference between typing \"limit_key\" and Aws\\Common\\Iterator\\AwsResourceIterator::LIMIT_KEY is 41 characters (66 characters if you import the class with a use statement)?\nI don't think constants add any value for data bags like this, and even less sense for data bags that source their data from string values provided from configuration files.\n. Regarding the map and filter functions: Should we add these methods given that PHP already has an idiomatic way of decorating iterators, and that these types of filter and map operations are really simple with generators?\n. These values are part of a configuration format, meaning they are already part of an external API. Being able to change the name of these keys seems like a weak argument considering that changing the name of these keys is a breaking change that should only be done in the most extreme cases.\nThis approach of defining constants for configuration keys could also lead to inconsistencies in the SDK. For example, would we add constants for configuration formats that have nested hashes and keys?\n. Why is this necessary?\n. I don't think this should be changed. Any reason why it was changed?\n. I'm not sure why this is better. I prefer not having to repeat the class name here because it means one less thing to worry about if we ever needed to rename a class.\n. The $description->getData('namespace') !== 'S3' part of this if statement was incorrect and caused all endpoints other than S3 with the \"globalEndpoint\" flag to always go to the us-east-1 region.\n. I wonder if switching off of gettype() would be faster here?\n. Does this need to be public if marshalItem() is publicly available?\n. I understand why you might want to take a JSON string and go directly to a DynamoDB formatted structure, but what's the use case of going from a DynamoDB structure to a JSON string? This really just calls json_encode().\n. Same question about this function. Does this need to be part of the public API?\n. Specifically, what use cases did you have in mind for this?\nThe reasoning for this class I thought was to make it easier to represent JSON like data in DynamoDB documents. I'm not sure that this class was designed for setting single values where you might pass them into something like a create table operation. I think you'd want to be much more explicit in those cases and provide the DynamoDB types directly.\n. What's the use case for this method? Is this meant to convert a result object into a JSON document? I am trying to understand why this class needs to have every method as public vs only exposing the minimum and opening it up later if requested.\n. Maybe mup_threshold, but threshold alone wouldn't be descriptive enough IMO.\n. I feel like they're pretty different use cases though. I wanted to keep this one pretty high to better take advantage of concurrent requests. It doesn't make as much sense for a single mup to be as high though.\n. Yes. Good catch.\n. What do you think about calling this marshalJson()? I think it makes it more clear.\n. I think the name of this still throws me off a bit. If this was called marshalJson, then I think it would be more clear as to what it does.\n. I thought about that too, but I kept these here for backwards compatibility with v2. Maybe this doesn't matter that much and they should be moved anyway?\n. Fixed\n. Interesting. Maybe these were customized in v2.\n. Good catch. This function is now invoked before default parameters are merged in. This function relied on the fact that \"scheme\" was set to \"https\" by default.\nI was thinking about updating this function, but now I'm wondering if we should even do this. \"endpoint\" is supposed to be set to an absolute URL. This code here attempts to create a URL using a scheme and treat the endpoint as a host. No other client does this, and I'd like to add clarifications to the \"endpoint\" config option to clarify that it must be absolute. Does that sound OK?\n. What if we just updated the getDomainClient() function to return an absolute URL? That could do the user a favor of assuming a default scheme of https, while not introducing a one-off behavior of a single client (i.e., endpoint accepting a domain and not an actual endpoint).\n. I don't think we should. Maybe to make this distinction more clear, get*() should become create*() (e.g., createS3())?\n. Is this used the waiter object somehow? If not, I think this could be removed and the later check for this attribute could be changed to empty().\n. ?\n. Note that this change removed our reliance on PHP's FilterIterator which had the requirement of rewinding it first before the iterator was valid.\n. I removed this function because I didn't see a need for it (at least yet). It seems like an implementation detail of the iterator leaking through.\n. I removed this as well. Modifying the paginator options after you begin paginating seems potentially problematic. Maybe we could add a setLimit() function if necessary as that seems to be the only option someone would/should change after they begin iterating.\n. Note that I added a Subscriber suffix and shallowed up the namespaces.\n. No need to rewind now that we're using a generator.\n. By tracking the opened path, we can reopen the path when dir_rewinddir is called (used in things like the Symfony Finder).\n. I changed nextStat to a hash so that it caches more than just a single element, but rather caches each element as it is iterated. This significantly reduces the number of requests made in the stream wrapper, and these cached items are cleared out when a dir resource is closed.\n. This appending of the openedBucket prefix was actually causing cache misses and wasn't necessary.\n. Here I've created my own implementation of RecursiveDirectoryIterator that doesn't have weird rewind semantics like PHP's built-in RecursiveDirectoryIterator. The built-in iterator will immediately read the first file using dir_readdir, then it will be rewound and call dir_rewinddir when you begin iterating. This caused the first element of our generators to be skipped.\nHere I've just reimplemented RecursiveDirectoryIterator using a generator and a queue of values to yield. This works on any type of stream wrapper and does not call unnecessary rewinds.\n. The generator check was not necessary here.\n. Whoops! :)\n. Fixed\n. \"type\" is no longer used btw.\n. Good idea\n. identify\n. Should this be named isUploaded to be consistent with the other is* methods?\n. What is \"partData\" here? Can that be documented?\n. I suggest either marking this class as internal or refactoring it to use private properties with protected accessor methods.\n. Is the strval necessary here? Aren't the keys always strings?\n. Unused use here\n. Could this be called something else like \"errorHandler\"?\n. Does this need to pass by reference and return true/false? Could it just return a new value?\n. Note: I used call_user_func here because it doesn't cause significant overhead and it circumvents the need to use a temporary variable.\n. With the addition of ignore_invalid is an invalid_handler still necessary?\n. Looking at how Set works now, I wonder if we actually should make it immutable. This would push validation into the constructor of the Set, cast each value to string (which it looks like is a requirement), and we could drop getValues(). Also note that the other wrappers are immutable.\n. I've used initTransaction here now that it's public.\n. Can you include the $type value in the message as well?\n. What would you think about making $dir required here, and the defaultProvider function passes in the relative directory pointing to the models shipped with the SDK?\n. Is there anything we can do to show that the versions returned from this function are specific to the manifest file?\n. I think this can be removed in favor of providing the string \"Aws\\Api\\ApiProvider::defaultProvider\" to the \"fn\" attribute of this option.\n. One idea might be to make a ManifestApiProvider class that has the static function getApiVersions().\n. newline missing\n. Fixed\n. If this is static, doesn't it leave a reference to this uploader around forever?\n. You can't because it's using $this in the closure. I'd just not make a static cache of the closure.\n. ah right... Sorry! How many valid sizes are there? Is there a reason to compute them at runtime?\n. What about making this implement PromisorInterface so that calling promise() will start an async upload, and calling cancel on the promise will abort the upload? That could possibly remove the need for an uploadAsync method.\n. There's some weirdness around $this->state. It's either a promise or an UploadState object. Because this is a protected property, I would suggest making this always an UploadState or always a promise. I would suggest making it always a promise. When synchronously calling getState(), just return the wait result of the promise. By making this always a promise, you can remove a lot of duplicated conditional checks and make subclasses have to know less about how this works.\nAlso note that the functions that yield the upload state can just always yield the value regardless of if it's a promise or not.\n. This could just become return $this->state->wait()\n. You can yield regular values and promises. I suggest yielding regardless of it's a promise or not (but I also suggest always making it a promise).\n. Same as above: could yield always\n. Could become $this->state = Promise\\promise_for(new UploadState($id));\n. Can you document how to pass custom parameters to the different operations?\n. Is there a way to pass custom parameters to each operation? If so, can this be documented here?\n. Same as the Glacier class: can we document how parameters work here?\n. Why does account_id need special treatment here? I thought that it was added when creating a Glacier command: https://github.com/aws/aws-sdk-php/blob/abstract-http/src/Glacier/GlacierClient.php#L42\n. Whoops!\n. This was dead code that isn't necessary\n. No longer relevant\n. I saw that in the flysystem .travis.yml file and copied it over. Does it not do anything?\n. This was missing from the debug list for some reason\n. This ensures that the first failed transfer fails the aggregate promise\n. This ensures that the first failed transfer fails the aggregate promise.\n. Nah, all that's there is:\n- PutObject\n- UploadPart\n- CreateMultipartUpload\n- CompleteUploadPart\nShould I just replace CompleteUploadPart with CompleteMultipartUpload?\n. I think this block of code needs to be updated to not thrown when the credentials file is empty. parse_ini_file() could return [] on an empty file, which would cause an exception. Perhaps instead we should check if the result if === false.\n. just for readability, I'd probably move the assignment of parsing the ini file into it's own line and then add another conditional to ensure that it is not false.\n. Why was this removed? Did it go somewhere else?\n. Could this be moved up to the beginning of the case statement and simply return a new stdclass()?\n. Why this change? Was it not working before?\n. Ah I see.\n. Eh, true, but it's more explicit.\n. These should all be shape references\n. Can you mention that this is calculated automatically? Most users will and should ignore this parameter so I want to make sure the docs are clear.\n. Ah I see\n. Any reason this is protected?\n. Any reason this is protected? I try to avoid protected members since they expose an implementation detail of an implementation detail to subclasses.\n- https://books.google.com/books?id=ka2VUBqHiWkC&pg=PA89&lpg=PA89&dq=effective+java+protected+members&source=bl&ots=yZFkOmt-NX&sig=oTFs5pu9W45xG6wOQGBnL5nZ4l8&hl=en&sa=X&ei=QdWSVdCBMc_1oASB9KOwAw&ved=0CDoQ6AEwBA#v=onepage&q=effective%20java%20protected%20members&f=false\n- http://programmers.stackexchange.com/questions/162643/why-is-clean-code-suggesting-avoiding-protected-variables\n. I think Graham is saying that you can omit the empty function and just call if ($this->members) {\n. Why do we need the prefixes here?\n. I don't think this credential provider should have any knowledge of a cache at all. Caching should be a wrapper around other providers.\n. Why cache the credentials by default? This only seems valuable if you know that you are using instance profile credentials.\n. What does this do? What's the motivation for a default cache here?\n. I think this carried over from v2, and I'd rather not see it repeated in V3. I don't actually think we should compute a cache key like this.\n. This should not know about the cache\n. This shouldn't know about the cache either. Caching should wrap other providers like the memoize function.\n. Don't we already have a default location for AWS caching that the JSON loader uses?\n. I wasn't saying the memoize function should have external caching built into it, but rather that there could be another function similar to the memoize function that allows for a customizable external cache. There may be some refactoring that could be done to share code, but I think these are separate abstractions.\n. Can you add a docblock here?\n. We usually don't put a new line here\n. Missing newline\n. should be a line between the ns and the use statements.\n. We don't use inheritdoc in the SDK.\n. I think it might be better to first do a file_exists and then a regular file_get_contents call.\n. I think this conditional is unnecessary\n. This isn't what we want. We want to call then on the provider and return a promise that is fulfilled with the credentials. In that callback we'd then set the credentials in the cache.\n. Ah interesting. I did not know that. Can you add a comment here to reflect that so that we don't forget?\n. Yeah sounds good to me.\n. :+1:  Agreed, let's use casts instead\n. I wonder if $value = array_map('strval', $value); would be faster/more clear?\n. Can the client actually be serialized?\n. Are you sure that things like the event dispatcher of a client with its anonymous functions can be serialized?\n\nOn Jul 30, 2015, at 5:24 PM, Jonathan Eskew notifications@github.com wrote:\nIn src/Aws/Common/Credentials/RefreshableInstanceProfileCredentials.php:\n\n@@ -42,6 +42,21 @@ public function __construct(CredentialsInterface $credentials, InstanceMetadataC\n         $this->client = $client ?: InstanceMetadataClient::factory();\n     }\n-    public function serialize()\n-    {\n-        return json_encode(array(\n-            'credentials' => parent::serialize(),\n-            'client' => serialize($this->client),\n  It can. There's no custom serialization defined, so serialize will just capture all object properties. The output is a bit large so I could cut out whatever is unnecessary.\n\n\u2014\nReply to this email directly or view it on GitHub.\n. I think we should make this more readable by moving the assignment and conditions to their own lines.\n. Nitpick: I think we use two spaces between sections.\n. I think this should be @retries as it is a meta parameter that is not part of the API. We are already using @ to designate other parameters as SDK related rather than API related (e.g., @http), so I think it makes sense.\n. Instead of two ternary checks here, could we just create an $instanceProfileProvider variable that is first created as a stock instance profile provider and then decorated with a cache if the cache parameter is present? I think that would make this easier to follow.\n. One minor optimization might be to store the count in a variable and compute it once\n. Might want to store in a variable and compute strlen only once\n. I think you would at least want to lock to a commit sha here.\n. Done https://github.com/mtdowling/Burgomaster/releases/tag/0.0.3\n. You can just do: $this->fail('....')\n. Can you add a line here that documents the host pattern and what the default value allows?\n. Why not just try/catch around the invocation of $this->parser, and catch SimpleXML exceptions? After catching then check if it's one of our known ambiguous errors and throw the appropriate retryable error.\n. Why is this code necessary? It also violates the return type as we need to return a ResultInterface here not a promise.\n. Is this try/catch necessary? Aren't we covered by the error parsing before parsing with the wrapped parser?\n. Ah I see. Makes sense.\n. So is this parser for when S3 returns a ListObjects response that is invalid XML?\n. If this is meant to catch errors parsing XML, then I think catching \\Exception is a little too broad. In fact, we should only be catching XML parsing errors to ensure that only that specific error is being retried. If we just catch all parsing errors, then we'd be retrying everything multiple times when we fail to parse. Instead of a catch-all like this, I think we should hand-pick what parsing errors are retryable (i.e., XML errors).\n. Maybe this should be RetryMalformedResponseParser to better show the intent?\n. Extra new line\n. Should this be marked @internal?\n. ? on the next line\n. This file should at least have a comment to describe what it does and the usage.\n. What is this code doing? Are you supposed to pass in the classes to update as an argument?\n. Space between variable and array push token\n. Can this just become something much simpler like: {$this->getApiDefinition()['metadata']['serviceFullName']} client\n. What's the motivation for adding the description of the method in the method annotation? It adds a lot of extra weight to each client? I'm not even sure where this text would show up in the editor. I'd prefer the doc be used for just listing which versions of an API the method is compatible with.\n. Why is enabling the opcache necessary?\n. Is there a way to comment in latte scripts? If so, this definitely warrants a comment to describe why this is needed.\n. Can you split this into separate statements for readability?\n. This should be separate statements I think. First create, and then call. Spanning multiple lines in a ctor and then calling a method in the expression makes this hard to read.\n. This should be two statements.\n. $docBlockLines [] -> $docBlockLines[]\n. Do these examples actually cause us to try with a 2gb file?\n. Mark as @internal?\n. I think this approach is fine for now. Let's try to see if we can come up with a better approach going forward though.\n. I think the behavior of removing the CreateBucketConfiguration structure should always be performed when the supplied value is us-east-1. Right now it's only removed in the provided value is empty. If the user supplies us-east-1, we should also remove that as that is the current behavior.\n. I don't see us-east-1 listed as a valid LocationConstraint in the S3 docs: http://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketPUT.html. You're supposed to omit the LocationConstraint altogether if you want to create a bucket in us-east-1. I was saying that the pull request should be updated to also remove the LocationConstraint if the value it's set to is us-east-1.\n. Missing docblock\n. Can you rename these two parameters, bucket and key, to something like $toBucket, $toKey or $destBucket, $destKey?\n. It seems weird to duplicate the createCredentialsMethod on multiple clients when they do exactly the same thing and don't really create credentials specific to the client. I wonder if we should just add a factory/creational function to Credentials that creates them from an array, mark the Sts method createCredentials deprecated, and make that simply proxy to the factory method.\n. Is there any way to add some kind of mention of the parse error in the returned error?\n. I think this needs to still be an integration test. We need to ensure that we're actually able to send requests concurrently and the handler bindings actually work. I'd feel much more confident in the implementation if we still sent real requests for these basic tests.\n. It would be testing promises, Guzzle bindings with the SDK, SDK clients, et. al. At minimum, I think we need integration tests for the bindings (I can't remember if those exist somewhere else offhand). Since we ship the SDK with Guzzle bindings, I think we should have an integration test that makes sure those bindings work correctly. I think integration tests for this are just a really easy way to make sure something isn't super broken before a release. It's nice just to make sure that the bindings are correct I think.\n. Sorry for being late, but this needs a docblock\n. I could go either way on if this class should exist. My main concern if how we can maintain this class as CloudFront policies evolve. For example, if they introduce more complexity in policies (e.g, nested structures and many more top-level parameters), how will this class and the constructor evolve?\n. I don't remember why I didn't do it this way before, but what about creating a generator that yields these build up commands rather than storing them all in an array? Given that we're downloading based on listObjects, the number of GetObject commands could be practically infinite.\n. Does it need to be required?\n. Needs error message\n. Add docs\n. docs\n. constant?\n. I think these should be opt-in by default instead of opt-out by default (meaning things default to off and must be turned on). This allows us to add more stats features in the future without automatically opting users into each new check. If we keep this opt-out by default, then each time we add a new stats feature, users will need to update their configuration to opt-out of the new feature.\n\nPresumably, if someone is utilizing the array form of the stats config, they explicitly configured it for a reason.\n. What about making the default should be false by default?\n. Given that this class is used to power almost every transaction, it might be better to only use the additional layer of promise if the user requires transfer stats.\nI also don't know of a case where you would get the then callback invoked and the on_stats callback would not have been completed. I wonder if there's a different way to get at this data without a promise.\n. Can you comment on why this is necessary?\n. Isn't this making the assumption that all instances of AwsException have the same constructor? I wonder if there's a more efficient or different way of achieving this.\n. static function?\n. The extra function decoration, while minor, is unnecessary overhead for such a core component of the SDK. I'd prefer an if statement that only invokes the previously set on_stats if it is present.\n. Why not move the __on_transfer_stats decoration code to the layers of the SDK that actually set the value of on_stats? Otherwise, you're introducing a new configuration option that might not be necessary.\n. What I'm suggesting is that you don't need this abstraction. Instead, you could do any necessary function wrapping in the places that you've added that currently set __on_transfer_stats. Using __on_transfer_stats doesn't really solve the issue either: what if there are several internal components of the SDK that need to augment __on_transfer_stats? You'd then need the same type of function wrapping.\n. What about allowing the context of an exception to be mutable?\n. Need to update this too for the new option name\n. These are still defaulting to all on. If I specify ['timer' => true], then you'd also add in retries and http as true unless I explicitly set them to false. I think the less surprising thing to do here would be to require users to explicitly opt-in to stats features when they provide an associative array.\n. Ohhh, I see what you're doing here. Since it's really clever code, please add a comment to describe what's happening here. I also don't see any explicit tests for this.\n. Yeah, I'm not sure retrying these would ever result in a success.\n. Does it matter that we've mutated the command in this method? Maybe we can clone the command or, a more involved change, we could return the removed handler when calling remove, and add it back at the end of the method.\n. Is this necessary?\n. Is the user provides an explicit signing region in the client constructor, it should override what comes back from the endpoint provider. Is that case handled?\n. Is this an unrelated change?\n. Couldn't you first check if the list of regions for the partition matches the given region, and only then fall back to regex based results?\n. This is good, but on a side note: I wonder if we should add \"sdkPreferredSignatureVersion\" or something similar to the config file to remove this from each implementing SDK.\n. In order to allow this interface to be more extensible, perhaps this should be a more targeted method name... Like getPartitionFromRegion($region) or something.\n. Unrelated change?\n. Isn't this trying to get a partition by partition name, but the method only works with region names?\n. Without a description, it's not clear what this method does.\n. I wonder if exposing any of the CommandPool options is something we should support? What if someone doesn't want to do these operations concurrently or wants to control the concurrency level?\n. Can you add tests for configuring multi-region clients?\n. It might be a good idea to add a test to ensure s3v4 can be resolved. While it's dead simple right now, it would help in case of a future refactor.\n. How is this one resolving? \"us-east-1\" doesn't match the region explicitly nor do I see a region regex that it would match? Is this a test for an unknown service and and unknown region (e.g., fallback to assuming the aws partition as a last resort)? If so, maybe you should add a test that is just for unknown service, and one that is just for unknown region.\n. Ah ok\n. Do we also need list permissions, or is that lumped into read permissions?\n. The changes made to this file are untested.\n. There are actually N number of transfers in progress at any given time. When a failure occurs, the transfer stops at the point of failure, leaving the transfer in a state in which there are N number of transfers that are incomplete. I believe that when you wait on the Transfer it will throw when the first error is encountered, leaving the other currently in-progress transfers incomplete. You might want to double check the behavior here. If that's the case, then you would need to provide context on all of the in-progress transfers and the failed transfer when throwing the exception.\n. missing new line\n. Maybe create a helper method for this that only checks for \\Error if it exists.\n. Why not just look for Throwable?\nhttp://php.net/manual/en/class.throwable.php\n. I think this should be configured in the constructor, not each time a request is created. This just overwrites the host header, so if you wanted to hit a custom endpoints (like a test endpoint for example), this would not allow that.\n. Does this need to be a separate parameter than AWS_PROFILE? Is that what other SDKs are doing for this feature?\n. Should this be \"internal only\" given that the default STS client could be dangerous considering it uses the latest version? I think it would make sense to document that and support it.\n. Could this either be inlined into the default client ctor or given a better name like $defaultClientArgs?\n. Why are you calling this synchronously only to wrap it in a promise? Use assumeRoleAsync.\n. Should add a \":\" before the $reason\n. Missing spaces around ? and :\nShould this default to the default server URI?\n. Shouldn't this use getEcsUri to allow a custom URI?\n. We use camelCase for variables in the SDK\n. This line really needs a comment to explain what's going on\n. Can this be private?\n. Please use camelCase for consistency with the rest of the SDK\n. Why is this on a new line?\n. Shouldn't you pass in the $config array? Otherwise the config is not being used\n. camelCase\n. camelCase\n. /s/Use/Uses/\n. Missing space\n. This doesn't belong as part of the assume role provider. This should be moved into the default provider\n. What is going on here? This seems overly complex\n. I'd like to see if we can limit the API surface area here and utilize a single option. I think it would be a good idea to see how all of the other SDKs are handling this.\n. It's possible that people want to use the default credential chain, but also need to provide custom values to each chain depending on whether or not that chain is actually utilized... This might be a more complex problem that would need to be solved in a separate update, so maybe this can be left out for now.\n. I don't see a reason to publicly expose this. If you can't figure out a good way to test this if it's private, then let's add the @internal annotation to the docblock.\n. Is there a reason to make the key uppercased? SDK specific array keys are generally lower camel cased.\n. Can you add a docblock?\n. What are the contents of inProgress? Looks like it's Failed and Uncompleted. What do you think about just making these positional arguments to the constructor? What are the contents of Uncompleted supposed to represent?\n. Could this be called something like failedFile? Or is there a delineation you're making between source and file?\n. Maybe call this remainingFiles?\n. Extra newline\n. This wouldn't work on Windows. Maybe move this out into a method that can be reused throughout your tests. Something like this should work for a non-recursive delete:\nprivate function rmdir($dir) {\n    array_map('unlink', glob(\"$dir/*.*\"));\n    rmdir($dir);\n}\n. This looks repeated multiple times in the test. Can you separate this into a function?\n. Missing space between ) and {\n. why is all needed here?\n. Shouldn't \"uncompleted\" (probably should be renamed to remaining) track the source filename and not the destination?\n. Could this just be $this->inFlightTransfers[] = ... I don't see the need to make this an array with a Uncompleted and Failed key.\nCould we also maybe use an SplObjectStorage to keep track of these commands, using attach, contains and detach? http://php.net/manual/en/class.splobjectstorage.php\n. no need for the null check\n. What's the purpose of mutating a stateful property on the class rather than just providing $file as a constructor argument to the TransferException?\n. This is class S3 specific, no? Could we move this to the Aws\\S3\\Exception namespace instead and call it S3TransferException?\n. Maybe change this to \"An error occurred while transferring file: \"\n. This would then be updated to just remove the object from the SplObjectStorage\n. I don't think we should be changing the exception here. All we need to do here is to remove the command from the inFlightTransfers object.\n. This is where you'd wrap the exception by adding a then method on the promise() call that decorates the given exception, adds the information about the failed command, and the list of inFlightTransfers / remainingTransfers.\n. You don't really need the conditional here. You can concat null, empty strings, etc.\n. Is this still needed?\n. Since there's no longer an argument needed, the body of request could just be moved into __invoke.\n. I think you can just pass the string value as-is without wrapping it first in a Uri object.\n. Extra spaces around array brackets.\n. If reason can be an array, then you're potentially calling ->getMessage() on an array.\n. Should we document the client option?\n. Should you check to make sure that the $profile exists?\n. Should probably add @internal\n. Missing newline\n. Can you keep this as filename?\n. Can you consistently use $filename instead? That's more common than $fileName.\n. Could just return nothing (i.e., omit the return '' so that null is returned.\n. return null|string Returns a string describing any error that could occur.\n. This could just be if ($msg) {\n. The previous approach was used so that it would be able to handle an invalid profile name that couldn't be found in the ini file. The current implementation would throw a warning when the profile can't be found because you're accessing an array key that may not exist.\nCan you add a test for this too?\n. Can you make the indentation here look like it used to?\n. This new chain setup looks really expensive -- we are potentially reading and ini parsing both the credentials files and config files twice just in this chain, and looking at the assumeRole method, that number could climb to 6.\nHave you benchmarked this? How does this affect the common usage of the SDK with environment variables and instance profile credentials?\n. Would all these chain calls really load the ini file and ini parse it this many times?\n. You don't need a promise_for when you know the return value of assumeRoleAsync will always be a promise.\n. Shouldn't this be string|null?. Singular, right? getFilePath. What happens if this if/else branch and the branch above do not evaluate? The filePath value is not set. What exceptions would come through here that aren't an array or AwsException? Any random exception thrown in a promise I guess?. Side note: It's really weird that $prev might be an array. We should look at this and see if we can make $prev always an exception in a backwards compatible way so it's a bit more predictable.. Maybe add some docs to state that the filePath may be null if the bucket and key could not be ascertained from the previous exception. Which, by the way... This class is coupling itself to S3 now even though it's meant to be generically used for both S3 and Glacier.. I don't think this will work... This is now tying this class to S3, but it's also used in Glacier: https://github.com/aws/aws-sdk-php/blob/master/src/Glacier/MultipartUploader.php#L6. Maybe the filePath needs to be injected into the exception in the constructor instead of extracted from a previous exception.. Note: StsClient not STSClient. Can we link to the PHP API docs?. Which ini file? I think you mean, \"to avoid unnecessarily fetching STS credentials on every API operation. The memoize function will handle automatically refreshing the credentials when they expire.\" This guidance needs to also be moved up above the code sample in an \"Important\" admonition so everyone knows to do this more prominently.. Looks like you're missing a heading for this credential provider. Right now it looks like this is part of the instanceProfile provider section.. Separating this into a trait doesn't seem necessary. Maybe it's an artifact leftover from a previous iteration of this PR, but can you move this back in until there's a need to do this?. Why are you not returning the loaded $data? Right now, you are running this method twice, loading and parsing the same INI file. This method should return the parsed ini file and we should not re-parse the file again.. This method doesn't really work... Maybe checkProfile should be combined with this method so that you're only loading the ini file once.. Can you document what this is (e.g., the default profile in the AWS CLI)? There's some documentation about the default credential chain that I think needs to be updated to reflect this change as well.. I'm weary of supporting this use case. In order to do this, we'd need to create an STS client on the developer's behalf, correct? Which means we'd need to hardcode a dependency on the \"latest\" version of STS, correct? That would be a risky contract considering STS could ship a major version at some point that could break this credential provider. Are we doing anything to mitigate this? If not, how you feel about requiring a client be passed in and removing the 'credentials' + 'region' option?. should this be plural?. Did you update the documentation that talks about default credential resolution to talk about this new behavior?. Should this be its own method just like how the \"removeCredentialProviders\" method works?. This is still coupling the generic MultipartUploadException to S3. Can you refactor this as previously suggested?. Any reason to not add the caught exception as a previous exception when creating the CredentialsException?. This is coupling concrete exceptions to the abstract class. This isn't a good way to achieve what you want.\nConsidering this class is marked as internal, why not make this an argument passed in to the constructor instead?. This looks like it isn't needed. Extra new line. This seems to be missing information about when and why an array would be provided.. Please document when or why would this be null.. Why not use $config instead? It's a bit awkward to put a scalar argument after an associative array argument. Can you also call it exceptionClass if an argument or exception_class if a config value?. Is there a reason to not just have a getBucket and getKey method instead of returning these as a single concatenated string?. I'd just push this responsibility off onto instances of MultipartUploadException. Have them pull it out of the $prev array just like how they're pulling key and bucket out.\n$prev should always be full of UploadPart operations which, if I recall correctly, have a Body parameter that must be a stream. You could use that if present, pull out the metadata, and populate the filename.. Maybe call this getSourceFilename to better clarify that this is the filename of the data that was being uploaded.. No newline should be between the docblock and the class.. And you should use single quotes for strings that don't use interpolation.. I suggest switching all this code over to use DirectoryIterator and http://php.net/manual/en/directoryiterator.isdot.php. Why is this \"$TAG\" and not \"$tag\"?. Seems like a switch would work better here.. I like the idea of using an optional associative array for configuration options... Something like:\npublic function __construct($service, $region, array $options = [])\nWhere it accepts an \"unsigned\" key-value pair.. No, we are going to switch from things like s3v4 and consolidate them into just v4 and v4-unsigned-body instead.. Can you include the JSON error code? You could also attach the PayloadParserTrait to this class just use the parseJson method.. Why is this not in a 'string' case statement below like other types?. This isn't properly validating jsonvalue. If the shape has the jsonvalue trait, then the provided value MUST be something that can be json_encoded. Therefore, you should have a branch here that is only conditional on if the jsonvalue trait is present. Then inside of that branch, you ensure that the value can be json encoded. If it cannot, than you must add an error via addError. Otherwise, if the value can be json encoded, then return early.\nI did a quick benchmark to see the fastest method to check if a value can be json_encoded. A function that check for the appropriate types or interfaces is almost always faster than attempting to json_encode and look for errors. Here's my gist with the example functions: https://gist.github.com/mtdowling/ed5ea633ec8358e6d2d399a8c0aa1220. I essentially whitelist specific types and have a special case for object.. Validation should be isolated from serialization. If I run my input data through the validator, then it should ensure that the values I provided are valid based on the model. This is a separate process that runs before serialization. You can also get much better error messages from validation than from a serialization error because the validator tracks error context whereas serialization does not.\n. As discussed over email, \"object\" can also be encoded regardless of if it is JsonSerializable. So this method could really just be simplified into !is_resource. JSON encoding a callable just always generates \"{}\", so I'd be fine with allowing it.\nphp\nprivate function canJsonEncode($data)\n{\n    return !is_resource($data);\n}. Agreed. I suggest getAwsErrorMessage() to match the other methods.. Why is this being removed?. Let's break this out from a ternary expression into some more readable like if statements.. ",
    "ashishtilara": "Thanks for the prompt reply.\nHowever, what happens in 6 months time when some of my code uses 1.4, some of it uses 2.0, and some of it uses 2.2? Or in a years time when some is using 3.0?\nYou can't (and shouldn't) have to guarantee perfect backwards-compatibility between all future versions.\nWhat strategy should I use for using SDKs 2.0, 2.1, 2.2, 3.0, 3.1, etc all in the same PHP context side-by-side? Is there anything that can be done upstream (by you) to help? \n. @skyzyx Pardon my frustration, but this doesn't really answer my concerns. We have a massive code base, and it is literally impractical for us to move wholesale to a new library version every time you push out an update. We will likely always have classes that use 1.4, others using 1.5 and now potentially new classes using 2.0, and these classes all need to be able to used together. We don't have the resources or the risk-appetite to update everything all at once.\nEven though you say you don't anticipate future breaking changes, you can never guarantee that, so we are stuck in a predicament. If there will never be upstream support for use-cases such as ours, then we will just have to figure something out ourselves. Thanks for your input.\n. ",
    "skyzyx": "\nHowever, what happens in 6 months time when some of my code uses 1.4, some of it uses 2.0, and some of it uses 2.2? Or in a years time when some is using 3.0?\nYou can't (and shouldn't) have to guarantee perfect backwards-compatibility between all future versions.\n\nThe SDK should be viewed as a complete set by itself. Once SDK 2.x has complete support for all services, you should plan to migrate fully, and all of the 2.x code in your project should all come from the same version of the SDK.\nSDK 1.x started life in 2005 as a third-party open-source project called Tarzan. That codebase lasted for 7 years, and we expect that the code that we've recently shipped should last us another several years \u2014 even if we eventually version it as 3.0, 4.0, etc.\nThe reason why this particular change is so impactful is because of the changes that occurred in the PHP language when 5.3 came out. Switching to namespaces forced us to break everything. From the changes we've seen in 5.4 and in previews of 5.5, we don't see changes of this magnitude having to happen again for a long time.\n\nWhat strategy should I use for using SDKs 2.0, 2.1, 2.2, 3.0, 3.1, etc all in the same PHP context side-by-side? Is there anything that can be done upstream (by you) to help?\n\nYou shouldn't. Any SDK code that you use should all be the same version of 1.x or all the same version of 2.x (and possibly 3.x and 4.x). Again, it took us 7 years before we had to go back and do a ground-up rewrite of the code.\nInstead of thinking in terms of version numbers, think in terms of generations.\nFirst Generation:\n- Tarzan 1.0 (2005)\n- Tarzan 1.1 (2005)\n- Tarzan 2.0 (2008)\n- CloudFusion 2.5 (2010; renamed)\n- AWS SDK for PHP 1.0 (2010; renamed; formerly CloudFusion 3.0)\n- ...\n- AWS SDK for PHP 1.5.15 (2012; equivalent to CloudFusion 3.5.15)\nSecond Generation:\n- AWS SDK for PHP 2.0 (2012; equivalent to Guzzle-based SDK for AWS 1.0)\n- ...\nWe currently provide compatibility for jumping between generations of SDK. You should use all the same version of one generation alongside all the same version of the other generation. Even though this appears to be a jump from 1.x to 2.x, you shouldn't necessarily expect the same kinds of changes from our eventual 3.0.\nIt would be reasonable to suggest that a future 3.0 release might leverage Traits (new in PHP 5.4; not compatible with 5.3). In this case, some changes might be required, but we anticipate them being adjustments to the existing SDK 2.0 API, instead of being a wholesale rewrite that we're presently experiencing with the move from the older Tarzan/CloudFusion codebase to the new Guzzle-based codebase.\nAny time we've been forced to release a breaking change, we've made it a point to call it out very specifically in our release notes, change log, API reference documentation, and other update-related documentation. On the off-chance that a minor breaking change has to be made, we will continue to thoroughly document those changes in the places that we think people will look (it's always a good idea to read release notes before upgrading, but we know that some customers don't).\nIf you're using Composer to install either SDK (definitely a best practice, and the de-facto successor to PEAR/Pyrus), it would be wise to scope updates down to minor releases (e.g., 1.5., 2.0.) to make sure that you don't jump up to a new major version (e.g., 1., 2.) without knowing it (whereas I'm defining version numbers as <architecture>.<major>.<minor>, like the Linux kernel, Mac OS X, and Firefox releases before 4.0).\nMake sense? :)\n. Could you submit a pull request and any relevant unit tests for this?\n. Apparently this also includes some doc assets that don't impact the code.\n. Whoa\u2026!\n. @jeremeamia: I've already implemented something like this in another project, but it's not as clean of an implementation as is ideal. Essentially, you take the key pair *.pem file and ping the hostname using the ssh shell command in a loop until it connects. Once it's connectable, you return true.\n. If you were to want to implement it (presumably via a custom Waiter), would you rather rely on the ssh2_connect() function exposed by the SSH2 extension, or a pure-PHP implementation such as phpseclib? There's nothing in the web service API for this, so it would need to be coded in PHP instead of extending the existing Waiter DSL. Right?\n. Composer says I'm using SDK 2.3.4 + Guzzle 3.6.0:\n``` json\nsnip...\n{\n    \"name\": \"aws/aws-sdk-php\",\n    \"version\": \"2.3.4\",\n    \"source\": {\n        \"type\": \"git\",\n        \"url\": \"https://github.com/aws/aws-sdk-php.git\",\n        \"reference\": \"2.3.4\"\n    },\n    \"dist\": {\n        \"type\": \"zip\",\n        \"url\": \"https://api.github.com/repos/aws/aws-sdk-php/zipball/2.3.4\",\n        \"reference\": \"2.3.4\",\n        \"shasum\": \"\"\n    },\nsnip...\n{\n    \"name\": \"guzzle/guzzle\",\n    \"version\": \"v3.6.0\",\n    \"source\": {\n        \"type\": \"git\",\n        \"url\": \"https://github.com/guzzle/guzzle.git\",\n        \"reference\": \"v3.6.0\"\n    },\n    \"dist\": {\n        \"type\": \"zip\",\n        \"url\": \"https://api.github.com/repos/guzzle/guzzle/zipball/v3.6.0\",\n        \"reference\": \"v3.6.0\",\n        \"shasum\": \"\"\n    },\nsnip...\n``\n. I'm currently sending over 220 files at a time to S3. Let me take a look at the batch abstraction. Thanks!\n. I'm looking at the definition of thetransferRequests()` method in the Guzzle API docs (formatting mine):\n\npublic Guzzle\\Batch\\BatchBuilder::transferRequests( integer $batchSize = 50 )\nConfigures the batch to transfer batches of requests. Associates a \\Guzzle\\Http\\BatchRequestTransfer object as both the transfer and divisor strategy.\nParameters\n- $batchSize  Batch size for each batch of requests\nReturns\n- Guzzle\\Batch\\BatchBuilder\n\nI'm having trouble understanding what this does, exactly, in real-world terms. If I increase this, will it go faster?\n. @gwis: Sure, I understand that. :)\nMy question wasn't about batching in general as much as that specific method (compared to, say, autoFlushAt()).\n. So far\u2026\nphp\n$batch = BatchBuilder::factory()\n    ->transferCommands(10)\n    ->autoFlushAt(40)\n    ->build();\n\u2026seems to work without throwing exceptions. Anything higher (stepping by 5) kicks up dual exceptions when running in a Symfony Console application:\n```\n[Guzzle\\Batch\\Exception\\BatchTransferException]                             \n  Exception encountered while transferring batch: Errors during multi transfer\n  Error completing request\n[Guzzle\\Service\\Exception\\CommandTransferException]\n  Errors during multi transfer                       \n  Error completing request\n```\nI understand what autoFlushAt() does. I suppose my question was about what transferCommands() is, specifically, and how it's different from autoFlushAt().\nFor example, when I read this, I see \"Hey you. I am commanding you to transfer these commands here.\" But it's unclear why I'm passing 10 to this method. Or why passing 15 triggers an exception. And the description in the API reference doesn't get me any closer to understanding.\n. Yessir. Thank you for the clearer explanation. :)\n. Sweet. Using the Guzzle HEAD, I'm able to push both to at least 100 and the performance is much better.\nAdditionally, pushing both to 200 causes the exceptions to be thrown, but the messages make way more sense for debugging.\nThanks!\n. Here's a small snippet of my code:\nphp\n$batch = BatchBuilder::factory()\n    ->transferCommands(20)\n    ->autoFlushAt(20)\n    ->notify(function(array $items) {\n        foreach ($items as $item) {\n            // $item->getRequest()->getBody() # Something, something?\n        }\n    })\n    ->build();\n. I tried the latter earlier. That was how I got php://temp. I looked again at my code and saw that I'm passing:\nphp\n$batch->add($s3->getCommand('PutObject', array(\n    'Bucket'   => $bucket,\n    'Key'      => str_replace($strip, '', $file),\n    'Body'     => fopen($file, 'r+'),\n    'ACL'      => Permission::PUBLIC_READ,\n    'Metadata' => array(\n        'DeployedBy' => $_ENV['USER'],\n    )\n)));\nThe S3Client::putObject() docs say that Body can be a string, an fopen() handle, or a Guzzle\\Http\\EntityBodyInterface object (I haven't yet looked to see which constructable classes are valid here).\n. Looking again, I see SourceFile as an option in one of the examples, but it isn't listed anywhere in the parameter listing. :/\nLet me give that a try.\n. So, SourceFile was a dead-end. Switching back to Body and fopen(), the following did NOT work because getBody() was returning NULL:\nphp\n$command->getRequest()->getBody()->getUri();\nHowever, this one DID work because it returned the proper object.\nphp\n$command['Body']->getUri();\nI seem to have it working now. Thanks for the help!\n. So it works if I set:\nphp\nuploadFromGlob(__DIR__ . '/template-*.json')\nIt might be worthwhile to call this out in the example and/or the docs.\nAs a follow-up, I'm trying to take these local files and upload them into the root of the bucket. This doesn't seem to work:\nphp\nsetKeyPrefix('/')\nAny tips?\n. That seems to work. Thanks!\n. We didn't have that as something we checked for in SDK1, but it was the first thing that @brendandixon got tripped-up on when he tried to use the SDK for the first time.\nI have no opinion either way, but I thought the anecdote was worth mentioning.\n. Amazon Web Services (AWS) refers to all of Amazon's infrastructure-based services, including cloud servers and storage, databases, messaging queues and the like.\nThe Product Advertising API is not part of Amazon Web Services, but rather, the Amazon Associates program \u2014 which is entirely unrelated to AWS (despite the fact that it is a web service and is provided by Amazon; confusing, I know).\nAs such, the official word from AWS is that there will never be first-party support for the Product Advertising API in the AWS SDK for PHP. If you're interested in a PAAPI SDK, you should make a request to that team.\n. What's a document?\n. Ah, CloudSearch. You didn't lead with that. :)\n. :+1: \n. Yeah, I have some experience here. Making the requests is easy. Putting together an .sdf document for the content you want to index could certainly use some helpers. For example, the public documentation explains that since the service uses XML serialization on the backend, even if you create a JSON-based .sdf file, its contents are still required to follow the (stricter) XML serialization rules. I learned that one the hard way.\nThe old-school CloudSearch CLI Tools (as opposed to the new Unified AWS CLI Tools) have some nice convenience actions for generating .sdf indexes from a set of files you want to index. That may be helpful for some people. I ended up building .sdf creation directly into my View layer so that I could automate the content indexing easier.\nThere are a couple of other areas where the CloudSearch search API could be smoothed over. For example, getting the total number of search results while focusing on a single facet requires two requests. This can be batched using Guzzle directly, but having a PHP interface to smooth out the rough edges and add some convenience would be a big benefit for end-users.\nIt's not just making the raw request; it's about maintaining the same programmatic interface that we're already used to, and making it easier to work with the service responses.\n. If the purpose of the SDK is to offer access to the low-level web service APIs with minor user-centric enhancements, then it makes sense that this would be too broad in scope. At the same time, if this feature request only supports Apache and SimpleDB, it feels a bit too narrow of a use-case to be included because it's non-AWS specific (except that it leverages SimpleDB), IMO.\nIf it supported either (a) more web servers (e.g., Nginx + PHP-FPM, HHVM), (b) more storage backends (e.g. DynamoDB, Redshift), or (c) both, then I could see it being a more useful addition (although this would be counter to the idea of a primarily low-level SDK).\nI think that the best solution for making this feature available to customers long-term is to make a Composer-installable package that solves this specific problem for developers who have it. (I use HHVM and DynamoDB in my own infrastructure, so I think it makes more sense as an optional add-on as opposed to being built-in).\n/2\u00a2\n. HHVM has APC built-in; it's not an extension in that environment.\n. Did this affect 64-bit systems, or only 32?\nIIRC, we had documented this as a known issue at one point (which I can't seem to find now). If we're still calling this out someplace, then it would be good to ensure we're providing accurate information.\n. Docs on 5.6 aren't thorough yet. :)\nI'll do a little more digging to see if I can get that question answered. Since 5.6\u03b11 is out, I just wanted to make sure that we were aware of what needed to be reviewed. This might be nothing, but better to know than not know.\nKeeping the ball.\n. It looks like you're right \u2014 they're separate issues.\nBruno Skvorc, reporting for Sitepoint explains:\n\nUploads of over 2GB are now accepted\nUntil 5.6, no uploads of 2GB and over were supported in PHP. This is no longer the case, as the changelog states, and uploads of arbitrary size are now supported.\n\nThe known issue has to do with 32-bit systems being able to push more than 2 GB of data. This change in PHP 5.6 has do do with the size of uploads being sent via POST to a PHP-enabled server.\nMore context:\n- https://github.com/php/php-src/blob/PHP-5.6/NEWS\n- https://bugs.php.net/bug.php?id=65944\nSorry for the fire drill. :)\n. Out of curiosity, are you batch-deleting? Does the process take more than 15 minutes?\n. What about improving the user-facing exception messaging?\nOn Tue, Feb 4, 2014 at 9:04 AM, Michael Dowling notifications@github.comwrote:\n\nThe fix to this issue is to use a specific region. There isn't anything we\ncan do in the SDK, but we are tracking this in our internal backlog to\ndocument the error and any workarounds.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/aws/aws-sdk-php/issues/224#issuecomment-34081817\n.\n\n\nRyan Parman\nhttp://ryanparman.com\n\"If you don't have passion for what you do, you won't be very good at it.\"\n. @andrew-s: Why not take a swag at submitting a pull request which handles these kinds of error messages more appropriately?\n. :+1: for supporting AWS CLI config syntax.\n. You need to encode the non-ASCII header value.\n\nhttp://docs.aws.amazon.com/ses/latest/APIReference/API_SendEmail.html\n\nBy default, the string must be 7-bit ASCII. If the text must contain any other characters, then you must use MIME encoded-word syntax (RFC 2047) instead of a literal string. MIME encoded-word syntax uses the following form: =?charset?encoding?encoded-text?=. For more information, see RFC 2047.\n\nhttps://en.wikipedia.org/wiki/MIME\n\nSince RFC 2822, conforming message header names and values should be ASCII characters; values that contain non-ASCII data should use the MIME encoded-word syntax (RFC 2047) instead of a literal string. This syntax uses a string of ASCII characters indicating both the original character encoding (the \"charset\") and the content-transfer-encoding used to map the bytes of the charset into ASCII characters.\n\nhttp://tools.ietf.org/html/rfc2047\n\nAn 'encoded-word' may replace a 'text' token (as defined by RFC 822) in any Subject or Comments header field, any extension message header field, or any MIME body part field for which the field body is defined as '*text'.  An 'encoded-word' may also appear in any user-defined (\"X-\") message or body part header field.\n\n\n@jeremeamia: This might be a good use-case for a utility class to be added to the SDK.\n. Use Composer on deployment, and a few hundred kilobytes is really not a big deal.\nBut yes, I agree that support for this should be added to the SDK. Thanks for making the team aware of this need.\n. Just a note that GET, POST, PUT, DELETE, and other HTTP verbs all require a different signature. If you try to use a PUT pre-signed URL to do a POST, it will (and should) fail.\n. You need to set 'MetadataDirective' => 'REPLACE' to do an in-place update.\nhttp://docs.aws.amazon.com/aws-sdk-php/latest/class-Aws.S3.S3Client.html#_copyObject\n. > So there is no at way to just 'add' one metadata field?\nThere is. You need to tell S3 to copy the object over itself with new data. You do this by setting 'MetadataDirective' => 'REPLACE' as part of your call.\n\n\n[\u2026] how is this accomplished?\n\nSomething like this:\nphp\n$s3->copyObject(array(\n    'Bucket'          => 'mybucket',\n    'CopySource'      => 'mybucket/test.gz',\n    'Key'             => 'test.gz',\n    'ACL'             => CannedAcl::PUBLIC_READ,\n    'CacheControl'    => \"max-age=60, public\",\n    'ContentType'     => 'text/plain',\n    'ContentEncoding' => 'gzip',\n    'Expires'         => '0', // 1 Jan 1970, 12:00am GMT\n    'Metadata'        => array(\n        'updated'        => '1', // x-amz-meta-updated\n        'custom-header2' => '2', // x-amz-meta-custom-header2\n        'custom-header3' => '3', // x-amz-meta-custom-header3\n    ),\n    'MetadataDirective' => 'REPLACE',\n));\n. > [\u2026] I just want to add one [\u2026]\nHere's the catch: you need to read the properties of the object you want to maintain, and re-apply them when you replace an object with itself (i.e., an in-place update) because otherwise S3 will either ignore them, or reset the values to default (e.g., the ContentType value).\nS3 is kinda dumb that way, but if you understand that S3 treats a object-copy the same way that it treats a fresh upload, this approach makes sense. You must explicitly set the properties that you want on the new copy and/or updated object. S3 doesn't maintain any state in this case.\nMake sense?\n. Nope. Please retire ASAP. :)\n. Weird. Looks like Multipart-MIME to me. Is this coming in from an HTML form?\n. http://www.php.net/manual/en/xmlwriter.installation.php\n\nThe XMLWriter extension was initially a PECL extension for PHP 5. It was later added to the PHP source (bundled) as of PHP 5.1.2. This extension is enabled by default.\n\nDid you disable it during compilation? Does your OS have a separately-installable package for it?\n. People are still using PHP 5.3.x??? ;)\n. Yeah, I was kidding. But I've been running 5.5 for most of the year now. I make it a point to stay up-to-date and validate my projects against newer versions sooner rather than later.\n. > I am trying to upload the directory from local path to S3 [\u2026]\nHow many files in this directory? How good is your connection? (Ethernet? Wi-Fi? LTE?)\nI've run into some similar issues before and am curious about certain details that shouldn't make a difference, but might.\n. http://docs.aws.amazon.com/aws-sdk-php/guide/latest/configuration.html#setting-a-custom-endpoint\nHaving said that, AWS makes NO effort to intentionally support anything other than AWS services. If something doesn't work as expected (e.g., Authentication), it's up to you to do the fixing. :+1:\n. Yes, it is a problem. DynamoDB expects all input to be US-ASCII/UTF-8 encoded. You can handle this conversion up-front with mb_string, iconv, or intl extensions.\n. Check out this FAQ:\nhttp://docs.aws.amazon.com/aws-sdk-php/guide/latest/faq.html?highlight=cacert.pem\n. Looking at the cURL error codes:\n\nCURLE_SSL_CACERT_BADFILE (77)\nProblem with reading the SSL CA cert (path? access rights?)\n\nI know that the Poodle vulnerability was patched last week. If you're not running the very latest version of the SDK with the latest cacert.pem file, that may be part of the problem.\n. Have you tried updating Composer?\n. You need to manually set the content type to text/cpp when uploading.\nhttp://docs.aws.amazon.com/aws-sdk-php/latest/class-Aws.S3.S3Client.html#_putObject\n. The file lib/requestcore/requestcore.class.php only exists in v1.x.\n. > What's the \"^\" mean?\nhttps://getcomposer.org/doc/01-basic-usage.md#package-versions\n\nCaret Operator: Very useful for projects that follow semantic versioning. ^1.2.3 is equivalent to >=1.2.3 <2.0. For more details, read the next section below.\n[\u2026]\nThe ^ operator behaves very similarly but it sticks closer to semantic versioning, and will always allow non-breaking updates. For example ^1.2.3 is equivalent to >=1.2.3 <2.0.0 as none of the releases until 2.0 should break backwards compatibility. For pre-1.0 versions it also acts with safety in mind and treats ^0.3 as >=0.3.0 <0.4.0\n. I was wondering when this was going to happen. :smile: \n. Anywhere hosted where this can be previewed? Or do I need to generate the docs manually to see them myself?\u00a0\n\nOn July 31, 2015 at 10:02:33 AM, Jonathan Eskew (notifications@github.com) wrote:\nThis looks awesome!  \n\u2014\nReply to this email directly or view it on GitHub.\n. At a very fundamental level, it's not going to happen. The guts of SDK v2 are tied tightly to Guzzle v3.\nInstead, the current release of the SDK (v3) supports Guzzle v5 and Guzzle v6.\n. @fbm-static: That will absolutely not happen. Sorry.\nHaving said that, security fixes for PHP 5.4 end in approximately 2 weeks. Running any version of PHP earlier than 5.5 is simply irresponsible at this point.\n. This is the way that the value is returned by S3 \u2014 a string containing quotes around the value.\nAFAICT, it has always been there\u2026 no? I can say with 100% certainty that it was there in SDK 1.x.\n. Lame. :(\n@timothy-r: If you are simply looking to generate a cryptographic signature that can be signed with a secret key and passed over the public Internet to another trusted server, take a look at skyzyx/signer. It\u2019s an implementation of the AWS Signature v4 that was extracted from the SDK and the AWS-specific pieces were either removed or repurposed.\nAlternatively, you may also be able to use a JWT library to accomplish this since there\u2019s a tremendous amount of overlap between the two use cases.\n. Was removing this return intentional?\n. Actually, PSR-1 explicitly avoids making recommendations for variable/property names\u2026 only method names.\n. Allowing map() and filter() methods which are chainable would be farking awesome.\n. ",
    "jeremeamia": "@ashishtilara I'm not sure I fully understand your use case. We've never had anyone come to us that required dependencies on a full gamut of different versions of the SDK within the same codebase. I'd like to better understand what the problem actually is and what solutions you might have in mind. You had mentioned including the version in the namespace, but I have not seen any other libraries that are doing this, and it also is not really compatible with PSR-0. Are their any other libraries that you consume that allow you to run multiple versions in the same codebase? I'd be willing to look at those for ideas, but I'm not personally aware of any other libraries out there that support the type of behavior you are asking for. I really would like to see what we can do to help you, but I'm not sure if there is bulletproof solution that is beneficial to everyone else, that doesn't involve you having to do some refactoring at some point.\n. Good catch. Thanks.\n. I responded to your forum post earlier today with a fix: https://forums.aws.amazon.com/thread.jspa?threadID=109616&tstart=0\nThe docs are wrong for the Body option. We will work on getting that fixed.\n. Yes, file_get_contents() should work fine.\nAlso, you can use the EntityBody object from Guzzle in conjunction with fopen:\n'Body' => EntityBody::factory(fopen($file, 'r'))\nThis might work better for larger files since you won't have to load the entire file into memory at once as you would with file_get_contents().\nI apologize for misleading you with my previous comment.\n. Yep, you got it! For any enum-like values we have classes under the Enum namespace for that client. Hopefully we can better call information like that out as we improve our docs.\n. Right here on the GitHub issue tracker is going to be the easiest for us to track. Thanks.\n. We'll check it out.\n. Seems like the file extension is enforced through the uploadify option fileTypeExts. This will not be 100% accurate since it is just looking at the filename and not the mime. I would do some sort of validation on the object after it has been written to S3 and before you use it.\nYou should probably do the URL redirecting in the javascript layer (instead of php) in the onUploadSuccess callback with window.location perhaps.\nIt looks like you're making good progress on this though. Sounds like an interesting blog post once you have it all ironed out.\n. What happens when you turn on errors beforehand? Do you get any error messages?\nerror_reporting(-1);\nini_set('display_errors', 'on');\nrequire 'aws.phar';\nWhat version of PHP are you using?\n. Suhosin does not allow phars by default. You need to enable phars specifically. I've not personally done this before, but I've read that you can do it by adding a PHP ini setting to the suhosin.ini (possibly located at /etc/php5/cli/conf.d/suhosin.ini for Ubuntu).\nsuhosin.executor.include.whitelist = phar\nLet me know how that goes.\n. Thanks for working through that with me. I seems like something we should add to our documentation as well.\n. Instead of self or static, does using Enum work?\n```\npublic static function values()\n{\n    $class = get_called_class();\n    if (!isset(Enum::$cache[$class])) {\n        $reflected = new \\ReflectionClass($class);\n        Enum::$cache[$class] = $reflected->getConstants();\n    }\nreturn Enum::$cache[$class];\n\n}\n```\nThat was the intention of using self.\n. Some of the parameters have changed in name and function from the 1.x version of the SDK. I think what might be happening is that setting the session_lifetime to 0 is actually causing an immediate expiration of your sessions. If you want the session lifetime to be its max, just don't set it all.\nTry this:\n$result = $client->registerSessionHandler(array(\n    'table_name' => 'my_sessions',\n    'hash_key'             => 'userid',\n//  'session_lifetime'     => 0, // Option changed. If max time is desired, do not specify this option\n    'consistent_reads'     => true,\n//  'session_locking'      => false, // Option changed. Locking is also off by default\n    'max_lock_wait_time'   => 15,\n    'min_lock_retry_microtime' => 5000, // Option name changed\n    'max_lock_retry_microtime' => 50000 // Option name changed\n));\nCheck out the API docs for the session handler. The options are outlined in the docs for the factory() method.\nLet me know if this works out.\n. No problem!\n. Well, that's interesting. I'll definitely investigate it. Just to help me reproduce the issue, what are the throughput settings for the dynamodb table and what is the approximate size of your sessions?\n. @etiennea We looked into the issue and verified that the session handler in version 2 can be slower when APC is not installed and enabled. If you do not have APC enabled, we recommend that you do that to ensure the best performance for the session handler. You should also set the apc.shm_size ini setting to at least 64MB. This allows various parts of the client instantiation process to be cached, including loading the service description. We saw that session handler in version 2 of the SDK performs close to twice as fast than in version 1 under these conditions.\n. Nope, APC just needs to be enabled in your INI so that opcode caching is in effect. Is there any other information that would be useful to help us find out why it appears to be slower on your system? What kind of environment are you on? How are you benchmarking the two session handlers?\nI was comparing them yesterday, and with APC enabled, version 2 was always faster than version 1. Here are samples of two scripts I was using to test.\nFor SDK version 1:\n``` php\n<?php\n$time = microtime(true);\nrequire 'aws-sdk-for-php/sdk.class.php';\nrequire 'config-sdk1.php';\n$db = new AmazonDynamoDB();\n$db->register_session_handler(array(\n    'session_locking' => false,\n    'table_name'      => 'sessions',\n    'hash_key'        => 'id',\n));\nsession_start();\n$_SESSION['random_value'] = rand(0, 100);\nsession_commit();\necho microtime(true) - $time;\n```\nFor SDK version 2:\n``` php\n<?php\n$time = microtime(true);\nrequire 'vendor/autoload.php';\n$aws = Aws\\Common\\Aws::factory('config-sdk2.php');\n$db = $aws->get('dynamodb');\n$db->registerSessionHandler(array(\n    'locking_strategy' => null,\n    'table_name'       => 'sessions',\n    'hash_key'         => 'id',\n));\nsession_start();\n$_SESSION['random_value'] = rand(0, 100);\nsession_commit();\necho microtime(true) - $time;\n```\nThese scripts include the time to bootstrap the SDK, instantiate the DynamoDB client, register the session handler, read from the session, and write to the session. I ran them hundreds and hundreds of times, took averages, etc. I also has some other scripts that measured each part of these scripts in more finer grained steps.\n. The update to the Elastic Beanstalk platform in December now supports PHP 5.4. Running the SDK on the new platform should have better results.\n. That's correct. We do not have support for SQS yet. We will be working over time to add every service.\n. @isleshocky77 I read through your posts on the forum. After we get the UploadPart operation fixed. We can help you with the code.\n. The UploadPart operation is fixed in the master branch.\n@jsor, you were seeing some unexpected behavior with your example because the SDK was actually executing PutObject instead of UploadPart since the UploadId and PartNumber params were not be handled correctly. That's why a file was uploaded, but it was only the most recent part, and why ListParts didn't return anything. This should work correctly now.\n@jsor and @isleshocky77, let me know if you have any additional problems.\nYou should look into the Aws\\S3\\Model\\MultipartUpload\\UploadBuilder. We'll try to get some more documentation out about this class, but it might be an easier way to manage your uploads.\n. Can you share you current code on a pastebin? I haven't been having any troubles this morning.\n. The UploadBuilder/Transfer and related classes are not a part of the batching system. \"Source\" is required, because that is the file/body you are going to upload. The UploadBuilder/Transfer chunks and sends the file for you in parts. Like I said, this feature needs more documentation, which we hope to provide at some point.\nYour batch builder example is incorrect though. it should be transferCommands instead of transferRequests. I think you were having trouble with this on a different thread.\nI'm not sure what is causing the signature error in your manual upload example, but it does seem like you are using the EntityBody class incorrectly. If I were doing it manually, I would probably read off of a file handler like @jsor was doing in the original post on this ticket.\n. Here is an example using the Aws\\S3\\Model\\MultipartUpload\\* classes:\n``` php\n<?php\nrequire '/path/to/vendor/autoload.php';\nuse Aws\\Common\\Aws;\nuse Aws\\Common\\Enum\\Size;\nuse Aws\\Common\\Exception\\MultipartUploadException;\nuse Aws\\S3\\Model\\MultipartUpload\\UploadBuilder;\n// Get the S3 client\n$s3 = Aws::factory('/path/to/your/config/file.php')->get('s3');\n// Create a transfer object from the builder\n$transfer = UploadBuilder::newInstance()\n    ->setClient($s3)                        // An S3 client\n    ->setSource('/path/to/the/source/file') // Can be a path, file handle, or EntityBody object\n    ->setBucket('your-bucket-name')         // Your bucket\n    ->setKey('your-object-key-name')        // Your desired object key\n    ->setMinPartSize(5 * Size::MB)          // Minimum part size to use (at least 5 MB)\n    ->build();\n// Perform the upload\ntry {\n    $transfer->upload();\n} catch (MultipartUploadException $e) {\n    echo $e->getMessage() . \"\\n\";\n    $transfer->abort();\n}\n```\nThe multipart upload system also has several events you can hook into. For example, if you wanted to add in some event listeners to help with debugging you could do something like the following.\n``` php\n// Create a transfer object from the builder\n// ... (See the preceding code sample for this portion)\n// Attach event listeners to the transfer object for debugging purposes\n$dispatcher = $transfer->getEventDispatcher();\n$dispatcher->addListener($transfer::BEFORE_UPLOAD, function ($event) {\n    echo \"About to start uploading parts.\\n\";\n});\n$dispatcher->addListener($transfer::AFTER_UPLOAD, function ($event) {\n    echo \"Finished uploading parts.\\n\";\n});\n$dispatcher->addListener($transfer::BEFORE_PART_UPLOAD, function ($event) {\n    $partNumber = $event['command']->get('PartNumber');\n    $size = $event['command']->get('Body')->getContentLength();\n    echo \"About to upload part {$partNumber}, which is {$size} bytes.\\n\";\n});\n$dispatcher->addListener($transfer::AFTER_PART_UPLOAD, function ($event) {\n    $partNumber = $event['command']->get('PartNumber');\n    $eTag = $event['command']->getResult()->get('ETag');\n    echo \"Finished uploading part {$partNumber}, which has an ETag of {$eTag}.\\n\";\n});\n$dispatcher->addListener($transfer::AFTER_COMPLETE, function ($event) {\n    echo \"Upload completed.\\n\";\n});\n$dispatcher->addListener($transfer::AFTER_ABORT, function ($event) {\n    echo \"Upload aborted.\\n\";\n});\n// Perform the upload\n// ... (See the preceding code sample for this portion)\n```\nYou can add additional parameters like ACL and content-type to the upload by using the UploadBuilder::setHeaders() method and the appropriate header keys. We plan on adding additional helper methods to make adding this data easier, but it is currently possible. Also, you can use the UploadBuilder::setAcp() method if you want to use the Acp object for building complex access control lists.\nWe hope to make all of the features better documented in the future.\n. The var_dump() output looks correct. I did notice in your code sample though that you assign the variable as $eTag but then us a lowercase \"t\" when you reference it in the $this->log() call as $etag. Is that why it is not showing up?\n. @isleshocky77 Did there end up being a problem, or was it just a typo?\n. This problem may already be addressed. Please test again with the master branch of the SDK. If the problem still persists, we'll definitely take a look.\n. Just saw that this was posted on the forums as well: https://forums.aws.amazon.com/thread.jspa?threadID=110788&tstart=0.\nLet me know if this is an issue after using the master branch. Thanks.\n. Thanks for verifying. I'm glad it works. We should be tagging a release soon so you don't have to pull off of master.\n. @xmarcos That forum issue definitely looks like clock skew, but we'll look into it more before posting back on the forum.\n. Not entirely sure based on this information. If you do $e->getPrevious() with the MultipartUploadException you received, you will see that it is an InvalidPartException being thrown. This means you are getting a 400-level response from one of the calls to S3 (see the S3 error responses), probably CompleteMultipartUpload. You should try to attach the wire logger ($s3->addSubscriber(\\Guzzle\\Plugin\\Log\\LogPlugin::getDebugPlugin()) to figure out the request that is failing. Seeing the actual request and response going over the wire may help us figure out what's wrong. Let me know when you have more info.\nAlso try a full path in setSource() just to make sure that's not the problem.\n. Do me one other favor. We have a couple of issues fixed in the master branch (including one with the UploadPart operation) that are not in a tagged version yet (coming soon). Could you make sure you are using the latest copy of the master branch? With Composer you can do this by by setting the SDK dependency to:\njavascript\n{\n    \"require\": {\n        \"aws/aws-sdk-php\": \"dev-master\"\n    }\n}\nGo ahead and give that a shot. If that doesn't work, then I can look into it more.\n. Great, I thought it would. Thanks. Keep an eye out for the next tag. Sorry you had to run into that issue.\n. Whoops! Thanks for letting us know. We'll get it fixed.\n. OK, this this should be fixed now. Sorry for the troubles.\n. It's a small but good change. Thanks.\n. Thanks! We are working on some end-user documentation on the docs branch. I'll have to add a page about the session handler soon.\n. More flexibility. The docs branch is setup for static site generation with Sphinx and reStructured Text. That way we have flexibility in the location and look of the docs. There are several PHP projects using Sphinx for docs already. If you install it, you can generate the docs site locally right now with a single command. We just haven't published the site anywhere yet. With the holidays over now, we plan to get moving on this. Restructured text is very similar to Markdown, so we hope to have others contribute to the docs as well once we get the ball rolling.\n. Hey,\nWe recently published our user guide here: http://docs.aws.amazon.com/aws-sdk-php-2/guide/latest/index.html.\nI have written up a guide to the session handler that we'll be adding soon. Care to review? You can see the content here: https://github.com/jeremeamia/aws-sdk-php/blob/399d736a347d190e3d10883db413c371f593fd66/docs/feature-dynamodb-session-handler.rst. Let me know if you think anything is missing or could be clarified. Thanks.\n. I don't want to suggest auto_prepend_file, because as Michael points out, it's not always good for every use case. It also makes it hard to reuse the same DynamoDB client in other parts of your app, which you definitely should try and do since you can take advantage of the persistent connection created the first time it's used and not have to make the SSL handshake again.\n. I added some more advisory text in this commit. Do you think that this helps?\n. @ralph-tice Thanks! \n@amenadiel To address your comments\u2026\n\nI've seen no mention for session autostart setting\n\nThe session.auto_start setting is not meant to work with custom session handlers built for use with session_set_save_handler().\n\nat least for me dynamodb sessions just doesn't work\n\nWhat do you mean by this? Are you saying that you think there is a bug or that it just does not fit your use case?\n\nI'm pretty happy with elasticache handling my sessions though\n\nSure, ElastiCache can also be a good solution for sessions.\n\nI'm eager to see more sdk sintactyc sugar upcoming related to elasticache\n\nDo you have any specific ideas for what we could do in the SDK?\n\na good drop in replacememt for vanilla memcache drivers.\n\nThe ElastiCache team has a custom memcache driver that supports auto discovery of all the nodes in your cache cluster. See Installing the ElastiCache Cluster Client for PHP in the ElastiCache service's Developer Guide.\n. Closing this issue now. @amenadiel feel free to comment back on here if you get a chance. Thanks.\n. The guide is now published: http://docs.aws.amazon.com/aws-sdk-php-2/guide/latest/feature-dynamodb-session-handler.html\n. Awesome! Thanks for doing this. I believe @mtdowling has already reached out to you with a CLA, correct?\n. Yeah, why don't you send two separate ones, that way we can get this approved and use it for some other clients.\n. Thanks for letting us know. We'll look into it. The API docs are all generated based on the service descriptions. There is likely something we need to tweak in one of our generation processes, so there isn't really anything to submit a PR to. Thanks for offering though. :-)\n. We're still working on examples and getting the API docs corrected.\nIn the meantime, here is another example using the BatchGetItem Iterator and \"calculated\" keys.\n``` php\n<?php\n$keys = array();\nforeach (range('a', 'z') as $key) {\n    $keys[] = array('HashKeyElement' => array('S' => 'key-' . $key));\n}\n$items = $dynamodb->getIterator('BatchGetItem', array(\n    'RequestItems' => array(\n        'your-table-name' => array(\n            'Keys' => $keys\n        )\n    )\n));\nforeach ($items as $item) {\n    print_r($item);\n}\n``\n. I'm not sure I if what you are saying is correct. Have you tested out what you are describing?Item::fromArrayusesAttribute::factoryto make intelligent guesses at what each attribute's type should be. Nothing in the example you are referring to should result in there being aStringSet`. It's possible I might also be misunderstanding you.\n. OK, we'll look into that too. Thanks for letting us know.\n. Hey, no worries! Thanks for looking back into it for us. :smile:\n. It sure is, Dr. Laravel. :smile:\nUnfortunately we cannot share any specific roadmaps, but since service coverage is pretty much the top priority for us right now, it hopefully won't be too long.\n(Sorry about the \"Dr. Laravel\"-thing, but it does have a ring to it.)\n. SQS support is now complete in master!\n. It'll happen soon! We are actively working on expanding service support. Unfortunately, I can't give you an exact timeline.\n. Tarzan Core? Wow, you've been using the SDK a long time, haven't you. :smile: I'll make sure to post back here when we have SimpleDB ready to go. FYI, we've started support for this in the refactor_descriptions branch.\n. Hmmm... interesting project, but no thanks.\n. Thanks Todd. We'll look into making this work better.\n. See the API docs for DescribeInstances.\nValues should also plural (i.e.,Value should be Values). Sorry we didn't catch that the first time around. It should look like:\n'Filters' => array(\n    array(\n        'Name'   => 'instance-state-name',\n        'Values' => array('running')\n    )\n)\nThere is also an example in the EC2 integration tests.\n. Added it here: 6ea519a. Let me know if that works for you. Thanks for the suggestion!\n. I'm assuming you are looking at the upload example on the README, right? Config files are addressed in the README as well: https://github.com/aws/aws-sdk-php#using-a-custom-configuration-file.\nAlso, you are not required to use a config file, here is an example without one.\n. No problem!\n. For [1], I just committed something that should fix that: 807cec5. Seems related to #13.\nFor [2], hmmm... off hand, I have no idea, so I'll have to investigate. If you are able to debug a little and find out which part of the session handler (writes, reads, etc.) it is happening in that would help. Also, some questions:\n1. How often does this happen?\n2. How big are your sessions?\n3. What is your table throughput?\n4. Do you know for sure it is happening in the session handler?\n. Yeah, I'd try using a shutdown function or custom exception handler and see if you can get a better stack trace or debug_backtrace and see what part of the session handler is being used before the cURL exception occurs. Thanks. I'll look more into it this week.\n. Thanks for letting us know. In this case, it is actually intentional, since we do not have SQS in any tagged release at this time. Once we tag our next release we will update the website, since at that time SQS (and SNS, SES, Auto\nScaling, and CloudWatch) will available through all mediums, including GitHub, Composer, PEAR, and the phar.\n. @tdondich We are shipping a zip starting today. You can grab it at http://pear.amazonwebservices.com/get/aws.zip.\nWe will also be adding this section to the user guide: https://github.com/aws/aws-sdk-php/blob/master/docs/installation.rst#installing-via-zip\n. Thanks for pointing this out. We'll look into it.\nPossibly related: https://forums.aws.amazon.com/thread.jspa?threadID=117580&tstart=0\n. The PHP SDK supports the entire SNS API. The functionality you are describing doesn't sound immediately familiar to me, but for a question like this you should consult the Amazon SNS documentation or ask a question of the Amazon SNS forum. If the service supports it, then so does the SDK.\n. Ah ha, I see what you are asking about now. Neither version 1 or 2 of the SDK provides that functionality, so as of right now it would be something you'd have to implement yourself. There are two pages in the documentation that might help with this though: Verifying the Signatures of Amazon SNS Messages and Example Code for an Amazon SNS Endpoint Java Servlet.\nI will definitely make note that you requested this feature and bring it up with the team to see if it's something we want to add. However, if it's something you want to take a crack at, we wouldn't mind a pull request either. :smile:\n. Awesome, sounds great! Thanks.\n. Thanks! This looks great. I'll take a deeper look to come up with some suggestions and testing advice. Do you have any example code for using it? A gist maybe?\n. This is now a part of the SDK: https://github.com/aws/aws-sdk-php/tree/master/src/Aws/Sns/MessageValidator. Here is an example of how to use it.\n``` php\n<?php\nrequire 'vendor/autoload.php';\nuse Aws\\Sns\\MessageValidator\\Message;\nuse Aws\\Sns\\MessageValidator\\MessageValidator;\n$message = Message::fromRawPostData();\n$validator = new MessageValidator();\nif ($validator->isValid($message)) {\n    echo $message->get('Message');\n} else {\n    echo 'INVALID MESSAGE';\n}\n```\n. @poisa Good idea! Using a client with the caching plugin should make it so the request to fetch the cert doesn't have to happen on every request. Here is the most up-to-date page on the caching plugin: http://guzzlephp.org/plugins/cache-plugin.html. @BenMorel Let me know if that works out for you.\n. I am not sure. I would ask that question on the Amazon SNS forum. I will also try to follow up on this internally.\n. Thanks for that info @poisa. Make sure to find me at the conference and say hello!\n. Looks great. We'll get to you with some more feedback tomorrow. I've also submitted this to our internal review process. We will find out if we need you to sign a CLA. Thanks again.\n. Alright, I gave this a good look through, and it looks really good. Thank you! The logic all seems to be in place and you have some nice serparation of concerns. I do have some suggestions though, so bare with me. First, There are definitely some names I want to change. Second, I see what you did with the strategies, which is good, but I think it might be better to organize it a little differently, which will also make the whole thing eaiser to test as well.\nCurrently, you have a setup like this (disregarding the exceptions)\n- Aws\\Sns\\Validator\\SnsValidator\n- Aws\\Sns\\Validator\\Strategy\\StrategyInterface\n- Aws\\Sns\\Validator\\Strategy\\NotificationStrategy\n- Aws\\Sns\\Validator\\Strategy\\SubscriptionConfirmationStrategy\nAnd you are using stdClass to pass around the message data.\nI think it would make more sense to have message classes that encapsulate the data and creates the strings to sign. I would prefer a structure like this:\n- Aws\\Sns\\MessageValidator\\MessageInterface\n- Aws\\Sns\\MessageValidator\\AbstractMessage\n- Aws\\Sns\\MessageValidator\\NotificationMessage\n- Aws\\Sns\\MessageValidator\\SubscriptionConfirmationMessage\n- Aws\\Sns\\MessageValidator\\MessageValidator\nBasically the AbstractMessage would contain the factory logic for creating the concrete message classes, include common logic like getting the data, and defining an abstract getStringToSign() function. This function would be the same as the buildSignatureBody method of your strategy classes but would be defined in the concrete message classes instead. I'd prefer the method to be named getStringToSign() because we typical use the term \"string to sign\" when we're dealing with the other signature classes in the SDK.\nSince all of the message and message factory logic will be in the Message objects, you can update your validator object to be a stateless object that basically only has one method: validate(). The constructor should still check for the presence of openssl, and all of the openssl calls should be in that class only, like you have it now.\nThe benefit of this new setup is that it is much easier to unit test. The message class is very straight forward to test, and the only tricky stuff would be in the validator. We would gain the ability to mock the Message object though, which could help with testing the validator. It's also not a difficult change since it is mainly just moving existing code around.\nAs far as usage goes, it would like only a little different from what you showed me yesterday. This is what I would picture:\n``` php\n<?php\nuse Aws\\Sns\\Model\\MessageValidator\\AbstractMessage;\nuse Aws\\Sns\\Model\\MessageValidator\\MessageValidator;\n$message = AbstractMessage::fromArray(json_decode($HTTP_RAW_POST_DATA, true));\n$validator = new MessageValidator();\nif ($validator->validate($message)) {\n    echo 'Yay! The message came from AWS.';\n} else {\n    echo 'Uh oh. The message may NOT have come from AWS.'\n}\n```\nSo again, thank you so much for doing this! I'd love to see my suggestions incorporated into your code if you have time. If not, I totally understand, and I can take what you've done and work on it myself. As I see it, you've done the hard part for me already. Just let me know what you feel like doing and if your up to doing a little more work.\nI do need you to sign a CLA for me though. Please send an email to the email address I have listed on my GitHub profile, and I'll prepare that for you. Thanks!\n. This is looking really good! I made a pull request to your pull request from my personal fork with some additional changes and a start on the unit tests.\n. Sounds good. Thanks for your work on this. Just so you are aware, I added some todos at the top of this PR (not all of them are for you; they are for my benefit). Let me know if you have any questions or concerns.\n. I created a branch on the repository called feature-snsmsg with all of our changes so far. I think you might be able to change this pull request to point to that branch, but you'll definitely want to pull from that branch as well. I updated the tests for the SNS Message Validator and rebased to get all of the current code from master. You can quickly see my changes on this commit: b7f70c0.\n. This is now a part of the SDK: https://github.com/aws/aws-sdk-php/tree/master/src/Aws/Sns/MessageValidator. Thank you! since I merged this into a feature branch before merging to master. I'm going to go ahead an close the PR.\n. @poisa FYI: For Version 3 of the SDK, we've broken the SNS Message Validator out into a separate project: https://github.com/aws/aws-php-sns-message-validator\n. We are looking into this. Thanks for pointing it out.\n. Hello!\nYou can use getCredentials() method to get the Credentials object, which has setters for the keys. See the section in our brand new user guide called Manually Setting Credentials.\nAs far as mocking goes, we mock client-server interactions all throughout the SDK tests. One way to do this is to mock Command objects to return a particular result (Model object). Another way is to use Guzzle's mock response feature like we do in this S3 iterators integration test.\nLet us know if you need more help or if you have any suggestions about how we can improve our docs or code to facilitate your use case.\n. Well, you are right, I don't think that works right after all. When you do $s3 = S3Client::factory(); It creates the default credentials object which is a RefreshableInstanceProfileCredentials object. Even though you've set explicit credentials, an expiration has not been set, so the RefreshableInstanceProfileCredentials considers the credentials as expired. Because of this, it tries to refresh the credentials by looking for an Instance Profile in your environment, which is only something available on a configured EC2 instance.\nSo, to get around this limitation right now you can add $credentials->setExpiration(null) like this:\n``` php\n$s3 = S3Client::factory();\n$credentials = $s3->getCredentials();\n$credentials->setAccessKeyId('key');\n$credentials->setSecretKey('secret');\n$credentials->setExpiration(null);\n```\nThat should tell the RefreshableInstanceProfileCredentials object that your credentials don't expire. You should only have to so that once regardless of how many times the credentials are modified.\nHowever, it does show that having a public setCredentials method would be handy so that you don't have to worry about refreshable credentials, if you don't need them. Then, in the future you could do:\nphp\n$s3 = S3Client::factory();\n$s3->setCredentials(new Credentials('<key>', '<secret>'));\nWe will add this feature and updating the docs to our TODO list.\n. Great! I also went ahead and updated the master branch to add a setCredentials method. See 23092ec.\n. Thanks Ryan!\n. OK, all good now?\n. I will add a task for myself to add a BatchGetItem example in the user guide. The API docs do still have an outstanding issue (#28) though.\n. :ship:\n. We don't have any examples in our user guide yet, but you might find the integration test useful: https://github.com/aws/aws-sdk-php/blob/master/tests/Aws/Tests/Ses/Integration/IntegrationTest.php#L115-L130.\n. That looks awesome. I'll have to look at the .rst file more closer in the morning.\n. It seems like we might have an issue with this particular scenario, as it seems like the Content-Length is not set properly, even when it's provided. We'll look into this and see what we can do. Sorry it is causing you problems.\n. This issue is also duplicated on the AWS forums. We will track the issue here and post the resolution on the forums.\n. Interesting. We'll definitely look into this.\n. @SteveEdson It looks like @skyzyx is right. Just because the instance is running does not mean you can SSH into it. There is nothing in the API that allows you to know whether or not you can SSH into. The way to do this is with a custom waiter. The process of creating a custom waiter is described in the Custom Waiters section of the user guide. @skyzyx are you interested implementing this?\n. We do not currently plan on implementing this as part of the SDK.\n. Ha ha, no worries. At least it is an easy fix.\n. Yeah, that definitely looks like a problem. Thanks for letting us know.\n. Thanks for letting us know. We'll look into getting this fixed.\n. Did you get everything working?\n. Awesome. I appreciate your work here very much. We'll get it merged in soon after the internal review is complete.\n. I have been asked to see if you are willing to sign a CLA (see our CONTRIBUTING.md for more info). Please send me an email using the address on my GitHub profile and I'll send you the CLA. Thanks.\n. @tomsmaddox Can I get your email address? Please send an email to the email address on my GitHub profile. Thanks.\n. CLA received and approved. Will merge this in soon. Thanks.\n. Merged. Awesome. That PR was perfect. Thanks again.\n. Thanks for letting us know. We'll get that fixed soon. In the meantime (and if you are not using the phar), you can try changing NetworkInterfaceSet to NetworkInterface on line 6828 of the EC2 service description.\n. Hmmm... there seems to be a couple of serialization problems here. We'll definitely try to get this fixed soon.\nI pushed a temporary fix to my personal fork of the SDK, so I could show you the diff: https://github.com/jeremeamia/aws-sdk-php/commit/136280c3bea5d5105e6cad0efc53f55691216a5b. These changes should fix the issue.\nHowever, you may have an issue with the parameters you are sending in your code. I fixed up the structure of your NetworkInterfaces parameter here: https://gist.github.com/jeremeamia/d958021e02b4c22fdc36. While this request doesn't succeed for me (since these values are specific to your account), it no longer was triggering parameter structure validation errors from the service. So give it a shot, and let me know if that works.\nI'm sorry you are running into this issue, but thanks for bringing it to our attention and helping us debug.\n. Interesting. I'm definitely not seeing that error anymore. Can you share the code that you are using? You said you are using PEAR; how are you including the SDK?\n. What source file are you editing to make the changes I have suggested? Are you editing the actual aws.phar somehow? Otherwise, I don't think it will pick up your changes. If you send me an email to the address on my GitHub profile, I could send you a copy of the phar with the changes applied.\n. This is a separate issue, so will address it on #104.\n. Well, that explains why it wasn't installing for me last week. Thanks.\n. Thanks Carl!\n. Good question. We have been trying to figure this out as well. We'll talk about it and get back to you. Any specific ideas you wanted to discuss?\n. Well, we'll definitely consider it. The Flow framework is only available in the Java SDK, but we've had requests for it in other SDKs as well, now including PHP. It's a pretty significant undertaking though, so we'll see what happens.\nWhat we're really trying to figure out is where higher-level abstractions should live: in the SDK itself or in separate packages (pros and cons to both). Also, maintaining many higher-level abstractions (for 30+ services) is a lot of work, too.\nHave I mentioned lately that we're hiring? Anyone want to come work with us in Seattle?\n. Nice meme! I agree that it could be useful, but there are also some pitfalls. I'll have to talk to @mtdowling about this.\n. After thinking about this I've decided that it is probably not a good idea. There are many operations that accept as input, array structures from the output of other operations. This would complicate those scenarios. A chainable interface would definitely be nice, IMO, but might make more sense in a higher-level interface. Of course, this brings us back to #90.\n. Yup, that is a mistake, and is also pointless. For __call magic methods, PHP always sets $args to an array. Tests are still passing, so I'll go ahead and merge. Thanks.\n. Thanks for the update. I was going to look into this today, so you definitely saved me some time. I'm still curious about the issue though. How was mod_rewrite making this happen?\n. Looks good.\n. You will want to use transferCommands if you are batching commands, not transferRequests, which is for batching requests. I don't think there is a good formula for selecting $batchsize though. For you, it seems like 220 woud be too high, since you are experiencing problems with that many at once. You may have to experiment to find the value that gives you the best performance.\n. The transferCommands and transferRequests methods basically just determine which strategies to use for the batching and make sure that the proper classes are instantiated by the BatchBuilder.\n. Everything looks good! Nice work! :+1:\n. Everything got merged for 2.4.0\n. That is a strange error to get in this situation. I'd be interested in seeing the raw XML body from the response. Try attaching the basic wire logger to the client before creating the iterator:\nphp\n$client->addSubscriber(\\Guzzle\\Plugin\\Log\\LogPlugin::getDebugPlugin());\nIf you run your code from the command line, then this will output the raw requests and responses so you can see them. We should checkout the XML response and see if there is anything strange in the XML.\n. Interesting. Were you not able to print out the actual XML response body with the logger? So far, all I know from what you've provided is that there is an exception thrown when the XML response is parsed by SimpleXML. Part of the stack trace mentions that there is \"Extra content at the end of the document\" as well, which seems very strange. I need to see what the XML being returned actually looks like to see whether or not there is an issue with the actual response from the service or with how the SDK is trying to read it.\nSo let's try working in the following piece of code to see if we can get the actual XML body that is causing the trouble. To do this we'll use Guzzle's history plugin. We'll use the iterator like normal, but make sure to catch the exception when the bad parsing occurs and print out the offending XML.\n``` php\n$iterator = $client->getIterator('ListObjects', array(/ ... /));\n$history = new \\Guzzle\\Plugin\\History\\HistoryPlugin;\n$client->addSubscriber($history);\ntry {\n    foreach ($iterator as $object) {/ ... /}\n} catch (\\Guzzle\\Common\\Exception\\RuntimeException $e) {\n    echo $e->getMessage() . \"\\n\";\n    echo $history->getLastResponse();\n}\n```\nThanks for helping me debug this. Let me know what results you see.\n. Were you able to look into this some more?\n. OK... Well, if you run into something like this again. Let us know.\n. Sorry we have responded to this issue sooner. The problem you are having is with your 'Steps' parameter. The docs describe this parameter like this:\n\nSteps - array([hash]) Array of associative arrays where each associative array matches the following schema - A list of steps to be executed by the job flow.\n\nSo it should look more like this:\nphp\n$args = array(\n    // ...\n    'Steps' => array(\n        array(\n            'Name' => $this->_emr_job_name,\n            'HadoopJarStep' => array(\n                'Jar' => $this->path_to_jar,\n                'Name' => $this->_emr_job_name,\n                'ActionOnFailure' => 'TERMINATE_JOB_FLOW',\n                'MainClass' => $this->_hadoop_class,\n                'Args' => $this->_args,\n           ),\n       ),\n    ),\n    // ...\n);\nDoes that make sense? Do you have any suggestions on how the documentation can be clearer?\n. There is an issue with one of the ElastiCache iterator configs. Other than that, everything looks good.\n. Are you just looking for this: https://github.com/aws/aws-sdk-php/commit/e5e4384c26b58915ff2eb38f775a00db6e93a587 ?\n. Looks good.\n. We don't currently document which specific exceptions are thrown per operation. Each service client does have a base exception you can catch though (e.g., Aws\\S3\\Exception\\S3Exception).\n. Cool. Seems to cover everything I can think of off of the top of my head. Looks easy enough to add more later if we want. One of the lines says date.timezone in is currently set to '' but should be set to true., but I think the suggestion to set it to true is not right.\n. This is possible from the response object. Guzzle mentions it in its docs, but it might be worth mentioning in ours as well since SimpleXMl was the result format in SDK1.\nphp\n$command = $ec2Client->getCommand('TheEc2OperationName', array(/* ... args ... */);\n$result = $command->execute();\n$responseXml = $command->getResponse()->xml();\n$responseXml should be a SimpleXMLElement object.\n. The SDK will throw an exception if an operation is unsuccessful. If no exception is thrown, then you can assume the operation succeeded. This behavior is documented in the Quick Start Guide. From the content of your question, I am assuming you are using the DynamoDB client. If this is case, you should catch \\Aws\\DynamoDb\\Exception\\DynamoDbException if you want to handle errors.\nNow, BatchWriteItem is a little different. If you attempt to write more items than your provisioned throughput capacity allows, DynamoDB will do as many as you can and return the \"UnprocessedItems\" back to you in the response. So for BatchWriteItem, you could have a successful operation, but you still need to verify that all of your items were processed. See the API docs for BatchWriteItem. To make this easier on yourself, you might consider using the WriteRequestBatch abstraction.\n. We have created a few third-party modules ourselves (with community help), so please take a look at these to see what kind of things we have done.\n- https://github.com/aws/aws-sdk-php-silex\n- https://github.com/aws/aws-sdk-php-laravel\n- https://github.com/aws/aws-sdk-php-zf2\n. Looks good to me. It seems to address #120 and is not a BC for anyone currently catching the Guzzle CurlException.\n. Thanks for letting us know about this and for your ideas about potential solutions. We'll look into this and see what will work best. Increasing the CURLOPT_CONNECTTIMEOUT is a good temporary solution.\n. This issue was fixed via #131 and was tagged in 2.4.4. Let us know if this worked out. Thanks.\n. :+1: \n. No BC, right?\n. We are looking into this, but it looks like there definitely is a problem.\n. Hey @doapp-ryanp, I am working on a fix for this right now in PR #139. I am doing some more testing before I merge it into master, but I wanted to let you know about it in case you want to give it a try.\n. OK, I merged this into master now, so stay tuned for a release sometime soon.\n. Now released in 2.4.4.\n. Related: http://www.php.net/manual/en/function.strval.php#75496\n. I don't think there is much we can do in the SDK itself to convert floats to strings with the desired precision, but sprintf should help you in your case. I'm going to go ahead and close this. If you come up with any ideas though, let us know.\n. That will not work. The PHP SDK mirrors the actual API exactly, which requires the types be specified, even for values nested in M and L values. We are discussing ideas in #357 including one I have in this gist. If you have ffedback or ideas you'd like to share, please do so on #357.\n. Any update, @maknz?\n. CURLOPT_DEBUG is not a real cURL option. Also, curlopts should be passed in as an associative array. I fixed the previous response in the issue to use CURLOPT_VERBOSE and use the correct associative array syntax. Sorry about the confusion. Try the following, and let me know if it works out better.\nphp\n$log = fopen('/path/to/log/file.log', 'a');\n$s3->getConfig()->set('curl.options', array(\n    CURLOPT_VERBOSE => true,\n    CURLOPT_STDERR  => $log\n));\n. :+1: I like it. I'll submit this for review.\n. Review looks good. I just need to confirm that you are willing to submit this code under the Apache 2.0 License.\n. Thanks!\n. :ship: \n. :+1: Seems like this should help with interoperability with frameworks. I'll submit it for our internal review. I also need you to confirm that you are willing to submit this code under the Apache 2.0 License. Thanks.\n. Thanks for being a good sport. :-)\n. Thanks!\n. Look fine to me. Thanks.\n. Endpoint is also documented as being part of the response in the service's API documentation for RestoreDBInstanceToPointInTime. It's possible that it is only provided in the response if there is a value for it. Were you expecting this value for a particular use case?\n. I will bring this up with RDS team and see if I can find out any more information about it.\n. Interesting... we'll look into this. This does not cause an error with S3, but, like you have observed, it doesn't seem to match the docs.\n. The \"subfolder\" should be a part of the second argument, which should be the full object key. Try this:\n$key = 'folder/' . $filename;\n$s3URL = $s3->getObjectUrl('bucket', $key, '+1 hour');\n. This should solve the issue you are having. Feel free to reopen if you have any additional problems.\n. :ship:\n. I'll have to look into this more tomorrow, but you should try attaching the wire logger to see if the actual request and response look correct. Do the following sometime after you instantiate the SNS client, but before you run publish:\nphp\n$sns->addSubscriber(\\Guzzle\\Plugin\\Log\\LogPlugin::getDebugPlugin());\nYou should get some output to your console/terminal that includes the raw request and response.\n. Ah ha, great. Sorry I was not able to help much; I've been traveling. I'm glad that you found the solution though. Wouldn't you mind sharing an example of what worked?\n. Thanks. I should have recognized that issue earlier.\n. This looks like it should be reported on the version 1.x SDK: https://github.com/amazonwebservices/aws-sdk-for-php. We no longer consistently make updates to that project anymore, but it would be good to make sure the issue is reported there just in case.\n. Also related: #163\n. Thanks for reporting this. We'll look into this to confirm the issue and proposed fixes, and also contact the team that publishes the test suite.\n. What version of the SDK are you using?\n. The exact SDK version can be found by echoing \\Aws\\Common\\Aws::VERSION.\nYou might consider doing a composer update if you haven't in a while, in case there have been fixes for this problem in Guzzle of the SDK.\nBut let's also determine if this is a problem. What operation are you performing? What does the exception message you are getting tell you? How often does it happen?\nYou could also gather the cURL verbose output of a failed request, which could give us more information about what might be happening. To set that up, you could do something like this:\nphp\n// Assumes you are using the Laravel service provider as mentioned by @matisoffn\n$client = Aws::get('s3', [\n    'curl.options' => [CURLOPT_VERBOSE => true],\n]);\n. You're right, it is AWS. However, you still haven't told me anything about the error (e.g., what is the message) or provided the cURL verbose output. Are you having trouble getting this information?\nAlso, does the error happen every time? What code are you actually running?\n. Could you catch the exception, so we can get more information about it?\nphp\ntry {\n    $s3->putObject(...);\n} catch (\\Exception $e) {\n    echo $e->getMessage(); // <-- what does this echo?\n}\n. Does just echo $e; give you any more info?\nWell, curl error 60 means: \"CURLE_SSL_CACERT (60) Peer certificate cannot be authenticated with known CA certificates\". That would make sense why it is happening every time and not retrying. Where are you running this code? Does your bucket name have a \".\" in it?, and have you specified a region in your config? Also, check out the ssl.certificate_authority setting in our SDK user guide.\n. It hangs indefinitely? That's strange.\n1. Have you had this problem in the past or just with the latest version of the SDK?\n2. Where are you running the code? \n3. Try attaching the wire logger to see if you can get any more information about what is being sent and received over the wire when this problem occurs.\n4. Can you share the exact code snippet you are using?\n5. Is the queue empty?\n. Yeah, sounds like a network issue. I'm glad it's working now.\n. I'll check this out. Thanks for letting us know.\n. Are you getting any errors when you call addPermission?\nAre you using the correct ID? It needs to be the AWS Account ID (without hyphens), not the access key. The account ID is a 12-digit number.\n. Well, I'm not quite sure what is happening here. This doesn't really seem like an issue with the SDK, but you can try attaching a wire logger to verify that the request is being sent as expected. If something looks fishy in the request or response, let me know, and I'll dig into it.\nIt may be a good idea to post this issue on the SQS forum.\n. The SDK includes a script called compatibility_test.php in the root of the repo that can be run from the CLI or web server. This includes, among other things, a check for the required minimum cURL version. See http://docs.aws.amazon.com/aws-sdk-php/guide/latest/requirements.html#compatibility-test\n. @coredumperror, I talked to @mtdowling some more about this, and we decided that there was a simple way to offer the older phars and zips going forward, without much effort on our part. To do this we will start publishing the phar and zip for each future release through the GitHub release tool. If you look at the 2.4.10 release I did this morning you will see the download buttons for the 2.4.10 aws.zip and aws.phar.\nThough the SDK does not currently follow semver, it does follow a similar form of semantic versioning. I have included a new FAQ item in our user guide that explains our versioning scheme, in case you need help deciding which versions to use for your module.\nKeep us updated about your Drupal module work. It sounds great.\n. You need to sign up for the Kinesis limited preview if you wish to use the service. I'm not sure how soon we will have a PHP client though due to the limited preview status. I'll leave this issue open and make sure to post here once it's available.\n. Looks good. Let's double check the new iterators though.\n. Based on what you have said, this sounds like the expected behavior of Amazon SES. In the Verifying Domains in Amazon SES section of the SES Developer Guide, it says:\n\nWhen you verify an entire domain, you are verifying all email addresses from that domain, so you don't need to verify email addresses from that domain individually.\n. Thanks for the review. I'll tidy up.\n. Finally fixed the failing test on TravisCI (caused by a timezone difference), rebased this branch, and merged into master.\n. That seems strange. Thanks for bringing this to our attention. I'll try and reproduce the issue and see if I can figure out what might be going wrong.\n. I'm sorry that you are have a hard time with this, and I'd definitely like to help. To start, can you share the code you are using to instantiate the SQS client? (Please replace any sensitive information, like credentials, with fake data.)\n. If you are sure that your keys are correct, then here are a few things that I would look into.\n- If your user is an IAM user, please make sure that the user's permissions allow you to create queues.\n- You are using version 2.3.4 of the SDK. The latest is 2.4.11. I suggest that you update your SDK.\n- The proxy configuration may not be right. Also, you should take a look at the blog post by @mtdowling: Sending requests through a proxy. He describes a way to configure the proxy that is slightly different than how you are doing it.\n- The proxy may be doing something weird to the request. You can see what the SDK is sending over the wire by attaching the wire logger to your client. If you have some way to compare that to what your proxy sends out, then you could see if they are different.\n. Thanks!\n. Nice catch! Tracking down problems in a session handler is not particularly easy, since it happens outside of the rest of your code flow.\n\nWhen I wrote this, I must have assumed that either (1) DynamoDB was fine with empty values, or (2) session_encode() always returns something non-empty. Neither of those are true though, apparently.\nIt looks like this PR should fix the problem. Thanks.\nI just need to run the PR by our legal team, and I'll get it merged in for our next release.\n. Code review complete. Closing, cleaning history, and merging into master locally.\n. Thanks for reporting this. We'll look into it.\n. The SDK does not fully support the CloudSearch document or search APIs. There are some technical reasons why we don't at the moment, but I'd like to see us support it more fully as well. I'll mark this as a feature request and bring it up with the team.\nHowever, you should know that the document API is very simple. It requires only a basic HTTP client, like Guzzle, which is included with the SDK. Requests to your document endpoint do not need to be signed, since they run on your own AWS infrastructure provisioned by CloudSearch, and are restricted by IP.\nTake a look at the following links to help you get started:\n- Making Document Service Requests in Amazon CloudSearch\n- Amazon CloudSearch Document Service API Reference\n- Guzzle Documentation\nUsing this information, I came up with a short code example of how you can upload documents. Please keep in mind that I have not tested this, so it may need a little tweaking. If you cannot get it to work, please let me know and I will look into it further.\n``` php\n// Setup a CloudSearch Document client\n$endpoint = 'http://doc-{domain_name}-{domain_id}.us-east-1.cloudsearch.amazonaws.com/{api_version}';\n$client = new \\Guzzle\\Http\\Client($endpoint, array(\n    'domain_name' => 'YOUR_CLOUDSEARCH_DOMAIN_NAME',\n    'domain_id'   => 'YOUR_CLOUDSEARCH_DOMAIN_ID',\n    'api_version' => '2011-02-01',\n));\n// Upload documents\n$request = $client->post('documents/batch');\n$request->setBody('[{\"your\":\"documents\"},{\"and\":\"data\"}]', 'application/json');\n// OR: $request->setBody(fopen('/path/to/your/documents', 'r'), 'application/json');\n$result = $request->send()->json();\n```\nAnd, of course, you will want to make sure that your documents are prepared in the correct format for CloudSearch.\n. @greggilbert @gregholland @paulstatezny @skyzyx The SDK has support for searching and uploading documents via the new CloudSearchDomainClient as of version 2.6.9.\n. I have no idea. That would be a question for the CloudSearch forum. :smile:\n. That is strange. What version of the SDK and PHP are you using? I see that you are using the phar; is APC enabled?\n. Are you running this script from the CLI? If so, set apc.enable_cli to Off. We've come across this issue in the past, and we have no other solution than to disable apc.\n. Great, that is a good solution as well. I should add an FAQ item about this.\nThere are a couple of other potential solutions to consider as well:\n1. Don't use the phar. Install the SDK via Composer or the zip.\n2. Use PHP 5.5. It has Zend OpCache built in, which is better than APC, and we have not observed this problem in that environment.\n. Added a FAQ item: http://docs.aws.amazon.com/aws-sdk-php/guide/latest/faq.html#why-am-i-seeing-a-cannot-redeclare-class-error\n. Strange. I haven't seen that before. Anyways, this is one reason we recommend not using the phar if you are using APC. Your filtering solution is good though.\n. Merged in locally\n. Closing this. But if you still have ideas, feel free to open an issue.\n. Hey, I just tried this code, and it seemed to work fine. You shouldn't need to specify ContentLength or add any curlopts.\n``` php\n$s3 = S3Client::factory(['key' => '*', 'secret' => '*']);\n// Add wire logging to see raw request and response\n$s3->addSubscriber(\\Guzzle\\Plugin\\Log\\LogPlugin::getDebugPlugin());\n$s3->putObject([\n    'Bucket' => 'my-bucket',\n    'Key'    => 'subdir/',\n    'Body'   => '',\n]);\n```\nAlso, why are you creating empty pseudo-folders in the first place? I can't think of any good reason why this would be needed.\n. Hmmm... that is an interesting lead, but makes sense since @bikegriffith was only seeing the issue when using Boris, which does use pcntl.\nWhat happens if, instead of using the same client to make the 2nd request, you instantiate a new client object to make that request?\n. Looks good to me, other than Travis saying it failed.\n. LGTM!\n. @mtdowling How does this look? You should probably try building it locally too and make sure I didn't do something silly.\n. Weird. I'm glad you tracked that down. :ship: \n. @mtdowling I think this branch is ready to go. Can you take another look? Sorry, it's a beast of a PR.\n. Note: Unit and integ tests are passing and all iterator configs are up-to-date.\n. Thanks @mtdowling. Here we go!\n\n. We probably shouldn't execute that APC logic on the HHVM. You can use the HHVM_VERSION constant in PHP to check for HHVM.\n. :ship:\n. :ship:\n. This stack trace is odd because the name of the class in # 4 of the trace is being truncated (e.g., Guzzle/Http/Curl/C). Are you sure you provided the entire stack trace?\nHow fast are you doing the requests? Maybe there is some kind of network saturation going on. Can you trace this down to a cURL error number?\n. @jehord We were never able to reproduce this issue. If you are are having a similar problem, please open a new issue with details to help us reproduce and debug.\n. Can you think of a good place for this doc update? I'm not sure if we've documented that limitation, other than the issue with the 32-bit systems, which is still an issue.\n. SDKv2: http://docs.aws.amazon.com/aws-sdk-php/guide/latest/faq.html#why-can-t-i-upload-or-download-files-greater-than-2gb\nSDKv1: https://github.com/amazonwebservices/aws-sdk-for-php/blob/master/_docs/KNOWNISSUES.md#2gb-limit-for-32-bit-stacks-all-windows-stacks\nI'm not sure that this change actually fixes our issue. I was under the impression that this PHP 5.6 change has to do with uploads to a PHP script, and would not affect the SDK's ability to transfer larger files to S3. Am I wrong in my thinking?\n. Sounds good. Thanks.\n. No worries. Discussions are good.\n. Can we see a code example please?\n. Checkout PR #240 and see if that works for you.\n. How are you sending it?\n. Cool, I'm glad you got it working. Thanks for helping out @skyzyx. I've put it into our internal backlog to consider adding something to help with this.\n. Thanks! We'll verify this and merge it in soon.\n. Sorry, for letting this sit so long. It looks good, and I'm just verifying with our legal team that it's OK to merge.\n. Everything is approved on our end. @peterkaminski, I just wanted to confirm that you are OK with us accepting the code under the Apache 2.0 license. Thanks.\n. @peterkaminski ping!\n. Yay! Merged.\n. No, you must create clients for each region, and then perform the DescribeInstances operation for each of them. This is because each region is independent and has a different URL for the API.\n. @mtdowling Is it possible that the file handle is still open? That would prevent the unlink() from being successful, right?\n. Looks good.\n. @sobytes Are you still having trouble with this? Are you getting any kind of error when you try to stream the private content?\n. Awesome! Thanks for the update. I'm glad you figured it out. I'm going to go ahead and close this issue, because I don't think there was anything wrong with what the SDK was doing.\n. Thanks for letting us know about this.\n\nWe periodically get exceptions around DynamoDB throughput throttling\n\nHow periodically? Are there any more details that you can share that you feel would be relevant?\n\nthe documentation for WriteRequestBatch seems to suggest that it will automatically reprocess failed entries, which I took to mean that it'll handle the backoff & retry logic.\n\nIt should both automatically reprocess failed (unprocessed) entries AND handle the backoff/retry logic. This is the first time someone has reported a problem with this, so I think both parts are probably working as intended. I have an idea of might be happening though that I'll have to confirm with the Amazon DynamoDB team. \nUsually, the BatchWriteItem API will not throw this kind of exception, and will process all or some of the items, and return the ones it couldn't do back to you in the \"UnprocessedItems\" part of the response. The WriteRequestBatch will re-queue these. The DynamoDB client\u2014for any request that consumes throughput\u2014also automatically retries requests, up to 11 times (iirc), if a ProvisionedThroughputExceededExceptions are encountered. However, if it happens more than 11 times, the exception will bubble up. I think that is what may be happening. I'll check with DynamoDB to see if that is consistent with what they would expect, and if so, I'll add some additional logic to WriteRequestBatch to account for that scenario.\nAre you doing a sufficient volume of puts that you think you may be getting throttled this heavily?\n. Ah, that does seem to make sense then.\nHere is where the number of retries is set: https://github.com/aws/aws-sdk-php/blob/master/src/Aws/DynamoDb/DynamoDbClient.php#L111\nHere is where the delay between retries is determined: https://github.com/aws/aws-sdk-php/blob/master/src/Aws/DynamoDb/DynamoDbClient.php#L172\nLet me know if adjusting the retries alleviates the problem. I'll talk to the DynamoDB team and take the steps we need to to make the WriteRequestBatch more resilient. Thanks.\n. @onethumb Have you been able to test out adjusting the number of retries?\nI confirmed with the DynamoDB team that if no items from the BatchWriteItem request can be processed, then that will cause the ProvisionedThroughputExceededException (I asked them to add this to their docs as well). That's consistent with my theory, and is something I should be able to handle in the WriteRequestBatch abstraction.\n. Fixed by c882b65\n. This is odd. Can you show us some code that reproduces this problem?\n. I cannot reproduce this error, and I think there is something strange going on.\nFirst, let me explain how the exceptions work in the SDK. We produce Exception classes only for exceptions that are modeled in service descriptions. The RDS service description does not contain InvalidParameterValue. So it is correct that the exception class doesn't exist, but it should not cause an error. The exception factory class checks if the Exception class exists, and if it doesn't reverts to throwing the provided default exception. In the case of the RdsClient, it should be throwing Aws\\Rds\\Exception\\RdsException.\nDo you have another autoloader registered that might be interfering with the normal class_exists() behavior?\n. Cool. Thanks for double checking that. I'm going to go ahead and close this.\n. :ship: Good changes.\n. :ship: \n. Thanks for letting us know. We'll look into it and see if there is actually an issue with the SDK, or if something else might be going on. Link to our Twitter discussion: https://twitter.com/gwagner85/status/444201245499469825\n. Sorry I've let this sit so long. We were not able to find any issues. Is this still causing problems for you.\n. Oh, nevermind, you are using RabbitMQ now. Well, we'll make a note internally about this, but please reopen if you have a chance to look into SQS again.\n. There's an anonymous function on that line. If you are not using version 5.3 or higher of PHP, that would cause the syntax error you are seeing. The AWS SDK for PHP requires version 5.3.3+ of PHP.\n. Sounds like a good idea. Let me know if you'd like to submit a pull request for this. Either way, we'll put it in our backlog.\n. We've added this to our internal backlog.\n. Thanks for letting us know. We'll look into it. In the meantime, if you happen to find out any more information that might help, please update this issue. Thanks.\n. Haven't tagged yet. Is it better to change the alias before or after you tag?\n. I think I got that. :wink:\n. This feature is supported by the SDK. :smiley: See Executing Commands in Parallel in the user guide.\n. The getObject(), putObject(), and other methods of the S3Client class just proxy to getCommand() method. So\nphp\n$result = $s3Client->getObject($params);\nis the same as\nphp\n$result = $s3Client->getCommand('GetObject', $params)->getResult();\nCommand object can be grouped together and executed lazily, so that is why they are used to execute operations in parallel.\nI you want to download several objects in parallel from the same bucket or key prefix, you might want to take a look at the special downloadBucket() method.\n. This is the method that will be used instead of what was suggested in #229 and #230. Other AWS SDKs and tools will be supporting the same format, if they don't already.\n. @miklos-martin Can you provide us any more details and answer @mtdowling's question please? Thanks.\n. We'll look into this, but I'm curious as to why you are trying to create an object with a key of \".\".\n. I have not located the root cause, but I have reproduced the issue using the following code.\n``` php\n<?php\nrequire DIR . '/../vendor/autoload.php';\n$s3 = Aws\\S3\\S3Client::factory();\n$s3->addSubscriber(\\Guzzle\\Plugin\\Log\\LogPlugin::getDebugPlugin());\n$s3->putObject([\n    'Bucket' => 'my-test-bucket',\n    'Key'    => '.',\n    'Body'   => fopen(FILE, 'r'),\n]);\n```\nOutput:\n```\nRequest:\nPUT / HTTP/1.1\nHost: my-test-bucket.s3.amazonaws.com\nUser-Agent: aws-sdk-php2/2.6.1 Guzzle/3.9.1 curl/7.30.0 PHP/5.4.24\nContent-Type: text/x-php\nContent-MD5: 3VqKG0vXdAj2ZsN8ooiJAw==\nDate: Wed, 07 May 2014 17:51:02 +0000\nAuthorization: AWS {{REDACTED}}:gqYfb7Uj6rCG1O/t/mruuqjTf4Y=\nContent-Length: 291\nphp\n\n<prequire DIR . '/../vendor/autoload.php';\n$s3 = Aws\\S3\\S3Client::factory();\n$s3->addSubscriber(\\Guzzle\\Plugin\\Log\\LogPlugin::getDebugPlugin());\n$s3->putObject([\n    'Bucket' => 'my-test-bucket',\n    'Key'    => '.',\n    'Body'   => fopen(FILE, 'r'),\n]);\nResponse:\nHTTP/1.1 403 Forbidden\nx-amz-request-id: {{REDACTED}}\nx-amz-id-2: {{REDACTED}}\nContent-Type: application/xml\nTransfer-Encoding: chunked\nDate: Wed, 07 May 2014 17:51:00 GMT\nConnection: close\nServer: AmazonS3\nxml version=\"1.0\" encoding=\"UTF-8\"?\nSignatureDoesNotMatchThe request signature we calculated does not match the signature you provided. Check your key and signing method.{{REDACTED}}{{REDACTED}}{{REDACTED}}gqYfb7Uj6rCG1O/t/mruuqjTf4Y=PUT\n3VqKG0vXdAj2ZsN8ooiJAw==\ntext/x-php\nWed, 07 May 2014 17:51:02 +0000\n/my-test-bucket/{{REDACTED}}\n```\n\nI haven't found anything specific yet, @mtdowling, but do you know if Guzzle or the SDK would prevent a key of \".\"? It seems to get dropped from the request path.\n. :+1:\n. Sorry for the slow response on this. I would guess that 'arn:aws:iam::cloudfront:' . $ARN is not producing a valid ARN for your intended principal. What is the value of $ARN? It seems like your questions are not specific to the SDK though, and may be better suited for the CloudFront or S3 forums, where you will be able to get advice from people who are experts with those particular services.\n. I don't remember reopening this, but it somehow is reopened. I'm re-closing it, but feel free to continue commenting, if you desire.\n. What makes you believe you had the wrong repo?\n. Gotcha. OK.\n. I've also seen cases where it is in $_SERVER and not getenv(). Plus, if it is in $_SERVER, that is much faster.\n. Wrote a little script that runs on CLI or WEB that outputs HOME using various means:\n```\n$ php ~/Projects/sandbox/envy.php \nCLI:\nvariables_order: GPCS\nserver: /Users/lindblom\nenv: NULL\ngetenv: /Users/lindblom\n$ php -d variables_order=GPCSE ~/Projects/sandbox/envy.php \nCLI:\nvariables_order: GPCSE\nserver: /Users/lindblom\nenv: /Users/lindblom\ngetenv: /Users/lindblom\n$ HOME=/tmp php ~/Projects/sandbox/envy.php \nCLI:\nvariables_order: GPCS\nserver: /tmp\nenv: NULL\ngetenv: /tmp\n$ php ~/Projects/sandbox/envy.php (where putenv('HOME='); added)\nCLI:\nvariables_order: GPCS\nserver: /Users/lindblom\nenv: NULL\ngetenv: NULL\n$ php ~/Projects/sandbox/envy.php (where unset($_SERVER['HOME']); added)\nCLI:\nvariables_order: GPCS\nserver: NULL\nenv: NULL\ngetenv: /Users/lindblom\n$ php -S localhost:9000\nWEB:\nvariables_order: GPCS\nserver: NULL\nenv: NULL\ngetenv: /Users/lindblom\n$ php -d variables_order=GPCSE localhost:9000\nWEB:\nvariables_order: GPCSE\nserver: NULL\nenv: /Users/lindblom\ngetenv: /Users/lindblom\n```\nIt seems like $_SERVER['HOME'] is never populated for web requests. If it is supposed to be, then I am missing something.\n. Well, what do you think, @mtdowling? Any other thoughts or ideas? I tried [asking others about$_SERVER['HOME']](https://twitter.com/jeremeamia/status/462714498676043776), but didn't get a response.\n. Can you please tell me what you mean by \"not built properly\"? You did not include any information indicating how it is incorrect.\n. Instead of events, I added a callback. Check out: f3775a51228f338e324e058987040260921c2837\n. [It does handle this](https://github.com/aws/aws-sdk-php/blob/master/src/Aws/DynamoDb/Resources/dynamodb-2012-08-10.php#L4184-L4189), since it is the very purpose of the iterator. Do you have evidence that it is not working?\n. Yeah, all of the iterators for every service are designed to handled tokens/markers/keys so you don't have to. That's why we made them. :-)\n. Thanks @Remper. :+1:\n. I don't believe this would be caused by anything in the AWS SDK for PHP, but I will make a couple of recommendations:\n1. Make sure your PHP code and JavaScript code are using the same AWS region.\n2. Ask this same question on the [Amazon S3 Forum](https://forums.aws.amazon.com/forum.jspa?forumID=24), and consider contacting [AWS Support](https://aws.amazon.com/support) if this is a time-sensitive issue.\n. You're debug output of the SendMessage request shows that what you are sending to SQS looks very odd. This is because the value you passing in as the QueueUrl is incorrect. If you look at the [API docs forgetQueueUrl](http://docs.aws.amazon.com/aws-sdk-php/latest/class-Aws.Sqs.SqsClient.html#_getQueueUrl), you'll see that it returns aGuzzle\\Service\\Resource\\Model` object (which we refer as a modeled response), not a string. This is how all of the methods that correspond to API calls work.\nSo, what you should be doing is something like this:\n``` php\n$result = $sqs->getQueueUrl(array(\n    'QueueName' => 'JobDistilleryCompanyCrawl',\n    'QueueOwnerAWSAccountId' => '191519130263',\n));\n$sqs->sendMessage(array(\n    'QueueUrl' => $result['QueueUrl'],\n    'MessageBody' => 'Awesome!'\n));\n``\n. No problem! :smile: \n. We are planning to eventually adopt Guzzle 4, but we currently do not have any specific timelines to share about that. Adopting Guzzle 4 will require breaking changes (mostly minor) to the SDK, and will require us move to version 3.0.\n. No problem. :smile: I'll try to remember to post on this issue if we make any announcements regarding Guzzle 4 support in the future.\n. I just blogged about Guzzle 4 and the SDK earlier today: http://blogs.aws.amazon.com/php/post/TxNB95LUU2J6X7/Guzzle-4-and-the-AWS-SDK\n. @ChrisTerBeke [We announced the preview of Version 3 almost 2 months ago](http://blogs.aws.amazon.com/php/post/Tx3RR28K9AAHPD1/Version-3-Preview-of-the-AWS-SDK). We are still improving V3 though and will be releasing the stable 3.0 in February or March, most likely.\n. Ah, well that's a problem we weren't aware of. Would you please open an issue(s) with some more details so we can address this?\n. There is no way around the limitation of the S3 API. You will still need to list all the objects and filter them to find what you need. You can use iterators to do this fairly easily though, using a technique similar to what we do in [S3Client::deleteMatchingObjects](https://github.com/aws/aws-sdk-php/blob/master/src/Aws/S3/S3Client.php#L634-L640):\n``` php\nuse Guzzle\\Iterator\\FilterIterator;\n$objects = $this->getIterator('ListObjects', array('Bucket' => $bucket, 'Prefix' => $prefix));\n$objects = new FilterIterator($objects, function ($current) {\n    return substr($current['Key'], -7) === 'process';\n});\n// Includes only objects where the key ends with \"process\".\nforeach ($objects as $object) {\n    // Do something with the object \n}\n```\nThe other thing to consider is if storing your objects differently would help. Your could store your objects such that \"done\" or \"processed\" was the first part of the key (a.k.a. the folder), then prefixes would work. This may not specifically work in your situation, but hopefully it can give you an idea.\n. Sounds good! :smile: \n. I'm not sure how to sync the time with WAMP. I assume it uses the Windows time/clock. Is your Windows time correct? Is it possible to configure your machine to sync with an NTP server?\n. That looks like a good fix. Were you able to confirm it works for you?\n. Thanks for the fix!\n. Just added some code that should solve the problem: e1c2619.\n. I just tagged a release, so it's available via Composer now. I'll add the phar sometime in the next couple of hours, and I'll make sure to comment on here when it's available.\n. Here you go: https://github.com/aws/aws-sdk-php/releases/tag/2.6.8\n. @eric-tucker @zelll @ChrisZieba Did this fix clear up the issue?\n. Definitely. I actually already have this as a backlog item. If anyone wants to do a pull request though, let me know.\n. Oh boy... thanks for catching that.\n. Gathering comments from other places:\nComment from Derek R. on the blog post:\n\nMoving to require PHP >= 5.4 is inevitable and appropriate for a 3.0.0 release but I feel like it would require at least maintenance releases be continued for version 2 of the SDK for a time.\n\nComment from @WyriHaximus on Twitter:\n\nYeah it looks great! Love to find a way to make my adapter compatible :D!\n. @FrenkyNet Oh, I think @skyzyx knows that world all too well. :smiley:\n. From dawalama on the blog comments:\nThank you for detailed analysis. Guzzle 4 really does have nice improvements. Improved performance and better async requests handling are sufficient reasons for the update in my opinion. I vote for the update. \nA new major version for SDK would be really welcomed.\n. Thanks for your input everyone!\n. Thanks, we'll look into this.\n. Thanks @DavidPrevot. I'm going to run this by our open source approvers before we merge.\n. Took a little longer than expected, but this has been cleared. Merged.\n. Interesting. Do your SEE-C requests (getting and putting) work as expected without using the standard API (i.e., without pre-signed URLs)?\n. The SDK will handle the encoding and hashing for you, so you just need to provide the raw key. Have your tried just providing the algorithm and raw key to getObjectUrl()? If that is not working, then I'll dig further. I'm also interested in your use case; why are wanting to use SSE-C features with pre-signed URLs? \n. OK, thanks bringing this to our attention. We'll continue to look into it and see if there is an issue.\n. We've talked to the S3 team and found out that even if you pre-sign the URL, you still have to send a request to the URL that includes some of the SSE-C related headers. In other words, the SDK is working correctly, but pre-signing the URL doesn't actually provide you with many benefits since the URL cannot be used as a link. I can dig up the specific details if this is something you want to do (please reopen the issue if it is), otherwise, I would advise that this is probably not something you should use pre-signed URLs for.\n. Using additional parameters is possible using PHP's lesser-known-but-native stream idioms. You must use the $context parameter of fopen() and pass in a stream context. For example:\n\n``` php\n$s3 = Aws\\S3\\S3Client::factory(/.../);\n$s3->registerStreamWrapper();\n$f = fopen('s3://bucket/key', 'w', false, stream_context_create(array(\n    's3' => array(\n        'CacheControl' => 'no-cache',\n        'ACL' => 'public-read',\n    )\n)));\n// ...fwrite-ing all the bits...\nfclose($f);\n```\nYou can include any parameters that are a part of the underlying SDK operation, which in this case in the PutObject operation.\n. The rename() function triggers the S3 CopyObject and DeleteObject operations. The CopyObject operation can be a little tricky in S3, especially if you are wanting to change any of the object's metadata like the cache headers. Try adding 'MetadataDirective' => 'REPLACE', in your parameters, and the new CacheControl value should work.\nThe issue you are having with the ACL parameter probably doesn't have anything to do with the actual parameter. You probably don't have access to either the source or destination bucket/key. \n\nNote: Attach the SDK's wire logger, and you'll see that you are getting a 403 from S3.\n\nphp\n$s3Client->addSubscriber(\\Guzzle\\Plugin\\Log\\LogPlugin::getDebugPlugin());\n. That sounds likely. I think there actually is a separate permission required that should look something like s3:PutObjectAcl. If adding that doesn't work, then you may want to ask for some help on the S3 forum, where you should be able to get better help for S3-specific questions/issues.\n. Cool, that seems like it would do the trick. I'll do some testing with this.\n. Hmmm... strange. We don't examine the rest of the response body when we throw exceptions. The fact that this response has additional data is actually an edge case compared to the behavior of other services and operations, so I don't think we should really modify anything to support that. Note that you can get the Response object using the getResponse() method of the exception.\n. @migueleliasweb Any update on your end?\n. Thanks. That text comes directly from the service, so I will create an internal ticket to that team.\n. Answered on StackOverflow:\n\nI suspect, since the error is talking about a bad string to long (number) conversion, that your DDB_READ_CAPACITY_UNITS and DDB_WRITE_CAPACITY_UNITS values are problem being read as strings from $_SERVER. Try casting/converting them to integers.\nphp\n   'ProvisionedThroughput' => array(\n       'ReadCapacityUnits' => (int) $_SERVER['DDB_READ_CAPACITY_UNITS'],\n       'WriteCapacityUnits' => (int) $_SERVER['DDB_WRITE_CAPACITY_UNITS'],\n   )\n. Thanks.\n. What version of the SDK are you using, version 1.x or 2.x?\n. Those samples from the docs are out of date. Thank you for bringing those to our attention. We'll work on getting these updated. Sorry for the confusion.\n\nVersion 2 of the SDK has been available for about 20 months, and is the preferred tool to use. This is the repository for version 2, and here is the documentation. Version 2 of the SDK throws exceptions when non-200-level errors are returned from the service, except in cases where it can automatically retry the request (mostly related to intermittent throttling or connection issues).\nVersion 1 of the SDK is no longer supported, but the code can be found in the amazonwebservices/aws-sdk-for-php repository. Version 1 does not throw exceptions by design, and you must use the ->isOK() method to check if there was an error or not. This is demonstrated throughout the version 1 API docs including in this example.\n. What kind of changes do you want to make? It's possible to set cURL options when you instantiate a client without making changes (see Configuring the SDK).\nphp\n$ses = \\Aws\\Ses\\SesClient::factory(array(\n    'region' => 'us-east-1',\n    'curl.options' => array(\n        CURLOPT_TIMEOUT => 5\n    )\n));\n. @djlosch What would your proposed changes be? I'm not really sure what advice to give about testing it without understanding what it is you'd like to change.\n. The SDK already retries failed requests up to 3 times (with exponential backoff) using the Guzzle 3 Backoff plugin. Could you share details about the errors you are seeing and what your proposed code  changes are? What does the function you mentioned look like and where you are putting it in the SDK?\n. The backoff plugin configuration is setup here. The plugin requires the backoff strategy object to be passed in. In our case, it is a highly decorated strategy where each wrapper concerns itself with different things (e.g., cURL errors, throttling errors, HTTP errors, expired credential errors). You will also want to look at the Guzzle 3 Backoff Plugin code. \nIf you look specifically at the class that handles cURL errors, you'll see that  there is a list of cURL errors that we do retry. If this is not working for you, then:\n1.) The specific cURL error you are getting is not one that is handled by this plugin. If this is the case, then we'll need to have a discussion about whether or not it should be.\n2.) The requests are failing more than 4 times. This causes the cURL exception to bubble up.\n. This error means that one or more of the keys you are trying to get is not actually a part of the schema of your table. I suspect that \"user1\" ad \"user2\" are probably not key or attribute names of your table, right? Are you sure you are not meaning to do something like this:\nphp\n$response = $dynamoDbClient->batchGetItem(array(\n    \"RequestItems\" => array(\n        \"friends\" => array(\n            \"Keys\" => array(\n                array( \"id\" => array(Type::STRING => \"someID\")),\n                array( \"id\" => array(Type::STRING => \"anotherID\")),\n            ),\n            \"AttributesToGet\" => array(\"id\", \"created\", \"name\") // Assuming \"name\" is a valid attribute\n        )\n    ),\n    \"ReturnConsumedCapacity\" => \"TOTAL\",\n));\n. Oh, I see. To access data in a secondary index, you need to use the query() operation. batchGetItem() is only for getting items by their IDs, and does not look at secondary indexes. A good example of querying a secondary index is available in the DynamoDB Developer Guide.\nI'm going to go ahead and close the issue, but please reopen if you have follow up questions or need additional help. Thanks.\n. It is supported. Message attributes are provided and retrieved when you send and receive the message, respectively. Please see the API docs for SendMessage and ReceiveMessage, which will show you the parameters and formatting you need for message attributes.\nI'll go ahead and close this issue, but please reopen if you have follow up questions or need additional help.\n. We should probably have all the methods that return $this have @return static, right?\n. Sounds good. If you want to update the PR to reflect this info, that would be great. You might also want to check the concrete S3 and Glacier UploadBuilder classes to make sure they are behaving correctly as well. Thanks.\n. Oh, I was only expecting you to update the UploadBuilders, but OK.\nThese changes seem fine. @mtdowling what do you think?\n. Thanks for letting us know about this. I just pushed a fix to master that will be included in the next tagged release.\n. No problem. I'm glad you were able to work through it. :smile: \n. The best place for asking general questions about service usage is on the AWS forums. Here is the Amazon SNS Forum. I'm going to go head and close this issue, since it is not specific to the SDK.\n. It's true, we did have some Hadoop/EMR helpers defined in V1. We never ported them to V2 because other work got prioritized, and we never had any customer requests for them... well.. until now, at least. :smile:\nHowever, those hadoop[...].class.php files in the V1 SSK do not appear to have any dependencies, so you could actually just pull them out of there and use them. I am unsure they still work as expected, since I am not personally experienced with Hadoop and Pig, and I know that both of those projects have evolved over the past 4 years whereas those PHP files have not.\n. Cool.\n. 1. Is the the error is happening at getDomainClient() or when you try to use $domain?\n2. You aren't passing any credentials into the domain client from what you are showing me here. getDomainClient()'s second argument is an array of parameters, like you provide to other clients.\n3. If there is an issue aside from no credentials being provided, it might already be fixed in the master branch. If you could try that out, that would be helpful.\n. Thanks for giving that a shot. I'm still not certain where the problem is actually occurring though, so it would be great if you could get me a wire log. Details about attaching the log plugin are here, but essentially, if you could run some code that looks like the following, and give me the wire log output (with sensitive info redacted please), that would be great.\n``` php\nuse Aws\\CloudSearch\\CloudSearchClient;\nuse Guzzle\\Plugin\\Log\\LogPlugin;\n$logPlugin = LogPlugin::getDebugPlugin();\n$csConfigClient = CloudSearchClient::factory([ / key, secret, region / ]);\n$csConfigClient->addSubscriber($logPlugin);\n$csDomainClient = $csConfigClient->getDomainClient('your-domain', [ / key, secret / ]);\n$csDomainClient->addSubscriber($logPlugin);\n$csDomainClient->search(['query' => 'foo']);\n``\n. You should not pass abase_urlto theCloudSearchClient. TheCloudSearchClientdetermines the URL based on the region. TheCloudSearchDomainClient` is what you must provide your search endpoint to.\n. I apologize, I think that we are having trouble communicating. I believe the problems you have are just that you are configuring the client(s) incorrectly. Let me try to clarify some things.\nThere are two CloudSearch-related clients:\n1. Aws\\CloudSearch\\CloudSearchClient \u2013 Used to create, configure, and reindex domains. This client send requests to the CloudSearch service's endpoint. When you configure this client, you need to provide your credentials and a region. The endpoint is determined by the SDK using the provided region, so you should not provide the base_url when creating this client.\n2. Aws\\CloudSearchDomain\\CloudSearchDomainClient \u2013 Used to search and upload documents to a specific domain. This client sends requests directly to a domain's search endpoint. When you configure this client, you need to provide your credentials and the full search endpoint URL (via base_url).\nThe CloudSearchClient:: getDomainClient() method is a helper method that can retrieve your search endpoint given the domain ID, and create a CloudSearchDomainClient.\nI hope this helps clarify things and that you can get the right settings to the right clients. If you are still having trouble, please provide a more complete code sample that shows what you are doing. That way, I can get a better picture of what might be going wrong.\n. Yay! I'm glad have it working. I also have some changes to our user guide documentation pending that should make this clearer too after our next tagged release.\n. Wow, nice work on this. It looks like there are some issues with the service description, but it's pretty close. Our team will need to talk and see if this PR is something we can accept.\n. It turns out that we cannot accept this PR. At the current time, we are not able to support this client via our internal processes. Thanks for your submission though, and I hope it was easy enough to get working for your purposes. We are tracking this as a feature request internally.\n. Nice! I'll take a closer look tomorrow. Thanks.\n. Hey @radford, I'm finally getting to a point where I can look at this. I noticed that you included the failure states for each operation in the 'success.value'. Could you explain your reasoning there? I would have put those in the 'failure.value'.\n. We've been busy getting ready for AWS re:Invent, but I'll be circling back to this after next week. It turns out the CloudFormation stack status is a fairly complex state machine, and I'm also working on some changes in waiters for Version 3 of the SDK. I will make sure that stack waiters get added to both versions, even if I can't directly accept the PR. I'll follow up again in 1-2 weeks. Thanks.\n. I'm going to close PR, because what was submitted is not exactly what we need, but we have it on our backlog to add these. Thanks.\n. Well, this request never made it out of the backlog. The implementation that was provided in this PR was not quite right. The SDK does not have waiters for everything, only the ones that have been requested the most or contributed. I don't work at AWS anymore though, so you will have to continue discussion with the current devs by reopening this issue or starting a new one.\n. Yep, the base_url option is what you need to do that.\n. That is almost right. You should set the 'base_url' to 'http://s3.example.com' and leave off the 'my-bucket/'.\nThe 'PathStyle' param is needed to stop the S3Client from putting the bucket into the hostname. Normally, the SDK does this automatically, because this is how normal virtual-style bucket addressing works in S3. Using the 'PathStyle' param tells the SDK to leave the bucket in the path.\n. Ack! We'll get that fixed soon. Thanks for letting us know about that.\n. Should be fixed on the release page now: https://github.com/aws/aws-sdk-php/releases/tag/2.6.16\nAlso, I committed a change to prevent the issue in the future.\n. I'm sorry about these troubles. We are using a new script to generate the phar and zip, and sounds like we still have a kink. I'll update the phar again, once the issue is fixed.\n. I updated the release with the updated aws.phar. It was working for me, but I'll wait for you to confirm that it solves the issue before I close this again. Thanks.\n. Good! Thanks for your patience. :smile: \n. Where/when did you get your copy of the aws.phar? There was an issue earlier last week, but it has already been resolved. If you download the aws.phar from 2.6.26 release page, then it should be working fine.\nSince, I've already fixed this, I'm going to go ahead and close the issue. If, after you've re-download the phar, you are still having trouble, please reopen this. Thanks.\n. OK good. It's likely you just got a copy before it was fixed or before GitHub's page cache was refreshed.\n. It would be great to see a wire log of one of the requests that fails; otherwise, I won't be able to properly diagnose the problem.\nRegarding the aws-autoloader issue, please re-download the SDK from the 2.6.26 release page, which was fixed last week.\n. Agreed with @skyzyx. You are going to want to handle those data conversions outside of DynamoDB and the SDK.\nIf you aren't following what the DynamoDB docs say regarding that, then there is no guarantee it will work correctly. Neither the SDK or PHP itself is designed to work with multibyte characters either. That's why I'm interested to see what the wire trace looks like.\n. Sorry for not responding to this sooner.\nUsing the latest version of the SDK, is this still an issue? I've never had problems with the doesObjectExist() method before, so this seems strange.\n\nBy the way, is it faster to check if an object exists before storing it rather than simply uploading the object directly for small (~150kb) objects?\n\nI would assume it is slower, since you are doing an additional request. What is the intention behind checking whether or not it exists before uploading?\n. TL;DR maybe.\nThis is a great question, and is one we've definitely asked ourselves. We have been trying to come up with some ideas about this, but it turns out to be a little difficult. The current format*() methods (and underlying Attribute class) only support S, N, SS, and NS, which were the only types that existed when that code was written. Since then, DynamoDB has added B, BS, BOOL, NULL, L, and M. They could potentially add more in the future. Who knows? Someday there could be different types of numbers, dates or times, etc.\nWhat makes supporting something like the format*() difficult now, is that there is a lot of ambiguity in how PHP native types map to DynamoDB types. For example: PHP only has an array type, but DynamoDB has lists (L), maps (M), and sets (*S). Also, DynamoDB has strings (S) and binary/blob (B) values, while PHP just has string. Sure, you could introspect values (e.g., does this array have numerical indexes or string keys?) and/or declare a set of conventions (e.g., all stream resources will be assumed to be binary/blob values), but there are still problems with consistency and consecutive roundtrips with the data. Let me give a step-by-step example regarding the list/map/set types.\n1. Let's say an $item is provided that looks like this: ['foo' => ['bar']] (or ['foo' => [0 => 'bar']])\n2. Well, that would get serialized into {\"foo\":{\"SS\":[\"bar\"]}}\n3. Oh wait, with the new list type available now, you probably wanted {\"foo\":{\"L\":[{\"S\":\"bar\"}]}}\n4. OK, let's go with list. So, now let's say we retrieved the item later, and the SDK had a way to convert the item's DynamoDB serialization back to a native PHP. We'd end up with this again: ['foo' => [0 => 'bar']], whether it was a list or a set.\n5. What if we then did $item['foo']['fizz'] = 'buzz'; That would give us an array like ['foo' => [0 => 'bar', 'fizz' => 'buzz']].\n6. This is no longer a list or a set, it's now a map, and would be serialized to {\"foo\":{\"M\":{\"0\":{\"S\":\"bar\"},\"fizz\":{\"S\":\"buzz\"}}}}\n7. You know what, I didn't need that data, let's just unset($item['foo'][0], $item['foo']['fizz']). Now I have just ['foo' => []], which could be a list, map, or set. :frowning:\nOK, so doing something along the same lines as the format*() methods might not work, but we've considered some other options as well.\n1. Define and require the use of some objects to \"box\" values that are ambiguous (e.g., Set, Map, List, Binary). This has usability drawbacks since you would need to import 1 or more additional objects. When reading these values out of DynamoDB, you would need to re-box them so they remain consistent if they end up being put back into DynamoDB. The boxed values would probably make the values more difficult to work with.\n2. Some kind of item/document builder object/methods. This would work be fairly easy to work with an even provide autocomplete support in IDEs, but would end up being more verbose than the raw API.\n3. Do something like format*(), but support only a subset of types. This could work, but would only support a subset. If more types are added in the future, they may or may not fit into this model. Even if they do, it might not be in a backwards-compatible way.\n4. Use a light schema-based approach where a schema can be defined and an object/method would transform the array into a the API structure. This would work, but defining the schema could be tedious, especially if the document is large or complex. Also, heterogeneous lists could not be supported, since defining a schema for that would not make sense.\n5. Accepting a literal JSON document and transforming into the API. This could work, but it would only support the types that JSON natively supports, not set or binary values.\n6. Full blown ORM. This would be a lot of work and might be overkill for most use cases.\nSo, yeah. We'd like to provide something to help DynamoDB users, but we want it to:\n- Be easier to use than the raw API.\n- Support all the types.\n- Support potential future types without breaking changes.\n- Not be significantly more verbose than the raw API.\nI'm just not sure we've thought of something yet that provides all of these benefits. If you or anyone else has ideas/suggestions/opinions/etc., we'd be happy to hear them.\n. Note: I'm removing one of @Sazpaimon's previous comments from this thread, since it's off topic, and will reply to it where it was cross-posted on #368.\n. @Sazpaimon I've been toying with the idea (for both V2 and V3) of a DocumentMarshaler class that would look like this gist. Do you think something like this would be helpful?\n. @Sazpaimon Hey, I updated my gist/idea with some more code and docblocks. Along with the (un)marshalDocument methods which handle JSON documents, there are also the (un)marshalItem and (un)marshalValue methods. These are basically replacements for deprecated formatAttributes and formatValue methods, respectively. These should work because I am not providing support for set (*S) or binary (B) types. What do you think about this idea? Any suggestions?\n. I don't think I'm going to worry about sets, but I might be able to use Guzzle's stream class to represent binary types. I'm going to play around with that after I write up some unit tests.\n\nAlso, for unmarshalling binary types, I would unbase64 the data (if it isn't already)\n\nGood idea. I think V3 does it automatically, but I'll have to double check that for V2.\n. @Sazpaimon and @ando-masaki, I've put together a PR for my marshaler idea: #406.\n. Resolved by 0a9c4fd. /cc @Sazpaimon \nNote: In Version 2 of the SDK, binary (B) and set (*S) types will not be supported. When this is added into Version 3 (that requires PHP 5.4+), I'll add support, because I will be able to use the JsonSerializable interface to make sure the wrapper objects for binary and set types can be json_encode()'d.\n. Yeah, I just confirmed with the aws-cli team that the latest version of the cli should be writing out a compatible credentials file. The config file is different and is unique to the cli.\n. Hey @mtdowling, why is the requirement 5.4.1+. @skyzyx used that in the README updates, but he said he got that from the compat test.\n. OK, but why specifically 5.4.1+ instead of 5.4.0+\n. :ship: Ship it!\nThanks for the chat on IRC, @mtdowling. This should be safe according to the spec and will reduce CPU usage and request size (not significantly, but could be nice for the DynamoDbClient).\n. Looks good.\n. That's one solution, but not necessarily an easy one. Thanks for bringing this use case to our attention. We'll see if there is something we can change to make this easier.\n. Hey Armando, thanks for the detailed bug report! :smile:\nI made sure to double check the S3 object copy docs, and this is not an actual bug. The docs say:\n\nWhen copying an object, you can preserve most of the metadata (default) or specify new metadata. However, the ACL is not preserved and is set to private for the user making the request.\n\nSo, with that in mind, you can just add 'ACL' to your CopyObject request.\nphp\n$s3->copyObject([\n    'Bucket'     => $bucket,\n    'Key'        => $keyCopy,\n    'CopySource' => \"{$bucket}/{$keyOriginal}\",\n    'ACL'        => 'public-read'\n]);\n. Closing as \"no-issue\", but please reopen if you need additional help. Thanks.\n. No problem! FlySystem is a pretty cool library, but it's definitely a tough job to handle all of the little edge cases in each of the adapters. @FrenkyNet, the FlySystem author, knows where to find me though, if he needs help with any S3 questions.\n. @Sazpaimon, I feel like all of my responses to you are always walls of text, but I think it's important to explain our intentions. Here we go.\n\n\nThe Aws\\DynamoDb\\Enum\\Type class seems to have not been updated for the new attributes. Even more troubling, it seems all of the Dynamo Enums in V3 have been removed. Is there a reason for this?\n\nWe have not updated any enum classes for any services for several months, and we have removed all documentation that refers to enum classes, favoring the use of string literals. As you noticed, we've removed all enum classes from v3. We've been wanting to remove them for a while, but we needed a major version bump to actually do it. The explanation is below.\n\nAre we expected to re-create these Enums ourselves?\n\nIt's not necessary to recreate them, because they map directly to actual, publicly documented string literals.\nEnums in Version 2 of the SDK were concrete classes within the public API of the SDK that contained constants representing groups of valid parameter values to use when making calls to service operations. We thought, at the time, that these would be helpful to customers, but we quickly ran into issues.\n1. Enum values change over time - Services add and remove enum values from their APIs over time. Sometimes services make backward-incompatible changes to their API (they change their API version, and the SDK has to support both versions). We have to make sure the Enum classes are backward-compatible, even if the services' APIs change. Because of this, many Enum classes in Version 2 contain constants that may or may not be valid, depending on which version of the API you are using.\n2. Some Enum values are reserved words in PHP - Some service APIs end up declaring Enum values that are named the same as a reserved word in PHP. In those cases, we have to change the name to something else in order to put it in an enum class. This makes some of the Version 2 enum classes inconsistent with the actual service's API.\n3. Enum classes add bloat to the SDK - There are over 150 Enum classes in Version 2.\n4. Statically generated enum classes conflict with our dynamically driven API model approach. - The SDK currently supports multiple API versions based on a user-supplied \"version\" parameter. By statically generating enum classes, we are locking those enum classes to a specific API version, while at the same time, promising that you can change the API version of a client at runtime. These two things are not compatible with one another.\n5. Enum classes provide little to no value\n   1. Enum classes required you to know about and import (via use) them before you could actually use them.\n   2. They provided autocomplete support, but only for IDE-using customers, and only in the case where you know which enum to use for nested API parameters.\n   3. There is no way to indicate which Enums are used with each operation and to which parameters.\n   4. Writing out an Enum value is longer than writing the actual value (e.g., CannedAcl::PUBLIC_READ vs. 'public-read').\n   5. The constant name is basically the same as the literal string value, so the constant isn't actually abstracting anything.\nBecause of these issues, we decided that we needed to remove Enums. For v2, we stopped updating them and stopped documenting them, and in Version 3, we have removed them completely.\n. We have removed the fine-grained exception classes that lived in the each of the services namespaces (e.g., Aws\\Rds\\Exception\\{SpecificErrorCase}Exception) for very similar reasons that we removed Enums. The exceptions thrown by a service/operation are dependent on which API version (i.e, they can change from version to version) is used. Also, we are not technically able to provide the complete list of what exceptions can be thrown by a given operation (long story). This makes the fine-grained exception classes fairly useless.\nSimilarly to Enums, we have not been updating these classes for several months, and have removed them from our documentation.\nYou can (and have always been able to) catch the root exception class for each service (e.g. Aws\\Rds\\Exception\\RdsException). In V3, you can use the getAwsErrorCode() method of the exception to check for specific error codes. This is functionally equivalent to catching different exception classes, but provides that function without adding bloat to the SDK or setting false expectations.\n. Those (enums and exceptions) are both things we will be documenting in our UPGRADING.md and migration guides before our stable release.\n. Added beginnings of an UPGRADING guide for the 2.x -> 3.x changes.\n. Hey @benmadin,\n\nThis might not be the right place to ask for help, but coming into using the php sdk just as you release the beta, we are struggling a bit to find examples in existing code and modify them for v3.\n\nUnderstood. We are still actively developing the beta and we have not really created any user guide content specific to v3 yet. We'll get there.\n\nan example might be the s3 multipart upload, which seems to have changed substantially. Is there any more documentation around yet for these? It's unclear how one moves from the uploadBuilder to actually upload() (for which there doesn't appear to be a method?)\n\nIt has changed, but not too much. See http://docs.aws.amazon.com/aws-sdk-php/v3/api/Aws/S3/Multipart/UploadBuilder.html in the v3 API docs. \n\nEdit: My apologies, we tried the example with 2.7 and it didn't work either, so likely there is a part of the example that we have missed...\n\nGotcha. Feel free to open up another issue if you need help on that.\n. Thanks for letting us know about this. I'm surprised that this issue hasn't come up before. Your proposed solution makes sense, so we'll do some testing and get it fixed.\n. Thanks for your detailed request. We will discuss this and get back to you shortly.\n. We been talking about doing something like this. It is in our backlog now, but we're not sure if we are going to do it for just V3 or both V2 and V3. I'll go ahead and close this issue though in favor of us tracking the feature in our internal backlog. Thanks.\n. This might be a bug in how the SDK is sending the date values for this service.\n1. Can you provide a wire log of the request just so we can some clarification on what is going on.\n2. You might be able to patch it by putting a timestampFormat line, like in the SWF service description, into the Cognito Sync service description. Let me know if that works.\n. Hello @coredumperror, thanks for your question about this. One reason why you are not able to find good documentation about this is because we've intentionally left out information about the DSL we use to drive the waiters. Our PHP SDK was the first of the AWS SDKs to implement this type of feature, starting 2 years ago. Since then, we've been refining the specification for the waiters DSL, and will be changing how the internals and configurations in the future. However, you can see the waiter definitions for a service be looking in the service description for that service. Here is the link to the S3 waiter definitions. The wait logic for the waiters driven by waiter definitions is in the ConfigResourceWaiter class.\n. To be fair, there is a little blurb at the bottom of the Waiters page in the user guide that says:\n\nThe DSL is not documented yet, since it is currently subject to change...\n\nBut now I have this issue to refer to as well. :smile:  Anyway, I hope that this information was helpful.\n. :+1:\n. @recarv Have you tried asking in the Composer IRC channel or reporting an issue to Composer? I don't believe there is anything we can do about this. We've not received any other reports about this kind of issue either.\n. Let us know what you find out. Until then you might be able use the latest phar/zip.\n. If you look in the CHANGELOG, the ability to detach an instance with the AutoScalingClient was not added until version 2.6.13. You will need an updated version of the SDK to use that feature.\n. Sweet! :ship: \n. Look at Aws\\S3\\Model\\PostObject and see if that has what you are looking for.\n. :thumbsup:\n. Thanks for trying to help with this, but this has been discussed in #357, and there is no way we can modify these methods in backward-compatible way. This is why they were marked deprecated. I'm currently working on a new idea to replace them. I'm afraid I can't accept this PR, but I'd appreciate any feedback/ideas you would like to share on #357.\n. @mtdowling Since the Marshaler has no state, should I just make these methods static?\n. @mtdowling Also, any more thoughts about binary and/or set support?\n. @mtdowling: \n\nMixing key formats between strings and numbers will result in a SerializationException error from DynamoDB.\n\nActually, you can. I just tested it out, because I wasn't sure who was right. json_encode() happily handles this by making it a map, and converts the integer keys to string keys. A mixed key format is not possible in JSON, so by the time is gets to DynamoDB, it's fine.\nphp\njson_encode(['foo', 'bar', 'baz' => 1]);\n//> {\"0\":\"foo\",\"1\":\"bar\",\"baz\":1}\n@ando-masaki @Sazpaimon:\nI just tested a few mixed key and non-sequential list cases against DynamoDB, and a payload (for PutItem) like this is totally valid: {\"Item\":{\"id\":{\"N\":\"500\"},\"data\":{\"M\":{\"0\":{\"S\":\"foo\"},\"2\":{\"S\":\"baz\"}}}},\"TableName\":\"throttleme\"}. You just have to make sure to mark it as M when you send it to DynamoDB.\nThere is one small problem though (at least in SDKv3) where the SDK's client-side validation for map shapes does not accept arrays that have a key of 0, which makes sense, or \"0\", which is the problem in this use case. Special thanks to PHP's type juggling on that one, sigh... We will have to address that.\nI could make modifications to the marshaler to handle these cases you all have brought up:\nphp\n    // ...\n    if ($this->isTraversable($value)) {\n        $type = 'L';\n        if ($value instanceof \\stdClass || !isset($value[0])) {\n            $type = 'M';\n        }\n        $data = [];\n        $expectedIndex = -1;\n        foreach ($value as $k => $v) {\n            $data[$k] = $this->marshalValue($v);\n            if ($type === 'L' && (!is_int($k) || $k != ++$expectedIndex)) {\n                $type = 'M';\n            }\n        }\n        $value = $data;\n    }\n    // ...\n@Sazpaimon:\n\nI've already made my case known for supporting sets and binary, by using classes that provide some form of type hinting for attributes. \n\nStill thinking about this one.\n\nInitially I was okay with not supporting sets, but the lack of UpdateItem support for lists and maps still leaves use cases for sets.\n\nWhat exactly is lacking? make sure to double check Modifying Items and Attributes from the DynamoDB docs, and please note that the PHP SDK's V2 DynamoDB API docs do not include the new APIs (I think this is explained on the other issue), but you can look at the V3 API docs.\n. Yeah, I think you're right. The problem is that PHP treats an array key/index of 0 and \"0\" exactly the same. So, the limitation will be that you cannot have a map with a key of \"0\" (string or integer), and I think that is OK for PHP land.\n. Closing this for now, as far as v3 goes. Opening a new PR for a V2 version.\n. I am able to reproduce the issue (thank you for sharing your code sample). We'll look into it.\n. @ianbytchek You do not need to set the Signature. If you specify the region as eu-central-1, the SDK will automatically use SignatureV4. This is issue is with generating pre-signed URLs only. I believe I have located the issue, and we'll be fixing this ASAP.\n. What version of the SDK are you using? When you send a request, what does it do? What error do you get?\n. @eroux @helmutschneider Both of the S3-SigV4 issues should be fixed now. Let me know if you run into any other issues.\n. So... I still cannot reproduce this. Are you using the 3.0.0-beta.1 tag or dev-master? If not dev-master, try with that.\n. > dev-master works :+1:\nOh, good.\n\ntime for beta2 soon? ;)\n\nSounds like it...\n\nsetting the version to latest did give an error, while it worked in the beta.\n\nEh? That's working for me, too... What error?\n. Use 3.x@dev. See https://github.com/aws/aws-sdk-php/issues/414#issuecomment-64876238\n. I'm not sure about that one. @mtdowling, did we have a plan for that method?\n. This is intentional. See https://github.com/aws/aws-sdk-php/blob/v3/UPGRADING.md#removal-of-fine-grained-exception-classes for an explanation.\n. :+1: \n. :+1:\n. @mtdowling I updated the PR to make the [un]marshalValue methods private and to use gettype.\n. :ship:\n. Sweet. :ship:\n. I think the ClientFactory needs to be updated too, right? Also, why bother with a hardcoded provider?\n. Cool, let me know when you want me to take another look. This should end up being a nice feature addition.\n. We have 3 locations that contain creational logic for Credentials:\n- Aws\\Credentials\\Credentials::factory\n- Aws\\Credentials\\Provider\n- Aws\\Common\\ClientFactory::handle_credentials\nIt seems like maybe we could consolidate that by removing Credentials::factory and refactoring that logic into the Provider and/or ClientFactory. This is not really a big problem though, just a thought.\n. If you want to use version 3, please set your dependency to 3.x@dev. I was mistaken when suggesting dev-master, since that would be the latest v2 code, not v3.\n. Yep, what @abhinavlal said sounds correct.\n. Sure. Does something like this help?\n``` php\n$item = [\n    'id' => ['N' => '382'],\n    // Map\n    'name' => ['M' => [\n        'first' => ['S' => 'John'],\n        'last'  => ['S' => 'Doe'],\n    ]],\n    // List\n    'colors' => ['L' => [\n        ['S' => 'red'],\n        ['S' => 'green'],\n        ['S' => 'blue'],\n    ]],\n    // List of Maps\n    'phone' => ['L' => [\n        ['M' => [\n            'type'   => ['S' => 'home'],\n            'number' => ['S' => '5555555555'],\n        ]],\n        ['M' => [\n            'type'   => ['S' => 'mobile'],\n            'number' => ['S' => '5555555556'],\n        ]],\n    ]],\n];\n$dynamoDbClient->putItem([\n    'TableName' => 'YOUR_TABLE',\n    'Item' => $item,\n]);\n```\nAs of the 2.7.7 release, we added the Aws\\DynamoDb\\Marshaler class that makes this even easier.\nphp\n$marshaler = new Marshaler();\n$dynamoDbClient->putItem([\n    'TableName' => 'YOUR_TABLE',\n    'Item' => $marshaler->marshalItem([\n        'id' => 382,\n        'name' => [\n            'first' => 'John',\n            'last'  => 'Doe',\n        ],\n        'colors' => ['red', 'green', 'blue'],\n        'phone' => [\n            [\n                'type'   => 'home',\n                'number' => '5555555555',\n            ],\n            [\n                'type'   => 'mobile',\n                'number' => '5555555556',\n            ],\n        ],\n    ])\n]);\n. The code looks correct, and I'm pretty sure it's only because you are using DynamoDB Local that the value does not appear in the result. See http://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Tools.DynamoDBLocal.html#Tools.DynamoDBLocal.Differences\n. Here is a link to a blog post related to using M and L types: http://blogs.aws.amazon.com/php/post/Tx3QE1CEXG8QG1Z/DynamoDB-JSON-and-Array-Marshaling-for-PHP\n. Someday... :smile: \n. It looks like your parameters are formatted incorrectly. If you look at the PHP SDK's API docs for respondDecisionTaskCompleted, it shows that decisions should be an array or associative arrays, where you have it as a single associative array. It needs to be\n// ...\n'decisions' => array(\n    array(\n        'decisionType' => '...',\n        // ...\n    ),\n ),\n // ...\n. Support for that parameter was added in version 2.7.1 of the SDK, so please make sure you are using that version or later.\nIf your version is correct, then please share with use the code you are using and a wire log of the request, so we can see what might be going awry.\n. Seems straight forward enough. Thanks Guzzle. :smile: \n. Hello!\nTL;DR: It does not do this intentionally.\nWhen DynamoDB added the B and BS types, we ran into issues with processing the JSON of DynamoDB responses. We had originally turned off the traversing of the JSON data for DynamoDB, because it makes processing the results faster (i.e, it's just a straight json_decode). Unfortunately, it also made it difficult to apply base64_decode. So, if you turn on the processing, it will do it. It's an obscure Guzzle 3 thing, but this is how you do it:\nphp\n$ddb = \\Aws\\DynamoDb\\DynamoDbClient::factory(array(\n    // ...\n    'command.params' => array(\n        'command.response_processing' => 'model',\n    ),\n    // ...\n));\nHowever, this is a little weird and it would slow down the entire client. We could add decoding logic to the Marshaler, but we would need to be careful how we do it. If someone did have the 'response_processing' option enabled on their client, then we wouldn't want the marshaller to double decode the values. The marshalers decoding logic would need to be opt-in, via a constructor param or something. So, if you wanted to do a PR along those lines, then that could work.\nNote: in V3, this will not be a problem since B and BS values are automatically encoded/decoded by the client always.\n(Sorry for not replying sooner, I've been out on vacation.)\n. Your thoughts @Sazpaimon?\n. I hear ya, and I know it's not ideal at the moment. I'll go ahead and close this issue then, since it will be fixed in V3.\n. Looks good! Thanks. The current behavior was intentional, but this PR makes it work more consistently.\nI'm out for the holidays, but we'll work on pulling this in after I get back. In the mean time, would you please fill out, print, and sign the Amazon CLA? You can send it to the email address listed on my GitHub profile. This is part of our contribution process along with the Apache 2.0 license.\n. I've submitted this for review. Thanks for sending the CLA.\n. Everything is good on our end. Merging.\n. Oh, wow! Did you type all that up by hand? We actually have tools internally that we use to generate the service description files, so we generally don't accept PRs that create or modify those files (accept for iterators and waiters). I'm curious to see how close your description is to what our tool outputs though. Support for the Amazon EC2 Container Service has not been officially sanctioned for the SDKs, which is why we haven't added it just yet. I would hold off doing any more work on this than what you personally need for the time being. After the holidays, we'll get this sorted out with the ECS team.\n. Basically, you should be using the 'ACL' param \u2013OR\u2013 the 'Grants' and 'Owner' params, not both. The error message you received does not make that very obvious, but that message comes directly from the service, and the SDK just passes it along. The Access Control List Overview page in the S3 developer guide will help explain the difference between the simpler Canned ACL ('ACL' param) and the finer-grained Access Control Policies ('Grants' and 'Owner' params).\n. > Is there any way in PHP SDK to get the XML sent to Amazon, this way I'll be able to understand where the XML is not well-formed?\nDefinitely! You can attach a wire logger to see the raw request and response. Doing that will definitely help us debug.\n. I just noticed that a potential problem could be that \"Id\" under \"Owner\" should be uppercase: \"ID\".\n. I just wrote some test code to check my \"Id\" vs. \"ID\" theory, and I'm almost positive that is your problem. \"ID\" should be uppercase.\n. This is the issue tracker for Version 2+ of the SDK. You seem to be using Version 1 of the SDK, which we no longer support. However, we recently accepted a pull request on the Version 1 repository that fixes your issue. You can download a zip of the source of the amazonwebservices/aws-sdk-for-php repo if you need the fix. It's also just a one line change if you want to patch your current copy.\n. Cool!\n. Hmmm... yeah, that POST thing is definitely a regression. Sorry that it caused you trouble, but thank you for letting us know. You are using version 2.7.13, right? I'm going to tag a new release with the fix some time today.\n. Tagged a release with the fix.\n. > Do we really need to have a \"waiters2\" format? Could we not just change the existing one given that 3.0 is still a wip.\nFor now, yes. The \"2\" is important to our team's internal processes.\n. Cool. I'm going to hold off until #451 is merged, and then reapply my changes with updated namespaces and such.\n. :open_mouth: Wow... I'll look through this some more in the morning.\n. We have a lot of things in the SDK that require iterator-like behavior. Bumping to 5.5 to use generators will be amazing. We've run into a lot of \"gotchas\" with the SPL iterator classes that aren't worth dealing with. It's also a pain to have to write our own iterator classes for things, when you can do the same thing with a single generator function. I'm totally in favor of this version requirement change. If you think you're transducers library is the right solution for the SDK, I'm not opposed to that. It seems pretty useful. I still have a few more things to check out on this PR, before you merge.\n. :ship: \n. The 'Body' parameter of putObject() can accept a stream created by fopen(). So if you do fopen('http://...', 'r'), then it could work. However, S3 requires that the content-length be provided. When you upload a local file, the SDK will calculate that for you. In the case of a stream, you would need to explicitly specify that using the 'ContentLength' parameter.\n. LGTM. I did notice that the Sqs client factory is not marked as internal though.\n. Cool, that should do the trick. :ship: \n. :ship:\n. LGTM!\n. Cool. That looks good.\n. I inlined some suggestions. Take them or leave them. Looks like a good addition for the user experience though. :smile:\n. Seems like a good way to separate out the signature logic.\n. Hooray generators!\n. I like it. Seems like a good way to contain all that information. :ship:\n. Seems like a reasonable change. I'm inclined to just make it public though. Same with unmarshalValue. Do you want to update this pull request?\n. Thanks!\n. LGTM\n. Cool. That looks good. :ship:\n. Cool! :ship: \n. @Sazpaimon you might be interested in this.\n. @mtdowling pushed a new commit with a NumberValue wrapper and a few other changes. Let me know what you think.\n. @mtdowling Updated SetValue stuff to make it immutable. Take another look. If you are good with this now, I'll rebase and merge. \n. This looks good. :ship: \n. That looks good! Cool.\n. :ship: Looks good.\n. Nice! I like where this is going.\n. :+1: Yeah, this looks great.\n. If you need to get this working for yourself, you could just set the 'credentials' config option directly with the fromIni method.\nphp\n$awsProfile = array(\n    'credentials' => Credentials::fromIni(SQS_PROFILE, SQS_CREDENTIAL_FILE),\n    'region'      => SQS_REGION\n);\n. We'll look into it and see if the issue is with the content formatting or our doc generation. Thanks for letting us know about the problem, and I'm glad you figured it out.\n. Cool. Can you amend the commit to change your tabs to 4 spaces to match the rest of the file?\n. They share so much logic, that it didn't seem necessary to separate them. And patterned after the other Provider objects in the SDK, the ApiProvider has static methods that return a callable.\n. I can't seem to reproduce this error. Here is my code:\n``` php\n$s3 = Aws\\S3\\S3Client::factory([\n   'profile' => 'lindblom',\n   'region'  => 'eu-central-1',\n]);\n$s3->addSubscriber(\\Guzzle\\Plugin\\Log\\LogPlugin::getDebugPlugin());\n$bucket = 'just-peachy-frankfurt';\n$key = 'lovely-little-file.txt';\n$s3->getObjectAcl(['Bucket' => $bucket, 'Key' => $key]);\n$s3->putObjectAcl(['Bucket' => $bucket, 'Key' => $key, 'ACL' => 'public-read']);\n```\nAnd here is the logger output:\n```\nRequest:\nGET /lovely-little-file.txt?acl HTTP/1.1\nHost: just-peachy-frankfurt.s3.eu-central-1.amazonaws.com\nUser-Agent: aws-sdk-php2/2.7.18 Guzzle/3.9.2 curl/7.30.0 PHP/5.6.4\nx-amz-content-sha256: [REDACTED]\nx-amz-date: 20150216T164736Z\nAuthorization: AWS4-HMAC-SHA256 Credential=[REDACTED]/20150216/eu-central-1/s3/aws4_request, SignedHeaders=host;x-amz-content-sha256;x-amz-date, Signature=[REDACTED]\nResponse:\nHTTP/1.1 200 OK\nx-amz-id-2: [REDACTED]\nx-amz-request-id: 99CFEE3BB9B065F3\nDate: Mon, 16 Feb 2015 16:47:29 GMT\nContent-Type: application/xml\nTransfer-Encoding: chunked\nServer: AmazonS3\nxml version=\"1.0\" encoding=\"UTF-8\"?\n[REDACTED][REDACTED]FULL_CONTROL\nRequest:\nPUT /lovely-little-file.txt?acl HTTP/1.1\nHost: just-peachy-frankfurt.s3.eu-central-1.amazonaws.com\nUser-Agent: aws-sdk-php2/2.7.18 Guzzle/3.9.2 curl/7.30.0 PHP/5.6.4\nx-amz-acl: public-read\nx-amz-content-sha256: [REDACTED]\nx-amz-date: 20150216T164738Z\nAuthorization: AWS4-HMAC-SHA256 Credential=[REDACTED]/20150216/eu-central-1/s3/aws4_request, SignedHeaders=host;x-amz-acl;x-amz-content-sha256;x-amz-date, Signature=[REDACTED]\nContent-Length: 0\nResponse:\nHTTP/1.1 200 OK\nx-amz-id-2: [REDACTED]\nx-amz-request-id: 172CD5CAAA08BF85\nDate: Mon, 16 Feb 2015 16:47:30 GMT\nContent-Length: 0\nServer: AmazonS3\n```\nCould you attach a logger like I've done and provide the output. That might help us see what the issue is.\n. > Thanks for the reply! No offense meant, but no wonder you can\u2019t reproduce this error if you don't use my test case :-)\n\nThe error happens only if I modify the array returned by get->('Grants'), see my code above. Have you tried running it?\n\nYeah, I know, I wanted to show you how it looks when it is working correctly. What you were doing with the $grants wasn't significant to the putObjectAcl request, because you weren't passing that back in.\n...Except this time it was significant, because you were messing with the $key variable. :smile: The wire logging helps out a lot, and helps you see when the stuff getting sent over the wire doesn't look right, like that 0. I'm glad that you were able to solve the problem.\n. Thanks for taking them time to send a pull request. However, the docs are correct as they are currently written. We recently made a change in the latest version of the SDK to allow the 'credentials' key, and are choosing to document it this way going forward since it is forward compatible with the upcoming Version 3 of the SDK. If you upgrade to the latest version, 'credentials' will work. Using 'key' and 'secret' at the root will continue to work as well, since that is how we used document it, including in that blog post.\n. Nice. I like this change. :ship:\n. IAM has a global endpoint. If you are specifying sa-east-1 as the region, it is probably signing the request with sa-east-1, which would be incorrect for IAM. I will be pushing a fix to the SDK today that should solve this problem and allow your region configuration to work. Another way to avoid the problem is to configure your IAM client to use the us-east-1 region.\n. Fixed by 6e8bb87.\n. When you call getDomainClient(), you'll need to provide your credentials somehow. We do not automatically use the ones from the general CloudSearch client you call getDomainClient() from, since they may not necessarily be the ones your domain is configured to use. See the section of our user guide on the getDomainClient() helper method for more details. If you have any more questions, feel free to reopen the issue. Thanks.\n. (Note: Conversation continued from Twitter)\nThanks for the details!\nTo get the SDK working...\nFirst, you need to include the aws-autoloader.php file. This uses the spl_autoload_register() function internally to register the SDK's classes so they are available to your code. It looks like you have done that already. :smile:\nSecond, if you take a look at the top of the Using the Amazon S3 PHP client page in our docs, you'll see that before we reference/use the S3Client class, we have the line use Aws\\S3\\S3Client;. This is because the SDK's code is all namespaced, and we use PHP's use statement to \"import\" the class, which also aliases it to the shorter name.\nSo, if you change the top of your file and add the use statement, the S3Client class should work.\n``` php\nrequire_once('getid3.php');\nrequire('aws-autoloader.php');\nuse Aws\\S3\\S3Client;\n// ...\n```\nNext, you'll need to make sure that you provide your AWS credentials somehow so that the SDK is able to authenticate with the Amazon S3 service.\nFinally, I noticed that you set 'key' => $accessKey in your usage of the putObject() method. That is not quite right. The \"key\" in this case, is referring to the key (name) of the object (file) you are storing in S3. It should probably be something derived from your $FullFileName variable if you want to make it unique and recognizable.\nI hope this helps. :smile:\n. This repo is for Version 2+ of the SDK. The repo for version 1 is at amazonwebservices/aws-sdk-for-php, but we no longer update that version of the SDK.\nIt is strange that you would get that error all of a sudden though, because we have not changed anything in the library in a long time. The only thing we changed recently in version 1 is that we tagged a 1.6.3 release that removed a particular CURLOPT that doesn't work on PHP 5.6. That doesn't seem related to your problem though.\n. OK. Well, I guess I'll go ahead and close this issue then. Must be ghosts. :ghost: I'm glad you have it working with the newer SDK.\n. If you have any questions about the expected latency of those operations, you should ask on the AWS forums.\nFor some general things to consider about the SDK, make sure to take a look at some of our performance tips. Measuring performance with xdebug can be problematic, because enabling xdebug, in general, slows down the execution of the code.\nYou could also consider doing those CloudFront requests concurrently. See Executing Commands in Parallel from our user guide.\n. Thanks!\n. Cool! LGTM. :smile: \n. Merged manually via 61af047e\n. The only thing I'm not sure about is \"limit\", which was kind of confusing. I thought, at first, that \"limit\" meant the total limit, and had to read through everything to see that it meant the limit of one batch.\n. I just think it's confusing since PHP devs are used to thinking about limit like a SQL LIMIT. It's probably not a big deal. Other terms we've used in the past include size, pool_size, batch_size, and concurrency; but I'm not sure if those are better. Let's just leave it as limit.\n. It loos like you are setting the signature config option incorrectly.\n1. You shouldn't need to provide this configuration value at all, because the SDK will automatically use the correct signature based on the service you are using and region you are specifying.\n2. That is the wrong object anyway. It would need to be an instance of Aws/S3/S3SignatureV4, but like I said in (1), you should not need to set this explicitly.\n. I'm not able to reproduce this error, and I've tested your code on several different sized files. Are you using the latest version of the SDK? How do you know you are getting an infinite loop?\n. No, I don't think it should. PHP behaves a little differently when doing fread on a remote stream vs. a local file. In the PHP Manual entry for fread it says:\n\nif the stream is read buffered and it does not represent a plain file, at most one read of up to a number of bytes equal to the chunk size (usually 8192) is made; depending on the previously buffered data, the size of the returned data may be larger than the chunk size.\n. Looks good!\n. LGTM\n. @GrahamCampbell This is just one part of a big refactor on a WIP branch.\n. We'll get there. It's just going to take more than 1 commit. There are a lot of moving parts. :smile: We should have green tests again sometime next week. And by that time we'll have PSR7 compatibility, better async support, and better decoupling between our SDK and HTTP layers.\n. I see what you did there. :wink:\n. Hello!\n1. I'm not familiar enough with the specifics of the Route 53 API to give you specific advice on what your parameters should be, but you'll need to use the Route53Client::changeResourceRecordSets method. This example from Route 53's API docs seems like it might be conceptually close to what you are trying to do. If you need additional guidance on how to use the Route 53 service, you should visit their forum.\n2. That question would be better suited for the AWS CloudFormation forum, since they are going to be able to provide the best help for writing CloudFormation templates.\n\nI'm going to go ahead and close this issue since you will need to head over to the AWS forums to get your questions answered. If you end up needing any specific help with the SDK related to these questions, feel free to reopen this issue.\n. To include an attachment in your email, you must manually create a multipart email message, with all of the necessary data. This includes a MIME part that contains an appropriate Content-Type header, along with the MIME-encoded content, and a Content-Disposition header, to specify whether the content is to be displayed inline or treated as an attachment.\nOnce you have constructed your message, you can send it using the SendRawEmail API, which can be done using the the SesClient::sendRawEmail method in the SDK.\n. The ElasticLoadBalancingClient::describeLoadBalancers() method would be the way to get information about a load balancer.\n. Closing. Feel free to reopen if needed.\n. Very cool. :+1: \n. Looking good so far\n. LGTM. Man... promises...\n. The bucket name is probably \"ps-scores\", and \"data/\" should be part of the key (filename). If that doesn't work you will need to provide us the code you are running that is causing the error.\n-- Jeremy Lindblom\nPHP Software Engineer, AWS\n\nOn Apr 19, 2015, at 10:34 PM, Dave Berry notifications@github.com wrote:\nI am getting the same error\nAws\\S3\\Exception\\SignatureDoesNotMatchException: AWS Error Code: SignatureDoesNotMatch, Status Code: 403, AWS Request ID: DF90119C6E86B26D, AWS Error Type: client, AWS Error Message: The request signature we calculated does not match the signature you provided. Check your key and signing method., User-Agent: aws-sdk-php2/2.8.1 Guzzle/3.9.3 curl/7.35.0 PHP/5.5.9-1ubuntu4.7 Laravel/4.2.17 L4MOD/1.1.0\nNot sure what signature version I'm using, but I am using us-east-1\nThe bucket is already created, but the bucket name is ps-scores/data (so no periods in it).\nThe filename being uploaded is basically a uuid: fbf7bb3a-ba6f-41dc-a27f-e69955553F31.psc\nThis was happening in 2.8.0 and 2.8.1\nI can replicate the error every time.\n\u2014\nReply to this email directly or view it on GitHub.\n. LGTM\n. I'll address it there.\n. @mtdowling Code and tests are complete now, but some are failing on Travis (looking into it). Ready for CR if you haven't gotten around to it yet.\n. Gonna merge now. We can make other updates if needed, but this is looking good.\n. :+1: \n. Merged manually.\n. Glad it's working now. Thanks for the update. We'll go ahead and close this.\n. Looks good. PHPStorm must be too forgiving about the ordering on the @var declarations.\n. Ouch, that is an unfortunate naming collision.\n\nThe operation is still executable by using command objects though.\nphp\n$result = $cloudHsmClient->getCommand('GetConfig', [...])->getResult();\nWe'll discuss what we want to do about the naming collision long-term.\nAre these issues you are submitting things you are running into while using the SDK, or are you doing an audit of the code?\n. Yeah, we'll address the issue. Thank you for bringing it to our attention.\n. How did you determine that this PR has the correct name?\n. Just reproduced this problem, and will accept the PR. Thanks.\nNote: We do not encourage catching granular exceptions like this. See this for an explanation.\n. Yep, that makes sense. Thanks for reporting these issues and providing PRs..\n. You are right, we don't use these. Error data is parsed from the response based on the service's over-the-wire format. See https://github.com/aws/aws-sdk-php/tree/master/src/Aws/Common/Exception/Parser. The lists of errorResponses in the service descriptions are not really significant.\n. \u00a1Me gusto mucho! :+1:\n. Let's see what the SDK is sending over the wire. Then we'll be able to tell where the problem is. Please attach the wire logger to your client provide the output for the updateDistribution request you are making.\n. Nice, thanks. OK, I don't see OriginPath in there, do you mind sharing the code you are working on?\n. Also, the field you are having trouble with (OriginPath) is a newer feature that is only available in version 2.7.19 and higher. You may just need to update your copy of the SDK.\n. Great. No problem. Code on!\n. Simple. :ship: \n. :ship: \n. LGTM. Should we add any notes about eu-central-1?\n. Cool!\nPHP7...\n. This is the issue tracker for the AWS SDK for PHP. Are you looking for the Go SDK? \n. Thanks for bringing this to our attention. I've verified that this patch makes the operation work correctly. Since we generate service description files before putting them in the SDK, there may be a change we need to make to our upstream tools. We'll get back to you soon.\n. This is not needed for v3. It works fine as is. The service description formats are little different from v2.\n. We have not experienced this issue with the aws.phar, and we run our test suite with the phar regularly. Could you provide a code sample the reproduces the issue?\n. WAT?\n. Thank you for bringing this to our attention. We will investigate this to see if we can reproduce or find a cause for this behavior.\nThe uploader object has an event dispatcher that you can hook into to create your own \"safety-net\". :smile: Here is an example:\n``` php\n// Configure your builder.\n$uploader = UploadBuilder::newInstance()->build();\n// Attach some event listeners to keep track of parts uploaded.\n$dispatcher = $uploader->getEventDispatcher();\n$numUploads = 0;\n$dispatcher->addListener($uploader::BEFORE_PART_UPLOAD, function ($event) use (&$numUploads) {\n    $numUploads++;\n});\n$dispatcher->addListener($uploader::AFTER_UPLOAD, function ($event) use (&$numUploads) {\n    $countedParts = count($event['state']);\n    // Compare the number of parts recorded in the upload's state to the ones you counted.\n    if ($countedParts !== $numUploads) {\n        throw new \\RuntimeException(\"Multipart upload is missing parts for completion. \"\n            . \"Found {$countedParts}, but expected {$numUploads}.\");\n    }\n});\n// Trigger the upload.\n$uploader->upload();\n```\nLet us know if you are able to provide any additional information.\n. Trying to figure those out. I did not have those earlier. :-/\n. Ah, test state got me down. All better.\n. It's good. Just forgot.\n. Looks good. Nice updates.\n. Reviewing this now.\n. Looks good, except for the one comment I made on the S3 Transfer changes.\n. I'm not sure I understand quite what you mean. Is there an error?\n. :+1: \n. BucketStyle middleware only gets added if bucket_endpoint config is specified: https://github.com/aws/aws-sdk-php/pull/581/files#diff-4ca0826aef3c8f72db542dfae087e933R70\n. Thanks, we'll take a closer look at this soon. At a glance, it looks like some good changes.\n. Looks great! :ship:\n. Tracking this internally now.\n. Yup. Thanks.\n. Could we do >=5.3,<7?\n. The V3 SDK is not picking up the credentials you specified, because we do not support using 'key' and 'secret' in the same way as V2. This segment of the Migration Guide explains why and links to the Configuration page where you can find the correct settings. TL;TR: 'key' and 'secret' should be nested under 'credentials' (e.g., [..., 'credentials' => ['key' => AWS_KEY, 'secret' => AWS_SECRET_KEY], ...]),\n. Latest stable version of the SDK has the file here: https://github.com/aws/aws-sdk-php/tree/master/src/data/email/2010-12-01\nHowever, depending on how you are instantiating the SesClient, you might be hitting a bug, that I just fixed here: https://github.com/aws/aws-sdk-php/commit/c66b1cfa66feee34cc589a8c4583691b14f89c28\n. This fix is included in v3.0.2.\n. Those are the V2 docs, but they still need to be updated. Here's the V3 User Guide.\n. Fixed by e9b8623c1e23f533d969ef2269a043f7d4838e18\n. Cool. LGTM.\n. Does this work?\nphp\n$result = $S3client->getObject([\n    'Bucket' => 'my-bucket',\n    'Key'    => 'development/d2f78c44-3206-4718-95d5-3c84f45b49c4.txt',\n]);\nSubfolders should be a part of the key, not the bucket.\n. By the way, KeyPrefix is not a valid parameter. Make sure you are using the correct API documentation. I'm glad you have it working now. :smile:\n. Thanks for reporting this. We'll look into it.\n. ```\n$ make package\ntime php build/packager.php \nmake: time: Command not found\nmake: *** [package] Error 127\nThe command \"make package\" exited with 2.\n``\n. LGTM me. Thanks. :shipit: \n. You need to sync your server's time to an ntp server. As the error indicates: \"RequestTimeTooSkewed` The difference between the request time and the current time is too large.\"\n. :smile: \n. If I understand you correctly, you are:\n- Doing a multipart upload\n- Doing it is several parts, potentially across multiple requests\n- Having trouble uploading the correct part of the file (i.e., starting in the right place)\nFirst, make sure you are setting the PartNumber correctly. Parts uploaded with the same PartNumber overwrite each other.\nSecond, you can use some of Guzzle's stream helpers (see the Guzzle\\Http namespace) to get the desired part of the file.\n``` php\nuse Guzzle\\Http\\EntityBody;\nuse Guzzle/Http/ReadLimitEntityBody;\n$body = new ReadLimitEntityBody(EntityBody::factory($file), 5 * 1024 * 1024, $seekTo);\n// Then later pass this in to the uploadPart operation\n// ...\n    'Body' => $body,\n// ...\n```\nDoes this help?\nNote: This code assumes you are using Version 2 of the SDK. If you are using Version 3, let me know.\n. We may be releasing an official Symfony bundle soon, so I don't think we are going to accept this PR. However, your bundle looks nice. I added it to our forum: https://forums.aws.amazon.com/ann.jspa?annID=1976, and also mentioned it in a tweet.\n. Thanks for bringing this up. We are working on a response.\n. I was just going to suggest you do that. See https://github.com/guzzle/guzzle/issues/1107. Glad that worked out.\n. Nice writeup, @mtdowling, and nice work on the React handler, @WyriHaximus.\n. Thanks!\n. The Command Pool feature would probably be your best route. Check that out and let us know if you have any additional questions.\n. Closing for now. Let us know if you run into questions.\n. We actually did mention it in the migration guide in this section. We plan on releasing it as a separate package, since it is useful on its own and doesn't really need to depend on the SDK. I'll try to get that repo published soon (this week?), and I'll make sure to ping this isue.\n. Yeah, we've extracted it out and completed the code, and it is now under internal review.\n. The repository is here: https://github.com/aws/aws-php-sns-message-validator. We are still working on it before we release a stable version. Not much is changing though from what there was in the V2 SDK. I'm going to close this issue. Feel free to take the new package for a test drive if you want to help out, and open any issues on that repo that you might encounter. Thanks.\n. Sorry that tripped you up. Where in the documentation would that have been the most useful to you?\n. Yeah, I could see how you would find that misleading. I was also thinking some modifications here and here might be helpful too. I no longer work on the SDK, but I'm sure @jeskew could make something good happen.\n. We will look into this (and might have some more questions), but is there a reason why you are using Guzzle 5 instead of Guzzle 6?\n. > The error stated that curl_multi_exec is disabled.\nYep, that would definitely be a problem. The SDK and Guzzle can work without cURL, but it checks if cURL is enabled in general, not if curl_multi_exec specifically is enabled.\n\nI'm using the default config, so not sure why it's using Guzzle 5.\n\nWould you mind sharing your composer.json so we can determine why? If you are not explicitly using Guzzle 5, then it would be best to use Guzzle 6.\n\nI'll let you know if that fixes the issue.\n\nThanks.\n. Looks like jelovac/bitly4laravel requires Guzzle 5. You should ask them if they would consider offering a version that uses Guzzle 6. I may take a look at it at some point as well.\nAnyways, I'm glad you have it working now.\n. It looks like this was missing, so I've added support via #629.\nUntil the next version, you can achieve the same effect using the \"sink\" HTTP option:\nphp\n$result = $s3Client->getObject([\n    'Bucket' => $cloudBucket,\n    'Key'    => $currFileName,\n    '@http'  => ['sink' => $tempFullFile]\n]);\n. Awesome. You're official now. :ship:\n. What type of instances are you using? How is the load on the server? Any other similar errors for other files in your application? Any other patterns or information to share?\nThe error is clearly not caused by the file actually missing. I suspect that you may be hitting the system's limit on maximum open file descriptors. This is known to cause sporadic \"file not found\" type errors.\n. Well, not necessarily. All this means is that something you did used more memory than you have. 134217728 bytes is 128MB. You might consider checking the memory_limit INI setting and increasing it. \nDo you have any code to share about what you are doing? We could take a look at how you are using the SDK if there is anything to optimize.\n. @mtdowling New integ tests added for this. Other tests were passing as well (tried them all before the PR).\n. > Any Ideas?\nMake sure you have https:// or http:// in your URL. I think that should fix your issue.\n\n'version' => '2013-01-01', // This caused an error if I didn't add it someone should put something in the documentation but w/e\n\nHere are the docs on 'version'. Where were you reading that might benefit from some clarification?\n. A made some general changes in #641that solve this issue in a different way.\n. Looks great aside from my one comment. Nice work on this. Thank you.\n. What are we waiting on for this PR?\n. Looks good!\n. I'm going to merge this so the fix is applied, but we can update the test later.\n. Thanks for reporting this. We'll look into it.\n. Thanks.\n. :ship: \n. I think replacing is fine, since this addition makes the test more complete.\n. Yeah, we have a bad tag. Will be pushing 2.8.12 momentarily.\n. Thanks for the heads up! The 2.8.11 release was accidentally tagged to a commit from Version 3 of the SDK, which is why things appeared to be missing. We've removed the 2.8.11 release completely, and 2.8.12 should work for you. Closing.\n. What version of the SDK are you using?\n. What patch version of 2.8? We'll investigate this issue.\n. @BardiaAfshin, my co-worker, @jeskew, will be addressing you questions soon.\n. We just tagged 2.8.11, and it was a bad tag. We are fixing that in just a minute. Version 3 of the SDK does not support SimpleDB anymore though, since the service does not support authentication with Signature Version 4.\n. > Crap, now to figure out how to run two versions.\nNot necessarily. If you really need SimpleDB in V3, you can get the code from before this commit. I'll also make a note that you asked for it.\n\nOhh, could the removal be added to the release notes? It might save someone else from having to search through the various updates.\n\nUmmm... yeah, I will do it. I'm not sure why we missed doing that beforehand. Thanks.\n. @ghost Hey, I saw that you closed this. Did you find out something?\n. :ship: \n. I believe setting the 'ContentSHA256' parameter should work. We'll work on fixing the docs.\n. @dstevenson \n\nI attempted to set the ContentSHA256 but it's not part of the StructureShape definition for the PutObjectRequest\n\nYou're right. My mistake. It is a parameter for Glacier upload operations, and we plan on adding support for it in S3 now as well.\n\nI think a potential workaround might be to use a middleware that prepends the sign step and adds the header\n\nYep. Your code for that looks great.\nThanks for letting us know about this issue.\n. Closing this now. Improvements related to this issue are being made in #661.\n. LGTM.\n. Looks good.\n. :+1:\n. Yep, that sounds like the best solution. I'll make a note that we need to add something to our docs about this.\n. @mtdowling and @jeskew ping\n. Hmmm...\n. We messed up the 2.8.11 release, so we created 2.8.12 and removed 2.8.11. Sorry about the confusion.\n. Even if there are unsuccessful UploadPart operations somewhere in the middle of the multipart upload, it's best to let it all finish, because the UploadState tracks exactly which parts failed. When you get the state from the exception at the end, you can create a new MultipartUploader from the state that will only attempt to upload the missing parts. An example of this technique is demonstrated here: http://docs.aws.amazon.com/aws-sdk-php/v3/guide/service/s3-multipart-upload.html#recovering-from-errors\n. I will add some clarifications to the documentation. Thanks for the feedback.\n. Can you show us what your code looks like now?\n. Does the \"Invoking the wait callback did not resolve the promise\" error happen every time, or just some of the time?\nSide Note: I added some more documentation to the user guide for multipart uploads as per our earlier discussion. See https://github.com/aws/aws-sdk-php/commit/2df31b39e2c2cd818f190d05fb85d4f4ec5c5524\n. @zexz We just noticed this problem as well. It's fixed in d34b5e628ae5f41e614f12db1741c3d1960777d1.\n. It looks good to me. If you are confident about it, go ahead and merge.\n. The SDK gets the role and role's credentials from the instance itself, so I'm not sure how it could be assuming the wrong role. How are you determining if it is incorrect? What version of the SDK are you using and how are you instantiating the client?\n. Thanks for reporting this. It may take us some time to look into it and try to reproduce the issue. If we have any follow up questions, we'll let you know.\n. Also, if you have the time and desire to try out the multipart uploader in Version 3, that would help us determine if the issue is isolated to just Version 2. The V3 docs for this feature are here.\n. Thanks for giving that a shot. We'll have to do some research into this.\n. Even though the URL to the docs says \"latest\", those are actually the docs for Version 2, so that is correct. We are working on deprecating all the \"latest\" links. The links you should use for each version of the SDK are the following:\n- Version 3:\n  - User Guide: http://docs.aws.amazon.com/aws-sdk-php/v3/guide/\n  - API Reference: http://docs.aws.amazon.com/aws-sdk-php/v3/api/\n- Version 2:\n  - User Guide: http://docs.aws.amazon.com/aws-sdk-php/v2/guide/\n  - API Reference: http://docs.aws.amazon.com/aws-sdk-php/v2/api/\nThese links are what we use on our GitHub README and product home page.\n. Looks good to me.\nNot that it's terribly important, but I'm curious about why you are extending the class.\n. If you are using Version 3 of the SDK (V3), you should be looking at this page: http://docs.aws.amazon.com/aws-sdk-php/v3/guide/guide/waiters.html. You can find the available waiters for a service in the V3 API docs, for example, here are Glacier's: http://docs.aws.amazon.com/aws-sdk-php/v3/api/api-glacier-2012-06-01.html#waiters\nAll the docs with \"latest\" in the URL are actually for V2, we are working on fixing incoming links and adding redirects to clear up confusion there.\n. :smile: \n. @e-oz \n\nIt's frustrating this library only works with Composer.\n\nWhere did you get that impression? The README says:\n\nYou can get started in minutes by installing the SDK through Composer or by downloading a single zip or phar file from our latest release (emphasis added).\n\nAnd further down it says:\n\nUsing Composer is the recommended way to install the AWS SDK for PHP. (...) Please see the Installation section of the User Guide for more detailed information about installing the SDK through Composer and other means. (emphasis added)\n\nAnd then when you go to the \"Installation section of the User Guide\", it has 3 sections: \n- Installing via Composer\n- Installing via Phar\n- Installing via Zip\n\n\nComposer is too awful to use.\n\nI think you might be in the minority on that opinion. Most would say that Composer makes it easier, not harder, to use third-party code. AFAIK, every major PHP library and framework has moved to using Composer for installation and autoloading. So, if you are interesting in using others' libraries, you better get used to the idea of using Composer. Also, the Packagist stats are pretty interesting.\n. Happy to help. While I'll still believe Composer is the best option, the phar and zip options are good if you have an existing project that is setup in a way that makes Composer difficult to incorporate. I'm glad you have it working now.\n. Seems like a good change, but going to do a more in-depth review before merging. Thanks.\n. Looks good.\n. I like this more that the previous pull request.\n. :ship: \n. Why do you need to do this?\n. If this passes our internal team code review, then it seems fine to me.\n. :ship: \n. LGTM.\n-- Jeremy Lindblom\nPHP Software Engineer, AWS\n\nOn Jul 20, 2015, at 1:58 PM, Jonathan Eskew notifications@github.com wrote:\n(and make account state-related failures count as a skip instead of a fail)\nYou can view, comment on, or merge this pull request online at:\nhttps://github.com/aws/aws-sdk-php/pull/700\nCommit Summary\nMake the smoke tests less dependent on account state (and make account state-related failures count as a skip instead of a fail)\nFile Changes\nM features/smoke/codedeploy/codedeploy.feature (8)\nM features/smoke/emr/emr.feature (14)\nM tests/Integ/SmokeContext.php (91)\nPatch Links:\nhttps://github.com/aws/aws-sdk-php/pull/700.patch\nhttps://github.com/aws/aws-sdk-php/pull/700.diff\n\u2014\nReply to this email directly or view it on GitHub.\n. Cool!\n. What do you think @mtdowling & @jeskew ?\n. I'm not sure we should let people override the whole UA string. We always want aws-sdk-php/VERSION as the first part of the UA string.\n. :ship: \n. If we had a model, we wouldn't have to handwrite it, and we could generate docs.\n. You should be using the ContentType parameter, not MetaData for this, as documented in the V2 and V3 API reference for PutObject. Example: \n\nphp\n$s3->putObject([\n    'Bucket'=>$bucket,\n    'Key'=>$key,\n    'Body'=>$body,\n    'CacheControl'=>'max-age=172800',\n    'ContentType'=>$mime\n]);\n. Looks good.\n. :ship: \n. Maybe you could add a check for a protocol on the provided endpoint and throw an exception if it's not there with a helpful message. Letting a bad endpoint go all the way to a cURL error seems like it is no fun. This is especially important for this service because customers tend to copy & paste their cloudsearch endpoint right from the console, which does not include a protocol.\n. Cool!\n. LGTM. :-) Hi guys!\n. I'm not really for or against this feature, but I did want to weigh in on the PSR-11 status. ZF and Aura have already implemented ContainerInterface, but many other FIG peeps are opposed or uninterested in the spec. It will likely be a long time before it is voted on and may not end up getting accepted. ContainerInterface will continue to exist regardless of the PSR though.\n. I'm pretty sure the SDK cannot support that by default. The SDK does transformations on the URL based on S3's requirements and is probably not treating the \"s3/\" as part of the base endpoint, since it is not a part of the host.\n. More Pros:\n- People's Satis repos will be smaller.\n- Can release an \"S3-Only\" package, which people occasionally look for.\n- Service clients with special dependencies (e.g., openssl, yaml, etc.) can have their own dependency chains.\nMore Cons:\n- No compelling argument to support the time required to make the change. Are there really any compelling and tangible benefits?\n- Currently service descriptions are all in one folder and the tools that help build the SDK rely on this fact.\n- Messaging \u2013 Making sure people know when the services they are using are updated may become more difficult. For example, are the release notes split up as well?\n- People who don't use Composer (there are still a lot) won't care about this at all and might be confused by it.\n- Cross-advertising services via the tooling will no longer happen naturally.\n. You are experiencing the infamous PHP\u2013Windows error: http://docs.aws.amazon.com/aws-sdk-php/v3/guide/faq.html#why-can-t-i-upload-or-download-files-greater-than-2gb\n. Nice work. :smile: \n. Sounds like this should find it's way to the docs or blog.\n. First, make sure a Set (S), not List (L), is actually the type that you want.\nThe SetValue exists because an array like following [0 => 3, 1 => 4, 2 => 2] is too ambiguous for the Marshaler to infer it's type (i.e., it can't tell if you want {\"NS\": [3, 4, 2]} or {\"L\": [{\"N\": 3}, {\"N\": 4}, {\"N\": 2}]}). In order for the data to make a roundtrip from output to input and preserve the types, the SetValue wrapper is required. However, if you are just reading data, I can see the usefulness of adding the new options to the Marshaler. However, 'wrap_booleans' should be 'wrap_binaries' instead.\n. @jeskew the reason I didn't make SetValue implement ArrayAccess is that I did not want it to be mutable in ways that would corrupt the data. For example, $set['foo'] = 'bar'; would be bad.\n. What about disabling wrappers in a one-off way. Like as a second argument to unmarshalItem. Not sure I'm 100% sold on that idea, but it is an idea.\n. I think @bakura10 is looking for something to make the use case of \"I'm just reading data and not writing it back\" easier. When you are not making a roundtrip, the wrapper objects are clumsy. I agree that adding a class-level option to turn off wrappers seems unsafe, but adding a parameter on unmarshalItem to turn off wrappers seems fine. In that case, you make the decision to not use wrappers just for the one-off context that you are calling it in. I'm not 100% sold on whether or not you should make that change, but it seems like it could be useful and would not cause problems, IMO.\n. Looks good!\n. :fried_shrimp:\n. You may also want to check out this: https://github.com/WyriHaximus/react-guzzle-psr7\n. Interesting results. :confused: \n. This looked \"fun\" to do. ;-)\n. Doh! \ud83d\ude26  Nice catch @pspiller . > Looks like from your comment that this doesn't happen on each request. Please correct me if I am wrong.\nIt happens on every request to buckets in us-west-2. The versions of our app interacting exclusively with buckets in us-east-1 still work.\nWe've downgraded to version \"~3.24.0\" for the time being.\nSDK is configured like this:\nphp\n$aws = new Sdk([\n    'region' => 'us-east-1',\n    'version' => 'latest',\n    'credentials' => [\n        'key' => getenv('AWS_KEY'),\n        'secret' => getenv('AWS_SECRET'),\n    ],\n    'S3' => [\n        'bucket_region_cache' => $cache,\n    ],\n]);\n// ...\n$s3 = $aws->createMultiRegionS3();\n// ...\n$s3->putObject([\n    // standard stuff (e.g., Bucket, Key, SourceFile, ServerSideEncryption, ACL)\n]);\nThe code and config has been like this for almost a year, the only thing that has changed is the SDK version.. Tagging @Azuka, in case he wants to follow the conversation or add details.. Hmmm... yeah, it probably should be. \"Good eye!\", as my little league coach would say.\n. Yep, I already changed that one. Running tests.\n. Maybe a more specific runtime exception like UnexpectedValueException would be better. Also, I think checkIntegrity() would be better returning a boolean value and the download() method should throw the exception. What do you think about that?\n. What is the rationalization for removing this section? Is there anything that should be kept or recommended in a different way?\n. Makes sense.\n. Actually, I wish we would have had them earlier. It would've made refactoring a lot easier. I would consider them useful to the internals in case we end up changing our iterator spec in the future.\n. You are thinking about a future world in which people are exclusively using PHP 5.4+ or PHP 5.5+. The reason you implemented the MapIterator and FilterIterator classes in Guzzle is because, while PHP does have an idiomatic way of decorating iterators, there are no iterators in PHP 5.3 worth using to decorate with to get this behavior. I thought it would be a nice little addon to expose the usefulness of those iterator classes without having to find and import them. Not an altogether important feature, so if you want to object, I'll remove it.\n. > These values are part of a configuration format, meaning they are already part of an external API. Being able to change the name of these keys seems like a weak argument considering that changing the name of these keys is a breaking change that should only be done in the most extreme cases.\nGood point there. The changes I made then are technically breaking changes since 3 of the keys changed names. We should probably discuss this a little more to see how we can prevent breaks but still  move forward to adopting the new spec.\n. They are chainable, but this is for the Iterator objects, not the Model objects.\n. I forget... why is --prefer-source a good idea here?\n. That makes sense. Thanks for clarifying.\n. Yeah, I didn't follow that either.\n. Yeah, it seems a little faster to use gettype at the top and do string comparisons instead of is_* checks.\n. We originally had a formatValue function. There are places in the API where you only need to provide a single [TYPE => VALUE] value. I don't know how valuable it actually it though.\n. unmarshalItem returns associative arrays for maps, but has an option to return stdClass instead, which is the most lossless format. The unmarshalDocument document calls unmarshalItem with that option.\n. Yeah, this method is necessary to roundtrip JSON documents essentially. If anything, we could make the [un]marshalValue methods private.\n. Mostly for things in the UpdateItem operation. I think we could probably make this method and it's inverse private though, since I don't think they are as useful.\n. iterators...\n. I called this \"threshold\" in the S3Client::upload method. We should probably be consistent, but I don't care what name we use.\n. I used a different value in the S3Client::upload method. We should probably use the same value.\n. That seems good.\n. Got it.\n. Why 5? Should this be $this->concurrency?\n. I'm cool with that. :+1: \n. We should move these constants to the Provider class, since they aren't used in the Credentials class anymore.\n. It would be better to set $args['credentials'] to the provider callable than to its result.\n$args['credentials'] = Provider::ini($args['profile']);\nCurrently, if the provider returns null (like if you specify a non-existent profile), then the user gets a cryptic error like\n\nUncaught exception 'InvalidArgumentException' with message 'credentials is a required option'\n\nThis happens because the ClientFactory::handle_credentials method does not do anything with null values, and passes that straight into the client object.\n. I would be surprised if someone was actually referencing these directly.\n. ??? - These 2 lines shouldn't have changed.\n. Why this change?\n. Are we going to provide any kind of client \"caching\" feature (i.e., $sdk->getS3() === $sdk->getS3()) in V3?\n. :wink:\n. Hmmm... there is a potential problem, because CloudSearchClient::describeDomains does not include a scheme when you retrieve the endpoint. This is because the endpoints are accessible via http and https, and it is left up to the user to choose. I had the code set up the way it was, because it is likely that the endpoint provided won't include a scheme, especially when using the CloudSearchClient::getDomainClient method.\n. Sounds good to me!\n. Doh! Looks like I forgot about something there...\n. You might want to review/update this section a little. :wink: \n. Could also do $this->{\"missing_{$key}\"}($args). I usually avoid call_user_func* when I can. It doesn't really matter much though, so whatever you prefer is fine.\n. I almost feel like maybe a <<< would be better here. Meh...\n. Sure.\n. Well, you need to be able to change the type and value. Maybe it could return the same type of array that marshalValue returns or null. That might be better.\n. You would think that, but PHP treats numerical keys as numbers no matter how hard you try to make them strings.\n. Well, they are 2 solutions to the same problem. With ignore_invalid, they are simply dropped from the marshaled data. With invalid_handler, you can massage the data (e.g., convert empty strings to null values). I'm not sure I would want to remove one or the other. What do you think?\n. I wasn't sure what to do with this, since there are no instances of providers any more. Any specific ideas?\n. Ah, nice!\n. content-type?\n. Probably. Stupid closure bindings. If I make the closure static (unbound), it should fix that, right?\n. Oh wait, it's not storing the closure. It's calling array_map and just storing the result.\n. I use $this->state->{methodOnUploadState} everywhere though, so it needs to be an UploadState. Maybe I should track the promise that resolves to the UploadState in a different variable, private to the AbstractUploader?\n. Aborting as a cancel action doesn't make sense. It's a decision you make. For example, you might create a Uploader object for the sole purpose of aborting the upload. That is why I made upload/uploadAsync and abort/abortAsync. Trying to combine them is very weird. I at least chose the same convention as our client objects though, with the async suffix.\n. 13 items. I'll just go head and precompute them.\n. Added to the constructor.\n. There is in the params option. This is now documented in each concrete MUP.\n. Added to the constructor\n. It's needed so it can be overridden if the archive is for an account other than the one performing the operations, but I updated the docblock to remove \"required\" and added a clarification.\n. It's already there, a few lines down.\n. Yep. Apparently, I can't read or write.\n. Yeah, that makes sense. That way, the next block of code throws the exception, which is a more accurate message for that case. I'll update the PR.\n. Updated.\n. Not needed because this middleware's sole reason to exist now is to handle bucket endpoints.\n. good call\n. Whitespace\n. Are you going to replace this with the protocol test?\n. You could use the manifest file for this data. It's best not to repeat it, because we'll forget to update it.\n. Gotcha. I'm working on something that will allow you to remove the rest of that array except the EFS region.\n. Happens a few lines down.\n. Yep, the changes to this middleware make it so the user can provide the x-amz-content-sha256 header. The Signature checks for the presence of the header before doing the work to calculate the signature. Setting the header manually is basically on optimization, or in the case of a non-seekable stream, a necessity.\n. No worries! We appreciate you taking a look at the PR. :-)\n. Feels like cheating. You should at least be able to override the directory in the constructor, IMO.\n. We used underscores instead of hyphens for other config options.\n. aws-sdk-php/{version} needs to be the first part.\n. - Changing $name to $method doesn't work at all.\n- I don't believe there is a need to remove the array typehint.\n- But yes, the = [] should be removed.\n. I don't use that function very often, but when you need it, it's just what the doctor ordered.\n. These types should probably be sorted in some kind of reasonable way.\n. :disappointed: \n. Not sure, but maybe a coroutine would make this easier to read/understand.\n. This comment should probably remain.\n. I'm curious why these are all considered retry-able.\n. Confused by this. Isn't this overwriting $err over and over?\n. ",
    "martinbean": "Thanks, @jeremeamia. I tried the wrapping the file name in fopen() as per your suggestion, but get the following exception now:\n\nValidation errors: [Body] must be of type string or object\n. Actually, I think I may have solved it: I used file_get_contents() instead.\n. Hi Jeremy. No problem. Thanks for clearing this up for me! Much appreciated. I\u2019m new to this Guzzle :wink: \n. Another question: how do I mark a uploaded file as public? I\u2019m reading the API documentation again (http://docs.amazonwebservices.com/aws-sdk-php-2/latest/class-Aws.S3.S3Client.html) but it\u2019s not too clear the expected values. I\u2019m guessing I want to set something for the ACL key, but not what to set it as. Sample values would be great if possible?\n\nEDIT: Fixed it myself (again)! To mark a file as public, I imported the Aws\\S3\\Enum\\CannedAcl class and used the CannedAcl::PUBLIC_READ class constant.\n. Cool. Is there a channel where I can report small things like this that are hard-to-understand or could be better-documented if I come across anything?\n. No problem.\n. I\u2019ve decided to use Uploadify to upload the file via Flash, but still having difficulties.\nWith the new PHP SDK, how do I pass custom fields as policy options to PostObject? I have the following in my PHP script:\nphp\n$bucket = (string) $container['misc_config']->images->amazon->bucket;\n$options = array(\n    'acl' => CannedAcl::PUBLIC_READ,\n    'Content-Type' => 'audio/mpeg',\n    'key' => 'audio/a-test-podcast.mp3',\n    'success_action_redirect' => (string) $container['misc_config']->main->base_url . 'upload/success/',\n    'success_action_status' => 201,\n    //array('starts-with', '$key', 'audio/'),\n    //array('starts-with', '$folder', ''),\n    array('starts-with', '$Filename', ''),\n    //array('starts-with', '$\n);\n$postObject = new PostObject($container['amazon_s3'], $bucket, $options);\nBut I keep getting the following error in the XML response:\n\nInvalid Policy: Invalid Condition: unknown operation 'array'.\n\nIf I remove array('starts-with', '$Filename', '') from the above code snippet, then I get the following error message:\n\nInvalid according to Policy: Extra input fields: filename\n\nWhat\u2019s the syntax for adding these additional fields to my policy?\nEDIT: Found the solution, and I don\u2019t even know how! In my $options array, I now have:\nphp\n$options = array(\n    'acl' => CannedAcl::PUBLIC_READ,\n    'Content-Type' => 'audio/mpeg',\n    'key' => 'audio/a-test-podcast.mp3',\n    'success_action_redirect' => (string) $container['misc_config']->main->base_url . 'upload/success/',\n    'success_action_status' => 201,\n    'filename' => '^'\n);\nDon\u2019t ask my have I managed to arrive at this solution, since it\u2019s not documented anywhere I don\u2019t think!\nAlthough I am now faced with yet another issue. When using a Flash-based uploaded, how can I actually redirect on successful upload? My JavaScript function to instantiate Uploadify looks like this:\njs\n$('#file_upload').uploadify({\n    'buttonClass': 'button',\n    'buttonText': 'Upload',\n    'formData': <?php echo json_encode($formInputs); ?>,\n    'fileObjName': 'file',\n    'fileTypeExts': '*.mp3',\n    'height': 36,\n    'multi': false,\n    'onUploadError': function(file, errorCode, errorMsg, errorString) {\n        console.log('onUploadError', file, errorCode, errorMsg, errorString);\n    },\n    'onUploadSuccess': function(file, data, response) {\n        console.log('onUploadSuccess', file, data, response);\n    },\n    'swf': '/assets/site/swf/uploadify.swf',\n    'uploader': '<?php echo $uploadPath; ?>',\n    'width': 120\n});\nThe content of my success page is logged in the console as the data argument, but I\u2019d like to actually redirect to that page, rather than having the response available to me in JavaScript. Is this an issue with Flash, JavaScript, Uploadify, what?\nFinally, how can I enforce that only files with an .mp3 extension are uploaded?\n. Hi @jeremeamia. Yeah, I feel as though I\u2019m making good progress with it. I\u2019m going to close this as I seem to be pretty much there.\nAlso, I was planning on writing a blog post detailing how I used the PHP SDK to get uploads working :wink: \nHappy Thanksgiving! Thanks again, Jeremy.\n. Thanks for getting back to me. I did open a Stack Overflow issue; AWS support said they couldn\u2019t help me as I don\u2019t have a support plan, and the only response I got on the AWS forums was a support agent asking me to private message them further details but never heard back when I did.\nNonetheless, I have since diagnosed the issue. The objects in my S3 bucket weren\u2019t publicly-accessible, and I hadn\u2019t added an Origin Access Identity to allow CloudFront to access the objects.",
    "dominikgehl": "Ok, found out how to make it work:\n$rf->create(\n         'GET', \n         $s3->getBaseUrl() . '/' . BUCKET . '/testing123.jpg'\n    )->setClient($s3)\n. Would be nice to have a wrapper though which accepted a bucket name, file name and expiration time and would do the above magic behind the scenes ...\n. ",
    "louiszuckerman": "Still no errors/output with the extra error reporting stuff.\nPHP 5.3.10-1ubuntu3.4 with Suhosin-Patch (cli) (built: Sep 12 2012 18:59:41) \n(ubuntu precise with latest updates)\n. Success!  That worked perfectly.  Thank you so much.  :D\n. I should note that I found out this disallowing-phars behavior is a result of my installing the extra php5-suhosin package on top of the basic apache/php packages in ubuntu precise.  on a fresh server without the extra php5-suhosin package the phar works right away, without needing any extra ini file settings.  Here's the php -v output from the two installs, for reference...\nubuntu with just libapache2-mod-php5:\nPHP 5.3.10-1ubuntu3.4 with Suhosin-Patch (cli) (built: Sep 12 2012 18:59:41) \nCopyright (c) 1997-2012 The PHP Group\nZend Engine v2.3.0, Copyright (c) 1998-2012 Zend Technologies\nubuntu with libapache2-mod-php5 and optional php5-suhosin package:\nPHP 5.3.10-1ubuntu3.4 with Suhosin-Patch (cli) (built: Sep 12 2012 18:59:41) \nCopyright (c) 1997-2012 The PHP Group\nZend Engine v2.3.0, Copyright (c) 1998-2012 Zend Technologies\n    with Suhosin v0.9.33, Copyright (c) 2007-2012, by SektionEins GmbH\nwhen the optional php5-suhosin package is installed you need to add the ini directive as mentioned above to load the aws.phar.\nthanks again & hth\n. ",
    "digitaldoener": "``` php\npublic static function values()\n{\n    $class = get_called_class();\n    if (!isset(Enum::$cache[$class])) {\n        $reflected = new \\ReflectionClass($class);\n        Enum::$cache[$class] = $reflected->getConstants();\n    }\nreturn static::$cache[$class];\n\n}\n```\nDo not work.\nI think this bug report https://bugs.php.net/bug.php?id=53915 (fixed in 5.3.6) should characterize the error. The bug should occur up to php version 5.3.6, too. Since \\Aws\\Common\\Enum.php is an abstract class and every of the listed subclasses (http://docs.amazonwebservices.com/aws-sdk-php-2/latest/class-Aws.Common.Enum.html) inherid from this class the error should effect every where self::[constant] is used.\nLike in \n``` php\nnamespace Aws\\DynamoDb\\Enum;\nuse Aws\\Common\\Enum;\nclass Type extends Enum\n{\n    const S  = 'S';\n    const N  = 'N';\n    const B  = 'B';\nconst SS = 'SS';\nconst NS = 'NS';\nconst BS = 'BS';\n\nconst STRING     = self::S;\nconst NUMBER     = self::N;\nconst BINARY     = self::B;\n\nconst STRING_SET = self::SS;\nconst NUMBER_SET = self::NS;\nconst BINARY_SET = self::BS;\n\n}\n```\nI use this call to cause the error\nphp\n                Zend_Debug::dump(\\Aws\\DynamoDb\\Enum\\Type::values());\n                $test = array(\n                        'Id' => '10101',\n                        'test' => 10101,\n                );\n                Zend_Debug::dump(\\Aws\\DynamoDb\\Model\\Item::fromArray($test));\n. ",
    "etiennea": "Yes thanks this works great! \n. Performance is horrible, it takes more than double the time to connect, I have reverted to the v1\n$dynamodb = new AmazonDynamoDB();\n// Register the DynamoDB Session Handler.\n$handler = $dynamodb->registerSessionHandler( array(\n    'table_name' => 'my_sessions',\n    'hash_key'             => 'userid',\n    'session_lifetime'     => 0,\n    'consistent_reads'     => true,\n    'session_locking'      => false,\n    'max_lock_wait_time'   => 15,\n    'min_lock_retry_utime' => 5000,\n    'max_lock_retry_utime' => 50000\n    ));\nany tip on how to correct this would be appreciated\n. Read Throughput 120\nWrite Throughput 40\nstrlen(serialise($_SESSION)) will vary from 40 to max 3000\n. The setting is already 64mb and apc is enabled in phpinfo() do I have to do anything special to enable it for the sdk 2 in the old sdk i had set 'default_cache_config' => 'apc' ?\n. No other frameworks yet just session handler on a fresh beanstalk php 64bit small instance.\nFor the script you have above \nFor sdk2 I get value 0.1648998260498047 sometime over 0.2\nFor sdk1 I get 0.028363943099976 \nI have tried this on several instances, perhaps I should update my ami. \n. ",
    "miccheng": "Don't think its officially supported yet. You'll have to fall back to version 1.5.* for the moment.\n. ",
    "isleshocky77": "@mtdowling Do you think you could take a look at this thread having to do with multipart upload?\nhttps://forums.aws.amazon.com/thread.jspa?messageID=402725&#402725\n. So the uploadPart() is now requiring an UploadId and a PartNumber which it wasn't doing before; so I think this is a step in the right direction; however, I can't get it to work now. It keeps saying:\nThe request signature we calculated does not match the signature you provided. Check your key and signing method.\nI get this error even when I se ValidateMD5 to false.\n. This is my code for doing the Mulitpart Upload manually.\n``` php\n        $body = EntityBody::factory(fopen($fullPath, 'r'));\n        $this->log('createMultipartUpload: Start');\n        $response = $s3->createMultipartUpload(array(\n            'Bucket'      => $awsConfig['bucket_name'],\n            'Key'         => $Item->getFilename(),\n            'ContentType' => $Item->getMimetype(),\n            'ACL'         => CannedAcl::PUBLIC_READ,\n        ));\n        $uploadId = $response->getPath('UploadId');\n        $this->log('createMultipartUpload: Finished with id: ' . $uploadId);\n    $this->log('uploadPart: Starting');\n    $chunkSize = 5242880;\n    $start     = 0;\n    $part      = 0;\n    $length    = $body->getSize();\n    while($start < $length) {\n        $end = $start + $chunkSize;\n        if($end > $length) {\n            $end = $length;\n        }\n        $body->seek($start);\n        $body->setSize($end);\n        $this->log('Sending part #'. $part . ' containing '.$start.' through '. $end . ' of ' . $length);\n\n//             try {\n                $response = $s3->uploadPart(array(\n                    'Bucket'      => $awsConfig['bucket_name'],\n                    'Key'         => $Item->getFilename(),\n                    'UploadId'    => $uploadId,\n                    'PartNumber'  => $part,\n                    'Body'        => $body,\n//                     'ContentMD5'  => hash_file('md5', $fullPath),\n                    'ValidateMD5' => false,\n                ));\n                var_dump($response);\n                exit;\n                $eTag = $response->getPath('ETag');\n                $requestId = $response->getPath('RequestId');\n                $this->log('Part Received with Etag: '.$eTag.' and RequestId: ' .$requestId);\n                $part++;\n                $start += $chunkSize;\n//             } catch(Exception $e) {\n//                 $this->log('Part Failed');\n//             }\n        }\n        $this->log('uploadPart: Finished');\n```\nOuput\n```\n\n\nFile      Sample Video: Sample.m4v\ncreateMultipartUpload: Start\ncreateMultipartUpload: Finished with id: fm3eOkQVwB91FGKsbMOSns9W5uB97fgx_iNhE6cMN1GnvAfkfm3AP2JmOHAQ0VwpYqT0.GD5Uo3EgSS.IUbi71tph0_k37bcKpcmpkOuXU6ofbBWhRo.SvkuK9SXaBp6\nuploadPart: Starting\nSending part #0 containing 0 through 4125718 of 4125718\n\n\nThe request signature we calculated does not match the signature you provided. Check your key and signing method.  \n```\n. I also wanted to let you know I have the Multipart UploadBuidler working; however, I don't know how to be able to add properties (i.e. ContentType and ACL) to it.\n``` php\n$batch = \\Aws\\S3\\Model\\MultipartUpload\\UploadBuilder::newInstance()\n   ->setBucket($awsConfig['bucket_name'])\n   ->setKey($Item->getFilename())\n   ->setClient($s3)\n   ->setSource($body)\n   ->build()\n   ->upload()\n   ;\nvar_dump($batch);\n```\nI thought this was going to be used like the batch in Guzzle like the below sample; however, \"Source\" is required in MultipartUpload\\UploadBuilder\nThis is the batch examples from Guzzle.\n``` php\n $batch = BatchBuilder::factory()\n   ->transferRequests(10)\n   ->autoFlushAt(10)\n   ->notify(function (array $transferredItems) {\n      echo 'Transferred ' . count($transferredItems) . \"items\\n\";\n      })\n   ->build();\n$this->log('Batch Add: Adding a new command');\n   $batch->add($s3->getCommand('UploadPart', array(\n      'Bucket'      => $awsConfig['bucket_name'],\n      'Key'         => $Item->getFilename(),\n      'Body'        => $body,\n      'ContentMD5'  => hash_file('md5', $fullPath),\n   )));\n   $this->log('Batch Add: Finished');\n   $this->log('Batch Flush: Start');\n   $batch->flush();\n   $this->log('Batch Flush: Finished');\n   exit;\n```\n. Yeah. Although I'm still having the issue with the signature error; it appears the double quotes does not effect finding it in the path and I'm able to get it on other returns.\n. @mtdowling yes, it appears the key name has a space in it. The key information is coming straight out of the ListObject iterator, so I would think that should be handled.\nHowever, if I go in the S3 control panel the filename appears like the following\n\nAbcdefg_Abc_Ab01AbcAbcdefghijk_v2.9+MASTER (1).xls\n\nThe link to it is\n\nhttps://s3.amazonaws.com/cdn.sourcedomain.com/site-beta/ftp_uploads/99-user/Abcdefg_Abc_Ab01AbcAbcdefghijk_v2.9%2BMASTER+(1).xls\n. I ended up using the AWS CLI library instead. If I have any other issues I'll try this.\n. \n",
    "jsor": "Thanks, will give it a try.\n. Thanks @jeremeamia, that was exactly what i was looking for. Will close this issue for now, thanks.\n. ",
    "peacemoon": "it is solved using master branch. Thanks\n. ",
    "xmarcos": "Any chance this could be related to this issue? https://forums.aws.amazon.com/thread.jspa?threadID=134543. \n. @jeremeamia thanks, i didn't want to open an issue because it seems to be a server side problem. \nFYI: there is a consistent 5 minute and 20 second difference between the time on the server and the expected one on the Amazon endpoint.\n. ",
    "romainneutron": "You're pretty right :)\nHere's the end of the log : \n```\nRequest:\nPOST /my-object-key-name?uploadId=UgwSKmLtZljNyu5Rcq9xTqccTQnJR7kOdHZEClqSRCcqlgMEdMtX.rcuo6gfrrZGEJTatI1KA5D6N5UkczkaYQ-- HTTP/1.1\nHost: sweet-kittens.s3-eu-west-1.amazonaws.com\nContent-Length: 192\nContent-Type: application/xml\nUser-Agent: aws-sdk-php2/2.0.1 Guzzle/3.0.5 curl/7.28.0 PHP/5.4.8 MUP\nDate: Fri, 07 Dec 2012 01:36:58 +0000\nAuthorization: AWS J2WL4M4DLWWA:8rgETAGPW9+A9w8kUA78W6ESmdE=\nxml version=\"1.0\"?\n1\"3f5cdde45f6fd80eecdf8a0f59e6d8f7\"\nResponse:\nHTTP/1.1 400 Bad Request\nx-amz-request-id: 0EC7216139D51EA3\nx-amz-id-2: 3S4dwp5i7TQq+noQNIeTFyW02Hkbz9DPMKhw/eCeE2fNhTqdV30KBn/d0h6Mc1/i\nContent-Type: application/xml\nTransfer-Encoding: chunked\nDate: Fri, 07 Dec 2012 01:36:57 GMT\nConnection: close\nServer: AmazonS3\nInvalidPartOne or more of the specified parts could not be found.  The part may not have been uploaded, or the specified entity tag may not match the part's entity tag.UgwSKmLtZljNyu5Rcq9xTqccTQnJR7kOdHZEClqSRCcqlgMEdMtX.rcuo6gfrrZGEJTatI1KA5D6N5UkczkaYQ--3f5cdde45f6fd80eecdf8a0f59e6d8f70EC7216139D51EA33S4dwp5i7TQq+noQNIeTFyW02Hkbz9DPMKhw/eCeE2fNhTqdV30KBn/d0h6Mc1/i1\nErrors: 0\nAn error was encountered while performing a multipart upload: One or more of the specified parts could not be found.  The part may not have been uploaded, or the specified entity tag may not match the part's entity tag.\nRequest:\nDELETE /my-object-key-name?uploadId=UgwSKmLtZljNyu5Rcq9xTqccTQnJR7kOdHZEClqSRCcqlgMEdMtX.rcuo6gfrrZGEJTatI1KA5D6N5UkczkaYQ-- HTTP/1.1\nHost: sweet-kittens.s3-eu-west-1.amazonaws.com\nUser-Agent: aws-sdk-php2/2.0.1 Guzzle/3.0.5 curl/7.28.0 PHP/5.4.8 MUP\nDate: Fri, 07 Dec 2012 01:36:59 +0000\nAuthorization: AWS J2WL4M4DLWWA:DhDBbobxSY7flUkE0BwUp+gi2Vs=\nResponse:\nHTTP/1.1 204 No Content\nx-amz-id-2: OsTiLfo8Lcy1cDYtAnEgNciIL1NjewNb1/jbybIrtdTgp6dbrdFv/+GtD1eLC0G1\nx-amz-request-id: CDF73C4E0D465BFF\nDate: Fri, 07 Dec 2012 01:36:59 GMT\nServer: AmazonS3\nErrors: 0\n```\n. I confirm that it solved this issue !\n. ",
    "doapp-ryanp": "Looks like a good start. I'm not familiar with RST files - idea here your gonna run them through a compiler of somesort and spit out HTML that will be hosted on github pages or something?\nWhatever tool you do use, I think it would be pretty valuable to choose one that is easy for people to contribute to - so people like myself who would be willing to contribute can easily send pull requests.\nAlso direct contextual (faq.blah#sometopic) linking from the api docs to these docs would be sweet.  That way you just PHPdoc the link when your writing the SDK - then you can link from something like http://docs.amazonwebservices.com/aws-sdk-php-2/latest/class-Aws.DynamoDb.DynamoDbClient.html#_scan to http://newurl.com/service-dynamo.html#scanExample\njust my 2c.\n. So I added debug of what is going over the wire and it looks like guzzle is messing up the format...\nphp\n...\n$sns->addSubscriber(LogPlugin::getDebugPlugin());\n...\nOutput:\n...\nAction=CreatePlatformApplication&Version=2010-03-31&Name=Tmp&Platform=GCM&Attributes.1.Name=PlatformPrincipal&Attributes.1.Value=NA&Attributes.2.Name=PlatformCredential&Attributes.2.Value=myValidApiKey\n...\nWhen it should look something like (note missing 'entry' and 'Name' instead of 'key' and 'Value' instead of 'value':\nAttributes.entry.2.key=PlatformPrincipal\n&SignatureMethod=HmacSHA256\n&Attributes.entry.1.value=AIzaSyClE2lcV2zEKTLYYo645zfk2jhQPFeyxDo\n&Attributes.entry.2.value=There+is+no+principal+for+GCM\n&AWSAccessKeyId=AKIAIOSFODNN7EXAMPLE\n&Signature=82sHzg1Wfbgisw3i%2BHA2OgBmRktsqUKFinknkq3u%2FQ4%3D\n&Timestamp=2013-07-01T15%3A49%3A50.354Z\n&Name=gcmpushapp\n&Attributes.entry.1.key=PlatformCredential\n&Action=CreatePlatformApplication\n&Version=2010-03-31\n&SignatureVersion=2\n&Platform=GCM\n. Thanks. I also tried setting the sentAs attr in the sns resource but that did not make a difference. My guzzle knowledge is not that deep - otherwise I'd spend more time trying to submit a fix.\nOnce you fix, can you please make sure to test all the new Push service related methods, as this is blocking us - would like to avoid this method getting fixed but running into issues in the others.\n. Sorry, I know this was just reported, but any idea a timeline for this fix? days/weeks?  Thinking that if its in guzzle land it make take a while.\nWe are trying to get this into our production apps, and trying to decide if we should go another direction (maybe write some java code using the aws sdk for java)\n. @jeremeamia thanks for the heads up. I wont be able to get to it for another day or so. My composer dependency chain makes it hard for me to test non tagged versions - so I'll probably wait til it gets into one.\n. Thanks!\n. So there is no way to just 'add' one metadata field? If I have 10 things (content type, encoding, expires, cache control, custom headers etc..) and I just want to add one how is this accomplished?\n. Re: the catch - thats what I was saying (sorry I probably didn't explain well).  To update an existing object and add just 1 metadata field, you need to read exising, merge new, then overwite.\nIMO this really isn't an 'update'.  Think of the case where another process is changing the Expires header after i've done the read and before i've done the merge and overwite.  It'll wipe out the new Expires.\nI get that s3 is not a DB, and now that I understand how it works, and that its not possible I can live with it.\nThanks for the quick response on this.\n. ",
    "tonydspaniard": "+1 for this. \nThe new SDK is awesome\n. ",
    "stmpy": "I was having this same issue occur on aws-sdk-php version 2.1.0, and found out what others may have been experiencing, also what appears to be occuring in the original bug. \nThe issue was that I was not url encoding the filename before requesting the preSignedUrl, once I added the url encoding everything worked peachy.\nIn the original description he is url encoding the filename for the $extra variable, but not the filename in the request.\nJust incase anyone else happens to run into this w/ the same issue, it isn't w/ aws.\n. ",
    "ralph-tice": "is there any reason not to use a github wiki?\n. Just read through it and looks like you covered it all from the code side, but I think you should include the php.ini changes also.  I don't think you should default to the php.ini gc probability if it's recommended to run it on a cron instead, and I think it's pretty important to run it on a cron instead of the default PHP way so you don't have throughput problems, but instead of changing functionality you could just say set this:\nsession.gc_probability = 0\nand:\nauto_prepend_file = /var/www/dynamodb_session_handler/register_sessionhandler.php\nseem warranted to go over, unless there's a reason not to use auto_prepend.\n. After reading your comments I think you're both right about auto_prepend_file being pretty environment specific (and not generally a great idea).\nNot sure I'm on the same page with the PHP directive -- does ini_set() address your particular concerns?  Default PHP session GC behavior is really bad for a production DynamoDB table if you have lots of sessions created, you'll GC during peak and generally overrun your max throughput or be forced to over-provision.\n. Yeah, that seems to cover all the bases.\n. Thanks for the quick response! On a Friday!\n[2] happened 4 times in 4 hours.\nMy sessions are averaging around 4-5kb in size.\nI have my session table provisioned at 80/80 read/write, and on the 5 minute graph I'm peaking at around 3k.  Nobody's been able to explain how the provisioned numbers relate to the 5 minute capacity graphs, though.\nI'm only using this deployment of the SDK for session handling, so I don't think there's any chance of it occurring from any other cause, if you mean am I using DynamoDB for anything besides session handling in this case, I am not.\nI have gc_probability set to zero, so it's not garbage collection related for sure, but I'm not sure if it was a read or a write.  I'm not sure how to trap errors further, should I register a shutdown handler to record a better stack trace than I'm currently getting?\n. `[client 10.122.59.14] PHP Fatal error:  Uncaught exception 'Guzzle\\Http\\Exception\\CurlException' with message '[curl] 65: necessary data rewind wasn't possible [url] https://dynamodb.us-east-1.amazonaws.com/ [info] array (\n  'url' => 'https://dynamodb.us-east-1.amazonaws.com/',\n  'content_type' => NULL,\n  'http_code' => 0,\n  'header_size' => 0,\n  'request_size' => 534,\n  'filetime' => -1,\n  'ssl_verify_result' => 0,\n  'redirect_count' => 0,\n  'total_time' => 0.002426,\n  'namelookup_time' => 4.5E-5,\n  'connect_time' => 4.9E-5,\n  'pretransfer_time' => 0.00111,\n  'size_upload' => 9620,\n  'size_download' => 0,\n  'speed_download' => 0,\n  'speed_upload' => 3965375,\n  'download_content_length' => -1,\n  'upload_content_length' => -1,\n  'starttransfer_time' => 0.001117,\n  'redirect_time' => 0,\n  'certinfo' =>   array (  ),\n  'redirect_url' => '',\n) [debug] ' in /var/www/dynamodb_session_handler/vendor/guzzle/guzzle/src/Guzzle/Http/Curl/CurlMulti.php:561\nStack trace:\n0 /var/www/dynamodb_session_handler/vendor/guzzle/guzzle/src/Guzzle/Http/Curl/CurlMulti.p in /var/www/dynamodb_session_handler/vendor/guzzle/guzzle/src/Guzzle/Http/Curl/CurlMulti.php on line 561`\nDidn't do anything different to get this output, so not sure if it's related or not but it looks to be a write to me...?\n. ",
    "amenadiel": "I've seen no mention for session autostart setting, but at least for me dynamodb sessions just doesn't work,\nI'm pretty happy with elasticache handling my sessions though, so I'm eager to see more sdk sintactyc sugar upcoming related to elasticache, wirh a good drop in replacememt for vanilla memcache drivers.\n. ",
    "claylo": "You're welcome! Yes, I have the CLA already from @mtdowling. Should I go ahead and send that in for this PR? I've got another one I'm working on for SimpleDb support, but I don't mind sending in two CLAs for them if you're ready to take this PR.\n. ",
    "breerly": "Adding an example call to the guide would be helpful.\nSee https://github.com/aws/aws-sdk-php/issues/63\n. Also an example of how to interact with the returned result.\n. Here's a working example of working with the data after it is returned.\nvar_dump($results->getPath('Responses/table_name_here/Items'));\n. ",
    "ngzhongcai": "Still wrong?\nhttp://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/DynamoDB.html#batchGetItem-property\n. ",
    "schmittjoh": "The size is about 11kb, and it's the same file for both systems.\nOn one system (PHP 5.4.9, Ubuntu 12.04), it consistently fails with the above error, on the other system (PHP 5.4.6, Ubuntu 12.10) it consistently works.\n. It still puzzles me a bit that the uploader works when constructing an in memory source explicitly, do you have an idea why that is?\nAnother thought, couldn't the uploader just choose the mechanism that is suitable for the given source?\n. $url is a URL, i.e. (file://some-path), which can be used with\nfopen/file_get_contents.\nThe contents of this URL do not change anymore although they might have\nchanged before calling the upload builder.\nOn Fri, Feb 1, 2013 at 1:25 AM, Michael Dowling notifications@github.comwrote:\n\nWhat does the variable $url contain? A string that contains a URL (e.g. '\nhttp://www.example.com'), a resource returned by fopen('http://example.com',\n'r'), an EntityBody object, or the path to a file? If it's the path to a\nfile, is it possible that the file is frequently being changed and fstat\ncache http://php.net/manual/en/function.clearstatcache.php isn't\npicking up size changes in the file, thus reporting an erroneous filesize()?\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/aws/aws-sdk-php/issues/29#issuecomment-12974403.\n. \n",
    "RamyTalal": "I'm having the same issue when uploading an image to S3. I'm using the putObject method. I tried 'Body => file_get_contents('thefile') and 'SourceFile' => '/path/to/the/file.ext'. \nThe issue occurs randomly. \n(Running on PHP 5.4.10 and Centos 6)\n. @mtdowling \n```\n2.6.18-308.8.2.el5.028stab101.1 #1 SMP Sun Jun 24 20:25:35 MSD 2012 x86_64 x86_64 x86_64\nPHP 5.4.10\ncurl 7.19.7 (x86_64-redhat-linux-gnu) libcurl/7.19.7 NSS/3.13.1.0 zlib/1.2.3 libidn/1.18 libssh2/1.2.2\nProtocols: tftp ftp telnet dict ldap ldaps http file https ftps scp sftp \nFeatures: GSS-Negotiate IDN IPv6 Largefile NTLM SSL libz\n```\n. ",
    "gwilym": "I've been noticing this a lot when using concurrency > 1, on several systems. Most of the time an uploaded folder will begin to get 400's due to timeouts after the first 10 or so requests.\nI did some superficial investigation into this (PHP userland layers at the lowest) and near as I can tell this may be a cURL-related bug. I certainly can't see the Guzzle library doing anything wrong.\nDespite giving cURL the correct content lengths, occasionally the request body reading function given to cURL by Guzzle will get called a second time after correctly signalling EOF with a blank string response.\nThis is always followed by a 20 second stop and eventually a timeout.\ne.g. during PUTs in Guzzle RequestMediator ::readRequestBody (remember that curl multi is being used here so the jumbled output is just the \"parallel\" curl handle execution)\n```\n ~ \nhttps://.s3.amazonaws.com//overview_hero.jpg ~ 16384\nhttps://.s3.amazonaws.com//overview_interior.jpg ~ 16384\nhttps://.s3.amazonaws.com//p-s-win-portmans095.jpg ~ 16384\nhttps://.s3.amazonaws.com//overview_hero.jpg ~ 16384\nhttps://.s3.amazonaws.com//overview_interior.jpg ~ 1304\nhttps://.s3.amazonaws.com//p-s-win-portmans095.jpg ~ 16384\nhttps://.s3.amazonaws.com//overview_hero.jpg ~ 16384\nhttps://.s3.amazonaws.com//overview_interior.jpg ~ 0\nhttps://.s3.amazonaws.com//p-s-win-portmans095.jpg ~ 16384\nhttps://.s3.amazonaws.com//overview_hero.jpg ~ 16384\nhttps://.s3.amazonaws.com//p-s-win-portmans095.jpg ~ 16384\nhttps://.s3.amazonaws.com//overview_hero.jpg ~ 16384\nhttps://.s3.amazonaws.com//overview_hero.jpg ~ 3348\nhttps://.s3.amazonaws.com//overview_hero.jpg ~ 0\nhttps://.s3.amazonaws.com//overview_interior.jpg ~ 0\n(delay begins)\n```\nNote that last line has been hit twice despite the first one returning a 0-length string. My gut feeling is cURL is getting into a state where it's expecting more data and so it never concludes the request, though I can't say for sure if libcurl itself is at the root of this.\nPHP 5.3.26-1~dotdeb.0 with Suhosin-Patch (cli) (built: Jun  9 2013 03:35:34)\ncURL Information => 7.21.0\nI'm not proposing a re-open of this because I can't be 100% sure, just leaving my findings behind. Someone may find my info handy if they try to pick it up later. Personally I don't know enough C to go digging around in the PHP or libcurl source.\n. As stated, I can't figure out how to fix it. Some advice on the mocking set up there would be useful.\n. Understandable and thank you :+1:\n. Thanks. I was under the impression it was much simpler than this, judging by the HTTP requests that the S3 console generates. Such as:\n```\n-> /DeliverHttp\n{\n\"method\":\"PUT\",\n\"url\":\"https://s3-ap-southeast-2.amazonaws.com/bucket/bar/baz/\",\n\"headers\":{\n    \"x-amz-s3-console-metadata-version\":\"2010-03-09\",\n    \"x-amz-s3-console-folder\":\"true\",\n    \"User-Agent\":\"S3Console/0.4\",\n    \"Date\":\"Thu, 15 Aug 2013 22:21:33 GMT\",\n    \"Authorization\":\"...\",\n    \"x-amz-security-token\":\"...\"\n},\n\"entitySpec\":null\n}\n```\nPerhaps the console is doing some magic in relation to the x-amz-s3-console-folder header, but the result is ideal because it is simply queried afterwards with a...\nhttps://s3-ap-southeast-2.amazonaws.com/bucket?prefix=bar/&max-keys=100&marker=bar/&delimiter=/\n. ",
    "konradkiss": "I did test it. Every attribute had [\"SS\" => value], which I found weird. I traced it to the Attribute::factory call before going with a new Item(...), so I don't have more info on that.\nI'll be happy to dive in deeper tomorrow. It was a simple data array that works great with PutItem otherwise. Regardless, I should get back to you in a few hours about what else I can find.\n. Jeremy, it looks like I'm not able to reproduce the problem I was having. I'm terribly sorry to have wasted your time. Everything works as designed and Attribute::factory assigns types as expected.\n. ",
    "taylorotwell": "Cool, thanks. :)\nMainly wondering because I want to include a driver into Laravel 4's unified queue API.\nOn Jan 15, 2013, at 8:03 PM, Jeremy Lindblom notifications@github.com wrote:\n\nIt sure is, Dr. Laravel. \nUnfortunately we cannot share any specific roadmaps, but since service coverage is pretty much the top priority for us right now, it hopefully won't be too long.\n(Sorry about the \"Dr. Laravel\"-thing, but it does have a ring to it.)\n\u2014\nReply to this email directly or view it on GitHub.\n. Thanks!\n\nOn Jan 15, 2013, at 8:20 PM, Jeremy Lindblom notifications@github.com wrote:\n\nAwesome! I'll make sure to ping you when it's available.\n\u2014\nReply to this email directly or view it on GitHub.\n. Great! Thanks so much!\n. Got it, thanks!. OK Thanks!. \n",
    "CHH": "Thanks for the reply. I'm really looking forward to migrate away from Tarzan Core to the AWS SDK v2!\n. Awesome!\n. ",
    "sadarby": "Thank you so much! My signed url's are working 100% of the time. Your effort is greatly appreciated.\n. ",
    "davidtiede": "I'm having the same issue, and just downloaded the latest aws.phar file from http://aws.amazon.com/sdkforphp/. The SDK shows version 2.1.0 but the getSignedUrl() doesn't have this commit. Any ETA on when the aws.phar file will be updated?\n. Okay, thanks. Guess this is a good time to learn about Composer then.\n. ",
    "theRealDrumBum": "Good catch! It changed the results, but it wasn't why I expected. I changed it to handle a very simple case first:\n'Filters' => array(\n                array(\n                    'Name'   => 'instance-state-name',\n                    'Value'  => 'running'\n                )\nAnd didn't get any results..., where without the Filter I get 197 results (mixed with running and stopped states).\n. Awesome! Thanks for pointing that out.\n. ",
    "e-oz": "Thank you.\n. It's frustrating this library only works with Composer. Composer is too awful to use.\n. @jeremeamia thanks, Phar fixed all issues! Sorry for taking your time and thanks for help.\n. ",
    "maknz": "Hi,\nI'm having this exact problem when doing file uploads to Amazon S3 using the SDK. \nAws \\ Common \\ Exception \\ TransferException\n[curl] 65: necessary data rewind wasn't possible [url] https://bucketr.s3.amazonaws.com/file.png\nIs this related? I've only started seeing these errors today, and they are intermittent.\n. Hi,\nI haven't been able to reproduce this recently; I think it's very likely to be my connection. The problem doesn't seem to occur at all when running on EC2 in the same AZ as S3.\nShould it arise again / become prevalent, I'll post back.\n. Hi,\nI get this error Use of undefined constant CURLOPT_DEBUG - assumed 'CURLOPT_DEBUG' when trying to output debug information.\nI am using this code:\ncode>\n$s3 = AWS::get('s3');\n\n$s3->getConfig()->set('curl.options', array('body_as_string', true));\n$s3->getConfig()->set('curl.options', array(CURLOPT_DEBUG, true));\n\nThe error is still occuring with one particular image when sending from an instance in Singapore to an S3 bucket in Sydney.\n. Phew, thought I screwed up a copy/paste there.\nAdding body_as_string with the right associative syntax worked, that problematic image which would always produce the rewind error is no longer.\nThanks for your help!\n. There are no logs to suggest that -- do I need to do something special to make cURL/Guzzle report that?\n. Sorry, how do I set that option? I've tried $s3->getConfig()->set('client.backoff.logger', true); and $s3->getConfig()->set('client.backoff', array('logger' => true);.\nI'm using the Laravel SDK, and I've done a composer update, which didn't help.\n. To clarify, which of those above is correct for setting that option?\n. Hi,\nThanks for that. Unfortunately I'm not seeing any warnings raised in my logs by adding that option.\nThe code for uploading the original (large) image:\n```\n$s3 = Aws\\S3\\S3Client::factory([\n   'key' => Config::get('aws.key'),\n   'secret' => Config::get('aws.secret'),\n   'region' => Config::get('aws.region'),\n   'client.backoff.logger' => 'debug'\n]);\n$s3->getConfig()->set('curl.options', array('body_as_string' => true));\n $s3->upload(Config::get('aws.bucket'), $imageModel->filename, fopen($path, 'r'), 'public-read');\n```\nWith output\n2014-02-17 11:54:18] log.ERROR: Aws\\S3\\Exception\\SignatureDoesNotMatchException: AWS Error Code: SignatureDoesNotMatch, Status Code: 403, AWS Request ID: xxxxxxx, AWS Error Type: client, AWS Error Message: The request signature we calculated does not match the signature you provided. Check your key and signing method., User-Agent: aws-sdk-php2/2.5.2 Guzzle/3.8.1 curl/7.28.1 PHP/5.5.3 MUP\n. I've changed all instances of upload() to putObject(), and it's working just fine. So it must have been a problem with the multipart uploads. Happy to keep debugging if you're interested in solving it.\n. body_as_string was used to resolve #140. Removing it now hasn't helped, still getting the signature error.\nHere are the logs from the cURL:\n```\n Connection #0 to host bucket.s3-ap-southeast-2.amazonaws.com left intact\n Connection #0 seems to be dead!\n Closing connection #0\n About to connect() to bucket.s3-ap-southeast-2.amazonaws.com port 443 (#0)\n   Trying 54.240.195.5...\n Connected to bucket.s3-ap-southeast-2.amazonaws.com (54.240.195.5) port 443 (#0)\n Connected to bucket.s3-ap-southeast-2.amazonaws.com (54.240.195.5) port 443 (#0)\n successfully set certificate verify locations:\n   CAfile: /path/to/cacert.pem\n  CApath: none\n SSL connection using AES256-SHA\n Server certificate:\n    subject: C=US; ST=Washington; L=Seattle; O=Amazon.com Inc.; CN=.s3-ap-southeast-2.amazonaws.com\n    start date: 2013-11-07 00:00:00 GMT\n    expire date: 2014-11-02 23:59:59 GMT\n    subjectAltName: bucket.s3-ap-southeast-2.amazonaws.com matched\n    issuer: C=US; O=VeriSign, Inc.; OU=VeriSign Trust Network; OU=Terms of use at https://www.verisign.com/rpa (c)10; CN=VeriSign Class 3 Secure Server CA - G3\n    SSL certificate verify ok.\n\nPUT /one-of-the-resized-images.JPG HTTP/1.1\nHost: bucket.s3-ap-southeast-2.amazonaws.com\nUser-Agent: aws-sdk-php2/2.5.2 Guzzle/3.8.1 curl/7.28.1 PHP/5.5.3\nx-amz-acl: public-read\nContent-Type: image/jpeg\nContent-MD5: [redacted]\nDate: Tue, 18 Feb 2014 21:06:12 +0000\nAuthorization: AWS [redacted]\nContent-Length: 888778\n\n\nWe are completely uploaded and fine\n< HTTP/1.1 200 OK\n< x-amz-id-2: [redacted]\n< x-amz-request-id: [redacted]\n< Date: Tue, 18 Feb 2014 21:06:06 GMT\n< ETag: \"[redacted]\"\n< Content-Length: 0\n< Server: AmazonS3\n< \nConnection #0 to host bucket.s3-ap-southeast-2.amazonaws.com left intact\nRe-using existing connection! (#0) with host bucket.s3-ap-southeast-2.amazonaws.com\nConnected to bucket.s3-ap-southeast-2.amazonaws.com (54.240.195.5) port 443 (#0)\nPOST /the-original-file-which-breaks.JPG?uploads HTTP/1.1\nHost: bucket.s3-ap-southeast-2.amazonaws.com\nx-amz-acl: public-read\nContent-Type: image/jpeg\nUser-Agent: aws-sdk-php2/2.5.2 Guzzle/3.8.1 curl/7.28.1 PHP/5.5.3 MUP\nDate: Tue, 18 Feb 2014 21:06:13 +0000\nAuthorization: AWS [redacted]\nContent-Length: 0\n\n\n\n< HTTP/1.1 403 Forbidden\n< x-amz-request-id: [redacted]\n< x-amz-id-2: [redacted]\n< Content-Type: application/xml\n< Transfer-Encoding: chunked\n< Date: Tue, 18 Feb 2014 21:06:06 GMT\n< Server: AmazonS3\n< \n* Connection #0 to host bucket.s3-ap-southeast-2.amazonaws.com left intact\n```\n(Note: I only changed the offending file back to using upload(), the others are still using putObject(), but from what I can tell due to their file size, they'd be handed off to putObject() anyway)\n. Also fixed here, thanks!\n. :+1:, kill it.\n. I had this problem in #140 some time ago, setting the body_as_string option fixed it:\nphp\n$client->getConfig()->set('curl.options', array('body_as_string' => true));\nAlthough, I'm sure @mtdowling et al would like to figure out the actual cause, but that could be worth a shot to see if the same fix works for you?\n. @pulkit-clowdy you're using the wrong syntax. Use array('body_as_string' => true) instead of array('body_as_string', true).\n. :+1: I'm all for being progressive. Plenty of projects are dropping 5.3. Some might get caught out if they're coupled to Guzzle 3.x, but given this would be a major release, BC breaks should be expected.\n. ",
    "poisa": "Michael, thanks for your help. I don't mind mocking that method (or using\nanother branch) but I don't see how this will help me. Mind you that my\nunderstanding of the sdk is quite basic and that my first encounter with\nGuzzle was when I popped the hood on the sdk!\nOK, so lets say I mock getInstanceProfileCredentials(), how do I get from\nthere to the user-data? Will this solution work offline? (just curious\nabout the offline thing, not that I need it, but I DO need to work with the\nuser-data!)\nMuch appreciated!\n. > You can add an abstraction layer over the client and mock your abstraction layer\nI got so obsessed with making the mock work that I completely missed the easiest solution. I will still check the mock plugin out especially since I see myself doing this over pretty soon, but given the deadlines this is by far the fastest solution (and a pretty decent one too). \nMichael, thank you so much for your help!\n. @jeremeamia thanks for your prompt answer. Let me try again because I think I explained myself poorly (which happens quite often).\nWhat I mean is. On my endpoint I get something like this:\n\"Type\" : \"Notification\",\n  \"MessageId\" : \"69a0b5a1-9243-5bbd-b243-fc0764a702da\",\n  \"TopicArn\" : \"arn:aws:sns:eu-west-1:594219141893:MyTest\",\n  \"Subject\" : \"My subject\",\n  \"Message\" : \"My message\",\n  \"Timestamp\" : \"2013-02-26T17:45:08.636Z\",\n  \"SignatureVersion\" : \"1\",\n  \"Signature\" : \"<snip>\",\n  \"SigningCertURL\" : \"<snip>\",\n  \"UnsubscribeURL\" : \"<snip>\"\nIs there a way to plug this data into the SDK so that I know for certain that this message was posted by Amazon and not sent there by an attacker?\n. @jeremeamia I've looked at the pages you suggested and I've come up with a working solution. I'd like to take you up on you pull request offer though I'm not familiar with the current SDK so I might make something of a mess. I'll submit my PR when it's ready and I'll let you guide me through if that's ok with you!\nSo far I have a main class that does this:\n1) Checks that the domain certificate is Amazon's.\n2) Validates the signature depending on what strategy you inject it with. Currently I have two strategies: one for checking Notification messages and another for SubscriptionConfirmation messages. \n. Jeremy, I have no idea how to unit test something like this where a lot of \"confidential\" data is required. Of course I've manually tested the whole thing but I'd appreciate pointers on how to tackle this.\n. Sorry, completely slipped my mind. By the way, if this ever gets merged into the main repo I have no problem in creating some documentation.\nHere's a use case: https://gist.github.com/poisa/5053319\n. @BenMorel I'm not a Guzzle guru but I think that this can be achieved by adding a cache to the Guzzle http client. You pass this to the Aws\\Sns\\MessageValidator in its constructor. Here's a link that explains it: http://guzzlephp.org/guide/http/caching.html#caching\n. @BenMorel You are welcome. After reading your message I've looked at one of my app's logs (it receives a few thousand SNS messages per day depending on use) and can confirm that all my certificate urls are the same. As you said, there are cases where this might change (eg. Amazon's private key gets stolen/replaced, or they decide to store the certificates some place else) but for the last week this has not changed in my setup.\nBy the way @jeremeamia, I'll see you next week at webandphpcon2013 :)\n. Sounds good! Sent you an email.\n. Jeremy, I merged your pull request (thank you github for that lovely big-green-iPad-friendly button). My current project is taking a bit more than expected so I can't quite follow this through just yet but will definitely do so asap!\n. I've added some unit tests for the NotificationMessage class to get 100% code coverage. Let me know if I'm on the right track so that I can do the tests for the SubscriptionConfirmationMessage. \nBy the way, I don't know how to test the MessageValidator class itself so I'm gonna need some help with that.\n. Cool, I'll check it out. Thanks for the credit!\n. Jeremy, I've been banging my head with this because I'm using the exact same code in the docs and it's not working for me.\nThis does not work:\n``` php\nrequire '../vendor/autoload.php';\nuse Aws\\S3\\S3Client;\n$s3 = S3Client::factory();\n$credentials = $s3->getCredentials();\n$credentials->setAccessKeyId('key');\n$credentials->setSecretKey('secret');\n// var_dump($s3->getCredentials()); // I can see the proper credentials here\n$s3Data = array(\n    'Bucket' => 'bucket',\n    'Key'    => 'image.jpg',\n    'SaveAs' => 'image.jpg',\n);\n$s3->getObject($s3Data);\n```\nFatal error: Uncaught exception 'Guzzle\\Http\\Exception\\CurlException' with message '[curl] 28: Connection timed out after 1002 milliseconds [url] http://169.254.169.254/latest/meta-data/iam/security-credentials/' in [snip]/Aws/Common/InstanceMetadata/InstanceMetadataClient.php on line 85\nGuzzle\\Http\\Exception\\CurlException: [curl] 28: Connection timed out after 1002 milliseconds [url] http://169.254.169.254/latest/meta-data/iam/security-credentials/ in [snip]/guzzle/guzzle/src/Guzzle/Http/Curl/CurlMulti.php on line 578\nThis works fine:\n``` php\nrequire '../vendor/autoload.php';\nuse Aws\\Common\\Aws;\n$s3 = Aws::factory(array(\n    'key'    => 'key',\n    'secret' => 'secret',\n))->get('s3');\n$s3Data = array(\n    'Bucket' => 'bucket',\n    'Key'    => 'image.jpg',\n    'SaveAs' => 'image.jpg',\n);\n$s3->getObject($s3Data);\n``\n. Jeremy, setting the credentials' expiration to null worked like a charm. Thanks!\n. @cjyclaire I'm just interested in the plain text message. I originally tried every method in the suggested class since I saw that's where all the meat was but could only find the XML by itself, not the plain text.\n. @cjyclaire Yes, that will definitely work for my use case. Thanks!. @kstich that's a good idea! I wasn't aware that Guzzle contained all those JS goodies.. @howardlopez Thanks for the links. According to the ClientResolver class the retry middleware is added in thesign` step. I wasn't aware of how the default clients were created.\n(I'm still amazed at how well put together this SDK is!)\n. @diehlaws This works perfectly. Thanks for the in-depth example!. This works beautifully now. Thanks!. ",
    "eugenebond": "Ops, sorry for that. I've checked my files and I have a mix of old and new files for some reason and my version of  src/Aws/S3/S3Signature.php didn't yet have rawurldecode($request->getPath()) and thus me missing to change that. Anyway, hope you will be able to figure out some easy workaround for PECL case.\n. ",
    "tdondich": "OS: Ubuntu 12.04 LTS\nWeb Server: Apache 2.2.22\nPHP Version: 5.3.10-1ubuntu3.5 (Suhosin Patch 0.9.10)\nSDK Version: Latest Release, AWS PHP SDK 2\nYes, running Apache.  No, this is an initial install of AWS SDK.\nApache restarts to manually clear the cache.\nWhen using apc.stat=1 , everything works fine.  When using apc.stat=0, things fail horribly.\nI believe it has something to do with the autoloader.\n. I've determined that with phar: archives , realpath, which is used by recent versions of APC will not resolve the path. Therefore, any configuraitons with apc.stat=0 will fail and fail hard.\nI suggest providing an archive version of aws sdk php that is NOT bundled in phar with an OPTIONAL class-loader initialization (you could simply just have the file be the phar stub).  Putting all Vendor and Aws top-level directories also make it easier for people to write their own class loader.  This, for example, is my class loader:\nphp\nspl_autoload_register(function ($class) {\n    @include_once(dirname(__FILE__) . '/lib/' . str_replace('_', DIRECTORY_SEPARATOR, str_replace('\\\\', DIRECTORY_SEPARATOR, $class)) . '.php');\n});\nThis classloader avoids relative paths making apc.stat=0 safe and fast.\nBecause the new SDK uses a lot of different php files, the stat calls add up.  On high performant sites, we need to provide an alternative to make sure apc.stat=0 functions.\n. Actually, Phar's work fine with APC.  The only thing that does NOT, is stat=0.  By not calling this out there is a false sense that the phar can be used in full production environments (which many use stat=0).  It bit me in the buttocks, and I'm sure as more people attempt to adopt v2, it'll hit them as well.\nAnd I totally agree, the AWS directory follows PSR-0.  Everything else does as well as LONG as you extract EACH of the dependencies in the correct place (of which they are not in PSR-0 state inside the phar archive.  For example, vendor/symfony/event-dispatcher/Symfony/Component/EventDispatcher is not a valid PSR-0 file path for the \\Symfony\\Component\\EventDispatcher class.  So a little tweaking is required when dealing with the archive.\n. Sure, I could consider using Composer.  As it stands right now, this is a production application with dependencies already baked in. So migrating to Composer would be a process in itself. :) I'm giving a +1 to the adding more to the documentation/installation process to help point this out for those environments which don't have the resources/plan to use Composer and already have their own autoloaders and dependency layout.\n. ./Aws\n./Guzzle\n./Doctrine\n./Symfony\n./Monolog\n(The vendor src dirs have the PSR-0 ready directory there. You'd just raise them top-level).\nThis way you create a bundle that is not Composer dependent and doesn't rely on Phar (for those of us that use apc.stat=0).  It'd just be important to create a download that includes all dependencies (at the specifically supported versions).\n. ",
    "BenMorel": "Thanks for adding this feature!\nAs far as I understand it though, it has to download the SigningCertURL every time it needs to check the authenticity of a incoming notification. I'd personally check every single incoming notification to avoid any possible hijacking of the system, so that's quite a big overhead.\nIs there any alternative to this?\n. @poisa Thank you, great idea!\n@jeremeamia Yes it should be perfect, provided that the SigningCertURL is that same across requests, can you please confirm this?\n. I've run a few consecutive publish on an SNS topic, and the SigningCertURL stayed consistent. So also there's no guarantee it'll ever be the same, it looks definitely useful to use a query cache, so thanks again @poisa !\n. Thanks, TBH I can't see how this would break code for existing users of the SDK, as they are probably not aware of the Guzzle exceptions anyway, so are probably already catching Aws exceptions, expecting them to catch any problem while querying the API. This update would just make the SDK work as they already expect it to work.\nI could catch \\RuntimeException or even \\Exception, but that's an antipattern! I only want to catch this specific exception, not any other. What if somewhere is thrown a \\RuntimeException('cURL extension is not loaded')? I don't want to catch this one, it needs to bubble up and be reported!\n. @mtdowling Your approach makes a lot of sense, and I agree that ServiceResponseException should indeed represent a problem detected in a response received. Using this class above was just an (unfortunate) example.\nThrowing Aws\\Common\\Exception\\RuntimeException is a reasonable approach.\nAnother one would be to add another subclass for this, such as ServiceRequestException.\n. @mtdowling @jeremeamia Great, thanks!\n. @mtdowling @gwis @jeremeamia What do you think?\n. Indeed, this might not really be a problem as it is, but still, I guess this is a good practice :)\n. Ok alright then, thanks!\n. Not necessarily, I was just personally surprised that the behaviour of the API did not match the doc: at first I wrote my program assuming the Endpoint would be there, before finding out empirically that I needed to wait for the instance to be available, and then call DescribeDBInstances to get the Endpoint.\nIt's no big deal really, expect that it's kinda scary to have to rely on empirical programming rather than just deriving our work from a well-written AWS documentation!\n. ",
    "mkrinke": "@mtdowling that is strange, i can definitly reproduce it using version 2.1.2 (guzzle 3.2.0). However upgrading to 2.2.0 (guzzle 3.3.1) does indeed fix the problem, thank you!\n. ",
    "manojtyagi": "Is there a way to  make sure the S3 server is reachable/connected ?\nS3Client::factory(...,...)\nalways returns me the same object.   I am using a script to grab a file via cron job.\nI want to send an email to myself if the S3 server is not reachable at anytime. \nOne method i am using  is  doesBucketExist  To make sure that the bucket is reachable. \nNot sure if that is the best way to handle this exception. \n. There are  multiple levels :\n- The network Connectivity \n- The credentials are incorrect\n  -Bucket doesn't exist  \nI am looking the solution for the first two, WITHIN MY SCRIPT.\nI can check on the command prompt.  My script is working fine and  reading the data from the Bucket. I am willing to add this error handling, to be notified , just in case It fails to connect to the S3 server because of any of those 2 reasons.\n. doesBucketExist()    can be performed after the connection.\nIn those 2  situations,  The script will break before  connection even.  \nBtw, I have registered in the forum , but I didnt find any link/option to post there. I can search though. \n. Thanks  Michael.   This will help me.\n. ",
    "justinhasoffers": "@breerly in RequestItems, you need to have your table name be they key to the array of Keys... like in the example of #28\n. ",
    "gonzobrandon": "Looks like I had to base64_encode the raw email string\n. ",
    "danielrhodeswarp": "Can you please confirm if the Async plugin is supposed to work with the DynamoDB and SWF clients?\n. Very useful information and good to know. I did wonder! Thanks for that mtdowling.\n. Related and possibly of use as a solution!\nI've just done some quick messing around, and it does indeed look like printf() and sprintf() do not honour ini_get('precision').\nInstead they seem to print the full accuracy of the float in question (ie. 6 digits for microtime(true)).\nInteresting...\n. To clarify, sprintf() [and etc] seems to default to 6 decimal places but you can specify more - even more than ini_get('precision')\n. If I remember correctly, if you don't explicity give each file you upload a MIME type, it will receive the default MIME type of the bucket. Not sure what uploadDirectory() is doing though.\n. ",
    "bakura10": "Hi,\nI've tried the Async plugin for sending push notifications using SNS (because those are not really critical I don't care if an error occur for any reason).\nHowever none of the notifications are received. If I remove the async plugin, it works.\n. Well, I'm the co-author of two Zend Framework 2 modules that are higher level libraries over AWS services: SlmQueue for SQS and SlmMail for SES (and other providers).\nI think it's a nice way for higher-level interfaces. Often, those are tight to a given framework architecture (for instance SlmMail relies on Zend\\Mail), which make it a bit harder to provide a generic solution in the SDK. But you can still install them and use them even if you're not using ZF 2 so it's cool.\nHwoever there is a place where I'd love to have a higher level library in official SDK : Simple Workflow. Java has an official framework (Flow) for that. Using Doctrine annotation, I think something nice could be bundled with the official SDK. I had plan for trying it, but it takes too much time. But please @jeremeamia , consider it :D.\n. Seattle is a bit far from Paris unfortunately =)...\n. Oops... Can be closed then :D\nEnvoy\u00e9 de mon iPhone\nLe 2 juil. 2013 \u00e0 01:39, Jeremy Lindblom notifications@github.com a \u00e9crit :\n\nAre you just looking for this: e5e4384 ?\n\u2014\nReply to this email directly or view it on GitHub.\n. :+1: for this, I signed up to the preview, but not having the PHP integration is a bit annoying :(.\n. Ha sorry, didn't see this file! Awesome then.\n. I have also received a lot of requests from updating to Guzzle 4 for a lot of integrations I've made, so having everything unified into Guzzle 4 is something I'd like to have.\n\nMinimal version of 5.4 does not bother me! Big +1 from me\n. Hhmm not entirely, the problem is that I realized the mistake because one of my method had an \"array\" typehint. I was quite surprised, before I found that it was actually the SDK wrapping around a SetValue. So I fixed it by adding a \"->toArray()\" but as it comes from nowhere it's a bit confusing.\n. I've updated the PR (only with wrap_sets as I'm actually unsure about the binary).\n. I like this idea :). Should I do it as part of this PR or a new one?\n. Yeah, but this definitely adds some boilerplate and won't work recursively. I'm not sure to understand why the addition of a new \"unwrap\" would prevent the round-trip. This is just for the unmarshaling for allowing easier and more consistent consumption. Currently the presence of those SetValues just force us to add those toArray whenever we detect a SetValue.\n. Definitely, we have various layer of data anyway, so what we retrieve from DynamoDB is converted to entity most of the time, and then the entity is dehydrated before being sending back to DynamoDB. The wrapper just make our life a bit less complex, and also really less intuitive when we just need to retrieve the raw data just for consumption. Let me know if I cna update this PR in any way: )\n. I understand the rationale behind this and it was mostly from a usage point of view. In some of our micro-services, we just work with the array result, so we may have that kind of code (completely unrealted to any DynamoDB and know nothing about DynamoDB):\n``` php\npublic function myFunction(array $info)\n{\n    $this->otherFunction($info['my_list']);\n}\npublic function otherFunction(array $list)\n{\n}\n```\nFrom a user point of view, we just expect a simple array in myFunction (which is what we receive), and we were tricked multiple times when trying to access a property thinking it will just be a plain array, but actually receiving the SetValue.\nBut anyway, if you think this is out of the scope, we will just subclass the Marshaler :).\n. I've actually been able to solve this issue by using the \"before\" option of the WriteRequestBatch. The callable is executed and I can prefix my tables :).\nI'm therefore closing.\n. For reference if someone has the same issue, here is a prefixer that works for everything:\n``` php\n    public function __invoke(CommandInterface $command): CommandInterface\n    {\n        $commandName = $command->getName();\n    // For batch requests, we need to modify the name of all tables within the batch\n    if ($commandName === 'BatchWriteItem' || $commandName === 'BatchGetItem') {\n        $requestItems = [];\n\n        foreach ($command['RequestItems'] as $tableName => $requests) {\n            $newTableName                = $this->resolveTableName($tableName);\n            $requestItems[$newTableName] = $requests;\n        }\n\n        $command['RequestItems'] = $requestItems;\n    } else {\n        $command['TableName'] = $this->resolveTableName($command['TableName']);\n    }\n\n    return $command;\n}\n\nprivate function resolveTableName(string $tableName): string\n{\n    $parts = explode('.', $tableName);\n\n    // For tables that are already qualified, it returns the untouched name\n    if (count($parts) > 1) {\n        return $tableName;\n    }\n\n    return 'app.' . substr($this->appId, 0, 6) . '.' . $tableName;\n}\n\n```\n. Awesome, thanks a lot! :)\n. Hi !\nWhat you want here is likely restricting user upload so that a given user is only allowed to post files into a sub-folder in your S3 bucket (for instance if your user is \"foo\", you want him to upload only files under the \"foo/\" path).\nThis is what I do:\n```php\n$conditions = [\n      ['acl' => 'public-read'],\n      ['bucket' => $s3Bucket],\n      ['starts-with', '$key', $username . '/']\n];\n$postSigner = new PostObjectV4($s3Client, $s3Bucket, [], $conditions, '+7 days');\n```\nYou can see that the correct thing is \"starts-with\", not \"key\". Then, it's up to you, in the HTML where you upload the file (either through a hidden field, or with an Ajax call), to add the \"key\" attribute, that will need to start by the given key you've indicated in the starts-with condition.\nAs far as I know there is no way to specify a prefix when generating the signature, and then having the prefix automatically appended when you upload the file: you'll need to make sure to specify it in the HTML attribute.. That's strange... it works here (although I'm uploading the image through an Ajax call).\nI can see though that what is sent in Ajax call is indeed \"username/{$filename}\" as well. However, when I check the S3 bucket and the file, S3 autoamtically replaced it with the filename.\nIf you're uploading it using Ajax, did you make sure that you're properly sending this boundary in the payload ?\nContent-Disposition: form-data; name=\"file\"; filename=\"your_filename.png\". Everything seems correct to me :/. The only difference between your code and mine is that I pass an empty array as third parameters while you do pass form inputs (I generate them manually in the HTML). But I don't think this can have any impact at all...\nSorry :(. I'll let someone from AWS answering you!. Oh... i should have spot it :d. I know way too much that kind of \"issue\"... :D. Hey.\nI couldn't make it any clearer :D.. Should be $publicKey for PSR-1\n. Should be $privateKey for PSR-1\n. Should be $privateDetails\n. You guessed it... =)\n. ",
    "mathewvp": "OS - Ubuntu 11.10 running on a VM\nPhp - PHP Version 5.3.22-1~dotdeb.0\nRunning php5-fpm with nginx\n. Update: Its working when I disable apc. Btw, are there any sample codes with full implementation?\n. Php is run as FastCGI, not cli. I was looking for sample code implementation for sending emails using SES\n. Thanks a lot. Closing this issue as #47 is already open regarding apc and sdk\n. ",
    "simontaisne": ":+1: Yay good job !\n. ",
    "joehoyle": "Hi @mtdowling \nthanks for taking a look! I tried what you suggested (and also added ContentType incase that was needed, but I still can't get it to go through, I am still getting the same error with:\nPHP\n$s3->putObject(array(\n    'Bucket' => 'my-bucket',\n    'Key'    => 'my-object',\n    'ContentLength' => 1073741824,\n    'ContentType' => 'application/zip',\n    'Body'   => fopen( 'http://ipv4.download.thinkbroadband.com/1GB.zip', 'r' ),\n    'ACL'    => Aws\\S3\\Enum\\CannedAcl::PUBLIC_READ,\n));\n. Thanks, will take a look!\n. FWIW I also ran into this. This seems pretty easy to reproduce with a long file list (say 500k and lower memory limit). @cjyclaire I don't think there's much more description needed here, as I think it's pretty evident (from what I understand) that the $commands array is never reduced, therefore memory will only ever increase.\n. ",
    "tylermenezes": "This reproduces it for me:\n$aws = Aws::factory('aws-config.json');\n    $s3 = $aws->get('s3');\n    $s3->copyObject([\n                'Bucket' => 'videos.framebase.io',\n                'Key' => 'some/file.mp4',\n                'CopySource' => '/xyzzy.framebase.io/some/file.mp4'\n     ]);\nIf I change it to $s3->putObject(...), it works fine.\nI'm using the latest version from the official PEAR repo.\n. I tried the Phar file and the version from the PEAR repo, including the latest version of both. Loaders shouldn't have caused a problem when using the Phar file, since spl_autoload is only called when a class isn't loaded, and everything is loaded at once when using the phar. \nCommit 7f4fd58137 fixes the issue.\n. ",
    "dfedde": "wow I feel feel like a retard. thanks for asking if curl_version() worked I guess I forgot to install php5-curl.\nIts all working now.\nthank you. \n. ",
    "eliasdelatorre": "I was getting the same error after installing PHP 5.6 on Ubuntu 18.04 which, by default, comes with PHP 7.2 I forgot to install php5.6-curl when installing all the required libraries. I was using simple Drupal db_query function that stopped working after the PHP downgrade. Installing cURL worked.. ",
    "recipe": "Solution is to use POST requests instead of GET with the combination of header  'Content-Type: application/x-www-form-urlencoded; charset=UTF-8'\n. Thank you!\n. ",
    "JackMurton": "Thank you Michael, that was pretty fast;\n. ",
    "mattfiocca": "Thanks for the quick reply! \nThat's useful information and thank you. And, after digging through the v1 API some more, it appears they are doing kind of the same thing you mention about filtering through a list object.\nphp\n// https://github.com/amazonwebservices/aws-sdk-for-php/blob/master/services/s3.class.php\npublic function delete_all_objects($bucket, $pcre = self::PCRE_ALL)\n. I'm not sure if its appropriate to paste this here, but i have a followup question about this. I'm trying this:\n``` php\nrequire_once \"aws.phar\";\nuse Aws\\S3\\S3Client;\nuse Aws\\S3\\Model\\ClearBucket;\n$s3Client = S3Client::factory(array(\n    'key' => AWS_ACCESS_KEY,\n    'secret' => AWS_PRIVATE_KEY\n));\n$iterator = $s3Client->getIterator('ListObjects', array(\n    'Bucket' => AWS_FS_BUCKET,\n    'Prefix' => '/myprefix/' // top level pseudo \"folder\" in bucket\n));\nforeach($iterator as $i=>$val)\n{\n    print_r($val);\n}\n$clear = new ClearBucket($s3Client, AWS_FS_BUCKET);\n// Be sure to set the custom iterator to ensure that you only delete keys with the prefix\n$clear->setIterator($iterator);\n// Clear out the matching objects using batches in parallel\n$clear->clear();\n```\nThis returns no errors, prints nothing in the iterator loop, and doesn't remove my pseudo folder or any items with that prefix in my bucket. Thoughts?\n. ah-ha!\nphp\n$iterator = $s3Client->getIterator('ListObjects', array(\n    'Bucket' => AWS_FS_BUCKET,\n    'Prefix' => '/myprefix/' // top level pseudo \"folder\" in bucket\n));\nneeds to be\nphp\n$iterator = $s3Client->getIterator('ListObjects', array(\n    'Bucket' => AWS_FS_BUCKET,\n    'Prefix' => 'myprefix/' // top level pseudo \"folder\" in bucket\n));\nThe main issue was in the \"/\" as the first character in 'Prefix'. Removing that first slash prefix fixed the issue for me. It was returning 0 results with the slash.\n. spoke too soon. after running the ClearBucket, i'm now getting:\nphp\nNotice: Undefined index: VersionId in phar:///aws.phar/src/Aws/S3/Model/ClearBucket.php on line 167\n. quick response. nice. and works well. \nthank you.\n. ",
    "simkimsia": "Hi there, how can I contribute to the docs? I find it difficult to find this particular code in the http://docs.aws.amazon.com/aws-sdk-php-2/latest/class-Aws.S3.S3Client.html\n. Hi @mtdowling I want to contribute to the content. It is not about the particular layout per se. By this code, I meant using Iterator in the ClearBucket as you have shown.\n. this issue is solved for command object but not for custom request object.\nhttp://docs.aws.amazon.com/aws-sdk-php-2/latest/class-Aws.S3.S3Client.html#_createPresignedUrl\nI managed to get back the \n<bucket>.s3.amazonaws.com/<key>\nbut only when I use the command object.\nPlease fix this for the custom request object as well.\n. Thank you, @mtdowling I will be looking at this a little later. I am now trying to integrate the file upload functions of my app with the aws sdk. \nI noticed that the README of this repo calls for contributions to third party module integration.\nMy app uses CakePHP 2.4 and I am only integrating a few S3 functions in the SDK. Once I have a more stable module, how do I contribute to that?\n. I used the exact code sample from http://docs.aws.amazon.com/aws-sdk-php-2/latest/class-Aws.S3.S3Client.html#_createPresignedUrl\n``` php\n$key = 'data.txt';\n$url = \"{$bucket}/{$key}\";\n// get() returns a Guzzle\\Http\\Message\\Request object\n$request = $client->get($url);\n// Create a signed URL from a completely custom HTTP request that\n// will last for 10 minutes from the current time\n$signedUrl = $client->createPresignedUrl($request, '+10 minutes');\n```\nI got back the error message in xml about the public end point being wrong.\n. No, i did not see the documentation stating it this way. But i also did not see the documentation warning me that the custom request object returning a url that cannot work as it is.\nAs an un-initiated user, I expect both methods (command and custom request) to provide me with a presigned url that \"simply worked\".\nIs there a way to still use the custom request object method but still returns a presigned url that simply worked?\nif so, I don't mind updating the docs to highlight this. Provided the docs are available on github of course.\n. I see. \nMy googling always takes me the docs at http://docs.aws.amazon.com/aws-sdk-php-2/latest/class-Aws.S3.S3Client.html#_createPresignedUrl\nrather than http://docs.aws.amazon.com/aws-sdk-php-2/guide/latest/service-s3.html#creating-a-pre-signed-url\nHence I did not see that. Is this http://docs.aws.amazon.com/aws-sdk-php-2/latest/class-Aws.S3.S3Client.html#_createPresignedUrl available for edit?\n. ",
    "jamesmoey": "Source code reference https://github.com/aws/aws-sdk-php/blob/master/src/Aws/S3/Resources/s3-2006-03-01.php#L3070\n. ",
    "tomsmaddox": "As a workaround, I've just added the following to my function (and removed the expire & die calls)\n// s3->putObject annoyingly keeps file handles open.\n        // So this unset and garbage collection run is necessary to tidy them up.\n        unset($s3);\n        gc_enable();\n        $this->logger->post(\"GC\", gc_collect_cycles());\n        gc_disable();\n. Hi Jeremy,\nYes thanks, your comments were spot on.\nGot the tests working by configuring the services.json file and then tested\nthe waiters in my backup scripts and they're a-ok.\nThanks for the help, hope this helps others get more similar commits in.\nOn 13 May 2013 19:35, \"Jeremy Lindblom\" notifications@github.com wrote:\n\nDid you get everything working?\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/aws/aws-sdk-php/pull/84#issuecomment-17831348\n.\n. \n",
    "rickydunlop": "I'm using the latest version, with the following code\n``` php\n$bucket = 'my_bucket';\n$key = $_GET['name'];\n$request = $s3->put(\"{$bucket}/{$key}\", array(\n    'x-amz-acl' => 'public-read',\n    'Content-Type' => $_GET['type']\n));\n$preSignedUrl = $s3->createPresignedUrl($request, '+10 minutes');\n```\nI did a little more research and found that I can get it to work if I encode the file name\nphp\n$key = rawurlencode($_GET['name']);\nIs there any difference in using putObject() over put()?\n. ",
    "rquadling": "Could this be extending to work on DBSnapshots?\nWhen I copy a db snapshot (for example) being able to wait until it is available would be useful.\n. I'm happy to generate a PR, but I think my knowledge of the internals of AWS RDS services mean that I don't know all the available states or transitions to make me an ideal candidate for that. Though willing to learn.. Alternatively, is there a way to build a generic waiter?. Invalid parameters to the stream_context_create() function will return False (according to the source) : http://git.php.net/?p=php-src.git;a=blob;f=ext/standard/streamsfuncs.c;h=54aef6afa00e24ad6022fa2f51952bbb6dd3f4b0;hb=2b392c9c10045b5ba1388846643f9f7eec45d3c7#l1153\n. That's a great option. I've always found it annoying to get a set of data and have to iterate to find by a key.\n. So should these generated files be excluded from the repo? Maybe built and included in the versioned archive? If I want to work with the source, it would be my responsibility to build via make compile-json (and keep them uptodate if I'm playing around with the contents of data, but if I use composer install or composer update (and using dist rather than source), then I'd get them pre-built?. I'm not sure I explained things fully.\nIdeally locally compiling shouldn't result in a diff if they are only build artefacts.\nOnly the .json files should be in the repo. The .json.php should only be in the distributions. The .json.php should be .gitignored.\nAnyway, thanks for looking at this. . Seems 0.0 is written rather than 0 for some values.. Sure. I'll use if (empty()) though. :-D. Aha. Yep!. ",
    "kstich": "@rquadling Would you mind opening up a separate issue for this request?. Hi everyone! Please take a look at #1395 which provides an implementation for this request.. This was released in Version 3.38.0. @usamamashkoor If you are on a version between 3.31.0 and 3.31.2, #1326 is possibly causing this  and we'd recommend updating to 3.31.3. If you are on another version, please file a new issue with some sample code that can be used to reproduce the problem.. @oberman @ppaulis @senorbacon\nWe are currently unable to reproduce the issue and are opening new discussions to explore any possible leads. As part of trying to get to the bottom of this issue, we would like to reaffirm what your broken states are.\n\nAre you all still experiencing these cURL errors?\nAre the errors inconsistent with regards to time, capacity, and network traffic?\nAre the errors arbitrary in that they are different cURL messages, or are they the same?\n\nAdditionally, if you're still experiencing errors, can you update us with your current state on the following information:\n\nPHP Version\ncURL Version\nOpenSSL Version\nGuzzle Version\nWhat AWS service the issue is occurring with\nIf you have tried contacting the service/support team with regards to the issue\nWhat steps in this thread, via comment #, you have tried to resolve the issue\nException text\nIf possible, the outputs of curl_errno or (if Guzzle 6+) GuzzleHttp\\Exception\\RequestException::getHandlerContext for each of the arbitrary errors\n\nWe know this is a lot of information to ask for, but it's all in hopes of driving this to a resolution.\nThank you!. @oberman Thanks for all the updated information. cURL error 56 is retried, so you must be running over the specified number of retries.\nYou mentioned that there are fewer of these errors now than before. What, if anything, has changed in that time?\nIs there any correlation between this cURL error surfacing and your outbound traffic? It's possible there is an issue with how many TLS connections are running simultaneously from your application.. Re: cURL 56 - Apologies, I was looking at outdated information for that portion of the response. \nAre the cURL 56 errors you are receiving specific to an AWS service, or is it across multiple services? You've mentioned both SQS and DynamoDB in this thread, but with a decrease in DynamoDB issues as well.. @ppaulis @oberman Thank you both for the updates on your current state, we're continuing to try to diagnose the issue. Please keep us updated if you have any new or interesting information, we will do the same.. #### Root Cause\nGuzzle v5 and Guzzle v6 do not trigger automatic retry of CURLE_RECV_ERROR like was the case in Guzzle v3. The AWS SDK for PHP v2 uses Guzzle3, where AWS SDK for PHP v3 specifies Guzzle v5 or v6.\nBut why?\nAt this point in the request/response lifecycle, there is no guarantee that retrying on that error code would be free of side-effects. ~This same thought from Guzzle applies to the SDK itself, so the error code is not retried in an SDK specific manner.~\nNow what?\nIf you are receiving this error, and believe the operation is safe to retry in your situation (no destructive or unintended side-effects, etc.), you may wish to retry the error in your own code.\nMiddlewares Solution\nCreate an additional 'retry-curl56' middleware with a custom decider for just this case and add it to the client's handler list via appendSign. You can read more about working with the SDK's middlewares here.\n  . After some further discussion based around the operations being handled, it is believed we can retry this at the SDK level since it's not safe to do at the Guzzle level. Support is being added for automatic retry of CURLE_RECV_ERROR via #1463. \n  . This was included in the 3.52.0 release.. Hi @bakura10, please see #1284 for an implementation of this request. Thanks!. @bakura10 We're pleased to say that this feature request has been integrated in to the SDK, you can find the release here.. Given that a single pipe still works and the associated pain points mentioned by @mtdowling in #1205, I'd move to close this as won't fix but would like to hear other thoughts.\nThis could also be backlogged for some future release where the composer minimum version update is less of a sticking point.. This PR is being closed due to the discussion in #1205. It will be considered for updating in future SDK releases where requiring a newer minimum version of Composer is an easier ask for SDK users. Thank you for the contribution @Christoph-Harms!. Extracts from older versions of the docs show that, at least at some point, a single pipe was valid. It's likely supported in an undocumented fashion since it didn't seem to break when I ran a composer install earlier today.\nI got 6.2.3 for guzzle, which is valid under the ^6.2.1 portion of the flag.\n\n\nInstalling guzzlehttp/guzzle (6.2.3): Downloading (100%)    . Closing in sync with #1204. It will be considered for updating in future SDK releases where requiring a newer minimum version of Composer is an easier ask for SDK users. Thank you for the contribution @Christoph-Harms!. Hi @helhum, I'm having trouble reproducing the issue either through scandir or passing the path to a DirectoryIterator. Could you please provide a reproduction or additional details so we can attempt?. It has been 30 days without a response to the question of reproduction on the issue this PR attempts to fix; the PR is being closed. If this fix is still necessary, please file an issue or resubmit the PR with more details. We hope to hear from you soon!. @AntoineWattier Thanks for raising this issue with us. Please see #1313 for additional functionality for the ObjectUploader/MultipartCopy abstractions.. @AntoineWattier These options should now be able to be passed through via $options['params'] as #1313 was merged in to 3.31.0.  This also introduces a 'before_lookup' callback for selective changes on the HeadObject command. Thanks again!. Hi @s3bubble, it looks like this is being worked on in the \"Add method to get service error message if available (like v2)\" PR as well.. Closing this PR due to age since response. Additionally, an update to functionality for #1232 will be included with the updates for #1312.. @soukicz Due to the long standing nature of the non-key-retaining generator, would you mind changing this to an optional $config['preserve_iterator_keys'] that defaults to false (old behavior) and documenting the behavior to the CommandPool constructor?. Resolving in favor of #1376 . It looks like many of the tests in FunctionsTest.php don't properly map to coverage, as @imshashank found, even though they have passing tests written. We could re-introduce the functions file, update the tests with @covers tags, and follow-up by increasing coverage with more tests. Thoughts?. This was merged a short while ago. Thanks again @bradynpoulsen!. I'm going to close this PR and leave open the associated issue (#1239) for comment.. While making these updates results in buildable documentation, the iterable pseudo-type was added in PHP 7.1. Since the SDK is compatible with PHP >= 5.5, this could cause issues for users on versions < 7.1 who don't have iterable.. Hi @literat, I'm receiving an exception for the required-but-missing 'region' in my test configuration. I've not set it in the environment via AWS_REGION or in the arguments passed to the ClientResolver. Can you provide more details or a reproduction?\n\n\nI'm on PHP 7.0.17 with aws-sdk-php 3.25.6 installed as well.. Hi @mwoodring, the StorageClass property needs to be set on invocation of the CreateMultipartUpload call, which is the 'initiate' step of an upload. You can operate on this command by setting the before_initiate option for your MultipartUploader.\n$opt = [\n    ...\n    'before_initiate' => function(\\Aws\\CommandInterface $command) {\n        $command['StorageClass'] = 'STANDARD_IA';\n    }\n]. Hi @sleipi, I've confirmed that this is a reproducible bug and will be working to solve it soon. Thank you for the detailed report!. This fix has been merged in and will be built in to our next release, thanks for the report @sleipi!. @imshashank Completed that minor change.. If we're okay with the changes to the required minimum versions of PHPUnit, this looks good to me.. Hi @JasonNewton1,\nIt's possible your SDK files are failing to be included, see this page for more information. If you're still having issues after verifying you've done the above inclusions, can you please provide some more detail about your file and project? Thanks!. That's some progress! Now it looks like the path you're pointing to for the autoload.php is likely wrong. Try debugging with get_include_path to see where your script is trying to include from and then adjust the require's path to the proper location.. Hey @McQaws,\nPHP automatically performs a stream flush before a close. No information is passed to the stream_close portion of the stream wrapper. Instead, this message should be captured from the stream_flush related function: fflush.\nUpdating your sample to include this will produce the error and a FLUSH FAILED message:\n$ok = fflush($hndl);\nif ($ok) { echo \"FLUSH OK\"; }\nelse     { echo \"FLUSH FAILED\"; }. Sadly, it looks like that's going to run in to the same problem, since file_put contents likely also relies on the implicit fflush before fclose that doesn't pass along any error. From the file_put_contents() docs:\n\nThis function is identical to calling fopen(), fwrite() and fclose() successively to write data to a file.\n\nThis was reported behavior all the way back in PHP5.3.8 and seems to be unpatched: https://bugs.php.net/bug.php?id=60110. Given that it's an underlying PHP stream wrapper pattern, what would have been helpful for you in knowing this in advance? We can update the notes in our StreamWrapper documentation where the fwrite -> fclose pattern is shown, but I'm not sure if there are other references you used that we should also update. Thanks for your feedback!. This is a helpful set of notes; we'll work to make this section more clear since I could see it tripping people up in the future as well. Thanks again!. Hi @rollsappletree, I'm unable to reproduce this issue with an image of my own. Given the warning that you're receiving about corrupt JPEG data, I believe that your image is likely corrupt in some way. That is subsequently affecting the data passed to getimagesize, which probably reports differently based on how much of the file has been read in before it is invoked.. @sators Check out the Rds\\AuthTokenGenerator for generating your token.. Yes, that entire string should be used as the password.. Strings can be valid callables, we should see if we can perform a better check for this.. Hi @se-robapodaca,\nI believe this is already supported through the PutObjectTagging method of the S3Client. If you were referring to another action, can you please provide some extra details? Thanks!. I'm unsure as to why codecov didn't load those; that seems like an issue on its end as opposed to with the file changes here.\nThe coverage itself dropped because we re-introduced a file that was previously uncovered (functions.php) which has a slightly lower coverage than the overall repository currently. There are a few small tests that I've just added, but the majority of the missing coverage is due to a tracking issue with PHPUnit and the static manifest variable in \\Aws\\manifest().. Looks good to me.. Hi @tinonetic,\nThis question is a bit more appropriate for somewhere like StackOverflow than an issue with the SDK itself.. Hi @sm2017, could you please provide a little more information, or potentially a small reproduction, of your issue at hand? That will help us to investigate further what's causing your issue and either guide adjustments or work on updates. Thanks!. We're still investigating this issue and potential changes in relation to it. I'm attaching some PHP documentation links here in case they are of help.\nCollecting Cycles\nPerformance Considerations. While we continue to look in to if this is an issue specifically with the PHP SDK, we would like to encourage you to contact S3 support via one of the methods provided on the AWS Contact Us page. To speed up the process, you can include the x-amz-request-id and x-amz-id-2 headers that are visible when running curl for the presigned url with the verbose option set via-v|--verbose.. @4406arthur It looks like the 'CopySource' parameter needs to be prefixed with the source bucket: \n\nThe name of the source bucket and key name of the source object, separated by a slash (/).. @4406arthur Sorry, I must have missed that comment. We're working with the service team to get to a resolution.. As of Version 3.28.5, host-style URLs are the default for S3 and will be returned as such for host-style valid names. The use_path_style_endpoint S3Client option can be used for path-style URLs.. Where a region specific endpoint is available, it should reduce latency according to this documentation. Hi @skworden, could you please provide us with sample of how you're setting up your Cognito access through the SDK? This will help us in both investigating your use case and identifying any potential changes to make. Thanks!. @skworden Have you tried using the 'validate' option when creating your CognitoIdentityProviderClient?. @skworden The validate option can be passed to any of the clients through their config. The SDK doesn't pull your own requirements, it uses those supplied by the service itself. If you're performing your own validation, you may want to use the 'validate' option to disable the SDK supplied validations. This will send your request to the service without performing validation and potentially blocking on failure; what happens from there is dependent on the service.. @danielealbano Sorry for the confusion on this topic! I believe the CloudWatch limit to 20 MetricDatum/PutMetricData is detailed here under 'MetricDatum items' with the other API resource limits. Let us know if that clears things up.. Hi @felipefcm, I'm unable to recreate your issue; is there any other information you can supply to help reproduce the issue? Please note that the your $result is an instance of Aws\\Result, not stdClass. LogResult should be accessible via $result['LogResult'] or $result->get('LogResult').. Hi @felipefcm, I'm unable to reproduce this issue with both your lambda and a similar one in Python2.7. I also see the log entries for the lambda invocations in CloudWatch. Can you please provide your SDK version, Guzzle version, and PHP version along with any other information you might deem relevant?. @felipefcm, That's nearly the same environment I am reproducing against. At this point, I would recommend reaching out to the Lambda team and letting them know you are sometimes not receiving the LogResult field of your invoke calls and include the details we have gone over here.. Hi @paulvreyes, the third parameter to downloadBucket is a keyPrefix, which translates to a sub-folder when downloading files if it resolves to a path. To get your desired behavior, you would need to interact with the Transfer class and its promise directly.. @MarkVaughn Thank you for the extra information, we're working to resolve this as soon as possible.. @paulvreyes @MarkVaughn, a PR ( #1286 ) was just posted that we hope will resolve this issue. Thank you both for bringing this issue to our attention.. @paulvreyes @MarkVaughn We believe this issue is fixed in 3.27.3. Thank you both again for your assistance.. I think we probably need to move a few of the PHP extensions that are in 'require-dev' up in to 'require' to make this 100% consistent:\n\"ext-pcre\": \"*\",\n        \"ext-spl\": \"*\",\n        \"ext-json\": \"*\",\n        \"ext-simplexml\": \"*\",\n\nThat leaves ext-openssl (which has extension_loaded checks; it remains in 'suggest') and ext-dom (which is used only for generating documentation and guides) in 'require-dev'. Thoughts?. @forkeer It looks like something broke with this PR. We'll be happy to take a look if you have changes to submit, although it might be cleaner to create a new PR. . @forkeer We're closing this PR because there hasn't been a response to the concern about it being broken. If you do have changes to submit, please do so in a new PR.. Closing this PR because it breaks the contract that ObjectUploader makes in its constructor with regards to $this->options. A new PR will be filed to fix the issue reported in #1312.. @vanzay Glad you were able to solve the problem! Since this isn't an issue with the SDK itself, we're closing the issue.. This doesn't touch any of those files, I think this is an issue with Codecov instead.. This should throw a NotAuthorizedException if you execute an adminDeleteUser call without being authorized to do so.. @remicollet Thanks for submitting this PR, we'll be taking a look soon.. Thanks again for opening this PR @remicollet! We've got 7.2a2 working locally and it is getting through the unit and smoke test suites. There is a minor compatibility change in the integration tests for the count() changes akin to this commit that needs to occur in BatchingContext. Would you please update the PR with that change?. @siwinski The change should be local to the step. We can supply the fix and run the integration tests if you'd rather we handle it.. PHP 7.2 is forked and feature frozen, and building the SDK on beta1 with these combined fixes is successful.. @msvrtan Thank you for bringing up the concern with the version bump; we'll discuss how to handle this size of change in the future.\nWhat about the update broke your testing setup? Any details you have may help us to mitigate that style of issue in the future.. @msvrtan Appreciate the additional info, thank you! As I said above, we'll discuss how to handle this size of change in the future to hopefully alleviate this kind of issue from happening.. Thank you for requesting this enhancement! Like with the similar #1239, there is a decent amount of design around standards and practices, as well as interoperability with the other SDKs. We will keep this thread up to date on changes in this area of the SDK. We'd love to hear if there are more requests for this enhancement, so let us know here.. Thanks for the feedback @netroby. The use_path_style_endpoint option is for only the S3Client and is documented with the constructor. While the links you shared above are for all Aws\\Sdk based clients, we can likely improve the documentation around this new flag. What kind of additional information would you find helpful?. According to the Basic SDK Usage guide\n\nAll of the general client configuration options are described in detail in the configuration guide. The array of options provided to a client may vary based on which client you are creating. These custom client configuration options are described in the API documentation of each client.\n\nWe will consider adding a similar notation to the overall configuration guide, but it's not feasible to maintain all service-client specific configuration options in the main SDK configuration document. Thanks for your feedback.. I'm closing this issue since we've pulled in #1303, it will be updated to the documentation on the release. Thanks again for the feedback.. @ProdigyView Are you receiving any logging from a 'CompleteMultipartUpload' call in your usage of the MultipartUploader?. With the debug flag passed to the client itself, you should be seeing the full request building handler stack being run through which would print out messages to CompleteMultipartUpload. Can you update us on the current state of your debugging code? Given what you have in the first post, it should look similar to this to trigger the logs:\n$s3 = S3Client::factory(array(\n  'credentials' => array(\n    'secret' => 'xxx',\n    'key' => 'xxxxx'\n  ),\n  'region' => 'us-west-2',\n  'version' => '2006-03-01',\n  'debug' => true\n));. @ProdigyView Can you please search your logs for \"CompleteMultipartUpload\"? There should be ~3 occurrences per use of MultipartUploader->upload().. @ProdigyView That's just a logging safety convention. Given some of the previous logging, are you receiving (and possibly suppressing) any other exceptions?\nAdditionally, have you considered using ObjectUploader? It looks to handle a very similar use case to what you provided in the original post.. @ProdigyView That's the failed request that causes the file not to be finished on the S3 end. I am wondering if there are any other exceptions or failures in the logs that may have caused a state that breaks the CompleteMultipartUpload call, as it's not something I've been able to reproduce. Are you seeing this issue occur with every upload that you run with this code?. Hi @rampo83, Thank you for using the AWS SDK for PHP. Questions about using the AWS APIs are more suited for sites like StackOverflow than as issues with the PHP SDK itself. In case you haven't already, please reference these pieces of API documentation: CreateJob, ReadJob, Shape-JobInput, and Shape-JobOutput.\nIf you believe there is an issue with the SDK itself, please provide some further clarity on the issue and sample code which we can use to reproduce the issue. Thanks!. CreateJob does not specify a DetectedProperties field on response. You can pass the job ID in that response to GetJob to receive the DetectedProperties field.\nI see there is a discrepancy in how the SDK docs display this with the ElasticTranscoder documentation referenced herein and will work on updating it.. @tarunkumar2215 It looks like you have whitespace in your credentials array key: 'credentials  ' should be 'credentials', notably without the two spaces.. It looks like you went the route of just removing the specified credentials to solve the issue. For later reference, see the Credentials guide for the SDK here.\nClosing as reported fixed.. Hi @webexpert4rv, sorry you've encountered this issue. Can you please post a code sample so we can try to reproduce this issue? Additionally, can you post what versions of Guzzle, Curl, and OpenSSL you are using? This will hopefully help us narrow things down a bit.. Closing due to age, please feel free to reopen if you have any updates.. Closing in favor of #1308. Please refrain from filing duplicate issues, thank you!. @gabriel403 Thank you for posting this issue and giving some conditions relevant to #1291. Is this happening for any (16mb+) file with this method after updating from the PR? And confirming I'm reading your details correctly: after updating from the PR, the exception no longer occurs, but the file is not encrypted according to the passed parameters?. @gabriel403 We've confirmed this issue, thank you again for the details. We are currently working on a fix for the issue and expect it to be available soon. We will be closing #1291 because it breaks the contract that ObjectUploader makes in its constructor with regards to $this->options and filing a new PR.. @gabriel403 Please see #1313 for the fix for this bug and additional functionality for the ObjectUploader/MultipartUploader abstractions.. @gabriel403 These options should now be passing through as #1313 was merged in to 3.31.0. Thanks again!. @luismontreal Thanks for bringing this to our attention. There were no changes explicit to SQS in version 3.28.0. Version 3.28.1 brought verification of MessageAttributes but will throw an exception if not valid.\nCan you please provide the versions of PHP and the SDK that you are using, as well as some sample code and a sample response? I'd also recommend reaching out to the localstack team to see if something changed on their end.. This is correct, the field MD5OfMessageAttributes does not exist on a Message if no attributes were set. I've also posted to the issue you created on the localstack repo.\n. PHP 7.2 is forked and feature frozen, and building the SDK on beta1 with these combined fixes is successful.. Hi @TakesTheBiscuit, it looks like you're viewing the v2 guide. Here's the createPresignedUrl guide for v3.. Do you have a page that directed you to the v2 docs? We can look in to what's causing that and potentially improving the messaging around v2 -> v3 docs as well.. #1320 has been merged in.. Hi @gohanman, it looks like you're trying to use a SenderID for a country that might not support it. Please see this documentation for restrictions on SenderID.. #1325 has been merged in and will be bundled in to the next SDK release.. #1325 was bundled in to version 3.32.4. Thanks again for your work @daum!. Posted a fix that attempts to read the passed <Region> out of the body of the AuthorizationHeaderMalformed response, or proceeds to make another request to determine the correct region (emulating a 301 with x-amz-bucket-region.). The fix from #1330 was added to the release of 3.31.7. Thanks everyone for reporting this issue!. Thanks again for this work @daum! . Thanks everyone for bringing this to our attention. It looks like you end up with an internal MultipartUploader which is being passed a seekable stream, which is not setting the ContentLength on the part to the proper size. We will have a PR up shortly fixing this issue.. We believe this issue has been fixed as of version 3.31.3. Thank you again for the reports.. Closing due to lack of update for the requested changes.. @cwhite92 and @agursoy, I've not been able to reproduce this issue. I have successfully uploaded large files (over two minutes in upload time) using a bare bones file form with PostObjectV4 data applied to it. How are you submitting your request (basic form, ajax, etc.)? Do you receive any other information as to why the request fails? And are you or the browser modifying the request in any way? I've seen similar errors in the past when sending a Content-Length header that does not match the size of the object.\nI'd also recommend seeking assistance through the S3 Forum or AWS Customer Support.. Can you try after changing your Content-Lenght to Content-Length?\nA little additional detail: My previous upload that was over 2 minutes was a 777mb file. I uploaded another today that was roughly 1.5gb and took roughly 5 minutes and also completed successfully.. Thanks @cwhite92 and @agursoy! Here's the related issue on Dropzone for those that are interested.. @holtkamp That error message is being surfaced from SQS, not the SDK. I've reached out to the service team regarding the issue. You can also try to engage with them on the AWS forums. The service team has acknowledged this issue and is working on an update. Not sending the 'FifoQueue' attribute if it is false, like in your workaround, is the recommended course of action until an update is released.\nI am closing this issue as we have confirmed it is not related to the PHP SDK specifically.. Hi @usamamashkoor, it doesn't look like you're using the PHP SDK here. I'd recommend seeking assistance from the S3 Developer Forum.\nYou could also look in to using PostObjectV4 for generating your inputs.. Hi @Karthi-SRV, I am unable to reproduce your issue. The $presignedUrl I receive when I run through your code is valid for the specified '+5 minutes' and, afterwards, shows a similar AccessDenied error as you see. If you need the url to be available for a longer period of time, you'll need to change the $expires value you pass to createPresignedRequest().. The portion of the error you are highlighting is saying that the time of the server (<ServerTime>) you are trying to access is 10s after the expiration (<Expires>) of the $presignedUrl; the URL isn't being accessed before it expires. If you need a longer living URL, you'll have to update the expires time to be longer (as you did in moving it to '+10 minutes') or guarantee that you're accessing it before the expiration.\nAs this doesn't seem like an issue with the SDK, I'll be closing it.. @henry911 Is what you are looking in AuthTokenGenerator?. Hi @rpradhi, from the sound of it, you're looking for the AWS SDK for Ruby.\nClosing since this isn't related to the PHP SDK.. That is true for something like the ObjectUploader, since it's an abstraction over MultipartUploader and the direct S3Client commands. The S3Client follows the pattern you linked, the MultipartUpload -> MultipartUploadException handles exceptions in any number of its subcommands.\n\nWhen an error occurs during the multipart upload process, a MultipartUploadException is thrown.. An AwsException is a single instance of a failure. The MultipartUploadException handles potentially one or many AwsException instances, based on where in the abstraction it fails. The MultipartUploadException then allows you to do nice things like retrying specific parts of the upload or abort the upload on a true failure. There could technically be a layer between these and RuntimeException, but I'm not sure how much benefit it would provide as these as they're only surfaced when using an abstraction.\n\nPHP 7.1 did add multi-catch exception handling via a | if you have that available.. This list is compiled from the V3 documentation 'Exception Summary' section.\nDerivatives of \\RuntimeException:\n AwsException - Has many other extending exceptions for services.\n CouldNotCreateChecksumException\n CredentialsException\n MultipartUploadException - Has the discussed S3MultipartUploadException extend it.\n ParserException\n UnresolvedApiException\n UnresolvedEndpointException\n UnresolvedSignatureException\nDerivatives of \\Exception:\n* DeleteMultipleObjectsException. It's much more advisable to be targeting what exception you're catching and how you want to handle resolving it on an case by case basis. While some of these cases are similar, that could be handled (< 7.1) with separate catch blocks that send the exception out to a function or (>= 7.1) using the | operator mentioned above.\nSomething like:\nphp\ntry {\n    $result = $s3Client->upload(...);\n} catch (AwsException $e) {\n    markFailedUpload($e);\n} catch (MultipartUploadException $e) {\n    markFailedUpload($e);\n    $params = $e->getState()->getId();\n    $result = $s3Client->abortMultipartUpload($params);\n}. I'm able to make successful doesBucketExist and headBucket calls to existing and non-existing buckets using the same IAM policy as you are. Have you checked that the Bucket specific permissions are what you expect and accessible to the User?. Can you please post a small sample of how you're building the S3Client and making your API calls?. Is it possible that your $bucketName has a portion of a key in it? A 'test-bucket' call to doesBucketExist is working, but 'test-bucket/test' is returning false through a 403.. You can try adding the 'debug' flag when creating your client to view the full debug log of your request.. The difference for the V2 SDK is that, with the $accept403 option, it's returning true on a 403 even if the bucket might not exist. This was probably it just masking your underlying issue. If you've verified the policy and ownership structure, credentials to the client you're using, and the permissions on the bucket, I'm not sure where the potential issue is.\nI'd recommend seeking assistance through the S3 Forum or AWS Customer Support armed with the information you get when running with the 'debug' flag if it's asked for.. Hi @adam1010 , I'm not able to reproduce your issue while using the same SDK version as you. Are you possibly doing any other manipulation to the URL before using it for the cURL call? Also, please make sure that you're copying all of the characters of the URL when performing the cURL, leaving a single character off the signature parameter (usually the last in the string) will cause the error you're seeing.. Hi @thatryan, without a sample of your code or email, we can't help pinpoint any problem you're having. Here's the PHP API Docs for SES' SendRawEmail operation, the SES API Reference for SendRawEmail, and an SES provided Developer Guide in case they can offer some assistance.. See #1341 for tracking the SimpleXMLElement dependency update. Will continue to look in to your issue as well.. Try removing the base64_encode call around $header here - via #1295.\nAdditionally, your 'Source' and 'Destinations' parameters should not be inside 'RawMessage'.. @wjgilmore Can you tell us which version of the AWS SDK has been pulled in for your use here? I'm able to get past the 'Operation not found' check in AwsClient for the 'CreateDataSourceFromS3Async' operation.. \"~3.0\" means any 3.X version but not 4.X (see the Composer documentation on the ~).\nIf you open your local copy of src/data/machinelearning/2014-12-12/api-2.json, does the entry for the \"CreateDataSourceFromS3\" call exist?. @wjgilmore Sometimes it's the simple things, best of luck!. @thatryan Are you specifying the identity you created via the 'SourceArn' option to the sendRawEmail() parameters as documented in the API docs or specifying the 'X-SES-SOURCE-ARN' as stated in this guide? . The operations you're performing in the original post are adding information to the identity, the sendRawEmail needs to know which identity to grab its extra information from. That is the correct place to find the Identity ARN via the console. Has this solved your issue?. Is this inconsistent or the same file and part every time? Can you please also post how you're creating the client at $this->s3? Have you tried using the 'debug' option to see more detail about the specific failure?. Do you have an update on this issue @danielclariondoor ?. You'll need to pass a debug=\"true\" to wherever you define your aws service:\nxml\n<aws:S3 version=\"2006-03-01\" debug=\"true\"/>. Closing issue on lack of response, please reply with any update.. Those are use statements for namespaced classes, PHP's namespace guide can be found here. You can find a small sample of using SES with PHP here.\nYou can also ask a question on StackOverflow and tag it with aws-php-sdk for further usage assistance as the GitHub issues are used for bug fixes and feature requests.. As it was stated over on the Guzzle issue, this means you're waiting for a response from SQS - either due to not using the promise based *Async() operations or having no other work to do while waiting for the response to an *Async request.\nIf this happens consistently, we'd encourage you to post on the SQS forum or contact support via one of the methods provided on the AWS Contact Us page.. Thanks for the fix!. You can use the Access Control List (ACL) settings on PutObject by using the 'ACL' parameter on the S3Client->putObject() call. The 'Make Public' action is the equivalent of using the 'public-read' Canned ACL.. What version of the PHP SDK are you using? And what is the output of running php -m at the terminal?\nI am unable to reproduce this issue on v3.33.1 of the SDK, as both properly assert that error code.\n```php\n$excepted = false;\ntry {\n    $s3->getObject([\n        'Bucket' => $bucket,\n        'Key' => $missingFile,\n    ]);\n} catch (\\Aws\\S3\\Exception\\S3Exception $e) {\n    $excepted = true;\n    Assert::assertEquals('NoSuchKey', $e->getAwsErrorCode());\n} finally {\n    Assert::assertTrue($excepted);\n}\n$excepted = false;\ntry {\n    $s3->getObject([\n        'Bucket' => $bucket,\n        'Key' => $missingFile,\n        'SaveAs' => $sinkPath\n    ]);\n} catch (\\Aws\\S3\\Exception\\S3Exception $e) {\n    $excepted = true;\n    Assert::assertEquals('NoSuchKey', $e->getAwsErrorCode());\n} finally {\n    Assert::assertTrue($excepted);\n}\n``. I'm able to reproduce your issue when I move down toguzzlehttp/guzzle5.3.1. Looks like this is a bug in the way we're handlingSaveAs/sink/save_toin the Guzzle v5GuzzleHandler. We'll be posting a PR in the near future to resolve this issue.. @tomfotherby #1359 has a potential fix for this issue if you'd like to take a look.. Hey @jaredhabeck, looks like you're just slightly off in the argument formatting for the eventual call to [DescribeSpotInstanceRequests`](http://docs.aws.amazon.com/aws-sdk-php/v3/api/api-ec2-2016-11-15.html#describespotinstancerequests) - I believe you'll need: \nphp\n'SpotInstanceRequestIds' => [\n    $spotRequestId\n]. That call has the ability to take no parameter (in addition to 'Filters' and 'SpotInstanceRequestIds') and return a number of 'SpotInstanceRequests', meaning there's not a way to validate as it isn't required.\nClosing as the issue has been resolved.. We use the GitHub issues for tracking bugs and feature requests. This would be best suited to a question on StackOverflow tagged with aws-php-sdk.\nHere's a guide on using PHPMailer with SES and you can use $sesClient->generateSmtpPassword($credentials) to get SMTP information from a CredentialsInterface.. I'm unable to recreate this issue by passing in a stream opened in the same way as your code and how flysystem uploads the file. Can you retrieve the contents of $filesystem->getConfig() before your call to $filesystem->putStream($localFile, $remote); please?\nAdditionally, #1326 was a similar issue related to flysystem and the S3 MultipartUploader functionality - please make certain you're running version 3.31.3 or higher of the SDK.. Can you please supply the versions of the AWS SDK for PHP, Flysystem, and Flysystem Aws S3 V3 adaptor you're using? Can you also update with a code sample of how you're invoking the ->upload() function?. @fjmoralesp Your issue is likely then the same as #1326 - please update to 3.31.3 or higher.. Thanks for all of the info @afust-pica9. I'll be taking another pass at attempting to reproduce your issue based on it.\nHowever, it looks like there weren't any updates in 3.33.0 or 3.33.1 that affected S3 or the underlying layers. Do you happen to know what version of the SDK you were on before the issue arose?. I'm still unable to reproduce your issue @afust-pica9. Can you try enabling debug logging and supplying the contents for a failed upload?. Closing due to lack of reply. Please let us know here if there's any new information.. cURL error 52 in this context that no response was received by cURL after completely sending your request. It does not mean that there were no items in the queue. This is usually due to a network or service issue, I'd recommend that you open a support ticket with AWS Support.. The 'TargetPrefix' field needs to be inside the 'LoggingEnabled' array:\nphp\n$result = $s3_client->putBucketLogging([\n    'Bucket' => \"sourcebucket\", // REQUIRED\n    'BucketLoggingStatus' => [ // REQUIRED\n        'LoggingEnabled' => [\n            'TargetBucket' => 'logbucket',\n            'TargetGrants' => [\n                [\n                    'Grantee' => [\n                        'Type' => 'Group',\n                        'URI' => 'http://acs.amazonaws.com/groups/global/AllUsers',\n                    ],\n                    'Permission' => 'FULL_CONTROL',\n                ],\n            ],\n            'TargetPrefix' => 'test_prefix/',\n        ],\n    ],\n]);\nHere's the API documentation for putBucketLogging.. Generators were added in PHP 5.5, which is the minimum version for version 3 of AWS SDK for PHP.. Which of the operations is throwing this exception: putItem or getItem?. You've specified a 'name' key in your table creation but are not using it for the getItem call, meaning your keys are mismatched.. These occur due to differences in PHP versions for running compile-json in and would be rewritten by our process. Closing.. That's correct - if you open up the vendor folder when using composer, you'll only see the .json.php files. We include them here in case you wish to build from source.. S3 Transfer Acceleration is done through 'use_accelerate_endpoint' and 'use_dual_stack_endpoint' options on the S3Client that can also be configured at a per-operation level.. Will squash before merging - I was unable to recreate the issue in a local Travis instance so had to do a bunch of commit/push sets to trigger builds.\nNo changelog document as this is not applicable to consumers of the SDK.. Many of the EC2 api operations provide their own 'DryRun' parameter.. The SDKs don't provide a separate 'DryRun' parameter aside from what services may provide. The --dry-run option visible in the CLI documentation is an example to show the functionality that uses the EC2 operation describe-instances.. That documentation is for the CLI S3 custom commands (cp, mv, etc.) and it only prints the commands that it will attempt to run if you invoke it without --dry-run. It doesn't do any permissions checking to see if these operations could be performed with the current role.. These occur due to differences in PHP versions for running compile-json in and would be rewritten by our process. Thanks for pointing them out though!. You would replace the detected Guzzle http handler that the SDK uses by default by setting the 'handler' or 'http_handler' option with an HTTPlug handler implementation that satisfies the specific interface. This would set the SDK to use HTTPlug through your handler.\nAs @mtdowling said above, if a PSR is finalized for HTTP clients, we can look in to supporting them.. It looks like s3:// URIs are being sent to the custom endpoint parser, as they don't match the S3 pattern regex, where they fail to be parsed properly. I think we can add some support for this.. /cc @jeskew @cjyclaire . Can you please post how you're generating the values to pass to Flysystem? Most insterested around:\n\nSince the source is a \"php://temp\" stream.. Thanks for the added info. How is the $body value you pass to flysystem generated (file_get_contents(), fopen(), etc.)?. Thanks @tiran133. I've confirmed this issue and have started review on the PR that attempts to fix it.. This fix was released in version 3.36.28.. We are not currently writing updates for v2 of the AWS SDK for PHP. If you'd like to submit a fix, please create a pull request based on a branch of v2.8.\n\nAs a workaround, you can set the 'Credentials/Expiration' value on the $result before passing it to createCredentials:\nphp\n$result->setPath(\n    'Credentials/Expiration',\n    strtotime($result->getPath('Credentials/Expiration'))\n);. I'm not able to reproduce this issue in 5.6 (osx) or in 7.1 (ubuntu). I'm using the following script:\n```php\n// Snip as it's similar enough\n$outputFile = '\\tmp\\test.tmp';\n$realSize = $result['Body']->getSize();\n$inputStream = $result['Body']->detach();\n$outputStream = fopen($outputFile, 'w+b');\n$bufferSize = $argv[1];\n$countedSize = 0;\nwhile (!feof($inputStream)) {\n    $buffer = fread($inputStream, $bufferSize);\n    fwrite($outputStream, $buffer);\n    $countedSize += strlen($buffer);\n}\nfclose($inputStream);\nfclose($outputStream);\n$writtenSize = strlen(file_get_contents($outputFile));\necho 'Real Size: ' . $realSize . PHP_EOL;\necho $bufferSize . ' counted: ' . $countedSize . PHP_EOL;\necho $bufferSize . ' written: ' . $writtenSize . PHP_EOL;\nif ($realSize != $countedSize || $realSize != $writtenSize) {\n    echo \"SIZE MISMATCH\";\n}\n```\nThese are technically operations on the Psr7\\Stream, so I don't believe this is an issue specific to the SDK. Have you tried replacing the $result['Body'] with a local file wrapped in a Psr7\\stream_for() and continuing to ->detach(), etc. to see if the issue persists?\nAlso, can you please let us know which versions of Guzzle and Psr7 you are using?. Can you please capture the size of the stream before invoking ->detach();?\nphp\n$realSize = $result['Body']->getSize();\n$inputStream = $result['Body']->detach();\nAlso, have you tried replacing the $result['Body'] with a local file wrapped in a Psr7\\stream_for() and continuing to ->detach(), etc. to see if the issue persists?\nThis will help us narrow down if the issue is related to the SDK, the service, or an SDK dependency.. I also cannot reproduce this issue on Windows with PHP 7.1.2 and the bundled ZIP version 3.36.20 of the SDK.. Closing due to lack of reply. Please let us know here if there's any new information.. It looks like OpenSSL is having trouble verifying your certificates. Check out this thread and the specific error you receive \"error:1416F086:SSL routines:tls_process_server_certificate:certificate verify failed\" to work on solving it.\nClosing since this isn't an issue with the SDK.. Is it possible for you to use the 'ContentSHA256' option with a provided sha256 hash instead of an md5? This would automatically be picked up and signed by the ApplyChecksumMiddleware:\nphp\n$cmd = $s3Client->getCommand(\"PutObject\", array(\n            \"Bucket\" => $bucket[\"bucket\"],\n            \"Key\" => \"prueba\",\n            \"ContentSHA256\" => $objectSha256\n        ));\nOtherwise, you'll need to provide 'X-Amz-Content-Md5' as the header name instead of just 'Content-MD5' for ->withHeader in your first mapRequest middleware. If you create a custom Middleware instead of using mapRequest, you'll have access to both the $command and $request. . You will have to sign the checksum header that's being passed in some way. This can either be done as above (passing the checksum to the server to generate the URL for uploading) or by fully signing the URL on the app.. Closing due to lack of reply. Please let us know here if there's any new information.. Closing this as a duplicate of #808. Please chime in there with your use case(s).. There are no plans to drop 5.x support on the 3.x line of the AWS SDK for PHP.. Codecov report of decreased coverage is incorrect. Details show that 95.99% of the diff is covered which is higher than the current 91.90% coverage.. Can you please supply a small snippet of code that shows how you're setting up your pagination to run in to this issue?. Thanks for the extra information @Flythe, I've confirmed this issue.. Got a \ud83d\udc4d  via @xibz.. It looks like the directory that you're trying to save your file(s) to does not exist, as we pass this error from Guzzle. You can try using the Transfer manager (based on ListObjects) that will create directories for you.. It looks like PostObjectV4 is handling the uri generation itself. We should also look at other abstractions that take an S3Client object and update them to obey $s3->getConfig['use_path_style_endpoint']. Thanks for the report!. Those are pieces of the request object, not the response. After receiving the result from S3, you can find the URL at $result['@metadata']['effectiveUri'].. Can you provide a sample piece of code, including the 'Bucket' and 'Key' parameters, that's resulting in the GetObject request you have in the original image? I'm unable to reproduce your issue without duplicating the bucket name into the 'Key' parameter as well.. You should not be including the bucket name in the key; that is likely the cause of your issue here.\nThe SDK uses virtual host style URIs for S3 by default, whereas the console shows path style. You can read more on virtual host or path style URIs here. If you want the SDK to use path style URIs, try the 'use_path_style_endpoint' option when creating your S3Client.. That is the correctly rendered virtual host style URI. Getting explicitly the path style URI, matching what you see on the console, requires using the 'use_path_style_endpoint' mentioned above.\nThe following URIs resolve to the same file:\nS3 Console link (path style): \n://s3.amazonaws.com/pragmarx/backup/databases/most-recent/[something].backup.sql.gz\nSDK 'effectiveUri' (virtual host style): \n://pragmarx.s3.amazonaws.com/backup/databases/most-recent/[something].backup.sql.gz. These branches are for different major versions and will not be merged.. This issue comes from not having your credentials set up correctly. The MockHandler will return the supplied response(s), but the SDK still needs a valid credential set to construct the requests. Please see this guide for setting up your credentials.. Error 1 is related to your call to fopen on a URL, it failed to make a request to the endpoint specified.\nError 2 looks like OpenSSL is having trouble starting up in your call to fopen.\nCan you post a sample of your code where you're invoking the receiveMessage(), etc. calls?. It looks like these are both intermittent issues with spinning up either a cURL call or OpenSSL in a cURL call. I'd recommend starting with updating the cURL and OpenSSL installations to see if that helps resolve the issue.. Check out these guides on Commands and Promises.\nClosing as this isn't an issue with the SDK. Try asking a question on StackOverflow and tagging it with aws-php-sdk.. This Permanent Redirect (HTTP 301) can occur when accessing a bucket name that exists but you do not control. Please make sure that your 'Bucket' field in ->getObject is set to exactly the bucket name you created on S3.. What URL do you receive if you force path style endpoints?\n\nuse_path_style_endpoint: (bool) Set to true to send requests to an S3 path style endpoint by default. Can be enabled or disabled on individual operations by setting '@use_path_style_endpoint\\' to true or false. Note: you cannot use it together with an accelerate endpoint.\n\nThis is intended as an investigatory step, not as a full solution.. Closing due to lack of reply. Please let us know here if there's any new information.. In effect, 'subfolder' is just an extension of your key. You should be using the bucket name of 'bucket-name' and the key of 'subfolder/key'.. A call to define() cannot be undone and is, therefore, not safely testable due to side effects. That makes for 2 of the 3 lines affected; this is a hit we have to take on coverage.. Can you please post a code sample that is triggering this issue for you? Can you also please provide the version of the SDK you're using?. Thanks for the report @richqi, I've marked this as a bug and we'll work on a fix.. Can you please provide the versions of the SDK, cURL, OpenSSL, and PHP that you're using?. It looks like this is an issue with cURL specifically, not the SDK. I'd recommend updating cURL, verifying the permissions on your .pem file, and potentially reaching out on StackOverflow.\nClosing since this isn't SDK related.. Thanks for the catch! Doing some further looks beyond this one as well.. I'm not able to reproduce this issue on version 3.36.35. Do you know what specific version of 3.36 you received the error on? Does an entry for 's3' exist in your src/data/manifest.json[.php] file?. Let us know if you recover the version that wasn't working for you so we can investigate.. Can you please supply a few entries of what you receive from $data['Marker']? Also, what version of the SDK are you using?. That endpoint is exposed through the RDS CLI and not the AWS SDKs. Can you let us know where you were informed that 'This is a known bug with the AWS API' so we can follow up? Thanks!. Closing since this looks like an API issue with a couple posted workarounds. Thanks @jellis!. Thanks for doing these updates!\nCan you please point to the documentation that states that a version req of ^5.7 is required for the updates in this PR? Is there something I'm missing that wasn't covered in 5.4.3?. When updating a dependency, it's best to verify that the new minimum you take is exactly the lowest necessary for the updates that are being made. Please go ahead and take this down to ^4.8.35|^5.4.3.. Can you please also add a Changelog Document to this PR?. Thanks @carusogabriel!. Thanks for submitting this update.\nThere's an integration suite of tests available for the StreamWrapper that should be updated to verify this functionality alongside the old.. In your first sample, where does the $file variable come from?. Thanks for the extra info, just trying to get a complete picture of your situation to work on reproducing the issue.. It seems like the use of 'php://input' is the culprit here.\nstream_get_meta_data($fd) will report that the stream is seekable, and invoking fseek($fd, 0) on it will set the pointer to 0, but a subsequent read will return no contents. None of this triggers any complaints. It looks like a new handle is needed each time.\nSpecific to this issue: Because we read the contents of the stream to generate the SHA, we do the first read mentioned above, and subsequent reads for sending to S3 return no contents. Your EntityBody::factory call must have some sort of local caching involved so that the subsequent read succeeds. Guzzle also provides this functionality through the CachingStream, you'll need to wrap the handle in a stream first though.\n\n```php\n$fh = fopen('php://input', 'r+');\necho '0:' . json_encode(stream_get_meta_data($fh)) . '.' . PHP_EOL;\necho '1:' . fread($fh, 1000) . '.' . PHP_EOL;\necho '2:' . ftell($fh) . '.' . PHP_EOL;\nfseek($fh, 0);\necho '3:' . ftell($fh) . '.' . PHP_EOL;\necho '4:' . fread($fh, 1000) . '.' . PHP_EOL;\n$fh = fopen('php://input', 'r+');\n$fs = GuzzleHttp\\Psr7\\stream_for($fh);\n$cs = new \\GuzzleHttp\\Psr7\\CachingStream($fs);\necho '6:' . $cs . '.' . PHP_EOL;\n$cs->rewind();\necho '7:' . $cs . '.' . PHP_EOL;\n```\nbash\n$ curl -d \"this is a 24 char string\" -X POST {URL}\n0:{\"timed_out\":false,\"blocked\":true,\"eof\":false,\"wrapper_type\":\"PHP\",\"stream_type\":\"Input\",\"mode\":\"rb\",\"unread_bytes\":0,\"seekable\":true,\"uri\":\"php:\\/\\/input\"}.\n1:this is a 24 char string.\n2:24.\n3:0.\n4:.\n6:this is a 24 char string.\n7:this is a 24 char string.. Closing due to lack of reply. Please let us know here if there's any new information.. Thanks for the report, a fix is in progress.. 'NextContinuationToken' is for listObjectsV2, while 'NextMarker' is for listObjects. Is the 'NextMarker' in the $s3Result?. The AWS SDKs provide both versions of ListObjects under different operation names. It looks like, from the S3 documentation on V1 of ListObjects, that 'NextMarker' is only returned if you've specified a 'Delimiter'.\n\nThis element is returned only if you specify a delimiter request parameter. If the response does not include the NextMaker and it is truncated, you can use the value of the last Key in the response as the marker in the subsequent request to get the next set of object keys.\n\nThe AWS SDK for PHP also has paginators that can handle the pagination of the ListObjects[V2] operations for you.. You can use the ObjectUploader or $s3Client->upload($bucket, 'test/empty-file', $fh), which creates an ObjectUploader for you, to properly handle fluctuating file sizes like this.. There are no comparable tests for the Glacier\\MultipartUploader. Since the changes exercise the underlying AbstractUploadManager, the tests would not be validating any functionality that is different from the S3\\MultipartUploader.. Anything beyond our previous conversations, @jeskew?. So we can work on reproducing the issues to investigate, can you provide some samples as to your getObject and upload calls? Additionally, when you say \n\nto download an object that does not exist\n\ndo you mean the object does not exist locally or on S3?. Can you supply the versions of PHP, the AWS SDK for PHP, Guzzle, and Guzzle/Psr7 you're using? . I'm unable to reproduce either of these issues, I'd suspect there's something set in the NFS configurations that's causing this issue.\n\n'SaveAs' issue:\n```php\n$client = $sdk->createS3();\ntry {\n    $client->getObject([\n        'Bucket' => 'bucket',\n        'Key' => 'filethatdoesntexist.txt',\n        'SaveAs' => '/nfsmountdir/filethatdoesntexist.txt'\n    ]);\n} catch (Exception $e) {}\n$output = [];\nexec('ls /nfsmountdir', $output);\necho PHP_EOL . json_encode($output); // File exists in the directory\nunlink('/nfsmountdir/filethatdoesntexist.txt');\n$output = [];\nexec('ls /nfsmountdir', $output);\necho PHP_EOL . json_encode($output); // File no longer exists\n```\n\n->upload issue:\n```php\n$resource = fopen('/nfsmountdir/largefile.zip', 'rb');\nif ($resource === false) {\n    throw new \\Exception('Failed to open file');\n}\n$stream = \\GuzzleHttp\\Psr7\\stream_for($resource);\ntry {\n    $output = [];\n    exec('ls /nfsmountdir', $output);\n    echo PHP_EOL . json_encode($output); // File exists in the directory\n$client->upload('bucket', 'largefile.zip', $stream);\n\n} catch (Exception $e) {}\n$stream->close();\n$output = [];\nexec('ls /nfsmountdir', $output);\necho PHP_EOL . json_encode($output); // File exists in the directory\nunlink('/nfsmountdir/largefile.zip');\n$output = [];\nexec('ls /nfsmountdir', $output);\necho PHP_EOL . json_encode($output); // File no longer exists\n```. Still not able to reproduce, updated my sample to the following.\n\n'SaveAs' issue:\n```php\n$client = $sdk->createS3();\ntry {\n    $client->getObject([\n        'Bucket' => 'bucket',\n        'Key' => 'filethatdoesntexist.txt',\n        'SaveAs' => '/nfsmountdir/temp/filethatdoesntexist.txt'\n    ]);\n} catch (Exception $e) {}\n$output = [];\nexec('ls -a /nfsmountdir/temp', $output);\necho PHP_EOL . json_encode($output) . PHP_EOL; // [\".\",\"..\",\"filethatdoesntexist.txt\"]\nunlink('/nfsmountdir/temp/filethatdoesntexist.txt');\n$output = [];\nexec('ls -a /nfsmountdir/temp', $output);\necho PHP_EOL . json_encode($output) . PHP_EOL; // [\".\",\"..\"]\nrmdir('/nfsmountdir/temp'); // No error, directory removed\n```\n\n->upload issue:\n```php\n$resource = fopen('/nfsmountdir/temp/largefile.zip', 'rb');\nif ($resource === false) {\n    throw new \\Exception('Failed to open file');\n}\n$stream = \\GuzzleHttp\\Psr7\\stream_for($resource);\ntry {\n    $output = [];\n    exec('ls -a /nfsmountdir/temp', $output);\n    echo PHP_EOL . json_encode($output) . PHP_EOL; // [\".\",\"..\",\"largefile.zip\"]\n$client->upload('bucket', 'largefile.zip', $stream);\n\n} catch (Exception $e) {}\n$output = [];\nexec('ls -a /nfsmountdir/temp', $output);\necho PHP_EOL . json_encode($output) . PHP_EOL; // [\".\",\"..\",\"largefile.zip\"]\n$stream->close();\n$output = [];\nexec('ls -a /nfsmountdir/temp', $output);\necho PHP_EOL . json_encode($output) . PHP_EOL; // [\".\",\"..\",\"largefile.zip\"]\nunlink('/nfsmountdir/temp/largefile.zip');\n$output = [];\nexec('ls -a /nfsmountdir/temp', $output);\necho PHP_EOL . json_encode($output) . PHP_EOL; // [\".\",\"..\"]\nrmdir('/nfsmountdir/temp'); // No error, directory removed\n```\n\nThis still seems like an NFS configuration issue. Maybe try reaching out to EBS?. The tests I ran above were on Debian Jessie, so I don\u2019t expect much of a change but will give it a shot.. @mrmark Were you able to do any other investigating to resolve this issue? If not, I'll make another attempt soon on trying to reproduce the issue at hand.. It looks like it's trying to find the GuzzleV5 StreamDecoratorTrait based on 'GuzzleV5/PsrStream.php' in that message. Guzzle is a already listed as a dependency for the SDK.\n\nWhat version of Guzzle is listed in your composer.lock file?\nWhat does the stack trace look like when this error occurs?. For this, you should be using the Guzzle\\Psr7 namespaced classes, such as Stream that are a dependency of Guzzle v6, and not including the separate Guzzle\\Stream package.. Running version 2 and 3 of the AWS SDK for PHP together is not possible. You can follow the upgrading guide and use the compatibility test included in version 3 of the SDK to help update.. How did you install the AWS SDK for PHP? If you used Composer, you can manage your version of Guzzle through that.. No code changes, documentation only for #1440 . Amazon Transcribe is in preview, you can request an invitation to the preview program here.. Support for Amazon Transcribe was released in version 3.51.0.. At the time, the $command['Metadata'] may be empty. Try:\n\nphp\n$uploader = new MultipartUploader($this->s3Client, $source, [\n    'bucket' => AWS_S3_BUCKET_UPLOAD,\n    'key'    => $key,\n    'before_initiate' => function (\\Aws\\Command $command) use ($duration) {\n        if (empty($command['Metadata'])) {\n            $command['Metadata'] = [];\n        }\n        $command['Metadata']['duration_sec'] = $duration;\n    },\n]);. I'm unable to reproduce your issue without requiring an additional dependency outside of the AWS SDK for PHP.\nThe SDK dependency on sebastian/recursion-context comes via phpspec/prophecy via phpunit/phpunit. phpspec/prophecy requires \"sebastian/recursion-context\": \"^1.0|^2.0|^3.0\", where ^2.0 satisfies the requirement for \"php\": \">=5.3.3\".\nWhat packages in your composer.lock file require sebastian/recursion-context?. The Route53 ListResourceRecordSets operation itself limits this to 100 per response. 'MaxItems' is for setting a sub-100 item response cap.. Using 'StartRecordName' reads that the output will be in ASCII order starting from that name. Your filtering would need to be passed what will be the first sorted name for that 'HostedZoneId' for capturing.. Timeouts are not altered or supplied by the AWS SDK for PHP, just passed through to Guzzle. Guzzle applies the passed params in either Handler\\CurlFactory or Handler\\StreamHandler.\nThe SDK documentation you link to clearly states:\n\nUse 60 to wait indefinitely (the default behavior).\n\nAs this is configurable at runtime, I don't see a reason to change the default behavior here. A request is being made, the default expectation is that it should be completed - short-circuiting it should be the exceptional case. Do you have any specific reasoning for a change?. Thanks for your effort in helping keep code clean \ud83d\udcaa . Sending a bulk templated email should be done via '->sendBulkTemplatedEmail()'\nThere is no 'TemplateData' parameter for this operation. Also note that 'ReplacementTemplateData' should be next to, not inside, 'Destination'. 'DefaultTemplateData' is documented as:\n\nType: string\nA list of replacement values to apply to the template when replacement data is not specified in a Destination object. These values act as a default or fallback option when no other data is available.\nThe template data is a JSON object, typically consisting of key-value pairs in which the keys correspond to replacement tags in the email template.\n\nSimilar applies for 'ReplacementTemplateData':\n\nType: string\nA list of replacement values to apply to the template. This parameter is a JSON object, typically consisting of key-value pairs in which the keys correspond to replacement tags in the email template.. Please supply an updated version of the parameters you are passing to ->sendBulkTemplatedEmail().. From my previous reply:\n\n\nAlso, note that 'ReplacementTemplateData' should be next to, not inside, 'Destination'. \n'DefaultTemplateData' is documented as:\n\nType: string\nA list of replacement values to apply to the template when replacement data is not specified in a Destination object. These values act as a default or fallback option when no other data is available.\nThe template data is a JSON object, typically consisting of key-value pairs in which the keys correspond to replacement tags in the email template.\n\nSimilar applies for 'ReplacementTemplateData':\n\nType: string\nA list of replacement values to apply to the template. This parameter is a JSON object, typically consisting of key-value pairs in which the keys correspond to replacement tags in the email template.\n\n\nBoth of these values are to be strings representing JSON objects (set them to json_encode({CURRENT_VALUE})). 'DefaultTemplateData''s value of 'test' is not a JSON object, and will likely also lead to the exception you're receiving.. It looks like you're trying to pass two sets of 'ReplacementTemplateData' or 'DefaultTemplateData' at a time instead of one.\nphp\n...\n    'Destinations' => [\n        [\n            'Destination' => [\n                'ToAddresses' => ['my_email@gmail.com'],\n            ],\n            // The replacement data is 'key'=>'value' where 'key' matches the {{key}} in your template.\n            'ReplacementTemplateData'=> json_encode(['nome'=>'Thadeau', 'fulano'=>'A']), \n        ], // Another destination would go after this with different address(es) and data.\n    ],\n...\nThe same change should apply to 'DefaultTemplateData'.. Guzzle will attempt to keep the client connection alive if the same client is used for requests. This can be seen by using the debug flag and observing the cURL output.\n\n```php\necho $s3->getObject([\n    'Bucket' => $bucket,\n    'Key' => 'test1.png',\n]);\nsleep(5);\necho $s3->getObject([\n    'Bucket' => $bucket,\n    'Key' => 'test2.png',\n]);\n```\noutputs:\n...* Connected to {$bucket}.s3.us-west-2.amazonaws.com (52.218.160.40) port 443 (#0)...\n* Connection #0 to host {$bucket}.s3.us-west-2.amazonaws.com left intact...\n* Found bundle for host {$bucket}.s3.us-west-2.amazonaws.com: 0x7fb63265a010 [can pipeline]...\n* Re-using existing connection! (#0) with host {$bucket}.s3.us-west-2.amazonaws.com...\n* Connection #0 to host {$bucket}.s3.us-west-2.amazonaws.com left intact.... This will vary according to multiple factors, including your networking settings and the service's keep-alive settings. You can search for or ask for further information regarding S3's behavior on their forum.. Please see this guide for S3 Pre-Signed URLs. If you have further general usage questions, please ask on StackOverflow and tag it with aws-php-sdk.. This looks like it's an issue in the model we have for S3 using the \"Size\" shape which is marked as an integer. This means it would cap down to 32bit PHP_INT_MAX on 32bit systems - the number you're seeing. It looks like the service is returning the full size, it's just being cast to an int and capped. \nWe'll either have to fix this by converting the field to use the long type, causing potential breaks since it was expected behavior, or add a new field (\"SizeFull\" with the correct long type) to this operation and others that use the \"Size\" shape.\nCan you confirm that you're on a 32bit version of PHP please?. This fix was released in version 3.50.0.. I've been able to reproduce the issue and successfully use the CLI to replicate your success. It looks like the 'PreSignedUrl' doesn't contain the DestinationRegion parameter and I believe this is what's causing the failure.. This fix was released in version 3.50.0.. We currently do not support AWIS or Alexa Top Sites in the AWS SDK for PHP. Please voice your opinion via alexa.com/help or the AWIS and Alexa Top Sites forums.. Is what you are looking for something similar to the Amazon Cognito Identity SDK for JavaScript? You can also find documentation on Amazon Cognito User Pools here.. You can use the Cognito Identity Provider APIs to perform user authentication, signup, and more.. I am unable to reproduce your issue with the following code on version 3.50.0 of the AWS SDK for PHP:\n```php\n$cmd = $s3->getCommand('PutObject', [\n    'Bucket' => $bucket,\n    'Key' => $key,\n]);\n$uri = (string) $s3->createPresignedRequest($cmd, '+300 seconds')->getUri();\n$request = new \\GuzzleHttp\\Psr7\\Request(\n    'PUT',\n    $uri,\n    [],\n    'This is a test.'\n);\n$client = new \\GuzzleHttp\\Client([]);\n$client->send($request);\n```\nCan you please provide the patch version of 3.48 of the AWS SDK for PHP that you were using? Was the AWS SDK for PHP the only dependency that updated for you in that time? Is it possible there were credentials or permissions changes that have been made that could cause the signature mismatch?. Was the AWS SDK for PHP the only dependency that updated for you in that time or were there others as well? Can you provide how you're then consuming the pre-signed URL?\nPlease note that when creating pre-signed URLs, the headers that are present when a request is signed must also be present when using the pre-signed URL. This means that creating things like pre-signed PUT requests with a content-type require that the actual PUT you send use the same content-type.. >As of PHP 5.4, anonymous functions may be declared statically.\nThe AWS SDK for PHP requires PHP 5.5 or higher. Please verify you have a compatible version of PHP and a version of PhpStorm that can handle these features.. Since you have a version of PHP that supports this functionality, this is likely an issue with your IDE. I'd recommend contacting JetBrains support.. I'm not able to reproduce your issue. Is there anything consistent about the files or timing (same file(s) over time, same size files, etc.)? What happens if you try to re-upload one of the double size files? Is this only files uploaded with 'rb' or does it occur with files uploaded with 'r' as well?. I'm still not able to reproduce this issue. Can you check the 'Content-Length' sent via enabling the 'debug' flag?\nThe SDK passes the resource from your fopen call directly to Psr7\\stream_for() and in to the GuzzleHandler via a Psr7\\Request object. Can you please provide the versions of GuzzleHttp\\Guzzle and GuzzleHttp\\Psr7 you are using?. Closing due to lack of reply. Please let us know here if there's any new information and we can continue to discuss. If you do, can you please provide the versions of GuzzleHttp\\Guzzle and GuzzleHttp\\Psr7 you are using?. The AWS SDK for PHP released an update in version 3.52.0 that retries this specific CURLE code. The AWS SDK for PHP requires cURL and its PHP extension to run. This error is because you do not have the cURL PHP extension installed or configured properly.. Apologies, this is from the Getting Started portion of the README file.\n\nMinimum requirements \u2013 To run the SDK, your system will need to meet the minimum requirements, including having PHP >= 5.5 compiled with the cURL extension and cURL 7.16.2+ compiled with a TLS backend (e.g., NSS or OpenSSL).\n\nWe'll work on updating the other relevant portions of the documentation or providing a compatibility layer.. This fix was released in 3.52.1.. I am not able to reproduce your issue on v3.52.1. That error message reads like you're passing an array of ['api_provider' => (callable)] as the value for 'api_provider' instead of just the callable.\n\nWhat version of the AWS SDK for PHP are you using?\nWhat was your method for installing?\nHave you set any other configuration related to the SDK or the client?\nHave you made any changes to the S3Client file or others?. V2 of the AWS SDK for PHP does not use modeled exceptions like in the Java SDK, the file and class do not exist. A 'InvalidMessageContentsException' would be generated as a SqsException with ->getExceptionCode() returning 'InvalidMessageContentsException'.. It looks like Transcribe produces only a .json file output. You can make a feature request for .rst file output support on the Transcribe forums.. You can use the built in getPaginator functionality, specifying the 'GetUsage' API call and your parameters to handle pagination automatically. Many times, a paginated operation will not return the token (in this case 'position') if the response has not been paginated.. It looks like this is an issue with the API Gateway response on GetUsage, I've forwarded this along to the service team.. The service team has acknowledged this issue and is working on an update. I am closing this issue as we have confirmed it is not related to the PHP SDK specifically and will follow up when we have been informed of an update.. Based on this forum thread, the Elastic Transcoder FAQ, and no direct mention of support for this in the API documentation, I do no believe this is currently possible. You may wish to ask on their forums for further details.. Please make sure your account has access to write to the specified bucket. You can read more on S3 Resource Permissions here.. The HTTP request for the contents is completed before we generate a response. You can check the transfer stats by setting the 'stats' => true config setting.\n\nAdditionally, if your $start and $end are 0 and the file size, respectively, you will still be downloading the whole file.. The $result['Body'] for a getObject call is a Psr7\\Stream object. I'd recommend asking on that repository.. isConnectionError returns true for these cases if you use cURL and these cases if you use file streams. You can inspect the handler context via getHandlerContext on the underlying exception.. It looks like you've already submitted guzzle/guzzle#2012 to update their list of file stream based connection errors.\nClosing this issue as it doesn't seem to be an issue with the SDK itself.. Hi @eliysha, the S3\\StreamWrapper only supports the ://bucket/key format after the passed $protocol target (default 's3'). It relies on the passed $client for handling the region of the bucket. This URI would successfully parse out if the scheme was https instead. Are you able to successfully use the URI you're testing for in any case? I don't see where the SDK would provide this URI, how is it generated?\nAdding this parsing to the S3UriParser when it is not supported by the S3StreamWrapper would create a false expectation. I don't think adding the related capabilities to the S3StreamWrapper makes sense either, given the above information about $client.. It looks like this is in reference to #839. The AWS SDK for PHP does not support the same sync functionality as the CLI. I'm going to leave this issue up for now for comment.. Thanks for the report here.\nI think, since the $expires annotation of string|integer|null comes along with the comment of UTC Unix timestamp used when signing with a canned policy., that the better first pass here is to cast this to an int. This allows for no new exception being necessary while still fulfilling the expectations on both ends.\nAfter that, we can take a task for improving the interface to allow for passing a DateTime like SigV4's ->presign() functionality.. Thanks for bringing this to our attention. For note, the exponentialDelay function uses what we call \"Full Jitter\".\nI'll be taking a look at reproducing this and potentially making changes to the behavior based on some discussions, like we have for the DynamoDbClient.. @jeichorn Can you please provide which specific operations you're running in to issues with?. It looks like Route53 caps at 5 requests per second on their API before sending back a Throttling message that we retry. Can you please provide a sample of how you're invoking these API calls?. Can you please clarify what the 'NOT' is prefacing in this paragraph?\n\nBut the following , NOT , it just affect by default_socket_timeout and real timeout is between 4-12 seconds when I set default_socket_timeout less than 4 it is 4 and when I set default_socket_timeout more than 12 it is 12 , and for example when I set default_socket_timeout equal to 2 real timeout is 6 seconds\n\nAlso, can you please post the contents of $e->getMessage() from inside the catch block?. It looks like your additional time is being used by retries. When I set the 'retries' => 0 option, I receive the same roughly 1s timeout.\nIn your first case (credentials commented out), the process is failing to receive any credentials for the client and stopping there. In the second case (credentials uncommented), the process is attempting multiple getObject calls (1+3 retries by default) and failing each time before the final failure.. I've confirmed this issue. It looks like, when using virtual host style urls, the preceding slash is removed when signing. This would result in an invalid signature.. Can you please post the code you're using to generate the call to ->batchDetectSentiment()?. I was able to successfully call the ->batchDetectSentiment() operation as follows:\nphp\n$client->batchDetectSentiment([\n    'LanguageCode' => 'en',\n    'TextList' => [\"I really love that shirt\", \"Foo bar is a good company\"],\n]);\nPlease make sure the structure for your operation has the correct input data. You can also use the 'debug' flag to help in this process.. The use of array_values should be left up to the implementer in these cases. If we have this by default in the SDK, it means we'll be changing the structure of the data that you generated which may lead to other, more hidden, issues down the line.\nThe documentation also shows that this is an unindexed array.\n```php\n$result = $client->batchDetectSentiment([\n    // Key               Value\n    'LanguageCode' => '', // REQUIRED\n    'TextList' => ['', ...], // No keys\n]);\n// Shows sub-elements with indexes are represented differently.\n...\n[\n    // Key      Value\n    'Index' => ,\n    'Sentiment' => 'POSITIVE|NEGATIVE|NEUTRAL|MIXED',\n    'SentimentScore' => [\n        // Key     Value\n        'Mixed' => ,\n        'Negative' => ,\n        'Neutral' => ,\n        'Positive' => ,\n    ],\n],\n...\n``. V2 of the AWS SDK for PHP [uses virtual host style bucket addressing by default](https://github.com/aws/aws-sdk-php/blob/2.8/src/Aws/S3/BucketStyleListener.php#L39). If you wish to switch to path style, removing the'mybucket.'from before the endpoint you set, include the'PathStyle' => true,option in your client configuration or S3 methods.. Looks like #422 wasn't merged back in the day - which means this is really an enhancement request to widen the scope of SigV4's timestamp support to the classes that implement\\DateTimeIntreface.. Thanks for the enhancement request. In the mean time, the API versions are [listed under the Service APIs heading here](https://docs.aws.amazon.com/aws-sdk-php/v3/api/) or [insrc/data/manifest.json(.php)in the SDK](https://github.com/aws/aws-sdk-php/blob/master/src/data/manifest.json) and the region availability is listed [in the AWS docs](https://docs.aws.amazon.com/general/latest/gr/rande.html) and [insrc/data/endpoints.json(.php)` in the SDK](https://github.com/aws/aws-sdk-php/blob/master/src/data/endpoints.json).\nThe manifest information can be retrieved using the manifest($service) function.. Copy-over of comment from #1516:\n\nIt would be nice if there was a utility class that was able to do lookups for things like regions and other instance metadata without having to be pre-configured ahead of time. Whether these are static calls or just an instance that doesn't require the region and other required parameters.. Currently, that would be by reaching out to the Instance Identity Document. We provide something like this for credentials via the InstanceProfileProvider, but not for region at this time.. I'm going to close this as an enhancement related to #1515, updating the title of that and posting this comment over.. I think a discussion around the .changes folder could be warranted, given we currently exclude CHANGELOG.md. How about an issue (and potentially PR) about it so we can have some time for community comment?. Thanks for this enhancement request. There are some verifications we would need to make around interoperability with the other SDKs. We will keep this thread up to date on changes in this area of the SDK. We'd love to hear if there are more requests for this enhancement, so let us know here.. The AWS SDK for PHP sets up 3 retries by default. The Dynamo and S3 clients both have some customization around their retry strategy. You can read more about the 'retries' parameter here.\n\nRetries are handled by the RetryMiddleware. When a response is received, the middleware runs its $decider, usually the default unless configured otherwise, to determine if the request should be retried. Unless it's configured otherwise, the default backoff method uses a \"Full Jitter\" strategy.. The 'content-type' header is not signed because signing it would potentially cause a signature mismatch when sending a request through a proxy or if modified at the HTTP client level.. S3 does not restrict the ContentType for you. The 'content-type' header is used to describe the contents and is attached to the object as metadata. If you wish to restrict the content uploaded, you'd have to perform the restriction in your application.. You can find the AWS SDK for PHP API documentation for the Cognito Identity Provider service here and the service Developer Guide here. You should be able to find what you need from there or you can reach out on the Cognito forums here.\nIf you're looking for the AWS SDK for JS, here's the link for that repository.. I was able to test verify this like so:\n```php\n$command = $client->getCommand('PutObject', [\n    'Bucket' => 'mybucket',\n    'Key' => 'mykey',\n    'ACL' => 'public-read',\n    'Metadata' => [\n        'foo' => 'bar',\n    ],\n]);\n$command->getHandlerList()->appendBuild(\n    Middleware::mapRequest(function (RequestInterface $request) {\n        // Return a new request with the added header\n        return $request->withHeader('X-Foo-Baz', 'Bar');\n    }),\n    'add-header'\n);\necho (string) $client->createPresignedRequest($command, '+5 minutes')->getUri();\n```\nhttps://mybucket.s3.ap-southeast-2.amazonaws.com/mykey?x-amz-acl=public-read&x-amz-meta-foo=bar&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=EXAMPLE%2F20180503%2Fap-southeast-2%2Fs3%2Faws4_request&X-Amz-Date=20180503T225335Z&X-Amz-SignedHeaders=host%3Bx-amz-acl%3Bx-amz-meta-foo%3Bx-foo-baz&X-Amz-Expires=300&X-Amz-Signature=eb1ba0587f4b1357e7f6e81862755420578dc929650f522766a2ad14b0be7f83\nRunning a \\GuzzleHttp\\Client::put or direct curl call on this URL shows that you need to pass the header as well.\nGuzzle sample:\n```php\n$guzzle = new \\GuzzleHttp\\Client();\n$guzzle->put($url, [\n    'body' => 'This is a test.',\n]); // Fails on SignatureDoesNotMatch\n$guzzle = new \\GuzzleHttp\\Client();\n$guzzle->put($url, [\n    'body' => 'This is a test.',\n    'headers' => [\n        'X-Foo-Baz' => 'Bar',\n    ],\n]); // Succeeds\n```\nThe unit, smoke, and integration tests (when including the fix from #1543) all pass for the changes herein. That all said, there are a few changes required that I'll be noting in a review shortly. Thanks for your contribution!\nNote on S3\nAccording to this S3 documentation regarding query authentication:\n\nLists the headers that you used to calculate the signature. The following headers are required in the signature calculations:\n\nThe HTTP host header.\nAny x-amz-* headers that you plan to add to the request.\n\n\nS3, when calculating its signature to compare to the one in the request, automatically includes the headers for x-amz-* alongside the headers listed in the X-Amz-SignedHeaders query parameter (in this case, only host.) This includes the x-amz-meta-* headers from the 'Metadata' parameter on PutObject. They do not need to be listed out individually in the signed headers.. The configuration options are only accessible through the constructor. You can use an Aws\\Sdk object for shared configuration across clients, but cannot mutate client configuration.. Setting the parameter 'ACL' => 'public-read' in your ->putObject request is what causes this requirement. You can read more about S3's IAM Permissions here.. Closing this in favor of #1528. If that is in error, please update your issue with a sample of code that's causing your error response and reply here letting us know.. Basic health checks do not allow an 'AlarmIdentifier' to be set. Removing that key/value pair from the 'HealthCheckConfig' should result in a successful call.\nphp\n$response = $this->awshelper->performOperation(\n    'route53',\n    'NO',\n    'createHealthCheck', 'xx.log',\n    [\n        'CallerReference' => $callerReference, // REQUIRED\n        'HealthCheckConfig' => [ // REQUIRED\n            'FailureThreshold' => 1,\n            'IPAddress' => '<my IP>',\n            'Port' => 80,\n            'RequestInterval' => 30,\n            'Type' => 'TCP', // REQUIRED\n        ]\n    ]\n);\nThe following from the API documentation specifies that 'AlarmIdentifier' itself is not required but, if you do send it, that both 'Name' and 'Region' are required.\n'AlarmIdentifier' => [\n    'Name' => 'something', // REQUIRED\n    'Region' =>'us-east-2', // REQUIRED\n],. I am fully able to use the AWS Service Provider for Laravel 5 in a new Laravel 5.4 project. The Service Provider provides access to the AWS SDK for PHP (this repo), which contains the AWS related updates. If you are having trouble with the Service Provider, I recommend filing an issue there.\nRegarding the AWS Service Provider for Silex, this also provides access to the AWS SDK for PHP, which contains the AWS related updates. Additionally, \"Silex is in maintenance mode.\".\nThe Service Providers themselves are typically low on maintenance unless the framework makes changes. The separation of repositories is meant to have issues with the SDK itself be filed on this repository, and issues with the various Service Providers specifically (configuration, etc.) be filed on their specific repositories.. Can you please provide a code sample for how you're doing the putItem request via the Marshaler, as well as the specific error you're receiving?. I am unable to reproduce your issue. It looks like your connection to SES is being refused when cURL initially attempts to establish it. I found this similar thread on the Amazon SES forums. I'd suggest posting there or opening a ticket with AWS Support. You may wish to try updating OpenSSL as well, since this is an HTTPS connection.. Can you please post a code sample for what is generating the SendEmail issue you posed the stack trace for?. I am able to use the SnsClient. Please make sure you've followed the whole Getting Started guide for the AWS SDK for PHP. Feel free to comment and reopen if you are still running in to issues afterwards.. Thank you for posting this update, however, we're not accepting non-critical updates to v2 of the AWS SDK for PHP.. This is caused by a problem with the underlying I/O, not the SDK itself. Try using openssl_error_string or curl_strerror to find more information. You may also try updating your version of OpenSSL or cURL.. Closing as a duplicate of #1533 - See this comment about using a Paginator to get list all of your channels.. Can you please supply the code you're using to call getObjectUrl as well as for creating the S3Client that is used?\nAlso, what version of the SDK are you using?. It's likely that, at some point in the client configuration, your 'scheme' configuration is set to 'http', overriding the default 'https' value. Removing this, or setting it directly on the client (see below) should resolve your issue.\nphp\n$client = new S3Client([\n    'version' => '2006-03-01',\n    'region'  => 'us-west-2',\n    'scheme'  => 'https'\n]);. Can you please supply the code you're using to make the requests in question? . The docs state that 'This call accepts only one resource-identifying parameter.' and the OpsWorksException's message is 'Please provide one or more layer IDs or a stack ID' (emphasis added.)\nI am able to use both of the following API calls successfully:\n```php\n$client->describeLayers([\n    'StackId' => 'aaaaaaaa-bbbb-cccc-dddd-eeeeeeeeeeee',\n]); // Returns all Layers in the Stack\n$client->describeLayers([\n    'LayerIds' => [\n        'aaaaaaaa-bbbb-cccc-dddd-eeeeeeeeeeee'\n    ]\n]); // Returns the specific Layer, regardless of Stack\n```\nIf you still have issues after adopting the singular identifying parameter, please post the exact exception message you receive.. The documentation you linked states the following for the Record parameter:\n\nRecord\nType: Associative array of custom strings keys (VariableName) to strings\nA map of variable name-value pairs that represent an observation.\n\nMeaning something like this is required:\nphp\n$client->predict([\n    'MLModelId' => 'ml-XXXXX',\n    'PredictEndpoint' => 'https://XXXXXXXX',\n    'Record' => ['ExampleData' => 'ExampleValue',...],\n]). Have you tried just using a 'Name' => 'Value' pair, like so?\nphp\n$ml->predict([\n    'MLModelId' => 'ml-ooXXXXXX',\n    'PredictEndpoint' => 'https://XXXXXX',\n    'Record' => [\n        'Var2' => 'This is an apple.'\n    ]\n]);\nIf you have issues related to the responses of the service, I'd recommend reaching out on the Machine Learning forum or on StackOverflow with the proper tags.. If the file is working in other browsers, it doesn't seem like an issue with S3 or the SDK. I'd recommend asking for help on a site like StackOverflow.. The AWS ElasticSearch API (Docs here) does not provide this functionality. You can use this guide on signing an ElasticSearch Data Plane request to sign a DELETE request for your index.\nThe ElasticSearch Indexing User Guide may also be helpful.. Do you have any debug logging for the requests that are failing due to stale cached credentials? That will help significantly in tracking down the issue here.. Adding an exception here would be a behavior change and muddy the object being deleted. The object, having been previously deleted, does not exist in the current version of the bucket.\nWe could add a behavioral flag for $includeDeleteMarkers = false to doesObjectExist (and subsequently checkExistenceWithCommand) to return true if the object being checked is a delete marker. Would that fit what you're looking for?. Could you instead use the ['http']['proxy'] configuration option available on the SDK clients?. Thank you for your feature request. We regularly review suggestions like these to help prioritize what features we add to the SDK. But, currently, we do not plan to add direct support for a reverse proxy to the AWS SDK for PHP.. Thanks for submitting this update. We're looking in to other potential issues that could arise from using INI_SCANNER_RAW and will update here after that is complete.. It doesn't look like MediaTailor has an API to expose through the SDK, you can reach out on their forum to let them know you're hoping for it.. The StreamWrapper will perform a trigger_error call instead of throwing an exception. If you need to handle these cases, I'd recommend moving to using S3Client::getObject and using the SaveAs parameter, where you can wrap it in a try/catch block\nphp\ntry {\n    $client->getObject([\n        'Bucket' => $bucket,\n        'Key' => $data['key'],\n        'SaveAs' => $filename,\n    ]);\n} catch (S3Exception $e) {\n    ...\n}. When using the StreamWrapper, exceptions like this Connection timed out issue are turned in to warnings due to PHP limitations. The SDK attempts to retry connection errors by default, but you may have reached the retry attempt limit. If you wish to be able to catch these exceptions, you'll need to use the S3Client directly.. It looks like you have multiple installations of the AWS SDK for PHP enabled for your project. Please ensure you only have one installation of the SDK in your project.. Can you provide a code sample for what is generating this issue for you? Also, can you please point to what you're looking at regarding the .Net and Java SDKs?. It looks like someone else asked about this issue on the CloudFront forums, you should express your request for this feature in that thread.\n\n\"This is an expected behavior as CloudFront currently does not support KMS server-side encryption for S3. The reason this does not work is that viewer requests through CloudFront does not have access to the KMS credentials used to encrypt the s3 objects. We would recommend using signed URLs and Origin Access Identities without KMS. We are aware of this limitation and a feature request is submitted to the CloudFront team.\". Closing as duplicate of #1273. The Aws::createClient() method is not provided by any the AWS SDK for PHP, so I'd recommend investigating with where you're getting that from.\n\nThat said, it looks like your client needs a 'region' configuration provided to it.. It looks like there are issues with the way you're packaging your dependencies. The AWS SDK for PHP takes Guzzle as a dependency. It appears as if you have multiple copies of this dependency included. This is not an issue with the SDK.. I see the parameter being passed to the service with your same configuration. If you're not receiving an exception from this API call with more details, you should open a support ticket with AWS Support. You may need information that can be found by turning on the 'debug' setting.. The StreamWrapper removes the 'b' and 't' mode settings because the operation is binary safe. What it doesn't do is pull the compression level, causing a Mode not supported failure. In looking at this, I tried making that update only to hit a stream_cast issue to generate a File Descriptor, which we're not capable of generating from what I've found.\n\nThe default 'w' stream should be safe for gzipped content if it comes in gzipped. I'm able to write gz files using only a 'w' stream this way:\n```php\n$textContents = str_repeat('This is a test string' . PHP_EOL, 512);\n$handler = fopen($textBucketPath, 'w');\nfwrite($handler, gzencode($textContents, 9));\nfclose($handler);\necho 'Generated .gz file: ';\nvar_dump($textContents === gzdecode(file_get_contents($textBucketPath)));\n// Generated .gz file: bool(true)\n```\nThe final file is 87B and decompresses back to the original size.\nIf you're on PHP >= 7.0, you can use the deflate_init/deflate_add functions like so:\nphp\n$handler = fopen($textBucketPath, 'w');\n$deflateContext = deflate_init(ZLIB_ENCODING_GZIP, ['level' => 9]);\nfor ($i = 0; $i < 512; $i++) {\n    fwrite($handler, deflate_add($deflateContext, $textPortion, ZLIB_NO_FLUSH));\n}\nfwrite($handler, deflate_add($deflateContext, '', ZLIB_FINISH));\nfclose($handler);\nThen use the matching function for your ZLIB_ENCODING_* constant or the inflate_init/inflate_add functions to un-gzip.. Can you please provide a code sample and the version of the AWS SDK for PHP you're using?. It looks like you're having trouble configuring your DynamoDB Scan call. I'd recommend taking a look at the AWS SDK for PHP DynamoDbClient::scan documentation and the DynamoDB documentation around the 'ScanFilter' parameter.\nIf you're still having issues, I'd recommend either asking a question on StackOverflow tagged with aws-php-sdk, opening a support ticket with AWS Support, or reaching out on the DynamoDB forums.. I'm unable to reproduce your issue here - I can use both the UrlSigner class and the ->getSignedUrl method to sign urls. I don't see anything obvious wrong with your inputs either.\nI'd recommend verifying each piece of the configuration you're using, including specifying the trusted signer for the distribution and that you're accessing the signed url before it expires as you didn't list these above. Amazon CloudFront - Serving Private Content with Signed URLs and Signed Cookies\nIf you're still having issues, I'd recommend asking a question on StackOverflow, opening a support ticket with AWS Support, or reaching out on the CloudFront Forum.. The discrepancy here is that you're skipping over the 'with jitter' section of the documentation. You can read about exponential backoff with full jitter here. We can add an @link annotation to this documentation block.. What are you trying to sign the contents of a pre-signed upload URL for? Having a signed body for a pre-signed upload URL would mean that the consumer of the pre-signed URL would have to upload contents with the same signature - you'd have to know what they're going to upload when generating the URL. Using the UNSIGNED-PAYLOAD allows the consumer to upload whatever contents they wish to the file. You can read more about pre-signed upload URLs here.. You have supplied an empty region string to the client. Please follow the configuration guidelines for setting the region parameter.. Is it possible you should be using createWafRegional? You can read about the differences here.. Can you please provide a code sample for the call you're making?. Could you collect all the $pool->promise() return values and wrap them in an EachPromise?. Thanks for your request, this was added in Version 3.65.1.. It looks like your SimpleXML extension isn't hooked up properly to your PHP install on your EC2 server. I'd recommend searching on or asking a question on StackOverflow or opening a support ticket with AWS Support to help troubleshoot this issue.. Still needs to move to the proper location like .changes/nextrelease/namespaced_phpunit_classes.json.. I'm able to reproduce this issue, we're working on a fix and hope to have it up shortly.. Version 3.24.4 of the SDK was released before PHP 7.2 was available. The last pull request to remove each() calls (#1419) was included in release 3.36.36. Please upgrade to that version to clean up these deprecation warnings.. Looks like this could use a tiny update for test coverage.. What about in_array('callable', $a['valid']) instead for this check?. I read the thread as 'Now that SendMessage allows you to set MessageAttributes, can we validate those on ReceiveMessage as well?'. @bakura10, your input here would be beneficial if possible.\nValidating on SendMessage{Batch} could easily be a separate enhancement request if it is wanted, as the format and closure structure will need to change pretty significantly to capture the necessary data.. I'm making a small set of changes along these lines for clarity, thanks for the feedback!. Should also add a test that a string default that is a parameter-less function (say, gc_collect_cycles) is not invoked.. It's consistent with the calculateMessageAttributesMd5 definition which handles the case of improperly formed 'MessageAttributes'. Since all SQS messages will have a 'Body' field, I'm okay with making this change if we are alright with the inconsistency.. This was because the implicit else condition of !isset($msg['MD5OfMessageAttributes']) is pre-confirmed on the initial if, when we check to see isset($msg['MessageAttributes']).. Any granularity on customizing these operations should still be handled through the 'before_X' callbacks, including the new before_lookup in ObjectCopier. The changes around 'params' here are to build out handling the contracts in the ObjectUploader/ObjectCopier and also allow ease of use for the parameters that are consistent across the operations.. Yes, but only to fulfill what was previously defined and handled in a broken and inconsistent manner. See #1232 . Non-blocker: double . at the end.. Isn't this still susceptible to being incorrect? The only way to guarantee the strtotime case for convertExpires() will be in coordination with the $startTimestamp is to pass in a $now.\nConsider the case:\n1:23:45.9999 - $expiresTimestamp generated from '+1 week'\n1:23:46.0005 - $startTimestamp generated via time()\n1:23:46:0100 - convertExpires() is called, generating 604799. Good catch, re-throwing on failure to get a region (other reason for AuthorizationHeaderMalformed and on other error codes.. Yep, I caught this when reviewing with @cjyclaire's mention above, re-throwing here as well.. Going to keep these separate since the Client handles the cache and the middleware generates into the coroutine instead of returning.. I thought this was reachable but couldn't generate an exception that escaped to this level that would be caught here; cleaning up.. The generator will continue execution when looking for a potential next value to yield and would proceed to throw in all states past the yield invocation on L248 in doing so. This is different from the re-throw in S3ClientTrait because a the return blocks further execution for that scope.. It does fix the issue of throwing an exception on the calculation, but not the correctness of the relative time cases. For instance, the tests for the AuthTokenGenerator detect an explicit expires length. Actually, it looks like these are already failing very infrequently.\nWe could take this PR as is and supply another fix for relative times, though it'd be preferable to fix both at the same time. The change for this would be using convertToTimestamp in convertExpires for the non-strtotime cases and passing the $startTimestamp to strtotime's second param. strtotime will properly ignore being given a $now parameter when not using a relative time. This would be in lieu of doing a convertToTimestamp before sending in to convertExpires.. Since we're so far removed from the transition from V2, these links could be moved all the way down to a subsection of the ### Related AWS Projects section. This would also mean removing the **current** label changes from this changeset, as it's further implied by not being near the V2 information.. @daum Thanks for working through this with us. I think we're both on the same page with regards to the problem and that the solution being worked towards both fixes the issues at hand and maintains our contracts.\nFor a relative expires, which will be the only used case for an optional $relativeTimeBase in convertToTimestamp, we'll already have $startTimestamp and the intent of the contract is the relative expires is based on that. The default/non-existent value should just be null, as that will have strtotime use its own default if we're not overriding. This should probably also warrant a new test on the relative time handling of SignatureV4 of the '+6 days' the suite is set up for; you'll have to pass in the 'start_time' based on the 'aws_time' as an option.. Thanks for making these updates @daum!\nIt seems like I was unclear previously when talking about this section, apologies. I don't think there needs to be a separate relativeTimeBase option, but instead use the $startTimestamp we already have for this line passed to $relativeTimeBase of convertToTimestamp when generating $expiresTimestamp. I can't think of any cases where someone would want 'relativeTimeBase' to not be the same as 'start_time', let me know if I'm missing something.\nAnd a small formatting update: please add a space after the , in this function call.. The ternary located here isn't actually needed. strtotime will use its own default ($now = time()) on being passed null and will correctly ignore the $now when passed a non-relative $time string.\nphp\n$time = time();\n$date = date(DATE_ATOM, $time);\necho strtotime($date); // 1501704830\necho strtotime($date, null); // 1501704830\necho strtotime($date, $time); // 1501704830\necho strtotime('+15 minutes', $time); // 1501705730. That's lovely, this should stay a ternary then. Sorry for the misdirection!\n1339 will temporarily take care of the Travis issue and we'll rerun these jobs.. #1339 has been merged in.. An example of what our 3 line ternary looks like can be found here.. For this case, the ternary can be one line but the arguments would be separated.\nphp\nstrtotime(\n    $dateValue,\n    $relativeTimeBase === null ? time() : $relativeTimeBase\n);. @TakesTheBiscuit Would you like to update this PR to handle the above change? Let us know either way!. This should either be 'For the previous version of the SDK:' or 'For Version 2 of the SDK:', as the current is grammatically incorrect. I'd prefer the latter in this case.. This should be 'DBSnapshotNotFound', as specified here. The waiters-2.json.php compile will also need to be redone.. There are a few requirements that look like they will need their floors raised. I've added this to the backlog to take care of before 7.2 official release, as well as looking at updating our test suite.. Instead of setting the config value here, the conditional below should just be:\nphp\nif (!empty($config['preserve_iterator_keys'])). Yep!. Apologies for this quick redux, I meant in the other comment to remove this whole conditional setting of 'preserve_iterator_keys' and replace the conditional in the $mapFn loop below with the empty() check. Sorry for the confusion!. Just noticed this after merging in the RDS waiter PR, the changelog file needs to be in a named file inside a nextrelease folder:\n\nA changelog document is a small JSON blob placed in the .changes/nextrelease folder. It should be named clearly and uniquely, akin to a branch name.. This can just read 'Set the ContentType if not already present' for clarity.. This !isset( should be empty(, otherwise we'll allow setting the content type to ''.. This was already the case, it was overriding the supplied ContentType as well. This should read:\nNo longer override supplied ContentType parameter when performing a multipart upload.. Yeah, it looks like CodeCov is taking the first report and using that, which explains the wild shifts as well. I'm going to set this up to use 7.1 with latest dependencies only, putting us on stable grounds that we can update with further PHP releases.. It is not returning a promise, but an AesStreamInterface. This is why it's the only piece wrapped in the Promise\\promise_for call.. The $strategy->save() call is made before the PutObject call for the ciphertext. The instruction file will remain on S3 if there is a failure after it has been uploaded, not automatically deleted. I'll add a note to the guide and update the doc block for this file.. 'kms_cmk_id' is how the key is indexed in the materials description, this is consistent with the Java (and other) SDK implementations. The 'KeyId' naming on Encrypt is used for the $kmsKeyId variable.. CEK is a highly used domain specific noun. I'd hesitate to change this to something longer or changing it to EncryptionKey as suggested below. I'll update the documentation for each method that handles a CEK to read:\ncontent encryption key (CEK). It is a bucket name and it is fragile. This is how the AWS SDK for Go handles it and we do something similar for the other full integration tests as well. These tests need to talk to other SDKs and don't get a getResourcePrefix added. I'm open to suggestions on this.. Yeah, I wanted to be explicit here though since there are other return statements in this function.. Good catch!. The thought on async basing the manipulation was if someone invokes $s3EncryptionClient->putObjectAsync(), let's defer the heavy lifting that makes up the encryption work and other calls until they are executed. If someone invokes $s3EncryptionClient->putObject(), the work is immediately waited for anyways.. This update needs to go in its own file, not the previous merged change blob.\nA changelog document is a small JSON blob placed in the .changes/nextrelease folder. It should be named a clearly and uniquely, akin to a branch name.\n\nYou can find a sample in this PR.\nNit since this has to move: the \"category\" should be the singular \"Test\" to match the namespaces for those files.. This check should verify that the field is !empty(), not just check if it's truthy, as its current state will give off 'Undefined index' notices. It also needs to work for all $prefix values.. This update needs to supply similar behavior for ListObjects and CopyObject as the tests above it.. Should the initial $key assignment be at the beginning of this else block?. Some of these characters will be removed due to backslashing, they should be \\\\. Can you also please capitalize the first letter in the description contents?. Yes, previously it would have just fallen out of the switch and continued the loop via line 119 since $shortCipher would be null and not supported.. Can you replace the 'ap-southeast-2' with a <region> specification here? . Can you please add the following introduction here, before the policy sample:\nrst\nTo use the DynamoDB session handler, your `configured credentials <https://docs.aws.amazon.com/aws-sdk-php/v3/guide/guide/credentials.html>`_\nmust have permission to use the DynamoDB table that `you created in a previous step <https://docs.aws.amazon.com/aws-sdk-php/v3/guide/service/dynamodb-session-handler.html#create-a-table-for-storing-your-sessions>`_.\nThe following IAM policy contains the minimum permissions that you need. To use this policy, replace the Resource value\nwith the Amazon Resource Name (ARN) of the table that you created previously. For more information about creating and\nattaching IAM policies, see `Managing IAM Policies <https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_manage.html>`_\nin the *AWS Identity and Access Management User Guide*.. Since this applies to a lot of the namespaces, can you leave this as an empty string \"\" please?. I believe use GuzzleHttp\\Promise; can also be removed.. This is technically unused, but the type annotation for private $client; can be updated to /** @var StsClient */ instead.. Looks like this was intended to alleviate the full namespace definition on line 134.\nThis could be return Promise\\rejection_for( instead.. Looks like this was intended to alleviate the full namespace definition on line 26.\nThis could be return Promise\\promise_for(new Result([])); instead.. It looks like this could was meant to alleviate the fully qualified namespaces on line 116, line 143, line 295, and line 322.\nThis could be * @expectedException AwsException instead.. Please update this to \"Removed or adjusted unused imports.\". Since this applies to a lot of the namespaces, can you leave this as an empty string \"\" please?. Could this be refactored to instead keep the else and have a singular return?\n```php\n        if ($this->newServiceFlag) {\n            //Minor Version Bump if a feature is being released\n            ++$tag[1];\n            $tag[2] = 0;\n        } else {\n            ++$tag[2];\n        }\n    return implode(\".\", $tag);\n\n```. Ran some light tests on this one, looks like the fully qualified namespace is needed in the annotation. Apologies!. I've left a new comment on that thread (reproduced below) that states the change:\n\nAfter some further discussion based around the operations being handled, it is believed we can retry this at the SDK level since it's not safe to do at the Guzzle level. Support is being added for automatic retry of CURLE_RECV_ERROR via #1463.. Guzzle5's ConnectException is just a sub-classed RequestException. The RequestException doesn't surface any of the information in getHandlerContext() (which doesn't exist in Guzzle5) from what I can tell.. I don't believe doc blocks will require having the use statement for the class.. This should be resource|HashContext since it may also be a resource type for older versions of PHP. The same change should apply to the @return annotation on Line 68. Yes, applyDocFilters functions are only called at time of doc building, not at time of json model compiling.. That's an acceptable change, it's done in other places.. This should be \"Api\\\\Parser\" to reflect the namespace.. Style: Please keep the closing paren and open brace ) { on the next line.. Style: Please keep the closing paren and open brace ) { on the next line.. Style: Please keep the closing paren and open brace ) { on the next line.. Style: Please keep the closing paren and open brace ) { on the next line.. Style: This should be in a single line, not over multiple.. Style: Please keep the closing paren and open brace ) { on the next line.. Can you please update this to read:\nRetries CURLE_RECV_ERROR on all RequestException, not just ConnectException.. Can you add test(s) that pass in objects that aren't \\DateTime but extend \\DateTimeInterface? Looks like that's only \\DateTimeImmutable.. The thought is that the current tests already passed with this line as \\DateTime, the addition of a test with \\DateTimeImmutable expresses that the change is working as intended.. Instead of specifying the whole namespace in the extends, can you port this to a use statement and just reference the class name?\n\n```php\nuse PHPUnit\\Framework\\TestCase;\nclass S3EncryptionMultipartUploaderTest extends TestCase\n``. This should be marked as afeaturein the Changelog Document, not just anenhancement..getPresignHeadersshould to also exclude the headers that matchstripos($name, 'x-amz-') !== 0)to prevent bloating presigned URLs with the headers that don't need to be there - See my'Note on S3'`.. Can you please update the formatting for this line, since it's over 80 characters?\nphp\nif (isset($blacklist[$lname])\n    || $lname === strtolower(self::AMZ_CONTENT_SHA256_HEADER)\n) {\n    unset($parsedRequest['headers'][$name]);\n}. Can you add a sentence here reflecting that any additional headers that are added and signed must be supplied when ultimately sending the request?. Can you also please update the formatting for this line, since it's over 80 characters?\nphp\nif (!isset($blacklist[$lName])\n    && $name !== self::AMZ_CONTENT_SHA256_HEADER\n    && stripos($name, 'x-amz-') !== 0\n) {\n    $presignHeaders[] = $lName;\n}. Can you use implode here over its alias function join? This line being over 80 is okay since changing it would make this section harder to read.. Would be nice to have a set of cases in here with trailing numbers as well.. Can you update this to be \"Signature\\\\SignatureV4\" please?. Missed this in the other PR so I'm submitting an update, but these should go in a unique file in the .changes/nextrelease/ folder.. Extra whitespace.. This configuration should be local to the middleware generation below and behind a curl extension check since we don't require cURL. See this PR and it's related bug.. Since this is changing a transport level option, I'd recommend adding setting this as a \"feature\" level to get a minor version bump.. Is this wanted for all operations or just a subset?. Would be nice to have a couple comments in here:\n\nExplain what the regex is capturing\nExplain the parameter replacement since the data structure is ambiguous. Can you add a test for the exceptional case?. \n",
    "snaki4": "Hi,\nI've tried to change 6827 & 6829 (one by one and both together) - doesn't help.\nP.S. version(aws-sdk-php2/2.3.2) from pear has another line numbers, but I have checked that I was inside required \"call\"\n. Hi,\nThx for the update. Changed line, removed second one, error still present:\nError - Aws\\Ec2\\Exception\\Ec2Exception: AWS Error Code: UnknownParameter, Status Code: 400, AWS Request ID: a0677b2c-9b8a-4810-a2f1-e0d6687f354e, AWS Error Type: client, AWS Error Message: The parameter NetworkInterfaceSet is not recognized, User-Agent: aws-sdk-php2/2.3.2 Guzzle/3.4.3 curl/7.22.0 PHP/5.3.10-1ubuntu3.6\n. Ok, See there is part of the AWS SDK(with your patch):\n\n                        'NetworkInterfaces' => array(\n                            'type' => 'array',\n                            'sentAs' => 'NetworkInterface',\n                            'items' => array(\n                                'name' => 'NetworkInterfaceSet',\n                                'type' => 'object',\n                                'properties' => array(\n                                    'NetworkInterfaceId' => array(\n                                        'type' => 'string',\n                                    ),\n                                    'DeviceIndex' => array(\n                                        'type' => 'numeric',\n                                    ),\n                                    'SubnetId' => array(\n                                        'type' => 'string',\n                                    ),\n                                    'Description' => array(\n                                        'type' => 'string',\n                                    ),\n                                    'PrivateIpAddress' => array(\n                                        'type' => 'string',\n                                    ),\n                                    'Groups' => array(\n                                        'type' => 'array',\n                                        'sentAs' => 'SecurityGroupId',\n                                        'items' => array(\n                                            'name' => 'SecurityGroupId',\n                                            'type' => 'string',\n                                        ),\n                                    ),\n                                    'DeleteOnTermination' => array(\n                                        'type' => 'boolean',\n                                        'format' => 'boolean-string',\n                                    ),\n                                    'PrivateIpAddresses' => array(\n                                        'type' => 'array',\n                                        'items' => array(\n                                            'name' => 'PrivateIpAddressesSet',\n                                            'type' => 'object',\n                                            'properties' => array(\n                                                'PrivateIpAddress' => array(\n                                                    'required' => true,\n                                                    'type' => 'string',\n                                                ),\n                                                'Primary' => array(\n                                                    'type' => 'boolean',\n\nThat is last result:\n\nArray\n(\n    [SpotPrice] => 1.00\n    [InstanceCount] => 1\n    [Type] => one-time\n    [ValidUntil] => 2013-05-21T03:00:00\n    [LaunchSpecification] => Array\n        (\n            [ImageId] => ami-3c72e855\n            [KeyName] => misha\n            [InstanceType] => c1.medium\n            [Placement] => Array\n                (\n                    [AvailabilityZone] => us-east-1d\n                )\n            [BlockDeviceMappings] => Array\n                (\n                    [Ebs] => Array\n                        (\n                            [DeleteOnTermination] => 1\n                        )\n                )\n            [SubnetId] => subnet-7a7bb610\n            [NetworkInterfaces] => Array\n                (\n                    [0] => Array\n                        (\n                            [SubnetId] => subnet-7a7bb610\n                            [PrivateIpAddresses] => Array\n                                (\n                                    [0] => Array\n                                        (\n                                            [PrivateIpAddress] => 10.91.10.99\n                                        )\n                                )\n                        )\n                )\n            [SecurityGroupIds] => Array\n                (\n                    [0] => sg-4f51a420\n                )\n        )\n)\nError - Aws\\Ec2\\Exception\\Ec2Exception: AWS Error Code: UnknownParameter, Status Code: 400, AWS Request ID: 6bc5436f-6014-4531-b264-c2c3540aef00, AWS Error Type: client, AWS Error Message: The parameter NetworkInterfaceSet is not recognized, User-Agent: aws-sdk-php2/2.3.2 Guzzle/3.4.3 curl/7.22.0 PHP/5.3.10-1ubuntu3.6\n\nThat is part of php file:\n\n#!/usr/bin/php -q\n?php\nrequire \"AWSSDKforPHP/aws.phar\";\n$config=array();\n$config['key'] = '';\n$config['secret'] = '';\n$config['region'] = '';\n$dialer_idx = trim($argv[1]);\n$dialer_priv = trim($argv[2]);\n$dialer_pub = trim($argv[3]);\n$ami_id = trim($argv[4]);\n$cur_now = strtotime(date(\"Y-m-d 23:00:00\"));\n$utc = new DateTime(null, new DateTimeZone('UTC'));\n$utc->setTimestamp($cur_now);\n$valid_until = $utc->format(\"c\");\n$request = array(\n        'SpotPrice'     => '1.00',\n        'InstanceCount' => 1,\n        'Type'          => 'one-time',\n        'ValidUntil'    => $valid_until,\n        'LaunchSpecification'   => array(\n                'ImageId'               => $ami_id,\n                'KeyName'               => 'misha',\n                'InstanceType'  => 'c1.medium',\n                'Placement'     => array(\n                        'AvailabilityZone'      => 'us-east-1d',\n                ),\n                'BlockDeviceMappings'   => array(\n                        'Ebs'   => array(\n                                'DeleteOnTermination'   => 1,\n                        )\n                ),\n                'SubnetId'      => 'subnet-7a7bb610',\n                'NetworkInterfaces'     => array(\n                        array(\n                                'SubnetId'              => 'subnet-7a7bb610',\n                                #'PrivateIpAddress'     => $dialer_priv,\n                                'PrivateIpAddresses'    => array(\n                                        array('PrivateIpAddress'        => $dialer_priv),\n                                )\n                        )\n                ),\n                'SecurityGroupIds'      => array('sg-4f51a420'),\n        )\n);\nprint_r($request);\n//$ec2Client = Ec2Client::factory($config);\nuse Aws\\Common\\Aws;\nuse Aws\\Ec2\\Exception\\Ec2Exception;\n$aws = Aws::factory($config);\n$ec2 = $aws->get(\"ec2\");\ntry\n{\n        $result = $ec2->requestSpotInstances($request);\n        print_r($result);\n}\ncatch (Ec2Exception $e)\n{\n        echo \"Error - $e\";\n}\n?>\n\n. Hi,\nYes, I just wake up and got it that I'm using phar and not php file and I need to recompile that file.\nEmail sent to you.\n. ",
    "fabianotessarolo": "Hello Michael! \nI'll change my code!\nThank you very much.\n. ",
    "digitechinternational": "I am using  PHP phar  to get the  list of files  from S3 bucket in a loop.  \nphp\n$iterator = $client->getIterator('ListObjects', array(\n    'Bucket'    => $config->S3_bucket,\n    'Prefix'    => $config->S3_prefix,\n    'Delimiter' =>\"/\"\n));\nIt works fine, but sometime I get the  following error , which breaks the process: \nPHP Fatal error:  Uncaught exception 'Guzzle\\Common\\Exception\\RuntimeException' with message\n'Unable to parse response body into XML: String could not be parsed as XML' in\nphar:///var/www/html/teflon/aws.phar/vendor/guzzle/guzzle/src/Guzzle/Http/Message/Response.php:938`\nAs this execution is internal to the PHP.phar ,  i am not sure if there is any flag/setting/way to bypass this error and keep the script continuing.  OR if there is any other check before the above command which is eliminating this error. \nPlease do let me know if you have any suggestion.\nThanks \n. This is the actual text it shows on the command line.  As it doesn't happen each time  , its very difficult to  reproduce. But, this is the text when it stopped working. My intent is to just keep the loop going, even if it doesnt return any data. Sorry for the long text. \nWarning: SimpleXMLElement::__construct(): /\nfalse\n<?xml in phar:///var/www/html/teflon/aws.phar/vendor/guzzle/guzzle/src/Guzzle/Http/Message/Response.php on line 936\nPHP Warning:  SimpleXMLElement::__construct():                                                                                ^ in phar:///var/www/html/teflon/aws.phar/vendor/guzzle/guzzle/src/Guzzle/Http/Message/Response.php on line 936\nWarning: SimpleXMLElement::__construct():                                                                                ^ in phar:///var/www/html/teflon/aws.phar/vendor/guzzle/guzzle/src/Guzzle/Http/Message/Response.php on line 936\nPHP Warning:  SimpleXMLElement::__construct(): Entity: line 3: parser error : Extra content at the end of the document in phar:///var/www/html/teflon/aws.phar/vendor/guzzle/guzzle/src/Guzzle/Http/Message/Response.php on line 936\nWarning: SimpleXMLElement::__construct(): Entity: line 3: parser error : Extra content at the end of the document in phar:///var/www/html/teflon/aws.phar/vendor/guzzle/guzzle/src/Guzzle/Http/Message/Response.php on line 936\nPHP Warning:  SimpleXMLElement::__construct(): neat-fil in phar:///var/www/html/teflon/aws.phar/vendor/guzzle/guzzle/src/Guzzle/Http/Message/Response.php on line 936\nWarning: SimpleXMLElement::__construct(): neat-fil in phar:///var/www/html/teflon/aws.phar/vendor/guzzle/guzzle/src/Guzzle/Http/Message/Response.php on line 936\nPHP Warning:  SimpleXMLElement::__construct(): ^ in phar:///var/www/html/teflon/aws.phar/vendor/guzzle/guzzle/src/Guzzle/Http/Message/Response.php on line 936\nWarning: SimpleXMLElement::__construct(): ^ in phar:///var/www/html/teflon/aws.phar/vendor/guzzle/guzzle/src/Guzzle/Http/Message/Response.php on line 936\nPHP Fatal error:  Uncaught exception 'Guzzle\\Common\\Exception\\RuntimeException' with message 'Unable to parse response body into XML: String could not be parsed as XML' in phar:///var/www/html/teflon/aws.phar/vendor/guzzle/guzzle/src/Guzzle/Http/Message/Response.php:938\nStack trace:\n0 phar:///var/www/html/teflon/aws.phar/vendor/guzzle/guzzle/src/Guzzle/Service/Command/LocationVisitor/Response/XmlVisitor.php(20): Guzzle\\Http\\Message\\Response->xml()\n1 phar:///var/www/html/teflon/aws.phar/vendor/guzzle/guzzle/src/Guzzle/Service/Command/OperationResponseParser.php(122): Guzzle\\Service\\Command\\LocationVisitor\\Response\\XmlVisitor->before(Object(Aws\\S3\\Command\\S3Command), Array)\n2 phar:///var/www/html/teflon/aws.phar/vendor/guzzle/guzzle/src/Guzzle/Service/Command/OperationResponseParser.php(96): Guzzle\\Service\\Command\\OperationResponseParser->visitResult(Object(Guzzle\\Service\\Description\\Parameter), Object(Aws\\S3\\Command\\S3Command), Object(Guzzle\\Http\\Message\\Response))\n3 phar:///var/www/html/teflon/aws.phar/vendor/guzzle/guzzle in phar:///var/www/html/teflon/aws.phar/vendor/guzzle/guzzle/src/Guzzle/Http/Message/Response.php on line 938\nFatal error: Uncaught exception 'Guzzle\\Common\\Exception\\RuntimeException' with message 'Unable to parse response body into XML: String could not be parsed as XML' in phar:///var/www/html/teflon/aws.phar/vendor/guzzle/guzzle/src/Guzzle/Http/Message/Response.php:938\nStack trace:\n0 phar:///var/www/html/teflon/aws.phar/vendor/guzzle/guzzle/src/Guzzle/Service/Command/LocationVisitor/Response/XmlVisitor.php(20): Guzzle\\Http\\Message\\Response->xml()\n1 phar:///var/www/html/teflon/aws.phar/vendor/guzzle/guzzle/src/Guzzle/Service/Command/OperationResponseParser.php(122): Guzzle\\Service\\Command\\LocationVisitor\\Response\\XmlVisitor->before(Object(Aws\\S3\\Command\\S3Command), Array)\n2 phar:///var/www/html/teflon/aws.phar/vendor/guzzle/guzzle/src/Guzzle/Service/Command/OperationResponseParser.php(96): Guzzle\\Service\\Command\\OperationResponseParser->visitResult(Object(Guzzle\\Service\\Description\\Parameter), Object(Aws\\S3\\Command\\S3Command), Object(Guzzle\\Http\\Message\\Response))\n3 phar:///var/www/html/teflon/aws.phar/vendor/guzzle/guzzle in phar:///var/www/html/teflon/aws.phar/vendor/guzzle/guzzle/src/Guzzle/Http/Message/Response.php on line 938\n. Yes this is working now.  \nThanks  \nFrom: Jeremy Lindblom [mailto:notifications@github.com] \nSent: Wednesday, July 03, 2013 9:11 PM\nTo: aws/aws-sdk-php\nCc: digitechinternational\nSubject: Re: [aws-sdk-php] Unable to parse response body into XML (#104)\nWere you able to look into this some more?\n\u2014\nReply to this email directly or view it on GitHub https://github.com/aws/aws-sdk-php/issues/104#issuecomment-20454326 .  https://github.com/notifications/beacon/fjNc75Mvd0Tm1OJEfpovDPNOJ3nRAIGdKyc9WsNUDnlgLhpwdbCPtRNU6nACOapy.gif \n. ",
    "iwankgb": "It would be great to fix it as it makes unit testing difficult (you can't mock a client).\n. ",
    "markdwhite": "I'd only just added the library to the project and was initially running with the PHPDepend defaults. The symptom is easily solved by adding an ignore to the ant task for pdepend.\nBut I had no idea if there was something untoward that was the cause behind this, which is why I raised it. If it's all as it should be, I think I'm done here.\nThanks for the quick response.\n. ",
    "westinpigott": "Nevermind.  There was a routing error in my route 53 that was directing to the wrong domain.\n. ",
    "ghost": "Sorry, turns out it was nothing to do with DynamoDB.\nAt first the problem couldn't be replicated on local session files, then it started happening locally too. Traced it to mod_rewrite.\n. I'm still investigating the issue myself. The .htaccess file had something like this:\nRewriteEngine On\nRewriteBase /\nRewriteCond %{REQUEST_FILENAME} !-f\nRewriteCond %{REQUEST_FILENAME} !-d\nRewriteCond %{REQUEST_FILENAME} !-l\nRewriteRule ^(.*)$ index.php?u=$1 [B,L,QSA]\nThe situation is made more convoluted by the fact that we tried integrating directives from HTML5 Boilerplate / Initializr. The code I cited above was actually in a separate php file (not index.php) so the rewrite shouldn't have been triggered because of RewriteCond %{REQUEST_FILENAME} !-f. However, once the redirect was removed, lo and behold, the duplicate session problem cleared up. Haha... Will update with findings if you're still curious.\n. Found the answer in my access_log, browser was asking for favicon.ico.\nmod_rewrite redirected the request to index.php?u=/favicon.ico, accounting for the second session.\n. If you use body_as_string for large files you will get a memory error.\nYou could try something like this:\nphp\nif (filesize($file_name) <= 3000000) { // 3mb\n    $client->getConfig()->set('curl.options', array('body_as_string' => true));\n}\nelse {\n    $client->getConfig()->remove('curl.options');\n}\n. Thanks for looking into this Jeremy, and Happy Thanksgiving!\nI don't think IAM is the problem here, but I'm attaching the policy nonetheless:\n{\n    \"Statement\":[{\n        \"Effect\":\"Allow\",\n        \"Action\":\"dynamodb:*\",\n        \"Resource\":\"*\"\n    }]\n}\n. Turns out DynamoDB doesn't like empty strings for its attribute value. Upon unsetting the last session variable, AbstractLockingStrategy will try to update 'data' attribute with an empty string, causing it to fail. I've sent a pull request (#191) that addresses this. \n. Thanks for providing the open source SDK! I'm just scratch my own itch here.\nPreviously, I was on v1 and base64_encodeing my sessions to get around session_encode not playing nice with private variables, so I only just found this.\n. Please look into #206!\n. Hey Michael,\nThanks very much for your input! I'll definitely keep you guys posted on the progress.\nHowever, I'm a bit curious about when you said it was too specific. Is there currently an AWS recommended way of managing error and access logs from web servers in elastic load balanced / auto scaling groups?\nCheers,\nJames\n. Thanks for your input, Ryan! I totally agree what I have in its current form is too specific to be beneficial to the majority of the SDK's user base.\nIn terms of scope, however, I feel this feature (after getting support for other web servers and storage backends) could be as relevant as DynamoDB session handler, managing logging as DynamoDB session handler manages sessions in an ELB/AS environment.\nI guess part of the reason I raised this issue is to seek affirmation I'm not needlessly reinventing the wheel here and that it's not something that's being actively worked on by the good guys at AWS.\nFor now, I will take your advice and make a composer package, finishing up the missing stuff I need, though I don't have much incentive to work on additional web servers / backends. Hopefully contributors will step in to scratch their own itch and we can revisit the issue of incorporating into the SDK once all the bits are there.\nThanks everyone for your insightful and constructive feedback, it's much appreciated.\n. Followed by an apachectl restart. Thank you for posting the fix!\n. it is working , i was did completeMultipartUpload  without null $parts unfortunately \n. Thank You @jeremeamia for your reply \nI found my mistake !\nwhenever a part is uploaded it returns the  'ETag' , I should update Etag with partnumber in an array for every part uploading.  but I did not do that , so last Etag only was sent to completeMultipartUpload so last part only saved ! \n. @jeremeamia I wrote this report the other day, but closed it after I received no response (and deleted my account). I figured that it must be a problem on my end, as it seemed to me more people would be complaining if this was actually an issue with the Transfer Manager. \nAre you having problems, too?\nI have been using putObject() as a work around, so my frustrations with Transfer Manager haven't disturbed my work too greatly. Though I would much, much prefer that I could get it working again as it was a vastly better solution than putObject() for my program.\nOh well.\n. @acoulton While this problem has become increasingly frustrating for me, I am pleased to learn that someone else is experiencing difficulty with the Transfer Manager so I feel less crazy about it.\nI deleted the account which was used to create the initial issue report, so I cannot reopen it. I feel silly for having closed it, though I felt embarrassed.\n@jeremeamia Would you be able to reopen this?\n. I can probably stop using bucket endpoints as a workaround\n. Thanks!\n. @harshavardhana X-Amz-Content-Sha256 is an actual sha256 string\n@jeskew if i understand you correctly the sdk produces X-Amz-Content-Sha256 = sha256sum('UNSIGNED-PAYLOAD') since im requesting an object?\n. https://XXXXX.XXX/XXX.XX?X-Amz-Content-Sha256=e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=6YHM698SXMHMBTEGT0YM%2F20160407%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20160407T015753Z&X-Amz-SignedHeaders=host&X-Amz-Expires=1200&X-Amz-Signature=f2f62d5b02bce74c15bebe2cf553ca00def36bb8faec416fdaf6d03f622cb96a\nthis is the request the newest version of the sdk is producing with:\n$command = $client->getCommand('GetObject', [\n            'Bucket' => $bucket,\n            'Key'    => '5f1e5d9d5f052XXXXX9d62a5d49fa334a.jpg'\n        ]);\n        $request = $client->createPresignedRequest($command, '+20 minutes');\n. The body is UNSIGNED-PAYLOAD but X-Amz-Content-Sha256 is sha256('').\n$ cat /dev/null|sha256sum\ne3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855  -\n$\nits this function:\nreturn Psr7\\hash($request->getBody(), 'sha256'); SignatureV4.php Line 146\nDouble checked with my request above\n@jeskew is this normal? please clarify.\n. @cjyclaire Thanks for your reply. So the getAwsErrorCode() will give me one of the exceptions listed above depending on error?\n. @mtdowling @cjyclaire @claylo @trevorrowe @skyzyx Thank you so much for your help. Special thanks to @mtdowling and @cjyclaire. I would suggest you guys make the documentation clearer on exception handling. I am assuming it's basically because you use \"unchecked exceptions\"\n. @cjyclaire So am i doing right? Just wanted to double check:\nphp\ntry {\n    $this->snsClient->publish(...);\n} catch (SnsException $e) {\n    switch ($e->getAwsErrorCode()) {\n        case 'EndpointDisabledException':\n        case 'NotFoundException':\n            /// do something\n            break;\n    }\n}\n. ",
    "gws": "@skyzyx\n\nSending many HTTP requests serially (one at a time) can cause an unnecessary delay in a script's execution. Each request must complete before a subsequent request can be sent. By sending requests in parallel, a pool of HTTP requests can complete at the speed of the slowest request in the pool, significantly reducing the amount of time needed to execute multiple HTTP requests. Guzzle provides a wrapper for the curl_multi functions in PHP.\n\nhttp://guzzlephp.org/http-client/client.html#sending-requests-in-parallel\n. Some more information on the Exception Translation pattern: http://stackoverflow.com/a/11819104\n. :+1:\n. Might if (PHP_VERSION_ID >= 50400) work better here? It's designed for comparisons, and version strings can have interesting vendor-specific attributes in them.\n. ",
    "nartmal": "I see. thanks\n. ",
    "dinglidingli": "Hey guys,\nI've started getting this error in my CLI results:\n[2013-07-05 13:43:42] log.ERROR: exception 'ErrorException' with message 'Invalid argument supplied for foreach()' in /dev-site/vendor/aws/aws-sdk-php/src/Aws/Sqs/Md5ValidatorListener.php:46\nEdit: So I'm getting this error because my queue resultset was empty, just FYI\n. ",
    "Sammaye": "So it is a matter of trial and error to find out what specific errors I can catch at the moment.\nOk kool thanks for the help :)\n. Though if buckets no longer need regions (my bucket was in us-west-2 originally and my old code, on the original API, uses it there) then isn't the clients setting of region on the S3Client a bug?\n. I have just checked some code: https://github.com/Sammaye/aws_worker/blob/master/encoder.php#L17 that works on the old API wth the same value for the bucket url videos.stagex.co.uk.\nIt seems that is was working globally without a region\n. So even if all my other stuff is in us-west-2 and s3buckets don't use regions anymore this shouldnt be changed so s3buckets dont take the region into account? Because I cannot find anyway to make a region specific bucket that's why I carried on this thread.\nIt seems like functionality that could be removed.\n. well you say it might in west-1 but my URL to view buckets is actually: https://console.aws.amazon.com/s3/home?region=us-west-2 and I remember making it there to make the transfer between my SQS queues and other components, including ec2 (in the same region) as fast as possible.\n. Wait no I am an idiot, it was never like that. I remember now, it was only US/EU etc buckets.\nOk ignore me on that, though considering the new behaviour of buckets it does seem that this is functionality that could be removed still and buckets shouldnt take the clients region.\n. Ok Kool thanks for the help :)\n. ",
    "brendandixon": "Hmm, no memory of getting tripped up on this. Still, it seems a good idea.\n. ",
    "atulatri": "Hi,\nSame is happening with 'SetAccessControlPolicy'.\nThanks & Regards,\nAtul Atri.\n. ",
    "bigtallbill": "For anyone who comes across this it was because i was forgetting to include the AppId option. Doh!\n. ",
    "jhkchan": "I disagree with what @mtdowling said as on S3 Management Console you can create empty folder with the button provided by Amazon. I tested the API and we can actually create the same empty folder with a slash at the end of the key. So the code:\n$s3Client->putObject(array(\n    'Bucket' => $bucket,\n    'Key'    => $folder . '/',\n    'Body'   => '',\n));\nworks perfectly.\nI would regard StreamWrapper as a simple wrapper and should not add too many custom logic. So it should either link to API CreateBucket to create bucket or PutObject with a slash at the end to create folder. The current implementation of StreamWrapper breaks some PHP libraries as they enforce folder existence by calling mkdir(), which you issue an error. Just let the API do the job.\n. ",
    "biranchi2018": "Why this bug is closed ?\nIts 2018 and the bug still exits......\nsns.createPlatformApplication promise does not return anything, either success or failure...... ",
    "waylandzhang": "Hi Jeremy,\nSo I can not use the sdk method \"PutItem\" like this way:\n\n//some json raw data\n$row = '{\n    \"Day\": \"Monday\",\n    \"UnreadEmails\": 42,\n    \"ItemsOnMyDesk\": [\n        \"Coffee Cup\",\n        \"Telephone\",\n        {\n            \"Pens\": {\n                \"Quantity\": 3\n            }\n        }\n    ]\n}';\n// write to DynamoDB\n$result = $client->putItem(array(\n    'TableName' => 'My_Table',\n    'Item' => array(\n        'Id'      => array('S' => 'Id000001'),\n        'data' => array('M' => $raw),\n        ),\n    )\n);\n\nThis seems doesn't working for the current SDK. Do we have to parse each node in the raw json and give something like 'S' => value instead?\nThanks\n. Thank you, that helps a lot.\n. Thanks for reply, I actually worked on this with my company internal purpose and it works fine.\nI'll post any issue if I found in the future, meanwhile if there is anyone interested in this client please find me on github, i'm happy to share.\n. ",
    "mexitek": "if I use\nphp\n  include('aws.phar');\n  // more code ...\nHow can I start using the latest master branch of Guzzle? I am not that familiar with phar files. Any ideas are appreciated.\n. Still not sure if this is a bug or if it's something I am doing wrong. Any advice would be appreciated. Thanks.\n. 1. Ubuntu 12.04.2 LTS (GNU/Linux 3.2.0-40-virtual i686)\n2. PHP 5.4.14\n3. No APC\n4. Not command line. Apache is running the script.\n5. aws.phar version: 2.4.2\n. I edited the issue description. I notice that it's anytime I try to use the stream wrapper. With is_file() I get the same error. I feel more and more that's it's something I am doing wrong, just not sure what.\n. I switched to aws.phar version 2.4.5 and it went away! Everything works as expected now. :+1: \nClosing the issue.\n. ",
    "fireproofsocks": "Not an issue, but related: if you are using this package to bind the PHP internals (file_exists, getimagesize, etc) and you're using those to determine MIME type, your server will download the file locally during some of those requests, and if your server fills up, it may report the mime-type as application/octet-stream instead of the file's true MIME type.  Clean up your server, and the functions will report accurately again.. ",
    "andy3rdworld": "What are the performance implications ( if any ) of using the following as compared to the cURL default?\n$client->getConfig()->set('curl.options', array('body_as_string' => true));\n. ",
    "Danack": "I just saw this once whilst using the S3Client::uploadDirectory function. Re-running the code produced no error.\nI think the below is the relevant part - I have the complete log available if required.\n```\n\nRe-using existing connection! (#1) with host s3-eu-west-1.amazonaws.com\n\nConnected to s3-eu-west-1.amazonaws.com (178.236.4.160) port 443 (#1)\n\nPUT /satis.basereality.com/packages/Danack/Auryn/Danack_Auryn_v0.7.0.zip HTTP/1.1\nHost: s3-eu-west-1.amazonaws.com\nUser-Agent: aws-sdk-php2/2.5.4 Guzzle/3.8.1 curl/7.28.1 PHP/5.4.9\nx-amz-acl: private\nContent-Type: application/zip\nContent-MD5: oUxKjJiLYCHLyDPJkONstw==\nDate: Tue, 05 Aug 2014 11:44:45 +0000\nAuthorization: AWS 12345=\nContent-Length: 24766\n\n\n\nRe-using existing connection! (#2) with host s3-eu-west-1.amazonaws.com\n\n\nConnected to s3-eu-west-1.amazonaws.com (178.236.4.160) port 443 (#2)\n\nPUT /satis.basereality.com/packages/Danack/Auryn/Danack_Auryn_v0.7.1.zip HTTP/1.1\nHost: s3-eu-west-1.amazonaws.com\nUser-Agent: aws-sdk-php2/2.5.4 Guzzle/3.8.1 curl/7.28.1 PHP/5.4.9\nx-amz-acl: private\nContent-Type: application/zip\nContent-MD5: 6fjzNy9pPSjqkLyObE9RbQ==\nDate: Tue, 05 Aug 2014 11:44:45 +0000\nAuthorization: AWS 12345=\nContent-Length: 24891\n\n\n\nRe-using existing connection! (#3) with host s3-eu-west-1.amazonaws.com\n\n\nConnected to s3-eu-west-1.amazonaws.com (178.236.4.160) port 443 (#3)\n\nPUT /satis.basereality.com/packages/Danack/Auryn/Danack_Auryn_v0.8.0.zip HTTP/1.1\nHost: s3-eu-west-1.amazonaws.com\nUser-Agent: aws-sdk-php2/2.5.4 Guzzle/3.8.1 curl/7.28.1 PHP/5.4.9\nx-amz-acl: private\nContent-Type: application/zip\nContent-MD5: eXePvliWnY9QJ/u6SadkAA==\nDate: Tue, 05 Aug 2014 11:44:45 +0000\nAuthorization: AWS 12345=\nContent-Length: 24984\n\n\n\nRe-using existing connection! (#4) with host s3-eu-west-1.amazonaws.com\n\n\nConnected to s3-eu-west-1.amazonaws.com (178.236.4.160) port 443 (#4)\n\nPUT /satis.basereality.com/packages/Danack/Auryn/Danack_Auryn_v0.8.1.zip HTTP/1.1\nHost: s3-eu-west-1.amazonaws.com\nUser-Agent: aws-sdk-php2/2.5.4 Guzzle/3.8.1 curl/7.28.1 PHP/5.4.9\nx-amz-acl: private\nContent-Type: application/zip\nContent-MD5: Vj3FM3VTezU7jc/GYEhwbw==\nDate: Tue, 05 Aug 2014 11:44:45 +0000\nAuthorization: AWS 12345=\nContent-Length: 25296\n\n\n\nWe are completely uploaded and fine\n\nWe are completely uploaded and fine\nWe are completely uploaded and fine\nWe are completely uploaded and fine\nWe are completely uploaded and fine\nConnection died, retrying a fresh connect\nnecessary data rewind wasn't possible\nClosing connection #1\nConnection died, retrying a fresh connect\nnecessary data rewind wasn't possible\nClosing connection #4\n< HTTP/1.1 200 OK\n< x-amz-id-2: 0HN/lPmrGgVgyOZmFzwQBH7NpfOXgkLTxBzMWmabh4aP02JP4EHtfEgVPC0aIAmd\n< x-amz-request-id: D23E7F5E770A7F98\n< Date: Tue, 05 Aug 2014 11:44:46 GMT\n< ETag: \"ab48b49903547cfe7e77e40be7564550\"\n< Content-Length: 0\n< Server: AmazonS3\n< \nConnection #0 to host s3-eu-west-1.amazonaws.com left intact\n< HTTP/1.1 200 OK\n< x-amz-id-2: WKuK57dIhZ00sfBsfOlmRESvE1fyF4niMz65Lmn42C6M3zsiq6c13kELgxRxUs1w\n< x-amz-request-id: 8A9C81672855A789\n< Date: Tue, 05 Aug 2014 11:44:46 GMT\n< ETag: \"e9f8f3372f693d28ea90bc8e6c4f516d\"\n< Content-Length: 0\n< Server: AmazonS3\n< \nConnection #2 to host s3-eu-west-1.amazonaws.com left intact\n< HTTP/1.1 200 OK\n< x-amz-id-2: ngyFaQesD32A/fhfKjQrOkExXQZHR0TeCbCky+bbcpCdYgtpbfYAeDAj9xq2MkMH\n< x-amz-request-id: 86B5223A854BFE93\n< Date: Tue, 05 Aug 2014 11:44:46 GMT\n< ETag: \"79778fbe58969d8f5027fbba49a76400\"\n< Content-Length: 0\n< Server: AmazonS3\n< \n\nConnection #3 to host s3-eu-west-1.amazonaws.com left intact\nPHP Fatal error:  Uncaught exception 'Guzzle\\Service\\Exception\\CommandTransferException' with message 'Errors during multi transfer\n(Guzzle\\Http\\Exception\\CurlException) /documents/projects/github/Bastion/Bastion/vendor/guzzle/guzzle/src/Guzzle/Http/Curl/CurlMulti.php line 338\n[curl] 65: necessary data rewind wasn't possible [url] https://s3-eu-west-1.amazonaws.com/satis.basereality.com/packages/Danack/Auryn/Danack_Auryn_v0.7.0.zip\n0 /documents/projects/github/Bastion/Bastion/vendor/guzzle/guzzle/src/Guzzle/Http/Curl/CurlMulti.php(279): Guzzle\\Http\\Curl\\CurlMulti->isCurlException(Object(Guzzle\\Http\\Message\\EntityEnclosingRequest), Object(Guzzle\\Http\\Curl\\CurlHandle), Array)\n1 /documents/projects/github/Bastion/Bastion/vendor/guzzle/guzzle/src/Guzzle/Http/Curl/CurlMulti.php(244): Guzzle\\Http\\Curl\\CurlMulti->processResponse(Object(Guzzle\\Http\\Message\\EntityEnclosingRequest), Object(Guzzle\\Http\\Curl\\CurlHandle), Array)\n2 /documents/projects/github/Bastion/Bastion/vendor/guzzle/guzzle/src/Guzzle/Http/Curl/Curl in /documents/projects/github/Bastion/Bastion/vendor/guzzle/guzzle/src/Guzzle/Service/Exception/CommandTransferException.php on line 25\n\n\n```\n. Well I found the problem:\nhttp://docs.aws.amazon.com/AWSEC2/latest/APIReference/ec2-api-permissions.html#d0e52225\n\nThe following Amazon EC2 API actions currently do not support resource-level permissions. To use these actions in an IAM policy, you must grant users permission to use all resources for the action by using a * wildcard for the Resource element in your statement.\n\nI still think it would be nice to have the required resource to be discoverable or listed in the exception message....\nI guess maybe I should report the policy simulator has a bug in it if the above policy is saying 'allowed' in the policy simulator for the 'describeInstances' access, even though the resource element is incorrect?\n. ",
    "Molkobain": "Thank you for the explaination, I understand now why I've been searching the web in vain for that damn SDK... It's a shame they didn't made a complete sdk for all their services. I'm going to take a look at the forum you linked.\nThanks !\n. ",
    "pcolby": "Absolutely.  You can have the code under Apache 2.0 license :)\nCheers.\n. Thanks guys! :)\n. Regarding licensing, you can have these code changes under any license you want, including Apache.\n. ",
    "datibbaw": "I hereby agree to submit this under Apache 2 license :)\n. Seems like a good idea :)\n. ",
    "jezhailwood": "No problem. If I pass in a resource with a URL encoded query string like this:\n```\n$resource = 'http://example.com/file.jpg?response-content-disposition=attachment%3B%20filename%3D%22new-name.jpg%22';\n$client = Aws\\CloudFront\\CloudFrontClient::factory(array(\n  'key_pair_id' => 'KEY_PAIR_ID',\n  'private_key' => 'PATH_TO_PRIVATE_KEY',\n));\n$url = $client->getSignedUrl(array(\n  'url' => $resource,\n  'expires' => time() + 3600,\n));\n```\n$url equals this:\nhttp://example.com/file.jpg?response-content-disposition=attachment; filename=\"new-name.jpg\"&Expires=1378496226&Signature=D4csBANkktLxoAQ5I2QtwU31RXP77lOpE17W9cO3eutP7vTcBqyeMoj1zTX0tZIsE9QXpy24lzDgGIADC9wjVRRSRLxxFK5wG8NCU6e1eMpsIha3s5xL-hzHpOFWL3~CFnV6AITxfGfLg-VKYhaG6lthB8L6vjYxHn~6uKhfWns_&Key-Pair-Id=APKAI6HTGYOCCJ3QVD4Q\n. ",
    "banasiak": "Thanks for the quick reply, mtdowling.  The Content-Length header seems to be there.\n```\n About to connect() to cloudsearch.us-east-1.amazonaws.com port 443 (#0)\n   Trying 72.21.195.216...\n Connected to cloudsearch.us-east-1.amazonaws.com (72.21.195.216) port 443 (#0)\n successfully set certificate verify locations:\n   CAfile: none\n  CApath: /etc/ssl/certs\n SSL connection using RC4-MD5\n Server certificate:\n    subject: C=US; ST=Washington; L=Seattle; O=Amazon.com Inc.; CN=cloudsearch.us-east-1.amazonaws.com\n    start date: 2011-11-03 00:00:00 GMT\n    expire date: 2013-11-03 23:59:59 GMT\n    common name: cloudsearch.us-east-1.amazonaws.com (matched)\n    issuer: C=US; O=VeriSign, Inc.; OU=VeriSign Trust Network; OU=Terms of use at https://www.verisign.com/rpa (c)10; CN=VeriSign Class 3 Secure Server CA - G3\n*    SSL certificate verify ok.\n\nPOST / HTTP/1.1\nHost: cloudsearch.us-east-1.amazonaws.com\nUser-Agent: aws-sdk-php2/2.4.5 Guzzle/3.7.3 curl/7.29.0 PHP/5.4.9-4ubuntu2.3\nContent-Type: application/x-www-form-urlencoded; charset=utf-8\nContent-Length: 256\n\n\nupload completely sent off: 256 out of 256 bytes\n< HTTP/1.1 200 OK\n< x-amzn-RequestId: f7c0615f-1cc2-11e3-b7fa-918f1f1b385e\n< Content-Type: text/xml\n< Content-Length: 1277\n< Date: Fri, 13 Sep 2013 22:22:27 GMT\n< \nConnection #0 to host cloudsearch.us-east-1.amazonaws.com left intact\n\nRequest:\nPOST / HTTP/1.1\nHost: cloudsearch.us-east-1.amazonaws.com\nUser-Agent: aws-sdk-php2/2.4.5 Guzzle/3.7.3 curl/7.29.0 PHP/5.4.9-4ubuntu2.3\nContent-Type: application/x-www-form-urlencoded; charset=utf-8\nAction=DescribeDomains&Version=2011-02-01&DomainNames.member.1=rab-test&Timestamp=2013-09-13T22%3A22%3A17%2B00%3A00&SignatureVersion=2&SignatureMethod=HmacSHA256&AWSAccessKeyId=*&Signature=*\nResponse:\nHTTP/1.1 200 OK\nx-amzn-RequestId: f7c0615f-1cc2-11e3-b7fa-918f1f1b385e\nContent-Type: text/xml\nContent-Length: 1277\nDate: Fri, 13 Sep 2013 22:22:27 GMT\n\n\n\n\n1\n\narn:aws:cs:us-east-1:676109577446:search/rab-test\nsearch-rab-test-e7c74w5xlay7klvfk7kndwqqgy.us-east-1.cloudsearch.amazonaws.com\n\n0\ntrue\nsearch.m1.small\n676109577446/rab-test\n1\nfalse\nrab-test\ntrue\nfalse\n\narn:aws:cs:us-east-1:676109577446:doc/rab-test\ndoc-rab-test-e7c74w5xlay7klvfk7kndwqqgy.us-east-1.cloudsearch.amazonaws.com\n\n\n\n\n\nf7c0615f-1cc2-11e3-b7fa-918f1f1b385e\n\n\nErrors: 0\n\nFound bundle for host cloudsearch.us-east-1.amazonaws.com: 0x13282b0\nRe-using existing connection! (#0) with host cloudsearch.us-east-1.amazonaws.com\n\nConnected to cloudsearch.us-east-1.amazonaws.com (72.21.195.216) port 443 (#0)\n\nPOST / HTTP/1.1\nHost: cloudsearch.us-east-1.amazonaws.com\nUser-Agent: aws-sdk-php2/2.4.5 Guzzle/3.7.3 curl/7.29.0 PHP/5.4.9-4ubuntu2.3\nContent-Type: application/x-www-form-urlencoded; charset=utf-8\nContent-Length: 13835\n\n\n\nupload completely sent off: 13835 out of 13835 bytes\n< HTTP/1.1 505 HTTP Version Not Supported\n< Date: Fri, 13 Sep 2013 22:22:27 GMT\n< Connection: close\n< \n\nClosing connection 0\n\nRequest:\nPOST / HTTP/1.1\nHost: cloudsearch.us-east-1.amazonaws.com\nUser-Agent: aws-sdk-php2/2.4.5 Guzzle/3.7.3 curl/7.29.0 PHP/5.4.9-4ubuntu2.3\nContent-Type: application/x-www-form-urlencoded; charset=utf-8\nAction=UpdateSynonymOptions&Version=2011-02-01&DomainName=rab-test&Synonyms=%7B%22synonyms%22%3A%7B%22n%22%3A%5B%22north%22%5D%2C%22s%22%3A%5B%22south%22%5D%2C%22e%22%3A%5B%22east%22%5D%2C%22w%22%3A%5B%22west%22%5D%2C%22aly%22%3A%5B%22alley%22%5D%2C%22anx%22%3A%5B%22annex%22%5D%2C%22apt%22%3A%5B%22apartment%22%5D%2C%22arc%22%3A%5B%22arcade%22%5D%2C%22ave%22%3A%5B%22avenue%22%5D%2C%22bch%22%3A%5B%22beach%22%5D%2C%22bg%22%3A%5B%22burg%22%5D%2C%22bgs%22%3A%5B%22burgs%22%5D%2C%22bldg%22%3A%5B%22building%22%5D%2C%22blf%22%3A%5B%22bluff%22%5D%2C%22blfs%22%3A%5B%22bluffs%22%5D%2C%22blvd%22%3A%5B%22boulevard%22%5D%2C%22bnd%22%3A%5B%22bend%22%5D%2C%22br%22%3A%5B%22branch%22%5D%2C%22brg%22%3A%5B%22bridge%22%5D%2C%22brk%22%3A%5B%22brook%22%5D%2C%22brks%22%3A%5B%22brooks%22%5D%2C%22bsmt%22%3A%5B%22basement%22%5D%2C%22btm%22%3A%5B%22bottom%22%5D%2C%22byp%22%3A%5B%22bypass%22%5D%2C%22byu%22%3A%5B%22bayou%22%5D%2C%22chse%22%3A%5B%22chase%22%5D%2C%22cir%22%3A%5B%22circle%22%5D%2C%22cirs%22%3A%5B%22circles%22%5D%2C%22clb%22%3A%5B%22club%22%5D%2C%22clf%22%3A%5B%22cliff%22%5D%2C%22clfs%22%3A%5B%22cliffs%22%5D%2C%22cmn%22%3A%5B%22common%22%5D%2C%22cmns%22%3A%5B%22commons%22%5D%2C%22cor%22%3A%5B%22corner%22%5D%2C%22cors%22%3A%5B%22corners%22%5D%2C%22cp%22%3A%5B%22camp%22%5D%2C%22cpe%22%3A%5B%22cape%22%5D%2C%22cres%22%3A%5B%22crescent%22%5D%2C%22crk%22%3A%5B%22creek%22%5D%2C%22crse%22%3A%5B%22course%22%5D%2C%22crst%22%3A%5B%22crest%22%5D%2C%22cswy%22%3A%5B%22causeway%22%5D%2C%22ct%22%3A%5B%22court%22%5D%2C%22ctr%22%3A%5B%22center%22%5D%2C%22ctrs%22%3A%5B%22centers%22%5D%2C%22cts%22%3A%5B%22courts%22%5D%2C%22curv%22%3A%5B%22curve%22%5D%2C%22cv%22%3A%5B%22cove%22%5D%2C%22cvs%22%3A%5B%22coves%22%5D%2C%22cyn%22%3A%5B%22canyon%22%5D%2C%22dept%22%3A%5B%22department%22%5D%2C%22dl%22%3A%5B%22dale%22%5D%2C%22dm%22%3A%5B%22dam%22%5D%2C%22dr%22%3A%5B%22drive%22%5D%2C%22drs%22%3A%5B%22drives%22%5D%2C%22dv%22%3A%5B%22divide%22%5D%2C%22est%22%3A%5B%22estate%22%5D%2C%22ests%22%3A%5B%22estates%22%5D%2C%22expy%22%3A%5B%22expressway%22%5D%2C%22ext%22%3A%5B%22extension%22%5D%2C%22exts%22%3A%5B%22extensions%22%5D%2C%22fl%22%3A%5B%22floor%22%5D%2C%22fld%22%3A%5B%22field%22%5D%2C%22flds%22%3A%5B%22fields%22%5D%2C%22fls%22%3A%5B%22falls%22%5D%2C%22flt%22%3A%5B%22flat%22%5D%2C%22flts%22%3A%5B%22flats%22%5D%2C%22frd%22%3A%5B%22ford%22%5D%2C%22frds%22%3A%5B%22fords%22%5D%2C%22frg%22%3A%5B%22forge%22%5D%2C%22frgs%22%3A%5B%22forges%22%5D%2C%22frk%22%3A%5B%22fork%22%5D%2C%22frks%22%3A%5B%22forks%22%5D%2C%22frnt%22%3A%5B%22front%22%5D%2C%22frst%22%3A%5B%22forest%22%5D%2C%22fry%22%3A%5B%22ferry%22%5D%2C%22ft%22%3A%5B%22fort%22%5D%2C%22fwy%22%3A%5B%22freeway%22%5D%2C%22gdn%22%3A%5B%22garden%22%5D%2C%22gdns%22%3A%5B%22gardens%22%5D%2C%22gln%22%3A%5B%22glen%22%5D%2C%22glns%22%3A%5B%22glens%22%5D%2C%22grn%22%3A%5B%22green%22%5D%2C%22grns%22%3A%5B%22greens%22%5D%2C%22grv%22%3A%5B%22grove%22%5D%2C%22grvs%22%3A%5B%22groves%22%5D%2C%22gtwy%22%3A%5B%22gateway%22%5D%2C%22hbr%22%3A%5B%22harbor%22%5D%2C%22hbrs%22%3A%5B%22harbors%22%5D%2C%22hl%22%3A%5B%22hill%22%5D%2C%22hls%22%3A%5B%22hills%22%5D%2C%22hngr%22%3A%5B%22hanger%22%5D%2C%22holw%22%3A%5B%22hollow%22%5D%2C%22hts%22%3A%5B%22heights%22%5D%2C%22hvn%22%3A%5B%22haven%22%5D%2C%22hwy%22%3A%5B%22highway%22%5D%2C%22inlt%22%3A%5B%22inlet%22%5D%2C%22is%22%3A%5B%22island%22%5D%2C%22iss%22%3A%5B%22islands%22%5D%2C%22jct%22%3A%5B%22junction%22%5D%2C%22jcts%22%3A%5B%22junctions%22%5D%2C%22knl%22%3A%5B%22knoll%22%5D%2C%22knls%22%3A%5B%22knolls%22%5D%2C%22ky%22%3A%5B%22key%22%5D%2C%22kys%22%3A%5B%22keys%22%5D%2C%22lbby%22%3A%5B%22lobby%22%5D%2C%22lck%22%3A%5B%22lock%22%5D%2C%22lcks%22%3A%5B%22locks%22%5D%2C%22ldg%22%3A%5B%22lodge%22%5D%2C%22lf%22%3A%5B%22loaf%22%5D%2C%22lgt%22%3A%5B%22light%22%5D%2C%22lgts%22%3A%5B%22lights%22%5D%2C%22lk%22%3A%5B%22lake%22%5D%2C%22lks%22%3A%5B%22lakes%22%5D%2C%22ln%22%3A%5B%22lane%22%5D%2C%22lndg%22%3A%5B%22landing%22%5D%2C%22lowr%22%3A%5B%22lower%22%5D%2C%22mdw%22%3A%5B%22meadow%22%5D%2C%22mdws%22%3A%5B%22meadows%22%5D%2C%22ml%22%3A%5B%22mill%22%5D%2C%22mls%22%3A%5B%22mills%22%5D%2C%22mnr%22%3A%5B%22manor%22%5D%2C%22mnrs%22%3A%5B%22manors%22%5D%2C%22msn%22%3A%5B%22mission%22%5D%2C%22mt%22%3A%5B%22mount%22%5D%2C%22mtn%22%3A%5B%22mountain%22%5D%2C%22mtns%22%3A%5B%22mountains%22%5D%2C%22mtwy%22%3A%5B%22motorway%22%5D%2C%22nck%22%3A%5B%22neck%22%5D%2C%22ofc%22%3A%5B%22office%22%5D%2C%22opas%22%3A%5B%22overpass%22%5D%2C%22orch%22%3A%5B%22orchard%22%5D%2C%22ovlk%22%3A%5B%22overlook%22%5D%2C%22ph%22%3A%5B%22penthouse%22%5D%2C%22pkwy%22%3A%5B%22parkway%22%5D%2C%22pl%22%3A%5B%22place%22%5D%2C%22pln%22%3A%5B%22plain%22%5D%2C%22plns%22%3A%5B%22plains%22%5D%2C%22plz%22%3A%5B%22plaza%22%5D%2C%22pne%22%3A%5B%22pine%22%5D%2C%22pnes%22%3A%5B%22pines%22%5D%2C%22pr%22%3A%5B%22prairie%22%5D%2C%22prt%22%3A%5B%22port%22%5D%2C%22prts%22%3A%5B%22ports%22%5D%2C%22psge%22%3A%5B%22passage%22%5D%2C%22pt%22%3A%5B%22point%22%5D%2C%22pte%22%3A%5B%22pointe%22%5D%2C%22pts%22%3A%5B%22points%22%5D%2C%22radl%22%3A%5B%22radial%22%5D%2C%22rd%22%3A%5B%22road%22%5D%2C%22rdg%22%3A%5B%22ridge%22%5D%2C%22rdgs%22%3A%5B%22ridges%22%5D%2C%22rds%22%3A%5B%22roads%22%5D%2C%22riv%22%3A%5B%22river%22%5D%2C%22rm%22%3A%5B%22room%22%5D%2C%22rnch%22%3A%5B%22ranch%22%5D%2C%22rpd%22%3A%5B%22rapid%22%5D%2C%22rpds%22%3A%5B%22rapids%22%5D%2C%22rst%22%3A%5B%22rest%22%5D%2C%22rte%22%3A%5B%22route%22%5D%2C%22shl%22%3A%5B%22shoal%22%5D%2C%22shls%22%3A%5B%22shoals%22%5D%2C%22shr%22%3A%5B%22shore%22%5D%2C%22shrs%22%3A%5B%22shores%22%5D%2C%22skwy%22%3A%5B%22skyway%22%5D%2C%22smt%22%3A%5B%22summit%22%5D%2C%22spc%22%3A%5B%22space%22%5D%2C%22spg%22%3A%5B%22spring%22%5D%2C%22spgs%22%3A%5B%22springs%22%5D%2C%22sq%22%3A%5B%22square%22%5D%2C%22sqs%22%3A%5B%22squares%22%5D%2C%22st%22%3A%5B%22street%22%5D%2C%22sta%22%3A%5B%22station%22%5D%2C%22ste%22%3A%5B%22suite%22%5D%2C%22stra%22%3A%5B%22stravenue%22%5D%2C%22strm%22%3A%5B%22stream%22%5D%2C%22sts%22%3A%5B%22streets%22%5D%2C%22ter%22%3A%5B%22terrace%22%5D%2C%22tpke%22%3A%5B%22turnpike%22%5D%2C%22trak%22%3A%5B%22track%22%5D%2C%22trce%22%3A%5B%22trace%22%5D%2C%22trfy%22%3A%5B%22trafficway%22%5D%2C%22trl%22%3A%5B%22trail%22%5D%2C%22trlr%22%3A%5B%22trailer%22%5D%2C%22trwy%22%3A%5B%22throughway%22%5D%2C%22tunl%22%3A%5B%22tunnel%22%5D%2C%22un%22%3A%5B%22union%22%5D%2C%22uns%22%3A%5B%22unions%22%5D%2C%22upas%22%3A%5B%22underpass%22%5D%2C%22uppr%22%3A%5B%22upper%22%5D%2C%22via%22%3A%5B%22viaduct%22%5D%2C%22vis%22%3A%5B%22vista%22%5D%2C%22vl%22%3A%5B%22ville%22%5D%2C%22vlg%22%3A%5B%22village%22%5D%2C%22vlgs%22%3A%5B%22villages%22%5D%2C%22vly%22%3A%5B%22valley%22%5D%2C%22vlys%22%3A%5B%22valleys%22%5D%2C%22vw%22%3A%5B%22view%22%5D%2C%22vws%22%3A%5B%22views%22%5D%2C%22whf%22%3A%5B%22wharf%22%5D%2C%22wl%22%3A%5B%22well%22%5D%2C%22wls%22%3A%5B%22wells%22%5D%2C%22xing%22%3A%5B%22crossing%22%5D%2C%22xrd%22%3A%5B%22crossroad%22%5D%2C%22xrds%22%3A%5B%22crossroads%22%5D%2C%22north%22%3A%5B%22n%22%5D%2C%22south%22%3A%5B%22s%22%5D%2C%22east%22%3A%5B%22e%22%5D%2C%22west%22%3A%5B%22w%22%5D%2C%22alley%22%3A%5B%22aly%22%5D%2C%22annex%22%3A%5B%22anx%22%5D%2C%22apartment%22%3A%5B%22apt%22%5D%2C%22arcade%22%3A%5B%22arc%22%5D%2C%22avenue%22%3A%5B%22ave%22%5D%2C%22beach%22%3A%5B%22bch%22%5D%2C%22burg%22%3A%5B%22bg%22%5D%2C%22burgs%22%3A%5B%22bgs%22%5D%2C%22building%22%3A%5B%22bldg%22%5D%2C%22bluff%22%3A%5B%22blf%22%5D%2C%22bluffs%22%3A%5B%22blfs%22%5D%2C%22boulevard%22%3A%5B%22blvd%22%5D%2C%22bend%22%3A%5B%22bnd%22%5D%2C%22branch%22%3A%5B%22br%22%5D%2C%22bridge%22%3A%5B%22brg%22%5D%2C%22brook%22%3A%5B%22brk%22%5D%2C%22brooks%22%3A%5B%22brks%22%5D%2C%22basement%22%3A%5B%22bsmt%22%5D%2C%22bottom%22%3A%5B%22btm%22%5D%2C%22bypass%22%3A%5B%22byp%22%5D%2C%22bayou%22%3A%5B%22byu%22%5D%2C%22chase%22%3A%5B%22chse%22%5D%2C%22circle%22%3A%5B%22cir%22%5D%2C%22circles%22%3A%5B%22cirs%22%5D%2C%22club%22%3A%5B%22clb%22%5D%2C%22cliff%22%3A%5B%22clf%22%5D%2C%22cliffs%22%3A%5B%22clfs%22%5D%2C%22common%22%3A%5B%22cmn%22%5D%2C%22commons%22%3A%5B%22cmns%22%5D%2C%22corner%22%3A%5B%22cor%22%5D%2C%22corners%22%3A%5B%22cors%22%5D%2C%22camp%22%3A%5B%22cp%22%5D%2C%22cape%22%3A%5B%22cpe%22%5D%2C%22crescent%22%3A%5B%22cres%22%5D%2C%22creek%22%3A%5B%22crk%22%5D%2C%22course%22%3A%5B%22crse%22%5D%2C%22crest%22%3A%5B%22crst%22%5D%2C%22causeway%22%3A%5B%22cswy%22%5D%2C%22court%22%3A%5B%22ct%22%5D%2C%22center%22%3A%5B%22ctr%22%5D%2C%22centers%22%3A%5B%22ctrs%22%5D%2C%22courts%22%3A%5B%22cts%22%5D%2C%22curve%22%3A%5B%22curv%22%5D%2C%22cove%22%3A%5B%22cv%22%5D%2C%22coves%22%3A%5B%22cvs%22%5D%2C%22canyon%22%3A%5B%22cyn%22%5D%2C%22department%22%3A%5B%22dept%22%5D%2C%22dale%22%3A%5B%22dl%22%5D%2C%22dam%22%3A%5B%22dm%22%5D%2C%22drive%22%3A%5B%22dr%22%5D%2C%22drives%22%3A%5B%22drs%22%5D%2C%22divide%22%3A%5B%22dv%22%5D%2C%22estate%22%3A%5B%22est%22%5D%2C%22estates%22%3A%5B%22ests%22%5D%2C%22expressway%22%3A%5B%22expy%22%5D%2C%22extension%22%3A%5B%22ext%22%5D%2C%22extensions%22%3A%5B%22exts%22%5D%2C%22floor%22%3A%5B%22fl%22%5D%2C%22field%22%3A%5B%22fld%22%5D%2C%22fields%22%3A%5B%22flds%22%5D%2C%22falls%22%3A%5B%22fls%22%5D%2C%22flat%22%3A%5B%22flt%22%5D%2C%22flats%22%3A%5B%22flts%22%5D%2C%22ford%22%3A%5B%22frd%22%5D%2C%22fords%22%3A%5B%22frds%22%5D%2C%22forge%22%3A%5B%22frg%22%5D%2C%22forges%22%3A%5B%22frgs%22%5D%2C%22fork%22%3A%5B%22frk%22%5D%2C%22forks%22%3A%5B%22frks%22%5D%2C%22front%22%3A%5B%22frnt%22%5D%2C%22forest%22%3A%5B%22frst%22%5D%2C%22ferry%22%3A%5B%22fry%22%5D%2C%22fort%22%3A%5B%22ft%22%5D%2C%22freeway%22%3A%5B%22fwy%22%5D%2C%22garden%22%3A%5B%22gdn%22%5D%2C%22gardens%22%3A%5B%22gdns%22%5D%2C%22glen%22%3A%5B%22gln%22%5D%2C%22glens%22%3A%5B%22glns%22%5D%2C%22green%22%3A%5B%22grn%22%5D%2C%22greens%22%3A%5B%22grns%22%5D%2C%22grove%22%3A%5B%22grv%22%5D%2C%22groves%22%3A%5B%22grvs%22%5D%2C%22gateway%22%3A%5B%22gtwy%22%5D%2C%22harbor%22%3A%5B%22hbr%22%5D%2C%22harbors%22%3A%5B%22hbrs%22%5D%2C%22hill%22%3A%5B%22hl%22%5D%2C%22hills%22%3A%5B%22hls%22%5D%2C%22hanger%22%3A%5B%22hngr%22%5D%2C%22hollow%22%3A%5B%22holw%22%5D%2C%22heights%22%3A%5B%22hts%22%5D%2C%22haven%22%3A%5B%22hvn%22%5D%2C%22highway%22%3A%5B%22hwy%22%5D%2C%22inlet%22%3A%5B%22inlt%22%5D%2C%22island%22%3A%5B%22is%22%5D%2C%22islands%22%3A%5B%22iss%22%5D%2C%22junction%22%3A%5B%22jct%22%5D%2C%22junctions%22%3A%5B%22jcts%22%5D%2C%22knoll%22%3A%5B%22knl%22%5D%2C%22knolls%22%3A%5B%22knls%22%5D%2C%22key%22%3A%5B%22ky%22%5D%2C%22keys%22%3A%5B%22kys%22%5D%2C%22lobby%22%3A%5B%22lbby%22%5D%2C%22lock%22%3A%5B%22lck%22%5D%2C%22locks%22%3A%5B%22lcks%22%5D%2C%22lodge%22%3A%5B%22ldg%22%5D%2C%22loaf%22%3A%5B%22lf%22%5D%2C%22light%22%3A%5B%22lgt%22%5D%2C%22lights%22%3A%5B%22lgts%22%5D%2C%22lake%22%3A%5B%22lk%22%5D%2C%22lakes%22%3A%5B%22lks%22%5D%2C%22lane%22%3A%5B%22ln%22%5D%2C%22landing%22%3A%5B%22lndg%22%5D%2C%22lower%22%3A%5B%22lowr%22%5D%2C%22meadow%22%3A%5B%22mdw%22%5D%2C%22meadows%22%3A%5B%22mdws%22%5D%2C%22mill%22%3A%5B%22ml%22%5D%2C%22mills%22%3A%5B%22mls%22%5D%2C%22manor%22%3A%5B%22mnr%22%5D%2C%22manors%22%3A%5B%22mnrs%22%5D%2C%22mission%22%3A%5B%22msn%22%5D%2C%22mount%22%3A%5B%22mt%22%5D%2C%22mountain%22%3A%5B%22mtn%22%5D%2C%22mountains%22%3A%5B%22mtns%22%5D%2C%22motorway%22%3A%5B%22mtwy%22%5D%2C%22neck%22%3A%5B%22nck%22%5D%2C%22office%22%3A%5B%22ofc%22%5D%2C%22overpass%22%3A%5B%22opas%22%5D%2C%22orchard%22%3A%5B%22orch%22%5D%2C%22overlook%22%3A%5B%22ovlk%22%5D%2C%22penthouse%22%3A%5B%22ph%22%5D%2C%22parkway%22%3A%5B%22pkwy%22%5D%2C%22place%22%3A%5B%22pl%22%5D%2C%22plain%22%3A%5B%22pln%22%5D%2C%22plains%22%3A%5B%22plns%22%5D%2C%22plaza%22%3A%5B%22plz%22%5D%2C%22pine%22%3A%5B%22pne%22%5D%2C%22pines%22%3A%5B%22pnes%22%5D%2C%22prairie%22%3A%5B%22pr%22%5D%2C%22port%22%3A%5B%22prt%22%5D%2C%22ports%22%3A%5B%22prts%22%5D%2C%22passage%22%3A%5B%22psge%22%5D%2C%22point%22%3A%5B%22pt%22%5D%2C%22pointe%22%3A%5B%22pte%22%5D%2C%22points%22%3A%5B%22pts%22%5D%2C%22radial%22%3A%5B%22radl%22%5D%2C%22road%22%3A%5B%22rd%22%5D%2C%22ridge%22%3A%5B%22rdg%22%5D%2C%22ridges%22%3A%5B%22rdgs%22%5D%2C%22roads%22%3A%5B%22rds%22%5D%2C%22river%22%3A%5B%22riv%22%5D%2C%22room%22%3A%5B%22rm%22%5D%2C%22ranch%22%3A%5B%22rnch%22%5D%2C%22rapid%22%3A%5B%22rpd%22%5D%2C%22rapids%22%3A%5B%22rpds%22%5D%2C%22rest%22%3A%5B%22rst%22%5D%2C%22route%22%3A%5B%22rte%22%5D%2C%22shoal%22%3A%5B%22shl%22%5D%2C%22shoals%22%3A%5B%22shls%22%5D%2C%22shore%22%3A%5B%22shr%22%5D%2C%22shores%22%3A%5B%22shrs%22%5D%2C%22skyway%22%3A%5B%22skwy%22%5D%2C%22summit%22%3A%5B%22smt%22%5D%2C%22space%22%3A%5B%22spc%22%5D%2C%22spring%22%3A%5B%22spg%22%5D%2C%22springs%22%3A%5B%22spgs%22%5D%2C%22square%22%3A%5B%22sq%22%5D%2C%22squares%22%3A%5B%22sqs%22%5D%2C%22street%22%3A%5B%22st%22%5D%2C%22station%22%3A%5B%22sta%22%5D%2C%22suite%22%3A%5B%22ste%22%5D%2C%22stravenue%22%3A%5B%22stra%22%5D%2C%22stream%22%3A%5B%22strm%22%5D%2C%22streets%22%3A%5B%22sts%22%5D%2C%22terrace%22%3A%5B%22ter%22%5D%2C%22turnpike%22%3A%5B%22tpke%22%5D%2C%22track%22%3A%5B%22trak%22%5D%2C%22trace%22%3A%5B%22trce%22%5D%2C%22trafficway%22%3A%5B%22trfy%22%5D%2C%22trail%22%3A%5B%22trl%22%5D%2C%22trailer%22%3A%5B%22trlr%22%5D%2C%22throughway%22%3A%5B%22trwy%22%5D%2C%22tunnel%22%3A%5B%22tunl%22%5D%2C%22union%22%3A%5B%22un%22%5D%2C%22unions%22%3A%5B%22uns%22%5D%2C%22underpass%22%3A%5B%22upas%22%5D%2C%22upper%22%3A%5B%22uppr%22%5D%2C%22viaduct%22%3A%5B%22via%22%5D%2C%22vista%22%3A%5B%22vis%22%5D%2C%22ville%22%3A%5B%22vl%22%5D%2C%22village%22%3A%5B%22vlg%22%5D%2C%22villages%22%3A%5B%22vlgs%22%5D%2C%22valley%22%3A%5B%22vly%22%5D%2C%22valleys%22%3A%5B%22vlys%22%5D%2C%22view%22%3A%5B%22vw%22%5D%2C%22views%22%3A%5B%22vws%22%5D%2C%22wharf%22%3A%5B%22whf%22%5D%2C%22well%22%3A%5B%22wl%22%5D%2C%22wells%22%3A%5B%22wls%22%5D%2C%22crossing%22%3A%5B%22xing%22%5D%2C%22crossroad%22%3A%5B%22xrd%22%5D%2C%22crossroads%22%3A%5B%22xrds%22%5D%7D%7D&Timestamp=2013-09-13T22%3A22%3A19%2B00%3A00&SignatureVersion=2&SignatureMethod=HmacSHA256&AWSAccessKeyId=&Signature=\nResponse:\nHTTP/1.1 505 HTTP Version Not Supported\nDate: Fri, 13 Sep 2013 22:22:27 GMT\nConnection: close\nErrors: 0\n```\n. Thanks!  I already invalidated those keys, though.\n. I tried today with dev-master a705110.  I see the new Authorization: AWS4-HMAC-SHA256 header, but still receive the same HTTP 505 error.\n. Oh, sorry.  Yes, that is my other account.  It seemed to be working fine this morning.  I was able to set the previously mentioned synonyms on my domain.  However, running the same script this afternoon results in the same HTTP 505 error as before.  I think its safe to say this isn't an SDK problem.  Can you pass this along to the back-end team, or do you recommend I contact them in some other way?  Thanks again for your help.\n. ",
    "doapp-richard": "Its working today with 2.4.6, so the server guys must have fixed something on their side.  Thanks for looking into this for me!\n. ",
    "tatsuya6502": "Thank you for looking into this. I checked AWS SDK for Java and Ruby, and found that they have been using CompleteMultipartUpload element from very beginning.\nAWS SDK for Java, RequestXmlFactory.java, first commit on Nov 9,2010\njava\nxml.start(\"CompleteMultipartUpload\");\nAWS SDK for Ruby, s3/client.rb, first commit on Jul 14, 2011\nruby\nreq.body =\n  \"<CompleteMultipartUpload>#{parts_xml}</CompleteMultipartUpload>\"\nI have no idea why MultipartUpload does not cause an error with S3, but I would recommend to follow the API document and use CompleteMultipartUpload.\n. Thank you! I'll give it a try.\n. My colleague tried the latest SDK on the master branch against the S3 API compatible storage and found that multipart upload works now. Thanks a lot for working on the fix!\n. Thank you very much for the code snippet. Wow, this is more than I expected!\nI'll give it a try and share it with my customer. I'll also keep trying to reproduce the problem. I'll let you know if I find anything.\n. > > They found so far 7 objects, out of ~1 million uploads, are truncated.\n\nIs there any information that you can share about the 7 files that you observed as truncated? What file size was uploaded and what was the expected size? Is there a common pattern regarding the sizes (e.g., they are all X size but were expected to be Y size)?\n\nThanks for looking into this issue. Here is the information. All files (except 6) seem to miss the last part. I don't see any strong relationships between sizes.\n```\nunit: bytes\nnote: 5242880 = 5MB (part size)\nFile Size on S3  Expected Size    Diff       Upload Parts\n1:    5242880         6996186     -1753306   5242880 * 1 + 1753306\n2:    5242880         8493874     -3250994   5242880 * 1 + 3250994\n3:    5242880         8493891     -3251011   5242880 * 1 + 3251011\n4:    5242880         8493894     -3251014   5242880 * 1 + 3251014\n5:   10485760        11936050     -1450290   5242880 * 2 + 1450290\n6:   18712585        13469705     -5242880   5242880 * 3 + 2983945\n7:   31457280        32336458      -879178   5242880 * 6 +  879178\n```\n\n\nThey use setConcurrency(300) and the default values for other parameters.\n\nThat's a very large number. I think you'll get better throughput by reducing this number significantly. At 300 concurrent requests, you're probably at 100% CPU and possibly saturating your network connection.\n\nUnderstood. I think the high concurrency number wouldn't be related to this issue because about 99% of files they have uploaded are smaller than 20MB. The actual concurrency number will be <= 4.\n. Unfortunately, I couldn't reproduce the problem and no further information available to proceed the investigation.\nLet me close this issue for now because:\n- I couldn't reproduce the problem.\n- It seems nobody else has this problem.\n- I reviewed the source codes of AWS SDK for PHP 2 and Guzzle, and couldn't find anything that would explain how the problem occurred. Guzzle's Http\\Curl\\CurlMulti and PHP's cURL modules look good to me.\n- I don't have access to my customer's application source codes though I have received some code fragments of it. As far as I know, it downloads S3 objects from an S3 bucket and uploads them to another S3 bucket. There may be other places that things can go wrong.\nIf the problem occurs again and I get more information, I'll reopen this issue.\nThanks!\n. ",
    "bracki": "Ah, great. This should be far more prominent. No search on Google brought this up, but maybe now ;).\n. ",
    "VikasPace": "\nThis should solve the issue you are having. Feel free to reopen if you have any additional problems.\n\nThank You. my issue is resolved  now. ",
    "aesposito": "Request:\n```\nRequest: POST / HTTP/1.1 Host: sns.us-west-2.amazonaws.com User-Agent: aws-sdk-php2/2.4.3 Guzzle/3.7.2 curl/7.24.0 PHP/5.4.4 Content-Type: application/x-www-form-urlencoded; charset=utf-8 x-amz-date: 20131002T125422Z Authorization: AWS4-HMAC-SHA256 Credential=.../20131002/us-west-2/sns/aws4_request, SignedHeaders=content-type;host;user-agent;x-amz-date, Signature=1a4bcf2b927e62e9a6a7ccd29233b5a81ccad3e4cb56b691f720105ec6d32474 Action=Publish&Version=2010-03-31&TopicArn=arn...&Message=Message&MessageStructure={ \"default\": \"MessageStructure\", \"APNS_SANDBOX\": \"{\\\"aps\\\":{\\\"alert\\\": \\\"message\\\",\\\"sound\\\":\\\"default\\\",\\\"custom\\\":\\\"info\\\"}}\"}\n```\nResponse:\n```\nResponse: HTTP/1.1 200 OK x-amzn-RequestId: 9e0883cf-77af-5373-b3ec-c9096e6fff61 Content-Type: text/xml Content-Length: 294 Date: Wed, 02 Oct 2013 12:54:24 GMT fd010fc0-5ded-5b3e-ab2b-4506442db72c 9e0883cf-77af-5373-b3ec-c9096e6fff61 # Errors: 0\n```\n. I haven't found a solution for this problem, from AWS console it's working perfectly.\n. I have found the solution.\nMessageStructure parameter must receive \"json\" and Message must receive the information\n. Of course\n$sns->publish(array(\n    'TopicArn' => 'arn...',\n    'Message' => '{ \"default\": \"Message\", \"APNS_SANDBOX\": \"{\\\"aps\\\":{\\\"alert\\\": \\\"message\\\",\\\"sound\\\":\\\"default\\\",\\\"custom\\\":\\\"info\\\"}}\"}',\n    'MessageStructure' => 'json',\n));\nThanks\n. I don't get any error, the SQS was created fine, but when I publish from my SNS doesn't register the messages in the queue.\n\n\nIf I change from the AWS console permissions to \"Everybody (*)\", it register the messages.\n\n\nMy AWS account ID is fine.\n\n. Ok, thanks.\nIs there a way to send \"Everybody (*)\" to AWSAccountIds?\n. ",
    "vidswap": "This patch does not work for me. Is there something I'm missing? I get continual and daily curl 18 errors. Any ideas? \n. To be honest, I'm not sure how to track the retries in guzzle so I don't know. Would something like this work or does the backoff bypass this event till the final failure? \n$request->getEventDispatcher()->addListener('request.error', function (Event $e) {\n   //log retries\n});\nI'm also using whatever default backoff is used in the AWS sdk. I haven't configured anything of my own. \nI guess I could turn on AWS logging\u2026 I'll do that. \n. ",
    "matisoffn": "Just ran into this error.  Still no solution?  I haven't changed anything and all of the sudden I'm getting this exception thrown.\n. @jeremeamia I'm using aws/aws-sdk-php-laravel.  I believe it uses the aws-sdk-php repo as well.  It's aws/aws-sdk-php-laravel v1.1.2\nIt's the same exception Aws\\Common\\Exception\\TransferException\n. Prior to commenting on the issue I ran a composer self-update and a composer update and the problem persisted.\nI've echoed out \\Aws\\Common\\Aws::VERSION and it output 2.8.1.  I'll try getting the cURL output and post back here.\n. I added the array as a second parameter to my AWS::get('s3'); line and got the same error from Laravel.  It's AWS::get('s3') for me, not Aws::get('s3')\n. The snippet of code that's throwing the error can be seen below.\nOne error: [curl] 60: [url] https://my-bucket.s3-us-west-2.amazonaws.com/{dynamic_id_here}.png\nThere's also this: Aws\\Common\\Exception\\TransferException\n\u2026/\u00advendor/\u00adaws/\u00adaws-sdk-php/\u00adsrc/\u00adAws/\u00adCommon/\u00adClient/\u00adAbstractClient.php258\n``` php\n$s3 = AWS::get('s3');\n$s3->putObject(array(\n    'Bucket'    => 'my-bucket',\n    'Key'       => $id . '.png',\n    'Body'      => fopen('avs/' . $id . '.png', 'r')\n));\n``\n. @jeremeamia did that and got the[curl] 60: [url] https://my-bucket.s3-us-west-2.amazonaws.com/{dynamic_id_here}.png` back from it.\n. The code was being run in a production application.  Bucket name does not have a \".\" in it, but it does have a hyphen \"-\" in it.\nI've specified the correct region.  Any idea why I'm still having this error?\n. Same problem, just abandoned trying to fix it.\n. ",
    "weknowsoftware": "I get nothing. It doesn't return from the call!\nThis behaviour occurs periodically. I can't predict it.\n-------- Original Message --------\nFrom: Michael Dowling notifications@github.com\nSent: Mon Oct 14 21:09:41 GMT+01:00 2013\nTo: aws/aws-sdk-php aws-sdk-php@noreply.github.com\nCc: simonguerrero simon@guerrero.net\nSubject: Re: [aws-sdk-php] receiveMessage sometimes hangs (#165)\n\nUsing latest code from github, calling receiveMessage with five second timeout, max messages of 10 and requesting all AttributeNames sometimes doesn't return from receiveMessage.\n\nWhat happens? Do you get an exception? Do you get an empty result with no messages?\n\nReply to this email directly or view it on GitHub:\nhttps://github.com/aws/aws-sdk-php/issues/165#issuecomment-26284749\n. ```\nI'd never had this before until\n    the other day. Subsequent to me posting the issue, the machine\n    running the client was rebooted for maintenance purposes. Since\n    then, it hasn't done it. Maybe some network issue?\n    Either way, not reproducible. Thanks for the help!\n-------- Original Message --------\n  Subject: Re: [aws-sdk-php] receiveMessage sometimes hangs (#165)\n  From: Jeremy Lindblom notifications@github.com\n  To: aws/aws-sdk-php aws-sdk-php@noreply.github.com\n  Cc: simonguerrero simon@guerrero.net\n  Date: 17/10/2013 17:24\nIt hangs indefinitely? That's strange.\n  Have you had this problem in the past or just with the\n      latest version of the SDK?\n    Where are you running the code? \n    Try attaching\n        the wire logger to see if you can get any more\n      information about what is being sent and received over the\n      wire when this problem occurs.\n    Can you share the exact code snippet you are using?\n    Is the queue empty?\n  \u2014\n    Reply to this email directly or view\n      it on GitHub.\n```\n. ",
    "hidehara": "Thanks for trying the code on Mac.\nI should change some for Mac&Linux I think. \nI'm thinking that I want to add env. control statement. Is that comfortable for the project ?\n# Make command to each environment Linux/Mac and Windows\n    if os.name == 'nt':\n        sh = 'php -r \\\"$c = include \\'' + path + '\\'; echo json_encode($c);\\\"'\n    else:\n        sh = 'php -r \\'$c = include \"' + path + '\"; echo json_encode($c);\\''\n. Hello, @mtdowling \nI checked the code which be fixed again on Windows 7 (x64) and Ubuntu 12.04 (x86). I got success on the both env.\nIf you check the code again on your Mac, I'm fine to know. \n. Thanks too!\n. ",
    "rcambien": "You're welcome !\n. ",
    "slatem": "The key in question had the following characters:\n&#;\n. I was able to delete the keys using Cloudberry S3 Explorer PRO, but could not list the keys or delete them using the PHP API\n. The\n```\n\n```\n\ncharacter was not in the key.\n. Ok. Guess we can leave this closed. I can't duplicate this and since I already deleted the keys in question I'll let you know if it pops up again.\n. ",
    "frankdejonge": "Thanks for you explanation, makes it very clear why the current state is as it is. I'll look forward to some less work-intensive solutions to this problem. \n. The only thing I can think which might put people off here is the raised minimum php version, which I don't think is enough of an argument not to do this. PHP 5.3 has been EOL for quite a long time now. People can't expect to have the latest packages on an old php version, that's just how these things go. :+1: \n. @skyzyx welcome to the wonderful world of \"maintaining open source packages\". Yes, there are quite a lot of people that are on 5.3 still. There's got to be some real incentive to make them move forward though, if more maintainers do it, it'll all move a little quicker.\n. @jeremeamia seems like @skyzyx and me must live in a different part of the spectrum then :P. Btw, Flysystem will also be moving to 5.4 in the next release (which will shortly lead to the 1.x). I've asked around about it too, all I got were \"go for it\" responses.\n. @jeremeamia: from what I've read, it depends. When you're creating a new instance, it's @return static to respect late static binding. Then when you're referring to $this it's also valid to do @return $this which also points to the correct class. Only self points to the class itself, which makes no sense when in an abstract class.\n. So I've talked to the phpDocumentor lead, and he basically confirmed everything we discussed here. self shouldn't be in an abstract class. Where a new instance is returned using new static() it should say @return static. In all the cases the current instance is returned it should say @return $this.\n. @jeremeamia I'll get on that tomorrow, sleep time now. Later!\n. @jeremeamia took a little longer than expected. I have only done the ones in S3 and Glacier. The factory method in the clients seem to only supply the namespace, from which a client is resolved by convention. In these cases the return type is the classname, this because it's dependant on another class to resolve the class name rather than the class itself. So in theory even if that method was moved to another class in the same namespace, the same client would still be returned.\n. @jeremeamia / @mtdowling anything I can do for you guys concerning this issue?\n. @mtdowling no worries, IDE's thank you for merging it :dancers: \n. @jeremeamia I'm using the beta1 now, I'll try again with dev-master. Sec!\n. @jeremeamia dev-master works :+1: time for beta2 soon? ;)\n. @wyrihaximus you might want to know this too!\n. @jeremeamia side-note, setting the version to latest did give an error, while it worked in the beta. If you want I can create another issue for that.\n. I'm getting the following error:\nGuzzle\\Common\\Exception\\InvalidArgumentException:\nUnable to open /.../vendor/aws/aws-sdk-php/src/Aws/S3/Resources/s3-latest.php\nfor reading in /.../vendor/guzzle/guzzle/src/Guzzle/Service/AbstractConfigLoader.php on line 120\n. @mtdowling ah yeah, disregard that. I had the min-stability setting wrong when trying dev-master instead of 3.x@dev. Since the issue is fixed in develop, I'll close this one.\n. Just as a reason to keep it in: I found this one to be one of the strong suits of the library. Consumers will want this behaviour one way or the other, which might result in additional request for listings combined with batch deletes.\n. I'll have to dig into how those iterators work. I think I'll manage. From a user perspective I think I'm not the only one who'll want this. So having it baked into the SDK would seem like something people will like. Ultimately it's up to you of course, I can only express my wishes :dancers: \n. Come to think of it, it would allow for better testing in consumer code. Now if I'd want to test the implementation I'd have to replicate all the internal calls made to the S3Client, which requires the implementation to have extensive knowledge of what happens under the hood. This will add a maintenance load on every test suite handling this situation, is more error prone and likely to break (even in patch versions due to it's intrusive nature). Having the method there makes it far easier to mock just that one call. So if not for the functionality ... do it for the puppies! ... I mean, tests. Do it for the tests.\n. @mtdowling thank you :+1: \n. That explains it. Seeing the other PR, it might be good to update the docs on this one. They lead me to believe such an exception class should have been there. I reckon I'm not the only one.\n. Or maybe just explain that the \"error\" is and how it's represented. Either I overlooked that, or it's just not there.\n. These: http://docs.aws.amazon.com/aws-sdk-php/v3/api/Aws/S3/s3-2006-03-01.html#copyobject-errors\n. @mtdowling all good! Thanks!\n. :+1:\n. This is because of how instance resolving is setup. The class doesn't return a new instance of itself, it passes it's namespace to a resolver. In theory this method could be placed in any other class in the same namespace and it would still resolve to the same instance. So, it is indeed the same class. But it otherwise it should be self and not static like the others. Because this setup doesn't allow for extension unless you follow the same FQCN resolving and thus structure.\n. @mtdowling the class retuned by that method is neither $this or an instance of the class. It's the $this of another class. I've looked at the other decorators, which all return the current instance ($this) in these cases when overwriting. It's breaking the interface when returning something else.\n. @mtdowling  I've added the reasoning behind this below the other occurrence.\n. ",
    "henrychen95": "At first, I also though \"preg_match('/^[a-z0-9][a-z0-9-.]*[a-z0-9]?$/', $bucket)\" can avoid upper characters. But users reported they can't added bucket successfully, then I found this issue. And I also think we need make the same behave at S3 web console, so we need let users can input upper characters when they chose US Standard region.\n. ",
    "laurencei": "Ok - I've got some new information - it seems related to when I try to upload to anywhere except the US Standard bucket. I can now upload successfully on a new bucket I created on US Standard. But when I switch the region and bucket to US-WEST-2 I get:\n[curl] 56: Problem (2) in the Chunked-Encoded data [url] https://s3-us-west-2.amazonaws.com/{BUCKET_NAME}/test2.jpg\nSo for now I'll just use US Standard - it works correctly.\np.s. using CURLOPT_VERBOSE didnt show any new information in the error logs.\n. ",
    "jdgriffith": "I'm having the same issue. What is the correct method for setting pulic-read or make public on a directory upload? I am currently attempting the code below:\n$s3->uploadDirectory($directory, $this->bucket, $path, ['params' => ['ACL' => 'public-read']])\n. ",
    "werdender": "Having the same issue. In sources ACL not configuring, as I see:\n```\n    private function getS3Args($path)\n    {\n        $parts = explode('/', str_replace('s3://', '', $path), 2);\n        $args = ['Bucket' => $parts[0]];\n        if (isset($parts[1])) {\n            $args['Key'] = $parts[1];\n        }\n    return $args;\n}\n\n```\nSDK version is\nconst VERSION = '3.10.0';\n. ",
    "aik099": "Dependencies are checked properly in Guzzle's composer.json file (see https://github.com/guzzle/guzzle/blob/master/composer.json), which AWS SDK PHP depends on. Maybe you're installing AWS SDK PHP not through Composer and in that case there is no helper script to check dependencies for you.\n. Interesting. Maybe we can suggest a web-based dependency check to be added to Composer. This way if CLI dependencies are met, then you can optionally check web dependencies as well.\n. Nothing is outputted on the screen (except file/folder list, that Finder generates). Adding debug logging doesn't help either. I even don't know if a credentials for AWS I've specified during S3 client creation were correct. Totally no errors or exceptions of any kind.\n. > Are you sure that the finder is yielding files?\nSure. This was first thing I've checked. It's showing files and maybe dirs as well.\n\nYou can force and upload and not use the ChangedFilesIterator by calling force(true) on the builder. Does forcing the upload actually transfer files?\n\nI know, but the point was exactly to do a clever upload of only missing files. That's why I specifically omitted force option.\n\nThat is correct. We don't currently have plans to add support for deleting files that weren't found in the sync. This issue is a good example of why we made that decision: if we were to delete files that didn't match your iterator, we would have deleted your entire bucket.\n\nNot necessarily. This is currently implemented in s3cmd (see http://s3tools.org/s3cmd-sync). It does it somehow, but I wanted to use PHP-based solution to make my all my application parts use same library for interacting with AWS. Like rsync for S3.\n. > I'm not saying that you should run it like that forever, but I was hoping you could give a shot and let me know if it still doesn't upload anything. I'm just trying to eliminate variables so that this can be debugged.\nYes, with force option it does something for a long time. At the end I've got this:\nFatal Error: Uncaught exception 'Guzzle\\Service\\Exception\\CommandTransferException' with message 'Errors during multi transfer (Aws\\S3\\Exception\\RequestTimeoutException) Your socket connection to the server was not read from or written to within the timeout period. Idle connections will be closed. (Aws\\S3\\Exception\\RequestTimeoutException) Your socket connection to the server was not read from or written to within the timeout period. Idle connections will be closed.' in /project/vendor/guzzle/guzzle/src/Guzzle/Service/Exception/CommandTransferException.php:25 Stack trace: #0 /project/vendor/guzzle/guzzle/src/Guzzle/Service/Client.php(236): Guzzle\\Service\\Exception\\CommandTransferException::fromMultiTransferException(Object(Guzzle\\Http\\Exception\\MultiTransferException)) #1 /project/vendor/guzzle/guzzle/src/Guzzle/Service/Client.php(140): Guzzle\\Service\\Client->executeMultiple(Array) #2 /project/vendor/aws/aws-sdk-php/src in /project/vendor/guzzle/guzzle/src/Guzzle/Service/Exception/CommandTransferException.php on line 25\n\nIf you'd like an alternative official AWS mechanism for syncing with Amazon S3, then I recommend you check out the CLI: https://github.com/aws/aws-cli.\n\nDoes AWS CLI provide more intelligent rsync algorithm for s3, then AWS PHP SDK?\n. Findings about Finder:\n1. It returns directories as well as files and this makes ChangedFilesInterator to actually compare directories by file rules and return that nothing is changed. No solution yet, maybe there is something unusual about SplFileInfo objects, that Finder is returning.\n2. If ChangedFilesInterator isn't used (e.g. when force option is set), then due mixed directory/file list from Finder iterator we get timeout exception during transfer, because of recursive directory upload attempt using file upload rules. Adding ->files() to Finder class object solves this timeout problem.\n\nHave you tried just using the uploadDirectory method of the S3Client class? This works for me every time and should cover most used cases.\n\nIt worked almost instantly (but it have uploaded \".svn\" sub-folders as well). That's why I was using Finder, because there is no native way to exclude files/folders from being uploaded). This makes me thinking, that empty sub-folders might not be uploading at all using uploadDirectory method. By the way, one more look at ChangedFilesInterator class revealed that it does comparison by same rules as rsync does (file size and change time), which is amazing.\n\nIf your question is if the CLI deletes remote files, then yes. As far as how flexible it is, you have much more flexibility and extensibility with the PHP SDK. If you don't need this flexibility and you are able to install the CLI, then I think it's a great option.\n\nSo is that AWS CLI package a library of it's own or it's just an example of usage of AWS PHP SDK in CLI? And why AWS CLI understands how to delete files during rsync, but AWS PHP SDK doesn't? Maybe it's possible to add that \"delete process code\" to AWS PHP SDK as well?\n. I've found this file (https://github.com/aws/aws-cli/blob/develop/awscli/customizations/s3/comparator.py), which seem to be the core logic that determines what to sync. It seems, that if we'd had 2 iterators (one from s3 and other from local fs), then processing them using logic from that class might actually do the trick.\nThere might be some problems (for me at least) to figure out how keep all that concurrency stuff working when adding/deleting files to S3. Because ending up with a large file set to upload/delete kind of defeats the purpose of using iterators in first place.\nAlso comments in that file states, that file lists on both fs (s3 and local) needs to be specifically sorted to allow correct processing. I'm not sure that s3 does sort files returned by ls or similar command.\n. Great.\nSo, would it be possible to add an option (disabled by default) to enable this file deletion on target directory (this is s3, when synced from local to s3; this is local directory, when synced from s3 to local)?\nIf I understood correctly, then both s3/local fs iterators don't sort the produced file list in any way. Without it it's pretty hard to implement the correct deletion.\n. Let me explain the whole idea, why I need file deletion:\n1. I have S3 bucket, that acts as source for CloudFront CDN\n2. when users uploads a video on a website, then I (asynchronously) upload it to S3 (check is made every minute) for it to be available on CDN faster\n3. once in 5 minutes I do an rsync from local video folder to S3 (just in case)\nWhat concerns me is that if it happens, that during large video upload (e.g. via PutObject) the rsync kicks in and will try to upload same video in parallel. From S3 docs I've found out, that in parallel request case to same object the later made request wins and no data corruption happens. Though one of (PutObject or rsync) will get an error back I guess. Also a new video maybe uploaded locally while rsync is in progress.\nThat's why I wanted to use same AWS SDK to somehow ensure that both video transfers fails.\n. > Deletion is only useful for cleaning up files remotely that aren't present locally.\nGreat. What if I will be checking if a file exists on S3 while in parallel it's being uploaded using multi-part upload. Would \"file_exists\" check pass on that file or it's not visible on S3 until it's uploaded at 100%?\n\nI'll go ahead and close this issue as you should be able to upload files using the Symfony Finder component now. This modification will be available in the next tagged release.\n\nI've tested by adding @dev to composer.json and all worked perfectly.\n. ",
    "dotancohen": "Thank you. In fact, I did install on my Kubuntu desktop with Composer and on a webserver by wget'ing the .zip file. Only on the webserver (non-composer install) did I have the issue. I suppose that if you are pushing Composer as the 'official' way to install the ASW SDK then there is no need to support 'unofficial' installs such as the .zip file.\nNote that newly-installed PHP modules are available on the CLI, but not in Apache until Apache is restarted (at least on Ubuntu Server and similar variants). That might be an issue if installing via Composer where all is ostensibly fine yet the website being developed \"does not work\" so giving an error message \"Please install 'php5-curl' or restart your webserver\" may still be in order.\nThank you for your attention to the matter.\n. Thank you jeremeamia. Unaware of that file I copied the ~/Aws directory only to my ~/Classes directory and then started to complain! I suppose that I should have been more thorough and examined the remaining contents of the .zip.\nOf course, the issue of newly-installed PHP modules being available only on the CLI remains.\n. Actually, grepping the code for CURLE_COULDNT_RESOLVE_HOST I see that this is an upstream Guzzle issue, not specific to AWS SDK.\n. Right, thanks! I guess that covers all the concerns and this issue can be closed. Thank you for the information and thank you for the useful SDK.\n. ",
    "mattheu": "Ah oops I did PR against aws:master.\nIt is a bug fix though!\n. ",
    "danielbachhuber": "@mattheu We're currently using an older version of Guzzle where Guzzle\\Service\\Client had $args = null: https://github.com/humanmade/guzzle/blob/ad64f76d344d1044af82662b4a4f02606098669c/src/Guzzle/Service/Client.php#L88\nIt looks like master fixes this, although we'll need to test for regressions before we can update.\n. ",
    "coredumperror": "Wow, that's just what I needed! Now that's what I call customer service!\n. After discussing this with my boss, I decided to add an addition check to the seek operation. Now, attempting to seek more than 50 megabytes into a file will throw an exception, since that would require reading 50+ megs into RAM. Since Drupal is already quite heavy, an additional 50 megs seems like a good limit.\nThe check will probably never trigger, but throwing a descriptive error message is better than letting a RAM shortage crash the request.\n. Thank you for the info about the PHP temp stream. I was totally unaware of that.\nWith your confirmation that my idea is decent, I'm going to go ahead and release S3 File System v1.0. Thanks again for the input!\n. Nope, this doesn't seem to be happening any more, using v2.6.3. That's great!\n. What could be causing those invalid parameters to be sent in, though? I'm calling into Guzzle in the same way that the SDK's StreamWrapper does.\nUnfortunately, I haven't been able to reproduce the issue ever since I re-installed my Drupal site to try to make the bug go away. But knowing that the parameters must have somehow gotten corrupted means I'll have somewhere to start from if it happens again.\n. We managed to reproduce the problem again, and the parameters are totally valid. stream_context_create() is being called like this:\nstream_context_create(array(), array('stream_class' => 'Guzzle\\Http\\EntityBody')\nThat should be completely valid, but somehow stream_context_create() is returning False on my boss's machine. But it returns a valid resource on my machine using the exact same parameters. We're using the same version of PHP, as well: 5.3.29.\nDo you guys have any idea how that could be happening? Some kind of configuration difference, perhaps?\n. Hmmmm... that could be it. I'll have to check with him to see if our php memory limit settings differ. \nThanks for the tip!\n. Thanks for the very quick reply! Perhaps the API documentation could make the reason for the lack of waiter docs a little more explicit? It's an understandable exclusion, but not informing your users why it's excluded leads to this this exact conversation.\n. That looks very promising. Thank you for the lightning fast response!\n. Great, thanks!\n. Glad to hear it! Thankfully this isn't a blocker (just makes debugging more annoying), so I can wait for the full release.\n. ",
    "adlawson": "@jeremeamia awesome :+1: \n. ",
    "misbach": "Thanks @mtdowling \n. ",
    "saurabh-diffion": "Please note that in my test case the sender's domain was verified but not the email address. When I send from a non-verified domain, I get the error message.\n. ",
    "Supacoco": "Hello jeremeamia thanks for your reply.\nPlease find below the piece of code I use to instantiate the SQS Client.\n``` php\nrequire 'vendor/autoload.php';\nuse Aws\\Sqs\\SqsClient;\n$config = array(\n  'key'    => 'my-key',\n  'secret' => 'my-secret',\n  'region' => 'us-west-2',\n  'curl.options' => array(\n    CURLOPT_PROXY => '10.10.10.10:1337',\n    )\n  );\n$sqs = SqsClient::factory($config);\n```\nAnd then I try to create a queue:\nphp\n$result = $sqs->createQueue(array('QueueName' => 'test-queue'));\nprint_r($result->get('QueueUrl');\n. Hello jeremeamia,\nSorry for the lack of feedback, I wasn't able to solve this issue without being at the office.\nI'm now using the lastest version of the SDK. I also switched to @mtdowling way to declare proxy configuration.\nUnfortunately, I still have the forbidden error. That might point out that this issue stem from the firm's proxy.\nI'm waiting for a network administrator to check what the proxy sends out and then compare both requests.\nI'll keep you posted.\nThanks a lot.\n. You were right, the account I was using wasn't allowed to access SQS.\nThanks a lot for your time and patience :)\n. ",
    "muhammad-sannan-confiz": "I able to access sqs console from the browser, is there any thing extra that I am missing to access the sqs. \nM trying to push a message from my lambda to sqs. Here is the exception. \nPS  IAM policy that m using has the access right on lambda for sqs. \n{\n  \"errorMessage\": \"Access to the resource https://sqs.us-west-2.amazonaws.com/ is denied. (Service: AmazonSQS; Status Code: 403; Error Code: AccessDenied; Request ID: c61c0698-505c-50fe-8c67-5602cb84e824)\",\n  \"errorType\": \"com.amazonaws.services.sqs.model.AmazonSQSException\",\n  \"stackTrace\": [\n    \"com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleErrorResponse(AmazonHttpClient.java:1543)\",\n    \"com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1181)\",\n    \"com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:962)\",\n    \"com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:675)\",\n    \"com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:649)\",\n    \"com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:632)\",\n    \"com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$300(AmazonHttpClient.java:600)\",\n    \"com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:582)\",\n    \"com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:446)\",\n    \"com.amazonaws.services.sqs.AmazonSQSClient.doInvoke(AmazonSQSClient.java:1504)\",\n    \"com.amazonaws.services.sqs.AmazonSQSClient.invoke(AmazonSQSClient.java:1480)\",\n    \"com.amazonaws.services.sqs.AmazonSQSClient.listQueues(AmazonSQSClient.java:1010)\",\n    \"com.amazonaws.services.sqs.AmazonSQSClient.listQueues(AmazonSQSClient.java:1022)\",\n    \"S3EventHandler.handleRequest(S3EventHandler.java:76)\",\n    \"S3EventHandler.handleRequest(S3EventHandler.java:28)\"\n  ]\n} \n. hi, \nI found out the issue, IAM policy that i was using has access to only east region where as my sqs was on west. So  before using  sqs from lambda make sure you have correct policy attached.\n. ",
    "muhqu": "Thanks for this detailed response! \nSo you think that a code 18 is safe to retry at guzzle level, e.g. for any request that will be made throu it? \n[curl] 18: GnuTLS recv error (-9): A TLS packet with unexpected length was received.\nHere is what the CURL docs say about this error code:\n\nCURLE_PARTIAL_FILE (18)\nA file transfer was shorter or larger than expected. This happens when the server first reports an expected transfer size, and then delivers data that doesn't match the previously given size.\n\nI'm not so sure that this error should be considered retry-able in any case. If for example the error happened while making  PUT, POST or DELETE requests, it is not clear whether or not the request was received and handled by the web server or not. In this very example, using S3 listObjects, it's clearly a retry-able case, but I would not say in any case... \nIn my opinion this should be handled thru some CurlSafeToRetryWhenReadOnlyBackoffStrategy e.g.  error codes that are safe to retry when making \"read only\" requests. But as it is domain specific whether a request is considerable read-only or not, it should not be a default backoff strategy. Rather the S3Client should add this special read-only backoff strategy to all requests/commands it can consider read-only. e.g. list_, get_ etc.\nWhat do you think?\n. > The BackoffPlugin doesn't work well in situations where a request is not idempotent. This shouldn't be an issue with any AWS services...\nWell, I would not say that it's not an issue, to blindly retry if you're not 100% sure the request has not come throu and has already been handled by the server. Situations where I would consider this to be an issue are:\n- DynamoDB UpdateItem request with the action ADD to increment some counter\n  In this case you might end-up incrementing the counter twice without knowing.\n- DynamoDB UpdateItem request with ReturnValues: ALL_OLD or UPDATED_OLD\n  In this case you might come to false conclusions because you don't know that your update has been applied twice and therefor the old attributes you retrieve are not the right ones... \nIf my assumptions are correct, I strongly encourage to not handle this type of error at the transport layer (guzzle) in general. Only if the request can be declared as idempotent it should be handled by the BackoffPlugin. I'm sure this will require the SDK to know for every single operation type whether it can be considered idempotent or not.\nBtw, the EC2 API especially offers a client-token parameter which allows the user to make the otherwise not RunInstances, TerminateInstances [,..] requests explicitly idempotent. See Ensuring Idempotency in the EC2 User Guide.\nI wonder how this type of error is handled by any of the other AWS SDKs ( e.g. Java, .NET, Ruby, ... ).\n. @mtdowling: Thanks for keeping track of this!\n. ",
    "peteruk08": "A workaround for this is to use the added callback in Issue #135 and manually use setOption() on the builder object to set the Storage Class and metadata.  While this works, it seems like this should be done anyway as when uploadDirectory (or Sync) is called with those parameters anyway.\n. Found another one, it doesn't seem possible to upload this SWF even with a simple putObject(), the outcome is always \"The Content-MD5 you specified did not match what we received.\"\nhttp://cdn.blinkx.com/stream/b/37/Blip/20120617/1845998477/1845998477_swf_0.swf\n. Also sorry, I suppose this report should really be in the Guzzle repo?\n. Just to clarify we're on the same page here, I'm saying I grabbed the latest copy of the AWS zip and applied the change https://github.com/guzzle/guzzle3/commit/42ea3b0447817eaf203ceb62eb10c96eb9fab2a2  - This fixed the issue reported in #283 but seems to cause the problem I am describing in this issue.  You're saying there are other changes within the upcoming Guzzle release that cause this issue to not occur?\n. Ah excellent, thanks. In that case I'll close this issue. Can always repost if the problem continues. \n. Thanks very much, I'll give it a test at work tomorrow.  Keep up the good work!\n. ",
    "greggilbert": "It's the basic data block in CloudSearch. Basically I think the SDK should support being able to send in data.\n. Ha, that's awesome. Thanks!\n. ",
    "gregholland": "I'd love to see CloudSearch document and search supported by the SDK. Although it's easy enough to roll your own using guzzle, some helpers for building up queries and a nicer way to handle errors would be great. (guzzle returns 400 bad request without the actual error from CS making trouble shooting a pain)\nI'd also love to use my keys to make search/document requests rather than relying on i.p restriction policies, but I'm assuming that is more of an issue for the CloudSearch team and is possibly the main reason why there is no search/document functionality in the SDK. \n. Good stuff, thanks! Do you know if the CS team plan on implementing IAM for the search and document endpoints?\n. I finally upgraded from v2 to v3 of the SDK a couple of weeks ago and have been seeing lots of these errors. For me they only occur on query requests to DynamoDB using Aws\\ResultPaginator.\nError executing \"Query\" on \"https://dynamodb.us-east-1.amazonaws.com\"; AWS HTTP error: cURL error 56: SSL read: error:00000000:lib(0):func(0):reason(0), errno 104 (see http://curl.haxx.se/libcurl/c/libcurl-errors.html) Unable to parse error information from response - Error parsing JSON: Control character error, possibly incorrectly encoded\nI've read this entire thread but still have no idea how to go about solving it. Has anyone had any luck getting to the bottom of this? If so, care to share?\nPHP: 7.1.10\nOpenSSL: 1.0.1t\nCurl: 7.38.0\nGuzzle: 6.3.0\nAWS Services: DynamoDB (Specifically Query using the ResultPaginator)\nThanks!. Also occurring with the following setup:\nPHP: 7.2.0\nOpenSSL: 1.1.0f\nCurl: 7.52.1\nGuzzle: 6.3.0\nAWS Service: DynamoDB only using the ResultPaginator.\n. ",
    "paulstatezny": "Completely agree with @gregholland. Both features would be incredibly helpful.\n. ",
    "craiga": "Version 2.4.12 of the SDK, 5.3.3.7 of PHP, APC is enabled.\n```\n$ php -v \nPHP 5.3.3-7+squeeze18 with Suhosin-Patch (cli) (built: Dec 12 2013 09:20:04) \nCopyright (c) 1997-2009 The PHP Group\nZend Engine v2.3.0, Copyright (c) 1998-2010 Zend Technologies\n    with Xdebug v2.1.0, Copyright (c) 2002-2010, by Derick Rethans\n    with Suhosin v0.9.32.1, Copyright (c) 2007-2010, by SektionEins GmbH\n$ php --info | grep -i apc\nAdditional .ini files parsed => /etc/php5/cli/conf.d/apc.ini,\napc\nAPC Support => enabled\nAPC Debugging => Disabled\napc.cache_by_default => On => On\napc.canonicalize => On => On\napc.coredump_unmap => Off => Off\napc.enable_cli => On => On\napc.enabled => On => On\napc.file_md5 => Off => Off\napc.file_update_protection => 2 => 2\napc.filters => no value => no value\napc.gc_ttl => 3600 => 3600\napc.include_once_override => Off => Off\napc.lazy_classes => Off => Off\napc.lazy_functions => Off => Off\napc.max_file_size => 1M => 1M\napc.mmap_file_mask => no value => no value\napc.num_files_hint => 1000 => 1000\napc.preload_path => no value => no value\napc.report_autofilter => Off => Off\napc.rfc1867 => Off => Off\napc.rfc1867_freq => 0 => 0\napc.rfc1867_name => APC_UPLOAD_PROGRESS => APC_UPLOAD_PROGRESS\napc.rfc1867_prefix => upload_ => upload_\napc.rfc1867_ttl => 3600 => 3600\napc.serializer => default => default\napc.shm_segments => 1 => 1\napc.shm_size => 32M => 32M\napc.slam_defense => On => On\napc.stat => On => On\napc.stat_ctime => Off => Off\napc.ttl => 0 => 0\napc.use_request_time => On => On\napc.user_entries_hint => 4096 => 4096\napc.user_ttl => 0 => 0\napc.write_lock => On => On\nsuhosin.apc_bug_workaround => Off => Off\n```\n. OK, that fixed it for my test script, but disabling APC isn't a viable solution for our production environment.\nI found that setting apc.filters to stop APC caching files inside PHAR archives fixed the issue for me:\napc.filters = \"^phar://\"\n. Cool, but I was seeing this error when running PHP under Apache as well as directly from the command line.\n. ",
    "GrahamCampbell": "Moving to guzzle 4 when it get's released would be cool. :P\n. Sweet!\n. I'll split this into two pulls. One for cs fixes, one for hhvm in travis.\n. Hmmm. It seems someone else has alleady applied my cs fixes.\n. It's interesting how php 5.4/5.5 tests now fail with \"ReflectionException: Class Mock_SplFileInfo_xxxxxxxx is an internal class that cannot be instantiated without invoking its constructor\". Also, hhvm fails with \"Could not use \"PHPUnit_Runner_StandardTestSuiteLoader\" as loader\".\n. This is a bit of a mess really. I think an idea for 2.6 would be to ditch php 5.3 support, and try and get hhvm to pass on phpunit 3.8.\n. The SDK is clearly not ready for hhvm or phpunit 3.8. I will close this.\n. PHPUnit 3.8 is \"beta\". It will be released on March 7th as stable.\n. I usually do it before, but the way packagist works, it really makes no difference. Branch aliasing is not applied to individual states in the git history, it is applied to the entire branch through all states, so whatever you last set is applied to the entire branch, regardless of what was there before, so just do whatever you feel like. I don't think I explained that very well...\n. Version 3 of this sdk uses guzzle 5.\n. @cdnsteve. If you want have 5.3 and upwards use the current aws sdk. If you have php 5.4 and upwards, you'll be able to use the new aws sdk (when it comes out). It certainly doesn't require php 5.4.x - that would be absurd - it only requires you to be using at least php 5.4. Both the current, and upcoming aws sdks will run fine on php 5.5.\n. @mtdowling Will this be guzzle 5 now then?\n. :-1: On this. There's no point. We should start testing on hhvm on the next major version of the sdk.\n. There's no point in wasting travis resources on a test suite that won't even run.\n. Cool. :)\n. The 3.0 beta tag appeared in my notifications. :)\nI'm looking forward to checking this stuff out.\n. Why not just 5.4+?\n. k\n. ~3.0@dev. :P\n. Do we really need to have a \"waiters2\" format? Could we not just change the existing one given that 3.0 is still a wip.\n. Ok. :)\n. AWSome. :)\n. I wonder if all the client classes should be moved to their own namespace too?\n. k\n. Me too! :)\n. ...................EEEE.......EE..E............................ 504 / 874 ( 57%)\n......FF.FFFF.......EE...EE..............................Trying to @cover or @use not existing class or interface \"Aws\\S3\\ApplyMd5Subscriber\".\nmake: *** [travis] Error 2\n. Sure. Though, refactoring with failing tests is a \"refuckter\", lol.\n. AWSome. :P\n. :shipit:\n. Ok, and the dependencies are installable now. :)\n. @mtdowling Similar to ~, only follows semantic versioning.\nSome examples for you:\n- ^0.1 would translate to 0.1.*,>=0.1.0\n- ^0.1.4 would translate to 0.1.*,>=0.1.4\n- ^1.0 would translate to 1.*,>=1.0.0\n- ^1.1 would translate to 1.*,>=1.1.0\n- ^1.4.2 would translate to 1.*,>=1.4.2\n\"would translate to\" isn't quite how they do it, but i've made it easy to read. :)\n. @hacfi I know - that's why I said \"isn't quite how they do it, but i've made it easy to read\".\n. Probably makes sense.\n. https://github.com/aws/aws-sdk-php/tree/v3\n. Also, moving to guzzle 5 is non-trivial for the aws sdk.\n. As I understand, the plan was to have it ready in May - it's nothing to do with me though. :)\nPing @jeremeamia.\n. Can be done by just running composer require on travis, getting the version from an environment variable.\n. We used to do this in flysystem to require deps for php that weren't installable on hhvm due to unmet extension requirements. :)\n. :rocket: :ship: :sunrise: \n. :-1: composer has supported this for over 6 months now.\n. I'd have to check the source to be 100% sure.\n. ?\n. :+1:\n. Though, the version of php 7 travis use isn't the beta release, so it still might break since it's just built from the master...\n. :-1: \n. You probably shouldn't be trying to mock this anyway.\n. > I think you should leave the array typehint on $args and add a test case that makes sure the class is mockable.\nYeh. The typehint should stay.\n. Loads JSON files and compiles them into PHP arrays\nMissing fullstop.\n. Could you provide more information please. \"it crashes\" is impossible to debug. :)\n. PHP version, sdk vesion, error logs, any other context information that might be useful.\n. Is your php version in cli the same as the one your using through fpm?\n. :+1:\n. Exactly. The 2.8 branch is deprecated too tbh.\n. TLDR is nobody should be starting a new app using anything less than php 5.5.\n. I think he means the 2.8 of the aws sdk, not guzzle?\n. > I think you are misunderstanding my point here, I just wanted guzzle/guzzle to be renamed to guzzle/guzzlehttp ( no version change).\nThat's not possible. v3 in BOTH libraries is still deprecated anyway.\n. The latest version of guzzle IS used.\n. Not sure why. Ping @jeremeamia @mtdowling @jeskew.\n. Meh, will create a new PR. CBA to clone the repo. Can do this from the web interface.\n. There you go: https://github.com/aws/aws-sdk-php/pull/755.\n. I thought travis removed hhvm-nightly support?\n. php 5.5+ is required for sdk v3\n. For future reference, most projects define the minimum php version in their composer.json file: https://github.com/aws/aws-sdk-php/blob/master/composer.json#L19. ;)\n. Don't we need to add this to \"suggest\"?\n. Are you using process isolation in phpunit? That's known to cause issues that look exactly like that if a test errors.\n. > This is a very old one and should be fixed someday:\nYou need to upgrade to aws sdk v3.\n. > I refer here to the composer.json of the current master branch of this repo.\nYou can't. Look at it. There are no guzzle/ packages in there?\n. > guzzlehttp has been abandoned and is now guzzle/guzzle. Therefore I almost always get this warning when installing/updating via composer.\nI'm, pretty sure you are wrong here. guzzlehttp/guzzle is the new package.\n. Did you make sure you ran php artisan config:cache after making changes to the env/config files?\n. You can probably setup guzzle 6's retry middleware.\n. :+1:\n. You need to upgrade to aws sdk v3.\n. Ping @mtdowling.\n. You don;t need to check if a class exists. PHP won;t try to load it.\n. \ud83d\udc4e See my above comment. Just check instanceof Throwable and Exception. No need for the conditional logic.\n. \ud83d\udc4d \n. You should not try to use 3.x tags on guzzlehttp/guzzle since the idea of having two packages is that you can install them both at once, so if you want 3.x, you really do have to use the deprecated package. TBH, I'm not sure why the 3.x tags arn't deleted off the new repo? // cc @mtdowling \n. To ensure we don't hit github api limits when downloading dependencies. Using --prefer-dist, which is the composer default means we are hitting the github api on each download. Using --prefer-source doesn't hit the api, it clones the repos we need.\n. lol\n. you don't need brackets around __DIR__\n. ?\n. It tells travis to use the new container architecture which is way faster. Old repos don't have it enabled by default on travis, so you must include it for now if you want the container architecture.\n. ~5.3||^6.0.1 wouldn't avoid it, and would result in a parse error.\n. ~5.3||~6.0.1||~6.1 would work fine.\n. The one you get if you use a really old verion of composer...\n. Well, i can't remember the exact error off the top of my head. If you're interested try it out I guess.\n. ?\n. Ah right. :)\n. the return null; is uneeded isn't it?\n. Just a micro-optimization. It's faster not to call emtpy isn't it?\n. this statement isn't needed\n. Using (double) $value is faster and is identical in terms of the casting behaviour.\n. (int) $value, etc\n. Or rather, (float) $value.\n. cs\n. :+1: @jeremeamia \n. result not results\n. there is no test coverage for when is_array returns false\n. what if it's expected twice so the parent of the parent is Aws\\AwsClient?\n. you really need a loop here\n. The package annotation shouldn't be used. It's only there to replace namespaces in php 5.2, and you've missmatched it with the namespace here anyway.\n. this phpdoc adds no value\n. Maybe use assertSame otherwise things like 0 and null pass this assertion.\n. Calling instanceof on a non-exitence class is fine. PHP won;t try to autoload it because it's clever enough to see that it's clearly false if the definition hasn't even been loaded yet.\n. ",
    "carltondickson": "Yes this seems to work for the download issue, thanks for that!\nI think I have noticed a separate issue with the KeyConverter though (this is an issue in the current version by the way, not just this pull request) regarding downloading existing files.\nUnless the line I have included below is at the end of the convert function I get $key values such as \"C:/xampp/htdocs/project_name/app/data\\csv_filename.csv\", notice the backslash...which causes a problem in ChangedFilesIterator->getTargetData as that value doesn't match \"C:/xampp/htdocs/project_name/app/data/csv_filename.csv\" so it always downloads the file.\n// Replace Windows directory separators to become Unix style, and convert that to the custom dir separator\n$key = str_replace('/', $this->delimiter, str_replace('\\\\', '/', $key));\nShall I create a new ticket for this or is this a case where we should be using setSourceFilenameConverter to create our converters?\n. I'll email a screenie to better explain the above comment about KeyConverter issue\n. I have remove the code for downloadSync and went for reading the bucket with the dir command so that I can select particular files, I think sync wasn't the right approach to take in my case.\nAs for those setting, I can't be 100% sure but delimiter was the default value...\nbaseDir was something like \"daily_report_v4/\"\nprefix was something like \"CompanyName_V3\"\nMy object on s3 would be something like \"s3//bucketname/daily_report_v4/CompanyName_V3-Jan-1-2014-ReportName.csv\"\nIf that doesn't help let me know and I'll put together a quick example in github and get the exact values\n. ",
    "bikegriffith": "I'm still futzing with curl options to see what works.  The CURLOPT_SSLVERSION=>1 doesn't actually seem work (my bad), so I've removed it.  And it's not just erroring on putObject, but rather for many other calls.\nWith curl debugging on, here's what I'm seeing:\nFirst call works OK:\n```\n[3] > $s3->putObject(['Bucket'=>'DSITWConfig', 'Key'=>'TESTOH/foo', 'Body'=>'test']);\n About to connect() to s3.amazonaws.com port 443 (#0)\n   Trying 72.21.195.65... * Connected to s3.amazonaws.com (72.21.195.65) port 443 (#0)\n Initializing NSS with certpath: sql:/etc/pki/nssdb\n   CAfile: /home/mike/src/tests3/vendor/guzzle/guzzle/src/Guzzle/Http/Resources/cacert.pem\n  CApath: none\n SSL connection using SSL_RSA_WITH_RC4_128_SHA\n Server certificate:\n       subject: CN=s3.amazonaws.com,O=Amazon.com Inc.,L=Seattle,ST=Washington,C=US\n       start date: Sep 09 00:00:00 2013 GMT\n       expire date: Sep 10 23:59:59 2014 GMT\n       common name: s3.amazonaws.com\n*       issuer: CN=VeriSign Class 3 Secure Server CA - G3,OU=Terms of use at https://www.verisign.com/rpa (c)10,OU=VeriSign Trust Network,O=\"VeriSign, Inc.\",C=US\n\nPUT /DSITWConfig/TESTOH/foo HTTP/1.1\nHost: s3.amazonaws.com\nUser-Agent: aws-sdk-php2/2.4.12 Guzzle/3.7.4 curl/7.19.7 PHP/5.4.23\nContent-MD5: CY9rzUYh03PK3k6DJie09g==\nDate: Wed, 08 Jan 2014 17:14:52 +0000\nAuthorization: AWS (redacted)\nContent-Length: 4\n\n< HTTP/1.1 200 OK\n< x-amz-id-2: (redacted)\n< x-amz-request-id: BE71F0D8CE0243BC\n< Date: Wed, 08 Jan 2014 17:15:04 GMT\n< ETag: \"098f6bcd4621d373cade4e832627b4f6\"\n< Content-Length: 0\n< Server: AmazonS3\n<\n* Connection #0 to host s3.amazonaws.com left intact\nRequest:\nPUT /DSITWConfig/TESTOH/foo HTTP/1.1\nHost: s3.amazonaws.com\nUser-Agent: aws-sdk-php2/2.4.12 Guzzle/3.7.4 curl/7.19.7 PHP/5.4.23\nContent-MD5: CY9rzUYh03PK3k6DJie09g==\nDate: Wed, 08 Jan 2014 17:14:52 +0000\nAuthorization: AWS (redacted)\nContent-Length: 4\ntest\nResponse:\nHTTP/1.1 200 OK\nx-amz-id-2: (redacted)\nx-amz-request-id: BE71F0D8CE0243BC\nDate: Wed, 08 Jan 2014 17:15:04 GMT\nETag: \"098f6bcd4621d373cade4e832627b4f6\"\nContent-Length: 0\nServer: AmazonS3\nErrors: 0\n\u00e2 object(Guzzle\\Service\\Resource\\Model)(\n)\n[4] >\n```\nThen the next call fails (even if it's the exact same call):\n```\n[6] > $s3->listObjects(['Bucket' => 'DSITWConfig']);\n About to connect() to s3.amazonaws.com port 443 (#0)\n   Trying 72.21.195.65... * Connected to s3.amazonaws.com (72.21.195.65) port 443 (#0)\n   CAfile: /home/mike/src/tests3/vendor/guzzle/guzzle/src/Guzzle/Http/Resources/cacert.pem\n  CApath: none\n NSS error -8023\n Expire cleared\n Closing connection #0\n About to connect() to s3.amazonaws.com port 443 (#0)\n   Trying 72.21.195.65... * Connected to s3.amazonaws.com (72.21.195.65) port 443 (#0)\n   CAfile: /home/mike/src/tests3/vendor/guzzle/guzzle/src/Guzzle/Http/Resources/cacert.pem\n  CApath: none\n NSS error -8023\n Expire cleared\n Closing connection #0\n About to connect() to s3.amazonaws.com port 443 (#0)\n   Trying 72.21.195.65... * Connected to s3.amazonaws.com (72.21.195.65) port 443 (#0)\n   CAfile: /home/mike/src/tests3/vendor/guzzle/guzzle/src/Guzzle/Http/Resources/cacert.pem\n  CApath: none\n NSS error -8023\n Expire cleared\n Closing connection #0\n About to connect() to s3.amazonaws.com port 443 (#0)\n   Trying 72.21.195.65... * Connected to s3.amazonaws.com (72.21.195.65) port 443 (#0)\n   CAfile: /home/mike/src/tests3/vendor/guzzle/guzzle/src/Guzzle/Http/Resources/cacert.pem\n  CApath: none\n NSS error -8023\n Expire cleared\n* Closing connection #0\nPHP Fatal error:  Uncaught exception 'Guzzle\\Http\\Exception\\CurlException' with message '[curl] 35:  [url] https://s3.amazonaws.com/DSITWConfig' in /home/mike/src/tests3/vendor/guzzle/guzzle/src/Guzzle/Http/Curl/CurlMulti.php:338\nStack trace:\n0 /home/mike/src/tests3/vendor/guzzle/guzzle/src/Guzzle/Http/Curl/CurlMulti.php(279): Guzzle\\Http\\Curl\\CurlMulti->isCurlException(Object(Guzzle\\Http\\Message\\Request), Object(Guzzle\\Http\\Curl\\CurlHandle), Array)\n1 /home/mike/src/tests3/vendor/guzzle/guzzle/src/Guzzle/Http/Curl/CurlMulti.php(244): Guzzle\\Http\\Curl\\CurlMulti->processResponse(Object(Guzzle\\Http\\Message\\Request), Object(Guzzle\\Http\\Curl\\CurlHandle), Array)\n2 /home/mike/src/tests3/vendor/guzzle/guzzle/src/Guzzle/Http/Curl/CurlMulti.php(227): Guzzle\\Http\\Curl\\CurlMulti->processMessages()\n3 /home/mike/src/tests3/vendor/guzzle/guzzle/src/Guzzle/Http/Curl/CurlMulti.php(211): Guzzle\\Http\\Curl\\CurlMulti->executeHandles()\n4 /home/mike/src/tests3/vendor/guzzle/guzzle/src/Guzzle/Http/Curl/CurlMulti.php in /home/mike/src/tests3/vendor/aws/aws-sdk-php/src/Aws/Common/Client/AbstractClient.php on line 286\n```\nI attempted to reconnect before making a 2nd call and that doesn't help.\n. Found an article suggesting that fix is to recompile CURL against NSS instead of openssl.\nhttp://www.sebdangerfield.me.uk/2012/10/nss-error-8023-using-aws-sdk-for-php/\nIs that really a solution?  I haven't tried yet.\n. @mtdowling Yes, I did read that wrong.  I edited the original issue to include the output of curl --version, which shows I'm compiled against NSS.  After more hacking around all afternoon (upgrading NSS, trying different versions of Guzzle, every curlopt in the book, etc.) I think I might have to go that route, which I'm really really not excited about :-/\n. . o O ( wondering out loud if Guzzle's persistent connections have anything to do with this )\n. Latest findings (after rebuilding curl and libcurl from the source RHEL rpms) is that this might just be an artifact of trying to run commands in Laravel's php artisan tinker shell.   Stuffing the same 5 lines in a TestCase allows them to run successfully with no other hijinks.\nThis could be the fault of Boris.\n. ",
    "varunwy": "I am getting the same error and I have some idea on what's going on here.\nBelow are the steps to reproduce this problem while using PHP aws sdk.\n1. make an aws api call\n2. use pcntl to fork a child process\n3. In the child process make another aws api call\nYou will get error:\nAws\\Common\\Exception\\TransferException: [curl] 35: A PKCS #11 module returned CKR_DEVICE_ERROR, indicating that a problem has occurred with the token or slot.\nThis problem will happen only if you use curl with NSS. Curl with Openssl works fine.\nAccording to this discussion (https://bugzilla.mozilla.org/show_bug.cgi?id=331096), the following \nis illegal: A parent process uses PKCS11 module, then forks a child and the child tries to use PKCS11 module\nThe solution is to reinitialise all PKCS#11 modules in the child process. Any idea how do we do that? \n. I am already creating a new client object in every child process. Same error.. Any updates?\n. ",
    "allella": "If you happen to be connecting to a server utilizing CloudFlare then error 35 may be due to an out of date version of libcurl. Solution to error 35 on CentOS 6.x\n. ",
    "techouse": "I can confirm that this is a Tinker Shell issue. Running the same in PHP's interactive shell everything works fine.. ",
    "rgjodekerken": "I came across this thread while looking for a solution for this problem. \nI just found the solution so I'm sharing it with you guys.\nStarting tinker with the environment variable NSS_STRICT_NOFORK=DISABLED solves the issue.\nSource: https://cohanrobinson.com/php-curl-libcurl-error-on-subsequent-requests/\n. ",
    "kar2905": "Yes.\n1. I was using the S3 client to insert new objects in the bucket. I have the AWS access key and secret as environment variables.\n2. No. \n3. No. \n4. It almost occurs every 50-200 inserts.\n5. Yes. \n. ",
    "ahmdelemam": "i have the same error when i trying to put many objects by using loop.\n. ",
    "jehord": "Hello!\nDo you have resolution for this issue ?\n. I used yandex disk api. Not the latest version of the SDK, since you said that the latest version will not work with yndex disk api.\nFull stack trace i'll post later\n.  in D:\\Programs\\OpenServerMini\\OpenServer\\domains\\funsite-lc.ru\\vendor\\guzzle\\http\\Guzzle\\Http\\Message\\Request.php at line 569\n560561562563564565566567568569570571572573574575576577578     * Process a received response\n     \n     * @param array $context Contextual information\n     * @throws RequestException|BadResponseException on unsuccessful responses\n     /\n    protected function processResponse(array $context = array())\n    {\n        if (!$this->response) {\n            // If no response, then processResponse shouldn't have been called\n            $e = new RequestException('Error completing request');\n            $e->setRequest($this);\n            throw $e;\n        }\n```\n    $this->state = self::STATE_COMPLETE;\n// A request was sent, but we don't know if we'll send more or if the final response will be successful\n$this->dispatch('request.sent', $this->getEventArray() + $context);\n\n```\n1. in D:\\Programs\\OpenServerMini\\OpenServer\\domains\\funsite-lc.ru\\vendor\\guzzle\\http\\Guzzle\\Http\\Message\\Request.php \u2013 Guzzle\\Http\\Message\\Request::processResponse(['handle' => Guzzle\\Http\\Curl\\CurlHandle]) at line 378\n   372373374375376377378379380381382383384                    }\n                   $this->dispatch('request.before_send', array('request' => $this));\n               }\n               break;\n           case self::STATE_COMPLETE:\n               if ($oldState !== $state) {\n                   $this->processResponse($context);\n                   $this->responseBody = null;\n               }\n               break;\n           case self::STATE_ERROR:\n               if (isset($context['exception'])) {\n                   $this->dispatch('request.exception', array(\n2. in D:\\Programs\\OpenServerMini\\OpenServer\\domains\\funsite-lc.ru\\vendor\\guzzle\\http\\Guzzle\\Http\\Message\\EntityEnclosingRequest.php \u2013 Guzzle\\Http\\Message\\Request::setState('complete', ['handle' => Guzzle\\Http\\Curl\\CurlHandle]) at line 49\n   43444546474849505152535455 \n       return parent::__toString() . $this->body;\n   }\npublic function setState($state, array $context = array())\n   {\n       parent::setState($state, $context);\n       if ($state == self::STATE_TRANSFER && !$this->body && !count($this->postFields) && !count($this->postFiles)) {\n           $this->setHeader('Content-Length', 0)->removeHeader('Transfer-Encoding');\n       }\nreturn $this->state;\n}\n3. in D:\\Programs\\OpenServerMini\\OpenServer\\domains\\funsite-lc.ru\\vendor\\guzzle\\http\\Guzzle\\Http\\Curl\\CurlMulti.php \u2013 Guzzle\\Http\\Message\\EntityEnclosingRequest::setState('complete', ['handle' => Guzzle\\Http\\Curl\\CurlHandle]) at line 319\n   313314315316317318319320321322323324325 \n   protected function processResponse(RequestInterface $request, CurlHandle $handle, array $curl)\n   {\n       // Set the transfer stats on the response\n       $handle->updateRequestFromTransfer($request);\n       // Check if a cURL exception occurred, and if so, notify things\n       $state = $request->setState(RequestInterface::STATE_COMPLETE, array('handle' => $handle));\n       // Only remove the request if it wasn't resent as a result of the state change\n       if ($state != RequestInterface::STATE_TRANSFER) {\n           $this->remove($request);\n       }\n   }\n   /*\n4. in D:\\Programs\\OpenServerMini\\OpenServer\\domains\\funsite-lc.ru\\vendor\\guzzle\\http\\Guzzle\\Http\\Curl\\CurlMulti.php \u2013 Guzzle\\Http\\Curl\\CurlMulti::processResponse(Guzzle\\Http\\Message\\EntityEnclosingRequest, Guzzle\\Http\\Curl\\CurlHandle, ['msg' => 1, 'result' => 56, 'handle' => Resource id #936]) at line 244\n   238239240241242243244245246247248249250     /\n   private function processMessages()\n   {\n       while ($done = curl_multi_info_read($this->multiHandle)) {\n           $request = $this->resourceHash[(int) $done['handle']];\n           try {\n               $this->processResponse($request, $this->handles[$request], $done);\n               $this->successful[] = $request;\n           } catch (\\Exception $e) {\n               $this->removeErroredRequest($request, $e);\n           }\n       }\n   }\n5. in D:\\Programs\\OpenServerMini\\OpenServer\\domains\\funsite-lc.ru\\vendor\\guzzle\\http\\Guzzle\\Http\\Curl\\CurlMulti.php \u2013 Guzzle\\Http\\Curl\\CurlMulti::processMessages() at line 227\n   221222223224225226227228229230231232233        // The first curl_multi_select often times out no matter what, but is usually required for fast transfers\n       $selectTimeout = 0.001;\n       $active = false;\n       do {\n           while (($mrc = curl_multi_exec($this->multiHandle, $active)) == CURLM_CALL_MULTI_PERFORM);\n           $this->checkCurlResult($mrc);\n           $this->processMessages();\n           if ($active && curl_multi_select($this->multiHandle, $selectTimeout) === -1) {\n               // Perform a usleep if a select returns -1: https://bugs.php.net/bug.php?id=61141\n               usleep(150);\n           }\n           $selectTimeout = 1;\n       } while ($active);\n6. in D:\\Programs\\OpenServerMini\\OpenServer\\domains\\funsite-lc.ru\\vendor\\guzzle\\http\\Guzzle\\Http\\Curl\\CurlMulti.php \u2013 Guzzle\\Http\\Curl\\CurlMulti::executeHandles() at line 211\n   205206207208209210211212213214215216217                }\n           }\n           if ($blocking == $total) {\n               // Sleep to prevent eating CPU because no requests are actually pending a select call\n               usleep(500);\n           } else {\n               $this->executeHandles();\n           }\n       }\n   }\n/**\n   - Execute and select curl handles\n7. in D:\\Programs\\OpenServerMini\\OpenServer\\domains\\funsite-lc.ru\\vendor\\guzzle\\http\\Guzzle\\Http\\Curl\\CurlMulti.php \u2013 Guzzle\\Http\\Curl\\CurlMulti::perform() at line 105\n   99100101102103104105106107108109110111        $this->handles = new \\SplObjectStorage();\n       $this->requests = $this->resourceHash = $this->exceptions = $this->successful = array();\n   }\npublic function send()\n   {\n       $this->perform();\n       $exceptions = $this->exceptions;\n       $successful = $this->successful;\n       $this->reset();\nif ($exceptions) {\n       $this->throwMultiException($exceptions, $successful);\n8. in D:\\Programs\\OpenServerMini\\OpenServer\\domains\\funsite-lc.ru\\vendor\\guzzle\\http\\Guzzle\\Http\\Curl\\CurlMultiProxy.php \u2013 Guzzle\\Http\\Curl\\CurlMulti::send() at line 91\n   85868788899091929394959697            // Add this handle to a list of handles than is claimed\n           $this->groups[] = $group;\n           while ($request = array_shift($this->queued)) {\n               $group->add($request);\n           }\n           try {\n               $group->send();\n               array_pop($this->groups);\n               $this->cleanupHandles();\n           } catch (\\Exception $e) {\n               // Remove the group and cleanup if an exception was encountered and no more requests in group\n               if (!$group->count()) {\n                   array_pop($this->groups);\n9. in D:\\Programs\\OpenServerMini\\OpenServer\\domains\\funsite-lc.ru\\vendor\\guzzle\\http\\Guzzle\\Http\\Client.php \u2013 Guzzle\\Http\\Curl\\CurlMultiProxy::send() at line 282\n   276277278279280281282283284285286287288        if (!($requests instanceof RequestInterface)) {\n           return $this->sendMultiple($requests);\n       }\ntry {\n       /** @var $requests RequestInterface  */\n       $this->getCurlMulti()->add($requests)->send();\n       return $requests->getResponse();\n   } catch (ExceptionCollection $e) {\n       throw $e->getFirst();\n   }\n}\n10. in D:\\Programs\\OpenServerMini\\OpenServer\\domains\\funsite-lc.ru\\vendor\\guzzle\\http\\Guzzle\\Http\\Message\\Request.php \u2013 Guzzle\\Http\\Client::send(Guzzle\\Http\\Message\\EntityEnclosingRequest) at line 198\n    192193194195196197198199200201202203204    public function send()\n    {\n        if (!$this->client) {\n            throw new RuntimeException('A client must be set on the request');\n        }\n```\nreturn $this->client->send($this);\n```\n\n}\n\npublic function getResponse()\n{\n    return $this->response;\n}\n\n\n\nin D:\\Programs\\OpenServerMini\\OpenServer\\domains\\funsite-lc.ru\\vendor\\nixsolutions\\yandex-php-library\\src\\Yandex\\Disk\\DiskClient.php \u2013 Guzzle\\Http\\Message\\Request::send() at line 100\n    949596979899100101102103104105106     * @return Response\n     */\n    protected function sendRequest(RequestInterface $request)\n    {\n        try {\n            $request = $this->prepareRequest($request);\n            $response = $request->send();\n        } catch (ClientErrorResponseException $ex) {\n$result = $request->getResponse();\n    $code = $result->getStatusCode();\n    $message = $result->getReasonPhrase();\n12. in D:\\Programs\\OpenServerMini\\OpenServer\\domains\\funsite-lc.ru\\vendor\\nixsolutions\\yandex-php-library\\src\\Yandex\\Disk\\DiskClient.php \u2013 Yandex\\Disk\\DiskClient::sendRequest(Guzzle\\Http\\Message\\EntityEnclosingRequest) at line 352\n346347348349350351352353354355356357358            $request = $client->createRequest(\n            'PUT',\n            $path . $file['name'],\n            $headers,\n            file_get_contents($file['path'])\n        );\n        $this->sendRequest($request);\n}\n}\n/*\n13. in D:\\Programs\\OpenServerMini\\OpenServer\\domains\\funsite-lc.ru\\common\\components\\RobotComponent.php \u2013 Yandex\\Disk\\DiskClient::uploadFile('/funny_images/', ['path' => 'D:\\Programs\\OpenServerMini\\OpenS...', 'name' => '433c0eab64d945fbcdd6fb1c8001b073...']) at line 298\n292293294295296297298299300301302303304                    '/funny_images/',\n                array(\n                    'path' => $dir . $fileName,\n                    //'size' => filesize($fileName),\n                    'name' => $fileName\n                )\n            );\n                //Yii::$app->yadisk->disk->showImage();\n                $content .= BaseUrl::toRoute(['cloud/images', 'path' => '/funny_images/'. $fileName]);\n    }\n    return $content;\n}\n14. in D:\\Programs\\OpenServerMini\\OpenServer\\domains\\funsite-lc.ru\\common\\components\\RobotComponent.php \u2013 common\\components\\RobotComponent::parseArticleData('http://trinixy.ru/108097-prikoln...', '//[@id=\"dle-content\"]/div[@clas...') at line 321\n315316317318319320321322323324325326327                    $modelParsUrlData->site_id=$site->id;\n                $modelParsUrlData->url_param_id=$param->id;\n                $modelParsUrlData->url=$pars_url;\n                $modelParsUrlData->save();\n                $url_articles = $modelParsUrlData::find()->where(array('url_param_id' => $param->id))->all();\n                foreach($url_articles as $url_article){\n                    $article = Yii::$app->robot->parseArticleData($url_article->url, $param->xpath_article);\n//VarDumper::dump($article, 10, true);\n                if(!empty($article)){//\u0415\u0441\u043b\u0438 \u043f\u043e\u043b\u0443\u0447\u0438\u043b\u0438 \u043a\u0430\u043a\u0438\u0435 \u0442\u043e \u0434\u0430\u043d\u043d\u044b\u0435 \u0432\u043d\u0443\u0442\u0440\u0438 \u0437\u0430\u043f\u0438\u0441\u0438\n                    $modelParsArticles->site_id = $site->id;\n                    $modelParsArticles->url_data_id = $url_article->id;\n                    $modelParsArticles->article = $article;\n15. in D:\\Programs\\OpenServerMini\\OpenServer\\domains\\funsite-lc.ru\\frontend\\controllers\\RobotController.php \u2013 common\\components\\RobotComponent::parse(common\\models\\ParsSites) at line 26\n202122232425262728293031     * @param string $username\n */\npublic function actionStart()\n{\n    $sites = ParsSites::find()->all();\n    foreach($sites as $site){\n        Yii::$app->robot->parse($site);\n    }\n}\n\n\n}\n17. frontend\\controllers\\RobotController::actionStart()\n18. in D:\\Programs\\OpenServerMini\\OpenServer\\domains\\funsite-lc.ru\\vendor\\yiisoft\\yii2\\base\\InlineAction.php \u2013 call_user_func_array([frontend\\controllers\\RobotController, 'actionStart'], []) at line 55\n495051525354555657        $args = $this->controller->bindActionParams($this, $params);\n        Yii::trace('Running action: ' . get_class($this->controller) . '::' . $this->actionMethod . '()', METHOD);\n        if (Yii::$app->requestedParams === null) {\n            Yii::$app->requestedParams = $args;\n        }\nreturn call_user_func_array([$this->controller, $this->actionMethod], $args);\n}\n}\n. Ok\n. ",
    "cubicleWar": "I have tried both batch and single object deleting, it does take a while to return but even six hours later the items remain on S3. I have also verified my keys are correct using doesObjectExisit.\n. Here is an example, note I am using Lavarel with the the aws-sdk-php-bundle : \n```\nforeach($this->options['image_versions'] as $version => $options) {\n    $keys[] = array('Key' => $tennantUuid . '/' . ($version != '' ? $version . '/' : '') . Input::get('file'));\n}\n$s3 = AWS::get('s3');\n$response = $s3->deleteObjects(array(\n    'Bucket'        => 'fluxey-images',\n    'Objects'       => $keys\n));\n```\nThis particular piece of code was running fine until I did my last composer update in which the only relevant packages updated were aws/aws-sdk-php (2.5.1 > dev-master 73e90b8) and guzzle (cd97afb > 028195b).\nI have another server set up with exactly the same codebase pulled from the git repository which I haven't run composer update on and it works fine. I think this rules out any problems with the codebase or settings in s3, leaving the aws/aws-sdk-php as the main suspect.\n. This bug has been fixed somewhere between 73e90b8 and f720d79\n. ",
    "bap840": "First, thank you very much for taking the time to respond to this issue.  So I set \nzend.detect_unicode = Off\nand also\ndetect_unicode = Off  (although I don't think this line did anything)\nin my php.ini and I noticed it is now off when doing phpinfo()\nUnfortunately the problem is still happening.  Is there another way to load the sdk besides using the phar file directly that will avoid my issue?\n. Another idea.. is there a way to make it use JSON instead of XML?\n. So I just tried installing via composer and I'm getting the exact same error.... however since composer downloads the actual .php files I was able to edit Response.php line 886 with the following HACK\n$b = (string) $this->body ?: '<root />';\n        $last = substr($b, -1); \n        if ($last !== \">\") $b = rtrim($b, $last);\n        $xml = new \\SimpleXMLElement($b, LIBXML_NONET);\nI know it's a huge hack, but it works.  Still I would be interested in better ways to get rid of the excess character being appended to the end of the XML document.. if you have any other ideas.  Thanks again.\n. I'm thinking it's the client because it only manifests when going through WAMP and not on my live site which is Linux (ubuntu)\n. php 5.4.12\ncurl 7.29.0\nlibxml 2.7.8\n. @kstich, I had the same problem (but with a multipart upload) and I've modified my code to pass in the sha256 checksum via \"ContentSHA256\" instead of as a Content-MD5 header as you suggested, however X-Amz-Content-Sha256 parameter seems to be ignored, no matter what I pass in it goes through.. here is a recent request I tried\nhttps://s3-us-west-1.amazonaws.com/files.team.eon.plus/dev/10138/12752/file.3.json?uploadId=.DniEZhPWhy2hG6w5MtB.YhNQ0uSn4wRurfSmjvfmIEICyxvb.M4OIRsQHXs6hWhTuV8.cKy8KRFUGqxu10zby2uLOs_v868YMJq1dgMma2tZNvAkfGwV.0wHxNIJt3_&partNumber=1&X-Amz-Content-Sha256=56122b952b633eb3422e687c04deba894a1334220fed1ad58c21553d2459691e-DUMMY&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAJ3VMBV5CC2RKTACQ%2F20171101%2Fus-west-1%2Fs3%2Faws4_request&X-Amz-Date=20171101T234004Z&X-Amz-SignedHeaders=host&X-Amz-Expires=3600&X-Amz-Signature=b19484a3bd21fb0aab1abd6415f16213ff1bfc4c8bb4031e2679b0ddfe5929d1\nYou can see I appened \"-DUMMY\" to the end of my checksum but it still went through... before if I tried the same thing with my Content-MD5 header it would be rejected.  Any help would be appreciated!\n(btw I have the same problem now if I try to send x-amz-content-md5 as part of the query string ..it's also ignored and goes through with any value). ",
    "bstratton-rmn": "Ok I can see those commits - Many thanks.\n. ",
    "SpareShade": "Ok, that was my lack of knowledge of how aws works, and I guess docs can't always provide the full and detailed explanations...\nso my first workaround was to use AcpBuilder and passing it as \n``` php\n$result = $client->putObjectAcl(array(\n    'ACP' => AcpBuilder::newInstance() ... ,\n'Bucket' => 'string',\n'Key' => 'string',\n\n));\n```\nbut the actually correct way of using the array was to define the 'Owner' if a 'Grantee's array was passed in too (and of course removing the 'ACL' string\nphp\n$result = $client->putObjectAcl(array(\n    'Grants' => array(\n        array(\n            'Grantee' => array(\n                'DisplayName' => 'string',\n                'EmailAddress' => 'string',\n                'ID' => 'string',\n                // Type is required\n                'Type' => 'string',\n                'URI' => 'string',\n            ),\n            'Permission' => 'string',\n        ),\n        // ... repeated\n    ),\n    'Owner' => array(\n        'DisplayName' => 'string',\n        'ID' => 'string',\n    ),\n    // Bucket is required\n    'Bucket' => 'string',\n    // Key is required\n    'Key' => 'string',\n));\ncouldn't find any reference on stack overflow for this, so i thought to kind of answer it here for the needy like me :)....\nthank you for building a great sdk ....\n. ",
    "andrew-s": "@mtdowling That does indeed work but that's not mentioned in the documentation - and the odd error messages add to the confusion.\nI think there's a level of inconsistency here, even if the default region is us-east-1 - why does then in that case my non-DNS named buckets work? I think they should all be equal or improve the documentation to mention this caveat. \nWhat are your thoughts?\n. Can I ask what the fix was for this as you closed the ticket? I think also the error messages in the SDK should be improved as they're too vague this specific issue.\n. Yeah, I agree with @skyzyx on this - you could easily detect if it's a DNS like name (perhaps look for a \".\" in the bucket name) and present a different message to use a region or a non-dns like name?\n. ",
    "IxSi": "1) The basic version:\n$baseDir = '/var/...';\ntry {\n$obj = UploadSyncBuilder::getInstance();\n$obj\n    ->setClient($client)\n    ->setBucket($bucket)\n    ->setAcl('public-read')\n    ->setMultipartUploadSize(1073741824)\n```\n//->setBaseDir($baseDir)\n//->uploadFromGlob($baseDir . '/tl_files/generatedPdf/*')\n//->setKeyPrefix('test')\n->uploadFromDirectory($baseDir . '/tl_files/generatedPdf')\n->setKeyPrefix('test' . '/tl_files/generatedPdf')\n->setOperationParams( array('ContentDisposition' => 'attachment') )\n->enableDebugOutput($handle)\n->build()\n->transfer(); \n```\n} catch(Exception $e) {\n    echo $e->getMessage() . '\\n';\n    file_put_contents($logError, date('d.m.Y H:i:s ') . $e->getMessage() . PHP_EOL, FILE_APPEND);\n}\n2) The debug output:\n// First upload \nUploading /var/.../tl_files/generatedPdf/testarchiv2.zip -> test/tl_files/generatedPdf/testarchiv2.zip (178079 bytes)\nUploading /var/.../tl_files/generatedPdf/testarchiv1.zip -> test/tl_files/generatedPdf/testarchiv1.zip (178079 bytes)\nUploading /var/.../tl_files/generatedPdf/testz.pdf -> test/tl_files/generatedPdf/testz.pdf (13250 bytes)\nUploading /var/.../tl_files/generatedPdf/testarchiv4.zip -> test/tl_files/generatedPdf/testarchiv4.zip (178079 bytes)\nUploading /var/.../tl_files/generatedPdf/testarchiv.zip -> test/tl_files/generatedPdf/testarchiv.zip (178079 bytes)\nUploading /var/.../tl_files/generatedPdf/pdf1.pdf -> test/tl_files/generatedPdf/pdf1.pdf (13250 bytes)\nUploading /var/.../tl_files/generatedPdf/testarchiv3.zip -> test/tl_files/generatedPdf/testarchiv3.zip (178079 bytes)\nUploading /var/.../tl_files/generatedPdf/pdf.pdf -> test/tl_files/generatedPdf/pdf.pdf (13250 bytes)\n(1s)\n// Further uploads = none \n(0s)\nAdditional info: I made that test with uploadFromGlob() again, it lasts about 7 seconds, while that basic version is uploading in about 1s, but I don't know wether this is important.\n. ",
    "aequasi": "unless https://github.com/guzzle/service/blob/master/Builder/ServiceBuilder.php#L85 resolves this. Testing, will close if it does.\n. ",
    "clifflu": "The PR is faulty, thanks to notes from Ryan Parman. I have deleted my original fork, and would send a new one later today. \n. No problem. Should be done this weekend :)\n. ",
    "PSiAU": "I think this bug might have regressed. I'm getting this error:\nPHP Fatal error:  Uncaught Aws\\Ec2\\Exception\\Ec2Exception: AWS Error Code: InvalidParameterCombination, Status Code: 400, AWS Request ID: e3deac8f-2a15-460b-bfed-de5db66d87a2, AWS Error Type: client, AWS Error Message: Network interfaces and an instance-level security groups may not be specified on the same request, User-Agent: aws-sdk-php2/2.8.6 Guzzle/3.9.3 curl/7.37.1 PHP/5.5.20\n  thrown in phar:///usr/local/bin/aws.phar/Aws/Common/Exception/NamespaceExceptionFactory.php on line 91\nAnd heres the code:\nphp\n$response = $ec2Client -> runInstances(array(\n    'ImageId' => 'ami-fd9cecc7',\n    'MinCount' => '1',\n    'MaxCount' => '1',\n    'InstanceType' => 'c4.xlarge',\n    'KeyName' => 'mykey',\n    'NetworkInterfaces' => array(array(\n        'DeviceIndex' => 0,\n        'SubnetId' => 'subnet-xxxxxxxx',\n        'AssociatePublicIpAddress' => true\n    )),\n    'SecurityGroupIds' => array('sg-xxxxxxxx','sg-xxxxxxxx'),\n));\n. I'm using 3.0.2.\nHere's the equivalent CLI command:\naws --profile myprofile ec2 run-instances --image-id ami-fd9cecc7 --count 1 --instance-type c4.xlarge --key-name mykey --security-group-ids sg-xxxxxxxx sg-xxxxxxxx --subnet-id subnet-xxxxxxxx --associate-public-ip-address\n. Got it.\nI should have been doing this instead:\n$response = $ec2Client->runInstances([\n    'ImageId' => 'ami-fd9cecc7',\n    'InstanceType' => 'c4.xlarge',\n    'KeyName' => 'mykey',\n    'MaxCount' => '1',\n    'MinCount' => '1',\n    'NetworkInterfaces' => [[\n        'DeviceIndex' => 0,\n        'SubnetId' => 'subnet-xxxxxxxx',\n        'AssociatePublicIpAddress' => true,\n        'Groups' => ['sg-xxxxxxxx','sg-xxxxxxxx'],\n    ]],\n]);\nThe security groups should be specified under NetworkInterfaces using \"Groups\".\n. I'm running on v3.0.6 of the SDK\nMaybe the SDK isn't getting a role at all. As a test i'm doing a simple describe instances operation and it's getting a UnauthorisedOperation back. The instance definitely has a role attached with pretty much god access (* on everything).\nHere's the code, it doesn't get any simpler.\n```\nrequire \"aws.phar\";\ndate_default_timezone_set('Australia/Sydney');\n$ec2client = new Aws\\Ec2\\Ec2Client([\n    'version' => 'latest',\n    'region' => 'ap-southeast-2'\n]);\n$result = $ec2client->describeInstances([\n    'InstanceIds' => ['i-xxxxxxxx'],\n]);\n$ip = $result['Reservations'][0]['Instances'][0]['PrivateIpAddress'];\necho \"$ip\";\n```\nHere's what I get back:\nPHP Fatal error:  Uncaught exception 'Aws\\Ec2\\Exception\\Ec2Exception' with message 'Error executing \"DescribeInstances\" on \"https://ec2.ap-southeast-2.amazonaws.com\"; AWS HTTP error: Client error: 403 UnauthorizedOperation (client): You are not authorized to perform this operation. - <?xml version=\"1.0\" encoding=\"UTF-8\"?>\nUnauthorizedOperationYou are not authorized to perform this operation.730ef2e3-3ccc-4c8e-863b-05016f8b9995'\nexception 'GuzzleHttp\\Exception\\ClientException' with message 'Client error: 403' in phar:///home/bambooagent/tmp/blah/aws.phar/GuzzleHttp/Middleware.php:68\nStack trace:\n0 phar:///home/bambooagent/tmp/blah/aws.phar/GuzzleHttp/Promise/Promise.php(199): GuzzleHttp\\Middleware::GuzzleHttp{closure}(Object(GuzzleHttp\\Psr7\\Response))\n1 phar:///home/bambooagent/tmp/blah/aws.phar/GuzzleHttp/Promise/Promise.php(165): GuzzleHttp\\Promise\\Promise::callHandler(1, Object(GuzzleHttp\\Psr7\\Response), Array)\n2  in phar:///home/bambooagent/tmp/blah/aws.phar/Aws/WrappedHttpHandler.php on line 152\n. I've double checked and there's definitely no credentials in the access and secret key environment variables, nor in ~/.aws/credentials.\n. I get a straight up Access Denied running that code:\nPHP Fatal error:  Uncaught exception 'Aws\\S3\\Exception\\S3Exception' with message 'Error executing \"ListBuckets\" on \"https://s3-ap-southeast-2.amazonaws.com/\"; AWS HTTP error: Client error: 403 AccessDenied (client): Access Denied - <?xml version=\"1.0\" encoding=\"UTF-8\"?>\nAccessDeniedAccess Denied11EF0336C2D7FA65dahAjv7uDfhfi1ojed28/8sNMyJSUyeoc0xwf6J5t94dNJbyNl50PCDISY2+Tu3gpVdwvhYC0ec='\nIt didn't even spit out a key.\nThe curls are returning the correct role for the instance, and also returning a key, secret, and token. If I plug the key secret and token into the code, it runs just fine.\nIt's almost as if the SDK isn't making the request for the temporary credentials, or if it is, it's not using them.....\n. Nice that worked.\nIt's returning a different key each time I run it. The curl returns the same key if I run it a few times consecutively, but not that PHP code. Not sure what that means....\n. The only output is \"Request sent with key: xxxxxxxxx\" (substituted the real key with xxx's).\nI think it's worth noting that this assumed role problem doesn't happen with the 2.8.13 SDK. I thought i'd give it a try with the older SDK and things are working as expected. It picks up the assumed role just fine.\n. Found the cause of this.\nWe use proxies in our environment. The SDK was talking to the AWS API through the proxies, and it was thus returning the role information of the proxy servers.\n. No i'm using paginators with route53 for the first time, so I can't say if this worked in previous versions.\n. ",
    "neildchen": "May I report what I have got using aws-sdk apiVersion: '2016-11-15' with node.js\n\nmessage: 'Network interfaces and an instance-level security groups may not be specified on the same request',\n  code: 'InvalidParameterCombination',\n  time: 2017-02-22T04:09:22.463Z,\n  requestId: 'ed4f81ed-0447-4770-ac82-605fee6d5d28',\n  statusCode: 400,\n  retryable: false,\n\nand this is my code:\njs\nvar params = {\n   ImageId: myConfig.ImageId, // use our own ami\n   InstanceType: myConfig.InstanceType, // use our own type\n   MinCount: 1,\n   MaxCount: 1,\n   KeyName: myConfig.KeyName,\n   //SecurityGroupIds: myConfig.SecurityGroupIds, // which is string like \"sg-123abc32\"\n   //[2017-02-22] commented out due to complaint of \"Network interfaces and an instance-level security groups may not be specified on the same request\"; it works without this key\n   NetworkInterfaces: [\n    {\n      AssociatePublicIpAddress: false, // false here\n      DeviceIndex: 0,\n    }\n  ]\n};. ",
    "dasbiswajit": "Thanks it solved my issue. @mtdowling Thank you.. ",
    "JennyYang19": "@mtdowling thank you for solving this prolem. ",
    "lyrixx": "Thank\n. ",
    "johnkramlich": "I am also experiencing the same issue, which I noticed as of about 1pm CST on Tuesday, 2/18/14.  The issue may have been present prior to then, however, multipart uploads seemed to be working for me as of a few days ago.  I had not made any code changes on my end from the time things were working till the time they stopped working.  The only thing I can think of was that I might have done a composer update. Prior to the composer update my composer.json file used verson 2.* for the SDK. I'm not sure what actual version was installed.  When I noticed the error, I tried specifying v2.5.2 of aws-sdk-php as well as dev-master.  Both seem to be experiencing the same issue.\nMy current workaround is to use setMultipartUploadSize() and pass it a value higher than the largest file I need to upload.  That's obviously less than ideal, but it works.\nHere is the relevant part of my PHP code, without setMultipartUploadSize()\n``` php\n$client = S3Client::factory(array(\n    'key'    => $awsKey,\n    'secret' => $awsSecret,\n    'client.backoff.logger' => 'debug',\n    'curl.options' => array(CURLOPT_VERBOSE => true)\n));\n$sync = UploadSyncBuilder::getInstance()\n            ->setClient($client)\n            ->setBucket($s3bucket)\n            ->setConcurrency($concurrentFilesToUpload)\n            ->setAcl(CannedAcl::PUBLIC_READ)\n            ->setKeyPrefix($objectPrefix)\n            ->setSourceIterator($allFiles)\n            ->setSourceFilenameConverter($StripPathFilenameConverter)\n            ->enableDebugOutput($resource)\n            ->force(false)\n            ->build();\n    $sync->transfer();\n\n```\nAnd the output I get from curl\n```\n About to connect() to s3.amazonaws.com port 443 (#0)\n   Trying 72.21.215.100... * Connected to s3.amazonaws.com (72.21.215.100) port 443 (#0)\n successfully set certificate verify locations:\n   CAfile: /home/golfinst/vendor/guzzle/guzzle/src/Guzzle/Http/Resources/cacert.pem\n  CApath: /etc/ssl/certs\n SSL connection using AES256-SHA\n Server certificate:\n    subject: C=US; ST=Washington; L=Seattle; O=Amazon.com Inc.; CN=s3.amazonaws.com\n    start date: 2013-09-09 00:00:00 GMT\n    expire date: 2014-09-10 23:59:59 GMT\n    subjectAltName: s3.amazonaws.com matched\n    issuer: C=US; O=VeriSign, Inc.; OU=VeriSign Trust Network; OU=Terms of use at https://www.verisign.com/rpa (c)10; CN=VeriSign Class 3 Secure Server CA - G3\n    SSL certificate verify ok.\n\nPOST /video.test.mygolfinstructor.com/movies/private-instruction/attachments/0000/15MB.mov?uploads HTTP/1.1\nHost: s3.amazonaws.com\nContent-Type: video/quicktime\nUser-Agent: aws-sdk-php2/2.5.2 Guzzle/3.8.1 curl/7.22.0 PHP/5.3.10-1ubuntu3.9 MUP\nDate: Wed, 19 Feb 2014 07:45:59 +0000\nAuthorization: AWS [redacted]\nContent-Length: 0\n\n< HTTP/1.1 403 Forbidden\n< x-amz-request-id: [redacted]\n< x-amz-id-2: [redacted]\n< Content-Type: application/xml\n< Transfer-Encoding: chunked\n< Date: Wed, 19 Feb 2014 07:45:59 GMT\n< Server: AmazonS3\n<\n* Connection #0 to host s3.amazonaws.com left intact\n```\nPlease let me know if there is anything else I can do to assist in troubleshooting this issue.\n. Michael, thanks for the quick response. I am on dev-master of both Guzzle and the aws-sdk.  I verified that your commit fixes the issue I was experiencing.\n. ",
    "Rudigern": "For others that come across this issue, I had to \"touch\" a bunch of files in S3 to trigger a Lambda function. I found 2 things to be aware of: \n\nOnly urlencode() the filename, not the full path\nOnly urlencode() the source, not the destination\n. \n",
    "rishabhdua": "The exception message is this \n[curl] 28: Resolving timed out after 7987 milliseconds [url] http://dynamodb.us-east-1.amazonaws.com/\nI co-related that but don't know why AWS support team keeps on asking for the requestIds and straight forward deny that we can't debug further if there are no requestId.\n. ",
    "rzds": "It's a shame this huuuuge SDK doesn't already do the encoding of the source and destinations.\nI end up doing something like this:\n``` php\n<?php\nrequire 'vendor/autoload.php';\nuse Aws\\Ses\\SesClient;\n$client = SesClient::factory(array(\n    'key'    => 'XXXXXXXXXXXXXXXX',\n    'secret' => 'XXXXXXXXXXXXXXXXXXXXXXXXXXXXXX',\n    'region' => 'eu-west-1'\n));\n$encoded_name = base64_encode(\"Gr\u00e9gory Smith\");\n$address = \"=?utf-8?B?$encoded_name?= some@address.com\";\n$result = $client->sendEmail(array(\n    // Source is required\n    'Source' => 'someaddress@gmail.com',\n    // Destination is required\n    'Destination' => array(\n        'ToAddresses' => array($address)\n    ),\n    // Message is required\n    'Message' => array(\n        // Subject is required\n        'Subject' => array(\n            // Data is required\n            'Data' => 'The subject is Gr\u00e9gory',\n            'Charset' => 'utf-8',\n        ),\n        // Body is required\n        'Body' => array(\n            'Text' => array(\n                // Data is required\n                'Data' => 'The message is Gr\u00e9gory',\n                'Charset' => 'utf-8',\n            )\n        )\n    )\n));\n?>\n```\n. ",
    "AnzeR": "Thank you for quick response.\nI tried to make URL the same like the one generated by aws-sdk so I add Content-Type parameter. It doesn't work in both cases. \n. Hi.\nThis resolve the issue. That you for your help.\n. The code is the same like in #239 \n``` php\n$client = S3Client::factory(array('region' => 'eu-west-1','key' => 'myKey','secret' => 'mySecret'));\n$command = $client->getCommand('PutObject', array(\n    'Bucket' => 'myBucket',\n    'Key' => 'testing/'.time(),\n    'ContentType' => 'image/jpeg',\n    'Body' => '',\n    'ContentMD5' => false,\n    'ACL' => 'public-read' //this is the problem :(\n));\n$signedUrl = $command->createPresignedUrl('+5 minutes');\necho(\"Now run from console for upload:\\ncurl -v -H \\\"Content-Type: image/jpeg\\\" -T /tmp/temp.jpg '\" . $signedUrl . \"'\");\n```\nWith ACL attribute I got SignatureDoesNotMatch\nIf I remove it, the object is uploaded but without public read permission.\nUpdate:\nHere is a snippet for aws-sdk\njavascript\nvar AWS = require('aws-sdk');\nAWS.config.update({ accessKeyId: 'myKey', secretAccessKey: 'mySecret', region: 'eu-west-1' });\nvar s3 = new AWS.S3();\nvar params = {\n    Bucket: 'myBucket',\n    Key: 'testing/nodeJS_' + (+new Date),\n    ContentType: 'image/jpeg',\n    Expires: 60 * 5,\n    ACL: 'public-read'\n};\ns3.getSignedUrl('putObject', params, function(err, url) {\n    console.log('The URL is: ', url);\n    console.log('Now run from console for upload:\\n\\ncurl -v -H \"Content-Type: image/jpeg\" -T /tmp/temp.jpg \\'' + url + '\\'');\n});\nWhich actually works. Object automatically becomes public. \nIt also adds x-amz-acl=public-read and Content-Type=image%2Fjpeg in the URL\n. ",
    "LukasRos": "I had a similar problem; presigned Put request worked even without ContentMD5 => false until two days ago but then it suddenly stopped working. Before finding this solution (which also works) I solved it by making a RequestInterface with $client->put().\nAny explanation why this behavior occured only recently and not before?!\n. ",
    "foush": "I too am testing this functionality using this code\n$client = S3Client::factory(array('region' => 'us-east-1','key' => 'xxx','secret' => 'xxx',));\n$url = $client->getCommand('PutObject', array(\n            'Bucket' => $this->getBucket(),\n            'Key' => $uploadKey,\n            'ContentType' => 'image/jpeg',\n            'Body'        => '',\n            'ContentMD5'  => false\n        ))->createPresignedUrl('+5 minutes');\nWhich generates a URL string for me. However, I have tried sending both a POST and a PUT request to this result URL attaching an image file as binary post body data, and I get:\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<Error><Code>SignatureDoesNotMatch</Code><Message>The request signature we calculated does not match the signature you provided. Check your key and signing method.</Message> ...\nAm I calculating the URL incorrectly or am I sending the file improperly?\n. Using the above method for generating the URL.\nI am using POSTman (chrome plugin) to make RESTful calls to test if something works. I have tried a POST and a PUT to the resulting presigned URL, attaching the image as binary body data. Ought I use some other way?\n. Chrome Inspected Request sent over network:\nMethod\nPUT\nHeaders\nAccept:*/*\nAccept-Encoding:gzip,deflate,sdch\nAccept-Language:en-US,en;q=0.8\nCache-Control:no-cache\nConnection:keep-alive\nContent-Length:24964\nContent-Type:image/jpeg\nHost:droparoo-dev-john.s3.amazonaws.com\nOrigin:chrome-extension://fhbjgbiflinjbdggehcddcbncdddomop\nPostman-Token:8fe25ca0-a0be-b3d1-4602-46e5734a48ce\nUser-Agent:Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1985.125 Safari/537.36\nQuery Params\nAWSAccessKeyId:XXX\nExpires:1406834171\nSignature:XXX\nWhere XXX indicates the presumed correct value is present\n. @skyzyx that's a good point, thank you! I tried POSTing to the pre-signed URI out of desperation. I'm expecting to use PUT\n. ",
    "KingFinlayson": "Hi - I'm having a similar issue where the code below results in a 403 forbidden http response. Any help would be great!\n$command = $this->s3->getCommand('PutObject', array(\n                'Bucket' => $bucket,\n                'Key' => $remoteName,\n                'Body'   => '',\n                'ContentType' => 'video/wmv',\n                'ContentMD5'  => false\n            ));\n$signedUrl = $command->createPresignedUrl('+5 minutes');\n$client = new GuzzleHttp\\Client();\n            $response = GuzzleHttp\\put($signedUrl, [\n                'body'    => fopen($filePath, 'r')\n            ]);\n. I tried that with the code below but I was still receiving the 403:\n$client = new GuzzleHttp\\Client();\n            $response = GuzzleHttp\\put($signedUrl, [\n                'body'    => fopen($filePath, 'r'),\n                'headers' => ['Content-Length' => filesize($filePath)]\n            ]);\nI think I'll try with cURL to see if it may be an issue with the Guzzle client.\n. Thanks I got it, I just had the wrong ContentType in the S3 command! It's Friday forgive me :).\n. ",
    "fdidron": "Hi Everyone,\nI'm getting a similar issue, using SDK version 2.16.6.\nHere is my code :\n```\n$s3Creds = array('key' => AWS_KEY, 'secret' => AWS_SECRET, 'region' => 'ap-northeast-1');\n$s3 = S3Client::factory($s3Creds);\n//$photo->hashId is a 8 character alpha numeric string\n$command = $s3->getCommand('PutObject', array(\n    'Bucket'      => S3_BUCKET_NAME,\n    'Key'         => $photo->hashId . '/raw.jpg',\n    'ContentType' => 'image/jpeg',\n    'Body'        => ''\n));\n$signedUrl = $command->createPresignedUrl('+50 minutes');\necho urldecode($signedUrl);\n```\nThen later I'm running the following curl command : \ncurl -v -T ./image.jpg \"<signed url>\"\nAs a result I get a 403 error :\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<Error><Code>SignatureDoesNotMatch</Code><Message>The request signature we calculated does not match the signature you provided. Check your key and signing method.</Message>\n...\n. Forgot to add the Content Type Header on my curl request ...\ncurl -v -H 'Content-Type: image/jpeg' -T ./image.jpg '<signed url>'\nWorked beautifully, I'm ashamed !\nFlorian\n. ",
    "svillafe": "@fousheezy Did you were able to do it with postman?? I have the same problem.\n. @mtdowling The problem was that the user who were signing the url didn't have rights to upload pictures to that bucket. Thanks your comments.\n. ",
    "ihorbovkit": "Hi have next error  - \nbash\nFatal error: Call to undefined method Aws\\Command::createPresignedUrl()\nwith this code - \n``` bash\n$s3Creds = array('key' => AWS_KEY, 'secret' => AWS_SECRET, 'region' => 'ap-northeast-1');\n$s3 = S3Client::factory($s3Creds);\n//$photo->hashId is a 8 character alpha numeric string\n$command = $s3->getCommand('PutObject', array(\n    'Bucket'      => S3_BUCKET_NAME,\n    'Key'         => $photo->hashId . '/raw.jpg',\n    'ContentType' => 'image/jpeg',\n    'Body'        => ''\n));\n$signedUrl = $command->createPresignedUrl('+50 minutes');\necho urldecode($signedUrl);\n```\nI'm using AWS SDK S3 - Release v2.1.39\n. @shooding \nUsing s3.0.0 v3 - I did the following to get this to work.\n$command = $s3->getCommand('GetObject', array(\n            'Bucket'      => $this->customerBucket,\n            'Key'         => $fileName,\n            'ContentType' => 'image/png',\n            'ResponseContentDisposition' => 'attachment; filename=\"'.$fileName.'\"'\n        ));\n$signedUrl = $s3->createPresignedRequest($command, \"+1 week\");\n. Using s3.0.0 v3 - I did the following to get this to work.\nbash\n$command = $s3->getCommand('GetObject', array(\n            'Bucket'      => $this->customerBucket,\n            'Key'         => $fileName,\n            'ContentType' => 'image/png',\n            'ResponseContentDisposition' => 'attachment; filename=\"'.$fileName.'\"'\n        ));\n$signedUrl = $s3->createPresignedRequest($command, \"+1 week\");\n. ",
    "shooding": "@ihorbovkit \nYou will have to call createPresignedRequest in AWS SDK for PHP 3.x\nAre you sure it's really v2.1.39 (which is SDK for Javascript)?\n. ",
    "nazreen": "In case someone else is looking at this and is in a similar situation as me, I got a similar SignatureDoesNotMatchError when my s3 bucket's CORS Configuration did not contain <AllowedHeader>*</AllowedHeader>\nI ran into this when moving from one bucket to another, copying all the settings except for the CORS Configuration.. ",
    "renschler": "Struggled with this for a while, was getting the same Signature Does Not Match Error.\nThe change that fixed my issue was going from\nfetch(presignedURL, { method: \"PUT\", body: svg })\nto \nfetch(presignedURL, { method: \"PUT\", body: new Blob([svg], {type: file.type})})\nwhere svg was a variable storing the result of an uploaded svg file.\n```\nlet reader = new FileReader();\nreader.onload = (e) => {\n    let svg = atob(e.target.result.split(/[,;]/).pop()); // decode svg file from base64\n    // perform fetch\n\n}\n```. ",
    "peterkaminski": "Yes, I'm happy to contribute the code under the Apache 2.0 license.  Thanks!\n. ",
    "deepak8484": "( ! )\u00a0Fatal error: Class 'Aws\\Sns\\SnsClient' not found in E:\\wamp\\www\\sms\\mail.php on line\u00a030\nwhat should i do.. ",
    "nodesocket": "Ok, thanks.\n. ",
    "gavrichards": "Yes. Before the multi-part upload, this script gets the size of the file, and if it is smaller than 5 MB, it uses putObject() instead.\nIt then does unlink() on the file, and there is no warning.\nSo this has brought me to believe it is related to the multi-part solution.\n. Thanks guys, that works a treat.\n. ",
    "whyleyc": "It seems this might be caused by setConcurrency(15) - If I switch to a concurrency of 1 the part number timestamps are all correct. I suspect what might be happening here is:\n- By using concurrency several HTTP PUT msgs are constructed and sent simultaneously (hence the same timestamp on each).\n- Because of local bandwidth constraints only one 5MB multipart is getting uploaded ok\n- The others eventually timeout, as evidenced by the following HTTP 400 error in the debug log:\nYour socket connection to the server was not read from or written to within the timeout period. Idle connections will be closed.\n- Because the debug logfile prints HTTP interaction information on receipt of the response message it appears as though the parts are being 'PUT' sequentially (not concurrently) with the same timestamp on each.\n. Thanks - I'm testing on a poor network so this explains things. Out of interest I'll eventually be deploying the code onto EC2 instances - are there any guidelines for optimal concurrency settings when going from EC2 -> S3 for multipart upload ?\n. Ok, makes sense. Thanks for your help.\n. ",
    "wonjun27": "\ud83d\udc4d \n. ",
    "dkathan": "Best way I found was to clone the environment to a new name and remove the old one.   The site URL will change however. \n. ",
    "christianbradley": "You can keep the old url by swapping the cnames after the clone, and before destroying the original (using eb cli):\neb swap original-environment-name -n new-environment-name\n. ",
    "sobytes": "Hi Both\nApologies i actually managed to get this working in the end but by using a custom policy.\n$customPolicy = <<<POLICY\n                        {\n                            \"Statement\": [\n                                {\n                                    \"Resource\": \"{$resourceKey}\",\n                                    \"Condition\": {\n                                        \"IpAddress\": {\"AWS:SourceIp\": \"{$_SERVER['REMOTE_ADDR']}\"},\n                                        \"DateLessThan\": {\"AWS:EpochTime\": {$expires}}\n                                    }\n                                }\n                            ]\n                        }\nPOLICY;\n. So I think i am making progress but i am getting \"Invalid principal in policy\"\n$res = $s3->putBucketPolicy(array(\n        'Bucket' => $BucketName,\n        'Policy' => json_encode(array(\n            'Statement' => array(\n                array(\n                    'Sid' => 'PolicyForCloudFrontPrivateContent',\n                    'Action' => array(\n                        's3:GetObject'\n                    ),\n                    'Effect' => 'Allow',\n                    'Resource' => array(\n                        \"arn:aws:s3:::{$BucketName}/*\"\n                    ),\n                    'Principal' => array(\n                        'AWS' => array(\n                            'arn:aws:iam::cloudfront:' . $ARN\n                        )\n                    )\n                )\n            )\n        ))\n    ));\n. Hi i am also getting this issue.\n{\"error\":true,\"message\":\"[curl] 6: Could not resolve host: elastictranscoder.sa-east-1.amazonaws.com [url] https:\\/\\/elastictranscoder.sa-east-1.amazonaws.com\\/2012-09-25\\/pipelines\"}\n. Sorry please close this I should have got back aws elastic transcoder doesn't support that region yet. \n. Hi @mtdowling \nThe problem i have is i can't do that because the region maybe different every time depending on where the user has setup their region, so i need to do everything based on user settings.\nThats why i am calling \n```\n//Check Bucket Region Important\n    $result = $client->getBucketLocation(array(\n        // Bucket is required\n        'Bucket' => $bucket\n    ))->toArray();\n$region = $result['Location'];\n\nif(!empty($region)){\n    if($region != 'EU'){\n        $client->setRegion($region);\n    }\n}\n\n```\nAnd setting the region there.\nWhich works fine but had this issue today with a user using v4 signature?\nThanks\n. Hi @mtdowling,\nThanks for this and this now works.\n```\n$client = S3Client::factory(array(\n         'key'    => 'key',\n                  'secret' => 'secret'\n        ));\n        $result = $client->getBucketLocation(array(\n            'Bucket' => $bucket\n        ))->toArray();\n    $region = $result['Location'];\n\n    $client = S3Client::factory(array(\n        'key'    => 'key',\n                'secret' => 'secret'\n        'region' => $region\n    ));\n\n```\nWhich is obviously not the correct way to handle this could you give a correct example.\nThanks\n. Ok great thanks @mtdowling \n. Hi @cjyclaire \nThanks for the response still dont know where i am going wrong with this i have it working to setup a rtmp distribution but i cannot seem to get this working for a web distribution, i have tried what you suggested and get exactly the same errors? \nMy bucket is definitely in the correct region. Plus the link you sent to the api is for CloudFormationClient not Cloudfront i am referencing this, \nhttps://docs.aws.amazon.com/aws-sdk-php/v3/api/api-cloudfront-2016-01-28.html#createdistribution\n```\n$CallerReference = 'testing this setup' . uniqid();\n    $Comment         = 'Added via sobytes';\n    $Bucket          = 'sobytes';\n    $Origin          = $Bucket . \".s3.amazonaws.com\"; \n$client = CloudFrontClient::factory(array(\n    'key'    => get_key(),\n    'secret' => get_secret_key(),\n    'region' => 'eu-west-1',\n    'version' => 'latest',\n));\n\n try {\n\n    $Reference = $client->createCloudFrontOriginAccessIdentity(array(\n        'CallerReference' =>  $CallerReference,\n        'Comment'         =>  $Comment\n    ));\n\n    $newref = $Reference['Id'];\n\n    $result = $client->createDistribution([\n        'DistributionConfig' => [ // REQUIRED\n            'CacheBehaviors' => [\n                'Items' => [\n                    [\n                        'ForwardedValues' => [ // REQUIRED\n                            'Cookies' => [ // REQUIRED\n                                'Forward' => 'whitelist', // REQUIRED\n                                'WhitelistedNames' => [\n                                    'Items' => ['Origin'],\n                                    'Quantity' => 1, // REQUIRED\n                                ],\n                            ]\n                        ]\n                    ],\n                ],\n                'Quantity' => 1, // REQUIRED\n            ],\n            'CallerReference' => $CallerReference,\n            'Comment' => $Comment,\n            'DefaultCacheBehavior' => [ // REQUIRED\n                'ForwardedValues' => [ // REQUIRED\n                    'Cookies' => [ // REQUIRED\n                        'Forward' => 'whitelist', // REQUIRED\n                        'WhitelistedNames' => [\n                            'Items' => ['Origin'],\n                            'Quantity' => 1, // REQUIRED\n                        ],\n                    ],\n                ],\n            ],\n            'DefaultRootObject' => '<string>',\n            'Enabled' => false, // REQUIRED\n            'Logging' => array(\n                // Enabled is required\n                'Enabled' => false,\n                // Bucket is required\n                'Bucket' => $Origin,\n                // Prefix is required\n                'Prefix' => 'logs/',\n            ),\n            'Origins' => [ // REQUIRED\n                'Items' => [\n                    [\n                        'CustomHeaders' => [\n                            'Items' => [\n                                [\n                                    'HeaderName' => '', // REQUIRED\n                                    'HeaderValue' => '', // REQUIRED\n                                ],\n                                // ...\n                            ],\n                            'Quantity' => 0, // REQUIRED\n                        ],\n                        'CustomOriginConfig' => [\n                            'HTTPPort' => 0, // REQUIRED\n                            'HTTPSPort' => 0, // REQUIRED\n                            'OriginProtocolPolicy' => 'https-only', // REQUIRED\n                            'OriginSslProtocols' => [\n                                'Items' => [''], // REQUIRED\n                                'Quantity' => 0, // REQUIRED\n                            ],\n                        ],\n                        'DomainName' => $Origin, // REQUIRED\n                        'Id' => $newref, // REQUIRED\n                        'S3OriginConfig' => [\n                            'OriginAccessIdentity' => 'origin-access-identity/cloudfront/' . $newref,\n                        ],\n                    ],\n                    // ...\n                ],\n                'Quantity' => 1, // REQUIRED\n            ],\n        ],\n    ]);\n\n} catch (Exception $e) {\n\n    echo \"<pre>\";\n    print_r($e->getMessage());\n\n}\ndie(); // dont forget\n\n```\nthanks\n. Hi @cjyclaire,\nFirst off i would like to apologies for being dumb i was using code from the new api 3 instead of 2.\nThis works thanks.\n```\ntry {\n    $result = $client->createDistribution(array(\n        'Aliases' => array('Quantity' => 0),\n        'CacheBehaviors' => array('Quantity' => 0),\n        'Comment' => $Comment,\n        'Enabled' => true,\n        'CallerReference' => $CallerReference,\n        'DefaultCacheBehavior' => array(\n            'MinTTL' => 3600,\n            'ViewerProtocolPolicy' => 'allow-all',\n            'TargetOriginId' => $originId,\n            'TrustedSigners' => array(\n                'Enabled'  => true,\n                'Quantity' => 1,\n                'Items'    => array('self')\n            ),\n            'ForwardedValues' => array(\n                'QueryString' => false,\n                'Cookies' => array(\n                    'Forward' => 'whitelist',\n                    'WhitelistedNames' => array(\n                        // Quantity is required\n                        'Quantity' => 1,\n                        'Items' => array('Origin'),\n                    )\n                )\n            )\n        ),\n        'DefaultRootObject' => 'foo.txt',\n        'Logging' => array(\n            'Enabled' => false,\n            'Bucket' => '',\n            'Prefix' => '',\n            'IncludeCookies' => true,\n        ),\n        'Origins' => array(\n            'Quantity' => 1,\n            'Items' => array(\n                array(\n                    'Id' => $originId,\n                    'DomainName' => $Origin,\n                    'S3OriginConfig' => array(\n                        'OriginAccessIdentity' => 'origin-access-identity/cloudfront/' . $originId\n                    )\n                )\n            )\n        ),\n        'PriceClass' => 'PriceClass_All',\n    ));\n\n    printf('%s - %s', $result['Status'], $result['Location']) . \"\\n\";\n\n} catch (Exception $e) {\n\n    echo \"<pre>\";\n    print_r($e->getMessage());\n\n}\ndie();\n\n```\n. ",
    "onethumb": "It's typically during unusually high periods of unexpected load, so this theory makes sense and closely matches what I was guessing might be happening.\nIf you know off the top of your head where the retry logic lives, I can play around with it (maybe even just trivially up the # of retries) and see what happens. \nWe've had cases where our DynamoDB throttling is due to partition-level throttling, rather than table-level throttling, which we have no control over, so that could be the case here, too.\n. We're also seeing the GuzzleHttp\\Promise\\RejectionException issue intermittently and haven't yet determined the root cause.\n. Oh, awesome reply, and awesome functionality.\nI rescind everything other than that the case of the args should be consistent.  :)\n. Closing, since there's a better way to handle this.  See: https://github.com/aws/aws-sdk-php/issues/712\n. Current working theory is that the resource is being cast as a string at some point, but it doesn't look like our code is doing the casting, so might be in the SDK somewhere?\n. I don't think so, we're getting & executing the command immediately, and I don't see any way it could be getting serialized.  This chunk of code is pretty simple.  If you're not able to reproduce, I'll see if I can extract the relevant bits into a standalone test case.\n. ",
    "miklos-martin": "Thanks\nBut @ has no effect now.\n. Ah, i thought the third options parameter holds the QUIET flag, my bad.\nEverything is fine.\n. Never mind, I was mistaken\n. @mtdowling, @jeremeamia Sorry for the delay.\nTake this composer.json:\njson\n{\n    \"require\": {\n        \"aws/aws-sdk-php\": \"2.5.4\"\n    }\n}\nand the script that @mtdowling provided.\nCreate(!) a bucket so the createPseudoDirectory() method will run, not just the createBucket() method\nRun the script, and you will get the following result:\n```\nPHP Warning:  A header you provided implies functionality that is not implemented in /.../aws-test/vendor/aws/aws-sdk-php/src/Aws/S3/StreamWrapper.php on line 719\nPHP Stack trace:\nPHP   1. {main}() /.../aws-test/test.php:0\nPHP   2. mkdir() /.../aws-test/test.php:8\nPHP   3. Aws\\S3\\StreamWrapper->mkdir() /.../aws-test/test.php:8\nPHP   4. Aws\\S3\\StreamWrapper->createPseudoDirectory() /.../aws-test/vendor/aws/aws-sdk-php/src/Aws/S3/StreamWrapper.php:387\nPHP   5. Aws\\S3\\StreamWrapper->triggerError() /.../aws-test/vendor/aws/aws-sdk-php/src/Aws/S3/StreamWrapper.php:831\nPHP   6. trigger_error() /.../aws-test/vendor/aws/aws-sdk-php/src/Aws/S3/StreamWrapper.php:719\nWarning: A header you provided implies functionality that is not implemented in /.../aws-test/vendor/aws/aws-sdk-php/src/Aws/S3/StreamWrapper.php on line 719\nCall Stack:\n    0.0001     228352   1. {main}() /.../aws-test/test.php:0\n    0.0175    3089912   2. mkdir() /.../aws-test/test.php:8\n    0.0175    3090760   3. Aws\\S3\\StreamWrapper->mkdir() /.../aws-test/test.php:8\n    0.0175    3091736   4. Aws\\S3\\StreamWrapper->createPseudoDirectory() /.../aws-test/vendor/aws/aws-sdk-php/src/Aws/S3/StreamWrapper.php:387\n    1.4195    5792088   5. Aws\\S3\\StreamWrapper->triggerError() /.../aws-test/vendor/aws/aws-sdk-php/src/Aws/S3/StreamWrapper.php:831\n    1.4195    5792504   6. trigger_error() /.../aws-test/vendor/aws/aws-sdk-php/src/Aws/S3/StreamWrapper.php:719\nfalse\n```\n. This can be a bit confusing:\n\nCreate(!) a bucket so the createPseudoDirectory() method will run, not just the createBucket() method\n\nApologies, my memory tricked me, I remembered that this short if: https://github.com/aws/aws-sdk-php/blob/master/src/Aws/S3/StreamWrapper.php#L384 said that if the bucket exists, create the directory, if not, create the bucket.\nBut the problem is the same anyway.\n. Now I can't reproduce this error, don't know what was that then.\nThanks\n. Hey, \nI ran into it today, with an ancient 2.7.17 sdk version, and it seems like it is having trouble with freshly created buckets only. The SDK correctly sends the PUT request to BUCKET_NAME.s3.amazonaws.com AWS will respond for a while with something like\n```\nResponse:\nHTTP/1.1 307 Temporary Redirect\nx-amz-bucket-region: eu-west-1\nx-amz-request-id: ...\nx-amz-id-2: ....\nLocation: https://BUCKET_NAME.s3-eu-west-1.amazonaws.com/.....\nContent-Type: application/xml\nTransfer-Encoding: chunked\nDate: Fri, 25 Sep 2015 13:00:57 GMT\nServer: AmazonS3\nxml version=\"1.0\" encoding=\"UTF-8\"?\nTemporaryRedirectPlease re-send this request to the specified temporary endpoint. Continue to use the original request endpoint for future requests.BUCKET_NAMEBUCKET_NAME.s3-eu-west-1.amazonaws.com......\n```\n\nThis issue resolves with time, that's why we were unable to reproduce it after a few hours/days, I guess.\n. @xaben what version of the SDK and guzzle are you using? Can it be some region specific issue? In what region is your bucket?\nI reported the same here: #273 but after a while the error was gone and I couldn't reproduce it anymore.\n. ",
    "jamesmbowler": "Also the same case for InvalidParameterCombinationException.php\n. Any incorrect parameter will trigger it, like this (Iops must be >= 1000):\n```\n$client = Aws\\Rds\\RdsClient::factory(app()->params['aws_creds']);\n$result = $client->createDBInstance(array(\n                'DBInstanceIdentifier' => 'somename',\n                'AllocatedStorage' => 150,\n                'DBInstanceClass' => 'db.t1.micro',\n                'Engine' => 'MySQL',\n                'MasterUsername' => 'asdf',\n                'MasterUserPassword' => 'password',\n                'DBSecurityGroups' => array( 'default'),\n                'VpcSecurityGroupIds' => array(),\n                'AvailabilityZone' => AWS_REGION.'a',\n                'MultiAZ' => false,\n                'EngineVersion' => '5.6.13',\n                'AutoMinorVersionUpgrade' => true,\n                'LicenseModel' => 'general-public-license',\n                'Iops' => 10,\n));\n```\nThe InvalidParameterValueException.php is just missing in some Service Directories.\n. Oh, that must be it, yeah I'm working in a framework that has an\nautoloader, thanks for clearing that up.\nOn Wed, Mar 12, 2014 at 10:46 PM, Jeremy Lindblom\nnotifications@github.comwrote:\n\nI cannot reproduce this error, and I think there is something strange\ngoing on.\nFirst, let me explain how the exceptions work in the SDK. We produce\nException classes only for exceptions that are modeled in service\ndescriptions. The RDS service descriptionhttps://github.com/aws/aws-sdk-php/blob/master/src/Aws/Rds/Resources/rds-2013-09-09.phpdoes not contain\nInvalidParameterValue. So it is correct that the exception class doesn't\nexist, but it should not cause an error. The exception factory classhttps://github.com/aws/aws-sdk-php/blob/master/src/Aws/Common/Exception/NamespaceExceptionFactory.php#L74-L102checks if the Exception class exists, and if it doesn't reverts to throwing\nthe provided default exception. In the case of the RdsClient, it should be\nthrowing Aws\\Rds\\Exception\\RdsException.\nDo you have another autoloader registered that might be interfering with\nthe normal class_exists() behavior?\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/aws/aws-sdk-php/issues/255#issuecomment-37502103\n.\n. \n",
    "benmadin": "+1\n. Hi,\nThis might not be the right place to ask for help, but coming into using the php sdk just as you release the beta, we are struggling a bit to find examples in existing code and modify them for v3. \nan example might be the s3 multipart upload: \n(http://docs.aws.amazon.com/aws-sdk-php/guide/latest/service-s3.html#uploading-large-files-using-multipart-uploads)\nwhich seems to have changed substantially. Is there any more documentation around yet for these? It's unclear how one moves from the uploadBuilder to actually upload() (for which there doesn't appear to be a method?)\nEdit: My apologies, we tried the example with 2.7 and it didn't work either, so likely there is a part of the example that we have missed...\ncheers\nBen\n. NP. The documentation at http://docs.aws.amazon.com/aws-sdk-php/guide/latest/service-s3.html#uploading-large-files-using-multipart-uploads\nhas \nphp\n$uploader = UploadBuilder::newInstance()-> blah\ngave us Fatal error: Call to undefined method Aws\\S3\\Multipart\\UploadBuilder::newInstance() in ...\nbut we were able to work around it using:\nphp\n$uploader = new UploadBuilder; \n$doer = $uploader->setClient($s3)\n    ->setSource(' blah blah\n('tis on php 5.5.18, using either 2.7 or 3.beta)\ncheers\nBen\n. OK - I hadn\u2019t seen that one! Thanks for all the help. \ncheers\nBen\n\nOn 2014-12-10, at 04:53 , Michael Dowling notifications@github.com wrote:\nYeah, we removed all instances of newInstance() as PHP 5.4+ now allows you to do this:\n(new UploadBuilder)->setClient($s3)->....\n\u2014\nReply to this email directly or view it on GitHub.\n\n\nBen Madin\nt : +61 8 6102 5535\nm : +61 448 887 220\ne : ben@ausvet.com.au\nAusVet Animal Health Services\nWestern Australia\nAusVet's website:  http://www.ausvet.com.au\nThis transmission is for the intended addressee only and is confidential information. If you have received this transmission in error, please delete it and notify the sender. The contents of this email are the opinion of the writer only and are not endorsed by AusVet Animal Health Services unless expressly stated otherwise. Although AusVet uses virus scanning software we do not accept liability for viruses or similar in any attachments. Thanks for reading.\n. Hi,\nI was having this same problem, but as per the advice from this issue changed from 3.0.0-beta1 to dev-master but now am seeing : \nUnable to open /var/www/admin/vendor/aws/aws-sdk-php/src/Aws/S3/Resources/s3-latest.php for reading\nit is true - such a file does not exist, the only file in the directory is s3-2006-03-01.php\ncheers\nBen\n. Thanks - better now! cheers Ben\n. ahh, that must be it. httpd is running as _www \nis there any 'nice' way to redirect the sdk to use a specific file - I tried optimistically to pass 'filename' in the factory as per :\n\nstring|null   $filename   Pass a string to specify the location of the credentials files. If null is passed, the SDK will attempt to find the configuration file at in your HOME directory at ~/.aws/credentials.\n\nbut I still get the same '/.aws/credentials' not found message.\n. Hi,\nWe seem to have lost track of how to instantiate a new instance - service builder no longer works for us since the latest update, with \\Aws\\Common\\Aws no longer available (replaced by \\Aws\\Sdk)\nWhen we try something like :\nphp\nuse Aws\\Ec2\\Ec2Client;\n$aws = new Ec2Client([\n    'region'  => 'ap-southeast-1',\n    'version' => 'latest',  \n    'key'    => 'AKWI49IFK3FKSDLWO3<-NOT!',\n    'secret' => '1t_i5_a_working_secret',\n    ]\n);\nWe are getting Aws\\Exception\\UnresolvedCredentialsException 'Could not load credentials'\nI'm sure we've missed something...! but I'm not even sure where in the api we find out way through this?\ncheers\nBen\n. Hi Michael,\n$ composer show -i\naws/aws-sdk-php                         3.0.0-beta.1 AWS SDK for PHP - Use Amazon Web Services in your PHP project\nbower-asset/bootstrap                   v3.3.4       The most popular front-end framework for developing responsive, m...\nbower-asset/jquery                      2.1.4\nbower-asset/jquery-ui                   1.11.4\nbower-asset/jquery.inputmask            3.1.63       jquery.inputmask is a jquery plugin which create an input mask.\nbower-asset/punycode                    v1.3.2\nbower-asset/typeahead.js                v0.10.5\nbower-asset/yii2-pjax                   v2.0.4\ncebe/markdown                           1.1.0        A super fast, highly extensible markdown parser for PHP\nezyang/htmlpurifier                     v4.6.0       Standards compliant HTML filter written in PHP\nfzaninotto/faker                        v1.4.0       Faker is a PHP library that generates fake data for you.\nguzzlehttp/command                      0.7.0        Provides the foundation for building command based web service cl...\nguzzlehttp/guzzle                       5.2.0        Guzzle is a PHP HTTP client library and framework for building RE...\nguzzlehttp/log-subscriber               1.0.1        Logs HTTP requests and responses as they are sent over the wire (...\nguzzlehttp/message-integrity-subscriber 0.2.0        Verifies the integrity of HTTP responses using customizable valid...\nguzzlehttp/retry-subscriber             2.0.0        Retries failed HTTP requests using customizable retry strategies ...\nguzzlehttp/ringphp                      1.0.7        Provides a simple API and specification that abstracts away the d...\nguzzlehttp/streams                      3.0.0        Provides a simple abstraction over streams of data\nkartik-v/yii2-krajee-base               v1.7.6       Base library and foundation components for all Yii2 Krajee extens...\nkartik-v/yii2-widget-select2            v2.0.0       Enhanced Yii2 wrapper for the Select2 jQuery plugin (sub repo spl...\nmtdowling/jmespath.php                  1.1.1        Declaratively specify how to extract elements from a JSON document\nphpspec/php-diff                        v1.0.2       A comprehensive library for generating differences between two ha...\npsr/log                                 1.0.0        Common interface for logging libraries\nreact/promise                           v2.2.0       A lightweight implementation of CommonJS Promises/A for PHP\nswiftmailer/swiftmailer                 v5.4.0       Swiftmailer, free feature-rich PHP mailer\nyiisoft/yii2                            2.0.4        Yii PHP Framework Version 2\nyiisoft/yii2-bootstrap                  2.0.4        The Twitter Bootstrap extension for the Yii framework\nyiisoft/yii2-codeception                2.0.4        The Codeception integration for the Yii framework\nyiisoft/yii2-composer                   2.0.3        The composer plugin for Yii extension installer\nyiisoft/yii2-debug                      2.0.4        The debugger extension for the Yii framework\nyiisoft/yii2-faker                      2.0.3        Fixture generator. The Faker integration for the Yii framework.\nyiisoft/yii2-gii                        2.0.3        The Gii extension for the Yii framework\nyiisoft/yii2-jui                        2.0.4        The Jquery UI extension for the Yii framework\nyiisoft/yii2-swiftmailer                2.0.4        The SwiftMailer integration for the Yii framework\nand the composer.json file has:\n\"aws/aws-sdk-php\": \"3.x@dev\"\ncheers\nBen\n. Thanks Michael,\nConfusing... If I set it on the entire composer, I have three other packages that won't load. but if I put the git commit on the sdk only, it won't pull it. \nI'll try a git pull instead.\ncheers\nBen\n. Okidoki - I'll leave it for now, I tried pulling the v3 branch straight into the vendor directory in my project, but I seem to be having some sort of problem with loading a function from the \\Aws space:\n``` php\n public static function defaultProvider()\n    {\n        $data = \\Aws\\load_compiled_json(DIR . '/../data/endpoints.json');\n    return new PatternEndpointProvider($data['endpoints']);\n}\n\n```\nis throwing Call to undefined function Aws\\load_compiled_json() \nso I'll wait. Thanks for that!\ndo I continue to use \"3.x@dev\" in composer?\n. ",
    "AlexKovalevych": "agree, i've added that checking before removing pseudo directory\n. ",
    "ranacseruet": "Ah, OK, thanks for link. I was looking into method level(not in command level), similar as \"getObject\" one on 'S3Client' class. Won't it be helpful to have a \"getObjects\" method in such way?\n. Thanks, yes, I do understand it now after you given the link on first reply. However, I was using an work around for 'GetObjects' method wrapper like this: https://gist.github.com/ranacseruet/9167580 . As there is no direct REST command(such as 'GetObjects'), my second thought was to wrap the mechanism into a single method, so developers will might be able to directly execute them instead of write those code snippet over and over again.\n. ",
    "ctjctj": "@jeremeamia it wasn't my choice.  Owncloud seems to be attempting to duplicate a unix file like structure and creates a \".\" object as part of that effort.  I've coded a work around and presented it to owncloud.  At this time I'm waiting for the Pull Request to be completed.\nUnfortunately the fact that it is a really strange object name to use it does cause an issue.  Other interfaces to AWS S3 do not have a problem creating signatures for objects with a name of \".\"\n. ",
    "angelapper": "I also face this problem using owncloud :(\n. ",
    "radarseven": "Hey Michael,\nI appreciate the response. Here is where I am confused. Using the same stream wrapper and Finder call - one without an object key and one with an object key - I get two different responses.\nIn the first case, without an object key, just using the bucket name, I get an S3 Exception error:\nAws \\ S3 \\ Exception \\ InvalidAccessKeyIdException with the message The AWS Access Key Id you provided does not exist in our records.\nIn the second case, same Finder call, but also passing an object key in addition to the bucket name, I get an instance of InvalidArgumentException with the message The \"/key\" directory does not exist. from the Finder class.\nAny ideas why I would get different responses here?\n. ",
    "jacobbednarz": "Super quick! :cake:\n. ",
    "raul782": "Let's kill it and have one minute of silence :)\n. ",
    "tgskiv": "Hello, guys.\nCan you recommend what to do for CakePHPusers that cant automatically install PHPUnit 3.7 with all dependencies.\nAny PEAR mirror? \nMaybe some packages?\n. ",
    "nodefortytwo": "Updated to version 2.6.1 the issue is still present.\n. The error was the lowercase \"key\" although this is obviously my typo a more useful error message would be good.\n. It appears 3.18.8 actually broke the generation, 3.18.7 confirmed working for me.\n. ",
    "jspizziri": "@skyzyx I'm using Gaufrette (within a Symfony2 app). The file itself is coming in via a form, however after I get the upload I grab the filecontents and write it the the Guafrette filesystem, which points at a S3 bucket.\n. Oops, I just realized I meant to file this in the aws-sdk-for-php repo. I had them both open at the same time.\nI'll close this issue, but if you have any input in that context please feel free to weight in.\n. Because I'm using https://github.com/amazonwebservices/aws-sdk-for-php? Or is it still valid here?\n. @jeremeamia , I refiled the issue here: https://github.com/amazonwebservices/aws-sdk-for-php/issues/66 but I can reopen it here if thats appropriate.\n. ",
    "jackpf": "Thanks for the response, it's definitely the right region since 99% of the time it uploads without problems but every once in a while this exception happens. I've not been able to see any pattern. Is there a way of making the logging more verbose?\n. ",
    "pulkitjalan": "I have been having the same issue recently. Using the body_as_string option did not fix the problem for me.\n. ```\ntry {\n    if (File::size($localPath) < self::BODY_AS_STRING_LIMIT) {\n        Log::info('Setting curl option \"body_as_string\" true');\n        $this->s3->getConfig()->set('curl.options', array('body_as_string', true));\n    } else {\n        Log::info('Setting curl option \"body_as_string\" false');\n        $this->s3->getConfig()->set('curl.options', array('body_as_string', false));\n    }\n$this->s3->upload(\n        $bucket, $destinationPath, $file, $this->acl, array('params' => $options)\n);\n\n} catch (Exception $e) {\n    Log::info($e->getMessage());\n}\n```\nLog:\n[2014-05-08 07:29:14] local.INFO: s3://bucket/path/file.extension [] []\n[2014-05-08 07:29:16] local.INFO: Setting curl option \"body_as_string\" true [] []\n[2014-05-08 07:29:16] local.ERROR: [curl] 65: necessary data rewind wasn't possible [url] https://s3.amazonaws.com/bucket/path/file.extension [] []\n. thanks @maknz, that seems to have fixed the issue.\n. ",
    "crozet-magenta": "Hello, I know this thread is 4 years old but I still get this issue randomly...\nit seems that 'body_as_string' cannot be used anymore with sdk version 3 (i'm using 3.52.2), so is there an other workaround ?. ",
    "jmarrero": "I will explore the other possibility and get back to you. But so far we where trying to keep using PEAR as long as posible.\n. I understand, thank you.\n. ",
    "teu": "Think it is a case of lack of fast failure with improper configuration. \nWhat we managed to figure out is the php sdk s3 factory does not fail-fast without an explicitly set region, but will just start to create wrong urls, without the region prefix (e.g. s3-eu-west-1..)\n. ",
    "jctebbal": "Thank you Michael,\nwe shall do like that.\nOn 7 May 2014 18:10, Michael Dowling notifications@github.com wrote:\n\nThe SDK creates presigned URLs based on the provided input. We, however,\ndo not send a request to the URL to ensure that the URL is correct (e.g.,\nthe region is correct, the bucket exists, key exists, you have permission\nto view the resource, etc.). This is something that the SDK is not going to\nsupport by default, but it is something that you could do in your\napplication after creating the URL if desired.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/aws/aws-sdk-php/issues/287#issuecomment-42454938\n.\n. \n",
    "Hounddog": "Actually i did recieve an error and not just malformed json... That was due to postman expecting something else. It seems cause i set a region that i recieved an error. after removing the region i was able to upload. Does this actually mean not to specify a region for s3? if yes this should propably be documented somewhere as i could not find this as of now.\n. @mtdowling thx for the information.\n. ",
    "dv1r": "Apparently Centos needs it separately\nI fixed it by installing yum install php-xml\nThank you for the help :+1:\n. ",
    "kyleferguson": "I decided just to filter results afterwards, manually.\n. ",
    "liuggio": "They should not be broken...\nThe reason is that the png is a 301 to svg version, is invisible thanks to the github proxy :)\n. ",
    "xaben": "These are the versions I encountered the issue with:\naws/aws-sdk-php (2.6.5)\nguzzle/guzzle (v3.9.1)\nBucket is in Ireland\nTested again today same computer, same install, but from a different network location and the issue disappeared.\nThe first network location was an ADSL. And now I'm on cable broadband. Both are from same ISP.\n. ",
    "hlherrera": "This error occur all the time.\nThis is my endpoint: elastictranscoder.us-west-2.amazonaws.com.\nThis is my request body: \n{\"Input\":{\"Key\":\"video\\/851\\/1bb3522d-7171-4913-84ed-66d85101f9cd.mpg\",\"FrameRate\":\"auto\",\"Resolution\":\"auto\",\"AspectRatio\":\"auto\",\"Interlaced\":\"auto\",\"Container\":\"auto\"},\"Outputs\":[{\"Key\":\"video\\/1bb3522d-7171-4913-84ed-66d85101f9cd-web.mp4\",\"ThumbnailPattern\":\"\",\"Rotate\":\"auto\",\"PresetId\":false}],\"PipelineId\":\"1400860506209-ntuzk4\"}\nI have created my authorization with my aws credentials.\nWhen I finished upload a video to s3, Im trying to create a job in elastic transcoder to automatically convert the video.\n. ",
    "cjyclaire": "@sobytes, It looks like this issue is more about your host. So, could you provide more information for troubleshooting? e.g. How often does this occurs? Is there any other host that could be connect without issue? Is your DNS working correctly?\n. @mooseh Could you provide more information about the issue? Are you seeing the same trouble with original issue? It would be nice if you could provide some code snippet that help me to reproduce the issue to see where might go wrong?\n. @josef-diazlop , could you provide more information about your problem? When did you see this happen? Is this a problem that happened after some version upgrade? In last fix, retries are enhanced in handling the original issue.\n. @josef-diazlop  , thank you for following up. Since on ubuntu works fine, and the error message is 500 on server-side, I'd consider it has less to do with SDK. Your feedback is of great value, I'd recommend aws forum for direct conversation with service team ;)\n. @dearsina Appreciate the feedback, it has been tracked in our backlog already and will be prioritize shortly I believe :).. @storage1 could you provide more information for troubleshooting? Maybe a code snippet for us to reproduce the issue?\n. @storage1, it seems that your issue is caused by uploading a directory with putOject, I'd suggest check out S3 Transfer in uploading directories.\nAlso, I'd also suggest double check that gc is enabled when you do gc_collect_cycles() ;)\n. @storage1, Apology for that's not the case, since single file situation works for me with the code provided. Taking a look into the Error message, it sounds like that stream reader fails to locate the file?\n. @storage1, AWS SDK for PHP uses the PSR-7 StreamInterface internally as its abstraction over PHP streams. information can be found here. \nI'm unable to reproduce the issue locally with your code, so I'd ask for more information. Could you check  that directory is correct, garbage collection does working correctly, and putObject works well without unlink? \nAlso you could take a look at our discussion here: #929 \n. @jgardezi Do you have security token used in your credentials? If so, could you double check it is valid? . @jgardezi It's not necessary. I cannot reproduce the issue on my side with postObjectV4. could you tell me the SDK version used? Also could you show me your http debug log when making this request? see debug,http-debug. Rework instrumentation, tests added.\nTrying to figure out PHPUnit test failure\n. Add and fix test case, rework error code.\n@jeskew \n. Rework Test & Make changes as commented.\n@jeskew \n. fix 80 wrap\n@jeskew \n. Could you tell more about when is this happening actually? Did you look into the argument that pass in to GuzzleHandler that cause the error?\n. Does this happens every time when reading fails? Are these all the errors that appears when reading fails? Could you somehow trace that exception to where it actually happens perhaps? \n. It looks like a problem of client-side address validation, could you try checking the format of email address and its verification?\n1.  http://docs.aws.amazon.com/aws-sdk-php/v3/api/api-email-2010-12-01.html#sendemail\n2.  http://docs.aws.amazon.com/ses/latest/DeveloperGuide/verify-addresses-and-domains.html\nThen can you share more about the changes you made in the config files?\n. Looks good to me.\n:shipit: \n. @dortort  Actually S3\\StreamWrapper::stream_flush does return false when copy() encounters an error, yet copy() doesn't return that failure to user. I'll looking into this issue and update.\n. @skiddle As it is an uncaught exception, I don't think there is any recent changes in php SDK relates to elastic search, did you check whether there is any update in elastic search SDK? Is there any other changes made today could cause this trouble?\n. This is resolved in Pull Request#971, PostObject is updated to PostObjectV4.\n. :shipit: \n. @enVolt  Since the error is 503, and it does work in interactive php, I'd consider it more like a no SDK issue. Yet, I'd still suggest double check the url and whether the message body would somehow break something? Also, did you get any other php errors related?\n. Make changes as comments, use traits, make src clean.\n@jeskew \n. fixed.\n/cc @jeskew \n. Add cucumber test, signature doesn't match error\n@jeskew \n. I believe it's now ready for another review. @jeskew \n. Looks sweet.\n:shipit: \n. Looks good to me.\n. Sweet. :shipit: \n. I think it's now ready for another review.\ncc:/ @jeskew \n. Fix date issue, add formInput field and rework test.\ncc:/ @jeskew \n. Rework cucumber test and timestamp etc.\ncc:/ @jeskew \n. Now it's review ready.\ncc:/ @jeskew \n. cc:\\ @jeskew \n. \u00a0Docs updated and deprecated tag marked\ncc:\\ @jeskew \n. Close this issue with PR #976 that resolves it. And now Content-MD5 is added for S3 PutBucketReplication, thanks for catching the bug.\n. looks good. :shipit: \n. The release version has been fixed, thank you.\n. Looks sweet. :shipit: \n. :shipit: \n. As error suggests, I'd recommend double check the header fields in your php code. Also at the end of mail.txt, there seems to be an extra \"--\", of course you can ignore this if it's just a typo in providing information. Meanwhile I'm working on reproducing the issue to see whether some of your headers went wrong.\n. @sattalk , in the return array of your getEmailRawData() function, could you try not to base64_encode the message? Since the file_content is base64 encode already? It could be a double encoded issue.\nReferred in PHP SDK, sendRawEmail(), 'Data' field.\nIf this doesn't work, I'd suggest double check the email_data.\n. Fixed. @jeskew \n. Thanks for point this out with a real case, yet I think in current version for our sdk, we do have a upper bound already with 20sec, same with java. Also there is a PR #937 related, and the way we do it is exactly return mt_rand(0, min(20000, (int) pow(2, $retries - 1) * 100)).\n. For Guzzle, the decode_content option is set to true by default. So, the content-encoding header will be removed from the response, thus the content-length header will has a length of 0 for gunzipped response body, which gives cURL error 61.\nFor the workaround for this, you could manually set decode_content to false in http options of client config:\nphp\n$s3 = new Aws\\S3\\S3Client([\n    'http' => [\n        'decode_content' => false,\n    ],\n]);\nOr add '@http' => ['decode_content' => false], in the getObject command.\n. Looks good.\n. There is a test for decode_content set to false, then test for overwrite that to true could be added.\n. Sweet! :shipit: \n. Looks good. :shipit: \n. @stephanlindauer , Could you provide the SDK version that you are using and parameters that passed in? Since I'm unable to reproduce this issue. As in current version of SDK, I believe we support SignatureV4 in putObject by default already.\n. Cool! \n. I can see the point in having it as a constant, yet, it seems now 'UNSIGNED-PAYLOAD' is defined in getPresignedPayload() only. \n. Thanks to all.\n. Thanks!\n. @pavankumarkatakam Good to hear about the suggestion, as PHP SDK V3 is supported with jsempath already. Here is related docs.\n. @saval, I couldn't reproduce the error with the code provided, yet your thoughts about the \"Text file busy\" error make sense to me. Could you provide more information about this scenario? Why you would need to unlock the file before its shutdown? Is there any thing particular about the file itself?\n. No problem! Glad that you worked out!\n. @IgorDePaula , May I ask the version of SDK that you're using? the Amazon Cognito Identity Provider service support is added in v3.18.0 release. Also, where did you see the description docs?\n. @IgorDePaula, Login options are passed in when you construct a CognitoIdentityProvider. API\nsample code would look like:\nphp\n$provider = new CognitoIdentityProvider(\n            'poolId',\n            ['region' => 'us-east-1',  'version' => 'latest'],\n            [\n                'www.amazon.com' => 'access-token-old',\n                'graph.facebook.com' => 'access-token-fb',\n            ]\n        );\n. @IgorDePaula Will this helps? CognitoIdentityProviderClient, CognitoIdentityProviderClient has APIs for sign-up.\n. @IgorDePaula Great thanks for your clarification! I'm contacting service team to get more information, and will keep following up.\n. @IgorDePaula , just confirmed from service team, Cognito does support web apps for developer authenticated identities workflow. Apologies for the noises on doc website, yet for this workflow, it uses the same APIs for web as on mobile. \nOnce you have the login and token, you just need to call GetId and either GetOpenIdToken or GetCredentialsForIdentity with the logins map updated appropriately, and Cognito will vend credentials for you. These APIs won't care about the source of the request at all.\nThere is a blog with details.\nThere is the public doc guide for Developer Authenticated Identities, and you can ignore the noise like\"(Android and iOS only)\" ;)\nIf all of theses still doesn't help with your question, please let me know.\n. No, they do support, it uses the same APIs for web as on mobile.\n. I fully understand your situation, I'm not suggesting using any external providers.\nWith developer authenticated identities, you can register and authenticate users via your own existing authentication process, while still using Amazon Cognito to synchronize user data and access AWS resources. And this process is in referred in the doc,\nYou need to implement an Identity Provider, then update the login map, getting a token and connect to identity. \nAs I've mentioned, once you have the login and token, you just need to call GetId and either GetOpenIdToken or GetCredentialsForIdentity with the logins map updated appropriately, and Cognito will vend credentials for you. These APIs won't care about the source of the request at all.\nThe service team also mentioned that if you have further questions about their service, they are more than happy to help you on the forum! I believe you can register the aws developer forum with ease.\n. > But by you said, I suppose that I have my own authentication process. I\n\nsignup the user on cognito and receive yout token. I storage this token on\nmy database. When this user is authenticate on my system, I verify you\nusername, password on my database and yout token on cognito. This is the\nprocess?\n\nYes, that's exactly what I'm suggesting! After you have finished authentication, you can use the provider with your authentication.\nApology for those information cause confusion so far. Since this question is more about the service instead of SDK itself, I'd suggest you can discuss it directly with service team on the forum with your aws account login, they have much patience in guiding customers walking through the usage.\nBy the way, it's confirmed that web app flow is supported, so there is no worries for the possibility. I'm going to close this issue, yet feel free to reopen if you have further questions!\n. Great thanks for your effort and we really appreciated! Since we have a separate PR which fixes Integration Test problem at the same time, and a fixed version release is out, I'm going to close this PR.\n. Thanks for notice! New version has been released with fix. Free free to open if you have further questions.\n. Thanks for notice, working on a fix release right now.\n. Thanks to all! The fixed release is out, please feel free to open if you have further questions.\n. @xibz  That's true.\n. @jeskew make sense! Thank you to all!\n. It seems that cucumber test have a typo in command passed in as well, fix has been updated.\n. Fix updated.\n@xibz @jeskew @mtdowling \n. Inject serviceIdentifier to metadata to make sure that Paginators, Waiters and docs loaded right.\n. Thanks for catching that, since in aws/aws-sdk-php-v3-bridge, SimpleDB is supported with SignatureV2. Yet current endpoint in SDK require Signature V4 for SimpleDB, that's why you would have trouble for authentication. We're working on fixing it. \n. @mvanbaak , a new release for aws/aws-sdk-php-v3-bridge is out with fix. \nOnce you require aws/aws-sdk-php-v3-bridge:0.2.0 it should work. Cheers!\n. The PR looks good to me! Thank you for catching that!\n. @davzie Any ideas? taken a second look, I'm curious as well\n. Looks good to me! Thanks!\n. @GeekLad Thanks for the information, from Guzzle, it no longer requires cURL in order to send HTTP requests. Guzzle will use the PHP stream wrapper to send HTTP requests if cURL is not installed. Alternatively, you can provide your own HTTP handler used to send requests. Guzzle Docs\n. That's true ;). Feel free to reopen the issue if you have further questions.\n. Thanks for the pull request (and conversation :) ). The intent is to list the AWS-provided projects there; I've updated the README to make that clearer. PR: #1006\n. LGTM, Thanks for catching! \n. Thanks for catching! Looks good to me.\n. @mariusgrigaitis, $sqs->sendMessage() delivers a message to the specified QueueUrl. ~~For configuration in Client Endpoint, these are the endpoints supported in SQS.~~\n. @mariusgrigaitis , thanks for clarification. Double checked with boto team, so both our SDKs use default service behavior. And with your code snippets, endpoint is configured in client correctly and sqs works fine. \nAWS endpoint is the entry point for web service to make the request. I couldn't reproduce any issue, could you provide more information? For example, what boto behavior did you observe? \n. @mariusgrigaitis thanks for the information! Yet I still couldn't reproduce the issue, I can see that request is made from customized endpoint. \nMeanwhile, the error is thrown at sendMessage() when making the request, referred in your error message: 403 Forbidden,  ExpiredToken (client): The security token included in the request is expired, could you double check your credentials? \nAlso, POST http://sqs.us-east-1.amazonaws.com/123456789012/test is the correct behavior for sendMessage(), which is also the same behavior for python.\n. @mariusgrigaitis ,thanks,  I'm able to reproduce the issue right now, and working on investigation.\nSince my error is different from the credential error that you received, and as you mentioned, credentials are not checked by the faked endpoint, could you try add 'credentials' => false in client configuration? I'm proposing this because the SDK tries to source credentials from the environment to sign request by default, I'm curious whether disabling this behavior would help?\n. @mariusgrigaitis It's true that PHP and Python behave differently. This is because in PHP SDK, we always use the QueueUrl, while in boto, they 'override' the QueueUrl with endpoint_url.\nI've reached out to service team to confirm this behavior. The QueueUrl provided by the service should be taken as the canonical source of truth should the endpoint ever change or be diverted for whatever reason, such as DNS CNAME changes, or queue domain migrations (quite unlikely). It is the unique location for that queue, and using the hardcoded endpoint in an SDK URL in pythons case acts like an 'override'. So PHP would stay the same.\nProbably, the fake end point should return a fake endpoint url instead of a real one:\njson\n\u279c  /tmp aws --endpoint=http://192.168.99.100:81 sqs create-queue --queue-name test\n{\n    \"QueueUrl\": \"http://sqs.us-east-1.amazonaws.com/123456789012/test\"\n}\nThank you so much for your patience and respondence!\n. So, I'm going to close this issue, yet free free to reopen the issue if you have further questions. ;)\n. @vmpj you're on the right track! ~~yet the config should be Content-MD5 instead of ContentMD5~~. $image should be under SourceFile\n. @genomachino17 Thanks for bringing this up! \nOriginally, iterator is mapped in createDownloadPromise(), a similar issue popped up then $command[] is applied as a soft fix. #917 \nSince enhancement has been made in guzzle/promises, perhaps I'd revert the command array change after reproducing this issue and fully testing it.\n. @genomachino17 Does the original iterator method resolve this issue? Since I couldn't reproduce the old issue to ensure that the guzzle enhancement is enough to bring iterator back. Any updates?\n. I'll close this for no follow-ups available, yet feel free to reopen with any questions or updates :)\n. Since SDK V3 has this customize option for retries already, thank you for this PR for v2.8!\n. @hanoii , since this PR introduces new logic in configuration, could you add some tests as well?\n. Great thanks for your efforts in contributing the test! \nCould you also add another test for default retry behavior without 'client.backoff.retries' configured? \nIt's true that each \"client\" has its own builder, yet S3 and DynamoDb are configured with $retries beside default behavior, thus it would be nice to have both of them tested. \n. Awesome, it looks good to me. I'll merge it.\n. @hanoii, I'd also consider the networking might be causing this error, while there is limitations in networking enhancing and SDK version upgrade, it seems like working with retry configuration is not a bad choice.\nIf you think there could other factors that might causing this trouble and could be reproduced, I'd more than happy to help to dig more into the issue :)\nThen, for multipart upload, it's true that a part is in the range from 5MB to 5GB, 10000 parts per upload.\n. @hanoii, Cool, that's true, yet in SDK, by default, there is limit in the min size of file part due to performance consideration.\n. I'm going to close this issue while keep PR #1021 open. Feel free to reopen this issue if you have further monitoring information :)\n. @hanoii , so just to confirm the information, are you still on SDK v2.8 and PHP 5.3? Could you provide more information on SDK related usage? (Still with same file multipart upload issue?) Could you provide some code snippets that might help reproduce the issue? Or will you consider it to be a service side issue that I might communicate with service team?\n. @hanoii thanks for the information inputs, good to know the response details.\nWith Error 400, it might have to do with malformed syntax, any thoughts on this perhaps? It's interesting that some 400 bad request could make it to 200 after retries without making changes while some end in 403. Could you double check your credentials and access with this file just in case? May I know which guzzle version is used? Could you try update guzzle version as well? In looking more into the wire, could you try request.option here with parameters like on-stats (Not sure your guzzle version for this)?\nAnother quick question for this issue, are those retries happened with this particular 900mb files only? Or this happens with whatever is uploaded?\n. @hanoii thanks for the information, sounds it has more to do networking, since socket connection made to server-side is failed due to timeout, I couldn't think of how service team could help look more into that, however, please feel free to ask questions on AWS forum if you would like to communicate with service team directly. \nI understand that those retries are frustrating, yet there is nothing on SDK side could help with that. \nMore information about configuration for socket timeout in v2, it defaults to 60 seconds and is configurable via the default_socket_timeout in php.ini setting.\n. @hanoii S3 respond with that error when a client hasn't sent any bytes for 20 seconds. Since retries have been exhausted and error still surfaced, you might be experiencing a lot of contention on your local network.\n. @hanoii Apology for the confusion, I'm afraid that v2 is using guzzle/guzzle (Guzzle v3), which is abandoned. \nI understand it might be difficult for you to move to SDK v3 right now, yet I strongly recommend doing so in the long run, SDK v3 has more flexible configurations both for retry and http stats, it uses guzzlehttp/guzzle (Guzzle v6) instead, which introduces up-to-date fix and enhancement.\n. @rips-hb, Since xml that doesn't conform to the published xsd schema and fails the validation, I'd suggest some work around. This is caused by improperly encoded path, since the file name contains special characters, could you try '/' .  rawurlencode(SPECIAL_FILE_KEY) for those files?\n. @rips-hb ,Since an XML parsing exception due to unescaped XML characters in the XML response is a server side issue that cannot be fixed by the SDK, so it would be great if you could let the Amazon S3 team know that this is important to you by posting to their forum. Great thanks for your feedback!\nI'll going to close this issue, yet feel free to reopen if you have further questions :)\n. @joecwallace Thanks for the information!\n$source (except streams) passed to a MultipartUploader will be automatically rewound before uploading. The UploadState object can be used to resume an upload that failed to complete. It will only attempt to upload parts that are not already uploaded. \nWhen tracing the file when exception occurs, you could try $e->getFile()\nMeanwhile, since SDK V3 is used, I'd suggest using Transfer Manager instead of uploadDirectory()(Though we still support it ;) ).  Configuration for Transfer Manager is more flexible in many ways.\n. @joecwallace That's true, thanks for follow-up clarification, I'd tag it as a feature request, it would be nice if SDK could provide more information in S3 Transfer Exceptions, and will work on it soon.\n. Closing, tracking this with PR under review instead :).\n. Use PR #1038 to track instead\n. @adamjimenez , Could you provide the SDK version that is using? There is update in last release on EC2 models.\n. @adamjimenez , I see. Could you try wait() on the promise? For example:\nAfter you create InternetGateway like:\n$promise = $client->createInternetGatewayAsync([/* ... */]);\nThen you could force wait on the promise until it completes:\n$promise->wait();\n. Awesome\uff01\n. @sattes-faction , thanks for bringing this up. It's true that SDK stream wrapper doesn't has the touch() function right now, here is what are already supported.\nBefore considering this feedback as a feature request, could you provide more information about the use case? There should be some work-arounds available with direct S3 client operations. So, could you share more about benefits of having touch() in stream wrapper?\n. Cool, thanks for the follow-up information.\nClosing, yet feel free to reopen if you have further questions :)\n. @cjunge-work , could you provide more information about what SDK behavior related to this? Are you suggesting configuration of a S3 client with guzzle client handler perhaps? Or you are suggesting some guzzle request feature? \nFor guzzle issue not SDK related, you could also open issue at Guzzle repo here.\n. @cjunge-work , it's true that SDK doesn't expose the Guzzle handler while provide handler options itself.\n\nI wish to add a caching layer to the Guzzle requests, using a Guzzle 6 middleware\n\nMay I ask more details about this \"caching layer\" that you are expecting? Then I could try provide some work-arounds there or tag it as a feature request in our backlog. Also we can figure out whether it should be a guzzle feature or a SDK feature to work on.\n. Thanks for the information, I'd tag this as a feature request.\n. @cjunge-work, More information on @kkopachev suggestions, you could probably try http_handler configuration in SDK Client.\n. @cjunge-work Glad that it works! Closing, feel free to reopen if you have further questions ;)\n. @robgil, thanks for catching, SDK does maintain V2, yet strongly recommends migrating to V3.\nWorking on the fix, will follow up with PR.\n. @robgil In v2, SDK doesn't provided region endpoint list or expose that, and resource files are deprecated and no longer used. It's a feature of v3 that supports new endpoint.json file and introduces PartitionEndpointProvider.\nThere is no changes that SDK v2 can make to enable a region support without breaking backwards compatibility. The eu-central-1 should be already rendered since it's supported. Apologies for the confusion, closing and please feel free to reopen if you have further questions.\n. It's now ready for another review. @mtdowling \n. @ovr , notice that tests are failing, also PHP SDK relies on the guzzlehttp/promises for its promises implementation. I'm curious, may I ask which editor is used? Intellij doesn't show this.\nClosing, yet feel free to reopen if you have further questions :)\n. Interesting, good to know, thank you.\n. I'll follow up with a new commit with more documentation for caching credentials.\n. @harveyslash , that's an interesting feedback to know, thanks. \nFirst of all, SDK generates the presigned-URL on the client side, and leave verification job to s3 server as you mentioned. Server-side verification requires credentials and bucket/key information etc., which I couldn't see how client-side may help with that. If the Url has been tampered, server-side verification will fail.\n. Closing due to ages, yet please feel free to reopen if you have further questions or thoughts :)\n. Closing, in SDK V3, composer.json uses guzzlehttp/guzzle instead of  guzzle/guzzle, yet feel free to reopen if you have further questions :)\n. @n0rbyt3 , could you try $s3->waitUntil('BucketExists', ['Bucket' => $bucket]); after create bucket?\n. @n0rbyt3 Thanks for the information and explanation!\nIf HeadBucket returns AccessDeniedError, it doesn't make sense for SDK to allow createBucket without enough permission. It's true that there is Api behavior change because SDK V3 exposes 4XX errors to users for troubleshooting intention, yet it doesn't hurt usage, avoids using $accept403, checking false value instead makes more logical sense, perhaps open a PR in knplabs/gaufrette could fix that.\nif (!$client->doesBucketExist($bucket)) {\n       // or try catch exception here\n        $client->createBucket($bucket);\n}\n. Closing due to ages, yet feel free to reopen if you have further questions :)\n. @eric-tucker Appreciate your feedback, shared configuration(including AssumeRole) across SDKs is already in out backlog, which will be in the implementation queue soon!\n. Ping updates here, PR #1077 is opened in supporting config profile as well as assume role.\n. Closing, tracking the PR in progress instead.\n. @cjunge-work , thanks for catching that!\n. @sprosov I couldn't reproduce the error locally with the code snippet provided with normal files.\n\nI disabled 'decode_content' option in client initialization and in getCommand call, but that didn't help. \n\nAre you suggesting that the file causing the issue is .bin file using .gzip? Could you try turn on debug to provide more information about the error?\n. We have talked about the PR offline, closing, tracking with #1140 instead. @timothy-r , Thanks for your efforts in this PR, the intention for reusing the signature library is totally understandable, however, from SDK and security perspective in maintaining the AWS repository, it's not appropriate to allow validation from other application operations other than AWS usage. Enabling certain features with future maintenance is beyond AWS SDK scope.\nSince the signature v4 algorithm is public with AWS docs, feel free to look at our implementation or other third party libraries in terms of signature only.\n. Thanks for the discussion going on, the behavior observed is true for dynamoDB, and as dynamoDB suggests, LastEvaluatedKey value will be presented if the operation did not return all matching items in the table, then use ExclusiveStartKey in next request and repeat. API reference see here. The way that how Java SDK handles this with query() seems to be making requests lazily in the same way.\nPerhaps the best way to handle this also has to do with your data, if data size is not huge, simple way like single query request with filter then perform limit locally might works better.\nIn customizing searching results, you might also be interested in JMESPath pattern in PHP SDK.\nYour feedback is valuable to us, feel free to communicate with DynamoDB service team directly on AWS Forums if you like ;)\n. Closing due to ages, yet feel free to reopen if you have further comments or questions :)\n. @msencenb , the way you configure 'handler' option looks good to me, ~~could you try also add the 'region' and 'version' information here?~~\n\nand after digging into the code, it appears that the S3Client.php constructor doesn't accept the handler parameter.\n\nCould you explain a bit more about how you find that?\n. @msencenb thanks for the information, it looks like a aws-sdk-php-laravel issue actually.\nDiving deeper into the issue, because a Sdk object is constructed before calling createClient(), which contains default configuration for http_handler of a GuzzleHandler. Since http_handler supersedes any provided handler option, passing handler option later when constructing a S3Client will not work. \nApplying the mock at handler list is a sufficient workaround because it force the HandlerList using a MockHandler after a S3Client is initialized.\n. @caschbre , thanks for reporting this,  it is using 'GET' in retrieving results, I'll tag it as a feature request of adding option in configuring 'POST' when performing the search. I'll keep following up in this issue thread.\n. Closing, PR fix #1045 is merged, feel free to reopen if you have further questions or comments :)\n. Thanks for reporting, appreciate that! Working on investigating and fix.\n. The  Fix PR is merged and will be tagged in tomorrow release, thanks!\n. @Dayjo 3.18.33 containing this fix is out, thank you for the patience :)\n. @stevenmunro apology for the trouble, and thank you for reporting. Yet may I ask the error message when simplexml is disabled? When I was trying to reproducing the issue, lacking simplexml would result in 400 Bad Request which will cause the script failure and this 400 Error will not be retried by SDK. And since the request is failed, it won't arrive at server-side. \nSo, could you provide more details about your testing case then we could dig more into this issue?\n. @stevenmunro still unable to reproduce, could you run the compatibility-test.php file to see the output? SDK already has check forsimplexml here, if not installed, will output messages accordingly.\n. The PR contains fix is merged in, and will be tagged in today's release.\n. @neophyt3 , the value passed in need to be appropriate for use with the algorithm specified in the x-amz-server-side\u200b-encryption\u200b-customer-algorithm header. You don't need to inject the header by yourself, you just need to make sure the SSE-C value is valid.\nAPI doc here.\nAWS Public documentation about the algorithm here.\n. @zelding , I couldn't reproduce the issue with V3, could you use try-catch block to catch the exception then get the exception message by using getMessage() to get more information?\nAlso, may I ask which SDK version are you using? Is the AmazonSDK referring \\Aws\\Sdk() actually?\n. Uh, interesting, I still couldn't reproduce, region and version are required parameters for any AwsClient. Out of curiosity, could you try creating an SNSClient by:\n$sns = new Aws\\Sns\\SnsClient([\n    'region'  => 'eu-central-1',\n    'version' => 'latest',\n    'scheme' => 'http',\n]);\nWill that also produce same error message?\n. @zelding When I'm trying to reproduce:\n```\nrequire DIR . \"/../vendor/autoload.php\";\n$sns = new Aws\\Sns\\SnsClient([\n    'region'  => 'eu-central-1',\n    'version' => 'latest',\n    'scheme' => 'http',\n]);\nvar_dump($sns->getConfig());\n```\nor by\n```\n$sharedConfig = [\n    'region'  => 'eu-central-1',\n    'version' => 'latest',\n    'scheme'  => 'http',\n];\n$sdk = new \\Aws\\Sdk($sharedConfig);\n$sns = $sdk->createSns();\nvar_dump($sns->getConfig());\n```\nboth give correct output \narray(3) {\n  [\"signature_version\"]=>\n  string(2) \"v4\"\n  [\"signing_region\"]=>\n  string(12) \"eu-central-1\"\n  [\"signing_name\"]=>\n  string(3) \"sns\"\n}\nMay I ask is this \"Missing client configuration error\" the only error thrown? Do you get same error message when creating other service client? \n. @zelding thanks for the updates, so are you still using same configuration parameters in getting those errors?\nFrom the error message, it seems to have trouble with 'Endpoint'? SDK configuration takes 'endpoint' parameter.\n. @zelding Soft ping~ any updates so far?\n. No problem, thanks!\n. @deviantintegral , thank your for the feedback! \nThis method is marked private since it's called when computing the value for x-amz-expires field which needs to be included in the query string part of a pre-signed URL. I don't feel it appropriate to make this method alone public or a trait since from SDK side, it just lives inside the signature lifecycle.\nMeanwhile I totally understand your need, instead of calling the same method, how about parsing the pre-signed URL instead? \nSince you could get a pre-signed URL(doc here), then you could also taking advantage of \\GuzzleHttp\\Psr7\\Uri class where you could call getQuery() to get query string from a Uri object with ease. After that, extracting the value for X-Amz-Expires would work.\nThoughts?\n. Notice that the failing is due to \\Error class is available in PHP7 only, working on work-arounds.\n. CI is passing now and ready for another review. @mtdowling \n. @jarrettj It's correct that SDK would use the servers Role if no credentials were passed in, no env variable is set and ~/.aws/credentials is set. Could you double check that there is credentials attached with your instance correctly?\nTo see the key associated with your instance, you could execute this commandLine\ncurl http://169.254.169.254/latest/meta-data/iam/security-credentials/<role name>\nTo get the role name, you can run the following:\ncurl http://169.254.169.254/latest/meta-data/iam/security-credentials/\n. @bestis thank you for pointing this out, I just open a PR contains the fix, a new release will be out containing the changes once this PR is merged.\n. Closing, fix PR is merged and will be tagged in tomorrow release.\n. Interestingly, in original commit for the changes #1053 , using instanceof causing CI failing for PHP version < 7, that's why checking condition is added. I'm trying to figure out why.\nEdit:\nThe original failing is caused by creating an \\Error object in unit test without checking existence.\n. This PR has been updated and ready for another review. @mtdowling @xibz \n. @awsdev543 Thanks for reporting, I could reproduce the issue in both PHP and Ruby SDK.\nSince this is a newly released API and SDK doesn't customize it, I'm reaching out to service team internally for feedback and solution.\nYou could communicate with service team directly on AWS Forums as well if you like.\n. It looks like others are struggling with same issue here.\n. @awsdev543  Service is working on fixing the inconsistency between docs and API, current workaround is to pass all keys in UPPERCASE in auth parameters:\n...\n'AuthParameters' => [\n            'USERNAME' => 'xxx',\n            'PASSWORD' => 'xxx',\n        ],\n...\n. Closing, I'll pin the thread when doc fix is out, yet please feel free to reopen if you have further questions.\n. Guzzle Version\nIn SDK V3, composer.json uses guzzlehttp/guzzle instead of  guzzle/guzzle. You need upgrade to SDK V3. \nClosing, yet feel free to reopen if you have further questions.\n. @sobytes To begin with, may I ask which SDK version is used?\nFirst of all in constructing client\n\n$client = CloudFrontClient::factory(array(\n        'key'    => get_key(),\n        'secret' => get_secret_key()\n    ));\n\nwon't work due to missing configuration parameters, you need to do something like below after setting credentials correctly (if you are using SDK V3):\n$client = new CloudFrontClient([\n    'region' => 'us-west-2',\n    'version' => 'latest',\n]);\nThen the following output indicate missing or incorrect parameters in performing Api calls. You could get parameter details from our API Doc ~~here~~.\nIn troubleshooting each operation, I'd not suggest put everything inside one try-catch block instead of trying to locate the problem step by step when performing API calls.\n. @sobytes Apology for the wrong link, here is a quick catch:\n$Reference = $client->createCloudFrontOriginAccessIdentity([\n    'CloudFrontOriginAccessIdentityConfig' => [\n        'CallerReference' =>  $CallerReference,\n        'Comment'         =>  $Comment,\n    ]  \n]);\nMissing CloudFrontOriginAccessIdentityConfig will cause error start from$Reference.\nI recommend break this single huge try-catch block into step by step procedure(with smaller try-catch block), then fix missing or invalid parameter error step by step.\n. No problem at all, glad that you worked it out! \nClosing, yet feel free to reopen with any further comments or questions :)\n. @schristiaans , you could try catch SnsException by Aws\\Sns\\Exception\\SnsException, then call getAwsErrorCode() to get the error information.\n. @schristiaans No problem at all :) , I agree that our documentation should make it clear on exception handling. And I'll open a PR to address that in our SDK guide shortly\n. @schristiaans apology for not pointing this out at first place, but I think you need to remove the \"Exception\" in case condition, something like:\nphp\ntry {\n    $this->snsClient->publish(...);\n} catch (SnsException $e) {\n    switch ($e->getAwsErrorCode()) {\n        case 'EndpointDisabled':\n        case 'NotFound':\n            /// do something\n            break;\n    }\n}\n. Closing, tracking it with the opening PR, yet feel free to reopen with further questions :)\n. @samukt May I ask which SDK version is using? This PhoneNumber feature comes in since v3.18.22, our api doc here, which is same with javascript SDK.\nThe doc link suggests v2, we do maintain SDK v2 with bug fix, yet we no longer provide API feature update for SDK v2.\n. Thanks for the response, closing, yet please feel free to reopen with further questions :)\n. @deviantintegral Thanks for the suggestion! I'll make changes in another commit.\n. Closing, yet feel free to reopen if you have further questions.\n. @sureshcsk I just tested V3.18.37 and couldn't reproduce issue with putObject() in ap-south-1.\n\nbut the path of Sdk.php of aws sdk is in ...\n\nThis file path looks good to me, is there any concerns with that?\n\nNot able to putobject and there is no echo after putobject; ...\n\nIs there any error message or exceptions thrown? Could you double check that the bucket is available and your client is initialized without problem?\nUsing same configuration parameters as your putObject, I can call putObject and receive expected response.\n. @sureshcsk Could you try to var_dump() the response for PutObject? Does the response make sense? \n. @sureshcsk Are you suggesting var_dump($result) gives an empty array?\nCould you try add 'debug' => true in S3 Client configuration to see what's going on wire?\n. @mdevine82 , in using Aws\\S3\\S3MultiRegionClient, you don't have to include region parameter in constructing the client. \nMore docs about S3MultiRegionClient.\n. @mdevine82 S3 multi-region clients maintain an internal cache of the regions in which given buckets reside. By default, each client has its own in-memory cache. \nTo share a cache between clients or processes, you could supply an instance of Aws\\CacheInterface as the bucket_region_cache option to your multi-region client. doc\n. @mdevine82 Yes, it will.\n. @mdevine82 I couldn't reproduce the issue, here is the code when I trying to reproduce:\n```\n$s3Client = new Aws\\S3\\S3MultiRegionClient([\n     'version'     => 'latest',\n]);\n$bucket = 's3-bucket-sandbox-test';\n$region = $s3Client->determineBucketRegion($bucket);\ntry {\n    $key = rand(1,10000) . '.txt';\n    $response = $s3Client->putObject([\n        'Body' => '12345',\n        'Bucket' => $bucket,\n        'Key' => $key,\n        '@region' => $region\n    ]);\n    var_dump($response);\n$cmd = $s3Client->getCommand('GetObject', [\n    'Bucket' => $bucket,\n    'Key' => $key,\n]);\n$request = $s3Client->createPresignedRequest($cmd, '+30 seconds');\n$presignedUrl = (string) $request->getUri();\nvar_dump($presignedUrl);\n\n} catch (\\Aws\\S3\\Exception\\S3Exception $e) {\n    echo $e->getMessage();\n}\n```\nIt produces correct response and signed URL. Then with the signed URL, file could be retrieved correctly.\n. @mdevine82 Interesting, so in terms of determineBucketRegion(), it actually makes a HeadBucket request to determine the bucket region. Could you double check the bucket permission in used?\nAsking on the AWS forum is how you could talk with service team directly(It seems like you already do so).\n. @mdevine82 , thanks for getting back, it looks like a credential problem. At first I thought the hard-code credential configuration in code snippet is shown as testing purpose, I should have mentioned better credential practices(env, profile, IAM etc.).\ndetermineBucketRegion is a PHP SDK customized method and getBucketLocation is a standard S3 API, I'll try to reproduce your case to see if determineBucketRegion doesn't catch credentials in the first place.\n. @mdevine82 I couldn't produce issue when trying to handle credential with environment variables or hard coded credentials with both method. If no credentials provided, Aws\\Exception\\CredentialsException is thrown.\nIn looking into the CredentialProvider being used, you could call $s3Client->getCredentials(), if there is problem with your credentials, Exception should be thrown.\n. @mdevine82 That's weird, apology for this late reply, yet is there still trouble with determineBucketRegion() even with correct credentials? Could you provide some console output here so that I could see what might go wrong?\nHaving 'debug' => true in client configuration and using determineBucketRegion, what's the getCredential() output and the wire log output? Also it would be nice if you could provide error message again to see if anything changes and double check that it's determineBucketRegion causing the trouble.\n. @mdevine82 Could you try to update your CA bundle and make sure that it lives in correct location? Instructions can be found here. \nFrom the wire log, both shows that Hostname in DNS cache was stale and\n none CApath: /etc/ssl/certs * SSL connection using ECDHE-RSA-AES128-GCM-SHA256. \nThis didn't trigger an error for getBucketLocation because it's a standard S3 API call with GET, which means it has complete life cycle containing 'signer' step that uses AWS Signature V4. \nYet in determineBucketRegion when performing HEAD, 'signer' is removed from handlerList due to performance concerns, in this way, certification fails to SSL CA bundles sole with cURL (where resolve() depends on). That's why HTTP/1.1 403 Forbidden is exposed.\nHope this helps.\n. Ping here, the PR contain the fix just merged, will be tagged in our release today.\n. @Pierozi Thanks for catching! Looks good to me, I'll merge it.\n. @jeremeamia That's a fair point, I'll add some release note around that in next release with a minor version bump, thanks for catching!\n. Closing, new release with minor version bump is out.\n. @jeroenbaas I couldn't reproduce a memory leak issue with the code you provided, the memory usage looks normal to me. In the while-true loop, I can observe memory usage increase and free-up normally.\nDo you have garbage collection enabled? I recorded memory usage cycling from 3 MB to 8 MB and then back again after garbage collection completed.\nWith garbage collection disabled, memory usage will slowly climb until it reaches the configured limit and a fatal out-of-memory error is thrown.\nMay I ask the cURL and Guzzle version that is being used? \nI notice that ssl.certificate_authority is configured, are you referencing issue #957 ?\n\n'ssl.certificate_authority'=>false,\n\nIn v3, you need to do 'http'    => [\n        'verify' => false\n    ] instead.(Doc)\n. @jeroenbaas I see, it's not 404 that requires the garbage collection, it's this looping might requires the gc actually. The pattern observed is similar in regardless of the 404 error.\n. @jeroenbaas Interesting. Without the block causing 404 error, I still got that pattern, yet the increasing rate is slower:\n1 3685784\n2 3712640\n3 3738824\n4 3766288\n5 3792472\n6 3818656\n7 3844840\n8 3871024\n9 3897208\n10 3929024\n11 3955208\n12 3981392\n13 4007576\n14 4033760\n15 4059944\n16 4086128\n17 4112312\n18 4138496\n19 4164680\n20 4194960\n ...\nWith 404 error:\n1 3788472\n2 3878232\n3 3969240\n4 4058968\n5 4148696\n6 4238424\n7 4333784\n8 4423512\n9 4513240\n10 4602968\n11 4692696\n12 4782424\n13 4872152\n14 4965976\n15 5055704\n16 5153624\n17 5243352\n18 5333080\n19 5422808\n20 5512536\n...\nI also tested with http verify set to false, which makes no difference to me.\nMay I ask the cURL and Guzzle Version being used?\n. @jeroenbaas Thanks for the information, looks like you're using a EC2 instance, could you provide the instance type? Then I could try to reproduce the issue there?\n. Closing due to ages, yet please feel free to reopen if you have further question or comments :)\n. @fideloper Thanks for reporting! I can reproduce the issue, working on the fix.\n. @fideloper PR contains the fix is out and under review.\nI looked into the Documentation confusion here. Each API version block in SDK documentation class page is automatically generated based on the serviceFullName based on API models. Both ElasticLoadBalancing and ElasticLoadBalancingV2 uses full name as ElasticLoadBalancing, that's why they showed up here (since V2 is a newer model). Working on the doc fix right now.\nFortunately, detail method page with instructions is correct here [1] [2]. I'll add more documentation in PHP SDK guide for it.\nApology for the confusion and thank you for your patience :)\n. PR containing Doc fix and endpoint fix has been merged, this fix will be tagged in today's release.\n. Release contains the fix is out, closing, yet feel free to reopen if you have further comments or questions :)\n. @mimarcos Thanks for the information. To help us look more into this, may I ask the SDK version, Guzzle version and cURL version being used? \nAlso, could you provide some wire log with instruction here when the error happens?\n. @mimarcos I see, that's true, openSSL emits error 104 (ECONNRESET) when the connection is reset by a peer host, and this error is happening while cURL is attempting to read data. cURL is therefore surfacing this as a read error (cURL error 56 - CURLE_RECV_ERROR).\nIt sounds similar to issue #924 here, could you check the openssl version and CA bundle being used with instruction provided there? I couldn't reproduce the issue locally, I understand that it might be tricky to get more details about a failing request, yet it would be very helpful if you could. If you are still consistently seeing this issue, I'd be more that happy to engage the service team on this to see if they could help.\n. Closing due to ages, yet please feel free to reopen with further comments or questions :)\n. @mwleinad , your error message indicates InvalidParameterValue, there appears to be invalid ReceiptHandle, could you double check that? Should that be ReceiptHandler instead? Are you also seeing the cURL error there? Looks like your problem is different.\n. @mcfedr AwsException is expected to represent an exception that is thrown when a AWS command fails at runtime with a failed request. Its context is part of design decision made by different AWS service models as you can see here. Then, CouldNotCreateChecksumException is an exception thrown when the request body is not seekable, where the request should fail immediately and should not be made to server-side until the issue has been fixed.\nIn working around your case, you could check more general exception first, then checking if the exception is an AwsException later.\n. Closing due to ages, yet please feel free to reopen if you have further question or comments :)\n. @jeskew Make sense! This test case is added in the latest commit.\n. @tiagobrito Could you share more about the generator mentioned and what do you expect to do with each object?\nI'm guessing that you are talking about the generator object that MultipartUploader returns or a CommandPool. The usage of both allows flexible customization, so it would be nice to shared more about your needs first before we go further.\n. @tiagobrito Apology for the late reply on this.\nIn terms of enhancing the performance of 'scan' API, first you could try the ScanFilter parameter in filtering the result. Then, 'Limit' parameter would also be helpful. Since LastEvaluatedKey is included in each request result,  using that value as ExclusiveStartKey in the next call and repeat could do the work. With those approaches, the number of items return per call could be reduced.\nAlso in searching Result object, you might also be interested in JMESPath.\n. Closing due to ages, yet please feel free to reopen if you have further question or comments :)\n. @eric-tucker yes it does. With AssumeRoleCredentialProvider, by passing assume_role_params with MFA configurations will do the work.\n. We have talked about this PR offline, and a lot of refractor is done regarding the complexity there. \nI create a separate AssumeRoleCredentialResolver to resolve some chaining logic when applying assumeRole credential from profile. This also make it more flexible if customer wants to utilize part of resolving logic in credential chain.\nI think this PR is ready for a second review now. @mtdowling \n. We have talked offline with the behavior there. Refractor is done and documentation for this behavior is added. Ready for another review :) @mtdowling \n. Ecs fix has been separated from this PR and this PR has been rebased and up-to-date.. We have discussed this offline, considering Resolver for profiles could introduce fair latency in fetching credentials, this part is separated from PR #1137 . @mtdowling Of course, here are some examples:\nafter model changes introduced, git status:\n```\nChanges not staged for commit:\n  (use \"git add ...\" to update what will be committed)\n  (use \"git checkout -- ...\" to discard changes in working directory)\n    modified:   src/data/application-autoscaling/2016-02-06/api-2.json\n    modified:   src/data/cognito-identity/2014-06-30/api-2.json\n    modified:   src/data/dynamodb/2012-08-10/docs-2.json\n    ...\n\n```\ngit status --porcelain gives:\nM src/data/application-autoscaling/2016-02-06/api-2.json\n M src/data/cognito-identity/2014-06-30/api-2.json\n M src/data/dynamodb/2012-08-10/docs-2.json\n ...\nrun make changelog then CHANGELOG.md will look like:\n```\nCHANGELOG\nnext release\n\nAws\\ApplicationAutoScaling - Added support for latest API updates.\nAws\\CognitoIdentity - Added support for latest API updates.\n ...\n```\n\nThis make changelog command will automated generate release notes for API related changes.\nI won't add support for detect new service at this point, because new service model change requires separate build command, which also need manual inputs. I'd like to have this merged as a start point of testing some basic release automation.\n. @mtdowling Actually, I agree I'd like to see that happen instead of general message here (how other SDK handle their release notes currently). I think I'll work on that before merging this PR :)\n. Closing, now the information could be accessible in a different way.. @enVolt , I couldn't reproduce the issue based on your code snippets as following:\n``` php\n$send_msg_config = [\n    'DelaySeconds' => 15,\n    'MessageAttributes' => [\n        'Attribute1' => [\n            'DataType' => 'String',\n            'StringValue' => 'attr_value',\n        ],\n    ],\n    'MessageBody' => 'I am testing',\n    'QueueUrl' => $queueUrl,\n];\nwhile (TRUE) {\n    // !NOTE: I'm doing this for testing purpose,\n    // This might cause bill expense for your account\n    // send message periodically with delay\n    try {\n        $p = $sqs->sendMessage($send_msg_config);\n    } catch (\\Exception $e) {\n        echo $e->getMessage();\n    }\nlogInfo(\"Trying to get Messages\");\n$result = $sqs->receiveMessage([\n    'QueueUrl'            => $queueUrl,\n    'WaitTimeSeconds'     => 20,\n    'MaxNumberOfMessages' => 10\n]);\n\nif (!$result['Messages']) {\n    logInfo(\"No Messages Received\");\n    continue;\n}\n\nlogInfo(\"Messages Received\", $result->get(\"Messages\"));\n// Do Something with $result['Messages']\n\n}\nfunction logInfo()\n{\n    $r = [];\n    foreach (func_get_args() as $input) {\n        if (gettype($input) == \"string\") {\n            $r[] = $input;\n        } else {\n            $r[] = json_encode($input);\n        }\n    }\n    $string = implode($r, ', ');\n    var_dump($string);\n}\n```\nIt will produce normal result:\nphp\nstring(22) \"Trying to get Messages\"\nstring(582) \"Messages Received, [{\"MessageId\":\"6281a449-6ffd-40b7-8f53-ffe2d6aff6f6\",\"ReceiptHandle\":\"AQEBidNepzUOJ5VTgg5kt7u43hTEo9hciXoyC9Brhtr9OwRD6wAzdYEdHgsqB56RXhd9COEz0CoQ\\/xJf2X9fyrJMCpJj4CTI2fuGwpQjm5LVRfp9JdcI+m891ufvwA21f6RnXaFnHJuj9JGe6tRA6KOppfOtkMrccyu49H1Er\\/J06cqqipujW0upv8zuN4Cc1Rh9sGDQsb\\/u0X90mV8BHy7Q+NrRptuYCETrgTlwegsAeWhyq3F9FfrrlYfJhBtaW0PJojsFehY\\/VihKfClYVX2yyfkrE2kFJ0DAVqL5iZKO4rivYeOSHHr\\/y\\/S+nj+VM1n\\/JkQ02\\/RSlZBPKe0nDkMzKZ+5qsPoWLAM9A0hD6Z6XZoa4ZoRz6uDzNHBP\\/SsrFmO5cWDjyxl2Cl3f9+X7eCrGw==\",\"MD5OfBody\":\"e838a98aa0f8a7236c573c309b066fda\",\"Body\":\"I am testing\"}]\"\nstring(22) \"Trying to get Messages\"\nstring(1136) \"Messages Received, [{\"MessageId\":\"104cc519-1af9-40b6-8931-5ab711421df5\",\"ReceiptHandle\":\"AQEBrtYX6I9nzGcCGvNNZfD2czVkPNwupH8rtR6CgZn7PDPOAuvdJ\\/Ykhm3hzCTrclaSnpV1Qjgh+3CSHsFJ+Dhq6YGYNQZVddzPCIPBY4Rn3yOKuVyB7qHRhrskfalPAzqA2PIYUlhfI7tdoz0cGloH9o0afApgx6hTDXLRPDfhpvpZIDu6z1dgJynJ0qRTqYAMvPQWH8509kTpq1AYd6UaBNr2LzUQUqG2VGxpEl4dB7LIiDKry1f+2hTZ\\/JecDgHOvIK7Qu54nKiylO6bhea5mONCqTeApWtFP5SKe1KtkfwUQGn+pBbr84kvXbR36XdEHmMCJP0ZenHxaASO6OqJAWlDKK3iVIBs\\/3bVT2LFkCcxPYV5IvoZuS7YHSKb20LZwQGLTKYqRdzW52v6c7LwRw==\",\"MD5OfBody\":\"e838a98aa0f8a7236c573c309b066fda\",\"Body\":\"I am testing\"},{\"MessageId\":\"527afd84-2db3-4255-bc0c-6b5b366c1839\",\"ReceiptHandle\":\"AQEBDe+jOpJ0fuGavtR1YsZCYFa9BzbmQ7VePeT4V3AyEqdOMIcqiD\\/pYNvxy6Q8fTKPhqBbXfw8e1EZ95LT+7qsIGg6vUd\\/Ml\\/F6LMIE+zO5z23qhrlzs\\/3Uxqmvpn7bFL6MAs0QPtWmpWdwSusORcXbP21+KApNdMBnjpd\\/x\\/lXBo+yuWJ9eAB1d2iesbuiMhS1LCQOzu6tBf\\/XdI2+nTSDctBRogviCMOTeiLYdKQIVHK2hSpBr98DJxuzqaf0cWUxdsveTcTabvFGLUiUW6ajbSDKnOXiv6PprdcItn3E02FjNUy28XvFT8oB4rLhTShkPCAmC17sC2W2MQHSQSTJo5APF0j73ATuj+VSfNtTK8bjKcxjNlnbgYXjcUEggNou81o1M4XlfFk9c3LeWJ3UA==\",\"MD5OfBody\":\"e838a98aa0f8a7236c573c309b066fda\",\"Body\":\"I am testing\"}]\"\n ...\n ...\nCould you try to perform a single call instead of a looping call for receiveMessage to see if there is trouble with that first? If there is a trouble with that and you couldn't figure out, could you provide the wire log for me to see where might went wrong?\n. @enVolt Thanks for the clarification, good to know.\nIt would be nice if you get more error log for the issue, I'd suspect something wrong with request, and it retries for 3 time be default. Does the \"Blank Now\" means it takes quite a long time to response or does it suggests that scripts ends with no clue?\nCould you also try checking local network or get more information about transfer status with guide here?\n. Closing due to ages, yet feel free to reopen if you have further data, comments or questions :)\n. @enVolt Thanks for getting back on this, would it be possible for you to configure the timeout to produce the unexpected shutdown quickly? I'm curious about the stuck runner wire log to see what's actually going on with the request there. \nSince you have mentioned, those 2 consumers are similar, could it be something within the difference that might cause the trouble? Also, as this is involved with laravel, not sure whether it would be possible for you to test the functionality solo with PHP SDK? Maybe we could scope whether it is laravel related from there.\nI understand that those unexpected behavior might be painful to debug through, thanks for your patience!\n. @assembledadam Appreciated your feedback, it's true that response organized in array format, it's a general model design decision. So, may I ask for more information before we move further to make the API experience better? \n\nput this straight back into CloudSearch with uploadDocument()\n\nMay I ask more about this behavior? Are you expecting using the result from search to 'document' field in UploadDocuments as json format?\nIs this use case the main reason why you find the array unhelpful?\n. @assembledadam Appreciate the clarification with details, I'll tag it as a feature request in our backlog. Since it's not appropriate for SDK to change the raw response, it's fair that CloudSearch client could have a helper formatter that handles the issue. Does that sounds good to you? \n. Hi @LionelMarbot , apology for the confusion,\n1. It's moved into a separate package aws/aws-php-sns-message-validator\n2. Yes, it's an official repo for message validation which is standalone and does not depend on the AWS SDK for PHP or Guzzle.\nWe separate the validator because there appears to be high demand for using it solo. More information for upgrading guide can be found here.\n. @pushsoftdev , thanks for the error log, yet could you also provide some code snippets that could help reproduce the issue? Then we could see where might go wrong.\n. @pushsoftdev \nSo, you will need to wait on the promise at some point. The SDK isn't forking off background processes or farming work off to a NodeJS-style persistent global event loop, so scripts will need to include a directive to handle outstanding promises.\nWhat the SDK can do with promises is handle them concurrently, which can speed up scripts that involve a lot of I/O time. In fact, the transfer manager promise is composed of promises tracking the upload of individual files. The individual promises are handled five at a time, and when all uploads are complete, the transfer manager's meta promise is fulfilled. \nIf you wanted to upload multiple directories, you could compose several transfer manager promises into another meta promise:\n``` php\n$promises = [];\n$source = \"s3://bucket_a/prefix\";\n$dest = \"LOCAL_DIR\";\n$manager = new \\Aws\\S3\\Transfer($client, $source, $dest);\n$promises []= $manager->promise();\n$source = \"s3://bucket_b/prefix\";\n$dest = \"LOCAL_DIR\";\n$manager = new \\Aws\\S3\\Transfer($client, $source, $dest);\n$promises []= $manager->promise();\n$metaPromise = \\GuzzleHttp\\Promise\\all($promises)->wait();\n```\nThe SDK would then interleave those two directory uploads and spend less time simply waiting for HTTP responses.\nHope it helps.\n. @pushsoftdev Thanks for getting back, in consideration of better concurrency, how about a CommandPool?\n. Looks like your questions have been answered. Closing, yet feel free to reopen with further comments or questions :) \n. @EagleEyeJohn It's a dependency for psr instead guzzle, here is guzzlehttp/psr7.\n. @tatoshko Could you double check the Key parameter? The Error message shows that there is no such key in the bucket.\n. @tatoshko I'm a bit confuse, could you explain a bit what is the problem? Are you suggesting that you couldn't catch the exception?\nFor a non-existent key, the error should be thrown and caught, and doesObjectExists should return false. Here is my code snippet that matches the behavior described:\nphp\ntry {\n   var_dump($client->doesObjectExist('bucket', 'invalid_key.txt'));\n   $res = $client->getObject([\n       'Bucket' => 'bucket',\n       'Key' => 'invalid_key.txt',\n  ]);\n  var_dump($result);\n} catch (\\Exception $e) {\n  echo $e->getMessage();\n}\nIt outputs:\nphp\nbool(false)\nError executing \"GetObject\" on \"https://s3-us-west-2.amazonaws.com/s3-bucket-sandbox-test/invalid_key.txt\"; AWS HTTP error: Client error: `GET https://s3-us-west-2.amazonaws.com/bucket/invalid_key.txt` resulted in a `404 Not Found` response:\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<Error><Code>NoSuchKey</Code><Message>The specified key does not exist.</Message> (truncated...)\n NoSuchKey (client): The specified key does not exist. - <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<Error><Code>NoSuchKey</Code><Message>The specified key does not exist.</Message><Key>invalid_key.txt</Key><RequestId>65D71CE9721E29ED</RequestId><HostId>+DxYl1X7khosUwlH5oJmYeUrdiN6DjUmpvsh3bRdh30n8SNeSQugVGylL6fDrCYzBha2OcAknKo=</HostId></Error>\nIn addition, when a valid key is used, there is no error.\n. @tatoshko Interesting, I just tried the latest version and still couldn't reproduce the issue.\nDoes a valid key seeing this issue as well? Could you provide the OS information, php version, guzzle version then I could try to reproduce?\n. @tatoshko I just tried php5.6 and still couldn't reproduce the issue. Could you verify php 5.6 is installed properly as well as development environment by running compatibility-test with guide?\nAlso, instead of var_dump entire Exception, I'd suggest using getMessage() for cleaner message. Furthermore, getObject is a standard API call and SDK doesn't build customization on it.\n. @tatoshko I tried to enable xdebug, it still doesn't reproduce the issue. Though, if I try to var_dump the exception with xdebug turned on, it produce the the same error response structure as you described except that the exception is caught.\n. Closing, thanks for all your efforts in those information.\n. Thanks for the PR!\n. @jonshilton Here is the AWS public guide for the Filter string in Searching for Users Using the ListUsers API, it needs to be a filter string of the form \"AttributeName Filter-Type \"AttributeValue\"\". Quotation marks within the filter string must be escaped using the backslash () character. \n. @steenbag May I ask a quick question first? Which version of SDK is used? The documentation you mentioned looks like V3 guide of what's new, however the src/Common/ structure is suggesting SDK V2.\n. @lastday154 using asynchronous request would be helpful, you just need to add \"Async\" and handles promise. Could you provide more details about your usage case then we can see where could be enhanced?\nIf you are looking for deeper dive in asynchronous request, #621 is great thread to follow, especially the comments from @mtdowling. Feel free to let me know if you have further questions.\n. @lastday154 I think Michael's write-up explain the approach really well already, so if you are looking for exact detail code examples with putLogEventsAsync, you could try something like below:\n``` php\n// after $token is acquired\n$promiseGenerator = function ($total) use ($client, $token) {\n    for ($i = 0; $i < $total; $i++) {\n        yield $client-> putLogEventsAsync([\n            'logGroupName' => \"logGroup\",\n            'logStreamName' => \"logStream\",\n            'logEvents' => [\n                [\n                    'message' => \"...\",\n                    'timestamp' => \"timestamp\",\n                ]\n            ],\n            'sequenceToken' => $token\n        ]);\n    }\n};\n$fulfilled = function($result) {\n    echo 'Got result: ' . var_export($result->toArray(), true) . \"\\n\\n\";\n};\n$rejected = function($reason) {\n    echo 'Rejected: ' . $reason . \"\\n\\n\";\n};\n// Create the generator that yields 1000 total promises.\n$promises = $promiseGenerator(1000);\n// Create a promise that sends 50 promises concurrently by reading from\n// a queue of promises.\n$each = Promise\\each_limit($promises, 50, $fulfilled, $rejected);\n// Trigger a wait. Note that if you use an event loop then this is not\n// necessary.\n$each->wait();\n```\nAbove is the code example for ensuring several promise in flight at any given time. If you are looking for get rid of wait(), you can try the code snippet provided by Michael in the second part with cURL, explore Guzzle handler or customize your handler.\nHope this helps.\n. @lastday154 thanks for getting back, I'm a bit confused with \n\nBut it does not send all of my requests. Instead of sending requests to cloudwatch logs, it only sends first requests.\n\nCould you explain why you find that? Are you suggesting the request with the initial token? The code snippet provided needs to be feed with tokens when it changes, it won't magically fetch token for you.\n. @lastday154, in terms of fetching the token, there is also describeLogStreamsAsync, then you use the same approach with that method to ensure several promises in flight in enhancing the concurrency there. \nAs I have mentioned, if you don't want to wait for any promise at all, you could follow the advice from thread:\n\nThe SDK is designed to be able to work with any sort of HTTP client. Some clients might use an event loop that you can tick externally, while others might require you to call wait.\nThe SDK will use cURL or the PHP stream wrapper by default if you do not configure a custom HTTP handler for the SDK. This is accomplished by using Guzzle by default. However, keep in mind that you can use any HTTP client with the SDK (more on that later).\nIf you are using the PHP stream wrapper, then there's no way to send the requests other than to call wait. This is because the PHP stream does not allow concurrent requests.\nWhen using cURL, you would need to tick the cURL loop in order to asynchronously progress the transfers. Most non-blocking event loops require that they are ticked to progress the transfers. Using a cURL handler with the SDK is no different. Here's how you could use a Guzzle handler that is coupled to cURL to manually tick the cURL event loop:\n\n``` php\nrequire 'vendor/autoload.php';\nuse Aws\\Sdk;\nuse GuzzleHttp\\Promise;\nuse GuzzleHttp\\Handler\\CurlMultiHandler;\nuse GuzzleHttp\\HandlerStack;\n$curl = new CurlMultiHandler();\n$handler = GuzzleHttp\\HandlerStack::create($curl);\n$sdk = new Sdk([\n    'http_handler' => $handler,\n    'region' => 'us-west-2',\n    'version' => 'latest',\n]);\n$client = $sdk->createS3();\n$p1 = $client->listBucketsAsync()->then(function () { echo '-done 1-'; });\n$p2 = $client->listBucketsAsync()->then(function () { echo '-done 2-'; });\n$aggregate = Promise\\all([$p1, $p2]);\n// Tick the curl loop manually.\nwhile (!Promise\\is_settled($aggregate)) {\n    $curl->tick();\n}\n```\n\nIf you write HTTP handlers for other clients that use an event loop that is ticked automatically (because you are calling run or something on an event loop), then manually ticking an event loop or calling wait is unnecessary.\nHere are a couple examples of creating custom HTTP handlers for the SDK: https://github.com/aws/aws-sdk-php/tree/master/src/Handler. You could create a custom handler to bind the SDK to an event loop of your choice. Here is more information on SDK handlers: http://docs.aws.amazon.com/aws-sdk-php/v3/guide/guide/handlers-and-middleware.html#creating-custom-handlers\n. @lastday154 It's true that the promise needs to be resolved first then to fetch result from. Yet how to resolve the promise is up to you. If you don't want to implement another handler, you need to call wait somehow, however, it's feasible to enhance the concurrency in resolving the promise there, which will make it faster. \n. @lastday154 Okay, the initial way you implemented without Async works with no problem, you want to make it faster. You have to make 2 API calls: describeLogStreamsthen putLogEvents to achieve that. To make each call faster, you can use the approach described above to enhance the concurrency for Asyn calls. Something look like this:\n\n``` php\nif (!isset($token)) {\n    $result = promiseResolveHelper(1000, 50, $client, 'describeLogStreamsAsync', [\n        'logGroupName' => $logGroup,\n        'logStreamNamePrefix' => $logStream,\n    ]);\n    $token = $result['logStreams']['0']['uploadSequenceToken'];\n}\ntry {\n    $response = promiseResolveHelper(1000,50, $client, 'putLogEventsAsync', [\n        'logGroupName' => $logGroup,\n        'logStreamName' => $logStream,\n        'logEvents' => [\n            [\n                'message' => $line,\n                'timestamp' => $microTime,\n            ]\n        ],\n        'sequenceToken' => $token\n    ]);\n    if (isset($response['nextSequenceToken'])) {\n        $token = $response['nextSequenceToken'];\n    }\n} catch (CloudWatchLogsException $e) {\n    // do sth\n}\nfunction promiseResolveHelper($total, $concurrency, $client, $command, $params) {\n    $promiseGenerator = function ($total) use ($client, $command, $params) {\n        for ($i = 0; $i < $total; $i++) {\n            $cmd = $client->getCommand($command, $params);\n            yield $client-> execute($cmd);\n        }\n    };\n    $fulfilled = function($result) {\n        return $result;\n    };\n    $rejected = function($reason) {\n        echo 'Rejected: ' . $reason . \"\\n\\n\";\n        // You could throw exception here\n    };\n$promises = $promiseGenerator($total, $command, $params);\n$each = Promise\\each_limit($promises, $concurrency, $fulfilled, $rejected);\nreturn $each->wait();\n\n}\n```\nAbove code gives the idea there, you could do more refractor work to make it cleaner.\n. Closing due to ages, feel free to reopen with further comments or questions.\n. @coderlazy Thanks for the information! Since it happens randomly, would it be possible for you to catch some wire log information (debug option could be configured to have more helpful information) here to see what might go wrong with it? It would also be helpful if you could provide some code snippets that might help me to reproduce the issue.\nMeanwhile, for general SSL problem, could you double check that your CA bundle is configured correctly? Also, http configuration could be tuned, for example, you can customize the peer SSL/TLS certificate verification using verify http option as well.\n. @coderlazy Quick question, which Ses or Sqs API in PHP SDK are you using? I didn't see any SendEmailSes or pushOn there, are you suggesting SendEmail?\n. Closing due to ages, yet feel to reopen with further comments, details or questions :)\n. @tomnijboer It is possible definitely and you are on the right track. The way that you use assumeRole looks good to me, and I couldn't reproduce the error there. Could you double check those parameter values? Also, instead of extract AccessKeyId or SecretAccessKey manually by yourself, SDK also has a createCredentials method that helps create a valid Credential object for you.\n\nWhen I run the aws cli I need to supply a session token, but I don't know where to put in with the php code.\n\nI'm a bit confused, referring to cli examples, required parameters appear same. Could you explain a bit here? Are you suggesting general session token parameter as part of a credential content that used to initialize a StsClient?\nFurthermore, #1077 PR is under review, which adds an AssumeRoleCredentialProvider that make this much easier. :)\n. Great\uff01\n. @bakura10 Thank you for the PR! The addition and test case both looks good to me. I'll merge it and tag it in next release.\n. @siwinski It's true that this PR cannot be merged in directly, thanks for noticing! I'll take a look.\n. Closing, this is taken care of in the latest release, thanks!\n. @praditha s3 bucket URL still follows the path style. It's not changed, I believe aws/aws-sdk-php-laravel#109 is the same one, I'll close that one and track the issue here.\n. @praditha Okay, thanks for editing the question to make it clear, I think this is a issue or question that you need to open with CEPH instead of us. Looks like they have implemented their integration interface with aws-sdk-php on their side. There is nothing that aws-sdk-php could help to change that.\nPS:\nI'm not sure whether this will help you since you are not using S3 at all, yet client endpoint could be configured to make the request from. A bit confused by your description of connecting to s3 from another storage service there.\n. Closing, since this is not a issue that could be fixed or configured from SDK side. AWS SDK will not help maintain those integration interfaces built upon us. Thanks.\n. @peterkomar Glad that you work out the problem by yourself, \n\ncalling gc_collect_cycles before remove file \n\nThis works because GuzzleHttp\\Stream object holds onto a resource handle until its __destruct method is called. Normally, this means that resources are freed as soon as a stream falls out of scope, but sometimes, depending on the PHP version and whether a script has yet filled the garbage collector's buffer, garbage collection can be deferred. gc_collect_cycles will force the collector to run and call __destruct on all unreachable stream objects.\n. @nicolae-stelian Thanks for reporting this, taking a look\n. @nicolae-stelian may I ask a quick question before moving forward, which php version is used? Also, are you using aws-sdk-php-zf2 ?\n. @nicolae-stelian Interestingly, I clone you repo and couldn't reproduce the error.\nMy PHP version:\nPHP 7.0.11 (cli) (built: Sep 22 2016 13:53:33) ( NTS )\nCopyright (c) 1997-2016 The PHP Group\nZend Engine v3.0.0, Copyright (c) 1998-2016 Zend Technologies\nMy OS:\nOS X 10.11.6\nKernel Version: Darwin 15.6.0\nI'll try an Ubuntu instance to see if I could reproduce it there.\nAlso, just as a reminder, you need to be really careful with the credential information in the script. Since I receive session_token expire, so I assume it's not the real credential bunds to your AWS account.\nEdited:\nApology for pointing to zf2, I intended to suggest aws-sdk-php-symfony instead.\n. @nicolae-stelian I just tried in ubuntu environment with your SQS code and still couldn't reproduce the issue there. It would be helpful if you could provide some error log with SDK specific behavior related. Then I can see how can I help from there.\n. No problem at all, thanks for getting back and providing information here ;)\n. @dovanmanh080485 Could you provide a code snipper that help to reproduce your error? Then I can see how can I help.\n. @dovanmanh080485 Looks like you are trying to replace unicode with pattern similar like /[^\\u0009\\u000a\\u000d\\u0020-\\uD7FF\\uE000-\\uFFFD]/ instead of remove them. So you might need to something like:\nphp\n$doc = preg_replace(\n    '/[\\x{0009}\\x{000a}\\x{000d}\\x{0020}-\\x{D7FF}\\x{E000}-\\x{FFFD}]/u',\n    '',\n    $raw_doc\n);\nTo remove all invalid unicode characters.\nMore information about u, x.\n. @scottopolis The problem is with setting http at scheme (which is used to connect to the service), for IAM, either their protocol or endpoints are HTTPS only. May I ask are you suggesting you succeed in using http previously?\n. @scottopolis Hmm, interesting, IAM models haven't been updated for 2 months. May I ask the version that this works?\nSince I can reproduce this, taking a second look ...\n. @scottopolis I just verified with several version of SDK, old and new, this won't succeed as well. IAM service doesn't change in the time frame you have mentioned. Different service has different protocol constraints, and I believe IAM service requires https. Also, I just verified with Ruby SDK, they won't allow this as well.\nSo, as you have mentioned:\n\nI am sending from an http site for development, so I have 'scheme' => 'http' set.\n\nThe scheme here is the URI scheme to use when connecting to AWS service instead of your development site, are you suggesting that your developing environment doesn't allow that? Could you provide more information there? Then I can see how can I help.\n. @scottopolis Good to know, so endpoint and scheme are those parameters to change endpoint URL. Yet, there is a list of endpoints supported by AWS services. You are on the right track to configure that, yet service varies in protocols and endpoints. If you are trying to apply different mechanism to different services, you could take a look at endpoint_provider.\n. @umesecke Thank you for the feedback and thoughts! I see your case, so firstly, as you have mentioned, this buffering is necessary for non-seekable streams definitely. So the main concern is why SDK checks 'plainfile' type in qualification for a seekable stream.\nSo, MultipartUpload needs to ensure each part matches precisely with stream resource or there could be discrepancy between contents. SDK uses LimitStream in Guzzle to achieve that. For php fread, there could be unexpected behavior for streams not represents a plain file:\n\nif the stream is read buffered and it does not represent a plain file, at most one read of up to a number of bytes equal to the chunk size (usually 8192) is made; depending on the previously buffered data, the size of the returned data may be larger than the chunk size.\n. @umesecke Appreciate your feedback, your case does have a point worth considering, for limit bytes read, there is a PR guzzle/psr7#105 opened with implementation of a ByteCountingStream that may help with the situation. I'll tag this as a feature request and track it in our backlog.\n. @umesecke I research around a bit on this, there is another crucial reason why buffering is needed.\n\nLimit from fread is still the cause:\n\nat most one read of up to a number of bytes equal to the chunk size (usually 8192) is made\n\nNoted that 5MB is the minimum part limit for a MultipartUpload for S3\uff0cif SDK lets an option for default streaming behavior, it will never succeed in following upload by part, which means, the privilege for Multipart is out of option for those large resource streams.\n. @mtdowling That's a good point, make sense to me. I'll dig deeper into this. Allowing the cache size is indeed a good point worth considering.\n. @remipelhate Thanks for the information! I couldn't reproduce it locally with SDK solo, I doubt it might not be a SDK issue. So normally, in the signer step, after the \"Enter\" information, there should be something like:\n``` php\n Rebuilt URL to: http://localhost:8000/\n   Trying ::1...\n TCP_NODELAY set\n Connected to localhost (::1) port 8000 (#0)\n\nPOST / HTTP/1.1\nHost: localhost:8000\nX-Amz-Target: DynamoDB_20120810.ListTables\nContent-Type: application/x-amz-json-1.0\naws-sdk-invocation-id: cebc32d4b6af36db6e619d1099796a08\naws-sdk-retry: 0/0\nX-Amz-Date: 20161011T171124Z\nAuthorization: AWS4-HMAC-SHA256 Credential=[KEY]/20161011/us-west-2/dynamodb/aws4_request, SignedHeaders=aws-sdk-invocation-id;aws-sdk-retry;host;x-amz-date;x-amz-target, Signature=[SIGNATURE]\nUser-Agent: aws-sdk-php/3.19.11 GuzzleHttp/6.2.1 curl/7.50.3 PHP/7.0.11\nContent-Length: 2\n\n\nupload completely sent off: 2 out of 2 bytes\n< HTTP/1.1 200 OK\n< Content-Type: application/x-amz-json-1.0\n< x-amz-crc32: 1315925753\n< x-amzn-RequestId: 8bace1da-6c08-4887-b4aa-08afc1f4696e\n< Content-Length: 17\n< Server: Jetty(8.1.12.v20130726)\n< \nCurl_http_done: called premature == 0\nConnection #0 to host localhost left intact\n\n<- Leaving step sign, name 'signer'\n...\n```\nLooks like there is no logging information for connection. Since as mentioned there is no issue with the web shell, so I assume port:8000 is running all right. Credentials shouldn't be the problem for DynamoDB local although is needed. \nMay I ask which version of SDK is used? Are you using aws-sdk-php-laravel? \n. @remipelhate I see, I doubt that connection could be the problem yet error or exception failed to be surfaced. Could you try to configure timeout in the client to make it fall immediately to see if more errors could be surfaced?\n. @remipelhate The error indicates Hostname was NOT found in DNS cache, it might has to do with cURL. Could you try to upgrade cURL? Also, could you double check that port is running ok with dynamoDB?\n. @remipelhate Hmm, interesting. Good to know it's working now. I did use the latest version of DynamoDB Local in testing, which doesn't reproduce the issue for me.\nSince this doesn't look like a SDK issue, I'm closing this issue for now, yet feel free to reopen with further comments or question :)\n. @vrkansagara Nothing changes in OpsWorks recently, I just test describeLayers and couldn't reproduce the issue there. May I ask are you suggesting those errors suddenly happened with Opsworks only? Could you double check your local network? Is there any proxy issues?\n. @vrkansagara Looks like you are using http instead of https, did you set scheme to http on purpose? Not all service allows for HTTP, refer protocol information see.\n. @vrkansagara Appreciate the feedback, I'll track it in our backlog as a feature request, thank you. :)\n. @socketman2016 \n\n1- How can I force Async methods return \\React\\Promise\n\nYou could configure handler option per client to change it to a React handler instead of a guzzle one.\n\n2- How can I throw uncaught exception in \\GuzzleHttp\\Promise\\Promise $onFulfilled\n\nIf there is exception or error thrown, it won't be wrap into a fulfilled promise, it will be wrapped in a rejected promise. More details about promise see.\nHope these helps.\n. @socketman2016 Apology for rely late on this, good to know that you tried it at first place. Reading your issue opened at reactphp\\promise, looks like you are having problems with surfacing exceptions? Since you are mentioning Async calls in particular, are you trying to use event loop like #901 ? You might also want to have a look at react-guzzle-psr7.  Could you provide more information before moving forward?\n. @socketman2016 Apology for the late response, looks like your main concern is catching exceptions. So, it's true that Guzzle doesn't have done() method. Yet then() or reject() is sufficient for catching a RejectedPromise that contains exception or error.\nYou could do something like:\nphp\n$promise->then(null, function ($reason) {\n    echo $reason;\n});\n// OR\n$promise->reject('Error!');\n// Outputs \"Error!\"\n. Closing due to ages, yet feel free to reopen with further comments or questions :)\n. @WilliamMayor SDK accepts the path style in prefix, however there might be some problem with your code snippet, looks like in your each chunk loop, deleteObjects is called every time, which means the 'Object' => []' array never get appended. To fix that, you need to do something like:\n``` php\n$param = [\n    'Bucket' => $bucket,\n    'Delete' => [\n        'Objects' => [],\n    ],\n];\nforeach ($chunks as $chunk) {\n    $param['Delete']['Objects'] []= $chunk;\n}\n$res = $s3->deleteObjects($param);\nvar_dump($res);\n```\nIn this way, deleteObjects is called only once to delete batch of objects with same prefix.\nAlso, regrading $prefix param used in your code, if you are trying to representing a folder path, you need to use parent-folder/ instead of /parent-folder/.\nFinally, for the > character, for example, it's true that for those XML characters that doesn't conform to the published xsd schema will fail the validation, I'd suggest some work around. Since the file name contains special characters, could you try rawurlencode(SPECIAL_FILE_KEY) for those files?\n. @WilliamMayor Oh I see, thanks for clarification on those. That's true, then it should be fine, good to know about the path style is being taken care of. If you see further trouble with that, please let me know.\nThen for those special characters, so, an XML parsing exception due to unescaped XML characters in the XML response is a server side issue that cannot be fixed by the SDK, sorry to know if that is something cannot change on your side. However, it would be great if you could let the Amazon S3 service team know that this is important to you by posting to their  forum. Thank you again for your feedback on this.\n. Closing due to ages, yet feel free to reopen with further comments or questions :)\n. @tamc Thanks for noticing, I'll fix it. \nSo for cache, $cache represents \nCache to store credentials, and by saying Defaults to using a simple file-based cache when none provided., it is talking about the updated cache type when no credentials found in cache.\n. Closing the issue and tracking with  PR #1108 instead, yet feel free to reopen with further comments or questions :)\n. @TheRoSS Thanks for the feedback and noticing this behavior, apology for the confusion there. \nI understand that ends a directory with '/' appears normal behavior, however, in S3 convention(from web console) and all of SDK examples, you will notice there is also no end '/', in addition, php __DIR__ also has no '/' in ending etc. \nIt make sense to me that the extra '/' in the filename needs to be removed, yet the way you fix it will change the behavior with current SDK. For example, given directory s3://bucket/test, if there is also a s3://bucket/test/sub-test. Result for using Aws\\recursive_dir_iterator(\"s3://bucket/test/\") will list only the first level items, and Aws\\recursive_dir_iterator(\"s3://bucket/test\") will list every thing in side test recursively. Changing the behavior there might break other customer depending this function. \nIn summary, it's generally a good practice to omit the extra '/' there, having '//' in result serves as an indicator for removing the extra '/' there. Appreciate your thoughts on this, if you still prefer no double '/' exist in result, I could open a PR to fix it from result instead of fixing it in generating process. Thoughts?\n. Closing due to ages, yet feel free to reopen with further questions or comments :)\n. @cpinto It appears related to SSL or cURL issue that needs to be fixed from OS side instead of SDK side,  so for workaround, here is verify option that you could change in client configuration.\nIf this still happens in regardless of upgrade solution etc, it would be helpful if you could provide a wire log information with debug option.\n. @cpinto Glad to know that you figured this out, and thank you for sharing this with others. Closing the issue right now, feel free to reopen with further comments or questions :)\n. @remipelhate Appreciate your thoughts for this PR! \nIt appears as a reasonable request at first glance, yet taking a second thought, given that those Reserved Words in DynamoDB doesn't bound to their service interface, and will fail API after a request has been made, you have also notice that there is also discrepancy between docs and latest API behavior. It looks like a change that requires continuous maintenance from SDK side with manual update only. Looking at this huge list, it adds up redundancy to SDK as well considering that this list might keep appending. Furthermore, if this was merged, people might depend on this functionality for validation words, it could easily break people if the const list has been updated or hasn't been updated in time. Thus I think it would be a more clean way to let it fall naturally. Thoughts? :)\n. @algore Your code snippet looks good to me, the problem is DefaultSenderID support status varies by country. Here is the Supports Sender IDs information form for all regions.\n. Closing, yet feel free to reopen with further comments or questions :)\n. @ppaulis SDK doesn't build customization upon that API, taking a look to see if I could reproduce the issue ...\n. @ppaulis Apology for getting back late on this, yet I couldn't reproduce the issue with maximumPageSize parameter, I didn't do anything different with your code snippet in calling the API. Thus I'm curious about why it goes wrong on your side.\nMay I ask is there other error thrown when you notice this behavior? Could you double check that maximumPageSize is spelled correctly? Because non-required typo parameters won't make it to server side in SDK.\n. Closing due to ages, yet feel free to reopen with further questions or updates! :)\n. @ondrejhlavacek I just tried getAuthorizationToken API and it works fine for me, I suspect the issue is with your $awsCredentials. The error message suggests problem with security token, and your code new Credentials(KEY_ID, SECRET_KEY) seems to ignore token parameter, that might be the problem. So you would need to do something like:\n$awsCredentials = new Credentials(KEY_ID, SECRET_KEY, SECURITY_TOKEN)\nAlso, looking at your CLI command, a profile credentials is used. You could also try using profile  in PHP SDK with instructions here?\n. @ondrejhlavacek Interesting, thanks for these information inputs. Could you try \n$resp = $ecrClient->getCredentials() to see whether the credential is resolved correctly? And perhaps error could be surfaced before making the request.\n. @robertmylne Yes, you can :). Check out the SNSClient here, also you might also be interested in aws/aws-php-sns-message-validator.\n. Closing, yet feel free to reopen if you have further questions or comments :)\n. @skuppuraj Can you provide a Code snippet or some background information about this?\nFrom your error code, it indicates that Key parameter is not set correctly in your command.\n. Closing due to ages, yet feel free to open with further details or comments :)\n. @simshaun Thanks for the information. So first of all, it's true that SDK return path-style instead of  CNAME style(virtual host style). \nUsing virtual hosted bucket URLs can affect application performance in two ways: 1) a DNS lookup will need to be made on every single bucket, and 2) requests are routed by default to the region provided. Could you explain a bit why you would need the pattern? Could you also provide the documentation that you talking about having the pattern? \n. @simshaun No problem at all, v2 allows this pattern due to old signature pattern. It is not recommended anyways. Thanks for the explain there, I see. So for work-arounds, it's possible to use these CNAME style like:\nphp\n$s3 = new \\Aws\\S3\\S3Client([\n    'version' => <version>,\n    'region' => <region>,\n    'endpoint' => '<bucketName>.s3.amazonaws.com',\n    'bucket_endpoint' => true,\n]);\n. @poisa Thanks for the information, may I ask are you looking for catching the error code? Or you want the plain txt message?  You might be interested in AwsException\n. @poisa Thanks, you could use \\Aws\\Api\\ErrorParser\\XmlErrorParser. You could do something like:\nphp\ntry {\n   ...\n} catch (Sns\\Exception\\SnsException $e) {\n    $resp = $e->getResponse();\n    $parser = new \\Aws\\Api\\ErrorParser\\XmlErrorParser();\n    $msg = $parser($resp);\n    var_dump($msg['message']);\n}\n. Closing due to ages, yet feel free to reopen with further details or questions :). @torfeld6 Interestingly, I couldn't reproduce the issue with file names containing '_' or '@', transfer manager works fine for me. Could you provide more information? May I ask the version of the SDK and the version of PHP being used?\n. Closing due to ages, yet feel free to reopen with further details or questions. @torfeld6 Taking a look .... @torfeld6 Really appreciate your input there! And I really hope to reproduce possible bug there and fix it. However I still have trouble in reproducing the issue there. I tried both upload a local directory to s3 or download a directory from s3 to my local disk. Both source contains a exactly same file name \"close_hi@zx.png\", and both result looks good with correct name \"close_hi@zx.png\". \nCould you give more information that could help me to reproduce the issue?. @cmpaul Your code snippet looks good to me, waitUntil('TableNotExists') is checking whether delete operation has been completed. Could you double check which line cause the error? It could be caused by an incorrect name in deleteTable instead. For the segmentation fault, it might has to do with your OS environment or xdebug.. @cmpaul Ah interesting, I'll take a look to see if I could reproduce it there.. @cmpaul Taking a second look, first, when deleting table, you need to do:\n$ddb->deleteTable([\n    'TableName' => 'toDelete'\n]);\ninstead of passing in the string directly. Since you succeed in deleting the table, then I assume it's just a typo in your code snippet.\nI couldn't reproduce the issue locally still. Yet it's worth to be noticed that this might be caused by running the same script twice, because first attempt succeed, and second attempt cannot find the table since it has been deleted.. Thanks for getting back, closing, yet feel free to reopen with further details :). @pspiller Thanks for the PR fix! It helps keep consistency with our public doc guide. @vfuentesedcorp the problem is with your params, for the option array, here are available options. Could you explain your params usage there? Are you trying to transfer only object qualified with public-read? If so, you should try before option instead. @vfuentesedcorp Taking  a second look at your code snippet, are you trying to \"transferring\" files from a S3 location to another S3 location? If so, S3 transfer manager/ uploadDirectory won't work because it works when : \nIf the $source argument is an Amazon S3 URI, then the $dest argument must be a local directory (and vice versa).\nIf my above assumption is correct, you would need to use copyObject instead.. @vfuentesedcorp Adding to my last comment, if not, there is an example of how to customize command arguments inside Transfer Manager.\nIn this case, I assuming you are trying to download files with 'public-read' from S3 to local dir?\nThen you might need to do something like:\nphp\n$uploader = new Transfer($s3Client, $source, $dest, [\n    'before' => function (\\Aws\\Command $command) {\n        if (in_array($command->getName(), ['GetObject'])) {\n            // Do something with \n           // GetObject (http://docs.aws.amazon.com/aws-sdk-php/v3/api/api-s3-2006-03-01.html#getobject)\n        }\n    },\n]);\nSince when GetObject requires Bucket and Key to identifier a object, in order to filter out objects with required permissions, you might need to use GetObjectAcl to help filter out qualified objects first then transfer those qualified objects.. @vfuentesedcorp I see, so I'm a bit confused, from outside of S3, how do you know whether a file is public-read? If you are trying to set those files to public-read in S3, you would just need to do:\nphp\n$uploader = new Transfer($s3Client, $source, $dest, [\n    'before' => function (\\Aws\\Command $command) {\n        // Commands can vary for multipart uploads, so check which command\n        // is being processed\n        if (in_array($command->getName(), ['PutObject', 'CreateMultipartUpload'])) {\n            // Apply a canned ACL\n            $command['ACL'] = strpos($command['Key'], 'CONFIDENTIAL') === false\n                ? 'public-read'\n                : 'private';\n        }\n    },\n]);\nYou can configure the command with anything you would like to set with PutObject and CreateMultipartUpload. @vfuentesedcorp appreciate for following up, so to clear to my confusions, are you trying to filter out some files when upload to s3 from a directory? And all parameter options for PutObject doesn't work for you? \nI totally understand that unsatisfied performance feels bad, I'd like to see how can I help or optimize it there. Yet condition based filtering is taken cared by S3 service operational APIs, it's not appropriate for SDK to make changes there.. Closing, yet feel free to reopen with further details or questions :). @acinader Taking a look, working on the fix.. @acinader A fix has been merged into master, could you try it to see if that fixes the issue? SDK cannot change the resolving order in the default credential provider because it's unified across all SDKs.\nYet this bug might be caused by ECS credential failure fails to be wrapped inside the promise to make it through the chain correctly. If the fix doesn't work, please let me know.. @ArthurGuy Hmm, interesting, could you provide you php version, OS information then I could see if I could reproduce it there? Also may I ask which version of SDK start causing this break for you?\nEdited, to clarify the problem, are you suggesting that with current chain, using assume role always fails? (InstanceCredentialProvider cannot be resolved)?. @ArthurGuy So for sdk went from 3.19.28 to 3.19.31, there is no changes happen in Credentials. I'll take a look at Guzzle to see if something happen there. Thanks for the information, I'll see if I could reproduce. Just a quick workaround, if you are using instanceCredentials only, you could try use instanceProfile directly.. @ArthurGuy I looked at guzzle, I don't think there is any related changes neither. I couldn't reproduce this issue locally. Could you provide more information? Is using instanceProfile working for you? . @ArthurGuy Interesting, thank you for your patience on this and appreciated that, could you open an issue in guzzle repo as well? I'll try again with guzzle 1.3.0 to see If I could reproduce. @ArthurGuy Interestingly, I still couldn't reproduce the issue with a fresh launched ec2 instance:\nUbuntu Server 16.04 LTS (HVM), SSD Volume Type - ami-a9d276c9 with uname -a:\nuname -a\nLinux ip-172-31-39-144 4.4.0-45-generic #66-Ubuntu SMP Wed Oct 19 14:12:37 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux\nMy dependencies (via composer)\nphp\naws/aws-sdk-php        3.19.33 AWS SDK for PHP - Use Amazon Web Services in your PHP...\nguzzlehttp/guzzle      6.2.2   Guzzle is a PHP HTTP client library\nguzzlehttp/promises    1.3.0   Guzzle promises library\nguzzlehttp/psr7        1.3.1   PSR-7 message implementation\nmtdowling/jmespath.php 2.3.0   Declaratively specify how to extract elements from a ...\npsr/http-message       1.0.1   Common interface for HTTP messages\nMy php version\nphp -v\nPHP 7.0.8-0ubuntu0.16.04.3 (cli) ( NTS )\nCopyright (c) 1997-2016 The PHP Group\nZend Engine v3.0.0, Copyright (c) 1998-2016 Zend Technologies\n    with Zend OPcache v7.0.8-0ubuntu0.16.04.3, Copyright (c) 1999-2016, by Zend Technologies\n@dittto Thanks for the info., may I ask are you also finding this failing since guzzle 1.3.0? What's you php version and OS environment?. @ArthurGuy Yeah, it is using the instance metadata credentials, also since it's a newly launch instance with an assume role, I didn't do any customization either.\nWhat I did is require straight forward. I launched a new ec2 instance with assume role that enabled with s3 access. I installed PHP 7 and SDK with all dependencies with composer. And I'm able to perform S3 operations with successful response. Also, given some time, tracking trace, the metadata credential is cached and correctly fetched.. Thanks for the following inputs, I'll take a second look at Guzzle changes.\nAlso, may I ask your php extensions loaded? Just curious, is xdebug enabled? Is PHP SDK used directly or integrated with other framework? May I gather those data from your guys?. @Nizari Quick question, does this start since guzzle version upgraded to 1.3.0 ?. Closing, thank you all for the efforts in this, appreciate that. . Thanks for catching! . @stavros-liaskos You just need to pass in an empty array, no parameters is needed for that API.  If it doesn't work, feel free to let me know and I could contact service team for their inaccurate documentation.. @stavros-liaskos I see, that's a fair impression to have at first glance. This Get API is per user actually. Then in distinguishing different users, you might need to have different clients or playing around with identity operations. Thanks for the feedback, I'm closing this issue for now, yet feel free to reopen with further comments or questions. :). @jaypea Thanks for the information and your efforts in upgrading, appreciate that!\nSo as you have observed, the discussion in #1118 refers to a general S3 workaround, which is not same as PostObjectV4 case. In your case, you would like to have virtual-host style URL with http scheme. Then you could simply do:\nphp\n    $s3 = new \\Aws\\S3\\S3Client([\n    'region' => 'us-west-2',\n    'version' => 'latest',\n    'scheme' => 'http'\n   ]);\nI assume this is what you are doing right now as you have mentioned using 2 S3Client objects with another client object configured as your code snippets.\nIt's true that automatic switching is provided between path style and virtual style hosting of buckets when necessary. The bug is caused a customized endpoint with virtual style already, I'm working on the fix and will have a PR out shortly.. @jaypea The fix PR has been merged, will be tagged in today's release, thank you for your patience! :). @hilmanrdn Could you try +5 seconds in your code?\nFor createPresignedRequest, when $expires is a string, it needs to follow strtotime converting rules.. @hilmanrdn Hmm that's interesting, I couldn't reproduce the issue, I tried your code snippet with that change and it works for me (with correct expiration).  Could you double check the object policy? Does it show accessed denied without pre-signed URL?. Sound good, closing, yet feel free to reopen if you have further questions or more details :). Updated, ready for another review :) @xibz . @saintberry Thanks for noticing, so that PR doesn't remove the check, it lets the request to fall when wrapped in promise instead of throw exception immediately. Appreciate pointing out the latency there, timeout is configurable via provider now.\nHowever since the ecs credential is not cached correctly, it makes the latency an issue. The  PR #1133 looks like making the correct fix by caching the ecs credentials properly. I'll take a deeper look into that and deliver the fix shortly. Thank you for your patience.. @anniballo Thanks for the information and appreciate your patience there, the PR is in progress, and I'm keeping a close eye on it. Could you try pass in the timeout parameter as a quick work around for this? Or as another workaround, could you use them directly instead of calling defaultProvider() currently?. The fix PR has been merged, will be tagged in the next release :), free free to reopen if that fix doesn't work :). @saintberry Just Out. :). @vasanthperiyasamy Thanks for the work! Looks good to me, appreciate your efforts on this! Just merged, will be tagged in next release :). @GCalmels Thanks for the information, I just tested when using SDK directly, and it is not a issue, I assume you have this issue with symfony. Just a quick check first, you should configure command something like this:\n``` php\n$command = $s3->getCommand('PutObject', [\n    'Bucket' => $bucket,\n    'Key' => $key,\n    'Body' => 'foo bar',\n]);\n// Just an example for using '@'\n$command['@http'] = [\n    'debug' => true, \n];\n$resp = $s3->execute($command);\nOr this will work as well: php\n$s3->putObject([\n   'Bucket' => $bucket,\n    'Key' => $key,\n    'Body' => 'foo bar',\n    '@http' => [\n        'debug' => true\n    ]\n]);\nOr if the `@` really is the pain, you could try configure it at client level: php\n$s3 = new Aws\\S3\\S3Client([\n    'region' => 'us-west-2',\n    'version' => 'latest',\n    'http' => [\n       'debug' => true,\n    ]\n]);\n``\nHope these helps.. @GCalmels I see, if the client level configuration doesn't work for you, sorry to say but where is nothing for SDK do there. Even if SDK changes the@behavior, it would easily break other customers. I'd say it's a issue need to bring up withSymfonyor perhaps the thrid party repo instead of us. Closing for now, yet feel free to reopen with more questions or details :). @GCalmels Of course that change won't affect code since it's just inside documentation block, yet I'm curious that, since it's only a documentation change, may I ask how will this helping the situation?. @GCalmels I understand, yet I mean even if this '@' lives in documentation?@httpwill be sourced in the code like [here](https://github.com/aws/aws-sdk-php/blob/master/src/AwsClient.php#L212) anyways?. Closing to due ages, and I don't see this is a member for the correct operation, feel free to reopen with further details :). @jarrettj API docs for [SesClient](http://docs.aws.amazon.com/aws-sdk-php/v3/api/class-Aws.Ses.SesClient.html), AWS developer guide for [SES](http://docs.aws.amazon.com/ses/latest/DeveloperGuide/Welcome.html). Hope this helps!. Provided above are official SDK API docs forSESservice and officialSESservice guide. Closing, yet feel free to reopen with further questions or request in detail :). Addressing feedback, removing trait file, fix documentation.\nReady for review. @mtdowling :). Changes have been made, including requiresclient` parameter etc.. @mtdowling . @mtdowling Of course, I'll definitely squash them before merge :) And I have wrapped it as previous exception in the last commit.\n@jeskew It would be great if you could help review this PR as well :)\n@virgofx Appreciate your patience and feedback on this :). @virgofx Just merged, will be tagged in our next release coming in one day or two :). @jjmontgo Thanks for the information, taking a looking to reproduce the issue ... It would be great if you could show me the debug trace as the same time :)\nJust a quick response, the error message show similar to #1125 , which might due to a guzzle issue, could you try down grade guzzle to 1.2.0 to see if it that works?. @jjmontgo Thanks for the inputs, really appreciate that. Quick get back, I couldn't reproduce the issue locally, I'll try to see if I could reproduce it on the instance. The code snippet looks good to me though. Just to clarify, you are suggesting using v3.20.X right?. @jjmontgo I cannot reproduce the issue at the instance environment either, looks like this is more a permission denied issue from your side. Here is some general documentation that you might be interested:\nElasticbeanstalk PHP Example\nSo regarding permissions, \n1. The object policy\n2. The credentials used in making the request\nCould you tell me the 2 points above in your beanstalk environment? You need to make sure they both qualifies and matches.. @jjmontgo Thanks for getting back! Interesting, I'm curious why this will affect PostObject solo, since both behavior depends on sourcing qualified credentials for a S3Client. If gu.zle is the cause, both credential sourcing will be failed.\nAs a workaround if this is a guzzle issue, could you try add credentials parameter with a specific credentialProvider to see if that helps? . @mtdowling Totally agreed on providing the filename there, and thanks for the suggestion. I made changes in the latest commit.\nYet I kind feel I'm \"polluting\" the constructor or config array in the way I'm doing right now, since determineSource is private within AbstractUploader, do you have any suggestions to make these better?. @mtdowling I have updated this PR with S3 namespace exception, it will be great if you could have another look at it when you have time : ). @jeskew All fixed : ) Ready for another review. Online PHP editor is fun : )\n. @mcblum Could you provide more information there? For example, what is the error message when it fails? The exact code snippet on the guide work fine for me, so it would be great if you could provide more useful information in reproducing your case.. Closing due to ages, yet feel free to reopen with further question or details :). @amank12 Not sure what are you exactly asking for, are you asking integration credentials that you got from the javascript code to PHP SDK usage? Or you are just suggesting a PHP file/function?\nIntegrate credentials with PHP SDK.\nIf you are not looking for PHP SDK integration, sorry to say but this is not the appropriate place for this question. StackOverflow would be a better place for this. Our github issue is used to tracking possible SDK bugs or feature request.\nClosing, appreciate your attention anyways :). Looks like questions have been answered or lacking information for ages, closing, yet feel free to reopen with further contents :). Thank you for the fix, appreciate that!. @bakura10 Appreciate the feedback, to clarify the feature request there, current validateMd5() only works for ReceiveMessage validation, with recent MessageAttributes introduced to sendMessage, SDK could help calculate the MD5 message digest for SQS message attributes there and helps with the validation. (Feel free to add more clarification there :))\nI'll tag this as a feature request and have it in our backlog, it will be in our implementation queue soon :). :shipit:  . :shipit: Notice: it is important that this should be fixed from root.. @Thamaraiselvam It would be much more helpful if you could provide some code snippets that might help us to reproduce this issue.\nAlso could explain more about how you encounter the issue? Does \"uploading files to S3\" means using the TransferManager or simple putObject request? Also, could you provide more information about your use case? Such as, what's the version of SDK/Guzzle used? what's the OS environment?. @kchan4 Interesting, thank you for providing those information! A quick note first, I noticed that you are using SDK 3.18.X, it would be great if you could try upgrade to our latest version and latest guzzle version to see if any enhancement happens.\nSo from SDK side, it actually just makes request to EC2 server host to fetch those metadata credentials. There could be certain possibilities that some limitation exists from server side. If you like, I can open a ticket internal with Ec2 with those data point to see if the limitation is the cause.\nMeanwhile, to enhance the performance there, you might be interested in experimenting a bit around customizing the timeout behavior there. \nDoes that sound good to you?. Adding to my last comment, may I ask how do you cache those credentials? Are you sharing the cache across processes?. @pavankumarkatakam Thanks for the information, I heard your request, our services never stop launching new regions : ), which would relieve the pain there. Because initializing a client for a region is handy and easy, making cross region calls from server side might be expensive.\nMeanwhile, since this is a feature that should be considered by service side instead of SDK side. I'll contact service team internally to let them know about this request. Thank you again for the feedback.. Closing, yet feel free to reopen with further questions or adding more comments. : ). @ivankristic Aws\\Ec2\\Exception\\Ec2Exception is the class you want to hit for exactly : )\nI'm curious how you get \n\n\"FatalErrorException in ~/vendor/aws/aws-sdk-php/src/WrappedHttpHandler.php line 192:\n\nClass 'Aws\\EC2\\Exception\\EC2Exception' not found\nSo I tried to send a bad request and catches Aws\\Ec2\\Exception\\Ec2Exception correctly. Could you show perhaps some code snippets that might help reproduce the error?. @ivankristic  Interestingly I still could reproduce this locally, what I do is quite simple, I create \\Aws\\Ec2\\Ec2Client object and make an API with wrong param value\nphp\ntry {\n    $ec2->runScheduledInstances([\n        'ScheduledInstanceId' => 'sdasda',\n        'InstanceCount' => 1,\n        'LaunchSpecification' => [\n            'ImageId' => 'sdasda'\n        ]\n    ]);\n} catch (\\Aws\\Ec2\\Exception\\Ec2Exception $e) {\n    echo $e->getMessage();\n}\nIt catches exception and gives error message correctly.\nAlso, recently our Getting Help section has been updated, feel free to engaged in other channels as well. If you still finds it might be a bug regarding this repo, it would be great if you could share your SDK version, php version, guzzle version, OS version etc., and we can see if we could reproduce it there.. @ivankristic Glad that you work it out!\nIt's true that Ec2 doesn't provide those directly, since instance types and image types is always increasing, and api operation always requires more specific information to get expected results.\nIt surprises me that you are hard coding all those values, may I ask why you are doing so? Is this some feature that you would like to see? \nClosing issue for now since the issue is resolved, yet feel free to reopen or open a new issue with further explanation regarding potential feature request : ). @duythien Here is the paginator guide. The paginator is essentially an iterator of results and flexible for use.\nFor ListObject or ListObjectV2, you can specify the MaxKeys to limit number of keys returned in the response.. Exactly what I was thinking to do : )\n:shipit: . @sanfx Also, we have updated our Getting Help section recently, feel free to engaged in other channel for help. \nI did a quick check for dynamoDB local, and couldn't reproduce the issue. Since your error message indicates MissingAuthenticationToken, are you using session token in your credentials? is it valid as well? Is using dynamoDB local the only situation that you have this error with? Could you double check your credentials in the file are valid and sourced correctly? Above are some quick troubleshooting suggestion that you might want to look at.\nIf you still finds this might be a bug for this repo, could you provide more background information that could help us reproduce the issue? For example, is SDK used directly or integrated with other frameworks? etc.\nClosing due to ages, yet feel free to reopen with further question and details :). @Sazpaimon Appreciate the contribution and information! I fully understand that those language bites are unpleasant, and I heard you for those situation described. \nYou code fix looks good with tests, yet I'm wondering how/ or whether should we take this, definitely DynamoDb Marshaler is not the only place that uses this (Such as some protocol request serializer body etc.) And I'm feeling like merging temporary language fix one by one doesn't sound like a maintainable and sustainable solution.\nThoughts?\n@imshashank @jeskew @mtdowling . Thank you all for inputs! I merged this and will add task in our backlog to track following fixes.. LGTM . Closing due to ages, yet feel free to reopen with further question or details : ). Closing and track #1192 instead. @oaksu Thanks for the info, if this is the configuration shared across client, you could try\nphp\n'http' => [\n    'proxy' => ...\n]\ninstead of @http, @http is the configure that passed per command.\nMore proxy documentation.\nAlso, we have updated our getting help section recently, feel free to engaged in other channel as well. If you still finds this might be a bug for this repo, feel free to reopen, and it would be great if you could provide some code snippet or logs that helps reproducing : ). Thanks for the fix! Looks good to me, I'll merge it : ). @anniballo I noticed you have mentioned \"the problem happens just for a few of them\", are you suggesting the error occurs with several specific request only? Could you provide some code snippets that might help us reproduce the issue? Also, may I ask the PHP version that you are using?\nMeanwhile, our README.md#Getting Help  section has been updated recently, feel free to get engaged in other channels to get help as well.. @joshdifabio Looks like this might be a bug that we need to fix, taking a look, I'll open a PR the address the issue \nThank you for reporting : ). @joshdifabio Actually, not. PHP does have the same behavior with java.\nThe php example provided gives $retries from 0, that's the correct behavior for no retries.\nWhen looking at it from 1, it matches.\nThough the initial delay of up to 50ms. is not caught, taking a look at the docs right now .... @jeskew Ah I see, thanks for pointing that out. I'll open a PR to fix that.. The fix PR has been merged, the fix will be tagged in next release. Closing, yet feel free to reopen with further question and details : ). @4406arthur Quick question, as from the doc link, S3 doesn't listed in the services that support sigv2. So may I ask more information? Such as how you find having sigv2 in v3 useful? \nGenerally, Signature V4 is highly recommend across AWS services, and we intentionally avoid V2 signing in V3, so I'm curious and happy to hear your feedback on this : ). @seliger Thanks for the information! This is a know bug, PR #1171 is opened to fix the issue, we are working on that.\nA proposed workaround currently is having endpoint param hardcoded in the client as:\nphp\n$appStream = new AppStreamClient([\n    'region' => 'us-east-1',\n    'version' => 'latest',\n    'endpoint' => 'https://appstream2.us-east-1.amazonaws.com/'\n]);\nAppreciate your patience : )\nUpdate: tracking #1192 instead.. @artifakt-io Thanks for reporting, taking a look .... @artifakt-io Quick question, what's the PHP SDK version that is used?\nCould you show me some code snippets that could help reproduce the issue?\nWhen I tried with latest version of SDK with:\n``` php\n$s = new \\Aws\\Shield\\ShieldClient([\n    'region' => 'us-east-1',\n    'version' => 'latest'\n]);\nvar_dump($s->listAttacks([]));\n``\nAnd it doesn't yell at me with errors? \nSoendpoint.json` file is shared across SDKs that is not appropriate for customization, yet if you have some interesting findings that suggests that this should be a global endpoint, I'm happy to see and try to fix this from root : ). @AndyDunn Quick question, may I ask the PHP version used? PHP SDK V3 requires PHP >= 5.5. @AndyDunn Interesting, are you install PHP SDK V3 freshly? (including Guzzle)? Would it be possible that you might have some old version guzzle installed somewhere?. LGTM :shipit: . @madduci That's expected, our service clients are automatically generated instead of hand written. \nOur API docs can be found here, also we have a SNS Message Validator  which could be used solo as well : ). @jeskew Nice, I'll take a look shortly.\nA quick question, initially, when introducing idempotency_auto_fill, I thought about having that as client wide option, yet didn't do so due to performance concerns. Since this will require additional iterates for all members. Especially when it defaults to true.\nRuby has this enabled as default across all client though, yet I'm not sure will this hurts the performance for PHP? Any thoughts?. LGTM :shipit: . @Fanda36 Thanks for the information, looks like you might be looking for Presign-Post instead of Presigned Url. The Link I provided contains instruction for usage : )\nAlso, we have updated our #getting help sections recently, feel free to get involved in other channels as well. Happy to help with any possible SDK bugs and feature requests : ). Closing due to ages, yet feel free to reopen with further comments or questions : ). PR #1219 containing the fix has been merged, closing. Thanks to all : ). @NinoSkopac Could you explain a bit how you find this doesn't work? I tried to reproduce this with a PollyClient, yet retries parameter works fine for me. I can see that when I changes retries number, retry attempts changes accordingly.\nYou could try to verify by turning on debug or http-debug options to see what's going on wire.. @NinoSkopac I noticed that you have mentioned 'async' behavior, different people might do this quite differently, so it would be great if you could provide some code snippets that might help us reproduce the issue : ). LGTM :shipit: . @jeskew Sounds good to me, :shipit: . Have talked offline and verified changes, LGTM, :shipit: . Since the jsonPolicy is set after the policy has been fully updated, and in signer, it just use an base64_encode version of the policy, so I do think the raw json version policy is final while the policy in signature is not final.\nYet it also make sense to me if it's private, because now it's called in junit test for checking policy before signer. Thus in case of customer usage, this function doesn't matter that much to be exposed as public.\n. This first if statement seems to be the same with function resolveRegion($service, $region) despite that array is returned. While I can see that these 2 functions have different purpose, can you resolve the duplicates? Since $region parameter is only returned when the if check fails?\n. While the logic look right, I'm little confused about the array operations going on there, is the $possibilities just has single element? If not, why just pop the last element of the result (especially when 'anonymous' ranks last)? If so, is there any privilege for using array_intersect for checking existence of an element in an array?\n. It is just a personal thing that I feel it is more nature and common to name the getter and setter like 'getOffset', 'setOffset', 'unsetOffset'. \n. In consideration for completeness and consistency, how about adding the test for unknown region and check whether default region is added as well?\n. I see, make sense to me.\n. Okay, then it should be alright.\n. Make sense, I'll make that consistent.\n. I see, are you suggesting it is the \"ContentSHA256\" causing the trouble only?\n. That's true, I'm also thinking about testing it throughly, will follow up with a new commit.\n. I'd suggest mock parameter in testing instead of constant, which feels more nature in mocking client behavior.\n$client->getConfig('client.backoff');\n$client->getConfig('client.backoff.retries'));\n. Make sense.\n. That's true, make sense.\n. I thought about this, will add tests for tracking InProgress\n. Make sense, done in latest commit.\n. Make sense, will do!\n. That makes perfect sense to me, changes is made in the latest commit. @mtdowling \n. @mtdowling That's true, changes are made in the latest commit.\n. Thanks for notice! It actually works at my local v2.8.1, and its original commit merged in 2009, I'll find out the initial version that this feature comes out.\nAdd Notes:\ngit status --porcelain is tagged since git v1.7.0\n. I made changes in getEcsUri() to return a complete version of URI instead of part of it. Also I removed input $url param in request() to let it call getEcsUri() by itself.\n. Documentation added.\n. Actually it does. A sample assume role profile will look like:\n[assumes_role]\nrole_arn=arn:aws:iam::123456789:role/foo\nsource_profile=default\nregion=us-west-2\nThe source_profile is the profile name used to source the credential to create the StsClient making assumeRole API call, it could be a static profile name in ~/.aws/credentials or ~/.aws/config or an assume role profile name in either of those files. If it is not provided, SDK will only use the credential provider chain options after the shared credential/config files, in current case, SDK would check the EC2 or ECS credential providers, and nothing else, to attempt to assume the role.\n. That's true, so, if the source_profile is provided, there could be situations:\n1. source_profile is same with current assume role profile looking at:\n   SDK will use the credential provider chain options after assume role, to avoid an infinite loop. Currently:\n   - Shared Credential Profile Static Credentials (This Profile)\n   - Shared Config Profile Static Credentials (This Profile)\n2. source_profile is a different profile:\n   SDK will use the credential provider chain options beginning with assume role, which does allow for chaining assume role calls together. Currently:\n   - Assume Role Credentials (Other Profile)\n   - Shared Credential Profile Static Credentials (Other Profile)\n   - Shared Config Profile Static Credentials (Other Profile)\nThat's the general behavior across SDKs, since PHP SDK also support fileName where a customize file path could be provided besides credentials or config file. That customized file should check before these two default files. Those unified logic does appear complicated, I'll add detail explanation in our guide before this gets merged.\n. I thought about this, from OO design perspective, it should be a private helper method, yet making it public helps making testing much more cleaner and easier. assumeRole is tricky compared to other instance credential providers since it needs to resolve credentials and verify parameters before initializing AssumeRoleCredentialProvider. I'd like to hear your thoughts on this, if you think it's definitely crucial to keep it private, I could try to figure out testing around that.\n. Oops, that's true.\n. Make sense to me, I thought exposing version parameter itself might appear superfluous yet also worried about the possibility for client customization. I'll remove the comment and let the client a public option there.\n. Renaming done.\n. Since there is a profile parameter already (which checks for credential file only), I thought it could be handy when customer knows clearly what they are doing, they don't need to create a provider. It doesn't overlap with AWS_PROFILE, it just make it quicker in deciding which type of file is looking at.\nFor other SDKs, I know Ruby just has profile parameter only, and it use credential chain to check credential file before checking config file.\n. I thought about that, I didn't pass in the $config array because, I'd like to have the defaultProvider() plays the role of our SDK default credential chain instead of a provider where any customization could happen. I think it's a general good practice for our customer to be aware of what credential provider they are using when they really wants to apply some customization. Thoughts?\nThe current $config living there is serving for those instance credential providers.\n. Oh I might misunderstand your comments, are you suggesting $config is not being used by EcsCredentialProvider or InstanceCredentialProvider inside assumeRole resolve chain?\n. If $reason is an array, $reason['exception'] will get the $previousone. Then getMessage() is not applied to array.\n. Actually, the behavior stays the same. Because checkProfile is called first before getProfileData, it would have thrown exception already for this situation. Current test covers this already. Let me know if I misunderstood what you are suggesting :)\n. @xibz That's true, thanks for catching, I'll fix it!. There is a problem with the logic here, even with ENV existing, there is possibility that the ecs credential fails when making the request, in that case, instanceProfile needs to be retrieved.. Reference latest SQS model: https://github.com/aws/aws-sdk-php/blob/master/src/data/sqs/2012-11-05/api-2.json#L266, I didn't see MessageGroupId to be a required parameter?. Adding to last comment, I'm confused since this is not a member for the operation either? Could you tell me more about how you find adding this helpful?. That's true, however, for a default credential provider, it's aimed at a general credential chain that makes most sense instead of catching all possible scenarios. Especially, this a unified behavior across all SDKs for the situation that failing fetching ECS credentials will be followed with an Ec2 metadata credential attempt. Rather than return ecsCredentials solo, I'd prefer something like below:\nphp\nreturn self::chain(\n    self::ecsCredentials($config),\n    self::instanceProfile($config)\n);. How about  changing this to one line like:\nphp\nif(empty(getenv(EcsCredentialProvider::ENV_URI))){\n    ...\nIn this way, if the ENV present with an empty string, it will fail quickly as well.. I'm in favored of having a client passed in actually, having credentials and region makes it appears more convenient at first glance but actually introduces more constraints. I'll change that :) . Yes, thanks for catching!. Yes I have:  https://github.com/aws/aws-sdk-php/pull/1137/files#diff-f3a01c5f1a23795bd23ca728685e3449R385. Hmm, make sense, will fix that. Hmm that's true, I was thinking about the API call scenario there ..., I should have made it accept a more general exception there.. I can make it wrapped as a previous exception there, this makes more sense if the exception caught is not only scoped to an AwsException.. Agreed, I added documentation and changed this to the constructor in the latest commit :). Yeah, I also feel weird about leaving the scalar argument there. I hesitated to put it in config array originally because all options for config is documented publicly, and I'm not that sure whether exception_class parameter is appropriate (or make sense?) to document there.\nI moved it to the config class with renaming in the latest commit :). Actually, no. Taking a second thought, providing Bucket and Key is actually more useful, they could be concatenated easily anyways. I changed those to Bucket and Key in the last commit :). That's correct, and makes perfect sense to me, thanks Michael. I fixed it and did renaming in the latest commit.\nI have also done some manually testing with \"real\" Exceptions and checked this work as expected :). There is an extra '#'. Could you provide a cooler tag link like: http://stackoverflow.com/questions/tagged/aws-php-sdk ?. Are we going to remove 'nextrelease' directory sometime? When can it not be a dir?\nIf that's the case, could we throw exception let release to stop instead of passing silently? (Maybe you are thinking doc only releases)?. Could you change those nested if to a cleaner way? Such as 'docupdate' continue, set flag to true when there is new service, and add entry for changes?. Yeah, that's true,, I also noticed the co-exist \"appstream\" and \"appstream2\", they cleared up the \"appstream\" 6 days ago, that's why changes are not synced. I try remove \"appstream\" and keep \"appstream2\" only in endpoint.json file. Yet it still doesn't work because we use service identifier.\nMake sense, I'll make the service change instead.. @jeskew Actually... perhaps not, when I try to change service identifier, it will fail at resolving api models:\nAws\\Exception\\UnresolvedApiException: The appstream2 service does not have version: ..\nThis is because their api model is not named as appstream2 .... Instead putting all methods into this ChangelogBuilder with same order as script file, I'm actually suggesting having a ChangelogBuilder class that could be used to initialize ChangelogBuilder objects, perhaps a class that takes directory parameter or some option array for constructor, and then have some method to be called. You can take a look at other builder class we have in the repo, for example this.. Some formatting suggestions, there should be space around '.'. For example, if we have an OO class, we could have $changelogEntries as a private field, once initialize, could be easily called by other methods.. Sorry perhaps my example mislead you there,  I'm actually suggesting if (empty($files)) {. Space around '='. For those helper method, perhaps we could have it private?. Could you move documentation block here?. Testing expected exceptions, you could follow: https://phpunit.de/manual/current/en/appendixes.annotations.html#appendixes.annotations.expectedException\nSome example in our repo: https://github.com/aws/aws-sdk-php/blob/master/tests/JsonCompilerTest.php#L25. This might be same, but could you make sure that those function naming also follows camelCases?. Same @expectedException tag suggestions as above. Unit test will based on behaviors, not necessarily goes to one per method. You could test different behaviors against public methods that will be called for a ChangelogBuilder object. Perhaps I'm missing something but I don't think cleanJSON is a method that have to be used publicly as a separate call every time ?. If the read changelog is only called once across all those methods, you can have it as a variable there. Yet if it will be called for multiple methods, I'd recommend having a field for it.\nOf course it's up to you to take my suggestions or not.. This line is kind of too long, could you make it in 2 lines? perhaps separate a $msg variable, then throws the exception?. This else block context looks like doesn't need to be in the block?. \\Exception seems too general, could you try a more specific one? maybe RuntimeException ?. same, might not necessarily needs to be inside the else block, if exception occurs, should have been stopped anyway. Same more specific exception suggestion. Could you add a bit more documentation of fields descriptions and their types?. Ah, interestingly parameters inputs looks like this, it's a bit weird and not self-explanatory.\nYou could have a param array for the constructor, which would allow more flexible changes in the future.\nOr, a easier short term way, change the order for constructer, to have $verboseFlag go first, then others could have default value as empty string. This will make constructor looks cleaner like: new ChangelogBuilder(true);. needs a new line here. an extra line. Looks like have extra new lines?. an extra line : ). Maybe (array $params)?. An extra line could be removed. Make sense, I used to make it exactly like MultipartException scenario, taking a second look, it does provides enough info. in this way. I'll fix that : ). @jeskew Actually I thought about the this behavior inefficiency there, so I did empty check for those values in case I overwrote them. Though this still doesn't avoid unnecessary iterates.\nTaking a look at array_pop(), it will remove the element from prev, which will leave incomplete messaging. Looks like php doesn't have something like array_peek? I didn't find a clean way to do peek(). Thoughts? : ). Maybe my naming here is kind of confusing? What I'm trying to use the class namespace directly like this, using S3MultipartUploadException::class doesn't seem to work.\nMaybe you are suggesting better way that I did get? Could you explain a bit about it? Or perhaps I can do better naming for the parameter there? : ). Awesome, thanks! I'll fix this. Good to know, thanks! I'll fix it. Looks good, just curious, is this fix as work-around for some unexpected global endpoints or this is a normal behavior?. Great, it would also help reduce the time when changelogBuilder is trying to replacing string : ). Hmm, that's true, same as how I figured. Anyways, this looks good : ). Has this change be covered in tests?. Maybe also some tests in ClientResolver?. This is great  : ) Thanks for adding this. I thought about using isset originally, then it could potentially allow an empty string as input. Thoughts?. Maybe just [] ?. The 'unsigned' parameter name sound a bit vague, is might cause the confusion that it's trying to unsign request completely. Could you rename it perhaps something like unsign-body?. Perhaps also add some documentation for those possible values and a bit description?. This works, yet the check here feels a bit weird, since getPayload has already checked both $this->unsigned and http scheme, could we check whether $payload is self::UNSIGNED_PAYLOAD instead?. these are all test cases with https, maybe also add test for http and verify it follows regular V4. Besides verify the header exists, perhaps also verifys it's using 'UNSIGNED-PAYLOAD'. Looks good, could you also point out that this relates to APIs with streaming behaviors? . using $this->unsigned appears simple at first glance, yet as you have also mentioned, this check in targeting at whether a request is valid for unsigned payload, so a complete check is checking the boolean and scheme as same time, which is already done so in the getPayload. So even this works, this check is incomplete, and a complete check will duplicate the check here. That's why I'm suggesting checking payload value directly.. Any reason why not use if($shape->offsetGet('jsonvalue')) directly?  offsetGet returns value when the key is set or it will returns null, php if takes care of these.. Same. Why decode then encode there? Perhaps you are trying to expect customer provide an valid json string input? If so why not use it directly with base64_encode? Also, if we are relying on customers to provide a vaild JSON string, could you somehow raise error if they are providing invalid inputs? Or maybe better documentation for what's expected for those inputs?. Perhaps we could call out the fact that we are using json_encode, which could help debugging if this cause troubles for some customer. Such as:\nUnable to encode the provided value with 'json_encode'.. I'm a bit confused with those check, maybe we could just simply skip the check like: \nphp\nif ($shape['jsonvalue']) {\n    return;\n}\nI think, since you already have error handling in RestSerializer than throws error for those objects that couldn't be JSON serialized, and the encode value would be string anyways if succeed, what's the purpose for verifying the actually object type there?. Could you remove the extra line?. Also here?. Maybe a better naming for this helper method? like computeMessageAttributesMd5 or perhaps caculateMessageAttributesMd5 or perhaps messageAttributesMd5? get feels like fetching value from response instead of constructing it. Perhaps to echo this for MD5OfBody, you could have another function bodyMd5(..) that simply returns md5($msg['Body'] when available.. From the feature request thread, looks like customer is looking for this functionality for sendMessage as well?. Any reason for this helper function being public?. It would be great if we are checking MD5OfMessageAttributes and MD5OfBody in the same pattern here.\nI'd prefer the \"non-nesting if\" we have for MD5OfBody currently:\n``` php\nif (isset($msg['MD5OfBody'])\n    && self::calculateBodyMd5($msg) !== $msg['MD5OfBody']) ...\n// Then\nif (isset($msg['MD5OfMessageAttributes'])\n    && self:: calculateMessageAttributesMd5($msg) !== $msg['MD5OfMessageAttributes']) ...\n``. Sounds cool, thanks for the explain : ). If the exception caught is not \"AuthorizationHeaderMalformed\", does this just make the exception \"slient\"?. Just a bit thought, I'm open on this. Could we make the error code check atdetermineBucketRegionFromExceptionBodyfunction instead perhaps? This helps reduce duplicate code, and easy to extend if there will be another special case in the future. \nHmm, yet it might be a bit trickier to do this at the Multi region client.. Looks like callinggetDetermineBucketRegionGeneratorhere is checkingAuthorizationHeaderMalformedtwice if cache is not found. This doesn't hurt, though might be worth considering separate the checking logic? (below). To be consistent with the way you throw exceptions above, perhaps just throw this after theifcheck instead of in aelse` block?. Same here. Maybe add a bit document about supporting stream wrapper format there. why a hash? why not a array of supported ciphers?. Typo, decrypted text data?. Hmmm \"return\" is sufficient without \"break\"?. Hmm PHP will return null if there is no return. What's the failed scenario look like? Especially for \"getObject\"? It will be great if some error scenario (besides invalid arguments) can be tested as well. Does this integration test used to succeed with \"continue\"?. ",
    "stristr": "Extra info: this has been tried with and without curl.options throwing a fork in things. Everything else seems very vanilla. I know there's an exponential backoff strategy used by default for large transfers, but is there one for making the multipart request? Should we need one?\n. @mtdowling...so I'm on an m1.medium, with two listeners on two different SQS queues and relatively busy with CMS web traffic. To reiterate, though, I'm getting these errors on multipart upload request creation, not on transfer, so I don't believe concurrency should have anything to do with it. So I'll try a larger instance. Is the exponential backoff strategy definitely in place for the service request that creates the multipart upload ID?\n. @mtdowling, no, not executing multiple multi-part uploads from the same instance. I am, however:\n- Polling SQS every 3 seconds on two separate listeners\n- One job queue is the one described here, which makes multi-part uploads in a single thread\n- The other job queue is frequently dispatching push tokens to SNS\nThis is mainly a CMS/worker server. It isn't doing very much web traffic in the grand scheme of things. Both jobs and the CMS are talking to RDS.\nI resized to m3.medium (it was on my list anyway), and have turned on CURLOPT_VERBOSE. I'll send more info as it becomes available.\n. @mtdowling: here is a typical failing request:\n```\n Connected to %bucket%.s3.amazonaws.com (176.32.97.201) port 443 (#427)\n   CAfile: /path/to/vendor/guzzle/guzzle/src/Guzzle/Http/Resources/cacert.pem\n  CApath: none\n SSL connection using TLS_RSA_WITH_AES_128_CBC_SHA\n Server certificate:\n       subject: CN=.s3.amazonaws.com,O=Amazon.com Inc.,L=Seattle,ST=Washington,C=US\n       start date: Apr 09 00:00:00 2014 GMT\n       expire date: Apr 09 23:59:59 2015 GMT\n       common name: .s3.amazonaws.com\n*       issuer: CN=VeriSign Class 3 Secure Server CA - G3,OU=Terms of use at https://www.verisign.com/rpa (c)10,OU=VeriSign Trust Network,O=\"VeriSign, Inc.\",C=US\n\nPOST /path/to/file?uploads HTTP/1.0\nHost: %bucket%.s3.amazonaws.com\nx-amz-acl: public-read\nCache-Control: max-age=31104000, public\nContent-Type: image/jpeg\nExpires: Sun, 24 May 2015 03:24:45 GMT\nUser-Agent: aws-sdk-php2/2.6.5 Guzzle/3.9.1 curl/7.35.0 PHP/5.5.12 MUP\nDate: Thu, 29 May 2014 03:24:45 +0000\nAuthorization: AWS %access-token%\nContent-Length: 0\n\nHTTP/1.1 200 OK\n< x-amz-id-2: %amz-id-2%\n< x-amz-request-id: %request-id%\n< Date: Thu, 29 May 2014 03:24:46 GMT^M\n< Connection: close\n Server AmazonS3 is not blacklisted\n< Server: AmazonS3\n<\n SSL read: errno -5961 (PR_CONNECT_RESET_ERROR)\n TCP connection reset by peer\n Closing connection 427\n```\nThe constants are:\n- Content-Length: 0 on the request\n- SSL read: errno -5961 (PR_CONNECT_RESET_ERROR) on the response\n. @mtdowling,\nSo I can try it with HTTP/1.1. The reason I'm not is because of the error in the closed ticket #173\u2014I am getting this same \"impossible\" error ([curl] 56: Problem (2) in the Chunked-Encoded data). This other problem manifests only for certain files, only in the context of a reused HTTP/1.1 keep-alive connection. Here's the example verbose cURL output for that issue (not very helpful):\n```\n Connection #120 to host %bucket%.s3.amazonaws.com left intact\n Found bundle for host %bucket%.s3.amazonaws.com: 0x1d55520\n Re-using existing connection! (#120) with host %bucket%.s3.amazonaws.com\n Connected to %bucket%.s3.amazonaws.com (205.251.243.89) port 443 (#120)\n\nPUT /path/to/file.ext partNumber=1&uploadId=e88zMWExPon9LCsC.Bnb9HodmaGO7kDkxoWtl.xtSNJeA69ekxjkzd48j2StmdRN.ERi_m558214Zt8WvSnl4Q-- HTTP/1.1\nHost: %bucket%.s3.amazonaws.com\nContent-MD5: UuFGFaCQghNwYxOzByF34w==\nUser-Agent: aws-sdk-php2/2.6.0 Guzzle/3.7.4 curl/7.35.0 PHP/5.4.23 MUP\n...\n We are completely uploaded and fine\n< HTTP/1.1 200 OK\n< x-amz-id-2: %amz-id-2%\n< x-amz-request-id: %request-id%\n< Date: Sun, 01 Jun 2014 01:10:45 GMT\n< Content-Type: application/xml\n< Transfer-Encoding: chunked\n Server AmazonS3 is not blacklisted\n< Server: AmazonS3\n< \n Problem (2) in the Chunked-Encoded data\n Closing connection 120\n```\n\n(Different server and different version of PHP...same version of cURL and the SDK...and I\"ve seen this issue on every version of PHP we use. Again, only certain files. But retrying these errors doesn't help.)\n. @mtdowling, any chance you had a chance to look at this?\n. @mtdowling, upgrading Guzzle didn't work.\nHowever, good news\u2014discretely upgrading cURL to 7.36.0-2.45.amzn1 seems to have fixed all problems.\nI'm sorry, because this obviously wasn't a problem with SDK at all. The cURL release notes for 7.36.0 specifically talks about some NSS fixes, so I'm guessing this is what it was.\nClosing out...thanks for all your help!  Maybe you can tell your friends on the Amazon Linux AMI team your users want them to switch to OpenSSL :smile:\n. ",
    "keithyau": "No, im using a EC2 directly calling the API.\nPrevious issue: \nhttps://github.com/boto/boto/issues/1673\nhttps://groups.google.com/forum/#!topic/boto-dev/1et5yX6o7s8\nCode snip, just a simply createStack call:\n``` php\n   $stack = array(\n          'StackName' => $stack_name.'-vpc',\n          'Parameters' => array(\n                array(\n                    'ParameterKey' => 'AZ1',\n                    'ParameterValue' => $user_AZ[0],\n                ),\n                array(\n                    'ParameterKey' => 'AZ2',\n                    'ParameterValue' => $user_AZ[1],\n                ),\n          ),\n          'TemplateBody' => $stack_template_body,\n          'TimeoutInMinutes' => 20,\n          'NotificationARNs' => array($ARN_string . \"MGC-STATUS-CHECK\"),\n          'OnFailure' => 'DELETE',\n         ...\n        );\n    try {\n          $result = $cf->createStack($stack);\n    } catch (Exception $e) {\n      error_log($e->getMessage());\n    }\n\n```\nPlease kindly advice and many thanks\n. For the PHP SDK, how to provide the debug output ? we switched on display error, and in the error_log we got php fatal error:505\nPlease help and thanks\nKeith\n. And it really looks like the boto issue i shared with \"CloudFormation\" \"Create Stack\" \"505\". Or please kindly advice if i made anything wrong.\n. No, we are not talking about Auto-scaling here. Since we are just trying to create VPC and it fails, sometimes it create auto-scaling group will fail as well.\nThe issue is not region specific. Just it happens more in SINGAPORE, others got fewer rate of fail, but still happening.\nOur template is not excess the template body limit which is 51200 bytes. So, i believe it is not about uploading to S3. http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/cloudformation-limits.html\n. i want to fix the problem as well, just the maintainer here closed the issue :(\nAWS forum say resolved and here say not his business ~~\n. I believe we already try best in AWS forum\nhttps://forums.aws.amazon.com/message.jspa?messageID=548383#548383\n. ",
    "chankongching": "Dear Support,\nThe mentioned issue only exist in Singapore AZ.\nOn Wed, Jun 4, 2014 at 8:00 PM, Michael Dowling notifications@github.com\nwrote:\n\nI don't think that Boto issue is related to this issue. Can you provide\ndebug output of the call?\nhttp://docs.aws.amazon.com/aws-sdk-php/guide/latest/faq.html#how-can-i-see-what-data-is-sent-over-the-wire\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/aws/aws-sdk-php/issues/299#issuecomment-45143502.\n\n\nRegards,\nChan Kong Ching,\nJacky\n. ",
    "kevin-canadian": "Has anyone ever resolved this? I'm seeing the 505 error sporadically from both \"aws cloudformation validate-template\" and \"aws cloudformation create-stack\". The problem happens about 5% of the time using eu-west-1, and 50% of the time using ap-southeast-1 (I'm located in Sweden).  My template is about 26000 bytes, and I'm using the AWS CLI.\n. ",
    "sergeychernyshev": "Sorry, my bad, I didn't realize this is the case - I clearly skipped the:\n$queueUrl = $result->get('QueueUrl');\nline in documentation.\nI was also unsure if the body is supposed to be encoded and didn't get suspicious when it was long and weird.\nThank you for the advise!\n. ",
    "globsecure": "@jeremeamia  - thank you for your attention, stay tuned for news of the AWS! \n. ",
    "ChrisTerBeke": "When is this happening? I'm waiting for this for a few months now and thinking about switching to another storage provider because you still aren't up to date.\n. The problem is that v3 currently still requires the cURL extension which is not available on for instance the google app engine platform. Even when I use the stream wrapper, instantiating the s3 client produces the error that the cURL constants are not defined.\n. @mtdowling thanks! It works now :)\n. So, in the Google App Engine case, I should upload it to could storage and then do something like gs://bucket/ca-bundle.crt?\n. I certainly will raise an issue with the GAE team, SSL seems something that can't be left out :P \nIn that case, I'll temporary disable SSL while in development mode.\n. How can I disable the SSL check? the following things do not work:\n'scheme'   => 'http'\n$client->disable_ssl_verification();\n$client->disable_ssl();\n. Are you sure the SDK is ignoring it, I still get the same errors:\nError executing Aws\\S3\\S3Client::putObject() on \"URL_HERE\"; No system CA bundle could be found in any of the the common system locations.\n. I still use the stream wrapper btw\n. The stack trace says the error starts here (in the AWS wrapper at least):\n\n. Maybe one step earlier: Aws\\S3\\StreamWrapper stream_flush\n. Thanks, I'll give it a shot and report back to you!\n. Worked! Is it cool if I send you some info on all the steps I needed to take to get it working on app engine? Maybe you can somehow incorporate that in your documentation?\n. I use Laravel on Google App engine, never had this problem before until a few days ago.\n. ",
    "tarunlalwani": "Hi Jereme,\nThanks for the update. Finally I did something like that only, only except the FilterIterator i used the array_filter method of PHP after converting the iterator to array.\nRegards,\nTarun\n. ",
    "radford": "Yes, with this fix you can actually wait for a volume.\n. @jeremeamia, I thought the same thing when I wrote my first version of the patch.  After looking at the code in checkPath() and the other service definitions, I decided that success.value and failure.value were with respect to the wait, not the operation, i.e. failure means the operation can never finish while as success indicate the operation finished.  Let me know if this is not correct and I'll update the patch accordingly.\n. @samsamm777, FWIW I'm still interested in this, and I'd be happy to try and update my patch given some feedback and/or direction.\n. @samsamm777, thanks for pointing that out.  They weren't in v3 last time I looked.  That's enough motivation to try upgrading again.\n. It does look that way.  I attempted to fix up the DescribeInstances configuration in a way that was compatible with v2 in #421.\n. Oops, this was on the wrong branch,  I made a new one (#421) on v3.\n. 2.6.14\n. 2.7.9 as well.\n. It looks like this is an optimization for the case of an empty prefix.  But it's only an optimization for large uploads to a relatively empty bucket.  When you have a small upload to a large bucket this is slow.  For my test case, when the optimization makes sense (small bucket) it's 6x faster and when it doesn't (huge bucket) it's 6x slower.\nEither way it seems wrong to be trying to open URLs locally, for which there is zero chance of success, especially when the API has no URLs in it to begin with.\n. Thanks for taking the time to look into this.\nAssuming your abcba bucket has many files in it.  Run\nstrace -e lstat php ./upload.php\nwhere upload.php is your script and you should see an lstat(2) call for all the php source files followed by one for every file in the bucket with the following form.\nlstat(\"/home/mtdowling/...blah.../s3://abcba.s3.amazonaws.com/...blah...\");\nYou shouldn't need debug nor any curl.options to see the effect.\n. > I'm not seeing any performance impact from this.\nI see a large performance impact.  How long does it take to recursively list your destination bucket?  If it's not minutes, then then your test bucket isn't big enough. :)\n. > how are you measuring the performance impact?\nIt takes 10 minutes to list my bucket and 6 seconds to upload my directory of small files with a prefix.  If the prefix is \"\" however then it takes 10 minutes to upload my directory of small files for a slowdown of 100x.\n. The fix seems to work; now lstat(2) is no longer called for ever file in the bucket.  FWIW, I do still see a call to gettimeofday(2) for every file in the bucket.\nLooking at your fix, I'm surprised it wasn't in the StreamWrapper as Calling getRealPath() is still \"broken\" for anyone who calls it.\nI still see 80% CPU utilization for 22s to upload 5 zero sized files to a relatively small bucket and 80% for over 20minutes (and counting; who knows how long it'll take) to my large bucket.\n. Thanks for fixing the obvious bug of calling lstat(2) on URLs, but there still remains the different behavior for uploadDirectory() based on prefix.  Given that the difference can be so drastic, you might consider documenting the behavior so that others can choose the correct one for their use case.  Something like: \"if the prefix is \"\", uploadDirectory() will traverse the entire bucket which can be very inefficient when uploading to large buckets.\"\n. Good idea.  I'll update the patch.\n. @jeskew, any update on this?\n. ",
    "ChrisZieba": "+1 for a fix\n. Yes, thanks @jeremeamia !\n. ",
    "et304383": "This affects 5.4.29 PHP as well.\nThis is going to require a fix to the SDK otherwise the latest point release of PHP cannot be used.\n. When will this be available for download?\n. I haven't had much luck running the build for the phar file.  Can you provide a way to download an updated phar file please?\n. We don't use composer and need the phar file manually.  A couple hours is blocking dozens of developers from working.  Any way to please rush this?  The php release (5.4.29) was May 29th so this has been broken for three weeks now.\n. I believe it did for us.\n. Neither.  Right now, I'm using a python script to do the Signature 4 signing as outlined here:\nhttp://docs.aws.amazon.com/general/latest/gr/sigv4-signed-request-examples.html\nWith modifications for Elasticsearch of course.\nThen, I have to put it into Lambda (tried deploying under WSGI - wow, what a pain) and then put API Gateway in front of it and pass in temporary credentials.  Basically, I had to write an entire REST API because none of the SDKs support the application operations against Elasticsearch in AWS (just domain operations at the AWS object level).\n. I appreciate the work-arounds.  Are you implying that support for the ES API through the AWS SDK won't be added anytime soon?\n. Can you clarify where $request comes from in the example you've given?\n. @cjyclaire I don't believe this was resolved with #1137 .\nI am still encountering issues with using an assume role from the credentials file that looks like this:\n[myprofilethatassumesrole]\noutput = json\nregion = us-east-1\nrole_arn = arn:aws:iam::123456789012:role/MyAssumeRole\nsource_profile = myprofilewithcreds\nI get this exception:\nFatal error: Uncaught Aws\\Exception\\CredentialsException: No credentials present in INI profile 'myprofilethatassumesrole' (/var/www/.aws/credentials) in /var/www/html/vendor/aws/aws-sdk-php/src/Credentials/CredentialProvider.php:394\nI don't see in #1137 where a change was made to actually call this when the profile being used is an assume role type profile.  Having to change the code to work with assumed roles (by declaring a custom credential provider) isn't an option.. Does this include MFA support?  Or is that in a future release?\n. +1\n. Thanks!  Note I fixed my source_profile value (was profile1).  That was just a typo in my example.  My real config is correct.  :). So to start, I'm not using a terminal.  I'm doing this through a web server.  But to prove this isn't working on the CLI either, here are my files:\n[etucker: php_profiles_test]$ ll\ntotal 36\n-rw-rw-r-- 1 etucker etucker    54 Jul  4 11:19 composer.json\n-rw-rw-r-- 1 etucker etucker 12972 Jul 12 14:03 composer.lock\n-rw-rw-r-- 1 etucker etucker   236 Jul 12 14:02 index.php\ndrwxrwxr-x 8 etucker etucker  4096 Jul 12 14:03 vendor\ncomposer.json:\n{\n  \"require\": {\n    \"aws/aws-sdk-php\": \"3.31.0\"\n  }\n}\nindex.php:\n```\n<?php\nvar_dump(getenv('HOME'));\nvar_dump(getenv('AWS_PROFILE'));\nrequire 'vendor/autoload.php';\n$ec2_client = new Aws\\Ec2\\Ec2Client([\n  'region' => 'us-east-1',\n  'version' => 'latest'\n]);\nvar_dump($ec2_client->describeInstances());\n```\nRunning:\n```\n[etucker: php_profiles_test]$ export AWS_PROFILE=profile2\n[etucker: php_profiles_test]$ php index.php \n/home/etucker/play/php_profiles_test/index.php:2:\nstring(13) \"/home/etucker\"\n/home/etucker/play/php_profiles_test/index.php:3:\nstring(4) \"profile2\"\nPHP Fatal error:  Uncaught Aws\\Exception\\CredentialsException: Error retrieving credentials from the instance profile metadata server. (cURL error 28: Connection timed out after 1001 milliseconds (see http://curl.haxx.se/libcurl/c/libcurl-errors.html)) in /home/etucker/play/php_profiles_test/vendor/aws/aws-sdk-php/src/Credentials/InstanceProfileProvider.php:79\nStack trace:\n0 /home/etucker/play/php_profiles_test/vendor/guzzlehttp/promises/src/Promise.php(203): Aws\\Credentials\\InstanceProfileProvider->Aws\\Credentials{closure}(Object(GuzzleHttp\\Exception\\ConnectException))\n1 /home/etucker/play/php_profiles_test/vendor/guzzlehttp/promises/src/Promise.php(156): GuzzleHttp\\Promise\\Promise::callHandler(2, Array, Array)\n2 /home/etucker/play/php_profiles_test/vendor/guzzlehttp/promises/src/TaskQueue.php(47): GuzzleHttp\\Promise\\Promise::GuzzleHttp\\Promise{closure}()\n3 /home/etucker/play/php_profiles_test/vendor/guzzlehttp/guzzle/src/Handler/CurlMultiHandler.php(96): GuzzleHttp\\Promise\\TaskQueue->run()\n4 /home/etucker/play/php_pro in /home/etucker/play/php_profiles_test/vendor/aws/aws-sdk-php/src/Credentials/InstanceProfileProvider.php on line 79\n[etucker: php_profiles_test]$ \n```\nSo the profile isn't even working.  It's trying to call out to the metadata server.\nIf I explicitly set the profile:\nindex.php\n```\n<?php\nvar_dump(getenv('HOME'));\nvar_dump(getenv('AWS_PROFILE'));\nrequire 'vendor/autoload.php';\n$ec2_client = new Aws\\Ec2\\Ec2Client([\n  'profile' => 'profile2',\n  'region' => 'us-east-1',\n  'version' => 'latest'\n]);\nvar_dump($ec2_client->describeInstances());\nRunning:\n[etucker: php_profiles_test]$ php index.php \n/home/etucker/play/php_profiles_test/index.php:2:\nstring(13) \"/home/etucker\"\n/home/etucker/play/php_profiles_test/index.php:3:\nstring(4) \"profile2\"\nPHP Fatal error:  Uncaught Aws\\Exception\\CredentialsException: No credentials present in INI profile 'profile2' (/home/etucker/.aws/credentials) in /home/etucker/play/php_profiles_test/vendor/aws/aws-sdk-php/src/Credentials/CredentialProvider.php:394\nStack trace:\n0 /home/etucker/play/php_profiles_test/vendor/aws/aws-sdk-php/src/Credentials/CredentialProvider.php(303): Aws\\Credentials\\CredentialProvider::reject('No credentials ...')\n1 /home/etucker/play/php_profiles_test/vendor/aws/aws-sdk-php/src/Middleware.php(122): Aws\\Credentials\\CredentialProvider::Aws\\Credentials{closure}()\n2 /home/etucker/play/php_profiles_test/vendor/aws/aws-sdk-php/src/RetryMiddleware.php(159): Aws\\Middleware::Aws{closure}(Object(Aws\\Command), Object(GuzzleHttp\\Psr7\\Request))\n3 /home/etucker/play/php_profiles_test/vendor/aws/aws-sdk-php/src/Middleware.php(207): Aws\\RetryMiddleware->__invoke(Object(Aws\\Command), Object(GuzzleHttp\\Psr7\\Request))\n4 /home/etucker/play/php_profiles_test/vendor/aws/aws-sdk-php/src/ClientResolver.php(582): Aws\\Middleware:: in /home/etucker/play/php_profiles_test/vendor/aws/aws-sdk-php/src/Credentials/CredentialProvider.php on line 394\n```\nSo in this case I see the explicit error that the credentials aren't set.\nPut it this way - tell me where in the CredentialsProvider logic that you're checking for role_arn?\nI grepped the source code for role_arn and got no results.\n[etucker: aws-sdk-php]$ grep -r 'role_arn' .\n[etucker: aws-sdk-php]$\nThere is no logic to support role based profile in the source code.. Thank you for clarifying / admitting. That is exactly why I created #1030 a year ago.  It was closed but still hasn't been resolved.. @PavloPixart of course it's related to 1030.  That's why I mentioned it in the first line of the issue and again later on.. ",
    "evan-king": "Now that the fire is out, perhaps we can go back and address this a little more rigorously?  This wouldn't have happened and won't happen again if the checks being performed were bitwise comparisons against existing known individual flags.\n. https://github.com/aws/aws-sdk-php/pull/316\n. ",
    "stevenscg": "This sounds great!  We haven't hooked in to the lower-level functionality yet, so I wouldn't expect any significant problems upgrading.  PHP 5.4 requirement is fine as well.\n. @ArthurGuy @llernestal I'm having the same issue with createPresignedRequest() with almost the same setup as Lars. My aws/aws-sdk-php is 3.19.30.. I specifically downgraded from guzzlehttp/promises 1.3.0 to 1.2.0 with AWS SDK 3.19.30 and the issue was no longer present.. ",
    "moffe42": "Would be great. The AWS SDK is the final part of our software stack that needs to be upgraded, for us to use Guzzle 4. So please go ahead. But you might need to keep up maintenance on the 2.x branch, until the most of the community keeps up with Guzzle 4.\n. ",
    "WyriHaximus": "@jeremeamia He's not the only one :smile:. Seen stuff run on 5.2 not so long ago. Same goes for old MySQL or VCS software etc etc. @FrenkyNet is right, people are still running old software but they have a working SDK for that version. So we don't have to stay stuck there. I'm very welcome moving on and using Guzzle 4 in the SDK :+1:.\n. @mtdowling thanks for the compliments :+1: . Like to point out that making the handler work with the SDK is one of the reasons I started working on it (back in the Guzzle v4 era (it's a long story)). My main focus is to get it following the Guzzle handler specs and requirements and then start making sure it works well with the SDK.\n. ",
    "stof": "One of the thing to be careful about is that this will be a BC break in the SDK, as your return values are the Guzzle3 models in many places, and upgrading to Guzzle4 would then force to change the API of the SDK (unless you depend on Guzzle4 for the logic and on Guzzle3 at the same time for the returned model, which would be insane).\nI think this new major version of the SDK should decouple its public API from Guzzle, to avoid such issues in the future. Low levels APIs would use Guzzle, but the public API of the SDK should rely on its own model classes (or an arrays for collections). This would mean that most users of the SDK would not need to alter their code the next time a Guzzle major version happens.\n. This is only phpdoc, so it is not affecting their code. IDEs will still understand that iterable means array|\\Traversable.. Cyclic objects graph are not collected as soon as the variable goes out of scope (as the refcount never reaches 0), but the next time PHP decides to run the GC to delete unused cyclic graphs. gc_collect_cycles is a way to force PHP to run it now instead of waiting for PHP to run it (which may be delayed until PHP considers that it needs to free some memory).\nAn actual memory leakage would not release memory during GC. It would just keep increasing.\nIf you have disabled the GC in your php.ini, cyclic object graphs would indeed create memory leaks, but then it is your fault (as you disabled the mechanism aimed at releasing memory).. However, there is still a room for optimization in the SDK here to try to remove the cyclic object graph (so that refcounting can release the memory faster). Note that I'm perfectly fine about using the path-style URL for all API calls performed by my backend server though.. @kstich will it use foo.s3.amazonaws.com or foo.s3-eu-west-1.amazonaws.com by default for a bucket being in Ireland ? The doc linked in the release notes mention that both are valid. OK, I got my answer. It uses foo.s3-eu-west-1.amazonaws.com.. @kstich for a European bucket, is it better to use foo.s3.amazonaws.com or foo.s3-eu-west-1.amazonaws.com when referencing it ?. ",
    "fruitl00p": "Back on upgrading SDK -> Guzzle4: Upping the minimal version to 5.4+ seems reasonable if the current version would still receive some maintenance releases. Other than that: :+1: \n. ",
    "dwenaus": "please upgrade to Guzzle 4\n. ",
    "cdnsteve": "From my perspective, we're currently using PHP 5.3 due to our hosting providers setup at Acquia. They do support PHP 5.5 so we're eventually looking to migrate to that but PHP 5.4 isn't an option. \nIf the upstream has a new release we should support that too.\n. @jeremeamia \nInteresting, apparently its version 1.0. Is that deprecated?\n. Please note that the PHP sample provided here is using version 1.0\nhttp://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSGettingStartedGuide/PreparingSamples.html\nSame here:\nhttp://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSGettingStartedGuide/ListQueues.html\n. I can confirm that using version 2x of the SDK does not have this particular issue. It works well when using a try/catch block as expected.\nEG:\nError communicating with AWS SQS Service: The specified queue does not exist or you do not have access to [error]\nit.\n. ",
    "Sazpaimon": "Since PHP 5.3 is actually now EOL (see http://php.net/eol.php) it makes sense to bump up the minimum version of the SDK to 5.4 anyway.\n. I believe a literal JSON document would be the best option, as I think that's what most people that want to utilize maps and lists would be expecting to use. The lack of support for sets I don't see as particularly problematic as lists seem to cover all the use cases for sets unless looking at the raw API requests and responses. If the lack of binary support is an actual problem, one completely far out suggestion that I can think of is perhaps supporting BSON would be worth looking into, though I don't know what kind of can or worms that would open.\n. This seems to be a good start as far as supporting raw JSON goes. My hope is that we can get to a point where you can input and output native PHP types and have it just automagically work, but I'm still not sure how we can get there given everything that's been stated\n. Looks pretty good. If I had to many any suggestion as a workaround to support sets and binary types, would be to implement classes that basically provide the marshaller type hints those attributes. I don't know how practical such a thing would be, though.\nNow that I think about it, I think I just basically described the old Attribute class.\nAlso, for unmarshalling binary types, I would unbase64 the data (if it isn't already)\n. Supporting sets would be useful if the user is going to do something like ADD/DELETE later on, as AFAIK, the list/map type does not support appending via the UpdateItem method, or if you want to ensure at the data layer that all your items are the same type and unique, so lists cannot replace sets 100% of the time.\n. I'll just echo what I said in #357 as this might be more appropriate:\nThe Aws\\DynamoDb\\Enum\\Type class seems to have not been updated for the new attributes. Even more troubling, it seems all of the Dynamo Enums in V3 have been removed. Is there a reason for this? Using the Enums has been very useful in my project to help with verbosity when building Dynamo API calls. Are we expected to re-create these Enums ourselves?\n. Thanks for the response (even though I don't agree with all the points made, personally).\nAnother thing that seems to have been stripped down are exceptions. Version 2 had php exception types for the various Dynamo exceptions, but now they all seem to be rolled up until the very generic DynamoDBException class. What's the rationale for that?\n. I've already made my case known for supporting sets and binary, by using classes that provide some form of type hinting for attributes. Initially I was okay with not supporting sets, but the lack of UpdateItem support for lists and maps still leaves use cases for sets.\n. Sorry about that, I was still looking at AttributeUpdates instead of UpdateExpression. It looks like UpdateExpression covers that use case very well, then.\n. I'm not a fan of having to make it a configurable setting, instead of having a way for it to automagically just work. Maybe if the class required the client to be passed in instead, and have the marshaller figure it out from there, but that's only marginally better. Since V3 is just beyond the horizon, I can also accept this as just a quirk of the marshaller wait for V3 for it to be fixed.\n. Actually, the latter can be useful if you wanted to take action after the nth retry. For instance, if you have a DynamoDB table where after the 6th retry, you retry the request to another table.\n. I can also see usefulness in having the total amount of time that was spent sleeping as well\n. I feel it'd useful for giving an idea on how long a particular command affected a user, specifically for user-facing apps that need to communicate with, say DynamoDB tables.\n. That would be fine. I'm more thinking about ways to maintain feature parity with v2 of the SDK (which allowed you to put the sleep time for an individual retry in the client.backoff.logger.template config option), but something that would give more accurate stats would probably be a better idea.\n. The current version seems to omit setting the stats config in the _applyRetryConfig method in DynamoDbClient\n. I normally wouldn't suggest making a change due to a bug in the language itself, but this is now the second time this code has bitten me; both times due to issues in PHP itself:\nThe first was when you do an array_column on a DynamoDB item after unmarshaling it (https://bugs.php.net/bug.php?id=71660)\nThe second was the issue from the example (https://bugs.php.net/bug.php?id=74020)\nI think if I, as an individual developer, had to report two different issues to the PHP team triggered by the exact same code, there might be other landmines hiding somewhere so I figured best to avoid them altogether.. Agreed, working around language bugs in userland never smells quite right, and I debated in my head for a while whether or not to patch this since the first time I got bit by this. I decided in favor of making this PR because:\n1. Switching from a foreach-by-reference to a regular foreach is a trivial change.\n2. I never liked foreach-by-refererence in the first place \ud83d\ude04 \nI understand that those in and of themselves aren't compelling reasons and would be fine if you guys decide against merging this as I can still work around this bug in my own code at least until I upgrade to PHP 7.1 where this bug has a much smaller likelihood of cropping up.. ",
    "gondo": "+1\n. ",
    "ianbytchek": "+1\n. :+1: Any progress with this? I understand this was supposed to fix it, but using either:\nphp\nS3Client::factory([\n    'key'       => 'xxx',\n    'secret'    => 'xxx',\n    'region'    => 'eu-central-1',\n    'params' => [\n        'signature'    => 'v4'\n    ]\n]);\nor this:\nphp\nS3Client::factory([\n    'key'       => 'xxx',\n    'secret'    => 'xxx',\n    'region'    => 'eu-central-1',\n    'signature'    => 'v4'\n]);\ndoesn't fix it\u2026 I've just spent a day on deployment with S3 being the last step. This becomes a big set back\u2026\n. @jeremeamia thanks! Weird. I saw that the signature is not required but not specifying it didn't work either, so I thought it's signature related. I can connect via Transmit but not via SDK. I'm gonna Google more, this is probably something obvious\u2026\n. @jeremeamia I think it's a different issue that I'm dealing with. I've posted it separately here, perhaps you can point me in the right direction.\n. Cool. Finding that mentioned in the docs would certainly have helped my emotions. Thanks @mtdowling!\n. Sounds good, thanks. V3 looks super exciting, I know it's in beta, is it planned for the end of the year or there's still a lot of work?\n. Version 2.7.6. \nphp\n$client = S3Client::factory([\n    'key'            => '\u2026',\n    'secret'         => '\u2026',\n    'region'         => Region::FRANKFURT,\n    'base_url'       => 'https://s3.amazonaws.dev',\n    'command.params' => ['PathStyle' => true],\n    'ssl.certificate_authority' => '\u2026'\n]);\nIf I use no region (SDK uses the default I assume?) or use something like 'myregion', it works fine (note, with PathStyle = true, making me think that Riak CS supports it). So, if I pass the PathStyle to the factory should it get me covered when sending commands or should I be passing it like here https://github.com/aws/aws-sdk-php-laravel/issues/36#issuecomment-49673098 as well?\nAm I correct in thinking that the client for Frankfurt will use a different url, like s3-{REGION}.amazonaws.dev vs. the common {REGION}.s3.amazonaws.dev? I think that would explain the problems.\n. Because production is the default environment, development cascades over it. I was trying to deviate as little from it as possible including not changing the region. Riak CS sits behind the HAProxy locally, I'll try playing with url rewriting, which should do the trick. Alternatively I can just stick to a default region. Thanks!\n. Wrong repo. Very embarrassing. \n. ",
    "jymboche": "I've only done a few tests, but reverted to using standard \"ServerSideEncryption\" instead. In my tests, I just used putObject() with SSECustomerAlgorithm and SSECustomerKey. I could then use a regular getObject() and specify SSECustomerAlgorithm and SSECustomerKey and successfully download the object.\nA couple things I was confused on however, is if I need SSECustomerKeyMD5, and whether or not I needed to base64_encode the SSECustomKey. The generic AWS docs said I did, but it seemed to only work if I just gave the non base64_encoded value.\n. Yupp, my getObjectUrl() looks just like the above code. $secret is the raw key.\nIm using getObjectUrl() for jpegs. That way I can render the private ACL object directly in the browser by setting the img src to the signed url. Otherwise, to my knowledge, I'd have to use php to set the header, and stream the data from getObject(), which would be additional load on the server. Correct?\nIn any case, I've changed my mind and allowing S3 to manage the keys with the regular ServerSideEncryption is just fine for my use-case. I was just testing out the new sse-c feature.\n. ",
    "rayrigam": "@jeremeamia thank you! Following your instructions, I tested this (and it works!):\nphp\n$f = fopen('s3://bucket/key', 'w', false, stream_context_create(array(\n    's3' => array(\n        'CacheControl' => 'max-age=31536000'\n    )\n)));\n// ...fwrite-ing all the bits...\nfclose($f);\nHowever, when trying to pass the Stream Context to the php rename() function, as follows, the Cache-Control Metadata is not added to the S3 object, even though the object is successfully renamed:\nphp\n$f = rename('s3://bucket/key_old', 's3://bucket/key_new', stream_context_create(array(\n    's3' => array(\n        'CacheControl' => 'max-age=31536000'\n    )\n)));\nIs the php rename() function not currently supported for adding the S3 Stream Context?\nAlso, specifying 'ACL' => 'public-read' in the above rename stream context, throws:\n\nPHP Warning:  Access Denied in /app/vendor/aws/aws-sdk-php/src/Aws/S3/StreamWrapper.php on line 735\n. @jeremeamia thanks again! Adding the 'MetadataDirective' => 'REPLACE', to the Stream Context S3 paramaters does indeed solve the problem!\n\nRegarding the other issue (about the ACL parameter), could it be that my IAM user does not have permission to set the ACL parameter of S3 Objects? I currently use an IAM user with permission to: \"s3:DeleteObject\", \"s3:GetObject\", \"s3:PutObject\". Is any other permission required?\n. Thanks! Indeed, an error was thrown when defining the ACL parameter in my Stream Context because my IAM user did not have the necessary permission. To make it all work, I added the following to my IAM user policy:\n\"s3:DeleteObject\", \"s3:DeleteObjectAcl\", \"s3:GetObject\", \"s3:GetObjectAcl\", \"s3:PutObject\", \"s3:PutObjectAcl\"\n. ",
    "migueleliasweb": "After some research, I found out that guzzle does the curl_multi_close() after each batch request, but somehow that does not seem to be enough.\nThe gc_collect_cycles() could be a good idea, I'll try to fork the repo and implement this.\nPs: If the problem is within the curl extension I'll be very sad...\n. PHP 5.5.14\n. @jeremeamia ,I tried using the \"gc_collect_cycles()\" but it end up not helping as much as I would want it to.\nBy using the gc_collect i was able to upload around 8k images before the server runs out of memory or the max open files error blow again.\nI was in a tight schedule with the project i was coding, so i ended up spawning 10 workers and split the work between them, so as they were completing their job (and dying) the file handlers were closing and the server integrity, preserved....\nThe sdk still need this fix (in my opinion)...\n. Ps: curl 7.32.0 (x86_64-redhat-linux-gnu) and PHP 5.5.14\n. Recently I upgraded my curl version to 7.35.0, I'll run a test and upload all files again. (if the error persist, I'll try to upgrade to 7.37.1 then rerun all tests)\nLet's see if this solves the problem =D.\nIf so, we might want to update the docs and signalize the problem with versions under 7.35 so everyone knows how to proceed when this occours.\nNews anytime soon !\n. ",
    "lakshay148": "It happens all the time \n. ",
    "Nyholm": "The point is to get a sense of how well we are doing on the HHVM. As you can see this will not break the build if the HHVM tests fail. \n. I updated the PR. The tests are running now!\n. ",
    "craigbartholomew": "I'm guessing you meant 'directory' and not 'bucket'?\nYes - it still happens:\n$result = self::$client->listObjects(array(\n    'Bucket'  => $params['Bucket'],\n    'Prefix'  => 'bundles/framework/images',\n    'MaxKeys' => 1\n));\nLooks like this:\n```\nGuzzle\\Service\\Resource\\Model Object\n(\n    [structure:protected] => \n    [data:protected] => Array\n        (\n            [Name] => nlcontent-static\n            [Prefix] => bundles/framework/images\n            [Marker] => \n            [MaxKeys] => 1\n            [IsTruncated] => \n            [Contents] => Array\n                (\n                    [0] => Array\n                        (\n                            [Key] => bundles/framework/images/\n                            [LastModified] => 2014-07-24T16:30:58.000Z\n                            [ETag] => \"d41d8cd98f00b204e9800998ecf8427e\"\n                            [Size] => 0\n                            [Owner] => Array\n                                (\n                                    [ID] => 9d8fac9e83f71a47530b5631b20eee2b2c75dcfad9b424e840f97b1794c2e411\n                                    [DisplayName] => me\n                                )\n                        [StorageClass] => STANDARD\n                    )\n\n            )\n\n        [RequestId] => D314B312F88CDB78\n    )\n\n)\n```\nHere's the relevant part of the trace:\nPHP Warning:  Pseudo directory is not empty in /home/craig/www/website/vendor/aws/aws-sdk-php/src/Aws/S3/StreamWrapper.php on line 747\n...\nPHP  10. rmdir() /home/craig/www/website/vendor/symfony/symfony/src/Symfony/Component/Filesystem/Filesystem.php:144\nPHP  11. Aws\\S3\\StreamWrapper->rmdir() /home/craig/www/website/vendor/symfony/symfony/src/Symfony/Component/Filesystem/Filesystem.php:144\nPHP  12. Aws\\S3\\StreamWrapper->triggerError() /home/craig/www/website/vendor/aws/aws-sdk-php/src/Aws/S3/StreamWrapper.php:422\nPHP  13. trigger_error() /home/craig/www/website/vendor/aws/aws-sdk-php/src/Aws/S3/StreamWrapper.php:747\n. ",
    "djlosch": "Sorry for the slow reply @jeremeamia.\nThe curl options definitely help.  I can't believe I missed that in the docs as that literally solves half the problem.  \nThe second half of this problem is a function that manages the repeated attempts.  The only reason I suggested pushing it back upstream is that it's mimicking the TTL design pattern in networking.  The potential list of use cases in which you wouldn't want to do this are so incredibly small -- we now use it for literally every project that uses SES.\n. Error message is just the normal curl timeout message.  Then the SDK does not retry at all (or if it does, it's completely silent).  Any idea where the code/settings are for the auto-retry?  I'd like to check it out as that clearly didn't work and multiple people have reported the same problems on the AWS support forums.\nMy function is just a simple wrapper that attempts to connect up to a variable number of times, and if it connects, returns the response.  Otherwise, continues retrying until it reaches the max attempts count, and then throws if it still has failed.  I'll throw up some code soon so you can see an example.\n. ",
    "mrJakez": "HI @jeremeamia,\nthanks for your reply. As far as I can see the key attributes (user1 & user2) are set correctly. I've attached a screenshot of the corresponding index configuration.\nMaybe I've to specify the index name (user1-index and user2-index)?\n\nCheers\nDennis\n. ",
    "KenLFG": "Nevermind, was turned on to Wire Logging which showed there was a response but apparently I can't json_encode it by default. However, I can json_encode a subelement. Go figure. I'm still having an issue but it appears to be related to a scan filter instead. Will repost if it turns out to be a real issue related to the SDK. Sorry for the wasted post! :)\n. ",
    "DouglasMedeiros": "Hi @jeremeamia !\nThanks for help!\n1. getDomainClient()\n2. Trying \n$domainClient = $configClient->getDomainClient('domain-1', ['key' => KEY, 'secret' => SECRET]);\n1. Try use \"dev-master\"\nThe error persists. :(\n. @jeremeamia \nTest 1\nphp\n $csConfigClient = CloudSearchClient::factory(['key' => 'key', 'secret' => 'secret', 'region' => 'sa-east-1', 'base_url' => 'SearchEndpoint']);\nReturn:\n```\n[curl] 3:  malformed [url] /\nException: TransferException\nFile: /var/app/current/vendor/aws/aws-sdk-php/src/Aws/Common/Client/AbstractClient.php\nLine: 256 - $wrapped = new TransferException($e->getMessage(), null, $e);\n```\nTest 2 - using http:// or https:// + SearchEndpoint\nReturn:\nException: CloudSearchException\nFile: /vendor/aws/aws-sdk-php/src/Aws/Common/Exception/NamespaceExceptionFactory.php\nLine: 91 - $class = new $className($parts['message']);\n. @jeremeamia \nNot send base_url, get error:\nYou must provide the endpoint for the CloudSearch domain.\n. Hi @jeremeamia !\nProblem solved! \nVery grateful for your help and patience!\n. ",
    "samsamm777": "@jeremeamia I cant find any docs on waiters for cloudformation?\n. @radford So I found out this is all implemented in the SDK version 3. Its NOT documented anywhere though.\nphp\n$this->cloudFormationClient->waitUntil(\"StackCreateComplete\", [\n    \"StackId\" => $myStackId\n]);\n. ",
    "jeskew": "It's possible they were added in a different repository. The JSON waiter definitions are shared among the AWS SDKs. v2 uses an artifact built from the shared files, whereas v3 uses them more or less directly, so there's a lot of functionality that v3 gets for free.\n. @marcelodornelas I see that you're specifying a region. Is this request creating a latency resource record set? \nA SetIdentifier is used to distinguish weighted, latency, failover, and geolocation resource record sets. As mentioned above, some parameters are conditionally required depending on what kind of record set you're creating.\n. @oberman Can you provide any guidance on how to reproduce the issue?\n. @oberman Could you try with this version of the phar? I believe it should fix the issue you're facing.\n. @thomas83 does it only happen once every few weeks in production?\n. I added a call to Phar::mapPhar('aws.phar') in the stub, which supposedly reads the file's manifest into memory and binds the contents of the phar to phar://aws.phar. This seems to be done implicitly when loading a phar from the filesystem but not when a phar is loaded from opcache.\n. That's a good point. I updated the PR to also set a versioned alias. You wouldn't actually be able to load two different versions in the same process -- the files containing function definitions for the SDK, Guzzle, and JMESPath would conflict with each other -- but that should take care of any issues arising from a shared opcache.\nBtw, could you tell me about about the environment in which you ran into this issue? We've been consistently unable to reproduce this issue on freshly provisioned Ec2 instances running either Apache or Nginx w/ php-fpm.\n. @fredden Done. That will go out in the next v2 release.\n. @halkyon I was able to reproduce the error you saw and fixed it with #980. The fix will be included in the next 2.8.x release.\n. This looks great, @EronHennessey. Could you change this PR to target the 2.8 branch instead of master?\n. Reopened as #705 to target 2.8.\n. This was addressed in v3 in #813. Please feel free to reopen if you have any questions or concerns.\n. @billwhitney An exception should only be thrown if you have an error handler that converts errors to exceptions, as PHPUnit's error handler does. All the StreamWrapper is doing is calling trigger_error.\nI was able to reproduce the issue with @file_get_contents returning an empty string instead of false and believe #707 should fix it.\n. The JSON compiler has been moved to a build step, so you shouldn't be affected by the issue this was causing anymore. Please feel free to reopen if you have any question.\n. @UlrichEckhardt do you still have a script that tests this behavior? I believe it has been addressed since this ticket was opened.\n. Closing, as all the test cases listed in the original ticket now respond correctly to is_dir.\n. Hi datibbaw -- the documentation you've linked to covers version 2 of the SDK, which does offer a setRegion method. The SDK's master branch is version 3, which doesn't support changing the region on an already instantiated client. You would need to create a new client and pass the desired region in as a constructor argument.\nI'm going to go ahead and close the issue, but please feel to reopen it if you have any questions.\n. @melusom The validator classes included in the phar and zip distributions, which are what we use to generate the API docs. PSR-7 interfaces and Guzzle classes are documented as well, though it is more clear in those cases that the code isn't part of the SDK.\nWould it have helped if the aws/aws-php-sns-message-validator were suggested by composer?\n. Ditto :+1: \n. Thanks for bringing this to our attention. We are working on a response.\n. PR #638 has been merged into master and should fix this.\nI'm going to go ahead and close the issue, but please feel to reopen it if you have any questions.\n. As currently written, this will not handle all characters. Below the change you made, there is a separate call to parse_str, which urldecodes query keys and converts a few characters (like spaces and periods) to underscores.\n. @Gator92 The test case I tried to add was something along the lines of https://example.com/path.ext?query parameter=query value. The query did not survive the call to parse_str, even when properly encoded.\n. This is waiting on https://github.com/guzzle/guzzle/pull/1261 to get finalized and merged into a release.\n. The endpoints,json file is part of the repository, so it should always be present. Does the exception occur at a certain time every hour, and is there a cron job running at the same interval that would somehow interfere with file access?\n. Jenkins might be using up a significant percentage of your open files limit. Try increasing your system's limit on file descriptors.\n. The tests look nice, too. :shipit: \n. Those are actually the docs for v2 of the SDK (please excuse the \"latest\" in the URL; we're working on that). The v3 documentation can be found here.\n. As we discussed in the Gitter chat, highlight takes a JSON string with the fields mentioned in the CloudSearch developer guide.\nI'm going to go ahead and close the issue, but please feel to reopen it if you have any questions.\n. Nothing. I'll go ahead and merge it in.\n. Thanks for bringing this to our attention. We are working on a response.\n. PR #638 has been merged into master and should fix this.\nI'm going to go ahead and close the issue, but please feel to reopen it if you have any questions.\n. @allen-infante Could you try updating to a later release of the SDK? The issue you're describing was addressed in #638.\n. :+1: This is great. I really wasn't looking forward to maintaining a separate alias => namespace map in the behat suite.\n. It looks like the connection to SQS is happening during Laravel's queue:work command. Did you mean to use a different queue driver?\n. This error described is not a client-side check, but an authentication error thrown by SQS. Credentials are scoped to a calendar day, so it is not possible to disable including date information in the signing process.\n. Thanks for reporting this, @janvennemann . Looks like XMLWriter won't add attributes to a node once any children have been written, so I added a sorting step to make sure attributes precede child nodes in the data passed to the writer. \nThis fix has been merged into master and will go out with our next release.\n. Hi Bruce,\nHow large is the object you're trying to put, and are there any proxy servers you might be hitting (e.g., Squid boxes)?\n. Sorry for the delay on addressing this, but v3.2.6 contains a fix that should eliminate this error. Please feel free to reopen if it comes up again.\n. The latest version of CloudFront available to the SDK looks like 2014-11-06. Are you trying to use features released on the 17th?\n. Addresses #643 \n. Hi Hakadel,\nThat error can occur when the SDK is unable to locate credentials in the usual places and tries to authenticate using an instance profile, which can only be done on EC2.\nAre you using environment variables or a credentials file to authenticate?\n. Hi @ragboyjr,\n~~Do you have stack trace or something that could help us debug your issue? Aws\\Common appears to be present in the latest release.~~\n2.8.11 has been deleted and superseded by 2.8.12\n. Hi @BardiaAfshin,\nThanks for reporting this. Non-string members of lists were not being properly serialized in JSON payloads, and this was addressed in #658. \nBy the way, it looks like you were trying to send the basic formatting example from our documentation, which is just meant to show what format input should take and is not likely to work as is. The second error you encountered is related to this -- the request you sent tried to add an entry to a stream named string, which does not exist on your account. You can find more information on how to get started with Kinesis on the AWS documentation site.\n. Closing, as the relevant pull request has been merged into master. Please feel free to reopen if you have any questions.\n. Hi Bardia,\nThe fix has not yet been included in a stable release. If you'd like to try it out before the next release, you can temporarily change your composer file to require dev-master instead of a stable version.\n. Hi @ArthurGuy,\nThere's a preview release of a SimpleDB client for v3 of the SDK here. If you're still looking for this functionality, would you mind taking a look and letting me know if it meets your needs?\n. Are you using DynamoDb local? If not, do you know how large of a response from DDB you were expecting?\n. Thanks for reporting this. We're working on a fix.\n. The fix has been merged into master and will go out with the next release. Please let me know if the issue persists or if you have any questions.\n. @acoulton's fix has been merged into master. Thanks for reporting this, @ethet!\n. :+1:\n. LGTM.\n. :shipit: \n. Hi @ravenbyron,\nI'm having a hard time reproducing that error. Could you attach a debug log to your lambda client and share the output?\n. So the line that was throwing the error is trying to call a method with a signature of ($object, &$scalar) with what it assumes to be an array member. Pass-by-reference is allowed for array members but not for substrings, which is what happens when $value[$key] gets called on a string instead of an array.\n. Hi @BardiaAfshin,\nThe KPL is currently only recommended for use through the java wrapper library. Their platform recommendations assume your producer code is written in Java.\nWould you be interested in seeing something similar for PHP?\n. I've added a note to our internal backlog. Please feel free to reopen if you have any further questions.\n. Given a large dataset with deeply recursive structures, this change decreased the memory usage of Aws\\Api\\StructureShape::getMembers from 32MB to 23KB on a paginated scan. This also decreased the amount of memory allocated by Aws\\Api\\ShapeMap::resolve from 15.6MB to 50KB. The full blackfire profile can be viewed here.\nResolving the shape members as it's currently done involves allocating a new array and populating it via ShapeMap lookups on every call to getMembers. Blackfire showed that function allocated around 378MB of memory over the course of a DDB query scan. Memory usage by this method never exceed 32MB at any given time, so those arrays are clearly being taken care of by GC... eventually. This change also dropped total GC time by 100ms. \nCopy-on-write semantics for pass-by-value arrays means that it's safe to return the same array on every call to getMembers while still only keeping one master copy instead of generating hundreds of thousands for the GC to handle.\n. @teseo Can you post a bit more detail? This ticket is still open because we have been unable to reproduce it.\n. @teseo please see my comment on issue #674.\n. @mwoodring Got it. The multipart uploader will create a fresh file handle when you give it a path, but it won't automatically rewind an open handle passed to its constructor. I'll update the docs to show how to use this technique with a stream instead of a path.\n. I've updated the documentation to warn about streams not being rewound automatically. Please feel free to reopen if you have any further questions.\n. It's not currently configurable. Are you running into an issue?\n. Just to clarify, are all fields being returned as arrays (even ones that are supposed to contain scalar values), or is the issue that both lists and maps are represented as arrays in the response?\n. That definitely makes sense. Is your CloudSearch domain protected by any kind of authentication? If not, the easiest solution for you might be to use an HTTP client outside of the context of the SDK.\n. Unfortunately there is not. Cloudsearch's json format isn't something that any of the SDKs can understand natively.\n. Let me do some investigation on this -- the json files are auto-generated, so any changes would get wiped out in an update. I'll need to check to find out why pretty=true is being added to those endpoints.\n. I still don't have an answer for you about why pretty=true is hardcoded into the cloudsearch endpoints, but it's a lot easier to sign an arbitrary request in v3 than it was in v2. You can use the SDK to manage your domain and then do something like the following to execute a search:\n``` php\nuse Aws\\Credentials\\CredentialProvider;\nuse Aws\\Signature\\SignatureV4;\nuse GuzzleHttp\\Client;\nuse GuzzleHttp\\Psr7\\Request;\n// Prepare a CloudSearchDomain request\n$request = new Request('GET', 'https://..cloudsearch.amazonaws.com/2013-01-01/search?q=star+wars&return=title');\n// Get your credentials from the environment\n$credentials = call_user_func(CredentialProvider::defaultProvider())->wait();\n// Construct a request signer\n$signer = new SignatureV4('cloudsearch', '');\n// Sign the request\n$request = $signer->signRequest($request, $credentials);\n// Send the request\n$response = (new Client)->send($request);\n$results = json_decode($response->getBody());\nif ($results->hits->found > 0) {\n    echo $results->hits->hit[0]->fields->title . \"\\n\";\n}\n``\n. I've added addressing the hardcodedpretty=truequery parameter to our team's internal backlog. Please feel free to reopen this if you have any further questions.\n. By default, theInstanceProfileProvideris only used if no credentials are detected in theAWS_ACCESS_KEY_IDandAWS_SECRET_ACCESS_KEYenvironment variables or in an ini file located at~/.aws/credentials`. Are you sure there's nothing in that file or in those environment variables?\nThe Node SDK uses the same environment variables and credentials filepath, but node and PHP might be reading different values if there are any putenv calls in your PHP code or if there's a .aws/credentials file in the home directory of the apache or fpm user (node would look in the home directory of whatever user launched the node process).\n. You could check what key is being used by the SDK and comparing this to what is assigned to your EC2 instance.\nFor example, you could extract the key being used by the SDK by placing the following in a file at your project root and running it normally:\n``` php\n<?php\nrequire 'vendor/autoload.php';\n$key = null;\n$s3 = new \\Aws\\S3\\S3Client([\n    'version' => 'latest',\n    'region' => 'ap-southeast-2',\n    'debug' => [\n        'scrub_auth' => false,\n        'logfn' => function ($message) use (&$key) {\n            $pattern = '@Authorization: AWS4-HMAC-SHA256 Credential=([^/]+)/@';\n            $matches = [];\n            if (preg_match($pattern, $message, $matches)) {\n                $key = $matches[1];\n            }\n        }\n    ],\n]);\n$s3->listBuckets();\nvar_dump($key);\n```\nTo see the key associated with your instance, you could execute this commandL\ncurl http://169.254.169.254/latest/meta-data/iam/security-credentials/<role name>\nTo get the role name, you can run the following:\ncurl http://169.254.169.254/latest/meta-data/iam/security-credentials/\n. The 403 must be getting converted to an exception before the log function is called. Try this script instead:\n``` php\n<?php\nrequire 'vendor/autoload.php';\nuse Aws\\Credentials\\CredentialProvider;\nuse Aws\\Credentials\\CredentialsInterface;\ncall_user_func(CredentialProvider::env())\n    ->then(function (CredentialsInterface $creds) {\n        echo \"Found credentials on the environment with key {$creds->getAccessKeyId()}\\n\";\n    })\n    ->otherwise(function () {\n        call_user_func(CredentialProvider::ini())\n            ->then(function (CredentialsInterface $creds) {\n                echo \"Found credentials on the filesystem with key {$creds->getAccessKeyId()}\\n\";\n            })\n            ->otherwise(function () {\n                call_user_func(CredentialProvider::instanceProfile())\n                    ->then(function (CredentialsInterface $creds) {\n                        echo \"Found credentials on the instance with key {$creds->getAccessKeyId()}\\n\";\n                    })\n                    ->otherwise(function () { echo \"No credentials found.\\n\"; });\n            });\n    });\n$s3 = new \\Aws\\S3\\S3Client([\n    'version' => 'latest',\n    'region' => 'us-west-2',\n]);\ntry {\n    $s3->listBuckets();\n} catch (\\Aws\\Exception\\AwsException $e) {\n    $matches = [];\n    if (preg_match(\n        '@AWS4-HMAC-SHA256 Credential=([^/]+)/@',\n        $e->getRequest()->getHeaderLine('Authorization'),\n        $matches\n    )) {\n        echo \"Request sent with key: {$matches[1]}\\n\";\n    }\n}\n``\n. Does it always say \"Found credentials on the instance\"?\n. Closing, as we were never able to reproduce. Please feel free to reopen if you have any further questions.\n. Thanks for tracking down the root cause, @PSiAU. We were all stumped by this one.\n. The SDK should be callingbase64_encodefor you, so you might be sending doubly encoded data in your first example. Did you try'ZipFile' => file_get_contents($target_path)`?\n. @teseo can you post the full exception message?\n. @teseo It looks like one of the parts did not upload successfully within the configured retry limits on your client. You can keep track of upload state and retry the unsuccessful uploads:\n``` php\n$source = '/path/to/large/file.zip';\n$uploader = new MultipartUploader($s3Client, $source, [\n    'bucket' => 'your-bucket',\n    'key'    => 'my-file.zip',\n]);\ndo {\n    try {\n        $result = $uploader->upload();\n    } catch (MultipartUploadException $e) {\n        $uploader = new MultipartUploader($s3Client, $source, [\n            'state' => $e->getState(),\n        ]);\n    }\n} while (!isset($result));\n```\nIt sounds like you're using a multipart upload for very small files, which would make your code more complex than it needs to be. If you're not sure whether an object is large enough to warrant a multipart upload, you could try using the upload method on Aws\\S3\\S3Client, which will make the decision for you.\n. @teseo The multipart uploader will accept a path for backwards-compatibility reasons, but there was no such BC concern on S3Client's upload method.\n. I believe this was fixed in 2b43a459549012d4859c45537edc5b02fa7b4b68. @Beejer, are you still experiencing this issue?\n. @teseo You should call fopen on the location and pass the handle to S3Client->upload().\n. Ah I didn't realize you were still on v2.\n. After doing some more research, I was unable to reproduce the error exactly as described, but I did observe a multipart upload entering a \"hanging\" state wherein a curl_multi_select call would never resolve. (Something similar is described in this SO post.) I was unable to force the v3 multipart uploader to trigger a similar error after extensive testing, and the RejectionException reported above appears to have been addressed by https://github.com/aws/aws-sdk-php/commit/2b43a459549012d4859c45537edc5b02fa7b4b68.\nregister_shutdown_function being bypassed is a strong indicator that the issue is not caused by userland code like the SDK but rather by curl_multi. @Beejer if you're still experiencing this error, is there a chance that the PHP process is hanging and being killed by an external process?\n. Closing, as there hasn't been any recent activity and all signs point to the initial diagnosis (something going on with curl_multi) being correct. The \"hanging\" described above only happened after I downgraded to PHP 5.4 and went away when I reinstalled a current version of PHP, so that could be tried if the issue pops up again.\nPlease feel free to reopen if you have any questions or concerns.\n. Loading the file containing that function is handled by composer, not the SDK. In what context are you seeing this issue?\n. Is this by any chance happening during a composer-triggered script? They have a few outstanding issues related to the fact that files are not automatically loaded during composer scripts.\n. Maybe running composer self-update would take care of this?\n. You might also want to raise an issue with composer. v2 does not use Composer's files autoloader, you wouldn't run into this issue if you went that route.\n. You will need to replace your custom autoloader with the standard composer autoloader (i.e., vendor/autoload.php). Only using the classmap is not compatible with the AWS SDK (or the Guzzle libraries it depends on). The composer autoloader will still use the optimized classmap generated by composer update -o.\n. Sorry to take so long to get back to you on this. Part of PHP's native SessionHandlerInterface is the stipulation that only strings will be passed to the write method. The means for creating those strings is controlled by the session.serialize_handler configuration value, but no serialize handler should be returning binary.\nIf you were worried about the performance implications of needing to base64 encode your data, then this change wouldn't make a huge difference, as binary fields are base64 encoded by the SDK before being sent to DynamoDb in any case.\n. The history for this change is pretty noisy, so I'll squash the commits once everything looks ok.\n. @acoulton I added a test and merged this in as #703. Thanks for putting this together!\n. This is a bit out of our wheelhouse, but I have a couple questions:\n1. Do you see the same issue if you disable phalcon?\n2. Can you trigger a segfault by attempting to send any request over SSL, or just to AWS endpoints?\n. This has grown too big for a single PR, so I'm splitting it into #714 and a follow-up.\n. Hi @pavelsc,\nWhat version of the SDK are you using?\n. I'm having trouble reproducing this error. Could you construct your client with 'debug' => true to capture the request that's being sent to DynamoDb?\nThis is how I'm trying to recreate the error, but the query command always executes successfully:\n``` php\n<?php\nrequire DIR . '/../vendor/autoload.php';\n$ddb = new \\Aws\\DynamoDb\\DynamoDbClient([\n    'version' => 'latest',\n    'region' => 'us-east-1',\n]);\n$ddb->createTable([\n    'TableName' => 'cdr',\n    'AttributeDefinitions' => [\n        [\n            'AttributeName' => 'customerId',\n            'AttributeType' => 'N',\n        ],\n        [\n            'AttributeName' => 'uniqueId',\n            'AttributeType' => 'S',\n        ],\n        [\n            'AttributeName' => 'callDate',\n            'AttributeType' => 'N',\n        ],\n    ],\n    'KeySchema' => [\n        [\n            'AttributeName' => 'customerId',\n            'KeyType' => 'HASH',\n        ],\n        [\n            'AttributeName' => 'uniqueId',\n            'KeyType' => 'RANGE',\n        ],\n    ],\n    'LocalSecondaryIndexes' => [\n        [\n            'IndexName' => 'callDateIndex',\n            'KeySchema' => [\n                [\n                    'AttributeName' => 'customerId',\n                    'KeyType' => 'HASH',\n                ],\n                [\n                    'AttributeName' => 'callDate',\n                    'KeyType' => 'RANGE',\n                ],\n            ],\n            'Projection' => [\n                'ProjectionType' => 'KEYS_ONLY',\n            ],\n        ],\n    ],\n    'ProvisionedThroughput' => [\n        'ReadCapacityUnits' => 1,\n        'WriteCapacityUnits' => 1,\n    ],\n]);\n$ddb->waitUntil('TableExists', [\n    'TableName' => 'cdr',\n]);\n$result = $ddb->query([\n    'ExpressionAttributeValues' => array(\n        ':customer_id' => array('N' => '1'),\n        ':start_date' => array('N' => (string) strtotime('yesterday')),\n        ':end_date' => array('N' => (string) time())\n    ),\n    'IndexName' => 'callDateIndex',\n    'KeyConditionExpression' => 'customerId = :customer_id AND callDate BETWEEN :start_date AND :end_date',\n    'TableName' => 'cdr',\n]);\n$ddb->deleteTable([\n    'TableName' => 'cdr',\n]);\n$ddb->waitUntil('TableNotExists', ['TableName' => 'cdr']);\n``\n. @pavelsc I tried that as well and got the same result. Hardcoded credentials and credentials gleaned from the environment both turn into an instance ofAws\\Credentials\\Credentials` before a request is sent, so that shouldn't change the behavior of the request serializer.\nIf you use environmental credentials, does the issue disappear?\n. @pavelsc Were you able to test with environmental credentials?\n. Closing as unable to reproduce. Please feel free to reopen if you run into the issue again or if you have any further questions.\n. A likely fix was merged into master. Please reopen if you encounter this error again.\n. 2.8 and master are both actively developed, the first housing version 2 of the AWS SDK for PHP and the second housing version 3. There are no plans in the short or long term to merge the two.\n. I think @dstevenson's suggestion is the way to go: you can instantiate an S3Client to achieve the behavior you want like so:\nphp\n$s3 = new \\Aws\\S3\\S3Client([\n    'version' => <version>,\n    'region' => <region>,\n    'endpoint' => '<bucketName>.s3.amazonaws.com',\n    'bucket_endpoint' => true,\n]);\nThis pattern also supports CNAME'd bucket endpoints. \n. @dstevenson endpoint is a conditionally required parameter there: if you pass in 'bucket_endpoint' => true when creating an S3 client, you need to also pass in an endpoint.\nThe use case we're supporting with this is CNAME'd bucket endpoints, not virtual hosted bucket endpoints. We do not explicitly support virtual hosted bucket endpoints, though you can use them if you pass the virtual hosted bucket URL in as if it were a vanity endpoint and your bucket name meets the requirements specified by S3. It's not something that would work with every bucket.\nUsing virtual hosted bucket URLs can affect application performance in two ways: 1) you need to do a DNS lookup on every single bucket, and 2) requests are routed by default to us-east-1. Since the SDK requires that you specify a region anyway, as SigV4 signing keys include the region name, you're better off speaking directly to the appropriate S3 region instead of bouncing off us-east-1. This is exactly the kind of low-level detail the SDK is supposed to abstract away.\n. Closing as working as intended. Please feel free to reopen if you have any further questions.\n. A fix has been merged to master. Thanks for reporting and testing out the change!\n. Addresses (among other things) #687 \n. This does not appear to be a bug in the SDK. Filtering seems to happen after snapshots have been chunked into pages, so you can expect a finite number of zero-result pages if you use a filter that returns no results.\nPerhaps if you try increasing the MaxResults value to something larger you should be able to verify that pagination does not continue infinitely.\n. I opened up a pull request on Guzzle to have the StreamHandler attach a content-length header of 0 automatically, as that's what cURL would do. In the meantime, you could use a piece of Guzzle middleware to calculate the content length of your requests for you:\n``` php\nuse Aws\\S3\\S3Client;\nuse Aws\\Handler\\GuzzleV6\\GuzzleHandler;\nuse GuzzleHttp\\Client;\nuse GuzzleHttp\\HandlerStack;\nuse GuzzleHttp\\Handler\\StreamHandler;\n$stack = HandlerStack::create(new StreamHandler());\n$stack->push(function (callable $handler) {\n    return function (\n        \\Psr\\Http\\Message\\RequestInterface $request,\n        array $options\n    ) use ($handler) {\n        if (0 === $request->getBody()->getSize()) {\n            $request = $request\n                ->withHeader('Content-Length', 0);\n        }\n    return $handler($request, $options);\n};\n\n});\n$handler = new GuzzleHandler(new Client(['handler' => $stack]));\n$client = new S3Client([\n    'http_handler' => $handler,\n    'version' => 'latest',\n    'region' => 'us-west-2',\n]);\n$client->registerStreamWrapper();\nmkdir('s3://mybucket/newdir');\n``\n. Guzzle's new release (6.1.0) has a bundled fix. Please feel free to reopen if you have any questions or concerns.\n. I don't have a copy of that release and am unable to reproduce. The closest I could find is [2.1.2](https://github.com/aws/aws-sdk-php/tree/2.1.2), which came out in February of 2013. Is there any chance you could upgrade? Our latest v2 release is [2.8.14](https://github.com/aws/aws-sdk-php/tree/2.8.14), and v3 (currently at [3.2.0](https://github.com/aws/aws-sdk-php/tree/3.2.0/)) came out a couple months ago.\n. Glad upgrading helped. Please feel free to reopen if you have any further questions.\n. This looks related to #664. What return value are you sending back in your lambda call tocontext.succeed, and would it be possible to emit an object instead of a string?\n. I don't believe it's mentioned anywhere in the PHP SDK documentation, but [Lambda passes all return values throughJSON.stringify](http://docs.aws.amazon.com/lambda/latest/dg/programming-model.html#programming-model-context-object-succeed), so you were actually ending up with double-encoded values (on which the SDK was only callingjson_decode` once).\n. This looks awesome! :shipit: \n. @skyzyx You would need an examples file that could be loaded by this line, and none of the services have one yet.\n. This was merged in as part of #837.\n. Hi @stefandoorn,\nThe root cause of the error you encountered is that an instance profile credentials object can't be effectively woken up from cache right now if the credentials have expired. There's an open pull request to fix this.\n. A fix for this was included in today's release. Please feel free to reopen if you have any further questions.\n. @stefandoorn Glad to hear clearing the cache worked. \nUnfortunately, I'm not sure we can reliably clear caches on upgrading, as Composer will only execute post-update scripts registered on a project's root, not on that project's dependencies. In v3, we use the version number as part of the cache key, and that might be something we could look into backporting.\n. The service doesn't implement a supported protocol, so a model would be of limited value. (Also, we would have to handwrite the model.)\n. Without a model, this would just be an instance of GuzzleHttp\\Client with the appropriate base url set. Since this URL is available as a constant on Aws\\Credentials\\InstanceProfileProvider, this client would be little more than:\n``` php\nclass InstanceMetadataClient\n{\n    private $client;\npublic function __construct()\n{\n    $this->client = new \\GuzzleHttp\\Client([\n        'base_uri' => \\Aws\\Aws\\Credentials\\InstanceProfileProvider::SERVER_URI\n    ]);\n}\n\npublic function get($path)\n{\n    return $this->client->get($path);\n}\n\n}\n```\nI'm not sure how much value that adds. Closing for now, but it can be revisited if demand exists.\n. I updated the multipart uploader documentation to show what you can do with before_{operation} callbacks and altered the constructor to take case-insensitive options in its config array.\n. Closing in favor of #724\n. Thanks for reporting; the attached PR will go out in the next release.\n. I think you should leave the array typehint on $args and add a test case that makes sure the class is mockable.\n. Cool; I didn't realize the mocking issue only affected mockery. Could you squash the commits?\n. Thanks!\n. @r3wt Can you share the code that isn't working? If I execute the following:\nphp\n$firstVersion = $s3->putObject([\n    'Bucket' => <my-bucket>,\n    'Key' => 'response.xml',\n    'Body' => fopen(__DIR__ . '/response.xml', 'r'),\n    'ContentType' => 'application/xml',\n]);\nthen responses to a GetObject operation come back with a header of Content-Type: application/xml. \n. The content type should be provided to putObject as ContentType, not Content-Type. Does making that change fix the issue?\n. To change the metadata on an object in S3, you'll need to copy the object over itself and pass in a MetadataDirective of REPLACE. This script would replace the content type on a single object:\n``` php\nuse PHPUnit_Framework_Assert as Assert;\n$s3 = new \\Aws\\S3\\S3Client([\n    'region' => '',\n    'version' => 'latest',\n]);\n$bucket = '';\n$key = '';\n$s3->copyObject([\n    'Bucket' => $bucket,\n    'Key' => $key,\n    'CopySource' => \"$bucket/$key\",\n    'ContentType' => 'image/png',\n    'MetadataDirective' => 'REPLACE',\n]);\n$metadata = $s3->headObject([\n    'Bucket' => $bucket,\n    'Key' => $key,\n]);\nAssert::assertEquals('image/png', $metadata['ContentType']);\n```\n. Hi,\nThis project provides support for creating and managing Redis and Memcached instances through Amazon ElastiCache but does not ship with any clients for interacting with them. CacheProvider looks like it's from the doctrine/cache package, which comes with prebuilt implementations of Redis- and Memcached-based cache clients, both of which are flushable. You can read more in their documentation.\n. The autoloader file will only appear in the aws.zip file, not the download labelled Source Code (Zip). Aws.zip also contains the SDK's resolved dependencies so that you don't have to rely on composer to pull those down.\n. You'll need to specify a protocol on the endpoint (either http or https).\n. @jeremeamia That's a good point. I opened a PR to do so.\n. @mtdowling I would prefer not to trigger a call to stat if it can be avoided since we're counting on bypassing disk I/O and pulling these files from the PHP opcache. opcache_is_script_cached almost does what I would want, but it's only available on >=5.6, so the check would end up fairly convoluted:\nphp\nif ((function_exists('opcache_is_script_cached') && opcache_is_script_cached($path)) \n    || file_exists($path)\n) {\n    return include($path);\n}\nIt seems cleaner to just let include delegate to the opcache and filesystem as needed. Maybe I should remove the silencer and let the function trigger a warning before moving on to searching out the JSON file?\n. If the response is null, that would generally mean that an error was raised by the HTTP handler. I added a separate check to make sure that exceptions without responses are retried within the waiter's maxAttempts limit.\n. This looks like a documentation issue, as prependBuild means \"do this before building a request object.\" Does changing that to $command->getHandlerList()->appendBuild( get rid of the error?\n. Are you using an EC2 instance?\n. Do you mean S3?\nMake sure you're providing credentials to your application. This can be done in several ways, such as by setting environment variables, placing them in an ini file, and passing them in directly when constructing a client. When no credentials are provided in the client constructor, the SDK will first look for credentials in the AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY and then for an ini file at ~/.aws/credentials. If no credentials have been found by that point, the SDK will attempt to contact the EC2 instance metadata service, which is the operation you see failing.\n. How are you supplying credentials to the SDK?\n. So this code is running on an Ec2 instance?\n. The fact that the instance metadata service is unreachable indicates that your code is not running on an Ec2 instance. This means that you will need to find a way to provide credentials to the instance of S3Client, either by passing them in directly or by setting up an ini file or environment variables as described here.\nIt's possible that everything was working before because the appropriate environment variables were being set by Apache or Nginx or because there was an ini file at ~/.aws/credentials that has since been removed.\n. There are a few services that have global endpoints and have to be signed with the us-east-1 region, including Route53, CloudFront, IAM, and STS. Let me make sure I have a complete listing, and I'll add this information to the documentation.\n. I added this to the SDK guide in #739. Please feel free to reopen if you have any further questions.\n. @stefandoorn This PR does not use the version number as part of the key, as the mechanism proposed is simpler than what is in v2. In v2, the object cached included an InstanceMetadataClient; in this case, all the cache will store is your key, secret, token, and expiration.\nI was referring in #710 to the cache used by Aws\\JsonCompiler, which has since been removed.\n. The only thing it was waiting on was for me to clear up a merge conflict, which is now done. I'm merging into master, so it will go out with the next release.\n. Looks like you figured this one out. Please feel free to reopen if you have any further questions.\n. It doesn't look like this an issue with the SDK, as the crash log shows libdispatch segfaulting. Can you cause php-fpm to crash by curling https://aws.amazon.com?\n. There's a Aws\\S3\\BatchDelete class that will clear a bucket with a static method call. Try calling BatchDelete::fromListObjects($s3Client, ['Bucket' => $bucketName])->delete();\n. Thanks for reporting this. \nWhat appears to be happening is that the key is used to construct the URL, which for a GetObject command looks like https://<endpoint>/<bucket>/<key>. The only check performed on keys is that they must be strings (there's no check to make sure the key isn't an empty string), so you're ending up with a signed URL for https://<endpoint>/<bucket>/. This is a distinguished from a ListObjects command by a trailing slash, and S3 does not appear to care about trailing slashes.\nI don't believe this issue can be quickly fixed in the SDK, so if you're running into it regularly, then you might want to stick an if (empty($file_data->ref)) check above call to getCommand in the code you posted.\n. I think this should still use the search method so that you could pass in something more complex than a key. The issue is that the command created by getCommand was not being executed.\n. @radford Sorry, I thought I was waiting on your update. I can work on this.\n. $value should be a handle on php://temp opened in Aws\\TraceMiddleware. Is there any chance the commands are being serialized and executed later?\n. @onethumb A standalone test case would be great if you could set one up.\n. @onethumb Any chance you could provide a test case?\n. @onethumb I was taking a look at old issues today and think I found what's going on with this one: I believe that the script generating the error is closing the resource at STDOUT. Guzzle's debug_resource function will use STDOUT if it is available but assumes it will not be closed, as the CLI SAPI is supposed to open STDOUT at the start of a script and close it at the end. The general error can reproduced by running the following:\n``` php\n<?php\ndefined('STDOUT') || define('STDOUT', fopen('php://stdout', 'w'));\ndefine('STDOUT2', fopen('php://stdout', 'w'));\nfunction checkStdOut() {\n    if (is_resource(STDOUT)) {\n        fwrite(STDOUT2, \"Resource!\\n\");\n    } else {\n        fwrite(STDOUT2, \"Not a resource: \" . (int) STDOUT . \"\\n\");\n    }\n}\ncheckStdOut();\nfclose(STDOUT);\ncheckStdOut();\n```\nSTDOUT might be getting closed by accident if it's being passed to the constructor of a GuzzleHttp\\Psr7\\Stream object, which will close its underlying stream when __destruct is called.\nHope that helps! Please feel free to reopen if you have any question or concerns.\n. Is there a particular reason you need to use the 2.8 branch of the SDK? Migrating from guzzle 3 required a complete rewrite, which is what's in the master branch. \n. It's not just for 5.3 compatibility; it's the source for all v2 releases. As much as we'd like everybody to have already upgraded to v3, it only came out a few months ago, so a large number of projects are still relying stable 2.8.* releases. \nThe original reason for renaming guzzle/guzzle to guzzlehttp/guzzle was so that you could use both in a composer-managed project. I've seen several applications in the wild use v2 of the AWS SDK (and thus rely on Guzzle 3) as well as a more modern version of Guzzle (for instance, as a dependency of Goutte), so a legitimate purpose is being served by using guzzle/guzzle.\n. I get it, but having v2 of the SDK use guzzle/guzzle means that your application can depend on other libraries (such as Goutte, Payum, Elastica, or SDKs for other APIs like Auth0) that require a different version of Guzzle. \nIf 2.8 switched its dependency from guzzle/guzzle to guzzlehttp/guzzle, it would no longer be compatible with any of those libraries.\n. 2.3 to 2.8 is a big upgrade with some backwards-incompatible changes (we didn't start adhering to strict semantic versioning until 3.0). \nAre you trying to send an empty body with an UploadPart request?\n. I see. The officially supported way to orchestrate a client-side upload is to use pre-signed URLs. Doing so will remove the need to manually dispatch and intercept client events and will also ensure that you don't sign the request with the wrong body.\n. Out of curiosity, do you see the same issue of you drop the concurrency to 1 or 2? If so, that would point firmly in the direction of file handles being left open.\n. What appears to be happening is that you're hitting your file descriptor limit before PHP starts a garbage collection cycle. Each of the files handled by the transfer manager gets converted to an instance of GuzzleHttp\\Psr7\\Stream, which takes care of the resource lifecycle with a standard RAII pattern -- the class will close the handle in its __destruct method. That method will be called when the garbage collector tries to reclaim memory, which doesn't happen automatically until the PHP engine has identified 10,000 zvals as potentially garbage. OS X has a file descriptor limit of 1024, so you'd hit that first if you're running a simple script that just uploads a directory.\nI'm looking into how to fix this in the long term, but for now you have two options: \n1. Disable garbage collection, or \n2. Trigger it before every file upload.\nPHP lets you turn off its cyclic garbage collector by calling gc_disable(). This will make PHP use refcounted garbage collection instead, meaning that an object's memory will be freed and its destructor called as soon as it falls out of scope. The danger with this approach is that circular references created while the garbage collector is turned off will not be detected, even if you turn the GC back on after finishing your upload. (If you're running a simple script (or handling a request) that terminates right after the upload finishes anyway, this would probably be your easiest option.)\nIf you'd rather not turn off the GC or if the upload is the first step of a multi-step workflow, you can have the transfer manager manually trigger garbage collection before initiating each individual upload. Given the code you posted above, that option would look like this:\n``` php\n$source = \"/tmp/folder\";\n$dest = \"s3://bucket/key/\";\n$options = [\n    \"concurrency\" => 5,\n    'before' => 'gc_collect_cycles',\n];\n$manager = new Transfer($s3Client, $source, $dest, $options);\n$res = $manager->transfer();\n```\nYou might want to check out the PHP manual's description of how the GC works before deciding which option makes the most sense for your application.\n. The phar and zip downloads for version 3.3.4 of the SDK include version 6.0.2 of Guzzle, which is the latest stable release. There are some changes in the master branch of Guzzle that have not yet made it into a release yet; those are listed here.\nIf you're using the phar, you can verify this by navigating to the folder in which aws.phar is located and running php -r 'require \"aws.phar\"; echo GuzzleHttp\\ClientInterface::VERSION . \"\\n\";'.\n. @GrahamCampbell can you squash out the first commit?\n. That's a fair point about Scrutinizer. Are you getting a lot of 'unknown method' type warnings on PRs?\n. Method annotations on clients were added back in #760 and should go out with the next release.\n. Pulling in updates to the smoke tests.\n/cc @mtdowling \n. Which version of the SDK are you using?\n. rename should work the way you're using it. Does the operation fail silently, or do you get an error and stack trace?\n. rename would emit warnings and return false instead of throwing an exception (similar to what would happen when operating on local files). Did the function return true and not send anything to your PHP/Nginx/Apache logs?\n. It's a blocking operation that always returns a boolean value. On a file system, rename operations can fail for a number of reasons (permissions, nonexistent destination directories, etc), which will cause rename to return false.\nOnce the function returns, the operation will have either succeeded or failed. You wouldn't have to wait.\n. I did. I'm asking to figure out where the error originated. If the operation failed without throwing any warnings and without returning a value, that would mean that there's an issue with the stream wrapper. If it did register a warning and return false, that would indicate an issue with the S3Client.\n. That will depend on how your system is configured, but running php -i | grep error_log should tell you where the file is located.\n. It sounds like this might be related to a fix that went out in yesterday's release (3.3.5). Could you upgrade, try again, and let me know if that solves your issue?\n. This is an issue where either all versions of v3 would work out none of them would. I'll start on a fix.\n. Just merged in the fix. Let me know if anything comes up.\n. Stream wrappers only respond with booleans, but the wrapper will trigger warning-level errors. Where and whether these are warnings are recorded is controlled by your environment's configuration, as detailed in here. Whether they are recorded is controlled by the error_reporting setting and where is controlled by error_log. Note that these might be set by your webserver's configuration and might differ between the PHP CLI and a server environment.\nWith the master branch, I am able to copy files over 5GB. A working example is at https://gist.github.com/jeskew/e23b5ef2ec51da90aff3\n. I updated the gist. Rename calls a multipart copy under the hood.\n. @mtdowling The new annotations don't end up in the docs.\n. The magic was introduced with .json.php compilation build step, where fast linting makes a real difference:\n```\na45e60c78b4f:aws-sdk-php eskewj$ time php build/compile-json.php \nreal    0m9.634s\nuser    0m7.076s\nsys     0m1.960s\na45e60c78b4f:aws-sdk-php eskewj$ time php -dopcache.enable_cli=1 build/compile-json.php \nreal    0m2.051s\nuser    0m1.807s\nsys     0m0.107s\n```\nUnfortunately, I ended up having to use php -l for docblock generation, as the opcache cannot recompile code that has already been loaded into a given process if it declares named symbols. php -l seems to be fast enough.\n```\na45e60c78b4f:aws-sdk-php eskewj$ time php build/annotate-clients.php --all\nreal    0m2.900s\nuser    0m1.909s\nsys     0m0.721s\n```\n. @mtdowling I think you've got a point about the async methods. I'll add those in and remove the ini param from the makefile.\n. What version of the SDK are you using?\n. Any information you can provide about your execution environment would be helpful. I'm unable to reproduce using the default HTTP handler with the following stack:\nSDK: 3.3.5\nPHP: 5.6.12\nGuzzle: 6.1.0\ncURL 7.43.0\nOSX: 10.10.5\n. Well, I think the error message could use some work. Let me see if I can surface the exact issue more explicitly.\n. The underlying issue was resolved upstream in https://github.com/guzzle/promises/releases/tag/1.0.3 .\nPlease feel free to reopen if you have any questions or concerns.\n. @lonormaly It still needs to be reviewed. You can check out the feature branch directly if you'd like to do some exploratory testing.\n. @mtdowling The latest commit adds support for copying versions of objects, as that portion of the key cannot be URL encoded.\n. You would normally see a 403 when you're using the wrong credentials. Are you using environment variables, an ini file, or the EC2 metadata service?\nThe most common cause of this error is that PHP tends to be run by a webserver, which has a different user account and therefore a different home directory. This would mean that the CLI would pick up the /home/<cli-user>/.aws/credentials file while your PHP would pick up the /home/<fpm-user>/.aws/credentials file. If no credentials file exists in the home directory of the server user, the SDK will attempt to pull an IAM role from the EC2 instance.\n. I believe the CLI's cp command defaults to an ACL of 'private', so your user might not have had permission to create 'public-read' objects (which you're doing in the PHP sample) until after you added it to the group. In any case, the PHP sample required higher permissions than did the python script.\n. Thanks for reporting this! I'll make sure to update the docs with the next release. FWIW, the v3 docs are brought up to date more regularly and have the same API operations as v2.\n. Sorry to take so long to update this ticket. The V2 documentation page linked to is correct for V2, but InvalidationBatch is a required parameter in V3. We're working on making the differences between v2 and v3 more visible.\n. This has been fixed in the latest release, 2.8.21.\n. Reset connection errors are retried up to three times before the request fails. You could try increasing that value by passing adding a retries key in the options array being passed to the client constructor:\nphp\n$s3=new Aws\\S3\\S3Client([\n    'version' => 'latest',\n    'region' => 'us-east-1',\n    'credentials' => [\n        'key' => KEY,\n        'secret' => SECRET\n    ],\n    'scheme' => 'http',\n    'retries' => 11,\n]);\nI suspect that you would see fewer reset connections if you allowed the SDK to use https, but I don't have any numbers to back that up.\n. The S3 client does implement exponential delay for request retries. The retries option means \"retry this request up to X times,\" so increasing it from 11 to 3 is unlikely to be related to the increased number of errors you're seeing; anything that would fail with 11 retries would fail more quickly with 3.\nThe one thing I can think of is that Guzzle 6 only adds an 'Expect: 100-continue' header to requests with payloads greater than 1 MB. Are the failing payloads all smaller than 1 MB?\n. @jimmaay You might want to try adding the expect header yourself (check out the docs page on mapRequest middleware if you're unsure how to do so).\nBut since you're seeing this error with requests on which that header has already been set, I think the safest route would be for you to aggressively retry uploads that fail.\n. The backoff as implemented will increase rapidly with each retry. Increasing the number of retries to a value you feel comfortable with is the easiest way, though you can wrap the upload in a try/catch loop in your code, too, if you'd prefer.\n. If you get a successful response, then the request will not be retried. But you're right -- retrying those failures will put more strain on your hardware, which can be problematic if the root of your issue is contention on your end of the network. (It will also marginally increase the load on S3, but I think those servers can handle it. :) )\n. I think you'd get farther talking to someone at S3. Have you tried reaching out to AWS support or asking a question on the S3 forum? AWS support would have a lot more insight into your account.\n. 500 errors do not originate from the SDK, which is why I think S3 support would be able to give you a more comprehensive answer. FWIW, the exponential backoff algorithm in the SDK was provided by the S3 team, so I presume it's in line with what they want. From what I've read on the S3 forum, 500 errors are not unusual.\n. Looks like the connection reset errors would not be retried with all versions of Guzzle. I added an additional check for it in the S3 retry handler in 2f985ba.\n. The retry handler should not be retrying 400 responses in any case. Could you provide more verbose output? I've been trying to reproduce with the following script, which returns 400 responses to all S3 commands:\n``` php\n<?php\nrequire DIR . '/../vendor/autoload.php';\nuse Aws\\S3\\S3Client;\nuse Aws\\S3\\Exception\\S3Exception;\nuse GuzzleHttp\\Exception\\ClientException;\nuse GuzzleHttp\\Promise\\RejectedPromise;\nuse GuzzleHttp\\Psr7\\Response;\nuse Psr\\Http\\Message\\RequestInterface;\n$handler = function (RequestInterface $request) {\n    return new RejectedPromise([\n        'connection_error' => true,\n        'exception' => ClientException::create($request, new Response(400)),\n        'response' => null,\n    ]);\n};\n$s3 = new S3Client([\n    'region' => 'us-west-2',\n    'version' => 'latest',\n    'http_handler' => $handler,\n]);\n$command = $s3->getCommand('PutObject', [\n    'Bucket' => 'bucket',\n    'Key' => 'key',\n]);\n$pool = new \\Aws\\CommandPool($s3, [$command], [\n    'fulfilled' => function () {\n        echo \"GREAT SUCCESS!\\n\";\n    },\n    'rejected' => function (S3Exception $e) {\n        echo \"REJECTED: {$e->getMessage()}\\n\";\n    },\n]);\n$pool->promise()->wait();\n```\nRunning the above always outputs REJECTED: Error executing \"PutObject\" on \"https://s3-us-west-2.amazonaws.com/bucket/key\"; AWS HTTP error: Client error response [url] https://s3-us-west-2.amazonaws.com/bucket/key [http method] PUT [status code] 400 [reason phrase] Bad Request.\nIs there a loop surrounding the code snippet you provided in the original issue?\n. I saw there was a special case for the particular error message you mentioned and that it would in fact be retried indefinitely. This issue would be solved by cef4687.\n. I did. I think the best the SDK can do in that case is to limit the number of retries to what's been configured. I was unable to reproduce the issue with the link you provided, but you should open an issue on Guzzle if you routinely encounter errors trying to download something with a Guzzle client.\n. Hi @thenetimp,\nAre you using a ~/.aws/config file with the CLI? Files at that location are not loaded by the PHP SDK.\n. I ask because the CLI allows users to specify a default region in ~/.aws/config, while the SDK requires that regions be specified at runtime. KMS clients must be instantiated with a region, as the region is used as a part of the service's authentication scheme.\n. One of the CLI engineers confirmed that a region is required for performing operations on KMS with the CLI, so your default region is likely stored somewhere on your environment. Running aws configure list will display the configuration stored on your environment and tell you where each variable is coming from.\nYou can find more information about CLI configuration here.\n. I don't see any of that set in the config file you posted. Do you mean the file that adds a cache to the default settings?\n. IAM instance roles are a way of managing credentials, not regions. You would still need to set the region on your instance of the client class. The blog post includes an example of how to set a region in a config file:\n``` php\n<?php\nreturn array(\n    'includes' => array('_aws'),\n    'services' => array(\n        'default_settings' => array(\n            'params' => array(\n                'region' => 'us-west-1',\n            )\n        )\n    )\n);\n```\nBecause you're using instance profiles, you do not need to provide credentials in this file. If you would like a default region to be set, then you must provide it either in the config file or in your client's constructor.\n. Thanks for reporting this! Looks like Java, JavaScript, and Ruby all use aws_session_token.\n. I like the idea of supporting chmod with the S3 StreamWrapper, but I'm not sure I agree with how you're mapping permissions. Why is this looking at the user permissions instead of global permissions? I would personally be surprised if calling chmod('s3://bucket/key', 0700) made an object world-readable.\n. I'd prefer to have @mtdowling weigh in before proceeding. \n. After spending some time trying to come up with the \"right\" octal to ACL mapping, I have to say I agree with @mtdowling and I don't think there's any way to do so cleanly. Would it help your use case if stream_metadata were implemented and always returned false?\n. @femans Is there a reason why using options set on a stream context wouldn't solve your issue? You stated that you needed to upload items to S3 and have them be publicly readable, which can be accomplished as follows:\nphp\n$handle = fopen(\"s3://$bucketName/$key\", 'w', null, stream_context_create([\n    's3' => ['ACL' => 'public-read'],\n]));\nfwrite($handle, \"I'm publicly visible!\");\nfclose($handle);\nHaving chmod return false instead of throwing an error sounds like a better experience, but I would want to be able to accurately model 0642 before adding support for a full-fledged chmod. It looks like this wouldn't be possible with canned ACLs.\n. Sure. You might also want to take a look at filesystem abstraction libraries like Flysystem and Gauffrette. Flysystem only supports two permission levels (completely private or completely public), but it sounds like that would fit your use case.\nI'll open a separate PR to have stream_metadata always return false.\n. From what you describe, it sounds like HHVM does not create a session ID before calling the session handler methods associated with session_start. session_id is only supposed to return an empty string if session_start has not yet been called, but it's actually session_start that's calling the open method. \nWith the change you made, is it possible to start a new session? Returning false from open should cause session_start to fail.\n. So this change causes the same number of requests to fail, just at the call to session_start instead of when the session is written?\n. Oh I see. Without a session ID (or rather, with a session ID of ''), how are those sessions saved?\n. So it looks like the the runtime will pass in the correct session ID when writing to a session, so this wouldn't affect requests where you were writing data on new sessions. It will, however, affect calls made in the close method, where a timestamp is written using the value saved at $this->openSessionId.\nI think your initial impulse was right and that the open method should be rewritten such that $this->openSessionId = session_id() ?: session_regenerate_id(); (along with a comment that it's to support HHVM and accompanied by a test). I would be open to merging in that pull request.\n. You would need to make sure that a session ID is sent to DynamoDb even if write is never called. You can mock out the entire HTTP handler and call assertion methods there if you'd prefer. A good example of how to do that would be the testRetriesConnectionErrors in the Aws\\Test\\S3\\S3ClientTest class.\n. Would you be interested in translating what you've done above into a pull request?\nI would also encourage you to file a bug report with HHVM, as the behavior you're describing differs from that of the Zend engine. \n. @Lansoweb are you using v2 or v3?\n. Thanks for trying that out. I opened #812 as a possible fix.\n. #812 was merged into master and will go out with the next release.\n. The 'DeleteMarker' property does not indicate whether your call to deleteObject succeeded. (If you did not receive an error response, you can assume your deletion succeeded.) If you have turned on versioning on your bucket, calling deleteObject will set the current version of an object to a delete marker. You can remove a delete marker by calling deleteObject on an object and providing the marker's version ID, which will effectively restore your object to its last version prior to being deleted. This would be the only situation in which 'DeleteMarker' would be true -- when the target of your deleteObject call is itself a delete marker.\n. DeleteObject is supposed to be completely idempotent, so returning the same response whether or not a file still exists is by design. DeleteObject should return an error response (400- or 500-level) if your request did not succeed.\nIf you need to make sure that the file has been completely removed before taking another action, you can call doesObjectExist in a loop until it returns false. V3 offers an objectNotExists waiter that accomplishes this.\n. Can you attach a debug plugin to the s3 client and post the output?\n. Any update on this?\n. Ok. I'm closing this issue for now but feel free to reopen it if you're able to reproduce the error.\n. @amitchhajer Can you attach a debug plugin to the s3 client and post the output?\n. Sorry, I thought you were using v2. For v3, you can turn on debugging by adding 'debug' => true to your call to S3Client::factory. No need for a debug plugin.\n. The auth header captured is: Authorization: AWS4-HMAC-SHA256 Credential=app/console doctrine:migaS3/20151028/ap-southeast-1/s3/aws4_request, SignedHeaders=host;x-amz-acl;x-amz-content-sha256;x-amz-date, Signature=[SIGNATURE].\nIs 'app/console doctrine:migaS3' somehow being set as the AWS_ACCESS_KEY_ID environment variable?\n. Thanks for submitting this!\n. I don't think the behavior of permissions on mkdir can change without a major version bump. chmod can use a different mapping, and maybe there can be an opt-in mechanism for mkdir. I can pick this up if you'd prefer, as this is more work than you initially signed up for with #776.\n. Closing in favor of #776.\n. Would you still want an exception to be thrown when the remote path does not exist?\n. The underlying issue was resolved upstream in https://github.com/guzzle/promises/releases/tag/1.0.3 .\nPlease feel free to reopen if you have any questions or concerns.\n. You can pass a callback to as a before_upload option to the multipart uploader. Looks from #786 that you found this on your own.\n. A before_upload callback will let you keep track of which uploads have started. It's probably what you want to use if you're looking to make a progress bar or something similar.\nInstances of Aws\\S3\\MultipartUploader, Aws\\S3\\MultipartCopy, and Aws\\Glacier\\MultipartUploader have a getState method that returns an instance of Aws\\Multipart\\UploadState. That object has a getUploadedParts method that returns a sorted array of the parts that have been successfully uploaded, on which you can call count.\n. I would guess that it had something to do with the $tempSource parameter you were passing in.  Looks from #787 that you found this on your own.\n. Using an instance of Aws\\S3\\MultipartUploader will download the copy source and upload it to another key. You can use Aws\\S3\\S3Client::copy or an instance of Aws\\S3\\MultipartCopy to perform a copy operation on S3 instead.  Looks from #788 that you found this on your own.\n. There was an issue with the way the byte range was being specified on the last chunk of a multipart copy. A fix has been pushed to master and will go out with the next release.\n. I would rather wait for the PSR-11 to land before adopting an interface as a standard. Until then, you could use a thin wrapper around an instance of Aws\\Sdk as a bridge (like your sample implementation), and that would work just as well outside the SDK (as a small package with a dependency on the SDK) as it would integrated into it.\n. Until container-interop is adopted by a standards body, I don't think integrating it into the SDK is the right move. However, I would encourage you to publish the sample implementation above (or something similar) as a separate package that depends on the SDK, as it doesn't take much code to integrate the Aws\\Sdk service locator with a simple container interface, and that code works just as well in a third-party package as it would in the SDK itself.\nPlease feel free to reopen the issue if you have any questions or concerns.\n. Thanks!\n. ~~Try adding a trailing slash to the localhost endpoint (as described in the Guzzle docs).  Without a trailing slash, relative URLs should replace the final element in the base URL.~~\nThis was incorrect; the behavior you're looking for is not currently possible with the SDK.\n. @jeremeamia is correct; there is no native support for this behavior in the SDK. Because the S3 API is defined with URLs beginning with '/' (e.g., createBucket is mapped to /{Bucket} and putObject is mapped to /{Bucket}/{Key+}), you cannot use a subfolder as an S3 endpoint.\n. Is there a reason you need to use an endpoint like https://foo.s3.amazonaws.com/? \nIf you use the default endpoint, you will only need to establish one SSL connection. If you want to use two clients to perform a rename operation, each talking to a different endpoint, you would double the SSL overhead incurred.\n. I think you'll be happier with the results that way. The advantage offered by virtual-hosted buckets is that you don't need to know the region in which a bucket resides, but since you're already specifying that your bucket is in us-east-1, just using one client will increase performance.\nClosing, as it sounds like you're comfortable with the proposed solution. Please feel free to reopen if you have any questions or if any blockers come up.\n. I agree with @mtdowling that this would be out of scope for the SDK but would be a good complementary project. The difference between development and production environments is more of an application-level concern than a library-level one.\nPlease feel free to reopen the issue if you have any questions or concerns.\n. I didn't commit and push one final change on Friday, which was to generate the sitemap after the API docs had been written out. The previous sitemap only contained links to class, namespace, and function docs.\n. This PR:\n- Adds a listing of an API version's methods underneath each version link on a client class's page,\n- Adds redirects for forward-compatible APIs,\n- Updates the API doc sitemap after writing out the version docs, and\n- Automates the production of the quick links on the homepage.\n/cc @chrisradek \n. Hi @lesterjanpearson,\nRoute53 requests need to be signed against the 'us-east-1' region. The same applies to anything listed on the regions and endpoints page as having a single global region or a region of \"N/A.\"\nHope that helps! Please feel free to reopen if you have any questions or concerns.\n. Thanks for the contribution!\n. Could you try deleting the file .pem file in /tmp and seeing if that helps?\n. Looks like it's related to https://github.com/guzzle/guzzle/issues/568.\n. Thanks for reporting this! #802 should fix the issue you're seeing.\n. #802 was included as part of 2.8.23. Please feel free to reopen if you have any questions or concerns.\n. Are you sure you're looking at the right documentation? The docs you linked to are for the current version (v3), but the error reported indicates that you're using version 2. The v2 docs for the deleteObjects operation indicate that this is expected with v2.\n. In which region is the bucket you're uploading to located?\n. You'll need to interact with the bucket using a client scoped to the region in which the bucket resides. The error should disappear if you change the region to 'eu-west-1' in the code snippet posted above.\n. Hi @Alorel,\nThis is something we've considered but that would be difficult to implement cleanly in a backwards compatible way. One thing we have yet to figure out is if this approach would be any faster: instead of just resolving a single dependency, aws/aws-sdk-php, composer would need to resolve a dependency graph. In your case, this wouldn't just be s3-sdk-php and sqs-sdk-php but also supporting packages that handle fetching credentials, marshalling requests, signing requests, etc. My understanding is that composer's speed tends to be tied more closely to the complexity of a project's dependency graph than to the number of bytes pulled down, so we would need to gauge the size and nature of how such a change would affect installing the SDK.\nAre you seeing any particular issues that you believe a smaller dependency would solve?\n. @Alorel It sounds like you might benefit from using rsync -c or something else that checksums files before replacing them instead of SCPing or SFTPing the whole kit and kaboodle.\nI'm going to leave this issue open in case anyone else wants to chime in on why they would or would not want a modularized SDK.\n. For people who don't use Composer, this change would be invisible. We would still distribute the SDK as a phar.\n. Thanks for reporting this. I opened a PR on Guzzle to try to fix the issue upstream.\n. The PR has been merged into a guzzle/psr7 release, so running composer update should fix this for you. Please feel free to reopen if you have any questions or concerns.\n. Since the exception message provided indicates that a cURL \"failed to connect\" error was encountered, I think you're probably right in the assumption that the problem is in how the CNAME URL is forwarding to S3. Did you follow the instructions on the S3 documentation concerning how to set up CNAME access to buckets?\n. Closing, as the issue doesn't seem to lie with the SDK. Please feel free to reopen if you have any questions or concerns.\n. Hi @jrivera294,\nThe error you're seeing is not being thrown by S3 but by the EC2 credentials metadata service -- when an S3Client can't locate credentials in environment variables or in an ini file at ~/.aws/credentials, it will attempt to fetch instance profile credentials. \nWhat's probably happening is that you have valid credentials in an ini file on your development machine and have not assigned an IAM role to the instance running the code above.  Passing false for your credentials will create an anonymous client and keep the SDK from trying to find credentials.\n. @SlightlyCyborg Can you post the code that is triggering the error?\n. @dhensby I think that concern can be addressed without reverting this patch, both because this PR was for the current 3.x branch of development and because #821 should be able to support HHVM and PHP 7.\n. It does look like Guzzle is interpreting the value as milliseconds. Thanks for the detailed report and all the investigation into this!\n. This was fixed in #816 and will go out in the next release.\n. I believe the DynamoDB docs imply that (50 * 2 ** (retries - 1)) should be a maximum value: \"For example, up to 50 milliseconds before the first retry, up to 100 milliseconds before the second, up to 200 milliseconds before third, and so on.\"\nThis change will cause the delay to be an average of 25 ms before the first retry, but never more than 50 ms.\n. This looks like the issue reported in #376. Are you using NSS as the TLS backend for cURL? (You can find out by running var_dump(curl_version());.) It looks like some users have had to migrate from NSS to OpenSSL in response to similar errors.\n. On the plus side, PHP 7 will support large files and 64-bit numbers. Unfortunately, only RC releases are available for download. A GA release should be coming out soon (i.e., before December), though!\n. This appears to be a known limitation of using PHP's SessionHandlerInterface: there's no way to hook into session_regenerate_id using the object-oriented version of session_set_save_handler. This StackOverflow question has a bit more detail, including a note that PHP < 5.5 won't be able to hook into session_regenerate_id even using the callback-based session_set_save_handler.\nI think the session handler should keep track of the ID associated with the data it has read. Keeping that context means that the check to see if data has changed will be more reliable.\n. Fixes to v2 (#825) and v3 (#826) have been merged. Thanks for all your help on finding the right solution to this, @dhensby!\n. Since you're planning on changing session IDs, I think this PR needs to take account of the fact that you're writing to multiple IDs. Maybe $this->dataRead should be changed from a string to an array mapping IDs to session data?\n. What if you change the session ID twice?\n. You're not updating the stored session ID ($this->openSessionId) when you know that the open session's ID might have changed. If you'd prefer to handle this in a way other than using an array, that's fine. But if you're updating this class to knowingly handle multiple session IDs, it should do so throughout.\n. In the code posted, the write method will not be used. A single write will be triggered by the __destruct method, which will be called by the runtime at the end of the PHP process. The problem this introduces is that the session handler tracks the session data read ($this->dataRead), the open session ID ($this->openSessionId), and whether any data has been written to the session ($this->sessionWritten). Those are all scalar values that assume the session ID will not change.\nIf there were a way in PHP >=5.3.3 (which is the version v2 has to support) to hook into session_regenerate_id, that hook could be used to wipe out $this->dataRead, $this->openSessionId, and $this->sessionWritten. But since that hook was introduced in PHP 5.5, the tracking variables themselves need to be modified so that they can keep track of the ID, data, and write status of multiple session IDs, as those session IDs can be changed with no notification an unlimited number of times.\nI get that the PR as written solves the particular issue you ran into under the circumstances you're expecting, but I think it can be made more general. \n. @tractorcow Exactly. Just checking the id provided to write does nothing to correct that assumption.\n. Following the chat on gitter, I'm sold on calling this a bug instead of an enhancement, as we're making it harder to follow OWASP session management recommendations.\n. Closing in favor of #825 and #826. Thanks for all your work on this and the fruitful discussion.\n. You mention that the errors started appearing a few days ago; did this follow an upgrade? If so, what version did you upgrade from?\nAnd have you found any way to reliably reproduce this error, or does it occur completely randomly?\n. Do you have a full trace for the first screenshot above?\n. I see that you're using the 'SaveAs' parameter on the getObject call -- is it possible that what's causing the promise to be rejected is a file system error? Just a hunch, but that may explain why the issue is so hard to reproduce.\n. @Halama @ondrejhlavacek Is there any chance you could try with dev-master? #827 should make the underlying error more visible, but it has not yet been included in a release.\n. @ondrejhlavacek Any updates?\n. @ondrejhlavacek I'm tempted to just say that no news is good news on this. It's possible that whatever was causing the error parsing error is now just getting retried and succeeding on the second attempt.\n. Glad to hear it! Please feel free to reopen if you have any questions or concerns.\n. That error should be retried, but it looks like it isn't.\n. @dhensby I added the test you wrote as well to this PR.\n. Thanks for your work on this, but I think this use case is already covered by our phar and zip distributions. The zip distribution contains all of the SDK, its dependencies, and an autoloader.\n. You need to provide credentials through the lavarel package's configuration file, through a ini file at ~/.aws/credentials, through the AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY environment variables, or by assigning your instance an IAM role if you're running on EC2. A full rundown can be found here.\nBy default, the SDK will try all the above, and the cURL timeout is the last attempt failing.\nHope that helps! Please feel free to reopen if you have any further questions.\n. @GrahamCampbell So they did.\n. I installed HHVM nightly (3.11.0-dev) on a local VM and am no longer getting a parse error. Plenty of other stuff appears broken, though.\n. Hi @vlajos,\nThe SDK for PHP doesn't have support for client side encryption yet, but it's something that's on our radar. Are you specifically looking for interop with the Java and Ruby libraries (i.e., to be able to decrypt items encrypted by those SDKs and vice versa)?\n. This is a feature that we'd like to add. I'm going to leave this open as a feature request to track work and community support.\n. @davedevelopment For an official S3 encryption client in the PHP SDK, compatibility with the Java and Ruby SDKs is a must. That means that the client will need to be able to decrypt information encrypted with an AES GCM cipher mode, which is used by SES and the Java and Ruby SDKs. \nUnfortunately, there's no good way to do so in any released version of PHP. The standard OpenSSL extension does not support AEAD (Authenticated Encrypt with Associated Data) cipher modes, so GCM encryption and decryption would rely on a different, non-standard extension or cypto code written in PHP. There is an accepted RFC adding GCM support to PHP's OpenSSL extension and an open RFC to make libsodium a standard extension. GCM support for will definitely land in PHP 7.1, and I'd like to know what the API will look like before writing code against it.\n. That's true. The current Ruby implementation allows you to select a cipher mode, so it is definitely possible to create an encryption client in PHP that is mostly compatible with Ruby and Java.\nIf you write a PHP encryption client only supports non-AEAD cipher modes, then anything it encrypts will be decipherable by the Ruby and Java implementations, and it will be able to decrypt a subset of the objects encrypted by the Ruby and Java clients. You could, for example, configure the Ruby and Java clients to only use AES-256-CBC and write a PHP client that can read and write that cipher.\n. @mtdowling I updated the makefile and moved the concurrency test back to the integ suite. There was an unrelated hiccup in Travis (var_dump overloading was set to 2 for PHP 7, causing many of the validator tests to fail), which is addressed in the last commit.\n. Hi @muratsplat,\nThe error you're seeing is being returned by your local instance of DynamoDB but by the SDK. By default, the SDK tries to source credentials from the environment to sign request. You can disable this behavior by passing 'credentials' => false to the client constructor.\nPlease feel free to reopen if you have any further questions or concerns.\n. It looks like DynamoDB local does require credentials, but it has no credential store to check them against. Testing locally, I found that any key/secret pair would work. Try creating a Dynamo client with hard-coded credentials that wouldn't work in production:\nphp\n$dynamodb = new \\Aws\\DynamoDb\\DynamoDbClient([\n     'region'   => 'us-west-2',\n     'version'  => 'latest',\n     'endpoint' => 'http://localhost:8000',\n     'credentials' => [\n         'key' => 'not-a-real-key',\n         'secret' => 'not-a-real-secret',\n     ],\n]);\nUnless you start DynamoDB Local with the --sharedDb flag, the credentials are used to keep separate databases for separate users.\nHope that helps!\n. The test failure is unrelated and is addressed in #832 \n. @mtdowling I updated the PR to walk the model and insert marker values for dates and blobs. This allows for better support of structures and maps, too.\n. Hi @pampy,\nThanks for putting this together! I saw that the EMR model was a little out of date and pulled in an up-to-date version in the latest release (3.11.5).\n. The intent of the transfer manager is to always blindly download everything within the provided s3 path. The AWS CLI does provide a sync command that provides functionality like what you're describing, and you should use that when you only want to download modified files.\n. The upload will block until it's completed. Do you get the same error if you call gc_collect_cycles() before deleting the local file?\n. The GuzzleHttp\\Stream object holds onto a resource handle until its __destruct method is called. Normally, this means that resources are freed as soon as a stream falls out of scope, but sometimes, depending on the PHP version and whether a script has yet filled the garbage collector's buffer, garbage collection can be deferred. gc_collect_cycles will force the collector to run and call __destruct on all unreachable stream objects.\n. Only getSignedUrl is available at the moment, but signed cookie support sounds like a cool feature.\n. Hi @ScuzzyAyanami,\nThe statistics operations you linked to are not yet supported in the AWS SDKs. It is on our radar, though.\nIn the meantime, if you need to access the statistics endpoints immediately, you can use the SDK to sign an arbitrary request. There was some discussion on how to do so on this ticket.\n. Support for statistics was added in https://github.com/aws/aws-sdk-php/commit/444c31b0786487ce4e8ae3b1d7c55990221db3c2.\nPlease feel free to reopen if you have any questions or concerns.\n. The response is parseable (we can extract headers and a status code), but the body is not. S3 responses are normally XML, but that is not true when executing a GetObject call.\nIn the specific error case that prompted this PR, the HTTP status code was 200 -- the connection seems to have been reset after headers and some data had already been sent.\n. Can you post the full error?\n. No worries! Glad you were able to solve the issue.\n. @mtdowling I moved the policy generation to a method on the signer and removed support for generating custom policies. Anything more complicated than the canned policy would need to be passed in to the signer as a JSON string.\n. Requests that are rejected for having expired credentials should be retried by the DynamoDB client (as configured here). Could you try adding a retry logger (via the client.backoff.logger option) to the DynamoDB client to see if it is retrying?\n. The log captures when requests are retried, not when credentials are refreshed. Even if the first request goes out with expired credentials because the metadata service did not respond, then the ExpiredCredentialsChecker should retry the request. Is that not happening?\n. The instance metadata client does not retry by default because the SDK is designed to fail fast when no valid credentials are available. However, the client is a regular Guzzle client to which you can add a retry subscriber if it fits your use case. Essentially, you will need to pass in a credentials object on which you have configured the client to your needs:\n``` php\n<?php\nrequire 'vendor/autoload.php';\nuse Aws\\Common\\Enum\\ClientOptions as Options;\nuse Aws\\Common\\Credentials\\Credentials;\nuse Aws\\Common\\InstanceMetadata\\InstanceMetadataClient;\nuse Aws\\DynamoDb\\DynamoDbClient;\nuse Guzzle\\Plugin\\Backoff\\BackoffPlugin;\n// Create an instance profile client with custom timeouts\n$instanceClient = InstanceMetadataClient::factory();\n// Add a retry subscriber to the instance metadata client\n$instanceClient\n    ->getEventDispatcher()\n    ->addSubscriber(BackoffPlugin::getExponentialBackoff());\n// Create a cache adapter that stores data on the filesystem\n$cacheAdapter = ;\n// Create a credentials object using the factory method\n$credentials = Credentials::factory(array(\n    Options::CREDENTIALS_CACHE     => $cacheAdapter,\n    Options::CREDENTIALS_CLIENT    => $instanceClient,\n    Options::CREDENTIALS_CACHE_KEY => 'shared_iam_credentials'\n));\n$ddb = DynamoDbClient::factory(array('credentials' => $credentials));\n$ddb->listTables();\n```\nHope that helps!\n. Closing for now, as the above code sample should show you how to enable retries on the InstanceMetadataClient as appropriate for your application. Please feel free to reopen if you have any questions or concerns.\n. That's a good point; this isn't documented in a comprehensive way right now, and I'll open a pull request to address that.\nHow often is the instance metadata returning an error? It should be a very rare occurrence.\n. Hi @eric-tucker,\nAre you using an Elasticsearch library like Elastica or Elasticsearch-php?\n. There's a lot of great Elasticsearch tooling, some of which is pretty easy to inject a signer into. Take Elasticsearch-PHP, for example. The following code will let you use the Elasticsearch-PHP API and automatically sign all requests for you:\n``` php\n$psr7Handler = \\Aws\\default_http_handler();\n$signer = new \\Aws\\Signature\\SignatureV4('es', );\n$credentialProvider = \\Aws\\Credentials\\CredentialProvider::defaultProvider();\n$handler = function (array $request) use ($psr7Handler, $signer, $credentialProvider) {\n    // Amazon ES listens on standard ports (443 for HTTPS, 80 for HTTP).\n    $request['headers']['host'][0] = parse_url($request['headers']['host'][0])['host'];\n// Create a PSR-7 request from the array passed to the handler\n$psr7Request = new \\GuzzleHttp\\Psr7\\Request(\n    $request['http_method'],\n    (new \\GuzzleHttp\\Psr7\\Uri($request['uri']))\n        ->withScheme($request['scheme'])\n        ->withHost($request['headers']['host'][0]),\n    $request['headers'],\n    $request['body']\n);\n\n// Sign the PSR-7 request with credentials from the environment\n$signedRequest = $signer->signRequest(\n    $psr7Request,\n    call_user_func($credentialProvider)->wait()\n);\n\n// Send the signed request to Amazon ES\n/** @var \\Psr\\Http\\Message\\ResponseInterface $response */\n$response = $psr7Handler($signedRequest)->wait();\n\n// Convert the PSR-7 response to a RingPHP response\nreturn new \\GuzzleHttp\\Ring\\Future\\CompletedFutureArray([\n    'status' => $response->getStatusCode(),\n    'headers' => $response->getHeaders(),\n    'body' => $response->getBody()->detach(),\n    'transfer_stats' => ['total_time' => 0],\n    'effective_url' => (string) $psr7Request->getUri(),\n]);\n\n};\n$client = \\Elasticsearch\\ClientBuilder::create()\n    ->setHandler($handler)\n    ->setHosts(['https://:443'])\n    ->build();\n```\nThat would give you IAM authentication and access to the entire Elasticsearch-PHP API.\nHope that helps!\n. Adding an ES data plane client isn't currently on our timeline. There are a lot of mature, fully-featured, open-source Elasticsearch clients already on the market, and my recommendation would be using one of them along with the SDK's signer.\nI will let the ES team know about your request. Customer input is always taken into consideration.\n. In the handler, the request comes from Elasticsearch-PHP. You would pass the $handler function to Elasticsearch\\ClientBuilder::setHandler to tell Elasticsearch-PHP to pass all requests through it.\nThey document handlers here.\n. @NielsCorneille The snippet just uses a Guzzle client, not an SDK client, so you can decorate it with whatever middleware works for you. Your solution is very similar to how we handle promise rejections in the SDK.\n. Hi @ronneseth,\nI'm still looking into this, but in the meantime you can disable client-side validation by passing `'validate' => false to the Route53Client constructor.\n. Glad you were able to figure that out. I'll see if I can do anything about the validator message.\n. Thanks for reporting this. Did the code provided above just start throwing errors without any changes on your part?\n. While I agree that this PR seems to capture the original intent of the referenced commit, I don't think this change is completely backwards compatible. What if other users are expecting IDs prefaced with '/delegationset/'?\n. Good point. I misread this at first and thought it was transforming the response, not the request. Could you add a test?\n. /ping @mtdowling @chrisradek \n. Hi @IgorDePaula,\nIt looks like you installed version 3 of the SDK but are reading the instructions for version 2. (The Aws\\Common\\Aws class is only part of version 2 of the SDK, not version 3.) To create a new SQS client, you can either use the new keyword:\nphp\n$sqs = new \\Aws\\Sqs\\SqsClient([\n    'version' => '2012-11-05',\n    'region' => '<your-preferred-region>',\n]);\nor the \\Aws\\Sdk service locator:\nphp\n$locator = new \\Aws\\Sdk([\n    'version' => 'latest',\n    'region' => '<your-preferred-region>',\n]);\n$sqs = $locator->createSqs();\nIn either case, you will need to provide AWS credentials to the SDK. (The \\Aws\\Exception\\CredentialsException you encountered occurs when no credentials are provided to the SDK, and it attempts to source them from the EC2 Instance Metadata Service.) We have a guide on how to provide credentials and best practices for doing so here.\nHope that helps! Please feel free to reopen if you have any questions or concerns.\n. The code posted is written for version 2 and the error indicates that version 3 is installed. Have you tried any of the methods listed here for providing credentials to the v3 SDK?\n. You could do this by passing in a 'before' option when creating the transfer manager. This would be a callable that takes an instance of \\Aws\\Command and mutates it as necessary. To apply the same ACL to all uploads, you could do something like the following:\nphp\n$transfer = new \\Aws\\S3\\Transfer($client, $source, $dest, [\n    'before' => function (\\Aws\\Command $command) {\n        if (in_array($command->getName(), ['PutObject', 'CreateMultipartUpload'])) {\n            $command['ACL'] = 'authenticated-read';\n        }\n    },\n]);\nWhile I can see the utility of specifying a single ACL to be applied to all files, I think the callback approach provides more flexibility. In many cases, you would want to apply different ACLs to files being uploaded based on their local permissions or patterns in the filename, and that's something that a callable supports.\n. Looks like the before callback option isn't documented very thoroughly. I'll open a PR to fix that.\n. @NielsCorneille does using the before callback resolve this issue for you?\n. Closing, as the transfer manager's documentation was updated to show how to provide an ACL via a callback. Please feel free to reopen if you have any questions or concerns.\n. Similar behavior can be achieved by passing a callable to the transfer manager's constructor (see my comment on the related issue). I'm not sure a blanket ACL is as useful as a callable that can selectively apply ACLs.\n. Closing in favor of #858, as I believe using a callback is a better option. Please feel free to reopen if you have any questions or concerns.\n. You will need to wait on the promise at some point. The SDK isn't forking off background processes or farming work off to a NodeJS-style persistent global event loop, so scripts will need to include a directive to handle outstanding promises.\nWhat the SDK can do with promises is handle them concurrently, which can speed up scripts that involve a lot of I/O time. In fact, the transfer manager promise is composed of promises tracking the upload of individual files. The individual promises are handled five at a time, and when all uploads are complete, the transfer manager's meta promise is fulfilled. If you wanted to upload multiple directories, you could compose two transfer manager promises into another meta promise:\n``` php\n$promises = [];\n$source = \"c:/my-folder/90\";\n$dest = \"s3://my-bucket/90\";\n$manager = new \\Aws\\S3\\Transfer($client, $source, $dest);\n$promises []= $manager->promise();\n$source = \"c:/my-other-folder/90\";\n$dest = \"s3://my-other-bucket/90\";\n$manager = new \\Aws\\S3\\Transfer($client, $source, $dest);\n$promises []= $manager->promise();\n$metaPromise = \\GuzzleHttp\\Promise\\all($promises)->wait();\n```\nThe SDK would then interleave those two directory uploads and spend less time simply waiting for HTTP responses.\nHope that helps!\n. The SDK uses Guzzle promises, which support synchronous waiting by calling ->wait() on a promise. Since a PHP process doesn't have a global event loop like node, scripts need to explicitly run through outstanding promises. Calling wait is the easiest way to do so.\n. Closing, as it seems like all questions have been answered. Please feel free to reopen if you have any questions or concerns.\n. The error is returned by the service, not the SDK, so distinguishing between \"wrong signature version\" errors and legitimate \"signature does not match\" errors would be error-prone and might break applications that know to listen for signature does not match errors to trigger an upgrade to a v4 signer.\nWould you have any objection to just using v4 signatures for all S3 requests? This can be done by setting the signature configuration option to 'v4', and it's what we opted to do in v3 of the SDK.\n. When trying to reproduce this, I was able to make successful requests to both us-west-1 and us-west-2 using v2 signing. Where did you read that those regions were dropping support for that signature version?\n. The CloudSearchDomain client only covers a subset of the API. For more specialized use cases, you can use the SDKs signers to sign an arbitrary PSR-7 request with your AWS credentials.\nHope that helps!\n. /ping @mtdowling @chrisradek \n. /ping @mtdowling @chrisradek \n. Hi @pihish,\nThe code snippet will hang forever, as the condition on the while loop will not be false until after the promise is fulfilled, but wait won't be called until the while loop terminates.\nIf you just want to see updates, you could try setting the debug option on the transfer manager:\nphp\n$manager = new \\Aws\\S3\\Transfer($client, $source, $dest, ['debug' => true]);\nThis will write information on which requests have been successfully completed to stdout.\n. You can pass a callback as the before option to the transfer manager constructor if the 30-second interval isn't crucial:\nphp\n$manager = new \\Aws\\S3\\Transfer($client, $source, $dest, ['before' => function () {\n    // make an HTTP request\n}]);\nThis would ping your endpoint before the upload of each individual file (several times for files larger than 16 MB).\n. Have you tried caching the credentials? If you pass in an instance of Aws\\CacheInterface as the credentials option to a client constructor, the SDK will cache instance profile credentials instead of hitting the metadata service every time.\n. You can pass a callback as the before option to the transfer manager constructor. The callback can modify the UploadPart and CreateMultipartUpload commands to support custom metadata as needed. This approach allows more flexibility than passing in custom options, as you might want to set cache control metadata based on file extension or otherwise tailor your options to the specific file being uploaded.\nHope that helps! Please feel free to reopen if you have any questions or concerns.\n. The keys would differ from command to command. Any option that could be passed to a direct call of a command can be accessed and modified as a key on the command object. For the transfer manager, all commands passed to the before callback will be a PutObject command, a CreateMultipartUpload command, an UploadPart command, or a CompleteMultipartUpload command.\nYou can control what the minimum size to trigger a multipart upload is with the mup_threshold option passed to the transfer manager's constructor.\n. It sounds like the certificate authority is out of date. Could you try setting the ssl.certificate_authority configuration option to either 'system' (to use your operating system's CA bundle) or to the path of an updated bundle?\nSee our FAQ for more information on updating the CA bundle.\n. Exactly.\n. You would need to create that file.\n. Aws\\CommandPool is a v3 feature. In v2, BatchBuilder is a good way to send requests in batches.\n. What version of the SDK are you using? It sounds like #101 is describing the same issues you're seeing, and it was solved by upgrading to a then-current version of Guzzle.\n. @arpan03 What if instead of sleeping for 15 seconds you call gc_collect_cycles()?\n. @arpan03 Any luck with gc_collect_cycles()?\n. Sounds good. I'll go ahead and close this issue, as it sounds like everything's working. Please feel free to reopen if you have any questions or concerns.\n. The requests are sent in parallel, and all should be resolved before a multi transfer exception is thrown.\n. The 'B' key stands for binary in this case, not boolean. Boolean values are flagged as 'BOOL' and unmarshalled as raw boolean values. \nThe reason binary and set values are wrapped is because the PHP representation of their raw values are indistinguishable from strings and arrays, respectively, so the wrapper is necessary if you want to successfully write the same data you read from DynamoDb. Rather than making set wrapping an optional behavior, Aws\\DynamoDb\\SetValue could implement ArrayAccess, which would let you use sets as if they weren't wrapped. Would that work for you?\n. @jeremeamia Good point; if I went that route I would probably make offsetSet throw an exception when invoked.\nI don't think I would want to disable set wrapping in any case, as that behavior could be handled by an end-user's code when they know they absolutely need arrays. Even when just reading, the distinction between a set and a list can be important.\n. I'm not sold on the idea. The most important thing the marshaller needs to be able to do is to round-trip data. If you need data in a different format than what is returned, you can transform the array returned to you:\nphp\n$data = array_walk_recursive($data, function (&$leaf) {\n    if ($leaf instanceof SetValue) {\n        $leaf = $leaf->toArray();\n    }\n});\nThe reason I'm hesitant to add $unwrap as an optional parameter to unmarshalItem is that unmarshalValue already has an optional second argument used to control different behavior. \n. array_walk_recursive will work recursively.\nOnce a set has been transformed into an array, it is indistinguishable from a list. That means that calling $marshaler->unmarshalItem($data, true); would have no inverse, thus preventing you from saving back the data you read. Both the wrap_numbers option and the SetValue class exist to make sure that unmarshalItem is fully reversible; the former prevents loss of precision and integer overflows on 32-bit systems (e.g., PHP 5 on Windows), and the latter prevents the conflation of multiple data types into just array.\nPerhaps you could encapsulate walking and transforming the array into a class in your application? Something like:\n``` php\nclass LossyMarshaler extends Marshaler\n{\n    public function unmarshalItem(array $data, $unwrap = false)\n    {\n        $unmarshaled = parent::unmarshalItem($data);\n        if ($unwrap) {\n            $unmarshaled = array_walk_recursive($unmarshaled, function (&$leaf) {\n                if ($leaf instanceof SetValue) {\n                    $leaf = $leaf->toArray();\n                }\n            });\n        }\n    return $unmarshaled;\n}\n\n}\n```\n. That can be achieved by walking the unmarshaled array or subclassing the marshaler. In that case, the customer knows they are doing something potentially unsafe. @bakura10 I understand that this PR would mean that you wouldn't need to subclass the marshaler in your application, but it's not a usage pattern that the SDK should officially support. DynamoDB treats sets and lists very differently, and I would rather see the SDK move towards reflecting Dynamo semantics.\nThat said, is there anything that could be done to SetValue that would make your data access more intuitive? Maybe implementing support for add, remove, contains, or union? If your data model requires a set and not a list, the wrapper class should provide some value, at least enough value that you would typehint as SetValue instead of as array.\n. @bakura10 That actually sounds like an argument for wrapping sets -- if your data model is using the same key to store either a set or a list, the distinction seems important.\nI opened #876 to track improvements to the SetValue wrapper. Please feel free to chime in there if you think of anything that could be helpful.\n. That's a network-level error, so a definitive root cause would be difficult to pin down. Can you catch the error and retry?\n. There is a built-in retry mechanism, but it won't be triggered for the particular cURL error you're seeing. The reasoning behind that is that connections can be reset after a connection has been made and work has been started, so it could be unsafe to retry non-idempotent requests. For 'ReceiveMessage,' a connection could be reset after a message has been pulled from the queue, which would count towards the 'MaximumReceives' configured on a queue's redrive policy.\n. @MiroCillik does retrying on your end solve the issue?\n. What version are you using?\n. Outputs takes an array of associative arrays, each of which can have a string at Key and a string at PresetId. Your method call should look a bit like this:\nphp\n$out = $ets->createJob([\n    // ...\n    'Outputs' => [\n        [\n            // ...\n            'Key' => 'key',\n            'PresetId' => 'Id',\n        ],\n    ],\n]);\nI'll admit that \"[Outputs][PresetId] must be an associative array\" is a bit confusing, but it's saying that each value under [Outputs] needs to be an associative array so that you can specify multiple output targets.\n. @GrahamCampbell I took it out of the composer suggestions, as anyone with a PSR-6 compliant cache will already have the interfaces present. doctrine/cache is suggested because they provide a cache implementation that can be used, whereas psr/cache does not.\n. What value are you passing to context.succeed in your lambda function?\n. Does everything work as expected if you return a POJO instead of a string?\n. A bit of background on this: in JavaScript, Lambda's documentation explicitly states that you should be passing an object to context.succeed, and V2's Lambda client was built with this in mind. Obviously, there is no such limitation in the Java documentation, where there are instructions on how to return a POJO, a string, or a byte stream. In V3 of the SDK, we treat the function's return as a PSR-7 stream, which can contain any of those data types.\nI'm not sure that v2 could be changed in a backwards-compatible to support multiple return types from Lambda. I believe the issue you're seeing can be avoided either by having your Lambda function return a POJO or by upgrading to V3 of the SDK.\n. It appears that the supplied iterator is ignored and that the base_dir parameter is used instead.\nAdditionally, the docs on that parameter are confusing: it sounds like you must yield absolute filenames and supply a base directory, and those should be mutually exclusive. Given the way the transfer manager's constructor is validating the semantics of a transfer between $source and $destination, I would opt for having iterators supply relative paths and requiring a a base_dir.\n. The Iterator parameter was already documented as requiring absolute paths (this is how it worked in v2), so changing it to expect relative paths would be a breaking change. You're right that there is a bit of redundancy, but the base_dir parameter is really there to indicate what subfolder of a given bucket should be considered the top level folder when downloading. When a user supplies a path string as a source, that string can be used as both a source and a base_dir, but iterators are more ambiguous.\nYou can always use the \\Aws\\map function to prepend the base dir to each item in an iterator that yields relative paths:\nphp\n$justOneFile = \\Aws\\map(\n    new \\ArrayIterator(['photos/christmas.jpg']), \n    function ($relativePath) {\n        return  \"s3://bucket/media/$relativePath\";\n    }\n);\nAws\\map is a lazy mapping function, so you can use it for $justOneFile or $overOneMillionFiles and not have to worry about memory.\n. Moving to our backlog.\n. The flags passed to url_stat (and subsequently to triggerError) are set by the PHP engine, and STREAM_URL_STAT_QUIET is set because is_file, file_exists, and is_dir are not supposed to trigger errors when a file does not exist -- they are just supposed to return false. \nThe is_file, file_exists, and is_dir functions should never throw exceptions when a path is not found, so the stream wrapper might not be a good fit for your application if you rely on exceptions being thrown. The HeadObject command will throw an exception if a key does not exist, so that might be the method you want to use.\n. Thanks!\n. @jeremeamia Best code review ever. :sheep: \n. I don't think you'll be able to see improvement on the metrics you're examining.\nIn the left hand chart, you're measuring wall clock time, and in both cases, the function that Guzzle uses to wait for a response comes in at ~85 seconds. It's about five seconds faster in the Guzzle 6 example, but in both cases what's being measured is network I/O time. Which is to say, no significant improvement is likely because that number is dependent on the performance of DynamoDB, which does not change between the first and second example.\nIn the right hand chart, what appears to be tracked is total memory used without taking memory freed into account. Guzzle\\Psr7\\Stream::read is a wrapper for fread. A small amount of data is read from a stream with fread and immediately returned, which will cause the PHP engine to use and free $length bytes of memory. If you pipe a 2 GB stream through a wrapper, then 2 GB of memory will be \"used\", with peak memory usage never exceeding what's necessary to read a single chunk.\nIf you don't think that's an accurate characterization of the test you're running, code you share the code used for your benchmark? We regularly run a suite of performance tests to check for memory leaks on large downloads and uploads (you can view a graph of the most recent results here), and I haven't seen the Guzzle stream classes hold on to any memory that should have been freed.\n. Closing, as I believe I've answered your question about memory usage. Please feel free to reopen if you have any further questions or concerns.\n. It doesn't look like the stack trace attached is using the SDK; rather, a standard Guzzle client is sending an unsigned post request to a bucket URL. An empty 100 response is the server telling you to go ahead and send your payload.\n. No worries, @ondrejhlavacek.\nS3 responds with that error when a client hasn't sent any bytes for 20 seconds. The SDK does retry those errors (error responses are checked here), and we have a test that simulates this error.\nIf you're seeing this error surface, that means that it occurred after the SDK exhausted all of its retries. Clients are configured to attempt three retries by default, but you can override this by setting the retries option on a client to any integer. You can also override the number of retries on a per-operation level by setting @retries in the arguments passed to PutObject.\nYou might also want to look at what might be causing this error. You might be experiencing a lot of contention on your local network.\n. Are you providing the file to the PutObject command as a resource handle? Guzzle was not rewinding streams before sending them to cURL prior to version 6.2.0 (https://github.com/guzzle/guzzle/pull/1422), which could explain why all request retries time out.\n. The credentials file should be in ini format but not have a .ini extension. It should have a 'default' section defined with your key and secret:\n```\n$ less ~/.aws/credentials\n[default]\naws_access_key_id = key\naws_secret_access_key = secret\n```\nUsing a credentials file or environment variables is the recommended way of providing credentials on your own server, but if you prefer to hard code the key and secret, it must be passed in as part of an argument hash, not as positional parameters. For example:\nphp\n$s3Client = new \\Aws\\S3\\S3Client([\n    'version' => '2006-03-01',\n    'region' => $yourPreferredRegion,\n    'credentials' => [\n        'key' => $key,\n        'secret' => $secret,\n    ],\n]);\n. Can you share an error message or stack trace?\n. Make sure the user your code is running as has permission to read the the file. Also, make sure that you've created the file for the correct user. The SDK will attempt to read from the home folder of whatever user the code is running as.\n. The file will need to be in the home folder of the user running the code. If your server executes code as www-data, then on an Ubuntu server the credentials file would need to be stored as /home/www-data/.aws/credentials. The www-data user should have access to its own home folder and its contents.\n. Once you move the file into www-data's home folder, you should change the ownership of the file and the .aws folder to www-data as well. Call chown -R www-data:www-data /home/www-data/.aws (this may need a few tweaks depending on your linux distro). You will need to be root or have sudo privileges to do so.\n. For v2, you will need to remove the BucketLoggingStatus array from parameters passed to putBucketLogging and place the LoggingEnabled array at the top level of the hash:\nphp\n$s3Client->putBucketLogging([\n    'Bucket' => 'this-is-my-super-cool-test',\n    'LoggingEnabled' => [\n        'TargetBucket' => 'my-super-cool-logging-bucket',\n        'TargetPrefix' => 'logs/this-is-my-super-cool-test/',\n    ],\n]);\nThis is a difference between v2 and v3. Make sure you follow the v2 documentation when using version 2.4.10.\n. Clients are not serializable due to the closures used in their configuration arrays. Does your test work if you don't serialize a client?\n. @steefaan Does Graham's comment sound related?\n. I was able to reproduce this error by including a client in a data provider and running a test with the --process-isolation flag. I believe you would see a similar error if a client resides in any part of global state that PHPUnit attempts to share between isolated processes.\nGiven the degree to which this library relies on closures and allows users to pass in closures as configuration parameters, I don't see a backwards-compatible way to make clients safely serializable. I think it would be appropriate to add a __sleep method to Aws\\AwsClient that immediately throws an exception so that this failure case is more clear.\n. If this PR is for version 2, you want to open it against the 2.8 branch, not the master branch.\n. Do you have a more detailed error message? The only modeled error responses for GetUser are NoSuchEntityException and ServiceFailureException.\n. Each service has an exception class that is used by default if no concrete class exists for a given error. The line in the SDK at the top of the call stack is not attempting to instantiate a class, just asking whether that class exists:\nphp\n$className = class_exists($className) ? $className : $this->defaultException;\nIt looks like the error is being thrown in spl_autoload_call. Do you have an autoloader registered that throws exceptions or triggers errors? \nMany exceptions are not modeled, so even if this particular class is added, you will likely see this error again if its underlying cause is not addressed.\n. Cool. I'll close this PR for now, as the issue probably won't be solved by adding a new exception class. Please feel free to reopen if you have any questions or concerns.\n. /cc @mtdowling @xibz \n. Nice! :boat: \nI don't think I can make a similar fix for v2, since validation is exclusively handled by Guzzle prior to 3.0.\n. The result of GetItem contains additional metadata on how much DynamoDB capacity was consumed by the operation. This information is not available on values yielded by getIterator, nor would it be available on individual items returned from a BatchGetItems request. \nThe value at $result['Item'], however, is an associative array, so you could pass that to the same mapping mechanism as you would an item yielded from an scan iterator.\n. I played around with a few ways to get an item in DynamoDB, and, based on the amount of metadata returned by different methods of getting the same data, I believe it's fair to say that we do have a consistent result type: an associative array. \nWhere that's located on a response will depend on which operation is called, but $ddb->getItem($params)['Item'] is identical to $ddb->batchGetItem($params)['Responses'][$tableName][0].\nHope that helps! Please feel free to reopen if you have any questions or concerns.\n. Unfortunately, Travis only tests against HHVM 3.6.6 at the moment. The latest failure of the current version of the SDK (V3) was caused by the yield syntax incompatibility. Based on what was mentioned in the HHVM issue, I expect v3 would work on HHVM 3.11, but I don't have a good way to automatically test all releases and pull requests until Travis upgrades their HHVM runtime.\nThe V2 failure linked to appears to be caused by an HHVM incompatibility in PHPUnit. I haven't investigated what's causing it, but it might also be fixed in 3.11.\n. I ran a test (#902) to see if the SDK was compatible with HHVM >=3.11. It appears that while the HHVM team fixed the syntax error that was thrown when yield was placed inside parentheses, yielding into generators still does not work the same in PHP and HHVM.\n. I opened a new issue on HHVM, as there is still an incompatibility with the way generators work between HHVM 3.11 and Zend PHP.\n. @mtdowling Thanks for the feedback! \nI updated this PR to be a slightly more expansive change. The base multi-region client just implements AwsClientInterface, and method implementations that are shared between a standard AwsClient and the new multi-region version have been moved to an AwsClientTrait that is used by both. Similarly, there's now an S3ClientInterface implemented by the standard S3Client and its multi-region counterpart, with code being shared between the two with a trait. S3 helper classes like the stream wrapper and the multipart uploader have been updated to take an instance of S3ClientInterface instead of an instance of S3Client.\n. @mtdowling I added support for multiregion clients to the Sdk class and added some documentation.\n. Converting HTTP responses into exceptions is something that's normally handled by a Guzzle client. Rather than passing your handler stack directly to an SDK client as its http_handler, you could use it to create a Guzzle client:\n``` php\n$httpHandler = new \\GuzzleHttp\\Handler\\CurlMultiHandler();\n$eventLoop->addPeriodicTimer(0, [$httpHandler, 'tick']);\nif (function_exists('curl_exec')) {\n    $httpHandler = \\GuzzleHttp\\Handler\\Proxy::wrapSync($httpHandler, new \\GuzzleHttp\\Handler\\CurlHandler());\n}\n$guzzleClient = new \\GuzzleHttp\\Client([\n    'handler' => \\GuzzleHttp\\HandlerStack::create($httpHandler),\n]);\n```\nWith that Guzzle client, you can create an HTTP handler that converts Guzzle exceptions to a standardized form expected by SDK middleware:\n``` php\n$handler = new \\Aws\\Handler\\GuzzleV6\\GuzzleHandler($guzzleClient);\n$client = new DynamoDbClient([\n    'region' => 'eu-west-1',\n    'profile' => 'default',\n    'version' => 'latest',\n    'http_handler' => $handler,\n]);\n```\nHope that helps! Please feel free to reopen if you have any questions or concerns.\n. @jeremeamia Yeah, that's a lot of failures. It looks like they all trace back to a usage of Promise\\coroutine. HHVM 3.11 may have fixed the syntax bug, but their generator semantics are clearly still different from PHP.\n. Instances of CachingStream support seeking beyond what has already been read. Are you seeing the same error (caused by getimagesize calling fseek) as described in #192?\n. The fix for #192 has been part of v3 since it launched, as described in #587. Can you describe the error you're seeing and the steps needed to correct it? That's not clear from the test included in this PR.\n. Thanks for the thorough investigation! From the testing I've done, this issue appears to only affect certain JPEG images. Since it's caused by inconsistent fseek behavior, I would prefer to fix it in Guzzle or the SDK rather than forcing affected users to fix it themselves. \nThat said, I'm not opposed to allowing some customizability on how caching stream wrappers are instantiated, but I think it would make more sense to pass in a callable that takes a stream as an argument and returns a seekable stream. Just passing in a class name forces all custom caching streams to support calling the constructor with a single positional argument.\n. @andythorne Is this still necessary with the upstream change to Guzzle?\n. This would break any code that extends the session handler and has a different, unrelated method named 'formatId.'\nWould it make sense in your application to modify the session key directly? You can do so by calling session_id with the new ID or by registering a session handler with a defined create_sid handler.\n. Another option would be to provide a custom SessionConnectionInterface when creating the SessionHandler. You could extend one of the existing implementations and overwrite the formatKey method, which is not private.\n. I'll reach out to the beanstalk team to see what their upgrade plans are. In the meantime, you can force a beanstalk instance to upgrade composer with a .ebextensions/.config file, as described in this blog post.\n. I sent a message to the Beanstalk team, but since the COMPOSER_AUTH support looks brand new, running a composer self-update through a .ebextensions command is your best bet.\nPlease feel free to reopen if you have any questions or concerns.\n. The AmazonS3 drupal module uses a fork of the SDK, and the error reported is being triggered by a customization not present in the official SDK release. You will need to open this issue on the fork.\n. @deviantintegral Thanks for tracking that down. I was just going by the line numbers, which are not the same between the two versions.\nAccording to php.net, the wrapper_data returned by stream_get_meta_data is meant to be specific to a stream implementation. If users want to override the metadata returned for the S3 stream wrapper, then the wrapper data should be compatible -- either an array or a class implementing ArrayAccess.\n. V3 clients require that you specify a version, so I don't think the right class is loading in your application. The Publish command is called as part of the SDK's integration tests, so I can confirm that it works with an SnsClient instantiated with new.\n. There's no flag to do what you're describing automatically, but the $source constructor parameter can be either a string or an Iterator. If you've enabled the S3 stream wrapper, you could do something like the following to sync files from a bucket to a local folder:\n``` php\n// create an Iterator that will yield everything in the bucket\n$iterator = $s3->getPaginator('ListObjects', ['Bucket' => $bucketName])\n    ->search('Contents[].Key');\n// map the iterator to yield absolute paths instead of just the key\n$iterator = \\Aws\\map($iterator, function ($key) use ($bucketName) {\n    return \"s3://$bucketName/$key\";\n});\n// Apply a filter to exclude files that do not be downloaded according to custom criteria\n$iterator = \\Aws\\filter($iterator, function ($s3Path) use ($bucketName, $localFolder) {\n    // map the file in the bucket to a file in a local folder\n    $localPath = $localFolder . str_replace(\"s3://$bucketName\", '', $s3Path);\nif (file_exists($localPath) && filesize($localPath) === filesize($s3Path)) {\n    // this file exists locally and both the remote and local files have the same\n    // size, so exclude it from the download iterator\n    return false;\n}\n\n// this file did not exist locally or the local and remote files were of a \n// different size, so include it in the download iterator\nreturn true;\n\n});\n// Create the download manager with the iterator\n$downloadManager = new \\Aws\\S3\\Transfer($client, $iterator, $localFolder, [\n    'base_dir' => \"s3://$bucketName\",\n]);\n// execute the transfer\n$downloadManager->transfer();\n```\nIf you've stored a file MD5 as metadata on the bucket, you could check that instead of the file's size and path. Constructing an iterator allows for the flexibility to define whatever exclusion criteria are most appropriate for your data.\n. Are you using \\Aws\\recursive_dir_iterator? \nThat function creates a generator that sends asynchronous HTTP requests as you iterate over it, which means that it cannot be resumed after sending a blocking request (which happens when filesize($s3Path) is called). Try something like the following to avoid that error:\n``` php\n// Get the name and size of every object in a bucket\n// This pages over your bucket and returns an\n$iterator = $client->getPaginator('ListObjects', ['Bucket' => $bucketName])\n    ->search('Contents[].{Key: Key, Size: Size}');\n// Apply a filter to exclude files that do not be downloaded according to custom criteria\n$iterator = \\Aws\\filter($iterator, function (array $s3Data) use ($bucketName, $localFolder) {\n    // map the file in the bucket to a file in a local folder\n    $localPath = \"$localFolder/{$s3Data['Key']}\";\nif (substr($s3Data['Key'], -1, 1) === '/') {\n    // this is a directory marker\n    return false;\n}\n\nif (file_exists($localPath) && filesize($localPath) === $s3Data['Size']) {\n    // this file exists locally and both the remote and local files have the same\n    // size, so exclude it from the download iterator\n    return false;\n}\n\n// this file did not exist locally or the local and remote files were of a\n// different size, so include it in the download iterator\nreturn true;\n\n});\n// map the iterator to yield absolute paths instead of just the key\n$iterator = \\Aws\\map($iterator, function (array $s3Data) use ($bucketName) {\n    return \"s3://$bucketName/{$s3Data['Key']}\";\n});\n// Create the download manager with the iterator\n$downloadManager = new \\Aws\\S3\\Transfer($client, $iterator, $localFolder, [\n    'base_dir' => \"s3://$bucketName\",\n]);\n// execute the transfer\n$downloadManager->transfer();\n```\nUsing a search over a paginator will also reduce the number of HTTP requests made.\n. The S3 client will retry failures up to three times each, and errors will only be surfaced once all retries have been exhausted. You can increase the number of by setting the 'retries' configuration parameter when creating an S3 client.\nAws\\Exception\\MultipartUploadException extends \\RuntimeException, so you would need to catch that separately from any S3Exceptions or AwsExceptions.\n. Closing, as catching Aws\\Exception\\MulitpartUploadException would stop the program from exiting with an uncaught exception. Please feel free to reopen if you have any questions or concerns.\n. Are you encountering a different exception? The one mentioned in the description of this issue is an Aws\\Exception\\MultipartUploadException.\n. The SDK does not use separate threads, as userland multithreading is not available on most PHP installations.\n. Are you sure you've set up a catch block typed against \\Aws\\Exception\\MultipartUploadException and that an instance of \\Aws\\Exception\\MultipartUploadException is slipping through that catch block?\nOne possible source of confusion here is that exceptions thrown in a catch block will not be caught by subsequent catch blocks in the same try/catch/finally statement. For example:\nphp\ntry {\n    throw new Exception;\n} catch (Exception $e) {\n    throw new RuntimeException(\"Can't catch me!\");\n} catch (RuntimeException $e) {\n    echo \"Caught it!\\n\";\n}\nwill terminate with Fatal error: Uncaught exception 'RuntimeException' with message 'Can't catch me!'\n. I am unable to reproduce. This script:\n``` php\n<?php\nrequire 'vendor/autoload.php';\nuse Aws\\Exception\\MultipartUploadException;\nuse Aws\\Multipart\\UploadState;\ntry {\n    throw new MultipartUploadException(new UploadState([]));\n} catch (MultipartUploadException $e) {\n    echo \"Caught it!\\n\";\n}\n```\noutputs \"Caught it!\" as expected. If I had to guess, I would say that your issue is that the file containing the code you shared above is not importing the MultipartUploadException into its namespace with a use Aws\\Exception\\MultipartUploadException; statement.\n. Reporting the number of retries and supporting an on_retry callable aren't mutually exclusive. and the former seems more obviously useful. If there's a use case that isn't solvable by inspecting the number of retries reported, we can revisit on_retry in the future.\n. @Sazpaimon The SDK randomizes the time between retries, so I'm not sure how useful that data point would be.\n. @mtdowling I added more documentation, renamed __on_transfer_stats to http_stats_receiver, and set up the WrappedHttpHandler to throw an exception if a user supplied a value as an http option.\n. When Guzzle's decode_content option is set to true, the content-encoding header will be removed from the response, as will the content-length header if the decoded content (i.e., the gunzipped response body) has a length of 0. decode_content isn't really applicable on a HEAD request, so I'll look into fixing this in Guzzle.\nIf you need an immediate workaround, you can manually set decode_content to false in the http options passed in a part of your client config:\nphp\n$s3 = new Aws\\S3\\S3Client([\n    'http' => [\n        'decode_content' => false,\n    ],\n]);\nYou could also set this just on the HeadObject command if you are normally relying on the SDK to gunzip objects:\nphp\n$headers = $s3->headObject(array(\n    'Bucket' => '***',\n    'Key' => 'test.css',\n    '@http' => ['decode_content' => false],\n));\n. Glad everything worked out. Please feel free to reopen this issue if you have any questions or concerns.\n. There is code to support uploading via post in Aws\\S3\\PostObject, but we don't have any examples in our documentation. I'll work on getting something written up.\n. This behavior is not a bug but intentional client-side validation. According to the DynamoDB documentation, items with empty string attributes will be rejected as invalid by the service.\n. Unfortunately, DynamoDB does not allow empty strings at any level of a document. \nWhen trying to run a PutItem command with a map containing an empty string, for example:\nphp\n$ddb->putItem([\n    ...\n    'Item' => [\n        ....\n        'arbitraryData' => ['M' => ['Key' => ['S' => '']]],\n    ]\n]);\nan error will be returned from the service. I ran the code above and got the following error: Error executing \"PutItem\" on \"https://dynamodb.us-east-1.amazonaws.com\"; AWS HTTP error: Client error response [url] https://dynamodb.us-east-1.amazonaws.com [status code] 400 [reason phrase] Bad Request ValidationException (client): One or more parameter values were invalid: An AttributeValue may not contain an empty string -\nIf you need to save an empty value, I believe your best option would be to use null instead of an empty string. DynamoDB does support saving fields with a type of null.\n. Do you have garbage collection enabled? \nRunning the following script, I recorded memory usage cycling from 5 MB to 17 MB and then back again after garbage collection completed:\n``` php\n$s3 = new \\Aws\\S3\\S3Client([\n    'region' => 'us-west-2',\n    'version' => 'latest',\n]);\n// Make sure the bucket exists\nif (!$s3->doesBucketExist($bucket)) {\n    $s3->createBucket(['Bucket' => $bucket]);\n    $s3->waitUntil('BucketExists', ['Bucket' => $bucket]);\n}\n// Create a 3 MB file\n$tempFile = tempnam(sys_get_temp_dir(), time());\nfile_put_contents($tempFile, str_repeat('x', 3 * 1024 * 1024));\nfor ($i = 0; $i < 15000; $i++) {\n    // Put the file 15,000 times\n    $s3->putObject([\n        'Bucket' => $bucket,\n        'Key' => \"{$i}.ext\",\n        'Body' => fopen($tempFile, 'r'),\n    ]);\n// Report memory usage following each upload\necho str_pad($i, 7, ' ', STR_PAD_LEFT) . ': '\n    . number_format(memory_get_usage())\n    . \"\\n\";\n\n}\n```\nWith garbage collection disabled, memory usage will slowly climb until it reaches the configured limit and a fatal out-of-memory error is thrown.\n. Please feel free to reopen if enabling garbage collection does not resolve the memory issue for you.\n. The AmazonSNS class is part of version 1 of the AWS SDK for PHP, and this repository houses versions 2 & 3. You can find version 1 over at https://github.com/amazonwebservices/aws-sdk-for-php.\n. You can inspect the previous exception bound to a MultipartUploadException, which in the case you're describing would be an instance of Aws\\S3\\Exception\\S3Exception with a message of Error executing \"CreateMultipartUpload\" on \"https://s3-<region>.amazonaws.com/<bucket>/<key>?uploads\"; AWS HTTP error: Client error response [url] https://s3-<region>.amazonaws.com/<bucket>/<key>?uploads [status code] 404 [reason phrase] Not Found NoSuchBucket (client): The specified bucket does not exist.\nI think it would reasonable to suffix the previous exception message to the MultipartUploadException message, which would then be verbose but helpful.\n. It looks like the connection is being reset while OpenSSL is attempting to decrypt data, which can be caused by the client and server being unable to agree on a TLS protocol. Which version of OpenSSL are you using?\nOpenSSL emits error 104 (ECONNRESET) when the connection is reset by a peer host, and this error is happening while cURL is attempting to read data. cURL is therefore surfacing this as a read error (cURL error 56 - CURLE_RECV_ERROR). \nI'll look into why the SDK's behavior in response to this event would differ from v2 to v3.\n. Is the CA bundle at /tmp/cacert.pem the bundle provided on the curl website?\n. v2 of the SDK uses Guzzle 3, which includes its own certificate authority bundle. v3 uses Guzzle 5/6, which rely on an external bundle.\nCan you set the openssl.cafile ini setting and then check what is output by running openssl_get_cert_locations()? (This function is only defined in PHP >= 5.6.) Since running curl on the command line fails unless you specify a CAFile option, this will need to be set in PHP.\n. Is it possible that the processes that terminate following an OpenSSL error are either setting the SSL_CERT_FILE environment variable or unsetting the openssl.cafile ini variable? Clearly OpenSSL/cURL is loading this ini value (the equivalent of the command-line CAfile option) normally, but as you mentioned, the error you're seeing is occurring in a random minority of cases. \nv2 of the SDK uses Guzzle 3, which will use a vendored CA bundle by default, whereas v3 (via Guzzle 5/6) will use PHP, OS, and OpenSSL configuration as its default. You can override this on a client by specifying the verify HTTP option. For example:\nphp\n$dynamoClient = new \\Aws\\DynamoDb\\DynamoDbClient([\n    'version' => 'latest',\n    'region' => 'us-west-2',\n    'http' => [\n        'verify' => '/tmp/cacert.pem',\n    ],\n]);\n. One last question: are you connecting to AWS through any kind of network proxy? A similar error message was reported on the Docker repository: https://github.com/docker/docker/issues/2011\nThey mention that this can happen to traffic from a Docker container, and one comment mentions that they saw a similar error when using a VPN.\n. Since neither specifying an openssl.cafile ini variable nor a verify http option worked, the error is not related to the CA bundle. One other change from Guzzle 3 to Guzzle 6 is that Guzzle now reuses cURL handles between requests. There's an open pull request on OpenSSL that would remedy a failure case where you connect to a pool of hosts (the fleet addressable as swf.us-west-2.amazonaws.com, for example) and negotiate an SSL session, save that session, and then use the session to connect to a different host in the same pool. If the second host only supports TLS 1.0 and the first host supports TLS 1.2, the session will not downgrade but will instead error out. There was a lot of discussion on the JavaScript SDK about how this issue was affecting DynamoDB customers.\nCould you try forcing TLS 1.0? In v3, you would do so like this:\nphp\n$ddb = new \\Aws\\DynamoDb\\DynamoDbClient([\n    ...\n    'http' => [\n        'curl' => [\n            CURLOPT_SSLVERSION => CURL_SSLVERSION_TLSv1_0,\n        ]\n    ]\n]);\nOpenSSL is working on a fix but it has not yet been accepted and released.\n. curl.options will only be used by a v2 client. Did you get the same cURL/OpenSSL error codes (cURL error 56: SSL read: error:00000000:lib(0):func(0):reason(0), errno 104)?\n. The root cause might be something else that's supported by one server and not another. I'm still unable to reproduce the issue, so if you can capture any more context about a failing request that would be very helpful.\nIn the meantime, could you verify that this is related to sharing cURL handles? You can disable handle sharing by creating a custom Guzzle client like so:\n``` php\nuse Aws\\Handler\\GuzzleV6\\GuzzleHandler;\nuse Aws\\Swf\\SwfClient;\nuse GuzzleHttp\\Client;\nuse GuzzleHttp\\Handler\\CurlFactory;\nuse GuzzleHttp\\Handler\\CurlHandler;\nuse GuzzleHttp\\Handler\\CurlMultiHandler;\nuse GuzzleHttp\\Handler\\Proxy;\nuse GuzzleHttp\\HandlerStack;\n// Create a Guzzle client that will not share cURL handles\n$guzzleClient = new Client([\n    'handler' => HandlerStack::create(Proxy::wrapSync(\n        new CurlMultiHandler(['handle_factory' => new CurlFactory(0)]),\n        new CurlHandler(['handle_factory' => new CurlFactory(0)])\n    ))\n]);\n// Use the no-shared handle client to create an AWS client\n$swfClient = new SwfClient([\n    'region' => 'us-east-1',\n    'version' => 'latest',\n    'http_handler' => new GuzzleHandler($guzzleClient),\n]);\n``\n. @ppaulis Just wanted to give you an update. I've engaged the SWF team on this and will keep you posted.\n. @serkanozcelik and @ParadoxNL cURL error 35 is retried by the SDK. If it's being surfaced in your application, all retries must have been exhausted.\n. I would need to look into what would happen if the affected field was part of an index. You wouldn't technically be saving a string field with a null value but rather a null field with no value. Instead of setting$valuetonull, you would instead want to return['NULL' => true]`.\n. Simply casting empty strings to a null value wouldn't work in all cases. If the key containing an empty string is part of an index, then changing its type will result in a server-side validation exception based on the type mismatch. Dynamo strictly enforces the type of values used in indices. Secondly, this would be problematic for string sets, which can only contain strongly-typed string attributes, not null values. \nThis change would allow users to save empty strings, but only if they weren't part of a set or defined in the table's (or any index's) key schema. It would also persist values that couldn't be effectively round-tripped: when unmarshalling a null value, should it be converted to null or to an empty string? Encountering a null where a string was expected might break code that relies on an implicit schema, especially if the same table is being used by multiple applications written in different languages.\n. I took a look at how the :nil_as_empty_string feature is implemented in aws-sdk-ruby-record, and I don't think it's appropriate for a generic marshaller/unmarshaller. In a data mapper library, a marshaller would have enough context to know if casting an empty string to null is a safe action -- it could use the defined metadata to figure out whether the string is an attribute or part of a set and whether the key was used in an index -- and more importantly it could figure out when unmarshalling whether a given null field should be unmarshalled to null or to an empty string. \nSince the marshaller in the PHP SDK works on fields and set members, not full records, I don't think it could be rewritten to support an 'emptyStringAsNull' option without significantly changing the class's interface. If you know that your use case is safely only saving empty strings as non-indexed attributes, you can perform this transformation on the data provided to the marshaller:\nphp\n$data = array_walk_recursive($data, function (&$leaf) {\n    if ('' === $leaf) {\n        $leaf = null;\n    }\n});\n. Individual files can be downloaded using the GetObject command or uploaded using the upload method on S3Client. You could also supply a single-item iterator to the transfer manager, but GetObject and upload would be more efficient.\n. Sorry; I missed the word 'async' in your question. You can call $s3Client->getObjectAsync and $s3Client->uploadAsync to perform the asynchronous equivalent of GetObject and upload, respectively.\n. Client methods that are suffixed with 'Async' are asynchronous versions of a command. GetObjectAsync takes the same arguments as GetObject and returns a promise that will be fulfilled with the result documented for GetObject or rejected with an AwsException. More general documentation on async methods in the SDK can be found here.\nAs to the progress question: are you using the default (Guzzle 6) HTTP handler? If so, you can take advantage of the progress request option, which takes a callable that will be invoked as a request progresses. You could pass that along to the HTTP handler in an operation with the http SDK configuration parameter:\nphp\n$client = new S3Client([\n    'region' => 'us-west-2',\n    'version' => 'latest',\n    'http' => [\n        'progress' => function (\n            $bytesToDownload,\n            $bytesDownloadedSoFar,\n            $bytesToUpload,\n            $bytesUploadedSoFar\n        ) {\n            // act on this progress information\n        }\n    ],\n]);\n. You can use the before callback to modify commands before they are dispatched by the transfer manager. You could bind a progress callback to each command like so:\nphp\n$lastDatetime = time();\nreturn new \\Aws\\S3\\Transfer($this->client, 's3://mybucket/path/to/folder/',\n    'c:/path/to/local/', array(\n        'base_dir' => '/path/to/folder/',\n        'before' => function (CommandInterface $command) use (&$lastDatetime){\n            $command['@http']['progress'] = function (\n                $bytesToDownload,\n                $bytesDownloadedSoFar,\n                $bytesToUpload,\n                $bytesUploadedSoFar\n            ) {\n                // do something on progress\n            };\n            $currentDatetime = time();\n            if($currentDatetime - $lastDatetime >= 30){\n                // Do something every 30 seconds during the download\n                $lastDatetime = $currentDatetime;\n            }\n        }\n    )\n);\n. progress callables on a command report on the progress of individual files, and before callables on a transfer manager are invoked before each command is dispatched. There's no transfer-manager level progress callable, as that would require walking the source and stating each object before beginning the transfer. You could use the before callback to update a running total of bytes to transfer and then refer to that data in the associated progress callback, but there would be a performance cost to doing so.\n. Not at this time. New services are being added to the most recent version of the SDK, and v2 is continuing to receive bug fixes as necessary.\n. I'm not sure I would classify this as \"a client library going rogue.\" Guzzle takes ownership of- and uses a RAII abstraction over streams because they are not easily sharable. An HTTP client will necessarily modify a resource handle in potentially destructive ways; if a handle is used a part of a request, its internal cursor will be moved to the end of the resource. This would render a non-seekable stream useless and leave a seekable stream in a different state.\nYou could prevent the file from being closed by wrapping it in a GuzzleHttp\\Psr7\\Stream before handing it to a PutObject command. If the input to GuzzleHttp\\Psr7\\stream_for is an instance of Psr\\Http\\Message\\StreamInterface, it will be returned unmodified. This will let you keep the stream in scope and ineligible for garbage collection as long as you'd like, though you will need to handle cursor juggling on your own.\n. Checking if $array[0] is set is a quick heuristic to determine if a given array is being used as a list or a dictionary. More comprehensive checks typically involve copying all or part of an array, which is something the SDK tries to avoid. A popular StackOverflow question has suggestions to check if array_keys($array) === range(0, count($array) - 1) or even simply if array_values($array) === $array, both of which are also just heuristics, as array modifiers like array_filter will generally not modify keys, meaning that the result of an expression like array_filter(range(1,10), function ($v) { return $v % 2 === 0; }) will appear to all the above methods to be an associative array, even though it's just a filtered range.\nEven though there's no perfect solution, we do need to know in certain cases if an array is acting as a list or a map, as those tend to take different forms when serialized and sent to a service. isset($array[0]) is the most efficient check that works for the data the SDK is expecting. This PR only checks if is_array($array) (I couldn't find a test case where isset($array[null]) returned true), which means that the modified checkAssociativeArray method would cease distinguishing between lists and dictionaries.\n. Are you providing an SMTP password to the SES client?\n. What character encoding was used to create the path string? The x-amz-copy-source header uses rawurlencode to convert characters in an object key to URL-safe characters, and it's being rendered in the request as Beyonce??IMG_18022016_111346.png.\n. Can you check if 'Beyonce%CC%81IMG_18022016_111346.png' === rawurlencode(urldecode('Beyonce%CC%81IMG_18022016_111346.png')) on the affected system? I'm unable to reproduce locally, but that may be because I have the iconv and mbstring extensions installed.\n. @lonormaly Were you able to test out the affected environment using the code in my previous comment?\n. I'm going to go ahead and close this issue, as it appears to be related to the system running PHP and not to the SDK. Please feel free to reopen if you have any questions or concerns.\n. This still looks like an environment-specific error. Were you able to check your system as described above? Does running 'Beyonce%CC%81IMG_18022016_111346.png' === rawurlencode(urldecode('Beyonce%CC%81IMG_18022016_111346.png')) return true?\n. What output do you get from running php -r \"var_dump(get_loaded_extensions());\"'?\n. I'm still unable to reproduce. Copying a file with the exact name provided works fine for me. Is there a particular version of PHP you're seeing this error on?\n. Sorry, what I meant to ask was: which version of PHP are you using on the system where the error is appearing? I am unable to reproduce on PHP 5.6.20 or PHP 7.0.5.\n. I'm unable to reproduce on PHP 5.5, either. The root cause of the issue is somewhere in how PHP is configured on your system, which is causing the character encoding of the string provided to not be understood by rawurlencode. That is a native PHP function, so there really isn't anything the SDK can do about it.\n. Ah, I see that you're directly using the multipart copier. You will need to URL encode the source key. This is something that Aws\\S3\\S3Client::copy or an instance of Aws\\S3\\ObjectCopier would take care of for you.\nThe multipart copier and uploader take the source as a string and rely on the user (or a higher level of abstraction in the SDK) to take care of encoding. Altering this interface would be a breaking change.\n. The portion of the $tempSource variable that corresponds to the S3 key of the original source should URL encoded using rawurlencode. If you would prefer to have this done on your behalf, you could use Aws\\S3\\S3Client::copy.\n. The source needs to be of the form <bucket>/<url-encoded key>. From the error you posted, it appears that you are URL encoding the entire parameter.\ncopy inspects the source and determines whether to use a single or multipart copy. It also determines what part size to use.\n. copy takes care of URL encoding. You should not encode the key if you're using the copy method, only if you're directly creating a multipart copy object.\n. MultipartCopy is meant to be a low-level abstraction over the different requests that make up a multipart copy. It assumes that the copy source passed to its constructor is the one you mean to send to S3. If the MultipartCopy constructor suddenly started URL encoding portions of the source path, then any code passing in a properly encoded path would break. For example, the string 'Beyonce%CC%81IMG_18022016_111346.png' would be encoded as 'Beyonce%25CC%2581IMG_18022016_111346.png,' as the percent sign is a character that needs to be URL encoded.\nIf you're getting the path in the form of <bucket>/<key>, then you can encode the key portion of the path in this manner:\nphp\nlist($bucket, $key) = explode('/', $path, 2);\n$path = $bucket . '/' . rawurlencode($key);\nIf the path also contains a version ID, you would also need to separate the query string from the key.\ncopy accepts the key, bucket, and version ID as separate parameters, which is why it's able to encode everything on your behalf. You can still catch a multipart exception when using copy, just as you were doing with the low-level MultipartCopy object:\nphp\ntry {\n     $response = $s3->copy($sourceBucket, $sourceKey, $destinationBucket, $destinationKey, $acl);\n} catch (\\Aws\\S3\\Exception\\S3Exception $e) {\n    // This exception is thrown if a single-part copy fails\n} catch (\\Aws\\Exception\\MultipartUploadException $e) {\n    // This exception is thrown if a multipart copy fails\n    $response = $s3->copy($sourceBucket, $sourceKey, $destinationBucket, $destinationKey, $acl, [\n        'state' => $e->getState(),\n    ]);\n}\n. :sheep: :it:\n. There are a few things the SDK is doing on any given request that are CPU intensive. Amazon Kinesis Firehose is only accessible via HTTPS endpoints, so each PutRecord call performs communication over TLS. Additionally, each request is signed (i.e., converted to a canonical form, the SHA-256 HMAC of which is used as the Authorization HTTP header). You could run a profile to see exactly where CPU time is being spent, but TLS and request signing are the most likely culprits.\nThe Redis protocol, however, is a simple TCP-based protocol that is designed to be cheap to parse and serialize. Redis' security model is based on restricted network access rather than encryption or signatures, so by using Elasticache inside a VPC you avoid the CPU overhead of TLS communication, HTTP message serialization, and HTTP message signing. You should expect Redis to take comparatively little CPU.\nFor Firehose, have you looked into using PutRecordBatch to send multiple updates at once? That would probably be a more efficient approach.\n. I think you should see some improvements by moving to batch puts. Please feel free to reopen if you have any questions or concerns.\n. @dpgover Are you using the stream wrapper to write log messages to an object in S3? This may be a bug in the Guzzle V5 handler, but there's isn't enough information to determine if it is one (and if so how to reproduce and fix it).\n. I think the root cause of the error is the first warning in the message Invalid callback Monolog\\Handler\\StreamHandler::customErrorHandler, cannot access private method Monolog\\Handler\\StreamHandler::customErrorHandler().\nAre you at any point explicitly rejecting any promises? This might also be done by a third-party library.\n. I'm going to go ahead and close this issue, as it does not appear to be a bug in the SDK. Please feel free to update or reopen if you get more information or if you have any questions or concerns.\n. Thanks!\n. @Poojal123 The address supplied is being rejected with an InvalidParameterValue exception. Have you verified that the address of the recipient is valid?\n. The \"Illegal address\" error can refer to the recipient's address, as described in this AWS forum thread.\n. @Poojal123 Your IAM user does not have permission to use the VerifyEmailIdentity method. These questions might be better suited to the SES forum, as the SDK is just surfacing errors from SES.\n. Closing, as these are not issues with the SDK but instead errors returned by SES and surfaced by the SDK. Please feel free to reopen if you have any questions or concerns.\n. Thanks!\n. Looking further into this issue, it appears that copy will return false if there is an issue when reading from S3. When writing to S3 via the stream wrapper, copy calls url_stat, stream_open, stream_write as many times as is necessary to write all data read from the copy source, and then stream_flush when all bytes have been written. copy will only return false if either stream_open or stream_write does. Since the StreamWrapper does not actually write any data to S3 until stream_flush, copy will never return false due to a non-existent bucket or a permissions issue. According to a PHP bug report, this is a known issue with copy and should be considered the expected behavior.\nThe stream wrapper could check that the bucket exists in stream_open and maybe even attempt to write an empty file to verify that the wrapper's client has sufficient permissions to write to the requested location, but that would degrade performance and introduce a potential race condition if a bucket existed at the start of a copy operation and was subsequently deleted. copy would still return true in that case even though the final PutObject command would have failed. \nGiven the odd semantics of copy, I'm not sure this issue can be fixed in a satisfactory way. I'm inclined to just remove it from the list of supported methods in the documentation, as it only mostly works. @mtdowling, any thoughts?\n. The example in the docs is intended as more of a starting point than a complete implementation, as you would need to implement some form of error handling for a full solution. Since Amazon Elasticsearch Service launched and that example was written, more Elasticsearch tooling has added support for AWS Authentication. There's official support in Elastica >= 3.1.1, and a plugin for Elasticsearch-PHP that decorates the standard handler can be found here.\n. @enVolt -- One potential cause would be where credentials are being loaded. By default, the SDK will look for credentials in a number of places, including an .ini file at ~/.aws/credentials. It's possible that the system user under which you run php -a has such a file in its home directory, while the system user under which Apache is running does not. Apache typically runs as www-data on Ubuntu and Debian systems, though this can vary.\n. Closing, as the fact that the issue does not occur when using php -a indicates that this is not an issue with the SDK. Please feel free to reopen if you have any questions or concerns.\n. We discussed the backwards compatibility concerns of this change in person (i.e., that the same policy cannot be applied to a V4 form without modification). Let me know when this is updated and ready for another review.\n. Hi @harpreetsb,\nIf you use putObjectAsync instead of putObject, $result will be a promise that will be fulfilled with the result of the PutObject command or rejected with an exception. Promises have a cancel method, but whether calling cancel has any effect would depend on a few factors, including the state of the promise at the time of the call (only pending promises can be cancelled).\nCalling cancel will not delete the object from a bucket if the upload is already complete, so you may need to complete clean up steps beyond a simple cancellation to fully abort an upload.\n. Changing the signed header from 'Host' to 'host' appears to be what other AWS SDKs do. I opened a PR to address this.\n. @mtdowling I believe I covered what we discussed, and this PR is ready for another review. \n. @mtdowling I didn't add a test for each service in each region, but the trickier cases were covered by Aws\\Test\\PatternEndpointProviderTest::testResolvesEndpoints. I moved those cases to the PartitionEnpointProviderTest.\n. This PR has been updated with the latest version of endpoints.json. In addition, I've removed the mapRegions method to keep this change focussed.\n/cc @mtdowling @xibz @cjyclaire \n. @mtdowling I see your \ud83d\udea2 and raise you a pile of documentation and tests.\n. I'm confused as to what the issue is. S3 requests should always include a checksum for data integrity; for presigned URLs, you can use the string 'UNSIGNED PAYLOAD' instead to allow any or no payload. This is in line with the docs you quote and matches the behavior of S3.\n. It looks like requests are being signed and then presigned, which is where that extra X-Amz-Content-Sha256 is coming from. S3 accepts the request either way, so I will need to investigate whether this can be fixed without affecting other presigned requests.\n. I'm unable to reproduce using V2. Memory usage holds steady while executing the script you provided. I'm using PHP 5.6.\nCan you check to make sure garbage collection is enabled? If gc_enabled() returns true and you still see memory usage climbing, there might be differences between how garbage collection is handled in PHP versions. I see that you're using PHP 5.3, which has not been officially supported for some time.\n. Running the same script but replacing the last two lines in the loop with echo memory_get_usage() . \"\\n\";, I see that memory usage holds at a steady 5,160,840 bytes used for the duration of the script:\n``` php\n$strBucket = 's3eskew';\n$strKey = 'keykey';\n$client = Aws\\S3\\S3Client::factory([\n    'region' => 'us-west-2'\n]);\n$client->putObject(['Bucket' => $strBucket, 'Key' => $strKey, 'Body' => 'abc123']);\nfor ($i = 0; $i < 10000; $i++) {\n    $arrArgs = [\n        'Bucket' => $strBucket,\n        'Key' => $strKey\n    ];\n    $arrHeader = $client->getObject($arrArgs);\n    echo memory_get_usage() . \"\\n\";\n}\n```\nSwitching to use hard-coded credentials had no effect on the outcome; memory usage held at a constant  5,159,272 bytes when not using environmental credentials. The memory leak you've detected must be somewhere else in your application.\n. I'm going to go ahead and close this issue, as the memory leak is not reproducible from the posted script, but feel free to reply with questions. We can continue this discussion here even if the issue is closed.\n. memory_get_usage, when called with no parameters, will always tell you how much memory a PHP script is currently using. This is not necessarily the same as how much memory has been allocated by the PHP engine, as PHP may hold onto memory that belonged to freed objects so that it can be allocated to newly created objects. Reassigning memory is cheaper than calling malloc, so you may be seeing a PHP engine optimization doing something unexpected. Without knowing any specifics about your system, I would recommend auditing which PHP extensions are installed and checking if any of those have known memory leaks.\nIf there were an application-level memory leak, you would almost certainly see it in memory_get_usage, so application and library code is unlikely to be the culprit. Are you using Elastic Beanstalk, by the way? I saw a forum issue that might be related.\n. The fix from #580 has been merged into the 2.8 branch and will go out with the next release. Thanks for reporting this!\n. @zamaliphe I'm unable to reproduce. doesObjectExist internally catches all S3Exceptions and only propagates 500-level errors.\nFrom the code snippet you posted, it doesn't look like the exception is related to the doesObjectExist method call. $s3->doesObjectExist('dev.nkforex.com', 'zam.jpg' ) would call HeadObject on 'zam.jpg', while the exception is encountered when calling HeadObject on an object named 'files/articles/image/40500/%25D8%25A7%25D9%2584%25D8%25AA%25D8%25AD%25D9%2584%25D9%258A%25D9%2584-%25D8%25A7%25D9%2584%25D9%2581%25D9%2586%25D9%2589-%25D8%25B3%25D9%2587%25D9%2585-%25D8%25B3%25D9%2584%25D8%25A7%25D9%2585%25D8%25A9-%25D8%25B3%25D9%2588%25D9%2582-%25D8%25A7%25D9%2584%25D8%25A7%25D8%25B3%25D9%2587%25D9%2585-%25D8%25A7%25D9%2584%25D8%25B3%25D8%25B9%25D9%2588%25D8%25AF%25D9%2589.jpg.temp'.\n. @zamaliphe Both doesObjectExist and doesBucketExist catch all S3Exceptions here. If an exception is thrown, they return false; otherwise, they return true. \n. @zamaliphe I'm confused; a method can either return a value or throw an exception, but it cannot do both. If you are able to catch an S3Exception, it will have a getResponse method that returns the service's response.\n. Closing, as I'm unable to reproduce. If you can catch an S3Exception, then it will have a getResponse method that returns a the server's response (as a PSR-7 ResponseInterface) or null if no response was received.\n. Thanks for reporting this. The StreamWrapper class has an internal cache that is not being invalidated when writing to an S3 path; I opened a pull request to correct the behavior.\n. SimpleDB is supported via this package.\nPlease feel free to reopen if you have any questions or concerns.\n. Are you creating S3 clients anywhere else? The error message posted above should only appear when no version is specified in a client's constructor. The specific configuration you're using looks very similar to the configuration used in the bundle's test suite, so that combination gets tested regularly.\n. Closing, as I'm unable to reproduce the issue. The error posted is specifically thrown when a client is constructed without a version parameter, so I believe the snippets posted are not where the error is being thrown.\n. The 2.8 branch needs to continue working with PHP 5.3. The default cache requires that a few optional dependencies be installed, including the APC extension and doctrine/cache, and it's only loaded if a cache is not explicitly provided. On a system with PHP 7, you can provide an instance of ApcuCache to the client factory.\n. Is ApcuCache compatible with PHP 5.3? If not, then customers wanting to use the 2.8 branch with PHP 7 will need to supply a concrete cache instead of relying on the default.\n. On the APCu documentation page, the extension authors mention that they provide a separate module to make the apc_* functions compatible with PHP 7. I think it's reasonable to say that if you would like to continue using V2 of the SDK while upgrading your environment to PHP 7, you can either supply a concrete cache or install APCu's backwards compatibility module.\nThanks for reporting! Please feel free to reopen if you have any questions or concerns.\n. file_exists just calls the HeadObject on an operation and returns false if an exception is thrown or true otherwise. Could you trying calling HeadObject directly and inspecting the error thrown by the service?\n. file_exists should return true if the error code returned is AccessDenied. It looks like there are a number of error codes that could be provided with a 403 response; I'll need to look into which definitively indicate a file exists.\nChecking via listObjects would work, but it would be an inefficient solution for buckets with a large number of objects. The SDK would need to walk the response looking for the correct key; if the bucket was large enough to have a multiple pages of objects, file_exists might require sending multiple requests sequentially.\n. There are several 403 responses that would not necessarily indicate that an object exists, such as SignatureDoesNotMatch, InvalidAccessKeyId, RequestTimeTooSkewed, NotSignedUp, etc.\nYou could conclude that an object exists from AccessDenied, InvalidObjectState, or InvalidPayer. Was the error code you were encountering one of those?\n. getReasonPhrase will always return the stock phrase associated with an HTTP status code. The Guzzle PSR-7 library has a full list of possible reason phrases; they are not related to the error codes returned by S3. \nI ran a few tests and found that the stream wrapper's url_stat implementation (used by file_exists, is_file, is_readable, etc) will always return false when you do not have permission to read the object. This would be consistent with how file_exists works on Windows, SELinux, and other systems where you can restrict file visibility.\nFor your particular use case, do you need to call file_exists at all? Both is_readable and is_writable check if a file exists as part of their normal workflow.\n. There are local filesystem permissions that restrict your ability to see which files exist. For example, file_exists will return false if a file exists in a directory on which your user doesn't have execution/traversal privileges. Try the following from a directory you own:\nmkdir subdir\ntouch subdir/file.ext\nphp -r 'if (file_exists(\"subdir/file.ext\")) { echo \"file exists!\\n\"; } else { echo \"file does not exist!\\n\"; }'\nchmod 0644 subdir\nphp -r 'if (file_exists(\"subdir/file.ext\")) { echo \"file exists!\\n\"; } else { echo \"file does not exist!\\n\"; }'\nRight now, file_exists in the stream wrapper will always return the same result as is_readable. This is because PHP stream wrappers map both of those functions to url_stat, which requires read access to the S3 object. Permissions in S3 do not map cleanly to permissions on a local filesystem, so this may not be something that can or should change.\n. I don't think performing a ListObjects operation to rescue a HeadObject that failed with a 403 code would be a good solution in all cases. Assume you have a bucket with keys that are sequential integers from 1 to 1,000,000. If you want to call a stat function on the object named '17' that you didn't have permission to read, you would need to call ListObjects with a prefix of '1' and then iterate (and paginate!) through the result until you found the object with the desired key. If '17' is the last item in that list, you will have gone through thousands of object records before getting the data you need.\nreaddir populates the stat cache to make recursive directory iteration more efficient. That means that a subsequent call to url_stat will be able to return what was cached rather than having to send a HeadObject request. Maybe readdir shouldn't populate the cache? It would make iteration less efficient but would simplify the behavior of url_stat.\n. There's a noticeable performance impact. I think the right solution here is to document that the stream wrapper works most intuitively on buckets and objects to which you have read permissions, given how different S3 permissions are from local FS octal permissions.\n. Unfortunately, the stream wrappers don't offer any way to hook into clearstatcache, so that would need to be an instance method or a context option passed in to file_exists. \n. Looking good! The last step is to make sure all the documentation uses this new class. Users new to this feature should be guided to the V4-signing PostObject, so the pre-signed post documentation should be updated, and the v2 signer should be marked with an @deprecated annotation.\n. :shipit: \n. The instance profile credential provider uses a coroutine, which HHVM does not support due to a known incompatibility between Zend and HHVM generators. I'm afraid you will need read the instance profile credentials on your own when using HHVM. (This can be can be as simple as json_decode(file_get_contents('http://169.254.169.254/latest/meta-data/iam/security-credentials/default')).)\n. :shipit: \n. Glad using the before option worked out for you. You might also want to look into modifying the TableNamePrefixer class to handle more of the DynamoDB API. Right now, the class will not effectively map batch actions (like BatchGetItem or BatchWriteItem), as the shape of the input provided to those commands is a bit different from PutItem or GetItem. (See the parameter syntax of BatchWriteItem for an example. This is the command used by WriteRequestBatch.)\n. File type isn't something that's normally returned by stat. I'm not sure if unknown/unsupported values would get stored in PHP's stat cache.\n. Is there a PHP function that takes a path and will parse out the underlying resource's content type? filetype will only tell you if a path represents a file/directory/symlink/etc. \n. I was unable to get either mime_content_type or finfo_open to work with any remote stream (as described here). mime_content_type appears to try to read the stream, so it might not be as efficient as just calling HeadObject. You can manually save the result in a cache to get around any limitations of the PHP stat cache.\n. #767 was added because the idempotency characteristics of S3's API are well-defined and well-understood. That's generally not true of other services, where idempotency depends on factors unknown to the SDK when it's executing a command. Whether a ReceiveMessage command is idempotent depends on whether dead letter queues with automatic redrive policies have been configured; GetObject, however, is always idempotent.\nInstances of ConnectException are also always idempotent, as Guzzle reserves that exception type for cases when it is unable to establish any connection with the remote server. CURL_RECV_ERROR, however, is used when an error occurs while receiving a response. If you encounter this error while putting a message in an SQS queue, you cannot know if your message was received and fully processed.\n. null would be encountered if there is no response from the server. type is calculated from the HTTP status code as $code[0] == '4' ? 'client' : 'server'.\nIf you encounter a server error when executing a SendMessage command and retry it, you might end up with duplicate messages in the queue. I think you can be more cavalier with the retry decider if you make a few assumptions about the commands you're executing, but it's hard to say without clear context. Which command is leading to this error?\n. @oberman The retry middleware's internal logic can be overridden when it's not appropriate for a given use case. The middleware's constructor takes a callable decider function, which by default takes the general case into account. If you know that your DynamoDB commands have a ConditionExpression that makes them effectively idempotent or that your backend can handle duplicate SQS messages, then you can provider a more opinionated decider function.\n. This is a simple change, but could you augment the relevant test?\n. :ship: \ud83c\uddee\ud83c\uddf9 \n. When this behavior came up before in #915, the fix (#916) was scoped to just HeadObject commands because I didn't know if anyone was relying on content decoding. Given the case described above and the way this behavior would complicate the behavior of Aws\\S3\\Transfer -- downloading a bucket of compressed tar balls would write uncompressed files to disk -- I think you could make a case that this is a necessary bug fix, even if it could be a breaking change for some users.\nOutside of the context of S3, however, compression is a transport-level concern. I think we should make decode_content a documented http option on clients and set it to false in an S3-specific middleware.\n. @genomachino17 Due to backwards compatibility concerns, the workaround @cjyclaire provided above may need to be the official solution in v3.\n. Good point. I added a test to ensure the value is passed to http handlers when specified on a client or a command.\n. It looks like the failing tests are due to a bug introduced in #953. I'll update this PR with a commit to address them.\n. If you define the value as a constant in S3SignatureV4 (not SignatureV4), then you can refer to the constant in getPresignedPayload and in the documentation.\n. Looking at the previous implementation of S3SignatureV4, the value of 'ContentSHA256' was never used as the presigned payload. The server must have ignore the value in a x-amz-content-sha-256 query parameter when the canonical request was signed with a body checksum of 'UNSIGNED-PAYLOAD.'\n. @xibz Those tests are currently failing on master.\n. You might want to add a test with an exception that does contain a response, as would be generated by the situation discussed in #1065. (An x-amz-bucket-region header would be present on a 4XX response, and no exception would be surfaced.) \n. @dbrownxc SQS's developer guide cautions that you should not create a queue URL on your own, as the format is not guaranteed to remain stable forever. The queue ARN contains the region, AWS account ID, and queue name, which can be used to call the GetQueueUrl operation.. @kchan4 \nCredentialProvider::memoize caches credentials across operations, but not across processes. If you provide an instance of Aws\\CacheInterface as your credentials, the SDK will use the default credential provider chain and store the result in the provided cache. The SDK ships with adapters for Doctrine\\Cache and PSR-6 compliant caches.. The trait and interface have the same methods, since the trait is used by classes implementing the interface. One of the main developers of PhpDocumentor advises that traits add a @see annotation so that users who land on the trait documentation page can click over to the canonical documentation in the interface. (This was the reasoning behind #1162.)\nIf you would prefer to duplicate the documentation in the built API docs, could you do so during the doc build process rather than by duplicating the source?. Hi @moulderr,\nThanks for your contribution, but it looks like the proposed change wouldn't alter the behavior of the SDK. Aws\\Result implements ArrayAccess by way of Aws\\ResultInterface, which means that $this[$key] is equivalent to calling $this->offsetGet($key), which is defined in Aws\\HasDataTrait.\nDid you encounter any unexpected behavior that prompted this pull request?. The error message seems to indicate that your system clock and that of the ElasticBeanstalk service are not in sync. Could you check if syncing the system clock fixes the issue?. Hi @hamadata,\nCloudFront does not require a Cloudfront-Expires attribute when cookies are signed with custom policies, but that attribute is required when signed cookies use canned policies. As demonstrated lower in the CookieSignerTest class, the attribute is not included for custom policy-based cookies. \nCustom policies must include a DateLessThan condition, so the Cloudfront-Expires attribute is not included in those cookies. . Hi @dev4press,\nYou can accomplish this by providing customization callbacks as described in the developer guide. A multipart upload's storage class is specified in the CreateMultipartUpload operation, so you customize the command in the before_initiate callback:\nphp\n$source = '/path/to/large/file.zip';\n$uploader = new MultipartUploader($s3Client, $source, [\n    'bucket' => 'your-bucket',\n    'key'    => 'my-file.zip',\n    'before_initiate' => function (\\Aws\\Command $command) {\n        // $command is a CreateMultipartUpload operation\n        $command['StorageClass'] = 'REDUCED_REDUNDANCY';\n    },\n]);\nServer-side encryption is a bit trickier, as the type of SSE used will determine which commands should be modified. For KMS-based SSE, you would customize the CreateMultipartUpload command:\nphp\n$uploader = new MultipartUploader($s3Client, $source, [\n    'bucket' => 'your-bucket',\n    'key'    => 'my-file.zip',\n    'before_initiate' => function (\\Aws\\Command $command) {\n        // $command is a CreateMultipartUpload operation\n        $command['ServerSideEncryption'] = 'aws:kms';\n        $command['SSEKMSKeyId'] = $kmsKeyId;\n    },\n]);\nIf you're using server-side encryption with a customer-provided key, this key will need to be added to all uploaded parts:\nphp\n$secretSymmetricEncryptionKey = ...;\n$uploader = new MultipartUploader($s3Client, $source, [\n    'bucket' => 'your-bucket',\n    'key'    => 'my-file.zip',\n    'before_initiate' => function (\\Aws\\Command $command) {\n        // $command is a CreateMultipartUpload operation\n        $command['ServerSideEncryption'] = 'AES256';\n        $command['SSECustomerAlgorithm'] = 'AES256';\n        $command['SSECustomerKey'] = $secretSymmetricEncryptionKey;\n    },\n    'before_upload' => function (\\Aws\\Command $command) {\n       // $command is an UploadPart operation\n        $command['SSECustomerAlgorithm'] = 'AES256';\n        $command['SSECustomerKey'] = $secretSymmetricEncryptionKey;\n    },\n]);\nHope that helps! Please feel free to reopen if you have any questions or concerns.. Hi @rizwandogar111,\nI'm not exactly sure what you mean. SDK operations should always return an instance of Aws\\ResultInterface or throw an exception. Could you provide more detail as to which response you want to get in json format?. This PR should be limited to the last commit. Can you create a checkout a new branch from the tip of master and cherry-pick the last commit onto it?. :shipit: . Hi @joshdifabio,\nThe SDK's retry middleware uses a \"full jitter\" strategy to minimize both the number of retries that must be made and the amount of time a request takes to succeed after factoring in retries. A more detailed explanation of the approach taken and some of the advantages it has over other methods can be found on the AWS architecture blog.\nThe delay decider function only takes account of the number of retries already attempted because the type of error isn't a reliable indicator of when a retry might succeed. A ProvisionedThroughputExceededException exception might be immediately retryable (if the request arrived a few milliseconds before a throttling window ended), and a 503 or 504 might represent a server state that isn't actually retryable for another 30 seconds. FWIW, the retry delay strategy implemented in the SDK for DynamoDB is based on that service's official recommendation.. @cjyclaire I think there's an extra - 1 in the default delay decider that causes the first retry to only be delayed by network round trip time. The delay function is meant to be called with the number of retries that have already been attempted rather than the ordinal of the current retry.. LGTM! \ud83d\udea2 \ud83c\uddee\ud83c\uddf9 . @4406arthur Is there any reason this shouldn't be released as a separate package that depends on the AWS SDK? Just FYI: we have a separate package for SimpleDB compatibility with V3 that contains an implementation of SigV2. You could create a package that depends on that and adds the S3SignatureV2 class from this PR.\nI see a lot of code specific to Google Cloud Storage in this PR, which would just be added bloat for Amazon S3 users.. @AndyDunn Are you using a globally installed version of PHPUnit (installed with composer require -g)? That would cause the version of Guzzle installed with composer require -g to be prioritized over the one installed locally in your project.\nI believe a few testing tools have Guzzle 4 as a dependency, so this situation can occur even if you didn't manually install Guzzle globally. The easiest remediation would be to use PHPUnit a package-local installation or a PHAR.. That's a fair point, but the middleware only iterates over the top-level members of an input shape. My worry in not adding the option to Aws\\ClientResolver is that, per the spec, the SDK should be able to support idempotency tokens in any service. Having to add a custom parameter declaration to each service client that adds idempotency tokens doesn't seem like a sustainable approach in the long run.. The test failures in Travis look related to a change in version 1.4 of GuzzleHttp\\Psr7. . @fernandoval Version 1.4.0 contained a potentially breaking change that would affect customers using S3 presigned posts. The SDK's dependency constraint allows for version 1.4.1.\n@karllhughes  Since UriResolver::resolve was only added in 1.4.0, a change to use it instead of Uri::resolve would need to check whether UriResolver::resolve exists before invoking it. Continuing to use Uri::resolve means we do not need to make that check at run time, so I'd be in favor of leaving those uses of Uri::resolve unchanged.. Can I ask what's breaking? Using Uri::resolve in version 1.4.1 triggers a deprecation warning which is immediately silenced. It should only appear in logs if you have enabled scream mode and set error reporting to include E_USER_DEPRECATED-level notices.. @marklocker Version 1.4.0 contained a backwards-incompatible change that was reverted in 1.4.1 (see https://github.com/guzzle/psr7/issues/138 for context). Since UriResolver was only added in version 1.4.0, switching to the new method would mean dropping support for earlier versions of Guzzle's PSR7 library or introducing a shim method into the SDK.\nThe deprecation notice was added to inform consumers that Uri::resolve will be removed from version 2.0.0 of the library, so I think continuing to use the deprecated method is the best of the three available options. I'll open a PR on the PSR7 library to replace the call to trigger_error with a @deprecated annotation.. The calls to trigger_error have been removed in version 1.4.2 of guzzlehttp/psr7, and the master branch of the SDK has been updated to require ^1.4.1 and to invoke UriResolver::resolve.. You're correct, @mattzuba; I'll update the tag.. I think it's also worth making the other change mentioned in #1209. The interface for credentials specifies that getSecurityToken returns either a string or null, and environmental credentials are returning false for that method.. LGTM :shipit:. What would you think about just using UriResolver::resolve and requiring version ^1.4.1 of guzzlehttp/psr7? Aws\\Api\\Serializer\\RestSerializer is used by every API operation for a number of services, and I'd like to avoid runtime checks if possible.. There's a doesBucketExist method on the V3 client.. @imshashank There are some cases that would slip past the validator that would still cause errors in json_encode. You should add test cases for the error conditions this PR is explicitly handling.. You'll need to set the seekable option to true when registering the stream wrapper. Otherwise, S3 streams cannot be rewound.. @cjyclaire Yes, I have verified with a manual test. I could add a functional test to the multi-region client to verify that the CompleteMultipartUpload operation is sending the right data.. Hi @isegal,\nThe SaveAs parameter directs Guzzle to use the file specified as the target in which to spool all response data, which in the case of a 304 happens to be an empty string. I'll open an issue on Guzzle to see if an extra branch should be added to here to only write if the data received has a length greater than 0.. :shipit: . This line was meant to exclude a file at the project root named Config but was also excluding a folder from the AWS manifests for a service named config.\n. Travis was failing to interpret the manifest file and wasn't testing any new builds.\n. The tags on the feature files aren't 1-to-1 matches to what's in the manifest file. I removed the exact matches from the latest commit but there's still a sizable config array.\n. I'll hold on this PR until #641 is in master.\n. \\Aws\\manifest worked for everything except ElasticLoadBalancing, so I just changed the tag on that feature.\n. No reason. Changing to private.\n. I only want to generate the hash once per instance.\n. Oh, right. I find empty to be clearer than an exclamation point when reading code, and using empty avoids the implicit cast to boolean that simply calling if (!$this->members) { would entail.\n. No caching is enabled by default, but the cache promise is at the top of the provider chain so that caches are checked before credentials are instantiated. I agree with another comment you made on this PR that caching should be built into self::memoize instead.\n. Would it make sense to use the access key instead (so that it's not tied to a particular host)?\n. You're right. I had it in as documentation (to signal that null is the intentional return value for this code path), but the docblock already states that the method can return null.\n. I think you actually want this warning to surface. It won't halt script execution and will let you know that you should proactively create this directory to avoid a race condition. (A post-dump-autoload call to Aws\\JsonCompiler::purge() in your composer file will create the default directory under safe conditions.)\n. It can. There's no custom serialization defined, so serialize will just capture all object properties. The output is a bit large so I could cut out whatever is unnecessary. \n. The anonymous functions would not be serialized. The only part of the client that would get serialized would be its config array, and unserializing it would just use that config to call the constructor. \nThe other option would be to use a __wakeup method instead and have it call InstanceMetadataClient::factory(), i.e., mimic what happens when you create a new RefreshableInstanceProfileCredentials without a client. (This is what is done by default in Credentials::factory().) Doing so would involve fewer changes but would wipe out any custom InstanceMetadataClient, but I'm not sure anyone has ever created or used a custom one of those.\n. I didn't notice they used to be in alphabetical order until I read your comment. I see no reason to break with tradition on that.\n. Any chance you could release a 0.0.3? I was going to leave it at master until the next release (but a sha would work just as well).\n. @mtdowling The ambiguous errors will be valid XML, they just won't be the shape expected by the RestXML parser for a copy operation. I exposed the service's error parser to this class and used that instead of SimpleXML, but the copy errors need to be handled separately from list errors, as those will return a 200 and some fraction of a valid XML document (rather than a 200 and the wrong XML document).\n. They are separate errors, having nothing in common besides their being error responses with a 200 status code, so they could be caught by separate parsers. \n. Unfortunately, SimpleXMLElement will only throw one class of exception: Exception. It seemed convenient, as a generic MalformedResponseParser could be used with any service, not just S3, but for that to be true I think I would need to define an Aws\\ParserException and translate JSON and SimpleXML errors into that.\n. @mtdowling Renamed to RetryableMalformedResponseParser.\n. That enables fast file linting with opcache_compile. Unfortunately, it turns out that you cannot lint code declaring symbols in this way, so I can remove that parameter.\n. Absolutely!\n. I'll drop it to 200MB.\n. How would a client configured to talk to us-east-1 then specify a location constraint of a different region?\nThe S3 docs indicate that you can specify a location constraint when calling createBucket against us-east-1.\n. I was overzealous in consolidating the two methods -- the credentials shape returned by Cognito Identity is slightly different from that returned by STS, so I ended up keeping them separate.\n. I added an extra phrase to the $serviceError that points out that the body could not be parsed.\n. Since we support pluggable HTTP backends, wouldn't that be more a test of Guzzle and Guzzle\\Promises than of the SDK? Even in an integration test, GuzzleHttp\\Promise\\all could handle promises sequentially and the result would be the same.\n. @mtdowling That's a good point. I could simplify the class by having it only support canned policies (resource + expiration time), as that's a much stronger contract than CloudFront's current policy spec.\n. No. I could move this to the suggestions and dev dependencies blocks.\n. This assumption is commonly made throughout the SDK. The other options would be to make the transfer statistics modifiable on an instantiated AwsException or to break through the property's visibility restriction with closure binding or reflection.\n. Currently, no layer of the SDK is setting on_stats, though users of the Guzzle 6 handler are free to do so. __on_transfer_stats is set up as an option that is understood by both Guzzle handlers, and it has a different name so that no user code that is currently collecting Guzzle 6 stats would be affected.\n. The only blocker on that approach is that Guzzle 5 does not support on_stats or provide a GuzzleHttp\\TransferStats object that could fulfill the callback signature required for Guzzle 6 on_stats callables. Just using on_stats would simplify this feature but would make it incompatible with Guzzle 5.\n. D'oh! A fix is squashed into the last commit.\n. The value passed in is either an associative array or a boolean. The individual collectors only default to true if $value === true, not if it's an array. In all other cases collectors default to false.\n. The host name should be normalized to a fixed length and character set. Maybe you could use the md5 or crc32 of the hostname instead? \n. This middleware will throw an error if $request is null. You could make $request non-nullable and mention that it must be added after the build step in the documentation block, as we do in Aws\\S3\\ApplyChecksumMiddleware.\n. This is a nitpick, but our naming structure would probably render this as invocationId instead of invocationID.\n. You might want to use a dataProvider instead.\n. This will need to be more unique than just uniqid. I think using gethostname was a good choice but would prefer to see the hostname normalized. How about uniqid(md5(gethostname(), true)?\n. This documentation block should point out that the middleware can only be used after the build step in a handler stack. Also, please limit all lines to 80 or fewer characters.\n. I think this assertion should be in the $mock function rather than inspecting a symbol that is updated by reference.\n. $retryMW is not being invoked in this test, so no assertions are being made.\n. This looks very similar to what's in Aws\\Signature\\SignatureV4. Could you use the same code in both (maybe via a trait)?\n. This looks like test-specific code. I don't think it should be in the src folder.\n. Why not just require the file from the signature namespace and proxy to those functions, e.g., have gmdate in the Aws\\S3 namespace call \\Aws\\Signature\\gmdate? That way, updates only need to occur in a single location.\n. I'm not sure what the point of this trait is. Couldn't classes just use Aws\\Signature\\SignatureTrait directly?\n. I looked into this, and commands are used up when executed or presigned. I can push up a change that clones the command, but I wasn't sure if it was in scope for this PR.\n. No, I was just being overzealous. I'll squash it out.\n. getSigningKey (with the cache) should be part of the trait.\n. public and protected methods should be avoided for anything that is called from within the class. Methods should be private by default (especially in a trait).\n. Since a SigV4 policy needs to include conditions about the x-amz-date, x-amz-credential, and x-amz-algorithm fields, I don't believe this is a backwards compatible change. Any policy that is currently valid for a SigV2 POST upload would fail with a message like 'Invalid according to Policy: Extra input fields: x-amz-date' if the PostObject class suddenly started using SigV4 signatures.\nThis is an issue because the SigV2 PostObject constructor can take a policy as a string. Since the signer will need to add fields to the policy scoping the date, algorithm, and credential, you might want to follow the SDK V2 example instead.\n. This step will need to be broken up into several steps that each do a single thing explicitly. Currently, the logic of how a POST upload is completed is contained in the context class instead of in this feature file.\n. Yes, but the mapRegions function doesn't make much sense without it. If the iterator provided to the command pool yields meaningful keys, they are only currently passed to the before callback, not fulfilled or rejected.\n. I believe that case would be handled, but I agree that there should be an explicit test for overriding signing_region and signing_name.\n. I was thinking of the S3 ACL 'read' grant, which corresponds to ListBucket, ListBucketVersions, and ListBucketMultipartUploads for buckets and GetObject, GetObjectVersion, and GetObjectTorrent for objects. I'll call those out here, as that's a very specific definition of 'read.'\n. This needs to be a bit more explicit. Cucumber features are both tests and documentation for how to interact with the code. You might want to change this to either a series of steps or something like \"I create a POST object with the following conditions:\" followed by a a string that you can JSON-decode.\n. All these loops and branches make a large surface area for bugs. Could this be made simpler by just taking a list of conditions, using Boto3's generate_presigned_post as an example?\nInstead of taking an array of options in the constructor, take $conditions as an array of arrays. Some of them will be a single key => value pair like ['acl' => 'public-read'] , while others will be a three-member tuple like ['starts-with', '$Content-Type', 'image/'].\n. This should be polymorphic. Usually the SDK will take an integer, DateTime, or string when a date value is needed.\n. I don't think this should be a public method. It was private in the SignatureV4 class.\n. This constant is only used once and is unlikely to be used in another method. You could just put this string directly in the getResourceName method.\n. Is there a reason the file is so large? This could take a couple of seconds to upload when running the tests on spotty wifi. It's not a huge deal, but I think a ~128K file exercises the same functionality. Also, there's no real need for a constant here: just 128 * 1024 is fine.\n. This one scenario is testing both V2 and V4 signature POST uploads. They should be split into separate scenarios.\n. I think this is too restrictive. What if somebody needs to use a specific expiration that is not an even number of hours from the current time? If this argument instead just represents a specific time, customers will have a lot more flexibility in what expiration they can specify.\nTake a look at how the SDK serializes timestamps sent to services for an example of taking a specific time as a DateTime, string, or integer.\n. I don't think this is relevant to the test.\n. Rather than branching here, why not just create two different methods? V2 and V4 signing have different interfaces.\n. I'm not sure adding these values to the form inputs is the right thing to do. What if it's already in the array supplied by the user? How does S3 respond to duplicate fields?\nI would just trust that the user has added what they need to the form inputs and the conditions arguments.\n. Users should not have to specify the algorithm as part of their policy. That needs to be added by the signer.\n. This method needs to modify the policy to add the appropriate credential, algorithm, and date to the list of conditions.\n. The 'X-Amz-Credential' condition should be handled by the signer. As you mentioned when we discussed this in person, the condition field names are case sensitive, and that complexity should be encapsulated in the signer.\n. null would to be lowercase. The SDK follows the PSR-2 coding style.\nBut I see below that you're interpreting a null as meaning '+1 hours'. Why not just make that string the default value? \n. This could be simplified to TimestampShape::format(is_null($expiration) ? '+1 hours' : $expiration, 'iso8601'); It could be simplified even further if the default value of $expiration were '+1 hours'; all validation would then be handled by TimestampShape::format.\n. There's a race condition here. You're resolving credentials in getV4Conditions and later on line 87. Credentials can be refreshable, which means that you might end up creating a policy with one set of credentials and then signing it with another. Resolve the credentials once.\n. I don't think these values need to be bound to the class. Why not just resolve them when signing?\n. I'm not sure this should be exposed as a public method on a V4 signer class. The policy isn't final until it's signed.\n. You can just call createScope; it's part of the SignatureTrait.\nMaybe it would make sense to resolve the region differently if a user injected a multi-region client. You could call determineBucketRegion to find out the appropriate region.\n. The V2 signer just takes a string. Would it make more sense to use a pyString instead of a table?\n. Both tests should be more fleshed out. It's still not clear from this cucumber test what inputs a PostObject is expecting. Maybe a separate step should add the form fields?\n. This should be type hinted as an array.\n. This is being added in getPolicyAndSignature, isn't it? $policy here can just be defined as:\nphp\n$policy = [\n    'expiration' => TimestampShape::format($expiration, 'iso8601'),\n    'conditions' => $options,\n];\nThat would capture your intention here more concisely by communicating the role of $options here.\n. That's a good point. I'll factor that out into a separate method.\n. I'm using array_intersect here to make sure that only signature versions that are supported by the SDK are pulled from the endpoints file. For example, Import/Export's signature versions are: ['v2', 'v4']; the intersection of that against the SDK-supported signature versions will return an array with just one element: v4\nphp\n$supported = ['s3v4', 'v4', 'anonymous'];\n$valid = array_intersect($supported, ['v2', 'v4']);\n// $valid is [1 => 'v4'];\nIf the service and SDK support more than one signature version, we should use them in the order of preference provided by the $supportedBySdk variable. I should be using array_shift instead of array_pop to enforce that.\n. These method signatures are defined by ArrayAccess, so we're stuck with the names they have, unfortunately.\n. Will do!\n. This is a no-op. You're assigning the current value of the header to the be the new value of the header.\n. I believe so. The test must be discarding the supplied value and using what's returned by getPresignedPayload for both scenarios.\n. Rather than a series of functions that use global variables, could this file be written as a class with a $baseDir and $newService instance variables?. This could go in the build/ folder rather than in a new scripts/ folder. That way you won't need to update the phar builder or the .gitattributes file to exclude this from the distributed artifact.. This is more of a style nit, but all-caps names are only used in the SDK for const class and interface members. Since you mutate this variable and it does not outlive the scope of this function, it would be more idiomatic to use camelCase.. Oh, I see that this is not reading the changelog but holding entries to add to the changelog. Maybe rename this to $entries (or $newChangelogEntires if you want to be more explicit).. The indentation, bracket placement, and spacing should follow the PSR-2 standard.. createTag. The cmp function could be an inline anonymous function.. Per PSR-1, code that defines symbols (like functions) should not be in the same file as code that causes side effects.. I think service rather than endpoint should be injected, e.g., if (empty($args['service'])) { $args['service'] = 'appstream2'; }. That should take care of the endpoint and transport scheme resolution.. Is this necessary? You could just instantiate a new ChangelogBuilder directly in the test cases.. I would recommend using a more granular exception so that you don't accidentally accept an unrelated exception as a test success. I also find the @expectedException annotation more clear than calling $this->setExpectedException, but that's an optional change.. s/testread/testRead/. s/testread/testRead/. s/testcreate/testCreate/. s/testcreate/testCreate/. s/testcreate/testCreate/. All caps should really only be used with constants. Did you test with https://github.com/aws/aws-sdk-php-v3-bridge to make sure the SDB compatibility package still works with this change?. I agree. The base directory should remain the same throughout a changelog build, so passing it as an argument to the class's constructor would be preferable.. false should be lower-cased. You might want to run a style fixer (e.g., http://cs.sensiolabs.org/) on this file to get everything PSR-2 compliant. . This test shouldn't occur within the SDK's package on disk. (The test runner might not have permission to write to this directory on all systems.) \nUse sys-get-temp-dir to get the system's temporary directory and then create a subdirectory therein. . Also, this should be private. It looks like only buildChangelog and fixEndpointFile are called from outside the class. Every other method should be private.. If the constructor signature changes, then the signature of this method likely would too, as you would need to test different values for those new constructor parameters. You would need to change all code that calls this function anyway.. No, it looks like the output would match what's already in the endpoints.json file. Is there a reason we need to strip this out, though? The SDK will already hard-codes the signature versions it supports in the partition file, and omitting the 'signatureVersions' block would normally mean that the default value of ['v4'] should be used, which is not accurate for SDB.. Should be S3MultipartUploadException::class. Should be S3MultipartUploadException::class. Optional: you can still bind options passed in the parameters array to private class properties. I personally prefer to use properties like $this->baseDir as opposed to having an array of parameters with string keys. The array makes no guarantee as to which keys are present, whereas scalar properties are always defined (though they may be null).. Style nit: it would be better to just call this verbose.. Does the build script not read any parameters from argv?. It's a flag on the command line. Inside the class, the behavior is verbose or it is not verbose.. Since the builder allows users to customize its behavior from the constructor, I think the CLI should accept those inputs from the command line. It sounds like this script will currently only work if run from the correct folder, which means automation scripts would need to cd into the right place before executing this command and then cd out afterwards.\nIdeally, the CLI should honor -v, --outputDir/-o, and --inputDir/-i.. I think it's fine to make that the default behavior, but it seems odd not to expose this configurability to the command line.. I think it's much clearer to bind values pulled from the parameters array to instance properties instead of putting them in an internal parameters array. Is there any advantage to using an array instead of instance properties?. Is adding a todo any less work than adding a call to get_opt? At the very least, you should accept a -v flag instead of always setting verbose to true. Additionally, if no option to configure inputs and outputs is offered, the todo comment should specifically call out from which directory this script expects to be run.. That's an excellent reason for the constructor accepting arguments as a hash to the constructor, but there's nothing preventing you from accepting an array and saving data to instance variables. Saving this data as a params array makes the data this class requires less clear (i.e., you need to inspect the constructor to figure it out; you can't just look at the names of the instance variables) and also breaks IDE autocompletion.. Think of it this way: if there are no distinct advantages to using an internal parameters array, then why do it if you know it will prevent IDEs from autocompleting these fields? I personally think it's much clearer to see:\n```php\nclass X {\n    /*\n     * @var string\n     /\n    private $property1;\n/**\n * @var int\n */\nprivate $property2;\n\n/**\n * @var bool\n */\nprivate $property3;\n\n}\n```\nthan it is to see:\nphp\nclass X {\n    /**\n     * @var array\n     */\n    private $properties;\n}\nIn the first case, you can explicitly call out the data type and name of each expected property, and IDEs will be able to warn on type mismatches. In the second case, you're forced to inspect the constructor to see what properties are available, and no type checking can be provided automatically. We accept community contributions to all PHP files in this repository (though build tool contributions are vanishingly rare), so we shouldn't complicate the developer experience unless there's a compelling reason to do so.\nAdditionally, automated tooling will flag dynamically declaring a new instance property as a code smell, but cannot do so if a method adds something new to the properties array. I just do not see any advantage to using an array of properties inside the class but can identify several disadvantages. It would also be a departure from standard practice in the SDK (e.g., AwsClient) which is why I'd like to understand the motivation.. The colon is used for options that take a value. This should be getopt('v'). [] instead of array(). Optional: a ternary $params['verbose'] = isset($option['v']) ? $option['v'] : true would be more concise.. variables should be lowerCamelCase. properties should be lowerCamelCase. lowerCamelCase. $this->collectPathInfo overwrites the $bucket, $key, and $filename each time it is called. Since exceptions encountered when performing a multipart upload would always pertain to the same bucket and key, I don't think this is a bug so much as it is an inefficiency.\nWould it make sense just have a compound condition for the if statement that ensures the array is not empty and collects path info from either the head or the tail of that array, whichever represents the most recently encountered error? E.g.:\nphp\nif (is_array($prev) && $error = array_pop($prev) {\n    $this->collectPathInfo($error->getCommand);\n}. Ah, I missed that $prev gets passed on to the parent constructor. You could use each($prev)[1].. If you import the exception with a use statement, ::class should just expand the classname into a fully-qualified one. Example. I've seen warnings emitted in the aws-sdk-php-symfony build. My understanding of the endpoints.json format when writing the Partition class was that a partitionEndpoint would always be present if a service self-reported as not regionalized. That doesn't seem to be true in all cases.. I'll change this to just check $value['idempotencyToken'] === true.. We don't have specific unit tests for XmlBody; this code is exercised by the protocol tests (i.e., the JSON files updated in this PR).. Added!. I don't believe this is accurate for any S3 endpoint. Also, this is not the appropriate place for a changelog diff.. This looks specific to Google Cloud Storage and should live in a separate package.. This constant is also available on S3SignatureV4, which extends this class. You could remove the constant from S3SignatureV4.. This will only have an effect on presigned requests. I think you meant to put this block inside the getPayload method.. I think the $unsigned = here is unnecessary. If you feel an additional positional parameter makes the constructor less clear, it would be more in line with the rest of the SDK to add an $options array (containing an 'unsigned_payload' => true member here) as the final parameter. That would allow more options to be added over time.. Could you also add a black-box test that specifies a date, credentials, service, region, and expected signature with the unsigned option set?. A new provider would be one way to do it. You could also modify the existing testProvider to provide an additional parameter indicating if the ~request~ payload should be signed.. I think you still want all S3 presigned URLs to use unsigned payloads.. Ok I see why getPresignedPayload was removed. S3 currently only uses unsigned payloads for presigned urls, whereas this change would make all requests to S3 use an unsigned payload.. The $options = isn't necessary. Also, why the string 'true' instead of just the boolean true? (Both apply to line 115 as well). For simplicity's sake, you could ensure this is always a boolean, e.g. isset($options['unsigned']) && $options['unsigned'] === true. Optional: this should dereference the constant as SignatureV4::UNSIGNED_PAYLOAD. The fact that SignatureV4 is the parent of this class is incidental to the value being used. (This could prevent future copy-paste errors.). Optional: $options['unsigned'] should always be a boolean, so you could be more explicit in what values are accepted (e.g., $this->unsigned = isset($options['unsigned']) && $options['unsigned'] === true). Is there an s3v4-unsigned-body that should be checked?. Instead of an optional positional argument, you could use an optional array of named arguments:\nphp\npublic function presign(<args>, array $options = [])\n{\n     $startTimestamp = isset($options['start_time']) ? $options['start_time'] : time();. Why not just if ($securityToken = $credentials->getSecurityToken()) {?. if (!empty($variable)) is shorthand for if (isset($variable) && $variable). Both consider the condition met if $variable contains a truthy value and fail if it contains falsy one (including an empty string).. The intent of the test would be clearer if you just used a literal value here instead of reading an environment variable. You need to go up to the setup method to see that $_SERVER['aws_time'] is set to strtotime('December 5, 2013 00:00:00 UTC'), which is a layer of indirection that this PR makes unnecessary.. This should also just be strtotime(...). Optional: conditional assignments like this are clearer as ternary expressions than as a series of statements.. This could just be expressed as if ($shape['jsonvalue']) {. if ($member['jsonvalue']) {. Why decode and then encode?. Empty strings are falsy. http://php.net/manual/en/language.types.boolean.php#language.types.boolean.casting. I don't think this safely handles all data types. https://3v4l.org/L64A8. This assumes that jsonvalue headers are being passed in as JSON-encoded strings. I don't think that's a correct assumption.. This doesn't look like the protocol test that was added to other SDKs.. Do you have any tests exercising this error path? There's no standard ParserException in PHP, and I don't think the SDK declares any exceptions in the root namespace.. If the value is a resource, json_last_error() will return JSON_ERROR_UNSUPPORTED_TYPE, but users could also encounter other errors specific to the act of encoding like JSON_ERROR_DEPTH or JSON_ERROR_RECURSION. (Cf. http://php.net/manual/en/function.json-last-error.php)\nMy point was that this PR could add a test to make sure providing invalid data for a jsonValue header throws a catchable exception. Prior to https://github.com/aws/aws-sdk-php/pull/1229/commits/93abe6baf75b7b73e1c2f7973c541fc5bb337d01, the code would have raised an uncatchable fatal error, but the tests were still green. The tests currently only exercise the happy path and not any expected error conditions.. I don't think either of these imports are used anywhere in this file.. Maybe this should be explicitly \\InvalidArgumentException? I believe PHPUnit evaluates expected exceptions in the global namespace, but per the imports  this is actually expecting \\Doctrine\\Instantiator\\Exception\\InvalidArgumentException, which I assume your IDE imported automatically when searching for InvalidArgumentException;. I think this is too close in spirit to the getMessage method this class is inheriting from \\Exception. A name like getServiceMessage (or something similar) would be better, as it would call out why this is different from the regular exception message and why it may or may not be present on an instance of AwsException.. Might be overkill, but what you're explicitly fixing is that the concise message is not the same as the $message argument passed to the AwsException constructor ('Foo' in this case); you could add an assertion that the value returned by the new method is not 'Foo'.. This for loop isn't really doing anything anymore. You could just set $this->newServiceFlag to count(array_filter($arr, function($change) { return $change->type === 'NEW_SERVICE'; }) > 0  and return the array that was passed in. (Or keep the loop, drop the $cleanedJSON array, and just return $arr.). 4.8 is pretty far out of date at this point and isn't guaranteed to be compatible with PHP 7 or 7.1. To make sure the tests are still compatible with all of the versions we test in Travis, I would recommend ^4.8.35|^5.4.0|^6.0.0. Looks like the underscore-namespaced class names were dropped from PHPUnit 6. ^4.8.35|^5.4.0 should avoid the failures that were reported for PHP 7 and 7.1. Alternatively, you could migrate to the new test class naming convention (e.g., replace PHPUnit_Framework_TestCase with \\PHPUnit\\Framework\\TestCase).. One potential problem with this approach is that it assumes parameters always have the same form and semantic meaning for each constituent command of a multipart operation. I believe that's true now (though I can't say for sure without looking at every parameter used by {Create,Complete,Abort}MultipartUpload/UploadPart), but is that something the can be relied on indefinitely? There's nothing in S3's API structure that guarantees that, which is I believe why multipart uploads must today be customized by supplying 'before_X' callbacks.\nTake the following scenario: Say S3 adds a foo parameter to CreateMultipartUpload that takes a structure. Assume that it's a new kind of tagging. A couple months later, they release a feature that lets you apply additional foo to the individual parts of a multipart upload. Because UploadPart is a streaming operation, foo must be sent as a header, which means the SDK must receive it as a serialized string (like how the SDK takes Tagging as a string in PutObject (where it's a header) and as a structure in PutObjectTagging (where it's serialized as part of the request body)). After merging in this PR, that would be a breaking change for the PHP SDK, whereas today it wouldn't be.. Not a blocker, but there should be an empty line between this method and the previous one.. Not a blocker, but missing preceding new line (as above).. Not a blocker: missing new line. If you can't get the region from this error, should it be rethrown?. More of a nit, but I would expect this method to return false on error if it's swallowing exceptions.. Wouldn't this case already be handled by the determineBucketRegionAsync on the regional client?. Why use this method instead of just directly calling yield $this->determineBucketRegionAsync($bucketName);? If the intention is just to avoid repeating yield, I think this should be private. Is this scenario only failing because of incompatibilities in PHPUnit? They don't officially support PHP 7.2 yet, so it makes sense not to treat this as a release blocker.\nThis shouldn't be a blocker for this change, but in a follow-up PR it might be worth upgrading the tests to PHPUnit's namespaced class style so that you can also allow newer environments to run PHPUnit 6.. I think this may be to blame for the purported coverage drop. There may be code paths not taken in PHP 7.1. Is this the name of a particular bucket? Including a resource identifier here seems fragile.. Couldn't AesTaggedGcmEncryptingStream be removed if you just used an AppendStream here?. Very nit but passim: I believe SDK style requires no newline between the file opener and the namespace declaration.. What's a cmk? Could abbreviations be avoided in this parameter name, or is the expanded version too long to reasonably type?. Same as above comment but for 'cek'. What if saving this instruction file fails? Is the ciphertext blob deleted from S3?. encrypt isn't returning a promise, is it?. I believe this is a copy-paste error. IIRC, this case (and the following one) were meant to exercise round-tripping of multibyte characters, e.g.:\nphp\n[Psr7\\stream_for('\u062f\u0633\u062a\u200c\u0646\u0648\u0634\u062a\u0647\u200c\u0647\u0627 \u0646\u0645\u06cc\u200c\u0633\u0648\u0632\u0646\u062f')],\n[Psr7\\stream_for('\u0420\u0443\u043a\u043e\u043f\u0438\u0441\u0438 \u043d\u0435 \u0433\u043e\u0440\u044f\u0442')],. From context, I'm guessing that CEK stands for \"content encryption key\" and CMK stands for \"content master key\"; why not just use EncryptionKey and MasterKey as the parameter names?. The test classes should probably be in a namespace corresponding to the directory structure.. I guess it's more of a style issue, but why does this need to be in an async context at all? With the exception of $this->client->putObjectAsync, this chain of functions seems to be operating synchronously and then returning non-promise values. Wouldn't it be easier to read if it was just iterative statements that returned the promise from putObjectAsync at the end?. gotcha that makes sense. The S3 test should be creating and deleting buckets via BeforeSuite and AfterSuite hooks. I understand that this test suite has to work this way to integrate with other SDK tests, but maybe in the future there could a test that works on multiple accounts.. Style nit: there should be a line break after elseif (. Style nit: there should be a line break before ) {. Does Guzzle 5 use the errno property for a different piece of information? Rather than forking based on the version of Guzzle used, you could use errno if available and check the message otherwise.\nThis would let you skip the message parsing if errno were added to Guzzle 5 at some point in the future.. My impression from https://github.com/aws/aws-sdk-php/issues/924#issuecomment-355646600 was that CURLE_RECV_ERROR would not be retried by default. Did you mean it wouldn't be retried by Guzzle but would be retried by the SDK?. Rather than checking the version, couldn't you check if the getHandlerContext method exists?. Does the model need to be modified if the member is being added in Aws\\Rds\\RdsClient::applyDocFilters?. ",
    "docmattman": "Got it working using the following code:\n$client = S3Client::factory(array(\n    'key'    => 'MY_GCS_KEY',\n    'secret' => 'MY_GCS_SECRET',\n    'base_url' => 'https://storage.googleapis.com'\n));\n. ",
    "dmyers": "I'm passing the 'base_url' to the S3Client::factory in my config is that the wrong way to set it?\nTrying to set the key to test, and the base_url to http://s3.yourdomain.com without a bucket I get:\nbash\n[curl] 6: Could not resolve host: test [url] http://test\n. Sorry for the delay I think what I was testing with was something like this. I tried with and without a bucket name since I think it is supposed to be passed through the CNAME.\nphp\n$client = \\S3Client::factory(\n    'key' => $aws_key,\n    'secret' => $aws_secret,\n    'base_url' => 'http://s3.example.com',\n);\n$url = $client->getObjectUrl('', 'test');\necho $url;\n. ",
    "kara-todd": "I'm also having a bit of trouble with this.\nI've set the base_url parameter like so:\n'base_url' => 'https://s3.amazonaws.com/my-bucket/',\n 'key'      => $key,\n 'secret'   => $secret,\nHowever, when I call $s3Client->getObjectUrl('my-bucket', 'file.txt') \nI always get back https://my-bucket.s3.amazonaws.com/file.txt\nAm I misunderstanding how this feature works?\n. Per the conversation on Gitter with @jeremeamia it was explained that changing the getObjectUrl call to the following fixed this issue:\nphp\n$client = S3Client::factory(\n    'key' => $key,\n    'secret' => $secret,\n    'base_url' => 'http://s3.example.com/my-bucket/',\n);\n$url = $client->getObjectUrl('my-bucket', 'file.txt', null, ['PathStyle' => true])\nSo the url would now resolve as: http://s3.example.com/my-bucket/file.txt\n. ",
    "yellow1912": "This workaround does allow me to use getObjectUrl correctly, but the upload feature won't work till I applies another trick posted also by @jeremeamia \nhttps://gist.github.com/leftnode/e3107493f078516555f3\nHopefully this will help someone saves hours of fiddling with the code.\n. Unfortunately I celebrated too early. It seems like by setting host I generate another error:\n\"The request signature we calculated does not match the signature you provided. Check your key and signing method.\"\n. ",
    "hdave30": "I am trying to use the delete_all_objects to delete all contents in my S3 bucket. This is how I have been using it.\n$folder = \"#uploaded/uploaded_files_original/.*#\";\n$s3Client -> delete_all_objects('bucket',$folder);\n. ",
    "dcarrillo": "Hi again,\nAfter this change,  when you're using phar including, the require has to be like that:\nrequire 'phar://aws.phar/aws-autoloader.php\nUntil now, and according to doc (http://docs.aws.amazon.com/aws-sdk-php/guide/latest/quick-start.html#including-the-sdk), including was just\nrequire '/path/to/aws.phar';\nSo, if this is the expected behaviour, documentation has to be updated. \nKind regards.\n. Tested again, it works as usual and as expected.\nThank you very much.\n. ",
    "svenvarkel": "Hi!\nI downloaded the phar from here: https://github.com/aws/aws-sdk-php/releases\nIt works fine by downloading it from the link that you gave: https://github.com/aws/aws-sdk-php/releases/download/2.6.16/aws.phar#\nbr,\nSven\n. ",
    "saj1919": "@jeremeamia  i will attach logs but can you explain about String format normally used for dynamodb. Is using non utf-8 in string is a problem ? \n. ",
    "inakianduaga": "\nUsing the latest version of the SDK, is this still an issue?\n\nIt was the latest at the time of writing this issue, I haven't looked at the problem since then, since it's not a dealbreaker for my application (I can just reupload without checking for existance first, see below).\n\nWhat is the intention behind checking whether or not it exists before uploading?\n\nI wanted to avoid reuploading if the file already existed on the assumption that it would be slower to reupload than to check for existance. But since I couldn't get the doesObjectExist method to work, at the moment I'm basically reuploading every time, which works just fine but might be (maybe) suboptimal.\nAt any rate, the Laravel framework is including a built in S3 implementation driver in the upcoming 5.0 release (based on AWS php-sdk), so I'll probably switch to that at some point, and I'm imagining everything will be working there since it should be heavily tested.\n. Yeah, that's fine, I really have no time to look into this at the moment\n. ",
    "Mihailoff": "Have found a solution https://github.com/aws/aws-sdk-php/blob/f91d8b6b380da2633e39dbe791a70af9fe6e846a/src/Aws/Common/Enum/ClientOptions.php#L58\nThis should work\nnew Guzzle\\Cache\\DoctrineCacheAdapter(new Doctrine\\Common\\Cache\\FilesystemCache('/path'));\n. ",
    "annoyingmouse": "Thank you @jeremeamia for inviting my input - I'm afraid I'm not sure what I can offer though. It looks like the best way to get the JSON into DynamoDB would be to pass a correctly formatted string though... I'm not sure how this would mess the current SDK though as I'm only taking baby-steps with DynamoDB at the minute. Sorry to of such little help but you did ask ;-)\n. ",
    "sapphirecat": "Hmm, it seems I symlinked ~/.aws/config to ~/.aws/credentials at one point.  This is probably not a bug, then.\n. That's good to hear.  Thanks!\n. ",
    "flagoworld": "Did that - doesn't seem to fix it. :[\nI am using default... us-east-1\n. Restarted apache - I'm not using FPM for the moment.\nI have tried contacting other services including logging in to S3 with a GUI and the same credentials my application uses. Only the application using the PHP library is affected.\n!!\nDisregard my last comment. I don't know what happened during my test, but it SEEMS to be working now after restarting apache. Thanks! Will close this if it does in fact work consistently\n. ",
    "rstaats": "Hi, Thanks for posting this solution, you saved my day! Note: an apachectl graceful will not suffice, this didn't start working until I did a full on httpd restart. \n. ",
    "shorif2000": "i am still getting this error. ",
    "ram0l-zz": "+1\n. ",
    "krzaczek": "Ok I think I found a solution.\nWhen creating S3Client i can pass an instance of Aws\\Common\\Signature\\SignatureInterface to the 'signature' parameter.\n. Here is the stack trace.\n``` log\n[Fri, 17 Oct 14 09:20:38 +0200] [ERROR] Not a valid S3 endpoint: https://leof.localhost/bucket/avatars/22/0/9/5196265.jpg : #0 /opt/web/php-jabol-startup/vendor/aws/aws-sdk-php/src/Common/Signature/S3Signature.php(158): Aws\\S3\\S3UriParser->parse('https://leofs.local...')\n1 /opt/web/php-jabol-startup/vendor/aws/aws-sdk-php/src/Common/Signature/S3Signature.php(129): Aws\\Common\\Signature\\S3Signature->createCanonicalizedResource(Object(GuzzleHttp\\Message\\Request))\n2 /opt/web/php-jabol-startup/vendor/aws/aws-sdk-php/src/Common/Signature/S3Signature.php(52): Aws\\Common\\Signature\\S3Signature->createCanonicalizedString(Object(GuzzleHttp\\Message\\Request))\n3 /opt/web/php-jabol-startup/vendor/aws/aws-sdk-php/src/Common/Subscriber/Signature.php(42): Aws\\Common\\Signature\\S3Signature->signRequest(Object(GuzzleHttp\\Message\\Request), Object(Aws\\Common\\Credentials\\Credentials))\n4 /opt/web/php-jabol-startup/vendor/guzzlehttp/guzzle/src/Event/Emitter.php(109): Aws\\Common\\Subscriber\\Signature->onBefore(Object(GuzzleHttp\\Event\\BeforeEvent), 'before')\n5 /opt/web/php-jabol-startup/vendor/guzzlehttp/guzzle/src/RequestFsm.php(126): GuzzleHttp\\Event\\Emitter->emit('before', Object(GuzzleHttp\\Event\\BeforeEvent))\n6 /opt/web/php-jabol-startup/vendor/guzzlehttp/guzzle/src/RequestFsm.php(91): GuzzleHttp\\RequestFsm->before(Object(GuzzleHttp\\Transaction))\n7 /opt/web/php-jabol-startup/vendor/guzzlehttp/guzzle/src/Client.php(256): GuzzleHttp\\RequestFsm->__invoke(Object(GuzzleHttp\\Transaction))\n8 /opt/web/php-jabol-startup/vendor/guzzlehttp/command/src/AbstractClient.php(88): GuzzleHttp\\Client->send(Object(GuzzleHttp\\Message\\Request))\n9 /opt/web/php-jabol-startup/vendor/aws/aws-sdk-php/src/Common/AwsClient.php(152): GuzzleHttp\\Command\\AbstractClient->execute(Object(GuzzleHttp\\Command\\Command))\n10 /opt/web/php-jabol-startup/vendor/guzzlehttp/command/src/AbstractClient.php(76): Aws\\Common\\AwsClient->execute(Object(GuzzleHttp\\Command\\Command))\n11 /opt/web/php-jabol-startup/vendor/freshmind/jabol/library/Jabol/Cdn/Media.php(284): GuzzleHttp\\Command\\AbstractClient->__call('headObject', Array)\n12 /opt/web/php-jabol-startup/vendor/freshmind/jabol/library/Jabol/Cdn/Media.php(284): Aws\\S3\\S3Client->headObject(Array)\n13 /opt/web/php-jabol-startup/vendor/freshmind/jabol/library/Jabol/Cdn/Media.php(96): Jabol\\Cdn\\Media->getS3()\n14 /opt/web/php-jabol-startup/app/frontend/controllers/MediaController.php(37): Jabol\\Cdn\\Media->serve()\n15 [internal function]: MediaController->CatalogAction('cache', 'vH72uWECvs4YMPQ...', 'orbrazek-10.jpg')\n16 [internal function]: Phalcon\\Dispatcher->dispatch()\n17 /opt/web/php-jabol-startup/library/App/Core/Bootstrap.php(91): Phalcon\\Mvc\\Application->handle()\n18 /opt/web/php-jabol-startup/public/index.php(21): App\\Core\\Bootstrap->run(Array)\n19 {main}\n```\n. Perfect thx.\npawel (at) mobile\n\nOn 24 pa\u017a 2014, at 23:53, Michael Dowling notifications@github.com wrote:\nClosed #366.\n\u2014\nReply to this email directly or view it on GitHub.\n. Hi @mtdowling \n\nCurrent dev-v3 breaks the signature. Let me show You why. Let's say we are requesting a file https://leofs.localhost/cdn-sciaga-beta/avatars/21/11/8/2675262.jpg \nphp\n                $s3 = S3Client::factory([\n                        'credentials'       => $newCredentials,\n                        'version'           => '2006-03-01',\n                        'endpoint'          => 'https://leofs.localhost',\n                        'force_path_style'  => true,\n                        'defaults'          => [\n                            'Bucket'        => $Di->get('config')->aws->s3->bucket\n                        ]\n                    ]);\nThis will result in an 403 error and here is why.\nIn file Aws\\S3\\S3UriParser.php on line 42 $this->parseCustomEndpoint($url); will output\nphp\narray (\n  'path_style' => true,\n  'bucket' => 'cdn-sciaga-beta',\n  'key' => 'avatars', --->> THIS IS WRONG\n  'region' => NULL,\n)\nThis is because the url is parsed by spliting it by \"/\" and assuming that it's always path_style and the key part doesn't include \"/\" character.\nPawel\n. @mtdowling now it works .. as long as You set 'force_path_style'  => true. \npawel\n. ",
    "netroby": "Add use_path_style_endpoint => true may help. It need to be add to configure document. If is only S3Client, you can mark as [s3client only]\nThe following example shows how to pass options into an Amazon S3 client constructor. You using s3client as configure example. but not show us the addition configure option. That's not good for user.\nNot every one will first to look at API document. ",
    "aanton": "Thanks for clarifying and the quick response!\nWe are using a library (FlySystem) and found this behavior using the \"rename\" & \"copy\" methods, but we can not set any additional options (so the object is always copied as private). So we decide to investigate first this great SDK and clarify it it was a bug or not. Now we will ask for support to the FlySystem team, or look for workarounds.\nRegards,\nArmando\n. ",
    "iwai": "New feature request.\nstream upload file to S3 from stream in event loop, I tried this code.\n``` php\nuse React\\EventLoop;\nuse Aws\\S3\\S3Client;\nuse WyriHaximus\\React\\RingPHP\\HttpClientAdapter;\nuse Guzzle\\Http\\EntityBody;\n$loop  = EventLoop\\Factory::create();\n$s3 = S3Client::factory([\n    'version' => 'latest',\n    'key'     =>  '',\n    'secret'  =>  '',\n    'ringphp_handler'  => new HttpClientAdapter($loop)\n]);\n$s3->putObject([\n    'Bucket'        => 'backetName',\n    'Key'           => 'example/2005992462970.jpeg',\n    'Body'          => EntityBody::factory(fopen('/tmp/2005992462970.jpeg', 'r+')),\n    // or 'Body' => \\GuzzleHttp\\Stream\\Stream::factory(fopen('/tmp/2005992462970.jpeg', 'r+')),\n    '@future'       => true\n])->then(function ($response) {\n    echo 'Complete';\n});\n$loop->run();\n```\nphp\nexception 'GuzzleHttp\\Ring\\Exception\\RingException' with message 'Waiting did not resolve future' in vendor/guzzlehttp/ringphp/src/Future/BaseFutureTrait.php:58\nOr there might be a problem with the HttpClientAdapter?\nHowever, dynamodb putItem worked.\n. ",
    "choiks141": "thanks :)\n. ",
    "ricardclau": "Hi @jeremeamia thanks for replying back this fast\nWire Log for the updateRecords call:\n```\nRequest:\nPOST /identitypools/us-east-1%3A87469e67-9916-44d2-9514-5049c6e81e17/identities/us-east-1%3Aa5dfeaec-6263-4074-a456-4f49125c7365/datasets/test HTTP/1.1\nHost: cognito-sync.us-east-1.amazonaws.com\nUser-Agent: aws-sdk-php2/2.7.1 Guzzle/3.9.2 curl/7.30.0 PHP/5.6.2\nContent-Type: application/json\nx-amz-date: 20141020T162530Z\nAuthorization: AWS4-HMAC-SHA256 Credential=AKIAIHL7XESDCUX3X5GQ/20141020/us-east-1/cognito-sync/aws4_request, SignedHeaders=host;x-amz-date, Signature=ed5e54150b4000279762622808a86c88a6493b4c02930d48739fe4e1049be30b\nContent-Length: 491\n{\"RecordPatches\":[{\"Op\":\"replace\",\"Key\":\"keytest\",\"Value\":\"valuetest1413822330\",\"SyncCount\":0,\"DeviceLastModifiedDate\":\"2014-10-20T16:25:30Z\"}],\"SyncSessionToken\":\"H4sIAAAAAAAAAB3OUW+CMBiF4b\\/UumGCyS4YhQorXyMUanunQialncbBBv31w92\\/58npltzrI\\/iTDKdKHnre5z2Y6FebCEGFMCcXzEQya3EYOUmtqtACogg0aZ0Wn1j5rGdxbrv9+\\/3iUvR0MnObgSSek2wBU8xg6te1uWvaTC21qKvtsDZ9eyxGLVPfxmjmLsGaaqtkY5hQz90IJHqBGF+B1BsmSgdkmAvTGPDFBqrsO3PltaXp+r\\/kSmL7b9Jm9fJwh3eM07P4kW455eIrqPedmbrt7ACn20d5W6ag+lBsfNghjN7+AIf0bpsHAQAA\"}\nResponse:\nHTTP/1.1 400 Bad Request\nx-amzn-RequestId: b61a91dc-5875-11e4-8903-bd7a59eefce4\nx-amzn-ErrorType: SerializationException:http://internal.amazon.com/coral/com.amazon.coral.service/\nContent-Type: application/json\nContent-Length: 85\nDate: Mon, 20 Oct 2014 16:25:31 GMT\nnnCoection: close\n{\"Message\":\"class java.lang.String can not be converted to milliseconds since epoch\"}\n```\nSo, DeviceLastModifiedDate is in ISO8601 format which I assume is created by either the SDK or Guzzle itself?\nI have also tried approach 2, but the timestamp is still serialized in ISO8601 format although I have changed cognitoSync-2014-06-30.php, am I missing something?\nPlease let me know if this is enough or I can help with some extra logs\n. Hi @jeremeamia the date formatting problem has disappeared in 2.7.2 (I patched wrongly cognitoSync-2014-06-30.php in my local code) but I am getting now a 403 credentials problem.\nIn the API it does not say the ClientContext is mandatory and AFAIK I am supplying all needed fields. Everything else I have tried seems to work which makes me think there is another bug here. \nI am providing the logs in case they are helpful\n```\nRequest:\nPOST /identitypools/us-east-1%3A87469e67-9916-44d2-9514-5049c6e81e17/identities/us-east-1%3Aa5dfeaec-6263-4074-a456-4f49125c7365/datasets/test HTTP/1.1\nHost: cognito-sync.us-east-1.amazonaws.com\nUser-Agent: aws-sdk-php2/2.7.2 Guzzle/3.9.2 curl/7.30.0 PHP/5.6.2\nContent-Type: application/json\nx-amz-date: 20141024T103450Z\nAuthorization: AWS4-HMAC-SHA256 Credential=AKIAIHL7XESDCUX3X5GQ/20141024/us-east-1/cognito-sync/aws4_request, SignedHeaders=host;x-amz-date, Signature=1a1327177c46dcba95fdecd6258633f9cfa90f951e903e026fe37f397cb54481\nContent-Length: 483\n{\"RecordPatches\":[{\"Op\":\"replace\",\"Key\":\"keytest\",\"Value\":\"valuetest1414146890\",\"SyncCount\":0,\"DeviceLastModifiedDate\":1414146890}],\"SyncSessionToken\":\"H4sIAAAAAAAAAB3O3U6DMBiA4QvagS3Mny3xYFLoyuhHYIXSnpnBQgsoiaDQqxc9f\\/PkbdbY6QrcuzzMV5mZ1DCP27xTNhq4QVhZ9ZiIcNEim5SMWzC416Q0mnRO22wPhJkkiPvm\\/Dbehgj9Ocx+LkBCxEXmc9H9gFP+1oyalnNNe9QUfbc1pq74pGXk6gAt6RBiTXWvZGkToVawfAJy8iHALZDCS0Q+AOkWbksLjntwZV9syNuaRtt\\/niqJ+3+TlpsXH474eM9fHhhV+\\/NuvNfT+uzSG3qa6cXt\\/O8L58UpCCo7zuiDoNdfLMwmWAcBAAA=\"}\n```\n```\nResponse:\nHTTP/1.1 403 Forbidden\nx-amzn-RequestId: 6272023c-5b69-11e4-a087-954ee4c655e2\nx-amzn-ErrorType: NotAuthorizedException:http://cognito-sync.amazonaws.com/doc/2014-06-30/\nContent-Type: application/json\nContent-Length: 73\nDate: Fri, 24 Oct 2014 10:34:50 GMT\n{\"message\":\"These credentials are not valid for accessing this resource\"}\n```\n. ",
    "eroux": "Thanks for your answers. The environment is a freshly created Beanstalk Environment, with the default Webserver, PHP configuration, and latest version of AWS PHP SDK. curl_version returns something like \n[\"version_number\"]=468480\n  [\"age\"]=3\n  [\"features\"]=165789\n  [\"ssl_version_number\"]=0\n  [\"version\"]= \"7.38.0\"\n  [\"ssl_version\"]= \"NSS/3.16.2 Basic ECC\"\nand adding 'curl.options' => [CURLOPT_VERBOSE => true] to my DynamoDB factory doesn't change anything to the verbosity of the error...\nIt happens on around 0.5% of the requests in my tests... at first, when I start to have these errors, they appear on almost all requests.\nAny clue?\n. Hmm, it seems you were right, looking at the log, curl is not more verbose, but I sometimes get\n[Wed Oct 29 09:45:43.828957 2014] [mpm_prefork:error] [pid 2743] (12)Cannot allocate memory: AH00159: fork: Unable to fork new process\nso I guess it's what you said... Thanks for helping me debugging! \nShould I just improve the memory limit of PHP, or should I change EC2 instance? If I should change my instance, what do you think would be appropriate for handling around 1000 requests/second (though I know it's not the right place to ask)?\n. I have to admit I'm a bit surprised by the lack of attention to this... Am I the only person in the world trying to get private URLs for buckets in central Europe?\n. ",
    "adamf321": "I've created a test and uploaded it to our server, you can run it by going to http://cep.eduxy.net/CepWizard/asset/plustest\nThe code it's running is:\n\n$this->initConfig();\n        $s3 = S3Client::factory([\n            'curl.options' => [CURLOPT_VERBOSE => true], // Show what's sent over the wire\n            'key'    => $this->config['cep']['s3']['key'],\n            'secret' => $this->config['cep']['s3']['secret']\n        ]);\n        $bucket = $this->config['cep']['s3']['bucket'];\n$s3->copyObject(['Bucket' => $bucket, 'Key' => 'plus-copy', 'CopySource' => $bucket.'/test(+7).mp4']);\n\nThe file test(+7).mp4 exists in the route of the bucket.\nIf I run another test with a file that has no plus, then run the plus test again, it works. I guess for now we'll just strip pluses out of filenames to get it working.\n. ",
    "johannesnagl": "looks like there's an open issue #380 for that!\n. ",
    "recarv": "Yeah, that was the first thing I updated.  Using:\nComposer version 1.0-dev (a309e1d89ded6919935a842faeaed8e888fbfe37) 2014-10-20 19:16:14\n. Is there anything that I can do to provide more information on Composer?  I've read some people have issue with phpUnit, and doing a dump, I'm not that familiar with Composer.\n. The paths are correct unfortunately. I have not tried asking in the composer IRC channel, but I will do so.\n. ",
    "jbouzekri": "Sorry I forgot to provide this information. It is the latest stable version of the v2 branch : 2.7.2\n. hihi, thanks for your feedback.\n. ",
    "matt-d": "Any plans to release an updated deb for ubuntu?  \nWe use userdata via a launch configuration for our deploy/upgrade processes - being able to do:\n!/bin/bash\napt-get update\napt-get install -y php5-cli php-aws-sdk\n...\nwithout the need for a full composer install is pretty handy...\n. ",
    "warroyo": "I had the clientToken set by accident...doh\n. ",
    "fpapadopou": "Thank you both! Best regards @skyzyx @mtdowling \n. ",
    "ando-masaki": "Thanks. And I have a comment to your commit.\nSee https://github.com/aws/aws-sdk-php/commit/fb0d0b98918477af3b46165036928493c6b8a72b#diff-bd7189632068bf9a2d501a8022c43342R72 .\n. Oh, I see. I was consented it.\nYhank you very much to explain clearly for me.\n. ",
    "helmutschneider": "Possibly related:\n$client->putObject([\n    'Key' => 'cars',\n    'Bucket' => 'SOME-BUCKET',\n    'Body' => ''\n]);\nFails with the following on eu-central-1:\nAws\\S3\\Exception\\S3Exception: AWS Error Code: XAmzContentSHA256Mismatch, Status Code: 400, AWS Request ID: 6A71FC42EF0E8116, AWS Error Type: client, AWS Error Message: The provided 'x-amz-content-sha256' header does not match what was computed., User-Agent: aws-sdk-php2/2.7.3 Guzzle/3.9.2 curl/7.37.1 PHP/5.5.14\nThe error only appears on requests with an empty body. Works if these lines are removed:\nhttps://github.com/aws/aws-sdk-php/blob/master/src/Aws/S3/S3SignatureV4.php#L50-52\n. :+1: \n. ",
    "marcinx": "Found another missing exception: OperationNotPermittedException\n. ",
    "tmugford": "If it helps, I\u2019m using 2.7.5 via Composer. Here\u2019s a simplified code sample:\n``` php\nrequire 'vendor/autoload.php';\nuse Aws\\CloudFront\\CloudFrontClient;\n$cloudFront = CloudFrontClient::factory(array(\n  'key' => 'key',\n  'secret' => 'secret',\n  'region' => 'region',\n  'private_key' => 'private_key',\n  'key_pair_id' => 'key_pair_id',\n));\n$signed_url = $cloudFront->getSignedUrl(array(\n  'url' => 'http://bucket/object?response-content-disposition=attachment;filename=foobar.txt',\n  'expires' => time() + 300, // 5 minutes\n));\n``\n. OK, it seems I\u2019ve misunderstood. It appears that usingrawurlencodeon the query string argument _does_ work. In retrospect, it\u2019s obvious why this is required given the characters that must appear in the value forresponse-content-disposition(namely;and=`), and therefore why Guzzle encodes the value as well.\n. ",
    "abhinavlal": "You are getting curl error code 7. As documented here - http://curl.haxx.se/libcurl/c/libcurl-errors.html it means CURLE_COULDNT_CONNECT (7) Failed to connect() to host or proxy. Can you check if you the internet connectivity on your server. Also there might be firewall blocking all outgoing connection.\n. ",
    "sysoftni": "Thanks for reply guys...\ninternet connectivity is there and firewall is also disabled.\nbut if you open the url(where curl 7 error comes) https://s3-us-west-2.amazonaws.com/xn--lsin-loa.se , it shows access denied. but i dont think problem is with that as it works perfectly on localhost..\nHope i'll able to solve this soon...\n. ",
    "EronHennessey": "(I know the issue is closed now, but wanted to add...) The SWF team has been duly notified!\n. ",
    "dookzoom": "Perfect!  Thank you so much.\nI wonder if you can help me with one more thing,\nI can't seem to get the ReturnConsumedCapacity.  Here's what I have.\n$item = [\n                        'p' => ['N' => $pid],\n                        'pw' => ['S' => $randomString],\n```\n                ];\n            $response = $dynamodbClient->putItem([\n                'TableName' => 'playerState',\n                'Item' => $item,\n                'Expected' => [\n                    'p' => ['Exists' => false]\n                ],\n                'ReturnConsumedCapacity' => 'TOTAL',\n            ]);\n\n```\nprint_r($response);\nAll I get is this:\n(\n    [structure:protected] =>\n    [data:protected] => Array\n        (\n        )\n)\nIs it because I am on DynamoDb Local? Or am i doing something wrong?\nSincerely,\nDuc\n. Great thanks again!\nI'm sure I read that before but forgot.\nSincerely,\nDuc\n\nFrom: Jeremy Lindblom [notifications@github.com]\nSent: Tuesday, December 02, 2014 5:09 PM\nTo: aws/aws-sdk-php\nCc: Duc Pham\nSubject: Re: [aws-sdk-php] How to use new data types M and L (#418)\nThe code looks correct, and I'm pretty sure it's only because you are using DynamoDB Local that the value does not appear in the result. See http://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Tools.DynamoDBLocal.html#Tools.DynamoDBLocal.Differences\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/aws/aws-sdk-php/issues/418#issuecomment-65314547.\n. ",
    "adamaveray": "Turns out it's only in PHP5.5. Oh well!\n. ",
    "sokac": "\"name\": \"aws/aws-sdk-php\",\n        \"version\": \"3.0.0-beta.1\",\n        \"reference\": \"432d49bc25014b3646512f2822b5e004dcff7f4b\"\n. ",
    "andythorne": "Seems good! It will of course break anyone that calls the rmdir method directly, but then again they shouldn't really be calling it anyway...\n. I agree it's not really a breaking change\n. Yep, exactly the same. I replicated the fix in #192 for master and it worked. \n. No problem, I'll have to get back to you on Monday though as I'm away all weekend! \n. getimagesize() never sets $whense to SEEK_END, so i'm not sure how that fixed the problem. (see https://github.com/php/php-src/blob/master/ext/standard/image.c#L480)\nAnyway, I've spend a long time debugging and there's certainly some strange behaviour happening, but I can only replicate it with 1 image atm (https://gist.github.com/andythorne/d636142a777407073695)\nThis is the current trace flow with the current CachingStream:\n- image size (from AWS ContentLength: 24167 header)\n- call getimagesize() (with seekable => true)\n- read 8192 bytes from remote stream and added to local stream\n- eof() called, returns false\n- seek on 9392 bytes with $whence = SEEK_SET\n  - read in 1200 bytes (9392 - 8192 bytes)\n  - S3 returns 397 bytes (seems odd, like it's reading to a certain block size)\n- eof() called, returns false\n- read 8192 bytes\n- ends and getimagesize() returns false\nThe test doesn't reflect the fix for the seekable length - this  PR will allow you to override the CachingStream class.\nWe are using https://gist.github.com/andythorne/d1d036abfd2dbdc5e60c#file-seekablecachingstream-php-L70 atm to do this. The key lines are 70-72. Where in the above flow the stream only reads 397 bytes, this code sees it's still behind and carries on reading. The next read correctly fetches all the request bytes. \nSo, maybe the proper fix is in the Guzzle CachingStream class that needs to loop the read in seek to check we have forward-seeked enough?\n. Yeah I agree, the more I investigated the more sense it made to patch Guzzle.\nI'll modify the PR to use a callable tomorrow.\n. Updated, you can now set seekable_handler to a callable to return a instance of StreamInterface\n. Updated\n. It offers the ability to customise the seekable stream, so it offers some value - but it's no longer directly tied to the guzzle changes.\n. @mtdowling Yeah I think that's fair. The original bug was a guzzle one, so I'll close this one down. Thanks for your help @jeskew\n. ",
    "mym": "Damn, was indeed a version issue... thanks a lot :)\n. ",
    "agokaiso": "```\nfunction create() {\n    $params = array(\n        'DBInstanceClass' => 'db.m3.large',\n        'Engine' => 'MySQL',\n        'MasterUsername' => 'xx',\n        'VpcSecurityGroupIds' => array('sg-xxxxxxxx'),\n        'DBSubnetGroupName' => 'xx',\n        'PreferredMaintenanceWindow' => 'wed:10:00-wed:10:30',\n        'DBParameterGroupName' => 'xx',\n        'BackupRetentionPeriod' => 7,\n        'PreferredBackupWindow' => '19:00-19:30',\n        'MultiAZ' => true,\n        'EngineVersion' => '5.5.40a',\n        'AutoMinorVersionUpgrade' => true,\n    );\n/*\n$params['DBInstanceIdentifier'] = 'xx';\n$params['AllocatedStorage'] = 100;\n$params['StorageType'] = 'standard';                        // Magnetic\n$result = $this->client->createDBInstance($params);\n*/\n\n\n$params['DBInstanceIdentifier'] = 'xx';\n$params['AllocatedStorage'] = 100;\n$params['StorageType'] = 'gp2';                             // Magnetic\n// $params['Iops'] = 1000;                                  // Provisioned IOPS (SSD)\nprint_r($params);\n\n$result = $this->client->createDBInstance($params);\n\n}\n```\n. ",
    "anextro": "( ! ) Fatal error: Uncaught exception 'Guzzle\\Http\\Exception\\CurlException' with message ' in C:\\wamp\\www\\sappy\\vendor\\guzzle\\guzzle\\src\\Guzzle\\Http\\Curl\\CurlMulti.php on line 561\n( ! ) Guzzle\\Http\\Exception\\CurlException: [curl] 56: SSL read: error:00000000:lib(0):func(0):reason(0),  'content_type' => 'application/xml', 'http_code' => 400, 'header_size' => 287, 'request_size' => 432, 'filetime' => -1, 'ssl_verify_result' => 0, 'redirect_count' => 0, 'total_time' => 1.0629999999999999, 'namelookup_time' => 0, 'connect_time' => 0.17199999999999999, 'pretransfer_time' => 0.51600000000000001, 'size_upload' => 21665, 'size_download' => 0, 'speed_download' => 0, 'speed_upload' => 20380, 'download_content_length' => -1, 'upload_content_length' => 7876, 'starttransfer_time' => 0.51600000000000001, 'redirect_time' => 0, 'redirect_url' => '', 'primary_ip' => '54.231.136.80', 'certinfo' => array ( ), 'primary_port' => 443, 'local_ip' => '192.168.1.5', 'local_port' => 59495, ) [debug] in C:\\wamp\\sappy\\vendor\\guzzle\\guzzle\\src\\Guzzle\\Http\\Curl\\CurlMulti.php on line 561\nCall Stack\nTime    Memory  Function    Location\n1   0.0037  158272  {main}( )   ..\\index.php:0\n2   0.0749  1147592 require_once( 'C:\\wamp\\www\\sappy\\system\\core\\CodeIgniter.php' ) ..\\index.php:293\n3   2.3512  3409944 call_user_func_array:{C:\\wamp\\www\\m-ecommerce\\system\\core\\CodeIgniter.php:359} ( )  ..\\CodeIgniter.php:359\n4   2.3513  3410208 ImageFix->callpipe( )   ..\\CodeIgniter.php:359\n5   29.7650 9391784 ImagePipe->main( )  ..\\imagefix.php:115\n6   29.7650 9391848 ImagePipe->pipeImage( ) ..\\ImagePipe.php:27\n7   191.7043    10480832    ImageCrop->uploadImage( )   ..\\ImagePipe.php:68\n8   191.7067    10481840    ImageCrop->saveDifferentSizes( )    ..\\ImageCrop.php:109\n9   193.6928    10609232    Awss3->uploadPublic( )  ..\\ImageCrop.php:314\n10  193.7204    10610152    putObject ( )   ..\\awss3.php:130\n11  193.7204    10610352    Aws\\Common\\Client\\AbstractClient->__call( ) ..\\awss3.php:130\n12  193.7204    10610408    Guzzle\\Service\\Client->__call( )    ..\\AbstractClient.php:102\n13  193.7209    10611584    Guzzle\\Service\\Command\\AbstractCommand->getResult( )    ..\\Client.php:93\n14  193.7209    10611584    Guzzle\\Service\\Command\\AbstractCommand->execute( )  ..\\AbstractCommand.php:218\n15  193.7209    10611584    Guzzle\\Service\\Client->execute( )   ..\\AbstractCommand.php:167\n16  193.7546    10632344    Guzzle\\Http\\Client->send( ) ..\\Client.php:223\nI am using aws sdk version 2.0.1\nguzzle is < v3.\nI am trying to do multiple uploads at once. The number is say 1000. The upload works fine at 10 but since i have a large number i want to do more like 100 at once.\nStrategies i'm thinking of applying\n\nIncreasing my php max_execution_time setting in php.ini\nutilizing my sleep() function to slow down the script.\n. Thank you for spotting that.\n\nBut what could cause curl to throw an ssl read error only when i do\nplenty uploads but would not throw it when it's reduced ?\nOn 12/12/14, Michael Dowling notifications@github.com wrote:\n\nI see you got an SSL read error: [curl] 56: SSL read: error. This error is\nautomatically retries several times before failing. I suggest lowering the\nnumber of simultaneous requests you are sending to work around this problem.\nUpdating your version of cURL or OpenSSL/NSS could potentially help, but I'm\nnot sure if it will.\n\nReply to this email directly or view it on GitHub:\nhttps://github.com/aws/aws-sdk-php/issues/433#issuecomment-66836654\n. Okay. I will do that and get back to everyone on that.\nOn Dec 12, 2014 10:58 PM, \"Michael Dowling\" notifications@github.com\nwrote:\nOne potential way to try to figure out why it's happening would be to\nevaluate a tcpdump that shows the error occurring.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/aws/aws-sdk-php/issues/433#issuecomment-66841804.\n. \n",
    "mizuluffyhwang": "Have the same problem when increase the number of jobs. I'm using eBay API, think it may related to time out issue\n. ",
    "nickschuch": "All by hand :)\nThanks for adding this! Ill take it for a spin.\n. ",
    "DrySs": "Hey jeremeamia,\nAnd thanks for your reply.\nBut if I remove param 'ACL' and only use 'Grants' and 'Owner' params (like in the screenshot I provided in my first post), then I get the following error:\nThe XML you provided was not well-formed or did not validate against our published schema\nIs there any way in PHP SDK to get the XML sent to Amazon, this way I'll be able to understand where the XML is not well-formed?\nThe params I used with s3->putBucketAcl() are the same in previous screenshot, without 'ACL' you told me to remove (because I want to use Access Control Policies).\nThanks for your help\n. Hmm tryed to use LogPlugin, but not sure this is the best way to understand why AWS reply a bad request, because I cannot get XML details:\n. @jeremeamia: WON-DER-FULL !!!\nYou're amazing, you found my bug :-)\nMany many many thanks for your help.\n. Thanks for your reply :)\nI am the only one to have this kind of slow latency?\nI know that xdebug slows down, but even without xdebug, request are really slow...\nIf I go deeper in xdebug report, it's always coming from cURL operations, with big big load time...\n. I'm in France and trying to request Ireland, there is no region closer to Paris than Ireland's one...\n208 ms for a request is veeeeeeeryyyy looooong.\nIf you have to do many calls to API (not concurrently, because your process have logical order), it can take many seconds to load a page!\n. Thanks for your reply :)\nTryed the following simple code but this does not return XML sent to AWS:\n$this->cdn->addSubscriber(LogPlugin::getDebugPlugin());\nWhich code to use to get XML sent with wire logger?\nThanks for your help :+1: \n. My bad, it does return XML!\n. Note this is the same behavior while creating new distribution with createDistribution() method.\n. I currently use 2.7.12... I try to update right now :)\n. Wonderfull, it's now working :)\nAs usual, many thanks for your very fast reply and perfect help !! :+1: \n. My bad, it's working well :)\nBut the documentation is wrong about PriceClass :p\n. Hi @mtdowling :)\nYou're right, using urlencode before sending string to getSignedUrl make it works!\nThanks :)\n. ",
    "chiukit": "Thanks for your reply. I am using 2.7.11 which is not Version 1 of the SDK. BTW, I have fixed this issue by myself and just to report what happen after PHP upgraded.\n. ",
    "ratilalispl": "remove this line curl_setopt($curl_handle, CURLOPT_CLOSEPOLICY, CURLCLOSEPOLICY_LEAST_RECENTLY_USED) from  file lib/requestcore/requestcore.class.php because This option is deprecated.. ",
    "itecedor": "Yes gc is enabled. Memory used before the changeResourceRecordSets call is 2883584. Increasing the memory limit does seem to allow the request to complete... didn't occur to me to try that as this is the limit we have set on production! Will ask the sysadmin. Thank you, sorry for the stupid question.\n. ",
    "otggroup": "Michael you are a genius!!!! that was my issue.  \nThe AWS PHP docs show the select query separated by a comma as if you had to pass the NextToken separately.\nhttp://docs.aws.amazon.com/AWSSDKforPHP/latest/index.html#m=AmazonSDB/select\nAnd http://docs.aws.amazon.com/AWSSDKforPHP/latest/index.html#m=AmazonSDB/select\nMan maybe I just totally misread it but I am so happy you found my error. A million thanks to you...\nLouis\n. ",
    "abctaobaoa": "There is not StorageType about Action by PHP SDK.\n``` php\nRequest:\nPOST / HTTP/1.1\nHost: rds.ap-southeast-1.amazonaws.com\nUser-Agent: aws-sdk-php2/2.6.16 Guzzle/3.9.2 curl/7.37.1 PHP/5.5.19\nContent-Type: application/x-www-form-urlencoded; charset=utf-8\nx-amz-date: 20150106T030405Z\nAuthorization: AWS4-HMAC-SHA256 Credential=/20150106/ap-southeast-1/rds/aws4_request, SignedHeaders=content-type;host;x-amz-date, Signature=\nAction=CreateDBInstance&Version=2013-09-09&DBInstanceIdentifier=xx&AllocatedStorage=100&DBInstanceClass=db.m3.large&Engine=mysql&MasterUsername=xx&MasterUserPassword=xx&VpcSecurityGroupIds.member.1=xx&DBSubnetGroupName=xx&PreferredMaintenanceWindow=wed%3A08%3A00-wed%3A08%3A30&DBParameterGroupName=xx&BackupRetentionPeriod=7&PreferredBackupWindow=19%3A00-19%3A30&MultiAZ=true&EngineVersion=5.5.40a&AutoMinorVersionUpgrade=true\nResponse:\nHTTP/1.1 200 OK\nx-amzn-RequestId: ad893530-9550-11e4-8d61-af59e5528f86\nContent-Type: text/xml\nContent-Length: 2877\nDate: Tue, 06 Jan 2015 03:04:07 GMT\n\n\n\n7\ncreating\ntrue\n\n\nactive\nxx\n\n\nxx\n19:00-19:30\nwed:08:00-wed:08:30\n\nmysql\n\n****\n\ngeneral-public-license\n\nxx\nComplete\nxx\nxx\n\n\nActive\nxx\n\nap-southeast-1a\nfalse\n\n\n\nActive\nxx\n\nap-southeast-1b\nfalse\n\n\n\n\n\n\nin-sync\nxx\n\n\n5.5.40a\n\n\ndefault:mysql-5-5\nin-sync\n\n\n\nfalse\ntrue\n100\nxx\ndb.m3.large\n\n\n\nad893530-9550-11e4-8d61-af59e5528f86\n\n\n```\nIt's right by AWS CLI.\n``` shell\nRequest:\n2015-01-06 11:06:59,606 - MainThread - botocore.endpoint - DEBUG - Making request for  (verify_ssl=True) with params: {'query_string': '', 'headers': {}, 'url_path': '/', 'body': {'Engine': u'mysql', 'MultiAZ': 'true', 'DBParameterGroupName': u'xx', 'MasterUsername': u'xx', 'MasterUserPassword': u'xx', 'AutoMinorVersionUpgrade': 'true', 'PreferredBackupWindow': u'19:00-19:30', 'StorageType': u'gp2', 'Version': '2014-09-01', 'DBInstanceIdentifier': u'xx', 'AllocatedStorage': 100, 'EngineVersion': u'5.5.40a', 'Action': 'CreateDBInstance', 'DBInstanceClass': u'db.m3.large', 'DBSubnetGroupName': u'xx', 'VpcSecurityGroupIds.member.1': u'xx', 'PreferredMaintenanceWindow': u'wed:08:00-wed:08:30', 'BackupRetentionPeriod': 7}, 'method': 'POST'}\nResponse:\n2015-01-06 11:07:00,675 - MainThread - botocore.parsers - DEBUG - Response headers:\n{'content-length': '2774',\n 'content-type': 'text/xml',\n 'date': 'Tue, 06 Jan 2015 03:07:00 GMT',\n 'x-amzn-requestid': '14920212-9551-11e4-baa8-9ff8d4ef3dde'}\n2015-01-06 11:07:00,675 - MainThread - botocore.parsers - DEBUG - Response body:\n\n\n\n7\ncreating\ntrue\n\n\nactive\nxx\n\n\nxx\n19:00-19:30\nwed:08:00-wed:08:30\n\nmysql\n\n****\n\ngeneral-public-license\n\nxx\nComplete\nxx\nxx\n\n\nActive\nxx\n\nap-southeast-1a\n\n\n\nActive\nxx\n\nap-southeast-1b\n\n\n\n\n\n\nin-sync\nxx\n\n\n5.5.40a\n\n\ndefault:mysql-5-5\nin-sync\n\n\nfalse\n\ntrue\ngp2\n100\ndb.m3.large\nxx\n\n\n\n14920212-9551-11e4-baa8-9ff8d4ef3dde\n\n\n```\n. ",
    "vjroby": "Come a bit late, I was using 2.7.13. Thanks.\nBest regards.\n. ",
    "syrakozz": "thank you, \nis there any future release that it will calculate ContentLength for the stream\nit will be very usefull\n. ",
    "hostep": "Ow ok, I didn't knew this.\nYes there is another autoloader in the picture: the one from the Magento e-commerce system.\nI tried to use your library in a Magento system and Magento uses its own autoloader. So I had to adapt your autoloader a little bit by adding a third 'true' param to the spl_autoload_register function, which prepends your autoloading to the default autoloader from Magento. (I'm using the zip version btw)\nIs this enough info?\nIt's not a big problem for us, I simply noticed it while testing something out.\n. ",
    "marcelodornelas": "Hi, \nI'm creating CNAME records programatically and I'm using 'changeResourceRecordSets'\nI'm using the latest stable SDK3 and the 'SetIdentifier' parameter should not be required but the call doesn't work without it (it return an invalid request / InvalidInput error). \nAny ideas?\nMany thanks,\nMarcelo\nAws\\Route53\\Exception\\Route53Exception: Error executing \"ChangeResourceRecordSets\" on \"https://route53.amazonaws.com/2013-04-01/hostedzone/Z3OG2D51K8XHLI/rrset/\"; AWS HTTP error: Client error: 400 InvalidInput (client): Invalid request - <?xml version=\"1.0\"?> SenderInvalidInputInvalid request7bc3f118-2bb2-11e5-aa40-9b497bc5150b in /Applications/MAMP/htdocs/admin.local/httpdocs/aws/vendor/aws/aws-sdk-php/src/WrappedHttpHandler.php on line 152\n\n. ",
    "mooseh": "Im experiencing an issue when sending a delete to an A record when there is more than 1 with the same name just weighted difference. I get \"InvalidChangeBatchTried to delete resource record set [name='~REMOVED~.', type='A'] but it was not found\" although that was generated from A listResourceRecordSets\n. ",
    "jaydens": "Hi @mtdowling - I had tried that prior as I read in docs however it still doesn't catch the error.\n. ",
    "TiagoGouvea": "Worked to me:\ntry {\n        /* @var $result \\Aws\\Result */\n        $result = $sesClient->sendEmail($args);\n    } catch (\\Aws\\Ses\\Exception\\SesException $e) {\n        // Ok\n    }\n. ",
    "auralon": "OK, it seems that the Access Key ID was in fact being incorrectly passed to the factory method.  Meaning the \"The AWS Access Key Id you provided does not exist in our records.\" message was the one I needed to pay attention to.  By adding the base_url/region I thought I was moving in the right direction, but it was a red herring.  Once I retrieve the credentials, I simply pass them to the s3 client factory method, as below:\n```\n$s3 = \\Aws\\S3\\S3Client::factory(array(\n    'key'    => $credentials->AccessKeyID,\n    'secret' => $credentials->SecretAccessKey,\n    'token' => $credentials->SessionToken\n));\n$result = $s3->putObject(array(\n    'Bucket'     => $credentials->BucketName,\n    'Key'        => $credentials->Key,\n    'SourceFile' => '/www/auralon/test.png',\n    'ACL'        => 'public-read',\n));\n```\nThis seems to have worked, sorry to waste your time!\n. Hopefully someone might find this post useful at some point, if they encounter the same problem!\n. ",
    "Maykonn": "I'm with the same error here\n. ",
    "taz77": "Would love to see Frankfurt support the same signing as all of the other regions. Frankfurt right now is completely useless.\n. ",
    "AlexanderMatveev": "@mtdowling @taz77  Please see http://stackoverflow.com/questions/26533245/the-authorization-mechanism-you-have-provided-is-not-supported-please-use-aws4\n\"http://docs.aws.amazon.com/AmazonS3/latest/dev/UsingAWSSDK.html explains how to enable V4 in the various SDKs, assuming you are using an SDK that has that capability.\"\n. ",
    "ye": "Argh, will run unit tests later before sending another pull request. \n. @jeremeamia albeit it's a bit of of less straightforward than my approach, that's still great. I wish it could be documented in the official doc. http://docs.aws.amazon.com/aws-sdk-php/guide/latest/credentials.html#credential-profiles\n. ",
    "csdaraujo": "Figured it out. It1s a plain and simple JSON object where the key is the name of the expression and the value is the expression (as string).\n{\n    myexpr1: 'log10(field1)/field2',\n    myexpr2: 'field2*2+field3'\n}\nStill, I think that description can improve a lot. Can I help you guys with it?\n. ",
    "justechn": "Interesting, before I added base_url, I got a message like this \"The bucket you are attempting to access must be addressed using the specified endpoint. Please send all future requests to this endpoint: \"xxx.s3.amazonaws.com\".\" The xxx is the name of my bucket. That message made me think I needed the base_url. \nHowever, when I added \n$client = S3Client::factory(array(\n            'profile' => 'default',\n            'region' => 'us-west-2'\n        ));\nit seems to be working.\nThanks\nIt would be great if the error message actually provided useful information. As it is they are misleading and confusing.\n. ",
    "tistre": "Thanks for the reply! No offense meant, but no wonder you can\u2019t reproduce this error if you don't use my test case :-)\nThe error happens only if I modify the array returned by get->('Grants'), see my code above. Have you tried running it?\nThanks for the logging hint. Here\u2019s the output I get \u2013 the problem seems to be that the S3 client magically appends a zero to the bucket name (files.strehle.de turns into files.strehle.de0 on the second request).\n```\nRequest:\nGET /files.strehle.de/tim/webarchive/2011/08-16/73/61/file60zbihus8uom98wioxv.jpg?acl HTTP/1.1\nHost: s3.eu-central-1.amazonaws.com\nUser-Agent: aws-sdk-php2/2.7.18 Guzzle/3.9.2 curl/7.26.0 PHP/5.4.36-0+deb7u3\nx-amz-content-sha256: [REDACTED]\nx-amz-date: 20150217T102130Z\nAuthorization: AWS4-HMAC-SHA256 Credential=[REDACTED]/20150217/eu-central-1/s3/aws4_request, SignedHeaders=host;x-amz-content-sha256;x-amz-date, Signature=[REDACTED]\nResponse:\nHTTP/1.1 200 OK\nx-amz-id-2: [REDACTED]\nx-amz-request-id: 029A6283EF45ADAC\nDate: Tue, 17 Feb 2015 10:21:31 GMT\nContent-Type: application/xml\nTransfer-Encoding: chunked\nServer: AmazonS3\nxml version=\"1.0\" encoding=\"UTF-8\"?\n[REDACTED][REDACTED]FULL_CONTROL\nErrors: 0\nRequest:\nPUT /files.strehle.de0?acl HTTP/1.1\nHost: s3.eu-central-1.amazonaws.com\nUser-Agent: aws-sdk-php2/2.7.18 Guzzle/3.9.2 curl/7.26.0 PHP/5.4.36-0+deb7u3\nx-amz-acl: public-read\nx-amz-content-sha256: [REDACTED]\nx-amz-date: 20150217T102130Z\nAuthorization: AWS4-HMAC-SHA256 Credential=[REDACTED]/20150217/eu-central-1/s3/aws4_request, SignedHeaders=host;x-amz-acl;x-amz-content-sha256;x-amz-date, Signature=[REDACTED]\nContent-Length: 0\nResponse:\nHTTP/1.1 404 Not Found\nx-amz-request-id: 22B660812B007737\nx-amz-id-2: [REDACTED]\nContent-Type: application/xml\nTransfer-Encoding: chunked\nDate: Tue, 17 Feb 2015 10:21:30 GMT\nServer: AmazonS3\nxml version=\"1.0\" encoding=\"UTF-8\"?\nNoSuchBucketThe specified bucket does not existfiles.strehle.de022B660812B007737UsWriMFM6G7E8xttN8ucodoFta9WIyhxQOTqIkJzPdhF2gGBEjrmFAXWA8N2H7AFD7ESGdcE+FA=\nErrors: 0\n```\n. Argh. Sorry, I\u2019ve been dumb and blind. I overwrote $key in my test case :-( I apologize for wasting your time!\n. ",
    "cmsdroff": "Thanks for the feedback Jeremy, i've adjusted the composer file to update to the latest 2.* and have moved from 2.6 to 2.7, both methods now work, if only i'd known this earlier :)\n. ",
    "robert-dinu": "It wasn't an SDK issue. AWS API is poor documented.\n. ",
    "thovt93": "I have the same issue, have anyone fixed?. ",
    "SteveEdson": "I assumed the credentials were passed through automatically like in other services. I've got it working now though, thanks again for your help!\n. ",
    "bhill2588": "``` php\n        require_once('getid3.php');\n        require('aws-autoloader.php');\n    // Initialize getID3 engine\n    $getID3 = new getID3;\n\n  $accessKey       = '{REDACTED}';\n  $bucket          = '{REDACTED}';\n  $server          = \"{REDACTED}\";\n  $user            = \"admin\";\n  $pass            = \"{REDACTED}\";\n  $database        = \"{REDACTED}\";\n  $date            = new MongoDate();\n  $DirectoryToScan = 'C:\\moozik\\public\\data\\mp3'; // change to whatever directory you want to scan\n  $dir             = opendir($DirectoryToScan);\n  $s3              = S3Client::factory();\n\n\n    while (($file = readdir($dir)) !== false) {\n        $FullFileName = realpath($DirectoryToScan.'/'.$file);\n        if ((substr($FullFileName, 0, 1) != '.') && is_file($FullFileName)) {\n            set_time_limit(30);\n            $result = $s3->putObject(array(\n                        'Bucket'       => $bucket,\n                        'Key'          => $accessKey,\n                        'SourceFile'   => $FullFileName));\n\n            $ThisFileInfo = $getID3->analyze($FullFileName);\n\n            getid3_lib::CopyTagsToComments($ThisFileInfo);\n\n            $pathname = $result['ObjectURL'];\n            $artist = $ThisFileInfo['comments']['artist'][0];\n            $title = $ThisFileInfo['comments']['title'][0];\n            $playtime = $ThisFileInfo['playtime_string'];\n\n            $connection = new MongoClient($server);\n\n\n            $dbname = $connection->selectDB($database);\n            $songs = $dbname->songs;\n\n            $song = array(\n                    'title' => $title,\n                    'artist' => $artist,\n                    'duration' => $playtime,\n                    'filename' => $pathname,\n                    'date' => new MongoDate(),\n                    'genre_id' => 1,\n                    'playlists_id' => 501\n                );\n\n            if ($songs->insert($song)) {\n                echo $title.\" added successfully.\\n\".$pathname;\n            };\n\n\n\n        }\n    }\n\n```\n. oh wow! Thank you this was perfect!\n. ",
    "weichenglee": "I tried your code and the Content-Length header was included.\nHowever, I tried another test to stress the server and the problem is reproducible. (not AWS)\nThe results of the two tests are different.\n``` php\n<?php\nrequire './aws.phar';\nuse Aws\\S3\\S3Client;\n$aws_config = array(\n        'key' => \"xxxx\",\n        'secret' => \"xxxx\",\n        'base_url' => \"http://s3.hicloud.net.tw\",\n        'scheme' => \"http\",\n        'curl.options' => array(\n                'CURLOPT_VERBOSE' => true,\n        ),\n);\n$client = S3Client::factory($aws_config);\nfor ($i = 0; $i <= 1000; $i++) {\n        $in_json = array(\n                \"Bucket\" => \"bat-wadeli\",\n                \"Key\" => \"zero_file/zero\",\n                \"SourceFile\" => \"/volume1/zero\"\n        );\n        $res = $client->putObject($in_json);\n}\n?>\n```\nand got\n```\n\nPUT /zero_file/zero HTTP/1.1\nHost: bat-wadeli.s3.hicloud.net.tw\nUser-Agent: aws-sdk-php2/2.7.21 Guzzle/3.9.2 curl/7.36.0 PHP/5.5.21\nDate: Thu, 12 Mar 2015 06:48:26 +0000\nAuthorization: AWS SE41NTAxMjQ3OTEzODM2MTMzNDY1NDE:WFJMj4WuessE5DXtGjmVRQQAaLM=\nContent-Length: 0\n\n< HTTP/1.0 500 Internal Server Error\n< x-amz-request-id: 6RORGA1TVTPN1PUM\n< x-amz-id-2:\n< Content-Type: application/xml\n< Date: Thu, 12 Mar 2015 06:48:27 GMT\n< Connection: close\n<\n\nPUT /zero_file/zero HTTP/1.1\nTransfer-Encoding: chunked\nHost: bat-wadeli.s3.hicloud.net.tw\nUser-Agent: aws-sdk-php2/2.7.21 Guzzle/3.9.2 curl/7.36.0 PHP/5.5.21\nDate: Thu, 12 Mar 2015 06:48:27 +0000\nAuthorization: AWS SE41NTAxMjQ3OTEzODM2MTMzNDY1NDE:r8z8kfhDHWbN+PWWBSfpjOcr8GE=\n\n< HTTP/1.0 411 Length Required\n< x-amz-request-id: ZESZXB7QM931H7F5\n< x-amz-id-2:\n< Content-Type: application/xml\n< Date: Thu, 12 Mar 2015 06:48:28 GMT\n< Connection: close\n<\nPHP Fatal error:  Uncaught Aws\\S3\\Exception\\MissingContentLengthException: AWS Error Code: MissingContentLength, Status Code: 411, AWS Request ID: ZESZXB7QM931H7F5, AWS Error Type: client, AWS Error Message: You must provide the Content-Length HTTP header., User-Agent: aws-sdk-php2/2.7.21 Guzzle/3.9.2 curl/7.36.0 PHP/5.5.21\n  thrown in phar:///usr/syno/aws/aws.phar/Aws/Common/Exception/NamespaceExceptionFactory.php on line 91\n```\n. ",
    "bassrock": "I am using 1.6.2. Possibly I need to update\n. @jeremeamia yea no idea why it all of a sudden started happening. I just switched to the newer sdk and now I am not having issues. At first I thought maybe a back end change happened or something.\n. ",
    "ferozfirru": "hi , i too having slow issues with my php-aws-sdk-v3 , the auto-loader itself taking aroung 0.5 seconds and 2MB memory consumption. im using only s3client from this Aws-sdk no other features, please tell me how to make it run faster? can i load only s3 related modules in autoloader?. hi the autoloader is taking around 2mb memory and 0.5 to 0.7 seconds loading time, i want to use only S3 client, i dont want all other sdk classes how can i ?. ",
    "klasalvia": "I see what you are saying - this could be a bigger issue than just file size. I also tried the example on larger file sizes (66G) and it did not work. Since this is a PHP limitation, I might be better off using another SDK. Thanks for your quick response.\n. ",
    "lonnylot": "Versions:\nlonnykapelushnik:~/Development/bond/aws-test  (master) \u26a1  composer show -i\naws/aws-sdk-php               2.7.22  AWS SDK for PHP - Use Amazon Web Services in your PHP project\naws/aws-sdk-php-laravel       1.1.2   A simple Laravel 4 service provider for including the AWS SDK for PHP.\nError:\n[2015-03-18 18:41:45] production.ERROR: Aws\\CloudFront\\Exception\\InvalidArgumentException: AWS Error Code: InvalidArgument, Status Code: 400, AWS Request ID: 6d38e6da-cd9e-11e4-8f00-dd185ddb5737, AWS Error Type: client, AWS Error Message: The parameter originPath is null., User-Agent: aws-sdk-php2/2.7.22 Guzzle/3.9.2 curl/7.37.1 PHP/5.5.22 Laravel/4.2.17 L4MOD/1.1.0 [] []\nExample code: https://github.com/lonnylot/aws-test\nYou'll need to add a .env.php file to the root dir w/:\n<?php\nreturn [\n    'aws_key' => '',\n    'aws_secret' => '',\n    'bucket_name' => ''\n];\n. And to run the example:\nphp artisan test:aws\n. @mtdowling thanks for the response. IIRC I had tried passing in the originPath as '' and it threw some sort of error. I'll test it out and if there are more problems I can open up a support ticket.\n. ",
    "eveyrat": "In fact i'm wrong about infinite loop but i can't make bigger chunks just like this :\necho fread($stream, 5 * 1024 * 1024 );\nIt's working perfectly when using a local file instead of the s3 stream.\n. The following snippet should return the same length for the chunk and the actual filesize of my object.\n$filename = 's3://bucket/sample.mp4';\n    //$filename = '/var/www/sample/web/uploads/sample.mp4';\n    $stream = fopen($filename, 'r');\n    printf('chunkSize: %d - filesize: %d', \n        strlen(fread($stream, filesize($filename))),\n        filesize($filename)\n    );\nOutput :\ns3 wrapper : chunkSize: 8192 - filesize: 16121660\nlocal file : chunkSize: 16121660 - filesize: 16121660\n. ",
    "hacfi": "Small correction: according to https://getcomposer.org/doc/01-basic-usage.md#next-significant-release-tilde-and-caret-operators-\nit will be\n- ^0.1 would translate to 0.1.*,>=0.1.0 and <0.2.0\n- ^0.1.4 would translate to 0.1.*,>=0.1.4 and <0.2.0\n. @GrahamCampbell oops..sorry - didn\u2019t read that part :)\n. Found 2 more: ConfigServiceClient & KinesisClient\n. @jeremeamia I created (yet another) Symfony bundle for the SDK (see https://github.com/hacfi/aws-bundle) and spent several hours debugging the code to find new feature ideas (wouldn\u2019t call that a proper audit). The main reason for creating this bundle was adding the option to specify a \"default parameters file\" so irrelevant (static) parameters don\u2019t clutter the application code. I use the SDK a lot but don\u2019t depend on CloudHsm at all..just called most of the requests that exist :). Feel free to close the PR..was just a hint that you should maybe add a helper method in the CloudHsmClient so users don\u2019t have to use >getCommand('GetConfig', ...\n. Debugged it with XDebug and made sure it used the correct class and the Symfony error page confirms it:\nhttps://cloud.githubusercontent.com/assets/428841/7263221/02e648ac-e880-11e4-8d91-7516fcc40a35.png\nOn Tuesday, April 21, 2015, Jeremy Lindblom notifications@github.com\nwrote:\n\nHow did you determine that this PR has the correct name?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/aws/aws-sdk-php/pull/545#issuecomment-94927977.\n. @jeremeamia Thanks for the hint! I only catch this exception during a getBucketTagging call to trigger a putBucketTagging request. Look forward to version 3 and guzzle 5!\n. @jeremeamia Thanks for the clarification! I only noticed this because I tried using errorResponses to fix #545 and noticed it didn\u2019t work.\n. @skyzyx Thanks for the quick response! You\u2019re right..the SDK actually returns it wrapped in quotes. A bit weird but ok :)\n. \n",
    "ducnguyenhuy": "Could you tell me how to get \"verbose cURL output\"?\nThanks.\n. So this will help to output more details about error, right? \nI have tried to use a loop and try/catch to retry calling. Max of retrying is 3 times. Do you think it's good? \nThanks\n. Ok, thanks.\nI will check again and let you know.\n. Thanks for suggestion.\nAdding retry works and try/catch to show error in log. But the error's still \"Error completing request\". Does that mean the server takes so long to response?\nDuc.\n. Not yet, this issue happens somtime. So I need more time to check if it shows or not.\nThanks.\n. ",
    "JustinLien": "@mtdowling region us-west-1.\nHow do I find out which signature version being utilized?\nI tested it again this morning and everything seem to be resolved. Sorry, for your trouble. I spend the whole day yesterday and this was what I concluded, however; as of now, it seem to be working fine.\n. I'm going to close this issue since I cannot replicate it today.\nThanks again!\n. ",
    "codivist": "I am getting the same error\nAws\\S3\\Exception\\SignatureDoesNotMatchException: AWS Error Code: SignatureDoesNotMatch, Status Code: 403, AWS Request ID: DF90119C6E86B26D, AWS Error Type: client, AWS Error Message: The request signature we calculated does not match the signature you provided. Check your key and signing method., User-Agent: aws-sdk-php2/2.8.1 Guzzle/3.9.3 curl/7.35.0 PHP/5.5.9-1ubuntu4.7 Laravel/4.2.17 L4MOD/1.1.0\nNot sure what signature version I'm using, but I am using us-east-1\nThe bucket is already created, but the bucket name is ps-scores/data (so no periods in it).\nThe filename being uploaded is basically a uuid: fbf7bb3a-ba6f-41dc-a27f-e69955553F31.psc\nThis was happening in 2.8.0 and 2.8.1\nI can replicate the error every time.\nI can open a new issue if that would be better.\n. Yes, my bad, the bucket is ps-scores and data/ is the folder that it is getting uploaded to (in the key name)\n. my putObject code:\nphp\n$upload = $s3->putObject(\n                            array(\n                                'Bucket'      => 'ps-scores',\n                                'Key'         => 'data/uuidFileName.psc',\n                                'Body'        => \\File::get('S3Files/' . Input::get('matchUUID') . '.psc'),\n                                'ContentType' => 'binary/octet-stream',\n                                'Metadata'    => [\n                                    'matchType'   => 'type III',\n                                    'matchName'   => 'uuidFileName',\n                                    'matchDate'   => 2015/04/19,\n                                    'inputMethod' => 'uploaded from device',\n                                ]\n                            )\n                        );\n. bah! Nevermind. Just sat down with our iOS developers and they introduced an issue in the matchName that was causing only certain uploads to fail.  They are fixing their code and we can move on happy and care free knowing it wasn't us! \nThanks\nFYI, incase anyone has this problem again. It was due to apostrophes (') being allowed in the name.\n. ",
    "cizal": "After further research I located the problem to the AWS Service Provider for Laravel 4 and opened the issue there: https://github.com/aws/aws-sdk-php-laravel/issues/66. \n. ",
    "Im0rtality": "That's great. Do you guys have any estimates/guestimates on v3 release?\n. ",
    "MichaelShilling": "Awesome! I'll keep an eye out for this in the next minor release!\nThank you!\n. ",
    "bjornbos": "So wouldn't it need\n'location' => 'body'\nthen (just like with the InvokeAsync method)? It's not working at the moment because it's not receiving any payload...\n. ",
    "gummoe": "Sorry for my delayed response, and thank you for helping me clarify my thoughts!\nYou are correct that this is the class in question: https://github.com/aws/aws-sdk-php/blob/master/src/Aws/Sns/MessageValidator/Message.php\nYou are also correct that the fromRawPostData() function does NOT check the Content-Type of the HTTP request. \nHowever...I may be wrong, but the value obtained from the file_get_contents('php://input') returns the entire raw request. An example of a notification from SNS is included below:\n```\nPOST /app_dev.php/event/test/test1 HTTP/1.1\nHost: [sanitized] \nUser-Agent: Amazon Simple Notification Service Agent\nContent-Length: 1135\nAccept-Encoding: gzip,deflate\nConnection: close\nContent-Type: text/plain; charset=UTF-8\nX-Amz-Sns-Message-Id: [sanitized]\nX-Amz-Sns-Message-Type: Notification\nX-Amz-Sns-Subscription-Arn: [sanitized]\nX-Amz-Sns-Topic-Arn: [sanitized]\nX-Forwarded-Proto: https\nX-Real-Ip: 72.21.217.131\n{\n\"Type\" : \"Notification\",\n\"MessageId\" : \"[sanitized]\",\n\"TopicArn\" : \"[sanitized]\",\n\"Subject\" : \"This is the subject\",\n\"Message\" : \"[sanitized]\",\n\"Timestamp\" : \"2015-04-16T17:37:05.278Z\",\n\"SignatureVersion\" : \"1\",\n\"Signature\" : \"[sanitized]\",\n\"SigningCertURL\" : \"[sanitized]\",\n\"UnsubscribeURL\" : \"[sanitized]\"\n}\n```\nThe json_decode function expects the entire string (in this case, php://input) to be JSON-encoded, which the raw request is not. I agree with you that the content-type shouldn't matter, but it also seems to be the case that the fromRawPostData() function needs to extract the correctly formatted 'payload'-portion of the SNS request before calling json_decode. Happy to provide any additional information or tests that would be helpful! \n. Yup! php://input seems to return the entire request (including headers) in my environment. \n. Nothing fancy:\nPHP v5.3.29\nComposer.json has AWS SDK set to: \"aws/aws-sdk-php\": \"2.7.*\"\nHosting using MAMP PRO. \n. @mtdowling - sorry for the delayed response and the confusion. I've been able to confirm that this was the result of a parsing error in my testing environment that was incorrectly placing request headers into the raw post body. Thank you for your patience and for excusing my usage of your time!\n. ",
    "fustundag": "To clear misunderstanding  : I can create presigned url successfully but if I want to use it, it gives SignatureDoesNotMatch error\ncurl -v -H \"Content-Type: image/jpeg\" -T myimage.jpg \"[presigned_url]\"\n. I think I found problem : \nWhen eu-central-1 region used, S3SignatureV4 is required. S3SignatureV4 extends Aws/Common/Signature/SignatureV4 class.\nAt Aws/Common/Signature/SignatureV4.php:273, all createPresignedRequest calls cloned with GET method. But if request is for \"PutObject\", \"PUT\" should be used.\n. To test it, I chnaged Aws/Common/Signature/SignatureV4.php:273 line to \n$sr = RequestFactory::getInstance()->cloneRequestWithMethod($request, 'PUT');\nand I added \"Expires: 600\" header to curl cmd:\ncurl -v -H \"Expires: 600\" -H \"Content-Type: image/jpeg\" -T myimage.jpg \"[presignedUrl]\"\nI put image file successfully to bucket.\n. ",
    "deviantintegral": "That was easily the most taxing commit of the day.\n. That's great! I didn't realize the V3 branch was that close to release. And I'm kicking myself for not checking the abstract-http branch.\nWhat would you think about changing the static array to Doctrine's ArrayCache(), so it can be swapped out? The cache methods are private, so I'd still have to patch the class to inject something else.\nWhat's your definition of \"soon\"? I've got to have a beta out of the Drupal module next week, and need to be production ready in 3-4 weeks, which puts me before the GA release. Do you know of anyone already using the v3 branch in production?\n. > It needs to be a static value because you need to be able to persist the cache\nYeah, that's why this PR defines the cache as a static CacheAdapterInterface.\n\nAs for using Doctrine's array cache: what's the use case for this?\n\nDrupal's image styles are generated on request, and not ahead of time. Many complex sites define dozens of image styles, but only a subset may be actually used for a given image. To regenerate an image, the derivative is deleted. Generating all derivatives in a queue is problematic because you're  blocking a site editor for around 4 seconds times the number of derivatives. Background queuing will cause images to 404 until they are generated.\nThe image module does a few stat calls to make sure that any directories and the image exist, which adds up to several S3 calls. By caching the stat calls across multiple clients, we can get a serious performance boost. Even in the simplest one-call test to see if an image exists, this cache is dropping an image derivative request from ~275ms to ~50ms.\n(if only Lambda had events for 404s!)\nSo to answer your question; I don't really care about using an ArrayCache, but I just need to be able to pass in a CacheAdapterInterface so we can make the cache shared across requests.\n. This looks great. Thanks!\n@mtdowling I'm backporting this to the 2.x branch just so what we're using isn't vastly different. Should I open a PR for that when I'm done, or would you rather keep this in v3 only?\n. Lets see if this clears things up.\n1. When we construct an S3 client, we use setCommandFactory() to use our own CompositeFactory instead.\n2. We override factory(), so that we can call out to Drupal hooks (the module_invoke_all and drupal_alter()) before and after creating the command. It's a Drupalism for sure, but it's what Drupal 7 devs will expect when making small changes for actual site builds.\n3. When we call S3Client::getObjectUrl(), our custom factory works fine, but it has no knowledge of the $expires parameter and can't pass one back.\nAnd here's a few very simple examples we'll be shipping with our docs of where we've seen this functionality used on existing sites with the old module / SDK.\nThe nice thing about this new implementation is that it works for any S3 command that is created, and gives developers direct access to the command object if they need it. We could add additional Drupal hooks to wrap around calling getObjectUrl(), but that won't work if anyone wants to use the SDK directly. I could also subclass S3Client, but I ran into relative directory issues with includes when I subclassed into a different namespace.\n. Another way to think of this is that it's possible to send invalid arguments to getObjectUrl() and get a broken URL back. If I add ResponseCacheControl to $args, the only way I know I need to set $expires to a non-zero value is from the API docs.\nPerhaps this isn't really about the command factory, but the returned request from prepare(). That object can cause a URL to be required to be presigned to be functional, but those query params are ignored.\nI'm going to do a PR to unblock some of my other work; hopefully that will make things clearer.\n. That works, and for now I can manage the small patch to do what we need on the Drupal side.\nOn the v3 branch I think I can use event emitters in Guzzle instead, avoiding the AWS sdk component entirely.\n. Turns out this is triggered by some subset of EXIF info. When the M_APP15 JPEG marker is found, php_skip_variable() calls _php_stream_seek(). I was just able to trigger this issue with an 85KB image, while a 6MB image with no EXIF data works fine. It's not just \"having EXIF\" either - images from my Nexus 5 work fine, but images from my standalone camera fail.\n. I've filed an upstream report over at https://bugs.php.net/bug.php?id=69706\nI'd be concerned about the memory load if someone tries to seek on a large file like a video asset. Would it be better to back the response body on disk instead?\n. Actually, it looks like the code does exist in the 2.8 branch?\nHere's current diff of patches I have against the 2.8.27 release, mostly related to caching that's in the 3.x SDK now.\nI haven't tested this, but DrupalRemoteStreamWrapper is a class from Remote stream wrapper that has nothing to do with the S3 stream wrapper. So how is it's class getting mixed up with wrapper_data in the s3 stream wrapper? Shouldn't that value be controlled by the stream wrapper itself? This is feeling like a bug in either Remote Stream Wrapper or Filehash. If not, perhaps @PatchRanger you could come up with a unit test demonstrating this more clearly?\n. I thought about that, but I wasn't sure how stable the query parameters would be. It felt kind of hacky to be parsing them like that from outside the SDK. But if you think that's fine, then it works for me.\n. Thanks for the docs update! This would have answered my questions. Just some super minor wording suggestions.\n. I really don't like this. I'd expected to be able to do at least a 1 second sleep, but on my local it looks like the \"1 second\" cache is valid up to around 1.5 seconds later (and isn't consistent). We could implement our own cache that extends ArrayCache() and implements subsecond cache lifetimes, but that seems bit overkill. Thoughts?\n. This test was actually returning an unusable URL since expires was null.\n. Minor grammar / casing here, how about:\n\nBesides AwsException, each AWS service client has it's own exception class that inherits from [snip]\n. > With the API specific errors listed under the Errors section of each method, you can determine more specific error types to catch.\n. Actually to not be passive:\nYou can determine more specific error types to catch with the API specific errors listed under the Errors section of each method.\n. \n",
    "arvindgt": "Yes Mtdowling, your previous comment was helpful. thank you for your answer.\nMy ubuntu server's click was delaying by 6-7 minutes from UTC. Issue resolved..\n. ",
    "cgltower": "Yes, I agree.  I will try that, its just a load of work.\nFor now Ive worked around this version of framework's autoload with a hack\nfunction autoload($class) {\n        // if file does not exist in LIBS_PATH folder [set it in config/config.php]\n        $pathnclass = str_replace(\"\\\\\" , DIRECTORY_SEPARATOR , $class);\n        if (file_exists(LIBS_PATH . $class . \".php\")) {\n            require LIBS_PATH . $class . \".php\";\n        } elseif (file_exists(LIBS_PATH . 'aws-sdk-php' . DIRECTORY_SEPARATOR . $pathnclass . \".php\")) {    \n            require LIBS_PATH . 'aws-sdk-php' . DIRECTORY_SEPARATOR . $pathnclass . \".php\";\n        } else {\n            exit ('The file ' . $class . '.php is missing in the libs folder. ' . LIBS_PATH . 'aws-sdk-php' . DIRECTORY_SEPARATOR . $pathnclass );\n        }\n    }\nWhich is sort of working but might be causing issues when trying to use namespaces in try/catch exceptions such as Aws\\Ec2\\Exception.  But that would be a different qu.  Thanks for your help.\n. ",
    "gangadhar": "@mtdowling Thanks for the reply. We tried the following two methods and it did not work! Do you have any other ideas?\nphp\n$fh = fopen(\"s3://$s3_bucket/$file\", \"r\");\n$filter = stream_filter_append($fh, \"convert.base64-encode\", STREAM_FILTER_READ);\nfpassthru($fh);\nstream_filter_remove($filter);\nfclose($fh);\nnew approach for reading file:\nphp\nreadfile(\"php://filter/read=convert.base64-encode/resource=s3://$s3_bucket/$file\");\n. @mtdowling : Thanks! I confirm. Your fix is working for us. Appreciate your quick response.\n. ",
    "kkopachev": "for people coming from search engines: there is php's bug report on that issue https://bugs.php.net/bug.php?id=75910. I had to use createCommand/execute to avoid warnings about unknown methods.\nMissing @method phpdocs usually makes me think there is no such method and there is no magic call method as well.\n. @cjunge-work How about https://github.com/aws/aws-sdk-php/blob/master/src/Sdk.php#L164-L166 and https://github.com/aws/aws-sdk-php/blob/master/src/AwsClient.php#L93-L109 ?\n. Since timeout values are passed through to Guzzle, then docs should be changed to say:\n\nUse 0 to wait indefinitely (the default behavior).\n\nBecause statement \"use 60 seconds to wait indefinitely\" is ambiguous.\nAdditionally, and that's probably belongs to Guzzle issue tracker, Curl handler uses no timeout at all, while stream handler will use default-socket-timeout ini-setting.\nNow the issue this is all coming from is that we got some weird situation couple times, when a job which does few requests to ElasticTranscoder to submit a job and request status update hang for some long period. strace/ltrace showed same output as mentioned https://github.com/guzzle/guzzle/issues/1940\nThat was time-sensitive issue, so I didn't have time to figure out what connection it is waiting for. Job also does other requests, so it's possible that it was not caused by AWS SDK. \nFrom my point of view, it would be great to have default timeout set to some reasonable value. Otherwise with Curl handler request might hang forever (not 100% sure here).. Documentation you referred to says cURL is not required, but recommended.. @kstich I am confused. If cURL extension is required, then it must be in https://github.com/aws/aws-sdk-php/blob/master/composer.json#L18-L27\nGuzzle itself provides stream handler wrapper to use if no cURL extension installed.. Botocore (python sdk) checks it at https://github.com/boto/botocore/blob/03234ba7528defa57de12426d578d94e17296910/botocore/session.py#L90-L92. ",
    "aobermueller": "Makes sense.  Thanks.\n. ",
    "ahmadpriatama": "I want to get list of directories/files which match to some pattern, like we can do with PHP glob function on local filesystem. But instead of searching on local filesystem, i'm trying to get list of directories/files on S3 bucket. So, after following instruction on S3 Stream Wrapper and calling glob('somePattern'), it just return empty array. I wonder whether glob function is also supported like opendir, readdir, etc.?\n. anyway i had implement a function that emulate glob, written below\n``` php\n/\n * S3StreamHelper class file.\n \n * @author Ahmad Priatama ahmad.priatam@gmail.com\n * @package application.components.helpers\n * @since 2015.04.26\n /\nclass S3StreamHelper {\n/**\n     * Emulate php glob function\n * from http://www.delorie.com/djgpp/doc/libc/libc_426.html\n */\npublic function glob($pattern) {\n    $return = [];\n\n    $patternFound = preg_match('(\\*|\\?|\\[.+\\])', $pattern, $parentPattern, PREG_OFFSET_CAPTURE);\n    if ($patternFound) {\n        $parent = dirname(substr($pattern, 0, $parentPattern[0][1] + 1));\n        $parentLength = strlen($parent);\n        $leftover = substr($pattern, $parentPattern[0][1]);\n        if (($index = strpos($leftover, '/')) !== FALSE) {\n            $searchPattern = substr($pattern, $parentLength + 1, $parentPattern[0][1] - $parentLength + $index - 1);\n        } else {\n            $searchPattern = substr($pattern, $parentLength + 1);\n        }\n\n        $replacement = [\n            '/\\*/' => '.*',\n            '/\\?/' => '.'\n        ];\n        $searchPattern = preg_replace(array_keys($replacement), array_values($replacement), $searchPattern);\n\n        if (is_dir($parent.\"/\") && ($dh = opendir($parent.\"/\"))) {\n            while($dir = readdir($dh)) {\n                if (!in_array($dir, ['.', '..'])) {\n                    if (preg_match(\"/^\". $searchPattern .\"$/\", $dir)) {\n                        if ($index === FALSE || strlen($leftover) == $index + 1) {\n                            $return[] = $parent . \"/\" . $dir;\n                        } else {\n                            if (strlen($leftover) > $index + 1) {\n                                $return = array_merge($return, self::glob(\"{$parent}/{$dir}\" . substr($leftover, $index)));\n                            }\n                        }\n                    }\n                }\n            }\n        }\n    } elseif(is_dir($pattern) || is_file($$pattern)) {\n        $return[] = $pattern;\n    }\n\n    return $return;\n}\n\n}\n```\n. ",
    "trangiiieee": "Thanks @ahmadpriatama for the script. It was a life saver!. ",
    "mateusz": "LOL :-) Thanks! For Googler's benefit: the same issue filed in Go SDK\n. ",
    "NobleUplift": "I'll try to isolate the important bits because the codebase is fairly large. It's a basic S3 get:\n$s3 = AwsS3::getInstance();\n            $contents = $s3->retrieveFileContents('json/' . $token . '.json', 'cdn');\n            echo $contents;\nAwsS3 works as follows:\n```\nnamespace common\\aws;\nclass AwsS3 {\n    private $client;\nprivate static $instance;\n\npublic static function getInstance() {\n    if (!isset(self::$instance)) {\n        self::$instance = new AwsS3();\n    }\n    return self::$instance;\n}\n\nprivate function __construct() {\n    AwsBootstrapper::initialize();\n    $this->client = S3Client::factory(AwsConfig::getAwsCredentials());\n}\n\n...\n\n}\n```\nThe bootstrapper is what loads the phar, and it's just an empty class:\n```\nnamespace common\\aws;\nrequire_once('aws.phar');\n/\n * Documentation for AWS PHP classes can be found at:\n *      http://docs.aws.amazon.com/aws-sdk-php-2/latest/index.html\n \n/\nclass AwsBootstrapper {\n    public static function initialize() {\n        /\n         * The PHP 'use' keyword does not call the autoloader.\n         * Rather, the autoloader is only called when the class is found in the code.\n         * \n         * Therefore, an empty 'initialize' method must be called so that other\n         * classes receive aws.phar.\n         /\n    }\n}\n```\nThis is only one of our servers where we have this issue, and it is only initializing S3. On our web server, we initialize S3, SQS, and SES without these errors.\n. My coworker is the one who did the testing, so I may have missed something trying to read the code myself. I've also made a change in our loading practice for AWS (as I mentioned above) which I want to test on Monday, so I'll update you after that.\n. This is the version of PHP we're using:\nPHP 5.5.9-1ubuntu4.9 (cli) (built: Apr 17 2015 11:44:57) \nCopyright (c) 1997-2014 The PHP Group\nZend Engine v2.5.0, Copyright (c) 1998-2014 Zend Technologies\n    with Zend OPcache v7.0.3, Copyright (c) 1999-2014, by Zend Technologies\nI got this when I dropped the 2.8.2 version of the phar into Eclipse. Is there a way that I can verify my phar, maybe an MD5 hash?\n\nI have not yet had a chance to re-run that test.\n. My coworker and I were able to test the 2.7.27 aws.phar and it might have worked. I believe the cause is one or both of the following:\n1. The require_once('aws.phar'); was inside of a namespace.\n2. Our Autoloader was causing a conflict on the second load of AWS:\n```\n   function autoloader($class) {\n   $class = ltrim($class, '\\');\n   $ns1 = substr($class, 0, 7);\n   if ($ns1 === false) {\n       return;\n   } else if ($ns1 != 'ournamespace') {\n       return;\n   }\n$filename  = '';\n   $namespace = '';\nif ($last_ns_pos = strripos($class, '\\')) {\n       $namespace = strtolower(substr($class, 0, $last_ns_pos));\n       $class = substr($class, $last_ns_pos + 1);\n       $filename  = str_replace('\\', DIRECTORY_SEPARATOR, $namespace) . DIRECTORY_SEPARATOR;\n   }\n$filename .= str_replace('_', DIRECTORY_SEPARATOR, $class) . '.php';\nif (stream_resolve_include_path($filename)) {\n       require_once $filename;\n   } else {\n       throw new \\Exception('Class at location ' . $filename . ' not found.');\n   }\n   }\nspl_autoload_register('autoloader');\n   ```\nThe require_once('aws.phar'); now resides above the autoloader method.\nWe aren't exactly certain which version of AWS we just tested on. At a later date, we will try with the most up-to-date 2.8 branch and see if it works.\n. Thanks r-baker! If possible, could you link me or write instructions for how to do both of these? Then I can forward them to my coworker and we'll try a higher version.\n. My coworkers are actually at the AWS conference in Chicago this week, so I won't have the server access to test this change. Sorry guys.\n. My coworker and I were able to test this today using the phar posted here 6 days ago. Is the phar based on version 3.0.6?\nAt first, we received this error:\nphp\nPHP Fatal error:  Uncaught exception 'InvalidArgumentException' with message 'Missing required client configuration options: \\n\\nversion: (string)\\n\\n  A \"version\" configuration value is required. Specifying a version constraint\\n  ensures that your code will not be affected by a breaking change made to the\\n  service. For example, when using Amazon S3, you can lock your API version to\\n  \"2006-03-01\".\\n  \\n  Your build of the SDK has the following version(s) of \"dynamodb\": * \"2012-08-10\"\\n  \\n  You may provide \"latest\" to the \"version\" configuration value to utilize the\\n  most recent available API version that your client's API provider can find.\\n  Note: Using 'latest' in a production application is not recommended.\\n  \\n  A list of available API versions can be found on each client's API documentation\\n  page: http://docs.aws.amazon.com/aws-sdk-php/v3/api/index.html. If you are\\n  unable to load a specific API version, then you may need to update your copy of\\n  the SDK.' in phar:///usr/local/advanse/lib/php/aws.phar/Aws/ClientResolver.php: in phar:///usr/local/advanse/lib/php/aws.phar/Aws/ClientResolver.php on line 319\nBut that was easily fixed, and then we got this error:\nphp\nPHP Catchable fatal error:  Argument 2 passed to Aws\\\\AwsClient::getCommand() must be of the type array, integer given, called in phar:///usr/local/advanse/lib/php/aws.phar/Aws/AwsClient.php on line 167 and defined in phar:///usr/local/advanse/lib/php/aws.phar/Aws/AwsClient.php on line 211\nIs the second error on me or inside the phar?\n. ",
    "rjbaker": "I have had the same issue with exact same error. From my limited testing it appears to be caused by Zend Opcache. Disabling the opcache (not recommended) or adding the aws.phar path to the list of ignored files fixes this.\n. I'm not 100% sure excluding aws.phar from the cache is the \"right\" thing to do. That said the AWS docs offer this as a solution for phar/APC issues: http://docs.aws.amazon.com/aws-sdk-php/v2/guide/faq.html\nCreate a blacklist file e.g. /etc/php5/opcache-blacklist.txt\n```\nphp.ini\nopcache.blacklist_filename=/etc/php5/opcache-blacklist.txt\n```\n```\nopcache-blacklist.txt\n/path/to/aws.phar\n```\n. Seems to be just the AWS phar.\nRunning Ubuntu 14.04. Standard distro. PHP 5.6.9-1+deb.sury.org~trusty+2\n. ",
    "Frozenfire92": "I had a similar problem and wanted to share to help any other googlers\nthe environment\nUbuntu 15.04\nPHP 5.6.4-4ubuntu6 (cli) (built: Apr 17 2015 15:47:51) \nServer version: Apache/2.4.10 (Ubuntu)\nServer built:   Mar  9 2015 11:53:48\nthe error\nPHP Warning:  require(phar://aws.phar/aws-autoloader.php): failed to open stream: phar error: invalid url or non-existent phar &quot;phar://aws.phar/aws-autoloader.php&quot\nthe solution\n\nchange the phar file permissions chmod 755 /path/to/aws.phar\nadd the following to /etc/php5/apache2/php.ini opcache.enable=0\nrestart apache service apache2 restart\n. Will look into it next week\n. Looking into it now!\n. Nope, same issue\n\nPHP Fatal error:  require_once(): Failed opening required '/var/www/site/includes/aws.phar' (include_path='.:/usr/share/php:/usr/share/pear') in /var/www/site/index.php on line 10\nThe line in question looks like\nrequire_once dirname(__FILE__) . '/includes/aws.phar';\n. Yes you are correct, my bad. I'm not entirely sure what is going on :confused: \n. ",
    "madz7z": "I have the same problem after update to latest v2.8.12.\nIt seems to not happen when including php file is located in the same folder with aws.phar\nAnd it has something to do with opcache, after apache restart the very first time it works and then the error happens.\n. Fix #662 worked for me.\n1. Cloned 2.8 branch \n2. Applied fix from #662 \n3. Built phar\n. Unfortunately issue is back in other place after restarting apache. Strange thing that it disappeared in initial place. One more thing: with or without #662 getting same results.\nHad to switch to ZIP version.\n. ",
    "bs-thomas": "The issue still seems to exist with the latest SDK 3.2.1 build.  The environment is just the same as r-baker's.\nI'm not gonna write too much details here, as it's gonna be the same errors and stuff.  I just wanted to raise a headsup, that the bug is still here.\nThanks!\n. I guess it'd be great to have everyone try this, and wait for a few weeks down the road and see if this problem happens again (since it doesn't always happen)?\n. I wouldn't say it's once every few weeks, but the issue is quite random.  It looks completely fine after I upload it to production, tested it.  Then, after a day, it would suddenly throw out this error.  Once this error happens, it will stay there until we do something about it.\nThen, when our developers try to investigate, we \"rename the file (eg. to .bak), and then rename it back\", and it would start working again.\nPretty awkward eh?\n. Yeah it's pretty weird.  Currently I'm forced to use the full ZIP folder package because of this issue, which has been around for quite a few versions in this SDK.\n. @dstevenson I'm facing the exact issue you're talking about.  I was wondering though, if there is any temporary workaround that works for the scenario.  Thank you!. @jeskew  Wow that helped a lot.  Thank you very much!  This is the final working code I have.  Thank you once again!\n```\n// Create the stream wrapper\n$s3Client = \\Aws\\S3\\S3Client::factory(array(\n            'version' => 'latest',\n            'region' => $region,\n            'credentials' => array(\n                'key'    => $key,\n                'secret' => $secret,\n            ),\n        ));\n$s3Client->registerStreamWrapper();\n$context = stream_context_create(array(\n    's3' => array(\n        'seekable' => true\n    )\n));\n$stream = fopen($filePath, 'r', false, $context);\n$reader = \\League\\Csv\\Reader::createFromStream($stream);\n```. ",
    "oberman": "Same happened to me (3.2.6), so this issue seems very real still.  I switched to the .zip install pattern (http://stackoverflow.com/questions/28000901/why-aws-phar-runs-once-then-wont-load-again), which works. \n. @jeskew the linked phar didn't fail after several refreshes.  the 3.2.6 phar would fail 100% of the time on the 2nd refresh.\nwhat was the change, out of curiosity?\n. Thanks!  I looked up the docs:\nhttp://php.net/manual/en/phar.mapphar.php\nI noticed you are using a static symbolic name (aws.phar).  There is a comment in the docs that this might cause problems for people using multiple versions of the aws.phar at the same time (*).  Could the symbol (and filename) be qualified with the version #?  I renamed my phar, just to make bookkeeping easier (e.g. aws.phar -> aws-3.2.6.phar).  But, apparently, due to the internal symbolic name, issues could remain.  \n(*) Seems like an odd use case, but I have an example: I have a web host for testing that uses vhosts, each vhost could be running a different checkout and thus different aws.phar. \n. -amazon-linux-ami/2014.09 (not updated, 248 packages need updating)\n-An old version of Codeigniter (don't ask)\n-I can post a full phpinfo, but trying to slice out what I think matters first:\nApache/2.4.10 (Amazon) OpenSSL/1.0.1i-fips PHP/5.5.22\napcu    4.0.3\nPhar EXT version    2.0.2\nPhar API version    1.1.1\nOpcache has a million values, but my ini:\nzend_extension=/usr/lib64/php/5.5/modules/opcache.so\nopcache.memory_consumption=128\nopcache.interned_strings_buffer=8\nopcache.max_accelerated_files=4000\nopcache.revalidate_freq=0\nopcache.fast_shutdown=1\nopcache.enable_cli=1\n. I've been debugging a similar issue and didn't see this thread until just now.  I'm looking at my logging to see if I can add anything.  For me, it's SQS in us-east.  It's extremely transient for me.  I had it happen 5 times on 3/22/16 but nothing yesterday. \nHere is one of my failed requests, which was reported up the stack as \"AWS HTTP error: cURL error 56: TCP connection reset by peer\":\n```\n Hostname sqs.us-east-1.amazonaws.com was found in DNS cache\n   Trying 54.239.27.139...\n Connected to sqs.us-east-1.amazonaws.com (54.239.27.139) port 443 (#0)\n   CAfile: /etc/pki/tls/certs/ca-bundle.crt\n  CApath: none\n SSL connection using TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA\n Server certificate:\n       subject: CN=queue.amazonaws.com,O=\"Amazon.com, Inc.\",L=Seattle,ST=Washington,C=US\n       start date: Sep 25 00:00:00 2015 GMT\n       expire date: Dec 23 23:59:59 2016 GMT\n       common name: queue.amazonaws.com\n*       issuer: CN=Symantec Class 3 Secure Server CA - G4,OU=Symantec Trust Network,O=Symantec Corporation,C=US\n\nPOST /825286309336/attributeUpdateProd HTTP/1.1\nHost: sqs.us-east-1.amazonaws.com\nContent-Type: application/x-www-form-urlencoded\nX-Amz-Date: 20160322T064347Z\nAuthorization: AWS4-HMAC-SHA256 Credential=[KEY]/20160322/us-east-1/sqs/aws4_request, SignedHeaders=host;x-amz-date, Signature=[SIGNATURE]\nUser-Agent: aws-sdk-php/3.15.2 GuzzleHttp/6.1.1 curl/7.40.0 PHP/5.5.31\nContent-Length: 894\n\n\nupload completely sent off: 894 out of 894 bytes\nSSL read: errno -5961 (PR_CONNECT_RESET_ERROR)\nTCP connection reset by peer\nClosing connection 0\n```\n\nI haven't changed any curl reuse or certificates defaults.  I only changed the connect timeout to be smaller, and enabled debug logging.\n. Well, one thing I can share/ask: are you tracking internal retries of the API? \nTo log retries you can use the Middleware hooks. I also write to a rolling buffer in the hook to be retroactively logged if things are taking too long.  That's what the whole \"getGuzzleSlowRequestPlugin\" is all about.  In fact, I was really debugging slow requests and happened upon this curl error 56 issue. For me, I see around 1 in 10,000 requests to SQS that are >= 3 seconds.\nMy Middleware hook looks like this:\nprivate static function getClient() {\n        static $sqs = null;\n        if ($sqs == null) {\n            try {\n                $sqs = Aws::sdk()->createSqs([\n                    'debug' => [\n                        'logfn' => function ($msg) {\n                            self::getGuzzleSlowRequestPlugin()->log($msg);\n                        },\n                        'stream_size' => 0,\n                        'http' => true,\n                    ],\n                    'http' => [\n                        'on_stats' => function ($stats) {\n                            $handlerStatsMessage =\n                                \"HandlerErrorData: \" . print_r($stats->getHandlerErrorData(), true) . \"\\n\"\n                                . \"HandlerStats: \" . print_r($stats->getHandlerStats(), true);\n                            self::getGuzzleSlowRequestPlugin()->setLastHandlerStats($handlerStatsMessage);\n                            self::getGuzzleSlowRequestPlugin()->log($handlerStatsMessage);\n                        },\n                    ],\n                ]);\n                $sqs->getHandlerList()\n                    ->after('retry', '', function (callable $handler) {\n                        return function (\n                            \\Aws\\CommandInterface $command,\n                            \\Psr\\Http\\Message\\RequestInterface $request\n                        ) use ($handler) {\n                            return $handler($command, $request)\n                                ->then(\n                                    function ($result) {\n                                        //onFulfilled, I can peek at the success response here\n                                        return $result;\n                                    },\n                                    function ($value) {\n                                        //onRejected, probably an amazon exception\n                                        if ($value instanceof AwsException) {\n                                            $requestId = $value->getAwsRequestId();\n                                            $errorCode = $value->getAwsErrorCode();\n                                            $errorType = $value->getAwsErrorType();\n                                            $statusCode = $value->getStatusCode();\n                                            $connectionError = $value->isConnectionError();\n                                            $timeSinceStart = -1;\n                                            if (isset($_SERVER[\"REQUEST_TIME_FLOAT\"])) {\n                                                $timeSinceStart = microtime(true) - $_SERVER[\"REQUEST_TIME_FLOAT\"];\n                                            }\n                                            Logger::getLogger(__CLASS__)->error(\"sqs retry requestId[$requestId]\"\n                                                . \" errorCode[$errorCode] errorType[$errorType] statusCode[$statusCode]\"\n                                                . \" connectionError[$connectionError] timeSinceStart[$timeSinceStart]\"\n                                                . \" lastHandlerStats[\" . print_r(self::getGuzzleSlowRequestPlugin()->getLastHandlerStats(), true) . \"]\");\n                                        }\n                                        return \\GuzzleHttp\\Promise\\rejection_for($value);\n                                    }\n                                );\n                        };\n                    });\n            } catch (\\Exception $e) {\n                throw new AmazonException(\"Cannot get the SQS client.\", $e);\n            }\n        }\n        return $sqs;\n    } // getClient\n. Yes and no.  Internal (to the SDK) curl errors with retries are unavoidable.   But if you mess around with the sdk settings you can reduce the amount times you run out of retries causing an error to bubble up.  \nThe settings are here:\nhttps://docs.aws.amazon.com/aws-sdk-php/v3/guide/guide/configuration.html\nMy goal was to maximize API call success while minimizing time spent waiting on a bad remote server (or servers) so I globally changed:\n    retries, 3 -> 40\n    connect_timeout, infinity to 1 second\nI left the request timeout (timeout) as the infinity default, but I overrode it on a per service or per service method call (usually to 1 second) if I knew that service or service method was idempotent (e.g. doing the same request twice doesn't have any problematic side effects, trivial examples x++ is NOT idempotent but setting x to equal 3 is).  It's important to know that on connect_timeout or timeout the SDK internally retries, thus the retries bump from 3 to 40 (to give AWS time to reroute traffic from bad to good servers, or to allow a server to recover, etc...).\nI also added instrumentation to my code to peek into the SDK internal retries.  See the graph of my last week.\n\nYou can see that retries happen constantly (avg of 1 every 2 minutes).  But, there are spikes.  In this case ganglia is dividing the truth, and \"max 15.7\" was really a period where I saw ~300 retries in 1 minute.  It's important to know that I'm making on the order of 10k AWS API calls per minute so even 300 retries was barely noticed.  \n. This is a long thread now and I've contributed in two ways:\n1.) cURL error 56:  Searching my logs, I see this exception is exceedingly rare, but still happens.  By \"exceedingly rare\" I mean 4 times in 2017 out of what I guess are billions of calls to AWS services.   My info:\nphp55-5.5.38-2.119.amzn1.x86_64\ncurl-7.47.1-9.66.amzn1.x86_64\nopenssl-1.0.1k-15.96.amzn1.x86_64\nAWS SDK 3.24.4 \nGuzzle 6.2.3\nThe most recent failure was on 2017-06-07 10:27:04 GMT-4.  The stack trace:\nexception 'GuzzleHttp\\Exception\\RequestException' with message 'cURL error 56: TCP connection reset by peer (see http://curl.haxx.se/libcurl/c/libcurl-errors.html)' in REDACTED/vendor/guzzlehttp/guzzle/src/Handler/CurlFactory.php:187\nStack trace:\n0 REDACTED/vendor/guzzlehttp/guzzle/src/Handler/CurlFactory.php(150): GuzzleHttp\\Handler\\CurlFactory::createRejection(Object(GuzzleHttp\\Handler\\EasyHandle), Array)\n1 REDACTED/vendor/guzzlehttp/guzzle/src/Handler/CurlFactory.php(103): GuzzleHttp\\Handler\\CurlFactory::finishError(Object(GuzzleHttp\\Handler\\CurlMultiHandler), Object(GuzzleHttp\\Handler\\EasyHandle), Object(GuzzleHttp\\Handler\\CurlFactory))\n2 REDACTED/vendor/guzzlehttp/guzzle/src/Handler/CurlMultiHandler.php(180): GuzzleHttp\\Handler\\CurlFactory::finish(Object(GuzzleHttp\\Handler\\CurlMultiHandler), Object(GuzzleHttp\\Handler\\EasyHandle), Object(GuzzleHttp\\Handler\\CurlFactory))\n3 REDACTED/vendor/guzzlehttp/guzzle/src/Handler/CurlMultiHandler.php(108): GuzzleHttp\\Handler\\CurlMultiHandler->processMessages()\n4 REDACTED/vendor/guzzlehttp/guzzle/src/Handler/CurlMultiHandler.php(123): GuzzleHttp\\Handler\\CurlMultiHandler->tick()\n5 REDACTED/vendor/guzzlehttp/promises/src/Promise.php(246): GuzzleHttp\\Handler\\CurlMultiHandler->execute(true)\n6 REDACTED/vendor/guzzlehttp/promises/src/Promise.php(223): GuzzleHttp\\Promise\\Promise->invokeWaitFn()\n7 REDACTED/vendor/guzzlehttp/promises/src/Promise.php(267): GuzzleHttp\\Promise\\Promise->waitIfPending()\n8 REDACTED/vendor/guzzlehttp/promises/src/Promise.php(225): GuzzleHttp\\Promise\\Promise->invokeWaitList()\n9 REDACTED/vendor/guzzlehttp/promises/src/Promise.php(267): GuzzleHttp\\Promise\\Promise->waitIfPending()\n10 REDACTED/vendor/guzzlehttp/promises/src/Promise.php(225): GuzzleHttp\\Promise\\Promise->invokeWaitList()\n11 REDACTED/vendor/guzzlehttp/promises/src/Promise.php(62): GuzzleHttp\\Promise\\Promise->waitIfPending()\n12 REDACTED/vendor/aws/aws-sdk-php/src/AwsClientTrait.php(59): GuzzleHttp\\Promise\\Promise->wait()\n13 REDACTED/vendor/aws/aws-sdk-php/src/AwsClientTrait.php(78): Aws\\AwsClient->execute(Object(Aws\\Command))\nThis error is a bummer because the SDK doesn't try to retry it.  I think it might be the only error like that?\n2.) The general issue of failures at the curl level.  I'm able to control the time a call takes and the success rate of the call by configuring timeouts and number of retries (see my most previous post).  In my log file processing I break internal retries into the following categories:\ncapacity = DynamoDB specific and is generally zero.\nerror = {5xx, PR_CONNECT_RESET_ERROR, PR_NOT_CONNECTED_ERROR}.  Generally has been decreasing over time.  A year ago I could have measured this category at the hour level (e.g. at least 1 event/hour), but recently I would have to expand to day (and even then I have days without an error).\ntimeout = {timed out before SSL handshake, Connection timed out after, Resolving timed out after, Operation timed out}.  This is steady and I see happen at the \"10 minute level\" (e.g. at least 1 event every 10 minutes).  Again, as previously noted I have extremely agressive timeout settings and a generous number of retries.. curl error 56: \nis this a change?  I was told curl errorno 56 is not retried other than AWS API calls that are guaranteed to be idempotent:\nhttps://github.com/aws/aws-sdk-php/issues/983\nfewer errors: \nmy error tracking doesn't distinguish between {5xx, PR_CONNECT_RESET_ERROR, PR_NOT_CONNECTED_ERROR}.  I peeked at today's logging and the 2 errors I had in the last four hours were 500 errors against DynamoDB (that retried and succeeded).  The last time I had consistent networking problems was before August 2016.  AWS released a server-side fix (details of that hidden from me) of DyanmoDB around then that dropped that class of problem I was having.\n. I think we're mixing things up by talking about different issues at the same time:\ncurl 56 = I see no upward or downward trend.  It's extremely rare (a small # per month).  I see it in SQS and DDB, but that makes sense since those are my two highest volume APIs.  I dug up the latest time it happened and will paste it at the bottom.  \nAWS API errors = I've seen two plateaus.  One before August 2016 and one after.  It was higher before and lower now.  I believe before was networking errors + 5xx errors.  After is just 5xx errors.  \nThe last curl 56 error I saw:\n```\n Rebuilt URL to: https://dynamodb.us-east-1.amazonaws.com/\n Hostname dynamodb.us-east-1.amazonaws.com was found in DNS cache\n   Trying 52.119.226.86...\n Connected to dynamodb.us-east-1.amazonaws.com (52.119.226.86) port 443 (#0)\n   CAfile: /etc/pki/tls/certs/ca-bundle.crt\n  CApath: none\n ALPN/NPN, server did not agree to a protocol\n SSL connection using TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256\n Server certificate:\n       subject: CN=dynamodb.us-east-1.amazonaws.com,O=\"Amazon.com, Inc.\",L=Seattle,ST=Washington,C=US\n       start date: Sep 30 00:00:00 2016 GMT\n       expire date: Dec 02 23:59:59 2017 GMT\n       common name: dynamodb.us-east-1.amazonaws.com\n*       issuer: CN=Symantec Class 3 Secure Server CA - G4,OU=Symantec Trust Network,O=Symantec Corporation,C=US\n\nPOST / HTTP/1.1\nHost: dynamodb.us-east-1.amazonaws.com\nX-Amz-Target: DynamoDB_20120810.Query\nContent-Type: application/x-amz-json-1.0\naws-sdk-invocation-id: 4190b86981be52048a57ea4ac7a102e4\naws-sdk-retry: 0/0\nX-Amz-Date: 20170607T142703Z\nAuthorization: AWS4-HMAC-SHA256 Credential=[KEY]/20170607/us-east-1/dynamodb/aws4_request, SignedHeaders=aws-sdk-invocation-id;aws-sdk-retry;host;x-amz-date;x-amz-target, Signature=[SIGNATURE]\nUser-Agent: aws-sdk-php/3.24.4 GuzzleHttp/6.2.1 curl/7.47.1 PHP/5.5.38\nContent-Length: 295\n\n\nupload completely sent off: 295 out of 295 bytes\nSSL read: errno -5961 (PR_CONNECT_RESET_ERROR)\nTCP connection reset by peer\nClosing connection 0\n```. While researching my options (which including messing with retry middleware) I found #767.  It sounds like this is a \"rare but persistent\" issue that happens to multiple SDK consumers, so I'm proposing the SDK handle it for everyone by default rather than by custom configuration. \n. This is great information!  I agree it's clearly not appropriate for the SDK to blindly retry CURL_RECV_ERROR.\n\nI'm trying to figure out if there is a generic way of detecting which exceptions to retry if I know the call is idempotent.  I see that AwsException provides getAwsErrorType().  I think the possible values are null, \"client\" and \"server\" (strings) and I'd want to retry \"server\".  What about null?  It's hard to puzzle through from the code but I think it's null on connection errors.  I'm not sure what other conditions lead to null though...\n. My backend processing can handle duplicate SQS messages.  I believe everyone is in the same boat (needing to ensure SQS processing is idempotent) because of the \"at least once\" guarantee of ReceiveMessage.\nI believe I've also seen this particular error in DynamoDB API calls, but I'd have to dig through old logging to be sure.  The only two APIs I use at high volume are SQS and DynamoDB, so it makes sense that a rare event like this would pop out on those two for me.\nI pretty much try to make sure all my AWS API calls either are either idempotent or at least don't cause chaos if applied multiple times due to these kinds of networking glitch possibilities.  What I haven't been great at is knowing all of the places in configuration or integration of the SDK to properly retry requests to maximize the probability they are applied \"at least once\".\n. Clearly I need to upgrade to a newer version of the SDK!  This is a duplicate of #937. \n. ",
    "fredden": "@jeskew, any chance you could add a call to Phar::mapPhar('aws.phar') into the stub for the v2.8 branch? We're getting this error with v2.8.19.\n. ",
    "neoacevedo": "for any who uses AWS Phar: the main issue is in the require_once that invokes only once the Phar file and \"let it\" in memory. While, you should to use require or include moreover if you have to make more than one calling to aws.phar file.\n. OK, with aws.phar 3.54.3 I'm having this performance using it on Amazon S3.. I think the scheme key is the correct option. I unknew this option.. Good, my algorithm checks if the directory is empty. If isn't empty doesn't allow to delete it, if is empty, it can be deleted.\nBut in previous versions of aws SDK (phar version < 3.54) I was able to delete it calling deleteObject and passing as key the directory path, but from the phar version 3.55.12, I'm not able to achieve it.. Actually I make it but it simply, doesn't work.. Yes, I add a /. The main problem was in:\n$results = $s3->listObjects([\n    'Bucket' => 'your-bucket',\n    'Prefix' => 'path/to/your/directory/'\n]);\nin $results variable, Contents array was returning as first element the Prefix (I don't know why, in the documentation is not any explanataion) with or witouh any other element inside Contents, adding or not 'Delimiter' => '/' I got the same result, so I had to delete that first element into Contents array and finally become deleted.\nSame performance with iterator (it didn't delete the directory):\n```\n$results = $s3->listObjectsV2([\n    'Bucket' => 'your-bucket',\n    'Prefix' => 'path/to/your/directory'\n]);\nif (isset($results['Contents'])) {\n    foreach ($results['Contents'] as $result) {\n        $s3->deleteObject([\n            'Bucket' => 'your-bucket',\n            'Key' => $result['Key']\n        ]);\n    }\n}\n```\nAfter removing the first element into Contents array, it could delete the directory.. Hi @diehlaws, I'm sorry for my late answer.\nIn effect, I have contacted to the TYPO3's project owner https://github.com/TYPO3/phar-stream-wrapper/issues/21\nI was thinking TYPO3 was part of AWS SDK.\nKind Regards.. ",
    "lifeofguenter": "Yes makes more sense - was actually searching for an example only to find them in the same file ;)\nI changed the commit accordingly.\n. +1. ",
    "grepollo": "Hi, \nThe scenario is I can save the data with Chinese character, then when I get that data and use putItem to another Table the issue will occur, I already found some way around, it has something to do with character encoding. \nSo what I did is convert to utf8 the text first before saving it to the table and it seems working. \nDo you have better suggestion on how to do it? Or is there an option in getItem to set it to utf8 character encoding always? \nThanks. \n. Hi, \nI tried to check the JsonCompiler code, I saw the above code which has an option to set cache directory:\nconst CACHE_ENV = 'AWS_PHP_CACHE_DIR';\nSo what I did is add that variable on my env files and set the new path, and its now working. If someone has the same issue with this, just change the path and make sure that directory is writeable. \nThanks.\n. ",
    "oleghind": "Request that SDK generates looks like this (extensions of the files are cut off) and I still can't find at what point the SDK doing this. Any ideas?\njavascript\n<?xml version=\"1.0\"?>\n<Delete xmlns=\"http://s3.amazonaws.com/doc/2006-03-01/\"><Object>\n<Key>2b/e1/3a/bf/33c663ff370496f2965c0d702a023e59</Key></Object><Object>\n<Key>2b/e1/3a/bf/33c663ff370496f2965c0d702a023e59</Key></Object><Object>\n<Key>2b/e1/3a/bf/33c663ff370496f2965c0d702a023e59</Key></Object></Delete>\n. AWS version 2.8.4, OS Ubuntu 14.04 server\n``` javascript\nRequest:\nPOST /?delete HTTP/1.1\nHost: .s3.amazonaws.com\nUser-Agent: aws-sdk-php2/2.8.4 Guzzle/3.9.3 curl/7.35.0 PHP/5.5.9-1ubuntu4.9\nContent-Type: application/xml\nContent-MD5: 5ubmUwuogmPI6rTSP7zt9Q==\nDate: Mon, 18 May 2015 17:59:26 +0000\nAuthorization: AWS *\nContent-Length: 304\nxml version=\"1.0\"?\n2b/e1/3a/bf/33c663ff370496f2965c0d702a023e592b/e1/3a/bf/33c663ff370496f2965c0d702a023e592b/e1/3a/bf/33c663ff370496f2965c0d702a023e59\nResponse:\nHTTP/1.1 200 OK\nx-amz-id-2: AHNqQB+XU3+CiES34oD2qe2co0HN0UhNf7FTIRcNUppi8rdrbnMIcYFBP0+MpcCRtrMKFgY1YZw=\nx-amz-request-id: A2F4F0AAEE27EB13\nDate: Mon, 18 May 2015 17:59:13 GMT\nConnection: close\nContent-Type: application/xml\nTransfer-Encoding: chunked\nServer: AmazonS3\nxml version=\"1.0\" encoding=\"UTF-8\"?\n2b/e1/3a/bf/33c663ff370496f2965c0d702a023e59\nErrors: 0\n```\n. I see that in constructor of AbstractCommand.php of Guzzle $parameters are correct, but in EntityBody.php in the factory function $resource contains array of files with extensions cut off, so it is somewhere in between, maybe you can give me some ideas where Guzzle performing any manipulations with the data and I can try to debug\n. Ok this cut happens  during $xmlWriter->writeRaw of the XmlVisitor.php of Guzzle package, so might be a bug in my version of xmlwriter, closed.\n. ",
    "pako-pl": "Any suggestions as to how to work around this issue until the bug gets fixed? Right now we have to manually deploy our changes to Lambda functions.\n. Sorry, I missed the note about SNS Message Validator (probably because the specific class name wasn't mentioned there). Great to hear that the package will be released soon.\n. Edit: Changing \"region\" to \"us-east-1\" fixed the problem. Maybe it should be included in the documentation?\n. ",
    "halkyon": "According to https://github.com/aws/aws-sdk-php/releases/tag/2.8.29 it was fixed in 2.8.29, but I'm running this version and still getting the same \"Please provide a source for function code.\" result even though I'm passing a base64-encoded string to the ZipFile argument. Any ideas?\n. @jeskew awesome! Thanks for that.\n. ",
    "shadiakiki1986": "Hi Michael. Thanks for your feedback.\nMy bad, I meant 7.22 and 7.36 instead of 1.22 and 1.36.\nThe tests pass on both my machine and on my EC2 instance with cURL 7.22, which is why I didn't try updating cURL during the travis setup to be >= 7.28. The difference between my machine and the travis machine is that the travis worker is using gnutls, as you can see here whereas my machine is using OpenSSL/1.0.1.\nI really think that this is a matter of php compiled with gnutls (doesn't work) versus openssl (works). It seems to me that since the AWS dynamodb connection falls back to a different server IP with each connection (_54.239.20.120 here in build 33, _176.32.101.251 here in build 34), some of the AWS dynamodb servers are setup to allow the connection to go through, and some others not. For example, I just repeated build 34 a few times. The first couple of times both the testUpdateItem and testPutItem were failing. In the last time, the testUpdateItem passed and only the testPutItem failed. What's special about _176.32.101.251 versus _54.239.20.120 so that the former would pass for the updateItem test? Am I thinking along the right lines?\nPS: I have yet to try to compile php with openssl on the travis worker and show that the tests pass, but I think that would require sudo. I've been avoiding using sudo so as not to have to wait long between builds, as described here\n. No I didn't. Let me try and get back to you\n. So I installed curl 7.35. Still, some builds are passing and other builds are not, but now instead of getting the error message \"gnutls fatal error\", I get '''illegal parameter url''' (ref line). I read somewhere that it's because I'm using gnutls 2.* (ref line ), so I tried to upgrade gnutls to 3.1.28 by compiling from source, but I failed at making this work. With apt-get I managed to upgrade gnutls from '''2.12.14''' to '''2.12.23'''. Here is the log file. You can see this by searching for the text \"2.12.23\" (sorry, no direct link to the line because the log file is too long to be displayed in travis-ci). But since I need to go up to gnutls 3.*, I compiled it from source. If you search for \"pkg-config --modversion gnutls\", you can see that the library is being installed as 3.1.28, but the '''curl_version''' still gives ssl_version being \"GnuTLS/2.12.23\" after the install. So my problem now is getting php to use the gnutls I compiled. I launched a separate project to focus on the php-gnutls issue, which would help solve this issue here. I'll post to the gnutls mailing list to ask for some help \n. I compiled curl 7.42.1 with gnutls 3.1.28 from source. On a vanilla ubuntu 12.04 installation, php automatically links to the newly compiled libraries and my phpunit tests to dynamodb pass without any gnutls fatal errors. However, I couldn't get php on the travis-ci worker (which uses ubuntu 12.04 and phpenv) to link to my newly compiled libraries, so my same code with the same phpunit tests fail on travis-ci. My log file on line 2327 shows php using gnutls 2.12. Line 2371 shows that gnutls 3.1.28 is correctly installed. Line 2384 shows the library that I suspect is keeping php from using my compiled version. Any thoughts on this? I feel that I'm stuck on something that will turn out to be fundamentally silly\n. I just managed to pass the project on travis-ci. The problem was the php from phpenv. Once I installed php5-cli and specified /usr/bin/php5 to be used in the .travis.yml file, I managed to use my compiled gnutls and curl. More on this later \n. I pretty much have nothing to add to this. Here's the build that passes. I'm closing this issue as I'm content with my workaround of installing a separate CLI for php and using that. Thanks Michael\n. ",
    "KacerCZ": "Encountered this issue after switching from AWS SDK 2.x to 3.0.3.\nCalling fstat() on open stream always returns size 0.\n. ",
    "vuchl": "Yepp, that's what I thought.\n. ",
    "sinky": "Ahhhh,\ngetObjectUrl no longer has a third parameter.\nThis Works:\n```\n$cmd = $client->getCommand('GetObject', [\n    'Bucket' => $bucket,\n    'Key'    => 'test1G.dat'\n]);\n$request = $client->createPresignedRequest($cmd, '+20 minutes');\n$presignedUrl = (string) $request->getUri();\n```\n. ",
    "haoxi911": "Hi @sinky,\nI met exactly same issue when upgrade from sdk v2 to v3, and also, I realized that v2 API allows us to sign a URL with a maximum expiration date up to 80+ years, and I used such way to make a permanent link.\nHowever, the v3 SDK only allows us to set expiration date up to 1 week, so I didn't see a workaround to create a permanent accessible signed URL.\nDo you have any ideas? Note: I am able to configure the bucket policy to make it public read only, but want to know if there is another way.\nThanks,\nKevin. For anyone met this issue, you may want to encode your S3 object keys with htmlspecialchars($key, ENT_XML1 | ENT_QUOTES, 'UTF-8');.\ndeleteObjects is a little bit different from other S3 APIs, that it sends payload as XML text, hence we need to escape characters like & < > \" before composing the request.\n. More information...\n```\n\n\n\ndate_default_timezone_set('America/New_York')\n$startTimestamp = time();\n$timestamp = strtotime('+6 days 3 hours', $startTimestamp)\n=> 1541318214\n$timestamp - $startTimestamp\n=> 532800\n$timestamp = strtotime('+6 days 2 hours', $startTimestamp)\n=> 1541311014\n$timestamp - $startTimestamp\n=> 525600\n```\n\n\n\n. Hi @diehlaws, thanks for your explanation. I think I will want to keep my server time same as New York time (we are in GA). Do you know if there is a best practice or something that discourage us to do so?\nFor now, I am using +6 days 23 hours. Thanks!. @diehlaws Thank you!. ",
    "stuartcarnie": "I added that, and received an error:\nfprintf(): 51 is not a valid stream resource in /var/www/develop/odin/core/vendor/guzzlehttp/guzzle/src/Handler/StreamHandler.php:367:\nWhich is here\n. :+1: \n. ",
    "nikunjtrapasiya": "yup that work. But surprisingly the KeyPrefix code was working 2 hours ago and not any more.\nI will try all the methods and report any issues.\n. All the methods are working fine with above suggestion by @jeremeamia \n. ",
    "UlrichEckhardt": "I just looked over the changes and the test surprises me. file_get_contents() there seems to be expected to throw an exception, is that right? Reason I ask is that that's a breaking interface change when compared to PHP's implementation, which returns false in case the files isn't present or not readable.\n. Oh, yes, that makes sense. Thank you!\n. Which version are you using? This issue was only fixed in version 2.8.13 of the 2.x branch and I'm not sure about the 3.x branch at all, neither whether the bug as present there nor whether it was fixed there.\n. ",
    "billwhitney": "I am having problems with Stream Wrapper file_get_contents not returning FALSE on an error. In this case the error is \"file does not exist\". file_get_contents throws an exception, or if I use @file_get_contents it does not return FALSE but instead returns empty file contents. Am I missing something? The above comment by UlrichEckhardt still seems to be an issue.\n. I am using 2.8.14. When I used an earlier version it did not even throw an exception, so I assume I am using a \"fixed\" version.\n. ",
    "smagnaschi": "Sorry for being late.\nThe problem in the path was due to github textbox stripping slashes.\nThe problem happens when the path starts with a double slash \\ (a UNC path for example), this gets translated and normalized to a single slash \\ which breaks the path.\nI've used the modified JsonCompiler class from your last commit but the problem remains.\nHere is a stack trace\n```\nFatal error:  Uncaught exception 'InvalidArgumentException' with message 'File not found: \\SIMONE-PC\\USERS\\Simone\\Documents\\WebProjects\\devel.rockol.it\\app\\vendor\\aws\\aws-sdk-php\\src\\Api/../data/manifest.json, realpath: /SIMONE-PC/USERS/Simone/Documents/WebProjects/devel.rockol.it/app/vendor/aws/aws-sdk-php/src/data/manifest.json' in \\SIMONE-PC\\USERS\\Simone\\Documents\\WebProjects\\devel.rockol.it\\app\\vendor\\aws\\aws-sdk-php\\src\\JsonCompiler.php:151\nStack trace:\n0 \\SIMONE-PC\\USERS\\Simone\\Documents\\WebProjects\\devel.rockol.it\\app\\vendor\\aws\\aws-sdk-php\\src\\JsonCompiler.php(79): Aws\\JsonCompiler->loadJsonFromFile('\\\\SIMONE-PC\\USE...', '/SIMONE-PC/USER...')\n1 \\SIMONE-PC\\USERS\\Simone\\Documents\\WebProjects\\devel.rockol.it\\app\\vendor\\aws\\aws-sdk-php\\src\\functions.php(157): Aws\\JsonCompiler->load('\\\\SIMONE-PC\\USE...')\n2 \\SIMONE-PC\\USERS\\Simone\\Documents\\WebProjects\\devel.rockol.it\\app\\vendor\\aws\\aws-sdk-php\\src\\Api\\ApiProvider.php(96): Aws\\load_compiled_json('\\\\SIMONE-PC\\USE...')\n3 \\SIMONE-PC\\USERS\\Simone\\Documents in \\SIMONE-PC\\USERS\\Simone\\Documents\\WebProjects\\devel.rockol.it\\app\\vendor\\aws\\aws-sdk-php\\src\\JsonCompiler.php on line 151\n```\n. I tried this and it seems to work ok for my case: https://github.com/aws/aws-sdk-php/pull/613 \n. ",
    "mascreativa": "It worked! Thank you.\n. ",
    "NoahTwine": "Thanks for the super-quick answer! So I guess we will have to change every instance were we call the SDK.\nOne other question: After declaring the version number at an instance of the SDK, we get another error saying that the AWS credentials under .aws/credentials could not be accessed.\nSpecifically:\nUncaught exception 'Aws\\Exception\\CredentialsException' with message 'Error retrieving credentials from the instance profile metadata server. (cURL error 28: Connection timed out after 1005 milliseconds (see http://curl.haxx.se/libcurl/c/libcurl-errors.html))' in /Users/MYUSER/DIRECTORY-OF-PROJECT/lib/vendor/aws/aws-sdk-php/src/Credentials/InstanceProfileProvider.php\nCurrently these credentials are in the root under  ~.aws/credentials. Does version 3 of the SDK expect these credentials to have a different level or permissions or be placed in a different location?\nThat would be help to know. Thanks.\n. @mtdowling thanks much for your help. Yes we are good to close.\n. ",
    "smkhawaja": "I have the same issue. Running a php job on my vmware workstation guest and getting the following error\n[Aws\\Exception\\CredentialsException]\n  Error retrieving credentials from the instance profile metadata server. (cURL error 28:\n  Connection timed out after 1002 milliseconds (see http://curl.haxx.se/libcurl/c/libcurl-\n  errors.html))\naws s3 ls commands working fine. \n. ",
    "foaly-nr1": "My bad. Got going with 2.8.9 as it's tagged \"Latest release\" and I didn't check further.\n. ",
    "EwanValentine": "curl setopt's are unsupported by HHVM 3.7, upgrading to nightly builds (3.8.0-dev) resolved this for the time being. \n. Full stack trace\n```\nexception 'GuzzleHttp\\Exception\\ClientException' with message 'Client error: 403' in /home/vagrant/65Contrib/vendor/guzzlehttp/guzzle/src/Middleware.php:68\nStack trace:\n0 /home/vagrant/65Contrib/vendor/guzzlehttp/promises/src/Promise.php(199): GuzzleHttp\\Middleware::GuzzleHttp{closure}(Object(GuzzleHttp\\Psr7\\Response))\n1 /home/vagrant/65Contrib/vendor/guzzlehttp/promises/src/Promise.php(165): GuzzleHttp\\Promise\\Promise::callHandler(1, Object(GuzzleHttp\\Psr7\\Response), Array)\n2 /home/vagrant/65Contrib/vendor/guzzlehttp/promises/src/FulfilledPromise.php(39): GuzzleHttp\\Promise\\Promise::GuzzleHttp\\Promise{closure}(Object(GuzzleHttp\\Psr7\\Response))\n3 /home/vagrant/65Contrib/vendor/guzzlehttp/promises/src/TaskQueue.php(60): GuzzleHttp\\Promise\\FulfilledPromise::GuzzleHttp\\Promise{closure}()\n4 /home/vagrant/65Contrib/vendor/guzzlehttp/guzzle/src/Handler/CurlMultiHandler.php(96): GuzzleHttp\\Promise\\TaskQueue->run()\n5 /home/vagrant/65Contrib/vendor/guzzlehttp/guzzle/src/Handler/CurlMultiHandler.php(123): GuzzleHttp\\Handler\\CurlMultiHandler->tick()\n6 /home/vagrant/65Contrib/vendor/guzzlehttp/promises/src/Promise.php(240): GuzzleHttp\\Handler\\CurlMultiHandler->execute(true)\n7 /home/vagrant/65Contrib/vendor/guzzlehttp/promises/src/Promise.php(217): GuzzleHttp\\Promise\\Promise->invokeWaitFn()\n8 /home/vagrant/65Contrib/vendor/guzzlehttp/promises/src/Promise.php(261): GuzzleHttp\\Promise\\Promise->waitIfPending()\n9 /home/vagrant/65Contrib/vendor/guzzlehttp/promises/src/Promise.php(219): GuzzleHttp\\Promise\\Promise->invokeWaitList()\n10 /home/vagrant/65Contrib/vendor/guzzlehttp/promises/src/Promise.php(261): GuzzleHttp\\Promise\\Promise->waitIfPending()\n11 /home/vagrant/65Contrib/vendor/guzzlehttp/promises/src/Promise.php(219): GuzzleHttp\\Promise\\Promise->invokeWaitList()\n12 /home/vagrant/65Contrib/vendor/guzzlehttp/promises/src/Promise.php(62): GuzzleHttp\\Promise\\Promise->waitIfPending()\n13 /home/vagrant/65Contrib/vendor/aws/aws-sdk-php/src/AwsClient.php(201): GuzzleHttp\\Promise\\Promise->wait()\n14 /home/vagrant/65Contrib/vendor/aws/aws-sdk-php/src/AwsClient.php(166): Aws\\AwsClient->execute(Object(Aws\\Command))\n15 /home/vagrant/65Contrib/vendor/laravel/framework/src/Illuminate/Queue/SqsQueue.php(100): Aws\\AwsClient->__call('receiveMessage', Array)\n16 /home/vagrant/65Contrib/vendor/laravel/framework/src/Illuminate/Queue/SqsQueue.php(100): Aws\\Sqs\\SqsClient->receiveMessage(Array)\n17 /home/vagrant/65Contrib/vendor/laravel/framework/src/Illuminate/Queue/Worker.php(171): Illuminate\\Queue\\SqsQueue->pop()\n18 /home/vagrant/65Contrib/vendor/laravel/framework/src/Illuminate/Queue/Worker.php(145): Illuminate\\Queue\\Worker->getNextJob(Object(Illuminate\\Queue\\SqsQueue), NULL)\n19 /home/vagrant/65Contrib/vendor/laravel/framework/src/Illuminate/Queue/Worker.php(109): Illuminate\\Queue\\Worker->pop('sqs', NULL, 0, '3', '3')\n20 /home/vagrant/65Contrib/vendor/laravel/framework/src/Illuminate/Queue/Worker.php(85): Illuminate\\Queue\\Worker->runNextJobForDaemon('sqs', NULL, 0, '3', '3')\n21 /home/vagrant/65Contrib/vendor/laravel/framework/src/Illuminate/Queue/Console/WorkCommand.php(103): Illuminate\\Queue\\Worker->daemon('sqs', NULL, 0, 128, '3', '3')\n22 /home/vagrant/65Contrib/vendor/laravel/framework/src/Illuminate/Queue/Console/WorkCommand.php(71): Illuminate\\Queue\\Console\\WorkCommand->runWorker('sqs', NULL, 0, 128, true)\n23 [internal function]: Illuminate\\Queue\\Console\\WorkCommand->fire()\n24 /home/vagrant/65Contrib/vendor/laravel/framework/src/Illuminate/Container/Container.php(502): call_user_func_array(Array, Array)\n25 /home/vagrant/65Contrib/vendor/laravel/framework/src/Illuminate/Console/Command.php(150): Illuminate\\Container\\Container->call(Array)\n26 /home/vagrant/65Contrib/vendor/symfony/console/Command/Command.php(259): Illuminate\\Console\\Command->execute(Object(Symfony\\Component\\Console\\Input\\ArgvInput), Object(Symfony\\Component\\Console\\Output\\ConsoleOutput))\n27 /home/vagrant/65Contrib/vendor/laravel/framework/src/Illuminate/Console/Command.php(136): Symfony\\Component\\Console\\Command\\Command->run(Object(Symfony\\Component\\Console\\Input\\ArgvInput), Object(Symfony\\Component\\Console\\Output\\ConsoleOutput))\n28 /home/vagrant/65Contrib/vendor/symfony/console/Application.php(878): Illuminate\\Console\\Command->run(Object(Symfony\\Component\\Console\\Input\\ArgvInput), Object(Symfony\\Component\\Console\\Output\\ConsoleOutput))\n29 /home/vagrant/65Contrib/vendor/symfony/console/Application.php(195): Symfony\\Component\\Console\\Application->doRunCommand(Object(Illuminate\\Queue\\Console\\WorkCommand), Object(Symfony\\Component\\Console\\Input\\ArgvInput), Object(Symfony\\Component\\Console\\Output\\ConsoleOutput))\n30 /home/vagrant/65Contrib/vendor/symfony/console/Application.php(126): Symfony\\Component\\Console\\Application->doRun(Object(Symfony\\Component\\Console\\Input\\ArgvInput), Object(Symfony\\Component\\Console\\Output\\ConsoleOutput))\n31 /home/vagrant/65Contrib/vendor/laravel/framework/src/Illuminate/Foundation/Console/Kernel.php(94): Symfony\\Component\\Console\\Application->run(Object(Symfony\\Component\\Console\\Input\\ArgvInput), Object(Symfony\\Component\\Console\\Output\\ConsoleOutput))\n32 /home/vagrant/65Contrib/artisan(36): Illuminate\\Foundation\\Console\\Kernel->handle(Object(Symfony\\Component\\Console\\Input\\ArgvInput), Object(Symfony\\Component\\Console\\Output\\ConsoleOutput))\n33 {main}\nNext exception 'Aws\\Sqs\\Exception\\SqsException' with message 'Error executing \"ReceiveMessage\" on \"https://sqs.us-east-1.amazonaws.com/your-queue-url\"; AWS HTTP error: Client error: 403 SignatureDoesNotMatch (client): Signature expired: 20150619T100249Z is now earlier than 20150619T100713Z (20150619T102213Z - 15 min.) - <?xml version=\"1.0\"?>SenderSignatureDoesNotMatchSignature expired: 20150619T100249Z is now earlier than 20150619T100713Z (20150619T102213Z - 15 min.)c0040c63-5c2f-55a9-8be0-419eb3ba4217' in /home/vagrant/65Contrib/vendor/aws/aws-sdk-php/src/WrappedHttpHandler.php:152\nStack trace:\n0 /home/vagrant/65Contrib/vendor/aws/aws-sdk-php/src/WrappedHttpHandler.php(76): Aws\\WrappedHttpHandler->parseError(Array, Object(GuzzleHttp\\Psr7\\Request), Object(Aws\\Command))\n1 /home/vagrant/65Contrib/vendor/guzzlehttp/promises/src/Promise.php(199): Aws\\WrappedHttpHandler->Aws{closure}(Array)\n2 /home/vagrant/65Contrib/vendor/guzzlehttp/promises/src/Promise.php(170): GuzzleHttp\\Promise\\Promise::callHandler(2, Array, Array)\n3 /home/vagrant/65Contrib/vendor/guzzlehttp/promises/src/RejectedPromise.php(40): GuzzleHttp\\Promise\\Promise::GuzzleHttp\\Promise{closure}(Array)\n4 /home/vagrant/65Contrib/vendor/guzzlehttp/promises/src/TaskQueue.php(60): GuzzleHttp\\Promise\\RejectedPromise::GuzzleHttp\\Promise{closure}()\n5 /home/vagrant/65Contrib/vendor/guzzlehttp/guzzle/src/Handler/CurlMultiHandler.php(96): GuzzleHttp\\Promise\\TaskQueue->run()\n6 /home/vagrant/65Contrib/vendor/guzzlehttp/guzzle/src/Handler/CurlMultiHandler.php(123): GuzzleHttp\\Handler\\CurlMultiHandler->tick()\n7 /home/vagrant/65Contrib/vendor/guzzlehttp/promises/src/Promise.php(240): GuzzleHttp\\Handler\\CurlMultiHandler->execute(true)\n8 /home/vagrant/65Contrib/vendor/guzzlehttp/promises/src/Promise.php(217): GuzzleHttp\\Promise\\Promise->invokeWaitFn()\n9 /home/vagrant/65Contrib/vendor/guzzlehttp/promises/src/Promise.php(261): GuzzleHttp\\Promise\\Promise->waitIfPending()\n10 /home/vagrant/65Contrib/vendor/guzzlehttp/promises/src/Promise.php(219): GuzzleHttp\\Promise\\Promise->invokeWaitList()\n11 /home/vagrant/65Contrib/vendor/guzzlehttp/promises/src/Promise.php(261): GuzzleHttp\\Promise\\Promise->waitIfPending()\n12 /home/vagrant/65Contrib/vendor/guzzlehttp/promises/src/Promise.php(219): GuzzleHttp\\Promise\\Promise->invokeWaitList()\n13 /home/vagrant/65Contrib/vendor/guzzlehttp/promises/src/Promise.php(62): GuzzleHttp\\Promise\\Promise->waitIfPending()\n14 /home/vagrant/65Contrib/vendor/aws/aws-sdk-php/src/AwsClient.php(201): GuzzleHttp\\Promise\\Promise->wait()\n15 /home/vagrant/65Contrib/vendor/aws/aws-sdk-php/src/AwsClient.php(166): Aws\\AwsClient->execute(Object(Aws\\Command))\n16 /home/vagrant/65Contrib/vendor/laravel/framework/src/Illuminate/Queue/SqsQueue.php(100): Aws\\AwsClient->__call('receiveMessage', Array)\n17 /home/vagrant/65Contrib/vendor/laravel/framework/src/Illuminate/Queue/SqsQueue.php(100): Aws\\Sqs\\SqsClient->receiveMessage(Array)\n18 /home/vagrant/65Contrib/vendor/laravel/framework/src/Illuminate/Queue/Worker.php(171): Illuminate\\Queue\\SqsQueue->pop()\n19 /home/vagrant/65Contrib/vendor/laravel/framework/src/Illuminate/Queue/Worker.php(145): Illuminate\\Queue\\Worker->getNextJob(Object(Illuminate\\Queue\\SqsQueue), NULL)\n20 /home/vagrant/65Contrib/vendor/laravel/framework/src/Illuminate/Queue/Worker.php(109): Illuminate\\Queue\\Worker->pop('sqs', NULL, 0, '3', '3')\n21 /home/vagrant/65Contrib/vendor/laravel/framework/src/Illuminate/Queue/Worker.php(85): Illuminate\\Queue\\Worker->runNextJobForDaemon('sqs', NULL, 0, '3', '3')\n22 /home/vagrant/65Contrib/vendor/laravel/framework/src/Illuminate/Queue/Console/WorkCommand.php(103): Illuminate\\Queue\\Worker->daemon('sqs', NULL, 0, 128, '3', '3')\n23 /home/vagrant/65Contrib/vendor/laravel/framework/src/Illuminate/Queue/Console/WorkCommand.php(71): Illuminate\\Queue\\Console\\WorkCommand->runWorker('sqs', NULL, 0, 128, true)\n24 [internal function]: Illuminate\\Queue\\Console\\WorkCommand->fire()\n25 /home/vagrant/65Contrib/vendor/laravel/framework/src/Illuminate/Container/Container.php(502): call_user_func_array(Array, Array)\n26 /home/vagrant/65Contrib/vendor/laravel/framework/src/Illuminate/Console/Command.php(150): Illuminate\\Container\\Container->call(Array)\n27 /home/vagrant/65Contrib/vendor/symfony/console/Command/Command.php(259): Illuminate\\Console\\Command->execute(Object(Symfony\\Component\\Console\\Input\\ArgvInput), Object(Symfony\\Component\\Console\\Output\\ConsoleOutput))\n28 /home/vagrant/65Contrib/vendor/laravel/framework/src/Illuminate/Console/Command.php(136): Symfony\\Component\\Console\\Command\\Command->run(Object(Symfony\\Component\\Console\\Input\\ArgvInput), Object(Symfony\\Component\\Console\\Output\\ConsoleOutput))\n29 /home/vagrant/65Contrib/vendor/symfony/console/Application.php(878): Illuminate\\Console\\Command->run(Object(Symfony\\Component\\Console\\Input\\ArgvInput), Object(Symfony\\Component\\Console\\Output\\ConsoleOutput))\n30 /home/vagrant/65Contrib/vendor/symfony/console/Application.php(195): Symfony\\Component\\Console\\Application->doRunCommand(Object(Illuminate\\Queue\\Console\\WorkCommand), Object(Symfony\\Component\\Console\\Input\\ArgvInput), Object(Symfony\\Component\\Console\\Output\\ConsoleOutput))\n31 /home/vagrant/65Contrib/vendor/symfony/console/Application.php(126): Symfony\\Component\\Console\\Application->doRun(Object(Symfony\\Component\\Console\\Input\\ArgvInput), Object(Symfony\\Component\\Console\\Output\\ConsoleOutput))\n32 /home/vagrant/65Contrib/vendor/laravel/framework/src/Illuminate/Foundation/Console/Kernel.php(94): Symfony\\Component\\Console\\Application->run(Object(Symfony\\Component\\Console\\Input\\ArgvInput), Object(Symfony\\Component\\Console\\Output\\ConsoleOutput))\n33 /home/vagrant/65Contrib/artisan(36): Illuminate\\Foundation\\Console\\Kernel->handle(Object(Symfony\\Component\\Console\\Input\\ArgvInput), Object(Symfony\\Component\\Console\\Output\\ConsoleOutput))\n34 {main}\n^Z\n```\n. Apologies, this suddenly started working again! I suspect it wasn't this library, but something Laravel related as you suggested. Thanks! \n. No I don't believe so. I upgraded the aws php sdk and the instantiation of the signed CloudFront url factory changed and I wasn't sure where to find the CloudFront API version. I'll try the version you just suggested and report back, cheers!\n. That's kind of worked, but now I'm getting \n\\nFatal error: syntax error, unexpected T_YIELD in /home/forge/default/vendor/aws/aws-sdk-php/src/Credentials/InstanceProfileProvider.php on line 50\n. Sorry, I think that's HHVM related. Thanks for the help! \n. ",
    "adamlc": "I'm trying to do something similar too. I'm trying to get everything to run within a React event loop\n. @Briareos You're right, I've just come across that at the moment, very disappointing indeed :(\nI've also tried adding them to the guzzle promise queue and calling run on a periodic timer, as per Guzzle's docs, but that also seems to be blocking, which kind of defeats the whole purpose!\n. Fantastic! Thanks for explaining that @mtdowling :smiley: \n. @mtdowling just to let you know that WyriHaximus/react-guzzle-psr7 works prefectly with the SDK! Thanks @WyriHaximus :dancer: \n. ",
    "Briareos": "@adamlc That was my first attempt too, but I was disappointed when I noticed that all async command statuses are pending and you need to call the blocking function Promise::wait() for them to start processing.\n. Hey @mtdowling, thanks for the great explanation!\nThere is one unexpected behavior that I noticed in the first case - the promises are resolved only after a whole batch finishes. There is indeed an N number of promises in flight; but not requests. I'll try to illustrate  with Promise\\each_limit($promises, 3, $success, $fail) (with [==] representing request duration):\nPipe 1: [===]            $success() [==============] $success() [==...\nPipe 2: [=====]          $success() [======]         $success() [==...\nPipe 3: [==============] $success() [========]       $success() [==...\nWhat I expected to get is:\nPipe 1: [===] $success() [==============] $success() [==...\nPipe 2: [=====] $success() [======] $success() [==...\nPipe 3: [==============] $success() [========] $success() [==...\n. Unfortunately I don't have a clear way of isolating the problem. I noticed however different behavior in Guzzle 5 and 6. Guzzle 5 behaves as described above, and Guzzle 6 works as expected (it doesn't block). Still, in both it's kinda random regarding how many parallel tasks it's processing at the same time; ie. if I say 2 parallel tasks, it will sometimes process 2 and other times 3 (it's always +1).\nWhat I did to discover the problem is run my script, which is pretty much your example no.1:\ngit clone https://github.com/Briareos/aws-test.git\ncd aws-test\ncomposer install\nphp test.php\nAnd in another terminal to observe the downloading files:\nwatch -n.1 'ls -l aws-test/out'\nIt will download files in parallel, but differently in Guzzle 5 and Guzzle 6. I will investigate it more next week.\n. Hey @mtdowling, just wanted to let you know that https://github.com/guzzle/promises/pull/5 fixes the issue regarding the number of tasks that are processed in parallel, that I mentioned in the post above.\nThe issue that persists is only in Guzzle 5, that still behaves like I described in https://github.com/aws/aws-sdk-php/issues/621#issuecomment-111430093\nI won't be investigating the issue any more because I've decided to migrate to Guzzle 6.\n. ",
    "StevenBullen": "@jeremeamia \nAny update on this? :)\n. Thanks\n. Great news and will do. Thanks again.\n. ",
    "atkinsonm": "Glad I stumbled upon this post. I had been scouring the Internet trying to find out why I couldn't load AWS\\Sns\\Message for close to 2 hours. If it is no longer included in the SDK AWS really needs to update the documentation.\n. @jeremeamia right about here: http://docs.aws.amazon.com/aws-sdk-php/v3/api/namespace-Aws.Sns.html. I found it a little misleading as if it was still part of the SDK\n. Absolutely. I'm up and running by using the aws/aws-php-sns-message-validator repo. I appreciate the help!\n. @jeskew Ah I see. I did the composer install. Yeah, the suggestion would've helped.\n. ",
    "PhiloNL": "Thanks! I may have found the issue. I've been looking at the server log and there was an error which was not showing up in the Laravel log. The error stated that curl_multi_exec is disabled. I've contacted the system administrator to enable this function. I'll let you know if that fixes the issue. I'm using the default config, so not sure why it's using Guzzle 5.\n. Enabling curl_multi_exec resolved the issue!\nI'm using some other packages that seem to leverage Guzzle as well. Is it possible that one of these packages is using Guzzle 5 and the SDK is inheriting this?\n\"laravel/framework\": \"5.1.*\",\n        \"philo/laravel-translate\": \"2.*\",\n        \"jelovac/bitly4laravel\": \"3.*\",\n        \"itbz/fpdi\": \"dev-master\",\n        \"spatie/laravel-backup\": \"~2.1\",\n        \"league/flysystem-dropbox\": \"~1.0\",\n        \"vinkla/hashids\": \"~1.1\",\n        \"googleads/googleads-php-lib\": \"~5.8\",\n        \"maatwebsite/excel\": \"~2.0.0\",\n        \"bugsnag/bugsnag-laravel\": \"1.*\",\n        \"jwage/purl\": \"0.0.6.*\",\n        \"ilya/fuzzy\": \"~1\",\n        \"dimsav/laravel-translatable\": \"~5.1\",\n        \"laravel/socialite\": \"~2.0\",\n        \"thujohn/twitter\": \"~2.0\",\n        \"sammyk/laravel-facebook-sdk\": \"~2.0@dev\",\n        \"facebook/php-sdk-v4\": \"~5.0@dev\",\n        \"league/flysystem-aws-s3-v3\": \"~1.0\",\n        \"intervention/image\": \"~2.2\"\n. I'll contact them. Thanks! :+1: \n. ",
    "Gator92": "@jeskew The goal here is to send the same url to the canned policy that is sent to the signing. So what you're saying is if the original url has a query string, that would have to be taken into consideration as well? The pull I made does not consider this, but is a step above sending the raw url to the canned policy creation. Will adjust and squash the pull so that the same url that is used for sigining is also used for the canned policy. Not sure if there are unit tests here but will append those as necessary.\n. ",
    "iblessedi": "No, the exception is not on the certain time. Also it is not because of any cron jobs. I have one php CLI script with receives data from the SQS and does some actions with the messages. So, I am not sure why I see that error. \n. We use m1.small instance type. Load on the server is not high. However we have Jenkins installed there,so it may be that Jenkins CI causes much load on server (not sure). We do not see such errors on any other parts of our project. So, how can we fix this issue?\n. Yes, it helped. However not I get the following error:\nPHP Fatal error:  Allowed memory size of 134217728 bytes exhausted (tried to allocate 1048577 bytes) in /var/app/current/php/vendor/guzzlehttp/psr7/src/Stream.php on line 211\nPHP Fatal Error 'yii\\base\\ErrorException' with message 'Allowed memory size of 134217728 bytes exhausted (tried to allocate 1048577 bytes)'\nSo, seams like that's smth wrong inside of the sdk?\n. I have a CLI script which receives messages from the SQS and does some actions with that. However right now the queue is empty and an error raises with empty queue. I do smth like this:\nwhile(true){\n            if(self::$_quitSignal){\n                exit;\n            }\n            $client = new \\Aws\\Sqs\\SqsClient([\n                'version'  => Yii::$app->params['aws']['sdkVersion'],\n                'region'  => Yii::$app->params['aws']['defaultRegion'],\n                'credentials' => [\n                    'key' => Yii::$app->params['aws']['accessKey'],\n                    'secret' => Yii::$app->params['aws']['accessSecret'],\n                ],\n            ]);\n            $result = $client->receiveMessage(array(\n                'QueueUrl' => Yii::$app->params['aws']['sqs']['postQueue']\n            )); \n            if($result->getPath('Messages')) {\n                foreach ($result->getPath('Messages') as $message) {\n                    try {\n                        $body = \\yii\\helpers\\Json::decode($message['Body']);\n                        Handler::handle($body['url'], $body['data'], $body['headers'], $body['ip']);\n                        $client->deleteMessage(array(\n                            'QueueUrl' => Yii::$app->params['aws']['sqs']['postQueue'],\n                            'ReceiptHandle' => $message['ReceiptHandle'],\n                        ));\n                    } catch (app\\handlers\\HandlerException $e){\n                        $client->deleteMessage(array(\n                            'QueueUrl' => Yii::$app->params['aws']['sqs']['postQueue'],\n                            'ReceiptHandle' => $message['ReceiptHandle'],\n                        ));\n                        Yii::error($e->getMessage() . PHP_EOL . $e->getTraceAsString(), 'processor');\n                    }\n                }\n            } else {\n                sleep(1);\n            }\n        }\n    } catch (\\Exception $e){\n        Yii::error($e->getMessage() . PHP_EOL . $e->getTraceAsString(), 'processor');\n        $this->start();\n    }\n. Definetely! I will try to create client before the loop and return here. Thanks!\n. Yes, that fixed the problem. Thank you!\n. ",
    "UnbrandedTech": "This is the page I was reading\nhttp://docs.aws.amazon.com/aws-sdk-php/guide/latest/service-cloudsearchdomain.html\n. Is there a irc channel or a gitter that I can ask questions? There are a lot of really confusing things in the Documentation. Like \nCloudSearchDomainClient->search(\n    'highlight' => '<string>' //this is a string\n)\nbut in the api documentation \nhttp://docs.aws.amazon.com/cloudsearch/latest/developerguide/highlighting.html\nIt's in a dot notation:\nhighlight.field={\"max_phrases\":<int>,\"format\":<text / html>,\"pre_tag\":\"*<string>*\",\"post_tag\":\"*%*\"}\nHow would you do this in the SDK?\n. BTW, @jeremeamia you were right. It works, now I have different Issues.\n. ",
    "allen-infante": "Hi, when migrated same  issue.\nhttp://docs.aws.amazon.com/aws-sdk-php/v3/api/api-sqs-2012-11-05.html#deletemessagebatch\n```\n$result = $client->deleteMessageBatch([\n'**Entries**' => [ // REQUIRED\n    [\n        'Id' => '<string>', // REQUIRED\n        'ReceiptHandle' => '<string>', // REQUIRED\n    ],\n    // ...\n],\n'QueueUrl' => '<string>', // REQUIRED\n\n]);\n```\nwas working with previous version and with these parameters, now throw error :\nError executing \"DeleteMessageBatch\" on \"https://XXXXX\";\nAWS HTTP error: Client error: `POST https://XXXXX` resulted in a `400 Bad Request` response:\n<?xml version=\"1.0\"?><ErrorResponse xmlns=\"http://queue.amazonaws.com/doc/2012-11-05/\">\n<Error><Type>Sender</Type><Code>MalformedInput</Code><Message>Top level element may not be treated as a list</Message><Detail/></Error><RequestId>68e958b1-c421-5ae2-a345-17c09b1ecd0a</RequestId></ErrorResponse>\nif debug url generated is like this : \n&Action=DeleteMessageBatch\n&Version=2012-11-05\n&**Entries**.1.Id=XXX\n&**Entries**.1.ReceiptHandle=ZZZ\nhttp://docs.aws.amazon.com/en_us/AWSSimpleQueueService/latest/APIReference/API_DeleteMessageBatch.html\nbut sample display in this way :\n&Action=DeleteMessageBatch\n&Version=2012-11-05\n&**DeleteMessageBatchRequestEntry**.1.Id=XXX\n&**DeleteMessageBatchRequestEntry**.1.ReceiptHandle=ZZZ\ndoes this matter?\nRegards\n. ",
    "docteurklein": "still have some weird errors about batch delete:\n``\nPHP Fatal error:  Uncaught exception 'Aws\\Sqs\\Exception\\SqsException' with message 'Error executing \"DeleteMessageBatch\" on \"[REDACTED]\"; AWS HTTP error: Client error:POST [REDACTED]resulted in a400 Bad Request` response:\nFatal error: Uncaught exception 'Aws\\Sqs\\Exception\\SqsException' with message 'Error executing \"DeleteMessageBatch\" on \"[REDACTED]\"; AWS HTTP error: Client error: POST [REDACTED] resulted in a 400 Bad Request response:\n<?xml version=\"1.0\"?>SenderM (truncated...)\n MissingParameter (client): The request must contain the parameter DeleteMessageBatchRequestEntry.1.Id. - <?xml version=\"1.0\"?>SenderMissingParameterThe request must contain the parameter DeleteMessageBatchRequestEntry.1.Id.f74f6bec-537c-5ae5-87df-3f9b26a68c7f'\nGuzzleHttp\\Exception\\ClientException: Client error: POST [REDACTED] resulted in a 400 Bad Request response:\n<?xml version=\"1.0\"?>SenderM (truncated...)\n in /usr/src/app/vendor/guzzlehttp/guzzle/src/Exception/RequestException.php:113\nStack trace:\n0 /usr/src/app/vendor/guzzlehttp/guzzle/src/Middleware.php(66): GuzzleHttp\\Exception\\RequestException::create(Object(GuzzleHttp\\Psr7\\Request), Object(GuzzleHttp\\Psr7\\Response))\n1 /usr/src/app/vendor/guzzlehttp/promises/src/Promise.php(203): GuzzleHttp\\Middleware::GuzzleHttp{closure}(Object(GuzzleHttp\\Psr7\\Response))\n2 /usr/src/app/vendor/guzzlehttp/promises/src/Promise.php(156): GuzzleHttp\\Promise\\Promise::callHandler(1, Object(GuzzleHttp\\Psr7\\Response), Array)\n3 /usr/src/app/vendor/guzzlehttp/promises/src/TaskQueue.php(47): GuzzleHttp\\Promise\\Promise::GuzzleHttp\\Promise{closure}()\n4 /usr/src/app/vendor/guzzlehttp/guzzle/src/Handler/CurlMultiHandler.php(98): GuzzleHttp\\Promise\\TaskQueue->run()\n5 /usr/src/app/vendor/guzzlehttp/guzzle/src/Handler/CurlMultiHandler.php(125): GuzzleHttp\\Handler\\CurlMultiHandler->tick()\n6 /usr/src/app/vendor/guzzlehttp/promises/src/Promise.php(246): GuzzleHttp\\Handler\\CurlMultiHandler->execute(true)\n7 /usr/src/app/vendor/guzzlehttp/promises/src/Promise.php(223): GuzzleHttp\\Promise\\Promise->invokeWaitFn()\n8 /usr/src/app/vendor/guzzlehttp/promises/src/Promise.php(267): GuzzleHttp\\Promise\\Promise->waitIfPending()\n9 /usr/src/app/vendor/guzzlehttp/promises/src/Promise.php(225): GuzzleHttp\\Promise\\Promise->invokeWaitList()\n10 /usr/src/app/vendor/guzzlehttp/promises/src/Promise.php(267): GuzzleHttp\\Promise\\Promise->waitIfPending()\n11 /usr/src/app/vendor/guzzlehttp/promises/src/Promise.php(225): GuzzleHttp\\Promise\\Promise->invokeWaitList()\n12 /usr/src/app/vendor/guzzlehttp/promises/src/Promise.php(62): GuzzleHttp\\Promise\\Promise->waitIfPending()\n13 /usr/src/app/vendor/aws/aws-sdk-php/src/AwsClientTrait.php(58): GuzzleHttp\\Promise\\Promise->wait()\n14 /usr/src/app/vendor/aws/aws-sdk-php/src/AwsClientTrait.php(77): Aws\\AwsClient->execute(Object(Aws\\Command))\n15 /usr/src/app/src/PimAI/SqsWorker/App.php(71): Aws\\AwsClient->__call('deleteMessageBa...', Array)\n16 /usr/src/app/bin/sqs-to-gps.php(33): PimAI\\SqsWorker\\App->__invoke()\n17 {main}\nNext Aws\\Sqs\\Exception\\SqsException: Error executing \"DeleteMessageBatch\" on \"[REDACTED]\"; AWS HTTP error: Client error: POST [REDACTED] resulted in a 400 Bad Request response:\n<?xml version=\"1.0\"?>SenderM (truncated...)\n MissingParameter (client): The request must contain the parameter DeleteMessageBatchRequestEntry.1.Id. - <?xml version=\"1.0\"?>SenderMissingParameterThe request must contain the parameter DeleteMessageBatchRequestEntry.1.Id.f74f6bec-537c-5ae5-87df-3f9b26a68c7f in /usr/src/app/vendor/aws/aws-sdk-php/src/WrappedHttpHandler.php:191\nStack trace:\n0 /usr/src/app/vendor/aws/aws-sdk-php/src/WrappedHttpHandler.php(100): Aws\\WrappedHttpHandler->parseError(Array, Object(GuzzleHttp\\Psr7\\Request), Object(Aws\\Command), Array)\n1 /usr/src/app/vendor/guzzlehttp/promises/src/Promise.php(203): Aws\\WrappedHttpHandler->Aws{closure}(Array)\n2 /usr/src/app/vendor/guzzlehttp/promises/src/Promise.php(174): GuzzleHttp\\Promise\\Promise::callHandler(2, Array, Array)\n<?xml version=\"1.0\"?>SenderM (truncated...)\n MissingParameter (client): The request must contain the parameter DeleteMessageBatchRequestEntry.1.Id. - <?xml version=\"1.0\"?>SenderMissingParameterThe request must contain the parameter DeleteMessageBatchRequestEntry.1.Id.f74f6bec-537c-5ae5-87df-3f9b26a68c7f'\nGuzzleHttp\\Exception\\ClientException: Client error: POST [REDACTED] resulted in a 400 Bad Request response:\n<?xml version=\"1.0\"?>SenderM (truncated...)\n in /usr/src/app/vendor/guzzlehttp/guzzle/src/Exception/RequestException.php:113\nStack trace:\n0 /usr/src/app/vendor/guzzlehttp/guzzle/src/Middleware.php(66): GuzzleHttp\\Exception\\RequestException::create(Object(GuzzleHttp\\Psr7\\Request), Object(GuzzleHttp\\Psr7\\Response))\n1 /usr/src/app/vendor/guzzlehttp/promises/src/Promise.php(203): GuzzleHttp\\Middleware::GuzzleHttp{closure}(Object(GuzzleHttp\\Psr7\\Response))\n2 /usr/src/app/vendor/guzzlehttp/promises/src/Promise.php(156): GuzzleHttp\\Promise\\Promise::callHandler(1, Object(GuzzleHttp\\Psr7\\Response), Array)\n3 /usr/src/app/vendor/guzzlehttp/promises/src/TaskQueue.php(47): GuzzleHttp\\Promise\\Promise::GuzzleHttp\\Promise{closure}()\n4 /usr/src/app/vendor/guzzlehttp/guzzle/src/Handler/CurlMultiHandler.php(98): GuzzleHttp\\Promise\\TaskQueue->run()\n5 /usr/src/app/vendor/guzzlehttp/guzzle/src/Handler/CurlMultiHandler.php(125): GuzzleHttp\\Handler\\CurlMultiHandler->tick()\n6 /usr/src/app/vendor/guzzlehttp/promises/src/Promise.php(246): GuzzleHttp\\Handler\\CurlMultiHandler->execute(true)\n7 /usr/src/app/vendor/guzzlehttp/promises/src/Promise.php(223): GuzzleHttp\\Promise\\Promise->invokeWaitFn()\n8 /usr/src/app/vendor/guzzlehttp/promises/src/Promise.php(267): GuzzleHttp\\Promise\\Promise->waitIfPending()\n9 /usr/src/app/vendor/guzzlehttp/promises/src/Promise.php(225): GuzzleHttp\\Promise\\Promise->invokeWaitList()\n10 /usr/src/app/vendor/guzzlehttp/promises/src/Promise.php(267): GuzzleHttp\\Promise\\Promise->waitIfPending()\n11 /usr/src/app/vendor/guzzlehttp/promises/src/Promise.php(225): GuzzleHttp\\Promise\\Promise->invokeWaitList()\n12 /usr/src/app/vendor/guzzlehttp/promises/src/Promise.php(62): GuzzleHttp\\Promise\\Promise->waitIfPending()\n13 /usr/src/app/vendor/aws/aws-sdk-php/src/AwsClientTrait.php(58): GuzzleHttp\\Promise\\Promise->wait()\n14 /usr/src/app/vendor/aws/aws-sdk-php/src/AwsClientTrait.php(77): Aws\\AwsClient->execute(Object(Aws\\Command))\n15 /usr/src/app/src/PimAI/SqsWorker/App.php(71): Aws\\AwsClient->__call('deleteMessageBa...', Array)\n16 /usr/src/app/bin/sqs-to-gps.php(33): PimAI\\SqsWorker\\App->__invoke()\n17 {main}\nNext Aws\\Sqs\\Exception\\SqsException: Error executing \"DeleteMessageBatch\" on \"[REDACTED]\"; AWS HTTP error: Client error: POST [REDACTED] resulted in a 400 Bad Request response:\n<?xml version=\"1.0\"?>SenderM (truncated...)\n MissingParameter (client): The request must contain the parameter DeleteMessageBatchRequestEntry.1.Id. - <?xml version=\"1.0\"?>SenderMissingParameterThe request must contain the parameter DeleteMessageBatchRequestEntry.1.Id.f74f6bec-537c-5ae5-87df-3f9b26a68c7f in /usr/src/app/vendor/aws/aws-sdk-php/src/WrappedHttpHandler.php:191\nStack trace:\n0 /usr/src/app/vendor/aws/aws-sdk-php/src/WrappedHttpHandler.php(100): Aws\\WrappedHttpHandler->parseError(Array, Object(GuzzleHttp\\Psr7\\Request), Object(Aws\\Command), Array)\n1 /usr/src/app/vendor/guzzlehttp/promises/src/Promise.php(203): Aws\\WrappedHttpHandler->Aws{closure}(Array)\n2 /usr/src/app/vendor/guzzlehttp/promises/src/Promise.php(174): GuzzleHttp\\Promise\\Promise::callHandler(2, Array, Array)\n3 /usr/src/app/vendor/guzzlehttp/promises/src/RejectedPromise.php(40): GuzzleHttp\\Promise\\Promise::GuzzleHttp\\Promise{closure}(Array)\n3 /usr/src/app/vendor/guzzlehttp/promises/src/RejectedPromise.php(40): GuzzleHttp\\Promise\\Promise::GuzzleHttp\\Promise{closure}(Array)\n4 /usr/src/app/vendor/guzzlehttp/promises/src/TaskQueue.php(47): GuzzleHttp\\Promise\\RejectedPromise::GuzzleHttp\\Promise{closure}()\n5 /usr/src/app/vendor/guzzlehttp/guzzle/src/Handler/CurlMultiHandler.php(98): GuzzleHttp\\Promise\\TaskQueue->run()\n6 /usr/src/app/vendor/guzzlehttp/guzzle/src/Handler/CurlMultiHandler.php(125): GuzzleHttp\\Handler\\CurlMultiHandler->tick()\n7 /usr/src/app/vendor/guzzlehttp/promises/src/Promise.php(246): GuzzleHttp\\Handler\\CurlMultiHandler->execute(true)\n8 /usr/src/app/vendor/guzzlehttp/promises/src/Promise.php(223): GuzzleHttp\\Promise\\Promise->invokeWaitFn()\n9 /usr/src/app/vendor/guzzlehttp/promises/src/Promise.php(267): GuzzleHttp\\Promise\\Promise->waitIfPending()\n10 /usr/src/app/vendor/guzzlehttp/promises/src/Promise.php(225): GuzzleHttp\\Promise\\Promise->invokeWaitList()\n4 /usr/src/app/vendor/guzzlehttp/promises/src/TaskQueue.php(47): GuzzleHttp\\Promise\\RejectedPromise::GuzzleHttp\\Promise{closure}()\n11 /usr/src/app/vendor/guzzlehttp/promises/src/Promise.php(267): GuzzleHttp\\Promise\\Promise->waitIfPending()\n12 /usr/src/app/vendor/guzzlehttp/promises/src/Promise.php(225): GuzzleHttp\\Promise\\Promise->invokeWaitList()\n5 /usr/src/app/vendor/guzzlehttp/guzzle/src/Handler/CurlMultiHandler.php(98): GuzzleHttp\\Promise\\TaskQueue->run()\n13 /usr/src/app/vendor/guzzlehttp/promises/src/Promise.php(62): GuzzleHttp\\Promise\\Promise->waitIfPending()\n6 /usr/src/app/vendor/guzzlehttp/guzzle/src/Handler/CurlMultiHandler.php(125): GuzzleHttp\\Handler\\CurlMultiHandler->tick()\n14 /usr/src/app/vendor/aws/aws-sdk-php/src/AwsClientTrait.php(58): GuzzleHttp\\Promise\\Promise->wait()\n7 /usr/src/app/vendor/guzzlehttp/promises/src/Promise.php(246): GuzzleHttp\\Handler\\CurlMultiHandler->execute(true)\n8 /usr/src/app/vendor/guzzlehttp/promises/src/Promise.php(223): GuzzleHttp\\Promise\\Promise->invokeWaitFn()\n9 /usr/src/app/vendor/guzzlehttp/promises/src/Promise.php(267): GuzzleHttp\\Promise\\Promise->waitIfPending()\n10 /usr/src/app/vendor/guzzlehttp/promises/src/Promise.php(225): GuzzleHttp\\Promise\\Promise->invokeWaitList()\n11 /usr/src/app/vendor/guzzlehttp/promises/src/Promise.php(267): GuzzleHttp\\Promise\\Promise->waitIfPending()\n12 /usr/src/app/vendor/guzzlehttp/promises/src/Promise.php(225): GuzzleHttp\\Promise\\Promise->invokeWaitList()\n13 /usr/src/app/vendor/guzzlehttp/promises/src/Promise.php(62): GuzzleHttp\\Promise\\Promise->waitIfPending()\n14 /usr/src/app/vendor/aws/aws-sdk-php/src/AwsClientTrait.php(58): GuzzleHttp\\Promise\\Promise->wait()\n15 /usr/src/app/vendor/aws/aws-sdk-php/src/AwsClientTrait.php(77): Aws\\AwsClient->execute(Object(Aws\\Command))\n16 /usr/src/app/src/PimAI/SqsWorker/App.php(71): Aws\\AwsClient->__call('deleteMessageBa...', Array)\n17 /usr/src/app/bin/sqs-to-gps.php(33): PimAI\\SqsWorker\\App->__invoke()\n18 {main}\nthrown in /usr/src/app/vendor/aws/aws-sdk-php/src/WrappedHttpHandler.php on line 191\n15 /usr/src/app/vendor/aws/aws-sdk-php/src/AwsClientTrait.php(77): Aws\\AwsClient->execute(Object(Aws\\Command))\n16 /usr/src/app/src/PimAI/SqsWorker/App.php(71): Aws\\AwsClient->__call('deleteMessageBa...', Array)\n17 /usr/src/app/bin/sqs-to-gps.php(33): PimAI\\SqsWorker\\App->__invoke()\n18 {main}\nthrown in /usr/src/app/vendor/aws/aws-sdk-php/src/WrappedHttpHandler.php on line 191\n```\ninstalled version: \"aws/aws-sdk-php\": \"^3.54\", via composer.\nAny idea? In the meantime I'm looping over the messages and delete them one by one.. ",
    "dstevenson": "@jeremeamia @mtdowling I am testing out the changes released for this bugfix in 3.0.7 and the protocol is not parsed when the streamwrapper executes a rename. It defaults to s3 which will not exist in the default stream context.\nSince we're working with 2 paths here I could see how that can be a little tricky since in theory the paths could have different protocols. Maybe it would be best to enforce that the $path_from and $path_to have the same protocol?\n. @jeremeamia I attempted to set the ContentSHA256 but it's not part of the StructureShape definition for the PutObjectRequest so it's dropped by the time the RequestInterface makes it way to the signature step.\nI think a potential workaround might be to use a middleware that prepends the sign step and adds the header, trying that out now\n. Anyone having this issue can use this workaround:\n$hash = 'CALCULATED';\n/** @var $client \\Aws\\S3\\S3Client */\n$client->getHandlerList()->prependSign(\n    \\Aws\\Middleware::mapRequest(function(\\Psr\\Http\\Message\\RequestInterface $req) use ($hash) {\n        return $req->withHeader('x-amz-content-sha256', $hash);\n    }),\n    'add-x-amz-content-sha256'\n);\n. I think you're looking for the S3Client configuration parameter bucket_endpoint: true\n. I see, so you still end up with host: s3.amazonaws.com instead of host: bucket.s3.amazonaws.com. Not sure if that's a bug or not, but I think you can manually pass in to the S3Client endpoint: https://bucket.s3.amazonaws.com/ and get the behavior you expect\n. @jpinner-lyft Maybe you can use your own custom Middleware as a workaround for the functionality you're looking for?\n. I think @jpinner-lyft is on to something here.\nSuppose you create some client:\n$client = new S3Client([\n    \"credentials\" => [\n        \"key\"    => ...,\n        \"secret\" => ...,\n    ],\n    \"region\"   => ...,\n    \"version\"  => ...,\n    \"bucket_endpoint\" => true\n]);\nUsing this client to generate a presignedRequest for a GetObject appears to yield the incorrect result:\n```\n$cmd = $client->getCommand('GetObject', [\n  'Bucket' => 'bucket',\n  'Key'    => 'key'\n]);\n$request = $client->createPresignedRequest($cmd, \"+30 minutes\");\n// yields https://s3.amazonaws.com/key?{signature stuff here...}\n(string) $request->getUri();\n// Host is s3.amazonaws.com\n$request->getHeader(\"host\")\n```\nWhen it should be giving us something like\n```\n// https://bucket.s3.amazonaws.com/key?{signature stuff here...}\n(string) $request->getUri();\n// Host is bucket.s3.amazonaws.com\n$request->getHeader(\"host\")\n```\n. Just wanted to post here since this might help anyone else who may be wondering why they have an issue here.\nIf you utilize SplFileObject to handle your S3 objects via the StreamWrapper then you will get a RuntimeException when using the PSR7 library that triggers the E_USER_DEPRECATED warning. \nSplFileObject internally replaces the error handler during its constructor and thus will throw a RuntimeException on the trigger_error.\nI reported this behavior in the issue jeskew opened above. This isn't an issue with AWS library nor the PSR7 library, but since it's something we can't control in the PHP source, hopefully this will help convince them to not use trigger error in this way.. @jeskew thanks for your help in the battle getting this pushed through. @jeremeamia The S3SignatureV4 will always try to set the x-amz-content-sha256 header to the hash of the request body or POST fields on a PutObject. \n. Ah, sorry, didn't see this test was appending the build step not the sign step\n. On the fence about this unit test since it's technically only unit testing native PHP functionality that's outside the scope of the StreamWrapper. Thoughts?\n. This should be safe because PHP natively ensures a rename is issued on the same wrapper.\n. Actually, I'm beginning to like this now since if we ever forget, or behavior in PHP changes, we'll probably need to modify rename accordingly.\n. Done.\n. ",
    "titolendable": "I faced this problem today, I adjusted the time on my machine and now everything works. Is there a way to disable this check?\n. ",
    "BruceWouaigne": "Files are around 2MB. I also tried using a multipart upload, same thing. And no proxies at all.\nAgain, with a previous version of the aws php wrapper (2.4) everything was fine.\n. ",
    "josef-diazlop": "Hey guys, i have the same problem, i'm on 3.2.6 version, it happens 40% of times.\nDo you have any workaround?\n. I uploaded to s3 a bulk collection around 60 files, some are about 50 MB, 5 or 6, but most are about 650 KB, all compressed in zip format. I get this error in 40% of the uploads. I try to reproduce it but it seems to be totally random. This happens in previous version as in actual.\nToday I'll try to gather more information. \nI also update to 3.18.15, now everything seems fine. If I find something I'll let you know.\n thanks.\n. @cjyclaire \nI uploaded to production and the problem persists. On ubuntu 14.04 and ubuntu 16.04 (our development environment ) goes fine, but in debian jessie we have this problems.\nAll the environments are updated.\nwe have this additional info in the error:\nUpload of dump contents to remote directory failed with message An exception occurred while uploading parts to a multipart upload. The following parts had errors:\n- Part 7: Error executing \"UploadPart\" on \"https://{domain}/Attachment.chunks.bson.gz.04?partNumber= [status code] 500 [reason phrase] Internal Server Error InternalError (server): We encountered an internal error. Please try again. - <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n  InternalErrorWe encountered an internal error. Please try again.{requestid}{hostid}\nThanks for the help.\n. ",
    "Hakadel": "Hello @jeskew,\nOk i see, I just set ENV variables and it's working now, but it's strange because my config was on the code.\nAWS_ACCESS_KEY_ID, AWS_BUCKET, AWS_REGION and AWS_SECRET_ACCESS_KEY\nSo with env vars all is working.\nThank you ;)\n. ",
    "mpoiriert": "You should delete the tag or replace but don't leave it there because it could break a lot of project base on the fact a tag is suppose to be stable...\n. ",
    "ragboyjr": "@jeskew, No stack trace, just a fatal error: Fatal error: Class 'Aws\\Common\\Credentials\\Credentials' not found in /data/gitsites/tige/mywake-cms/master/vendor/tige-mywake/lib/app/config/s3.php on line 7. It looks like you've already released 2.8.12, and that looks like it'll be compatible\nThanks for the quick responses!\n. Yup, works great, thanks!\n. ",
    "BardiaAfshin": "Tested on 3.0 and 2.8\nSame problem\n\u2014\nSent from Mailbox\nOn Wed, Jun 24, 2015 at 3:12 PM, Jeremy Lindblom notifications@github.com\nwrote:\n\nWhat version of the SDK are you using?\nReply to this email directly or view it on GitHub:\nhttps://github.com/aws/aws-sdk-php/issues/650#issuecomment-115025933\n. here's a copy of my composer.json (originally I was using the aws-sdk-php version 3.0)\n\njson\n{\n    \"name\": \"bafshin/php-kinesis\",\n    \"require\": {\n        \"aws/aws-sdk-php\": \"^2.0\"\n    },\n    \"authors\": [\n        {\n            \"name\": \"me\",\n            \"email\": \"me@me.com\"\n        }\n    ],\n    \"prefer-stable\": true\n}\ncomposer update (just ran it)\n``` json\ncomposer update\nLoading composer repositories with package information\nUpdating dependencies (including require-dev)\n  - Removing guzzlehttp/guzzle (6.0.1)\n  - Removing guzzlehttp/psr7 (1.1.0)\n  - Removing psr/http-message (1.0)\n  - Removing guzzlehttp/promises (1.0.1)\n  - Removing mtdowling/jmespath.php (2.2.0)\n  - Installing symfony/event-dispatcher (v2.7.1)\n    Loading from cache\n\n\nInstalling guzzle/guzzle (v3.9.3)\n    Loading from cache\n\n\nRemoving aws/aws-sdk-php (2.8.11)\n\nInstalling aws/aws-sdk-php (2.8.12)\n    Downloading: 100%\n\nsymfony/event-dispatcher suggests installing symfony/dependency-injection ()\nsymfony/event-dispatcher suggests installing symfony/http-kernel ()\nguzzle/guzzle suggests installing guzzlehttp/guzzle (Guzzle 5 has moved to a new package name. The package you have installed, Guzzle 3, is deprecated.)\nWriting lock file\nGenerating autoload files\n```\nBUT WAIT, I just re ran my little snippet of code, and now I'm getting Resource not found exception\nlog\nFatal error: Uncaught Aws\\Kinesis\\Exception\\ResourceNotFoundException: AWS Error Code: ResourceNotFoundException, Status Code: 400, AWS Request ID: 4ba4e619-1ac7-11e5-b5ab-5dc35cf881ac, AWS Error Type: client, AWS Error Message: Stream string under account {REDACTED} not found., User-Agent: aws-sdk-php2/2.8.11 Guzzle/3.9.3 curl/7.37.1 PHP/5.5.20\n. Any discoveries on this? The exception is rather vague, and the place it falls in the code, it's not immediately apparent on what the problem is.\n. just re-pinging this issue. @jeremeamia \n. Is this latest changes merged to stable branch? I'm still getting this excpetion\nhere's a snippet of my compser.json\njson\n{\n    \"name\": \"bardionios/php-kinesis\",\n    \"require\": {\n        \"aws/aws-sdk-php\": \"^3.0\"\n    },\n    \"authors\": [\n        {\n            \"name\": \"bardionios\",\n            \"email\": \"b@example.com\"\n        }\n    ],\n    \"prefer-stable\": true,\n    \"minimum-stability\": \"stable\"\n}\n. tested, works.\n. Definately.\n. ",
    "ArthurGuy": "Crap, now to figure out how to run two versions :cry: \nThanks\n. Ohh, could the removal be added to the release notes? It might save someone else from having to search through the various updates.\n. I found a very old single file library for the sqs service, it has a few bugs but is perfect for now.\nThanks for your quick response with this.\n. I have updated to use the master branch but this hasn't made any changes, I am still getting the same error as before.. The sdk went from 3.19.28 to 3.19.31 and the guzzle promise library went from 1.2.0 to 1.3.0, this was the update for us that caused the failure.\nWe are running on Ubuntu 16.04 LTS and php 7.0.8.\nThe library is being pulled in as part of the laravel framework but for the previous test I did explicitly pull in the master branch of the sdk.. I have placed user credentials onto the server which has gotten around this, if a solution isn't readily available I will switch to the instanceProfile method.. instanceProfile works, its just that combination in the initial message that's behaving incorrectly.\nI have tried a few combinations and its definitly the guzzlehttp/promises library, if I pull it back to 1.2.0 everything works but if I leave it to go to 1.3.0 everything fails.. @cjyclaire stupid question but you were trying to get credentials from a role and there was nothing else in place to take precedence?. It looks like it's nearly been solved.\nhttps://github.com/guzzle/promises/pull/55. ",
    "kamichidu": "I'm using DynamoDB, not a local.\nI know how large of the response. It is lower than 100kb.\nI tried to reproduce it under my mac osx yosemite or ubuntu, but I couldn't reproduce it.\nMy production env. is under win server 2012R2, I always get the strange exception.\n. Thanks a lot!\nThere are things I can help, I'll help for fixing.\n. ",
    "acoulton": "@ethet @jeremeamia I can reproduce this, could you repoen? The issue seems to be when you try (as the example in the docs) to upload to the root of an S3 bucket.\nThis appears to be because the wrapper attempts to upload all files with an empty Key (eg to PUT to the bucket root URL).\n``` php\n$client = new S3Client([\n  'region' => 'eu-west-1',\n  'version' => '2006-03-01',\n]);\n$client->uploadDirectory(\n    '/tmp/test-dir',\n    's3://bucket,\n    NULL, // Same error happens with '' and '/'\n    [\n        'before'      => function (CommandInterface $command) {\n            print_r($command->getName().PHP_EOL);\n            print_r($command->toArray());\n        }\n    ]\n);\n```\ngives: \nPutObject\nArray\n(\n    [Bucket] => bucket\n    [SourceFile] => /tmp/test-dir/some-file\n    [Key] =>\n    [@http] => Array\n        (\n        )\n)\nThere's some quite confusing handling of leading/trailing / characters within the Transfer class, including normalizePath which is documented as Normalize a path so that it has a trailing slash but in fact does the opposite.\nIt looks like currently:\n- the s3 bucket must have a trailing / otherwise getS3Args will not parse any base Key and so createS3Key will return an empty string for any filename. This is only the case now if there is a non-empty key prefix.\n- local directory paths must not have a trailing / otherwise both source file name and s3 key end up with a // like /tmp/test-dir//some-file and bucket//some-file - this results in the upload being successful but the files are all uploaded one-level down to an un-named directory.\n. Sorry, I pasted this into the github editor incorrectly. I don't have a local clone handy but will squash this if/when you confirm you're happy with it.\n. This method does the opposite of the previous doc-comment. Stripping trailing / does appear to be important, however, as filesystem paths with trailing / produce unexpected results elsewhere in the class.\n. It is not clear why this method should ever return an empty string - I can't find anything useful in the git history and there doesn't appear to be test coverage. Since this seems to be causing #653 I've assumed it's safe to remove this behaviour - I have not reviewed to see if this has unexpected results elsewhere.\n. ",
    "swayam01": "I am currently facing the same issue, Where this workaround should be stored. ",
    "ravenbyron": "Here is the output with the debug turned on \n```\nResponse:\nHTTP/1.1 200 OK\nContent-Type: application/json\nDate: Sat, 04 Jul 2015 01:09:37 GMT\nx-amzn-Remapped-Content-Length: 0\nx-amzn-RequestId: 56bad745-21e9-11e5-956e-f5e7a51b657d\nContent-Length: 18\nConnection: keep-alive\n\"{\\\"rank\\\":\\\"1\\\"}\"\nErrors: 0\n```\n. So this helped me realize that my lambda function was sending a json string to context.succeed instead of an object so it was getting double json'ed. I was able to fix my lambda code and this works now but not sure if this is still a bug.\n. Thank you I had a feeling it was along these lines, I enabled the stream wrapper with the instructions here http://docs.aws.amazon.com/aws-sdk-php/v3/guide/service/s3-stream-wrapper.html  Got the iterator working properly but every file  returns the below error after a successful download.\nPHP Fatal error:  Cannot resume an already running generator in .../vendor/guzzlehttp/promises/src/EachPromise.php on line 173\n\"name\": \"aws/aws-sdk-php\", \"version\": \"3.15.0\"\nPHP 5.6.17\n. That worked great I owe you guys a beer at re:Invent!!!\n. ok that sucks... is there any way to do what the console does it seems to call listResourceRecordSetWithPrefix?  or should I just add the \"StartRecordName\" and then do filtering in my code from there? My goal is to find a record so I can update it...\n. ",
    "mwoodring": "Jeremy,\nI have seen the documentation about recovering from errors before. The documentation does not mention that it will upload all parts then exit with an error. Also not mentioned is that creating a new uploader will only upload the parts with errors. Thank you for giving me that info.\n\nMark\nFrom: Jeremy Lindblom notifications@github.com<mailto:notifications@github.com>\nReply-To: aws/aws-sdk-php reply@reply.github.com<mailto:reply@reply.github.com>\nDate: Wednesday, July 01, 2015 at 10:41\nTo: aws/aws-sdk-php aws-sdk-php@noreply.github.com<mailto:aws-sdk-php@noreply.github.com>\nCc: Mark Woodring mark.woodring@walsworth.com<mailto:mark.woodring@walsworth.com>\nSubject: Re: [aws-sdk-php] Multipart Upload reporting errors at end of upload in sdk v3 (#669)\nEven if there are unsuccessful UploadPart operations somewhere in the middle of the multipart upload, it's best to let it all finish, because the UploadState tracks exactly which parts failed. When you get the state from the exception at the end, you can create a new MultipartUploader from the state that will only attempt to upload the missing parts. An example of this technique is demonstrated here: http://docs.aws.amazon.com/aws-sdk-php/v3/guide/service/s3-multipart-upload.html#recovering-from-errors\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/aws/aws-sdk-php/issues/669#issuecomment-117720038.\n. Jeremy,\nI changed my code around to use the method show in the recovering from errors section of the multipart upload documentation. I am now getting this error:\nPHP Fatal error:  Uncaught exception 'GuzzleHttp\\Promise\\RejectionException' with message 'The promise was rejected with reason: Invoking the wait callback did not resolve the promise' in /usr/scripts/vendor/guzzlehttp/promises/src/functions.php:111\nStack trace:\n0 /usr/scripts/vendor/guzzlehttp/promises/src/functions.php(495): GuzzleHttp\\Promise\\exception_for('Invoking the wa...')\n1 /usr/scripts/vendor/guzzlehttp/promises/src/Promise.php(199): GuzzleHttp\\Promise{closure}('Invoking the wa...')\n2 /usr/scripts/vendor/guzzlehttp/promises/src/Promise.php(152): GuzzleHttp\\Promise\\Promise::callHandler(2, 'Invoking the wa...', Array)\n3 /usr/scripts/vendor/guzzlehttp/promises/src/TaskQueue.php(60): GuzzleHttp\\Promise\\Promise::GuzzleHttp\\Promise{closure}()\n4 /usr/scripts/vendor/guzzlehttp/promises/src/Promise.php(228): GuzzleHttp\\Promise\\TaskQueue->run()\n5 /usr/scripts/vendor/guzzlehttp/promises/src/Promise.php(261): GuzzleHttp\\Promise\\Promise->waitIfPending()\n6 /usr/scripts/vendor/guzzlehttp/promises/src/Promise.php(219): Guzz in /usr/scripts/vendor/guzzlehttp/promises/src/functions.php on line 111\nAny ideas how to resolve this error?\nThanks,\nMark\nFrom: Jeremy Lindblom notifications@github.com<mailto:notifications@github.com>\nReply-To: aws/aws-sdk-php reply@reply.github.com<mailto:reply@reply.github.com>\nDate: Wednesday, July 01, 2015 at 11:52\nTo: aws/aws-sdk-php aws-sdk-php@noreply.github.com<mailto:aws-sdk-php@noreply.github.com>\nCc: Mark Woodring mark.woodring@walsworth.com<mailto:mark.woodring@walsworth.com>\nSubject: Re: [aws-sdk-php] Multipart Upload reporting errors at end of upload in sdk v3 (#669)\nI will add some clarifications to the documentation. Thanks for the feedback.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/aws/aws-sdk-php/issues/669#issuecomment-117748094.\n. The code is as follows:\n$s3Uploader = new MultipartUploader ($s3Client, $fileHndl, $opt);\ndo {\ntry {\n$response = $s3Uploader->upload ();\nif ($response->hasKey ('ObjectURL'))\n$awsURL = $response->get ('ObjectURL');\nelse\nvar_dump ($response); // ** testing only\n}\ncatch (MultipartUploadException $e) {\nshowError ($e, $currFileName, \"Multi upload exception\");\nfwrite (STDERR, \"        restarting upload on parts with errors\" . PHP_EOL);\n$s3Uploader = new MultipartUploader ($s3Client, $fileHndl, [\n'state' => $e->getState (),\n]);\n}\n}\nwhile (! isset ($response));\nThe showError function just prints out the errors from the exception.\nThanks,\nMark\nFrom: Jeremy Lindblom notifications@github.com<mailto:notifications@github.com>\nReply-To: aws/aws-sdk-php reply@reply.github.com<mailto:reply@reply.github.com>\nDate: Thursday, July 02, 2015 at 09:45\nTo: aws/aws-sdk-php aws-sdk-php@noreply.github.com<mailto:aws-sdk-php@noreply.github.com>\nCc: Mark Woodring mark.woodring@walsworth.com<mailto:mark.woodring@walsworth.com>\nSubject: Re: [aws-sdk-php] Multipart Upload reporting errors at end of upload in sdk v3 (#669)\nCan you show us what your code looks like now?\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/aws/aws-sdk-php/issues/669#issuecomment-118055475.\n. I tried the error recovery method on only one 6GB file. I tried it several times on that job and each time I received the fatal error.\n\nMark\nFrom: Jeremy Lindblom notifications@github.com<mailto:notifications@github.com>\nReply-To: aws/aws-sdk-php reply@reply.github.com<mailto:reply@reply.github.com>\nDate: Thursday, July 02, 2015 at 11:10\nTo: aws/aws-sdk-php aws-sdk-php@noreply.github.com<mailto:aws-sdk-php@noreply.github.com>\nCc: Mark Woodring mark.woodring@walsworth.com<mailto:mark.woodring@walsworth.com>\nSubject: Re: [aws-sdk-php] Multipart Upload reporting errors at end of upload in sdk v3 (#669)\nDoes the \"Invoking the wait callback did not resolve the promise\" error happen every time, or just some of the time?\nSide Note: I added some more documentation to the user guide for multipart uploads as per our earlier discussion. See 2df31b3https://github.com/aws/aws-sdk-php/commit/2df31b39e2c2cd818f190d05fb85d4f4ec5c5524\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/aws/aws-sdk-php/issues/669#issuecomment-118080549.\n. I changed the source parameter from a file resource handle (fopen) to a path. Now when the upload gets an error and goes through the catch function to restart the upload, it works as expected. Every time I used a file resource handle as the source and there were errors, it would crash with the message like I showed on July 1.\n. imshashank,\nThe docs that you reference are for a whole file upload. I am talking about a multi part upload. There is no stack trace because the file upload completes successfully. Using the management console, looking at the files in the bucket shows that the files are in the storage class of STANDARD and not STANDARD-IA which is what I'm trying to achieve on upload.. ",
    "zexz": "Hello, i'm not sure if it does belong here, but we have issues with multipart upload (just switched to aws v3.1.0), here is the error log:\n```\n[11-Jul-2015 05:40:37 America/Chicago] PHP Fatal error:  Uncaught exception 'Aws\\Exception\\MultipartUploadException' with message 'An exception occurred while uploading parts to a multipart upload. The following parts had errors:\n- Part 1: Error executing \"UploadPart\" on \"https://path.to.file.zip?partNumber=1&uploadId=gNqjjR_8MAQRvyI1fT64GnYb7yaR5IOqnSE.o61g1n1rr2fX4fTX4B4bqHbjXhIwZceDdd2x0sNQ.GAh.3LUhpJiAt1UY0HiWgBRim.X48ScEpzWLbJNil4flu6NHUAo\"; AWS HTTP error: Client error: 400 XAmzContentSHA256Mismatch (client): The provided 'x-amz-content-sha256' header does not match what was computed. - \nXAmzContentSHA256MismatchThe provided 'x-amz-content-sha256' header does not match what was computed.WFrAfemdE4ksWJYvOd3mzAyQwiNPtYLQAwtpTfgkOOw=585ac07de99d13892c58962f39dde6cc0c90c2234fb582d0030b694df82438ecA1B4825462511096oJgr/fh2NHil/VnAfw9a72kQ1RaR8wi1rzD7GSLvzbQ3vQx8NarAPoL6QwD8hc62AzQmjAtqjAw=\n- Part 2: Error executing \"UploadPart\" on \"https://path.to.file.zip?partNumber=2&uploadId=gNqjjR_8MAQRvyI1fT64GnYb7yaR5IOqnSE.o61g1n1rr2fX4fTX4B4bqHbjXhIwZceDdd2x0sNQ.GAh.3LUhpJiAt1UY0HiWgBRim.X48ScEpzWLbJNil4flu6NHUAo\"; AWS HTTP error: Client error: 400 XAmzContentSHA256Mismatch (client): The provided 'x-amz-content-sha256' header does not match what was computed. - \nXAmzContentSHA256MismatchThe provided 'x-amz-content-sha256' header does not match what was computed.UKx/sV39DhZ3Hx4fYdi98YZ1VopkZV7IdgbHECEP078=50ac7fb15dfd0e16771f1e1f61d8bdf18675568a64655ec87606c710210fd3bfA44FB9D552743B7C7u5EMrtFNOSTI9K1pyUO2kL0EmAK+VM7Qr+yOdFGcoATA7zQzNoGtti1abJsRbOUIHikeGOkpUk=\n- Part 3: Error executing \"UploadPart\" on \"https://path.to.file.zip?partNumber=3&uploadId=gNqjjR_8MAQRvyI1fT64GnYb7yaR5IOqnSE.o61g1n1rr2fX4fTX4B4bqHbjXhIwZceDdd2x0sNQ.GAh.3LUhpJiAt1UY0HiWgBRim.X48ScEpzWLbJNil4flu6NHUAo\"; AWS HTTP error: Client error: 400 XAmzContentSHA256Mismatch (client): The provided 'x-amz-content-sha256' header does not match what was computed. - \nXAmzContentSHA256MismatchThe provided 'x-amz-content-sha256' header does not match what was computed.jNKAAI7PNUNtccYs9vZmD2K7rYF5NJxtsPcshEwzxTM=8cd280008ecf35436d71c62cf6f6660f62bbad8179349c6db0f72c844c33c533FD0464B481C0C1A8PPVJdcdVa24A12RFWPf9VJ98cPVex2hLBPFQZQaOOlVd0v18DCUiYQtLDhgzTBb8j1WIRhU23ZM=\n- Part 4: Error executing \"UploadPart\" on \"https://path.to.file.zip?partNumber=4&uploadId=gNqjjR_8MAQRvyI1fT64GnYb7yaR5IOqnSE.o61g1n1rr2fX4fTX4B4bqHbjXhIwZceDdd2x0sNQ.GAh.3LUhpJiAt1UY0HiWgBRim.X48ScEpzWLbJNil4flu6NHUAo\"; AWS HTTP error: Client error: 400 XAmzContentSHA256Mismatch (client): The provided 'x-amz-content-sha256' header does not match what was computed. - \nXAmzContentSHA256MismatchThe provided 'x-amz-content-sha256' header does not match what was computed.VfZiQ/vvOIAzTW2Iohu8wdjOWI0eFfToA5XRAGGDMi8=55f66243fbef3880334d6d88a21bbcc1d8ce588d1e15f4e80395d1006183322fBDEFC5CD47CB77B1zobhlZ3m0w1lbEFRMfY4Yfvz4R1GeM3P91gL3rrqLp1OOUlOSPQBo2VvP+9f1PqApiWIP8FlTWo=\n- Part 5: Error executing \"UploadPart\" on \"https://path.to.file.zip?partNumber=5&uploadId=gNqjjR_8MAQRvyI1fT64GnYb7yaR5IOqnSE.o61g1n1rr2fX4fTX4B4bqHbjXhIwZceDdd2x0sNQ.GAh.3LUhpJiAt1UY0HiWgBRim.X48ScEpzWLbJNil4flu6NHUAo\"; AWS HTTP error: Client error: 400 XAmzContentSHA256Mismatch (client): The provided 'x-amz-content-sha256' header does not match what was computed. - \nXAmzContentSHA256MismatchThe provided 'x-amz-content-sha256' header does not match what was computed.iihTifIEEGAVVrNeGA0bn5Uuz6Hx6ZWDipPr+siHpLc=8a285389f20410601556b35e180d1b9f952ecfa1f1e995838a93ebfac887a4b74A998BE462A789C9OOW31pVc09eAmK7ZeZXbCokOf0vUTXvQ7ofCaKoDjCpHSKFCcJkzTEjy6sy3GwC4sCdtm1Zervc=\n- Part 7: Error executing \"UploadPart\" on \"https://path.to.file.zip?partNumber=7&uploadId=gNqjjR_8MAQRvyI1fT64GnYb7yaR5IOqnSE.o61g1n1rr2fX4fTX4B4bqHbjXhIwZceDdd2x0sNQ.GAh.3LUhpJiAt1UY0HiWgBRim.X48ScEpzWLbJNil4flu6NHUAo\"; AWS HTTP error: Client error: 400 XAmzContentSHA256Mismatch (client): The provided 'x-amz-content-sha256' header does not match what was computed. - \nXAmzContentSHA256MismatchThe provided 'x-amz-content-sha256' header does not match what was computed.bTQC9QW84OKeUuC3JFxsBDwqFeg/89vsECAkGnrIBFg=6d3402f505bce0e29e52e0b7245c6c043c2a15e83ff3dbec1020241a7ac80458A5BB9A5B0D459730djuRmeJ4Xv2Twy0kuGVQkGIgu5sHlLrSJ7qk7Dd7Hr5ohiZ+/YG5yZQiwvudYIXDp7oKzRKnBI8=\n- Part 6: Error executing \"UploadPart\" on \"https://path.to.file.zip?partNumber=6&uploadId=gNqjjR_8MAQRvyI1fT64GnYb7yaR5IOqnSE.o61g1n1rr2fX4fTX4B4bqHbjXhIwZceDdd2x0sNQ.GAh.3LUhpJiAt1UY0HiWgBRim.X48ScEpzWLbJNil4flu6NHUAo\"; AWS HTTP error: Client error: 400 XAmzContentSHA256Mismatch (client): The provided 'x-amz-content-sha256' header does not match what was computed. - \nXAmzContentSHA256MismatchThe provided 'x-amz-content-sha256' header does not match what was computed.i9nHCbbym5VbnuisiNNoHKefev1LzdrZ8Yq7Zs490D0=8bd9c709b6f29b955b9ee8ac88d3681ca79f7afd4bcddad9f18abb66ce3dd03d694465F5C4A1DA31QLiftIN26TJqjgToq88Xmk06h4gj1niEk5KyNOvdB7mfuWdmWCLs7uE6jItwN7/GjzHERFR5zrw=\n- Part 8: Error executing \"UploadPart\" on \"https://path.to.file.zip?partNumber=8&uploadId=gNqjjR_8MAQRvyI1fT64GnYb7yaR5IOqnSE.o61g1n1rr2fX4fTX4B4bqHbjXhIwZceDdd2x0sNQ.GAh.3LUhpJiAt1UY0HiWgBRim.X48ScEpzWLbJNil4flu6NHUAo\"; AWS HTTP error: Client error: 400 XAmzContentSHA256Mismatch (client): The provided 'x-amz-content-sha256' header does not match what was computed. - \nXAmzContentSHA256MismatchThe provided 'x-amz-content-sha256' header does not match what was computed.0so55Y7l5hVZTrgb+vMa6fFgPtJrbEa3uZD6orqHF4A=d2ca39e58ee5e615594eb81bfaf31ae9f1603ed26b6c46b7b990faa2ba8717805B8946D6E6FB38D8LKV1SzbaW4V9ldtg2H8mDR9RKPFHdEGd5pCDJXqYp5ZarVCoRomsE9C9hZrhqQZfOZVdJlAmTa0=\n- Part 10: Error executing \"UploadPart\" on \"https://path.to.file.zip?partNumber=10&uploadId=gNqjjR_8MAQRvyI1fT64GnYb7yaR5IOqnSE.o61g1n1rr2fX4fTX4B4bqHbjXhIwZceDdd2x0sNQ.GAh.3LUhpJiAt1UY0HiWgBRim.X48ScEpzWLbJNil4flu6NHUAo\"; AWS HTTP error: Client error: 400 XAmzContentSHA256Mismatch (client): The provided 'x-amz-content-sha256' header does not match what was computed. - \nXAmzContentSHA256MismatchThe provided 'x-amz-content-sha256' header does not match what was computed.Bmd3ZanEUhSBmuspStuhj7IwWkwBiJuxW2rbOpy8Eog=06677765a9c45214819aeb294adba18fb2305a4c01889bb15b6adb3a9cbc1288765F16E2ABC87A06eUes9BW1WFcU55wGdbhFTiCQF+hkotSrDBhKRX8FnlaQRGmjUppB9JC4H1aLsXF6u5zQdbuNt6Q=\n- Part 9: Error executing \"UploadPart\" on \"https://path.to.file.zip?partNumber=9&uploadId=gNqjjR_8MAQRvyI1fT64GnYb7yaR5IOqnSE.o61g1n1rr2fX4fTX4B4bqHbjXhIwZceDdd2x0sNQ.GAh.3LUhpJiAt1UY0HiWgBRim.X48ScEpzWLbJNil4flu6NHUAo\"; AWS HTTP error: Client error: 400 XAmzContentSHA256Mismatch (client): The provided 'x-amz-content-sha256' header does not match what was computed. - \nXAmzContentSHA256MismatchThe provided 'x-amz-content-sha256' header does not match what was computed.etOd96Xj4yFl6dzfbvQKrOo7lNkHZ0IcoAqAk7BRDp0=7ad39df7a5e3e32165e9dcdf6ef40aacea3b94d90767421ca00a8093b0510e9dD2883C7E4D7EA9C10EsPq7dKMVlNy6W6VmhCRXD+es+l6bzmnXDO44dX94yrRfeufYwaj+8eiEtmjRoj3dCVSFXjS68=\n- Part 11: Error executing \"UploadPart\" on \"https://path.to.file.zip?partNumber=11&uploadId=gNqjjR_8MAQRvyI1fT64GnYb7yaR5IOqnSE.o61g1n1rr2fX4fTX4B4bqHbjXhIwZceDdd2x0sNQ.GAh.3LUhpJiAt1UY0HiWgBRim.X48ScEpzWLbJNil4flu6NHUAo\"; AWS HTTP error: Client error: 400 XAmzContentSHA256Mismatch (client): The provided 'x-amz-content-sha256' header does not match what was computed. - \nXAmzContentSHA256MismatchThe provided 'x-amz-content-sha256' header does not match what was computed.Zl4vFt+ifvoItLdUfG2CR8gn2d3DIm7CH/gMSdcTQPU=665e2f16dfa27efa08b4b7547c6d8247c827d9ddc3226ec21ff80c49d71340f594DBBFAE50D54BBCPErpqjm+hzVTXW6YK0DiCsIZFKZxivNnnmj16hax/H0ciHj+kcv3y69wldeBGK2Xk7aMQxpyDZk=\n- Part 14: Error executing \"UploadPart\" on \"https://path.to.file.zip?partNumber=14&uploadId=gNqjjR_8MAQRvyI1fT64GnYb7yaR5IOqnSE.o61g1n1rr2fX4fTX4B4bqHbjXhIwZceDdd2x0sNQ.GAh.3LUhpJiAt1UY0HiWgBRim.X48ScEpzWLbJNil4flu6NHUAo\"; AWS HTTP error: Client error: 400 XAmzContentSHA256Mismatch (client): The provided 'x-amz-content-sha256' header does not match what was computed. - \nXAmzContentSHA256MismatchThe provided 'x-amz-content-sha256' header does not match what was computed.0wSTMCRb3CQeyQtkEKM49TuRlGt2nK9hfRBlUxAwZPY=d3049330245bdc241ec90b6410a338f53b91946b769caf617d106553103064f66E36484A826E0562+YUMWK53gpMhr4JiLWWIhLnEL/i1e+GRPfFkPpkC/MzaivoV8X/dz3L9WTFs01mIWNYN0BcXqg8=\n- Part 13: Error executing \"UploadPart\" on \"https://path.to.file.zip?partNumber=13&uploadId=gNqjjR_8MAQRvyI1fT64GnYb7yaR5IOqnSE.o61g1n1rr2fX4fTX4B4bqHbjXhIwZceDdd2x0sNQ.GAh.3LUhpJiAt1UY0HiWgBRim.X48ScEpzWLbJNil4flu6NHUAo\"; AWS HTTP error: Client error: 400 XAmzContentSHA256Mismatch (client): The provided 'x-amz-content-sha256' header does not match what was computed. - \nXAmzContentSHA256MismatchThe provided 'x-amz-content-sha256' header does not match what was computed.TO0RHsjJvm36fuFCSRFhO2UWfvIvG5B9OiQbkYiNFgk=4ced111ec8c9be6dfa7ee1424911613b65167ef22f1b907d3a241b91888d1609F4C7D4220236F615a9CXJ4LEChIPGsVRh8r2GcOf+GSzkgCWsQxk0QaRx9xvMXseXG/DNJ2ds42WHLOY6dolPx/ez2w=\n- Part 12: Error executing \"UploadPart\" on \"https://path.to.file.zip?partNumber=12&uploadId=gNqjjR_8MAQRvyI1fT64GnYb7yaR5IOqnSE.o61g1n1rr2fX4fTX4B4bqHbjXhIwZceDdd2x0sNQ.GAh.3LUhpJiAt1UY0HiWgBRim.X48ScEpzWLbJNil4flu6NHUAo\"; AWS HTTP error: Client error: 400 XAmzContentSHA256Mismatch (client): The provided 'x-amz-content-sha256' header does not match what was computed. - \nXAmzContentSHA256MismatchThe provided 'x-amz-content-sha256' header does not match what was computed.PzmxWL0SFrDHepD6EdX+UoBKZz60KUPyP9n0ZYC+/LM=3f39b158bd1216b0c77a90fa11d5fe52804a673eb42943f23fd9f46580befcb37590DFD6928545FCqM0eVKmkpxg52AbQL+2evghaXR9CXBnpUK0YcKMhbKC2wJKdOqCTCBjb5r9c17aMcqv0RoGZK7M=\n' in /vendor/aws/aws/aws-sdk-php/src/Multipart/AbstractUploader.php:130\nStack trace:\n0 [internal function]: Aws\\Multipart\\AbstractUploader::Aws\\Multipart{closure}()\n1 /vendor/aws/guzzlehttp/promises/src/functions.php(489): Generator->send(NULL)\n2 /vendor/aws/guzzlehttp/promises/src/Promise.php(199): GuzzleHttp\\Promise{closure}(NULL)\n3 /vendor/aws/guzzlehttp/promises/src/Promise.php(152): GuzzleHttp\\Promise\\Promise::callHandler(1, NULL, Array)\n4 /vendor/aws/guzzlehttp/promises/src/TaskQueue.php(60): GuzzleHttp\\Promise\\Promise::GuzzleHttp\\Promise{closure}()\n5 /vendor/aws/guzzlehttp/guzzle/src/Handler/CurlMultiHandler.php(96): GuzzleHttp\\Promise\\TaskQueue->run()\n6 /vendor/aws/guzzlehttp/guzzle/src/Handler/CurlMultiHandler.php(123): GuzzleHttp\\Handler\\CurlMultiHandler->tick()\n7 /vendor/aws/guzzlehttp/promises/src/Promise.php(240): GuzzleHttp\\Handler\\CurlMultiHandler->execute(true)\n8 /vendor/aws/guzzlehttp/promises/src/Promise.php(217): GuzzleHttp\\Promise\\Promise->invokeWaitFn()\n9 /vendor/aws/guzzlehttp/promises/src/Promise.php(261): GuzzleHttp\\Promise\\Promise->waitIfPending()\n10 /vendor/aws/guzzlehttp/promises/src/Promise.php(219): GuzzleHttp\\Promise\\Promise->invokeWaitList()\n11 /vendor/aws/guzzlehttp/promises/src/Promise.php(261): GuzzleHttp\\Promise\\Promise->waitIfPending()\n12 /vendor/aws/guzzlehttp/promises/src/Promise.php(219): GuzzleHttp\\Promise\\Promise->invokeWaitList()\n13 /vendor/aws/guzzlehttp/promises/src/Promise.php(62): GuzzleHttp\\Promise\\Promise->waitIfPending()\n14 /vendor/aws/aws/aws-sdk-php/src/Multipart/AbstractUploader.php(86): GuzzleHttp\\Promise\\Promise->wait()\n15 /vendor/aws/aws/aws-sdk-php/src/S3/S3Client.php(303): Aws\\Multipart\\AbstractUploader->upload()\n16 /vendor/aws/league/flysystem-aws-s3-v3/src/AwsS3Adapter.php(509): Aws\\S3\\S3Client->upload('icpayment', 'project_name...', 'PK\\x03\\x04\\x14\\x00\\x00\\x00\\x08\\x00{+\\xEBF\\xB7...', 'private', Array)\n17 /vendor/aws/league/flysystem-aws-s3-v3/src/AwsS3Adapter.php(95): League\\Flysystem\\AwsS3v3\\AwsS3Adapter->upload('project_name...', 'PK\\x03\\x04\\x14\\x00\\x00\\x00\\x08\\x00{+\\xEBF\\xB7...', Object(League\\Flysystem\\Config))\n18 /vendor/aws/league/flysystem/src/Filesystem.php(81): League\\Flysystem\\AwsS3v3\\AwsS3Adapter->write('project_name...', 'PK\\x03\\x04\\x14\\x00\\x00\\x00\\x08\\x00{+\\xEBF\\xB7...', Object(League\\Flysystem\\Config))\n19 /system/lib/FILE.php(100): League\\Flysystem\\Filesystem->write('project_name...', 'PK\\x03\\x04\\x14\\x00\\x00\\x00\\x08\\x00{+\\xEBF\\xB7...')\n20 /system/lib/FILE.php(117): FILE::write('project_name...', 'PK\\x03\\x04\\x14\\x00\\x00\\x00\\x08\\x00{+\\xEBF\\xB7...')\n21 /cron/ftp_import.php(67): FILE::upload('/tmp/2015-07-11...', 'project_name...')\n22 {main}\nthrown in /vendor/aws/aws/aws-sdk-php/src/Multipart/AbstractUploader.php on line 130\n```\nthis looks a bit odd to me - because file size is only 65MB\nusing latest flysystem awss3v3 adapter with aws 3.1.0\n. ",
    "teseo": "I'm afraid is still happening in  (3.3.0)\n. Exception message:\nAn exception occurred while completing a multipart upload.\nException Trace:\n```\n0 /path/vendor/guzzlehttp/promises/src/Promise.php(199): Aws\\Multipart\\AbstractUploader->Aws\\Multipart{closure}(Object(Aws\\S3\\Exception\\S3Exception))\n1 /path/vendor/guzzlehttp/promises/src/Promise.php(152): GuzzleHttp\\Promise\\Promise::callHandler(2, Object(Aws\\S3\\Exception\\S3Exception), Array)\n2 /path/vendor/guzzlehttp/promises/src/TaskQueue.php(60): GuzzleHttp\\Promise\\Promise::GuzzleHttp\\Promise{closure}()\n3 /path/vendor/guzzlehttp/guzzle/src/Handler/CurlMultiHandler.php(96): GuzzleHttp\\Promise\\TaskQueue->run()\n4 /path/vendor/guzzlehttp/guzzle/src/Handler/CurlMultiHandler.php(123): GuzzleHttp\\Handler\\CurlMultiHandler->tick()\n5 /path/vendor/guzzlehttp/promises/src/Promise.php(240): GuzzleHttp\\Handler\\CurlMultiHandler->execute(true)\n6 /path/vendor/guzzlehttp/promises/src/Promise.php(217): GuzzleHttp\\Promise\\Promise->invokeWaitFn()\n7 /path/vendor/guzzlehttp/promises/src/Promise.php(261): GuzzleHttp\\Promise\\Promise->waitIfPending()\n8 /path/vendor/guzzlehttp/promises/src/Promise.php(219): GuzzleHttp\\Promise\\Promise->invokeWaitList()\n9 /path/vendor/guzzlehttp/promises/src/Promise.php(261): GuzzleHttp\\Promise\\Promise->waitIfPending()\n10 /path/vendor/guzzlehttp/promises/src/Promise.php(219): GuzzleHttp\\Promise\\Promise->invokeWaitList()\n11 /path/vendor/guzzlehttp/promises/src/Promise.php(62): GuzzleHttp\\Promise\\Promise->waitIfPending()\n12 /path/vendor/aws/aws-sdk-php/src/Multipart/AbstractUploader.php(86): GuzzleHttp\\Promise\\Promise->wait()\n13 /path/vendor/path/Job/MoveToAwsJob.php(121): Aws\\Multipart\\AbstractUploader->upload()\n14 /path/vendor/slm/queue-beanstalkd/src/SlmQueueBeanstalkd/Worker/BeanstalkdWorker.php(33): Module\\Job\\MoveToAwsJob->execute()\n15 /path/vendor/slm/queue/src/SlmQueue/Strategy/ProcessQueueStrategy.php(70): SlmQueueBeanstalkd\\Worker\\BeanstalkdWorker->processJob(Object(Module\\Job\\MoveToAwsJob), Object(SlmQueueBeanstalkd\\Queue\\BeanstalkdQueue))\n16 [internal function]: SlmQueue\\Strategy\\ProcessQueueStrategy->onJobProcess(Object(SlmQueue\\Worker\\WorkerEvent))\n17 /path/vendor/zendframework/zend-eventmanager/src/EventManager.php(444): call_user_func(Array, Object(SlmQueue\\Worker\\WorkerEvent))\n18 /path/vendor/zendframework/zend-eventmanager/src/EventManager.php(205): Zend\\EventManager\\EventManager->triggerListeners('process.job', Object(SlmQueue\\Worker\\WorkerEvent), Array)\n19 /path/vendor/slm/queue/src/SlmQueue/Strategy/ProcessQueueStrategy.php(55): Zend\\EventManager\\EventManager->trigger('process.job', Object(SlmQueue\\Worker\\WorkerEvent))\n20 [internal function]: SlmQueue\\Strategy\\ProcessQueueStrategy->onJobPop(Object(SlmQueue\\Worker\\WorkerEvent))\n21 /path/vendor/zendframework/zend-eventmanager/src/EventManager.php(444): call_user_func(Array, Object(SlmQueue\\Worker\\WorkerEvent))\n22 /path/vendor/zendframework/zend-eventmanager/src/EventManager.php(205): Zend\\EventManager\\EventManager->triggerListeners('process.queue', Object(SlmQueue\\Worker\\WorkerEvent), Array)\n23 /path/vendor/slm/queue/src/SlmQueue/Worker/AbstractWorker.php(46): Zend\\EventManager\\EventManager->trigger('process.queue', Object(SlmQueue\\Worker\\WorkerEvent))\n24 /path/vendor/slm/queue/src/SlmQueue/Controller/AbstractWorkerController.php(49): SlmQueue\\Worker\\AbstractWorker->processQueue(Object(SlmQueueBeanstalkd\\Queue\\BeanstalkdQueue), Array)\n25 /path/vendor/zendframework/zend-mvc/src/Controller/AbstractActionController.php(82): SlmQueue\\Controller\\AbstractWorkerController->processAction()\n26 [internal function]: Zend\\Mvc\\Controller\\AbstractActionController->onDispatch(Object(Zend\\Mvc\\MvcEvent))\n27 /path/vendor/zendframework/zend-eventmanager/src/EventManager.php(444): call_user_func(Array, Object(Zend\\Mvc\\MvcEvent))\n28 /path/vendor/zendframework/zend-eventmanager/src/EventManager.php(205): Zend\\EventManager\\EventManager->triggerListeners('dispatch', Object(Zend\\Mvc\\MvcEvent), Object(Closure))\n29 /path/vendor/zendframework/zend-mvc/src/Controller/AbstractController.php(118): Zend\\EventManager\\EventManager->trigger('dispatch', Object(Zend\\Mvc\\MvcEvent), Object(Closure))\n30 /path/vendor/zendframework/zend-mvc/src/DispatchListener.php(93): Zend\\Mvc\\Controller\\AbstractController->dispatch(Object(Zend\\Console\\Request), Object(Zend\\Console\\Response))\n31 [internal function]: Zend\\Mvc\\DispatchListener->onDispatch(Object(Zend\\Mvc\\MvcEvent))\n32 /path/vendor/zendframework/zend-eventmanager/src/EventManager.php(444): call_user_func(Array, Object(Zend\\Mvc\\MvcEvent))\n33 /path/vendor/zendframework/zend-eventmanager/src/EventManager.php(205): Zend\\EventManager\\EventManager->triggerListeners('dispatch', Object(Zend\\Mvc\\MvcEvent), Object(Closure))\n34 /path/vendor/zendframework/zend-mvc/src/Application.php(314): Zend\\EventManager\\EventManager->trigger('dispatch', Object(Zend\\Mvc\\MvcEvent), Object(Closure))\n35 /path/public/index.php(17): Zend\\Mvc\\Application->run()\n36 {main}\"\n``\n. It is also happening to v3 and with pretty small files (in my case 3KB file). Exception:An exception occurred while completing a multipart upload.I'm using  (3.3.0)  ofaws/aws-sdk-php` \n. Exception message:\nAn exception occurred while completing a multipart upload.\nException Trace:\n```\n0 /path/vendor/guzzlehttp/promises/src/Promise.php(199): Aws\\Multipart\\AbstractUploader->Aws\\Multipart{closure}(Object(Aws\\S3\\Exception\\S3Exception))\n1 /path/vendor/guzzlehttp/promises/src/Promise.php(152): GuzzleHttp\\Promise\\Promise::callHandler(2, Object(Aws\\S3\\Exception\\S3Exception), Array)\n2 /path/vendor/guzzlehttp/promises/src/TaskQueue.php(60): GuzzleHttp\\Promise\\Promise::GuzzleHttp\\Promise{closure}()\n3 /path/vendor/guzzlehttp/guzzle/src/Handler/CurlMultiHandler.php(96): GuzzleHttp\\Promise\\TaskQueue->run()\n4 /path/vendor/guzzlehttp/guzzle/src/Handler/CurlMultiHandler.php(123): GuzzleHttp\\Handler\\CurlMultiHandler->tick()\n5 /path/vendor/guzzlehttp/promises/src/Promise.php(240): GuzzleHttp\\Handler\\CurlMultiHandler->execute(true)\n6 /path/vendor/guzzlehttp/promises/src/Promise.php(217): GuzzleHttp\\Promise\\Promise->invokeWaitFn()\n7 /path/vendor/guzzlehttp/promises/src/Promise.php(261): GuzzleHttp\\Promise\\Promise->waitIfPending()\n8 /path/vendor/guzzlehttp/promises/src/Promise.php(219): GuzzleHttp\\Promise\\Promise->invokeWaitList()\n9 /path/vendor/guzzlehttp/promises/src/Promise.php(261): GuzzleHttp\\Promise\\Promise->waitIfPending()\n10 /path/vendor/guzzlehttp/promises/src/Promise.php(219): GuzzleHttp\\Promise\\Promise->invokeWaitList()\n11 /path/vendor/guzzlehttp/promises/src/Promise.php(62): GuzzleHttp\\Promise\\Promise->waitIfPending()\n12 /path/vendor/aws/aws-sdk-php/src/Multipart/AbstractUploader.php(86): GuzzleHttp\\Promise\\Promise->wait()\n13 /path/vendor/path/Job/MoveToAwsJob.php(121): Aws\\Multipart\\AbstractUploader->upload()\n14 /path/vendor/slm/queue-beanstalkd/src/SlmQueueBeanstalkd/Worker/BeanstalkdWorker.php(33): Module\\Job\\MoveToAwsJob->execute()\n15 /path/vendor/slm/queue/src/SlmQueue/Strategy/ProcessQueueStrategy.php(70): SlmQueueBeanstalkd\\Worker\\BeanstalkdWorker->processJob(Object(Module\\Job\\MoveToAwsJob), Object(SlmQueueBeanstalkd\\Queue\\BeanstalkdQueue))\n16 [internal function]: SlmQueue\\Strategy\\ProcessQueueStrategy->onJobProcess(Object(SlmQueue\\Worker\\WorkerEvent))\n17 /path/vendor/zendframework/zend-eventmanager/src/EventManager.php(444): call_user_func(Array, Object(SlmQueue\\Worker\\WorkerEvent))\n18 /path/vendor/zendframework/zend-eventmanager/src/EventManager.php(205): Zend\\EventManager\\EventManager->triggerListeners('process.job', Object(SlmQueue\\Worker\\WorkerEvent), Array)\n19 /path/vendor/slm/queue/src/SlmQueue/Strategy/ProcessQueueStrategy.php(55): Zend\\EventManager\\EventManager->trigger('process.job', Object(SlmQueue\\Worker\\WorkerEvent))\n20 [internal function]: SlmQueue\\Strategy\\ProcessQueueStrategy->onJobPop(Object(SlmQueue\\Worker\\WorkerEvent))\n21 /path/vendor/zendframework/zend-eventmanager/src/EventManager.php(444): call_user_func(Array, Object(SlmQueue\\Worker\\WorkerEvent))\n22 /path/vendor/zendframework/zend-eventmanager/src/EventManager.php(205): Zend\\EventManager\\EventManager->triggerListeners('process.queue', Object(SlmQueue\\Worker\\WorkerEvent), Array)\n23 /path/vendor/slm/queue/src/SlmQueue/Worker/AbstractWorker.php(46): Zend\\EventManager\\EventManager->trigger('process.queue', Object(SlmQueue\\Worker\\WorkerEvent))\n24 /path/vendor/slm/queue/src/SlmQueue/Controller/AbstractWorkerController.php(49): SlmQueue\\Worker\\AbstractWorker->processQueue(Object(SlmQueueBeanstalkd\\Queue\\BeanstalkdQueue), Array)\n25 /path/vendor/zendframework/zend-mvc/src/Controller/AbstractActionController.php(82): SlmQueue\\Controller\\AbstractWorkerController->processAction()\n26 [internal function]: Zend\\Mvc\\Controller\\AbstractActionController->onDispatch(Object(Zend\\Mvc\\MvcEvent))\n27 /path/vendor/zendframework/zend-eventmanager/src/EventManager.php(444): call_user_func(Array, Object(Zend\\Mvc\\MvcEvent))\n28 /path/vendor/zendframework/zend-eventmanager/src/EventManager.php(205): Zend\\EventManager\\EventManager->triggerListeners('dispatch', Object(Zend\\Mvc\\MvcEvent), Object(Closure))\n29 /path/vendor/zendframework/zend-mvc/src/Controller/AbstractController.php(118): Zend\\EventManager\\EventManager->trigger('dispatch', Object(Zend\\Mvc\\MvcEvent), Object(Closure))\n30 /path/vendor/zendframework/zend-mvc/src/DispatchListener.php(93): Zend\\Mvc\\Controller\\AbstractController->dispatch(Object(Zend\\Console\\Request), Object(Zend\\Console\\Response))\n31 [internal function]: Zend\\Mvc\\DispatchListener->onDispatch(Object(Zend\\Mvc\\MvcEvent))\n32 /path/vendor/zendframework/zend-eventmanager/src/EventManager.php(444): call_user_func(Array, Object(Zend\\Mvc\\MvcEvent))\n33 /path/vendor/zendframework/zend-eventmanager/src/EventManager.php(205): Zend\\EventManager\\EventManager->triggerListeners('dispatch', Object(Zend\\Mvc\\MvcEvent), Object(Closure))\n34 /path/vendor/zendframework/zend-mvc/src/Application.php(314): Zend\\EventManager\\EventManager->trigger('dispatch', Object(Zend\\Mvc\\MvcEvent), Object(Closure))\n35 /path/public/index.php(17): Zend\\Mvc\\Application->run()\n36 {main}\"\n```\n. Hello Jeskew,\nThank you very much for the responses. My current code looks like this:\nphp\n                $options = array(\n                    'params' => array(\n                        'ServerSideEncryption' => 'aws:kms',\n                        'SSEKMSKeyId' => $keyIdAlias,\n                    ));\n                $options['before_initiate'] = function ($command) use ($options) {\n                    foreach ($options['params'] as $k => $v) {\n                        $command[$k] = $v;\n                    }\n                };\n            $uploader = new MultipartUploader($s3Client, $fileName, [\n                    'Bucket' => $bucket,\n                    'Key'    =>  $keyName,\n                    'acl'    => 'private'\n                ] + $options);\n\n\n            try{\n                echo \"3 Let's upload the file ...\" . PHP_EOL;\n\n                $result = $uploader->upload();\n\n                if(!empty($result['ObjectURL'])){\n                    echo \"4 object uploaded!....\" . PHP_EOL;\n\n\n                } else {\n                    echo \"3 Transfer failed!....\" . PHP_EOL;\n                }\n\n            }catch (\\Exception $e){\n                echo \"Failed in step 3\" . PHP_EOL;\n                var_dump($e->getMessage());\n            }\n\n```\nThe reason I'm doing this and not with the regular S3Client and upload is that when I was sending big files it actually did not enter the if where the multipart upload works, in the other hand the uploaded file was a text file with the big file location. I decided to use Multipart Upload as I didn't know this makes a difference with big and small files. I also found easier to integrate my KMS system with S3 on v3 this way.\n. I can see int the S3Client upload method the next:\n```\n * @param mixed  $body    Object data to upload. Can be a\n *                        StreamInterface, PHP stream resource, or a\n *                        string of data to upload.\n\n```\nWhen I used it and I was passing the path to a large file, it understood I tried to send a text file with the location of the large file as content.\n. I can also see that MultipartUploader extends from AbstractUploader class and this class uses the method determineSource to manipulate the $source variable like this:\nphp\n  /\n     * Turns the provided source into a stream and stores it.\n     \n     * If a string is provided, it is assumed to be a filename, otherwise, it\n     * passes the value directly to Psr7\\stream_for().\n     \n     * @param mixed $source\n     \n     * @return Stream\n     /\n    private function determineSource($source)\n    {\n        // Use the contents of a file as the data source.\n        if (is_string($source)) {\n            $source = Psr7\\try_fopen($source, 'r');\n        }\n    // Create a source stream.\n    $stream = Psr7\\stream_for($source);\n    if (!$stream->isReadable()) {\n        throw new IAE('Source stream must be readable.');\n    }\n\n    return $stream;\n}\n\n```\nWhile S3Client directly goes for \nphp\n        // Perform the required operations to upload the S3 Object.\n        $body = Psr7\\stream_for($body);\n```\nWithout determining what source it is. I don't know if this is for a reason or is a bug.\n. @jeskew from your response, what would be the ideal way to upload a few Gigs file, through \nS3Client->upload() and passing a string with the file location?\n. @jeskew passing fopen to S3Client worked perfectly for me. It could be great to specify in the documentation fopen('/path-to-file.ext') insteand of '/path-to-file.ext' when calling S3Client.\nThanks for your support!\n. I am going to respond to myself in case other people struggle. This code works with server side encryption and pretty much any other configuration params in the old CreateMultiPart, UploadPart and Completemultipart. See List of params available to be passed\n``` php\n            $options = array(\n                'params' => array(\n                    'ServerSideEncryption' => 'aws:kms',\n                    'SSEKMSKeyId' => $keyIdAlias,\n                ));\n            $options['before_initiate'] = function ($command) use ($options) {\n                foreach ($options['params'] as $k => $v) {\n                    $command[$k] = $v;\n                }\n            };\n\n            $uploader = new MultipartUploader($s3Client, $fileName, [\n                    'Bucket' => $bucket,\n                    'Key'    =>  $keyName,\n                    'acl'    => 'private'\n                ] + $options);\n\n```\nThis is based on the S3Client class taking a look at how the parameters are sent. \nI hope this helps \n. ",
    "mayrop": "It's just hard to handle response since sdk format returns different structure. On sdk format, every field is returned as an array, even though that field is not configured as an array in the \"Index Fields\". On json format, all the fields are returned in the same structure they are defined in the \"Index Fields\". When you handle the data returned from sdk format, it is hard to know if the field was actually an array initially or not...\n. Not sure which option would be... but below is an example of both responses, with sdk format & json format.\nSo for example... a request to CloudSearch in JSON format would look like this:\nhits: {\n    found: 20993,\n    start: 0,\n    hit: [\n        {\n            id: \"case_37bfcab9-380e-4e2b-ab9a-4b560527b666\",\n            fields: {\n                field1: \"etc\",\n                field2: \"etc2\",\n                field3: \"This is a description\",\n                account_status: [\n                    \"New\"\n                ],\n...\nFrom Json response, we get scalar values for field1, field2, field3... since those fields are either literal, integer or text in my \"Index Fields\" configuration (exactly what I would expect).\nA request to CloudSearch in SDK format would look like this:\nhits: {\n    found: 20993,\n    start: 0,\n    hit: [\n        {\n            id: \"case_37bfcab9-380e-4e2b-ab9a-4b560527b666\",\n            fields: {\n                field1: [\n                    \"etc\"\n                ],\n                field2: [\n                    \"etc2\"\n                ],\n                field3: [\n                    \"This is a description\"\n                ],\n                account_status: [\n                    \"New\"\n                ],\n...\nSo from the response, there's no way to know which fields are actually configured as array and which ones are literal/text/integer/etc, since all of them are returned as arrays.\nHope it makes sense :)\n. Yes it is protected... I actually had my own system to query cloudsearch... but thought it was better to start using the sdk after version 3 was released.... \nI assume there is no plan to make the format flexible... \n. Uhm ok, that's sad. I thought it was a matter of being able to do the requestUri flexible here: \nhttps://github.com/aws/aws-sdk-php/blob/master/src/data/cloudsearchdomain/2013-01-01/api-2.json#L17\nThough some code here:\nhttps://github.com/aws/aws-sdk-php/blob/master/src/Api/Operation.php#L22\nOh well.\n. Also.. sorry... but by hardcoding the pretty to true.. the requests are slower since requests size is bigger...\nI'm getting 1,121KB request (when I fetch for example 1000 results from a search) with pretty=true, and I get 746KB without pretty=true. It is a very significative difference when you're talking about big searches... and I get 1,196KB when I add format=sdk since it adds more characters for arrays.\nAnyway. \n. ",
    "yukulelix": "This works, thanks !\nThe documentation said you need a base64 encoded zip so I didn't even think of it !\n. ",
    "Lickwid": "Mine requires that I have base64_encode or I get the error...Which version was this changed in?\n. ",
    "by225": "Ended up using the Ruby SDK to upload the original 7G file.  It worked the first time, though apparently without concurrency.\nJust tested PHP SDK v3 on a different server with PHP 5.5.\nWith the region an empty string, I got: An exception occurred while initiating a multipart upload.\nWith the region set explicitly to the default value, the script exits with a fatal error: Uncaught exception 'GuzzleHttp\\Promise\\RejectionException' with message 'The promise was rejected with reason: Invoking the wait callback did not resolve the promise' in .../GuzzleHttp/Promise/functions.php:111\nI'm not calling promise() directly.  Just using basic upload from: http://docs.aws.amazon.com/aws-sdk-php/v3/guide/service/s3-multipart-upload.html\n. I still get the same results with 2.8.19: the last set of parts does not upload, and the operation ends with no error.\n. ",
    "nsmithuk": "@jeremeamia thanks.\nThe simplest way for us to add compression and encryption support to a SessionHandler is to extend the class. Then we pull in a PHP Trait that overrides the read() and write() methods, so they filter the data however we need, before passing it to the parent class.\n. Overwriting formatKey in my own SessionConnection is a splendid idea. I've gone with that. Thank you.\n. ",
    "neochief": "Ah, my bad, you're right. I was confused by going back and forth between google results and docs navigation. Thanks for the prompt reply, anyway!\n. ",
    "jarrettj": "Using zend framework 2. With composer. Very weird. Php version 5.5. Must be my environment.\n. Nope. \nIt's a normal controller in zend framework. I use composer install -o and I load the autoloader file as normal. Then I followed your sample docs on how to set up s3 client.\nWhen I step through the code with a debugger it for some reason fails and can't find any functions in that function.php file.\nCrap!\n. I did that already. Will try using another major version rather. A coworker used v2 with no problems. Will try that first. Thanks.\n. I've been using composer with zend framework 2 since the beginning of this project. Composer is not an issue. Using many packages via composer.\n1.) Ubuntu 13.10\n2.) No. I don't even know what that is.\n3)\nZend\\Loader\\AutoloaderFactory::factory(array(\n    'Zend\\Loader\\ClassMapAutoloader' => array(\n        'Composer' => __DIR__ . '/vendor/composer/autoload_classmap.php',\n    )\n));\nI use composer update -o to generate the autoload_classmap.php file. It's a zf2 optimization. That way all the files needed are preloaded. \nI checked the composer vendor folder to see if the functions.php file is loaded, and it is. In the autoload_files.php file.\nIn my controller I do the following:\n```\nuse Aws\\Sdk as Aws;\n$sdk = new Aws($config['aws']);\n$s3Client = $sdk->createS3();\n```\nThe $config['aws'] has the following in it:\n'aws' => [\n         'credentials' => [\n             'key'    => '*****',\n             'secret' => '*****'\n         ],\n         'region'   => 'us-east-1',\n         'version'  => 'latest',\n     ]\nError:\nFatal error: Call to undefined function Aws\\default_http_handler() in /var/www/conference/vendor/aws/aws-sdk-php/src/Sdk.php on line 75\n4.) I will try this. Like I said before, a coworker used v2.* in composer. And had no issues. Our projects use similar stacks. I will try that as well.\nThanks for the prompt responses.\nCheers.\n. Sorry for late reply. @jeskew you were correct.\nFor anyone using the same zf2 autoloader optimisation, add the following to your init_autoloader.php:\n$includeFiles = require __DIR__ . '/vendor/composer/autoload_files.php';\nforeach ($includeFiles as $file) {\n    require $file;\n}\nThanks to all. It's working now :)\nCheers.\n. There's no ENV set, no ~/.aws/credentials file either. \nDoing:\ncurl http://169.254.169.254/latest/meta-data/iam/security-credentials/\ngives nothing.\nMaybe that is my issue? But the instance has a Role attached for sure! Maybe I should raise an issue with AWS. My test and live server returning nothing for that curl request.\nWhen running:\ncurl http://169.254.169.254/latest/meta-data/iam/security-credentials/aws-elasticbeanstalk-ec2-role\nit works though. And returns a success message with access key, secret and token in it.\nThe AWS CLI works without credentials as well. I did the following:\naws s3 ls s3::/bucket/my/file\nThat works as well.\nJust the PHP SDK that I'm doing something wrong. I will relook at the documentation and try a simple example outside of my Zend Framework 2 application. \n. Solved.\nI created a cloned environment from our production environment. The testing site however needed it's own test bucket for images. The location of the images changed, so obviously they do not exist in the test bucket. \nWhen doing a single call it works. But my application does a few hundred requests to images that do not exist and maybe that is why the credentials server stops responding.\nA workaround, and good practice, is to cache the credentials. This avoids hitting the metadata service a couple of hundred times per request.\n[\n         'region'   => 'us-east-1',\n         'version'  => 'latest',\n         'bucket' => 'test-bucket',\n         'credentials' => new \\Aws\\DoctrineCacheAdapter(new \\Doctrine\\Common\\Cache\\ApcuCache)\n]\nThat worked for me. :)\n. ",
    "pavelsc": "Hi,\n3.0.4 - 2015-06-11\nAnd here is my connection code, if it may help.\n$this->dynamoDbClient = DynamoDbClient::factory(array(\n    'region' => 'us-east-1',\n    'version' => 'latest',\n    'credentials' => [\n        'key' => $awsConf['access_key_id'],\n        'secret' => $awsConf['secret_access_key']\n    ]\n));\n. new details in the top comment\n. @jeskew you didn't use hardcoded credentials in your connection\nhttp://docs.aws.amazon.com/aws-sdk-php/v3/guide/guide/credentials.html#hardcoded-credentials\n. ",
    "spamoom": "After much wall punching, turns out everyone at our end missed the tiny green banner at the top of the docs pointing to V3 #fml\nCould I maybe suggest it's made bigger / fixed to the top as you scroll? http://docs.aws.amazon.com/aws-sdk-php/v2/api/class-Aws.Glacier.GlacierClient.html\n. ",
    "jpinner-lyft": "@dstevenson when setting bucket_endpoint: true the \"BucketEndpointMiddleware\" is added to the stack but that does not modify the host header to prepend the bucket name.\n. @jeremeamia I discovered this while running tests against the pre-signed URLs generated by v3 of the library for \"PutObject\" calls. The previous version would create virtual-hosted style URLs while v3 generates the path-style URLs instead. In general, the system I'm integrating into expects the virtual-hosted style URLs to proxy calls to S3.\n@jeskew That requires one client to be instantiated per bucket-name and fails when generating commands that do not contain a 'Bucket' parameter or when the command is in the exclusion list (e.g. GetBucketLocation).\n. If the intent of \"BucketEndpointMiddleware\" is to convert from path-style URLs to virtual-hosted style URLs, and for the conversion to happen via configuration in v3 instead of automatically as before, I'm happy to create a pull request.\n. @mtdowling thanks for clarifying the intent of the \"bucket_endpoint\" option.\nCould you clarify how the region and sig-v4 are related to the URL style? Since the request is canonicalized before signing and includes both the path and host header, how does this switch to v4 affect the URL style? As I understand it, the signature processing and request routing should be independent.\n. @dstevenson I can write middleware the replicates the \"BucketStyleListener\" functionality from v2 -- also am happy to create a pull request that does so.\nIt sounds like the move away from virtual-hosted style URLs was intentional, however it seems at odds with documentation for the REST API.\n@mtdowling Please correct me if I'm wrong but I believe it is possible to re-introduce the automatic transform without affecting the ability to sign the requests.\n. @jeskew Thank you for the detailed explanation.\n. ",
    "okinaka": "Thank you!\n. ",
    "MagicalTux": "Tested and confirmed to solve issue raised in #694 \n. ",
    "bensigma": "I'm using the node.js environment and there i'm returning a json-encoded string currently. \nIf i return a json object the conversion works. Have not seen that this is a requirement somewhere in the documentation. Thanks for the hint.\nIn case others stumble upon that: \nI get json data from a php executable. This data needs to be converted from bytes into a string, which needs to be converted to a js JSON object - apparently:\nphp.stdout.on('data', function (data) {\n        var mdata = data.toString();\n        // must return json object - string will not be converted correctly in aws-sdk\n        context.succeed(JSON.parse(mdata));\n    });\n. ",
    "stefandoorn": "Hi @jeskew, \nThanks for the clarification :) I will be happy to test it live as soon as it made it into a release.\n. Hi @jeskew, wanted to let you know I had some problems on/after deploy:\nAws\\Ec2\\Exception\\Ec2Exception: AWS Error Code: AuthFailure, Status Code: 401, AWS Request ID: XXX, AWS Error Type: client, AWS Error Message: Authorization header or parameters are not formatted correctly., User-Agent: aws-sdk-php2/2.8.17 Guzzle/3.9.3 curl/7.40.0 PHP/5.5.26 [] []\nRemoving the cached credentials by hand solved things, error only happened once. Not sure if more people have this problem, but atleast good you know it happened I think :) Not sure if invalidating cache is possible on release, but would be a nice feature for sure :)\n. This feature would be very welcomed by me. We are working on a high traffic website, using IAM credentials for EC2, AutoScaling & DynamoDB. The more caching on these requests, the better. I'm currently upgrading v2 to v3 and in v2 it was fairly easy to cache the credentials. Before I did that we could hit some API limits under high traffic. Now when migrating to v3 I couldn't find that feature. Ofcourse it's possible to implement a solution myself (as suggested in here als: https://twitter.com/awsforphp/status/638072096497795072), but I think more people would be happy to use a common feature like this.\n. @jeskew. Recently we discussed something related to cache in this issue (2.8): https://github.com/aws/aws-sdk-php/issues/710. In there you mentioned about the cache keys using version names. Am I right that this PR is going to implement the feature and right now it aint implemented yet?\n. @jeskew Perfect. When is it expected for this PR to be shipped? I would be happy to continue my migration from v2 to v3 :)\n. ",
    "benconnito": "The value it might have is that it would be compatible with 2.* versions of the SDK. Obviously it is easy to implement this on our own, but just a simple spot not to have introduced a breaking change.\n. ",
    "iam-merlin": "For reference (same issue on a different class with merge) : https://github.com/aws/aws-sdk-php/pull/92\n. @GrahamCampbell , it's simply to test a dumb factory :/, this is the only reason to mock aws/sdk.\n. Hi,\nSDK is already mocked using phpunit (in aws test). For mockery, it seems it's a problem with an old version and a specific test for mockery need a new dev dependency (and that is not, in my point of view, a good idea). Now it's ok for me.\nI think the last dumb change we previsouly discuss can be accepted now.\n. done ;)\n. oups, my bad. sorry for the parameter rename, indeed ofc.\n. ",
    "r3wt": "@jeremeamia It is still not working. We are doing exactly as you directed, but no change. We are serve the files through cloudfront, but they show up as file downloads to the client instead of showing them in browser.\n. php\n$s3->putObject([\n    'Bucket'=>$bucket,\n    'Key'=>$key,\n    'Body'=>$body,\n    'CacheControl'=>'max-age=172800',\n    'Content-Type'=>$mime\n]);\nwhere $bucket is the target bucket, $key is a string, $body is a blob, and $mime is one of following possible types:\nimage/png\nimage/jpeg\nimage/gif\nWhen accessing image directly, the browser prompts for a file download. this is not a desired behavior.\n. @jeskew i can make the change, but what can i do about images that are uploaded already? we have around 10k files uploaded already. I wish php was strictly typed, and you could define the shape of arrays. it would make it so much better. Also Content-Type is a standard header. why did you remove the hyphen? random deviations from accepted protocols are a bad practice.\n. ",
    "yvele": "@jeskew I'm glad I found your answer here because the doc is not clear about that.\nIt's so confusing :\n- There is a Metadata, a ContentType and a  MetadataDirective field\n- If I only pass a ContentType nothing appends.. Not even an error\n- Even I don't pass  Metadata, I have to pass MetadataDirective in order to update ContentType\n- ContentType update does not have the same behaviour as other header (eg ContentEncoding)\nPS: I've submitted an issue about JavaScript API documentation : https://github.com/aws/aws-sdk-js/issues/1092\n. ",
    "keithbmcd": "thanks jeskew!\n. ",
    "coveralls": "\nChanges Unknown when pulling 9e48a0178d15741fdfafb69b171fbbd25587a1db on jeskew:fix/compile-json-at-build-time into  on aws:master.\n. ",
    "vov4ik08": "no, ec3\n. yes s3, but not help\nmy code\n```\n<?php\nrequire 'vendor/autoload.php';\nuse Aws\\S3\\S3Client;\n// Instantiate an Amazon S3 client.\n$s3 = new S3Client([\n    'version' => 'latest',\n    'region'  => 'us-west-2'\n]);\n// Upload a publicly accessible file. The file size and type are determined by the SDK.\ntry {\n    $s3->putObject([\n        'Bucket' => '.....t',\n        'Key'    => '.....',\n        'Body'   => fopen('c:\\test.txt', 'r'),\n        'ACL'    => 'public-read',\n    ]);\n} catch (Aws\\Exception\\S3Exception $e) {\n    echo \"There was an error uploading the file.\\n\";\n}\n```\nThis problem reproduced after today update aws-php-sdk\n. in the IAM console.\n. i do not know, i try only for s3, 2 weeks ago, everything worked and ip 169.254.169.254 not ping\n. Thanks, I'll try and accomplish your goal\n. ",
    "sandboxws": "My bad, I didn't provide enough info :)\nSystem info:\nPHP: 5.6.12\nApache: 2.2\nOS: OS X Yosemite\nAWS SDK: 3.2 (latest)\nPHP in cli is the same version as the one used with fpm\nbash\nphp -v\nPHP 5.6.12 (cli) (built: Aug 21 2015 02:38:12) (DEBUG)\nCopyright (c) 1997-2015 The PHP Group\nZend Engine v2.6.0, Copyright (c) 1998-2015 Zend Technologies\n    with Xdebug v2.3.3, Copyright (c) 2002-2015, by Derick Rethans\nbash\nphp-fpm -v\nPHP 5.6.12 (fpm-fcgi) (built: Aug 21 2015 02:38:13) (DEBUG)\nCopyright (c) 1997-2015 The PHP Group\nZend Engine v2.6.0, Copyright (c) 1998-2015 Zend Technologies\n    with Xdebug v2.3.3, Copyright (c) 2002-2015, by Derick Rethans\nNothing is logged in the php-fpm logs except for the crash log, however, here is a snippet of the os crash report\n```\nProcess:               php-fpm [65985]\nPath:                  /usr/local/Cellar/php56/5.6.12/sbin/php-fpm\nIdentifier:            php-fpm\nVersion:               0\nCode Type:             X86-64 (Native)\nParent Process:        php-fpm [65974]\nResponsible:           php-fpm [65974]\nUser ID:               1311657067\nDate/Time:             2015-08-21 03:17:10.350 +0400\nOS Version:            Mac OS X 10.10.4 (14E46)\nReport Version:        11\nAnonymous UUID:        A1CF1434-D22F-394E-5CDD-9EAE017F66A6\nSleep/Wake UUID:       942FD2DB-02DE-4402-941D-A30F3A903ED6\nTime Awake Since Boot: 37000 seconds\nTime Since Wake:       4200 seconds\nCrashed Thread:        0  Dispatch queue: com.apple.main-thread\nException Type:        EXC_BAD_ACCESS (SIGSEGV)\nException Codes:       KERN_INVALID_ADDRESS at 0x0000000000000110\nVM Regions Near 0x110:\n--> \n    __TEXT                 0000000102b52000-0000000103658000 [ 11.0M] r-x/rwx SM=COW  /usr/local/Cellar/php56/5.6.12/sbin/php-fpm\nApplication Specific Information:\ncrashed on child side of fork pre-exec\nThread 0 Crashed:: Dispatch queue: com.apple.main-thread\n0   libdispatch.dylib               0x00007fff9073e5d2 _dispatch_queue_wakeup_with_qos_slow + 525\n1   libdispatch.dylib               0x00007fff90733cfc _dispatch_mach_msg_send + 1690\n2   libdispatch.dylib               0x00007fff907335e4 dispatch_mach_send + 326\n3   libxpc.dylib                    0x00007fff8de48ebd _xpc_connection_send_message_with_reply_f + 125\n4   libxpc.dylib                    0x00007fff8de48dce xpc_connection_send_message_with_reply_sync + 185\n5   com.apple.CoreFoundation        0x00007fff8b15c1c8 -[CFPrefsPlistSource sendRequestNewDataMessage:toConnection:error:] + 88\n6   com.apple.CoreFoundation        0x00007fff8b15b8f8 __50-[CFPrefsPlistSource alreadylocked_requestNewData]_block_invoke + 152\n7   com.apple.CoreFoundation        0x00007fff8b15355b _CFPrefsWithDaemonConnection + 331\n8   com.apple.CoreFoundation        0x00007fff8b15b80e -[CFPrefsPlistSource alreadylocked_requestNewData] + 254\n9   com.apple.CoreFoundation        0x00007fff8afda070 _copyValueForKey + 208\n10  com.apple.CoreFoundation        0x00007fff8afd9f73 -[CFPrefsPlistSource copyValueForKey:] + 51\n11  com.apple.CoreFoundation        0x00007fff8b0027a0 CFPreferencesCopyValueWithContainerblock_invoke + 32\n12  com.apple.CoreFoundation        0x00007fff8afd8e75 +[CFPrefsSource withSourceForIdentifier:user:byHost:container:perform:] + 1045\n13  com.apple.CoreFoundation        0x00007fff8b00273c _CFPreferencesCopyValueWithContainer + 236\n14  com.apple.security              0x00007fff8da9dc5a _SSLContextReadDefault + 46\n15  libsystem_pthread.dylib         0x00007fff9036aaf4 pthread_once_handler + 65\n16  libsystem_platform.dylib        0x00007fff919f6f13 _os_once + 73\n17  libsystem_pthread.dylib         0x00007fff9036aa93 pthread_once + 57\n18  com.apple.security              0x00007fff8da9c4d3 SSLCreateContextWithRecordFuncs + 292\n19  com.apple.security              0x00007fff8da9c361 SSLCreateContext + 21\n20  libcurl.4.dylib                 0x0000000103b705ab darwinssl_connect_common + 574\n21  libcurl.4.dylib                 0x0000000103b6f9c6 Curl_ssl_connect_nonblocking + 77\n22  libcurl.4.dylib                 0x0000000103b3c517 Curl_http_connect + 77\n23  libcurl.4.dylib                 0x0000000103b4a443 Curl_protocol_connect + 129\n24  libcurl.4.dylib                 0x0000000103b5b89b multi_runsingle + 902\n25  libcurl.4.dylib                 0x0000000103b5b46d curl_multi_perform + 159\n26  php-fpm                         0x0000000102cac6db zif_curl_multi_exec + 444\n```\nHope this helps!\n. @jeskew you arrived at the same conclusion a minute before me, there seems to be a problem with fpm, curl and openssl on my machine. This is definitely not a problem with the SDK!\n. Just for the record, I solved the problem by running php-fpm as root!\n. ",
    "byustephen": "Thanks for looking into this. I've already added in a if statement to see if the key (or in this case the $file_data->ref) is empty. I just thought it was a security issue that should be looked into. Cheers!\n. Thank you for letting me know. I was expecting to know if a file was successfully deleted, and it seemed that the delete marker was the likely place. \nThen what do you recommend for knowing that the file was removed? Should I just try to get meta data on the file, and if no meta data then that confirms it is gone?\nAlso, why does the deleteObject response not throw an issue when it tries to delete a file it is not there? The response shows the exact same thing whether the file was deleted, or if I try to delete the same file but it is not there. \nThanks for taking the time to answer this. \n. Thank for the excellent reply. I'm good, so you can close this thread.\n. ",
    "jmcbee": "You can use https://packagist.org/packages/guzzlehttp/guzzle#v3.8.1. It sucks having the need to make our app 5.3.x compatible of which your newer versions of the SDK does not seem to appear be.\n. I was assuming that the 2.8 branch (where a new version was released 9 days ago) was for 5.3 compatibility, I checked newer versions but they're requiring newer PHP versions. I was just hoping the guzzle/guzzle dependency could be renamed to guzzlehttp/guzzle of the same version since other packages are now using that package instead of the now deprecated one (of which I don't know the reason why).\nhttps://packagist.org/packages/guzzle/guzzle#v3.7.4 (deprecated)\nhttps://packagist.org/packages/guzzlehttp/guzzle#v3.7.4\nhttps://github.com/aws/aws-sdk-php/blob/2.8/composer.json#L20\n. I think you are misunderstanding my point here, I just wanted guzzle/guzzle to be renamed to guzzle/guzzlehttp ( no version change). \n. Thanks guys.\n. ",
    "Sensirex": "I trying get \nuploadId and key on first step\nheaders \"Authorization\" and \"x-amz-date\" on second step\nsend it to client\nClient uploads the file yourself using headers (second step), uploadId and key (first step).\nAfter the update, this method stopped working.\nIn response, i received the error:\n-You must specify a non-null value for the body... (first step)\n-something about the secret key   (second step)\nsmall changes in SDK have corrected it, but I think there is another way, am i right?\n. Thanks a lot, it work great :)\n. ",
    "torfeld6": "I actually did try dropping the concurrency to one, but same issue.\n. I'm sorry for the delay, other projects came up.\nThe full exception:\n{\n    \"code\": 0,\n    \"file\": \"/Users/josh/Documents/Workspace/pitcher-zerodrive/vendor/aws/aws-sdk-php/src/WrappedHttpHandler.php\",\n    \"line\": 192,\n    \"message\": \"Error executing \\\"GetObject\\\" on \\\"https://s3-eu-west-1.amazonaws.com/pitcherzerodrive/158684/extracted/assets/player/close_hi%25402x.png\\\"; AWS HTTP error: Client error: `GET https://s3-eu-west-1.amazonaws.com/pitcherzerodrive/158684/extracted/assets/player/close_hi%25402x.png` resulted in a `404 Not Found` response:\\n<?xml version=\\\"1.0\\\" encoding=\\\"UTF-8\\\"?>\\n<Error><Code>NoSuchKey</Code><Message>The specified key does not exist.</Message> (truncated...)\\n NoSuchKey (client): The specified key does not exist. - <?xml version=\\\"1.0\\\" encoding=\\\"UTF-8\\\"?>\\n<Error><Code>NoSuchKey</Code><Message>The specified key does not exist.</Message><Key>158684/extracted/assets/player/close_hi%402x.png</Key><RequestId>361D2C42536A6171</RequestId><HostId>R36PZxWpzlAfwYp4tSkbrcEsascMgRw9IdyUHbxyfSVQRf/+4jhmUEhYcI47u5fBqZpBwFdl6+E=</HostId></Error>\",\n    \"name\": \"Exception\",\n    \"previous\": {\n        \"code\": 404,\n        \"file\": \"/Users/josh/Documents/Workspace/pitcher-zerodrive/vendor/guzzlehttp/guzzle/src/Exception/RequestException.php\",\n        \"line\": 111,\n        \"message\": \"Client error: `GET https://s3-eu-west-1.amazonaws.com/pitcherzerodrive/158684/extracted/assets/player/close_hi%25402x.png` resulted in a `404 Not Found` response:\\n<?xml version=\\\"1.0\\\" encoding=\\\"UTF-8\\\"?>\\n<Error><Code>NoSuchKey</Code><Message>The specified key does not exist.</Message> (truncated...)\\n\",\n        \"name\": \"Exception\",\n        \"stack-trace\": [\n            \"#0 /Users/josh/Documents/Workspace/pitcher-zerodrive/vendor/guzzlehttp/guzzle/src/Middleware.php(65): GuzzleHttp\\\\Exception\\\\RequestException::create(Object(GuzzleHttp\\\\Psr7\\\\Request), Object(GuzzleHttp\\\\Psr7\\\\Response))\",\n            \"#1 /Users/josh/Documents/Workspace/pitcher-zerodrive/vendor/guzzlehttp/promises/src/Promise.php(203): GuzzleHttp\\\\Middleware::GuzzleHttp\\\\{closure}(Object(GuzzleHttp\\\\Psr7\\\\Response))\",\n            \"#2 /Users/josh/Documents/Workspace/pitcher-zerodrive/vendor/guzzlehttp/promises/src/Promise.php(156): GuzzleHttp\\\\Promise\\\\Promise::callHandler(1, Object(GuzzleHttp\\\\Psr7\\\\Response), Array)\",\n            \"#3 /Users/josh/Documents/Workspace/pitcher-zerodrive/vendor/guzzlehttp/promises/src/TaskQueue.php(47): GuzzleHttp\\\\Promise\\\\Promise::GuzzleHttp\\\\Promise\\\\{closure}()\",\n            \"#4 /Users/josh/Documents/Workspace/pitcher-zerodrive/vendor/guzzlehttp/guzzle/src/Handler/CurlMultiHandler.php(96): GuzzleHttp\\\\Promise\\\\TaskQueue->run()\",\n            \"#5 /Users/josh/Documents/Workspace/pitcher-zerodrive/vendor/guzzlehttp/guzzle/src/Handler/CurlMultiHandler.php(123): GuzzleHttp\\\\Handler\\\\CurlMultiHandler->tick()\",\n            \"#6 /Users/josh/Documents/Workspace/pitcher-zerodrive/vendor/guzzlehttp/promises/src/Promise.php(246): GuzzleHttp\\\\Handler\\\\CurlMultiHandler->execute(true)\",\n            \"#7 /Users/josh/Documents/Workspace/pitcher-zerodrive/vendor/guzzlehttp/promises/src/Promise.php(223): GuzzleHttp\\\\Promise\\\\Promise->invokeWaitFn()\",\n            \"#8 /Users/josh/Documents/Workspace/pitcher-zerodrive/vendor/guzzlehttp/promises/src/Promise.php(266): GuzzleHttp\\\\Promise\\\\Promise->waitIfPending()\",\n            \"#9 /Users/josh/Documents/Workspace/pitcher-zerodrive/vendor/guzzlehttp/promises/src/Promise.php(225): GuzzleHttp\\\\Promise\\\\Promise->invokeWaitList()\",\n            \"#10 /Users/josh/Documents/Workspace/pitcher-zerodrive/vendor/guzzlehttp/promises/src/Promise.php(269): GuzzleHttp\\\\Promise\\\\Promise->waitIfPending()\",\n            \"#11 /Users/josh/Documents/Workspace/pitcher-zerodrive/vendor/guzzlehttp/promises/src/Promise.php(225): GuzzleHttp\\\\Promise\\\\Promise->invokeWaitList()\",\n            \"#12 /Users/josh/Documents/Workspace/pitcher-zerodrive/vendor/guzzlehttp/promises/src/Promise.php(62): GuzzleHttp\\\\Promise\\\\Promise->waitIfPending()\",\n            \"#13 /Users/josh/Documents/Workspace/pitcher-zerodrive/vendor/guzzlehttp/promises/src/Coroutine.php(65): GuzzleHttp\\\\Promise\\\\Promise->wait()\",\n            \"#14 /Users/josh/Documents/Workspace/pitcher-zerodrive/vendor/guzzlehttp/promises/src/Promise.php(246): GuzzleHttp\\\\Promise\\\\Coroutine->GuzzleHttp\\\\Promise\\\\{closure}(true)\",\n            \"#15 /Users/josh/Documents/Workspace/pitcher-zerodrive/vendor/guzzlehttp/promises/src/Promise.php(223): GuzzleHttp\\\\Promise\\\\Promise->invokeWaitFn()\",\n            \"#16 /Users/josh/Documents/Workspace/pitcher-zerodrive/vendor/guzzlehttp/promises/src/Promise.php(266): GuzzleHttp\\\\Promise\\\\Promise->waitIfPending()\",\n            \"#17 /Users/josh/Documents/Workspace/pitcher-zerodrive/vendor/guzzlehttp/promises/src/Promise.php(225): GuzzleHttp\\\\Promise\\\\Promise->invokeWaitList()\",\n            \"#18 /Users/josh/Documents/Workspace/pitcher-zerodrive/vendor/guzzlehttp/promises/src/Promise.php(62): GuzzleHttp\\\\Promise\\\\Promise->waitIfPending()\",\n            \"#19 /Users/josh/Documents/Workspace/pitcher-zerodrive/vendor/guzzlehttp/promises/src/EachPromise.php(101): GuzzleHttp\\\\Promise\\\\Promise->wait()\",\n            \"#20 /Users/josh/Documents/Workspace/pitcher-zerodrive/vendor/guzzlehttp/promises/src/Promise.php(246): GuzzleHttp\\\\Promise\\\\EachPromise->GuzzleHttp\\\\Promise\\\\{closure}(true)\",\n            \"#21 /Users/josh/Documents/Workspace/pitcher-zerodrive/vendor/guzzlehttp/promises/src/Promise.php(223): GuzzleHttp\\\\Promise\\\\Promise->invokeWaitFn()\",\n            \"#22 /Users/josh/Documents/Workspace/pitcher-zerodrive/vendor/guzzlehttp/promises/src/Promise.php(62): GuzzleHttp\\\\Promise\\\\Promise->waitIfPending()\",\n            \"#23 /Users/josh/Documents/Workspace/pitcher-zerodrive/models/ExtractedFile.php(75): GuzzleHttp\\\\Promise\\\\Promise->wait()\",\n            \"#24 /Users/josh/Documents/Workspace/pitcher-zerodrive/modules/api/controllers/ExtractedFileController.php(52): app\\\\models\\\\ExtractedFile->downloadTo('/private/var/tm...')\",\n            \"#25 [internal function]: app\\\\modules\\\\api\\\\controllers\\\\ExtractedFileController->actionUpload()\",\n            \"#26 /Users/josh/Documents/Workspace/pitcher-zerodrive/vendor/yiisoft/yii2/base/InlineAction.php(55): call_user_func_array(Array, Array)\",\n            \"#27 /Users/josh/Documents/Workspace/pitcher-zerodrive/vendor/yiisoft/yii2/base/Controller.php(154): yii\\\\base\\\\InlineAction->runWithParams(Array)\",\n            \"#28 /Users/josh/Documents/Workspace/pitcher-zerodrive/vendor/yiisoft/yii2/base/Module.php(454): yii\\\\base\\\\Controller->runAction('upload', Array)\",\n            \"#29 /Users/josh/Documents/Workspace/pitcher-zerodrive/vendor/yiisoft/yii2/web/Application.php(100): yii\\\\base\\\\Module->runAction('api/extracted-f...', Array)\",\n            \"#30 /Users/josh/Documents/Workspace/pitcher-zerodrive/vendor/yiisoft/yii2/base/Application.php(375): yii\\\\web\\\\Application->handleRequest(Object(yii\\\\web\\\\Request))\",\n            \"#31 /Users/josh/Documents/Workspace/pitcher-zerodrive/web/index.php(13): yii\\\\base\\\\Application->run()\",\n            \"#32 {main}\"\n        ],\n        \"type\": \"GuzzleHttp\\\\Exception\\\\ClientException\"\n    },\n    \"stack-trace\": [\n        \"#0 /Users/josh/Documents/Workspace/pitcher-zerodrive/vendor/aws/aws-sdk-php/src/WrappedHttpHandler.php(102): Aws\\\\WrappedHttpHandler->parseError(Array, Object(GuzzleHttp\\\\Psr7\\\\Request), Object(Aws\\\\Command), Array)\",\n        \"#1 /Users/josh/Documents/Workspace/pitcher-zerodrive/vendor/guzzlehttp/promises/src/Promise.php(203): Aws\\\\WrappedHttpHandler->Aws\\\\{closure}(Array)\",\n        \"#2 /Users/josh/Documents/Workspace/pitcher-zerodrive/vendor/guzzlehttp/promises/src/Promise.php(174): GuzzleHttp\\\\Promise\\\\Promise::callHandler(2, Array, Array)\",\n        \"#3 /Users/josh/Documents/Workspace/pitcher-zerodrive/vendor/guzzlehttp/promises/src/RejectedPromise.php(40): GuzzleHttp\\\\Promise\\\\Promise::GuzzleHttp\\\\Promise\\\\{closure}(Array)\",\n        \"#4 /Users/josh/Documents/Workspace/pitcher-zerodrive/vendor/guzzlehttp/promises/src/TaskQueue.php(47): GuzzleHttp\\\\Promise\\\\RejectedPromise::GuzzleHttp\\\\Promise\\\\{closure}()\",\n        \"#5 /Users/josh/Documents/Workspace/pitcher-zerodrive/vendor/guzzlehttp/guzzle/src/Handler/CurlMultiHandler.php(96): GuzzleHttp\\\\Promise\\\\TaskQueue->run()\",\n        \"#6 /Users/josh/Documents/Workspace/pitcher-zerodrive/vendor/guzzlehttp/guzzle/src/Handler/CurlMultiHandler.php(123): GuzzleHttp\\\\Handler\\\\CurlMultiHandler->tick()\",\n        \"#7 /Users/josh/Documents/Workspace/pitcher-zerodrive/vendor/guzzlehttp/promises/src/Promise.php(246): GuzzleHttp\\\\Handler\\\\CurlMultiHandler->execute(true)\",\n        \"#8 /Users/josh/Documents/Workspace/pitcher-zerodrive/vendor/guzzlehttp/promises/src/Promise.php(223): GuzzleHttp\\\\Promise\\\\Promise->invokeWaitFn()\",\n        \"#9 /Users/josh/Documents/Workspace/pitcher-zerodrive/vendor/guzzlehttp/promises/src/Promise.php(266): GuzzleHttp\\\\Promise\\\\Promise->waitIfPending()\",\n        \"#10 /Users/josh/Documents/Workspace/pitcher-zerodrive/vendor/guzzlehttp/promises/src/Promise.php(225): GuzzleHttp\\\\Promise\\\\Promise->invokeWaitList()\",\n        \"#11 /Users/josh/Documents/Workspace/pitcher-zerodrive/vendor/guzzlehttp/promises/src/Promise.php(269): GuzzleHttp\\\\Promise\\\\Promise->waitIfPending()\",\n        \"#12 /Users/josh/Documents/Workspace/pitcher-zerodrive/vendor/guzzlehttp/promises/src/Promise.php(225): GuzzleHttp\\\\Promise\\\\Promise->invokeWaitList()\",\n        \"#13 /Users/josh/Documents/Workspace/pitcher-zerodrive/vendor/guzzlehttp/promises/src/Promise.php(62): GuzzleHttp\\\\Promise\\\\Promise->waitIfPending()\",\n        \"#14 /Users/josh/Documents/Workspace/pitcher-zerodrive/vendor/guzzlehttp/promises/src/Coroutine.php(65): GuzzleHttp\\\\Promise\\\\Promise->wait()\",\n        \"#15 /Users/josh/Documents/Workspace/pitcher-zerodrive/vendor/guzzlehttp/promises/src/Promise.php(246): GuzzleHttp\\\\Promise\\\\Coroutine->GuzzleHttp\\\\Promise\\\\{closure}(true)\",\n        \"#16 /Users/josh/Documents/Workspace/pitcher-zerodrive/vendor/guzzlehttp/promises/src/Promise.php(223): GuzzleHttp\\\\Promise\\\\Promise->invokeWaitFn()\",\n        \"#17 /Users/josh/Documents/Workspace/pitcher-zerodrive/vendor/guzzlehttp/promises/src/Promise.php(266): GuzzleHttp\\\\Promise\\\\Promise->waitIfPending()\",\n        \"#18 /Users/josh/Documents/Workspace/pitcher-zerodrive/vendor/guzzlehttp/promises/src/Promise.php(225): GuzzleHttp\\\\Promise\\\\Promise->invokeWaitList()\",\n        \"#19 /Users/josh/Documents/Workspace/pitcher-zerodrive/vendor/guzzlehttp/promises/src/Promise.php(62): GuzzleHttp\\\\Promise\\\\Promise->waitIfPending()\",\n        \"#20 /Users/josh/Documents/Workspace/pitcher-zerodrive/vendor/guzzlehttp/promises/src/EachPromise.php(101): GuzzleHttp\\\\Promise\\\\Promise->wait()\",\n        \"#21 /Users/josh/Documents/Workspace/pitcher-zerodrive/vendor/guzzlehttp/promises/src/Promise.php(246): GuzzleHttp\\\\Promise\\\\EachPromise->GuzzleHttp\\\\Promise\\\\{closure}(true)\",\n        \"#22 /Users/josh/Documents/Workspace/pitcher-zerodrive/vendor/guzzlehttp/promises/src/Promise.php(223): GuzzleHttp\\\\Promise\\\\Promise->invokeWaitFn()\",\n        \"#23 /Users/josh/Documents/Workspace/pitcher-zerodrive/vendor/guzzlehttp/promises/src/Promise.php(62): GuzzleHttp\\\\Promise\\\\Promise->waitIfPending()\",\n        \"#24 /Users/josh/Documents/Workspace/pitcher-zerodrive/models/ExtractedFile.php(75): GuzzleHttp\\\\Promise\\\\Promise->wait()\",\n        \"#25 /Users/josh/Documents/Workspace/pitcher-zerodrive/modules/api/controllers/ExtractedFileController.php(52): app\\\\models\\\\ExtractedFile->downloadTo('/private/var/tm...')\",\n        \"#26 [internal function]: app\\\\modules\\\\api\\\\controllers\\\\ExtractedFileController->actionUpload()\",\n        \"#27 /Users/josh/Documents/Workspace/pitcher-zerodrive/vendor/yiisoft/yii2/base/InlineAction.php(55): call_user_func_array(Array, Array)\",\n        \"#28 /Users/josh/Documents/Workspace/pitcher-zerodrive/vendor/yiisoft/yii2/base/Controller.php(154): yii\\\\base\\\\InlineAction->runWithParams(Array)\",\n        \"#29 /Users/josh/Documents/Workspace/pitcher-zerodrive/vendor/yiisoft/yii2/base/Module.php(454): yii\\\\base\\\\Controller->runAction('upload', Array)\",\n        \"#30 /Users/josh/Documents/Workspace/pitcher-zerodrive/vendor/yiisoft/yii2/web/Application.php(100): yii\\\\base\\\\Module->runAction('api/extracted-f...', Array)\",\n        \"#31 /Users/josh/Documents/Workspace/pitcher-zerodrive/vendor/yiisoft/yii2/base/Application.php(375): yii\\\\web\\\\Application->handleRequest(Object(yii\\\\web\\\\Request))\",\n        \"#32 /Users/josh/Documents/Workspace/pitcher-zerodrive/web/index.php(13): yii\\\\base\\\\Application->run()\",\n        \"#33 {main}\"\n    ],\n    \"type\": \"Aws\\\\S3\\\\Exception\\\\S3Exception\"\n}\nAnd this is my code:\npublic function addFilesAsync($source)\n    {\n        $client = getS3Client(\\Yii::$app->params[\"s3\"][\"pitcher\"]);\n        $bucket = \\Yii::$app->params[\"s3\"][\"pitcher\"][\"bucket\"];\n        $dest = \"s3://$bucket/$this->ID/extracted/\";\n        $manager = new Transfer($client, $source, $dest);\n        return $manager->promise();\n    }\nhelpers.php\nfunction getS3Client($config, $bucket = null)\n{\n    if ($bucket) $config[\"bucket\"] = $bucket;\n    return (new \\Aws\\Sdk)->createMultiRegionS3($config);\n}\nparams.php\n's3' => [\n        'bucket' => 'pitcherzerodrive',\n        'pitcher' => [\n            'bucket' => 'pitcherzerodrive',\n            'version' => 'latest',\n            'credentials' => [\n                'key'    => '<key>',\n                'secret' => '<secret>',\n            ],\n            'http'    => [\n                'verify' => false\n            ]\n        ]\nthe file on S3: \n\nsdk version:\naws/aws-sdk-php              3.19.32 AWS SDK for PHP - Use Amazon Web Services in your PHP project. the url encode hack I posted first doesn't work, because the downloaded file names are url encoded. a diff of both folders (SVf3TT = downloaded, before = original):\nOnly in SVf3TT/assets/player: close_hi%402x.png\nOnly in before/assets/player: close_hi@2x.png\nOnly in SVf3TT/assets/player: close_n%402x.png\nOnly in before/assets/player: close_n@2x.png\nOnly in SVf3TT/assets/player: close_p%402x.png\nOnly in before/assets/player: close_p@2x.png. I realised it encoded the @ twice. @ -> %40 -> %2540. Hope that helps. New quick and dirty fix that works for me:\n```\nS3/Tansfer.php Line 284\n/* @return Iterator /\nprivate function getDownloadsIterator()\n{\n    if (is_string($this->source)) {\n        $listArgs = $this->getS3Args($this->sourceMetadata['path']);\n        if (isset($listArgs['Key'])) {\n            $listArgs['Prefix'] = $listArgs['Key'] . '/';\n            unset($listArgs['Key']);\n        }\n    $files = $this->client\n        ->getPaginator('ListObjects', $listArgs)\n        ->search('Contents[].Key');\n    $files = Aws\\map($files, function ($key) use ($listArgs) {\n        return \"s3://{$listArgs['Bucket']}/\" . urldecode($key);\n    });\n    return Aws\\filter($files, function ($key) {\n        return substr($key, -1, 1) !== '/';\n    });\n}\n\nreturn $this->source;\n\n}\n```\nThe cause of the issue is the following returns a list of  encoded instead of decoded urls:\nS3/Tansfer.php Line 294\n$files = $this->client\n                ->getPaginator('ListObjects', $listArgs)\n                ->search('Contents[].Key');. ",
    "jlestel": "I still see differences between files in Guzzle last version (https://github.com/guzzle/guzzle) and your last release (https://github.com/aws/aws-sdk-php/releases/tag/3.3.4).\n. Ok @jeskew !\nThank's for the quick reply.\n. ",
    "donatj": "Do the parameters to the methods ever change based on API versions? If not, you get a description on @method docs and I would see throwing a simple API v2 only or API v3+ only or such in the description as sufficent.\nWhile a PhpStorm plugin would be nice, it doesn't help out with tools we use to analyze PRs like Scrutinizer.\n. Yup ;)\n\n. ",
    "lonormaly": "Latest - 3.3.4 \n. Silently I guess... I don't get any notification about its success or failure; I waited more than an hour and checked at the target path and the file wasn't there so I assumed it failed.\nDid you try it on large files (I tried on 18GB file)\n. It didn't return anything. \nI waited long time.\nShould the operation take few ms or should I expect to wait minutes / (hours?)\n. I'm aware of that, still what I described occurred repeatedly...\nDid you try to rename large files yourself?\nOn Friday, September 11, 2015, Jonathan Eskew notifications@github.com\nwrote:\n\nIt's a blocking operation that always returns a boolean value. On a file\nsystem, rename operations can fail for a number of reasons (permissions,\nnonexistent destination directories, etc), which will cause rename to return\nfalse http://php.net/manual/en/function.rename.php.\nOnce the function returns, the operation will have either succeeded or\nfailed. You wouldn't have to wait.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/aws/aws-sdk-php/issues/759#issuecomment-139417708.\n\n\nThanks\n       Shai\n. Thanks for that.\nHow can I trace it?\nOn Friday, September 11, 2015, Jonathan Eskew notifications@github.com\nwrote:\n\nI did. I'm asking to figure out where the error originated. If the\noperation failed without throwing any warnings and without returning a\nvalue, that would mean that there's an issue with the stream wrapper. If it\ndid register a warning and return false, that would indicate an issue with\nthe S3Client.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/aws/aws-sdk-php/issues/759#issuecomment-139449407.\n\n\nThanks\n       Shai\n. Now I get false on the response... but how can I get more info? The php_error.log from MAMP folder doesn't say anything related.\nAnother interesting issue - \"copy\" command never returns a response...\nAlso, just to remind that smaller files get renamed easily.\n. So sorry but it didn't fix it :(\nThanks for the efforts. Do you know on what version did it work for sure?\n. You're awesome! thank you so much dear friend :)\n. How can I get more info about the reason the rename failed? currently the response is only boolean right?\n. Unfortunately the fix didn't fix that error for us. tried to rename file with size 16.5GB. If you want i can get you a specific file\n. Hey, thanks for that!\nThe gist is for Multipart upload not use of StreamWrapper with rename command\n. This is more than great! When will the PR be merged? Can't wait to test it :)\n. Please! This one is really important for us :+1: \n. Thank you!\nI can't see how this answers my question. Using before_upload I can determine which part is being uploaded but not the overall progress of the upload. We do use concurrency so this is no real indication of the current progress of the upload.\n. Actually nothing in $tempSource was wrong. In fact when using MultipartCopy it works perfectly.\nI would suggest to check this one more thoroughly because this is a big deal if this operation cuts one's file for no reason.\n. I'm not sure about the encoding, currently I have no control over the name of the files\n. Hey, sorry for not responding.\nUnfortunately this issue is being reproduced time and time again.\nFile name is Beyonce\u0301.jpg Other files the function works perfectly...\nHere is agist of our function and exception logs:\nhttps://gist.github.com/lonormaly/60b5fb5a71f8e27194e142f810941b82\nThank you!\n. true\n. array(59) {\n  [0]=>\n  string(4) \"Core\"\n  [1]=>\n  string(4) \"date\"\n  [2]=>\n  string(4) \"ereg\"\n  [3]=>\n  string(6) \"libxml\"\n  [4]=>\n  string(7) \"openssl\"\n  [5]=>\n  string(4) \"pcre\"\n  [6]=>\n  string(4) \"zlib\"\n  [7]=>\n  string(6) \"bcmath\"\n  [8]=>\n  string(3) \"bz2\"\n  [9]=>\n  string(8) \"calendar\"\n  [10]=>\n  string(5) \"ctype\"\n  [11]=>\n  string(3) \"dba\"\n  [12]=>\n  string(3) \"dom\"\n  [13]=>\n  string(4) \"hash\"\n  [14]=>\n  string(8) \"fileinfo\"\n  [15]=>\n  string(6) \"filter\"\n  [16]=>\n  string(3) \"ftp\"\n  [17]=>\n  string(7) \"gettext\"\n  [18]=>\n  string(3) \"SPL\"\n  [19]=>\n  string(5) \"iconv\"\n  [20]=>\n  string(8) \"mbstring\"\n  [21]=>\n  string(5) \"pcntl\"\n  [22]=>\n  string(7) \"session\"\n  [23]=>\n  string(5) \"posix\"\n  [24]=>\n  string(10) \"Reflection\"\n  [25]=>\n  string(8) \"standard\"\n  [26]=>\n  string(5) \"shmop\"\n  [27]=>\n  string(9) \"SimpleXML\"\n  [28]=>\n  string(4) \"soap\"\n  [29]=>\n  string(7) \"sockets\"\n  [30]=>\n  string(4) \"Phar\"\n  [31]=>\n  string(4) \"exif\"\n  [32]=>\n  string(7) \"sysvmsg\"\n  [33]=>\n  string(7) \"sysvsem\"\n  [34]=>\n  string(7) \"sysvshm\"\n  [35]=>\n  string(9) \"tokenizer\"\n  [36]=>\n  string(4) \"wddx\"\n  [37]=>\n  string(3) \"xml\"\n  [38]=>\n  string(9) \"xmlreader\"\n  [39]=>\n  string(9) \"xmlwriter\"\n  [40]=>\n  string(3) \"zip\"\n  [41]=>\n  string(3) \"PDO\"\n  [42]=>\n  string(4) \"curl\"\n  [43]=>\n  string(2) \"gd\"\n  [44]=>\n  string(4) \"json\"\n  [45]=>\n  string(6) \"mcrypt\"\n  [46]=>\n  string(8) \"memcache\"\n  [47]=>\n  string(5) \"mysql\"\n  [48]=>\n  string(6) \"mysqli\"\n  [49]=>\n  string(9) \"pdo_mysql\"\n  [50]=>\n  string(10) \"pdo_sqlite\"\n  [51]=>\n  string(8) \"readline\"\n  [52]=>\n  string(4) \"sasl\"\n  [53]=>\n  string(7) \"sqlite3\"\n  [54]=>\n  string(6) \"xmlrpc\"\n  [55]=>\n  string(3) \"xsl\"\n  [56]=>\n  string(8) \"newrelic\"\n  [57]=>\n  string(5) \"mhash\"\n  [58]=>\n  string(12) \"Zend OPcache\"\n}\n. Nope. Did you use the function in the gist to copy the file?\nOn Friday, 22 April 2016, Jonathan Eskew notifications@github.com wrote:\n\nI'm still unable to reproduce. Copying a file with the exact name provided\nworks fine for me. Is there a particular version of PHP you're seeing this\nerror on?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly or view it on GitHub\nhttps://github.com/aws/aws-sdk-php/issues/933#issuecomment-213161963\n\n\nThanks\n       Shai\n. PHP 5.5.9-1ubuntu4.14 (cli) (built: Oct 28 2015 01:34:46)\nCopyright (c) 1997-2014 The PHP Group\nZend Engine v2.5.0, Copyright (c) 1998-2014 Zend Technologies\n    with Zend OPcache v7.0.3, Copyright (c) 1999-2014, by Zend Technologies\n. We're not doing anything special in the PHP installation.\nplease try to copy a file named \"Beyonce\u0301.jpg\" using the specific gist function I attached and let me know wether you're able to copy the file or not from one bucket to another.\nWe really need this function to work. We have received a 'true' for the comparison you asked on.\nWe're using SDK version 3.9.2\nI'm pretty sure it has to do with a. the way we use the API or b. a bug with the SDK.\nReally appreciate the efforts!\n. It will be really helpful if you could provide the exact code I need to\nalter, if you don't mind. Thanks so much\nOn Sat, Apr 23, 2016 at 6:18 PM, Jonathan Eskew notifications@github.com\nwrote:\n\nAh, I see that you're directly using the multipart copier. You will need\nto URL encode the source key. This is something that Aws\\S3\\S3Client::copy\nor an instance of Aws\\S3\\ObjectCopier would take care of for you.\nThe multipart copier and uploader take the source as a string and rely on\nthe user (or a higher level of abstraction in the SDK) to take care of\nencoding. Altering this interface would be a breaking change.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly or view it on GitHub\nhttps://github.com/aws/aws-sdk-php/issues/933#issuecomment-213757026\n\n\nThanks\n       Shai\n. Thanks for that, wrapping the $tempSource with rawurlencode returns error from the MultipartCopy function: \n\n\nWe really need the Multipart capabilities, does the ordinary copy implements the multipart issue as well? We need to support really big files (Up to 5TB)\n. We tried to wrap only the filename with rawurlencode and we tried to wrap the key part as you suggested and we experienced no success...\n1. When we tried to encode only the filename we got this error:\n   2016-04-25 12:43:39 Error: [Aws\\S3\\Exception\\S3Exception] Error executing \"HeadObject\" on \"https://s3.amazonaws.com//Local/Artworks/Artwork1812/us-east-1%3Add906849-5f27-4ca4-bbf7-56fbacda93c3/Beyonce%25CC%2581.jpg\"; AWS HTTP error: Client error: 404 NotFound (client): 404 Not Found (Request-ID: BF7FEDF0952042E6) \n   Stack Trace:\n   #0 /Users/shaisnir/Development/Niio/Server/app/vendor/aws/aws-sdk-php/src/WrappedHttpHandler.php(76): Aws\\WrappedHttpHandler->parseError(Array, Object(GuzzleHttp\\Psr7\\Request), Object(Aws\\Command))\n   #1 /Users/shaisnir/Development/Niio/Server/app/vendor/guzzlehttp/promises/src/Promise.php(199): Aws\\WrappedHttpHandler->Aws{closure}(Array)\n   #2 /Users/shaisnir/Development/Niio/Server/app/vendor/guzzlehttp/promises/src/Promise.php(170): GuzzleHttp\\Promise\\Promise::callHandler(2, Array, Array)\n   #3 /Users/shaisnir/Development/Niio/Server/app/vendor/guzzlehttp/promises/src/RejectedPromise.php(40): GuzzleHttp\\Promise\\Promise::GuzzleHttp\\Promise{closure}(Array)\n   #4 /Users/shaisnir/Development/Niio/Server/app/vendor/guzzlehttp/promises/src/TaskQueue.php(60): GuzzleHttp\\Promise\\RejectedPromise::GuzzleHttp\\Promise{closure}()\n   #5 /Users/shaisnir/Development/Niio/Server/app/vendor/guzzlehttp/guzzle/src/Handler/CurlMultiHandler.php(96): GuzzleHttp\\Promise\\TaskQueue->run()\n   #6 /Users/shaisnir/Development/Niio/Server/app/vendor/guzzlehttp/guzzle/src/Handler/CurlMultiHandler.php(123): GuzzleHttp\\Handler\\CurlMultiHandler->tick()\n   #7 /Users/shaisnir/Development/Niio/Server/app/vendor/guzzlehttp/promises/src/Promise.php(240): GuzzleHttp\\Handler\\CurlMultiHandler->execute(true)\n   #8 /Users/shaisnir/Development/Niio/Server/app/vendor/guzzlehttp/promises/src/Promise.php(217): GuzzleHttp\\Promise\\Promise->invokeWaitFn()\n   #9 /Users/shaisnir/Development/Niio/Server/app/vendor/guzzlehttp/promises/src/Promise.php(261): GuzzleHttp\\Promise\\Promise->waitIfPending()\n   #10 /Users/shaisnir/Development/Niio/Server/app/vendor/guzzlehttp/promises/src/Promise.php(219): GuzzleHttp\\Promise\\Promise->invokeWaitList()\n   #11 /Users/shaisnir/Development/Niio/Server/app/vendor/guzzlehttp/promises/src/Promise.php(261): GuzzleHttp\\Promise\\Promise->waitIfPending()\n   #12 /Users/shaisnir/Development/Niio/Server/app/vendor/guzzlehttp/promises/src/Promise.php(219): GuzzleHttp\\Promise\\Promise->invokeWaitList()\n   #13 /Users/shaisnir/Development/Niio/Server/app/vendor/guzzlehttp/promises/src/Promise.php(62): GuzzleHttp\\Promise\\Promise->waitIfPending()\n   #14 /Users/shaisnir/Development/Niio/Server/app/vendor/aws/aws-sdk-php/src/AwsClient.php(202): GuzzleHttp\\Promise\\Promise->wait()\n   #15 /Users/shaisnir/Development/Niio/Server/app/vendor/aws/aws-sdk-php/src/AwsClient.php(167): Aws\\AwsClient->execute(Object(Aws\\Command))\n   #16 /Users/shaisnir/Development/Niio/Server/app/vendor/aws/aws-sdk-php/src/S3/MultipartCopy.php(165): Aws\\AwsClient->__call('headObject', Array)\n   #17 /Users/shaisnir/Development/Niio/Server/app/vendor/aws/aws-sdk-php/src/S3/MultipartCopy.php(165): Aws\\S3\\S3Client->headObject(Array)\n   #18 /Users/shaisnir/Development/Niio/Server/app/vendor/aws/aws-sdk-php/src/S3/MultipartCopy.php(140): Aws\\S3\\MultipartCopy->fetchSourceMetadata()\n   #19 /Users/shaisnir/Development/Niio/Server/app/vendor/aws/aws-sdk-php/src/S3/MultipartCopy.php(134): Aws\\S3\\MultipartCopy->getSourceMetadata()\n   #20 /Users/shaisnir/Development/Niio/Server/app/vendor/aws/aws-sdk-php/src/S3/MultipartUploadingTrait.php(75): Aws\\S3\\MultipartCopy->getSourceSize()\n   #21 /Users/shaisnir/Development/Niio/Server/app/vendor/aws/aws-sdk-php/src/Multipart/AbstractUploadManager.php(220): Aws\\S3\\MultipartCopy->determinePartSize()\n   #22 /Users/shaisnir/Development/Niio/Server/app/vendor/aws/aws-sdk-php/src/Multipart/AbstractUploadManager.php(60): Aws\\Multipart\\AbstractUploadManager->determineState()\n   #23 /Users/shaisnir/Development/Niio/Server/app/vendor/aws/aws-sdk-php/src/S3/MultipartCopy.php(57): Aws\\Multipart\\AbstractUploadManager->__construct(Object(Aws\\S3\\S3Client), Array)\n2. When we tried to encode the whole key, without the bucket, we got the same error:\n   2016-04-25 12:46:04 Error: [Aws\\S3\\Exception\\S3Exception] Error executing \"HeadObject\" on \"https://s3.amazonaws.com//Local%252FArtworks%252FArtwork1813%252Fus-east-1%253Add906849-5f27-4ca4-bbf7-56fbacda93c3%252FBeyonce%25CC%2581.jpg\"; AWS HTTP error: Client error: 404 NotFound (client): 404 Not Found (Request-ID: E3700F6340C48952)\nStack Trace:\n0 /Users/shaisnir/Development/Niio/Server/app/vendor/aws/aws-sdk-php/src/WrappedHttpHandler.php(76): Aws\\WrappedHttpHandler->parseError(Array, Object(GuzzleHttp\\Psr7\\Request), Object(Aws\\Command))\n1 /Users/shaisnir/Development/Niio/Server/app/vendor/guzzlehttp/promises/src/Promise.php(199): Aws\\WrappedHttpHandler->Aws{closure}(Array)\n2 /Users/shaisnir/Development/Niio/Server/app/vendor/guzzlehttp/promises/src/Promise.php(170): GuzzleHttp\\Promise\\Promise::callHandler(2, Array, Array)\n3 /Users/shaisnir/Development/Niio/Server/app/vendor/guzzlehttp/promises/src/RejectedPromise.php(40): GuzzleHttp\\Promise\\Promise::GuzzleHttp\\Promise{closure}(Array)\n4 /Users/shaisnir/Development/Niio/Server/app/vendor/guzzlehttp/promises/src/TaskQueue.php(60): GuzzleHttp\\Promise\\RejectedPromise::GuzzleHttp\\Promise{closure}()\n5 /Users/shaisnir/Development/Niio/Server/app/vendor/guzzlehttp/guzzle/src/Handler/CurlMultiHandler.php(96): GuzzleHttp\\Promise\\TaskQueue->run()\n6 /Users/shaisnir/Development/Niio/Server/app/vendor/guzzlehttp/guzzle/src/Handler/CurlMultiHandler.php(123): GuzzleHttp\\Handler\\CurlMultiHandler->tick()\n7 /Users/shaisnir/Development/Niio/Server/app/vendor/guzzlehttp/promises/src/Promise.php(240): GuzzleHttp\\Handler\\CurlMultiHandler->execute(true)\n8 /Users/shaisnir/Development/Niio/Server/app/vendor/guzzlehttp/promises/src/Promise.php(217): GuzzleHttp\\Promise\\Promise->invokeWaitFn()\n9 /Users/shaisnir/Development/Niio/Server/app/vendor/guzzlehttp/promises/src/Promise.php(261): GuzzleHttp\\Promise\\Promise->waitIfPending()\n10 /Users/shaisnir/Development/Niio/Server/app/vendor/guzzlehttp/promises/src/Promise.php(219): GuzzleHttp\\Promise\\Promise->invokeWaitList()\n11 /Users/shaisnir/Development/Niio/Server/app/vendor/guzzlehttp/promises/src/Promise.php(261): GuzzleHttp\\Promise\\Promise->waitIfPending()\n12 /Users/shaisnir/Development/Niio/Server/app/vendor/guzzlehttp/promises/src/Promise.php(219): GuzzleHttp\\Promise\\Promise->invokeWaitList()\n13 /Users/shaisnir/Development/Niio/Server/app/vendor/guzzlehttp/promises/src/Promise.php(62): GuzzleHttp\\Promise\\Promise->waitIfPending()\n14 /Users/shaisnir/Development/Niio/Server/app/vendor/aws/aws-sdk-php/src/AwsClient.php(202): GuzzleHttp\\Promise\\Promise->wait()\n15 /Users/shaisnir/Development/Niio/Server/app/vendor/aws/aws-sdk-php/src/AwsClient.php(167): Aws\\AwsClient->execute(Object(Aws\\Command))\n16 /Users/shaisnir/Development/Niio/Server/app/vendor/aws/aws-sdk-php/src/S3/MultipartCopy.php(165): Aws\\AwsClient->__call('headObject', Array)\n17 /Users/shaisnir/Development/Niio/Server/app/vendor/aws/aws-sdk-php/src/S3/MultipartCopy.php(165): Aws\\S3\\S3Client->headObject(Array)\n18 /Users/shaisnir/Development/Niio/Server/app/vendor/aws/aws-sdk-php/src/S3/MultipartCopy.php(140): Aws\\S3\\MultipartCopy->fetchSourceMetadata()\n19 /Users/shaisnir/Development/Niio/Server/app/vendor/aws/aws-sdk-php/src/S3/MultipartCopy.php(134): Aws\\S3\\MultipartCopy->getSourceMetadata()\n20 /Users/shaisnir/Development/Niio/Server/app/vendor/aws/aws-sdk-php/src/S3/MultipartUploadingTrait.php(75): Aws\\S3\\MultipartCopy->getSourceSize()\n21 /Users/shaisnir/Development/Niio/Server/app/vendor/aws/aws-sdk-php/src/Multipart/AbstractUploadManager.php(220): Aws\\S3\\MultipartCopy->determinePartSize()\n22 /Users/shaisnir/Development/Niio/Server/app/vendor/aws/aws-sdk-php/src/Multipart/AbstractUploadManager.php(60): Aws\\Multipart\\AbstractUploadManager->determineState()\n23 /Users/shaisnir/Development/Niio/Server/app/vendor/aws/aws-sdk-php/src/S3/MultipartCopy.php(57): Aws\\Multipart\\AbstractUploadManager->__construct(Object(Aws\\S3\\S3Client), Array)\n24 /Users/shaisnir/Development/Niio/Server/app/Controller/AppController.php(358): Aws\\S3\\MultipartCopy->__construct(Object(Aws\\S3\\S3Client), '/niio.temp.medi...', Array)\n\nThe main reason why we didn't chose Copy over MultipartCopy is its ability to retry and resume and retain its progress, is that possible with the regular copy?\n. But we can't use multipart copy with the encoding solution you suggested. Using copy will demand a major change in the flow and it's a sensitive part on our code, also the copy function doesn't support resume from same place.\nDo you have some suggestion on how to properly use the encode solution you suggested? Why does MultipartCopy doesn't support encoding just like the Copy function? \n. \n",
    "fkizewski": "Sorry, i forget to put it: 3.3.5\n. Hi Jonathan.\nThanks to try to help me.\nI've just find my error: because it's a stream, it's necessary to rewind the stream into the exception! So now all is ok.\nPlease close this issue.\nBest regards, Fabrice\n. This is a good idea to be more clear :)\n. ",
    "liquorvicar": "Ok, I have resolved the immediate issue by adding the IAM account I was using to a group we had set up. But that still doesn't really explain why it wasn't working via the PHP SDK as without that change it was working via the python CLI tool.\n. I set up the credentials using the python CLI client (as per here http://docs.aws.amazon.com/cli/latest/userguide/cli-chap-getting-started.html). I understand that has stored my creds in ini files in my home dir. I am running the python CLI client and my test php script from my local machine using the same user. One is working and one isn't.\n. @jeskew That was it. Thanks for your help.\n. ",
    "jimmaay": "Thanks for the response. I was using https before and switched to http a week ago to see if that would help. I will try increasing the retries and let you know how that goes.\n. Increasing retries to 11 actually doubled the daily amount of errors. Instead of curl errors now it's returning 500 errors:\n```\nPHP Fatal error:  Uncaught exception 'Aws\\S3\\Exception\\S3Exception' with message 'Error executing \"PutObject\" on \"http://s3.amazonaws.com/mybucket/sdsadadster.jpg\"; AWS HTTP error: Server error: 500 InternalError (server): We encountered an internal error. Please try again. - <?xml version=\"1.0\" encoding=\"UTF-8\"?>\nInternalErrorWe encountered an internal error. Please try again.0021CF042749BE89jhdJOv7Iv4203YhvFGiTMu1AJs7xW5t+N/MOaMiaYBgrBmQbqHUoMYHGtBpfr9MYAkRNJSkHVM8='\nexception 'GuzzleHttp\\Exception\\ServerException' with message 'Server error: 500' in /home/public_html/composer/vendor/guzzlehttp/guzzle/src/Middleware.php:68\nStack trace:\n0 /home/public_html/composer/vendor/guzzlehttp/promises/src/Promise.php(199): GuzzleHttp\\Middleware::GuzzleHttp{closure}(Object(GuzzleHttp\\Psr7\\Response))\n1 /home/public_html/composer/vendor/guzzlehttp/promises/src/Promise.php(152): GuzzleHttp\\Promi in /home/public_html/composer/vendor/aws/aws-sdk-php/src/WrappedHttpHandler.php on line 152\n```\n. Searching into 500 errors brings me to the \"Handling Errors\" section of https://aws.amazon.com/articles/1904 and they recommend exponential backoff. Is this built into the PHP SDK? I would have thought they would have figured it out after 7 years with S3, my daily peak is only around 20 requests/second.\n. My failing payloads are mostly less than 1 MB but can occasionally go over to a max of 2MB.\n. What is the best way to aggressively retry uploads? I am fine if the upload takes a minute even. Is there an option to increase the exponential backoff timing without changing the library?\n. I'll try increasing retries to 50. But after I reset retries back to default yesterday the error rate came back to normal. I think retrying aggressively may put more load on the the S3 servers and in return give me more errors?\n. You said it was up to x number of retries, this just means that if it's successful then it won't retry or are there other breaks on retries?\n. When I upped the retries to 11 Amazon's servers returned 500 (We encountered an internal error. Please try again.) errors as responses, so it's for sure not my servers in that case.\n. Okay I will bring it up on the S3 forums and their startup support. I was hoping there would be a simple programmatic solution as it's only 1-10 errors a day out of thousands of requests. The retries thing seemed like it would work. Would building a replicatable PHP test case for developers to test help with this? It seems to be a problem either with not enough backoff or something related to retries. A simple case would be to create a script that uploads 100-200 random sized text blocks between 100kb-200kb per second, I'm sure there will be 500 errors.\n. Yes I read it is not unusual (unfortunately) to get 500 errors from S3. I just wish the SDK's exponential back-off retries would stop any errors like they said it would.\n. This seems like it will fix it. I will implement and let you know how it goes. Thanks for the help.\n. That worked. No more errors. Thanks again for the help!\n. ",
    "sashless": "In addition i tried to limit the retries ( default is 3 from viewing the docs ) but that has no result..\n'credentials' => [\n            'key' => 'xx',\n            'secret' => 'xx',\n        ],\n        'region' => 'us-east-1',\n        'version' => '2006-03-01',\n        'http'    => [\n            'connect_timeout' => 5\n        ],\n        'retries' => 1,\n        'debug' => [\n            'logfn'        => function ($msg) { echo $msg . \"\\n\"; },\n        ]\n. Awesome,  thanks for working on that! \nWhat makes me wondering is why this particular file fails to upload with 'Could not read from stream..'  where it works without that error using the old Sdk. \nShould I create another issue? \n. The file in law is https://mozilla.github.io/pdf.js/web/compressed.tracemonkey-pldi-09.pdf\nand uploaded like \n``` php\n$httpClient = new \\GuzzleHttp\\Client();\n$commandConfig = [\n    'Bucket' => 'xx',\n    'Key' => 'xx',\n    'Body' => $httpClient->get($pdfUrl)->getBody(),\n    'ACL' => 'xx'\n];\n$command = $this->s3Client->getCommand('PutObject', $commandConfig);\n$pool = new CommandPool($client, [$command], [\n    'fulfilled' => function (ResultInterface $result) use (&$results) {\n        $this->onUploadSucces($result, $results);\n    },\n    'rejected' => function (AwsException $reason) use (&$results) {\n        $this->onUploadError($reason, $results);\n    },\n]);\n$promise = $pool->promise();\n$promise->wait();\n```\nUploading the same using  version 2.* + guzzle 3.8 was working fine without the error Could not read from stream..\n. Hey @jeskew have you seen my comment above? \n. Ah yea, of course. Thank you!\nJonathan Eskew notifications@github.com schrieb am Fr., 25. Sep. 2015\n00:44:\n\nI did. I think the best the SDK can do in that case is to limit the number\nof retries to what's been configured. I was unable to reproduce the issue\nwith the link you provided, but you should open an issue on Guzzle if you\nroutinely encounter errors trying to download something with a Guzzle\nclient.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/aws/aws-sdk-php/issues/768#issuecomment-143068842.\n. \n",
    "till": "@jeskew Neat, that looks like it could do the trick.\n. ",
    "thenetimp": "I am not using ~/.aws/config  We have an IAM role set up.  There are no actual credentials set up on the server.  The server is setup to get temporary credentials from IAM roles.\n. The IAM policy assigned to the role assigned to server sets the region as 'us-east-1'  and is suppose to be one of the params returned from config.php.   config.php is returning no acceskey, no secret and no region. The issue is not that I am not passing the region, the issue is that the policy/role credentials and region are not being picked up by the code from the php library.\n. My understanding is that IAM/Roles allows me to use aws services from an EC2 server without having to define the key and secret and region.  That using the code in config.php somehow gets that information from AWS, but since I have never done this before, I do not know 100% what to expect from the config.php code as it was taken from this aws blog post.\nhttps://blogs.aws.amazon.com/php/post/Tx1F82CR0ANO3ZI/Providing-credentials-to-the-AWS-SDK-for-PHP\nSo from what is posted here it seems to me that I do not have to define the region, that it would do it for me. \n. I see, thank you.  The documentation was not clear on that imo.  Adding the region to that array resolved the issue.  After your earlier suggestion I had tried adding it other places with failure.\n. ",
    "femans": "Hi Jeskew, honestly, I think you are right. I didn\u00b4t think about it too much because I used the mapping code of mkdir, which I assumed to be right. But is that then sound, or should this too be different? Because in that case the mapping function needs to be updated.\nI wouldn\u00b4t mind improving the mapping and commit it. \nI will look in to it tomorrow, when I have a bit of time, but if in the meanwhile you have a suggestion on better mapping, this would be welcome.\n. Ok I examined the code, and it appears indeed at least odd to me how the permissions are mapped, if not a minor security issue. I don\u00b4t know how I overlooked this to begin with, except lack of time. I only used chmod to set public-read files. I think this should be changed as well for mkdir.\nMy suggestion would be to map the following canned acl\u00b4s:\npublic-read: the third digit is 4 or 5\npublic-read-write: the third digit is 6 or 7\nprivate: the rest\nthe rest don\u00b4t get mapped. The second digit is omitted. Its a little rudimentary, but I guess more accurate than allowing public-read for 7xx, and authenticated-read for 6xx.\nI will write the code tomorrow-ish and submit another pull request. \n. So if the suggested mapping is agreed, I will build it in this branch.\n. I added the alternative mapping function for chmod, next to the one for mkdir. It isn\u00b4t very clean to have two different mapping functions for no apparent reason, but at least mkdir functionality is not compromised.\n. Hi Michael, I totally agree that octal permissions don\u00b4t map well.\nHowever I don\u00b4t have the experience that the current behaviour is returning false, because I simply got a not implemented error, breaking my code, which costed me ample time debugging (which brought me to the idea of implementing it). For a large code base this makes it more complicated to use it as a drop in library, for which I think it was designed; otherwise I could just as well write my own wrapper to do filestuff, (which was my other idea). \nI think it would be more prudent to have it at least available, returning false in some or all cases, or the default acl, or do something useful, maybe depending stream context options. I agree with Jonathan on this, I also like the idea of support for chmod. \nBesides this, the mapping I designed is less confusing than the one in place for mkdir, (no offense, please). Because if you do mkdir('foo/bar', 0700), you don\u00b4t expect it to be publicly readable (now it is), but with 0655 you do (now it isn\u00b4t). However since S3 doesn\u00b4t really work with directories I don\u00b4t regard this as a big issue, although I didn\u00b4t really test it through.\n. Hi, I think it would be an improvement over a non-implemented. And there is obviously no perfect mapping possible, I agree.\nHowever, for us I don\u00b4t see an alternative to either keep using my fork, or creating my own streamwrapper which would be a spinoff of the one in the sdk, because there are too many private methods and variables to be able to simply subclass it. Both options are preferred to changing the aws settings in the code, because I want the codebase to stay filesystem agnostic. In the meanwhile both options are suboptimal, because staying up to date with the official sdk requires merging every time. But that is ok.\n. As I said, I would like the code to remain as agnostic as possible of the filesystem. This makes it easier to deploy across different types of servers. \nThis means using the common operations to do common things, most preferably leaving the code in tact that was there while migrating to s3, and not have a filesystem specific handler; otherwise I have to make a wrapper for this that can switch between different handles for different filesystems. In which case using my own fork is simply easier at this point. \nBut I will look into the option of wrapping it in a different way as you suggest. Thanks!\n. Thanks Jonathan for the suggestions, and looking into it.\nOn Thu, Oct 8, 2015 at 5:35 PM, Jonathan Eskew notifications@github.com\nwrote:\n\nClosed #776 https://github.com/aws/aws-sdk-php/pull/776.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/aws/aws-sdk-php/pull/776#event-430448661.\n. If you agree with this mapping, I can add it to the chmod branch using a different function name for the mapping method, this is not too much work. I can do this easily tomorrow. Let\u00b4s discuss this further in that pull request, before this gets more confusing :D\nI can understand you are hesitant to change mkdir mapping in the same version, and I don\u00b4t immediately see how to build in this opt-in mechanism, so this is something which you could pick up, yes. Then I leave this pull request to you to decide what you want to do with it.\n. \n",
    "maugar": "With the change I made, the session handler works as expected. New sessions are created the first time, and successfully reopen, from the second request.\nI can confirm that when open returns false, session_start fails with the fatal error reported.\n. No, with my modification (open function always returning true) the call to session_start never fails and the session handling works as expected. \n. New sessions are saved with the correct format PHPSESSID_0b565957e3977d905a194bedbaacedb7\nI suppose the session ID of a new session is generated (internally by HHVM?) after the call to open function.\nMy first attempt to fix this issue was the following:\n```\npublic function open($savePath, $sessionName)\n{\n    $this->savePath = $savePath;\n    $this->sessionName = $sessionName;\n    $this->openSessionId = session_id();\n// Create a new session id if session_id() returns \"\"\nif (!((bool) $this->openSessionId)) {\n    $this->openSessionId = session_regenerate_id();\n}\nreturn true;\n\n}\n```\nBut then I tried just returning true, without generating a new session ID, and it was working as well.\n. After further tests, I realised that session_regenerate_id() does not generate a new session ID unless a valid session ID already exists. Therefore, if session_id() returns '', also session_regenerate_id() will return ''.\nThe following version of open() works despite the fact that the assignment $this->openSessionId =session_id() is skipped.\nphp\n public function open($savePath, $sessionName){\n    $this->savePath = $savePath;\n    $this->sessionName = $sessionName;\n    return true;\n}\nI also took a look at the test https://github.com/maugar/aws-sdk-php/blob/master/tests/DynamoDb/SessionHandlerTest.php. Since the actual connection with the DynamoDb is mocked, I am not sure how to write a meaningful test for the modification I'd like to propose.\n. ",
    "Lansoweb": "I made some tests with PHP 7.0.0RC6 today and it has the same issue:\nFailed to initialize storage module: user (path: )\nAnd using the return true fixed it too.\n. @jeskew v2 ... will make the same test with v3.\n. @jeskew Updated my project to V3. then opened the browser i get:\nFatal error: session_start(): Failed to initialize storage module: user (path: /tmp)\nIf I put the \"return true\" by the end on open method the session works.\n. @jeskew Glad to help. Great work btw!\n. @jeskew Thanks!!\n. ",
    "Cranke": "Its working now, can't reproduce, what i did.\nWill update here if I remember.\n. ",
    "amitchhajer": "Having same issue. It is giving me error only when using from an aws server. Works well when uploading from my local machine.\n. hey, tried setting up the log plugin but getting below error\n1. installed https://packagist.org/packages/guzzle/plugin-log and added \nuse Guzzle\\Plugin\\Log\\LogPlugin;\n2. $s3Client = S3Client::factory(array(\n            'credentials' => array(\n                'key'    => $this->s3Key,\n                'secret' => $this->s3Secret\n            ),\n            'region' => 'ap-southeast-1',\n            'scheme' => 'https',\n            'version' => 'latest'\n        ));\n$s3Client->addSubscriber(LogPlugin::getDebugPlugin());\ngives: \n\"Catchable Fatal Error: Argument 2 passed to Aws\\\\AwsClient::getCommand() must be of the type array, object given, called in /home/vagrant/www/shifoo/vendor/aws/aws-sdk-php/src/AwsClient.php on line 167 and defined in /home/vagrant/www/shifoo/vendor/aws/aws-sdk-php/src/AwsClient.php line 211\"\n. here it is:\n``\n-> Entering step init, name 's3.ssec'\ncommand was set to array(3) {\n    [\"instance\"]=>\n    string(32) \"0000000064a40cfc000000004c9763f1\"\n    [\"name\"]=>\n    string(9) \"PutObject\"\n    [\"params\"]=>\n    array(5) {\n      [\"Bucket\"]=>\n      string(25) \"prod-user-uploaded-images\"\n      [\"Key\"]=>\n      string(61) \"MENU-ITEM-IMAGES/7ebdc1fd4ef261154d5dddbc958dcb48534b6d27.png\"\n      [\"Body\"]=>\n      resource(11) of type (stream)\n      [\"ACL\"]=>\n      string(11) \"public-read\"\n      [\"@http\"]=>\n      array(1) {\n        [\"debug\"]=>\n        resource(12) of type (stream)\n      }\n    }\n  }\nrequest was set to array(0) {\n  }\n-> Entering step init, name 's3.source_file'\nno changes\n-> Entering step init, name 's3.save_as'\nno changes\n-> Entering step init, name 's3.location'\nno changes\n-> Entering step validate, name 'validation'\nno changes\n-> Entering step build, name 'builder'\nrequest.instance was set to 0000000064a40c39000000004c9763f1\n  request.method was set to PUT\n  request.headers was set to array(3) {\n    [\"X-Amz-Security-Token\"]=>\n    string(7) \"[TOKEN]\"\n    [\"Host\"]=>\n    array(1) {\n      [0]=>\n      string(31) \"s3-ap-southeast-1.amazonaws.com\"\n    }\n    [\"x-amz-acl\"]=>\n    array(1) {\n      [0]=>\n      string(11) \"public-read\"\n    }\n  }\nrequest.body was set to \ufffdPNG\n  \u001a\nIHDR\u0001Rh\b\u0006\ufffdCx\ufffd\u0019tEXtSoftwareAdobe ImageReadyq\ufffde<\u0010\bIDATx\ufffd\ufffd]\ufffdq\u001b;\u0012\ufffdY\ufffd/]/\ufffdO{4\u0015\u0001\ufffd\u0011\ufffd\ufffd\ufffdEd\u0004\"#\ufffd\u0018\ufffd\ufffd\bH]\ufffd\ufffd\"\u0010u\u0713f\ufffdZ\ufffd\bv\ufffd\ufffd\ufffd\u0005\ufffd\ufffdg\ufffd;\u0018\ufffd\ufffd\ufffd\ufffd,\ufffd\ufffd\u0010ht\u007f\ufffd\u001a\n!H\u001a\u007f\ufffd\ufffd=H\ufffd/:\u0010\u0001$O\ufffd\u03d0\u0004\ufffd\u0014;\u0012\ufffdB\u001a R\ufffdIt\u0004\u0012\n\ufffd3\ufffd\ufffd$\ufffd\u0005$\ufffd\ufffd\u0014\ufffd(\ufffd\u0014\ufffd(\ufffd\u0014\ufffd(\"\u0005\ufffd(\"\u0005$\n  \"\u0005$\ufffd>\ufffd\ufffd\ufffd.\u00e1\u0004\ufffda\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffdY\ufffd\ufffd\u001f\u007f/\ufffd#\ufffd\u001e\ufffd\ufffd\u001b\ufffd\ufffd\u000f/\u0573\ufffd\ufffdu\u0576\ufffd\ufffd2\ufffd\ufffd\ufffd!\u000e\ufffde\ufffd\ufffd\ufffdO\ufffds\ufffds,@\ufffdq\ufffdK \ufffdy\ufffd\ufffd\ufffdRq\ufffd\ufffd\ufffd7\n\u0019\u071d\ufffdG\ufffdU;\ufffd5\u07ddW\u007f\ufffdTO^\ufffd\u05c9\ufffd\u001e\ufffdw/\u001d\ufffd\ufffd\b\ufffd\ufffd\ufffd\ufffd4\ufffd{\ufffd\ufffd{r\ufffd>\ufffd\ufffd\ufffd\u0014g5\ufffd|]\ufffd\ufffdw\ufffd0\ufffd:qbU?\u0006\u0001\u01e2\ufffdNL\ufffd}\ufffd\ufffdG~;\ufffd\ufffd<\u0010\ufffdHo\ufffd\u0004BB\u001a\ufffd\ufffd\ufffd\ufffd\u0001\ufffdI\ufffd\ufffds\ufffdyp,BcEZ\u0018\u001a4\u0019\ufffd\ufffd&\n  \ufffd\ufffd\ufffdT\u03f5p\u007fB\ufffd\ufffd\ufffd\ufffdz\ufffdM8\ufffd=\ufffdw\ufffd\ufffd<\u000f\ufffdw\ufffdN\ufffdfL\u001a\ufffdD\ufffd\ufffd\u007f\ufffd<\u000e\ufffda\ufffdGf:u-\ufffd\ufffd\ufffdKGvJD\ufffdE\ufffdC\u0012\ufffd\ufffds\ufffd+&\u050d\ufffd\ufffd\ufffd \ufffd]\ufffd?\ufffd\ufffd\u83a2}\ufffd}\ufffd\ufffd\ufffd\ufffdh\ufffd\ufffd\ufffd\ufffd\ufffd\u000f\"\ufffdY\ufffd\ufffd\ufffdl\ufffd\ufffd\ufffd\u001f\ufffdg\ufffdc\ufffd\ufffdT6\ufffdhpH \ufffd\ufffdA{g\ufffd\ufffd\u0016\u073eWGJE\ufffd\ufffdZ\ufffd\ufffd\u001a\ufffd^\ufffd\ufffd\ufffdb\ufffd8\ufffdMF\ufffdX}\ufffd]\u0019w\ufffd;\ufffd\ufffd\ufffd\ufffd3\u0011\ufffdc\ufffdH\u0507\u0013\ufffd\u001e\ufffd\ufffd;\ufffdg\u05f3\ufffdI\ufffd\ufffd\ufffd\u0017L\ufffd R[%\ufffd\ufffdP=\u0019L\ufffdNdTcZE\ufffd\ufffdD\ufffd\ufffd \u0019\u0015/'\ufffd\ufffdq\ufffd\u001d\u0564%\ufffd3I\ufffdD\u0649-\u001arb\ufffdI\ufffd\ufffdg] RG\ufffd\ufffd\ufffd\ufffd;Y(\ufffd\u0423\ufffdt\ufffd\ufffd\ufffd\u0004\ufffd(\ufffd\ufffd\u0016\ufffd\ufffdDi\u0016\ufffd\ufffdY\u062f\ufffd\ufffd\ufffd \u0487\ufffdT\ufffd\ufffd\ufffdOF\ufffdM\ufffdRoh\ufffd\ufffd\ufffd\ufffd5\ufffdH\u007f\u0019\ufffdk\ufffd\ufffd)\u000ff\ufffd\ufffd\ufffd\fL\ufffd\ufffd\ufffd\ufffd2R\ufffd\u0013\ufffd\ufffd\ufffd\ufffd\u001c\ufffd\ufffd(\u0015C\ufffd\ufffdN\ufffd\ufffd\ufffd\u0004R\ufffd\ufffd\ufffd\f\ufffd\ufffd\ufffd\u0183\"\u0005\ufffd\ufffd\u0016m[cL\ufffdDY\ufffd\ufffdZ2\ufffd\u000b\ufffd\ufffd~\ufffdsc\ufffd\ufffd\ufffd\ufffd-\u001e\ufffd\ufffd\ufffd\ufffdY\ufffd\ufffd\ufffd\b\ufffd\ufffd\ufffd &\ufffd\ufffd\ufffd\u00071\ufffdT\ufffd\u001c\ufffd\ufffdEcq\ufffd\u0410s\ufffd(H\ufffd\u0006F\ufffd\ufffd\ufffdN\ufffd\u001f\ufffd\ufffd\u001c\ufffd\\\ufffd\u007f\ufffd\ufffd($\ufffd\ufffdF\ufffd0\ufffdWMc\ufffd\ufffd\u051c\ufffd\ufffd\u000e\ufffd\ufffd    \ufffd\ufffd\ufffd\u001e\ufffd\u0001$\ufffd2\ufffd?S\u0006\ufffd\ufffd\ufffdRz\ufffdme\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd}\ufffdQ\u025f;\u0004\u02af\ufffd\u0013\u001b\ufffd4\ufffd\u0014S\ufffd\ufffdM\ufffdxYF.\ufffd\ufffdv}:E\ufffd\ufffdH\ufffd'\ufffdL\ufffd\ufffdka\u007f\"\ufffd\u0552 \ufffd\ufffd\ufffd\ufffdSn\ufffd\"n\u001d\ufffd\ufffd\u000b\ufffd\ufffd\ufffdM\ufffd>g\ufffdr-B\u001a\ufffd\u0007\u0012\ufffd'\ufffd\ufffd\ufffd7\ufffd\ufffd\ufffdJ\ufffd\ufffd\ufffd\ufffd7:\ufffd|\ufffd\ufffd\uf683B\ufffd\ufffd\ufffd@o\ufffd^B\ufffd\ufffdN\u04bcb\ufffdr\ufffd\ufffd~\ufffd!>\ufffdE\ufffd\ufffde\ufffdDq\ufffd\ufffd|q8\u0011\ufffd\ufffdov(yX\ufffd\ufffd\ufffd\ufffd\ufffd\ufffdF\ufffd\ufffd\ufffdk\ufffd  \ufffdF2I$\ufffd3$\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffdv\u00127\ufffd\n9\ufffd\ufffd\ufffd-L\ufffd\\\ufffd\ufffd\ufffd\ufffd\ufffd$O\ufffd\u0013\u0005;\ufffdZ5$\u0619\ufffd\ufffd\ufffdd\ufffd-\ufffdu\ufffd/JN\ufffd.\ufffd\u0012\ufffd\u0017\"\ufffd\u0011\ufffd\n\ufffd2KC\ufffd^\u01d0\n\ufffdt\ufffd'\ufffd\ufffd0\ufffd\u0017\ufffdF\ufffd\ufffd(\ufffd\ufffd\ufffdg\ufffdA\ufffd\ufffd\ufffd<}dqR\ufffd\ufffd\ufffd{Y\u0007l\u000e5\ufffd\ufffd\u03e5R\ufffd3\ufffd\ufffd)\u0010)\ufffdpm\u05b8\ufffd\u0006\u0011\u00162\u05f5\ufffd\ufffd:\ufffd\ufffd\ufffd\u0004o\u0019\ufffd\ufffdW\u000f\u0011\ufffdw2D\ufffd#\ufffd\ufffd&\ufffd\ufffd_\u000e\ufffdpO\u0127\ufffdH\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffdR\u0004\ufffd\n&R\ufffd\ufffd\ufffdG89\"6!\n\u04e9\ufffd\ufffd\ufffd,\ufffdD.]\u001d%d}\ufffd\ufffd\ufffd\ufffdL\u0012\u0262\ufffdr\ufffd!\ufffdM\ufffd?\ufffd\ufffd\ufffd\ufffd\ufffd\u000b\u000b\ufffd\ufffd\ufffd^\ufffd|\u001e)\u0013\ufffd\ufffd\ufffd\ufffd%\ufffdo\u0600\u05d6}\u001d(\ufffd\ufffdf\ufffd\ufffddo|\ufffdp,\ufffd\u0006\ufffd\ufffd\ufffd\u0418M#\ufffd\ufffd\ufffd4?G\ufffd\u001d;L\ufffdtW\ufffdo\ufffd\u0007\ufffd\u000f,\ufffd\ufffd#\ufffd\ufffdD\ufffd\ufffd0\ufffd\ufffd\ufffd\ufffd\ufffd\u0015xl\ufffd\ufffd\u0010EW.]\ufffdM\ufffd;\ufffdyJoJ\bR\ufffd}T\ufffd\ufffdUh\u0013'f\ufffd\u0017;\u000e\ufffd+\ufffd\u0018\ufffdT$\u0004>\u0611\ufffdL\ufffdy\u0006\ufffd\ufffd(\ufffd[\u001b\ufffd?\ufffd\ufffd\ufffd\ufffd\ufffd\ufffdz\ufffdu\ufffd\ufffd\u001e\ufffd\ufffd4\ufffdL\ufffd\ufffd\u0499KO\u030e\ufffd\ufffdH\ufffd_5\ufffd\ufffdo\ufffd\u0019\u0019\ufffd\ufffd2\ufffd>\ufffdEZ\ufffd3\u0537q@\ufffd7\ufffd.\ufffd\ufffd{tj\ufffdK.\ufffd\ufffd\ufffd\ufffd\ufffdV\ufffd\ufffd\ufffdnd?\ufffdD<\ufffd\ufffd\ufffd\u001b\ufffdpT\ufffd;\ufffd,\u0012O\ufffd\u000b\u0011\ufffd\ufffd\u0006\ufffd\ufffdu(\u0012u@\ufffd\ufffdG\ufffd\ufffd\ufffd\ufffd\ufffd/\ufffd\ufffd\ufffd\u0288\ufffd)\ufffd\ufffd\ufffd)S\u0007\ufffd\ufffd\ufffd\ufffd\"I\ufffd_\u0006T\ufffd7\ufffd\ufffd\ufffd0\ufffd\u0007\ufffd\u000e\ufffd\ufffd)?\ufffdS\u001a\ufffd\ufffd\ufffd\u0006S\ufffdC\ufffd\ufffd;[/?\ufffd\ufffd-7CZ\ufffd\ufffd\ufffd\ufffdu\ufffd\ufffd\ufffd\ufffdE\ufffd\ufffd\ufffdh4\ufffd\ufffd\u0005\ufffd\ufffd#\ufffd\ufffd\n\u001cH)\ufffdhT\ufffd\ufffd\ufffd\ufffd\\\ufffdr7  \ufffd.\ufffd\u0012)\u0013\ufffdJI\u0019\ufffd)\u000f\ufffd\ufffd}M\ufffdh\ufffd\ufffd@\ufffd\ufffd\ufffd6\ufffdb\u0010DS\ufffd\ufffd=\u0018tf0\ufffd\ufffd    \ufffd\u0016\ufffd$\ufffd\u0011\n/gq0\ufffd\u001b\u0014\u000e\ufffdm:\ufffd\u001d0\ufffdU\ufffd#O\ufffd\ufffd\ufffdB\ufffdA\ufffd\ufffdo\u07f1<\ufffd\u0006\ufffdBw\u01b0F4\ufffd\u0004\ufffd\ufffd)\ufffd1,gQ\u0237$\u8b29\u7ef81D\u001ei\u00111)?x6\u0503^9\ufffd]\u06f6\u0011)\u001cY3r\ufffdif\ufffd\u06d6\ufffd\u0469\ufffd'\ufffd/\ufffd\ufffd\ufffd\ufffd\u0626QLd:\ufffd1s\ufffd\ufffd'G\ufffd\ufffd\ufffd\ufffdg\ufffd}\ufffd?\ufffda\ufffd\ufffd\u0001\ufffd1\ufffdj\ufffdEl\ufffd'\ufffd\u0519'\ufffd\ufffd\u0015\u007fHh\u0005\u000b\ufffd\ufffd\ufffd\u007fR\u0798\ufffdd\u0017Bc\ufffd\ufffd6\ufffd\u001cE\ufffd/\ufffdB\ufffd\ufffd\ufffd\n  \ufffd\ufffd1\ufffdz?f\u0004\ufffdD\ufffdQ9/\ufffd\u0001\ufffd\ufffd\ufffd\ufffd;~\ufffd\ufffd  \ufffdNM\ufffd\ufffd?\ufffd\ufffdtd\ufffd\ufffd~\ufffdS\u001d\ufffd\"\u000e2}\u0013z\ufffd\ufffd]\ufffd\ufffdqg#>\ufffd\ufffd\u000f)/\ufffd\ufffd\b#\ufffd\b\ufffd\ufffd\u0018@\ufffd.\ufffd6\ufffd\ufffdqd8\ufffd\ufffd\u084d\ufffd\ufffd\ufffdu\ufffde\ufffd#\ufffdX\ufffd\ufffdMM3\ufffd\ufffdV\ufffd5\ufffdw\\\ufffd\ufffdU\ufffd\ufffd^\u05d6H7\b>\u000b\ufffdT\ufffd/\ufffd\ufffd\ufffd\ufffd\ufffdH\u00175=S\ufffd\ufffd\ufffdT\ufffde^5\u0236\ufffd]\u0013D\ufffd\ufffd\ufffds\ufffdy%\ufffd\ufffd77\u0005\u0010is\ufffdr\ufffd82&\ufffd\ufffd\ufffdz\ufffd\ufffd}\u000be\ufffd;e+\u0efd&J\ufffd9i!\ufffd\u0012\ufffd\u0004\u0010\u001c-\ufffd\ufffd\ufffd6\\D\ufffd&\ufffd\n]\ufffd\u0536\ufffdj\ufffd\ufffd@\ufffd,o\ufffd\ufffd\ufffd\ufffd\u0314\ufffd!U\u0011\ufffd\u0007\ufffd\u0006\ufffd\ufffd\ufffd;\ufffd1\ufffd?\ufffdM\n  \ufffd@o\ufffd\u001f)\u001d\u0019\ufffd\ufffd<\ufffdB\ufffd\ufffd\ufffd\ufffd\f\u001b\u000e}\ufffd?\u0010SD\ufffdvd\u0006\ufffdp_$JQ\ufffd\ufffdU\ufffdt\u0010\ufffd)\ufffd\ufffd\u0140\ufffdN\ufffdRL;sA\ufffd\u001b\u000b\ufffd\u01b0q\ufffdA\ufffdw\ufffd\u036d0+hm|a\ufffd#]\ufffdb\u0723F\ufffd\ufffd\ufffd?l_\ufffd\ufffdH\u95bf\ufffd\ufffdg\by\u000b\ufffd\ufffd|Djx$\ufffd\ufffd\ufffd\ufffd\ufffdS'\u001f0\ufffdT<\ufffd\ufffd\ufffdA\ufffdY\ufffdfm\ufffd\ufffdY\ufffd_\"j\u0017k\ufffd+Mr\ufffd\u07d5\ufffdv\u0011\ufffd\u0010\ufffdv\ufffdS?\nc@\ufffd?\ufffd\ufffd\ufffd\u0005ym\ufffd\ufffd\ufffd\bM\ufffd\ufffd\ufffd\ufffd\ufffdk\ufffd\u0001\u001b\ufffdD\ufffd/\u0569e9\ufffd\ufffdK\ufffd\ufffdWu\ufffd\ufffd\u00103MoT\ufffdl\u0011\\  GgL\u0209\u0012\ufffd\ufffdR\ufffd\ufffd\ufffd\ufffdF\ufffdR\ufffd\u0006\u0016\ufffd9\ufffd\ufffd\ufffd\ufffd:9\ufffd\ufffde\ufffd\ufffd\u0014JU/\u0017[\ufffd\"\ufffdDH\ufffdd\ufffdSRp\ufffd\ufffd\u00b2\ufffd\ufffdP.)\u001d\ufffdwY\ufffdb\ufffd\ufffd\u000b\u000e\ufffd8\u0014(\u0011\ufffd\ufffd\ufffd\ufffd\ufffd1\ufffd\ufffd=\ufffd\ufffd\ufffdc\f\ufffdI\ufffd\ufffd\ufffd\ufffd\u000b\ufffd\ufffd\\6\ufffd\ufffd\ufffd#]G\ufffd\u001d\ufffd>\ufffd\ufffdB\ufffd\ufffd\ufffdch4\ufffdH\ufffd.8\ufffdi\ufffd\u0013\ufffd\u048e\ufffd\ufffdG,b\ufffd\u00048\ufffd\ufffd\u0555k\u001c\ufffdB\ufffdtR\ufffd\u0017\ufffd\u0019T\ufffd!\u0708\ufffd\ufffdc\ufffd\u0011\ufffdy\ufffd\ufffd\ufffd~\ufffdn\ufffd\ufffd\ufffd7\ufffd[OW^#\ufffdc\u032f$u\ufffd\u0012\ufffd\ufffd\ufffd\ufffd\ufffd  o4\ufffdD\b\ufffd\u001f\ufffd\ufffd-\"\ufffd\ufffd#\u0001\ufffd\u001d\ufffd\ue1b3)\ufffd\ufffd\u000e\ufffdt\ufffd\ufffd\u0637\ufffd\ufffd  \ufffdI\ufffd7\"\ufffd\u001c2\ufffd\ufffd\ufffd\ufffd\n\ufffd-\ufffdhO#\u001am\ufffd\ufffdNO&N\ufffd\ufffd\u0146\ufffd\ufffd\ufffdo\ufffd\ufffd\u054b\u001eon6%\ufffdG\ufffd\ufffd\ufffd-\ufffd\ufffdE2\ufffdC\ufffd\ufffd\ufffd\ufffd<\ufffd\ufffd\ufffd%\ufffd\ufffd\ufffd\ufffd\ufffd'jc:\u000e%\ufffd\ufffd\n\ufffd\ufffd\ufffd\ufffd\ufffdS$\u0007D|\ufffd\ufffd\ufffd0\ufffd\ufffdIh^QJ~j\ufffd\ufffd;\ufffd\ufffd\ufffdQ\ufffd\ufffd\bS\ufffdZ\ufffd\ufffdw\f\u0082   \ufffd\ufffd\u02a6x\"\ufffd>\u0014    _\ufffd\ufffd-\ufffd>\ufffd\u00185\ufffd\ufffds\ufffd\ufffd[\u05e9\ufffd>\u001b\ufffd\ufffd^&\ufffdv\u0011\ufffdLyl\ufffd\f\ufffd\ufffdA\ufffdR#\ufffd\ufffd\u001a_\ufffd6e\ufffd\ufffd\ufffdL\ufffd\ufffd\ufffd\ufffd\ufffd)Ni\ufffd\u0016\ufffdx\ufffdYKj\bq\ufffd\ufffd\ufffdN\u0016\ufffd\u05e9\u0015\ufffd7!\ufffd\ufffd\ufffdGw\ufffdKn5\f\ufffd~\ufffdU\ufffd\ufffd\ufffd$ZD\ufffd\ufffd\ufffd\ufffde\ufffd&\ufffd\ufffd\ufffd0\ufffd6\ufffdt\ufffd5\ufffdag|\ufffd\ufffd\u007f\n_\ufffd\ufffd|\ufffd\ufffdU\ufffd\u0770\ufffdC\ufffdG\ufffd-\ufffd\ufffd\u0089\ufffd,\u001e}\u0004i\ufffd\ufffdw\ufffd\ufffdaz,_ZF\ufffd\ufffd\ufffd\ufffd\ufffdm\ufffd|\u0016\u2ac5\u0010\ufffd\ufffddD-i\ufffdL\ufffd\ufffdE\ufffd\ufffda?\ufffd\"\u001f\ufffd\ufffd\ufffd\u001e\u0004\u001b\ufffdD\u0018n\ufffd\u001af\ufffdX\ufffd3\ufffdR\ufffdiyl_\ufffd>\ufffd\ufffd;\ufffd\ufffd;\u0005v.\b\ufffd\ufffd\u0563\ufffd\ufffd\u001f\ufffd\ufffds\ufffd\ufffdLw\ufffdV_\ufffd\ufffd<\ufffdG\ufffd(\ufffd\ufffdf]R)Dl\ufffd\ufffd2KxmT\ufffdE3\ufffd$'\ufffd\ufffd\u0018\ufffd^\bG\ufffd\u01d0\ufffd\ufffdlwV\ufffd\ufffd)\ufffd\ufffdt\ufffd\ufffd\ufffdjm\ufffd_zO_\ufffdO\ufffd+4\ufffd.\u394b\u02faz\ufffd$\ufffd\u05fa\u0016\ufffd\ufffd~\ufffd\ufffde\ufffd\ufffd\ufffdn\ufffd\ufffd\ufffd\ufffd\ufffd=\ufffd\ufffd\ufffdr\ufffd\ufffd\ufffd\ufffd\u0019\u00149\ufffd\ufffd:\ufffdx&\ufffd+k\ufffdeOM;z\ufffd\ufffd\u000fu\u0015X\u0011\ufffd\ufffd\ufffd\ufffd6\ufffd\ufffd1t\ufffd\u0007\"\ufffd\ufffd\ufffd\ufffdz\ufffdf\u0017\ufffd:\ufffd\u001d\u0019\ufffd\u0005;\ufffdb\ufffdQ\ufffd\ufffd\u0018mu\ufffd\ufffdp\ufffd\ufffdI\ufffdZ\ufffd\ufffd\u0013.\ufffd\ufffd8\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\u007f\ufffd\ufffd\u0014\ufffdd\ufffd\u001d=\ufffd\ufffd\ufffd\ufffdHJ  \ufffdzlG\ufffd39\ufffd)x&\ufffd\ufffd0\ufffd<@A\u0010E\ufffd\ufffdi\ufffd\ufffd\ufffd\ufffd#\ufffdo\ufffd\ufffd\ufffd5\ufffdA\ufffd#\u0431\u0017v\ufffd-\u0018E\ufffdB|>\ufffd%\ufffd6\u0013\ufffdjL\ufffd\ufffdD    \ufffddVA\ufffd^Q\ufffdL\ufffd\ufffd3\u0013B\ufffd\ufffd\ufffds\ufffd\ufffd\ufffd\ufffd\ufffd;\ufffdc\ufffd\u0018\\9\ufffd\\\u0131^\ufffd\u0012\ufffd\ufffd\ufffd_I\ufffd\ufffd\u0011\ufffd\ufffd\ufffd\ufffdpW\ufffd(\ufffd\ufffd\ufffd:\ufffd\ufffd\ufffd0/\ufffd\ufffd\u0114W\n  \ufffdFy&\ufffdo.Itp_u_YYL\ufffdR\ufffd!\u000e\u000fE\ufffd\\W\ufffd\ufffdD\ufffdL\ufffdV\ufffd\u0018\ufffd\ufffdF\ufffd<\ufffd\ufffd\"\ufffd:\ufffd<=/\"n\u001f9\ufffde\u000bE;\ufffd-  y\ufffdk\u001a\ufffd\u04dfT\ufffd_%\ufffd\ufffd\u053dL\ufffd|b2\n\ufffdy\ufffd\ufffd\ufffdr\ufffd\ufffd5\u001e\ufffdb\ufffd\ufffdN\ufffdZ}~ \ufffd6\u001db\ufffdLD\ufffdMAd\ufffdQ\ufffd\ufffd%\ufffd\u001c\ufffd\ufffd\ufffd\ufffd(\ufffd\ufffd;x \ufffdt0\ufffd\ufffd\ufffd\ufffd\ufffdn.\"\ufffdM\ufffdq\u001d\ufffd\ufffdj\ufffd\ufffd\u001a\ufffd<\ufffd6\ufffdco\ufffdb\ufffdF\u0011tGQ\ufffd\ufffdi>\ufffd\u0004\ufffd\ufffd\ufffd\ufffd'J\ufffd&\u0179\u06fc\ufffdq.\u04ab\ufffd5\u0013\ufffd\ufffd\u02add?\u0015\ufffdj\u00167i\ufffd\ufffdz\ufffdQ:\ufffd<\ufffd\ufffd\ufffd\ufffd/\u0013\ufffd\u0011r\u039e\ufffd\u0014'\f>\ufffd\u001e\"j \ufffd\ufffd6\ufffd4\ufffdg\ufffd\u04ac\ufffd\u001d\ufffd\ufffd\u0005d\ufffd\ufffd\u0411Q\ufffd\u3e6d\ufffdw\ufffd\ufffd  t\ufffd\u000e\u000f\ufffdLy\u0006|'OJ\u0015\ufffdJ\ufffd\ufffd\u0014\u0017\ufffd}\ufffd\u001a\ufffd\ufffd\ufffd-7 \\\ufffd|iB \ufffd\ufffd\ufffd=\ufffd\ufffdT!\ufffdP\ufffd\ufffd\u01b0\ufffd%;\ufffd\ufffdhv\ufffdE^\ufffds\ufffd\ufffde_|\ufffd\ufffd\ufffdZ_\ufffd\u0004\ufffd\ufffd\ufffdK>\u0001\ufffd\ufffd\ufffd<\ufffdu\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd~\ufffddy\"\u03e9\u000b\u0019\ufffd\ufffdQ\u007f\ufffd\u0006\ufffd\ufffd\u000f\ufffd\ufffd\ufffd2\ufffd\u05f5\u001aO\ufffdv\ufffd\\\ufffdZ\ufffd\ufffd\u000e\u001d\ufffd\ufffdk\u06cb\ufffdH)\ufffd\ufffd\ufffd\ufffdc\ufffd\ufffd\u0007z\ufffd\ufffd4\ufffd\u0012\ufffdr\u007fh\ufffd\ufffd\u0382\\J\ufffdq\ufffd\ufffdf\ufffdw\ufffd\ufffd\ufffd\ufffd]^\u001c!\ufffd'c\ufffd\u001ez4l\u001asJ\ufffd_\ufffd\\Nb\ufffd\ufffdp\u007f\\\u0010\ufffd\ufffd\ufffds\ufffdc\u0006\ufffd\ufffd\u007f%\ufffd\u0014~\ufffd\u0007{\ufffd\ufffd<F\"\ufffdM0[\ufffd\ufffdt\ufffd\ufffd\ufffd\ufffd\ufffd  \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd6\ufffd\u02f2\ufffd\ufffd\ufffd\ufffd\ufffd#\ufffd\ufffd\ufffdu\ufffd\ufffd\ufffdX?\ufffd\u0019\ufffdWG\ufffd\ufffd]\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffdh\ufffd\ufffd\n;\ufffd\ufffd\ufffd\u02f7.\ufffd\ufffdO\"p\ufffd#\ufffd\ufffd\ufffd{n(w\ufffd\ufffdA\ufffd\u001e5\ufffd\u001e0l!\ufffdz\ufffdI~fs\ufffd\ufffd\ufffd\u0001\ufffdr_~\ufffd\u0685\ufffd\ufffd\ufffdU\ufffd\ufffd\f\ufffd\ufffd%rgr\ufffd\ufffd]o+\ufffd+\ufffd\u01da_\u0019\ufffd\ufffd\ufffd\u001a\ufffd|W\ufffdc\ufffd)m\ufffdy\ufffd\ufffd\ufffd\u001c\ufffd\ufffd\ufffd\ufffd:\"\u0005\ufffd\n\u001d\ufffdD\n  \"\u0005\ufffd\ufffdH\u0001\u0010)\ufffd\u0014D\n  \"\u0005@\ufffd R\u0010)\ufffd\u0014\ufffd\ufffdH\u0001@\ufffd RD\n  \"\u0005\ufffd\ufffdH\u0001\u0010)\ufffd\u0014D\n  \"\u0005@\ufffd\ufffdp\u0016\ufffd;\n  \ufffd\ufffd/1,\ufffdI\ufffd\ufffd\f\ufffd@-\u0001\ufffdO\ufffdIEND\ufffdB`\ufffd\n  request.scheme was set to https\n  request.path was set to /prod-user-uploaded-images/MENU-ITEM-IMAGES/7ebdc1fd4ef261154d5dddbc958dcb48534b6d27.png\n-> Entering step build, name ''\nrequest.instance changed from 0000000064a40c39000000004c9763f1 to 0000000064a40c37000000004c9763f1\n  request.headers.User-Agent was set to array(1) {\n    [0]=>\n    string(17) \"aws-sdk-php/3.8.2\"\n  }\n-> Entering step build, name 's3.checksum'\nno changes\n-> Entering step build, name 's3.content_type'\nrequest.instance changed from 0000000064a40c37000000004c9763f1 to 0000000064a40c34000000004c9763f1\n  request.headers.Content-Type was set to array(1) {\n    [0]=>\n    string(9) \"image/png\"\n  }\n-> Entering step sign, name 'retry'\nno changes\n-> Entering step sign, name 'signer'\nrequest.instance changed from 0000000064a40c34000000004c9763f1 to 0000000064a40c17000000004c9763f1\n  request.headers.X-Amz-Content-Sha256 was set to array(1) {\n    [0]=>\n    string(64) \"08879a5305494cc0c099361f1b65cd58215a4b8551cc675f78fd6d2de276972d\"\n  }\nrequest.headers.X-Amz-Date was set to array(1) {\n    [0]=>\n    string(16) \"20151028T033355Z\"\n  }\nrequest.headers.Authorization was set to array(1) {\n    [0]=>\n    string(251) \"AWS4-HMAC-SHA256 Credential=app/console doctrine:migaP7JAKIAIYSKALZTFUU3A/20151028/ap-southeast-1/s3/aws4_request, SignedHeaders=host;x-amz-acl;x-amz-content-sha256;x-amz-date, Signature=[SIGNATURE]\n  }\n-> Entering step sign, name 's3.put_object_url'\nno changes\n-> Entering step sign, name 's3.permanent_redirect'\nno changes\n- Hostname was NOT found in DNS cache\n-   Trying 54.231.243.138...\n- Connected to s3-ap-southeast-1.amazonaws.com (54.231.243.138) port 443 (#822)\n- successfully set certificate verify locations:\n-   CAfile: none\n  CApath: /etc/ssl/certs\n- SSL connection using ECDHE-RSA-AES128-SHA\n- Server certificate:\n-    subject: C=US; ST=Washington; L=Seattle; O=Amazon.com, Inc.; OU=S3-B; CN=*.s3-ap-southeast-1.amazonaws.com\n-    start date: 2015-08-04 00:00:00 GMT\n-    expire date: 2015-12-31 23:59:59 GMT\n-    subjectAltName: s3-ap-southeast-1.amazonaws.com matched\n-    issuer: C=US; O=VeriSign, Inc.; OU=VeriSign Trust Network; OU=Terms of use at https://www.verisign.com/rpa (c)10; CN=VeriSign Class 3 Secure Server CA - G3\n-    SSL certificate verify ok.\n\nPUT /prod-user-uploaded-images/MENU-ITEM-IMAGES/7ebdc1fd4ef261154d5dddbc958dcb48534b6d27.png HTTP/1.1\nHost: s3-ap-southeast-1.amazonaws.com\nx-amz-acl: public-read\nContent-Type: image/png\nX-Amz-Content-Sha256: 08879a5305494cc0c099361f1b65cd58215a4b8551cc675f78fd6d2de276972d\nX-Amz-Date: 20151028T033355Z\nAuthorization: AWS4-HMAC-SHA256 Credential=app/console doctrine:migaS3/20151028/ap-southeast-1/s3/aws4_request, SignedHeaders=host;x-amz-acl;x-amz-content-sha256;x-amz-date, Signature=[SIGNATURE]\nUser-Agent: aws-sdk-php/3.8.2 GuzzleHttp/6.1.0 curl/7.35.0 PHP/5.5.9-1ubuntu4.11\nContent-Length: 4198\n- upload completely sent off: 4198 out of 4198 bytes\n  < HTTP/1.1 400 Bad Request\n  < x-amz-request-id: C92EB847871E8801\n  < x-amz-id-2: H8vjDCd7FsIUFGWUiwUhB9l2286SRT8k+bdecNY4hUEYjYrPjLPHBKXJuCNh7Zxk2roXx6VY+xs=\n  < Content-Type: application/xml\n  < Transfer-Encoding: chunked\n  < Date: Wed, 28 Oct 2015 03:34:56 GMT\n  < Connection: close\n- Server AmazonS3 is not blacklisted\n  < Server: AmazonS3\n  < \n- Closing connection 822\n\n<- Leaving step sign, name 's3.permanent_redirect'\nerror was set to array(13) {\n    [\"instance\"]=>\n    string(32) \"0000000064a40ce4000000004c91fab1\"\n    [\"class\"]=>\n    string(28) \"Aws\\S3\\Exception\\S3Exception\"\n    [\"message\"]=>\n    string(750) \"Error executing \"PutObject\" on \"https://s3-ap-southeast-1.amazonaws.com/prod-user-uploaded-images/MENU-ITEM-IMAGES/7ebdc1fd4ef261154d5dddbc958dcb48534b6d27.png\"; AWS HTTP error: Client error: 400 AuthorizationHeaderMalformed (client): The authorization header is malformed; the Credential is mal-formed; expecting \"/YYYYMMDD/REGION/SERVICE/aws4_request\". - <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n  AuthorizationHeaderMalformedThe authorization header is malformed; the Credential is mal-formed; expecting \"<YOUR-AKID>/YYYYMMDD/REGION/SERVICE/aws4_request\".C92EB847871E8801H8vjDCd7FsIUFGWUiwUhB9l2286SRT8k+bdecNY4hUEYjYrPjLPHBKXJuCNh7Zxk2roXx6VY+xs=\"\n    [\"file\"]=>\n    string(73) \"/home/ubuntu/www/shifoo/vendor/aws/aws-sdk-php/src/WrappedHttpHandler.php\"\n    [\"line\"]=>\n    int(152)\n    [\"trace\"]=>\n    string(3394) \"#0 /home/ubuntu/www/shifoo/vendor/aws/aws-sdk-php/src/WrappedHttpHandler.php(76): Aws\\WrappedHttpHandler->parseError(Array, Object(GuzzleHttp\\Psr7\\Request), Object(Aws\\Command))\n  #1 /home/ubuntu/www/shifoo/vendor/guzzlehttp/promises/src/Promise.php(199): Aws\\WrappedHttpHandler->Aws{closure}(Array)\n  #2 /home/ubuntu/www/shifoo/vendor/guzzlehttp/promises/src/Promise.php(170): GuzzleHttp\\Promise\\Promise::callHandler(2, Array, Array)\n  #3 /home/ubuntu/www/shifoo/vendor/guzzlehttp/promises/src/RejectedPromise.php(40): GuzzleHttp\\Promise\\Promise::GuzzleHttp\\Promise{closure}(Array)\n  #4 /home/ubuntu/www/shifoo/vendor/guzzlehttp/promises/src/TaskQueue.php(60): GuzzleHttp\\Promise\\RejectedPromise::GuzzleHttp\\Promise{closure}()\n  #5 /home/ubuntu/www/shifoo/vendor/guzzlehttp/guzzle/src/Handler/CurlMultiHandler.php(96): GuzzleHttp\\Promise\\TaskQueue->run()\n  #6 /home/ubuntu/www/shifoo/vendor/guzzlehttp/guzzle/src/Handler/CurlMultiHandler.php(123): GuzzleHttp\\Handler\\CurlMultiHandler->tick()\n  #7 /home/ubuntu/www/shifoo/vendor/guzzlehttp/promises/src/Promise.php(240): GuzzleHttp\\Handler\\CurlMultiHandler->execute(true)\n  #8 /home/ubuntu/www/shifoo/vendor/guzzlehttp/promises/src/Promise.php(217): GuzzleHttp\\Promise\\Promise->invokeWaitFn()\n  #9 /home/ubuntu/www/shifoo/vendor/guzzlehttp/promises/src/Promise.php(261): GuzzleHttp\\Promise\\Promise->waitIfPending()\n  #10 /home/ubuntu/www/shifoo/vendor/guzzlehttp/promises/src/Promise.php(219): GuzzleHttp\\Promise\\Promise->invokeWaitList()\n  #11 /home/ubuntu/www/shifoo/vendor/guzzlehttp/promises/src/Promise.php(261): GuzzleHttp\\Promise\\Promise->waitIfPending()\n  #12 /home/ubuntu/www/shifoo/vendor/guzzlehttp/promises/src/Promise.php(219): GuzzleHttp\\Promise\\Promise->invokeWaitList()\n  #13 /home/ubuntu/www/shifoo/vendor/guzzlehttp/promises/src/Promise.php(62): GuzzleHttp\\Promise\\Promise->waitIfPending()\n  #14 /home/ubuntu/www/shifoo/vendor/aws/aws-sdk-php/src/AwsClient.php(202): GuzzleHttp\\Promise\\Promise->wait()\n  #15 /home/ubuntu/www/shifoo/vendor/aws/aws-sdk-php/src/AwsClient.php(167): Aws\\AwsClient->execute(Object(Aws\\Command))\n  #16 /home/ubuntu/www/shifoo/src/Sequoia/HackBundle/Manager/MenuItemManager.php(485): Aws\\AwsClient->__call('putObject', Array)\n  #17 /home/ubuntu/www/shifoo/src/Sequoia/HackBundle/Manager/MenuItemManager.php(485): Aws\\S3\\S3Client->putObject(Array)\n  #18 /home/ubuntu/www/shifoo/src/Sequoia/HackBundle/Controller/MenuItemController.php(338): Sequoia\\HackBundle\\Manager\\MenuItemManager->addFile(Array, Object(Symfony\\Component\\HttpFoundation\\FileBag), '114')\n  #19 [internal function]: Sequoia\\HackBundle\\Controller\\MenuItemController->postMenuitemImageAction()\n  #20 /home/ubuntu/www/shifoo/app/bootstrap.php.cache(2974): call_user_func_array(Array, Array)\n  #21 /home/ubuntu/www/shifoo/app/bootstrap.php.cache(2936): Symfony\\Component\\HttpKernel\\HttpKernel->handleRaw(Object(Symfony\\Component\\HttpFoundation\\Request), 1)\n  #22 /home/ubuntu/www/shifoo/app/bootstrap.php.cache(3085): Symfony\\Component\\HttpKernel\\HttpKernel->handle(Object(Symfony\\Component\\HttpFoundation\\Request), 1, true)\n  #23 /home/ubuntu/www/shifoo/app/bootstrap.php.cache(2335): Symfony\\Component\\HttpKernel\\DependencyInjection\\ContainerAwareHttpKernel->handle(Object(Symfony\\Component\\HttpFoundation\\Request), 1, true)\n  #24 /home/ubuntu/www/shifoo/web/app_dev.php(14): Symfony\\Component\\HttpKernel\\Kernel->handle(Object(Symfony\\Component\\HttpFoundation\\Request))\n  #25 {main}\"\n    [\"type\"]=>\n    string(6) \"client\"\n    [\"code\"]=>\n    string(28) \"AuthorizationHeaderMalformed\"\n    [\"requestId\"]=>\n    string(16) \"C92EB847871E8801\"\n    [\"statusCode\"]=>\n    int(400)\n    [\"result\"]=>\n    NULL\n    [\"request\"]=>\n    array(6) {\n      [\"instance\"]=>\n      string(32) \"0000000064a40c17000000004c9763f1\"\n      [\"method\"]=>\n      string(3) \"PUT\"\n      [\"headers\"]=>\n      array(8) {\n        [\"X-Amz-Security-Token\"]=>\n        string(7) \"[TOKEN]\"\n        [\"Host\"]=>\n        array(1) {\n          [0]=>\n          string(31) \"s3-ap-southeast-1.amazonaws.com\"\n        }\n        [\"x-amz-acl\"]=>\n        array(1) {\n          [0]=>\n          string(11) \"public-read\"\n        }\n        [\"User-Agent\"]=>\n        array(1) {\n          [0]=>\n          string(17) \"aws-sdk-php/3.8.2\"\n        }\n        [\"Content-Type\"]=>\n        array(1) {\n          [0]=>\n          string(9) \"image/png\"\n        }\n        [\"X-Amz-Content-Sha256\"]=>\n        array(1) {\n          [0]=>\n          string(64) \"08879a5305494cc0c099361f1b65cd58215a4b8551cc675f78fd6d2de276972d\"\n        }\n        [\"X-Amz-Date\"]=>\n        array(1) {\n          [0]=>\n          string(16) \"20151028T033355Z\"\n        }\n        [\"Authorization\"]=>\n        array(1) {\n          [0]=>\n          string(251) \"AWS4-HMAC-SHA256 Credential=app/console doctrine:migaS3AccessKey/20151028/ap-southeast-1/s3/aws4_request, SignedHeaders=host;x-amz-acl;x-amz-content-sha256;x-amz-date, Signature=[SIGNATURE]\n        }\n      }\n      [\"body\"]=>\n      string(4198) \"\ufffdPNG\n  \u001a\nIHDR\u0001Rh\b\u0006\ufffdCx\ufffd\u0019tEXtSoftwareAdobe ImageReadyq\ufffde<\u0010\bIDATx\ufffd\ufffd]\ufffdq\u001b;\u0012\ufffdY\ufffd/]/\ufffdO{4\u0015\u0001\ufffd\u0011\ufffd\ufffd\ufffdEd\u0004\"#\ufffd\u0018\ufffd\ufffd\bH]\ufffd\ufffd\"\u0010u\u0713f\ufffdZ\ufffd\bv\ufffd\ufffd\ufffd\u0005\ufffd\ufffdg\ufffd;\u0018\ufffd\ufffd\ufffd\ufffd,\ufffd\ufffd\u0010ht\u007f\ufffd\u001a\n!H\u001a\u007f\ufffd\ufffd=H\ufffd/:\u0010\u0001$O\ufffd\u03d0\u0004\ufffd\u0014;\u0012\ufffdB\u001a R\ufffdIt\u0004\u0012\n\ufffd3\ufffd\ufffd$\ufffd\u0005$\ufffd\ufffd\u0014\ufffd(\ufffd\u0014\ufffd(\ufffd\u0014\ufffd(\"\u0005\ufffd(\"\u0005$\n  \"\u0005$\ufffd>\ufffd\ufffd\ufffd.\u00e1\u0004\ufffda\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffdY\ufffd\ufffd\u001f\u007f/\ufffd#\ufffd\u001e\ufffd\ufffd\u001b\ufffd\ufffd\u000f/\u0573\ufffd\ufffdu\u0576\ufffd\ufffd2\ufffd\ufffd\ufffd!\u000e\ufffde\ufffd\ufffd\ufffdO\ufffds\ufffds,@\ufffdq\ufffdK \ufffdy\ufffd\ufffd\ufffdRq\ufffd\ufffd\ufffd7\n\u0019\u071d\ufffdG\ufffdU;\ufffd5\u07ddW\u007f\ufffdTO^\ufffd\u05c9\ufffd\u001e\ufffdw/\u001d\ufffd\ufffd\b\ufffd\ufffd\ufffd\ufffd4\ufffd{\ufffd\ufffd{r\ufffd>\ufffd\ufffd\ufffd\u0014g5\ufffd|]\ufffd\ufffdw\ufffd0\ufffd:qbU?\u0006\u0001\u01e2\ufffdNL\ufffd}\ufffd\ufffdG~;\ufffd\ufffd<\u0010\ufffdHo\ufffd\u0004BB\u001a\ufffd\ufffd\ufffd\ufffd\u0001\ufffdI\ufffd\ufffds\ufffdyp,BcEZ\u0018\u001a4\u0019\ufffd\ufffd&\n  \ufffd\ufffd\ufffdT\u03f5p\u007fB\ufffd\ufffd\ufffd\ufffdz\ufffdM8\ufffd=\ufffdw\ufffd\ufffd<\u000f\ufffdw\ufffdN\ufffdfL\u001a\ufffdD\ufffd\ufffd\u007f\ufffd<\u000e\ufffda\ufffdGf:u-\ufffd\ufffd\ufffdKGvJD\ufffdE\ufffdC\u0012\ufffd\ufffds\ufffd+&\u050d\ufffd\ufffd\ufffd \ufffd]\ufffd?\ufffd\ufffd\u83a2}\ufffd}\ufffd\ufffd\ufffd\ufffdh\ufffd\ufffd\ufffd\ufffd\ufffd\u000f\"\ufffdY\ufffd\ufffd\ufffdl\ufffd\ufffd\ufffd\u001f\ufffdg\ufffdc\ufffd\ufffdT6\ufffdhpH \ufffd\ufffdA{g\ufffd\ufffd\u0016\u073eWGJE\ufffd\ufffdZ\ufffd\ufffd\u001a\ufffd^\ufffd\ufffd\ufffdb\ufffd8\ufffdMF\ufffdX}\ufffd]\u0019w\ufffd;\ufffd\ufffd\ufffd\ufffd3\u0011\ufffdc\ufffdH\u0507\u0013\ufffd\u001e\ufffd\ufffd;\ufffdg\u05f3\ufffdI\ufffd\ufffd\ufffd\u0017L\ufffd R[%\ufffd\ufffdP=\u0019L\ufffdNdTcZE\ufffd\ufffdD\ufffd\ufffd \u0019\u0015/'\ufffd\ufffdq\ufffd\u001d\u0564%\ufffd3I\ufffdD\u0649-\u001arb\ufffdI\ufffd\ufffdg] RG\ufffd\ufffd\ufffd\ufffd;Y(\ufffd\u0423\ufffdt\ufffd\ufffd\ufffd\u0004\ufffd(\ufffd\ufffd\u0016\ufffd\ufffdDi\u0016\ufffd\ufffdY\u062f\ufffd\ufffd\ufffd \u0487\ufffdT\ufffd\ufffd\ufffdOF\ufffdM\ufffdRoh\ufffd\ufffd\ufffd\ufffd5\ufffdH\u007f\u0019\ufffdk\ufffd\ufffd)\u000ff\ufffd\ufffd\ufffd\fL\ufffd\ufffd\ufffd\ufffd2R\ufffd\u0013\ufffd\ufffd\ufffd\ufffd\u001c\ufffd\ufffd(\u0015C\ufffd\ufffdN\ufffd\ufffd\ufffd\u0004R\ufffd\ufffd\ufffd\f\ufffd\ufffd\ufffd\u0183\"\u0005\ufffd\ufffd\u0016m[cL\ufffdDY\ufffd\ufffdZ2\ufffd\u000b\ufffd\ufffd~\ufffdsc\ufffd\ufffd\ufffd\ufffd-\u001e\ufffd\ufffd\ufffd\ufffdY\ufffd\ufffd\ufffd\b\ufffd\ufffd\ufffd &\ufffd\ufffd\ufffd\u00071\ufffdT\ufffd\u001c\ufffd\ufffdEcq\ufffd\u0410s\ufffd(H\ufffd\u0006F\ufffd\ufffd\ufffdN\ufffd\u001f\ufffd\ufffd\u001c\ufffd\\\ufffd\u007f\ufffd\ufffd($\ufffd\ufffdF\ufffd0\ufffdWMc\ufffd\ufffd\u051c\ufffd\ufffd\u000e\ufffd\ufffd    \ufffd\ufffd\ufffd\u001e\ufffd\u0001$\ufffd2\ufffd?S\u0006\ufffd\ufffd\ufffdRz\ufffdme\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd}\ufffdQ\u025f;\u0004\u02af\ufffd\u0013\u001b\ufffd4\ufffd\u0014S\ufffd\ufffdM\ufffdxYF.\ufffd\ufffdv}:E\ufffd\ufffdH\ufffd'\ufffdL\ufffd\ufffdka\u007f\"\ufffd\u0552 \ufffd\ufffd\ufffd\ufffdSn\ufffd\"n\u001d\ufffd\ufffd\u000b\ufffd\ufffd\ufffdM\ufffd>g\ufffdr-B\u001a\ufffd\u0007\u0012\ufffd'\ufffd\ufffd\ufffd7\ufffd\ufffd\ufffdJ\ufffd\ufffd\ufffd\ufffd7:\ufffd|\ufffd\ufffd\uf683B\ufffd\ufffd\ufffd@o\ufffd^B\ufffd\ufffdN\u04bcb\ufffdr\ufffd\ufffd~\ufffd!>\ufffdE\ufffd\ufffde\ufffdDq\ufffd\ufffd|q8\u0011\ufffd\ufffdov(yX\ufffd\ufffd\ufffd\ufffd\ufffd\ufffdF\ufffd\ufffd\ufffdk\ufffd  \ufffdF2I$\ufffd3$\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffdv\u00127\ufffd\n9\ufffd\ufffd\ufffd-L\ufffd\\\ufffd\ufffd\ufffd\ufffd\ufffd$O\ufffd\u0013\u0005;\ufffdZ5$\u0619\ufffd\ufffd\ufffdd\ufffd-\ufffdu\ufffd/JN\ufffd.\ufffd\u0012\ufffd\u0017\"\ufffd\u0011\ufffd\n\ufffd2KC\ufffd^\u01d0\n\ufffdt\ufffd'\ufffd\ufffd0\ufffd\u0017\ufffdF\ufffd\ufffd(\ufffd\ufffd\ufffdg\ufffdA\ufffd\ufffd\ufffd<}dqR\ufffd\ufffd\ufffd{Y\u0007l\u000e5\ufffd\ufffd\u03e5R\ufffd3\ufffd\ufffd)\u0010)\ufffdpm\u05b8\ufffd\u0006\u0011\u00162\u05f5\ufffd\ufffd:\ufffd\ufffd\ufffd\u0004o\u0019\ufffd\ufffdW\u000f\u0011\ufffdw2D\ufffd#\ufffd\ufffd&\ufffd\ufffd_\u000e\ufffdpO\u0127\ufffdH\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffdR\u0004\ufffd\n&R\ufffd\ufffd\ufffdG89\"6!\n\u04e9\ufffd\ufffd\ufffd,\ufffdD.]\u001d%d}\ufffd\ufffd\ufffd\ufffdL\u0012\u0262\ufffdr\ufffd!\ufffdM\ufffd?\ufffd\ufffd\ufffd\ufffd\ufffd\u000b\u000b\ufffd\ufffd\ufffd^\ufffd|\u001e)\u0013\ufffd\ufffd\ufffd\ufffd%\ufffdo\u0600\u05d6}\u001d(\ufffd\ufffdf\ufffd\ufffddo|\ufffdp,\ufffd\u0006\ufffd\ufffd\ufffd\u0418M#\ufffd\ufffd\ufffd4?G\ufffd\u001d;L\ufffdtW\ufffdo\ufffd\u0007\ufffd\u000f,\ufffd\ufffd#\ufffd\ufffdD\ufffd\ufffd0\ufffd\ufffd\ufffd\ufffd\ufffd\u0015xl\ufffd\ufffd\u0010EW.]\ufffdM\ufffd;\ufffdyJoJ\bR\ufffd}T\ufffd\ufffdUh\u0013'f\ufffd\u0017;\u000e\ufffd+\ufffd\u0018\ufffdT$\u0004>\u0611\ufffdL\ufffdy\u0006\ufffd\ufffd(\ufffd[\u001b\ufffd?\ufffd\ufffd\ufffd\ufffd\ufffd\ufffdz\ufffdu\ufffd\ufffd\u001e\ufffd\ufffd4\ufffdL\ufffd\ufffd\u0499KO\u030e\ufffd\ufffdH\ufffd_5\ufffd\ufffdo\ufffd\u0019\u0019\ufffd\ufffd2\ufffd>\ufffdEZ\ufffd3\u0537q@\ufffd7\ufffd.\ufffd\ufffd{tj\ufffdK.\ufffd\ufffd\ufffd\ufffd\ufffdV\ufffd\ufffd\ufffdnd?\ufffdD<\ufffd\ufffd\ufffd\u001b\ufffdpT\ufffd;\ufffd,\u0012O\ufffd\u000b\u0011\ufffd\ufffd\u0006\ufffd\ufffdu(\u0012u@\ufffd\ufffdG\ufffd\ufffd\ufffd\ufffd\ufffd/\ufffd\ufffd\ufffd\u0288\ufffd)\ufffd\ufffd\ufffd)S\u0007\ufffd\ufffd\ufffd\ufffd\"I\ufffd_\u0006T\ufffd7\ufffd\ufffd\ufffd0\ufffd\u0007\ufffd\u000e\ufffd\ufffd)?\ufffdS\u001a\ufffd\ufffd\ufffd\u0006S\ufffdC\ufffd\ufffd;[/?\ufffd\ufffd-7CZ\ufffd\ufffd\ufffd\ufffdu\ufffd\ufffd\ufffd\ufffdE\ufffd\ufffd\ufffdh4\ufffd\ufffd\u0005\ufffd\ufffd#\ufffd\ufffd\n\u001cH)\ufffdhT\ufffd\ufffd\ufffd\ufffd\\\ufffdr7  \ufffd.\ufffd\u0012)\u0013\ufffdJI\u0019\ufffd)\u000f\ufffd\ufffd}M\ufffdh\ufffd\ufffd@\ufffd\ufffd\ufffd6\ufffdb\u0010DS\ufffd\ufffd=\u0018tf0\ufffd\ufffd    \ufffd\u0016\ufffd$\ufffd\u0011\n/gq0\ufffd\u001b\u0014\u000e\ufffdm:\ufffd\u001d0\ufffdU\ufffd#O\ufffd\ufffd\ufffdB\ufffdA\ufffd\ufffdo\u07f1<\ufffd\u0006\ufffdBw\u01b0F4\ufffd\u0004\ufffd\ufffd)\ufffd1,gQ\u0237$\u8b29\u7ef81D\u001ei\u00111)?x6\u0503^9\ufffd]\u06f6\u0011)\u001cY3r\ufffdif\ufffd\u06d6\ufffd\u0469\ufffd'\ufffd/\ufffd\ufffd\ufffd\ufffd\u0626QLd:\ufffd1s\ufffd\ufffd'G\ufffd\ufffd\ufffd\ufffdg\ufffd}\ufffd?\ufffda\ufffd\ufffd\u0001\ufffd1\ufffdj\ufffdEl\ufffd'\ufffd\u0519'\ufffd\ufffd\u0015\u007fHh\u0005\u000b\ufffd\ufffd\ufffd\u007fR\u0798\ufffdd\u0017Bc\ufffd\ufffd6\ufffd\u001cE\ufffd/\ufffdB\ufffd\ufffd\ufffd\n  \ufffd\ufffd1\ufffdz?f\u0004\ufffdD\ufffdQ9/\ufffd\u0001\ufffd\ufffd\ufffd\ufffd;~\ufffd\ufffd  \ufffdNM\ufffd\ufffd?\ufffd\ufffdtd\ufffd\ufffd~\ufffdS\u001d\ufffd\"\u000e2}\u0013z\ufffd\ufffd]\ufffd\ufffdqg#>\ufffd\ufffd\u000f)/\ufffd\ufffd\b#\ufffd\b\ufffd\ufffd\u0018@\ufffd.\ufffd6\ufffd\ufffdqd8\ufffd\ufffd\u084d\ufffd\ufffd\ufffdu\ufffde\ufffd#\ufffdX\ufffd\ufffdMM3\ufffd\ufffdV\ufffd5\ufffdw\\\ufffd\ufffdU\ufffd\ufffd^\u05d6H7\b>\u000b\ufffdT\ufffd/\ufffd\ufffd\ufffd\ufffd\ufffdH\u00175=S\ufffd\ufffd\ufffdT\ufffde^5\u0236\ufffd]\u0013D\ufffd\ufffd\ufffds\ufffdy%\ufffd\ufffd77\u0005\u0010is\ufffdr\ufffd82&\ufffd\ufffd\ufffdz\ufffd\ufffd}\u000be\ufffd;e+\u0efd&J\ufffd9i!\ufffd\u0012\ufffd\u0004\u0010\u001c-\ufffd\ufffd\ufffd6\\D\ufffd&\ufffd\n]\ufffd\u0536\ufffdj\ufffd\ufffd@\ufffd,o\ufffd\ufffd\ufffd\ufffd\u0314\ufffd!U\u0011\ufffd\u0007\ufffd\u0006\ufffd\ufffd\ufffd;\ufffd1\ufffd?\ufffdM\n  \ufffd@o\ufffd\u001f)\u001d\u0019\ufffd\ufffd<\ufffdB\ufffd\ufffd\ufffd\ufffd\f\u001b\u000e}\ufffd?\u0010SD\ufffdvd\u0006\ufffdp_$JQ\ufffd\ufffdU\ufffdt\u0010\ufffd)\ufffd\ufffd\u0140\ufffdN\ufffdRL;sA\ufffd\u001b\u000b\ufffd\u01b0q\ufffdA\ufffdw\ufffd\u036d0+hm|a\ufffd#]\ufffdb\u0723F\ufffd\ufffd\ufffd?l_\ufffd\ufffdH\u95bf\ufffd\ufffdg\by\u000b\ufffd\ufffd|Djx$\ufffd\ufffd\ufffd\ufffd\ufffdS'\u001f0\ufffdT<\ufffd\ufffd\ufffdA\ufffdY\ufffdfm\ufffd\ufffdY\ufffd_\"j\u0017k\ufffd+Mr\ufffd\u07d5\ufffdv\u0011\ufffd\u0010\ufffdv\ufffdS?\nc@\ufffd?\ufffd\ufffd\ufffd\u0005ym\ufffd\ufffd\ufffd\bM\ufffd\ufffd\ufffd\ufffd\ufffdk\ufffd\u0001\u001b\ufffdD\ufffd/\u0569e9\ufffd\ufffdK\ufffd\ufffdWu\ufffd\ufffd\u00103MoT\ufffdl\u0011\\  GgL\u0209\u0012\ufffd\ufffdR\ufffd\ufffd\ufffd\ufffdF\ufffdR\ufffd\u0006\u0016\ufffd9\ufffd\ufffd\ufffd\ufffd:9\ufffd\ufffde\ufffd\ufffd\u0014JU/\u0017[\ufffd\"\ufffdDH\ufffdd\ufffdSRp\ufffd\ufffd\u00b2\ufffd\ufffdP.)\u001d\ufffdwY\ufffdb\ufffd\ufffd\u000b\u000e\ufffd8\u0014(\u0011\ufffd\ufffd\ufffd\ufffd\ufffd1\ufffd\ufffd=\ufffd\ufffd\ufffdc\f\ufffdI\ufffd\ufffd\ufffd\ufffd\u000b\ufffd\ufffd\\6\ufffd\ufffd\ufffd#]G\ufffd\u001d\ufffd>\ufffd\ufffdB\ufffd\ufffd\ufffdch4\ufffdH\ufffd.8\ufffdi\ufffd\u0013\ufffd\u048e\ufffd\ufffdG,b\ufffd\u00048\ufffd\ufffd\u0555k\u001c\ufffdB\ufffdtR\ufffd\u0017\ufffd\u0019T\ufffd!\u0708\ufffd\ufffdc\ufffd\u0011\ufffdy\ufffd\ufffd\ufffd~\ufffdn\ufffd\ufffd\ufffd7\ufffd[OW^#\ufffdc\u032f$u\ufffd\u0012\ufffd\ufffd\ufffd\ufffd\ufffd  o4\ufffdD\b\ufffd\u001f\ufffd\ufffd-\"\ufffd\ufffd#\u0001\ufffd\u001d\ufffd\ue1b3)\ufffd\ufffd\u000e\ufffdt\ufffd\ufffd\u0637\ufffd\ufffd  \ufffdI\ufffd7\"\ufffd\u001c2\ufffd\ufffd\ufffd\ufffd\n\ufffd-\ufffdhO#\u001am\ufffd\ufffdNO&N\ufffd\ufffd\u0146\ufffd\ufffd\ufffdo\ufffd\ufffd\u054b\u001eon6%\ufffdG\ufffd\ufffd\ufffd-\ufffd\ufffdE2\ufffdC\ufffd\ufffd\ufffd\ufffd<\ufffd\ufffd\ufffd%\ufffd\ufffd\ufffd\ufffd\ufffd'jc:\u000e%\ufffd\ufffd\n\ufffd\ufffd\ufffd\ufffd\ufffdS$\u0007D|\ufffd\ufffd\ufffd0\ufffd\ufffdIh^QJ~j\ufffd\ufffd;\ufffd\ufffd\ufffdQ\ufffd\ufffd\bS\ufffdZ\ufffd\ufffdw\f\u0082   \ufffd\ufffd\u02a6x\"\ufffd>\u0014    _\ufffd\ufffd-\ufffd>\ufffd\u00185\ufffd\ufffds\ufffd\ufffd[\u05e9\ufffd>\u001b\ufffd\ufffd^&\ufffdv\u0011\ufffdLyl\ufffd\f\ufffd\ufffdA\ufffdR#\ufffd\ufffd\u001a_\ufffd6e\ufffd\ufffd\ufffdL\ufffd\ufffd\ufffd\ufffd\ufffd)Ni\ufffd\u0016\ufffdx\ufffdYKj\bq\ufffd\ufffd\ufffdN\u0016\ufffd\u05e9\u0015\ufffd7!\ufffd\ufffd\ufffdGw\ufffdKn5\f\ufffd~\ufffdU\ufffd\ufffd\ufffd$ZD\ufffd\ufffd\ufffd\ufffde\ufffd&\ufffd\ufffd\ufffd0\ufffd6\ufffdt\ufffd5\ufffdag|\ufffd\ufffd\u007f\n_\ufffd\ufffd|\ufffd\ufffdU\ufffd\u0770\ufffdC\ufffdG\ufffd-\ufffd\ufffd\u0089\ufffd,\u001e}\u0004i\ufffd\ufffdw\ufffd\ufffdaz,_ZF\ufffd\ufffd\ufffd\ufffd\ufffdm\ufffd|\u0016\u2ac5\u0010\ufffd\ufffddD-i\ufffdL\ufffd\ufffdE\ufffd\ufffda?\ufffd\"\u001f\ufffd\ufffd\ufffd\u001e\u0004\u001b\ufffdD\u0018n\ufffd\u001af\ufffdX\ufffd3\ufffdR\ufffdiyl_\ufffd>\ufffd\ufffd;\ufffd\ufffd;\u0005v.\b\ufffd\ufffd\u0563\ufffd\ufffd\u001f\ufffd\ufffds\ufffd\ufffdLw\ufffdV_\ufffd\ufffd<\ufffdG\ufffd(\ufffd\ufffdf]R)Dl\ufffd\ufffd2KxmT\ufffdE3\ufffd$'\ufffd\ufffd\u0018\ufffd^\bG\ufffd\u01d0\ufffd\ufffdlwV\ufffd\ufffd)\ufffd\ufffdt\ufffd\ufffd\ufffdjm\ufffd_zO_\ufffdO\ufffd+4\ufffd.\u394b\u02faz\ufffd$\ufffd\u05fa\u0016\ufffd\ufffd~\ufffd\ufffde\ufffd\ufffd\ufffdn\ufffd\ufffd\ufffd\ufffd\ufffd=\ufffd\ufffd\ufffdr\ufffd\ufffd\ufffd\ufffd\u0019\u00149\ufffd\ufffd:\ufffdx&\ufffd+k\ufffdeOM;z\ufffd\ufffd\u000fu\u0015X\u0011\ufffd\ufffd\ufffd\ufffd6\ufffd\ufffd1t\ufffd\u0007\"\ufffd\ufffd\ufffd\ufffdz\ufffdf\u0017\ufffd:\ufffd\u001d\u0019\ufffd\u0005;\ufffdb\ufffdQ\ufffd\ufffd\u0018mu\ufffd\ufffdp\ufffd\ufffdI\ufffdZ\ufffd\ufffd\u0013.\ufffd\ufffd8\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\u007f\ufffd\ufffd\u0014\ufffdd\ufffd\u001d=\ufffd\ufffd\ufffd\ufffdHJ  \ufffdzlG\ufffd39\ufffd)x&\ufffd\ufffd0\ufffd<@A\u0010E\ufffd\ufffdi\ufffd\ufffd\ufffd\ufffd#\ufffdo\ufffd\ufffd\ufffd5\ufffdA\ufffd#\u0431\u0017v\ufffd-\u0018E\ufffdB|>\ufffd%\ufffd6\u0013\ufffdjL\ufffd\ufffdD    \ufffddVA\ufffd^Q\ufffdL\ufffd\ufffd3\u0013B\ufffd\ufffd\ufffds\ufffd\ufffd\ufffd\ufffd\ufffd;\ufffdc\ufffd\u0018\\9\ufffd\\\u0131^\ufffd\u0012\ufffd\ufffd\ufffd_I\ufffd\ufffd\u0011\ufffd\ufffd\ufffd\ufffdpW\ufffd(\ufffd\ufffd\ufffd:\ufffd\ufffd\ufffd0/\ufffd\ufffd\u0114W\n  \ufffdFy&\ufffdo.Itp_u_YYL\ufffdR\ufffd!\u000e\u000fE\ufffd\\W\ufffd\ufffdD\ufffdL\ufffdV\ufffd\u0018\ufffd\ufffdF\ufffd<\ufffd\ufffd\"\ufffd:\ufffd<=/\"n\u001f9\ufffde\u000bE;\ufffd-  y\ufffdk\u001a\ufffd\u04dfT\ufffd_%\ufffd\ufffd\u053dL\ufffd|b2\n\ufffdy\ufffd\ufffd\ufffdr\ufffd\ufffd5\u001e\ufffdb\ufffd\ufffdN\ufffdZ}~ \ufffd6\u001db\ufffdLD\ufffdMAd\ufffdQ\ufffd\ufffd%\ufffd\u001c\ufffd\ufffd\ufffd\ufffd(\ufffd\ufffd;x \ufffdt0\ufffd\ufffd\ufffd\ufffd\ufffdn.\"\ufffdM\ufffdq\u001d\ufffd\ufffdj\ufffd\ufffd\u001a\ufffd<\ufffd6\ufffdco\ufffdb\ufffdF\u0011tGQ\ufffd\ufffdi>\ufffd\u0004\ufffd\ufffd\ufffd\ufffd'J\ufffd&\u0179\u06fc\ufffdq.\u04ab\ufffd5\u0013\ufffd\ufffd\u02add?\u0015\ufffdj\u00167i\ufffd\ufffdz\ufffdQ:\ufffd<\ufffd\ufffd\ufffd\ufffd/\u0013\ufffd\u0011r\u039e\ufffd\u0014'\f>\ufffd\u001e\"j \ufffd\ufffd6\ufffd4\ufffdg\ufffd\u04ac\ufffd\u001d\ufffd\ufffd\u0005d\ufffd\ufffd\u0411Q\ufffd\u3e6d\ufffdw\ufffd\ufffd  t\ufffd\u000e\u000f\ufffdLy\u0006|'OJ\u0015\ufffdJ\ufffd\ufffd\u0014\u0017\ufffd}\ufffd\u001a\ufffd\ufffd\ufffd-7 \\\ufffd|iB \ufffd\ufffd\ufffd=\ufffd\ufffdT!\ufffdP\ufffd\ufffd\u01b0\ufffd%;\ufffd\ufffdhv\ufffdE^\ufffds\ufffd\ufffde_|\ufffd\ufffd\ufffdZ_\ufffd\u0004\ufffd\ufffd\ufffdK>\u0001\ufffd\ufffd\ufffd<\ufffdu\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd~\ufffddy\"\u03e9\u000b\u0019\ufffd\ufffdQ\u007f\ufffd\u0006\ufffd\ufffd\u000f\ufffd\ufffd\ufffd2\ufffd\u05f5\u001aO\ufffdv\ufffd\\\ufffdZ\ufffd\ufffd\u000e\u001d\ufffd\ufffdk\u06cb\ufffdH)*\ufffd\ufffd\ufffd\ufffdc\ufffd\ufffd\u0007z\ufffd\ufffd4\ufffd\u0012\ufffdr\u007fh\ufffd\ufffd\u0382\\J\ufffdq\ufffd\ufffdf\ufffdw\ufffd\ufffd\ufffd\ufffd]^\u001c!\ufffd'c\ufffd\u001ez4l\u001asJ\ufffd_\ufffd\\Nb\ufffd\ufffdp\u007f\\\u0010\ufffd\ufffd\ufffds\ufffdc\u0006\ufffd\ufffd\u007f%\ufffd\u0014~\ufffd\u0007{\ufffd\ufffd\n      string(5) \"https\"\n      [\"path\"]=>\n      string(88) \"/prod-user-uploaded-images/MENU-ITEM-IMAGES/7ebdc1fd4ef261154d5dddbc958dcb48534b6d27.png\"\n    }\n    [\"response\"]=>\n    array(4) {\n      [\"instance\"]=>\n      string(32) \"0000000064a40c2e000000004c9763f1\"\n      [\"statusCode\"]=>\n      int(400)\n      [\"headers\"]=>\n      array(8) {\n        [\"X-Amz-Security-Token\"]=>\n        string(7) \"[TOKEN]\"\n        [\"x-amz-request-id\"]=>\n        array(1) {\n          [0]=>\n          string(16) \"C92EB847871E8801\"\n        }\n        [\"x-amz-id-2\"]=>\n        array(1) {\n          [0]=>\n          string(76) \"H8vjDCd7FsIUFGWUiwUhB9l2286SRT8k+bdecNY4hUEYjYrPjLPHBKXJuCNh7Zxk2roXx6VY+xs=\"\n        }\n        [\"Content-Type\"]=>\n        array(1) {\n          [0]=>\n          string(15) \"application/xml\"\n        }\n        [\"Transfer-Encoding\"]=>\n        array(1) {\n          [0]=>\n          string(7) \"chunked\"\n        }\n        [\"Date\"]=>\n        array(1) {\n          [0]=>\n          string(29) \"Wed, 28 Oct 2015 03:34:56 GMT\"\n        }\n        [\"Connection\"]=>\n        array(1) {\n          [0]=>\n          string(5) \"close\"\n        }\n        [\"Server\"]=>\n        array(1) {\n          [0]=>\n          string(8) \"AmazonS3\"\n        }\n      }\n      [\"body\"]=>\n      string(382) \"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n  AuthorizationHeaderMalformedThe authorization header is malformed; the Credential is mal-formed; expecting \"<YOUR-AKID>/YYYYMMDD/REGION/SERVICE/aws4_request\".C92EB847871E8801H8vjDCd7FsIUFGWUiwUhB9l2286SRT8k+bdecNY4hUEYjYrPjLPHBKXJuCNh7Zxk2roXx6VY+xs=\"\n    }\n  }\nInclusive step time: 0.06715989112854\n<- Leaving step sign, name 's3.put_object_url'\nno changes\n  Inclusive step time: 0.067346096038818\n<- Leaving step sign, name 'signer'\nno changes\n  Inclusive step time: 0.067626953125\n<- Leaving step sign, name 'retry'\nno changes\n  Inclusive step time: 0.068167924880981\n<- Leaving step build, name 's3.content_type'\nno changes\n  Inclusive step time: 0.068434953689575\n<- Leaving step build, name 's3.checksum'\nno changes\n  Inclusive step time: 0.068613052368164\n<- Leaving step build, name ''\nno changes\n  Inclusive step time: 0.068833112716675\n<- Leaving step build, name 'builder'\nno changes\n  Inclusive step time: 0.069123983383179\n<- Leaving step validate, name 'validation'\nno changes\n  Inclusive step time: 0.069763898849487\n<- Leaving step init, name 's3.location'\nno changes\n  Inclusive step time: 0.070501089096069\n<- Leaving step init, name 's3.save_as'\nno changes\n  Inclusive step time: 0.07062816619873\n<- Leaving step init, name 's3.source_file'\nno changes\n  Inclusive step time: 0.070780992507935\n<- Leaving step init, name 's3.ssec'\nno changes\n  Inclusive step time: 0.071077823638916\n\"Error executing \\\"PutObject\\\" on \\\"https:\\/\\/s3-ap-southeast-1.amazonaws.com\\/prod-user-uploaded-images\\/MENU-ITEM-IMAGES\\/7ebdc1fd4ef261154d5dddbc958dcb48534b6d27.png\\\"; AWS HTTP error: Client error: 400 AuthorizationHeaderMalformed (client): The authorization header is malformed; the Credential is mal-formed; expecting \\\"\\/YYYYMMDD\\/REGION\\/SERVICE\\/aws4_request\\\". - <?xml version=\\\"1.0\\\" encoding=\\\"UTF-8\\\"?>\\nAuthorizationHeaderMalformed<\\/Code>The authorization header is malformed; the Credential is mal-formed; expecting \\\"<YOUR-AKID>\\/YYYYMMDD\\/REGION\\/SERVICE\\/aws4_request\\\".<\\/Message>C92EB847871E8801<\\/RequestId>H8vjDCd7FsIUFGWUiwUhB9l2286SRT8k+bdecNY4hUEYjYrPjLPHBKXJuCNh7Zxk2roXx6VY+xs=<\\/HostId><\\/Error>\"\n``\n. Yes right. I messed up a little in parameters. My bad.\nThanks for the help.\n. ",
    "IamAbhiKaushik": "Hey, can you tell me how did you solve the error? \n. ",
    "KevinM2k": "For me, the error was simply having the key as 'credentials'. It should be 'credential' without the s on the end.. ",
    "soukicz": "It should throw exception or download empty directory - I am not sure what is consistent with other methods.\n. If I want to (for example) download 100 000 files from S3 and process them, I can identify them by my key. It is concept from Guzzle (https://github.com/guzzle/guzzle/pull/1203) - I can process huge amounts of requests with low memory footprint.\nCurrently I would have to mantain second array as map from my keys to numbered array.\n```php\n// usage with my PR\n$client = new \\Aws\\S3\\S3Client();\n$commandGenerator = function () use ($client) {\n    foreach (some_query() as $data) {\n        $s3Path = 'prefix/' . $data['id'];\n        $command = $client->getCommand('GetObject', [\n            'Bucket' => 'bucket',\n            'Key' => $s3Path\n        ]);\n        yield $s3Path => $command;\n    }\n};\n$commands = $commandGenerator();\n$pool = new \\Aws\\CommandPool($client, $commands, [\n    'concurrency' => 15,\n    'fulfilled' => function (\n        \\Aws\\ResultInterface $result,\n        $iterKey\n    ) {\n        // $iterKey contains S3 path and can be used\n    },\n]);\n$promise = $pool->promise();\n$promise->wait();\n```. @imshashank yeah, I thought that too. It might be one of those bugs, that are around for so long that they are considered feature :)\nBut it is currently inconsistent with Guzzle Promises and with is own before callback - https://github.com/aws/aws-sdk-php/commit/aeb44703298d0868d7a6970158ac6314d9417060#diff-2c9e9a3c5ebdbfa8c9268af99ad83f3a If someone was using before callback, it has different key than resolved promise.\n. ",
    "bgruszka": "Ok. I tried, but the result is the same.\nendpoint http://localhost:9444/s3/ finished with request to http://localhost:9444/bucket (without /s3)\n. ",
    "roborourke": "This is possible in the node sdk, and the client endpoint object contains the path property. I don't really get why it should be stripped out for a custom endpoint.. ",
    "jozeperez": "That worked! Thanks. Question: Why did this happen?\n. ",
    "jbhamilton": "You are correct my friend. Thanks, you can close this.\n. ",
    "motsmanish": "'region' => 'eu-west-1'\nThanks & Regards,\nManish Motwani\nhttp://ManishMotwani.com http://manishmotwani.com/\nOn 27 October 2015 at 21:01, Jonathan Eskew notifications@github.com\nwrote:\n\nIn which region is the bucket you're uploading to located?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/aws/aws-sdk-php/issues/805#issuecomment-151542094.\n. us-east-1 worked for me.\n. \n",
    "chrisradek": ":shipit: \n. :shipit: \n. :ship: :it:\n. :shipit:\n. I'm not sure which way is preferred, but the JavaScript SDK uses the guidelines in the DynamoDB docs for calculating the delays for the DynamoDB service, and uses something close to the AWS guidelines as the default for other services (instead of multiplying by 100 ms, it multiplies by 30). \n. :shipit:\n. :shipit:\n. :shipit: \n. :shipit:\n. :shipit:\n. :shipit:\n. :shipit:\n. :shipit: :+1: \n. :shipit: \n. :shipit:\n. :shipit:\n. :shipit:\n. :shipit:\n. :boat:\n. :shipit:\n. :shipit:\n. :shipit:\n. LGTM\n. You have an extra trailing comma here.\n. ",
    "Alorel": "@jeskew It's mostly about personal preference on not having to download packages you don't need.\nIn my specific use case (which is not, by far, applicable to a lot of users) having fewer components to download would be a significant speedup as my company requires its employees to live-sync all changes to a virtual host on the development server instead of using localhost. Because Composer's update process involves fully removing a package and then simply replacing it with a new one, the IDE ends up taking a lot of time deleting the entire SDK from the remote server and then re-uploading the new version even though, in many cases, only a few files have changes in them.\n. ",
    "texdc": "Pros:\n- Use only what you need (the AWS mantra)\n- If one AWS service changes, the update is localized\nCons:\n- It's a pain to split everything up and maintain version history (but, it's possible)\n- Increased management complexity\nGenerally speaking, I'd favor keeping packages specific to their function/purpose instead of lumping everything together.  Obviously, there are drawbacks.  I'm probably overlooking other issues/points.\n. ",
    "cdaguerre": "Having a single package bloats autoloading and actually has runtime performance impacts, which in my opinion are equally important to dev time comfort.\nFor instance, your composer classmap gets an extra 358 classes only to use S3.\nMaybe at least a S3 subtree split would make some people happy!\nIt would however require a sort of common package for shared classes, of course.\nI'd definitely +1 the possibility to require specific components only.\n. ",
    "rlweb": "+1\n. ",
    "ericofusco": "I'd love to see this split on composer too. I'm not sure it's worthy like everybody said, but I believe most of the applications use just few services.  \nFor my case I use S3 and DynamoDB only.\n. ",
    "jjok": "+1. ",
    "timbotron": "+1. ",
    "AviBueno": "+1\nHaving to pull the entire SDK just to be able to send emails via SES is an overkill.\nOn a different project I use SQS and SES and it's still an overkill to pull, deploy and autoload heaps of unused code.. ",
    "mattkamsler": "+1\nI actually came here to see if this had been suggested yet. I am working on a project that just requires S3. I think it would be worth looking into.. ",
    "chippyash": "+1\nThe AWS composer package is so big that it blows composer memory limits on local and build servers and we have to employ the COMPOSER_MEMORY_LIMIT=-1 trick just because of it.  Composer run time is significantly increased because of the size of the lib.  \nWe've just had a situation where composer gave up trying to install aws-sdk because it timed out and left us with a broken package that broke the site completely.  Fortunately we had a copy of it elsewhere and was able to upload it manually.\nLike others, we really only use S3 so making a separate package for it makes a huge amount of sense to me.\nIf you can publish the dependency graph for S3 usage, I'd even be prepared to write a script that repackages just the stuff required.. I've got a package that just takes in the S3 stuff now with a utility to run to check for updates to the main package. Be useful if one of the aws guys could review it?. ",
    "chumanfu": "+1. ",
    "Pierstoval": "I also agree about splitting the SDK into components.\nThis would still be possible with a simple git subtree, exactly like what @Symfony does. Then you can keep the same workflow for upgrades, versions, etc., and hook in the git/github process to apply the modifications to the subsplits.\nMaybe you could watch this conference: https://www.youtube.com/watch?v=ZVsDA6GhKOU to get some inspirations \ud83d\ude42 . > (all of Symfony's sub-packages get bumped whenever the framework master package gets bumped, regardless of whether there's any breaking changes in the sub-trees, the actual packages - these should have been individually versioned to ensure a meaningful upgrade process for developers; it's just laziness, really.)\nIt's not laziness. It's just that the framework (symfony/symfony) is actually not \"one full package\", it instead is dozens and dozens of packages that should each follow the same release processes and rules. Therefore follow the same versioning policy.\nThere are 3 active branches for Symfony, so this means that each month, at least 150 git tags are created, bugs on lower branches should be ported to more recent branches if the bug exists, so in the end, the majority of packages will benefit from at least one new commit on each release.\nAnd if one release does not bring anything to a package (this was the case for symfony/asset:v4.2.0...v4.2.1 for example, check here), then there's no harm in adding a tag. This helps the community and the developers remember that x.y.z should be the same for all the packages they use. This is also highly efficient with symfony/flex that can use the extra.symfony.require Composer parameter to restrict to a specific Symfony version that will apply to EVERY package that is in the main symfony/symfony repository.\nI think having such system for AWS could be almost painles, since AWS don't really need full BC and don't seem to need to support \"older\" branches, therefore there will only be one tag per package, and global maintenance can still be in the main repo (and it must, actually).\nFabien Potencier created splitsh to ease the setup of such system, but git subtree could be nice too, it's just a bit more complex, but completely feasible.\nIt saddens me that we have to download 11MB of package when the SDK releases a new version, while we could download like 200KB instead for just the S3 client (it's more than 50 times lighter!).. ",
    "mindplay-dk": "+1\nHundreds of largely unrelated components and services rolled into one package.\nIf there's a breaking change to any one of these, every user of any of these hundreds of dependencies will be affected.\nI mean, for crying out. This is not how Composer works. This is just not how it's done.\nI'd have thought a huge company like Amazon would know better \ud83d\ude44\n\nThis would still be possible with a simple git subtree, exactly like what @symfony does\n\nI'd like to not that this is a really bad idea as well - it only solves the somewhat esoteric problem of deploying too many files (which is mostly not really a problem, assuming you have byte-code caching enabled, and you let Composer generate an optimized auto-loader, which you should be doing in production anyhow) and doesn't address the versioning issue. (all of Symfony's sub-packages get bumped whenever the framework master package gets bumped, regardless of whether there's any breaking changes in the sub-trees, the actual packages - these should have been individually versioned to ensure a meaningful upgrade process for developers; it's just laziness, really.)\nI can't really believe I'm out here explaining this to Amazon \ud83d\ude10. ",
    "jrivera294": "Solution to establish an anonymous connection:\nChange:\n        $s3 = new S3Client([\n            'region'  => 'us-west-2',\n            'version' => '2006-03-01',\n            'scheme' => 'http',\n        ]);\nTo:\n        $s3 = new S3Client([\n            'region'  => 'us-west-2',\n            'version' => '2006-03-01',\n            'scheme' => 'http',\n            'credentials' => false\n        ]);\nI still don't know why my local machine can establish the connection, but the ec2 server can't do it without this line.\n. Wow, I had never thought it. It works. Than you very much!\n. ",
    "SlightlyCyborg": "I am getting this same error, but I am passing my root credentials to the Client in PHP.\n. ",
    "dhensby": "This is going to mess with a possible resolution #821 as we need to track the changing of session ids to be able to reliably determine if session data needs to be written\n. DynamoDB's SessionHandler writing empty session when the ID changes is not a limitation of PHP's SessionHanderInterface IMO. It's a limitation of the current implementation of doWrite on the AbstractLockingStrategy, which attempts to reduce data overhead by only sending session data to DynamoDB when the session data has changed.\nIf the session_id() changes, it will create a new record in DynamoDB, but it won't send the data.\nThe solution is for DynamoDB to detect if either the data OR the session ID has changed. This could be done 1 of 2 ways.\n1. Track the originally read session ID and check to see if the written one has changed against it.\n2. Ping Dynamo DB to see if a record with the ID exists and, if not, send the whole data set.\n1 is more elegant in my opinion because if the session ID changes several times (and the session is written each time) the session data should be sent; we don't really need to know if the session ID has changed against the last regenerated ID we just need to know if it changed at all. It also has the advantage of saving reads against Dynamo DB.\n\nedit:\nFurther to my point of why I don't think it's an error in PHP's SessionHandlerInterface: when using standard PHP sessions, this bug is not present.\n. Fantastic, thanks\n. Unfortunately, I'm not sure how best to test this actually works as expected, my attempted test didn't work too well.\n. @jeskew at the moment it works pretty flawlessly on my site without need to have an array mapping.\nThe idea isn't to run two session IDs side-by-side, but more when you want to simply give the visitor a new session ID. so the old session ID doesn't need to be kept.\nsession_regenerate_id ([ bool $delete_old_session = false ] ) if delete old session is true then we definitely don't need to keep it in memory; though the fact it could be kept is relevant.\n. Also, I contest that this is an \"enhancement\", it's a bug because at the moment if you regenerate your session ID whithout changing the session data the data is never sent to dynamodb and so you lose it.\n. Then it'll be written twice. \nWhat circumstances are you thinking it would cause a problem? \n. To be clear, the bug this is fixing is that if a call to session_regenerate_id() is made and the session data is not modified then the session will be lost because the data never gets sent.\nThis is a bug as it is a normal use of the built in PHP function\n. For the avoidance of doubt, this is to fix a bug where session_regenerate_id() used with NO changes to the session data causes empty sessions to be written to DynamoDB.\nWhen using that as context consider the following: it doesn't matter how many times you call session_regenerate_id() during runtime, only the LAST session_id() generated will be stored in DynamoDB (unless you manually force a write). This is because the session is not written when session_regenerate_id() is called but, instead, when PHP's session engine says to write it.\nie:\nsession_start();\nfor ($i =0; $i < 10; ++$i) {\n    session_regenerate_id(true);\n}\ndie;\n^ this will only cause ONE hit to DynamoDB with an ID that is different to the one that was used at the beginning\nTherefore, the point of handling multiple sessions in this instance is not a valuable use of resource.\nWhen evaluating whether the session should be written, we should only care if the session ID we are committing now has changed from the session ID we previously committed.\n. Thanks @jeskew I'll test this when I get into the office. \n. @jeskew OK - I've tested this locally and it behaves as expected now.\nI also cherry-picked my test from https://github.com/dhensby/aws-sdk-php/commit/6f03bf320835beb590cc61ceb80e25b9554a73db and the test suite passed too.\n. :)\n. @mtdowling that was my original approach in #822 \n. ",
    "mikemherron": "After testing this a bit more, I think the issue may be that the delay is being implemented as milliseconds rather than seconds. Usually the output of the test script is as above (once the script starts 'Retrying', it never manages to successfully submit a request again), but have now seen the below happen:\n```\n17:50:56: Finished putItem\n17:50:56: Starting putItem\n17:50:56:Retrying with delay:0\n17:50:56:Retrying with delay:0.05\n17:50:56:Retrying with delay:0.1\n17:50:56:Retrying with delay:0.2\n17:50:56:Retrying with delay:0.4\n17:50:56:Retrying with delay:0.8\n17:50:56:Retrying with delay:1.6\n17:50:56:Retrying with delay:3.2\n17:50:56:Retrying with delay:6.4\n17:50:56: Finished putItem\n17:50:56: Starting putItem\n17:50:56:Retrying with delay:0\n17:50:56:Retrying with delay:0.05\n17:50:56:Retrying with delay:0.1\n17:50:56:Retrying with delay:0.2\n17:50:57:Retrying with delay:0.4\n17:50:57:Retrying with delay:0.8\n17:50:57:Retrying with delay:1.6\n17:50:57:Retrying with delay:3.2\n17:50:57:Retrying with delay:6.4\n17:50:57:Retrying with delay:12.8\n17:50:57:Retrying with delay:25.6\n```\nSo in this one case, it did manage to recover. but just failed again on the next request.\n. I think the delay is being passed to Guzzle in the GuzzleHandler just as it's defined in the RetryMiddleware, but Guzzle actually interprets this as as milliseconds. Hacking in the below change to my local vendor folder (Handler/GuzzleV5/GuzzleHandler.php) seems to resolve the issue (around line 100)\nif (isset($options['delay'])) {\n-        $ringConfig['delay'] = $options['delay'];\n+       $ringConfig['delay'] = $options['delay'] * 1000;\n            unset($options['delay']);\nWhen running the 2 scripts side by side, you can then see the delays being implemented correctly.\nHowever, not sure if this is the correct place for the change?\n. No problem.\nBased on the comment in Waiter (\" // Set the delay. (Note: handlers except delay in milliseconds.\"), feels like the fix should actually go in DynamoDB client where the delay function is provided to the RetryMiddleware?\n$list->appendSign(\n            Middleware::retry(\n                RetryMiddleware::createDefaultDecider($value),\n                function ($retries) {\n                    return $retries\n                        ? (50 * (int) pow(2, $retries - 1)) / 1000\n                        : 0;\n                }\n            ),\n            'retry'\n        );\nRemove the division by 1000?\n. ",
    "danm": "Yeah, I was using NSS as that seems to be what the Amazon Linux AMI is compiled with. Since, I recompiled curl with OpenSSL and then PHP and I am not getting the same error. Thanks for the headsup\n. ",
    "pihish": "Lucky me!!!\n. I am still a little confused on how I can wait on the promises.  I am thinking of promises in a JavaScript context - as callback functions I define when the event is completed.  Can you show me example syntax on how this is done?\n. @jeskew Thanks, but all this does is write to output notes about the individual files as they are being transferred to S3.  I need to make a HTTP call to my endpoint every X number of seconds.  \nA good analogy would be the following JavaScript code using the AWS JS SDK:\n```\nvar s3Bucket = new AWS.S3({\n    params: {\n        Bucket: 'pihishsBucket'\n    }\n});\nvar uploadRequest = s3Bucket.upload(someParameters, function(){\n    // do something when upload finishes\n});\nuploadRequest.on('httpUploadProgress', function(data){\n    console.log('progress is ' + data.loaded + ' of ' + data.total);\n    // make my custom HTTP call every time upload progress changes\n});\n```\nI am essentially trying to mimic the behavior of uploadRequest.on.  I want to do something asynchronously as the upload progresses. \n. I ended up doing something like this\n```\n$lastDatetime = time();\n$manager = new \\Aws\\S3\\Transfer($client, $source, $dest, ['before' => function(){\n    global $lastDatetime;\n    $currentDatetime = time();\n    if(($currentDatetime - $lastDatetime) >= 30){\n        // check that the interval is 30 seconds\n        echo date('g:i:s a');\n        echo \"\";\n        // call endpoint\n        $lastDatetime = $currentDatetime;\n    }\n}]);\n```\nSince I am uploading a lot of files from a folder, I didn't want each file upload to trigger a HTTP call and have the sum of calls crash my server.  So I decided to check the current UNIX timestamp and compare it with a reference UNIX timestamp and only make the HTTP call if the difference is more than 30 seconds.\n. That did the trick.  For anyone else wondering what the end code look like:\n$s3Client>uploadDirectory( '/path/to/local',\n        'kushbucket', '/path/to/s3/location', array(\n            'before' => function(\\AWS\\Command $command){\n                    $command['ACL'] = 'public-read';\n                    $command['ContentDisposition'] = 'attachment';\n             },\n        )\n    );\nIs there a complete list of all keys \\AWS\\Command accepts?  In my V2 code, I also had an explicit \"multipart_upload_size\" => 1000 declaration.  How do I set a custom multipart upload size threshold in V3?\n. Is there documentation somewhere which describes on how to use getObjectAsync()?  The only mention of it I see in the documents - or anywhere on the web for that matter - is at https://docs.aws.amazon.com/aws-sdk-php/v3/api/api-s3-2006-03-01.html#getobject.  According to this, it has the same method signatures as getObject().  \nSpecifically, I'd like to know how I can pass in the equivalent of a 'before' callback function like in the Transfer Manager.  This is what I have using the Transfer Manager that I would like to mimic using the getObjectAsync() method:\n$lastDatetime = time();   \n        return new \\Aws\\S3\\Transfer($this->client, 's3://mybucket/path/to/folder/',\n            'c:/path/to/local/', array(\n                'base_dir' => '/path/to/folder/',\n                'before' => function()use(&$lastDatetime){\n                    $currentDatetime = time();\n                    if($currentDatetime - $lastDatetime >= 30){\n                        // Do something every 30 seconds during the download\n                        $lastDatetime = $currentDatetime;\n                    }\n                }\n            )\n        );\nEssentially, I want do something every 30 seconds during a long folder (or in the case of getObjectAsync() a long file) download.\n. Thanks, I was able to make it work using the progress callback.\nIs there a similar progress callback on the Transfer Manager (code example from my last post)?  I'd like to be able to show real-time download progress on a folder transfer too if possible.\n. This is giving me the total / downloaded progress of each individual file.  Any way, I can get the total size of the folder I am downloading, and the aggregate downloaded size so far for everything within that folder?\n. ",
    "tractorcow": "\nThose are all scalar values that assume the session ID will not change.\n\nIsn't this assumption the problem? Because it seems that the user can have a new ID regenerated for them.\nhttp://php.net/manual/en/function.session-regenerate-id.php\nThe docs here clearly state that the ID is changed, although the existing session should still be retained, but recorded alongside this ID. If you are throwing away the session data after this change, then PHP session behaviour is not being maintained properly.\n. Ok, I think I understand. :) I hope @dhensby can find a solution that solves than issue then.\n. ",
    "ondrejhlavacek": "@jeskew It happened in versions 3.9.4 and 3.10.0. It might have started after upgrading from version 3.2., I don't recall this happening in 3.2.. It occurs completely randomly. Series of following attempts to perform the same action didn't throw any error. \n. @jeskew sure, I just blanked out some sensitive params and deleted the 2 parent exceptions that didn't have any aws-sdk-php related info\nhttps://s3.amazonaws.com/keboola-logs/debug-files/2015/11/12/2015-11-12-12-33-35-5644790faa91a-exception-excerpt.html\n. \u00a0@jeskew Deployed to our production environment. Will update this issue when it fails again. Thanks!\n. @jeskew No failures after the update! Keeping an eye on that.\n. @jeskew There were two apps running the 3.10.x version, one got upgraded to dev-master on Nov 16, the other to 3.11.3 on Dec 3. We haven't seen similar errors in any of the two apps since mid November. Considering it solved. Thanks a lot!\n. @jeskew We had 2 similar errors today (3.11.4)\nhttps://s3.amazonaws.com/keboola-logs/debug-files/2015/12/11/2015-12-11-13-40-39-566ac4472ca1c-exception-excerpt.html\n. Thanks, trying out the latest version. Btw I'm seriously impressed how quickly you resolve issues here. Good job!\n. Oh, I'm really sorry for this! Closing the issue.\n. I'd like to reopen this issue, maybe I can shed some more light. We're still trying to figure out this issue together with AWS Support and before tcpdumping traffic on a traffic-heavy node this might be worth a shot. \nBasically what happens is that the first attempt ends with cURL error 52: empty reply and all following retries are immediately terminated as 400 Bad Request (RequestTimeout).\nHere's a dump of some communication with AWS support, S3 logs, AWS debug logs - https://gist.github.com/ondrejhlavacek/6405f56ce8e0105bb1f9.\n. Yes, it's a resource handle. I'll try updating guzzle to the 6.2.0 version. Thanks a lot, will let you know if it helped.\n. I have no security token, only the key id  and secret key from IAM. The\nserver I'm trying the aws-sdk-php on has no aws profiles set uo and\neventually will use STS to authenticate, but for development I opted for\nmanual creation via IAM. My local machine I ran the CLI on has aws\nprofiles, but they again only have key id and secret key (and they are the\nsame as on the dev server).\nOn Nov 1, 2016 1:35 AM, \"cjyclaire\" notifications@github.com wrote:\n\n@ondrejhlavacek https://github.com/ondrejhlavacek I just tried\ngetAuthorizationToken API and it works fine for me, I suspect the issue\nis with your $awsCredentials. The error message suggests problem with\nsecurity token, and your code new Credentials(KEY_ID, SECRET_KEY seems to\nignore token parameter, that might be the problem. So you would need to do\nsomething like:\n$awsCredentials = new Credentials(KEY_ID, SECRET_KEY, SECURITY_TOKEN)\nAlso, looking at your CLI command, a profile credentials is used. You\ncould also try using profile\nhttp://docs.aws.amazon.com/aws-sdk-php/v3/guide/guide/configuration.html#profile\nwith instrcutions here\nhttp://docs.aws.amazon.com/aws-sdk-php/v3/guide/guide/credentials.html#using-the-aws-credentials-file-and-credential-profiles\n?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/aws/aws-sdk-php/issues/1114#issuecomment-257362868,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAeYC6Fq17T1SkDjgT_X-NTqTziXNve9ks5q5ibpgaJpZM4KkUQl\n.\n. Oh my... Shame on me. You were right from the beginning. The KEY_ID was invalid, copied it from a test that tested invalid credentials and didn't notice it. I am so sorry, everything works fine now.\n. \n",
    "huyanping": "Thanks, I will talk to my colleagues about this\n. ",
    "piyushpatil027": "Hi Jonathan,\nHope you are doing great. Thanks for reply me. Issue has been solved.\nThanks.\nOn Thu, Nov 19, 2015 at 10:25 PM, Jonathan Eskew notifications@github.com\nwrote:\n\nClosed #829 https://github.com/aws/aws-sdk-php/issues/829.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/aws/aws-sdk-php/issues/829#event-469397912.\n\n\nThanks & Regards\nPiyush Patil\nSoftware Engineer\nSky Id : piyushpatil27\nEmail Id : piyushpatil027@gmail.com\nMobile No : 09096871715\n. ",
    "vlajos": "Hi @jeskew,\nThank you.\nWe want to use php on both sides so probably we will implement something for ourselves.\nI just wanted to check if there is something already existing.\nBest regards,\nLajos\n. Thank you!\n. Thank you!. ",
    "forktheweb": "I'm working on a client-side token based encryption mechanism that plays nice with the sdk and follows the json web tokens standard.  I'll let you know when the code matures a bit... shouldn't be too long :)\nSImilar to this in concept:  https://github.com/lexik/LexikJWTAuthenticationBundle/blob/master/Resources/doc/index.md\n. ",
    "davedevelopment": "@jeskew any news you could share on this? I'm about to embark on a feature that will be needing something like this and I don't want to have to start rolling my own if it's something that will make it's way in shortly. Also, for me, compatibility with the java and ruby SDKs would be a bonus.\n. @jeskew thanks for the clarification, I'll probably roll my own solution for now and re-encrypt everything once I can rely on the SDK.\n. Having just re-read that issue, I've at least been able to create something that is compatible with the current ruby implementation, so going forward I might be ok, assuming the PHP SDK would be open to compatibility with the current ruby implementation as well.\n. @leith my code isn't all that integrated with the SDK like the other libs, it's sitting just above it. I'm happy to share this, though it's integrated with closed source code, so would rather do so by email, drop me a line and I'll sort something out for you.\n. ",
    "leith": "@davedevelopment Any chance of your encryption client addition to the PHP SDK being made available as a starting point for this PR/others who want to use it? :smile: \nThis seems like a feature that could be added without AEAD support initially, then add AEAD support once the API is available in PHP 7.1 (and adding notes that 7.1 would become the required PHP version for those wanting to use that cipher).\n. ",
    "dearsina": "@jeskew Now that 7.1 has been released with GCM support, can we have client-side encryption for the PHP SDK? Pretty please? :)\nhttps://wiki.php.net/rfc#php_71. @jeskew Hope you had a good winter. How's that backlog looking? \ud83d\ude04 . ",
    "mariordev": "Any update regarding client-side encryption support added to the PHP SDK? It would be extremely useful! Thank you!. Thank you!!! \ud83d\udc4d . ",
    "imshashank": "@dearsina Thanks for reaching out to us. We are working on the specs for adding client-side encryption at the current moment. Unfortunately, we don't have an estimated time for that as of now.\nMeanwhile, I will keep this thread updated.. @ppaulis Is there any update on the issue? Are you all getting random curl errors? . @anthonybarsotti  Thank you for the feedback and thoughts! \nYou are correct that, The getQueueUrl method only accepts Queue's name and will return the queue URL. Also listSubscriptionsByTopic can be used to get url when an arn is passed.\nCan you please elaborate on how are you generating the arn manually & also is there a specific reason you need to generate the url using arn as there are other ways like you mentioned of getting the QueueUrl?\n. @anthonybarsotti Thanks for the reply.\nUsing method listSubscriptionsByTopic you can get the the subscription's endpoint. It is detailed in our API docs.\nTo help me better understand your problem, please elaborate which URL you want to get and also it will be helpful to know the reason are you trying to get this.\nThanks.. Hi,\nSorry that you received the error without an exception being thrown.\nHowever, I tested the method and received no error messages. \nI would suggest that you please check the php error log for any errors.\nAlso, we suggest updating to the latest version of aws-sdk-php and reading the documentation.\nHope this response was helpful. Please let us know if we can do anything else for you.. @samueleastdev I was able to get policy using the above method in v2.7.  Can u please make sure that the bucket name, region and other parameters are same when you call from code and when you call from CLI.. I am glad the problem was fixed. :-). Hi,\nThanks for reaching out to us. Can you please elaborate your question, it will be really helpful if you can provide us more details.\n. @Thamaraiselvam Can u please share the code you are using to call the method. Also which version of PHP and SDK are you using ?. This is a merge to prevent documentation from being broken. We are working on finding a solution or broken example files in the meantime.. Thanks for reaching out to us.\nSo to answer your questions:\n3) I am not sure why would that happen, can you please share the logs and the error message you get.\n4)\ntimeout is the total request time in connecting, uploading and downloading. This method has the same behavior as https://curl.haxx.se/libcurl/c/CURLOPT_TIMEOUT.html\nconnect_timeout is the amount of time to allow for connecting only (e.g., resolving DNS, establishing a connection, etc.). This method has the same behavior as https://curl.haxx.se/libcurl/c/CURLOPT_CONNECTTIMEOUT.html\nHope this answers your questions.. @sm2017 Hi, Can you please try out uploading the file to S3. Timeouts also depend on network speed, OS and sytem configuration. \nAlso is there a use-case where you want to specify a request timeout to a custom value. It will be really helpful if you can elaborate and share the code that you are trying to call.. @pavankumarkatakam UPDATE: Can u please elaborate on why would you want to add region to the request? \nThis will help us in identifying the pain points.\nThanks.. @ivankristic In the code, the class is defined as \"Aws\\EC2\\Exception\\Ec2Exception' . I will try to repro the bug locally and report my findings. \nAlso is the Ec2Client creating the exception name with capital C?. @ivankristic \nDid creating the new Ec2Client resolve your problem or are you still getting the error.\nIf the error is still not resolved, can you please provide the code with inputs so that we can report it.\n. @jeskew I have fixed the @see links. Also what other approach would you suggest, the trait and interface have all similar methods.. @sanfx  Can you please provide us with more details on how are you setting up the browser. Also how are you trying to reach to your server. It will be helpful if you can share any code snippets you are using to talk to the dynamo local.. @cjyclaire Updated the file with the requested changes. Please have a look :-) . @cjyclaire @jeskew Please have a look at the scripts, it's a PHP script to automate writing changelog.\nAll features, doc-update etc are put inside a folder \".changes/nextrelease\". \nthe script creates a new tag and create a json file with name x.y.z.json in folder \" .changes\"\nIt also updates the CHANGELOG.md file and fixes the endpoints file.\n. @cjyclaire @mtdowling @jeskew Can u please review the code, I have made the changes requested. \nAlso, this script will only be used during build only.. @cjyclaire Made the requested changes.. @cadavre Thanks for reporting the issue.  I think you forgot to add the \"prefix\" parameter in your request.\nCan you please try the request again with the 'Prefix' => '', parameter. A blank parameter will use the whole bucket.\n Updated request: \n$s3Client->putBucketLifecycleConfiguration(\n    [\n        'Bucket' => $bucketName,\n        'LifecycleConfiguration' => [\n            'Rules' => [\n                [\n                    'AbortIncompleteMultipartUpload' => [\n                        'DaysAfterInitiation' => 3\n                    ],\n                    'Prefix' => '',\n                    'Status' => 'Enabled'\n                ],\n            ],\n        ],\n    ]\n);\nPlease let me know if this fixes your error.. Thanks for bringing this to our notice, we will look at the SDK documentation and fix it ASAP.. @mschroeder Thanks for pointing out the typo. . Closing issue since there has been no response from customer for over 10 days now.. @AndyDunn Sorry for the trouble you are experiencing. \nThis is a strange issue and to be able to repro the issue, we would need more information. Can u help us understand what steps you followed to install the sdk and laravel. Also it would be great if you can send us the link to installation instructions that you followed so that we can repro the issue.\n. @LionelMarbot This looks like an issue related to how PHP interprets different encodings. \nWe would suggest looking at the documentation for mb_encode_mimeheader( ) here: http://php.net/manual/en/function.mb-encode-mimeheader.php\nPlease let us know if that helps you.\n. Thanks for bringing this to our notice, please give us some time to repro the issue on our end and verify.. @mtdowling @jeskew @cjyclaire Made the changes requested, please have a look.. @mtdowling @jeskew @cjyclaire Please have a look at the updated code. I have tested that the code will work.. Updated the PR, please have a look. @mtdowling @jeskew @cjyclaire . @cjyclaire Can you please elaborate? . @cjyclaire Added a check to unsign payload only for https requests and also updated the tests.. @jeskew @cjyclaire @mtdowling Thanks a lot for the valuable feedback and helping me in making the code better. Merging this now :-) . @quantizer Thanks for reporting the issue. Can you please share the stacktrace of the error. Also which version of php, OS & curl you are using. This would help us in reproducing the issue.\nCan you please share the code for upload that you have used and option that are being passed.\nThanks.. @quantizer @javer \nSo far I have not been able to reproduce the issue. I have tested on php 5.6, php 7.1 and using the latest SDK and also 3.20.3. For me, the file gets uploaded and I receive no exceptions or errors.\nDo you guys receive this error every time?  . @quantizer @javer Thanks for clarifying. I am investigating the bug right now and will keep this thread updated.. @quantizer @javer Hey, I have been working on reproducing this bug and finding a fix. The bug was reproducible if the bucket is in a different region and the client was initiated with a different region.\nCan you please confirm that the region passed to the client is same as the region for your bucket.\nAlso, the developer guide explains more on this:\nhttp://docs.aws.amazon.com/aws-sdk-php/v3/guide/service/s3-multiregion-client.html\n// If you would like to specify the region to which to send a command, do so\n// by providing an @region parameter\n$objects = $s3Client->listObjects([\n    'Bucket' => $bucketName,\n    '@region' => 'eu-west-1',\n]);\n. @javer Hi, S3MultiRegionClient is used to avoid creating a new client for buckets in different regions and hence the same client can be used for different regions by just passing a variable.\nAlso, I can confirm that I created a client in us-eat-1 and was able to upload an object to a bucket in ap-southeast-2 etc.\nAre u getting the same error when you have created a client in us-east-1 and trying to upload an object to a bucket in region like ap-southeast-2?\n. @quantizer @javer I am sorry that it took long for me to fully understand the bug. I appreciate your patience. \nThere are two approaches that the SDK uses in multiregionclient. For some errors, the SDK will look at the header and fix the problem for the client regarding bucket regions. For errors the SDK is not able to resolve, it will just show the error.\nWe are working on more explicit error messages and figuring out a better approach to working with multiple region clients in S3.\nI have marked this as a bug. Will keep this thread updated.\nThanks a lot for your patience.. @quantizer @javer The fix for the bug has been merged. Sorry that it took a bit longer to fix this one. Hope this will resolve the bug for you.. @rairlie Please clarify is you are talking about S3 objects presigned URL.\nIf yes then that is the limitation of AWS S3 and the SDK just uses their API to create the presign URL. You can submit this as a feature request with the S3 team.  \nPlease let me know if you need help with anything.. @rairlie Sorry for the misunderstanding. Yes please send us a PR. \nThanks. @rairlie Also I would suggest having a look at what values the AWS S3 takes.\nFor presign we can only provide a value of \"expires-in\" and that is what the PHP SDK provides.\nI dont see value in providing a value which says expire 24 hours from sometime on yesterday. Also with the S3 API, there is no way to create a URL with expiration starting from tomorrow or some time in future.\nhttp://docs.aws.amazon.com/cli/latest/reference/s3/presign.html\nI hope I understood you correctly.\n. @rairlie Thanks for the response. Please reply to the questions asked in the PR and we can then work to merge the code to the main repo.. PR merged.. @mattzuba Thanks for reporting this.\nCan u please help us repro the error.  Are you using temporary credentials, also what is the error that you see? It would be really helpful if you can share the code or the stacktrace of the error.\n. @mattzuba Sorry for the delayed response, I will add a PR to fix the bug soon.. @mattzuba Sorry for the delay. We have merged a fix for the bug. Thanks a lot for helping us figure out the bug and suggesting a fix. . @rairlie Have you tested a presigned URL with a start time set in future?. LGTM . Fix for https://github.com/aws/aws-sdk-php/issues/1209. @jeskew Made the changes asked. Thanks for the suggestions.. @NinoSkopac Sorry for the delay. Have you tried to contact the Polly team to increase your rate limit?\n. @NinoSkopac Sorry for the late reply. The PHP SDK use Guzzle library to make any requests. All async requests if failed are automatically retried.\nHere is some documentation on the concept of Promise: https://docs.aws.amazon.com/aws-sdk-php/v3/guide/guide/promises.html\nIt will be a lot more helpful if you can share us what errors you are getting so that we can reproduce it.\n. @NinoSkopac  PHP SDK will use multi_curl for sending async requests. A new request is retried when a previous once fails and hence the order is preserved until it's scheduled using multi_curl.\nAre you still facing this issue?\n. @generak13  I would recommend using the latest version of our SDK (v3).\nAlso, these errors can be caused by a lot of factors including network issues, timeouts etc. Also, you can probably catch the error and retry the request if it fails but that will depend on your use case.\nI guess it will be hard to reproduce this error you can, however, turn on the debug mode and that will help us figure out the problem behind the error.. @JordanDalton Hi, Can you please share the code to help us repro the issue. \nFor the PHP SDK the client is called Mturk, the docs are provided here: http://docs.aws.amazon.com/aws-sdk-php/v3/api/api-mturk-requester-2017-01-17.html\nWe will reach out to the Mturk team to help you out with the issue.. LGTM  :shipit:\n. @mrityunjay308 Have you tried checking the string length during runtime?\nWe have tried the operation but were not able to reproduce the error. Can you please share the code with us that is causing the error for you.\n. LGTM :shipit: . @daryl-williams Hi, to get a list of instances you can use the method describeInstances().\nYou can read the documentation here: http://docs.aws.amazon.com/aws-sdk-php/v3/api/api-ec2-2016-11-15.html#describeinstances\nAlso, you can search on StackOverflow for answers to the similar question.\nPlease let me know if you need any more help.. @daryl-williams I'm not an expert in this service, but a quick perusal of the documentation doesn't have anything jump out at me. As mentioned in our README, I would recommend asking on Stack Overflow though.\n. @Thamaraiselvam Hi, thanks for reaching out. This looks more of an AWS S3 question. We would recommend that you contact the AWS S3 team and they will be able to help you better.\nYou can read the answer to your question in the FAQs on AWS S3 portal: https://aws.amazon.com/s3/faqs/\nAlso here are some docs on versioning: http://docs.aws.amazon.com/AmazonS3/latest/dev/Versioning.html\nHere are few resources on how to reach AWS S3: https://aws.amazon.com/contact-us/\nPlease let me know if I can be of help.. @MaxOrelus Hi, Just trying to clarify. Is there a reason you haven't used the ElasticSearch client.\nYou can read more about it here: https://docs.aws.amazon.com/aws-sdk-php/v3/api/api-es-2015-01-01.html\nYou can create an ES client and can simply call the methods/operations using that client. The SDK will take care of signing the request and handling the response.\nPlease let me know if you need help with anything else.\n. @daryl-williams I'm not an expert in this service, but a quick perusal of the documentation doesn't have anything jump out at me. As mentioned in our README, I would recommend asking on Stack Overflow though. Make sure you label the question with aws-php-sdk label.\n. @manuelz89 Thanks for reaching out to us. \nCan you please share the error that you are getting. Also have you looked at the headers being sent in your request.\nAlso, are you only trying to create the pre signed URL? We would recommend that you use the AWS PHP SDK as it would take care of signing the request and handling errors for you.\nYou can have a look at the docs here: https://aws.amazon.com/documentation/sdk-for-php/\nPlease let me know if this was helpful or if you need help with anything else.. @manuelz89 In your code, you are actually not using the PHP SDK at all.\nI would strongly recommend that you use the PHP SDK as it will take care of the signature, security, retries etc for you.\nYou can install the SDK using composer or by just downloading the zip. Instructions are here: \nThe code to upload a file using the SDK : https://github.com/aws/aws-sdk-php/blob/master/README.md \nAnother way of doing this is to use presigned POST. Pre-signed POSTs allow you to give write access to a user without giving them AWS credentials.\nYou can read more here: http://docs.aws.amazon.com/aws-sdk-php/v3/guide/service/s3-presigned-post.html\nExample code: https://github.com/awsdocs/aws-doc-sdk-examples/blob/master/php/example_code/s3/PutObject.php\n```\n<?php\nrequire 'vendor/autoload.php';\nuse Aws\\S3\\S3Client;\nuse Aws\\Exception\\AwsException;\n/\n * Put an Object inside Amazon S3 Bucket.\n \n * This code expects that you have AWS credentials set up per:\n * http://docs.aws.amazon.com/aws-sdk-php/v3/guide/guide/credentials.html\n /\n$USAGE = \"\\n\" .\n    \"To run this example, supply the name of an S3 bucket and a file to\\n\" .\n    \"upload to it.\\n\" .\n    \"\\n\" .\n    \"Ex: php PutObject.php  \\n\";\nif (count($argv) <= 2){\n    echo $USAGE;\n    exit();\n}\n$bucket = $argv[1];\n$file_Path = $argv[2];\n$key = basename($argv[2]);\ntry{\n    //Create a S3Client\n    $s3Client = new S3Client([\n        'region' => 'us-west-2',\n        'version' => '2006-03-01',\n        'credentials' => array(\n                'key'    => 'YOUR_AWS_ACCESS_KEY_ID',\n                'secret' => 'YOUR_AWS_SECRET_ACCESS_KEY',\n    )\n]);\n$result = $s3Client->putObject([\n    'Bucket'     => $bucket,\n    'Key'        => $key,\n    'SourceFile' => $file_Path,\n]);\n\n} catch (S3Exception $e) {\n    echo $e->getMessage() . \"\\n\";\n}\n```\n. @manuelz89 Hi, Has the issue been resolved for you. Did the sample code help you? Please let us know so that we can close this issue.. @manuelz89 Yes, you can send pre signed posts to S3 using the SDK.\nHere is documentation with sample code on how to do this -> \nhttps://docs.aws.amazon.com/aws-sdk-php/v3/guide/service/s3-presigned-post.html\nPlease let me know if you need help with anything.\n. @NinoSkopac Hi, we would recommend contacting the Polly team on how they use SQS or charge the users.. @NinoSkopac Sorry If I misunderstood you. But if you haven't made any calls to the SQS service then the SDK will not implicitly call that service.\nHence if you are only making requests to Polly, then the requests are sent to only Polly Client.\nIt will really helpful if you can share your code with and we can have a look.. @jeskew @mtdowling @cjyclaire Please have a look at this PR. Updated the code, please have a look.. @jeskew @cjyclaire @mtdowling Please review the code. Thanks. @jeskew The exception in catch in serializer is only done for a sanity check. The errors are always caught up way before anything reached serializer. Also, json_encode will only fail when the value is a resource.\nIf this looks good, I can go ahead and merge this.. @jeskew Added more tests to cover the error paths. @jeskew Sorry that imports were added by the IDE. Removed it and updated the code. Thanks for all the help and for reviewing the code. @mtdowling @cjyclaire @jeskew . LGTM. Hey,\nI tried the following code with no errors. I also see the object was copied to the new bucket.\nIs there something I am missing?\nWere you able to reproduce the issue with any other options being passed?\nP.S. I tried this was an administrator account and hence have permission for everything.\n```\n<?php\nrequire 'vendor/autoload.php';\nuse Aws\\S3\\S3Client;\n$s3Client = new S3Client([\n    'region' => 'us-west-2',\n    'version' => '2006-03-01'\n]);\n$promise = new \\Aws\\S3\\ObjectCopier(\n    $s3Client,\n    array(\n        'Bucket' => 'bucket',\n        'Key' => 'file',\n    ),\n    array(\n        'Bucket' => 'bucket',\n        'Key' => 'file',\n    ),\n    'private',\n    array(\n        'RequestPayer' => 'requester',\n    )\n);\n$promise->copy();\n```. @texano00 Thanks for bringing this to our notice.\nThis looks like its a bug with DynamoDb as I was able to reproduce the bug using the CLI.\nWe would recommend contacting the DynamoDb team directly so that you can track the bug. We will open an issue with DynamoDb as well.. @texano00 That's great. We will notify the DynamoDb team anyway. . @maltepet I would recommend reading these docs to create client and send request:\nhttp://docs.aws.amazon.com/aws-sdk-php/v3/api/api-firehose-2015-08-04.html\nAlso, the SDK code is generated using models if you are interested you can read more about how the SDK works here : http://docs.aws.amazon.com/aws-sdk-php/v3/guide/\nPlease let me know if you have any other queries.\n. Closing issue due to inactivity.. @s3bubble Hi, thanks for reaching out to us. The above error message is returned by the service and we believe is more helpful for our users as they receive the full error now. Is there a reason that you would not want to log the full error?. @s3bubble Hey, So PHP SDK uses a class called \"AWSException\" which is inherited by specific service classes.\nExample: https://github.com/aws/aws-sdk-php/blob/master/src/S3/Exception/S3Exception.php\nThis is the main class https://github.com/aws/aws-sdk-php/blob/master/src/Exception/AwsException.php\nYou can get different parts of the message. But, we would strongly suggest not showing error messages directly to users as they are provided directly by service response.\nI would suggest that you show errors based on error code returned and then show your own error string to the users based on the error code.\n. @s3bubble Hi, I have merged the PR https://github.com/aws/aws-sdk-php/pull/1241\nNow you can get the concise error message using  $e->getAwsErrorMessage()\nHope this helps.. @bradynpoulsen You can provide the endpoint as part of the client.\nPlease have a look at the documentation here: https://docs.aws.amazon.com/aws-sdk-php/v3/guide/guide/configuration.html#endpoint\nWe would like to know more about your scenario and why would you prefer setting up endpoints as environment variable.\n. @bradynpoulsen Hi, thanks a lot for this suggestion and for opening the PR. This is a design change and would affect all existing customers. We are working out the standards and practices to follow before allowing endpoints to be configured via endpoints URL. Please give us some time before we implement the feature. Thanks a lot for your patience.. @soukicz Hey, thanks for opening this PR. can you please explain the reasoning for the PR. Was it causing you some errors?. @soukicz I am just concerned if this might break some of our users if they are using the integer indexes and with this change, the array would have keys instead.. @bradynpoulsen I just ran a coverage with the functions.php included and the report showed 0% coverage for functions.php. \nAlthough all of the method from functions.php are called in other classes and test classes.\nI suspect this to be the reason that the file was excluded as it is incorrectly represented in coverage report.\n. @bradynpoulsen Thanks for opening this PR. As explained in the issue thread, we are working on finalizing standards for the environment variables. We want a consistent experience across all services. Will keep this thread updated.. @cjyclaire @jeskew Can you please quick review this.. @aftabnaveed Hi, Thanks for opening this issue. The choice to make every request/response an array of arrays is a design choice.\nIs there a use-case in which treating request/response as object help you in certain ways that can not be achieved by arrays. We would love to hear about your thinking. Please elaborate on the enhancement so we can understand it better. Thanks.. @aftabnaveed Thanks for the query again. So this is more of a design choice based on how the SDK actually works. \nIf you look inside the SDK code you will realize we use models and the operations etc are loaded dynamically. So if we wanted to move to using classes it won't help your IDE as the code is dynamic and IDE wouldn't be able to help in that.\nIn terms of performance, we don't see an advantage of using methods and classes as first AWS has over 90 services hence would be a lot of code generated for the SDK.\nWe will keep your suggestion for future consideration. Thanks for the feedback. Please let me know if I can help you with something else.. LGTM :shipit: \nP.S. Please rebase and squash before merge.. @emirb Thanks for the PR.Looks good to me.  Merging it.. @mwoodring Sorry you had the problem. You can read more documentation on how to use this here-> http://docs.aws.amazon.com/aws-sdk-php/v3/api/class-Aws.S3.S3ClientInterface.html#_upload\nIt will be easier for us to help you if you can share the error with the stack trace.\n. @mwoodring Hi, is there any update on the issue? Closing this issue since there has been no update for over 15 days. Please feel free to reopen this in future is the issue comes again.. @mtdowling coveralls has reported wrong code coverage for other open-source projects. \nAlso, it was fairly easier to integrate codeov as compared to coverall for PHP projects.. @bpicolo Thanks for reporting this. I will create a PR soon to fix this probable flaw. I am trying to see if there are more places where the method is_callable is used. Please let me know if you see this anywhere else as well in the SDK code.. @bpicolo Hi, I have investigated this issue and there are a lot of fo places where this keyword is called. I'll apply a fix to the line specified. We are just taking the time to make sure that we don't break any of our customer's code.. The fix has been merged. Closing this pull request.. @jeskew @kstich @mtdowling @cjyclaire Please let me know if this requires any more changes?. @dtirer Yes, @dgadelha is right. PHP-SDK uses the Guzzle library to manage requests. The tick is just like a clock tick which checks the status of requests and performs various operations until the operation is complete. \nWe have tested with SQS and it is generally very fast. Are you making these requests from inside of AWS Network (EC2 or lambda) or from outside the network?\nYou can read more about Promises used by PHP-SDK: https://docs.aws.amazon.com/aws-sdk-php/v3/guide/guide/promises.html\nYou can directly test using guzzle as well: http://docs.guzzlephp.org/en/latest/request-options.html\n. @dtirer Also, these delays can be caused by factors such as lag in the network, packets being lost or a lot of uncontrollable features. The SDK will retry failed requests automatically based on the default configuration for the service.\nPlease let us know what you find in your testing and please share the results.. @dtirer I see the newrelic logs, this looks more like an SQS issue. For further investigation, I recommend reaching out to the SQS team and letting them know that you are getting delays in the request. You can provide them with request ID and they can provide logs for the specific request. . @dtirer I am happy to help you in resolving this issue for your code. But since we have ruled that the SDK is not causing this issue, I would be closing this issue. I would recommend opening tickets with the specific AWS team and using requestId to debug the issue.\nPlease feel free to reopen this issue in future if there is something we can help you with. . @dtirer Is time_starttransfer reported in SDK logs or by newrelic? Also in newrelic you can see how long different parts of the requests are taking. It will be helpful if you can share the stack or logs that you have aggregated so far to help us understand what might be causing the problem.. @mvanbaak We are working on this and you will see an update very soon. We don't have an sla as of now but I will keep this thread updated.. @mvanbaak RDS IAM Auth has been merged. Updated the CHANGELOG-> https://github.com/aws/aws-sdk-php/blob/master/CHANGELOG.md. @mtdowling @kstich Please check this and let me know if this looks good. Thanks . @kstich I can not see the modified files in the coverage report. Is there a reason that the coverage dropped? And can it be fixed?. @danguer Thanks a lot for opening this PR. Will merge this one travis build finishes. \nP.S. The last build on travis failed, have kicked the build again.. @ssigwart @stof @sm2017  Thanks for reporting the issue. We will add this our backlog to reduce PHP SDK memory footprint. \nIn the meantime, please feel free to share any strategies, code or examples you have to solve this issue or to reduce memory footprint. \n. @sm2017 @ssigwart @stof We will be closing this issue as this has more to do with how PHP handles cyclic object graphs than with the SDK. We will also add a task to out backlog to look at how PHP uses memory and optimize it. \nPlease feel free to add any comments or reopen the issue if you see any memory leaks as a result of PHP SDK.\n. @ssigwart Thanks for reporting this. Please give me some time to investigate the issue. I will keep this thread updated. In the meantime can you please make sure that you have the garbage collector turned on in your php.ini files.. This issue is a duplicate for #1273 . \nTo keep all conversations in one thread, closing this issue.. @4406arthur Sorry that you had to face this issue. Please help us understand your use case by providing some more information. \nAre you trying to download the object from s3 to your local system using \"copy\" command? Please correct me if I am wrong. Also please share with us the stack trace of the error that you are getting when you run the curl command.\nP.S. You can also look at sample code for \"get\" and \"put\" operation in s3 on this previous issue: https://github.com/aws/aws-sdk-php/issues/239\n. @4406arthur sorry for late reply. But it looks like the copysource needs to be a bucket/key format as described in the documentation. \nhttp://docs.aws.amazon.com/aws-sdk-php/v3/api/api-s3-2006-03-01.html#copyobject \nPlease try it and let me know if you still face the issue.\n. @4406arthur What error did you get, can you please share.. @4406arthur  I used your code for region us-west-2 & ap-northeast-2 and it worked perfectly fine for me. \nCan you make sure that you have the appropriate file permission to be able to copy the given object? \nCan you please run this on some test file and try to copy that in the same folder and please share if you are still getting the error. \nP.S. for the curl command I just used the file_bytes as 0\ncurl --request PUT -H 'Content-Length: 0' --url '<presigned URL>'. @4406arthur This is more of a curl requirement. Since you are copying object across buckets in S3, you are not really sending any file in the request. . @4406arthur Has there been any update on this issue? Did you try to open a ticket with the S3 team?. Thanks for the response. Please let us know if you need help with anything else.. LGTM :shipit: . @Tietew Thanks for reporting this issue. Please give us some time to investigate the issue. I'll keep this thread updated with what I find.. @Tietew Sorry for the delayed response.  I was able to send the request with no errors. \nHave you seen this error for any other requests as well? . @Tietew My bad, I should have asked the question in a different way.\nI looked up at the documentation for simplexml extensions and to disable it, I had to compile PHP from source and pass the flag --disable-simplexml specifically.\nAlso, did adding the ext-simplexml to composer fix your issue or you had to install the php extension for Ubuntu as well? I added the simplexml to my composer but I was getting a different error afterward.\nIt will be really helpful if you can provide us with the steps as where did you install PHP from ie. apt-get or compiled from source?. Sorry for the delay, created a PR for the requirements -> https://github.com/aws/aws-sdk-php/pull/1342 . @esbenp Sorry that you are having this issue.\nI have not seen this kind of error before but since this is a long running supervisor process, I would recommend scheduling the garbage collector to run after every few minutes (can be done using php.ini). I would also recommend monitoring the memory of the system to make sure that nginx is not killing php processes if they exceed certain memory limit.\nAlso, apache/nginx has a max_execution_time setting which can cause the process to die if it is running for a certain time. \nI'll recommend you to have a look at that as well. Please let me know if this fixes your problem or we can dig deeper. . @ankitsam This is more of a docker question. I would recommend simplifying the command or better use ENTRYPOINT in your docker image to run this so that you don't have to override at runtime.\nYou can do this in your Dockerfile\nWORKDIR /repo\nRUN chmod +x /repo/run.sh \nCMD [\"<path to run.sh>\"]\nor better replace CMD with\nENTRYPOINT [\"/repo/run.sh\", \"arg1\"]\nBesides that this is documentation for ecs run task-> http://docs.aws.amazon.com/aws-sdk-php/v3/api/api-ecs-2014-11-13.html#runtask\nIn any case, you can simplify the command as such:\n'command' => ['/repo/run.sh arg1']\nDocker will run any command on the default shell sh\nAnother way to debug is to ssh into the EC2 box and look at running containers and you can see their output by running\ndocker logs <container-id>\nI will be able to help you better if you share what is the error you are getting. Please let me know if there is something I can help you with.. @ankitsam  I suspect this has to do something with the way parameters are passed to ECS task. I would recommend looking at DockerFile 'CMD' usage:\n https://docs.docker.com/engine/reference/builder/#cmd\nYou can simplify the command as ->\n'command' => ['/repo/run.sh', \"arg1\"]\nSince this is a usage question, I will be marking this issue as closed since this doesn't look like an SDK specific error. \nWe recommend asking this on StackOverflow as described in ReadMe file. \nPlease let me know if you need any more help. Thanks\n. @weshooper Thanks for opening this PR. Can you please elaborate on what this PR fixes also it would be helpful if you share the code that caused the error.. @vanzay Sorry that you had to face this issue. Please help us better understand the error. \nIt will be helpful if you can share your error stack & code snippet so that we can reproduce the issue on our end.. Is there a reason this is reducing code coverage?. @ebylund Thanks for opening this PR but the SDK classes use third person derivative language format for English comments.\nExample: https://github.com/aws/aws-sdk-php/blob/master/src/S3/S3Client.php#L22\nClosing this PR as it does not follow our language style used in the SDK.\nPlease feel free to update this thread if you think we closed this PR as a mistake. . @cdtweb Sorry that you are facing this problem. Can you verify that the base64_encode($raw_message) is not empty.  Also, do you receive any errors when you send the message.\nFor reference this is the documentation for sendRawEmail-> http://docs.aws.amazon.com/aws-sdk-php/v3/api/api-email-2010-12-01.html#sendrawemail\n. @cdtweb Hi, I tested the sesClient and sent myself a raw message. I was able to see the string passed as body in rawEmail, in the email I received in my inbox.\nCan you please confirm that you are correctly forming the body of the message. Please have a look at the SES guide for sending the raw email.\nhttp://docs.aws.amazon.com/ses/latest/DeveloperGuide/send-email-raw.html\nI would recommend having a look at the sendEmail method [http://docs.aws.amazon.com/aws-sdk-php/v3/api/api-email-2010-12-01.html#sendemail]. It will take care of setting header/body/etc parameters.\nPlease let me know if this doesn't resolve your issue. \n. @cdtweb I was able to figure out the difference in v2 and v3 of PHP SDK in method sendRawEmail.\nin V2, the field data needs to be base64 encoded. This is what the field would look like-> 'RawMessage' => [ 'Data' => base64_encode($msg)] . \nBut In v3, the field  'RawMessage' => [ 'Data' => $msg ] doesn't need to be base64 encoded as the v3 SDK will take care of it and encode the data attribute before sending to service endpoint.\nSorry for the confusion, we will update our docs to better reflect this. Thanks a lot for bringing this to our notice.\nPlease let us know and update this thread to verify if this resolves your issue.  . @cdtweb Closing this issue and adding a task to backlog to update docs for SESClient sendRawEmail.. LGTM :shipit: . LGTM. Merging. @ProdigyView Can you please specify the PHP SDK version, PHP version and which OS are you using.. @ProdigyView Thanks for sharing the debug logs. You can turn on debugging in s3Client by passing the flag 'debug' => true. It will give you a lot more detailed logs and will also show if any parts during uploading the file have failed.\n. @ProdigyView I have tested your code and I was able to upload files as large as 7GB without any errors. I tested the above code without acl and before_initiate properties.\nDo you see the object uploaded in AWS S3 bucket? \nAlso, does your $result object returns NULL each time?. @bakura10 This doesn't look like something caused by the PHP SDK.\nI would recommend having a look at your IAM keys and policies. Some keys/assumed roles can be set to expire after a certain period of time.\nClosing this issue since it looks more related to service teams and not the SDK. Please feel free to open the issue if you think the problem is being caused by the PHP SDK.. @tarunkumar2215 Are you running this on an EC2 machine or a different host?. LGTM :shipit: . @dioogo91 Will this change behavior of existing clients?. @match Thanks for opening this PR, but I will close this PR as the regions were updated in the endpoints file by the Athena team. \nWe apologize for the delay in updating the endpoints file. We will make sure that we update these endpoints as soon as they become GA. . @daum Thanks for pointing out this issue. Please give us some time to repro this issue. In the meantime, please feel free to open a PR with the above-discussed fix.. @daum Currently working on testing on the use cases described by @kstich. Sorry, we can not provide a date of completion for this task as of yet.. @eric-tucker Sorry, that you are facing this issue. Please give us some time to repro this issue.. @eric-tucker Hi, I tested with setting  AWS_PROFILE to point to the different account and it worked for me.  Environment variables set in bash using export for a terminal are only valid for that session until the terminal is closed.\nThe php script must be invoked inside the same terminal or it won't be able to access the env variable. You can use getenv['AWS_PROFILE'] to check what is the value of env variable that is being seen in your php script.. @eric-tucker I am sorry I misunderstood. PHP SDK doesn't support assuming role using the credentials file. You will have to explicitly tell the SDK to assumeRole.\nYou can see this example on how to use STS client for assumeRole-> http://docs.aws.amazon.com/aws-sdk-php/v3/guide/guide/credentials.html#using-assume-role-credentials\nThanks for notifying this feature to us and we will add this to our backlog to add support for assume-role based on role_arn in credentials file.. @jeremeamia The last update to the S3MultiRegionClient was done in this PR -> https://github.com/aws/aws-sdk-php/pull/1231. The code update was in response to a bug. \nLooks like from your comment that this doesn't happen on each request. Please correct me if I am wrong.\nAlso, it will be really helpful is you can provide me with any information helpful in reproducing the bug.. @jeremeamia Thanks for the update. Working on figuring out the issue.. @jeremeamia We are currently investigating the issue. We have figured out that the error doesn't occur if you configure the client with use_path_style_endpoint.\nWill push a bug fix very soon.. @dioogo91 The changes LGTM. Have asked @kstich to review and approve.. @dioogo91 Please add a changelog blog as described here with this PR -> https://github.com/aws/aws-sdk-php/blob/master/CONTRIBUTING.md\n. @dioogo91 On a second look I have some questions.\nPlease have a look at the below code snippet which works for me:\n$key = openssl_random_pseudo_bytes(256/8);\n  $client->putObject([\n        'Bucket' => $bucket,\n        'Key'    => 'testfile',\n        'Body'   => \"Test this\",\n        'SSECustomerAlgorithm' => 'AES256',\n        'SSECustomerKey' => $key\n    ]);\nThis is using total of 256 bits as the key for the SSE-C. The algorithm will fail if the key size is not correct.\nAlso customers need to provide a key that is not base64_encoded.  So for yml files you can save keys in base64 and then base64 decode them before passing to the s3Client.\nPlease help us in understanding the scenario if we are missing some usecase which can't be handled without this change.. @cwhite92 Can you please check the max_execution_time in your php.ini file, it is 30 seconds by default. Also, the file will have a post_max_size which is set to 8MB by default. \nPlease let me know if changing these values in php.ini fixes the issue for you.\n. @rtowings Sorry for the confusion. The SDK will look at the home directory of the given user. It looks like here the process is assumed to being run by root because the HOME environment variable is not set.\nYou can set the HOME env variable that is used by the SDK to look for credentials file. As described here -> https://github.com/aws/aws-sdk-php/blob/33e8f7083fcb54fd926a8569cb81a0795e8c2633/src/Credentials/CredentialProvider.php#L328 \nYou can read more about credential chain & providers in our official SDK guide -> http://docs.aws.amazon.com/aws-sdk-php/v3/guide/guide/credentials.html\nPlease let me know if this helps in resolving your issue.. @rtowings Looks like I didn't see that you have answered the question already. Please let us know if you have any recommendations to improve the credentials workflow.\nClosing this issue since this looks to be resolved. Please re-open this issue if you need any more help.. @Mezzle Sorry for the error. We will update our composer to reflect a dependency on SimpleXMLElement. Thanks for notifying us.\nThis extension is enabled by default in PHP 5.1.2+.  Can you please tell us where did you install PHP from so that we can better understand the source of the issue.. The PR fixes the above issue -> https://github.com/aws/aws-sdk-php/pull/1342. @kstich Updated the test and compatibility code, please let me know if anything else required.. @danielclariondoor Sorry to hear about this. Can you please share which version of SDK, you are using? \nAnd does this error occur on every request or just sometimes.. @afust-pica9 Hi, I am not really familiar with league/flysystem. Please give us some time to analyze. It will be really helpful if you can share the code snippet used for uploading this file. Also, what is the average size of file that you are trying to upload.. @afust-pica9 Thanks for sharing the code but I don't see AWS PHP SDK being used in the example code you shared with us. \nI have tested multipartUploaded locally and did not see these errors. It will be helpful if you can share the multipartUploader code as well so I can try to reproduce the errors.\n. nextrelease directory will be inside a \" .changes\" folder, it will have release notes for next release. I just added checks to make sure that the directory is there and it exists.  even for a doc only update the release notes will be inside the nextrelease folder.. Any idea on how can we test private methods?. Wouldn't make sense as of now to use this variable as its only used one during reading the changelog from files.. This was made so that in future if we want to change the constructor or add more variables then we will have only one place to change.. Thanks for the suggestion.\nHowever the directory will be part of the repo and hold resources like CHANGELOG-valid.md & CHANGELOG-invalid.md files. I will move all writing or editing to temp folder.. The NEW_SERVICE flag is set when a new service is being released  Besides that, all other updates except \"DOC_UPDATE\" are pushed to the changelog. \nPlease, let me know why do u thing switch case would do better here.\n. I havent tested it with the SDB package yet. Do u think this will break that? Since this is only for changelog Automation.. Done\n. Moved the changes to build folder and passed a dir variable in the constructor. Using single codes now and fixed the formatting.. Thanks for the suggestion. Implemented this . we need to remove the signatureversion \"v2\" since php sdk doesn't support it. Also having this value results in bunch of tests to fail. No, doesnt need any parameters from argv. But it's a flag and not a variable. So i thought it would be better naming.. The default behavior is to run it from the base directory. The option for outputDir was added for unit test to test the methods and that Changelog is written at the correct place. \nThe script will be called from base directory and will not need any variables to run normally.. All array properties are set in the constructor and none of the values are REQUIRED parameters here. The constructor makes sure that each key has a value when a object is instantiated .. I can make that a todo for future. But for not we don't need this functionality as the configs are only & only used by unit tests.. Using array allows to pass valued as one variable to the constructor. Also, none of these values are required and goes with the other comment on why we don't have outputdir etc on command line. The bind values will be set in the constructor only and constructor design will make sure that the array values are all set always.\nSo the values were only passed for making the unit tests work and I don't see a lot of advantage in having these values binded.\n . Done. May be I am missing something here but binding would not make things any clearer. The way I am thinking this is that the params array will still be passed and you can look at the constructor to see what values are being set which is the same thing you would do for checking binded parameters. So I don't see any advantage in binding parameters over passing an array and then setting values.. doesnt array() looks more neat. @jeskew \nDo u mean to add a new testprovider method as well showing how the request must look like with 'UNSIGNED-PAYLOAD'  for testSignRequestUnsignedPayload() test ?. I am going to do this with a new provider since all signatures will need to be changed as well.. Removed it from the S3SignatureV4 class. I am not sure if using if ($securityToken = $credentials->getSecurityToken()) { will check for isEmpty($securityToken). Doesn't it only check for truth/false values?\n. Added more documentation. This was added so that we only add the header X-Amz-Content-Sha256 to v4-unsigned requests.\nChecking if the payload is UNSIGNED_PAYLOAD is same as checking for the boolean flag. \nCan you please help me in understanding why would it be better to check for self::UNSIGNED_PAYLOAD and not the simple boolean flag.. There are already cased with http in the test class. However, I have added more test cases to the existing testprovider() method.\n. Added additional check. I just wanted to make sure that the value is properly checked. WE have had bugs before where a value was set to empty strings and it resolved as true. I believe it never hurts to be too careful.. I can wrap this around an exception, is that what you meant? This would throw an exception as of now if the json string is not valid.\nAlso, this is the most concise way to remove all newline characters, tabs etc and make sure that the string we are encoding can be decoded back.. I added this additional check to make sure that this would fail if for some reason the variable was set to empty string.. The same reason described above. This I found to be a concise way of removing any new line characters etc.. @mtdowling Do you have any suggestions on what other checks can be added.. Extra space. Get Info about Access Key's Last Usage. Would recommend updating this to -> List Account Aliases\n.  PayloadParserTrait has methods only for decoding JSON and not for encoding.. Update the code.. Have updated the exception type to: InvalidArgumentException.\nThis is only added as a safety net as the error will only be thrown if the value is a resource.. Good suggestion. Implemented this. Thanks. Thanks for the suggestions. Have changed the method name to getAwsErrorMessage. Can you please make sure the length of line is less than 80 characters.. Can you please make sure the length of the line is less than 80 characters. Try dividing the line into multiple lines\n. Can you please add some validation to the variable.. just for clarity can you rename arn to queueArn. I notices that for now I'll add this task to backLog to replace PHPUnit_Framework_TestCase with \\PHPUnit\\Framework\\TestCase.\nHave updated the composer.json file with values: ^4.8.35|^5.4.0\",. That seems like a more appropriate fix.. Split in multiple lines. Please break this into multiple lines.. Added more test. ) { \nshould be on next line. Why don't you make this a generic function and pass the \"$message['Body']\" variable directly to this instead of $message.. fair enough. Can you please rename ISSUE-LOGGING-ENABLED to ISSUE_LOGGING_ENABLED\nmore suited for an env variable format.. would suggest better renaming like error-issues.log. Suggestion: Use the standard log issue format example:->\ndate LEVEL Issue. Wouldn't this change behavior of existing clients?. Fari enough. Please update the changelog message to reflect that this might change the behavior of the client.. Since the size is already being calculated here it can be stored in a variable which can be used later to set the ContentLength. Please break this into lines less than 80 characters in length.. Please break this into lines less than 80 characters in length.. Please break this into multiple lines. We try to keep all line length to be of less than 80 characters in the PHP SDK.. Please break this into multiple lines. We try to keep all line length to be of less than 80 characters in the PHP SDK.. Please rephrase the sentence so someone without context can easily understand it. Recommendation:\nBuild exception from the content of file pointed by the sink when the value is a file path. Also, sets the body with the content of the file instead of the path.. ",
    "tolidano": "Pinging on this - how is this coming along?. ",
    "muratsplat": "Hi @jeskew \nI was added credentials parameter as \"false\" you said. Now it returns different error.\n`` sh\nNext exception 'Aws\\DynamoDb\\Exception\\DynamoDbException' with message 'Error executing \"CreateTable\" on \"http://localhost:8000\"; AWS HTTP error: Client error:POST http://localhost:8000resulted in a400 Bad Request` response:\n{\"__type\":\"com.amazonaws.dynamodb.v20120810#MissingAuthenticationToken\",\"message\":\"Request must contain either a valid ( (truncated...)\n MissingAuthenticationToken (client): Request must contain either a valid (registered) AWS access key ID or X.509 certificate. - {\"__type\":\"com.amazonaws.dynamodb.v20120810#MissingAuthenticationToken\",\"message\":\"Request must contain either a valid (registered) AWS access key ID or X.509 certificate.\"}' in /home/mo/works/projects/dynamodb-test/vendor/aws/aws-sdk-php/src/WrappedHttpHandler.php:159\nStack trace:\n0 /home/mo/works/projects/dynamodb-test/vendor/aws/aws-sdk-php/src/WrappedHttpHandler.php(77): Aws\\WrappedHttpHandler->parseError(Array, Object(GuzzleHttp\\Psr7\\Request), Object(Aws\\Command))\n1 /home/mo/works/projects/dynamodb-test/vendor/guzzlehttp/promises/src/Promise.php(199): Aws\\WrappedHttpHandler->Aws{closure}(Array)\n2 /home/mo/works/projects/dynamodb-test/vendor/guzzlehttp/promises/src/Promise.php(170): GuzzleHttp\\Promise\\Promise::callHandler(2, Array, Array)\n3 /home/mo/works/projects/dynamodb-test/vendor/guzzlehttp/promises/src/RejectedPromise.php(40): GuzzleHttp\\Promise\\Promise::GuzzleHttp\\Promise{closure}(Array)\n4 /home/mo/works/projects/dynamodb-test/vendor/guzzlehttp/promises/src/TaskQueue.php(60): GuzzleHttp\\Promise\\RejectedPromise::GuzzleHttp\\Promise{closure}()\n5 /home/mo/works/projects/dynamodb-test/vendor/guzzlehttp/guzzle/src/Handler/CurlMultiHandler.php(96): GuzzleHttp\\Promise\\TaskQueue->run()\n6 /home/mo/works/projects/dynamodb-test/vendor/guzzlehttp/guzzle/src/Handler/CurlMultiHandler.php(123): GuzzleHttp\\Handler\\CurlMultiHandler->tick()\n7 /home/mo/works/projects/dynamodb-test/vendor/guzzlehttp/promises/src/Promise.php(240): GuzzleHttp\\Handler\\CurlMultiHandler->execute(true)\n8 /home/mo/works/projects/dynamodb-test/vendor/guzzlehttp/promises/src/Promise.php(217): GuzzleHttp\\Promise\\Promise->invokeWaitFn()\n9 /home/mo/works/projects/dynamodb-test/vendor/guzzlehttp/promises/src/Promise.php(261): GuzzleHttp\\Promise\\Promise->waitIfPending()\n10 /home/mo/works/projects/dynamodb-test/vendor/guzzlehttp/promises/src/Promise.php(219): GuzzleHttp\\Promise\\Promise->invokeWaitList()\n11 /home/mo/works/projects/dynamodb-test/vendor/guzzlehttp/promises/src/Promise.php(261): GuzzleHttp\\Promise\\Promise->waitIfPending()\n12 /home/mo/works/projects/dynamodb-test/vendor/guzzlehttp/promises/src/Promise.php(219): GuzzleHttp\\Promise\\Promise->invokeWaitList()\n13 /home/mo/works/projects/dynamodb-test/vendor/guzzlehttp/promises/src/Promise.php(62): GuzzleHttp\\Promise\\Promise->waitIfPending()\n14 /home/mo/works/projects/dynamodb-test/vendor/aws/aws-sdk-php/src/AwsClient.php(202): GuzzleHttp\\Promise\\Promise->wait()\n15 /home/mo/works/projects/dynamodb-test/vendor/aws/aws-sdk-php/src/AwsClient.php(167): Aws\\AwsClient->execute(Object(Aws\\Command))\n16 /home/mo/works/projects/dynamodb-test/app/Http/Controllers/Dynamodb.php(37): Aws\\AwsClient->__call('createTable', Array)\n17 /home/mo/works/projects/dynamodb-test/app/Http/Controllers/Dynamodb.php(37): Aws\\DynamoDb\\DynamoDbClient->createTable(Array)\n18 [internal function]: App\\Http\\Controllers\\Dynamodb->connection()\n19 /home/mo/works/projects/dynamodb-test/vendor/illuminate/container/Container.php(503): call_user_func_array(Array, Array)\n20 /home/mo/works/projects/dynamodb-test/vendor/laravel/lumen-framework/src/Application.php(1399): Illuminate\\Container\\Container->call(Array, Array)\n21 /home/mo/works/projects/dynamodb-test/vendor/laravel/lumen-framework/src/Application.php(1364): Laravel\\Lumen\\Application->callControllerCallable(Array, Array)\n22 /home/mo/works/projects/dynamodb-test/vendor/laravel/lumen-framework/src/Application.php(1335): Laravel\\Lumen\\Application->callLumenController(Object(App\\Http\\Controllers\\Dynamodb), 'connection', Array)\n23 /home/mo/works/projects/dynamodb-test/vendor/laravel/lumen-framework/src/Application.php(1303): Laravel\\Lumen\\Application->callControllerAction(Array)\n24 /home/mo/works/projects/dynamodb-test/vendor/laravel/lumen-framework/src/Application.php(1288): Laravel\\Lumen\\Application->callActionOnArrayBasedRoute(Array)\n25 /home/mo/works/projects/dynamodb-test/vendor/laravel/lumen-framework/src/Application.php(1207): Laravel\\Lumen\\Application->handleFoundRoute(Array)\n26 /home/mo/works/projects/dynamodb-test/vendor/laravel/lumen-framework/src/Application.php(1442): Laravel\\Lumen\\Application->Laravel\\Lumen{closure}()\n27 /home/mo/works/projects/dynamodb-test/vendor/laravel/lumen-framework/src/Application.php(1213): Laravel\\Lumen\\Application->sendThroughPipeline(Array, Object(Closure))\n28 /home/mo/works/projects/dynamodb-test/vendor/laravel/lumen-framework/src/Application.php(1153): Laravel\\Lumen\\Application->dispatch(NULL)\n29 /home/mo/works/projects/dynamodb-test/public/index.php(28): Laravel\\Lumen\\Application->run()\n30 /home/mo/works/projects/dynamodb-test/server.php(12): require_once('/home/mo/works/...')\n31 {main}\n```\n. @jeskew Thanks :+1: \nYour suggestion solved my problem. It looks php client connects DynamoDb in local.. \n. ",
    "storage1": "Unfortunately i still get an Internal Sever Error on my EC2, when i tr uo unlink an uploaded file. Even if i call gc_collect_cycles() before. Im using SDK verison 3.*. \nBest regards\n. of course. My function looks like this:\n```\nfunction moveFileToS3($localDir, $s3Key){\n    global $s3client;\n    $s3client->putObject(array(\n        \"Bucket\" => AWS_S3_BUCKET_NAME,\n        \"Key\" => $s3Key,\n        \"ACL\" => \"authenticated-read\",\n        \"SourceFile\" => $localDir,\n        \"Metadata\" => array()\n    ));\ngc_collect_cycles();\nunlink($localDir);\n\n}\n``\n. Thanks for the reply. i checked and gc is running as supposed. I'm using theputObject` because i am always uploading a single specific file.\nI investigated further and this is the Error message i am getting from php, wehen using unlink:\n```\nFatal error: Uncaught exception 'RuntimeException' with message 'Unable to open /var/app/current/api/media/tmp/post_picture/20658-small.jpg using mode r: fopen(/var/app/current/api/media/tmp/post_picture/20658-small.jpg): failed to open stream: No such file or directory' in /var/app/current/vendor/guzzlehttp/psr7/src/functions.php:285\nStack trace:\n0 [internal function]: GuzzleHttp\\Psr7{closure}(2, 'fopen(/var/app/...', '/var/app/curren...', 293, Array)\n1 /var/app/current/vendor/guzzlehttp/psr7/src/functions.php(293): fopen('/var/app/curren...', 'r')\n2 /var/app/current/vendor/guzzlehttp/psr7/src/LazyOpenStream.php(37): GuzzleHttp\\Psr7\\try_fopen('/var/app/curren...', 'r')\n3 /var/app/current/vendor/guzzlehttp/psr7/src/StreamDecoratorTrait.php(31): GuzzleHttp\\Psr7\\LazyOpenStream->createStream()\n4 /var/app/current/vendor/guzzlehttp/psr7/src/StreamDecoratorTrait.php(81): GuzzleHttp\\Psr7\\LazyOpenStream->__get('stream')\n5 /var/app/current/vendor/aws/aws-sdk-php/src/Middleware.php(231): GuzzleHttp\\Psr7\\LazyOpenStream->getMe in /var/app/current/vendor/guzzlehttp/psr7/src/functions.php on line 285\n``\n. Yeah but there should not be an active stream reader, becauseputObjectis declared as a synchronous function.  So everything should be released from the file whenunlinkis called. Alsounlink` is a php-function and no function handled by guzzle... \n. ",
    "php5developer": "I've same issue on Windows 7 PHP 5.6.25.\nSDK version is 3.36.8.\ngc_collect_cycles() helped me.. ",
    "Agimovel": "None of the above solutions worked for me. So instead of lock that file with this process, I throw her on a variable:\n```\n$my_virtual_file        = fopen($filepath,'r');\n'Body'          => $my_virtual_file         , // Use this instead of \"SourceFile\"\n\nflose($my_virtual_file);\nunlink($filepath);\n```. ",
    "ScuzzyAyanami": "Thanks, I've actually got my own client which I mimic this SDK's behaviors and inputs so when mine breaks I just switch over to the AWS PHP SDK. I'm able to fetch stats through my own code, but in the future id like to align it to behave in the same way as this code base does. :+1:\n. ",
    "raulmangolin": "Sure!\nThis the full error:\nFatal error: Uncaught Aws\\CloudSearch\\Exception\\InvalidTypeException: AWS Error Code: InvalidType, Status Code: 409, AWS Request ID: 14cd15c7-a05f-11e5-9d81-331d916aa968, AWS Error Type: client, AWS Error Message: Invalid configuration, User-Agent: aws-sdk-php2/2.8.24 Guzzle/3.9.3 curl/7.22.0 PHP/5.5.27-1+deb.sury.org~precise+1 thrown in /var/www/awesome/admin/Vendor/aws/aws-sdk-php/src/Aws/Common/Exception/NamespaceExceptionFactory.php on line 91\n. @jeskew I found in AWS Documentation a parameter who I haven't see before.\nJust need \"aliases\" parameter in JSON.\nhttp://docs.aws.amazon.com/cloudsearch/latest/developerguide/configuring-analysis-schemes.html\nI'm sorry for the inconvenience and thank you.\n. ",
    "ajaygarga": "I have added the following code to add the retry logger using Zend 2 framework Logger. Please let me know if this correct.\n```\n          $logFilePath = '/tmp/dynamodb.log';\n    if (! is_file ( $logFilePath )) {\n        Util::putFileContent ( $logFilePath, \"AUTOCREATE\\n\" );\n    }\n    $writer = new \\Zend\\Log\\Writer\\Stream ( $logFilePath );\n$filter = new Priority ( ZendLogger::DEBUG );\n$writer->addFilter ( $filter );\n\n$zend_logger = new Logger ();\n$zend_logger->addWriter ( $writer );\n\n$logAdaper = new Zf2LogAdapter ( $zend_logger );\n$params ['client.backoff.logger'] = $logAdaper;\n       $dynamodbClient = DynamoDbClient::factory ( $params );\n\n```\nBut even when I edit the cached credentials file with the past time and test it, nothing is written to the log. I am not sure whether if I am missing something.\nThe expired credentials are not getting refreshed as I see in the cached credentials shows ''token.ttd\" as past date (say 9th Dec or so). Please refer the stacktrace attached.\nstacktrace.txt\n. Yes it seems like the retry is not happening as I see nothing comes up in the backoff retry log. What could be the reason?\n. I figured out the issue why the Zend 2 cache is not updated. The issue is Zf2CacheAdapter of Guzzle doesn't pass the expiration of cache to Zend cache as per this code\npublic function save($id, $data, $lifeTime = false, array $options = null)\n    {\n        return $this->cache->setItem($id, $data);\n    }\nGuzzle doesn't pass the lifetime to Zend cache because Zend cache doesn't support ttl at the item level. Zend support ttl only at the Cache level.\nSo I modified the code to use Doctrine cache and it worked. \n. Thanks for the suggestion and the sample code. I feel it should be documented as using the ''Using IAM roles for Amazon EC2 instances\" is the recommended way to provide the credentials, the InstanceMetadataClient is used only in this case and it is very likely to fail and retry.\n. We faced issue connecting to instance metadata and InstanceMetadataClient.getInstanceProfileCredentials() throwing an InstanceProfileCredentialsException() 4 to 5 times in two weeks. That was majorly because the credentials cache issue (Zf2Cache was not getting refreshed as mentioned earlier) and hence the code was hitting the instance metadata for every request to dynamo db.\n. ",
    "NielsCorneille": "Thanks a lot for the snippet to sign requests. \nErrors outputted \"The promise was rejected\" instead of the normal exceptions (for example: Missing404Exception). \nI seems to be fixed by adding this promise:\nphp\n$oResponse = $oPsr7Handler($oSignedRequest)->then(\nfunction (\\Psr\\Http\\Message\\ResponseInterface $oResponse)\n{\n    return $oResponse;\n},\nfunction ($aError)\n{\n    return $aError['response'];\n}\n)->wait();\nNot sure if that is the correct fix, feel free to suggest a better one!\n. ",
    "sergey-rud": "@jeskew 's solution started working when I changed key 'host' to 'Host':\n```\n// Amazon ES listens on standard ports (443 for HTTPS, 80 for HTTP).\n$request['headers']['Host'][0] = parse_url($request['headers']['Host'][0])['host'];\n// Create a PSR-7 request from the array passed to the handler\n$psr7Request = new \\GuzzleHttp\\Psr7\\Request(\n    $request['http_method'],\n    (new \\GuzzleHttp\\Psr7\\Uri($request['uri']))\n        ->withScheme($request['scheme'])\n        ->withHost($request['headers']['Host'][0]),\n    $request['headers'],\n    $request['body']\n);\n```. ",
    "spellmen": "\n@jeskew 's solution started working when I changed key 'host' to 'Host':\n```\n// Amazon ES listens on standard ports (443 for HTTPS, 80 for HTTP).\n$request['headers']['Host'][0] = parse_url($request['headers']['Host'][0])['host'];\n// Create a PSR-7 request from the array passed to the handler\n$psr7Request = new \\GuzzleHttp\\Psr7\\Request(\n    $request['http_method'],\n    (new \\GuzzleHttp\\Psr7\\Uri($request['uri']))\n        ->withScheme($request['scheme'])\n        ->withHost($request['headers']['Host'][0]),\n    $request['headers'],\n    $request['body']\n);\n```\n\nThanks a lot. Its working fine. \nBut i'm facing an issue with parse_url function. I'm using latest version of php and elasticsearch. So that the parse_url function should be like this\n $request['headers']['Host'][0] = parse_url($request['headers']['Host'][0],PHP_URL_HOST);. ",
    "ronneseth": "Hi @jeskew:\nOnce I do that, the API is now returning an error. Is it easy for me to pull out the actual XML to verify I'm not making some silly mistake? I've successfully added TXT, A, MX, and CNAME records to Route 53 using the API.\nPHP Fatal error:  Uncaught exception 'Aws\\Route53\\Exception\\Route53Exception' with message 'Error executing \"ChangeResourceRecordSets\" on \"https://route53.amazonaws.com/2013-04-01/hostedzone/Z17XV3HFQ5R8OG/rrset/\"; AWS HTTP error: Client error: 400 InvalidInput (client): Invalid XML ; cvc-complex-type.2.4.b: The content of element 'ResourceRecord' is not complete. One of '{\"https://route53.amazonaws.com/doc/2013-04-01/\":Value}' is expected. - <?xml version=\"1.0\"?>\nSenderInvalidInputInvalid XML ; cvc-complex-type.2.4.b: The content of element 'ResourceRecord' is not complete. One of '{\"https://route53.amazonaws.com/doc/2013-04-01/\":Value}' is expected.b7ab7e69-a2e8-11e5-9324-27bdc410832c'\nexception 'GuzzleHttp\\Exception\\ClientException' with message 'Client error: 400' in /home/revolution/server-tools/aws_sdk/GuzzleHttp/Middleware.php:69\nStack trace:\nin /home/revolution/server-tools/aws_sdk/Aws/WrappedHttpHandler.php on line 152\n. Hi @jeskew --\nI found my issue. I accidentally had \"ResourceRecord\" wrapping AliasTarget. I read this post https://forums.aws.amazon.com/thread.jspa?threadID=152380 and realized that the error message is confusing in this case -- it has nothing to do with \"Value\". Since it didn't find AliasTarget at that level, it assumes that you must specify Value.\nSorry for the premature bug, please close.\n. Closing since this is a non issue, just a confusing Validator mesage (ideally it should say Value or AliasTarget required).\n. ",
    "xgin": "Take a look at the example of the request: http://docs.aws.amazon.com/Route53/latest/APIReference/API_CreateHostedZone.html#API_CreateHostedZone_Examples\nThe parameter is <DelegationSetId>NZ8X2CISAMPLE</DelegationSetId>\nBut SDK constructs <DelegationSetId>/delegationset/NZ8X2CISAMPLE</DelegationSetId> - such request is always failed with NoSuchDelegationSet error.\n. ",
    "IgorDePaula": "No, I read both documentation version and pratice with both version, nothing work satisfactory.\n. Yes.\nEm 15/03/2016 20:09, \"Jonathan Eskew\" notifications@github.com escreveu:\n\nAre you providing an SMTP password to the SES client?\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly or view it on GitHub\nhttps://github.com/aws/aws-sdk-php/issues/932#issuecomment-197063142\n. I read again the documentation, and see that for send email from instances ec2, i not need of credentials. Sorry by confusion.\n. I am using the version 3.* (on my composer). I got signup the user, confirm\nhim, but not login. I not found any method for login using an web app, like\na site in php.\n\n2016-05-17 17:08 GMT-03:00 cjyclaire notifications@github.com:\n\n@IgorDePaula https://github.com/IgorDePaula , May I ask the version of\nSDK that you're using? the Amazon Cognito Identity Provider service\nsupport is added in v3.18.0 release. Also, where did you see the\ndescription docs?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly or view it on GitHub\nhttps://github.com/aws/aws-sdk-php/issues/994#issuecomment-219837591\n. Ok, im my case, I do not use any external provider like facebook or google\nplus, only cognito. In Cognito provider, what method I use for verify the\nuser login and passord?\n\n2016-05-17 17:36 GMT-03:00 cjyclaire notifications@github.com:\n\nLogin options are passed in when you construct a CognitoIdentityProvider.\nsample code would look like:\n$provider = new CognitoIdentityProvider(            'poolId',            ['region' => 'us-east-1',  'version' => 'latest'],            [                'www.amazon.com' => 'access-token-old',                'graph.facebook.com' => 'access-token-fb',            ]        );\nAlso, please make sure that you have v3.18.0+ because this service is\nintroduced to PHP SDK since v3.18.0.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly or view it on GitHub\nhttps://github.com/aws/aws-sdk-php/issues/994#issuecomment-219845701\n. The doc of cognito indentity provider has any documentation.\n\nhttp://docs.aws.amazon.com/aws-sdk-php/v3/api/class-Aws.CognitoIdentity.CognitoIdentityProvider.html\n2016-05-17 17:40 GMT-03:00 principe.borodin@gmail.com \nprincipe.borodin@gmail.com:\n\nOk, im my case, I do not use any external provider like facebook or google\nplus, only cognito. In Cognito provider, what method I use for verify the\nuser login and passord?\n2016-05-17 17:36 GMT-03:00 cjyclaire notifications@github.com:\n\nLogin options are passed in when you construct a CognitoIdentityProvider.\nsample code would look like:\n$provider = new CognitoIdentityProvider(            'poolId',            ['region' => 'us-east-1',  'version' => 'latest'],            [                'www.amazon.com' => 'access-token-old',                'graph.facebook.com' => 'access-token-fb',            ]        );\nAlso, please make sure that you have v3.18.0+ because this service is\nintroduced to PHP SDK since v3.18.0.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly or view it on GitHub\nhttps://github.com/aws/aws-sdk-php/issues/994#issuecomment-219845701\n. can any explain for me or other users how work authentication and signup in\nphp by cognito? I configured it by console. I signup users already.\n\n\n2016-05-17 17:41 GMT-03:00 principe.borodin@gmail.com \nprincipe.borodin@gmail.com:\n\nThe doc of cognito indentity provider has any documentation.\nhttp://docs.aws.amazon.com/aws-sdk-php/v3/api/class-Aws.CognitoIdentity.CognitoIdentityProvider.html\n2016-05-17 17:40 GMT-03:00 principe.borodin@gmail.com \nprincipe.borodin@gmail.com:\n\nOk, im my case, I do not use any external provider like facebook or\ngoogle plus, only cognito. In Cognito provider, what method I use for\nverify the user login and passord?\n2016-05-17 17:36 GMT-03:00 cjyclaire notifications@github.com:\n\nLogin options are passed in when you construct a CognitoIdentityProvider\n.\nsample code would look like:\n$provider = new CognitoIdentityProvider(            'poolId',            ['region' => 'us-east-1',  'version' => 'latest'],            [                'www.amazon.com' => 'access-token-old',                'graph.facebook.com' => 'access-token-fb',            ]        );\nAlso, please make sure that you have v3.18.0+ because this service is\nintroduced to PHP SDK since v3.18.0.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly or view it on GitHub\nhttps://github.com/aws/aws-sdk-php/issues/994#issuecomment-219845701\n. I read this page already, this classe singup users only, not log in it.\n\n\n\n2016-05-17 17:54 GMT-03:00 cjyclaire notifications@github.com:\n\n@IgorDePaula https://github.com/IgorDePaula Will this helps?\nCognitoIdentityProviderClient\nhttp://docs.aws.amazon.com/aws-sdk-php/v3/api/class-Aws.CognitoIdentityProvider.CognitoIdentityProviderClient.html\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly or view it on GitHub\nhttps://github.com/aws/aws-sdk-php/issues/994#issuecomment-219850624\n. My sdk isntallation version is versions : * 3.18.6\n\n2016-05-17 17:56 GMT-03:00 principe.borodin@gmail.com \nprincipe.borodin@gmail.com:\n\nI read this page already, this classe singup users only, not log in it.\n2016-05-17 17:54 GMT-03:00 cjyclaire notifications@github.com:\n\n@IgorDePaula https://github.com/IgorDePaula Will this helps?\nCognitoIdentityProviderClient\nhttp://docs.aws.amazon.com/aws-sdk-php/v3/api/class-Aws.CognitoIdentityProvider.CognitoIdentityProviderClient.html\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly or view it on GitHub\nhttps://github.com/aws/aws-sdk-php/issues/994#issuecomment-219850624\n. I have much to thank you for attention. Sorry about that. I can not post\nanything on their forum.\n\n\n2016-05-18 16:12 GMT-03:00 cjyclaire notifications@github.com:\n\nGreat thanks for your clarification! I'm contacting service team to get\nmore information, and will keep following up.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly or view it on GitHub\nhttps://github.com/aws/aws-sdk-php/issues/994#issuecomment-220128476\n. I understand that cognito service do not authenticate an user on php app\nfor example, only mobile app. This is correct?\nEm 19/05/2016 18:50, \"cjyclaire\" notifications@github.com escreveu:\n@IgorDePaula https://github.com/IgorDePaula , just confirmed from\nservice team, Cognito does support web apps foe developer authenticated\nidentities workflow. Apologies for the noises on doc website, yet for this\nworkflow, it uses the same APIs for web as on mobile.\nOnce you have the login and token, you just need to call GetId and either\nGetOpenIdToken or GetCredentialsForIdentity with the logins map updated\nappropriately, and Cognito will vend credentials for you. These APIs won't\ncare about the source of the request at all.\nThere is a blog\nhttps://mobile.awsblog.com/post/TxBVEDL5Z8JKAC/Use-Amazon-Cognito-in-your-website-for-simple-AWS-authentication\nwith details.\nThere is the public doc guide for Developer Authenticated Identities\nhttp://docs.aws.amazon.com/cognito/latest/developerguide/developer-authenticated-identities.html,\nand you can ignore the noise like\"(Android and iOS only)\" ;)\nIf all of theses still doesn't help with your question, please let me know.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly or view it on GitHub\nhttps://github.com/aws/aws-sdk-php/issues/994#issuecomment-220456233\n. Ok, how I authenticate an user on php app for example? I do not want use an\nexternal proider like facebook, twitter, but only cognito. How I capture\nthe user password and login for authenticate the user on cognito? I read\n\"session app\" on anywhere on docs, but test another browser, for generate\nanother session, and not generate another session, I \"logged as previous\nuser\".\n\n2016-05-19 21:25 GMT-03:00 cjyclaire notifications@github.com:\n\nNo, they do support, it uses the same APIs for web as on mobile.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly or view it on GitHub\nhttps://github.com/aws/aws-sdk-php/issues/994#issuecomment-220487624\n. This blog I tested, generate all steps, but I confused how authenticate\nuser login and user password only cognito, and not using external provider.\n\n2016-05-19 21:55 GMT-03:00 principe.borodin@gmail.com \nprincipe.borodin@gmail.com:\n\nOk, how I authenticate an user on php app for example? I do not want use\nan external proider like facebook, twitter, but only cognito. How I capture\nthe user password and login for authenticate the user on cognito? I read\n\"session app\" on anywhere on docs, but test another browser, for generate\nanother session, and not generate another session, I \"logged as previous\nuser\".\n2016-05-19 21:25 GMT-03:00 cjyclaire notifications@github.com:\n\nNo, they do support, it uses the same APIs for web as on mobile.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly or view it on GitHub\nhttps://github.com/aws/aws-sdk-php/issues/994#issuecomment-220487624\n. I appreciate very much yout attention. I know that you do not suggest use\nexternal provider, but this process is confused.\n\n\nI followed all steps on page quoted previuosly. But I do not understand\nwhere my user is put your login and password for authenticate on my system.\nI signup and confirm signup the users by api and sdk already, but where my\nuser will put login and password and what method from sdk/api my\napplication I call for this action?\nIf cognito storage username and password from my user, I do not need have\nmy own authentication process.\nBut by you said, I suppose that I have my own authentication process. I\nsignup the user on cognito and receive yout token. I storage this token on\nmy database. When this user is authenticate on my system, I verify you\nusername, password on my database and yout token on cognito. This is the\nprocess?\nHow you said, I need implement the Identity Provider, what this provider\nneed do?\nCan you show me a draft of this process, please? If no, all right, I\ndesist. You help me very much already.\nAgain, I appreciate very much yout attention.\n2016-05-20 2:36 GMT-03:00 cjyclaire notifications@github.com:\n\nI fully understand your situation, I'm not suggesting using any external\nproviders.\nWith developer authenticated identities, you can register and authenticate\nusers via your own existing authentication process, while still using\nAmazon Cognito to synchronize user data and access AWS resources. And this\nprocess is in referred in the doc\nhttp://docs.aws.amazon.com/cognito/latest/developerguide/developer-authenticated-identities.html\n,\nYou need to implement an Identity Provider, then update the login map,\ngetting a token and connect to identity.\nAs I've mentioned, once you have the login and token, you just need to\ncall GetId and either GetOpenIdToken or GetCredentialsForIdentity with the\nlogins map updated appropriately, and Cognito will vend credentials for\nyou. These APIs won't care about the source of the request at all.\nThe service team also mentioned that if you have further questions about\ntheir service, they are more than happy to help you on the forum! I believe\nyou can register the aws developer forum\nhttps://forums.aws.amazon.com/forum.jspa?forumID=173&start=0 with ease.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly or view it on GitHub\nhttps://github.com/aws/aws-sdk-php/issues/994#issuecomment-220521337\n. Again, I appreciate very much yout attention. Thanks very much.\n\n2016-05-20 14:42 GMT-03:00 cjyclaire notifications@github.com:\n\nClosed #994 https://github.com/aws/aws-sdk-php/issues/994.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly or view it on GitHub\nhttps://github.com/aws/aws-sdk-php/issues/994#event-667452257\n. \n",
    "neerajrathod05": "Hey, Will this work for uploading directory to S3 \n$s3->uploadDirectory($new_sourcefolderpath, $bucket, $new_destinationfolder). ",
    "gdsdougherty": "That's what I ended up doing, but if that's the case this should probably be noted in the documentation somewhere, at the very least on the v2 specific documentation.  New users would definitely have issues, unless this information is more readily available.\n. Also, the information at the top of this section of the documentation seems to be outdated.\nhttp://docs.aws.amazon.com/AmazonS3/latest/dev/UsingAWSSDK.html#specify-signature-version\n. Hmm, it could have been an issue on my end, but I was unable to make any requests to us-west-2 with v2, and a day later, unable to make requests to any us-west-1 or 2 using v2.  It was only after changing to v4 this morning that I was able to get any responses other than the signature mismatch.  I guess I was assuming it was being phased out, but it could have been a service issue.  This would be a lot easier to distinguish if AWS let users make general inquiries about services, regardless of payment level.\n. ",
    "Dacascas": "Yes, big thanks I think its help\n. ",
    "alesi-cloud": "thank you.\nI'll try install PHP 5.5 or later then.\n. ",
    "nickmeessen": "Ahhh, didn't notice this feature it in the docs. I've implemented this yesterday morning and continued to monitoring it. No errors thus far, so it seems to resolve our issue.\nThanks! :)\n. ",
    "arpan03": "Getting same error after trying automatically flushing a queue autoFlushAt(10)\n$batch = BatchBuilder::factory()\n            ->transferCommands(10)\n            ->autoFlushAt(10)\n            ->build();\nError: \n```\nPHP Fatal error:  Uncaught exception 'Guzzle\\Service\\Exception\\CommandTransferException' with message 'Errors during multi transfer\n(Guzzle\\Http\\Exception\\CurlException) //vendor/guzzle/guzzle/src/Guzzle/Http/Curl/CurlMulti.php line 359\n[curl] 77: error setting certificate verify locations:\n  CAfile: //vendor/guzzle/guzzle/src/Guzzle/Http/Resources/cacert.pem\n  CApath: /etc/ssl/certs [url] https://email.us-east-1.amazonaws.com/\n0 //vendor/guzzle/guzzle/src/Guzzle/Http/Curl/CurlMulti.php(292): Guzzle\\Http\\Curl\\CurlMulti->isCurlException(Object(Guzzle\\Http\\Message\\EntityEnclosingRequest), Object(Guzzle\\Http\\Curl\\CurlHandle), Array)\n1 //vendor/guzzle/guzzle/src/Guzzle/Http/Curl/CurlMulti.php(257): Guzzle\\Http\\Curl\\CurlMulti->processResponse(Object(Guzzle\\Http\\Message\\EntityEnclosingRequest), Object(Guzzle\\Http\\Curl\\CurlHandle), Array)\n2 //vendo in //vendor/guzzle/guzzle/src/Guzzle/Batch/Batch.php on line 62\n```\n. For SesClient's factory?\n$client = SesClient::factory(array(\n        'credentials' => array(\n            'key'    => 'abc',\n            'secret' => '123',\n        ),\n        'region' => 'us-east-1',\n        'ssl.certificate_authority' => 'system'\n    ));\n. Ok I will try it and let you know!! Thanks!! \nBtw I was reading this: http://docs.aws.amazon.com/aws-sdk-php/v2/guide/credentials.html#credential-profiles \nBut I can't find ~/.aws/credentials folder under my HOME directory. Do I have to create it?\n. Thanks!! I'd do that :+1: \n. Btw if you can answer this then it would be super cool of you. For sending bulk emails through SES would you recommend using batchbuilder or commandpool or something else?\n. Hi Jonathan,\nNow I'm getting this error: \n```\nPHP Fatal error:  Uncaught exception 'Guzzle\\Service\\Exception\\CommandTransferException' with message 'Errors during multi transfer\n(Guzzle\\Http\\Exception\\CurlException) /var/www/vhosts/specificfeeds.com/vendor/guzzle/guzzle/src/Guzzle/Http/Curl/CurlMulti.php line 359\n[curl] 35: error:02001018:system library:fopen:Too many open files [url] https://email.us-east-1.amazonaws.com/\n0 /var/www/vhosts/specificfeeds.com/vendor/guzzle/guzzle/src/Guzzle/Http/Curl/CurlMulti.php(292): Guzzle\\Http\\Curl\\CurlMulti->isCurlException(Object(Guzzle\\Http\\Message\\EntityEnclosingRequest), Object(Guzzle\\Http\\Curl\\CurlHandle), Array)\n1 /var/www/vhosts/specificfeeds.com/vendor/guzzle/guzzle/src/Guzzle/Http/Curl/CurlMulti.php(257): Guzzle\\Http\\Curl\\CurlMulti->processResponse(Object(Guzzle\\Http\\Message\\EntityEnclosingRequest), Object(Guzzle\\Http\\Curl\\CurlHandle), Array)\n2 /var/www/vhosts/specificfeeds.com/vendor/guzzle/guzzle/src/Guzzle/Http/Curl/CurlMulti.php(240): Guzzle\\Http\\Curl\\CurlMulti->processMessages()\n3 /var/www/vhos in /var/www/vhosts/specificfeeds.com/vendor/guzzle/guzzle/src/Guzzle/Batch/Batch.php on line 62\n```\nI suspect this is due to too many concurrent requests. How should I fix this?\n. Hi Jonathan,\nI fixed it by breaking the main commands array in chunks of smaller arrays executing them one at a time and then putting the code to sleep for 15 secs between each array execution. Do you think this is the right way to do it? \n$chnkArray = array_chunk($this->commands,500);\nforeach($chnkArray as $command)\n{\n    $client->execute($command);\n    sleep(15);\n}\n. Using version 2.8.24 \nInstalled it through composer a few days back.\n. @jeskew I did read about this during my research did not try it though, now that you have recommended it I would definitely give it a try and let you know the results for the same!\n. @jeskew right now this is low on priority since these errors pushed us back quite a bit and now that this is fixed we are focusing on other things. But nevertheless, we will try it out in the coming week and I will let you know the results! \nThanks for all your help Jonathan!! really appreciate your prompt responses and feedback!!\n. @jeskew Hi Jonathan, I have a question:\nIf you remember, I am dividing the array which contains 1000s of mails in chunks of 500 each like this -\n$chnkArray = array_chunk($this->commands,500);\nforeach($chnkArray as $command)\n{\n    $client->execute($command);\n    sleep(15);\n}\nWhat happens when I get an error from SES, something like this: \n\nErrors during multi transfer\n(Aws\\Ses\\Exception\\SesException) /var/www/vhosts/vendor/aws/aws-sdk-php/src/Aws/Common/Exception/NamespaceExceptionFactory.php line 91\nIllegal address\n\nNow let's say it happened in the first chunk of 500 emails after 250 emails were sent to SES already, how does SES process the remaining 249 emails? Does it stop processing further or does it continue with the rest of the mails in that chunk? \n. @jeskew Thanks Jonathan, that's a very good news!! (for me), I was worried that I'm not sending out all the emails if I get error for any user. \nThanks again!!\n. ",
    "MiroCillik": "Isn't there a built-in backoff for these types of errors? I have set 'retries' to 20 in SqsClient, but it seems that it has no effect.\n. Yes it helped, thanks!\nOn Jan 12, 2016 7:45 PM, \"Jonathan Eskew\" notifications@github.com wrote:\n\n@MiroCillik https://github.com/MiroCillik does retrying on your end\nsolve the issue?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/aws/aws-sdk-php/issues/870#issuecomment-171009120.\n. Same problem here.\n. \n",
    "kirkbushell": "AWS sdk version: 3.11.7\n. Ohhhhh. I thought it wsa saying that the presetId itself needed to be an array!\nThanks :)\n. ",
    "nathankurtyka": "@jeskew  Thanks for the quick reply!\nI'm using a Java-backed Lambda (so no concept of context.succeed). But my Java Lambda is returning a String.  \nThe returned string value comes back, but the Model object clearly isn't initialized correctly (eg there's not internal array of values.. only a string value). \nLet me know if I can provide any more info. This will become a bigger problem for us in the near future.\n. ",
    "cwhite92": "Thanks for the quick turnaround! The client never has to know I had an ugly hack to manually remove objects that I didn't want to download in the first place. ;)\n. @jeskew: Unfortunately I think there may still be an issue with the implementation of this. The new changes allow me to download a subset of objects provided by an iterator, which is great, but the way I have to provide the keys of those objects seems counter-intuitive. I'll use some code from your tests as an example.\n```\n$justOneFile = new \\ArrayIterator(['s3://bucket/path/to/key']);\n$downloader = new Transfer($s3, $justOneFile, '/path/to/download/dir', [\n    'base_dir' => 's3://bucket/path',\n]);\n```\nIt seems redundant that I should be specifying the full path to the object (including the s3:// scheme and bucket name) in both the iterator and base_dir. Would it not be better to do something like:\n```\n$justOneFile = new \\ArrayIterator(['photos/christmas.jpg']);\n$downloader = new Transfer($s3, $justOneFile, '/path/to/download/dir', [\n    'base_dir' => 's3://bucket/media',\n]);\n```\nThe above will translate to s3://bucket/media/photos/christmas.jpg. I'm not sure that my iterator should be concerned with the S3 scheme or bucket name?\nRegardless, the main issue is now fixed, so thank you. :+1: \n. > the base_dir parameter is really there to indicate what subfolder of a given bucket should be considered the top level folder when downloading\nAh! In that case the current implementation does make sense. Just a case of me misunderstanding the role of the base_dir parameter I suppose. Thanks again.\n. @imshashank Thanks for getting back to me. The request isn't actually going via PHP, so those configuration options aren't relevant here. PHP is just being used to create the signed upload URL - the browser sends the file directly to S3.. @agursoy Thanks for looking into it! Glad you found the problem.\nIt seems that this isn't related to S3 after all, so I'll close this issue. Thanks all.. ",
    "usamamashkoor": "@ondrejhlavacek and @jeskew i am uploading the files to amazon s3 using jquery ajax to show progress bar and i am facing the same problem. Here is my scenario can you please \nI am facing one problem the file progress bar is working correctly but when it reaches 100% then the request in the browsers still runs for 2 to 3 minutes after 2 to 3 minutes the ajax request return this response \nRequestTimeout Your socket connection to the server was not read from or written to within the timeout period. Idle connections will be closed\nand file is does not upload on S3.\nkindly please help me on this issue thanks.\n. @kstich i am uploading the file using Jquery Ajax and uploading files using JavaScript Xhr object here is some code \nxhr.open('POST', 'bucket_name', true)\nthen i am using the progress event listener to track the progess of the upload so that i can show progress bar to the user \nxhr.upload.addEventListener(\"progress\", uploadProgress, false);\ni think that issue is related to Laravel php amazon s3 api can you please help me on this or suggest me how can i fix this if i really need to submit a new issue then sure i will do it with more code but just want to confirm it before submitting the issue.\nThanks.     . Hi @kstich Thanks for the reply.Can you please suggest how i can upload files to s3 bucket using JavaScript SDK and can also show user the progress bar during the file upload and note please i am uploading video files thanks.It will be really helpful thanks in advance.. ",
    "dadixon": "Thanks.  I created a new file that's just credentials with the correct format and I still get errors.\n. Cannot read credentials from /.aws/credentials\n. should I open it to everyone? www-data?\n. nvm.  that doesn't do anything either\n. I've moved the folder to my user's home folder and I still get this error.  I changed the mods also to see if that was it but I get the same error.  Thank you for baring with me.\n. Works locally. :)\n. Seems like I'm having this same issue but with s3.\nhttp://stackoverflow.com/questions/31883374/aws-credentials-not-working-aws-credentials\n. I might just have to go with the hard coded way. :(\n. ",
    "rtowings": "For others that find this thread...\nThe issue is that the sdk uses the environment variable $HOME to establish the users home directory.\nYou MUST set $HOME to an appropriate value before instantiating a client (otherwise the SDK looks for the credentials in root and will fail).. Ahhh...  The SDK attempts to get the Home directory from the environment variable $HOME and prepends the value to /.aws/credentials.  Thus, if HOME is NULL then getHomeDir() returns /.aws/credentials and will attempt to find the credentials in root.\nWORKAROUND is to set the HOME variable to an appropriate value before instantiating a client.\nSeems like this could be handled a little better, though.. Hi @Bhavna93  - you need to set the HOME variable early in your process.  A good place would be in your index.php when you setup the rest of your environment.  Just add something like:\n```\n//HOME is required for AWS store credentials to work.\nputenv('HOME=/var/www');\nrequire '../vendor/autoload.php';\nuse Aws\\Sms\\SmsClient;\n```\nIn this instance, the credentials would be found in /var/www/.aws/credentials - but you want to set the HOME variable to wherever you have placed your credentials.. ",
    "siva-chegondi": "@dadixon \"Cannot read credentials from /.aws/credentials\"\nError itself saying it is looking under \"/\" not under home folder, so place .aws folder under \"/\" and give all permissions to www-data user  ( chmod 700 /.aws/ ) ( chown -R www-data:www-data /.aws )\nHave Fun,\n@smartsiva. ",
    "mathewpeterson": "I did not realize I was using v2 of the client. I've upgraded to v3 it's working as expected. \nThank you!\n. ",
    "xibz": "Looks good \n. :ship: \n. Overall, it looks good. Have a couple comments. \n. LGTM, :shipit: \n. LGTM, :shipit:\n. LGTM, :shipit:\n. LGTM, :shipit:\n. Im assuming there's already a test for thishttp_decode_content option. If not, I'd add that, otherwise it looks good. \n. Should UNSIGNED-PAYLOAD be stored as a constant to drive consistency?\n. Looks good. :shipit:\n. @cjyclaire - This change looks good. When modifying the .feature files, the cucumber test still pass, correct?\n. @cjyclaire :shipit:\n. @mtdowling - No idea. I am confused there as well. Which is why I asked the question. However, I know the previous value UNSIGNED-PAYLOAD was originally used. So, the constant must be of that value. However, I think we need more testing or at least fix the testing to ensure the proper things are tested. We need a mock signature test with UNSIGNED-PAYLOAD.\n. These are integration tests. Either S3 is accepting the space character as valid or something else is wrong with the test itself. \n@cjyclaire - Can you confirm that you are getting expected responses from S3 when signing UNSIGNED PAYLOAD and signing UNSIGNED-PAYLOAD?\n. @jeskew @cjyclaire - A test should have thrown an error. A sufficient test should be added to prevent this from happening again. Thanks for chiming in :)!\n. @jeskew - Thanks for the help and clarification!\n. These changes look good, :shipit:\n. LGTM, :shipit:\n. LGTM, :shipit:\n. LGTM, :shipit:\n. \ud83d\udea2 \n. :shipit:\n. No need for the ternary\nphp\n'retries' => $value === true,\n. same here\n. UNSIGNED-PAYLOAD for consistency with s3.getPresignedPayload\n. Should we handle when the regex doesn't match? Maybe not. Also is there a way we can test that this works?\n. It looks like --porcelain is only supported in versions 2.10.0 and above. This may be an issue with the git versions we use. May want to check on that.\n. Okay, that's good. When I was running the command it was erroring out with porcelain not defined. Must've misspelled it!\n. Shouldn't this check only for bucket prefix matching against {{bucket}}.{{host}}? Because what if they used a host with the same bucket name?. Should be\nphp\nif (strpos($uri->getHost(), $this->bucket . '.') !== 0) {\n}. ",
    "steefaan": "@jeskew I need some more time to test it because I need to observe a deadline and have no time to test it against SDK 3.x now, because it works for 2.x and I have opened and issue for the next version to migrate to 3.x.\nIf you want to close this issue, you can do this, I would reopen it if it doesn't work. I will also post an answer as soon as I have tried it.\n. ",
    "adam-ut": "Thought I selected that. I'll fix it. Thanks!\n. Here's the error message: Unable to find class file for class \"Aws\\Iam\\Exception\\AccessDeniedException\"\nHere's the stack trace:\n- spl_autoload_call was called \n- class_exists was called from NamespaceExceptionFactory.php:74 \n- Aws\\Common\\Exception\\NamespaceExceptionFactory->fromResponse was called from ExceptionListener.php:55 \n- Aws\\Common\\Exception\\ExceptionListener->onRequestError was called \n- call_user_func was called from EventDispatcher.php:164 \n- Symfony\\Component\\EventDispatcher\\EventDispatcher->doDispatch was called from EventDispatcher.php:53 \n- Symfony\\Component\\EventDispatcher\\EventDispatcher->dispatch was called from Request.php:589 \n- Guzzle\\Http\\Message\\Request->processResponse was called from Request.php:378 \n- Guzzle\\Http\\Message\\Request->setState was called from EntityEnclosingRequest.php:49 \n- Guzzle\\Http\\Message\\EntityEnclosingRequest->setState was called from CurlMulti.php:303 \n- Guzzle\\Http\\Curl\\CurlMulti->processResponse was called from CurlMulti.php:257 \n- Guzzle\\Http\\Curl\\CurlMulti->processMessages was called from CurlMulti.php:240 \n- Guzzle\\Http\\Curl\\CurlMulti->executeHandles was called from CurlMulti.php:224 \n- Guzzle\\Http\\Curl\\CurlMulti->perform was called from CurlMulti.php:111 \n- Guzzle\\Http\\Curl\\CurlMulti->send was called from CurlMultiProxy.php:94 \n- Guzzle\\Http\\Curl\\CurlMultiProxy->send was called from Client.php:284 \n- Guzzle\\Http\\Client->send was called from AbstractClient.php:256 \n- Aws\\Common\\Client\\AbstractClient->send was called from Client.php:136 \n- Guzzle\\Service\\Client->execute was called from AbstractCommand.php:153 \n- Guzzle\\Service\\Command\\AbstractCommand->execute was called from AbstractCommand.php:189 \n- Guzzle\\Service\\Command\\AbstractCommand->getResult was called from Client.php:76 \n- Guzzle\\Service\\Client->__call was called from AbstractClient.php:104 \n- Aws\\Common\\Client\\AbstractClient->__call was called from AwsConnector.class.php:123 \n- Aws\\Iam\\IamClient->getUser was called from AwsConnector.class.php:123\n. Yes, there are other spl_autoload_call functions being loaded in our framework which could be doing as you've outlined. I'll have a look around the code using the information you've provided.\nThank you!\n. Thanks, @jeskew!\n. ",
    "elazar": "@jeskew Don't know that this is an option you can consider, but it looks like it's possible to use HHVM 3.11 in a Trusty beta environment. See this issue for details.\n. ",
    "thaiphan": "Thanks!\n. ",
    "PatchRanger": "Thank you for clarification, it helps!\n. ",
    "ajcerqueti": "You're exactly right, the config was instantiating a SnsClient perfectly (apart from the missing version argument), but I was injecting a S3Client I'd also configured into my service instead, hence it not failing until it couldn't find the command! Thanks, I've corrected my config in case anyone using Symfony finds this.\n. ",
    "bill-s": "Jonathan,\nThanks.  I would suggest that the AWS S3 documentation\u2019s example be updated to include the information you provided.\nThe biggest complaint about the SDK is its sparse documentation and examples.\nBill\n\nOn Feb 22, 2016, at 4:11 PM, Jonathan Eskew notifications@github.com wrote:\nClosing, as catching Aws\\Exception\\MulitpartUploadException would stop the program from exiting with an uncaught exception. Please feel free to reopen if you have any questions or concerns.\n\u2014\nReply to this email directly or view it on GitHub https://github.com/aws/aws-sdk-php/issues/912#issuecomment-187446240.\n. Simply adding \"catch (MultipartUploadException $e) {...}\" does NOT catch the exception.  What needs to change to catch the exception?\n\nWhile increasing the retry count has reduced the frequency, the upload is still failing occasionally on the same large file.\n. Yes.  I have added a \"catch (MultipartUploadException $e) {...}\" immediately after the \"catch (S3Exception) {...}\" in the code above.  It does not catch the exception.  When doing the multi-part upload, is it running on a separate thread which may not be caught by this \"suspended\" thread?\n. At any rate, it appears that there is bug in the library which is preventing the calling code to catch the exception.  Since I cannot reopen this issue, should I file a new one?\n. This is the revised code with the added catch:\n$manager = new \\Aws\\S3\\Transfer($client, $source, $dest /*, array(\"debug\"=>true) */);\ntry {\n    // Perform the transfer synchronously.\n    $manager->transfer();\n    }\ncatch (S3Exception $e) {\n    $errMsg = 'S3 Error: ' . $e->getMessage();\n    log_message(ERROR, AWS_S3_ERROR_CLASS, $errMsg . '[$dest]');\n    }\ncatch (MultipartUploadException $e) {\n    $errMsg = 'S3 Error/Multipart Upload: ' . $e->getMessage();\n    log_message(ERROR, AWS_S3_ERROR_CLASS, $errMsg . '[$dest]');\n    }\ncatch (AwsException $e) {\n    $errMsg = 'AWS Error: RequestID=' . $e->getAwsRequestId() . ' Type= ' .\n        $e->getAwsErrorType() . 'Error Code=' . $e->getAwsErrorCode();\n    log_message(ERROR, AWS_S3_ERROR_CLASS, $errMsg . '[$dest]');\n    }\nIf I need to change something, please let me know.  This does NOT catch the MultipartUploadException.\n. I did not have the \"use\" statement, which I've now added.  Unfortunately, I won't know for a while whether that is my problem...  Thanks for your patience.\nI strongly suggest AWS's simple documentation example be expanded to include information on the multipart upload when uploading a folder.  It will save others a lot of grief.\n. ",
    "kringkaste": "Thanks for your help. It works for me, but you are right, it isn't really applicable on a HEAD request. The solution in your PR in guzzle would be ok, so anyone who likes to get the original header can get it. I hope it will be accepted.\n. ",
    "FabioQ": "Thanks @jeskew , smooth fix, tested it right now and it solved my problem with iterator.\nWaiting for the release on master\n. ",
    "kaihendry": "The code once upon time lived on http://aws.amazon.com/code/Amazon-S3/1618 but the download link is broken. You can find the code on Github though but it's a total PITA.\n. I've managed to cobble a PHP script together myself. I'll be interested in how or if the filename is transmitted and content types. Multiple file uploads would be nice bonus.\n. It's not clear to me how you do your signature. Have a look at https://github.com/kaihendry/phps3post/blob/master/index.php. ",
    "jgardezi": "Hi,\nI have tried to use this example https://docs.aws.amazon.com/aws-sdk-php/v3/guide/service/s3-presigned-post.html. \nMy code is\nuse Aws\\S3\\PostObjectV4;\n// Set some defaults for form input fields\n$formInputs = [\n   'acl' => 'private',\n   'key' => $key,\n   'Content-Type' => '',\n];\n\n// Construct an array of conditions for policy\n$options = [\n    ['acl' => 'private'],\n    ['bucket' => 'mybucket'],\n    ['starts-with', '$key', ''user/eric/''],\n];\n\n$postObject = new PostObjectV4(\n    $this->s3Client,\n    'mybucket',\n    $formInputs,\n    $options,\n    '+2 hours'\n);\n\n$postObject->getFormAttributes();\n$postObject->getFormInputs();\n\nIt always gives me the following error.\n400: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<Error><Code>InvalidToken</Code><Message>The provided token is malformed or otherwise invalid.</Message><Token-0>false</Token-0><RequestId>2A47E5917AA7B78B</RequestId><HostId>4gCS/w3dSqGDzlhPk0BiwtlAQqgyijfR7RRtKLpYv7DIS1i4HNgWgvKqLTSfB301BTbEbZTC+mI=</HostId></Error>\nCan someone help me on this?\nKind regards\nJaved Gardezi. @kaihendry the example you have posted is not using aws-sdk. I though aws-sdk would handle this for you.\nIn the above example. I am using aws-sdk.\n. @cjyclaire No, I am not passing token in the credentials. I have read about it here http://docs.aws.amazon.com/AmazonS3/latest/dev/UsingDevPay.html. \nI haven't found any example on it using php. Is token necessary in this case?. Ok, in S3Client 'debug'   => true is breaking my CORS. So I can not post my debug logs here. Unless there is other way to debug this.\nS3Client\nS3Client([\n            'version' =>latest,\n            'region'  =>ap-southeast-2,\n            //'debug'   => true\n        ]);\nI was using aws/aws-sdk-php version 3.20.2 and now 3.20.4. Both are giving me same issue.\nFrom my above code. I am getting the following form Attributes ($postObject->getFormAttributes())\n{\n         \"action\": \"https://mybucket.s3-ap-southeast-2.amazonaws.com\",\n         \"method\": \"POST\",\n         \"enctype\": \"multipart/form-data\"\n      },\nForm input ($postObject->getFormInputs())\n{\n         \"acl\": \"private\",\n         \"key\": \"user/eric/VnHz-2016-12-12-11-47-12.jpg\",\n         \"Content-Type\": \"image/jpeg\",\n         \"X-Amz-Security-Token\": false,\n         \"X-Amz-Credential\": \"AKIAIZ2HSMBW647AX5OA/20161212/ap-southeast-2/s3/aws4_request\",\n         \"X-Amz-Algorithm\": \"AWS4-HMAC-SHA256\",\n         \"X-Amz-Date\": \"20161212T004714Z\",\n         \"Policy\": \"eyJleHBpcmF0aW9uIjoiMjAxNi0xMi0xMlQwMjo0NzoxNFoiLCJjb25kaXRpb25zIjpbeyJhY2wiOiJwcml2YXRlIn0seyJidWNrZXQiOiJsb2NhbC1jb2xsZWN0b3J5In0sWyJzdGFydHMtd2l0aCIsIiRrZXkiLCJuc3d0cmFuc3BvcnRcL3N0b3JhZ2VcL3VwbG9hZHNcL2ltYWdlc1wvIl0seyJ4LWFtei1zZWN1cml0eS10b2tlbiI6ZmFsc2V9LHsiWC1BbXotRGF0ZSI6IjIwMTYxMjEyVDAwNDcxNFoifSx7IlgtQW16LUNyZWRlbnRpYWwiOiJBS0lBSVoySFNNQlc2NDdBWDVPQVwvMjAxNjEyMTJcL2FwLXNvdXRoZWFzdC0yXC9zM1wvYXdzNF9yZXF1ZXN0In0seyJYLUFtei1BbGdvcml0aG0iOiJBV1M0LUhNQUMtU0hBMjU2In1dfQ==\",\n         \"X-Amz-Signature\": \"51155ce0a5c0bb6861e3cd50bcf361734bcca96128b32d53cac69f04a80d9baf\"\n      }\nNow my Post Request Pay Load looks like this\n`\n------WebKitFormBoundarykv0b2lzbjooiMKod\nContent-Disposition: form-data; name=\"acl\"\nprivate\n------WebKitFormBoundarykv0b2lzbjooiMKod\nContent-Disposition: form-data; name=\"key\"\nuser/eric/VnHz-2016-12-12-11-47-12.jpg\n------WebKitFormBoundarykv0b2lzbjooiMKod\nContent-Disposition: form-data; name=\"Content-Type\"\nimage/jpeg\n------WebKitFormBoundarykv0b2lzbjooiMKod\nContent-Disposition: form-data; name=\"X-Amz-Security-Token\"\nfalse\n------WebKitFormBoundarykv0b2lzbjooiMKod\nContent-Disposition: form-data; name=\"X-Amz-Credential\"\nAKIAIZ2HSMBW647AX5OA/20161212/ap-southeast-2/s3/aws4_request\n------WebKitFormBoundarykv0b2lzbjooiMKod\nContent-Disposition: form-data; name=\"X-Amz-Algorithm\"\nAWS4-HMAC-SHA256\n------WebKitFormBoundarykv0b2lzbjooiMKod\nContent-Disposition: form-data; name=\"X-Amz-Date\"\n20161212T004714Z\n------WebKitFormBoundarykv0b2lzbjooiMKod\nContent-Disposition: form-data; name=\"Policy\"\neyJleHBpcmF0aW9uIjoiMjAxNi0xMi0xMlQwMjo0NzoxNFoiLCJjb25kaXRpb25zIjpbeyJhY2wiOiJwcml2YXRlIn0seyJidWNrZXQiOiJsb2NhbC1jb2xsZWN0b3J5In0sWyJzdGFydHMtd2l0aCIsIiRrZXkiLCJuc3d0cmFuc3BvcnRcL3N0b3JhZ2VcL3VwbG9hZHNcL2ltYWdlc1wvIl0seyJ4LWFtei1zZWN1cml0eS10b2tlbiI6ZmFsc2V9LHsiWC1BbXotRGF0ZSI6IjIwMTYxMjEyVDAwNDcxNFoifSx7IlgtQW16LUNyZWRlbnRpYWwiOiJBS0lBSVoySFNNQlc2NDdBWDVPQVwvMjAxNjEyMTJcL2FwLXNvdXRoZWFzdC0yXC9zM1wvYXdzNF9yZXF1ZXN0In0seyJYLUFtei1BbGdvcml0aG0iOiJBV1M0LUhNQUMtU0hBMjU2In1dfQ==\n------WebKitFormBoundarykv0b2lzbjooiMKod\nContent-Disposition: form-data; name=\"X-Amz-Signature\"\n51155ce0a5c0bb6861e3cd50bcf361734bcca96128b32d53cac69f04a80d9baf\n------WebKitFormBoundarykv0b2lzbjooiMKod\nContent-Disposition: form-data; name=\"file\"; filename=\"VnHz-2016-12-12-11-47-12.jpg\"\nContent-Type: image/jpeg\n------WebKitFormBoundarykv0b2lzbjooiMKod--\n`\nThe error i am getting is\n400: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<Error><Code>InvalidToken</Code><Message>The provided token is malformed or otherwise invalid.</Message><Token-0>false</Token-0><RequestId>B79FBA93126F2A4B</RequestId><HostId>2AH3TG0Kx8s+NcrUkBeRGFpfClF6Eh7DhssfeIKLB93ZC3uvHOsiP+1zzknmI15yZBxlc3alRTg=</HostId></Error>\n@cjyclaire Is the above information good enough for debug or need more information.\n. ",
    "trsteel88": "I'm struggling with this as well.\nI want to create a signed PutObject that adheres to certain conditions.\n```php\n        /* @var Command $uploadCmd /\n        $uploadCmd = $s3Client->getCommand('PutObject', array(\n            'Bucket' => 'some-bucket',\n            'ACL' => 'private',\n            'Key' => 'image-'.time().'.png',\n            'ContentType' => 'image/jpeg',\n            'ContentLength' => 5776,\n        ));\n    $signedUrl = $s3Client->createPresignedRequest($uploadCmd, new \\DateTime('+1 hour'));\n\n    echo $signedUrl->getUri();die;\n\n```\nI've set a content length, content type. However, when posting to this url, it allows me to put any file of any size/type.\nShouldn't the api respond with an access denied error?\nHow do I enforce conditions on this url?. ",
    "neilghosh": "I think I should have been clear that , I am talking about the empty top level attribute. Consider the following example \nMarshaler->marshalJson(\n'{\n  \"key01\": {\n    \"subkey\": \"\"\n  }\n}'\n);\nThis returns the error as well. \nFor PutItem the attribute itself is not \"\" but its somewhere inside the complex value. \nI understand Storing it as a map actually exposes all the values in all level for querying. If the intention is to now allow empty string at any depth, I would have no choice but to store the json as string. Not sure if its more efficient if my intention is to always query the whole value from the key. But it defeats the purpose of document storage where the use case may require few values to be empty . If the business logic prunes it before sending to DynamoDB , there is no way to retrive it back.\n. ",
    "Pierozi": "I meet the same issue, i don't think patch Marshaler is a good idea because it's application side decision to have or not empty string.\nYou can easily replace your empty string by null with : \n``` php\narray_walk_recursive($data, function(& $item) {\n    if ('' === $item) {\n        $item = null;\n    }\n});\n```\n. ",
    "worldlyjohn": "For me it was a permission problem. PHP output shows fatal error, but when I check my custom log I get a 403 Forbidden from AWS.  Make sure the key/secret has the correct permission.  Turns out you need PutObjectAcl when doing a PutObject\n. make sure you have permission for both PutObject and PutObjectAcl\n. ",
    "Yahampath": "Hi,\nI have the same issue exception 'Symfony\\Component\\Debug\\Exception\\FatalErrorException' with message 'Allowed memory size of 134217728 bytes exhausted (tried to allocate 172540994 bytes)' in <project_path>/vendor/guzzlehttp/psr7/src/Stream.php:94\nI'm using guzzle ~6.0 on Laravel 5.3\n. ",
    "ppaulis": "Hey @jeskew ,\nthanks for the quick reply! What I have tried so far:\n- using the standard setting without specifying a SSL/TLS version\n- I tried with TLS 1.2:\n<?php\nreturn\n    [\n        'aws' =>\n        [\n            'region' => 'us-east-1',\n            'curl.options' => [\n                'CURLOPT_SSLVERSION' => 'CURL_SSLVERSION_TLSv1_2'\n            ]\n        ]\n    ]\nor by directly using the value of the constant:\n'CURLOPT_SSLVERSION' => 6\nIt's a ZF2 application, so this code is in a aws.local.php file in my application configuration. I'll try to put it directly in the creation of the SwfClient.\nI tried, so far, OpenSSL 1.0.1f and 1.0.2g. Both with the same result.\nThanks for your help!\nPascal\n. I also tried using the CA cert bundle from the curl website by specifying it in the php.ini with openssl.cafile, no luck :-/\n. a few additional infos:\nubuntu@ip-10-0-11-226:/tmp$ curl -V\ncurl 7.36.0 (x86_64-pc-linux-gnu) libcurl/7.36.0 OpenSSL/1.0.1f zlib/1.2.8 libidn/1.28\nProtocols: dict file ftp ftps gopher http https imap imaps ldap ldaps pop3 pop3s rtsp smtp smtps telnet tftp \nFeatures: AsynchDNS GSS-Negotiate IDN IPv6 Largefile NTLM NTLM_WB SSL libz TLS-SRP\nand here's the output of a connection attempt to the SWF endpoint:\n```\nubuntu@ip-10-0-11-226:/tmp$ openssl s_client -connect \"swf.us-west-2.amazonaws.com:443\"\nCONNECTED(00000003)\ndepth=1 C = US, O = Symantec Corporation, OU = Symantec Trust Network, CN = Symantec Class 3 Secure Server CA - G4\nverify error:num=20:unable to get local issuer certificate\nverify return:0\nCertificate chain\n 0 s:/C=US/ST=Washington/L=Seattle/O=Amazon.com, Inc./CN=swf.us-west-2.amazonaws.com\n   i:/C=US/O=Symantec Corporation/OU=Symantec Trust Network/CN=Symantec Class 3 Secure Server CA - G4\n 1 s:/C=US/O=Symantec Corporation/OU=Symantec Trust Network/CN=Symantec Class 3 Secure Server CA - G4\n   i:/C=US/O=VeriSign, Inc./OU=VeriSign Trust Network/OU=(c) 2006 VeriSign, Inc. - For authorized use only/CN=VeriSign Class 3 Public Primary Certification Authority - G5\nServer certificate\n-----BEGIN CERTIFICATE-----\n(removed for the sake of readability)\n-----END CERTIFICATE-----\nsubject=/C=US/ST=Washington/L=Seattle/O=Amazon.com, Inc./CN=swf.us-west-2.amazonaws.com\nissuer=/C=US/O=Symantec Corporation/OU=Symantec Trust Network/CN=Symantec Class 3 Secure Server CA - G4\n---\nNo client certificate CA names sent\n---\nSSL handshake has read 2762 bytes and written 621 bytes\n---\nNew, TLSv1/SSLv3, Cipher is AES128-SHA\nServer public key is 2048 bit\nSecure Renegotiation IS NOT supported\nCompression: NONE\nExpansion: NONE\nSSL-Session:\n    Protocol  : TLSv1\n    Cipher    : AES128-SHA\n    Session-ID: DDAB0212D8D9B49556328037D194D03EB7F22E50B0548E447B015F024874963C\n    Session-ID-ctx: \n    Master-Key: D1968248414FFB3CB5AE009DB37E5836837F8257F1540170936EDC45BDDC3CFC60ECE93168D9D15D8B89765D917B1782\n    Key-Arg   : None\n    PSK identity: None\n    PSK identity hint: None\n    SRP username: None\n    Start Time: 1457086840\n    Timeout   : 300 (sec)\n    Verify return code: 20 (unable to get local issuer certificate)\n---\n```\nWhen I run instead:\nopenssl s_client -connect \"swf.us-west-2.amazonaws.com:443\" -CAfile /tmp/cacert.pem\nI get a return code 0:\n```\nubuntu@ip-10-0-11-226:/tmp$ openssl s_client -connect \"swf.us-west-2.amazonaws.com:443\" -CAfile /tmp/cacert.pem \nCONNECTED(00000003)\ndepth=2 C = US, O = \"VeriSign, Inc.\", OU = VeriSign Trust Network, OU = \"(c) 2006 VeriSign, Inc. - For authorized use only\", CN = VeriSign Class 3 Public Primary Certification Authority - G5\nverify return:1\ndepth=1 C = US, O = Symantec Corporation, OU = Symantec Trust Network, CN = Symantec Class 3 Secure Server CA - G4\nverify return:1\ndepth=0 C = US, ST = Washington, L = Seattle, O = \"Amazon.com, Inc.\", CN = swf.us-west-2.amazonaws.com\nverify return:1\n\nCertificate chain\n 0 s:/C=US/ST=Washington/L=Seattle/O=Amazon.com, Inc./CN=swf.us-west-2.amazonaws.com\n   i:/C=US/O=Symantec Corporation/OU=Symantec Trust Network/CN=Symantec Class 3 Secure Server CA - G4\n 1 s:/C=US/O=Symantec Corporation/OU=Symantec Trust Network/CN=Symantec Class 3 Secure Server CA - G4\n   i:/C=US/O=VeriSign, Inc./OU=VeriSign Trust Network/OU=(c) 2006 VeriSign, Inc. - For authorized use only/CN=VeriSign Class 3 Public Primary Certification Authority - G5\n\nServer certificate\n-----BEGIN CERTIFICATE-----\n...\n-----END CERTIFICATE-----\nsubject=/C=US/ST=Washington/L=Seattle/O=Amazon.com, Inc./CN=swf.us-west-2.amazonaws.com\nissuer=/C=US/O=Symantec Corporation/OU=Symantec Trust Network/CN=Symantec Class 3 Secure Server CA - G4\n---\nNo client certificate CA names sent\n---\nSSL handshake has read 2762 bytes and written 621 bytes\n---\nNew, TLSv1/SSLv3, Cipher is AES128-SHA\nServer public key is 2048 bit\nSecure Renegotiation IS NOT supported\nCompression: NONE\nExpansion: NONE\nSSL-Session:\n    Protocol  : TLSv1\n    Cipher    : AES128-SHA\n    Session-ID: D908CDB1F5DD27E4281543407894B92FA33A181172068DA98FCAD4CF77E2963C\n    Session-ID-ctx: \n    Master-Key: 658CED64C142D37CCCAC7F2163F707357D071894874288AEF5B364DD81A533A88B111BB68059F9C8073A47F86EA212BC\n    Key-Arg   : None\n    PSK identity: None\n    PSK identity hint: None\n    SRP username: None\n    Start Time: 1457087123\n    Timeout   : 300 (sec)\n    Verify return code: 0 (ok)\n---\n``\n. btw, the problem appears both on us-east-1 and us-west-2\n. @jeskew yes, it is.\n. I have currently running a few machines with an older version of our application, that uses the v2 of the SDK. To double check, that the exception doesn't appear on this version. I'll keep you updated!\n. @jeskew The result is as expected. The same machines with the old application version (sdk v2.8.18, guzzle 3.7.1) don't produce the exceptions.\n. @jeskew without specifying the theopenssl.cafile` I get:\narray(8) {\n  [\"default_cert_file\"]=>\n  string(21) \"/usr/lib/ssl/cert.pem\"\n  [\"default_cert_file_env\"]=>\n  string(13) \"SSL_CERT_FILE\"\n  [\"default_cert_dir\"]=>\n  string(18) \"/usr/lib/ssl/certs\"\n  [\"default_cert_dir_env\"]=>\n  string(12) \"SSL_CERT_DIR\"\n  [\"default_private_dir\"]=>\n  string(20) \"/usr/lib/ssl/private\"\n  [\"default_default_cert_area\"]=>\n  string(12) \"/usr/lib/ssl\"\n  [\"ini_cafile\"]=>\n  string(0) \"\"\n  [\"ini_capath\"]=>\n  string(0) \"\"\n}\nand after specifying openssl.cafile in the php.ini, I get:\narray(8) {\n  [\"default_cert_file\"]=>\n  string(21) \"/usr/lib/ssl/cert.pem\"\n  [\"default_cert_file_env\"]=>\n  string(13) \"SSL_CERT_FILE\"\n  [\"default_cert_dir\"]=>\n  string(18) \"/usr/lib/ssl/certs\"\n  [\"default_cert_dir_env\"]=>\n  string(12) \"SSL_CERT_DIR\"\n  [\"default_private_dir\"]=>\n  string(20) \"/usr/lib/ssl/private\"\n  [\"default_default_cert_area\"]=>\n  string(12) \"/usr/lib/ssl\"\n  [\"ini_cafile\"]=>\n  string(15) \"/tmp/cacert.pem\"\n  [\"ini_capath\"]=>\n  string(0) \"\"\n}\nI'm currently using PHP 5.6.18.\nI already tested this configuration, with the CA bundle specified in the php.ini. The exceptions keep appearing :-/\n. We never handled the SSL_CERT_FILE or openssl.cafile manually so far, because guzzle v3 handled this perfectly itself.\nI'll give it a try with the http option and keep you updated!\nThank you, Jonathan, for your time!\n. I specified the ca bundle now manually in the php code:\n$aws->createSwf([\n            'version' => '2012-01-25',\n            'http' => [\n                'verify' => '/tmp/cacert.pem',\n            ]\n        ]);\nthe region, domain, etc. is loaded via the ZF2 config files.\n. Hmm, we are using simple EC2 instances with Ubuntu in a VPC on AWS. The outgoing traffic passes through a NAT (http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_NAT_Instance.html). But I'm not sure if the connection to the AWS services like SWF remain in the internal network... I'll check that with a collegue and get back to you.\nThanks!\n. @jeskew In confirm that connections to SWF go through the NAT\n. @jeskew The 'verify' => '/tmp/cacert.pem' unfortunately doesn't help either :-/\n. it's in place:\n```\nprotected function getClient()\n    {\n        /* @var \\Aws\\Sdk $aws /\n        $aws = $this->getServiceLocator()->get(Sdk::class);\n    return $aws->createSwf([\n        'version' => '2012-01-25',\n        'http' => [\n            'curl' => [\n                CURLOPT_SSLVERSION => CURL_SSLVERSION_TLSv1_0,\n            ]\n        ]\n    ]);\n}\n\n```\nI'll keep you updated! Thanks!\n. This sounds like a very serious lead.. It would explain the fact that the error is completely random.\n. Got again an exception, but I'm trying now with:\n```\nprotected function getClient()\n    {\n        /* @var \\Aws\\Sdk $aws /\n        $aws = $this->getServiceLocator()->get(Sdk::class);\n    return $aws->createSwf([\n        'version' => '2012-01-25',\n        'curl.options' => [\n            CURLOPT_SSLVERSION => CURL_SSLVERSION_TLSv1_0,\n        ]\n    ]);\n}\n\n```\nI think I've seen that somewhere in the docs...\n. Yes I still get them :-/\nSent from my Phone\nAm 08.03.2016 7:03 nachm. schrieb \"Jonathan Eskew\" <notifications@github.com\n\n:\ncurl.options will only be used by a v2 client. Did you get the same\ncURL/OpenSSL error codes (cURL error 56: SSL read:\nerror:00000000:lib(0):func(0):reason(0), errno 104)?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/aws/aws-sdk-php/issues/924#issuecomment-193894388.\n. @jeskew just scanned multiple times the SWF endpoint in us-east-1:\n\nhttps://www.ssllabs.com/ssltest/analyze.html?d=swf.us-east-1.amazonaws.com\nOn different IPs, the result seems always the same. TLS 1.2 not supported, but TLS 1.0 is.\n. I just tried to force TLS 1.2, and indeed, I get the following:\nGuzzleHttp\\Exception\\ConnectException\n cURL error 35: error:14077102:SSL routines:SSL23_GET_SERVER_HELLO:unsupported protocol (see http://curl.haxx.se/libcurl/c/libcurl-errors.html)\n. I'm still getting the exceptions, but I'll check if I can get more verbose output from curl about this.\nThanks!\nPascal\n. I just added:\nsudo ssldump -a -A -H -i eth0 > /tmp/ssldump.txt &\nI'll get back to you in a few hours!\nThanks!\n. @jeskew I'm getting this in my ssldump output:\na0 22 63 89 f1 ba 0f 6f 29 63 66 2d 3f ac 8c 72 \n          c5 fb c7 e4 d4 0f f2 3b 4f 8c 29 c7 \n      ServerHelloDone\n80 3  0.0060 (0.0009)  C>SV3.1(262)  Handshake\n      ClientKeyExchange\n        EncryptedPreMasterSecret[256]=\n          1d eb bc eb 0e 84 8d a2 db 4b db 39 23 e6 a2 b2 \n          <...........................................>\n          8a 32 21 3a 43 73 4b ef 52 cd 27 24 1f 9a 31 17 \n          3c b7 8eERROR: Length mismatch\n 53 eb 71 68 38 86 26 ce 43 4c 84 ba 66 \n          6f 9e a4 da ff d9 b2 21 0c 25 56 0e 61 21 cd 70 \n          ad f7 fa cb da 57 dd d3 79 d6 b0 e9 0a 1a 75 58 \n          05 09 95 e7 85 d4 f4 ee 6c ba 36 38 1e 51 ca 47 \n80 4  0.0060 (0.0000)  C>SV3.1(1)  ChangeCipherSpec\n80 5  0.0060 (0.0000)  C>SV3.1(48)  Handshake\n80 6  0.0110 (0.0050)  S>CV3.1(1)  ChangeCipherSpec\n80 7  0.0110 (0.0000)  S>CV3.1(48)  Handshake\n80 8  0.0118 (0.0007)  C>SV3.1(32)  application_data\n80 9  0.0119 (0.0000)  C>SV3.1(1520)  application_data\n80 10 0.0140 (0.0021)  C>SV3.1(32)  application_data\n80 11 0.0140 (0.0000)  C>SV3.1(96)  application_data\nGoogling this error often showed me a problem with tcpdump where you have to specify the packet size with -s. But for ssldump, there is no such option. So I'm wondering if this could mean something.\n. @jeskew Thank you very much!\n. @oberman Thanks a lot for sharing! I'm happy for every bit of information you can send me :)\n. got another random error:\nPart 14: Error executing \"UploadPart\" on \"https://s3-us-west-2.amazonaws.com/..........\"; AWS HTTP error: cURL error 52: Empty reply from server (see http://curl.haxx.se/libcurl/c/libcurl-errors.html)\n. @hanoii Do you have a more specific error message that you can share with us?\n. @srinivasudadi9000 it seems to me that there is a package missing on your system? Did you install the php-curl package?. @srinivasudadi9000 Well it depends on your machine... Sometimes it's php-curl, or php5-curl, etc. The command to install it depends in your operating system.\nBecause your questions are clearly not related to the original topic of this issue, I suggest that you ask it rather on a site like stackoverflow.\nGreetings,\nPascal. @kstich sorry for the late reply! Here are already a few the infos you asked for:\n\nPHP : PHP 7.0.15-0ubuntu0.16.04.4\ncURL : curl 7.47.0 (x86_64-pc-linux-gnu) libcurl/7.47.0 GnuTLS/3.4.10 zlib/1.2.8 libidn/1.32 librtmp/2.3\nProtocols: dict file ftp ftps gopher http https imap imaps ldap ldaps pop3 pop3s rtmp rtsp smb smbs smtp smtps telnet tftp \nFeatures: AsynchDNS IDN IPv6 Largefile GSS-API Kerberos SPNEGO NTLM NTLM_WB SSL libz TLS-SRP UnixSockets\nOpenSSL : 1.0.2g\nGuzzle : 6.2.1\nAWS Services : Mainly on SWF, but having seen it also on Put operations for S3 if I remember well\n\nI will try to provide you with the detailed error output asap.\nThanks!. Thanks guys! That was not an easy one to find :-). Great, thank you! Don't hesitate to post me your code snippet even if it\nworks. Perhaps this will give me more indications.\nSent from my Phone\nAm 27.10.2016 6:59 nachm. schrieb \"cjyclaire\" notifications@github.com:\n\n@ppaulis https://github.com/ppaulis SDK doesn't build customization\nupon that API, taking a look to see if I could reproduce the issue ...\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/aws/aws-sdk-php/issues/1113#issuecomment-256705883,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ABiPHxIYAUqJHimXiDp7Qkwp1nmcM_B-ks5q4Nh2gaJpZM4KiW0_\n.\n. \n",
    "serkanozcelik": "Randomly receive this error when issuing a create snapshots for EC2 with using sdk v3. This may be the same or related issue.\ncreateSnapshot:Error executing \"CreateSnapshot\" on \"https://ec2.eu-west-1.amazonaws.com\"; AWS HTTP error: cURL error 35: error:1408F10B:SSL routines:SSL3_GET_RECORD:wrong version number (see http://curl.haxx.se/libcurl/c/libcurl-errors.html)\nAWS PHP SDK v3 ## 3.18.1 - 2016-04-21\nSource : AWS eu-west region - Ireland\nPHP 5.5.9-1ubuntu4.16 \ncurl 7.35.0 (x86_64-pc-linux-gnu) \nlibcurl/7.35.0 \nOpenSSL/1.0.1f \nI have tried;\n'http' => [\n            'curl' => [\n                CURLOPT_SSLVERSION => CURL_SSLVERSION_TLSv1_0,\n            ]\n        ]\nbut no chance, not affected.\n. ",
    "ParadoxNL": "I have the exact problem, except for the SQS service, where it fails on the \"Receive Message\" part.\n[2016-05-02 13:31:52] local.ERROR: exception 'Aws\\Sqs\\Exception\\SqsException' with message 'Error executing \"ReceiveMessage\" on \"https://sqs.eu-central-1.amazonaws.com/id/queue\"; AWS HTTP error: cURL error 35: error:1408F10B:SSL routines:SSL3_GET_RECORD:wrong version number (see http://curl.haxx.se/libcurl/c/libcurl-errors.html)'\nGuzzleHttp\\Exception\\ConnectException: cURL error 35: error:1408F10B:SSL routines:SSL3_GET_RECORD:wrong version number (see http://curl.haxx.se/libcurl/c/libcurl-errors.html) in /project/vendor/guzzlehttp/guzzle/src/Handler/CurlFactory.php:186\nSource: \nPHP 7.0.5\ncurl 7.35.0\nOpenSSL 1.0.1f\n. ",
    "senorbacon": "We're seeing this problem as well, and like @ppaulis, we only started seeing when we upgraded from PHP SDK 2 to 3. It is also fairly random for us as well. @jeskew has the SWF team made any headway with this?\n. ",
    "hanoii": "I think I could be facing the same issue, only that I am using the sdk v2 and attempting to do a simple s3 operation. I can't get to pin down the issue. At one point I disabled curl ssl_verifypeer and curl_verifyhost and it worked but I think it was by chance, and similar to the random behaviour explained here, as it stopped working at some point even with that.\nI am behind a proxy and am pretty sure this has something do, although it worked OK before and it seems to be working OK very randomly.\n. @ppaulis sorry for the poorly written comment and with lack of details. I edited the previous comment a bit and as far as a specific error message, unfortunately there's not much I have to share.\nThe error message is Aws\\Common\\Exception\\TransferException: [curl] 28: Operation timed out after 0 milliseconds with 0 out of 0 bytes received [url] https://xxxxxx in Aws\\Common\\Client\\AbstractClient->send() (line 258 de .../aws-sdk-php/Aws/Common/Client/AbstractClient.php).\nThis is a module within a web app, I attempted several things to obtain more output, adding a LogPlugin didn't help at all. What I ended up doing, besides adding debug => true to both the config, request.options and curl.options was also manually editing CurlHandle.php and changing php://temp of the verbose output to php://output and this is what I get:\nBTW, I ended up removing the proxy as it turned out that I have direct access now, and that made it work, but to provide you with the following, I added the proxy back again, and it worked 4-5 times and then throw the following (I think I stripped most of the details):\n```\n Rebuilt URL to: https://x.y.z/?prefix=somedirectory%2F\n Hostname was NOT found in DNS cache\n   Trying x.x.x.x...\n Connected to pro.xy (x.x.x.x) port 8080 (#0)\n* Establish HTTP proxy tunnel to x.y.z:443\n\nCONNECT x.y.z:443 HTTP/1.1\nProxy-Connection: Keep-Alive\nHost: x.y.z\nUser-Agent: aws-sdk-php2/2.8.30 Guzzle/3.9.3 curl/7.35.0 PHP/5.5.9-1ubuntu4.17 ITR\nDate: Fri, 03 Jun 2016 21:18:44 +0000\nAuthorization: AWS **\n\n< HTTP/1.0 200 Connection established\n< \n Proxy replied OK to CONNECT request\n successfully set certificate verify locations:\n   CAfile: ...//sites/all/libraries/aws-sdk-php/Guzzle/Http/Resources/cacert.pem\n  CApath: /etc/ssl/certs\n Operation timed out after 0 milliseconds with 0 out of 0 bytes received\n Closing connection 0\n Rebuilt URL to: https://x.y.z/?prefix=somedirectory%2F\n Hostname was found in DNS cache\n   Trying x.x.x.x...\n Connected to pro.xy (x.x.x.x) port 8080 (#1)\n Establish HTTP proxy tunnel to x.y.z:443\n\nCONNECT x.y.z:443 HTTP/1.1\nProxy-Connection: Keep-Alive\nHost: x.y.z\nUser-Agent: aws-sdk-php2/2.8.30 Guzzle/3.9.3 curl/7.35.0 PHP/5.5.9-1ubuntu4.17 ITR\nDate: Fri, 03 Jun 2016 21:18:50 +0000\nAuthorization: AWS **\n\n< HTTP/1.0 200 Connection established\n< \n Proxy replied OK to CONNECT request\n successfully set certificate verify locations:\n   CAfile: ...//sites/all/libraries/aws-sdk-php/Guzzle/Http/Resources/cacert.pem\n  CApath: /etc/ssl/certs\n Operation timed out after 0 milliseconds with 0 out of 0 bytes received\n Closing connection 1\n Rebuilt URL to: https://x.y.z/?prefix=somedirectory%2F\n Hostname was found in DNS cache\n   Trying x.x.x.x...\n Connected to pro.xy (x.x.x.x) port 8080 (#2)\n Establish HTTP proxy tunnel to x.y.z:443\n\nCONNECT x.y.z:443 HTTP/1.1\nProxy-Connection: Keep-Alive\nHost: x.y.z\nUser-Agent: aws-sdk-php2/2.8.30 Guzzle/3.9.3 curl/7.35.0 PHP/5.5.9-1ubuntu4.17 ITR\nDate: Fri, 03 Jun 2016 21:18:56 +0000\nAuthorization: AWS **\n\n< HTTP/1.0 200 Connection established\n< \n Proxy replied OK to CONNECT request\n successfully set certificate verify locations:\n   CAfile: ...//sites/all/libraries/aws-sdk-php/Guzzle/Http/Resources/cacert.pem\n  CApath: /etc/ssl/certs\n Operation timed out after 0 milliseconds with 0 out of 0 bytes received\n Closing connection 2\n Rebuilt URL to: https://x.y.z/?prefix=somedirectory%2F\n Hostname was found in DNS cache\n   Trying x.x.x.x...\n Connected to pro.xy (x.x.x.x) port 8080 (#3)\n Establish HTTP proxy tunnel to x.y.z:443\n\nCONNECT x.y.z:443 HTTP/1.1\nProxy-Connection: Keep-Alive\nHost: x.y.z\nUser-Agent: aws-sdk-php2/2.8.30 Guzzle/3.9.3 curl/7.35.0 PHP/5.5.9-1ubuntu4.17 ITR\nDate: Fri, 03 Jun 2016 21:19:04 +0000\nAuthorization: AWS **\n\n< HTTP/1.0 200 Connection established\n< \n Proxy replied OK to CONNECT request\n successfully set certificate verify locations:\n   CAfile: ...//sites/all/libraries/aws-sdk-php/Guzzle/Http/Resources/cacert.pem\n  CApath: /etc/ssl/certs\n Operation timed out after 0 milliseconds with 0 out of 0 bytes received\n* Closing connection 3\n```\nThings I tried/comments:\n- Disabling opcache, same behaviour.\n- Setting CURLOPT_SSL_VERIFYPEER and CURLOPT_SSL_VERIFYHOST to 0, same behaviour\n- The timeout being 0 I think comes from curl/curl#619, I tried different amount of timeouts, including incredible long ones.\n. I actually attempted to look into that, but I saw the whole backoff functionality not that much tested, but will try to look at it.\n. @cjyclaire I added the test, although here I am gonna need feedback/help/guidance if you are not happy with it. I am basically testing that the config is properly set and then that it actually reached the truncated strategy of the default builder, however, each \"client\" has its own builder, so I am not testing each of those. Awaiting your comments and or directions into properly adding a test to it.\n. @cjyclaire what about now? I removed the constant as it really seems to be used everywhere on the tests.\n. Just added two more tests for also testing the setting of the retries on those clients that have non-default backoff plugins.\n. Oh, it;s 10000, I thought they were just 1000. That should be fine. You can still configure a minimum below 5MB, couldn't you?\nRight now I have it configured (with my PR) for 5mb and 6 retries and the uploaded ended fine. Logging to a file a spotted quite a few of these requestimeouts, but the transfer completed fine. It should happen automatically every 2 days, so will monitor.\n. I am again facing a lot of RequestTimeout, now even more than before. Do you know how much is that timeout on AWS side?\nI am unsure how to debug further. I shouldn't have that bad of a network for a 5m part to be retried 6+ times until properly sent.\nAny ideas?\n. Yes, all the same. The usage is very simple actually, just a file upload an a scheduled cron task on a php web app and yes, it is still the same multipart upload issue.\nUnfortunately I don't think I am doing anything out of the ordinary for this, I suspect it's some kind of networking issue but I would be great to know what happens on your side. I know I am getting ReequestTimeout, how long is that? Is it that the network is uploading too slow? or is it that the communication is reset. Any more insight would really help.\n. @cjyclaire \nHave a look at this, right now I have it configured at 12 retries.\nFormat is part number, http response, AWS request ID (For if its useful)\nPart number 3,200 OK,E29045090A479E70\nPart number 1,200 OK,C0735B3AC0A24E33\nPart number 2,400 Bad Request,550F063BFE6F43FC\nPart number 2,400 Bad Request,531E8EA05FFE24E4\nPart number 2,400 Bad Request,F45FBA2AF70F52CE\nPart number 2,400 Bad Request,AF9D7F92B6863ED8\nPart number 2,400 Bad Request,2278FF6ED2013467\nPart number 2,200 OK,E0227F6D47E66F6C\nPart number 4,200 OK,24139C41952CACEC\nPart number 5,400 Bad Request,DF9A7E58B20679E6\nPart number 6,400 Bad Request,08CE9A928C986754\nPart number 5,400 Bad Request,325A1F03E248695D\nPart number 6,400 Bad Request,D56E5A5F2DA2E660\nPart number 5,400 Bad Request,9790D3DD80708E25\nPart number 6,400 Bad Request,C36E39E901A31291\nPart number 5,400 Bad Request,39907E5F4AE56449\nPart number 6,400 Bad Request,0255C13E0C075809\nPart number 5,400 Bad Request,F3BFD3B9843DB4F6\nPart number 6,400 Bad Request,76DA70BFAE431700\nPart number 5,400 Bad Request,4BE0615A0A3E2C91\nPart number 6,400 Bad Request,ED0BEC47A82B5707\nPart number 5,400 Bad Request,390074A84402B550\nPart number 6,400 Bad Request,554EC93B2C02C56A\nPart number 5,400 Bad Request,0F5BB1F7E1EC98FD\nPart number 6,400 Bad Request,AB6E00A14B3FAEB4\nPart number 5,200 OK,DEB3FA9716291469\nPart number 6,400 Bad Request,1BF320DA1C6CAA81\nPart number 6,400 Bad Request,8AD1934F7D5E121E\nPart number 6,400 Bad Request,9011A15DDD87D18B\nPart number 6,403 Forbidden,6F8E28862CF71D41\nThe last one is the first time I saw it, the XML response of the Forbidden one is:\nxml\n<Error>\n  <Code>RequestTimeTooSkewed</Code>\n  <Message>The difference between the request time and the current time is too large.</Message>\n  <RequestTime>Mon, 11 Jul 2016 14:02:29 +0000</RequestTime>\n  <ServerTime>2016-07-11T14:19:36Z</ServerTime>\n  <MaxAllowedSkewMilliseconds>900000</MaxAllowedSkewMilliseconds>\n  <RequestId>6F8E28862CF71D41</RequestId>\n  <HostId>Not sure if this is sensitive, just in case I removed it.</HostId>\n</Error>\nIt was the 12th attempt, but I think that's a coincidence. Maybe the request time is fixed as the initiate time rather than the retried time? That could be another fix but still it really strikes me why that many timeouts.\nSometimes I up to 50x parts, this was timeout really soon.\n. @cjyclaire \nIt's not malformed syntax, all of the 400 errors are the same:\nxml\n<Error>\n  <Code>RequestTimeout</Code><Message>Your socket connection to the server was not read from or written to within the timeout period. Idle connections will be closed.</Message>\n  <RequestId>325A1F03E248695D</RequestId>\n  <HostId></HostId>\n</Error>\nThe guzzle version is the one shipped with latest version of AWS SDK 2.8 and as far as Guzzle/Common/Version.php goes, it's 3.9.3.\nIt's certainly not a credentials issue. Updating guzzle, sure, I guess I could try. Would it work with any latest version of guzzle? No internal API changes? Any version in particular?\nI searched project-wise for on-stats and found nothing, so probably not supported in the version shipped.\nThe retries happen with this particular 900mb file. It's not always the same file, it's a backup of a directory that happens every two days, and once the backup is finished, it attempts to upload it to AWS. So it's a temporary tar file made of that directory which is always growing a bit overtime.\nOn the same site/server/code I have also another file being backed up, which is a database dump, short of 6mb. There are some of those missing (which means the upload failed, but some still there, recent ones, so this one sometimes goes through).\nWith each of this files a very small info file is also generated and uploaded, the info file is always uploaded to s3. It's around 500 bytes.\n. @cjyclaire but isn't the timeout on AWS side? I mean, how would AWS give me a 400 RequestTimeout error at a different moment that the one I set with something like default_socket_timeout?\n. What about updating guzzle, do you think is a good idea? what version should I update to?\n. Actually I use the  constant by looking at other tests, and in a way it made sense as you are also testing that the constant matches the actual string parameter, don't you think?\n. ",
    "srinivasudadi9000": "Hi  ppaulis  \nI would like to send push notifications for android \nso for that i wrote code using curlinit() method initialization \nbut aws server showing like as below connection closed\n\n. in webservice it is showing error like as below \nFatal error: Call to undefined function   curl_init()  in /home/ubuntu/projects/furreka/trunk/server/furreka_rest_api/api.php on line 267. Hi ppaulis please guide me , what is that package\n$fields = array\n(\n    'registration_ids'  => $registrationIds,\n    'data'          => $msg\n);\n$headers = array\n(\n    'Authorization: key=' . API_ACCESS_KEY,\n    'Content-Type: application/json'\n);\n$ch = curl_init();\ncurl_setopt( $ch,CURLOPT_URL, 'https://gcm-http.googleapis.com/gcm/send' );\ncurl_setopt( $ch,CURLOPT_POST, true );\ncurl_setopt( $ch,CURLOPT_HTTPHEADER, $headers );\ncurl_setopt( $ch,CURLOPT_RETURNTRANSFER, true );\ncurl_setopt( $ch,CURLOPT_SSL_VERIFYPEER, false );\ncurl_setopt( $ch,CURLOPT_POSTFIELDS, json_encode( $fields ) );\n$result = curl_exec($ch );\n// print_r($headers );\ncurl_close( $ch );\n        $userdetails = array\n        ( \"Userdetails\"=>failure,\n           \"status\"=>$result,\n           \"users\"=>$users\n\n        );\n\n       print_r(json_encode($userdetails));\n\n. I wrote code for sending push notification for android and my local server 192 ..working fine but coming to aws - ec2 server showing fatal errror ..so how to resolve it ..If possible can you send the package related to curl . how to install php-curl package in aws ppaulis can you send me any reference . ",
    "bkdotcom": "\nInstead of setting $value to null, you would instead want to return ['NULL' => true].\n\nd'oh.. you are correct...  I updated my snippet\n. thanks for investigating the possibility\n. ",
    "ericnorris": "While I understand, I don't think I agree. It seems entirely reasonable to assume that you should be able to close a file handle within the same scope that you opened it. At a minimum, nowhere in the SDK documentation does it mention this particular side effect is possible.\nIf the SDK simply called detach() on any \"wrapped\" resource streams, this could be prevented without the caller being responsible for understanding the internals of the black box. Alternatively, the SDK could return the wrapped stream to leave it up to the caller. The latter seems less intrusive, but I can't comment as to how easy that would be to implement.\nThis created a very subtle bug in the code I had written, and I'd like to spare other developers from the pain of creating dummy stream filters to debug where fclose() was being called.\n. Thanks for your amazingly quick responses @jeskew and @mtdowling, I am super impressed! I had already implemented the GuzzleHttp\\Psr7\\Stream solution in my code, but I filed the issue anyway because of my aforementioned desire to spare other developers.\nIf it's something that can't be changed, then it can't be changed - I'll go ahead and close this issue, but I'd definitely recommend updating the documentation!\n. ",
    "arosenhagen": "I refer here to the composer.json of the current master branch of this repo.\n. yes - that's the problem ;-) guzzlehttp has been abandoned and is now guzzle/guzzle. Therefore I almost always get this warning when installing/updating via composer. \n\"guzzlehttp/guzzle\": \"~5.3|~6.0.1|~6.1\",\n\"guzzlehttp/psr7\": \"~1.0\",\n\"guzzlehttp/promises\": \"~1.0\",\n. *shrgs...shame on me. Had a dependency to another package - so you're right: guzzlehttp/guzzleis the current one. sorry!\n. ",
    "dpgover": "Not really, sorry.\nI have a queue running that reads a file from S3. I supposed that sometimes that reading fails and \"someone\" tries to write the fail to a log... That's the only thing I know and I cant reproduce it manually. :disappointed: \n. That's correct. I'm using the stream wrapper to write log messages to a file in S3.\nI know there's a lot of information missing about the problem. But I cannot reproduce it outside the production servers... and the error log has only the information I wrote on the issue :disappointed: \n. ",
    "Poojal123": "my .env file\nMAIL_DRIVER=ses\nMAIL_HOST=email-smtp.us-east-1.amazonaws.com\nMAIL_PORT=587\nMAIL_ENCRYPTION=''\nMAIL_USERNAME='my uer name'\nMAIL_PASSWORD='password'\nAWS_ACCESS_KEY_ID='key'\nAWS_SECRET_ACCESS_KEY='secret key'\nAWS_REGION=us-east-1\nand my config/mail.php file\nphp\n\n<preturn [\n```\n/\n|--------------------------------------------------------------------------\n| Mail Driver\n|--------------------------------------------------------------------------\n|\n| Laravel supports both SMTP and PHP's \"mail\" function as drivers for the\n| sending of e-mail. You may specify which one you're using throughout\n| your application here. By default, Laravel is setup for SMTP mail.\n|\n| Supported: \"smtp\", \"mail\", \"sendmail\", \"mailgun\", \"mandrill\", \"log\"\n|\n/\n'driver' => env('MAIL_DRIVER', 'ses'),\n/\n|--------------------------------------------------------------------------\n| SMTP Host Address\n|--------------------------------------------------------------------------\n|\n| Here you may provide the host address of the SMTP server used by your\n| applications. A default option is provided that is compatible with\n| the Mailgun mail service which will provide reliable deliveries.\n|\n/\n'host' => env('MAIL_HOST', ''),\n/\n|--------------------------------------------------------------------------\n| SMTP Host Port\n|--------------------------------------------------------------------------\n|\n| This is the SMTP port used by your application to deliver e-mails to\n| users of the application. Like the host we have set this value to\n| stay compatible with the Mailgun e-mail application by default.\n|\n/\n'port' => env('MAIL_PORT', 587),\n/\n|--------------------------------------------------------------------------\n| Global \"From\" Address\n|--------------------------------------------------------------------------\n|\n| You may wish for all e-mails sent by your application to be sent from\n| the same address. Here, you may specify a name and address that is\n| used globally for all e-mails that are sent by your application.\n|\n/\n'from' => ['address' => null, 'name' => null],\n/\n|--------------------------------------------------------------------------\n| E-Mail Encryption Protocol\n|--------------------------------------------------------------------------\n|\n| Here you may specify the encryption protocol that should be used when\n| the application send e-mail messages. A sensible default using the\n| transport layer security protocol should provide great security.\n|\n/\n'encryption' => 'tls',\n/\n|--------------------------------------------------------------------------\n| SMTP Server Username\n|--------------------------------------------------------------------------\n|\n| If your SMTP server requires a username for authentication, you should\n| set it here. This will get used to authenticate with your server on\n| connection. You may also set the \"password\" value below this one.\n|\n/\n'username' => env('MAIL_USERNAME', ''),\n/\n|--------------------------------------------------------------------------\n| SMTP Server Password\n|--------------------------------------------------------------------------\n|\n| Here you may set the password required by your SMTP server to send out\n| messages from your application. This will be given to the server on\n| connection so that the application will be able to send messages.\n|\n/\n'password' => env('MAIL_PASSWORD','' ),\n/\n|--------------------------------------------------------------------------\n| Sendmail System Path\n|--------------------------------------------------------------------------\n|\n| When using the \"sendmail\" driver to send e-mails, we will need to know\n| the path to where Sendmail lives on this server. A default path has\n| been provided here, which will work well on most of your systems.\n|\n/\n'sendmail' => '/usr/sbin/sendmail -bs',\n/\n|--------------------------------------------------------------------------\n| Mail \"Pretend\"\n|--------------------------------------------------------------------------\n|\n| When this option is enabled, e-mail will not actually be sent over the\n| web and will instead be written to your application's logs files so\n| you may inspect the message. This is great for local development.\n|\n/\n'pretend' => false,\n```\n];\nand my config/service.php file\n'ses' => [\n        'key' =>  getenv('AWS_ACCESS_KEY_ID'),\n        'secret' => getenv('AWS_SECRET_ACCESS_KEY'),\n        'region' => 'us-east-1',\n              'version' => 'latest',\n],\nphp 5.6\nframwork laravel 5.1\naws-sdk-php :3.17\nafter all this changes getting same error  plz help me to solve this \nError executing \"SendRawEmail\" on \"https://email.us-east-1.amazonaws.com\"; AWS HTTP error: Client error: 400 InvalidParameterValue (client): Illegal address - \n\nSender\nInvalidParameterValue\n. i verified my from email address in ses (IAM) console but then also getting same error as follow\nError executing \"SendRawEmail\" on \"https://email.us-east-1.amazonaws.com\"; AWS HTTP error: Client error: 400 InvalidParameterValue (client): Illegal address -\nSender\nInvalidParameterValue\nplz reply fast....\n. when i use VerifyEmailIdentity method to verify mail address that time getting this error\nError executing \"VerifyEmailIdentity\" on \"https://email.us-west-2.amazonaws.com\"; AWS HTTP error: Client error: 403 AccessDenied (client): User: arn:aws:iam::<ID>:user/user name is not authorized to perform: ses:VerifyEmailIdentity - <ErrorResponse xmlns=\"http://ses.amazonaws.com/doc/2010-12-01/\">\n<Error>\n<Type>Sender</Type>\n<Code>AccessDenied</Code>\n<Message>User: arn:aws:iam::<ID>:user/user name is not authorized to perform: ses:VerifyEmailIdentity</Message>\n</Error>\n<RequestId>c5f33708-f7da-11e5-9de4-7d09fc8f79d0</RequestId>\n</ErrorResponse>\n. ",
    "bensebborn": "I'm not aware of anything. I just did an upgrade to check if anything changed, from 3.17.2 => 3.17.3\nSame issue though. I just caught the exception and the message is:\n```\nThe promise was rejected\nCatchable fatal error: Argument 1 passed to GuzzleHttp\\Ring\\Core::proxy() must implement interface GuzzleHttp\\Ring\\Future\\FutureArrayInterface, null given, called in /home/vhosts/x.com/vendor/elasticsearch/elasticsearch/src/Elasticsearch/Connections/Connection.php on line 283 and defined in /home/vhosts/x.com/vendor/guzzlehttp/ringphp/src/Core.php on line 335\n```\n. ",
    "enVolt": "PS - There is no activity in Apache error/access log\n. Sorry for the confusion, whenever I start the script (via Artisan command), it works fine. Sometime for a day, sometime for week. This is happening so randomly (unexpected stop). For instance, yesterday it was happening almost every hour (publish rate is 3-4 messages per minute, if receiver is stopped, I got to know almost instantly via cloudwatch alerts). It has happened 2-3 times after that within a period of one hour. But at this moment it's working fine since then.\nThough I won't be able to share exact log stream, but here is how it looks like\n[2016-09-08 15:04:37] production.INFO: [ProcessMessage] Trying to get Messages\n[2016-09-08 15:04:43] production.INFO: [ProcessMessage] Messages Received, [{\"MessageId\":\"17a58698-5459-4451-b512-d7cb7eff613b\",\"ReceiptHandle\":\"AQEBgD3EjSGMwwH92ZmY8LfrCH2rIPpJ6nqmkdrQaXjM7SQWk9K-----------QvjTzZnDoTCbx+6G6GK___7d4aDWESI3bz\\/zrfmnkWY7haHG5Q==\",\"MD5OfBody\":\"47b1e415d424e3a5e795e96844212b47\",\"Body\":\"{\\\"__v\\\":0,\\\"updatedAt\\\":\\\"2016-09-08T09:34:37.308Z\\\",\\\"createdAt\\\":\\\"2016-09-08T09:34:37.308Z\\\",\\\".....some random json......\"}]\n[2016-09-08 15:04:43] production.INFO: [ProcessMessage] Processing message, 17a58698-5459-4451-b512-d7cb7eff613b \n... Some more logs related to processing ...\n[2016-09-08 15:04:44] production.INFO: [ProcessMessage] Trying to get Messages\n[2016-09-08 15:05:04] production.INFO: [ProcessMessage] No Messages Received\n[2016-09-08 15:05:04] production.INFO: [ProcessMessage] Trying to get Messages\n[2016-09-08 15:05:24] production.INFO: [ProcessMessage] No Messages Received\n[2016-09-08 15:05:24] production.INFO: [ProcessMessage] Trying to get Messages\n..\n..\n[2016-09-08 15:15:05] production.INFO: [ProcessMessage] No Messages Received\n... Blank Now ...\n. Script is ending with no clue as it seems. Outside side while loop, there is again a Log statement, and script is never reaching to that point. I'll add further http debug params, and get back to you on this.\n. Update (no need to reopen) -\nSo I have two similar consumer running. At a point when one is working fine, and one is not, basis on what I find, I have a feeling that this is something PHP (or Laravel) related.\nSince Supervisor status shows that process is running state, but it's not doing anything at all, so most probable thing is that it's blocked somewhere.\nAfter getting the PID of Supervisor's process, cat /proc/pid/stack output is as following -\nOne which is working fine (id 11111)\n[<ffffffff811e5319>] poll_schedule_timeout+0x49/0x70\n[<ffffffff811e5cac>] do_select+0x58c/0x750\n[<ffffffff811e603c>] core_sys_select+0x1cc/0x2d0\n[<ffffffff811e61eb>] SyS_select+0xab/0xf0\n[<ffffffff814dc76e>] entry_SYSCALL_64_fastpath+0x12/0x71\n[<ffffffffffffffff>] 0xffffffffffffffff\nOne which is stucked  (id - 12345)\n[<ffffffff811da111>] pipe_wait+0x61/0x90\n[<ffffffff811da224>] pipe_write+0x84/0x430\n[<ffffffff811d205a>] __vfs_write+0xaa/0xe0\n[<ffffffff811d2682>] vfs_write+0xa2/0x1a0\n[<ffffffff811d3386>] SyS_write+0x46/0xa0\n[<ffffffff814dc76e>] entry_SYSCALL_64_fastpath+0x12/0x71\n[<ffffffffffffffff>] 0xffffffffffffffff\nAfter going through strace (strace -s 99 -ffp 12345) (one which is not doing anything)\nProcess 12345 attached\nwrite(3, \"Messages Received, [{\\\"MessageId\\\":\\\"5c08dcad------------------------------------c156\\\",\\\"ReceiptHandle\\\":\\\"AQEBnnq1cW\"..., 2999\nFurthermore this message doesn't exist in log files.\nand strace for the process which is working fine is happy and active.\nProcess 11111 attached\nselect(7, [6], [], [], {0, 367160})     = 0 (Timeout)\nclock_gettime(CLOCK_MONOTONIC, {12832229, 640712218}) = 0\npoll([{fd=6, events=POLLIN|POLLPRI|POLLRDNORM|POLLRDBAND}], 1, 0) = 0 (Timeout)\nclock_gettime(CLOCK_MONOTONIC, {12832229, 640783748}) = 0\nclock_gettime(CLOCK_MONOTONIC, {12832229, 640817079}) = 0\nclock_gettime(CLOCK_MONOTONIC, {12832229, 640843675}) = 0\nselect(7, [6], [], [], {1, 0})          = 0 (Timeout)\nclock_gettime(CLOCK_MONOTONIC, {12832230, 642328374}) = 0\npoll([{fd=6, events=POLLIN|POLLPRI|POLLRDNORM|POLLRDBAND}], 1, 0) = 0 (Timeout)\nclock_gettime(CLOCK_MONOTONIC, {12832230, 642473522}) = 0\nclock_gettime(CLOCK_MONOTONIC, {12832230, 642547629}) = 0\nclock_gettime(CLOCK_MONOTONIC, {12832230, 642605662}) = 0\nselect(7, [6], [], [], {1, 0})          = 0 (Timeout)\nclock_gettime(CLOCK_MONOTONIC, {12832231, 643619322}) = 0\npoll([{fd=6, events=POLLIN|POLLPRI|POLLRDNORM|POLLRDBAND}], 1, 0) = 0 (Timeout)\nclock_gettime(CLOCK_MONOTONIC, {12832231, 643746043}) = 0\nclock_gettime(CLOCK_MONOTONIC, {12832231, 643788635}) = 0\nclock_gettime(CLOCK_MONOTONIC, {12832231, 643850121}) = 0\nselect(7, [6], [], [], {1, 0}^CProcess 30278 detached\nThe log file is not getting rotated, and is around 1.5 GB as of now.\nIf you've any other input, how can I avoid this, please suggest\nAppreciate your quick reverts.\nThanks\n. @cjyclaire as it turns out, this is not Laravel or AWS SDK related, this might be related to supervisor itself. These two consumers I've described here are manual implementation by me (getting message, processing and deleting afterwards). \nLaravel also provides Job/Queue feature for async processing, we're also using SQS for saving Job payload (so SQS communication is underneath the framework). It's also happening there since last week.\nI've raised this issue on supervisor issue page, and following up with them.\nI'll update here as soon as I've any update.\nThanks!\n. ",
    "harshavardhana": "\nI'm confused as to what the issue is. S3 requests should always include a checksum for data integrity; for presigned URLs, you can use the string 'UNSIGNED PAYLOAD' instead to allow any or no payload. This is in line with the docs you quote and matches the behavior of S3.\n\n@tehscorpion can you post the X-Amz-Content-Sha256 here which is generated after createPresignedRequest? need to see if it's \"UNSIGNED-PAYLOAD\" or an actual sha256? \n. > @harshavardhana X-Amz-Content-Sha256 is an actual sha256 string\n\n@jeskew if i understand you correctly the sdk produces X-Amz-Content-Sha256 = sha256sum('UNSIGNED-PAYLOAD') since im requesting an object?\n\nHere is the problem \nhttp://localhost:9001/testbucket/bigfile?X-Amz-Content-Sha256=e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=WLGDGYAQYIGI833EV05A%2F20160407%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20160407T015905Z&X-Amz-SignedHeaders=host&X-Amz-Expires=600&X-Amz-Signature=cdfa1c72f0ae65e982662c6d55e035e795f979fbbba9530eaaf5bec697628c75\nThe problem is evident here in X-Amz-Content-Sha256 - this is basically sha256 of \"empty\" string. \n. Sample code. \n``` php\n<?php\n// Include the SDK using the Composer autoloader\ndate_default_timezone_set('America/Los_Angeles');\nrequire 'vendor/autoload.php';\n$client = new Aws\\S3\\S3Client([\n                              'version' => 'latest',\n                              'region'  => 'us-east-1',\n                              'endpoint' => 'http://localhost:9001'\n                              ]);\n// Create S3 client\n$command = $client->getCommand('GetObject', [\n                                   'Bucket' => 'testbucket',\n                                   'Key' => 'bigfile'\n                                   ]);\n$request = $client->createPresignedRequest($command, '+10 minutes');\n$presignedUrl = (string) $request->getUri();\necho $presignedUrl;\n```\n. Moving to aws-sdk-go and testing . I don't see X-Amz-Content-Sha256 being added when there is no \npayload.\n$ go run presign-get.go\n2016/04/07 00:02:01 The URL is http://localhost:9000/testbucket/bigfile?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=WLGDGYAQYIGI833EV05A%2F20160407%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20160407T070201Z&X-Amz-Expires=900&X-Amz-SignedHeaders=host&X-Amz-Signature=293fd94fff4b8f68781593836654d7f558e6b67f3aa3c637c03203fbbc889761\nSample program\n```\npackage main\nimport (\n    \"log\"\n    \"time\"\n\"github.com/aws/aws-sdk-go/aws\"\n\"github.com/aws/aws-sdk-go/aws/credentials\"\n\"github.com/aws/aws-sdk-go/aws/session\"\n\"github.com/aws/aws-sdk-go/service/s3\"\n\n)\nfunc main() {\n    newSession := session.New()\n    s3Config := &aws.Config{\n        Credentials:      credentials.NewStaticCredentials(\"WLGDGYAQYIGI833EV05A\", \"BYvgJM101sHngl2uzjXS/OBF/aMxAN06JrJ3qJlF\", \"\"),\n        Endpoint:         aws.String(\"http://localhost:9000\"),\n        Region:           aws.String(\"us-east-1\"),\n        DisableSSL:       aws.Bool(true),\n        S3ForcePathStyle: aws.Bool(true),\n    }\n    // Create an S3 service object in the default region.\n    s3Client := s3.New(newSession, s3Config)\nreq, _ := s3Client.GetObjectRequest(&s3.GetObjectInput{\n    Bucket: aws.String(\"testbucket\"),\n    Key:    aws.String(\"bigfile\"),\n})\nurlStr, err := req.Presign(15 * time.Minute)\n    if err != nil {\n    log.Println(\"Failed to sign request\", err)\n}\nlog.Println(\"The URL is\", urlStr)\n\n}\n```\n. ",
    "pinguo-shenqiang": "Sorry, I test it again in my PHP5.6 machine. It is still present :( \nMy memory usage is still climbing. So comfuse .\nThere is some information about how to reproduce it. Perhaps it is useful for debug.\nI use Composer to install the AWS SDK (follow as http://docs.aws.amazon.com/aws-sdk-php/v2/guide/installation.html)\nPHP Version => 5.6.19\n```\n<?php\nrequire 'vendor/autoload.php';\nuse Aws\\Common\\Credentials\\Credentials;\n$strBucket = 'album-analysis-cn';\n$strKey = '0000000456ea6e5781937361ea8b/2/0';\n$accessKeyId = '...';\n$secretAccessKey = '...';\n$credentials = new Credentials($accessKeyId, $secretAccessKey);\n$client = Aws\\S3\\S3Client::factory(\n     array(\n        'credentials' => $credentials,\n        'region' => 'cn-north-1',\n        'client.backoff' => false\n     )\n);\nvar_dump(gc_enabled());\n$i = 0;\nwhile ($i++ >= 0) {\n     $arrArgs = array(\n         'Bucket' => $strBucket,\n         'Key'    => $strKey\n     );\n     $arrHeader = $client->getObject($arrArgs);\n     echo $i . \"\\n\";\n     gc_collect_cycles(); \n}\n```\nCould you help me to resolve it? Thx\nBTW,  php upgrading is a huge project to me. I can not use the latest SDK because of php version.\n. thanks to jeskew :+1: . i resolved my question finally. \nThe summary is that the use of curl (libcurl version 7.19.7) with ssl_verifyPeer creates a serious memory leak. :(\nThe document is https://forums.aws.amazon.com/thread.jspa?threadID=72310\nSo I change my code as bellow:\n```\n<?php\n$accessKeyId = '...';\n$secretAccessKey = '...';\n$credentials = new Credentials($accessKeyId, $secretAccessKey);\n$options = array(\n    'credentials' => $credentials,\n    'region' => 'cn-north-1',\n);\n$libCurl = curl_version();\nif (version_compare($libCurl['version'], '7.19.7') <= 0) {\n    $options = array_merge(\n        array('ssl.certificate_authority' => false),\n        $options\n    );\n}\n$client = Aws\\S3\\S3Client::factory($options);\n...\n```\nPerhaps it will help other person. \n. ",
    "zamaliphe": "it return false but also throw an exception \n. yes i hard coded the file name while i was testing \nbut i still get the same thing \ni did try to debug the call however the ver 3 OF AWS-SDK is using ASync call and I'm unable to get the server response XML \n. both doesObjectExist() and doesBuckettExist() throw an unhandled exception  \nType  Aws\\S3\\Exception\\S3Exception\nNow lets test why ?\nmy main guess  start with the last line \n'trace' => string '#0 /home/www/nkforex/vendor/aws/aws-sdk-php/src/WrappedHttpHandler.php(97): Aws\\WrappedHttpHandler->parseError \nthe WrappedHttpHandler is checking only for a certain return \nbut get something else \ni have no experience with new PSR7 type of request \nbut i can try\nyou should check only for a valid request then return true \notherwise you should return false specially on this 2 calls because other plugins 3rd party libs is using them all the time \n. @jeskew  thanks for your quick reply \nit looks like it should work \nbut for some reason the exception are not handled \ni will try again tomorrow  maybe i found something \nbut if you can just help me get the server response i would be much appreciated \n. done the following test and all give the same result \n*create new php project ( empty folder )\ninstall aws ver 3 using composer\n require '../vendor/autoload.php';\n run my code \nsame thing \n. ",
    "nighthtr": "@zamaliphe @jeskew was xdebug included? I think the problem is this. ",
    "tflin": "The doctrine/cache has included ApcuCache class(Doctrine\\Common\\Cache\\ApcuCache) with the same interface of its ApcCache. So it does not need us to provide an instance of ApcuCache. It should be very easy to modify the original code using doctrine ApcCache to use doctrine ApcuCache.\n. ",
    "servocoder": "Yes, I can see the exception:\nError executing \"HeadObject\" on \"https://s3.eu-central-1.amazonaws.com/my-bucket/image.jpeg\"; AWS HTTP error: Client error: HEAD https://s3.eu-central-1.amazonaws.com/my-bucket/image.jpeg resulted in a 403 Forbidden response:\n (client): 403 Forbidden (Request-ID: 2X4CE03AD2B98F56) - \nSo, the file exists and could be listed via readdir(), but there is exception on \"HeadObject\". Perhaps the exception should be parsed to define if file is forbidden or not exists. As you can see the exception is different when there is no file:\nError executing \"HeadObject\" on \"https://s3.eu-central-1.amazonaws.com/my-bucket/no-image.jpeg\"; AWS HTTP error: Client error: HEAD https://s3.eu-central-1.amazonaws.com/my-bucket/no-image.jpeg resulted in a 404 Not Found response:\n NotFound (client): 404 Not Found (Request-ID: 6B113BC5E2B12153) - \nAnother way I see is to check file existence via some other S3 client method. For example listObjects, because it lists file in case it is forbidden.\n. Ok, I have found that there no need to parse exception message. Just check $e->getResponse()->getStatusCode(); - contains 404 if file not exists or 403 if forbidden. Just check the code to define.\n. Yea, I see what you are talking about. There is really mess with codes. Hope there is a solution, because now file_exists() isn't reliable to check file existence.\n. I agree with states you listed, but I have got Forbidden in $e->getResponse()->getReasonPhrase();. In my debug console I can see a list with \"reasonPhrase\" and it is different from this list. I have found the same list of phrases in GuzzleHttp\\Psr7\\Response file. The phrase remains Forbidden even when I use wrong access key. Do you know the way I could get error message from the list above?\n. It seems I have figured out.\nTo get correct error message from the list you have to call getObject insted of HeadObject and check for $e->getAwsErrorCode(). For example, if I use incorrect access key HeadObject returns null error code while getObject returns InvalidAccessKeyId.\nThere are my cases:\nno file - \"NoSuchKey\"\nforbidden via S3 console - \"AccessDenied\"\n. I want explain my case and what I expect to achieve. I have 2 checks for file:\nstep1 - check if file exists (file_exists)\nstep2 - check file permissions (is_readable/is_writable)\nIf file is exists and forbidden via S3 console I expect the validation passes step1, but fails on step2. It seems logical for me and this is consistent with PHP behavior. We have decided to consider file as existing if error code is one of AccessDenied, InvalidObjectState, or InvalidPayer. The second step should check if file exists AND error code is not AccessDenied. Do you agree with such process?\n. The stream wrapper is really returns false for all cases (either file not existed or permission restricted, etc.), but I don't agree with you in the statement that Windows and Linux have the same workflow. I have checked on Windows and Linux and I can confidently say that file_exists returns true on both OS regardless of file permissions.\nFor my case, I could obvious rewrite my code to the specific behavior of file_exists of the stream wrapper, but it is very inconvenient to take in consideration such inconsistencies. As I noticed you try to keep PHP stream wrapper most compatible with Windows/Linux workflow, but the current behavior doesn't fit to this.\n. I see, it's a bad news. In this case you can close this issue. Thank you for the explanations.\n. One more note regarding this problem.\nI have faced that file_exists returns different result based on the way a file was mapped. If you check key for file_exists after it was mapped with readdir or an iterator, it returns true. But if you check the key for file_exists that wasn't previously mapped with listing it returns false. As I described before, that happens because listing returns correct metadata regardless of file permissions.\n. I understand and agree with you. Caching from readdir is a good solution, I like it too. You could also want to document file_exists behavior, which returns various result based on the way it was cached, as I described in my last message.\nIt would be great if the wrapper would provide an opportunity to drop cache of specified file. This would allow to fix file_exists behavior after it was cached via readdir. Such solution also can be used more widely to check the actual state of object after it is changed, deleted, etc.\n. I see, the main idea was to cache ContentType of an object, to read it later without extra HEAD request to S3. Perhaps there is another way to do this.\n. I guess the mime_content_type() does it. But it was deprecated in PHP >= 5.5, if that fact will affects to the wrapper, then some other function should be used, otherwise I suggest to utilize mime_content_type().\n. Another option is to use finfo_open() in this way:\n$finfo = finfo_open(FILEINFO_MIME_TYPE);\necho finfo_file($finfo, \"path/to/filename.ext\");\nfinfo_close($finfo);\nBut it looks too complicated.\n. I see, thank you for putting effort into this\n. ",
    "danielnitz": "@jeskew thank you for getting back this quick and the explanation, very much appreciated.\n. ",
    "sattalk": "@cjyclaire Thanks for your response. Yes, the extra \"--\" is just a typo. I am running out of ideas. Tried few more options. I am attaching a code snippet - function that generates the raw email string in my code. \nsample.txt\n. @cjyclaire Thanks mate! that was the issue. \nIts strange; I thought the entire message had to be base64 encoded. From the same documentation: \n\nContent must be base64-encoded, if MIME requires it.\n\nI was doing it this way with v2 of the API. When I switched to v3 recently, it stopped working. The documentation is a bit confusing. \nAppreciate your help!! Thanks again!\n. ",
    "genomachino17": "Thanks everyone. That workaround was sufficient.\n. Thanks for the reply. I'll look up #917 to see if I can make use of the previous method.\n. Sorry for no follow-up on this. Simply plugging in the original iterator method ended up working, and still works today. I've been away from the case for a while, so I'm not certain that I can confirm that the guzzle enhancement was enough to bring the iterator back for good -- but I can confirm that it has worked for my purposes since then.\n. ",
    "stephanlindauer": "so i finally figured out that i have to supply region information. if i tell the sdk that i want to access eu-central-1 everything works. thanks for your reply.\n. ",
    "saval": "Ok, seems it works. Yesterday I have worked on the development workstation with local server, Windows folder with project's code mounted to the Linux and probably issue in this. Today I checked this script on the test server and file was deleted as expected. So, sorry for false alarm, please close the issue. Thanks for your time.\n. ",
    "t2adung": "\nI\nsignup the user on cognito and receive yout token\n\nI have same issue. Please guide me how to get user token after signup the use on cognito. Thanks.\n. ",
    "engharb": "Hello,\nplease have a look here may you will find answers about your questions\nhttps://github.com/pmill/aws-cognito\nregards,. @kstich Thank you very much. I thought that I can use aws-sdk-php directly to authenticate the logged in username/password, by passing that values from my login-form to the sdk-php (As i did before by logging in to social media using hauth-sdk).\nIs it possible to authenticate a user to AWS Cognito using sdk-php?\nRegards,. @jschwarzwalder \nI did not find a solution how to verify the credentials and I do not want to do that using S3Client class.. @jschwarzwalder thanks a lot, I really have tried it does not work as I want.. Hello @jschwarzwalder,\nas I explained before I used to login to Federated Identity pool using my FB account and I need a way to validate the Keys in my Php-backend server (kind of PHP SDK which only verifies the keys I do not want to touch any AWS services). Currently Iam using AWS-SDK-PHP but I did not find a method that has the same functionality as I expect.\nRegards, . Thanks a lot, I will try it soon.. ",
    "Halama": "Same for me. I've just updated to 3.18.9 from 3.18.6 and presigned urls are broken.\n. ",
    "davzie": "No worries! Have a super day!\n. Hey @marekjalovec , @cjyclaire \nI had this issue with PHP 7 if that's any help. I can't ramp my brain up into gear to revisit this just yet though :)\n. ",
    "marekjalovec": "just one question: how is it possible to \"(int) pow(2, $retries - 1) * 100\" be a float? that (int) should convert it safely to int and min() returns the value without changing it's type\n. i've tested it here http://sandbox.onlinephpfunctions.com/ and it worked just fine. maybe some weird build of php?\n. AWS SDK works fine with Nette FW without custom extension for DI container (I'm using it that way every day on one project), it's just one line to create a new AWS Client and this even forces you to checkout your key to the repository, which is not something you should do at all.\n. @paveljanda I am not saying I create more clients, just the one singleton. but this extension encourages users to checkout aws credentials into config.neon file in the project repository. you should never have such stuff in your repo. ever.\n. You should maybe emphasize that in the docs, so people won't screw up :-)\n@juniwalk i am not saying it \"forces\" you, just \"encourages\". read my comment please. the docs for this extension note this: \"Configure extension in your config.neon file:\". That's my whole point.\nedit: you are right, i wrote \"forces\" in the first one. sorry for that.\n. @paveljanda ^3.2.6 means >=3.2.6 <4.0.0, so it's pretty fine with latest release 3.18.14\n. ",
    "GeekLad": "Understood.  The problem is that Guzzle performs the detection of cURL and\nif you're trying to use cURL with Google App Engine it does not work.  If I\ndisable cURL in my php.ini so that Guzzle works, it breaks other libraries\nI'm using that require cURL.  I need to be able to enable cURL on my host,\nbut prevent Guzzle from using cURL.\nOn Mon, May 23, 2016 at 2:41 PM cjyclaire notifications@github.com wrote:\n\n@GeekLad https://github.com/GeekLad Thanks for the information, from\nGuzzle https://github.com/guzzle/guzzle, it no longer requires cURL in\norder to send HTTP requests. Guzzle will use the PHP stream wrapper to send\nHTTP requests if cURL is not installed. Alternatively, you can provide your\nown HTTP handler used to send requests. Guzzle Docs\nhttp://docs.guzzlephp.org/en/latest/faq.html\nAlso, PHP SDK\nhttp://docs.aws.amazon.com/aws-sdk-php/v3/guide/guide/configuration.html#http-handler\nv3 now supports customize HTTP handler as well.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly or view it on GitHub\nhttps://github.com/aws/aws-sdk-php/issues/1004#issuecomment-221052448\n. @bshaffer I didn't realize you could enable the full cURL extension, I thought you had to use curl_lite, which is indeed what I'm doing.  It may be I was looking at old documentation before they allowed access to the full cURL extension.  Enabling the full curl.so extension should do the trick.  Thanks!\n. \n",
    "bshaffer": "@GeekLad there is a way to specify the handler explicitly:\nphp\n$handler = new GuzzleHttp\\Handler\\StreamHandler;\n$client = new GuzzleHttp\\Client(['handler' => $handler]);\nAlso, cURL is supported by appengine, so if you're experiencing issues you should file a bug here. You're probably using curl_lite, and you just need to enable the full curl.so extension in your php.ini. See this documentation for more information.\n. ",
    "paveljanda": "@marekjalovec I don't think this is the best approach.\n1, It is not necessary to create more instances of S3Client. (or other asw sdk clients)\n2, Nette DIC takes care of singletons perfectly\n3, Configuration parameters should stay in config (*.neon) files.\n4, Nette Extensions are perfect way how to distribute these configurations / classes registration along other projects / teams.\nYou should use it, it saves time. :)\n. @marekjalovec And what do you mean by \"checkout your key to the repository, which is not something you should do at all\"?\nThis is just an extension, config parameters stay in project. Or did you mean something else?\n. Where do store the secret key?\nThe thing is, you should leave all configuration parameters (credentials etc) at one place. Nette Framework gives you the ability to use perfect neon format. So put it in there.\nAnother thing is being earnest and storing these data in for example config.local.neon only on your server..\n. It's pretty bad luck for Nette framework then. Also I don't see any advantage of aws taking care of these extensions (the symfony bundle requires aws/aws-sdk-php in version ^3.2.6, so it's not that aws provides support for latest versions of aws-sdk-php).\nAnyway, thanks for the reply.\n. ",
    "JakubKontra": "@marekjalovec I'm probably blind. I can't see problem there. You don't need put your credentials into config.neon. You can use for example config.local.neon / config.production.neon to specify it.\n. ",
    "juniwalk": "@marekjalovec If you think that it forces you to commit your secret keys to git then you are using it wrong in the first place.\nYou are supposed to create separate *.neon file and fill params there and then include it in config.neon or in your bootstrap.php. You of course have to add it into .gitignore file. That is how you use it.\n. ",
    "fprochazka": "Improving the documentation with the usage pattern is definitely a good idea.\nThe package may be simple, but isn't that an advantage really?\nImho even a single class deserves a package, if it makese sense. I see no problem with this.\n\nAlso guys, wouldn't it be better to create issues in https://github.com/ublaboo/aws-sdk-nette-extension instead of spamming >220 watchers?\n. ",
    "mariusgrigaitis": "What I'm trying to achieve is mock SQS by providing different endpoint.\nStrange thing is that different SDKs behave differently. I looked into boto3, boto2 (python aws sdk) and wasn't able to replicate such case.\n. That's the different behavior I'm talking about:\nThe PHP Script\n``` php\n\u279c  /tmp cat php/test.php\n<?php\nrequire \"vendor/autoload.php\";\nuse Aws\\Sqs\\SqsClient;\n$sqs = new SqsClient([\"version\" => \"latest\", \"region\" => \"us-east-1\", \"endpoint\" => \"http://192.168.99.100:81\"]);\n$queueUrl = $sqs->getQueueUrl([\"QueueName\" => \"test\"]);\nprint $queueUrl->get(\"QueueUrl\");\nprint $sqs->sendMessage([\"QueueUrl\" => $queueUrl->get(\"QueueUrl\"), 'MessageBody' => 'test']);\n```\nThy Python script\n``` python\n\u279c  /tmp cat python/test.py\nimport boto3\nsqs = boto3.client('sqs', endpoint_url=\"http://192.168.99.100:81\")\nqueue = sqs.get_queue_url(QueueName=\"test\")\nprint queue.get(\"QueueUrl\")\nprint sqs.send_message(QueueUrl=queue.get(\"QueueUrl\"), MessageBody='test')\n```\nCreate mocked queue\njson\n\u279c  /tmp aws --endpoint=http://192.168.99.100:81 sqs create-queue --queue-name test\n{\n    \"QueueUrl\": \"http://sqs.us-east-1.amazonaws.com/123456789012/test\"\n}\nPython testcase (works)\n\u279c  /tmp python python/test.py\nhttp://sqs.us-east-1.amazonaws.com/123456789012/test\n{u'MD5OfMessageBody': '\\n            098f6bcd4621d373cade4e832627b4f6\\n        ', 'ResponseMetadata': {'HTTPStatusCode': 200, 'RequestId': '\\n            27daac76-34dd-47df-bd01-1f6e873584a0\\n        '}, u'MessageId': '\\n            ca1c8579-bd21-43c2-a9a7-cbf4ec51f959\\n        '}\nPHP testcase (gets 403 because sends request to real aws endpoint)\n``\n\u279c  /tmp php php/test.php\nhttp://sqs.us-east-1.amazonaws.com/123456789012/testPHP Fatal error:  Uncaught exception 'Aws\\Sqs\\Exception\\SqsException' with message 'Error executing \"SendMessage\" on \"http://sqs.us-east-1.amazonaws.com/123456789012/test\"; AWS HTTP error: Client error:POST http://sqs.us-east-1.amazonaws.com/123456789012/testresulted in a403 Forbidden` response:\n<?xml version=\"1.0\"?>SenderE (truncated...)\n ExpiredToken (client): The security token included in the request is expired - <?xml version=\"1.0\"?>SenderExpiredTokenThe security token included in the request is expired9d8418fe-dc5a-5790-b1bd-572ea7598154'\nexception 'GuzzleHttp\\Exception\\ClientException' with message 'Client error: POST http://sqs.us-east-1.amazonaws.com/123456789012/test resulted in a 403 Forbidden response:\n<?xml version=\"1.0\"?><ErrorResponse xmlns in /private/tmp/php/vendor/aws/aws-sdk-php/src/WrappedHttpHandler.php on line 192\nFatal error: Uncaught exception 'Aws\\Sqs\\Exception\\SqsException' with message ' in /private/tmp/php/vendor/aws/aws-sdk-php/src/WrappedHttpHandler.php on line 192\nGuzzleHttp\\Exception\\ClientException: Client error: POST http://sqs.us-east-1.amazonaws.com/123456789012/test resulted in a 403 Forbidden response:\n<?xml version=\"1.0\"?>SenderE (truncated...)\n in /private/tmp/php/vendor/guzzlehttp/guzzle/src/Exception/RequestException.php on line 107\nCall Stack:\n    0.0001     230528   1. {main}() /private/tmp/php/test.php:0\n    0.0269    3273240   2. Aws\\Sqs\\SqsClient->sendMessage() /private/tmp/php/test.php:9\n    0.0269    3273592   3. Aws\\AwsClient->__call() /private/tmp/php/test.php:9\n    0.0270    3274440   4. Aws\\AwsClient->execute() /private/tmp/php/vendor/aws/aws-sdk-php/src/AwsClientTrait.php:78\n    0.0275    3308616   5. GuzzleHttp\\Promise\\Promise->wait() /private/tmp/php/vendor/aws/aws-sdk-php/src/AwsClientTrait.php:59\n    0.0276    3308664   6. GuzzleHttp\\Promise\\Promise->waitIfPending() /private/tmp/php/vendor/guzzlehttp/promises/src/Promise.php:62\n    0.0276    3308664   7. GuzzleHttp\\Promise\\Promise->invokeWaitList() /private/tmp/php/vendor/guzzlehttp/promises/src/Promise.php:225\n    0.0281    3336416   8. GuzzleHttp\\Promise\\Promise->waitIfPending() /private/tmp/php/vendor/guzzlehttp/promises/src/Promise.php:269\n    0.0281    3336416   9. GuzzleHttp\\Promise\\Promise->invokeWaitList() /private/tmp/php/vendor/guzzlehttp/promises/src/Promise.php:225\n    0.0281    3336464  10. GuzzleHttp\\Promise\\Promise->waitIfPending() /private/tmp/php/vendor/guzzlehttp/promises/src/Promise.php:266\n    0.0281    3336464  11. GuzzleHttp\\Promise\\Promise->invokeWaitFn() /private/tmp/php/vendor/guzzlehttp/promises/src/Promise.php:223\n    0.0281    3336560  12. GuzzleHttp\\Handler\\CurlMultiHandler->execute() /private/tmp/php/vendor/guzzlehttp/promises/src/Promise.php:246\n    1.5296    3340808  13. GuzzleHttp\\Handler\\CurlMultiHandler->tick() /private/tmp/php/vendor/guzzlehttp/guzzle/src/Handler/CurlMultiHandler.php:123\n    1.5296    3340856  14. GuzzleHttp\\Promise\\TaskQueue->run() /private/tmp/php/vendor/guzzlehttp/guzzle/src/Handler/CurlMultiHandler.php:96\n    1.5297    3335944  15. GuzzleHttp\\Promise\\Promise::GuzzleHttp\\Promise{closure}() /private/tmp/php/vendor/guzzlehttp/promises/src/TaskQueue.php:61\n    1.5297    3336248  16. GuzzleHttp\\Promise\\Promise::callHandler() /private/tmp/php/vendor/guzzlehttp/promises/src/Promise.php:156\n    1.5297    3336296  17. GuzzleHttp\\Middleware::GuzzleHttp{closure}() /private/tmp/php/vendor/guzzlehttp/promises/src/Promise.php:203\nAws\\Sqs\\Exception\\SqsException: Error executing \"SendMessage\" on \"http://sqs.us-east-1.amazonaws.com/123456789012/test\"; AWS HTTP error: Client error: POST http://sqs.us-east-1.amazonaws.com/123456789012/test resulted in a 403 Forbidden response:\n<?xml version=\"1.0\"?>SenderE (truncated...)\n ExpiredToken (client): The security token included in the request is expired - <?xml version=\"1.0\"?>SenderExpiredTokenThe security token included in the request is expired9d8418fe-dc5a-5790-b1bd-572ea7598154 in /private/tmp/php/vendor/aws/aws-sdk-php/src/WrappedHttpHandler.php on line 192\nCall Stack:\n    0.0001     230528   1. {main}() /private/tmp/php/test.php:0\n    0.0269    3273240   2. Aws\\Sqs\\SqsClient->sendMessage() /private/tmp/php/test.php:9\n    0.0269    3273592   3. Aws\\AwsClient->__call() /private/tmp/php/test.php:9\n    0.0270    3274440   4. Aws\\AwsClient->execute() /private/tmp/php/vendor/aws/aws-sdk-php/src/AwsClientTrait.php:78\n    0.0275    3308616   5. GuzzleHttp\\Promise\\Promise->wait() /private/tmp/php/vendor/aws/aws-sdk-php/src/AwsClientTrait.php:59\n    1.5323    3498032   6. GuzzleHttp\\Promise\\RejectedPromise->wait() /private/tmp/php/vendor/guzzlehttp/promises/src/Promise.php:65\n```\n. Credentials are not checked by fake endpoint, so any request to mocked request should pass credentials check. Expired token error means code hit the real endpoint.\nYou can create fake endpoint using moto (to test the case):\nbash\npip install moto\nmoto_server sqs\n. As I have mentioned, it's not a problem with credentials. Problem is that it hits real AWS endpoint (and I explicitly have wrong credentials set) and this behavior is inconsistent with boto (boto does not hit)\nLets put it the other way:\n1) Ensure that SDK is not connecting to real endpoint by changing /etc/hosts: 127.0.0.1 sqs.us-east-1.amazonaws.com\n2) php php/test.php\n```\nhttp://sqs.us-east-1.amazonaws.com/123456789012/testPHP Fatal error:  Uncaught exception 'Aws\\Sqs\\Exception\\SqsException' with message 'Error executing \"SendMessage\" on \"http://sqs.us-east-1.amazonaws.com/123456789012/test\"; AWS HTTP error: cURL error 7: Failed to connect to sqs.us-east-1.amazonaws.com port 80: Connection refused (see http://curl.haxx.se/libcurl/c/libcurl-errors.html)'\nexception 'GuzzleHttp\\Exception\\ConnectException' with message 'cURL error 7: Failed to connect to sqs.us-east-1.amazonaws.com port 80: Connection refused (see http://curl.haxx.se/libcurl/c/libcurl-errors.html)' in /private/tmp/php/vendor/guzzlehttp/guzzle/src/Handler/CurlFactory.php:186\nStack trace:\n0 /private/tmp/php/vendor/guzzlehttp/guzzle/src/Handler/CurlFactory.php(150): GuzzleHttp\\Handler\\CurlFactory::createRejection(Object(GuzzleHttp\\Handler\\EasyHandle), Array)\n1 /private/tmp/php/vendor/guzzlehttp/guzzle/src/Handler/CurlFactory.php(103): GuzzleHttp\\Handler\\CurlFactory::finishError(Object(GuzzleHttp\\Handler\\CurlMultiHandler), Object(GuzzleHttp\\Handler\\EasyHandle), Object(GuzzleHttp\\Hand in /private/tmp/php/vendor/aws/aws-sdk-php/src/WrappedHttpHandler.php on line 192\nFatal error: Uncaught exception 'Aws\\Sqs\\Exception\\SqsException' with message 'Error executing \"SendMessage\" on \"http://sqs.us-east-1.amazonaws.com/123456789012/test\"; AWS HTTP error: cURL error 7: Failed to connect to sqs.us-east-1.amazonaws.com port 80: Connection refused (see http://curl.haxx.se/libcurl/c/libcurl-errors.html)' in /private/tmp/php/vendor/aws/aws-sdk-php/src/WrappedHttpHandler.php on line 192\nAws\\Sqs\\Exception\\SqsException: Error executing \"SendMessage\" on \"http://sqs.us-east-1.amazonaws.com/123456789012/test\"; AWS HTTP error: cURL error 7: Failed to connect to sqs.us-east-1.amazonaws.com port 80: Connection refused (see http://curl.haxx.se/libcurl/c/libcurl-errors.html) in /private/tmp/php/vendor/aws/aws-sdk-php/src/WrappedHttpHandler.php on line 192\nCall Stack:\n    0.0001     230528   1. {main}() /private/tmp/php/test.php:0\n    0.0264    3273152   2. Aws\\Sqs\\SqsClient->sendMessage() /private/tmp/php/test.php:9\n    0.0264    3273504   3. Aws\\AwsClient->__call() /private/tmp/php/test.php:9\n    0.0264    3274352   4. Aws\\AwsClient->execute() /private/tmp/php/vendor/aws/aws-sdk-php/src/AwsClientTrait.php:78\n    0.0271    3308536   5. GuzzleHttp\\Promise\\Promise->wait() /private/tmp/php/vendor/aws/aws-sdk-php/src/AwsClientTrait.php:59\n    0.3251    3745848   6. GuzzleHttp\\Promise\\Promise->wait() /private/tmp/php/vendor/guzzlehttp/promises/src/Promise.php:65\n    0.3252    3745848   7. GuzzleHttp\\Promise\\Promise->wait() /private/tmp/php/vendor/guzzlehttp/promises/src/Promise.php:65\n    0.3252    3745848   8. GuzzleHttp\\Promise\\Promise->wait() /private/tmp/php/vendor/guzzlehttp/promises/src/Promise.php:65\n    0.3252    3745888   9. GuzzleHttp\\Promise\\RejectedPromise->wait() /private/tmp/php/vendor/guzzlehttp/promises/src/Promise.php:65\n```\nSame result, different exception.\nPython example works, PHP example doesn't\n. ",
    "vmpj": "Got it working but only if I used sha265:\n$sha256 = hash_file(\"sha256\", $image);\n        $result = $s3->putObject([\n            \"Bucket\" => getenv(\"S3_BUCKET_NAME\"),\n            \"Key\" => $key,\n            \"SourceFile\" => $image,\n            \"ACL\" => \"public-read\",\n            \"ContentSHA256\" => $sha256\n        ]);\nNote in my first example I was using the key image instead of SourceFile which obviously would not work.\n. ",
    "MarkMurphy": "Same problem today using \"ContentMD5\" => $checksum. ",
    "rips-hb": "Thanks for the answer. I already do replace the character when writing the files as a workaround, it just feels a bit hackish to me. If I forget to do it somewhere in the future I will end up with a undeletable directory.\n. ",
    "joecwallace": "@cjyclaire - Thank you for the reply. :-)\nI think my question is still open though: how do I recover from the MultipartUploadException in the catch block above?\nTo provide a more concrete example:\n1. I have a directory /foo/bar\n2. In /foo/bar are 3 files: x.txt, y.txt, & z.txt\n3. I execute $client->uploadDirectory('/foo/bar', 'my-bucket')\n4. A MultipartUploadException is thrown while uploading y.txt\n5. I catch the exception and wish to call (new MultipartUploader($client, '/foo/bar/y.txt', ['state' => $e->getState()]))->upload()\nBut how do I know that y.txt is the file that was being uploaded when the exception was thrown? As far as my code is aware, it could have been x.txt or z.txt. Is the filename provided by the exception?\nI see that you've mentioned $e->getFile(), but my understanding is that getFile() will return the name of the PHP file in which the exception was created/thrown. Is that incorrect?\n. ",
    "adamjimenez": "3.18.19\nOr should the waiter be called: \"InternetGatewayExists\"\nThis waiter also doesn't exist.\nI'm trying to create an internet gateway and then immediately tag it.\nSometimes this fails because the gateway is not yet created.\n. that worked a treat, thank you.\n. ",
    "sattes-faction": "Hi guys, thanks for you answer!\nI'm using the SDK in combination with the pimcore CMS system, where it's used to store the images and generated thumbnails on S3. The touch-command is used for the thumbnail creation. This is the code line:\nhttps://github.com/pimcore/pimcore/blob/6695c0348df6072938199c57255c8b56f7276037/pimcore/models/Asset/Image.php#L70\nIn the meantime the pimcore guys muted the touch command so it doesn't result in an error using the SDK stream wrapper.\n. ",
    "cjunge-work": "The SDK uses Guzzle as the HTTP transport, and I can't seem to find how to alter the middleware that Guzzle uses.\nIt seems that the Guzzle handler is not exposed by the SDK, and the SDK instantiates the handler itself.\n. Guzzle already supports adding middleware that can enable caching (or other features). Eg. https://github.com/Kevinrob/guzzle-cache-middleware\nTo allow adding caching of S3 content it should be a simple case of configuring and adding that middleware to the Guzzle client. But, due to how the SDK instantiates and accesses the client, it is not exposed outside of the SDK, as far as I can see.\nExposing some way of configuring the Guzzle client to add the middleware would be great!\n. @kkopachev @cjyclaire that looks like it might solve the issue. I will test and reply back soon.\n. It is possible to add a custom http_handler with the caching enabled. Below is an example Symfony configuration to handle it:\n```\ncsa_guzzle:\n    profiler: %kernel.debug%\n    logger: %kernel.debug%\n    clients:\n        aws: ~\n    cache:\n        enabled: true\n        adapter: aws.cache.adaptor\nservices:\n    aws.cache.service:\n        class: Doctrine\\Common\\Cache\\FilesystemCache\n        public: false\n        arguments: [\"%kernel.cache_dir%/aws\", 3600]\n    aws.cache.adaptor:\n        class: Test\\DoctrineAdapter\n        public: false\n        arguments: [\"@aws.cache.service\"]\n    http.handler:\n        class: Aws\\Handler\\GuzzleV6\\GuzzleHandler\n        arguments: [\"@csa_guzzle.client.aws\"]\n    aws.sdk:\n        class: Aws\\Sdk\n        public: false\n        arguments:\n            -\n                credentials:\n                   key: \"%amazon_s3.key%\"\n                   secret: \"%amazon_s3.secret%\"\n                region: \"%amazon_s3.region%\"\n                version: '2006-03-01'\n                http_handler: \"@http.handler\"\n    s3.client:\n        class: Aws\\S3\\S3Client\n        factory: [\"@aws.sdk\", createS3]\n        public: true\n```\nIt's kinda messy, but it works!\nNote: I added a custom DoctrineAdaptor that filters out the AWS headers that change between each request (invocation ID, date, authorization).\n. ",
    "robgil": "Thanks @cjyclaire. We're planning to move, but need to go live first in a new region. \n. ",
    "ovr": "@cjyclaire PHPStorm\n\nMaybe it's related to this\nhttps://bitbucket.org/kalessil/phpinspectionsea/src\n. ",
    "n0rbyt3": "In 2.8 it worked because of an extra flag accept403 in doesBucketExist, which is true by default:\nphp\npublic function doesBucketExist($bucket, $accept403 = true, array $options = array())\n{\n    return $this->checkExistenceWithCommand(\n        $this->getCommand('HeadBucket', array_merge($options, array(\n            'Bucket' => $bucket\n        ))), $accept403\n    );\n}\nWhy has this been removed?\n. @cjyclaire That is not what I want to do. I want to read a file from an existing bucket but I don't have the permission to create a bucket.\nIn 2.8:\nread() -> (calls) ensureBucketExists() -> HeadBucket command -> AccessDeniedError (403) is thrown and caught and because $accept403 is true, true is returned, so the bucket exists and the file could get read\nIn 3.0:\nread() -> (calls) ensureBucketExists() -> HeadBucket command -> AccessDeniedError (403) is thrown and caught and false is returned, so the bucket does not exist and CreateBucket command is executed which also throws AccessDeniedError (403) and that exception is not caught, so read() fails\n. I should have mentioned that I use knplabs/gaufrette package which provides the read() method. And that uses the ensureBucketExists()method internally which calls S3Client::doesBucketExist of the aws/aws-sdk-php package. But the behaviour of doesBucketExist has changed internally as described if an error 403 occurs.\nI hope, it will clarify the confusion with the method names.\n. Or more easier: extend the class, override the method and simply return true. This will safe an extra call. If the bucket really doesn't exist, further requests will fail as expected. A new option \"createBucketIfNotExists\" (defaults to true for BC) would be welcome.. ",
    "dmolnarqu": "I ran into this issue as well. When checking if bucket exists, I always get false.\nI created S3Client and could successfully store and retrieve objects, but doesBucketExist() always returns false. \nI noticed there's a check for 'AccessDenied' aws error code, but it still fails, because I am getting NULL as aws error code with 403 response.\nIs there more robust alternative to that check?. @SteffenDE \nI managed to work around the issue by implementing custom bucketExists method.\nPHP\npublic function bucketExists($bucket) {\n    // $client->doesBucketExist() returns false on denied access since 3.X, we don't want that\n    try {\n        $client = My_Aws_Client::get();\n        $command = $client->getCommand('HeadBucket', ['Bucket' => $bucket]);\n        $client->execute($command);\n        return true;\n    } catch (S3Exception $e) {\n        if ($e->getAwsErrorCode() == 'AccessDenied' || $e->getStatusCode() == 403) {\n            return true;\n        }\n        if ($e->getStatusCode() >= 500) {\n            throw $e;\n        }\n        return false;\n    }\n}\n. ",
    "SteffenDE": "How can this be closed? doesBucketExist fails when HeadBucket returns 403, even if the bucket exists and getting/putting objects works fine. This is a critical issue for me...\nThis code does not seem to work as it should, or am I wrong?\nhttps://github.com/aws/aws-sdk-php/blob/ab1cf3e90aa5ad04946e9de3978a339d64528bb1/src/S3/S3ClientTrait.php#L289. @dmolnarqu \nReplacing this single line:\nhttps://github.com/aws/aws-sdk-php/blob/ab1cf3e90aa5ad04946e9de3978a339d64528bb1/src/S3/S3ClientTrait.php#L289\nWith if ($e->getAwsErrorCode() == 'AccessDenied' || $e->getStatusCode() == 403) fixes this for me.\nThe problem is that Nextcloud (I'm trying to use a bucket for external storage) seems to have another location where it tries to do a HeadBucket call, which also fails (https://forums.aws.amazon.com/thread.jspa?threadID=130203).. ",
    "sprosov": "Objects in S3 are binary files with special format.\nToday I studied SDK code whole day and finally found decision. \nAt first, we use GuzzleHttp v5 library. I found that for this version 'decode_content' option doesn't work. 'http' option (so 'decode_content' too) doesn't include to validOptions array in GuzzleV5 handler and it excluded from options array when handler initialize request.\nWe can't quickly move to Guzzle v6, so I create simple handler with required options and set it in 'http_handler' when initialize S3 client. It resolved my problem.\nThere is my code. \n$this->s3Client   = new S3Client(\n            [\n                'credentials'       => $credentials,\n                'region'            => 'us-east-1',\n                'version'           => '2006-03-01',\n                'signature_version' => 'v4',\n                'http_handler'      => function(RequestInterface $r, array $opts = []) {\n                    $client = new Client();\n                    $client->setDefaultOption('decode_content', false);\n                    $handler = new GuzzleHandler($client);\n                    return $handler($r, $opts);\n                }\n            ]\n        );\n. ",
    "g0ddish": "My bad, I was using v2 documentation\nhttp://docs.aws.amazon.com/aws-sdk-php/v2/guide/installation.html\n. ",
    "timothy-r": "We want to make signed requests from a lambda function to a PHP application and would rather use the AWS signature library than our own version.\n. ",
    "wreckingadm": "I believe you only get one limit on a query, so you may want to pick the most efficient for the DB call (such as the one that is going to end up using less bandwidth), then do your second filter directly in PHP on the query results. It's a bit more handling code-side rather than directly in the DB query, but it will give you more control for what your trying to do in the end.\n. ",
    "paithal-uc": "If i do that in PHP, if the query result is less than the limit i need to do the query again and again until the result count reaches the limit or there are no more results.\nIf i do this way, number of queries i hit to db becomes huge. So its better if you return the results with the query filter values while applying the query.\n. ",
    "nikhil-uc": "We have 300+ million records in DynamoDB and doing a recursive calls just to get the limit on filters or getting a large set of data and applying filters programmatically is not so convenient and time consuming when the data needs to be displayed to the user.\nAny hopes or suggestions?. ",
    "andybrand": "You might be able to get what you need by using a secondary index. Using the classic RDB example, customer - order example: you have one table for customers and one for orders. The Orders table has a Key consisting of Customer - HASH, Order - RANGE. So if you wanted to get the latest 10 orders, there would be no way to do it without a scan\nBut if you create a Global Secondary Index on orders of \"Some Constant\" -- HASH, Date RANGE, and queried against that index, they query would do what you want and only charge you for the RCUs involved with the records returned. No expensive scan needed. Note, writes will be more expensive, but in most cases, there are many more reads than writes.\nNow you have your original problem if you want to get the 10 biggest orders for a day larger than $1000. The query would return the last 10 orders, and then filter out those less than $1000.\nIn this case, you could create a computed key of Date-OrderAmount, and queries against that index would return what you want.\nIt's not as simple as SQL, but you need to think about access patterns in SQL too. If if you have a lot of data, you need to create Indexes in SQL or the DB will happily to table scans on your behalf, which will impair performance and raise your costs.\nNote that everything I proposed is normalized in the sense that there is only one source of truth. You are not duplicating data -- you are merely recasting views of it to get what you need from DynamoDB.\nBear in mind that the CONSTANT as a HASH s subject to the 10GB per partition limit, so you would need to design around it if you had a lot of active data. For example, depending on your expected access pattern, you could use Customer and not a constant as a HASH. Or use STreams to organize the data (or subsets) in other ways.\n. ",
    "msencenb": "Actually, I misspoke; the constructor calls parent::__construct which appears to accept handler as a valid option, although I haven't traced through the code to find out how the handler is getting setup up. \nUnfortunately my test case does not work when I pass handler in as a constructor argument. But, if I explicitly call setHandler with my mock it fires correctly. \n$s3->getHandlerList()->setHandler($this->mock);\n. ",
    "LiuJoyceC": "\ud83d\udc11 \ud83c\uddee\ud83c\uddf9 \n. \ud83d\udea2 \n. ",
    "moee": "I experienced a similar problem after the upgrade to 3.18.32, as it also breaks CloudSearch for me in the eu-west-1 region (I'm not sure if it's only a problem of this particular region, though).\nConsider this example:\nphp\n$sdk = new \\Aws\\Sdk();\n$cloudsearch = $sdk->createClient(\n    'CloudSearchDomain',\n    [\n        'endpoint' => 'https://***********.eu-west-1.cloudsearch.amazonaws.com',\n        'region' => 'eu-west-1',\n        'version' => '2013-01-01'\n    ]\n);\nprint_r($cloudsearch->search([ 'query' => 'test' ])['hits']['found']);\nWith 3.18.31 I get the correct result. However, if I update to 3.18.32 I get the following fatal error.\nFatal error: Uncaught exception 'Aws\\CloudSearchDomain\\Exception\\CloudSearchDomainException' with message 'Error executing \"Search\" on \"https://***********.eu-west-1.cloudsearch.amazonaws.com/2013-01-01/search?format=sdk&pretty=true&q=test\"; AWS HTTP error: Client error: `POST https://***********.eu-west-1.cloudsearch.amazonaws.com/2013-01-01/search?format=sdk&pretty=true&q=test` resulted in a `403 Forbidden` response:\n{\"__type\":\"#IncompleteSignature\",\"error\":{\"message\":\"[*Deprecated*: Use the outer message field] When Content-Type:appli (truncated...)\n. ",
    "Dayjo": "Awesome, thank you @cjyclaire \nOnce this release has gone out, I shall try and update once again and test it. Will this be 3.18.33? Or in the 2.8.x? \nNot sure which is for what.\n. This is now working by the way \ud83d\udc4d \n. ",
    "stevenmunro": "Hi @cjyclaire \nWhile it was timing out the only error message that was dumped to my PHP-error.log is the following;\n[26-Jul-2016 00:00:22 UTC] PHP Fatal error:  Maximum execution time of 30 seconds exceeded in /var/www/html/vendor/guzzlehttp/psr7/src/MessageTrait.php on line 147\n[26-Jul-2016 00:00:22 UTC] PHP Stack trace:\n[26-Jul-2016 00:00:22 UTC] PHP   1. {main}() /var/www/html/sms.php:0\n[26-Jul-2016 00:00:22 UTC] PHP   2. send_sms() /var/www/html/sms.php:35\n[26-Jul-2016 00:00:22 UTC] PHP   3. Aws\\AwsClient->publish() /var/www/html/sms.php:28\n[26-Jul-2016 00:00:22 UTC] PHP   4. Aws\\AwsClient->__call() /var/www/html/sms.php:28\n[26-Jul-2016 00:00:22 UTC] PHP   5. Aws\\AwsClient->execute() /var/www/html/vendor/aws/aws-sdk-php/src/AwsClientTrait.php:78\n[26-Jul-2016 00:00:22 UTC] PHP   6. GuzzleHttp\\Promise\\Promise->wait() /var/www/html/vendor/aws/aws-sdk-php/src/AwsClientTrait.php:59\n[26-Jul-2016 00:00:22 UTC] PHP   7. GuzzleHttp\\Promise\\Promise->waitIfPending() /var/www/html/vendor/guzzlehttp/promises/src/Promise.php:62\n[26-Jul-2016 00:00:22 UTC] PHP   8. GuzzleHttp\\Promise\\Promise->invokeWaitList() /var/www/html/vendor/guzzlehttp/promises/src/Promise.php:225\n[26-Jul-2016 00:00:22 UTC] PHP   9. GuzzleHttp\\Promise\\Promise->waitIfPending() /var/www/html/vendor/guzzlehttp/promises/src/Promise.php:269\n[26-Jul-2016 00:00:22 UTC] PHP  10. GuzzleHttp\\Promise\\Promise->invokeWaitList() /var/www/html/vendor/guzzlehttp/promises/src/Promise.php:225\n[26-Jul-2016 00:00:22 UTC] PHP  11. GuzzleHttp\\Promise\\Promise->waitIfPending() /var/www/html/vendor/guzzlehttp/promises/src/Promise.php:266\n[26-Jul-2016 00:00:22 UTC] PHP  12. GuzzleHttp\\Promise\\Promise->invokeWaitFn() /var/www/html/vendor/guzzlehttp/promises/src/Promise.php:223\n[26-Jul-2016 00:00:22 UTC] PHP  13. GuzzleHttp\\Handler\\CurlMultiHandler->execute() /var/www/html/vendor/guzzlehttp/promises/src/Promise.php:246\n[26-Jul-2016 00:00:22 UTC] PHP  14. GuzzleHttp\\Handler\\CurlMultiHandler->tick() /var/www/html/vendor/guzzlehttp/guzzle/src/Handler/CurlMultiHandler.php:123\n[26-Jul-2016 00:00:22 UTC] PHP  15. GuzzleHttp\\Promise\\TaskQueue->run() /var/www/html/vendor/guzzlehttp/guzzle/src/Handler/CurlMultiHandler.php:96\n[26-Jul-2016 00:00:22 UTC] PHP  16. GuzzleHttp\\Promise\\FulfilledPromise::GuzzleHttp\\Promise{closure}() /var/www/html/vendor/guzzlehttp/promises/src/TaskQueue.php:61\n[26-Jul-2016 00:00:22 UTC] PHP  17. Aws\\Middleware::Aws{closure}() /var/www/html/vendor/guzzlehttp/promises/src/FulfilledPromise.php:39\n[26-Jul-2016 00:00:22 UTC] PHP  18. Aws\\Signature\\SignatureV4->signRequest() /var/www/html/vendor/aws/aws-sdk-php/src/Middleware.php:127\n[26-Jul-2016 00:00:22 UTC] PHP  19. Aws\\Signature\\SignatureV4->buildRequest() /var/www/html/vendor/aws/aws-sdk-php/src/Signature/SignatureV4.php:64\n[26-Jul-2016 00:00:22 UTC] PHP  20. GuzzleHttp\\Psr7\\Request->__construct() /var/www/html/vendor/aws/aws-sdk-php/src/Signature/SignatureV4.php:336\n[26-Jul-2016 00:00:22 UTC] PHP  21. GuzzleHttp\\Psr7\\Request->setHeaders() /var/www/html/vendor/guzzlehttp/psr7/src/Request.php:45\n[26-Jul-2016 00:00:22 UTC] PHP  22. is_array() /var/www/html/vendor/guzzlehttp/psr7/src/MessageTrait.php:147\n. Here is the script\nphp\n\n<prequire 'vendor/autoload.php';\nfunction send_sms() {\n$sns = Aws\\Sns\\SnsClient::factory(array(\n    'region' => 'ap-southeast-1',\n    'version' => 'latest',\n    'credentials' => array(\n       'key'    => 'XXremovedXX',\n       'secret' => 'XXremovedXX',\n    ),\n    'scheme' => 'http',\n  ));\n$attributes = [\n    'AWS.SNS.SMS.SMSType' => [\n      'DataType' => 'String',\n      'StringValue' => 'Promotional',\n    ],\n  ];\n$result = $sns->publish(array(\n    'Message' => 'Test Message',\n    'PhoneNumber' => '+XXXXXXXXXXX',\n    'MessageAttributes' => $attributes\n  ));\nreturn true;\n}\nsend_sms();\n. Sorry about the formatting. Please let me know if you need anything else.\n. Hi @cjyclaire \nAmazon technical support responded just hours ago that they were able to reproduce the issue in a newly created environment without the php-simplexml package installed. They have flagged the wrong behavior to the SDK team in order to provide a fix.\n. ",
    "neophyt3": "@cjyclaire I have gone through the docs earlier, in there it mentions that in order to use sse-c we need to pass headers while requesting which can be passed by injecting middleware into the aws client object\nhere is the guide link\nwhen I tried injecting those custome algo and key headers it failed and throws error that it could not verify the signature coz those headers were not included in the signature creation when I injected it,\nso I found other way, by adding \n$s3Client->getHandlerList()->appendInit(\n  Middleware::mapCommand(function (CommandInterface $command) {\n    $command['SSECustomerKey'] = md5('test');\n    $command['SSECustomerAlgorithm'] => 'AES256';\n    $command['SSECustomerKeyMD5'] => md5(md5('test'), true),\n  return $command;\n}),'s3.ssec');\nBut this also failed and throwed an error that it could not parse the response xml...\nI can successfully put my  file with sse-c enctyption by doing this\n$result = $client->putObject([\n    'SSECustomerAlgorithm' => 'AES256',\n    'SSECustomerKey' => md5('test'),\n    'SSECustomerKeyMD5' => md5(md5('test'), true),\n    ...\n]);\nBut the problem is that I have to use client obbject directly, and I am using Laravel aws wrapper\nSo I thought that I could get the client object once and change the sse-c middleware by Middleware::mapcommand() so that I dont have to go everywhere and add those direct method codes..\n. ",
    "zelding": "I am using the latest sable version, according to composer it is 3.18.33.\nThe message I showed is the caught exception message; i just left out the block, because i thought its not important.\nlog_errors_max_len is 1024 bytes, so i guess it should be enough; I've tried to throw and catch an Exception with the message abcdefghijklmnopqrstuvwxyz- copied 20 times, and it wasn't truncated.\nYes the AmazonSDK is the class alias for Aws\\Sdk.\nI'll update the original post with these infos.\n. @cjyclaire Yes it does. The exact same message :(\nI even checked if goto is set in the disable_functions; but it is not.\nI also updated the SDK to 3.18.35, even though it shouldn't have an effect here; and it didn't.\n. Something has changed. I've tried again with the $sns = new Aws\\Sns\\SnsClient method, and now it seems to call the AWS service, because now it gives me bad request 400, but if i try $sns = $sdk->createSns() it still produces the same error.\nThe new errorMessage is this.\n``\nError executing \"Subscribe\" on \"http://sns.eu-central-1.amazonaws.com\";\nAWS HTTP error: Client error:POST http://sns.eu-central-1.amazonaws.comresulted in a400 Bad Request` response:\nSender\nInvalidPara (truncated...)\nInvalidParameter (client): Invalid parameter: Endpoint - \nSender\nInvalidParameter\nInvalid parameter: Endpoint\nSOME_LONG_STRING_OF_CODE\n```\n@cjyclaire Yes that was the only error. I haven't tried to create other clients. Would you recommend one, that requires the minimum effort to set up?\n. Sry for not responding in a while. I left a company where I had the issue, but ive passed on the link to this thread so the ppl who take after me can continue.\nI suggest you close the thread for now, and I'm sure others will copmment, when they encounter the issue.\n. ",
    "RyanDwyer": "I've found the problem. I've got the same issue when setting up a production environment. The issue happens if one of the $args has a null value, and the argument is required and has no default.\nWhile reading the below, keep in mind that isset() will return false if the key is set and has a null value, while array_key_exists() will return true if the key is set and has a null value.\nSee ClientResolver.php, resolve():\nif (!isset($args[$key])) {\n    if (isset($a['default'])) {\n        // Merge defaults in when not present.\n        if (is_callable($a['default'])\n            && (\n                is_array($a['default'])\n                || $a['default'] instanceof \\Closure\n            )\n        ) {\n            $args[$key] = $a['default']($args);\n        } else {\n            $args[$key] = $a['default'];\n        }\n    } elseif (empty($a['required'])) {\n        continue;\n    } else {\n        $this->throwRequired($args);\n    }\n}\n\nThe above is using isset(), so the null argument triggers a call to throwRequired().\nprivate function throwRequired(array $args)\n{\n    $missing = [];\n    foreach ($this->argDefinitions as $k => $a) {\n        if (empty($a['required'])\n            || isset($a['default'])\n            || array_key_exists($k, $args)\n        ) {\n            continue;\n        }\n        $missing[] = $this->getArgMessage($k, $args, true);\n    }\n    $msg = \"Missing required client configuration options: \\n\\n\";\n    $msg .= implode(\"\\n\\n\", $missing);\n    throw new IAE($msg);\n}\n\nHere, it's using array_key_exists(). This check returns true for the null argument, and the continue statement is run. The $missing array is ultimately empty.\nThe array_key_exists() check should be replaced with isset($args[$k]).. ",
    "bestis": "Or should it even be:\nclass_exists('Throwable', false)\n. ",
    "awsdev543": "Thanks. Works with all uppercase auth parameters.\n. ",
    "bmd": "Unfortunately, I'm maintaining a legacy project on a PHP 5.4 server (the documented minimum requirement for SDK3 is 5.5) so I'm stuck with the 2.8 SDK for now.\nIf the 2.8 SDK is formally EOL'd, not a problem. It's more of an annoyance than anything else. Just raising for awareness.\n. ",
    "samuelfaj": "Was the version...\nI've downloaded from here: http://docs.aws.amazon.com/aws-sdk-php/v3/guide/getting-started/installation.html\nIt's working on last version.\n. ",
    "ismaelirc": "@samukt It works yet? I've downloaded the v3 version in the same link that you post and use this code \n`$this->sns = SnsClient::factory(array(\n                    'region' => 'us-west-2',\n                    'version' => 'latest',\n                    'credentials' => [\n                        'key' => 'key',\n                        'secret' => 'secret'\n                    ]\n        ));\n    $result = $this->sns->publish(array(\n            'PhoneNumber' => '+559999999',\n            'Message' => 'string',\n            'MessageStructure' => 'DefaultSMSType',\n            'MessageAttributes' => array(\n                'AWS.SNS.SMS.SMSType' => array('StringValue'=>'Promotional','DataType'=>'String'),\n                '\"AWS.SNS.SMS.SenderID' => array('StringValue' => \"mySenderID\", 'DataType' => 'String')\n            ),\n        ));\n        return $result;`\n\nand I recive this error:\nMessage: Error executing \"Publish\" on \"https://sns.us-west-2.amazonaws.com\"; AWS HTTP error: Client error:POST https://sns.us-west-2.amazonaws.comresulted in a403 Forbiddenresponse: Sender SignatureDo (truncated...) SignatureDoesNotMatch (client): The request signature we calculated does not match the signature you provided. Check your AWS Secret Access Key and signing method. Consult the service documentation for details.\nI already created the IAM user and set de credentials on config array.. Hi, Stas. I did not find the reason. I configured the AIM user again, I\nchanged the secret and key and works.\nBut I really wanna know whats was happen...\nEm ter, 26 de set de 2017 \u00e0s 13:51, Stas Kuryan notifications@github.com\nescreveu:\n\n@ismaelirc https://github.com/ismaelirc Did you find a reason why it\nhappened? I face the same issue.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/aws/aws-sdk-php/issues/1061#issuecomment-332263182,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AJ0noLw-9L4TtC8KxWqHoIZYZgJSVklHks5smSuGgaJpZM4JdJZ9\n.\n. Ok!\n\nEm ter, 26 de set de 2017 \u00e0s 16:03, Stas Kuryan notifications@github.com\nescreveu:\n\nThanks, I found a reason. The problem was in secret key. I added an extra\nsymbol to it by mistake.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/aws/aws-sdk-php/issues/1061#issuecomment-332302108,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AJ0noH_QN8pV36zJTinOEv5Z1Me-VtKEks5smUqMgaJpZM4JdJZ9\n.\n. \n",
    "Stafox": "@ismaelirc  Thanks, I found a reason. The problem was in secret key. I added an extra symbol to it by mistake.. ",
    "sureshcsk": "@cjyclaire \nThanks for the reply.No concerns about the path.\nThere is no error message shown after the putobject is called.\nHow to check whether the client is initialized or not.We also used try and catch for the putobject,but no error is caught.\nBucket exits in the S3 and bucket permission is Any AWS Authenticated User,Everyone.\nThanks\nsuresh\n. @worldlyjohn \nI have permission for list,upload,delete.\nPlease check the attachment \n\n. @cjyclaire\nThanks for reply. \nDone var_dump(s3Client(.....)); \nFor s3Client Worked. \nAlso tried to var_dump($s3->putobject(array(....)));\nNothing displayed.Empty screen.\n. @cjyclaire  All your great support appreciated.\nI made a mistake with composer.\nI installed the composer twice.\nInstalled the composer from https://getcomposer.org/download/\nand again installed the composer from \nInstall Composer\n curl -sS https://getcomposer.org/installer | php\nToday created a new ec2 instance.Installed the composer only once from composer page directly and installed the aws-sdk-php.\nNow able to put the object in S3 with no issues.\nOnce again great thanks for help and time.\nThanks\nsuresh \n. @cjyclaire \nI got fixed because I installed the sdk under the html folder in ec2 aws linux.\nWhen again tried to create new instance and installed the sdk in html/awsbucket/\nThe putobject is not working.Not able to var_dump (putrobject).\nSuccess Use Case:\nInstalled sdk under html/\nrequire PHP :\nrequire 'vendor/autoload.php';\nIssue Use case:\nInstalled sdk under html/awsbucket\nrequire 'awsbucket/vendor/autoload.php';\nIs there any aws sdk path need to be defined when sdk in html/awsbucket/ ?\n. @cjyclaire \nGetting the below error from wire log :\nError retrieving credentials from the instance profile metadata server. (Client error: GET http://169.254.169.254/latest/meta-data/iam/security-credentials/ resulted in a 404 Not Found response: string(90) \n. Hi.\nI modified the S3 client initiate \nFrom \n$s3 = new Aws\\S3\\S3Client([\n'key'=> $config['s3']['key'],\n'secret' => $config['s3']['secret'],\n'region' =>$config['s3']['region'],\n'version' => $config['s3']['version'],\n'signature' => $config['s3']['signature']\n]);\nTo\n$s3 = new Aws\\S3\\S3Client([\n'credentials' => [\n        'key'    => 'my-key',\n        'secret' => 'my-secret'\n        ],\n'region' =>$config['s3']['region'],\n'version' => $config['s3']['version'],\n'signature' => $config['s3']['signature']\n]);\n. ",
    "mdevine82": "Alright, so if I remove the region altogether from both the constructor and the putObject.  Will it do the lookup automatically and store in that internal cache?\n. I simply want to know that if I don't supply the region to any calls it will do the lookup itself.  So removing the region parameter from the S3MultiRegionClient and  putObject it will automatically do the lookup itself?\n. Great thanks, I'll run some tests and confirm it all works and then close this out.\n. Now when I try to get a presigned URL it tosses a 403 Forbidden.  It seems like I'm so close but missing one thing.  I just used the code example here for the preSigned\nThis came off a putObject request that had worked with the changes you requested.\n```\n$cmd = $s3Client->getCommand('GetObject', [\n            'Bucket' => $bucket,\n            'Key'    => $url\n        ]);\n    $request = $s3Client->createPresignedRequest($cmd, '+30 seconds');\n\n```\n. Is there somewhere else I should post to look into why I'm getting a 403 even with your code on the createPresignedRequest() call.  I will also note that it still gives a 403 on the determineBucketRegion() call.  So if I remove that it actually succeeds on the putObject.\nIt seems like some sort of permissions issue but the credentials being used have full permissions\n{\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": \"s3:*\",\n            \"Resource\": \"*\"\n        }\n    ]\n}\nThanks for your help on this.\n. Currently working with AWS Support, I've sent them the wire logs to find out what the issue is.  I'll post the resolution, to hopefully help someone else.\n. So I'm back with some new information based on some digging with the guys over at support.\nAfter I sent in the wire logs I got the following response back from them\n\nI can see that the x-amz-security-token header is being provided in the HTTP request, but I don't see the authentication header. This also explains why the server side logs show that the request came in unauthenticated.\n\nThey had me try putting the credentials into a environment variable but that didn't work.  From there I put a breakpoint into the SDK code around the CredentialProvider::env() function.  I noticed that it never ended up in here when running the following.\n$region = $s3Client->determineBucketRegion($bucket);\nThe support person also suggested using getBucketLocation($args) instead and with that I noticed that the following would actually go into CredentialProvider and work correctly.\n$region = $s3Client->getBucketLocation(['Bucket' => $bucket]);\nLet me know what you think and if there is anything you want me to send.\n. Well the strange part about it was no matter how I had the credentials input determineBucketRegion() would always fail and getBucketLocation() would always succeed.  This means if they were hardcoded or setup via environment variable.  While $s3Client->getCredentials(), would succeed in both instances of returning the FulfilledPromise.\n. Here is the output from both calls, I removed the actual credentials from the top but both of them were identical:\n GetBucketLocation\ndetermineBucketRegion\n. Updated there CA cert bundle didn't change anything along with hardcoding the openssl.cafile = /etc/ssl/certs/cacert.pem parameter in my php.ini file.  I wish we could figure out what's going on with it but by this time we already have worked around the issue and don't need to use the S3MultiRegionClient() anymore.  I was just trying to see if I could help you resolve the issue.\nIf you want to ask any additional questions or things for me to try, I'm game.\n. ",
    "Bubelbub": "Generating a new key works... Hmmm...\n. ",
    "jeroenbaas": "re: ssl.certificate_authority: I was indeed trying to see if this issue was related to another issue and left that piece in the code while debugging.\nIf I leave the code running a little longer it indeed seems to reset. I've not come across this behavior before in my own applications. Adding gc_collect_cycles() at each iteration seems to kill the intermediate increase in memory.\nStill don't quite get why 404 errors cause the garbage in the first place.\n. It does seem that way with me. commenting out the lines 41-48 you get:\n1 3620648\n2 3620680\n3 3620680\n4 3620680\n5 3620680\n6 3620680\n7 3620680\n8 3620680\n9 3620680\n10 3620680\n11 3620680\n12 3620680\ni.e. no excess memory used after each iteration. The \"memory leak\" only happens if you request a non-existent key from S3. So there is definitely something odd with server errors, at least with 404s.\n. Not sure how I can find out on from my EC2 instance; I thought guzzle was installed by composer when installing the aws-sdk?\nCurl:\n    [version_number] => 468992\n    [age] => 3\n    [features] => 952221\n    [ssl_version_number] => 0\n    [version] => 7.40.0\n    [host] => x86_64-redhat-linux-gnu\n    [ssl_version] => NSS/3.16.2.3 Basic ECC\n    [libz_version] => 1.2.8\nGuzzle:\nAssuming the composer/vender/guzzle/guzzlehttp/CHANGELOG.md is how to find out:\n6.0.2\n. m4.large\n. ",
    "fideloper": "Using the latest AWS CLI tool (which has elb and elbv2 commands):\nThis is on my account which has one \"classic\" ELB (no ALB's):\n``` bash\nNo v2 ELB's here\naws elbv2 --profile=helpspot --region=us-east-1 describe-load-balancers\n{\n   \u00a0\"LoadBalancers\": []\n}\n```\n``` bash\nOur ELB listed here\naws elb --profile=helpspot --region=us-east-1 describe-load-balancers \n{\n   \u00a0\"LoadBalancerDescriptions\": [ / LB's listed here / ]\n}\n```\nUsing the --debug option, I can see the end point used in v2 and regular ELB commands is https://elasticloadbalancing.us-east-1.amazonaws.com/\nThanks\n. And, finally, I see the documentation for 2012-06-01 within the V2 client API. That doesn't work with the V2 client, but this DOES work with \"V1\" client, if I specify that version:\nThis works \ud83c\udf89 \nNote the API version and class name is not V2 (although it's documented under the v2 client):\n``` php\n<?php\nrequire_once('vendor/autoload.php');\n$elb = new Aws\\ElasticLoadBalancing\\ElasticLoadBalancingClient([\n    'version' => '2012-06-01',\n    'region' => 'us-east-1',\n    'profile' => 'helpspot',\n]);\n// http://docs.aws.amazon.com/aws-sdk-php/v3/api/api-elasticloadbalancingv2-2015-12-01.html#describeloadbalancers\n$balancers = $elb->describeLoadBalancers();\ndd($balancers->get('LoadBalancerDescriptions'));\n```\nJust to test this doesn't work, with error The elasticloadbalancing service does not have version: 2015-12-01.:\n``` php\n<?php\nrequire_once('vendor/autoload.php');\n$elb = new Aws\\ElasticLoadBalancing\\ElasticLoadBalancingClient([\n    'version' => '2015-12-01',\n    'region' => 'us-east-1',\n    'profile' => 'helpspot',\n]);\n// http://docs.aws.amazon.com/aws-sdk-php/v3/api/api-elasticloadbalancingv2-2015-12-01.html#describeloadbalancers\n$balancers = $elb->describeLoadBalancers();\ndd($balancers->get('LoadBalancers'));\n// Throws Aws\\Exception\\UnresolvedApiException \n```\nConclusion\n\nThere may be an bug or missing endpoint for the v2 client\nDocumentation \"issue\" with the non-v2 client documentation \"hiding\" in the V2 client docs\n\nThanks!\n. Thank you! Appreciate the feedback and updates!\n. ",
    "mimarcos": "AWS: 3.18.39, cURL: 7.47.0, and Guzzle: 6.2.1.\nI don't know if I'll be able to easily flip that switch for wire log and get you information, at least right now. If it were happening on every request I'd be OK doing this in a test environment, but since the issue seems to pop up so sporadically, and has only presented in production, I'm not sure this is feasible.\n. ",
    "mwleinad": "We are also getting this, very sporadically (once a day). Any new idea of what could be causing this?\nAws\\Sqs\\Exception\\SqsExceptionvendor/aws/aws-sdk-php/src/WrappedHttpHandler.php in parseError\nerrorError executing \"ChangeMessageVisibility\" on \"https://sqs.us-east-1.amazonaws.com/231142459289/production_general\"; AWS HTTP error: Client error: POST https://sqs.us-east-1.amazonaws.com/231142459289/production_general resulted in a 400 Bad Request response: <?xml version=\"1.0\"?>SenderI (truncated...) InvalidParameterValue (client): Value AQEBTOZkqURu9XzHi+eHL4zOs/KvmSnWo2M2M7bKaetOdH4tEr5FN5+IlEfWDZFNTpYvYf/9A5X27i1oDMljNnxX1b1WabxomWiu6HOCi9y3+o4HlpLuLfUFe5dMJ8NIgjEMLtek0MFwDAYJqK7SyJW3jBpPtxrkdSS71DLg7lh9g4c6JOZSKAsmEbCzs3Qtb1M+IftmLgXLTr289w81KekRwJ6yLpzt+gtsJO00eCT+MvMPWRC6RMmN5cF+75zliMKDEFTqQ2sDEbUIvGpe7ksWrBvlwYup7bffobLaz8XZn2D3tAD7iuxyp4+lv8JZGKFpz0PFD6HJ93f2ql2R61ZGnMzFE/Th7tsmn8/rWipQJnfvGjJ1b0Nfh62HOUY2Q+c2jNMn4/MwfFpWqmDeOZMfuw== for parameter ReceiptHandle is invalid. Reason: Message does not exist or is not available for visibility timeout change. - <?xml version=\"1.0\"?><ErrorResponse xml {clipped}\n. ",
    "mcfedr": "I'm being a bit slow, It seems a shame there isn't a top-of-tree exception for all things that can go wrong in Aws SDK, this is a very common pattern in libraries, and very helpful. In the case where I noticed this problem, it didn't matter to me whether a request was made and failed, or this other problem, I just want to catch something that tells me the SDK couldn't do what I asked it to do.\nIts nice not to have to just catch \\Exception because that can have the side effect of catching other exceptions that really were unexpected.\n. ",
    "jcchavezs": "Will this be merged soon?\n. ",
    "tiagobrito": "@cjyclaire I'm doing $dynamoDbClient->getIterator('scan' [...]) and this will return an Iterator that will make multiple calls on Dynamodb for more than 1000 items or when the data > 1MB. I want to be able to create an Hydrator to convert the items on demand and not to have to hold for all the results in the Iterator.\n. The ScanFilter or FilterExpression isn't recommended for \"big exclusions\" from the original scan, right ?\nSo my option is to implement the logic using the LastEvaluatedKey and ExclusiveStartKey and Limit.\nThanks for your help\n. ",
    "fenying": "I post data to the generated URL\nxml\n<CompleteMultipartUpload>\n  <Part>\n    <PartNumber>1</PartNumber>\n    <ETag>\"90fa27e977c04aa4d078919682d76a54\"</ETag>\n  </Part>\n  <Part>\n    <PartNumber>2</PartNumber>\n    <ETag>\"eca63d229126d5200a88e0ebc5336bed\"</ETag>\n  </Part>\n</CompleteMultipartUpload>\nAnd S3 responsed with following info:\n``` xml\n\nSignatureDoesNotMatch\nThe request signature we calculated does not match the signature you provided. Check your key and signing method.\nAKIAP2AFGJ3NXBSAMPLE\nAWS4-HMAC-SHA256\n20160905T083913Z\n20160905/cn-north-1/s3/aws4_request\ne18441ca55291f6d28209e6aad4d375b852cac7f03e04af39e71a359c9de21ad\nf0b572ae321c3e9a891fbb096b14f587b881607a51a12815b5d29c2021e66b98\n41 57 53 34 2d 48 4d 41 43 2d 53 48 41 32 35 36 0a 32 30 31 36 30 39 30 35 54 30 38 33 39 31 33 5a 0a 32 30 31 36 30 39 30 35 2f 63 6e 2d 6e 6f 72 74 68 2d 31 2f 73 33 2f 61 77 73 34 5f 72 65 71 75 65 73 74 0a 65 31 38 34 34 31 63 61 35 35 32 39 31 66 36 64 32 38 32 30 39 65 36 61 61 64 34 64 33 37 35 62 38 35 32 63 61 63 37 66 30 33 65 30 34 61 66 33 39 65 37 31 61 33 35 39 63 39 64 65 32 31 61 64\nPOST\n/xxxxxxxxx-bucket/debug/upload.data\n%3CCompleteMultipartUpload%3E%0A%20%20%3CPart%3E%0A%20%20%20%20%3CPartNumber%3E1%3C%2FPartNumber%3E%0A%20%20%20%20%3CETag%3E%2290fa27e977c04aa4d078919682d76a54%22%3C%2FETag%3E%0A%20%20%3C%2FPart%3E%0A%20%20%3CPart%3E%0A%20%20%20%20%3CPartNumber%3E2%3C%2FPartNumber%3E%0A%20%20%20%20%3CETag%3E%22eca63d229126d5200a88e0ebc5336bed%22%3C%2FETag%3E%0A%20%20%3C%2FPart%3E%0A%3C%2FCompleteMultipartUpload%3E=&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAP2AFGJ3NXBSAMPLE%2F20160905%2Fcn-north-1%2Fs3%2Faws4_request&X-Amz-Date=20160905T083913Z&X-Amz-Expires=581&X-Amz-SignedHeaders=host&uploadId=0O7pQ8x5hGJCpdQzYHlaXNZwH7.bmTu5w0NwnMJIpBVlSoa8MxxVNs.35MED0AzhkRFakan9tYOksewrerTerpbMdewf0yW1yQyQDbB8w4BUe6MMhbsuHb25vsWo2d6s\nhost:s3.cn-north-1.amazonaws.com.cn\nhost\nUNSIGNED-PAYLOAD\n50 4f 53 54 0a 2f 62 61 69 63 68 75 61 6e 2d 73 74 6f 72 61 67 65 2f 64 65 62 75 67 2f 75 70 6c 6f 61 64 2e 64 61 74 61 0a 25 33 43 43 6f 6d 70 6c 65 74 65 4d 75 6c 74 69 70 61 72 74 55 70 6c 6f 61 64 25 33 45 25 30 41 25 32 30 25 32 30 25 33 43 50 61 72 74 25 33 45 25 30 41 25 32 30 25 32 30 25 32 30 25 32 30 25 33 43 50 61 72 74 4e 75 6d 62 65 72 25 33 45 31 25 33 43 25 32 46 50 61 72 74 4e 75 6d 62 65 72 25 33 45 25 30 41 25 32 30 25 32 30 25 32 30 25 32 30 25 33 43 45 54 61 67 25 33 45 25 32 32 39 30 66 61 32 37 65 39 37 37 63 30 34 61 61 34 64 30 37 38 39 31 39 36 38 32 64 37 36 61 35 34 25 32 32 25 33 43 25 32 46 45 54 61 67 25 33 45 25 30 41 25 32 30 25 32 30 25 33 43 25 32 46 50 61 72 74 25 33 45 25 30 41 25 32 30 25 32 30 25 33 43 50 61 72 74 25 33 45 25 30 41 25 32 30 25 32 30 25 32 30 25 32 30 25 33 43 50 61 72 74 4e 75 6d 62 65 72 25 33 45 32 25 33 43 25 32 46 50 61 72 74 4e 75 6d 62 65 72 25 33 45 25 30 41 25 32 30 25 32 30 25 32 30 25 32 30 25 33 43 45 54 61 67 25 33 45 25 32 32 65 63 61 36 33 64 32 32 39 31 32 36 64 35 32 30 30 61 38 38 65 30 65 62 63 35 33 33 36 62 65 64 25 32 32 25 33 43 25 32 46 45 54 61 67 25 33 45 25 30 41 25 32 30 25 32 30 25 33 43 25 32 46 50 61 72 74 25 33 45 25 30 41 25 33 43 25 32 46 43 6f 6d 70 6c 65 74 65 4d 75 6c 74 69 70 61 72 74 55 70 6c 6f 61 64 25 33 45 3d 26 58 2d 41 6d 7a 2d 41 6c 67 6f 72 69 74 68 6d 3d 41 57 53 34 2d 48 4d 41 43 2d 53 48 41 32 35 36 26 58 2d 41 6d 7a 2d 43 6f 6e 74 65 6e 74 2d 53 68 61 32 35 36 3d 55 4e 53 49 47 4e 45 44 2d 50 41 59 4c 4f 41 44 26 58 2d 41 6d 7a 2d 43 72 65 64 65 6e 74 69 61 6c 3d 41 4b 49 41 50 32 41 46 47 4a 33 4e 58 42 4d 4d 4b 46 57 41 25 32 46 32 30 31 36 30 39 30 35 25 32 46 63 6e 2d 6e 6f 72 74 68 2d 31 25 32 46 73 33 25 32 46 61 77 73 34 5f 72 65 71 75 65 73 74 26 58 2d 41 6d 7a 2d 44 61 74 65 3d 32 30 31 36 30 39 30 35 54 30 38 33 39 31 33 5a 26 58 2d 41 6d 7a 2d 45 78 70 69 72 65 73 3d 35 38 31 26 58 2d 41 6d 7a 2d 53 69 67 6e 65 64 48 65 61 64 65 72 73 3d 68 6f 73 74 26 75 70 6c 6f 61 64 49 64 3d 30 4f 37 70 51 38 78 35 68 47 4a 43 70 64 51 7a 59 48 6c 61 58 4e 5a 77 48 37 2e 62 6d 54 75 35 77 30 4e 77 6e 4d 4a 49 70 42 56 6c 53 6f 61 38 4d 78 78 56 4e 73 2e 33 35 4d 45 44 30 41 7a 68 6b 52 46 61 6b 61 6e 39 74 59 4f 6b 73 65 77 72 65 72 54 65 72 70 62 4d 64 65 77 66 30 79 57 31 79 51 79 51 44 62 42 38 77 34 42 55 65 36 4d 4d 68 62 73 75 48 62 32 35 76 73 57 6f 32 64 36 73 0a 68 6f 73 74 3a 73 33 2e 63 6e 2d 6e 6f 72 74 68 2d 31 2e 61 6d 61 7a 6f 6e 61 77 73 2e 63 6f 6d 2e 63 6e 0a 0a 68 6f 73 74 0a 55 4e 53 49 47 4e 45 44 2d 50 41 59 4c 4f 41 44\n8E52F8C1F38D1970\nEzGHTaKdYXbVyTwnWN1vh5nsUccbUy85MCa9oICN/g0AEpMvu2NsNcwIfp7yG2V1hk8kTYXldcQ=\n\n```\n. I've solved this problem. I just set CURL method as \"POST\", instead of \"post\"......\n. ",
    "assembledadam": "It isn't helpful because you are returning an array for fields that could only possibly have a single value - the array datatype for values that are scalar or dates is not suitable. When trying to extract the results, how can I tell if the field is supposed to contain one or many values?  Here's my usecase, which I would expect is fairly common:\nAWS CloudSearch does not have an 'update' operation, you can only 'add' and 'delete'. To have the same effect as an update, you have to use the 'add' operation with an ID which overwrites the existing document.\nThe gotcha here is that say you need to update a field in your document - by running 'add' with only the field you want to update, all the other fields are deleted (the whole document is entirely replaced).\nSo in order to update a value, we need to get the data from the original document. So I query CloudSearch.\nConsider the following search index fields:\nname: text\nadded_on: date\naliases: text-array\nWhen I query the data, I would get back something like this:\narray:2 [\n  \"id\" => \"some-permalink\"\n  \"fields\" => array:3 [\n    \"name\" => array:1 [\n      0 => \"foo\"\n    ]\n    \"permalink\" => array:1 [\n      0 => \"2016-06-21T11:25:22Z\"\n    ]\n    \"aliases\" => array:1 [\n      0 => \"bar\",\n      1 => \"baz\n    ]\nAs you can see from the original post, this is not compatible with the format the SDK expects for 'add' operations, and generally isn't useful for anything for the reasons above.\nSo to make it useful, we must first reconstruct the document, figuring out a way to distinguish fields that have multiple values to fields that have just one, then update the value, and then 'add' the document.\nIt's of course impossible to determine if the field expects single or multiple values, so the best we can do is extract the data with this rather hacky snippet: \n```\n$document = [];\nforeach ($result['hit'][0]['fields'] as $field => $value) {\n    if (count($value) == 1) {\n        $document[$field] = $value[0];\n    } else {\n        $document[$field] = $value;\n    }\n}\n```\nThis would be avoided if you only returned arrays for fields that could contain multiple values.\n. I haven't looked into the SDK code yet so I didn't realise it was the raw API response - that's not great!\nIt means that the SDK has no additional scope to identify the datatype of the fields over the SDK consumer, so the approach I outlined above to 'fix' the output is probably the best way.\nYes, in lieu of making the API output more useful (a different team I guess), a formatter helper would be great, thanks for the quick reply.  I could create this and submit a pull request but it's crunchtime on a project I'm working on at the moment, so would be later on this year.\n. ",
    "jaygurnani": "This is still an issue and I am currently using the .NET v3.3.0.0 SDK. I'm calling Search() using the AmazonCloudSearchDomainClient() and getting single valued items back in an array. This means it is then impossible to upload the exact same item into another domain easily. Is there an ETA for when this will be fixed? . ",
    "howardlopez": "@jaygurnani To make an equivalent request of a formatter helper for the .NET SDK, please submit an issue there.. I can't give any set timeline, but this is in our backlog and we are definitely paying attention to this feedback.. It looks like the result set for your request is being split up - you should also see a 'NextToken' attribute in the Result object that you can use to get the next result(s). \nOne thing you can try is using a paginator for the ListChannels command:\n```php\n$results = $client->getPaginator('ListChannels');\nforeach($results as $result) {\n    var_dump($result->get('Channels'));\n}\n```\n. @norbert-yoimo Glad it helped, and thanks for bringing this to our attention. We also found that the result sets are being split up in unpredictable ways, and will be letting the service team know about it.. @NeilJ247: I tested using your parameters, and the \u00fa and \u00e1 characters, with and without the Marshaler, with the SDK and the AWS CLI, and I was unable to reproduce the error - the item was created just fine. Were you getting the error for your exact sample you posted, or was it a more complicated structure? If it was, would you mind sending the entire real code sample (with sensitive data redacted)? \nOne thing we can try - if you have the CLI installed, would you mind running this command, substituting the appropriate table name and primary key attributes?\naws dynamodb put-item --debug --table-name \"YourTableName\" --item '{\"YourPrimaryKey\": {\"S\": \"SomePrimaryKeyValue\"}, \"key1\": {\"S\": \"Jes\u00fas\"}, \"key2\": {\"M\": {\"key3\": {\"S\": \"Jes\u00fas\"}}}}'\nPlease let us know whether or not that succeeds, and what the debug output is.\n. @dfrnks Would you mind also putting the code for the PutObject call and the stack trace you got from it?. You're right about S3's 'directory' representation, which means you can't simply delete them as you would a normal file directory. One simple way to delete a 'directory' is to get all the objects of a certain prefix (i.e. your 'directory' path) using listObjectsV2 and then delete them with deleteObject or deleteObjects\n```php\n$results = $s3->listObjectsV2([\n    'Bucket' => 'your-bucket',\n    'Prefix' => 'path/to/your/directory'\n]);\nif (isset($results['Contents'])) {\n    foreach ($results['Contents'] as $result) {\n        $s3->deleteObject([\n            'Bucket' => 'your-bucket',\n            'Key' => $result['Key']\n        ]);\n    }\n}\n``. @neoacevedo To delete an empty S3 'directory', a trailing slash is necessary at the end of the path. In the example below,path/to/directorywould not work as theKey`.\nphp\n$s3->deleteObject([\n    'Bucket' => 'your-bucket', \n    'Key' => 'path/to/directory/'\n]);. @neoacevedo Just to clarify, you're adding a / to the end of the directory path, and the directory is empty (i.e. there are no files whose path begins with the directory path)? \nIf so, I'd suggest doing an experiment by creating an empty blank directory in the S3 console, and then trying that deleteObject call on it, and letting us know how that goes.. The first element returned should be the prefix, as ListObjectsV2 is finding both the directory (whose key is the prefix) and the objects inside. The way to get rid of the directory is to delete all of the objects returned, including the prefix (directory) object itself, which looks like it's working for you.. The getProducts method specified here in the PricingClient docs returns both rateCode and sku in the PriceList. \nIf this doesn't answer your question, can you be more specific about what you are trying to do, and also give a code example as a starting point?. Unfortunately that is not our product. I would suggest contacting their support channels directly or looking for other community support around the product.. If you're using composer, this should work within your composer file to get v2.8.x of the SDK:\n\"require\": { \n        \"aws/aws-sdk-php\": \"~2.8\" \n    }\nThis option lets you specify checking out the 2.8 branch directly, which should lead to the same result.\n\"require\": { \n        \"aws/aws-sdk-php\": \"2.8.x-dev\" \n    }\nIf this is not what you're asking, please elaborate with the code or command sample that you're using.. I used your exact composer file and was able to successfully run composer update on a new project, so I suspect you may be having an issue with local repo changes or config settings. Have you tried running composer update in an empty directory with that composer file? \nThere are a variety of errors in that log. Since you are getting a message about outdated packages, you can try running the following to update composer itself and clear package cache:\ncomposer self-update\ncomposer clear-cache\nSince there are permissions errors as well, I'd verify that the relevant directories have the proper write access. \nIf you're still having Composer issues, the Composer Github repo may be a good resource for further troubleshooting.\n. It looks like there was an internal issue resulting in the declared timestamp format issued by MediaConvert not matching the actual timestamp value being sent. We worked with the service team to push out a fix and have released a new version of the SDK, which should rectify this problem. Let us know if you have any further issues.\nThanks for bringing this to our attention!. If I understand correctly, you're getting it to work properly with the normal PHP command line executable in your environment, but not on your host's environment. Can you provide the server environment and PHP CLI version, and any other pertinent information about your host's particular configuration? . You can take a look at the ClientResolver class, which adds the default RetryMiddleware implementation.\nIf you're simply looking to change the max number of retries, you can set this in the client constructor.. Can you provide a complete code sample of what you're trying?. @lhaley2011 We've released an update to address this. Thanks for notifying us!. @takkaria Thanks for bringing this to our attention! Please feel free to submit a pull request, and we'll go from there.. @takkaria Your PR has been merged. Thank you!. Can you provide your code for assuming the IAM role, as well as any error message you're receiving when it fails to reach the resource?. The PHP SDK documentation for the UpdateItem command should be a helpful resource. Note that the UpdateExpression parameter contains the logic for updating, which DynamoDb has documented extensively here. \nIf you're still having issues after trying with the resources above, please post the exact start and preferred end state of the item, as well as a code sample of your completed attempt with updateItem. . A few questions:\n- Can you verify the format returned by the marshaler (in $eav) is the format specified for ExpressionAttributeValues as laid out in the PHP SDK docs for UpdateItem?\n- In your ConditionExpression, is the attribute being checked being referenced properly (see item attribute reference)? \n- If you're still having issues, can you provide a complete code sample with more context, as well as an error message?. Most of our SDK releases support new features being released by services, which as you can imagine come out quite frequently. Hopefully the upcoming updates by @greysteil will help reduce the PR noise on your end.. The way the // REQUIRED comment works on the parameter syntax examples is that they signify requirement if its parent is being used. For example, here's the beginning part of the CreateDistribution syntax:\n$result = $client->createDistribution([\n    'DistributionConfig' => [ // REQUIRED\n        'Aliases' => [\n            'Items' => ['<string>', ...],\n            'Quantity' => <integer>, // REQUIRED\n        ],\n        ...\n    ]\n)\nQuantity within Aliases is marked as required because if Aliases is supplied, then Quantity within Aliases must be supplied. Aliases itself is not required, and thus if Aliases is not supplied, then Quantity is not required.. With respect to the DistributionConfig shape, it looks like there is a bug in the docs generation for the Required flag there. We'll post back here when a fix is in place.. The code to resolve this issue has been merged in, but the API documentation won't be updated until the next release is issued. Keeping this issue open until then.. The API documentation is now updated with the corrected logic for Required members in shapes.. @downsider Thank you for your PR. Code looks good and it's now been merged.. Can you provide a code sample & PHP version (and any relevant context), as well as the full stack trace of the error message? . @scottf-tvw Can you also provide a code sample and stack trace, if available?. @luudv, Thanks for the code sample. Can you tell me your exact PHP version and the full stack trace of the error (if one was output)?. Thanks for the additional data, everyone. It looks like this issue occurs for earlier versions of PHP 5.5.x, which contained a bug with late static bindings and closures. As already alluded to in this thread, you have a couple quick fix options:\n As a temporary fix, you can revert to an SDK release before 3.69.0\n If you have the ability, you can update PHP to the latest patch version - PHP 5.5.38 does not contain the bug, and neither does 5.6+\nWe're also working on a fix to avoid the condition triggering the bug in PHP, so that earlier versions of 5.5 can continue functioning properly. We'll update this thread when it is available.. A fix for this issue has been merged into the repository and is available for pulling on the master branch. For those using Composer, the changes will be incorporated into the next release.. The fix is now included in the release 3.69.4.. ext-pcntl and ext-sockets were added to require-dev, and are thus only needed if you're going to be developing for the SDK itself. The standard Composer workflow is to create a composer.json file for your own project and then declare dependencies therein. For example:\n{\n    \"require\": {\n        \"aws/aws-sdk-php\": \"3.*\"\n    }\n}\nPackages declared as dependencies will only add the packages declared in their own require property, and ignore require-dev. The above composer file will work in an environment without ext-pcntl or ext-sockets when running a composer install. \nLet us know if you have a use case that can't be resolved with the above.. Is there any reason you're not just declaring the SDK as a dependency in your project's composer file? The require-dev packages only apply to the root project, as outlined by the Composer docs, so if the SDK is listed as a project dependency, the require-dev packages won't be considered. This happens regardless of what environment you're in. composer install shouldn't be run on the SDK directory itself unless you're planning to do development work on the SDK itself. . First of all, thank you for your well-structured issue report. \nA couple of things to try:\n- Delete your composer.lock file and your vendor directory, and then run composer update again. This will basically reinstall all your dependencies from scratch and could help clear up any funkiness in your composer install.\n- Make sure that the SDK (or individual components) are not installed anywhere else in your company's codebase. If there are multiple Composer installations that both bring in the SDK, for instance, it could cause this. This could also be happening if SDK components are brought in from a source directory, in addition to the Composer vendor directory.. I haven't been able to reproduce your issue - if I had, the SDK would be broken for all our customers. As a basic test, you can try creating a new directory in your environment and running:\ncomposer require aws/aws-sdk-php\nThen create a new PHP file with your require:\nrequire_once 'vendor/autoload.php';\nIf this works in your environment, then you know that something about your project is creating the conflict. \nAt this point you can try testing with your full project's composer.json file in this directory and see if that works. If it does, then you know the issue is related to other code or imports in your project. \nIf it doesn't, could you post your project's composer.json file?. Unfortunately we cannot merge in your PR which addresses an issue specific to your environment.\nIs it possible that there may be code being brought in outside the scope of your search? Could code be being brought in from above your project root, or from another volume or somewhere else on your network? \nIf you have access to a file-searching utility like grep, you could run a search for any declaration of the Aws namespace:\ngrep -R \"namespace Aws\" *\nI would recommend running searches like that from the root of your environment (i.e. /), and from any distinct location you suspect your project may be bringing in code. . One last thing I would suggest is to see if anything in your codebase is requiring the functions.php file directly. That would trigger a \"Cannot redeclare\" error as well. Good luck!. Closing this PR as discussed in #1655 .. Thanks for making the change, will be merged after Reinvent.. The error suppression in this function has been refactored and merged in, so memory errors should now be emitted. It's currently available on the master branch and will be in the next release.. @huubvdw, your investigation and PR are appreciated. Unfortunately, PutObject only requires the Content-MD5 header when there are object lock parameters, so we won't be able to accept your PR to change the general PutObject behavior in this way. \nS3 and the SDK teams are aware of the issue and have discussed options internally, but for now, you'll have to supply your own ContentMD5 parameter to object lock requests with PutObject. For example:\n$client->putObject([\n    'Bucket' => 'your_bucket',\n    'Key' => 'your_key',\n    'Body' => 'your_body',\n    'ObjectLockMode' => 'GOVERNANCE',\n    'ObjectLockRetainUntilDate' => $someDate\n    'ContentMD5' => base64_encode(md5('your_body', true))\n]);\n. Thank you for the PR, but it can't be accepted per the discussion on the associated issue.. Can you provide a complete sample of the test code that is failing?. Thanks to MattStypa, this issue should now be resolved with #1705. The code is currently merged into the master branch and will be included in the next release.. @MattStypa Thanks for the PR. It's been approved and merged.. Thanks for the PR, but unfortunately version 2.x of the SDK is deprecated and no longer accepting PRs for new features. You are welcome to submit PRs to the current version (3.x), however.. @sordev We published a release 3.87.7 that includes a fix for the service model. This error should be resolved now, but let us know if you run into any more issues.. The reason the previous PR was rejected is that doing a completely thorough check would require copying the array, or at least all of its keys, which would have a performance impact for every call that checks maps. The check for $array[0] is a quick heuristic solution that unfortunately doesn't work for cases like yours. \nOne potential option is to add to the heuristic by also checking the last key of the array and comparing it to the array length (minus one). This would make it far less likely that a random input map looks like an indexed array, as it would have to match both the first and last index.\nIf you'd like to submit a PR, it would be welcome. \nOtherwise, we'd add it to our backlog, but I wouldn't be able to make any promises regarding the timeframe.. Thanks for the making the revisions - PR has been merged.. @amfahrj Looks like this issue stemmed from changes in stream behavior in PHP 7+. We just merged in a fix in #1744, which will be included in the next release.. Unfortunately, custom handlers aren't currently supported with the default credential provider chain. The default provider shouldn't be checking STS on its own, but perhaps you're referring to the instance profile provider? \nIn any case, you should be able to supply your own custom provider using your handler.. Yes, the default provider will call other providers as part of its chain (including InstanceProfileProvider), some of which can send requests.. A PR was merged addressing this issue, but keeping this open until release and propagation to the hosted docs occurs.. The fix should now be propagated to the docs pages: https://docs.aws.amazon.com/aws-sdk-php/v3/api/api-logs-2014-03-28.html#filterlogevents\nNote you may have to clear your browser cache to see the change.. It may be more helpful to call out the specific file offending filename here.. Needs space between fake-secret-id and |. Added a couple of cases with trailing numbers.. Agreed about the curl extension check, but keeping this as a class variable allows re-use for testing, minimizing future code changes should options change.. Agreed.. Current scope is for all operations.. Let's leave nightly in here. It may be redundant with 7.3 for some period of time, but going forward we'll always want to test against the latest build.. As above, let's leave nightly in here.. Added appropriate in-line comments in this revision. Test added in this revision.. The Aws namespace is added by default, so the category here should just be \"Waiter\".. Style nitpick: can the variable name be more descriptive, like $isAssociative?. More style nitpicks:\n Can the variable be more descriptive, like $expectedIndex? \n Can you remove the whitespace after the variable?. Can you add in another test to check for the case of an array with the proper indexed array keys, but out of order? For example:\n[2 => 'foo', 0 => 'bar', 1 => 'baz']. Can you add in a test to check that the copy keeps config options from the original (for options that were not overwritten)?. ",
    "LionelMarbot": "Ok, thanks!\n. @imshashank Thank you for coming back to me.\nWe did indeed look at mb_encode_mimeheader( ) in detail and it also looks like the output Encuesta \"Mejora del =?UTF-8?B?RGVzZW1wZcOxbyBkZWwgQWdlbnRlIGRlIFNlcnZp?= =?UTF-8?B?Y2lvIg==?= <foo@bar.com> is RFC 2047 compliant.\nNevertheless, AWS SES won't accept it.\nWe will probably use following workaround:\n$source = \"=?UTF-8?B?\" . base64_encode($name) . \"?= <{$from}>\";\nThat way, the full name is encoded (including the part before the special character \u00f1) and this works with AWS SES.\nThanks anyway!\nBest,\nLionel. ",
    "pushsoftdev": "include_once 'Services/S3Manager.php';\n$S3Manager = new S3Manager();\n$keyPrefix = \"KEY PREFIX IN S3\";\n$destinationPath = \"LOCAL DIR PATH\";\n$transferManager = $S3Manager->getTransferManagerWithSourceAndDestinationPath($keyPrefix, $destinationPath);\n$promise = $transferManager->promise();\n$promise->then(function() {\n    echo \"Success\";\n});\n$promise->otherwise(function($reason){\n    echo \"Failed to transfer\";\n    var_dump($reason);\n});\n. The Transfer manager tries to transfer the images but nothing gets saved in my local directory. After some digging around, I have used \n$metaPromise = \\GuzzleHttp\\Promise\\all($promises)->wait();\nwhich solves the problem. But this is not useful as it synchronous which causes the PHP timeout in my server.\nPlease let me know if my statements are not clear enough.\n. Thanks for the reply. But please consider the below situation.\n1. My server has a PHP execution timeout of 120 seconds.\n2. The folder in the S3 has 1 GB of data to be synced to my local directory.\n3. It will work if I put the GuzzelHTTP\\Promises\\all(promises)->wait(). But\n   since this wait function will block the main thread, my PHP execution time\n   will reach the limit.\nHow to overcome this? Please let me know if I am still not understood\nproperly.\nOn Tue, 13 Sep 2016 at 12:13 AM cjyclaire notifications@github.com wrote:\n\n@pushsoftdev https://github.com/pushsoftdev\nSo, you will need to wait on the promise at some point. The SDK isn't\nforking off background processes or farming work off to a NodeJS-style\npersistent global event loop, so scripts will need to include a directive\nto handle outstanding promises.\nWhat the SDK can do with promises is handle them concurrently, which can\nspeed up scripts that involve a lot of I/O time. In fact, the transfer\nmanager promise is composed of promises tracking the upload of individual\nfiles. The individual promises are handled five at a time, and when all\nuploads are complete, the transfer manager's meta promise is fulfilled.\nIf you wanted to upload multiple directories, you could compose several\ntransfer manager promises into another meta promise:\n$promises = [];$source = \"s3://bucket_a/prefix\";$dest = \"LOCAL_DIR\";$manager = new \\Aws\\S3\\Transfer($client, $source, $dest);$promises []= $manager->promise();$source = \"s3://bucket_b/prefix\";$dest = \"LOCAL_DIR\";$manager = new \\Aws\\S3\\Transfer($client, $source, $dest);$promises []= $manager->promise();$metaPromise = \\GuzzleHttp\\Promise\\all($promises)->wait();\nThe SDK would then interleave those two directory uploads and spend less\ntime simply waiting for HTTP responses.\nHope it helps.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/aws/aws-sdk-php/issues/1083#issuecomment-246449009,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AT_Ertbvc4ExTg56_362QaBWBp_o0ercks5qpZ1BgaJpZM4J6fWV\n.\n\nRegards,\nPushparaj J\n. As you know, while using CommandPool also, we need to call the wait() function on the promise. This will synchronously wait until all the commands are completed. So there are more chances of PHP execution timeout. \n\nAm I correct?\n. The only way to handle this is setting PHP execution timeout to an infinite number so that the PHP script will execute until all the files are being downloaded.\nLet me know If I am wrong.\n. ",
    "tatoshko": "@cjyclaire I deliberately used a non-existent key. If i use doesObjectExists i have same error.\n. @cjyclaire \n\nshould return false\n\nBut not.\n\nshould catch\n\nBut not catch.(See issue subject Uncaught exception 'GuzzleHttp\\Exception\\ClientException')\nYour example for me calling fatal error, the script stops execution. In php error log i have this:\n``\nPHP message: PHP  17. GuzzleHttp\\Middleware::GuzzleHttp\\{closure}($response = class GuzzleHttp\\Psr7\\Response { private $reasonPhrase = 'Not Found'; private $\n2016/09/16 10:01:01 [error] 82552#0: *2677 FastCGI sent in stderr: \"PHP message: PHP Warning:  Uncaught exception 'GuzzleHttp\\Exception\\ClientException' with message 'Client error:HEAD https://s3.eu-central-1.amazonaws.com/wbncdn/not/exists/dir/test.pngresulted in a404 Not Found` response:\n' in /www/tato/public_html/vendor/guzzlehttp/guzzle/src/Exception/RequestException.php:107\nStack trace:\n0 /www/tato/public_html/vendor/guzzlehttp/guzzle/src/Middleware.php(65): GuzzleHttp\\Exception\\RequestException::create(Object(GuzzleHttp\\Psr7\\Request), Object(GuzzleHttp\\Psr7\\Response))\n1 /www/tato/public_html/vendor/guzzlehttp/promises/src/Promise.php(203): GuzzleHttp\\Middleware::GuzzleHttp{closure}(Object(GuzzleHttp\\Psr7\\Response))\n2 /www/tato/public_html/vendor/guzzlehttp/promises/src/Promise.php(156): GuzzleHttp\\Promise\\Promise::callHandler(1, Object(GuzzleHttp\\Psr7\\Response), Array)\n3 /www/tato/public_html/vendor/guzzlehttp/promises/src/TaskQueue.php(61): GuzzleHttp\\Promise\\Promise::GuzzleHttp\\Promise{closure}()\n4 /www/tato/public_html/vendor/guzzlehttp/guzzle/src/H...\nPHP message: PHP Stack trace:\nPHP message: PHP   1. {main}() /www/tato/public_html/index.php:0\nPHP message: PHP   2. Aws\\S3\\S3Client->doesObjectExist($bucket = 'wbncdn', $key = 'not/exists/dir/test.png', $options = uninitialized) /www/tato/public_html/index.php:39\nPHP message: PHP   3. Aws\\S3\\S3Client->checkExistenceWithCommand($command = class Aws\\Command { private $name = 'HeadObject'; private $handlerList = class Aws\\HandlerList { private $handler = class Aws\\WrappedHttpHandler { private $httpHandler = class Aws\\Handler\\GuzzleV6\\GuzzleHandler { private $client = class GuzzleHttp\\Client { private $config = array ('handler' => class GuzzleHttp\\HandlerStack { private $handler = class Closure { public $static = array ('default' => class Closure { public $static = array ('default' => class GuzzleHttp\\Handler\\CurlMultiHandler { private $factory = class GuzzleHttp\\Handler\\CurlFactory { private $handles = array (0 => resource(104) of type (curl\n2016/09/16 10:01:01 [error] 82552#0: *2677 FastCGI sent in stderr: \"fPending() /www/tato/public_html/vendor/guzzlehttp/promises/src/Promise.php:266\nPHP message: PHP  11. GuzzleHttp\\Promise\\Promise->invokeWaitFn() /www/tato/public_html/vendor/guzzlehttp/promises/src/Promise.php:223\nPHP message: PHP  12. GuzzleHttp\\Handler\\CurlMultiHandler->execute(TRUE) /www/tato/public_html/vendor/guzzlehttp/promises/src/Promise.php:246\nPHP message: PHP  13. GuzzleHttp\\Handler\\CurlMultiHandler->tick() /www/tato/public_html/vendor/guzzlehttp/guzzle/src/Handler/CurlMultiHandler.php:123\nPHP message: PHP  14. GuzzleHttp\\Promise\\TaskQueue->run() /www/tato/public_html/vendor/guzzlehttp/guzzle/src/Handler/CurlMultiHandler.php:96\nPHP message: PHP  15. GuzzleHttp\\Promise\\Promise::GuzzleHttp\\Promise{closure}() /www/tato/public_html/vendor/guzzlehttp/promises/src/TaskQueue.php:61\nPHP message: PHP  16. GuzzleHttp\\Promise\\Promise::callHandler($index = 1, $value = class GuzzleHttp\\Psr7\\Response { private $reasonPhrase = 'Not Found'; private $statusCode = 404; private $headers = array ('x-amz-request-id' => array (0 => '5DE510A7903B66D7'), 'x-amz-id-2' => array (0 => 'eHdVcXQwGRSTDQhKoDKgDCA4SxDFrFGU20M7QuZczWVfpgsMYrhpTKKJvX45erAshfD/oMKIvig='), 'Content-Type' => array (0 => 'application/xml'), 'Transfer-Encoding' => array (0 => 'chunked'), 'Date' => array (0 => 'Fri, 16 Sep 2016 00:00:55 GMT'), 'Server' => array (0 => 'AmazonS3')); private $headerNames = array ('x-amz-request-id' => 'x-amz-request-id', 'x-amz-id-2' => 'x-amz-id-2', 'content-type' => 'Content-Type', 'transfer-encoding' => 'Transfer-Encoding', 'date' => 'Date', 'server' => 'Server'); private $protocol = '1.1'; private $stream = class GuzzleHttp\\Psr7\\Stream { private $stream = resource(102) of type (stream); private $size = 0; private $seekable = TRUE; private $readable = TRUE; private $writable = TRUE; private $uri = ...\nPHP message: PHP  17. GuzzleHttp\\Middleware::GuzzleHttp{closure}($response = class GuzzleHttp\\Psr7\\Response { private $reasonPhrase = 'Not Found'; private $\n```\n. OSX El Capitan 10.11.6\nPHP 5.6.18\nGuzzle 6.2.1 (from was-sdk required from aws-sdk)\nworkaround step-by-step:\n1. mkdir test && cd test\n2. composer init\n3. composer require aws/aws-sdk-php\n4. touch index.php\ncomposer.json\n{\n    \"name\": \"test/public_html\",\n    \"authors\": [\n        {\n            \"name\": \"test\",\n            \"email\": \"t@t.t\"\n        }\n    ],\n    \"require\": {\n        \"aws/aws-sdk-php\": \"dev-master\"\n    }\n}\nIn index.php:\n```\n<?php\nrequire_once DIR . \"/vendor/autoload.php\";\n$credentials = new \\Aws\\Credentials\\Credentials('', '');\n$client = new \\Aws\\S3\\S3Client([\n    'version'     => 'latest',\n    'region'      => 'eu-central-1',\n    'credentials' => $credentials,\n]);\ntry {\n    $result = $client->doesObjectExist(\n        'my-bucket',\n        'not/exists/dir/test.png'\n    );\nvar_dump($result);\n\n} catch (Exception $e) {\n    var_dump($e);\n}\necho 'OK';\n```\n. @cjyclaire PHP 7.0.10 installation solved the problem. But what to do with 5.6? i'am should use 5.6 version for some projects.\n. @cjyclaire it's xdebug problem. Try to enable.\n\n...extension will greatly slow down the SDK\n\nChange to \"It breaks your app\"\nAmazing... if i want use SDK i can not use xdebug.\n. @cjyclaire ok. I'am identified the problem.\nxdebug.collect_params = 4\nAll works only if collect_params = 1 or disabled.\n. @cjyclaire thx, think you may close that issue, likely it's not possible to solve.\n. finally, i set max_execution_time greater value(3600) and a page was loaded. Weight of the page ~440mb. 440mb stack trace.\n. @jtpenny, you need to read this first AWS error handling\n. @jtpenny, enjoy (=\n. ",
    "jtpenny": "I am seeing the same problem. \ntry {\n        $s3Client = $qsrDB->s3connection();\n        $result = $s3Client->getIterator('ListObjects',[\n            'Bucket'=>$bucket,\n            'Prefix'=> $prefix\n        ]);\n} catch(\\Exception $e) {\n    echo 'There was a problem.';\n}\nThe code works fine, but when I put in a bad bucket to force the exception, it is not caught. \nFatal error: Uncaught exception 'Aws\\S3\\Exception\\S3Exception' with message 'Error executing \"ListObjects\" on \"---\"; AWS HTTP error: Client error: GET --- resulted in a 404 Not Found response: <?xml version=\"1.0\" encoding=\"UTF-8\"?> NoSuchBucketThe specified bucket does not exist NoSuchBucketThe specified bucket does not existD1EDC36CA6DCD448' exception 'GuzzleHttp\\Exception\\ClientException' with message 'Client error: `GET --- in phar:///aws.phar/Aws/WrappedHttpHandler.php on line 192\nPHP 5.6 using the AWS Phar. Xdebug is not enabled. Any new ideas on this?\n. @tatoshko I tried Aws\\S3\\Exception\\S3Exception but it didn't work. However, getIterator returns a promise, and the asynchronous section helped. Thank you for the help.\n. ",
    "s3bubble": "I am also getting this exact error and I cannot find a fix. \nhttps://github.com/aws/aws-sdk-php/issues/1237\nI have tried everything suggested in this post I am running php 7.0.12. @imshashank,\nAny update on this, how can I just display the message?\nThanks\nSam. Hi @imshashank,\nYou say \"You can get different parts of the message.\" can you explain on how to do this it does not make sense for us to add custom error messages to every call when the sdk returns the correct one.\nTake this error.\n~~~\nError executing \u201cListBuckets\u201d on \u201chttps://s3.amazonaws.com/\u201d; AWS HTTP error: Client error: GET https://s3.amazonaws.com/ resulted in a 403 Forbidden response:\n<?xml version=\u201d1.0\u2033 encoding=\u201dUTF-8\u2033?>\nNotSignedUpYour account is not signed up for the S3 (truncated\u2026)\nNotSignedUp (client): Your account is not signed up for the S3 service. You must sign up before you can use S3. \u2013 <?xml version=\u201d1.0\u2033 encoding=\u201dUTF-8\u2033?>\nNotSignedUpYour account is not signed up for the S3 service. You must sign up before you can use S3.. Make sure you have the correct policy permissions set\n~~~ \nWe could just respond this which would work fine for our setup.\n~~~\nYour account is not signed up for the S3 service. You must sign up before you can use S3.\n~~~\nThanks. Ok i have managed to get what we need by just editing line 176 in the WrappedHttpHandler.php \n~~~\n$serviceError .= $parts['message'];\n~~~ \nThanks for your time. Ah great thanks @kstich for the update ;). ",
    "jonshilton": "@cjyclaire fantastic, thanks for providing the info.\n. ",
    "steenbag": "Ah, OK I believe I have found my issue.  I was using \"~3.0\" as my dependency (originally discovered the issue using \"aws/aws-sdk-php\": \"*\"). Based on that, it looks like composer was installing 432d49bc25014b3646512f2822b5e004dcff7f4b, which appears to be fairly outdated. Changing the version dependency to \"aws/aws-sdk-php\": \"3.19.8\" seems to fix the issue for me.\nThanks,\nAndrew\n. ",
    "lastday154": "@cjyclaire . Thanks you for response.\nBasically, We are running a online shopping website, and We want all the log writing to cloudwatch logs as fast as possible. The function above just let you upload the log directly to a log stream. So basically you have to get the token, and using putLogEvents to put the log into cloud. \nSo if I change the putlogEvent to putLogEventAsync without calling promise->wait().\nIt is much faster but it does not upload to cloud watch log. like\n public function writeToCloudWatch($line)\n    {\n        $microTime = $this->timeProvider->getMicroTime();\nif (!isset($this->nextSequenceToken)){\n        $responseModel = $this->client->describeLogStreams([\n            'logGroupName' => $this->logGroup,\n            'logStreamNamePrefix' => $this->logStream,\n        ]);\n        $this->nextSequenceToken = $responseModel['logStreams']['0']['uploadSequenceToken'];\n    }\n    try {\n        $promise = $this->client->putLogEventsAsync([\n            'logGroupName' => $this->logGroup,\n            'logStreamName' => $this->logStream,\n            'logEvents' => [\n                [\n                    'message' => $line,\n                    'timestamp' => $microTime,\n                ]\n            ],\n            'sequenceToken' => $this->nextSequenceToken\n        ]);\n    } catch (CloudWatchLogsException $e) {\n        // We don't want any exception when putLogEvents is not working\n    }\n}\nSo my first question is why we need to call wait to get Async working?\nBecause if we call wait it seems like Sync. \n. @mtdowling I am really waiting your response after seeing all the excellent answers.\nKind of admire your talent. Coming back my problem.\nI want to log my input directly to a cloud watch log stream AS FAST AS POSSIBLE.\nIt would be great if you can give me specific code to work with cloudwatch logs.\nHere is the code I am using.\n    /*\n     * @param string $line\n     /\n    public function writeToCloudWatch($line)\n    {\n        $microTime = $this->timeProvider->getMicroTime();\n```\n    if (!isset($this->nextSequenceToken)){\n        $responseModel = $this->client->describeLogStreams([\n            'logGroupName' => $this->logGroup,\n            'logStreamNamePrefix' => $this->logStream,\n        ]);\n        $this->nextSequenceToken = $responseModel['logStreams']['0']['uploadSequenceToken'];\n    }\n    try {\n        $promise = $this->client->putLogEventsAsync([\n            'logGroupName' => $this->logGroup,\n            'logStreamName' => $this->logStream,\n            'logEvents' => [\n                [\n                    'message' => $line,\n                    'timestamp' => $microTime,\n                ]\n            ],\n            'sequenceToken' => $this->nextSequenceToken\n        ]);\n        try {\n            $result = $promise->wait();\n        } catch (CloudWatchLogsException $e) {\n            echo $e->getMessage();\n        }\n        if (isset($result['nextSequenceToken'])) {\n            $this->nextSequenceToken = $result['nextSequenceToken'];\n        }\n} catch (CloudWatchLogsException $e) {\n    // We don't want any exception when putLogEvents is not working\n}\n\n}\n```\n. Hi @cjyclaire . I have implemented the code you share.\nBut it does not send all of my requests. Instead of sending requests to cloudwatch logs, it only sends first requests.\nfor ($i=0; $i<50; $i++) {\n```\n$previousTime = microtime();\n$cloudWriter->writeToCloudWatch(\"hi$i\"); //only send \"hi0\" to cloudwatch\n$currentTime = microtime();\n$interval = $currentTime - $previousTime;\necho $interval * 1000;\necho \" #$i.\\n\";\n```\n}\nit would be great if you give me a online chat time where we can use teamview.\n. @cjyclaire . Basically, you send asynchronous with the same token, it will only send the first request's data only.\nBut if you want to get the token, the only way to get it is from promise->wait when using Async, or from nextSequenceToken when using Sync. Either way, it is not truly asynchronous workflow.\npublic function writeToCloudWatch($line)\n    {\n        $microTime = $this->timeProvider->getMicroTime();\n        if (!isset($this->nextSequenceToken)) {\n            $responseModel = $this->client->describeLogStreams([\n                'logGroupName' => $this->logGroup,\n                'logStreamNamePrefix' => $this->logStream,\n            ]);\n            $this->nextSequenceToken = $responseModel['logStreams']['0']['uploadSequenceToken'];\n        }\n        try {\n            for($i=0; $i<10; $i++) {\n                $this->client->putLogEventsAsync([\n                    'logGroupName' => $this->logGroup,\n                    'logStreamName' => $this->logStream,\n                    'logEvents' => [\n                        [\n                            'message' => $line,\n                            'timestamp' => $microTime,\n                        ]\n                    ],\n                    'sequenceToken' => $this->nextSequenceToken,\n                ]);\n            }\n        } catch (CloudWatchLogsException $e) {\n            // We don't want any exception when putLogEvents is not working\n            $e->getMessage().'\\n';\n        }\n        $this->loop->run();\n    }\n. @cjyclaire \nBut how can you get the token from describeLogStreamsAsync without calling wait. Because it returns you only promises object, isn't it ?\n. @cjyclaire , if you could be more specific about how to resolve the promise with describeLogStreamAsync so that it can directly fetch the token to putLogEventsAsync, my problem will be solved immediately. \nHere is the code I am using but cannot directly do the fetching.\npublic function writeToCloudWatch($line)\n    {\n        $microTime = $this->timeProvider->getMicroTime();\n        if (!isset($this->nextSequenceToken)) {\n            $responseModel = $this->client->describeLogStreams([\n                'logGroupName' => $this->logGroup,\n                'logStreamNamePrefix' => $this->logStream,\n            ]);\n            $this->nextSequenceToken = $responseModel['logStreams']['0']['uploadSequenceToken'];\n        }\n        try {\n            for($i=0; $i<10; $i++) {\n                $this->client->putLogEventsAsync([\n                    'logGroupName' => $this->logGroup,\n                    'logStreamName' => $this->logStream,\n                    'logEvents' => [\n                        [\n                            'message' => $line,\n                            'timestamp' => $microTime,\n                        ]\n                    ],\n                    'sequenceToken' => $this->nextSequenceToken,\n                ]);\n            }\n        } catch (CloudWatchLogsException $e) {\n            // We don't want any exception when putLogEvents is not working\n            $e->getMessage().'\\n';\n        }\n        $this->loop->run();\n    }\n. ",
    "kratosvn": "Thank you for reply, \nand here is code i using to send mail (i using Laravel):\n1. I add command send mail to queue\n   foreach ($customer_list as $customer) {\n          /* create an email here*/\n         //add command send mail to queue from 1 to 10\n         if ($count_queue == 10) {\n             $count_queue = 0;\n         }\n         if ($count_queue == 0) {\n             try {\n                 $q = Queue::pushOn(QUEUE_EMAIL, new \\App\\Jobs\\SendEmailSes($customer->email, [], $email->title, $body, $idc, $customer->id, $email->id, $key_message), '');\n               } catch (Exception $ex) {\n               debugLog('ex: ' . $ex);\n           }\n           } else {\n           try {\n               $q = Queue::pushOn(QUEUE_EMAIL . '_' . $count_queue, new \\App\\Jobs\\SendEmailSes($customer->email, [], $email->title, $body, $idc, $customer->id, $email->id, $key_message), '');\n               } catch (Exception $ex) {\n               debugLog('ex: ' . $ex);\n           }\n       }\n       $count_queue += 1;\n   }\n   2.Queue config\n   'sqs' => [\n           'driver' => 'sqs',\n           'key' => env('AWS_ACCESS_KEY_ID'),\n           'secret' => env('AWS_SECRET_ACCESS_KEY'),\n           'queue' => 'https://sqs.ap-southeast-1.amazonaws.com/xxx/xxx',\n           'region' => 'ap-southeast-1',\n       ],\n   3. I using Supervisor to run queue\n   you can see in here.\nThank you.\n. ",
    "webexpert4rv": "I am also getting this error, not sure what is causing it. And most of the time it would work, I am just worried if it might be silently failing in some cases, which I am not getting information of. I am also using Laravel \naws/aws-sdk-php                       3.20.12   \nexception 'Aws\\Sqs\\Exception\\SqsException' with message 'Error executing \"ReceiveMessage\" on \"https://sqs.ap-southeast-1.amazonaws.com/***/default\"; AWS HTTP error: Error creating resource: [message] fopen(): SSL: crypto enabling timeout\n[file] /var/www/nrgedge/vendor/guzzlehttp/guzzle/src/Handler/StreamHandler.php\n[line] 312\n[message] fopen(): Failed to enable crypto\n[file] /var/www/nrgedge/vendor/guzzlehttp/guzzle/src/Handler/StreamHandler.php\n[line] 312. ",
    "stueynet": "Same. Happens really infrequently. Here is mine:\nAws\\Sqs\\Exception\\SqsException \u00b7 Error executing \"ReceiveMessage\" on \"https://sqs.us-west-2.amazonaws.com/***/***\"; AWS HTTP error: cURL error 6: Could not resolve host: sqs.us-west-2.amazonaws.com (see http://curl.haxx.se/libcurl/c/libcurl-errors.html). ",
    "bisw": "Getting somehow same problem randomly.\nexecuting \"CreateQueue\" on \"https://sqs.ap-south-1.amazonaws.com\"; AWS HTTP error: Error creating resource: [message] fopen(): SSL: Handshake timed out\n[file] /var/www/html/gearman/vendor/guzzlehttp/guzzle/src/Handler/StreamHandler.php\n[line] 323\n[message] fopen(): Failed to enable crypto\n[file] /var/www/html/gearman/vendor/guzzlehttp/guzzle/src/Handler/StreamHandler.php\n[line] 323\n[message] fopen(https://sqs.ap-south-1.amazonaws.com): failed to open stream: operation failed\n[file] /var/www/html/gearman/vendor/guzzlehttp/guzzle/src/Handler/StreamHandler.php\n[line] 323. ",
    "marcos-guerrero": "The same random error happens here, this time with EC2 \"DescribeTags\"\nError executing \"DescribeTags\" on \"https://ec2.eu-west-1.amazonaws.com\"; AWS HTTP error: Error creating resource: [message] fopen(): SSL: Handshake timed out [file] C:\\Users\\Public\\PHPScripts\\clases\\3rdparty\\aws-php-sdk\\vendor\\guzzlehttp\\guzzle\\src\\Handler\\StreamHandler.php\n[line] 323\n[message] fopen(): Failed to enable crypto [file] C:\\Users\\Public\\PHPScripts\\clases\\3rdparty\\aws-php-sdk\\vendor\\guzzlehttp\\guzzle\\src\\Handler\\StreamHandler.php\n[line] 323\n[message] fopen(https://ec2.eu-west-1.amazonaws.com): failed to open stream: operation failed [file] C:\\Users\\Public\\PHPScripts\\clases\\3rdparty\\aws-php-sdk\\vendor\\guzzlehttp\\guzzle\\src\\Handler\\StreamHandler.php\n[line] 323\n. ",
    "tnijboer": "@cjyclaire thanks for your reply.\nI will try the createCredentials instead.\nWhat I mean with the session token, is that the assumeRole from the cli is providing a \"SessionToken\".\nAnd you need it to pass trough the api to use the temporary credentials.\n\nSessionToken  -> (string)\nThe token that users must pass to the service API to use the temporary credentials. \n\nBut I see the createCredentials is setting them as well. \nWill let you know\n. That did the trick.\nTicket can be closed\n. ",
    "siwinski": "THANKS @cjyclaire \n. @kstich I see where the count() fix could be applied in the step definition, but would you want it there or \"higher up\" in the response?  If in the response, where are Failed and Successful set?. > @siwinski The change should be local to the step. We can supply the fix and run the integration tests if you'd rather we handle it.\n@kstich Sorry, I got busy with other things and never got to this.  I see you did in #1318 though.  Thanks!. ",
    "praditha": "@cjyclaire , I mean I wanna change the https://s3.us.amazonaws.com/s3://test to my CEPH storage e.g. https://mycephstorage.com/s3://test\n. ",
    "peterkomar": "Thanks for explanations \n. ",
    "nicolae-stelian": "Hi @cjyclaire.\nI don't use aws-sdk-php-zf2  ( I use symfony and not zend framework). I install the library using composer in an symfony 3.1 project:\ncomposer.phar require aws/aws-sdk-php\nThis is the code that generate the error https://github.com/nicolae-stelian/sf-sqs/blob/master/src/AppBundle/Command/SqsCommand.php\nMy php version is:\nPHP 7.0.11-1+deb.sury.org~trusty+1 (cli) ( NTS )\nCopyright (c) 1997-2016 The PHP Group\nZend Engine v3.0.0, Copyright (c) 1998-2016 Zend Technologies\n    with Zend OPcache v7.0.11-1+deb.sury.org~trusty+1, Copyright (c) 1999-2016, by Zend Technologies\n    with Xdebug v2.4.0RC2, Copyright (c) 2002-2015, by Derick Rethans\nMy linux distribution is:\n```\nDISTRIB_ID=LinuxMint\nDISTRIB_RELEASE=17.3\nDISTRIB_CODENAME=rosa\nDISTRIB_DESCRIPTION=\"Linux Mint 17.3 Rosa\"\nNAME=\"Ubuntu\"\nVERSION=\"14.04.5 LTS, Trusty Tahr\"\nID=ubuntu\nID_LIKE=debian\nPRETTY_NAME=\"Ubuntu 14.04.5 LTS\"\nVERSION_ID=\"14.04\"\nHOME_URL=\"http://www.ubuntu.com/\"\nSUPPORT_URL=\"http://help.ubuntu.com/\"\nBUG_REPORT_URL=\"http://bugs.launchpad.net/ubuntu/\"\n```\n. @cjyclaire I test and record the error. Please take a look https://youtu.be/VaXlLOoBVjA .\nI will investigate more deeply this error.  I will check my environment and I will try to reproduce error in a virtual machine. After this I will send you more details. \n. @cjyclaire apology for answering late. The problem is the xdebug extension. I don't know if is really a problem of sdk or xdebug. (In production environment nobody use xdebug). If you want to see my tests: https://youtu.be/yPdYWSqV6pE . \nThank you very much for your time.\n. ",
    "dovanmanh080485": "Hello @cjyclaire,\nfunction utf8_string($string){\n    $string = mb_convert_encoding($string, \"UTF-8\");\n    return preg_replace(\n            array(\n                    '/\\x00/', '/\\x01/', '/\\x02/', '/\\x03/', '/\\x04/',\n                    '/\\x05/', '/\\x06/', '/\\x07/', '/\\x08/', '/\\x09/', '/\\x0A/',\n                    '/\\x0B/','/\\x0C/','/\\x0D/', '/\\x0E/', '/\\x0F/', '/\\x10/', '/\\x11/',\n                    '/\\x12/','/\\x13/','/\\x14/','/\\x15/', '/\\x16/', '/\\x17/', '/\\x18/',\n                    '/\\x19/','/\\x1A/','/\\x1B/','/\\x1C/','/\\x1D/', '/\\x1E/', '/\\x1F/'\n            ),\n            array(\n                    \"\\u0000\", \"\\u0001\", \"\\u0002\", \"\\u0003\", \"\\u0004\",\n                    \"\\u0005\", \"\\u0006\", \"\\u0007\", \"\\u0008\", \"\\u0009\", \"\\u000A\",\n                    \"\\u000B\", \"\\u000C\", \"\\u000D\", \"\\u000E\", \"\\u000F\", \"\\u0010\", \"\\u0011\",\n                    \"\\u0012\", \"\\u0013\", \"\\u0014\", \"\\u0015\", \"\\u0016\", \"\\u0017\", \"\\u0018\",\n                    \"\\u0019\", \"\\u001A\", \"\\u001B\", \"\\u001C\", \"\\u001D\", \"\\u001E\", \"\\u001F\"\n            ),\n            $string\n            );\n}\nThis's my code to remove all invalid utf-8 character. Could you help me to modify this function ?\nThank you !\n. @cjyclaire Hello,\nreturn preg_replace('/[\\x{0009}\\x{000a}\\x{000d}\\x{0020}-\\x{D7FF}\\x{E000}-\\x{FFFD}]/u','',$string);\nRthinks it doesn't work, it return  NULL string.\n. ",
    "scottopolis": "Yes I was able to use http before successfully, this has only been in the last week or so that it stopped working.\n. The SDK files were version 3.19.4, I also just updated to the latest and it didn't make a difference.  My API calls all use 'latest' for version.\n. Everything works fine if I use https, so I guess I'll just do that.\nMy only question would be: is it possible to make API calls without https, like from a development site? Is there a setting to allow that?\nThanks\n. I'll take a look at that, thanks for your help!\n. ",
    "umesecke": "@cjyclaire Ah, thanks for the hint on fread's behaviour! I wasn't aware of this.\nSo it makes sense to treat all non-plainfile streams as non-seekable and reading them from start to end because neither does Psr7\\LimitStream::read() enforce the $length parameter nor does StreamInterface::read() says anything about that.\nBut of course we could easily create such an implementation of a limiting stream that enforces the limit in read(). Worst case scenario the underlying stream has to fetch up to one chunk of data again. If the implementation is seekable it should be able to handle this.\nI can understand that there may be performance implications for some implementations if reading is not exactly aligned to chunk size. But in my case I would gladly trade those off for not having to buffer gigabytes of data to disk.\nPerhaps we could add an option that enables those extra limits and in return rely fully on the stream's seeking ability without buffering to disk. Maybe wrapping the stream into a stream decorator that acts as a marker for me telling the sdk that I have understood the possible implications and that I am able to handle that. ;)\n. @cjyclaire sounds great! thanks\n. I also did some further investigation and realized that the buffering is not done for the whole stream upfront but only just for the parts that are going to be uploaded next (I had to catch up with generator functions in PHP). So most of my points from the beginning seem to be obsolete. As the temporary file should be closed (and deleted) by GC after the upload of a part is finished the impact on disk size should be minimal (partsize x concurrent uploads). In that case the benefits of preloading the parts IMHO outweight the downsides by far.\nSo for my use-case the only thing left is the case of resuming a failed upload. As the stream is not handled as seekable, the data of the already successfully uploaded parts is read again from the stream to advance the offset to the beginning of the part to be re-uploaded (see getUploadCommands() in AbstractUploader).\nI can see the possible problems when the amount of data puffered for a part does not equal the part size (fread reading up to a chunksize of data) vs. skipping the part by the exact part size. Maybe we can track the offsets of the already uploaded parts in the upload state so that even if the offsets are somewhat skewed we are still able to seek to the beginning of the next part.\n. ",
    "remipelhate": "Hi @cjyclaire! Thanks for responding this quickly :)! \nI'm using version 3.18.30 of the PHP SDK. I'm not using the aws-sdk-php-laravel package, as it has very little value for what we actually need. We're not using facades, nor do we manually resolve bindings from the IoC container. That's why we manually bind the DynamoDbClient through our own ServiceProvider. I did try using aws-sdk-php-laravel to see if it fixed the issue, but it didn't... I also tried running DynamoDb on different ports, but I always get the same result.\nI know it's very unlikely that this is an SDK issue. I'm just hoping you guys might have an idea what I may be doing wrong. I really have no clue at all.\n. Alright, I updated the config to timeout after 1s without retries.\n\n\nconfig/services.php\nphp\n'aws' => [\n    'dynamodb' => [\n        'credentials' => [\n            'key' => env('AWS_ACCESS_KEY_ID'),\n            'secret' => env('AWS_SECRET_ACCESS_KEY'),\n        ],\n        'region' => env('AWS_REGION'),\n        'version' => env('AWS_VERSION', 'latest'),\n        'endpoint' => env('AWS_ENDPOINT', 'http://127.0.0.1:8000'),\n        'debug' => env('APP_DEBUG', false),\n        'retries' => 0,\n        'http' => [\n            'timeout' => 1,\n        ],\n    ],\n],\n\nThis results in:\n```\n-> Entering step sign, name 'signer'\n\nrequest.instance changed from 0000000030b94d34000000001acbd48c to 0000000030b94d58000000001acbd48c\n  request.headers.X-Amz-Date was set to array(1) {\n    [0]=>\n    string(16) \"20161012T073114Z\"\n  }\nrequest.headers.Authorization was set to array(1) {\n    [0]=>\n    string(231) \"AWS4-HMAC-SHA256 Credential=[KEY]/20161012/eu-west-1/dynamodb/aws4_request, SignedHeaders=aws-sdk-invocation-id;host;x-amz-date;x-amz-target, Signature=[SIGNATURE]\n  }\n\nRebuilt URL to: http://127.0.0.1:8000/\nHostname was NOT found in DNS cache\nTrying 127.0.0.1...\n\nConnected to 127.0.0.1 (127.0.0.1) port 8000 (#0)\n\nPOST / HTTP/1.1\nHost: 127.0.0.1:8000\nX-Amz-Target: DynamoDB_20120810.CreateTable\nContent-Type: application/x-amz-json-1.0\naws-sdk-invocation-id: eda99d5d6198b4e4e2f47d4e577d2dae\nX-Amz-Date: 20161012T073114Z\nAuthorization: AWS4-HMAC-SHA256 Credential=[KEY]/20161012/eu-west-1/dynamodb/aws4_request, SignedHeaders=aws-sdk-invocation-id;host;x-amz-date;x-amz-target, Signature=[SIGNATURE]\nUser-Agent: aws-sdk-php/3.18.30 GuzzleHttp/6.2.1 curl/7.35.0 PHP/5.5.9-1ubuntu4.17\nContent-Length: 327\n\n\n\nupload completely sent off: 327 out of 327 bytes\n\nOperation timed out after 1004 milliseconds with 0 bytes received\nClosing connection 0\n```\n\nand eventually throws:\n[Aws\\DynamoDb\\Exception\\DynamoDbException]                                                                                                                                                                          \nError executing \"CreateTable\" on \"http://127.0.0.1:8000\"; AWS HTTP error: cURL error 28: Operation timed out after 1004 milliseconds with 0 bytes received (see http://curl.haxx.se/libcurl/c/libcurl-errors.html)\n[GuzzleHttp\\Exception\\ConnectException]                                                                                                   \ncURL error 28: Operation timed out after 1004 milliseconds with 0 bytes received (see http://curl.haxx.se/libcurl/c/libcurl-errors.html)\nThe same happens when I increase the timeout value and/or the amount of retries...\n. Yes I see that, but I it eventually connects, right (Connected to 127.0.0.1 (127.0.0.1) port 8000 (#0))? Also, when I set credentials to false in the config, I do get a 400 response:\n[Aws\\DynamoDb\\Exception\\DynamoDbException]                                                                                                                 \n  Error executing \"CreateTable\" on \"http://127.0.0.1:8000\"; AWS HTTP error: Client error: `POST http://127.0.0.1:8000` resulted in a `400 Bad Request` resp  \n  onse:                                                                                                                                                      \n  {\"__type\":\"com.amazonaws.dynamodb.v20120810#MissingAuthenticationToken\",\"message\":\"Request must contain either a valid ( (truncated...)                    \n   MissingAuthenticationToken (client): Request must contain either a valid (registered) AWS access key ID or X.509 certificate. - {\"__type\":\"com.amazonaws  \n  .dynamodb.v20120810#MissingAuthenticationToken\",\"message\":\"Request must contain either a valid (registered) AWS access key ID or X.509 certificate.\"}\nSo it really looks like it was able to connect to DynamoDb local... And since I can use the web shell, I presume it works well on that port as well.\n. @cjyclaire Ok, apparently @Ocramius has similar issues (on alpine). He installed DynamoDb Local using Docker (https://hub.docker.com/r/tray/dynamodb-local/), which now works fine. I did so too (keeping the same configuration/setup as described above) and as a matter of fact, it all magically works. So it looks like there may be something wrong with the latest version of DynamoDb Local..?\n. Alright, makes sense. It does indeed rather belong a the repository offering the ability to build DynamoDB queries. That's in fact the reason I needed it in the first place. I developed a DynamoDB QueryBuilder that automatically generates a placeholder for an attribute name when it's a reserved word. Thanks for the insights guys!\n. ",
    "vrkansagara": "I have double checked with the api and debug log still getting the same error and there is no proxy issue as far as I have already concern this issue with my head and he is already double checked with it. \nThe same cli works fine with the random id and layer also in same machine.\n\nMay I ask are you suggesting those errors suddenly happened with Opsworks only?\n\nYes , This issue occurs only with OpsWorks.\n. oh yes it's working find but don't know why still there is empty replay from the server even if there is stack and layers are there.\n. @cjyclaire  It's working fine. Here is little suggestion from my side.\nWe require to push some message in exception using that correct your endpoint or say something use http or https so developer  quickly got notified there is something wrong with code base.\nAlso if later on endpoint changed it will also see documentation and correct it.\nIt will also helpfull for a developer to save 24 to 36 hours from the community support.\nmy suggestion as bellow\nif there is wrong with code base  the response from aws-sdk would be\n```\n{\n.....\n'ref-link': 'https://docs.aws.amazon.com/general/latest/gr/rande.html'\n....\n}\n```\nI am having too much load with work otherwise I will generate PR against this issue and improve it.\nif you need more suggestion let me ping thanks :+1:  @cjyclaire \n. ",
    "sm2017": "Before asking question , I change handler in this way but Async methods always return \\GuzzleHttp\\Promise\\Promise (Also some errors ocuured)\nI extends \\GuzzleHttp\\Handler\\CurlMultiHandler and override __invoke\n``` php\n    public function __invoke(RequestInterface $request, array $options)\n    {\n        $easy = $this->factory->create($request, $options);\n        $id = (int) $easy->handle;\n    $promise = new \\React\\Promise\\Promise(\n        [$this, 'execute'],\n        function () use ($id) { return $this->cancel($id); }\n    );\n\n    $this->addRequest(['easy' => $easy, 'deferred' => $promise]);\n\n    return $promise;\n}\n\n```\nCan you show me how can I change handler for \\Aws\\S3\\S3Client to do this?\n. I try react-guzzle-psr7 but any Async method returns GuzzleHttp\\Promise\\Promise\nfor example there is no done() function\nIf you run \n``` php\n<?php\nrequire 'vendor' . DIRECTORY_SEPARATOR . 'autoload.php';\n$loop = \\React\\EventLoop\\Factory::create();\n$handler = new \\WyriHaximus\\React\\GuzzlePsr7\\HttpClientAdapter($loop);\n$client = new \\GuzzleHttp\\Client([\n    'handler' => \\GuzzleHttp\\HandlerStack::create($handler),\n]);\n$client->getAsync('http://github.com/')->done(function ($response) {\n    var_export($response);\n    var_export((string) $response->getBody());\n});\n$loop->run();\n```\nthis error will be occurred\nFatal error: Call to undefined method GuzzleHttp\\Promise\\Promise::done() in C:\\Users\\WebMaster\\Desktop\\S3\\react-guzzle-psr7.php on line 12\nAs you can see the returned value of Async method is  GuzzleHttp\\Promise\\Promise Not ReactPHP Promise\nCan you show me how is possible in this example to convert 404 error to uncaught exception?\n. May be I'm in wrong \nLet me ask a question relative to my problem , this is an example \n``` php\nuse Aws\\Exception\\AwsException;\n$promise = $client->listTablesAsync();\ntry {\n    $result = $promise->wait();\n} catch (AwsException $e) {\n    // handle the error.\n}\n```\nIf I use Event Loop for this example I never can use $promise->wait() so try and catch is not meaning , Can you convert this example to use event loop and try catch for promise?\n. Thanks \nBut I want to know , how can I get total free disk space of bucket in php??\nAssume I have a bucket with 100GB total space and 10GB used space . so I have 90 GB free space in the bucket. Is there any way to get free space in php??. OK , thanks a lot. I found answer of question #1 here\n\nhttps://github.com/aws/aws-sdk-php/blob/33e8f7083fcb54fd926a8569cb81a0795e8c2633/docs/guide/configuration.rst#http\n\nBut I have an issue\nWhen I send connect_timeout=5 , I see the error after 5 sec , when I set connect_timeout=0.1 , I see error after 0.1 sec but when I set connect_timeout=1 , it dont workes and I see error after 5 sec , why?\n4- Whats is diffrence betwean connect_timeout and timeout exactly?\n. > 3. I am not sure why would that happen, can you please share the logs and the error message you get\nI don't try my self , I just ask a question\n5- When I send connect_timeout=5 , I see the error after 5 sec , when I set connect_timeout=0.1 , I see error after 0.1 sec but when I set connect_timeout=1 , it dont workes and I see error after 5 sec , why?\n6- Accodring to aws guide For connect_timeout and  timeout\n\nUse 60 to wait indefinitely (the default behavior).\n\nAccodring to CURLOPT_TIMEOUT and CURLOPT_CONNECTTIMEOUT\nFor connect_timeout \n\nSet to zero to switch to the default built-in connection timeout - 300 seconds\n\nFor timeout\n\nDefault timeout is 0 (zero) which means it never times out during transfer.\n\nSo , Which is correct for  aws-sdk-php and S3Client?\n. Run the following code\n```php\n<?php\nrequire 'vendor/autoload.php';\n$sdk = new Aws\\Sdk([\n    'version' => 'latest',\n    'region'  => 'us-west-2'\n]);\n$s3 = $sdk->createS3();\n$promise = $s3->getObjectAsync([\n    'Bucket' => 'bucket',\n    'Key' => 'memoryLeakage',\n]);\n$promise->wait();\necho \"Before gc_collect_cycles()\\n\\n\\n\";\ngc_collect_cycles();\necho \"\\n\\n\\nAfter gc_collect_cycles()\";\n```\nPlease add __destruct method to these classes\nGuzzleHttp\\Psr7\\Request\nGuzzleHttp\\Psr7\\Stream\nGuzzleHttp\\Psr7\\Uri\nAws\\RetryMiddleware\nAws\\S3\\PutObjectUrlMiddleware\nAws\\S3\\PermanentRedirectMiddleware\nAws\\Command\nAws\\HandlerList\n\nphp\npublic function  __destruct(){\n    echo __METHOD__.\"\\n\";\n}\nWith no memory leakage you must not see anything between Before gc_collect_cycles() and After gc_collect_cycles()\nAs my script is long execute , I see that these objects never free up memory even after days. The best way to free up memory is assigning null to variable when its garbage\nFor anonymous function remember this note\nRun this code \n```php\nclass A{\n    public $a;\nfunction __destruct(){\n    echo \"This object(A) removed from memory<br/>\\n\";\n}\n\n}\nclass B{\n    public $B;\nfunction __destruct(){\n    echo \"This object(B) removed from memory<br/>\\n\";\n}\n\n}\n$a = new A;\n$a->a = function()use($a){\n    //unset($a);\n};\n$b = new B;\n$b->b = $a;\nunset($a,$b);\necho \"You think both \\$a and \\$b are freed from memory But \\$a is in memory\\n\";\n````\nSo it is safe to use unset inside anonymous function\nAs you see object of class A is in memory and destructed in shutdown. How many times it keep alive connection? 10 sec ? Minutes?. > Is there anything consistent about the files or timing (same file(s) over time, same size files, etc.)?\nNo , there is no pattern\n\nWhat happens if you try to re-upload one of the double size files? \n\nUploaded correctly , No double size\n\nIs this only files uploaded with 'rb' or does it occur with files uploaded with 'r' as well?\n\nAs reproduction is hard , I don't know , But I never use 'r' mode , I just use 'rb' mode. @kstich I try to reproduce issue , wait 24 hours for my reply. @kstich When I use cURL it returns true and when stream it returns false and getHandlerContext is empty array , I NEED TO USE STREAM. @kstich , I have a file located on multiple S3 servers\nI want to create a script that detect server/network failure , When you request a file that located on server A , B and C , the script randomly pick a server for example B , the if B cannot serve file for any reason (Network failure , down time , ...) switch to server A\nHow can I detect it ? it seems  isConnectionError is not my response. @kstich sorry . I make a mistake , The following code can reproduce issue\nphp\nini_set('default_socket_timeout', 1);\n$start = microtime(true);\ntry {\n    $s3 = new \\Aws\\S3\\S3Client([\n        'region' => 'us-east-1',\n        'version' => 'latest',\n        'endpoint' => 'https://10.10.10.100',\n//        'credentials' => [\n//            'key' => 'a',\n//            'secret' => 'b'\n//        ],\n        'http' => [\n            'stream' => true,\n            'synchronous' => true,\n        ]\n    ]);\n    $result = $s3->getObject(array(\n        'Bucket' => 'a',\n        'Key' => 'b'\n    ));\n} catch (Exception $e) {\n    echo $e->getMessage().\"\\n\";\n}\nIf you execute the above mentioned code , execute time is about 1 second , and $e->getMessage() is \n\nError retrieving credentials from the instance profile metadata server. (cURL error 28: Connection timed out after 1000 milliseconds (see http://curl.haxx.se/libcurl/c/libcurl-errors.html))\n\nBut when you uncommend credentials part , execute time is about 4 seconds while we want get timeout exception after 1 second , in this case $e->getMessage() is \n\nError executing \"GetObject\" on \"https://10.10.10.100/a/b\"; AWS HTTP error: Error creating resource: [message] fopen(https://10.10.10.100/a/b): failed to open stream: Connection timed out\n[file] vendor/guzzlehttp/guzzle/src/Handler/StreamHandler.php\n[line] 324\n\nAs you know fopen timeout must be equal todefault_socket_timeout. ",
    "WilliamMayor": "Hi @cjyclaire, thanks for the response.\nI was under the impression that you could only send 1000 objects at a time to the deleteObjects endpoint. That's why I had the chunking. If I read it right your code sends an array of arrays of 1000 keys to the function. Does that do the queued calls for me?\nI'll check the spurious leading slash in our code but I think we're doing the right thing. We have a global path prefix that I omitted in the example code. So the actual prefix would look like $PREFIX/parent-folder/. If we're doing this incorrectly, wouldn't we see a 404 instead of a malformed XML error?\nFinally (and thanks again), is the suggestion to change the keys of the objects in S3 or to URL encode the keys in the delete request? We have a feature that allows public upload of files so we cannot guarantee that the file names would conform to any rules we set. We discovered this bug after a penetration test discovered some mishandling of special characters on our end. We're trying to write some tests for these cases so we avoid regression.\n. ",
    "cpinto": "Thanks for sharing the debug option @cjyclaire. As I turned it on, it revealed a trace of the HTTP connection. \nSearching for one of the errors yielded some guidance on disabling a fork verification for NSS, as per https://developer.mozilla.org/en-US/docs/Mozilla/Projects/NSS/Reference/NSS_environment_variables. \nThe relevant command is: export NSS_STRICT_NOFORK=DISABLED which can be set up as an environment variable for EB easily. \nOnce this is done, no more errors occur. That said, Mozilla clearly states it's an error to do this but I couldn't find an explanation as to why, which makes me a bit uneasy. However, given that this is the best way forward (I couldn't find an updated libcurl that works with Amazon Linux AMI and compiling the source code to build a custom package feels as it could break at any time) I'll be using it.\nThanks again for sharing the instructions to turn on the HTTP debug.\n. ",
    "SamuelNorbury": "@cpinto I recently had a similar problem, specifically querying an S3 bucket twice in succession from a Laravel application, and your fix seems to work, though I'm not feeling overly confident about it.\n. ",
    "cwhittl": "I just used to, scary fix but it works...  Thank you!. ",
    "yuloh": "Hi,\nI have also run into this error.  I don't think there is really anything the SDK can do to prevent it but I figured I should share what I learned in case anyone else has this issue.\nThe issue seems to be caused by a combination of compiling the PHP curl extension against a version of curl that uses NSS instead of OpenSSL or LibreSSl and sharing a curl handle between a parent process and a child process forked using the pcntl extension.\nHere is a minimal reproduction script:\n```php\n<?php\n$ch = curl_init();\ncurl_setopt($ch, CURLOPT_URL, \"https://www.example.com/\");\ncurl_exec($ch);\nvar_dump(curl_error($ch));\n$pid = pcntl_fork();\nif ($pid == -1) {\n     die('could not fork');\n} else if ($pid) {\n     // we are the parent\n     pcntl_wait($status); //Protect against Zombie children\n} else {\n     // we are the child\n    curl_exec($ch);\n    var_dump(curl_error($ch));\n}\n```\nIf you run this script you should see:\nstring(0) \"\"\nstring(63) \"Bulk data encryption algorithm failed in selected cipher suite.\"\nThe error seems to be specifically caused by calling curl_exec($ch) in both the parent and the child process.  If you only call curl_exec in either the parent or the child you will not encounter an error.\nThe workarounds that seem to work are:\n1.) Set the environment variable NSS_STRICT_NOFORK to DISABLED.  You probably shouldn't do this, I'm guessing they added that protection for a reason.\n2.) Compile curl with a different TLS backend.  This is explained here.\n3.) Don't ever call curl_exec in a forked process.  You probably can't always avoid this, but sometimes it's a decent fix.  For example, if using php artisan tinker in Laravel you can disable forking to prevent this.\nUnfortunately there doesn't seem to be a way to force the PKCS#11 crypto module to re-initialize.  Even if you call curl_close on the handle in the parent process it seems to re-use the crypto module.  For example, this code does not work:\n```php\n<?php\n$ch = curl_init();\ncurl_setopt($ch, CURLOPT_URL, \"https://www.example.com/\");\ncurl_exec($ch);\nvar_dump(curl_error($ch));\ncurl_close($ch);\n$pid = pcntl_fork();\nif ($pid == -1) {\n     die('could not fork');\n} else if ($pid) {\n     // we are the parent\n     pcntl_wait($status); //Protect against Zombie children\n} else {\n     // we are the child\n    $ch = curl_init();\n    curl_setopt($ch, CURLOPT_URL, \"https://www.example.com/\");\n    curl_exec($ch);\n    var_dump(curl_error($ch));\n    curl_close($ch);\n}\n``. @jonmcclung any option besidesnss` should work.  The default instructions in the link above should work fine.\nI don't think the AWS SDK cares which ssl backend curl uses.  It should work just fine with any of them.  I've personally confirmed that it works fine with LibreSSL.. ",
    "jonmcclung": "@yuloh So, instead of compiling with --without-ssl --with-nss, we should compile with which option? Does anyone know which TLS the AWS SDK expects?. @yuloh Thanks! I can confirm that normal OpenSSL seems to solve the problem as well!. ",
    "skovmand": "I have just seen this in the docs. So closing issue again: https://github.com/aws/aws-sdk-php/blob/master/docs/guide/configuration.rst#scheme\n. ",
    "simshaun": "@cjyclaire \nIt looks like I'm an idiot and didn't notice I was looking at v2 docs at the time.\n- http://docs.aws.amazon.com/aws-sdk-php/v2/api/class-Aws.S3.S3Client.html#_getObjectUrl\n- http://docs.aws.amazon.com/aws-sdk-php/v2/guide/service-s3.html#creating-a-pre-signed-url\nAs far as the use-case, I'm integrating some legacy services that use the CNAME-style into a newer app which uses v3. The few clients still using the old system are being fed data that contains those URLs. My fear is that if I start sending a different URL format, stuff on their side might break. It's just easier for me to keep using the CNAME-style than try to coordinate with these companies. If I need to I'll just hard-code the endpoint in these special cases.\n. Thanks\n. ",
    "jbruni": "I am also having troubles related to encoding of file names... I am using the stream wrapper, and the same code works for \"regular\" key names (ASCII chars), but fails for keys like Sele\u00e7\u00e3o_04.jpg.\nSomething \"URL encodes\" the key name and the result is an exception.. The relevant part of my code is as follows:\n```php\n    $s3Client = new \\Aws\\S3\\S3Client([\n        // settings here\n    ]);\n    $s3Client->registerStreamWrapper();\n$s3Prefix = 's3://' . $bucket . '/';\n$context = stream_context_create([\n    's3' => ['seekable' => true]\n]);\n\n$stream = fopen($s3Prefix . $key, 'r', false, $context);\n// read operations\nfclose($stream);\n\n``. To reproduce, I suggest uploading a file with such name (Sele\u00e7\u00e3o_01.png, for example), and then trying to access it using the code snippet above (providing proper settings, $bucket, and $key, of course). It is fairly simple.. Nevermind... in my case, the$keyI was passing was already \"url encoded\", so the issue was before hitting AWS SDK. And the solution was as easy as applying aurldecode` to it.. ",
    "cmpaul": "@cjyclaire Thanks for the quick response. The odd thing is that the deleteTable() call works fine and the table is deleted. I think it has something to do with the waitUntil() -- if I remove that, no segfault. I'll try disabling xDebug and see if that helps (environment is Ubuntu 14.04, PHP 5.5.9).. @cjyclaire Indeed, that's a typo in my original code snippet, I'm passing the 'TableName' in the options array.\nI really think the issue is with the waitUntil -- the stack trace I'm able to intercept indicates that it's trying to execute a DescribeTable (which doesn't work because the table was deleted). It may not be related to this waitUntil, though... I'm wondering if I have another Promise queued up in the Guzzle TaskQueue from a previous command.\nI'll post here with more updates once I'm able to confirm. Thanks again for the investigation!. @cjyclaire No luck in finding out the root cause. I'm still getting the error locally, though my code works fine without the waitUntil, so I'm just going to roll with that for now. Since you can't reproduce, I'd say it's probably related to my environment and you can go ahead and close this issue.. ",
    "vfuentesedcorp": "@cjyclaire I used it only to indicate that the files were public. Yes, I am transferring only public files. I'm going to try what you tell me.\nThanks, I'll tell you if it works.\n. @cjyclaire,  Do you tell me to use the option before to check if a file exists?\nImplement Transfer:\n...\n $urlDirectory=$this->getContainer()->get('kernel')->getRootDir() . '/../web/bundles';\n  $dest = 's3://'.$this->bucket.\"/\".$urlToSaveDirectory;\n        $manager = new Transfer($this->s3, $urlDirectory, $dest);\n        $result=$manager->transfer();\n...\nI do not quite understand how to use the option before for uploads from multiple directories.\nCould you guide me a little?\nThanks!\n. @cjyclaire \n- Thank you very much, but what I'm doing is transferring files from outside of S3 to S3\n- Used PutObject \n- Still taking the same time\n- I do not understand what I should include in the function ( if (in_array($command->getName(), ['PutObject'])) { )\n$manager = new Transfer($this->s3, $urlDirectory, $dest, [\n                'before' => function (Command $command) {\n                    // Set custom cache-control metadata\n                   if (in_array($command->getName(), ['PutObject'])) {                        \n                     $command['CacheControl'] = 'max-age=3600';\n                      ....\n                    }\n                 },\n                'debug'=>true\n            ]);. @cjyclaire I tried what indicas me. Yet still it is taking too long, no improvement.\nI do not know what else to prove the truth, anyway the delay is not so long, but it would be nice to be able to optimize it and not always upload all the files.. @cjyclaire \nWe had a problem and I had to leave this aside. I tell you, it turns out that I had not tried running the command from the server, I was doing it from my local to S3, I tried it from server to S3 and it takes less than 5 seconds. So perfect! It worked with the configurations that you indicated to me, with Transfer and with uploadDirectory ().\nVery thankful.. ",
    "dittto": "@cjyclaire I've encountered an issue with this as well. We've tried running master with Guzzle 1.3.0 but still get the same error raised above.\nI broke apart the defaultProvider() and there seems to be an issue with chaining and memoizing together:\n```php\n$provider = Aws\\Credentials\\CredentialProvider::defaultProvider();\ncall_user_func( $provider )->wait(); // fails with \"The promise was rejected with reason: Invoking the wait callback did not resolve the promise\"\n$env_creds = Aws\\Credentials\\CredentialProvider::env();\ncall_user_func( $env_creds )->wait(); // fails with \"Could not find environment variable credentials in AWS_ACCESS_KEY_ID/AWS_SECRET_ACCESS_KEY\"\n$ini_creds = Aws\\Credentials\\CredentialProvider::ini();\ncall_user_func( $ini_creds )->wait(); // fails with \"Cannot read credentials from /.aws/credentials\"\n$ecs_creds = Aws\\Credentials\\CredentialProvider::ecsCredentials();\ncall_user_func( $ecs_creds )->wait(); // fails with \"Error retrieving credential from ECS (cURL error 28: Connection timed out after 1001 milliseconds...\"\n$instance_creds = Aws\\Credentials\\CredentialProvider::instanceProfile();\ncall_user_func( $instance_creds )->wait(); // returns valid credentials\n$chain = Aws\\Credentials\\CredentialProvider::chain( $env_creds, $ini_creds, $ecs_creds, $instance_creds );\ncall_user_func( $chain )->wait(); // returns valid credentials\n$simple_memo = Aws\\Credentials\\CredentialProvider::memoize( $instance_creds );\ncall_user_func( $simple_memo )->wait(); // returns valid credentials\n$chain_memo = Aws\\Credentials\\CredentialProvider::memoize( $chain );\ncall_user_func( $chain_memo )->wait(); // fails with \"The promise was rejected with reason: Invoking the wait callback did not resolve the promise\"\n. @cjyclaire we're running the following:\n`CentOS Linux release 7.2.1511 (Core)` with:\nLinux i-7d8f51bd 3.10.0-327.36.3.el7.x86_64 #1 SMP Mon Oct 24 16:09:20 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux\n```\nThe dependencies are the same version numbers as those you've listed, apart from my test was using aws-sdk-php on master.\nThe PHP version is:\nPHP 5.6.25 (cli) (built: Aug 26 2016 15:39:31)\nCopyright (c) 1997-2016 The PHP Group\nZend Engine v2.6.0, Copyright (c) 1998-2016 Zend Technologies\n    with Zend OPcache v7.0.6-dev, Copyright (c) 1999-2016, by Zend Technologies\nIf I get some time free then I may try creating a little project with just this bug in it, as currently it's part of a giant Wordpress-based project.. ",
    "llernestal": "Hi, I'm getting this issue when creating a pre-signed URL with Aws\\S3\\S3Client->createPresignedRequest()\n```\nUncaught GuzzleHttp\\Promise\\RejectionException: The promise was rejected with reason: Invoking the wait callback did not resolve the promise in /vendor/guzzlehttp/promises/src/functions.php:112\nStack trace:\n0 /vendor/guzzlehttp/promises/src/Promise.php(75): GuzzleHttp\\Promise\\exception_for('Invoking the wa...')\n1 /vendor/aws/aws-sdk-php/src/S3/S3Client.php(313): GuzzleHttp\\Promise\\Promise->wait()\n```\n```\naws/aws-sdk-php: 3.20.1\nguzzlehttp/promises: 1.3.0\nphp: 7.0.13\nuname -a\nLinux 48d6274ffe2f 4.4.0-47-generic #68-Ubuntu SMP Wed Oct 26 19:39:52 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux\n```\nI'm using CredentialProvider::defaultProvider(). When I explicitly set guzzlehttp/promises: 1.2.0 it works without a problem.. @cjyclaire I use the SDK directly, no xdebug enabled.\nLoaded PHP modules\n```\n[PHP Modules]\ncalendar\nCore\nctype\ncurl\ndate\ndom\nexif\nfileinfo\nfilter\nftp\ngettext\nhash\niconv\njson\nlibxml\nmemcache\nmysqli\nmysqlnd\nopenssl\npcntl\npcre\nPDO\npdo_mysql\npdo_sqlite\nPhar\nposix\nreadline\nReflection\nsession\nshmop\nSimpleXML\nsockets\nSPL\nsqlite3\nstandard\nsysvmsg\nsysvsem\nsysvshm\ntokenizer\nwddx\nxml\nxmlreader\nxmlwriter\nxsl\nZend OPcache\nzip\nzlib\n[Zend Modules]\nZend OPcache\n```. ",
    "Nizari": "We've had this issue as well! It still worked on version 3.19.21, but somewhere after that all our new servers crashed. As of today we still have this issue when we use the CredentialProvider::defaultProvider(). Hmmm, can't say for sure. What guzzle requirements does version 3.19.21 have in composer? If it's below 1.3.0, this could be related. Is anyone on this gullze issue?. ",
    "sagikazarmark": "I was able to reproduce the issue: https://github.com/reproduce/guzzle/tree/issue1668\nAfter a quick look in the code: the way how coroutines are handled has been changed in 1.3.0, which might be the guilty one.. Hi guys!\nWe are waiting for a final review before merging and tagging 1.3.1 with a fix. I will keep you posted.. https://github.com/guzzle/promises/releases/tag/v1.3.1 is out containing a fix (confirmed in the reproduction environment). ",
    "stavros-liaskos": "@cjyclaire It is working with an empty array but I was expecting to pass some actual arguments! No wonder I never found a definition for that.. To be accurate, I was expecting to pass in the array an Id of a user so I can get the statistics only for that user, but that's probably another issue.\nThanks for the quick reply . Actually I am really curious to know why set the argument of this function to an empty array. Why have an argument then in the first place?. ",
    "jaypea": "@cjyclaire thanks for the response and for fixing the root cause of my problem this fast.. ",
    "hilmanrdn": "i tried that, and still can visit the link after 5 seconds, i'm not sure if it has something todo, with default time setting between my local machine and server time. well if its work for others, than it's something wrong with me, i'll keep you update if i found the problem, thanks!. ",
    "useless-stuff": "Hello,\nWe are using the DynamoDB Session Handler in our PHP application and just noticed this same 1sec delay in every request to our application. Doing some digging we also found that the DynamoDB client when using the EC2 IAM role (instead of directly passing credentials in the config) was the culprit. We just moved our production infrastructure to hardcoded credentials but would like to move back to using the IAM role (for obvious security and simplicity) when fixed. Do you have any more information on this?\nThanks\nDiego. Is a bit complex make some tests currently, we are an e-commerce company and December is a nightmare ;)\nI'll let you know if I can try these workarounds.\nThank you so much for your answer.. Hello,\nthank you so much for your interest :)\nI did a bit of research and tests on my log files during the weekend and I've discovered that happens just with a specific request.\nThat request comes from a malicious bot who looking for vulnerabilities on the server, injects weird code in the user agent or inside the HTTP header fields.\nWe have some information about the user agent in the session and that drives crazy our application.\nSo, basically, it's not the SDK issue :)\nAgain\nThank you for your time and sorry for the useless post.. ",
    "saintberry": "Thanks @vasanthperiyasamy for the PR / fix.\n@cjyclaire do you have a rough timeframe on when this will get tagged? At work we're happy waiting on this fix before doing a deployment so we don't have to push out any workarounds.. ",
    "GCalmels": "Hey,\nThanks for the fast answer. I'm not using aws-php directly but with flysystem-aws-s3-v3\nThe \"@http\" is just an false annotation for Symfony and I have no clue about what I need to do to change this! . Ok, I will try on an other level! Thank you again. Hi @cjyclaire,\nWould it not be possible to change @http by @http on this line because it is just documentation and there is no influence for the code, I guess?\nAnd there are two other documentation like @http : @waiter on the same file : AwsClientInterface. Hi @cjyclaire,\nBecause Symfony think that @http is an annotation and it does not know this (there are not \"use .../http).... @cjyclaire Yes but it is not a problem in the code! It is only the problem with /** @http ... */ because Symfony sees it as annotation in the documentation but not in the code. Hi, I found my bug, in fact, it was Doctrine/Common/Annotation which does not like @http annotation. But it is possible to register false annotation on the autoload file!\nSorry! . ",
    "virgofx": "Any chance this can get merged? This is badly needed!. @cjyclaire Looking forward in getting this integrated. Any news?. This was throwing a CredentialsException instead of an AwsException if you do the following:\nUpdate local ~/.aws/config or ~/.aws/credentials to be invalid INI from PHP perspective but valid INI from other formats.\nBasically add a # as a comment in the beginning of a line in the credentials file. This will cause PHP to bug out when parsing (even though this is still valid INI) and then throw a different than expected AwsException as the message is related to the fallback provider (instance metadata) timing out.\nPerhaps need a higher level Exception or change of type?. ",
    "jjmontgo": "Hi cjyclaire:\nThanks for the response.\nI tried setting debug to true in the client on constructor but there was no new output.  \nI also tried downgrading guzzlehttp/promises to 1.2.0 and there was no change in the error message.\nWould really love to be able to execute upload files to S3 directly with form posts.  I've had problems with the guzzle library in the past and always ended up having to write my own functions with curl.\n. Yup!\n{\n    \"require\": {\n        \"aws/aws-sdk-php\": \"^3.20\"\n    }\n}\nI'm also wondering if it's just something else I'm doing wrong.  Maybe I don't have a security group set on S3 or something, and am just getting an unhelpful error message.... As an alternative I tried uploading the file directly to the instance and then using PutObject to send it to S3.\nI received an authentication error:\nFatal error: Uncaught exception 'Aws\\S3\\Exception\\S3Exception' with message 'Error executing \"PutObject\" on \"https://s3.amazonaws.com/elasticbeanstalk-us-east-1-853047570978/daybg-pdf.png\"; AWS HTTP error: Client error:PUT https://s3.amazonaws.com/elasticbeanstalk-us-east-1-853047570978/daybg-pdf.pngresulted in a403 Forbidden` response:\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\nAccessDeniedAccess Denied7F5EC1 (truncated...)\n AccessDenied (client): Access Denied - <?xml version=\"1.0\" encoding=\"UTF-8\"?>\nAccessDeniedAccess Denied7F5EC1EEA8772404f1qjomNtlVuo0nFxN/f6fPdJ3uD5d2Vf4GX5iG8x1oY4oHc3D+HjfASo/WHKYrZK+jrEyMGAMCU='\nGuzzleHttp\\Exception\\ClientException: Client error: PUT https://s3.amazonaws.com/elasticbeanstalk-us-east-1-853047570978/daybg-pdf.png resulted in a 403 Forbidden response:\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\nAccessDeniedAccess Denied in /var/app/current/vendor/aws/aws-sdk-php/src/WrappedHttpHandler.php on line 192`\nDo you think this is causing the previous guzzle error?  Could you direct me to any resources for authorizing a beanstalk environment to an S3 bucket?  If I can figure out how to authorize myself to put objects into the bucket I'll see if the guzzle error goes away..\nThanks.. @cjyclaire Hmm! I'm going to have to read up on IAM.\nI attached an AmazonS3FullAccess policy to my beanstalk EC2 role.  Then I went back to the PutObject attempt and re-ran it.  It worked, and uploaded the file to the S3 bucket.\nSo I went back to my code example with PostObject, and got the same Guzzle error as before.  \nIn both cases I'm instantiating the client the same:\n$client = new Aws\\S3\\S3Client([\n    'region' => 'us-east-1',\n    'version' => 'latest',\n    'debug' => true,\n]);\nThe credentials would also be the same here.  I think it really is a problem with guzzle?. @cjyclaire YES!  That definitely helps.  I added another key/secret pair (I didn't keep the old secret), added it directly to S3Client, and it WORKS!  It created all the form parameters, I posted a file to S3, and it redirected back to the page.  Perfect!\nOf course I don't know why there's a guzzle issue, but I can check this off as working thanks to you.. Thanks for the reply @bakura10!\nI think I've left out too much detail in my original post.  Here is the $conditions array I was using:\n$options = [\n            ['acl' => 'public-read'],\n            ['bucket' => $this->bucketName],\n            ['starts-with', '$key', 'activity/'],\n        ];\nIt's pretty much the same as yours. The $formInputs I specified and passed into the third parameter of PostObjectV4.  I then pass them to my form (alongwith any other hidden inputs):\n$this->formInputs = $postObject->getFormInputs();\nI then print $this->formInputs out as hidden inputs in my form:\n(I've removed all the input values except for \"key\", for clarity)\n<input type=\"hidden\" name=\"acl\" value=\"\">\n            <input type=\"hidden\" name=\"key\" value=\"activity/{$filename}\">\n            <input type=\"hidden\" name=\"X-Amz-Credential\" value=\"\">\n            <input type=\"hidden\" name=\"X-Amz-Algorithm\" value=\"\">\n            <input type=\"hidden\" name=\"X-Amz-Date\" value=\"\">\n            <input type=\"hidden\" name=\"Policy\" value=\"\">\n            <input type=\"hidden\" name=\"X-Amz-Signature\" value=\"\">\nSo I've included the key with the right prefix in the form, alongwith the {$filename} variable.  \nIs there any reason activity/{$filename} is the resulting object name?  Am I perhaps putting the {$filename} variable in the wrong place?\nThanks for the help.. I'm doing a direct form post to S3, and S3 is redirecting me back to the form after it completes.\nThat seems to work fine, except for the name of the resulting object.\nI should also note that if I am uploading the file to the root folder of the bucket (without the /activity), the filename appears correct.\nIs there anything obviously wrong with the form?\n(input values removed for brevity and replaced the bucketname with <>)\n```\n\n\n\n\n\n\n\n\n\n            Choose an image to upload       \n\n\n```. Oh MY GOSH.\nIt's ${filename} not {$filename}. :)\nThanks @bakura10 \nI'm going to go hide in a hole of shame.... ",
    "mcblum": "@cjyclaire Sorry! Got swamped at work :) Here's the PHP function I'm using to generate the URL:\n```\n/\n     * Client requests a signed upload URL that's only valid\n     * for a minute.\n     \n     * @return string\n     /\n    public function initializeUpload( Request $request ) {\n        $s3 = new S3Client( [\n            'credentials' => [\n                'key'    => getenv( 'AWS_ACCESS_KEY_ID' ),\n                'secret' => getenv( 'AWS_SECRET_ACCESS_KEY' )\n            ],\n            'version'     => 'latest',\n            'region'      => 'us-east-1',\n        ] );\n    $key      = Auth::user()->client->slug . '/uploads/' . substr( md5( rand() ), 0, 64 ) . '/';\n    $full_key = $key . $request->filename;\n\n    $command = $s3->getCommand( 'PutObject', [\n        'ACL'    => 'public-read',\n        'Bucket' => getenv( 'AWS_BUCKET' ),\n        'Key'    => $full_key\n    ] );\n\n    $signedUrl = (string) $s3->createPresignedRequest( $command, '+1 minute' )->getUri();\n\n    return response()->json( [\n        'key' => $key,\n        'url' => $signedUrl\n    ], 200 );\n}\n\n```\nThen in the JS frontend I'm using ng-file-upload to actually carry out the upload. I've verified that it is submitting all of the correct fields. If I change the php-generated response for the one that I encoded myself, it works, but obviously that stuff can't be public.\nI also tried generating a policy and signature with V4PostObject or something like that, and that didn't work either.. ",
    "arosolino": "You are already using JavaScript to retrieve the credentials so they have already have been transferred through the clients connection. The best thing you could do is use a secure (https) connection.. ",
    "anthonybarsotti": "Can you elaborate on how to get the URL from listSubscriptionsByTopic? Currently that method is providing the SQS ARN as the endpoint property of the return value for that function, not the URL.. ",
    "dbrownxc": "Caveat.  I'n not using this project but I'm interested in the answer to OP question.  I am using the SDK for another language, but I'm using the ARN for some technologies (SNS) but the URL for this one (SQS).  Since we are trying to generalize and avoid confusion, we are trying to just use ARNs everywhere in our configs.  Is there a future proof method for converting the ARN to a URL, or is it subject to change?. Perfect, thanks!. ",
    "Bikeman868": "Thank you for posting this question. Most of the AWS APIs are very consistent in how you address entities, but SQS is very weak in this regard. The various API methods accept and return either ARNs, queue names or queue URLs seemingly in a random pattern that makes them very difficult to deal with. Eventually I created a data structure within my application that allows me efficiently convert between these three identifiers.\nIf someone from the SQS team is reading this, please sort out this mess. Thanks.. ",
    "mnfgul": "This class is available in another library. Would be great if it was specified in the documentation. . ",
    "novotnyj": "It seems that problem was on our site - upload was running in two processes simultaneously.. ",
    "samueleastdev": "Hi @imshashank,\nDoes.\n~~~\n$result = $client->getBucketPolicy(array(\n            // Bucket is required\n            'Bucket' => MYBUCKET\n        ));\nprint_r($result->toArray());\n~~~\nreturn your bucket policy can you try that on a bucket that has a policy attached it definitely doesn't work.. @imshashank please close it was my fault i was running json encode on a already encoded json.\nThis works.\n~~~\n$check = $client->doesBucketPolicyExist($Bucket);\n    if(isset($check)){\n\n        $result = $client->getBucketPolicy(array(\n            // Bucket is required\n            'Bucket' => $Bucket\n        ));\n\n        $policy = $result->toArray();\n\n        echo json_encode(array(\n            'error' => false,\n            'message' => 'Successfully Returned',\n            'policy' => json_decode($policy['Policy'])\n        ));\n\n    }else{\n\n        echo json_encode(array(\n            'error' => false,\n            'message' => 'Successfully Returned',\n            'policy' => 'No Bucket Polcy Attached'\n        ));\n\n    }\n\n~~~. Hi @imshashank \nThanks for the response the before $e -> getMessage() was just catching the error message and i could return this via ajax to my users.\nTake this for instance.\n~~~\nError executing \"CreateBucket\" on \"https://s3-us-west-2.amazonaws.com/sam\"; AWS HTTP error: Client error: PUT https://s3-us-west-2.amazonaws.com/testing resulted in a 409 Conflict response:\nBucketAlreadyExistsThe requested bucket name is not  (truncated...)\n BucketAlreadyExists (client): The requested bucket name is not available. The bucket namespace is shared by all users of the system. Please select a different name and try again. - \nBucketAlreadyExistsThe requested bucket name is not available. The bucket namespace is shared by all users of the system. Please select a different name and try again.\n~~~\nBefore I could just return the relevant error message with $e -> getMessage() to my users which from the example above would be.\n~~~\nThe requested bucket name is not available. The bucket namespace is shared by all users of the system. Please select a different name and try again. \n~~~\nAll the extra info is great for a developer but it adds extra confusion for my users.\nNot sure how I could just get the message only from version 3?\nThanks\n. @imshashank thanks very much ;). Hi @kstich \nYes, we managed to get it working with the Paginator call.\nBut surely this shouldn\u2019t be closed because the listChannels call itself doesn\u2019t work as expected? \nAnd the way we have to do this is very messy?\n```\n$MediaLiveClient = new MediaLiveClient(['version' => 'latest', 'region' => 'us-east-1']);\n$ListChannels = $MediaLiveClient->getPaginator('ListChannels');\n$ChanelsArray = [];\nforeach($ListChannels as $Result)\n    {\n    if (count($Result->get('Channels')) > 0)\n        {\n        foreach($Result->get('Channels') as $key => $value)\n            {\n            array_push($ChanelsArray, $value);\n            }\n        }\n    }\necho count($ChanelsArray);\n```\nIt returns empty arrays thats why we have to check for a count and the call is slower because of the multiple iterations.\nPlease consider reopening and applying a fix.\nThanks\n. Ok thanks. ",
    "thamaraiselvam": "I rarely get this exception Guzzle\\Http\\CachingEntityBody supports only SEEK_SET and SEEK_CUR seek operations while uploading files to S3.\nI am not sure what is went wrong.. Actually it happens while calling listMultipartUploads here is stack\nGuzzle\\Common\\Exception\\RuntimeException: Unable to parse response body into XML: String could not be parsed as XML\n#14 /S3/guzzle/guzzle/src/Guzzle/Http/Message/Response.php(899): xml\n#13 /S3/guzzle/guzzle/src/Guzzle/Service/Command/LocationVisitor/Response/XmlVisitor.php(17): before\n#12 /S3/guzzle/guzzle/src/Guzzle/Service/Command/OperationResponseParser.php(134): visitResult\n#11 /S3/guzzle/guzzle/src/Guzzle/Service/Command/OperationResponseParser.php(86): handleParsing\n#10 /S3/guzzle/guzzle/src/Guzzle/Service/Command/DefaultResponseParser.php(39): parse\n#9 /S3/guzzle/guzzle/src/Guzzle/Service/Command/OperationCommand.php(87): process\n#8 /S3/aws/aws-sdk-php/src/Aws/S3/Command/S3Command.php(58): process\n#7 /S3/guzzle/guzzle/src/Guzzle/Service/Command/AbstractCommand.php(193): getResult\n#6 /S3/guzzle/guzzle/src/Guzzle/Service/Client.php(138): execute\n#5 /S3/guzzle/guzzle/src/Guzzle/Service/Command/AbstractCommand.php(153): execute\n#4 /S3/guzzle/guzzle/src/Guzzle/Service/Command/AbstractCommand.php(189): getResult\n#3 /S3/guzzle/guzzle/src/Guzzle/Service/Client.php(76): __call\n#2 /S3/aws/aws-sdk-php/src/Aws/Common/Client/AbstractClient.php(104): __call\n#1 $this->client->listMultipartUploads(array(\n            'Bucket' => $this->as3_bucket,\n        ));. According to Amazon:\n\nWrite, read, and delete objects containing from 1 byte to 5 terabytes of data each. The number of objects you can store is unlimited.\n\nReference:\nhttp://stackoverflow.com/a/3981067/2975952\nhttps://aws.amazon.com/s3/details/. ",
    "Skarlso": "@sm2017 What @Thamaraiselvam was trying to say with this is, that there is no such concept in regards to S3. You don't have a bucket with 100GB total space, you have A Bucket, and that's it.. So, I realize that this is a bad approach to stuff. \nI'll come up with a nicer approach and post it here later and then close this Issue. \nI'm going to wrap the AWS Api as I don't own the API and should not Mock it out because of that. I'll be using a persistence layer of my own, and than use AWS below it or my own mock with DI. More to follow. . Ok, so now, I'm mocking out the wrapper, that returns a client for me. Which is much better, because I own the wrapper, which can be mocked up and the client than does whatever the hell I want. For DI I'm using PHP-DI.. ",
    "kchan4": "If you can open the ticket for me, that will be great!\nI will definitely try with a upgraded version of SDK as well.\nAside from that, I thought the \\Aws\\Credentials\\CredentialProvider::memoize already caching the credentials across processes, no?\n . @jeskew  You are right!  I thought the memoize is write to a file and share across the process, but I was wrong.\nI've implemented my own caching using CacheInterface and the errors is gone!\nThank you @jeskew  @cjyclaire !. ",
    "pavankumarkatakam": "Actually, my application is in production stage and used by several members. I'm using S3 (region: ap-souteast-1) to upload the images as well as documents. Now if I want to make use of Rekognition the S3 object should be in the same region  because the credentials are given to create the RekognitionClient it is taking to the S3 credentials But rekognition is only available in the three regions im using us-east-2. So my intention is to make the rekognition such that it can take the s3 object from other region also.. ",
    "ivankristic": "@imshashank \ni c/p the exception, it throws it as it is written here.\nAnd no, the class in the code is \"Aws\\Ec2\\Exception\\Ec2Exception\"\nIts all capitalised and PascalCase, nothing is full uppercase. @cjyclaire \ni know i want to hit that class, that is exactly the problem cause i get FatalError cause WrappedHttpHandler hits the wrong one.\nJust create new valid Ec2Client instance and call runInstances($params) with empty $params values\nWhat ive sent was \n$params = [\n     'ImageId' => 'null',\n     'InstanceType' => 'null',\n     'KeyName' => 'null',\n     'Placement' => [\n          'AvailabilityZone' => 'null',\n          ],\n     'MaxCount' => 1,\n     'MinCount' => 1,\n     'SecurityGroups => ['null'],\n     ];\n(Though its irrelevant, null is string cause of HTML Form Input). @cjyclaire @imshashank \nOk, i figured out what happens and why.\nIve made helper class that extends Ec2Client because there is no method to get $instanceTypes, and i couldnt figure out how to format valid filter params in request for AMIs to display only those provided by AWS, so i hardcoded those $images for myself for now until i have more time to figure those out.\nAnyhow, while my class is called EC2Helper, i get that FatalError, but when i rename my class to Ec2Helper, i get Ec2Exception as i should.\n```\nclass EC2Helper extends Ec2Client\n{\n    protected $key, $secret;\nprotected $instanceTypes = [\n    't2.nano', 't2.micro', 't2.small', 't2.medium', 't2.large', 't2.xlarge', 't2.2xlarge', 'm1.small',\n    'm1.medium', 'm1.large', 'm1.xlarge', 'm3.medium', 'm3.large', 'm3.xlarge', 'm3.2xlarge', 'm4.large', 'm4.xlarge',\n    'm4.2xlarge', 'm4.4xlarge', 'm4.10xlarge', 'm4.16xlarge', 'm2.xlarge', 'm2.2xlarge', 'm2.4xlarge', 'cr1.8xlarge',\n    'r3.large', 'r3.xlarge', 'r3.2xlarge', 'r3.4xlarge', 'r3.8xlarge', 'r4.large', 'r4.xlarge', 'r4.2xlarge',\n    'r4.4xlarge', 'r4.8xlarge', 'r4.16xlarge', 'x1.16xlarge', 'x1.32xlarge', 'i2.xlarge', 'i2.2xlarge', 'i2.4xlarge',\n    'i2.8xlarge', 'hi1.4xlarge', 'hs1.8xlarge', 'c1.medium', 'c1.xlarge', 'c3.large', 'c3.xlarge', 'c3.2xlarge',\n    'c3.4xlarge', 'c3.8xlarge', 'c4.large', 'c4.xlarge', 'c4.2xlarge', 'c4.4xlarge', 'c4.8xlarge', 'cc1.4xlarge',\n    'cc2.8xlarge', 'g2.2xlarge', 'g2.8xlarge', 'cg1.4xlarge','p2.xlarge', 'p2.8xlarge', 'p2.16xlarge', 'd2.xlarge',\n    'd2.2xlarge', 'd2.4xlarge', 'd2.8xlarge', 'f1.2xlarge', 'f1.16xlarge'\n];\n\nprotected $images = [\n    'Amazon Linux AMI 2016.09.1 (HVM)' => 'ami-1e299d7e',\n    'Red Hat Enterprise Linux 7.3 (HVM)' => 'ami-6f68cf0f',\n    'SUSE Linux Enterprise Server 12 SP2 (HVM)' => 'ami-e4a30084',\n    'Ubuntu Server 16.04 LTS (HVM)' => 'ami-b7a114d7',\n    'Microsoft Windows Server 2016 Base' => 'ami-e69c2986',\n    'Microsoft Windows Server 2016 Base with Containers' => 'ami-64982d04',\n    'Microsoft Windows Server 2016 Base Nano' => 'ami-edf3448d',\n    'Microsoft Windows Server 2016 with SQL Server 16 Express' => 'ami-ebc97d8b',\n    'Microsoft Windows Server 2016 with SQL Server 16 Web' => 'ami-52ca7e32',\n    'Microsoft Windows Server 2016 with SQL Server 16 Standard' => 'ami-37b40057',\n    'Microsoft Windows Server 2012 R2 Base' => 'ami-cf9722af',\n    'Microsoft Windows Server 2012 R2 with SQL Server 16 Express' => 'ami-0a3f8a6a',\n    'Microsoft Windows Server 2012 R2 with SQL Server 16 Web' => 'ami-ef32878f',\n    'Microsoft Windows Server 2012 R2 with SQL Server 16 Standard' => 'ami-383a8f58',\n    'Microsoft Windows Server 2012 R2 with SQL Server 14 Express' => 'ami-67922707',\n    'Microsoft Windows Server 2012 R2 with SQL Server 14 Web' => 'ami-44972224',\n    'Microsoft Windows Server 2012 R2 with SQL Server 14 Standard' => 'ami-909722f0',\n    'Microsoft Windows Server 2012 Base' => 'ami-2e92274e',\n    'Microsoft Windows Server 2012 with SQL Server 12 Express' => 'ami-929c29f2',\n    'Microsoft Windows Server 2012 with SQL Server 12 Web' => 'ami-289f2a48',\n    'Microsoft Windows Server 2012 with SQL Server 12 Standard' => 'ami-d89d28b8',\n    'Microsoft Windows Server 2008 R2 Base' => 'ami-6d91240d',\n    'Microsoft Windows Server 2008 R2 with SQL Server 8 Express and IIS 7' => 'ami-bb9421db',\n    'Microsoft Windows Server 2008 R2 with SQL Server 8 Web' => 'ami-7f96231f',\n    'Microsoft Windows Server 2008 R2 with SQL Server 8 Standard' => 'ami-819722e1',\n    'Microsoft Windows Server 2008 Base 64b' => 'ami-d1de6ab1',\n    'Microsoft Windows Server 2008 Base 32b' => 'ami-84df6be4',\n    'SUSE Linux Enterprise Server 11 SP4 (PV)' => 'ami-baab0fda',\n    'Ubuntu Server 14.04 LTS (HVM)' => 'ami-30e65350',\n    'Amazon Linux AMI 2016.09.1 (PV)' => 'ami-a3299dc3',\n    'Microsoft Windows Server 2003 R2 Base 64' => 'ami-31912451',\n    'Microsoft Windows Server 2003 R2 Base 32' => 'ami-bd9722dd'\n];\n\npublic function __construct()\n{\n    $this->key = Config::get('remote.calls.AWS.key');\n    $this->secret = Config::get('remote.calls.AWS.secret');\n\n    $credentials = new Credentials($this->key, $this->secret);\n\n    parent::__construct([\n        'credentials' => $credentials,\n        'region' => Config::get('remote.calls.AWS.region'),\n        'version' => Config::get('remote.calls.AWS.version')\n    ]);\n}\n\npublic function describeInstanceTypes()\n{\n    return $this->instanceTypes;\n}\n\npublic function describeHardcodedImages()\n{\n    return $this->images;\n}\n\npublic function createName($input)\n{\n    $params['Tags'] = [[\n        'Key' => 'Name',\n        'Value' => $input['Name']\n    ]];\n    $params['Resources'] = [\n        $this->getInstanceId($input['Instance'])\n    ];\n    $this->createTags($params);\n}\n\nprotected function getInstanceId($instance)\n{\n    return $instance->get('Instances')[0]['InstanceId'];\n}\n\n}\n```\nSDK = 3.20.13,\nPHP = 5.6.4,\nGuzzle = 6.2.2,\nOS = Ubuntu 16.04. ",
    "sanfx": "still the same error.\nif I just change it to local , however I open the url http://ceres.local:8000 (ceres.local is name on my local network) in browser , downloads the file ceres.local which has this in it:\n{\"__type\":\"com.amazonaws.dynamodb.v20120810#MissingAuthenticationToken\",\"message\":\"Request must contain either a valid (registered) AWS access key ID or     X.509 certificate.\"}\nis it due to missing credentials ? however I did saved my creditential file ~/.aws/creditentials with private key and secret key !. ",
    "cadavre": "Verified with latest SDK version \u2013 the same error.. And this is how raw Request body looks like:\nxml\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<LifecycleConfiguration xmlns=\"http://s3.amazonaws.com/doc/2006-03-01/\">\n    <Rule>\n        <AbortIncompleteMultipartUpload>\n            <DaysAfterInitiation>3</DaysAfterInitiation>\n        </AbortIncompleteMultipartUpload>\n        <Status>Enabled</Status>\n    </Rule>\n</LifecycleConfiguration>. Yes, this solved the problem. Then, why AWS SDK PHP v3 documentation doesn't state that Prefix shall be REQUIRED?\n'Prefix' => '<string>',. ",
    "b8ne": "Hi @jeskew thanks for the quick reply.  Yep that was the problem.\nWas running off a Homestead/Vagrant setup, after a bit of searching I found clock problems are common in VM setups.\nCheers. ",
    "lucianocn": "http://docs.aws.amazon.com/aws-sdk-php/v3/guide/guide/configuration.html?highlight=timeout#connect-timeout\nhttp://docs.aws.amazon.com/aws-sdk-php/v3/guide/guide/configuration.html?highlight=timeout#timeout. ",
    "hamadata": "@jeskew \nThanks, that was my misunderstanding (-_-;). ",
    "dnizetic": "I posted a similar question on SO: https://stackoverflow.com/questions/51643372/aws-s3-php-sdk-how-to-specify-aes256-encryption-when-doing-multipart-upload. \nFound this issue which seems to confirm my assumption - thanks!. ",
    "NinoSkopac": "Awesome!. Sure: http://stackoverflow.com/questions/42755367/aws-sdk-for-php-how-to-retry-asynchronous-failures. @cjyclaire Did you find out anything?. Yeah, and they declined. . Hey @imshashank. I am familiar with Guzzle and Promises.\n\nAll async requests if failed are automatically retried.\n\nQuestion: is their order preserved in case of a failed request?\nLet's say I fire 10 async reqs, and request #5 fails - is it still going to be in 5th place once it's been retried and it succeeds?\n\nIt will be a lot more helpful if you can share us what errors you are getting so that we can reproduce it.\n\nSure. Keep in mind that these errors are from 23 days ago, when I wasn't capping the concurrent request count to 80 with CommandPool:\n``\nPromise rejection: exception 'Aws\\Polly\\Exception\\PollyException' with message 'Error executing \"SynthesizeSpeech\" on \"https://polly.eu-west-1.amazonaws.com/v1/speech\"; AWS HTTP error: Server error:POST https://polly.eu-west-1.amazonaws.com/v1/speechresulted in a503 Service Unavailable` response:\nUnable to parse error information from response - Error parsing JSON: Syntax error'\nGuzzleHttp\\Exception\\ServerException: Server error: POST https://polly.eu-west-1.amazonaws.com/v1/speech resulted in a 503 Service Unavailable response:\nin /var/www/read2me.online/vendor/guzzlehttp/guzzle/src/Exception/RequestException.php:111\nStack trace:\n0 /var/www/read2me.online/vendor/guzzlehttp/guzzle/src/Middleware.php(65): GuzzleHttp\\Exception\\RequestException::create(Object(GuzzleHttp\\Psr7\\Request), Object(GuzzleHttp\\Psr7\\Response))\n1 /var/www/read2me.online/vendor/guzzlehttp/promises/src/Promise.php(203): GuzzleHttp\\Middleware::GuzzleHttp{closure}(Object(GuzzleHttp\\Psr7\\Response))\n2 /var/www/read2me.online/vendor/guzzlehttp/promises/src/Promise.php(156): GuzzleHttp\\Promise\\Promise::callHandler(1, Object(GuzzleHttp\\Psr7\\Response), Array)\n3 /var/www/read2me.online/vendor/guzzlehttp/promises/src/TaskQueue.php(47): GuzzleHttp\\Promise\\Promise::GuzzleHttp\\Promise{closure}()\n4 /var/www/read2me.online/vendor/guzzlehttp/guzzle/src/Handler/CurlMultiHandler.php(96): GuzzleHttp\\Promise\\TaskQueue->run()\n5 /var/www/read2me.online/vendor/guzzlehttp/guzzle/src/Handler/CurlMultiHandler.php(123): GuzzleHttp\\Handler\\CurlMultiHandler->tick()\n6 /var/www/read2me.online/vendor/guzzlehttp/promises/src/Promise.php(246): GuzzleHttp\\Handler\\CurlMultiHandler->execute(true)\n7 /var/www/read2me.online/vendor/guzzlehttp/promises/src/Promise.php(223): GuzzleHttp\\Promise\\Promise->invokeWaitFn()\n8 /var/www/read2me.online/vendor/guzzlehttp/promises/src/Promise.php(267): GuzzleHttp\\Promise\\Promise->waitIfPending()\n9 /var/www/read2me.online/vendor/guzzlehttp/promises/src/Promise.php(225): GuzzleHttp\\Promise\\Promise->invokeWaitList()\n10 /var/www/read2me.online/vendor/guzzlehttp/promises/src/Promise.php(62): GuzzleHttp\\Promise\\Promise->waitIfPending()\n11 /var/www/read2me.online/vendor/guzzlehttp/promises/src/Promise.php(65): GuzzleHttp\\Promise\\Promise->wait(true)\n12 /var/www/read2me.online/vendor/guzzlehttp/promises/src/functions.php(201): GuzzleHttp\\Promise\\Promise->wait()\n13 /var/www/read2me.online/src/Read2Me/Aws.php(71): GuzzleHttp\\Promise\\unwrap(Array)\n14 /var/www/read2me.online/src/Read2Me/AudioConversion.php(199): Read2Me\\Aws::pollySynthesize('Donald Trump co...')\n15 /var/www/read2me.online/src/Read2Me/AudioConversion.php(184): Read2Me\\AudioConversion->createAudio()\n16 /var/www/read2me.online/bin/conversion.php(99): Read2Me\\AudioConversion->getAudio()\n17 /var/www/read2me.online/bin/conversion.php(49): class@anonymous->audioConversion()\n18 /var/www/read2me.online/vendor/symfony/console/Command/Command.php(262): class@anonymous->execute(Object(Symfony\\Component\\Console\\Input\\ArgvInput), Object(Symfony\\Component\\Console\\Output\\ConsoleOutput))\n19 /var/www/read2me.online/vendor/symfony/console/Application.php(826): Symfony\\Component\\Console\\Command\\Command->run(Object(Symfony\\Component\\Console\\Input\\ArgvInput), Object(Symfony\\Component\\Console\\Output\\ConsoleOutput))\n20 /var/www/read2me.online/vendor/symfony/console/Application.php(189): Symfony\\Component\\Console\\Application->doRunCommand(Object(class@anonymous), Object(Symfony\\Component\\Console\\Input\\ArgvInput), Object(Symfony\\Component\\Console\\Output\\ConsoleOutput))\n21 /var/www/read2me.online/vendor/symfony/console/Application.php(120): Symfony\\Component\\Console\\Application->doRun(Object(Symfony\\Component\\Console\\Input\\ArgvInput), Object(Symfony\\Component\\Console\\Output\\ConsoleOutput))\n22 /var/www/read2me.online/bin/conversion.php(111): Symfony\\Component\\Console\\Application->run()\n23 {main}\nNext Aws\\Polly\\Exception\\PollyException: Error executing \"SynthesizeSpeech\" on \"https://polly.eu-west-1.amazonaws.com/v1/speech\"; AWS HTTP error: Server error: POST https://polly.eu-west-1.amazonaws.com/v1/speech resulted in a 503 Service Unavailable response:\nUnable to parse error information from response - Error parsing JSON: Syntax error in /var/www/read2me.online/vendor/aws/aws-sdk-php/src/WrappedHttpHandler.php:192\nStack trace:\n0 /var/www/read2me.online/vendor/aws/aws-sdk-php/src/WrappedHttpHandler.php(97): Aws\\WrappedHttpHandler->parseError(Array, Object(GuzzleHttp\\Psr7\\Request), Object(Aws\\Command), Array)\n1 /var/www/read2me.online/vendor/guzzlehttp/promises/src/Promise.php(203): Aws\\WrappedHttpHandler->Aws{closure}(Array)\n2 /var/www/read2me.online/vendor/guzzlehttp/promises/src/Promise.php(174): GuzzleHttp\\Promise\\Promise::callHandler(2, Array, Array)\n3 /var/www/read2me.online/vendor/guzzlehttp/promises/src/RejectedPromise.php(40): GuzzleHttp\\Promise\\Promise::GuzzleHttp\\Promise{closure}(Array)\n4 /var/www/read2me.online/vendor/guzzlehttp/promises/src/TaskQueue.php(47): GuzzleHttp\\Promise\\RejectedPromise::GuzzleHttp\\Promise{closure}()\n5 /var/www/read2me.online/vendor/guzzlehttp/guzzle/src/Handler/CurlMultiHandler.php(96): GuzzleHttp\\Promise\\TaskQueue->run()\n6 /var/www/read2me.online/vendor/guzzlehttp/guzzle/src/Handler/CurlMultiHandler.php(123): GuzzleHttp\\Handler\\CurlMultiHandler->tick()\n7 /var/www/read2me.online/vendor/guzzlehttp/promises/src/Promise.php(246): GuzzleHttp\\Handler\\CurlMultiHandler->execute(true)\n8 /var/www/read2me.online/vendor/guzzlehttp/promises/src/Promise.php(223): GuzzleHttp\\Promise\\Promise->invokeWaitFn()\n9 /var/www/read2me.online/vendor/guzzlehttp/promises/src/Promise.php(267): GuzzleHttp\\Promise\\Promise->waitIfPending()\n10 /var/www/read2me.online/vendor/guzzlehttp/promises/src/Promise.php(225): GuzzleHttp\\Promise\\Promise->invokeWaitList()\n11 /var/www/read2me.online/vendor/guzzlehttp/promises/src/Promise.php(62): GuzzleHttp\\Promise\\Promise->waitIfPending()\n12 /var/www/read2me.online/vendor/guzzlehttp/promises/src/Promise.php(65): GuzzleHttp\\Promise\\Promise->wait(true)\n13 /var/www/read2me.online/vendor/guzzlehttp/promises/src/functions.php(201): GuzzleHttp\\Promise\\Promise->wait()\n14 /var/www/read2me.online/src/Read2Me/Aws.php(71): GuzzleHttp\\Promise\\unwrap(Array)\n15 /var/www/read2me.online/src/Read2Me/AudioConversion.php(199): Read2Me\\Aws::pollySynthesize('Donald Trump co...')\n16 /var/www/read2me.online/src/Read2Me/AudioConversion.php(184): Read2Me\\AudioConversion->createAudio()\n17 /var/www/read2me.online/bin/conversion.php(99): Read2Me\\AudioConversion->getAudio()\n18 /var/www/read2me.online/bin/conversion.php(49): class@anonymous->audioConversion()\n19 /var/www/read2me.online/vendor/symfony/console/Command/Command.php(262): class@anonymous->execute(Object(Symfony\\Component\\Console\\Input\\ArgvInput), Object(Symfony\\Component\\Console\\Output\\ConsoleOutput))\n20 /var/www/read2me.online/vendor/symfony/console/Application.php(826): Symfony\\Component\\Console\\Command\\Command->run(Object(Symfony\\Component\\Console\\Input\\ArgvInput), Object(Symfony\\Component\\Console\\Output\\ConsoleOutput))\n21 /var/www/read2me.online/vendor/symfony/console/Application.php(189): Symfony\\Component\\Console\\Application->doRunCommand(Object(class@anonymous), Object(Symfony\\Component\\Console\\Input\\ArgvInput), Object(Symfony\\Component\\Console\\Output\\ConsoleOutput))\n22 /var/www/read2me.online/vendor/symfony/console/Application.php(120): Symfony\\Component\\Console\\Application->doRun(Object(Symfony\\Component\\Console\\Input\\ArgvInput), Object(Symfony\\Component\\Console\\Output\\ConsoleOutput))\n23 /var/www/read2me.online/bin/conversion.php(111): Symfony\\Component\\Console\\Application->run()\n24 {main}\n```. @imshashank \nI'm not facing the issue since I've migrated to CommandPool. Thanks for clarification about order. I'm gonna close this.. So, you're saying that CommandPool in the SDK operates differently based on the service?. It doesn't really matter I guess, since SQS calls are free (I'd rather not share the code).\nThank you for your time Shashank.. ",
    "joshdifabio": "Thanks for the prompt reply! That makes sense, however, as far as I can tell, the full jitter delay implementation in the PHP SDK is broken: the first retry always happens instantly, whereas there should be an upper limit of 100ms for the first retry delay (or 50ms in the case of DynamoDB requests).\nHere's a table comparing the delay upper limits for the PHP and Java SDKs.\n| Retry no. | Java upper limit | PHP upper limit | PHP upper limit for DynamoDB\n| - | - | - | - |\n| 1 | 100ms | 0ms | 0ms |\n| 2 | 200ms | 100ms | 50ms |\n| 3 | 400ms | 200ms | 100ms |\n| 4 | 800ms | 400ms | 200ms |\nSo, unless I'm missing something, the PHP SDK is not behaving the same as the Java SDK, nor is it behaving in line with the DynamoDB docs, which recommend an initial delay of up to 50ms.\nJava upper limit\nPHP upper limit. Thanks guys!. ",
    "4406arthur": "@cjyclaire Hi, I know sigv4 is great improvement, but I found lots of storage service provider which adopted s3 api gateway did not change yet..., I hope it could support downward compatibility, maybe lots of developer meet same issue.. @imshashank , hi\n\nAre you trying to download the object from s3 to your local system using \"copy\" command?\n\nno, I want to copy object in S3. In my use case the prefix of key just like path in file system.\nsometime the user archives the data, copy that object to another path for long life live,\nand some ACL policy.\n\nstack trace of the error that you are getting when you run the curl command\n\n<Error>\n<Code>RequestTimeout</Code>\n<Message>Your socket connection to the server was not read from or written to within the timeout period. Idle connections will be closed.</Message>\n<RequestId>xxxxxxxx9E41506</RequestId>\n<HostId>xxxxxxxxxxxxxBZTmK7yjpCKF81ho9cAeCc</HostId>\n</Error>\nand I can use presignedURL with put,get,delete,head expectedly.\n. @imshashank , I prefix with bucket name, It cannot work also.. same as before.\nhere is my presignedURL\n\nI can work only carry Content-Length: 0, but it will upload a 0 size object.\n\n\n\nHeader with Content-Length, will return timeout issue\u2026...\n\nxml\n<Error>\n<Code>RequestTimeout</Code>\n<Message>Your socket connection to the server was not read from or written to within the timeout period. Idle connections will be closed.</Message>\n<RequestId>xxxxxxxx9E41506</RequestId>\n<HostId>xxxxxxxxxxxxxBZTmK7yjpCKF81ho9cAeCc</HostId>\n</Error>. @imshashank , with Content-Length 0 respond perfectly fine , I knew.\nbut, does it really copy a object or just a empty object ?. after trigger this CopyObject presignedURL with Content-Length 0, \nI see a 0 size object in my S3.\nIts weird, this copy operation just like nothing happen.\nI will see a copy object in my s3 that is my expectation.\n. After trying same thing on go-aws-sdk, I got the same result.\nIts not a php-aws-sdk issue.\nthanks for help.\n. @kstich , I have tried before, my reply on May 18.\nI think signatureVersion v4 will cause this issue.\nhttps://github.com/aws/aws-sdk-js/issues/281. ",
    "seliger": "Also --\ncoreyseliger:~/workspace/appstream-auth $ composer info\naws/aws-sdk-php        3.22.7 AWS SDK for PHP - Use Amazon Web Services in your PHP project\nguzzlehttp/guzzle      6.2.2  Guzzle is a PHP HTTP client library\nguzzlehttp/promises    v1.3.1 Guzzle promises library\nguzzlehttp/psr7        1.3.1  PSR-7 message implementation\nmtdowling/jmespath.php 2.4.0  Declaratively specify how to extract elements from a JSON document\npsr/http-message       1.0.1  Common interface for HTTP messages. It is possible to work around this issue:\n$appstream = $sdk->createAppstream([ \n        'endpoint' => 'https://appstream2.us-east-1.amazonaws.com/' \n    ]);\n(Not sure if it breaks the rest of the API, but does allow me to continue with this specific use case...). ",
    "AndyDunn": "@cjyclaire I'm using PHP 7.1.2 in Apache and 7.0.10 on the command line.. @cjyclaire I would have thought the only version would be in my laravel website folder, not from somewhere else on my hard drive. I installed PHP AWS SDK as part of the default Laravel 5.4 installation and after checking the guzzle installation, it is definitely 6.2.\nI just don't understand why aws-sdk-php would be finding a guzzle which isn't within my laravel folder?. @imshashank Okay, I have this narrowed down to an error when running phpunit from Laravel. If I call the API method via Postman it saves the file to S3 correctly.\nNot sure how laravels phpunit uses an older version of guzzle, but I can at least live with skipping this test and only using it via a full API call.. @jeskew Wonderful idea. I just ran 'composer global show' and it came back with:\nguzzlehttp/guzzle                  4.2.2\nI also tried using the phpunit installed with Laravel and the tests executed perfectly. Thanks a lot for the help.. ",
    "Tobion": "This $uri = $uri->withPath($this->bucket); needs to be changed to $uri = $uri->withPath('/'.$this->bucket); in https://github.com/aws/aws-sdk-php/blob/master/src/S3/PostObjectV4.php#L152 and https://github.com/aws/aws-sdk-php/blob/master/src/S3/PostObject.php#L135 to fix the logic as the bucket does not seem to contain the leading slash for a proper path.. ~1.3.1 is probably not what you really want as it will not allow 1.4 anymore. You should consider using ^1.3.1\n. ",
    "mwgamble": "The change to the version restriction on mtdowling/jmespath.php doesn't appear to have been updated. Any chance of this happening soon?. ",
    "sky257": "\n@imshashank Thank you for coming back to me.\nWe did indeed look at mb_encode_mimeheader( ) in detail and it also looks like the output Encuesta \"Mejora del =?UTF-8?B?RGVzZW1wZcOxbyBkZWwgQWdlbnRlIGRlIFNlcnZp?= =?UTF-8?B?Y2lvIg==?= <foo@bar.com> is RFC 2047 compliant.\nNevertheless, AWS SES won't accept it.\nWe will probably use following workaround:\n$source = \"=?UTF-8?B?\" . base64_encode($name) . \"?= <{$from}>\";\nThat way, the full name is encoded (including the part before the special character \u00f1) and this works with AWS SES.\nThanks anyway!\nBest,\nLionel\n\nThank you !!!. ",
    "fernandoval": "There is some bizarre dependence.\nWhen updating guzzlehttp/psr7 to v1.4.0 (or when this occurs by updating guzzlehttp/guzzle to v6.2.3) the aws/aws-sdk-php is automaticaly downgraded to version 3.18.23 by Composer.\nI has an application the use guzzle and I have a require to guzzlehttp/guzzle ^6.2.1 into my composer.json, before the require to aws-sdk-php. After running an update today, this started to happen.\nI moved the require of aws-sdk-php to before of the require of the guzzle. Then the guzzlehttp/psr7 was downgraded to v1.3.1 and guzzlehttp/guzzle was downgraded to v6.2.2.. @jeskew I did not use Uri::resolve nor UriResolver::resolve in my project and I will not change the aws-sdk-php code.\nI use only GuzzleHttp/Client to send a POST request (only).\nMy mistake was put the require to Guzzle before the require to AWS-SDK. I use the same dependency as guzzlehttp/guzzle that aws/aws-sdk-php.\nThis is not OK:\n\"require\": {\n        \"guzzlehttp/guzzle\": \"^6.2.1\",\n        \"aws/aws-sdk-php\": \"*\"\n}\nThis is OK:\n\"require\": {\n        \"aws/aws-sdk-php\": \"*\",\n        \"guzzlehttp/guzzle\": \"^6.2.1\"\n}\nThis is OK too:\n\"require\": {\n        \"aws/aws-sdk-php\": \"*\"\n}\nI put dependency on GuzzleHttp/Guzzle in my project, because if in future I stop to use the AWS SDK, I will have to remember to include the dependency back.\n. After update to v3.23.2, the GuzzleHttp/Psr7 v1.4.1 was installed. Then deprecated error came back.\nNow I had to include a request to v1.3.1 of the PSR7 in my composer.json file.\n\"require\": {\n        \"guzzlehttp/psr7\": \"1.3.1\",\n        \"aws/aws-sdk-php\": \"*\",\n        \"guzzlehttp/guzzle\": \"^6.2.1\"\n    }\n. @jeskew ignore a \"deprecated error\" is not a solution. This is to hide the trash under de carpet.\nIn my application, this is not silenced with a single \"E_USER_DEPRECATED -level notice\" because the thrower error invokes an exception handler that stops the normal operation of the program.. ",
    "xmeltrut": "I think this needs fixing, either by switching to the new UriResolver class or by updating the composer version tag to ensure it does not upgrade to Guzzle PSR-7 1.4.\nYou can use the workaround to re-order your dependencies. However, if AWS and Guzzle are brought in as a dependency, you do not have any control over it.. ",
    "Christoph-Harms": "\nWhen updating guzzlehttp/psr7 to v1.4.0 (or when this occurs by updating guzzlehttp/guzzle to v6.2.3) the aws/aws-sdk-php is automaticaly downgraded to version 3.18.23 by Composer.\n\nI think this could also be caused by the issue pointed out in #1205.. To add to the discussion, take a look at jarektkaczyk/eloquence#155 and the Issue mentioning it.. Welp, now I'm not sure anymore... The documentation clearly states that a logical OR is expressed by a double pipe and makes no mention of a single pipe. I'll set up a test project as soon as I have the time (probably this weekend).. ",
    "freefri": "I think the problem was introduced with this commit https://github.com/aws/aws-sdk-php/commit/62c6666292146e972fccc8711068d4dc5e83e56b by  @jeskew  I think guzzlehttp/psr7 it should still be ~1.3.1.\nWas there any reason to make this change? Maybe we can revert this change? \nIn the meanwhile I'm adding it to my composer.json as @fernandoval has proposed, since this problem is breaking an important functionality in my system. . Aha, so that's the trick. I've actually didn't have a close look to the problem. I've just got execution stopped in my php script. But now that you said so, I realize that we are handling errors and warnings as exceptions in a module of our system (this is a long story), so the problem is only in our system (that's explaining why people is not complaining). Since it is only a critical issue for me and nobody else, I will just force to keep guzzlehttp/psr7 1.3.1, and I will keep an eye in this issue in case the status is changing.. Same problem here with version 3.69.\nFor me it's working with version 3.67.. ",
    "marklocker": "@jeskew What was the reason for specifically excluding 1.4.0, and not anything greater than that? I guess your reasoning was unrelated to the issue discussed here because I still get the deprecation warning with 1.4.1, which makes sense, they're not likely to randomly \"un-deprecate\" the method in future versions.\nI also agree with @fernandoval, a deprecation message isn't something to ignore. I haven't had time to look unfortunately but surely it's an easy fix to switch to using the new method?. ",
    "edsonhoraciojunior": "You asking that just made me realise that there's no such thing as edit a object at S3, which is why there's no reason for a lock. Thank you, i'll close the issue.. ",
    "Fanda36": "I found problem. If need upload new file, then must use PutObject. #1142 \n$cmd = self::$client->getCommand('PutObject', [\n            'Bucket' => 'MY_BUCKET',\n            'Key' => 'test.png',\n ]);. ",
    "quantizer": "@imshashank \nPHP version 7.1.0 (also tested with 7.1.2)\ncurl 7.35.0 (also tested with 7.51.0)\nOS Ubuntu 14.04\noriginal exception:\n[Aws\\Exception\\MultipartUploadException]                                                                                   \n  An exception occurred while completing a multipart upload: Error executing \"CompleteMultipartUpload\" on \"http://s3.ca-central-1.amazonaws.com/filepath?uploadId=UPLOADID\"; AWS HTTP error: Client error: `POST http://s3.ca-central-1.amazonaws.com/filepath?uploadId=UPLOADID` resulted in a `400 Bad Request` response:                                                                                                  \n  <Code>MalformedXML</Code><Message>The XML you provided was not well-formed or did not validate against our publis (truncated...)                                                                                                                                             \n   MalformedXML (client): The XML you provided was not well-formed or did not validate against our published schema - <Code>MalformedXML</Code><Message>The XML you provided was not well-formed or did not validate against our published schema</Message><RequestId>         \n  Request_ID</RequestId><HostId>HOST_ID</HostId>\nException trace:\n() at workdir/vendor/aws/aws-sdk-php/src/Multipart/AbstractUploadManager.php:135\n Aws\\Multipart\\AbstractUploadManager->Aws\\Multipart\\{closure}() at workdir/vendor/guzzlehttp/promises/src/Promise.php:203\n GuzzleHttp\\Promise\\Promise::callHandler() at workdir/vendor/guzzlehttp/promises/src/Promise.php:156\n GuzzleHttp\\Promise\\Promise::GuzzleHttp\\Promise\\{closure}() at workdir/vendor/guzzlehttp/promises/src/TaskQueue.php:47\n GuzzleHttp\\Promise\\TaskQueue->run() at workdir/vendor/guzzlehttp/guzzle/src/Handler/CurlMultiHandler.php:96\n GuzzleHttp\\Handler\\CurlMultiHandler->tick() at workdir/vendor/guzzlehttp/guzzle/src/Handler/CurlMultiHandler.php:123\n GuzzleHttp\\Handler\\CurlMultiHandler->execute() at workdir/vendor/guzzlehttp/promises/src/Promise.php:246\n GuzzleHttp\\Promise\\Promise->invokeWaitFn() at workdir/vendor/guzzlehttp/promises/src/Promise.php:223\n GuzzleHttp\\Promise\\Promise->waitIfPending() at workdir/vendor/guzzlehttp/promises/src/Promise.php:266\n GuzzleHttp\\Promise\\Promise->invokeWaitList() at workdir/vendor/guzzlehttp/promises/src/Promise.php:225\n GuzzleHttp\\Promise\\Promise->waitIfPending() at workdir/vendor/guzzlehttp/promises/src/Promise.php:269\n GuzzleHttp\\Promise\\Promise->invokeWaitList() at workdir/vendor/guzzlehttp/promises/src/Promise.php:225\n GuzzleHttp\\Promise\\Promise->waitIfPending() at workdir/vendor/guzzlehttp/promises/src/Promise.php:62\n GuzzleHttp\\Promise\\Promise->wait() at workdir/vendor/guzzlehttp/promises/src/Coroutine.php:65\n GuzzleHttp\\Promise\\Coroutine->GuzzleHttp\\Promise\\{closure}() at workdir/vendor/guzzlehttp/promises/src/Promise.php:246\n GuzzleHttp\\Promise\\Promise->invokeWaitFn() at workdir/vendor/guzzlehttp/promises/src/Promise.php:223\n GuzzleHttp\\Promise\\Promise->waitIfPending() at workdir/vendor/guzzlehttp/promises/src/Promise.php:266\n GuzzleHttp\\Promise\\Promise->invokeWaitList() at workdir/vendor/guzzlehttp/promises/src/Promise.php:225\n GuzzleHttp\\Promise\\Promise->waitIfPending() at workdir/vendor/guzzlehttp/promises/src/Promise.php:62\n GuzzleHttp\\Promise\\Promise->wait() at workdir/vendor/guzzlehttp/promises/src/Coroutine.php:65\n GuzzleHttp\\Promise\\Coroutine->GuzzleHttp\\Promise\\{closure}() at workdir/vendor/guzzlehttp/promises/src/Promise.php:246\n GuzzleHttp\\Promise\\Promise->invokeWaitFn() at workdir/vendor/guzzlehttp/promises/src/Promise.php:223\n GuzzleHttp\\Promise\\Promise->waitIfPending() at workdir/vendor/guzzlehttp/promises/src/Promise.php:266\n GuzzleHttp\\Promise\\Promise->invokeWaitList() at workdir/vendor/guzzlehttp/promises/src/Promise.php:225\n GuzzleHttp\\Promise\\Promise->waitIfPending() at workdir/vendor/guzzlehttp/promises/src/Promise.php:62\n GuzzleHttp\\Promise\\Promise->wait() at workdir/vendor/aws/aws-sdk-php/src/S3/S3ClientTrait.php:27\n Aws\\S3\\S3MultiRegionClient->upload() at workdir/src/somephpfile.php\nand completeMultipartUpload with debug true option:\ncommand was set to workdir/vendor/aws/aws-sdk-php/src/TraceMiddleware.php:255:\n array(3) {\n   'instance' =>\n   string(32) \"instance_ID\"\n   'name' =>\n   string(23) \"CompleteMultipartUpload\"\n   'params' =>\n   array(5) {\n     'MultipartUpload' =>\n     array(1) {\n       'Parts' =>\n       array(0) {\n       }\n     }\n     'UploadId' =>\n     string(128) \"UPLOAD_ID\"\n     'Bucket' =>\n     string(24) \"bucket_name\"\n     'Key' =>\n     string(57) \"filepath\"\n     '@http' =>\n     array(3) {\n       'timeout' =>\n       int(3600)\n       'connect_timeout' =>\n       int(60)\n       'debug' =>\n       resource(4633) of type (stream)\n     }\n   }\n }\nAs working code for multi-region uploading we are using:\nS3MultiRegionClient::getClientFromPool($region)->upload();. @imshashank yes, @javer right. I forgot to mention about non-default region for uploading.. @imshashank this fix will be in next release?. ",
    "javer": "Simple PHP snippet to reproduce this issue:\n```php\n$filename = tempnam(sys_get_temp_dir(), 'aws-test-');\nfile_put_contents($filename, str_repeat('a', 20 * 1048576));\n$client = new \\Aws\\S3\\S3MultiRegionClient([\n    'credentials' => [\n        'key' => 'KEY',\n        'secret' => 'SECRET',\n    ],\n    'region' => 'us-east-1',\n    'version' => 'latest',\n]);\n$client->upload('bucket-in-ap-southeast-2', 'test/20mb', fopen($filename, 'r'));\n```\nResults in:\n``\nFatal error: Uncaught GuzzleHttp\\Exception\\ClientException: Client error:POST https://s3-ap-southeast-2.amazonaws.com/bucket-in-ap-southeast-2/test/20mb?uploadId=5NZL0AK90PlIA61j4qQ7MwVKj5L_e9nIdVsf6qFQ_.J9Dqbr9NV0bqzdVOH_sSoxnBKwZ5L6Sk_kJ1OkSLIovbxfB26ddjLa3otfEKfjYIOKgIbNmHGQmco43AxN0Xlqek88C5YZ0VxzWGf1mC5.yw--resulted in a400 Bad Request` response:\nMalformedXMLThe XML you provided was not well-formed or did not validate against our publis (truncated...)\n in vendor/guzzlehttp/guzzle/src/Exception/RequestException.php:111\nStack trace:\n0 vendor/guzzlehttp/guzzle/src/Middleware.php(65): GuzzleHttp\\Exception\\RequestException::create(Object(GuzzleHttp\\Psr7\\Request), Object(GuzzleHttp\\Psr7\\Response))\n1 vendor/guzzlehttp/promises/src/Promise.php(203): GuzzleHttp\\Middleware::GuzzleHttp{closure}(Object(GuzzleHttp\\Psr7\\Response))\n2 vendor/guzzlehttp/prom in vendor/aws/aws-sdk-php/src/Multipart/AbstractUploadManager.php on line 135\n```\nIf you decrease file size to 10 MB (i.e. less than ObjectUploader::DEFAULT_MULTIPART_THRESHOLD = 16777216 which triggers using MultipartUploader) - everything is ok.. @imshashank Have you tested with the bucket located not in us-east-1 region? It's important moment, bucket should be not from the default region, this issue is 100% reproducible for me with buckets located in regions: ap-southeast-2, ca-central-1, eu-west-2, i.e. any non-default region. \nWhen I changed bucket to the bucket in us-east-1 region - everything works well.. @imshashank\nAlso after turning debug option to ON I saw that with bucket in non-default region UploadPart command has never entered \"mup\" sign handler, but if I use bucket in the default region - this handler was called:\nhttps://github.com/aws/aws-sdk-php/blob/master/src/Multipart/AbstractUploader.php#L60\nI suppose it's due to the late determining that we should call regional client instead of the default (only after PermanentRedirectException): https://github.com/aws/aws-sdk-php/blob/master/src/S3/S3MultiRegionClient.php#L208\nAfter that getRegionalizedCommand creates a new Command just from the name and arguments:\nhttps://github.com/aws/aws-sdk-php/blob/master/src/S3/S3MultiRegionClient.php#L259\nI guess it's the place where \"mup\" handler from the original command is lost, because creating a new command clones HandlerList from the regional client and doesn't take into account handlers from the original command.. @imshashank Yes, I can confirm that MultiRegionClient was initiated with default us-east-1 region, but for uploading a bucket from another region (ap-southeast-2) was used. I believe it's for what MultiRegionClient is intended for - when I don't care which region should be used. If I would know the region of the bucket - there is no sense to use MultiRegionClient.\nAlso I have checked your recommendation to use @region for upload command and can confirm that it doesn't work as described, this option doesn't have any influence on the executing flow, you can check it yourself by enabling the debug mode. More over, this option doesn't change anything regardless multipart uploading flow or simple is used.\nSo I can confirm that option @region doesn't work at all at least for S3MultiRegionClient::upload().\nBut I can confirm that option @region works at least for S3MultiRegionClient::headObject().. @imshashank \nAs I have already written in my first post in this issue, you should try to upload file with size bigger than 16 MB (value of constant ObjectUploader::DEFAULT_MULTIPART_THRESHOLD).\nFor about 2 weeks you have been trying to reproduce a bug for which I have posted the script which reproduces this bug with 100% probability. Have you ever run it?\nSo I need to repeat reproducing steps again.\nHow to reproduce\n1. Create a bucket bucket-in-ap-southeast-2 in the ap-southeast-2 region.\n2. Create AWS IAM key/secret pair and grant full access to this bucket.\n3. Create empty directory aws-test and install AWS SDK:\ncomposer require 'aws/aws-sdk-php':'3.20.3'\n4. Create script test.php with the following content:\n```php\n<?php\nrequire DIR . '/vendor/autoload.php';\n$filename = tempnam(sys_get_temp_dir(), 'aws-test-');\nfile_put_contents($filename, str_repeat('a', 20 * 1048576));\n$client = new \\Aws\\S3\\S3MultiRegionClient([\n    'credentials' => [\n        'key' => 'AWS_KEY',\n        'secret' => 'AWS_SECRET',\n    ],\n    'region' => 'us-east-1',\n    'version' => 'latest',\n]);\n$client->upload('BUCKET', 'test/20mb', fopen($filename, 'r'));\n5. Replace in this script:\n* AWS_KEY - put actual AWS key\n* AWS_SECRET - put actual AWS secret\n* BUCKET - put actual bucket name in ap-southeast-2 region\n6. Do not change anything else in the script.\n7. Run script:sh\nphp test.php\n```\nActual result\n``\nFatal error: Uncaught GuzzleHttp\\Exception\\ClientException: Client error:POST https://s3-ap-southeast-2.amazonaws.com/BUCKET/test/20mb?uploadId=...resulted in a400 Bad Request` response:\nMalformedXMLThe XML you provided was not well-formed or did not validate against our publis (truncated...)\n in vendor/guzzlehttp/guzzle/src/Exception/RequestException.php:111\nStack trace:\n0 vendor/guzzlehttp/guzzle/src/Middleware.php(65): GuzzleHttp\\Exception\\RequestException::create(Object(GuzzleHttp\\Psr7\\Request), Object(GuzzleHttp\\Psr7\\Response))\n1 vendor/guzzlehttp/promises/src/Promise.php(203): GuzzleHttp\\Middleware::GuzzleHttp{closure}(Object(GuzzleHttp\\Psr7\\Response))\n2 vendor/guzzlehttp/promises/src/Promise.php(156): Guzzl in vendor/aws/aws-sdk-php/src/Multipart/AbstractUploadManager.php on line 135\n```\nExpected result\nThere shouldn't be any errors and file should be successfully uploaded to the bucket.. ",
    "rairlie": "@imshashank \nI'm talking about aws-sdk-php/src/Signature/SignatureV4.php:\nphp\n    public function presign(\n        RequestInterface $request,\n        CredentialsInterface $credentials,\n        $expires\n    ) {\n    ...\n        $httpDate = gmdate(self::ISO8601_BASIC, time());\n    ...\n        $parsed['query']['X-Amz-Date'] = gmdate('Ymd\\THis\\Z', time());\nAnd also the method:\nphp\n    private function convertExpires($expires)\n    {\n    ...\n        $duration = $expires - time();\nNotice they both use time(). This means the URL can only be generated with a start time of 'now'. I can't create one valid from yesterday or tomorrow. This seems a restriction imposed by the SDK. There's no reason to have this limitation?\nWhat I'm proposing is changing it so time can be passed as a parameter to presign(). I've tried this locally and it works expected - would you consider a PR?. @imshashank \n\nI dont see value in providing a value which says expire 24 hours from sometime on yesterday.\n\nMy use case is: I have a web service that returns a pre-signed URL for an S3 object. I want this URL to be valid for 7 days, starting every Monday at 00:00 hrs. By being able to control the start time, the web service will return the same URL any time I call it during that week. So if I call it on Friday I'll get the same URL as when I call it on Monday. This is important because if the URL is constant it can be cached. If I can only generate URLs valid from 'now', I'll return a different URL every time, which will be a cache miss.\nI can think of other use cases, but this is the one I have. So with the change in the PR, I'm able to generate constant URLs.\nYou're right, the s3 presign() method doesn't allow a start time to be specified, but as I don't use this method I didn't change it (I'm using S3SignatureV4 directly).. > Have you tested a presigned URL with a start time set in future?\n@imshashank Yes, I've tested it. It generates the URL, and the URL becomes valid when the start time arrives. I haven't added a test for it as its no different to any other start date in that respect.\nNote the V4 Signature documentation says \"requests are valid within 15 minutes of the timestamp in the request\", which is what I see when using this. If I generate a URL valid from 14 minutes from now, it will be immediately usable. If I generate one valid 16 mins from now, I will have to wait 1 minute before I can use it. When trying to use it before that (an S3 presigned URL), I receive:\n<Error>\n    <Code>AccessDenied</Code>\n    <Message>Request is not yet valid</Message>\n...\nWhich seems right to me?\nLet me know if you've any other questions I've missed.. Thanks :thumbsup: . No problem, now changed. No problem, now changed.. Felt conditional was clearer due to line length, but happy to change.. ",
    "mattzuba": "I have the AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY set in my environment so that the SDK will pick those up and use them.\nMy code to generate the form fields is as follows:\n```php\n$s3Client = new \\Aws\\S3\\S3Client([\n    'version' => 'latest',\n    'region' => 'us-west-2',\n]);\n$formInputs = [\n    'key' => 'uploads/${filename}',\n    'success_action_status' => 201,\n];\n// Extra policy for this action\n$extraPolicy = [\n    ['bucket' => 'mybucket'],\n    ['starts-with', '$key', 'uploads'],\n    ['success_action_status' => '201'],\n];\n$postObject = new Aws\\S3\\PostObjectV4($s3Client, 'mybucket', $formInputs, $extraPolicy);\n$formInputs = $postObject->getFormInputs();\nheader('Content-Type: application/json');\necho json_encode($formInputs);\n```\nThis produces:\njson\n{\n\"key\": \"uploads/${filename}\",\n\"success_action_status\": 201,\n\"X-Amz-Security-Token\": false,\n\"X-Amz-Credential\": \"AKIAJPPEHZKMTDMQW5TQ/20170308/us-west-2/s3/aws4_request\",\n\"X-Amz-Algorithm\": \"AWS4-HMAC-SHA256\",\n\"X-Amz-Date\": \"20170308T205114Z\",\n\"Policy\": \"eyJleHBpcmF0aW9uIjoiMjAxNy0wMy0wOFQyMTo1MToxNFoiLCJjb25kaXRpb25zIjpbeyJidWNrZXQiOiJteWJ1Y2tldCJ9LFsic3RhcnRzLXdpdGgiLCIka2V5IiwidXBsb2FkcyJdLHsic3VjY2Vzc19hY3Rpb25fc3RhdHVzIjoiMjAxIn0seyJ4LWFtei1zZWN1cml0eS10b2tlbiI6ZmFsc2V9LHsiWC1BbXotRGF0ZSI6IjIwMTcwMzA4VDIwNTExNFoifSx7IlgtQW16LUNyZWRlbnRpYWwiOiJBS0lBSlBQRUhaS01URE1RVzVUUVwvMjAxNzAzMDhcL3VzLXdlc3QtMlwvczNcL2F3czRfcmVxdWVzdCJ9LHsiWC1BbXotQWxnb3JpdGhtIjoiQVdTNC1ITUFDLVNIQTI1NiJ9XX0=\",\n\"X-Amz-Signature\": \"cd870f0ce18db47b882ae1695b0ac9052bd001d87002abaa0bb70b074988864d\"\n}\nBase64 Decoding the Policy above yields:\njson\n{\n  \"expiration\": \"2017-03-08T21:51:14Z\",\n  \"conditions\": [\n    {\n      \"bucket\": \"mybucket\"\n    },\n    [\n      \"starts-with\",\n      \"$key\",\n      \"uploads\"\n    ],\n    {\n      \"success_action_status\": \"201\"\n    },\n    {\n      \"x-amz-security-token\": false\n    },\n    {\n      \"X-Amz-Date\": \"20170308T205114Z\"\n    },\n    {\n      \"X-Amz-Credential\": \"AKIAJPPEHZKMTDMQW5TQ/20170308/us-west-2/s3/aws4_request\"\n    },\n    {\n      \"X-Amz-Algorithm\": \"AWS4-HMAC-SHA256\"\n    }\n  ]\n}\n(Note the x-amz-security-token = false, above)\nWhen sending a post with these fields and my file over to AWS S3, this is the XML error returned:\nxml\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<Error>\n   <Code>InvalidToken</Code>\n   <Message>The provided token is malformed or otherwise invalid.</Message>\n   <Token-0>false</Token-0>\n   <RequestId>AABD1381EFED69CE</RequestId>\n   <HostId>pVJxZVIFPvOdVW8hvVxJK14J6YLDCPcopyu7PZIQWTdO/WHjfGvYXoJlrDsNrTY0f+8RHGP52FQ=</HostId>\n</Error>\nThe offending code is either https://github.com/aws/aws-sdk-php/blob/master/src/S3/PostObjectV4.php#L59 (the null check should be converted to a !empty() check), or https://github.com/aws/aws-sdk-php/blob/master/src/Credentials/CredentialProvider.php#L222 (getenv(self::ENV_SESSION) should be changed to getenv(self::ENV_SESSION) ?: null). @imshashank - can you help explain the usage-question tag?  I just glanced over the docs again and unless I'm misunderstanding, a token is only required when using temporary credentials, which I'm not using.  I'm using a permanent access/secret key.  Fixing either or both items that I mentioned at the bottom of my last post correct the issue for me.. ",
    "jmarien": "I cannot get the phpunit tests running - no docs on this and no time to investigate, so closing.. ",
    "eh-steve": "For the record, I agree with @jeskew that trigger_error is totally unnecessary for a deprecation warning which doesn't depend on detection at runtime (given PHP's crazy handling of errors in certain contexts), and that @deprecated is sufficient, but I also think that dependant packages have a responsibility to handle 3rd party deprecations as soon as possible.. That's a solution which would work great for me personally, and keeps things more up to date, although it might result in some unresolvable composer conflicts for certain consumers (i.e. other packages which deliberately chose the opposite approach of requiring Psr7 1.3.x because 1.4.x broke things for them due to trigger_error).\nI totally understand why we want to avoid runtime checks like this - it would be a lot easier if Psr7 just merged your PR...\nOverall I'd be happy with requiring version ^1.4.1. ",
    "albertocubeddu": "Hopefully this pull request will be merged ASAP. . ",
    "jmandrade": "Evidently I was looking at methods in the wrong place. Thank you @jeskew! . ",
    "helhum": "This might not be the right place to fix it, but it illustrates what needs to be fixed.\nPlease guide me whether this should be adapted elsewhere of if tests need to be added to cover this case. Thanks.. ",
    "huntsper": "Added the remaining EC2 topics, and all of the CloudWatch and IAM topics.. Added the remaining S3 and SQS examples. This is ready to go, now.. ",
    "daryl-williams": "Hi imshashank, Thanks for your reply. I am familiar with the describeInstances() method, but it seems to always return a list of instances in my default region. What I need is to get a list of instances per region.\nI am current;y trying to do this by looping over the result of the describeRegions method and calling describeInstances() with the name of the current region (in the loop), like so:\n$response = $ec2->describeRegions();\n foreach ($response['Regions'] as $ndx => $regions) {\n    $regionName = substr($regions['RegionName'], 0, strlen($regions['RegionName']));\n ``\n    // Get the instances within this region.\n    $obj = $ec2->describeInstances([\n      'version'  => '2016-11-15',\n      'region'   => $regionName,\n      'debug' => false,\n      'credentials' => [\n          'key'    => \"xxxx\",\n          'secret' => \"yyyy\",\n      ],\n    ]);\nBut I still get a the full list of from my default region.\nThanks for your help,\nDaryl\n. ",
    "MaxOrelus": "I wasn't aware of those documents. Essentially, I'm just looking for a coded example of instantiating a elasticsearch client for AWS and it's sequenced request to ES root path. Once I understand what that looks like, I can start to understand what I need to do moving forward. I'm a JS developer more than PHP.\nSo in a nutshell I'm trying to\n\ninstantiate a client (w/credentials and all)\nsimple request for root '/'\nmove on to indexing and search request\n\nCan you do all that from the ES client you linked to earlier? I looked through the methods and the client api looks limited.\nThe whole reason I'm jumping through all these hoops is because there's no way that I know of to set AWS Elasticsearch service to have CORS enabled (so I can do this via JS client side). I've been working with ES for a little over 2 years now and the setup is straight forward when it's not on AWS, but AWS doesn't let you set CORS to allow and just take the access key and secret token via fetch...\n```php\n<?php\nuse Aws\\ElasticsearchService\\ElasticsearchServiceClient as ElasticsearchServiceClient;\n$config = get_option($this->plugin_name);\n$auth = array(\n  'key' => $config['es_access_key'],\n  'secret' => $config['es_secret_key']\n);\n$client = new ElasticsearchServiceClient([\n  'version' => 'latest',\n  'region'  => 'us-east-1',\n  'credentials' => $auth,\n  'endpoint' => 'http://name.us-east-1.es.amazonaws.com'\n]);\n```\nThis is what I got so far, but I'm not sure if it's right or not. I'm just looking for an example.... ",
    "manuelz89": "Hi @imshashank , this is the error I get:\n<Error>\n<Code>SignatureDoesNotMatch</Code>\n<Message>\nThe request signature we calculated does not match the signature you provided. Check your key and signing method.\n</Message>\n<AWSAccessKeyId>AKIAJSMK7OWOJTOITCRA</AWSAccessKeyId>\n<StringToSign>\neyJleHBpcmF0aW9uIjoiMjAxNy0wMy0yNFQyMTowMTo0OVoiLAogICAgICAgICAgICAgICAgImNvbmRpdGlvbnMiOlsKICAgICAgICAgICAgICAgICAgICB7ImJ1Y2tldCI6InRiZHByb2plY3RhcHAifSwKICAgICAgICAgICAgICAgICAgICBbInN0YXJ0cy13aXRoIiwiJGtleSIsIjEyMzQ1Njc4OV9tYWluXyJdLAogICAgICAgICAgICAgICAgICAgIHsiYWNsIjoicHVibGljLXJlYWQifSwKICAgICAgICAgICAgICAgICAgICBbInN0YXJ0cy13aXRoIiwiJENvbnRlbnQtVHlwZSIsImltYWdlLyJdLAogICAgICAgICAgICAgICAgICAgIHsieC1hbXotbWV0YS11dWlkIjoiOTg3NjU0MzIxIn0sCiAgICAgICAgICAgICAgICAgICAgeyJ4LWFtei1zZXJ2ZXItc2lkZS1lbmNyeXB0aW9uIjoiQUVTMjU2In0sCiAgICAgICAgICAgICAgICAgICAgeyJ4LWFtei1jcmVkZW50aWFsIjoiQUtJQUpTTUs3T1dPSlRPSVRDUkEvMjAxNzAzMjQvdXMtd2VzdC0yL3MzL2F3czRfcmVxdWVzdCJ9LAogICAgICAgICAgICAgICAgICAgIHsieC1hbXotYWxnb3JpdGhtIjoiQVdTNC1ITUFDLVNIQTI1NiJ9LAogICAgICAgICAgICAgICAgICAgIHsieC1hbXotZGF0ZSI6IjIwMTcwMzI0VDIxMDE0OVoifQogICAgICAgICAgICAgICAgXQogICAgICAgICAgICAgICAgfQ==\n</StringToSign>\n<SignatureProvided>\n55f2f6d95803c09140e5294c5328a841b290730e052c18a62dcc170141449f1f\n</SignatureProvided>\n<StringToSignBytes>\n65 79 4a 6c 65 48 42 70 63 6d 46 30 61 57 39 75 49 6a 6f 69 4d 6a 41 78 4e 79 30 77 4d 79 30 79 4e 46 51 79 4d 54 6f 77 4d 54 6f 30 4f 56 6f 69 4c 41 6f 67 49 43 41 67 49 43 41 67 49 43 41 67 49 43 41 67 49 43 41 67 49 6d 4e 76 62 6d 52 70 64 47 6c 76 62 6e 4d 69 4f 6c 73 4b 49 43 41 67 49 43 41 67 49 43 41 67 49 43 41 67 49 43 41 67 49 43 41 67 49 43 42 37 49 6d 4a 31 59 32 74 6c 64 43 49 36 49 6e 52 69 5a 48 42 79 62 32 70 6c 59 33 52 68 63 48 41 69 66 53 77 4b 49 43 41 67 49 43 41 67 49 43 41 67 49 43 41 67 49 43 41 67 49 43 41 67 49 43 42 62 49 6e 4e 30 59 58 4a 30 63 79 31 33 61 58 52 6f 49 69 77 69 4a 47 74 6c 65 53 49 73 49 6a 45 79 4d 7a 51 31 4e 6a 63 34 4f 56 39 74 59 57 6c 75 58 79 4a 64 4c 41 6f 67 49 43 41 67 49 43 41 67 49 43 41 67 49 43 41 67 49 43 41 67 49 43 41 67 49 48 73 69 59 57 4e 73 49 6a 6f 69 63 48 56 69 62 47 6c 6a 4c 58 4a 6c 59 57 51 69 66 53 77 4b 49 43 41 67 49 43 41 67 49 43 41 67 49 43 41 67 49 43 41 67 49 43 41 67 49 43 42 62 49 6e 4e 30 59 58 4a 30 63 79 31 33 61 58 52 6f 49 69 77 69 4a 45 4e 76 62 6e 52 6c 62 6e 51 74 56 48 6c 77 5a 53 49 73 49 6d 6c 74 59 57 64 6c 4c 79 4a 64 4c 41 6f 67 49 43 41 67 49 43 41 67 49 43 41 67 49 43 41 67 49 43 41 67 49 43 41 67 49 48 73 69 65 43 31 68 62 58 6f 74 62 57 56 30 59 53 31 31 64 57 6c 6b 49 6a 6f 69 4f 54 67 33 4e 6a 55 30 4d 7a 49 78 49 6e 30 73 43 69 41 67 49 43 41 67 49 43 41 67 49 43 41 67 49 43 41 67 49 43 41 67 49 43 41 67 65 79 4a 34 4c 57 46 74 65 69 31 7a 5a 58 4a 32 5a 58 49 74 63 32 6c 6b 5a 53 31 6c 62 6d 4e 79 65 58 42 30 61 57 39 75 49 6a 6f 69 51 55 56 54 4d 6a 55 32 49 6e 30 73 43 69 41 67 49 43 41 67 49 43 41 67 49 43 41 67 49 43 41 67 49 43 41 67 49 43 41 67 65 79 4a 34 4c 57 46 74 65 69 31 6a 63 6d 56 6b 5a 57 35 30 61 57 46 73 49 6a 6f 69 51 55 74 4a 51 55 70 54 54 55 73 33 54 31 64 50 53 6c 52 50 53 56 52 44 55 6b 45 76 4d 6a 41 78 4e 7a 41 7a 4d 6a 51 76 64 58 4d 74 64 32 56 7a 64 43 30 79 4c 33 4d 7a 4c 32 46 33 63 7a 52 66 63 6d 56 78 64 57 56 7a 64 43 4a 39 4c 41 6f 67 49 43 41 67 49 43 41 67 49 43 41 67 49 43 41 67 49 43 41 67 49 43 41 67 49 48 73 69 65 43 31 68 62 58 6f 74 59 57 78 6e 62 33 4a 70 64 47 68 74 49 6a 6f 69 51 56 64 54 4e 43 31 49 54 55 46 44 4c 56 4e 49 51 54 49 31 4e 69 4a 39 4c 41 6f 67 49 43 41 67 49 43 41 67 49 43 41 67 49 43 41 67 49 43 41 67 49 43 41 67 49 48 73 69 65 43 31 68 62 58 6f 74 5a 47 46 30 5a 53 49 36 49 6a 49 77 4d 54 63 77 4d 7a 49 30 56 44 49 78 4d 44 45 30 4f 56 6f 69 66 51 6f 67 49 43 41 67 49 43 41 67 49 43 41 67 49 43 41 67 49 43 41 67 58 51 6f 67 49 43 41 67 49 43 41 67 49 43 41 67 49 43 41 67 49 43 41 67 66 51 3d 3d\n</StringToSignBytes>\n<RequestId>519901476BA681F1</RequestId>\n<HostId>\nnq9Ld7OVzddRtN6AaVIZuqs/gRN1BssW5k/qGyU5jqf/7LwF+q1oWy0JAYsDQplPlSfZJ5EZdAQ=\n</HostId>\n</Error>\nIs there anything specific I should be looking for within the headers? Everything seems to be the appropriate value.\nWhat I am trying to do is a direct upload from the client to S3, instead of having to pass files through my server first. Per the documentation, this is the way to go, right? . Hi @imshashank I haven't been able to solve this yet. In any case, would using pre-signed POSTs or URLs (via SDK) give me the possibility for direct client uploads (as in without passing files through the server hosting my website first) to a given bucket? If you can confirm this then I'll switch method per your recommendation. Thanks!. Thanks @imshashank, will proceed with this. . ",
    "rmgalante": "You may close this issue. We do not have root access to our AWS account. Our service provider gave us the incorrect Access Key Id. It was not a Cloudfront Access Key Id. We asked them to verify that the key they gave us was the Cloudfront Access Key Id at the same time that I submitted this issue. It wasn't. So they sent the correct key. Now, the code works. It was the access key id that was wrong. Sorry for wasting your time.. ",
    "AntoineWattier": "Hi, \nI tried the transfer between 2 buckets of the same account and effectively there is no issue: I suppose the fact to be bucket owner overrides the RequestPayer header. \nBut when you try with a distant bucket with request payer turned on (with an user authorized in your non-owned-bucket policy) any RequestPayer request results in a 403.\nUpdated example: \n```php\n<?php\nrequire 'vendor/autoload.php';\nuse Aws\\S3\\S3Client;\n$s3Client = new S3Client([\n    'region' => 'us-west-2',\n    'version' => '2006-03-01'\n]);\n$promise = new \\Aws\\S3\\ObjectCopier(\n    $s3Client,\n    array(\n        'Bucket' => 'non-owned-bucket',\n        'Key' => 'file',\n    ),\n    array(\n        'Bucket' => 'bucket',\n        'Key' => 'file',\n    ),\n    'private',\n    array(\n        'RequestPayer' => 'requester',\n    )\n);\n$promise->copy();\n```\nMy no-owned-bucket policy is the following:\njson\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"AllowAll\",\n            \"Effect\": \"Allow\",\n            \"Principal\": {\n                \"AWS\": \"arn:aws:iam::XXX\"\n            },\n            \"Action\": [\n                \"s3:GetObject\",\n                \"s3:ListBucket\"\n            ],\n            \"Resource\": [\n                \"arn:aws:s3:::non-owned-bucket\",\n                \"arn:aws:s3:::non-owned-bucket/*\"\n            ]\n        }\n    ]\n}. ",
    "texano00": "@imshashank Thanks. \nIf the DynamoDB team reply me i update you here.. @imshashank \nI've found a solution using the 'ExpressionAttributeNames' param.\nThis works!\n$response = $dynamodb->query(array(\n    'TableName' => 'informabene-invii',\n    'Limit' => 1,\n    'ScanIndexForward' => false,    \n    'KeyConditionExpression' => '#hash_doc = :v_hash',\n    'ExpressionAttributeValues' =>  array(\n        ':v_hash' => array('S' => 'a886b25d524217f5d6c9c8c008664733')\n    ),\n    'ExpressionAttributeNames' => array(\n      \"#hash_doc\" => \"hash\"\n    )\n));. ",
    "DanielApt": "Not the DynamoDB team, but I can at least comment and explain the issue:\nCertain attribute names are reserved, see the full list here. hash is one of these, which is causing your error. \n@texano00's solution is the correct solution: use an ExpressionAttributeName to get around this limitation.\n. ",
    "bradynpoulsen": "As a heads up, I am starting a PR to provide an implementation of this. We have 2 different use cases right now.\n1) Local Environment\nWe inherit everything from the environment currently and are wanting to stub out the APIs. But it feels weird that we can currently configure everything through the environment but have to add some sort of if condition to add the 'endpoint' configuration only during non-production environments.\n2) Feature Deployments via Docker\nWe are deploying our application in a disposable way for integration testing against feature/bug branches to verify issues are being resolved. Same thing as above that everything is currently configured through the environment (excluding the version), but they have to go through a proxy to pass through the outbound firewall.\nMaybe it's just me, but it feel inconsistent to allow key things (credentials/environment) to easily be configured through environment variables, but then prevent something like the endpoint configuration from being allowed.. @imshashank Sounds good to me. I think these environment variables could be useful with any of the SDKs, so if a standard can be established, that always sounds like the better direction.. That's interesting. I added 2 functions and tests for them in PR #1243 and ran coverage with functions.php included and coverage was reported for the ones I added.. I agree, anytime coverage can be tracked for code that would be executed in the distributed package sounds like a winner.. I will squash these commits down once it looks good from further modifications to prevent Github reporting several commits that will close the related issue. ",
    "glebsts": "+1. We are using Atlassian LocalStack running in docker to test our app running in docker, and we can set i.e. access key and default region to same as in localstack through app container env vars, but we cannot change endpoint to point to localstack url in same manner. Seems weird.. ",
    "bvitale": "+1. ",
    "JayAndCatchFire": "Any movement on this?  It's been a while since we last heard about this.. ",
    "hholst80": "I know this use-case isn't going to fly but... It would be much better to be able to NOT having to specify an alias or wrapper for aws and \"just work\". If I know the awscli is installed somewhere in the container the following would work and it would allow us to spawn a regular S3 enabled container and configure the S3 access through environment variables, only.\n.env \nAWS_SECRET_KEY_ID=foo\nAWS_SECRET_ACCESS_KEY=bar\nAWS_S3_ENDPOINT=endpoint.csp.com\ndocker-compose.yml\nversion: 3\nservices:\n  foo:\n    image: ubuntu:18.04\n    environment:\n      - AWS_SECRET_KEY_ID\n      - AWS_SECRET_ACCESS_KEY\n      - AWS_S3_ENDPOINT. ",
    "aftabnaveed": "I always found arrays to be more verbose and hard to read. In my use case I want to do some basic error checking like making sure all the required fields are provided before hitting the API server. It also makes it easy to read. Here is an example from my EC2 wrapper. Apparently this is much similar to the java implementation of the API. \n```php\ntry {\n$runInstanceRequest = new RunInstancesRequest();\n        $runInstanceRequest\n            ->withImageId('ami-c9da80a9') \n            ->withInstanceType('t1.nano')\n            ->withMinCount(1)\n            ->withMaxCount(1)\n            ->withKeyName('xxx-key')\n            ->withTags(['Key' => 'Name', 'Value' => 'Ubuntu'])\n            ->withSecurityGroups(['default']);\n    $ec2Client->runInstances($runInstanceRequest->toArray());\n\n} catch(InvalidArgumentException $e) {\n   echo $e->getMessage();\n}\n. > If you look inside the SDK code you will realize we use models and the operations etc are loaded dynamically. So if we wanted to move to using classes it won't help your IDE as the code is dynamic and IDE wouldn't be able to help in that.\nIf you explicitly define models like I did above. It for sure will help IDE auto suggest code. In case of arrays, one need to make sure keys are appropriately added and nested which apparently makes it hard to debug. . ",
    "codecov-io": "Codecov Report\n\nMerging #1252 into master will increase coverage by 0.94%.\nThe diff coverage is n/a.\n\n\n```diff\n@@             Coverage Diff             @@\nmaster   #1252      +/-\n===========================================\n+ Coverage     91.25%   92.2%   +0.94%   \n===========================================\n  Files           140     140            \n  Lines          5728    6129     +401   \n===========================================\n+ Hits           5227    5651     +424   \n+ Misses          501     478      -23\n```\n| Impacted Files | Coverage \u0394 | Complexity \u0394 | |\n|---|---|---|---|\n| src/Sdk.php | 90% <\u00f8> (\u00f8) | 0 <0> (-14) | :arrow_down: |\n| src/Ec2/Ec2Client.php | 100% <\u00f8> (\u00f8) | 0 <0> (-1) | :arrow_down: |\n| src/Handler/GuzzleV6/GuzzleHandler.php | 93.93% <0%> (-6.07%) | 0% <0%> (-11%) | |\n| src/Api/Serializer/Ec2ParamBuilder.php | 69.23% <0%> (-5.77%) | 0% <0%> (-7%) | |\n| src/Api/ApiProvider.php | 96.49% <0%> (-1.63%) | 0% <0%> (-21%) | |\n| src/AwsClientTrait.php | 35.29% <0%> (-1.07%) | 0% <0%> (-14%) | |\n| src/TraceMiddleware.php | 96.27% <0%> (-0.26%) | 0% <0%> (-45%) | |\n| src/Rds/RdsClient.php | 100% <0%> (\u00f8) | 0% <0%> (-1%) | :arrow_down: |\n| src/History.php | 100% <0%> (\u00f8) | 0% <0%> (-20%) | :arrow_down: |\n| src/S3/RetryableMalformedResponseParser.php | 100% <0%> (\u00f8) | 0% <0%> (-3%) | :arrow_down: |\n| ... and 78 more | |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 8c12f61...1533baf. Read the comment docs.\n. # Codecov Report\n:exclamation: No coverage uploaded for pull request base (master@05df188). Click here to learn what that means.\nThe diff coverage is n/a.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster    #1258   +/-\n=========================================\n  Coverage          ?   92.24%         \n  Complexity        ?     2166         \n=========================================\n  Files             ?      140         \n  Lines             ?     6139         \n  Branches          ?        0         \n=========================================\n  Hits              ?     5663         \n  Misses            ?      476         \n  Partials          ?        0\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 05df188...735474f. Read the comment docs.\n. # Codecov Report\n:exclamation: No coverage uploaded for pull request base (master@05df188). Click here to learn what that means.\nThe diff coverage is n/a.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster    #1263   +/-\n=========================================\n  Coverage          ?   92.23%         \n  Complexity        ?     2170         \n=========================================\n  Files             ?      142         \n  Lines             ?     6258         \n  Branches          ?        0         \n=========================================\n  Hits              ?     5772         \n  Misses            ?      486         \n  Partials          ?        0\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 05df188...61155a2. Read the comment docs.\n. # Codecov Report\nMerging #1266 into master will increase coverage by 0.06%.\nThe diff coverage is 100%.\n\n\n```diff\n@@             Coverage Diff              @@\nmaster    #1266      +/-\n============================================\n+ Coverage     92.27%   92.33%   +0.06%   \n- Complexity     2179     2222      +43   \n============================================\n  Files           142      142            \n  Lines          6305     6407     +102   \n============================================\n+ Hits           5818     5916      +98   \n- Misses          487      491       +4\n```\n| Impacted Files | Coverage \u0394 | Complexity \u0394 | |\n|---|---|---|---|\n| src/ClientResolver.php | 98.91% <100%> (+0.02%) | 98 <0> (+2) | :arrow_up: |\n| src/S3/Transfer.php | 97.34% <0%> (-0.04%) | 106% <0%> (+35%) | |\n| src/S3/S3Client.php | 100% <0%> (\u00f8) | 51% <0%> (\u00f8) | :arrow_down: |\n| src/Sdk.php | 90.19% <0%> (+0.19%) | 20% <0%> (+6%) | :arrow_up: |\n| src/Api/Shape.php | 86.66% <0%> (+0.95%) | 5% <0%> (\u00f8) | :arrow_down: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update df40f2b...88ae63d. Read the comment docs.\n. # Codecov Report\nMerging #1268 into master will decrease coverage by 0.01%.\nThe diff coverage is n/a.\n\n\n```diff\n@@             Coverage Diff              @@\nmaster    #1268      +/-\n============================================\n- Coverage     92.25%   92.23%   -0.02%   \n- Complexity     2167     2170       +3   \n============================================\n  Files           140      142       +2   \n  Lines          6143     6258     +115   \n============================================\n+ Hits           5667     5772     +105   \n- Misses          476      486      +10\n```\n| Impacted Files | Coverage \u0394 | Complexity \u0394 | |\n|---|---|---|---|\n| src/Api/Shape.php | 85.71% <0%> (-0.96%) | 5% <0%> (\u00f8) | |\n| src/functions.php | 89.69% <0%> (\u00f8) | 0% <0%> (?) | |\n| src/Rds/AuthTokenGenerator.php | 100% <0%> (\u00f8) | 3% <0%> (?) | |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update fd25264...9bdedfd. Read the comment docs.\n. # Codecov Report\nMerging #1269 into master will increase coverage by 0.02%.\nThe diff coverage is 100%.\n\n\n```diff\n@@             Coverage Diff              @@\nmaster    #1269      +/-\n============================================\n+ Coverage     92.25%   92.27%   +0.02%   \n- Complexity     2167     2170       +3   \n============================================\n  Files           140      141       +1   \n  Lines          6143     6162      +19   \n============================================\n+ Hits           5667     5686      +19   \n  Misses          476      476\n```\n| Impacted Files | Coverage \u0394 | Complexity \u0394 | |\n|---|---|---|---|\n| src/Rds/AuthTokenGenerator.php | 100% <100%> (\u00f8) | 3 <3> (?) | |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update fd25264...b0de18c. Read the comment docs.\n. # Codecov Report\nMerging #1270 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster    #1270   +/-\n=========================================\n  Coverage     92.27%   92.27%         \n  Complexity     2170     2170         \n=========================================\n  Files           141      141         \n  Lines          6162     6162         \n=========================================\n  Hits           5686     5686         \n  Misses          476      476\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update a843acc...3038088. Read the comment docs.\n. # Codecov Report\nMerging #1272 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster    #1272   +/-\n=========================================\n  Coverage     92.27%   92.27%         \n  Complexity     2170     2170         \n=========================================\n  Files           141      141         \n  Lines          6162     6162         \n=========================================\n  Hits           5686     5686         \n  Misses          476      476\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update a843acc...4b4c817. Read the comment docs.\n. # Codecov Report\nMerging #1279 into master will increase coverage by <.01%.\nThe diff coverage is n/a.\n\n\n```diff\n@@             Coverage Diff              @@\nmaster    #1279      +/-\n============================================\n+ Coverage     92.23%   92.23%   +<.01%   \n  Complexity     2170     2170            \n============================================\n  Files           142      142            \n  Lines          6258     6259       +1   \n============================================\n+ Hits           5772     5773       +1   \n  Misses          486      486\n```\n| Impacted Files | Coverage \u0394 | Complexity \u0394 | |\n|---|---|---|---|\n| src/Api/Shape.php | 86.66% <0%> (+0.95%) | 5% <0%> (\u00f8) | :arrow_down: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update a80af8a...6cca1f0. Read the comment docs.\n. # Codecov Report\nMerging #1283 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster    #1283   +/-\n=========================================\n  Coverage     92.25%   92.25%         \n  Complexity     2179     2179         \n=========================================\n  Files           142      142         \n  Lines          6279     6279         \n=========================================\n  Hits           5793     5793         \n  Misses          486      486\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 9fc7aa9...89d6e13. Read the comment docs.\n. # Codecov Report\nMerging #1284 into master will increase coverage by 0.06%.\nThe diff coverage is 100%.\n\n\n```diff\n@@             Coverage Diff              @@\nmaster    #1284      +/-\n============================================\n+ Coverage     92.25%   92.32%   +0.06%   \n- Complexity     2179     2196      +17   \n============================================\n  Files           142      142            \n  Lines          6279     6331      +52   \n============================================\n+ Hits           5793     5845      +52   \n  Misses          486      486\n```\n| Impacted Files | Coverage \u0394 | Complexity \u0394 | |\n|---|---|---|---|\n| src/Sqs/SqsClient.php | 100% <100%> (\u00f8) | 31 <9> (+17) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update eb10e43...2abc9b9. Read the comment docs.\n. # Codecov Report\nMerging #1286 into master will decrease coverage by 1.28%.\nThe diff coverage is 100%.\n\n\n```diff\n@@             Coverage Diff              @@\nmaster    #1286      +/-\n============================================\n- Coverage     92.25%   90.97%   -1.29%   \n+ Complexity     2179     2178       -1   \n============================================\n  Files           142      142            \n  Lines          6279     5881     -398   \n============================================\n- Hits           5793     5350     -443   \n- Misses          486      531      +45\n```\n| Impacted Files | Coverage \u0394 | Complexity \u0394 | |\n|---|---|---|---|\n| src/S3/Transfer.php | 97.25% <100%> (-0.06%) | 71 <0> (\u00f8) | |\n| src/Api/Parser/PayloadParserTrait.php | 47.05% <0%> (-11.77%) | 5% <0%> (\u00f8) | |\n| src/Rds/RdsClient.php | 88.88% <0%> (-11.12%) | 1% <0%> (\u00f8) | |\n| src/Ec2/Ec2Client.php | 88.88% <0%> (-11.12%) | 1% <0%> (\u00f8) | |\n| src/Signature/SignatureTrait.php | 80.95% <0%> (-9.53%) | 4% <0%> (\u00f8) | |\n| src/Api/ErrorParser/JsonRpcErrorParser.php | 90.9% <0%> (-9.1%) | 5% <0%> (\u00f8) | |\n| src/S3/RetryableMalformedResponseParser.php | 92.3% <0%> (-7.7%) | 3% <0%> (\u00f8) | |\n| src/Api/Parser/Crc32ValidatingParser.php | 92.3% <0%> (-7.7%) | 4% <0%> (\u00f8) | |\n| src/WrappedHttpHandler.php | 86.76% <0%> (-7.61%) | 16% <0%> (\u00f8) | |\n| src/IdempotencyTokenMiddleware.php | 87.09% <0%> (-7.03%) | 13% <0%> (-1%) | |\n| ... and 82 more | |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update eb10e43...f5e2c01. Read the comment docs.\n. # Codecov Report\nMerging #1287 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster    #1287   +/-\n=========================================\n  Coverage     92.27%   92.27%         \n  Complexity     2179     2179         \n=========================================\n  Files           142      142         \n  Lines          6305     6305         \n=========================================\n  Hits           5818     5818         \n  Misses          487      487\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 5530121...d6118ac. Read the comment docs.\n. # Codecov Report\nMerging #1288 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster    #1288   +/-\n=========================================\n  Coverage     92.27%   92.27%         \n  Complexity     2179     2179         \n=========================================\n  Files           142      142         \n  Lines          6305     6305         \n=========================================\n  Hits           5818     5818         \n  Misses          487      487\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update a8ebe40...612216b. Read the comment docs.\n. # Codecov Report\nMerging #1291 into master will decrease coverage by 0.06%.\nThe diff coverage is 100%.\n\n\n```diff\n@@             Coverage Diff              @@\nmaster    #1291      +/-\n============================================\n- Coverage     92.28%   92.22%   -0.07%   \n  Complexity     2181     2181            \n============================================\n  Files           142      142            \n  Lines          6312     6312            \n============================================\n- Hits           5825     5821       -4   \n- Misses          487      491       +4\n```\n| Impacted Files | Coverage \u0394 | Complexity \u0394 | |\n|---|---|---|---|\n| src/S3/ObjectUploader.php | 91.48% <100%> (-6.39%) | 12 <0> (\u00f8) | |\n| src/Multipart/AbstractUploadManager.php | 81.42% <0%> (-1.43%) | 18% <0%> (\u00f8) | |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update af653f2...a522787. Read the comment docs.\n. # Codecov Report\nMerging #1293 into master will decrease coverage by 2.05%.\nThe diff coverage is n/a.\n\n\n```diff\n@@             Coverage Diff              @@\nmaster    #1293      +/-\n============================================\n- Coverage     92.35%   90.29%   -2.06%   \n============================================\n  Files           142      142            \n  Lines          6367     6005     -362   \n============================================\n- Hits           5880     5422     -458   \n- Misses          487      583      +96\n```\n| Impacted Files | Coverage \u0394 | Complexity \u0394 | |\n|---|---|---|---|\n| src/Handler/GuzzleV6/GuzzleHandler.php | 0% <0%> (-93.94%) | 0% <0%> (-11%) | |\n| src/Ec2/Ec2Client.php | 88.88% <0%> (-11.12%) | 0% <0%> (-1%) | |\n| src/Rds/RdsClient.php | 88.88% <0%> (-11.12%) | 0% <0%> (-1%) | |\n| src/CloudTrail/LogFileIterator.php | 89.58% <0%> (-10.42%) | 0% <0%> (-39%) | |\n| src/Signature/SignatureTrait.php | 80.95% <0%> (-9.53%) | 0% <0%> (-4%) | |\n| src/Api/ErrorParser/JsonRpcErrorParser.php | 90.9% <0%> (-9.1%) | 0% <0%> (-5%) | |\n| src/WrappedHttpHandler.php | 86.76% <0%> (-7.76%) | 0% <0%> (-16%) | |\n| src/S3/RetryableMalformedResponseParser.php | 92.3% <0%> (-7.7%) | 0% <0%> (-3%) | |\n| src/Api/Parser/Crc32ValidatingParser.php | 92.3% <0%> (-7.7%) | 0% <0%> (-4%) | |\n| src/IdempotencyTokenMiddleware.php | 87.09% <0%> (-7.03%) | 0% <0%> (-14%) | |\n| ... and 77 more | |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 374225e...8ede94a. Read the comment docs.\n. # Codecov Report\nMerging #1294 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster    #1294   +/-\n=========================================\n  Coverage     92.35%   92.35%         \n  Complexity     2198     2198         \n=========================================\n  Files           142      142         \n  Lines          6367     6367         \n=========================================\n  Hits           5880     5880         \n  Misses          487      487\n```\n| Impacted Files | Coverage \u0394 | Complexity \u0394 | |\n|---|---|---|---|\n| src/Sqs/SqsClient.php | 100% <\u00f8> (\u00f8) | 31 <0> (\u00f8) | :arrow_down: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 7422680...ecd083d. Read the comment docs.\n. # Codecov Report\nMerging #1297 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster    #1297   +/-\n=========================================\n  Coverage     92.35%   92.35%         \n  Complexity     2198     2198         \n=========================================\n  Files           142      142         \n  Lines          6367     6367         \n=========================================\n  Hits           5880     5880         \n  Misses          487      487\n```\n| Impacted Files | Coverage \u0394 | Complexity \u0394 | |\n|---|---|---|---|\n| src/RetryMiddleware.php | 100% <\u00f8> (\u00f8) | 37 <0> (\u00f8) | :arrow_down: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update ac3d508...4115885. Read the comment docs.\n. # Codecov Report\nMerging #1298 into master will decrease coverage by 0.02%.\nThe diff coverage is 0%.\n\n\n```diff\n@@             Coverage Diff              @@\nmaster    #1298      +/-\n============================================\n- Coverage     92.35%   92.33%   -0.03%   \n- Complexity     2198     2199       +1   \n============================================\n  Files           142      142            \n  Lines          6367     6376       +9   \n============================================\n+ Hits           5880     5887       +7   \n- Misses          487      489       +2\n```\n| Impacted Files | Coverage \u0394 | Complexity \u0394 | |\n|---|---|---|---|\n| src/History.php | 100% <\u00f8> (\u00f8) | 20 <0> (\u00f8) | :arrow_down: |\n| src/LruArrayCache.php | 92% <0%> (-8%) | 11 <1> (+1) | |\n| src/Signature/SignatureProvider.php | 100% <0%> (\u00f8) | 11% <0%> (\u00f8) | :arrow_down: |\n| src/Api/TimestampShape.php | 100% <0%> (\u00f8) | 8% <0%> (\u00f8) | :arrow_down: |\n| src/S3/S3Client.php | 100% <0%> (\u00f8) | 51% <0%> (\u00f8) | :arrow_down: |\n| src/AwsClient.php | 100% <0%> (\u00f8) | 30% <0%> (\u00f8) | :arrow_down: |\n| src/S3/PutObjectUrlMiddleware.php | 100% <0%> (\u00f8) | 10% <0%> (\u00f8) | :arrow_down: |\n| src/DynamoDb/Marshaler.php | 98.11% <0%> (+0.01%) | 52% <0%> (\u00f8) | :arrow_down: |\n| src/Api/Shape.php | 86.66% <0%> (+0.95%) | 5% <0%> (\u00f8) | :arrow_down: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 1342e21...93ba536. Read the comment docs.\n. # Codecov Report\nMerging #1299 into master will increase coverage by 0.27%.\nThe diff coverage is 98.3%.\n\n\n```diff\n@@             Coverage Diff              @@\nmaster    #1299      +/-\n============================================\n+ Coverage     92.35%   92.63%   +0.27%   \n- Complexity     2198     2304     +106   \n============================================\n  Files           142      142            \n  Lines          6367     6649     +282   \n============================================\n+ Hits           5880     6159     +279   \n- Misses          487      490       +3\n```\n| Impacted Files | Coverage \u0394 | Complexity \u0394 | |\n|---|---|---|---|\n| src/S3/S3Client.php | 100% <100%> (\u00f8) | 51 <0> (\u00f8) | :arrow_down: |\n| src/S3/S3EndpointMiddleware.php | 99.06% <97.67%> (-0.94%) | 41 <22> (+15) | |\n| src/RetryMiddleware.php | 100% <0%> (\u00f8) | 67% <0%> (+30%) | :arrow_up: |\n| src/Sqs/SqsClient.php | 100% <0%> (\u00f8) | 35% <0%> (+4%) | :arrow_up: |\n| src/S3/Transfer.php | 97.44% <0%> (+0.06%) | 93% <0%> (+22%) | :arrow_up: |\n| src/Sdk.php | 90.19% <0%> (+0.19%) | 20% <0%> (+6%) | :arrow_up: |\n| src/ClientResolver.php | 99.5% <0%> (+0.59%) | 127% <0%> (+29%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 1342e21...d58588f. Read the comment docs.\n. # Codecov Report\nMerging #1303 into master will decrease coverage by <.01%.\nThe diff coverage is n/a.\n\n\n```diff\n@@             Coverage Diff              @@\nmaster    #1303      +/-\n============================================\n- Coverage     92.39%   92.38%   -0.01%   \n- Complexity     2213     2219       +6   \n============================================\n  Files           142      142            \n  Lines          6414     6435      +21   \n============================================\n+ Hits           5926     5945      +19   \n- Misses          488      490       +2\n```\n| Impacted Files | Coverage \u0394 | Complexity \u0394 | |\n|---|---|---|---|\n| src/Sdk.php | 90.19% <0%> (+0.19%) | 20% <0%> (+6%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update ad2206b...133d010. Read the comment docs.\n. # Codecov Report\nMerging #1304 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster    #1304   +/-\n=========================================\n  Coverage     92.39%   92.39%         \n  Complexity     2213     2213         \n=========================================\n  Files           142      142         \n  Lines          6414     6414         \n=========================================\n  Hits           5926     5926         \n  Misses          488      488\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 1f19f74...db156fc. Read the comment docs.\n. # Codecov Report\nMerging #1309 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster    #1309   +/-\n=========================================\n  Coverage     92.39%   92.39%         \n  Complexity     2213     2213         \n=========================================\n  Files           142      142         \n  Lines          6414     6414         \n=========================================\n  Hits           5926     5926         \n  Misses          488      488\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 7654000...d8c1c6f. Read the comment docs.\n. # Codecov Report\nMerging #1313 into master will increase coverage by 0.02%.\nThe diff coverage is 100%.\n\n\n```diff\n@@             Coverage Diff              @@\nmaster    #1313      +/-\n============================================\n+ Coverage     92.39%   92.41%   +0.02%   \n- Complexity     2206     2216      +10   \n============================================\n  Files           142      142            \n  Lines          6414     6448      +34   \n============================================\n+ Hits           5926     5959      +33   \n- Misses          488      489       +1\n```\n| Impacted Files | Coverage \u0394 | Complexity \u0394 | |\n|---|---|---|---|\n| src/S3/MultipartCopy.php | 89.85% <100%> (+0.62%) | 19 <0> (+2) | :arrow_up: |\n| src/S3/MultipartUploader.php | 98.07% <100%> (+0.2%) | 14 <0> (+2) | :arrow_up: |\n| src/S3/ObjectCopier.php | 95.74% <100%> (+0.5%) | 10 <0> (+1) | :arrow_up: |\n| src/S3/MultipartUploadingTrait.php | 100% <100%> (\u00f8) | 16 <0> (+2) | :arrow_up: |\n| src/S3/ObjectUploader.php | 100% <100%> (+2.12%) | 9 <0> (-3) | :arrow_down: |\n| src/Sdk.php | 90.19% <0%> (+0.19%) | 20% <0%> (+6%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update d234cb5...d89459b. Read the comment docs.\n. # Codecov Report\nMerging #1314 into master will decrease coverage by 0.04%.\nThe diff coverage is 40%.\n\n\n```diff\n@@             Coverage Diff              @@\nmaster    #1314      +/-\n============================================\n- Coverage     92.39%   92.34%   -0.05%   \n- Complexity     2206     2207       +1   \n============================================\n  Files           142      142            \n  Lines          6414     6418       +4   \n============================================\n+ Hits           5926     5927       +1   \n- Misses          488      491       +3\n```\n| Impacted Files | Coverage \u0394 | Complexity \u0394 | |\n|---|---|---|---|\n| src/S3/SSECMiddleware.php | 89.65% <40%> (-10.35%) | 11 <0> (+1) | |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update d234cb5...10eaf1f. Read the comment docs.\n. # Codecov Report\nMerging #1317 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster    #1317   +/-\n=========================================\n  Coverage     92.39%   92.39%         \n  Complexity     2206     2206         \n=========================================\n  Files           142      142         \n  Lines          6414     6414         \n=========================================\n  Hits           5926     5926         \n  Misses          488      488\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 69696dd...bfa905d. Read the comment docs.\n. # Codecov Report\nMerging #1318 into master will increase coverage by 0.11%.\nThe diff coverage is n/a.\n\n\n```diff\n@@             Coverage Diff             @@\nmaster   #1318      +/-\n===========================================\n+ Coverage     92.39%   92.5%   +0.11%   \n- Complexity     2206    2243      +37   \n===========================================\n  Files           142     142            \n  Lines          6414    6538     +124   \n===========================================\n+ Hits           5926    6048     +122   \n- Misses          488     490       +2\n```\n| Impacted Files | Coverage \u0394 | Complexity \u0394 | |\n|---|---|---|---|\n| src/S3/MultipartUploadingTrait.php | 100% <0%> (\u00f8) | 20% <0%> (+6%) | :arrow_up: |\n| src/S3/ObjectCopier.php | 95.65% <0%> (+0.41%) | 13% <0%> (+4%) | :arrow_up: |\n| src/S3/MultipartCopy.php | 90% <0%> (+0.76%) | 24% <0%> (+7%) | :arrow_up: |\n| src/S3/ObjectUploader.php | 100% <0%> (+2.12%) | 20% <0%> (+8%) | :arrow_up: |\n| src/S3/MultipartUploader.php | 100% <0%> (+2.12%) | 24% <0%> (+12%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 69696dd...63bd79e. Read the comment docs.\n. # Codecov Report\nMerging #1320 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster    #1320   +/-\n=========================================\n  Coverage     92.42%   92.42%         \n  Complexity     2210     2210         \n=========================================\n  Files           142      142         \n  Lines          6426     6426         \n=========================================\n  Hits           5939     5939         \n  Misses          487      487\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 21480f1...782b765. Read the comment docs.\n. # Codecov Report\nMerging #1320 into master will decrease coverage by 0.54%.\nThe diff coverage is n/a.\n\n\n```diff\n@@             Coverage Diff              @@\nmaster    #1320      +/-\n============================================\n- Coverage     92.42%   91.87%   -0.55%   \n- Complexity     2210     2466     +256   \n============================================\n  Files           142      142            \n  Lines          6426     7523    +1097   \n============================================\n+ Hits           5939     6912     +973   \n- Misses          487      611     +124\n```\n| Impacted Files | Coverage \u0394 | Complexity \u0394 | |\n|---|---|---|---|\n| src/Handler/GuzzleV6/GuzzleHandler.php | 84.78% <0%> (-9.16%) | 14% <0%> (+3%) | |\n| src/LruArrayCache.php | 93.33% <0%> (-6.67%) | 11% <0%> (+1%) | |\n| src/S3/AmbiguousSuccessParser.php | 95% <0%> (-5%) | 6% <0%> (\u00f8) | |\n| src/CloudFront/CloudFrontClient.php | 14.28% <0%> (-3.9%) | 11% <0%> (\u00f8) | |\n| src/CognitoIdentity/CognitoIdentityProvider.php | 92.85% <0%> (-3.15%) | 5% <0%> (\u00f8) | |\n| src/S3/Exception/S3MultipartUploadException.php | 92% <0%> (-2.74%) | 14% <0%> (\u00f8) | |\n| src/Signature/SignatureTrait.php | 88% <0%> (-2.48%) | 4% <0%> (\u00f8) | |\n| src/Credentials/Credentials.php | 57.89% <0%> (-2.11%) | 11% <0%> (\u00f8) | |\n| src/IdempotencyTokenMiddleware.php | 92.1% <0%> (-2.02%) | 14% <0%> (\u00f8) | |\n| src/Sdk.php | 88.23% <0%> (-1.77%) | 14% <0%> (\u00f8) | |\n| ... and 124 more | |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 21480f1...550a325. Read the comment docs.\n. # Codecov Report\nMerging #1325 into master will increase coverage by <.01%.\nThe diff coverage is 100%.\n\n\n```diff\n@@             Coverage Diff              @@\nmaster    #1325      +/-\n============================================\n+ Coverage     91.87%   91.87%   +<.01%   \n- Complexity     2465     2466       +1   \n============================================\n  Files           142      142            \n  Lines          7518     7523       +5   \n============================================\n+ Hits           6907     6912       +5   \n  Misses          611      611\n```\n| Impacted Files | Coverage \u0394 | Complexity \u0394 | |\n|---|---|---|---|\n| src/Signature/SignatureV4.php | 100% <100%> (\u00f8) | 45 <6> (+1) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 7c34cf0...851c29f. Read the comment docs.\n. # Codecov Report\nMerging #1327 into master will increase coverage by <.01%.\nThe diff coverage is 100%.\n\n\n```diff\n@@             Coverage Diff              @@\nmaster    #1327      +/-\n============================================\n+ Coverage     92.42%   92.42%   +<.01%   \n- Complexity     2210     2211       +1   \n============================================\n  Files           142      142            \n  Lines          6426     6430       +4   \n============================================\n+ Hits           5939     5943       +4   \n  Misses          487      487\n```\n| Impacted Files | Coverage \u0394 | Complexity \u0394 | |\n|---|---|---|---|\n| src/S3/SSECMiddleware.php | 100% <100%> (\u00f8) | 11 <0> (+1) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 582e776...ed0b43e. Read the comment docs.\n. # Codecov Report\nMerging #1328 into master will increase coverage by <.01%.\nThe diff coverage is 100%.\n\n\n```diff\n@@             Coverage Diff              @@\nmaster    #1328      +/-\n============================================\n+ Coverage     92.42%   92.42%   +<.01%   \n  Complexity     2210     2210            \n============================================\n  Files           142      142            \n  Lines          6426     6427       +1   \n============================================\n+ Hits           5939     5940       +1   \n  Misses          487      487\n```\n| Impacted Files | Coverage \u0394 | Complexity \u0394 | |\n|---|---|---|---|\n| src/S3/MultipartUploader.php | 98.11% <100%> (+0.03%) | 14 <0> (\u00f8) | :arrow_down: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 582e776...84eb605. Read the comment docs.\n. # Codecov Report\nMerging #1330 into master will decrease coverage by 0.59%.\nThe diff coverage is 100%.\n\n\n```diff\n@@             Coverage Diff             @@\nmaster    #1330     +/-\n===========================================\n- Coverage     92.42%   91.82%   -0.6%   \n- Complexity     2210     2234     +24   \n===========================================\n  Files           142      142           \n  Lines          6427     7427   +1000   \n===========================================\n+ Hits           5940     6820    +880   \n- Misses          487      607    +120\n```\n| Impacted Files | Coverage \u0394 | Complexity \u0394 | |\n|---|---|---|---|\n| src/S3/S3ClientTrait.php | 27.61% <100%> (+10.95%) | 35 <7> (+7) | :arrow_up: |\n| src/S3/S3MultiRegionClient.php | 95.65% <100%> (+1.2%) | 18 <10> (+3) | :arrow_up: |\n| src/Handler/GuzzleV6/GuzzleHandler.php | 84.44% <0%> (-9.5%) | 11% <0%> (\u00f8) | |\n| src/S3/AmbiguousSuccessParser.php | 95% <0%> (-5%) | 6% <0%> (\u00f8) | |\n| src/CloudFront/CloudFrontClient.php | 14.28% <0%> (-3.9%) | 11% <0%> (\u00f8) | |\n| src/CognitoIdentity/CognitoIdentityProvider.php | 92.85% <0%> (-3.15%) | 5% <0%> (\u00f8) | |\n| src/S3/Exception/S3MultipartUploadException.php | 92% <0%> (-2.74%) | 14% <0%> (\u00f8) | |\n| src/Signature/SignatureTrait.php | 88% <0%> (-2.48%) | 4% <0%> (\u00f8) | |\n| src/Credentials/Credentials.php | 57.89% <0%> (-2.11%) | 11% <0%> (\u00f8) | |\n| ... and 126 more | |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update bb28d91...db298a8. Read the comment docs.\n. # Codecov Report\nMerging #1339 into master will increase coverage by 0.94%.\nThe diff coverage is n/a.\n\n\n```diff\n@@             Coverage Diff              @@\nmaster    #1339      +/-\n============================================\n+ Coverage     90.92%   91.87%   +0.94%   \n- Complexity     2234     2465     +231   \n============================================\n  Files           142      142            \n  Lines          6449     7518    +1069   \n============================================\n+ Hits           5864     6907    +1043   \n- Misses          585      611      +26\n```\n| Impacted Files | Coverage \u0394 | Complexity \u0394 | |\n|---|---|---|---|\n| src/S3/AmbiguousSuccessParser.php | 95% <0%> (-5%) | 6% <0%> (\u00f8) | |\n| src/CloudFront/CloudFrontClient.php | 14.28% <0%> (-3.9%) | 11% <0%> (\u00f8) | |\n| src/Api/Serializer/Ec2ParamBuilder.php | 73.33% <0%> (-3.59%) | 7% <0%> (\u00f8) | |\n| src/CognitoIdentity/CognitoIdentityProvider.php | 92.85% <0%> (-3.15%) | 5% <0%> (\u00f8) | |\n| src/Signature/SignatureTrait.php | 88% <0%> (-2.48%) | 4% <0%> (\u00f8) | |\n| src/Sdk.php | 88.23% <0%> (-2.09%) | 14% <0%> (\u00f8) | |\n| src/AwsClientTrait.php | 34.28% <0%> (-2.08%) | 14% <0%> (\u00f8) | |\n| src/functions.php | 88.18% <0%> (-1.34%) | 0% <0%> (\u00f8) | |\n| src/Api/ApiProvider.php | 96.87% <0%> (-1.31%) | 21% <0%> (\u00f8) | |\n| src/Waiter.php | 97.97% <0%> (-0.85%) | 41% <0%> (\u00f8) | |\n| ... and 108 more | |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update d34a577...1d07487. Read the comment docs.\n. # Codecov Report\nMerging #1340 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster    #1340   +/-\n=========================================\n  Coverage     91.87%   91.87%         \n  Complexity     2465     2465         \n=========================================\n  Files           142      142         \n  Lines          7518     7518         \n=========================================\n  Hits           6907     6907         \n  Misses          611      611\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 7c34cf0...d7b86cb. Read the comment docs.\n. # Codecov Report\nMerging #1342 into master will decrease coverage by 0.01%.\nThe diff coverage is n/a.\n\n\n```diff\n@@             Coverage Diff              @@\nmaster    #1342      +/-\n============================================\n- Coverage     91.87%   91.86%   -0.02%   \n- Complexity     2466     2471       +5   \n============================================\n  Files           142      142            \n  Lines          7523     7544      +21   \n============================================\n+ Hits           6912     6930      +18   \n- Misses          611      614       +3\n```\n| Impacted Files | Coverage \u0394 | Complexity \u0394 | |\n|---|---|---|---|\n| src/Sdk.php | 87.27% <0%> (-0.97%) | 19% <0%> (+5%) | |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update e49a541...0679fb5. Read the comment docs.\n. # Codecov Report\nMerging #1343 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster    #1343   +/-\n=========================================\n  Coverage     91.87%   91.87%         \n  Complexity     2466     2466         \n=========================================\n  Files           142      142         \n  Lines          7523     7523         \n=========================================\n  Hits           6912     6912         \n  Misses          611      611\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update e49a541...3add05c. Read the comment docs.\n. # Codecov Report\nMerging #1349 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster    #1349   +/-\n=========================================\n  Coverage     91.87%   91.87%         \n  Complexity     2466     2466         \n=========================================\n  Files           142      142         \n  Lines          7523     7523         \n=========================================\n  Hits           6912     6912         \n  Misses          611      611\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 6d9a29b...1a47517. Read the comment docs.\n. # Codecov Report\nMerging #1352 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster    #1352   +/-\n=========================================\n  Coverage     91.87%   91.87%         \n  Complexity     2466     2466         \n=========================================\n  Files           142      142         \n  Lines          7523     7523         \n=========================================\n  Hits           6912     6912         \n  Misses          611      611\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update db5bd8d...c8158db. Read the comment docs.\n. # Codecov Report\nMerging #1353 into master will not change coverage.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster    #1353   +/-\n=========================================\n  Coverage     91.87%   91.87%         \n  Complexity     2466     2466         \n=========================================\n  Files           142      142         \n  Lines          7523     7523         \n=========================================\n  Hits           6912     6912         \n  Misses          611      611\n```\n| Impacted Files | Coverage \u0394 | Complexity \u0394 | |\n|---|---|---|---|\n| src/ClientResolver.php | 98.76% <100%> (\u00f8) | 99 <0> (\u00f8) | :arrow_down: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 980351b...25b762d. Read the comment docs.\n. # Codecov Report\nMerging #1357 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster    #1357   +/-\n=========================================\n  Coverage     91.87%   91.87%         \n  Complexity     2466     2466         \n=========================================\n  Files           142      142         \n  Lines          7523     7523         \n=========================================\n  Hits           6912     6912         \n  Misses          611      611\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 027bf2f...4c6fab9. Read the comment docs.\n. # Codecov Report\nMerging #1359 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster    #1359   +/-\n=========================================\n  Coverage     91.87%   91.87%         \n  Complexity     2466     2466         \n=========================================\n  Files           142      142         \n  Lines          7523     7523         \n=========================================\n  Hits           6912     6912         \n  Misses          611      611\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 2820644...b9f3811. Read the comment docs.\n. # Codecov Report\nMerging #1360 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster    #1360   +/-\n=========================================\n  Coverage     91.87%   91.87%         \n  Complexity     2466     2466         \n=========================================\n  Files           142      142         \n  Lines          7523     7523         \n=========================================\n  Hits           6912     6912         \n  Misses          611      611\n```\n| Impacted Files | Coverage \u0394 | Complexity \u0394 | |\n|---|---|---|---|\n| src/Ses/SesClient.php | 100% <\u00f8> (\u00f8) | 1 <0> (\u00f8) | :arrow_down: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 586a9c0...9c1ccf2. Read the comment docs.\n. # Codecov Report\nMerging #1362 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster    #1362   +/-\n=========================================\n  Coverage     91.87%   91.87%         \n  Complexity     2466     2466         \n=========================================\n  Files           142      142         \n  Lines          7523     7523         \n=========================================\n  Hits           6912     6912         \n  Misses          611      611\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update eee45c7...736823f. Read the comment docs.\n. # Codecov Report\nMerging #1367 into master will increase coverage by 0.03%.\nThe diff coverage is 100%.\n\n\n```diff\n@@             Coverage Diff              @@\nmaster    #1367      +/-\n============================================\n+ Coverage     91.87%   91.91%   +0.03%   \n- Complexity     2466     2470       +4   \n============================================\n  Files           142      142            \n  Lines          7524     7554      +30   \n============================================\n+ Hits           6913     6943      +30   \n  Misses          611      611\n```\n| Impacted Files | Coverage \u0394 | Complexity \u0394 | |\n|---|---|---|---|\n| src/S3/MultipartUploadingTrait.php | 100% <100%> (\u00f8) | 17 <0> (+1) | :arrow_up: |\n| src/Ses/SesClient.php | 100% <0%> (\u00f8) | 2% <0%> (+1%) | :arrow_up: |\n| src/AwsClient.php | 100% <0%> (\u00f8) | 30% <0%> (\u00f8) | :arrow_down: |\n| src/Signature/SignatureProvider.php | 100% <0%> (\u00f8) | 13% <0%> (\u00f8) | :arrow_down: |\n| src/Api/TimestampShape.php | 100% <0%> (\u00f8) | 8% <0%> (\u00f8) | :arrow_down: |\n| src/S3/PutObjectUrlMiddleware.php | 100% <0%> (\u00f8) | 11% <0%> (\u00f8) | :arrow_down: |\n| src/Ec2/Ec2Client.php | 100% <0%> (\u00f8) | 1% <0%> (\u00f8) | :arrow_down: |\n| src/Route53/Route53Client.php | 100% <0%> (\u00f8) | 7% <0%> (+2%) | :arrow_up: |\n| src/DynamoDb/Marshaler.php | 98.3% <0%> (+0.01%) | 52% <0%> (\u00f8) | :arrow_down: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 104d189...1a8388a. Read the comment docs.\n. # Codecov Report\nMerging #1370 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster    #1370   +/-\n=========================================\n  Coverage     91.88%   91.88%         \n  Complexity     2466     2466         \n=========================================\n  Files           142      142         \n  Lines          7529     7529         \n=========================================\n  Hits           6918     6918         \n  Misses          611      611\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 5f10172...8369de0. Read the comment docs.\n. # Codecov Report\nMerging #1371 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster    #1371   +/-\n=========================================\n  Coverage     91.87%   91.87%         \n  Complexity     2466     2466         \n=========================================\n  Files           142      142         \n  Lines          7524     7524         \n=========================================\n  Hits           6913     6913         \n  Misses          611      611\n```\n| Impacted Files | Coverage \u0394 | Complexity \u0394 | |\n|---|---|---|---|\n| src/Api/ShapeMap.php | 100% <0%> (\u00f8) | 7% <0%> (\u00f8) | :arrow_down: |\n| src/Api/Parser/PayloadParserTrait.php | 61.11% <0%> (+2.28%) | 5% <0%> (\u00f8) | :arrow_down: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 0420dcb...541f916. Read the comment docs.\n. # Codecov Report\nMerging #1376 into master will increase coverage by <.01%.\nThe diff coverage is 100%.\n\n\n```diff\n@@             Coverage Diff              @@\nmaster    #1376      +/-\n============================================\n+ Coverage     91.88%   91.88%   +<.01%   \n- Complexity     2466     2468       +2   \n============================================\n  Files           142      142            \n  Lines          7529     7532       +3   \n============================================\n+ Hits           6918     6921       +3   \n  Misses          611      611\n```\n| Impacted Files | Coverage \u0394 | Complexity \u0394 | |\n|---|---|---|---|\n| src/CommandPool.php | 100% <100%> (\u00f8) | 21 <5> (+2) | :arrow_up: |\n| src/Middleware.php | 98.61% <0%> (\u00f8) | 58% <0%> (\u00f8) | :arrow_down: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 5f10172...e2a579a. Read the comment docs.\n. # Codecov Report\nMerging #1382 into master will increase coverage by <.01%.\nThe diff coverage is n/a.\n\n\n```diff\n@@             Coverage Diff              @@\nmaster    #1382      +/-\n============================================\n+ Coverage     91.87%   91.88%   +<.01%   \n  Complexity     2466     2466            \n============================================\n  Files           142      142            \n  Lines          7524     7529       +5   \n============================================\n+ Hits           6913     6918       +5   \n  Misses          611      611\n```\n| Impacted Files | Coverage \u0394 | Complexity \u0394 | |\n|---|---|---|---|\n| src/Api/ShapeMap.php | 100% <0%> (\u00f8) | 7% <0%> (\u00f8) | :arrow_down: |\n| src/S3/PutObjectUrlMiddleware.php | 100% <0%> (\u00f8) | 11% <0%> (\u00f8) | :arrow_down: |\n| src/Signature/SignatureProvider.php | 100% <0%> (\u00f8) | 13% <0%> (\u00f8) | :arrow_down: |\n| src/AwsClient.php | 100% <0%> (\u00f8) | 30% <0%> (\u00f8) | :arrow_down: |\n| src/Api/TimestampShape.php | 100% <0%> (\u00f8) | 8% <0%> (\u00f8) | :arrow_down: |\n| src/DynamoDb/Marshaler.php | 98.3% <0%> (+0.01%) | 52% <0%> (\u00f8) | :arrow_down: |\n| src/Api/Parser/PayloadParserTrait.php | 61.11% <0%> (+2.28%) | 5% <0%> (\u00f8) | :arrow_down: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update f8bb15a...843b0c9. Read the comment docs.\n. # Codecov Report\nMerging #1385 into master will increase coverage by 0.04%.\nThe diff coverage is 100%.\n\n\n```diff\n@@             Coverage Diff              @@\nmaster    #1385      +/-\n============================================\n+ Coverage     91.88%   91.92%   +0.04%   \n- Complexity     2466     2479      +13   \n============================================\n  Files           142      142            \n  Lines          7529     7568      +39   \n============================================\n+ Hits           6918     6957      +39   \n  Misses          611      611\n```\n| Impacted Files | Coverage \u0394 | Complexity \u0394 | |\n|---|---|---|---|\n| src/S3/S3UriParser.php | 100% <100%> (\u00f8) | 21 <3> (+4) | :arrow_up: |\n| src/CommandPool.php | 100% <0%> (\u00f8) | 28% <0%> (+9%) | :arrow_up: |\n| src/Middleware.php | 98.61% <0%> (\u00f8) | 58% <0%> (\u00f8) | :arrow_down: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 4bf27e8...d096a4e. Read the comment docs.\n. # Codecov Report\nMerging #1387 into master will not change coverage.\nThe diff coverage is 100%.\n\n\n```diff\n@@           Coverage Diff            @@\nmaster   #1387   +/-\n========================================\n  Coverage      91.9%   91.9%         \n- Complexity     2472    2473    +1   \n========================================\n  Files           142     142         \n  Lines          7545    7545         \n========================================\n  Hits           6934    6934         \n  Misses          611     611\n```\n| Impacted Files | Coverage \u0394 | Complexity \u0394 | |\n|---|---|---|---|\n| src/S3/MultipartUploadingTrait.php | 100% <100%> (\u00f8) | 17 <0> (+1) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 755ff10...deacd69. Read the comment docs.\n. # Codecov Report\nMerging #1391 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff            @@\nmaster   #1391   +/-\n========================================\n  Coverage      91.9%   91.9%         \n  Complexity     2472    2472         \n========================================\n  Files           142     142         \n  Lines          7545    7545         \n========================================\n  Hits           6934    6934         \n  Misses          611     611\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 010aea6...7e7d88f. Read the comment docs.\n. # Codecov Report\nMerging #1395 into master will decrease coverage by 0.33%.\nThe diff coverage is 95.91%.\n\n\n```diff\n@@             Coverage Diff              @@\nmaster    #1395      +/-\n============================================\n- Coverage      91.9%   91.56%   -0.34%   \n- Complexity     2472     2521      +49   \n============================================\n  Files           142      156      +14   \n  Lines          7545     7045     -500   \n============================================\n- Hits           6934     6451     -483   \n+ Misses          611      594      -17\n```\n| Impacted Files | Coverage \u0394 | Complexity \u0394 | |\n|---|---|---|---|\n| src/TraceMiddleware.php | 96.77% <\u00f8> (+0.62%) | 47 <0> (\u00f8) | :arrow_down: |\n| src/S3/Crypto/HeadersMetadataStrategy.php | 100% <100%> (\u00f8) | 5 <5> (?) | |\n| src/S3/Crypto/InstructionFileMetadataStrategy.php | 100% <100%> (\u00f8) | 6 <6> (?) | |\n| src/Crypto/KmsMaterialsProvider.php | 100% <100%> (\u00f8) | 8 <8> (?) | |\n| src/Crypto/Cipher/Cbc.php | 100% <100%> (\u00f8) | 10 <10> (?) | |\n| src/Crypto/MetadataEnvelope.php | 100% <100%> (\u00f8) | 7 <7> (?) | |\n| src/Crypto/MaterialsProvider.php | 100% <100%> (\u00f8) | 3 <3> (?) | |\n| src/Crypto/DecryptionTrait.php | 92.18% <92.18%> (\u00f8) | 11 <11> (?) | |\n| src/Crypto/AesGcmDecryptingStream.php | 93.1% <93.1%> (\u00f8) | 7 <7> (?) | |\n| src/Crypto/AesGcmEncryptingStream.php | 93.54% <93.54%> (\u00f8) | 8 <8> (?) | |\n| ... and 137 more | |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 1ba6da5...29bfb98. Read the comment docs.\n. # Codecov Report\nMerging #1397 into master will increase coverage by 0.01%.\nThe diff coverage is 100%.\n\n\n```diff\n@@             Coverage Diff              @@\nmaster    #1397      +/-\n============================================\n+ Coverage      91.9%   91.91%   +0.01%   \n- Complexity     2472     2474       +2   \n============================================\n  Files           142      142            \n  Lines          7545     7561      +16   \n============================================\n+ Hits           6934     6950      +16   \n  Misses          611      611\n```\n| Impacted Files | Coverage \u0394 | Complexity \u0394 | |\n|---|---|---|---|\n| src/S3/MultipartUploadingTrait.php | 100% <100%> (\u00f8) | 17 <0> (+1) | :arrow_up: |\n| src/Middleware.php | 98.6% <0%> (-0.01%) | 58% <0%> (\u00f8) | |\n| src/Ec2/Ec2Client.php | 100% <0%> (\u00f8) | 1% <0%> (\u00f8) | :arrow_down: |\n| src/Ses/SesClient.php | 100% <0%> (\u00f8) | 2% <0%> (+1%) | :arrow_up: |\n| src/Rds/RdsClient.php | 100% <0%> (\u00f8) | 1% <0%> (\u00f8) | :arrow_down: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 6a4c264...3007b7e. Read the comment docs.\n. # Codecov Report\nMerging #1403 into master will decrease coverage by 0.01%.\nThe diff coverage is 50%.\n\n\n```diff\n@@             Coverage Diff              @@\nmaster    #1403      +/-\n============================================\n- Coverage      91.9%   91.88%   -0.02%   \n- Complexity     2473     2475       +2   \n============================================\n  Files           142      142            \n  Lines          7544     7546       +2   \n============================================\n+ Hits           6933     6934       +1   \n- Misses          611      612       +1\n```\n| Impacted Files | Coverage \u0394 | Complexity \u0394 | |\n|---|---|---|---|\n| src/S3/PostObjectV4.php | 0% <0%> (\u00f8) | 14 <0> (+1) | :arrow_up: |\n| src/S3/PostObject.php | 100% <100%> (\u00f8) | 14 <0> (+1) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 2109582...9c3a61c. Read the comment docs.\n. # Codecov Report\nMerging #1404 into master will not change coverage.\nThe diff coverage is 50%.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster    #1404   +/-\n=========================================\n  Coverage     91.88%   91.88%         \n  Complexity     2475     2475         \n=========================================\n  Files           142      142         \n  Lines          7546     7546         \n=========================================\n  Hits           6934     6934         \n  Misses          612      612\n```\n| Impacted Files | Coverage \u0394 | Complexity \u0394 | |\n|---|---|---|---|\n| src/S3/PostObjectV4.php | 0% <0%> (\u00f8) | 14 <0> (\u00f8) | :arrow_down: |\n| src/S3/PostObject.php | 100% <100%> (\u00f8) | 14 <0> (\u00f8) | :arrow_down: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 356d36a...9c3a61c. Read the comment docs.\n. # Codecov Report\nMerging #1406 into master will increase coverage by <.01%.\nThe diff coverage is 100%.\n\n\n```diff\n@@             Coverage Diff              @@\nmaster    #1406      +/-\n============================================\n+ Coverage     91.88%   91.89%   +<.01%   \n  Complexity     2475     2475            \n============================================\n  Files           142      142            \n  Lines          7546     7547       +1   \n============================================\n+ Hits           6934     6935       +1   \n  Misses          612      612\n```\n| Impacted Files | Coverage \u0394 | Complexity \u0394 | |\n|---|---|---|---|\n| src/ResultPaginator.php | 100% <100%> (\u00f8) | 27 <0> (\u00f8) | :arrow_down: |\n| src/Api/ShapeMap.php | 100% <0%> (\u00f8) | 7% <0%> (\u00f8) | :arrow_down: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 356d36a...b1d9da4. Read the comment docs.\n. # Codecov Report\nMerging #1411 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster    #1411   +/-\n=========================================\n  Coverage     91.89%   91.89%         \n  Complexity     2475     2475         \n=========================================\n  Files           142      142         \n  Lines          7547     7547         \n=========================================\n  Hits           6935     6935         \n  Misses          612      612\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update adc4e99...fc0b965. Read the comment docs.\n. # Codecov Report\nMerging #1414 into master will decrease coverage by 0.02%.\nThe diff coverage is 33.33%.\n\n\n```diff\n@@             Coverage Diff              @@\nmaster    #1414      +/-\n============================================\n- Coverage     91.89%   91.86%   -0.03%   \n- Complexity     2475     2476       +1   \n============================================\n  Files           142      142            \n  Lines          7547     7551       +4   \n============================================\n+ Hits           6935     6937       +2   \n- Misses          612      614       +2\n```\n| Impacted Files | Coverage \u0394 | Complexity \u0394 | |\n|---|---|---|---|\n| src/ClientResolver.php | 98.16% <33.33%> (-0.61%) | 100 <0> (+1) | |\n| src/Api/ShapeMap.php | 100% <0%> (\u00f8) | 7% <0%> (\u00f8) | :arrow_down: |\n| src/Middleware.php | 98.61% <0%> (\u00f8) | 58% <0%> (\u00f8) | :arrow_down: |\n| src/Api/Shape.php | 86.66% <0%> (+0.95%) | 5% <0%> (\u00f8) | :arrow_down: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update a6d17d3...e64c3a4. Read the comment docs.\n. # Codecov Report\nMerging #1417 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster    #1417   +/-\n=========================================\n  Coverage     91.89%   91.89%         \n  Complexity     2475     2475         \n=========================================\n  Files           142      142         \n  Lines          7547     7547         \n=========================================\n  Hits           6935     6935         \n  Misses          612      612\n```\n| Impacted Files | Coverage \u0394 | Complexity \u0394 | |\n|---|---|---|---|\n| src/CloudFront/CloudFrontClient.php | 14.28% <0%> (\u00f8) | 11% <0%> (\u00f8) | :arrow_down: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update cb1cd05...df7f8a7. Read the comment docs.\n. # Codecov Report\nMerging #1419 into master will increase coverage by <.01%.\nThe diff coverage is 100%.\n\n\n```diff\n@@             Coverage Diff              @@\nmaster    #1419      +/-\n============================================\n+ Coverage     91.89%   91.89%   +<.01%   \n  Complexity     2475     2475            \n============================================\n  Files           142      142            \n  Lines          7547     7548       +1   \n============================================\n+ Hits           6935     6936       +1   \n  Misses          612      612\n```\n| Impacted Files | Coverage \u0394 | Complexity \u0394 | |\n|---|---|---|---|\n| src/DynamoDb/Marshaler.php | 98.31% <100%> (+0.01%) | 52 <0> (\u00f8) | :arrow_down: |\n| src/S3/Exception/S3MultipartUploadException.php | 92% <100%> (\u00f8) | 14 <0> (\u00f8) | :arrow_down: |\n| src/CloudFront/CloudFrontClient.php | 14.28% <0%> (\u00f8) | 11% <0%> (\u00f8) | :arrow_down: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 050aa33...8a14408. Read the comment docs.\n. # Codecov Report\n:exclamation: No coverage uploaded for pull request base (master@b25d801). Click here to learn what that means.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster    #1420   +/-\n=========================================\n  Coverage          ?   91.89%         \n  Complexity        ?     2478         \n=========================================\n  Files             ?      142         \n  Lines             ?     7555         \n  Branches          ?        0         \n=========================================\n  Hits              ?     6943         \n  Misses            ?      612         \n  Partials          ?        0\n```\n| Impacted Files | Coverage \u0394 | Complexity \u0394 | |\n|---|---|---|---|\n| src/PresignUrlMiddleware.php | 100% <100%> (\u00f8) | 9 <0> (?) | |\n| src/Rds/RdsClient.php | 100% <100%> (\u00f8) | 1 <0> (?) | |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update b25d801...81eaa32. Read the comment docs.\n. # Codecov Report\nMerging #1423 into master will increase coverage by 0.69%.\nThe diff coverage is n/a.\n\n\n```diff\n@@             Coverage Diff              @@\nmaster    #1423      +/-\n============================================\n+ Coverage     91.56%   92.25%   +0.69%   \n  Complexity     2525     2525            \n============================================\n  Files           156      156            \n  Lines          7052     6687     -365   \n============================================\n- Hits           6457     6169     -288   \n+ Misses          595      518      -77\n```\n| Impacted Files | Coverage \u0394 | Complexity \u0394 | |\n|---|---|---|---|\n| src/DoctrineCacheAdapter.php | 44.44% <0%> (-2.93%) | 9% <0%> (\u00f8) | |\n| src/Api/Serializer/Ec2ParamBuilder.php | 75% <0%> (-1.93%) | 7% <0%> (\u00f8) | |\n| src/S3/Exception/S3MultipartUploadException.php | 89.47% <0%> (-1.01%) | 14% <0%> (\u00f8) | |\n| src/LruArrayCache.php | 92% <0%> (-0.86%) | 11% <0%> (\u00f8) | |\n| src/CloudSearchDomain/CloudSearchDomainClient.php | 76.92% <0%> (-0.86%) | 7% <0%> (\u00f8) | |\n| src/Multipart/AbstractUploadManager.php | 83.33% <0%> (-0.46%) | 23% <0%> (\u00f8) | |\n| src/S3/MultipartCopy.php | 87.71% <0%> (-0.42%) | 19% <0%> (\u00f8) | |\n| src/Api/Parser/AbstractRestParser.php | 93.75% <0%> (-0.37%) | 27% <0%> (\u00f8) | |\n| src/Sdk.php | 90% <0%> (-0.33%) | 14% <0%> (\u00f8) | |\n| src/Api/Serializer/RestSerializer.php | 95.23% <0%> (-0.27%) | 49% <0%> (\u00f8) | |\n| ... and 120 more | |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 11e3734...5de6a26. Read the comment docs.\n. # Codecov Report\n:exclamation: No coverage uploaded for pull request base (master@f572a05). Click here to learn what that means.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster    #1424   +/-\n=========================================\n  Coverage          ?   93.92%         \n  Complexity        ?     2834         \n=========================================\n  Files             ?      171         \n  Lines             ?     7576         \n  Branches          ?        0         \n=========================================\n  Hits              ?     7116         \n  Misses            ?      460         \n  Partials          ?        0\n```\n| Impacted Files | Coverage \u0394 | Complexity \u0394 | |\n|---|---|---|---|\n| src/S3/StreamWrapper.php | 97.16% <100%> (\u00f8) | 144 <0> (?) | |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update f572a05...c922031. Read the comment docs.\n. # Codecov Report\n:exclamation: No coverage uploaded for pull request base (master@11e3734). Click here to learn what that means.\nThe diff coverage is n/a.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster    #1430   +/-\n=========================================\n  Coverage          ?   91.56%         \n  Complexity        ?     2525         \n=========================================\n  Files             ?      156         \n  Lines             ?     7052         \n  Branches          ?        0         \n=========================================\n  Hits              ?     6457         \n  Misses            ?      595         \n  Partials          ?        0\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 11e3734...cfadb82. Read the comment docs.\n. # Codecov Report\nMerging #1433 into master will increase coverage by 0.12%.\nThe diff coverage is 95.55%.\n\n\n```diff\n@@             Coverage Diff              @@\nmaster    #1433      +/-\n============================================\n+ Coverage     92.25%   92.37%   +0.12%   \n- Complexity     2525     2538      +13   \n============================================\n  Files           156      160       +4   \n  Lines          6687     6729      +42   \n============================================\n+ Hits           6169     6216      +47   \n+ Misses          518      513       -5\n```\n| Impacted Files | Coverage \u0394 | Complexity \u0394 | |\n|---|---|---|---|\n| src/S3/MultipartUploader.php | 97.43% <\u00f8> (\u00f8) | 14 <0> (\u00f8) | :arrow_down: |\n| src/S3/Crypto/S3EncryptionClient.php | 98.83% <\u00f8> (+2.11%) | 22 <0> (-13) | :arrow_down: |\n| src/Crypto/AbstractCryptoClient.php | 100% <\u00f8> (+5.88%) | 1 <0> (-6) | :arrow_down: |\n| src/Glacier/MultipartUploader.php | 98.78% <\u00f8> (\u00f8) | 23 <0> (\u00f8) | :arrow_down: |\n| src/S3/Crypto/S3EncryptionMultipartUploader.php | 100% <100%> (\u00f8) | 9 <9> (?) | |\n| src/Multipart/AbstractUploadManager.php | 83.78% <100%> (+0.45%) | 25 <2> (+2) | :arrow_up: |\n| src/S3/Crypto/CryptoParamsTrait.php | 91.66% <91.66%> (\u00f8) | 13 <13> (?) | |\n| src/Crypto/Cipher/CipherBuilderTrait.php | 93.33% <93.33%> (\u00f8) | 6 <6> (?) | |\n| src/DynamoDb/SetValue.php | 80% <0%> (-20%) | 5% <0%> (\u00f8) | |\n| src/Sdk.php | 90% <0%> (\u00f8) | 14% <0%> (\u00f8) | :arrow_down: |\n| ... and 8 more | |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 3f88cb0...f9be1dc. Read the comment docs.\n. # Codecov Report\nMerging #1435 into master will decrease coverage by 0.02%.\nThe diff coverage is n/a.\n\n\n```diff\n@@             Coverage Diff              @@\nmaster    #1435      +/-\n============================================\n- Coverage     92.25%   92.22%   -0.03%   \n  Complexity     2525     2525            \n============================================\n  Files           156      156            \n  Lines          6687     6687            \n============================================\n- Hits           6169     6167       -2   \n- Misses          518      520       +2\n```\n| Impacted Files | Coverage \u0394 | Complexity \u0394 | |\n|---|---|---|---|\n| src/DynamoDb/SetValue.php | 80% <0%> (-20%) | 5% <0%> (\u00f8) | |\n| src/ApiGateway/ApiGatewayClient.php | 0% <0%> (\u00f8) | 3% <0%> (\u00f8) | :arrow_down: |\n| src/Sdk.php | 90% <0%> (\u00f8) | 14% <0%> (\u00f8) | :arrow_down: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update c1cd58a...76078a2. Read the comment docs.\n. # Codecov Report\nMerging #1440 into master will increase coverage by 0.1%.\nThe diff coverage is 100%.\n\n\n```diff\n@@             Coverage Diff             @@\nmaster    #1440     +/-\n===========================================\n+ Coverage     92.25%   92.36%   +0.1%   \n- Complexity     2525     2527      +2   \n===========================================\n  Files           156      157      +1   \n  Lines          6687     6691      +4   \n===========================================\n+ Hits           6169     6180     +11   \n+ Misses          518      511      -7\n```\n| Impacted Files | Coverage \u0394 | Complexity \u0394 | |\n|---|---|---|---|\n| src/Pinpoint/PinpointClient.php | 100% <100%> (\u00f8) | 2 <2> (?) | |\n| src/Sdk.php | 90% <0%> (\u00f8) | 14% <0%> (\u00f8) | :arrow_down: |\n| src/ApiGateway/ApiGatewayClient.php | 0% <0%> (\u00f8) | 3% <0%> (\u00f8) | :arrow_down: |\n| src/AwsClient.php | 100% <0%> (+1.2%) | 30% <0%> (\u00f8) | :arrow_down: |\n| src/CloudHsm/CloudHsmClient.php | 100% <0%> (+100%) | 3% <0%> (\u00f8) | :arrow_down: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update d0abb0b...f730f29. Read the comment docs.\n. # Codecov Report\nMerging #1443 into master will not change coverage.\nThe diff coverage is 85.18%.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster    #1443   +/-\n=========================================\n  Coverage     92.37%   92.37%         \n  Complexity     2538     2538         \n=========================================\n  Files           160      160         \n  Lines          6729     6729         \n=========================================\n  Hits           6216     6216         \n  Misses          513      513\n```\n| Impacted Files | Coverage \u0394 | Complexity \u0394 | |\n|---|---|---|---|\n| src/S3/S3EndpointMiddleware.php | 99% <100%> (\u00f8) | 41 <0> (\u00f8) | :arrow_down: |\n| src/S3/ObjectUploader.php | 100% <100%> (\u00f8) | 9 <0> (\u00f8) | :arrow_down: |\n| src/DynamoDb/Marshaler.php | 98.09% <100%> (\u00f8) | 52 <0> (\u00f8) | :arrow_down: |\n| src/HashingStream.php | 100% <100%> (\u00f8) | 6 <0> (\u00f8) | :arrow_down: |\n| src/History.php | 100% <100%> (\u00f8) | 20 <0> (\u00f8) | :arrow_down: |\n| src/Waiter.php | 98.8% <100%> (\u00f8) | 41 <0> (\u00f8) | :arrow_down: |\n| src/RetryMiddleware.php | 98.82% <100%> (\u00f8) | 37 <0> (\u00f8) | :arrow_down: |\n| src/Api/Serializer/RestSerializer.php | 95.23% <100%> (\u00f8) | 49 <0> (\u00f8) | :arrow_down: |\n| src/S3/StreamWrapper.php | 97.41% <100%> (\u00f8) | 142 <0> (\u00f8) | :arrow_down: |\n| src/Crypto/MetadataEnvelope.php | 100% <100%> (\u00f8) | 7 <0> (\u00f8) | :arrow_down: |\n| ... and 3 more | |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update e72f243...b30fdf5. Read the comment docs.\n. # Codecov Report\n:exclamation: No coverage uploaded for pull request base (master@9ad17d6). Click here to learn what that means.\nThe diff coverage is n/a.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster    #1444   +/-\n=========================================\n  Coverage          ?   92.37%         \n  Complexity        ?     2538         \n=========================================\n  Files             ?      160         \n  Lines             ?     6729         \n  Branches          ?        0         \n=========================================\n  Hits              ?     6216         \n  Misses            ?      513         \n  Partials          ?        0\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 9ad17d6...cb758c6. Read the comment docs.\n. # Codecov Report\n:exclamation: No coverage uploaded for pull request base (master@9ad17d6). Click here to learn what that means.\nThe diff coverage is n/a.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster    #1445   +/-\n=========================================\n  Coverage          ?   92.37%         \n  Complexity        ?     2538         \n=========================================\n  Files             ?      160         \n  Lines             ?     6729         \n  Branches          ?        0         \n=========================================\n  Hits              ?     6216         \n  Misses            ?      513         \n  Partials          ?        0\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 9ad17d6...69c4fa7. Read the comment docs.\n. # Codecov Report\n:exclamation: No coverage uploaded for pull request base (master@a9468ab). Click here to learn what that means.\nThe diff coverage is n/a.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster    #1448   +/-\n=========================================\n  Coverage          ?   92.37%         \n  Complexity        ?     2538         \n=========================================\n  Files             ?      160         \n  Lines             ?     6729         \n  Branches          ?        0         \n=========================================\n  Hits              ?     6216         \n  Misses            ?      513         \n  Partials          ?        0\n```\n| Impacted Files | Coverage \u0394 | Complexity \u0394 | |\n|---|---|---|---|\n| src/RetryMiddleware.php | 98.82% <\u00f8> (\u00f8) | 37 <0> (?) | |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update a9468ab...9b1976d. Read the comment docs.\n. # Codecov Report\nMerging #1451 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster    #1451   +/-\n=========================================\n  Coverage     92.37%   92.37%         \n  Complexity     2538     2538         \n=========================================\n  Files           160      160         \n  Lines          6729     6729         \n=========================================\n  Hits           6216     6216         \n  Misses          513      513\n```\n| Impacted Files | Coverage \u0394 | Complexity \u0394 | |\n|---|---|---|---|\n| src/RetryMiddleware.php | 98.82% <0%> (\u00f8) | 37% <0%> (\u00f8) | :arrow_down: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update ffcb011...6f67f04. Read the comment docs.\n. # Codecov Report\nMerging #1453 into master will not change coverage.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster    #1453   +/-\n=========================================\n  Coverage     92.37%   92.37%         \n  Complexity     2538     2538         \n=========================================\n  Files           160      160         \n  Lines          6729     6729         \n=========================================\n  Hits           6216     6216         \n  Misses          513      513\n```\n| Impacted Files | Coverage \u0394 | Complexity \u0394 | |\n|---|---|---|---|\n| src/Multipart/AbstractUploadManager.php | 83.78% <\u00f8> (\u00f8) | 25 <0> (\u00f8) | :arrow_down: |\n| src/Middleware.php | 98.23% <\u00f8> (\u00f8) | 38 <0> (\u00f8) | :arrow_down: |\n| src/Handler/GuzzleV6/GuzzleHandler.php | 100% <\u00f8> (\u00f8) | 14 <0> (\u00f8) | :arrow_down: |\n| src/S3/Crypto/S3EncryptionClient.php | 98.83% <\u00f8> (\u00f8) | 22 <0> (\u00f8) | :arrow_down: |\n| src/S3/S3EndpointMiddleware.php | 99% <\u00f8> (\u00f8) | 41 <0> (\u00f8) | :arrow_down: |\n| src/S3/Transfer.php | 97.35% <\u00f8> (\u00f8) | 74 <0> (\u00f8) | :arrow_down: |\n| src/DynamoDb/WriteRequestBatch.php | 100% <\u00f8> (\u00f8) | 36 <0> (\u00f8) | :arrow_down: |\n| src/S3/S3Client.php | 99.27% <\u00f8> (\u00f8) | 66 <0> (\u00f8) | :arrow_down: |\n| src/Crypto/AesGcmDecryptingStream.php | 92.85% <\u00f8> (\u00f8) | 7 <0> (\u00f8) | :arrow_down: |\n| src/Credentials/EcsCredentialProvider.php | 100% <\u00f8> (\u00f8) | 9 <0> (\u00f8) | :arrow_down: |\n| ... and 9 more | |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update e72f243...b7b33e0. Read the comment docs.\n. # Codecov Report\n:exclamation: No coverage uploaded for pull request base (master@77aa275). Click here to learn what that means.\nThe diff coverage is n/a.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster    #1455   +/-\n=========================================\n  Coverage          ?   92.37%         \n  Complexity        ?     2538         \n=========================================\n  Files             ?      160         \n  Lines             ?     6729         \n  Branches          ?        0         \n=========================================\n  Hits              ?     6216         \n  Misses            ?      513         \n  Partials          ?        0\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 77aa275...b2df320. Read the comment docs.\n. # Codecov Report\nMerging #1460 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster    #1460   +/-\n=========================================\n  Coverage     92.37%   92.37%         \n  Complexity     2538     2538         \n=========================================\n  Files           160      160         \n  Lines          6729     6729         \n=========================================\n  Hits           6216     6216         \n  Misses          513      513\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 1f27217...87537c5. Read the comment docs.\n. # Codecov Report\nMerging #1461 into master will not change coverage.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster    #1461   +/-\n=========================================\n  Coverage     92.37%   92.37%         \n  Complexity     2538     2538         \n=========================================\n  Files           160      160         \n  Lines          6729     6729         \n=========================================\n  Hits           6216     6216         \n  Misses          513      513\n```\n| Impacted Files | Coverage \u0394 | Complexity \u0394 | |\n|---|---|---|---|\n| src/PresignUrlMiddleware.php | 100% <100%> (\u00f8) | 9 <0> (\u00f8) | :arrow_down: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 40dc24b...1f91af7. Read the comment docs.\n. # Codecov Report\nMerging #1463 into master will decrease coverage by <.01%.\nThe diff coverage is 100%.\n\n\n```diff\n@@             Coverage Diff              @@\nmaster    #1463      +/-\n============================================\n- Coverage     92.37%   92.36%   -0.01%   \n- Complexity     2538     2549      +11   \n============================================\n  Files           160      160            \n  Lines          6729     6815      +86   \n============================================\n+ Hits           6216     6295      +79   \n- Misses          513      520       +7\n```\n| Impacted Files | Coverage \u0394 | Complexity \u0394 | |\n|---|---|---|---|\n| src/RetryMiddleware.php | 98.93% <100%> (+0.11%) | 42 <0> (+5) | :arrow_up: |\n| src/Handler/GuzzleV6/GuzzleHandler.php | 85.29% <0%> (-14.71%) | 14% <0%> (\u00f8) | |\n| src/S3/StreamWrapper.php | 97.13% <0%> (-0.28%) | 142% <0%> (\u00f8) | |\n| src/Sqs/SqsClient.php | 100% <0%> (\u00f8) | 31% <0%> (\u00f8) | :arrow_down: |\n| src/S3/Crypto/S3EncryptionMultipartUploader.php | 100% <0%> (\u00f8) | 9% <0%> (\u00f8) | :arrow_down: |\n| src/S3/PermanentRedirectMiddleware.php | 100% <0%> (\u00f8) | 8% <0%> (\u00f8) | :arrow_down: |\n| src/Credentials/AssumeRoleCredentialProvider.php | 100% <0%> (\u00f8) | 5% <0%> (\u00f8) | :arrow_down: |\n| src/ResultPaginator.php | 100% <0%> (\u00f8) | 23% <0%> (\u00f8) | :arrow_down: |\n| src/Credentials/EcsCredentialProvider.php | 100% <0%> (\u00f8) | 9% <0%> (\u00f8) | :arrow_down: |\n| src/S3/PutObjectUrlMiddleware.php | 100% <0%> (\u00f8) | 10% <0%> (\u00f8) | :arrow_down: |\n| ... and 17 more | |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 6363e4b...4a70fc3. Read the comment docs.\n. # Codecov Report\nMerging #1464 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster    #1464   +/-\n=========================================\n  Coverage     92.37%   92.37%         \n  Complexity     2538     2538         \n=========================================\n  Files           160      160         \n  Lines          6729     6729         \n=========================================\n  Hits           6216     6216         \n  Misses          513      513\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 4159562...ef260d0. Read the comment docs.\n. # Codecov Report\n:exclamation: No coverage uploaded for pull request base (master@cb9f2ba). Click here to learn what that means.\nThe diff coverage is n/a.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster    #1465   +/-\n=========================================\n  Coverage          ?   92.36%         \n  Complexity        ?     2538         \n=========================================\n  Files             ?      160         \n  Lines             ?     6786         \n  Branches          ?        0         \n=========================================\n  Hits              ?     6268         \n  Misses            ?      518         \n  Partials          ?        0\n```\n| Impacted Files | Coverage \u0394 | Complexity \u0394 | |\n|---|---|---|---|\n| src/PhpHash.php | 100% <\u00f8> (\u00f8) | 12 <0> (?) | |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update cb9f2ba...2df86bf. Read the comment docs.\n. # Codecov Report\n:exclamation: No coverage uploaded for pull request base (master@cb9f2ba). Click here to learn what that means.\nThe diff coverage is n/a.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster    #1466   +/-\n=========================================\n  Coverage          ?   92.37%         \n  Complexity        ?     2538         \n=========================================\n  Files             ?      160         \n  Lines             ?     6729         \n  Branches          ?        0         \n=========================================\n  Hits              ?     6216         \n  Misses            ?      513         \n  Partials          ?        0\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update cb9f2ba...782ee44. Read the comment docs.\n. # Codecov Report\n:exclamation: No coverage uploaded for pull request base (master@c7f3148). Click here to learn what that means.\nThe diff coverage is n/a.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster    #1468   +/-\n=========================================\n  Coverage          ?   92.37%         \n  Complexity        ?     2538         \n=========================================\n  Files             ?      160         \n  Lines             ?     6729         \n  Branches          ?        0         \n=========================================\n  Hits              ?     6216         \n  Misses            ?      513         \n  Partials          ?        0\n```\n| Impacted Files | Coverage \u0394 | Complexity \u0394 | |\n|---|---|---|---|\n| src/Rds/RdsClient.php | 100% <\u00f8> (\u00f8) | 1 <0> (?) | |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update c7f3148...eaf5f67. Read the comment docs.\n. # Codecov Report\n:exclamation: No coverage uploaded for pull request base (master@c7f3148). Click here to learn what that means.\nThe diff coverage is n/a.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster    #1469   +/-\n=========================================\n  Coverage          ?   92.36%         \n  Complexity        ?     2538         \n=========================================\n  Files             ?      160         \n  Lines             ?     6786         \n  Branches          ?        0         \n=========================================\n  Hits              ?     6268         \n  Misses            ?      518         \n  Partials          ?        0\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update c7f3148...89e58aa. Read the comment docs.\n. # Codecov Report\nMerging #1470 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster    #1470   +/-\n=========================================\n  Coverage     92.36%   92.36%         \n  Complexity     2538     2538         \n=========================================\n  Files           160      160         \n  Lines          6786     6786         \n=========================================\n  Hits           6268     6268         \n  Misses          518      518\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update e4785a0...6440137. Read the comment docs.\n. # Codecov Report\nMerging #1471 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster    #1471   +/-\n=========================================\n  Coverage     92.36%   92.36%         \n  Complexity     2538     2538         \n=========================================\n  Files           160      160         \n  Lines          6786     6786         \n=========================================\n  Hits           6268     6268         \n  Misses          518      518\n```\n| Impacted Files | Coverage \u0394 | Complexity \u0394 | |\n|---|---|---|---|\n| src/Rds/RdsClient.php | 100% <\u00f8> (\u00f8) | 1 <0> (\u00f8) | :arrow_down: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update b4a6982...146855f. Read the comment docs.\n. # Codecov Report\nMerging #1473 into master will decrease coverage by <.01%.\nThe diff coverage is 100%.\n\n\n```diff\n@@             Coverage Diff              @@\nmaster    #1473      +/-\n============================================\n- Coverage     92.36%   92.36%   -0.01%   \n- Complexity     2538     2546       +8   \n============================================\n  Files           160      160            \n  Lines          6786     6811      +25   \n============================================\n+ Hits           6268     6291      +23   \n- Misses          518      520       +2\n```\n| Impacted Files | Coverage \u0394 | Complexity \u0394 | |\n|---|---|---|---|\n| src/Credentials/InstanceProfileProvider.php | 100% <100%> (\u00f8) | 13 <0> (+2) | :arrow_up: |\n| src/Sdk.php | 90.19% <0%> (+0.19%) | 20% <0%> (+6%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update ed33085...a3f2446. Read the comment docs.\n. # Codecov Report\nMerging #1474 into master will not change coverage.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster    #1474   +/-\n=========================================\n  Coverage     92.37%   92.37%         \n+ Complexity     2540     2539    -1   \n=========================================\n  Files           160      160         \n  Lines          6790     6790         \n=========================================\n  Hits           6272     6272         \n  Misses          518      518\n```\n| Impacted Files | Coverage \u0394 | Complexity \u0394 | |\n|---|---|---|---|\n| src/Api/Parser/XmlParser.php | 100% <100%> (\u00f8) | 25 <0> (-1) | :arrow_down: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 05ebeb7...7f5bcb2. Read the comment docs.\n. # Codecov Report\nMerging #1478 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster    #1478   +/-\n=========================================\n  Coverage     92.38%   92.38%         \n  Complexity     2544     2544         \n=========================================\n  Files           160      160         \n  Lines          6798     6798         \n=========================================\n  Hits           6280     6280         \n  Misses          518      518\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 1bc98d7...4f7f720. Read the comment docs.\n. # Codecov Report\nMerging #1481 into master will increase coverage by <.01%.\nThe diff coverage is 100%.\n\n\n```diff\n@@             Coverage Diff              @@\nmaster    #1481      +/-\n============================================\n+ Coverage     92.38%   92.38%   +<.01%   \n- Complexity     2544     2560      +16   \n============================================\n  Files           160      160            \n  Lines          6798     6802       +4   \n============================================\n+ Hits           6280     6284       +4   \n  Misses          518      518\n```\n| Impacted Files | Coverage \u0394 | Complexity \u0394 | |\n|---|---|---|---|\n| src/RetryMiddleware.php | 98.97% <100%> (+0.04%) | 58 <0> (+16) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 5f4a419...11ec412. Read the comment docs.\n. # Codecov Report\nMerging #1482 into master will increase coverage by 1.79%.\nThe diff coverage is n/a.\n\n\n```diff\n@@             Coverage Diff              @@\nmaster    #1482      +/-\n============================================\n+ Coverage     92.38%   94.17%   +1.79%   \n  Complexity     2560     2560            \n============================================\n  Files           160      160            \n  Lines          6802     6802            \n============================================\n+ Hits           6284     6406     +122   \n+ Misses          518      396     -122\n```\n| Impacted Files | Coverage \u0394 | Complexity \u0394 | |\n|---|---|---|---|\n| src/S3/S3ClientTrait.php | 96.8% <0%> (+67.02%) | 33% <0%> (\u00f8) | :arrow_down: |\n| src/S3/PostObjectV4.php | 100% <0%> (+100%) | 14% <0%> (\u00f8) | :arrow_down: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 3dc318d...1d533a0. Read the comment docs.\n. # Codecov Report\nMerging #1487 into master will not change coverage.\nThe diff coverage is 88.88%.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster    #1487   +/-\n=========================================\n  Coverage     94.17%   94.17%         \n  Complexity     2560     2560         \n=========================================\n  Files           160      160         \n  Lines          6802     6802         \n=========================================\n  Hits           6406     6406         \n  Misses          396      396\n```\n| Impacted Files | Coverage \u0394 | Complexity \u0394 | |\n|---|---|---|---|\n| src/IdempotencyTokenMiddleware.php | 87.09% <0%> (\u00f8) | 13 <0> (\u00f8) | :arrow_down: |\n| src/S3/S3EndpointMiddleware.php | 99% <100%> (\u00f8) | 41 <0> (\u00f8) | :arrow_down: |\n| src/Api/Serializer/RestSerializer.php | 95.5% <100%> (\u00f8) | 49 <0> (\u00f8) | :arrow_down: |\n| src/History.php | 100% <100%> (\u00f8) | 20 <0> (\u00f8) | :arrow_down: |\n| src/Waiter.php | 98.8% <100%> (\u00f8) | 41 <0> (\u00f8) | :arrow_down: |\n| src/Crypto/EncryptionTrait.php | 96.05% <100%> (\u00f8) | 10 <0> (\u00f8) | :arrow_down: |\n| src/Multipart/AbstractUploadManager.php | 84% <100%> (\u00f8) | 25 <0> (\u00f8) | :arrow_down: |\n| src/DynamoDb/Marshaler.php | 98.09% <100%> (\u00f8) | 52 <0> (\u00f8) | :arrow_down: |\n| src/HandlerList.php | 100% <100%> (\u00f8) | 53 <0> (\u00f8) | :arrow_down: |\n| src/Sdk.php | 90% <100%> (\u00f8) | 14 <0> (\u00f8) | :arrow_down: |\n| ... and 9 more | |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 69ba4e0...cf69bac. Read the comment docs.\n. # Codecov Report\nMerging #1489 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster    #1489   +/-\n=========================================\n  Coverage     94.17%   94.17%         \n  Complexity     2560     2560         \n=========================================\n  Files           160      160         \n  Lines          6802     6802         \n=========================================\n  Hits           6406     6406         \n  Misses          396      396\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 6dd007c...b393dc8. Read the comment docs.\n. # Codecov Report\nMerging #1496 into master will increase coverage by <.01%.\nThe diff coverage is 100%.\n\n\n```diff\n@@             Coverage Diff              @@\nmaster    #1496      +/-\n============================================\n+ Coverage     94.17%   94.18%   +<.01%   \n- Complexity     2560     2563       +3   \n============================================\n  Files           160      160            \n  Lines          6802     6808       +6   \n============================================\n+ Hits           6406     6412       +6   \n  Misses          396      396\n```\n| Impacted Files | Coverage \u0394 | Complexity \u0394 | |\n|---|---|---|---|\n| src/S3/S3UriParser.php | 100% <100%> (\u00f8) | 24 <0> (+3) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update a6a4656...b12e7eb. Read the comment docs.\n. # Codecov Report\nMerging #1497 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster    #1497   +/-\n=========================================\n  Coverage     94.17%   94.17%         \n  Complexity     2560     2560         \n=========================================\n  Files           160      160         \n  Lines          6802     6802         \n=========================================\n  Hits           6406     6406         \n  Misses          396      396\n```\n| Impacted Files | Coverage \u0394 | Complexity \u0394 | |\n|---|---|---|---|\n| src/S3/Transfer.php | 97.36% <\u00f8> (\u00f8) | 74 <0> (\u00f8) | :arrow_down: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update fbbbf39...e23449c. Read the comment docs.\n. # Codecov Report\nMerging #1500 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster    #1500   +/-\n=========================================\n  Coverage     94.17%   94.17%         \n  Complexity     2560     2560         \n=========================================\n  Files           160      160         \n  Lines          6802     6802         \n=========================================\n  Hits           6406     6406         \n  Misses          396      396\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update ef20dc8...e1a0b53. Read the comment docs.\n. # Codecov Report\nMerging #1502 into master will decrease coverage by 0.01%.\nThe diff coverage is 0%.\n\n\n```diff\n@@             Coverage Diff              @@\nmaster    #1502      +/-\n============================================\n- Coverage     94.17%   94.16%   -0.02%   \n  Complexity     2560     2560            \n============================================\n  Files           160      160            \n  Lines          6802     6803       +1   \n============================================\n  Hits           6406     6406            \n- Misses          396      397       +1\n```\n| Impacted Files | Coverage \u0394 | Complexity \u0394 | |\n|---|---|---|---|\n| src/CloudFront/Signer.php | 0% <0%> (\u00f8) | 10 <0> (\u00f8) | :arrow_down: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update ebd052e...981b7da. Read the comment docs.\n. # Codecov Report\nMerging #1506 into master will not change coverage.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster    #1506   +/-\n=========================================\n  Coverage     94.16%   94.16%         \n  Complexity     2560     2560         \n=========================================\n  Files           160      160         \n  Lines          6803     6803         \n=========================================\n  Hits           6406     6406         \n  Misses          397      397\n```\n| Impacted Files | Coverage \u0394 | Complexity \u0394 | |\n|---|---|---|---|\n| src/RetryMiddleware.php | 98.97% <100%> (\u00f8) | 58 <0> (\u00f8) | :arrow_down: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 299ac47...4e55883. Read the comment docs.\n. # Codecov Report\nMerging #1508 into master will increase coverage by <.01%.\nThe diff coverage is 100%.\n\n\n```diff\n@@             Coverage Diff              @@\nmaster    #1508      +/-\n============================================\n+ Coverage     94.16%   94.16%   +<.01%   \n- Complexity     2560     2562       +2   \n============================================\n  Files           160      160            \n  Lines          6803     6805       +2   \n============================================\n+ Hits           6406     6408       +2   \n  Misses          397      397\n```\n| Impacted Files | Coverage \u0394 | Complexity \u0394 | |\n|---|---|---|---|\n| src/RetryMiddleware.php | 99% <100%> (+0.02%) | 60 <0> (+2) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update e33c7bb...a8e9e74. Read the comment docs.\n. # Codecov Report\nMerging #1509 into master will increase coverage by <.01%.\nThe diff coverage is 100%.\n\n\n```diff\n@@             Coverage Diff              @@\nmaster    #1509      +/-\n============================================\n+ Coverage     94.16%   94.16%   +<.01%   \n- Complexity     2562     2563       +1   \n============================================\n  Files           160      160            \n  Lines          6805     6807       +2   \n============================================\n+ Hits           6408     6410       +2   \n  Misses          397      397\n```\n| Impacted Files | Coverage \u0394 | Complexity \u0394 | |\n|---|---|---|---|\n| src/Signature/S3SignatureV4.php | 100% <100%> (\u00f8) | 7 <0> (+1) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 30d0d9a...9e9837e. Read the comment docs.\n. # Codecov Report\nMerging #1514 into master will not change coverage.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster    #1514   +/-\n=========================================\n  Coverage     94.16%   94.16%         \n  Complexity     2563     2563         \n=========================================\n  Files           160      160         \n  Lines          6807     6807         \n=========================================\n  Hits           6410     6410         \n  Misses          397      397\n```\n| Impacted Files | Coverage \u0394 | Complexity \u0394 | |\n|---|---|---|---|\n| src/Signature/SignatureV4.php | 100% <100%> (\u00f8) | 45 <0> (\u00f8) | :arrow_down: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 4b8286d...b725691. Read the comment docs.\n. # Codecov Report\nMerging #1517 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster    #1517   +/-\n=========================================\n  Coverage     94.16%   94.16%         \n  Complexity     2563     2563         \n=========================================\n  Files           160      160         \n  Lines          6807     6807         \n=========================================\n  Hits           6410     6410         \n  Misses          397      397\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update c414752...48ff8da. Read the comment docs.\n. # Codecov Report\nMerging #1519 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster    #1519   +/-\n=========================================\n  Coverage     94.16%   94.16%         \n  Complexity     2563     2563         \n=========================================\n  Files           160      160         \n  Lines          6807     6807         \n=========================================\n  Hits           6410     6410         \n  Misses          397      397\n```\n| Impacted Files | Coverage \u0394 | Complexity \u0394 | |\n|---|---|---|---|\n| src/DynamoDb/DynamoDbClient.php | 100% <0%> (\u00f8) | 9% <0%> (\u00f8) | :arrow_down: |\n| src/Ec2/Ec2Client.php | 100% <0%> (\u00f8) | 1% <0%> (\u00f8) | :arrow_down: |\n| src/Sdk.php | 90% <0%> (\u00f8) | 14% <0%> (\u00f8) | :arrow_down: |\n| src/CloudFront/CloudFrontClient.php | 18.18% <0%> (\u00f8) | 11% <0%> (\u00f8) | :arrow_down: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 2255939...d14c8e3. Read the comment docs.\n. # Codecov Report\nMerging #1523 into master will increase coverage by 0.01%.\nThe diff coverage is 100%.\n\n\n```diff\n@@             Coverage Diff              @@\nmaster    #1523      +/-\n============================================\n+ Coverage     94.16%   94.18%   +0.01%   \n- Complexity     2563     2570       +7   \n============================================\n  Files           160      160            \n  Lines          6807     6823      +16   \n============================================\n+ Hits           6410     6426      +16   \n  Misses          397      397\n```\n| Impacted Files | Coverage \u0394 | Complexity \u0394 | |\n|---|---|---|---|\n| src/Signature/SignatureV4.php | 100% <100%> (\u00f8) | 51 <5> (+6) | :arrow_up: |\n| src/Credentials/EcsCredentialProvider.php | 100% <0%> (\u00f8) | 9% <0%> (\u00f8) | :arrow_down: |\n| src/Sdk.php | 90% <0%> (\u00f8) | 14% <0%> (\u00f8) | :arrow_down: |\n| src/Pinpoint/PinpointClient.php | 100% <0%> (\u00f8) | 2% <0%> (\u00f8) | :arrow_down: |\n| src/Ec2/Ec2Client.php | 100% <0%> (\u00f8) | 1% <0%> (\u00f8) | :arrow_down: |\n| src/DynamoDb/DynamoDbClient.php | 100% <0%> (\u00f8) | 9% <0%> (\u00f8) | :arrow_down: |\n| src/Rds/RdsClient.php | 100% <0%> (\u00f8) | 1% <0%> (\u00f8) | :arrow_down: |\n| src/Api/Service.php | 85.08% <0%> (+0.26%) | 43% <0%> (+1%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 6c22515...43f7365. Read the comment docs.\n. # Codecov Report\nMerging #1531 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster    #1531   +/-\n=========================================\n  Coverage     94.16%   94.16%         \n  Complexity     2563     2563         \n=========================================\n  Files           160      160         \n  Lines          6807     6807         \n=========================================\n  Hits           6410     6410         \n  Misses          397      397\n```\n| Impacted Files | Coverage \u0394 | Complexity \u0394 | |\n|---|---|---|---|\n| src/Waiter.php | 98.8% <\u00f8> (\u00f8) | 41 <0> (\u00f8) | :arrow_down: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 6642a13...8a1e5dc. Read the comment docs.\n. # Codecov Report\nMerging #1541 into master will increase coverage by 0.01%.\nThe diff coverage is n/a.\n\n\n```diff\n@@             Coverage Diff              @@\nmaster    #1541      +/-\n============================================\n+ Coverage     94.16%   94.18%   +0.01%   \n- Complexity     2563     2569       +6   \n============================================\n  Files           160      160            \n  Lines          6807     6822      +15   \n============================================\n+ Hits           6410     6425      +15   \n  Misses          397      397\n```\n| Impacted Files | Coverage \u0394 | Complexity \u0394 | |\n|---|---|---|---|\n| src/DynamoDb/DynamoDbClient.php | 100% <0%> (\u00f8) | 15% <0%> (+6%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 8c3093d...1f8fac4. Read the comment docs.\n. # Codecov Report\nMerging #1542 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster    #1542   +/-\n=========================================\n  Coverage     94.16%   94.16%         \n  Complexity     2563     2563         \n=========================================\n  Files           160      160         \n  Lines          6807     6807         \n=========================================\n  Hits           6410     6410         \n  Misses          397      397\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 1215249...a3b2307. Read the comment docs.\n. # Codecov Report\nMerging #1543 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster    #1543   +/-\n=========================================\n  Coverage     94.16%   94.16%         \n  Complexity     2563     2563         \n=========================================\n  Files           160      160         \n  Lines          6807     6807         \n=========================================\n  Hits           6410     6410         \n  Misses          397      397\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update fcafbbd...423f58a. Read the comment docs.\n. # Codecov Report\nMerging #1545 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster    #1545   +/-\n=========================================\n  Coverage     94.16%   94.16%         \n  Complexity     2563     2563         \n=========================================\n  Files           160      160         \n  Lines          6807     6807         \n=========================================\n  Hits           6410     6410         \n  Misses          397      397\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update ec7b972...c93df9b. Read the comment docs.\n. # Codecov Report\nMerging #1554 into master will increase coverage by <.01%.\nThe diff coverage is 100%.\n\n\n```diff\n@@             Coverage Diff              @@\nmaster    #1554      +/-\n============================================\n+ Coverage     94.16%   94.16%   +<.01%   \n  Complexity     2563     2563            \n============================================\n  Files           160      160            \n  Lines          6807     6808       +1   \n============================================\n+ Hits           6410     6411       +1   \n  Misses          397      397\n```\n| Impacted Files | Coverage \u0394 | Complexity \u0394 | |\n|---|---|---|---|\n| src/Credentials/EcsCredentialProvider.php | 100% <100%> (\u00f8) | 9 <0> (\u00f8) | :arrow_down: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update d5b8f10...c0d786f. Read the comment docs.\n. # Codecov Report\nMerging #1555 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster    #1555   +/-\n=========================================\n  Coverage     94.16%   94.16%         \n  Complexity     2563     2563         \n=========================================\n  Files           160      160         \n  Lines          6807     6807         \n=========================================\n  Hits           6410     6410         \n  Misses          397      397\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update d5b8f10...46d6cd6. Read the comment docs.\n. # Codecov Report\nMerging #1558 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster    #1558   +/-\n=========================================\n  Coverage     94.16%   94.16%         \n  Complexity     2563     2563         \n=========================================\n  Files           160      160         \n  Lines          6808     6808         \n=========================================\n  Hits           6411     6411         \n  Misses          397      397\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 6ae50cf...635d878. Read the comment docs.\n. # Codecov Report\nMerging #1559 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster    #1559   +/-\n=========================================\n  Coverage     94.16%   94.16%         \n  Complexity     2563     2563         \n=========================================\n  Files           160      160         \n  Lines          6808     6808         \n=========================================\n  Hits           6411     6411         \n  Misses          397      397\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 09ae633...981599f. Read the comment docs.\n. # Codecov Report\nMerging #1563 into master will not change coverage.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster    #1563   +/-\n=========================================\n  Coverage     94.16%   94.16%         \n  Complexity     2563     2563         \n=========================================\n  Files           160      160         \n  Lines          6808     6808         \n=========================================\n  Hits           6411     6411         \n  Misses          397      397\n```\n| Impacted Files | Coverage \u0394 | Complexity \u0394 | |\n|---|---|---|---|\n| src/Credentials/CredentialProvider.php | 98.38% <100%> (\u00f8) | 64 <0> (\u00f8) | :arrow_down: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 795332f...c65d388. Read the comment docs.\n. # Codecov Report\nMerging #1569 into master will not change coverage.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster    #1569   +/-\n=========================================\n  Coverage     94.16%   94.16%         \n  Complexity     2563     2563         \n=========================================\n  Files           160      160         \n  Lines          6808     6808         \n=========================================\n  Hits           6411     6411         \n  Misses          397      397\n```\n| Impacted Files | Coverage \u0394 | Complexity \u0394 | |\n|---|---|---|---|\n| src/Signature/SignatureV4.php | 100% <100%> (\u00f8) | 45 <0> (\u00f8) | :arrow_down: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 3e588fc...ba740a4. Read the comment docs.\n. # Codecov Report\nMerging #1570 into master will increase coverage by <.01%.\nThe diff coverage is 100%.\n\n\n```diff\n@@             Coverage Diff              @@\nmaster    #1570      +/-\n============================================\n+ Coverage     94.16%   94.17%   +<.01%   \n- Complexity     2563     2564       +1   \n============================================\n  Files           160      160            \n  Lines          6808     6810       +2   \n============================================\n+ Hits           6411     6413       +2   \n  Misses          397      397\n```\n| Impacted Files | Coverage \u0394 | Complexity \u0394 | |\n|---|---|---|---|\n| src/Api/Service.php | 85.08% <100%> (+0.26%) | 43 <1> (+1) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 7df3b62...e8c6738. Read the comment docs.\n. # Codecov Report\nMerging #1571 into master will increase coverage by <.01%.\nThe diff coverage is 100%.\n\n\n```diff\n@@             Coverage Diff              @@\nmaster    #1571      +/-\n============================================\n+ Coverage     94.17%   94.17%   +<.01%   \n- Complexity     2564     2566       +2   \n============================================\n  Files           160      160            \n  Lines          6810     6817       +7   \n============================================\n+ Hits           6413     6420       +7   \n  Misses          397      397\n```\n| Impacted Files | Coverage \u0394 | Complexity \u0394 | |\n|---|---|---|---|\n| src/Api/Service.php | 85.95% <100%> (+0.86%) | 45 <0> (+2) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 146c248...c20f17c. Read the comment docs.\n. # Codecov Report\nMerging #1582 into master will increase coverage by 0.02%.\nThe diff coverage is 100%.\n\n\n```diff\n@@             Coverage Diff              @@\nmaster    #1582      +/-\n============================================\n+ Coverage     94.17%   94.19%   +0.02%   \n- Complexity     2564     2576      +12   \n============================================\n  Files           160      160            \n  Lines          6810     6837      +27   \n============================================\n+ Hits           6413     6440      +27   \n  Misses          397      397\n```\n| Impacted Files | Coverage \u0394 | Complexity \u0394 | |\n|---|---|---|---|\n| src/Api/Serializer/JsonBody.php | 100% <100%> (\u00f8) | 18 <0> (+1) | :arrow_up: |\n| src/Api/Parser/AbstractRestParser.php | 94.02% <100%> (+0.27%) | 29 <0> (+2) | :arrow_up: |\n| src/Api/Parser/JsonParser.php | 100% <100%> (\u00f8) | 14 <0> (+2) | :arrow_up: |\n| src/Api/Parser/XmlParser.php | 100% <100%> (\u00f8) | 27 <0> (+2) | :arrow_up: |\n| src/Api/Serializer/QueryParamBuilder.php | 98.5% <100%> (+0.06%) | 28 <0> (+1) | :arrow_up: |\n| src/Api/Serializer/XmlBody.php | 100% <100%> (\u00f8) | 35 <0> (+1) | :arrow_up: |\n| src/Api/Serializer/RestSerializer.php | 95.91% <100%> (+0.41%) | 52 <0> (+3) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 344df6f...964aeb7. Read the comment docs.\n. # Codecov Report\nMerging #1583 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster    #1583   +/-\n=========================================\n  Coverage     94.17%   94.17%         \n  Complexity     2564     2564         \n=========================================\n  Files           160      160         \n  Lines          6810     6810         \n=========================================\n  Hits           6413     6413         \n  Misses          397      397\n```\n| Impacted Files | Coverage \u0394 | Complexity \u0394 | |\n|---|---|---|---|\n| src/RetryMiddleware.php | 99% <\u00f8> (\u00f8) | 60 <0> (\u00f8) | :arrow_down: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 344df6f...13c0cd1. Read the comment docs.\n. # Codecov Report\nMerging #1584 into master will decrease coverage by <.01%.\nThe diff coverage is 100%.\n\n\n```diff\n@@             Coverage Diff              @@\nmaster    #1584      +/-\n============================================\n- Coverage     94.17%   94.16%   -0.01%   \n+ Complexity     2564     2563       -1   \n============================================\n  Files           160      160            \n  Lines          6810     6808       -2   \n============================================\n- Hits           6413     6411       -2   \n  Misses          397      397\n```\n| Impacted Files | Coverage \u0394 | Complexity \u0394 | |\n|---|---|---|---|\n| src/Signature/S3SignatureV4.php | 100% <100%> (\u00f8) | 6 <0> (-1) | :arrow_down: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 344df6f...4b9ae29. Read the comment docs.\n. # Codecov Report\nMerging #1597 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster    #1597   +/-\n=========================================\n  Coverage     94.19%   94.19%         \n  Complexity     2576     2576         \n=========================================\n  Files           160      160         \n  Lines          6837     6837         \n=========================================\n  Hits           6440     6440         \n  Misses          397      397\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 4aa6687...d8812e9. Read the comment docs.\n. # Codecov Report\nMerging #1599 into master will increase coverage by 0.01%.\nThe diff coverage is 100%.\n\n\n```diff\n@@             Coverage Diff             @@\nmaster   #1599      +/-\n===========================================\n+ Coverage     94.19%   94.2%   +0.01%   \n- Complexity     2576    2582       +6   \n===========================================\n  Files           160     160            \n  Lines          6837    6849      +12   \n===========================================\n+ Hits           6440    6452      +12   \n  Misses          397     397\n```\n| Impacted Files | Coverage \u0394 | Complexity \u0394 | |\n|---|---|---|---|\n| src/Endpoint/PartitionEndpointProvider.php | 100% <100%> (\u00f8) | 17 <6> (+6) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update f027675...727e705. Read the comment docs.\n. # Codecov Report\nMerging #1601 into master will increase coverage by <.01%.\nThe diff coverage is 100%.\n\n\n```diff\n@@             Coverage Diff             @@\nmaster   #1601      +/-\n===========================================\n+ Coverage      94.2%   94.2%   +<.01%   \n- Complexity     2582    2583       +1   \n===========================================\n  Files           160     160            \n  Lines          6849    6854       +5   \n===========================================\n+ Hits           6452    6457       +5   \n  Misses          397     397\n```\n| Impacted Files | Coverage \u0394 | Complexity \u0394 | |\n|---|---|---|---|\n| src/TraceMiddleware.php | 96.68% <100%> (+0.11%) | 48 <1> (+1) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update daff999...cd5cb49. Read the comment docs.\n. # Codecov Report\nMerging #1602 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff            @@\nmaster   #1602   +/-\n========================================\n  Coverage      94.2%   94.2%         \n  Complexity     2583    2583         \n========================================\n  Files           160     160         \n  Lines          6854    6854         \n========================================\n  Hits           6457    6457         \n  Misses          397     397\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update adb4543...e402aad. Read the comment docs.\n. # Codecov Report\nMerging #1603 into master will decrease coverage by 0.55%.\nThe diff coverage is n/a.\n\n\n```diff\n@@             Coverage Diff              @@\nmaster    #1603      +/-\n============================================\n- Coverage      94.2%   93.65%   -0.56%   \n============================================\n  Files           160      160            \n  Lines          6854     6757      -97   \n============================================\n- Hits           6457     6328     -129   \n- Misses          397      429      +32\n```\n| Impacted Files | Coverage \u0394 | Complexity \u0394 | |\n|---|---|---|---|\n| src/Handler/GuzzleV6/GuzzleHandler.php | 0% <0%> (-85.3%) | 0% <0%> (-14%) | |\n| src/RetryMiddleware.php | 95.95% <0%> (-3.05%) | 0% <0%> (-60%) | |\n| src/functions.php | 87.5% <0%> (-2.09%) | 0% <0%> (\u00f8) | |\n| src/CloudSearchDomain/CloudSearchDomainClient.php | 76% <0%> (-0.93%) | 0% <0%> (-7%) | |\n| src/Multipart/AbstractUploadManager.php | 83.33% <0%> (-0.67%) | 0% <0%> (-25%) | |\n| src/Middleware.php | 98.14% <0%> (-0.27%) | 0% <0%> (-38%) | |\n| src/Glacier/GlacierClient.php | 95.91% <0%> (-0.24%) | 0% <0%> (-17%) | |\n| src/S3/S3MultiRegionClient.php | 94.66% <0%> (-0.21%) | 0% <0%> (-26%) | |\n| src/MultiRegionClient.php | 96.2% <0%> (-0.19%) | 0% <0%> (-37%) | |\n| src/S3/BatchDelete.php | 97.26% <0%> (-0.18%) | 0% <0%> (-36%) | |\n| ... and 22 more | |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 22ad993...8e3ccfe. Read the comment docs.\n. # Codecov Report\nMerging #1604 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster    #1604   +/-\n=========================================\n  Coverage     94.21%   94.21%         \n  Complexity     2589     2589         \n=========================================\n  Files           160      160         \n  Lines          6867     6867         \n=========================================\n  Hits           6470     6470         \n  Misses          397      397\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 5b442ca...73d7210. Read the comment docs.\n. # Codecov Report\nMerging #1605 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff            @@\nmaster   #1605   +/-\n========================================\n  Coverage      94.2%   94.2%         \n  Complexity     2583    2583         \n========================================\n  Files           160     160         \n  Lines          6854    6854         \n========================================\n  Hits           6457    6457         \n  Misses          397     397\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 22ad993...5ee9421. Read the comment docs.\n. # Codecov Report\nMerging #1607 into master will not change coverage.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster    #1607   +/-\n=========================================\n  Coverage     94.21%   94.21%         \n  Complexity     2589     2589         \n=========================================\n  Files           160      160         \n  Lines          6867     6867         \n=========================================\n  Hits           6470     6470         \n  Misses          397      397\n```\n| Impacted Files | Coverage \u0394 | Complexity \u0394 | |\n|---|---|---|---|\n| src/Credentials/CredentialProvider.php | 98.38% <100%> (\u00f8) | 64 <0> (\u00f8) | :arrow_down: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update de95515...d664fb5. Read the comment docs.\n. # Codecov Report\nMerging #1608 into master will decrease coverage by 0.55%.\nThe diff coverage is n/a.\n\n\n```diff\n@@             Coverage Diff              @@\nmaster    #1608      +/-\n============================================\n- Coverage     94.21%   93.66%   -0.56%   \n============================================\n  Files           160      160            \n  Lines          6867     6770      -97   \n============================================\n- Hits           6470     6341     -129   \n- Misses          397      429      +32\n```\n| Impacted Files | Coverage \u0394 | Complexity \u0394 | |\n|---|---|---|---|\n| src/Handler/GuzzleV6/GuzzleHandler.php | 0% <0%> (-85.3%) | 0% <0%> (-14%) | |\n| src/RetryMiddleware.php | 95.95% <0%> (-3.05%) | 0% <0%> (-60%) | |\n| src/functions.php | 87.5% <0%> (-2.09%) | 0% <0%> (\u00f8) | |\n| src/CloudSearchDomain/CloudSearchDomainClient.php | 76% <0%> (-0.93%) | 0% <0%> (-7%) | |\n| src/Multipart/AbstractUploadManager.php | 83.33% <0%> (-0.67%) | 0% <0%> (-25%) | |\n| src/Middleware.php | 98.14% <0%> (-0.27%) | 0% <0%> (-38%) | |\n| src/Glacier/GlacierClient.php | 95.91% <0%> (-0.24%) | 0% <0%> (-17%) | |\n| src/S3/S3MultiRegionClient.php | 94.66% <0%> (-0.21%) | 0% <0%> (-26%) | |\n| src/MultiRegionClient.php | 96.2% <0%> (-0.19%) | 0% <0%> (-37%) | |\n| src/S3/BatchDelete.php | 97.26% <0%> (-0.18%) | 0% <0%> (-36%) | |\n| ... and 22 more | |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 0daae62...089b30e. Read the comment docs.\n. # Codecov Report\nMerging #1610 into master will increase coverage by <.01%.\nThe diff coverage is 100%.\n\n\n```diff\n@@             Coverage Diff              @@\nmaster    #1610      +/-\n============================================\n+ Coverage     94.21%   94.22%   +<.01%   \n- Complexity     2589     2590       +1   \n============================================\n  Files           160      160            \n  Lines          6867     6869       +2   \n============================================\n+ Hits           6470     6472       +2   \n  Misses          397      397\n```\n| Impacted Files | Coverage \u0394 | Complexity \u0394 | |\n|---|---|---|---|\n| src/Signature/SignatureV4.php | 100% <100%> (\u00f8) | 52 <0> (+1) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 0daae62...096c65d. Read the comment docs.\n. # Codecov Report\nMerging #1614 into master will not change coverage.\nThe diff coverage is 87.5%.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster    #1614   +/-\n=========================================\n  Coverage     94.22%   94.22%         \n  Complexity     2590     2590         \n=========================================\n  Files           160      160         \n  Lines          6869     6869         \n=========================================\n  Hits           6472     6472         \n  Misses          397      397\n```\n| Impacted Files | Coverage \u0394 | Complexity \u0394 | |\n|---|---|---|---|\n| src/Credentials/InstanceProfileProvider.php | 100% <\u00f8> (\u00f8) | 13 <0> (\u00f8) | :arrow_down: |\n| src/Credentials/EcsCredentialProvider.php | 100% <\u00f8> (\u00f8) | 9 <0> (\u00f8) | :arrow_down: |\n| src/Api/DocModel.php | 0% <0%> (\u00f8) | 12 <0> (\u00f8) | :arrow_down: |\n| src/Glacier/GlacierClient.php | 96.15% <100%> (\u00f8) | 17 <1> (\u00f8) | :arrow_down: |\n| src/DynamoDb/DynamoDbClient.php | 100% <100%> (\u00f8) | 9 <0> (\u00f8) | :arrow_down: |\n| src/S3/StreamWrapper.php | 97.13% <100%> (\u00f8) | 142 <3> (\u00f8) | :arrow_down: |\n| src/S3/S3Client.php | 99.29% <100%> (\u00f8) | 66 <0> (\u00f8) | :arrow_down: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update a77e679...ba02c02. Read the comment docs.\n. # Codecov Report\nMerging #1618 into master will increase coverage by 0.06%.\nThe diff coverage is n/a.\n\n\n```diff\n@@             Coverage Diff              @@\nmaster    #1618      +/-\n============================================\n+ Coverage     94.22%   94.28%   +0.06%   \n- Complexity     2590     2626      +36   \n============================================\n  Files           160      160            \n  Lines          6869     6983     +114   \n============================================\n+ Hits           6472     6584     +112   \n- Misses          397      399       +2\n```\n| Impacted Files | Coverage \u0394 | Complexity \u0394 | |\n|---|---|---|---|\n| src/Signature/SignatureV4.php | 100% <0%> (\u00f8) | 82% <0%> (+30%) | :arrow_up: |\n| src/TraceMiddleware.php | 96.73% <0%> (+0.04%) | 48% <0%> (\u00f8) | :arrow_down: |\n| src/Sdk.php | 90.56% <0%> (+0.56%) | 20% <0%> (+6%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 894658d...3930460. Read the comment docs.\n. # Codecov Report\nMerging #1624 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster    #1624   +/-\n=========================================\n  Coverage     94.22%   94.22%         \n  Complexity     2590     2590         \n=========================================\n  Files           160      160         \n  Lines          6869     6869         \n=========================================\n  Hits           6472     6472         \n  Misses          397      397\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update fa7689b...4eb0833. Read the comment docs.\n. # Codecov Report\nMerging #1627 into master will decrease coverage by 0.27%.\nThe diff coverage is 85.37%.\n\n\n```diff\n@@             Coverage Diff              @@\nmaster    #1627      +/-\n============================================\n- Coverage     94.22%   93.94%   -0.28%   \n- Complexity     2590     2658      +68   \n============================================\n  Files           160      163       +3   \n  Lines          6869     7073     +204   \n============================================\n+ Hits           6472     6645     +173   \n- Misses          397      428      +31\n```\n| Impacted Files | Coverage \u0394 | Complexity \u0394 | |\n|---|---|---|---|\n| src/S3/S3Client.php | 99.29% <\u00f8> (\u00f8) | 66 <0> (\u00f8) | :arrow_down: |\n| src/S3/S3MultiRegionClient.php | 94.87% <\u00f8> (\u00f8) | 26 <0> (\u00f8) | :arrow_down: |\n| src/Api/Parser/AbstractParser.php | 100% <\u00f8> (\u00f8) | 1 <0> (\u00f8) | :arrow_down: |\n| src/S3/RetryableMalformedResponseParser.php | 86.66% <0%> (-13.34%) | 4 <1> (+1) | |\n| src/S3/GetBucketLocationParser.php | 84.61% <0%> (-15.39%) | 6 <1> (+1) | |\n| src/Api/Parser/RestJsonParser.php | 58.33% <0%> (-41.67%) | 6 <2> (+2) | |\n| src/Api/Parser/Crc32ValidatingParser.php | 86.66% <0%> (-13.34%) | 5 <1> (+1) | |\n| src/S3/AmbiguousSuccessParser.php | 88.88% <0%> (-11.12%) | 7 <1> (+1) | |\n| src/Api/Parser/JsonRpcParser.php | 100% <100%> (\u00f8) | 6 <1> (+1) | :arrow_up: |\n| src/Exception/EventStreamDataException.php | 100% <100%> (\u00f8) | 3 <3> (?) | |\n| ... and 8 more | |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 3e59439...cf92c84. Read the comment docs.\n. # Codecov Report\nMerging #1629 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster    #1629   +/-\n=========================================\n  Coverage     93.94%   93.94%         \n  Complexity     2658     2658         \n=========================================\n  Files           163      163         \n  Lines          7073     7073         \n=========================================\n  Hits           6645     6645         \n  Misses          428      428\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 6c0b3df...8ba705e. Read the comment docs.\n. # Codecov Report\nMerging #1635 into master will decrease coverage by 0.04%.\nThe diff coverage is 91.52%.\n\n\n```diff\n@@             Coverage Diff             @@\nmaster   #1635      +/-\n===========================================\n- Coverage     93.94%   93.9%   -0.05%   \n- Complexity     2662    2830     +168   \n===========================================\n  Files           164     171       +7   \n  Lines          7085    7542     +457   \n===========================================\n+ Hits           6656    7082     +426   \n- Misses          429     460      +31\n```\n| Impacted Files | Coverage \u0394 | Complexity \u0394 | |\n|---|---|---|---|\n| src/Result.php | 100% <\u00f8> (\u00f8) | 6 <0> (\u00f8) | :arrow_down: |\n| ...rc/S3/Exception/DeleteMultipleObjectsException.php | 100% <\u00f8> (\u00f8) | 4 <0> (\u00f8) | :arrow_down: |\n| src/Exception/MultipartUploadException.php | 100% <\u00f8> (\u00f8) | 10 <0> (\u00f8) | :arrow_down: |\n| src/Api/Parser/AbstractParser.php | 100% <\u00f8> (\u00f8) | 1 <0> (\u00f8) | :arrow_down: |\n| src/Exception/CouldNotCreateChecksumException.php | 100% <\u00f8> (\u00f8) | 2 <0> (\u00f8) | :arrow_down: |\n| src/S3/RetryableMalformedResponseParser.php | 86.66% <0%> (\u00f8) | 4 <0> (\u00f8) | :arrow_down: |\n| src/S3/GetBucketLocationParser.php | 84.61% <0%> (\u00f8) | 6 <0> (\u00f8) | :arrow_down: |\n| src/MockHandler.php | 86.95% <0%> (-10.61%) | 21 <4> (+4) | |\n| src/functions.php | 84.31% <0%> (-5.27%) | 0 <0> (\u00f8) | |\n| src/Api/Parser/Crc32ValidatingParser.php | 86.66% <0%> (\u00f8) | 5 <0> (\u00f8) | :arrow_down: |\n| ... and 31 more | |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 2986beb...35f2f6c. Read the comment docs.\n. # Codecov Report\nMerging #1636 into master will decrease coverage by <.01%.\nThe diff coverage is 91.66%.\n\n\n```diff\n@@             Coverage Diff              @@\nmaster    #1636      +/-\n============================================\n- Coverage     93.94%   93.94%   -0.01%   \n- Complexity     2658     2662       +4   \n============================================\n  Files           163      164       +1   \n  Lines          7073     7085      +12   \n============================================\n+ Hits           6645     6656      +11   \n- Misses          428      429       +1\n```\n| Impacted Files | Coverage \u0394 | Complexity \u0394 | |\n|---|---|---|---|\n| src/Lambda/LambdaClient.php | 91.66% <91.66%> (\u00f8) | 4 <4> (?) | |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 701152f...72726ce. Read the comment docs.\n. # Codecov Report\nMerging #1639 into master will increase coverage by <.01%.\nThe diff coverage is 100%.\n\n\n```diff\n@@             Coverage Diff              @@\nmaster    #1639      +/-\n============================================\n+ Coverage     93.94%   93.94%   +<.01%   \n- Complexity     2662     2663       +1   \n============================================\n  Files           164      164            \n  Lines          7085     7087       +2   \n============================================\n+ Hits           6656     6658       +2   \n  Misses          429      429\n```\n| Impacted Files | Coverage \u0394 | Complexity \u0394 | |\n|---|---|---|---|\n| src/Api/Serializer/JsonBody.php | 100% <100%> (\u00f8) | 19 <0> (+1) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 86fe79d...e431e15. Read the comment docs.\n. # Codecov Report\nMerging #1641 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff            @@\nmaster   #1641   +/-\n========================================\n  Coverage      93.9%   93.9%         \n  Complexity     2830    2830         \n========================================\n  Files           171     171         \n  Lines          7542    7542         \n========================================\n  Hits           7082    7082         \n  Misses          460     460\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 975bdc3...d44fe55. Read the comment docs.\n. # Codecov Report\nMerging #1642 into master will decrease coverage by <.01%.\nThe diff coverage is 100%.\n\n\n```diff\n@@             Coverage Diff             @@\nmaster   #1642      +/-\n===========================================\n- Coverage      93.9%   93.9%   -0.01%   \n  Complexity     2830    2830            \n===========================================\n  Files           171     171            \n  Lines          7542    7541       -1   \n===========================================\n- Hits           7082    7081       -1   \n  Misses          460     460\n```\n| Impacted Files | Coverage \u0394 | Complexity \u0394 | |\n|---|---|---|---|\n| ...ientSideMonitoring/ApiCallMonitoringMiddleware.php | 96.42% <\u00f8> (-0.13%) | 13 <0> (\u00f8) | |\n| src/ClientSideMonitoring/ConfigurationProvider.php | 99.14% <100%> (\u00f8) | 53 <0> (\u00f8) | :arrow_down: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 65c88e0...cff5ca5. Read the comment docs.\n. # Codecov Report\nMerging #1643 into master will increase coverage by <.01%.\nThe diff coverage is 100%.\n\n\n```diff\n@@             Coverage Diff             @@\nmaster   #1643      +/-\n===========================================\n+ Coverage      93.9%   93.9%   +<.01%   \n- Complexity     2830    2831       +1   \n===========================================\n  Files           171     171            \n  Lines          7542    7553      +11   \n===========================================\n+ Hits           7082    7093      +11   \n  Misses          460     460\n```\n| Impacted Files | Coverage \u0394 | Complexity \u0394 | |\n|---|---|---|---|\n| ...entSideMonitoring/AbstractMonitoringMiddleware.php | 97.93% <\u00f8> (-0.25%) | 41 <0> (-1) | |\n| ...eMonitoring/ApiCallAttemptMonitoringMiddleware.php | 97.01% <100%> (+0.29%) | 37 <1> (+1) | :arrow_up: |\n| ...ientSideMonitoring/ApiCallMonitoringMiddleware.php | 97.56% <100%> (+1%) | 14 <1> (+1) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 92ade99...18e35f1. Read the comment docs.\n. # Codecov Report\nMerging #1644 into master will increase coverage by 0.01%.\nThe diff coverage is 100%.\n\n\n```diff\n@@             Coverage Diff              @@\nmaster    #1644      +/-\n============================================\n+ Coverage     93.91%   93.92%   +0.01%   \n  Complexity     2832     2832            \n============================================\n  Files           171      171            \n  Lines          7554     7572      +18   \n============================================\n+ Hits           7094     7112      +18   \n  Misses          460      460\n```\n| Impacted Files | Coverage \u0394 | Complexity \u0394 | |\n|---|---|---|---|\n| src/RetryMiddleware.php | 98.24% <100%> (+0.11%) | 59 <13> (-5) | :arrow_down: |\n| src/Exception/AwsException.php | 96.66% <100%> (+0.3%) | 28 <2> (+2) | :arrow_up: |\n| ...ientSideMonitoring/ApiCallMonitoringMiddleware.php | 97.82% <100%> (+0.32%) | 17 <3> (+3) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 9cd8ff1...b55ee65. Read the comment docs.\n. # Codecov Report\nMerging #1647 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster    #1647   +/-\n=========================================\n  Coverage     93.92%   93.92%         \n  Complexity     2832     2832         \n=========================================\n  Files           171      171         \n  Lines          7572     7572         \n=========================================\n  Hits           7112     7112         \n  Misses          460      460\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 0c8b65f...556f241. Read the comment docs.\n. # Codecov Report\nMerging #1651 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster    #1651   +/-\n=========================================\n  Coverage     93.92%   93.92%         \n  Complexity     2832     2832         \n=========================================\n  Files           171      171         \n  Lines          7572     7572         \n=========================================\n  Hits           7112     7112         \n  Misses          460      460\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 1a7baa9...354087f. Read the comment docs.\n. # Codecov Report\nMerging #1657 into master will increase coverage by <.01%.\nThe diff coverage is 84.61%.\n\n\n```diff\n@@             Coverage Diff              @@\nmaster    #1657      +/-\n============================================\n+ Coverage     93.92%   93.92%   +<.01%   \n  Complexity     2832     2832            \n============================================\n  Files           171      171            \n  Lines          7572     7574       +2   \n============================================\n+ Hits           7112     7114       +2   \n  Misses          460      460\n```\n| Impacted Files | Coverage \u0394 | Complexity \u0394 | |\n|---|---|---|---|\n| src/functions.php | 84.61% <84.61%> (+0.3%) | 0 <0> (\u00f8) | :arrow_down: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 58af8e2...73f9291. Read the comment docs.\n. # Codecov Report\nMerging #1658 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster    #1658   +/-\n=========================================\n  Coverage     93.92%   93.92%         \n  Complexity     2832     2832         \n=========================================\n  Files           171      171         \n  Lines          7572     7572         \n=========================================\n  Hits           7112     7112         \n  Misses          460      460\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 904e9e2...00f166d. Read the comment docs.\n. # Codecov Report\nMerging #1665 into master will increase coverage by 0.03%.\nThe diff coverage is 100%.\n\n\n```diff\n@@             Coverage Diff              @@\nmaster    #1665      +/-\n============================================\n+ Coverage     93.92%   93.95%   +0.03%   \n- Complexity     2832     2842      +10   \n============================================\n  Files           171      172       +1   \n  Lines          7572     7611      +39   \n============================================\n+ Hits           7112     7151      +39   \n  Misses          460      460\n```\n| Impacted Files | Coverage \u0394 | Complexity \u0394 | |\n|---|---|---|---|\n| src/ClientResolver.php | 98.03% <\u00f8> (\u00f8) | 100 <0> (\u00f8) | :arrow_down: |\n| src/AwsClient.php | 100% <100%> (\u00f8) | 33 <2> (+2) | :arrow_up: |\n| src/functions.php | 84.76% <100%> (+0.44%) | 0 <0> (\u00f8) | :arrow_down: |\n| src/EndpointParameterMiddleware.php | 100% <100%> (\u00f8) | 8 <8> (?) | |\n| src/Sdk.php | 90% <0%> (\u00f8) | 14% <0%> (\u00f8) | :arrow_down: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 8278052...74eabf9. Read the comment docs.\n. # Codecov Report\nMerging #1672 into master will increase coverage by 0.01%.\nThe diff coverage is 100%.\n\n\n```diff\n@@             Coverage Diff              @@\nmaster    #1672      +/-\n============================================\n+ Coverage     93.95%   93.97%   +0.01%   \n- Complexity     2857     2866       +9   \n============================================\n  Files           174      174            \n  Lines          7664     7684      +20   \n============================================\n+ Hits           7201     7221      +20   \n  Misses          463      463\n```\n| Impacted Files | Coverage \u0394 | Complexity \u0394 | |\n|---|---|---|---|\n| src/DynamoDb/DynamoDbClient.php | 100% <100%> (\u00f8) | 9 <0> (\u00f8) | :arrow_down: |\n| src/RetryMiddleware.php | 98.48% <100%> (+0.23%) | 68 <29> (+9) | :arrow_up: |\n| src/Sdk.php | 90% <0%> (\u00f8) | 14% <0%> (\u00f8) | :arrow_down: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 3b6ebec...8949a70. Read the comment docs.\n. # Codecov Report\nMerging #1673 into master will decrease coverage by <.01%.\nThe diff coverage is 100%.\n\n\n```diff\n@@             Coverage Diff              @@\nmaster    #1673      +/-\n============================================\n- Coverage     93.95%   93.95%   -0.01%   \n+ Complexity     2857     2855       -2   \n============================================\n  Files           174      174            \n  Lines          7664     7661       -3   \n============================================\n- Hits           7201     7198       -3   \n  Misses          463      463\n```\n| Impacted Files | Coverage \u0394 | Complexity \u0394 | |\n|---|---|---|---|\n| src/Waiter.php | 98.76% <100%> (-0.05%) | 39 <0> (-2) | |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update e02575a...19d6564. Read the comment docs.\n. # Codecov Report\nMerging #1674 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster    #1674   +/-\n=========================================\n  Coverage     93.92%   93.92%         \n  Complexity     2847     2847         \n=========================================\n  Files           173      173         \n  Lines          7625     7625         \n=========================================\n  Hits           7162     7162         \n  Misses          463      463\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 5e36d7d...a2a1e11. Read the comment docs.\n. # Codecov Report\nMerging #1680 into master will decrease coverage by <.01%.\nThe diff coverage is 100%.\n\n\n```diff\n@@             Coverage Diff              @@\nmaster    #1680      +/-\n============================================\n- Coverage     93.97%   93.97%   -0.01%   \n+ Complexity     2866     2864       -2   \n============================================\n  Files           174      174            \n  Lines          7684     7682       -2   \n============================================\n- Hits           7221     7219       -2   \n  Misses          463      463\n```\n| Impacted Files | Coverage \u0394 | Complexity \u0394 | |\n|---|---|---|---|\n| src/functions.php | 84.9% <100%> (+0.14%) | 0 <0> (\u00f8) | :arrow_down: |\n| src/Waiter.php | 98.76% <0%> (-0.05%) | 39% <0%> (-2%) | |\n| src/Lambda/LambdaClient.php | 91.66% <0%> (\u00f8) | 4% <0%> (\u00f8) | :arrow_down: |\n| src/S3/ApplyChecksumMiddleware.php | 100% <0%> (\u00f8) | 7% <0%> (\u00f8) | :arrow_down: |\n| src/Sdk.php | 90% <0%> (\u00f8) | 14% <0%> (\u00f8) | :arrow_down: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 11f4ad5...2922ba9. Read the comment docs.\n. # Codecov Report\nMerging #1688 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster    #1688   +/-\n=========================================\n  Coverage     93.97%   93.97%         \n  Complexity     2864     2864         \n=========================================\n  Files           174      174         \n  Lines          7682     7682         \n=========================================\n  Hits           7219     7219         \n  Misses          463      463\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 02a3ea0...5f65eb4. Read the comment docs.\n. # Codecov Report\nMerging #1695 into master will not change coverage.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster    #1695   +/-\n=========================================\n  Coverage     93.97%   93.97%         \n  Complexity     2864     2864         \n=========================================\n  Files           174      174         \n  Lines          7682     7682         \n=========================================\n  Hits           7219     7219         \n  Misses          463      463\n```\n| Impacted Files | Coverage \u0394 | Complexity \u0394 | |\n|---|---|---|---|\n| src/S3/ApplyChecksumMiddleware.php | 100% <100%> (\u00f8) | 7 <0> (\u00f8) | :arrow_down: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 651557d...fb8821e. Read the comment docs.\n. # Codecov Report\nMerging #1702 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster    #1702   +/-\n=========================================\n  Coverage     93.97%   93.97%         \n  Complexity     2864     2864         \n=========================================\n  Files           174      174         \n  Lines          7682     7682         \n=========================================\n  Hits           7219     7219         \n  Misses          463      463\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update a0353c2...3619248. Read the comment docs.\n. # Codecov Report\nMerging #1705 into master will increase coverage by <.01%.\nThe diff coverage is 100%.\n\n\n```diff\n@@             Coverage Diff              @@\nmaster    #1705      +/-\n============================================\n+ Coverage     93.97%   93.97%   +<.01%   \n- Complexity     2864     2865       +1   \n============================================\n  Files           174      174            \n  Lines          7682     7686       +4   \n============================================\n+ Hits           7219     7223       +4   \n  Misses          463      463\n```\n| Impacted Files | Coverage \u0394 | Complexity \u0394 | |\n|---|---|---|---|\n| src/Credentials/CredentialProvider.php | 98.43% <100%> (+0.05%) | 65 <1> (+1) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update a0353c2...b54d5c1. Read the comment docs.\n. # Codecov Report\nMerging #1706 into master will increase coverage by 0.01%.\nThe diff coverage is 95.65%.\n\n\n```diff\n@@             Coverage Diff              @@\nmaster    #1706      +/-\n============================================\n+ Coverage     93.97%   93.98%   +0.01%   \n- Complexity     2864     2873       +9   \n============================================\n  Files           174      174            \n  Lines          7682     7699      +17   \n============================================\n+ Hits           7219     7236      +17   \n  Misses          463      463\n```\n| Impacted Files | Coverage \u0394 | Complexity \u0394 | |\n|---|---|---|---|\n| ...eMonitoring/ApiCallAttemptMonitoringMiddleware.php | 96.92% <\u00f8> (-0.1%) | 37 <0> (\u00f8) | |\n| ...entSideMonitoring/AbstractMonitoringMiddleware.php | 98.01% <100%> (+0.08%) | 41 <0> (\u00f8) | :arrow_down: |\n| ...ientSideMonitoring/ApiCallMonitoringMiddleware.php | 98.41% <94.73%> (+0.58%) | 26 <9> (+9) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update e9a22aa...6523c2f. Read the comment docs.\n. # Codecov Report\nMerging #1707 into master will increase coverage by 0.25%.\nThe diff coverage is 99.45%.\n\n\n```diff\n@@             Coverage Diff              @@\nmaster    #1707      +/-\n============================================\n+ Coverage     93.98%   94.23%   +0.25%   \n- Complexity     2873     2995     +122   \n============================================\n  Files           174      178       +4   \n  Lines          7699     8072     +373   \n============================================\n+ Hits           7236     7607     +371   \n- Misses          463      465       +2\n```\n| Impacted Files | Coverage \u0394 | Complexity \u0394 | |\n|---|---|---|---|\n| src/EndpointDiscovery/EndpointList.php | 100% <100%> (\u00f8) | 12 <12> (?) | |\n| src/EndpointDiscovery/Configuration.php | 100% <100%> (\u00f8) | 6 <6> (?) | |\n| src/AwsClient.php | 100% <100%> (\u00f8) | 35 <2> (+2) | :arrow_up: |\n| src/ClientResolver.php | 98.06% <100%> (+0.03%) | 102 <2> (+2) | :arrow_up: |\n| src/EndpointDiscovery/ConfigurationProvider.php | 100% <100%> (\u00f8) | 49 <49> (?) | |\n| .../EndpointDiscovery/EndpointDiscoveryMiddleware.php | 99% <99%> (\u00f8) | 50 <50> (?) | |\n| src/Sdk.php | 90% <0%> (\u00f8) | 14% <0%> (\u00f8) | :arrow_down: |\n| ... and 2 more | |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 6570fab...e8c3c78. Read the comment docs.\n. # Codecov Report\nMerging #1720 into master will decrease coverage by <.01%.\nThe diff coverage is 80%.\n\n\n```diff\n@@             Coverage Diff              @@\nmaster    #1720      +/-\n============================================\n- Coverage     94.23%   94.23%   -0.01%   \n- Complexity     2995     2996       +1   \n============================================\n  Files           178      178            \n  Lines          8072     8077       +5   \n============================================\n+ Hits           7607     7611       +4   \n- Misses          465      466       +1\n```\n| Impacted Files | Coverage \u0394 | Complexity \u0394 | |\n|---|---|---|---|\n| src/Credentials/InstanceProfileProvider.php | 97.87% <80%> (-2.13%) | 14 <0> (+1) | |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 7c00779...d114aa5. Read the comment docs.\n. # Codecov Report\nMerging #1731 into master will increase coverage by <.01%.\nThe diff coverage is 100%.\n\n\n```diff\n@@             Coverage Diff              @@\nmaster    #1731      +/-\n============================================\n+ Coverage     94.23%   94.23%   +<.01%   \n- Complexity     2996     2998       +2   \n============================================\n  Files           178      178            \n  Lines          8077     8082       +5   \n============================================\n+ Hits           7611     7616       +5   \n  Misses          466      466\n```\n| Impacted Files | Coverage \u0394 | Complexity \u0394 | |\n|---|---|---|---|\n| src/Api/Validator.php | 98.34% <100%> (+0.07%) | 54 <0> (+2) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 21a4dd3...f7cee86. Read the comment docs.\n. # Codecov Report\nMerging #1732 into master will increase coverage by <.01%.\nThe diff coverage is 100%.\n\n\n```diff\n@@             Coverage Diff              @@\nmaster    #1732      +/-\n============================================\n+ Coverage     94.23%   94.23%   +<.01%   \n- Complexity     2996     2998       +2   \n============================================\n  Files           178      178            \n  Lines          8077     8085       +8   \n============================================\n+ Hits           7611     7619       +8   \n  Misses          466      466\n```\n| Impacted Files | Coverage \u0394 | Complexity \u0394 | |\n|---|---|---|---|\n| src/Api/Validator.php | 98.38% <100%> (+0.11%) | 54 <0> (+2) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 21a4dd3...a1b4242. Read the comment docs.\n. # Codecov Report\nMerging #1734 into master will increase coverage by <.01%.\nThe diff coverage is 100%.\n\n\n```diff\n@@             Coverage Diff              @@\nmaster    #1734      +/-\n============================================\n+ Coverage     94.25%   94.25%   +<.01%   \n- Complexity     3000     3001       +1   \n============================================\n  Files           178      178            \n  Lines          8089     8091       +2   \n============================================\n+ Hits           7624     7626       +2   \n  Misses          465      465\n```\n| Impacted Files | Coverage \u0394 | Complexity \u0394 | |\n|---|---|---|---|\n| src/Sdk.php | 90.62% <100%> (+0.62%) | 15 <1> (+1) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 93784f1...25e1606. Read the comment docs.\n. # Codecov Report\nMerging #1735 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster    #1735   +/-\n=========================================\n  Coverage     94.23%   94.23%         \n  Complexity     2998     2998         \n=========================================\n  Files           178      178         \n  Lines          8085     8085         \n=========================================\n  Hits           7619     7619         \n  Misses          466      466\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 6d313ff...40ba954. Read the comment docs.\n. # Codecov Report\nMerging #1738 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster    #1738   +/-\n=========================================\n  Coverage     94.23%   94.23%         \n  Complexity     2998     2998         \n=========================================\n  Files           178      178         \n  Lines          8085     8085         \n=========================================\n  Hits           7619     7619         \n  Misses          466      466\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 2666416...596e207. Read the comment docs.\n. # Codecov Report\nMerging #1742 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster    #1742   +/-\n=========================================\n  Coverage     94.23%   94.23%         \n  Complexity     2998     2998         \n=========================================\n  Files           178      178         \n  Lines          8085     8085         \n=========================================\n  Hits           7619     7619         \n  Misses          466      466\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 4f04241...b427c94. Read the comment docs.\n. # Codecov Report\nMerging #1744 into master will increase coverage by 0.01%.\nThe diff coverage is 100%.\n\n\n```diff\n@@             Coverage Diff              @@\nmaster    #1744      +/-\n============================================\n+ Coverage     94.23%   94.25%   +0.01%   \n- Complexity     2998     3000       +2   \n============================================\n  Files           178      178            \n  Lines          8085     8089       +4   \n============================================\n+ Hits           7619     7624       +5   \n+ Misses          466      465       -1\n```\n| Impacted Files | Coverage \u0394 | Complexity \u0394 | |\n|---|---|---|---|\n| src/S3/StreamWrapper.php | 97.45% <100%> (+0.31%) | 144 <0> (+2) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 875786b...56e1be2. Read the comment docs.\n. # Codecov Report\nMerging #1752 into master will decrease coverage by 0.04%.\nThe diff coverage is 0%.\n\n\n```diff\n@@             Coverage Diff             @@\nmaster   #1752      +/-\n===========================================\n- Coverage     94.25%   94.2%   -0.05%   \n- Complexity     3001    3004       +3   \n===========================================\n  Files           178     178            \n  Lines          8091    8095       +4   \n===========================================\n  Hits           7626    7626            \n- Misses          465     469       +4\n```\n| Impacted Files | Coverage \u0394 | Complexity \u0394 | |\n|---|---|---|---|\n| src/CloudFront/Signer.php | 0% <0%> (\u00f8) | 13 <2> (+3) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 6b8821a...cd5b1e0. Read the comment docs.\n. \n",
    "JasonNewton1": "Hi there,\nI'm using Composer so I've made the change to now read:\n```\n<?php\nuse Aws\\S3\\S3Client;\nuse Aws\\S3\\Exception\\S3Exception;\nerror_reporting(E_ALL);\nini_set('display_errors', 1);\nrequire 'Library/WebServer/Documents/vendor/autoload.php';\n$cloudFront = new Aws\\CloudFront\\CloudFrontClient([\n    'region'  => 'eu-west-1',\n    'version' => 'latest',\n    ]);\n```. But I now get the following error:\n```\nWarning: require(Library/WebServer/Documents/vendor/autoload.php): failed to open stream: No such file or directory in /Library/WebServer/Documents/cloudfront.php on line 9\nFatal error: require(): Failed opening required 'Library/WebServer/Documents/vendor/autoload.php' (include_path='.:') in /Library/WebServer/Documents/cloudfront.php on line 9\n```. This is the full path to auto upload.php\nrequire 'Library/WebServer/Documents/vendor/autoload.php';\nSo I don't know why it can't see it.\nHere is the whole thing (screenshot) - I removed my keys.\nhttps://dl.dropboxusercontent.com/content_link/cp5I506kFELKeZozkj1Set3AoxXxyckODXVuwMUtSMKMvzBAcya8zywxSBNonqbZ/file\n. I had the vendor in two locations.\nThanks. ",
    "McQgit": "OK, I'll try adding the fflush before fclose.  But, what about file_put_contents?. I confirm the fflush before an fclose catches the error.  Alas, I use file_put_contents far more often so - ouch.\nI think my long-term solution is to create common functions for writing using the S3 API.  But, in the meantime I am going to use filesize() and filemtime() to detect errors in crucial spots.  Is there any care I need to take other than calling clearstatcache() before those functions?  Do I need to include an \"s3://\" prefixed filename with it, for example?  Thanks. Nevermind on the clearstatcache().  I now realize that would only introduce failure scenarios using that with filesize and filemtime for error detection.\nI have developed my own function for putting S3 objects using the API.  That gives me better control than even using fopen/fflush/fclose as I can process \"5xx\" class errors separately from the others.  That is, the class of responses for my function are \"successful\", \"permanent error no retry\", and \"error - retry\".\nIt would have been helpful to me to have seen something in the SDK docs that hinted at the problems identifying write failures.  I would have used the API when designing the app instead of the stream wrapper.  The failure rates for php writes with local files is so tiny it's rarely worth bothering.  But, S3 failures are more common so this is a larger exposure with the stream wrapper that could be better explained.  Until you clarified it with these comments I didn't know you hadn't added code in the sdk stream wrapper to overcome weaknesses in the php stream layer.. The nuances of your using the PHP stream wrapper \"underneath\" wasn't apparent to me.  One reason is I hadn't ever studied it before and, two, you never say you use it specifically in your docs :)  The names are similar, sure, but stream and wrapper are very common terms.  Your lead paragraph in the docs simply says the AWS stream wrapper allows use of the php file functions and so I took that to mean the php semantics of those functions was supported.  I read plenty of helpful notes unique to S3 in those docs (the restriction on fopen modes, issues using temp streams, and so on) but never anything about error handling being any different.\nSo, something like the following would have helped me (thanks):\nDetecting Upload Errors\nBecause of the underlying PHP Stream Wrapper interface, file_put_contents() will not return \"false\" when errors occur uploading a file.  Use fopen() or the SDK API PutObject if you need to identify these errors.\nTo identify upload errors when using fopen(), you must use an fflush() after all the writes and before the fclose().  The fflush() will return \"false\" for any errors.  An fclose() will return \"true\" even if there were preceding errors.\n. ",
    "rollsappletree": "\nhere one image to test this issue.\nCheers\nC. ",
    "dgadelha": "As far as I could see from your stack trace, the HTTP requests seems to be using Promises. Probably because the HTTP request runs asynchronously. The tick() function might be a simple function to check if the HTTP request has succeeded and, if not yet, wait a second and check again.\nSo, if the scheme is really this one up there, the 5 seconds delay happening in your requests are just the client waiting for the HTTP response. Then, the Amazon servers take 5 seconds to answer you.\nI suggest you to try out doing some tests to see if it's really taking ~5 seconds to give you a result (call it manually [curl, etc], check how this can be optimized or setup a background worker to send those requests for you (if they're not essential).\nThanks.. ",
    "dtirer": "@dgadelha @imshashank thanks for your replies.  I'll have to try a test, though for some these, background the publishing to SQS may not be an option.\nIn general, most of the requests are pretty fast.  However, for a given endpoint, there's maybe 1-2 an hour that take a very long time.\nI am making these requests from inside a VPC.  The EC2 instances are in a private subnet, but have access to the internet via a NAT Gateway.. @imshashank @dgadelha I'll be trying some tests today.  But today, I just noticed that a handful of requests in a given time period were taking a very long time to send messages to SQS, about 60 seconds.\nHere is the new relic trace for that: https://ibb.co/bDBLfk\nduring this time period, the requests-per-mintue were particularly low for our servers, so it doesn't seem to be traffic-related. @imshashank @dgadelha well im waiting to here back from AWS, however, i went on my Staging server and and new'd up the AWSClient and basically followed this example: \nhttp://docs.aws.amazon.com/aws-sdk-php/v3/guide/examples/sqs-examples-send-receive-messages.html\nHowever, for the message, I replaced it with the average size of my message which is roughly 1000 bytes.\nRequests were very fast. I then tried putting requests within a for loop and sending 100 and 200 and they were also incredibly fast.   And thats already many more than I would be experiencing concurrently. I then tried 2000 within a for loop and it took a few seconds to complete, but thats to be expected and is not really my use case\nSo I think this rules out any potential issues with my VPC setup.  The instances seems perfectly capable of making outgoing requests from the private subnet.  And the SDK seems to perform well.\nI suppose this leaves 1) something on AWS's end or 2) The Laravel codebase, which wraps the PHP SDK and may be configuring things differently. @imshashank After speaking with SQS, it seems that while some of the requests are latent on the SQS end, the vast majority seem to be slow on the PHP application end.  I'm currently trying to isolate components to see where the issue is.  So far I have tried:\n\n\nusing a barebones endpoint in my laravel app, which accesses the AWS PHP SDK directly (not using the Laravel \"Queue\" wrappers).  This resulted in the same issues.\n\n\nI'm currently trying a raw PHP script, outside the Laravel application.  The script contains the exact same code as in the above example (using AWS PHP SDK directly). I'm going to let it it bake for about an hour or so and see if I still record long requests in New Relic.\n\n\nI assume that if #2 above works fine, that means theres something with how Laravel handles outgoing requests which is causing the issue.\nI'll report back shortly. @imshashank So after lots of discussions with SQS, it seems that the bulk of these issues are not SQS related.  As of it now, it seems either be the network, or the PHP SDK.\nOne thing I recently tried doing was adding all the CURL Stats and sending them to New Relic with the long requests.  As it turns out, connecting to SQS is very fast.  In every case, 99% of the time is taken during time_starttransfer.  Any idea what this might be?. @imshashank time_starttransfer is a statistic that Curl provides, since the SDK is using Curl for the requests.  The images from my previous posts above are still applicable, as far as stack trace.  But attached are some more recent ones, with the new data I've collected from the Curl stats. \nhttps://ibb.co/b2TXFF\nhttps://ibb.co/gLbKvF\nhttps://ibb.co/ncVONv\nhttps://ibb.co/f5XsFF\nhttps://ibb.co/nvFCFF. I also profiled a basic endpoint using Blackfire (https://blackfire.io).  This endpoint simply news up the AWS PHP SDK and sends 1 kb message to SQS.  Attached are some images of the results.  It seems curl_multi_select is called 11 times and curl_multi_exec is called 13 times.  And this is where all the time is being taken\nhttps://ibb.co/nhDSXv\nhttps://ibb.co/iL4qkF\nhttps://ibb.co/drNbQF. @moeseth I did not, though I'm glad to see I'm not crazy :)\nMy conclusion was that php is not great for outbound connections. And a certain percentage of the requests just seem to take a while. \nI had also tried building a Go application that sends 5 request/minute to sqs, and I didn't experience the issue. Only the ocassional sqs latency. \nSo I've since used sqs for less, and only for the cases I truly need it. ",
    "moeseth": "@dtirer did you solve this issue? I'm having the same problem.\n\n. @dtirer We have been heavily relying upon sqs. Looking for a quick fix :-(. ",
    "sators": "@imshashank Is there any example / documentation on how to use somewhere?. Thanks @kstich for your help - I did dig around and and find Rds\\AuthTokenGenerator...I'm using this sample code below to try to obtain a token:\n```\n// Use the default credential provider\n$provider = CredentialProvider::defaultProvider();\n$RdsAuthGenerator = new Aws\\Rds\\AuthTokenGenerator($provider);\n$token = $RdsAuthGenerator->createToken('', 'us-east-2', 'iamuser');\nvar_dump($token);\n```\nThis is outputting:\nstring(1023) \"Action=connect&DBUser=iamuser&X-Amz-Security-Token=<random token>&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=<random credential>&X-Amz-Date=20171012T162656Z&X-Amz-SignedHeaders=host&X-Amz-Expires=900&X-Amz-Signature=<random signature>\"\nIs this entire string supposed to be the MySQL password/token? ...or is it only supposed to be the value from the X-Amz-Security-Token parameter?. Anyone visiting in the future might find this gist helpful: https://gist.github.com/sators/38dbe25f655f1c783cb2c49e9873d58a. ",
    "mvanbaak": "Thanks!. ",
    "se-robapodaca": "My bad. I was looking at v2 sdk documentation instead of v3. Thanks!. ",
    "tinonetic": "Thank-you @kstich . Sorry about that.. ",
    "ssigwart": "Using gc_collect_cycles() after every few calls when uploading large amounts of data to S3 works well to free the memory for me.. Garbage collection is on in my php.ini files.  I am able to free the memory if I manually call gc_collect_cycles().. ",
    "Xylane": "Hello,\nI'm sorry but I don't understand why this issue is closed.\nIssue exists since more than 1 year, and it's not just a question of \"how PHP handles cyclic object graphs\" : the function uploadPart() is used in combination with createMultipartUpload(), which is made especially to upload large files. So it HAS to be \"optimised\" for this usage, and actually it isn't.\nMoreover, the script given as example in your official documentation is generating fatal error (allowed memory size exhausted), as soon as the memory limit for this script is lower than the size of the file you wish to upload (even when GC is enabled).\nIn my humble opinion, this issue is caused by a bad usage of PHP features, and must be addressed : this is developer's job to know how PHP is handling cyclic object graphs and to adapt his script accordingly in order to avoid fatal errors.\nFinally, if you considers that this issue cannot be fixed because of PHP some limits (let me have huge doubts, since for example high-level API doesn't have this issue for example), you shoud at least document this issue in your official website, list possible solutions and update your example accordingly (for example by using gc_collect_cycle()).\nIn any case, there is something to do. And closing the issue without doing nothing is not a valid \"solution\"...\nRegards,\nLionel. I totally agree with you thebeanieman, but it's soooo much easier to propose crappy code and to state that \"issue is related to PHP\" than to spend some time in order to check where to put some \"unlink\" or \"unset\" function in order to properly release the memory :-/\nThis issue has been opened almost 2 years ago, and nothing has been done. Example given by the official documentation to \"upload large object\" continue to return fatal error when uplooading large object, and \"developers\" of  Amazon continue to consider that it's not an issue.\nCongrats guys !. And anyway the garbage collector is just a help. Look at C or C++ languages : there is no garbage collector and you must release memory manually.\nWhen you use properly a programming language and you have devent skills, you always release the memory manually. You don't only count on garbage collector that is more random and can create side effect.\nGarbage collector is just a plan B to avoid crash in case of \"poorily coded app\" or if you forgot to release some memory.\nIn this case, it's clearly stated that the example given in the official documentation is made for uploading large files. How is it possible not to release memory after uploading each part ? What is the logic of splitting a large file in small parts if you need at least as much memory as the full file to upload ???\nThe only reason is that Amazon developers don't want to \"waste\" time fixing their own code ! I'm sorry but it's unacceptable.. PSR will never say not to release memory. Trust me :)\nIt's not a question of best practice, and even if it was official documentation should mention in its code how to use its own framework properly (which is not the case).\nThe issue is that Amazon's developer prefer to state that \"PHP garbage collector is not doing its job\" than to write clean code which will not depend on PHP garbage collector :(. Haha yes, managing memory is not easy. That's why recent programming language include garbage collector to help developers.\nBut you shouldn't 100% rely on it. It's just giving some help but it cannot do everything instead of you. In the current situation it seems clear that garbage collector is not able to release memory automatically. Fine, and what? Does it prevent developers to release memory manually? No, it doesn't.\nSo please, Amazon, fix it by yourself !. Thanks diehlaws, really.\nAs @thebeanieman, don't hesitate to inform us if I can help to test some patch !. @jschwarzwalder Of course :)\nThe documentation I mentioned in my initial post (https://docs.aws.amazon.com/AmazonS3/latest/dev/LLuploadFilePHP.html) IS using MultipartUploader (line 16). uploadPart is just a low-level function used to manually define which part you want to upload once you initiated the multipartUpload using $s3->createMultipartUpload(). \nThe following page mentions clearly that \"You can upload large files to Amazon S3 in multiple parts. You must use a multipart upload for files larger than 5 GB. [...] If you need to pause and resume multipart uploads, vary part sizes during the upload, or do not know the size of the data in advance, use the low-level PHP API. For more information, see Using the AWS PHP SDK for Multipart Upload (Low-Level API). \nAnd this AWS PHP SDK for Multipart Upload (Low-Level API) is exactly what I use.\nRegarding the changes to make in the documentation, one basic workaround is to manually call gc_collect_cycles() after using uploadPart().\nSo the code will look like : \n```\n<?php\nrequire 'vendor/autoload.php';\nuse Aws\\S3\\S3Client;\n$bucket = ' Your Bucket Name ';\n$keyname = ' Your Object Key ';\n$filename = ' Path to and Name of the File to Upload ';\n$s3 = new S3Client([\n    'version' => 'latest',\n    'region'  => 'us-east-1'\n]);\n$result = $s3->createMultipartUpload([\n    'Bucket'       => $bucket,\n    'Key'          => $keyname,\n    'StorageClass' => 'REDUCED_REDUNDANCY',\n    'ACL'          => 'public-read',\n    'Metadata'     => [\n        'param1' => 'value 1',\n        'param2' => 'value 2',\n        'param3' => 'value 3'\n    ]\n]);\n$uploadId = $result['UploadId'];\n// Upload the file in parts.\ntry {\n    $file = fopen($filename, 'r');\n    $partNumber = 1;\n    while (!feof($file)) {\n        $result = $s3->uploadPart([\n            'Bucket'     => $bucket,\n            'Key'        => $keyname,\n            'UploadId'   => $uploadId,\n            'PartNumber' => $partNumber,\n            'Body'       => fread($file, 5 * 1024 * 1024),\n        ]);\n        $parts['Parts'][$partNumber] = [\n            'PartNumber' => $partNumber,\n            'ETag' => $result['ETag'],\n        ];\n        $partNumber++;\n        gc_collect_cycles();\n    echo \"Uploading part {$partNumber} of {$filename}.\" . PHP_EOL;\n}\nfclose($file);\n\n} catch (S3Exception $e) {\n    $result = $s3->abortMultipartUpload([\n        'Bucket'   => $bucket,\n        'Key'      => $keyname,\n        'UploadId' => $uploadId\n    ]);\necho \"Upload of {$filename} failed.\" . PHP_EOL;\n\n}\n// Complete the multipart upload.\n$result = $s3->completeMultipartUpload([\n    'Bucket'   => $bucket,\n    'Key'      => $keyname,\n    'UploadId' => $uploadId,\n    'MultipartUpload'    => $parts,\n]);\n$url = $result['Location'];\necho \"Uploaded {$filename} to {$url}.\" . PHP_EOL;\n```\nPlease note that THERE IS a memory leak, but GC mechanism is able to detect it and to free the memory when you call it.\nThe best would be to find what is causing this memory consumption and fix it inside the SDK. I guess that you forgot to unset some variable somewhere in the uploadPart() function...\nPlease note that PHP garbage collection mechanism is just there to assist in memory management, but if your code is well written it shouldn't be necessary.\nRegards,\nLionel. ",
    "dinamic": "Guys, there's definitely a problem in the library. One of our cron job workers dies on regular basis with no stacktrace and no information to debug. However, the out of memory error happens in GuzzleHttp\\Psr7\\Stream::__toString(), which is be called by the AWS PHP SDK.\nWe are using streams as in php streams. There seem to be some flaw that would not use the buffer you intended to use for big files, and would cause the stream to be casted to string, which does put the whole file in memory.. ",
    "thebeanieman": "Im a web developer with 16 years experience and today I wasted 5 hours dicking around trying to get Amazon S3 to upload a 78G file by following amazons documentation and code examples. It was only after reading this issue that I was able to upload the file by using using gc_collect_cycle() as suggested. I was working on a server that had cpanel and php installed, I could not tweak the config. I think it is VERY important that this issue is looked into again as not everybody can change the host configuration from the default of their host to one that suits Amazon. The memory was not being released in the loop and increased with each iteration until it met the server limits. I could set ini_set('memory_limit','78000M'); even if the server had that much ram and it would still FAIL. This really needs looking into rather than just closing it saying change your server config to suit the API.. I understand there code might be good practice, but what good is that if people that are not server admins can't use it? The entire point of an API is to HELP PEOPLE todo something. It's not like people deliberately turned settings off and came moaning. . When I was 15 I started a JAVA course, I was good at programming but I gave up as I was confused about having to deal with memory allocation before I even used a variable!\nI agree about all of your comments. Over the years I have learned not to presume anything as people have different environments. Im very good at saying I have 16 years experience and boosting, but you know what, I only know what I know, im not a Webmaster yet, and the way things are going with all these Fancy Frameworks, I never will be. I can do PHP thou! ha. \nIm not here to rant, im here to offer my help and advice to help get this fixed so other people are not left pulling there hair out wondering why they follow the documentation and it does not work! I have had all sorts of curl issues today, tried loads. I agree the code should reset the variables / memory by itself. Maybe thats not in PSR standards? I just make good code that works lol. I worked on a website host with a Magento site, the garbage collector never ran, they resulted in using a cron to delete the millions of session files ! ha. No problem. Please let me know if you need any tests done or any clarification. . ",
    "diehlaws": "Thank you for your contributions to the discussion on this issue @dinamic, @thebeanieman, and @Xylane. I'm re-opening the issue to bring it up during our sprint to the rest of the team for further investigation on this behavior.. Hey @zubair72, thanks for reaching out to us about this. Unfortunately I wasn't able to reproduce this on PHP 7.0 and the AWS SDK for PHP version 3.64.7.\nCould you give us the exact error message you're seeing from your shared hosting environment? Knowing more details about the hosting provider you're using, specifically their php configuration and/or a link to the specs of the host you're using (if available), would be helpful.. Thanks for the additional information. I'm able to reproduce the error Parse error: syntax error, unexpected '$value' (T_VARIABLE) in /var/www/html/html/include/aws/Aws/functions.php on line 36, but only when using a PHP version older than 5.5. Please keep in mind that the current major version of the AWS SDK for PHP only supports PHP versions 5.5 and newer.\nThis sounds like a case where your hosting provider may be using an older version of PHP, either for your entire site or specific directories. I'd suggest checking the PHP Selector plugin in cPanel and looking for PHP versions older than 7.0 defined in your .htaccess file.. Hey @koushikSen, thanks for reaching out about this. Just to rule out simple environment misconfigurations, can you check the phpinfo() output on your EC2 instance to ensure that the XML and SimpleXML packages are installed and enabled? If you've checked that they are, we'll need more information to troubleshoot this - a code sample would be ideal so we can try reproducing this behavior on our end.. Thanks for the additional info. Just to rule out your local environment as the culprit, could you try running the following sample code that uses the SimpleXML package to see if you continue to receive this error? You may be running multiple PHP builds with different packages between CLI and web on this machine.\n```php\n<?php\n$string = <<\n\nTestTitle\n\n  Here is some text\n \n\nXML;\n$xml = simplexml_load_string($string);\nprint_r($xml);\n?>\n```. Thanks for reaching out to us about this @seme1. The errors you're seeing appear to stem from two separate issues.\n\nPHP Warning:  session_start(): Failed to read session data\n    This appears to be due to a particularity introduced with PHP 7.1 - the load session must return an empty string instead of null. More information on this topic can be found in various Github issues (1, 2, 3) and StackOverflow posts (1, 2), as well as the comment section of the session_start function in the PHP documentation.\nPHP Fatal error:  Uncaught Aws\\\\Exception\\\\CredentialsException: Unexpected instance profile response code and PHP Fatal error:  Uncaught Aws\\\\Exception\\\\CredentialsException: Error retrieving credentials from the instance profile metadata server. (cURL error 7: Failed to connect to 169.254.169.254 port 80: Connection refused (see http://curl.haxx.se/libcurl/c/libcurl-errors.html))\n    These errors suggest that the EC2 metadata service on your instance is not working as expected. Unfortunately the first error does not make it clear which HTTP response code was returned by the metadata service, however the second error makes it clear the connection was actively refused by the metadata service. \n\nYou can check your instance's EC2 metadata service by running cURL against it (e.g. $ curl http://169.254.169.254/2018-03-28/meta-data/instance-id should return the instance ID). If you see that your instance frequently runs into problems when interacting with the EC2 metadata service, a good start to troubleshooting this behavior would be to stop the instance and start it again so it gets moved to different hardware. If you continue to see unexpected behavior from the EC2 metadata service after this on your instance you may want to open a Premium Support case under the EC2 - Linux service, or post on the AWS Forums, for further assistance down this avenue.. Hey @pranavansp, thanks for reaching out about this. This error typically comes up as a result of a typo in your credentials so I suggest double checking your key and secret values to make sure there aren't any spaces or other unexpected characters at the beginning or end of these. If you've double-checked that these values are correct, please enable wire logging and provide the output for our review.. @pranavansp Thanks for getting back to us, and I'm glad you were able to solve the issue. Please keep in mind that issues in this repository address problematic behaviors specific to the AWS SDK for PHP, not issues relating to the services themselves.\nThe issue you linked suggests this behavior is caused by special characters being present in the secret key, however I've tested your code with several IAM users' credential sets that contain non-alphanumeric characters ( / and +) in the secret key and was unable to elicit a SignatureDoesNotMatch exception from SQS. Since I'm not able to reproduce this behavior from my end, I cannot determine the conditions under which it can occur. I suggest reaching out to our Premium Support team by opening a support case under the SQS service since they will have a better idea of what to look for when reproducing a service issue like this.. Thanks for reaching out to us about this @rayzor65. This seems like a reasonable feature request so I have labeled the issue as such. We will keep the thread updated as more information is made available for this feature.. Thanks for your patience on this @rayzor65. We are actively working on implementing this feature, unfortunately I cannot provide an ETA for its release but it will be available soon.. Hi @ranrinc, thanks for reaching out. It sounds like the AWS SDK for PHP is unable to locate the AWS credentials file in your home directory. If ~/.aws/credentials is present and readable you should be able to run sample.php via your terminal (php sample.php) with successful results. If your web server is running as a different user than the one used to run aws configure then it's possible your web server does not have read access to your credentials file.. Supplying credentials to the AWS SDK for PHP can be done in several different ways: \n1. Set the environment variables AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY for the user you're using for your web server\n2. Create a new shared credentials file (or copy your existing one and change its permissions accordingly) in the home directory of the user you're using for your web server so that it can access this file\n3. Assume an IAM role (best used when running your application in EC2)\n4. Use a credential provider\n5. Generate temporary credentials from AWS STS\nEach of the options above has an example shown in the documentation page I linked. You can also hard-code credentials into your application per the example shown below, however this method is not recommended since it is surprisingly easy to forget to remove credentials from code before committing it to a repository. This can result in your AWS credentials being exposed to more people than you intend, which often results in a compromised AWS account. As such, this method should be used as a last resort or when testing code locally that will never be accessible to others.\nphp\n$key = 'AKIAIOSFODNN7EXAMPLE'\n$secret = 'wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY'\n$s3 = new S3Client([\n    'region' => 'us-east-1',\n    'version' => '2011-06-15',\n    'credentials' => new Credentials($key, $secret)\n]);. Since the behavior you're describing does not happen consistently, this does not sound like a bug in the AWS SDK for PHP but rather unexpected behavior with the ECS task role. It is very possible that the ECS Container Agent is failing to talk to the ECS Task Metadata endpoint when you're seeing these 403's. \nFor further guidance with this, I suggest reaching out to our Premium Support team by opening a support case under the Container Service (ECS) service, or posting in the AWS Forums.. Hi @alanchavez88, thanks for reaching out about this. It sounds like the AWS credentials you're specifying in $key and $secret may not be valid. I was able to create a Cognito user without errors with your code by commenting out the STS bits and passing my key and secret key directly to the Cognito client as shown below:\n```php\n$client = new AwsClient([\n    'service' => 'cognito-idp',\n    'credentials' => new Credentials($key, $secret),\n    'region' => 'us-east-1',\n    'version' => '2016-04-18'\n]);\n/* @var CognitoIdentityProviderClient $client /\n$client->adminCreateUser([\n    'UserPoolId' => $userPoolId,\n    'Username' => $username\n]);\n```\nIt's possible that the IAM user whose credentials you're using has MFA enabled. If this is the case, your getSessionToken call needs to include the MFA device's serial number as it was registered in IAM for this user as well as a fresh token code from the MFA device. The token code would be best implemented as a variable that is requested from the user at the time the page is loaded.\nphp\n$credentials = new Credentials($key, $secret, $sts->getSessionToken([\n    'SerialNumber' => 'string',\n    'TokenCode' => 'string',\n]));. Thanks for reaching out to us about this @tranthienbinh1989. The error message you're receiving sounds like the API keys you're using are either invalid (typo) or missing (deleted from IAM). Can you double check that the AWS SDK for PHP is receiving the correct credentials from the credential provider? \nIf you can confirm the API key you're using with the AWS SDK for PHP should work for this call, can you please provide a code sample for us to reproduce this behavior?. Thanks for the clarification, that definitely doesn't sound like an issue with your credentials if other queries work as expected. Unfortunately I'm not able to reproduce this behavior on my end using the code attached below with PHP v7.2.8 and the AWS SDK for PHP v3.67.5. \n```php\n  $client = CloudSearchDomainClient::factory(array(\n      'endpoint' => 'http://search-test-gbhs3sykvj2ajcnajyf2tdwcky.us-west-2.cloudsearch.amazonaws.com',\n      'version' => '2013-01-01',\n      'credentials' => new Credentials($key, $secret),\n      'debug' => true\n  ));\n$result = $client->search([\n  'query' => '0',\n  ]);\necho $result;\n  ```\nI tested 0, '0', '\"0\"', 'q=0', and q=\"0\" for the query value, all of which returned a successful (albeit empty) response. If this approach is not similar to the manner in which you are executing your search queries, can you please provide a code sample so we can get a better idea of how you're using the AWS SDK for PHP to interact with your CloudSearch domain?. Hi @talk2alie, it looks like you may have posted this in the wrong repository - I'm not seeing anything related to PHP in your question. You can open a new issue under the repository for the AWS SDK for .NET here.. Hi @PhilETaylor, thanks for reaching out to us about this. I was unable to reproduce this on a docker container using the php:7.3.0beta3-apache image. \nCan you give us some more details about the versions you're using for PHP and the the AWS SDK for PHP? Including a link to the exact PHP build used when receiving this error would be ideal. In addition to this, providing a more complete code sample would help us test the exact operations with which you're running into trouble.. Thanks for the clarification @engharb. The AWS SDK for PHP does not have a way to check a set of AWS credentials without issuing a request to an AWS service. \nThe easiest way to verify a set of AWS credentials is to grant permission to the IAM entity associated with these credentials to issue a request that does not return sensitive information about your account (EC2 DescribeRegions is a good example), then have your PHP backend server issue the API request you've chosen for credential verification and look for the error returned by the service when it receives invalid credentials (e.g. HTTP 401 AuthFailure response for EC2).. Thanks for bringing this to our attention @dawolf. The reason isDefault works when IsDefault doesn't is because isDefault is not getting sent in the API call since it is not included in the Certificates shape - you can test this by swapping the lowercase i with any other letter and watch the request succeed. As for the error you're receiving when including a properly capitalized IsDefault in your addListenerCertificates input, it looks like the ELB API docs state not to include this property in the Certificate data type when specifying a certificate as an input parameter.\nI'll be reaching out internally to the ELB team about improving this data type, as the current iteration seems pretty counter-intuitive from the end user's perspective.. Thanks for reaching out to us @zoomphoto. While I'm able to reproduce the error you're encountering, it seems like this behavior is caused by the manner in which you're specifying the FaceIds value. The $cleanFaces variable is being initialized as a string, then added as the sole element to the array corresponding to the value for FaceIds. This results in the service treating the full list of Face IDs as a single ID, which fails validation for the regular expression specified in the error message.\nInstead of passing the list of Face IDs as a single string in the array, you could initialize $cleanFaces as an array with each of the Face IDs as an element in the array and then specify the variable as the FaceIds value as shown below.\n```php\n$cleanFaces = [\n'f4a4001e-0881-4d60-a143-44f5898307f6',\n'133391cf-d837-43cc-9acb-5e81925add38',\n'20769223-6be2-4274-a92c-bfd06f74edd1',\n'8af7e6bc-aec2-4e40-abbe-4dd8dc83f861',\n'ba1b1524-88ec-4d69-8a75-43689f78a305' \n];\n$result = $rek->deleteFaces([\n'CollectionId' => \"{$workingEvent}\", \n'FaceIds' => $cleanFaces,\n]);\n``. Thanks for reaching out to us @HeJianqiao. Unfortunately it is not clear where$body` is being declared, or what its value should be; I'm also not entirely sure what the sentence below means. Could you please clarify these two points for me?\n\nWhen while loop is going, memory overflow(only if i put the new client option in while loop or call the 'gc_collect_cycles' function). \n\nYour usage of the client object looks correct, and using fopen() will use a buffer to stream data from the file so attempting to upload a file larger than your memory limit shouldn't cause a memory overflow. Please keep in mind that the default memory limit for PHP is 128MB at this time, is there any particular reason you're lowering it to 20MB on this script?\nI tried reproducing this with the code shown below, using the same PHP version and memory limit; unfortunately I was not able to elicit any errors from PHP, the AWS SDK for PHP, or S3.\nCode:\n```php\n$s3Client = new S3Client([\n    'version' => 'latest',\n    'region'  => 'us-east-1',\n    'credentials'  => $credentials,\n]);\n$memlim = ini_get('memory_limit');\necho \"Memory Limit: {$memlim}\\n\";\n$files = scandir('walls/');\n$totalup = 0;\nforeach ($files as $img) {\n    $file = fopen('walls/'.$img, 'r');\n    $size = filesize('walls/'.$img);\n    $totalup = $totalup+$size;\n    echo \"Uploading {$size} bytes... \";\n    while (!feof($file)) {\n        $params = [\n            'Bucket' => $bucket,\n            'Key' => 'walls/'.$img,\n            'Body' => $file\n        ];\n        $result = $s3Client->putObject($params);\n        echo \"HTTPcode: {$result['@metadata']['statusCode']} | \";\n    }\n}\necho \"\\nTotal bytes uploaded: {$totalup}\\n\";\n```\nOutput:\nroot@38cc413a1287:/var/www/html/GH/1645# php -version\nPHP 7.0.17 (cli) (built: Mar 21 2017 23:29:22) ( NTS )\nCopyright (c) 1997-2017 The PHP Group\nZend Engine v3.0.0, Copyright (c) 1998-2017 Zend Technologies                                                                                                            \nroot@38cc413a1287:/var/www/html/GH/1645# du -h --max-depth=1 walls/                                                                                                       \n65M     walls/                                                                                                                                                            \nroot@38cc413a1287:/var/www/html/GH/1645# php putManyObjects.php                                                                                                          \nMemory Limit: 20M            \nUploading 12288 bytes... HTTPcode: 200 | Uploading 4096 bytes... HTTPcode: 200 | Uploading 288058 bytes... HTTPcode: 200 | Uploading 288058 bytes... HTTPcode: 200 | Uploading 288058 bytes... HTTPcode: 200 | Uploading 288058 bytes... HTTPcode: 200 | Uploading 422183 bytes... HTTPcode: 200 | Uploading 422183 bytes... HTTPcode: 200 | Uploading 422183 bytes... HTTPcode: 200 | Uploading 422183 bytes... HTTPcode: 200 | Uploading 347236 bytes... HTTPcode: 200 | Uploading 347236 bytes... HTTPcode: 200 | Uploading 347236 bytes... HTTPcode: 200 | Uploading 347236 bytes... HTTPcode: 200 | Uploading 158693 bytes... HTTPcode: 200 | Uploading 158693 bytes... HTTPcode: 200 | Uploading 158693 bytes... HTTPcode: 200 | Uploading 158693 bytes... HTTPcode: 200 | Uploading 191295 bytes... HTTPcode: 200 | Uploading 191295 bytes... HTTPcode: 200 | Uploading 191295 bytes... HTTPcode: 200 | Uploading 191295 bytes... HTTPcode: 200 | Uploading 318089 bytes... HTTPcode: 200 | Uploading 318089 bytes... HTTPcode: 200 | Uploading 318089 bytes... HTTPcode: 200 | Uploading 318089 bytes... HTTPcode: 200 | Uploading 209999 bytes... HTTPcode: 200 | Uploading 209999 bytes... HTTPcode: 200 | Uploading 209999 bytes... HTTPcode: 200 | Uploading 209999 bytes... HTTPcode: 200 | Uploading 43756 bytes... HTTPcode: 200 | Uploading 43756 bytes... HTTPcode: 200 | Uploading 43756 bytes... HTTPcode: 200 | Uploading 43756 bytes... HTTPcode: 200 | Uploading 422665 bytes... HTTPcode: 200 | Uploading 422665 bytes... HTTPcode: 200 | Uploading 422665 bytes... HTTPcode: 200 | Uploading 422665 bytes... HTTPcode: 200 | Uploading 952982 bytes... HTTPcode: 200 | Uploading 952982 bytes... HTTPcode: 200 | Uploading 952982 bytes... HTTPcode: 200 | Uploading 952982 bytes... HTTPcode: 200 | Uploading 333980 bytes... HTTPcode: 200 | Uploading 333980 bytes... HTTPcode: 200 | Uploading 333980 bytes... HTTPcode: 200 | Uploading 333980 bytes... HTTPcode: 200 | Uploading 141096 bytes... HTTPcode: 200 | Uploading 141096 bytes... HTTPcode: 200 | Uploading 141096 bytes... HTTPcode: 200 | Uploading 141096 bytes... HTTPcode: 200 | Uploading 778278 bytes... HTTPcode: 200 | Uploading 778278 bytes... HTTPcode: 200 | Uploading 778278 bytes... HTTPcode: 200 | Uploading 778278 bytes... HTTPcode: 200 | Uploading 236332 bytes... HTTPcode: 200 | Uploading 236332 bytes... HTTPcode: 200 | Uploading 236332 bytes... HTTPcode: 200 | Uploading 236332 bytes... HTTPcode: 200 | Uploading 399413 bytes... HTTPcode: 200 | Uploading 399413 bytes... HTTPcode: 200 | Uploading 399413 bytes... HTTPcode: 200 | Uploading 399413 bytes... HTTPcode: 200 | Uploading 114157 bytes... HTTPcode: 200 | Uploading 114157 bytes... HTTPcode: 200 | Uploading 114157 bytes... HTTPcode: 200 | Uploading 114157 bytes... HTTPcode: 200 | Uploading 104024 bytes... HTTPcode: 200 | Uploading 104024 bytes... HTTPcode: 200 | Uploading 104024 bytes... HTTPcode: 200 | Uploading 104024 bytes... HTTPcode: 200 | Uploading 146058 bytes... HTTPcode: 200 | Uploading 146058 bytes... HTTPcode: 200 | Uploading 146058 bytes... HTTPcode: 200 | Uploading 146058 bytes... HTTPcode: 200 | Uploading 484364 bytes... HTTPcode: 200 | Uploading 484364 bytes... HTTPcode: 200 | Uploading 484364 bytes... HTTPcode: 200 | Uploading 484364 bytes... HTTPcode: 200 | Uploading 295864 bytes... HTTPcode: 200 | Uploading 295864 bytes... HTTPcode: 200 | Uploading 295864 bytes... HTTPcode: 200 | Uploading 295864 bytes... HTTPcode: 200 | Uploading 145076 bytes... HTTPcode: 200 | Uploading 145076 bytes... HTTPcode: 200 | Uploading 145076 bytes... HTTPcode: 200 | Uploading 145076 bytes... HTTPcode: 200 | Uploading 176613 bytes... HTTPcode: 200 | Uploading 176613 bytes... HTTPcode: 200 | Uploading 176613 bytes... HTTPcode: 200 | Uploading 176613 bytes... HTTPcode: 200 | Uploading 1192643 bytes... HTTPcode: 200 | Uploading 1192643 bytes... HTTPcode: 200 | Uploading 1192643 bytes... HTTPcode: 200 | Uploading 1192643 bytes... HTTPcode: 200 | Uploading 1196719 bytes... HTTPcode: 200 | Uploading 1196719 bytes... HTTPcode: 200 | Uploading 1196719 bytes... HTTPcode: 200 | Uploading 1196719 bytes... HTTPcode: 200 | Uploading 558302 bytes... HTTPcode: 200 | Uploading 558302 bytes... HTTPcode: 200 | Uploading 558302 bytes... HTTPcode: 200 | Uploading 723372 bytes... HTTPcode: 200 | Uploading 723372 bytes... HTTPcode: 200 | Uploading 723372 bytes... HTTPcode: 200 | Uploading 723372 bytes... HTTPcode: 200 | Uploading 317533 bytes... HTTPcode: 200 | Uploading 317533 bytes... HTTPcode: 200 | Uploading 317533 bytes... HTTPcode: 200 | Uploading 317533 bytes... HTTPcode: 200 | Uploading 15117 bytes... HTTPcode: 200 | Uploading 15117 bytes... HTTPcode: 200 | Uploading 15117 bytes... HTTPcode: 200 | Uploading 42454 bytes... HTTPcode: 200 | Uploading 42454 bytes... HTTPcode: 200 | Uploading 42454 bytes... HTTPcode: 200 | Uploading 42454 bytes... HTTPcode: 200 | Uploading 5189379 bytes... HTTPcode: 200 | Uploading 5189379 bytes... HTTPcode: 200 | Uploading 5189379 bytes... HTTPcode: 200 | Uploading 281177 bytes... HTTPcode: 200 | Uploading 281177 bytes... HTTPcode: 200 | Uploading 281177 bytes... HTTPcode: 200 | Uploading 281177 bytes... HTTPcode: 200 | Uploading 321708 bytes... HTTPcode: 200 | Uploading 321708 bytes... HTTPcode: 200 | Uploading 321708 bytes... HTTPcode: 200 | Uploading 321708 bytes... HTTPcode: 200 | Uploading 187965 bytes... HTTPcode: 200 | Uploading 187965 bytes... HTTPcode: 200 | Uploading 187965 bytes... HTTPcode: 200 | Uploading 187965 bytes... HTTPcode: 200 | Uploading 133850 bytes... HTTPcode: 200 | Uploading 133850 bytes... HTTPcode: 200 | Uploading 133850 bytes... HTTPcode: 200 | Uploading 560584 bytes... HTTPcode: 200 | Uploading 560584 bytes... HTTPcode: 200 | Uploading 560584 bytes... HTTPcode: 200 | Uploading 560584 bytes... HTTPcode: 200 | Uploading 385870 bytes... HTTPcode: 200 | Uploading 385870 bytes... HTTPcode: 200 | Uploading 385870 bytes... HTTPcode: 200 | Uploading 385870 bytes... HTTPcode: 200 | Uploading 459479 bytes... HTTPcode: 200 | Uploading 459479 bytes... HTTPcode: 200 | Uploading 459479 bytes... HTTPcode: 200 | Uploading 459479 bytes... HTTPcode: 200 |\nTotal bytes uploaded: 67225188. Thanks for the additional information @HeJianqiao. We are looking further into this as it can be reproduced with calls to other services as well. In the meantime, if you are able to raise the memory limit for your production environment this should help prevent the memory exhaustion errors you are currently facing.. Upon further investigation we have found that, while the SDK (or one of its dependencies) does appear to have cyclic references that have to be cleaned up by PHP's garbage collector, garbage collection does occur naturally after a certain amount of references have built up. \nUnfortunately this doesn't occur frequently enough to work as expected with a memory limit as low as your use case requires, so you'll need to call gc_collect_cycles() manually to avoid memory exhaustion errors if you are unable to raise your memory limit above 20M.. Thanks for reaching out to us @ricardofiorani. I am able to reproduce this, however this behavior looks to be expected from Guzzle - you'll want to invoke tick manually for the API call to actually be sent rather than relying on wait() to do so for you. \nAs explained here, asynchronous calls involving promise do not automatically call tick as this could unintentionally resolve other promises, leading to unforseen consequences depending on what promises are involved within your use case. Further discussion on this topic can be found in this thread.. Thanks for reaching out to us @matthewfleming. What you are experiencing is expected behavior from S3 - Multipart uploads throw an MalformedXML exception when zero parts are requested, but S3 doesn't explicitly call this out in the error message returned. You can use ObjectUploader instead of MultiPartUploader as shown below, since ObjectUploader will issue a putObject call for files smaller than 16MB, and automatically calls MultiPartUploader if the file size exceeds 16MB.\n```php\n$s3Client = new S3Client([\n    'version' => 'latest',\n    'region' => $container->getParameter('s3_video_bucket.region'),\n    'credentials' => [\n        'key' => $container->getParameter('s3_video_bucket.access_key_id'),\n        'secret' => $container->getParameter('s3_video_bucket.secret_access_key')\n    ]\n]);\n$bucket = $container->getParameter('s3_video_bucket.name');\n$localPath = realpath($uploadPath . $filename);\n$remotePath = \"$bucket/$filename\";\n$uploader = new ObjectUploader($client, $bucket, $filename, $localPath);\n$result = $uploader->upload();\n```\nIf your use case does not allow replacing MultiPartUploader with ObjectUploader, you can instead carve out a special case for zero-byte files to issue a putObject call.\nphp\nif (filesize($localPath) == 0) {\n    $result = $client->putObject([\n        'Bucket' => $bucket,\n        'Key' => $filename,\n        'Body' => fopen(\"{$localPath}\", 'r'),\n    ]);\n} else {\n    $uploader = new MultipartUploader($client, $localPath, [\n        'bucket' => $bucket,\n        'key' => $filename,\n    ]);\n    $result = $uploader->upload();\n};. Thanks for reaching out to us @ErikThiart. Please keep in mind that the default credentials provider looks for credentials in the following locations, and does not take static credentials declared in-code into account.\n\nEnvironment variables\n\"default\" profile in ~/.aws/credentials (relative to the user running the script)\nECS credentials if ECS environment variable is presented\nEC2 instance profile credentials\n\nIn addition to this, a credentials provider does not equate to a credentials interface, which is what the signRequest call is looking for in its second argument. Per the CredentialProvider class documentation page:\n\nCredential providers are functions that accept no arguments and return a promise that is fulfilled with an Aws\\Credentials\\CredentialsInterface or rejected with an Aws\\Exception\\CredentialsException.\n\nIn order to use credentials from the list above via the default credential provider you need to complete the promise returned by the credential provider with wait() as shown below.\nphp\n$provider = CredentialProvider::defaultProvider();\n$credentials = $provider()->wait();\nAlternately, if you prefer to keep the credentials in-code, you can declare your Access Key ID and Secret Access Key as separate variables and use those in a new Credentials() call as shown below. This will require using Aws\\Credentials\\Credentials instead of Aws\\Credentials\\CredentialProvider\nphp\n$key = 'AccessKeyId';\n$secret = 'SecretAccessKey';\n$creds = new Credentials($key, $secret);\nIn summary, lines 23-30 in your example need to be changed to do one of the following.\n\nMap the specified credentials to a new Credentials interface\nConsume credentials from the default provider chain by completing the promise initiated from the specified CredentialProvider. Thanks for the additional information! The error response looks like it is being returned by your API rather than the API Gateway service itself. Typically an error message involving a security token (as opposed to a session token) means the IAM credentials in use are incorrect, or the IAM user that the credentials map to has MFA enabled and the MFA token is not being passed in. To answer your question directly, the session token is generally only required for calls that use temporary credentials such as the credentials resultant from a GetSessionToken call, or when using an assumed role. Calls that use static API keys for an IAM user shouldn't require a session token to successfully authenticate.\n\nDepending on what you're using for authentication with your API, it may require a session token to properly complete authentication - using the sample PetStore API with the AWS_IAM authorization type I was able to issue GET and POST requests using the Access Key ID and Secret Access Key for an IAM user with the appropriate permissions. However, judging by this AWS Forums post, using a Cognito Identity Pool as your authorizer seems to require including the session token in the credential set for requests sent to your API.\nThe SigV4 signing looks to be happening correctly with your code, so this appears to be more of an issue with the service. If you continue to experience problems authenticating against your API you may want to open a new support case under the API Gateway service.. Hi @shangdev, thanks for reaching out to us. The way in which you are separating the error code, message, and HTTP status code in your new WPError() call is correct. If you were to, for example, run this with working credentials that do not have the correct permissions to list the CloudFront distributions on your account, the first argument to new WPError() would be \"AccessDenied\", the second would be \"User: arn:aws:iam::ACCOUNT-ID:user/USERNAME is not authorized to perform: cloudfront:ListDistributions\" and the third would be \"403\" as you are expecting.. Hi @shangdev, thanks for reaching out to us. Looking at the provided screenshot it appears your code is being executed with the IAM Role AdWords as expected, however that role does not have the correct IAM permissions to issue the ListDistributions API call. You'll want to review the IAM policies attached to this role to ensure it is allowed to perform this action.. Thanks for reaching out to us @shiratsu. Unfortunately I'm not able to reproduce this behavior with your code, using it to send emails to myself as well as the mailbox simulator results in a successfully sent message each time. \nSince this result is not occurring consistently on your end, this behavior looks to be originating from SES rather than the AWS SDK for PHP. I suggest opening a new support case under the SES service so this behavior can be further investigated from the service side.. Hi @samueleastdev, thanks for reaching out to us. The feature you are describing is a customization of the AWS Console that is not available for any of the AWS SDKs; the CreateChannel API for MediaPackage does not include an option to automatically create a CloudFront distribution upon channel creation. \nIn order to create a CloudFront distribution for your newly created MediaPackage channel you will first need to create an endpoint for your channel with a CreateOriginEndpoint call, then issue a CloudFront CreateDistribution call using the endpoint you created for the channel as the distribution's origin.\nIf you would like to see this console feature implemented as part of the MediaPackage API, I suggest submitting a service feature request by creating a new support case under the MediaPackage service, or posting in the AWS Elemental MediaPackage Forums.. Hi @blacksaildivision, thanks for reaching out to us. Is there a particular reason your ~/.aws/ directory cannot be added to open_basedir in your PHP config? That would be the easiest way to resolve this. \nIf your use case does not support this approach, another way to resolve this would be to set your $HOME environment variable in the session in which your application is running to a directory inside of the paths you've specified in open_basedir so that the SDK looks for .aws/config in a path that can be accessed by PHP in your application.. In this case it is not S3Client that's trying to read .aws/config but a different package instead. We want the SDK to check the config file for any options that aren't otherwise specified in code, however you can override this behavior by setting the environment variable AWS_CSM_ENABLED to false.. Hi @haoxi911, thanks for reaching out to us. Your assessment that this is due to daylight saving time is correct. Since strtotime takes DST into consideration when using a time zone that is affected by DST, and DST in US ends Nov 4 at 2 AM (less than a week from now), the transition from 1AM to 2AM on Nov 4 takes 7200 seconds instead of 3600 as expected of a typical 1-hour period. This in turn adds an hour to your presign request's duration of 1 week, resulting in 608400 seconds instead of the expected 604800 seconds for a normal week's duration. \nIn order to avoid this you'll want to either specify a time zone that is not affected by DST (e.g. Etc/GMT-4), or set your presign request's duration less than or equal to 6 days and 23 hours.. No problem! I'm not aware of any common/best practices that advise against using any given timezone, but keeping the presign duration at the maximum value of 1 week with a timezone that observes DST would result in this issue resurfacing once a year when the clock is set back an hour for that timezone. \nIf changing the presign duration works better for your use case I advise changing that instead of the timezone. Please don't hesitate to reach out again if you run into other issues!. Hi @iNviNho, thanks for reaching out to us about this. The AWS SDK for PHP currently has no connection timeout set by default, so it will wait indefinitely for a response from the service. This behavior can be changed in the client options as shown in our Configuration Options documentation page.\nI see that the issue you linked in the AWS SDK for JavaScript resulted in that SDK setting a default timeout of 60 seconds on all sockets, this is a feature worth considering for our SDK as well so I'll mark the issue as a feature request and bring it up in our next sprint.. Thanks for the additional information @iNviNho. I did some testing on this and found that, even without specifying a connection timeout in my SqsClient parameters, disabling the connection on my local host in between iterations of a while loop that retrieves messages from an SQS queue resulted in the following error after 20 seconds per the value of WaitTimeSeconds on the receiveMessage call.\n```\nPHP Fatal error: Uncaught exception 'Aws\\Sqs\\Exception\\SqsException' with message 'Error executing \"ReceiveMessage\" on \"https://sqs.us-west-2.amazonaws.com//Queue1\"; AWS HTTP error: cURL error 7:  (see http://curl.haxx.se/libcurl/c/libcurl-errors.html)'\nGuzzleHttp\\Exception\\ConnectException: cURL error 7:  (see http://curl.haxx.se/libcurl/c/libcurl-errors.html) in /Users/alex/php-sdk/aws-sdk/Aws/WrappedHttpHandler.php on line 191\nAws\\Sqs\\Exception\\SqsException: Error executing \"ReceiveMessage\" on \"https://sqs.us-west-2.amazonaws.com//Queue1\"; AWS HTTP error: cURL error 7:  (see http://curl.haxx.se/libcurl/c/libcurl-errors.html) in /Users/alex/php-sdk/aws-sdk/Aws/WrappedHttpHandler.php on line 191\nCall Stack:\n    0.0009     395328   1. {main}() /Users/alex/php-sdk/GH/1662/sqsDisco.php:0\n   10.2083    3944160   2. Aws\\Sqs\\SqsClient->receiveMessage() /Users/alex/php-sdk/GH/1662/sqsDisco.php:23\n   10.2083    3944536   3. Aws\\Sqs\\SqsClient->__call() /Users/alex/php-sdk/GH/1662/sqsDisco.php:23\n   10.2084    3945176   4. Aws\\Sqs\\SqsClient->execute() /Users/alex/php-sdk/aws-sdk/Aws/AwsClientTrait.php:77\n   10.2093    3960816   5. GuzzleHttp\\Promise\\Promise->wait() /Users/alex/php-sdk/aws-sdk/Aws/AwsClientTrait.php:58\n   30.2513    4154992   6. GuzzleHttp\\Promise\\RejectedPromise->wait() /Users/alex/php-sdk/aws-sdk/GuzzleHttp/Promise/Promise.php:65\n```\nModifying this test to introduce a try/catch block resulted in a shorter (but still present) error message, and kept the while loop going which resulted in the loop retrieving messages again after re-enabling my local host's connection. Following is the code I used to test with try/catch and its corresponding output.\nCode\n```php\n<?php\ndate_default_timezone_set(\"America/Los_Angeles\");\nrequire_once(\"/path/to/aws-sdk/aws-autoloader.php\");\nuse Aws\\Sqs\\SqsClient;\nuse Aws\\Exception\\AwsException;\n$client = new SqsClient([\n    'region' => 'us-west-2',\n    'version' => 'latest',\n//    'debug' => true,\n//    'http' => [\n//        'connect_timeout' => 1\n//    ],\n]);\n$queue = 'https://sqs.us-west-2.amazonaws.com//Queue1';\nwhile (true) {\n    $now = date('m/d/Y h:i:s a', time());\n    echo \"{$now}: Attempting message retrieval...\\n\";\n    try {\n        $resp = $client->receiveMessage([\n            'QueueUrl' => $queue,\n            'MaxNumberOfMessages' => 10,\n            'WaitTimeSeconds' => 20\n        ]);\n        $mCount = count($resp['Messages']);\n        $successdate = date('m/d/Y h:i:s a', time());\n        echo \"{$successdate}: Retrieved {$mCount} messages.\\n\";\n    } catch (AwsException $e) {\n        $errdate = date('m/d/Y h:i:s a', time());\n        echo \"{$errdate}: Error retrieving messages:\\n\".$e->getMessage().\"\\n\";\n    }\n    sleep(10);\n}\n```\nOutput\n$ php sqsDisco.php \n11/09/2018 05:28:12 pm: Attempting message retrieval...\n11/09/2018 05:28:12 pm: Retrieved 10 messages.\n11/09/2018 05:28:22 pm: Attempting message retrieval...\n11/09/2018 05:28:22 pm: Retrieved 10 messages.\n[Disabled connection here]\n11/09/2018 05:28:32 pm: Attempting message retrieval...\n11/09/2018 05:29:07 pm: Error retrieving messages:\nError executing \"ReceiveMessage\" on \"https://sqs.us-west-2.amazonaws.com/<accout-id>/Queue1\"; AWS HTTP error: cURL error 7:  (see http://curl.haxx.se/libcurl/c/libcurl-errors.html)\n11/09/2018 05:29:17 pm: Attempting message retrieval...\n11/09/2018 05:29:18 pm: Error retrieving messages:\nError executing \"ReceiveMessage\" on \"https://sqs.us-west-2.amazonaws.com/<account-id>/Queue1\"; AWS HTTP error: cURL error 7:  (see http://curl.haxx.se/libcurl/c/libcurl-errors.html)\n[Enabled connection here]\n11/09/2018 05:29:28 pm: Attempting message retrieval...\n11/09/2018 05:29:28 pm: Retrieved 10 messages.\n11/09/2018 05:29:38 pm: Attempting message retrieval...\n11/09/2018 05:29:38 pm: Retrieved 10 messages.\n^C\nCould you test the above code to see if it behaves differently from your implementation? If you still see the same behavior with the example above it's likely that the connection hanging indefinitely is caused by something in your environment other than the AWS SDK for PHP.. Thanks for the update @iNviNho! I'm glad to hear you were able to narrow down the scope of this behavior. After discussing this feature request with the rest of the team we've decided not to implement a default HTTP connection timeout for the current version of the AWS SDK for PHP since it would introduce a breaking change, however this feature will be considered for the next major version of the SDK.\nUnfortunately Docker containers and how they handle networking compared to physical hosts or EC2 instances is not within my area of expertise, however this article seems like a good place to start if you have not already come across it during your troubleshooting steps.. Hi @alquerci, thanks for reaching out about this. Unfortunately the Instance Profile Credential Provider does not use the same retry middleware that is used when issuing service API calls, and its default Guzzle handler does not automatically retry the Instance Profile retrieval when an unsuccessful response is returned by the Instance Metadata service. \nThat being said, it is possible to pass your own handler to the Credential Provider so that retries can be implemented to fit your use case. Below is an example of implementing a custom handler on an Instance Profile Credential Provider for an S3 client based on our Handlers and Middleware documentation page.\n```php\n$myHandler = function (CommandInterface $cmd, RequestInterface $request) {\n    $result = new Result(['foo' => 'bar']);\n    // Implement retry logic here\n    return Promise\\promise_for($result);\n};\n$provider = CredentialProvider::instanceProfile([\n    'handler' => $myHandler\n]);\n$creds = $provider()->wait();\n$client = new S3Client([\n    'region' => 'us-west-2',\n    'version' => 'latest',\n    'credentials' => $creds\n]);\n```. @alquerci, apologies for the delay in response from our end. To answer your questions directly:\n\nFor further investigation how is it possible that the Instance Metadata service return an invalid JSON on the response body?\n\nThere are a few cases where the instance metadata service is known not to work as expected:\n- The instance metadata service can be unavailable if the OS has no route to the metadata service as seen here. \n- Requests to the metadata service are throttled on a per-instance basis as mentioned in the 'Throttling' section of this documentation page.\n- There are a few AWS Forum posts that discuss issues interfacing with the instance metadata service here and here. The first one may be exclusive to m5.large instances, however the second one may apply to your use case depending on how your auto-scaling group starts the application that uses the SDK.\nUnfortunately the behavior you're describing does not sound similar to any of the above since you're receiving an OK 200 response from the metadata service (just not a complete/expected response), so it's possible you've encountered an edge case with Auto-Scaling and the instace metadata service that warrants further investigation from the EC2 service team. I recommend opening a new support case under the EC2 service as the Premium Support engineers working under that profile will have more in-depth knowledge of how the instance metadata service works.\n\nIs it planned to patch the Instance Profile Credential Provider?\n\nI've brought this up as a feature request in our most recent sprint, and it has been added as a backlog item to investigate in the near future.\n\nIs the Instance Profile Credential Provider open for to be patched with a pull request?\n\nWe would be happy to review a PR for this! You may review our contribution guidelines here.. Hi @sh-ogawa, thanks for reaching out to us about this. This behavior sounds similar to #1645 where we found that the SDK (or one of its dependencies) does appear to have cyclic references that have to be cleaned up by PHP's garbage collector, however garbage collection does occur naturally after a certain amount of references have built up. \nWhen running the code you provided I see that memory usage builds up from 3.29MB to about 8.19MB across ~500 iterations of the loop before PHP's garbage collection begins naturally. This is a result of the SDK favoring a faster runtime over a lighter memory footprint, however you can call gc_collect_cycles() manually to override this behavior. Including this function at the end of the loop resulted in the memory usage remaining at a consistent 3.29MB instead of slowly increasing to ~8MB before garbage collection begins automatically.. Thanks for reaching out to us about this @dreamer-89, it certainly sounds like unexpected behavior stemming from the SDK. Can you provide complete code samples showing the error and success cases so that we can verify this behavior on our end?. Thanks for reaching out to us @AngieRd. The error you're receiving suggests there is an issue with the clock on the host generating the request signature (likely the same host issuing the request, unless you're presigning the request elsewhere and passing it to the host sending the request). This could be caused by things such as the time zone not being set properly on this machine, or its clock being more than a few minutes out of sync at the time the request is signed. You may need to double check the machine's time zone and synchronize its clock with a public NTP server to avoid this behavior.\nI was able to reproduce the error message with valid IAM user credentials by manually setting my local machine's clock back 6 minutes, however this error was caught as an AwsException as expected using the catch block shown below.\nphp\ntry {\n    $result = $SesClient->sendEmail([\n      // sendEmail parameters\n    ]);\n    $messageId = $result['MessageId'];\n    echo(\"Email sent! Message ID: $messageId\".\"\\n\");\n} catch (AwsException $e) {\n    // output error message if request fails\n    echo(\"The email was not sent. Error message: \".$e->getAwsErrorMessage().\"\\n\");\n    echo \"\\n\";\n}. You're right, if the timezone was incorrect all requests would fail rather than a small percentage. If there are multiple requests that succeed immediately before the one that hangs indefinitely, it would stand to reason that your machine's clock is at least sufficiently in sync that this error shouldn't come up. \nThat being said, it may still be a good idea to synchronize the machine's time with NTP (e.g. sudo ntpdate pool.ntp.org) to see if it helps reduce/eliminate the frequency of this error, however if you can provide a complete code sample (removing sensitive data such as source/destination addresses, hard-coded API keys, etc) this would help us better understand exactly how you're interacting with SES and potentially identify components that might result in this behavior.\nTo answer your question about setting a timeout for API calls, you can specify a connection timeout in the http options when initializing the service client as shown below.\nphp\n$SesClient = new SesClient([\n    'http' => [\n        'connect_timeout' => 10\n    ],\n    'version' => 'latest',\n    'region'  => 'us-west-2',\n]);. Hi @techman1984, thanks for reaching out to us about this. The error you're seeing typically comes up when a service client is initialized without a 'version' parameter - in this case it looks to be an S3 Client. If you cannot see the full stack trace pointing out where this error is originating you may want to search your application's directory for PHP files containing an S3 client initialization, then check to make sure those are specifying an API version in the client parameters. This can be done with grep as shown below. \ngrep -rnw '/path/to/application/' -e 'new S3Client' -e 'new Aws\\S3\\S3Client' --exclude-dir=/path/to/aws-sdk-for-php/\n-r will search recursively,\n-n will include the line number where the expression was found,\n-w will match the whole word instead of partial matches,\n-e will search for the given expression, and can be used multiple times in the same command to search for multiple expressions,\n--exclude-dir is used to exclude the SDK's installation directory so you don't mistakenly remove 'S3Client' from any files in the SDK.\nI hope this helps you identify where your S3Client is being improperly initialized! Please don't hesitate to reach back out if you continue running into trouble with this.. Unfortunately the troubleshooting steps taken on your end are not clear to me - it sounds like you tried searching for 'new S3Client' and 'new Aws\\S3\\S3Client' and weren't able to find any instances of these expressions, please correct me if I'm wrong. It might be worth searching for just 'S3Client' as well since the S3 Client initialization in version 2 of the AWS SDK for PHP is different from version 3.\nIf your grep search does not show any files that initialize an S3 Client in your entire WordPress directory, you shouldn't be getting this error. Reviewing the full stack trace would point out the files that are involved in this error, which would help narrow down where to look for this misconfiguration. \nIt is possible that one or more of your WordPress plugins conflicts with the AWS SDK for PHP v3.x as seen here, here, here, and here. I suggest disabling your WP plugins systematically to ensure this isn't the cause of your error. \nYou could disable all WP plugins and enable them one by one until you start seeing this error again to identify the culprit, or take a binary search approach where you disable half your plugins, check to see if the behavior persists, if it does then disable half of the remaining enabled plugins and repeat until things work as expected. From there, enable one by one the plugins from the last set of plugins that were disabled until you start seeing the error again to see which plugin is conflicting with the SDK.. Hi @DrCheer, thanks for reaching out to us. Unfortunately the Elastic Transcoder Service API does not support applying metadata to S3 objects created during a transcode job. If you would like to see this feature added to the service I suggest creating a new feature request either by opening a new support case under the ETS service or posting in the Elastic Transcoder forums.\nThat being said, you can use an S3Client  in the application initiating your transcode job that calls HeadObject on the source object to retrieve the Content-Disposition header value, then call CopyObject with 'MetadataDirective' => 'REPLACE' and the ContentDisposition value pulled from the source object. I have included an example for this below.\n```php\n// Calling CreateJob and pulling job ID for waiter\ntry {\n    $newJob = $etsClient->createJob($createParams);\n    $jobId = $newJob['Job']['Id'];\n    echo \"Created ETS job with ID {$jobId}.\\n\";\n} catch (AwsException $e) {\n    echo \"Unable to create transcode job:\\n\".$e->getMessage();\n}\n// Reading pipeline properties to retrieve output bucket.\ntry {\n    $pipeline = $etsClient->readPipeline(['Id' => $createParams['PipelineId']]);\n    $bucket = $pipeline['Pipeline']['OutputBucket'];\n} catch (AwsException $e) {\n    echo \"Error retrieving pipeline's output bucket:\\n{$e}\\n\";\n}\n// Retrieving source object metadata.\ntry {\n    $source = $s3Client->headObject([\n        'Bucket' => $bucket,\n        'Key' => $createParams['Input']['Key'],\n    ]);\n} catch (AwsException $e) {\n    echo \"Error retrieving source object metadata:\\n{$e}\\n\";\n    exit(1);\n}\n// Waiting for transcode job to complete before continuing\ntry {\n    echo \"Waiting for transcode job to complete...\";\n    $etsClient->waitUntil('JobComplete', ['Id' => $jobId]);\n    echo \"Done!\\n\";\n} catch (AwsException $e) {\n    echo \"Waiter error:\\n{$e}\\n\";\n    exit(1);\n}\n// Listing output objects in case we need to iterate over several (e.g. transcoding to HLS).\ntry {\n    $files = $s3Client->listObjects([\n        'Bucket' => $bucket,\n        'Prefix' => $createParams['OutputKeyPrefix'] . $createParams['Output']['Key'],\n    ]);\n} catch (AwsException $e) {\n    echo \"Failed to list output objects:\\n{$e}\\n\";\n}\nforeach ($files['Contents'] as $file) {\n    try {\n        // Retrieve output object metadata to preserve Content-Type header value.\n        $object = $s3Client->headObject([\n            'Bucket' => $bucket,\n            'Key' => $file['Key']\n        ]);\n    } catch (AwsException $e) {\n        echo \"Failed to retrieve object metadata:\\n{$e}\\n\";\n    }\n    try {\n        // Copying object back to itself, applying Content-Disposition header value from source object and preserving\n        // existing Content-Type header value.\n        $s3Client->copyObject([\n            'Bucket' => $bucket,\n            'CopySource' => $bucket . '/' . $file['Key'],\n            'ContentDisposition' => $source['ContentDisposition'],\n            'ContentType' => $object['ContentType'],\n            'Key' => $file['Key'],\n            'MetadataDirective' => 'REPLACE'\n        ]);\n        echo \"Updated metadata for object {$file['Key']}\\n\";\n    } catch (AwsException $e) {\n        echo \"Failed to copy object:\\n{$e}\\n\";\n    }\n}\n```. Hello @taylorotwell, thanks for reaching out to us. While a bit confusing, this is expected behavior from the service's API. Per the Get Base Path Mapping and Update Base Path Mapping API reference pages:\n\nTo specify an empty base path in the request URL, set the base_path path variable to (none).\n\nI was able to reproduce the behavior you're describing when setting'basePath'=> '', both for getBasePathMapping and updateBasePathMapping calls, however when setting 'basePath' => '(none)' both operations worked as expected. \nUnfortunately this behavior is not made clear in the documentation for the AWS SDK for PHP (or our other SDKs), so I've reached out to the appropriate team internally to have this note included in our documentation.. Hello @poisa, thanks for reaching out to us about this. CommandPool's 'rejected' state is contingent upon receiving an error response from the service to which the API call is being sent, and the try/catch block you're using here doesn't try each command in the CommandPool separately but rather the entire pool at once. Since parameter validation for API calls sent by the SDK occurs on the client end just before the call is made, API calls with invalid parameters short circuit the SDK and end the script right before the invalid API call is sent over the wire.\nThere are currently two ways to resolve this:\n\nDisabling client-side parameter validation \nThis can be done by setting validate => false when initializing the service client as shown here. This will cause the service to return an HTTP 400 error with an AwsException on invalid API calls that will cause the CommandPool to go into the 'rejected' state for this command, so the try/catch block will work as expected\nValidating your commands before adding them to the CommandPool \nThis is a bit more involved since you'll need to retrieve the shape for the command parameters on each command being validated, but you can iterate over your $commands array to validate each one then add the ones that pass validation to a separate array to be used in the command pool. I have included an example below that integrates command validation into the example you have provided.\n\n```php\n<?php\nuse Aws\\CommandInterface;\nuse Aws\\CommandPool;\nuse Aws\\Exception\\AwsException;\nuse Aws\\ResultInterface;\nuse Aws\\Ses\\SesClient;\nuse Aws\\Api\\Validator;\nuse Aws\\Api\\Service;\nuse Aws\\Api\\ApiProvider;\nrequire 'vendor/autoload.php';\n$sesClient = new SesClient([\n    'version' => 'latest',\n    'region'  => 'us-east-1',\n]);\n$commands = [\n    // Intentionally leaving out 'Message' key to provide Command Pool\n    // with an invalid object.\n    $sesClient->getCommand('SendEmail', [\n        'X-MESSAGE-ID' => 123,\n        'Source'       => 'example@example.com',\n        'Destination'  => [\n            'ToAddresses' => ['success@simulator.amazonses.com'],\n        ]\n    ]),\n    // Including 'Message' key but using unverified source address so that\n    // the command passes validation but returns an error response from the service.\n    $sesClient->getCommand('SendEmail', [\n        'X-MESSAGE-ID' => 456,\n        'Source'       => 'example@example.com',\n        'Destination'  => [\n            'ToAddresses' => ['success@simulator.amazonses.com'],\n        ],\n        'Message' => [\n            'Body' => [\n                'Text' => [\n                    'Data' => 'This message was sent with the AWS SDK for PHP'\n                ]\n            ],\n            'Subject' => [\n                'Data' => 'PHP SDK Test'\n            ]\n        ]\n]),\n// Valid command with verified source address/domain to show success case.\n$sesClient->getCommand('SendEmail', [\n    'X-MESSAGE-ID' => 789,\n    'Source'       => 'example@verified.domain',\n    'Destination'  => [\n        'ToAddresses' => ['success@simulator.amazonses.com'],\n    ],\n    'Message' => [\n        'Body' => [\n            'Text' => [\n                'Data' => 'This message was sent with the AWS SDK for PHP'\n            ]\n        ],\n        'Subject' => [\n            'Data' => 'PHP SDK Test'\n        ]\n    ]\n])\n\n];\n// Initializing command validator.\n$validator = new Validator();\n// Specifying SES API to use in validation.\n$api = new Service(\n    ApiProvider::resolve(\n        ApiProvider::defaultProvider(),\n        'api',\n        'email',\n        '2010-12-01'\n    ),\n    ApiProvider::defaultProvider()\n);\n// Initializing array to add commands that pass validation.\n$validCommands = [];\n// Iterating through command list to validate each command.\nforeach ($commands as $command) {\n    $operation = $api->getOperation($command->getName());\n    try {\n        $validator->validate(\n            $command->getName(),\n            $operation->getInput(),\n            $command->toArray()\n        );\n        printf(\"Command with message id %d is valid\\n\", $command['X-MESSAGE-ID']);\n        // Add command to $validCommands if it is valid.\n        array_push($validCommands, $command);\n    } catch (\\Exception $e) {\n        printf(\"Invalid command with message id %d:\\n\", $command['X-MESSAGE-ID']);\n        var_dump($e->getMessage());\n    }\n}\ntry {\n    // CommandPool now uses $validCommands array instead of the original $commands array\n    $pool = new CommandPool($sesClient, $validCommands, [\n        'before'    => function (CommandInterface $cmd, $iteratorId) use ($validCommands) {\n            $command = $validCommands[$iteratorId]->toArray();\n            printf(\"Before: message id %d\\n\", $command['X-MESSAGE-ID']);\n        },\n        'fulfilled' => function (ResultInterface $result, $iteratorId) use ($validCommands) {\n            $command = $validCommands[$iteratorId]->toArray();\n            // Adding SES message ID to success case.\n            printf(\"fulfilled: message id %d with SES ID %s\\n\", $command['X-MESSAGE-ID'], $result['MessageId']);\n        },\n        'rejected'  => function (AwsException $reason, $iteratorId) use ($validCommands) {\n            $command = $validCommands[$iteratorId]->toArray();\n            printf(\"rejected: message id %d:\\n%s\\n\", $command['X-MESSAGE-ID'], $reason->getMessage());\n        },\n    ]);\n    $promise = $pool->promise();\n    $promise->wait();\n} catch (\\Exception $e) {\n    printf(\"CommandPool try/catch block: %s. %s\\n\", get_class($e), $e->getMessage());\n}\n``. Hi @riandyrn, thanks for reaching out to us. Your understanding of theconnect_timeout` option in the service client is correct, however it looks like retries are not being taken into account in your client code. \nIf you set 'retries' => 0 in your DynamoDbClient you should see that the client will indeed take 2 seconds to timeout, however if you leave retries at the default value  of 3 and enable debug logging on the client ('debug' => true) you can see that the client will retry the request after it times out, and will add delays on top of the 2 second timeout on subsequent retries.. Hi @vitoacme, thanks for reaching out to us. Unfortunately I'm not able to reproduce the described behavior after downgrading my PHP SDK to version 3.80.0, running your code sample with a valid phone number returned the expected response from Pinpoint on my end.\nJudging by the version string provided it looks like you're consuming the SDK via the aws.phar file, it's possible that you may have downloaded version 3.80.0 and didn't upgrade to this version using Composer. Another possibility is that something in your environment is preventing the SDK from working as expected.\nCould you try your code sample using the SDK's .zip file for version 3.80.0? If that does not change the behavior on your end I suggest testing this version of the SDK on a fresh host (e.g. EC2, docker, or a physical host that normally does not run PHP applications) to rule out potential environment problems on the host(s) where this is occurring.. Hi @AlessandroMinoccheri, thanks for reaching out to us. When using getIterator, the object returned is a Generator upon which you need to iterate to retrieve the results it contains (e.g. with foreach) as shown below.\n```php\n$iterator = $this->client->getIterator('Scan', array(\n    'TableName' => $this->table\n));\nforeach($iterator as $item) {\n    var_dump($item);\n}\n```. The response from both methods should contain the same dataset that DynamoDB provided as a response to the API call, however using the generator eliminates the need to allocate memory for the complete response so that memory is only allocated for the components you want to retrieve from this response. \nIn the example below you can see that scanning a table via scan and invoking getIterator to scan the same table returns different output (for example metadata for the request is not included in the iterator), however the returned table contents are the same.\n```php\nroot@362f454aeab1:/var/www/html/GH/1686# cat ddbIterator.php\n<?php\nrequire_once(\"/var/www/html/aws-sdk/aws-autoloader.php\");   \nuse Aws\\DynamoDb\\DynamoDbClient;      \n$client = new DynamoDbClient([      \n    'region' => 'us-west-2',        \n    'version' => 'latest', \n    // 'debug' => true,  \n]);  \n$scan = $client->scan([  \n    'TableName' => 'go-2223'        \n]);  \necho \"Scan results:\\n\";  \nvar_dump($scan);\n$iterator = $client->getIterator('Scan', [     \n    'TableName' => 'go-2223'        \n]);  \necho \"\\nDumping iterator result:\\n\";\nvar_dump($iterator);       \necho \"\\nIterating over iterator result:\\n\";    \nforeach($iterator as $item) {       \n    var_dump($item);     \n}  \nroot@362f454aeab1:/var/www/html/GH/1686# php ddbIterator.php\nScan results: \nobject(Aws\\Result)#152 (2) {\n  [\"data\":\"Aws\\Result\":private]=>\n  array(4) {\n    [\"Items\"]=>\n    array(1) {\n      [0]=>\n      array(3) {\n        [\"key\"]=>\n        array(1) {\n          [\"S\"]=>\n          string(5) \"value\"\n        }\n        [\"go-2223\"]=>\n        array(1) {\n          [\"S\"]=>\n          string(4) \"test\"\n        }\n        [\"test\"]=>\n        array(1) {\n          [\"S\"]=>\n          string(4) \"true\"\n        }\n      }\n    }\n    [\"Count\"]=>\n    int(1)\n    [\"ScannedCount\"]=>\n    int(1)\n    [\"@metadata\"]=>\n    array(4) {\n      [\"statusCode\"]=>\n      int(200)\n      [\"effectiveUri\"]=>\n      string(40) \"https://dynamodb.us-west-2.amazonaws.com\"\n      [\"headers\"]=>\n      array(7) {\n        [\"server\"]=>\n        string(6) \"Server\"\n        [\"date\"]=>\n        string(29) \"Thu, 13 Dec 2018 21:57:26 GMT\"\n        [\"content-type\"]=>\n        string(26) \"application/x-amz-json-1.0\"\n        [\"content-length\"]=>\n        string(3) \"103\"\n        [\"connection\"]=>\n        string(10) \"keep-alive\"\n        [\"x-amzn-requestid\"]=>\n        string(52) \"T0GOQ6BAI074ITRG0U12BGN54BVV4KQNSO5AEMVJF66Q9ASUAAJG\"\n        [\"x-amz-crc32\"]=>\n        string(10) \"1031448073\"\n      }\n      [\"transferStats\"]=>\n      array(1) {\n        [\"http\"]=>\n        array(1) {\n          [0]=>\n          array(0) {\n          }\n        }\n      }\n    }\n  }\n  [\"monitoringEvents\":\"Aws\\Result\":private]=>\n  array(0) {\n  }\n}\nDumping iterator result:\nobject(Generator)#120 (0) {\n}\nIterating over iterator result:\narray(3) {\n  [\"key\"]=>\n  array(1) {\n    [\"S\"]=>\n    string(5) \"value\"\n  }\n  [\"go-2223\"]=>\n  array(1) {\n    [\"S\"]=>\n    string(4) \"test\"\n  }\n  [\"test\"]=>\n  array(1) {\n    [\"S\"]=>\n    string(4) \"true\"\n  }\n}\n``. Hi @AlessandroMinoccheri, in order for us to better troubleshoot this behavior can you provide the version of the AWS SDK for PHP that you are using, along with debug output when attempting this call? You can enable debug logging by setting'debug' => true,in the DynamoDB client configuration options.. Thanks for the additional info. TheUser-Agentvalue shows you're using version 3.80.2 of the AWS SDK for PHP, which should definitely support executing DynamoDB'sTransactWriteItemsAPI call. Can you try updating the SDK, either via Composer or using the phar/zip files in our [releases](https://github.com/aws/aws-sdk-php/releases) page, to see if you continue receiving this error?. Looking more closely at the error response in your initial post I see that you're running this against a local instance of DynamoDB rather than a DynamoDB endpoint in AWS. It is possible that your local version of DynamoDB is not new enough to supportTransactWriteItems`,  or that the local version of DynamoDB has not yet had an update to include this API call. \nCan you try running this call against a DynamoDB table that exists in AWS rather than on your local host to rule out the AWS SDK for PHP as the culprit for this behavior? If this call doesn't work on an AWS endpoint you should check the API model for DynamoDB on your local copy of the SDK to ensure it includes the TransactWriteItems API, this can be found in the file src/data/dynamodb/2012-08-10/api-2.json in your SDK directory.. @AlessandroMinoccheri @bcdxn Thanks for the additional information & feedback on this! I've reached out to the DynamoDB team internally asking for a status and timeline on updating the DynamoDB Local Docker container to support Transactions, once I have more info on this I'll update the issue accordingly.. Thanks to everyone involved in the thread for your patience and feedback on this! The DynamoDB team responded to my internal request today stating that they are actively working on an update for DynamoDB Local that implements support for Transactions API calls. While they unfortunately do not have a release date for this update, it should be available fairly soon.. Thanks for reaching out to us @taylorotwell. Unfortunately there is no way to force a given value for the Cache-Control header when creating a pre-signed PUT URL for S3. The PUT call that is issued using the pre-signed URL must include the desired Cache-Control header value, if this header is not present in the PUT request issued with the pre-signed URL it will not be included in the object's metadata.\nWhile a pre-signed PUT URL cannot dictate the value of the Cache-Control header for the resultant PUT request, you can create a pre-signed POST request that uses a POST policy restricting what the request can and cannot contain as shown here. More information on uploads to S3 using HTTP POST can be found on this page of the S3 API reference.. Hi @growbeauty, thanks for reaching out to us. The warning you're receiving looks like it is referring to the PHP client for Elasticsearch rather than the AWS SDK for PHP - I was able to locate the function you mention in another repository that does not belong to Amazon. Opening an issue under that repository would be more appropriate in this case.. Hi @fedor-softgrad, thanks for reaching out to us. Service operations in the AWS SDK for PHP are case sensitive, so while listIPSets is a valid operation for the WafRegional client there is no listIpSets operation for this client. I was able to run your code with successful output after capitalizing listIpSets to match its documentation.. Hi @huubvdw, thanks for reaching out to us about this, and for submitting the associated PR #1695. We will review the PR shortly to ensure it is valid to merge.. Thanks for reaching out to us @achertovsky. Asynchronous commands such as deleteMessageBatchAsync return a promise that must be adressed separately from the asynchronous command in order for the corresponding API call to be sent to the service. If you are performing these asynchronous calls one at a time you can synchronously force their promises to complete via $promise->wait(), however if you are performing multiple asynchronous calls that you want to complete simultaneously you can chain their corresponding promises or combine them as shown in the documentation page linked above.. No problem! The PHP SDK documentation has a page for the Promise class that details the methods associated with it, however I find the SDK's developer guide page for Promises to be more comprehensive in explaining what they are and how they should be used within the context of the SDK. The package the SDK uses for promises can be found here, and Guzzle's quickstart documentation page goes into more detail on Promises for more general use cases.. Hi @cw-tomita, thanks for reaching out to us about this and I apologize for the delay in response from our end. I'm able to reproduce this behavior across two EC2 instances with Amazon Linux configured identically with the exception of the installed version of cURL. The AWS SDK for PHP does not appear to be a factor in the difference in behavior between the two instances as I was able to test your code with multiple versions of the SDK across both instances with similar results - in every case, the instance with cURL version 7.53.1-16.85 installed showed a higher CPU utilization and considerably more \"brk\" system calls than the instance with cURL version 7.53.1-16.84 installed.\nI suggest opening a new support case for Premium Support under the EC2-Linux service suggesting contact with the Amazon Linux service team for additional support on this from the OS side as they will have more expertise with Amazon Linux releases and their corresponding package updates.. Hi @html5maker, thanks for reaching out to us. Unfortunately I haven't been able to reproduce the behavior you're describing. I replaced str_random(1024) in your code snippet with random_bytes(1024) to avoid the need for Laravel, and ran that on Docker containers using the images you mention. I'm seeing around 200MB of memory usage for these. I also tested increasing the amount of PutObject commands to 1,000 and 10,000 to check for a significant increase in memory usage as the number of commands being added to the CommandPool increases, and only saw a slight increase that does not appear to correlate to any memory leak.\n```\n$ docker run --rm -v $PWD:/app/:ro gcr.io/google-appengine/php70 php /app/index100.php\nconsumed memory: 203.39M\n$ docker run --rm -v $PWD:/app/:ro gcr.io/google-appengine/php70 php /app/index1000.php\nconsumed memory: 209.38M\n$ docker run --rm -v $PWD:/app/:ro gcr.io/google-appengine/php70 php /app/index10000.php\nconsumed memory: 225.52M\n$ docker run --rm -v $PWD:/app/:ro gcr.io/google-appengine/php71 php /app/index100.php\nconsumed memory: 204.00M\n$ docker run --rm -v $PWD:/app/:ro gcr.io/google-appengine/php71 php /app/index1000.php\nconsumed memory: 210.73M\n$ docker run --rm -v $PWD:/app/:ro gcr.io/google-appengine/php71 php /app/index10000.php\nconsumed memory: 253.21M\n$ docker run --rm -v $PWD:/app/:ro gcr.io/google-appengine/php72 php /app/index100.php \nconsumed memory: 203.57M\n$ docker run --rm -v $PWD:/app/:ro gcr.io/google-appengine/php72 php /app/index1000.php\nconsumed memory: 207.53M\n$ docker run --rm -v $PWD:/app/:ro gcr.io/google-appengine/php72 php /app/index10000.php\nconsumed memory: 240.33M\n```\nIn addition to this I configured Laravel on a custom Docker container to use str_random instead of random_bytes for the sake of testing this to completion and saw that it only used 15MB of memory:\n```\nphp artisan php1699hundred\nconsumed memory: 15.53M\nphp artisan php1699thousand\nconsumed memory: 15.39M\nphp artisan php1699tenthou\nconsumed memory: 14.84M\n```\nIs it possible that there's something else in your environment that could be affecting the reported memory usage?. @html5maker @vbarbarosh We are investigating improvements in memory management of the AWS SDK for PHP and will be tracking progress on this in #1273. Please don't hesitate to contribute to the discussion on that issue with your findings relating to this behavior.. Hi @kostiag, thanks for reaching out to us. The S3 Encryption Client (Aws\\S3\\Crypto\\S3EncryptionClient) was added to the AWS SDK for PHP in the release of version 3.38.0 on Nov 8 2017. This release is 32 minor versions ahead of version 3.6.0, which was released Oct 6 2015. \nIs there any particular reason you cannot use a version of the AWS SDK for PHP that is newer than 3.6.0? That version is quite old and will be missing a lot of functionality that has been added to the SDK since 2015.. Thanks for the clarification @kostiag, and I'm glad to hear that updating the SDK worked as expected. I'll be closing out the issue here since it sounds like things are now in order on your end. If you continue to see issues related to the S3 Encryption Client, please do not hesitate to comment on the issue so we may re-open it to continue working on this.\nSoftware version numbers typically do not follow the same rules as standard decimal notation, instead they are broken down into three individual numbers separated by periods - major, minor, and patch version numbers. As a result, instead of multi-digit numbers signifying sub-versions of their single-digit counterparts, single-digit numbers will typically signify an older version than multi-digit numbers. This applies to major versions (20.0.0 is newer than 3.0.0), minor versions (1.29.0 is newer than 1.5.0), and patch numbers (1.2.15 is newer than 1.2.8).. Thanks for reaching out to us @tmizuma! Can you give us some more context on what you are trying to do with the AWS SDK for PHP that is not currently available? \nIt sounds like you want to use the AWS SDK for PHP as a GraphQL client to invoke functions configured in your GraphQL API in AppSync, however please do correct me if I misinterpreted your request. Unfortunately, the AppSync API does not support invoking functions created in a GraphQL API within AppSync so you won't be able to use the AWS SDK for PHP for this purpose. That being said, there are other PHP libraries you can use as GraphQL clients, some of which are listed on this page.. Hi @melliks, thanks for reaching out to us. Can you specify which example code you are running when you encounter this issue? Providing a link to the page where you found this code would be ideal.\nIn addition to the above, what versions of PHP and the AWS SDK for PHP are you using for this example code?. Hello @ihabzee, thanks for reaching out to us. It sounds like the AWS SDK for PHP is not present in the expected location when you're pushing your code to your server. You'll want to verify that the vendor directory specified in the path for your include_once command is present on the server. If it is, You should see the contents of src/ from our repository in aws/aws-sdk-php/src/ within this vendor/ directory (where autoload.php is located) on your server. \nDepending on your specific use case, it you may find it to be a more consistent experience to install the AWS SDK for PHP on your server either via the prepackaged phar of the SDK or via the ZIP file of the SDK as outlined in this documentation page.. Hi @pawansharma-headspire, thanks for reaching out to us. Unfortunately I'm not able to reproduce this from my end, however given your description of the issue it sounds like this may have been a transient error in loading your private key. It does not seem likely that any changes were made to PHP (or to the AWS SDK for PHP) in your environment, as such this does not sound like an issue that stems from the SDK itself. This sounds like it was most likely caused by something happening with the environment in which you were running this code, my best guess would be that the storage device containing your private key may not have been available to fully load the key. \nWere you able to read your private key to ensure its integrity during the time you were receiving this error? If you being to see this behavior consistently during the given timeframe, that would be the first troubleshooting step that I would suggest performing. It may also be a good idea to write a standalone PHP script that loads your private key via openssl_sign() (or just file_get_contents()) without using the AWS SDK for PHP to see if this behavior persists without involving our SDK.. Hi @DennMonn, thanks for reaching out to us. Can you provide a code sample showing how the SES client and SendEmail parameters are being initialized, as well as how the SendEmail call is being issued? Providing sanitized debug output for calls returning this error would also be helpful - you can enable debug logging by setting the parameter 'debug' => true in the service client's configuration as shown here.. Hi @tomekit, thanks for reaching out to us. Per the documentation for using S3 Stream Wrapper with the AWS SDK for PHP:\n\nAlthough copy will generally work with the Amazon S3 stream wrapper, some errors might not be properly reported due to the internals of the copy function in PHP. We recommend that you use an instance of AwsS3ObjectCopier instead.\n\nI tested the provided code (appending a HeadObject call for the resulting object in S3), unfortunately I was unable to reproduce the described behavior - The resulting output is attached below. \nIf using the S3 Stream Wrapper is required for your use case, using other functions such as file_put_contents or fwrite to write content to S3 via the Stream Wrapper should return any errors that come up more consistently than using copy for the same purpose. Otherwise, using an instance of AwsS3ObjectCopier as suggested in our documentation would be a better approach for this.\n$ php StreamWrapper.php                                                     \nCopying /tmp/largefile.zip to s3://php-1717/largefile.zip at 01/25/2019 10:58:06 pm UTC                            \nbool(true)\nHeadObject result for https://php-1717.s3.us-west-2.amazonaws.com/largefile.zip:\nContentLength:\n4000000008\nLastModified:\n2019-01-25T22:58:50+00:00. Hi @alaaNabawy, thanks for reaching out to us. Is the object for which you are trying to generate a pre-signed URL encrypted with AWS-KMS? If so,  this looks to be a duplicate of #1568; unfortunately CloudFront has not yet implemented support to pre-sign URLs for KMS-encrypted S3 objects. Other CloudFront users have commented on this post in the CloudFront forums regarding this behavior, I'd suggest adding a comment on there expressing interest in this feature being implemented on the service.\nIn the meantime, changing the object's encryption to None or AES-256 in S3 should allow your pre-signed URLs to work as expected.. Hi @sordev, thanks for reaching out to us about this. This behavior looks to be caused by a mismatch between the timestamp format provided in DLM's API model and the format the SDK is expecting in the response for this call. We are actively working with the service team to get this resolved.. Hi @alex-at-cascade, thanks for reaching out to us. The S3 Transfer Manager in the AWS SDK for PHP is used to upload entire directories to an Amazon S3 bucket and download entire buckets/directories to a local directory. Unfortunately your suggestion would be a breaking change for the SDK so we will not be implementing it for the current major version.\nIf the objects you are looking to download exist within the same S3 prefix in your Bucket you can instead issue a ListObjects call with the string matching the common starting portion of the file names as the 'Prefix' parameter for the call, and iterate through the 'Contents' portion of its response to download the desired objects from S3.. Hi @sharanyaa, thanks for reaching out to us about this. Memory usage of the AWS SDK for PHP has increased slightly between versions 3.21.0 and 3.87.7 - publishing the same message to the same SNS topic using these two versions I can see that 3.87.7 uses approximately 950KB more RAM than 3.21.0.\nroot@efd7432844c8:/var/www/html/GH/1723# php snsPublish3.21.php\nUsing AWS SDK for PHP version 3.21.0\nAllocated RAM before initializing SNS client: 670856 bytes.\nAllocated RAM before initializing message: 2501960 bytes.\nAllocated RAM before publishing to topic: 2504304 bytes.\nAllocated RAM after publishing message: 3058408 bytes.\nroot@efd7432844c8:/var/www/html/GH/1723# php snsPublishLatest.php \nUsing AWS SDK for PHP version 3.87.7\nAllocated RAM before initializing SNS client: 645384 bytes.\nAllocated RAM before initializing message: 3429016 bytes.\nAllocated RAM before publishing to topic: 3431360 bytes.\nAllocated RAM after publishing message: 4048592 bytes.\nIt sounds like your application is running just below the 256MB memory size you've allowed, and updating the SDK to the latest version is pushing the application's memory usage over this limit. If there's no way to reduce memory usage in other areas of your application I'd suggest raising the memory limit by a few MB to accommodate the additional memory usage of the newer version of the SDK.. @sharanyaa Thanks for the update on this, and I do apologize for the long delay in response from our end. We are investigating improvements in memory management of the AWS SDK for PHP and will be tracking progress on this in #1273. Please don't hesitate to contribute to the discussion on that issue with your findings relating to this behavior.. Hello @amfahrj, thanks for reaching out to us about this. We generally don't recommend using copy when using the S3 StreamWrapper with the AWS SDK for PHP since errors might not be properly reported due to the internals of the copy function in PHP. \nThat being said, I'm able to reproduce the described behavior using other functions such as file_put_contents and fwrite to copy an empty file (or empty string) to S3 using the StreamWrapper. I've marked this as a feature request and will bring it up during our next sprint.. Hi @martinssipenko, thanks for reaching out to us. Unfortunately I'm not able to reproduce the described behavior on my end using PHP 7.2.9 and version 3.87.13 of the AWS SDK for PHP. I'm using a Cognito Identity pool with a Cognito User pool as the Identity pool's authentication provider and was able to issue the assumeRoleWithWebIdentity call successfully using a JWT token retrieved from a getOpenIdToken call.\nWhat is the origin of the JWT token you're using on this call? It's possible that this behavior is specific to the authentication provider you're using. In addition to this, where are you seeing the X-Amz-Security-Token header set for this call? Looking at the assumeRoleWithWebIdentity request as it is sent in the debug output on my end does not appear to include the X-Amz-Security-Token header you mention:\n```\n\nPOST / HTTP/1.1\nHost: sts.amazonaws.com\nContent-Type: application/x-www-form-urlencoded\naws-sdk-invocation-id: 9b1a1cf859a7733aeaf0bd6f5cdbcfae\naws-sdk-retry: 0/0\nUser-Agent: aws-sdk-php/3.87.13 GuzzleHttp/6.3.3 curl/7.54.0 PHP/7.2.9\nContent-Length: 1066\n\n\nupload completely sent off: 1066 out of 1066 bytes\n< HTTP/1.1 200 OK\n``. Hi @kevinulrich, thanks for reaching out to us. Unfortunately I haven't been able to reproduce the behavior you're describing -is_dircalls to S3 paths that are either nonexistent or object keys returnfalse` as expected in my tests.\n\nCan you provide a code sample that exhibits this behavior?. Hi @ThomasBuisson, thanks for reaching out to us. Which versions of PHP and the AWS SDK for PHP are you using when you encounter this behavior? Seeing a code sample would also help us get a better idea of how you're interacting with Cognito with our SDK.\nUnfortunately I'm not able to reproduce this using PHP 7.2.9 and version 3.87.13 of the AWS SDK for PHP - calling listUsers with preferred_username as the sole string in AttributesToGet returns the expected output as shown below.\n```php\n$client = new Aws\\CognitoIdentityProvider\\CognitoIdentityProviderClient([\n    'region' => 'us-west-2',\n    'version' => 'latest',\n]);\n$users = $client->listUsers([\n    'UserPoolId' => $pool,\n    'AttributesToGet' => [\n        'preferred_username'\n    ]\n]);\necho \"{$users}\\n\";\n```\n```\n$ php listUsers.php \nModel Data\n\nData can be retrieved from the model object using the get() method of the\nmodel (e.g., $result->get($key)) or \"accessing the result like an\nassociative array (e.g. $result['key']). You can also execute JMESPath\nexpressions on the result data using the search() method.\n{\n    \"Users\": [\n        {\n            \"Username\": \"diehlaws\",\n            \"Attributes\": [\n                {\n                    \"Name\": \"preferred_username\",\n                    \"Value\": \"alex\"\n                }\n            ],\n            \"UserCreateDate\": \"2019-02-19T18:29:22+00:00\",\n            \"UserLastModifiedDate\": \"2019-02-19T18:29:42+00:00\",\n            \"Enabled\": true,\n            \"UserStatus\": \"FORCE_CHANGE_PASSWORD\"\n        }\n    ],\n```. Thanks for reaching out to us about this @poisa. I've added a backlog item to have this behavior corrected and will bring it up during our next sprint.. Hi @papacarp, thanks for reaching out to us about this. Unfortunately we won't be able to accept a PR for this change since the API model in the file you mention is provided by the API Gateway team; any update that gets pushed to this file from their end would overwrite this change.\nI've reached out to the service team about having this change made to the API model from their end, once I have additional information I'll update the issue accordingly.. Hello @gman-wa, thanks for reaching out to us about this. Implementing an HTTP configuration option to remove the Expect HTTP request header from requests such as S3's PutObject seems like a reasonable feature request. I've marked the issue as such and will bring this up during our next sprint.. Hi @GordonLesti, thanks for reaching out to us. The behavior you're describing does sound like the SDK isn't encoding the slash in the Bucket name correctly. Can you enable debug logging and provide the corresponding output for our review? This can be done by setting 'debug' => true within the service client options.\nThe API model in the api-2.json file you mention is provided by the S3 team so unfortunately any update that gets pushed to this file from their end would undo changes made to it via a pull request.. Thanks for the additional information @GordonLesti. Looking further into this, S3 has never allowed / as a valid character in an S3 Bucket. At this time the naming conventions for an S3 Bucket in any region require that the name only contain numbers, lower-case letters, hyphens, and periods as described here. As that page mentions, buckets created in the us-east-1 region before March 1 2018 were allowed to use uppercase letters and underscores in addition to the DNS-compliant requirements for other regions, however forward or back slashes have not been allowed characters for S3 Bucket names in any region at any point in time.\nI suspect the case here is that the portion of the bucket's name after the forward slash is actually the top-level prefix corresponding to the path to which you are attempting to upload your object. Are you able to see the bucket with the forward slash in its name from the S3 web console?. Thanks for reaching out to us @GriffinMeyer. 'S3OriginConfig' is not documented as a required field because it should not be present if the origin being configured for the CreateDistribution call is a custom (non-S3) origin. That being said, the error received when omitting this field during the creation of an S3-backed distribution could certainly be improved. I'll bring this up with the rest of the team next week to determine the best approach to resolve this.. Thanks for reaching out to us @siarheipashkevich. Unfortunately the AWS SDK for PHP does not have a service client available for Amazon Textract at this time - only the AWS CLI, Boto3, and the AWS SDK for Java can currently interact with this service; once the service is fully released it will be usable with more of our SDKs including the AWS SDK for PHP.. @siarheipashkevich today's release includes a service client for the Amazon Textract Preview program:\n\nAws\\Textract - This release is intended ONLY for customers that are officially part of the Amazon Textract Preview program. If you are not officially part of the Amazon Textract program THIS WILL NOT WORK. Our two main regions for Amazon Textract Preview are N. Virginia and Dublin. Also some members have been added to Oregon and Ohio. If you are outside of any of these AWS regions, Amazon Textract Preview definitely will not work. If you would like to be part of the Amazon Textract program, you can officially request sign up here - https://pages.awscloud.com/textract-preview.html. To set expectations appropriately, we are aiming to admit new preview participants once a week until General Availability.\n\nDocumentation for the service client is also now available on this page. Please let us know if you have additional questions about this!. Thanks for reaching out to us @SerikK. Is this an error that you receive consistently when trying to use the S3 Transfer Manager to upload objects to S3? Can you enable debug logging on the SDK and provide the resultant output here?. Hi @GodfallentheCelestial, thanks for reaching out to us. This sounds like a problem with the certificate cURL is using on the host on which you are running XAMPP. As suggested by the error message, checking the presence of the certificate at C:\\xampp\\apache\\bin\\curl-ca-bundle.crt and the permissions for this file and the folder in which it resides would be a good place to start troubleshooting this behavior.. Hi @atrauzzi, thanks for reaching out to us. What versions of PHP and the AWS SDK for PHP are you using when encountering this error? Can you provide a code sample for this so we can better understand exactly how you are attempting to interact with S3 using the AWS SDK for PHP?. Thanks for reaching out to us @ellllllen. Unfortunately I haven't been able to reproduce this behavior with php7.0-curl. Can you provide a code sample so we can better understand how you're using the SDK to call sendMessageAsync?. Thanks for the additional information @ellllllen, this is indeed related to https://github.com/guzzle/guzzle/issues/1899. As mentioned in that issue, Guzzle starts to use cURL as the handler when cURL extension for PHP is installed, and at this point your async calls can be executed in parallel. Your sendMessageAsync call will return a promise that needs to be handled after the API call is issued in your sendMessage function.\nThis could be done by resolving the promise within your sendMessage function, however this would have the same effect as replacing the sendMessageAsync call with the sendMessage call as you mentioned for your fix in that the API call would not be truly asynchronous.\n```php\n    public function sendMessage($message, $url, $groupId, $dedupeId)\n    {\n        $promise = $this->client->sendMessageAsync([\n            'MessageBody' => $message,\n            'QueueUrl' => $url,\n            'MessageGroupId' => $groupId,\n            'MessageDeduplicationId' => $dedupeId,\n        ]);\n    $promise->wait();\n}\n\n}\n```\nAlternately, You could have your sendMessage function return the promise returned by the sendMessageAsync call to resolve it later in your code, either after something else happens once your sendMessage function is called or batching multiple promises to resolve at the same time as shown below.\n```php\n    public function sendMessage($message, $url, $groupId, $dedupeId)\n    {\n        $promise = $this->client->sendMessageAsync([\n            'MessageBody' => $message,\n            'QueueUrl' => $url,\n            'MessageGroupId' => $groupId,\n            'MessageDeduplicationId' => $dedupeId,\n        ])->then(function($response) {\n           echo \"Message sent with ID {$response['MessageId']}\\n\";\n        });\n    return $promise;\n}\n\n}\n```\n```php\n$sdk = new Sdk();\n$config = array(\n    'region' => 'us-west-2',\n    'version' => 'latest',\n);\n$service = new SqsService($sdk, $config);\n$promises = [];\nfor ($i = 1; $i < 10; $i++) {\n    $message = \"message\".$i;\n    $dedupeId = \"xyz\".$i;\n    $promise = $service->sendMessage($message, $url, $groupId, $dedupeId);\n$promises[$i] = $promise;\n\n}\nforeach ($promises as $promise) {\n    $promise->wait();\n}\n``. Hi @sahilsharma011, thanks for reaching out to us. At this time the SDK does not provide a way to list queries executed in a given request to DynamoDB. The AWS SDKs serve primarily as a wrapper for API calls to AWS services with minimal customizations, that being said if you have a particular implementation of this feature in mind for which you'd like to submit a PR our contribution guidelines are available [here](https://github.com/aws/aws-sdk-php/blob/master/CONTRIBUTING.md). Please don't hesitate to follow up with any questions you may have for us.. Hi @kamran-jabbar, thanks for reaching out to us. Unfortunately you cannot create a pre-signed S3 URL without an expiration date - [Signature Version 4](http://docs.aws.amazon.com/general/latest/gr/sigv4_signing.html) allows a maximum expiry time of 7 days.. If the files are being uploaded to a private bucket to which the IAM user/role corresponding to your API keys has permission to access (either via the IAM policies attached to the user/role or the bucket policy attached to the S3 Bucket) you should be able to issue aGetObject` call to download objects that have been uploaded to the bucket.\nIf you need to access the object via its S3 URL instead of issuing an API call with the SDK, then you'll need to generate a pre-signed URL to access it - in this case the best approach would be to have your application generate pre-signed URLs with a short expiration time (e.g. 30-120 seconds) each time you need to retrieve objects from this bucket. . In order for a given IAM user to access an object in an S3 Bucket they must be allowed the action s3:GetObject on the object in question \n(\"Resource\": \"arn:aws:s3:::bucketname/path/to/object.txt), the path in which the object exists (\"Resource\": \"arn:aws:s3:::bucketname/path/to/*), or a larger portion of the bucket that includes the path to the object (\"Resource\": \"arn:aws:s3:::bucketname/*). This can be granted by an IAM policy attached to the user (or to a group the user is a part of), or by a Bucket Policy attached to the S3 Bucket.\nThat being said, the response you've provided looks like the GetObject call worked - I see that the HTTP status code is 200 (if the user didn't have appropriate permissions that would be a 403), and the ContentLength looks to be substantially larger than the XML response that S3 would return for an HTTP 403 response.\nIf you're looking to use getObject to save the file you're trying to download to the host that is running the code issuing this API call, you'll want to specify the path to which you want to save this file locally as the value for SaveAs in the parameters for the call.. Thanks for reaching out to us @Xylane. I've re-opened the issue you mentioned to track this and will be closing this issue as a duplicate. Please don't hesitate to comment with any additional findings on this matter.. Hi @liubo1985, thanks for reaching out to us. The issue you mention refers to prepending the bucket name to the virtual-style endpoint, resulting in the endpoint containing the bucket name twice when the endpoint is specified in the client config parameters. S3Client for Version 3 of the AWS SDK for PHP defaults to using virtual-style URLs when building the endpoint for a given request, so this behavior is expected for the current major version of the SDK. \nThat being said, if your use case requires using path-style URLs instead you can specify this behavior by setting use_path_style_endpoint to true in the client configuration parameters as shown below.\n```\n$credentials = new Credentials($key, $secret);\n$client = new S3Client([\n    'credentials' => $credentials,\n    'region' => 'us-west-2',\n    'version' => '2006-03-01',\n    'scheme' => 'http',\n    'use_path_style_endpoint' => true,\n]);\n``. Hi @neoacevedo, thanks for reaching out to us. This behavior looks to be caused by the manner in which TYPO3'sPharStreamWrapperparses the content of ouraws.pharpackage. As mentioned in https://github.com/TYPO3/phar-stream-wrapper/issues/21 replacingrequire('/path/to/aws.phar');withrequire('phar:///path/to/aws.phar/aws-autoloader.php');` allows the AWS SDK for PHP to be loaded as expected from my end, as such I would suggest using this workaround for the time being. Please don't hesitate to reach back out to us if this workaround does not work as expected on your end.. ",
    "lifedup": "I'm using the Cognito signup call, if all of the input the user enters is correct then it works as exspected. The error won't be in the catch (it's in the try) as it's thrown by the actual signup function in the SDK before it sends to the Cognito API.   I think the SDK shouldn't try and validate, it should let the API endpoint do the validation or allow a way to catch the errors.  Also, it would be nice if the API endpoint sent back the Attribute that was throwing the validation error like this SDK does (it returns [password] and the API just says user attribute error and doesn't say which attribute(s) has/have the error).\n$app->cognito is just a class i made with the required variables for the code below and it also returns the AWS CognitoIdentityProviderClient.  I also authenticate via IAM roles.  Here is a portion of the class that the $app->congnito->client is returning. \nnew CognitoIdentityProviderClient(['region' => $this->region, 'version' => $this->version]);\n```\n    private function secretHash($username) {\n        $clientId = $app->cognito->clientId;\n        $clientSecret = $app->cognito->clientSecret;\n        $s = hash_hmac('sha256', $username . $clientId, $clientSecret, true);\n        return base64_encode($s);\n    }\nprivate function generateUsername() {\n    $username = str_replace(['@', '.'], '', $this->email);\n    if (strlen($username) > 128) {\n        $username = substr($username, 0, 122) . mt_rand(10000, 99999);\n    }\n    return $username;\n}\n\npublic function signUp() {\n   $username = $this->generateUsername();\n    try {\n        $app->cognito->client->signUp([\n            'ClientId' => $app->cognito->clientId,\n            'Password' => $this->password,\n            'SecretHash' => $this->secretHash($username),\n            'UserAttributes' => [\n                [\n                    'Name' => 'given_name',\n                    'Value' => $this->given_name\n                ],\n                [\n                    'Name' => 'family_name',\n                    'Value' => $this->family_name\n                ],\n                [\n                    'Name' => 'email',\n                    'Value' => $this->email\n                ]\n            ],\n            'Username' =>  $username\n        ]);\n    } catch (CognitoIdentityProviderException $e) {\n        return [$e->getAwsErrorMessage(), $e->getAwsErrorCode(), $e->getAwsErrorType()];\n    }\n}\n\n```. How would using the validate option help this issue in the signup process? This process doesn't use the class you referenced. It uses CognitoIdentityProviderClient.  Are you saying to disable the validation to stop the multiple validation.. Great! Thanks for your help.  I'll be honest we dumped the PHP SDK, Conginto, and Lambda due to these issues as well as a couple others and we made our own user management but I will see if this works now.. ",
    "danielealbano": "Yes, it is the limit, sorry for opening a not-really-useful issue. I even tried to google it, but nothing popped out.\nAnyway, it would be nice if all the limits would stay \"togheter\", at least you don't have to jump around in the configuration.. ",
    "felipefcm": "I could reproduce the issue by logging a big enough data. Here is the Node.js code of the lambda function:\n```javascript\n\"use strict\";\nexports.handler = (event, context, callback) => {\nvar data = '';\n\nfor(let i = 0; i < 422; ++i)\n    data += 'a';\n\nconsole.log(data);\n\ncallback(null);\n\n};\n```\nIf I use 421 characters instead, LogResult is non-empty. In CloudWatch the data is always being logged, as expected. When invoking the function via aws-cli I always get the logs as well.. php: 5.6.30\nguzzlehttp/guzzle: 6.2.3\naws-sdk-php: 3.27.0\nCan't provide much more information at the moment, but when the issue happens I simply comment out some console.log calls and it works fine again.. ",
    "MarkVaughn": "This behavior changed, previously it was downloading the content. Now it's downloading the folder, breaking things.  How do you achieve the previous behavior?. I think this might be where the breaking changes happened. https://github.com/aws/aws-sdk-php/commit/3341425eada1461d23dc6a17249f72a03becca37\nBy reverting my sdk version back I'm able to get the desired behavior back. However that's not a long term solution. The changes you made in this file seem to be for the sake of fixing a \"possible security issue\" but ends up changing behavior of downloading the folder instead of the folder content.\n. ",
    "Tietew": "@mtdowling Thank you for explanation.\nTo unload SimpleXML extension:\n RedHat/CentOS: comment out extension=simplexml.so in /etc/php.d/simplexml.ini\n Debian/Ubuntu: run phpdismod simplexml\n\nWe cannot catch Throwable because it doesn't exist pre-PHP 7. I\n\nOh, sorry I forgot PHP 5 doesn't have Throwable.\nHow about removing RuntimeException type declaration?\nGuzzle doesn't assume argument types of rejection callable.. @imshashank Sorry for late reply.\nI had installed php from apt. Ubuntu's php package does not contain many extensions by default.\nDetailed reproduce step:\n1. Launch a new EC2 instance from AMI \"Ubuntu Server 16.04 LTS (HVM), SSD Volume Type\".\n2. Run sudo apt install php-cli composer unzip. (PHP 7.0.18-0ubuntu0.16.04.1 will be installed)\n3. Run composer require aws/aws-sdk-php.\n4. Run the script first I wrote.\n5. The error occurs.\nEvidence of my suggestion:\n1. Run composer require ext-simplexml.\n2. Get an error The requested PHP extension ext-simplexml * is missing from your system.\n3. Run sudo apt install php-xml.\n4. Run composer update.\n5. Run the script.\n6. No errors occur.\nNote: Don't forget to attach an EC2 role which allows to call sts:AssumeRole.\n. ",
    "waseemdarwin": "Facing the same issue with many workers processing multiple messages. @esbenp did you got the Root Cause for this?. ",
    "ankitsam": "@imshashank  i need to pass the parameter to docker image as the docker  image functionality is based on parameter, so i cant use the entrypoint/command in docker image as well as cant use fixed parameter values in  entrypoint/command in task definition, \nI logged in via ssh and ran the docker run command above & it worked, but it didnt worked via both task or ecs api, so its not necessarily related to just sdk.\n. @imshashank I read more about it, turns out the parameteres are passed properly and script is getting executed but giving error after some time. The .sh file calls via xvfb-run script.py a python script which uses selenium/firefox and is giving error on line driver = webdriver.Firefox() as \nselenium.common.exceptions.WebDriverException: Message: Failed to decode response from marionette\nBut its working when i run via docker-run, i m not sure if its related to No TTY issue? or any other permission issue?. ",
    "nswarnkar": "Any resolution met so far?\n. ",
    "weshooper": "@kstich makes sense thanks, and thanks @gabriel403 for reporting #1312!. ",
    "vanzay": "Here is some logs from the server:\n\n[Tue May 23 10:09:28.805533 2017] [:error] [pid 23435] [client 172.31.28.194:54830] PHP Warning:  Error writing session PHPSESSID_2pae278kvbh86v61a9upueone4: Error executing \"UpdateItem\" on \"https://dynamodb.eu-central-1.amazonaws.com\"; AWS HTTP error: Client error: POST https://dynamodb.eu-central-1.amazonaws.com resulted in a 400 Bad Request response:\\n{\"__type\":\"com.amazon.coral.service#AccessDeniedException\",\"Message\":\"User: arn:aws:sts::314382625344:assumed-role/aws-e (truncated...)\\n AccessDeniedException (client): User: arn:aws:sts::314382625344:assumed-role/aws-elasticbeanstalk-ec2-role/i-0d1204078bf4e29eb is not authorized to perform: dynamodb:UpdateItem on resource: arn:aws:dynamodb:eu-central-1:314382625344:table/sessions - {\"__type\":\"com.amazon.coral.service#AccessDeniedException\",\"Message\":\"User: arn:aws:sts::314382625344:assumed-role/aws-elasticbeanstalk-ec2-role/i-0d1204078bf4e29eb is not authorized to perform: dynamodb:UpdateItem on resource: arn:aws:dynamodb:eu-central-1:314382625344:table/sessions\"} in /var/app/current/vendor/aws/aws-sdk-php/src/DynamoDb/StandardSessionConnection.php on line 145\n\nIt seems that app doesn't have permissions to write data to DynamoDB from Elastic Beanstalk environment.. I've attached AmazonDynamoDBFullAccess policy to aws-elasticbeanstalk-ec2-role and it works now.\nThanks.. ",
    "cdtweb": "@imshashank I can verify that the result of base64_encode($raw_message) is the base64 encoded message. I also tested that the encoded message, when decoded, is the exact same as the raw message (as it should be).\nI am receiving a warning but I haven't been able to figure out how it is related:\nWarning: A non-numeric value encountered in ../vendor/aws/aws-sdk-php/src/Api/Serializer/QueryParamBuilder.php on line 108\nThank you for posting that link. However, I have already gone over it several times and still haven't been able to get it to work.\n. @imshashank I believe that my message body is formatted correct. When I use AWS SDK for PHP v2 I can send the email and it comes through in my mailbox exactly as intended. Using the exact same message body, with AWS SDK for PHP v3, the email is getting delivered, but everything is missing except for the \"From:\" address. It is very strange as there is no difference between the calls other than the SDK version. \nAs far as using sendEmail I was under the impression that I could not send attachments. Do you have an example on how I could send an attachment with this method?. @imshashank It does work. Thank you very much for your help!. ",
    "nokrosis": "Just tried in a clean server, Exceptions are thrown correctly, the development server was using credentials file with enough permissions.. ",
    "remicollet": "With this:\n```\n$ php72 vendor/bin/phpunit  -d memory_limit=1G  --testsuite=unit --no-coverage\nPHPUnit 5.7.20 by Sebastian Bergmann and contributors.\n.............................................................   61 / 2121 (  2%)\n.............................................................  122 / 2121 (  5%)\n.............................................................  183 / 2121 (  8%)\n.............................................................  244 / 2121 ( 11%)\n.............................................................  305 / 2121 ( 14%)\n.............................................................  366 / 2121 ( 17%)\n.................................................SSSSSSSSSSSS  427 / 2121 ( 20%)\nSSSSSSSSSSSSSSSSSS...........................................  488 / 2121 ( 23%)\n.............................................................  549 / 2121 ( 25%)\n.............................................................  610 / 2121 ( 28%)\n.............................................................  671 / 2121 ( 31%)\n.............................................................  732 / 2121 ( 34%)\n.............................................................  793 / 2121 ( 37%)\n.............................................................  854 / 2121 ( 40%)\n.............................................................  915 / 2121 ( 43%)\n.............................................................  976 / 2121 ( 46%)\n............................................................. 1037 / 2121 ( 48%)\n.........................S..................SSSSSS........... 1098 / 2121 ( 51%)\n............................................................. 1159 / 2121 ( 54%)\n............................................................. 1220 / 2121 ( 57%)\n............................................................. 1281 / 2121 ( 60%)\n............................................................. 1342 / 2121 ( 63%)\n............................................................. 1403 / 2121 ( 66%)\n............................................................. 1464 / 2121 ( 69%)\n............................................................. 1525 / 2121 ( 71%)\n............................................................. 1586 / 2121 ( 74%)\n............................................................. 1647 / 2121 ( 77%)\n............................................................. 1708 / 2121 ( 80%)\n............................................................. 1769 / 2121 ( 83%)\n............................................................. 1830 / 2121 ( 86%)\n............................................................. 1891 / 2121 ( 89%)\n............................................................. 1952 / 2121 ( 92%)\n............................................................. 2013 / 2121 ( 94%)\n............................................................. 2074 / 2121 ( 97%)\n...............................................               2121 / 2121 (100%)\nTime: 4.49 seconds, Memory: 34.01MB\nOK, but incomplete, skipped, or risky tests!\nTests: 2121, Assertions: 4965, Skipped: 37.\n```. Travis passes with \"nightly\", only failing when COMPOSER_OPTS=\"--prefer-lowest\", as compatibility with PHP 7.2 have been fixed in recent PHPUnit versions. @kstich sorry, no idea how to reproduce the issue you're referring to\nI don't really use this library (only maintaining its package in downstream distribution as used by other projects and running QA with tons of libraries for PHP 72).\nThis PR was only open to enlighten the breakage, and point to a possible fix.\nPlease pull what you think is correct, and add what is missing.\n. @siwinski perhaps you have more idea about this. ",
    "msvrtan": "Hey @kstich \nCan changes like this at least get a minor version bump instead of pushing them into patch? \nJust updated from 3.28.1 to 3.28.6 and had my testing setup broken due to this, as I update multiple libs it took me quite some time to locate the issue (since it was a patch update didn't expect this to break). \n. Hi @kstich ,\nOn our CI we are running mock s3 on http:/localhost:3000/project-name and this change started throwing errors cause http://project-name.localhost:3000/ was not found.\nAs we had more then few packages updated, it was hard to locate from the error what broke it .I skipped sdk package few times as it had a patch release change and it took me few hours of updating one by one library to locate when it breaks :(\nI'm not sure if this change should even trigger the major version update or just a minor one, but I really didnt expect it in patch version (as only bugfixed should be done there by semantic versioning). \ud83d\udc4d . ",
    "Lakshanz": "+1. ",
    "ProdigyView": "@imshashank I am using 3.29.2 as my PHP SDK on PHP 5.7 on local and HHVM 3.19, which is basically PHP 7, on my server.. @imshashank I also want to add, I've been debugging the Guzzle function, there is data being uploaded by monitoring the curl request:\n```\nGuzzleHttp\\Psr7\\Response Object\n(\n    [reasonPhrase:GuzzleHttp\\Psr7\\Response:private] => OK\n    [statusCode:GuzzleHttp\\Psr7\\Response:private] => 200\n    [headers:GuzzleHttp\\Psr7\\Response:private] => Array\n        (\n            [x-amz-id-2] => Array\n                (\n                    [0] => HIxEHM/Y/WTZOeq3kTcJE0KxTQvMv+rXG2K4LPZOVCkKZfkkL3YohiQs6KUqO8/6/okl7hKBOKU=\n                )\n        [x-amz-request-id] => Array\n            (\n                [0] => 7354E6AD57FA83A6\n            )\n\n        [Date] => Array\n            (\n                [0] => Mon, 12 Jun 2017 10:34:30 GMT\n            )\n\n        [ETag] => Array\n            (\n                [0] => \"80d285d7d0aa807cc4e05148e693bb1f\"\n            )\n\n        [Content-Length] => Array\n            (\n                [0] => 0\n            )\n\n        [Server] => Array\n            (\n                [0] => AmazonS3\n            )\n\n    )\n\n[headerNames:GuzzleHttp\\Psr7\\Response:private] => Array\n    (\n        [x-amz-id-2] => x-amz-id-2\n        [x-amz-request-id] => x-amz-request-id\n        [date] => Date\n        [etag] => ETag\n        [content-length] => Content-Length\n        [server] => Server\n    )\n\n[protocol:GuzzleHttp\\Psr7\\Response:private] => 1.1\n[stream:GuzzleHttp\\Psr7\\Response:private] => GuzzleHttp\\Psr7\\Stream Object\n    (\n        [stream:GuzzleHttp\\Psr7\\Stream:private] => Resource id #565\n        [size:GuzzleHttp\\Psr7\\Stream:private] => \n        [seekable:GuzzleHttp\\Psr7\\Stream:private] => 1\n        [readable:GuzzleHttp\\Psr7\\Stream:private] => 1\n        [writable:GuzzleHttp\\Psr7\\Stream:private] => 1\n        [uri:GuzzleHttp\\Psr7\\Stream:private] => php://temp\n        [customMetadata:GuzzleHttp\\Psr7\\Stream:private] => Array\n            (\n            )\n\n    )\n\n)\n```\nIs there some place else you recommend I be debugging?. Update on this, I turned debug on in the http for the client, this is what I get:\n```\n< HTTP/1.1 100 Continue\n We are completely uploaded and fine\n< HTTP/1.1 200 OK\n< x-amz-id-2: xxxxxtlxmBsj1TdyuHNSVjL0mpVOdlYik1krvq46sZc2ZAapOOgG54gAd4IkSYSFL37NflAMA8Mc=\n< x-amz-request-id: EC69C0176F3EE64A\n< Date: Wed, 14 Jun 2017 11:22:32 GMT\n< ETag: \"d07c4a8df3efef57149f08024a2bbs63\"\n< Content-Length: 0\n< Server: AmazonS3\n< \n Connection #4 to host example.s3-us-west-2.amazonaws.com left intact\n Found bundle for host example.s3-us-west-2.amazonaws.com: 0x7ff031e84f00\n Re-using existing connection! (#4) with host example.s3-us-west-2.amazonaws.com\n* Connected to example.s3-us-west-2.amazonaws.com (54.231.176.249) port 443 (#4)\n\nPUT /ef740de5-46f1-4ddd5-8d19-5ddd2142b7e5?partNumber=156&uploadId=cRUWjT5j_JKbibxoGgHnBz7D3GPioqraVbtP9UsOi.Qqo71oAPhe5PMy31b56hHYKZ41QBSsN6BsP59VDrgzwrOxeEUXH9q7m_4nPBPrOD45Q8l5azVJH6CAT6yqjthJ HTTP/1.1\nExpect: 100-Continue\nHost: example.s3-us-west-2.amazonaws.com\nContent-Type: application/octet-stream\naws-sdk-invocation-id: c90115f92b306bccdcdd58395ec08ef6\naws-sdk-retry: 0/0\nX-Amz-Content-Sha256: 21aabf8300eb711bb23fb424129ba6c7818a3252a419c3371cb026ebfa87888f\nX-Amz-Date: 20170614T112234Z\nAuthorization: AWS4-HMAC-SHA256 Credential=FDSDFSSSJJMPLX54OYKQ/20170614/us-west-2/s3/aws4_request, SignedHeaders=aws-sdk-invocation-id;aws-sdk-retry;host;x-amz-content-sha256;x-amz-date, Signature=SDSKCMDKSOCc6d4a4ec062e09cd582b03981ad9704bdc4973516711b4820cbdb9e\nUser-Agent: aws-sdk-php/3.29.2 GuzzleHttp/6.2.1 curl/7.29.0 PHP/5.6.99-hhvm\nContent-Length: 52428800\n\n< HTTP/1.1 100 Continue\n We are completely uploaded and fine\n< HTTP/1.1 200 OK\n< x-amz-id-2: Ps0jlXqp3L5WfCBbXtKpuSmVoGOgXqHj7OiglzlhMOquk3kvhDB0PY9rYV76yLn3wpvuV/PvNL8=\n< x-amz-request-id: A5F4D4809F449AA4\n< Date: Wed, 14 Jun 2017 11:22:35 GMT\n< ETag: \"501458025c9466f28fefe8115eae1629\"\n< Content-Length: 0\n< Server: AmazonS3\n< \n Connection #4 to host example.s3-us-west-2.amazonaws.com left intact\n Found bundle for host example.s3-us-west-2.amazonaws.com: 0x7ff031e84f00\n Re-using existing connection! (#4) with host example.s3-us-west-2.amazonaws.com\n* Connected to example.s3-us-west-2.amazonaws.com (54.231.176.249) port 443 (#4)\n\nPUT /ef740de5-46f1-4dd5-8d19-5d3d2142b7e5?partNumber=157&uploadId=cRUWjT5j_JKbibxoGgHnBz7D3GPioqraVbtP9UsOi.Qqo71oAPhe5PMy31b56hHYKZ41QBSsN6BsP59VDrgzwrOxeEUXH9q7m_4nPBPrOD45Q8l5azVJH6CAT6yqjthJ HTTP/1.1\nExpect: 100-Continue\nHost: example.s3-us-west-2.amazonaws.com\nContent-Type: application/octet-stream\naws-sdk-invocation-id: f10f4a3bbdf1105b47ffa51bea3f4a15\naws-sdk-retry: 0/0\nX-Amz-Content-Sha256: 5cc343d114d55dde976cc5efcf8456d6afd95b57556efe631753f63a369a30ba\nX-Amz-Date: 20170614T112237Z\nAuthorization: AWS4-HMAC-SHA256 Credential=JMSAFDF2JJMPLX54OYKQ/20170614/us-west-2/s3/aws4_request, SignedHeaders=aws-sdk-invocation-id;aws-sdk-retry;host;x-amz-content-sha256;x-amz-date, Signature=23fc15c787433b558d4b0af21c06fff2c4c80c72abf39428a9cf74febc38e34a\nUser-Agent: aws-sdk-php/3.29.2 GuzzleHttp/6.2.1 curl/7.29.0 PHP/5.6.99-hhvm\nContent-Length: 34855361\n\n< HTTP/1.1 100 Continue\n We are completely uploaded and fine\n< HTTP/1.1 200 OK\n< x-amz-id-2: iP3vtWEtgUqE5o4bpl5fHufGIVVkBXAYP39TiRJhz8mEU6LUiacjwk7nlKBmGNVAAQe6dVunid0=\n< x-amz-request-id: 66BDC13B25937BAB\n< Date: Wed, 14 Jun 2017 11:22:38 GMT\n< ETag: \"80d285d7d0aa807cc4e05148e693bb1f\"\n< Content-Length: 0\n< Server: AmazonS3\n< \n Connection #4 to host example.s3-us-west-2.amazonaws.com left intact\n We are completely uploaded and fine\n< HTTP/1.1 200 OK\n< x-amz-id-2: uh0sUQ7GnYqItlDbI4PjocnCOT4wOk4LlUuiUAKqQVoIwEFs3FnV44VJHJVvUFHNjOcMHIrEU6w=\n< x-amz-request-id: 4184BA5C5AB25C20\n< Date: Wed, 14 Jun 2017 11:22:18 GMT\n< ETag: \"5153794cf77190c221daf5c53a397dfa\"\n< Content-Length: 0\n< Server: AmazonS3\n< Connection: close\n< \n Closing connection 3\n``. @imshashank I have it on and it returns 200's, which are successes. Also should the content length be 0?. @imshashank I removed theaclandbefore_initiate `as well to replicate your test, and same result. I see the transfer of 8GB to the bucket, but the I do not see the actual file in the bucket. And yes it returns NULL each time.. @kstich Where would I find that class and function? Because I do not see an Aws/Result/CompleteMultipartUpload .. Ok, I got it, debug can be set for the entire client! @kstich @imshashank Here is the tail end of my debug:\n```\n-> Entering step init, name 'idempotency_auto_fill'\n\ncommand was set to array(3) {\n    [\"instance\"]=>\n    string(32) \"0000000000000000dc1bc010a293a940\"\n    [\"name\"]=>\n    string(10) \"UploadPart\"\n    [\"params\"]=>\n    array(6) {\n      [\"PartNumber\"]=>\n      int(157)\n      [\"Body\"]=>\n      object(GuzzleHttp\\Psr7\\LimitStream)#38852 (3) {\n        [\"offset\":\"GuzzleHttp\\Psr7\\LimitStream\":private]=>\n        int(8178892800)\n        [\"limit\":\"GuzzleHttp\\Psr7\\LimitStream\":private]=>\n        int(52428800)\n        [\"stream\"]=>\n        object(GuzzleHttp\\Psr7\\LazyOpenStream)#38851 (3) {\n          [\"filename\":\"GuzzleHttp\\Psr7\\LazyOpenStream\":private]=>\n          string(79) \"/data/sites/www.bingewave.com/partners/tmp/850831ec-8b0b-8597-d63d-7ab241ebe44c\"\n          [\"mode\":\"GuzzleHttp\\Psr7\\LazyOpenStream\":private]=>\n          string(1) \"r\"\n          [\"stream\"]=>\n          object(GuzzleHttp\\Psr7\\Stream)#38854 (7) {\n            [\"stream\":\"GuzzleHttp\\Psr7\\Stream\":private]=>\n            resource(675) of type (stream)\n            [\"size\":\"GuzzleHttp\\Psr7\\Stream\":private]=>\n            int(8213748161)\n            [\"seekable\":\"GuzzleHttp\\Psr7\\Stream\":private]=>\n            bool(true)\n            [\"readable\":\"GuzzleHttp\\Psr7\\Stream\":private]=>\n            bool(true)\n            [\"writable\":\"GuzzleHttp\\Psr7\\Stream\":private]=>\n            bool(false)\n            [\"uri\":\"GuzzleHttp\\Psr7\\Stream\":private]=>\n            string(79) \"/data/sites/www.bingewave.com/partners/tmp/850831ec-8b0b-8597-d63d-7ab241ebe44c\"\n            [\"customMetadata\":\"GuzzleHttp\\Psr7\\Stream\":private]=>\n            array(0) {\n            }\n          }\n        }\n      }\n      [\"UploadId\"]=>\n      string(128) \"ALqvQr0fPQ5zYG2q1FYcxVljDRu9KpOnXjYrCB1yRduofAW4dFWYzaLhnbqagsXKkBRZ3Zgqa2jIX.IKkGynGBXw3mZVX99SWkO78jycPtZjpZAlUJ6Ct9I8yB7y\"\n      [\"Bucket\"]=>\n      string(21) \"ex-videos-producition\"\n      [\"Key\"]=>\n      string(36) \"ef740de5-46f1-4dd5-8d19-5d3d2142b7e5\"\n      [\"@http\"]=>\n      array(2) {\n        [\"timeout\"]=>\n        int(12000)\n        [\"debug\"]=>\n        resource(676) of type (stream)\n      }\n    }\n  }\nrequest was set to array(0) {\n  }\n-> Entering step init, name 's3.ssec'\nno changes\n-> Entering step init, name 's3.source_file'\nno changes\n-> Entering step init, name 's3.save_as'\nno changes\n-> Entering step init, name 's3.location'\nno changes\n-> Entering step init, name 's3.auto_encode'\nno changes\n-> Entering step init, name 's3.head_object'\nno changes\n-> Entering step validate, name 'validation'\nno changes\n-> Entering step build, name 'builder'\nrequest.instance was set to 0000000000000000dc1bc010a28881b0\n  request.method was set to PUT\n  request.headers was set to array(2) {\n    [\"X-Amz-Security-Token\"]=>\n    string(7) \"[TOKEN]\"\n    [\"Host\"]=>\n    array(1) {\n      [0]=>\n      string(26) \"s3-us-west-2.amazonaws.com\"\n    }\n  }\nrequest.body was set to stream(size=34855361)\n  request.scheme was set to https\n  request.path was set to /ex-videos-producition/ef740de5-46f1-4dd5-8d19-5d3d2142b7e5\n  request.query was set to partNumber=157&uploadId=ZALqvQr0fPQ5zYG2q1FYcxVljDRu9YKpOnXjYrCB1yRduofAW4dFWYzaLhnbqagsXKkBRZ3Zgqa2jIX.IKkGynGBXw3mZVX99SWTkO78jyGcPtZjpZAlUJ6Ct9I8yB7y\n-> Entering step build, name ''\nrequest.instance changed from 0000000000000000dc1bc010a28881b0 to 0000000000000000dc1bc010a282d0f0\n  request.headers.User-Agent was set to array(1) {\n    [0]=>\n    string(18) \"aws-sdk-php/3.29.2\"\n  }\n-> Entering step build, name 's3.checksum'\nno changes\n-> Entering step build, name 's3.content_type'\nrequest.instance changed from 0000000000000000dc1bc010a282d0f0 to 0000000000000000dc1bc010a282daf0\n  request.headers.Content-Type was set to array(1) {\n    [0]=>\n    string(24) \"application/octet-stream\"\n  }\n-> Entering step build, name 's3.endpoint_middleware'\nrequest.instance changed from 0000000000000000dc1bc010a282daf0 to 0000000000000000dc1bc010a29230b0\n  request.headers.Host.0 changed from s3-us-west-2.amazonaws.com to ex-videos-producition.s3-us-west-2.amazonaws.com\n  request.path changed from /ex-videos-producition/ef740de5-46f1-4dd5-8d19-5d3d2142b7e5 to /ef740de5-46f1-4dd5-8d19-5d3d2142b7e5\n-> Entering step sign, name 'invocation-id'\nrequest.instance changed from 0000000000000000dc1bc010a29230b0 to 0000000000000000dc1bc010a2923270\n  request.headers.aws-sdk-invocation-id was set to array(1) {\n    [0]=>\n    string(32) \"605f3eecbe9230cd344e86794125f32f\"\n  }\n-> Entering step sign, name 'retry'\nrequest.instance changed from 0000000000000000dc1bc010a2923270 to 0000000000000000dc1bc010a2923290\n  request.headers.aws-sdk-retry was set to array(1) {\n    [0]=>\n    string(3) \"0/0\"\n  }\n-> Entering step sign, name 'signer'\nrequest.instance changed from 0000000000000000dc1bc010a2923290 to 0000000000000000dc1bc010a282d0f0\n  request.headers.X-Amz-Content-Sha256 was set to array(1) {\n    [0]=>\n    string(64) \"5cc343d114d55dde976cc5efcf8456d6afd95b57556efe631753f63a369a30ba\"\n  }\nrequest.headers.X-Amz-Date was set to array(1) {\n    [0]=>\n    string(16) \"20170622T235021Z\"\n  }\nrequest.headers.Authorization was set to array(1) {\n    [0]=>\n    string(247) \"AWS4-HMAC-SHA256 Credential=[KEY]/20170622/us-west-2/s3/aws4_request, SignedHeaders=aws-sdk-invocation-id;aws-sdk-retry;host;x-amz-content-sha256;x-amz-date, Signature=[SIGNATURE]\n  }\n-> Entering step sign, name 's3.put_object_url'\nno changes\n-> Entering step sign, name 's3.permanent_redirect'\nno changes\n-> Entering step sign, name 'mup'\nno changes\n\nFound bundle for host ex-videos-producition.s3-us-west-2.amazonaws.com: 0x7fecd8b05da0\nRe-using existing connection! (#4) with host ex-videos-producition.s3-us-west-2.amazonaws.com\nConnected to ex-videos-producition.s3-us-west-2.amazonaws.com (54.231.176.217) port 443 (#4)\nPUT /ef740de5-46f1-4dd5-8d19-5d3d2142b7e5?partNumber=157&uploadId=ZALqvQr0fPQ5zYG2q1FYcxVljDRu9YKpOnXjYrCB1yRduofAW4dFWYzaLhnbqagsXKkBRZ3Zgqa2jIX.IKkGynGBXw3mZVX99SWTkO78jyGcPtZjpZAlUJ6Ct9I8yB7y HTTP/1.1\nExpect: 100-Continue\nHost: ex-videos-producition.s3-us-west-2.amazonaws.com\nContent-Type: application/octet-stream\naws-sdk-invocation-id: 605f3eecbe9230cd344e86794125f32f\naws-sdk-retry: 0/0\nX-Amz-Content-Sha256: 5cc343d114d55dde976cc5efcf8456d6afd95b57556efe631753f63a369a30ba\nX-Amz-Date: 20170622T235021Z\nAuthorization: AWS4-HMAC-SHA256 Credential=[KEY]/20170622/us-west-2/s3/aws4_request, SignedHeaders=aws-sdk-invocation-id;aws-sdk-retry;host;x-amz-content-sha256;x-amz-date, Signature=[SIGNATURE]\nUser-Agent: aws-sdk-php/3.29.2 GuzzleHttp/6.2.1 curl/7.29.0 PHP/5.6.99-hhvm\nContent-Length: 34855361\n\n\n\n< HTTP/1.1 100 Continue\n We are completely uploaded and fine\n< HTTP/1.1 200 OK\n< x-amz-id-2: vs1C8pg0xf9CoyJPfJDCD8CJeZSatMHCXLm5QmBrrcssM9MPMLujwr47WuFzILZTsoJqPmq3tMQ=\n< x-amz-request-id: 6C9D0DB378819AA7\n< Date: Thu, 22 Jun 2017 23:50:22 GMT\n< ETag: \"80d285d7d0aa807cc4e05148e693bb1f\"\n< Content-Length: 0\n< Server: AmazonS3\n< \n Connection #4 to host ex-videos-producition.s3-us-west-2.amazonaws.com left intact\n<- Leaving step sign, name 'mup'\nresult was set to array(2) {\n    [\"instance\"]=>\n    string(32) \"0000000000000000dc1bc01092e8c5a0\"\n    [\"data\"]=>\n    array(7) {\n      [\"ServerSideEncryption\"]=>\n      string(0) \"\"\n      [\"ETag\"]=>\n      string(34) \"\"80d285d7d0aa807cc4e05148e693bb1f\"\"\n      [\"SSECustomerAlgorithm\"]=>\n      string(0) \"\"\n      [\"SSECustomerKeyMD5\"]=>\n      string(0) \"\"\n      [\"SSEKMSKeyId\"]=>\n      string(0) \"\"\n      [\"RequestCharged\"]=>\n      string(0) \"\"\n      [\"@metadata\"]=>\n      array(4) {\n        [\"statusCode\"]=>\n        int(200)\n        [\"effectiveUri\"]=>\n        string(246) \"https://ex-videos-producition.s3-us-west-2.amazonaws.com/ef740de5-46f1-4dd5-8d19-5d3d2142b7e5?partNumber=157&uploadId=ZALqvQr0fPQ5zYG2q1FYcxVljDRu9YKpOnXjYrCB1yRduofAW4dFWYzaLhnbqagsXKkBRZ3Zgqa2jIX.IKkGynGBXw3mZVX99SWTkO78jyGcPtZjpZAlUJ6Ct9I8yB7y\"\n        [\"headers\"]=>\n        array(6) {\n          [\"x-amz-id-2\"]=>\n          string(76) \"vs1C8pg0xf9CoyJPfJDCD8CJeZSatMHCXLm5QmBrrcssM9MPMLujwr47WuFzILZTsoJqPmq3tMQ=\"\n          [\"x-amz-request-id\"]=>\n          string(16) \"6C9D0DB378819AA7\"\n          [\"date\"]=>\n          string(29) \"Thu, 22 Jun 2017 23:50:22 GMT\"\n          [\"etag\"]=>\n          string(34) \"\"80d285d7d0aa807cc4e05148e693bb1f\"\"\n          [\"content-length\"]=>\n          string(1) \"0\"\n          [\"server\"]=>\n          string(8) \"AmazonS3\"\n        }\n        [\"transferStats\"]=>\n        array(0) {\n        }\n      }\n    }\n  }\nInclusive step time: 3.3253030776978\n<- Leaving step sign, name 's3.permanent_redirect'\nno changes\n  Inclusive step time: 3.3253960609436\n<- Leaving step sign, name 's3.put_object_url'\nno changes\n  Inclusive step time: 3.3254721164703\n<- Leaving step sign, name 'signer'\nno changes\n  Inclusive step time: 3.325679063797\n<- Leaving step sign, name 'retry'\nno changes\n  Inclusive step time: 3.632705450058\n<- Leaving step sign, name 'invocation-id'\nresult.data.@metadata.transferStats.http was set to array(1) {\n    [0]=>\n    array(0) {\n    }\n  }\nInclusive step time: 3.6328973770142\n<- Leaving step build, name 's3.endpoint_middleware'\nno changes\n  Inclusive step time: 3.6330103874207\n<- Leaving step build, name 's3.content_type'\nno changes\n  Inclusive step time: 3.6331486701965\n<- Leaving step build, name 's3.checksum'\nno changes\n  Inclusive step time: 3.6333231925964\n<- Leaving step build, name ''\nno changes\n  Inclusive step time: 3.6334493160248\n<- Leaving step build, name 'builder'\nno changes\n  Inclusive step time: 3.633572101593\n<- Leaving step validate, name 'validation'\nno changes\n  Inclusive step time: 3.6337416172028\n<- Leaving step init, name 's3.head_object'\nno changes\n  Inclusive step time: 3.6338307857513\n<- Leaving step init, name 's3.auto_encode'\nno changes\n  Inclusive step time: 3.6338713169098\n<- Leaving step init, name 's3.location'\nno changes\n  Inclusive step time: 3.6339173316956\n<- Leaving step init, name 's3.save_as'\nno changes\n  Inclusive step time: 3.6339585781097\n<- Leaving step init, name 's3.source_file'\nno changes\n  Inclusive step time: 3.634001493454\n<- Leaving step init, name 's3.ssec'\nno changes\n  Inclusive step time: 3.6340506076813\n<- Leaving step init, name 'idempotency_auto_fill'\nno changes\n  Inclusive step time: 3.6342120170593\n\nFound bundle for host ex-videos-producition.s3-us-west-2.amazonaws.com: 0x7fecd8b05da0\nRe-using existing connection! (#3) with host ex-videos-producition.s3-us-west-2.amazonaws.com\nConnected to ex-videos-producition.s3-us-west-2.amazonaws.com (54.231.176.217) port 443 (#3)\nPUT /ef740de5-46f1-4dd5-8d19-5d3d2142b7e5?partNumber=156&uploadId=ZALqvQr0fPQ5zYG2q1FYcxVljDRu9YKpOnXjYrCB1yRduofAW4dFWYzaLhnbqagsXKkBRZ3Zgqa2jIX.IKkGynGBXw3mZVX99SWTkO78jyGcPtZjpZAlUJ6Ct9I8yB7y HTTP/1.1\nExpect: 100-Continue\nHost: ex-videos-producition.s3-us-west-2.amazonaws.com\nContent-Type: application/octet-stream\naws-sdk-invocation-id: ecdb31382c999aaa0ccc7001352a99b0\naws-sdk-retry: 0/0\nX-Amz-Content-Sha256: 21aabf8300eb711bb23fb424129ba6c7818a3252a419c3371cb026ebfa87888f\nX-Amz-Date: 20170622T235020Z\nAuthorization: AWS4-HMAC-SHA256 Credential=[KEY]/20170622/us-west-2/s3/aws4_request, SignedHeaders=aws-sdk-invocation-id;aws-sdk-retry;host;x-amz-content-sha256;x-amz-date, Signature=[SIGNATURE]\nUser-Agent: aws-sdk-php/3.29.2 GuzzleHttp/6.2.1 curl/7.29.0 PHP/5.6.99-hhvm\nContent-Length: 52428800\n\n\n\n< HTTP/1.1 100 Continue\n We are completely uploaded and fine\n< HTTP/1.1 200 OK\n< x-amz-id-2: JRYKntbiNjyX22OcKMs53JLskqiKIPDiD5/kqF8xMBG6TrusFEO28+t+A1RgLFa3wz+HxvjuUQA=\n< x-amz-request-id: 45CE95F34880A67C\n< Date: Thu, 22 Jun 2017 23:50:21 GMT\n< ETag: \"501458025c9466f28fefe8115eae1629\"\n< Content-Length: 0\n< Server: AmazonS3\n< \n Connection #3 to host ex-videos-producition.s3-us-west-2.amazonaws.com left intact\n<- Leaving step sign, name 'mup'\nresult.instance changed from 0000000000000000dc1bc01092e8c5a0 to 0000000000000000dc1bc01092e8c210\n  result.data.ETag changed from \"80d285d7d0aa807cc4e05148e693bb1f\" to \"501458025c9466f28fefe8115eae1629\"\n  result.data.@metadata.effectiveUri changed from https://ex-videos-producition.s3-us-west-2.amazonaws.com/ef740de5-46f1-4dd5-8d19-5d3d2142b7e5?partNumber=157&uploadId=ZALqvQr0fPQ5zYG2q1FYcxVljDRu9YKpOnXjYrCB1yRduofAW4dFWYzaLhnbqagsXKkBRZ3Zgqa2jIX.IKkGynGBXw3mZVX99SWTkO78jyGcPtZjpZAlUJ6Ct9I8yB7y to https://ex-videos-producition.s3-us-west-2.amazonaws.com/ef740de5-46f1-4dd5-8d19-5d3d2142b7e5?partNumber=156&uploadId=ZALqvQr0fPQ5zYG2q1FYcxVljDRu9YKpOnXjYrCB1yRduofAW4dFWYzaLhnbqagsXKkBRZ3Zgqa2jIX.IKkGynGBXw3mZVX99SWTkO78jyGcPtZjpZAlUJ6Ct9I8yB7y\n  result.data.@metadata.headers.x-amz-id-2 changed from vs1C8pg0xf9CoyJPfJDCD8CJeZSatMHCXLm5QmBrrcssM9MPMLujwr47WuFzILZTsoJqPmq3tMQ= to JRYKntbiNjyX22OcKMs53JLskqiKIPDiD5/kqF8xMBG6TrusFEO28+t+A1RgLFa3wz+HxvjuUQA=\n  result.data.@metadata.headers.x-amz-request-id changed from 6C9D0DB378819AA7 to 45CE95F34880A67C\n  result.data.@metadata.headers.date changed from Thu, 22 Jun 2017 23:50:22 GMT to Thu, 22 Jun 2017 23:50:21 GMT\n  result.data.@metadata.headers.etag changed from \"80d285d7d0aa807cc4e05148e693bb1f\" to \"501458025c9466f28fefe8115eae1629\"\n  Inclusive step time: 4.2118287086487\n<- Leaving step sign, name 's3.permanent_redirect'\nno changes\n  Inclusive step time: 4.211991071701\n<- Leaving step sign, name 's3.put_object_url'\nno changes\n  Inclusive step time: 4.2121121883392\n<- Leaving step sign, name 'signer'\nno changes\n  Inclusive step time: 4.2123734951019\n<- Leaving step sign, name 'retry'\nno changes\n  Inclusive step time: 4.6742928028107\n<- Leaving step sign, name 'invocation-id'\nresult.data.@metadata.transferStats.http was set to array(1) {\n    [0]=>\n    array(0) {\n    }\n  }\nInclusive step time: 4.6745331287384\n<- Leaving step build, name 's3.endpoint_middleware'\nno changes\n  Inclusive step time: 4.6746783256531\n<- Leaving step build, name 's3.content_type'\nno changes\n  Inclusive step time: 4.6748399734497\n<- Leaving step build, name 's3.checksum'\nno changes\n  Inclusive step time: 4.6749849319458\n<- Leaving step build, name ''\nno changes\n  Inclusive step time: 4.6751408576965\n<- Leaving step build, name 'builder'\nno changes\n  Inclusive step time: 4.6752912998199\n<- Leaving step validate, name 'validation'\nno changes\n  Inclusive step time: 4.6754853725433\n<- Leaving step init, name 's3.head_object'\nno changes\n  Inclusive step time: 4.6756174564362\n<- Leaving step init, name 's3.auto_encode'\nno changes\n  Inclusive step time: 4.6757056713104\n<- Leaving step init, name 's3.location'\nno changes\n  Inclusive step time: 4.6758012771606\n<- Leaving step init, name 's3.save_as'\nno changes\n  Inclusive step time: 4.6758801937103\n<- Leaving step init, name 's3.source_file'\nno changes\n  Inclusive step time: 4.6759638786316\n<- Leaving step init, name 's3.ssec'\nno changes\n  Inclusive step time: 4.6760597229004\n<- Leaving step init, name 'idempotency_auto_fill'\nno changes\n  Inclusive step time: 4.6764361858368\nComplete UploadNULL\n```\nThis is the end.\n. Found it! @kstich  But there is weird, but it say authetnication error BUT this works for the regular s3 -> putObject.\n```\n- Leaving step sign, name 's3.permanent_redirect'\n\nerror was set to array(13) {\n    [\"instance\"]=>\n    string(32) \"0000000000000000dc1bc01092dbd710\"\n    [\"class\"]=>\n    string(28) \"Aws\\S3\\Exception\\S3Exception\"\n    [\"message\"]=>\n    string(1123) \"Error executing \"CompleteMultipartUpload\" on \"https://ex-videos-producition.s3-us-west-2.amazonaws.com/ef740de5-46f1-4dd5-8d19-5d3d2142b7e5?uploadId=ZALqvQr0fPQ5zYG2q1FYcxVljDRu9YKpOnXjYrCB1yRduofAW4dFWYzaLhnbqagsXKkBRZ3Zgqa2jIX.IKkGynGBXw3mZVX99SWTkO78jyGcPtZjpZAlUJ6Ct9I8yB7y\"; AWS HTTP error: Client error: POST https://ex-videos-producition.s3-us-west-2.amazonaws.com/ef740de5-46f1-4dd5-8d19-5d3d2142b7e5?uploadId=ZALqvQr0fPQ5zYG2q1FYcxVljDRu9YKpOnXjYrCB1yRduofAW4dFWYzaLhnbqagsXKkBRZ3Zgqa2jIX.IKkGynGBXw3mZVX99SWTkO78jyGcPtZjpZAlUJ6Ct9I8yB7y resulted in a 400 Bad Request response:\n  MalformedXMLThe XML you provided was not well-formed or did not validate against our publis (truncated...)\n   MalformedXML (client): The XML you provided was not well-formed or did not validate against our published schema - MalformedXMLThe XML you provided was not well-formed or did not validate against our published schemaF8B36AE804390FECoP/CL485WE/rrXBP3p82a9db+Ssk4rxmx6CjXcp/GKMPeGws/7FIYunBlfW0+7+HAhQejfu44As=\"\n    [\"file\"]=>\n    string(70) \"/data/sites/www.bingewave.com/libraries/Aws/Aws/WrappedHttpHandler.php\"\n    [\"line\"]=>\n    int(202)\n    [\"trace\"]=>\n    string(3210) \"#0 /data/sites/www.bingewave.com/libraries/Aws/Aws/WrappedHttpHandler.php(102): Aws\\WrappedHttpHandler->parseError()\n  #1 /data/sites/www.bingewave.com/libraries/Aws/GuzzleHttp/Promise/Promise.php(203): Closure$Aws\\WrappedHttpHandler::__invoke#3()\n  #2 /data/sites/www.bingewave.com/libraries/Aws/GuzzleHttp/Promise/Promise.php(174): GuzzleHttp\\Promise\\Promise::callHandler()\n  #3 /data/sites/www.bingewave.com/libraries/Aws/GuzzleHttp/Promise/RejectedPromise.php(40): Closure$GuzzleHttp\\Promise\\Promise::settle#3()\n  #4 /data/sites/www.bingewave.com/libraries/Aws/GuzzleHttp/Promise/TaskQueue.php(47): Closure$GuzzleHttp\\Promise\\RejectedPromise::then()\n  #5 /data/sites/www.bingewave.com/libraries/Aws/GuzzleHttp/Handler/CurlMultiHandler.php(96): GuzzleHttp\\Promise\\TaskQueue->run()\n  #6 /data/sites/www.bingewave.com/libraries/Aws/GuzzleHttp/Handler/CurlMultiHandler.php(123): GuzzleHttp\\Handler\\CurlMultiHandler->tick()\n  #7 /data/sites/www.bingewave.com/libraries/Aws/GuzzleHttp/Promise/Promise.php(246): GuzzleHttp\\Handler\\CurlMultiHandler->execute()\n  #8 /data/sites/www.bingewave.com/libraries/Aws/GuzzleHttp/Promise/Promise.php(223): GuzzleHttp\\Promise\\Promise->invokeWaitFn()\n  #9 /data/sites/www.bingewave.com/libraries/Aws/GuzzleHttp/Promise/Promise.php(267): GuzzleHttp\\Promise\\Promise->waitIfPending()\n  #10 /data/sites/www.bingewave.com/libraries/Aws/GuzzleHttp/Promise/Promise.php(225): GuzzleHttp\\Promise\\Promise->invokeWaitList()\n  #11 /data/sites/www.bingewave.com/libraries/Aws/GuzzleHttp/Promise/Promise.php(267): GuzzleHttp\\Promise\\Promise->waitIfPending()\n  #12 /data/sites/www.bingewave.com/libraries/Aws/GuzzleHttp/Promise/Promise.php(225): GuzzleHttp\\Promise\\Promise->invokeWaitList()\n  #13 /data/sites/www.bingewave.com/libraries/Aws/GuzzleHttp/Promise/Promise.php(62): GuzzleHttp\\Promise\\Promise->waitIfPending()\n  #14 /data/sites/www.bingewave.com/libraries/Aws/GuzzleHttp/Promise/Coroutine.php(65): GuzzleHttp\\Promise\\Promise->wait()\n  #15 /data/sites/www.bingewave.com/libraries/Aws/GuzzleHttp/Promise/Promise.php(246): Closure$GuzzleHttp\\Promise\\Coroutine::__construct()\n  #16 /data/sites/www.bingewave.com/libraries/Aws/GuzzleHttp/Promise/Promise.php(223): GuzzleHttp\\Promise\\Promise->invokeWaitFn()\n  #17 /data/sites/www.bingewave.com/libraries/Aws/GuzzleHttp/Promise/Promise.php(267): GuzzleHttp\\Promise\\Promise->waitIfPending()\n  #18 /data/sites/www.bingewave.com/libraries/Aws/GuzzleHttp/Promise/Promise.php(225): GuzzleHttp\\Promise\\Promise->invokeWaitList()\n  #19 /data/sites/www.bingewave.com/libraries/Aws/GuzzleHttp/Promise/Promise.php(62): GuzzleHttp\\Promise\\Promise->waitIfPending()\n  #20 /data/sites/www.bingewave.com/libraries/Aws/Aws/Multipart/AbstractUploadManager.php(83): GuzzleHttp\\Promise\\Promise->wait()\n  #21 /data/sites/www.bingewave.com/global/models/TempFiles.php(121): Aws\\Multipart\\AbstractUploadManager->upload()\n  #22 /data/sites/www.bingewave.com/global/models/TempFiles.php(154): TempFiles->sendToStorage()\n  #23 /data/sites/www.bingewave.com/partners/cli/TempFilesCli.php(15): TempFiles->uploadLocalFile()\n  #24 /data/sites/www.bingewave.com/libraries/helium/console.class.php(52): TempFilesCli->upload()\n  #25 /data/sites/www.bingewave.com/partners/helium.php(44): HeliumConsole::init()\n  #26 {main}\"\n    [\"type\"]=>\n    string(6) \"client\"\n    [\"code\"]=>\n    string(12) \"MalformedXML\"\n    [\"requestId\"]=>\n    string(16) \"F8B36AE804390FEC\"\n    [\"statusCode\"]=>\n    int(400)\n    [\"result\"]=>\n    NULL\n    [\"request\"]=>\n    array(7) {\n      [\"instance\"]=>\n      string(32) \"0000000000000000dc1bc01092fec4f0\"\n      [\"method\"]=>\n      string(4) \"POST\"\n      [\"headers\"]=>\n      array(9) {\n        [\"X-Amz-Security-Token\"]=>\n        string(7) \"[TOKEN]\"\n        [\"Host\"]=>\n        array(1) {\n          [0]=>\n          string(48) \"ex-videos-producition.s3-us-west-2.amazonaws.com\"\n        }\n        [\"Content-Type\"]=>\n        array(1) {\n          [0]=>\n          string(15) \"application/xml\"\n        }\n        [\"User-Agent\"]=>\n        array(1) {\n          [0]=>\n          string(18) \"aws-sdk-php/3.29.2\"\n        }\n        [\"aws-sdk-invocation-id\"]=>\n        array(1) {\n          [0]=>\n          string(32) \"e621a501eda11f51ac67f0545997a1f0\"\n        }\n        [\"aws-sdk-retry\"]=>\n        array(1) {\n          [0]=>\n          string(3) \"0/0\"\n        }\n        [\"X-Amz-Content-Sha256\"]=>\n        array(1) {\n          [0]=>\n          string(64) \"e61f015e3f445ee56d44d7dcef9ea6f315b1a76ef968c302927e9bd9bfc4a4b8\"\n        }\n        [\"X-Amz-Date\"]=>\n        array(1) {\n          [0]=>\n          string(16) \"20170622T234310Z\"\n        }\n        [\"Authorization\"]=>\n        array(1) {\n          [0]=>\n          string(247) \"AWS4-HMAC-SHA256 Credential=[KEY]/20170622/us-west-2/s3/aws4_request, SignedHeaders=aws-sdk-invocation-id;aws-sdk-retry;host;x-amz-content-sha256;x-amz-date, Signature=[SIGNATURE]\n        }\n      }\n      [\"body\"]=>\n      string(114) \"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n  \n  \"\n      [\"scheme\"]=>\n      string(5) \"https\"\n      [\"path\"]=>\n      string(37) \"/ef740de5-46f1-4dd5-8d19-5d3d2142b7e5\"\n      [\"query\"]=>\n      string(137) \"uploadId=ZALqvQr0fPQ5zYG2q1FYcxVljDRu9YKpOnXjYrCB1yRduofAW4dFWYzaLhnbqagsXKkBRZ3Zgqa2jIX.IKkGynGBXw3mZVX99SWTkO78jyGcPtZjpZAlUJ6Ct9I8yB7y\"\n    }\n    [\"response\"]=>\n    array(4) {\n      [\"instance\"]=>\n      string(32) \"0000000000000000dc1bc01092e45bd0\"\n      [\"statusCode\"]=>\n      int(400)\n      [\"headers\"]=>\n      array(8) {\n        [\"X-Amz-Security-Token\"]=>\n        string(7) \"[TOKEN]\"\n        [\"x-amz-request-id\"]=>\n        array(1) {\n          [0]=>\n          string(16) \"F8B36AE804390FEC\"\n        }\n        [\"x-amz-id-2\"]=>\n        array(1) {\n          [0]=>\n          string(76) \"oP/CL485WE/rrXBP3p82a9db+Ssk4rxmx6CjXcp/GKMPeGws/7FIYunBlfW0+7+HAhQejfu44As=\"\n        }\n        [\"Content-Type\"]=>\n        array(1) {\n          [0]=>\n          string(15) \"application/xml\"\n        }\n        [\"Transfer-Encoding\"]=>\n        array(1) {\n          [0]=>\n          string(7) \"chunked\"\n        }\n        [\"Date\"]=>\n        array(1) {\n          [0]=>\n          string(29) \"Thu, 22 Jun 2017 23:43:10 GMT\"\n        }\n        [\"Connection\"]=>\n        array(1) {\n          [0]=>\n          string(5) \"close\"\n        }\n        [\"Server\"]=>\n        array(1) {\n          [0]=>\n          string(8) \"AmazonS3\"\n        }\n      }\n      [\"body\"]=>\n      string(280) \"MalformedXMLThe XML you provided was not well-formed or did not validate against our published schemaF8B36AE804390FECoP/CL485WE/rrXBP3p82a9db+Ssk4rxmx6CjXcp/GKMPeGws/7FIYunBlfW0+7+HAhQejfu44As=\"\n    }\n  }\nInclusive step time: 0.68636846542358\n```. @imshashank Per one of your previous bug, I tried to put the prefix in because of the malformed xml error I was getting, but it did not resolve.\n$s3 = S3Client::factory(array(\n            'credentials' => array(\n                'secret' => xyz,\n                'key' => abc\n            ),\n            'prefix' => '',\n            'region' => 'us-west-2',\n            'version' => 'latest',\n            'debug' => true,\n            'http'    => [\n                'timeout' => 12000,\n                //'debug' => true\n            ]\n        ));;\n. @imshashank @kstich Any idea what can be causing the MalformedXML only for MultipartUploads?. @imshashank @kstich I noticed this, the key is set to [KEY] in the url. Is this correct?\n```\n^X^X Found bundle for host ex-videos-producition.s3-us-west-2.amazonaws.com: 0x7f9075881c60\n Re-using existing connection! (#0) with host ex-videos-producition.s3-us-west-2.amazonaws.com\n* Connected to ex-videos-producition.s3-us-west-2.amazonaws.com (54.231.176.197) port 443 (#0)\n\nPUT /ef740de5-46f1-4dd5-8d19-5d3d2142b7e5?partNumber=17&uploadId=gUF7fpnyEGS7uXQ38C4jN8T8z5aHSVkKLIqfdfUrx9oBHuSIN19jj_W2c2fBHbdURQcX_R0g70kNG7bivwM5yrwGRO1H5xWlQIpWu8XkzMBval5MopHbe95mDj0DtFoK HTTP/1.1\nExpect: 100-Continue\nHost: ex-videos-producition.s3-us-west-2.amazonaws.com\nContent-Type: application/octet-stream\naws-sdk-invocation-id: 9162a10ba5933818e99976c17b9a3fee\naws-sdk-retry: 0/0\nX-Amz-Content-Sha256: f42dc119d3f0cf61e69407245d32458fa8ad7ce32132552ba24f6ee669fcaf3d\nX-Amz-Date: 20170627T102000Z\nAuthorization: AWS4-HMAC-SHA256 Credential=[KEY]/20170627/us-west-2/s3/aws4_request, SignedHeaders=aws-sdk-invocation-id;aws-sdk-retry;host;x-amz-content-sha256;x-amz-date, Signature=[SIGNATURE]\nUser-Agent: aws-sdk-php/3.29.2 GuzzleHttp/6.2.1 curl/7.29.0 PHP/5.6.99-hhvm\nContent-Length: 52428800\n```. @kstich This wasn't the error right here? The malformed url?\n\n```\n- Leaving step sign, name 's3.permanent_redirect'\n\nerror was set to array(13) {\n    [\"instance\"]=>\n    string(32) \"0000000000000000dc1bc01092dbd710\"\n    [\"class\"]=>\n    string(28) \"Aws\\S3\\Exception\\S3Exception\"\n    [\"message\"]=>\n    string(1123) \"Error executing \"CompleteMultipartUpload\" on \"https://ex-videos-producition.s3-us-west-2.amazonaws.com/ef740de5-46f1-4dd5-8d19-5d3d2142b7e5?uploadId=ZALqvQr0fPQ5zYG2q1FYcxVljDRu9YKpOnXjYrCB1yRduofAW4dFWYzaLhnbqagsXKkBRZ3Zgqa2jIX.IKkGynGBXw3mZVX99SWTkO78jyGcPtZjpZAlUJ6Ct9I8yB7y\"; AWS HTTP error: Client error: POST https://ex-videos-producition.s3-us-west-2.amazonaws.com/ef740de5-46f1-4dd5-8d19-5d3d2142b7e5?uploadId=ZALqvQr0fPQ5zYG2q1FYcxVljDRu9YKpOnXjYrCB1yRduofAW4dFWYzaLhnbqagsXKkBRZ3Zgqa2jIX.IKkGynGBXw3mZVX99SWTkO78jyGcPtZjpZAlUJ6Ct9I8yB7y resulted in a 400 Bad Request response:\n  MalformedXMLThe XML you provided was not well-formed or did not validate against our publis (truncated...)\n   MalformedXML (client): The XML you provided was not well-formed or did not validate against our published schema - MalformedXMLThe XML you provided was not well-formed or did not validate against our published schemaF8B36AE804390FECoP/CL485WE/rrXBP3p82a9db+Ssk4rxmx6CjXcp/GKMPeGws/7FIYunBlfW0+7+HAhQejfu44As=\"\n    [\"file\"]=>\n    string(70) \"/data/sites/www.bingewave.com/libraries/Aws/Aws/WrappedHttpHandler.php\"\n    [\"line\"]=>\n    int(202)\n    [\"trace\"]=>\n    string(3210) \"#0 /data/sites/www.bingewave.com/libraries/Aws/Aws/WrappedHttpHandler.php(102): Aws\\WrappedHttpHandler->parseError()\n  #1 /data/sites/www.bingewave.com/libraries/Aws/GuzzleHttp/Promise/Promise.php(203): Closure$Aws\\WrappedHttpHandler::__invoke#3()\n  #2 /data/sites/www.bingewave.com/libraries/Aws/GuzzleHttp/Promise/Promise.php(174): GuzzleHttp\\Promise\\Promise::callHandler()\n  #3 /data/sites/www.bingewave.com/libraries/Aws/GuzzleHttp/Promise/RejectedPromise.php(40): Closure$GuzzleHttp\\Promise\\Promise::settle#3()\n  #4 /data/sites/www.bingewave.com/libraries/Aws/GuzzleHttp/Promise/TaskQueue.php(47): Closure$GuzzleHttp\\Promise\\RejectedPromise::then()\n  #5 /data/sites/www.bingewave.com/libraries/Aws/GuzzleHttp/Handler/CurlMultiHandler.php(96): GuzzleHttp\\Promise\\TaskQueue->run()\n  #6 /data/sites/www.bingewave.com/libraries/Aws/GuzzleHttp/Handler/CurlMultiHandler.php(123): GuzzleHttp\\Handler\\CurlMultiHandler->tick()\n  #7 /data/sites/www.bingewave.com/libraries/Aws/GuzzleHttp/Promise/Promise.php(246): GuzzleHttp\\Handler\\CurlMultiHandler->execute()\n  #8 /data/sites/www.bingewave.com/libraries/Aws/GuzzleHttp/Promise/Promise.php(223): GuzzleHttp\\Promise\\Promise->invokeWaitFn()\n  #9 /data/sites/www.bingewave.com/libraries/Aws/GuzzleHttp/Promise/Promise.php(267): GuzzleHttp\\Promise\\Promise->waitIfPending()\n  #10 /data/sites/www.bingewave.com/libraries/Aws/GuzzleHttp/Promise/Promise.php(225): GuzzleHttp\\Promise\\Promise->invokeWaitList()\n  #11 /data/sites/www.bingewave.com/libraries/Aws/GuzzleHttp/Promise/Promise.php(267): GuzzleHttp\\Promise\\Promise->waitIfPending()\n  #12 /data/sites/www.bingewave.com/libraries/Aws/GuzzleHttp/Promise/Promise.php(225): GuzzleHttp\\Promise\\Promise->invokeWaitList()\n  #13 /data/sites/www.bingewave.com/libraries/Aws/GuzzleHttp/Promise/Promise.php(62): GuzzleHttp\\Promise\\Promise->waitIfPending()\n  #14 /data/sites/www.bingewave.com/libraries/Aws/GuzzleHttp/Promise/Coroutine.php(65): GuzzleHttp\\Promise\\Promise->wait()\n  #15 /data/sites/www.bingewave.com/libraries/Aws/GuzzleHttp/Promise/Promise.php(246): Closure$GuzzleHttp\\Promise\\Coroutine::__construct()\n  #16 /data/sites/www.bingewave.com/libraries/Aws/GuzzleHttp/Promise/Promise.php(223): GuzzleHttp\\Promise\\Promise->invokeWaitFn()\n  #17 /data/sites/www.bingewave.com/libraries/Aws/GuzzleHttp/Promise/Promise.php(267): GuzzleHttp\\Promise\\Promise->waitIfPending()\n  #18 /data/sites/www.bingewave.com/libraries/Aws/GuzzleHttp/Promise/Promise.php(225): GuzzleHttp\\Promise\\Promise->invokeWaitList()\n  #19 /data/sites/www.bingewave.com/libraries/Aws/GuzzleHttp/Promise/Promise.php(62): GuzzleHttp\\Promise\\Promise->waitIfPending()\n  #20 /data/sites/www.bingewave.com/libraries/Aws/Aws/Multipart/AbstractUploadManager.php(83): GuzzleHttp\\Promise\\Promise->wait()\n  #21 /data/sites/www.bingewave.com/global/models/TempFiles.php(121): Aws\\Multipart\\AbstractUploadManager->upload()\n  #22 /data/sites/www.bingewave.com/global/models/TempFiles.php(154): TempFiles->sendToStorage()\n  #23 /data/sites/www.bingewave.com/partners/cli/TempFilesCli.php(15): TempFiles->uploadLocalFile()\n  #24 /data/sites/www.bingewave.com/libraries/helium/console.class.php(52): TempFilesCli->upload()\n  #25 /data/sites/www.bingewave.com/partners/helium.php(44): HeliumConsole::init()\n  #26 {main}\"\n    [\"type\"]=>\n    string(6) \"client\"\n    [\"code\"]=>\n    string(12) \"MalformedXML\"\n    [\"requestId\"]=>\n    string(16) \"F8B36AE804390FEC\"\n    [\"statusCode\"]=>\n    int(400)\n    [\"result\"]=>\n    NULL\n    [\"request\"]=>\n    array(7) {\n      [\"instance\"]=>\n      string(32) \"0000000000000000dc1bc01092fec4f0\"\n      [\"method\"]=>\n      string(4) \"POST\"\n      [\"headers\"]=>\n      array(9) {\n        [\"X-Amz-Security-Token\"]=>\n        string(7) \"[TOKEN]\"\n        [\"Host\"]=>\n        array(1) {\n          [0]=>\n          string(48) \"ex-videos-producition.s3-us-west-2.amazonaws.com\"\n        }\n        [\"Content-Type\"]=>\n        array(1) {\n          [0]=>\n          string(15) \"application/xml\"\n        }\n        [\"User-Agent\"]=>\n        array(1) {\n          [0]=>\n          string(18) \"aws-sdk-php/3.29.2\"\n        }\n        [\"aws-sdk-invocation-id\"]=>\n        array(1) {\n          [0]=>\n          string(32) \"e621a501eda11f51ac67f0545997a1f0\"\n        }\n        [\"aws-sdk-retry\"]=>\n        array(1) {\n          [0]=>\n          string(3) \"0/0\"\n        }\n        [\"X-Amz-Content-Sha256\"]=>\n        array(1) {\n          [0]=>\n          string(64) \"e61f015e3f445ee56d44d7dcef9ea6f315b1a76ef968c302927e9bd9bfc4a4b8\"\n        }\n        [\"X-Amz-Date\"]=>\n        array(1) {\n          [0]=>\n          string(16) \"20170622T234310Z\"\n        }\n        [\"Authorization\"]=>\n        array(1) {\n          [0]=>\n          string(247) \"AWS4-HMAC-SHA256 Credential=[KEY]/20170622/us-west-2/s3/aws4_request, SignedHeaders=aws-sdk-invocation-id;aws-sdk-retry;host;x-amz-content-sha256;x-amz-date, Signature=[SIGNATURE]\n        }\n      }\n      [\"body\"]=>\n      string(114) \"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n  \n  \"\n      [\"scheme\"]=>\n      string(5) \"https\"\n      [\"path\"]=>\n      string(37) \"/ef740de5-46f1-4dd5-8d19-5d3d2142b7e5\"\n      [\"query\"]=>\n      string(137) \"uploadId=ZALqvQr0fPQ5zYG2q1FYcxVljDRu9YKpOnXjYrCB1yRduofAW4dFWYzaLhnbqagsXKkBRZ3Zgqa2jIX.IKkGynGBXw3mZVX99SWTkO78jyGcPtZjpZAlUJ6Ct9I8yB7y\"\n    }\n    [\"response\"]=>\n    array(4) {\n      [\"instance\"]=>\n      string(32) \"0000000000000000dc1bc01092e45bd0\"\n      [\"statusCode\"]=>\n      int(400)\n      [\"headers\"]=>\n      array(8) {\n        [\"X-Amz-Security-Token\"]=>\n        string(7) \"[TOKEN]\"\n        [\"x-amz-request-id\"]=>\n        array(1) {\n          [0]=>\n          string(16) \"F8B36AE804390FEC\"\n        }\n        [\"x-amz-id-2\"]=>\n        array(1) {\n          [0]=>\n          string(76) \"oP/CL485WE/rrXBP3p82a9db+Ssk4rxmx6CjXcp/GKMPeGws/7FIYunBlfW0+7+HAhQejfu44As=\"\n        }\n        [\"Content-Type\"]=>\n        array(1) {\n          [0]=>\n          string(15) \"application/xml\"\n        }\n        [\"Transfer-Encoding\"]=>\n        array(1) {\n          [0]=>\n          string(7) \"chunked\"\n        }\n        [\"Date\"]=>\n        array(1) {\n          [0]=>\n          string(29) \"Thu, 22 Jun 2017 23:43:10 GMT\"\n        }\n        [\"Connection\"]=>\n        array(1) {\n          [0]=>\n          string(5) \"close\"\n        }\n        [\"Server\"]=>\n        array(1) {\n          [0]=>\n          string(8) \"AmazonS3\"\n        }\n      }\n      [\"body\"]=>\n      string(280) \"MalformedXMLThe XML you provided was not well-formed or did not validate against our published schemaF8B36AE804390FECoP/CL485WE/rrXBP3p82a9db+Ssk4rxmx6CjXcp/GKMPeGws/7FIYunBlfW0+7+HAhQejfu44As=\"\n    }\n  }\nInclusive step time: 0.68636846542358\n```. ",
    "rampo83": "Hi  kstich thank you for the answer but I wrote here because is more focused than other. And reading documentation (that I link too) seems that informations I need to get about the source video must be placed in the response. But I get the response and the structure I expect to see is different. No \"DetectedProperties\" present.\nThis is the documentation aobut for example createObject:\nhttp://docs.aws.amazon.com/aws-sdk-php/v3/api/api-elastictranscoder-2012-09-25.html#createjob\nMy code is something like this:\n```\n$job = $elasticTranscoder->createJob(array(\n'PipelineId' => '',\n\n'OutputKeyPrefix' => 'Output prefix added to the file',\n\n'Input' => array(\n    'Key' => 'the path from bucket to the file, with file name',\n    'FrameRate' => 'auto',\n    'Resolution' => 'auto',\n    'AspectRatio' => 'auto',\n    'Interlaced' => 'auto',\n    'Container' => 'auto',\n),\n\n'Outputs' => array(\n    array(\n        'Key' => 'myOutput.mp4',\n        'Rotate' => 'auto',\n        'PresetId' => '',\n    ),\n),\n\n));\n```\nWhat I would ask for to anyone knows deeply the PHP SDK is why the documentation says that I will found DetectedProperties in the result, data that must be automatically added from the Elastic Transcoder using the source file to encode, and I get a result without it.\nI think it something like an issue, or if not the documentation is really poor about this. I search in web for hours something usefull to solve the problem. And I write here because I found nothing.... Thank you kstich, I already find an explanation too. I see on AWS web interface that jobs with error final status does not have these data about the source video in the final report. But success jobs have it. So I try something new: instead of read createJob and readJob response during the video processing, I call readJob on a completed job and finally I get DetectedProperties on the response.\nI finally observe, as you do, that documentation really lacks about it, and this discrepancy made me waste really lot of time.. ",
    "tarunkumar2215": "I am running this code in localhost.. Fixed this issue. Here is the code below.\n```\n$client = AWS::createClient('Route53');\n  $result = $client->changeResourceRecordSets([\n      'ChangeBatch' => [\n          'Changes' => [\n              [\n                  'Action' => 'CREATE',\n                  'ResourceRecordSet' => [\n                      'AliasTarget' => [\n                          'DNSName' => HOSTED_ZONE_DNSNAME_LB,\n                          'EvaluateTargetHealth' => false,\n                          'HostedZoneId' => HOSTED_ZONE_ID_LB,\n                      ],\n                      // Name is required\n                      'Name' => 'test2.xyz.co.in.',\n                      // Type is required\n                        'Type' => 'A',\n                  ],\n              ],\n          ],\n          'Comment' => 'Creating new Route',\n      ],\n      'HostedZoneId' => HOSTED_ZONE_ID_R53, // Depends on the type of resource that you want to route traffic to\n  ]);\n\n```. ",
    "faizanakram99": "Still doesnt work\nI am using AWS elastic beanstalk\nWhy does it try to read credentials from /.aws/credentials ??\nI provided hard coded credentials in this format\n'profile' => 'default',\n                'version' => 'latest',\n                'region'  => 'us-west-2',\n                'credentials' => [\n                    'key' => 'key',\n                    'secret' => 'secret',\n                ],\nstill the same error.\nCreated environment variables in aws beanstalk with AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY, it still didnt work (same error).\nPHP Fatal error:  Uncaught Aws\\\\Exception\\\\CredentialsException: Cannot read credentials from \n/.aws/credentials in /var/app/current/symfony/vendor/aws/aws-sdk-php/src/Credentials/CredentialProvider.php:394\\nStack trace:\\n#0 /var/app/current/symfony/vendor/aws/aws-sdk-php/src/Credentials/CredentialProvider.php(290): Aws\\\\Credentials\\\\CredentialProvider::reject('Cannot read cre...')\\n#1 /var/app/current/symfony/vendor/aws/aws-sdk-php/src/Middleware.php(121): Aws\\\\Credentials\\\\CredentialProvider::Aws\\\\Credentials\\\\{closure}()\\n#2 /var/app/current/symfony/vendor/aws/aws-sdk-php/src/RetryMiddleware.php(192): Aws\\\\Middleware::Aws\\\\{closure}(Object(Aws\\\\Command), Object(GuzzleHttp\\\\Psr7\\\\Request))\\n#3 /var/app/current/symfony/vendor/aws/aws-sdk-php/src/Middleware.php(206): Aws\\\\RetryMiddleware->__invoke(Object(Aws\\\\Command), Object(GuzzleHttp\\\\Psr7\\\\Request))\\n#4 /var/app/current/symfony/vendor/aws/aws-sdk-php/src/ClientResolver.php(584): Aws\\\\Middleware::Aws\\\\{closure}(Object(Aws\\\\Command), Object(GuzzleHttp\\\\Psr7\\\\Request))\\n#5 /var/app/current/symfony/vendor/ in /var/app/current/symfony/vendor/aws/aws-sdk-php/src/Credentials/CredentialProvider.php on line 394, referer: http://qbil-dev.us-west-2.elasticbeanstalk.com/. ",
    "raoabid": "@faizanakram99 \nYou can do the following:\n$credentials = new Aws\\Credentials\\Credentials(AWS_ACCESS_KEY_ID , AWS_SECRET_ACCESS_KEY);\narray(\n'region'      => 'us-west-2',\n'version'     => 'latest',\n'credentials' => $credentials\n)\n. ",
    "gabriel403": "@kstich when using the sdk I get the exception printed above, when using the fork in #1291 I don't get the exception and the file uploads successfully, but the file is not encrypted using SSEC. Small files do encrypt just fine using the sdk and the fork from the other pr. . @kstich good news, thanks, anything else I can do to help just give me a nudge. ",
    "dioogo91": "@imshashank I closed the pull request because there's no way to safely guarantee that some string is base 64 encoded. The code I used only guarantees that it can be base 64 encoded.\nNonetheless I still think the SSECMiddleware should also allow the client the power to choose a custom base 64 encoded encryption key because of what I said on my first comment:\n\nThis reduces the password strength since it is not possible to use the full 256 values per byte, and only around 95 (printable ascii) values, effectively limiting the possible password entropy.\n\nMaybe another SSE Parameter that can be used as a flag to choose between the two choices. I'm working on it right now.. @imshashank to answer your question: it would change if the old SSE keys used would pass through base64_decode PHP function as a valid value. I think the solution I presented to you on my previous comment wouldn't change behaviour and would be painless to use it with old and new clients simultaneously. . @imshashank this version won't change behavior of existing clients: clients that don't want to use a custom base64 encoded key can either leave SSECustomerKeyIsBase64 option value undefined or set it as false or null. You can only use a custom base64 encoded key if you set the value as true.. @kstich any news?. @imshashank You're right: we could save it in base64, decode it and then pass it to the s3Client. The thing is every library or adapter that uses this SDK has to change its code in order to do that. \nNonetheless and most importantly I think this enhances the flexibility of the SDK because this gives the customer the alternative of using an already base64 encoded key without having to decode it every time the API methods are called. \nAs you said \"customers need to provide a key that is not base64_encoded\" and that is a limitation to the customer that my pull request tries to mitigate. Surely there are ways to go around this but I think this would be a nice enhancement to this SDK since it doesn't even change its default behaviour in any way.. ",
    "luismontreal": "Many thanks for your quick reply, you were right, I got the exception now from 3.28.1\nAws\\Sqs\\Exception\\SqsException: Attribute MD5 mismatch. Expected d41d8cd98f00b204e9800998ecf8427e, found No Attributes in /var/www/html/playground/vendor/aws/aws-sdk-php/src/Sqs/SqsClient.php on line 215\nFor some reason localstack always returns an MD5 of the message attributes even when I send no attributes (I'll talk to them)\nexample:\n[vagrant@localhost localstack]$ aws --profil local sqs send-message --queue-url http://localhost:4576/Development-Luis --message-body  'my message' --endpoint-url=http://localhost:4576\nResponse:\n{\n    \"MD5OfMessageBody\": \"3c9d24df935626c2a7eb6ae2e483cf32\", \n    \"MD5OfMessageAttributes\": \"d41d8cd98f00b204e9800998ecf8427e\", \n    \"MessageId\": \"a47c2ba2-bd04-442b-9423-b0d468b54ced\"\n}\nMany thanks once again. Thanks @ingluisjimenez  I agree this should be fixed on localstack's end.  amazon doesn't return md5 of empty string. ",
    "ingluisjimenez": "We are running through the same issue.\n@kstich the issue is with the validation of the message attributes. When there are no message attributes, localstack returns the md5 hash for an empty string, which is \"d41d8cd98f00b204e9800998ecf8427e\", but the calculateMessageAttributesMd5() method in the SqsClient class returns null, as there are no message attributes, so the hash doesn't match null.\nI'd say the issue should be fixed in the localstack side, as it should not be sending back MD5OfMessageAttributes when there are no attributes being sent.. returning md5('') in this line fixed the issue for me https://github.com/aws/aws-sdk-php/blob/master/src/Sqs/SqsClient.php#L125\nNot sure how do you want to handle this case.. ",
    "wellu": "Spent hours on debugging why the heck echoing file_get_contents from an S3 bucket didn't work as expected. Internally it uses trigger_error and that surely messes the output.. ",
    "TakesTheBiscuit": "Appreciate that -- the public links on the front end link there (to v2), i didn't notice anything that said V2 until you just mentioned it, and then it's only in the address bar / URI -- ideally the document would say on it V2? \n\n. @kstich makes total sense. @kstich sorry, yes, i think that's done now in b174abc . You're totally right, i don't know why i was using such poor Engrish! \n550a325\n. ",
    "gohanman": "D'oh. Sorry for not reading closely enough.. ",
    "daum": "Sure just made a PR here: https://github.com/aws/aws-sdk-php/pull/1325 .  Let me know if you want me to change or add anything. Will take a look at the failed test first thing in the morning.. @imshashank On the tests the reason they are failing is because they use reflection to allow direct access to the private convertExpires method (https://github.com/aws/aws-sdk-php/blob/master/tests/Signature/SignatureV4Test.php#L98).  Since I've updated the method to take the converted timestamp already they are now failing.  \nFor now what I did was update the tests to go through the entire presign function and then ensure the URL has the proper expiration duration on it.  This does encapsulate a little more than the specific duration test, since if the URL changes the tests would break, however I think it is better than adding an extra convertTimestamp to the convertExpires function.  Adding that extra, duplicate convertTimestamp in that method, would never do anything in production and only satisfy the test, so it isn't the best actual test.  Second it would be less efficient and less readable, as it would make someone who is quickly reading the code try to think of how the $expiresTimestamp could need to be converted when it already was in the only method which calls convertExpires.\nLet me know your thoughts on this and the updated tests.. @imshashank Wanted to check in on this, do you think this change will be implemented soon to fix the existing edge case?  Just debating on if we need to fork for the time being to account for it.. @imshashank Ok thanks!  Happy to help with any test cases if you'd like.. @imshashank All set just updated those lines.. @kstich @imshashank  Just wanted to check in on this to see if you think it will be integrated soon or if there are other questions.. You'd technically get a different duration however it won't throw an error.  Assuming that server times are synced up noone could request it in historical time (requiring the extra second of duration).    The goal of this PR is to fix the opposite case where the second rolls and you fall past the 1 week allow duration and it throws an error, even though the initial request is valid.. All set, removed.. @kstich Not sure I follow what the fix for both scenarios would be.  I think part of the issue is that if the SDK allows any relative timestamp to be passed there will always be scenarios when the duration for the expires length could be off.  The reason for that is that the thread could be paused/roll over a second at any given point, so unless the caller of presign includes a \"now\" parameter (options perhaps?) there is always the possibility of a slight skew.\nWe could add that as an option to pass, if passed any strtotime in the convertToTimestamp will use the \"now\" as the second parameter for the time it is based on.  \nThis would solve any specific duration requests from possibly being off by a tiny amount.  We could maintain BC by making it an optional parameter, so if it isn't passed it will just use the current time that the createSign is invoked, which majority of the time would be the same time.  \nLet me know if that makes sense.  I think the change to the existing PR would be fairly minimal.  I'd update convertToTimestamp to take a second parameter, which would be passed to the strtotime conversion in it.  Then in the options array it'd check for the \"now\" parameter, if non-existent it'd take current time.  What do you think the option should be?  \"relativeTimeBase\" could be more descriptive than now?\n. Ok sounds good, will try to update the PR with the additional test + other changes either later today or tomorrow hopefully.. Just updated to use the startTimestamp.  Let me know if you are ok with that ternary in the strtotime function for the convertTimestamp, wasn't sure if you'd prefer that or a nested if/else.\n. @kstich I think the segfault on the builds is coming from the precise to trusty upgrade, I can't replicate locally.  It also appears this is a change Travis made recently: https://blog.travis-ci.com/2017-07-11-trusty-as-default-linux-is-coming .  There were no segfaults before the upgrade, I can add the precise variable into the travis config if you want.  Let me know how you want to proceed.. @kstich Actually that's a fun PHP gotcha, here is what happens if I send it a null when using a relative timestamp (see last test case):\nphp\n$time = time();\n$date = date(DATE_ATOM, $time);\necho strtotime($date).\"\\n\"; // 1501704830\necho strtotime($date, null).\"\\n\"; // 1501704830\necho strtotime($date, $time).\"\\n\"; // 1501704830\necho strtotime('+15 minutes', $time).\"\\n\"; // 1501705730\necho strtotime('+15 minutes', null).\"\\n\"; // 900\nThe second parameter has to default to time() or a valid int otherwise it starts at 0 which is what you see in my last example.. Ok - will wait for you guys to merge that, then will merge those updates into this branch.. All set!. How do you want to break the ternary statements?  I personally think they are much more readable on one line like this but understand the 80 character comment, was just following other examples in the code like https://github.com/daum/aws-sdk-php/blob/6cd12a2744b9da9813faf1f7d6bf3f52d336c67a/src/Signature/SignatureV4.php#L39\nCan you just give me an example of how you'd like the ternary formatted for the multiple lines and I'll get that fixed up asap.  . Similar to my other comment - how would you like the ternary broken up.  If you would prefer I could also switch it to a if/else which would have a shorter line length.. Thanks just updated. ",
    "expertcoder": "It would also be nice if you could assume the role via the constructor argument also as well as AWS_PROFILE environmental variable\n$s3 = new Aws\\S3\\S3Client([\n    'region' => 'us-east-1',\n    'profile' => 'myrole',\n    'version' => 'latest',\n]);\nThis is not supported either it seems.. If you have time and energy, you an write your own \"Credential Provider\" in PHP to do almost anything you want (https://docs.aws.amazon.com/sdk-for-php/v3/developer-guide/guide_credentials.html). This happened to suit my needs, but understand that its annoying to have to spend time to implement something which could be supported by default.. I solved this problem months ago, I cant remember the specifics very well.\nHere is how I solved this with Symfony 4, I will cut and paste my Symfony config as inspiration. I believe you should be able to manually instantiate classes in pure PHP code if you are not using Symfony, and just try to replicate what the config files are doing. Sorry I can't be of better help but it would take me a while to remember the exact steps and provide a generic PHP example.\nCredentialProvider.yml (For prod/EC2) from memory this will just use the attached role\nservices:\n  aws_credential_provider:\n    class: Aws\\Credentials\\InstanceProfileProvider\n    public: false\n    factory: ['Aws\\Credentials\\CredentialProvider', instanceProfile]\nCredentialProvider_dev.yml (For \"local\" environments, to use a profile in ~/.aws/credentials folder)\n```\nFor the dev environment we first use the AWS Credentials from our AWS CLI profile\nlocated in ~/.aws/credentials. However PHP don't use the credentials directly, rather\nwe use them to assume an AWS Role, and PHP will use that role to make API calls.\naws_sts_client service is only needed on dev for assuming the role\nservices:\n  aws_credential_provider:\n    class: Aws\\Credentials\\AssumeRoleCredentialProvider\n    public: false\n    factory: ['Aws\\Credentials\\CredentialProvider', assumeRole]\n    arguments:\n      - client: '@aws_sts_client'\n        assume_role_params:\n            RoleArn: '%aws_role_arn%'\n            RoleSessionName: 'my_php_session' # This is an arbitrary name\n        version: latest\naws_sts_client:\n    class: Aws\\Sts\\StsClient\n    public: false\n    arguments:\n      - profile: '%aws_profile%'\n        region: '%aws_region%'\n        version: latest\n```\nS3Client.yml\nservices:\n  Aws\\S3\\S3Client:\n    public: true\n    arguments:\n      - credentials: '@aws_credential_provider'\n        region: '%aws_region%'\n        version: latest\nUsage:\n$s3Client = $this->container->get('Aws\\S3\\S3Client')\n. ",
    "jcolfej": "This bug is very annoying and prevents to develop properly ...\nCould we fix it definitively ? Knowing that it is a major \"bug\" !. The problem with that is that you must write 2 authentifications method : \n\none for developement with own credential provider\none for production that use EC2 Env to auth\n\nAnd you must detect if you are in production or not ... And this is not testable ...\nThe problem is that the SDK don't know what is \"source_profile\" .... I confirm the problem is scanner mod : http://php.net/manual/en/function.parse-ini-file.php\nIf you set scanner mod to INI_SCANNER_RAW, I don't have problem like you @ejunker ;)\nCan we patch it as soon as possible ?. ",
    "PavloPixart": "Hi there.\nIs there any news on that topic? I think that this topic could be related to: https://github.com/aws/aws-sdk-php/issues/1030 \nIs there any idea on how much it will take to be fixed? \nThank you.. @et304383 Apologies for that, I was focused on searching any answer on the latest messages. I saw that there have been no answer since 28 May. Do you think there is any chance to escalate to someone that could fix this problem sooner than later?. @expertcoder : Thank you for your solutions and I think it could be a possible way to solve the problem. But I think I will wait.\nI think that the official SDK should be fixed because the way I was using it was the proper way. I hope that someone that is working on this SDK will be able to fix the problem with an official release. . Any update?. I think that a minimum deadline should be set. To get our development stage working we have assign all the permission to a non admin account. So the current status isn't secure in any way. Is it possible to have an estimation on the time you will need to fix the bug? Thank you.. Any update?. A month has passed till the last update request. We are still working with an unsafe way of managing credentials in our development environment. Is there any way to get at least the required time to fix it as well as an estimate date for the release? Thank you.. Any update? Another month has passed more or less.. Hi there. We are blocked for the same reason. Are there any update? When do you think the fix will be available? We are fixing the credentials file by adding some quotes but it will be better to have it fixed directly inside the SDK. Thank you.. ",
    "almasry": "+1 . ",
    "durzo": "+1. ",
    "kyrozetera": "I just ran into this as well. Another workaround is not to configure a default region in the S3 client. In this case it doesn't throw the exception. However, this requires that you don't pass this config to the Sdk constructor, but configure the specific clients in their factory method or constructor instead.. ",
    "dvlpp": "I can confirm this issue. To be more specific, the spatie/laravel-backup package uses the well-known thephpleague/flysystem-aws-s3-v3 package under the hood, and the issue was also reported here: https://github.com/thephpleague/flysystem-aws-s3-v3/issues/113.. ",
    "abiturma": "I encountered the exact same problem.. ",
    "vkulov": "We are having the same issue. Had to downgrade to 3.30.4. I downloaded the aws php sdk zip from http://docs.aws.amazon.com/aws-sdk-php/v3/download/aws.zip for the sake of the example. I'm using the build-in guzzle package.\nThe version of the zip is 3.36.20 - 2017-10-04. We just got that bug on our staging/production environment. Our packages got updated by composer to aws-sdk-php v3.39.2\nAll of a sudden same thing happened to our servers. We download a file from S3 that's 981536 bytes and we end up with a file that's 978944. The difference being < our buffer size which is 4k currently.\nI didn't reply earlier because I thought the bug was limited to Windows. But now we are getting it on Ubuntu with php 7.1.11 after upgrading to aws-sdk-php v3.39.2. I guess this isn't happening to all files and it depends on the file contents. That's why I'm attaching an example file this happens on. Attaching a zip because github doesn't allow uploading .m4a files. Extract the zip.\nfa3fa4e0-06a5-4807-9d76-95f2f2336d11.zip\n. Tried a lot of combinations of aws-php-sdk versions, guzzle version, php versions and nothing seemed to work. We are not sure what changed to cause simple download from s3 to local disk to break.\nWe found that removing the loop that does fread and fwrite and replacing it with stream_copy_to_stream seems to fix the issue.. ",
    "HSken": "Same issue. ",
    "zlodes": "Same, last version, php 7.1.0. ",
    "graphem": "Confirmed, this is fixed. Thanks!!!!. ",
    "HugoMestre": "Nice fix @dioogo91. This will allow stronger passwords to be placed in configuration files, e.g. YAML files.. ",
    "agursoy": "Hi, I'm facing same issue. Is there any update for that? As @cwhite92 said, there is nothing related with our server. (Maybe amazon should change their php.ini :) ) \nFor details check @cwhite92 's post: http://cwhite.me/avoiding-the-burden-of-file-uploads/\n. First we're getting pre-signed data then making ajax request with dropzonejs. Weird thing is I can upload file 5 mb or something but I can not upload file 1 gb etc because it pass 30 sec.\nI also tried to add Content-Lenght in request but no lock.\nHere is my request payload:\n\nI also opened issue at Amazon Forum, you can track it from here. Sorry for late response, I found a problem after I commented here. Problem related to javascript library to upload S3. I also commented to @cwhite92 's blog page. I think you can delete this issue. \nThanks. ",
    "holtkamp": "@kstich thanks for the swift response... did not know there was a difference between the SDK and SQS support section \ud83d\ude0a . This is not a big issue, but I hope it now appears on a certain \"radar\" at Amazon.. ",
    "jpopadak": "Has there been any progress update on this? It has been a few months now.. ",
    "Karthi-SRV": "@kstich I have deployed the code in AWS EC2 instance with load balancer. When I click on a certain link it will generate the AWS $presignedUrl url  and it will redirect to it.  Till yesterday morning it was working fine and later it expired to all the link even though max 10s difference between Server time and Expires Time.\n```\n2017-07-20T04:18:45Z\n2017-07-20T04:18:55Z\n```\nFor Fixes\nOnce I Stoped load balancer and changed my code with expires time as '+10 minutes' in one server, It worked perfect, at certain time in another server with '+5 minutes', it worked. Then I have reverted my code as default and I added all the server in load balancer and It works. But, I can't identify that issue why it occurs and how? I can't identify whether it a library issue or AWS issue. . ",
    "henry911": "It appears so, not sure how I missed it. Thanks for the information!. ",
    "Bhavna93": "How to set HOME variable ?. ",
    "kevinfilteau": "Om macOS with MAMP is set putenv('HOME=/Users/myuser') and it worked fine to load the AWS credentials.. ",
    "WongYueKeung": "using system env to store credentials is a joke..\nno one can access ur access key as long as u store them in PHP code block, PHP is naturally designed to be run safely, no need external stuff, no need extra protection\nwhen the code is executed by the interpreter, it would be oly load to RAM, if someone makes it access and view ur raw code and file, which mean ur .aws folder is exploded.\nbtw getenv() in php should not be use like this in pratice, they are many way to store those credentials very organize for container such as using Yaconf@https://github.com/laruence/yaconf\nsimply find the=> static function: ini around line 288\naws-sdk-php/src/Credentials/CredentialProvider.php \n1.remove all if stament\n2.modify this one(at the end of ini function):\nreturn Promise\\promise_for(\n                new Credentials(\n                    $data[$profile]['aws_access_key_id'],\n                    $data[$profile]['aws_secret_access_key'],\n                    $data[$profile]['aws_session_token']\n                )\n            );\nto this(replace cap text to ur keys):\nreturn Promise\\promise_for(\n                new Credentials(\n                    'YOU_ACCESS_KEY',\n                    'SERECT_KEY',\n                    null\n                )\n            );\nand done!. ",
    "ababkov": "Surely at the lowest level the ideal situation would be an extension of a common base level AWS namespaced exception rather than a built in?. @kstich thanks for the dialog / back and forth on this one - appreciate it. \nCan understand the reason for the different exception format and the value of the additional information provided in the MultipartUploadException but still think there's a lost opportunity in terms of having a catchall at the top level (even if it wouldn't be particularly accurate). For my interest, and if the information is readily available to you, are there any other situations in the SDK (apart from Multipart uploads) where an exception that doesn't extend from AwsException is thrown?. So basically if we want to catch any generic AWS exception we should just be catching for \\Exception in all cases? Not ideal, but good to know.... At any point with almost any request you could also get a CredentialsException, ParserException or CounldNotCreateChecksumException though right?. ",
    "lookitsatravis": "It looks like this is a similar problem to #1029. However, that was over a year ago and this problem still exists. Using v2.8.* solves it because it does not treat AccessDenied as a failure.. @kstich Thanks for the question. The bucket in question Access Control List has everything enabled for the owner. There is no bucket policy set. Using v2.8 of the SDK I am able to do everything I expect with my IAM creds. It's only when I use v3 that I start to have problems. . @kstich Roughly, it looks like this. Keep in mind that the only thing that is failing is doesBucketExist all other read/write operations done via the SDK succeed as expected.\n```php\n$this->s3client = \\Aws\\S3\\S3Client::factory([\n    'key' => 'my_key',\n    'secret' => 'my_secret',\n    'region' => 'us-east-1',\n    'scheme' => 'https',\n    'version' => 'latest',\n]);\n// v2.8\nfunction ensureBucketExists($bucketName)\n{\n    /\n    This check works as expected, and I realize the the true argument explicitly says 403\n    is allowed which seems to be the differentiator.\n    /\n    if (!$this->s3Client->doesBucketExist($bucketName, true)) { \n        // create bucket\n    }\n}\n// v3.latest\nfunction ensureBucketExists($bucketName)\n{\n    /\n    This check fails with 403 Forbidden, so the block is executed even when the bucket exists\n    /\n    if (!$this->s3Client->doesBucketExist($bucketName)) {\n        // create bucket\n    }\n}\n```\nI don't know enough about how the SDK and REST API work to know whether this is a configuration problem, an issue with this lib, or some kind of bug with the API. All I know at this point is that the IAM user should have full access to S3, so HeadBucket should work, but it is not despite all other operations succeeding. The reason this seems strange is that v2.8 does work, though it seems like someone explicitly added a way to skip 403 responses during HeadBucket for some reason.\nThanks for the assistance.. @kstich I double checked that, and the bucket env var is correct. I also dumped it out at the point where the client makes the doesBucketExistand $bucketName is correct. Looking at the S3Exception message, the correct bucket path is listed. Full text of that message:\n'Error executing \"HeadBucket\" on \"https://nsc-dev.s3.amazonaws.com/\"; AWS HTTP error: Client error: `HEAD https://nsc-dev.s3.amazonaws.com/` resulted in a `403 Forbidden` response  (client): 403 Forbidden (Request-ID: D0C64BE8CF22F24D) -\nI mean, it really seems like there is something wrong with the IAM credentials at this point, or something weird about how that HeadBucket request is executed. Is there any other part of the S3Exception that would aid in figuring this out?. @kstich - I gave that a shot, and there isn't really anything new that I have gleaned from the output. The signature is generated, and then the request is performed. The output of the request is indeed 403 Forbidden. Is there some specific information that can be off assistance? It seems like this is a configuration problem on my end, but there are so many potential points of failure, and I've checked the ones I know about, and they seem okay. What led me to post here instead of on the AWS forums is the fact that SDK v2 works as I would expect with the same credentials.. Thanks for the assistance @kstich.. ",
    "Mezzle": "Just realised it does... and this is a conflict on my build system, it seems. Oh, sorry, re-reading, it defines them in dev... but not in live.. Hi there,\nIn Ubuntu/Debian - SimpleXML is packaged up into a seperate package. (php-xml) - which isn't neccesarily installed by default. \nI found this out when I'd created a docker image based from this Dockerfile https://github.com/stickeeuk/docker-php/blob/master/7.1/apache/Dockerfile. ",
    "adam1010": "Thanks @kstich -- No, I'm not doing any manipulation to the URL. I've tried it a few dozen times so I don't think it's an incomplete url problem.\nI'm using PHP 5.6.25\nHere's the full code for how I initialize the S3 object:\n$s3 = \\Aws\\S3\\S3Client::factory(['version'=>'latest','region'=>'us-east-1','http'=>'/cert.pem', 'credentials'=> ['key'=>'xxx','secret'=>'xxx']])\nHere's an example of the URL it spits out:\nhttps://xyz.s3.amazonaws.com/temp.jpg?X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=XXXJHZIS4IL7K46OOAA%2F20170807%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20170807T204004Z&X-Amz-SignedHeaders=host&X-Amz-Expires=43200&X-Amz-Signature=7e916541d17ef5ba9a6380fe7146e6ab61922f4c98d8d00bf4ca5fc3816a072f1\nDo you have any other ideas or theories I could test?. I fired up the signing code in a debugger and compared the output of curl's error response (in the  section) to what the SDK is signing (the $context['creq'] variable on line 98 of src/Signature/SignatureV4.php)\nI noticed they are NOT identical.  The SDK canonical request did not urlencode the ampersands  so they appear as &  instead of &   (whereas the error output from AWS via curl has them encoded as &) -- Also, the X-Amz-Date  and X-Amz-Expires  values to not match.   Not sure if either of these are relevant but I thought I'd mention it in case there is something different about my PHP configuration causing the issue.. SOLVED -- I output the URL to a text file and grabbed it from there (instead of printing it to the page) and that fixed the issue. Thanks a lot!. ",
    "thatryan": "Well right now my code looks like this example ha, \nhttp://docs.aws.amazon.com/ses/latest/DeveloperGuide/examples-send-using-sdk.html#SDK for PHP\nWhich is great, I have it working yay. But having trouble finding examples in that format for setting MIME types, what is a boundary, all that stuff.... Here is my current code, nothing is sending for me.\nhttps://gist.github.com/thatryan/6076cb3165c559f66793745c5b9c9fa5\nI tried echo and error_log() to output something, I have a file with 777 permissions named ses_errors.log that I am trying to output to when calling this script,\nphp /mypath/ses-send-mail.php > /logs/ses_errors.log\nBut nothing writes there.\nAdditionally, every time I run this, I get the following fatal error,\nPHP Fatal error:  Class 'SimpleXMLElement' not found in /home/ubuntu/vendor/aws/aws-sdk-php/src/Api/Parser/PayloadParserTrait.php on line 39\nThank you much.. Sorry, an update. Realized the server was on PHP 5.6 so I upgraded to 7.1 and ran again, this time it output a stack trace on that error,\nPHP Fatal error:  Uncaught Error: Class 'SimpleXMLElement' not found in /home/ubuntu/vendor/aws/aws-sdk-php/src/Api/Parser/PayloadParserTrait.php:39\nStack trace:\n#0 /home/ubuntu/vendor/aws/aws-sdk-php/src/Api/ErrorParser/XmlErrorParser.php(28): Aws\\Api\\ErrorParser\\XmlErrorParser->parseXml(Object(GuzzleHttp\\Psr7\\Stream))\n#1 [internal function]: Aws\\Api\\ErrorParser\\XmlErrorParser->__invoke(Object(GuzzleHttp\\Psr7\\Response))\n#2 /home/ubuntu/vendor/aws/aws-sdk-php/src/WrappedHttpHandler.php(175): call_user_func(Object(Aws\\Api\\ErrorParser\\XmlErrorParser), Object(GuzzleHttp\\Psr7\\Response))\n#3 /home/ubuntu/vendor/aws/aws-sdk-php/src/WrappedHttpHandler.php(101): Aws\\WrappedHttpHandler->parseError(Array, Object(GuzzleHttp\\Psr7\\Request), Object(Aws\\Command), Array)\n#4 /home/ubuntu/vendor/guzzlehttp/promises/src/Promise.php(203): Aws\\WrappedHttpHandler->Aws\\{closure}(Array)\n#5 /home/ubuntu/vendor/guzzlehttp/promises/src/Promise.php(174): GuzzleHttp\\Promise\\Promise::callHandler(2, Array, Array)\n#6 /home/ubuntu/vendor/guzzlehttp/promises/sr in /home/ubuntu/vendor/aws/aws-sdk-php/src/Api/Parser/PayloadParserTrait.php on line 39. Thank you, Sorry meant to update, got the XML package installed. However error log is now saying\n\n[08-Aug-2017 19:44:49 UTC] The email was not sent. Error message: Header section too long.. Got it, thank you!. No I did not, I suck at this documentation sorry. I thought my two functions above handled assigning it, so I should use the sourceArn instead of or in addition to?. Yeah I took this function straight from the docs,\n$result = $client->sendRawEmail([\n    'Destinations' => [ $dest_email_id\n    ],\n    'RawMessage' => [\n        'Data' => $header,\n    ],\n    'Source' => $source_email_id,\n    'SourceArn' => $source_arn,\n]);\n\nThe $sourceArn variable is a string up top that I copy/pasted from the console here..\n\n. No, I removed those two operations and changed my sendRawEmail function to the one I just posted using the arn. However, still goes to spam with the same identity info as in first post.. ",
    "wjgilmore": "Sure thing. My project is using aws-sdk-php version 3.32.4. When I run the specific snippet from original message, I see:\nInvalidArgumentException: Operation not found: CreateDataSourceFromS3 in \n/home/vagrant/dev.example.com/vendor/aws/aws-sdk-php/src/AwsClient.php:215\n\nReally strange since all other APIs work perfect.. Sigh I stand corrected. The aws-sdk-php-laravel package composer.json file indicates a ~3.0 release is used, which I suppose is the source of the issue because I'd imagine an AWS SDK PHP 3.0.X release did not yet include the ML API:\n\"aws/aws-sdk-php\": \"~3.0\",\n\nI don't see a tagged 3.0.X version being available at https://github.com/aws/aws-sdk-php so I can't confirm this (I'm sure there is some way). But in the meantime I'm just going to call the AWS SDK for PHP directly from my Laravel application and move on with life.\nThank you,\nJason\n. Indeed you are correct regarding Composer and the tilde, sorry for the mixup. However, a moment ago I just realized my problem. The code snippet found in my original report was incorrect:\n$s3 = AWS::createClient('s3');\n$s3->createDataSourceFromS3Async(\n...\n);\n\nThe createDataSourceFromS3Async method is of course not available to the S3 client. It is part of the MachineLearning client. :-)\nI made this correction and everything is working perfectly. Sorry about the false alarm.\nJason\n. ",
    "danielclariondoor": "Thanks for the response. \nVersion 3.32.5\nThis does not occur when I'm uploading when a 6GB directory, but it happens every time I try to upload a 15GB directory.\nI am able to upload the directory using the CLI however.. Different files I believe. \nI'm creating my s3 client using a symfony framework service call:\n  <service id=\"iso.erc.package.manager\" class=\"Iso\\RaterBundle\\Erc\\PackageManager\">\n            <argument type=\"service\" id=\"aws.sdk3.s3\" />\n            <argument>%s3_erc_bucket%</argument>\n        </service>\nI do not know how to turn the debug option on.\nAs a workaround, I am deleting files I don't need from the directory before uploading.. Thanks. This is helpful. I will try it.. ",
    "felipedmz": "Thank you @kstich . ",
    "tomfotherby": "Thanks for replying and doing a test yourself.\nI'm not sure why we are getting different results. I'm using a older version of guzzle, I wonder if that is it:\n\"guzzlehttp/guzzle\": \"5.3.1\",\nI updated the SDK to v3.33.1 with the same results.\nI updated my issue to include versions and put them here too:\nphp -v\nPHP 7.0.17 (cli) (built: Mar 17 2017 22:49:25) ( NTS )\nCopyright (c) 1997-2017 The PHP Group\nZend Engine v3.0.0, Copyright (c) 1998-2017 Zend Technologies\n    with Zend OPcache v7.0.17, Copyright (c) 1999-2017, by Zend Technologies\n    with Xdebug v2.5.5, Copyright (c) 2002-2017, by Derick Rethans\n```\nphp -m\n[PHP Modules]\ncalendar\nCore\nctype\ncurl\ndate\ndom\nexif\nfileinfo\nfilter\nftp\ngeoip\nhash\niconv\nimagick\njson\nlibxml\nmbstring\nmcrypt\nmemcached\nmongodb\nmysqlnd\nnewrelic\nOAuth\nopenssl\npcntl\npcre\nPDO\npdo_mysql\npdo_sqlite\nPhar\nposix\nreadline\nReflection\nsession\nSimpleXML\nsoap\nsolr\nSPL\nsqlite3\nstandard\ntokenizer\nxdebug\nxml\nxmlreader\nxmlwriter\nZend OPcache\nzip\nzlib\n[Zend Modules]\nXdebug\nZend OPcache\n``\n. I can confirm when I upgradeguzzlehttp/guzzlefrom5.3.1to6.3.0the problem goes away -\n i.e.NoSuchKeyis returned from thegetAwsErrorCode()function call when theSaveAsparam is used ingetObject()`.\nUnfortunately, I cannot upgrade guzzle in my app at this time so I'm stuck with this bug.\nFeel free to close this Issue if you deem it unimportant.. Your fix solved my issue, thank you, looking forward to it being merged.. I have tried this fix out and it is useful for me. Thank you for your work and time.. ",
    "jaredhabeck": "I'll take this to stack overflow first. . You were right! Wow. I've been banging my head against this for a while, I swear I had the argument arranged the way you suggested at some point.\nCould this be considered an SDK issue if there isn't an exception for the format I composed the argument in? Did it just send the call into no man's land? That's kind of what it looked like.\nThanks for the super fast response! Saved me a good bit of code.. Thanks again for your prompt help, @ksitch. . ",
    "afust-pica9": "@imshashank see snippet. They about 10~ 15megabytes\n```php\nuse GuzzleHttp\\Psr7;\nuse League\\Flysystem\\File;\nuse Pica9\\Core\\Filesystem\\FileLocator;\nuse League\\Flysystem\\FilesystemInterface;\n//omited...\n    public static function download($remoteFile, $localFile, FilesystemInterface $filesystem)\n    { \n        $remote = Psr7\\try_fopen($remoteFile, 'rb');\n    $filesystem->putStream($localFile, $remote);\n    if (is_resource($remote)) {\n        fclose($remote);\n    }\n}\n\n```\nI believe though it might be related to cURL curl google storage\nFor now I rebooted my php-fpm and changed cUrl to \ncurl 7.40.0 (x86_64-redhat-linux-gnu) libcurl/7.40.0 NSS/3.21 Basic ECC zlib/1.2.8 libidn/1.18 libssh2/1.4.2\n. @imshashank sure. \nFlysytem wraps aws; I dont call it directly.  flysystem\nHere is the relevant code from flystem:\nhttps://github.com/thephpleague/flysystem/blob/master/src/Filesystem.php#L108\nwhich calls the AWS Adapter, which is the included as a dependency aws\ncomoser.json looks like this\n`\n\"league/flysystem\": \"^1.0\",\n\"league/flysystem-aws-s3-v3\": \"^1.0\"\n. @kstich using 3.35.2. \n. @kstich I'll get you this: Can you retrieve the contents of $filesystem->getConfig(). @kstich \nHere is the  pattern that I'm seeing: \nWe are running php-fpm; When we restart php-fpm, uploads are fine for a period of time. \nWe are now using sdk 3.35.2. But after a while, the failures start (a few days). It seems that something is not cleaning up correctly or/and get corrupted. \nI'm wondering if I should pass a curl CURLOPT_FORBID_REUSE to close the connection. \nI looked at our logs and all the failures wrt to uploads stared just about the sdk 3.33.1 Aug 15. \nI don't believe we made substantial changes on our side. . @kstich  here is the config. Nothing really: \nobject(League\\Flysystem\\Config)#510 (2) {\n  [\"settings\":protected]=>\n  array(0) {\n  }\n  [\"fallback\":protected]=>\n  NULL\n}\nWhat seems to solve it is 'rebooting' php-fpm. I set a max requests  a child is allowed to hadle. Php-fpm re spawn a child (essentially 'cleaning' any issues in the 'current' child). . ",
    "fjmoralesp": "Hi!!\nWe have the same issue here, this code shows how is flysystem-aws-s3-v3 using asw php sdk:\n```\n/\n     * Upload an object.\n     \n     * @param        $path\n     * @param        $body\n     * @param Config $config\n     \n     * @return array\n     */\n    protected function upload($path, $body, Config $config)\n    {\n        $key = $this->applyPathPrefix($path);\n        $options = $this->getOptionsFromConfig($config);\n        $acl = array_key_exists('ACL', $options) ? $options['ACL'] : 'private';\n    if ( ! isset($options['ContentType'])) {\n        $options['ContentType'] = Util::guessMimeType($path, $body);\n    }\n\n    if ( ! isset($options['ContentLength'])) {\n        $options['ContentLength'] = is_string($body) ? Util::contentSize($body) : Util::getStreamSize($body);\n    }\n\n    if ($options['ContentLength'] === null) {\n        unset($options['ContentLength']);\n    }\n\n    $this->s3Client->upload($this->bucket, $key, $body, $acl, ['params' => $options]);\n\n    return $this->normalizeResponse($options, $key);\n}\n\n```\nThe S3 Client is instanced in the constructor like this:\n/**\n     * Constructor.\n     *\n     * @param S3Client $client\n     * @param string   $bucket\n     * @param string   $prefix\n     * @param array    $options\n     */\n    public function __construct(S3Client $client, $bucket, $prefix = '', array $options = [])\n    {\n        $this->s3Client = $client;\n        $this->bucket = $bucket;\n        $this->setPathPrefix($prefix);\n        $this->options = $options;\n    }\nwhich is loaded before the class:\nuse Aws\\S3\\S3Client;\n. Sure:\nleague/flysystem version 1.0.40\naws/aws-sdk-php version 3.31.2\nleague/flysystem-aws-s3-v3 version 1.0.18\nExample:\n```\n<?php\nnamespace MyProject\\Reports;\nuse PublicDisk;\nuse TemporalDisk;\nclass class DataSerializer\n{\n    public function movetoAws($myFile)\n    {\n        $path = 'app/myfiles';\n        $fileName = 'myTestFile';\n    PublicDisk::put($path . $fileName, fopen($myFile, 'r'));\n    TemporalDisk::delete($myFile);\n\n    return $path . $fileName;\n}\n\n}\n``PublicDiskandTemporalDiskare just fasade of LaravelIlluminate\\Filesystem\\FilesystemAdapter`. Thanks!!. ",
    "nexik": "Thanks,\nI will create ticket on AWS Support. ",
    "HowdyMedia": "I see that the server instance was tripping on \"yield\" while using PHP version 5.4.34. Generators became available in PHP 5.5.0, so it is a requirement to use the latest version of the API.. Beat me to it kstich, Didn't respond soon enough.... ",
    "tiran133": "I have the same problem. All my multipart uploads end up having application/octet-stream as Content-Type.\nRegardless if I set the param \"ContentType\"\nAny solution on this?\nHelp is appreciated.\nAlex\n. Sure.\nI read the file and store the content in a variable and pass it to flysysteme with a destination path, which then calls the upload function on the S3Client.\nhttps://github.com/aws/aws-sdk-php/blob/2d0508ddeac51bbb0e0f86465db6b7ac50a597fb/src/S3/S3ClientTrait.php#L24-L34\nWhich creates the ObjectUploader which creates the stream here\nhttps://github.com/aws/aws-sdk-php/blob/c0321509cccc20e6f0051ce9e36369e465388029/src/S3/ObjectUploader.php#L57\nThe body is then passed to the MultipartUploader which is the source.\nHope that helps.\nLet me know if you need anything else.\n. its a simple file_get_contents() to a path on the local drive.\n. ",
    "devopsberlin": "@kstich, the getItem operation throwing the error.. ",
    "gatsbyz": "@kstich I thought the \"RANGE\" key could be omitted for queries.. ",
    "arijitdas7": "use Aws\\S3\\S3Client;\n// set the arguments properly        \n$option_ar = array(\n            'version'   => 'latest',\n            'region'    => 'eu-west-1',\n            'credentials' => array(\n                'key'    => awsAccessKey,\n                'secret' => awsSecretKey,\n            ),\n            'use_accelerate_endpoint' => true // before use transfer accelerator, you must enabled this from your bucket properties.\n        );\n        $s3 = new S3Client($option_ar);. ",
    "AdrianAntunez": "Hi @kstich,\nThanks for the fast reply. The methods you pointed out are all EC2 related but what about the interaction with other services like S3, firehose, SQS... These are the ones I am interested in but I couldn't find any dryrun implementation in those services.\nRegards,\nAdrian. In fact most S3 commands have the dryrun option as it is pointed out here: https://docs.aws.amazon.com/cli/latest/reference/s3/cp.html so I am wondering why it has not been implemented in aws-sdk-php. ",
    "kapendat": "I found the most up to date of course running \nphp composer.phar require aws/aws-sdk-php .\nso disregard my request.\nthanks, \n-K. ",
    "Finesse": "I see that I can use an alternative HTTP client either via the AWS SDK handler param or Guzzle handler. The problem is that the AWS SDK makes me download Guzzle even if I don't use it. Many other libraries request different HTTP clients which leads to a bunch of HTTP clients in my vendor folder.\nChoosing Guzzle as a HTTP client is a business decision. But the AWS SDK is a library, and by definition, a library aims to be plugged to a business project, it can't be used by its own. Developers of business projects make business decision, library authors must not make the decision for them. So libraries need be as abstract as possible.\nIt's true that Guzzle is the de-facto HTTP client for PHP and that it is extendable, but it became the first choice before the rise of HTTPlug, which is going to become a PSR standard. With PHPPlug developers will have the business decision about the implementation in their hands: a Zend project may use Diactoros, a micro-framework project may use Buzz, any other framework may use Guzzle and so on.\nThere are some benefits for the AWS SDK developers. HTTPlug is a very stable project because it is an interface, that's why it will not have as many major versions as Guzzle has, so you won't need to update the AWS SDK for a new HTTP client version so often. And you won't need to support several versions of HTTP clients simultaneously like you do with Guzzle 5 and 6.. You use some Guzzle features which are not provided by HTTPlug (as a HTTP client implementation), don't you? If you do, how can I switch Guzzle to other implementation?\n\nSo this is where things are a bit nuanced -- Guzzle both has a client and implements PSR-7 messages. Perhaps you care more about the HTTP message implementation?\n\nI am talking about HTTP client implementation, not PSR-7 messages implementation.. I got you point. Thank you for the explanation. Will be watching the destiny of the PSR HTTP client.. I don't like that I can't set the client side monitoring (CSM) settings through the PHP code too. I don't want my application to read something beyond its directory because I don't want an implicit data flow so I will not add the home directory to the open_basedir.\nLuckily there are a few ways to disable CSM using only the application source code:\n\n\nAdd the following line of code before creating the S3Client object:\nphp\nputenv('AWS_CSM_ENABLED=false');\n\n\nGive the S3Client constructor a fake CSM cache object that returns what you want:\nphp\n$s3Client = new Aws\\S3\\S3Client([\n    // ...\n    'csm' => new class implements Aws\\CacheInterface {\n        public function get($key) {\n            return new Aws\\ClientSideMonitoring\\Configuration(false, 0); // Your configuration here\n        }\n        public function set($key, $value, $ttl = 0) {}\n        public function remove($key) {}\n    }\n]);\nThis is a dirty way but with no side effects.\n\n\nI believe the SDK needs a legit way to pass the CSM params through the constructor so that if they are presented, the SDK won't scan the disk.. ",
    "JefferyHus": "Well this is still happening with me. The fix seems to no work.\nThis is my code\nphp\n        $s3->upload(...['params' => [\n            'CacheControl' => 'max-age=2592000',\n            'ContentType' => mime_content_type($file)\n        ]]);. ",
    "rgarcia-martin": "Thanks @kstich , i will try that options but... What do you think about the posibility of use a header that fill the mobile app when upload the file without sign the content (the server generate the URL and send it to the mobile app without know any info from the file, then the mobile app launch the PUT request)?\nThank you again! :D. ",
    "imaimai86": "Any updates on this issue will be appreciated. ",
    "MNV": "I have the same issue with checksum providing ContentSha256 param in command for creating pre-signed URL. I think the problem is in getPresignedPayload() method (https://github.com/jeskew/aws-sdk-php/blob/4e29e28494f8dc69c2ae1b34a0a658e8ed9537fe/src/Signature/S3SignatureV4.php#L53) which always returns UNSIGNED-PAYLOAD that doesn't match real body content of the uploading file.. In this issue, there is a discussion (https://github.com/aws/aws-sdk-php/issues/1392#issuecomment-336573521) about providing ContentSha256 hash generating pre-signed URL to allow the server to know what they are going to upload.\nI would like to pass the checksum to the server to generate the URL for uploading and check that client tries to upload the same file for which pre-signed URL was given (https://github.com/aws/aws-sdk-php/issues/1392#issuecomment-336945352).. @kstich Have you planned to explore this problem? Do you need any extra information and details?. ",
    "Flythe": "Ran into the same problem a couple of days ago.\nSnippet:\n```\n$results = $s3->getPaginator('ListObjects', [\n     'Bucket' => 'bucket',\n     'Prefix' => 'prefix/',\n     'Marker' => 'marker'\n]);\n$key_arr = [];\nforeach ($results->search('Contents[].Key') as $key) {\n     $key_arr[] = $key;\n}\necho count(array_unique($key_arr));\necho count($key_arr);\n```. ",
    "aas395": "Hi Kevin,\nThank you so much for your quick response. It turns out this was an issue on my end. I had looked in the appropriate Guzzle file before submitting this to you, but I had misunderstood what the \"sink\" represented.\nThanks again!. ",
    "antonioribeiro": "It's all happening inside the AWS sdk, so there's no \"I can find the URL at...\", but debugging it further we can see that the effectiveUri is coming to the sdk exactly the way I'm getting it in the \"outside\": \n\nThis is fromAws\\WrappedHttpHandler::parseResponse().\n. This is how it's been executed:\n\nI'm adding the bucket to the path because, when I don't, I get an URL back without it:\n\n. I've tried it before. Not including the bucket name in the key results in the same: bucket being added as subdomain of s3, but not added to the URL path:\n\nThis is how I'm testing it now:\nphp\nRoute::get('/backup/get', function () {\n    $bucket = env('AWS_BUCKET'); // = pragmarx\n    $key = 'backup/databases/most-recent/[removed].backup.sql.gz';\n    try {\n        //Create a S3Client\n        $s3Client = new S3Client([\n            'region' => env('AWS_REGION'), // us-east-1\n            'version' => '2006-03-01',\n            'credentials' => CredentialProvider::env()\n        ]);\n        $result = $s3Client->getObject([\n            'Bucket'     => $bucket,\n            'Key'        => $key,\n        ]);\n    } catch (S3Exception $e) {\n        echo $e->getMessage() . \"\\n\";\n    }\n});\n. ",
    "luukau": "Thanks @kstich.\nSo the CakePHP CLI calls the following:\n```\n    public function popAll()\n    {\n        $this->QueueWorker->addFunction('testQueue1', function ($item) {\n            if ($this->params['verbose']) {\n                debug($item);\n            }            \n        return true; // return true to delete the message upon processing, false to leave the message in the queue\n    });\n    $this->QueueWorker->work();\n}\n\n```\nwhich in turn calls the 'work' function\n```\n    public function work($name = 'default', $iterations = self::MAX_MESSAGES_PROCESSED)\n    {\n        $this->log(sprintf(\"Starting %s worker\", $name), 'info', 'sqs');\n        $simpleQueue = $this->getSimpleQueue();\n        $i = 0;\n        while ($i < $iterations) {\n            foreach ($this->callbacks as $queue => $callback) {\n                if (!$this->_triggerEvent('Queue.beforeWork')) {\n                    break 2;\n                }\n            $job = $simpleQueue->receiveMessage($queue);\n            if (!empty($job) && $job->get('Messages')) {\n                $this->_work($queue, $job);\n            }\n\n            if (!$this->_triggerEvent('Queue.afterWork')) {\n                break 2;\n            }\n        }\n        if ($iterations > -1) {\n            $i++;\n        }\n    }\n}\n\n(part of the cakephp plugin https://github.com/lorenzo/cakephp-sqs) with the SimpleQueue class implementing:\n    public function receiveMessage($taskName)\n    {\n        $url = $this->queueUrl($taskName);\n    return $this->client()->receiveMessage(['QueueUrl' => $url]);\n}\n\n```\nAnd 'client' being an instance of the AWS SqsClient.\n. Any ideas @kstich?. From some strange reason the intermittent errors no longer appear. Still using the same components.\n- PHP 7\n- CakePHP 3.5.4, using https://github.com/lorenzo/cakephp-sqs\n- Guzzle 6.3.0\n- curl 7.47.0 (ubuntu 16.04)\n- OpenSSL 1.0.2g\nAlthough AWS PHP SDK is version - 3.36.34\nNot sure if this newer SDK version has fixed the problem, or something else has changed (ie local machines, cloud backend). Anyway I can't replicate issue anymore so I would say it is safe to close. \nI will keep your suggestions in mind in case it pops up again (and will report back). Thanks. So I am getting similar errors again in our production fifo queue, albeit only on the \"ReceiveMessage\".\nI haven't be able to establish an error pattern, other then it seems to occur randomly averaging at 4-5 times a month. \nCurrent s/w components:\n- PHP 7\n- CakePHP 3.5.17\n- Guzzle 6.3.3\n- AWS PHP SDK 3.82.6\n- openSSL 1.02g\n- curl 7.47.0\nError message is as follows:\nError: [Aws\\Sqs\\Exception\\SqsException] Error executing \"ReceiveMessage\" on \"https://sqs.us-west-2.amazonaws.com/.../....fifo\"; AWS HTTP error: Error creating resource: [message] fopen(): SSL: Handshake timed out\n[file] /opt/edi-handler/vendor/guzzlehttp/guzzle/src/Handler/StreamHandler.php\n[line] 323\n[message] fopen(): Failed to enable crypto\n[file] /opt/edi-handler/vendor/guzzlehttp/guzzle/src/Handler/StreamHandler.php\n[line] 323\n[message] fopen(https://sqs.us-west-2.amazonaws.com/.../....fifo): failed to open stream: operation failed\n[file] /opt/edi-handler/vendor/guzzlehttp/guzzle/src/Handler/StreamHandler.php\n[line] 323 in /..../vendor/aws/aws-sdk-php/src/WrappedHttpHandler.php on line 191\nI will update the version of curl on our production server, as per @kstich suggestion and will report back as soon as I can.. ",
    "vijay-webners": ":+1: . ",
    "m4tthumphrey": "The bucket name is set correctly. What's odd is the fact that the URL is changing.\nhttps://s3-eu-west-1.amazonaws.com/my-bucket/hello.txt is the correct URL but the client is looking for https://my-bucket.s3.eu-west-1.amazonaws.com/hello.txt. ",
    "richqi": "SDK version: 3.36.15\nSample Code:\n```php\n$rdsClient = new RdsClient([\n                'region' => 'us-west-2',\n                'version' => 'latest',\n                'credentials' => [\n                    'key' => 'xxx',\n                    'secret' => 'xxx',\n                ],\n ]);\ntry {\n    $result = $rdsClient->createDBInstanceReadReplica([\n        'AutoMinorVersionUpgrade' => 1,\n        'AvailabilityZone' => 'us-west-2a',\n        'CopyTagsToSnapshot' => 0,\n        'DBInstanceClass' => 'db.t2.micro',\n        'DBInstanceIdentifier' => 'test-db-1-rr', // REQUIRED\n        'EnableIAMDatabaseAuthentication' => 0,\n        'PubliclyAccessible' => 0,\n        'SourceDBInstanceIdentifier' => 'test-db-1-rw', // REQUIRED\n        'StorageType' => 'standard',\n    ]);\nreturn $result;\n\n} catch (\\Exception $e) {\n    return $e->getMessage();\n}\n```\nI removed \"CreateDBInstanceReadReplica\" from PresignUrlMiddleware::wrap in RdsClient.php to make it work.. ",
    "Freid001": "SDK: 3.30.4\ncURL: 7.47.0\nOpenSSL: 1.0.2g\nPHP: 7.0.22. ",
    "shangdev": "In fact, by restarting PHP-FPM, this problem may be solved!\n# service php-fpm restart. \ud83d\udc4c\uff0cThanks.. ",
    "axettone": "Tried with 3.30, it works.. Thank you kstich! I re-installed via composer and now I have 3.36.37 that works! I didn't remember the specific version I tried 2 days ago. I don't know what to say... :-(\nI'm sorry!. ",
    "jellis": "Hi all - just so everybody is aware. This is a known bug with the AWS API. They've recommended I download and install the RDS CLI to download the log files.\nThere is no known date for when the problem will be addressed.. As an aside, @kstich though - is it possible the http method mentioned in these docs has been implemented? It refers to the following\nGET /v13/downloadCompleteLogFile/sample-sql/log/ERROR.6\nhttp://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/RESTReference.html#RESTReference.DownloadCompleteDBLogFile.CLIversion. I had a ticket lodged with AWS support. I was advised not to bother using the AWS CLI/SDK's for downloading logs as they're aware there are issues with it. They advised to use the RDS CLI - which I hated.\nInstead, I hacked around the SDK and leveraged the signing method to generate a request to the endpoint documented on that link. See below.\n```\n<?php\nnamespace DbAudit\\Services\\Aws\\Rds;\nuse GuzzleHttp\\Client;\nuse GuzzleHttp\\Psr7\\Request;\nuse Aws\\Signature\\SignatureV4;\nuse Aws\\Credentials\\Credentials;\nclass Logs\n{\n/**\n * @var string\n */\nprotected $service = 'rds';\n\n/**\n * @var SignatureV4\n */\nprotected $signature;\n\n/**\n * @var Client\n */\nprotected $client;\n\n/**\n * @var Request\n */\nprotected $request;\n\n/**\n * @var string\n */\nprotected $region;\n\n/**\n * @var string\n */\nprotected $url = 'https://{service}.{region}.amazonaws.com/v13/downloadCompleteLogFile/{db_instance_identifier}/{log_file}';\n\n/**\n * Build up all the dependencies\n * A little complex, but gives a nice interface\n */\npublic function __construct()\n{\n    $this->region = config('aws.region');\n    $this->client = new Client($this->getParams());\n    $this->signature = new SignatureV4($this->service, $this->region);\n    $this->credentials = new Credentials(config('aws.credentials.key'), config('aws.credentials.secret'));\n}\n\n/**\n * Build up the full URL with variable substitution\n *\n * @param $db_instance_identifier\n * @param $log_file\n *\n * @return mixed\n */\nprotected function getUrl($db_instance_identifier, $log_file)\n{\n    return str_replace(\n        ['{service}', '{region}', '{db_instance_identifier}', '{log_file}'],\n        [$this->service, $this->region, $db_instance_identifier, $log_file],\n        $this->url\n    );\n}\n\n/**\n * The external method for retrieving the log file\n *\n * @param $db_instance_identifier\n * @param $log_file\n *\n * @return bool|string\n */\npublic function getLog($db_instance_identifier, $log_file)\n{\n    $this->request = new Request('GET', $this->getUrl($db_instance_identifier, $log_file), [\n        'Content-type' => 'application/json'\n    ]);\n\n    return $this->retrieveLogFile();\n}\n\n/**\n * Internal method for firing off the request and returning what's in the\n * response (if it exists)\n * @TODO: Download file straight to disk\n *\n * @return bool|string\n */\nprotected function retrieveLogFile()\n{\n    $response = $this->client->send($this->signature->signRequest($this->request, $this->credentials));\n\n    return $response->getStatusCode() == 200\n        ? $response->getBody()->getContents()\n        : false;\n}\n\n/**\n * Configure proxy information if required\n *\n * @return array\n */\nprotected function getParams()\n{\n    $params = [];\n\n    if (!empty(env('HTTP_PROXY')) && !empty(env('HTTPS_PROXY')) && !empty(env('NO_PROXY'))) {\n        $params['proxy']['http_proxy'] = env('HTTP_PROXY');\n        $params['proxy']['https_proxy'] = env('HTTPS_PROXY');\n        $params['proxy']['no_proxy'] = env('NO_PROXY');\n    }\n\n    return $params;\n}\n\n}\n```\nI hope this helps somebody else at some point.. ",
    "carusogabriel": "@kstich Actually, I didn\u2019t know that was a previous version of PHPUnit 5 that support this new namespace. Do you want me to down to 5.4.3?. Done :+1: . @kstich I've amended the commit and applied requested changes!. @kstich I've amended the first commit and applied your requested changes. Just one comment about annotations that I couldn't understand.\nAlso, I've made another one to alleviate some returns. \nThanks for your review.. I guess now everything is right. Sorry for the inconvenient.. Done. Good catch, gonna change it :+1: . As far as I know, annotations do not get benefited by uses. Am I wrong?. No problem :smile: . ",
    "DeepDiver1975": "Can I please get feedback on this? Disliked? Do I need to change anything? THX\n@awstools @kstich . > There's an integration suite of tests available for the StreamWrapper that should be updated to verify this functionality alongside the old.\n@kstich Please let me know how you want this to be add to the feature. THX. @kstich please review my proposal regarding the integration test. I still need to get the behat suite to run and see if it works ....... oooops. ",
    "patrickjahns": "@kstich @DeepDiver1975 - any progress here?. What does this require to be merged? . ",
    "samusenkoiv": "FYI\nThe following sample works totally fine:\n$body = EntityBody::factory(fopen('php://input', 'r+'));\n$result = $this->client->putObject(\n    [\n        'Bucket' => $this->bucket,\n        'Body' => $body,\n        'Key' => $file->getPath(),\n        'ACL' => 'public-read',\n        'ContentType' => 'application/octet-stream',\n    ]\n);\n. Why do you ask?\nIf your concern is about 'ContentLength', then this is not the issue. The ContentLength has the right value (same value I get in $result from aws) , I double checked that.. ",
    "jshrek": "@kstich \nNo NextMarker is not in the result either (see below). Do I need to include Marker in the original request to get a NextMarker back?\n```\nAws\\Result Object\n(\n    [data:Aws\\Result:private] => Array\n        (\n            [IsTruncated] => 1\n            [Marker] => \n            [Contents] => Array\n                (\n                    [0] => Array\n                        (\n                            [Key] => Some File Name.mp4\n                            [LastModified] => Aws\\Api\\DateTimeResult Object\n                                (\n                                    [date] => 2017-11-25 19:22:11.000000\n                                    [timezone_type] => 2\n                                    [timezone] => Z\n                                )\n                        [ETag] => \"blablabla\"\n                        [Size] => 69254403\n                        [StorageClass] => STANDARD\n                        [Owner] => Array\n                            (\n                                [DisplayName] => webmaster\n                                [ID] => blablabla\n                            )\n\n                    )\n\n            )\n\n        [Name] => my-bucket-name\n        [Prefix] => \n        [MaxKeys] => 1\n        [EncodingType] => url\n        [@metadata] => Array\n            (\n                [statusCode] => 200\n                [effectiveUri] => https://my-bucket-name.s3.us-west-1.amazonaws.com/?max-keys=1&encoding-type=url\n                [headers] => Array\n                    (\n                        [x-amz-id-2] => blablabla\n                        [x-amz-request-id] => blablabla\n                        [date] => Sat, 25 Nov 2017 20:25:14 GMT\n                        [x-amz-bucket-region] => us-west-1\n                        [content-type] => application/xml\n                        [transfer-encoding] => chunked\n                        [server] => AmazonS3\n                    )\n\n                [transferStats] => Array\n                    (\n                        [http] => Array\n                            (\n                                [0] => Array\n                                    (\n                                    )\n\n                            )\n\n                    )\n\n            )\n\n    )\n\n). As a side note, the documentation is confusing as nothing I have read so far ever suggested to me that there was a different command other than listObjects. This first link specifically references listObjects only:\nhttp://docs.aws.amazon.com/AmazonS3/latest/dev/ListingObjectKeysUsingPHP.html\nThis second link makes mention of using v2 of listObjects but I just assumed that when I use the new SDK I am getting the v2 of listObjects. How would anybody know that there is actually a different command listObjectsv2 to use, when it is not even mentioned on the page anywhere:\nhttp://docs.aws.amazon.com/AmazonS3/latest/API/v2-RESTBucketGET.html. Using listObjectsV2 instead of listObjects does correctly return NextContinuationToken.\nSo the only issue seems to be that listObjects does not return NextMarker.. ",
    "byterussian": "Forget it, the problem was a conflict with codeception outdated lib. . ",
    "yellowmamba": "Somehow I landed in an outdated document. Bumped it to 3.* and it works.. ",
    "polothy": "\ndo you mean the object does not exist locally or on S3?\n\nYes, the object does not exist in S3.  Sorry, updated the description.\nExample API calls:\nFor the getObject call:\nphp\n$args = ['region' => 'us-east-2']; // For us, credentials could be key/secret, empty (for IAM) or an instance of \\Aws\\CacheInterface\n$s3 = new \\Aws\\S3\\S3Client($args);\n$s3->getObject([\n    'Bucket' => 'my-bucket',\n    'Key'    => 'my-key-that-does-not-exist-in-s3',\n    'SaveAs' => '/path/to/file/on/nfs/mount',\n]);\nFor the upload call, the file being uploaded must be large enough to trigger a multipart upload:\n```php\n$args = ['region' => 'us-east-2']; // Credentials could be key/secret, empty (for IAM) or an instance of \\Aws\\CacheInterface\n$s3 = new \\Aws\\S3\\S3Client($args);\n$resource = fopen('/path/to/file/on/nfs/mount', 'rb');\nif ($resource === false) {\n    throw new \\Exception('Failed to open file');\n}\n$stream = \\GuzzleHttp\\Psr7\\stream_for($resource);\ntry {\n    $s3->upload('my-bucket', 'my-key', $stream);\n} catch (\\Aws\\S3\\Exception\\S3Exception $e) {\n    $stream->close();\n    throw $e;\n}\n$stream->close();\n```\nIn addition, remember that in order to see the error, you must attempt to delete the directory on the NFS mount in which the above files live.  The code for file deletion can be found here and the specific line that causes the error is line 126.\nPlease let me know if anything is unclear or if more information is needed, thanks!. * PHP: 7.1.10\n AWS SDK: 3.19.26\n guzzlehttp/guzzle: 6.2.2\n guzzlehttp/psr7: 1.3.1. If I make the following changes:\n ls -a instad of ls\n* Then attempt to delete the directory at the end\n```php\n$client = $sdk->createS3();\ntry {\n    $client->getObject([\n        'Bucket' => 'bucket',\n        'Key' => 'filethatdoesntexist.txt',\n        'SaveAs' => '/nfsmountdir/filethatdoesntexist.txt'\n    ]);\n} catch (Exception $e) {}\n$output = [];\nexec('ls -a /nfsmountdir', $output);\necho PHP_EOL . json_encode($output); // File exists in the directory\nunlink('/nfsmountdir/filethatdoesntexist.txt');\n$output = [];\nexec('ls -a /nfsmountdir', $output);\necho PHP_EOL . json_encode($output); // File no longer exists\nrmdir('/nfsmountdir');\n```\nI get this for the output:\n[\".\",\"..\",\"filethatdoesntexist.txt\"]\n[\".\",\"..\",\".nfs0000000001e099df00000008\"]\nWarning: rmdir(/nfsmountdir): Directory not empty in /path/to/s3test.php on line 46\nIt's interesting to note that the .nsf file didn't appear until the file was deleted, didn't know that was the case.\nWhile testing, I was using my Vagrant mount, so was /vagrant/nfsmountdir.  Using default mounting options in our Vagrantfile: config.vm.synced_folder \".\", \"/vagrant\", type: \"nfs\". Also, I'd like to add, I didn't file this issue to fix my Vagrant/dev environment :)  It can just be reproduced there.  We actually saw these errors in AWS where we are NFS mounting EBS to our web nodes.. Hrm, I could reach out to our account support, see if we can bring in someone from EBS.  I can try to do that next week.\nIn the meantime, maybe it has something to do with Ubuntu?  Our Vagrant and web nodes are running Ubuntu.  You can try it yourself with https://github.com/moodlerooms/moodle-vagrant-example - follow the steps in Vagrant install section to get it setup.  It mentions installing specific versions of Vagrant/VirtualBox, but you can likely install latest versions if you have them already (we do this because in the past, they randomly break each other).  I know this is asking a lot for reproducing a bug, so no worries if you don't have time for pursuing this route.. No, sorry, I haven't been able to.. ",
    "joshlopes": "I'm sorry, i thought i was not using the guzzle http stream, but i seem to be using it on one of my tests. \nBut the problem is basically that if i try to \"new PsrStream()\" from the aws vendor and i don't include the guzzlehttp/streams on my composer it will fail, because one dependency of PsrStream is the guzzlehttp/streams (DecoratorTrait). \nIf that makes sense.. ",
    "philiplb": "Hi,\nthank you for the reply.\nYes, we are using composer and the solution was to downgrade to Guzzle 5.3.1. First I thought your version settings at https://github.com/aws/aws-sdk-php/blob/master/composer.json#L20 would exclude 6.3, but actually they shouldn't. >= 6.2.1, < 7.0.0 should be fine with 6.3. Hm, will have to reinvestigate why just requiring 6.3 didn't work out.. ",
    "PauloMigAlmeida": "@kstich Thanks for reviewing this PR. I have done the changes pointed out in your code review. \nLet me know if you need any additional change before merging it into master.\nBest regards,\nPaulo Almeida. Hi @kstich,\nIntroduction pointed out added to the Required IAM permissions section. Let me know if you need anything else.\nBest regards,\nPaulo Almeida. Sure! Just did it.. Sure thing. ",
    "ucu2009": "that worked!,\nthank you. ",
    "MEGApixel23": "Looks like this issue is related to \"phpunit/phpunit\" v.6.2.2 package. I'll try to lower its version. Thanks for a quick response.. ",
    "plozmun": "Settings must be capitalize. ",
    "th4deu": "Thank you for your reply, @kstich .\nAlso, I've done these steps you detailed. Even using the \"sendBulkTemplatedEmail\" method, it continues requiring the TemplateData info.\nCan you try it and send me a working case of use, please? \nI've tried almost all of the possibilities I had in mind and it keeps returning me an error with this message (attached to this post)\n\nThanks!. Alright.\nHere's the array I'm passing into the method:\n```\n$array = [\n        'Source' => 'my_email@gmail.com',\n        'Template' => 'Template1', \n        'ConfigurationSetName' => 'EnvioTeste',\n        'Destinations' => [\n          [\n            'Destination' => [\n                'ToAddresses' => ['my_email@gmail.com'],\n                'ReplacementTemplateData'=>[['Name'=>'input1', 'Value'=>'My Name'],['Name'=>'input2', 'Value'=>'My Last Name']],\n            ],\n          ],\n      ],\n      // 'TemplateData' => 'test', // This is where the error happens. It requires this parameter even for bulk templated e-mail, and even sending this parameter in the array, it doesn't work.\n      'DefaultTemplateData' => 'test',\n  ];\n\n```\n. @kstich, thanks for your attention but I'm still not getting it.\nI've placed the ReplacementTemplateData outside Destination (but still inside Destinations) and I did the same with the DefaultTemplateData, placing it outside Destinations and passing the Json value (I did it before, just replaced the text for when pasted it here).\nHere's with the data, can you please help me to see what's wrong? I still get the \"TemplateData is invalid\" message.\n```\n$array = [\n        'Source' => 'my_email@gmail.com',\n        'Template' => 'Template1', \n        'ConfigurationSetName' => 'EnvioTeste',\n        'DefaultTemplateData' => json_encode([['Name'=>'nome', 'Value'=>'Thadeu'], ['Name'=>'fulano', 'Value'=>'A.']]), \n        'Destinations' => [\n          [\n            'Destination' => [\n                'ToAddresses' => ['my_email@gmail.com'],\n            ],\n        'ReplacementTemplateData'=> json_encode([['Name'=>'nome', 'Value'=>'Thadeu'], ['Name'=>'fulano', 'Value'=>'A']]), \n      ],\n\n      ],\n\n  ];\n\n    $result = $client->sendBulkTemplatedEmail($array);\n\n```\nBy the way I've passed the replacement tag's names as key in the array too, and the error is the same.. It worked now! Thank you very much.\nI've tested it before but I think it didn't work because the ReplacemenetTemplateData was inside Destination.\nThanks!. ",
    "bferrantino": "Hi @kstich , Thank you for your response. I have confirmed that I am using a 32-bit version of PHP (on a 64-bit Windows OS). (echo PHP_INT_MAX; returns 4).\nPlease let me know if you need any further information from me.\nThank you\nBrian\nPS: I saw there was an issue referenced, and that it looks to be pending review/merge. Do you have any idea on when that would be reflected? Or if there is a way I can change it locally to at least use the updated Long type. Any help is appreciated. Thanks!. I was actually able to change it locally, and it is working for me within my application. Thank you for your investigation and assistance with this. Please inform when the change is committed and I will pull in your latest code revisions through that date.\nThanks again\nBrian. ",
    "leledumbo": "I updated to v3 and the issue is gone, damn.. ",
    "xthiago": "@kstich is correct. This PR is a breaking change for those that uses strict_types=1 and rely on the type of fields. \ud83d\ude1e \nI would like to suggest the release of changes like that as major in the future. . ",
    "smartinec": "I was just suggesting that it may be the right time to add support since the services now support IAM. Why not add support to have a complete offering?. ",
    "MatthewBooth": "Hi\nI am using 3.48.0 exactly.\nThere are no permission changes on the IAM role I'm using. It has the following policy attached:\njson\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"Stmt1420044805001\",\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"s3:ListBucket\",\n                \"s3:GetObject\",\n                \"s3:GetObjectAcl\",\n                \"s3:PutObject\",\n                \"s3:PutObjectAcl\",\n                \"s3:ReplicateObject\",\n                \"s3:DeleteObject\"\n            ],\n            \"Resource\": [\n                \"arn:aws:s3:::bucket-name\",\n                \"arn:aws:s3:::bucket-name/*\",\n                \"arn:aws:s3:::another-bucket\",\n                \"arn:aws:s3:::another-bucket/*\"\n            ]\n        }\n    ]\n}\nI'm using Laravel and FlySystem to abstract out the Filesystem on my project. I can get signed URLs to objects that are within the buckets, no issues, using the same credentials that fail on Put. There is no bucket policy assigned (I want everything private on there with only IAM access).. Hi,\nThanks for that last reply. The key was that I was POSTing the data rather than PUTing the data. As soon as I changed the HTTP method to PUT it worked no problem.\nI'm an idiot.\nThanks!. ",
    "czhifa": "Many thanks for your reply, please see my php version in my local host/IDE\n\n\n. ",
    "larsbo": "Hi Kevin,\nThank you for the answer.\nI'm using version 3.52.0 installed with composer (composer require aws/aws-sdk-php).\nI didn\u2019t add any other params (like the mentioned api_provider). Just region and version.\nI didn\u2019t change anything of the package files, of course.\nThere are no other packages directly involved but there are other project files that will be loaded before the autoload function. Maybe there has been set any other global things that could manipulate the aws package?. Ok, it works fine as a stand-alone project without the other project files. But I still wonder what could cause the error. Any suggestions maybe? \ud83d\ude03 \nEdit:\nThe resolve method runs into this part https://github.com/aws/aws-sdk-php/blob/master/src/ClientResolver.php#L283\nwith $args[$key] filled with\nphp\nArray (\n    [0] => Aws\\Api\\ApiProvider\n    [1] => defaultProvider\n)\ninstead of\nphp\nAws\\Api\\ApiProvider Object\n~~~But I don't understand why :(~~~\nIssue solved: The dev vm missed the file Aws\\Api\\ApiProvider.php (still don't know why but after a fresh composer install everythink works) :). ",
    "chungryan": "right... thanks. ",
    "costasovo": "Thank you I didn't know about paginators in the SDK.  I tried to use it but I don't get the results for all api keys :(\n```\n$results = $gatewayClient->getPaginator('GetUsage', [\n    'startDate' => $dateFrom->format('Y-m-d'),\n    'endDate' => $dateTo->format('Y-m-d'),\n    'usagePlanId' => $usagePlan,\n]);\nforeach ($results as $result) {\n    $items = array_merge($items, $result['items']);\n}\n```\nI thought ( from the article )that while I iterate the paginator will make subsequent api calls and I will get the data for all keys in the usage plan.  In this example the foreach body was executed only once and I got 20 results but I know that there should be 90 results for given input.\nI tried also the async aproach $results->each(...) with same result.\nWhen I dump the $result['items'] there is still not the position property.\nMaybe I'm still missing something?. ",
    "alexmanno": "@kstich I fixed the style problems as you said. Ping @kstich . ",
    "mikew1": "Apologies: Laravel 5.5 (previously I was on 5.3) requires an additional config setting to achieve public visibility, which 5.3 apparently does not. In config/filesystems s3, adding:   'visibility' => 'public' solved  this.. ",
    "movAX13h": "duplicate of #1366 \nif you can not update PHP to a version >= 5.5: the last compatible version of the aws-sdk (lower than 3.0) is 2.8.23.. ",
    "jschwarzwalder": "I will verify that the new instructions are correct.\nThank you for submitting the change. . @atrauzzi Did the docs that kstich link help get you unstuck? \nWhere did you expect to find the page that lists all the versions?\nIs there other information you were looking for to help you get started?\nThanks for your feedback. \n. It should return a string that you can parse.\nTry using str_getcsv(), and write back if that doesn't help.\n. Thank you for reaching out.\nThere are many ways to connect to your ec2 instance depending on your ec2 configuration and which machine you are using to connect. Find detailed instructions at the EC2 docs.\nI have a Windows machine and used putty to connect to my Linux ec2 instance, but you might need to follow different steps.\nIf you authorized the ec2 itself, I do not believe that you need to specify credentials in the client construction.\nPlease message back if this does not resolve your issue.. See response to issue #1530. I want to be able to sort by Client Class instead of Service Name, but I have yet to figure out how to update the index for the ksort in a manner which doesn't replace the ServiceName. . @Xylane Can you help me understand what needs to be changed in the code example?\nWhen I look at the code in the S3 docs  it is using a normal s3 client instead of a multipartUploader like is included in the SDK for PHP Docs example. \nIt was my understanding that if a file is larger than 5GB, then you need to use a MultipartUploader. \n. It is possible.\nThere are instructions in the developer guide about varifying credentials.\nI think this article will be the most helpful:\nhttps://docs.aws.amazon.com/sdk-for-php/v3/developer-guide/guide_credentials_temporary.html. In any credentials example you can change the client and it should work the same.\nYou need to use an Identity Access Management Role to give users permission to access or use any AWS resources.\nThis article from IAM might be more helpful: https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers.html\n. Thank you for your patience. I am reaching out to someone who knows how to use IAM policies better, and I will see if they have any further guidance for you. . ",
    "Stadly": "This would be really nice to have!. ",
    "jeffkee": "My thoughts would be that such features such as syncing or file timestamp comparison etc. is up to each developer to create for their business purposes, not rely on Amazon. Amazon's SDK provides a gateway to perform the essential functions such as PUT requests to S3 securely and reliably, providing the framework. The rest of the work is up to each company. A file-sync module would essentially overstep the boundaries of what an SDK does, and become a utility of its own. Perhaps this is something a 3rd party developer can create & distribute as an open source framework to build upon, and still relying on the AWS provided SDK as the final tool. . ",
    "jeichorn": "We see random failures on route53 on a regular basis, and I hit it every time I ran a batch job against a zone updating 10k cnames.\nThe worst offender is cloudfront.  To make an update you first need to read the config using\ngetDistributionConfig then update with updateDistribution.  Doing a batch of updates against 100 distributions I was never able to read and update them all without setting retries to 10.. Missed this question.  In the route53 case, its using a paginator on a zone with 10k cnames, and doing update calls in each iteration.\n$iterator = $api->getPaginator(...);\nforeach($iterator as $item) {\n   $api->changeResourceRecordSets(...);\n}\nIts a similar use case in Cloudfront, only you hit problems quicker since an each update request an extra get first.\nBut really this is basic math.  As long as my api calls are fast, when i'm paginating like I will hit backoff.  Your full jitter increases the chance of a long sleep but it doesn't guarantee it.  In any case where I get the short sleep 3 times in a row i'm done.  If i do batch operation where i'm going to see 2k 503s given the math, i'm pretty much guaranteed to fail on the default settings.. ",
    "babarinde": "Sorry,\nThis was actually calling the wrong bucket.. ",
    "xuhuaiqu": "\n    $client = AWS::createClient('pinpoint');\n\n    $ios_payload = json_encode(array(\"aps\" => array(\"alert\" => \"This is a test message\", \"sound\" => 'default')));\n\n    $result = $client->createCampaign([\n        'ApplicationId' => '7e90856d1934410f978b59f4d7xxxxxx',\n        'WriteCampaignRequest' => [\n            'AdditionalTreatments' => [\n                [\n                    'MessageConfiguration' => [\n                        'APNSMessage' => [\n                            'Action' => 'OPEN_APP',\n                            'Body' => 'hello world',\n                            'RawContent' => $payload,\n                            'SilentPush' => false,\n                            'Title' => 'title',\n                        ],\n                    ],\n                    'Schedule' => [\n                        'Frequency' => 'ONCE',\n                        'StartTime' => 'IMMEDIATE', //date(DateTime::ISO8601)\n                    ],\n                    'SizePercent' => 1,\n                ],\n            ],\n            'IsPaused' => false,\n            'MessageConfiguration' => [\n                'APNSMessage' => [\n                    'Action' => 'OPEN_APP',\n                    'Body' => 'hello world',\n                    'RawContent' => $payload,\n                    'SilentPush' => false,\n                    'Title' => 'title',\n                ],\n            ],\n            'Name' => 'test no rawcontent5',\n            'Schedule' => [\n                'Frequency' => 'ONCE',\n                'IsLocalTime' => false,\n                'StartTime' => 'IMMEDIATE',\n            ],\n            'SegmentId' => 'bfc6dd7be3074e978ca40afxxxxxx',\n            'SegmentVersion' => 1,\n        ],\n    ]);\n    .\n\n",
    "r3oath": "There are a few moving parts, but it basically boils down to\n$scores = $this->getSentimentScores(\n    $this->client->batchDetectSentiment([\n        'LanguageCode' => $this->language,\n        'TextList' => $this->processComments($comments),\n    ])\n);\n$this->language returns 'en'.\n$this->processComments($comments) returns an array of strings, e.g:\n[\"I really love that shirt\", \"Foo bar is a good company\"]. So I can shed a little more light on this issue. I swapped out the batchDetectSentiment call with detectSentiment, passing the same comment data (albeit one comment at a time) and I'm not getting the described error. \nSo it seems to be happening with batched requests only. Judging by the response status code and the fact I am passing all data as expected/required, the strings are normalised (detectSentiment works correctly with all of them) \u2013 the issue is more than likely happening deeper in the SDK, or perhaps even with Guzzle.. The input I'm passing is per the API specifications: a list of strings. Here's an example list which fails for me, and the results of running the request with debug enabled.\nList of strings:\narray(4) {\n  [5]=>\n  string(47) \"Speed limits have nothing to do with  tiredness\"\n  [6]=>\n  string(61) \"False They generally happen because of draconian speed limits\"\n  [7]=>\n  string(38) \"False some drivers start driving tired\"\n  [8]=>\n  string(26) \"They forget to take breaks\"\n}\nDebug results:\n```\n-> Entering step init, name 'idempotency_auto_fill'\n\ncommand was set to array(3) {\n    [\"instance\"]=>\n    string(32) \"[CUT]\"\n    [\"name\"]=>\n    string(20) \"BatchDetectSentiment\"\n    [\"params\"]=>\n    array(3) {\n      [\"LanguageCode\"]=>\n      string(2) \"en\"\n      [\"TextList\"]=>\n      array(4) {\n        [5]=>\n        string(47) \"Speed limits have nothing to do with  tiredness\"\n        [6]=>\n        string(61) \"False They generally happen because of draconian speed limits\"\n        [7]=>\n        string(38) \"False some drivers start driving tired\"\n        [8]=>\n        string(26) \"They forget to take breaks\"\n      }\n      [\"@http\"]=>\n      array(1) {\n        [\"debug\"]=>\n        resource(493) of type (stream)\n      }\n    }\n  }\nrequest was set to array(0) {\n  }\n-> Entering step validate, name 'validation'\nno changes\n-> Entering step build, name 'builder'\nrequest.instance was set to [CUT]\n  request.method was set to POST\n  request.headers was set to array(4) {\n    [\"X-Amz-Security-Token\"]=>\n    string(7) \"[TOKEN]\"\n    [\"Host\"]=>\n    array(1) {\n      [0]=>\n      string(34) \"comprehend.eu-west-1.amazonaws.com\"\n    }\n    [\"X-Amz-Target\"]=>\n    array(1) {\n      [0]=>\n      string(40) \"Comprehend_20171127.BatchDetectSentiment\"\n    }\n    [\"Content-Type\"]=>\n    array(1) {\n      [0]=>\n      string(26) \"application/x-amz-json-1.1\"\n    }\n  }\nrequest.body was set to {\"LanguageCode\":\"en\",\"TextList\":{\"5\":\"Speed limits have nothing to do with  tiredness\",\"6\":\"False They generally happen because of draconian speed limits\",\"7\":\"False some drivers start driving tired\",\"8\":\"They forget to take breaks\"}}\n  request.scheme was set to https\n-> Entering step build, name ''\nrequest.instance changed from [CUT] to [CUT]\n  request.headers.User-Agent was set to array(1) {\n    [0]=>\n    string(19) \"aws-sdk-php/3.52.23\"\n  }\n-> Entering step sign, name 'invocation-id'\nrequest.instance changed from [CUT] to [CUT]\n  request.headers.aws-sdk-invocation-id was set to array(1) {\n    [0]=>\n    string(32) \"[CUT]\"\n  }\n-> Entering step sign, name 'retry'\nrequest.instance changed from [CUT] to [CUT]\n  request.headers.aws-sdk-retry was set to array(1) {\n    [0]=>\n    string(3) \"0/0\"\n  }\n-> Entering step sign, name 'signer'\nrequest.instance changed from [CUT] to [CUT]\n  request.headers.X-Amz-Date was set to array(1) {\n    [0]=>\n    string(16) \"20180315T053842Z\"\n  }\nrequest.headers.Authorization was set to array(1) {\n    [0]=>\n    string(247) \"AWS4-HMAC-SHA256 Credential=[KEY]/20180315/eu-west-1/comprehend/aws4_request, SignedHeaders=aws-sdk-invocation-id;aws-sdk-retry;host;x-amz-date;x-amz-target, Signature=[SIGNATURE]\n  }\n\nRebuilt URL to: https://comprehend.eu-west-1.amazonaws.com/\nFound bundle for host comprehend.eu-west-1.amazonaws.com: 0x7fb2ebe102a0 [can pipeline]\nRe-using existing connection! (#0) with host comprehend.eu-west-1.amazonaws.com\n\nConnected to comprehend.eu-west-1.amazonaws.com (54.194.137.78) port 443 (#0)\n\nPOST / HTTP/1.1\nHost: comprehend.eu-west-1.amazonaws.com\nX-Amz-Target: Comprehend_20171127.BatchDetectSentiment\nContent-Type: application/x-amz-json-1.1\naws-sdk-invocation-id: [CUT]\naws-sdk-retry: 0/0\nX-Amz-Date: 20180315T053842Z\nAuthorization: AWS4-HMAC-SHA256 Credential=[KEY]/20180315/eu-west-1/comprehend/aws4_request, SignedHeaders=aws-sdk-invocation-id;aws-sdk-retry;host;x-amz-date;x-amz-target, Signature=[SIGNATURE]\nUser-Agent: aws-sdk-php/3.52.23 GuzzleHttp/6.2.1 curl/7.54.0 PHP/7.1.13\nContent-Length: 234\n\n\n\nupload completely sent off: 234 out of 234 bytes\n< HTTP/1.1 400 Bad Request\n< Date: Thu, 15 Mar 2018 05:38:42 GMT\n< Content-Type: application/x-amz-json-1.1\n< Content-Length: 99\n< Connection: keep-alive\n< x-amzn-RequestId: [CUT]\n< \n\nConnection #0 to host comprehend.eu-west-1.amazonaws.com left intact\n\n<- Leaving step sign, name 'signer'\nerror was set to array(13) {\n    [\"instance\"]=>\n    string(32) \"[CUT]\"\n    [\"class\"]=>\n    string(44) \"Aws\\Comprehend\\Exception\\ComprehendException\"\n    [\"message\"]=>\n    string(497) \"Error executing \"BatchDetectSentiment\" on \"https://comprehend.eu-west-1.amazonaws.com\"; AWS HTTP error: Client error: POST https://comprehend.eu-west-1.amazonaws.com resulted in a 400 Bad Request response:\n  {\"__type\":\"SerializationException\",\"Message\":\"Start of structure or map found where not expected.\"}\n   SerializationException (client): Start of structure or map found where not expected. - {\"__type\":\"SerializationException\",\"Message\":\"Start of structure or map found where not expected.\"}\"\n    [\"file\"]=>\n    string(109) \"[CUT]/vendor/aws/aws-sdk-php/src/WrappedHttpHandler.php\"\n    [\"line\"]=>\n    int(191)\n    [\"trace\"]=>\n    string(7414) \"#0 [CUT]/vendor/aws/aws-sdk-php/src/WrappedHttpHandler.php(100): Aws\\WrappedHttpHandler->parseError(Array, Object(GuzzleHttp\\Psr7\\Request), Object(Aws\\Command), Array)\n  #1 [CUT]/vendor/guzzlehttp/promises/src/Promise.php(203): Aws\\WrappedHttpHandler->Aws{closure}(Array)\n  #2 [CUT]/vendor/guzzlehttp/promises/src/Promise.php(174): GuzzleHttp\\Promise\\Promise::callHandler(2, Array, Array)\n  #3 [CUT]/vendor/guzzlehttp/promises/src/RejectedPromise.php(40): GuzzleHttp\\Promise\\Promise::GuzzleHttp\\Promise{closure}(Array)\n  #4 [CUT]/vendor/guzzlehttp/promises/src/TaskQueue.php(47): GuzzleHttp\\Promise\\RejectedPromise::GuzzleHttp\\Promise{closure}()\n  #5 [CUT]/vendor/guzzlehttp/guzzle/src/Handler/CurlMultiHandler.php(96): GuzzleHttp\\Promise\\TaskQueue->run()\n  #6 [CUT]/vendor/guzzlehttp/guzzle/src/Handler/CurlMultiHandler.php(123): GuzzleHttp\\Handler\\CurlMultiHandler->tick()\n  #7 [CUT]/vendor/guzzlehttp/promises/src/Promise.php(246): GuzzleHttp\\Handler\\CurlMultiHandler->execute(true)\n  #8 [CUT]/vendor/guzzlehttp/promises/src/Promise.php(223): GuzzleHttp\\Promise\\Promise->invokeWaitFn()\n  #9 [CUT]/vendor/guzzlehttp/promises/src/Promise.php(267): GuzzleHttp\\Promise\\Promise->waitIfPending()\n  #10 [CUT]/vendor/guzzlehttp/promises/src/Promise.php(225): GuzzleHttp\\Promise\\Promise->invokeWaitList()\n  #11 [CUT]/vendor/guzzlehttp/promises/src/Promise.php(267): GuzzleHttp\\Promise\\Promise->waitIfPending()\n  #12 [CUT]/vendor/guzzlehttp/promises/src/Promise.php(225): GuzzleHttp\\Promise\\Promise->invokeWaitList()\n  #13 [CUT]/vendor/guzzlehttp/promises/src/Promise.php(62): GuzzleHttp\\Promise\\Promise->waitIfPending()\n  #14 [CUT]/vendor/aws/aws-sdk-php/src/AwsClientTrait.php(58): GuzzleHttp\\Promise\\Promise->wait()\n  #15 [CUT]/vendor/aws/aws-sdk-php/src/AwsClientTrait.php(77): Aws\\AwsClient->execute(Object(Aws\\Command))\n  #16 [CUT]/app/Services/Aws.php(53): Aws\\AwsClient->__call('batchDetectSent...', Array)\n  #17 [internal function]: App\\Services\\Aws->App\\Services{closure}(Array, 1)\n  #18 [CUT]/vendor/laravel/framework/src/Illuminate/Support/Collection.php(861): array_map(Object(Closure), Array, Array)\n  #19 [CUT]/app/Services/Aws.php(56): Illuminate\\Support\\Collection->map(Object(Closure))\n  #20 [CUT]/app/Console/Commands/CalculateSentimentCommand.php(82): App\\Services\\Aws->getAggregatedSentimentFor(Object(App\\Objects\\FacebookPost), Object(App\\Console\\Commands\\CalculateSentimentCommand))\n  #21 [CUT]/app/Console/Commands/CalculateSentimentCommand.php(49): App\\Console\\Commands\\CalculateSentimentCommand->packageSentimentFor(Object(App\\Objects\\FacebookPost))\n  #22 [internal function]: App\\Console\\Commands\\CalculateSentimentCommand->App\\Console\\Commands{closure}(Object(App\\Objects\\FacebookPost), 2)\n  #23 [CUT]/vendor/laravel/framework/src/Illuminate/Support/Collection.php(861): array_map(Object(Closure), Array, Array)\n  #24 [CUT]/app/Console/Commands/CalculateSentimentCommand.php(50): Illuminate\\Support\\Collection->map(Object(Closure))\n  #25 [internal function]: App\\Console\\Commands\\CalculateSentimentCommand->handle()\n  #26 [CUT]/vendor/laravel/framework/src/Illuminate/Container/BoundMethod.php(29): call_user_func_array(Array, Array)\n  #27 [CUT]/vendor/laravel/framework/src/Illuminate/Container/BoundMethod.php(87): Illuminate\\Container\\BoundMethod::Illuminate\\Container{closure}()\n  #28 [CUT]/vendor/laravel/framework/src/Illuminate/Container/BoundMethod.php(31): Illuminate\\Container\\BoundMethod::callBoundMethod(Object(Illuminate\\Foundation\\Application), Array, Object(Closure))\n  #29 [CUT]/vendor/laravel/framework/src/Illuminate/Container/Container.php(549): Illuminate\\Container\\BoundMethod::call(Object(Illuminate\\Foundation\\Application), Array, Array, NULL)\n  #30 [CUT]/vendor/laravel/framework/src/Illuminate/Console/Command.php(183): Illuminate\\Container\\Container->call(Array)\n  #31 [CUT]/vendor/symfony/console/Command/Command.php(252): Illuminate\\Console\\Command->execute(Object(Symfony\\Component\\Console\\Input\\ArgvInput), Object(Illuminate\\Console\\OutputStyle))\n  #32 [CUT]/vendor/laravel/framework/src/Illuminate/Console/Command.php(170): Symfony\\Component\\Console\\Command\\Command->run(Object(Symfony\\Component\\Console\\Input\\ArgvInput), Object(Illuminate\\Console\\OutputStyle))\n  #33 [CUT]/vendor/symfony/console/Application.php(946): Illuminate\\Console\\Command->run(Object(Symfony\\Component\\Console\\Input\\ArgvInput), Object(Symfony\\Component\\Console\\Output\\ConsoleOutput))\n  #34 [CUT]/vendor/symfony/console/Application.php(248): Symfony\\Component\\Console\\Application->doRunCommand(Object(App\\Console\\Commands\\CalculateSentimentCommand), Object(Symfony\\Component\\Console\\Input\\ArgvInput), Object(Symfony\\Component\\Console\\Output\\ConsoleOutput))\n  #35 [CUT]/vendor/symfony/console/Application.php(148): Symfony\\Component\\Console\\Application->doRun(Object(Symfony\\Component\\Console\\Input\\ArgvInput), Object(Symfony\\Component\\Console\\Output\\ConsoleOutput))\n  #36 [CUT]/vendor/laravel/framework/src/Illuminate/Console/Application.php(88): Symfony\\Component\\Console\\Application->run(Object(Symfony\\Component\\Console\\Input\\ArgvInput), Object(Symfony\\Component\\Console\\Output\\ConsoleOutput))\n  #37 [CUT]/vendor/laravel/framework/src/Illuminate/Foundation/Console/Kernel.php(121): Illuminate\\Console\\Application->run(Object(Symfony\\Component\\Console\\Input\\ArgvInput), Object(Symfony\\Component\\Console\\Output\\ConsoleOutput))\n  #38 [CUT]/artisan(37): Illuminate\\Foundation\\Console\\Kernel->handle(Object(Symfony\\Component\\Console\\Input\\ArgvInput), Object(Symfony\\Component\\Console\\Output\\ConsoleOutput))\n  #39 {main}\"\n    [\"type\"]=>\n    string(6) \"client\"\n    [\"code\"]=>\n    string(22) \"SerializationException\"\n    [\"requestId\"]=>\n    string(36) \"[CUT]\"\n    [\"statusCode\"]=>\n    int(400)\n    [\"result\"]=>\n    NULL\n    [\"request\"]=>\n    array(5) {\n      [\"instance\"]=>\n      string(32) \"[CUT]\"\n      [\"method\"]=>\n      string(4) \"POST\"\n      [\"headers\"]=>\n      array(9) {\n        [\"X-Amz-Security-Token\"]=>\n        string(7) \"[TOKEN]\"\n        [\"Host\"]=>\n        array(1) {\n          [0]=>\n          string(34) \"comprehend.eu-west-1.amazonaws.com\"\n        }\n        [\"X-Amz-Target\"]=>\n        array(1) {\n          [0]=>\n          string(40) \"Comprehend_20171127.BatchDetectSentiment\"\n        }\n        [\"Content-Type\"]=>\n        array(1) {\n          [0]=>\n          string(26) \"application/x-amz-json-1.1\"\n        }\n        [\"User-Agent\"]=>\n        array(1) {\n          [0]=>\n          string(19) \"aws-sdk-php/3.52.23\"\n        }\n        [\"aws-sdk-invocation-id\"]=>\n        array(1) {\n          [0]=>\n          string(32) \"[CUT]\"\n        }\n        [\"aws-sdk-retry\"]=>\n        array(1) {\n          [0]=>\n          string(3) \"0/0\"\n        }\n        [\"X-Amz-Date\"]=>\n        array(1) {\n          [0]=>\n          string(16) \"20180315T053842Z\"\n        }\n        [\"Authorization\"]=>\n        array(1) {\n          [0]=>\n          string(247) \"AWS4-HMAC-SHA256 Credential=[KEY]/20180315/eu-west-1/comprehend/aws4_request, SignedHeaders=aws-sdk-invocation-id;aws-sdk-retry;host;x-amz-date;x-amz-target, Signature=[SIGNATURE]\n        }\n      }\n      [\"body\"]=>\n      string(234) \"{\"LanguageCode\":\"en\",\"TextList\":{\"5\":\"Speed limits have nothing to do with  tiredness\",\"6\":\"False They generally happen because of draconian speed limits\",\"7\":\"False some drivers start driving tired\",\"8\":\"They forget to take breaks\"}}\"\n      [\"scheme\"]=>\n      string(5) \"https\"\n    }\n    [\"response\"]=>\n    array(4) {\n      [\"instance\"]=>\n      string(32) \"[CUT]\"\n      [\"statusCode\"]=>\n      int(400)\n      [\"headers\"]=>\n      array(6) {\n        [\"X-Amz-Security-Token\"]=>\n        string(7) \"[TOKEN]\"\n        [\"Date\"]=>\n        array(1) {\n          [0]=>\n          string(29) \"Thu, 15 Mar 2018 05:38:42 GMT\"\n        }\n        [\"Content-Type\"]=>\n        array(1) {\n          [0]=>\n          string(26) \"application/x-amz-json-1.1\"\n        }\n        [\"Content-Length\"]=>\n        array(1) {\n          [0]=>\n          string(2) \"99\"\n        }\n        [\"Connection\"]=>\n        array(1) {\n          [0]=>\n          string(10) \"keep-alive\"\n        }\n        [\"x-amzn-RequestId\"]=>\n        array(1) {\n          [0]=>\n          string(36) \"[CUT]\"\n        }\n      }\n      [\"body\"]=>\n      string(99) \"{\"__type\":\"SerializationException\",\"Message\":\"Start of structure or map found where not expected.\"}\"\n    }\n  }\nInclusive step time: 0.3991219997406\n<- Leaving step sign, name 'retry'\nno changes\n  Inclusive step time: 0.39935398101807\n<- Leaving step sign, name 'invocation-id'\nno changes\n  Inclusive step time: 0.39951109886169\n<- Leaving step build, name ''\nno changes\n  Inclusive step time: 0.3996479511261\n<- Leaving step build, name 'builder'\nno changes\n  Inclusive step time: 0.39979195594788\n<- Leaving step validate, name 'validation'\nno changes\n  Inclusive step time: 0.39995503425598\n<- Leaving step init, name 'idempotency_auto_fill'\nno changes\n  Inclusive step time: 0.40009808540344\nIn WrappedHttpHandler.php line 191:\nError executing \"BatchDetectSentiment\" on \"https://comprehend.eu-west-1.amazonaws.com\"; AWS HTTP error: Client error: POST https://comprehend.eu-west-1.amazonaws.com res\n  ulted in a 400 Bad Request response:                                                                                                                                     \n  {\"__type\":\"SerializationException\",\"Message\":\"Start of structure or map found where not expected.\"}                                                                        \n   SerializationException (client): Start of structure or map found where not expected. - {\"__type\":\"SerializationException\",\"Message\":\"Start of structure or map found where\n   not expected.\"}                                                                                                                                                             \nIn RequestException.php line 113:\nClient error: POST https://comprehend.eu-west-1.amazonaws.com resulted in a 400 Bad Request response:\n  {\"__type\":\"SerializationException\",\"Message\":\"Start of structure or map found where not expected.\"}      \n```. Upon further testing, it seems the issue may be stemming from the fact that each batch detect in my application is performing its job on an array of chunked comments. Hence, as seen in the example above some arrays aren't indexed starting with zero.\nPerhaps this detail should be noted in the documentation or array_values called on the SDK side to ensure the indexing matches what the API is expecting.. Every array declared in PHP is associative under the hood, eg: [\"foo\", \"bar\"] = [0 => \"foo\", 1 => \"bar\"]. \nSo if the API intrinsically requires sequential zero-first arrays, it either needs to make that apparent in the documentation, improve the error messaging, or from an SDK perspective call array_values. If the order of strings is critical, then even an exception or warning from the SDK that the array passed isn't sequential and zero-based will go a long way with helping your end users debug bizarre situations like the one I faced.\nAnd the end of the day, the fact that you can pass an array with a non-zero index and cause a SerializationException on the AWS side speaks to this issue requiring a little bit of attention.. ",
    "hm122": "Solved by increasing ulimit -n.. ",
    "atrauzzi": "Heck, let's include regions in with this too.. @kstich - Awesome, thanks.  Is there any way for me to grab the region of the machine I'm currently on via the SDK?. Not quite, I think my suggestion here is to get everything gathered together, but through PHP rather than having to hard code values or manually write http requests to the metadata service to get information.\nThen, for the constants, I guess they don't exist..?  So, I'm somewhat in a position of doing workarounds for the time being.. PHP 7.2, 3.88.0 version of the AWS PHP SDK.\nI'm not sure I can reproduce the issue as I'm not sure the error is even related to my usage of it.  It's within a laravel application in a background queue worker.\n```\n11 /var/app/current/vendor/aws/aws-sdk-php/src/AwsClientTrait.php(58): GuzzleHttp\\Promise\\Promise->wait()\n```\nI'm sitting atop quite a few otherwise reliable abstractions that work the rest of the time.  The only thing I can think is that I run one queue job that interacts with S3 and then it does another job in the same thread that does some S3 work.  But that's just function calls at the end of the day.\nIs it possible that you or guzzle are not disposing/recycling something correctly perhaps?. ",
    "KasperFranz": "A discussion about this is also .changes is also worth having as that folder is distrubted currently. Any eta on this? . Thank you.\nThis was copy pasted from your api document which i didn;t question that to much, \n(https://docs.aws.amazon.com/AmazonS3/latest/dev/UploadObjSingleOpPHP.html)\nagain thank you.. ",
    "Nilithus": "If you mean interoperability as in \"do the other SDK's use AWS_SHARED_CREDENTIALS_FILE to determine their shared credentials file location?\" I took the liberty of poking through some of the other sdks to check. It seems to be common practice:\n- The Go sdk checks it here\n- Nodejs looks like it checks here and it specifically calls out that it does it in a code comment above\n- Ruby has a determine_credentials_path method also calls out the expected behavior in code comment above\nGranted I didn't check all of them but seems pretty clear that the AWS_SHARED_CREDENTIALS_FILE should be respected.. ",
    "jonathanbull": "Great, my mistake. Thank you for the detailed reply.. @kstich Ah, I did see the jitter but I didn't realise that the '100ms base' was referring to just that aspect of the logic, and not the output as a whole. Thanks for clarifying.. ",
    "raresserban": "So how can I then restrict the ContentType of files that a user can upload to my S3 bucket?\nThere needs to be some sort of way to only allow a user to only upload video files for example.. ",
    "vamsideepak": "my problem is related to this . i mentioned contentType:'video/mp4' and uploading videos to s3 bucket after that getting that video urls those are working in web and android but not in safari can anyone tell me.\nnote: uploading video webm format.\nvar pet_signed_url = s3.getSignedUrl('putObject', {\n    Bucket: AWSConfig.BUCKET_NAME,\n    Key: pet_vedio_key,\n    ContentType: 'video/mp4',\n    Expires: parseInt(AWSConfig.PROFILE_VIDEO_EXPIRATION_TIME)\n}). ",
    "awprice": "Can I please get feedback on this? @kstich . Thanks for the review @kstich! I've updated my PR with those changes you requested.\nI didn't update the getPresignHeaders() function to include stripos($name, 'x-amz-') !== 0) as it completely strips out the headers that should actually be signed. This includes metadata headers and other important headers - thus defeating the purpose of this PR. With the inclusion of  stripos($name, 'x-amz-') !== 0), headers like x-amz-acl wouldn't of been signed, and could easily be changed by the person that will use the presigned URI.\nWhilst S3 includes the headers when comparing signatures, if they aren't signed properly in the PHP SDK, the user will be presented with a SignatureDoesNotMatch error.\nLet me know what you think.. @kstich Could I get some feedback on this PR?. @kstich Awesome, I've updated the PR.. @kstich Is this ready to merge?. ",
    "holyspecter": "Any progress on this issue?. ",
    "dingo-d": "So the issue seems to be that I am not connecting to the server. \nAny way to do this? An example would help a lot.. Hmm not entirely sure about that. I think I'd need to provide some kind of credentials. I'll probably end up working on the EC2 server instance so to avoid all the mess of working with credentials (I am using IAM on the EC2 so might as well use it :D). Thanks for the help.. ",
    "norbert-yoimo": "You are correct, the result was split up, using the NextToken attribute solved the issue.\nI'd suggest mentioning this in the docs, the result sets are split up in a surprising way, sometimes I get responses that contain 1 element (sometimes even 0 elements) that I have to iterate through.\nThank you. ",
    "jakesyl": "Regarding laravel: https://github.com/aws/aws-sdk-php-laravel/issues/115. Let me try to duplicate on a fresh repository.. ",
    "vesper8": "the auto-discovery seems not to be working. But after adding the provider manually to the app.php then all works good. ",
    "NeilJ247": "Example:\n```\n$itemData = [\n   \"key1\" => \"Jes\u00fas\", \n   \"key2\" => [\n       \"key3\" => \"Jes\u00fas\"\n    ]\n];\n$itemMarshaller = $this->getMarshaler();\nreturn $this->getDynamoDb()->putItem(\n    [\n        'TableName' => $this->getTableName(),\n        'Item' => $itemMarshaller->marshalItem($itemData),\n    ]\n);\n```\nException:\n``\nUncaught PHP Exception Aws\\DynamoDb\\Exception\\DynamoDbException: \"Error executing \"PutItem\" on \"https://dynamodb.eu-west-1.amazonaws.com\"; AWS HTTP error: Client error:PO\nST https://dynamodb.eu-west-1.amazonaws.comresulted in a400 Bad Requestresponse:\n{\"__type\":\"com.amazon.coral.service#SerializationException\"}\n SerializationException (client):  - {\"__type\":\"com.amazon.coral.service#SerializationException\"}\" at /html/vendor/aws/aws-sdk-php/src/WrappedHttpHandler.php line 192 Array\n(\n    [exception] => [object] (Aws\\DynamoDb\\Exception\\DynamoDbException(code: 0): Error executing \"PutItem\" on \"https://dynamodb.eu-west-1.amazonaws.com\"; AWS HTTP error: Client error:POST https://dynamodb.eu-wes\nt-1.amazonaws.comresulted in a400 Bad Requestresponse:\n{\"__type\":\"com.amazon.coral.service#SerializationException\"}\n SerializationException (client):  - {\"__type\":\"com.amazon.coral.service#SerializationException\"} at /html/vendor/aws/aws-sdk-php/src/WrappedHttpHandler.php:192, GuzzleHttp\\Exception\\ClientException(code: 400): C\nlient error:POST https://dynamodb.eu-west-1.amazonaws.comresulted in a400 Bad Request` response:\n{\"__type\":\"com.amazon.coral.service#SerializationException\"}\n at /html/vendor/guzzlehttp/guzzle/src/Exception/RequestException.php:113)\n[stacktrace]\n0 /html/vendor/aws/aws-sdk-php/src/WrappedHttpHandler.php(101): Aws\\WrappedHttpHandler->parseError(Array, Object(GuzzleHttp\\Psr7\\Request), Object(Aws\\Command), Array)\n1 /html/vendor/guzzlehttp/promises/src/Promise.php(203): Aws\\WrappedHttpHandler->Aws{closure}(Array)\n2 /html/vendor/guzzlehttp/promises/src/Promise.php(174): GuzzleHttp\\Promise\\Promise::callHandler(2, Array, Array)\n3 /html/vendor/guzzlehttp/promises/src/RejectedPromise.php(40): GuzzleHttp\\Promise\\Promise::GuzzleHttp\\Promise{closure}(Array)\n4 /html/vendor/guzzlehttp/promises/src/TaskQueue.php(47): GuzzleHttp\\Promise\\RejectedPromise::GuzzleHttp\\Promise{closure}()\n5 /html/vendor/guzzlehttp/guzzle/src/Handler/CurlMultiHandler.php(96): GuzzleHttp\\Promise\\TaskQueue->run()\n6 /html/vendor/guzzlehttp/guzzle/src/Handler/CurlMultiHandler.php(123): GuzzleHttp\\Handler\\CurlMultiHandler->tick()\n7 /html/vendor/guzzlehttp/promises/src/Promise.php(246): GuzzleHttp\\Handler\\CurlMultiHandler->execute(true)\n8 /html/vendor/guzzlehttp/promises/src/Promise.php(223): GuzzleHttp\\Promise\\Promise->invokeWaitFn()\n9 /html/vendor/guzzlehttp/promises/src/Promise.php(267): GuzzleHttp\\Promise\\Promise->waitIfPending()\n10 /html/vendor/guzzlehttp/promises/src/Promise.php(225): GuzzleHttp\\Promise\\Promise->invokeWaitList()\n11 /html/vendor/guzzlehttp/promises/src/Promise.php(267): GuzzleHttp\\Promise\\Promise->waitIfPending()\n12 /html/vendor/guzzlehttp/promises/src/Promise.php(225): GuzzleHttp\\Promise\\Promise->invokeWaitList()\n13 /html/vendor/guzzlehttp/promises/src/Promise.php(62): GuzzleHttp\\Promise\\Promise->waitIfPending()\n14 /html/vendor/aws/aws-sdk-php/src/AwsClientTrait.php(59): GuzzleHttp\\Promise\\Promise->wait()\n15 /html/vendor/aws/aws-sdk-php/src/AwsClientTrait.php(78): Aws\\AwsClient->execute(Object(Aws\\Command))\n16 /html/vendor/misha/audit-logger-bundle/src/Maglabs/Misha/AuditLoggerBundle/AuditLogger.php(188): Aws\\AwsClient->__call('putItem', Array)\n```\n. Sorry I will look at this ASAP again and get back to you.. Hi @howardlopez,\nIt turns out the issue was on our end. The problem was our MySQL database connection wasn't setting the charset which resulted in reading the non-encoded value and then trying to write/put that value to DynamoDB caused the exception which threw us off a bit.\nThanks for looking into this.\n. ",
    "dfrnks": "Track code\n2018-04-25 14:50:39, PosixPID(7578): Err: Error executing \"SendEmail\" on \"https://email.us-east-1.amazonaws.com\"; AWS HTTP error: count(): Parameter must be an array or an object that implements Countable, /var/www/html/WE/vendor/aws/aws-sdk-php/src/WrappedHttpHandler.php (192),   \n    2) Project/init.php(1886): Aws\\AwsClient->__call('sendEmail', Array)\n    3) Project/vendor/aws/aws-sdk-php/src/AwsClientTrait.php(78): Aws\\AwsClient->execute(Object(Aws\\Command))\n    4) Project/vendor/aws/aws-sdk-php/src/AwsClientTrait.php(59): GuzzleHttp\\Promise\\Promise->wait()\n    5) Project/vendor/guzzlehttp/promises/src/Promise.php(62): GuzzleHttp\\Promise\\Promise->waitIfPending()\n    6) Project/vendor/guzzlehttp/promises/src/Promise.php(221): GuzzleHttp\\Promise\\Promise->invokeWaitList()\n    7) Project/vendor/guzzlehttp/promises/src/Promise.php(265): GuzzleHttp\\Promise\\Promise->waitIfPending()\n    8) Project/vendor/guzzlehttp/promises/src/Promise.php(221): GuzzleHttp\\Promise\\Promise->invokeWaitList()\n    9) Project/vendor/guzzlehttp/promises/src/Promise.php(262): GuzzleHttp\\Promise\\Promise->waitIfPending()\n    10) Project/vendor/guzzlehttp/promises/src/Promise.php(230): GuzzleHttp\\Promise\\TaskQueue->run()\n    11) Project/vendor/guzzlehttp/promises/src/TaskQueue.php(61): GuzzleHttp\\Promise\\RejectedPromise::GuzzleHttp\\Promise{closure}()\n    12) Project/vendor/guzzlehttp/promises/src/RejectedPromise.php(40): GuzzleHttp\\Promise\\Promise::GuzzleHttp\\Promise{closure}(Array)\n    13) Project/vendor/guzzlehttp/promises/src/Promise.php(172): GuzzleHttp\\Promise\\Promise::callHandler(2, Array, Array)\n    14) Project/vendor/guzzlehttp/promises/src/Promise.php(201): Aws\\WrappedHttpHandler->Aws{closure}(Array)\n    15) Project/vendor/aws/aws-sdk-php/src/WrappedHttpHandler.php(101): Aws\\WrappedHttpHandler->parseError(Array, Object(GuzzleHttp\\Psr7\\Request), Object(Aws\\Command), Array). Yes, this is the code.\n$sesClient = new \\Aws\\Ses\\SesClient([\n    \"version\" => \"latest\",\n    \"region\" => \"us-east-1\",\n    \"credentials\" => [\n        \"key\"       => $config->email_user_aws,\n        \"secret\"    => $config->email_pass_aws\n    ]\n]);\ntry {\n    $send = $sesClient->sendEmail([\n        \"Source\" => utf8_encode(\"{$config->email_from_name} <{$config->email_from}>\"),\n        \"Destination\" => [\n            \"ToAddresses\" => $to,\n            \"CcAddresses\" => $cc,\n            \"BccAddresses\" => $cco,\n        ],\n        \"Message\" => [\n            \"Subject\" => [\n                \"Data\" => $assunto,\n                \"Charset\" => \"UTF-8\"\n            ],\n            \"Body\" => [\n                \"Html\" => [\n                    \"Data\" => $msg,\n                    \"Charset\" => \"UTF-8\"\n                ]\n            ]\n        ],\n        \"ReplyToAddresses\" => [\n            $config->email_no_reply\n        ]\n    ]);\n\n    if(is_array($send->toArray()) && isset($send->toArray()['MessageId'])){\n        return true;\n    }\n\n}catch (\\Exception $e){\n    throw new Exception($e);\n}\n\n. @howardlopez @kstich \nHi, I resolved the problem. \nI forced the version off guzzle for 6.3. Actually the problem is with the guzzle and not with the AWS. \nYou need to update the version for \"guzzlehttp/guzzle\": \"6.3\", to for this is work fine.\n. ",
    "heidemn": "@kstich this is not the answer we expected.... ",
    "iricketson": "@kstich \nphp\n// Instance of Aws\\OpsWorks\\OpsWorksClient\n$client = new OpsWorksClient($config);\n$results = $client->describeLayers([\n    'StackId' => 'stack-id-here',\n    'LayerIds' => ['layer-id-here'],\n]);\nRegardless of LayerIds parameter, the request returns all layers for the stack.. @kstich understood, I was able to get that to work with your clarification. Thanks for your time and efforts.. ",
    "reydajp": "@kstich \nThanks for you help.\nThe exception error gone but it's still weird.\n1 . The prediction score is different between my request from the SDK response and the AWS console one.\n2 . I noticed that even if I change the string value, this have not incidence on the prediction score and getting always the same.\nFrom aws console:\n\nThe score I supposed to have:\n\nMy code:\n```\n$ml->predict([\n            'MLModelId' => 'ml-ooXXXXXX',\n            'PredictEndpoint' => 'https://XXXXXX',\n            'Record' => [\n                'Name' => 'Var2',\n                'Type' => 'Text',\n                'Value' => 'This is an apple.'\n            ]\n        ]);\n```\nThe score from the SDK response:\nobject(Aws\\Result) {\n    [private] data => array(\n        'Prediction' => array(\n            'predictedLabel' => 'Diary',\n            'predictedScores' => array(\n                'Daily Essay' => (float) 0.3212323486805,\n                'Diary' => (float) 0.44966238737106,\n                'General/Others' => (float) 0.22910524904728\n            ),\n            'details' => array(\n                'Algorithm' => 'SGD',\n                'PredictiveModelType' => 'MULTICLASS'\n            )\n        ),\n        '@metadata' => array(\n            'statusCode' => (int) 200,\n            'effectiveUri' => 'https://XXXXXXXXXX',\n            'headers' => array(\n                'x-amzn-requestid' => '0x4398adb',\n                'content-type' => 'application/x-amz-json-1.1',\n                'content-length' => '224',\n                'date' => 'Wed, 16 May 2018 23:58:19 GMT'\n            ),\n            'transferStats' => array(\n                'http' => array(\n                    (int) 0 => array()\n                )\n            )\n        )\n    )\n}\nIf i change the string value with 'The sky is blue.\nScore from AWS Console:\n{\n    \"Prediction\": {\n        \"details\": {\n            \"Algorithm\": \"SGD\",\n            \"PredictiveModelType\": \"MULTICLASS\"\n        },\n        \"predictedLabel\": \"Daily Essay\",\n        \"predictedScores\": {\n            \"Daily Essay\": 0.383843332529068,\n            \"Diary\": 0.3686932623386383,\n            \"General/Others\": 0.2474634051322937\n        }\n    }\n}\nScore from the SDK response:\nsame as the first example of SDK response.\nI think My Record parameter syntax is not valid. That is why I'm getting always the same score.\nSo what is the right syntax ?\n. @kstich \nYes I tried with the 'Name' => 'Value' pair but getting the same result....\nOk I will take a look at the forum. \nThanks for you help.\n. ",
    "sterichards": "I have now passed the above hurdle, after comparing my code to other examples and a bit of guess work:\n```\n$credentials = new Credentials('key','secretKey');\n$aws = new AwsClient([\n    'credentials' => $credentials,\n    'service' => 'comprehend',\n    'region' => 'eu-west-1',\n    'version' => '2017-11-27'\n]);\n```\nHope this helps anybody else that searching for this information. After further investigation I realise any previous code examples above are useless. I wont delete them as there's no reason to with this explanation\nBelow is a fully working code example of using AWS Comprehend using the AWS PHP SDK:\n```\n$credentials = new Credentials('key','secretKey');\n$comprehend = new ComprehendClient([\n    'credentials' => $credentials,\n    'service' => 'comprehend',\n    'region' => 'eu-west-1',\n    'version' => '2017-11-27'\n]);\n$batchDetechItems = $comprehend->batchDetectSentiment([\n    'LanguageCode' => 'en',\n    'TextList' => [\n        'Sentiment analysis text item 1',\n        'Sentiment analysis text item 2',\n        'Sentiment analysis text item 3'\n        // etc etc etc.....\n    ]\n]);\nforeach ($batchDetechItems as $item) {\n    print_r($item);\n}\n```. ",
    "PabloWestphalen": "Oh absolutely. I'm new to the SDK, and my proposal was the first yet probably naive solution i could think of (which is why i created the issue instead of going straight for a PR). Sure a flag like that would work too.. ",
    "LCamel": "If we issue the ['http']['proxy'] option with 'scheme' => 'https', the proxy will not be able to cache the response.\nA S3Client with the (forward) proxy option will initiate a CONNECT request, and all the following traffic between it and the S3 server will be TLS encrypted, which can not be comprehended by the proxy server.. ",
    "SansWord": "This issue did annoy us, too. Please add support so we don't have to use ad-hoc solution, too.. ",
    "vishal-akg": "this method isn't suitable for downloading large files and i didn't get how using SaveAs parameter is going to prevent from connection timeout issue?\n. ",
    "tomekit": "Hi @kstich,\nI am affected by the StreamWrapper timeouts issue as well.\nThe SDK v2 had throw_exceptions stream context option and this option did the job even on PHP 7.2. \nI've thought upgrading to SDK v3 might resolve some of the timeout issues, but unfortunately they are still there, except I no longer can capture the exception.\nIs there anything in PHP preventing reinstating that exception error handling feature?\nCan StreamWrapper reliably auto-reconnect during the file copy? In my case the ~5GB file is failing, but the copy happens from the eu-west-1 EC2 node directly to S3 in eu-west-1, so it shouldn't be a big deal?\nOld SDK v2 settings that could throw an exception:\nstream_context_set_default([\n            's3' => [               \n                'throw_exceptions' => true\n            ]\n        ]);. I can't dedicate more time to help reproducing it. The region I've used is eu-west-1, perhaps there are some differences between the us-west-2 that you've used.\nFrom what it looks like I should just stop using copy. Thanks for your help . ",
    "dbpolito": "````php\n$cloudFront = new CloudFrontClient([\n    'region'  => $this->region,\n    'version' => '2016-01-28',\n]);\n$url = $this->httpHost . '/' . $key;\n$condition = [\n    'DateLessThan' => [ 'AWS:EpochTime' => time() + $expiration ],\n];\n$cookies = $cloudFront->getSignedCookie([\n    'private_key' => $this->pem,\n    'key_pair_id' => $this->cloudfrontKey,\n    'policy'      => json_encode([\n        'Statement' => [\n            [\n                'Resource' => $url . '/*',\n                'Condition' => $condition,\n            ],\n        ],\n    ], JSON_UNESCAPED_SLASHES),\n]);\n````\nThis is working just fine when i change Encryption to: None or AES-256.. ",
    "douglasjam": "Doing a foreach and writing with gzencode does not work.\nBut deflat_init and deflat_add works properly, thank you.\n```\n$handler = fopen('/tmp/test.csv', 'w');\n$deflateContext = deflate_init(ZLIB_ENCODING_GZIP, ['level' => 9]);\n$strings = [\n    'Hello, how are you?' . PHP_EOL,\n    'I am fine thanks' . PHP_EOL,\n    'Hello, how are you?' . PHP_EOL,\n];\nforeach ($strings as $string) {\n    fwrite($handler, deflate_add($deflateContext, $string, ZLIB_NO_FLUSH));\n}\nfwrite($handler, deflate_add($deflateContext, '', ZLIB_FINISH));\nfclose($handler);\necho gzdecode(file_get_contents('/tmp/test.csv'));\n```. ",
    "ninoman": "SDK version: 3.55.4\ncode sample: \n\n\n. ",
    "syed1994": "i try to use composer.json as you mentioned above but getting same error.Am using php 5.3.8 . I have attached screen shots for your reference.\n\n\n. ",
    "bscutt": "Interesting, it does look like it should be return mt_rand(100, (int) min(20000, (int) pow(2, $retries) * 100)); in that case \ud83e\udd14 . ",
    "abiodunjames": "I'm facing this issue currently. ",
    "rvelhote": "Yessss. That's precisely it. Thank you very much! :1st_place_medal: . ",
    "bitcine-darcy": "Note some fields get populated at runtime (INPUT, DESTINATION). And I removed some sensitive information.\n````\n$client = MediaConvertClient::factory($credentials);\n$config = array(          \n    'JobTemplate' => 'arn:aws:mediaconvert:us-east-1:xxxxxxxx:jobTemplates/DASH demux DRM',\n    'Queue' => 'arn:aws:mediaconvert:us-east-1:xxxxxxxx:queues/Default',\n    'UserMetadata' => array(),\n    'Role' => 'arn:aws:iam::xxxxxxxx:role/mediaConvert',\n    'Settings' => array(\n        'OutputGroups' => array(\n            array(\n                'Outputs' => array(\n                    array(\n                        'Preset' => 'bitcine-DASH_v1080_6000',\n                        'NameModifier' => '_v1080_6000'\n                    ),\n                    array(\n                        'Preset' => 'bitcine-DASH_v720_3000',\n                        'NameModifier' => '_v720_3000'\n                    ),\n                    array(\n                        'Preset' => 'bitcine-DASH_v540_2000',\n                        'NameModifier' => '_v540_2000'\n                    ),\n                    array(\n                        'Preset' => 'bitcine-DASH_v360_1200',\n                        'NameModifier' => '_v360_1200'\n                    ),\n                    array(\n                        'Preset' => 'bitcine-DASH_v270_400',\n                        'NameModifier' => '_v270_400'\n                    ),\n                    array(\n                        'Preset' => 'bitcine-DASH_a128',\n                        'NameModifier' => '_a128'\n                    )\n                ),\n                'OutputGroupSettings' => array(\n                    'Type' => 'DASH_ISO_GROUP_SETTINGS',\n                    'DashIsoGroupSettings' => array(\n                        'SegmentLength' => 30,\n                        'FragmentLength' => 2,\n                        'SegmentControl' => 'SINGLE_FILE',\n                        'HbbtvCompliance' => 'NONE',\n                        'Destination' => 'DESTINATION',\n                        'Encryption' => array(\n                            'SpekeKeyProvider' => array(\n                                'ResourceId' => '36c8c333d0944f69b2569ce6aa6656a2',\n                                'SystemIds' => [\n                                    'edef8ba9-79d6-4ace-a3c8-27dcd51d21ed',\n                                    '9a04f079-9840-4286-ab92-e65be0885f95'\n                                ],\n                                'Url' => 'https://xxxxxx.execute-api.us-east-1.amazonaws.com/EzDRMStage/copyProtection'\n                            )\n                        )\n                    )\n                )\n            )\n        ),        \n        'Inputs' => array(\n            array(\n                'AudioSelectors' => array(\n                    'Audio Selector 1' => array(\n                        'Tracks' => array(\n                            1\n                        ),\n                        'DefaultSelection' => 'DEFAULT',\n                        'SelectorType' => 'TRACK',\n                    )\n                ),\n                'FileInput' => 'INPUT'\n            )\n        )\n    )\n)   \n$result = $client->createJob($config);\n````. ",
    "zubair72": "Hello pls find the information as required.\n1- I am executing php under two enviroments i-Aws, ii, Standard shared hotsing\n2- I am using shell_exec to run php command line\n3- Under aws I use shell_exec and use php executable\n4- Under standard hosting I use shell_exec and php-cli executable\n5- I am using PHP 7.0 running on 64bit Amazon Linux/2.7.0 on aws\n7- I am using php 7.0 running on linux kernel 3.10.0-693.11.6.1.ELK.el6.x86_64 under shared hosting\nTo answer your question I am getting php work properly on aws environment but not on shared hosting environment, to add to this answer, I am using CLI from shell_exec and not directly from for command line. The loader also works fine from php-cli under windows environment.\nI'll be glad to provide any additional required information that can help.\nregards. Hi,\nI am using php version 7.025 with AWS SDK version 3.64.4\nexact error message is \"[16-Aug-2018 01:18:36 America/Chicago] PHP Parse error:  syntax error, unexpected '$value' (T_VARIABLE) in /home/smartap1/public_html/app062/include/aws/Aws/functions.php on line 36\"\nI am attaching the package error_log and err.txt under cron logs this error\npacakge is deployed to http://app062.smartapps4free.com\nLet me know if you  require the phpinfo access for any further information about server, I am using a shared account with hostgator.\nhtml-20180816001.zip\nI'll be glad to provide any further information required\nregards\n. I double checked the php version selected on cpanel that is version 7 and also my .htaccess file is clean for any such directive.\nIs there a possibility that php-cli version remains old no matter what version we select on cpanel?\nYes its correct php-cli version remains to 5.4.45 no matter what is selected on cpanel and seems to be host configuration error.\nThanks for your all the effort to identify the issue. Thanks its php version mismatch issue. ",
    "naughtymonkey1010": "Sorry, I didn't look at the documentation carefully. This can be solved with the parameter use_path_style_endpoint.. ",
    "YuK1Game": "Trial and error to solve.\n'GroupBy' => [\n        [\n            'Key'  => 'LINKED_ACCOUNT',\n            'Type' => 'DIMENSION',\n        ],\n        [\n            'Key'  => 'SERVICE',\n            'Type' => 'DIMENSION',\n        ],\n    ],\n. ",
    "mirror222": "sorry the delayed response. here are the codes:\n`<?php\nrequire './vendor/autoload.php';\nerror_reporting(E_ALL);\nini_set(\"display_errors\", 1);\n$params = array(\n    'credentials' => array(\n        'key' => 'XXXXXX',\n        'secret' => 'R/XXXXXXXX/Qc+XXXXXXXXXX',\n    ),\n    'region' => 'us-east-1', // < your aws from SNS Topic region\n    'version' => 'latest',\n    'debug'     => false,\n);\n$sns = new \\Aws\\Sns\\SnsClient($params);\n$args = array(\n    \"MessageAttributes\" => [\n                'AWS.SNS.SMS.SenderID' => [\n                    'DataType' => 'String',\n                    'StringValue' => 'EOS'\n                ],\n                'AWS.SNS.SMS.SMSType' => [\n                    'DataType' => 'String',\n                    'StringValue' => 'Transactional'\n                ]\n            ],\n    \"Message\" => \"[xxxxx] \u60a8\u7684\u9a8c\u8bc1\u7801\u662f\uff1a987651\uff0c10\u5206\u949f\u5185\u6709\u6548\u3002\u8bf7\u59a5\u5584\u4fdd\u62a4\u597d\u9a8c\u8bc1\u7801\uff0c\u9632\u6b62\u6cc4\u9732\u3002\",\n    \"PhoneNumber\" => \"+86xxxxxxxxxxxx\"\n);\n$result = $sns->publish($args);\nvar_dump($result);\n`. ",
    "koushikSen": "\nYes, they are enabled.\nHere is the codebase:: \nphp\n<?php public function delete($memory_id)\n{\n    $memory = new Memory();\n    $qaMemory = new MemoryAccess;       \n    $memoryInfo = $memory->where('memory_id',$memory_id)->first();\n    $qaMemory->where('user_id',Auth::user()->id)->where('memory_recall_datetime',$memoryInfo->recall_date)->first();\n    $qaMemory->delete();\n    if($memoryInfo->mime_type){\n        $path = $memoryInfo->memory_content;\n        $pathArray = explode('/', $path);\n        if(Storage::disk('s3')->exists(end($pathArray))){ // error occoured\n            $isDeleted =  Storage::disk('s3')->delete(end($pathArray));\n        } else{\n                //echo \"not found\";\n            }\n    }\n    $memory->delete();\n    return redirect('home')->with('registration_success', 'Registration successful');\n} ?>. in EC2 server : \n\nIn My localserver \n\nDisplaying simple array working fine in EC2, but displaying SimpleXML is when an error occurred.\n thrown in /var/www/html/xml.php on line 12\n[17-Aug-2018 04:29:41 UTC] PHP Fatal error:  Uncaught Error: Call to undefined function simplexml_load_string() in /var/www/html/xml.php:12\n. ",
    "dalisay": "I encountered this same issue and restarting php-fpm resolved it.\nsudo systemctl restart php-fpm. ",
    "skaiteo98": "Woah thanks @dalisay!! Can't believe that solved my issue!. ",
    "DQNEO": "Lots of unexpected failures.\nhttps://travis-ci.org/aws/aws-sdk-php/jobs/417659763\nLet me retry later , after my other PRs are merged.. Sure! did it. 1e6b06a9e5e8dfcf993d800decb84d810b821634. OK, both PRs are made for the same purpose, So I merged the messages into one line.. Oh, sorry I completely misunderstood the rule.\nI renamed it.  73d72101. Sure! I did it 5ee9421. Sure, I did it d14c8e3. ",
    "pranavansp": "Hi, @diehlaws Thanks for your replay. I have already double checked with credentials values but it gives the same error. Anyhow, I have solved the problem by regenerating access key. There is something issue on it. my suggestion to investigate that. I found that idea from this issue. \nAfter regenerating keys. SQS works fine with all methods. Thanks (Y)\n. @kstich @diehlaws its want to close but may i know your response for this issue? . ",
    "ranrinc": "@diehlaws \nYes you are right... my webserver will not have access to reach my home folder. Is there away to change the location of .aws/ to somewhere within the reach of the webserver? How should I change the code in order to do this? Thanks. ",
    "edersonbadeca": "I don't provide any of credentials configuration but the arnRole in the ecs task-definition which is already attached SqsFullAccess Policy.\nThe source that initializes the client is:\nphp\n$this->client = $sqsClient ?: SqsClient::factory(\\Config::get('aws'));\n The Config::get('aws') have this values:\nphp\nreturn [\n  'version' => 'latest',\n  'region'  => 'us-east-1',\n];\nThe full response is:\n04 Sep 2018 21:11:00.615[7373]: ip-<company>/master-<company-tag> [2018-09-04 21:11:00]\nproduction.ERROR: exception 'Aws\\Sqs\\Exception\\SqsException' with message \n'Error executing \"ReceiveMessage\" on \"https://sqs.us-east-1.amazonaws.com/<queue-URL>\"; \nAWS HTTP error: Client error: `POST https://<queue-URL>` resulted in a `403 Forbidden` \nresponse:\n. Note: This scenario happens sometimes, in the most cases the resource is well consumed.\nAnother usefull informations:\nPolicy used: arn:aws:iam::aws:policy/AmazonSQSFullAccess\njs\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Action\": [\n                \"sqs:*\"\n            ],\n            \"Effect\": \"Allow\",\n            \"Resource\": \"*\"\n        }\n    ]\n}. @diehlaws I will open a case for that. I thought that could be a behavior from sdk because I used the same strategy with python-sdk, in the same ec2 cluster and the same policy and it works good. \nThank you for your time and I will let you in touch. \n. ",
    "Dvuzh": "@howardlopez I have a problem, how write item in conditionExpression\n$mass = [\n            \":number\" =>''66666666',\n        ];\n        $eav = $marshaler->marshalJson(json_encode($mass,true));\n$params = [\n....\n'ConditionExpression' => ' phone[1].number = :number',\n'ExpressionAttributeValues' => $eav,\n];\ndynamoDb->updateItem($params);\n phone[0].number don't work. ",
    "alanchavez88": "@diehlaws thank you so much for getting back to me on this issue. It was indeed MFA enabled on that user. Thanks for pointing on the right direction.\n. ",
    "BentCoder": "Never mind. My bad. I seem to have two users per application.. ",
    "tranthienbinh1989": "@diehlaws Thank for your response. I'm pretty sure that my credentials are correct. Because it works with another query string example: q=1, q=\"0\" but q=0 throw an exception.. @diehlaws We are running PHP v7.2.9 and AWS SDK for PHP v2.7.27\nI think we can fix this issue by updating aws sdk but we won't do it at this moment. I go ahead to close this ticket. Thank you for your support.. ",
    "PhilETaylor": "Firstly sorry for the lack of helpful information in initial opening of this issue, was in a rush and did not have time to report it correctly. \nI tried to recreate this today, and build you a complete docker based test case with PHP 7.1 RC1 and failed to replicate it once. I attempted to replicate the Flysystem code in an isolated test case, and every test passed, with no exceptions thrown. \nSorry for wasting your time. I must have hit some edge case somehow the other day. I'll run my test docker from the office again on monday (at home now on \"other\" mac) and as long as that passes (which it should, thats the point of docker builds!) then I'll forget this ever happened. \nI'm happy to close this. If I get this again I'll create a replicable test case. . ",
    "dawolf": "Turns out the documentation does list the parameter IsDefault with a capital i, while the code works only with a lower case i and isDefault\nMaybe somebody can point me in the right direction to update the doc or fix the i. Thanks.. ",
    "greysteil": "\ud83d\udc4b Maintainer of Dependabot here.\nWe should be able to add something in Dependabot that makes this customisable. It'll take a couple of weeks, as I want to ship new dashboards first (so the feature has somewhere to live) but we'll get there.. ",
    "zoomphoto": "Thanks, I'll test.  The documentation is a little vague.. so for example:\nhttps://docs.aws.amazon.com/aws-sdk-php/v3/api/api-rekognition-2016-06-27.html#deletefaces\nIt shows that I can send:\n$result = $client->deleteFaces([\n    'CollectionId' => '<string>', // REQUIRED\n    'FaceIds' => ['<string>', ...], // REQUIRED\n]);\nWhich shows as a string, and when I echo the string the variable, and just paste it in, it works. \nI'll try out your method and let you know how it works... so many little nit-picky things to learn!\nThanks,. I was able to get this to work, sending the payload as an array.... but you have to remove the square braces to make it work.\nSo using mySQL to populate an array:\nwhile mysql{\n $variable[] = $mysql['response'];\n}\nThe once I have that array, I can just pass the array as the payload:\n$rek->deleteFaces([\n            'CollectionId' => \"{$workingEvent}\", \n            'FaceIds' => $variable,\n        ]);\nThat worked!  \n. ",
    "downsider": "Think I found a solution. See the PR linked to this issue. Thanks Howard. Appreciated :). ",
    "scottf-tvw": "I am having the same issue when updating to v3.69.0 with psr7 1.4.1 and calling Aws\\Sqs\\SqsClient. \nI quickly reverted back to 3.18.23 and guzzlehttp/psr7 1.4.0\n```ErrorException: Cannot instantiate abstract class Aws\\ClientSideMonitoring\\AbstractMonitoringMiddleware\n1 /invintus/Invintus-coreSys/vendor/aws/aws-sdk-php/src/ClientSideMonitoring/AbstractMonitoringMiddleware.php(83): handleFatalError\n```\n$credentialProvider,\n            $options,\n            $region,\n            $service\n        ) {\n            return new static( <-- crash here\n                $handler,\n                $credentialProvider,\n                $options,\n                $region,\n                $service\nphp 5.5.9. I will try to get something posted this weekend.. @howardlopez, thank you for the patch!. ",
    "luudv": "\nCan you provide a code sample & PHP version (and any relevant context), as well as the full stack trace of the error message?\n\n```php\n$credentials = new Aws\\Credentials\\Credentials($this->config['AWS_ACCESS_KEY_ID'] , $this->config['AWS_SECRET_ACCESS_KEY']);\n$s3_client = new S3Client([\n    'version' => $this->config['version'],\n    'region'  => $this->config['region'],\n    'credentials'  => $credentials,\n]);\ntry {\n$s3_client->putObject([\n    'Bucket' => $this->config['Bucket'],\n    'Key' => $file['name'],\n    'Body' => fopen($file['path'], 'r'),\n    'ACL' => $this->config['ACL'],\n]);\n} catch (Aws\\S3\\Exception\\S3Exception $e) {\n    echo \"There was an error uploading the file.\\n\";\n    echo $e->getMessage();\n}\n}\n```. ",
    "gabi77": "@howardlopez  I have exactly the same error with a version\nPHP 5.5.9-1ubuntu4.25 (cli) (built: May 10 2018 14:37:18)\nCopyright (c) 1997-2014 The PHP Group\nZend Engine v2.5.0, Copyright (c) 1998-2014 Zend Technologies\nBut no problem with a php version 5.6.35 locally.. Hi @quickly3\nYes, AWS-SDK-PHP rollback has version 3.67.20 it works on php 5.5.9. before 3.69.0 - 2018-10-04\nLog\n 3.69.0 - 2018-10-04\n    ...\n    Aws\\ClientSideMonitoring - Code for future SDK instrumentation and telemetry.\n    .... ",
    "quickly3": "just reversion to old one. ",
    "datawrangl3r": "Okay, to which version the SDK has to be rolledback for this to work?. Quick fix is to do:\ncomposer require  aws/aws-sdk-php:3.52.0. ",
    "HeJianqiao": "I am sorry that I am not write completely, Our production environment set the  memory limit for PHP is 20M.\nHere is about '$body':\nphp\n$s3Client = new S3Client([..........]);\n$file = fopen('/data/userKeys', 'r');   //file of the data key\nwhile (!feof($file)) {\n    $row = trim(fgets($file));\n    $body = file_get_contents($row);\n    $params = [\n        'Bucket' => 'avatar',\n        'Key' => md5($body),\n        'Body' => $body\n    ];\n    $s3Client->putObject($params);\n}\nand the error report:\nPHP Fatal error: Allowed memory size of 20971520 bytes exhausted (tried to allocate 1052672 bytes) in /var/www/scripts/aws/GuzzleHttp/Psr7/Stream.php on line 218. ",
    "ricardofiorani": "Honestly this is such a let down, but then, it's not aws's sdk fault so...\nAnyway, Thank you for your response @diehlaws . ",
    "joshuaadickerson": "In a development environment where this package is used, all of the require-dev packages are required from the dependencies. Thus the aws-sdk-php package breaks my development environment because I don't have sockets and pcntl extensions.\nThis is a BC break. How can it not be? You use dev requirements to run tests. This makes it so you can't even composer install.\nA lot of people disable or don't install those extensions for security purposes. Why not just check if they exist and use them if they do? Seems like a much safer way of doing it.. ",
    "ErikThiart": "Hey, @diehlaws thanks for the detailed response, that is what I did initially, but something does not add up. I am pretty sure we need to include the session token somewhere\nThis is my code\n```\n<?php\n// Require the Composer autoloader.\nrequire 'vendor/autoload.php';\nuse Aws\\Credentials\\Credentials;\nuse GuzzleHttp\\Client;\nuse GuzzleHttp\\Psr7\\Request;\nuse Aws\\Signature\\SignatureV4;\n$request = new Request(\n    'POST',\n    'https://example.com/api',\n    [\n        'body' => [\n            \"type\" => \"client\",\n            \"action\" => \"read\",\n            \"limit\" => 10\n        ]\n    ]    \n);\n$key = 'AccessKeyId';\n$secret = 'SecretAccessKey';\n$credentials = new Credentials($key, $secret);\n// Construct a request signer\n$region = 'eu-west-1';\n$signer = new SignatureV4(\"execute-api\", $region);\n// Sign the request\n$request = $signer->signRequest($request, $credentials);\n// Send the request\ntry {\n    $response = (new Client)->send($request);\n    print_r($response);\n}\n    catch (Exception $exception) {\n    $responseBody = $exception->getResponse()->getBody(true);\n    echo $responseBody;\n}\n$response = (new Client)->send($request);\n$results = json_decode($response->getBody());\n```\nThis is the response:\n```\n{\"message\":\"The security token included in the request is invalid.\"}\nFatal error: Uncaught GuzzleHttp\\Exception\\ClientException: Client error: POST https://example.com/api resulted in a 403 Forbidden response: {\"message\":\"The security token included in the request is invalid.\"} in C:\\Users\\Thor\\app\\vendor\\guzzlehttp\\guzzle\\src\\Exception\\RequestException.php on line 113\nGuzzleHttp\\Exception\\ClientException: Client error: POST https://example.com/api resulted in a 403 Forbidden response: {\"message\":\"The security token included in the request is invalid.\"} in C:\\Users\\Thor\\app\\vendor\\guzzlehttp\\guzzle\\src\\Exception\\RequestException.php on line 113\n```\nPS: When I do the auth call to get my access key and secret this is what is returned.\n{\n    \"status\": \"ok\",\n    \"result\": {\n        \"Tokens\": {\n            \"refresh_token\": \"eyJjdHkiOi..............truncated\"\n        },\n        \"Credentials\": {\n            \"AccessKeyId\": \"ASI..............truncated\",\n            \"SecretAccessKey\": \"uhmpjnK..............truncated\",\n            \"SessionToken\": \"FQoGZXIvYXdzEGoaDGZxLmRm..............truncated\",\n            \"Expiration\": \"2018-10-18T09:28:59+00:00\"\n        },\n        \"Access\": [\n            \"access_role_1\",\n            \"access_role_2\",\n            \"access_role_3\",\n            \"access_role_4\",\n            \"access_role_5\",\n            \"access_role_6\",\n            \"access_role_7\",\n            \"access_role_8\",\n            \"access_role_9\",\n            \"access_role_10\",\n            \"access_role_11\",\n            \"access_role_12\"\n        ]\n    }\n}\nShould we not be using that SessionTokensomewhere in the new Credentials() call?. Alright just to give some feedback in case someone else runs into this issue.\n1 you absolutely need to use the SessionToken in the new Credentials() call and then #2 you need to convert the body to JSON I got that bit based on this: __construct ( string $method, string|Psr\\Http\\Message\\UriInterface $uri, array $headers = [],string|null|resource|Psr\\Http\\Message\\StreamInterface $body = null, string $version = '1.1' ) from the Guzzle Request documentation.\nSo I ended up changing this:\n$request = new Request(\n    'POST',\n    'https://example.com/api',\n    [\n        'body' => [\n            \"type\" => \"client\",\n            \"action\" => \"read\",\n            \"limit\" => 10\n        ]\n    ]      \n);\nto this:\n$request = new Request(\n   'POST',\n   'https://example.com/api',\n   [],\n   '{\"type\":\"client\",\"action\":\"read\",\"limit\":10}'\n);\nAnd then added the SessionToken to the credentials call like so:\n$credentials = new Credentials($key, $secret, $session);\nand then it worked.. ",
    "shiratsu": "Thank you for your reply.\nI will ask aws forum. ",
    "tbarry-eminence": "I gave your first suggestion of deleting the composer.lock file and vendor directories, then running composer update, but unfortunately it fails in exactly the same manner.\nI've done a search against my entire project (dependencies included) for the aws/aws-sdk-php line in my composer.json, and only turned up one result. To be completely certain, I then ran an equivalent search for the function manifest declaration only to come up with one result.\nThe only thing that has resolved my issue has been the pull request which wraps all function declarations in a valid if (!function_exists()) check.. I also wasn't able to recreate this issue in a controlled environment. Both using a new project with only aws/aws-sdk-php and with my companies composer.json, the built in PHP server served my PHP script with vendor/autoload.php successfully.\nThis does tell me that it has to relate to other code or imports in my project; however, it tells me that while knowing that at no point is the aws/aws-sdk-php package being pulled in more than once. That leaves me with very little direction to go. If I am unable to find declarations of the src/functions,php functions anywhere else in my project then I am left without much I can do.\nIf you have any other testing ideas feel free to let me know. Otherwise, the most effective solution I have is my pull request.\nThanks, by the way. I hope I don't come across as rude; it's just extremely difficult to debug a problem when all your leads turn up empty. It's basically debugging blindly.. I think you have a sound stance. I'm off work so I'll have to take a look at trying grep tomorrow. I'll report back then with my findings, and hopefully resolve it that way.\nThanks for all your support. If it turns out to be an issue I can fix myself I'm perfectly happy with that.. Unfortunately, grep did not turn up anything unexpected. Within my entire environment, the only declarations of the namespace Aws string are within the vendor/aws/aws-sdk-php directory of my site project. It's painful to admin but I think the best course of action for now is to maintain and use my fork of the aws/aws-sdk-php repository.\nThanks again for your patience and support. Feel free to reject my pull request and close this issue; I'll just have to continue investigations on my own and hopefully find a solution at a later date.. Fantastic call on that last point, that did it!\nThis site is a legacy Drupal 7 site. It can be a bit daunting to try and figure out how things work at times. Turns out, there's a module called xautoload for Drupal 7 which attempts to scan the entire code base and build up a composer autoload of the potentially many composer.json files.\nIt's not even being used by any code, so by removing the module I am now able to require aws/aws-sdk-php without issue.\nI appreciate all the help once again, thanks!. ",
    "blacksaildivision": "Hi @diehlaws, thank you for your response. Indeed I could add .aws directory to open_basedir, that would solve the issue. However if I would like to keep it tightly limited to few only locations I cannot do that at the moment because of the warning.\nWhat is also interesting - when credentials are passed directly in array to S3Client constructor, why access to ~/.aws directory is checked? All information that is required to proper request to AWS API is already there. No need to check if directory is readable IMHO.  . ",
    "iNviNho": "Thank you @diehlaws for your response!\nI tried your suggestion and did set a client with these config params:\narray:4 [  \n  \"region\" => \"eu-west-1\",  \n  \"version\" => \"latest\",\n  \"http\" => array:1 [\n    \"connect_timeout\" => 30\n  ]\n]\nThen in while loop we call receiveMessage with following config: \n[\n            \"QueueUrl\" => $this->queue,\n            \"MaxNumberOfMessages\" => 10,\n            \"WaitTimeSeconds\" => 20\n        ];\nSo when connection is Ok, every 20 seconds we get a response with available message from queue. Then i switch off internet connection and receiveMessage call is stucked and dont throw any exception after 30seconds nor after any amount of time ...\nDo you please have any suggestion what may we try to do?. Thank you very much for your time to investigate and look into the issue. \nAfter some time, we realized that the problem may lie somewhere inside docker. We tried 2 different scenarios. Both ran as php-cli command:\na) Run outside of docker container. Everything went as it should. After connectivity was lost, no messages were obviously retrieved and after we reconnected we nicely continued to retrieve messages.\nb) Run inside of docker container. After connectivity was lost, no exception, no timeout, no response from a php-cli command. After we reconnect, nothing, no response.\nSo my assumption is that the problem is on docker side and i will try to discuss with our DevOps team. If you have any other suggestion, we will be grateful for any.\nThank you very much,. Thank you for your time and suggestions. Our DevOps team will take care of it and when we find a solution, we will post it here as well if possible. I think you can close Issue. Thank you.. ",
    "nmuntyanov": "Hi, @iNviNho  have you found solution?. ",
    "pushpesh4u": "Turns out I had to pass the complete path instead of a subset of the path in order for it to work.. ",
    "alquerci": "Hello @diehlaws, thanks to take care about this.\nFor sure extends the library is the solution to provide the expected behaviour. I create this issue because such feature/patch may have the place on aws-php-sdk and not only library user side. Because this issue is hard to detect and need to use a dedicated retry behaviour because it is bound to AWS infrastructure.\n\nFor further investigation how is it possible that the Instance Metadata service return an invalid JSON on the response body?\nIs it planned to patch the Instance Profile Credential Provider?\n\nNote: json_decode() does not throw an exception when on Invalid JSON, but just return false. On this case it most likely a bug on the Instance Metadata service. What do you think?\nEdit: Maybe the retry will not help if there is an issue on JSON syntax.\nMaybe log the error with the [JSON error message][0] may help to see more details.\n[0]: https://github.com/thePanz/json-exception/blob/v0.1.0/src/Json.php#L34-L39. @diehlaws I want to know whether the Instance Profile Credential Provider is open to be patched with a pull request?. @diehlaws Thank you for this feedback as it opens a road to a resolution.\nYour message grabs mainly all related issues. :+1: \nI am convinced that implementing a naive retry solution will not be enough and may aggravating the problem.\nThat's why your feedback provides a good clarification of the situation.\n. ",
    "albanx": "Closing: This has to do with execution time of PHP locally, so I have to increase that limit.. ",
    "sh-ogawa": "Hi @diehlaws , thanks for your reply.\nWe can also manually invoke garbage collection in PHP.\nIt became very helpful, so I would like to try this suggestion.\nThank you very much.. I was able to confirm that the memory does not continue to increase by using gc_collect_cycles().\nThank you very much.. ",
    "AngieRd": "Thank you for your response @diehlaws, weird thing here is that the 99% of the sendings the message ID is returned within 0.5 secs but 1 of 100 emails the request hang a long time, in the end, the email is sent, other times I get that error.\nI would suppose that if my server's timezone is wrong I should get same error in all requests to SES, right?. I have synchronized NTP Time in my server and now it's working. \nI let the link for Ubuntu SO that I followed \nhttps://www.tecklyfe.com/fix-ubuntu-timedatectl-ntp-sync-no/\nThank you so much for your help @diehlaws \nI think this ticket can closed.. ",
    "techman1984": "Hi Alex,\nThanks for sharing this. I have already tried that option but this does not solve the issue.\nAny other pointers?\nRegards,\nAshwini\nFrom: Alex Diehl notifications@github.com\nReply-To: aws/aws-sdk-php reply@reply.github.com\nDate: Thursday, November 8, 2018 at 6:57 PM\nTo: aws/aws-sdk-php aws-sdk-php@noreply.github.com\nCc: \"Sharma, Ashwini\" asharma@nascar.com, Mention mention@noreply.github.com\nSubject: Re: [aws/aws-sdk-php] Word Press | Missing required client configuration options : AWS PHP SDK 3.X (#1670)\nHi @techman1984https://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_techman1984&d=DwMFaQ&c=8SWcqD-NcC9ZXaoLXKDi8Q&r=_d7w9ILRskBV5XzzPAEGRzxZr5ZFOLkj5yfTwbD2Pi8&m=O_TW9xfXZ0p2TZmjTCtUzHALlr3L0rnug9CqXB6NDi8&s=pzYuEPTWNoht9wfyisnUgEcDY4GFvcSt3PqQbhs_sgw&e=, thanks for reaching out to us about this. The error you're seeing typically comes up when a service client is initialized without a 'version' parameter - in this case it looks to be an S3 Client. If you cannot see the full stack trace pointing out where this error is originating you may want to search your application's directory for PHP files containing an S3 client initialization, then check to make sure those are specifying an API version in the client parameters. This can be done with grep as shown below.\ngrep -rnw '/path/to/application/' -e 'new S3Client' -e 'new Aws\\S3\\S3Client' --exclude-dir=/path/to/aws-sdk-for-php/\n-r will search recursively,\n-n will include the line number where the expression was found,\n-w will match the whole word instead of partial matches,\n-e will search for the given expression, and can be used multiple times in the same command to search for multiple expressions,\n--exclude-dir is used to exclude the SDK's installation directory so you don't mistakenly remove 'S3Client' from any files in the SDK.\nI hope this helps you identify where your S3Client is being improperly initialized! Please don't hesitate to reach back out if you continue running into trouble with this.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHubhttps://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_aws_aws-2Dsdk-2Dphp_issues_1670-23issuecomment-2D437200371&d=DwMFaQ&c=8SWcqD-NcC9ZXaoLXKDi8Q&r=_d7w9ILRskBV5XzzPAEGRzxZr5ZFOLkj5yfTwbD2Pi8&m=O_TW9xfXZ0p2TZmjTCtUzHALlr3L0rnug9CqXB6NDi8&s=E-gtrREBsB9GLZguL8AOiNHwCAqpBuGaccoxdqztK0g&e=, or mute the threadhttps://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_notifications_unsubscribe-2Dauth_AqzaVWC6JF4OFJRXWbzIghod9MlPBJoDks5utMTrgaJpZM4YVn0-5F&d=DwMFaQ&c=8SWcqD-NcC9ZXaoLXKDi8Q&r=_d7w9ILRskBV5XzzPAEGRzxZr5ZFOLkj5yfTwbD2Pi8&m=O_TW9xfXZ0p2TZmjTCtUzHALlr3L0rnug9CqXB6NDi8&s=fDO2so-ug99kXyzo41Hd6z3pSjWHyB6c2Nj_q2-xtVQ&e=.\n. ++ Patrick\nHi Alex,\nThanks for your email. I think you are right and this seems to be a third-party plugin issue. I think I have found the plugin.. but instead of enabling or disabling plugins one by one, I used the following word press function \u2018exception::getTrace();\u2019. I think I\u2019m close now.\nAnyways, I appreciate your help.\nRegards,\nAshwini\nFrom: Alex Diehl notifications@github.com\nReply-To: aws/aws-sdk-php reply@reply.github.com\nDate: Thursday, November 8, 2018 at 8:34 PM\nTo: aws/aws-sdk-php aws-sdk-php@noreply.github.com\nCc: \"Sharma, Ashwini\" asharma@nascar.com, Mention mention@noreply.github.com\nSubject: Re: [aws/aws-sdk-php] Word Press | Missing required client configuration options : AWS PHP SDK 3.X (#1670)\nUnfortunately the troubleshooting steps taken on your end are not clear to me - it sounds like you tried searching for 'new S3Client' and 'new Aws\\S3\\S3Client' and weren't able to find any instances of these expressions, please correct me if I'm wrong. It might be worth searching for just 'S3Client' as well since the S3 Client initialization in version 2https://urldefense.proofpoint.com/v2/url?u=https-3A__docs.aws.amazon.com_aws-2Dsdk-2Dphp_v2_guide_service-2Ds3.html&d=DwMFaQ&c=8SWcqD-NcC9ZXaoLXKDi8Q&r=_d7w9ILRskBV5XzzPAEGRzxZr5ZFOLkj5yfTwbD2Pi8&m=RzshYNzjoxF3VE_RSAnz-AL0XOZ6YEvSPx1UC3-O8Fo&s=6lCqUpmyI55fjlp4bUTQnJj0yr1F9hg0c0sE-wJ_gBs&e= of the AWS SDK for PHP is different from version 3.\nIf your grep search does not show any files that initialize an S3 Client in your entire WordPress directory, you shouldn't be getting this error. Reviewing the full stack trace would point out the files that are involved in this error, which would help narrow down where to look for this misconfiguration.\nIt is possible that one or more of your WordPress plugins conflicts with the AWS SDK for PHP v3.x as seen herehttps://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_aws_aws-2Dsdk-2Dphp-2Dsymfony_issues_10-23issuecomment-2D163409593&d=DwMFaQ&c=8SWcqD-NcC9ZXaoLXKDi8Q&r=_d7w9ILRskBV5XzzPAEGRzxZr5ZFOLkj5yfTwbD2Pi8&m=RzshYNzjoxF3VE_RSAnz-AL0XOZ6YEvSPx1UC3-O8Fo&s=cQFZYI5aGKvCoJblhWXY6OwWg1cBeLMD8WfqYR5FbME&e=, herehttps://urldefense.proofpoint.com/v2/url?u=https-3A__wordpress.org_support_topic_missing-2Drequired-2Dclient-2Dconfiguration-2Doptions_&d=DwMFaQ&c=8SWcqD-NcC9ZXaoLXKDi8Q&r=_d7w9ILRskBV5XzzPAEGRzxZr5ZFOLkj5yfTwbD2Pi8&m=RzshYNzjoxF3VE_RSAnz-AL0XOZ6YEvSPx1UC3-O8Fo&s=-hIQPCW3zFIRdW9-FqUoqxZzkCuPFVNboxLhWpUF_U8&e=, herehttps://urldefense.proofpoint.com/v2/url?u=https-3A__wordpress.org_support_topic_the-2Daws-2Dsdk-2Dusage-2Dseem-2Dto-2Dconflict-2Dwith-2Dother-2Dplugins-2Dusing-2Daws-2Dsdk_&d=DwMFaQ&c=8SWcqD-NcC9ZXaoLXKDi8Q&r=_d7w9ILRskBV5XzzPAEGRzxZr5ZFOLkj5yfTwbD2Pi8&m=RzshYNzjoxF3VE_RSAnz-AL0XOZ6YEvSPx1UC3-O8Fo&s=DUz3IqMyuC1crZ6fRXl1ItXvw9IgKrkD7zh5D2uPjb8&e=, and herehttps://urldefense.proofpoint.com/v2/url?u=https-3A__wordpress.org_support_topic_missing-2Drequired-2Dclient-2Dconfiguration-2Doptions-2D2_&d=DwMFaQ&c=8SWcqD-NcC9ZXaoLXKDi8Q&r=_d7w9ILRskBV5XzzPAEGRzxZr5ZFOLkj5yfTwbD2Pi8&m=RzshYNzjoxF3VE_RSAnz-AL0XOZ6YEvSPx1UC3-O8Fo&s=knGF_nqc89RBgSXYBXuM0Bz4SHyPfViPgEWsvJX_TLw&e=. I suggest disabling your WP plugins systematically to ensure this isn't the cause of your error.\nYou could disable all WP plugins and enable them one by one until you start seeing this error again to identify the culprit, or take a binary search approach where you disable half your plugins, check to see if the behavior persists, if it does then disable half of the remaining enabled plugins and repeat until things work as expected. From there, enable one by one the plugins from the last set of plugins that were disabled until you start seeing the error again to see which plugin is conflicting with the SDK.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHubhttps://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_aws_aws-2Dsdk-2Dphp_issues_1670-23issuecomment-2D437218923&d=DwMFaQ&c=8SWcqD-NcC9ZXaoLXKDi8Q&r=_d7w9ILRskBV5XzzPAEGRzxZr5ZFOLkj5yfTwbD2Pi8&m=RzshYNzjoxF3VE_RSAnz-AL0XOZ6YEvSPx1UC3-O8Fo&s=E_h3ZN_qfYnk3wqGpgBgRQRdpbyIf7WGWl_DdvqHASQ&e=, or mute the threadhttps://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_notifications_unsubscribe-2Dauth_AqzaVe4WRH0yDJSVs032f97feIsLuOzdks5utNuXgaJpZM4YVn0-5F&d=DwMFaQ&c=8SWcqD-NcC9ZXaoLXKDi8Q&r=_d7w9ILRskBV5XzzPAEGRzxZr5ZFOLkj5yfTwbD2Pi8&m=RzshYNzjoxF3VE_RSAnz-AL0XOZ6YEvSPx1UC3-O8Fo&s=DX8F3KTG-AdS1ZG4ctOUcDNfCDnvXVsIEPXFi1DBDkU&e=.\n. Hi Alex... This was caused due to a third party plugin which was using SDK 2.0...since... now we migrated to 3.0.. it started throwing error. \nAnyways, Thanks for your help.. ",
    "riandyrn": "Awesome! Thanks a lot @diehlaws!. ",
    "vitoacme": "Hi! Not sure how but the SDK probably didn't install properly. When I ran composer require aws/aws-sdk-php again, it worked fine. Thank you for you time :). ",
    "AlessandroMinoccheri": "ok, but why the results are different? I thought that the response could be a different object but data I was expected that will be the same.. different object but same data inside, or not?. Thanks for the explanation, now I have to understand why results are different, but now is clear the difference between these methods.\nThanks. With debug mode there are a lot of info, I think that this could be useful:\n``\nerror was set to array(13) {\n    [\"instance\"]=>\n    string(32) \"000000007c825275000000003199846f\"\n    [\"class\"]=>\n    string(40) \"Aws\\DynamoDb\\Exception\\DynamoDbException\"\n    [\"message\"]=>\n    string(480) \"Error executing \"TransactWriteItems\" on \"http://localhost:8000\"; AWS HTTP error: Client error:POST http://localhost:8000resulted in a400 Bad Request` response:\n  {\"__type\":\"com.amazonaws.dynamodb.v20120810#UnknownOperationException\",\"message\":\"An unknown operation was requested.\"}\n   UnknownOperationException (client): An unknown operation was requested. - {\"__type\":\"com.amazonaws.dynamodb.v20120810#UnknownOperationException\",\"message\":\"An unknown operation was requested.\"}\"\n    [\"file\"]=>\n    string(101) \"/Users/alessandrominoccheri/Sites/broadway-dynamodb/vendor/aws/aws-sdk-php/src/WrappedHttpHandler.php\"\n    [\"line\"]=>\n    int(191)\n    [\"trace\"]=>\n    string(4430) \"#0 /Users/alessandrominoccheri/Sites/broadway-dynamodb/vendor/aws/aws-sdk-php/src/WrappedHttpHandler.php(100): Aws\\WrappedHttpHandler->parseError(Array, Object(GuzzleHttp\\Psr7\\Request), Object(Aws\\Command), Array)\n  #1 /Users/alessandrominoccheri/Sites/broadway-dynamodb/vendor/guzzlehttp/promises/src/Promise.php(203): Aws\\WrappedHttpHandler->Aws{closure}(Array)\n  #2 /Users/alessandrominoccheri/Sites/broadway-dynamodb/vendor/guzzlehttp/promises/src/Promise.php(174): GuzzleHttp\\Promise\\Promise::callHandler(2, Array, Array)\n  #3 /Users/alessandrominoccheri/Sites/broadway-dynamodb/vendor/guzzlehttp/promises/src/RejectedPromise.php(40): GuzzleHttp\\Promise\\Promise::GuzzleHttp\\Promise{closure}(Array)\n  #4 /Users/alessandrominoccheri/Sites/broadway-dynamodb/vendor/guzzlehttp/promises/src/TaskQueue.php(47): GuzzleHttp\\Promise\\RejectedPromise::GuzzleHttp\\Promise{closure}()\n  #5 /Users/alessandrominoccheri/Sites/broadway-dynamodb/vendor/guzzlehttp/guzzle/src/Handler/CurlMultiHandler.php(98): GuzzleHttp\\Promise\\TaskQueue->run()\n  #6 /Users/alessandrominoccheri/Sites/broadway-dynamodb/vendor/guzzlehttp/guzzle/src/Handler/CurlMultiHandler.php(125): GuzzleHttp\\Handler\\CurlMultiHandler->tick()\n  #7 /Users/alessandrominoccheri/Sites/broadway-dynamodb/vendor/guzzlehttp/promises/src/Promise.php(246): GuzzleHttp\\Handler\\CurlMultiHandler->execute(true)\n  #8 /Users/alessandrominoccheri/Sites/broadway-dynamodb/vendor/guzzlehttp/promises/src/Promise.php(223): GuzzleHttp\\Promise\\Promise->invokeWaitFn()\n  #9 /Users/alessandrominoccheri/Sites/broadway-dynamodb/vendor/guzzlehttp/promises/src/Promise.php(267): GuzzleHttp\\Promise\\Promise->waitIfPending()\n  #10 /Users/alessandrominoccheri/Sites/broadway-dynamodb/vendor/guzzlehttp/promises/src/Promise.php(225): GuzzleHttp\\Promise\\Promise->invokeWaitList()\n  #11 /Users/alessandrominoccheri/Sites/broadway-dynamodb/vendor/guzzlehttp/promises/src/Promise.php(267): GuzzleHttp\\Promise\\Promise->waitIfPending()\n  #12 /Users/alessandrominoccheri/Sites/broadway-dynamodb/vendor/guzzlehttp/promises/src/Promise.php(225): GuzzleHttp\\Promise\\Promise->invokeWaitList()\n  #13 /Users/alessandrominoccheri/Sites/broadway-dynamodb/vendor/guzzlehttp/promises/src/Promise.php(62): GuzzleHttp\\Promise\\Promise->waitIfPending()\n  #14 /Users/alessandrominoccheri/Sites/broadway-dynamodb/vendor/aws/aws-sdk-php/src/AwsClientTrait.php(58): GuzzleHttp\\Promise\\Promise->wait()\n  #15 /Users/alessandrominoccheri/Sites/broadway-dynamodb/vendor/aws/aws-sdk-php/src/AwsClientTrait.php(77): Aws\\AwsClient->execute(Object(Aws\\Command))\n  #16 /Users/alessandrominoccheri/Sites/broadway-dynamodb/src/DynamoDbEventStore.php(166): Aws\\AwsClient->__call('transactWriteIt...', Array)\n  #17 /Users/alessandrominoccheri/Sites/broadway-dynamodb/src/DynamoDbEventStore.php(142): Broadway\\EventStore\\DynamoDb\\DynamoDbEventStore->insertMessage(Object(Broadway\\Domain\\DomainMessage))\n  #18 /Users/alessandrominoccheri/Sites/broadway-dynamodb/test/DynamoDbEventStoreTest.php(175): Broadway\\EventStore\\DynamoDb\\DynamoDbEventStore->append('e238fe34-60df-4...', Object(Broadway\\Domain\\DomainEventStream))\n  #19 [internal function]: Tests\\DynamoDbEventStoreTest->testInsertMessageAndVisitEvents()\n  #20 phar:///usr/local/bin/phpunit/phpunit/Framework/TestCase.php(1120): ReflectionMethod->invokeArgs(Object(Tests\\DynamoDbEventStoreTest), Array)\n  #21 phar:///usr/local/bin/phpunit/phpunit/Framework/TestCase.php(971): PHPUnit_Framework_TestCase->runTest()\n  #22 phar:///usr/local/bin/phpunit/phpunit/Framework/TestResult.php(709): PHPUnit_Framework_TestCase->runBare()\n  #23 phar:///usr/local/bin/phpunit/phpunit/Framework/TestCase.php(926): PHPUnit_Framework_TestResult->run(Object(Tests\\DynamoDbEventStoreTest))\n  #24 phar:///usr/local/bin/phpunit/phpunit/Framework/TestSuite.php(728): PHPUnit_Framework_TestCase->run(Object(PHPUnit_Framework_TestResult))\n  #25 phar:///usr/local/bin/phpunit/phpunit/Framework/TestSuite.php(728): PHPUnit_Framework_TestSuite->run(Object(PHPUnit_Framework_TestResult))\n  #26 phar:///usr/local/bin/phpunit/phpunit/TextUI/TestRunner.php(521): PHPUnit_Framework_TestSuite->run(Object(PHPUnit_Framework_TestResult))\n  #27 phar:///usr/local/bin/phpunit/phpunit/TextUI/Command.php(188): PHPUnit_TextUI_TestRunner->doRun(Object(PHPUnit_Framework_TestSuite), Array, true)\n  #28 phar:///usr/local/bin/phpunit/phpunit/TextUI/Command.php(118): PHPUnit_TextUI_Command->run(Array, true)\n  #29 /usr/local/bin/phpunit(583): PHPUnit_TextUI_Command::main()\n  #30 {main}\"\n    [\"type\"]=>\n    string(6) \"client\"\n    [\"code\"]=>\n    string(25) \"UnknownOperationException\"\n    [\"requestId\"]=>\n    string(36) \"924c41b7-9367-4d2b-ae0e-185b42130264\"\n    [\"statusCode\"]=>\n    int(400)\n    [\"result\"]=>\n    NULL\n    [\"request\"]=>\n    array(6) {\n      [\"instance\"]=>\n      string(32) \"000000007c8253a7000000003199846f\"\n      [\"method\"]=>\n      string(4) \"POST\"\n      [\"headers\"]=>\n      array(9) {\n        [\"X-Amz-Security-Token\"]=>\n        string(7) \"[TOKEN]\"\n        [\"Host\"]=>\n        array(1) {\n          [0]=>\n          string(14) \"localhost:8000\"\n        }\n        [\"X-Amz-Target\"]=>\n        array(1) {\n          [0]=>\n          string(36) \"DynamoDB_20120810.TransactWriteItems\"\n        }\n        [\"Content-Type\"]=>\n        array(1) {\n          [0]=>\n          string(26) \"application/x-amz-json-1.0\"\n        }\n        [\"User-Agent\"]=>\n        array(1) {\n          [0]=>\n          string(18) \"aws-sdk-php/3.80.2\"\n        }\n        [\"aws-sdk-invocation-id\"]=>\n        array(1) {\n          [0]=>\n          string(32) \"XXX\"\n        }\n        [\"aws-sdk-retry\"]=>\n        array(1) {\n          [0]=>\n          string(3) \"0/0\"\n        }\n        [\"X-Amz-Date\"]=>\n        array(1) {\n          [0]=>\n          string(16) \"20181209T211315Z\"\n        }\n        [\"Authorization\"]=>\n        array(1) {\n          [0]=>\n          string(203) \"AWS4-HMAC-SHA256 Credential=not-a-real-key/20181209/us-west-2/dynamodb/aws4_request, SignedHeaders=host;x-amz-date;x-amz-target, Signature=[SIGNATURE]\n        }\n      }\n      [\"body\"]=>\n      string(784) \"{\"TransactItems\":[{\"Put\":{\"TableName\":\"dynamo_table\",\"Item\":{\"id\":{\"S\":\"67b74020-7199-4b51-8895-ecd9d9f2d7b9\"},\"uuid\":{\"S\":\"e238fe34-60df-4980-8797-6294cbc5840e\"},\"playhead\":{\"N\":\"4065\"},\"metadata\":{\"S\":\"{\\\"class\\\":\\\"Broadway\\\\Domain\\\\Metadata\\\",\\\"payload\\\":{\\\"values\\\":{\\\"id\\\":\\\"e238fe34-60df-4980-8797-6294cbc5840e\\\",\\\"foo\\\":\\\"bar\\\"}}}\"},\"payload\":{\"S\":\"{\\\"class\\\":\\\"class@anonymous\\u0000\\\\/Users\\\\/alessandrominoccheri\\\\/Sites\\\\/broadway-dynamodb\\\\/test\\\\/DynamoDbEventStoreTest.php0x1109ed254\\\",\\\"payload\\\":[]}\"},\"recorded_on\":{\"S\":\"2018-12-09T21:13:15.997248+00:00\"},\"type\":{\"S\":\"class@anonymous\\u0000\\/Users\\/alessandrominoccheri\\/Sites\\/broadway-dynamodb\\/test\\/DynamoDbEventStoreTest.php0x1109ed254\"}}}}],\"ClientRequestToken\":\"e741f91e-8b5e-4923-b57b-5aa49eb0aa5b\"}\"\n      [\"scheme\"]=>\n      string(4) \"http\"\n      [\"port\"]=>\n      int(8000)\n    }\n    [\"response\"]=>\n    array(4) {\n      [\"instance\"]=>\n      string(32) \"000000007c82539c000000003199846f\"\n      [\"statusCode\"]=>\n      int(400)\n      [\"headers\"]=>\n      array(5) {\n        [\"X-Amz-Security-Token\"]=>\n        string(7) \"[TOKEN]\"\n        [\"Content-Type\"]=>\n        array(1) {\n          [0]=>\n          string(26) \"application/x-amz-json-1.0\"\n        }\n        [\"x-amzn-RequestId\"]=>\n        array(1) {\n          [0]=>\n          string(36) \"924c41b7-9367-4d2b-ae0e-185b42130264\"\n        }\n        [\"Content-Length\"]=>\n        array(1) {\n          [0]=>\n          string(3) \"119\"\n        }\n        [\"Server\"]=>\n        array(1) {\n          [0]=>\n          string(23) \"Jetty(8.1.12.v20130726)\"\n        }\n      }\n      [\"body\"]=>\n      string(119) \"{\"__type\":\"com.amazonaws.dynamodb.v20120810#UnknownOperationException\",\"message\":\"An unknown operation was requested.\"}\"\n    }\n  }\nInclusive step time: 0.016627073287964\n```. I have update version to 3.81.3 but the error still the same . I'm using this docker and I think is not updated with Transaction\nhttps://hub.docker.com/r/amazon/dynamodb-local/\nI need to wait a new docker or there is a new updated version?. ",
    "bcdxn": "My team is also using the docker hub image (linked above) for local development. Any timeline for when that image will be updated to support transactions? Thanks!. ",
    "fedor-softgrad": "It works, thank you! I used the IDE autocomplete feature and didn't pay attention to this.. ",
    "alexvaque": "Using the Amazon 2018.03 x86_64 we found the same issue than you and just doing the following : \n\nyum downgrade libcurl curl libcurl-devel python27-pycurl  -y\n\nwe fixed the problem!\nNow with this package version is running fine:\ncurl-7.53.1-16.84.amzn1.x86_64\npython27-pycurl-7.19.0-17.12.amzn1.x86_64\nlibcurl-devel-7.53.1-16.84.amzn1.x86_64\nlibcurl-7.53.1-16.84.amzn1.x86_64. > I suggest opening a new support case for Premium Support under the EC2-Linux service suggesting contact with the Amazon Linux service team for additional support on this from the OS side as they will have more expertise with Amazon Linux releases and their corresponding package updates.\nDone. Amazon said that one solution is to downgrade as we did or try to compile the most recent CURL version.\nAlso, perhaps the problem is not only for Amazon Linux. Maybe others distributions could have the same issue.  They are investigating this.\n== References ==\nhttps://alas.aws.amazon.com/ALAS-2018-1112.html \nhttps://access.redhat.com/security/cve/CVE-2017-8816 \nhttps://access.redhat.com/security/cve/cve-2018-14618 \nhttps://curl.haxx.se/docs/CVE-2018-14618.html \nhttps://src.fedoraproject.org/rpms/curl/raw/f27/f/0022-curl-7.55.1-CVE-2018-14618.patch \nhttps://github.com/curl/curl/commit/57d299a499155d4b327e341c6024e293b0418243.patch \n. new Curl version released but seems that have the same behavior than the previous 7.53.1-16.85\ncurl-7.53.1-16.86.amzn1.x86_64\nlibcurl-devel-7.53.1-16.86.amzn1.x86_64\nlibcurl-7.53.1-16.86.amzn1.x86_64. ",
    "cw-tomita": "@alexvaque \nThanks for the report!\nYeah, we also manually downgraded the curl version and it's working fine so far.\nBut we don't want to fix the curl version forever and it can easily happen we start using the updated one in some other services or somewhere and pay extra EC2 fees. So, hopefully the performance issues is fixed in the newer version.. \ud83d\ude4f\nI also had made the ticket on AWS support. I should have written about it. But maybe two inquiries would attract more attention than only one inquiry. \ud83d\ude00. ",
    "vbarbarosh": "The problem still exists. But it seems to be in guzzlehttp/guzzle. It happens when using hight number (e.g. 50) for cuncurrency and only in google-appengine/php* images.\nhttps://github.com/GoogleCloudPlatform/php-docker/issues/468\nYou can reproduce it in the following way:\n$ cat > index.php\n<?php\n\nif (!shell_exec('which composer')) {\n    passthru('curl https://getcomposer.org/composer.phar > /usr/bin/composer && chmod a+x /usr/bin/composer');\n    passthru('apt update && apt install -y git');\n}\n\npassthru('composer require aws/aws-sdk-php');\n\nrequire_once 'vendor/autoload.php';\n\n$mem1 = meminfo();\n\n$s3_client = new \\Aws\\S3\\S3Client([\n    'credentials' => [\n        'key' => 'xxxxxxxxxx',\n        'secret' => 'xxxxxxxxxx',\n    ],\n    'region' => 'us-east-1',\n    'version' => '2006-03-01'\n]);\n\n$commands = [];\nforeach (range(1, 200) as $tmp) {\n    $commands[] = $s3_client->getCommand('PutObject', [\n        'Bucket' => 'xxxxxxxxxx',\n        'Key' => \"tmp/guzzle-issue/$tmp.txt\",\n        'Body' => $tmp,\n        'ContentType' => 'text/plain'\n    ]);\n}\n\n$pool = new \\Aws\\CommandPool($s3_client, $commands, ['concurrency' => 100]);\n$pool->promise()->wait();\n\n$mem2 = meminfo();\necho sprintf(\"consumed memory: %s\\n\", format_bytes($mem1['free'] - $mem2['free']));\n\nfunction meminfo()\n{\n    preg_match('/^MemFree:\\s*(\\d+)/m', file_get_contents('/proc/meminfo'), $free);\n    preg_match('/^MemAvailable:\\s*(\\d+)/m', file_get_contents('/proc/meminfo'), $available);\n    $free = $free[1]*1024;\n    $available = $available[1]*1024;\n    return compact('free', 'available');\n}\n\nfunction format_bytes($bytes)\n{\n    if ($bytes < 1000) {\n        return number_format($bytes, 2);\n    }\n    $kilo = $bytes/1024;\n    if ($kilo < 1000) {\n        return number_format($kilo, 2) . 'K';\n    }\n    $mega = $kilo/1024;\n    if ($mega < 1000) {\n        return number_format($mega, 2) . 'M';\n    }\n    $giga = $mega/1024;\n    if ($giga < 1000) {\n        return number_format($giga, 2) . 'G';\n    }\n    return number_format($giga/1024, 2) . 'T';\n}\n\n---end---\n$ docker run --rm -v $PWD/index.php:/app/index.php:ro gcr.io/google-appengine/php70 php /app/index.php\n[...]\nconsumed memory: 1.53G\n\n$ docker run --rm -v $PWD/index.php:/app/index.php:ro gcr.io/google-appengine/php71 php /app/index.php\n[...]\nconsumed memory: 1.83G\n\n$ docker run --rm -v $PWD/index.php:/app/index.php:ro gcr.io/google-appengine/php72 php /app/index.php\n[...]\nconsumed memory: 1.81G\n\n$ docker run --rm -v $PWD/index.php:/app/index.php:ro php:7.0 php /app/index.php\n[...]\nconsumed memory: 107.22M\n\n$ docker run --rm -v $PWD/index.php:/app/index.php:ro php:7.1 php /app/index.php\n[...]\nconsumed memory: 109.87M\n\n$ docker run --rm -v $PWD/index.php:/app/index.php:ro php:7.2 php /app/index.php\n[...]\nconsumed memory: 103.11M\n\n$ docker run --rm -v $PWD/index.php:/app/index.php:ro php:7.3 php /app/index.php\n[...]\nconsumed memory: 108.81M\n\n. ",
    "MattStypa": "I have the same problem right now. I have done some research into this. I will provide my findings soon, including demo.. I think this is a dupe of #1701. I provided my findings there.. This looks like the same issue as #1700 . # Problem\nIf the InstanceProfileProvider throws an error the CredentialProvider will get stuck in an infinite loop of errors. \nPrerequisites\n\nUsing instance metadata to provide credentials to aws-sdk-php\n\nHow to replicate\n\nStart the demo code with DISABLED network connection.\nEnable network connection.\nThe code will continue FAILING indefinitely.\n\nReal world scenario\n\nLaravel worker is processing Queue.\nAWS credentials expire after few hours.\nHTTP request to http://169.254.169.254/latest/meta-data/iam/security-credentials/ times out.\nInfinite loop of failures.\n\nProposed solution\nThe CredentialProvider should clean up when Promise is rejected.\nDemo code\n```\n<?php\nrequire DIR . '/vendor/autoload.php';\n$client = new Aws\\Sqs\\SqsClient([\n    'region' => 'us-east-1',\n    'version' => 'latest'\n]);\nwhile(true) {\n    try {\n        echo $client->getQueueUrl(['QueueName' => my_queue_name'])->get('QueueUrl') . \"\\n\";\n    } catch(Exception $e) {\n        echo \"FAIL\\n\";\n    }\nsleep(1);\n\n}\n```. ",
    "darkpsart": "Thanks @MattStypa for clarify the problem and how to fix it.. ",
    "kostiag": "I am so sorry. PHP version is 5.6 not 3.6 I just realized my mistake...\nNot sure I follow though how is 3.38 ahead of 3.6? I was trying to use sdk version 3.6 thats why I am confused why Crypto is pulled with 3.38 but not 3.6.\nbut ok that seems to have done it and installing 3.38 does the trick. . ",
    "jinjingbo": "@diehlaws \nI am facing the same issue. I have tried the third party php GraphQL clients you listed above. \nMy AppSync is using Cognito token for authorization. I can generate Cognito token successfully using AWS PHP SDK.\nHowever I cannot find PHP equivalent of Amplify federatedSignIn().  Without it, I cannot signing the query request from PHP server to AppSync.\nCould you give me some information/direction on:\n1. Is it possible to utilise AWS php SDK to implement federatedSignIn() ?\n2. If change the AppSync auth method to API_KEY Authorization (https://docs.aws.amazon.com/appsync/latest/devguide/security.html#aws-appsync-security), can we use any PHP SDK method to make the request?. ",
    "sordev": "work around was to check if it's actually unix timestamp or not. If not just return as is for now.. Thank you, hope fix comes soon :) . Thank you will check and let you know \ud83d\udc4d . ",
    "alex-at-cascade": "Thanks for your response and suggestions.  I understand that's a very common use case, to download entire \"directories\", assuming slash as a path separator.  (Although, to be honest, I couldn't find any documentation of that limitation.  I had been assuming normal S3 conventions, as is also followed by V2 of the SDK.)  It's very close to being able to support everything the standard S3 API can, other than the extraneous assumed path separator.  Just curious, would it be feasible to add an option in the $options array to leave the prefix as just a prefix (without mangling it)?. ",
    "sharanyaa": "Hi @diehlaws , thank you for your response. I tried increasing the memory limit like you suggested, but the fatal error reoccured when the allocated memory was exhausted. \nI was noticing the same issue occur when assuming a role with StsClient, trying to auth with an instance profile - Here's a test script I used to look into the issue. [aws/aws-sdk-php v3.87.2, guzzlehttp/guzzle v6.3.3]\n```\n<?php\nrequire 'vendor/autoload.php';\nuse Aws\\Exception\\AwsException;\nuse Aws\\Sts\\StsClient;\nfunction stsAssumeRole() {\n    $config = [\n        'version' => 'latest',\n        'region' => 'us-east-1',\n    ];\n    try {\n        $stsClient = new StsClient($config);\n        $result = $stsClient->assumeRole([\n            'RoleArn' => 'role_arn',\n            'RoleSessionName' => 'role_session_name',\n            'DurationSeconds' => 3600,\n        ]);\n        var_dump($result);\n    } catch (AwsException $e) {\n        // output error message if fails\n        var_dump($e->getMessage());\n    }\n}\nstsAssumeRole();\n?>\n```\nHere's the stacktrace from running this code with a memory limit of 512M.\n\nLooking into this further, it looks like more memory is used with each retry to read .aws/credentials and config.\nclock_gettime(CLOCK_MONOTONIC, {8437328, 61741166}) = 0\naccess(\"~/.aws/credentials\", R_OK) = -1 ENOENT (No such file or directory)\naccess(\"~/.aws/config\", R_OK) = -1 ENOENT (No such file or directory)\nbrk(0)                                  = 0x55f6628fe000\nbrk(0x55f662922000)                     = 0x55f662922000\nclock_gettime(CLOCK_MONOTONIC, {8437328, 62741526}) = 0\naccess(\"~/.aws/credentials\", R_OK) = -1 ENOENT (No such file or directory)\naccess(\"~/.aws/config\", R_OK) = -1 ENOENT (No such file or directory)\nclock_gettime(CLOCK_MONOTONIC, {8437328, 63633096}) = 0\naccess(\"~/.aws/credentials\", R_OK) = -1 ENOENT (No such file or directory)\naccess(\"~/.aws/config\", R_OK) = -1 ENOENT (No such file or directory)\nclock_gettime(CLOCK_MONOTONIC, {8437328, 64496996}) = 0\naccess(\"~/.aws/credentials\", R_OK) = -1 ENOENT (No such file or directory)\naccess(\"~/.aws/config\", R_OK) = -1 ENOENT (No such file or directory)\nclock_gettime(CLOCK_MONOTONIC, {8437328, 65356329}) = 0\naccess(\"~/.aws/credentials\", R_OK) = -1 ENOENT (No such file or directory)\naccess(\"~/.aws/config\", R_OK) = -1 ENOENT (No such file or directory)\nbrk(0)                                  = 0x55f662922000\nbrk(0x55f662946000)                     = 0x55f662946000\nclock_gettime(CLOCK_MONOTONIC, {8437328, 66404422}) = 0\naccess(\"~/.aws/credentials\", R_OK) = -1 ENOENT (No such file or directory)\naccess(\"~/.aws/config\", R_OK) = -1 ENOENT (No such file or directory)\nclock_gettime(CLOCK_MONOTONIC, {8437328, 67295000}) = 0\naccess(\"~/.aws/credentials\", R_OK) = -1 ENOENT (No such file or directory)\naccess(\"~/.aws/config\", R_OK) = -1 ENOENT (No such file or directory)\nclock_gettime(CLOCK_MONOTONIC, {8437328, 68217803}) = 0\naccess(\"~/.aws/credentials\", R_OK) = -1 ENOENT (No such file or directory)\naccess(\"~/.aws/config\", R_OK) = -1 ENOENT (No such file or directory)\nclock_gettime(CLOCK_MONOTONIC, {8437328, 69185933}) = 0\naccess(\"~/.aws/credentials\", R_OK) = -1 ENOENT (No such file or directory)\naccess(\"~/.aws/config\", R_OK) = -1 ENOENT (No such file or directory)\nbrk(0)                                  = 0x55f662946000\nbrk(0x55f66296a000)                     = 0x55f66296a000 \nPlease let me know if you you need additional information. Thanks!\n. Hi @diehlaws, have you been able to reproduce this behavior? Please let me know if you'll require more info to investigate.. Update: On debugging the issue further, it looks like when there are no local credential providers, CredentialProvider infinitely loops on rejected promises (from #1705) and doesn't hit remote credential providers. The issue was resolved by downgrading to v3.83.0 of the aws sdk.. ",
    "martinssipenko": "Update: I belive this is because SDK adds X-Amz-Security-Token header, if I add this header to post request I make using postman, I get the same error as with SDK.. Hi @diehlaws,\nLooks like I got this wrong, the issue was indeed our own OIDC which took more than 5 seconds to respond to AWS STS with configuration and jwks, which lead to timeouts on STS side.\nOn a related topic, if I don't specify 'credentials' => false, when constructing StsClient, the assumeRoleWithWebIdentity call fails in case there are no credentials present. At the end SDK tries to get credentials from EC2 meta and fails with exception. This API call does not require auth, so perhaps this could be fixed?\nOtherwise feel free to close this.. @howardlopez I've added an extra assertion, I hope it does what you meant.. my bad, fixed now\n. ",
    "kevinulrich": "Hey, sorry I wasn't responding earlier, I'm a little tied up these days.\nSo this behaviour does absolutely not happen wenn the AWS SDK is used with the actual AWS S3 API. It does however occur when using other S3 API implementations such as adobe/s3mock.\nWhen querying a nonexistant path, the AWS API will not return any <Contents> or <CommonPrefixes>. The problem that I'm having is that the implementation of adobe/s3mock actually returns a self-closing <CommonPrefixes /> tag, which will in turn cause the PHP SDK to return an array of 1 empty array which then again will cause is_dir() to return true.\nI don't think you could call it a bug on either side really but it's just some edge-case behaviour that should probably be sorted out on both sides. I have raised the issue with the mentioned library as well.. ",
    "joshkaplan": "Update: it's $credentials. On production, $credentials are null, and the SDK automatically fetches the creds using STS. The request to STS does not appear to use the passed in CurlMultiHandler, so the tick() does nothing.  If I perform one synchronous call with $sqs_client before queueing up the promises and ticking curl, ticking processes the requests as expected (since the creds have already been fetched and cached.)\n\nIs this a bug? Should the AWS SDK use the passed in http_handler for internal requests like this?\nFor now, how should I handle? I think I need to override credentials with a new CredentialProvider which will either use the same CurlMultiHandler, or somehow manually fetch and re-fetch the tokens from STS when I need them.. We are using the default provider. I guess I\u2019m misunderstanding exactly what it does. Does it send requests at all? My understanding was it does so lazily when assuming an IAM role. \n\nCustom provider makes sense to me. . ",
    "bwg": "well I screwed this up bad.  put the changefile in the wrong dir, didn't have a local branch.  closing to save myself some dignity.. ",
    "GordonLesti": "Hello @diehlaws \nthank you for your support. Here is the debug output. I have replaced the root directory of my project by path_to_project and I have replaced the bucket name by bucket-example/name-with-slash.\n```\n-> Entering step init, name 'idempotency_auto_fill'\ncommand was set to /path_to_project/vendor/aws/aws-sdk-php/src/TraceMiddleware.php:261:\n  array(3) {\n    'instance' =>\n    string(32) \"00000000081702ac000000000400e709\"\n    'name' =>\n    string(9) \"PutObject\"\n    'params' =>\n    array(5) {\n      'Bucket' =>\n      string(35) \"bucket-example/name-with-slash\"\n      'Key' =>\n      string(8) \"test_key\"\n      'Body' =>\n      class GuzzleHttp\\Psr7\\Stream#80 (7) {\n        private $stream =>\n        resource(134) of type (stream)\n        private $size =>\n        int(9)\n        private $seekable =>\n        bool(true)\n        private $readable =>\n        bool(true)\n        private $writable =>\n        bool(true)\n        private $uri =>\n        string(10) \"php://temp\"\n        private $customMetadata =>\n        array(0) {\n          ...\n        }\n      }\n      'ACL' =>\n      string(7) \"private\"\n      '@http' =>\n      array(1) {\n        'debug' =>\n        resource(145) of type (stream)\n      }\n    }\n  }\nrequest was set to /path_to_project/vendor/aws/aws-sdk-php/src/TraceMiddleware.php:261:\n  array(0) {\n  }\n-> Entering step init, name 's3.ssec'\nno changes\n-> Entering step init, name 's3.source_file'\nno changes\n-> Entering step init, name 's3.save_as'\nno changes\n-> Entering step init, name 's3.location'\nno changes\n-> Entering step init, name 's3.auto_encode'\nno changes\n-> Entering step init, name 's3.head_object'\nno changes\n-> Entering step validate, name 'validation'\nno changes\n-> Entering step build, name 'builder'\nrequest.instance was set to 0000000008170249000000000400e709\n  request.method was set to PUT\n  request.headers was set to /path_to_project/vendor/aws/aws-sdk-php/src/TraceMiddleware.php:261:\n  array(3) {\n    'X-Amz-Security-Token' =>\n    string(7) \"[TOKEN]\"\n    'Host' =>\n    array(1) {\n      [0] =>\n      string(26) \"s3.eu-west-1.amazonaws.com\"\n    }\n    'x-amz-acl' =>\n    array(1) {\n      [0] =>\n      string(7) \"private\"\n    }\n  }\nrequest.body was set to test_body\n  request.scheme was set to https\n  request.path was set to /bucket-example%2Fname-with-slash/test_key\n-> Entering step build, name ''\nrequest.instance changed from 0000000008170249000000000400e709 to 000000000817024b000000000400e709\n  request.headers.User-Agent was set to /path_to_project/vendor/aws/aws-sdk-php/src/TraceMiddleware.php:261:\n  array(1) {\n    [0] =>\n    string(19) \"aws-sdk-php/3.87.19\"\n  }\n-> Entering step build, name 'ApiCallMonitoringMiddleware'\nno changes\n-> Entering step build, name 'endpoint_parameter'\nno changes\n-> Entering step build, name 'EndpointDiscoveryMiddleware'\nno changes\n-> Entering step build, name 's3.checksum'\nno changes\n-> Entering step build, name 's3.content_type'\nrequest.instance changed from 000000000817024b000000000400e709 to 0000000008170246000000000400e709\n  request.headers.Content-Type was set to /path_to_project/vendor/aws/aws-sdk-php/src/TraceMiddleware.php:261:\n  array(1) {\n    [0] =>\n    string(24) \"application/octet-stream\"\n  }\n-> Entering step build, name 's3.endpoint_middleware'\nno changes\n-> Entering step sign, name 'invocation-id'\nrequest.instance changed from 0000000008170246000000000400e709 to 0000000008170242000000000400e709\n  request.headers.aws-sdk-invocation-id was set to /path_to_project/vendor/aws/aws-sdk-php/src/TraceMiddleware.php:261:\n  array(1) {\n    [0] =>\n    string(32) \"e6bb0a3caf3482626b52dc4fe02d4a8b\"\n  }\n-> Entering step sign, name 'retry'\nrequest.instance changed from 0000000008170242000000000400e709 to 000000000817023d000000000400e709\n  request.headers.aws-sdk-retry was set to /path_to_project/vendor/aws/aws-sdk-php/src/TraceMiddleware.php:261:\n  array(1) {\n    [0] =>\n    string(3) \"0/0\"\n  }\n-> Entering step sign, name 'signer'\nrequest.instance changed from 000000000817023d000000000400e709 to 0000000008170269000000000400e709\n  request.headers.X-Amz-Content-Sha256 was set to /path_to_project/vendor/aws/aws-sdk-php/src/TraceMiddleware.php:261:\n  array(1) {\n    [0] =>\n    string(64) \"4443c6a8412e6c11f324c870a8366d6ede75e7f9ed12f00c36b88d479df371d6\"\n  }\nrequest.headers.X-Amz-Date was set to /path_to_project/vendor/aws/aws-sdk-php/src/TraceMiddleware.php:261:\n  array(1) {\n    [0] =>\n    string(16) \"20190304T104322Z\"\n  }\nrequest.headers.Authorization was set to /path_to_project/vendor/aws/aws-sdk-php/src/TraceMiddleware.php:261:\n  array(1) {\n    [0] =>\n    string(221) \"AWS4-HMAC-SHA256 Credential=[KEY]/20190304/eu-west-1/s3/aws4_request, SignedHeaders=host;x-amz-acl;x-amz-content-sha256;x-amz-date, Signature=[SIGNATURE]\n  }\n-> Entering step sign, name 's3.put_object_url'\nno changes\n-> Entering step sign, name 's3.permanent_redirect'\nno changes\n-> Entering step attempt, name 'ApiCallAttemptMonitoringMiddleware'\nno changes\n\nTrying 54.231.134.123...\nTCP_NODELAY set\nConnected to s3.eu-west-1.amazonaws.com (54.231.134.123) port 443 (#0)\nALPN, offering http/1.1\nCipher selection: ALL:!EXPORT:!EXPORT40:!EXPORT56:!aNULL:!LOW:!RC4:@STRENGTH\nsuccessfully set certificate verify locations:\nCAfile: /etc/ssl/certs/ca-certificates.crt\n  CApath: /etc/ssl/certs\nSSL connection using TLSv1.2 / ECDHE-RSA-AES128-GCM-SHA256\nALPN, server did not agree to a protocol\nServer certificate:\nsubject: C=US; ST=Washington; L=Seattle; O=Amazon.com Inc.; CN=*.s3-eu-west-1.amazonaws.com\nstart date: Nov  8 00:00:00 2018 GMT\nexpire date: Nov  6 12:00:00 2019 GMT\nsubjectAltName: host \"s3.eu-west-1.amazonaws.com\" matched cert's \"s3.eu-west-1.amazonaws.com\"\nissuer: C=US; O=DigiCert Inc; OU=www.digicert.com; CN=DigiCert Baltimore CA-2 G2\n\nSSL certificate verify ok.\n\nPUT /bucket-example%2Fname-with-slash/test_key HTTP/1.1\nHost: s3.eu-west-1.amazonaws.com\nx-amz-acl: private\nContent-Type: application/octet-stream\naws-sdk-invocation-id: e6bb0a3caf3482626b52dc4fe02d4a8b\naws-sdk-retry: 0/0\nX-Amz-Content-Sha256: 4443c6a8412e6c11f324c870a8366d6ede75e7f9ed12f00c36b88d479df371d6\nX-Amz-Date: 20190304T104322Z\nAuthorization: AWS4-HMAC-SHA256 Credential=[KEY]/20190304/eu-west-1/s3/aws4_request, SignedHeaders=host;x-amz-acl;x-amz-content-sha256;x-amz-date, Signature=[SIGNATURE]\nUser-Agent: aws-sdk-php/3.87.19 GuzzleHttp/6.3.3 curl/7.52.1 PHP/7.0.33-0+deb9u2\nContent-Length: 9\n\n\n\nupload completely sent off: 9 out of 9 bytes\n< HTTP/1.1 403 Forbidden\n< x-amz-request-id: AF1287D4E082242F\n< x-amz-id-2: /5v06x16jr6AOgmnoHoSnE0ZcoLBI8hoQuehez4c8QbOlxZwGtnE8fqBMwEuUeFwhxbz2VhDzbk=\n< Content-Type: application/xml\n< Transfer-Encoding: chunked\n< Date: Mon, 04 Mar 2019 10:43:22 GMT\n< Connection: close\n< Server: AmazonS3\n<\n\nCurl_http_done: called premature == 0\nClosing connection 0\n\n<- Leaving step attempt, name 'ApiCallAttemptMonitoringMiddleware'\nerror was set to /path_to_project/vendor/aws/aws-sdk-php/src/TraceMiddleware.php:261:\n  array(13) {\n    'instance' =>\n    string(32) \"000000000817027e000000000400e709\"\n    'class' =>\n    string(28) \"Aws\\S3\\Exception\\S3Exception\"\n    'message' =>\n    string(3057) \"Error executing \"PutObject\" on \"https://s3.eu-west-1.amazonaws.com/bucket-example%2Fname-with-slash/test_key\"; AWS HTTP error: Client error: PUT https://s3.eu-west-1.amazonaws.com/bucket-example%2Fname-with-slash/test_key resulted in a 403 Forbidden response:\n  <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n  SignatureDoesNotMatchThe request signature we calcul (truncated...)\n   SignatureDoesNotMatch (client): The request signature we calculated does not match the signature you \"...\n    'file' =>\n    string(96) \"/path_to_project/vendor/aws/aws-sdk-php/src/WrappedHttpHandler.php\"\n    'line' =>\n    int(191)\n    'trace' =>\n    string(2452) \"#0 /path_to_project/vendor/aws/aws-sdk-php/src/WrappedHttpHandler.php(100): Aws\\WrappedHttpHandler->parseError(Array, Object(GuzzleHttp\\Psr7\\Request), Object(Aws\\Command), Array)\n  #1 /path_to_project/vendor/guzzlehttp/promises/src/Promise.php(203): Aws\\WrappedHttpHandler->Aws{closure}(Array)\n  #2 /path_to_project/vendor/guzzlehttp/promises/src/Promise.php(174): GuzzleHttp\\Promise\\Promise::callHandler(2, Array, Array)\n  #3 \"...\n    'type' =>\n    string(6) \"client\"\n    'code' =>\n    string(21) \"SignatureDoesNotMatch\"\n    'requestId' =>\n    string(16) \"AF1287D4E082242F\"\n    'statusCode' =>\n    int(403)\n    'result' =>\n    NULL\n    'request' =>\n    array(6) {\n      'instance' =>\n      string(32) \"0000000008170269000000000400e709\"\n      'method' =>\n      string(3) \"PUT\"\n      'headers' =>\n      array(10) {\n        'X-Amz-Security-Token' =>\n        string(7) \"[TOKEN]\"\n        'Host' =>\n        array(1) {\n          ...\n        }\n        'x-amz-acl' =>\n        array(1) {\n          ...\n        }\n        'User-Agent' =>\n        array(1) {\n          ...\n        }\n        'Content-Type' =>\n        array(1) {\n          ...\n        }\n        'aws-sdk-invocation-id' =>\n        array(1) {\n          ...\n        }\n        'aws-sdk-retry' =>\n        array(1) {\n          ...\n        }\n        'X-Amz-Content-Sha256' =>\n        array(1) {\n          ...\n        }\n        'X-Amz-Date' =>\n        array(1) {\n          ...\n        }\n        'Authorization' =>\n        array(1) {\n          ...\n        }\n      }\n      'body' =>\n      string(9) \"test_body\"\n      'scheme' =>\n      string(5) \"https\"\n      'path' =>\n      string(47) \"/bucket-example%2Fname-with-slash/test_key\"\n    }\n    'response' =>\n    array(4) {\n      'instance' =>\n      string(32) \"000000000817023f000000000400e709\"\n      'statusCode' =>\n      int(403)\n      'headers' =>\n      array(8) {\n        'X-Amz-Security-Token' =>\n        string(7) \"[TOKEN]\"\n        'x-amz-request-id' =>\n        array(1) {\n          ...\n        }\n        'x-amz-id-2' =>\n        array(1) {\n          ...\n        }\n        'Content-Type' =>\n        array(1) {\n          ...\n        }\n        'Transfer-Encoding' =>\n        array(1) {\n          ...\n        }\n        'Date' =>\n        array(1) {\n          ...\n        }\n        'Connection' =>\n        array(1) {\n          ...\n        }\n        'Server' =>\n        array(1) {\n          ...\n        }\n      }\n      'body' =>\n      string(2498) \"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n  SignatureDoesNotMatchThe request signature we calculated does not match the signature you provided. Check your key and signing method.AKIAJWMMFC4ZW55EFG6AAWS4-HMAC-SHA256\n  20190304T104322Z\n  20190304/eu-west-1/s3/aws4_request\n  54ad9b1b7ca9cd0393d274a98c87de2cdd83e6af4b4127a6146591dd308680ed23c4988a312cfe9cdb96773aff3cd1ea0103180ff62b766ee3fc38dffd4d0795</\"...\n    }\n  }\nInclusive step time: 0.26826000213623\n<- Leaving step sign, name 's3.permanent_redirect'\nno changes\n  Inclusive step time: 0.26882696151733\n<- Leaving step sign, name 's3.put_object_url'\nno changes\n  Inclusive step time: 0.26935696601868\n<- Leaving step sign, name 'signer'\nno changes\n  Inclusive step time: 0.26989102363586\n<- Leaving step sign, name 'retry'\nno changes\n  Inclusive step time: 0.27158904075623\n<- Leaving step sign, name 'invocation-id'\nno changes\n  Inclusive step time: 0.27221703529358\n<- Leaving step build, name 's3.endpoint_middleware'\nno changes\n  Inclusive step time: 0.27262783050537\n<- Leaving step build, name 's3.content_type'\nno changes\n  Inclusive step time: 0.27319407463074\n<- Leaving step build, name 's3.checksum'\nno changes\n  Inclusive step time: 0.2736279964447\n<- Leaving step build, name 'EndpointDiscoveryMiddleware'\nno changes\n  Inclusive step time: 0.274001121521\n<- Leaving step build, name 'endpoint_parameter'\nno changes\n  Inclusive step time: 0.2744128704071\n<- Leaving step build, name 'ApiCallMonitoringMiddleware'\nno changes\n  Inclusive step time: 0.27483820915222\n<- Leaving step build, name ''\nno changes\n  Inclusive step time: 0.27647399902344\n<- Leaving step build, name 'builder'\nno changes\n  Inclusive step time: 0.27696681022644\n<- Leaving step validate, name 'validation'\nno changes\n  Inclusive step time: 0.27816414833069\n<- Leaving step init, name 's3.head_object'\nno changes\n  Inclusive step time: 0.27856588363647\n<- Leaving step init, name 's3.auto_encode'\nno changes\n  Inclusive step time: 0.27892899513245\n<- Leaving step init, name 's3.location'\nno changes\n  Inclusive step time: 0.27940607070923\n<- Leaving step init, name 's3.save_as'\nno changes\n  Inclusive step time: 0.27975702285767\n<- Leaving step init, name 's3.source_file'\nno changes\n  Inclusive step time: 0.28015112876892\n<- Leaving step init, name 's3.ssec'\nno changes\n  Inclusive step time: 0.28056788444519\n<- Leaving step init, name 'idempotency_auto_fill'\nno changes\n  Inclusive step time: 0.2811450958252\n```\nBest Regards\nGordon. Hello @diehlaws \nthank you very much. You are right. I was never questioning the given credentials or was not aware that the toplevel prefix starts after the slash :( I am sorry that I wasted your time.\nBest Regards\nGordon. ",
    "GriffinMeyer": "Aha, that makes perfect sense. A better error would definitely be nice. Thank you for responding!. ",
    "siarheipashkevich": "Thanks for an awesome and up-to-date release @diehlaws!!!. ",
    "SerikK": "So, yes. Debug helped me to understand what is the problem. I have seen in debug that it is trying to put file, but server responses with 301 redirect. So I changed host to https and it is worked. I was trying to connect guzzle log and history plugins long time, but no luck. It did not stored requests. So, thanks.. ",
    "GodfallentheCelestial": "@diehlaws I got a fix. Since I was using xampp I checked the apache/bin directory and found that the cUrl file was missing. I added a certificate file there. Thank you so much.. ",
    "ellllllen": "Hi\nThanks for the response, this is our code:\n```\nuse Aws\\Sdk;\nuse Aws\\Sqs\\SqsClient;\nclass SqsService\n{\n    /*\n     * @var SqsClient\n     /\n    private $client;\npublic function __construct(Sdk $sdk, array $config)\n{\n    $this->client = $sdk->createSqs($config);\n}\n\npublic function sendMessage($message, $url, $groupId, $dedupeId)\n{\n    $this->client->sendMessageAsync([\n        'MessageBody' => $message,\n        'QueueUrl' => $url,\n        'MessageGroupId' => $groupId,\n        'MessageDeduplicationId' => $dedupeId,\n    ]);\n}\n\n}\n```\n. This affecting any emails we are sending async as well. Using method sendEmailAsync().. Might be related to: https://github.com/guzzle/guzzle/issues/1899. Thanks for your help! That has cleared everything up.\n\ud83d\udc4d . ",
    "kamran-jabbar": "@diehlaws thanks for your response. Is there any other way to get file access URL from the private bucket with an access token and key or only way to make the file permission as public?. In my app scenario, I just want to get file within my application, if any other person has the URL of the file that shouldn't be accessible. What is the recommended solution for this?. @diehlaws I have uploaded with access token and key. \nAfter calling the GetObject with the same access token and key getting the following response, is that due to the user has no access with that key and access token, and I have to give permission to that user readability/access from AWS settings? Right?\n```\nAws\\Result Object\n(\n    [data:Aws\\Result:private] => Array\n        (\n            [Body] => GuzzleHttp\\Psr7\\Stream Object\n                (\n                    [stream:GuzzleHttp\\Psr7\\Stream:private] => Resource id #6\n                    [size:GuzzleHttp\\Psr7\\Stream:private] => \n                    [seekable:GuzzleHttp\\Psr7\\Stream:private] => 1\n                    [readable:GuzzleHttp\\Psr7\\Stream:private] => 1\n                    [writable:GuzzleHttp\\Psr7\\Stream:private] => 1\n                    [uri:GuzzleHttp\\Psr7\\Stream:private] => php://temp\n                    [customMetadata:GuzzleHttp\\Psr7\\Stream:private] => Array\n                        (\n                        )\n            )\n\n        [DeleteMarker] => \n        [AcceptRanges] => bytes\n        [Expiration] => \n        [Restore] => \n        [LastModified] => Aws\\Api\\DateTimeResult Object\n            (\n                [date] => 2019-03-13 10:42:08.000000\n                [timezone_type] => 2\n                [timezone] => GMT\n            )\n\n        [ContentLength] => 291073\n        [ETag] => \"xxxx\"\n        [MissingMeta] => \n        [VersionId] => \n        [CacheControl] => \n        [ContentDisposition] => \n        [ContentEncoding] => \n        [ContentLanguage] => \n        [ContentRange] => \n        [ContentType] => application/octet-stream\n        [Expires] => Aws\\Api\\DateTimeResult Object\n            (\n                [date] => 2019-03-13 23:51:55.892883\n                [timezone_type] => 3\n                [timezone] => xxxx\n            )\n\n        [WebsiteRedirectLocation] => \n        [ServerSideEncryption] => \n        [Metadata] => Array\n            (\n            )\n\n        [SSECustomerAlgorithm] => \n        [SSECustomerKeyMD5] => \n        [SSEKMSKeyId] => \n        [StorageClass] => \n        [RequestCharged] => \n        [ReplicationStatus] => \n        [PartsCount] => \n        [TagCount] => \n        [ObjectLockMode] => \n        [ObjectLockRetainUntilDate] => Aws\\Api\\DateTimeResult Object\n            (\n                [date] => 2019-03-13 23:51:55.893702\n                [timezone_type] => 3\n                [timezone] => xxx\n            )\n\n        [ObjectLockLegalHoldStatus] => \n        [@metadata] => Array\n            (\n                [statusCode] => 200\n                [effectiveUri] => xxxxx\n                [headers] => Array\n                    (\n                        [x-amz-id-2] => xxxx\n                        [x-amz-request-id] => xxx\n                        [date] => Wed, 13 Mar 2019 18:52:29 GMT\n                        [last-modified] => Wed, 13 Mar 2019 10:42:08 GMT\n                        [etag] => \"xxxx\"\n                        [accept-ranges] => bytes\n                        [content-type] => application/octet-stream\n                        [content-length] => 291073\n                        [server] => AmazonS3\n                    )\n\n                [transferStats] => Array\n                    (\n                        [http] => Array\n                            (\n                                [0] => Array\n                                    (\n                                    )\n\n                            )\n\n                    )\n\n            )\n\n    )\n\n[monitoringEvents:Aws\\Result:private] => Array\n    (\n    )\n\n)\n```. ",
    "liubo1985": "Thank you for your reply. It works now. . ",
    "pborreli": "yes, as parent::wait is void\n. ",
    "hannesvdvreken": "I would do ~5.3||^6.0.1 or ~5.3||~6.0.1||~6.1 if you want to avoid the caret sign.\n. what parse error?\n. what you said is what I said then.\n. ",
    "fntlnz": ":+1:  for the coroutine idea!\n. ",
    "vasanthperiyasamy": "Thanks for the feedback. \nIf I undestand correctly, AWS_CONTAINER_CREDENTIALS_RELATIVE_URI env variable is set only for ECS tasks(in containers) that are configured to use \"EC2 Container Service Task Role\". If  ENV var exists and the credential request fails, falling back to instanceProfile will lead to inconsistent response between API calls.\ne.g: ECS Instance role without access to S3::BucketA and ECS Task role with S3::BucketA access\nAction: $client->listObjects();\nSuccess scenario\nprovider: EcsCredentialProvider\nrequest_1: http://169.254.170.2/v2/credentials/{some_uuid}\nresponse_1: HTTP/1.0    200 OK\nresult: [Object1, Object2...]\nFailure scenario - without fallback:\nprovider: EcsCredentialProvider\nrequest_1: http://169.254.170.2/v2/credentials/{some_uuid}\nresponse_1: timeout\nresult:  'CredentialsException - Error retrieving credential from ECS - Timeout'\nFailure scenario - with fallback:\nprovider: EcsCredentialProvider\nrequest_1: http://169.254.170.2/v2/credentials/{some_uuid}\nresponse_1: timeout\nprovider: InstanceProfileProvider\nrequest_2: http://169.254.169.254/latest/meta-data/iam/security-credentials/{some_instance_role}\nresponse_2: HTTP/1.0    200 OK\nresult: 'AccessDenied - ListObjects'\nthoughts?. ",
    "qmcree": "If we don't include the use statement, then we will need to change the docblock to explicitly state the HashContext is at the root namespace (\\HashContext) and not the Aws namespace.. ",
    "eliysha": "This is not working for s3://bucket.s3-regionaws.amazonaws.com/filepath. ",
    "theMadness": "Well, any object that implements that interface from scratch, or extend \\DateTimeImmutable would also match. Personally I'm not entirely sure what we should be testing here. \\DateTime implements \\DateTimeInterface and already passes the tests.. "
}
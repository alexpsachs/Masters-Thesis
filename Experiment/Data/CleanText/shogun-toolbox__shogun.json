{
    "shogun-toolbox": "I think you have to\ngit checkout master\ngit pull\ngit checkout \ngit rebase master\nthen the 16 commits hopefully go down to 1.\n. I think you have to\ngit checkout master\ngit pull\ngit checkout \ngit rebase master\nthen the 16 commits hopefully go down to 1.\n. I am happy with the patch except for one minor thing: please use m_q (or some longer name) for the class member variable. We try to avoid such cases (if the code becomes longer it is sometimes hard to figure out if you meant to access the local or the member variable).\n. I am happy with the patch except for one minor thing: please use m_q (or some longer name) for the class member variable. We try to avoid such cases (if the code becomes longer it is sometimes hard to figure out if you meant to access the local or the member variable).\n. I think it would be better to modify the code of the gaussian kernel to include this optional scaling, introducing two functions set_compact_enabled(bool enable) / get_compact_enabled.\n. I think it would be better to modify the code of the gaussian kernel to include this optional scaling, introducing two functions set_compact_enabled(bool enable) / get_compact_enabled.\n. I realized upon git pull that you didn't set your username / email settings http://help.github.com/git-email-settings/ . Please do and then I can finally merge this :)\n. I realized upon git pull that you didn't set your username / email settings http://help.github.com/git-email-settings/ . Please do and then I can finally merge this :)\n. You didn't it seems it says:\nAuthor: Miguel Angel Bautista bautista@ubuntu.(none)\nor was this intended? If so I will commit otherwise - please change and make sure your name /email appears in the git log.\nIn the worst case you have to save the patches (git format-patch), edit them manually and then push again on a fresh clone.\n. You didn't it seems it says:\nAuthor: Miguel Angel Bautista bautista@ubuntu.(none)\nor was this intended? If so I will commit otherwise - please change and make sure your name /email appears in the git log.\nIn the worst case you have to save the patches (git format-patch), edit them manually and then push again on a fresh clone.\n. Could you please rebase this patch to git master - sorry I was just fixing a few documentation strings before I saw your patch.\n. Could you please rebase this patch to git master - sorry I was just fixing a few documentation strings before I saw your patch.\n. Could you please rebase to git master and send another pull request - thanks!\n. Could you please rebase to git master and send another pull request - thanks!\n. I suggest to ask on IRC if you get stuck. No need to do everything from beginning to end right?\n. I suggest to ask on IRC if you get stuck. No need to do everything from beginning to end right?\n. k_ can also be renamed k :-)\n. there is still an SG_UNSTABLE in here, replace it with a call to init();\n. copyright is only 2011 Berlin Institute of Technology and Max Planck Society\n. Ahh sorry one more thing: The get_name function needs to return the exact class name (without C prefix). Then I am finally happy.\n. Please use proper copyright lines  - include your name, and if you don't have a Copyright line handy use (C) 2011 Berlin Institute of Technology and Max Planck Society\n. Sorry to only now realize this: You shouldn't change D_CUSTOM from 160 to 170 but instead give D_ATTENUATEDEUCLIDIAN=170. Please fix and then I apply. Sorry again for missing it in the first place.\n. some whitespace issue here?\n. here you need to write some description what this kernel computes / when it is useful. the minimum documentation is the formula...\n. please use the coding style\nif (!compact)\n{\n}\n. are these checks really necessary? if so it makes sense to do this test for both cases.\n. You should also change the documentation here  (formular)\n. a rebase is necessary - we are already at 420\n. why do you still have this commented function in here?\n. I guess you don't wnat the int32_t there\n. please add an extra\nprivate:\n then put fucntions there\nthen another private with variables.\n. please use an includepath relative to libshogun\n. SG_UNSTABLE can go here if you add serialization\n. please put the else if on an extra line\n. what do you need iostream for?\n. which license is this then? Is it compatible/\n. you need a git rebase... we are at 440 now already.\n. please return exaclty the class name without the C prefix\n. ",
    "lisitsyn": "Done.\n. this is on-going still, please do not merge it yet\n. Could you please paste the output of python_modular examples when python 3 is used?\n. Thanks, merged.\n. For now please remove temporary .h~ and .i~, they shouldn't be here\n. Sriram,\ncould you please elaborate it more? I know SIFT is non-free thing but this example does not provide much capabilities. For now it just creates features and nothing more..\n. Sriram,\nyeah, I understand this approach. The problem is this example does neither do any classification nor regression with these features. \n. No, reuse memory is probably more important than naming.\nAs for IRC I'd suggest to use all the things such as IRC, pull request and mailing list ;)\n. Anyway are you ready to merge it? \n. I have, that's why I was asking ;)\n. Ah yes one more thing, could you please add python example?\n. Ok one more issue before merging. Could you please update naming of your example to regression_..?\n. Please rebase your fork to get this pull request merged\n. Gentlemen,\nI would like to note that there is no need to call dpbtrf/dpbtrs manually in case you need it only once (dpbsv does the job).\nAnd I also don't really understand why not to create identity matrix manually (actually we have CMath::create_diagonal_matrix, however is it really needed there?).\n. Thanks!\n. Thanks, merged\n. Awesome! Merging\n. Thanks!\n. Yeah, why not? :)\n. Thanks! I'll check your code today/tomorrow\n. Your constructor is not calling base class constructor. Please avoid this error in future.\n. It is not necessary by the standard but rather confusing than saving code :)\n. Not a bug - add_subset works on indices as is.\n. To separate your branches you can create two new branches (cf. https://github.com/shogun-toolbox/shogun/blob/master/src/README.developer) and cherry-pick (cf. http://technosophos.com/content/git-cherry-picking-move-small-code-patches-across-branches) appropriate commits. However as this is done in master you would need to fork your branches before your commits.\n. I do not actually like this zig-zag dependency:\nshogun/classifier/svm/LibLinear -> shogun/lib/external/shogun_liblinear -> shogun/classifier/svm/Tron\nAnd I also do not think tron should be provided in API..\n. Great huge work. I'll check things again a little bit later\n. Last thing that should be done here (for me) is to move MultiClassSVM to multiclass folder (and all its descendants) and rename it to Multi_c_lassSVM\n. It is really huge! However I've checked the changes and they are ok for me - great work!\n. Impressive, thanks! Ok to merge once it is rebased and if nothing is broken\n. Yes, I think it is ok to duplicate necessary code there. As for your double free - it is caused by sgvector transition for sure. Btw I had a problem with your implementation - it had zeroes in training labels causing my liblinear to fail. Can that be?\n. @pluskid yeah we are in sgvector hell right now.. I'll let you know how did I reproduce that a little bit later\n. Source file lines are enough\n. Could you please paste the output of 'cc -dumpmachine'?\n. Strange case.. Looks like it should detect x64 right\n. Ok then $COMPFLAGS_C should have -m32. Anyway thanks for reporting - I'll try to find out why it is so.\n. Will be reopened in case of problem renewal\n. Will be reopened in case of problem renewal\n. Hey,\nunfortunately I am not an expert in macports but it seems the problem is related rather to gmp installation but not to shogun?\n. My strong suggestion is to compile it manually from latest git - we are currently in more or less stable state I think. We can try to sort out problems with compilation in some kind of real-time if you visit #shogun at freenode\n. I use ubuntu. \nIf you want to use Shogun with Python modular interface under Ubuntu you'd have to install following packages (you may do this using 'sudo apt-get install package-name')\ngit (to clone shogun :), pkg-config, python-dev, python-numpy, swig, libatlas3gf-base, libatlas-base-dev\nOnce you have all of them do:\ncd %shogun-dir%/src\n./configure --interfaces=python_modular\nmake\nsudo make install\nThen you may go to ../examples/undocumented/python_modular/ and run some examples to check whether it is working.\n. Ah just recently introduced bug - let me fix it in 5 minutes\n. > Right now I am using get_name() to obtain the identity of classes. Would adding enumerated \n\ntypes for specific classes be better for this situation?\n\nYes, strings are kind of evil for that.\n\nThis will be unique to ExactInferenceMethod: It can only use a GaussianLikelihood model. It thus\nchecks its model if its really the GaussianLikelhood class, and then downcasts the model. Is it appropriate\nto use downcasting in this way?\n\nYes.\n\nIn update_alpha_and_chol, I use lapack functions to calculate the cholesky decomposition of a matrix.\nStrangely, it returns the correct upper triangle of the matrix, but does not fill the lower triangle with zeros (it\nleaves junk data in those entries). Is lapack supposed to do this? I go ahead and fill the lower triangle with zeros \nanyway.\n\nLapack's dpotf does not guarantee any content in not related triangle so if you need zeros - it is the only way to ensure.\n\nRight now the apply_regression in GaussianProcessRegression throws an SG_ERROR if the data sent is NULL. Is \nthis appropriate? Should it try to handle this situation more gracefully?\n\nSure, it is ok.\nSorry for late answering\n. I am a little confused with some 'Live session' user here :)\n. Seems to be fixed\n. Is that true still?\n. Evil #666 PR\n. Evil #666 PR\n. Please rebase it\n. Please rebase it - for some reason it can't be merged now.\n. > I had to adapt a function that calculates the derivative of the gamma function for\n\nthe student's t likelihood distribution. Should this be integrated into shogun's math library?\n\nYes, please put it into mathematics/Statistics.*.\n\nI had some stability issues calculating the student t likelihood derivatives directly, so I instead \ncalculated them in log space like GPML. In order to convert back to parameter space, does it make\nmathematical sense to divide by the variable, i.e if the derivative is dy/dlog(x), can I simply multiply\nthis by dlog(x)/dx to obtain dy/dx? Or am I having a stupid math day?\n\nSounds like chain rule so yes that's ok.\n. Hey Andreas,\nwe are currently preparing for release (hopefully September the 1st is the date), so we will update it soon.\n. Will be solved after merge of #854 \n. Randomized PCA is not yet done, needs separate class\n. To be more precise - randomized PCA is going to be based on redsvd-like algorithm for eigendecomposition (already in tapkee). \n. Please comment if it is really relevant for 2.1\n. Out of scope for Shogun 2.1\n. Hey, thanks for reporting! Unfortunately I have no OSX around, so I'd appreciate if you attempt to fix that. To do that you could check src/shogun/io/SGIO.h:60:69 - that is the hack we use to handle changed API.\nLooks like currently that check doesn't work on your machine.\n. Thanks!\n. Thanks for your efforts! It would take time to review it though with such big changes inside.. \n. https://gist.github.com/lisitsyn/4739969 tried the example - it works here. Is it solved?\n. Thanks!\n. Closed as resolved.\n. Yes\n. The problem is probably fixed after this commit but please comment if it is proper way\n. Could you please add a class for that too?\n. Thanks!\n. Don't make me sad\n. Okay well at least LLE, LTSA, NPE and Isomap can be tested by checking neighborhood of embedded points - they should have the same neighborhood as in original space.\nSee https://github.com/shogun-toolbox/shogun/blob/master/tests/unit/converter/MultidimensionalScaling_unittest.cc as an example of unit-test for converter (it tests distance preserving, i.e. that pairwise distances stayed similar after reducting dimensionality)\n. Most methods are totally ill-posed\n. Thanks for such a thorough analysis in this example! As Fernando said we can't merge it to master so please do the same pull request against develop branch.\n. Hey, sorry for that long delay. I can reproduce the error on my machine. Will attempt to fix it.\n. Yeah, that's true - we need some defines here to support other compilers\n. Thanks, lets continue!\n. Alright, thanks!\n. Done by Soeren and Fernando\n. https://github.com/shogun-toolbox/shogun/commit/8678e229eca0932437a22d262c37d8f19dbc9b22\n. https://github.com/shogun-toolbox/shogun/commit/8678e229eca0932437a22d262c37d8f19dbc9b22\n. What's the status on this?\n. What's the status? Postponing.\n. What's the status?\n. It doesn't look like something that can be solved and it is not critical as long as LMNN converges.\nLets re-open it later. \n. I checked the code and it didn't make real sense to me - lets postpone it\n. I checked the code and it didn't make real sense to me - lets postpone it\n. It looks like AFTER and BEFORE should help there.\n. It looks like AFTER and BEFORE should help there.\n. Will try to do\nOn Friday, October 11, 2013, Soeren Sonnenburg wrote:\n\n@lisitsyn https://github.com/lisitsyn can you fix it?\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/shogun-toolbox/shogun/issues/1655#issuecomment-26164242\n.\n. Guys, I am failing to reproduce that - I tried to add some crap to /usr/local headers and tried removing a method (while it is present in library). Any idea how can I reproduce it?\n. Workaround is here thus moving to next version.\n. I don't think it is necessary to be done this way, it would be better if we had some easy access page where you can select class with smart autocompletion.\n\nI can do that thus assigning it to myself for next milestone 3.1.\n. I see no reason to do anything about it then\n. @votjakovr please report if you still not manage to reproduce that - currently postponing it\n. Is it critical 'result-wise' - does this change the results?\n. Is it solved?\n. I don't think this is a major issue then. At least it is not critical and can't be solved right now. \n@lambday please add an issue if you think we should get rid of it. \n. I agree - I'd go for a warning here, not an error \n. I agree - I'd go for a warning here, not an error \n. You've got a lot of RAM, ain't you? ;)\nThanks for your improvements, I am ok to merge it.\n@sonney2k please merge if agree.\n. Well the solution already exists you know - https://github.com/lisitsyn/stichwort but needs polishing and more advanced stuff.\n. Okay just some thoughts - we can start with just making OpenCL powered dot products available from the SGVector static dot product methods. \n. - I don't think switch thing looks good in any case - that's just something to emulate polymorphic behaviour\n- Object creation can be replaced with static variables inside the function\n- Typeid looks bad - its performance is not really determined. If we really need it I can propose faster solution\n. @lambday I am pretty sure recreating backends due to used types is quite bad. One would have to take care of its thread safety which is already a room for bug ;) Imagine the case of parallel  and  computations which reveals that it is basically single-threaded when safe. Furthermore, this is a thing which state affects what it does. Sometimes it creates something and sometimes it doesn't.\n. What is important is how to design it to be flexible and generic.\nTheo, could you please share your thougths on this?\n. > Could I try that?\nYeah sure. I think work on this task could be somehow distributed.\n\nNot sure if I'm too late to discover this, but I can start right now, and further implement CNN in this issue (#1974). \n\nWe actually broke it down so it makes sense to first implement some building blocks.\n\nBy the way, a naive question: Is this task relatively independent on other parts of Shogun? I mean is it okay to firstly focus on implementing NN and then consider how to connect codes with the entire Shogun project. \n\nNot really - I believe it would be better to start with implementing this as Shogun classes. Just feel free to ask if you have any questions about that. \n\nOne more question, is there any existing code relevant to deep learning project now? If not, we can start coding from scratch :)\n\nNo, we don't have anything on that yet.\n. Sorry for some delay.\n\nI'm interested in attempting to do this as well, could I have a go at it? \n\nWe have nothing on that yet so it is unlikely that we get some clash. So, anyone interested - just do it ;)\n\nAlso would implementing convolution be done after the basic blocks (bp, feed forward) are in place?\n\nI think they are not really coming together. It is ok to develop some convolution code independently from anything related to NNs.\n. Thanks! I'm ok to merge it once remaining comments are addressed.\n. Thanks! Keep on moving ;)\n. We've got a problem with new classes. There are generated clone tests (which basically test whether clone of an object equals to the original one) and they fail for NeuralNetwork, NeuralLinearLayer and NeuralLogisticLayer. To avoid failing tests in current develop branch I ignored these tests in 558f824443ef73ef1af38d86e285ee3f56da5546\n. Forget my last comment :)\n. Hey,\nI am not sure these README files should be merged so it'd be better if you removed them\n. I am fine as well. Once travis is done should be merged\n. @vigsterkr you've overdued this overdueness! ;)\n. Looks good! Thanks!\n. @lambday @karlnapf okay guys we talked a bit with @khalednasr and he has some plan to address these issues - lets see more changes in a bit\n. @vigsterkr ping\n. As of 038280845fd7fb886f4459996f1405f8ca8c1612 we require SWIG 3.0.7 which supports a feature that is being used by 7b7e1f3dbfae13877819c00b0f612a9a0af3b464 to overcome this problem in general.\nThanks, @tpokorra! \n. Looks good!\n. Yeah they are better removed if it works w/o them\n. Looks good, thanks!\n. Looks good - can we merge it?\n. ### Contact\nEmail: lisitsyn.s.o@gmail.com\nirc: lisitsyn\nPersonal\nOccupation: Software engineer at Yandex in Moscow, Russia.\nInterests: machine learning applied to real problems, beautiful code and building tools and libraries\nShogun\nI've been involved with Shogun since GSoC 2011 with the next stop at GSoC 2012 and consequent two years being a mentor. As a student I've been working on dimensionality reduction and multitask learning with the help of Christian Widmer. Since 2013 I am helping to mentor various GSoC projects of Shogun. \nAsk him about\n\nAnything about C++, Java, distributed things and other software engineering deals - I am in\nDimensionality reduction\nIf you have any ideas how to improve user experience with Shogun\nAnything else about Shogun - I can either help or guide you to someone helpful\n. @iglesias I'd say it is more flexible \n. Okay guys a few cases that come into my mind:\nIf we go with generic get/set without specific getters for field adding a new field is free - you just recompile the .cpp file (@iglesias).\nIn the same scenario headers are binary compatible with various versions of shogun (@iglesias).\nWe can remove all mentions of std vector/list/.. from headers because they are only in .cpp files so it will be faster (@karlnapf).\nWe can probably use simpler saving/loading that just works with that subobject (declared in .cpp) but not the object (declared in .h) - that's a thing I am thinking about yet.\n. @iml I thought it was fixed in https://github.com/shogun-toolbox/shogun/commit/fef8937d215db7f26e15cdc8b21201ee52e69e1f\n. @iml can I help you with pushing the new shogun version into brew? We now have some will to make shogun better distributable as this is super important. Please let me know if there are any issues yet to resolve. \n. @iml we actually spent quite a lot of time trying to understand why python-modular crashed on mac os x. It was the very same issue of linking to wrong libraries. \n\nThanks for the information I'll check what can I do\n. @iml we actually spent quite a lot of time trying to understand why python-modular crashed on mac os x. It was the very same issue of linking to wrong libraries. \nThanks for the information I'll check what can I do\n. For some reason #if defined is considered bad for our class_list script\n. @Saurabh7 I like this initiative to cleanup the code, will you have some time to finish the PR?\n. I believe that as long as we are source-based not distribution-based modern compiler is ok.\n. Hmm sorry didn't notice we still support C++ 98 :)\nCan we get rid of it? Any modern compiler supports C++11 now\n. @iglesias done ;)\n. Oh that's very ironic indeed \n. Other comments anyone? ;)\n. Gonna update doc in a minute\n. @vigsterkr yeah I was psychic about short and meaningful name - correct me if it is not the thing :)\n. Haha sorry I made a bad rebase\n. Thanks @besser82 ;)\n. Ok I rethinked this thing and going to push non C++11 version soon\n. I wouldn't expect any non-marginal performance impact yeah\n. The good thing about having the kernel method is that it is a pattern: http://en.wikibooks.org/wiki/More_C++_Idioms/Non-Virtual_Interface\n. Comments anyone?\n. @iglesias take a look\n. @iglesias no difference or faster. No need to use unique_ptr here anyway.\n. The question is what evaluate() returns :D\n. @abinashmeher999 what would be the key for this map?\n. Haha that's very neat that you review my language here @iglesias haha\n. Actually I'd love to get these fixes back to the website as well\n. Ok to merge?\n. Please benchmark that before merging. Simple add would benefit from vectorization more than from parallelization.\n. To answer the question whether openmp is needed here we need to consider what sizes we expect here. Unless we add really big vectors it would hurt to make it parallel.\n. @lambday compilers are smart enough to vectorize this loop\n@sanuj not sure but probably not, I'd rather let one core pass it with big strides than let it call all these omp routines\n. @sanuj yes whether it is possible\n. Add -O3\n. @sanuj sorry missed this message somehow.\nIt is true CNNs are useful for computer vision but this recent paper I mentioned uses it for text understanding quite successfully so it is worth investigating.\nAs for your mention of sigmoid thing - ReLUs are usually better due to no vanishing gradient effect (it is rather impossible to train a deep net with sigmoids) and the speed of its evaluation. \nFeel free to submit patches for these tasks on neural nets if you are interested! \n. Yeah for example that's the initialization of linear layer we now have: https://github.com/shogun-toolbox/shogun/blob/develop/src/shogun/neuralnets/NeuralLinearLayer.cpp#L63\n. @sanuj don't bother with initialization/learning of parameteric rectifier, it is more important to get initial weights of rectifiers \n. @sanuj yeah feel free to implement that then ;)\n. @sanuj I'd suggest you to add an other initialization mode that is switched with some enum\n. @ayush2913 cool! This could be a kind of challenging thing so lets do some iterations. If you understand the architecture of neural net they used we can go on to details otherwise lets discuss \n. @ayush2913 I'd suggest you to start with some helper functions that encode the text in the same fashion they do. The next step would be to check the notebook on CNNs http://shogun-toolbox.org/static/notebook/current/neuralnets_digits.html and try to build some network. Then you would transform a few texts and try to learn the network to classify them.\n. @ayush2913 I'd suggest you to start with some helper functions that encode the text in the same fashion they do. The next step would be to check the notebook on CNNs http://shogun-toolbox.org/static/notebook/current/neuralnets_digits.html and try to build some network. Then you would transform a few texts and try to learn the network to classify them.\n. @alishir cool! Thanks for the info\n. Hey @sanuj,\ntemporal here is just in the axis of letters coming in the text. So t=0 is the first letter and so on. This encoding is treated as an image so convolutional layers operate in the same fashion as in images (the pooling step is exactly the same). The output is domain-dependent, you just fit this architecture to the problem of classification of texts or something like that.\nPlease ask if you have other questions \n. @sanuj really sorry, things going a bit intense \n1) Number of unique chars in your alphabet times number of chars in your sequence\n2) Kernel is what convolution layer uses and learns. That's coefficients of the convolutional operator. The stride is 1, so they don't jump over more than one letter\n3) This should be some specific task like sentiment analysis or something like that\n. @sanuj my idea was to create an ipython notebook reproducing that in shogun. If it takes that much time we should either train on subset, or do something about it in shogun, or maybe just give up :)\n. I think @besser82 can assist you with that\nThe idea is to select commits you need and apply them one by one. Then you would push --force it into the branch. Although be careful to not lose any changes. \n. Ok test failures are not related to these changes.\n. Just a few minor issues more and we can merge it\n. Thanks!\n. @sanuj \n1) Yes, CDenseFeatures -> CDotFeatures\n2) Yes, both dense and sparse features are examples of dot features\n3) Dense features have to be in-memory but DotFeatures can be anything. As access is usually sequential it could be file or some resource accessed through network\n. @sanuj sorry. \nCDotFeatures is abstract class for sure. Inheriting from it means that these features can compute dot product with some arbitrary vector. It doesn't restrict any internal structure of data, anything like that. I have some not-yet-merged thing in PR #2788 so I am unsure how to proceed. \n. @arasuarun yes it is would be like a non-learnable layer, that transforms data though\n. @arasuarun yes it is would be like a non-learnable layer, that transforms data though\n. @arasuarun I think gaussian noise should be made up of just another layer, could be done later.\nAnyway, wouldn't making it input layer restrict us from putting it somewhere between other layers?\n. @lambday to not force all the shogun be C++11 :) Just an idea\n. We will put your portrait in a postmodern style just to the left of this pic \n. Use this mask ;)\n\n. Task one - smooth this picture and use it as a distribution.\n. @kevinhughes27 yeah I remember back in 2011 I was thinking: why all the examples are stored in a folder that sounds like it is a trash or something\n. @kevinhughes27 yeah I remember back in 2011 I was thinking: why all the examples are stored in a folder that sounds like it is a trash or something\n. Python dependencies are so easy to install so I'd not care\n. @adityaosp95 feel free! The code reporting error can be found at https://github.com/shogun-toolbox/shogun/blob/develop/src/shogun/neuralnets/NeuralNetwork.cpp. See functions train_bfgs and train_gradient_descent.\nI suggest you to use any neural network example to develop and test gradient output.\n. @sanuj sorry for late answer\n1) It's probably useful to check mean and max gradient values per layer\n2) Use set_loglevel of SGIO\n. @karlnapf it is useful if a function can not return anything sometimes. If you use pointer semantics absent value is nullptr but in case of value semantics one better use optional/maybe\n. @karlnapf it is useful if a function can not return anything sometimes. If you use pointer semantics absent value is nullptr but in case of value semantics one better use optional/maybe\n. @karlnapf yes that's safer and it has value semantics instead of pointer semantics\n. Just pushed reworked Maybe\n. Travis errors unrelated (gosh we have to fix travis), merging\n. There is a few pros if we consider feature to be an entity, we could\n1) name it\n2) disable/enable it\n3) use it as label interchangeably\nOn Friday, February 20, 2015, Heiko Strathmann notifications@github.com\nwrote:\n\nCant this be solved with a seperate table where people store their names?\nI like it but is it really something we should do from within shogun? Just\nincreases complexity....\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/shogun-toolbox/shogun/issues/2727#issuecomment-75287632\n.\n. Say you want to load a csv file and then select one attribute to predict. Currently we have CSV stuff but we can't handle such simple cases. That's why we should either don't work with any io at all or support it better.\n. @karlnapf I don't see that much complexity in this - all we need is to make features aware of what they store which sounds like a good design anyway\n. Yeap let me fix that first\n. Closing for now.\n. @karlnapf hayai is header only which is an advantage - we could just take the code once. As it is dead simple we can maintain that by ourselves\n. @karlnapf I guess if like you it too we should go for it ;)\n. Hey @ajaybhat, sure, feel free to take this over.\n\nI am not sure about structures to clean up, but I think you would notice some during namespaces moving.\n. Hey @neutralino \nI am surprised by this kind of error you get but the reason for this error is simple. Labels itself is an abstract class and you'd have to instantiate something more concrete like BinaryLabels\n. I am closing now as it seems to be resolved\n. Cool, thanks!\n. Well softmax is pretty calibrated.\nI met this issue when applied to some classification when ranking is important. SVM outputs some data which is not calibrated as well but you can extract ranking\n. Hey @neutralino,\ngood catch, thanks for pointing this out. Although I have no solution but rename all the 'initialize' member functions. \n. @karlnapf @RichardF77 I suspect there is some discrepancy between compiler (or used standard of C++) and stdlib. I am not sure why it is so but I'd check whether C++11 is enabled.\n. @karlnapf @RichardF77 I suspect there is some discrepancy between compiler (or used standard of C++) and stdlib. I am not sure why it is so but I'd check whether C++11 is enabled.\n. @RichardF77 did you get any progress on this problem? Need help?\n. Oops, my bad\n. Oops, my bad\n. Wow welcome back ;)\n. Could you please add confidences for binary classification as well?\n. Could you please add confidences for binary classification as well?\n. Thanks @khalednasr !\n. @khalednasr probably multiple inputs can be emulated through features as well\nI was thinking about siamese networks - this is a case of multiple inputs being used..\n. @khalednasr probably multiple inputs can be emulated through features as well\nI was thinking about siamese networks - this is a case of multiple inputs being used..\n. @khalednasr I am thinking of dropping input layer. We can directly ask CDotFeatures for dot product in linear unit. Convolution layer could retrieve features via add_to_dense_vec. What do you think?\n. @khalednasr I am thinking of dropping input layer. We can directly ask CDotFeatures for dot product in linear unit. Convolution layer could retrieve features via add_to_dense_vec. What do you think?\n. @sanuj yeah sorry I stopped at some point here. Can get back to that this weekend.\n. @sonney2k with @karlnapf initiative to generate examples from some proto language it shouldn't be a problem\n. @sonney2k with @karlnapf initiative to generate examples from some proto language it shouldn't be a problem\n. @sanuj this time excuse my delay :) can we name MANUAL thing as NORMAL and AUTO as HE_NORMAL (as per author of the paper, like in Keras https://github.com/fchollet/keras/blob/master/keras/initializations.py) ?\n. Ok this looks ok, thanks!\n. Out of curiosity, what's this kernel is good for?\n. Hey @mepatterson \nThis is kind of incompatibility of swig and yosemite (in default settings) we didn't know about at the moment of release. I suggest to either build latest shogun or apply this patch: https://github.com/shogun-toolbox/shogun/pull/2694\n. @mepatterson as you're using non-standard directories you're probably missing something like LD_LIBRARY_PATH which should be set to the directory with compiled .so\n. @mepatterson did that help in the end?\n. @aagarwal-gtr it's quite inconvenient to take your changes from the repo and your name would be missed in the contribution. Could you please follow the usual github workflow of pull request?\n. @aagarwal-gtr it's quite inconvenient to take your changes from the repo and your name would be missed in the contribution. Could you please follow the usual github workflow of pull request?\n. @karlnapf current status\n\n. 30409a71019c3a5b2247f994ff8208c8804b58fa merged that into feature branch feature/sphinxdoc\n. Can we merge this?\n. @sorig ah I see. Didn't know there are more changes to come\n. Thanks!\n. Thanks! Going to commit a fix in a few minutes\n. Header only library is mostly the same thing as supporting our own solution as we'd have to put the header(s) into our repo. We have to rely on something that is already distributed in major systems we want to support\n. I'm not sure Gitter is better. Such things are pretty temporary and when they are gone we have to move again. IRC is already older than me and I guess it would stay for a while :)\n. Looks good to merge. Thanks!\n. Lookz cool\n. Hmm I am so sorry I missed this PR. Gonna glance over it now\n. Hmm I am so sorry I missed this PR. Gonna glance over it now\n. @jaelim sorry about that. Is this still relevant?\n. @beew it looks like a problem with swig generated code. Could you please post this src/interfaces/octave_modular/CMakeFiles/octave_modular.dir/modshogunOCTAVE_wrap.cxx file somewhere?\n. @beew it looks like a problem with swig generated code. Could you please post this src/interfaces/octave_modular/CMakeFiles/octave_modular.dir/modshogunOCTAVE_wrap.cxx file somewhere?\n. @beew sorry for quite a slow support on that. This could be a swig issue but I better try to reproduce this locally\n. @MartinHjelm could you provide more details in the segfault you get? Miss that in in related issue.\n. @MartinHjelm can you somehow check whether the Python it was compiled against is the same to the one you're running it with?\n. @MartinHjelm ok I see that's the reason then. \nThese two pythons are of different version and when binary things happen it crashes. Actually happens quite a lot on Mac OS with brew.. \n. @nachitoys for some reason swig interface for bessel kernel was missed. I am going to add it in a minute.\nNot sure about non-symmetric kernels but probably it won't work well. Following Mercer theorem kernel is symmetric and PD.\n. @nachitoys for some reason swig interface for bessel kernel was missed. I am going to add it in a minute.\nNot sure about non-symmetric kernels but probably it won't work well. Following Mercer theorem kernel is symmetric and PD.\n. @nachitoys if you compile from sources - just update to the latest revision of the develop branch. In case you use some release I have bad news - sources are required :)\n. @nachitoys if you compile from sources - just update to the latest revision of the develop branch. In case you use some release I have bad news - sources are required :)\n. @nachitoys just to be sure - did you recompile and install it once you updated git?\n. @nachitoys just to be sure - did you recompile and install it once you updated git?\n. @nachitoys sorry for long answer. This looks like at some point the build was broken. I suggest you to try again with latest develop\n. Sorry, one more issue\n. @erip I don't think license is issue. As long as they are not exported to swig it should be ok with whatever name\n. @erip I don't think license is issue. As long as they are not exported to swig it should be ok with whatever name\n. @besser82 actually one issue is to be fixed\n. @besser82 yeah I think so\n. @besser82 @wiking ping\n. Too late! ;)\n. Tricky! :)\nDo all authors agree about license change?\n. @karlnapf @yorkerlin can we discuss it a bit more at #shogun? \n. Some thoughts:\n1. I feel it would be better to untie this thing from shogun structures. It would lead to cleaner API without maps and stuff. C-style API is much cleaner sometimes.\n2. I lack understanding of use cases of this kind of library? It seems reasonable for me to describe the code we want to get first. \n3. Its very OOP now while some functions could suffice. One of the best approaches is non-member functions operating on simple objects. \n. Thanks! Just one small issue - according to the doc, mkstemp returns valid file descriptor so the file stays opened. It would be better if we close it immediately once file is created\n. Ok its good to merge.\nCan be later improved with some separate function.\n. Travis errors looks unrelated\n. We would have to use multiple inheritance, it could be PITA you know :)\n. @karlnapf okay this works. Though we don't really know whether referenced is always serializable or serializable is always referenced :)\n. @karlnapf \n\nmaking the base class overhead much smaller\n\nI like the principle, although it is not really clear for me what to remove. \n\nserialisation\n\nI think it should stay as long as we are rare ML library with this feature.\n\nreference counting?\n\nI believe it should be done via shared pointers.\n. @yorkerlin looks cool, we should change neural nets to use that as well\n. @lambday does that speed up the compilation?\n. @lambday does that speed up the compilation?\n. @nachitoys that's really good results are still reproducible and no bug here involved :)\n@yorkerlin thanks for resolving this issue. \n. @nachitoys that's really good results are still reproducible and no bug here involved :)\n@yorkerlin thanks for resolving this issue. \n. It should be libshogun.dylib (as a link referring concrete version of dylib) either in linker's path or in the directory specified by -L/path/do/that/dir and the -lshogun flag should be used.\nLet me know if that helps\n. It should be libshogun.dylib (as a link referring concrete version of dylib) either in linker's path or in the directory specified by -L/path/do/that/dir and the -lshogun flag should be used.\nLet me know if that helps\n. Its DYLD not DYDL, could be the reason if it is like that in you env\n. Its DYLD not DYDL, could be the reason if it is like that in you env\n. @wcalhoun516 I am glad it is resolved!\n. @wcalhoun516 I am glad it is resolved!\n. @erip hey, what's that? :)\n. @erip hey, what's that? :)\n. @erlp thanks! I believe @vigsterkr is the man to handle this ;)\n. @erlp thanks! I believe @vigsterkr is the man to handle this ;)\n. @yorkerlin yeah I like map as well\n. danke\n. @yorkerlin sorry I am not really into your pace, this one looks ok\n. @jaelim this is definitely a problem with your PYTHONPATH environment. Be sure it points to correct directory. Please try echo $PYTHONPATH and check whether modshogun.so is available under one of printed directories.\n. Just yesterday, we had some user that used $HOME in .bashrc and in the end full path (w/o $HOME) worked. Be sure $HOME is replaced - it won't work otherwise\n. @jaelim good. Now set the LD_LIBRARY_PATH to the directory that has _modshogun.so inside\n. @jaelim ok, just be sure it is set properly. If it works in the directory with module then the PYTHONPATH is wrong\n. @jaelim no, its your power button.\n. @yorkerlin yes, that's pretty clear we should use proper ways of distrubution.\nAlthough the claim of breaking hardware by compilation is absolutely ridiculous.\n. I'd like to note that there is no such thing as 'building shogun package', it's rather standard CMake + Make + GCC + Swig toolchain. We can't and shouldn't control behaviour of tools. Indeed, it takes a lot of memory and we should clarify the design with some techniques to reduce that, but no need to care about every little detail of OS.\n. 1. Create some directory to store build (mkdir build)\n2. Go to that directory and run CMake (cd build; cmake -DENABLE_TESTING=ON ..)\n3. Build with regular make and use make test\n. Yeah, true\n. Ok what files fo you have in /usr/local/lib and what files do you have in that shogun-install directory?\nPYTHONPATH should point to the directory that has modshogun.py\nLD_LIBRARY_PATH should point to the directory that has modshogun.so\n. Yeah sorry, libshogun.so\n. Actually, from your description of the problem it should be a PYTHONPATH issue. Wrong LD_LIBRARY_PATH would lead to undefined reference errors; in your case its modshogun.py that is missed by python.\nI'd suggest you running \nPYTHONPATH=... LD_LIBRARY_PATH=... python -c 'import modshogun'\nuntil it finally works. This would be the correct configuration.\n. @Arnei yeah that's sort of strange. It detects modshogun.py but fails to load libshogun.so. Could it be that libshogun.so for python2.7 is used?\n@besser82 do you have any idea?\n. @Arnei does it work with other python version? \n. @Arnei oh, good it is resolved. Thanks for letting us know\n. I don't think buildbot doesn't support that, it has GCC 4.9.2.\nIt's a matter of configuration and we should use C++11.\n. @erip our machine running buildbot has modern GCC to support that\n. LGTM!\n. I am closing this issue as invalid.\n. @iarroyof I will try to get some time to do that in next few days\n. Sorry not yet\n. Oh this should be easy. Just be careful to check which python swig has compiled against. A great chance is that you're importing it in a different python - happens a lot on mac.\n. @karlnapf could be possible but I am not sure, it is very low-level binary stuff\n. hey guys,\nI'll start a branch.\n. Ok here is how we could do that:\n``` bash\ngit log --format='%aN <%aE>' src/shogun/features/DenseFeatures.h | sort | uniq -c | sort -nr | cut -d' ' -f5-\nSoeren Sonnenburg sonne@debian.org\nHeiko Strathmann heiko.strathmann@gmail.com\nvladislav.horbatiuk@gmail.com vladislav.horbatiuk@gmail.com\nlambday heavensdevil6909@gmail.com\nYuyu Zhang zhangyuyu2008@gmail.com\nThoralf Klein thoralf@fischlustig.de\nSergey Lisitsyn blackburn91@gmail.com\nKevin kevinhughes27@gmail.com\nFernando Iglesias fernando.iglesiasg@gmail.com\nEvgeniy Andreev gsomix@gmail.com\nBj\u00f6rn Esser bjoern.esser@gmail.com\n```\n. @yorkerlin could you please take a look?\n. @arasuarun it is a good thing to do! Maybe we can drop the enum and just pass the object instead?\n. @arasuarun this is indeed good. Although lets get really careful with design, RNNs are a bit more tricky. We could need some real redesign to support that, I don't know yet\n. @arasuarun this is indeed good. Although lets get really careful with design, RNNs are a bit more tricky. We could need some real redesign to support that, I don't know yet\n. @arasuarun yes sure. Sorry for late answer\n. I am not sure about this error, but as a general thing I'd recommend newer swig.\n. Travis failure is unrelated, flapping test.\n. :+1:\n. Can't commit to your repo, so use something like that:\n``` cmake\nadd test case for each generated example\n(not generated yet so have to fake filenames from META_EXAMPLES list)\nFOREACH(META_EXAMPLE ${META_EXAMPLES})\n    # assume a structure //listing.sg\n        STRING(REGEX REPLACE \"./(.).sg\" \"\\1\" EXAMPLE_NAME ${META_EXAMPLE})\n        STRING(REGEX REPLACE \"./(./.).sg\" \"\\1\" EXAMPLE_NAME_WITH_DIR ${META_EXAMPLE})\n        STRING(REGEX REPLACE \"/\" \"-\" EXAMPLE_NAME_WITH_DIR ${EXAMPLE_NAME_WITH_DIR})\n    STRING(REGEX REPLACE \"./(.)/..sg\" \"\\1\" EXAMPLE_REL_DIR ${META_EXAMPLE})\nSET(GENERATED_CPP ${CMAKE_CURRENT_BINARY_DIR}/${EXAMPLE_REL_DIR}/${EXAMPLE_NAME}.cpp)\nSET(GENERATED_CPP_TARGET ${EXAMPLE_NAME_WITH_DIR}-meta-cpp)\nSET(GENERATED_CPP_TEST ${EXAMPLE_NAME_WITH_DIR}-meta-cpp-test)\n\nADD_EXECUTABLE(${GENERATED_CPP_TARGET} ${GENERATED_CPP})\nADD_DEPENDENCIES(${GENERATED_CPP_TARGET} meta_examples shogun)\nTARGET_LINK_LIBRARIES(${GENERATED_CPP_TARGET} shogun)\nSET_SOURCE_FILES_PROPERTIES(${GENERATED_CPP} PROPERTIES GENERATED 1)\n\nLIST(APPEND CPP_EXAMPLES ${GENERATED_CPP_TARGET})\n\n# run test in source dir, to have access to \"data\" folder\n# run class from binary dir though as it is generated there\n# add class location to classpath to do that\nADD_TEST(NAME ${GENERATED_CPP_TEST}\n         COMMAND ${CMAKE_CURRENT_BINARY_DIR}/${GENERATED_CPP_TARGET}\n         WORKING_DIRECTORY ${CMAKE_CURRENT_SOURCE_DIR})\nLIST(APPEND INSTALL_EXAMPLES ${CMAKE_CURRENT_BINARY_DIR}/${GENERATED_CPP_TARGET})\n\nENDFOREACH()\nadd_custom_target(build_cpp_meta_examples ALL\n                                 DEPENDS ${CPP_EXAMPLES}\n                                 COMMENT \"Compiled generated cpp examples\")\nINSTALL(PROGRAMS ${INSTALL_EXAMPLES}\n        DESTINATION share/shogun/examples/libshogun\n        COMPONENT libshogun-examples)\n``\n. Looks good. Although I wish it had some explanation in the doc how the bias is calculated.\n. Thanks!\n. Looks good\n. @karlnapf I think that's different scope. If we use this newprogress(...)thing all around we can integrate it to anything.. \ud83d\udc4d to merge. @karlnapf what about you?. @geektoni hahah I was too excited! Sorry. This is a bit messed up. Feel free to reopen when cleaned up.. @SashaMalysheva what were the problems with LDA?. @karlnapf you can squash commits with a click now. \"Squash and merge\". \u00bfmerge lah?. daobao. You may implementbegin() { return 0; }andend() { return size(); }`. Prekrasno!. @karlnapf huh!. @luisffranca hey! Thanks for submitting this patch.\n@vigsterkr Travis fails with a style check related to original code. Do we merge it?. Back in the game! ;). Does it really fail on some platform? I am curious what makes it fail.. Do you happen to know what is the type before the cast? The whole world looks fragile to me if range produces not ints but something.. Oh I still don't get where floats come from but lets merge it \ud83d\udc4d . Ah now I get it. Travis job has got its time limit. \nMerging.. Thanks for the fix, Luis!. Just for the context: https://github.com/Homebrew/homebrew-science/commit/f96e4b8b9c22176c575a4b6a79886ef91b6ff7fc. @gbohner thanks!. :+1:. :+1:. Looks nice! No leaks, right?. @geektoni want to revive this thing?. Discussed in person with @geektoni and @karlnapf. So, we stopped working at it because we had no option of using the ctors with arguments.\nWe can get back to this PR and hopefully merge it once we get these two things done:\n1) Add create-by-name things like in #4022 \n2) Make all examples use that create-by-name thing. This makes a great gsoc-entrance task. What do we plan to use it for?\n. @karlnapf checks are green but codecov, take a look at the patch please.. yes as well as debian. \nLike that. Simple thing and tests are passing. Hence merging. I don't get it yet. @karlnapf ready to go. It is a major change so please take a look.. shogun.Classifier etc stops working once merged.. @vigsterkr yeah namespaces are good but IIRC in the end they were too intertwined.. Yeah too many dependencies to have separate modules so we gave up :). YOLO!. Adding code to export models. I am open for suggestions ;). I missed this PR but this looks great. What is missed to merge?. \n. \n. \n. Travis is happy so I'm merging. Started reviewing!. I just put some replacements you might want to make. Don't know if we need it though.. GREEN!. SH\u00d8G\u00dcN. looks good!. Ace! \u00a9 @karlnapf . @salonirk11 oh that's a good catch we have not foreseen. It seems we'd also need to change it to set(\"width\", 2.0) but this might require some tricks in examples generator.\nThanks for the note!. @shubham808 yeah factories are the way to go yet.\nGood news are that once #4063 is merged (rather soon) the put('parameter', value) start working. This means examples would start looking like that:\nkernel = shogun.kernel('GaussianKernel')\nkernel.put('log_width', 1.0)\n. I believe it should cover all of them. But I missed the type-checking part so need to add a few more tests and checks.. Some SGObject tests are failing yet. . @karlnapf ~~all green, could you please take a look again and merge if all good~~ merry xmas. Apparently this slows down the compilation as running tests on travis times out most of the times.. No I think it is where we store things like\n- Serialization ignored\n- Modelselection supported\n- .... @dgkim5360 thanks for the PR and welcome!\n@karlnapf I think this one is for you. @vigsterkr sir I did address your comments!. Thanks for the patch!\nPlease take a look at the Travis CI status, the tests are apparently failing.\n@karlnapf can you take a look?. Here you go. This might keep in branch but since we merged coreml I'd prefer to merge it also.. lgtm!. Ok I introduced a test that shows that vectors generally work. What does not work is combinatorial stuff like vectors of SGVectors. Will work on that next. Probably, but I am not sure we need it, map/unordered_map is especially tricky to handle.. I checked this thing and the reason is that OCTAVE_ROOT_DIR is detected via octave-config -p PREFIX which is not supported anymore. I didn't find any mentions of that in CHANGELOG of Octave but I guess we should adapt FindOctave. Luckily, this directory seems to be unused. This means deleting it from the argument list of find_package_handle_standard_args should help. . I was wondering if it could be some different kind of exception. Does it really has to be RuntimeError?. Or even KeyError as it is some kind of map. I am not sure if it makes sense to change now but I'd definitely change it at some point.. Thanks!. We discussed it with Heiko and probably it might be not that useful. \nThe similar thing can just happen in put.. Needs updating data.. @vigsterkr I am just afraid adding constness would make things more complex. It could make sense in pure C++ but since we pass the runtime boundary in the interfaces it might be not that good.. Ah and this also nearly doubles the amount of generated template code.. Oh what a huge thing :)\nLooks good but will try to read thoroughly.. Note that variance estimation is unbiased and squared residuals is divided by (count-1) (expect case of count = 1).\n. Note that there is no 1/sqrt{2\\pi} multiplier because it doesn't make sense in case of rate maximization.\n. You could use 'do { ... } while' for smaller code.\n. Could you please fix spacing here?\n. Ok, one major thing here - this cov/mean thing is being called each time compute is called.. Please move matrices part into the init\n. Why did you comment it out?\n. Could you please add comparison with naive quicksort index implementation? This way we can get sure it works properly\n. Wrong copyright probably\n. LOL :D\n. I need some small research on that - could you please check whether it computes all possible distances? Can it avoid computing some of them internally?\n. get_kernel_matrix would compute matrix each time you call it (even if you want num_cols only)\n. You may use dsymm\n. For filling with zeros you may use zero() method and loop for (j,j) elements\n. Can this naming be more informative? I am not sure but may be you would find some better name\n. Wrong tab there\n. Wrong tab there\n. Please use latex notation there\n. get_sigma could be there as well\n. That should be obviously wrong and cause invalid reads\n. SNE probably ;)\n. I believe you may use it for any type of features - only distance is required, right?\n. My main concern is still this full distance matrix. My suggestion is to add embed_distance(CDistance) method and handle 'to compute' case with CustomDistance. Something like that: https://github.com/lisitsyn/shogun/blob/libedrt/src/shogun/converter/DiffusionMaps.cpp\n. Parameters, function and other stuff should stay here\n. Should data we want to apply contain as much vectors as m_labels has?\n. Not a gaussian kernel ;)\n. Include GPL header and your name here\n. May be it would be better to use matrix there and then construct features\n. What is the motivation for ++x? \n. Please move constructors to the top\n. No need to redefine that?\n. Please use \nfor (...) \n{\n    ...\n}\n. Oh that can be a problem. Why don't you use clapack_dgetrf there?\n. Please drop Q parameter I don't think it is useful\n. Use instance of SGMatrix - it is pretty lightweight\n. Please follow code style there\n. Here and after - that is not the best way\n. Please remove that later\n. Hmhmh has it been discussed? I think we should provide an option to do some kind of precomputing there in the model selection use case.\n. I think this could be done with some general for machine prepare_for_ms method. However this shouldn't be done now\n. Wrong copyright\n. Is it compileable?\n. I meant there are some glitches (pound char instead of left parenthese) ;)\n. No, your commits will appear here \n. I am not sure pointers to SGMatrix would work for modular interfaces, did you test it with python modular or so?\n. Use <> include here - it may cause some compile-time glitches\n. I checked irc logs - you were asking about differences of SGMatrix and SGVector. There is some misunderstanding here - I meant it is ok to use referenced instance of SGMatrix/Vector here, not pointer.\n. Good idea\n. We actually can patch our true multiclass SVMs to propagate num_classes to the strategy when training\n. Could you please describe what is the motivation to have 2 parameters here?\n. I see this can be easily done with \"< int32_t, std::pair< float64_t, float64_t > >\" (hello Soeren! :D) and new method get_label_prob()\n. Please remove that due to ongoing removal of Array2\n. Please remove that due to ongoing removal of Array3\n. I didn't get the purpose of this class - what is it?\n. Got it\n. GOD_DAMN_AWESOME_DENSEFEATURES may be\n. Use get_computed_dot_feature_vector(), computing whole matrix is way too memory expensive\n. Will it come back?\n. We have confusion matrix computation func in CMulticlassEvaluation :)\n. IIRC It is different in libocas and shogun so every data_y[] should come with -1\n. Wrong ;)\n. c=0?\n. May be generalize SPE and FA ones to just max_iters?\n. Consider using noalias() next time\nhttp://eigen.tuxfamily.org/dox/TopicWritingEfficientProductExpression.html\n. The most efficient is to collect a vector of triplets (i,j,value) and use the setFromTriplets method. It would not work with eigen older than 3.1.0 though.\n. Check this out\nhttps://github.com/shogun-toolbox/shogun/blob/master/src/shogun/lib/tapkee/routines/locally_linear.hpp#L85\n. Please take a look at the DataGenerator class which may appear useful here \n. Indentation goes wrong here\n. And here - please use tab indentation with 4 spaces per tab\n. I didn't get what is it\n. Please add a typedef for that long type\n. Some bouncing tabs here \n. No, I mean these two SG_FREE are not aligned.\n. Good!\n. Please add a typedef for that pair\n. Bad naming here. Just let this function create a matrix.\n. just compute norm to avoid sqrt here\n. Please remove additional spaces - that would be more consistent with other code without them\n. Additional space here\n. sparse_matrix_from_triplets though\n. let's do that (end-begin)\n. here too\n. and here\n. diff_cos\n. diff_distance\n. Let's move that function to utils/sparse.hpp\n. Once sparse thing is moved to utils/sparse.hpp this should go\n. just norm\n. inRange\n. Please add license header with your name here\n. Replace it with using std::vector or replace all vectors with std::vector to avoid possible clashes\n. As there is no \"public:\" everything below is private - how to use that class?\n. Please follow the \na b(c)\n{\n}\nbraces style\n. Why not to make it member function to avoid friending it later?\n. @sonney2k \n. Can they be less random like zeros or so?\n. Could you please initialize it in loop still to avoid confusion?\n. As Fernando said it could slow down SWIG stuff\n. Please use latex \\f$ ... \\f$ there\n. I like accessor-style but lets make it get_V\n. Something must be missing there!\n. Typo!\n. Please don't use comma there and elsewhere\n. Why is it a pointer?\n. CMath::MACHINE_EPSILON\n. Yeah I meant impl of diagonalize ;)\n. Please use \n. Lets use <> include here\n. I'm still confused with that, what about passing zero-sized matrix instead of pointer here?\n. What about SGMatrix(NULL,0,0)?\n. Ha! My comment got cropped\n. #include <shogun/mathematics/ajd/JADiag.h>\n. register makes no real advantage nowadays - compiler knows how to do that\n. I'd use givens_stack\n. No need for { } here as it is just one-liner\n. May be CJointDiagonalizer or sth like that?\n. Is there a typo? Xcd?\n. Just curious - what was the trouble you solved with the anonymous namespace?\n. Could you please break it up to a few variables?\n. We actually don't use method ..space.. () thing but that's quite minor\n. I believe an include in the middle of file is pretty bad in general \n. No blank line here\n. Is it really worth to be 16 bit and why public?\n. Something more compact like SGVector<D> would look better I believe\n. Please (if you don't mind) use BSD license as it would be easier to continue Shogun's transition from GPLv3\n. I think it would make sense to add local typedef here (like MappedMatrix, MappedVector)\n. Just use SG_CALLOC to avoid filling it with zeros\n. This is somewhat utility file that is going to be modified by a number of people so may be we can remove that license plate here\n. It makes sense to use initializer list here\n. In C++ (unlike Java) members are usually declared after constructors, destructor and member functions\n. }ProbName; -> } ProbName;\n. It makes sense to use static const int here\n. There is SG_DEBUG to do that\n. That's pretty dangerous if it wasn't private\n. Why computer?\n. Please use usual \nif (...)\n  do\nelse\n  do\n. += makes sense here. I am not sure it is really relevant though.\n. You can avoid copy here ;)\n. I remember we have UNSTABLE but not deprecated\n. I think it would be better to have additional parentheses here\n. I am not sure but there could be some fill method to do that\n. Please avoid redundant = here with SGVector<float64_t> blabla(...)\n. Looks like it is too indented\n. This == from the beginning of the second line looks pretty surprising\n. namespace shogun\n{\n. The 'constructor' naming is not very informative. memcpy could be too specific as well. I don't have any better naming yet but it could be improved I think.\n. Indentation is ruined at some random places ;)\n. I think it makes sense to introduce a macro here instead\n. lambda ;)\n. W(i,j)*W(i,j) would be more efficient. Although no idea how much \n. Could you please explain what does this test tests? Would contraction_coefficient with other value lead to non-zero gradient? Can we also test non-zero gradient?\n. Inheritance from SGObject makes it quite big object. So I am not sure we really need it.\n. I guess 'outdated' references?\n. I'd say it would be very unlikely to see Mosek on Travis ;)\n. Can we generify this thing to add other kernels with less repetitive code?\n. @pickle27 Its just template specialization\n. get_sgmatrix I'd say\n. get_dense_features\n. No space before ;\n. get_cvMat\n. get_cvMat_from_features\n. True\n. Yeah will fix\n. Will remove\n. Why?\n. Lets make 0.01 a parameter\n. To restrict all operations but calling a member function of underlying object with ->. Unique pointer has some things like reset(), release(), etc and they can be easily mixed up with reset() of underlying object. E.g. these are different things but easy to confuse: \nstd::unique_ptr<T> obj;\nobj.reset();\nobj->reset();\n. Hmm probably I should do some magic here to push all the std stuff to .cpp.\nWe can't ignore it in SWIG because it would be needed through concrete classes anyway\n. alpha=0.01\n. Alpha\n. Indentation looks wrong here\n. Please make alpha a parameter of object (like m_alpha) which defaults to 0.01 but has some getter/setter\n. Put the default value of alpha here please ;)\n. Well it is just a cascade of all possible sizes\nThe main mess comes from no shared_ptr being used\n. Idea: can we add keywords here to annotate parameter names? width=2\n. Can we support print x,y,...,z syntax that concatenates all the stuff?\n. Should this file be in or it can be dynamic?\n. No, I'd suggest to not bother if it takes more than an hour\n. I think it is ok to drop them or make the keyword a comment. Don't want to push this but if it is easy - would be cool\n. Could you please name it get_type_list.sh?\n. That's perfect but it would be better if that file is generated on each build so that we don't have to change it manually\n. Yeahh will remove ;)\n. Yes\n. @karlnapf assert is bad?\n. Please rename it as it 'clashes' with init(CFeatures_, CFeatures_) and causes warning on clang\n. In file included from /Users/lisitsyn/Matters/OSS/shogun/src/shogun/kernel/CustomKernel.cpp:23:\nIn file included from /Users/lisitsyn/Matters/OSS/shogun/src/shogun/mathematics/linalg/linalg.h:188:\nIn file included from /Users/lisitsyn/Matters/OSS/shogun/src/shogun/mathematics/linalg/internal/modules/Redux.h:37:\n/Users/lisitsyn/Matters/OSS/shogun/src/shogun/mathematics/linalg/internal/implementation/VectorSum.h:111:15: error: no member named 'accumulate' in namespace 'std'\n                return std::accumulate(vec,vec+len,0);\n. Guys no worries I'll commit this directly, #include <numeric> solves the problem\n. Oops missed that\n. I think we rather need to have GPU dot features that can do dot product right in the GPU memory.\nIf we go with dotfeatures API we get at least two advantages:\n1) We can train with datasets that doesn't fit into RAM and GPU memory (but still use it as a kind of cache)\n2) We can use complex features (random fourier, homogeneous kernel mapping, random mapping, etc) that can be computed on the fly with less memory footprint. I think (with giant batches and deep nets) GPUing things that are done after computing input would significantly speed up the learning process anyway.\nThat's all motivated by this COFFIN thing: http://www.icml2010.org/papers/280.pdf. That's a really cool idea by @sonney2k that could be one of the main features of our neural nets\n. machine_int_t* should be ok\n. Please use doxygen syntax for equations like here: https://github.com/shogun-toolbox/shogun/blob/develop/src/shogun/kernel/GaussianKernel.h\n. EInitializationMode\n. Should be enough I think\n. Sorry I get back about that a bit late\nThere is a better standard api to do that: https://docs.python.org/2/library/argparse.html. Could you please change the code to use that so it gets much more flexible and easier to read?\n. Does it correspond to the if at line 314? Something strange with indentation\n. I am not sure you really need to use constref here - that's cheap and kernel is constructed pretty rare\n. Brevity :) \nNot sure about any benefits. SGVectors are usually passed by value \n. What about wrapping it like parameter(weights) so it does the same thing converting to string?\n. Two possible approaches to avoid strings concat here:\nTyped(data_type, \"outer_factor\") = -2*Parameter(CMath::PI)*Sqrt(Element)\nImplementation: all functions create wrappers for strings, all ops (like multiplication) just concat it to the left or to the right, assignment concats with =. Result is convertible to std::string\nor something formatted\nFormat(\"{type} {name} = -2*{pi}*sqrt(element)*pow({weights},2)}\", Parameter(\"type\") = data_type, Parameter(\"name\") = \"outer_factor\", ...)\nImplementation is probably easy\n. That looks cool for me.\nActually you don't need rewriting operators for basic types - just operators of Parameter and float/int so that any multiplication/addition/etc goes inside your object that represents expression.\n. For convenience, can we output the layer name as well? \n. Here and everywhere when we do using namespace and then declare namespace, we don't do it nested. I think it should be like \nnamespace shogun\n{\n  namespace distance\n  {\n. Lets name it initialize_neural_net as well. It does initialize weights now but if we change something we'd have to rename it.\n. Why?\n. I mean we better have namespace shogun::distance but currently it is just distance\n. Its not mentioned that the class is stateful as we provide no value to compute cost/gradient\n. supports\n. I'd go for just run or minimize\n. The purpose of class is not really clear\n. initialize to avoid ruby issues\n. init_minimization is better\n. What's classical vector representation?\n. Space missed\n. Space missed\n. And our representation\n. Something happened to indentation\n. : CWrappedMinimizer()\n. Ok sorry, true\n. It would make sense to put that as a comment into the code\n. Same here, better push it as a comment\n. Classes with one method are functions ;) Not sure what's best in this case\n. yeah, +1\n. @karlnapf @yorkerlin \nJust a doc comment - it is good to mention whether class is stateless or stateful. In this case we retrieve cost but give nothing so it is expected that the class already 'knows' the point where we compute it.\n. @karlnapf minimizer support_s_?\n. Could you fix indentation? Looks wrong\n. yes\n. Am I right it is C++11 only?\n. @yorkerlin yeah I mean will it compile w/o C99 or C++11?\n. Is it necessary for delta to be changed through reference?\n. Can we return pair of values?\n. \\| w \\|_1 = \\sum_i \\| w_i \\|\n. This looks a bit strange to me.. What is it for?\n. I think @param should come after Note ...\n. Message please ;)\n. I see\n. Can we put this matrix outside of test so its not repeated?\n. That's better! I have naming suggestion though, it is usually called 'fixture'\n. Why static and init? \nI think simple constructor would be better so that it is not possible to miss initialization.\n. Here and through the changes, new code's indentation doesn't match original one.\n. I am not sure about this thing - what is it for?\n. Could we pass an object instead? It is much cleaner and we won't have any need to maintain the enums\n. Oops, sorry, I already said that ;)\n. You need something like \nSET_SOURCE_FILES_PROPERTIES(${CMAKE_CURRENT_BINARY_DIR}/${EXAMPLE_REL_DIR}/${EXAMPLE_NAME}.cpp PROPERTIES GENERATED 1)\n. I think this could be resolved once add_executable is used.\n. Try adding #include <shogun/lib/some.h> and then using auto $name = some<C$type>($arguments) :wink: \n. @karlnapf probably yes. If there is any issue it should be possible to fix that.\n. Newline missed\n. Something wrong with indentation\n. No newline at the end\n. No newline at the end\n. No newline at the end\n. This looks suspicious you don't return anything\n. Why these functions? They are not even defined in the header file\n. ok, right\n. Use has\n. Please split these tests to some smaller ones\n. Is this comment ^ already addressed somehow?\n. Why to even have a class with the only static method? \n. What strategy did we choose and why?\n. Can we get more meaningful message? ;)\n. Why no message for build but some message for install?\n. Why not implementing std::hash for BaseTag?\n. Indentation here\n. Use push_back\n. Please not vec :) \nparams would be better\n. You can simplify that to return (map.find(tag) == map.end());\n. Indentation\n. const auto&\n. Could you rephrase so that we check if object has parameter?\n. Could you rephrase so that it is stressed that we check type?\n. Some indentation here as well\n. Could you please add Type<T> type() that returns corresponding type?\n. yeah explicit would be better to avoid extensive rehashing\n. Doc: this thing converts compiler-dependent name of class (demangles) to something human-readable. \n. Doc: This is a base interface for a policy to store some value. Policy works with some provided memory region and is able to set value, clear it and return the typename as string.\n. Puts provided value pointed by v (untyped to be generic) to storage\n. This is one concrete implementation of policy that uses pointers to store values.\n. Doc: used to store objects of arbitrary types.\n. Doc: casts hidden value to provided type, fails otherwise.\n. Erases value type i.e. converts it to Any\n. Tries to recall Any type, fails when type is wrong.\n. Lacks %newobject\n. It is not strictly correct, is it? I mean we have a bit more of them.\n. @sanuj ^ this is quite good, I agree\n. Not quite right @karlnapf. We can compare hashes then compare strings if hashes are equal.\n. bool is as strange as what we did. I am sorry about this crazy idea :)\n@sanuj lets switch to void. \n. It is used by value-semantics, not pointers. Forces to use copy ctor which is not implemented with Self.\n. create<CKernel>, this one is important for plugins switch\n. Please use dynamic_cast explicitly\n. Let it use Some instead of pointers\n. No newline - causes some red stuff here\n. @karlnapf @sanuj ok as it works lets remove swig part because we need that mighty list of base classes that should come from other PR. Not sure about all shogun classes.\n. lol\n. No!\n. Yes!\n. Not for end user\n. Why not const auto& it : self->map?\n. Indentation goes crazy\n. Please use one variable per line as this is error prone\n. Indentation\n. I\n. N\n. D\n. E\n. N\n. T\n. Implement some equals for this class\n. libepsilon is not the best name. That's shadowing the original linear_term. Ok let me highlight: \ncpp\nSGVector<float64_t> linear_term;\nif (...) {\n   SGVector<float64_t> linear_term = ...\n}. hey, sure, we don't need that variable at all. Just call the getter in-place.. Let's add one more progress(Range<T> range) that uses global SGIO to simplify things. It's initialized with init_shogun_with_defaults. Yes, it could be just getter call. Minor thing to me but would be better without variable.. Is this python3 only thing?. You may want to use Meyer's singleton here that's a bit more concise. Does that compile? Typo here. It would make sense to say if it references the pointer. Can we go auto? . Why returning a reference?. I believe void* m_data is a bit better but don't insist :). I'd suggest one member per line. This cast is unsafe, right? Should be documented or asserted. How is it related? :). I thought of RLang or #rstats (the twitter hashtag for R). Is it better?. I know. It is vague whether it is legally ok but I like it pretty much. van51 -> Evangelos Anagnostopoulos. van51 -> Evangelos Anagnostopoulos. OXPHOS -> Pan Deng\nlambday -> Soumyajit De. Shashwat Lal Das disappeared\niglesias -> Fernando Iglesias. D. Lehmann should not be in any list due to being D. Lehmann. puffin444 -> Jacob Walker. jiaolong -> Jiaolong Xu. pl8787 -> Liang Pang. hushell -> Shell Hu. Catch!. No that didn't help. But in the next patch I added something that helped ;)\nNow we don't have anything from .cpp in tags file.. @karlnapf this is the solution for ctags. It is kinda wrong but I didn't want to research more on that. For some reason it started generating tests for these ones that are abstract.. Yes, but we need another check now. Funny is that I don't think it actually was correct before.. @karlnapf need your comments here. They were registered by value which is not good.. Anything that goes into Any should have operator==, that's why I added it here. I think the parameter we store should be 'rich'. This means it is not just 'Any' but 'Any' + extras. Such as this model_selection_enabled.. Probably ok. Maybe needs a comment that it doesn't consider properties.. Was it missed? . Why not auto it : self->map?. It could be a custom visitor. Or we can add another interface 'Reducer'.. Yeah that's a good catch.. inline does not change anything as implementation is in .cpp. No that's actually to improve readability. I was having hard time checking all the * and &. Yes we can. Let me do that in other PR.. Well it does not compile otherwise.. I don't get it. We have a test for a simple class that has 'clone'.. ok got it now. Seems trivial to add? :). Well actually you don't have to test SGVector here, it might be some simple class (like Simple we have) to speed up compilation and decouple. It is a minor issue tho.. I am not sure if it works. But would work if we include std_set.i. This is a bit strange. Copy ctor doesn't ref?. I think we can introduce an std::cout-like object which supports <<. Internally it could use any logger we want.. Looks good. . I think it is fine to copy BinaryLabels most of the time. What should be rather copy-on-write is the data beneath.. I like primitive_name a bit better.. I like the approach. It would become the main interface.. @karlnapf We've got to resolve this somehow. It has to be the very same allocator that was used to allocate the array. It could be free vs. delete[] or even worse.. yes. Yes I've got it now. I guess we change it once we get rid of raw pointers.. I'd prefer labels as I was surprised to see 'lab' :). Good catch. @karlnapf I hacked it this way. You might specialize is_sg_base for specific T.. Wow, what's 8 here? :)\nI very much prefer getting rid of macro at all, if possible.. I think adding a few helper functions like do_not_compute_bias(perceptron_stop) would make code more readable.. This might be is_complete(perceptron_stop). @karlnapf this thing works now, but it definitely needs more thorough testing. Yeah I can add one.. Good idea. That's a bunch of questions :)\n\nSerialization is tricky. It seems the 'serializator' itself should know\nWe can explicitly throw something when trying to do equals\nClone is cheap but it won't work correctly. It would use the original object. Yeah but since it is here such thing might be useful anyway. Yeah map fits better here. It became possible since we use initializer list here {} and C++11 is enabled.. I'd extract variable here. I think this one is const. It could make sense to explicitly call it CMachineCRTP. I mean this->template as<P> might go into some variable.. We can use something like static_assert for meaningful messages. I don't quite agree, the type is already in the right part.. switch with default is better indeed. Yeah that's what I wrote above ;). What about making it an int enum?. it makes sense to return immediately without return_val. Oh that's too low-level. Can we introduce a constexpr function for that?. Lets typedef the actual type so we can change later.. Yeah maybe with some helper function so it looks shorter? There is some repeated code in these flags.. Typo!. Can we have something like that?\ntry:\n  for f in (\n    self._get,\n    self._get_real\n  ):\n    f(name)\nexcept:\n  ... Indentation looks strange over there. The naming is controversial :). Yes. We should use some fancy enum like https://github.com/aantron/better-enums you mentioned.\n\nThis is the only scalable way I can think of.. I like this approach but there is a tricky problem.\nHow do we ensure it already has the features initialized?. Could it be something like SG_ADD_OPTIONS(...., SG_OPTIONS(E_EXTERNAL, E_BLOCK_CONST, E_BLOCK_LINEAR, E_BLOCK_SQPOLY, E_BLOCK_CUBICPOLY, E_BLOCK_EXP)) for readability?. READ_ONLY might be a better name to avoid clashes. Is that intentional?. I am slightly missing the point of having only 2 dimensions now. Could you explain?. yeah something like that\nI also thought of something like\nvirtual AnyVisitor* enter_matrix(index_t* rows, index_t* cols) = 0;\nSo that it returns a new sub-visitor.\n. ",
    "serialhex": "Forgot about the Kernel.* files... will update & issue another request.\n. Forgot about the Kernel.* files... will update & issue another request.\n. @shogun-toolbox:  The reason I kept that there is because - like I said in my commit - the python swig bindings weren't compiling right (swig was throwing up warnings about how it didn't know whether to use the float32_t or float64_t function).  So instead of getting rid of it, I thought I would keep it there until I (or someone else) fixed that particular problem.  I guess I wasn't explicit enough this morning when I committed.  Either way I'm going to work on this today and see if i can squash this bug!\n. @shogun-toolbox:  The reason I kept that there is because - like I said in my commit - the python swig bindings weren't compiling right (swig was throwing up warnings about how it didn't know whether to use the float32_t or float64_t function).  So instead of getting rid of it, I thought I would keep it there until I (or someone else) fixed that particular problem.  I guess I wasn't explicit enough this morning when I committed.  Either way I'm going to work on this today and see if i can squash this bug!\n. @shogun-toolbox: Proof (or at least really strong evidence) that they are normally distributed random numbers: https://github.com/serialhex/shogun_extra/blob/master/norm_dist/normal_histogram.png\nI am getting to work on the SWIG problem in a few... just thought I'd post this while I had it.\n. @shogun-toolbox: Proof (or at least really strong evidence) that they are normally distributed random numbers: https://github.com/serialhex/shogun_extra/blob/master/norm_dist/normal_histogram.png\nI am getting to work on the SWIG problem in a few... just thought I'd post this while I had it.\n. I have added one commit.  This prevents SWIG from throwing an ambiguity fit due to there being 2 functions with default arguments, and it not being able to resolve between them.  See http://www.swig.org/Doc1.3/SWIGDocumentation.html#ambiguity_resolution_renaming and the part on \"Notes on %rename and %ignore\" in that section.  Effectively because of the fact that float32_t & float64_t are typedefs, SWIG can't parse them (for some reason) and I can't %rename them (not only is it in the doc, but I tried, trust me I tried!!)  I left the float32_t version with the default arguments of mean = 0 & variance = 1, to make life (hopefully) simpler for all who might use this.  Though, if Soeren or one of the other people running this project want I can easily take out the default arguments completely (I can see the benefits & detriments of both options) and recommit.  Otherwise, here it is to merge.  Thank you for your patience (and next time I'll simply wait until it's completely done! :P )\n. I have added one commit.  This prevents SWIG from throwing an ambiguity fit due to there being 2 functions with default arguments, and it not being able to resolve between them.  See http://www.swig.org/Doc1.3/SWIGDocumentation.html#ambiguity_resolution_renaming and the part on \"Notes on %rename and %ignore\" in that section.  Effectively because of the fact that float32_t & float64_t are typedefs, SWIG can't parse them (for some reason) and I can't %rename them (not only is it in the doc, but I tried, trust me I tried!!)  I left the float32_t version with the default arguments of mean = 0 & variance = 1, to make life (hopefully) simpler for all who might use this.  Though, if Soeren or one of the other people running this project want I can easily take out the default arguments completely (I can see the benefits & detriments of both options) and recommit.  Otherwise, here it is to merge.  Thank you for your patience (and next time I'll simply wait until it's completely done! :P )\n. that's the next step, i'll be working through them, cleaning up the code & making sure they work.  (i even installed debian on my machine to do all this fun stuff, as my old laptop/server doesn't want to compile shogun for some reason).\n. that's the next step, i'll be working through them, cleaning up the code & making sure they work.  (i even installed debian on my machine to do all this fun stuff, as my old laptop/server doesn't want to compile shogun for some reason).\n. this should work now.  (and i should remember to do a pull --rebase before i push request :P )\n. this should work now.  (and i should remember to do a pull --rebase before i push request :P )\n. from README.EXT:\nYou can use the macros\nStringValue() and StringValuePtr() to get a char* from a VALUE.\nStringValue(var) replaces var's value with the result of \"var.to_str()\".\nStringValuePtr(var) does same replacement and returns char*\nrepresentation of var.  These macros will skip the replacement if var\nis a String.  Notice that the macros take only the lvalue as their\nargument, to change the value of var in place.\nit's a macro that converts a ruby string to a C string.  also rb_str2cstr is depreciated, so this will work on either ruby 1.8 or 1.9.  though if you like i can probably separate the 2 commits.\n. ",
    "sonney2k": "On Sat, 2011-04-09 at 09:41 -0700, serialhex wrote:\n\nOkay, after much work & learning what to do & not to do I've finally got this.  Here is my patch for the Exponential Kernel.  I'm new to git so it's taken me a while, but things should go a lot smoother now.\n\nGood job! I am new to all this either - so don't worry :)\nAnyway applied!\nFor the one fact about the future of which we can be certain is that it\nwill be utterly fantastic. -- Arthur C. Clarke, 1962\n. On Sat, 2011-04-09 at 09:41 -0700, serialhex wrote:\n\nOkay, after much work & learning what to do & not to do I've finally got this.  Here is my patch for the Exponential Kernel.  I'm new to git so it's taken me a while, but things should go a lot smoother now.\n\nGood job! I am new to all this either - so don't worry :)\nAnyway applied!\nFor the one fact about the future of which we can be certain is that it\nwill be utterly fantastic. -- Arthur C. Clarke, 1962\n. Hi Karlnapf,\nOn Mon, 2011-04-11 at 01:35 -0700, karlnapf wrote:\n\nAs far as I have understood the serialization process, the only thing that is needed to add serialization support to a class is registering its attributes as parameters. Then CSGObject then does the rest.\nIn this case, the length of some matrices are depended on the initialisation, so, in contrast to Kernel.cpp, the parameters are registered after they are intitialized.\nSince the length of the matrices is not originally stored in the class, I created new member variables for them. Using stack variables for the add-method does not work. Is this way ok, or should I do this differently?\nI se- and deserialized an instance. It worked.\n\nThat is exactly how you should do it - just adding member variables for\nvector/matrix sizes and then mentioning them in par->add*() .\nSoeren\n\nFor the one fact about the future of which we can be certain is that it\nwill be utterly fantastic. -- Arthur C. Clarke, 1962\n. Hi Karlnapf,\nOn Mon, 2011-04-11 at 01:35 -0700, karlnapf wrote:\n\nAs far as I have understood the serialization process, the only thing that is needed to add serialization support to a class is registering its attributes as parameters. Then CSGObject then does the rest.\nIn this case, the length of some matrices are depended on the initialisation, so, in contrast to Kernel.cpp, the parameters are registered after they are intitialized.\nSince the length of the matrices is not originally stored in the class, I created new member variables for them. Using stack variables for the add-method does not work. Is this way ok, or should I do this differently?\nI se- and deserialized an instance. It worked.\n\nThat is exactly how you should do it - just adding member variables for\nvector/matrix sizes and then mentioning them in par->add*() .\nSoeren\n\nFor the one fact about the future of which we can be certain is that it\nwill be utterly fantastic. -- Arthur C. Clarke, 1962\n. Otherwise - nice plan!\n. Otherwise - nice plan!\n. It just strikes me that alesis is trying to achieve sth similar. He defines a Gaussian class derived from CDistribution and then could potentially define a CombinedDistribution object (in similar spirit to CombinedDistance/CombinedKernel) to get a mixture of distributions or Gaussians in this case.\n. I am actually very happy about this initial draft. I have tried to give you as much feedback as possible. Hopefully it will simplify the code a bit and I am all for doing this for the other feature types (when that one is finished and tested - I would suggest to write a python_modular function to test it, where you set a subset_idx that is exactly 0...(num_vec-1) - i.e. identical and then run some of the svm examples).\nI think this will enable us to only have to specify the data set once, setting different indices for training/validation/testing etc. Should be one of the most difficult parts already. The other difficult part is how to register the model parameters. When these two are resolved you are on track.\n. I am actually very happy about this initial draft. I have tried to give you as much feedback as possible. Hopefully it will simplify the code a bit and I am all for doing this for the other feature types (when that one is finished and tested - I would suggest to write a python_modular function to test it, where you set a subset_idx that is exactly 0...(num_vec-1) - i.e. identical and then run some of the svm examples).\nI think this will enable us to only have to specify the data set once, setting different indices for training/validation/testing etc. Should be one of the most difficult parts already. The other difficult part is how to register the model parameters. When these two are resolved you are on track.\n. Indeed it is not a good idea to copy the whole feature matrix just to be able to compute std-dev / variance. My proposal would be to add cov and mean functions to CDotFeatures - and try to be as efficient (code and memory wise there).\nHowever, it makes sense to have a function in CMath for computing the covariance / mean of a matrix (over rows or columns). Since e.g. CPCACut also needs this.\n. Indeed it is not a good idea to copy the whole feature matrix just to be able to compute std-dev / variance. My proposal would be to add cov and mean functions to CDotFeatures - and try to be as efficient (code and memory wise there).\nHowever, it makes sense to have a function in CMath for computing the covariance / mean of a matrix (over rows or columns). Since e.g. CPCACut also needs this.\n. You should also add serialization support. Look at the init() function of recently added kernels or of CKernel::init() on how that works.\n. couldn't find a thing to critizise :)\n. actually one thing, could you please add the forumla in Gaussian.h header such that it is generated in doxygen doc? \nthanks.\n. thanks for the patch - applied as is.\n. thanks for the patch - applied as is.\n. On Thu, 2011-04-14 at 12:15 -0700, karlnapf wrote:\n\ndone\n\nsubsetting is not yet working correctly though :(\nI ran the test suite - and it crashes - currently looking into it.\nSoeren\nFor the one fact about the future of which we can be certain is that it\nwill be utterly fantastic. -- Arthur C. Clarke, 1962\n. Thanks for the patch!\n. Looks like a plan to me. Go ahead.\n. Please rename to ParzenWindow (not ParsenWindow - sp mistake!)\n. I don't see it. You only use bessel functions of the first kind of order 1. The formula in the blog says that a different orders are used.\n. You can make your life a lot easier by utilizing CKernelMachine internally - so you should really, really use it.\nOtherwise I have some (minor coding style) change requests: put the 'else' in an 'if' clause on an extra line\nif ()\n{\n}\nelse\n{\n}\nVariable name should not look like m_nVecSize but m_vec_size.\nThanks!\n. Yes exactly. So could you please do this generalization?\nThanks.\nSoeren\nOn Mon, 2011-04-18 at 02:33 -0700, haipengwang wrote:\n\nDear Soeren Sonnenburg,\nThanks for the comments.\nI will try to incorporate CKernelMachine in the implementation.\nActually I think gaussian-kernel based parzen window estimator has very straight-forward formulation, and I think it doesn't cost too much to direct write a simple function to calculate gaussian-kernel distance, so in the beginning I don't consider incorporating CKernelMachine in the implementation.\nBut I agree with you that using CKernelMachine will make parzen window class more generalizable, and it is easy to generalize from gaussian kernel to other kernel functions.\nThanks.\nHaipeng Wang\n\n\nFor the one fact about the future of which we can be certain is that it\nwill be utterly fantastic. -- Arthur C. Clarke, 1962\n. I understand what you are saying. Nevertheless, you can use CKernelMachine internally to avoid code duplication. Note that coding style is still not properly fixed too.\n. Plus you should really utilize shoguns features, like CDotFeature classes.\n. Heh that is because you didn't specify the include path relative to the root of the libshogun directory. Minor thing. Applying and fixing.\n. I understand the concepts you outlined and you seem capable of doing this. Now the only thing that I don't know is whether you can write clean OOish readable code. So please clean up this mess(tm) now and write a simple feature streaming class that uses CFile / and a line reader there (no parallelization for now if you want - if this makes things too complex) - such that I can merge your patch.\nPlease ask if you have any questions.\n. Both. Isolate the parser stuff into CFile or so (see below) and then let your StreamingFeatures use it (clean up the interface). Parallelized reading would be nice but given the time constraint can be done later.\nYou have to write the (buffered) line reader yourself. The current CFile class (and its derivatives AsciiFile etc) don't have any streamed i/o functions. The getvector / getmatrix functions read the whole file content as vector / matrix. So it would be best if you added get_streamed_*_vector / matrix etc functions to CFile etc or create a new StreamingFile class with the necessary functions.\n. Both. Isolate the parser stuff into CFile or so (see below) and then let your StreamingFeatures use it (clean up the interface). Parallelized reading would be nice but given the time constraint can be done later.\nYou have to write the (buffered) line reader yourself. The current CFile class (and its derivatives AsciiFile etc) don't have any streamed i/o functions. The getvector / getmatrix functions read the whole file content as vector / matrix. So it would be best if you added get_streamed_*_vector / matrix etc functions to CFile etc or create a new StreamingFile class with the necessary functions.\n. I see that you made quite some steps forward. I mostly had minor comments. Nevertheless, your patch still needs some work before it can be merged.\n. I see that you made quite some steps forward. I mostly had minor comments. Nevertheless, your patch still needs some work before it can be merged.\n. Lets first get this one patch ready for merge into shogun and then do improvements on top of it.\n. Merged. Now it is time for benchmarks and improvements.\n. sorry only read your diff but didn't see it is from a branch other than\nmaster.\n. Apart from this - this patch looks nice.\n. Hi Lei,\nwhy doesn't \nm_parameters->add((CSGObject**) &subkernel, ...)\nwork?\n. Hi Lei,\nwhy doesn't \nm_parameters->add((CSGObject**) &subkernel, ...)\nwork?\n. Even though this patch looks good overall (just see my minor comments), this patch is becoming rather big. So please fix the minor issues and I will apply it.\nThanks.\n. Even though this patch looks good overall (just see my minor comments), this patch is becoming rather big. So please fix the minor issues and I will apply it.\nThanks.\n. I still see some whitespace issues in the patch and btw it does not compile when applied here.\nIf I understand you correctly you would like to add a pure virtual method \nvirtual void register_params()=0;\nto CSGObject. Then the constructor of CSGObject will call register_params() and within each register_params() one would call the base class e.g. CKernel::register_params(). I like the idea - it is quite a bit of work though.\n. I still see some whitespace issues in the patch and btw it does not compile when applied here.\nIf I understand you correctly you would like to add a pure virtual method \nvirtual void register_params()=0;\nto CSGObject. Then the constructor of CSGObject will call register_params() and within each register_params() one would call the base class e.g. CKernel::register_params(). I like the idea - it is quite a bit of work though.\n. I prefer what I suggested above - but you inspired this to say the least.\n. I prefer what I suggested above - but you inspired this to say the least.\n. Hi Siddharth, first of all thanks for your patch. \nI have a few questions:\n1) Did you test it (compare against SGD-QN?)  How can I test it?\n2) Did you have a look at your patch? https://github.com/shogun-toolbox/shogun/pull/55/files - it seems it has many whitespace issues. Could you also please remove curly brackets from for loops with a single statement?\n. The correct fix is actually to use '\"' instead of '<' and '>'. This is because we rewrite all include paths upon make install.\n. The correct fix is actually to use '\"' instead of '<' and '>'. This is because we rewrite all include paths upon make install.\n. ok fixed in master\n. ok fixed in master\n. Merging this version now. Would be great if you could finish your work though...\n. Merging this version now. Would be great if you could finish your work though...\n. Yes that looks great! I only changed the total_size to be of size_t type (arrays can be >2GB) and added a couple of white spaces to improve readability. Thanks for your patch!\n. Yes that looks great! I only changed the total_size to be of size_t type (arrays can be >2GB) and added a couple of white spaces to improve readability. Thanks for your patch!\n. Thanks merged.\n. Thanks merged.\n. Thanks for your patch. It has one (or two) issue(s) though: If you have a plain ascii file that just contains\n1 2 3\nYour could previously load it via (python modular code):\nfrom shogun.Library import \nfrom shogun.Features import \nf=AsciiFile(\"simplefile.ascii\")\nlab2=Labels()\nlab2.load(f)\nprint lab2.get_labels()\nsame for matrices\n1 2 3 4 5\n6 7 8 9 10\nWe absolutely have to support these plain files (as many people provide data like that). I haven't checked your changes for the binary format - but I guess the same issue (though breaking here is not as bad)\n. Thanks for your patch. It has one (or two) issue(s) though: If you have a plain ascii file that just contains\n1 2 3\nYour could previously load it via (python modular code):\nfrom shogun.Library import \nfrom shogun.Features import \nf=AsciiFile(\"simplefile.ascii\")\nlab2=Labels()\nlab2.load(f)\nprint lab2.get_labels()\nsame for matrices\n1 2 3 4 5\n6 7 8 9 10\nWe absolutely have to support these plain files (as many people provide data like that). I haven't checked your changes for the binary format - but I guess the same issue (though breaking here is not as bad)\n. Yes that would be better. This way there is no incompatible change and I can immediately apply your patch.\n. Yes that would be better. This way there is no incompatible change and I can immediately apply your patch.\n. Thanks merging.\n. Thanks merging.\n. Anyway merging this first attempt - hopefully it can be improved...\n. Please submit the getline patch independently (it has higher priority as it breaks things anyways).\nRegarding this patch: I don't like this solution. You still have lots of if ()'s in your code checking for labeled or unlabeled example. My suggestion would be to either have 2 parser classes instead of one (avoiding all the if's) or (what I would prefer) one class with a function get_next_example() that returns a vector and label if use_label is on and otherwise label undefined.\nPlease note that I recently introduced SGVector (cf DataType.h) - I suggest to use that wherever possible.\n. Yes, much better though I still have many comments. Besides some obvious bugs e.g. \nex_in_use_mutex=new pthread_mutex_t[buffer_size];\nshould probably not be buffer_size tall you should name your files like you name a class but without the C prefix, e.g. CParser -> Parser.h. In addition, we have SGVector now - which is what we use instead of double* x, int32_t len. So please use that as data structure in the following way: SGVector get_vector()   and float64_t get_label(). I thought a bit about the get_next_example / get_next_labelled example business and I think we should replace it by a simpler approach:\nThe workflow would be:\n1. fetch_example() (a function returning void)\n2. get_vector() and optionally (if available otherwise 0 get_label())\n3. release_example()\nSo please continue your work taking my comments into account.\n. Yes it is basically your workflow: All I would like to see changed is that you instead of get_next_* / free_feature_vector introduce 1-3 above. To make it explicit\n``` C++\nfeatures->start_parser();\nwhile (features->fetch_example())\n{\n   SGVector vec = features->get_vector();\n   float64_t lab=features->get_label(); / this is optional /\n   features->release_example();\n}\nfeatures->end_parser()\n```\n. You can assume that the features will either be all labeled or not.\nSo better issue an SG_ERROR when labels are requested but the file is unlabeled.\n. I've merged this but I have one more request: Could you please rename buffer.* / parser.* to match the class names defined inside them? E.g. CParseBuffer -> ParseBuffer.h\n. I would make this a static function in CAsciiFile\n. A static member function of the AsciiFile class - you can keep it\ngetline compatible though.\nOn Tue, 2011-06-07 at 02:40 -0700, frx wrote:\n\nThis would be a member of the AsciiFile class? Or should I stick to the\noriginal specification of getline and keep it outside the class?\nOn Tue, Jun 7, 2011 at 2:13 AM, sonney2k \nreply@reply.github.comwrote:\n\nI would make this a static function in CAsciiFile\n\nReply to this email directly or view it on GitHub:\nhttps://github.com/shogun-toolbox/shogun/pull/74#issuecomment-1311739\n\n\n\nFor the one fact about the future of which we can be certain is that it\nwill be utterly fantastic. -- Arthur C. Clarke, 1962\n. (but getline compatibility is definitely not a requirement)\n. Great work besides my really only minor comments\n. Yes, rationale is that the outer loops (or if clauses etc) have\nmultiple-line statements that should be enclosed in { } for readability.\n. exactly like this - when I find time to do a SGVector/SGMatrix transition we even won't need the additional ADD's - but for now this is exactly what I had in mind - great job!\n. But then len is not set. I asume that strings in ruby are '\\0' terminated - in shogun '\\0' can be part of the string even though not all interfaces support that. I don't know how to do it properly though.\n. I am not really sure what to do with this... closing for now.\n. did you test if your getline is faster / slower then the GNU getline... ? I tend to always use your implementation.\n. Could you please do a quick benchmark to support this?\nThanks!\n. can you also convert getline in CAsciiFile to use the buf read_line?\n. btw, it makes sense to also test for CSignal::cancel_computations() and return safely if the functions returns true.\nThis will enable ctrl+c to safely stop.\n. excellent work!\n. btw, instead of set_nth_mean - couldn't you use sth like set_means(SGMatrix)?\n. The code in these respective lines indeed is not OK. Regarding losses\n- Please put them in shogun/loss \n- Please note that we have these loss functions also in SGD/SGDQN - so please integrate these too and make SGD/SGDQN use them.\n. Yes I know. They nevertheless define a loss(.,.) and dloss function so it would just work fine and it does not make sense to have code duplication between sgd-qn / sgd / vw. So yes put a member variable loss in there new it with the right default and good.\n. yes I know - you just need to do this part for certain losses.\n. Nice work, I have a few comments though...\n. btw, did you test whether sgd/sgdqn are still working?\n. It is a monster patch again... :-(\n. does the out typemap work if you comment the no-working in typemap?\n. you indeed did some code reindention for a whole file... \n. yeah please put this under classifier/vw/ and attempt to merge the generic code into i/o classes etc\n. OK I think this is a good starting point now.\n. - I hopefully fixed the make clean target.\n- It makes a lot of sense to convert to using float32_t for online learners.\n- the  include stuff will have to stay though. maybe it can be improved by changing include order or so...\n. - The patch looks great :)\n- I think you don't need to search linearly but could use CMath::binary_search()\n- How about instead of hard coding anything in version.sh to modify the NEWS file to have a line like \nSHOGUN Release version 0.99.9 (libshogun 10.0, data 0.2, format 1.0)\nand then extract it?\n- Finally I didn't get from the patch how the conversion could actually work. Currently it is 'just' a map (old version,old type) -> (new_version, new type). I assume this map will either be part of SGObject or the respective class derived from it and then all translation is done in serializable_post?\n. - this should go into lib/Hash.{cpp,h}\n- changing data type can be done automatically at the end (after you've validated that everything works OK)\n. Well it is a good starting point - would be great if you could get these all to work reliably ...\n. please use float32_t instead of just float.\n. Could you please also support serialization for such classes via SG_ADD()?\n. yes exactly, basically one registers all variables that are essential to (de)serialize an object.\n. I am merging - but it would be great if you could do the float32_t transition and provide the api such that this can be used from all dot-feature based learners (like SGD ...)\n. How reliable is the hashset? Does it work even with collisions?\n. - Please only overload train_machine() - CMachine::train() just calls train_machine.\n- Please also add an example where Leons SGD is used instead of VW with the StreamingVwFeatures\n. I wanted to merge this pull request - but it cannot currently be merged - please merge and fix conflicts and then I will\n. Yes please, put in the the applications/vw folder!\n. pluskid did that\n. done :)\n. done\n. could you please add some python modular example for that? I guess you could even craft some nice graphical - but non-graphical is a must (also used for regression tests)\n. I would really prefer to have a switch instead of this rhs != lhs logic above. I mean it can make sense to compare lhs examples with each other via mahalanobis...\n. MSVC will certainly choke on pthreads - but besides that it should not be too hard or?\n. are you still working on this?\n. this patch is mostly ok but has quite a bit of commented code that should be removed... I marked just some small parts of it...\n. You should check your example with valgrind (it will leak memory and have some uninitialized memory reads). Besides, I would prefer a `real' implementation in shogun/regressions/ though.\n. your patch looks very nice to me - just fix the remaining TODOs and double check that it works (e.g. run it through valgrind / compare results with the scikits implementation) and it is fine to be merged!\nThanks a lot!\n. Nice work, I will have a more detailled look at your code today.\n. harshit - dot feature operations are documented here http://shogun-toolbox.org/doc/en/current/classshogun_1_1CDotFeatures.html\nso where are you stuck?\n. note that your patch cannot be applied (conflicts...)\n. closing this pull request and continuing discussion on the other rebased one\n. So do I understand this right - this is a prototypical implementation of the above paper in python which you would want to write in C++ in shogun for GSoC?\n. So do I understand this right - this is a prototypical implementation of the above paper in python which you would want to write in C++ in shogun for GSoC?\n. Well you are only half way done - you should do this within shogun to show that you can extend and work with its code base.\n. Could you please split this up, i.e. the fixes in one pull request and the cluster perf measure in a separate one?\n. One more general comment\nplease follow shogun coding style (cf. README.developer), e.g.\nfor (...)\n{\n}\nand in copyright use your real name such that your patch can be properly attributed.\n. Forgot to mention - nice work!\n. please let me know when you are done polishing this part. I've merged the test generation stuff in the meantime\n. did you get rid of the CStreamingAsciiFile errors too?\nthe liblinear warning is not so worrying - can happen depending on data/scaling :)\n. Thanks a lot for your work!\n. yes\n. Please add proper copyright headers.\n. Thanks for your work!\n. Harshit, looks like very nice work now! Please do the few final formatting fixes - mostly indention is still wrong / whitspace fixes and ifdefs need to be at beginning of lines and then this can be merged!\n. thanks a lot your git insight is very much welcome\n. Hi!\nI don't have time right now to review your code - so for now only some brief comments:\n- Please add proper copyright headers (see any other shogun class) - it is your code so you should get credit!\n- Please add doxygen style documentation in the header file (look at other shogun classes for an example)\nI will try to give more feedback this evening.\n. Nice work - even though we cannot merge this right now.\n. I was trying to give detailed feedback about this patch but never (still don't) know where to start: Problem is that putting all this opencl /viennacl dependent stuff all over kernels, kernelmachines, features with #ifdef's everywhere becomes messy.\nWith lapack things were easy we either had some wrapper function somewhere that uses the lapack variant when available at compile time or some 'simple' replacement or the whole class was ifdef'd.\nSo what I am thinking might be better is to have some lib/opencl.{cpp,h} or so with the relevant functions defined to reduce code 'clutter'.\n. Look at the git submodule data which is in data/testsuite/tests\na new qda file should be there - one can do the same as with shogun\nmaster. git add a file and send a pull request for shogun-data\nlets get python modular to work first...\nHTH\n\nFor the one fact about the future of which we can be certain is that it\nwill be utterly fantastic. -- Arthur C. Clarke, 1962\n. care to send a pull request?\n. ahh and there is no work needed to map the constructor - just do \nsubset=Subset(numpy.array([0,1,2,3], dtype=numpy.int32_t))\n. I am merging this - what is missing still is the shrinking stuff\n. please open a new pull request with the changes we discussed on IRC - closing this one.\n. Hi harshit,\nI think you are running things with different parameters. To check this you should just add some SG_PRINT's and print out all the parameters TSVM directly before/when it starts training and then compare once with running the external command and once inside shogun.\nSoeren\n. needs a complete rebase\n. I am not +1 for moving the multitask kernel normalizers - hmmhh. It actually doesn't even help to expose these normalizers to the outside but I would rather prefer that DA algos internally use these to achieve DA. This kind of is a change in paradigm (one could do lots of algorithms simply by combining method X with Y) but things were never explicit. But explicit is good and much easier to use and remember even for the original authors (as I can tell!).\n. please keep Tron.h/cpp where it was then OK\n. actually tron should be exposed via  libqp so ok we can move things for now but should keep in mind that we want it somewhere else later\n. thanks - we cannot use multiple inheritance though. so either we have multiclassmachine -> multiclasskernelmachine\nor kernelmachine -> multiclassmachine but I prefer the first option so one should probably use CMachine instead of CSVM and do appropriate casts. to do so maybe we need similar to what we have for features (feature properties) machine properties that then could be set to linear, multiclass, kernel or so.\n. needs a complete rebase\n. Nice work - did you compare this to the pinv solution? Is it faster more or less stable?\n. Any progress on this?\n. we need to change the api - the m_do_free flag can go away - we can just use m_refcount=NULL if we are not supposed to do ref/unref. \n- so change the constructor to accept a bool no_ref_counting -> and if true set m_refcount=NULL.\n- remove do_free stuff\n. needs a complete rebase\n. actually SGVector w is OK since this will only copy the ptr and the int for len (refcount). so it won't really make any difference...\n. needs a complete rebase\n. needs a complete rebase\n. needs a complete rebase\n. this issue is hopefully fixed in current git\n. this issue is hopefully fixed in current git\n. Is this to be merged? If so please put the structured output stuff in the shogun/structure folder (where the other old SO stuff is). Regarding naming, I think we should instead of SO.i etc use the full name - either Structure.i or StructuredOutput - at least in the base classes. For the rest it is totally fine to do abbreviations.\n. Please fix the indention problems and add some minimal example to examples/undocument/libhshogun.\n. ok, so now please add some minimal example!\n. note that I merged your PR yesterday and did some fixes to get it to compile with the new SO framework (things are under shogun/structure now). So please do a new PR with only the updates to this merge - this one here does no longer apply.\n. Can you please open a new pull request?\n. Can you please try if it works for your case? I admittedly would want to see a couple of changes to the lib - like use SG_FREE/ SG_CALLOC / SG_PRINT and no sse* optimized code if it does not give some big speedup (which I don't expect). the vector functions should probably be better merged into SGVector (if not yet available)...\nAll the rest can stay as is.\n. I am merging this for now but I think we should have this as new datatype in shogun/lib sparse stuff just with more overhead...\n. This is a feature of swig. One can use \nswig -includeall\nto get 'expected' behavior but then swig will attempt to wrap everything (or the generated wrapper file was not compilable - don't remember).\n. - I fully agree with 1 and 2.\n- 3) use the one from CMath\n- 4) I prefer overloading\n- 5) makes sense to add it yes with sane default\n- 6) it should not - maybe you derive from this abstract class some other class but forgot to implement get_name there?\n- 7) I think yes - but lets see what heiko says\n- 8) please ask gsomix about this\n. could you please ask Shai - what the training time for MNIST or so was - just to be able to compare the speed of the impl. in shogun and theirs?\n. @puffin444 - just use the 'true' incremental murmur hash. It is OK to break these existing two implementations but we need to document it in NEWS (under API changes). Regarding valgrind - please run configure with --enable-debug --disable-optimization - valgrind sometimes reports crap when the code is highly optimized.\n. And @puffin444 , @karlnapf  - please report the errors you get in CMap to gsomix otherwise he cannot fix them.\n@puffin444 once you addressed heikos comments this can be merged - ping any of us (heiko, sergey, me to do it)\n. what exactly where the errors?\n. what exactly where the errors?\n. can you give us any numbers how much this speeds up computations?\n. please avoid spaces between function names and brackets, e.g. don't do foo () but do foo() instead\n. is the indention level right (I cannot really see at github): 1 tab == one indention level? otherwise I am fine with the patch now.\n. is the indention level right (I cannot really see at github): 1 tab == one indention level? otherwise I am fine with the patch now.\n. I dont' know - better check shogun standard is\nset ts=4            \" tab == 4 spaces\n. I dont' know - better check shogun standard is\nset ts=4            \" tab == 4 spaces\n. lets do it differently then - as I wrote on IRC - write a function acquire_lock / release_lock in CMap and one manually has to call these if messing with CMap in threaded applications\n. lets do it differently then - as I wrote on IRC - write a function acquire_lock / release_lock in CMap and one manually has to call these if messing with CMap in threaded applications\n. (FYI github says that this cannot be automatically merged - probably needs some rebase?)\nRegarding your questions:\n1. Yes always use SG_MALLOC except for new CMyClass() business.\n2. Yes we exclusively use templates to support multiple types (and avoid code duplication here).\n3. Why do you need this assumption? Aren't you working on kernels only anyways and so can deal with any kind of feature mixtures in combined features?\n4. We can do that later - sergey is converting things to openmp currently so it will be much easier later. Ask him about it in IRC.\nAhh and keep the vector model selection stuff. We will soon need it!.\n. (FYI github says that this cannot be automatically merged - probably needs some rebase?)\nRegarding your questions:\n1. Yes always use SG_MALLOC except for new CMyClass() business.\n2. Yes we exclusively use templates to support multiple types (and avoid code duplication here).\n3. Why do you need this assumption? Aren't you working on kernels only anyways and so can deal with any kind of feature mixtures in combined features?\n4. We can do that later - sergey is converting things to openmp currently so it will be much easier later. Ask him about it in IRC.\nAhh and keep the vector model selection stuff. We will soon need it!.\n. so what is the status on this?\n. Please don't include any eigen or bias lapack stuff in a .h file\nViktor Gal reply@reply.github.com wrote:\n\nUse pkg-config to detect eigen3, as it supplies pkg-config \nscript and include eigen headers as they\nare ment to be included (see documentation of eigen).\nYou can merge this Pull Request by running:\ngit pull https://github.com/vigsterkr/shogun master\nOr you can view, comment on it, or merge it online at:\nhttps://github.com/shogun-toolbox/shogun/pull/670\n-- Commit Summary --\n- Use the supplied pkg-config script to detect eigen3\n-- File Changes --\nM src/configure (11)\nM src/shogun/mathematics/Math.h (2)\n-- Patch Links --\nhttps://github.com/shogun-toolbox/shogun/pull/670.patch\n https://github.com/shogun-toolbox/shogun/pull/670.diff\n\nReply to this email directly or view it on GitHub:\nhttps://github.com/shogun-toolbox/shogun/pull/670\n\n\nSent from Kaiten Mail. Please excuse my brevity.\n. closing for now\n. scilab should not be a dependency or did you mean scipy? if you create a graphical example - please move it to the graphical/ folder and add one simple function/script that in the examples folder that we can use for testing\n. matrix algebra with eigen doesn't need comments so all good!\n. What is really missing to the whole SO framework is a way to work w/o MOSEK. Anyway please do these changes above and then the code can be merged.\n. what is missing is some kind of test / example showing the effect of this and enabling us to detect whether future updates would break your code...\n. You missed one case when returning the type. Sorry for the brief statement I am on my mobile phone now.\nChiyuan Zhang notifications@github.com wrote:\n\nMainly the fix of the typo.\nYou can merge this Pull Request by running:\ngit pull https://github.com/pluskid/shogun fix-typo\nOr you can view, comment on it, or merge it online at:\nhttps://github.com/shogun-toolbox/shogun/pull/692\n-- Commit Summary --\n- rename typo: Euclidian => Euclidean\n- fix GMM typo.\n- fix DenseFeatures segfault.\n- fix KNN bug.\n-- File Changes --\nM src/interfaces/modular/Distance.i (8)\nM src/interfaces/modular/Distance_includes.i (4)\nM src/shogun/clustering/GMM.cpp (6)\nM src/shogun/converter/EmbeddingConverter.cpp (4)\nM src/shogun/converter/LocallyLinearEmbedding.h (2)\nM src/shogun/converter/MultidimensionalScaling.cpp (2)\nM src/shogun/converter/StochasticProximityEmbedding.cpp (2)\nM src/shogun/converter/StochasticProximityEmbedding.h (2)\nR src/shogun/distance/AttenuatedEuclideanDistance.cpp (16)\nR src/shogun/distance/AttenuatedEuclideanDistance.h (22)\nM src/shogun/distance/DirectorDistance.h (2)\nM src/shogun/distance/Distance.h (6)\nR src/shogun/distance/EuclideanDistance.cpp (18)\nR src/shogun/distance/EuclideanDistance.h (22)\nM src/shogun/distance/MinkowskiMetric.h (2)\nR src/shogun/distance/SparseEuclideanDistance.cpp (16)\nR src/shogun/distance/SparseEuclideanDistance.h (20)\nM src/shogun/features/DenseFeatures.cpp (10)\nM src/shogun/features/SubsetStack.cpp (11)\nM src/shogun/features/SubsetStack.h (4)\nM src/shogun/preprocessor/DimensionReductionPreprocessor.cpp (6)\nM src/shogun/ui/GUIDistance.cpp (8)\n-- Patch Links --\nhttps://github.com/shogun-toolbox/shogun/pull/692.patch\nhttps://github.com/shogun-toolbox/shogun/pull/692.diff\n\nReply to this email directly or view it on GitHub:\nhttps://github.com/shogun-toolbox/shogun/pull/692\n\n\nSent from Kaiten Mail. Please excuse my brevity.\n. i tried to mark those - but better grep -ir euclidian shogun ...\n. or you write a test checking if the output is still the same with the old /new version :D\n. shouldn't it be +1 ?\nOn Mon, 2012-08-13 at 12:53 -0700, Viktor Gal wrote:\n\nThe -1 should be removed as otherwise it fails with a segfault.\nWhen the -1 is removed the unit test that i've defined for SVMOcas and\nMulticlassOcas runs smoothly! ;)\n\u2014\nReply to this email directly or view it on GitHub. \n\n\nFor the one fact about the future of which we can be certain is that it\nwill be utterly fantastic. -- Arthur C. Clarke, 1962\n. This page is auto-generated, so could you please edit the google document with the feature table? Just fork this here https://docs.google.com/spreadsheet/ccc?key=0Aunb9cCVAP6NdDVBMzY1TjdPcmx4ei1EeUZNNGtKUHc and add the respective rows and let us know the url... \n. this is wrong though - you should let it check for pthread first and then check for mosek.\n. thanks\n. #854 is merged so closing this\n. We will of course help if someone attempts this and if there are issues. What is clear is that shogun got more complex considering that we have interfaces to various languages now.\n. I guess there is not more we can do here. Closing.\n. well would be nice (like would be having SGString*) but not necessary for 2.1. could be in >2.1 aswell\n. please be as specific as possible\n. any news on this?\n. that's fixed since 3e114b4894d2870f759ca394836a0fa4dd4720cf\n. hopefully fixed in 7a4c62e\n. This is not the right fix and would break all earlier OSX releases. Is perhaps __MAC_10_7_4 or so defined in your Availability.h?\n. it should work with latest git\n. alright then let us know exactly which version of OSX you are running\n. should be fixed now. reopen if it is not\n. the system is live :D\n. I would suggest we focus on the typemaps and one minimal example first.\nSo better remove all the *.pl files and various other rather unrelated changes. Then we get the typemaps going and examples one by one.\nIt should be easy (for me) to add the perl flags detection to configure if you send me one such working example.\n. nice work. please fix the minor things then it is good to be merged.\nNote that once we have this going should implement the 'new style' integration test support but this is good enough for now.\n. - you have to download data separately now\n- the ldconfig warning can be safely ignored\n- we have to check where newer version of R deposit their .so file indeed\n- do these examples fail even if you download the data?\n. You are right. I totally forgot about the download links on the website o_O. However there is a README.data in the repository and I added another line in the README now and quietened the ldconfig error.\nI am going to fix the website too.\n. Alright the website now also shows data download links e.g.: http://shogun-toolbox.org/page/news/onenew\nSo it is all fixed now.\n. I have a couple of formatting concerns, like \nif (foo)\n(note the space between if and ( ) and then next statement on new line, { } brackets like\nif ()\n{\n}\nSGStringList<char>\nsome tab / whitespace stuff\nbut otherwise hurray! what is still missing?\n. Cool that you finally made it! Thanks!\n. Hmmhh we should rather have HAVE_ATLAS defined under OSX or? I mean it seems to be full ATL or?\n. please change to work with latest master.\n. directors are only known to work with python_modular. that said we should probably test for that when compiling\n. could you please post some minimal example for us to try out?\n. I can at least reproduce the problem now. I will investigate and add this as a test.\n. I can finally reproduce the issue and have put a unit test for that. De-serialization actually works except for one thing: the inplace constructor is not called on the SGSparseVectors and so the thing crashes when the object is destroyed.\n. fixed in commit 2b60dbe596d28aaabed9122799b1aede31c484c5\n. Hmmhh. I cannot make any sense of that. Could you please update again and do a make distclean before attempting again?\n. Hi Jonas,\nplease do the minor changes and send another PR - btw who is root (your first commit...)?\n. closing until further info is provided.\n. This is not a bug in shogun. Just add --ldflags=-lpthread to your configure call and be happy.\n. I talked to alex last summer about this. Yes this will be in shogun and a GSoc Project lead by Quoc Le.\n. fixed in bdac6bc7101775743c621f4833a48899ecb2269a\n. lets please do that after the release\n. van51 implemented this now as dotfeatures\n. We will add general .csv readers anyways. So don't waste your time on that.\n. the other alternative would be writing typemaps to support a syntax like this: basically dictionaries of dictionaries, where keys can be strings or SGObjects.\n``` python\n{\n'C' : [2,4],\n 'kernel' : {\n               PowerKernel() : {\n                                  'degree' : [2,4],\n                                  'distance' : { MinkowskiMetric() : { 'k' : [1,2] } }\n                                },\n               GaussianKernel() : { 'width' : [2,4] },\n               DistantSegmentsKernel() : {\n                                            'delta' : [2,4],\n                                            'theta' : [2,4]\n                                         }\n            }\n} \n``\n. Heiko did add support for nu svm. So we just needed to re-generate the data.\n. I think a UUID is not necessary then just suffix/prefix stuff with SG to avoid conflicts. But I certainly don't mind having this standardized (but then update the README.developer too)\n. wouldn't it be even better to remove the branches filter altogether to get travis build everything?\n. it is not 100% clear indeed but we need to build on version branches too so no limit is what we need.\n. hmmhh where is that file generated?\n. what was it?\n. to register the enum useSG_ADD((machine_int_t) &your_enum, ...). to register the enum useSG_ADD((machine_int_t) &your_enum, ...)`\n. in shogun we usually only pass around pointers/references. SGVector/Matrix etc really only copy size & pointer when being passed around. So you need to inplace modify the vector assign a new vector to labels.\n. in shogun we usually only pass around pointers/references. SGVector/Matrix etc really only copy size & pointer when being passed around. So you need to inplace modify the vector assign a new vector to labels.\n. @hushell before merge though you should turn this into a single commit. You can do this with\ngit reset --soft ID\nand git commit / git push --force to this here again.\n. Heiko is pretty strict and you've done quite an intrusive patch. On the bright side. It usually is just the first patch that takes a while to get in.\n. now please make this just one commit instead of 2 and it can go in!\n. OK I guess you don't know what we do with data. It is actually a separate git repository (a git submodule).\nYou can fetch it by doing\ngit submodule update --init\nfrom the git root. Then go to the data/ submodule add a git remote pointing to the data repository and checkout the master branch. Then you should run generator.py  and git add the files that are generated in the data repository. Then git commit them and send a pull request for data.\nThen do git add data from the repository root and commit it and send a pull request against develop.\n. works - now for sth real :)\n. works - now for sth real :)\n. these should not have been there in the first place so OK with me :)\n. these should not have been there in the first place so OK with me :)\n. Thanks for the temporary workaround.\n. Thanks for the temporary workaround.\n. @karlnapf  too late. But it certainly is better than what we have (crashing)!\n. we really need a python3 buildbot to keep things from drifting\n. works! now for sth real!\n. I cannot parse this. Maybe you forgot to --rebase?\n. please rebase and send again!\n. could you please send a single commit not 4? Also the changelog update seems unintentional or?\nYou can use git reset --soft <version> to undo all commits and then git commit -a to commit them all in one commit.\n. I overlooked this but next time please send the PR against the develop branch. Thanks!\n. Oh and thanks for fixing this!!\n. excellent work!\n. I guess you have seen the buildbot failures. The buildbot is building each and every commit. Therefore next time please avoid broken commits / send a PR with a single single commit. You can do so by using\ngit reset --soft <version>\nand then git commit -a. Again thanks for your work!\n. It is totally sufficient that you write the unit test - see that it fails and fix it and send a single PR!\n. indeed. how is that possible btw?\n. I am merging this as this fixes a critical issue. But please could you create another patch that splits up the train function into reasonable sub-functions? It currently is just one big thing and it could be nicely split up - thanks!\n. actually you have to rebase against develop - conflicts...\n. huh?! seems like github is semi-broken - it is already merged...\n. sth is wrong here. please don't remove the data directory but just your commit. also please provide only a single git commit. you can do so by\ngit reset --soft <version>\nand then git commit -a again\n. then the only other thing missing is in the destructor SG_UNREF(preprocessed) / preproc\n. yes please make the corrections and please create just a single commit and don't forget to git rebase against develop not merge!\n. SG_ADD((CSGObject**) &preproc, \"preproc\", \"Array of preprocessors.\", MS_NOT_AVAILABLE);\netc\n. this still removes the data files!\n. I've merged the /= version of your commit thanks!\n. I am totally fine with the name. Until we figured sth better we can aswell merge it.\n. looks good to me now - thanks!\n. OK so the way it works is you do:\n`git reset --soft <id>\nfollowed by\ngit commit -a\nwith a proper commit msg.\n. ok then please turn this into a single commit and update your PR\n. @pickle27 please rebase against develop - we have conflicts and cannot merge until you have done this.\n. That is OK (we will fix it...)\n. your w matches mine perfectly - good job!\n. ok then please do the changes and give us just a single commit for this PR. you can do so by doing\ngit reset --soft ID\ngit commit -a\n. sry didn't see this.\n. Thanks!\n. I used this like 10 years ago to alloc memory / know the size of the feature matrix etc. I guess we don't need this anymore.\nOn March 27, 2015 5:24:18 PM EET, Heiko Strathmann notifications@github.com wrote:\n\nany ideas what the motivation for this was?\n\nReply to this email directly or view it on GitHub:\nhttps://github.com/shogun-toolbox/shogun/issues/1102#issuecomment-86973603\n\n\nSent from Kaiten Mail. Please excuse my brevity.\n. I used this like 10 years ago to alloc memory / know the size of the feature matrix etc. I guess we don't need this anymore.\nOn March 27, 2015 5:24:18 PM EET, Heiko Strathmann notifications@github.com wrote:\n\nany ideas what the motivation for this was?\n\nReply to this email directly or view it on GitHub:\nhttps://github.com/shogun-toolbox/shogun/issues/1102#issuecomment-86973603\n\n\nSent from Kaiten Mail. Please excuse my brevity.\n. Thanks a lot!\n. looks like you used some obsolete version of shogun - this is no longer relevant\n. waiting for a much improved version :)\n. Hmmhh, my excitement about this is rather limited :-1: I think the get_features-and compute is not really worth the hassle. However, converting our type* vec, int len stuff into sgvectors really is worth it so that part I would like to see more.\n. locally merged\n. heh - standard :hamster: \n. @hushell  these build logs appear when you build a modular e.g. python_modular interface. but you can see them also on http://shogun-toolbox.org/buildbot/waterfall (click warning).\nBTW the doc here is not really helpful... what is a what is b?\n. @vigsterkr  maybe you add a warning in configure pointing to this link if we don't find the sources but only the binaries?\n. There is one massive downside to this. One then cannot compile shogun w/o eigen3. So far it was possible to use shogun with zero dependencies. Then only the dependent parts became unavailable. E.g. dim red goes away if eigen3 is not around. MKL didn't work with GLPK. But e.g. string kernels & svms are totally unaffected.\nI am fine with removing math ops but we need to have some class say EigenFactory or so that can create eigen3 objects from SGVectors etc instead of making SGVector etc dependent on eigen3. \nI forgot to say. Back then researchers complained shogun is too hard to get to work because you have to install dependencies and compile and install it for real. So not depending on anything is strongly motivated.\n. and finally please make a single commit out of this by doing:\ngit reset --soft <version>\ngit commit -a\n. just use SG_PROGRESS for this\n. yes it does all this. but the problem is that e.g. an svm will have progress enabled and so will overwrite (on screen) the progress output. even worse it will do this for training and testing ...\n. the only hack I can think of is to disable progress output in the machine we are calling and later restore it...\n. We kind of have that already. Basically each SGObject has an io class. So one can set independent io objects to each SGObject and this way have logging for only the desired object. Note however that SGObjects will be default get some global io object assigned to be able to log anything if the user doesn't overload this.\n. it is not good to drop printing the line number and have no option for that at all then.\nI think the best solution would be to introduce an enum for what is printed (NOTHING, LINE_AND_FILE, PRETTY_FUNCTION) and then print based on that. Also note that PRETTY_FUNCTION is not in the standard so you might want to fall back to func if it does not exist.\n. it is not good to drop printing the line number and have no option for that at all then.\nI think the best solution would be to introduce an enum for what is printed (NOTHING, LINE_AND_FILE, PRETTY_FUNCTION) and then print based on that. Also note that PRETTY_FUNCTION is not in the standard so you might want to fall back to func if it does not exist.\n. fixed in e2142df4a17e39d4ec94023be6198c59d370a963\n. fixed in e2142df4a17e39d4ec94023be6198c59d370a963\n. Regarding the tokenizer. Please just have one DelimiterTokenizer class (instead of the WhiteSpace* etc) for which one can change the tokenization as follows:\nthe DelimiterTokenizer class has a byte[256] tokens; array that can be filled as wanted. then the tokenization check would work as follows: tokens[string[i]] == 1 -> tokenize  if it is 0 don't. \nYou can then also have functions like init_for_whitespace_tokenize() etc\nthat said it is a\n. The tokeinizer is pretty cool now and the converter is very close to be merged. Finish it and lets merge and move on!\n. do these changes and then it is ready to merge! good work!\n. I am confused. Do you want feedback / this merged or what now?\n. Merging for now but please explain why the copy constructor is not sufficient and  you do need a virtual get_copy()?\n. Merging for now but please explain why the copy constructor is not sufficient and  you do need a virtual get_copy()?\n. then just merge it yourself\n. then just merge it yourself\n. is this still happening with current git?\n. is this still happening with current git?\n. just one Q: I guess we should do this then for all interfaces? I mean if we want to be able to run these examples from the source dir it should be consistent.\n. Ok then. @tklein23 it would be cool to have some option to compile/check installed examples though.\n. thanks!\n. @karlnapf you know my opinion about this. Nevertheless, please make a table of pros/cons in the wiki with the facts.\n. Why do they have to be unique? They only need to be unique within each class\n. Yes the sqrt is needed to actually get the norm to 1.\n. yeah that is expected behaviour\n. we intend to use smart pointers after gsoc. overloading & might introduce leaks if someone passes the object to some function that just does sth on this object but doesn't unref. so yes the general solution is what we want to do!\n. This patch is too big. >1kLoC no sane way to properly review it :/\n. It might be you didn't install the proper ubuntu packages or that the ubuntu packages are broken... We have yet to see these issues and a couple of us are using ubuntu.\n. I guess they forgot in ubuntu to link it against fortran then...\n. seems like none of us has mosek here what's the error?\n. maybe this workaround is no longer needed - IDK. it certainly fails with -std=c++11 too (so I disabled it for c++11)\n. let me drop the workarounds altogether\n. Nice that it works now :+1: \nWould be good if we could make the quadratic features a general possibility w/ dotfeatures. This might also make the code more digestible.\n. pure magic!\n. yeah local would be good enough!\n. Nice! Any news on the liblinear experiment?\n. congrats to PR 1337 !\n. Well octave3.2 is way too old and certainly won't work with clang!\n. please don't use several commits 'doc update' but merge them into reasonable chunks\n. is it possible to have a single function that handles all classes for that?\n. @vigsterkr well make it OPTIONAL - if jinja2 is not available and cannot be bundled you don't get the obtain_from functions.\n. I think we can do it the same way we did with SG_REF\n. please provide some (unit) test code to reproduce this. it could be parallelization (try with nthreads=1) it could be only for symmetric matrices or non symmetric ones etc\n. The other option (which I would prefer) would be to actually pass a subset to a feature object when accessing it. The way with the key is troublesome if subsets are added/deleted.\n. Yes I know but I guess we only need that for custom algorithms?\n. we should discuss this on IRC\n. please send one for the converters too!\n. looks like eigen3 is not found I guess you need to add some include dirs\n. ok so cmdline_static works now. please now enable the tests for the other *_static\n. fixed\n. @vigsterkr it is fatbot http://futurama.wikia.com/wiki/Fatbot\nMaybe we should rethink our buildbot strategy and just build debian all the time and other archs nightly. Then we can have the notebook in some debian sid chroot and also in the same chroot the demo server!\n. @karlnapf how do you transfer data to the notebook and back?\n. Thinking about it - there is one big catch here. People will have $SHELL on our server if we allow this. And with this the will likely become root. I am pretty concerned security wise :-1: \n. @karlnapf - it remains a security nightmare. A machine takeover is what this really is. But sure if we create a virtual machine and destroy it after a few minutes then it is the best we can do.\n. It seems like it has problems to choose the right constructor\ne.g. \nx=RealFeatures(some_real_features)\ndoesn't work\nbut\nx=RealFeatures()\nx$set_features(some_real_features)\ndoes.\nI am surprised how much works (about 50% of the examples already!) If it really is just that then I will try to get the examples going.\n. It is not a good idea to build / download dependencies when we do make install (which is done as root!). But sure we could manually install the deps on the buildbots though that won't fix the 'unit tests buld'\n. ok closing for now.\n. Errm I am utterly confused now. Why not have RandomKitchenSinkDotFeatures? And then all algorithms could use this and RFDotFeatures would just overload some function in there and other classes of randomfeatures would be in there too?!\n. What exactly is slower?\n. Then I must be missing something. I don't see a reason why it should be slower.\n. @van51 could you please do a new PR and close this one or at least notify me somehow when you change anything?\n. not possible due to license conflicts.\n. not possible due to license conflicts.\n. For that to do equals should be thoroughly checked in unit tests. Then we could easily switch.\n. For that to do equals should be thoroughly checked in unit tests. Then we could easily switch.\n. @lisitsyn can you fix it?\n. or can we detect this version of ccache and then just disable it?\n. I can tell that we got a lot of feedback due to this and it is sth that can be easily extended.\n. nice job!\n. Could you please use relative path names to the data? Just assume that there is a data dir inside the notebooks file which points to the original data git.\n. @hushell don't worry too much about the deadline now. We would love to have you stay around post GSoC and contribute more code so you will have plenty time. Of course everyone has his live so it surely cannot be full time like GSoC was!\n. Looks good to me. Just this minor Q above corrected and I will merge!\n. woa!\n. @lisitsyn doxygen does that already though. Just enter anything in the search box http://www.shogun-toolbox.org/doc/en/latest/\n. Cool thanks. Lets wait for travis and then I will merge.\n. I think they just take too long...  - this cell ran for ~50mins cpu time before the timeout killed it\n. maybe you just take a subset or converge early or can you speed it up further ?\n. it is too slow anyways - slower than FGM (eating > 90mins of CPU before being killed)\n. Look closely:\nhttp://buildbot.shogun-toolbox.org/builders/nightly_default/builds/554/steps/notebooks/logs/stdio\n123mins CPU time!\n. @iglesias it took 2m16s now so it is OK.\n. so this was just a memory leak no other bug?\n. @pickle27 I wish you have more such nice rides! Maybe we can make this the new release motto: Powered by shinkansen and deutsche bahn (because I do most of my shogun work in the train too!).\n. Fixed in  PR #1755\n. Fixed in  PR #1755\n. Fixed in  PR #1755\n. Fixed in  PR #1755\n. I am installing colpack and arpack on fatbot to fix this.\n. ok done & it is detected lets see what the next build says\n. works now!\n. @hushell why don't you compute the objective for all approaches the same way then? I mean what hinders you to compute the objective of SGD the same way like it is done for BMRM? Or compute both so one can see both / compare both?\n. @hushell and DON\"T delete the objective comparison. It is the only useful way of comparing methods that try to solve the same problem and I liked it very much.\n. I would very much prefer not to just load trained models but rather to solve a slightly simpler task.\n. I would very much prefer not to just load trained models but rather to solve a slightly simpler task.\n. @iglesias well I can create tons of demos that pretend to be trained with shogun. \n. @iglesias well I can create tons of demos that pretend to be trained with shogun. \n. regarding outputs - no that is what debug is for. if you only want to have debug infos for one object well assign it a specific SGIO object that has debug enabled and voila\n. regarding outputs - no that is what debug is for. if you only want to have debug infos for one object well assign it a specific SGIO object that has debug enabled and voila\n. we should maybe have a relaxed validation in addition to the strict one either using a new function that then checks if the values are in +1/-1 or a bool flag in valid that checks then if all values really appear\n. we should maybe have a relaxed validation in addition to the strict one either using a new function that then checks if the values are in +1/-1 or a bool flag in valid that checks then if all values really appear\n. If you give a classifier just '1's you most likely made a mistake and strict validation could save you lots of time hunting bugs.\n. If you give a classifier just '1's you most likely made a mistake and strict validation could save you lots of time hunting bugs.\n. I am fine with changing this into warning - for this we should modify valid to test if things are +1/-1 and error of not and  if only one appears to warn\n. I am fine with changing this into warning - for this we should modify valid to test if things are +1/-1 and error of not and  if only one appears to warn\n. @lambday errm I would want to merge this now and see how long it compiles?! you can fix the issues later then\n. @lambday errm I would want to merge this now and see how long it compiles?! you can fix the issues later then\n. Hi Shinichi,\nthis is unfortunately some numerical issue that cannot be resolved in\nany other way than you normalizing the data properly. E.g. you could use\nthe SqrtDiag kernel normalizer ...\nSoeren\nOn Wed, 2013-10-23 at 05:56 -0700, Shinichi Goto wrote:\n\nBtw I forgot to report this.\nWhen I tried other types of features (with the same code), the error\nbelow has happened.\nMKL-direct: p = 1.500\nMKL-direct: nofKernelsGood = 0\nMKL-direct: Z = inf\nMKL-direct: eps = 1.000000e-02\nMKL-direct: t[  0] = -nan  ( diff = -nan = 1.129816e-02 - -nan )\nMKL-direct: t[  1] = -nan  ( diff = -nan = 1.129816e-02 - -nan )\nMKL-direct: t[  2] = -nan  ( diff = -nan = 1.129816e-02 - -nan )\nMKL-direct: t[  3] = -nan  ( diff = -nan = 1.129816e-02 - -nan )\nMKL-direct: t[  4] = -nan  ( diff = -nan = 9.967950e-01 - -nan )\nMKL-direct: preR = -nan\nMKL-direct: preR/p = -nan\nMKL-direct: sqrt(preR/p) = -nan\nMKL-direct: R = -nan\n[ERROR] In file /home/shinichi/shogun/src/shogun/classifier/mkl/MKL.cpp line 757: Assertion R >= 0 failed!\nterminate called after throwing an instance of 'shogun::ShogunException'\n* Aborted at 1382523623 (unix time) try \"date -d @1382523623\" if you are using GNU date *\nPC: @     0x7effd4cc4425 (unknown)\nAre they related? Or is this just because features that I'm using are\nnot good?\n\u2014\nReply to this email directly or view it on GitHub.\n. @koenvandesande there are a couple of build errors with this patch - see e.g. https://travis-ci.org/shogun-toolbox/shogun/jobs/13004454\n. lets do that properly after 3.0 is out, i.e. fix things that break and have it nice\n. lets do that properly after 3.0 is out, i.e. fix things that break and have it nice\n. please also add some minimal unit test checking that the cluster centers are actually used. to do so have a look at tests/unit/\n. please also add some minimal unit test checking that the cluster centers are actually used. to do so have a look at tests/unit/\n. Yes makes a lot of sense!\n. Yes makes a lot of sense!\n. @hushell very weird and doesn't really make sense to me - we have toe debug why this is a problem at some point\n. @hushell very weird and doesn't really make sense to me - we have toe debug why this is a problem at some point\n. That is actually not a quick fix but the correct fix!\n. Ok makes sense. Only issue I see is that I would love to unify SGReferencedData and ReferencedObject.\n\ntklein23 notifications@github.com wrote:\n\n@sonney2k - I started refactoring the structured label classes, but as\nalready supposed, I ran into problems with label ref counting:\nThe StructuredLabels/StructuredData is widely used in the SO framework\nand lots of code depends on Labels refcounting themselves.\nTo name one example, where it's not possible: In\nsrc/shogun/structure/MAPInference.h we're returning structdata\nreferences using get_structured_outputs(), which will be freed in the\ndestructor.  But if we're passing these structdata to\nstructlabels->add_label(structdata), then structlabels will try to\nfree the labels as well.\nSo far it's not possible to take care of all labels properly without\neither introducing mem-leaks or double-frees.  Could you provide a thin\nbase class, which does only the ref counting?\nI really think adding refcounting to StructuredData is the best way to\nkeep the impact of this refactoring as small as possible.  Refcounting\nwasn't the problem anyway, so it's only adding minimal overhead\ncompared to what we got so far.\n\nReply to this email directly or view it on GitHub:\nhttps://github.com/shogun-toolbox/shogun/issues/1764#issuecomment-29147081\n\n\nSent from Kaiten Mail. Please excuse my brevity.\n. we don't have a ifinite function - is this a typo? did you mean isfinite?\n. the rest looks good. we should decide about serialization of outputs and if this is not required it can be merged.\n.  just use mex\nViktor Gal notifications@github.com wrote:\n\n@karlnapf why me? i know square about matlab static interface :(\ni've asked several times how to generate .mex file but i never got an\nanswer...\n\nReply to this email directly or view it on GitHub:\nhttps://github.com/shogun-toolbox/shogun/issues/1777#issuecomment-35795551\n\n\nSent from Kaiten Mail. Please excuse my brevity.\n.  just use mex\nViktor Gal notifications@github.com wrote:\n\n@karlnapf why me? i know square about matlab static interface :(\ni've asked several times how to generate .mex file but i never got an\nanswer...\n\nReply to this email directly or view it on GitHub:\nhttps://github.com/shogun-toolbox/shogun/issues/1777#issuecomment-35795551\n\n\nSent from Kaiten Mail. Please excuse my brevity.\n. @vigsterkr So now with the instructions from yage99 - could you please fix the cmake script?\nmex MatlabInterface.cpp /path/to/libshogun.so  in the src/interfaces/matlab_static\n. @vigsterkr So now with the instructions from yage99 - could you please fix the cmake script?\nmex MatlabInterface.cpp /path/to/libshogun.so  in the src/interfaces/matlab_static\n. @karlnapf nothing for a README - we should just fix the cmake script  - the obsolete libstdc++ that mathworks is shipping issue is already in our FAQ http://www.shogun-toolbox.org/page/documentation/faq\n. @karlnapf nothing for a README - we should just fix the cmake script  - the obsolete libstdc++ that mathworks is shipping issue is already in our FAQ http://www.shogun-toolbox.org/page/documentation/faq\n. This version is severely outdated. We had 2.0 packages which due to bugs in dependencies never made it. Anyway, we are creating 3.0 packages but it will take time!\n. this is the behaviour http://buildbot.shogun-toolbox.org/builders/nightly_default/builds/650/steps/notebooks/logs/stdio\n. this is the behaviour http://buildbot.shogun-toolbox.org/builders/nightly_default/builds/650/steps/notebooks/logs/stdio\n. closing for now\n. closing for now\n. thanks for the patch. lets wait for travis and if green merge!\n. thanks for the patch. lets wait for travis and if green merge!\n. That is a swig / octave bug though http://octave.1599824.n4.nabble.com/Fwd-swig-bugs-1353-Octave-3-8-0-support-td4660724.html\n. That is a swig / octave bug though http://octave.1599824.n4.nabble.com/Fwd-swig-bugs-1353-Octave-3-8-0-support-td4660724.html\n. Anyway it built with the old stuff - merging.\n. Anyway it built with the old stuff - merging.\n. @tklein23 could you please inform vojtech and Michal Uricar uricar.michal@gmail.com about this?\n. @tklein23 could you please inform vojtech and Michal Uricar uricar.michal@gmail.com about this?\n. the correct fix would then to return sth and run generator.py and commit the new data.\n. the correct fix would then to return sth and run generator.py and commit the new data.\n. argh but well done @iglesias! Now that we are back to green lets have a no-merge policy once any of the stuff is red.\n. @karlnapf  so what is the state on this - can this be merged? IMHO this const* stuff shouldn't be blocking it.\n. Any news on this one?\n. Ohh man bummer! Very important issue to fix!\n. @vigsterkr well or at least some option to disable hdf5 \n. @mazumdarparijat @karlnapf The question is - should we not rather change SGReferencedData to be a templated class that has all the content in some globally shared block (not just the refcount)? This would mean though that we can no longer do mat.matrix but would have to do mat.get_matrix().\nThe other alternative is to switch over to using references in all places where we intend to modify the matrix. However we then cannot resize the memory. Which is IMHO not an issue. We could just leave the memory pointer as is and change the matrix size. It has been like this before anyways.\n. That is a swig name mangling option. \nIt will make examples more difficult to maintain though...\nOn March 28, 2015 11:19:14 PM EET, Sergey Lisitsyn notifications@github.com wrote:\n\nI suggest to adjust naming of methods in target languages so that\n'get_num_features' in Python would be 'getNumFeatures()' in Java. I\nconsider this quite important as it would look very ugly in, say, Java.\n@karlnapf @vigsterkr @lambday \n\nReply to this email directly or view it on GitHub:\nhttps://github.com/shogun-toolbox/shogun/issues/2789\n\n\nSent from Kaiten Mail. Please excuse my brevity.\n. DYLD_LIBRARY_PATH is your friend\nOn Fri, 2015-08-21 at 06:38 -0700, wcalhoun516 wrote:\n\nI tried:  \"-L/path/do/that/dir and the -lshogun flag should be used\" and it successfully compiles but still get a runtime error of \"Library not loaded: libshogun.17.dylib\"\n\nReply to this email directly or view it on GitHub:\nhttps://github.com/shogun-toolbox/shogun/issues/2894#issuecomment-133427024\n. what do you need stdio.h for? if you need to output sth use the shogun equivalents, SG_PRINT(), SG_WARNING(), SG_ERROR()\n. there is a public: missing here\n. should be delete[]\n. delete[]\n. you could return the length of the subset by reference. for swig based interfaces you should also add a function that has the signature\n\nvoid get_feature_subset(int32_t** subset_idx, int32_t* subset_len);\nand set_feature_subset(int32_t* subset_idx, int32_t subset_len);\n. namewise I prefer idx instead of inds\n. you are right we indeed need a function like this\n. pleae name them m_subset_idx and m_subset_idx_len;\n. better remove_feature_subset() ?\n. I think it is fair to assume that cleanup() removes the subset. So you don't need a cleanup_feature_vector_no_subset(i) / cleanup_feature_vector(i) will do.\n. so you could merge these two or?\n. You could just keep num_vectors here - just introduce a new member variable num_vectors_total (that contains the total number of vectors not the currently to be used number of vectors). It is probably used everywhere and often so should be a very fast operation.\n. that is probably all correct - but I am not sure this will ever be needed (though I could be wrong). I would add an ASSERT(!m_subset_idx) here and assume that we only need read-only functionality.\n. sigh some of the methods will need that - but OK ignore and fix later\n. can you put this index conversion into an inline function please - it occurs in a couple of places...\n. next time please code beautifications first in a separate patch\n. Either always reset when features change, or when num_vectors_total != p_num_vectors.\n. please put a bracket around the for loop  like\n{\n}\n. to be consistent with what I said above for num_vectors / num_vectors_total - I would save num_vectors in num_vectors_total as I would save max_string_length in max_string_length_total and then replace it here.\n. I think it makes sense to have\nvirtual int32_t get_num_vectors_all()\n{\nreturn get_num_vectors();\n}\nhere - don't you agree?\n. please use\nvoid foo()\n{\n}\ncoding style\n. you cannot know this - and I guess we will change this at at some later point. but for now: The get_feature_subset() function above would be callable by swig wrapped interfaces (one line %apply line in modular/Features.i is required for that). But unfortunately these interfaces require an allocated pointer and a copy of the data. Not nice I know but this is the way it is for now. So please introduce two such get_feature_subset functions - one like you have now but using references to set the data sizes that returns the subset index and one that allocates memory for the index (malloc this time!) / so a copy.\nLook at features/SimpleFeatures.h line 361 and below - it will be clear from that.\n. ahh sigh. same stupid issue with swig interfaces. we need to copy the data here. or have two functions copy_feature_subset and set_feature_subset. Don't change it just now - I seriously consider fixing up the swig interfaces.\n. shouldn't this just return idx by default and not be pure virtual?\n. please add one line of documentation for both varialbes.\n. you should always return max_string_length here and only adjust max_string_length to have to correct value.\n. please two lines.\n. again 2 lines\n. you cannot do that. it has to be\n%apply (int32_t* IN_ARRAY1, int32_t DIM1) {(int32_t* subset_idx, int32_t subset_len)};\nso change your function signature above add this line and all good.\n. you don't need an if (m_subset_idx) here - delete[] does that test anyways.\n. why not start with mean = NULL etc and just add ASSERT(mean && cov); when needed.\n. could you add 4 functions - one for setting/getting the mean vector and one for setting/getting the cov matrix?\n. out of memory ;-)\n. Copyright header missing.\n. Copyright header missing.\n. I think these 2 functions can go for now - until they are properly documented and used for other things.\n. for nested for loops please use curly braces\nfor (...)\n{\nfor (...)\n{\n}\n}\n. indention problems ?? lines 388-397 look strangely indended\n. I would prefer your init function assumes that mean and cov are already set globally)\n. please overload get_likelihood_example and then here return just CMath::log(get_likelihood_example)\nalso you have to free the feature vector I think.\n. why do you need set_data? There is already set_features in CDistribution.\n. get_features is also already in CDistribution\n. please add a int32_t* dim - with the length of the mean.\n. also a int32_t dim and ASSERT(dim==m_dim)\n. please also add int32_t* rows and cols.\n. also here and check sizes\n. newline before private please / protected too.\n. please split that up in two functions\nget_mean\nget_cov\n. two functions please instead of one\n. should probably be renamed to get_likelihood_example...\n. please varialbes for rows, cols and mean len (instead of just 1 dim), i.e.\ncov, rows, cols,  mean, len\n. m_cov_rows/cols / m_mean_length / m_inverse_* should also be inited 0\n. please use doxygen style (see e.g kernel/GaussianKernel.h)\n. please call base class - in this case I think CKernelMachine\n. it is called ParzenWindow btw (z instead of s)\n. variable names in shogun are all named like\nmy_nice_variable\nmember variables of classes have a m_ prefix.\n. Couldn't you use CKernelMachine and then just use the Kernel that is assigned?\n. ok sinze ParzenWindow in the end defines a distribution, it makes sense to derive from CDistribution and implement the methods in there. However, you can use a CKernelMachine internally and define some set_kernel / get_kernel functions such that one can set the GaussianKernel (or any other useful one).\n. please don't submit the GMM in one pull request - use a new branch for your ParzenWindow estimator and issue the pull request on that one - thanks!\n. whitespace problem?!\n. please use\nelse if (xxx)\n{\n}\nstyle\n. sorry but what is j1?\n. Yeah but CDistribution would be a good base class and internally you could store everything as a kernelmachine.\n. I don't understand why this is correct. Aren't you traversing over the features with lowest corresponding eigenvalues here?\n. I would suggest a different approach: \nInstead of having everything in the constructor, why not add a function to select one of the ways of reducing dimensionality (eig<threshold, %variance explained, fixed nr). I mean one constructor that has an ENUM  THRESHOLD, VARIANCE_EXPLAINED,  FIXED_NUMBER and then a float64_t argument would do. Underneath you store just this one variable in 'threshold' and depending on the enum you do different things with it. This also would make the code that selects the dims more digestible.\n. please do without this .cpp file (CEvaluation is just an interface after all).\n. I would change the naming into predicted and ground_truth\n. sth. you cannot know: get_int_label will throw an assertion error if the underlying label is not integer and indeed svm predictions are just real valued numbers.\nSo please do a CMath::sign of the prediction. and also please omit the curly braces in the if (there is just a single statement)\n. you don't have to put this there or? I mean it is already there in sgobject?\n. Is this the correct formula?\nhttp://crsouza.blogspot.com/2010/03/kernel-functions-for-machine-learning.html#bessel\nlooks different?!\n. you don't need this check\n. please issue a separate pull request for this (I guess you have to create a branch for that).\n. failing is better than returning default values: just put SG_NOTIMPLEMENTED; here and in related places\n. forgot to register params in this destructor\n. please add an empty line between these loops\n. the curly brackets here are not necessary\n. newline before the comment please (occurs also below...)\n. { on new line\n. { new line\n. only write predicted labels here\n. into contingency file please\n. get_name() should always return the class name without C prefix.\n(this problem happens a couple of times)\n. new line before for please, spec between for and ( please\n. sign is not necessary\n. I think it is better to check that ground_truth is a two_class_labeling via ground_truth->is_two_class_labeling() here\n. coding style wise - please move the curly bracket\noccurs quite often below.\n. indention problem (whitespace)\nalso occurs multiple times below\n. This can go then - we used this only to mark that serialization support is not yet there.\n. it makes more sense to add this function to CMath - but with a more fitting name (not really a matrix mul right?)\n. it would make sense to move the loss into e.g. lib/Loss.h and have a static class there with all the losses and their derivatives. This could be reused by SVMSGD too\n. new line here (after for loop)\n. new line here after if clause ends\n. same here newline missing\n. newline missing\n. these curly braces aren't needed\n. I think this should get a CStreamingFile as argument  nothing else. The stream buffer could then be set in the StreamingFile.\n. label should be optional (think of online clustering algorithms like BIRCH) - so either two functions get_next_supervised_example() / get_next_unsupervised_example(), two functions or a flag to not consider the first element to be a label.\n. the documentation should be in the .h file - doxygen style documented.\n. isn't this memcpy here wasting cpu cycles? shouldn't you directly get  a ptr to the data?\n. I think it is better to have the ML algorithm sitting on top call a function free_feature_vector() when it does no longer need that vector.\n. You return an int here - better return an enum with appropriate names to check for - much easier for the user to grasp.\n. the class and all the functions in this file here need documentation.\n. this should return C_STREAMING_SIMPLE\n. shouldn't you also stop the parser in the destructor?\n. current label should also be a float64_t - think of regression\n. this could work with sockets later... too\n. not sure if this is needed\n. I would drastically simplify the algorithm: on the first stream read I would figure out when '\\n' appears and then assume that all lines have the same length (for performance reasons - if john agrees).\n. On Wed, 2011-04-20 at 07:57 -0700, frx wrote:\n\n\n+}\n+\n+CStreamingFile::CStreamingFile(FILE* f, const char* name) : CFile(f, name)\n+{\n+}\n+\n+CStreamingFile::CStreamingFile(char* fname, char rw, const char* name) : CFile(fname, rw, name)\n+{\n+}\n+\n+CStreamingFile::~CStreamingFile()\n+{\n+}\n+\n+\n+#define GET_VECTOR(fname, conv, sg_type)                       \\\n\nBut for sparse features? If we are planning on having defaults for unspecified features (like VW does), I think it'd be difficult to work making the assumption of fixed line number.\nAnyways, I haven't implemented sparse feature reading yet, so that problem will be taken care of later. For dense features, I think we could use this.\n\nYou are right for the standard ascii representation that e.g. svmlight\nuses that won't work. But since this is all about online learning and\nthus usually speed one could define more appropriate ascii (if it has to\nbe ascii) formats that stores the length of the line in bytes and number\nof elements to come.\nThis of course assumes that examples arrive one by one.\nSoeren\nFor the one fact about the future of which we can be certain is that it\nwill be utterly fantastic. -- Arthur C. Clarke, 1962\n. On Wed, 2011-04-20 at 07:50 -0700, frx wrote:\n\n\n+\n-   ret_value = parser.get_next_example(feature_vector, length, label);\n  +\n-   // If all examples have been fetched, return 0.\n-   if (ret_value == 0)\n-       return 0;\n  +\n-   // Now set current_{feature_vector, label, length} for the object\n-   current_length = length;\n-   current_label = label;\n- \n-   // in case current_feature_vector isn't initialized\n-   if (current_feature_vector == NULL)\n-       current_feature_vector = new float64_t[length];\n- \n-   memcpy(current_feature_vector, feature_vector, length*sizeof(float64_t));\n\nI did this as a convenience measure. Copying the vector from the parser's buffer into the object's data member would enable me to immediately 'finalize' the current example and make that buffer space available for writing. Plus, using f->current_feature_vector would be more reliable since the contents don't change unless get_next_feature_vector is called again.\nIt would be a bit confusing when to finalize the example if the pointer is returned directly. Once the user finishes processing the example, he would have to finalize it explicitly from the code. \nBut I think with some more careful coding, the technique of returning the pointer directly would work. I'll look into this.\n\nI totally understand that this has a convenience / complexity tradeoff.\nBut since it can be avoided without too much hassle and it is all about\nspeed here I would prefer the more efficient solution (one pass through\nan example is the complexity of an usual online learning algorithm).\nSoeren.\nFor the one fact about the future of which we can be certain is that it\nwill be utterly fantastic. -- Arthur C. Clarke, 1962\n. On Wed, 2011-04-20 at 08:20 -0700, frx wrote:\n\n\n+}\n+\n+CStreamingFile::CStreamingFile(FILE* f, const char* name) : CFile(f, name)\n+{\n+}\n+\n+CStreamingFile::CStreamingFile(char* fname, char rw, const char* name) : CFile(fname, rw, name)\n+{\n+}\n+\n+CStreamingFile::~CStreamingFile()\n+{\n+}\n+\n+\n+#define GET_VECTOR(fname, conv, sg_type)                       \\\n\nSo is the online data going to have a different format? I mean is it not mandatory to make it compatible with the current ascii representation that we are using for batch algorithms now?\n\nBeing compatible is nice - but if we can get a significant speedup by\ndefining a new format then changing it is worthwhile. That being said\nfor now it is probably OK to use the slow and more compatible way.\nSoeren\nFor the one fact about the future of which we can be certain is that it\nwill be utterly fantastic. -- Arthur C. Clarke, 1962\n. On Wed, 2011-04-20 at 08:30 -0700, frx wrote:\n\n\n\nvirtual int32_t get_dim_feature_space()\n{\nreturn current_length;\n}\n  +\nvirtual int32_t get_next_feature_vector(float64_t* &feature_vector, int32_t &length, int32_t &label);\n  +\n  +\nprotected:\n  +\ninput_parser parser;\n  +\nFILE* working_file;\n  +\nfloat64_t* current_feature_vector;\nint32_t current_label;\n\n\nIn the longer run, we must encapsulate a Shogun features object here, right? Should I start changing the feature_vector and label to be Shogun objects, i.e., C...Features and CLabels? Though that would require changing much of the get_next_feature_vector code.\n\nNo it is correct to just use these basic types here (for\nSimpleStreamingFeatures) since shogun internally also only uses a\nfloat64_t* and an integer to store its length and a float64_t to store a\nsingle label.\n\nFor the one fact about the future of which we can be certain is that it\nwill be utterly fantastic. -- Arthur C. Clarke, 1962\n. Agreed. One could (greedily) assume that all lines in an ascii file have same length and adjust the read buffer to that size (and test if it ends with '\\n' - but it is probably not worth the effort given that binary data could be made more efficient to access.\n. SPECIFICITY is what you mean I guess :)\n. class have -> class has\nof measures -> of the measures\n. Cross correlation coefficient\n. specificity\n. newline before that\n. Binary file is as hacky as it seems. To implement a first draft of streamingfeatures it should just work for ascii and be general enough to work with anything else later on. But first ascii + parallel :)\n. We wanted to avoid that memcpy right?\n. please empty lines between function definitions\n. copyright / author is not right.\n. why do you use malloc instead of new[] ?\n. these { } are not necessary\n. what happens with the indention? { should be directly beneath the i of the if (same for loops)... looks like you do this everywhere - please fix\n. coding style is to use CInputParser here\n. you should use either E_EXAMPLE_TYPE or EXAMPLE_TYPE (similar above)\n. coding style wise, please put all the public functions at the top, followed by protected and private ones, then public, protected and private member variables.\n. On Mon, 2011-04-25 at 09:41 -0700, frx wrote:\n\nExamples are being stored in the memory as  followed\nby\n.. The example object only contains a pointer to the\nfeature\nvector, while the actual feature vector itself is next to the example\nobject\nin the memory.. Hence, I do a malloc combining the sizes of the\nexample\nobject+feature vector. I can use new[] but then I'll have to do new\nchar[...] because it is not a contiguous stream of objects of the same\ntype.\n\nYou could use a struct and then new - or am I missing something?\nSoeren\nFor the one fact about the future of which we can be certain is that it\nwill be utterly fantastic. -- Arthur C. Clarke, 1962\n. Would it be possible to write e.g.\ndouble[] y = x.get_labels();\ninstead of first allocating y and then issuing get_labels(y)?\n. all this  LabelledExample casting makes the code a bit hard to read. I would prefer to not use void* but two function one working with labelled and one with unlabelled data.\n. plase write new LabelledExample();\n. please add a newline after the if / else clause ends\n. should be for (int i=0; i<... (space between ; and i)\n. please hide that buffer_write_index*number_of_features+i in some inline function - lines are becoming to long and too hard to grasp\n. coding style wise, please use \n&examples_buff[buffer_write_index] and avoid that void* \n. please don't do the change index32_t -> index_t. yes we should do this in some later version but then better globally for all of shogun\n. again no index_t conversion please\n. use CMath::fill_vector() or range_fill_vector for dummy tests :)\n. please use new float64_t[max] / same below\n. please add brackets in outer for loop { }\n. same here\n. ideally we do these tests later via some python_modular example - but good for now. another general comment - please don't use stdlib if you can avoid it (doesn't seem you need it?). Finally please name variables my_nice_variable and if you have constants const float64_t CONST.\n. please rename toUnref / toRef to to_ref  / to_unref.\n. these { brackets can go here... (rule is no brackets for single line statements as we have here).\noccurs a couple of times.\n. why would you need to save the ms parameters again? I mean m_ms_parameters is a subset of m_parameters.\n. same here for loading\n. what is doubleArray and why is it needed? Can't you directly use double[] ?\n. I wish you could omit this and directly use x.set_labels(y) with y being of type double[]\n. submit this as extra patch please - but check that it is properly intented.\n. On Tue, 2011-05-17 at 00:31 -0700, sploving wrote:\n\n\n}\n+\n-       sgDoubleVector label = new sgDoubleVector(array.cast(), 4);\n\nI took a mistake. I think after defining the new type SGVector, we could use it in JAVA too. You mean SGVector is just a C++ class, not a Java class? In java, we still use double and in C++ we use SGVector, Am I right? Another thing, could you add some python example so that I could understand your new type well. Thanks\n\nYes exactly. I used this from python with the new SGVector / matrix.\nfrom shogun.Features import Labels, RealFeatures\nx=Labels()\nfrom numpy import array\ny=array([1.0,2.0,3.0])\nx.set_labels(y)\nprint x.get_labels()\ny=array([[1.0,2.0,3.0],[4.0,5.0,6.0]])\nx=RealFeatures(y)\nx.set_feature_matrix(y)\nprint x.get_feature_matrix()\n. you should assume that jblas is in the class path and not assume a specific jblas version\n. the init_shogun / exit_shogun stuff should only be necessary in java / c# - please remove\n. how do you know the length? Note that string in shogun is just a variable lenght vector, so could be a sequence of doubles too..  So I don't know if this is correct though the autodetection part above is sth I'd like to merge right away.\n. no { } necessary (happens a couple of times also below - no brackets for single line statements please)\n. the outer for loop should have { } - the inner in this case not.\n. { } for outer for loop please\n. we have an SG_NOTIMPLEMENTED; macro for that\n. what is that? PreProc.i does no longer exist!\n. exactly ... it has been renamed to Preprocessor.i\n. hmmhh instead of seeking couldn't you just copy the buffer and then read a little less to the next position?\n. please use add_matrix(kmatrix, \"kmatrix\", ...)\n. please return a bool instead of int\n. what is this? did you add anything to data? does the shogun.git require this new data set to work?\n. please use introduce a new constructor in SGMatrix that takes two arguments rows/cols and then allocates memory via new[num_vectors*dim_features] and use this instead of the manual malloc here. thanks!\n. please use features_subset_simple.cpp as file name (alternatives possible  - just some filename that starts with features_  ) thanks\n. I don't understand - can't you just do real_num=CFeatures::subset_idx_conversion(num); and done?!\n. why not throw an error then when subset is set?\nIn addition, I would remove the Nothing to do with the subset class comment.\n. doesn't \ngit checkout -f data\nwork?\n. The \nfeat=compute_feature_vector(num, len, feat);\nline computes the feature vector that in case of no preproc's is returned in the\nreturn feat ;\nline. If there are preprocessors feat is preprocessed with all of them. So I don't see the problem right now. Maybe we have to discuss on IRC if you still don't agree.\n. please move these two function calls also to CStreamingFeatures\n. these too\n. why do you use n_? not just n?\n. this should still be SG_UNREF - please never use delete on objects starting with a C prefix but SG_UNREF, this way if this object is still used by someone it won't be accidentally deleted.\n. space between for and ( please\n. please use SG_ERROR. in addition for multi-line strings you can use\nSG_ERROR(\"foo\"\n\"bar\"\n\"bla\");\n. please omit void\n. can we avoid these two lines? I mean StreamingSimpleFeatures could do this immediately\n. please use SG_PRINT()\n. please remove void - if a function has 0 arguments then there is no need to say foo(void) - foo() is totally sufficient.\n. OK maybe I don't understand right now - but why does this file exist? I guess we need to chat such that you can explain this to me a bit better.\n. this should take CSimpleFeatures as first arg right?\n. why not CSparseFeatures here?\n. Why not use SGVector as before?\n. why not use SGVector here?\n. why do you now still have these get_*string functions here? shouldn't these do overloading now too?\n. no overloading?\n. yes it is ok - I am not so sure about the templated fallback though - I mean how does the compiler know that it shouldn't use the templated function?\n. I understand that this simplifies the declarations for you but I would really appreciate if these function definitions were spelled out explicitly \n. I would prefer if you drop it - this really cries for trouble later.\n. what I am not sure about here - does overloading work? I mean does this templated get_vector definition override the specific get_vector ones in the base classes?\n. why do you do this? I mean you have get_vector defined with a certain type T so you just need a _single implementation above using  SGVector ...\n. this can wait until you have the experiments running but I would suggest to define an extra class CLoss and then define the loss() and derivative() inside it. This makes sense since SGD QN will require this too.\n. good catch! I think we should add some helper function to be able to set the dimensionality of  sparse streaming features upfront such that this is not becoming a bottleneck.\n. maybe apply_to_current_example() is a better name?\n. we have one problem here: we don't know how big the stream is (num_vectors unknown) but are returning a CLabels of fixed size. That makes sense when get_num_vectors returns() > -1 or we create a CDynamicArray  first that we then copy over to the Label object created at the end (to get output for the whole stream).\n. this function can die - the one below is sufficient\n. please add { } for the for loop.\n. yes exactly like that\n. these visual studio like comments should go\n. use SG_SPRINT please\nactually SG_PRINT here\n. please add some pointer (e.g. wikipedia url for fibonacci heap)\n. please put variables after functions - so in this case at the end\n. please use ::pow\n. why is that copying needed here? Can't you do without for example without delete candidate below?\n. don't you need to SG_REF() the loss?\n. you need to SG_REF() loss before returning\n. SG_REF() ...\n. SG_REF(loss);\n. SG_REF?\n. you need to call the base class constructor, CLossFunction() \nplease do this in other losses...\n. please add a constructor here that calls the base class\nCLossFunction() : SGObject()\n{\n}\n. did you mean SGStringList here?\n. my only concern here is that we might have a clash between free() and free...\n. we are switching to use index_t for that\n. - can this be merged into shogun/io/SGIO.* ?\n- can you avoid ?\n- would be great if this was used by asciifile etc\n. - there are a lot of class definitions in this file - please split it up\n- none of the classes is derived from SGObject (might make sense if overhead is too big though)\n. please call base class constructur, i.e. : CSGObject() (also below)\n. I think this doesn't need a warning. I mean we know that this is an old shogun so all transformations have to be applied.\n. you might want to use int32_t and float32_t here.\n. please add a newline here\n. could you please modify this switch statement to have no default case? this way we get a compiler warning if we don't check for all possible enum values. you can still get the old behavior by using return instead of break and having the SG_SERROR() call at the very bottom\n. On Wed, 2011-08-10 at 13:08 -0700, frx wrote:\n\n\nswitch (type)\n{\ncase C_NATIVE:\n    cache_writer = new CVwNativeCacheWriter(file_name, env);\n-       break;\n\nreturn;\n  case C_PROTOBUF:\n      SG_ERROR(\"Protocol buffers cache support is not implemented yet.\\n\");\n\n\n\nplease add it with\nifdef HAVE_PROTOBUF\nendif\n\nI haven't included the Protocol Buffers cache yet because that would add an unnecessary dependency for now.. Maybe when the rest is done we could add that.\n\nwe can easily do some configure test for that feature\n. it is probably no longer double or?\n. Hmmhh if you would follow the exact streamingdotfeatures signature other learners could use vwfeatures too. so would be better to not pas VwExample here and keep the length of the dense vec2\n. we agreed on a float32_t transition for streaming features\n. while you are at it could you please replace the int's here with int32_t everywhere?\n. please don't do that .\n. is that a whitespace here?\n. you don't need that if here\n. what is this?? don't commit data if you don't add anything there and push it.\n. typo - should be ascii\n. On Thu, 2011-12-22 at 14:02 -0800, Heiko Strathmann wrote:\n\n\n@@ -1 +1 @@\n-Subproject commit 9351068b2ec25d6086b9b5632caed7831d3abb17\n\nI dont know what this is. I never commited this.\nWe had this, I remember.\nI cannot revert the commit. \nany ideas?\n\nReply to this email directly or view it on GitHub:\nhttps://github.com/shogun-toolbox/shogun/pull/343/files#r312438\n\nyou get this when you do git commit -a  and data is modified or you did\nnot git submodule update\njust don't commit this next time please\n\nFor the one fact about the future of which we can be certain is that it\nwill be utterly fantastic. -- Arthur C. Clarke, 1962\n. what's the 200 here?\n. typo - scratch\n. so you need this strdup stuff in the migration to delete old params right? \n. No all good. I just wanted to make sure I understand what this is for.\n. Please don't use SGVector for types other than numerical ones, i.e. SGVector etc\n. Please use SGVector result(num_labels, true); instead (the do_free flag has to be set to let others know that this is a copy).\n. i ?\n. thanks for this fix and the one below!\n. On Wed, 2012-02-01 at 10:53 -0800, Heiko Strathmann wrote:\n\n\n@@ -97,6 +97,10 @@ void CKernel::resize_kernel_cache(KERNELCACHE_IDX size, bool regression_hack)\nbool CKernel::init(CFeatures* l, CFeatures* r)\n {\n-   /* make sure that features are not deleted i reset */\n\ntypo, but the SGREF is ok right? :)\n\ncannot hurt to do them!\n. could you please use CMath::log2 instead of the one from the math lib (which is not always available)?\n. where is v cleaned up / destroyed?\nI think you need to call v.destroy_vector() at the end\n. some whitespace problem here?\n. Could you make the mu == const part optional such that the second line in\nhttp://en.wikipedia.org/wiki/Mahalanobis_distance\ncan be used ( this (x-y) S^-1 (x-y) one? Then distance(i,j) can become meaningful again...\ne.g. via an option set_use_fixed_mu() or so.\n. S is the same thing you have, the covariance matrix computed over both lhs and rhs if lhs!=rhs (like it should be now).\n. well S (and mu) should be computed from as much available data as is there, so in case you have both lhs and rhs use both and lhs!=rhs use them otherwise just lhs :-)\n. please just use CMath::random(0.0, 0.5) ; for that\n. it seems you disabled the memorymapped file for win32 - so why do you include sys/mman.h ?\n. what is this needed for?\n. I don't see how this can work - the Signal.cpp file still has an ifndef WIN32 in there so none of the functions is actually implemented but they are now still called from everywhere...\n. please always use memset here - bzero is deprecated anyways as of 2008\n. bad whitespace change here\n. please always use this line (CMath::lgammal here)\ngamma is deprecated too :)\n. I think you need an ifdef WIN32 in front of this and I would prefer not to have the whole cblas file in here but just what is needed from shogun.\nwhat worries me again is that you might be able to compile libshogun but you won't be able to link against it because all these cblas_* functions are nowhere implemented?\n. this comment can be removed...\n. this define has different indention than the one below\n. so import_array() is always done?\n. get_kernel_matrix will get you a matrix - which you have to free via matrix.free_matrix() later on otherwise it will leak memory.\n. yeah you should really - precompute as much as is possible. then there is also no need to use the invsqrt approximation. so just store extra variables (or others processed).\n. no that is totally fine. I would expect examples to have more dims than there are classes ...\n. we usually add a private: statement again for member variables - btw - why do the others below have to be protected? isn't private sufficient?\n. another comment that should go...\n. another comment that should go...\n. there is more unused comments here\n. and here\n. please also add yourself as sole copyright holder - after all you wrote the code :)\n. please document these paramters - what is l? what itr?\nis l perhaps 1/C ? if so I would prefer to keep C around and set some reasonable default for itr and some function to get/set itr?\n. please ifdef your code with\nifdef HAVE_LAPACK\n. remove obsolete comment please\n. typo! DiagonalMatrix\n. type BIAS\n. add CT_NEWTONSVM to the EClassifierType in shogun/machine/Machine.h \n. please move this function to mathematics/Math.h\n. pleae use { } around the first for too and please indent with 1 tab per level\n. please use\ntype* w\nso float64_t* w\netc\n. newline here please\n. separate lines and doxygen comments for these variables\n. that matrix might be HUGE! is there a way to avoid working with the whole matrix, i.e. couldn't you just use vector by vector here?\nyou really cannot use that function, use get_computed_dot_feature_vector / or even the more efficient add_to_dense_vec / dense_dot / operations from CDotFeatures. \n. use SG_CALLOC (then you don't need the memset)\n. indent the break ...\n. why the extra newlines here?\n. SG_CALLOC here\n. could you please reduce the number of parameters of that function? for example w is a member variable anyways so w and dimensionality are known inside it. lambda = 1.0 / C is known too etc...\n. well you cannot use cblas_dgemm then but have to manually consider vector / by vector\n. some is wrong with indention here. and please use C as external parameter and internally use lambda = 1.0/C\n. please use 430 then\n. please don't use camelcase - but create_diagonal_matrix as name\n. indention in this function is flawed, it should be\nfor (int32_t i=0; i<size; i++)\n{\n}\nand\nif (i==j)\ntab ...\nelse\ntab ...\n. please drop that comment\n. I think all you have to do here is use the Linear Kernel with ByteFeatures, so name should be Linear\n. same here - just use Linear\n. same here - just use Linear\n. same here - just use Poly\n. same here - just use Gaussian\n. you might want to speed this up by getting the whole int label vector in best_map and only handing it into this function. this might help as get_in_label will do array bounds checking, conversion double -> int and testing if label is really an integer\n. couldn't you equivalently use if (l1->get_int_label(i) != m1 || l2->get_int_label(i) != m2) and rename find_match_count -> find_mismatch_count to really create positive costs?\n. permute sounds like the order of labels is permuted....\n. please use int32_t here\n. int32_t\n. you could also use SG_ADD here right?\n. SG_ADD for inner / outer degree missing both with MS_AVAILABLE\n. please use SG_ADD here too\n. and here too ...\n. and here too\n. wrong indention\n. wrong indention\n. please just one whitespace line\n. empty line before protected please\n. formatting is still strange should be\n{\n_tab_for (int32_t i=0; i<size; i++)\n...\nand any indention level is one tab not a space or so\n. whitespace missing: trainlab, int32_t itr)\n. please put lambda on new line\n. please let #ifdef's start at the beginning of the line\n. newline before for please and use for (int32_t i=0; i<x_n; i++) please\nbtw you could even better use CMath::fill_vector(out, x_n, 1.0) instead\n. #ifdef at beginning of line please\n. endif too\n. indention?\n. please don't  add \nifdef DEBUG_NEWTONSVM around\nSG_SPRINTS \nin the final version we don't want debug outputs.\n. fix indention and remove obsolete commented code please\n. w\nOn Tue, 2012-04-03 at 01:24 -0700, Harshit Syal wrote:\n\n\n\nfloat64_t _Xsv2=SG_MALLOC(float64_t,x_d_x_d);\ncblas_dgemm(CblasColMajor,CblasTrans,CblasNoTrans,x_d,x_d,size_sv,1.0,Xsv,size_sv,Xsv,size_sv,0.0,Xsv2,x_d);  \nfloat64_t *sum=SG_CALLOC(float64_t,x_d);\n\nfor(int32_t j=0;j<x_d;j++)\n{  \nfor(int32_t i=0;i<size_sv;i++)\nsum[j]+=Xsv[i+j*size_sv];\n}\nfloat64_t Xsv2sum=SG_MALLOC(float64_t,(x_d+1)(x_d+1));\n\nfor (int32_t i=0; i<x_d; i++)\n{\nfor (int32_t j=0; j<x_d; j++)\nXsv2sum[j_(x_d+1)+i]=Xsv2[j_x_d+i];\nXsv2sum[x_d*(x_d+1)+i]=sum[i];\n\n\nI dont knw what's the problem here, in my editor it is perfectly fine.\n\nwhich editor are you using?\nhave a look at\nREADME.developer\nwe use tabsize 4.\nSoeren\n. please move #ifdef to beginning of line (no indention)\n. please use SG_ADD instead of m_parameters->add\n. please break line after ~80 chars\n. if this is creating a diagonal matrix please use CMath::create_diagonal_matrix \n. white space fix:\nfor (int i=0; i<xxx; i++)\nand for single line for statements brackets are not needed\n. remove 1 empty line\n. remove comment please\n. remove useless protected please\n. please don't mix variables and functions in one block:\nuse\nprivate:\nfunction def 1\nfunction def 2\n...\nprivate:\nvariable 1\n...\n. could you please use SGVector as (return) arguments?\n. please add curly braces around first for loop and add an empty line after the for loops end\n. please put all implementations like that code here into the .cpp file\n. Ehh btw you don't even need that function - you can just use CLabels::get_unique_labels()\nSo if you do \nSGVector l=CLabels::get_unique_labels();\nyou have to call\nl.destroy_vector()\nwhen you are done using it\n. whats this il here?\n. please use { } around the if here (because the following statement requires > 1 line)\n. same here for the outer for loop\n. why do you need j outside the loops? I would prefer for (int32_t j=0; ...) statements that make clear that j is only used within this loop\n. please use as name set_use_covertree, same for variables m_use_covertree \n. can you please use the CMath::qnorm function here?\n. could you please extend the CMath::normone function with the dnrm2 stuff and use it here? for this please put the function in the .cpp file  - thanks.\n. err the SG_FREE below should probably be free_vector.\nset subkernel weights implementation depends on kernel. some kernels have subweights even, like CCombinedKernel, CWeightedDegreeStringKernel etc\nso these use a full vector, one CKernel has this weird set just 1 element function\n. err - train and apply should have exactly one SG_PROGRESS - no more. otherwise it is becoming meaningless.\n. I am not 100% sure about this - are you sure that store_model_features will always be called? will it work otherwise too?\n. what is so difficult in using shogun's CTime class?\nCTime time;\n///do sth\ntime.cur_time_diff_sec(true); // for wall clock time diff\ntime.cur_runtime_diff_sec(true); // for cpu time diff\n. The most common use case is m_q=1. While I would keep q!=1 support, I would just speed up the most common q=1 case with the covertree.\n. I would keep the q one - problem with k parameter and model selection is that it is much faster to train for k=1...max_k in one go and get all the results at the same time - not sure how one can make use of that in shogun's modsel procedure.\n. please use CMath::{max,min,pow,swap}, SG_ALLOC / SG_FREE and friends\n. something is wrong here whitespace wise - indention's in shogun are tabs of size 4\n. smame here and everywhere ... do you perhaps use spaces?\nPlease read src/README.developer - maybe that helps to adjust style.\nThanks\n. my general (speedup) comment here is that you don't need to call get_kernel_matrix() above but can just do:\nsigma-=kernel->kernel(j,i)*m_alpha[j];\nand probably precompute the kernel diagonal by just adding a function to CKernel like\nSGVector<float64_t> CKernel::get_diagonal()\nand then access things here with diagonal[i] instead of A.matrix[i+in] ...\n. such callback is not necessary here - please just use the enum switch you do above (and drop the function above)\n. please follow shogun naming, i.e. get_train_function would have been the correct way. however this function should just go away and merged with the train_machine one.\n. it is not possible to have an SGVector of some non-numeric type, use DynArray or DynamicArray or DynamicObjectArray instead.\n. why is this line commented out?\n. just remove the commented parts - it is not legal to use multiclass svms as MKL constraint generators (at least curently)\n. Yes it is a convention - SGVector is meant to be able to SGVector a,b; a+b; ab and other operations... So it was already a bug in the previous code.\n. the indention level in the .h file is still strange - please fix that - then it can be merged\n. what are these functions doing here? if you want to leave debug code in then please do the timing using CTime\n. @iglesias yeah postpone for later but better file a bug report such that this is not forgotten.\n. Use CDynamicObjectArray.\n. SG_ADD(&m_machines, \"machines\", \"Machines that jointly make up the multi-class machine.\")\n. could you please put all multiclass strategies into separate files? it is better to see them all separately - thanks!\n. please avoid any code in the .h file\n. I think you should also put this into multiclass strategy, e.g. add a function int32_t get_num_binary_classifiers() in there that will return the number of binary classifiers (or some other name get_num_machines() )\n. problem is though that #classes that can be found in label (subset)\nmight differ from #classes of the problem...\n. use SG_CALLOC then you don't need the *m_refcount=0\n. same here\n. SG_FREE(m_refcount) :)\n. please add yourself as sole copyright holder (instead of TU / max planck)\n. please use a whitespace after if, i..e\nif (chain_search(...))\nsame with for loops everywhere etc\n. that code will no longer compile...\n. why the spurious whitespace?\n. please use if (use_sg_mallocs) \n(one whitespace between if and ()\n. please - when there is just a single command - just use \nif (...)\n    return;\n. and why set and map?\nstl urgs\n. what is .clang_complete?\n. need to try that - I guess blackburn would be excited too\nOn Wed, 2012-05-09 at 01:45 -0700, Chiyuan Zhang wrote:\n\n\n@@ -45,6 +45,7 @@ configure.log\n /src/interfaces/r/Makefile\n /src/interfaces/r_modular/Makefile\n /src/examples\n+/src/shogun/.clang_complete\n\nIt's the config file for a vim plugin clang_complete. I added it to .gitignore to ignore it from git status. If you don't like it, I can remove this.\n\nReply to this email directly or view it on GitHub:\nhttps://github.com/shogun-toolbox/shogun/pull/509/files#r793833\n. this line is needed - otherwise serialization won't know the type - please uncomment.\n. and use SG_ADD here...\n. some documentation please :)\n. why do you need static_cast here? isn't int32_t(x) sufficient?\n. w/o looking at the code ... no need for round or alike?\n. please use SG_ADD instead\n. these kind of beatifications should be in a separate pull request..\n. it should be \n\nifndef _STRUCTURED_LOSS_H__\n. please use SG_SPRINT in the example instead of printf (not to confuse with SG_PRINT - the additional S is the static variant\n. indention weird here\n. it has bool return type so return false maybe :)\n. same here\n. set_options I guess :^)\nactually I would very much prefer that you set sane defaults (like you do)\nand then add one getter/setter method per parameter\n. something is weird here with the indention...\n. wrong indention\n. please always have a whitespace between if and ( and for and ( etc, like\nif (foo)\n. please use shogun's CTime class to do these measurements..\n. what is libgen?\n. SG_SPRINT instead of printf please\n. can you please for dynamic_cast not catch the exception but just check whether lf == NULL afterwards and then directly do the SG_ERROR here? this saves one indention level.\n. ok even better - could you add a function in LatentLabels - obtain_from_generic that returns the LatenLabels object (look at BinaryLabels.h for an example) - so you only need error handling once and good.\n. some spaces here please\n. remove extra newline please\n. well empty doc?\n. please have a space between if and condition, i.e., `if (condition)\n. I am pretty certain that you don't want that.\n. but rather:\nfloat64_t time=timer->start();\n. float64_t choose_feature=time->cur_time_diff();\ntimer->start();\nfloat64_t t_optimize = timer->cur_time_diff();\ntimer->start();\ncompute_pred();\nfloat64_t t_compute_pred = timer->cur_time_diff();\nand then the printing\n. well the linux kernel guys do it the very same way so please obey :D\n. noew remove the float_t casts and I merge!\n. please use new MyClass(); here\n. please use SG_MALLOC instead of new[]\n. rather use REQUIRE(m_dimension>0, \"Dimensions must be greater than zero\\n\") here\n. when doing casts please add a space like\n(CHOGFeatures*) m_features->get_sample(idx);\n. something is weir with indention here.\n. NICE_DENSEFEATURES???\nI think something like BUFFERPROTO_DENSEFEATURES or BUFFER or PYBUFFER_DENSE etc at least remotely match the name :D\n. please avoid the multiple ifdefs here: you can use the ifdef SWIGPYTHON in the NICE_DENSEFEATURES macro to do nothing or just define the macro to be empty.\n. please put on 2 lines\n. when doing casts, please have a space after the bracket )\n. if (xxx)\n. does that deal with negative indices?\n. this -num ... +num check and +=num_elements could be in an extra function\n. please add some test' for slicing with positive/ negative indices for 1d and 2d array access (with and w/o colon, single : ) etc\n. something is still weird with spaces ... mixture of tabs and whitespaces perhaps?\n. why is this needed? if shogun uses pthread related stuff in its header files the correct fix would be to move this code into the corresponding .cpp file\n. why do you use streaming features if you in the end only work with a dense feature matrix?\n. please use shogun's evaluation methods for that\n. since you never use svm locally you can avoid the SG_REF / SG_UNREF below\n. what do you need the kernel factories for? they really seem to be not necessary...\n. please use the CMath::qsort_index function here (at least inside this function)\n. why do you restrict the machines to libsvm? wouldn't any SVM work?\n. go left\n. please don't forget the \\n at the end of the error msg\n. please add a newline before and after the for loop\n. please use the REQUIRE macro, i.e.REQUIRE(feats != NULL, msg);` for cases like this\n. could you please use a single capital letter for the template type name, e.g. T\n. why that change? if you want a different precision make it an optional parameter\n. please use { }  to mark the code block in this for loop too for increased readability\n. I don't really get it then - in your kernel factory you are creating a kernel with default settings only - e.g. gaussian kernel with certain width. How can the user supply a certain width to be used? Wouldn't it be better to have some clone() / copy constructor for the kernel objects to create a clone of the kernel with its parameters?\n. but still you just need the standard CSVM interface for the things you are doing - so no need to work a specific svm implementation - CSVM should just work fine.\n. And why do you have to create a kernel? Couldn't the user just supply you a kernel object? At least this is what we do for any SVM...\n. typo: should be shallow\n. please add brackets around the if (,,,) { } to increase readability.\n. why are kernel factories still there now that you use shallow_copy() ?\n. same holds for  nested for loops  (and you do that a couple of times in the patch)\n. something is wrong with indention here...\n. what are these functions for? above it seems one can use any machine not just svms? If you implement shallow_copy for SVMs these functions would not be needed right?\n. that documentation is not really helpful... what is A / what is B (it would totally be OK just to write some formula / explanation once in the class doc not for the method).\n. please use (or at least utilize) MulticlassAccuracy::get_confusion_matrix()\n. doesn't\nclass CMath;\nand then code depending on it work w/o any math include work?\n. Please refactor MatrixFeatures to use SGMatrixList underneath and allow arbitrary matrix sizes (not just matrices that have same #cols). Please note that we had this  type*, int, int mess for quite a while and it really did no good so this really is an important change.\n. why do you change precision here? if you need a lower precision make it an optional parameter!\n. I guess we need to provide another load routine in dense features - but later.\n. actually this will cause build errors. I will remove that - please fix your code to work w/o -lpthread here\n. Seems like this was only still visible in the comments but not the code.\nChiyuan Zhang notifications@github.com wrote:\n\n\n@@ -4,7 +4,7 @@ VALGRINDOPTS=--tool=memcheck --error-limit=no\n--trace-children=yes --leak-check=\nINCLUDES=-I/usr/include/atlas\n LIBS=\n-LIBS_ADD=-lshogun\n+LIBS_ADD=-lshogun -lpthread\n\nOh, sorry. I remember I fixed this, no idea why it is still there.\n\nReply to this email directly or view it on GitHub:\nhttps://github.com/shogun-toolbox/shogun/pull/679/files#r1328760\n\n\nSent from Kaiten Mail. Please excuse my brevity.\n. there is still a D_EUCLIDIAN here\n. and here\n. and here\n. and here\n. hmmhh can you somehow check for os  greater equal 10.8 here? Otherwise we will have the same issue with the next os release.\n. why do you remove progress / gap computation?\n. this is definitely wrong - when maxtime = 0 it will not do a single iteration\n. will the uints here give us warnings from hell :D\n. why no longer -1?\n. why that? wouldn't it be better to use the number of threads shogun uses, i.e., parallel->get_num_threads()\n. please add a white space after the cast and put the else on a separate line\n. there is no need to cast to void ...\n. same here\n. and here\n. please add an init function which is called from constructors that registers all parameters to shoguns serialization framework, i.e.,\nSG_ADD(&degree, \"degree\", \"Degree of poly-kernel.\", MS_AVAILABLE);\nwould register degree to be a parameter that is also available for model selection\n. that line here is to long - 80 chars max is the limit!\n. is this really needed or could you just use shogun's parallel->get_num_threads()?\n. please add whitespaces between casts everywhere ... this is hardly readable.\n. please also everywhere limit line width to 80 chars\n. could you please turn w into an SGVector here such that\nSGVector<float64_t> w = SGVector<float64_t>()\nis the default argument? then you can check if\nw.vlen == 0\nbelow otherwise just do \nw=m_w\n. please always add new lines before the if\n. and please always do\nif (xxx)\n{\n  ...\n}\nelse\n{\n}\n. ahh it is the multiclass one right? but isn't libocas then using\n1...? so this should rather be +1 ?\nOn Thu, 2012-08-09 at 05:15 -0700, Sergey Lisitsyn wrote:\n\nIn src/shogun/lib/external/libocas.cpp:\n\n@@ -1170,7 +1637,7 @@ ocas_return_value_T msvm_ocas_solver(\n   /* initial cutting plane */\n   for(i=0; i < nData; i++)\n   {\n-    y2 = (uint32_t)data_y[i];\n-    y2 = (uint32_t)data_y[i]-1;\n\nIIRC It is different in libocas and shogun so every data_y[*] should\ncome with -1\n\u2014\nReply to this email directly or view it on GitHub. \n\n\nFor the one fact about the future of which we can be certain is that it\nwill be utterly fantastic. -- Arthur C. Clarke, 1962\n. please don't add doc / data here. use git submodule update --init to get in sync\n. please no different swig flags - you can edit your .config or write a  .config-local to override the flags locally\n. could you please use two SGVectors instead of the ptrs here?\n. please drop commented out code\n. please drop commented out code\n. please drop commented out code\n. please drop commented out code\n. please drop commented out code\n. no! just don't do this SGSparseVector v[size-1];\nbut instead dynamically allocate v with SG_MALLOC.\n. yeah sure you have to SG_FREE(vec) at the end\n. indention is wrong - we use tabs of size 4. also please remove the commented line\n. same here..\n. this cannot be correct. filename is a char* so it has to be SG_FREE() but then again it seems like the issue should rather be fixed in SerializableFile.\n. what is this file doing here?\nif you intended to document your examples - please put the documentation under shogun/examples/description - following the same filename conventions that you see there.\n. it can be expensive... so rather not\n. line 61 & 63 are wrong. you should just use SG_ADD(&preprocessed,...) and same for preproc.\n. make it eigen3 only then.\n. style: please use ((CCombinedKernel*) kernel)->... and k_idx++\n. please use CGaussianKernel* kernel here and avoid the casting one line below.\n. where did you get the kpca resdata reference from?\n. you don't need these lf/rf = NULL things. these were needed to test if there are more features than kernels. you still have to do the test below in the lf !=NULL checks but by comparing if f_idx != num_feature_obj\n. no newline here\n. please distinguish as much as possible function definitions and implementations:\nimplementations should be put in .cpp files and definitions in .h\n. please submit this file in a different PR against shogun-data (or leave it out for now and let  me do it later)\n. what do you need these names for?\n. please get rid of any std:: includes and functions in the .h file to the extent possible (it will increase compile time with swig enormously)\n. ohh look trailing whitespace!\n. when you do loops please do them in the\nfor (int i=0; i<foo; i++)\nstyle\n. please don't add the trailing ; to the SG_* macro.\nI have some corner case due to which ; is not needed (forgot which macro it was). so better stay consistent.\n. when you do if statements please do\nif (mystatement)\nspace after if and no space inside brackets\n. you can do an SG_ERROR here - that should never happen\n. could you use eigen_result/=CMath::sq(m_sigma) ?\n. coding style wise - please add a space after { and before } in all these single line return statements.\n. you have some whitespace issues here\n. and here..\n. please make this configurable, maybe even support sets of characters. you can very efficiently do this by defining a byte delimiters[256] array that has 1's at positions that should be delimiters, e.g. delimiters['\\n']=1;\nThis would help e.g. csv parsing.\n. yes please do - we never gave out the feature_list/array so it is not a problem\n. it is not legal to have 'null' feature objects in there so if the number of feature obj's is the same it should not happen. nevertheless an ASSERT(f1); etc won't hurt!\n. typo contains -> contained\n. Please also add a function that takes a char[256] delimiters as argument. Whenever delimiters[str[i]] != 0 the delimiting character is found. This can be a quick way when you have not just one char as delimiter but e.g. ';' '<space>', ',', '\\n'\n. contained\n. num -> number\n. please change the function signature to\nSGString<char> CLineReader::get_next_line()\nand add some bool has_next_line() that does the checks you do below so one could do while (has_next_line()) { ... }\n. do this NULL check inside SGString not here\n. rather create a constructor in SGString that does the allocation and size assignment than doing that manually here\n. please use private\n. private please\n. please describe in detail here how it works based on what you told me on IRC \n. you could directly read into the ring buffer here. I start wondering though if it makes sense to have 2 sizes: one for chunk size and one for max_string length. I mean if we need to keep a buffer of size max_string_length around anyway why use a smaller chunk_size? Or do you want to grow the string buffer for that?\n. something weird with indention here\n. you have 8 spaces here - please use 1 single tab for indenting\n. ohh true it might cause issues when attempting to read already serialized files. however I don't see any of the tests failing so it seems not to be an issue @karlnapf any idea why things still work?\n. it makes sense totally. I think we have to rename StringFeatures to VariableLengthVectors to avoid confusion?\n. well you could save to double's then in hdf5\n. Just push the real then the imag part.\n. we will be switching to using SGVector - please no longer use SGString\n. it is confusing to have 2 functiosn with same name almost doing the same in particular if only one is overloading one from a base class. so just drop the other.\n. well keep the const one!\n. I understand you need i below to display the wrong symbol. But IMHO the better fix is to use SG_PRINT in the print_histogram() function. - please do it this way and already send a separate(!) PR to fix this unparseable error message\n. ahh no! we cannot afford a dense matrix here. Just imagine your hash being 40bit or so and you have millions of documents. You directly have to create the sparse feature matrix structure. For N-grams you will then just get about number of chars in your document many strings. Note that this is still not optimal since you might have an ngram with index 240-1. So you should add a helper fucntion to sparse features compress_indices that creates a map for these random hashes -> 0...\n. well you need only d or bits. so drop bits and compute it from d (log2)\n. indeed it is not needed however if we restrict dim to be 2n then line 108 is more efficient :walking: \n. that is the best but there is one catch here. dim could be 2*32 and your hashed_doc vector too large to fit in memory. Better use DynamicArray for that and append all the indices. Then sort this array and when compute how often each index appears adding them to the .entry.\n. please use SGVector<char> here\n. no extra { } needed for single statements\n. please do not allocate on the stack - use SG_MALLOC here or some buffer\n. please use a larger block and make it a constant (say 10241024)\n. no shallow copy is what we want but SG_REF it!\n. doesn't SG_ADD work here too?\n. ohh this certainly defeats the purpose of dot features. with dotfeatures you should create the hash index here using some tokenizer on-the-fly and then just do result += vec2[index] ; there is way too much overhead otherwise\n. same here\n. IIRC this is just and upper bound. so use the string length\n. add SG_NOT_IMPLEMENTED here. it is ok to not do it.\n. don't do a temporary copy here but directly work on &sv.vector[start]\n. maybe you can extract these 2 lines into some static function to be used in the converter & here\n. again no temporaries here please\n. and reuse the function :)\n. inside hasheddocfeatures you have to sg_ref - rule is basically anyone keeping an object for later use has to SG_REF. anyone just passing an object into some other doesn't\n. please use SG_UNREF whenever possible instead of delete (occurs a couple of times)\n. use some const above max_line_length and better increase :-)\n. you should use the delimitertokenizer here and if it is not directly applicable make it applicable for finding the next string\n. please put constants into constant variables and please avoid to use 128 twice (it is confusing...)\n. please use init_shogun_with_defaults()\n. could you please use this function in classify_for_multiple_k to avoid code duplication\n. please add a \n#include <shogun/lib/config.h>\neven before the HAVE_EIGEN3 define (you will get build errors otherwise\n. please use brackets for if and for when you nest stuff - it is hard to read otherwise\n. here is the bug i+j is not unique should either be like benoit said or i*vec.size() + j\n. yeah no don't use & here please\n. that guy and the other above ar too long. lets have very few parameters and make it all optional with setters / getters.\nwe should by default not quote, and use ',' for writing and for readers expect , and \" if around - maybe auto-infer it based on the first line at some point\n. this function is too long now. please split it up in small logical units (>3 I guess)\n. please use \nfor ( )\n{\n}\nstyle (same with if () and function definitions...)\n. just write dense; here\n. don't you need this alloc to be vlen + 1?\n. why do you include SGObject in here?\n. m_refcount was an integer with the refcount so yes that should do it. btw you could consider keeping the refcount as part of the object (not a pointer).\n. you are right if done right it should be done in the same way we did sgreferenced data: there exists just a single RefCount object that is shared among all objects of this CSGObject and then when none exists it is deleted. currently we just don't do that and so I guess we will always have issues with the copy constructor. I would rather make the copy constructor private or something everywhere to ensure that we don't have garbage and then one by one later add things back sigh\n. @gsomix please check\n. @gsomix please check here too\n. I think he intended to check that this is 1. still weird\n. please keep it with an ASSERT then - I am pretty sure we have no test other than that so far or write a test and then remove this.\n. Err please don't - this constructor has 5 parameters already...\n. note that we leave out the 1/sqrt(2) term in the gaussian kernel - so when you do the test be aware.\n. please use the class CTime \n. And measure how long it takes to compute the Gaussian Kernel! I mean we are trying to speed things up - so we need to compare against it!\n. num_vecs = 10 is not a reasonable setup, because as long as we can easily compute the gaussian kernel matrix upfront we don't need any approximations. And for n=10 the pure gaussian kernel will always win. And we can easily compute it for n up to say 10000. So we don't need any approximations. So try with say 10 or 100 or 1000 dims and 100k 1e6, 1e7 examples!\n. same holds here use CTime and also use much more examples.\n. Yes all true. But the big difference is that your non-dot features won't fit in memory so your only chance is to use rfdot.\n. But right it makes more sense to compare liblinear/ocas  vs. e.g. LIbSVM both using the gaussian kernel/dotrf features for the benchmark.\n. please use CSVFile to load the labels / data\n. same here CSVFile please\n. use \nmodshogun;\nplease :)\n```\n. why these lines?\n. nice solution!\n. please use a CDynamicObjectArray for the layers and connections and rename Connection to CConnection to get it serialized\n. please use two lines for that and\nCNeuralLayer* m_input_layer;\n. you should just use m_parameters-> ...\n. I am not sure if this has bad consequences when there are multiple classes defined in a file? Do you / did you check?\n. there is sth weird with indention here\n. what do you need iostream for?\n. please put yourself as (C) only here - you are the sole author and thus deserve the credit\n. make this configurable with this value being the default\n. FindProtobuf.cmake is already in cmake's modules. So just require the appropriate cmake version and drop this fail and the one above.\n. please send a patch for this separately - it might come handy\n. please do not include generated code in the PR\n. we better get this to work via the CFile infrastructure\nso you write a CProtobufFile and support all the types in there.\n. we don't need to do it this way. just read in chunks of memory with the full protobuf message the usual fread functions and then decode the protobuf message. So no need for the posix file descriptor here and no need to use protobuf read functions.\n. ahh here is the file :)\nlets add another \nmessage ShogunVersion\n{\n   required int32 version = 1;\n   required SGDataType data_type = 2;\n   enum SGDataType\n  {\n    VECTOR=1;\n    MATRIX=2;\n    ...\n  }\n}\n. rather define\nmessage Int32Chunk\n{\n}\nwe can reuse that then for other data types (matrix ..)\n. note that chunks might have different lengths.\n. I would use the following format when writing stuff to disk:\n<size> (a uint32)\n<protomsg>\nsize denotes the length of the following protobuf message that you then decode. So whenever you write sth out you write it with a size marker prepending the actual message.\n. yeah but there is no need to explicitly compute phi you could still directly compute the dot product\n. Well the operations that are done with the Phi's is what we need to deal with. And these are the things we have in DotFeatures. However there is a big class of Kitchen sinks that just precomputes some random vectors to be used later and that class can be generalized and the RFDotfeatures are one particular case.\n. I know now. You should not use SGSparseMatrix of type Scalar but only one of the simple types like float64_t etc\n. using Scalar here cries for issues to come\n. a string comparison is not a good solution then rather use typeid(this) == typeid(df) \n. We better split this up into a shogun version header and a header for each data type.\n- so the length would be in the vector thing and the number of chunks could still be in here\n- for matrices this extra file would then contain rows / columns\n- for strings max string length, num strings, ...\n. ohh and for length better use uint64_t\n. this part here should be encapsulated in some function (finding number of blocks)\n. writing the header should be encapsulated in some function too\n. so this whole block would be hidden in write_global_header()\nfollowed by some call to write_vector_header()\n. and please name this differently - e.g. write_memory_block()\n. please leave out the CProtobufFile::get_vector stuff in such messages. This can be enabled with print_line_and_file in io if one really wants to know the precise location of the error.\n. could you encapsulate writing a message together with its 4 byte size header please\n. err it supports int64_t!\n. use a uint64 here already\n. I would suggest to use a single file\nChunks.proto\ndefining all the messages from byte to floatmax_t in there.\n. please include unistd.h and unlink() the file at the end\n. what is that needed for?\n. encapsulate these checks already in a function read_and_validate_header() that you pass the expected type\n. rather VectorHeader vector_header = read_vector_header(); etc\n. the read_memory_block can already do the allocation so rather\nvector=read_memory_block(...)\n. this is no longer sexy. too many arguments! please rather use sane defaults and add setters.\n. please add curly braces for the two liner in the for loop here\n. please ifdef the test with HAVE_PROTOBUF\n. cast this to int64_t - matrices might be way bigger than 2GB\n. total string length might be well over 2GB here so use in64_t too\n. please rename to elements_in_message\n. you can use a templated function for this - we only don't use templates in ProtobufFile since we are overloading CFile's various functions and overloading and templates is a pain in C++\n(but you don't have to do this if you don't want to)\n. you don't use num_messages here?! So why did you compute it in the first place and why does this function have this argument then?\n. I am confused here - set_array should not copy anything but take the stuff as is. But it not longer does this. So please don't free dic /dic_weights here and use the other set_array function that frees and doesn't copy\n. ahh ok I see. so the array got assigned a size bigger than it actually is\n. @van51 wish you were in IRC now to explain so now this way:\nI actually don't know why I used a dynarray at all anymore. I mean the dictionary is allocated from scratch every time with the length of the vector + what is already there. Then it is compacted in a merge step (only unique n-grams are stored!). It doesn't matter if the dyn array has a true size of 100MB if after the compaction only 10MB are used. So this function here is not needed.\n. don't do that - it may jeopardize convergence criteria (and I am admittedly not a big fan of using the risk)\n. this should still be heading not markdown type or you will have to figure out how to mark the notebook abstract\n. that is not the problem. we can use the second markdown cell if we want\n. yeah please remove this comment and also mention that this should contain some serious stand-a-lone shogun code  / load or generate data etc\n. I just tried - headings can also be markdown - so please lets stick to the rule. first markdown section == abstract. all the rest is header\n. diag_vlen is 32bit here - is that comparison sane? I've seen this pattern a couple of times (when you changed types to int64 for the variable in the loop.\n. why not write int64_t(c)*num_rows? and same below?\n. could we just have two operators - one for int32_t and one for int64_t please?\n. Then just add another one for uint32\nKoen van de Sande notifications@github.com wrote:\n\n\n@@ -208,7 +208,7 @@\n          * @param index dimension to access\n          \n/\n-        inline T& operator\n-        inline T& operator\n\nI've tried that, but then the compiler (clang and sometimes gcc) starts\ncomplaining that the operator[] is ambiguous when the argument is a\nuint32_t. My commit 6533afb above in the pull request was an attempt at\nthis (multiple versions), but it failed.\n\nReply to this email directly or view it on GitHub:\nhttps://github.com/shogun-toolbox/shogun/pull/1731/files#r7280269\n\n\nSent from Kaiten Mail. Please excuse my brevity.\n. Yeah it is a one-liner so 4 times the thing\nOn Tue, 2013-10-29 at 09:48 -0700, Koen van de Sande wrote:\n\nIn src/shogun/lib/SGVector.h:\n\n@@ -208,7 +208,7 @@\n         * @param index dimension to access\n         \n/\n-       inline T& operator\n-       inline T& operator\n\nLooking back at\nhttps://travis-ci.org/shogun-toolbox/shogun/builds/13077405 , with a\nthird one for uint32, it needs a fourth one for \"long int\"... or we\nmake the index type a template argument of the function? Or we go for\n4 versions.\n\u2014\nReply to this email directly or view it on GitHub.\n\n\nFor the one fact about the future of which we can be certain is that it\nwill be utterly fantastic. -- Arthur C. Clarke, 1962\n. please pass a SGVector centers here\n. same here SGVector ...\n. use an SGVector here too - basically whenever you need a vector :)\n. you don't really need that - just check if initial_centers.vlen > 0 \n. now you can really check the dimensions here - centers.vlen is your len\nso use\nREQUIRE(condition, \"error msg\")\n. space between if and ( and the next statement on a new line please\n. please add { } for the outer for loop (whenever number statements > 1)\n. for the final PR please provide us one commit only. you can use\ngit reset --soft <version_before_your changes>\nto get all your commits back to non-committed. Then just git commit -a everything\n. Why don't you use CLock?\n. Or how long is the locked state so the computation of grad->add(param, gradient); take?\n. CLock is potentially a busy lock(!) fast if you expect very short pauses\nand slow otherwise. but the op you are doing is el cheapo and so exactly\nwhat we use it for.\nThanks for the patch and keep it coming!\nOn Fri, 2013-11-15 at 00:24 -0800, Roman Votyakov wrote:\n\nIn src/shogun/machine/gp/InferenceMethod.cpp:\n\n@@ -31,6 +31,9 @@ struct GRADIENT_THREAD_PARAM\n    CMap >* grad;\n    CSGObject* obj;\n    TParameter* param;\n+#ifdef HAVE_PTHREAD\n-   pthread_mutex_t* lock;\n\nOops, sorry, I forgot about CLock.\nI don't understand the second question. Could you please explain a\nlittle bit? What do you actually mean?\n\u2014\nReply to this email directly or view it on GitHub.\n\n\nFor the one fact about the future of which we can be certain is that it\nwill be utterly fantastic. -- Arthur C. Clarke, 1962\n. no do a 1 here!\n. why is that SGObject include still here - drop it :)\n. we should talk to @iglesias about this - do we need serialization & clone etc for outputs?\n. do not forget to call free_feature_vector once done!\n. Yes you miss sth. You always need these fre** calls becauze there moght be preprocessor s involved\nHeiko Strathmann notifications@github.com wrote:\n\n\n@@ -115,6 +115,9 @@ float64_t\nCStringSubsequenceKernel::compute(int32_t idx_a, int32_t idx_b)\n     }\n// cleanup\n-  dynamic_cast*>(lhs)->free_feature_vector(avec,\n  idx_a);\n\nagain, why not use SGString or SGVector here? then memory handling is\nin autopilot.... Or do I miss something?\n\nReply to this email directly or view it on GitHub:\nhttps://github.com/shogun-toolbox/shogun/pull/1864/files#r9670171\n\n\nSent from Kaiten Mail. Please excuse my brevity.\n. you should move this up to the very top just after doing\n#include <shogun/lib/config.h>\n. the only problem with this patch that I see is that it is not in-place. and needs 3 times the memory of the old one\n. that should be EXPECT_TRUE here right?\n. return diff < eps;\n. if (tolerant)\n. Better put that into a different function fequals_abs()\n. don't use init_shogun_* / exit_shogun* from a modular interface\n. Yes indeed that would be slower (when we init the matrix later and\nbesides that one could use SG_CMALLOC if we really wanted this)\nOn Wed, 2015-03-25 at 03:44 -0700, Heiko Strathmann wrote:\n\n\n@@ -48,6 +48,7 @@ SGMatrix::SGMatrix(index_t nrows, index_t ncols, bool ref_counting)\n    : SGReferencedData(ref_counting), num_rows(nrows), num_cols(ncols)\n {\n    matrix=SG_MALLOC(T, ((int64_t) nrows)*ncols);\n-   zero();\n\nwhy that?\nWe dont want that I think\n\nReply to this email directly or view it on GitHub:\nhttps://github.com/shogun-toolbox/shogun/pull/2781/files#r27109755\n. Yeah maybe 10 or so SVMs :)\n\nOn Thu, 2016-06-16 at 01:14 -0700, Sergey Lisitsyn wrote:\n\nIn doc/cookbook/source/examples/classifier/kernel_svm.rst:\n\n+\n+Kernel Support Vector Machine is a binary classifier which finds a data-separating hyperplane in a Hilbert space induced by a positive definite kernel. The hyperplane is chosen to maximize the margins between the two classes. The loss function that needs to be minimized is:\n+\n+.. math::\n+\n-    \\max_{\\bf \\alpha} \\sum_{i=0}^{N-1} \\alpha_i - \\sum_{i=0}^{N-1}\\sum_{j=0}^{N-1} \\alpha_i y_i \\alpha_j y_j  k({\\bf x}i, {\\bf x}_j)\n  +\n  +subject to:\n  +\n  +.. math::\n  +\n-    0 \\leq \\alpha_i \\leq C, \\sum{i=0}^{N-1} \\alpha_i y_i = 0\n  +\n  +where :math:N is the number of training samples, :math:{\\bf x}_i are training samples, :math:k is a kernel, :math:\\alpha_i are the weights, :math:y_i is the corresponding label where :math:y_i \\in \\{-1,+1\\} and :math:C is a pre-specified regularization parameter.\n  +\n  +Implementation of Kernel SVM in Shogun uses LibSVM :cite:chang2011libsvm.\n\nIt is not strictly correct, is it? I mean we have a bit more of them.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub, or mute the thread.\n. \n",
    "frx": "Yeah. I think i missed out a function declaration too. Will push again.\n. problem with get_name! editing.\n. Okay.. You corrected the main part. Only a minor undocumented parameter is left to be changed.\n. Should I focus on changing the code of StreamingFeatures.{h,cpp} or the backend stuff (parser code)? And should I write the line reader myself or is there a library function for that already? I looked into the sources of AsciiFile.cpp and found some methods, but they read the entire matrix from the file first. (if my understanding is correct)\n. Made a simple StreamingFile class based on AsciiFile.. Currently, only a function to read a vector from a line is implemented. It sets the pointer to the vector and the number of features in the vector by reference.\n. You're right. I didn't modify StreamingFeatures or parser in my previous commit. Will modify them now.\n. I've made some more commits, mostly improvements in the parser code (both .h and .cpp). I don't intend this to be merged, but would like to get suggestions. I'll get back on it as soon as I'm free today (quite late in the day, probably).\n. Modified StreamingFeatures and parser code. Seems to be in working state (after only minimal testing). \nThe constructor for CStreamingFeatures takes another argument (whether the examples are to be interpreted as labelled or unlabelled).\n. So should I raise an exception if the features have been set to be\nunlabelled, and yet the user calls get_label()?\nThe parser knows whether the features are labelled/unlabelled depending on a\nflag set to true/false while initializing the StreamingFeatures object..\nWill there be a situation where some features may be labelled and others\nunlabelled in the same StreamingFeatures object? Then I'll have to use some\nother way of doing it.\nOn Sun, Jun 5, 2011 at 12:42 AM, sonney2k \nreply@reply.github.comwrote:\n\nYes it is basically your workflow: All I would like to see changed is that\nyou instead of get_next_* / free_feature_vector introduce 1-3 above. To make\nit explicit\n``` C++\nfeatures->start_parser();\nwhile (features->fetch_example())\n{\n  SGVector vec = features->get_vector();\n  float64_t lab=features->get_label(); / this is optional /\n  features->release_example();\n}\nfeatures->end_parser()\n```\n\nReply to this email directly or view it on GitHub:\nhttps://github.com/shogun-toolbox/shogun/pull/74#issuecomment-1302470\n. Done with most of the SGVector changes, preliminary testing done. Seems to be working, although I need to throw harder stuff at it..\n. Yes, I will. But there's one thing I haven't done properly in this: the same\nquestion of specifying size in terms of number of examples or size in terms\nof MB -- the code expects size in terms of MB but I think there are some\nsmall issues in the implementation of this that I want to discuss before\nfinalizing it.. Have we reached a conclusion on the size issue?\n\nOn Mon, Jun 6, 2011 at 3:05 AM, sonney2k \nreply@reply.github.comwrote:\n\nI've merged this but I have one more request: Could you please rename\nbuffer.* / parser.* to match the class names defined inside them? E.g.\nCParseBuffer -> ParseBuffer.h\n\nReply to this email directly or view it on GitHub:\nhttps://github.com/shogun-toolbox/shogun/pull/74#issuecomment-1306058\n. getline code here:\nhttps://gist.github.com/cc7378ba263013f45057\n\nWhich file should I place this in? This will fix the compilation issue, but later a better scheme can be found for the realloc() size..\n. This would be a member of the AsciiFile class? Or should I stick to the\noriginal specification of getline and keep it outside the class?\nOn Tue, Jun 7, 2011 at 2:13 AM, sonney2k \nreply@reply.github.comwrote:\n\nI would make this a static function in CAsciiFile\n\nReply to this email directly or view it on GitHub:\nhttps://github.com/shogun-toolbox/shogun/pull/74#issuecomment-1311739\n. Not tested much speedwise yet, but I expect it may be slower as GNU getline is better at the realloc() part.. So whatever read function they use probably has a better block size than the one used in mine. Anyway, the custom implementation only gets activated for OS X.. On other systems, CAsciiFile::getline() just calls GNU getline. The memory expanding step needs more thought as it grabs much more memory than required in some cases.\n. Did a test, but results aren't very good.. On a 24 MB file with approx 635000 newlines, GNU getline took around 0.11s while this one took about 0.68s. The bottleneck is the repeated fseek() that has to be done after every function call, since we're using fread..\n. The copyright/license part I'm not sure about..\n. Yes, in fact if we allow for templating these CStream classes too, then we can get rid of all those #define lines in the actual StreamingSimpleFeatures class by giving the specialized implementations of the vector reading functions right within CSimpleStreamFromSimpleFeatures and CSimpleAsciiStream for each type T. Then the \"set_vector_reader\" function would always point to one function only, get_vector() for whichever Stream object, instead of get_real_vector(), get_int_vector() etc.\n\nFor now, I'll make the templated versions and there should not be much difficulty in reverting to the original specification if we need to later.\n. But in this inheritance scheme, CSimpleStreamFromSimpleFeatures and CSimpleAsciiStream would not have much in common, though they are derived from CSimpleStream (if I have understood things right). Every class would become independent somewhat, and that's one reason I made this implementation actually - Streaming_File classes share a FILE stream, and StreamingFileFrom_Features share a CFeatures object. \nThere's nothing to be concerned about, though. I think this would hardly be a problem - just wanted to clarify whether you had the same thing in mind or thought of doing it in some way which I haven't grasped yet.\n. Shouldn't the CSimpleAsciiStream or any other object streaming from a file also derive from CFile? If I do this here, SimpleAsciiStream will inherit both from SimpleStream and CFile leading to multiple inheritance. SimpleStream may be made to derive from CFile, but this will be inconsistent with SimpleStreamFromSimpleFeatures.. And anyway, if one can make SimpleStream derive from CFile, then why not make a general FeatureStream base class which derives from CFile, and then make SimpleStream, SparseStream and StringStream derive from it?\nOr we can allow for multiple inheritance so SimpleAsciiStream can derive directly from AsciiFile.. What do you suggest?\n. I have finished doing it in the new way, but it seems very much tangled up.\nSo the problem is this: \nConsidering SimpleAsciiStream/SimpleStreamFromSimpleFeatures, we need different functions get_real_vector(float* vector, int len), get_int_vector(int*, int) etc. for different T in SimpleFeatures.\nBoth these Simple_Stream classes should implement each of these functions taking care of each T. Meaning each of them has their own version of get_int_vector, get_real_vector, etc. So their base class, SimpleStream, should contain the declarations of all these functions, right? And in the StreamingSimpleFeatures class, we decide which of these get___vector functions to use for reading depending on T in StreamingSimpleFeatures. (T==int => use SimpleStream::get_int_vector() etc.)\n. And what object should the parser work on? I mean it used to have a StreamingFile object as member, and the rest of the Streaming* classes were derived from StreamingFile. What should it use now?\n. Updated the structure of StreamingFile, making a new request..\n. Should I change anything more with this code before the merge?\n. This request shouldn't be merged! I'm keeping this open just so that it is easier to compare the two pull requests.. The updated one is meant for merging.\n. Testing-wise, I haven't done much for this. I checked it with a small dataset and the results for w, bias, and number of support vectors were identical to that of the batch version in shogun with one iteration. I'll test further today.\n. bscale in Shogun's version turns out to be 2*bscale of svmsgd2 (since 'n' is incremented twice in svmsgd2 as compared to a single n++ in shogun). This is changed, and the default loss function is changed (only temporary; a switch to more versatile selection of loss functions will have to be made) to SQUAREDHINGELOSS.\n. And, I get an error trying to compile upstream, 'error: using temporary as lvalue' at clustering/GMM.cpp:315,316,417,418,523,524.. Could be a local problem, I'll check again.\n. And, I get an error trying to compile upstream, 'error: using temporary as lvalue' at clustering/GMM.cpp:315,316,417,418,523,524.. Could be a local problem, I'll check again.\n. SGD and SGDQN use a preprocessor #if to impose a condition around a block for particular loss functions. Should we have enums for our loss function classes to be able to do something similar? Or is it not good to differentiate code of the learning algorithm on the basis of the loss function?\n. SGD and SGDQN use a preprocessor #if to impose a condition around a block for particular loss functions. Should we have enums for our loss function classes to be able to do something similar? Or is it not good to differentiate code of the learning algorithm on the basis of the loss function?\n. I didn't mean that part - there's a section of a code which says #if LOSS < LOGLOSS { block } # endif within the train() function. (Line 190 of SVMSGD.cpp)\n. I didn't mean that part - there's a section of a code which says #if LOSS < LOGLOSS { block } # endif within the train() function. (Line 190 of SVMSGD.cpp)\n. Yeah - I tested SGD and Online SGD.. They seemed to be still giving decent results.. Sorry about those SG_REF and base class constructor calls, I'll add them now.\n. Yeah - I tested SGD and Online SGD.. They seemed to be still giving decent results.. Sorry about those SG_REF and base class constructor calls, I'll add them now.\n. new[] is no longer used, and the initialization stuff is now placed along with the rest of the code.\n. new[] is no longer used, and the initialization stuff is now placed along with the rest of the code.\n. Removed iostream, string, fstream etc. Also added a couple of buffer read/write functions in IOBuffer which will later by used by the cache.\n. Removed iostream, string, fstream etc. Also added a couple of buffer read/write functions in IOBuffer which will later by used by the cache.\n. Moved substring to SGIO.h\n. Moved substring to SGIO.h\n. Also, would it be beneficial to revert to the relative include paths before compilation? With the  etc we use now, I find hard to catch errors occurring when an include file which I removed (or renamed) from the working folder is still present in my /usr/local/include/shogun folder from a previous make install.. The compilation will succeed on my computer but should actually be failing if the header file is not found in the source directory..\nmake clean should be removing all the .o files from the source directories, right? It behaves somewhat strangely for me now.. Unless I do a git clean -dfx or a make distclean, the .o files don't disappear..\n. Also, would it be beneficial to revert to the relative include paths before compilation? With the  etc we use now, I find hard to catch errors occurring when an include file which I removed (or renamed) from the working folder is still present in my /usr/local/include/shogun folder from a previous make install.. The compilation will succeed on my computer but should actually be failing if the header file is not found in the source directory..\nmake clean should be removing all the .o files from the source directories, right? It behaves somewhat strangely for me now.. Unless I do a git clean -dfx or a make distclean, the .o files don't disappear..\n. And do you think I should convert labels and/or weights to float32_t from float64_t for all the online learners we use currently?\n. And do you think I should convert labels and/or weights to float32_t from float64_t for all the online learners we use currently?\n. Added base classes for read and write of VW cache files. (They don't depend on much other VW code)\nTwo caches are currently implemented (but not included in this commit) - one using VW's native cache format, and the other using Google's Protocol Buffers. I will include them in another pull request.\n. Added base classes for read and write of VW cache files. (They don't depend on much other VW code)\nTwo caches are currently implemented (but not included in this commit) - one using VW's native cache format, and the other using Google's Protocol Buffers. I will include them in another pull request.\n. Yes, I will make the float32_t transition - I didn't want to change too much all at once. Also, the function conforming to the dot(float_, int) signature is present which indirectly calls the dot(VwExample_, float).. I'll try to use this from SGD and verify.\n. Yes, I will make the float32_t transition - I didn't want to change too much all at once. Also, the function conforming to the dot(float_, int) signature is present which indirectly calls the dot(VwExample_, float).. I'll try to use this from SGD and verify.\n. The rest of the changes are for StreamingSparseFeatures, taking care of some cases where there are dimensional inconsistencies between one example and the next.\nAlso, I have included a C++ example for online SGD.\n. The rest of the changes are for StreamingSparseFeatures, taking care of some cases where there are dimensional inconsistencies between one example and the next.\nAlso, I have included a C++ example for online SGD.\n. I've modified it further, but right now a lot of the code (eg. examples/libshogun/classifier_{libsvm, minimal_svm}.cpp) is encountering segmentation faults and the VW executable too gives some memory problems - is this occuring in general or do you think this is a problem with the VW code?\n. I've modified it further, but right now a lot of the code (eg. examples/libshogun/classifier_{libsvm, minimal_svm}.cpp) is encountering segmentation faults and the VW executable too gives some memory problems - is this occuring in general or do you think this is a problem with the VW code?\n. I tried them after merging from upstream around 7 hours back.. Could be a local problem for all I know..\ngdb on a classifier_libsvm.cpp executable shows something like this:\nProgram received signal SIGSEGV, Segmentation fault.\n0x0804be94 in shogun::CLabels::get_label (this=0x80531f8, idx=34717) at /usr/local/include/shogun/features/Labels.h:131\n131             return labels.vector[real_num];\n(gdb) bt\n0  0x0804be94 in shogun::CLabels::get_label (this=0x80531f8, idx=34717) at /usr/local/include/shogun/features/Labels.h:131\n1  0xb7e7ab82 in shogun::CLabels::get_num_classes (this=0x80531f8) at features/Labels.cpp:122\n2  0xb7e7a604 in shogun::CLabels::CLabels (this=0x80531f8, src=...) at features/Labels.cpp:42\n3  0x0804b3cf in main () at classifier_libsvm.cpp:69\n(gdb) p real_num\n$1 = 34717\n(gdb) \nand on classifier_minimal.cpp:\n[Thread debugging using libthread_db enabled]\nProgram received signal SIGSEGV, Segmentation fault.\n0x0804b4e4 in shogun::CLabels::set_label (this=0x8052398, idx=0, label=-1) at /usr/local/include/shogun/features/Labels.h:93\n93                  labels.vector[real_num]=label;\n(gdb) bt\n0  0x0804b4e4 in shogun::CLabels::set_label (this=0x8052398, idx=0, label=-1) at /usr/local/include/shogun/features/Labels.h:93\n1  0x0804b0c7 in main (argc=1, argv=0xbffff7e4) at classifier_minimal_svm.cpp:32\n(gdb) p real_num\n$1 = 0\n(gdb) \n. In general I reckon this seems to happen where SGObject initialization is present - kernel_gaussian.cpp crashes in a similar way:\nProgram received signal SIGSEGV, Segmentation fault.\n0x0804b734 in shogun::CKernel::kernel (this=0x8052ee8, idx_a=0, idx_b=0) at /usr/local/include/shogun/kernel/Kernel.h:231\n231             return normalizer->normalize(compute(idx_a, idx_b), idx_a, idx_b);\n(gdb) bt\n0  0x0804b734 in shogun::CKernel::kernel (this=0x8052ee8, idx_a=0, idx_b=0) at /usr/local/include/shogun/kernel/Kernel.h:231\n1  0x0804afd8 in main (argc=1, argv=0xbffff7e4) at kernel_gaussian.cpp:37\n(gdb) \nbut mathematics_confidence_intervals.cpp (which does not appear to create SGObjects) works fine, I think.\n@1040: 11:26AM$ ./mathematics_confidence_intervals\nsample mean: 5.500000. True mean lies in [3.334149,7.665851] with 95.000000%\nEND\n@1041: 11:26AM$ \n. ParseBuffer wasn't an SGObject before, so made it now.\n. I did this as a convenience measure. Copying the vector from the parser's buffer into the object's data member would enable me to immediately 'finalize' the current example and make that buffer space available for writing. Plus, using f->current_feature_vector would be more reliable since the contents don't change unless get_next_feature_vector is called again.\nIt would be a bit confusing when to finalize the example if the pointer is returned directly. Once the user finishes processing the example, he would have to finalize it explicitly from the code. \nBut I think with some more careful coding, the technique of returning the pointer directly would work. I'll look into this.\n. Yes... This takes care of the doubt in my previous comment also.\n. You mean DynArray has some other method for doing this, or that we should be using something other than DynArray?\n. But for sparse features? If we are planning on having defaults for unspecified features (like VW does), I think it'd be difficult to work making the assumption of fixed line number.\nAnyways, I haven't implemented sparse feature reading yet, so that problem will be taken care of later. For dense features, I think we could use this.\n. So is the online data going to have a different format? I mean is it not mandatory to make it compatible with the current ascii representation that we are using for batch algorithms now?\n. In the longer run, we must encapsulate a Shogun features object here, right? Should I start changing the feature_vector and label to be Shogun objects, i.e., C...Features and CLabels? Though that would require changing much of the get_next_feature_vector code.\n. Currently, how is BinaryFile being used in Shogun? I mean persistence in binary format could be equivalently carried out through one of the techniques being discussed for VW - Avro or Protocol Buffers (first I'll have to gauge the advantage, if any, obtained from using these methods)..\n. Examples are being stored in the memory as  followed by\n.. The example object only contains a pointer to the feature\nvector, while the actual feature vector itself is next to the example object\nin the memory.. Hence, I do a malloc combining the sizes of the example\nobject+feature vector. I can use new[] but then I'll have to do new\nchar[...] because it is not a contiguous stream of objects of the same type.\nOn Mon, Apr 25, 2011 at 4:33 AM, sonney2k \nreply@reply.github.comwrote:\n\n\n\nnumber_of_features =\n  current_number_of_features;\n  +\n// Now allocate mem for buffer\nif (example_type == LABELLED)\n{\nexample_memsize =\n  sizeof(LabelledExample) + sizeof(float64_t)*number_of_features;\ncurrent_example =\n  (LabelledExample*) malloc(example_memsize);\nexamples_buff =\n  (LabelledExample_) malloc(example_memsize_buffer_size);\n  +\n  current_feature_vector = (float64_t) ((char ) current_example +\n  sizeof(LabelledExample));\n  +\n}\n  +\nelse\n{\nexample_memsize =\n  sizeof(UnlabelledExample) + sizeof(float64_t)*number_of_features;\ncurrent_example =\n  (UnlabelledExample*) malloc(example_memsize);\n\n\nwhy do you use malloc instead of new[] ?\n\nReply to this email directly or view it on GitHub:\nhttps://github.com/shogun-toolbox/shogun/pull/52/files#r22420\n. Do you mean a struct with a flexible array member?\nLike:\nstruct x {\nexample ex;\nfloat fv[ ];\n}\n\nIt still looks somewhat complicated (in my opinion) to do the allocation for\nthis using 'new' while ensuring that all example and feature vectors lie in\ncontinuous memory locations.. No direct way strikes my mind right away..\nCurrently, examples are stored as:\nexamples_buff = _buffer_size, and it is\nnecessary that the example and vector are continuous.\nI can change the code so that storage is like:\nexamples_objects_buff = _buffer_size\nfeature_vectors_buff = *buffer_size\nwhich doesn't depend on this continuity criterion. I can use 'new' for this\nvery easily as well.\nShould I make this modification?\nAnd btw, why do you recommend 'new' and not 'malloc'?\nOn Tue, Apr 26, 2011 at 12:41 AM, sonney2k \nreply@reply.github.comwrote:\n\n\n\nnumber_of_features =\n  current_number_of_features;\n  +\n// Now allocate mem for buffer\nif (example_type == LABELLED)\n{\nexample_memsize =\n  sizeof(LabelledExample) + sizeof(float64_t)*number_of_features;\ncurrent_example =\n  (LabelledExample*) malloc(example_memsize);\nexamples_buff =\n  (LabelledExample_) malloc(example_memsize_buffer_size);\n  +\n  current_feature_vector = (float64_t) ((char ) current_example +\n  sizeof(LabelledExample));\n  +\n}\n  +\nelse\n{\nexample_memsize =\n  sizeof(UnlabelledExample) + sizeof(float64_t)*number_of_features;\ncurrent_example =\n  (UnlabelledExample*) malloc(example_memsize);\n\n\nOn Mon, 2011-04-25 at 09:41 -0700, frx wrote:\n\nExamples are being stored in the memory as  followed\nby\n.. The example object only contains a pointer to the\nfeature\nvector, while the actual feature vector itself is next to the example\nobject\nin the memory.. Hence, I do a malloc combining the sizes of the\nexample\nobject+feature vector. I can use new[] but then I'll have to do new\nchar[...] because it is not a contiguous stream of objects of the same\ntype.\n\nYou could use a struct and then new - or am I missing something?\nSoeren\nFor the one fact about the future of which we can be certain is that it\nwill be utterly fantastic. -- Arthur C. Clarke, 1962\n\nReply to this email directly or view it on GitHub:\nhttps://github.com/shogun-toolbox/shogun/pull/52/files#r22610\n. I'm allocating memory for the buffer only when the first example is seen. (if number_of_vectors_parsed == 0)\nThis code has to be changed as the thought behind this was to assume number of features as constant, and assume this to be equal to the number of features in the first example seen. After this is known, adequate memory space is allocated for the buffer by using sizeof(float64_t)*number_of_features.\n. That is a bottleneck in my code. The parser waits for the examples in the\nbuffer to be used rather than expanding the buffer to read more examples.\nMy technique assumes constant dimensionality, and won't work for sparse\nfeatures.. How do you recommend I do the memory allocation in this case?\n\nOn Wed, Apr 27, 2011 at 7:55 PM, JohnLangford \nreply@reply.github.comwrote:\n\n\n\nnumber_of_features =\n  current_number_of_features;\n  +\n// Now allocate mem for buffer\nif (example_type == LABELLED)\n{\nexample_memsize =\n  sizeof(LabelledExample) + sizeof(float64_t)*number_of_features;\ncurrent_example =\n  (LabelledExample*) malloc(example_memsize);\nexamples_buff =\n  (LabelledExample_) malloc(example_memsize_buffer_size);\n  +\n  current_feature_vector = (float64_t) ((char ) current_example +\n  sizeof(LabelledExample));\n  +\n}\n  +\nelse\n{\nexample_memsize =\n  sizeof(UnlabelledExample) + sizeof(float64_t)*number_of_features;\ncurrent_example =\n  (UnlabelledExample*) malloc(example_memsize);\n\n\nOn 04/27/2011 10:20 AM, frx wrote:\n\n\n\nnumber_of_features =\n  current_number_of_features;\n  +\n// Now allocate mem for buffer\nif (example_type == LABELLED)\n{\nexample_memsize =\n  sizeof(LabelledExample) + sizeof(float64_t)*number_of_features;\ncurrent_example =\n  (LabelledExample*) malloc(example_memsize);\nexamples_buff =\n  (LabelledExample_) malloc(example_memsize_buffer_size);\n  +\n  current_feature_vector = (float64_t) ((char ) current_example +\n  sizeof(LabelledExample));\n  +\n}\n  +\nelse\n{\nexample_memsize =\n  sizeof(UnlabelledExample) + sizeof(float64_t)*number_of_features;\ncurrent_example =\n  (UnlabelledExample_) malloc(example_memsize);\n  I'm allocating memory for the buffer only when the first example is seen.\n  (if number_of_vectors_parsed == 0)\n  This code has to be changed as the thought behind this was to assume\n  number of features as constant, and assume this to be equal to the number of\n  features in the first example seen. After this is known, adequate memory\n  space is allocated for the buffer by using\n  sizeof(float64_t)_number_of_features.\n\n\n\nI see.  In VW we double memory on demand with realloc() as needed---(See\nv_array.h).  This is pretty close to what the stl vector<> does.\n-John\n\nReply to this email directly or view it on GitHub:\nhttps://github.com/shogun-toolbox/shogun/pull/52/files#r23491\n. Can't say much about the proto buffers alternative yet. I will restructure my proto definition using repeated packed fields, but I think the structure of the .proto wouldn't be very clean. The performance and size will benefit, though. I will start it in a day or two because of some last minute work submissions at college.\n. Is it okay to use a LabelledExample object only? The only drawback would be\nthat in the case of unlabelled examples, the 'label' will not have a useful\nvalue and would just be taking up extra memory in the buffer. Is this space\n(=1 int for each example) large enough to make a separate UnlabelledExample\nobject?\n\nOn Sun, May 1, 2011 at 11:03 AM, sonney2k \nreply@reply.github.comwrote:\n\n\n@@ -192,39 +196,34 @@ void CInputParser::copy_example_into_buffer(void*\nexample)\nif (example_type == E_LABELLED)\n  {\n-             current_example_loc = (LabelledExample )\n-                     ((char _)examples_buff +\n  buffer_write_index_example_memsize);\n  -\n-             ((LabelledExample ) current_example_loc)->feature_vector =\n  (float64_t )\n-                     ((char ) current_example_loc +\n  sizeof(LabelledExample));\n-             current_example_loc = ((char _)examples_buff +\n  buffer_write_index_example_memsize);\n\ncoding style wise, please use\n&examples_buff[buffer_write_index] and avoid that void*\n\nReply to this email directly or view it on GitHub:\nhttps://github.com/shogun-toolbox/shogun/pull/74/files#r24682\n. I could, but that would be helpful only if we're running getline() in a while loop.. Otherwise, if one getline() is called and then some other file operation is used, it will lead to inconsistent results..\n. But I guess the most common use of getline() would be in a while loop, so it is worthwhile to implement the buffered version, even if that function has to be named something else (because a normal user of 'getline' would expect the file pointer to be after the \\n after each call).\n. I haven't updated this commit yet - StreamingAsciiFile used to hold the functions to read vectors from an ASCII file.. It will no longer be there once I finish the code in the way we decided afterwards.\n. Okay, so that StreamingSimpleFeatures has a constructor with a CSimpleFeatures* as argument? I'll make the changes.\n. CStreamingFile was unncecessarily deriving from CFile.. This was making the implementation of getvector etc necessary in the StreamingFile classes since they are pure virtual in CFile.\n. Removed the get_vector, get___string etc functions, since they no longer need to be implemented (CStreamingFile no longer derives from CFile).\n. Basic file functionality now has to be implemented here..\n. Main edit here..\nget_vector, get_vector_and_label etc are overloaded functions.\n\nThe macro just declares each function for each different types here.. The implementations in the .cpp file are basically SG_ERROR() since the functions actually required by the derived class would have been overloaded by it. And it is no longer possible to call a get_vector(bool_, int) from a StreamingFileFromSimpleFeatures class, since get_vector depends on the template parameter and its signature is:\nvoid get_vector(T_ &vec, int32_t len) (T==float64_t here).\n. File removed since class has been templated..\n. This is written just in case get_vector is called with unexpected arguments, though I suspect this won't ever be called.\nThe function pointer CStreamingFile::get_vector is used for reading, and it will probably either match the proper implementation in this class or one of the dummy implementations in the StreamingFile base class.\n. Function which is expected to be called, T depends on the class type.\n. Implementation depending on class type T..\n. This and StreamingFileFromStringFeatures.cpp haven't been updated to the new structure, but compilation still succeeds.. I'll change them if the design is accepted..\n. No longer needs the macro based definition.. The function for reading will be CStreamingFile::get_vector for all types.\n. Some functions have been changed to use float64_t* + int32_t instead of SGVector as I was finding it easier to use that way. (and also because CDotFeatures etc. follow this)\n. Constructor which 'converts' the conventional SimpleFeatures object into one which behaves like a Streaming source.\n. Example updated to use labels.\n. Since I found it easier to use individual (float_, int) variables for the weight vector during the SGD implementation and the code was clearer while using proper names like w_dim instead of accessing w_vec.vlen for the same purpose..  Sometimes we also need to SG_REALLOC the weight vector for streaming sparse features... The code for that too was easier to read with separate (float_, int) components. Also I wanted the syntax to be as similar to DotFeatures as possible, and that uses this convention too..\n. yeah - i hadn't updated the sparse and string files for this structure..\n. The templated fallback will most likely not be used if the proper dummy functions have been implemented in the StreamingFile base class since the compiler prefers normal functions over templated functions in case there's a clash between the two.. (so I've read)\n. I did this thinking if some new read function were to be added, you could just add the one line within this #define, and the macro would take care of the declarations for all data types used in shogun.. But I think it's better as you say to have them more explicit.\n. overloading should work because get_vector isn't templated - the class is templated, so the get_vector function defined here is specific. thus is overrides the corresponding specific get_vector in the base class..\n. you're right - i only need to define one implementation based on the class type T.. I got carried away by the macro thing and forgot that they aren't needed here..\n. Yes - I had made one version of this using a DynArray and later using get_array(), but do you think it we should do this in Online*Machine or make something like add_label() in CLabels itself?\n. The diag[] array has only zero elements for this kind of solver. We could remove this and shorten the following code considerably, but it could be problematic if someone else tried comparing this code with the original LibLinear.cpp in shogun. At least they look similar now if we let this be.\n. I haven't read about the m_linear_term much - how can it be integrated into the online version (unless we use a constant value), if it needs to be?\n. Due to various terms being zero, it turns out this is the only condition that will be satisfied. We could remove the other else statements.\n. Was unimplemented, gave a warning in swig. The other changes below are made since 'del' is a reserved python keyword.\n. Is this the correct way?\n. This particular thing could have remained SG_MALLOC.. I'll change it if the rest is ok.\n. Here is the line where particular use of new() is made. \nThe ring is initialized with buffer_size number of objects.\nFor example, in VW, the ring would contain objects of type VwExample. I would like them to be initialized through the constructor because each VwExample contains a VwLabel pointer inside it, which is initialized on construction. The labels is initialized only once (when the ring is constructed), and are reused when the number of examples exceeds the ring size.\n. The order needed to be changed here to avoid warnings regarding order of initialization of values when the constructor is called.\n. Can size_t remain size_t in shogun? Or should I change it to int32_t?\n. I'll change this to float32_t etc if the rest is okay.\n. It could be used by AsciiFile but we'd have to switch over to the buffered i/o in that also.. I have only done this in my Streaming_File classes as I didn't want to bring this major transition into the core shogun AsciiFile class yet.. I think it would require a complete modification of get_vector__ in AsciiFile.cpp, so I decided it would be better to bring about this change slowly.\niostream can be removed from this - it works without it. I'll remove the include line.\n. It would probably be ok to have VwEnvironment as an SGObject since only one instance of it is used by one VW instance.. Others like VwExample and VwLabel are used too many times so I decided not to make them SGObjects for now.\n. Added a default constructor taking no args\n. Yes, that's a better thing to do.. I've submitted what hopefully should be an improvement.\n. I haven't included the Protocol Buffers cache yet because that would add an unnecessary dependency for now.. Maybe when the rest is done we could add that.\n. This makes the example object ready for being used by the learning algorithm.\n. I'm not very clear about how a function like this would be used - it is easy enough to implement, but SG_NOTIMPLEMENTED for now.\n. Dot product between feature vector and vec2 (== weight vector). The length of vec2 is not specified as the function directly attempts to access the hashed index from the feature. The weight vector is of constant predetermined length.\n. If vec2 is of type double, I have kept it SG_NOTIMPLEMENTED since we may soon shift to float32_t type weights..\n. I haven't done this for AsciiFile yet. I tested an implementation locally, but this technique isn't very convenient when operating on a file read wholly into the memory. We can work on it, but I'll do it later. Using this get function reduces total time by half for online algorithms.\n. This mimics the default VW output format (useful for comparison/testing).\n. The offset functions are mainly used by the predict functions when features are paired.\n. Should classifiers use train() or train_machine()? The new train_machine has caused some ambiguity for me.. What's the correct method to implement?\n. that's what i got when i aligned it to match the new if (!no_training) { .. } block..\n. ",
    "ahmaurya": "Hi,\nI have made the necessary changes: Addition of compact option to GaussianKernel, Addition of InverseMultiQuadricKernel, Deletion of GaussianCompactKernel.\nThanks.\n. Hi,\nI have made the necessary changes: Addition of compact option to GaussianKernel, Addition of InverseMultiQuadricKernel, Deletion of GaussianCompactKernel.\nThanks.\n. Hi,\nThe formulation of Circular Kernel at http://crsouza.blogspot.com/2010/03/kernel-functions-for-machine-learning.html is wrong. Hence I used the formulation given by Genton in Classes of Kernels for Machine Learning: A Statistics Perspective\nThanks.\n. Hi,\nThe formulation of Circular Kernel at http://crsouza.blogspot.com/2010/03/kernel-functions-for-machine-learning.html is wrong. Hence I used the formulation given by Genton in Classes of Kernels for Machine Learning: A Statistics Perspective\nThanks.\n. You could use M_PI provided in C++ by default.\n. You should replace // type comments with /* ... / type documentation comments before every variable and function definition. This ensures that the generated documentation contains descriptions of each component of the class.\n. ",
    "Bautista": "Hi,\nI already set the username/email settings. Could you confirm i did it right? \nThanks :)\n. Hi,\nI already set the username/email settings. Could you confirm i did it right? \nThanks :)\n. I managed to set the user and email. Finally I think is all ok :)\n. I managed to set the user and email. Finally I think is all ok :)\n. Thanks, \nA new pull request is performed.\n. Sorry, \nMy fault in the first place since I didn't notice. Silly mistake.\n. ",
    "karlnapf": "In addition, the current version did not compile (spelling mistake of member variable)\n. In addition, the current version did not compile (spelling mistake of member variable)\n. Hi there,\nthanks for the feedback.\nI changed all the discussed points.\nHope, it is ok now\nHeiko\n. Hi there,\nthanks for the feedback.\nI changed all the discussed points.\nHope, it is ok now\nHeiko\n. I left the setter for the subset matrix as it is for the moment.\nThe rest works, I already tested it with libshogun\nTo wirte a python example, first the setter has to work with swig right?\nShouldnt I change it for now or should I wait for the update of the swig-stuff?\n. I left the setter for the subset matrix as it is for the moment.\nThe rest works, I already tested it with libshogun\nTo wirte a python example, first the setter has to work with swig right?\nShouldnt I change it for now or should I wait for the update of the swig-stuff?\n. done\n. oh no :(\ndo you already know which part crashes?\n. now it should be ok\n. Thanks :)\nI (hopefully) changed all the things you noted.\n. so if there is a nested for loop, where the innermost loop contains one command, you want brackets for all loops but the innermost?\n. alright then :)\nI created an example, (valid according to valgrind)\n. just added an example on how to change CSGObject parameters\nno memory leak according to valgrind\n. names changed ...\n. ok, done, among other things\nbut acutally I think this is less elegant than the multiple inheritance stuff.\n. ah sorry stop,\nI just thought of something better than the public subset variable ... do not merge yet\n. Hey Soeren,\nWe talked about smaller pull requests. This is the last large one.\nThe Grid-Search example currently is broken, but I have to touch some more classes to make it working (SimpleFeatures subset stuff) and the request would grow even further.\nAlso, some subset related stuff in labels may not be working correctly, but I need the example to be working before I can really check it. At least everything compiles.\n. I think there was a bug in LinearMachine (last commit)\n. worked in most of your comments\na few questions remain (see above)\n. added an example/test for subsets of labels\n. changed name of grid-search example and made it use LibLinear.\nfixed a bug in LibLinear\n. everything done except for the data thing.\nI got a lot of changes in another branch, which I will push when this one is merged\n. I just realized that some of my examples are segfaulting.\nWill fix this. Do you want another pull request for that or should I just append them here?\n. just corrected the segfaults, minor things\n. ready :)\n. by the way, there is a working example for a simple grid search now :)\nHowever, it still crashes for a larger number of parameter combinations, will check\n. added some bugfixes and other stuff\n. tried git merge --squash this time.\ndo you like it?\n. the example is more complex\nmodel selection of\nroot with\n    C1 with values: vector=[0.03125,0.0625,0.125,0.25,0.5,1,2,4,8,16,32]\n    C2 with values: vector=[0.03125,0.0625,0.125,0.25,0.5,1,2,4,8,16,32]\n    kernel:\"GaussianKernel\"\n        width with values: vector=[0.03125,0.0625,0.125,0.25,0.5,1,2,4,8,16,32]\n    kernel:\"PowerKernel\"\n        degree with values: vector=[2]\n        distance:\"MinkowskiMetric\"\n            k with values: vector=[1,2,3,4,5,6,7,8,9,10,11,12]\n. the example is more complex\nmodel selection of\nroot with\n    C1 with values: vector=[0.03125,0.0625,0.125,0.25,0.5,1,2,4,8,16,32]\n    C2 with values: vector=[0.03125,0.0625,0.125,0.25,0.5,1,2,4,8,16,32]\n    kernel:\"GaussianKernel\"\n        width with values: vector=[0.03125,0.0625,0.125,0.25,0.5,1,2,4,8,16,32]\n    kernel:\"PowerKernel\"\n        degree with values: vector=[2]\n        distance:\"MinkowskiMetric\"\n            k with values: vector=[1,2,3,4,5,6,7,8,9,10,11,12]\n. wow, already 175MB of memory for this example.\nok, going home, have a nice weekend!\n. wow, already 175MB of memory for this example.\nok, going home, have a nice weekend!\n. and extended pow function\n. and extended pow function\n. Machine now has interface for model storage\nKernel and Linear machines also work with this interface\nmodel storage is automatically activated when doing cross-validation and if a machine does not implement the store_model_features method (called automatically), an error message is thrown\n. Machine now has interface for model storage\nKernel and Linear machines also work with this interface\nmodel storage is automatically activated when doing cross-validation and if a machine does not implement the store_model_features method (called automatically), an error message is thrown\n. BTW: I think this is important, so I will do it before the registration with version number stuff\n. BTW: I think this is important, so I will do it before the registration with version number stuff\n. I just finished the possibility to extract names of parameters which are available for model selection\nalso added an example for this\n. I just finished the possibility to extract names of parameters which are available for model selection\nalso added an example for this\n. could you merge this?\nI have 5 more patches (divided stuff this time to have smaller pieces)\n. could you merge this?\nI have 5 more patches (divided stuff this time to have smaller pieces)\n. made the copy_subset method non-const (since it requires get_feature_vector which itself modifies the cache)\n. made the copy_subset method non-const (since it requires get_feature_vector which itself modifies the cache)\n. and I also included the copy_subset implementation for SparseFeatures\nalso iuncluded a test for it\n. and I also included the copy_subset implementation for SparseFeatures\nalso iuncluded a test for it\n. That were the last patches, will start on parameter version stuff now\n. That were the last patches, will start on parameter version stuff now\n. Also added VERSION_PARAMETER to versionstring.h\nThis is then added as variable in the Version class.\nThis variable is always written first in save_serializable.\n. Also added VERSION_PARAMETER to versionstring.h\nThis is then added as variable in the Version class.\nThis variable is always written first in save_serializable.\n. Currently, the parameter_version ids have to be incremented by hand by editing src/.version.sh\n. Currently, the parameter_version ids have to be incremented by hand by editing src/.version.sh\n. parameter version is now saved to files and being tried to load\nIf it does not exist, some warnings will appear along with suggestion to save with a more recent shogun version.\nNext step is the conversion stuff\n. parameter version is now saved to files and being tried to load\nIf it does not exist, some warnings will appear along with suggestion to save with a more recent shogun version.\nNext step is the conversion stuff\n. -Thanks :)\n-Yes, binary search is faster, but array has to be ordered, so inserting also becomes O(log n) instead of O(1), but ok then its applicable for larger datasets\n-The News file idea is nice, will do this\n-Yes, the conversion is a bit more complicated, will think of it today :)\nWill do the above stuff now and then begkin with translation\n. -Thanks :)\n-Yes, binary search is faster, but array has to be ordered, so inserting also becomes O(log n) instead of O(1), but ok then its applicable for larger datasets\n-The News file idea is nice, will do this\n-Yes, the conversion is a bit more complicated, will think of it today :)\nWill do the above stuff now and then begkin with translation\n. Just had another thought about the binary search stuff:\nIf the array has to be kept sorted, it has to be sorted after every insertion.\nSo, if an array is used, inserting has worst case O(n) costs. Then we have get: O(log n), insert O(n), which is worse than linear search and not sorting: get:O(n), insert O(1)\nThe alternative is to implement for example an AVL tree, which has O(log n) for insert and get, however this is a lot of work and since there will not be many parameters I thought it is ok to simply use the linear variant.\nAnother possibility would be a hashmap, which is also a lot of work to implement.\nI mean I could do this, but is it really needed?\n. Just had another thought about the binary search stuff:\nIf the array has to be kept sorted, it has to be sorted after every insertion.\nSo, if an array is used, inserting has worst case O(n) costs. Then we have get: O(log n), insert O(n), which is worse than linear search and not sorting: get:O(n), insert O(1)\nThe alternative is to implement for example an AVL tree, which has O(log n) for insert and get, however this is a lot of work and since there will not be many parameters I thought it is ok to simply use the linear variant.\nAnother possibility would be a hashmap, which is also a lot of work to implement.\nI mean I could do this, but is it really needed?\n. -base constructor calls added\n-added qsort and binary_search on arrays of pointers to CMath\n-ParameterMap now is based on a sorted array, example shows that it works\n-modified NEWS file and using this for parameter version extraction (perhaps you know a more elegant way to do this, I am not that much into sed magic :)\n. -base constructor calls added\n-added qsort and binary_search on arrays of pointers to CMath\n-ParameterMap now is based on a sorted array, example shows that it works\n-modified NEWS file and using this for parameter version extraction (perhaps you know a more elegant way to do this, I am not that much into sed magic :)\n. I also added the fix for SGMatrix\n. I also added the fix for SGMatrix\n. oh no, just found another bug, please not yet merge\n. oh no, just found another bug, please not yet merge\n. ok fixed, just forgot to add the new type to some switch cases\n. ok fixed, just forgot to add the new type to some switch cases\n. oh, and I added a test/example for the map. small yet\n. I got another patch ready, but I will wait until this one is merged :)\n. just extended and renamed one example (thought there was a bug)\n. just added handling of PT_SGOBJECT scalars in the load_file_parameter method\nalso added a test to the example.\nNow the only missing thing is a vector of PT_SGOBJECTs\nWhen this is done, I will start bringing my migration code from the sumer back to life. This patch fixes most of the problems (loading parameter from scratch) that prevented me from completing the migration stuff in the sumer so I hope it will be easy\nAnd now for some new years partying :) bye\n. argh, I dont know what I did here, but somehow I screwed my repo again.\nIs it possible to merge this? I got another new example in the pipeline\n. Its broken, re-opening\n. This kind of should work now.\nI am still unsure whether the errors in the test-suite are realted to migration bugs or not.\nRuns without mem-errors though\n. I lef this open with intention since currently, the things dont really work.\nAlthough the general framework does in principle\nI will focus on that as soon as I finish the GSoC project\n. I lef this open with intention since currently, the things dont really work.\nAlthough the general framework does in principle\nI will focus on that as soon as I finish the GSoC project\n. BTW i commented out some line in the python_modular example serialization_complex_example.py because it failed here (even before my changes)\nthis has to be fixed sometime, can I open issues for that here?\n. I have the another part ready, new parameters and dropping parameters, but will push later to make this patch not bigger\n. A note:\nThe type CT_SGVECTOR and CT_SG_MATRIX is technically not needed anymore. (it is not written to files anymore)\nHowever, in order to be able to read it, the CT_ types have to stay. If they are removed, any files written with 1.0 and 1.1 cannot be read anymore.\nI suggest to let in there for some time - then get rid of it\n. The motivation comes from the kernel machines\nIf you call train(SGVector indices), a kernel matrix has to be computed before, which then stays unchanged for all train(SGVector indices) calls. The locking is to exactly do this. Its like a preparation which has to be done before train(SGVector indices) can be called.\nNote that all the train_machine methods work on the kernel and the labels directly, so in order to only train on a subset, these have to be replaced by the ones for the correct indices (kernel by custom kernel, labels by label subset). In particular, they have to be restored when the multiple trainings on the fixed matrix are done. Therefore some kind of backup of the oiginal references is needed.\nIt could all be done in the train(SGVector inds) methods (check if a fixed kernel matrix exists, if not, compute it), however:\nThe backup of the labels has to be done for all machines, therefore I put it in the superclass\nImagine you want to fix a kernel matrix, train multiple times, then fix another kernel matrix and train. How would you do that? There must exist some kind of possibility to reset the fixed stuff.\nI think its nice to tell the machine: 1.) From now on data wont change 2.) train train train 3.) data may change again\n. Ive added the handling of this locked stuff in cross-validation.\nThere is an example which suggests that its faster. Kernel is only computed once.\nI also introduced subsets for the CustomKernel. This way, copying is not necessary which might be handy for large kernel matrices)\n. applied the stuff to example. nice :)\n. I am now using the lock based training for some more real-life examples. Especially for model selection with string kernels, its much faster. It also kind of blends nice into the current cross-validation. Just lock the model before and go for it.\n(Since its currently using apply(int32_t) it may be parallelized in the same way as apply() is for kernel machines.)\n. Test-suite now runs without segfaults/mem-errors :)\nThere are some errors though, but I dont know whether these are my fault. Do you?\n. That involves pretty much stuff in the migration suite\n. Hi, just saw this:\nI think you can get rid of this warning of LibLinear by using\nCLibLinear(L2R_L2LOSS_SVC)\n. I don't have this file classifier_multiclasslinearmachine.cpp, only the python example. Where did that come from?\nMmmmh, I am currently not really into this new multi class stuff, but what I know is that if you create a LibLinear instance with\nCLibLinear* svm=new CLibLinear(L2R_L2LOSS_SVC), this warning will not pop up.\nYou might want to take a look into the guide mentioned in the libliear faq for this:\nhttp://www.csie.ntu.edu.tw/~cjlin/papers/guide/guide.pdf\nappendix C\n. I see, it was in your request :) Sorry\nWhat if you do it without the DUAL? If your number of features is much smaller than the number of examples, working on the primal SVM term is faster than the dual.\n. Nice :)\n. Heres the code:\nhttps://gist.github.com/2346698\n. I have added an example that can be used to reproduce the error under libshogun/classifier_mkl_svmlight_modelselection_bug.cpp\nSee comments how to reproduce. MKL Modelselection works with LibSVM but not with SVMLight. See also above comment. Some MKL person has to fix this one.\n. Mate, pls dont post the same message 5 times at random locations, use the mailing list. Nice patch!\nI basically agree with Soeren in all points :)\n(Enums, License, Overloading)\nYour get_name problem is probably because you didn't re-define get_name as virtual method in the subclass - I had that problem a few times at least.\nAbout CMap: I also had some problems with it (some methods also dont work) perhaps you could fill in a bug report - things might get lost if you dont. Also, you could fix it :)\nAbout the ranges for the parameters. Yes that is kind of a design flaw in the registration of the parameters.\nCurrently, there is no way specifying ranges when you implement a class. We could add that. Soeren, what do you think?\nOn the other hand, users should be capable of choosing min/max values themselves and we should let them the controll over it. I could imagine a system where default values might be specified for numerical parameters and users may overwrite them using the ModelSelectionParameter tree. What do you two think? For now, I think we should use the min/max values from the tree since that is more intuitive to use than hard defaults.\nDid you (valgrind) run all the old model-selection examples? That would be important before merging since you changed the Parameter classes.\nIf yes and the stuff from my comments is ok, I would say, lets merge and have a closer look then.\n. I like the idea of hashing for parameter!\nHowever, I would modularise everything a bit: The most basic instance one can hash is the TParameter. I would add a hashing method there. Then you can iterator over all elements of the TParameter array in the Parameter class and ask for the hash, and append them (add a new method for this in Parameter) This is all a bit hard to overview in this case\nAnother thought I have is the close relation to serialisation. You create a copy of the Parameter and then hash the char interpretation of it. Serialisation does the same thing but saves to disc.\nIt might be a bit of a re-invention of the wheel to implement the hashing separately.\nAnother thing is that this is memory expensive since you create a complete copy of the objects in memory. Imagine large matrices. And then the speed. If you ask for a hash in every iteration of whatsoever, it might be slow because of all this copying per element.\nWouldnt it be better to hash in-place? Otherwise there is no difference to just hashing the ascii elements in the files from serialisation.\nBut that would need a complete difference approach, you would have to create hashes of all single parameters instead of appending them, and then recombine them to new hashes.\n. Great! I like this hashing approach. Its now fast and doesnt cost any memory. Also the code is nice to read.\nPlease change all the mentioned things, then I'll merge\n. Man, this sounds weird -- but I dont think it is a problem of you OS/Hardware.\nI had many cases of these strange looking bugs where it turned out to be an implementation error in the end. Especially with these parameter framework things.\nRead errors may sound not so important, but we still have to get rid of any before merging. Otherwise it gets harder to track them.\nTry to reproduce problems in the simplest possible way, then you will find the problems. If it totally doesnt work, we still can merge and search together.\nBTW. I had a few memery reading errors with CMap too, so there might be an error in there.\nBesides: I think its good to have this new hashing thing. Sounds exactly as made for us!\nFor the HashedFeatures, ask S\u00f6ren. If we break compatibility between old versions of shogun and the new one, I think its fine because its an all new-version anyway. But thats just my opinion.\n. I just saw this:\nYes, scalar parameters store a pointer to the data, all non-scalar and SG_object based store pointer on pointer to data.\nThis makes things particulary complicated if you want to create new parameters, since you have to allocate memory for pointer to data, but thats a different story :)\n. I just saw this:\nYes, scalar parameters store a pointer to the data, all non-scalar and SG_object based store pointer on pointer to data.\nThis makes things particulary complicated if you want to create new parameters, since you have to allocate memory for pointer to data, but thats a different story :)\n. Should be fixed now. Could you please confirm that?\n. Should be fixed now. Could you please confirm that?\n. again: Should be fixed now. See evaluation_cross_validation_multiclass_mkl.cpp example\n. again: Should be fixed now. See evaluation_cross_validation_multiclass_mkl.cpp example\n. I like ARD :)\n. I like ARD :)\n. 1.) I think these checks hurt nobody\n2.) Yes I think inheritance might be good here, reduces code and therefore error possibilities\n3.) not yet, I will think of it \n4.) same, lets talk about this soon\n5.) dont really get what you are asking here :)\n. 1.) I think these checks hurt nobody\n2.) Yes I think inheritance might be good here, reduces code and therefore error possibilities\n3.) not yet, I will think of it \n4.) same, lets talk about this soon\n5.) dont really get what you are asking here :)\n. I think its nice!\n. I think its nice!\n. Will work on that soon\nOn Sep 13, 2012 6:44 PM, \"Sergey Lisitsyn\" notifications@github.com wrote:\n\nIs that true still?\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/shogun-toolbox/shogun/issues/660#issuecomment-8536648.\n. Need to fix vw examples, then this can be closed.\n. still working with streaming features and hope to find the time for a test soon - as things are coming slowly together\n. All streaming bugs/leaks I menionted before are all fixed.\n\nHowever, the libshogun examples\nstreaming_vwfeatures.cpp\nstreaming_vowpalwabbit.cpp\ncrash with a null pointer exception and\nstreaming_stringfeatures.cpp\nstreaming_onlinesvmsgd.cpp\nstreaming_onlineliblinear.cpp\ndo not even compile.\nThey are commented out from the Makefile. I will kindly ask Shashwat Lal Das to fix this :)\nOr someone else would have to take the pain of going into this. The longer we wait, the more difficult it will get I guess.\n. There was another bug report on this recently:\nhttps://gist.github.com/karlnapf/5502520\nEnd with a double free corruption\n. There was another bug report on this recently:\nhttps://gist.github.com/karlnapf/5502520\nEnd with a double free corruption\n. I like that!\nA few thoughts: I think the test cases should be documented: This can be read form code in this case, but in more complicated ones, maybe a few lines comments would help. I usually dont like global variables, since I think local ones make code easier to read.\nAlso, I think its usually a good idea to use random data in numerical methods, and to compute ground truth-results by hand in the test case. This way there is a lower probability of forgetting a case. But this is not possible for all tests -- only for easy ones.\nUsage now is: make unit-tests? right?\nWhat happens if a test fails? I think gtest still continues and collects all errors?\nYou get an OK from me for this!\nAs for the abstract classes: I dont know much about mocking, but yes, it would be nice to test abstract classes themselves! Would that be hard to include?\n. I like that!\nA few thoughts: I think the test cases should be documented: This can be read form code in this case, but in more complicated ones, maybe a few lines comments would help. I usually dont like global variables, since I think local ones make code easier to read.\nAlso, I think its usually a good idea to use random data in numerical methods, and to compute ground truth-results by hand in the test case. This way there is a lower probability of forgetting a case. But this is not possible for all tests -- only for easy ones.\nUsage now is: make unit-tests? right?\nWhat happens if a test fails? I think gtest still continues and collects all errors?\nYou get an OK from me for this!\nAs for the abstract classes: I dont know much about mocking, but yes, it would be nice to test abstract classes themselves! Would that be hard to include?\n. Oh, and also it would be nice to actually see a test for a class method. But I think that should be straight-forward.\n. Oh, and also it would be nice to actually see a test for a class method. But I think that should be straight-forward.\n. OK to merge from me!\nSonney2k, blackburn?\n. Why do the tests fail if I change a class?\nBut I see the problem - what about having them in the shogun-repo? Are there any problems with that?\n. Yeah, this extra overhead sucks.\nSo I would say no submodule then. What do Soeren and blackburn say about this?\n. S\u00f6ren, Sergey? Opinions?\n. GO! :)\nWe need a framework for that\n. LOL, finally this is corrected, I wanted to do it but always postponed :)\n. Hi Sergey, this is the method for binary SVM labels and their scores.\nIt yet has to be integrated better. It currently the values from m_confidences and uses their sign as label. Then, the values in confidences are replaced by calibrated probabilities.\nI will send a way to use that for multiclass soon. Maybe you can have a thought on how to integrate?\n. Not worked on it yet\nOn Nov 2, 2012 7:05 AM, \"Soeren Sonnenburg\" notifications@github.com\nwrote:\n\nany news on this?\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/shogun-toolbox/shogun/issues/781#issuecomment-10006486.\n. Note: When adding new serialization parameters, we have to take care of the parameter mappings from version to version. Basically, we need to tell shogun that a parameter is new in a certain version.\n\nTalk to me on IRC for this. An Example is in the paramter registration of KernelMachine.cpp.\n. Postponed for now due to bug in parameter/serialization framework. Please ignore for now\nFirst, #1031 has to be solved\n. We are totally open for parallel algorithms - assuming that you mean single-machine parallel. Preferably with openmp\nWhat method do you have in mind?\nBTW: We aim to use this on large (and sparse) graphs (>100000 nodes and more)\n. Hi\nWell, I was sleeping :)\nFirst of all, do you have a description of the method?\nThe complexity sounds exactly as what we want - how does it depend on the number of colors?\nWe merge code with pull requests on github, so you have to fork our repository, pull it to your computer, modify locally, commit locally, push, and send us a pull request. \nI suggest you create a new class CSparseGraphColoring in the statistics folder of shogun. See any other shogun class or the development readme on how to do this. The next steps are unit-tests, an example, and documentation.\nYou can ask for details on IRC (heiko)\n. Hi\nthat sounds good.\nSo grab a copy of shogun and send a pull request on github.\nAlso, I would love to read a paper on the algorithm - could you provide one?\nHeiko\n. Agreed!\n. closing as we are now using colpack\n. @Froskekongen yeah that is true, but what would be the alternatives? This one at least works for now. What are you doing in KRYLSTAT?\n. Nice, thanks for that.\n@lambday feel free to have a look at this. It's not top priority now (since the other stuff works - at least with reasonably conditioned matrices), but it might be really useful to continue on this in a bit.\n. I think this should be fine. Closing.\nHow did you find the mistake? Would be good to have a unit test that failed before and works after the commit\n. Where are the unit tests ? :)\n. What about analytically solving things for toy examples?\nOn 28 Feb 2013 19:57, \"Sergey Lisitsyn\" notifications@github.com wrote:\n\nOkay well at least LLE, LTSA, NPE and Isomap can be tested by checking\nneighborhood of embedded points - they should have the same neighborhood as\nin original space.\nSee\nhttps://github.com/shogun-toolbox/shogun/blob/master/tests/unit/converter/MultidimensionalScaling_unittest.ccas an example of unit-test for converter (it tests distance preserving,\ni.e. that pairwise distances stayed similar after reducting dimensionality)\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/shogun-toolbox/shogun/pull/894#issuecomment-14253917\n.\n. Hi\nwho is supposed to review/merge this?\nI suggest to add some unit tests that assert basic functionality\n. Yes,\n\ncurrently our SVM class has two C parameters, one for left and one for right.\nUpon searching for best C-parameters, this has the annyoing consequence that we have to search for pairs of C1/C2. However, in practice, one usually just wants to have the same C for both sides of the hyperplane.\nCurrently, we cannot tell the modelselection framework to do this since C1/C2 are represented as two scalars.\nSo the idea is to replace the C parameter by a vector and then handle it in the code based on its length.\n1-element single C\n2-elements C1/C2 etc\nThen the modelselection framework can tune parameters based on this. In order to work with vectors this needs slight modification, however, we could start with the having one fixed C scenario, which should be easy.\nThis task is non-trivial, but with some commitment also possible  for someone who is good in reading/understanding c++\n. Also added an example to reproduce the mkl model selection issue\n. agreed!\n. Fixed classic Gp regression\n. Correction: It is not the parameter hashing framework.\nResults are still crap\n. fixed the classic GP regression, there were numerous bugs that caused wrong results (always) and chrashes (sometimes). Now partly uses Eigen3.\nThe other examples should be tested too at some point!\n. LaplacianInferenceMethod is done :)\n. Next step: python modular examples that also serve as integration tests\nFor this, an example has to be created similar to the exact regression one. Then integration test data is created with tests/integration/generator.py and the example filename as argument. This generates a number of files in the data dir in test. Those files have to be committed to the data repo, then the data version of the development repo has to be updated. Its a bit tricky, ask in IRC if unclear.\n. @votjakovr whats the status?\n. Thanks!\n. Thanks!\n. The integration test recently failed. preprocessor_kernelpca_modular in python modular\nThis means that the projection changed from when the test was created.\nUnfortunately, the example does not use sensible data which a human could check visually.\nI recently created such an example and noticed that the projection did not look as I expected it to. Since I was in a hurry, I investigated no further.\nIf you could write a simple but meaningful example in python that demonstrates that KPCA works as it should, we could update the old integration test. More importantly, if you could write a unit test for KPCA which for example checks the projection of a small toy example to be correct, this would be even better.\n. No, no.\n1.) Create an example for kernelPCA which illustrates the algorithm on meaningful data. This means one should see (if plotted) what the method does. If this is reasonable, the integration test should be updated (ask for that its easy). Challenge here is to create a nice example since for this you have to somehow understand kernel PCA\n2.) Create a unit test in c++. This means you should create a simple test case for which you can compute the results somehow else (best is another implementation which you trust, or derived by hand even better). Then assert the results of shogun's kernel PCA against this reference.\n. don't worry about the integration test for now. It is just this: Once the test works, make sure it is reproducible, then save results to a file (we have a script for this), then everytime the example is run, it is checked that the results match with those in the file. But this all comes after the example works.\n. Hi Cameron, thanks for the report!\nYou are indeed right that there is a typo! Feel free to fix it (this will break the integration test, whose data has to be re-created also.\nWe have some basic unit-tests that compare the output of the exact inference method with the GPML Matlab implementation. If you found a case where they are not equal, this is really useful for us!\nCould you give some more details on the negative numbers? If you found a bug, please provide some code to reproduce it, and/or even better, fix it :)\n. In order to set up gtest, just download it and pass the folder to configure (--gtest= ...) when compiling shogun\n. You have found a bug!\n. Hi Cameron,\nsorry for the late reply here ( I currently lost a bit track due to all\nthese GSoC mails :)\nI agree its kind of bad that the original scripts are not included.\nHowever, I currently don't really have a solution for this problem. What\nyou always can do is just reproduce the data and run GPML with the same\nmethods (thats what we do). Since unit tests are supposed to be simple, it\nshould be easy to reproduce in GPML toolbox.\nCovariance functions are just kernels in shogun. CGaussianKernel is the\nsquared exponential kernel (parameters are slightly different but easy to\nsync)\nhttps://github.com/shogun-toolbox/shogun/issues/1022\nBTW see, I don't know if I showed you this already. It would be great if\nyou found it what's going on here. And yes, it would be great to include\nthe 95% confidence interval in the example - even better would be a heatmap\nsimilar to the attached.\nTo set up gtest/gmock, download the libs and add their path to shoguns\nconfigure script (--gtest=bla) I just realised that I already wrote you\nthis, but shouldn't harm! :)\nYou can always get in touch on IRC if anything is unclear!\nBest and thanks!\nHeiko\n2013/4/25 goldbug notifications@github.com\n\nHi Heiko,\nThanks for your reply! Here's the code\n=> line 64 prints out the negative numbers for the covariance\nhttp://pastebin.com/QnKim9Db\nAlso, there's a question about the GP regression in shogun. In the GPML\ntoolbox, you are allowed to pick different covariance and likelihood and\nmean function. I can see there is a choice for likelihood in shogun. But,\nthe covariance definition is not as clear as GPML.\nIf you check out the gpml toolbox documentation, I am trying to add the\ngreay area that signifies the 99% confidence interval, but the covariance\nreturned from shogun is troublesome.\nhttp://www.gaussianprocess.org/gpml/code/matlab/doc/\nAlso, I have checked out the unit tests. They look good, but they are\nfairly simple and the original octave or matlab script is not provided, so\ni can't reproduce the results in octave or matlab. The same problem occured\nthat I don't know what the covariance function did the original author\npicked. In the future, I believe it would be great to use data files to\nsave results from gpml and also to record how did the creator generate\nresults from GPML.\nCameron\nP.S. Trying to set up gTest, but failed. :(\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/shogun-toolbox/shogun/issues/948#issuecomment-17016630\n.\n. That is very good!\nStill is some work to integrate into shogun properly.\nI suppose your are Erlend?\n. Hi!, better pick another one, @ylyhlh. There are plenty :)\n. I recommend any issue that is related to your project. If there are none,\ntry something generic, we got many. Examples are always good. You should\nalso ask the mentor of your project. Maybe some initials parts of the gsoc\nproject?\nKeep in mind that its quite late to start sending patches...\nBest, Heiko\nOn 17 May 2013 12:57, \"Hao Liu\" notifications@github.com wrote:\n@karlnapf https://github.com/karlnapf Sure, no problem. But could you\nplease recommend some possible issues I can help?\nThis is my proposal.\nhttp://www.google-melange.com/gsoc/proposal/review/google/gsoc2013/ylyhlh/4001\nI do appreciate your help a lot.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/shogun-toolbox/shogun/issues/949#issuecomment-18057403\n.\n. Done by @lambday \n. As I just found out, this has already been done :) Sorry for any confusion. Closing\n. Nice! Thanks\n. Hey,\n\nthe python modular test segfaults after this patch.\nCould you please fix it?\n. Indeed :)\n. Thanks for the info :)\nSo then this should be about checking whether arpack scales up to very large matrices with bad condition numbers. I had some bad experiences with Matlab's eigs (which calls ARPACK) in terms of memory requirements.\n. That sounds good! Closing this one for now\n. We use eigen3's LLT class for this\n. yes, exactly\n. This one:\nhttp://www.shogun-toolbox.org/doc/en/current/classshogun_1_1CExactInferenceMethod.html\n. Yes, will check.\nSorry for the long delay - family Easter business\n. Looks good! Send the PR\nOh and our convention is\nfor (...)\n{\n...\n}\n. Implemented. Closing\n. @lambday BTW you should add a little note in the features part of the NEWS file\n. yes\n. This is in eigen. Closing\n. Hi\n1.) SGSparseMatrix. In particular, @lambday added converting methods to the eigen3.cpp\n2.) Yes, a flag should determine whether you sample N(mu,C) or N(mu,C^(-1))\nNote that @lambday is already working on this. Maybe you want to do another issue to avoid frustration when one of you guys commits and the other's work is discarded?\n. Implemented :) Closing\n. Please separate things in the future - string kernel is not related to the other stuff. I would actually prefer if you could remove it for now and submit a bit later. Commented anyway. For the Kernel you also have to add the type in Kernel.h\nBut lets focus on the logdet for now. Could you add a few unit tests. Make sure you check their memory usage with valgrind.\n. Currently on the road. Will check tonight!\nOn 28 Mar 2013 02:16, \"Soumyajit De\" notifications@github.com wrote:\n\nI have removed all the previous messy commits. Added a unit test, with two\ntest-cases. One using a simple matrix, and another with a randomly\ngenerated positive definite matrix. Tested with make unit-tests, make\nvalgrind, and make valgrind-per-module. Please check. Sorry for all the\nmess :(\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/shogun-toolbox/shogun/pull/956#issuecomment-15564671\n.\n. Ok, well done!\n. yes, always listen to blackburn - serious programmer :)\n. lambday, yes I think this would be good to have.\n@lisitsyn? do you agree?\n. @lambday any news on the permutation?\n. nice!\n. Hi!\nNo there is nothing yet, but this is very easy to do. Just eg. create a 2d classification example and run LibSVM on it. Then map the probabilities and make sure they make sense in terms of how far points are from the decision plane. You could even use a linear kernel which makes things a bit easier to overview.\nSee the classification libSVM example for inspiration, but please use a more meaningful dataset.\n. Cool!\nPlease see comments in PR\n. Closing since all tasks done (almost)\n. nice work!\nApart from the comments, please dont use whitespaces between operators in c++ (apart from logical, so a && b)\nAnd the transpose for the samples. Then I'll merge\n. That should be alright. Currently at a workshop, will review the Pr tonight\nor so. You can add the precision version and (separate) test if you want\nH\nOn 13 Apr 2013 11:42, \"Soumyajit De\" notifications@github.com wrote:\n@karlnapf https://github.com/karlnapf tested with both precision_matrix\ntrue and false. Interestingly, the difference in norm between the\ncovariance matrix and sample covariance matrix is more where\nprecision_matrix=false (default). When it uses the inverse, the difference\nis really low.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/shogun-toolbox/shogun/pull/971#issuecomment-16331057\n.\n. Nice so far, a few things:\n\n-Never invert matrices - solve systems instead\n-Transposed stuff often causes bugs, one got to be careful there :)\n-do not compare matrix norms, rather make sure the norm of the difference of the matrices is small. I can always give you a matrix with the same norm where all elements are different.\n. Wait a bit with the sparse until this one works.\nBut we are getting there :)\nAll these issues are actually very useful to get aware of now since those will pop up for the GSoC project\n. Try more samples. The code is fine, just for this high dimension, the empirical covariance converges slowly.\nwith N=100000, I get 0.064\n. It also helps getting a feeling for norms, estimates etc. So smile and keep going :)\n. Ok very nice. I'll merge. This is good work!\n. Looks good, will check the leaks after merging\n. Okay. Its very good that you check this! If you need some comments, send a PR.\n. This was the reason? :)\nThanks for fixing!\n. Yes, I think this is not necessary, but it doesn't harm right?\n. Very nice! Just a few comments, which you can change within next PR\n. Thanks!\n. Nice work!\nCould you change the filename to match the existing ones?`Like classifier_libsvm_probabilities.cpp\nAlso, please add an entry to the Makefile in the examples directory\nAnd very important, run with valgrind to ensure there are not memory errors/leaks\nThe next step would be to do this in python (should be easy)\n. Open the Makefile. Add the name of your program to the list. Test it by\nexecuting make. If still in doubt. Ask in IRC.\nOn 19 Apr 2013 19:22, \"abinashpanda\" notifications@github.com wrote:\n\nHello,\nCould you please explain me how to add an entry to the Makefile in the\nexamples directory ?\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/shogun-toolbox/shogun/pull/987#issuecomment-16671636\n.\n. Hi!\n\nYou added the Makefile in the wrong directory. Instread, just add one line to the existing makefile\n. Ok, nice, ready to be merged soon.\nSee the minor comments and please run the example with valgrind and give a note whether its memory clean\n. Great!  Thanks and good work.\n. Looks good! I will merge once the thing is outsourced to eigen3.bla\n. Nice!\nI'll merge\n. Nice!\n@lisitsyn Could you have a look regarding the algorithm?\n. First of all, sorry for the delay in the feedback, I have been offline for a while.\nVery nice!\nSee the minor comments. \nAlso I would like to see multiple strategies for estimating the probabilities. You don't have to imlement all of them, but please provide the possibility to include multiple ones.\nVery good that you added an example. Have you checked it with valgrind?\nAlso, please add to Makefile so that it is executed on make tests\nOne thing that is missing for merging is a unit-test for your multiclass-confidence method.\nPlease create some simple multiclass scores by hand and assert correct results in the unit test\nSee the tests/unit/labels/ directory on how to create unit tests. Running make in this directory or make unit-tests in the main directory runs them.\nPlease ask if any anything unclear. And again, nice work! :)\n. nice! :)\nYou can just send a new one on develop.\n For compilation time, consider using ccache\nBest!\nHeiko\n2013/4/22 Shell Hu notifications@github.com\n\nThanks for the comments! Currently I only check the example I created,\nwhich is correct. I'll do valgrind and unit-tests ASAP. The compilation on\nmy Mac is really slow, more than 20 mins :/ I'll setup a new shogun\nenvironment on a desktop. Then everything should be finished soon. BTW, how\ncan I update the PR? since it's been closed. I updated something on my\nbranch ae87434https://github.com/shogun-toolbox/shogun/commit/ae874347776b1f77d6c17e620a6e4ac29af6ef14\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/shogun-toolbox/shogun/pull/991#issuecomment-16767330\n.\n. Ehm, something went wrong with git. (You probably have forgotten to rebase against develop)\nDo you see all the commits by other people? These should not be in there. Its hard to see what you changed this way\nPlease clean up and send another PR with only your changes. If unclear how to do that, ask in IRC\n. Very nice work!\nHave you checked also with make-valgrind to see if the tests are memory clean?\n. Cool! merging\n. I don't get why you send two PRs?\n\nThe python_modular example should be without plots, the graphical one with plots included.\n. Hi deepak,\nsorry if I confused you. So we need two files, one in modular, one in graphical. without and with plots respectively.\n. Both files need the data.\nOne example should just be with plots and one without\n. Put it to the tools folder and then include from both graphical and modular\nexample\nActually, it might be a good idea to add this to the DataGenerator class of\nshogun, but this can be done later\n2013/4/23 Deepak notifications@github.com\n\njust one last question ,the KPCA code imports the circular_data (which\nwill go in the graphical folder).so i should not inculde that in the\nmodular folder?right?\nIn nutshell: shogun / examples / undocumented / python_modular ---->The\nKPCA CODE: preprocessor_kpca.py\nshogun / examples / undocumented / python_modular / graphical ----->The\ncircular input data: preprocessor_kpca_graphical.py\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/shogun-toolbox/shogun/pull/1003#issuecomment-16849369\n.\n. examples/undocumented/python_modular/tools\n\n2013/4/23 Deepak notifications@github.com\n\nCould you specify which tools folder?I m unable to find one in the\nshogun's repository\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/shogun-toolbox/shogun/pull/1003#issuecomment-16851255\n.\n. Very nicely spotted btw :)\n. ok, nice! merging\n. This is great work! :)\n. Ok nice, so now the files are correct.\n\nSee my other comments on coding style and some minor changes\n. Hi!\nYes, partly because someone messed with the unit-tests.\nAnother part is due to your example change:\nEvery python modular example acts as an integration test. The stuff that is returned in your example is being compared to a file. Since you changed the example, the results are not the same anymore.\nWe will get rid of this by updating the file and submitting this to the shogun repository in addition to your new code, however, I would like to see a few additional changes to happen in terms of coding style, will give comments soon\n. Okay, we are slowly getting there :) Keep on!\nOnce all the comments are changed, we can start to change the integration test data so that travis builds work.\n. First of all, thanks for the PR :)\nA few general comments:\nPlease move the probability support flag to the multiclass strategy, makes no sense to have it in the machine and makes no sense to be able to set it by hand.\nPlease add support to handle multiple ways of computing probabilities (there exist many ways for OvO and OvA), handle this via a flag and switch over it in the rescale method of multiclass\nVery good that you added unit tests and examples, however, they broke the travis build. Always run make tests and make unit-tests before you submit anything. Make sure that your example are clear according to valgrind\nLet us know if you have questions.\n. Hi\nno worries, sometimes things are a bit tricky :)\nI like the idea of having a class that implements all the different methods.\nI would rather choose a different name, such as CMulticlassProbabilityHeuristics\nYes you broke the existing test somehow, multiclass things are always tricky in shogun :)\n. Hi!\nVery nice work!\nI am currently on the run, but will comment on some minor things later today - ready to merge soon!\n. Some general notes:\nPlease document all changes and the new framework in the class doxygen comments of all involved classes.\nDescribe it once somewhere and then reference to the class where the documentation is\nWe want people to know about this\nAlso please add your name in the headers of your modified files as:\nWritten (W) 2013 Heiko Strathmann\n. Very nice work!\nMany comments from my side, but most is minor - should be ready to merge soon!\n. Check CStatistics::fit_sigmoid and new parameters of\nCBinaryLabels::scores_to_probabilities Currently in a car, will get back\nlater today.\nOn 3 May 2013 04:04, \"Shell Hu\" notifications@github.com wrote:\n\nHi Heiko,\nCould you add the part dealing with the A and B in\nscores_to_probabilities() ? Or just tell me a little bit how to make it. I\nremember you mentioned using CStatistics, but I still have no idea what is\nthat.\nMy idea is changing the return type of the scores_to_probabilities\nfunction, so every time I call the function, I can store the A and B\neasily. I am not sure if this is a good solution.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/shogun-toolbox/shogun/pull/1017#issuecomment-17376603\n.\n. Check CStatistics::fit_sigmoid and new parameters of\nCBinaryLabels::scores_to_probabilities Currently in a car, will get back\nlater today.\nOn 3 May 2013 04:04, \"Shell Hu\" notifications@github.com wrote:\nHi Heiko,\nCould you add the part dealing with the A and B in\nscores_to_probabilities() ? Or just tell me a little bit how to make it. I\nremember you mentioned using CStatistics, but I still have no idea what is\nthat.\nMy idea is changing the return type of the scores_to_probabilities\nfunction, so every time I call the function, I can store the A and B\neasily. I am not sure if this is a good solution.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/shogun-toolbox/shogun/pull/1017#issuecomment-17376603\n.\n. Agreed :)\nOn 3 May 2013 07:05, \"Shell Hu\" notifications@github.com wrote:\nI know why I couldn't understand you. I haven't fetch upstream for a long\ntime. Great changes! Now the fitting becomes independent of the svm\nmachine. I'll update the code. If possible, let's merge the code tomorrow.\nThis PR seems being alive too long.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/shogun-toolbox/shogun/pull/1017#issuecomment-17379793\n.\n. Agreed :)\nOn 3 May 2013 07:05, \"Shell Hu\" notifications@github.com wrote:\nI know why I couldn't understand you. I haven't fetch upstream for a long\ntime. Great changes! Now the fitting becomes independent of the svm\nmachine. I'll update the code. If possible, let's merge the code tomorrow.\nThis PR seems being alive too long.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/shogun-toolbox/shogun/pull/1017#issuecomment-17379793\n.\n. Okay, looking good :)\nI still dont relly like the references to the SGVectors though.\n\nAnd one last thing is a unit test for every heuristic on a simple toy example where you assert against another implementation (by hand or similar)\nIf you dont have another implementation, add sanity checks. One thing you can at least to is for example to assert that the output label corresponds to the largest probability. Another thing is to assert that the ordering of the outputs does not change (I am currently not sure whether this is the case, but would be good to know anyway)\nI feel bad for adding more and more stuff to this patch, but I am really excited about how it looks already. We just need to do all this at some point, and the best is now. Will also be useful for many people.\n. Ah and on the valgrind issue, this comes from the SG_ADD calls :)\n. Hi!\nSorry for my long delay, I was in Germany without a computer.\n@hushell Don't worry about patience, I prefer high quality patches (as yours is) over the speed. Also, as @sonney2k mentioned, its the first one that takes time due to all the stuff we care about but mention nowhere.\nI will check the latest changes now and comment.\nPS:\n@sonney2k so I am almost as strict as you were when I joined :D\n. Totally happy with this. Nice work!\nCould you mention this extension in the NEWS file? People will rave about it :)\n. Hi goldbug, \nThanks!\ncould you send the PR against develop?\n. Hi!\nI currently cannot remember if I already told you, but the example change makes the travis build fail.\nThis is since you change the values that are returned by the python function. This is caused since you corrected it - the integration test stores the wrong numbers.\nPlease update the integration test - ask on IRC if unsure how to do this\nBest!\n. Okay, you can update the integration test now\n. nice, lets see whether it works\nBTW also try to develop in the development branch locally\n. nice, lets see whether it works\nBTW also try to develop in the development branch locally\n. solved by @votjakovr\n. @pamrash what exactly is the problem?\n. Thanks!\nWhy is eigen3 slow in SVD? Any evidence/information on that?\nYou should add a unit-test that makes sure the changed parts do produce results that make sense.\nOtherwise, changes might slip thorugh our hands. You probably did check for correct results already - so just make this explicit via a test\n. Without some kind of guarantee that your changes do not change results of the method, we cannot merge this yet\nGood luck with your exam :)\n. Without some kind of guarantee that your changes do not change results of the method, we cannot merge this yet\nGood luck with your exam :)\n. Hi\nWe currently cannot merge, you have to rebase your code.\nOtherwise its nice work!\n@pickle27 \n@monalisag \nOne thing, please don't use the DataGenerator class, its outdated. You can sample Gaussian with CStatistics::sample_from_gaussian\n. Thanks!\nOk seems fine (except integration test fails, but thats since internals changes, I will update the test once its merged)\nOnce you changed the DataGenerator, we can merge\n. Very very nice demo!\nMight even be added to the shogun-tutorial at some point.\nI agree with Fernando. Please change and then I will merge\n. please resend against develop branch rather than master\n. please resend against develop branch rather than master\n. ehm, yes, thanks :)\n. ehm, yes, thanks :)\n. Nice thanks!\nSo if the variance now changed, the regression python example should fail.\nIt doesn't - so what has changed?\nAlso please provide a unit test that ensures functionality of the modified part whenever to fix a bug\n. I see!\nSo the unit test worked before. If there was a bug, this means that it did not check the bug. Could you add a check for the particular thing that you fixed?\nThe python example will be changed afterwards.\nThanks! :)\n. nice work!\n. Hi Deepack,\nalmost! Now commit data in the main repository. This is just the current data version, which has to be updated since you changed it\n. Nice!\n. Nice!\n. note you should develop in the dev branch locally too\n. note you should develop in the dev branch locally too\n. The data commit is still missing,\ndo a commit in the development branch and select data\n. Let me do the data generation.\nJust re-submit only the python files and the (corrected) description and I will merge and do the data myself\n. Good point adding that!\nSomething is wrong with your newlines, could you change that? (See diff)\n. Good point adding that!\nSomething is wrong with your newlines, could you change that? (See diff)\n. ok then! :)\n. ok then! :)\n. closing since implemented\n. implemented by @van51 \n. There are two things here: algorithmic changes/approximations and how they affect the solution you are after (dividing the optimisation, etc). You would have to do the math for that, but I guess something can be done here.\nFrom an implementation side of view, Shogun currently lacks a proper framework for doing organised multicore or multimachine calculations -- we do most of the parallelisation using openmp or even pthreads directly in the algorithm implementation, which mixes up algorithm design and technical multicore details and thus makes it hard to parallelise algorithms systematically in Shogun.\nAny input here is highly appreciated\n. This is great!\nKeep us posted. It would be amazing to parallelise some of Shoguns algorithms, or work on a general framework for that. We could give you support with the library internals if you want to work on such things\n. Thanks indeed!\n@sonney2k sure to merge this in master?\n. Hi!\nThere should be added a unit test for the old implementation first (Some simple test that ensures the results make sense). Then we can find out whether the new implementation changes results, which is kind of important upon those type of changes.\n. Hi,\nsorry for the late reply, was travelling.\nThe build tests test 1-to-1 matching with files, so binary comparison of all the bits.\nIf you move to eigen3, the 12th digit might be different and this can cause the test to fail.\nA unit test is therefore the best way to check whether your changes make sense.\nBest way is to write a test for the old method, then make sure that it works with your re-implementation.\nLet me know if you have problems \n. what exactly is going on here?\nNote that you have to rebase against develop branch when you update your local repository.\nAlso, please send PRs against develop branch\n. Hi Roman,\nonce again, great work! I like that the framework is now fully based on eigen3 :)\nOne comment: If you change methods (like FITC or the likelihoods) please always add a unit test before you do that. Otherwise, we cannot check whether something changed. Minor comment since I expect these will follow shortly\n. I agree with @sonney2k here\nI meant to always make sure to monitor whether results change when one changes an implementation of an algorithm\n. We don't want to do this anymore. Hi,\nthanks a lot for the patch!\nThe code is a little complicated to read (these kind of problems can be solved in a more elegant way if done recursively). However, if it works, it doesnt really matter.\nMy first question: Does it work? Could you create an example and post the output of it? This has to be done anyway at some point for illustration and also for unit testing.\nSecond question: Is it memory clean? For that you will have to run your example in valgrind.\nI will also comment on some minor style issues in the code\n. Okay, nice! Looking forward to your reply\n. Hi!\nSorry for the long delay, was travelling.\nI am fine with the way you do it. Nice patch! :)\nAlso thanks for resolving the memory leak \n. @van51 while I wanted to write an example showing how to use this for modelselection, I realised that you change the order of the kernels.\nSo if the input kernel list is (a, b), then the output list's combined kernels should have subkernels in the same order (a[i],b[j]). Otherwise it gets confusing.\nI will shortly add the example with the way its done currently, so you will have to change the order in there (just two sub-kernels) or it will crash\nCould you add a note to the documentation of the method that the caller gets a list which does ref-counting\nThanks!\nHeiko\n. Here is the example that you will have to change once the ordering is fine\nhttps://github.com/shogun-toolbox/shogun/pull/1066\nShould be just 4 lines (exchange 0 and 1 in kernel index)\n. Hi!\nSorry for the delay, was travelling.\nThanks!\nLet me know once everything is unit-tested, we should be close by now! :)\n. As already mentioned, I will do the data commits for you, just give us three files: modular, graphical, and description.\nAnd please send the PR against develop rather than master.\nThanks!\n. Awesome, thanks! :)\n. Very nice!\nSo next step would be indeed kernel width selection and noise parameter selection. This is the set_sigma function of the likelihood. Should be adjustable by the gui also.\nParameter selection should be working via the ML2 gradient descent on the marginal likelihood\nBTW if you do that, it might be a good idea to add a modelselection GP example in the python_modular first and then just copy the code to this one.\nAnd a heatmap would be cool. I would also use 1.96  to get 95% confidence rather than 3.\n. I just tried the thing, pretty cool! :)\nOne thing that did not work is the noise level\nThere might be someone else working on #1080, please check that before you start coding \n. Btw check the go graphical example. Not the interactive one but the other\nOn 10 May 2013 23:10, \"goldbug\" notifications@github.com wrote:\n\nThanks.\nWill do the model selection in a the next PR and I guess that is what\ndescribed in #1080 https://github.com/shogun-toolbox/shogun/issues/1080.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/shogun-toolbox/shogun/pull/1082#issuecomment-17747309\n.\n. Oh and another thing: I have THW co2 data floating around somewhere as a\ntext file... Will check tomorrow\nOn 11 May 2013 00:10, \"Heiko Strathmann\" heiko.strathmann@gmail.com wrote:\nBtw check the go graphical example. Not the interactive one but the other\nOn 10 May 2013 23:10, \"goldbug\" notifications@github.com wrote:\n\nThanks.\nWill do the model selection in a the next PR and I guess that is what\ndescribed in #1080 https://github.com/shogun-toolbox/shogun/issues/1080.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/shogun-toolbox/shogun/pull/1082#issuecomment-17747309\n.\n. Yes. Sorry mobile phone. Btw the other example might be done by Roman, so\nbetter focuss on the interactive. Would be cool if all our features would\nhe in there....\nOn 11 May 2013 09:32, \"goldbug\" notifications@github.com wrote:\n\ngo graphical example?\nyou mean regression_gaussian_process_modular.py?\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/shogun-toolbox/shogun/pull/1082#issuecomment-17756359\n.\n. Hi!\nTo be honest, I have no idea what this code is doing, but it looks good. :)\nI suggest to add some unit tests that cover new/modified parts. Just recycle your local tests.\nExamples are already in, nice! Travis should not without you having modified the integration tests in the data repository. Since you changed the examples anyway, you have to re-create tests which is nice because you can change SG_ADD parameters on the fly\n. Merging since only new stuff, no modifications\n. Nice example, thanks!\nI will merge already, but could you add some more plotting stuff (e.g. plot variance of the gp)\n. and also please add a check for eigen3 as in themodular example\n. any ideas what the motivation for this was?\n. ok thanks\n. ok thanks\n. Very nice work! The next step would be matrices and vectors. We probably will not need all types covered for this since the code calls the same methods as the ones for which you added the tests.what is important though is to test borderline cases where eg. one dimension is empty.\n. Thanks for reporting! Ouch! We should probably just rewrite this method in a proper (and in fact readable) way\n. @sonney2k Could you document this method?\n. nice! see minor comment\n. Nice! See minor comments\n. YEAH!:)\n. Thanks!\n. wow! Another big one. I am currently on the road but will have a look\ntonight.\nOn 24 Jun 2013 16:44, \"Roman Votyakov\" notifications@github.com wrote:\n@karlnapf https://github.com/karlnapf please comment it :)\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/shogun-toolbox/shogun/pull/1187#issuecomment-19915541\n.\n. Hi Roman,\nNice! Seems like the refactoring is done now?\n\nOne thing on PRs:\nThe ones you send tend to be very big - which makes them hard to overview.\nCould you in the future try to send more smaller ones. For example: First rename/move files and includes. Second add new classes. Third whitespace changes. Dont be afraid to send many PRs - thats much better (make sure they work in travis though) PRs also show us what you are currently working on, and I dont have to ask questions all the time.\nIn particular try to avoid things that blow up the diff as different newline feeds, changes in comments etc. These make up about half of the changes in this particular PR, which is a bit annoying to scroll through (especially on mobile devices). Just do these things seperately.\nBut again, nice work, keep on going!\nLooking forward to see the first binary GP classifier! :)\n. Very nice!\n. gotta go home now, see you tomorrow!\n. Same here :)\n. Thanks :)\nOn 26 Jun 2013 22:28, \"Evangelos Anagnostopoulos\" notifications@github.com\nwrote:\n\nIt was merged just a moment after I updated it. I just added 3 more cases\nto handle some special ctrl characters, like \\t,\\n,\\r, that are more\nprobable to occur in a text. I'm just mentioning it to be covered!\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/shogun-toolbox/shogun/pull/1191#issuecomment-20081551\n.\n. Nice! thanks. It's also good to keep those things separate from the big pull requests.\n. I will merge now, you can change the minor {} in the next one\n. This is massive -- and very good work. I am pretty impressed :)\n\nAbout the number of shifts - they should be set in such way that the rational approximation has a certain accuracy - there are error bounds in the paper and I am sure this is also done in KRYLSTAT - if not, let me know I have code.\nOnce this is done, we can have some unit tests that assert that the provided accuracy holds. Note that the accuracy of the sum might fluctuate (go down and then up again) before a certain guarantee is reached. But it is possible to compute the number of shifts needed to have the error low.\n. I am confused, removing the C from all class names is not really a nice solution. Lets talk about this in IRC. Let me know when you are around\n. I am confused, removing the C from all class names is not really a nice solution. Lets talk about this in IRC. Let me know when you are around\n. good stuff! :)\nthe complex thing is annoying, but lets keep this for now\nOther changes are good\n@sonney2k do you agree with this extension of the class_list script?\n. good stuff! :)\nthe complex thing is annoying, but lets keep this for now\nOther changes are good\n@sonney2k do you agree with this extension of the class_list script?\n. Nice!\n. Nice!\n. What is the consequence of this?\nshoguns sparse matrix-vector product is way slower than eigen3's\n. What is the consequence of this?\nshoguns sparse matrix-vector product is way slower than eigen3's\n. Nice work! I will review the latex dox later on from doxygen html (hard to read in source code)\n. Nice work! I will review the latex dox later on from doxygen html (hard to read in source code)\n. Haha, man, thats a lot of nice work :)\n. Haha, man, thats a lot of nice work :)\n. Totally!\nMaybe after the EP is done, before we start on multiclass.\nWith all your new unit tests, things will be much easier to debug!\n. Totally!\nMaybe after the EP is done, before we start on multiclass.\nWith all your new unit tests, things will be much easier to debug!\n. yeah I agree. Thanks!\n. Hi Roman,\ndont worry too much about that for now. I just needed this to work for tomorrow\nAt some point, we should do a cleanup of all the swig interfaces, i.e., move all the interfaces to the modules where the files are located, some in machine, some regression, come classification - just to reflect our re factoring structure.\nBTW check this out :)\nhttp://nbviewer.ipython.org/5969622\nhttp://nbviewer.ipython.org/5969638\n. Hi Roman,\ndont worry too much about that for now. I just needed this to work for tomorrow\nAt some point, we should do a cleanup of all the swig interfaces, i.e., move all the interfaces to the modules where the files are located, some in machine, some regression, come classification - just to reflect our re factoring structure.\nBTW check this out :)\nhttp://nbviewer.ipython.org/5969622\nhttp://nbviewer.ipython.org/5969638\n. nice! :)\nplease set up a bug report with code for the case where unrefing the cloned object fails\n. nice! :)\nplease set up a bug report with code for the case where unrefing the cloned object fails\n. +1 for the not having to install before running tests :)\n. Nice patch.\nSorry for the delay, just returned to London\n. Nice patch.\nSorry for the delay, just returned to London\n. Good! Looking very forward to this! :)\n. yeah I am totally excited about those :)\nCurrent way to view them is to put the github file link to the public viwer webservice\nhttp://nbviewer.ipython.org/urls/raw.github.com/shogun-toolbox/shogun/develop/doc/ipython-notebooks/regression/gp_regression.ipynb\nBut that so far does not produce any results/plots. Still working on it :)\n. Yes, generating folds (and train/test machines on them) is all included in CCrossValidation. You can still do this with different splitting strategies, different performance measures (AUC, accuracy etc).\n. This should rather be fixed with the new tags done in GSoC 2016\nhttps://github.com/shogun-toolbox/shogun/wiki/GSoC-follow-up-blog-posts. One further nice consequence is: We can drop the migration framework.\nIt is never used, extremely hard to read/maintain code. It contains bugs that nobody is able to fix in reasonable time.\nThe reason why we have the migration framework is to not always have to re-submit integration tests when classes change. If we remove integration tests, no need for that.\nAs for serialised file format changes, we can just ignore newly added parameters in classes when loading from older files. This should be fine. Anything for complicated is totally unfeasible anyways.\n. I agree on this argument in principle, @sonney2k also made it.\nHowever, this is not what we do. Have a look into the integration tests, most of them do just call 1 function or 2 and then dump all objects to disc. In this state, all existing integration tests can easily replaced by unit.\nIf we wanted this type of big-picture behaviour, we should\n1.) Write tests that do big integration type things\n2.) Dump only numerical results of interest rather than the full class\nBut in fact, I dont even see the need for that since in Shogun, we do not have big parts of framework interacting with each other. Most things can be tested with units, can you think of an example?\n. I can do that in here, everyone can edit this entry:\nPro:\n- No need to update tests on parameter changes.\n- Simple numerical tests (such as these for kernels) can and should be replaced by unit tests\n- Our integration tests do not tell us what is wrong, just that something is wrong. Unit tests tell us what result is expected and what we get. This also provides information about what to expect. Integration tests just assert that there are no changes, which is a less strong check.\n- We usually \"fix\" integration tests via overwriting them without knowing whats going on. Dangerous.\n- API problems are detected by unit tests at compile time\n- Shogun does not have integration problems, rather most things break around single isolated methods.\n- One thing less to keep an eye on when building\nCons:\n- Unit tests do not provide big picture integration\n- We have to translate integration tests to unit tests before dropping, otherwise dangerous\n- There might be cases where unit tests are not applicable. However, we need examples for that.\n- We have to write unit tests (instead of just using the example we have to write anyways).\n- We don't have tests for the *_modular interfaces - with these tests we could have them for all.\n- Most of the issues we are having / had are and were detected by these tests.\n. This was finally done in #2938\n. fixed most bugs, still needs a cleanup\n@sonney2k could you maybe have a look. Would also be cool to have an example for this one.\n. This is a bit of a beast since it involves lots of code-refactoring - there are certainly more interesting tasks ;)\nI suggest you start by writing (passing) unit tests that cover most of the code in order to not introduce bugs when cleaning up\n. I think we actually can do this optionally:\nThe obtain_from_generic functions are mostly for users in the modular interfaces. For example in modelselection, it is not possible to get the kernel width of a GaussianKernel since get_kernel() just returns the CKernel interface and one cannot cast. If users want this, they have to compile this thing.\nFor developers, the obtain_from generic are not really important since its just two lines in c++ (and one usually knows the type to expect, so an assertion and a dynamic cast is fine\n. so?\n. This is not done yet, we want this for every shogun class. Ill re-open\n. This is not done yet, we want this for every shogun class. Ill re-open\n. This is now done via .as<T>\n4191 . thanks!\n. Eigen Problem seems solved.....merging\n. Nice patch! \n. Hi!\nThanks for the report. Its weird since it works on my machine. Could you explore the problem a bit and give me more information? Thanks!\n. I thought about that but I see the problem that we will have to re-implement all the calls on features.\nImagine an SVM which calls get_feature_vector a couple of times without even knowing that there is a subset.\n. No, again, X-validation on a SVM\n. Same here, probably this is cause by non-initialised memory\n2013/8/5 Roman Votyakov notifications@github.com\n\nWhen i run the following command:\nvalgrind --leak-check=full ./shogun-unit-test --gtest_filter=Malsar*\nit gives me this:\n==21180==\n==21180== HEAP SUMMARY:\n==21180== in use at exit: 298,572 bytes in 1,422 blocks\n==21180== total heap usage: 21,111 allocs, 19,689 frees, 1,092,326 bytes\nallocated\n==21180==\n==21180== 27,847 (176 direct, 27,671 indirect) bytes in 1 blocks are\ndefinitely lost in loss record 692 of 697\n==21180== at 0x4C2CD7B: malloc (in\n/usr/lib/valgrind/vgpreload_memcheck-amd64-linux.so)\n==21180== by 0x61959F7: operator new(unsigned long) (memory.cpp:92)\n==21180== by 0x5D9A871:\nshogun::CDenseFeatures::copy_subset(shogun::SGVector)\n(DenseFeatures.cpp:600)\n==21180== by 0x890F8C: generate_data() (MALSAR_unittest.cc:38)\n==21180== by 0x8911B7: MalsarL12Test_train_Test::TestBody()\n(MALSAR_unittest.cc:47)\n==21180== by 0x9DC3B2: void\ntesting::internal::HandleSehExceptionsInMethodIfSupported(testing::Test_,\nvoid (testing::Test::)(), char const) (gtest.cc:2090)\n==21180== by 0x9CBB66: void\ntesting::internal::HandleExceptionsInMethodIfSupported(testing::Test_, void\n(testing::Test::)(), char const) (gtest.cc:2126)\n==21180== by 0x9B9C44: testing::Test::Run() (gtest.cc:2161)\n==21180== by 0x9BA85A: testing::TestInfo::Run() (gtest.cc:2338)\n==21180== by 0x9BAEF6: testing::TestCase::Run() (gtest.cc:2445)\n==21180== by 0x9C059C: testing::internal::UnitTestImpl::RunAllTests()\n(gtest.cc:4243)\n==21180== by 0x9D89F2: bool\ntesting::internal::HandleSehExceptionsInMethodIfSupported(testing::internal::UnitTestImpl_,\nbool (testing::internal::UnitTestImpl::)(), char const) (gtest.cc:2090)\n==21180==\n==21180== 27,847 (176 direct, 27,671 indirect) bytes in 1 blocks are\ndefinitely lost in loss record 693 of 697\n==21180== at 0x4C2CD7B: malloc (in\n/usr/lib/valgrind/vgpreload_memcheck-amd64-linux.so)\n==21180== by 0x61959F7: operator new(unsigned long) (memory.cpp:92)\n==21180== by 0x5D9A871:\nshogun::CDenseFeatures::copy_subset(shogun::SGVector)\n(DenseFeatures.cpp:600)\n==21180== by 0x890F8C: generate_data() (MALSAR_unittest.cc:38)\n==21180== by 0x891487: MalsarClusteredTest_train_Test::TestBody()\n(MALSAR_unittest.cc:60)\n==21180== by 0x9DC3B2: void\ntesting::internal::HandleSehExceptionsInMethodIfSupported(testing::Test_,\nvoid (testing::Test::)(), char const) (gtest.cc:2090)\n==21180== by 0x9CBB66: void\ntesting::internal::HandleExceptionsInMethodIfSupported(testing::Test_, void\n(testing::Test::)(), char const) (gtest.cc:2126)\n==21180== by 0x9B9C44: testing::Test::Run() (gtest.cc:2161)\n==21180== by 0x9BA85A: testing::TestInfo::Run() (gtest.cc:2338)\n==21180== by 0x9BAEF6: testing::TestCase::Run() (gtest.cc:2445)\n==21180== by 0x9C059C: testing::internal::UnitTestImpl::RunAllTests()\n(gtest.cc:4243)\n==21180== by 0x9D89F2: bool\ntesting::internal::HandleSehExceptionsInMethodIfSupported(testing::internal::UnitTestImpl_,\nbool (testing::internal::UnitTestImpl::)(), char const) (gtest.cc:2090)\n==21180==\n==21180== 27,847 (176 direct, 27,671 indirect) bytes in 1 blocks are\ndefinitely lost in loss record 694 of 697\n==21180== at 0x4C2CD7B: malloc (in\n/usr/lib/valgrind/vgpreload_memcheck-amd64-linux.so)\n==21180== by 0x61959F7: operator new(unsigned long) (memory.cpp:92)\n==21180== by 0x5D9A871:\nshogun::CDenseFeatures::copy_subset(shogun::SGVector)\n(DenseFeatures.cpp:600)\n==21180== by 0x890F8C: generate_data() (MALSAR_unittest.cc:38)\n==21180== by 0x891757: MalsarTraceTest_train_Test::TestBody()\n(MALSAR_unittest.cc:73)\n==21180== by 0x9DC3B2: void\ntesting::internal::HandleSehExceptionsInMethodIfSupported(testing::Test_,\nvoid (testing::Test::)(), char const) (gtest.cc:2090)\n==21180== by 0x9CBB66: void\ntesting::internal::HandleExceptionsInMethodIfSupported(testing::Test_, void\n(testing::Test::)(), char const) (gtest.cc:2126)\n==21180== by 0x9B9C44: testing::Test::Run() (gtest.cc:2161)\n==21180== by 0x9BA85A: testing::TestInfo::Run() (gtest.cc:2338)\n==21180== by 0x9BAEF6: testing::TestCase::Run() (gtest.cc:2445)\n==21180== by 0x9C059C: testing::internal::UnitTestImpl::RunAllTests()\n(gtest.cc:4243)\n==21180== by 0x9D89F2: bool\ntesting::internal::HandleSehExceptionsInMethodIfSupported(testing::internal::UnitTestImpl_,\nbool (testing::internal::UnitTestImpl::)(), char const) (gtest.cc:2090)\n==21180==\n==21180== 71,471 (320 direct, 71,151 indirect) bytes in 1 blocks are\ndefinitely lost in loss record 695 of 697\n==21180== at 0x4C2CD7B: malloc (in\n/usr/lib/valgrind/vgpreload_memcheck-amd64-linux.so)\n==21180== by 0x61959F7: operator new(unsigned long) (memory.cpp:92)\n==21180== by 0x891533: MalsarClusteredTest_train_Test::TestBody()\n(MALSAR_unittest.cc:66)\n==21180== by 0x9DC3B2: void\ntesting::internal::HandleSehExceptionsInMethodIfSupported(testing::Test_,\nvoid (testing::Test::)(), char const) (gtest.cc:2090)\n==21180== by 0x9CBB66: void\ntesting::internal::HandleExceptionsInMethodIfSupported(testing::Test_, void\n(testing::Test::)(), char const) (gtest.cc:2126)\n==21180== by 0x9B9C44: testing::Test::Run() (gtest.cc:2161)\n==21180== by 0x9BA85A: testing::TestInfo::Run() (gtest.cc:2338)\n==21180== by 0x9BAEF6: testing::TestCase::Run() (gtest.cc:2445)\n==21180== by 0x9C059C: testing::internal::UnitTestImpl::RunAllTests()\n(gtest.cc:4243)\n==21180== by 0x9D89F2: bool\ntesting::internal::HandleSehExceptionsInMethodIfSupported(testing::internal::UnitTestImpl_,\nbool (testing::internal::UnitTestImpl::)(), char const) (gtest.cc:2090)\n==21180== by 0x9CDBC6: bool\ntesting::internal::HandleExceptionsInMethodIfSupported(testing::internal::UnitTestImpl_,\nbool (testing::internal::UnitTestImpl::)(), char const) (gtest.cc:2126)\n==21180== by 0x9C0286: testing::UnitTest::Run() (gtest.cc:3878)\n==21180==\n==21180== 71,599 (304 direct, 71,295 indirect) bytes in 1 blocks are\ndefinitely lost in loss record 696 of 697\n==21180== at 0x4C2CD7B: malloc (in\n/usr/lib/valgrind/vgpreload_memcheck-amd64-linux.so)\n==21180== by 0x61959F7: operator new(unsigned long) (memory.cpp:92)\n==21180== by 0x891803: MalsarTraceTest_train_Test::TestBody()\n(MALSAR_unittest.cc:79)\n==21180== by 0x9DC3B2: void\ntesting::internal::HandleSehExceptionsInMethodIfSupported(testing::Test_,\nvoid (testing::Test::)(), char const) (gtest.cc:2090)\n==21180== by 0x9CBB66: void\ntesting::internal::HandleExceptionsInMethodIfSupported(testing::Test_, void\n(testing::Test::)(), char const) (gtest.cc:2126)\n==21180== by 0x9B9C44: testing::Test::Run() (gtest.cc:2161)\n==21180== by 0x9BA85A: testing::TestInfo::Run() (gtest.cc:2338)\n==21180== by 0x9BAEF6: testing::TestCase::Run() (gtest.cc:2445)\n==21180== by 0x9C059C: testing::internal::UnitTestImpl::RunAllTests()\n(gtest.cc:4243)\n==21180== by 0x9D89F2: bool\ntesting::internal::HandleSehExceptionsInMethodIfSupported(testing::internal::UnitTestImpl_,\nbool (testing::internal::UnitTestImpl::)(), char const) (gtest.cc:2090)\n==21180== by 0x9CDBC6: bool\ntesting::internal::HandleExceptionsInMethodIfSupported(testing::internal::UnitTestImpl_,\nbool (testing::internal::UnitTestImpl::)(), char const) (gtest.cc:2126)\n==21180== by 0x9C0286: testing::UnitTest::Run() (gtest.cc:3878)\n==21180==\n==21180== 71,873 (312 direct, 71,561 indirect) bytes in 1 blocks are\ndefinitely lost in loss record 697 of 697\n==21180== at 0x4C2CD7B: malloc (in\n/usr/lib/valgrind/vgpreload_memcheck-amd64-linux.so)\n==21180== by 0x61959F7: operator new(unsigned long) (memory.cpp:92)\n==21180== by 0x891263: MalsarL12Test_train_Test::TestBody()\n(MALSAR_unittest.cc:53)\n==21180== by 0x9DC3B2: void\ntesting::internal::HandleSehExceptionsInMethodIfSupported(testing::Test_,\nvoid (testing::Test::)(), char const) (gtest.cc:2090)\n==21180== by 0x9CBB66: void\ntesting::internal::HandleExceptionsInMethodIfSupported(testing::Test_, void\n(testing::Test::)(), char const) (gtest.cc:2126)\n==21180== by 0x9B9C44: testing::Test::Run() (gtest.cc:2161)\n==21180== by 0x9BA85A: testing::TestInfo::Run() (gtest.cc:2338)\n==21180== by 0x9BAEF6: testing::TestCase::Run() (gtest.cc:2445)\n==21180== by 0x9C059C: testing::internal::UnitTestImpl::RunAllTests()\n(gtest.cc:4243)\n==21180== by 0x9D89F2: bool\ntesting::internal::HandleSehExceptionsInMethodIfSupported(testing::internal::UnitTestImpl_,\nbool (testing::internal::UnitTestImpl::)(), char const) (gtest.cc:2090)\n==21180== by 0x9CDBC6: bool\ntesting::internal::HandleExceptionsInMethodIfSupported(testing::internal::UnitTestImpl_,\nbool (testing::internal::UnitTestImpl::_)(), char const*) (gtest.cc:2126)\n==21180== by 0x9C0286: testing::UnitTest::Run() (gtest.cc:3878)\n==21180==\n==21180== LEAK SUMMARY:\n==21180== definitely lost: 1,464 bytes in 6 blocks\n==21180== indirectly lost: 297,020 bytes in 1,413 blocks\n==21180== possibly lost: 0 bytes in 0 blocks\n==21180== still reachable: 88 bytes in 3 blocks\n==21180== suppressed: 0 bytes in 0 blocks\n==21180== Reachable blocks (those to which a pointer was found) are not\nshown.\n==21180== To see them, rerun with: --leak-check=full --show-reachable=yes\n==21180==\n==21180== For counts of detected and suppressed errors, rerun with: -v\n==21180== ERROR SUMMARY: 6 errors from 6 contexts (suppressed: 2 from 2)\n\u0097\nReply to this email directly or view it on GitHubhttps://github.com/shogun-toolbox/shogun/issues/1360\n.\n. Quite ironic that memory unit tests leak ;)\nThanks again for finding those!\n. Thanks a lot for reporting those.\nIf you are keen on bugfixing: Most of these memory errors are cause by non-initialised memory in the std constructors...\n. added rename of log_pdf method to avoid name clash in jblas\n. While I agree in principle, I think there is something more subtle going on:\nSometimes, subsetted feature instances are duplicated, for example in the SVM store_model_features (grep for it).\nIf we clone, the full data matrix is copied, duplicate only does this for a subset AFAIK. Might be wrong though.\nThe way to go is to check all calls in the code .....\n. Are the feature matrices really shared? I dont think so. I think they are\ncopied\nBut if there is a subset, only the subset is copied, that is the main usage\nof it.\nUsing clone would be a bit more inefficient, but easier to maintain. We\nhave to investigate this.\n\n2013/8/7 Fernando Iglesias notifications@github.com\n\nYou are right. Clone is making a deep copy, while duplicate doesn't (as\nyou said, the features matrices are shared for instance).\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/shogun-toolbox/shogun/issues/1374#issuecomment-22242084\n.\n. I think I was wrong and confused this with another method.\n\nvoid CKNN::store_model_features() uses duplicate in the wrong way (same data, might cause bugs, clone would be better here)\ntemplate< class ST > CFeatures* CMatrixFeatures< ST >::duplicate() const returns NULL (!!!) with a comment todo\n@hushell\nI see that you are creating copy constructor based duplicate methods in CFactorGraph too. This is not good, better use clone\n@iglesias I searched for duplicate in our code. There seems to be not a single instance where clone is not replacing it. So I suggest to remove the abstract method from CFeatures and all subclasses and replace all calls by clone. Do you want to do this? :)\n. well spotted btw :)\n. The way that duplicate is used (at least in store model features) assumes that an independent copy is returned.\nIf you want a new feature object with the same matrix, use the copy constructor :)\n. KMeans should also be rewritten from scratch, or seriously fixed and tested and made more flexible (like init cluster centers by hand)\n. Of course, all PR are welcome at any time :)\n@iglesias coud you take care of this?\n. Of course, all PR are welcome at any time :)\n@iglesias coud you take care of this?\n. I second S\u00f6ren: Great work!\nAlso very nice to have some tests for such low-level parts :)\nSince you already touched things, converting assert to require is a lot better for users as they get information on what they did wrong.\n. Thanks!\n. Yeah I will deal with this once back in town. For now, decrease accuracy\n. Nice patch :)\n. Thanks! I'll do something about this when being back\n. Static interfaces dont exist anymore. Can't wait to tell the stats people here ;)\n. Got my Ok to merge,\nRoman, your efforts are well appreciated here, but please try to focus on the GP stuff more, since it is delayed. Warnings should  be done after gsoc or when  gsoc project is finished. Thanks for the good patch anyway!\n. Nice!\nWe should update unit tests though if we touch this. See the libshogun examples where I wrote test examples which can be migrated. \nAlso , please also update the other x validation splitting and add a note in the class description that it is thread safe random number generator.\nSorry for all the requests and thanks again;)\n. @votjakovr @vigsterkr Just wondering, what can they detect?\n. sweet! All that would be nice :)\n. We need it, otherwise some guards disable code that needs to be in (and tested)\nIt is a complex number datatype where real and imag part have both 64 bit\n. We need it, otherwise some guards disable code that needs to be in (and tested)\nIt is a complex number datatype where real and imag part have both 64 bit\n. Yep sorry its USE.\nHave a look in Mathematics.i where these things are used. The complex64 one is missing which might lead to problems\n. Yep sorry its USE.\nHave a look in Mathematics.i where these things are used. The complex64 one is missing which might lead to problems\n. fixed in #1500 \n. @lamday the template instantiations need to be in .cpp files. Changed this among most of your classes, but might have forgotten some. Otherwise causes weird errors, as well as problems with swig.\n. Thanks!\nHow is the status regarding your model selection investigations?\n. @votjakovr Could you pls make a list of those bugs, in a githu isse may? I started one a while ago #1231 \nI am also happy to discuss general changes to the framework. Please also document your ideas there, for after GSoC.\nIf we had a list of bugs/things to improve, this would really help in planning.\nPlease also have a look in x-validation for the GPs, as this is very important to compare its predictive performance against non-probabilistic methods such as SVM. Should be with minimal effort to make this work.\n. Travis caused by this patch?\ncannot see since on mobile phone\n. Is the test random?\n. Still worth investigating why it happened... But merged Pr for now\nOn 1 Sep 2013 19:08, \"Roman Votyakov\" notifications@github.com wrote:\n\n@karlnapf https://github.com/karlnapf, @pickle27https://github.com/pickle27,\nit seems like a random issue.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/shogun-toolbox/shogun/pull/1503#issuecomment-23629857\n.\n. Good! \nFirst step towards stability :)\n. Hi\nWhy having this?\nMost fast sorting algorithms are equally fast if the vector is sorted. Rather add an output flag to the sort method?\n. not really a big deal all those things :) and const is nice\n. Nice!\n\nI will try it now with ozone.\nBTW there are probably more bugs like those you found, so lets do some challenging tests on all sub-parts going :)\n. travis fails due to other things\n. thanks! :)\n. @votjakovr thanks for the hint :)\n. no dont need it in travis, the tests work without\n. I see, thanks!\n. I see, thanks!\n. attempt to do this in #1531 #1530 #1533\n@vigsterkr pls check if that is ok like this :)\n. This would be good to have on travis, since the log-det stuff is not tested otherwise\n. But if the buildbots do it, that should be fine. They have to work though ;)\n. But if the buildbots do it, that should be fine. They have to work though ;)\n. will you do that? Or should I?\n. will you do that? Or should I?\n. Thanks :)\n. Thanks :)\n. A little test for this would be nice, but already merging\n. travis is red but these things are unrelated\n. travis is red but these things are unrelated\n. lets see :)\n. lets see :)\n. Thanks for that! \nThis will help a lot!\n. Thanks for that! \nThis will help a lot!\n. CC=/usr/bin/gcc44 CXX=/usr/bin/g++44 cmake -DBUILD_EXAMPLES=Off -DENABLE_TESTING=On -DBUNDLE_COLPACK=On -DBUNDLE_EIGEN=On -DPythonModular=ON -DCMAKE_INSTALL_PREFIX=/home/ucabhst/shogun-install -DPYTHON_INCLUDE_DIR=/share/apps/python-2.7.1/include/python2.7 -DPYTHON_LIBRARY=/share/apps/python-2.7.1/lib/python2.7 -DBUNDLE_ARPREC=On ..\ngives\nCMake Error at /share/apps/cmake-2.8.11.2/share/cmake-2.8/Modules/FindPackageHandleStandardArgs.cmake:108 (message):\n  Could NOT find Jinja2 (missing: JINJA2_IMPORT_SUCCESS)\nCall Stack (most recent call first):\n  /share/apps/cmake-2.8.11.2/share/cmake-2.8/Modules/FindPackageHandleStandardArgs.cmake:315 (_FPHSA_FAILURE_MESSAGE)\n  cmake/FindJinja2.cmake:17 (find_package_handle_standard_args)\n  CMakeLists.txt:774 (FIND_PACKAGE)\nand cmake fails\n. CC=/usr/bin/gcc44 CXX=/usr/bin/g++44 cmake -DBUILD_EXAMPLES=Off -DENABLE_TESTING=On -DBUNDLE_COLPACK=On -DBUNDLE_EIGEN=On -DPythonModular=ON -DCMAKE_INSTALL_PREFIX=/home/ucabhst/shogun-install -DPYTHON_INCLUDE_DIR=/share/apps/python-2.7.1/include/python2.7 -DPYTHON_LIBRARY=/share/apps/python-2.7.1/lib/python2.7 -DBUNDLE_ARPREC=On ..\ngives\nCMake Error at /share/apps/cmake-2.8.11.2/share/cmake-2.8/Modules/FindPackageHandleStandardArgs.cmake:108 (message):\n  Could NOT find Jinja2 (missing: JINJA2_IMPORT_SUCCESS)\nCall Stack (most recent call first):\n  /share/apps/cmake-2.8.11.2/share/cmake-2.8/Modules/FindPackageHandleStandardArgs.cmake:315 (_FPHSA_FAILURE_MESSAGE)\n  cmake/FindJinja2.cmake:17 (find_package_handle_standard_args)\n  CMakeLists.txt:774 (FIND_PACKAGE)\nand cmake fails\n. done in #1550\n. done in #1550\n. A good start is CStatistics, for example covariance matrix methods. These are used all over the place and all require a LAPACK guard.\n. A good start is CStatistics, for example covariance matrix methods. These are used all over the place and all require a LAPACK guard.\n. Yeah its more for later....\nLAPACK keeps in annyoing me and most things can be done with eigen which also has the advantage that we can bundle it no zero dependency on installed stuff, which is handy in weird environments :)\n. You need to hide LAPACK from Shogun and then try to compile (this is when the guards will be important)\n. Not quite,\ngetting rid of lapack requires to actually remove the LAPACK class from code, and then check the include guard necessarity afterwards.\nSo rather grep for lapack linear algebra calls.\nAlso note that we want to replace things with the linalg interface, such as we already did for dot products.\n. What would be best:\n- Move all lapack functionality into the linalg framework (i.e. write a lapack backend for the dot-product and replace all lapack dot calls by that (which means we can remove the LAPACK guards in the individual files))\n- add more functionality to linalg, so that we can replace lapacks methods for SVD/linear solve/other.\nJust replacing lapack calls with eigen3 is not what we want to do. I think as we are mostly using lapack for dot products, that would be the first (and easiest) step\n. Yeah exactly,\nWe want both (eigen3 and lapack) implementations of dot in linalg.\nAnd then replace the eigen3/LAPACK calls in our code by the linalg calls. Then we dont need any include guards in the code itself, AND we are independent of future backend changes\n. Ah this topic is so painful\n@vigsterkr so I tried to hack cmake to link statically against these files but since we are creating a dynamic library that doesnt work. Now I just locally installed LAPACK (which doesnt offer native build support for creating dynamic libs, but I used a little hack shared by a user)\nI am changing this to \"bundle LAPACK\" since this would eliminate all these problems. And assigning to you :)\n. Ah this topic is so painful\n@vigsterkr so I tried to hack cmake to link statically against these files but since we are creating a dynamic library that doesnt work. Now I just locally installed LAPACK (which doesnt offer native build support for creating dynamic libs, but I used a little hack shared by a user)\nI am changing this to \"bundle LAPACK\" since this would eliminate all these problems. And assigning to you :)\n. We had this before. Always ref before unref. Will soon be obsolete when we\nhave sergeys automatic setters.\nOn 9 Sep 2013 18:55, \"tklein23\" notifications@github.com wrote:\n\nNo, didn't say SG_UNREF(model) will be a problem with model==NULL - in\nthis case SG_UNREF will become a NO-OP.\nBut I agree that this bug only happens in connection with some other bug\n\"outside\". Anyway, the bug can easily triggered with something like this:\nlabels = new Labels(); // ref_count == 0\nmodel->set_labels(labels); // allright, model now owns the labels\nmodel->set_labels(labels); // crash\nSo repeated calling of \"set_labels\" is not expected to harm. :)\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/shogun-toolbox/shogun/pull/1562#issuecomment-24099343\n.\n. This still happens if the object has a ref before. In particular if it has\nexactly one.\nOn 9 Sep 2013 19:14, \"Soeren Sonnenburg\" notifications@github.com wrote:\nwait sth like should be expected to crash\nthe rule is: as soon as you pass on an object so some other function and\ndid not SG_REF it - you have no guarantee that your object is still valid\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/shogun-toolbox/shogun/pull/1562#issuecomment-24100977\n.\n. Ok. I'll fix it tomorrow.\nWill be back hacking soon btw. Just have to finish a few things and half\ntakes also some time\nOn 18 Sep 2013 20:11, \"Viktor Gal\" notifications@github.com wrote:\n@karlnapf https://github.com/karlnapf let's do something about this ...\nplz\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/shogun-toolbox/shogun/issues/1574#issuecomment-24691110\n.\n. @vigsterkr I currently only have an antique machine where I cannot compile/run tests since it takes already 30mins to compile Shogun. Going for lunch now, have a look what travis says and feel free to merge\n. @vigsterkr I currently only have an antique machine where I cannot compile/run tests since it takes already 30mins to compile Shogun. Going for lunch now, have a look what travis says and feel free to merge\n. All modular interfaces fail since you forgot to change the .i interface files in interfaces/modular \njust enable python locally then you will see.\n\nBtw could you change all the solver summaries at the end of the iterations from debug to info?\nthanks for the patch, good work with the COCG :)\nOh btw there should be a libshogun example on how to use the computation framework, I.e. how to define on jobs and submit them. Once all other things are done.\nsee you!\n. I think a very very cool idea would also be to make the computation framework being able to use director classes.\nThis way, people could define jobs in the modular interfaces, whose methods are then still called from your c++ code.\nThis would be a total blast since shogun could then serve as a general computation engine for general tasks from whatever language.  I will open an issue on this soon.\n@iglesias do you think this is possible?\n. Travis fails, what's wrong there?\ncurrently mobile so cannot see\n. Travis fails, what's wrong there?\ncurrently mobile so cannot see\n. @iglesias  99 - integration-python_modular-tester-metric_lmnn_modular (Failed)\n. @iglesias  99 - integration-python_modular-tester-metric_lmnn_modular (Failed)\n. @lambday /home/travis/build/shogun-toolbox/shogun/src/shogun/lib/SGSparsatrix.h:85: Warning 503: Can't wrap 'operator *' unless renamed to a valid identifier.\nI do not think this is a problem, just add (not replace) a wrapper method to multiply to matrix or vector and then tell swig to ignore\n. @lambday /home/travis/build/shogun-toolbox/shogun/src/shogun/lib/SGSparsatrix.h:85: Warning 503: Can't wrap 'operator *' unless renamed to a valid identifier.\nI do not think this is a problem, just add (not replace) a wrapper method to multiply to matrix or vector and then tell swig to ignore\n. Assigning Soren since its mkl. \nBut @lisitsyn and @karlnapf should also have a look since it involves multi class and possibly x validation. \nSince two people asked already,  setting it for release\n. Not yet, very sorry.\nI will have some more time early next year, pls keep on pushing it ;)\n. Ah yeah, definitely! I will put an entrance label - maybe some GSoC student solves it. I also want to look into it, but so many things to do..... \n. Hi Eric,\nit would be awesome if you could contribute this back into Shogun :)\nHeiko\n2014-02-25 8:45 GMT+00:00 thereisnoknife notifications@github.com:\n\nHi Heiko, don't worry, I am about to solve this, and keep you inform. The\nthing is that the functionality I was looking is not support yet in shogun.\nI have to write it myself.\nCheers!\n2014-02-19 12:37 GMT+01:00 Heiko Strathmann notifications@github.com:\n\nAh yeah, definitely! I will put an entrance label - maybe some GSoC\nstudent solves it. I also want to look into it, but so many things to\ndo.....\n\nReply to this email directly or view it on GitHub<\nhttps://github.com/shogun-toolbox/shogun/issues/1589#issuecomment-35489658\n.\n\n\nReply to this email directly or view it on GitHubhttps://github.com/shogun-toolbox/shogun/issues/1589#issuecomment-35986267\n.\n. Hi Eric,\n\nit would be awesome if you could contribute this back into Shogun :)\nHeiko\n2014-02-25 8:45 GMT+00:00 thereisnoknife notifications@github.com:\n\nHi Heiko, don't worry, I am about to solve this, and keep you inform. The\nthing is that the functionality I was looking is not support yet in shogun.\nI have to write it myself.\nCheers!\n2014-02-19 12:37 GMT+01:00 Heiko Strathmann notifications@github.com:\n\nAh yeah, definitely! I will put an entrance label - maybe some GSoC\nstudent solves it. I also want to look into it, but so many things to\ndo.....\n\nReply to this email directly or view it on GitHub<\nhttps://github.com/shogun-toolbox/shogun/issues/1589#issuecomment-35489658\n.\n\n\nReply to this email directly or view it on GitHubhttps://github.com/shogun-toolbox/shogun/issues/1589#issuecomment-35986267\n.\n. Yeah resources should be strictly limited of course.\nThe cool thing is that all these 100s of people having told me they would have liked to try shogun but failed installing it now are served.\nIn conjunction with our examples notebooks and python examples, which we could offer as templates, this might vastly increase the user base. Also it's pretty sexy; )\n. I mean come on, it must be possible in 2013 to start a shell on a remote computer without being able to take it over ;)\nWhat about something like spawning a completely new guest user with limited access rights and start the ipython server process from there?\n\n@sonney2k Data transfer:\n- Shogun example data (maybe we should start with this only, only possible to reproduce examples)\n- Have an upload button on some web-page (that puts files to some $TEMP dir)\n- Have user accounts where people get access to some $HOME dir (we could actually charge money for this, but need more manpower)\n- python magic:\n  source = urllib2.urlopen(\"http://someUrl.com/somePage.html\").read()\nopen(\"/path/to/someFile\", \"wb\").write(source)\nI am currently really raving on this idea (I know I should focus on getting the release done, but ...). Combined with other parts of Shogun such as for example the planned computation framework via director classes, we could become so much more general than now. And it would even be possible to offer a remote-computing service. But I guess the point is that only we have to worry about how to install shogun.\n. I mean come on, it must be possible in 2013 to start a shell on a remote computer without being able to take it over ;)\nWhat about something like spawning a completely new guest user with limited access rights and start the ipython server process from there?\n@sonney2k Data transfer:\n- Shogun example data (maybe we should start with this only, only possible to reproduce examples)\n- Have an upload button on some web-page (that puts files to some $TEMP dir)\n- Have user accounts where people get access to some $HOME dir (we could actually charge money for this, but need more manpower)\n- python magic:\n  source = urllib2.urlopen(\"http://someUrl.com/somePage.html\").read()\nopen(\"/path/to/someFile\", \"wb\").write(source)\nI am currently really raving on this idea (I know I should focus on getting the release done, but ...). Combined with other parts of Shogun such as for example the planned computation framework via director classes, we could become so much more general than now. And it would even be possible to offer a remote-computing service. But I guess the point is that only we have to worry about how to install shogun.\n. @vigsterkr @sonney2k sweet! Looking forward.\nThere are more possibilities with an ipython server running with shogun. One could for example please connect from mobile devices, etc etc.\nI think that we could actually find sponsors for developing this. Many data science companies here in London for example are investing money into similar ideas. Imagine we could pay a developer to work on such an infrastructure full time for a year or so.... but let's start small :)\n. @vigsterkr @sonney2k sweet! Looking forward.\nThere are more possibilities with an ipython server running with shogun. One could for example please connect from mobile devices, etc etc.\nI think that we could actually find sponsors for developing this. Many data science companies here in London for example are investing money into similar ideas. Imagine we could pay a developer to work on such an infrastructure full time for a year or so.... but let's start small :)\n. @vigsterkr great!!\nI have a few suggestions:\n- do this as a nightly build to have it always with recent version and make sure that we don't loose it on the way after a while (I think you mentioned that you want to do this before)\n- I agree that existing notebooks should be copied into the directory where the ipython session runs in. Allow people to explore/modify existing things and is very nice for demo purposes\n- The web-editor has a menu for saving/exporting as ipython-notebook or python code. People can use this to download their changes.\n- For importing notebooks, we can thing about something later. Maybe some button somewhere next to the launcher to upload a notebook file to the session directory?\n- we might want to think about user accounts at some point, though I dont know how that could be done in a nice way\nSome documentation hints regarding what we want to do\n- Notebook profile: http://ipython.org/ipython-doc/stable/config/overview.html#profiles\n- Notebook security: http://ipython.org/ipython-doc/stable/interactive/public_server.html#notebook-security\n- Tips for running a public server: http://ipython.org/ipython-doc/stable/interactive/public_server.html#notebook-public-server\n. Just found out that it is very easy to import existing (on the FS) notebooks on the main-page of the notebook-server. Since it is also easy to export to the FS, this completely solves the in/out issue. Just copy our existing ones into the directory and leave the rest to the user.\n. The data imports all fail...\n. The data imports all fail...\n. Absolutely Awesome!\n. @vigsterkr who knows c# to add some guards there?\n. @vigsterkr who knows c# to add some guards there?\n. If you tell me how to check for SVMLight fom CSharp, I will change that.\nIn python we use try catch in the imports...\n. If you tell me how to check for SVMLight fom CSharp, I will change that.\nIn python we use try catch in the imports...\n. TYPE a=SHOGUNCLASS.METHODNAME(b,c,d)\nis basically all we need (we can even load data with that)\nEach language then needs a conversion. Or as said, we convert from python (which might be easier since one can run the \"original\" example\n. TYPE a=SHOGUNCLASS.METHODNAME(b,c,d)\nis basically all we need (we can even load data with that)\nEach language then needs a conversion. Or as said, we convert from python (which might be easier since one can run the \"original\" example\n. This is not up to date anymore as we have the cookbook now\n. Nice work!\n. Nice work!\n. We could change if other things work :)\nJust trying to compile a list of things that probably wont make it into 3.0.\nWe can also postpone things then\n. We could change if other things work :)\nJust trying to compile a list of things that probably wont make it into 3.0.\nWe can also postpone things then\n. ```\nls /usr/lib64/ | grep lapack\nliblapack.so.3\nliblapack.so.3.1\nliblapack.so.3.1.1\nls /usr/lib64/ | grep blas\nlibblas.so.3\nlibblas.so.3.1\nlibblas.so.3.1.1\nlibgslcblas.so\nlibgslcblas.so.0\nlibgslcblas.so.0.0.0\n```\nI also have an atlas installation in\n    ls /usr/lib64/atlas/\n    libatlas.a     libatlas.so.3.0  libcblas.so.3    libclapack.so.3    libf77blas.so      liblapack.a     liblapack.so.3.0  libptcblas.so.3    libptf77blas.so\nlibatlas.so    libcblas.a       libcblas.so.3.0  libclapack.so.3.0  libf77blas.so.3    liblapack.so    libptcblas.a      libptcblas.so.3.0  libptf77blas.so.3\nlibatlas.so.3  libcblas.so      libclapack.so    libf77blas.a       libf77blas.so.3.0  liblapack.so.3  libptcblas.so     libptf77blas.a     libptf77blas.so.3.0\nand tried \n-DBLAS_LIBRARIES=/usr/lib64/atlas\n-DLAPACK_LIBRARIES=/usr/lib64/atlas\nbut both did not work, undefined symbol\n. ```\nls /usr/lib64/ | grep lapack\nliblapack.so.3\nliblapack.so.3.1\nliblapack.so.3.1.1\nls /usr/lib64/ | grep blas\nlibblas.so.3\nlibblas.so.3.1\nlibblas.so.3.1.1\nlibgslcblas.so\nlibgslcblas.so.0\nlibgslcblas.so.0.0.0\n```\nI also have an atlas installation in\n    ls /usr/lib64/atlas/\n    libatlas.a     libatlas.so.3.0  libcblas.so.3    libclapack.so.3    libf77blas.so      liblapack.a     liblapack.so.3.0  libptcblas.so.3    libptf77blas.so\nlibatlas.so    libcblas.a       libcblas.so.3.0  libclapack.so.3.0  libf77blas.so.3    liblapack.so    libptcblas.a      libptcblas.so.3.0  libptf77blas.so.3\nlibatlas.so.3  libcblas.so      libclapack.so    libf77blas.a       libf77blas.so.3.0  liblapack.so.3  libptcblas.so     libptf77blas.a     libptf77blas.so.3.0\nand tried \n-DBLAS_LIBRARIES=/usr/lib64/atlas\n-DLAPACK_LIBRARIES=/usr/lib64/atlas\nbut both did not work, undefined symbol\n. All linked libs exist:\n    ls /usr/lib64/atlas/liblapack.so.3 /usr/lib64/atlas/libcblas.so.3 /usr/lib64/atlas/libf77blas.so.3 /usr/lib64/atlas/libatlas.so.3     /usr/lib64/liblapack.so.3\n    /usr/lib64/atlas/libatlas.so.3  /usr/lib64/atlas/libcblas.so.3  /usr/lib64/atlas/libf77blas.so.3  /usr/lib64/atlas/liblapack.so.3      /usr/lib64/liblapack.so.3\nAnd contain the symbol dstemr_\n    nm /usr/lib64/liblapack.so.3 | grep dstemr_\n    00000000001e46d0 T dstemr_\n. All linked libs exist:\n    ls /usr/lib64/atlas/liblapack.so.3 /usr/lib64/atlas/libcblas.so.3 /usr/lib64/atlas/libf77blas.so.3 /usr/lib64/atlas/libatlas.so.3     /usr/lib64/liblapack.so.3\n    /usr/lib64/atlas/libatlas.so.3  /usr/lib64/atlas/libcblas.so.3  /usr/lib64/atlas/libf77blas.so.3  /usr/lib64/atlas/liblapack.so.3      /usr/lib64/liblapack.so.3\nAnd contain the symbol dstemr_\n    nm /usr/lib64/liblapack.so.3 | grep dstemr_\n    00000000001e46d0 T dstemr_\n. Just tried moving  \"../../src/shogun/libshogun.so.14.0\" to the end of the library list in the compile command, but no help\n. Just tried moving  \"../../src/shogun/libshogun.so.14.0\" to the end of the library list in the compile command, but no help\n. @besser82 found a workaround, which is to put the liblapack.so first in the linking files. Stay tuned\n. @besser82 found a workaround, which is to put the liblapack.so first in the linking files. Stay tuned\n. In case you didn't see, have a look at your gist...\nOn 16 Sep 2013 20:31, \"Roman Votyakov\" notifications@github.com wrote:\n\nWhen i've created a function, something like:\nfunc(CMap map)\ni've found, that after call:\nfunc(map);\ni can't use map, since internal map's data was freed. Is this correct\nbehavior?\nFor example, the code to reproduce the problem can be found (there)[\nhttp://gist.github.com/votjakovr/6585230]. You can run valgrind with this\ncode and see the problem(?).\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/shogun-toolbox/shogun/issues/1605\n.\n. In case you didn't see, have a look at your gist...\nOn 16 Sep 2013 20:31, \"Roman Votyakov\" notifications@github.com wrote:\nWhen i've created a function, something like:\nfunc(CMap map)\ni've found, that after call:\nfunc(map);\ni can't use map, since internal map's data was freed. Is this correct\nbehavior?\nFor example, the code to reproduce the problem can be found (there)[\nhttp://gist.github.com/votjakovr/6585230]. You can run valgrind with this\ncode and see the problem(?).\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/shogun-toolbox/shogun/issues/1605\n.\n. Thanks! Will try now\n. Thanks! Will try now\n. Exactly. We should aim for high quality, with proper text and images as in the gsoc ones, not just some dumped code, as basic things can go to the examples\n. @sonney2k The others seem nice. Though we should aim for some basic style things that are done in all of them, such as linking against shogun documentation, etc.\n\nFor the SVM one, I think it should contain:\n-toy data example to illustrate some concepts (linear, gaussian kernel, parameters (width, C), evaluation with cross-validation and grid-search)\n-string data example, for example a full walk-through the spectrum kernel. I in fact started a notebook on this shortly after the workshop https://gist.github.com/karlnapf/6721192\n-multiclass\n-maybe some MKL stuff?\n-some OCAS stuff?\n-There are so many SVM related cool things in Shogun, and most of them could be illustrated in there\nAs said, I would aim for the same quality/extend as in the GSoC notebooks\n. Oh Agreed! Just random thing that came to my mind\nGrid search should be in though, and evaluation, and maybe multiclass,and a nice presentation beyond just commentless code\n. Just posted a \"hello\" on the julia-dev, lets see what they say.\n. Just posted a \"hello\" on the julia-dev, lets see what they say.\n. See https://groups.google.com/forum/#!msg/julia-dev/VNdVVnGXlXs/TJAISkHvZ8QJ\n. Absolutely, but what we need here is the ability of SWIG to link to Julia. This should not be too hard to be done by someone knowing SWIG and julia a bit .... @ChrisRackauckas you know someone?. closing since seems to be fixed\n. thanks!\n. Would be good to have this stuff documented in a notebook. I don't know nothing about it ;)\n. @pickle27 has written some ICA code in mathematics/ajd which uses this transformation.\nIf the preprocessor exists, this can be pulled out.\n. Feel free to work on this after GSoC :)\n. I think @votjakovr is on this\n. Sweet, lets see, he told me that its already done and he just needs to compile a PR from it\n. This is 3 lines in eigen 3. Compute covariance, compute cholesky,\ntriangular solve\nOn 8 Feb 2014 02:47, \"Kevin Hughes\" notifications@github.com wrote:\n\nguess this never happened eh? Its on my personal todo list so I'll leave\nit on there [image: :smile:]\n\nReply to this email directly or view it on GitHubhttps://github.com/shogun-toolbox/shogun/issues/1621#issuecomment-34526956\n.\n. Ah good old never ending to do lists;)\nOn 8 Feb 2014 05:56, \"Heiko Strathmann\" heiko.strathmann@gmail.com wrote:\nThis is 3 lines in eigen 3. Compute covariance, compute cholesky,\ntriangular solve\nOn 8 Feb 2014 02:47, \"Kevin Hughes\" notifications@github.com wrote:\n\nguess this never happened eh? Its on my personal todo list so I'll leave\nit on there [image: :smile:]\n\nReply to this email directly or view it on GitHubhttps://github.com/shogun-toolbox/shogun/issues/1621#issuecomment-34526956\n.\n. Exactly!\nOn 8 Feb 2014 06:00, \"Soeren Sonnenburg\" notifications@github.com wrote:\n\nSo it is an excellent entrance task\nHeiko Strathmann notifications@github.com wrote:\n\nThis is 3 lines in eigen 3. Compute covariance, compute cholesky,\ntriangular solve\nOn 8 Feb 2014 02:47, \"Kevin Hughes\" notifications@github.com wrote:\n\nguess this never happened eh? Its on my personal todo list so I'll\nleave\nit on there [image: :smile:]\n\nReply to this email directly or view it on\nGitHub<\nhttps://github.com/shogun-toolbox/shogun/issues/1621#issuecomment-34526956\n.\n\n\nReply to this email directly or view it on GitHub:\nhttps://github.com/shogun-toolbox/shogun/issues/1621#issuecomment-34535005\n\n\nSent from Kaiten Mail. Please excuse my brevity.\n\nReply to this email directly or view it on GitHubhttps://github.com/shogun-toolbox/shogun/issues/1621#issuecomment-34535055\n.\n. I think CPCA can do whitening. If not, the new preprocessor should do things in the same careful way as PCA does: SVD on covariance matrix, or SVD on feature matrix, based on N or D is larger.\nBut best would be to use CPCA (with corresponding mode) here, also in the ICA classes\n. One can whiten in a lot of different ways. What did you do? If your implementation adds something, is faster, or more flexible, then it might be worth having it too.\nBut we need to be clear on that, so why not send a PR.\nPCA also uses eigen libs mostly. \n. See the discussion at #1951 \n. We want to use the CPCA class here. It has a whitening option, that will take care of everything and handles doing the transformation with both EVD/SVD on data/covariance matrix based on dimensions. However, I just tried it, and it doesnt work properly (resulting covariance is diagonal, but not the identity)\n\nHere is a list of things to do:\n- Add unit tests for PCA that make sure whitening works for any case (all decompositions, all modes, both apply_to_feature_matrix and apply_to_feature_vector. Fix bugs that are there currently\n- Add a wrapper class CWhiteningPreprocessor that has a member of type CPCA. We do not want the whitening class be a dimension reduction preprocessor, thats why it should not inherit from PCA, but from DensePreprocessor\n- Offer all the different options how to decompose the matrices from PCA in the constructor. Hide everything that is related to dimension reduction, etc.\n- Carefully implement all other abstract methods from CDensePreprocessor, we might have to restrict ourselves to float64 due to the use of eigen, but not sure.\n- Carefully unit test all possible cases for doing whitening, similar to CPCA\n- add a python modular example how to use api (for all cases)\n. This is about to be solved in #2071\nNow the whitening preprocessor should just inherit from CPCA and initialise things with the parameters that will correspond to whitening. Very easy.\n. fixed\n. fixed\n. This only concerns libshogun and all tests pass locally, merging\n. This only concerns libshogun and all tests pass locally, merging\n. Indeed! Would be cool to have a unified interface....\n. Indeed! Would be cool to have a unified interface....\n. nice! This was one of the last bits :)\nMerging since failures are not due to this addition\n. nice! This was one of the last bits :)\nMerging since failures are not due to this addition\n. General comment: I am not too sure to drop all these example for sparse regression, ARD, students T kernel, and sum kernels. These things should be illustrated somewhere, so if you drop them, pls offer an example where its easy to do all those things, or even better a notebook.\nMaybe undo deletion of regression examples for now?\n. General comment: I am not too sure to drop all these example for sparse regression, ARD, students T kernel, and sum kernels. These things should be illustrated somewhere, so if you drop them, pls offer an example where its easy to do all those things, or even better a notebook.\nMaybe undo deletion of regression examples for now?\n. This all looks very nice!\nI really like the results of your efforts here! That are a lot of cleanups and make model-selection a lot easier.\nGood work!\n(waiting for travis and then merging)\n. travis went green, merging\n. That is already done for on a low-level basis and for all shogun classes :)\nI am currently compiling python to try it out\n. That is already done for on a low-level basis and for all shogun classes :)\nI am currently compiling python to try it out\n. python modular is not working here, using a fresh build :(\nI will try it over travis\n. python modular is not working here, using a fresh build :(\nI will try it over travis\n. yep! :)\n. yep! :)\n. This is an attempt to fix #1644 \n. This is an attempt to fix #1644 \n. and now also is a fix for #1654 \nAdded unit-tests to keep this working\n. I re-enabled the integration tests on travis. Since we soon will be able to get around this numerical hassle, I think its good to have them on\n. and finally, this seems to work.\nThere is one case left where I get an error with the new equals, which was not there before:\nkernel_histogram_word_string_modular.py\nleaving TParameter::compare_ptype(): PT_FLOAT64: data1=-404347826153043460096.000000, data2=-404347826153043525632.000000\nBut this is clearly not the problem of CSGObject::equals, but a numerical range problem (I wonder why that even worked before)\nBTW Ascii serialization is very in exact!\n. Weird, this is exactly how it happened to me.\nMaybe try changing a signature of a method?\n. merging for now, lets see how it works\n. Pls remove all cell outputs from the commit. We will add them automagically :)\nThis will keep the commit history/diff cleaner\n. There is something weird going on, have a look at the times needed for the tests\nhttps://travis-ci.org/shogun-toolbox/shogun/jobs/11665542 \nFrom test 97, they suddenly take a minute ...\n. Ok, this should in fact be fine, but what is happening with the integration tests.\nFrom 97, happens all the time \nhttps://travis-ci.org/shogun-toolbox/shogun/jobs/11766375\nIf this wouldnt happen, I think we would be fine in terms of runtime.\nIt doesnt happen locally on my machine\n. I just realised it happens on my machine with latest git. Will investiagate ... assigning to me\n. What's weird is that only a few take so long.\nI might also be the use of equals in debug mode..... checking soon\n. grid-search is an expensive thing, I will reduce the number of parameters a bit\n. see #1661 \n. thanks!\n. Very cool that ours is so much faster in exact regression :)\nOne way to compare the correctness of the results (for both EP and other things) is in fact to plot the surface of the log-marginal likelihood for a 2D parameter example (IE GaussianKernel with and its scale, fixed noise or similar)\nIf you evaluate it on a fixed grid around the mode in both Shogun and GPML, you can a) visually compare whether they look similar b) compute some divergence measure between those array c) see if the modes are maybe ambiguous (ie there are some longer ridges on which the ML is almost equal and the gradient is almost zero) this would explain the different results d) it looks very cool to plot this and the path of the optimization for the notebook\n. I do not really like feature matrices since then we would have to know what others can do.\nI think we should focus on what we can do, and do that in a more interesting looking form, i.e., a hand-written list of features. In fact, this should not be a list but more a general overview of what is possible -- nice to read, and well structured. We can then grow this, and also link against the notebooks or so.\nI will try do start something before the release, at least for the new features.\n. Ok, if people like that, we should do it.\nI guess I am just concerned about the usefulness of plain lists of things. What does model-selection (x) mean? \nAlso seems a lot of work to maintain such lists. But if its easy to extend, it is a good thing to do.\nI will also try to write a short paragraph on each feature/machine and explain some contexts and problems for what it can be used. Like in the \"Implemented method\" in doxygen, but much less list-like/technical and more high-level.\nScikit learn website is a great example of useful feature lists\n. Wacken Open Air? ;)\n. @vigsterkr The question is who renders the notebook.\nThere are three steps:\n1. Compute all output cells in the notebook (we can do that ourselves)\n2. Put the resulting file somewhere to get a fixed link\n3. Put this link onto http://nbviewer.ipython.org/ which produces a static browser version of the notebook (using the nbconvert tool of iypthon)\nThe problem I was talking about is that step 3 sometimes fail due to overload of this webservice. My suggestion is to have the same thing running for dealing with our notebooks (no external ones allowed)\nSee what I mean?\n. Cool, I did not know we already did that.\n. cool thanks! :)\n. This is very nice work!\nThanks for the method, I will use them in the notebook.\nHow are the real life examples coming along? Now we can use strings which is very cool since nobody else can ;)\n. +1\n. fixed the one in QuadraticTimeMMD\n. Thanks\n. Why would this Help?\nAlso, it was fine before the go model selection refactoring patch....\nbut maybe I miss Something?\n. Cool! Thanks\n. Haha :)\nAnd Ryanair, just worked on the mmd notebook from them...\n. Thanks!\n. @votjakovr any thoughts on this?\nI guess a quick fix (for all of those) is to remove parallel derivative evaluation\n. This should not be necessary, but who knows\n. @lamday any ideas about this?\n. Thanks for the patch and sorry for the delay from my side.\nYou should report the leak to the lib developers, maybe they know how to avoid it or fix it :)\nI totally agree with all the other changes in the PR\n. Ah thats a very nice idea so save trained models !\n. Ah thats a very nice idea so save trained models !\n. looks pretty reasonable -- did you encounter that problem?\nBut in fact, I would prefer that ensure_valid does behave better in some way\n@lisitsyn what are your thoughts?\n. looks pretty reasonable -- did you encounter that problem?\nBut in fact, I would prefer that ensure_valid does behave better in some way\n@lisitsyn what are your thoughts?\n. thinking about it some more, I actually agree with @sonney2k that we should not allow label instances where all labels are the same. It doesnt make any sense to allow for this and probably leads to problems like singularities.\nThis is not true for the predictions, they can be whatever.\n@josepablog you should check for your nonsense cases in your codes. Dont do supervised learning with only one label. You will only confuse yourself.\nIf this happens in cross-validation, you should either\n- use a lower number of folds\n- if you haven't yet, use stratified cross validation which makes sure that the ratio of labels is the same throughout all folds. This is reccomended in general. I might even add a warning to std cross-validation in classification. If stratified cross-validation fails, use less folds.\n. thinking about it some more, I actually agree with @sonney2k that we should not allow label instances where all labels are the same. It doesnt make any sense to allow for this and probably leads to problems like singularities.\nThis is not true for the predictions, they can be whatever.\n@josepablog you should check for your nonsense cases in your codes. Dont do supervised learning with only one label. You will only confuse yourself.\nIf this happens in cross-validation, you should either\n- use a lower number of folds\n- if you haven't yet, use stratified cross validation which makes sure that the ratio of labels is the same throughout all folds. This is reccomended in general. I might even add a warning to std cross-validation in classification. If stratified cross-validation fails, use less folds.\n. on it ...\n. on it ...\n. solved in #1714 \n. solved in #1714 \n. Thanks a lot for reporting!\nLooks like there are loads of uninitialised memory reads. This might well cause the crash.\nSo either you found a bug or there are uninitialised values in your matrices (better double check)\nI currently dont have the time to check this, but it maybe someone else can help?\n@lisitsyn maybe?\n. I think this is caused by an assertion that @sonney2k recently added. Maybe this should be taken out again until the bug is fixed?\n. Hi!\nWhy not instead just use two Gaussians that are close to each other (to generate some overlap)\nI am fine to merge though since then we have at least something :)\nMulticlass with 5 classes produces an error on my machine\nMy general suggestion here is to use a grid of (slightly overlapping) Gaussians and assign one label to each, works for two and multiclass. There is already a class to generate such data, CGaussianBlobsDataGenerator\n. Just looked at it -- nice!\nI would make the plots larger\nfigure(figsize=(18,5))\nor something\nAnd some minimal mathematical definition of whats going on would be nice.\nAnd some more algorithms - in particular their differences etc.\nMaybe some link to wikipedia for algorithms/general ML words used?\nBut pls merge it as soon as possible people will then look at it when we release tomorrow\n. Could you create a nbviewer link?\npaste the nb file into github gist and then put that link in the nbviewer page\n. Could you create a nbviewer link?\npaste the nb file into github gist and then put that link in the nbviewer page\n. Cool, thanks! \nA few comments (I know I am greedy :) )\n- could you link to the current class documentation whenever you mention a class name? its done with the html < a > command and href=\"...\"\n- Could you write a bit more in the abstract. For example that you train an online SGD SVM? \n- Also, if you outlined the general strategy of what you will do (steps) that would be cool \n- Why is streaming features cool?\n- say a few words about the interfaces, for example start/end parser, why that is etc\n- you compute the norm of w, could you say why you do this and what one can see? (also pls use print rather than just dumping the number)\n- for evaluation, use CEvaluation, which can compute the accuracy, which is much neater and cleaner in terms of output.\n- maybe some fancy plot? (dont know which one though)\n. Cool, thanks! \nA few comments (I know I am greedy :) )\n- could you link to the current class documentation whenever you mention a class name? its done with the html < a > command and href=\"...\"\n- Could you write a bit more in the abstract. For example that you train an online SGD SVM? \n- Also, if you outlined the general strategy of what you will do (steps) that would be cool \n- Why is streaming features cool?\n- say a few words about the interfaces, for example start/end parser, why that is etc\n- you compute the norm of w, could you say why you do this and what one can see? (also pls use print rather than just dumping the number)\n- for evaluation, use CEvaluation, which can compute the accuracy, which is much neater and cleaner in terms of output.\n- maybe some fancy plot? (dont know which one though)\n. no idea currently, but if the other things are in, that would already me a massive improvement\n. no idea currently, but if the other things are in, that would already me a massive improvement\n. Very nice work, judging from the code.\nTravis seems to not be happy due to something else, I'll restart and merge once passed\nthanks a lot, will add the example to the notebook again\n. Dear @Pepslee \nplease file a proper error description, with details on what is the problem for you, this way you will get a proper answer.\n. @vigsterkr Viktor, any news on the matlab with cmake?\nFor now the quickfix (unfortunately) is to use a modular interface, such as octave.\n. @vigsterkr Viktor, any news on the matlab with cmake?\nFor now the quickfix (unfortunately) is to use a modular interface, such as octave.\n. I asked because Matlab worked before cmake.\nBut sorry, I assumed you knew that stuff. Maybe the guy can help, lets see.\nWe have to think about static interfaces anyways - what to do, how to\nmaintain, etc. But later .....\n2014-02-22 5:47 GMT+00:00 Viktor Gal notifications@github.com:\n\n@karlnapf https://github.com/karlnapf why me? i know square about\nmatlab static interface :(\ni've asked several times how to generate .mex file but i never got an\nanswer...\n\nReply to this email directly or view it on GitHubhttps://github.com/shogun-toolbox/shogun/issues/1777#issuecomment-35795551\n.\n. I asked because Matlab worked before cmake.\nBut sorry, I assumed you knew that stuff. Maybe the guy can help, lets see.\n\nWe have to think about static interfaces anyways - what to do, how to\nmaintain, etc. But later .....\n2014-02-22 5:47 GMT+00:00 Viktor Gal notifications@github.com:\n\n@karlnapf https://github.com/karlnapf why me? i know square about\nmatlab static interface :(\ni've asked several times how to generate .mex file but i never got an\nanswer...\n\nReply to this email directly or view it on GitHubhttps://github.com/shogun-toolbox/shogun/issues/1777#issuecomment-35795551\n.\n. @yage99 thanks a lot for that!\n\n@iglesias it would be great to have that in some README - there are so many questions on this on our mailing list.\n. @yage99 thanks a lot for that!\n@iglesias it would be great to have that in some README - there are so many questions on this on our mailing list.\n. Hi!\nKernelRidgeRegression is enabled in the modular interfaces. If you compiled shogun from source, make sure to compile with LAPACK support, otherwise, the class is not included.\nHope that helps!\n. That's weird. Could you provide some background info's like os and shogun Version?\n@sonney2k @vigsterkr  Ideas?\n. nice! :)\n. Maybe even a GSoC project\n. I will soon start looking into this a bit more seriously.\nWhats your opinion on serialisation? As this is very closely related to the parameters in Shogun...\n. I will soon start looking into this a bit more seriously.\nWhats your opinion on serialisation? As this is very closely related to the parameters in Shogun...\n. Hi!\nThanks a lot, nice work!\nWhat about checking whether the indices in all folds cover all indices that are available, to detect whether indices were dropped on the way. Also, since you start with random data, maybe repeat the procedure a couple of times?\nSee a few minor comments.\nMight I ask, how did you got interested in doing this?\nThanks again!\nHeiko\n. Hi,\nyes the runs are the same, I missed that, sorry.\nYour help is appreciated a lot here! If you are interested in working more\non cross-validation based stuff, let me know. We have lots of open problems\nand ideas what to do.\nExamples:\n-Make Cross-validation work with all CMachine implementations (some are\nbroken, GPs, KNN, etc)\n-Clean up the model selection framework (grid-search mainly) and make the\nsyntax a bit nicer (see modelselection_*examples)\n-Implement new cross-validation schemes (leave one out, although thats just\na special case of the existing ones)\n-I also started working on a ipython notebook for cross-validation, I will\npush that soon, you could extend that.\nLet me know whether that sounds interesting :)\nHeiko\n2013/12/17 Saurabh7 notifications@github.com\n\nhi,I am sorry, i missed something in the workflow(non -fast forward\nupdates)... anyways i sent a new pull request #1795https://github.com/shogun-toolbox/shogun/pull/1795\nAren't the runs same as repeating the procedure??\nI was looking for something to contribute to machine learning and i have a\nlot of free time... :)\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/shogun-toolbox/shogun/pull/1793#issuecomment-30731165\n.\n. Hi,\n\nyes the runs are the same, I missed that, sorry.\nYour help is appreciated a lot here! If you are interested in working more\non cross-validation based stuff, let me know. We have lots of open problems\nand ideas what to do.\nExamples:\n-Make Cross-validation work with all CMachine implementations (some are\nbroken, GPs, KNN, etc)\n-Clean up the model selection framework (grid-search mainly) and make the\nsyntax a bit nicer (see modelselection_*examples)\n-Implement new cross-validation schemes (leave one out, although thats just\na special case of the existing ones)\n-I also started working on a ipython notebook for cross-validation, I will\npush that soon, you could extend that.\nLet me know whether that sounds interesting :)\nHeiko\n2013/12/17 Saurabh7 notifications@github.com\n\nhi,I am sorry, i missed something in the workflow(non -fast forward\nupdates)... anyways i sent a new pull request #1795https://github.com/shogun-toolbox/shogun/pull/1795\nAren't the runs same as repeating the procedure??\nI was looking for something to contribute to machine learning and i have a\nlot of free time... :)\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/shogun-toolbox/shogun/pull/1793#issuecomment-30731165\n.\n. Thanks again, good work!\nSome minor style issues remain (then its ready to merge)\nAnd my suggestion to check that the partitions are disjoint (=dont share indices) and cover all indices from the original labels, which would extremely useful\n. Thanks again, good work!\nSome minor style issues remain (then its ready to merge)\nAnd my suggestion to check that the partitions are disjoint (=dont share indices) and cover all indices from the original labels, which would extremely useful\n. Hi!\n\nCool!\nSo now for the (hopefully) last set of comments:\n- The test actually is about the splitting strategy, not about cross-validation (in the shogun class sense). Sorry for not having seen this before. Could you again rename to CSplitting strategy, and then have different test names for standard, stratified, etc\n- One unit test should be as simple as possible. In the two tests you have, you do check quite a few things. I think it would be good to split them. Like stratified_subset_coverage for the union test you just added, stratified_subsets_disjoint for the test for disjointness, stratified_label_ratio for testing that the label ratio in all folds is equal, etc. Do you see what I mean?\nThanks again for you cool work!\nH\n. Hi!\nCool!\nSo now for the (hopefully) last set of comments:\n- The test actually is about the splitting strategy, not about cross-validation (in the shogun class sense). Sorry for not having seen this before. Could you again rename to CSplitting strategy, and then have different test names for standard, stratified, etc\n- One unit test should be as simple as possible. In the two tests you have, you do check quite a few things. I think it would be good to split them. Like stratified_subset_coverage for the union test you just added, stratified_subsets_disjoint for the test for disjointness, stratified_label_ratio for testing that the label ratio in all folds is equal, etc. Do you see what I mean?\nThanks again for you cool work!\nH\n. I just checked the travis error, this is not due to your patch. I will merge now. If you could rename the file, split tests, that would be great.\n. I just checked the travis error, this is not due to your patch. I will merge now. If you could rename the file, split tests, that would be great.\n. Yeah so the file is currently called CrossValidation_unittests, but in fact they are for splitting strategy (which is the class passed to cross-validation), so just a file rename should be fine.\nThen splitting all indidivual tests into seperate unit-tests would be good.\nI will soon get back about some other stuff that we are interested in :)\nDo you have something that you like in particular?\nIf you want something challenging, have a look at a cross-validation example in the c++ examples and try to make it work with something that is not an SVM, like a GP or KNN.\n. Yeah so the file is currently called CrossValidation_unittests, but in fact they are for splitting strategy (which is the class passed to cross-validation), so just a file rename should be fine.\nThen splitting all indidivual tests into seperate unit-tests would be good.\nI will soon get back about some other stuff that we are interested in :)\nDo you have something that you like in particular?\nIf you want something challenging, have a look at a cross-validation example in the c++ examples and try to make it work with something that is not an SVM, like a GP or KNN.\n. This looks reasonable, nice example, feel free to submit this as a PR :)\nI remember producing a crash with X-validation and KRR, but I dont exactly remember what I did for that.\nWhat does valgrind say to your example?\nThough I think it would be good to make sure that all the training/testing/storing the model is done correctly. You could do this via emulating the x-validation by hand for a low number of folds, and then after each fold, check whether everything works as expected. Although the example is good evidence that is is true, there are sometimes subtleties that might be easily overseen (in particular, storing the classifier model after training and then changing the attached CFeatures instance afterwards sometimes causes problems since the stored model is affected, see store_model_features method of for example the CKernelMachine).\nOnce all this is done, could you also add a python-modular example for this? This way it is added to our integration testing system (which stored serialised dumps of all objects you return, have a look at the other examples) This then makes everything waterproof.\nGPs might also be worth a look :) but thats a bit more complicated and @votjakovr might have to be involved for that. I so far just implemented the store_model_features method to be empty, but that might be a bad idea.\nHeiko\n. This looks reasonable, nice example, feel free to submit this as a PR :)\nI remember producing a crash with X-validation and KRR, but I dont exactly remember what I did for that.\nWhat does valgrind say to your example?\nThough I think it would be good to make sure that all the training/testing/storing the model is done correctly. You could do this via emulating the x-validation by hand for a low number of folds, and then after each fold, check whether everything works as expected. Although the example is good evidence that is is true, there are sometimes subtleties that might be easily overseen (in particular, storing the classifier model after training and then changing the attached CFeatures instance afterwards sometimes causes problems since the stored model is affected, see store_model_features method of for example the CKernelMachine).\nOnce all this is done, could you also add a python-modular example for this? This way it is added to our integration testing system (which stored serialised dumps of all objects you return, have a look at the other examples) This then makes everything waterproof.\nGPs might also be worth a look :) but thats a bit more complicated and @votjakovr might have to be involved for that. I so far just implemented the store_model_features method to be empty, but that might be a bad idea.\nHeiko\n. You need to compile with Debug mode enabled (rather than Release)\nccmake gives you a list of options\n. You need to compile with Debug mode enabled (rather than Release)\nccmake gives you a list of options\n. Nice! Thanks!\nIf Fernandos patch fixed everything, we can merge this.\n. Nice! Thanks!\nIf Fernandos patch fixed everything, we can merge this.\n. I dont understand why this cannot be solved via reference counting/unreferencing?\n. I dont understand why this cannot be solved via reference counting/unreferencing?\n. Ok, I think we could merge this thing, but also feel free to look into this stuff (it might be horrible, such was loads of parts in the Shogun code written by me ;)\n. Ok, I think we could merge this thing, but also feel free to look into this stuff (it might be horrible, such was loads of parts in the Shogun code written by me ;)\n. Pls disable debug mode for the example and then I will merge\n. Pls disable debug mode for the example and then I will merge\n. @iglesias  I dont really know the cause for this currently, and do not have time to get into it properly (changes in end-Jan), but as for now, I would say we can put this into a github issue and then merge the example to get moving.\nMaybe duplicate is not such a good idea after all, so it might be a design flaw of mine. Thinking about it a bit more\n. @iglesias  I dont really know the cause for this currently, and do not have time to get into it properly (changes in end-Jan), but as for now, I would say we can put this into a github issue and then merge the example to get moving.\nMaybe duplicate is not such a good idea after all, so it might be a design flaw of mine. Thinking about it a bit more\n. @Saurabh7 could you issue a github bug and attach the valgrind output? and reference this pull request\n. @Saurabh7 could you issue a github bug and attach the valgrind output? and reference this pull request\n. Nice! If this fixes things, pls go and merge :)\n. Nice! If this fixes things, pls go and merge :)\n. Cool! LOO is something we definitely want.\nBut I think this can easily be done via inheriting from the std x-validation splitting class and just set the number of folds to the appropriate number (N-1) - no need to re-implement things. Or am I wrong?\n. Cool! LOO is something we definitely want.\nBut I think this can easily be done via inheriting from the std x-validation splitting class and just set the number of folds to the appropriate number (N-1) - no need to re-implement things. Or am I wrong?\n. yeah I think you would just inherit from the CrossValidationSplitting class and then in the constructor hide the number of folds argument and manually set it to N-1 in the base constructor.  This would then remove the need for any implementation (I think)\n. yeah I think you would just inherit from the CrossValidationSplitting class and then in the constructor hide the number of folds argument and manually set it to N-1 in the base constructor.  This would then remove the need for any implementation (I think)\n. Nice work! :)\n. Nice work! :)\n. Thanks!\n. Thanks!\n. Nice one! Thanks!\n. Nice one! Thanks!\n. Hey!\nAny plans for what to touch next? :)\n. Hey!\nAny plans for what to touch next? :)\n. We do. Just talked about it with Fernando and S\u00f6ren yesterday. I am\ncurrently travelling but will try to write it up in the plane. It is about\nhaving multiple views on the same CFeatures object, that are thread safe.\nThis would be incredibly useful for x-validation and also bagging/ensemble\nmethods such as random forests.\nOn 30 Dec 2013 14:41, \"Saurabh Mahindre\" notifications@github.com wrote:\n\nhi,\nI was looking at model-selection, grid search,etc...but ml isnt my major\nfield of study now so eerrr at this point Its hard to come up with\nsomething. I have a week or so of free (boring) holidays so if you guys\nhave anything substantial i can learn or help out with,pls do tell...\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/shogun-toolbox/shogun/pull/1802#issuecomment-31348318\n.\n. We do. Just talked about it with Fernando and S\u00f6ren yesterday. I am\ncurrently travelling but will try to write it up in the plane. It is about\nhaving multiple views on the same CFeatures object, that are thread safe.\nThis would be incredibly useful for x-validation and also bagging/ensemble\nmethods such as random forests.\nOn 30 Dec 2013 14:41, \"Saurabh Mahindre\" notifications@github.com wrote:\nhi,\nI was looking at model-selection, grid search,etc...but ml isnt my major\nfield of study now so eerrr at this point Its hard to come up with\nsomething. I have a week or so of free (boring) holidays so if you guys\nhave anything substantial i can learn or help out with,pls do tell...\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/shogun-toolbox/shogun/pull/1802#issuecomment-31348318\n.\n. What we are interested in are multiple views for Shogun's CFeatures\nobjects.\nLet me give you an example of what I mean with that. Say we are doing\ncross-validation for a SVM.\nWe divide data into K folds, train the SVM on K-1 of them, and then\nevaluate performance on the remaining fold.\nHowever, all those folds should come from the very same CFeatures object in\nShogun.\nOtherwise, it would be a waste copying all this data around (imagine large\ndatasets).\nTherefore, what we do is to set a view on this object, currently with a\nconcept called subsets.\nOne can add index subsets to the CFetaures instance, which then translate\nall subsequent access to the corresponding original index.\nWhen doing x-validation, the training subset contains all indices of the\nK-1 training folds.\nThe testing subset contains all indices of the testing fold.\nOnce a subset is added to a CFeatures instance, the latter behaves as it\nonly contained the indices in the subset.\nSubsets can be stacked up (subsets on subsets) which allows for nested\nsubsets.\n\nThe problem with that is the following:\nFor our example of the SVM, the trained SVM consists of a number of support\nvectors which are simply indices to the corresponding elements of the\ntraining data.\nFor the test fold in x-validation, we need however to change the subset of\nthe features from the training to the test data.\nThis means that the support vector indices that we stored in the SVM model\nare outdated.\nOur current solution is to introduce the store_model_features method which\nis called by cross validation.\nIt simply creates an internal copy or sometimes reference to the original\nfeatures object.\nThis is obviously not optimal since we are wasting memory and making things\ncomplicated if creating multiple references to the same feature data.\nThe initial idea of the subsets was to be able to change views on CFeatures\nobjects without having to change the object itself.\nHowever, in the current implementation, we effectively get the same thing -\njust without copying.\nThere are multiple ways to address this:\n1. Have a dictionary/map which maps some key (string) to a subset stack.\nEach access method can then specify the key or one could also change the\ncurrent active key.\nHowever, it is unclear how we can easily integrate this within the existing\nalgorithms without changing tons of access patterns.\nHow would the SVM know which of the current subsets to use?\nTelling it beforehand doesnt solve the problem since we are altering the\nCFeatures instance again as we are doing currently.\n1. Have a wrapper class CFeaturesView which takes an existing CFeatures\n   instance and translates all calls of that interface.\n   The subset stack could then be moved to this class and be removed from\n   CFeatures.\n   One could then, for example in x-validation, simply create multiple of\n   those wrapper/view instances, and pass them to say the SVM.\n   This would be quite clean in terms of interfacing with the cost of slight\n   overhead. Also it would be thread safe, as long as the views only allow\n   read only calls.\n   I quite like this idea in fact.\n   Downside might be complicated implementation and interaction with other\n   classes.\n   Some Shogun classes check for the type of the CFeatures instance, and this\n   additional layer might cause problems.\n   On the other hand if we restrict access, then it might work.\n   An open question to me is how to embed CFeaturesView in the class hierarchy.\n   If it inherits from CFeatures, many problems will appear when type checks\n   are done (via dyn_cast, not the implemented get_feature_type stuff, which\n   could be mapped), that might be dangerous\n   Still, this to be seems the only way.\n   S\u00f6ren might want to comment on the usage with CombinedFeatures, etc\n   Messy\n2. Another idea is to create a shallow copy of the CFeatures instance and\n   then add a subset to that instance.\n   This in fact already works with the current implementation of subsets, etc,\n   since everything is already implemented.\n   We would just have to properly implement shallow_copy (aka copy\n   constructor) for all CFeatures instances.\n   The shallow copy references the same data/attached stuff, but just\n   translates indices differently.\n   It is not thread safe though since the copied instance could be modified,\n   argh!\n   So along with the careful iumplementation of copy constructors, a careful\n   review of the feature access methods would be needed.\n   If all of them are proper const, the shallow copy can be a const object\n   which then would be thread safe, or could be made.\n   This seems the most feasible approach. Also it has the potential to a\n   clean, not too complicated solution without introducing new classes etc\nHaving a seperate instance for each view solves all problems for\nx-validation and bagging since one can simply create a new view and pass it\nto the algorithm.\nCreateing an additional view will then not change the old one that was used\nfor model trainig. train_model_features would become obsolete.\nIf you are interested in those kind of technical Shogun internals, let us\nknow.\nThis is a very interesting and challenging little project.\nData representation is also at the core of modern machine learning\nalgorithms from a practical perspective.\nYou might also have to change some parts of the x-validation code, which\nyou already showed that you like it.\nHeiko\n2013/12/30 Saurabh Mahindre notifications@github.com\n\nhi,\nI was looking at model-selection, grid search,etc...but ml isnt my major\nfield of study now so eerrr at this point Its hard to come up with\nsomething. I have a week or so of free (boring) holidays so if you guys\nhave anything substantial i can learn or help out with,pls do tell...\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/shogun-toolbox/shogun/pull/1802#issuecomment-31348318\n.\n. What we are interested in are multiple views for Shogun's CFeatures\nobjects.\nLet me give you an example of what I mean with that. Say we are doing\ncross-validation for a SVM.\nWe divide data into K folds, train the SVM on K-1 of them, and then\nevaluate performance on the remaining fold.\nHowever, all those folds should come from the very same CFeatures object in\nShogun.\nOtherwise, it would be a waste copying all this data around (imagine large\ndatasets).\nTherefore, what we do is to set a view on this object, currently with a\nconcept called subsets.\nOne can add index subsets to the CFetaures instance, which then translate\nall subsequent access to the corresponding original index.\nWhen doing x-validation, the training subset contains all indices of the\nK-1 training folds.\nThe testing subset contains all indices of the testing fold.\nOnce a subset is added to a CFeatures instance, the latter behaves as it\nonly contained the indices in the subset.\nSubsets can be stacked up (subsets on subsets) which allows for nested\nsubsets.\n\nThe problem with that is the following:\nFor our example of the SVM, the trained SVM consists of a number of support\nvectors which are simply indices to the corresponding elements of the\ntraining data.\nFor the test fold in x-validation, we need however to change the subset of\nthe features from the training to the test data.\nThis means that the support vector indices that we stored in the SVM model\nare outdated.\nOur current solution is to introduce the store_model_features method which\nis called by cross validation.\nIt simply creates an internal copy or sometimes reference to the original\nfeatures object.\nThis is obviously not optimal since we are wasting memory and making things\ncomplicated if creating multiple references to the same feature data.\nThe initial idea of the subsets was to be able to change views on CFeatures\nobjects without having to change the object itself.\nHowever, in the current implementation, we effectively get the same thing -\njust without copying.\nThere are multiple ways to address this:\n1. Have a dictionary/map which maps some key (string) to a subset stack.\nEach access method can then specify the key or one could also change the\ncurrent active key.\nHowever, it is unclear how we can easily integrate this within the existing\nalgorithms without changing tons of access patterns.\nHow would the SVM know which of the current subsets to use?\nTelling it beforehand doesnt solve the problem since we are altering the\nCFeatures instance again as we are doing currently.\n1. Have a wrapper class CFeaturesView which takes an existing CFeatures\n   instance and translates all calls of that interface.\n   The subset stack could then be moved to this class and be removed from\n   CFeatures.\n   One could then, for example in x-validation, simply create multiple of\n   those wrapper/view instances, and pass them to say the SVM.\n   This would be quite clean in terms of interfacing with the cost of slight\n   overhead. Also it would be thread safe, as long as the views only allow\n   read only calls.\n   I quite like this idea in fact.\n   Downside might be complicated implementation and interaction with other\n   classes.\n   Some Shogun classes check for the type of the CFeatures instance, and this\n   additional layer might cause problems.\n   On the other hand if we restrict access, then it might work.\n   An open question to me is how to embed CFeaturesView in the class hierarchy.\n   If it inherits from CFeatures, many problems will appear when type checks\n   are done (via dyn_cast, not the implemented get_feature_type stuff, which\n   could be mapped), that might be dangerous\n   Still, this to be seems the only way.\n   S\u00f6ren might want to comment on the usage with CombinedFeatures, etc\n   Messy\n2. Another idea is to create a shallow copy of the CFeatures instance and\n   then add a subset to that instance.\n   This in fact already works with the current implementation of subsets, etc,\n   since everything is already implemented.\n   We would just have to properly implement shallow_copy (aka copy\n   constructor) for all CFeatures instances.\n   The shallow copy references the same data/attached stuff, but just\n   translates indices differently.\n   It is not thread safe though since the copied instance could be modified,\n   argh!\n   So along with the careful iumplementation of copy constructors, a careful\n   review of the feature access methods would be needed.\n   If all of them are proper const, the shallow copy can be a const object\n   which then would be thread safe, or could be made.\n   This seems the most feasible approach. Also it has the potential to a\n   clean, not too complicated solution without introducing new classes etc\nHaving a seperate instance for each view solves all problems for\nx-validation and bagging since one can simply create a new view and pass it\nto the algorithm.\nCreateing an additional view will then not change the old one that was used\nfor model trainig. train_model_features would become obsolete.\nIf you are interested in those kind of technical Shogun internals, let us\nknow.\nThis is a very interesting and challenging little project.\nData representation is also at the core of modern machine learning\nalgorithms from a practical perspective.\nYou might also have to change some parts of the x-validation code, which\nyou already showed that you like it.\nHeiko\n2013/12/30 Saurabh Mahindre notifications@github.com\n\nhi,\nI was looking at model-selection, grid search,etc...but ml isnt my major\nfield of study now so eerrr at this point Its hard to come up with\nsomething. I have a week or so of free (boring) holidays so if you guys\nhave anything substantial i can learn or help out with,pls do tell...\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/shogun-toolbox/shogun/pull/1802#issuecomment-31348318\n.\n. Could you send me an email? I dont have your address...\n\n2013/12/30 Saurabh Mahindre notifications@github.com\n\nhi,\nI was looking at model-selection, grid search,etc...but ml isnt my major\nfield of study now so eerrr at this point Its hard to come up with\nsomething. I have a week or so of free (boring) holidays so if you guys\nhave anything substantial i can learn or help out with,pls do tell...\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/shogun-toolbox/shogun/pull/1802#issuecomment-31348318\n.\n. Could you send me an email? I dont have your address...\n\n2013/12/30 Saurabh Mahindre notifications@github.com\n\nhi,\nI was looking at model-selection, grid search,etc...but ml isnt my major\nfield of study now so eerrr at this point Its hard to come up with\nsomething. I have a week or so of free (boring) holidays so if you guys\nhave anything substantial i can learn or help out with,pls do tell...\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/shogun-toolbox/shogun/pull/1802#issuecomment-31348318\n.\n. Hi!\n\nTry enabling the bundle colpack option, for example with ccmake (although\nit should work without, currently testing locally.\nHope that helps!\nHeiko\n2014-02-11 2:23 GMT+00:00 Dominique notifications@github.com:\n\nWith both the latest release and the develop branch, I'm now getting\nUndefined symbols for architecture x86_64:\n  \"ColPack::GraphColoring::GetVertexColors(std::__1::vector >&)\", referenced from:\n      shogun::CProbingSampler::precompute() in ProbingSampler.cpp.o\n  \"ColPack::GraphColoringInterface::Coloring(std::__1::basic_string, std::__1::allocator >, std::__1::basic_string, std::__1::allocator >)\", referenced from:\n      shogun::CProbingSampler::precompute() in ProbingSampler.cpp.o\nld: symbol(s) not found for architecture x86_64\nHowever, CMake did find Colpack:\n-- The following OPTIONAL packages have been found:\n- GDB\n- BLAS\n- LAPACK\n- GLPK\n- ARPACK\n- Eigen3 (required version >= 3.1.2)\n- NLopt\n- ColPack\n- Doxygen\n- LibXml2\n- HDF5\n- CURL\n- ZLIB\n- BZip2\n- LibLZMA\n- SNAPPY\n- LZO\n- Spinlock\n- Threads\nAny idea how I can resolve this?\nThanks.\n\nReply to this email directly or view it on GitHubhttps://github.com/shogun-toolbox/shogun/issues/1804#issuecomment-34720819\n.\n. Shogun compiles without colpack on my machine. But I never used a Mac, so\nsomebody else should help here. If you find any fix yourself, please share\nit with us.\n\n2014-02-12 4:22 GMT+00:00 Dominique notifications@github.com:\n\nThe idea is to update the Homebrew http://brew.sh formula for Shogun,\nand to depend on an external colpack.\n\nReply to this email directly or view it on GitHubhttps://github.com/shogun-toolbox/shogun/issues/1804#issuecomment-34837642\n.\n. @MartinHjelm do you really need colpack? It is only used for our log-determinant estimators. You can safely leave it out if you dont want that\n. @MartinHjelm do you really need colpack? It is only used for our log-determinant estimators. You can safely leave it out if you dont want that\n. Thanks for noticing! Nothing changed in there, so it's the fault of the person who merged and then not fix it after the buildbot complained. I expect this to be a minority caused by changing a class variable.\n\nI will sort this out end of next week (back to shogun then)\n. Thanks for noticing! Nothing changed in there, so it's the fault of the person who merged and then not fix it after the buildbot complained. I expect this to be a minority caused by changing a class variable.\nI will sort this out end of next week (back to shogun then)\n. Nice, thats neat and clean. :)\nBut we need proper documentation (mentioning that one should only use read only calls) and get rid of the memory errors (as mentioned, these veryl likely come from the copy constructor)\nH\n. Nice, thats neat and clean. :)\nBut we need proper documentation (mentioning that one should only use read only calls) and get rid of the memory errors (as mentioned, these veryl likely come from the copy constructor)\nH\n. See comments, there needs to be some more documentation and the leaks have to be fixed. The current SG_UNREF lines have to go\n. @Saurabh7 still motivated to do this? :)\n. Its on my list of things to push ... but currently it cannot be merged due to lack of documentation and the SG_REF problems, the current solution is a hack which will cause problems later if we go for it.\n@Saurabh7 I will have a closer look soon and then suggest something, but could try to think about how to get rid of the memory leaks?\n. Its on my list of things to push ... but currently it cannot be merged due to lack of documentation and the SG_REF problems, the current solution is a hack which will cause problems later if we go for it.\n@Saurabh7 I will have a closer look soon and then suggest something, but could try to think about how to get rid of the memory leaks?\n. Do you still want to go for this one?\n. I am closing this one for now as it has been inactive for too long. Feel free to re-submit (with the existing issues solved)\n. Hi!\nCould you paste the debug output?\n. Hi!\nCould you paste the debug output?\n. Looks like and invalid free which speaks for a reference counting problem. The cloning itself worked, it's the destructor call.\nI have my exams very soon and will be back to shogun after Wednesday. And then sort this one out.\n. Looks like and invalid free which speaks for a reference counting problem. The cloning itself worked, it's the destructor call.\nI have my exams very soon and will be back to shogun after Wednesday. And then sort this one out.\n. Hi Roman,\ngreat catch that you made there. So things might be twice as fast after fixing ;)\nBut, is there no other way to avoid this?\nCloning the kernel will clone the features incl all data in memory.\n. Ill look into the kernel clone problem soon!\n. I like the idea of this!\nHow would you specify the parameters to update the hash for?\nMaybe just a list of things to update that one passes?\nWould you be interested in pushing that?\n. mmmh, cloning the data ..... what about manually setting a new kernel that uses the same features? would that work?\n. Nasty one. So its the vector of CSGObjects. I started playing with it, maybe Ill find a solution. That stuff is messy. But greatly spotted @lambday !\n. Yep!\n@votjakovr Let me know if there is progress on the GP stuff using kernel->clone\n. Cool! Mini-batch is the new hip word in ML :)\nYou can fix the seed for that and then check by hand whether the output is correct.\nPls no random checks in unit-tests that always comes back and bites us later\n. Cool! Mini-batch is the new hip word in ML :)\nYou can fix the seed for that and then check by hand whether the output is correct.\nPls no random checks in unit-tests that always comes back and bites us later\n. totally! Pls use a proper polymorphic OOP design for that.\n. totally! Pls use a proper polymorphic OOP design for that.\n. Nice, thanks!\nMaybe string examples next? :)\n. Grammar error. Tolerance was not yet reached......instead of doesnt reached\n. Maybe it would be good to briefly comment on the sweeps parameter in the class description of so? Like what this means and what tolerance means and there us no guarantee foe convergence\n. See marginal_likelihood_estimate method of CInferenceMethod and use EPInferenceMethod\n. The issue was positive definiteness of the covariance, not numerically though, there was something wrong. Will attach code soon.\n. Sorry that slipped through my fingers before. We should have that in.\n. Thanks for that, btw i will sooner or later dig out the code that broke\nthis ;)\nOn 11 Feb 2014 09:36, \"Roman Votyakov\" notifications@github.com wrote:\n\nOk ;)\n\nReply to this email directly or view it on GitHubhttps://github.com/shogun-toolbox/shogun/pull/1833#issuecomment-34738651\n.\n. What about reusing some of the plotting code via putting it into a\nFunction? This blows up the notebook quite a bit otherwise.\nBest!\nHeiko\nOn 3 Feb 2014 19:16, \"Parijat Mazumdar\" notifications@github.com wrote:\nI kept k=2 to illustrate the superiority of KMeans++ over simple KMeans.\nWith k=2 simple kmeans generates bad result more frequently than with k=4.\nThe references look fine to me locally. I can see both references. But in\nthe nbviewer something goes wrong. Its ok in the shogun cloud as well.\nLet me issue a fresh PR.\nRegards.\nOn Mon, Feb 3, 2014 at 8:07 PM, Fernando Iglesias\nnotifications@github.comwrote:\n\nIt looks indeed better with this example. A few suggestions:\n- What if you use k equal to four instead of two? From the plot it\n  looks much more there are four clusters, one for each of the vertices of\n  the rectangle where the data points are centred.\n- Regarding the references, why is there only one and the numeration\n  starts with 2? Also use [n] instead of [n]: (no semicolon).\nI will close this PR since it seems something went wrong and other past\nchanges were introduced.\n\nReply to this email directly or view it on GitHub<\nhttps://github.com/shogun-toolbox/shogun/pull/1834#issuecomment-33959352>\n.\n\n\nPARIJAT MAZUMDAR\nJUNIOR UNDERGRADUATE\nELECTRICAL ENGINEERING\nIIT DELHI\n\nReply to this email directly or view it on GitHubhttps://github.com/shogun-toolbox/shogun/pull/1834#issuecomment-33988991\n.\n. Good luck with the exams :)\nOn 4 Feb 2014 08:13, \"Parijat Mazumdar\" notifications@github.com wrote:\nok. I will define a function for plot. Let me do it next week. I have exams\nthis week. Thanks for your suggestion :)\nOn Tue, Feb 4, 2014 at 4:33 AM, Heiko Strathmann\nnotifications@github.comwrote:\n\nWhat about reusing some of the plotting code via putting it into a\nFunction? This blows up the notebook quite a bit otherwise.\nBest!\nHeiko\nOn 3 Feb 2014 19:16, \"Parijat Mazumdar\" notifications@github.com\nwrote:\n\nI kept k=2 to illustrate the superiority of KMeans++ over simple\nKMeans.\nWith k=2 simple kmeans generates bad result more frequently than with\nk=4.\nThe references look fine to me locally. I can see both references. But\nin\nthe nbviewer something goes wrong. Its ok in the shogun cloud as well.\nLet me issue a fresh PR.\nRegards.\nOn Mon, Feb 3, 2014 at 8:07 PM, Fernando Iglesias\nnotifications@github.comwrote:\n\nIt looks indeed better with this example. A few suggestions:\n- What if you use k equal to four instead of two? From the plot it\nlooks much more there are four clusters, one for each of the vertices\nof\nthe rectangle where the data points are centred.\n- Regarding the references, why is there only one and the numeration\nstarts with 2? Also use [n] instead of [n]: (no semicolon).\nI will close this PR since it seems something went wrong and other\npast\nchanges were introduced.\n\nReply to this email directly or view it on GitHub<\n\nhttps://github.com/shogun-toolbox/shogun/pull/1834#issuecomment-33959352\n\n.\n\n\nPARIJAT MAZUMDAR\nJUNIOR UNDERGRADUATE\nELECTRICAL ENGINEERING\nIIT DELHI\n\nReply to this email directly or view it on GitHub<\nhttps://github.com/shogun-toolbox/shogun/pull/1834#issuecomment-33988991\n.\n\n\nReply to this email directly or view it on GitHub<\nhttps://github.com/shogun-toolbox/shogun/pull/1834#issuecomment-34011524>\n.\n\n\nPARIJAT MAZUMDAR\nJUNIOR UNDERGRADUATE\nELECTRICAL ENGINEERING\nIIT DELHI\n\nReply to this email directly or view it on GitHubhttps://github.com/shogun-toolbox/shogun/pull/1834#issuecomment-34038473\n.\n. Thanks for reporting (all of those)\nI dont think that those are related.\nIs the output of the valgrind stored somewhere? I could not get it from the page.\nBut I can also just reproduce locally\n. Any progress here? Can we merge?\n. @thoralf said this is fine. Though not all changes are unit tested. But he is out of time and too complicated for entrance task.\n\nI suggest  you add a little item in the NEWS file :)\n. sweet thanks!\n. Hi\nWe know about this problem. A workaround is to install the hdf5 serial\npackages.\nBut a better solution would be to handle this properly. Stay tuned.\nHeiko\nOn 8 Feb 2014 06:53, \"beew\" notifications@github.com wrote:\n\nHello, I am trying to build shogun from git (feb8,2014) and ran into\nproblem with hdf5 while running 'make'. The terminal outputs:\n\" Building CXX object src/shogun/CMakeFiles/libshogun.dir/io/HDF5File.cpp.o\nIn file included from /usr/include/hdf5.h:24:0,\nfrom /home/bee/Downloads/shogun-build/shogun/src/shogun/io/HDF5File.cpp:17:\n/usr/include/H5public.h:63:20: fatal error: mpi.h: No such file or\ndirectory\ninclude \"\nIt is caused by hdf5 have openmpi enabled (I installed the packages\nhdf5-openmpi-dev and hdf5-openmpi7 instead of hdf5-dev and hdf5-7 because\nthey are required by another program)\nI am wondering if it is possible to compile shogun without hdf5?\nThanks.\n\nReply to this email directly or view it on GitHubhttps://github.com/shogun-toolbox/shogun/issues/1856\n.\n. Viktor, pls could you do something about this?\nI had the problem multiple times on clusters and other people have the same\nproblem, we cannot just blame it on the mpi package.\n\n2014-02-21 16:32 GMT+00:00 beew notifications@github.com:\n\nAny update? Still can't figure out how to add path to INCLUDES according\nto sonnety2k's post 12 days ago.\n\nReply to this email directly or view it on GitHubhttps://github.com/shogun-toolbox/shogun/issues/1856#issuecomment-35747112\n.\n. Viktor, pls could you do something about this?\nI had the problem multiple times on clusters and other people have the same\nproblem, we cannot just blame it on the mpi package.\n\n2014-02-21 16:32 GMT+00:00 beew notifications@github.com:\n\nAny update? Still can't figure out how to add path to INCLUDES according\nto sonnety2k's post 12 days ago.\n\nReply to this email directly or view it on GitHubhttps://github.com/shogun-toolbox/shogun/issues/1856#issuecomment-35747112\n.\n. @vigsterkr I got that, but would it be possible to add an option to check for hdf5 and if the header is not found, dont compile that?\n. @vigsterkr I got that, but would it be possible to add an option to check for hdf5 and if the header is not found, dont compile that?\n. Added an option to disable present hdf5 in #2656\n. Added an option to disable present hdf5 in #2656\n. Check the ccmake command, which shows you all options, or grep HDF5 in the CMakeLists.txt\n\nThen you will find that the option is -DUSE_HDF5=OFF\n. Noooo breaks integration tests. I wanted to see which ones via travis ;)\nOn 9 Feb 2014 19:22, \"Soeren Sonnenburg\" notifications@github.com wrote:\n\nMerged #1857 https://github.com/shogun-toolbox/shogun/pull/1857.\n\nReply to this email directly or view it on GitHubhttps://github.com/shogun-toolbox/shogun/pull/1857\n.\n. no this is a hack since it does not get rid of the problem that the original memory was uninitialised (which it should not be) The error must be more upstream, sorry :(\n. whooo, that was an evil one, well spotted :)\n. Was there actually an error with this? Thanks!\n. Very nice, clean work!\nI am happy to merge this. I just suggest to clean up the old-shogun-style kernel code a bit to use our new SG* memory structures which makes things much easier to read. See comments.\nThanks for that!\n. Indeed ;)\n\n2014-02-12 1:50 GMT+00:00 Soumyajit De notifications@github.com:\n\n@karlnapf https://github.com/karlnapf I should also change the name!\nSSK(Subsequence String Kernel)StringKernel is redundant :D\n\nReply to this email directly or view it on GitHubhttps://github.com/shogun-toolbox/shogun/pull/1862#issuecomment-34830729\n.\n. ok!\n. +1 from my side for this!\n. As for format, keep in mind we can serialise objects in Shogun\n. I like that a lot.\nWhat about actually doing PCA on the full dataset in the notebook? And then\ncompare KMeans performance for different numbers of latent dimensions?\n\nKeep on the good work!\nHeiko\n2014-02-14 16:52 GMT+00:00 Parijat Mazumdar notifications@github.com:\n\n\nadded real world example for KMeans notebook view the final notebook\n  at http://nbviewer.ipython.org/gist/mazumdarparijat/8765367\n\n\nYou can merge this Pull Request by running\ngit pull https://github.com/mazumdarparijat/shogun KMeans_notebook\nOr view, comment on, or merge it at:\nhttps://github.com/shogun-toolbox/shogun/pull/1870\nCommit Summary\n- added real data example in KMeans notebook\nFile Changes\n- M doc/ipython-notebooks/clustering/KMeans.ipynbhttps://github.com/shogun-toolbox/shogun/pull/1870/files#diff-0(301)\nPatch Links:\n- https://github.com/shogun-toolbox/shogun/pull/1870.patch\n- https://github.com/shogun-toolbox/shogun/pull/1870.diff\n\nReply to this email directly or view it on GitHubhttps://github.com/shogun-toolbox/shogun/pull/1870\n.\n. Very nice notebook!\n\nBTW could you put the paper link in the references, add a proper citation (as in a paper), and then put the link in that? PDF links change often...\n. Nice! Let us know if you have questions\n. Nice! Let us know if you have questions\n. Hey,\nvery cool that you are working on this. I have a few points (that are some more work, but would be really great to have). I would rather solve all of them at once to avoid mixtures of eigen3 and lapack code.\nHere we go:\n- Eigen3-ise everything, not just this method\n- For the unit testing, unit test every method (that does linear algebra). Multiple things work together here and its better to also test at this lower level, not just the result.\n- Unit tests should compare against some reference code (the current one is a straw man). I suggest the code of the \"Bayesian Reasoning and Machine Learning\" by David Barber. Matlab code (including PCA) can be downloaded here:  http://web4.cs.ucl.ac.uk/staff/D.Barber/pmwiki/pmwiki.php?n=Brml.Software\n- This brings up another very important point. There are two way to do PCA: Via Eigendecomposition of the feature covariance matrix, and via SVD on the feature matrix. One of the methods scales in N^3 and one in D^3 where N is number of data, and D is their dimension. We want to always use the one that is better (depending on D>N). This should be possible to set in and enum PCA_EIGENDECOMPOSITION, PCA_SVD, PCA_AUTOMATIC The last one should be the default. Both methods are implemented as 3 liners in the PCA.m code of the book I pasted above. This will make sure that PCA scales up properly.\n- A notebook comparing those two would be the glorious final :)\nAsk me if any questions.\n. Update: In order to get this one merged soon (pls go ahead S\u00f6ren)\nI made this an entrance task, see #1876 \n. Travis failed. I think this is because you need to send a seperate data patch after this one is merged. Not sure though ... @sonney2k  ?\n. @mazumdarparijat sorry for the delay in the data merge!\nUnable to checkout 'd929d871f1d9202ea43090ce105cfecb34ae6d42' in submodule path 'data'\nSo it still has problems finding the data. Correct hash?\n. Anyways, thanks for the explanation! Thats what I meant with backward transpose :) I think thats a very elegant solution now.\nAgain, once this works with the data, before doing anything else, make sure to unit test all cases of dimensions to really have this waterproof\n. something is wrong here, d929d871f1d9202ea43090ce105cfecb34ae6d42 does exist in data but travis complains that it doesnt. @mazumdarparijat could you double check the data versions ?\n. @mazumdarparijat removing the realloc is a fake fix :)\nThe full matrix is being stored in memory (although not visible since you change the num_rows variable). However, we want to free it, the memory it not used anymore - even if handled correctly later on. The problem is the automatic memory handling in SGMatrix, realloc seems to register the memory block differently, so the SGMatrix tries to free the old block. Have a look if you can change that. SGReferencedData is the place to look. You'll learn some quite some things about Shogun this way.\nWow I did not expect this thing to be so tricky ;)\nThanks for the unit tests!\n. Brilliant! Ready to merge this once the memory issue is resolved :)\n. @mazumdarparijat lets do the discussion here, otherwise its hard to reconstruct\nHere is a link to your mail be the sake of completeness http://permalink.gmane.org/gmane.comp.ai.machine-learning.shogun/4227\nI left some comments at https://gist.github.com/mazumdarparijat/9150465\n. @mazumdarparijat lets do the discussion here, otherwise its hard to reconstruct\nHere is a link to your mail be the sake of completeness http://permalink.gmane.org/gmane.comp.ai.machine-learning.shogun/4227\nI left some comments at https://gist.github.com/mazumdarparijat/9150465\n. Wow, I just learned something, I did not know that realloc is not guaranteed to keep the pointer constant if the new block is smaller. Just got my head around that (and its irritating to me in fact, but whatever). No way of signalling the other pointers to change to the newly allocated one. Sorry about pushing you in the wrong direction.\n@sonney2k I think the first option is the one to go, passing only references around doesnt feel good to me. Just to make sure whether I got you correct. The new one however also doesnt solve our original problem of having an in-place memory footprint. Realloc on the global block just creates another new block, copies things over, and then frees the old one (if allocation of new was successful).\nSo, as there is no way (without using std::vector) of making a memory block smaller in-place, we could just as well do this in the PCA code via creating new SGMatrix instances without messing around with global objects - same memory costs.\n@mazumdarparijat Could you just do this? Save the results in a new matrix, then replace the old one (as you had in the very first instance). I dont like the in-place option where we just change the \"visible\" size of the matrix while the whole thing is still in memory. The whole point of PCA is to reduce the dimensions/memory that is stored. However, since that might be a matter of perspective, why not have both behaviours implemented via a class enum PCA_MEMORY_MODE={REALLOCATE, IN_PLACE} with proper documentation and numbers how much memory the PCA operation needs for each, where REALLOCATE is the std behaviour.\n- REALLOCATE: old matrix + new matrix for a short time for first - new_matrix afterwards \n- IN_PLACE: old_matrix before and afterwards with no short bump.\n. Wow, I just learned something, I did not know that realloc is not guaranteed to keep the pointer constant if the new block is smaller. Just got my head around that (and its irritating to me in fact, but whatever). No way of signalling the other pointers to change to the newly allocated one. Sorry about pushing you in the wrong direction.\n@sonney2k I think the first option is the one to go, passing only references around doesnt feel good to me. Just to make sure whether I got you correct. The new one however also doesnt solve our original problem of having an in-place memory footprint. Realloc on the global block just creates another new block, copies things over, and then frees the old one (if allocation of new was successful).\nSo, as there is no way (without using std::vector) of making a memory block smaller in-place, we could just as well do this in the PCA code via creating new SGMatrix instances without messing around with global objects - same memory costs.\n@mazumdarparijat Could you just do this? Save the results in a new matrix, then replace the old one (as you had in the very first instance). I dont like the in-place option where we just change the \"visible\" size of the matrix while the whole thing is still in memory. The whole point of PCA is to reduce the dimensions/memory that is stored. However, since that might be a matter of perspective, why not have both behaviours implemented via a class enum PCA_MEMORY_MODE={REALLOCATE, IN_PLACE} with proper documentation and numbers how much memory the PCA operation needs for each, where REALLOCATE is the std behaviour.\n- REALLOCATE: old matrix + new matrix for a short time for first - new_matrix afterwards \n- IN_PLACE: old_matrix before and afterwards with no short bump.\n. @mazumdarparijat btw you are doing great work here! :)\nLooking forward to the tuned PCA stuff\n. @mazumdarparijat btw you are doing great work here! :)\nLooking forward to the tuned PCA stuff\n. No worries, and good luck with your presentation!\n. No worries, and good luck with your presentation!\n. @mazumdarparijat Great work! :)\nMy last annoying comments are all related to coding style, a minor problem with initialisers, and code redundancy. Ready to merge very very soon!\n. @mazumdarparijat Great work! :)\nMy last annoying comments are all related to coding style, a minor problem with initialisers, and code redundancy. Ready to merge very very soon!\n. travis seems fine, so lets merge. I will have an eye on the buildbot and correct in case of problems.\nThat was great and interesting work! Looking forward to the next thing you touch :)\n. travis seems fine, so lets merge. I will have an eye on the buildbot and correct in case of problems.\nThat was great and interesting work! Looking forward to the next thing you touch :)\n. Oh yeah, and please mention your fix in the news\n-Fix a memory error and added new in-place/re-alloc option in PCA by YOURNAME\n. Oh yeah, and please mention your fix in the news\n-Fix a memory error and added new in-place/re-alloc option in PCA by YOURNAME\n. @mazumdarparijat btw we are changing to BSD soon, have a look at\nhttps://github.com/karlnapf/shogun/blob/develop/src/shogun/statistics/QuadraticTimeMMD.h\nand feel free to change the header of the unit tests :)\nand again, mention yourself in the NEWS\n. @mazumdarparijat btw we are changing to BSD soon, have a look at\nhttps://github.com/karlnapf/shogun/blob/develop/src/shogun/statistics/QuadraticTimeMMD.h\nand feel free to change the header of the unit tests :)\nand again, mention yourself in the NEWS\n. See some simple example how to interface stan's distribution implementation from c++\nThis is a first step.\nhttps://gist.github.com/karlnapf/9354664\n. In addition, a very important aspect (and subtask) is to integrate stan's source into Shogun.\n- have a cmake option to bundle stan or to provide an installation to link against\n- stan comes with its own eigen3, we however want to use Shogun's\n- stan also depends on boost, so in case stan is bundled, we need to add a boost dependency\n- add linker and compiler flags if it is used\n- include guards\n- etc, see for example the way eigen3 is bundled.\n@sonney2k and @vigsterkr might have comments here\n. @vigsterkr mmmmh ok. What else could we do there? I just want to be able to access some of stan's code for automatic differentiation, distributions and mcmc. But the result can also be negative, just investigating.....\n. Definitely, but this is a big undertaking. Better start smaller\n. Another useful thing to do is to have a method that automatically selects the number of principal components (if the user desires) One could for example specify a fraction of all principal components variance. Say I want to capture 90% of the variance in the input data, then I select first few principal components until their cumulative sum of their eigenvalues is equal to 90% of the sum of all eigenvalues.\n. #1915 solved all of the above points\n. #1915 solved all of the above points\n. whitening doesnt work properly, the resulting covariance is not identity for some cases\n- whitening should be checked in unit tests for all possible cases\n- a python modular API example on all cases would be very helpful too\n. See also #1621\n. The covariance matrix is diagonal (which is good) but not the identity, so the scalling among the dimensions somehow is wrong (at least for square matrices with pca=PCA(True)\nAs you wrote all the tests for PCA already, could you try to fix that, should be quick and easy\n. Any updates on the whitening bug here?\nThere should also be a unit test for that.\n. in python? That is weird indeed. Have you double checked the header file definitions?\n. I dont know much about swig. Maybe file an issue, lets see what @sonney2k says to this\n. Yes, though the link against the linalg interface should still be done. But thats another task!\n. Not done,\nshould also be rephrased not to use Eigen3, but rather our linalg framework\n. There is so much literature on this topic. In particular PCA, so this notebook should be nice and clear and full of cool plots :)\n. @kislayabhi Looks like your pca repository is well suited to be used as a ressource :)\n. See also #1915 and all its additions to the PCA class. Those should be documented with examples where the pros and cons of the methods are discussed\n. See also #1915 and all its additions to the PCA class. Those should be documented with examples where the pros and cons of the methods are discussed\n. The old PCA code decomposed the covariance matrix of the data.\nFor 100x100 images, this is a 10000-dimensional square matrix, which causes problems when decomposing. @mazumdarparijat 's new SVD based PCA should solve this. As he said, computing the SVD of the data matrix should be cubic in the number of points, so that should work.\nIf it doesn't please post some simple python code (possibly on toy data) that illustrates the problem in an isolated way.\nThanks!\n. I just tried the new PCA implementation on datasets with\n- D=100000, N=100\n- N=100000, D=100\nand both work quite fast plain vanilla (using the std constructor p=PCA()). So that works - and is a major improvement to what we had before :)\n. Kernel PCA is still missing, but just a small addition to the existing PCA one.\n@kislayabhi interested on adding some things? we have a graphical example that we could put in\n. Yeah sure, basically we want this here:\nhttp://scikit-learn.org/stable/auto_examples/decomposition/plot_kernel_pca.html\nIt might also be worth re-organising the KPCA code and make it have the same nice properties as your kick-ass PCA implementation, and also phrased again the linalg framework!\n. We long have a PCA notebook. thanks!\n. Done and online\n. Thanks for reporting, Ill assign Bj\u00f6rn for now, @besser82 could you push this?\n. This can be done with the new stuff for checking floating point equality\n. Hi\nNice, so thats some macro hacking. Check the definitions of SG_ADD\nThe goal is that I can write\nSG_ADD(&m_compact, \"compact\", \"Compact enabled option\");\nfor\nSG_ADD(&m_compact, \"compact\", \"Compact enabled option\",MS_NOT_AVAILABLE, GRADIENT_NOT_AVAILABLE);\n(which is similar to the current behaviour that one can leave out the GRADIENT option and its set to false)\n. @lambday ah nice, I see, great work with this array unit test.\nOne thing I would like to see though is some more low level tests, have a look into the parameter tests, there is an explicit test for every case in copy_ptype. This should be extended to SG_VECTOR/SG_MATRIX where the primitive is sgobject. \nBut I will merge this for now, could you add these tiny tests also?\nThanks a ton on this!\n. @votjakovr does this fix the issues you had?\n@lambday what is the issue fixed by this?\n. Hi @mazumdarparijat I think this is a good fix! Lets see what travis says\nUnit tests for those cases would be amazing, in particular comparison to std implementation!\nAlso, dont forget to add you name in the NEWS file with the bugfixes :)\n. Travis says all is fine, but lets wait for the unit tests :)\n@mazumdarparijat thanks for you great work!\n. ok, lets wait for travis, do all tests pass locally on your machine?\n. I agree, @mazumdarparijat would you up for adding a couple of tests for that too?\n. done in #1997 \n. done in #1997 \n. Yes contributions are welcome here. Let us know if you need help. Reading existing codes, examples, and the gp book by Rasmussen is probably the best start. nice!, looking forward to your input!\n. Done!\n. done\n. Yeah set_num_permutation_iterations or so\nOn 1 Mar 2014 11:24, \"Soumyajit De\" notifications@github.com wrote:\n\n@karlnapf https://github.com/karlnapf working on it alongside #1906https://github.com/shogun-toolbox/shogun/issues/1906.\nWhat would be a good name for bootstrap_iterations? permutation_iterations?\n\nReply to this email directly or view it on GitHubhttps://github.com/shogun-toolbox/shogun/issues/1905#issuecomment-36422374\n.\n. Hi\nYes, those are the ones. But not just added plain, but with a nicely written paragraph around them. It's not too much work. \nShould also not be necessary to deal with the codebase as most things are self-explaining in the example. \nPlease keep an eye on what @lambday is doing, as this might change the API or some behaviour. For example, we will soon allow for different number of samples in the distributions, see #2022 \n. Hi Roman,\n\namazing! Kaggle solutions are extremely useful for us as they are popular, illustrative, and sexy :)\nIn fact, maybe we should have a separate notebook folder for such projects.\nI suggest you do a notebook on its own. The existing ones are more to talk about ups/downs of algorithms and give intuition. This is more about how to solve actual problems. I'll discuss this with the others soon.\n. See http://permalink.gmane.org/gmane.comp.ai.machine-learning.shogun/4379 for a discussion\n. How is the notebook coming along?\n. What is the reference?\n. pls let me know if anyone is working on this, as there is the need to discuss some things\n. pls let me know if anyone is working on this, as there is the need to discuss some things\n. @hushell how should the loading/saving of graphs work, could you elaborate a bit?\n. Thanks! That is appreciated!\nSee my minor comments. Also, could you please add a static gist to the notebook and put this into the nbviewer and paste the link for every PR on notebooks you do? For now, we have to do this as otherwise its hell to review your changes.  This will soon be automated, but for now has to be done manually\n. any progress on this? :)\n. Hope the exams went well then :)\n. Thanks!\nA bit more love on explaining things would be nice. Just to make it more pleasant to read - currently it's just a program with almost no comments, etc. There are loads of things one can say about this method that you added.\nBut thanks, thats already a good first step - so let's merge!\n. @mazumdarparijat Nice work once more! :)\nA few comments:\n- You should add you name in the headers of CPCA.h CPCA.cpp\n- See my comment of the SVD vs Eigendecomposition. In my opinion, this should be done now that you are touching things. There was a guy in IRC yesterday that had a problem where N=10 and D=20000, for which Shogun explodes - although its an easy problem. Let me know if you have questions on this. The implementation in the David Barber toolbox should help. (You should also compare against it).\n- Travis fails due to integration tests, but yeah those can be done after all other comments have been addressed\nAgain, nice work! Very useful.\n. Ah I just saw your comment on SVD, nevermind all my output from before, but maybe its useful! Nice!\n. BTW could you also update apply_to_feature_vector\nand unit test this one?\n. Ok one last thing: Documentation.\nCould you write a brief description of the basic math involved in PCA in the  \"@brief\" class doxygen comment (have a look at the class list for how to write math in doxygen). In particular, explain all those different modes and what it corresponds to mathematically and what the costs are and what users should choose.\nThis is the most important part since it will make sure people will use and appreciate all your changes. Without it, people just get confused :)\nI really like the PCA implementation now: Complete, fast, flexible. Good work!\nI feel bad for adding more and more to this, but it is just too useful :)\n. okay, just a few minor glitches regarding style and doc.\nCould you also commit your new data version? The other PR is already merged, so that will not break travis anymore\n. ok waiting for travis now and then this is merged finally :)\nCould you however still change the two minor things? Thanks!\n. ok waiting for travis now and then this is merged finally :)\nCould you however still change the two minor things? Thanks!\n. oh and pls add all this to NEWS\n-PCA now depends on Eigen3 instead of LAPACK [yourname]\n-New modes for PCA matrix factorizations: SVD & EVD, in-place or reallocating [yourname]\n. oh and pls add all this to NEWS\n-PCA now depends on Eigen3 instead of LAPACK [yourname]\n-New modes for PCA matrix factorizations: SVD & EVD, in-place or reallocating [yourname]\n. whooooo! finally! :)\n. whooooo! finally! :)\n. BTW we need a setter for EPCAMethod\n. @lambday Nice! Thats exactly what the problem was. Though for the Kernel one the index vector is too short. \nA few comments:\n- the PR is too large to review, could you split the thing into smaller ones? Maybe the renaming one first without any other changes? I just put some comments on the stuff I saw for now\n- Then the split of the classes without changing any code\n- I think there maybe should be another base to generalise over the shared code of the two new classes, but I am not sure @sejdino might have comments\nAs usual, this is very nice work!\n. @sejdino TestStatistic might have more than two sets of data points, and all this stuff is based on the fact that they have two. \nTestStatistic->TwoDistributionsStatoistic->IndependenceStatistic\nTestStatistic->TwoDistributionsStatoistic->TwoSampleStatistic\nthe class in between can take care of all the constructors, and storing the two feature objects\n. @sejdino TestStatistic might have more than two sets of data points, and all this stuff is based on the fact that they have two. \nTestStatistic->TwoDistributionsStatoistic->IndependenceStatistic\nTestStatistic->TwoDistributionsStatoistic->TwoSampleStatistic\nthe class in between can take care of all the constructors, and storing the two feature objects\n. @lambday looking forward to the next PR! This was already a good discussion. Pls make the next PR in more smaller bits though :)\n. @lambday looking forward to the next PR! This was already a good discussion. Pls make the next PR in more smaller bits though :)\n. Good point! I don't really know to be honest. Have to talk to @sejdino on that.\n. And done!\n@lambday You will have a long list of resolved issues for your proposal ;)\n. I did not check that to avoid going over the vector twice. I think it's documented.\nBut if you want to add a test that checks whether the vector is sorted (on the fly) AND finds the position in one cycle, that would be good.\n. I did not check that to avoid going over the vector twice. I think it's documented.\nBut if you want to add a test that checks whether the vector is sorted (on the fly) AND finds the position in one cycle, that would be good.\n. LAPACK guard was added in #1934 #1933 \n. But unit test is still missing. See also #1943 \n. Looks pretty good to me!\nJust travis fails, you will have to send a PR against data with the changed variable names updated in the intergration tests. Then commit the new data version in this PR (after the other is merged, otherwise travis doesnt see it)\n. Looks pretty good to me!\nJust travis fails, you will have to send a PR against data with the changed variable names updated in the intergration tests. Then commit the new data version in this PR (after the other is merged, otherwise travis doesnt see it)\n. @votjakovr Great work! :)\nSee my comments on some minor stuff.\nOne thing I would like to discuss is this memory issue arising from clone.\nBTW, what about the string features and GPs?\n. @votjakovr Great work! :)\nSee my comments on some minor stuff.\nOne thing I would like to discuss is this memory issue arising from clone.\nBTW, what about the string features and GPs?\n. How much faster are the GPs now?\nAlso, pls add those things to the NEWS :) People should connect your name with this cool code!\n. How much faster are the GPs now?\nAlso, pls add those things to the NEWS :) People should connect your name with this cool code!\n. travis segfaults\nlibshogun-classifier_gaussian_process_binary_classification \n. travis segfaults\nlibshogun-classifier_gaussian_process_binary_classification \n. Great work, @votjakovr \nKeep on pushing this!\n. This was written manually. We could obviously create class diagrams from code automagically, but that misses the whole point of this task since computers are bad in distinguishing unimportant things from important ones :) This task is more about getting familiar with the framework to be ready to extend it. Also see the latest put up entrance tasks on variational learning #1971. This diagram might be useful for extending the framework for such tasks \n. This was written manually. We could obviously create class diagrams from code automagically, but that misses the whole point of this task since computers are bad in distinguishing unimportant things from important ones :) This task is more about getting familiar with the framework to be ready to extend it. Also see the latest put up entrance tasks on variational learning #1971. This diagram might be useful for extending the framework for such tasks \n. yep\n. @vigsterkr do you know anyone that could help here?\n. @lambday \nok great!\nThis can be merged, my comments were only minor. I will now compile doxygen and read the docs to make sure everything is consistent there.\n. Oh and the notebooks have to be changed, yes. Could you do that next? Otherwise all notebooks are broken ;)\n. Wow that seemed like a lot of work, well done :) Much clearer now I like the new class structure.\n@sejdino check this out!\n. @mazumdarparijat very nice, thanks!\n. Lets keep an eye on this one, done soon!\n. And please talk to us before starting to work on this.\nSome more comments:\n- Sampling random numbers from distributions should rely on what @vigsterkr says here, i.e. c++ but not boost or stan\n- stan/boost should be used for log_pdf or cdf methods\n- For univariate distributions, we can in fact just rely on boost. Stan only for those complicated multivariate ones, but the class hierarchy and interfaces are independent of that anyways...\n. Another aspect is to be able to exploit stans automatic differentiation framework: It can automatically compute derivative for all distributions.\nThis is done via https://en.wikipedia.org/wiki/Automatic_differentiation\n. Stan currently is kind of out-of-the-game due to its non-existing packaging structure. But we might get back to this. Still the task of representing probability distributions properly remains. Mostly interface work\n. First thing would be to generalise the current (sparse) solver interface which computes min/max eigenvalues to compute all eigenvalues. Then from that, we can have subclasses for iterative-sparse and dense solvers. Then we can put eigen3 implementations inside, or others like ViennaCL.\nIt would be great if one could change all of Shogun's dense Eigensolvers (which currently should all be eigen3 based) to a GPU based implementation in a one-liner with some sort of global flag.\n. @lambday should guide this one, as he wrote most of the code. Pls let him (and us know) if you work on this\n. Oh and one more thing. All those solvers should also have constructors that directly accept SGMatrix/SGSparseMatrix which are internally put into the CLinearOperator instances\n. This looks almost too good to be true ;)\nWe should aim to refactor all matrix factorisations (maybe even have a base class for that) and linear solvers in a way such that we can easily change the backend\n. I really like this! Agreed with all of the points so far\n. I really like this! Agreed with all of the points so far\n. - Yeah storing the results for later is great.\n- directory would be good to, in particular for custom linear operators\n- CRS might be good too, what about making it optional to choose mode with a reccomendation what to do regarding performance\n. Yeah lets work on this one once the other PR is merged.\n. Yeah lets work on this one once the other PR is merged.\n. @pl8787 see #1953 \n. Great work guys! :)\n. Nice, thanks! Thats exactly what we were after :)\nYou should always compile and run all tests locally before sending a PR.\nUnit tests should just assert that there are the correct objects assigned and everything is initialised\n. Nice!\nWe are getting there, just a few doc updates and different std values.\nComputing log-dets of dense matrices is not done with this framework, but with a Cholesky - so std should be sparse matrix\n. yep! :)\n. @sunil1337 pls always double check everything in valgrind locally before sending patches. It is quite tough to spot memory leaks from code if you review like 10 patches a day ;)\nThanks for fixing it!\n. I talked to @vigsterkr \nLets eigen3ize everything with respect to low level matrix-{matrix,vector} multiplications and do all matrix factorization calls against a Shogun internal api where we can change the backend.\nPart of eigen3-izing things is to write a unit test for every case of every call that is changed.\n. densities should always be evaluated in log-domain to avoid numerical problems\n. Just also added some minor remarks :) Great work!\n. I dont really get what travis complained here about, all builds passed.\nI just restarted - lets see what happens\n. Looks like it worked now :)\nThanks a lot for those very useful tests!\n. @vigsterkr you are right, sorry about that. Anything I should/could do to make sure things are fine?\n. @vigsterkr I got it by now. BTW what about scanning for new memory leaks via travis? Its sometimes hard to foresee those.\n. @vigsterkr yeah I know, but remember us discussing triggering the buildbot to run stuff? Same as for the notebook rendering. Then we could have (optional) checks for mem.....\n. Thanks!\n. Hi @mazumdarparijat \nWow, impressive work! :)\nHowever, this patch is too big to review. We should not merge it like this as many things might slip through our hands.\nI suggest the following approach:\n- Remove everything from it for now (keep a local copy).\n- Then add just one class (two files max) - the most basic one.\n- Then add unit tests just for those.\n- Send a PR.\nThis should not include more than 3 files and not more than a few hundred lines. It will be much easier to think about the code, what it is supposed to do, how it should be documented, what unit tests should there be.\nThen we can discuss those changes (with the help of @iglesias ), and merge.\nThen you add another class (few files) and their unit tests. And so on.  \nWhat do you think about that?\n. @vigsterkr I agree with you on the structure. The point is that it is impossible to review all of this at once. Rather add things one-by-one. If there is a refactoring of some existing class, do that first (including tests etc) Then, in a separate PR, add new classes. Then finally examples.\nOtherwise things slip through.\n. @vigsterkr exactly like that ;)\n. @pl8787 yeah I was lazy :)\nJust run all of them and see. LIbrary errors are not the ones I am after, but API changes.\nThe GP ones had problems, I remember\n. Much easier to read :)\n. Wow, some pretty serious work going on here :)\n. @mazumdarparijat fine from my side now. Do you see why we prefer smaller PRs now? :)\nTry to make the next one even smaller.\n. @mazumdarparijat fine from my side now. Do you see why we prefer smaller PRs now? :)\nTry to make the next one even smaller.\n. GREEEEEN! :D\n. Hi,\nthanks for the patch. There are a few problems here, which is why I suggest a different approach:\n- whitening can be done via SVD on covariance or on feature matrix, your method here will blow up for high dimensional data\n- All of this is already implemented in the PCA class. Things should not be re-implemented\n- this class is highly incomplete ( e.g. apply_to_feature_vector ) is missing\nHowever, I just tried the whitening option of the PCA class, and it doesn't do the scaling correctly. The resulting transformation does not have unit covariance. So what I suggest is written up in #1621 \n. I dont think this class should be worked on any further, see comments in #1621\n. Closing, let's do this with CPCA\n. Great work!\n@vigsterkr is this a lib-linear issue that we maybe should contribute back?\n. @thoralf can you take care of this one?\n. Once this script is all working, please write a nice little manual how to use it. With examples etc.\nOr even a blog-post. We can put that on our web-page or blog.\n. I like that!\n. Thanks for the fix!\nCould you please post a valgrind output of the remaining issues?\n. Speaking of valgrind. Is all this memory clean?\n. Hey @thoralf \nCool stuff! Sorry for the vast amount of comments, but lots of this stuff does not obey style rules in Shogun - I hope you fine the motivation to update things :)\n. Hey!\nA few comments, and then I'll check the changes\n- Indentation: sorry but that's your job, not the one of others ;) I do this with eclipse sometimes, maybe ask on the mailing list for tools. Most of the time, I just obey the rules when I write code.\n- another rule is: no new/delete, except for new CSGObject instances. This is to ensure we have control over allocated memory in shogun core. It makes things easier to maintain.\n- File IO: It hurts Shogun a lot if we add \"on the spot\" implementations of things like reading files that are basically just hacks to provide a solution in one particular situation. They basically are impossible to maintain and we had many cases in the past where no-one has an idea how to fix a problem because of such solutions. Some proper IO class to handle this format (that is well tested and documented) is the only suitainable solution - so yes, this would be a great entry task.\n. As for the data-structure:\n- one way would be to use std::vector internally (hide all this to the outside). That's being done sometimes\n- Access would work via T* &vector[0], etc\n. Another possibility would be the use of DynArray, which can do all this stuff too\n. @tklein23 Hey! I like the way this is looking now. As clean as possible and all glitches are connected to each other in the issues you wrote. Apart from @vigsterkr 's comment, I have no objections to merge now. It is much easier to tidy this up later now due to all the references, and maybe somebody will do it....\n. And for this! :)\n. @vigsterkr Do you know whats wrong with travis these days? Random time-outs and errors it seems\n. @vigsterkr ok, that's what I do :) \nThis one could be merged since it worked at some point. Thoralf removed some commented out lines afterwards...\n. Very nice patch! Its's hard to see how things come out from the diff, but I think everything should be ok.\nBTW the unit tests are not that good for this class, pls really make sure that the results are the same by writing some examples and compare output in old/new implementation.\nMaybe even add some tests against arthus reference implementation? I dont know, maybe the existing ones are enough already.....what do you think?\n. I already merge, the minor issues can be dealt with in a seperate PR\nPls also update the NEWS with the API changes and new classes\n. Thanks!\nOne thing. Could you always post an nbviewer link to the current version of the notebook in the PR (with output)\nThe PR itself should not contain any output, so clear before you commit.\n. Data should be added to shogun-data (if open) and then the notebook should reference that.\nDownloading manually is not a choice\n. I really like where this is going, but there are currently many issues that need to be addressed.\nNotebooks in general needs loads of love to look nice and convincing.\nAgain, pls have a look into other ones such as the GP to get some idea where we want to go with those notebooks\n. I really like where this is going, but there are currently many issues that need to be addressed.\nNotebooks in general needs loads of love to look nice and convincing.\nAgain, pls have a look into other ones such as the GP to get some idea where we want to go with those notebooks\n. And thanks for the hard work :)\n. And thanks for the hard work :)\n. Great! Let us know when you have an updated version. You should put it under the same gist/nbviewer link.\n. Nice, I just had a look and I think it is already much cooler than before. I will have a detailed look asap and then get back\n. Nice, I just had a look and I think it is already much cooler than before. I will have a detailed look asap and then get back\n. Hey, just started reading through it and realised you did not update the PR itself.\nI will write a few comments here anyways, please align them with the notebook yourself (and then update the PR)\nFormal background:\n- \"echniques.In\" missing space, there are more missing spaces, there are also a few weird white-spaces, please fix thise (spaces after/before things)\n- please use consistent notation for vectors. All of them should be boldface, some of them are not, \"In this approach a high dimensional datapoint ...\" and also in the math afterwards.\n- matrix dimensions should be written as \"$F\\in\\mathbb{R}^{n\\times m}$\" or similar\n- Please set up constants for number of data-points (N), dimension of data points (D), and number of principal components to use (M) before you are using things.\n- >>,<< is $\\gg$ and $\\ll$\n- You talk about the basis $B$ without saying that the $b^i$ are the basic vectors, mention that before you use them. Also define the other terms in the E function of the squared error\n- trace is $\\text{trace}$ or you define a custom math operator for that. Please dont do strings in math mode\nDeriving the optimal linear reconstruction:\n- All the definitions should appear before they are used for the first time (see other comment)\nI think this introduction is great! What is missing though is the multiple ways that Shogun can do PCA. Just a small overview: One can do EVD on the covariance matrix if DN. Have a look into the updated CPCA documentation. In fact, there should be a little section at the end of the 2d/3d examples that illustrates what to use when. Also, there should be a plot of the Eigenspectrum of a covariance matrix somewhere to show that usually the eigenvalues decay very fast. Then you can also explain Shogun's ability to maintain say p% of the covariance. See what I mean there?\nPCA on 2D data:\n- Start all scentences with a capital letter, again whitespaces\n- \"We expect PCA to find the principal components.\" should be \"We epxect PCA to recover this line, which is a one-dimensional linear sub-space\"\n- substracting the mean should be done by Shogun (you do that in the 3d example)\n- Plot captions: \"One-dimensional sub-space with noise\", add labels for x and y axis\n- Covariance matrix. Please stay consistent with the above notation. And again, SVD is also possible to do PCA, depends on problem size\n- \"eigenvectors selection\" plot: Capital letter word starts, axis labels pls\n- Shouldnt the 1st Eigenvector be the blue one?\nStep 6: Deriving the....\n- \"The twoD_obsmatrix\" ???\n- yn=np.dot(E,twoD_obsmatrix) please use apply_to_feature_matrix or similar methods of CPCA\n3d PCA:\n- I think CPCA removes the mean automatially, no need for PruneVarMean or similar\n- please avoid output from matplotlib, _=plot(......)\n- \"since we set target dimension = 2 for 3d data\" please make this properly in terms of latex and spelling\n- yn=np.dot(E.T,train_f) pls use shogun for this\n- I like your 2d projected datapoints :) But you should not plot them twice, just with the residuals is enough\n. Eigenfaces: (Rename to: Practical Example: Eigenfaces)\n- Again, pls do consistent latex, whitespaces, and spelling\n- The image example is high dimensional, mention that PCA should be done using SVD on the data matrix, which is in O(N^3) rather than O(D^3) for covariance matrix\n- please make the training dataset plot a little larger figure(figsize=(10,10)) or so\n- substracting mean again should be handled by CPCA. Let me know if not.\n- If you only plot the top 20 Eigenfaces, why do you compute the first 100? Say why if there is a reason.\n- Step5 is essientially empty. Please write something proper here, and clean text.\n- yn=np.dot(E.T,train_f) can again be done via CPCA class\n- 1-2-3 Eigenfaces procedure consistent spelling (capital at scentence start)\n- test_proj = np.dot(E.T, test_f) should be done by shogun\nI'll stop for now. Once you updated things, pls update the gist nbviewer link and the PR. Then we can do another iteration. \nGREAT WORK! :)\n. Please make sure to rebase\ngit pull --rebase upstream develop before sending patches. Otherwise we get all those merges in the commit list.\n. Please make sure to rebase\ngit pull --rebase upstream develop before sending patches. Otherwise we get all those merges in the commit list.\n. You could maybe mention in the intro that PCA is completely unsupervised.\n. You could maybe mention in the intro that PCA is completely unsupervised.\n. \"above scenario, we infer that\"\nWe don't infer that :) Rather \"above scenario, we assume that\"\n. \"above scenario, we infer that\"\nWe don't infer that :) Rather \"above scenario, we assume that\"\n. \"defines a point in the hyperplane.\" - what do you mean with that? which hyperplane? Its the first time you mention this there, so you should either introduce what that is before (not math definition but what you that is here for PCA) or use a different word\n. \"defines a point in the hyperplane.\" - what do you mean with that? which hyperplane? Its the first time you mention this there, so you should either introduce what that is before (not math definition but what you that is here for PCA) or use a different word\n. The yni are the low dimensional co-ordinates of the data.\nSo if the y's are the vectors in the lower dimensional space and the b's are the coefficients, why are the b's bold and the y's not? Should be the other way around. Please also write the space these elements come from  b_1 \\in \\mathbb{R}^{\\bla}\nAh sorry those are vector components. But anyways, please write the space always after you introduce an element using \\in\n. The yni are the low dimensional co-ordinates of the data.\nSo if the y's are the vectors in the lower dimensional space and the b's are the coefficients, why are the b's bold and the y's not? Should be the other way around. Please also write the space these elements come from  b_1 \\in \\mathbb{R}^{\\bla}\nAh sorry those are vector components. But anyways, please write the space always after you introduce an element using \\in\n. \"Hence our motive is \"\nNo Hence here. Just \"Our Motive\"\n. \"Whilst the data vectors\" - No whilst here. Just \"The data vectors\"\n. \"number(M\u226aD)\" whitespace missing\n. \"Here the best basis vectors are defined as\"\nIts not the best basis vectors, just the basis vectors\n. \"The optimal bias c is given\"\nSame here, bias is not optimal in the squared error, its just some bias\n. \"To find the best basis vectors B \" Here we are talking about the best, so thats fine :)\n. ( i.e the basis vectors are mutually orthogonal and of unit length )\nno whitespace after ( and before )\n. EGD -- should be EVD I guess?\n. \"from matplotlib import pyplot\" You can assume that our notebook server is started with\nipython notebook --pylab inline so there is not need to import anything. Also, all functions are loaded to the std namespace, so you can just do plot(...) instead of pylab.plot(...)\n. Step 4: Calculate the eigenvectors and eigenvalues of the covariance matrix\nCould you initialise PCA with explicitly using the constructors options and commenting on them? (not in the source code, but in notebook)\nIn fact, before you start with the PCA code, could you add a litlle paragraph on the CPCA class itself, quickly comment on how its initialised (see above, constructor parameters), and what type of data it accepts, what type of preprocessor it is, that it is a preprocessor in Shogun that is used in a certain way, etc? This is all Shogun specific, but its important to also document those points\n. And in the above, also add link to the Shogun class list\n. \"Steps 5\" there is an \"s\" too much\n. \"\"\nCould you hide those type of outputs via _=plot(...)\n. \"Step6: Deriving the new data set\"\nWhat about \"Projecting the data to its principal conponents\", also a whitespace missing\n. matrices (with D feature dimensions) supplied via apply_to_feature_matrix methods.This tranformation\noutputs the M-Dimensional approximation of all these input vectors and matrices (where M<=min(D,N)).\nplease dont do such things in comments, but rather in the notebook directly\n. \"preprocessor(from next example)\" whitespace\n. ax.set_zlim(-30,30)```\nplease do ```_=ax.set_zlim(-30,30)``` to hide any python output for such plotting calls\n. #E is automagically filled by setting target dimension = M. This is different from the 2d data example \nwhere we implemented this step manually.\nPlease write this in the notebook rather than in a comment\n. \"PCA Performance\"\nI just saw this, maybe this should be moved a bit to the top. I commented that its missing earlier\n. Two-dimensional pq grayscale should be \"pq\" (in latex), so no \n. \"Step 1:Get\" whitespace\n. np.array etc can be array when you start the notebook with --pylab inline\n. Euclidean distance - please add a link to the class documentation\nhttp://www.shogun-toolbox.org/doc/en/latest/classshogun_1_1CEuclideanDistance.html\nIn fact, please do this for all classes you mention\n. Wow, I have to say I am really impressed by this notebook! :)\nIt is very nicely written, and the examples are just amazing.\nI hope you dont mind all my comments, it got so much better since the first iteration.\nI think once my current issues are resolved, I will iterate/read once more and then we can merge this, and tell the world how great it is. Please add a comment in the NEWS file\n-new PCA notebook by [yourname]\nGreat work!\n. BTW I agree on your last point with the np.dot.\nAlso I hope your exams went well!\n. Nice! Thanks for this example. A few points\n- You do some things with python that can/should be done with Shogun. Thats the whole point\n- The data should be in shogun-data - is this possible with the license?\n- Putting the thing into graphical is a good idea. However, an even better idea would be to make this a new PCA notebook. \n. Nice! Thanks for this example. A few points\n- You do some things with python that can/should be done with Shogun. Thats the whole point\n- The data should be in shogun-data - is this possible with the license?\n- Putting the thing into graphical is a good idea. However, an even better idea would be to make this a new PCA notebook. \n. There is already another PR with the eigenfaces example.\nThats a pitty since two people worked on the same thing. Have a look into\nthe existing notebook, we will go from there\n2014-03-08 19:05 GMT+00:00 ahcorde notifications@github.com:\n\nEigenfaces example integrating Opencv and Shogun. hey @pickle27https://github.com/pickle27!\nhave a look please.\nYou can merge this Pull Request by running\ngit pull https://github.com/ahcorde/shogun develop\nOr view, comment on, or merge it at:\nhttps://github.com/shogun-toolbox/shogun/pull/1964\nCommit Summary\n- Eigenfaces example\n- [ahcorde]EigenFaces\nFile Changes\n- A examples/undocumented/python_modular/graphical/eigenfaces.pyhttps://github.com/shogun-toolbox/shogun/pull/1964/files#diff-0(224)\nPatch Links:\n- https://github.com/shogun-toolbox/shogun/pull/1964.patch\n- https://github.com/shogun-toolbox/shogun/pull/1964.diff\n\nReply to this email directly or view it on GitHubhttps://github.com/shogun-toolbox/shogun/pull/1964\n.\n. There is already another PR with the eigenfaces example.\nThats a pitty since two people worked on the same thing. Have a look into\nthe existing notebook, we will go from there\n\n2014-03-08 19:05 GMT+00:00 ahcorde notifications@github.com:\n\nEigenfaces example integrating Opencv and Shogun. hey @pickle27https://github.com/pickle27!\nhave a look please.\nYou can merge this Pull Request by running\ngit pull https://github.com/ahcorde/shogun develop\nOr view, comment on, or merge it at:\nhttps://github.com/shogun-toolbox/shogun/pull/1964\nCommit Summary\n- Eigenfaces example\n- [ahcorde]EigenFaces\nFile Changes\n- A examples/undocumented/python_modular/graphical/eigenfaces.pyhttps://github.com/shogun-toolbox/shogun/pull/1964/files#diff-0(224)\nPatch Links:\n- https://github.com/shogun-toolbox/shogun/pull/1964.patch\n- https://github.com/shogun-toolbox/shogun/pull/1964.diff\n\nReply to this email directly or view it on GitHubhttps://github.com/shogun-toolbox/shogun/pull/1964\n.\n. Its a pitty that the two of you are working on the same thing. \nThis stuff you are doing here should def. be in the notebook. However, the current notebook already contains some eigenfaces stuff. Therefore, I suggest you to get back to the author of #1961 and synchronize. Once we have the basic notebook merged, we can add your stuff to it.\n\nHaving said that, I am also fine with having another graphical example in python_modular. Though I really would love to see this in the notebook later on - in fact I even prefer that.\nAs for the code, there are still some comments unresolved, for example, why to you do the PCA projection with numpy? You can do it in Shogun ...\n. No worries, and thanks for your efforts. Looking forward to see your stuff in the notebook, it looks great already\n. No worries, and thanks for your efforts. Looking forward to see your stuff in the notebook, it looks great already\n. Whats the status of this one? All issues fixed?\n. Yeah indeed, he did a good job.\nI think we should merge your code into a graphical example too, its always nice to have some more things, however, for that, we would have to iterate a few times and you would have to change the things we commented on. If you want to do that, please go ahead, if not, its also fine to start another task\n. Please use the faces data that we recently added to shogun-data.\nI currently cannot run this and see what it does since I dont have opencv. Once this and the other comments are resolved, I am happy to merge.\n. Ok, lets keep it then.\nThough it is not really integration. But I think its a nice illustration how to combine PCA and openCV. Pretty cool! :)\nOne the minor remaining comments are solved, we can merge\n. One thing: Could you make sure this works with the latest Shogun version if you haven't done yet?\nNeither travis nor buildbot test graphical examples.\n. segfaults on my machine\n. Yeah totally, this would be great to have in CPCA\nThe code you referenced is very old btw, I dont know why you reference 3.0. But anyways.\nOk we are almost ready. Could you please package all of your changes in one commit (never use empty commit messages btw)\nOn the crash: It sometimes segfaults, sometimes I get this\nReading images ...\nTraceback (most recent call last):\n  File \"faces.py\", line 129, in <module>\n    images = readImages(list_filenames);\n  File \"faces.py\", line 91, in readImages\n    images = np.empty( (IMAGE_HEIGHT*IMAGE_WIDHT, (len(list_filenames))-1))\nValueError: negative dimensions are not allowed\n. When running this from graphical folder in python_modular, I get\n$ python faces.py \nReading images ...\nTraceback (most recent call last):\n  File \"faces.py\", line 150, in <module>\n    images = readImages(list_filenames);\n  File \"faces.py\", line 109, in readImages\n    images = np.empty( (IMAGE_HEIGHT*IMAGE_WIDHT, (len(list_filenames))-1))\nValueError: negative dimensions are not allowed```\n. I guess this is to path problems, but I have latest shogun-data. Any ideas?\n. Ok the program now runs. What it does is to print lots of number to the terminal, please dont do that.\nThen, a small window opens, showing a face - and then nothing. Since it is very hard to guess whats going on, could you make the whole experience a bit nicer? Make the program tell people what is going on, etc\n. A few things:\n- you should use meaningful commit messages, these are all not telling us anything\n- could you please merge all commits into one?\n- I again cannot run this script as it cannot find files, please add an error message for that case. Also, please make sure it works from the folder, I have the latest Shogun git version for which it has to work\n. Almost there!\n. Ok nice.\nJust a few more things (I am sorry about the ongoing iterations here)\n- Can you please do the visualisation in a way that you (in the same window)\n  - show a subset of the data\n  - show the mean face\n  - show all the reconstructions\nCould you also add captions to the images, and scale them up? They are too small to look at (use interpolation=\"nearest\" when plotting, we dont want the pictures to the smoothed.\n. I guess what I am after is that not multiple windows pop up, but the user just gets one window with a nicely layouted presentation of the results, including data, mean, and reconstructions. With labels and everything. Keep in mind this should impress someone who opens it. Currently its a bit weird.\nBut since the example is really nice we should present it well.\n. Yes! Thats very nice now! :)\n. @vigsterkr ?\n. @vigsterkr ?\n. Nice, all those tests are very useful. I have one general doubt here:\nYou currently just test the accuracy of the lib-linear predictions. That is not a good way to unit test things (there is loads of ambiguity in those). What should really be done is to test the output of the w and bias result and compare it against the true liblinear version. I.e. write code in some liblinear interface (the one they provide) and make sure results are the same\nSee what I mean?\n. Yeah, such a unit test, and then compare the resulting model against a shogun-free liblinear\n. Those unit tests then can replace the current ones.\n. Pls dont mix up things in pull requests, we should merge the liblinear one first before adding one for HSIC.\nHaving said that, thanks for the additional tests. What about the tests for liblinear with a proper comparison, not just the accuracy?\n. Pls dont mix up things in pull requests, we should merge the liblinear one first before adding one for HSIC.\nHaving said that, thanks for the additional tests. What about the tests for liblinear with a proper comparison, not just the accuracy?\n. As travis currently fails, could you remove the hsic test for now and send a seperate PR?\n. As travis currently fails, could you remove the hsic test for now and send a seperate PR?\n. Just saw the new libliear tests, great work!\nOnce the minor issues are adresses, the hsic is removed (I want to deal with that seperately), travis passes, and you tell me that all tests are valgrind clean, we are ready to merge :)\n. Just saw the new libliear tests, great work!\nOnce the minor issues are adresses, the hsic is removed (I want to deal with that seperately), travis passes, and you tell me that all tests are valgrind clean, we are ready to merge :)\n. Great! Finally this is merged :) \n. Great! Finally this is merged :) \n. Good stuff, thanks!\nWhere does this code come from? Monica? Written on your own\n@monicadragan are you fine with taking parts of your code and put BSD license on it?\nNext thing: I have no idea whether this does what it should. Somebody would have to carefully check the codes (reference implementation)\nFinally, we have to do some very careful testing against some kind of reference implementation for id3. Maybe scikit-learn?\n. About the example, I would vote for putting that into a notebook along with some explanations/background.\nAnd unit tests, I would do them on something very simple, yet coplex enough to cover all cases in the code\n. About the example, I would vote for putting that into a notebook along with some explanations/background.\nAnd unit tests, I would do them on something very simple, yet coplex enough to cover all cases in the code\n. But it would be great to add the dataset to shogun-data and maybe use from modular example\n. But it would be great to add the dataset to shogun-data and maybe use from modular example\n. About the datasets:\n- python modular examples should all work on very small and simple toy data\n- notebooks can contain a few larger datasets, should run within a few minutes though.\n- maybe you can use some multiclass example data that is already there. For the API examples, it doesnt have to make sense, just for the notebook one.\n. About the datasets:\n- python modular examples should all work on very small and simple toy data\n- notebooks can contain a few larger datasets, should run within a few minutes though.\n- maybe you can use some multiclass example data that is already there. For the API examples, it doesnt have to make sense, just for the notebook one.\n. This seems mergable to me (apart from the few minor style issues)\nOne thing about the unit test: I really like this nice example, but could you also add a few custom engineered ones that systematically cover all cases in the code? We want 100% coverage of such algorithms\n. This seems mergable to me (apart from the few minor style issues)\nOne thing about the unit test: I really like this nice example, but could you also add a few custom engineered ones that systematically cover all cases in the code? We want 100% coverage of such algorithms\n. Whoo! I am excited about this one. Very nice looking code.\n@vigsterkr so thats one step closer to random forest like machines right? :)\n. Yeah! :)\nC4.5 is taken by somebody - I think at least. Yeah Bagging is working, I just need to do some homeworks for @vigsterkr to make them thread safe.\nBoosted trees are also useful. Though I dont know much about  them.\n. Note that those two tasks can easily be separated - although 2) depends on 1)\nBut you can choose to do just 1) and that is already great.\n. Hi guys, sorry for the delay.\nThere is a deadline soon (but please lets keep the discussion here in the topic, we can discuss other issues on the mailinglist)\n@yorkerlin So please lets try to merge things before the deadline. That means that you should rather try to focus on smaller bits and try to get them merged, rather than attempting huge patches which take long to iterate over. See my other comments. And see also the mailing list archives on how to best get into a project (which is to get your code merged)\n. @yorkerlin the code in your PR is quite nice already. But please dont pollute it with pre-mature code.  We appreciate a small amount of mature code (including time-consuming to write unit tests) more than a huge patch which is far from being merged. Try to focus on doing things on by one in a structured way.\n. Yep, totally. And if they are not there yet, they should go into CMath or some other (possibly new) helper classes\n. Yep, totally. And if they are not there yet, they should go into CMath or some other (possibly new) helper classes\n. You should try to do the entrance tasks one by one and aim to get the code merged into Shogun. This means you should address all our comments and try to send patches as small and as well-tested and well-written as possible. You are starting way to many construction sites here, stay focussed on one particular sub-problem and once thats merged, we can go on. Please let us aim at getting code you have shown us so far merged before starting anything new.\n. indeed! :)\n. Thanks! We will have a look very soon.\nIs it possible that you compare this against some other implementation also for double checking? It is very hard to go through code line by line without any errors slipping through our hands\n. Without having looked to your code yet, I guess then its possible to compare the results against the GPML2 version?\n. Cool, that helps. Give us some time to dive in then. And yes, do run comparisons. You should be able to make sure things work on your own. We will then double check\n. Please dont paste code/results in github messages, rather make a new gist https://gist.github.com/ and put the link here. Otherwise, the conversation gets too spammed. (I delete your comment for now)\n. Adding the registering method is easy. But the task is kind of involved since the cloning framework needs to work with this new structure type. Best way to start is to write (initially failing) unit tests for such things.\n. I think this is a very  hard entrance task in fact. Read the existing unit tests and implementation of ST_SPARSE to get a feeling. Memory management is tricky here.\n. This is way outdated. @vigsterkr yeah, very cool! Let hope somebody solves this. So for everyone, this is not only very useful for the deep learning project but in fact for the whole of Shogun\n. @vigsterkr yeah, very cool! Let hope somebody solves this. So for everyone, this is not only very useful for the deep learning project but in fact for the whole of Shogun\n. @vigsterkr yeah I got that. I think in the end we will have a mixture of cases where we use such a dot product unified interface (with multiple backends) and maybe some pure eigen3 calls in between (first because of history, lapack will also be in there for quite a while I guess, but also because some things are much more effective to solve in eigen3 internally, all depends on the case.) I am really looking forward to all this GPU linear algebra stuff.\n. @vigsterkr yeah I got that. I think in the end we will have a mixture of cases where we use such a dot product unified interface (with multiple backends) and maybe some pure eigen3 calls in between (first because of history, lapack will also be in there for quite a while I guess, but also because some things are much more effective to solve in eigen3 internally, all depends on the case.) I am really looking forward to all this GPU linear algebra stuff.\n. What is your plan do proceed? Pls discuss this with us first to avoid re-factoring all your work\n. This very first (and most important) step is to write an internface for the abstract class that will be used for such calls. Maybe even a little class diagram might help.\n. Hey guys,\nplease don't start with openCL. Start with eigen3. I would rather have the interface working ASAP with basic implementations (that do the same as we already have). This can and will be used from everywhere in Shogun then. It will also show flaws in the interface/API, which should be done before you put in sophisticated GPU codes\n. For the interface, I think the type can safely be the same\n. Please put all this stuff in the math/linagl folder and make abstract base classes such as for the eigensolver in #1930 We really want to unify things first, before doing any new coding\n. Also, please note that we already have a base class for linear operators with the method apply().\nAll the core interface should be written in that. We can then offer wrapper methods like the above one.\n. Yep that sounds like a sensible way to go. Maybe call this CVectorDotProduct and CSparseVectorProduct, which directly inherit from CLinearOperator.\n@lisitsyn thoughts?\n. Yep that sounds like a sensible way to go. Maybe call this CVectorDotProduct and CSparseVectorProduct, which directly inherit from CLinearOperator.\n@lisitsyn thoughts?\n. @lambday I really like this second example with the functor. In fact, I suggest you take the lead in guiding this here. I have a few thoughts:\n- no matter what interface we choose, there should always be some convenience methods that are ultra simple, and in particular independent of how things are implemented. i.e. specify the functor\n- it should be possible to switch between backends without changing the involved classes. So I would just have a VectorDotOperator and then can change the backend for all of those in Shogun globally. The example doesn't really do that since we need to specify the functor manually.\n- About the std dor vs GPU dot. We should think about whether doing dot products on the GPU is something the coder should decide or not (I think he should, so I agree). However, for the matrix factorisations, this should just depend on the globally set backend for such things.\n- @pratheekms your gist doesnt work here for some reason.\nThoughts? \n. But yeah, lets maybe discuss this a bit more @lambday feel free to guide this a bit. You wrote the initial parts of the linalg library we have :)\n. May I suggest to have different backends for dot products and matrix factorizations (but also the possibility to set them all at once). \n. May I suggest to have different backends for dot products and matrix factorizations (but also the possibility to set them all at once). \n. I like where this is going! :)\nQuestion: Why singleton? Almost any Shogun method uses linear algebra, so we could just have a global object. But open for discussing this.\n@lambday thoughts?\n. @lambday I agree wiht you, singelton is a way cleaner concept than global variables. In fact, why not transition all of the global ones (IO, etc) to singleton? Any thoughts on that?\n@lisitsyn @vigsterkr @sonney2k @iglesias ?\n. @tklein23 thanks for the feedback (and sorry for not including you in the list of people I asked, was late ;) )\nI agree with you on this - this kind of feedback is exactly what I was after. We can discuss in the issue you opened.\n. Keep on pushing this stuff, guys, very useful stuff! Also for the rest of Shogun\n. Just compiling Shogun in Release mode (which one did you test on)\nAn order of magnitude seems pretty large I agree. But it seems obvious that a compile time solution is faster. Lets see how different architectures deal with this.\nThe main question we should think about in my eyes is that if we do a macro approach, then swig support for those operations will be difficult. However, do we really want those things to be exposed to the outside? I think: no. This is framework internal stuff and meant as a tool for Shogun hackers to make their algorithms ready for new backends, and not really a wrapper for those backends to modular interfaces. So I currently tend to favour this solution.\nWhat is the flexibility loss concretely?\nThis is great work @lambday nice investigations - you really push Shogun forward.\n@sunil1337 I think your ideas still can be partly re-used. Without your patch, this discussion would never have started - well done!\n@vigsterkr @lambday I agree we should do some research on how to benchmark things, eigen3 might be a good starting point. However, we should not over-analyse things. The magnitude in runtime difference comes from 1000 calls - In algorithms however, the overhead of the calls is not the bottleneck but the operations themselves, i.e. matrix factorisations etc. Dot-products are relatively fast - I bet if we do the same thing with say Cholesky of a reasonable sized matrix, the overhead will be less significant. Any backend independent framework for linear algebra is better than the current state as we can trivially speed things up - those gains are way beyond any overhead caused by virtual method calls (keep in mind currently dot product are hard coded loops - so SIMD magic or whatsoever)\n. Just compiling Shogun in Release mode (which one did you test on)\nAn order of magnitude seems pretty large I agree. But it seems obvious that a compile time solution is faster. Lets see how different architectures deal with this.\nThe main question we should think about in my eyes is that if we do a macro approach, then swig support for those operations will be difficult. However, do we really want those things to be exposed to the outside? I think: no. This is framework internal stuff and meant as a tool for Shogun hackers to make their algorithms ready for new backends, and not really a wrapper for those backends to modular interfaces. So I currently tend to favour this solution.\nWhat is the flexibility loss concretely?\nThis is great work @lambday nice investigations - you really push Shogun forward.\n@sunil1337 I think your ideas still can be partly re-used. Without your patch, this discussion would never have started - well done!\n@vigsterkr @lambday I agree we should do some research on how to benchmark things, eigen3 might be a good starting point. However, we should not over-analyse things. The magnitude in runtime difference comes from 1000 calls - In algorithms however, the overhead of the calls is not the bottleneck but the operations themselves, i.e. matrix factorisations etc. Dot-products are relatively fast - I bet if we do the same thing with say Cholesky of a reasonable sized matrix, the overhead will be less significant. Any backend independent framework for linear algebra is better than the current state as we can trivially speed things up - those gains are way beyond any overhead caused by virtual method calls (keep in mind currently dot product are hard coded loops - so SIMD magic or whatsoever)\n. @lambday could you post a script for each version, I am confused what exactly to modify here to get the two different ones. Thanks!\n. @lambday could you post a script for each version, I am confused what exactly to modify here to get the two different ones. Thanks!\n. I agree with you in those things.\nFor the linear operators, it might be more problematic if they are not exposed to the outside, but again, we do not aim to be a linear algebra library, but simply need to use those as tools internally.\nCould you push the dot product, matrix operations, Cholesky+Cholesky solve, linear solve, SVD, Eigendecopmostion, and sum?\nHappy to have the other project stalled a bit - this is exactly what the community bonding period is for :)\n. @lambday I just compiled the code and read it. I think this is very nice - and I am glad we are finally starting to use modern c++ :) I cannot see any major problems with this approach - and as the others are being quiet - let us just start trying this and see where we go. \n@sunil1337 As for backends with GPU, I would wait a little bit (a week or two after this is merged) - in case we want to change the design, we dont want to change all the backend code, I would start with eigen3 (or even native) only and then once some people are using it, we will surely discover some problems and then change things. Thoughts on this? Maybe you can play with vienna cl locally or in a feature branch to get familiar? Or compare different GPU libs?\n. Good news!\nMaybe you can start drafting some gists, similar to those of @lambday . Not mergable code but just drafts of ideas that we can use as the base for a discussion.\nHow easy is it to install this thing? We might have to bundle it like eigen, json, etc\n. I like the easier solution more (and its fine for symmetric operations). What does the compile error look like when once forgets about this?\nWow I really like this clean interface, proper c++ magic!\n. Nice! Thats more reasonable\nI meant what happens if one calls this with the arguments the wrong way around? How cryptic is the compile error?\n. ok, that is cryptic, but not too cryptic for people who dont know traits, etc\n. MKL as in multiple kernel learning?\nComputing the kernel matrix for sure can be done on the GPU, though that might not be the bottleneck. I think the algorithms themselves (plane-cutting) cannot be parallelised to a large extend, but for sure all the linear algebra going on in there can be done using linalg, which will make things faster for sure.\nAre you interested in looking into this with the help of someone?\n@sonney2k How many dot-product-like and element-wise operations, and linear solves are there in the MKL code?\n. MKL as in multiple kernel learning?\nComputing the kernel matrix for sure can be done on the GPU, though that might not be the bottleneck. I think the algorithms themselves (plane-cutting) cannot be parallelised to a large extend, but for sure all the linear algebra going on in there can be done using linalg, which will make things faster for sure.\nAre you interested in looking into this with the help of someone?\n@sonney2k How many dot-product-like and element-wise operations, and linear solves are there in the MKL code?\n. I guess the first step would be to dive into the code, investigate performance systematically like you already started to do.\nFrom this, it should be clear what the bottlenecks in the code are. Very often, some very simple multicore techniques can massively speed up things. As for GPU, you would have to identify which operations could be put into a GPU. Candidates are mostly operations with (fixed & big) matrices.\n. Also have a look into the benchmark folder, there are a few example benchmarks we did in the past\n. Thankts, that is useful!\n@votjakovr Let me know if you have any objections, I will merge for now\n. Thankts, that is useful!\n@votjakovr Let me know if you have any objections, I will merge for now\n. Hej there!\nThanks for spotting. You are very welcome to add a few lines on this in the Readme and send a PR :)\n. Hej there!\nThanks for spotting. You are very welcome to add a few lines on this in the Readme and send a PR :)\n. Great you put that in there :)\n. Great you put that in there :)\n. An IPython notebook is the ideal output of this task. We can also start to play around with this data with the existing algorithms in Shogun so far.\n. @emtiyaz Can we add those datasets as examples in our repository? Then we could make the task to do that, and to visualise them in a notebook... Since we do not really want c++ code that only can load one particular dataset. We have loads of readers for different file formats, so rather bring it into a form that we can process later.\n. Nice, that will be useful!\n. @k29 Thanks for the link. I am a little confused though - you wrote this generic c++ code to read those files into STL data structures. First we do not use those data structures in Shogun, but our own. Second, as those files are just ASCII, you can just use the existing IO classes, such as CSVFile. With this one, it will just be a couple of lines to load the files.\nAn ipython notebook where you load the files would have been a bit more useful since this is what is needed to process the data further later on. Also, the ML here is being done from a notebook.\n. Hi @k29 \nYou wrote code that imports a CSVFile using Shogun. The goal however is to do ML on this data from an ipython notebook.\n. Yes, that's how we do it, but this is nothing new, you are just calling a Shogun class (we have examples for that in fact) But yeah, that's like the first two lines of the notebook that this task is about.\n. Nice!\nI guess you just took the old examples.\nDid you valgrind check this?\nMergable as soon as license is changed. Travis already passes.\n. No worries with the name :)\n. Travis failed, but I will merge anyways since this passed yesterday.\nPlease send another mini patch with the license change to BSD\n. See CQuadraticTimeMMD.h\n. updates here?\n. updates here?\n. pls do it in a seperate PR, it has nothing to do with the other stuff\n. pls do it in a seperate PR, it has nothing to do with the other stuff\n. Maybe mention the used software? Otherwise please merge, this is very useful\nAnother suggestion would be to put the formatting guidelines we have in this doc too, @lisitsyn @sonney2k I cannot find those currently, where are they again?\nJust restarted the travis build ....\n. Thanks a lot for that! :)\n. Hey Iris,\nthe first way: Having an object of the MMD class you need. Copying code makes this harder to maintain.\n. - This is a very nice patch! @emtiyaz also have a look\n- A few minor comments are on code-style. But thats mostly whitespace.\n- What you should definitely do is to carefully write proper meaningful unit-tests for all new methods added. We cannot merge before. Also, you should take some of the existing unit tests that check results of say binary classification with CLaplacianInferenceMethod and make sure that your class gives the same results.\n- BTW where is the code coming from? Your head? A book? A paper? The latter two should be mentioned.\n- Another question is whether we really want a new class for this. I would in fact prefer an enum in the original class that a user can set and that has a default value. Otherwise, one has to change class names to change the backend. What are your thoughts on that?\n. A list of methods to unit test. Please do more than one trivial test, but think about important cases.\n- CLaplacianInferenceMethodWithLBFGS::get_gradient_wrt_alpha\n- CLaplacianInferenceMethodWithLBFGS::get_psi_wrt_alpha\n- CLaplacianInferenceMethodWithLBFGS::update_alpha()\n- CLaplacianInferenceMethodWithLBFGS::evaluate\n- and the final results\nI am sure you might be able to re-use existing tests for this.\n. Note, all tests pass since you did not add any for this new class\n. Once this is all done and merged, I would be very interested in a little case-study that compares the two methods for speed and reliability. This could be added to the GP notebook. Let me know you thoughts\n. The information on where this is coming from should be in the class description, with references etc. That's very useful information for people using this.\n. We do not need a very large dataset to compare things. Have a look in the shogun-data repository. Those comparisons should be done (in an illustrative way) in the ipython notebook on GPs. Another place is the benchmarks folders, where there is room for such comparisons.\n. Yeah unit-tests can be re-factored easily, thats fine.\nComplexity of the method should also be mentioned in the class description\nI will go over the individual commits you added now.\n. I dont get why travis failed since all new unit tests passed. But just restarted.\nDid you valgrind check them? That is important too since I cannot see memory issues from code :)\n. Travis fails since there is a compile error in the unit tests (this is probably related to your change in the cmake, which should not be there as said) You defined a test name twice, just add some suffix as \"_bfgs\" or so.\nBTW: Please always run the unit tests + valgrind checks locally before sending any PR. This otherwise just wastes travis and buildbot time. These errors can easily be caught locally.\n. No worries, take your time!\n. Yes I think its good to add comments about the matlab code which was used to generate test numbers. We ususally do not include the full code though. Just saying: \"Comparing against the results of GPML on the very same data\" or something.\n. The reverse engineering is not optimal, but its all we can do since it is not feasible to maintain all the code that generates the results we are testing against in the unit tests. This is another reason why we should keep unit tests as simple and atomic as possible. They way you are doing it here is exactly the way to go.\n. Whether to use absolute or relative tolerance is a good question. I think the absolute tolerance is fine as we just want to make sure that the code doesn't do something obviously wrong (which is what unit tests are primarily for). However, it might be a good idea to also check relative tolerance. Note that sometimes, Shogun is more accurate than reference implementations, for example the Gaussian quadrature code that @votjakovr wrote I think produced better results than the one used in GPML. However, try to be as precise as possible if its within reasonable effort. So go for relative errors if you think they are better. If there are errors or things very unaccurate, fill in an issue and/or try to fix it.\nVery good points to discuss! That is appreciated :)\n. BTW Likelihoods should really never be returned or stored in non-log domain due to those problems  with small numbers. Whenever you see that, think whether its possible to make the code return numbers in log-space instead.\n. Yes, feel free to suggest fixes to any inconsistencies you find, both comments and code.\n. Looking forward to the next round of commits, this is going very nicely!\n. Ok great. Now let's get this merged asap. For that\n- please fix all the issues in the comments I  made. Many of those are general stuff, such as formatting and style\n- The PR has grown too large for me to overview. Please save things locally, and then send PRs which are as small as possible. Try to send things is small chunks where each one doesnt change more than a few files and lines. Its really hard to review this otherwise. I suggest first the updates to existing classes. Then new classes one by one, with all unit tests for them.\n. Ok great. Now let's get this merged asap. For that\n- please fix all the issues in the comments I  made. Many of those are general stuff, such as formatting and style\n- The PR has grown too large for me to overview. Please save things locally, and then send PRs which are as small as possible. Try to send things is small chunks where each one doesnt change more than a few files and lines. Its really hard to review this otherwise. I suggest first the updates to existing classes. Then new classes one by one, with all unit tests for them.\n. I will be very happy once this is in!\n. I will be very happy once this is in!\n. As for the tolerance of the tests, yeah you can use MATLAB's format long if thats necessary.\n. As for the tolerance of the tests, yeah you can use MATLAB's format long if thats necessary.\n. Ok, this is in fact now almost ready to merge. Very nice!\nSee the remaining minor issues. Travis also passes, so just one more minor commit and this is finally done :)\nLooking forward to the other codes, and its appreciated that you got them already working, but they really should come in the same form as this patch here.\n. Pls do the indentation in the same way as the other shogun class files do it. 4 charater tabstop IIRC\n. just do the same as in other Shogun classes, Just checked, its one tab. See for example\nhttps://github.com/shogun-toolbox/shogun/blob/develop/src/shogun/classifier/GaussianProcessBinaryClassification.h\n. Finally :) Great work!\n. @gsomix any updates here? @vigsterkr @tklein23 could someone of you give a summary of the findings so far? It is pretty cluttered I am not sure what the exact problem is atm\n. If there are only 10,000 lines, what does this 1 more feature vector contain? junk?\nI think its pretty bad to have this, since it means one cannot use Shogun to parse large files (we are calling ourselves large-scale machine learning toolbox)\n. If there are only 10,000 lines, what does this 1 more feature vector contain? junk?\nI think its pretty bad to have this, since it means one cannot use Shogun to parse large files (we are calling ourselves large-scale machine learning toolbox)\n. Hehe, so this is still around......\n@OXPHOS @vigsterkr this is yours I think\n. Yes, @vigsterkr put it on the milestone\nHe can comment on his own fixes there. But if we had this in, it would be great. You should catch him in IRC to push :)\n. gogogogo! :)\n. Did you check locally with valgrind?\n. Nice! Thanks for the fix! :)\n. Oh wow, thanks for figuring that out!\nI guess I can close as this patch is very old.\n. Oh wow, thanks for figuring that out!\nI guess I can close as this patch is very old.\n. Very nice patch! :) Thanks!\nSee some minor comments. Also, where does the implementation come from? Your head? GPML? GPStuff? It might be worth mentioning that. Also this will give me a hint of how much I should parse for efficiency of the codes.\n. Please bear in mind that we cannot copy code from GPStuff (which is GPL) and put the code under BSD. \n. Agreed on the not implemented things\n. Hey, if there is no other way to do things (e.g. its just implemented mathematical equations), it is totally fine to re-implement on your own without giving credit. Just if you actually copied code and only slightly modified it, you would have to mention this. Or if you re-implement something, but then you are allowed to change the license ( in my eyes)\n@sonney2k @vigsterkr @lisitsyn @iglesias comments?\n. Once licenses are added, we can merge, travis is green\n. Great! Another intro issue on your list ;) Good job!\n. Great! Another intro issue on your list ;) Good job!\n. Makes sense -- though I always count on the compiler optimisations in such cases :)\n. I agree, we should try to use as much C++11 as possible. This is well-tested and reliable code and we don't even add a new dependency.\nSo we can do most of the sampling using c++11 methods. For some multivariate distributions, we will need some amount of linear algebra operations. Those should be done against our Shogun internal (soon to exist) framework, see #1930 and #1973. Or, since the latter interface is not yet complete, use eigen3 (such as the multivariate Gaussian).\nIn additions, and maybe more important than sampling, is to be able to evaluate the pdf/cdf of those densities. AFAIK this is not supported by c++11. Also, one can do lots of things ineffective or even wrong. An example is again the Gaussian, the Cholesky decomposition of the covariance should only be computed once in the beginning (if not already specified) such that evaluating the pdf of samples does not have to do that again. For many distributions, evaluating the pdf of many points at once comes at the same cost as evaluating a single points (see again Gaussian). Another interesting feature would be to compute quantiles of a given number of points (see https://github.com/karlnapf/kameleon-mcmc)\nFor univariate distributions, pdf/cdf functions usually depend on table lookups or numerical integration (see for example CStatistics::gamma_cdf). Currently, we borrowed a couple of those implementations from ALGLIB. Apart from the problem that this is GPL, integrating such codes is not the best idea due to the overhead it creates and the impossibility of maintaining any bugs/changes since nobody understands the code. I would much rather depend on http://www.boost.org/doc/libs/1_55_0/libs/math/doc/html/dist.html which is mature, tested, etc. We can make the dependency optional since most shogun packages dont need complicated distributions. Using external libs should always be done in a way that we have minimal dependency.\nStan can serve as a\n- inspiration how to represent distributions\n- we could also borrow code for complicated PDFs\n- inspiration for auto-diff (which is out of scope for the mcmc GSoC project though)\n. Final point: We want a unified interface in Shogun, so merge all the existing probability classes into one\n- maximum likelihood learning\n- some other methods, see CDistribution\n. @sonney2k is currently dead - @vigsterkr @tklein23 I would merge this and have the debug stuff as a separate flag. Thoughts?\n. Thanks for this!\n. Thanks for this!\n. Sounds good but totally out of my expertise....\n. SWIG and matlab do work together, there is a number of project who are\ndoing it. Check the issues if you want to help us re-enabling this feature.\nMainly requires someone to dive into it. Interested?\n2017-11-21 12:41 GMT+00:00 Fernando J. Iglesias Garc\u00eda \nnotifications@github.com:\n\nThis will depend mainly on whether Matlab becomes supported by Swig.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/shogun-toolbox/shogun/issues/2017#issuecomment-346014947,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAqqv0fj75lcXt2H0jY54qr2aoxQHzSrks5s4sTugaJpZM4BqKCP\n.\n. I suggest you copy the unit test template and fill in your class that fails by hand, have this as a separate Shogun program and then debug to see whats wrong. Let me know if that helps, sometimes this stuff is tricky\n. Appreciated!\nSay, if you are doing these formatting changes every now and then, could you import all my comments on style into your formatting script command, and then put them in the readme?\nThis way we work towards a unified style in Shogun :)\n. Appreciated!\nSay, if you are doing these formatting changes every now and then, could you import all my comments on style into your formatting script command, and then put them in the readme?\nThis way we work towards a unified style in Shogun :)\n. Yeah I agree, this example should go to graphical, but most ideally to a notebook.\nThe modular examples should just illustrate API usage (see this years GSoC project on having simple API examples). They should use some of the toy data we have.\n. Very nice!\n. Ok great. This all looks like its how it should be.\n- Integration tests fail, could you double check that locally on your machine and maybe update? Though this should not be necessary. But you changed some classes, so that might be the reason. If results changed, we should investigate whats wrong\n- All unit tests pass. However, we did not test all the approximation methods for different number of samples. Could you (after this patch is merged) write unit tests for all of those. And could you send us the corresponding matlab/python whatever code that you check against, thats easiest to proofread.\n\nVery nice patch! :)\n. @sejdino Could you have a quick look over the new formulas for the non_symmetric sample numbers?\n. This is great! @sejdino and I recently actually needed MMD with different sample sizes, so thats another issue solved :) Nice work!\nI would still love to see some unit tests on some very very simple toy example, of which we can easily verify the matlab code and compare against Shogun\n. well spotted! :) Let me know where its coming from.\n. We actually want to use pylab inline, we start the notebook server with it\n. Hey,\nso most of the changes are not necessary due to the way we start the notebook server.\nAlso, by graphical python examples, I meant the folder python_modular/graphical\nAny other cleanups for the notebook are appreciated. For any notebook patch, please send a statically rendered link (with outputs)\n. Closing since no progress in the last weeks\n. Hey! Thanks for the patch.\nThis is quite far from being merged. Let us start a bit simpler: Let's do only methods for\n- sampling from a distribution (if possible, this is optional)\n- computing the log_pdf for single samples and many samples (not optional, so purely virtual methods, has to be implemented\n- initialisation, so think about constructors for different data types for discrete and continuous\n- As discussed, I would not distinguish multivariate or univariate, we can in fact do all distributions multivariate (if possible and makes sense), and then offer constructors to initialise them in a univariate way. Shogun's feature representations makes this possible\n- All methods should be against CFeatures, continuous, dense distributions can offer methods for SGVector, SGMatrix\n. Closing since no changes for a few weeks\n. http://nbviewer.ipython.org/gist/kislayabhi/9431770\n. Pretty happy with that, nice! Add an entry to the NEWS :)\n-added PCA notebook by YOURNAME\n. Could you not close and re-open, this spams my email :)\n. Could you not close and re-open, this spams my email :)\n. Lets make sure to merge the previous codes first. Then we can do this afterwards. It is very important that we actually get code merged for your application\n. This PR is too large to review properly.\nPlease aim to deliver this algorithm in parts as small as possible.\nOne PR after another adding one new methods, which is carefully unit tested.\nMaximum a few files and few hundred lines of code\n. Looks fine to me.\n. ??\n. Hi, thanks for your patch!\nHowever, this is far from being mergable. Please make sure:\n- that you follow Shogun's c++ style. You are largely violating code style, formatting, design patterns\n- dont re-implement things that are already there\n- why is all this in an example? Things should be in helper classes that are unit tested in a modular way\n- please send smaller bit PRs, adding features one by one, this is hard to review\n- document your code\n- aim for clean code that is ready to be merged. It takes us a lot of time to go through things, which in this case is not really necessary and should be checked by yourself before submitting any patch\n- if you have time pressure, focus on simple intermediate tasks rather than trying to solve everything at once\n- read Shogun code to see how things are done\n. Thanks for the update!\nYou can put this code into a class in mathematics. It is a big too big for putting it into CMath.\nPatch size:\n- first only class interface without implementations, we can discuss this\n- then implementations of all helper methods you use\n- then the final method, with unit tests etc\nIts much better already, but still quite far from being mergable. We usually do not implement methods in examples, but in the shogun core and then use example for illustration. Also make sure the method is usable from modular interfaces (ie class with interfaces with shogun base types)\n. Thanks for the update!\nYou can put this code into a class in mathematics. It is a big too big for putting it into CMath.\nPatch size:\n- first only class interface without implementations, we can discuss this\n- then implementations of all helper methods you use\n- then the final method, with unit tests etc\nIts much better already, but still quite far from being mergable. We usually do not implement methods in examples, but in the shogun core and then use example for illustration. Also make sure the method is usable from modular interfaces (ie class with interfaces with shogun base types)\n. nice! thanks!\n. nice! thanks!\n. I merged that though, but after merging the PR. What about with latest data version?\n. I merged that though, but after merging the PR. What about with latest data version?\n. thanks! sorry for causing that.\n. thanks! sorry for causing that.\n. Isnt this the duty of every developer on his own?\nEclipse for example supports all of this stuff out of the box.\nBut I agree, some tips could help, for example vim+clang_autocomplete and maybe advanced eclipse.\n. Isnt this the duty of every developer on his own?\nEclipse for example supports all of this stuff out of the box.\nBut I agree, some tips could help, for example vim+clang_autocomplete and maybe advanced eclipse.\n. I in fact totally agree :)\n. I in fact totally agree :)\n. Thanks!\nYeah, some links are cool, they structure the text and make it much easier to browse around in the notebooks\n. uh travis is broken because somebody merged a class change without updating integration tests. Ill merge this one anyways as its license only\n. Thanks for the license change!\n. @yorkerlin Hey, its nice to have fast code - thats very good to check. But again we need to structure it Just writing plan c++ example does not bring us anywhere - doing things \"later\" also is unfortunately not the way to go. Please aim to send small PRs one after another in which you incrementally build up your fast implementation. All code you show us should be written with the aim to merge it.\n- BTW Shogun has a CTime class for measuring runtime.\n- Yes use Eigen3 for now, we might want to change to a Shogun internal API for dot and matrix-vector products at some point\n- Please do not post all the program output here, you can do that in the gist in which you post the code itself.\n. @pl8787 I second @vigsterkr and @emtiyaz comments on not to translate matlab code into c++ directly. Rather try to understand what the code does and aim for a \n- readable\n- efficient, and\n- clear\nre-implementation. Test things one by one using unit-tests. Do not send these huge PRs but one thing at a time\n. Thanks for your efforts @pl8787 @yorkerlin ! It will be useful to have this once its ready\n. And again guys, please don't waste your time re-inventing dot-products for Shogun. We currently use Eigen. If you want to push this (fast, parallel dot-products, and GPU matrix-matrix products), then join work on the linear algebra API we are planning (there are issues on that)\n. Yeah, but thats all backend related. The interface will be general and then backend will do things to make sure everything is fine and fast. But the person using this does not have to deal with this.\n@lamday @lisitsyn @vigsterkr It might be worth though having some kind of way to easily switch between float32 and float64. Though I think that is problematic in general., Maybe we just have to go for double precision all the time\n. For reference data-sets the best thing would be to put them into a format that we can read from Shogun classes by hand and add them to the data-repository.\n. And users can then pre-process their individual datasets on their own using their favourite langauge\n. Thats 12 GB. You could also try to work in float32.\nWhat happens if you loop over the test examples without holding them in memory? I guess thats what you meant  @emtiyaz ?\n. Ok, nice!\nOnce my comments are resolved, this is ready to merge. And then we can do the whitening preprocessor based on this class, which is nice since it avoids having duplicate code\n. Sorry that was a mistake\n. Totally agree with everything :)\nSorry for delay, had some presentations and deadlines over the last days\n. Have a look at the string features examples. The main point is that training data, rather than being a matrix of a certain type (with fixed dimensions), it rather is a collection of sequences of characters (might be any type), which might be of different length.\n. Very nice initial work!\n@sonney2k Also have a look if you have comments.\nLet us do a few iterations to get this to an even cooler state:\n- Could you write an abstract that ties in well with the other ones on our webpage? Like the GP or GMM one, with a quick word on what the notebook is about, and what things you cover, and links to the wikipedia pages of the methods (on-the-fly in the text, see GMM notebook)\n- When do sampe the data, could you say why you are sampling exactly this one? Also, since you are sampling from a GMM, please plot the underlying densities (with 95% contour lines or heatmaps) as done in the GMM notebook, this gives a better understanding what the data-generating process\n- Please dont write text in math mode (for writing operators such as sign) either do  \\text{sign} or define your own operator in latex code (can do that in notebooks)\n- For all mathematical objects you use, before you use them, define them properly, in particular from which space they come. \n- Could you extend the paragraph on the foundations of MKL and mention which algorithms are used, how this is done in Shogun, what MKL classes shogun offers, etc\n- Describe the parameters of the MKL classes in that paragraph too, such as the norm \n- Before starting, please briefly explain the concept of combined features and combined kernels in Shogun. This is quite important so I think its good to have a separate little paragraph on this rather than doing it on the fly\n- Structure the notebook a bit more into sections, the first section is very long. Mention what you are going to do in every section. This makes it easier to read\n- I really like the decision boundary plots with the individual kernels and the combined one\n- Please use a spell checker, there are some typos. Also there are some whitespace errors, such as no space after a full stop\n- Just saw the mathematical formulation. This should go in the beginning with a comment like (skip if you just want code examples)\nOk, enough comments for now. Looking forward to iteration two :)\n. Ok here we go:\n- add your name to the beginning of the notebook. See GP notebook for example\n- \"So in a svm, defined as:\" Dont start sentences with \"so\".\n- SVM should be all capital in text\n- \"Multiple kernel learning is about using \" should be \"Multiple kernel learning (MKL) is about using \" since you are using the abbreviation later\n- in the mathds for the MKL program, there is some whitespace problems between 1 and C. Also, what is \\gamma?\n- \"here C is a pre-specified regularization parameter \" Should start with capital letter and end with a full stop\n- Shogun should be written with capital letter\n- \"For 1-norm MKL using one of the two approaches described in [1].\" This is gramatically wrong (no real sentence) please pay more attention to the way to write text\n- You mention  glpk or cplex, but the reader has no idea what those are. Add links to documentation in Shogun and maybe even say what those methods are doing.\n- SVMlight should link against Shogun documentation\n- \"preiction\" typo, please use a spell checker\n- \"lets\" is \"let's\"\n- I like the ellipses of the GMM. Can you do axis(\"equal\") to make things look nicer? Also always add a title and axis labels to each plot\n- \"Using a Combined kernel:\" This is nice. But then when you generate data, that should be a next section. \"MKL-Classifcation on a toy problem\" or similar\n- \"and and \" Please do more proofreading of the notebook before you send it.\n- in the python code, please dont do comments in the same line as code, always in a seperate line\n- In the heatmap of the classifier, could you plot the data points?\nNice! I like the new example you put also. I think after these comments, we either have one more iteration or can merge directly. I haven't carefully read everything yet, let me know when the updates are in. Then I'll have a look at the rest\n. \"In the multiple kernel learning problem for binary classification one is given N data points (xi,yi ) (yi\u2208\u00b11), where xi is translated via K mappings \u03d5k(x)\u2192RDk, k=1,...,K , from the input into K feature spaces (\u03d51(xi),...,\u03d5K(xi)) where Dk denotes dimensionality of the k-th feature space.\"\nI am not an MKL expert here, but this seems a bit weird. Why is the feature space R^D_k. First of all for many kernel the feature space is infinite. And also its not the real space, why do you use R. Where is this math coming from?\n. Sorry I have to leave now, will do more comments tomorrow\n. \"It is based on the GMNPSVM Multiclass SVM. Its termination criterion is set by set_mkl_epsilon(float64_t eps ) and the maximal number of MKL iterations is set by set_max_num_mkliters(int32_t maxnum).\" Could you add some comments on what values you put in there and why?\n. \"We use the USPS digit recognition dataset.\" Maybe put a link to something where this is described or so\n. \"loadmat('../../../data/multiclass/usps.mat')\"\npls use os.sep.join([list_of_folders_and_filename]) to stay OS independent\nThis should be done everywhere where you handle file paths. Also pls do a try catch\n. Keep in mind the notebook is started with --pylab so you dont need to import all those things such as random.\n. \"subset = random.permutation(len(Yall))\nXtrain = Xall[:, subset[:5000]]\nYtrain = Yall[subset[:5000]]\"\nAdd a little comment what you are doing there and why\n. \"We combine a Gaussian kernel and a PolyKernel. \"\nPls link the Gaussian kernel in shogun docs\n. Really like the digits example. But I think it would be good to say something on the kernel weights and analyise which kernel is good in capturing which feature of the digits. If you just apply MKL to this, you dont really gain something. Another way could be to have MKL with a few Gaussian kernel widths and then use this to select the best bandwidth or so? Then the example would have some sort of conclusion\n. The one class MKL example produces a warning. Pls get rid of that, or make sure it doesnt appear\n. I really like the one-class example!\nBut again, why would one do MKL here. Pls do some analysis of the resulting kernel weights.\n. Pls mention the one-class example under outlier detection in the abstract of the notebook\n. Very nice work: nice pictures and text! This can soon be merged!\n. I am still a bit unsure about the Math. But if you have it from the paper, that should be ok\n@sonney2k could maybe comment if he has some time (unlikely ;) )\n. Nice, thanks!\nLet me know once you addressed all the comments. Please also check for whitespace errors. (. without any spaces) I saw a few when scanning through the latest version\n. Well done!\n. Could you mention this notebook in the NEWS file?\n. I like the idea! Do you think its possible to unify this? The examples are quite different, some are interactive. However, if we for example separated all computing code from plotting/graphical code, this might work....\n. Ok, so maybe that can be the reference dataset?\n. Ok nice, that is much cleaner and easier to review than the earlier PRs.\nI added some comments and questions, let's go through them.\nCould you try to explain why you made this a subclass of LogitLikelihood?\nAlso, you will have to merge all commits to max. one per file.\n. Mmmmh, but the piecewise bound is not something that will be used as a likelihood model by a Shogun user. It's only used internally in variational inference class. So I suggest you rather have an internal object of the logit likelihood.\n. Cool, we are getting there!\nMaybe you should read a bit in other Shogun classes to get a better feeling of the style we use (SGVector pointers etc)\n. Yeah travis seems not your fault (very good to check locally before submitting)\nI will try to get this working again soon\n. So no, please dont pass function pointers to eigen. In Shogun, we rather do operations on SGVector directly.\n. Hi!\nSorry for the delayed response. We have a problem here: the PR is too big It is not really possible to review this - it takes very long. The code is good (at least as far as I can tell)\nWe though need to work on communication here. Could you please send a new PR with\n- changes in maximum 3 files, better less\n- all the small extensions that you added (like the Gaussian pdf) one-by-one in seperate PRs, with seperate unit tests each\n- It will take a few patches to reach the current state, but those each will be much easier to overview, discuss, improve, and test.\n- We have to do this as we cannot really give feedback for such big patches - I get lost too quickly\n- Also it makes it much easier to keep the big picture of what you are trying to do and why certain extensions are there\n- I suggest we start of with all the helper functions you added (like Gaussian pdf)\n- BTW: Always evaluate PDF functions in log-domain, to avoid over/underflows.\nPls dont get this wrong. The code is good (few minor glitches here and there, but that's fine we can work on that). But if we want to do GSoC, we have to merge things in digestible bits. Keep in mind me and emt are doing this on the side, so we cannot spend hours reading through code. Small bits  are therefor more likely to get feedback as it just takes a moment to do.\nLet me know if you have any questions on this.\n. Nice! :) Next one would be the null approximation tests I guess?\n. Plots are good to verify, but we need something which does some rough checking whether things are working. Doesnt need to be super accurate, but just to prevent obvious failures .... I mean we have a pretty good idea of how the distributions look like. Maybe compute some .9999 quantile on your machine and then assert against that\n. Very nice work! :)\nI second Fernando on having a single patch for all those changes\n. But in fact, its also fine to have seperate commits for each notebook. So fine to merge after the minor comments are resolved\n. argh, we need travis to be green to merge this. Otherwise its too risky.\n. @sonney2k any ideas on the memory leaks for the line reader?\n. @pl8787 Yeah we need a PR against data, and then you need to commit your data version and push it into this PR here.\nI like the code you wrote. Are the results consistent with @emtiyaz 's Matlab code?\nWhat about also translating that into a notebook and add some plots etc?\n@emtiyaz do you got any comments \n. Just to avoid confusion about this. Shogun's kernels are sometime parametrised differently than the GPML ones.\n. Hey!\nSorry for my delayed response. \nOk the first thing we should really look at is to prevent recomputing kernel matrices in the loop. In the example, it is possible to compute the kernel  matrix  once and then use submatrices within the loop. This has the advantage \n- that we only compute it once per data pair\n- that we do not allocate and free memory all the time (benefits of using c++)\nWe should do the very same thing for the features if possible. Please try to avoid re-allocating matrices in the code, but do this in front of loops. Inside, you can just re-used (and overwrite) the old one. Also note that CFeatures supports subset index sets, same as CCustomKernel (the precompute kernel matrix instance). Before we go on to more fancy stuff, this really should be clear and well thought through. Whenever one wants to write large-scale code, it is extremely important to think about such issues, in particular it is important for this project.\nSo carefull when you write something as SGMatrix<T>(num_foo, num_bar), it allocates new memory. \nPlease let us focus on one case for now (say regression), to keep the PR small.\nMore concrete:\n- generate_features allocates memory for users. Not necessary, and slows things down. Use subsetting on all data or, alternatively, subsetting on the kernel matrix\n- Gaussian processes with precomputed kernel matrices might in fact currently not work. I suggest you produce a new PR with a minimal working example. This might reveal (minor) things that need to be fixed.\n- Once this is done, modify one of your examples to work with a Custom kernel to avoid re-computing kernel matrices and re-allocating memory.\nLet's focus on getting one example to work properly and efficient rather than starting new stuff all the time. Let me know if you have any questions\n. Hi!\nYes, it would be good to do a minimal example with the CustomKernel. This allows us to identify possible issues. Pls have a look what the CustomKernel implementation does internally. It basically fakes the init method with Custom features. Passing another kernel instance in the constructor evaluates the full matrix and storing it. But the issues you encountered are exactly those that I mentioned earlier. Should be quite easy to fix. Let me know if you have any problems.\nH\n. See my comments there. Once thats done, we can come back to this\n. Updates?\n. Updates?\n. any updates here?\n. any updates here?\n. closing as inactive. That's a pitty. @pl8787 we should get this in at some point!\n. Thanks!\n. This is still the case with version 6.1.3\nCMake Error at src/interfaces/r/cmake_install.cmake:44 (file):\n  file cannot create directory: /usr/local/lib/R/site-library/shogun/libs.\n  Maybe need administrative privileges.\nCall Stack (most recent call first):\n  cmake_install.cmake:38 (include)\nused -DCMAKE_INSTALL_PREFIX=/home/heiko/bla. Good first issue for people interested in cmake. Great patch, thanks.\n. Could you make this happen @dhruv13J ? Thanks!\n. Could you make this happen @dhruv13J ? Thanks!\n. Nice, thats what we want, but I dont see the point in keeping the old method.\nIt should only be used for integers. Should be one call and then internally figure this out, whether to do the fequals or integer comparison\n. Why can't we template the method, we should not have double implementations for floating numbers ?\nOK for integers - the code is different anyways ...\n. MMh, but then we just need to offer another method for such cases (int, compex), but at least we can do float and double in one block.\nI just restarted travis since there was a weird failure.\nLet me know\n. Any updates here?\n. Closing as no update in two weeks\n. Closing as no update in two weeks\n. I guess this comes from a discussion. Could you briefly explain me what the motivation is behind this? Sorry I did not follow yet @lisitsyn @lambday @sunil1337 ?\n. @lambday please take the lead here. I will interfere if I don't like something. I guess I asked because I remember that we had the linear operator in this way and then reverted to the current mode. But if this interface will allow to represent everything (you arguments make sense) then pls go.\n@lisitsyn should probably comment on things here, he is the c++ god.\n. @sunil1337 I think this patch might break notebooks (and maybe modular examples, but these are catched by travis) Please double check that\n. Ok, great work.\nLooking forward to get this on. Sorry for my delayed response.\n@lambday pls tell me when you think things should be merged on this topic. I sometimes don't have the time to looks through things in detail but merging is always fine if you recommend it. It also keeps up momentum.\n. The singleton idea is nice, but actually we do not really need this. All of Shogun uses linear algebra, so its not a waste to instantiate the object. Also we want to be consistent with the rest of Shogun \n. home/heiko/git/shogun/shogun/src/shogun/lib/SGVector.h:58: Warning 503: Can't wrap 'operator std::complex< double >*' unless renamed to a valid identifier.\n. Could you pls address this issue?\n. Could we please avoid to have 5 PRs for one patch?\n@grecocd you can just update your old PR. Thanks!\n. Just had a quick look and I like where this is going!\nPls keep on iterating over notebooks, it greatly improves quality if a few people try to improve a few things a couple of iterations\n. KDE is essentially a mixture of Gaussians. We can put this under our distributions framework if someone bothers to re-write the API. For now, I would say, we should discuss the interface a bit. CDistribution doesnt seem to be a good match as @vigsterkr pointed out. @iglesias what are your thoughts?\n. Ok, why not use Distribution for now? We can then switch to MixtureModels once they are in\n. Any updates here?\n. Such a nice idea! :)\nIll give a few comments, and then we can iterate this.\n. Pls add your credits to the notebook. See the others\n. Could you pls link all algorithms that you mention to either wikipedia, and if you are referencing Shogun classes to the class documentation?\n. What about getting some better training data?\nOr alternatively, find digits in a handwritten sudoku?\nSomething that makes it work in the end\n. Pls also link to all opencv methods in their documentation\n. Talk a bit about whats going on when you correct the image skewed perspective\n. pls try to keep your python code a bit shorter. If necessary divide into sub-functions\n. What about showing all segmented numbers in a grid?\n. Step 3: Training\nPlease give a little more introduction (what methods, why, etc) and a better title\n. Same for predictions\n. I dont really like the printed sudoku output.\nWhy not just printing the array as a block?\n. ok thats it from me for now\n. Looking forward to see this growing!\n. Yeah! Very nice.\nI would do some axis(\"off\") on some of the plots (the array of digits).\nI think we can already merge this for now, pls go on improving it - this version is already so much nicer to read than the older one.\n. @vigsterkr Sorry, my fault. I reverted this in #2108\n@ahcorde You probably did not know that, but as Viktor said, we require you to send notebooks with output removed (not in the gist link though). Also, the jpg file should go into shogun-data. This should be quick to clean up.\n. Nice, I will merge already. Pls think about the vector version and add if you think its necessary.\n. Hej!\nOk this is in the direction of where we want to go. However, I am a bit unsure what you are exactly trying to do here. Adding feature index methods for the inference methods - isn't that something that should work under the hood? A user of this class just wants to subset the kernel object (on their own) and all the GP stuff should just work without any changes under the same interface. So the inference classes would then internally check if the kernel is a custom one and react appropriately. \nBut if you have some string arguments to change the interface here, please let me know, happy to discuss such things.\nBest\n. Ok, I will think about this and get back tomorrow (travelling so should have some time)\n. Having thought about this again, I don't understand why this is hard?\nYou can easily represent the \"data\" as indices in your program. Then the loop just re-sets the subsets on the kernel matrix and off we go. The GP framework then passes dummy features to the custom kernel, and all the get_kernel_matrix() calls do not do anything but looking up the precomputed matrix elements.\nRepresenting the data as indices also has the advantage that you do not re-allocate memory in every iteration.\nLet me know what you think!\n. Adding a new method to the kernel is useful in this case, but it would better if we could do something general here (sorry for dragging you into this, this is more interface design now).\nWhat about keeping the init method of the custom kernel as it is and just changing the features underneath to store the indices of the custom kernel. This way, all kernel code in Shogun should work immediately without any modifications: Setting subsets on features and then calling init changes the kernel subset. \nLet me be more specific:\n- Have a new feature sub-class (or add to CDummyFeatures, better) which holds an index set\n- Modify custom kernel's init method (which accepts dummy features) to update its row and column subset if the dummy features contain an index set.\n- In your code, you then pass the index sets to the dummy feature objects, which are internally passed to the custom kernel, so you dont need to mess around with the kernel directly, the GP code does all this for you.\nI would like this solution as it is very clean. @sonney2k @lisitsyn thoughts?\nTo answer your question: the float32 matrix method is to reduce memory footprints when storing kernel  matrices (factor 4)\n. Have a look at \n- CCustomKernel::init\n- CCustomKernel::dummy_init\n- CDummyFeatures\n. We should discuss this before you start writing things. I dont think its necessary to add a new class, and if we do, it should be a subclass of dummy features. But maybe we can even extend the existing one?\n. Maybe we can draw a little class diagram in dia?\nI think this class should in fact inherit from custom features. And yet, the init function then can do the job. Keep in mind that adding subsets to features also still should work with this\n. A little interface draft would be nice, this can be in c++, but doesnt need to be in PR quality yet, just to discuss\n. Yep thats good, though I would make those a subclass of CDummyFeatures.\nAlso, you will have to think of add_subset methods.\nBut +1 for this design. This is in fact exciting as it might allow to work with Custom kernels for all of Shoguns methods without the modifications that are curreently necessary\n. Keep in mind, your class will only manage indices, so no direct feature access, just the number and permutations of the kernel matrix, so most methods of CFeatures are not necessary. However, the subset permutation stuff would be good to have for the custom kernel too\n. Yes\n. It is not even yet possible to do what we intended to. I will close this PR for now, all the added things will not be needed in the future. You should code up a minimal example of a GP with a custom kernel next. No new methods added. But we need to alter the behaviour of CustomKernel first\n. It is not even yet possible to do what we intended to. I will close this PR for now, all the added things will not be needed in the future. You should code up a minimal example of a GP with a custom kernel next. No new methods added. But we need to alter the behaviour of CustomKernel first\n. @yorkerlin @emtiyaz \nVery nice code! This is also much easier and fun to read than those huge PRs (though still large, this size should be the maximum for PRs)\nSee my comments (most are minor, some are questions, mostly documentation things).\nOnce all this is addressed, ready to merge.\nAgain, very nice work :) !\n. Hi!\nAs said, pls address all of my comments and discuss if you are not sure what I mean or if you dont agree. But dont do this done stuff. Just update the PR, otherwise I loose overview (as now, confused what is in and what is not)\n. Hej!\nThe comments now are very minor. So ready to merge soon.\nI like this workflow much more!\n. Ready to merge. This valgrind thing irritates me. I don't actually get what is going on there. I just want to make sure that there are not memory leaks. (You added some more SG_UNREF calls, so I assume they leaked before). But ok, this is ready to be merged now. Very nice patch and lets keep the workflow like this.\n. Nice thanks!\nWhat is missing is to commit the new data version so that the image is available\n. Data version commit is still missing!\n. if you do a git status, you will also see the \"data\" you have to update\n. sweet! Please add this to the NEWS\n. sweet! Please add this to the NEWS\n. I agree, drafting is very important for this! I guess one first step would also be to wait for @sonney2k  comments since he wrote all this stuff (and he usually does not do things without having some thoughts)\nSome of my thoughts:\n- Singleton drawbacks can easily addressed via putting the instances into a map if one needs more than one: http://en.wikipedia.org/wiki/Multiton_pattern\n- I agree, not all objects should have to store IO/parallel. In fact, it always irritates people that one can change behaviour globally via any instance. I always do this awkward GaussianKernel().io.set_log_level(1) thing in python\n. Let me try to understand what you are after, currently I am a bit confused. Both parameter and model-selection parameters are bound to classes (which parameters are registered), but in fact also to instances of classes since the TParameter pointers point to the member variable memory.\n. Its not that different: Both parts allow to register member variables that already exist in memory for accessing them from outside. If you do not register model selection parameters, the list is empty, the overhead is minor, but in fact we could make the container NULL if there is nothing in there to reduce the footprint of CSGObject.\nA way to separate things would be to have a separate base class for algorithms or things that are used in algorithms, and then another base class (maybe the Shogun base) for framework parts. But I dont really see the use here. But maybe you know better?\n. Its not that different: Both parts allow to register member variables that already exist in memory for accessing them from outside. If you do not register model selection parameters, the list is empty, the overhead is minor, but in fact we could make the container NULL if there is nothing in there to reduce the footprint of CSGObject.\nA way to separate things would be to have a separate base class for algorithms or things that are used in algorithms, and then another base class (maybe the Shogun base) for framework parts. But I dont really see the use here. But maybe you know better?\n. All classes that register parameters for model-selection need the framework. Look for MS_AVAILABLE in the SG_ADD macro. Not sure where this is going now though\n. All classes that register parameters for model-selection need the framework. Look for MS_AVAILABLE in the SG_ADD macro. Not sure where this is going now though\n. Yep I agree,\n- data classes (CFeatures)\n- algorithm classes (CMachine and more, not so clearly distinct from others)\n- helper classes (CMath, file readers, etc)\nIf those could have base classes for dealing with things as model-selection parameters, this would  maybe be a bit cleaner.... \nIn fact, we could in general think about a more clean class hierarchy. Currently, there is lots of wild growing everywhere. Drawing class diagrams might help.\n. Yep I agree,\n- data classes (CFeatures)\n- algorithm classes (CMachine and more, not so clearly distinct from others)\n- helper classes (CMath, file readers, etc)\nIf those could have base classes for dealing with things as model-selection parameters, this would  maybe be a bit cleaner.... \nIn fact, we could in general think about a more clean class hierarchy. Currently, there is lots of wild growing everywhere. Drawing class diagrams might help.\n. This will all be better with tags, once merged. apply_inverse ? \nThis should be defined for a single vector and for a full feature instance\n. Cool, thanks! :)\n. Updates?\n. and another ping :)\n. @iglesias @parijat could you take care of this one? Closest to your project. Thanks\n. Updates?\n. Ok, I merge now, and you can send another patch with the documentation updates.\nThe next step is to make the custom kernel work with those guys here in the init method\n. Dont worry about travis here, its being weird\n. thanks!\n. Ok this is good. \nNow we have a problem as I just realised.\n- get_kernel_matrix returns a newly allocated matrix if a subset is active\n- we don't want that, this sabotages the whole point of what we are trying to do here\n- in Matlab, indexing matrices does exactly this, copies the memory.\n- We however want something as in python's numpy: same memory, different views\n- This is because, the GP classes work with kernel matrices directly. Not with the kernel() method.\n- So we need some kind of view on matrices for this to work.\n- before we do that, we need to make sure that the GP dont modify the kernel matrices (can you double check that everywhere? Needs to be changed if it happens\n- Once we are sure that kernel matrices are not modified by GP, we can think about some way to have views on matrices (like a subset in CFeatures, but now on SGMatrix) This should be also discussed with the others. I will open an issue for that\n- we are now in the middle of modifying Shogun's internals quite a bit, hope you enjoy that :)\n. Have a look at the subset stack architecture of CFeatures\n. ok, so what about moving this multiplication to some other place so that is is done in-place of the expression where the matrix is used in but without modifying the matrix itself?\nA*b gets (A*scale^2) * b\nModifying and then reverting is a bad idea. \n. This should be done in a separate PR to make sure that all unit tests are passing and you are not changing the behaviour. Also, pls update the documentation on this.\n. Ok this is almost ready to merge.\nThe next big problem is to think about get_kernel_matrix which for a subset will copy the matrix. But this is not what we want, so we need to think about a solution to this problem.\nLet me know your thoughts\n. No I mean just an API example for the subsets on the kernel where you create a matrix and add a subset. In python, this can also be an integration test (just do the same as the other examples)\nOnce this is done we can get back to the movie len thing\n. restarting travis on this to see whats going on\n. restarting travis on this to see whats going on\n. I dont see a GPU dot product here :)\n. Nice! Thanks for that.\nNow for these very very simple cases, what about some convenience methods that work without instances? (do the internally or so)\nDenseEigen3DotProduct<T>::dot(SGVector<T> a, SGVector<T> b)\n@lambday @lisitsyn @sunil1337 comments on that?\n. Ok then, pls see my (minor) comments. We can merge once those are resolved. Dont worry about the convenience methods I mentioned above for now. This is already very good work!\n. Ok nice so far so good. \nNow this example screams for documentation what it is about, what one can see, etc.\n. Ill do more comments once these style issues are addressed.\n. On this correctness business:\nThis should be done in unit tests. We do only want examples to be used for (API) illustrations. This current example is quite complicated to read and does not illustrate things in a good way.\nWhat do you think of making you checks for correctness unit tests instead? We still might keep the example for illustrations. Thoughts?\n. On this correctness business:\nThis should be done in unit tests. We do only want examples to be used for (API) illustrations. This current example is quite complicated to read and does not illustrate things in a good way.\nWhat do you think of making you checks for correctness unit tests instead? We still might keep the example for illustrations. Thoughts?\n. OK nice.\nSo apart from the inverse business I only am a bit sceptic of whats the purpose of this program:\n- examples should be simple and API only examples. Could you condense this out of this little program?\n- unit tests test for correctness of implementations, since you are comparing against some data, maybe move the stuff there?\n. OK nice.\nSo apart from the inverse business I only am a bit sceptic of whats the purpose of this program:\n- examples should be simple and API only examples. Could you condense this out of this little program?\n- unit tests test for correctness of implementations, since you are comparing against some data, maybe move the stuff there?\n. Just restarted travis pon the shogun-data errors, lets see if it works now\n. Just restarted travis pon the shogun-data errors, lets see if it works now\n. Yeah I know that. However, I am thinking on how we can put this into Shogun in a more useful way. See what I mean? Think Shogun here. Let me know your thoughts. Otherwise, the code is almost ready to merge.\n@emtiyaz maybe you have some more comments on what to do with this program?\n. Yeah I know that. However, I am thinking on how we can put this into Shogun in a more useful way. See what I mean? Think Shogun here. Let me know your thoughts. Otherwise, the code is almost ready to merge.\n@emtiyaz maybe you have some more comments on what to do with this program?\n. Another question is where these codes will be use(d, full) later? Always good to keep such things in mind. We dont just want to solve entrance tasks here, but push Shogun's internals\n. Another question is where these codes will be use(d, full) later? Always good to keep such things in mind. We dont just want to solve entrance tasks here, but push Shogun's internals\n. Ok, looking forward to see this.\n. Ok, looking forward to see this.\n. Ok then. All fine with that.\nWhat bout unit testing things? Do you think that would make sense?\n. Ok then. All fine with that.\nWhat bout unit testing things? Do you think that would make sense?\n. Travis fails, you have to sort out the data-version (has to match your merged data PR)\n. Travis fails, you have to sort out the data-version (has to match your merged data PR)\n. As this script is an example, it would also be nice to put in some more source code comments on what is going on (not just the matlab code but also intuition and big-picture)\n. As this script is an example, it would also be nice to put in some more source code comments on what is going on (not just the matlab code but also intuition and big-picture)\n. Sweet, I like that you investigated the inversion issue. One thing: sometimes matrices (especially when growing large) numerically become non psd (small negative eigenvalues due to rounding errors). So its good to keep that in mind and maybe check. But I think its ok here.\nEDIT: Just saw you are checking. Thats good.\nSo I guess that illustrates the point about not using matrix inverses quite nicely ;)\n. Sweet, I like that you investigated the inversion issue. One thing: sometimes matrices (especially when growing large) numerically become non psd (small negative eigenvalues due to rounding errors). So its good to keep that in mind and maybe check. But I think its ok here.\nEDIT: Just saw you are checking. Thats good.\nSo I guess that illustrates the point about not using matrix inverses quite nicely ;)\n. About documentation. You should add a little description for the example. In the code, that should be just some source code comments on what is going on roughly. And then there is the descriptions folder in examples, and generate_documented.sh which generates documented examples from the undocumented ones and the description files.\nImagine someone who don't know what variational inference is reads this code. You should explain\n- what problem it tries to solve, both mathematically and intuition\n- how it is structured, i.e. a list of what all the helper functions do\n. About documentation. You should add a little description for the example. In the code, that should be just some source code comments on what is going on roughly. And then there is the descriptions folder in examples, and generate_documented.sh which generates documented examples from the undocumented ones and the description files.\nImagine someone who don't know what variational inference is reads this code. You should explain\n- what problem it tries to solve, both mathematically and intuition\n- how it is structured, i.e. a list of what all the helper functions do\n. Its fine with not having unit tests here, I was just asking on what you think.\nI liked how this PR went - clean code, good size, good discussion. Well done!\nWill merge. Pls send the doc in another PR\n. Its fine with not having unit tests here, I was just asking on what you think.\nI liked how this PR went - clean code, good size, good discussion. Well done!\nWill merge. Pls send the doc in another PR\n. Ah s, you have to rebase against develop, I currently cannot merge this\n. Ah s, you have to rebase against develop, I currently cannot merge this\n. no need to resend the PR.\nYou can just rebase, and push to your fork with --force\n. Let us postpone this until we have made up our mind about the parameter framework.\n. Hi @srgnuclear  I was away for a while. But back now.\nshallow copies can be done via the copy constructor I guess?\n. cheers :)\n. haha :)\n. Very cool that you spotted this, and the way of writing a (faling) unit test before fixing the bug is totally right!\n. Very cool that you spotted this, and the way of writing a (faling) unit test before fixing the bug is totally right!\n. Hey!\nJust looked at the neural code for the first time. Nice!\nOn my (minor comments): Try to aim for a super clean coding style and error messages, documentation. Developers that work on this stuff after you will appreciate the former, user experience massively depends on how well the latter are.\n@lisitsyn you should encourage this a bit more! :)\n. Thanks!\nVery interesting to look into this :) I just got curious.\nBTW, I think there should be some re-finement work being done on the notebook. To make it read a bit easier, visually more pleasant etc. I might send some comments soon\n. Thanks!\nVery interesting to look into this :) I just got curious.\nBTW, I think there should be some re-finement work being done on the notebook. To make it read a bit easier, visually more pleasant etc. I might send some comments soon\n. Ok cool! Now what about some unit tests for the changed parts? Or are they already tested?\n. Ok cool! Now what about some unit tests for the changed parts? Or are they already tested?\n. Ill leave merging for @lisitsyn but I am fine with this\n. Ill leave merging for @lisitsyn but I am fine with this\n. just restarted the mac os travis. @mazumdarparijat the id3 unit tests fail on osx every now and then. Any ideas?\n. just restarted the mac os travis. @mazumdarparijat the id3 unit tests fail on osx every now and then. Any ideas?\n. All green now! :)\n. All green now! :)\n. I think @tklein23 is totally right and we should have a subclass of CSubsetSet which is called CAmnesicSubsetStack (cheesy name suggestion ;) ) And this class should just not remember anything, but just update the current one.\nIt has to be a separate class though - we cannot make them permanent since cross validation puts on and removes elements, and if there were some before, that requires only removing the last.\n. I think @tklein23 is totally right and we should have a subclass of CSubsetSet which is called CAmnesicSubsetStack (cheesy name suggestion ;) ) And this class should just not remember anything, but just update the current one.\nIt has to be a separate class though - we cannot make them permanent since cross validation puts on and removes elements, and if there were some before, that requires only removing the last.\n. Hi,\nah lots of problems:\n- CFeatures has this given interface for subsets, we might have to change it. Having another CSubsetStack subclass wont solve the problem since CFeatures/CLabels instantiate the thing on their own\n- We cannot just replace subsets. Imagine there is already an active one on the feature object, introduced by some other part of shogun. Deleting that might cause problems. All previous subsets should still be able to be removed\n- As I see this currently, we need a hybrid/burst mode. A method like set_amnesic_mode(bool), which will tell the subset stack to add subsets in an un-reversible way from now on, while keeping the previously added active subsets in memory. Once the mode is de-activated, the previous state before calling the method is restored. This would be some kind of \"I will add loads of subsets now and not remove any until I set the flag to false again, which removes all subsets I add from now\"\nWhat are your thoughts on that?\n. Hi,\nah lots of problems:\n- CFeatures has this given interface for subsets, we might have to change it. Having another CSubsetStack subclass wont solve the problem since CFeatures/CLabels instantiate the thing on their own\n- We cannot just replace subsets. Imagine there is already an active one on the feature object, introduced by some other part of shogun. Deleting that might cause problems. All previous subsets should still be able to be removed\n- As I see this currently, we need a hybrid/burst mode. A method like set_amnesic_mode(bool), which will tell the subset stack to add subsets in an un-reversible way from now on, while keeping the previously added active subsets in memory. Once the mode is de-activated, the previous state before calling the method is restored. This would be some kind of \"I will add loads of subsets now and not remove any until I set the flag to false again, which removes all subsets I add from now\"\nWhat are your thoughts on that?\n. - the features are creating the instance on their own because we want one instance. In fact to the outside world, we just want to add/remove subsets, and not mess around with the stack.\n- we cannot replace the subset stack. That is not the point of this mechanism. What if there already was some things on there? Those are lost then\n- sub-classing also does not work because of the two above reasons\n- the burst mode would solve all problems in an elegant way, I don't get why you don't like it. Its just a way of telling that stack that from now on, many subsets are added, but they dont need to be removed on by one but all of them at once, keeping any old stack\n. There are a few thousand iterations for large dimensional data. This means we store a vector each of dimension N + (N-1) + (N-2) + ...  +1 in the worst case. The burst mode would only store one N dimensional vector. \nThe interface also is clean, let me give an example:\n```\nfeats=RealFeatures(array([0,1,2,3,4,5]))\nfeats.add_subset(array([0,2,4,5]))\nfeats.add_subset(array([0,1,2]))\nfeats.get_subset_stack().set_burst_mode(true)\nfeats.add_subset(array([0,1]))\nfeats.remove_subset() # throws an error\nfeats.add_subset(array([0]))\ncould add more subsets in a row here, each of which causes the old one to be forgotten\nback to where we were before burst mode\nfeats.get_subset_stack().set_burst_mode(false)\nnow there are the old two subsets\n```\n. This is not possible. You get a feature object that might or might not have a subset assigned. The point is that you dont care, the object just behaves as if it had no subset stack at all but was smaller. For example cross-validation, adds and removes subsets. But in fact the feature object could have a subset even before. Therefore, we cannot just remove/replace, and this in turn does not allow to change the object to an instance of another class. See what I mean?\nAs for changing behaviour, I agree this is not good. But here its not changing behaviour forever, but only for the scope where you want to burst. This stops afterwards. Also, a hybrid mode is in fact desirable: we want to remember old subsets but forget new ones. \nJust discussing, let me know what you think. I just want the best solution so open for other ones\n. Nope, doesn't work either. The subsets themselves are not stored, but the merged version with all past ones with them (to avoid lookup chains). Effectively, its a single index vector which might be created from multiple subsets. Thats why its not possible to modify them since that influences the previously added ones implicitly.\nThe burst mode can be undone, thats the point, stack is not destroyed. Its like adding a landmark to which you can return, but only to this. And also, nothing new to be stored, just one more case in the add_subset method.\nLet me know, maybe there is another better way?\n. The stack works differently, maybe have a look in the implementation, examples, tests. (But maybe I also dont get your pseudocode - these methods dont exist so I dont know what they would do)\nThe things that are stored are the active index sets, which work on the original data.\nfeats=[0,1,2,3,4,5,6]\nfeats.add_subset([3,4,5,6]) # active index vector: [3,4,5,6]\nfeats.add_subset([1,2]) # active index vector: [4,5]\nfeats.add_subset([1]) # active index vector: [5]\nThe active ones are stored, if a subset is removed, the previous index vector is set active. I did this for fast removal and to avoid lookup chains. Fast lookup, fast removal, fast adding, some memory costs. Burst mode would just add a new active vector and the modify it. Until cancelled, which then restores the last active index set (and the previous ones still in memory)\n. Ok, let me try to understand this. What I dont get is \n- how this would solve our problem. In the tree pruning, its not that indices are added, but removed (from the middle). So the subsets get smaller ...\n- Why we can already do this : feats.get_subset_stack().get_last_subset().get_subset_idx() returns a vector to the latest subset. It is certainly possible to modify this. No need for new methods.\n- However, then one would have to modify the subset manually. This is non-trivial since as said, it is the current active index set on the original data. There might be a permutation in between. All possible to do from tree pruning class, but very messy. The point of SubsetStack was to remove this stuff from code and put it into a well tested class.\n. I get it. In fact, that behaviour is exactly what I am after. My point was to have a flag set for this. You want a new method that modifies the last one. \nfeats.set_burst_mode(true)\nfeats.add_subset([1,2,3])\nfeats.add_subset([3,2])\nfeats.set_burst_mode(false)\nwould be the same as\nfeats.add_subset([1,2,3])\nfeats.add_subset_burst([3,2]) # what you call restrict_subset_to_indices\nfeats.remove_subset()\nDo I understand you correctly?\n. Ok, so here is why I prefer the set_burst_mode solution: CFeatures have wrapper methods for the subsets, which pass on the added index vector to the subset stack. If we add a new method to the interface, we would need to wrap this to CFeatures too. (Reasons are within X-validation framework, which works on features - we cannot really change this anymore unles we are brave). However, a single call on the subset would cause the same behaviour, with the downside that the caller has to clean up in the sense that he has to unset the flag. BUT its not too bad if he doesnt: Removing subsets will simply not work, but the mappings will be correct.\n. Another solution might be a boolean flag for the method add_subset (with a default parameter)\n. Ok then\nfeats.add_subset(vec, reversible=true)\n- reversible=true is the default mode as implemented now, which stores all active index vectors to make it possible to remove them one after one\n- reversible=false modifies/replaces the current active index vector. This allows to add many subsets without memory costs, at the downside of only being able to remove all of them at once\nJust one option. The other is the set_burst_mode(bool) method.\n- the boolean flag in add_subset requires to change all add methods in CFeatures, CLabels, and maybe the CCustomKernel. Plus some internals in CSubsetStack.\n- the burst mode method requires to change some internals in CSubsetStack in a similar way as above. And the class gets another member to store mode\n. @mazumdarparijat @iglesias @lisitsyn opinions? Ignore the older posts, just read the last one.\n. Lets wait a bit on this to think more.\n. I just did this, seems cleanest. #2155\n. @mazumdarparijat let me know what you think, and if this is fine with everyone, feel free to implement the in-place method in SubsetStack\n. We have views now. Fine to merge once travis is green!\n. Fine to merge once travis is green!\n. I know all of those things are annoying and I dont agree on all style decisions myself, but we should at least be consistent\n. I agree with you - its annoying. BUT thats what we agreed upon, and this should be respected until we have something automagic. I would like to avoid talking about code style at all - its not that hard to remember, so pls when you write things by hand, respect Shogun's style. And I totally agree that we need something better here!\n. Lets just create some hook to do this soon.\n. No and no. I just passed the turing test! :)\n. Ah this old code is a mess!\n. Thanks for cleaning things.\n. Its still not possible to merge this.\nAlso, if you re-send things, pls put them all into one single commit\n. I see, ah the depths of the migration framework.\nSomebody basically has to touch this once again at some point, understand, and fix all the remaining errors. Works about 90% I currently have no idea why this is necessary, and I am sure it did not leak back when I added this, but now I have no idea, so its better to set this for now. Maybe put a little comment/reference to this PR so that its easier to find things later on\n. actually git blame should do it\n. All those operations should be moved to CMath, or, better to the linalg interface.\nHave a look at the dot product definitions in lingalg.\nOr, just replace the calls to those methods by eigen3, like for example eigenvalues, or similar.\n. Yeah, eigenvectors only make sense for float matrices in my eyes ....\n. BTW rather than moving this to CMath, it should be in linalg.\n@lambday can ellaborate a bit maybe\n. Check how it is done for the dot operations. We do not need a native implementation of eigenvectors, do you can check how things are done for eigen3 and then go in the same way, adding unit tests on the fly to ensure things work.\n@lambday btw should we also have a LAPACK backend?\n. For dense matrices, it almost makes never sense to only compute a few eigenvalues. Though it might be useful to have this interface as it can be the same for sparse matrices\n. Ok thanks!\n. Since travis was green last time. I will merge this now. \nPls send another patch with the updated description in the file\nNice work!\n. I removed the example from the CMakeList in my latest PR. Pls add again once it works (also include the description file)\nWhat about using fixed data? Thats the only way to ensure same results\n. Ok merging this. Also fix travis on the fly\n. o = new CSGObject(); // o->ref_count == 0\ndata->append_element(o); // o->ref_count == 1\no2 = data->get_first_element(); // o->ref_count == 2\ndata->delete_element(); // o->ref_count == 2\nAll this assumed that flag delete_data==true is set\n- appending the element increases ref-count, thats fine\n- getting an element from the list increases ref-count, that is fine\n- delete_element should decrease the ref-count by one. This is a bug.\nI just looked into CList.h and found the problem:\nThis first thing in delete_element is to CSGObject* data = get_current_element(); Then, if the delete_data is set, SG_UNREF is called on the element. However, the getter also increased the current element's ref-count. So there should be an additional SG_UNREF if data was not NULL. Or simply replace the first line in the method by CSGObject* dat =current ?  current->data : NULL; See my point?\nThis fix here is not necessary. Should rather be fixed in CList. Since the bug only appears if delete_data==true, which is not the std behaviour of CList, it should be ok to change - I don't think anyone uses CList with the flag different than default. It is not unit tested at all. But easily fixed...\n. Should be fixed in #2160 Closing\n. I agree. But let's maybe wait until the other thing is fixed\n. I agree. But let's maybe wait until the other thing is fixed\n. seems to be fixed now, @tklein23 ?\n. seems to be fixed now, @tklein23 ?\n. Thanks for making Shogun a better place guys :)\n. Thanks for making Shogun a better place guys :)\n. @lambday travis failing here is a bug in the MMC classes. FYI. Will try to fix it\n. @tklein23 added the tests. Thanks for the push :)\n@lambday investigating the other problems. CList now behaves as expected, but this makes quite a few MMD examples/tests segfault (I suspect NULL pointer exceptions, double frees, I am on it)\n. @lambday I currently cannot find the problem, could you have a look?. It has to do with the lists passed around in the MMD classes. The kernel::init call in compute_squared_MMD of CLinearTimeMMD fails because kernel rhs is not NULL, but the pointer is outdated so SEGFAULT.\nTo reproduce, just change the line of CList as in this PR. Once that is fixed, I can merge the unit tests in here.\n. If running with valgrind (which avoids the calls on invalid pointers), everything works\n. Hey @lambday Among some clean ups and the CList bugfix in the PR, I disabled all examples/unit tests using the linear time MMD for now to get this merged (dangerours this CList bug). Fixing should be possible with removing some SG_UNREF calls, but I could not find it today and now out of time.\n. cool, let's just wait for travis to give green\n. cool, let's just wait for travis to give green\n. @lambday cleaned up the commits a bit for you to more easily revert things once the problems are fixed\n. @lambday cleaned up the commits a bit for you to more easily revert things once the problems are fixed\n. @lambday I will close this PR for now, you have to reproduce the fix on your own. There are some other cases where we get segfaults from \"the fix\"\n. @lambday I will close this PR for now, you have to reproduce the fix on your own. There are some other cases where we get segfaults from \"the fix\"\n. Any updates here?\n. @lambday Oh thats bad to hear. Get well soon!\n. @lambday ok lets wait for travis here\n. @lambday ok lets wait for travis here\n. @lambday thanks for the hint.\n. @lambday thanks for the hint.\n. ok cleaned up PR, lets see\n. ok cleaned up PR, lets see\n. Ah man, I ended up doing some more tests, lots of traps here. But now it locally works- and even added some more proper cases. Let's see what travis says.\n. Ah man, I ended up doing some more tests, lots of traps here. But now it locally works- and even added some more proper cases. Let's see what travis says.\n. @pl8787 this custom kernel unit test is yours.\n@yorkerlin if this keeps on happening, just disable the unit test for now (put \"DISABLE_\" prefix in front of its name)\n. @pl8787 this custom kernel unit test is yours.\n@yorkerlin if this keeps on happening, just disable the unit test for now (put \"DISABLE_\" prefix in front of its name)\n. @pl8787 has fixed the unit test. If you rebase against latest develop, travis should run\n. @pl8787 has fixed the unit test. If you rebase against latest develop, travis should run\n. ok merging, thanks!\n. ok merging, thanks!\n. Maybe change the thing to an SGVector and register this one? I am sure that then the pointer will not be the same - this is happening at lots of other places.\n. Maybe change the thing to an SGVector and register this one? I am sure that then the pointer will not be the same - this is happening at lots of other places.\n. I see,\nadd_vector should work. I think maybe the best way is to test it properly to reproduce the problem in  a small test. ?\n. In principle there should be no differences, but JSON is extremely messy (our implementation)\nI think the best would be to have some unit tests that test for low-level behaviour of all serialisation classes to make sure all of them do the same, and if not, fix them.\n. Also mention the SWIG was of marking newly created objects\n. Thanks.\nPls make sure that all committed unit tests run and are memory clean according to valgrind (is this one valgrind checked?)\n. Thanks.\nPls make sure that all committed unit tests run and are memory clean according to valgrind (is this one valgrind checked?)\n. Thanks guys!\n. Thanks guys!\n. safe to merge before travis says green!\n. safe to merge before travis says green!\n. Nice thanks! See the comments.\nA python example would be nice, just do the very same thing, just illustrate API usage. \n. Ok data is merged. Now you need to commit the data version of this PR and return the subclasses kernel matrix in the python example (and address the minor comments) then we can wait for travis and then merge\n. Well done, thanks! :)\n. Your patches are really impressive @lambday \nI have nothing to complain. I guess all this is valgrind clean :) Will wait for travis to be sure and then this is ready to merge\n. Just realising something: The main gain here is parallel computing.\n- I would be interested in the overhead of doing these ::kernel calls versus directly accessing the matrix in CCustomKernel. I dont think there will be a large difference, but it would be good to quantify this (for large matrices). If the overhead is significant, these methods should be overloaded for the custom kernel. Although compiler optimisations should really take care of this.\n. Actually just read a bit on StackOverflow on this. Doesnt seem to make a difference\n. @lambday very nice job documenting this. So then we should stick with what you already did. Great work! I suggest to put this into benchmark and add a note in the documentation of custom kernel\n. Compiler optimisers are quite good, almost no cache misses.\nBTW what happens if you make Shogun only use one CPU?\n. thanks!\n. really nice stuff.\nWhat about unit testing the dropout stuff a bit?\nWhat about parallelising things? (Apart from basic linear algebra operations?) Thoughts on this?\nThe dropout also should be explained/compared in the notebook (please in general send separate PRs for notebooks)\n. Please put (scientific) references on the class headers, and maybe also links to files where this is explained. Could you also put the outputs in the rendered notebook (so that we can see them, not in the commit though)\n. What about visualising a network with graphviz in the notebook? :)\n. fine to merge from my side, just see the minor comments\n. For dropout effects to kick in we need large data-set, I agree. But for a small dataset and a fixed seed, there is a defined input/output behaviour that we should assert to be true. Just as a basic check that dropout is doing (on a low level implementation point of view, not statistically) what it should.\nI did not read the notebook before I asked you to write about this. Looks great btw the notebook.\nI thought it just might be nice to have a visualisation for people who never saw neural networks before. You know, just to look cool. But you decide!\n. @lisitsyn you should merge this (after reviewing). I am fine with it!\n@khalednasr nice work!\n. thanks! :)\n. Cool, looking forward!\n. Cool, looking forward!\n. well done @lambday \n. well done @lambday \n. @lambday about my patch, I will close the PR (too much unnecessary stuff). Could you just \"steal\" the unit tests and other things?\nI think if you check this out, reverse the commit that comments out all examples, and then rebase, it should work. Sorry for asking this, currently short on time.\n. @lambday about my patch, I will close the PR (too much unnecessary stuff). Could you just \"steal\" the unit tests and other things?\nI think if you check this out, reverse the commit that comments out all examples, and then rebase, it should work. Sorry for asking this, currently short on time.\n. actually, let me just remove the thing myself and rebase .... easier\n. actually, let me just remove the thing myself and rebase .... easier\n. It is not supported in the parameter framework (serialisation, clone, equals, registering (?) ) And typemaps, yes. Though not sure whether all target languages have the NDArray.\n. does this only surpress if the version is smaller?\n. does this only surpress if the version is smaller?\n. OK!\n. I guess the solution is that objects should never share the reference counting object, right?\n. @tklein23 why? dont get it\n@lisitsyn what is this supposed to do?\n. @tklein23  \"But then we should also forbit that  they share non-refcounted data.\"\n. I see, yeah its not a good idea to share references on non-refcounted data, that should be avoided. But how is that possible?\n. Yes, I agree. This is bad. That is why we should not use this, but the SG* datatypes which handle everything properly, as well as SGObject.\n. I see, mmmh, but forbidding copy constructors is not really what we want.\nThough we could do it, its really a convenience thing.  I dont know. Better would be to keep it and make it do shallow copies where pointers are shared (and handled internally, so no T* m_member members)\n. I could not agree more here! Every object should count its own references. This should solve the problem more or less right?\nTests that depend on shared reference counting objects are \"wrong\" anyways.\n. Cool!\nI feel some automagic unit tests for copy constructors could help later. But yes, problem can be dealt with separately.\n. Gotta leave now, will review later\n. Few suggestions:\n- another GP example where Shogun's cross-validation class is used (not sure if this work sthough ther eis entrance task about this)\n- a python example for this custom kernel training (and maybe also the cross-validation)\nSee my comments, most are minor, separating the training and test data is very important.\nNice work! What does valgrind say? \n. Ok valgrind needs to be clean. Can you investigate that a bit?\nActually the next step  should really be a unit test that ensure that the results of the custom kernel stuff are exactly the same as using a non-precomputed kernel.\n. possibly list > 0 is not acceptable, sorry. There is something fishy going on in the code.\nLet me know if you need help finding this.\n. possibly list > 0 is not acceptable, sorry. There is something fishy going on in the code.\nLet me know if you need help finding this.\n. Any updates?\n. Any updates?\n. I guess thats the parallel code. But I will have to investigate a bit.\nMaybe @votjakovr can also help here?\n. Dont worry about parallel for now, but maybe set the number of CPUs to use to 1 using sg_parallel.\nWhat you can do here is to compile Shogun with enabling tracing the memory allocations, this prints all allocated memory blocks at the end and where they were allocated. This could give some hints to where the leak is generated. valgrind also has lots of options to find that out.\n. any updates here?\n. any updates here?\n. closing as inactive\n. nice!\n. any updates here?\n. Thanks for the fix, pls remove debug and then I will merge\n. Thanks for the fix, pls remove debug and then I will merge\n. See travis, an integration test fails: python_modular-structure_plif_hmsvm_bmrm \n. If thats the class you changed, you have to run the generator.py on that example and commit to shogun-data, then update the data version in the main repository. Ask on IRC everyone know this\n. Thanks for keeping on this, I know its a bit annoying. But very useful!\n. You can run the tester on one particular example only via passing the name as command line argument.\nIn any case, commit only the one you really want to update, then things should work. Don't worry about the unrelated fails for now, just keep an eye on travis when you send the PR to make sure things work as expected\n. You can run the tester on one particular example only via passing the name as command line argument.\nIn any case, commit only the one you really want to update, then things should work. Don't worry about the unrelated fails for now, just keep an eye on travis when you send the PR to make sure things work as expected\n. This seems to work, can we merge?\n. Fine to merge apart from this unit test issue. Nice work!\n. Yeah, this way of testing is exactly what I meant! Nice work!\nJust restarting travis and then its ready to merge\n. yeah do plus and minus epsilon\n. Yep, OSX is the only problem, merging\n. btw you can stop/restart travis jobs\n. any updates here?\n. Travis is still not green again. This is the last patch before it started failing.\nI am currently very busy and dont have the overview whats  happening, but could anyone please fix it? Thanks :)\n. Any updates?\n. If you dont agree on this, we can also postpont (which means we wont ever do it ;) )\n. In fact, I think this is an operation that is defined on matrices. Quite low-level.\nMaybe the norm of an operator? But that is not yet needed really. Just something that computes sums of matrix like memory blocks. I think this is something that is quite orthogonal from sunils work so should be safe to do.\n. I think these are nice ideas!\nI will merge this now to allow you to finish other things, but let's not postpone this for too long. Things such as LinearAlgebra should always be done on the fly when needed, otherwise they never happen ;)\n. Great, I agree!\n. Nice! Thanks for these updates!\nSo, as this is a really big patch with lots of changes, have you double checked that\n- changes are all covered by unit tests\n- code is memory clean according to valgrind? (lots of memory changes here)\n. apart from the minor comments (whitespace, references on SGVectors) and the to-be-made-sure valgrind check, fine to merge from my side\n. @lisitsyn feel free to merge then.\n@khalednasr This is quite some stuff. I just checked the unit tests. Could you\n- separate the first test a bit? It is huge, and there are many tests in there, which should be separate test cases\n- the other tests are checking that classification results are correct. This is important. However, some tests of all the helper methods in your code might also be nice. This will be in particular useful when hunting for bugs later on. Its quite hard to find why the results (say your tests fail) are wrong if not all or most atomic helpers are tested separately.\n- finally, it is always good to have some tests for constructors/getters/setters\n. Ill be brave and merge before Sergey's ok so that you can go on :)\n. Thanks, lets try\n. Can this be merged? I guess so, travis seems okish\n. Nice, thanks!\nFor any notebook patch you send\n- make sure no output is included, clear all result cells (I think you already took care of that)\n- put a nbviewer link of the notebook with output, for us to review easily.\n. Could you post the notebook link?\n. Could you post the notebook link?\n. great!\nI think it will be good to spend some time with @lambday thinking about linear algebra that is needed.\nPls make sure to send the PRs separately, and always include the nbviewer link for any notebook patch\n. For the dataset, it is fine to use raw data. However, we do not want to have the same dataset twice in shogun, so either you remove the other one and change the codes that use it, or you use the other one directly. Either way I am fine.\nI close this PR for now, as we want this stuff to be in the linagl framework. Thats to discuss with @lambday Basically, just make a list of operations you need, and we will add them to the interface. This also means Shoguns GPs will soon be GPU accelerated! :)\n. Code looks fine, structure also, tests seem to make sense. But I have no wider understanding of whats going on, so pls somebody else also review and merge :)\n. Code looks fine, structure also, tests seem to make sense. But I have no wider understanding of whats going on, so pls somebody else also review and merge :)\n. python modular fails, Ill restart it since doesnt seem related\n. python modular fails, Ill restart it since doesnt seem related\n. Ill just merge, @lisitsyn has long delays these days ;)\nWhat about a few python modular API usage examples for all those methods?\nAre they all in the notebook yet? Thats another nice (visual) way of verifying that everything works correctly\n. cool! python examples are supposed to be just API usage illustration\n. To answer your questions:\n- SGVector/SGMatrix can be passed around copy by value. The objects share the same memory and do count references automatically.\n- did you encounter problems with the LU determinant? We had a discussion on this with @votjakovr maybe he can comment why this is done?\n- Math.h cannot include eigen3 headers. We want to move towards a Shogun internal linear algebra interface, see #1973 feel free to participate in the discussion. I have the feeling you could add valuable comments there\n- It is fine to pass eigen3 objects around, as long as these methods are not exposed to the outside world. I.e. private/protected helper methods.\n. The matrix operations class is a great idea. We are working on this. It should be a bit more general than your ideas here - but you are totally right with pulling things like log-determinats out.\nPls discuss with @lambday in #1973 \n@lambday it would be great if you could also think about adding the things that @yorkerlin needs for the GPs. We then cover many many things at once.\n@lambday @yorkerlin this is a great example of synergy effects of GSoC and is perfect for the pre-GSoC time. Having those problems solved in a general way will massively benefit the rest of Shogun\n. @yorkerlin \n- travis had problems, now fixed\n- let's just start with putting a few very fundamental operations into the linalg framework. Like the triangular solve, factorisation, etc. Complicated linear algebra operations can stay in eigen3 for now. GPs are strongly coupled with eigen anyways, so thats fine.\n@lambday could you push this hard this week? Then @yorkerlin can use at least the Cholesky solver and log-determinants.\nIts looking good guys! :)\n. That sounds good to me, but out of my expertise. I guess @vigsterkr has a comment on this too\n. @yorkerlin no pls do not add stuff to the matrix operations class. This class might be used for very GP specific operations (which I dont think exist). However, methods like the ones you mentioned are supposed to go to linear algebra framework.\n. @yorkerlin thats sounds good. You do your GP-specific transformations inside the helper class, and then call the linalg framework from within once you have reduced your tasks to standard problems.\nBTW have a look into the documentation of googletest how to select certain tests\n. any updates here?\n. any updates here?\n. i think it should be the same, a user doesnt care, he just wants to use one class.\nHowever, in the class itself, you should structure things properly\n. @yorkerlin I agree with you, things are different under the hood. However, a user should have the possibility to just say \"Laplace\" and then the corresponding Laplace method is used internally. I think this can be solved via introducing a wrapper class CLaplacianInference that checks the likelihood and then instanciates the corresponding inference method object internally. Then we could even hide the other classes from the modular interfaces and things might be cleaner. This is in particular interesting for users who are not familiar with too many details about these things.\n. Yeah the Girolami thing would be neat to have. He is my former supervisor and we in fact already talked about having this in Shogun. Though @emtiyaz had some not so promising results with this I believe\n. we should focus on the writeup now\n@yorkerlin the notebook really has to be improved - we want nice intuition, some text, cool examples, pictures - this should be possible to understand for people who have no idea about variational methods .... i will give some more feedback soon\ncould you please send a pull request with the notebook only?\n. August 11, but the last week is reserved for other things. We want to finish implementing/writing things within the next days\n. @yorkerlin whats the state of this one?\n. @lambday I second your thoughts on generality of linalg actually. However, it would still be cool to have expensive and simple operations in lingalg, like Cholesky, linear solve, etc. These are also used everywhere in Shogun, so we get a lot from generalising them.\n@yorkerlin could you address the points that @lambday mentioned, I think they are really good\n. @yorkerlin @lambday first step: Cholesky factorisation and linear solves. Maybe matrix matrix product, but only if the same matrix has to be multiplied many many times (otherwise makes no sense to use GPU)\n. ok!\n. Could you clean up this PR? It overlaps with the other one...\n. ok!\n. yep!\n. Ok, before I dive into this code, I have a few software engineering questions:\n- why is this a subclass of likelihood? You do not implements its interface\n- rather you add lots of new methods for variational inference, those should be pulled out to a base class and then this one specialises it\nBoth of those points are fundamental issues here. Class diagrams might help to understand such things. Let me know your thoughts\n. Please try to structure your responses more. This is very hard to read you know? We keep on telling you this (which is a bit annoying). Please just take some care when answering. There is the markdown in github and you should also always separate topics to multiple posts and not just dump all of your thoughts into on big chunk of text.\n. Base class:\n- So as I currently see it, this functionality is not yet existing, which justifies a new base class.\n- Where to put this in the class hierarchy? If it is a sub-class of  CLikelihoodModel it should implement its interface. If it is not, we can think about adding a new base class. But where in the class hierarchy should it go?\n- My feeling is that this is a helper class to compute variational expectations, which will be used by all variational inference classes.\n- Continuing this thought, what about putting this new class as a member in the variational inference base class?\n- VariationalExpectation as name?\n. We don't do multiple inheritance in Shogun. There are also no Java-like interfaces (unfortunately).\nCould I suggest to just send header files of base class interfaces (with empty implementation files) for now to get the interface clear without having to look at any code? Once the interfaces are clear, you can send in the method implementations and unit tests.\n. Hi\nThanks for the very nice answer! This is so much easier to understand :) (Sorry about my delayed reply!)\n- The reason I tend to solution 1 is that it keeps the variational methods and the likelihood separated. CLikelihoodModel subclasses should just deal with computing likelihoods and their derivatives (as the base class interfaces suggest). If we put another interface in there, it should be one that is implemented by all likelihood methods that inherit from it. But the existing std likelihoods have no reason to implement this.\n- One solution would be to  have Java-like Interfaces, where CLikelihoodModel implements multiple interfaces. But since we do not have multiple inheritance in Shogun, I prefer to have a separate base class that is added as a member to the variational inference base class. This way, everything is cleanly separated without any overhead.\n- A third option would be to have an abstract CLikelihoodModel subclass, say CVariationalLikelihood. This one then can implement the variational expectations. I think that could be nice. However it means that a new likelihood class would have to be implemented if one wants to use existing likelihoods.\n- Do you think that all variational methods require to implement a new likelihood? If yes, then this third option is best. If no, so there could be variational methods which use an existing likelihood, then I think we should go for the second solution as its cleanest\nLet me know your thoughts on this.\n. @yorkerlin Again,we need a separate interface for\n- get_variational_first_derivative\n- set_variational_distribution\n- the expectation\nyou cannot just put those into a non-abstract of CLikelihoodModel because then it is not general.\nTherefore, you cannot inherit from CLikelihoodModel directly. You either have to put a class in-between (with methods that throw errors if not overloaded). I suggest CVariationalLikelihood.\n. @yorkerlin I agree on the design that you did now. That is quite a few classes, but is very clean and modular. So very nice. Glad we discussed this in detail!\nNext step is to clean up the code and properly document all interfaces and class descriptions. Give a high level overview what the classes are supposed to do in the class description.\nI will do a detailed review later today or tomorrow.\n. @yorkerlin I agree on the design that you did now. That is quite a few classes, but is very clean and modular. So very nice. Glad we discussed this in detail!\nNext step is to clean up the code and properly document all interfaces and class descriptions. Give a high level overview what the classes are supposed to do in the class description.\nI will do a detailed review later today or tomorrow.\n. @yorkerlin thanks! Nice work! There are some minor comments in some files.\nTo get this merged:\n- Please send another PR with small incremental changes.\n- Refactoring first\n- Then the new classes (one by one)\n- the matrix operations class seperately\n- keep in mind: max 2-3 files and not too many lines\n. The dynamic cast is fine. Just put a require somewhere\n. The bug you found belong to a separate issue, could you move your comments there? And yes, spend time fixing it.\n. @yorkerlin nice work! Let me know once this can be merged (there were some minor things)\n@votjakovr thanks for the discussion - you see, someone takes good care of your efforts! :)\n. Ill merge this for now. Good work!\nCould you take care of my comments in a next PR?\n. Ill merge this for now. Good work!\nCould you take care of my comments in a next PR?\n. @lambday just read this beast\n- I cannot judge the cmake code. @vigsterkr @besser82  please review this\n- I like how the thing is used in the unit-tests. Thats really nice and clean\n- I am fine with the fact that global backend changes require a recompile. And also that linalg is not available if none of the libs is there. In fact, we should just offer native implementations for thigns as dot-products, matrix multiplications, etc. But thats just technical stuff - it is suicide to work with a ML toolbox without the ability to install a linear algebra lib. Also we are currently hard-relying on eigen3 + lapack Half of Shogun doesnt work if those are not there. So I dont see a problem with that.\n@lambday as this is a feature branch, feel free to merge things on your own. Once @vigsterkr gets back, you can always change things.\n. @lambday I agree, not being dependent on a particular one is the best. This is the case now, so we will make a step forward. Plus the fact that we can gain speed! Let's try to get this ready before GSoC starts, so yes, I think you should press ahead and change things so that at least you can use it yourself (and @yorkerlin maybe)\n1.) Yes, lets try to gets  @wiking s comments on this\n2.) yes!, also do a naive implementation there\n3.) Very important, yes!\n4.) should be quick\n5.) yes, very nice, but not priority, so maybe we can this later\n. @lambday I agree, not being dependent on a particular one is the best. This is the case now, so we will make a step forward. Plus the fact that we can gain speed! Let's try to get this ready before GSoC starts, so yes, I think you should press ahead and change things so that at least you can use it yourself (and @yorkerlin maybe)\n1.) Yes, lets try to gets  @wiking s comments on this\n2.) yes!, also do a naive implementation there\n3.) Very important, yes!\n4.) should be quick\n5.) yes, very nice, but not priority, so maybe we can this later\n. And?\n. @mazumdarparijat BTW you should avoid such long PRs. They are hard to review, as @iglesias mentioned, and whats more important, its more likely that errors are not spotted. You can just send the header file with empty implementation first, and then add the methods one-by-one. This seems cumbersome at first but has the advantage that all methods are properly reviewed, as well as their unit tests. Just a suggestion. Nice one the C4.5 ! :)\n. please double check this with our cmake experts, @vigsterkr and @besser82 \n. I like that you added this!\nBut this code has to be worked on quite a bit. Also, there are no unit tests.\nI suggest you remove everything but the header class from the PR and discuss this one first with @iglesias . Then second, you add a few methods + unit tests (again, proper unit tests). Then, in a third PR, you can add examples etc.\nBut nice work!\n. I agree with things in this patch! :)\n. I agree with things in this patch! :)\n. Hey,\nThis linalg::sum(linalg::square(block)) is maybe not the best way since it allocates new memory right? A method for summing up squares is fine in my eyes.\nThese complex eigen3 one-liners, we do not need to minic everything. Its not that all Shogun operations have to be done via the linalg interface, just the very essential ones. So for you, maybe a method for sum_colwise and sunm_rowwise might do the job?\n. About the block stuff, that is all nice! But we should always avoid copying. Or do I miss something here?\n. I am confused here. Do these block sum methods actually copy the memory around before summing?\n. Nice!\n. Nice! I am very excited that this works now.\n. Thanks!\n. Hey!\nThis patch is way too big. Please separate things and send patches with max 2-3 files changed and max 100 lines of code changed. Otherwise errors will slip through as its super hard to review this properly. Rather send 5 PRs which are smaller.\nBut good work!\n. Yeah I see. Still, I think this could have been separated a bit. Its fine for this one, just keep this in mind when you write patches, helps us a lot :)\n. ah. So nice and much cleaner!\n. Dont worry, travis caught the problem.\n. building opencv here is not good, takes too much travis time. We could to this on the buildbot later. As @pickle27 said, install from package manager\n. - Please put the notebook under the template we have (with your name etc)\n- Always use capital letters for Shogun\n- \"data and its attributes\" --  should be \"their\"\n- \"Machine learning implies we want to to make and improve predictions or behaviors based on some data.\". A major part of Machine Learning is exploit structure in existing data. That is to predict certain attributes on yet unseen data from the same or a similar generative process.\n- Try to be a bit more precise in your language. Don't use words as \"some\", \"we want to make\".\n- more later, got a meeting now\n. Very nice with the last example. Sorry I have been offline during the weekend, but will soon take a more detailed look. One thing you could do is to play a bit with the CFeatures interface to make people aware of dimensions and number of vectors (col-major format as opposed to say numpy)\n. Yeah like just demonstrating how to access features, feature vectors, feature matrix, and play with dimensions etc\n. - Introduction: You say the prediction can be of different types, supervised and unsupervised, but in unsupervised, things are not about prediction but rather to detect structure, reduce dimension, or find a good description for the data. Could you change that?\n. Always write Shogun with a capital S\n. \"To get off the mark, let us see how shogun handles the attributes of the data using CFeatures class. A feature object handles the attributes or individual measurable property of the data being observed.\" should be moved to feature representation paragraph, \n. Ask yourself what you want to say in the intro - it is a little unclear. I think you want to introduce ML - so why not dig out some quote from some book about ML?\n. Feature representation:\nSay that in Shogun, we believe it is a good idea to have different forms of data, rather than converting them all into matrices. For example for algorithms working on data streams (too large to fit into memory) or string data (explicit representation has a dimension which is too large)\n. \"The Data\" - data is written with a small d\n. \" learning problem.The Data reveals\" please watch your whitespaces, there is always a space after a \".\"\n. \"\nLet us consider an example, instead of just abstract concepts. We have a dataset about various attributes of individuals and we know whether or not they are diabetic. Now this is a classic machine learning problem.The Data reveals certain configurations of attributes that correspond to diabetic patients and others that correspond to non-diabetic patients. When given a set of attributes for a new patient, the goal is to predict whether the patient is diabetic or not. This type of learning problem falls under Supervised learning.\"\nThis paragraph does not belong where it is. You generate random data, so for now, you should not talk about interpretations, just say you generate some random points to demonstrate the features interface\n. #To import all shogun classes\nfrom modshogun import *\nshould be at the beginning of the notebook, not where it is now\n. \"#Generate some random data\nX = 2 * random.randn(20,2)\ntraindata=r_[X + 5, X + 9].T\nprint('Shape of data: (%s, %s)' %(traindata.shape[0], traindata.shape[1]))\"\nThis is weird. You can do\nprint(\"Shape of data: \", traindata.shape)\nand also what is this r_ thing doing?\n. \"Let's call this the feature matrix\"\nRemove.\nRather say: In numpy, this is a matrix of 2 row-vectors of dimension 40. However, in Shogun, this will be a matrix of 40 column vectors of dimension 2.\n. \" atribute like BMI, Glucose concentration etc.\"\nSince your data doesnt mean anything, you should not talk about attributes. Why not remove the random data and directly work with your BMI dataset? Just introduce it before - I think that would be better\n. Number of attributes: 2 and number of samples:40\nNumber of rows of feature matrix:2 and number of columns:40\nplease do consistent whitespaces\n. To be able to differentiate between data one has to label them.\nRather: In supervised learning problems, training data is labelled.\n. Again, you have not yet introduced the dataset in the labels section. I suggest you introduce it in the beginning and dont use the random dataset\n. Mention the other label types briefly and what they can represent\n. Also show the interface of the CLAbels class. get_num_labels, get_labels, etc\n. It is usually better to preprocess data to a standard form rather than handling it in raw form\nYou should say why. The reasons are well behaved-scalling, many algorithms assume centered data, and that sometimes one wants to de-noise data (with say PCA)\n. I think for each of the base classes you introduce (like here preprocessor), you should have a little list of the most important implementations of those classes, along with a brief comment on what they do.\n- Preprocessor: mean removing, PCA, unit-scalling\n- Labels: Binary, Regression\n- Features: ...\n. Out: True \nshould be avoided\n. For all plots:\naxis labels\nlegend\n. For the preprocessing, plot the data before and after to illustrate how the mean/scalling is changed\n. \"Basically one has to train the machine\" Please be consistent with your typewriter font. You did not use italic fonts for method names before\n. CMachine is Shogun's interface for general learning machines. Take a few lines to describe its interface in a structured way: Mention the important method names (as you already do) and what they do, maybe in a little bullet point list\n. Quickly mention what an SVM does: Finding a linear seperator with the largest possible margin \n. in the svm plot, you use get_values but you did not introduce that before. Please write about it in the Labels section.\n. \"surely close to what our human machine reasoned!\"\nremove that\n. Ah I see you introduce datasets later.\nIn this case, just dont talk about attributes before (on the random data) just say we generate some toy data and later on we will use something which actually means something\n. example:Pima\nwhitespace\n. Quickly introduce the LibSVMFile format\n. SparseRealFeatures\nyou did not introduce those before\n. mat=f.get_full_feature_matrix()\nmention what this method does\n. for the SVMs, mention what the parameters do, just briefly\n. In the SVM plots, could you highlight the decision boundary a bit? Make it thicker\n. This seems like a decent enough prediction. We can thus infer that individuals below a ceratin level of BMI and glucose are most certainly safe. To have more strict boundaries explore many more of shogun's classifiers including Kernel machines, etc.\nI dont like this paragraph, what about\nFor this problem, a linear classifier does a reasonable job in distinguishing labelled data. An interpretation could be that individuals below a certain level of BMI and glucose are likely to have no Diabetes. This can also be seen by looking at the weight vector of the separating hyperplane (print svm.get_w(), which tells you the linear relationship between the features).\nYou then talk about more strict boundaries, but this is not really what you want to say. You want to say: For problems. where the data cannot be separated linearly, there are more advanced classification method, as for example all of Shogun's kernel machines, but more on this later\n. Evaluating performance and Model selection\nYou should mention what the performance of a classifier is, and that we want to evaluate it on unseen data. In general, try to give introductions to the concepts you use\n. when you do the ROC accuracy, please use Accuracy as its more intuitive\n. \"es are : Weighted \" whitespace!\n. in the KRR example at the end, split the code for the individual regressions. Hard to read otherwise.\n. Please never do things like mat[7], just put this row into a seperate variable that has an easy to remember name\n. \"In this notebook we will see how a general machine learning problem is represented and solved in Shogun.\"\nMake it\n\"In this notebook, we will see machine learning problems are generally represented and solved in Shogun.\"\nEDIT by Fernando: Heiko probably meant \"we will see how\".\n. \"In this notebook we will see how a general machine learning problem is represented and solved in Shogun.\"\nMake it\n\"In this notebook, we will see machine learning problems are generally represented and solved in Shogun.\"\nEDIT by Fernando: Heiko probably meant \"we will see how\".\n. \"Machine learning concerns the construction and study of systems that can learn from data. A major part of Machine Learning is exploiting structure in existing data.\"\nmake it\n\"Machine learning concerns the construction and study of systems that can learn from data via  exploiting certain types of structure within these.\"\n. \"Machine learning concerns the construction and study of systems that can learn from data. A major part of Machine Learning is exploiting structure in existing data.\"\nmake it\n\"Machine learning concerns the construction and study of systems that can learn from data via  exploiting certain types of structure within these.\"\n. \"under uncertainty\"\nremove\n. \"Machine learning is usually divided into two main types: predictive or supervised learning and descriptive or Unsupervised learning.\"\nmake it\n\"Two main classes (among others) of Machine Learning algorithms are: predictive or Supervised learning and descriptive or Unsupervised learning.\"\n. \"Shogun provides a host of functionality to do these.\"\nmake it\n\"Shogun provides functionality to address those (and more) problem classes.\"\n. \"x to outputs y, given a labeled set of input-output pairs D=(xi,yi)Ni=1.\"\nSee my comments in the regression notebook, indicate where the elements come from, use bold notation for vectors, etc\n. I like your distinction between the supervised  and unsupervised a lot\n. \"The Data reveals certain \"\ndata with small d\n. \"This type of learning problem falls under Supervised learning.\"\n, in particular, classification\n. \"data_file=LibSVMFile('../../../data/toy/diabetes_scale.svm')\"\nYou should say what the resulting object here is, and that it will be used later to access the data\n. I would put the data section after the feature types\n. \"Shogun supports wide range of feature representations since we believe it is a good idea to have different forms of data, rather than converting them all into matrices..\"\nfew corrections: \"a\", two sentences, only one full stop at the end\n\"Shogun supports a wide range of feature representations. We believe it is a good idea to have different forms of data representations, rather than converting them all into matrices.\"\n. \", etc.\"\nThis is ugly, could you remove it :)\n. \"SpareRealFeatures (sparse features handling 64 bit float type data) are used to get the data from the file\"\nIf you put the data section after the feature one, you can blend this nicer. \"For this dataset, SpareRealFeatures.....\"\n. Please mention that get_full_feature_matrix returns a DenseFeatures object\n. Very nice the paragraph where you load the features\n. Add structured labels to the list of label types.\n. \"preproc=PruneVarSubMean()\"\nPlease pass a \"True\" in the constructor to make the class normalise the varaince of the variables as well. Also mention this. Its basically dividing every dimension through its std-deviation (This is also why we remove dimensions with constant values)\n. \"Note that the now processed data has zero mean.\"\nWhat about plotting the mean as a red line and put it also into the legend?\n. \"Prediction with machines\"\nThis is a bit fuzzy. What about\n\"Supervised Learning with Shogun's CMachine interface\"\n. Just checked http://www.shogun-toolbox.org/doc/en/current/classshogun_1_1CLibLinear.html\nClass description is almost empty. I will update this soon!\n. \"The whole XY grid is used as test data, i.e data to predict on.\"\nAdd a \"For visualising the classification boundary, the whole XY is used as test data, i.e. we predict the class on every point in the grid\"\n. Actually, since its a linear SVM, you could also directly plot the decision boundary since you can ask the SVM for its w-vector and the bias. Ask me if unclear\n. Please put a legend with the class label into the plot\n. \"This can also be seen by looking at the weight vector of the separating hyperplane (which tells you the linear relationship between the features)\"\nPlease do this in another cell (and plot the decision boundary that I mentioned above in there)\n. Really nice paragraph!\n. overfitting - link to wikipedia :)\n.  \"This is called overfitting. In this process of overfitting, the performance on the training examples still increases while the performance on unseen data becomes worse.\"\nmake it\n\" This is called overfitting. Maximising performance on the training examples usually results in algorithms explaining the noise in data (rather than actual patterns), which leads to bad performance on unseen data.\"\n. feats_t=feats[0:700]\nPlease put the numbers into a variable.\nAlso note that in python, you can do feats[n:] which selects all points from index n to the end\n. \"\nTo evaluate more efficiently cross-validation is used. As you might have wondered how are the parameters of the classifier selected? Shogun has a model selection framework to select the best parameters. More description of these things in this notebook.\"\nNIce! :)\n. \"The task is to estimate price of houses\"\nprices - plural\n. \" by StatLib library\"\nby the StatLib library. Link please\n. \"% lower status\"\nwrite percentage rather than using a symbol\n. \" This type of problems are solved using Regression analysis.\"\nIn one or two sentences, state what you will be doing in the code below\n. \"Again we train on the data and apply on the XY grid to get predicitions.\"\nOne sentence on kernel ridge regression.\n\"KRR is a non-parametric version of ridge regression where the kernel trick (wiki link) is used to solve a related linear ridge regression problem in a higher-dimensional space, whose results correspond to non-linear regression in the data-space.\"\n. Again, roughly say what you are doing in the code\n. \"Thus we have a relationship between the attributes we wanted. \"\nThe \"thus\" doesnt make sense. What about \"The out variable now contains.......\"\n. \"\"\nplease surpress these\n. OK!\nOnce these comments are addresses, this is ready for merge ! Ping me once you have updated.\n. The line is fine, I also suggest to put this into the heatmap plot of the SVM, just to point it out a bit more\n. Did you address all the other comments? Then we can merge this.\n. x2=[]\nfor i in range(len(x1)):\n    x2_= -( ( (w[0])*x1[i] + b )/w[1] )\n    x2.append(x2_)\nCould you do this a bite more elegant please? Without a loop? :) Pythonian pls\n. Ok please do and let me know when done. This is ready to be merged.\n. (as soon as the last comment was addressed)\n. Then let's merge! :) :)\nPlease announce any merged notebooks to the mailing list and ask for feedback!\n. closing since seems to be fixed now. Correct me if wrong\n. Nice and clean patch!. Thanks!\n. Very nice catch! Good work\n. Travis fails, restarting....\n. Can be merged since only ruby failed which seems unrelated\n. haha :)\nAny updates here?\n. I think this can be merged. Very nice work!\nSee me minor comments. Looking forward to play with this and compare things with @sejdino\n. You have to rebase this one\n. My comments are only minor things and doc updates, except for the forgotten SG_REF which has to be fixed before merge.\nBut as said, you will have to rebase against develop I currently cannot merge\n. Nice work!\nThis was easier to read - way easier\n. This looks good!\n. good by me!\n. I wonder how much faster this actually is\nBut cannot hurt as you said.\n. Completely not related to your work here, but it really annoys me that we have to add all those by hand. We cannot test this properly, it generates license issues etc etc. \n@iglesias @wiking @lisitsyn  @sonney2k @lambday What about having boost as an optional dependence for such things? At least their stats module? They do all this properly and we dont need to worry. Same for log-pdf of all common distributions, same for random number generators (those that are not in c++ and/or Shogun)\n. Just had a quick look and like where this is going a lot. \nI will give you some more detailed feedback on the weekend - deadline stress currently.\n. link the methods in the header to wikipedia\n. Regression analysis is mainly used to estimate relationship and dependence between variables.\nRegression analysis is the continuous case of supervised learning, i.e. for a given set of training pairs {(x_i,y_i)} \\in \\cal{X}\\times \\mathbb{R}, the goal is to come up with a function f:\\cal{X}\\rightarrow \\mathbb{R} that predicts the response for a yet unseen example x\\in\\cal{X}\nCarefule with the word dependence, this has nothing to do with regression\n. This finds applications in prediction and hence in machine Learning. \nThis doesnt make sense, why hence in machine learning?\nRather give an example of what regression is used for\n. The goal is to determine the values of parameters for a function that cause the function to best fit a set of data observations that you provide.\nThis is a bit unprecise. For clearly defined problems such as regression/classification, please exactly state the problem (See above) rather than using such sentences. Also avoid the word \"you\"\n. Mathematical formulation: clearly state the space that all variables come from\n. \" This method minimizes the sum of squared vertical distances between the observed responses\"\nwhy vertical? It is just the squared difference between response and linear fit\n. You did not define N\n. Here \u03c4 is some suitably chosen Tikhonov matrix. In many cases, this matrix is chosen as a multiple of the identity matrix \u03c4 I\nThis is nonsense (sorry to say that). tau is just a number in the way you write it. Please give some intuition what it does: Large values penalise complex linear functions with lots of large non-zero elements\n. I would move the LARS paragraph to a separate section later. It is a bit out of context\n. The way to introduce LARS btw is to first state the LASSO target, saying that this is just L1 norm ridge regression, and then afterwards mention the way it works (where did you get the text from?) \nWhen describing algorithms, try to be more precise\n. its not called residues, but residuals :)\n. I like the residual plots! But put on an axis description\n. Also please make the plots have the same axis scalling\n. for ridge regression. Why do you sample from higher dimensions? Explain that overfitting is more likely to occur in such\n. I like the ridge regression curve and your explanation. But clean the text up in terms of whitespaces.\n. Could you re-structure the lars one a bit: \n- first introduce the L1 problem\n- then say: one way to solve this is LARS, the idea is to\n- then give an example where one wants to do L1 rather than L2 (least squares) and gets an advantage.\n. Please update the notebook with the outputs so that I can review properly :)\n. \"his notebook demonstrates various regression methods provided in shogun.\" \nShogun capital, regression should have a wikipedia link\n. As well as the other methods mentioned in the notebook abstract\n. \"like Kernel Ridge regression are discussed along with real life examples.\"\nmake it\n\"are discussed and applied to toy and real-life data\"\n. \"earn a mapping from inputs x to outputs y, given a labeled set of input-output pairs D=(xi,yi)Ni=1.\"\nmake it\nfrom inputs $x_i\\in\\mathcal{X}$ to outputs $y_i \\in \\mathcal{Y}$ thats the standard for this kind of notation\nalso do a $\\subseteq  \\mathcal{X} \\times \\mathcal{Y}$ after the set of x_i,y_i pairs\n. Make the dataset also $\\mathcal{D}$\n. \"Regression involves estimating or predicting a response which finds applications in many fields like for predicting stock prices or predicting consumption spending, etc.\"\nYou already said that regression is estimating the repsosne, so just make it\n\"Regression finds applications in many fields like for predicting stock prices or predicting consumption spending, etc.\"\n. \"A simple regression model can be defined as\"\nmake it\n\"A linear regression model can is defined as\"\n. \"y= w.x +b\"\nplease be consistent with you boldface and italic. All vectors (x) should be bold, so you have to update the previous part here.\n. \"y= w.x +b\"\nalso say what b is (b is a bias term which allows the linear function to be offset from the origin of the used coordinate system) or similar\n. \"We need best fit a line to this data. \"\nmake it\n\"We aim to find the linear function (line) that best explains the data, i.e. that minimises some measure of deviation to the training data $\\mathcal{D}$. On such measure is the sum of squared distances......\n. \"If the coefficients are unconstrained, they are susceptible to high variance.\"\nmake it\n\"As we will see later, if the coefficients are unconstrained, they are susceptible to high variance and over-fitting.\"\n. \"This is what is done in Ridge regression which is L2 regularized form of least squares.\"\nminor correction:\n\"This is what is done in Ridge regression, which is a L2 (sum of squared components of w) regularized form of least squares.\"\n. \"The following system has to be minimized:\n12(\u2211i=1N(yi\u2212w\u22c5xi)2+\u03c4||w||2)\nHere \u03c4 imposes a penalty on the weights.\"\nThis is not a system. But an objective function. State the minimisation problem:\n$\\min_w [objective]\nand write \"which consists of the original objective function plus the L2 norm of the weight vector\"\n. In the intro, you might also well explain the idea of mapping the data through a non-linearity $\\phi(x)$ and then do regression, and then mention that in the objective is just defined in terms of inner products of the $\\phi(x)^T \\phi(x)$, which is why the kernel trick can be used. Ask me if unclear. But this should really be covered here. In fact, I suggest to update the paragraph a bit:\n- 1.) Define the sum of squared distance objective. Then say that one can differentiate wrt to w, set to zero  and solve the resulting linear system which can be done in closed form. See for example David barbers \"Bayesian reasoning and machine learning\" book for details on this.\n- 2.) then add the ridge term, just state the closed form solution (which is very similar)\n- 3.) then add the feature mapping and the kernel trick.\nThese three points should be in separate paragraphs\nEDIT: I think we should keep code and maths together. So introduce least squares, then demonstrate with code. Then introduce ridge regression, then code, then L1 regression and code. In the beginning, you should have a little summary saying that you introduce methods and then demonstrate with code\n. \"This value of w is pretty close to 2,\"\nInconsistent. You talked about 3 before\n. \"The sum of the squares of these residuals is minimised.\"\nmake it\n\"As we can verify visually, the sum of the squares of these residuals is minimised.\"\n. Actually, I just thou\n.  \"The value of tau for which these coefficients are at their stable values is often the best one.\"\nThis calls for a demonstration using cross-validation (see the study assignment I sent you). Maybe we can link against a notebook on cross-validation and overfitting where you take up on this. Let me know what you think!\n. \"LASSO (Least Absolute Shrinkage and Selection Operator) is a constrained version of Least Squares regression, which uses the constraint that \u2225\u03b2\u22251, the L1-norm of the parameter vector, is no greater than a given value.\"\nMake it\nLASSO (Least Absolute Shrinkage and Selection Operator) is another version of Least Squares regression, which uses a L1-norm of the parameter vector. This intuitively enforces sparse solutions, whereas L2-norm penalties usually result in smooth and dense solutions.\n. Another general idea: You should have a section where you run all linear regression methods described here on the same dataset and compare the solutions of the w vectors\n. Another general idea: Support vector regression (but that can also go into the SVM notebook)\n. \"One way to solve this regularized form is by using Least Angle Regression(LARS).\"\nwhitespace before (LARS) and the formatting here is a bit messed up with newlines etc\n. \"LARS is essentially forward stagewise made fast. LARS can be briefly described as follows.\"\nBe more careful with language here, you repeat LARS. Make it \"It can be briefly described as\" ...\n. \"First step is starting with an empty set.\" Make this an enumeration rather than a bullet list and then dont write \"First step\", \"Second step\", etc\n. Define \"equiangular\"\n. I think the notebook is ready to merge after you addressed all those comments. Very nice work!\n. ah sorry, laptop keyboard\n. Ok, I agree on x-validation, but verify on test set!\nX-validation notebook will come later!\nLooking forward to your updates, this is ready soon\n. @Saurabh7 I removed the warning from the code, should be gone\n. \"Kernel Ridge regression\" should have a wikipedia link in the abstract\n. How long does the cross-validation take?\n. \" compute the MSE as :\" now hitespace before the colon\n. I like the KRR description.\nThis is ready to merge!\n. I will merge and then see whether @vigsterkr complains about runtime. Nice job! \nPlease send updates to the few things I commented on.\nYou should also tell the mailing list about this merged notebook! To get feedback! NICE! :)\n. @yorkerlin I will be back to being present from now on, sorry that this took so long\n. Thanks a lot for the update!\nGood to hear that we are in schedule. The complexity things that you wrote here should be mentioned in the notebooks when you write about the methods. I am looking very forward to example of this. Where are you putting them? I think they could go into the GP notebook, but having another one for variationel inference is also fine. In fact, since you will have to write a long report at the end, you can just start this thing now and then we put a link in the GP notebook.\n. Restarting build with merged data.\nThanks for the patch. This looks really nice now. It is hard to overview everything in this PR, but we will extensively use and revise this soon from a practical perspective\n. @lambday feel free to merge yourself once travis is green\n. I think its fine if the old methods are not that well designed. They are not meant to be used, but are just for our internal comparisons and the paper later on. So totally ok, it's the correct ones that have to be designed properly.\nVery good work!\n. May I suggest to do a proper comparison with some other implementation, say scikit-learn?\nBoth in terms of speed and accuracy. @tklein23 and me recently had a discussion and think we should do this for any std method.\n. Nice! Thanks!\n. @yorkerlin can you isolate the error a bit more?\n. @yorkerlin btw ping me if travis is green and I can merge things (sometimes loose the overview here)\n. Looks good to me! Thanks.\nWaiting for travis, I restarted.\n. Hi there.\nIn fact this is a very good point as we are lacking to present this properly. Here are some ressources:\n- Shogun classes. Valid for all modular interfaces. http://www.shogun-toolbox.org/doc/en/latest/namespaceshogun.html\n- folder examples/undocumented/python_modular Contains python API example for almost everything you can do with Shogun\n- Our notebooks also cover quite a few things: http://www.shogun-toolbox.org/page/documentation/notebook\nIf you have questions about more particular things, please get back to us - our mailing list is quite active. If you want to contribute to documentation, that is more than welcome :)\n. Yeah should be - I mean it still represents the same objects: Covariance between h terms.\n. If you want to, see the nips 2012 paper: Optimal kernel choice for large scale two sample testing\n. \"Please wait patiently.\" :)\n. I cannot see the notebook output, all output cells empty\n. I cannot see the notebook output, all output cells empty\n. Travil failure unrelated, merging\n. Travil failure unrelated, merging\n. Thanks! :)\n. Thanks! :)\n. Nice, this is an elegant solution to the speed problem and I think \"the way to go(tm)\"\n. Nice, this is an elegant solution to the speed problem and I think \"the way to go(tm)\"\n. So please merge once the integration tests are fine\n. So please merge once the integration tests are fine\n. So x-validation works with the stuff? Nice :)\nIn fact, we should always check this for any regression/classification model. Its a pain to do afterwards when one doesnt understand the code anymore ;)\n. The notebook is really coming along nicely!\n. Thanks for the patch and see all my comments!\n. Ok nice!\nPlease pause the new KL stuff until all other things (see email) are done\n. Quite nice how the project comes along now! well done :)\n. Travis seems fine apart from some unrelated issue. \nMerging! Nice!\nAbout the 4th KL method - yes go for it. But I am also very keen on the notebook. Plan to put some effort into that - it is what really makes your project accessible to the world. Could you share any current state with me? Then I can already give some feedback.\n. Travis seems fine apart from some unrelated issue. \nMerging! Nice!\nAbout the 4th KL method - yes go for it. But I am also very keen on the notebook. Plan to put some effort into that - it is what really makes your project accessible to the world. Could you share any current state with me? Then I can already give some feedback.\n. @tklein23 this might be of interest for you and your buddy\n. thanks @chengsoonong !\n. Nice! I am excited we have this finally in! :)\n. The travis problems seem unrelated to me, so merging!\n. Please in all of your notebooks, say that they were created during the GSoC 2014, state your project and mentor. See the GP notebook for an example\n. \" The CLibSVM class of shogun\"\nShogun always capital!\n. \" based on a number of training examples (x,y)N\u22121i=0 where x\u2208\ue244 and y\u2208{\u22121,+1}. \"\nPlease follow exactly the notation of the regression notebook to have such things consistent. \ne.g. x_i, y_i and the \\subseteq \\mathcal{X} \\times \\mathcal{Y} ....\n. Why N-1 samples? Again, please stay consistent with your other notebooks\n. \"Now a SVM is a binary classifier that tries to separate objects of different classes by finding a (hyper-)plane such that the margin between the two classes is maximized:\"\nNo \"Now\".\n\"More formally, a support vector machine is defined as\"\nRepetition, \"more formally\". \n. The i in the k(x,x_i) should not be bold\n. Please use \\text{sign} if you write sign in math mode\n. You should state the decision function f(x) after having mentioned the optimisation problem, and say that the solution takes this particular form\n. General comment about the formal intro:\nWhat about defining SVM without kernels first, just linear SVM (you dont have kernel functions in the decision, but rather inner products  (for example x^T x_i ).\nThen, please also talk about slack variables and the soft margin (could also be later in the notebook)\nThen, once you have explained all concepts, you can say that a feature mapping phi(x) leads to a kernel-svm which implicitly does things in  a higher dimensional space. You could motivate this by trying to solve a non-linear problem with a linear SVM and then montivate the usage of a kernel from this. Play a bit with this\n. \" support vector machine with shogun:\"\nNote that ALL names of things: Support Vector Machine, Shogun, are always written with capital letters\n. Please also do the structure: Formal intro (skip if you just want code examples)\nYou mix the intro here with the generating data from Gaussians. \nI know thats now your work in fact, but it would be still cool if you improved this on the fly :)\n. \"_=scatter(traindata[0,:], traindata[1,:], c=trainlab, s=100)\"\nAs for all plots (and I know, its not your fault here:)\nAxis labels, legends, etc\nAlso, maybe use a few less, these points are way too cluttered.\n. I would remove the part on kernel matrices, and put it into another kernel notebook! (Add that to your list of things to do :)\n. Just realising I am mostly commenting on S\u00f6ren's part of the notebook :)\nBut maybe you could polish that a bit?\n. Just realising I am mostly commenting on S\u00f6ren's part of the notebook :)\nBut maybe you could polish that a bit?\n. \"Kernels in Shogun\"\nThis really nicely fits in the stuff I mentioend above: First introduce the linear SVM and then in this part introduce kernels, and illustrate them a bit. you could actually print the kernel matrices for those kernels too, I think it might be instructive.\n. \"Kernels in Shogun\"\nThis really nicely fits in the stuff I mentioend above: First introduce the linear SVM and then in this part introduce kernels, and illustrate them a bit. you could actually print the kernel matrices for those kernels too, I think it might be instructive.\n. \"CKernel is the base class for kernels in Shogun.\"\nPlease add some kind of intro to this paragraph, it starts quite sudden :)\n. \"gaussian_kernel=GaussianKernel(feats_train, feats_train, 2)\"\nI suggest that you use the power of polymorphism here, and create a python list of kernels\nkernels=[GaussianKernel(feats_train, feats_train, 2), PolyKernel(feats_train, feats_train, 2, True), ...]\nand then do kernels[0].get_kernel_matrix() or similar. Kind of cool!\n. \"Samples on the margin of separating hyperplane are called the support vectors. \"\nNot exactly. Support vectors are those training data, for which the alpha_i!=0. This includes margin errors, and points on the margin of the hyperplane. But there might be in fact points that lie on the margin, but are no SVs. (Though this rarely happens in practice). The reason why I write this is that there are loads of SVs shown in your plots. These are mostly margin errors.\nI suggest that you do a very easy classification problem and plot the SVs in there, with and without margin errors. (You need to introduce the soft-margin SVM first). You should just have a couple a training data, and then plot the hyperplane, and the SVs. This then gives a really nice intuition. Then, next thing is to do the nice plots you added already.\n. \"Support vectors using different kernels\"\nYou should interpret the plots a bit (for both the simple classification and this case)\n. I would also do a contour plot underneath the SVs to illustrate how margin and SVs (and margin errors) are aligned\n. \"Kernel Normalizers\"\nStart of with saying what this paragraph is about and why one should normalise kernels for SVMs\n. Also, please show how normalisation changes the results of the classification (google for a nice example where normalising has a large impact)\n. Maybe you could also give the multiclass notebook some love and cleanups. It doesnt really look nice yet\n. For the multiclass SVM, you should say that this is a true multiclass SVM that doesn not play these competition games with multiple machines. A reference paper might be nice.\n. General things to add:\nLinear SVM (there are loads in Shogun)\nSupport vector regression !!\n. \"_=imshow(km)\"\nmake it\n_=imshow(km, interpolation=\"nearest\")\n. \"i=1\n    for kernel in kernels:\n\"\nfor i, kernel in enumerate(kernels):\n. \"svm.set_kernel(kernel)\nsvm.train()\"\nnot needed, you can just remove those, also the whole visualisation of kernel matrices should be in the previous section.\n. \"We could now check a number of properties like\"\nthis is then the start of section \"Prediction using kernel based SVM\"\n. \"Now we train an SVM with this GaussianKernel. We use LibSVM but we could use any of the other SVM from shogun.\"\nmake all classes links to docs, Shogun capital letter\n. \"out=svm.apply(RealFeatures(grid))\nz=out.get_values().reshape((size, size))\"\nyou should make clear that a gaussian kernel is sued here, but that is just coming from the fact that it is computed in the loop above.\ni would put the svm.train call into this code block\n. and in the hearmap, comment on the non-linear boundary induced my the gaussian kernel\n. 2Soft margins and slack variables\"\nah here they are. so above the C should be remove completely\n. \"This in its dual form is the familiar equation \"\nwhy the N-1 in the sums\n. \"soft-margin SVM\" wikipedia or other link please :)\n. \"The result is that soft-margin SVM could choose decision boundary that has non-zero training error even if dataset is linearly separable but is less likely to overfit.\"\nsay that C controlls the amount. say that you will now show visual example for different values of C\n. libSVM\ncapital L and link\n. \"Often this leads to overfitting.\"\nin high dimensions, this might lead to over-fitting\n. easer\neasier\n. shoguns gaussian mixture model\nShogun Gaussian Mixture Model\n. \"plot_support_vectors(kernels, svm)\"\nremove, you got the more colorful one below\n. \"Kernel Normalizers\"\nyou should say why one wants to normalise kernels\n. \"The distinguishing properties of the kernels are visible in these classification outputs.\"\nI like the plot!\n. nice work. this is getting better and better @sonney2k will be happy :)\n. when you mention the GMNP svm, please put a link to multi-class svm class in shogun\n. also, would you mind cleaning up\nhttp://www.shogun-toolbox.org/doc/en/latest/classshogun_1_1CGMNPLib.html\na bit? docs?\n. another comment, you know that you can update gists? no need to create a new one for every version. that saves trees :)\n. \"display_km(kernels, svm)\"\nwhat about a comment on how the matrices look? there are clusters visible that are smooth for the gaussian and polynomial kernel and block-wise for the linear one. the gaussian one also smoothly decays from some cluster centre while the polynomial one oscillates within the clusters\n. \"Now we train an SVM with a GaussianKernel\"\nNow we train a SVM with a Gaussian kernel\"\n. when you talk about duality gaps, please reference the stephen boyd book on convex optimization, for example referenced here https://en.wikipedia.org/wiki/Convex_optimization\nit contains a full (and simple) explenaition of these concepts\n. very nice explanation of the C values! :+1: \n. \"Multiclass classification\"\nall the pages in the shogun class documentation that you reference could need a little workover, formatting cleanups, math in the descirption, references. Would you mind doing a few on the fly? (seperate PR)\n. Ok read everything. I really like how this notebook is coming along, great work!\nApart from the minor things i commented on, a section on probability outputs would be amazing. for both single and multiclass (although i dont know whether the latter actually works)\nI will already merge this, but please address all my comments in another pr\n. svm with custom kernel would still be a good one i think.\ncompute a custom kernel on some string features, use the distant segments kernel or similar\n. svm with custom kernel would still be a good one i think.\ncompute a custom kernel on some string features, use the distant segments kernel or similar\n. Check http://www.shogun-toolbox.org/doc/en/latest/classshogun_1_1CBinaryLabels.html\nThere is a method to transform scores to probabilities. If you search for the method name in the examples or unit test you should find how it's done. Let me know if you have any problems\n. Yep! Thanks a lot!\n. Yep! Thanks a lot!\n. Merging since only doc changes\n. What is the reference implementation? \nWhat do you mean by the results are failing sometimes?\n. Ok this is nice now. I guess we are waiting for it to pass the unit test that worked on the old implementation right?\n. @kislayabhi let me know once you are done!\n. very nice work!\nmost comments are cosmetics, the matrix inline re-use is worth thinking about a bit more\nalso, you will need to unit test all three enum cases. but that can be done in a seperate pr i guess\n. ping me when done :)\n. why does coverage report decreased testing coverage? did you test all branches of the code?\n. yes, basically whenever you write code, make sure that every line is covered by at least one unit test, and if not, that be concious about the fact that it is not covered. coverage should increase (a tiny bit) with every patch.\nbut first we should fix travis\n. some cosmetics are minor\nan issue are the non-initialised class members\n. the python integration test fails.\nthis is because you changed the class quite a lot.\nwhat you need to do is\n- go to tests/integration/python_modular\n- run python tester.py classifier_lda_modular.py This will fail due to your changes (and is what fails on travis)\n- run python generator.py classifier_lda_modular.py T will update the integration testing files (result of the python example that is compared against test file)\n- commit the corresponding files to the shogun-data repository (git status in the data dir and you will see)\n- commit the \"data\" in your main PR after the other PR was merged.\n- python modular travis will be green\n. could you squash all commits into one to keep the log clean?\nask me if you dont know how to do that\n. yes for the integration tests to work, we need to merge the shogun-data update first and thenn you need to add the data version to this PR here. let me know if you have problems with that\n. ok merged, send the data commit\n. and then we can merge this. long beast, but well worth the troubles :)\n. there should be a notebook on this, really cool that we have both N>>D and D>>N now\n. any idea whats wrong with the last python build?  we should merge this soon\n. any idea whats wrong with the last python build?  we should merge this soon\n. check the output of the clang python integration test \n[INFO] leaving LDA::equals(): parameters at position 5 with name \"w\" differs from other object parameter with name \"w\"\n. check the output of the clang python integration test \n[INFO] leaving LDA::equals(): parameters at position 5 with name \"w\" differs from other object parameter with name \"w\"\n. the number is different, which means that either the integration test file is not updated or there is something going wrong elsewhere\n. the number is different, which means that either the integration test file is not updated or there is something going wrong elsewhere\n. test 1/2 passes, did you update both?\n. test 1/2 passes, did you update both?\n. yes just checked, it seems fine.\ncan you have a look at the log of the output and investigate what goes wrong, it seems there are actually wrong results produced\n. @kislayabhi could you make this one priority? we should aim to have this merged asap before it gets under the wheels. Also, any notebook on this planned? :)\n. you need to update the data version of this patch then for things to take effect\n. @vigsterkr @sonney2k @pickle27 any ideas?\ni dont know that actually\n. you guys should be using xclip and pipes - that makes sure no conversion errors appear\n. haha! nice! :) finally\n. haha! nice! :) finally\n. This is all fine. Merging once travis gives ok\n. which is already the case\n. Okay. Does this test fail locally? If not, compare the data version, if yes, quickly double check whether results are sensible and then just update integration test file. Best thing in fact would be to identify the PR that first broke this - though some work as hidden in red travis.\nYou see, this is the reason to keep the build green, things slip through otherwise.\n. Okay. Does this test fail locally? If not, compare the data version, if yes, quickly double check whether results are sensible and then just update integration test file. Best thing in fact would be to identify the PR that first broke this - though some work as hidden in red travis.\nYou see, this is the reason to keep the build green, things slip through otherwise.\n. Note that re-generating data is not the way to fix broken tests. That should only be done when the class is changed. But Ill merge for now since then we are green again!\n. +1 for this!\nand also +1 for keeping the old test as sanity check.\nI merge this as travis gcc is green\n. doucment extensively in the doxygen class dox and the notebook! :)\n. I investigated a bit. This seems to be an issue of your ubuntu installation mixing up python versions. Shogun has been built against a python version with UCS2, while your python interpreter itself has been build against UCS4 - so the development files don't seem to match the binaries (of Python).\nCould you try re-installing all of your python installation? Did you ever mess around with local python installations or virtual-evn? If yes, you should remove them.\n. I investigated a bit. This seems to be an issue of your ubuntu installation mixing up python versions. Shogun has been built against a python version with UCS2, while your python interpreter itself has been build against UCS4 - so the development files don't seem to match the binaries (of Python).\nCould you try re-installing all of your python installation? Did you ever mess around with local python installations or virtual-evn? If yes, you should remove them.\n. @escorciav You just need to tell Shogun your Python version. This one has to match the one that you will be using Shogun from\n. Very nice!\nBut please unit test this (activated & deactivated) in some ways. As we are working in the internals of the solver, we really want to avoid any form of bugs. So not merging now.\nUnit tests should not so much focus on numerical results here but make sure that these changes really do what they should and dont messup existing things\n. Thanks! :)\n. I suggest to add a comparison FisherLDA vs PCA in some notebook! :)\n. valgrind should not complain about anything locally. errors like uninitialised reads are quite dangerours and thus have to be resolved\n. (almost) fine to merge,\nonly cosmetics remaining .... but pls address them.\nthe travis error seems unrelated, restarting\n. i dont think too many values would be passed, but you can also leave it if you dont like the modular idea. same for the other lda\n. there are a few cosmetic changes (please send another patch for them) but the thing is mergable :) nice work!\n. Nice! Thanks.\nSee my comments\n. Merging, see minor comments\n. The libshogun*.so is not found. So either run make install, or put its path\nin the build folder to the LD_LIBRARY_PATH\nHope that helps\nH\n2014-06-26 14:15 GMT+01:00 xiaoleiw notifications@github.com:\n\nHi all,\nThanks for your time in advance but i really have question when I want to\nmove my platform to another computer.\nAll the python scripts are able to run in my previous PC, and my new PC is\nthe same system but it appears to fail all the time.\nI re-install shogun for 5 times still the same error:\nFile \"Mainfunction.py\", line 14, in\nimport GetDictionary\nFile\n\"/home/wangx0e/Documents/xl/wangx0e/Desktop/WD_SVR_code/GetDictionary.py\",\nline 11, in\nfrom shogun.Features import StringCharFeatures, BinaryLabels, DNA\nFile \"/usr/local/lib/python2.7/dist-packages/shogun/Features/init.py\",\nline 1, in\nfrom modshogun import *\nFile\n\"/home/wangx0e/Documents/xl/wangx0e/Desktop/WD_SVR_code/modshogun.py\", line\n30, in\n_modshogun = swig_import_helper()\nFile\n\"/home/wangx0e/Documents/xl/wangx0e/Desktop/WD_SVR_code/modshogun.py\", line\n22, in swig_import_helper\nimport _modshogun\nImportError: libshogun.so.16: cannot open shared object file: No such file\nor directory\nHere is my version, which is the same python & system as successfully\nexecuted.\n~$ shogun\nlibshogun (x86_64/v3.2.0_2014-2-17_18:46197120)\n~$ python\nPython 2.7.3 (default, Feb 27 2014, 19:58:35)\n[GCC 4.6.3] on linux2\n~$ lsb_release -a\nNo LSB modules are available.\nDistributor ID: Ubuntu\nDescription: Ubuntu 12.04.4 LTS\nRelease: 12.04\nCodename: precise\nDO thanks a lot for your help .\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/shogun-toolbox/shogun/issues/2344.\n. Viktor, Bj\u00f6rn, any ideas? You are the experts for cmake\n\n2014-06-29 10:13 GMT+01:00 xiaoleiw notifications@github.com:\n\nHi,\nThanks a lot and really sorry to bother you again.\nThis time I tried to re-install the package and find it can not make\nsuccessfully anymore.\nI checked the CMakeError log and foundy the bugs as :\n/home/wangx0e/Documents/tools/shogun-3.2.0/CMakeFiles/CMakeTmp/CheckSymbolExists.cxx:8:19:\nerror: \u2018isfinite\u2019 was not declared in this scope\nand so on.\nThe log is attached here as well as the output file.\nI know it is impolite to show this kind of logs but actually I tried to\ngoogle for each error and found no solution.\nDo appreciated for your help.\nBest Regards!\nYours,\nWang Xiaolei\nOn Thu, Jun 26, 2014 at 7:32 PM, Heiko Strathmann \nnotifications@github.com\nwrote:\n\nThe libshogun*.so is not found. So either run make install, or put its\npath\nin the build folder to the LD_LIBRARY_PATH\nHope that helps\nH\n2014-06-26 14:15 GMT+01:00 xiaoleiw notifications@github.com:\n\nHi all,\nThanks for your time in advance but i really have question when I want\nto\nmove my platform to another computer.\nAll the python scripts are able to run in my previous PC, and my new\nPC\nis\nthe same system but it appears to fail all the time.\nI re-install shogun for 5 times still the same error:\nFile \"Mainfunction.py\", line 14, in\nimport GetDictionary\nFile\n\n\"/home/wangx0e/Documents/xl/wangx0e/Desktop/WD_SVR_code/GetDictionary.py\",\n\nline 11, in\nfrom shogun.Features import StringCharFeatures, BinaryLabels, DNA\nFile\n\"/usr/local/lib/python2.7/dist-packages/shogun/Features/init.py\",\nline 1, in\nfrom modshogun import *\nFile\n\"/home/wangx0e/Documents/xl/wangx0e/Desktop/WD_SVR_code/modshogun.py\",\nline\n30, in\n_modshogun = swig_import_helper()\nFile\n\"/home/wangx0e/Documents/xl/wangx0e/Desktop/WD_SVR_code/modshogun.py\",\nline\n22, in swig_import_helper\nimport _modshogun\nImportError: libshogun.so.16: cannot open shared object file: No such\nfile\nor directory\nHere is my version, which is the same python & system as successfully\nexecuted.\n~$ shogun\nlibshogun (x86_64/v3.2.0_2014-2-17_18:46197120)\n~$ python\nPython 2.7.3 (default, Feb 27 2014, 19:58:35)\n[GCC 4.6.3] on linux2\n~$ lsb_release -a\nNo LSB modules are available.\nDistributor ID: Ubuntu\nDescription: Ubuntu 12.04.4 LTS\nRelease: 12.04\nCodename: precise\nDO thanks a lot for your help .\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/shogun-toolbox/shogun/issues/2344.\n\n\u2014\nReply to this email directly or view it on GitHub\n<\nhttps://github.com/shogun-toolbox/shogun/issues/2344#issuecomment-47248260>\n.\n\n\n\nThis message and its contents, including attachments are intended solely\nfor the original recipient. If you are not the intended recipient or have\nreceived this message in error, please notify me immediately and delete\nthis message from your computer system. Any unauthorized use or\ndistribution is prohibited. Please consider the environment before\nprinting\nthis email.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/shogun-toolbox/shogun/issues/2344#issuecomment-47449500\n.\n. Nice! Merging\n. Very good that you do those benchmarks! Very good!\n\nThe code for the Shogun version should be put somewhere. In fact, we had a GSoC project on exactly this. Benchmarks that compare results and speed. Might be good to push this for all algos in a unified way (later)\nFor now: We are quite a bit slower than scikit. Is this due to multicore parallelism? Or other things? Your implementation should tak advantage of multicore!\n. Just restarted travis just in case, but I think your broke quite a few things here ;)\n. @mazumdarparijat @vigsterkr I don't agree that we should do this later. Experience tells this means it will never be done. So please spend some time in at least reaching scikit-learn. I know their random forest implementations are quite tuned, but still, they have all the python overhead - if we are slower, this means we are doing some things in a less efficient way. We should at least be as fast as them. No need to be faster - this can be done later.\n. Wow these are subtle!\nUnit tests for large numbers please! :)\n. 50.000 ? why would that take long?Use a linear kernel maybe? How long does it take?\n. sounds reasonable. that seems fine. just compute very few statistic/variance values. no need to do 100s.\ngood?\n. sounds reasonable. that seems fine. just compute very few statistic/variance values. no need to do 100s.\ngood?\n. Very nice work!\n. BTW just wondering, does travis actually compile the feature branch? :)\nSorry for stupid question if it does.\n. Mergable! See the minor comments.\nAmazing, we can now go ahead with the experiments\n. super nice and clean work, really good that the docs are in shape too, we are missing that in general in Shogun.\nfine to merge from my side!\n. super nice and clean work, really good that the docs are in shape too, we are missing that in general in Shogun.\nfine to merge from my side!\n. travis seems to be a diva again\n. travis seems to be a diva again\n. mmh, strange.\nbut the other tests also take ages, see the log that @sejdino pasted.\nThese things should not take more than a second on travis otherwise we run out of breath soon ;)\nthough somehow we should test these (reasonably) large numbers somehow ..... any ideas?\n. - nice plots, they should definitely go into the notebook\n- travis still fails\n- disable the big unit tests for now, we should however keep them in our heads and try to figure out a way to have a number of unit tests that are not executed on travis to save time @vigsterkr could we do this with a flag that is passed to the unit test execution?\n. - nice plots, they should definitely go into the notebook\n- travis still fails\n- disable the big unit tests for now, we should however keep them in our heads and try to figure out a way to have a number of unit tests that are not executed on travis to save time @vigsterkr could we do this with a flag that is passed to the unit test execution?\n. cool, thanks.\nlets wait what travis says\n. looks fine to me, but travis is freaking out currently, so cannot verify\n. @lambday do the tests pass locally (including python modular?)\n. @lambday do the tests pass locally (including python modular?)\n. so what do until this can be merged?\n. so what do until this can be merged?\n. ok go for it then!\n. yes i think that would be good\n. @lambday how is the state on this? i really would like to get it merged soon\n. @lambday yes that should be fine. Please go ahead\n. Super curious to dive into this code, and its way easier if its in develop\n. thanks! now i can play with it nicely without having to keep multiple branhces in mind. also can send minor patches\n. Big patch!\nBut very nice work. Most comments are style and minor, the SG_REF one needs to be addressed.\nPing me once you checked all, then I'll merge\n. also travis seems to complain\n. ah no thats json, no idea where this comes from, restarting\n. restarted! let me know once green.\nthe sgref changes would be good, though if you feel thats too much hassle for nothing, just document in the method names that the object is not SG_REF'ed\n. restarted! let me know once green.\nthe sgref changes would be good, though if you feel thats too much hassle for nothing, just document in the method names that the object is not SG_REF'ed\n. Your patches get better and better!\nReally happy with this one :+1: \n. Your patches get better and better!\nReally happy with this one :+1: \n. Nice one! This will allow to extend things to other tests\n. looks good on first sight. nice work :)\nI will read in more details later, gotta leave now\n. looks good on first sight. nice work :)\nI will read in more details later, gotta leave now\n. checked again, find to merge from my side. maybe @sejdino has some comments on the implementation?\n. checked again, find to merge from my side. maybe @sejdino has some comments on the implementation?\n. super nicely documented!\n. super nicely documented!\n. dont loose sleep over them, I just recently started paying a bit more attention to the doxygen we produce (as most is messy in Shogun :) )\nbut they are just cosmetics\n. see other mail on list from alex binder ;)\nwe basically want\n- working code\n- water proof code (unit tests)\n- examples that expose api\n- proper doxygen with math, reference papers, and high level description\nif this is done, the stuff is useful, if any is missing, usability is compromised massively\n. haha :)\n. Nice!\nyou are super productive :)\nI have nothing to complain, as soon as this is verified by travis, we can merge.\ni am incredibly excited to try this out in practice.\nHow does it compare to the matlab code speed wise?\n. Nice!\nyou are super productive :)\nI have nothing to complain, as soon as this is verified by travis, we can merge.\ni am incredibly excited to try this out in practice.\nHow does it compare to the matlab code speed wise?\n. man where does all this code come from faster than i can read it ? :)\nlooks very good. maybe @sejdino should double check the code, i have reviewed a lot today and am tired ;)\nfine to merge once travis says green\n. basic unit tests should be next.\nmake sure to test for some straw-men if users provide stupid input\n. So the import order. Phew, I have no idea what could cause this. Possibility: Python versions not matching.\nWhat are your system specs?\n@vigsterkr @sonney2k any ideas?\n. Could you double check that the python version shogun is coimpiled against is exactly the one of your system? you can do that via using ccmake and then go to advanced options which should should you the python that shogun uses.\n. thanks for the elaborations. Ah I told Roman back then not to use sigma=1 in unit tests.\nSo my suggestion to fix this is:\n- you write a unit test that tests the issues you found against some GPML or similar\n- this unit test should fail with the current implementation\n- you then fix the problem as suggested above\n- then send both unit test and patch at once.\n- if you could add unit tests with sigma != 1 where possible, that would maybe catch more problems?\nWell done!\nFor python examples, we want\n- one minimal API usage example\n- if you want to demonstrate reccomendation, please do that in a notebook\n- minimal reccomendation could also go into additional python examples\n. great!\na few things:\n- could you produce gists for the matlab reference code rather than putting it here? it will be lost here whereas gist links can be put in the code and are more accessible for later\n- the marginal likelihood estimate should always return something close to the approximate marginal likelihood. i think i added a test for this for some inference method - that should be enough. but if you feel that should be more waterproof, feel free to add tests for it\n- i think you might be right with the other bug - but i suggest to against first test it and then possibly correct - seems like you are on fire to find bugs :D\n. just checked, the marginal likelihood estimates are tested in the unit tests for inference method.\nwe could in theory move those tests to the individual inference methods. i put them in a separate test since its a method of the base class. maybe you can add some tests of those methods for all inference methods you added? this stuff is super useful, as it is a monte carlo estimate of the exact marginal likelihood\n. thanks guys!\n@abinashpanda what about putting the scikit comparison somewhere else? its not really tutorial for the method. Dont get me wrong, I love it that you did that, but I think the notebooks should cover only tutorial/method etc, everything else should be in other places. What are your and/or @tklein23 's thoughts on this?\n. looks good to me, merging, but in the future please always post a nbviewer link to the notebook with outputs\n. looks good to me, merging, but in the future please always post a nbviewer link to the notebook with outputs\n. a non-unicode character broke the python build on the buildbot\ncould you investigate?\n. a non-unicode character broke the python build on the buildbot\ncould you investigate?\n. looks like your cygwin environment is incomplete somehow. But I dont know what exactly is going on. @vigsterkr ?\n. looks like your cygwin environment is incomplete somehow. But I dont know what exactly is going on. @vigsterkr ?\n. lets wait what the others say on this, ping us if you dont get a reply within a couple of days, currently very busy :)\neasy solution: not use cygwin. but it should work as we have a buildslave running on it. correction had\n@vigsterkr @sonney2k what happened to the cygwin build, its offline\n. thanks for the patch, very nice work! that would have been undetected forever without you! :+1: \n. merging as travis error is unrelated (some octave error)\n. @votjakovr hey! Glad to see you're still alive :) How are things? Are you following the awesome work of  @yorkerlin ? Lots of potential to continue building upon this.\n. @votjakovr any plans to contribute? :D\n. @yorkerlin @votjakovr this would be most awesome if you paired up - you two are the GP specialists here :) \nIn fact, there is an infinite amount of stuff that could be done, @yorkerlin ideas are very useful already, so a good place to start.\n. Thanks\n. AWESOME! Will give this a proper read tomorrow, but already like the looks! :)\n. once you put the example in the form that allows integration testing, travis will complain about missing test data. You have to generate that using the generator.py script, commit the test files to shogun-data, and then update the data version of this one. Let me know if you have questions on that\n. thanks for the patch! :)\nlooking forward to the notebook\n. apart from my cosmetics comments, this is ready to be merged. thanks!\nnow lets see about travis\n. travis error seems unrelated.\nbut cann you please generate integration testing files for shogun data and update the data version of the main repository?\nthanks!\n. ping me once travis is done, ill merge then\n. ok travis is fine.\nplease address the other things in a separate pr. nice work!\n. that sounds similar to what i came across a while ago.\ndid you check the errata for this?\ni will do the math in a bit to verify\n. @emtiyaz could you have a look here. i am currently too busy to do this properly.\n@yorkerlin my suggestion is to code up your suggested solution (looks right on the first look) and then compare the results against some other toolbox such as gpstuff or pygp. If this works, then go for shogun. But I actually trust your math skills here, so go for it!\n. @yorkerlin lets keep the notebook discussion in the thread of the PR, see my comment in there\n. @yorkerlin all the bugs you collect (and verify via emt and numerical results), you should write them down in a LaTeX note, so that we can send them to the book authors\n. and yes, as @emtiyaz suggested, please focus on the notebook for now, i will try to give immediate feedback. you can do the multiclass stuff on the side if you want or if idling, but notebook is really most important now as its quite some work\n. sorry for the late review on this one, it will take a few more days.\n. add your credits in the header\n. \"Cross validation is aims to estimate an algorithms performance on unseen data.\"\nalgorithm's\n. \"Cross validation is aims to estimate\"\nCross-validation aims to estimate\n. \"marginal likelihood\"\nput some wikipedia links for marginal likelihood and Bayesian Statistics\n. \"Evaluating on training data\"\nEvaluation an algorithm's performance on training data\n. \"k<n\"\nits in fact less or equal (leave one out)\n. \"such that X1\u222aX2\u222a\u22ef\u222aXn such that Xi\u2229Xj=\u2205 for all i\u2260j.\"\nsuch that X1 .... Xn = X (forgot the =X), \"and such that.....\"\n. \"is stratified cross-validation (TODO link)\"\nput in a wiki link or reference the apples to apples paper\n. \"This divided the data \"\nThis divides the data\n. \"Leave One Out cross-validation\"\nalso create a split object for this strategy, you only do it for the two previous ones\n. \"for i in range(len(split_strategies))\"\nbtw in python you can do\nfor strategy in split_strategies:\nor\nfor i, strategy in enumerate(split_strategies)\n. \"_=plot_folds(split_strategies, 4)\"\ncould you make the fold indices be in increasing order?\n. \"LibSVM (TODO link) \"\npls put a link :)\n. \"ROC curve (AUC)\"\npls put some wiki links and possibly shogun classes\n. \"note that this call involved a lot of computation\"\nnote that this call potentially involves a lot of computation\n. \"# 20 runs and 95% confidence intervals\"\n25 runs!\n. \"which should be easy to conclude from the plots.\"\nMention that for the left one, it seems to not matter which tau to choose. Why might that be?\n. could you also put the tau plots in one coordinate system to point out that the kernel method performs better?\nIn the left plot you could plot the regression function for the best tau parameter\n. \"Now that we have the best parameters,\"\nCareful here!\nIt is not clear that for KRR, the kernel parameter is independet of the tau. Since you optimised them each with the other ones fixed, you might not yet be optimal. Please add a little section where you evaluate a 2d grid on tau/width and visualise as a heatmap. Mention the fact that one has to try all pairs if one wants to do this brute force, but that this is only ever feasible for a low number of parameters\n. Nice work so far, looking forward to the second iteration! :)\n. \"and their parameters selection\"#\nand selecting their parameters\n. \"Stratified splitting takes care that each fold has almost the same number of samples from each class. \"\nwhy not plot the loo splits also?\n. \"Grid search sufers\"\ntypo: suffers\n. \"which can lead to pretty huge \"\nremove the pretty, its not pretty ;)\n. remove the coming soon, its in there now :)\n. I think this is a very nice notebook, soon ready to merge\n. any udpates here?\n. Thats easy to add. Have a look at interfaces/modular/ModelSelection.i and ModelSelection_includes.i\nYou will have to add a line or two to both files to expose your loo class to all modular interfaces. Should be clear from example.\n. I really like this table. Could you add this to all notebooks (at least yours?)\n. ok notebook is ready to be merged. nice work!\n. restarted travis,\nall these changes are fine i think so i will merge it.\nlets keep a close look on travis just in case.\nbtw you should build some shogun-data integration testing files for this soon\n. i agree with @vigsterkr \nbut we should merge this asap so that all gsoc work can be re-phrased in terms of this - i expect some issues so we should have some buffer time.\n. \"There are clusters visible that are smooth for the gaussian and polynomial kernel and block-wise for the linear one\"\nGaussian is always with capital G\n. \"There are clusters visible that are smooth for the gaussian and polynomial kernel and block-wise for the linear one\"\nGaussian is always with capital G\n. \"A parameteric form of a sigmoid function\"\nIn the math below this, use \\exp rather than exp in the latex. Or, if the command is not define, you should use \\text{exp}. We dont want the letters of function names be interpreted as mathematical variables (italic typesetting)\n. \"A parameteric form of a sigmoid function\"\nIn the math below this, use \\exp rather than exp in the latex. Or, if the command is not define, you should use \\text{exp}. We dont want the letters of function names be interpreted as mathematical variables (italic typesetting)\n. \"The familiar \"S\" shape should be visible.\"\nReally like this plot!\n. \"The familiar \"S\" shape should be visible.\"\nReally like this plot!\n. Thanks! :)\n. Thanks! :)\n. json only supports up to 1e-6 precision, its lossy, maybe thats the reason\n. json only supports up to 1e-6 precision, its lossy, maybe thats the reason\n. thanks for the fix!\n. hi there,\ncould you please always provide a nbviewer link with the current state of the pr, statically rendered with ouputs? see @Saurabh7 prs if unsure: you paste the notebook into a gist (with outputs), and then put this into the nbviewer page\n. i guess you got the wrong file, this is the gp notebook\n. it is also missing the outputs, please render your notebook (only for the nbviewer link, not for the PR) - otherwise i cannot see what you have done\n. changes look fine to me, travis would help but is down.\nill merge since i might be less active the next days\ntravis seems unrelated\n. @yorkerlin could you open another pr for the notebook?\nmy comment so far:\n- please try to tell a story, write some motivation, what you are doing, why its interesting, etc etc\n- try to avoid huge methods in python but rather guide someone who presumable doesnt know kl inference to understand things. \n- the table in the end is nice, but could be presented in a nicer way. \n- interpretation of results might be nice\n- pro/con of the methods might be nice\n. exactly.\nin fact, you can start another notebook for this. Proper tutorial about the variational inference in Shogun.\nBut leave the example in the gp notebook also, just give some intuition. Also reference the new notebook from the gp notebook so that people dont miss out your cool stuff :)\n. I am fine with merging this, just wondering why you do these changes?\n. ok!\n. It depends on the error message and the particular reason why this fails (can you post it here?)\nOne way is to blacklist the example when the depenency is not met (cmake).\nThe meta examples doe this via FindMetaExamples. There is not systematic way for the old examples so far, so you could either translate the example to the meta examples, or just put a try catch around the line that makes the example fail.\n. @vigsterkr there might be some connections to the existing random number library here\n. @vigsterkr there might be some connections to the existing random number library here\n. I like the idea with the hyperlinks. Please go ahead!\n. I like the idea with the hyperlinks. Please go ahead!\n. \"\nIn Kernel Ridge Regression (1) we have seen the result to be a dense solution\"\nwe have seen that the solution weight vector was dense, i.e. $\\alpha_i\\neq0$ for most $i$\n. \"\nIn Kernel Ridge Regression (1) we have seen the result to be a dense solution\"\nwe have seen that the solution weight vector was dense, i.e. $\\alpha_i\\neq0$ for most $i$\n. \"In the SVM the penalty was paid for being on the wrong side of the discriminating plane. \"\n\"a penalty\"\n. \"In the SVM the penalty was paid for being on the wrong side of the discriminating plane. \"\n\"a penalty\"\n. \"we introduce a penalty for being far away from predicted line\"\n\"the learned regression function\", rather than \"predicted line\"\n. \"we introduce a penalty for being far away from predicted line\"\n\"the learned regression function\", rather than \"predicted line\"\n. \"once you are close enough\"\nwe, not you\n. We are given a labeled set of input-output pairs \ue230=(xi,yi)Ni=1\u2286\ue244\u00d7\ue245 where x\u2208\ue244 and y\u2208\ue245 and the primary problem is as follows:\nNote that \\mathcal{Y} is now \\mathbb{R}\n. \"This class also support the \u03bd-SVR regression version of the problem\"\nmake it\nShogun also supports the very similar v-SVR ....\n. \"Let us do comparison of time taken for the 2 different models simliar to that done in section 6 of [1]. The Boston Housing Dataset is used.\"\nThe plot above this is very nice. You should maybe mention that the linear kernel corresponds to a linear regression function, and the polynomial kernel correcponds to a polynomial regression function, and the Gaussian kernel corresponds to a non-parametric regression function.\n. The performance comparison at the end is a nice idea, but what is the interpretation of the values here? Could you make this a bit more meaningful? What do we see, what do we learn from this? etc. I am sure the referenced paper gives lots of inspiration for this.\nIt would also be interesting to compare KRR and SVR\n. I am merging this already since it looks good, but please address my comments in another PR\n. Updates?\n. Last call for updates before closing ...\n@kislayabhi this can go into an issue if the PR cannot be merged.\n. thanks!\n. could #2005 help?\n. Hi Wu,\nthanks for the patch and sorry for the late reply. The workshop organisation was quite some work and I  needed some break after that. Anyways - this patch is too big. Too many things are changed at once. Could you please separate things that do not belong together?\n. We need to close this soon due to inactivity.\nIt would be a pitty to lose the cool notebook though ...\n. Ok here are some comments:\n. \"Based on the notebook of Gaussian process by \"\ntypo: Gaussian Processes\n. I think in the intro, you dont have to mention the other gsoc projects, but put a link to the GP notebook\n. \"\nThis notebook is about the variational inference method used in classification models with Gaussian Process (GP) priors, which is called Gaussian Process Classification (GPC) models, in Shogun.\"\nmake it\nThis notebook describes how to do variational inference for Gaussian Process (GP) classification models in Shogun. \n. \"make prediction\"\nplural s is missing\n. \"automatically learn parameters in Shogun\"\nmake it\nautomatically learn hyper-parameters for GPs\n. \"where a function f:\ue23ed\u2192\ue23e drawn from a Gaussian Process prior \"\nis drawn from a ....\n. \"while f is a set of point.\"\nmake it\n\"while f is an n-dimensional vector\"\n. \"are also different because p(f) is measured in infinite dimensional (function) space while p(f) is measured in finite dimensional space.\"\nin an infinite dimensional space, at two places.\nWhat do you mean by \"is measured\", you probably want to say \"is a distribution on an infinite dimensional space\", or \"distribution on an n-dimensional space\"\n. \"poerstior, p(f|y,X)\"\nCan you write this out? I.e. prior (Gaussian) times likelihood (non-Gaussian in classification) Otherwise, people will have no idea why its not Gaussian\n. \"i-th row of X and f(Xi)=fi\"\nin this line, all the i indices are bold, but they should not. minor\n. \"\nMore detailed information about GPC (Skip if you just want code examples)\"\nIn the whole section, can you not start a new line after each sentence? It destroys the textflow of the typesetting\n. \" label (ynew) \"\nwhen you write text, such as \"new\" in math mode, please use \\text{new} to avoid the letters being treated as mathematical variables\n. \"a column vector, xnew\u2208\ue23ed based on p(ynew|y,X,xnew)\"\nin particular, dont make text in variables bold, same as the \"i\" index above. \"new\" subscript should not be bold\n. \"According to Bayes' theorem, we know that p(ynew|y,X,xnew)=\u222bp(ynew|f,xnew)p(f|y,X)df\"\nBayes theorem only give you the form of the posterior p(f|y,X). You want to say that the predictive distrbution for a new point is obtained by integrating the likeligood for a new point over the posterior distribution. (You could put a wiki link http://en.wikipedia.org/wiki/Posterior_predictive_distribution )\n. \"Informally, according to the property of GP about marginalization,\n\u222bp(ynew|f,xnew)p(f|y,X)df=\u222bp(ynew|fnew)p(fnew|y,X)dfnew.\"\ncan you put the math around a double $$ ? This will make it appear in its own centered line\n. The text in the section doesnt really flow nice. Could you add some sentences about intuition in between the math? You are currently just stating it\n. \"p(fnew|f)=p(f([X;xnewT]))p(f(X)) is followed by finite-dimensional Gaussian distribution\"\nis followed? This doesnt make sense in English. What do you mean here?\n. \"Note that If p(f|y) is followed by a Gaussian distribution, p(fnew|y,X) has a close form.\"\nSame here, this sentence doesnt make sense in English\n. \"However, in classification p(f|y) usually is NOT followed by Gaussian distribution since\"\nsame here, also, please no line break after this \"since\"\n. lots of \"is followed\" here, please change all of them\n. I like that you mention the EP connection :)\n. \"Based on the first session, variaitonal inference in GPC is about Minimizing the KL divergence listed as below:\"\n- \"As mentioned in the first section, ....\"\n- type in variational inference\n- minimising with lower case m\n- \"listed\" is not really the right word here. Use \"given\"\n. \"Another way to explain variational inference in GPC is about Maximizing a lower bound of the log of marginal likelihood, ln(p(y|X)).\"\n- maximizing lower m\n- \"is about\" -> \"is to maximise\"\n- the bound is \"on the log marginal likelihood\"\n. in the bound line, please us \\ln or better \\log rather than pure text (which is printed italic)\n. \"Dealing with the non-close form issue in GPC (Skip if you just want code examples)\"\nYou already wrote \"skip if you just want code examples\" three times. Can you put all the theory sections into one section with subsections and only write it once?\n. \"Dealing with the non-close form issue in GPC \"\nnon-closed\n. \"for multivariate Gaussian distribution.\"\nmake it\n\"of the multivariate Gaussian distribution.\"\n. lots of \"ln\" without using \\ln\n. \" denotes q(f|y) followed by multivariable N(\u03bc,\u03a3)\"\nfollowed by???\n. \"denotes qi(fi|yi)\"\nno bold indices\n. \"Again, variational inference in GPC is to approximate p(f|y) using a Gaussian distribution, q(f|y).\"\nPlease dont do linebreaks after each line\n. \"A toy example to visually explain variational inference via existing variaitonal methods in Shogun\"\nmake this\n\"A toy example to visually explain variational inference\"\n. \"posterior,p(f|y),\"\nmissing space after coma\n. \"xlist                    = np.arange(xmin, xmax, delta)\"\ncould you not do this space indentation? Just use \"xlist = np.arange ....\"\n. \"def Gaussian_points(Sigma,mu,xmin,xmax,ymin,ymax,delta=0.01):\"\nput a little comment what this function does.\nIn fact, why not describe what you will exactly do in the example before you give the code. It is a lot of code, and it would make sense to give the reader some high level of whats going to happen\n. @emtiyaz probably also has comments btw\n. \"Z = []\"\nin all python code, please dont build arrays by appending elements to lists, this is very bad python style.\nRather allocate an array using np.zeros(len(X)*len(Y))\n. \"def likelihood_points(X,Y,labels,likelihood)\"\nalso a short comment on what this thing is doing (as python comment) would be good\n. \"len(Z)/len(X)\"\ninteger division, careful! (I know it works here, but you could also just use len(Y) instead)\n. \"def approx_posterior_plot(methods, kernel_func, features, mean_func, labels, likelihood, kernel_log_scale, \n                          xmin, xmax, ymin, ymax, delta, plots)\"\nshort documentation of what this does please\n. inference                = methods[r][c]\nplease no spaces to align things\n. \"x=np.asarray([sqrt(2),-sqrt(2)]).reshape(1,2)\"\nplease put function definitions (above this line) and the code itself to a seperate cell.\nAlso please consider splitting things more: each function into a seperate cell for example\n. \"kernel_sigma             = 2_exp(2_kernel_log_sigma)\"\nyou should say why you do this here, ---- or just not do it :)\n. \"inf                      = inference(kernel_func, features, mean_func, labels, likelihood)\"\nwhen you use Shogun objects that you have not introduced before, you should talk about their API.\nIn fact, I suggest you put a little paragraph somewhere where you talk a bit about the class structure of the variational inference framework. With a link to the shogun class diagrams, as for example this one might be super useful.\nhttp://www.shogun-toolbox.org/doc/en/latest/classshogun_1_1CInferenceMethod.html\nYou should then talk about methods in the base class and how things are structured. Take all main methods and say what they do.\n@emtiyaz, this above link is also useful for you to review!\n. \"approx_posterior_plot(methods, kernel_func, features, mean_func, labels, likelihood, kernel_log_scale, \n                      xmin, xmax, ymin, ymax, delta, plots)\nplt.show()\"\nI really like the plots!\nTo structure things a bit:\n- you should plot the true posterior/likelihood/prior in seperate cells and then say: We want to approximate the posterior.\n- Then afterwards, you should just approximate it with one inference method, and interpret the results a bit \n- what about putting the principal components of the Gaussian in the plot?\n- Then, as a last step, you can produce the posterior for all of the inference methods. But keep in mind this is a just a visual example that explains that you approimate the posterior by a Gaussian.\n. Another comment, what I would find very interesting are some points on what the different of variational ifnerence and the Laplace approximation is. The reader might ask himself why do variational if its just another Gaussian approximation just as the Laplace..... you can then answer this question\n. the KLDualInferenceMethod seems to fail somehow ... why is that?\n. Please also say some sentences of what one can see in every plot you do, also you need to put axis labels to every plot\n. \"A toy example to demonstrate the usage of variational methods in Shogun\nWe apply variational methods to the sonar data set, which can be found at here.\"\nThis is not toy data, its \"real world data\"\n. Please say what is in this dataset, you can copy the description from the names file and say that its coming from there\n. \"def learning(inference, linesearch, x_train, x_test, y_train, y_test, kernel_log_sigma, kernel_log_scale):\"\nthis cell contains too much code. please split it into at least 5 sub cells, and for each of them, tell the reader whats going on, this is very hard to read as it currently is. Also, a few sentences of what you intend to do here (before you do it) would be very helpful\n. \"#print \"\\nusing %s\"%inf.get_name()\"\nplease no commented out code in the notebook\n. #nlz=inf.get_negative_log_marginal_likelihood()\n    #start = time.time()\n    #classification on train_data\n    #pred_labels_train = gp.apply_binary(features_train)\n    #classification on test_data\n    #pred_labels_test = gp.apply_binary(features_test)\n    #end = time.time()\n    #print \"cost %s seconds at prediction\"%(end-start)\nall this should go\n. \"assert y_test_positive+y_test_negative == len(y_test)\"\nplease no assertions in notebooks. This is interactive with the reader, just print whats going on. Tell the reader why you do things. The python code is not that easy to read (see above comment about splitting this)\n. \"assert prb_idx>1e-15\"\nwhats this?\n. \"Note that the x axis and y axis in these figures are the range of parameter of covariance function. The information bit is used to measure the accuracy of classification.\"\nNice plots!\n- axis labels!\n- what can we see?\n- structure this more\n- is there a ranking of the methods? (which one is best?)\n. \"Another toy example to demonstrate the usage of variational methods in Shogun\"\ncan you please be a bit more creative with your section headers? :) \n. \"which is used in the Gaussian Processes for Machine Learning textbook.\"\nwhich chapter? section?\n. \"def binary_extract(idx, features, labels):\"\nthis cell is also too big -- split it! Tell the reader whats going on\n. \"Training set statistics\npositive samples 406 negative samples 361 total samples 767\nTest set statistics\npositive samples 418 negative samples 355 total samples 773\"\nPlease do some more pretty formatting here: consistent capital letters at sentence beginnings, use colons, use full sentences\n. \"Laplace Method in GPC\u00b6\"\nplease think of a nicer section title. Also please again not linebreaks all the time in this section\n. \"we can show that the objective function, ln(p(f|y)),\"\n\\ln\n. \"cost 0.998830795288 seconds at training\"\nprint seconds using %.2f\n. \"Covariance variational method in GPC\"\nI stop here for now, please make sure:\n- That you resolve every of the things I mentioned\n- For each of them, please scan through the rest of the notebook and make sure that you resolve the things everywhere, so that I dont have to comment similar things again when I finish reviewing.\nI really like where the notebook is going and I think this will look absolutely great. However, it is far from being finished yet. This is partly to me and I am sorry for not reviewing it properly earlier. I hope we can get this finished this week even though gsoc is over now. Let's keep the momentum up :)\n. BTW the multiclass laplace approximation could go into the other GP notebook\n. @yorkerlin ok, good luck with that.\nWe plan a release very soon, and I would love to see the variational notebook in there - let's try to make this happen.\n. How long do they run now? We can only justify a few minutes for a notebook. You can maybe reduce the number of function evaluations and mention that the plots look the same when increased in the description\n. @emtiyaz I seond this idea\n. @yorkerlin nice thanks. It would be great if you focussed on the notebook before starting other things\n@emtiyaz @yorkerlin We might have a student doing a Master's project in Shogun, maybe he could do some GP implementatoin? What would suit a MSc project well? He could work with @yorkerlin in some way.\n@yorkerlin in Shogun, we have the FITC regression with pseudo inputs, thats all\n. Hey @yorkerlin \nNice thanks for the updates. I just had a look and a lot of things @emtiyaz and me mentioned are still unresolved. To avoid us all spending a lot of time doing the same thing again, could you address all suggestions we pointed out? Then it is way easier for us to do another reviewing iteration. But already looks better, lets keep this going to get it finished!\n. Thanks for the update! I will have a close look very soon!\n. Ok here are finally some suggestions for the notebook.\nCould you update the PR with the latest version? Lets aim to get this merged before the release.\n. \"This notebook describes how to do vvariational inference\"\ndouble v\n. \"We assume reader has some background in Bayesian statistics. For background in Bayesian statistics, please see the notebook about Gaussian Process.\nAfter providing a semi-formal introduction, we illustrate how to do training, make predictions, and automatically learn hyper-parameters for GPs in Shogun\"\nFormatting: Could you make those two paragraphs one single paragraph?\n. \" In other words, we want to learn posterior, p(f|y,X)\u221dp(y|f,X)\u2217p(f|GP), given training data points\"\nplease dont use a * start, but rather latex \\cdot to dentote multiplication, the * is usually used for convolution\n. What do you mena by p(f|GP)\nThis is just p(f) - maybe say that this is the prior over the GP functions\n. \"Informally, according to GP the following holds\"\naccodring to GP is not correct grammar, could you be more precise here?\n. \"is followed by finite-dimensional Gaussian distribution,\"\nalso not correct grammar: is followed by\nRather say, follows from the conditioning properties of jointly Gaussian variables\n. \"If we minimizing KL(P\u2225Q) instead of KL(Q\u2225P), it is about inference using expectation propagation (EP) in GPC.\"\nIf we minimize the KL(P\u2225Q) instead of KL(Q\u2225P), this is called Expectation Propagation (EP).\n. where Eq(\u22c5) is the expectation with respect to f.\nits \u1e83ith respect to q(f) in fact\n. BTW could you introduce the GPC acronym when you mention it the first time\nLike \"In Gaussian Process Classification (GPC) the goal is so, blablalba\"\n. \"Dealing with the non-closed form issue in GPC\"\nCan you mention that you can decompose the likelihood term because we assume independent observations?\n. \"and we will briefly discuss how to approximate it in later session.\"\nchange to \"later in the notebook\"\n. \"Remark: The true posterior is non-Gaussian\"\nIn this paragraph, can you link all the inference methods to the Shogun documentation?\nAlso please note that you never mentioned any of the classes before. What about having a little table of interence methods at the start, giving a short summary for each?\n. These plots of the posterior are great! However, there is too much code in front of them. I suggest you loosen this a bit up with some text. Try to make the cells shorter and say in the text what they are supposed to do. Then go through the different posterior plots one by one, and add a sentence for each plot so that you have text-plot-text-plot-text-plot ......\n. \"A real-world example\"\nThis paragraph is great, but you should say what you are doing before you do it. Give a short summary and say that you will compare all inference methods (from the above table) and plot the likelihood.\nAlso, avoid putting many cells after one another. Put the first cell, write a bit about it (even if just a helper method), then next cell, write about it, then do all the plots seperately again, and analyise each of them\n. \"Another real-world example\" same here, a little more explanation would be good, not a lot though, just a bit :)\n. \"This method is mentioned in the paper of Nickisch and Rasmussen in 2008,\"\nCould you put proper scientific reference at the end too? would be very helpful\n. \"\nRemark: This method may be the first variational method used in GPC. However, it is the slowest method among all implemented variational methods in Shogun in practice.\"\nWhat is good about it? You should say this\n. I will try to hang our in IRC a bit this week, so then we can talk.\nThe Stammtisch on Monday should happen, I will send an email\n. Hi @lambday \nsorry for the delayed response. I was on holiday :)\nI will read this shortly and try to give some feedback. We still have to finish this paper!\n. I agree with @pickle27 please remove for now.\nThe silent failure does not work if you do an exit(1), as this quits the Python interpreter and returns an error code. I am not away of IPython notebook features that allow conditional execution of the rest of the notebook if a cell fail - so remove for now, and put it into a gist for now, and link it to the tutorial you wrote, preferably in doxygen.\nThis one is release critical\n. This post is off-topic, please delete it and move to a different thread. We will discuss there\n. This post is off-topic, please delete it and move to a different thread. We will discuss there\n. Thats bad, precise is still widely used. Shall we drop support? But what can we do....\n. Do you know the source of the problem? What about renaming things that are exposed via modular?\n. All GSoC students, please help here. All others too\n@yorkerlin @mazumdarparijat @kislayabhi @khalednasr @Saurabh7 @abinashpanda @Jiaolong @lambday \n. Hi @kartiksomani \nsorry for the delayed reply. Yes the isse is still open, and we need help to fix it for the upcoming release. So pick any of the warnings in the buildbot logs and address them. Let me know if you have any questions on how to do this\n. Hi @kartiksomani \nsorry for the delayed reply. Yes the isse is still open, and we need help to fix it for the upcoming release. So pick any of the warnings in the buildbot logs and address them. Let me know if you have any questions on how to do this\n. done in #2583\n. What about linking this here? (replacing)\nhttp://www.shogun-toolbox.org/page/about/information\n. thanks fernando!\n. thanks fernando!\n. @lambday ? Just a quick summary?\n. It would be optimal if everyone would write one, I can collect them in a big Shogun post, and link to all yours. Should not be too hard and should not take more than an hour to write......\n. Go for it. @lambday will guide you.\nFirst step: Linear solves\n- Direct solve (A*x =b ), similar to numpy.lingalg.solve\n- Solve via cholesky, triangular solve, pseudo inverse, see also here\nSecond step: Factorisations\n- The three mentioned above, plus eigendecomposition\n- Also offer solve methods given the factorisation\nThird:\n- backends: eigen3, lapack, (native?? though not sure here)\n. @yingryic \nHi, cool that you are getting involved here.\nLet's maybe start with a plain linear solve? I checked what numpy does:\nhttp://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.solve.html\nhttp://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.lstsq.html#numpy.linalg.lstsq\nFirst wraps LAPACK's _gesv, i.e. assuming full rank and then solving exactly\nSecond does least squares.\nIf we had those, we can already replace a lot of things within Shogun (we can also justify a native implementation for them, there is a Gauss-Seidel implementation in KRR I think, you can copy that over if not some other class, search for it)\nWe need first to define the interface, and then provide all backend implementations. In particular useful will be to have viennacl here, see for example\nhttp://viennacl.sourceforge.net/viennacl-examples-dense-matrix.html\nOnce this is done, we want solvers like solve_triangular, solve_cholesky, solve_qr, solve_svd which do the job based on factorisations that one computed before. (Also need methods for that first)\nhttp://docs.scipy.org/doc/scipy/reference/generated/scipy.linalg.cho_solve.html\nhttp://docs.scipy.org/doc/scipy/reference/generated/scipy.linalg.lu_solve.html#scipy.linalg.lu_solve\nhttp://math.stackexchange.com/questions/109329/can-qr-decomposition-be-used-for-matrix-inversion\nKeep in mind this linalg stuff is about interface first, and the providing implementations for the methods in various backends (native/eigen/lapack)\n. @yingryic \nHi, cool that you are getting involved here.\nLet's maybe start with a plain linear solve? I checked what numpy does:\nhttp://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.solve.html\nhttp://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.lstsq.html#numpy.linalg.lstsq\nFirst wraps LAPACK's _gesv, i.e. assuming full rank and then solving exactly\nSecond does least squares.\nIf we had those, we can already replace a lot of things within Shogun (we can also justify a native implementation for them, there is a Gauss-Seidel implementation in KRR I think, you can copy that over if not some other class, search for it)\nWe need first to define the interface, and then provide all backend implementations. In particular useful will be to have viennacl here, see for example\nhttp://viennacl.sourceforge.net/viennacl-examples-dense-matrix.html\nOnce this is done, we want solvers like solve_triangular, solve_cholesky, solve_qr, solve_svd which do the job based on factorisations that one computed before. (Also need methods for that first)\nhttp://docs.scipy.org/doc/scipy/reference/generated/scipy.linalg.cho_solve.html\nhttp://docs.scipy.org/doc/scipy/reference/generated/scipy.linalg.lu_solve.html#scipy.linalg.lu_solve\nhttp://math.stackexchange.com/questions/109329/can-qr-decomposition-be-used-for-matrix-inversion\nKeep in mind this linalg stuff is about interface first, and the providing implementations for the methods in various backends (native/eigen/lapack)\n. @sudk1896 I don't think it is a good idea to get involved here. We will merge linalg soon and then there will be tons of stuff (easy) to do, but currently things a bit more complicated and not really good for getting involved in Shogun\n. SVD returns three objects, U, V, and a vector S with the singular values.\nNeed to think about the interface.\nI guess there can be two interfaces\n\na solve method that just uses the specified  method and matrix to solve a linear system (no factorization returned)\na factor method that turns a matrix into multiple factors (different for every method)\na solve method that takes the factors (or just a few of them), and then behaves like the first solve method, but with precomputed factorizations\nThe helper methods (solve_triagnular, etc) should also be available in linalg btw. E.g. for Cholesky you use a traingular solve inside, but maybe somebody also wants to use this expicitly. Will be the same for qr. Should go into the project.\nOnly Cholesky is in yet, we definitely need the others as well. You could open a new one for every solver type and reference this one to \"advertise\" it. Great, thanks!\n\n@sejdino does this fix the problems you had?\n. Yeah, I like this idea of having kernels for dot-prodct features, which then support dense and sparse operations.\n. Fine to merge. Comments are cosmetics\n. In an ideal world, things like this should be checked via unit tests, but I see that this is more work that just putting the assert. Your decision.\nWhat does ordering features per relevance give us? But yeah, maybe keep that in. So fine to merge then :)\n. I am merging this, as I think its fine. Let me know if you have concerns\n. ### Contact\nEmail: heiko.stratmann@gmail.com\nWebsite: http://www.herrstrathmann.de\nirc: HeikoS\nPersonal\nOccupation: PhD student at Gatsby Computional Neuroscience Unit, UCL\nInterests: Machine Learning, Computational Statistics, Neuroscience, Open-Source Software, Coffee Climbing, Jazz, Maths\nShogun\nHeiko initially got involved in Shogun by using it for his undergraduate thesis, and then later as a student in GSoC 2011 and GSoC 2012. From initial mostly code contributions, he then transitioned into organising the project as well as mentoring/admin work for GSoC (2013, 2014). Heiko now mostly thinks about where Shogun move next, presents it at and organises workshops, and reviews code of GSoC students. Along with S\u00f6ren, he is also voted head of the Shogun foundation.\nAsk him about\n\nShogun's deep internals, such as the parameter framework\nCross-validation and model-selection\nKernel embeddings statistics (MMD, HSIC, etc)\nGaussian processes\n. I'm closing this as its moved to\nhttps://github.com/shogun-toolbox/shogun/wiki/Developer_profiles\n\nPlease populate your own profile.\n@iglesias @pickle27 could we produce a view to the main website from this wiki page?\n. Issues that touch this:\n2181 #2136 #2123 #2000\n. This is nontrivial, please only attempt if you have an idea what you are doing.\nGoogling libraries for this is the first step. Note that we want just memory management of such structures, math operations will be done via linalg. @votjakovr @yorkerlin do you have an idea what causes this?\n. The clone was introduced by @votjakovr (to avoid some double training in GPs)\n@yorkerlin thanks for isolating it. Does the error also appear when one just clones a kernel without any GP class involved?\n. Thanks for pinging, no updates yet, but we definitely want to solve this. I might have some time next week, and bugfixing has priority since we plan a release\n. This kernel is quite old, I dont even know who wrote it. \nFeel free to send a patch with your findings corrected (code is probably fine, but documentation might need udpates)\nThanks!\n. I like the idea. How would this look like?\nThe reason for all the SG_ERROR etc is that we are lazy people who do not like to introduce new abstract methods and then go through the pain of implementing in all the sub-classes. ;)\n. @lambday could you put this discussion as a topic in the hack wiki? https://github.com/shogun-toolbox/shogun/wiki/Roadmap-Shogun-2015-hack\n. ah, sorry I just realised that I actually knew about this, (I had asked roman to implement this a while ago as I needed it for computing importance sample estimate of the marginal likelihood of GPs.\n. ah, sorry I just realised that I actually knew about this, (I had asked roman to implement this a while ago as I needed it for computing importance sample estimate of the marginal likelihood of GPs.\n. @jondo do you use windows? We lack of someone who is able to build such a thing.\n. @jondo do you use windows? We lack of someone who is able to build such a thing.\n. Ok got it\n. I think it is possible if you know what you are doing and are ready to spend a few nights hacking cmake. Otherwise, sadly, no. We want to change this but the priority compared to other features is dropping .... sorry\n. The first big step would be to build libshogun without any plugins. THAT would already be giant step forward. Interested? We can definitely help!\n. - channel of communication: irc and github\n- way of sharing information, github gist\n- yeah fork shogun, and send pull requests against our develop branch, we can then discuss your suggestions there\n- @vigsterkr might have some comments on the particular errors you have there\n- messed up path  names should be easy to fix actually\n. I think we had a cygwin build running previously http://buildbot.shogun-toolbox.org/waterfall\nI did not do that, but @sonney2k maybe he has ideas.\nCome say hello in irc\n. it will stay in the history anyways.\nBut I think @sonney2k should comment.\nFrom side, just drop it\nThanks for telling us @jondo \n. it will stay in the history anyways.\nBut I think @sonney2k should comment.\nFrom side, just drop it\nThanks for telling us @jondo \n. Do you want to fix them? :)\n. Do you want to fix them? :)\n. @sonney2k @lisitsyn @vigsterkr @ratsch  if you have no objections, I will delete this. We cannot maintain such things anyway.\n. great, if you do this, we can keep it - though I think it should be in a seperate repo against a released version of Shogun with its own tests etc \n. I have no idea, but i guess either @lisitsyn or @sonney2k know\n. I have no idea, but i guess either @lisitsyn or @sonney2k know\n. @besser82 could you take care of this?  Its release critical\n. @vigsterkr @sonney2k could you check?\nSame goes for class list\nhttp://www.shogun-toolbox.org/doc/en/latest/classshogun_1_1CSVM.html\n. This is a must have for release\n. @c3h3 Great to hear these things!\n@vigsterkr whats the status of the nightly docker build? \n. There are like 3 different topics in this thread :)\n. There are like 3 different topics in this thread :)\n. This is amazing!\nCould you tell us some more about this event, and what exactly you did\n@iglesias your notebook is featured :)\n@tklein23 @lisitsyn @sonney2k @vigsterkr \nwe should somehow link this to our website, twitter and blog....\n. great thanks!\n. @tklein23 could you post a shell magic script for that here? something that output the filenames of header files that include something else than <shogun/ ..... >\n. I think finding non Shogun includes in all header files should be a short one liner in bash.....\n@iglesias @tklein23 will produce this in like 10 secs :)\n. Here you go\nfind . -name '*.h' | xargs grep '#include'  | uniq  | grep -v '<shogun/'\nproduces a lot of hints. Not all of those need to be remove, but most I would say.\nYou dont look at the lib things for now, but start with more obvious things such as classifiers. For sure, none of the algorithm classes would have a non-shogun header included. Let us know how things go\nOh and very important, please do one PR per class. Dont mix up too many things, we want travis to make sure everything compiles before merging such things. Run all tests locally before sending PRs\n./lib/slep/tree/altra.h:#include <stdio.h>\n./lib/slep/tree/altra.h:#include <stdlib.h>\n./lib/slep/tree/altra.h:#include <math.h>\n./lib/slep/tree/altra.h:#include <string.h>\n./lib/slep/tree/general_altra.h:#include <stdio.h>\n./lib/slep/tree/general_altra.h:#include <stdlib.h>\n./lib/slep/tree/general_altra.h:#include <math.h>\n./lib/slep/tree/general_altra.h:#include <string.h>\n./lib/slep/overlapping/overlapping.h:#include <stdio.h>\n./lib/slep/overlapping/overlapping.h:#include <stdlib.h>\n./lib/slep/overlapping/overlapping.h:#include <math.h>\n./lib/slep/overlapping/overlapping.h:#include <string.h>\n./lib/slep/slep_options.h:#include <stdlib.h>\n./lib/slep/order/orderTree.h:#include <stdlib.h>\n./lib/slep/order/orderTree.h:#include <stdio.h>\n./lib/slep/order/orderTree.h:#include <time.h>\n./lib/slep/order/orderTree.h:#include <math.h>\n./lib/slep/order/sequence.h:#include <stdlib.h>\n./lib/slep/order/sequence.h:#include <stdio.h>\n./lib/slep/order/sequence.h:#include <time.h>\n./lib/slep/order/sequence.h:#include <math.h>\n./lib/slep/q1/ep21d.h:#include <stdlib.h>\n./lib/slep/q1/ep21d.h:#include <stdio.h>\n./lib/slep/q1/ep21d.h:#include <time.h>\n./lib/slep/q1/ep21d.h:#include <math.h>\n./lib/slep/q1/eppVector.h:#include <stdlib.h>\n./lib/slep/q1/eppVector.h:#include <stdio.h>\n./lib/slep/q1/eppVector.h:#include <time.h>\n./lib/slep/q1/eppVector.h:#include <math.h>\n./lib/slep/q1/ep1R.h:#include <stdlib.h>\n./lib/slep/q1/ep1R.h:#include <stdio.h>\n./lib/slep/q1/ep1R.h:#include <time.h>\n./lib/slep/q1/ep1R.h:#include <math.h>\n./lib/slep/q1/epsp.h:#include <stdlib.h>\n./lib/slep/q1/epsp.h:#include <stdio.h>\n./lib/slep/q1/epsp.h:#include <time.h>\n./lib/slep/q1/epsp.h:#include <math.h>\n./lib/slep/q1/epsgLasso.h:#include <stdlib.h>\n./lib/slep/q1/epsgLasso.h:#include <stdio.h>\n./lib/slep/q1/epsgLasso.h:#include <time.h>\n./lib/slep/q1/epsgLasso.h:#include <math.h>\n./lib/slep/q1/eppVectorR.h:#include <stdlib.h>\n./lib/slep/q1/eppVectorR.h:#include <stdio.h>\n./lib/slep/q1/eppVectorR.h:#include <time.h>\n./lib/slep/q1/eppVectorR.h:#include <math.h>\n./lib/slep/q1/ep21R.h:#include <stdlib.h>\n./lib/slep/q1/ep21R.h:#include <stdio.h>\n./lib/slep/q1/ep21R.h:#include <time.h>\n./lib/slep/q1/ep21R.h:#include <math.h>\n./lib/slep/SpInvCoVa/invCov.h:#include <stdlib.h>\n./lib/slep/SpInvCoVa/invCov.h:#include <stdio.h>\n./lib/slep/SpInvCoVa/invCov.h:#include <time.h>\n./lib/slep/SpInvCoVa/invCov.h:#include <math.h>\n./lib/memory.h:#include <new>\n./lib/OpenCV/SG2CVFactory.h:#include <opencv2/core/core.hpp>\n./lib/OpenCV/CV2SGFactory.h:#include <opencv2/core/core.hpp>\n./lib/OpenCV/OpenCVTypeName.h:#include <opencv2/core/core.hpp>\n./lib/v_array.h:#include <stdlib.h>\n./lib/Set.h:#include <cstdio>\n./lib/RefCount.h:#include <atomic>\n./lib/external/SFMT/SFMT.h:#include <stdio.h>\n./lib/external/SFMT/SFMT.h:#include <assert.h>\n./lib/external/SFMT/SFMT.h:  #include <inttypes.h>\n./lib/external/SFMT/SFMT.h:  #include <emmintrin.h>\n./lib/external/PMurHash.h:  #include <stdint.h>\n./lib/external/PMurHash.h:  #include  <limits.h>\n./lib/external/libqp.h:#include <math.h>\n./lib/external/brent.h:#include <vector>\n./lib/external/dSFMT/dSFMT.h:#include <stdio.h>\n./lib/external/dSFMT/dSFMT.h:#include <assert.h>\n./lib/common.h:#include <stdlib.h>\n./lib/common.h:#include <stdio.h>\n./lib/common.h:#include <sys/types.h>\n./lib/common.h:#include <stdint.h>\n./lib/common.h:#include <inttypes.h>\n./lib/common.h:#include <complex>\n./lib/GPUVector.h:#include <memory>\n./lib/Time.h:#include <sys/time.h>\n./lib/Time.h:#include <time.h>\n./lib/JLCoverTree.h:#include<math.h>\n./lib/JLCoverTree.h:#include<stdio.h>\n./lib/JLCoverTree.h:#include<assert.h>\n./lib/malsar/malsar_options.h:#include <stdlib.h>\n./lib/GPUMatrix.h:#include <memory>\n./lib/Signal.h:#include <signal.h>\n./lib/Trie.h:#include <string.h>\n./lib/Cache.h:#include <stdlib.h>\n./lib/Map.h:#include <cstdio>\n./base/init.h:#include <stdio.h>\n./loss/LossFunction.h:#include <math.h>\n./transfer/domain_adaptation/DomainAdaptationSVMLinear.h:#include <stdio.h>\n./transfer/domain_adaptation/DomainAdaptationSVM.h:#include <stdio.h>\n./transfer/multitask/MultitaskKernelPlifNormalizer.h:#include <algorithm>\n./transfer/multitask/MultitaskKernelPlifNormalizer.h:#include <vector>\n./transfer/multitask/MultitaskKernelTreeNormalizer.h:#include <algorithm>\n./transfer/multitask/MultitaskKernelTreeNormalizer.h:#include <map>\n./transfer/multitask/MultitaskKernelTreeNormalizer.h:#include <set>\n./transfer/multitask/MultitaskKernelTreeNormalizer.h:#include <deque>\n./transfer/multitask/MultitaskKernelTreeNormalizer.h:#include <vector>\n./transfer/multitask/MultitaskKernelNormalizer.h:#include <algorithm>\n./transfer/multitask/MultitaskKernelNormalizer.h:#include <vector>\n./transfer/multitask/MultitaskLinearMachine.h:#include <vector>\n./transfer/multitask/MultitaskLinearMachine.h:#include <set>\n./transfer/multitask/MultitaskKernelMaskNormalizer.h:#include <set>\n./transfer/multitask/MultitaskKernelMaskNormalizer.h:#include <string>\n./transfer/multitask/MultitaskKernelMaskNormalizer.h:#include <vector>\n./transfer/multitask/MultitaskKernelMklNormalizer.h:#include <algorithm>\n./transfer/multitask/MultitaskKernelMklNormalizer.h:#include <string>\n./transfer/multitask/MultitaskLogisticRegression.h:#include <vector>\n./transfer/multitask/MultitaskLogisticRegression.h:#include <set>\n./transfer/multitask/LibLinearMTL.h:#include <map>\n./transfer/multitask/MultitaskKernelMaskPairNormalizer.h:#include <string>\n./transfer/multitask/MultitaskKernelMaskPairNormalizer.h:#include <vector>\n./transfer/multitask/MultitaskKernelMaskPairNormalizer.h:#include <utility>\n./evaluation/ClusteringEvaluation.h:#include <vector>\n./distributions/HMM.h:#include <stdio.h>\n./machine/LinearMachine.h:#include <stdio.h>\n./machine/KernelMachine.h:#include <stdio.h>\n./machine/OnlineLinearMachine.h:#include <stdio.h>\n./machine/DistanceMachine.h:#include <stdio.h>\n./optimization/liblinear/shogun_liblinear.h:#include <vector>\n./regression/svr/LibSVR.h:#include <stdio.h>\n./regression/LeastAngleRegression.h:#include <vector>\n./mathematics/eigen3.h: #include <Eigen/Eigen>\n./mathematics/eigen3.h: #include <Eigen/Dense>\n./mathematics/eigen3.h:     #include <Eigen/Sparse>\n./mathematics/eigen3.h:     #include <unsupported/Eigen/SparseExtra>\n./mathematics/linalg/internal/opencl_util.h:#include <viennacl/ocl/backend.hpp>\n./mathematics/linalg/internal/opencl_util.h:#include <viennacl/ocl/kernel.hpp>\n./mathematics/linalg/internal/opencl_util.h:#include <viennacl/ocl/program.hpp>\n./mathematics/linalg/internal/opencl_util.h:#include <viennacl/ocl/utils.hpp>\n./mathematics/linalg/internal/opencl_util.h:#include <viennacl/tools/tools.hpp>\n./mathematics/linalg/internal/opencl_util.h:#include <string>\n./mathematics/linalg/internal/implementation/Sum.h:#include <string>\n./mathematics/linalg/internal/implementation/Max.h:#include <string>\n./mathematics/linalg/internal/implementation/MatrixProduct.h:#include <viennacl/linalg/prod.hpp>\n./mathematics/linalg/internal/implementation/MatrixProduct.h:#include <viennacl/matrix.hpp>\n./mathematics/linalg/internal/implementation/Add.h:#include <viennacl/linalg/matrix_operations.hpp>\n./mathematics/linalg/internal/implementation/Add.h:#include <viennacl/linalg/vector_operations.hpp>\n./mathematics/linalg/internal/implementation/ElementwiseProduct.h:#include <viennacl/linalg/matrix_operations.hpp>\n./mathematics/linalg/internal/implementation/Scale.h:#include <viennacl/linalg/matrix_operations.hpp>\n./mathematics/linalg/internal/implementation/Scale.h:#include <viennacl/linalg/vector_operations.hpp>\n./mathematics/linalg/internal/implementation/Dot.h:#include <viennacl/linalg/inner_prod.hpp>\n./mathematics/Statistics.h:#include <math.h>\n./mathematics/lapack.h:#include <mkl_cblas.h>\n./mathematics/lapack.h:#include <mkl_lapack.h>\n./mathematics/lapack.h:#include <Accelerate/Accelerate.h>\n./mathematics/lapack.h:#include <cblas.h>\n./mathematics/lapack.h:#include <acml.h>\n./mathematics/lapack.h:#include <clapack.h>\n./mathematics/Random.h:#include <limits>\n./mathematics/Math.h:#include <math.h>\n./mathematics/Math.h:#include <stdio.h>\n./mathematics/Math.h:#include <float.h>\n./mathematics/Math.h:#include <sys/types.h>\n./mathematics/Math.h:#include <unistd.h>\n./mathematics/Math.h:#include <pthread.h>\n./mathematics/Math.h:#include <ieeefp.h>\n./mathematics/munkres.h:#include <list>\n./mathematics/munkres.h:#include <utility>\n./mathematics/Mosek.h:#include <mosek.h>\n./mathematics/Cplex.h:#include <ilcplex/cplex.h>\n./mathematics/JacobiEllipticFunctions.h:#include <limits>\n./mathematics/JacobiEllipticFunctions.h:#include <math.h>\n./mathematics/JacobiEllipticFunctions.h:#include <arprec/mp_real.h>\n./mathematics/JacobiEllipticFunctions.h:#include <arprec/mp_complex.h>\n./preprocessor/PNorm.h:#include <stdio.h>\n./preprocessor/HomogeneousKernelMap.h:#include <stdio.h>\n./preprocessor/LogPlusOne.h:#include <stdio.h>\n./preprocessor/SortUlongString.h:#include <stdio.h>\n./preprocessor/PruneVarSubMean.h:#include <stdio.h>\n./preprocessor/SparsePreprocessor.h:#include <stdio.h>\n./preprocessor/SumOne.h:#include <stdio.h>\n./preprocessor/SortWordString.h:#include <stdio.h>\n./preprocessor/NormOne.h:#include <stdio.h>\n./preprocessor/RandomFourierGaussPreproc.h:#include <vector>\n./preprocessor/RandomFourierGaussPreproc.h:#include <algorithm>\n./kernel/string/SpectrumMismatchRBFKernel.h:#include <string>\n./kernel/string/SpectrumMismatchRBFKernel.h:#include <vector>\n./kernel/string/OligoStringKernel.h:#include <vector>\n./kernel/string/OligoStringKernel.h:#include <string>\n./kernel/string/SpectrumRBFKernel.h:#include <vector> // profile\n./kernel/string/SpectrumRBFKernel.h:#include <string> // profile\n./clustering/Hierarchical.h:#include <stdio.h>\n./clustering/KMeans.h:#include <stdio.h>\n./clustering/GMM.h:#include <vector>\n./multiclass/KNN.h:#include <stdio.h>\n./multiclass/GMNPLib.h:#include <math.h>\n./multiclass/GMNPLib.h:#include <limits.h>\n./multiclass/ecoc/ECOCDiscriminantEncoder.h:#include <vector>\n./multiclass/ecoc/ECOCDiscriminantEncoder.h:#include <set>\n./multiclass/tree/ConditionalProbabilityTree.h:#include <map>\n./multiclass/tree/RelaxedTree.h:#include <utility>\n./multiclass/tree/RelaxedTree.h:#include <vector>\n./multiclass/tree/VwConditionalProbabilityTree.h:#include <map>\n./multiclass/LaRank.h:#include <ctime>\n./multiclass/LaRank.h:#include <vector>\n./multiclass/LaRank.h:#include <algorithm>\n./multiclass/LaRank.h:#include <sys/time.h>\n./multiclass/LaRank.h:#include <set>\n./multiclass/LaRank.h:#include <map>\n./multiclass/ScatterSVM.h:#include <stdio.h>\n./neuralnets/DeepBeliefNetwork.h:#include <lib/SGMatrixList.h>\n./io/ProtobufFile.h:#include <google/protobuf/message.h>\n./io/SerializableXmlFile.h:#include <libxml/parser.h>\n./io/SerializableXmlFile.h:#include <libxml/tree.h>\n./io/LineReader.h:#include <stdio.h>\n./io/MLDataHDF5File.h:#include <hdf5.h>\n./io/SerializableFile.h:#include <stdio.h>\n./io/streaming/ParseBuffer.h:#include <pthread.h>\n./io/streaming/InputParser.h:#include <pthread.h>\n./io/SimpleFile.h:#include <stdio.h>\n./io/SimpleFile.h:#include <string.h>\n./io/SimpleFile.h:#include <sys/mman.h>\n./io/BinaryStream.h:#include <stdio.h>\n./io/BinaryStream.h:#include <sys/stat.h>\n./io/SGIO.h:#include <stdio.h>\n./io/SGIO.h:#include <string.h>\n./io/SGIO.h:#include <dirent.h>\n./io/SGIO.h:#include <locale.h>\n./io/SGIO.h:#include <sys/types.h>\n./io/SGIO.h:#include <unistd.h>\n./io/SGIO.h:#include <Availability.h>\n./io/SerializableHdf5File.h:#include <hdf5.h>\n./io/HDF5File.h:#include <hdf5.h>\n./io/MemoryMappedFile.h:#include <stdio.h>\n./io/MemoryMappedFile.h:#include <string.h>\n./io/MemoryMappedFile.h:#include <sys/mman.h>\n./io/MemoryMappedFile.h:#include <sys/stat.h>\n./io/MemoryMappedFile.h:#include <sys/types.h>\n./io/MemoryMappedFile.h:#include <fcntl.h>\n./io/MemoryMappedFile.h:#include <unistd.h>\n./io/protobuf/ShogunVersion.pb.h:#include <string>\n./io/protobuf/ShogunVersion.pb.h:#include <google/protobuf/stubs/common.h>\n./io/protobuf/ShogunVersion.pb.h:#include <google/protobuf/generated_message_util.h>\n./io/protobuf/ShogunVersion.pb.h:#include <google/protobuf/message.h>\n./io/protobuf/ShogunVersion.pb.h:#include <google/protobuf/repeated_field.h>\n./io/protobuf/ShogunVersion.pb.h:#include <google/protobuf/extension_set.h>\n./io/protobuf/ShogunVersion.pb.h:#include <google/protobuf/generated_enum_reflection.h>\n./io/protobuf/ShogunVersion.pb.h:#include <google/protobuf/unknown_field_set.h>\n./io/protobuf/Chunks.pb.h:#include <string>\n./io/protobuf/Chunks.pb.h:#include <google/protobuf/stubs/common.h>\n./io/protobuf/Chunks.pb.h:#include <google/protobuf/generated_message_util.h>\n./io/protobuf/Chunks.pb.h:#include <google/protobuf/message.h>\n./io/protobuf/Chunks.pb.h:#include <google/protobuf/repeated_field.h>\n./io/protobuf/Chunks.pb.h:#include <google/protobuf/extension_set.h>\n./io/protobuf/Chunks.pb.h:#include <google/protobuf/unknown_field_set.h>\n./io/protobuf/Headers.pb.h:#include <string>\n./io/protobuf/Headers.pb.h:#include <google/protobuf/stubs/common.h>\n./io/protobuf/Headers.pb.h:#include <google/protobuf/generated_message_util.h>\n./io/protobuf/Headers.pb.h:#include <google/protobuf/message.h>\n./io/protobuf/Headers.pb.h:#include <google/protobuf/repeated_field.h>\n./io/protobuf/Headers.pb.h:#include <google/protobuf/extension_set.h>\n./io/protobuf/Headers.pb.h:#include <google/protobuf/unknown_field_set.h>\n./io/SerializableJsonFile.h:#include <json.h>\n./metric/LMNNImpl.h:#include <Eigen/Dense>\n./metric/LMNNImpl.h:#include <set>\n./metric/LMNNImpl.h:#include <vector>\n./distance/Distance.h:#include <stdio.h>\n./structure/CCSOSVM.h:#include <mosek.h>\n./structure/DynProg.h:#include <stdio.h>\n./structure/DynProg.h:#include <limits.h>\n./structure/BeliefPropagation.h:#include <vector>\n./structure/BeliefPropagation.h:#include <set>\n./structure/BeliefPropagation.h:    #include <unordered_map>\n./structure/BeliefPropagation.h:    #include <tr1/unordered_map>\n./classifier/NearestCentroid.h:#include <stdio.h>\n./classifier/AveragedPerceptron.h:#include <stdio.h>\n./classifier/LPM.h:#include <stdio.h>\n./classifier/svm/GNPPLib.h:#include <math.h>\n./classifier/svm/GNPPLib.h:#include <limits.h>\n./classifier/svm/SVMLight.h:#include <stdio.h>\n./classifier/svm/SVMLight.h:#include <ctype.h>\n./classifier/svm/SVMLight.h:#include <string.h>\n./classifier/svm/SVMLight.h:#include <stdlib.h>\n./classifier/svm/SVMLight.h:#include <time.h>\n./classifier/svm/QPBSVMLib.h:#include <math.h>\n./classifier/svm/QPBSVMLib.h:#include <limits.h>\n./classifier/svm/GPBTSVM.h:#include <stdio.h>\n./classifier/svm/LibSVMOneClass.h:#include <stdio.h>\n./classifier/LPBoost.h:#include <stdio.h>\n./classifier/mkl/MKLMulticlass.h:#include <vector>\n./classifier/mkl/MKLMulticlassGLPK.h:#include <vector>\n./classifier/mkl/MKLMulticlassGradient.h:#include <vector>\n./classifier/mkl/MKLMulticlassGradient.h:#include <cmath>\n./classifier/mkl/MKLMulticlassGradient.h:#include <cassert>\n./classifier/mkl/MKL.h:#include <glpk.h>\n./classifier/mkl/MKL.h:#include <ilcplex/cplex.h>\n./classifier/mkl/MKLMulticlassOptimizationBase.h:#include <vector>\n./classifier/Perceptron.h:#include <stdio.h>\n. You have to do this one by one, not all of them at once. And then compile after each change.\n. The error message should help you solve the problem. What is it exactly? Are there any vectors in the interfaces?\n. Try forward declaring vector\n. just grep for includes and change all classes in that folder\n. some of @lisitsyn s stuff is header only I think.... Sergey?\n. yeah go for it :)\nAh wait, but lib is somehow different as there might be external libraries in there. Make sure to focus on those classes that are exposed via SWIG first\n. @mazumdarparijat this is yours! :)\n@iglesias can maybe help?\n. I think we should have all methods that are shared in the base class and subclasses only differ in a couple of lines how to compute the udpates\n. I think we should have all methods that are shared in the base class and subclasses only differ in a couple of lines how to compute the udpates\n. I mean that I do not like the idea of having this factory pattern, a single class should have a fixed behaviour. Different behaviours/algorithms should be implemented in sub-classes.\nPut all functionality that is used by both KMeans algorithms into the base class.\nHave the subclass implement the updates only, using the base class functionality.\n. I mean that I do not like the idea of having this factory pattern, a single class should have a fixed behaviour. Different behaviours/algorithms should be implemented in sub-classes.\nPut all functionality that is used by both KMeans algorithms into the base class.\nHave the subclass implement the updates only, using the base class functionality.\n. great ! \n. I love this idea, but SWIG will not compile faster, see https://github.com/shogun-toolbox/shogun/wiki/SWIG-issues\nMany other things will improve through this though.\n. why does this compile faster?\n. +1 from my side to all this\n. Merging since this works. \n. Also CStreamingFile, same problem\n. Even better: delete this crap and use a library to read data files\n. indeed, this this is our face to the world. great @lambday  :)\n. @lambday cool! But dont get to stressed over it!\nKeep in mind, we just want a simple example that showcases your GSoC work. A small example is better than no example. It would be a pitty if this was not in the release!\n. thanks!\n. Scary patch!\nLets monitor the valgrind output of buildbot\n. I think those things might have to be hand-crafted since they are needed for all modular languages.\nSo either we use a c++ unit test to test the stuff that is given to SWIG (no new framework needed) Or (and maybe even better), we write one file in the target language that asserts correctness of typemaps via some simple method calls that just return the identity, i.e. something like this.\nfrom modshogun import Identity\nmat=np.random.randn(10, 10)\nmat2=Identity.identity_matrix(mat)\nassert(mat.shape=mat2.shape)\nassert(all(mat==mat2))\nA general solution might be more elgant (i.e. doing this in c++ and then just assuming that SWIG handles things correctly if it gets a proper input), what are your thoughts?\n. I agree, this would allows us to save complicated test framework extensions as we can keep everything in the c++ googletest stuff. I might write a prototype soon.\nIn fact, this can be useful when writing the Matlab swig typemaps (see all my emails about matlab and swig)\n. Closing as the meta example tests actually do cover this. +1\n. +1\n. Should be fixed by\nhttps://github.com/shogun-toolbox/shogun/commit/9305aa9d1bab28d20979033b0fd7c8e2d545b85e\n. Should be fixed by\nhttps://github.com/shogun-toolbox/shogun/commit/9305aa9d1bab28d20979033b0fd7c8e2d545b85e\n. Just a quick thought, I think it is good to have things like CMath::sin because it makes us backend independent. However, it should only be accessible from c++, and therefore hidden from SWIG\n. Just a quick thought, I think it is good to have things like CMath::sin because it makes us backend independent. However, it should only be accessible from c++, and therefore hidden from SWIG\n. thats a good point. We for sure rely on c++. \nBut there for sure is a reason why we have those things - might be deprecated by now\n@sonney2k @lisitsyn what are your thoughts?\n. thats a good point. We for sure rely on c++. \nBut there for sure is a reason why we have those things - might be deprecated by now\n@sonney2k @lisitsyn what are your thoughts?\n. Swig and matlab is the way to go, see https://github.com/shogun-toolbox/shogun/wiki/GSoC_2017_project_swig. See also \nhttps://github.com/shogun-toolbox/shogun/wiki/Shogun-slim\n. See also \nhttps://github.com/shogun-toolbox/shogun/wiki/Shogun-slim\n. I think this was fine in fact. Why did you close it?\n. The design pattern of this is horrible. There might also be a way of dealing with this a bit more elgantly, but your patch seems also fine for now\n. Ok I will have a look.\nThe CFile class has all these methods in the interface, which are only overloaded for particular sub-classes. So maybe have different types of CFile classes? Or make the hierarchy slightly more structured. I think every of the implementations only are able to implement one of the blocks of virtual methods (say all dense vectors/matrices ....)\nBut it might be a good idea to also discuss with @vigsterkr \n. Yep that should be good, sorry for long delay\n. This has changed a lot due to the new cookbook. \nEventually we will remove the documentation part of doxygen (apart from maybe class docs), and move to something else. The current version is broken as well\nClosing for now...\n. What would be really cool if we could do all this via the github wiki editor, i.e. edit the doxygen pages that document a complex example from the wiki, and then it is passed to doxygen to render the doxygen main page. That should be possible right?\n. @iglesias all easy: just put all code into a libshogun example, add doxygen markers, and reference from doxygen page. See above example\n. See also https://github.com/karlnapf/shogun_manual\n. We now have the new API cookbook. @abinashmeher999 Not at all! Check the things I listed above and the example patch I referenced\nSorry for late reply\n. @iglesias did you check @lambday 's log-det stuff in your local build? Might contain more of those calls.\nI fully agree with removing them! SGMatrix/Vector should not contain any math code, as we agreed in SF last week!\n. Another nice place might be linalg. But definitely not now and not for those place where you had to change this (all small vectors)\n. Ill merge as its green!\n. @yorkerlin thanks for the patch!\nCould you put another nbviewer link?\n. Ill merge for now as all changes look fine, but I would like to do another full reading some point soon\n. @yorkerlin about the projects. I agree with them, in particular the earlier ones. You are right that they have high priority.\nOnce @lambday has put Cholesky into linalg framework, you can do use it.\nThe Laplace method for FITC would be cool to have too, as currently, we cannot do this.\nThe stochastic variational inference would be amazing too, indeed. Do you have a reference implementation in mind?\n. the doc/md folder is now a submodule, like data. So we can udpate the files from the wiki and then update the version in the shogun repository. \nCool thing is we can reference those files from doxygen, see for example\nhttp://www.shogun-toolbox.org/doc/en/latest/quickstart.html\nThis is great as all non-code soft documentation can be written as MD file and then put in the doxygen and we only have to maintain one version, which is then used for README files, wiki, doxygen -- all the same. Some cleanups needed though. I was about to replace all doxygen sites with readmemd files and migrate the content. Another thing is to put kislays tutorial in doxygen\n. A first step here could be to move all UCI examples in the UCI folder (or starting with a single one), and making sure the build does not break (unit tests, integration tests, notebooks)\n. The data repository can have some more clean-ups. But can also be done later. Let's discuss on mailing list, chat, or see github issue list\n. The data repository can have some more clean-ups. But can also be done later. Let's discuss on mailing list, chat, or see github issue list\n. Note that doxygen now uses the md files from the md submodule\n. Why this? All this can be done with target language interface, say numpy. No need to expose this from Shogun\n. Hi Wu\nthanks a ton for your updates.\nCould you add the usps files to the data repository and send a PR there?\nThen we can merge this one soon\n. Thanks!\nLets see how it goes.\n. Fine to merge from my side, but somebody else should also look\n@iglesias ?\n. Whats the state here?\n. @abhinavagarwalla thanks for taking care\n@iglesias will probably check soon\nsigh, we should really get rid of those horribly written hand-crafted file handlers ..... see https://github.com/shogun-toolbox/shogun/wiki/GSoC_2015_clean_up_infrastructure\nIdeas welcome!\n. @abhinavagarwalla any news on this?\n. news?\n. @sorig Thanks for answering here! :)\nHey, what about updating the QUICKSTART with your additions/details here? Maybe replace your /home/esben with the shogun install dir an mention how to set it.\n. Maybe put the error msg on a pastbin or gist and share, @iglesias is right\n. Thanks for the report!\nCan you please put such long error messages in a gist or pastebin? Otherwise its likely to get \"too long didn't read\". Thanks\n. @svle Please don't get this wrong, but could you please put a bit more care into your messages here? There is a lot of unnecessary, spam-like text (your email client spamming, quotes of previous messages, italian error messages that we do not understand, etc), which basically avoids that anyone reads this thread. You can also edit previous messages and clean them up, which I kindly suggest you to do.\nNow googling for your problem gives me the same suggestion as @iglesias and @lisitsyn gave. It is very weird that you get this problem. Did you make sure all other sources of error are checked (like full file-system, missing rights, etc)? Did you encounter a similar problem when compiling any other software?\n. Also with the latest version?\n. ok, well then we can close this.\n. ok, well then we can close this.\n. @lambday The values are actually index_t and float64, so switchting to the proper switches for SG_INFO (got locally nonsense numbers)\n. That is a bug in Ubuntu, install libhdf5-serial-dev instead of the openmpi one, that solves it\n. That is a bug in Ubuntu, install libhdf5-serial-dev instead of the openmpi one, that solves it\n. You are welcome to send a patch that allows to disable using hdf5 (should be easy if not already there, @vigsterkr ?). We cannot do anything against the wrong ubuntu package though.\n. You are welcome to send a patch that allows to disable using hdf5 (should be easy if not already there, @vigsterkr ?). We cannot do anything against the wrong ubuntu package though.\n. - The wrong include path in the mpi hdf5 is an ubuntu package problem.\n- You are right we currently do miss the feature to disable hdf5 when the mpi version is installed\n- Your solution also is fine, but better would be to add a cmake option to disable it. Thanks for sharing it.\n- Keep in mind this is an open-source project and we are doing things in our free time. You are very welcome to get involved via sending a patch. We lack manpower to fix such problems because many things have higher priority.\n- please report broken interfaces in a seperate thread with details\n. - The wrong include path in the mpi hdf5 is an ubuntu package problem.\n- You are right we currently do miss the feature to disable hdf5 when the mpi version is installed\n- Your solution also is fine, but better would be to add a cmake option to disable it. Thanks for sharing it.\n- Keep in mind this is an open-source project and we are doing things in our free time. You are very welcome to get involved via sending a patch. We lack manpower to fix such problems because many things have higher priority.\n- please report broken interfaces in a seperate thread with details\n. Added an option to disable present hdf5 in #2656\n@beew you could consider reporting this to the package maintainers of hdf5, would be appreciated from all sides\n. Added an option to disable present hdf5 in #2656\n@beew you could consider reporting this to the package maintainers of hdf5, would be appreciated from all sides\n. I think the bug is different to the one you posted there.\nThe bug here is that the mpi version of hdf5 has a wrong include path which causes the compilation error. You might have noticed that the missing include is not in a Shogun header, but a hdf5 one. See @sonney2k comments in #1856\n. A blacklist for notebooks would be good. @vigsterkr ?\n. A blacklist for notebooks would be good. @vigsterkr ?\n. Looking good, I just restarted travis after having merged the data PR. Let's see\n. Looking good, I just restarted travis after having merged the data PR. Let's see\n. stochasticgbmachine_modular.py\nError opening file '../../../data/multiclass/categorical_dataset/fm_housing.dat'\n. stochasticgbmachine_modular.py\nError opening file '../../../data/multiclass/categorical_dataset/fm_housing.dat'\n. See the travis build log of python_modular\n. See the travis build log of python_modular\n. Octave errors should be indeedn independent, so don't worry about them now. We need the python build to work\n. Octave errors should be indeedn independent, so don't worry about them now. We need the python build to work\n. Ok one more thing, could you please squash all commits into a single one? We want to keep the git history clean\n. Ok one more thing, could you please squash all commits into a single one? We want to keep the git history clean\n. I think thats unrelated. So I will merge now.\nThanks for the patch!\n. I think thats unrelated. So I will merge now.\nThanks for the patch!\n. can you please make a single patch out of this?\n. Note that dot is in the linalg interface already. Have a look!\nThis means that we can drop those things from CMath completely\n. @sanuj yes\n@lambday I see. We should then first implement a native dot product that does not depend on any external library. But we can at least parallelise it using openmp\n@sanuj Maybe start with the native dot implementation and then continue on this patch?\n. Thanks for taking care @lambday Ill leave this one for you and the gang\n. as @lambday said, the native linalg framework would not support all operations there are, but maybe give runtime errors if something is not implemented. But vector dot, matrix dot, etc should definitely be in there natively, and not in CMath.\nWe want to avoid having our methods cluttered all over Shogun\n. ok for now. But we have to touch all the places again once we have linalg.\nStill better than having this in SGVector, so thanks a lot\n. @srgnuclear could you try to avoid spamming us with pull requests? Only send it if you are really sure it is ok. There is a preview option. We get an email for every PR you send, which blows up our emails and take time from doing important stuff. Thanks and keep up the good work :)\n. just update this one here via pushing into your branch srgnuclear:replace_math.h\n. Could you also fix this in the other locations that this text appears?\nDoxygen, website, etc? Just grep for it\n. Please dont put screenshots in github issues, just explain what you are doing\n@vigsterkr can you help?\n. This is only the static interfaces and is deprecated .... only half of shogun is exposed through this.\n. Ah man what are all these screenshots up there?\n. Would be amazing if you could fix it ;)\nWe need more Mac developers.....\n. We currently don't know one, but we are working on it. Check back with @besser82 in irc for more details\n. issue closed? works for everyone?\n. Unit tests compare on super simple cases, and we try to keep them independent of shogun-data. That is why most examples are hardcoded.\nFor MKL regression, what you would do is to find a reference implementation (there might be none), or design a simple example that produces results that you check by hand and verify to make a lot of sense, and then use that.\n. Unit tests compare on super simple cases, and we try to keep them independent of shogun-data. That is why most examples are hardcoded.\nFor MKL regression, what you would do is to find a reference implementation (there might be none), or design a simple example that produces results that you check by hand and verify to make a lot of sense, and then use that.\n. Ah crap, sorry for that, I will do.\n. Duplicate of #2313 which also contains some workarounds\n. @Ialong u\u00fcdate your shogun and tell us whether it works, there might be no need to install ubuntu :)\n. That doesnt look like a shogun problem, but not sure\n. Sorry I need to revert this to restore the build.\nEither a bug was introduced or the tests were not properly updated.\nA broken build is unacceptable since more things are slipping through -- too long already\nPlease re-send a patch that works.\n. thx\n. Some thoughts:\n- the hdf5 warning makes me think this might be caused by your local installation, please double check\n- There is another (might be closed) thread on R modular, it works in principle.\n- try using the latest develop branch, we did lots of fixes for R after the last release (working on next one)\n@vigsterkr @matthuska should be able to help, too\n. @beew would you mind not putting such long outputs here directly? You can paste them at https://gist.github.com/\n@matthuska thanks for helping!\n. @besser82 put those things into NEWS.\nTo let people know that we are working on all those installation/compilation issues\n. Fine from my side, we should take advantage of all the modern stuff in the language these days anyways.\nI much rather prefer a modern code over being compatible to ancient computer systems.\n. So is there any conclusion here?\nMake c++11 mandatory?\n. We will wait (at least a bit) with c++11 (see discussion on mailing list)\nAs eigen3 is now always enabled, we dont need the native implementations anymore\n. We will wait (at least a bit) with c++11 (see discussion on mailing list)\nAs eigen3 is now always enabled, we dont need the native implementations anymore\n. GREAT work guys :)\n@lambday will read your tutorial for linalg these days\n. Hi, thanks for the list!\nI mean to reproduce standard examples in the notebook -- using Shogun\nNo cpp involved\n. Nice in principle. Can you send a PR then I can give comments.\nPlease always paste a link of the notebook with outputs in the PR.\nThe PR itself should be without outputs.\nLooking forward to reviewing it\n@lisitsyn any ideas on the reconstruction error? If we dont have it, would be good to add that to shogun, it is easy\n. Hey @lisitsyn \nAwesome! Looking forward to see this more developed and getting rid of all those ugly SG_REFs in our code!\n. Now starting to look at this....\n. Very good patch. Also right size :)\nThere are some minor things which are easy to fix.\nOne major change would be to put the GP related parts in CKernel to another place, as we cannot convolute such a fundamental base class with algorithm specific details, thats bad design.\nAny ideas how to do this alternatively?\n. I see,\nCan't you do this via one of the other methods? compute?\n@lambday @lisitsyn you are the c++ gods and can probably tell me whether making this low level method virtual will bite us\n@yorkerlin go ahead for now, we will see then. It might be ok if the FITC kernels only work for GP, but then put them into the GP folder, wrapping the existing kernel is then ok\n. One more thing for your bugfix for the ARDKernel: unit tests.\nSame goes for your new kernels in fact, very important if you wrap other objects as behaviour might implicitly change and that can slip through if no unit tests are there.\n. Ok do this for now, but we need to confirm with someone who knows about this stuff.\n. Asked Sergey, it is fine to make the call virtual, just make sure you check the indices\n. Sorry for the delay, now checking\n. Sorry for the delay, now checking\n. Ok I think the code is good to merge.\nGreat work, once again, Wu!\nThere are all these minor issues, which do not need to be addressed in this patch, but maybe in a next one? Or a small clean-up patch?\n. Ok I think the code is good to merge.\nGreat work, once again, Wu!\nThere are all these minor issues, which do not need to be addressed in this patch, but maybe in a next one? Or a small clean-up patch?\n. @lambday linalg on travis?\n. @lambday linalg on travis?\n. @yorkerlin once travis is sorted we can merge\n. @yorkerlin once travis is sorted we can merge\n. @yorkerlin I actually dont see a linalg include in the LinearARDKernel.cpp\n. @yorkerlin I actually dont see a linalg include in the LinearARDKernel.cpp\n. this is weird\n@lambday any ideas?\n@vigsterkr  ?\n. @yorkerlin Once we have figured this out, this is ready to merge. So feel free to open another PR that is based on this one? \n. @yorkerlin ok with the guard it would work then. \n@lambday is it really a good idea to need to have these guards everywhere? I dont like guards.....\n@vigsterkr Also it would be cool to be able to test these linalg things in travis, so what about having c++11?\n. @yorkerlin thanks for fixing the bug. Please put this in the NEWS file (and also mention that you also fixed in in GPML ;) )\nYou mentioned that the framework requires understanding of GPs. This means you should document it very carefully. Really important.\nIll restart travis\n. The travis error is due to memory limit. I'll merge the patch!\n. @yorkerlin please update the NEWS with all you additions, its good to expose it to people.\nA few examples would also be nice once everything else is merged. Looking forward\n. @abinashmeher999 go ahead.\nBut let's discuss how to do it first here.\nMy suggestion: Add another evaluation class which takes a list of evaluation objects, which are then all evaluated. This way, one can still pass a single object to the X-validation. There might be some tricky details, but thats a good first start to think about it. Let me know if that works\n. good point ;) @lisitsyn \n@abinashmeher999 we might have to change the interface for this.\nI dont see the advantage for a map, but nothing speaks against it either\n. The map thing is quite irrelevant as the number of metric will be small. There should be a method that returns all metrics as a vector. Accessing those via a string is a minor improvement that we could think about after having changed the internals.\nWhat is more important is the changes of member variables you described. I suggest to change to a list internally, while keeping the same interface if one only wants one metric. So go ahead and draft a PR. We can continue from there.\n. Please support @yorkerlin   with his operations, he writes the most amazing variational GP codes with them :) But of course, too much cluttering is not good either.\n. If you want infos on FA, check david barber's book \"Bayesian reasoning and machine learning\", which is available online.\nI would love to see the math for the generative model of FA in the docs. Also a good thing to learn about\n. That is good stuff, I heard about these.\nBUT given the current state of nn implementations in shogun it would be better to tune them for speed and run some comparisons with other libs. Reading over the code there is tons of potential speed ups \n. One thing to do would be to try to use the linalg library for some of the operations dones within the code.\nConvolution is an example. There might be others.\nThen using some simple parallelism (openmp) might also help.\nThen, thinking which operations could go on GPU?\n. Suggestion:\nTrain on subset in notebook. State that it takes much longer if done on full data.\nPut a dropbox or similar link to download the trained thing and allow to use the in notebook if available\n. @amit309b there wont be any guidelines what to do next, we require self-initiative. Also see above discussion. @amit309b please read on how to share ipython notebooks. This seems OK, but it is like 1% of the task.. Sorry for late reply, but yeah. Everything that can be broken by a user call should be require, otherwise assert (with a comment) is fine. Thanks for taking care\n. Sorry for the delay so far, we had to apply to GSOC and that took lots of attention. I will do soon\n. Ok thats some nice work for the first step. Thanks for that ! :)\nOnce thing: I suggest to evaluate mutiple metrics using openmp, if possible.\nWhats next to get this working?\n. openmp does manage the threads/cores I think\n@lisitsyn @lambday  ?\n. openmp does manage the threads/cores I think\n@lisitsyn @lambday  ?\n. Ok cool, this is getting ready. Sorry I did oversee the reference counting issue before, please check.\nNext step (and last before this can be merged):\nUnit test the new constructors and methods, these unit tests can be super minimal, but make sure everything works as you intended it to\n. Ok cool, this is getting ready. Sorry I did oversee the reference counting issue before, please check.\nNext step (and last before this can be merged):\nUnit test the new constructors and methods, these unit tests can be super minimal, but make sure everything works as you intended it to\n. Heya,\nany news on this? As far as I can see this is basically done. Just need to check this potential memory leak.\nGreat if we could get this merged soon to prepare for GSoC\n. Thanks for letting us know. It should not take more than an hour or so to get this merged.... so let's do it :)\nGood luck with your exams. \n. news?\n. First step please: Basic unit testing. There are a lot of tests in the static interfaces. Those have to be translated to unit tests before we touch the class, otherwise we dont know what breaks\n. Yes @iglesias these static examples are the ones I mean. Many of them work on toy data so can easily be reproduced as unit tests. Certainly, all methods in the static tests should be unit tested. Since we have static tests, we can easily write toy examples for unit testing the same methods, using Shogun's current own outputas ground truth (since existing tests somehow guarantee that results are correct)\n. Yes @iglesias these static examples are the ones I mean. Many of them work on toy data so can easily be reproduced as unit tests. Certainly, all methods in the static tests should be unit tested. Since we have static tests, we can easily write toy examples for unit testing the same methods, using Shogun's current own outputas ground truth (since existing tests somehow guarantee that results are correct)\n. The issue is still open, yes, and the file/class is still in the source tree.. all  public methods (and classes) that are exposed via doxygen should be documented\n. all  public methods (and classes) that are exposed via doxygen should be documented\n. Let us know if you need help!\n. Shogun's HMM classes are a bit old fashioned, this might involves refactoring them, cleaning up the legacy code\n. @nginn Hi there.\nGreat stuff!\nAre you interested in extending the linalg library with more operations like linear solves and more?\nSeems like a good fit to you! Especially as you are doing these great benchmarks.\n@lambday and @yorkerlin have plans to add the mentioned operations, so if you talk to them they can guide you\n. Haha, WHAT? :)\n. hahaha ;)\n. @AnishShah check the examples we have, all supervised methods we have are here:\nhttp://www.shogun-toolbox.org/doc/en/latest/classshogun_1_1CMachine.html\n. Hey @youssef-emad \nThis is great, exactly what I had in mind. Thanks for that! Can you send a pull request? Then we can comment line by line!\n. +1 And also on the renaming, which is true.\n. Lets move soon. But not sure whether we should do another release before GSoC.\n5.0 after GSoC is fine.\nIf you want to release before this switch, also good! But we did not fix too many things since then right?\nReleasing as often as possible is a good practice though\n. Requiring a new standard is a major change in my eyes too. But well, it is a matter of taste, just change it in the Dev branch and we can discuss numbers later\n. No own webserver, just packages integrated into standard ways to distribute packages\n. @vigsterkr is the expert here.\n. @vigsterkr is the expert here.\n. @souravsingh That would be amazing++ please send a patch and we can iterate on the PR\n. Actually, we have a setup.py now ( I closed the other issue) , so go for it! :)\n. @vigsterkr ?\n. Thanks for reviewing, @lisitsyn \n@sorig I think running the script that scrapes the .i files should happen at build time. So thats the only thing to be updated before this can be merged (at least from my side, lets see whether the others find time to review - we are currently overloaded with GSoC applicants)\nVery nice work!\n. @kevinhughes27 @lisitsyn \nyes, we aim to completely replace the existing examples and migrate them into the new system.\nWe just need patience until this works, so I suggest to set things up next to each other and have the GSoC applicants translate the examples.\n. @kevinhughes27 @lisitsyn \nyes, we aim to completely replace the existing examples and migrate them into the new system.\nWe just need patience until this works, so I suggest to set things up next to each other and have the GSoC applicants translate the examples.\n. @sorig Just checked, in Ubuntu, PLY is supported (not just part of it, but supported by Canonical)\nSince python has a super easy package managment system, I dont see too many problems. However, it would still be better to not have this dependency in. Are there any other options? Re-implementing things is also not an option. What about the C++ generators?\n@sonney2k @vigsterkr @besser82 what do you think?\n. @sorig Just checked, in Ubuntu, PLY is supported (not just part of it, but supported by Canonical)\nSince python has a super easy package managment system, I dont see too many problems. However, it would still be better to not have this dependency in. Are there any other options? Re-implementing things is also not an option. What about the C++ generators?\n@sonney2k @vigsterkr @besser82 what do you think?\n. Thought about it again, what about offering two versions? Fast parsing would then be optional, but still everyone could generate examples?\n. I had severe troubles with this on a cluster machine on which I use Shogun. So I think having both versions, where the fast one is optional when PLY is available would be a good way to go.\n. The handwritten parser has the problem that nobody will ever touch it again, which is bad. We had this scenario before, so I would like to avoid that -- even though you are right that we probably don't need to touch it again.\nPLY is the best option in my eyes too. However, it would be extremely annoying for people to not be able to produce any examples at all. Imagine you install Shogun, are not too familiar with python libraries, and then you cannot look at examples. That is why we need something that runs with no dependencies at all. I agree it's annoying to maintain two version of the same program, but its less annyoing that maintaining a manual parser, so I think that would be the way to go.\nWhat are your thoughts?\n. Great, I like this.\nMaybe add a unit test on one or two examples to make sure both parsers output the same?\nYes, a warning would be the best I think.\nWith this, we are soon ready to merge this.\n. OK, I merge this now.\nLet's see what happens.\n@vigsterkr could you add a new build with this enabled?\n. requests:\nCould you  move the README.md into the doc/md folder?\nThis will make it editable from the wiki. Then we can put a symlink to this folder?\n. Out of my competence scope. Can you explain a bit or point to references?\n. I see. We want this to make code look nicer?\nI don't see the different between NULL->method() and an error caused by absent value apart from cosmetics? Or do I miss something?\n. safer in the sense that we dont cause segfaults or read uninitialised memory? Nicer because no pointer semantics?\nCool I get it then.\n. Cool!\nWhat about a unit test?\n. Ok, all good to go. I will merge, but could you make a note of the minor style issues I noticed and address them in the next patch?\n. looks good to me, a few minor style issues.\nIs this covered by unit tests?\n. Dont know why, but will look at the other\n. Dont know why, but will look at the other\n. Cant this be solved with a seperate table where people store their names?\nI like it but is it really something we should do from within shogun? Just increases complexity....\n. In my eyes this would be an attempt to reinvent wheels. There are data storage libs which can do this.\nAlso I dont really see what we get other than convenience?\nI mean yeah, might be nice, but given the horrible horrible state the feature classes are in, I would rather do some proper design clean ups before adding to this shaky fundament. Just my two cents\n. Yeah I agree. Good thing to have.\nTo restate my main point: We don't want to do this by hand but use a lib to do this for us.\nAnd if we do, we should clean up first. The Features code is horrible enough already. Dont you think?\n. Like it, but this is not easily interpreted for example for SVMs.\nGood for feature selectors as @iglesias said.\nBut, this is again something that is super easily done in a target language and increases complexity of our codes. So not sure whether its worth it.\nWe should rather use some data-storing backend to do such things. Like reading features of a HDF5 frame aka python-pandas. That would be a more sophisticated solution which also has the advantage that its done properly. To me, such ideas often seam to be a quick temporary hack that are not used later on but make maintaining harder. But also not against it\n. Super high priority and easy - so go for it!\n. Super high priority and easy - so go for it!\n. Great!\nBut there are way more classes, so I re-open\n. Great!\nBut there are way more classes, so I re-open\n. Thats a good way to start, but keep in mind the list of methods should not contain every class we have, but every algorithm we implement, with a reference to the class(es) it is implemented in.\nYou don't need to look for missing unit tests, we have a test coverage script that does this, see buildbot\n. I think we can leave this for now as the last update was nice and we soon will switch to the new manual.\nIf you are keen, you can polish this a bit, many things are unresolved, general feedback is appreciated:\nhttps://github.com/karlnapf/shogun_manual\n. Closing for now\n. If you grep'ed for all of those, then fine to merge from my side\n.  @lambday yeah sure. I meant in the CMakeLists.txt.\nCool to have this now! :)\n. Travis errors unrelated, merging\n. Travis errors unrelated, merging\n. https://github.com/shogun-toolbox/shogun/wiki/Example_Generation\nThis works?\n. https://github.com/shogun-toolbox/shogun/wiki/Example_Generation\nThis works?\n. Hey @sorig \nCould you update the readme file to explain the thing about PLY and also how to enable the examples from cmake. Also put a note in the cmake readme.\nWe will enable the examples in the builds soon\nThanks!\n. oh, and add this to the NEWS, too (might have to start a new section)\n. Deep copy is implemented in CSGObject::clone() and works for all CSGObject.\nShallow copy does not make any sense to have, we should remove the method and let people do this via copy constructors\n. Deep copy is implemented in CSGObject::clone() and works for all CSGObject.\nShallow copy does not make any sense to have, we should remove the method and let people do this via copy constructors\n. Thanks\n. Thanks\n. travis error unrelated, merging\n. travis error unrelated, merging\n. I cannot judge whether all these changes are fine, they seem on first sight - so lets leave the rest to unit tests.\nThe obtain_from_generic methods should be uniform within Shogun, could you maybe open an issue (or look whether we have one) and put an example obtain from generic in there? Using something that exists in the code base, and then the task is to unify them all over Shogun\n. I cannot judge whether all these changes are fine, they seem on first sight - so lets leave the rest to unit tests.\nThe obtain_from_generic methods should be uniform within Shogun, could you maybe open an issue (or look whether we have one) and put an example obtain from generic in there? Using something that exists in the code base, and then the task is to unify them all over Shogun\n. And they should return NULL if given NULL in my eyes\n. And they should return NULL if given NULL in my eyes\n. Ill merge for now, maybe send a clean up PR later\n. Ill merge for now, maybe send a clean up PR later\n. +1\n. +1\n. Our framework also can give indices.\nI think we should include the cross-validation of methods within Shogun, it is very useful and easy to get wrong for people who are not familiar.\nHowever, brute force parameter (grid) search should be out, as this can easily be done with a nested loop given that x-validation is implemented.\n. Our framework also can give indices.\nI think we should include the cross-validation of methods within Shogun, it is very useful and easy to get wrong for people who are not familiar.\nHowever, brute force parameter (grid) search should be out, as this can easily be done with a nested loop given that x-validation is implemented.\n. @lambday could you merge this?\n. We made eigen3 a hard requirement in 7339764920061d35581287f2483c24cc6187901e\n. I understand what you have in mind with these three layers. Sounds good\nThere are a number of documentation glitches. Could you fix them?\n. I understand what you have in mind with these three layers. Sounds good\nThere are a number of documentation glitches. Could you fix them?\n. Hi Wu,\nI think whenevery somebody uses FITC based inference, we should return the approximation coming from the inducing points\n. We only want the O(n^3) approximation if we don't use the inducing framework\n. I am fine with doing the rest in the next PR\n. travis is fine except for unrelated things. merging!\nNICE! :)\n. Thanks for the updates.\nThere are still some glitches, but we are getting there ....\nIt is quite tricky to write good documentation, but your efforts are very much appreciated ! :)\n. These changes are very useful. Can we get them merged? @srgnuclear \n. closing since inactive\n. In fact, grepping for SG_MALLOC(float64_t would be a good idea :)\n. Hi and sorry for the delay. @lambday could you comment?\n. what is the state here?\n. I think this needs a re-work. It is a mess now. Maybe close and re-open something fresh?\n. Yes, that is still open.\nThere is another problem, that some classes are detected as singletons by doxygen. E.g. CDenseFeatures\n. Useful for the linalg GSoC project. https://github.com/shogun-toolbox/shogun/wiki/GSoC_2015_project_linalg\n. @sanuj it is not yet done. @lambday will guide this and merge the requirement patch too ;)\n. absolutely! I think most of linalg that is needed is there, so that should be easy. Make sure to send a few minimal unit tests before modifying the code .... There are still many loops who wait to be replaced with linalg calls. I like that framework. Is it well maintained? Thats a big plus for the google one ..... in addition to the readme being shorter ;)\n. I would like to avoid any overhead and stick to standard things to do that. But if it is that easy, why not\n. We now have benchmarks going.\n. @souravsingh hi thanks for being interested in this.\nIn fact, @lisitsyn and me just got a prototype working that is ready to be filled with examples. We will push this to the development branch within the next week. You can then have a look and help us populating and documenting it. There is a lot of very easy tasks that have to be done. You will see.\nPing me next week if the thing is not there by then. \n. Sorry for the delay, I was away.\nI just deleted my demo repo mentioned above.\nWe have a growing manual (aka cookbook) in develop now, here is the latest version:\nNote this is all executable code which is part of our test build, it is pretty easy to contribute new entries to this. We also want to pimp the visuals -- any help welcome. I close the issue for now\n. Oh yes, please please +1 +1\nAND do a little clean up on the fly, some of the structures do not make too much sense\n. Good to see you are one this!\n. Good to see you are one this!\n. Maybe that is not so bad. NN's notion of confidence is neither calibrated nor stable. Using them to quantify uncertainty usually is dangerous. Or has this changed?\n. @vigsterkr ?\n. Oh wow. Why did we never spot this yet?\nI will open an issue for this\n. Very interested here in other experiences with using Shogun from ruby\n. You are writing beautiful code here, @yorkerlin \nThere are a few minor style things and some missing REQUIRE checks, but good to go otherwise.\nThe lock thing I don't understand, could you explain it to me?\nGood to merge otherwise\n. Most of the other travis errors seem unrelated\n. Mmmmmh, maybe at least some of the math? Doesnt need all the derivations.\nBut just something so that if one has the paper lying next to him can read the code and see easily what is what?\n. I did not know the paper yet. Its good to reference it!\n. Ok, once the style and REQUIRE things are in, I will merge\n. I saw you sent a patch, is this the final one? Travis was ok except for some other errors\n. I saw you sent a patch, is this the final one? Travis was ok except for some other errors\n. I think the lock is better than creating all these multiple matrices \nSo merging!\nWell done!\n. I think the lock is better than creating all these multiple matrices \nSo merging!\nWell done!\n. See for example \nhttp://www.shogun-toolbox.org/doc/en/latest/classshogun_1_1CRandomKitchenSinksDotFeatures.html\n. Sounds good. But, I think somebody else already started looking at this. Please search github and the mailing list to make sure there is no redundant efforts :) If not, feel free to give it a go!\n. I agree with @iglesias \nNo features and vectors in a matrix.\nAs for semantics. Sparse matrices should really only be treated as linear operators that can be applied to vectors or chained. As a data structure, its more a dictionary of (i,j) pairs to a real number.\nThe () operator is useful for debuggin/construction, but it should never be used in practice. Imagine you iterate over num_rows, num_columns! Harmful!\nIt probably makes sense to mimic other libraries implenentation: Maybe check eigen3, scipy, and matlab?\n. Careful!\nThis is the final patch, see list here: http://permalink.gmane.org/gmane.comp.ai.machine-learning.shogun/5463\nWe cannot merge this without being absolutely certain that it doesnt break things.\nYou  basically have to search for all places where SGVSparseMatrix is imported and then check for the () operatior being invoked. A bit messy.\n. However, after thats done, we can merge. Thanks a lot for your help\n. It kind of is a dangerous thing for sparse structures as one should always iterate through the list of non-zero elements\n. It kind of is a dangerous thing for sparse structures as one should always iterate through the list of non-zero elements\n. Sorry for the late reply.\nI agree if it is not used then the order doesnt matter.\nWe should probably unify the interface with other langauges as @iglesias suggested.\n. Is this a fix for the recent broken test?\nIf not, changing names of parameters breaks integration tests, so they also need to be updated\n. cool! let me know how it goes\n. cool! let me know how it goes\n. Must be something with your system, as Shogun is known to build fine under Ubuntu 14.04\n@lisitsyn any ideas?\n. Must be something with your system, as Shogun is known to build fine under Ubuntu 14.04\n@lisitsyn any ideas?\n. Any updates here?\n. Any updates here?\n. I like these complexity comments a lot!\nCould you put a few of those in the class description (not the details, just the current implementation's costs and that one could speed this up via tuning implementation)\n. I like these complexity comments a lot!\nCould you put a few of those in the class description (not the details, just the current implementation's costs and that one could speed this up via tuning implementation)\n. About diagonal of kernel matrix:\n- check whether we have this. I remember we considered adding this thing a while ago\n- if not, add a method to the base class, that does it naively\n- sub-classes may override, but as high as possible in the hierarchy\n@lambday might have a comment here?\n. About diagonal of kernel matrix:\n- check whether we have this. I remember we considered adding this thing a while ago\n- if not, add a method to the base class, that does it naively\n- sub-classes may override, but as high as possible in the hierarchy\n@lambday might have a comment here?\n. Travis seems fine. Ill merge\n. Travis seems fine. Ill merge\n. Thanks!\n. Thanks!\n. This is a beautiful piece of work!\nI would love to see a bit shorter unit tests.\nCan I suggest that we re-work over the notebook a bit? \nAlso, API examples for FITC would be good. Minimal ones.\nFinally, could you benchmark your code against the other toolboxes to make sure there are no bottlenecks?\n. As my comments for only style, I merge this so that you can go on. As usual, it would be great if you could send a mini patch for them later\n. Really excited about your work, Wu. You are pushing Shogun towards being one of the most complete and efficient approx GP implementations out there! :)\n. Hey!\nAny news on this patch?\n. Good stuff @yorkerlin \nA few comments:\nI would for now remove the arixived papers from the list. We do not know yet how well they really work, so we should wait for them to get peer reviewed.\nAll the additional tools are also very useful. I would also like to suggest another paper, that is very similar, but not specifically GP and variational inference, but general kernel methods. Very good stuff as it scales kernel methods up to NN performance both in time and accuracy:\nScalable Kernel Methods via Doubly Stochastic Gradients http://arxiv.org/abs/1407.5599\nFor this, we would need to clean up the random kitchen sink implementation a bit. I dont know how interested you are in this, but it is certainly super general and useful for all of shoguns methods\n. Ok cool!\n. Ok cool!\n. Great!\nI am back to double check and merge code now.\nStill asking about benchmarks!\n. Looks fine!\nThanks.\n. Ah sorry I forgot about the zero().\nPlease send another patch to remove that.\n. Great stuff!\nThis is definitely useful\n@\u0142ambday should comment, too. I am surprised we did not add direct solvers back in the log-det project.\n@yingryic You added the solvers to the linalg classes for linear operators. This is good and can be merged (modulo some style issues).\nNow, we are interested in integrating the same type of thing into the linalg framework here:\nhttps://github.com/shogun-toolbox/shogun/wiki/README_linalg\nYou will noticed that this is quite different, not based on implementing classes. But rather defining an interface (similar to the dot products we have) and the implement various backends for it. Let me know hwo this goes\n. I dont know, wouldnt it be better to template the existing ones?\nAll this redundant code makes me nervous. If it is too messy to do (what do you think @lambday ) then we could have this one too. But the interfaces should be exactly the same pls.\n. I dont know, wouldnt it be better to template the existing ones?\nAll this redundant code makes me nervous. If it is too messy to do (what do you think @lambday ) then we could have this one too. But the interfaces should be exactly the same pls.\n. I agree with lambday, we should have the interface in linalg, and then make these linear operators work on that interface.\n@yingryic its good that you coded this up as you probably learned a lot. I suggest to keep this PR open for now and move to linalg? Then come back here?\n. I agree with lambday, we should have the interface in linalg, and then make these linear operators work on that interface.\n@yingryic its good that you coded this up as you probably learned a lot. I suggest to keep this PR open for now and move to linalg? Then come back here?\n. I will close this for now, but it would be great if we had something similar for linalg finally \n. Could you provide some more information of what you are doing?\nMake sure check the readme files, they explain how to use cmake and subsequent make\n. I close this for now\n. This comes from a recent cpu (i7) used with an outdated compiler (such as the one in Ubuntu 12.04)\nCould you share your system specs?\n. coveralls is the code coverage analysis\n. coveralls is the code coverage analysis\n. thanks for the patch! :)\nIll merge when you fixed the error message\n. thanks for the patch! :)\nIll merge when you fixed the error message\n. Thanks a lot! Really appreciated, this type of proper work\n. Thanks a lot! Really appreciated, this type of proper work\n. merging as travis is fine\n. merging as travis is fine\n. Welcome :)\n. Welcome :)\n. any news here?\n. Yep, with the automagically generated ones, we can do this quite easily. I dont know about swig though.\nTo be honest, I dont think this is too important .... we rather should have example documentation as the manual project @lisitsyn and me have been working on\n. Yep, with the automagically generated ones, we can do this quite easily. I dont know about swig though.\nTo be honest, I dont think this is too important .... we rather should have example documentation as the manual project @lisitsyn and me have been working on\n. We can always change this, once examples are generated automatically, all easy\n. We can always change this, once examples are generated automatically, all easy\n. its none of them, but Shognu requires a recent compiler that supports c++11\n2015-03-30 10:53 GMT+01:00 tengshaofeng notifications@github.com:\n\nI cmake ,then i face the following error.Sould I install MOSEK,ATLAS and\nso on?\ntbq@tbq-All-Series:~/shogun-4.0.0/build$ cmake\n-DCMAKE_INSTALL_PREFIX=\"$HOME/shogun-install\" / -DMatlabStatic=ON ..\n-- Could NOT find CCache (missing: CCACHE)\n-- Found SWIG: /usr/bin/swig2.0 (found suitable version \"2.0.4\", minimum\nrequired is \"2.0.4\")\n-- Using system's malloc\n-- Could NOT find MOSEK (missing: MOSEK_DIR MOSEK_INCLUDE_DIR\nMOSEK_LIBRARY)\n-- A library with BLAS API found.\n-- A library with LAPACK API found.\n-- Could NOT find ATLAS (missing: ATLAS_LIBRARIES ATLAS_INCLUDES)\n-- Could NOT find CBLAS (missing: CBLAS_LIBRARY)\n-- Could NOT find GLPK (missing: GLPK_LIBRARY GLPK_INCLUDE_DIR\nGLPK_PROPER_VERSION_FOUND)\n-- Could NOT find CPLEX (missing: CPLEX_LIBRARY CPLEX_INCLUDE_DIR)\n-- Could NOT find Eigen3 (missing: EIGEN_INCLUDE_DIR) (Required is at\nleast version \"3.1.2\")\n-- Could NOT find OPENCL (missing: OPENCL_INCLUDE_DIR) ATLAS\n-- Could NOT find ViennaCL (missing: VIENNACL_INCLUDE_DIR\nOPENCL_INCLUDE_DIRS OPENCL_LIBRARIES) (Required is at least version \"1.5.0\")\n-- checking for one of the modules 'libColPack>=1.0.9;ColPack>=1.0.9'\n-- Could NOT find NLOPT (missing: NLOPT_LIBRARY NLOPT_INCLUDE_DIR)\n-- Could NOT find LPSOLVE (missing: LPSOLVE_LIBRARIES LPSOLVE_INCLUDE_DIR)\n-- Could NOT find ColPack (missing: COLPACK_LIBRARIES COLPACK_LIBRARY_DIR\nCOLPACK_INCLUDE_DIR)\n-- Could NOT find ARPREC (missing: ARPREC_LIBRARIES ARPREC_INCLUDE_DIR)\n-- Could NOT find Doxygen (missing: DOXYGEN_EXECUTABLE)\n-- checking for one of the modules 'libjson>=0.11;json>=0.11;json-c>=0.11'\n-- Could NOT find LibXml2 (missing: LIBXML2_LIBRARIES LIBXML2_INCLUDE_DIR)\n-- Could NOT find HDF5 (missing: HDF5_LIBRARIES HDF5_INCLUDE_DIRS)\n-- Could NOT find CURL (missing: CURL_LIBRARY CURL_INCLUDE_DIR)\n-- Could NOT find BZip2 (missing: BZIP2_LIBRARIES BZIP2_INCLUDE_DIR)\n-- Could NOT find LibLZMA (missing: LIBLZMA_INCLUDE_DIR LIBLZMA_LIBRARY\nLIBLZMA_HAS_AUTO_DECODER LIBLZMA_HAS_EASY_ENCODER LIBLZMA_HAS_LZMA_PRESET)\n-- Could NOT find SNAPPY (missing: SNAPPY_LIBRARIES SNAPPY_INCLUDE_DIR)\n-- Lzo includes and libraries NOT found.\n-- Spinlock support found\n-- Linear algebra uses c++11 features. Please use a supported compiler\n-- Found PythonLibs: /usr/lib/libpython2.7.so (found version \"2.7.3\")\n-- Found NumPy: version \"1.9.0\"\n/usr/local/lib/python2.7/dist-packages/numpy-1.9.0-py2.7-linux-x86_64.egg/numpy/core/include\nCMake Error at cmake/FindOctave.cmake:125 (if):\nif given arguments:\n\"LESS\" \"3.8.0\"\nUnknown arguments specified\nCall Stack (most recent call first):\nCMakeLists.txt:1082 (FIND_PACKAGE)\n-- Configuring incomplete, errors occurred!\nSee also \"/home/tbq/shogun-4.0.0/build/CMakeFiles/CMakeOutput.log\".\nSee also \"/home/tbq/shogun-4.0.0/build/CMakeFiles/CMakeError.log\".\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/shogun-toolbox/shogun/issues/2790#issuecomment-87618363\n.\n. its none of them, but Shognu requires a recent compiler that supports c++11\n\n2015-03-30 10:53 GMT+01:00 tengshaofeng notifications@github.com:\n\nI cmake ,then i face the following error.Sould I install MOSEK,ATLAS and\nso on?\ntbq@tbq-All-Series:~/shogun-4.0.0/build$ cmake\n-DCMAKE_INSTALL_PREFIX=\"$HOME/shogun-install\" / -DMatlabStatic=ON ..\n-- Could NOT find CCache (missing: CCACHE)\n-- Found SWIG: /usr/bin/swig2.0 (found suitable version \"2.0.4\", minimum\nrequired is \"2.0.4\")\n-- Using system's malloc\n-- Could NOT find MOSEK (missing: MOSEK_DIR MOSEK_INCLUDE_DIR\nMOSEK_LIBRARY)\n-- A library with BLAS API found.\n-- A library with LAPACK API found.\n-- Could NOT find ATLAS (missing: ATLAS_LIBRARIES ATLAS_INCLUDES)\n-- Could NOT find CBLAS (missing: CBLAS_LIBRARY)\n-- Could NOT find GLPK (missing: GLPK_LIBRARY GLPK_INCLUDE_DIR\nGLPK_PROPER_VERSION_FOUND)\n-- Could NOT find CPLEX (missing: CPLEX_LIBRARY CPLEX_INCLUDE_DIR)\n-- Could NOT find Eigen3 (missing: EIGEN_INCLUDE_DIR) (Required is at\nleast version \"3.1.2\")\n-- Could NOT find OPENCL (missing: OPENCL_INCLUDE_DIR) ATLAS\n-- Could NOT find ViennaCL (missing: VIENNACL_INCLUDE_DIR\nOPENCL_INCLUDE_DIRS OPENCL_LIBRARIES) (Required is at least version \"1.5.0\")\n-- checking for one of the modules 'libColPack>=1.0.9;ColPack>=1.0.9'\n-- Could NOT find NLOPT (missing: NLOPT_LIBRARY NLOPT_INCLUDE_DIR)\n-- Could NOT find LPSOLVE (missing: LPSOLVE_LIBRARIES LPSOLVE_INCLUDE_DIR)\n-- Could NOT find ColPack (missing: COLPACK_LIBRARIES COLPACK_LIBRARY_DIR\nCOLPACK_INCLUDE_DIR)\n-- Could NOT find ARPREC (missing: ARPREC_LIBRARIES ARPREC_INCLUDE_DIR)\n-- Could NOT find Doxygen (missing: DOXYGEN_EXECUTABLE)\n-- checking for one of the modules 'libjson>=0.11;json>=0.11;json-c>=0.11'\n-- Could NOT find LibXml2 (missing: LIBXML2_LIBRARIES LIBXML2_INCLUDE_DIR)\n-- Could NOT find HDF5 (missing: HDF5_LIBRARIES HDF5_INCLUDE_DIRS)\n-- Could NOT find CURL (missing: CURL_LIBRARY CURL_INCLUDE_DIR)\n-- Could NOT find BZip2 (missing: BZIP2_LIBRARIES BZIP2_INCLUDE_DIR)\n-- Could NOT find LibLZMA (missing: LIBLZMA_INCLUDE_DIR LIBLZMA_LIBRARY\nLIBLZMA_HAS_AUTO_DECODER LIBLZMA_HAS_EASY_ENCODER LIBLZMA_HAS_LZMA_PRESET)\n-- Could NOT find SNAPPY (missing: SNAPPY_LIBRARIES SNAPPY_INCLUDE_DIR)\n-- Lzo includes and libraries NOT found.\n-- Spinlock support found\n-- Linear algebra uses c++11 features. Please use a supported compiler\n-- Found PythonLibs: /usr/lib/libpython2.7.so (found version \"2.7.3\")\n-- Found NumPy: version \"1.9.0\"\n/usr/local/lib/python2.7/dist-packages/numpy-1.9.0-py2.7-linux-x86_64.egg/numpy/core/include\nCMake Error at cmake/FindOctave.cmake:125 (if):\nif given arguments:\n\"LESS\" \"3.8.0\"\nUnknown arguments specified\nCall Stack (most recent call first):\nCMakeLists.txt:1082 (FIND_PACKAGE)\n-- Configuring incomplete, errors occurred!\nSee also \"/home/tbq/shogun-4.0.0/build/CMakeFiles/CMakeOutput.log\".\nSee also \"/home/tbq/shogun-4.0.0/build/CMakeFiles/CMakeError.log\".\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/shogun-toolbox/shogun/issues/2790#issuecomment-87618363\n.\n. Seems like a cmake problem.\nCan you put your system specs and this log into a new issue?\n. I dont think so. This needs discussion before started. So make sure to ask in IRC. @sonney2k @lisitsyn @vigsterkr will have good comments\n. thanks!\n. Very nice patch.\n\nWhat about some benchmarking/examples/profiling? I am really interested how fast all this runs\n. Merging as only minor comments\n. Really excited to see all this sparse classification in Shogun!\n. This is good to merge but I suggest to change these warning messages to be more informative\n. All good\nDouble check the abs value of convergence check\n. All good\nDouble check the abs value of convergence check\n. I'll merge when back on my computer, cannot do from a phone\n. I'll merge when back on my computer, cannot do from a phone\n. @iglesias Yeah I know, but I guess there is a point in not merging things on a phone, since it might be \"too quickly\" ;) I am very inclined to such actions!\n. @iglesias Yeah I know, but I guess there is a point in not merging things on a phone, since it might be \"too quickly\" ;) I am very inclined to such actions!\n. Here we go\n. Here we go\n. So I am confused now, what should I merge first?\n. Python error seems unrelated to merging\n. How do you call the function? This seems like a type error to me\n. Yeah I guess the error message could be more clear, stating the given type as well.\nIll open an issue on this.\n. Great interaction guys, well spotted @yorkerlin !\nPleasure to read along this discussion!\n. Great interaction guys, well spotted @yorkerlin !\nPleasure to read along this discussion!\n. So the idea is that everything happens in GPU memory?\n. So the idea is that everything happens in GPU memory?\n. If that works, I love the idea!\nI am a bit sceptical whether it is possible to actually do this? Thinking about all the different backends used....i.e. @yorkerlin heavily uses eigen3, but then also some linalg calls....\nMaybe we need to give this decision to the programmer? They know best when transfers should be done? And there should be a debug switch which counts the number of such transfers in order to profile shogun\n. If that works, I love the idea!\nI am a bit sceptical whether it is possible to actually do this? Thinking about all the different backends used....i.e. @yorkerlin heavily uses eigen3, but then also some linalg calls....\nMaybe we need to give this decision to the programmer? They know best when transfers should be done? And there should be a debug switch which counts the number of such transfers in order to profile shogun\n. These benchmarks are super useful. Please document carefully so they are more or less reproducible.\nMatrixInverse is forbidden in Shogun. I only allow QR, Cholesky, SVD and their brothers and sisters ;)\nI think the \"make the programmer decide\" approach might be best.\nAbout doing everything on the GPU memory. If the eigen maps still work on GPU memory, then we should do this. I think in fact that it is an awesome idea then. But some more thinking required here. Maybe someone else knows about graphic cards. @lisitsyn ?\n. These benchmarks are super useful. Please document carefully so they are more or less reproducible.\nMatrixInverse is forbidden in Shogun. I only allow QR, Cholesky, SVD and their brothers and sisters ;)\nI think the \"make the programmer decide\" approach might be best.\nAbout doing everything on the GPU memory. If the eigen maps still work on GPU memory, then we should do this. I think in fact that it is an awesome idea then. But some more thinking required here. Maybe someone else knows about graphic cards. @lisitsyn ?\n. Great code. Looking forward to try this method.\nI think we should really soon focus on a study (ipython notebook) where we compare all these methods with each other: memory, runtime, etc.\nMight even produce an arxiv paper if we do a super general experimental comparison\n. Merging as travis errors unrelated.\nPlease send a clean-up patch as usual\n. Matlab is know to have issues currently. We are working on a modular version, but that will take some time. Maybe its easiest for now to either use an older version of Shogun or change to a modular language binding?\n@vigsterkr ?\n. So as far as I can see this, the summary is:\n- Use GPU where possible if we have multiple matrix matrix products. However, this is only for situations where the matrix can be transferred to GPU once and then used multiple times. Or directly allocated on GPU\n- Never transfer otherwise. CPU performance is quite good with eigen3 (I guess it uses the LAPACK/BLAS backend?)\n- Exceptions should be possible (hybrid methods which at some point want to transfer the CPU computed matrix and then use it for many products on GPU). However, this should be in a very explicit way. Not automatic.\n- We need a way to count the number of transfers (optional) to monitor such things, give warning if there are too many or so\nMy questions:\n- can we get away with a not-so-explicit usage of matrices? What is it that we can decide automatically? I.e. I don't see @yorkerlin 's code to work fine on GPU only as it uses so many eigen3 CPU constructs. But for CG-type optimization algorithms, I could see that i.e. the kernel matrix is transfered to GPU once in the beginning and then used from there. Thoughts?\n  - LINALG_IGNORE_GLOBAL_USE_BACKEND(EIGEN3) seems to be a solution. But does this work with the above point?\n. BTW great work once again guys\n. @lambday the CG algorithms should work on GPU if possible. will speed it up so much! Also the preconditioning will get much cheaper then. Should add this at some point :)\n. I think we should make developers to be explicit whenever something should work on GPU. Or, alternatively, add a macro at the beginning of the file and then be explicit when objects should not be on GPU.\n@yorkerlin GPs work well on the GPU if one is careful. We don't want all these complicated eigen3 linear algebra operations to be on there, as GPUs are not that good in that. Rather all expensive operations should go. I am thinking mostly thinking of linear solves / inversions / factorisatilons. Transfering the matrix once doesnt harm that much and we get a lot of speedup. The rest should probably remain on CPU\n. @yorkerlin but I agree if linalg supports more operations (and those are fast, i.e. data never leaves GPU) then we should totally go for it\n. All seems really well! Most of my comments are about style and some minor refactorings.\nLet me know once you updated, then this is read to be merged\n. @lisitsyn For doing extrapolation with GPs.\n. Yeah squashing the commits would be good practice\n. Ok this is ready to go now\n. +1\n. Thanks for reporting. We are currently updating the mac build instructions, and this could be part of it\n. Thanks @youssef-emad \nIs it possible to send a pull request with this?.\nThanks!\n. Ah this is annoying.\nI don't like the solution of manually merging and the one proposed in your link at all....\nWe have to find something cleaner.\n. +1\n. I agree!\n. Feel free to merge\n. Let me know when to merge this\n. Good to go from travis side\n. Seems good to me, Ill merge\n. Good stuff\n. Ok I went through most things, we probably need another iteration.\nGood work!  Let us iterate while keeping travis intact. (so all renaming things later). I know its painful to change all the old-school code but it is definitely worth it. We will also benchmark the new implementation (there are lots and lots of improvements to be made, the implementation is rather naive)\nOne thing that makes me slightly nervous is that most of the functionaility is actually not unit tested. The class structure is rather complicated, with many switches etc. The unit test just covers really basic things. Could I suggest that you do another PR where you add a few unit tests before having the changes of this PR here? Then we can merge that first and have some more evidence that we did not break anything.\n. I will close this one for now and we continue the discussion in #2987 There seem to be some more structural changes needed in order to make this competitive\n. Great!\nSome intermediate things that I noticed:\n- the example files should not be named \"knn.sg.py\" but rather \"knn.py\"\n- The example code should not be in the sphinx folder, but rather in the global examples folder\n- Parsing and generating the examples should not happen within python sphinx. I would prefer if cmake would generate examples in target languages, and then sphinx just loads the files. Then we also can replace the current set of examples with the meta examples, and dont have two set of them, there will be lots of overlap otherwise. We then also work towards replacing the old API examples when we work on the documentation. Thoughts?\n- I dont think we should include this .. sgexample:: knn.sg:begin. The point is to not show the whole script. There should rather be a link to the whole listing on the bottom, but the text should not contain this. Thoughts?\n. @sorig check it out and let us know if you have feedback. Might also be nice for your writeup\n. All done, just need to merge now\n. - I would actually go to move all GP parameters to log-domain. We can still interface and store them in standard domain, but all dervatives in log-domain would be better for many reasons\n- inducing point model selection would be good to have I agree\n- not computing all gradients definitely is a good thing to have.\n- I would like to add more efficient (GPU, parallel) ways to compute gradients. A unified way to speed up is needed rather than all this ad-hoc code. What are your thoughts here?\n. I usually prefer general solutions that do things automatically to those that require extra arugments and thinking by the user. Users should not be able to decide whether a parameter is in log-domain or not, but we should do the appropriate thing automatically (in a transparent way)\nNot sure what to do though. How do other toolboxes do this? Any ideas?\n. I guess we cannot avoid the fact that Shogun developers have to specify the range of parameters when registering them. Therefore, I would go for optional lower and upper bound parameters. We can do those for scalars, vectors, and matrices (the latter two element-wise). For other constraints, such as positive definiteness, I guess we should stay with the current system. This also would be in line with e.h. stan's parameter representation. How would you do domains? With an enum DOMAIN_LOG, DOMAIN_STD, DOMAIN_TANH?\nOn the other hand, it might be an overkill as Shogun parameters are mostly unbounded or positive, so we just really need those two. Also modifying all the TParameter code is annoying (though not impossible, I could help there)\nNo matter what we do, we should be explicit and consistent throughout the toolbox.\nMaybe @lambday also has ideas?\n@lisitsyn ?\nBTW, what about just breaking all GP code and moving the GP parameters all to log-domain. The getters/setters can still be in normal domain though?\n. I guess we cannot avoid the fact that Shogun developers have to specify the range of parameters when registering them. Therefore, I would go for optional lower and upper bound parameters. We can do those for scalars, vectors, and matrices (the latter two element-wise). For other constraints, such as positive definiteness, I guess we should stay with the current system. This also would be in line with e.h. stan's parameter representation. How would you do domains? With an enum DOMAIN_LOG, DOMAIN_STD, DOMAIN_TANH?\nOn the other hand, it might be an overkill as Shogun parameters are mostly unbounded or positive, so we just really need those two. Also modifying all the TParameter code is annoying (though not impossible, I could help there)\nNo matter what we do, we should be explicit and consistent throughout the toolbox.\nMaybe @lambday also has ideas?\n@lisitsyn ?\nBTW, what about just breaking all GP code and moving the GP parameters all to log-domain. The getters/setters can still be in normal domain though?\n. okok, I think that should be a refactoring exercise\n. What are those comments based on ? Just the paper?\n40s vs 30m does not really seem too reasonable.\nI guess this is MAP inference right?\n. I see, MCMC is slow for sure\n. Good work @sorig \nApart from a few minors and the potential unit-wise operations on arrays, it is good to merge\n. Travis also seems happy\n. Yes, pls use element wise things that are new. Will make the code simpler and faster\n. Yes, pls use element wise things that are new. Will make the code simpler and faster\n. @sorig any estimate on when you have some spare time for this?\n. @sorig any estimate on when you have some spare time for this?\n. @sorig ping :)\n. @sorig it would be great if we could get this merged. The code is almost ready....\nCurrently it is not possible to merge because it has to be re-based\n. closing as inactive\n. As we discussed, this is a bug related to legacy code.\nThe method itself should probably be removed as the terminal might not be available in modular interfaces.\nWe should rather return an adequate data structure with the resulting parameters. But well\n. Fixed in #2820 \n. Seems fine to me! \n. And travis errors are unrelated too, merging\nThanks!\n. +1\n. All good!\n. Travis also agrees!\n. BUT we can bundle it like eigen3\nDont know. We just should get rid of all this hand-crafted code for that. It is\n- slow\n- unreliable\n- buggy\n- hard to maintain\n- impossible to read\n- blow up compile time\n. BUT we can bundle it like eigen3\nDont know. We just should get rid of all this hand-crafted code for that. It is\n- slow\n- unreliable\n- buggy\n- hard to maintain\n- impossible to read\n- blow up compile time\n. I did some more research and there does not seem to be any csv lib that is nice and bundled in many systems.....\nI still tend to just use this one\n. @vigsterkr I am happy to use any library. But I just want to\n- get rid of our own file parsing code (for the above reasons)\n- be able to read files bigger than 15k\nWhat do you suggest to do as next steps here?\n. @besser82 otherwise libcsv also sounds good.\n. @vigsterkr I just checked libvarchive. I don't get why you want to do that first? Why care about archives when we cannot deal with single files? Can you elaborate a bit?\n. This is still open I guess?. Travis failed due to undefined reference to get_feature_vector\n. Ok, merging!\nThanks!\n. Ok, merging!\nThanks!\n. @besser82 any ideas?\n. This is very nice stuff you added there.\nI think once all my comments are addressed, this is ready to be merged, no more iterations needed.\nReally fun to read\n. Amazing, merging!\n. Amazing, merging!\n. @nachitoys there are some python examples on this. We currently dont have a notebook. If you want to write one, that would be very welcome (and then we would have one ;) )\n. Sorry for the delay. I don't really know much about this. @sonney2k does, but he is busy, so I guess the only way to find out is to read code (and document afterwards so that this is sorted)\n. Wu, this notebook should really be in before the release.\nCan you prioritise?\n. Is this benchmark with compiler optimisation enabled?\nDo I get this correct\n- CPU element wise: 44 it/s\n- CPU explicit loop 18 it/s\n- GPU: 2413 it/s\nFactor 500?\n. Is this benchmark with compiler optimisation enabled?\nDo I get this correct\n- CPU element wise: 44 it/s\n- CPU explicit loop 18 it/s\n- GPU: 2413 it/s\nFactor 500?\n. If so, awesome :) Good to merge from my side! Check travis though, just restarted a few jobs\nAwesome work\n@sorig check it out, and make use of it in your kernel once this is merged\nAlso @sorig , once your project is done, are you interested in putting in the base class for stationary kernels we discussed? That pulls out computing the distances from the sub-classes and tries to speed them up with GPU?\n. If so, awesome :) Good to merge from my side! Check travis though, just restarted a few jobs\nAwesome work\n@sorig check it out, and make use of it in your kernel once this is merged\nAlso @sorig , once your project is done, are you interested in putting in the base class for stationary kernels we discussed? That pulls out computing the distances from the sub-classes and tries to speed them up with GPU?\n. Cool!\n@sorig feel free to use this in your patch!\n. @lambday awesome work!\n. Will this fix the travis errors due to memory limits? Would be cool if it did!\n. Will this fix the travis errors due to memory limits? Would be cool if it did!\n. Let me know what travis says and then I'll merge\n. Let me know what travis says and then I'll merge\n. Cheers!\n. It is good to have this option.\nBUT:\n- I don't like that users have to think about this\n- Gradients should be computed if model-selection is performed and otherwise not\n- without the user interacting\n- If users explicitly asks for gradients, should be computed lazily\n- So the setter for the flag is not the most elegant way to do all this\nThoughts?\n. Ok, please do the lazy one after the bugfix. We dont want users to need to be explicit about this\n. Dont know about this....We have irc.\nAlso I dont like bot PRs...\nwhat do you guys think?\n. Let's stay with IRC then\n. Any updates here?\n. Good stuff, thanks!\n. @lisitsyn has to comment here. I am not sure what is the most elegant way to go ahead here\n. Hi\nso see what you are after here is to have a way to exchange optimization methods in Shogun classes.\nLooks cool and elegant. So +1 in general. My questions are about necessarity\n- What does this gain us in general?\n- Most models in Shogun are fixed, and so are many inference algorithms. Is there really the option to change optimisers under the hood? Is that something the user should decide?\n- In GPs, we use all sorts of opt. methods, I guess you want to put the optimiser in the base class?\n- Then the question is: what is the gain as compared to doing this in the subclasses directly\n- Can you maybe come up with an example where this is used?\n. Yes, bug.\nWhy do we even have this class? See my other comment?\n. I think it should be removed\nWe should rather define masks for CFeatures in general, just like numpy does it\n. Again the question why we cannot just use the subset feature of DenseFeatures and just create a convenience class here?\n. No I agree then! Good!\n. Let's try it. But currently cannot merge, there is a conflict\n. For future: pls try to squash commits with redundant titles. Keeping the git log clean\n. Thanks for this!\nCould you pls clean up the commit history by squashing them? Then good to go\n. Good stuff!\nAgain just a few style things. Feel free to send another clean up patch.\nI cannot merge right now, you have to rebase\n. Great!\n. Hi @yorkerlin \nThis is good work. Don't worry about splitting ( unnecessary work now), just keep it in mind for future patches.\nI am fine to merge this, all my comments were about style. Consider sending a \"clean-up\" patch if you think some of the stuff is needed. I will check travis now and merge\n. Ok errors seem  unrelated. mergning\nThanks!\n. I would like to add some non-algorithm related points.\n- Benchmarks against other toolboxes. We want Shogun to be among the fastet ones, while having the most complete set of algorithms. This is really important\n- Notebook for all methods, with clear examples that maybe reproduce experiments from the relevant papers. Users get confused if we just offer 20 different inference methods. Need some decision tree what to use.\n- GPU and linalg stuff should be done on-the-fly. Chances are that most of the code is not touched again in quite a while, so I would rather go a bit slower but make full use of the linalg framework.\n. User guide:\nI suggest a tutorial style notebook, like the one you have, but with way less math and more hands on.\nBenchmarks:\n@lambday did some benchmarking, and I think we could use his ideas to do this in a principled way. Thoughts @lambday  ?\n. @Ialong also needs this for some work he is doing\n@yorkerlin you could also try to add it yourself.  Or @lambday can do it, he is fast in this stuff :)\n. Thanks for that, I will merge\n. Thanks!\n. Good stuff @yorkerlin I'll merge if travis is good\n. I feel there is a lot we can do here in terms of how elegant this problem of representing cost functions is solved. \nNot yet mergable due to some other issues\n@lisitsyn @lambday I would love to see your thoughts on such cost function representations? Seems like a very general problem.\nSee also #2877\n. Seems mergable\n. Hey, this is too much to review, could you split?\n. Hey @yorkerlin before I go on looking at this.\nWe need to change the workflow a bit. It is impossible to review this code. Too many files, too many classes. I also don't really get why we need this massive amount of new objects here. I see that you want to keep things flexible, but the code is quite cryptic -- not so much the individual files, the the collection of classes all together.\nI suggest\n1. Class diagram that describes the high-level structure (no details yet)\n2. Discussion, also with other devs. Cleaning up the design. @lisitsyn and @lambday should also have a look.\n3. Updates and iterating over 1-2\n1. Seperate PRs for the base classes, with documentation focussing on the high-level interaction between classes.\n2. Atomic PRs for the individual implementations, along with unit tests\n3. Benchmarking and notebooks\nWe cannot continue to merge more of this stuff in the current way -- it has gotten too complicated. I realise you want to push on implementing the cool algorithms, but I ask you to slow down a bit and make sure the fundamental framework is good before filling it with algorithms.\n. I generally agree with most of this. One thing I just thought is that a way to reduce overhead when adding Shogun classes would be to have a hierarchy of base classes with different functionality. See #2880\n@yorkerlin One thing that needs to be worked out is how to manage memory with these new classes. Somehow needs to delete instances .... who would that be?\nApart from that, there are a few minors but these base classes look good to merge (after minors are resolved)\n. Hi @yorkerlin \nVery nice, we are getting there.\nWhile I agree with you that this PR is about class design, the interface documentation is very important in that -- because it helps me understand what is going on, etc. I always see that as a part of the class design.\nI am quite happy with the current set up. A few things need discussion\n- memory management of the classes\n- The serialisation seems slightly cumbersome with these chains of the \"store context\" calls\nI have the feeling that we can maybe resolve both issues via putting in more structure in the Shogun base classes. For example, reference counting can definitely be solved. See the issue (feel free to participate in discussions there).\nI think this is almost mergable though, but we have to keep the above points in mind and should resolve them eventually.\nVery nice framework otherwise. Really like that.\n. @yorkerlin It turns out CSGObject inherits from SGRefObject, which you can use as a base class for all your new classes. Then they have automatic reference counting (SG_REF/SG_UNREF), but nothing else.\nThis solves the above issues\n. https://github.com/shogun-toolbox/shogun/blob/9305aa9d1bab28d20979033b0fd7c8e2d545b85e/src/shogun/base/SGObject.h\n. Yeah I know, just pointing out that we can solve things this way.\nJust requires some minimal work. But you can go withou the reference counting for now and then we update. Good?\n. @lisitsyn you added the auto. Any ideas?\n@vigsterkr might also know\n. @yorkerlin good point with the sparse vectors\n. This is getting there, ready to merge in my eyes. \nIf the guys dont reply about the auto problem, lets just change it back to standard and merge\n. Related to:\n- #2872\n. Turns out the stan people finally took out their math library of the rest of the framework:\nhttps://github.com/stan-dev/math\n. Could you send the PR against the feature branch in shogun?\n. I will close this one for now. Feel free to re-open and continue to work on any time. It still would be good to have this\n. Could also add another layer that provides all the sg_* fields\n. Why multiple inheritance? That's not my idea.\nIf the base classes are a hierarchy, one could just inherit from a certain stage in there? \n. Thats very similar, but for the SGMatrix, SGVector classes.\nExactly that type of thing is what I want to do for the base class CSGObject.\n. What about this:\nCReferencedObject -> CSerialisableObject -> CSGObject\nWhere the first one contains only the reference counting, the second adds the parameters/serialisation, and the third adds all the Shogun stuff\n. I see now. This is good enough for @yorkerlin s patch. Thanks I did not know\n. yeah smart pointers would be best.\n@lisitsyn what s your take on all this?\n- making the base class overhead much smaller\n- serialisation\n- reference counting?\n. @sonney2k why was the SGRefObject removed?\n. I dont get the point of removing the class. We could just hide it from SWIG?\nIf one class contains what two classes used to contain, nothing is really gained. We could revert that?\n@lisitsyn \n- remove migration at least\n- cant we use a library for serialisation? why re-invent the wheel there (and we do it badly, its slow as f***)\n- +1 for shared pointers\n. @yorkerlin what do you think?\nThis helps you with the reference counting if we merged it\n. updates here?\n. closing as inactive\n. @yorkerlin hi, it might be a good idea to talk to John if you are doing VW related work. We always wanted to do another round of integrating Shogun and VW closer to each other. Send me an email if you want me to introduce you\n. I think if we put linear solves /factorisations in the linalg package we will have a few very easy to use options:\n-perform the solve on the GPU directly, there is some gains to be made: http://gamma.cs.unc.edu/LU-GPU/\n-for sparse systems or conjugate gradient approaches, we can load the matrix to GPU and then the the matrix vector multiplications on the GPU. This gives a speedup too\n-There are also hybrid Cholesky factorization algorithms, that use the GPU. http://www.netlib.org/utk/people/JackDongarra/PAPERS/tile_magma_journal.pdf\nI think the way to do this is incremental once again. \n1.) should put all the inverse matrix stuff in linalg interface, with basic implementations\n2.) Make use of it where we can in Shogun, in particular the GP part\n3.) Add the backends that speed things up\n. modshogun was renamed to just shogun. updates here?\n. can you remove the NATIVE, and just use the default backend (is set to eigen3 these days)\n. Hi guys,\nsuch patches should be labelled properly. Rather than \"fix a typo\", it should be \"add forgotten return 0 statement\"\nIt is impossible to sort things later otherwise, or track down bugs from commit logs in the buildbot\n. Thanks Wu\nPlease try to write more meaningful error messages:\n\"Fix python_modular warnings by adding API documentation\"\n. Ah cool, I did not see that back then as I was in holiday.\nIs there any way we can push them so that this does not die ?\n. ha! next time I take my computer when cycling through Europe ;)\nDont know how to get people merge things, a ping is always good I guess\n. Lets just stay on this, ask periodically....sometimes helps ;) But it's a thin line to annoyance ;)\n. @lisitsyn this looks good doesnt it?\nShall we merge before release?\n. @iglesias @vigsterk Any objections?\n. Here we go\n. closing as per dicussion\n. sure, I agree, but this patch here broke csharp modular.\nI just try to get the build working again for the release. \nHappy to fix that properly after.\nWhat is your suggestion with the typemap?\n. I don't know how to do it and nobody else does it.\nOr can you prove me wrong? :)\n. The error you have actually comes from the fact that there is no feature data set internally. This can have multiple reasons. Let's try to isolate the problem a bit....\nWhat happens if you ask the feature object for its number of vectors and features? Or for the full matrix?\nfeats_train = SparseRealFeatures(train_data)\nprint feats_train.get_num_features()\nprint  feats_train.get_num_vectors()\nprint feats_train.get_sparse_feature_matrix()\n. Great, thanks @lisitsyn \n. Good. Want to start a PR?\n. We are more than 50%\nIf it is if interest for you we can push this pretty sokn. You can see some of the code is guarded via USE_GPL_SHOGUN.\nAll guarded files will eventually be moved to a separate repository (see feature/gpl branch).\nThe rule is:\n * We got all Shogun devs accepted to move their code to BSD.\n * There is some external code that is GPL and so do the Shogun classes that depend on it\n * If you find any GPL code file in Shogun that is not guarded (and not written by our own developers), sending a PR with the guard (that doesnt break the build when the cmake flag is turned on/off), helps big time.. Thanks for pointing this out\nWe can easily require a certain doxygen version with changing the line\ncmake\nFIND_PACKAGE(Doxygen)\nin  shogun/CMakeLists.txt to\ncmake\nFIND_PACKAGE(Doxygen 1.8.6)\nFeel free to send a patch to do that. But before, really make sure that starting a build from scratch with that doxygen version causes the error your described.\n. Ok then I have sent a patch\n. Before I put that stuff in, can't you edit the wiki on your machine and then send a PR with that?\n. Argh, apparently not.\nif you can copy the file locally, edit, and then send me an email with it, I can review and merge your changes\n. Gist is also fine\n. Great, thanks a lot @sanuj \nOne request. Could you respect the 80 character margin? As was done in the file before? This makes reading in text editors easier.....\n. Thanks @sanuj I merged it. See here\n. This is way too old. Thanks @besser82\n. Travis error seems not to be caused by doxygen, merging\n. please dont put such logs in the PR thread, but into an issue, and hidden behind a gist link\n. Way too old as well, feel free to revive whenever you want . Haha....yes definitely.\nWe all do this every now and then. It is hard to keep such things up over a long time.\nGenerally, I am all up for closing issues that are \"old\".\nWhat do you suggest to make us look really efficient?\n. I think it is better to document all thoughts, problems, bugs etc than to ignore them.\nAt least new people can choose from a large set.\nThe art is to group the issues we want to solve for a next release. In fact, we did this in the past. Feel free to suggest things here....\n. news?\n. hey @yorkerlin \nCould you take care of this? We should merge such small things asap if generally ok\n@arasuarun Lets wait for Wu to get  back, otherwise I can also merge it. But Wu is the expert\n. ping :)\n. This script is deleted now, so closing\n. Merging as only doxygen comments were modifed\n. Merging as only doxygen comments were modifed\n. merging as only doxygen doc updates\n. merging as only doxygen doc updates\n. Great to hear! :)\n. absolutely\n. Thanks a lot @kno10 for investigating this. I totally am for dropping such disasters, as they are doing more bad than good.\nIn fact, I am thinking about a GSoC project that is just about going through our \"simple\" ML algorithms, benchmarking them a bit, identifying problems (as you did here), and then either dropping or re-writing.\n. @kno10 I just had a look. If you are interested, you could attempt a re-write that solves the above problems. This might be a nice entrance task for the mentioned GSoC project as well\n. No worries. Thanks!\nI used your report in a GSoC project proposal here:\nhttps://github.com/shogun-toolbox/shogun/wiki/GSoC_2016_project_fundamental_usual_suspects\nHopefully we resolve a few of those during the summer!\n. Good to see you back here @Saurabh7 \nThanks a lot for this example. These results are quite extreme. Can you post the scripts that generated them somewhere? (Your own public repo for example). I would like to have a look at them.\nDo the methods produce the same results?\nWe now have to find out what makes the other implementations so fast (and copy the design). Then we can attempt to fix things.\nThe Shogun (new) version that is based on the discussions above seems already much faster, but still way beyond the others -- so maybe rather than re-structuring we can have a look at the other libs and attempt a re-implementation from scratch....after all KMeans is a dead simple algorithm.\nOne reason for speed differences might be multicore and vectorised linear algebra, which are relatively straight forward to put in our too.\nWhen re-implementing/re-structuring, we should make use of linalg. @lambday can probably help here as well as he ran benchmarks before.\n. Thanks! Some comments:\nSanity check: you build the shogun library with compiler optimizations enabled right? That is DBUILD_TYPE=Release ? and the script as well?\nI agree with all the other points. Using std anywhere is not really necessary. \nI agree with the documenting/testing. However, if the current code design is slow and needs to be replaced anyways, there is not really a need for this. Maybe as a first step, you could have a look into the sklearn implementation to check what they are doing from a high level perspective (same data-structures and operations?). This should give an answer to the question why it is so fast compared to other libs.\n. Very good investigation @Saurabh7 \nI think we should focus on making our distance computation faster then since that seems to be a major game changer. AFAK our Gaussian kernel pre-computes the squared norms of the diagonal of the kernel matrix. Maybe we can change our CDistance class to do the same here. \nBut let's go very systematically:\n1. Use a precomputed distance and benchmark against a precomputed distance in sklearn. This gives us an estimate on how much the distance computation makes a difference. There is the CCustomDistance in Shogun, dont know how to do this for sklearn, but you could use a hack. Based on the outcome of this, we can identify more potential bottlenecks.\n2. use precomputed x_squared norms in Shogun's Euclidean distance. The CGaussianKernel has inspiration for this.\n3. use linalg dot products to compute the distance rather than the loop over elements\n4. Sometimes computed on multiple data. Look out for loops over distance calls. This should be done via a single call that uses dot products between matrices and then using linalg. We might need to add a method distance_multiple(SGVector indices_a, SGVector indicesb) to the Distance class for this.\n5. Finally, we could parallelise on multi-cores. But let's keep that out for now\nThe other points, can you comment on each?\n- scaling every row/column in a matrix by a number should be added to linalg. @lambday can you have a look at this?\n- random init as you described sounds good. How does scikit do it?\n- the `shogun_2`` looks much better. But I did not get this \"constant cost\" reason.\n- Convergence. I agree, the WCSS is maybe a better idea. But let us only do this after the distance thing is solved.\n. Thanks for your comments @kno10 \n1. You are right. But precomputing the distance allows to benchmark the algorithm itself, without the time spent computing distances. Therefore, it is instructive to hunt for bottlenecks. Especially if we cannot gain a lot via point 2.) we have to find another handle on speed.\n2. True. Memory managment is key. @Saurabh7 maybe you can profile the memory usage to see what is going on, I saw a few \"copy\" commands on matrices when I had a look earlier. However, precomputing the squared norms helped in the Gaussian kernel case. Doesn't hurt for sure. And we have to start somewhere.\n3. Agreed. @Saurabh7 maybe this can be made an option later (and turned on by default for the minibatch version)\nSetting the sklearn tolerance is a good idea to compare\n. Ah crap, you are of course correct. Too many things going on at once. Precomputing is not useful at all. Thanks for the hint!\n@Saurabh7 how are things going?\n. Great, let us know when you had a chance to continue\n. Great, let us know when you had a chance to continue\n. - Thanks. Interesting that we are still slower! Shogun being slower should not be the case if the Implementation is similar\n- The linalg dot is interesting -- it should also not be the case ...\n- For convergence, did you set the sklearn criterion to mimic Shoguns one as we discussed above?\n- Also, could you put your codes into a notebook and post the link here, so that we can see what is going on in a single file? Like #2991\n- Finally @kno10 is really right. The implementation is overly complicated. At some point, we should make it simpler to read\n. Nice!\nOk, so it seems sklearn does somehow better for large k, where for small k the Python overhead makes it slower. This must be some systematic thing as well. However, this is great work and we should get it merged in the next step. Can you send a PR? \nThe other thing you might want to check is using openmp to parallelise the distance computation among multiple cores. BUT I would do that after your current changes are merged.\n. This is now solved\n. closing as never meant to be merged\n. Yes, you might want to get rid of the LAPACK calls in KRR\nThese can be replace with Eigen3 calls.\nUnit testing is also not existing here.\nIn addition, if you feel brave, you might want to add a linear solve to our linalg library. @lambday can give hints here\n. Nice one, feel free to send a patch with corrections\n- In linear regression, the bias term is the data mean. You can compute and add that explicitly. This is a great catch and should be fixed. Definitely, Shogun should take care of that by default. \n- you could get rid of the ugly lapack call cblas_daxpy \n  - either replace with a linalg solve. For that you would have to add that, see the Readme\n  - use eigen3 for the solve\nFinally, could you test this on a larger dataset in higher dimensions, can be synthetic as this is just for speed. Both N and D should be in [small, medium, large] where\n- small: few hundred\n- medium: 2000\n- large: >10000\nMake sure the Shogun implementation at any point in time only has one matrix in memory (i.e. 10000x10000). You can profile the memory usage as well.\n. Nice one, feel free to send a patch with corrections\n- In linear regression, the bias term is the data mean. You can compute and add that explicitly. This is a great catch and should be fixed. Definitely, Shogun should take care of that by default. \n- you could get rid of the ugly lapack call cblas_daxpy \n  - either replace with a linalg solve. For that you would have to add that, see the Readme\n  - use eigen3 for the solve\nFinally, could you test this on a larger dataset in higher dimensions, can be synthetic as this is just for speed. Both N and D should be in [small, medium, large] where\n- small: few hundred\n- medium: 2000\n- large: >10000\nMake sure the Shogun implementation at any point in time only has one matrix in memory (i.e. 10000x10000). You can profile the memory usage as well.\n. BTW the docs  state that the bias is set to 0. You can double check them as well, and clean up if there are problems. \nBut even if the docs mention the 0 bias, we should still change default behaviour\n. Can you also check whether the algorithm works when a preprocessor is attached to the features?\nThere are for example zero-mean preprocessors.\nBTW I think about the bias, we should have a boolean flag in the constructor that allows to turn off the bias (e.g. when the mean was remove already), where the default is that it is on.\n. @youssef-emad Could we move the discussion in its own issue? You can just open one if you have updates...\n. @Anjan1729 there is a long list above. Pick any of the ones you like. Easy algorithms preferred, what about LDA?\n. Hi @abhinavagarwalla Great! Can you open a seperate issue for this and we discuss there? We should keep this thread clean of discussions\n. Thanks, very nice work. @vigsterkr is right, these notebooks can only be a first step to see what is going on. Eventually we want the scripts to do this in the benchmark platform -- so that we can re-run. But of course the notebooks are a nice way of exploring.\nA few words on the benchmarks:\n- Aim for benchmarks that take at least a few seconds to run. Otherwise we only observe noise of the python interpreter being fired up. We want actual runtime. One or two short ones are ok to test this overhad. But in general, what counts is performance on longer runs.\n- N is large enough -- but can be larger (see first point) , what about larger D?\n- easy problems usually run faster than hard ones. Try to create harder regression problems -- higher dimensions -- more noise. Using N=20000, D=3 and linear functions does not represent an interesting problem.\n- Make sure the regulariser is set to the same value in both shogun and sklearn\n- if you find bugs (like a crash or wrong result), always report them in an issue and give a way to reproduce the bug\n. Thanks, very nice work. @vigsterkr is right, these notebooks can only be a first step to see what is going on. Eventually we want the scripts to do this in the benchmark platform -- so that we can re-run. But of course the notebooks are a nice way of exploring.\nA few words on the benchmarks:\n- Aim for benchmarks that take at least a few seconds to run. Otherwise we only observe noise of the python interpreter being fired up. We want actual runtime. One or two short ones are ok to test this overhad. But in general, what counts is performance on longer runs.\n- N is large enough -- but can be larger (see first point) , what about larger D?\n- easy problems usually run faster than hard ones. Try to create harder regression problems -- higher dimensions -- more noise. Using N=20000, D=3 and linear functions does not represent an interesting problem.\n- Make sure the regulariser is set to the same value in both shogun and sklearn\n- if you find bugs (like a crash or wrong result), always report them in an issue and give a way to reproduce the bug\n. Good luck with the assignment :)\n. Good luck with the assignment :)\n. Great that this finally happened! For the benchmarks, I guess we need to compare against something, e.g. sklearn. As I can see here, the comparison so far only happens in your notebook?.\n. Great that this finally happened! For the benchmarks, I guess we need to compare against something, e.g. sklearn. As I can see here, the comparison so far only happens in your notebook?.\n. In the notebook, it would be great if you could label things you print, it takes some time to parse this otherwise for me\n. In the notebook, it would be great if you could label things you print, it takes some time to parse this otherwise for me\n. Comments:\n- Use larger datasets -- timings at microsecond scale have very little meaning as Python takes longer to start running than the actual computations.\n- You should always make sure that the options are set in such way that the algorithms do exactly the same\n. Comments:\n- Use larger datasets -- timings at microsecond scale have very little meaning as Python takes longer to start running than the actual computations.\n- You should always make sure that the options are set in such way that the algorithms do exactly the same\n. Looks like you identified problems in Shogun's results. These should definitely be investigated. Very alarming. I suggest that you put benchmark scripts for sklearn as well. Also, let's isolate one of the algorithms and try to understand why Shogun's results are so different\nVery nice catch btw :) !\nBut I dont really agree with your conclusion ... we better find out whats going on there\n. Looks like you identified problems in Shogun's results. These should definitely be investigated. Very alarming. I suggest that you put benchmark scripts for sklearn as well. Also, let's isolate one of the algorithms and try to understand why Shogun's results are so different\nVery nice catch btw :) !\nBut I dont really agree with your conclusion ... we better find out whats going on there\n. Start with one of the algos maybe. KNN or so\n. Thanks! :)\n. @sonney2k @vigsterkr I think this might be easy to solve, but I dont know how?\nShould be back before GSoC I think, to attract people, getting more applications in, etc\n. Can you build Shogun on your system without CLion?\n. Hi @Soham0, welcome!\nIt is better to introduce yourself (and ask such questions) on the mailing list. We should discuss issue related things here to keep the thread clean.\nApart from that, @lgoetz is absolutely right, try to build a prototype of a Shogun class that is serialised with cereal. This task might be a bit more tricky than others, so only attempt it if you know a bit of c++\n. We have something as a result of pan's gsoc2016 project\n. Hi there,\ngreat, these are exactly the two files we need. Please send a Pull request against the cookbook feature branch (link above) rather than posting the files here. I will review it in there then.\nA note for the others -- there are many more examples to go\n. This is just the link checker failing, are you online?\nAlso pull the latest version, it works on my machine\n. This is just the link checker failing, are you online?\nAlso pull the latest version, it works on my machine\n. @tbskm I like the gaussian naive bayes example! Will merge soon.\n. @sanuj lets not discuss this here ... irc maybe?\n. Recently chatted to Viktor, and we think the best way to do this would be to create a shogun ppa for ubuntu first, and then to simply install this ppa on the docker image ...\nLooking at this image, I do not really understand the different to Shoguns development docker image\n. This is now in develop branch\n. Can you write a few sentences in the intro. Say that you care comparing the decision boundaries and classifier scores on two datasets, where one is linear seperable, and one is not\n. All plots should have axis labels and titles. For axis labels you can use latex, e.g. plt.xlabel(r\"$x_1$\") or similar\n. ```\ntrain\nlinear_svm_1.train()\n```\nminor: the train comment is not necessary if the method is called train ;)\n. I think it would be a good idea to restructure things a bit.\n- Use more instructive names than shogun_features_1/2, e.g. feats_linear, feats_nonlinear\n- Have a method that visualises a single classifier\n- Avoid linear_svm_1, linear_svm_2, in a single cell, but just build a single model and call the above method:\nc  = 0.5\nepsilon =1e-3\nsvm = LibLinear(c,feats_linear,labels_linear)\nsvm.set_liblinear_solver_type(L2R_L2LOSS_SVC) # would be cool to offer a constructor call to avoid this and the next method. Feel free to send another PR.\nsvm.set_epsilon(epsilon)\nsvm.train()\nplt.subplot(121)\nplot_model(svm)\nsvm.train(feats_nonlinear, labels_nonlinear)\nplt.subplot(122)\nplot_model(svm)\n. Avoid Python warnings\nFutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n  if self._edgecolors == 'face':\n. Avoid outputs such as\nOut[11]: True\n. Avoid outputs such as\nOut[11]: True\n. All the kernels and classes mentioned should contain a link to the class docs.\nUse it like CGaussianKernel with the shogun.ml/CGaussianKernelshortcut in the link\n. All the kernels and classes mentioned should contain a link to the class docs.\nUse it like CGaussianKernel with the shogun.ml/CGaussianKernelshortcut in the link\n. Really nice work.\nAs a final plot, could put all those plots in a big grid like this?\n. Really nice work.\nAs a final plot, could put all those plots in a big grid like this?\n. Thanks for the patch.\nWe usually do a couple of iterations over such notebooks before we merge them. It might seem a bit tedious to react to all those comments, but it really increases the quality of our public notebooks -- once uploaded, they usually do not get touched again, so we gotta make them nice in the first place ;)\nLooking forward to the update\n. Thanks for the patch.\nWe usually do a couple of iterations over such notebooks before we merge them. It might seem a bit tedious to react to all those comments, but it really increases the quality of our public notebooks -- once uploaded, they usually do not get touched again, so we gotta make them nice in the first place ;)\nLooking forward to the update\n. See datasets here\n. See datasets here\n. Minor typos:\n\"Shogun.The\"  -- missing space after period\n\"This Notebook\" -- small n\n. Minor typos:\n\"Shogun.The\"  -- missing space after period\n\"This Notebook\" -- small n\n. Can you give all your Python functions one-liner comments what they do\nPython\ndef function(...)\n\"\"\"\nThis functions does nothing\n\"\"\"\n. Can you give all your Python functions one-liner comments what they do\nPython\ndef function(...)\n\"\"\"\nThis functions does nothing\n\"\"\"\n. \"Shogun provide Liblinear which is a library for large- scale\"\nshould be\n\"Shogun provides Liblinear, which is a library for large-scale\"\n. \"Shogun provide Liblinear which is a library for large- scale\"\nshould be\n\"Shogun provides Liblinear, which is a library for large-scale\"\n. Naive Bayes\nyou changed font size of the links from here\n. Naive Bayes\nyou changed font size of the links from here\n. Great, so this is ready to be merged :)\nIn order to do so, please remove all cell outputs -- we dont want the pictures to be in the file.\nAlso please make sure the notebook runs through from scratch without error (I cannot check this here)\n. Great, so this is ready to be merged :)\nIn order to do so, please remove all cell outputs -- we dont want the pictures to be in the file.\nAlso please make sure the notebook runs through from scratch without error (I cannot check this here)\n. Sorry about the comment, it was unclear.\nTry to make \"Support Vector Machine\" also a link to the CSVM class docs, just like Naive bayes\nSame for \"SVM - Kernels\"  -- mention the CKernelMachine interface\n. Can you also link the CKernelMachine base class when you first talk kernel SVMs?\n. and for the output, leave the matplotlib inside (we need it), but just select cell->remove all outputs in the menu of the ipython notebook\n. Nice one! I will merge it! This our first larger GSoC student patch :)\n. I agree on the docstrings -- in fact I put them in my above comment. Can you update, @youssef-emad ?\n. yes, a small one :)\n. Thats the spirit :) Great way to introduce yourself with a patch! Thanks -- lets get it merged!\n. merging -- avoid the branch to diverge more\n. merging -- avoid the branch to diverge more\n. Hi @sanuj so CKernelRidgeRegression is the dual form of KRR, where a NxN matrix is inverted (the kernel matrix). With random features, this will be a MxM matrix where M is the number of random fourier features. Therefore, what we need to do here is to do a linear ridge regression in the random fourier feature space.\nIf it is already possible to do this, then you can create a notebook on approximate kernel methods. But I have the feeling some changes in the code might be needed. Looking forward to the outcome\n. Hi @sanuj so CKernelRidgeRegression is the dual form of KRR, where a NxN matrix is inverted (the kernel matrix). With random features, this will be a MxM matrix where M is the number of random fourier features. Therefore, what we need to do here is to do a linear ridge regression in the random fourier feature space.\nIf it is already possible to do this, then you can create a notebook on approximate kernel methods. But I have the feeling some changes in the code might be needed. Looking forward to the outcome\n. mmh\nI mean KRR with random features is exactly linear ridge regression in a different feature space.\nSo the transformed features should just be passed there -- better re-use existing code. But a wrapper class might help, call it CApproximateKRR -- it is not limited to random fourier features, there are other ways to do finite feature space approximations. Also looking at scikitlearn might be a good idea to see how they do it. The paper is exactly the one you should read :)\nExamples you should put should reproduce some of the results in some papers.\n- illustrative example\n- x-axis: number of rff features, y-axis performance on test set. All this for fixed number of data (large)\n- The idea is that you show the effect of the approximation both in training time and in accuracy\n- Pick a few synthetic and a few real world examples (see papers for inspiration)\n- first step is to make it work and have a nice interface, examples come later\n. mmh\nI mean KRR with random features is exactly linear ridge regression in a different feature space.\nSo the transformed features should just be passed there -- better re-use existing code. But a wrapper class might help, call it CApproximateKRR -- it is not limited to random fourier features, there are other ways to do finite feature space approximations. Also looking at scikitlearn might be a good idea to see how they do it. The paper is exactly the one you should read :)\nExamples you should put should reproduce some of the results in some papers.\n- illustrative example\n- x-axis: number of rff features, y-axis performance on test set. All this for fixed number of data (large)\n- The idea is that you show the effect of the approximation both in training time and in accuracy\n- Pick a few synthetic and a few real world examples (see papers for inspiration)\n- first step is to make it work and have a nice interface, examples come later\n. Thanks for the patch!\nBut I am a bit confused whether this actually helps. Could you both explain and clean up?\n. I see.\nThis is not a good way of choosing k in N without repetition. Better way is to create a permutation from 1,...,N and then select the first k values.\nCMath has a method to permute, and SGVector a method range_fill which you can use for that.\nA comment saying that \"initialise the cluster centroids randomly among all data\" would be helpful as well. Also make sure to update the class documentation if you change this.\nThanks and looking forward to the update\n. yes, it would be cool to add this in the notebook -- it is a nice and small update. Send it in a separate PR.\n. Getting there!\n. Oh btw, can you also double check the doxygen documentation of this method? And correct if necessary?\n. Great.\nNow what about adding a unit test for this random initialising method?\n2 tests: It should check that all clusters are initialised to one of the data points, and that there are no 2 clusters that are the same\nFor a very small dataset (3 points) and small (nontrivial) k (2)\n. Great.\nNow what about adding a unit test for this random initialising method?\n2 tests: It should check that all clusters are initialised to one of the data points, and that there are no 2 clusters that are the same\nFor a very small dataset (3 points) and small (nontrivial) k (2)\n. Also, it looks like you broke something. \nhttps://travis-ci.org/shogun-toolbox/shogun/jobs/113943490#L3923\nAlways run tests locally before sending a PR. At least the libshogun and the python examples\n. Also, it looks like you broke something. \nhttps://travis-ci.org/shogun-toolbox/shogun/jobs/113943490#L3923\nAlways run tests locally before sending a PR. At least the libshogun and the python examples\n. run tests via passing -DENABLE_TESTING=On to cmake, and then make unit-test.\nYou can run single tests via the shogun-unit-test binary and passing options. See the docs\n. run tests via passing -DENABLE_TESTING=On to cmake, and then make unit-test.\nYou can run single tests via the shogun-unit-test binary and passing options. See the docs\n. any news?\n. Travis is horribly broken, it doesnt compile!\nPlease dont send PRs with code that does not compile\n. I guess we will just use @Saurabh7 's changes. This is a pitty, we would like to avoid such redundant work in the future. But at least you learned a bit.\n@Saurabh7 please have a look in here and if there is anything that you can re-use, please do.\nIt would be cool to get KMeans into mlpacks benchmark system (or at least run the new version to compare shogun with other libs). So maybe one of you can do that? Remember in open-source communities, collaboration is key! :) But you can also move to another entrance task if you prefer\n. Great! Thanks for that.\nYou can also add the new class to the swig interfaces. See kernels.i in interfaces/modular\n. Make sure you run all tests locally. Some of them will break.\nA unit test for both Gaussian and compact kernel would be a great addition to this patch. But I am fine merging before that even -- once the other minor points are resolved\n. @tonmoy-saikia Documenting the unit test in the readme is a good idea. But please send a separate PR for this. Also check/grep unit-test in all readmes for this content before you do this to avoid redundancy\n. we use tabs, not spaces, and we use tabs of size 4\n. Good stuff. I will merge this.\nA unit test is the next step!\n. @vigsterkr Thanks, that slipped through somehow. Feel free to merge if you agree otherwise\n. @vigsterkr thanks for taking a look. Addressed all of them\n. This is why we need this GSoC project!\nSorry I am of no help here, but maybe @lisitsyn has an idea?\n. Thanks!\n. I agree in principle! This has many nice advantages.  In particular the fact that once the interface is fix, one can update/improve linalg without having to re-install Shogun.\nHowever, I have some second thoughts as well.\nOne thing is that if we have a seperate lib, this one will depend on the Shogun core. In that way it will never be possible to use it as its own library. On the other hand, we to use it in Shogun as a hard requirement. This means that we have a two way dependency. So making it separate is merely a restructuring of the src directory ....\nThere is quite a bit of hassle involved in creating a build for both systems, with seperate cmake environments etc. On the other hand, the math library would be c++ only, so not many builds added.\nI summary, I am pro, but advise some care when doing this. Also, if this transition is done, it should be done in a very short time window to avoid all sorts of legacy artefacts. @lambday @besser82 if you want to lead this, I would be happy to help.\nBut also curious what @vigsterkr says\n. Yes the connection to the SGTypes is what I meant. I am just afraid that we start doing this and then get stuck somewhere in the middle. In my eyes, we should focus on actually integrating linalg first, and building the plugin framework first. Once both of those things are done, we can put it in a separate lib. Having said that, we can obviously already start drafting something. But I would be careful to open another big construction side at this point in time -- so many other things going on. Apart from that, I am all up for it.\nLet's focus on integrating/finishing linalg first:\n- Make use of it in Shogun code\n- Put in operations that are desperately needed (cholesky, inverse)\n- Make sure the GPU integration works and is used in Shogun\n- Remove NATIVE backend ....\n- etc etc\n. What is the point of opening this exactly? :)\n. The point you asked why the kernel matrix is stored is simply: because the solution depends on a linear system involving the kernel matrix\n. Don't worry about such questions ;)\nYes definitely this is a separate task. So I will leave this open until it is done\n. Renamed it, this should actually be done in the base class -- every linear algorithm should the option of pre-computing the data mean manually, and do it automatically by default\n. sure it is a virtual function, but computing the data mean still should not happen in specialised classes, but in the base class. We can always add an additional method that calls a virtual method that is overloaded in subclasses...\n. In the notebook, why are the results not exactly the same? Can you check that the ridge has the same value?\n. What about larger datasets and dimensions? Timing might change then?\n. Ok had a look at the code. The bias addition should happen in the base class as said, and it should be possible to set the bias manually and disable that it is computed from data.\nCan you send a PR with the changes? I have a few more comments, but don't want to put them in your branch\n. The sub-classes will not compute the bias as it currently has to be set manually.\nTherefore, one way to do this would be to rename the train_machine methods (that dont compute bias) and add an additiona lmethod compute_bias() in the base class. Train_machine in the base class then (optionally) computes the bias and then calls the train_machine_no_bias of the specialsed classes.\nSomething like this, standard polymorhpic OOP\n. The sub-classes will not compute the bias as it currently has to be set manually.\nTherefore, one way to do this would be to rename the train_machine methods (that dont compute bias) and add an additiona lmethod compute_bias() in the base class. Train_machine in the base class then (optionally) computes the bias and then calls the train_machine_no_bias of the specialsed classes.\nSomething like this, standard polymorhpic OOP\n. Thats Indeed much better and actually the reason why we have this structure in the base class\n. Thats Indeed much better and actually the reason why we have this structure in the base class\n. Yes, the matrix is DxD where D is number of features.\nWe could implement a stochastic version of this, but even better would be to use an existing library to do so.....Vowpal wabbit for example\n. Can you please not paste logs directly in the chat? Nobody will look at this if you do. Post a gist.\nAlso, check your filesystem\n. Can you please not paste logs directly in the chat? Nobody will look at this if you do. Post a gist.\nAlso, check your filesystem\n. Very nice and polished PR! Fine to merge from my side apart from the minor things I commented.\nSomebody else has to check whether the computation itself makes sense though.\n. Very nice and polished PR! Fine to merge from my side apart from the minor things I commented.\nSomebody else has to check whether the computation itself makes sense though.\n. About loops.\nI think we can merge this before putting in openmp.\nBUT all linear algebra operations should be done with linalg. Never compute dot or matrix products by hand.\nOpenmp would be a next step. I think this should be done globally in the NN package (spend some time thinking about the design), and for sure a seperate PR. Agree @vigsterkr ?\n. About loops.\nI think we can merge this before putting in openmp.\nBUT all linear algebra operations should be done with linalg. Never compute dot or matrix products by hand.\nOpenmp would be a next step. I think this should be done globally in the NN package (spend some time thinking about the design), and for sure a seperate PR. Agree @vigsterkr ?\n. Would be a super useful piece of work to parallelise/linalg everything in NNs....\n. Would be a super useful piece of work to parallelise/linalg everything in NNs....\n. news?\n. Sure totally fine. \n. Sure totally fine. \n. Nice patch.\nI agree with @vigsterkr please use linalg, eigen, and openmp to make this stuff fast\n. Nice patch.\nI agree with @vigsterkr please use linalg, eigen, and openmp to make this stuff fast\n. news?\n. Few things that block this being merged.\n- travis is failing, your code does not compile\n- Shogun's NN should be refactored 1by1, not all at once (small PRs!)\n- Put all the open mp stuff already in. Once something is merged, it usually is unlikely that it is touched again, so we want to make it as good as we can in the first merge\n. Ok I see, you are right. I still am unsure whether this addition is really a good idea as people might forget about it and then get wrong results without a warning or an error.\nThe final option I see is to add a method to compute pairwise distances for all features rhs/lhs. In this case, we can definitely precompute the norms internally. I think this would also fit into KMeans wouldnt it?\nYou can use subsets on the features to specify which indices you want to use.\nLet me know what you think about that!\n. Storing this matrix is a bad idea indeed.\nI think let's maybe keep it as it is now, no need to let users pass things ...\nBut make sure to add reset() calls in all places where the features do get changed, to avoid problems described above.\n. Storing this matrix is a bad idea indeed.\nI think let's maybe keep it as it is now, no need to let users pass things ...\nBut make sure to add reset() calls in all places where the features do get changed, to avoid problems described above.\n. Good to merge apart from the minors. Can you also squash all this into a single commit?\n. Good to merge apart from the minors. Can you also squash all this into a single commit?\n. Nope, we disabled them. Soon to be replaced though\n. Nope, we disabled them. Soon to be replaced though\n. No, dont bother. We will replace them soon. For now, your changes are covered under a unit test\n. No, dont bother. We will replace them soon. For now, your changes are covered under a unit test\n. travis is fine, merging\n. travis is fine, merging\n. yes, please fix this soon\nAlso will make some minor comments above\n. yes, please fix this soon\nAlso will make some minor comments above\n. Let us take option 1, reset the norms whenever we know that the distance is changed (we know often), add a warning for the cases we dont know\n. Yes :)\nStart by writing the base class and then refactor a single kernel.\nAlways write unit tests for the kernel compute methods before you re-factor. You can use the tests to check your refactored version then\n. 1.) Design\nWe need\n- base class computes distance\n- offers way to precompute matrix of distances\n- stores it efficiently (distance is symmetric)\n  All this is a merge of CCustomDistance, CCustomKernel, CDistanceKernel\n2.) All current kernel should be put in this framework\n3.) There should be no performance penalties (benchmarking needed)\nDotFeatures is for dot products. The substraction you need to to otherwise, best way would be linalg or directly in eigen\n. This is a good start.\nBut needs another major iteration.\nCan you also add a unit test for both the compute_bias, and train_machine method? Using 2 and more datapoints?\n. This is a good start.\nBut needs another major iteration.\nCan you also add a unit test for both the compute_bias, and train_machine method? Using 2 and more datapoints?\n. Good! looking forward to the update!\n. Good! looking forward to the update!\n. Did you build shogun with -DENABLE_TESTING=ON ?\nIll check the rest tomorrow\n. Sorry I was travelling. Catching up now\n. Sorry I was travelling. Catching up now\n. This is ready to be merged. Can you please squash all commits into a single one? (we want to keep the git history clean)\n. This is ready to be merged. Can you please squash all commits into a single one? (we want to keep the git history clean)\n. travis is green. merging! :)\nNice one!\n. travis is green. merging! :)\nNice one!\n. Jo!\nThis patch caused a compiler warning. \nSee here and click on warnings.\nCan you fix that?\n. Nice thanks for that.\n. BTW we should include these graphical examples in the build, will open an issue\n. something like this. We just want to make sure that all the graphical scripts at least run.\nSome care will have to be taken when they go into infinite loops\nI suggest googling how to test software with graphical interfaces ... The 'agg' is just a hacky way to do it\n. Please do. Send a PR for a single example first as a prototype. Then we can apply this iteratively to all examples\n. @vigsterkr what ipython is on the buildbot?\n. If we have the new version (which we should) I am up for merging this....\n. can update?\n. Nice thanks, I will merge\n. Thanks for that! In this state, it is not that useful. But we can turn it into that easily.\n- this is a speed comparison. While the plots you made are nice, they are somehow redundant to the pca notebook we have. \n- To compare results, simply compare the norm of the eigenvector differences and the projected features.\n- No need for naive python implementation and matplotlib, though the former is good to do so that you understand the method. But not to include in the notebook.\n- The datasets are way too small to tell us anything about the speed. I suggest you make a table where you perform the performance for datasets with dimension (D) and number of datapoints (N) varying: N=(small, medium, large), D=(small, medium, large) and all combinations. Use a synthetic dataset for now.\n- PCA can be done in two ways: via Eigendecomposition of covariance and via SVD of data matrix. Shogun picks this automatically unless you specifiy by hand. This has drastic implications on the runtime.\n- Memory is another very important topic here. Can you profile that?\n- If you find things that are annoying about the shogun interface, or default settings (like scaling) that you dont like, please send a PR to fix them.\nLooking forward to the update\n. This is great work! In particular that you included the memory benchmarks.\nIndeed doing it as part of a benchmark system would be best -- especially since then we can document it and potentially include in a (new updated) overview paper of Shogun.\nIn terms of speed:\n- Shogun in AUTO always  chooses the mode that gives less complexity (in theory)\n- SVD in eigen3 is known to be slow, but more stable. This is not a problem in my eyes as the different will always be marginal (and it is indeed in the results). We could add this to linalg and use the same svd call as scikit does (I suspect lapack). But that is not important for such differences.\nFinally, the PCA code was tuned GSoC 2014. This is why it is very fast and reliable. The next step here would be to include these results in a maintainable way to that we can reproduce them without much hassle. If you do this, this could also be an example of how to do this. Do you want to go ahead with this?\nOnce again, this is great, in particular that we are an order of magnitude faster than scikit. \n. This is great work! In particular that you included the memory benchmarks.\nIndeed doing it as part of a benchmark system would be best -- especially since then we can document it and potentially include in a (new updated) overview paper of Shogun.\nIn terms of speed:\n- Shogun in AUTO always  chooses the mode that gives less complexity (in theory)\n- SVD in eigen3 is known to be slow, but more stable. This is not a problem in my eyes as the different will always be marginal (and it is indeed in the results). We could add this to linalg and use the same svd call as scikit does (I suspect lapack). But that is not important for such differences.\nFinally, the PCA code was tuned GSoC 2014. This is why it is very fast and reliable. The next step here would be to include these results in a maintainable way to that we can reproduce them without much hassle. If you do this, this could also be an example of how to do this. Do you want to go ahead with this?\nOnce again, this is great, in particular that we are an order of magnitude faster than scikit. \n. @zoq or @rcurtin might have comments\n. @zoq or @rcurtin might have comments\n. In big datasets, there are all sorts of weird effects happening on computers due to the way its memory is managed. This can reveil further problems we have in the Shogun implementation.\nFor now, I suggest that you start with using a single smallish dataset to keep things simple, and then increase\n. In big datasets, there are all sorts of weird effects happening on computers due to the way its memory is managed. This can reveil further problems we have in the Shogun implementation.\nFor now, I suggest that you start with using a single smallish dataset to keep things simple, and then increase\n. thanks @rcurtin @zoq for getting back!\n. thanks @rcurtin @zoq for getting back!\n. #3054 \n. #2991 \n. any updates here?. Might be nice to explore this further in a GSOC entrance task, especially doing a faster (LAPACK) SVD. No need to include the translates files.\nAlso, no need to store the feature matrix. Only the output labels in fact. Accuracy also not needed,in the other example they were for testing \n. Thanks a lot for this addition.\nWe are currently merging the sphinxdoc feature branch. Once it is done, you can re-send the PR against develop, and we will be able to look at a preview. Until then, be a bit patient :)\n. Thanks a lot for this addition.\nWe are currently merging the sphinxdoc feature branch. Once it is done, you can re-send the PR against develop, and we will be able to look at a preview. Until then, be a bit patient :)\n. @vigsterkr Ill leave handling this to you. Can be merged from my side\n. @vigsterkr Ill leave handling this to you. Can be merged from my side\n. Please re-send this one against the develop branch. The cookbook feature branch is now merged\n. Can collect patches that need this here:\n3044\n. Check the linalg readme and read the code and unit tests. Then try to add the operation. There is already a lot of summing in the redux module so this is a very simple task\n. Check the linalg readme and read the code and unit tests. Then try to add the operation. There is already a lot of summing in the redux module so this is a very simple task\n. There already is a sum redux module. But yes\n. There already is a sum redux module. But yes\n. @juancamilog I am super curious on the results of this, are there any already?\n. Note\n. @yorkerlin dont you think it might be more clever to put your ideas into the mlpack benchmarks?\nFor the GPs, if Shogun is faster than the gpflow and gpy, or even just scikit learn, that is the best start.\nI think we will stay with gcc for now ... ;)\n. We should compare a benchmark where we also learn the hyper-parameters using ML2.\n. Great, looking forward to see this.\nSuch complicated frameworks as GPs might really allow Shogun to play its muscles ...\n. @Ialong might be interested in this too\n. It would be good to add such plots to the benchmark system by mlpack. @yorkerlin don't you think it can be made work with octave?\n. Why is sklearn 10 times slower in training but faster in testing?\n. Good points. Make sure the benchmarks give the same results. It is meaningless to compare things that produce different results. Also if possible the same algorithm should be used. BUT, at the end of the day, what matter is \"user wants to do gp regression with exact inference, which toolbox is fastest\"\n. About vectorization: this is done via using linalg\nAbout multithread: Yes, definitely this should be done, using openmp if possible\n. Cool that Shogun is faster! :) Some things to do\n- we need to put this systematically into the mlpack benchmarks .. ipython notebooks get lost so we forget about the results. Then we can collect these results and publish at some point\n- you are right that use cases can differ and should be carefully investigated on their own\n- larger problems would be interesting, where training takes a few more seconds\n. Really nice!\nI agree on integrating this into the benchmark repo, as well as using multicore magic. @ialong might be interested\n. traivs timed out. restarted.\nOtherwise I am fine.\nGood to start with those that dont use shogun-data. Though this is disabled anyways and will be replaced soon\n. Thanks for that\n. Both this and Nystrom would start with KRR. KRR should put the method to solve the (K+\\lambdaI)^-1 system in a method. A subclass would then overload this method and implement the cheaper matrix inversion for Nystrom or incomplete cholesky\n. Both this and Nystrom would start with KRR. KRR should put the method to solve the (K+\\lambdaI)^-1 system in a method. A subclass would then overload this method and implement the cheaper matrix inversion for Nystrom or incomplete cholesky\n. Thanks!\n. I will review this soon, gotta run now\n. Good page!\nSorry about all the change requests ... I am still making up my mind about how to do this best. But I think we are almost there.\nBTW, also please add a test file in data/testsuite/meta/regression/least_squares_regression.dat in the data repository\n. OK one more thing. This class is just a wrapper for linear ridge regression with 0 regulariser.\nCould you just write a page for the ridge regression, and then mention that there is a wrapper class called LeastSquaresRegression which just sets the regulariser to 0?\n. Sweet, this is starting to look good.\nI wrote some more comments, mostly minor -- but I really want to get the first few pages right so that people have an easier time writing the next ones.\nMost importantly, make this an example for ridge regression. No need to have one for least squares. You can just mention this in the text. Ah I just realise that the data file then also has to be renamed and re-calculated -- I should not have merged it\n. All comments are minor. The data file does not have to be touched anymore. Ill merge data and wait for the final update on this one.\nNICE! :)\n. no idea what is wrong with travis?\n. the knn example times out ...\n. I just tried this locally and it did work. No idea :(\nJust compiling locally with the same cmake options\n. Very weird. travis passed after a few restarts....I'll merge for now\n@vigsterkr any ideas?\n. Something cheesy is going on there. The merged build failed.\nI then managed to reproduce this freeze locally, running cmake with only -DENABLE_TESTING=ON\n@sanuj can you investigate this a bit? I will also tomorrow\n. This has high priority as the develop build is broken\n. @sanuj @vigsterkr the timeout has nothing to do with this cookbook page here -- it happened before and it is the knn that times out. @sanuj It also has nothing to do with modular tests failing.\n. Thanks!\n. \"No need to include the translates files.\" -- in your previous PR there was all the python, R, octave files etc. Which is not necessary, but they are not in this PR so dont worry\n. Your example is not an executable listings. Make sure to run make test with having -DENABLE_TESTING=ON in your cmake. \nYou will see that the generated cpp doesnt compile\n. After you have run make test the cpp example will have produced a .dat file. Can you add this one to the testsuite in data and update the data version of this PR? Then we have an integration test for gaussian naive bayes\n. Finally, remove all existing gaussian naive bayes examples in the undocumented examples folder\n. Just merged the linear ridge regression one. Seek inspiration from the patch #3064 \n. Thanks for the patch :)\nThis needs some love until we can merge it, but I look forward to get the fast Kmeans in shogun going\n. Great, this patch is evolving nicely. All comments were minor.\nCould you send a seperate patch for the CDistance change?\nThen we can merge this one. The next one should be openmp for the loop over all data :)\n. Travis is also green, so this can be merged soon\n. Looks good to me. Squash!\n. Looks good to me. Squash!\n. BTW this is now the fast version?\nNext step: add this to benchmark!\n. BTW this is now the fast version?\nNext step: add this to benchmark!\n. I leave merging to someone else so that we have another pair of eyes on it\n@vigsterkr @lisitsyn @lambday \n. I leave merging to someone else so that we have another pair of eyes on it\n@vigsterkr @lisitsyn @lambday \n. Great. Make sure to add your changes to the scripts in these benchmarks. You might want to change the tolerance parameter of scikit. \nAlso, we really want to collect these results somewhere to document outcomes of the improvements\n. Great. Make sure to add your changes to the scripts in these benchmarks. You might want to change the tolerance parameter of scikit. \nAlso, we really want to collect these results somewhere to document outcomes of the improvements\n. BTW another next step would be to parallelise this thing using openMP. Use google to find inspiration of how to best do this.\n. BTW another next step would be to parallelise this thing using openMP. Use google to find inspiration of how to best do this.\n. The second is new? Which means we are now quite a bit faster. It would be good to check why we are faster in certain cases, and not in others. But anyways -- make sure that all these tests are going back to the benchmark system. Reference this PR when you send them a patch\n. Ok I will merge this. Great work!\nBTW do we have a notebook for KMeans?\nBTW2 maybe next check how to use openmp in kmeans ? I found a few clever implementations and it might be worth to include this here too.\n. All relevant (that is strongly typed languages) travis builds are green. Merging\n. run cmake with -DENABLE_TESTING=ON and then run make test or just make unit-tests\n. I have to run now ... will finish reviewing this later. Thanks for the patch :)\n@lambday can you also have a look?\n. This is great stuff. Thanks a lot for this patch.\nA few things:\n- can you please split this into multiple patches with smaller number of lines of code. There are lots of operations and this should be easy to do. At least two different ones for the vector and matrix\n- This needs unit tests before it can be merged. Have a look in tests/unit. You can run them as I stated in my first comment. You need to aim for >90% coverage in such fundamental things, especially with the viennacl code\n- This is why I suggest you send the methods one-by-one (one method, with unit test), so it is easier to review them and make sure no corner cases slip through.\n. Yeah let's do the ViennaCL after the eigen part is done :)\nIn general, aim for small patches, as small as possible. Rather have 10 PRs with each 15 lines than one with 150\n. Your cmake probem is solved?\n. @OXPHOS yes definitely, the unit tests native should be replaced with eigen3. In your case, just add tests for eigen3. Once you have done so, send them one-by-one in new pull requests -- travis will run all your added unit tests and we can see whether things worked. I will close this one for now  -- looking forward to the new one! :) (might give a few comments on the update you made)\nReally relevant, so I ping the patches that removed things. Please when removing something, always replace the unit tests and implementations for eigen3. We dont want to cut out features here. This is not for you @OXPHOS \n3081 #3080\n. Some minor changes required, otherwise fine to merge. Thanks!\n. Great. This is ready to get merged. We now need:\n- all commits are squashed into one\n- the comments are updated\n- travis is green\n. Thanks for the patch! :)\n. we don't need the backend itself anymore. And yes, all the implementation will also go.\nBUT before you delete them, make sure that the eigen3 counterpart is implemented (and also has unit test coverage)\n@lambday should comment\n. we don't need the backend itself anymore. And yes, all the implementation will also go.\nBUT before you delete them, make sure that the eigen3 counterpart is implemented (and also has unit test coverage)\n@lambday should comment\n. Nice thanks. I will merge this, please address the minor documentation issue in the next patch\n. Nice thanks. I will merge this, please address the minor documentation issue in the next patch\n. Merging as this is only affects interfaces\n. Merging as this is only affects interfaces\n. All linear solves should have the possibility to precompute the linear solve. That is factorize the matrix and offer a method that computes the solve given the output of the factors. See numpy again for this. For example, for LLT, this will be a triangular solve (one can do this with eigen). For the others it works imilarly\n. All linear solves should have the possibility to precompute the linear solve. That is factorize the matrix and offer a method that computes the solve given the output of the factors. See numpy again for this. For example, for LLT, this will be a triangular solve (one can do this with eigen). For the others it works imilarly\n. Great!\n. closed in #3097 \n. lets see if it worked :)\n. lets see if it worked :)\n. I think there also will be a description for krr, which can be removed\n. I think there also will be a description for krr, which can be removed\n. Check the descriptions folder in the examples dir\nOn Tuesday, 15 March 2016, Sanuj Sharma notifications@github.com wrote:\n\n@karlnapf https://github.com/karlnapf I don't fully understand your\nlast comment.\nRest is updated :)\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly or view it on GitHub\nhttps://github.com/shogun-toolbox/shogun/pull/3078#issuecomment-197052192\n. Cool this is good to merge.\nJust a thought I had: Can you also explain how to extract the alpha vector (and the w vector in linear regression)? This will (annoyingly) also change the data files, so we can do it in a separate patch (for linear regression).\n\nBut I think it is good to explain such things too.\n. Thanks!\n. Just to make sure: these are all covered by an eigen3 implementation yes?\n. I think you might have forgotten data\n. fixitplease :)\n. Sorry I just only saw this so my other comments might be outdated\n. Ill open an entrance task to remove them all. \n. Much nicer if the patch is small.\nThere are some issues with the way you used git. The rest is fine to merge :)\n. @lambday can I  leave merging this to you? :)\n. @lambday can I  leave merging this to you? :)\n. great thanks for that. I will ask @vigsterkr about the other warnings (coming from using latex in doxygen)\n. Please squash all commits into a single one for cookbook PRs\n. The PR should also contain an integration test file for the cookbook example. See the readme how to generate it. You will have to send a PR against the data repository and then include the new data version in this PR\nUPDATE: Just saw you sent a PR against data. But you need to update the data version in this PR as well for it to work\n. no idea why the error happens, but it does locally for me too. fixing ....\n. You referenced the meta example snippets with a wrong filename\n. .. sgexample:: gaussiannaivebayes.sg:create_features\nshould be\n.. sgexample:: gaussian_naive_\nbayes.sg:create_features\n. Just checked, that is done\n. No need to open new PR btw\n. what you need to do is to git commit data from the shogun root dir\n. This is great. It can be merged after the minor fixes and the data version being updated. I will merge the shogun data PR now\n. You can just update your old PRs using git push -f on your fork. But you can also just close them yourself.\n. You can just update your old PRs using git push -f on your fork. But you can also just close them yourself.\n. Learning is exactly the point of this exercise :)\n. Learning is exactly the point of this exercise :)\n. You still haven't updated the data version as I see this here\n. You still haven't updated the data version as I see this here\n. As a result traivs doesnt run the integration test for the gaussian naive bayes example, link\n. As a result traivs doesnt run the integration test for the gaussian naive bayes example, link\n. Thanks\n. Thanks!\n. Yes\nOn Saturday, 19 March 2016, Tianqi Tang notifications@github.com wrote:\n\nNew comer here. Sorry but have to ask what is GP short for? Gaussian\nProcess?\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly or view it on GitHub\nhttps://github.com/shogun-toolbox/shogun/issues/3091#issuecomment-198746804\n. This seems to be a name change. Feel free to send a fix for those notebooks...\nAs for GP, I think the best would be to have a sperate example for all use cases that we have\n- All inference methods\n- classification/regression\n\nStart with standard GP regression, standard GP classification. Keep the notebooks simple :)\nBTW I think we can have a seperate gp folder in the cookbook...\n. check the code #ifdef HAVE_NLOPT\nThis is what you need\n. Hi,\nthe cookbook pages should be very simple and minimal. Rather add new pages for sparse inference methods.\nthe readme doesnt cover changing the index. Feel free to send an update with a mini explanation\n. #3054 the notebook in here has the way to learn things with ml2\n. Giant patch.\nPlease try to split such things into smaller pieces in the future. It is better to merge 5 PRs on this than 1 -- otherwise things slip easily.\nFor this one, however, I checked everything, and it is fine apart from the ViennaCL ifdefs that you deleted. I commented on all of them. Once this is done, travis should be fine and we can merge :)\nThanks! I realise this is a lot of work and it is really appreciated\n. Giant patch.\nPlease try to split such things into smaller pieces in the future. It is better to merge 5 PRs on this than 1 -- otherwise things slip easily.\nFor this one, however, I checked everything, and it is fine apart from the ViennaCL ifdefs that you deleted. I commented on all of them. Once this is done, travis should be fine and we can merge :)\nThanks! I realise this is a lot of work and it is really appreciated\n. The travis failures have to be investigated as well here -- the build was fine before, and it should be fine after ... LDA is the only test failing on travis currently...\n. The travis failures have to be investigated as well here -- the build was fine before, and it should be fine after ... LDA is the only test failing on travis currently...\n. travis needs to be green before we can merge this .... why is the LDA test failing? what changed?\n. travis needs to be green before we can merge this .... why is the LDA test failing? what changed?\n. It is one particular unit test .... so no need to re-send all of this. We can merge this once the LDA thing is updated. I am sure it is an easy problem\n. Ok then. Travis is fine. I will dare to merge this -- lets keep an eye on the buildbot If you see that something is broken from the patch, feel free to send a fix asap\n. Nice work!\n. http://buildbot.shogun-toolbox.org/builders/deb1%20-%20libshogun/builds/3678/steps/compile/logs/stdio\n. Great! :) thanks\n. Great! :) thanks\n. Great! Thanks for the patch.\nCan you address our minor comments? I think it can be merged then....travis is fine\n. Great! Thanks for the patch.\nCan you address our minor comments? I think it can be merged then....travis is fine\n. I think this will also fix the buildbot error on kernel ridge regression \n. I think this will also fix the buildbot error on kernel ridge regression \n. - no need to do viennacl calls, just use eigen3 itself (if you add a LLT solve to linalg, please send a seperate PR, and then send another PR for using this linalg llt solve in KRR)\n  - for KRR training, the kernel matrix is always square symmetric psd\n  - Unit tests are very important, make sure to test a few corner cases\n. - no need to do viennacl calls, just use eigen3 itself (if you add a LLT solve to linalg, please send a seperate PR, and then send another PR for using this linalg llt solve in KRR)\n  - for KRR training, the kernel matrix is always square symmetric psd\n  - Unit tests are very important, make sure to test a few corner cases\n. Thanks for the patch! :) Useful.\nThe fact that it is smaller makes it way easier to review. And you see we pick up on more things as well ;)\n. Thanks for the patch! :) Useful.\nThe fact that it is smaller makes it way easier to review. And you see we pick up on more things as well ;)\n. Great, this is ready once the minor issues are addressed and all commits are squashed into one\n. Google\n. This is ready to be merged after the squash. If you could fix the tabs, that would be best.\n. Another useful link:\nhttps://git-scm.com/book/en/v2/Git-Tools-Rewriting-History\nhope it helps!\n. Great. Well done, and party time :)\n. Thanks for the patch!\nSome minor comments, should be easy to fix.\n. Thanks for the patch!\nSome minor comments, should be easy to fix.\n. Ok cool, this is really close to being merged. Sorry for being a bit picky about the comments/docs, but it is well worth it to bring things to a level where they do not have to be touched ever again anymore :)\n. ok this is ready :)\nSquash all commits into one and we can merge.\nThanks!\n. Great, thanks for the patch.\nWe need to do a few iterations to merge, but that is totally fine\n. I wont merge the data for now, so that any changed can be put in there too\n. Ok will merge data and then this one once travis is green.\nThanks!\n. I restarted travis after the data merge, it was failing....\n. I think the method can just take a preallocated alpha vector as a parameter. Then populate and return it. The train method then calls the setter. This way everything is very modular\n. Ok, we are there. Last two minor changes and we can merge.\n. Party:)\n. This has my strong +1\n. CUDA backend? The idea is more to have linalg operations that are independent of the library used. So that the algorithm code is free from explicit library calls (CUDA/openCL/etc).  Imagine CUDA now does things much better than ViennaCL in the future -- then we can easily change without touching the algorithm code\nI don't agree on doing CPU vs GPU under the hood. I was of your opinion before, but changed it after I saw what can happen.\nThis is extremely difficult to infer as it heavily depends on the use-case. We decided to make the GPU/CPU decision explicit to avoid the massive performance drops that we observed in benchmarks. Following the Python zen, muhahahah ;)\n. @vigsterkr we do quite a bit of GPU programming at my institute -- my opinion that it is hard to decide when/if/how to transfer data to the GPU comes from that. Also, there are quite a few papers out there where people specifically design (known) algorithms for GPUs. The architecture of the algorithms often fundamentally changes, and these changes cannot be inferred automatically in general, but only for a subset of simple design patterns. Being explicit, especially in the messy Shogun code, in my eyes therefore is better.\nI agree with you on making something as good as possible, so happy to discuss. About compile/runtime -- we had very clearly speaking benchmarks on this iirc. I would prefer the runtime switch as well, but I guess we need to think about this a bit more then. BTW this can also be changed later (runtime/compile time), the interface is more important now, right?\n. This is a great initiative, but we mostly want this solver to be part of linalg.\n@lambday can you comment a bit\n. Hey are you aware that Shogun stores variables as column vectors, where numpy does row vectors?\n. Oh man this QDA code can be done much cleaner I think\n. my question is: is this really a bug or does it come from the fact that you did pass shogun features that were not in the right order.\nIf you pass something to a sklearn algortihm, you have to pass the same thing transposed to Shogun.\nMeet me in irc if you have questions on this\n. So this is actually a bug?\nGood catch then...\n. Sweet! :)\n. Another thing to benchmark properly via the framework\n. As said, binary labels do not make any sense here.\nRidge regression should be lightning fast on these examples, unless you do something wrong. Note that Shogun takes features in column format, which is transposed compared to what sklearn takes as input\n. These results seem very nice, dont they?\nMake sure to include a link to this in your proposal\n. Next step is to merge the benchmark into the benchmark repo, and yes, a larger dataset would be a good idea. See this paper for some ideas\n. In that case, send a patch to disable the test. You can do that via prefixing DISABLED_LDATest\nThis is now an entrance task to fix the disabled LDA unit test\n. Cholesky is not yet there. Needs to be added as well.\nThe wikipedia page is correct, but we would not implement by hand but use eigen3 as in the link above\n. Indeed, as I said above\n. Great. That's it now :)\n. Thanks for the pach! :)\n. Thanks a lot for this\n. @lisitsyn ctags \n. Thanks, should be good. Travis error was unrelated. Merging :)\n. Hi\nPlease never send PRs of this size. 2-3 files max, as few lines of code as possible.\nWe can re-factor one by one, which is much less error prone\n. First patch should just add the row/colwise addition -- with unit test. Then the patches with using it should come\n. Wow nice!\nHow many cores did you use?\nCan you explain a bit on how you parallelized? I see the code, but what is the rationale? Are the operations in KMeans thread safe (important)? \nWhy is Kmeans on one core still so much slower on certain datasets?\nYes openmp is handled in Cmake\n. What about adding a benchmark for the k=7 as well?\n. Will merge. Nice one!\n. BTW this should be added to NEWS\n\"KMeans now uses multiple cores\"\n\"Major effeciency improvments and bugfixes for KMeans\"\n. Looks good!\nWhat I am missing is a flag to specify to compute row or column wise mean.\nBut @lambday had some ideas for that in isse #3100\n. Let us merge this one for now, and have a separate patch for col-wise\n. @OXPHOS if you could update the doxygen, then I can merge\n. Thanks for the nice patc! :)\n. Cheers!\n. It actually worked before, but this seems cleaner. \nThanks!\n. Ah! Lua is currently disabled -- overloading Shogun methods doesnt work in there. This is why the build did not fail. So this patch then solves the problem we did not have before :)\n. I think this is good. I will merge and continue to edit myself\n. yes, the test was never touched and passes\n. Check the code for the other embeddings (the converters ported from tapkee). This is probably a bit more tricky than the other entrance tasks we  have.. Good patch! Needs some minor changes, but in principle this is soon ready to be merged :)\n. Thanks for reviewing @iglesias :)\n. @arianepaola and another one for you :)\n. @besser82 we have new updated nightly binaries now, any way we can include fedora in there?\n@vigsterkr \n. pip install shogun\n. @arianepaola another one for you, you could in fact build things on the gist above\n@Eejya I don't really get the error you are having there. Does this also occur when you compile Shogun normally?\n. Done since 2151b9c8cce2aca068a421460e580d8c5d398293\n. related to #3118 \n. Actually, these methods should all go into our gaussian class (and this has to be re-worked as well)\n. Thanks for that! :)\n. Ok, so the readme should definitely say \"fix\". Great catch btw! :)\nWhat about putting up a benchmark for this initialisation in the benchmark framwork, or at least put this on a list.\n. Restarted travis -- timeout error. Let's see\nOtherwise I am fine to merge this.\n. BTW the example can be added to the notebook. That would be very nice\n. sklearn has kmeans++ so yes definitely compare it!\n. Fine to merge from my side\n. https://github.com/shogun-toolbox/shogun/commit/69f1f2e17d14863e70129077bba2e390cdddc8d1\nPlease think about such cases!\n. Nice work. Looking forward to have that in the benchmark framework. BTW what about these linalg overhead problems? What is the state there?\n. Great analysis. The reason why this is only noticeable in small dimensions simply will be the fact that for large dimensions, the dot product time dominates the call-by-value overhead. I don't see any reason for not using const references by default everywhere in linalg (apart from some of the in-place methods). Certainly, we have to do something about that, as the impact in KMeans was significant, which is not good.\n@lambday @vigsterkr can you comment?\n. As said, check the travis log, search for \"error\" and you will see that matplotlib is not there\n. I close this for now....Feel free to re-open if you wanna continue, but it is not really there yet\n. @vigsterkr you ok with that? I am\n. @vigsterkr the idea was mostly to check basic things such as imports, method names etc. So in a way, yes. But just executing, not asserting results. The problem currently is that they outdated (as in not executable) over time, and it takes time to fix.\n. Thanks for cleaning this up.\nNOTE: travis currently does not cover the BSD build. You will have to test this by hand.\n@vigsterkr can we add a USE_GPL_SHOGUN=OFF build to travis? Or at least the buildbot (more important)\n. @yorkerlin feel free to merge this if tested locally\n. Great.\nPlease keep an eye on both the buildbot and the local build with USE_GPL_SHOGUN set to ON and to OFF\n. I improved the situation in https://github.com/shogun-toolbox/shogun/commit/d2e1beacd9a156be2fb50c6c5f12269508224cab\nIt now tells the user to de-activate meta examples or install ply\n. It is these guys: shogun.ml/examples\nIf you want to generate the code for those locally, you need ply. If you have it installed and it is not detected, please send a report with details of your setup in a new issue.. Lets see :)\n. @yorkerlin could you put a bit more informative commit messages? :)\n. Can you give some more specifiy instructions here, so that somebody else would be able to do this? Not clear what this issue is about with this description\n. Details please! And a label would be good as well\n. I absolutely agree, this is far from optimal. In particular the HAVE_VIENNACL guard to resolve this  doesnt make any sense.\nWe had some discussions about a re-factored linalg. We will put that to life soon\n. In that case, the unit test should have an eigen3 suffix\n. Fine to merge otherwise. Please update the name, squash the commits, and then I will merge.\nThanks for that!\n. Great thanks!\nWill check the build whether it worked\n. good!\n. How is the speed difference when compared to nlopt?\n. Might be worth to have a look at these GPU accelerated SVM solvers and see how we can adapt this to Shogun and which operations we need.\n. Uh, man these speedups almost hurt -- 200x ??\nWe should definitely look more into this thing.\n. For the record: It is not as simple as putting linalg calls in the SVM solver. The GPU SVM implementations are quite different from the CPU ones, and partly achieve the speedups with accuracy tradeoffs. This is a more serious task. Contributions are welcome!. Seems fine\n. It is impossible to review this. A few high level comments:\n- the impl classes are made to hide c++ code from SWIG, which will only see the main interface class. Are you sure you want to remove them. See some other method, e.g. the metric learning does this\n- similarly, the base class for sure would not be exposed via SWIG\n- A class diagram would help, can you draw one so that we can discuss these changes?\n- make sure that things remain working, i.e. the notebook, the examples, etc\n- are there any speed implications coming from this?\n- if you want, I can put up a feature branch for you. There you can develop a bit more freely and then we can merge the whole thing....thoughts\nWe need another dev to give an opinion here before merging. @vigsterkr @lisitsyn ?\n. Thanks for the comments. I think it should be ok to merge then....\n. @vigsterkr from my side this can be merged. What do you think? Shall we maybe put this in a feature branch to allow for some more checks (on buildbot for example?) Or merge from here? I think the new structure is fine and travis seems to be fine as well. ....\n. (has to be rebased against #3217 though before merge)\n. Can you rebase? There will be some minor conflicts from #3217\nI will ping @vigsterkr to review so that we can merge soon\n. Can we push this in parallel to the other stuff you are doing @Saurabh7 ?\n. There is a problem with this implementation: It doesn't save any computation ;)\nI will comment in-line\n. Cool stuff! This is almost ready!\nNevermind the different sub-sampling techniques for now.\nNext steps:\n1.) Clean up things (resolve all TODOs etc, remove the example code from the patch for now)\n2.) Write unit test (at least two cases: ensuring that for m=n the results are the same, and that for a smaller m, the results are at least similar)\n3.) Squash all commits into a single one\nThen we can merge. Then\n4.) Meta example similar to the one I posted\n5.) Data patch with integration test\n6.) Cookbook page (if you want)\n. Hi,\nsorry about the delay.\nAbout the results.\nSince your alphas are very similar but the results are not:\nI would solve the KRR system to get an alpha that has dimension m. This makes evaluating the regression function cheaper (kernel is computed against less point).\nFor that you need to either set the support vectors and translate the alpha (filling zeros where needed), or overload the apply method of the base class. I have the feeling that your error is coming from something related to this\n. Let me know if you keep on having problems, I can pull your feature branch and potentially help\nNice one, we are getting there, the code is soon ready :)\n. No worries,\nI was out for work today. \nLet's get this merged, it is close :)\n. It doesnt really matter how it is done, important is that it is consistent :)\nCool this is really almost done now\n. Great stuff! Looking forward to merge this. You can also add an entry to the NEWS giving yourself credit.\n. Travis complains about your get_name unit test:\nhttps://travis-ci.org/shogun-toolbox/shogun/jobs/133729677#L3755\nFrom my side, you can as well just remove it\n. Ok, this is 1 step from being merged\n- fix the remaining minor things, in particular the travis\n- squash all commits into a single one\n- we can merge :)\nNext steps:\n- meta example (would be cool, ask for help if you wanna write one)\n- a little benchmark against full KRR\n- potentially an update of the ipython notebook where KRR is introduced\n- you choose :)\n. Ok travis is fine. So as soon as we have the kernel matrix computation sorted and done efficiently, we can squash the commits and merge.\n. so when all is done, the commits need to be squashed and we can merge. \n. Ok, fine to merge from my side.\nThere are a few minor things (I will comment) that you can address in a separate patch.\nE.g. the NEWS file should be updated with this.\nYou can try to add a meta example next, see all the other open PRs.\nSee the readme here\nThen yes, performance checks, we usually to this in this framework and mostly compare against sklearn. There are lots of examples where @Saurabh7 did similar benchmark comparisons.\n. @vigsterkr feel free to merge once reviewed\n. Yes, this update would be very useful. I don't know who did the VW port back then.\nBut you can definitely talk to John Langford, who is interestewd in these bindings to work. cc me in if you do\n. And yes, we should use it as an optimizer\n. You can only pass sub-classes of SGObject through swig\n. I think this has to do with the way the typemaps work. Can't you make the minimizer an SGObject? With the soon to arrive plugins, this should be fine\n. Yes I said that since we have the problem that there are too many shogun\nclasses which mess with the build. However with the plugin project soon\nhopefully being done, we might do it now. See what I mean?\nOn Tuesday, 19 April 2016, Wu Lin notifications@github.com wrote:\n\nShould I make a minimizer as a sub-class of SGObject now ?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly or view it on GitHub\nhttps://github.com/shogun-toolbox/shogun/issues/3174#issuecomment-211851516\n. @vigsterkr any clues? I thought we had fixed this\n. Ah yes, thanks for explaining. I think it might be a good idea to at least fix the docs online.\nOn the other hand, we can also just release ASAP and the 4.1 gets outdated anyways?\nWhat do you think is better?\n. Ok so this will be fixed once we release\n. This is definitely something we should look into. And we can do much better on single machines that with joblib.\n\nWe need some thread safety here. @Saurabh7 model selection should be part of your project. Can you come up with some plans there?\n. Yep definitely.\nThis is pretty straight forward to implement in the same way as the grid search works\n. We have some low level things that have to be sorted out first. @Saurabh7 is working on it, and RF are definitely in the scope of his project\n. Interesting. Can you reproduce this locally?\n. What error are they failing on?\n. You can also run the meta examples manually. What happens then?\n. Just run the generated code listing or compiled binary from its directory\nOn Thursday, 19 May 2016, OXPHOS notifications@github.com wrote:\n\nI don't think any specific info is provided. Do you mind showing me how to\nrun meta example manually? Thanks.\nRun make test:\nRunning tests...\nTest project /Users/zora/Github/shogun/build/examples/meta\n      Start  1: meta-examples-generator\n 1/19 Test  #1: meta-examples-generator ...........................................   Passed    2.37 sec\n      Start  2: generated_cpp-gaussian_naive_bayes\n 2/19 Test  #2: generated_cpp-gaussian_naive_bayes ................................   Passed    0.09 sec\n      Start  3: generated_cpp-knn\n 3/19 Test  #3: generated_cpp-knn .................................................   Passed    0.05 sec\n      Start  4: generated_cpp-linear_svm\n 4/19 Test  #4: generated_cpp-linear_svm ..........................................   Passed    0.06 sec\n      Start  5: generated_cpp-gaussian_process_regression\n 5/19 Test  #5: generated_cpp-gaussian_process_regression .........................   Passed    0.21 sec\n      Start  6: generated_cpp-kernel_ridge_regression\n 6/19 Test  #6: generated_cpp-kernel_ridge_regression .............................   Passed    0.04 sec\n      Start  7: generated_cpp-linear_ridge_regression\n 7/19 Test  #7: generated_cpp-linear_ridge_regression .............................   Passed    0.04 sec\n      Start  8: generated_python-classifier-gaussian_naive_bayes\n 8/19 Test  #8: generated_python-classifier-gaussian_naive_bayes .................._Failed    0.26 sec\n      Start  9: generated_python-classifier-knn\n 9/19 Test  #9: generated_python-classifier-knn ..................................._Failed    0.04 sec\n      Start 10: generated_python-classifier-linear_svm\n10/19 Test #10: generated_python-classifier-linear_svm ............................_Failed    0.03 sec\n      Start 11: generated_python-gaussian_processes-gaussian_process_regression\n11/19 Test #11: generated_python-gaussian_processes-gaussian_process_regression ..._Failed    0.03 sec\n      Start 12: generated_python-regression-kernel_ridge_regression\n12/19 Test #12: generated_python-regression-kernel_ridge_regression ..............._Failed    0.03 sec\n      Start 13: generated_python-regression-linear_ridge_regression\n13/19 Test #13: generated_python-regression-linear_ridge_regression ..............._Failed    0.03 sec\n      Start 14: generated_ruby-classifier-gaussian_naive_bayes\n14/19 Test #14: generated_ruby-classifier-gaussian_naive_bayes ...................._Failed    0.18 sec\n      Start 15: generated_ruby-classifier-knn\n15/19 Test #15: generated_ruby-classifier-knn ....................................._Failed    0.04 sec\n      Start 16: generated_ruby-classifier-linear_svm\n16/19 Test #16: generated_ruby-classifier-linear_svm .............................._Failed    0.04 sec\n      Start 17: generated_ruby-gaussian_processes-gaussian_process_regression\n17/19 Test #17: generated_ruby-gaussian_processes-gaussian_process_regression ....._Failed    0.05 sec\n      Start 18: generated_ruby-regression-kernel_ridge_regression\n18/19 Test #18: generated_ruby-regression-kernel_ridge_regression ................._Failed    0.05 sec\n      Start 19: generated_ruby-regression-linear_ridge_regression\n19/19 Test #19: generated_ruby-regression-linear_ridge_regression ................._Failed    0.04 sec\n37% tests passed, 12 tests failed out of 19\nTotal Test time (real) =   3.77 sec\nThe following tests FAILED:\n      8 - generated_python-classifier-gaussian_naive_bayes (Failed)\n      9 - generated_python-classifier-knn (Failed)\n     10 - generated_python-classifier-linear_svm (Failed)\n     11 - generated_python-gaussian_processes-gaussian_process_regression (Failed)\n     12 - generated_python-regression-kernel_ridge_regression (Failed)\n     13 - generated_python-regression-linear_ridge_regression (Failed)\n     14 - generated_ruby-classifier-gaussian_naive_bayes (Failed)\n     15 - generated_ruby-classifier-knn (Failed)\n     16 - generated_ruby-classifier-linear_svm (Failed)\n     17 - generated_ruby-gaussian_processes-gaussian_process_regression (Failed)\n     18 - generated_ruby-regression-kernel_ridge_regression (Failed)\n     19 - generated_ruby-regression-linear_ridge_regression (Failed)\nErrors while running CTest\nmake: *** [test] Error 8\n\u2014\nYou are receiving this because you commented.\nReply to this email directly or view it on GitHub\nhttps://github.com/shogun-toolbox/shogun/pull/3183#issuecomment-220364653\n\n\nSent from my phone\n. That is weird. Is it the same error as you get on travis?\nAlso, your Python examples also seem to fail, why is that? Are you sure your build is working fine?\n. Can we discuss this in another thread?\n. Yes move it!\nIs the current verison in the PR producing the problems you mentioend? Then I will close locally and check\n. Ill just use this version to check now\n. All these things cannot really happen ;)\nI suggest you clean up your branch and then we see how it goes ... I manually added your kmeans.sg locally and it works (under ruby)\n. Travis also seems very content\n. And it is indeed good that you removed the kmeans examples from the other example dir\n. This is great and ready to go. We just need some integration testing data now.\nBTW you can see in the first (cpp) travis build that the integration tests are executed:\nhttps://travis-ci.org/shogun-toolbox/shogun/jobs/133687607#L3921\nBut kmeans is not part of it -- this is since you need to update the data revision (as in other PR) after having sent a shogun-data patch\n. One way to fix this is to fix seeds, we have to decide whether we want to do this or not.\nJust talked to @lisitsyn and he is up for doing it.\nSo let us experiment here. Can you initialise the seed, update the data. Then it should be fixed\n@sanuj @Saurabh7 @arianepaola this also goes for your cookbook examples (GMM can have some data now as well)\n. http://www.shogun-toolbox.org/doc/en/latest/classshogun_1_1CMath.html#a4a93b2cd4ca6d7773e15f965501a856e\n. Single colon Math:init_random(0)\nLet me know if that still fails. Works on my machine\n. Can it be that your branch is not up to date? For me\nmake meta examples\nmake build_cpp_meta_examples\nand executing build/examples/meta/cpp/classifier/kmeans (from its directory)\nall work with the latest kmeans.sg from this PR (even with using 0 as seed)\nmake test also works for the Python example\nTravis also says they work (cpp). It is just the integration test doesnt work, which probably is since the data file is wrong. \nPython doesnt work due to a bug (see other email to  @sorig , we are fixing it)\nThe other things we need to investigate....will do tomorrow. (You can fix the cpp integration test nevertheless for now)\n. Just merged #3241 so the static calls should work now. Cleaner.\nMake sure to remove\nexamples/meta/parsetab.py\nexamples/meta/lextab.py\notherwise there might be weird errors\nCan you rebase, make sure the integration test works, and then ping me?\n. Let me know\n. ok so the java error should be resolved in #3251 \n. The ruby one is fixed in ae0e85aef2c97c88c77453e80cfb0b3379c140e4 (sorry I made a mistake earlier)\n. And the one in R is another copy paste error or mine.\nFixed and pushed in the second commit of #3251 \nNeed to wait for travis, then can merge that. Ping me if you see it is fine\n. Ok travis failure is from gmm. As soon as the minor adjustments in the rst are made, we can (finally!) merge :)\n. Some minor adjustments are needed. A rebase against develop should make travis happy (I disabled R meta examples)\n. fixed centers obviously changes the algorithm (and the integration test data)\nThe point I was making: do we need that? If so, you should mention what it does.\nThough I preferred if you changed the integration test data and removed it\n. Merged :)\n. Next time you squash: please put a meaningful commit message.\n\"Add kmeans cookbook and meta example\"\n. Waiting for travis and then merging\n. Aaaand merging\n. Thanks for that! There is a minor thing. If you have a minute, it would be great if you can update. Otherwise I will merge soon\n. Great thanks!\n. Thats the way :)\ngreat!\n. None of this should be required / or exposed to SWIG. It is all internal.\nIt is refactored anyways, you are right. But should be fixed for release anyways. Should not be to hard to hide all this. But maybe if it not easy, don't spend too much time on it\n.  No idea, this happened in another PR as well and @vigsterkr restarted it. But doesnt work here.\n. ping\n. Thanks!\n. Thanks for that!\n. Great. Thanks!\n. Thanks!\n. This mechanic is already in place, though hacky.\nSee https://github.com/shogun-toolbox/shogun/blob/develop/cmake/FindMetaExamples.cmake\n. I can send a fix later ...\n. Should be fixed in de9735a662eba90388f052ddf34c5d6ca6867a8d\n. There is a partial fix, and doing this properly is now in #3486\n. Absolutely!\nWe dont really have  GSoC project related to this. Maybe @Saurabh7 finds time, it is closest to his.\nOtherwise, you can also add these convenience wrapper classes easily\n. Clustering dir is just what it should be :)\n. No output for the test should be ok, as this can be covered by unit tests\n. If you want, you can make something like extract the cluster centres and variances. But this will probably change from run to run. Need to try\n. @vigsterkr I wasnt aware that these are failing. They were not a while ago\n. @sanuj I think you are responsible for the cookbook build failure:\nhttps://travis-ci.org/shogun-toolbox/shogun/jobs/132213321#L2585\n. We might need to change the grammar of csharp and java meta examples. See targets/*.json\nAfaik, csharp and java numerical types are always translated to Double. See what I mean?\nLet me know if you need help fixing this\n. Checking locally\n. Works on my machine using \"int\" for num_compoenents\n. Nope, it needs integration testing data, see cookbook readme\n. The only thing really missing is integration testing data, can you add? Then we can merge\n. Ah yes, sorry I forgot\n. In that case, lets merge :)\n. Thanks for that!\n. Doesn't hurt so I merge (even though I don't really see why something would crash otherwise, but well ;) ) \n. As @vigsterkr is fine, I go ahead and merge this.\n. This task is a good entrance task, but not the easiest, it requires some c++ experience. There is a number of points mentioned above. Also make sure to read the \"getting started\" guide on the wiki. This is not the right place to discuss such build problems. Pls do in irc or the mailing list. @sanuj @vigsterkr @lisitsyn \nBasic types are now not treated as objects (with include paths) anymore\n. Forgot a change, fixed and squashed into single commit\n. Merging, as old travis run and new one are green together\n. Thanks for the patch!\n- see my comments\n- add integration testing data (see readme of cookbook, separate patch, need to update data version here)\n- squash commits into one\n- done :)\n. Please ping me next time you update, I did not see :) (not notified when pr is udpates, only when there is a message below)\n. ah...sorry, the integration testing data is missing, you need to update the data version to the latest.\nJust commit data in your branch, it should show up in git status\n. You sent this data patch:\nhttps://github.com/shogun-toolbox/shogun-data/pull/87\nIt has commit hash: c2f6abd1a57e361a5222592334c9a15b6f02e8dc\nThe current data hash in the main repository (which tells shogun which revision of the data repo to use) is 1054d4f6a0a305540ddfdbbe6f4c05e14184efd1 \nAs you see, here, the data revision is an earlier one than your patch. This means that the integration tests (that the meta examples are part of) can't test this particular example.\nIn order to update, do git submodule update or pull it manually. After having done this, the data revision in the main repo should contain the latest hash of the data repo and you can add that to your PR here.\nLet me know if you still have questions on it, and don't feel bad about them ... takes a while to get through all that ;)\n. git submodule intit should also do it.\nBut it is just a clone of a repository, you can cd to shogun/data and do git pull as well\n. Thanks!\n. I am not sure I see the connection between extracting things and the error here, can you explain in the chat?\n. travis worked\n. All good, same list as for other cookbook on hierarchical\n. Ok this needs investigation :)\nCan you send me a gist with both problem illustrated, that I can put in my build and reproduce locally?\nThanks!\n. Ok got the error:\nYou constructed the QDA class with the (default) parameter store_vars=false\nThis means that you cannot extract covariances later.\nI fixed that for you in #3220\n. This is a good example of \"user(here you)  used shogun in a wrong way, but the error message is so cryptic that it doesnt help at all\" ;)\nWe better make sure such a thing never happens ;)\n. Feel free to send a separate patch with a new constructor for this case\n. Looks like you found another meta example bug ;)\nI will fix it, which will make the Python build fail....\n. You have to use True with a capital T for the Python example to work\n. It is very closely related, that is why you can link to LDA as well.\nThe wiki links are great! You can also get inspiration in the description\nI am checking the bibtex thing, that might be another bug ;)\n. Travis successfully executed the integration test https://travis-ci.org/shogun-toolbox/shogun/jobs/134037852#L3976\nmerging\n. Certainly doesnt harm. Thanks!\n. Could you fix the cast so that we can merge?\n. We would love to see this done. An efficient KKT solver would be such a blast to have in Shogun.\nCurrently, however, we are quite busy with the (just started) GSoC...So supervision is currently limited.\nThis doesnt hold you back from getting into it yourself. You can also send PRs and get comments from us, just expect delays ;)\nI think @vigsterkr has some code (more or less working as I understood) on this.\n. Yep you are right :)\nI am not sure about documentation, but you can browse the source a bit to see where such a solver could be used. Our class list is here:\nhttp://www.shogun-toolbox.org/doc/en/latest/namespaceshogun.html\nIn terms of optimization theory, you just need to know the basics of the method.\nIn terms of c++ optimization, it would be good to know how to make things fast ;) Maybe have a look around for inspiration of fast solvers in c++.\n@vigsterkr will know more\n. yeah definitely that\n. Deleting comment as linked in spam. Plus some REQUIRE cleanups\n. the debug code was removed in #3455 \n. You could work on any of the points I mentioned on top, e.g. replace all pointers to vectors with SGVector\n. please add an integration test file, see readme, needs to be a commit to shogun-data\n. Thanks!\nThis needs some tuning in the text and then is good to be merged\n. Not sure why java fails, checking locally\n. Cant reproduce the error locally :(\nrestarted travis, let's see\n. knn doenst fail, it is lda.\nCan you compile with java enabled locally and see what make test does?\n. The data revision update is also needed, i.e. the data version should be the one of your data merge in the other PR\n. Try calling the file lda.sg rather than LDA.sg\nSame for the cookbook page.\nSeems to be the only difference\nEDIT: I just checked. I can reproduce the error locally if i use LDA.sg. Has to do with the way javac treats filenames .... we just have to avoid upper case.\nCool, so this can be updated and merged soon then.\n. Great, perfect now. Thanks\n. Ah. I merged prematurely\nYour shogun-data file needs to match the meta example on \"lda\" rather than \"LDA\"\nCan you please rename the shogun-data file, and send another patch with the updated revision for this one.\nOtherwise this example is not part of the test-suite, see here:\nhttps://travis-ci.org/shogun-toolbox/shogun/builds/133839302\nPlease, next time take care of these things yourself. I.e. checking whether travis executes your example as a test.\nhttps://travis-ci.org/shogun-toolbox/shogun/jobs/133839303#L3929\n. Can you update the data revision, I just merged your patch\n. Just saying, why is there the GMM patch in here? Try to use feature branchs for such things locally so that things are not all together in the same branch, creating a mess ;)\n. Ok. From my side fine to merge once\n- my minor comments are resolved\n- unnecessary changes are removed\n- we have 100% unit test coverage of all tag based code\n. From my side, once all my (minor) comments are addressed, and all is squashed, we can merge\n. Cool, this was my last round of comments -- all of them were minor\nAfter they are solved, this is OK to be merged\n. Now or never :dancers: \n. Thanks!\n. Cool.\nWhat about a unit test that ensures that a single thread version produces the same as a multithreaded version and there are no memory errors?\n. Single core computation should not clone anything, i.e. set machine=m_machine\n. You can ask Shogun for the number of cores used via CParallel\nI think currently this mechanism kind of ignores openmp\n@lambday can you send a fix for develop?\n. What does valgrind say to this? unit test?\n. can you squash the commits after the rebase?\n. The benchmark should go to ryancurtins framework btw\n. We can merge as soon as you address the comments on the unit tests.\nAlso, if you write more benchmarks, put them in as unit tests as well.\nLet's get this merged by tomorrow early ok?\n. The times are interesting, I guess KNN is already multicore that is why doing multiple of them doesnt help?\n. As discussed\n- unit test should manually set number of threads to 3 (and still be memory clean with valgrind)\n- update @brief of the x-validation class: say that it runs with the current number of threads, and that it duplicates all objects (might be changed later)\n- update NEWS\n- squash commits\n. Great!\nThis is good to go once travis is green\nWe will re-visit for\n- parallel across runs\n- sharing feature matrices (needs some const cleanups)\n. Ok. Great!\nThis is a nice improvement.....we should get back to the shared features at some (later) point\n. This works locally (with java enabled). Testing whether travis fails, as in #3219\n. travis worked\n. I am OK mostly here (since it is feature branch)\nIt probably needs some clean ups\n@lambday can you also comment?\n. Needs rebase\n@vigsterkr from my side this is fine to merge\n. Integration testing data is missing\n. This is good to be merged after the minor comments have been addressed and travis runs the corresponding integration test\n. https://travis-ci.org/shogun-toolbox/shogun/jobs/134431490#L3975\nThis list needs to contain the test for the new cookbook (add data for that)\n. Thanks!\nWhats needed is integration testing data for travis to execute the test\n. Thanks!\nFine to merge after minor comments have been addressed and integration testing data is in\n. Updates?\n. Can you squash and write a better commit message? Want to merge this one soon. Way too old. Cool!\nI guess next step would be to test a GPU dot product, and then two cases:\n- GPU lib is available (and dot is computed on GPU after a transfer)\n  - GPU lib is not available (fall back to CPU)\n@lambday should also comment\n. Fine to merge from my side then\n. Fine to merge from my side, but some cmake guru should also check\n. thanks!\n. This is quite incomplete as of now, wont be able to run on travis.\nPlease always make sure make test work locally before sending patches (you can disable unit tests to speed this up -DTRAVIS_DISABLE*\nBTW did you look at the dataset you used? Can we get ground truth labels for the test features? Otherwise we cannot evaluate performance\n. Any updates here?\n. ground truth:\nopen the train data and labels file and see whether the pattern is easy. If so, just create a test labels file and send a PR to shogun data\nLet me know if you need help on this\n. Very good idea with the gist link actually\nPlease dont merge code without doc\n. After GSoC probably.\nFor now, please focus on clean-ups and refactorings. If you want to add new things, do that in a feature branch.\nI would also prefer some benchmarking or other type of stress testing before we go and add more and more methods.\n. Yeah, and that needs update. I talked to John Langford about this a while ago and he said we should ask for help, he will do so\n. Fine with me\n. Feel free to merge once travis is happy\n. Travis is green everywhere it matters, merging\n. Thanks!\nCan be merged once travis is fine\n. Can you provide explicit cmake command and post gists of the the output?\n. The link is 404\n. math init(0) will work after you removed\nexamples/meta/lextab.py\nexamples/meta/parsetab.py\n. Also, rebase against the just merged #3241 and you will have travis not complaining about static calls\n. ping me once it works (or fails)\n. travis fails?\n. no integration testing data yet\n. One more iteration and this is done. Looks nice already! :)\n. Ah travis moans, checking\n. Compile ruby and run it locally!\nCtest -R testname\n. I don't think this is error is related to the example. Look, it is caused by ld.\nTry to isolate the problem, and then ping me in irc\n. I am merging this as travis (apart from windows) was actually fine.. Needs data. Added a fix for static calls in R\n. Fixed the last error, merging\n. BTW you can better paste code snippets using three \"\" then it is displayed nicer\n. Its meant for single line code. Try use \"[newline] [code] [newline]\"\n. I added this in f2b6e3b0970c063124ee586488995e60c20f5853\n. Let me know if you think this one is better, closing for now\n. As discussed in irc, a try - except would be better\n. Good then, can you squash the commits into a single one? Then fine to merge\n. Thanks\n. Nice catch\"!\n. Thanks!\nTravis error is unrelated, merging\n. Ah name clash. This one here is ok.\n. yep, thanks! \n. Thanks\n. This also needs integration testing data. Let me know if you need help with that. It is also described in the cookbook readme\n. Looking forward to it!\n. This still needs integration testing data, ping me if you have questions\n. Any updates here?\n. Travis error unrelated, refactor patch, unit tests pass, diff looks good, merging\n. Thanks!\n. BTW.\nCan you please check that _all_ calls inside the omp are thread safe?\nOtherwise we might run into more problems later\n. @OXPHOS it is really important to read error messages.\n\"I triedmath.init_random()` and it didnt work\" is not useful. Also for yourself. Try to avoid this \"somehow doesnt work\" mindset.  The message in this case (code doesnt even translate) should have made you aware that something was done wrongly\n. This is good. Put the integration test data in, correct minor comments, squash and we can merge\n. http://www.jmlr.org/papers/volume11/escalera10a/escalera10a.pdf\n. All this confusion is actually due to me, as I didnt know about our seed=0 behaviour.\nSolution for all of this: use seed 1 :)\n. All green, merging. Please send a mini update for the minor comments\n. Leave the comparison example in, remove the others.\nBut for the others, please extend the existing cookbook\nYeah we can separate multiclass from classifer.\nBinary classifier\nMulticlass classifer\nFeel free to send a mini PR with adjustment for that. Thanks!\n. Code is OK, next step: integration testing data\n. can be merged once data is updated\n. I will merge this now, please send a mini correction for the above comment\n. It should be easier to use a bool vector...\n. @OXPHOS currently the meta language does not support\n- instantiating SG* types\n- accessing elements within\nI think we should add both. @sorig do you have an idea how much work that is? We would need to adjust the include path stuff (similar to what @OXPHOS did in this patch), and then wrapping the [] and () operators might be nice as well. What do you think?\n@OXPHOS Let's see what @sorig says before we continue here\n. 1. Remove the build dir\n2. Clean up the examples/meta/generator/ parser output\nIt works on my machine. Ask travis, it might confirm\n. @OXPHOS I can look at this, shall I just copy the meta example in this PR? Does it reproduce the errors.\nWith Java, there is a problem with strong types and implicit numerical downcasting ... Ill look intot his\n. The \"other\" problem should be solved in #3451 \nLooking into the other tomorrow or so.\n. There is another problem with csharp and boolean vectors --- we don't have a typemap, which means we cannot pass boolean vectors as input in the csharp interface, see #3452\nThis, sadly, means that for now, we have to change this example to not using boolean vectors that are hand-constructed as parameters. Instead, we can create an instance of the (mapped) BoolVector as an object and then use a method (say zero) to populate it (hacky)\n. Can you rebase? Should work now\n. I guess this needs an updated data version and a squash. It is all ready otherwise\n. Can you please run the tests locally (at least the cpp version) before putting things into the PR. This would have been detected locally.\nIs done with\nctest -R integration_meta_cpp-multiclass_classifier-cartree -V\n. See the error is coming from a missing shogun-data update\nThe generated test passed:\nhttps://travis-ci.org/shogun-toolbox/shogun/jobs/175608246#L3717\nBut checking against reference data failed:\nhttps://travis-ci.org/shogun-toolbox/shogun/jobs/175608246#L4322\n. And it even tells you the line where it differs:\nhttps://travis-ci.org/shogun-toolbox/shogun/jobs/175608246#L4427\n. So let's get it in then :)\n. Ah sigh, I thought I had solved that. But we have a mechanic to fix this...checking  ...\n. @OXPHOS rebase against develop, then it should work (I tried locally for all interfaces that failed in the last build here)\n. @OXPHOS You should always try these things locally before you push. At least in one modular language.\nSo here is the listing I used (no fixed random seed)\nhttps://gist.github.com/karlnapf/6cf4186dc77861681ceba938f397f2c0\nNote that meta language syntax is True False\nAlso, check out the docs for git commit --amend, which allows you to update your commit, so that the PR doesnt have like 5 commits, but just one. I dont understand the failure in octave. Checking again. The rest looks fine. Ok so works locally for oactave, I restarted the build\nJust need to squash the commits, then we should be able to merge (fingers crossed travis is fine). This seems to be specific to the travis setup, I don't have this problem locally ....\nEDIT: Managed to reproduce now.. @OXPHOS I solved the problem with octave. Man, this is a beast to get merged.\nBut I think you can squash the next time before you push ( after rebasing against develop once #3559 is merged, EDIT: it is merged now), since all the rest works. Nice one, this was a big effort to get these Boolean vectors working. But it really helped in figuring out corner cases of the whole system. Thanks for the patience :). Can you valgrind your mini example for leaks?\n. So this works now?\n. Integration test data?\n. You produced a type error in java:\nhttps://travis-ci.org/shogun-toolbox/shogun/jobs/137217805#L2624\n. And csharp: https://travis-ci.org/shogun-toolbox/shogun/jobs/137217806#L1602\n. Merging as good. Needs data. Cool thanks for that. Once we have a json update, I can take over\n. Let me know when the vector / matrix access stuff is there.\nIn the test file, it would be good to check both accessing matrix and vector\n. Great thanks\n@oxphos this will soon allow for vector element access in meta examples, after I added some minor adjustments\n. @OXPHOS you should be able to use something like this now\nCan you try?\n. keep in mind to remove old parser output\n. I currently cannot look at the rendered html\n@vigsterkr it says \"service not available\" behind the preview build link\n. Why not satisfied?\nFrom my side, all this needs is data and a squash, and then we can merge it! :)\n. Thanks! :)\n. I think your forgot the .rst :)\n. Also needs integration test data\n. Minor comments, pls send a separate fix. Merging. Thanks!\n. Covered by unit test this one?\nOtherwise fine to merge\n. This is all green already, so could merge. But waiting for adding the other test\n. Can I suggest we split this PR?\nHave the base class first (addition only, unit tests), then refactor other kernels one by one\nI will ignore all other files than translation invariant for now if that is ok. If not, let me know\n. Fine to merge from my side apart from some minor changes requires, the most important one being the class name change...\n. Fine to merge otherwise\n. Nice changes and useful. But man, this takes like 10 mins to do. Where is the other progress? :)\n. Lets aim to merge this soon, before the parallel stuff starts.\nJust ping me and squash to trigger that\n. @vigsterkr Shouldnt we fix these in a separate patch to keep this one smaller? (It is only about optimising speed with the existing implementation). But definitely should fix the static analysis errors\n. Fine to merge from my side, just some very minor comments\n. @vigsterkr check and feel free to merge\n. Great!\n. Most of my comments are minor.\nIf all the tests pass, ready to merge this, but keep an eye on the buildbot to make sure it really works\n. All good. See minor comments\n. Please squash and we can merge\n. Thanks!\n. It does! And it does need so first, and then the data version in here needs to be updated.\nBut all good otherwise and thanks!\n. Can you bump up the data revision, then we can test this....\n. Merge conflict.\nThis is since I merged other cookbooks in the meantime.\nYou have to rebase, sorry.\n. Ok merging as travis was green before\n. @OXPHOS please have a close eye on travis and the buildbot. Ping me asap if something broke\n. restarted some\n. Minor, all good here and travis was green before.\n. Data merged, you can update\n. Ok, need change the filename and some mini updates, then update data, then good to merge\n. Hi @sanuj \nCan we get this merged soon?. Would be cool to get this one in the end!\n. Merged data, you can update it in here\n. I guess this is almost ready, needs updated data version.\n@Saurabh7 pls check the travis build and tell me once it executed the integration test for this examples\nThen merge\n. needs rebase,\nany updates?\n. Nice!\nCan we have regression as well?\n. Thanks\n. Cookbook is good, needs only data.\nI have the same error locally (not sure what happened before when I said it worked) Will work on it now\n. I have a feature branch with a test for all vectors, which currently ---- is broken ;)\nI will try to fix things the coming week, we can keep this one open for now\n. ehm :)\nI actually fixed more of the travis in that branch today, see here\nWhat is left is csharp (can do now) and java (need patch from @sorig)\nUpdate: csharp works locally now, build coming here\n. please continue to ping me on this :)\n. I am sure this has to do with the types used.\nCan you change/try to fix the java output and then rerun it using ctest -R  generated_java-multiclass_classifier-chaid_tree Once you figured out what is wrong, I can fix it.\n. The way to solve it is to run the (generated java)  listing manually as I said above, and then correct it first, then find out why it is generated wrongly\n. This listing works for me on latest develop:\nheiko@heiko-ThinkPad-T440s:~/git/shogun/shogun_develop/shogun/build$ ctest -R generated_cpp-chaid_tree \nTest project /home/heiko/git/shogun/shogun_develop/shogun/build\n    Start 47: generated_cpp-chaid_tree\n1/1 Test #47: generated_cpp-chaid_tree .........   Passed    0.02 sec\n. I am using IntVector in there. The travis error is when compiling the generated cpp code, which fails due to a type error -> easy fix\n. Hi!\nCan you check whether things work now? They should, I fixed all problems and merged the latest patch by @sorig \nRebasing should do it\n. Great!!! :)\nOn Wednesday, 2 November 2016, OXPHOS notifications@github.com wrote:\n\n@karlnapf https://github.com/karlnapf They did work! Let me polish it\nand update the test dataset\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/shogun-toolbox/shogun/pull/3303#issuecomment-257946312,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAqqv7c4WYpxZviBDO_NjX6dJUFLOxCNks5q6M6FgaJpZM4I6iuU\n.\n\n\nSent from my phone\n. So? :)\n. All we need is a data commit to have this merged\n. This is fine to merge but something is wrong with this data version.\nDid you git pull in the data folder and the commit the appropriate version?\n. @vigsterkr the link on the buildbot of the cookbook build is gone ....\n. Ill merge as this finally is good :)\nNice\n. integration test data\n. ready, pls squash and add the merged data\n. Fine to merge from my side once trabis is green\n. @lambday can you please create a feature branch that @OXPHOS can send this patch against?\nI dont want to merge this into develop yet, but in order to keep the patches small, I definitely want to do multiple patches.\n. Cool @OXPHOS , it is getting there!\nI just started a new branch feature/linalg_second_take\n@OXPHOS can you send a PR against this one. We can later git-cherry-pick the old one (if necessary) and start merging things -- the interface will stay like this even if we change things inside (like macros)\n. integration data\n. Only minor changes remaining. Pls update, squash, and we can merge \n@yorkerlin check out the link Good how your stuff looks like, no? :)\n. Yep, we can change that, it is an easy grep....\n. That is, since EuclideanDistance can deal with sparse vectors, this is not needed anymore?\n. Fine to merge, should be added to the NEWS API changes\n. Can we rebase and merge this @lambday \nshouldnt be a lot of work .... +1 Good stuff.\nThough the typename should always come from provided features, currently the templated methods somehow make things redundant\n. Time to squash all commits into one and then I'm happy to merge\n. Man, what an effort. Well done!\nI hope you are happy with the result, we certainly are :)\nI will merge this now. Please\n- send a clean up patch for my latest set of (minor) comments\n- add to the NEWS file your contribution\n- think about what you learned and how we can generalise this to other Shogun classes, and which.\n. needs rename and squash, data, then can be merged\n. Great!\n. we want to use linalg for gpu stuff.\nc++11 should happen, but not before end of GSoC 2016 please.\n. I guess that since we are now switching to c++11, we can make use of the in built parallel of the language.\nomp doesnt work for windows?\n. Not sure either. omp is the easiest thing to use. But we cannot stop threads with it.\nC++11 might be better there, maybe send a patch using it?\n. yeah do it! BTW I also talked to Michaelis Titsias and he said he would have a look at our code and check where it can be improved, will write an email\n. Great!\nThis is getting there indeed. I made a ton of smaller comments that are about polishing the doc and general cleaning. \nI am fine to merge once travis passes (there are some nontrivial problems still)\n. - please add base class for GPU backend with purely virtual transfer methods\n- some minor comments\nmerge ready apart from these things\n. Just group the tests more nicely\n- Tests SGLinalg in general (without backend specific operations)\n- Tests for each backend separately\nThen we can squash and merge\n. Group tests, squash, travis green, merge\n. OK, all my comments were minor. \n@OXPHOS can you\n- change these mini things?\n- double check that the gpu vector is freed in destructor\n- squash again.\nThen fine to merge. @lambday do you want to have a final read and then press the button?\nFINALLY!\n. :dancers: \n. @vigsterkr why is there no cookbook preview here?\n. Almost ready to go\n. Thanks for that!\n. thanks!\n. Fine to merge from my side\nIf you want to, you can send another patch where you remove range_fill from SGVector, and then replace all the compile errors with sg_linalg\n. Pls squash\n. All travis parts that matter are green, merging\n. Yep pretty easy.\nNot sure what the default should be.\nI have the feeling that shogun develiopers should decide this. So we can add a flag to CMachine that it only uses features read-only, and then set it to false by default and override where we know.\nBut I feel there is a better way\n@vigsterkr @lisitsyn ?\n. Features will be read only soon. Remove the add and I am fine to merge\n. close?\n. I see, no need to re-open PRs if you force updates btw\n. Fine to merge from my side\n. Ill merge this one\nWe should definitely have one that slices in some direction\n. So all linalg::dot calls do still work? Great! :)\n. Lets wait for travis\n. I'll merge for now. Pls send a clean up patch for the minor issues. Thanks!!\n. The comments are more cosmetic nature and to keep the PR more sturcutred.\nAfter addressed, can be merged\n@lambday feel free to\n. Valgrind checked?\n. Awesome\n. Generally fine, but pls also apply the changes discussed in the matrix PR here\n. Just to make sure:\nWe want to replace existing lib linear codes with the new version, not manually apply the differences they did. This just seems to be the parallel sums?\n. Updates?\n. These differences are not OK. Any ideas on what they are caused by?\n. Ok then, make it merge ready and we can proceed!\n. its the integration tests, easy to fix, simple send an update of the data for them that was generated with your new liblinear. Make sure that the differences are only epsilon\n. Merged it, you need to update the data version then here as well.\n. Can you squash so that we can aim to merge this?\nBenchmark comparisons of linear classifiers?\n. Fine to merge from my side\nEDIT: After some minor updates, and some runtime before after and sklearn benchmark\n. Did you run the same silver type?\nShogun should also print the final value of objective if it hits max iter\nWhy is shogun faster with more iterations??\n. Just accept the labels that zoq provides and then convert them to the format shogun accepts. No multiclass strategy needed\n. @micmn this one might be cool to pick up by you ... . Absolutely,\nI think focussing on the optimizers first is fine!\n. Looks good to me, merging!\nWill try to do the nystrom one next week\n. This is Great! Cleaner!\n. This is cool!\nNow we need to tidy it up and integrate into the build properly\n. Lets proceed in feature branch.\nBut we need to be careful here --- we dont want this to be not merged for months or years, we might need to solve this ourselves.\nDo we have a hunch how to do a workaround?\n. Just checked, can't we provide a workaround? Doesnt seem hard\n. no variable needed.\n\u00b4-DRModular` is the cmake switch\nhave a look at the cmakelists.txt for R to see how you can run R examples\n. needs a rebase\n. Ah yeah, I think we need some SWIG compatible wrapper method to set the number of threads. Like for init seed.\nThis thing you do is not compatible with the meta grammar.\nI assume get_global_parallel doesnt work?\n. Pretty good, this is mergable once the minor things are addressed,\nwe can hold the integration data back until we sorted out the thread setting...\n. some minor adjustments and then merge ready\n. Much cleaner! Ready to merge when travis is green\n. Great thanks for that!\nThis test is very high level, can we test some of the subcodes of knn?\n. Updates? :)\nWould be good to get that merged as one of our GSoC students is currently working on speeding up KNN\n. Let's do it\n. Cosmetics, can be merged\n. @lisitsyn is the boss here\n. Yes we do plan to use them eventually. We in fact aim for a pointer free interface ....\nInternally, we can do whatever we like.\nSerialisation is currently being worked on, come to irc to discuss. \nI will close this issue for now, let's do sch discussions somewhere else, issues should be kept for more concrete stuff.\n. Make it a sub-class then....As I said, the motivation to not do this earlier is not existing anymore.\n. If you just dont do SG_ADD, then varialbes will be ignored when serializing.....\n. looking forward for the update here.\nYou can clean up and squash, document already\n. As to your questions:\n- ref counts do not really need to be serialized -- if you de-serialize an object, its refcounter starts from scratch. The only difference is if a class contains two references to the same memory, which of course needs to be reflected in the counter. I am not sure Shogun currently does any of that.\n- The enum thing seems to be the only way out the type madness, so OK\n- please send a whitespace PR, of course\n- not sure about the readable archive error, maybe lets discuss in IRC\n. Nice!\nLets clean up, split the patch, and merge it!\n. Any updates here?\nalso needs a rebase\n. Not sure whats next here, as said above, this needs be cleaned up and then can be merged. Currently, travis is broken\n. Can we also parallelise thisi eventually?\n. Why are we slower than sklearn? Any hunches? Looked at their code?\n. Mmmh this is getting too big, we need to chop it into pieces. Can you send smaller PRs with some explanation what you are doing?\n. Cool stuff. \nBut the KD tree is also necessary, and a general clean up.\nPlease do the patches one after another as small as possible so that we can review easier\n. needs data and ready to merge\n. Python bugs are generally possible but not likely on something like a sum method.\nI mean do @yorkerlin suggestion sanity check, and if not, replace things with the bug free version :). For your suggested change above, I guess these are rounding errors arising from the substraction.\nSo yes, do the triangular view thing. It will be slightly slower, but I think eigen3 will still vectorize things so all good.\nYour last message I dont understand. Once the correction is in, can we merge this one soon? Need to get those linalg things out of the way for GSoC 17. rebase\n. Looks good to me\n. Can you squash the commits to a single one?\n. I will be away for quite some time now, so leaving @vigsterkr to merge this\n. http://buildbot.shogun-toolbox.org/cookbook_pr/8574a1c30f7b78a2b5ac76318a817c7ec3146788/\n. The second check always renders the cookbook for you\n. Travis is failing due to interface problems. The SWIG changes you need are two files. Regression.i and Regression_includes.i\n. I am away for quite a while now so leave merging this to somebody else (ping ppl in irc)\n. Can you squash the commits?\n. If we add testing data, this can be merged. Wanna do that? :). Ill merge for now, feel free to send a test. rebase\n. OK from my side (as long as travis is)\n. Great, is there a benchmark? Not scalable in what sense?\n. Will be away for some time but answer when back in Sept\n. Ah I see...so we dont have these fancy stochastic EP updates for large scale sparse classification\n. Yes, I recently added a new repo for the wiki which can be cloned in a\nstandard way and then you can send a or against that\nWanna try?\nOn Monday, 31 October 2016, Soumyajit De notifications@github.com wrote:\n\nThis should be a wiki, no?\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/shogun-toolbox/shogun/pull/3414#issuecomment-257228555,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAqqvwdVSZPUfCdPNfkA66boxL7gK-Roks5q5Zd4gaJpZM4JfP6Q\n.\n\n\nSent from my phone\n. Ill do it\n. https://github.com/shogun-toolbox/shogun/wiki/README_tags\n. @OXPHOS hi there! Let's get this merged ! :)\nSo I guess we can avoid all this duplicate code with defining some macros no?\nHowever, I dont think we can get around explicitly listing all these cases (unfortunately) we had this problem before ...\nIt is not very nice, but I am fine to merge this stuff anyways -- can discuss in IRC.\n. Let's get this in! :). Rebase and merge? :). Well done on doing this review publicly on github!\nWe will probably do it this way next GSoC\n. This seems to be some LAPACK linking issue. Is you configuration of LAPACK up to speed?\n. Thanks for clarifying @beew !\n. @lambday can I leave discussing this one for you? And then one of us can merge once youre happy\n. Cool so can we merge this thing?\n. How much faster is it? :)\n. @micmn this one could be interesting for you to pick up as well. I have put this to: https://github.com/shogun-toolbox/shogun/wiki/README_plugins\nFeel free to edit it there. This is likely to be an eigen3 caused problem (i.e. KernelPCA is guarded)\nCan we have some more output?\n. Check your python setup\n. Ill close this for now. If any problems, let us know!\n. static interfaces are deprecated and will probably be removed very soon\n. We don't support the old command line interface anymore (even though the test-suite passed largely)\nI suggest you try the Python interface, which you can easily use from a notebook or so\n. Commits should be squashed to one\n. Fine to merge apart from this.\nIs ROC unit test covered? If not, would you mind adding one?\n. OK, some minor things then we can merge.\nAbout the unit test. If there was an uninitialised memory error earler, you should add a unit test that causes this error before your patch and that passes after your patch.\n. Merging, as travis failures were unrelated. Thanks!\nWhats the fail? Maybe open another issue if you have a minute\n. Great!\nThere is much more needed. I can help out, ping me in irc :)\n. This is part of #3446 so closing.\nThanks for the reporting\n. done\n. That seems a bug, thanks for reporting.\nJust to confirm, you run the above code ? Because the error message seems to have other things in it?\n. Thanks. I just realised the strings you put are uninitialized. Is that maybe cause of the error?\n. try the examples in octave_modular --- the ones you are running are outdated. Ill close this for now, let me know if you still have problems\n. Yep, this also depends on finally tackling a proper base class for distributions, #1998, #1929\n. @OXPHOS after this change, your chaid tree example works locally for me. I'll merge once travis is happy\n. @vigsterkr it is kind of annoying that the build is always marked as failing from the AppVeyor ....\n. Ah I see ...\nLets merge is anyways? I dont know how to fix them as I dont have windows ....\nAny ideas?\n. @OXPHOS ping me in irc about such things, faster :)\nRe-build shogun to solve the last error.\nThe first one works for me locally, can you maybe reproduce this on travis in a PR?\n. Shouhd be fixed in #3461 and the subsequent https://github.com/shogun-toolbox/shogun/commit/a39e8e4da4f3a71c1360b76c47e194548851326e\n. But we should still add the typemap -- even the build now has a workaround\nTypemap force @vigsterkr @sperka\n. Typemap force @vigsterkr @sperka\n. We currently have a workaround that avoids problems with this, but still would be good to fix\n. Thanks for reporting.\nBTW: The(static) matlab interface is deprecated. We plan to do a SWIG based on as well at some point\n. You are using a deprecated shogun interface, please use the modular interfaces.\nFor your error, this might be caused by GLPK not being installed when you compiled shogun.\nIf the error persists in modular interfaces, please reopen another issue\n. Thanks!\n. The error message tells you what is going on\nCMake Error at CMakeLists.txt:739 (message):\nCtags required for meta examples.\nEither install ctags or disable meta examples. Thanks!\n. Is this still happening?\n. Can you please remove the long log here and paste a link to a gist instead? Otherwise chances that somebody reads it go to zero.\nNext, could you post your system specs?\n. Sorry about the delay. This unfortunately is a bug. Will look into it and adding to 5.1 milestone\n. I locally have the same errors on ubuntu 16.04 with latest develop and standard packages, R version 3.3.1\nhttps://gist.github.com/karlnapf/69946d524a9773b6cb4d9479ce93097e\n. Thanks for the hint! This is since R 3.3.0\nJust for the record, adding define NO_C_HEADERS in the sg_print_functions.cpp (manually, after it was generated by SWIG), results in different compilation errors\nhttps://gist.github.com/karlnapf/9538ea56ead65bc1c470543ff6c3bd95\n. So the NEWS entry for 3.3.0 you refer to says:\nhttps://cran.r-project.org/doc/manuals/r-release/NEWS.html\nWhen R headers such as \u2018R.h\u2019 and \u2018Rmath.h\u2019 are called from C++ code in packages they include the C++ versions of system headers such as \u2018<cmath>\u2019 rather than the legacy headers such as \u2018<math.h>\u2019. (Headers \u2018Rinternals.h\u2019 and \u2018Rinterface.h\u2019 already did, and inclusion of system headers can still be circumvented by defining NO_C_HEADERS, including as from this version for those two headers.)\nThere is also this in the NEWS for 3.3.2\nUse of the C/C++ macro NO_C_HEADERS is deprecated (no C headers are included by R headers from C++ as from R 3.3.0, so it should no longer be needed).\n. Same error with R 3.3.2\n. Fixed via #3538 . Let me know if it worked :). Any ideas why we will get this error?\n. Ah yeah, easy to fix, you need to expose this method to SWIG.\nIt is currently guarded with '#ifndef HAVE_SWIG' in 'SGVector.h' and 'SGMatrix.h', along with most methods in these classes. Exposing the getter/setter will fix the problem and won't hurt anywhere else\n. Ill merge this and fix myself\n. should be solved with your patch #3463 \n. Fine to merge once travis is happy\n. Cool!\nCurrently, doing static calls via the swig r interface doesn't work. If\nthey would work this is solved. So check swig and r and how it works in\nshogun. Nontrivial so only attempt if you know what you're doing\nOn Tuesday, 25 October 2016, Sourav Singh notifications@github.com wrote:\n\n@karlnapf https://github.com/karlnapf I would like to work on the\nissue.How do I start?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/shogun-toolbox/shogun/issues/3466#issuecomment-256097670,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAqqv8a5bahf78-pQ1xaYLagfmVQqpc7ks5q3jdcgaJpZM4KeE-H\n.\n\n\nSent from my phone\n. Nope\nI activated them in this commit in a feature branch\nhttps://github.com/shogun-toolbox/shogun/commit/e9c079b91a134de642c3cbdf61b52e1ae0dc81e5\nAs you can see, travis and the r build failed.\nYou can check this out locally, enable r interface, and run the tests to\nreproduce the error locally\nOn Wednesday, 26 October 2016, Sourav Singh notifications@github.com\nwrote:\n\n@karlnapf https://github.com/karlnapf I found the patch which fixed the\nstatic calls here-#3251\nhttps://github.com/shogun-toolbox/shogun/pull/3251 So I believe I need\nto enable meta examples one by one and check if they pass\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/shogun-toolbox/shogun/issues/3466#issuecomment-256449810,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAqqvzaKPJPNnFaT4ryeMRLCVrgzhpdoks5q36ecgaJpZM4KeE-H\n.\n\n\nSent from my phone\n. This has high priority as we are currently not executing any R bindings in any test (removed the modular examples)\n@vigsterkr \n. Thanks!\nHey, could you please start writing informative commit messages?\n\"Fix a bug\" is a no go as not helpful at all\n. Thanks :)\n. Hey WU,\nit would be really cool to have meta examples for this stuff. Really simple to turn your code above into that.\nEven better would be a cookbook, even though that is more work, but at least the meta example is needed!\nAlso, such changes should be mentioned in the NEWS file!\n. Fine to merge\n. The script should be turned into a cookbook for the GPs......\nAt least a meta example (super easy)\nBut I can do that\n. Can you please update the news file?\n. And as said, an api example would help \n. - Removes modular interfaces for all languages but python\n- removes all static interface examples and desriptions\n- removes all GUI* static interface classes of libshogun\n. @vigsterkr please merge if you are fine with it\n. First build passed. Since all are the same I merge\n. We actually disabled the feature in develop as there are problems with\ndeadlocks which we need to look into.\nYou can always activate it back and play with it. Tell us what you do and\nif it works and we can put it back in\nThe locked thing is on my list as well\nH\nOn Wednesday, 26 October 2016, Leon Kuchenbecker notifications@github.com\nwrote:\n\nCrossvalidation is currently not parallel if the machine is locked\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/shogun-toolbox/shogun/issues/3479, or mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAqqv34zcwVpXzRPqBle5QYJA89QUlP7ks5q30xKgaJpZM4KhION\n.\n\n\nSent from my phone\n. @vigsterkr so right now the nightly build is green even though the opencv notebook still fails. I guess in terms of keeping things passing, we should make it fail if a notebook fails, no?\n. Another nice task is to clean up all the old code in there (use SGVector rather than pointers), and make use of linalg or eigen3 rather than all these loops.\nFinally, replace pthreads with C++11 threads or openmp\n. Yes that's all still useful to clean up. Start with something simple.... not everything at once . Let's discuss such questions in IRC and keep the thread clean of them.. This is not about improving performance. This is about outputting the state of optimization algorithms.\n. Steps:\n\nUse SG_PROGRESS in cases where it is not used, i.e. all outer loops where it makes sense. This is more a task of identifying methods that dont have any progress at all (x-validation, grid-search, gradient model selection, etc)\nFix behaviour of SG_PROGRESS if wrong\nIf there are design short-comings in SG_PROGRESS, discuss/suggest an improvement. Investigate how other libs do this, check if we can adapt ideas. Potentially refactor all of SG_PROGRESS uses in Shogun\n\nAll steps are independent but I think this is a good order of approaching them\nAnother cool thing would be to integrate SG_PROGRESS with ipython so that notebooks can show this. LeastAngleRegression needs one. What about other languages?. I think if you rebase it should work\n. I can rebase the feature branch..... shall I?\n. rebase. Nope not relevant ... restarting. All yes :)\nGenerally dop more comparisons, and make the notebook nicer overall. You can browse the web a bit for more inspiration of where KNN is useful and where a fast KNN is useful\n. You directly can open the notebook in your browser. Google for ipython notebook and how to use it. Then you send a standard patch on via github pull request. Please clear all output of the notebook before sending it. And also please provide an html link to a fully rendered version (say on gist)\n. Can we moive the discussion about your changes into a PR?\n. Yes\n. Google on how to submit pull requests on github\n. BTW you can always update old PRs, no need to open new ones allt he time\n. Hi @MikeLing \nIt is hard to see from a gist what you changed/added. Can you send a PR, it will show me the diff. For that, pls remove the output of the notebook. Then in addition post a gist like the one above, so I can see how it looks.\nThen we can discuss in the PR. Make Shogun's eigen3 use it. If that doesnt make it clear (nontrivial task, will require solving lots of subproblems using the Google-it approach), better pick another entrance task\n. Absolutely! :)\nNice entrance task as well\n. the template is in the notebooks folder doc/ipython-notebooks -- as well as all the notebooks\n. Well, this is to document the ipython notebooks that shogun has: how to run / change / add new ones / what style they are in. Probably not the best thing to start with if you have to ask what we want there. This comes since FindMetaExamples.cmake sometimes removes certain meta examples.\nThe cookbook should still build fine, just ignoring that particular page.... \n. Yeah, it is not really a solve but rather a workaround :) We should deal with this properly....\n. Fixed in #3665\nThanks!. Thanks!\nCan you post a gist of the notebook with outputs?\nAlso, can you mention what changed? The diff is not helpful here\n. BTW can we please split this patch into two:\n1. Just updating the format, no changes at all\n2. The changes you did\nThanks!\n. As we cannot see the changes in your diff (all lines are different due to the format change), can you please send a PR where you only update the format (no other changes) first. Then we merge this, and then you can send the changes you made. This way, I can see them in the diff which is important for future bug hunting (knowing in which patch were which changes etc)\n. Can you make this a single commit? This is called \"squashing\"\n. Ah I see, I guess you need to rebase against develop branch\n. We'll fix the other ones differently. Thanks for this\n. the opencv one is the only one failing for now I think, this is due to an API change arising from re-structuring the opencv codebase....\n. Please git grep for the interface names in the shogun root directory. They are hidden at more places.\nWe can only merge this for the next major release unfortunately. But it is appreciated anyways\n. Hi!\nCan you please first send a pull request with no changes in the notebook, apart from the formatting changes? This patch has every line in the notebook marked as different, which makes it impossible for me to compare. Then, once that is merged, send a second PR with the changes (which then will show up nicely). Thanks!\n. Thanks!\n. So I guess the leak was due to a forgotten SG_UNREF?\nIs the unit test binary checked with valgrind? Or at least your test now?\n. Nice one!\n. Thanks for the nice report. I actually dont know what is going on there. maybe @vigsterkr knows?\n. Cool thanks!\nCan you post a gist of the notebook with outputs?\n. Can you please post the full notebook so I can see how it looks?\nReading the text changes is a bit time consuming in diff otherwise\n. So you didn't add the KDtree to the last comparison?\n. No you did not, as I wrote above I mean the last comparison.\nThe change you did is useful, but it would be cooler to apply it to the whole notebook you know\n. closing for now. Feel free to re-open if you put updates. Nice thanks.\nCan you post a gist of the notebook so I can see how it looks?\n. Sure, in this case that's ok since no outputs have been changed. My bad :)\n. Can you please squash the commits?\nSee our new DEVELOPING.md readme in the wiki for how our process works\nThen ready to merge. Please dont close pull reqeusts and then open other ones. You can always update a PR. The only way around \"doing something terrible\" is to read the docs.\nCheck our develoing.md readme in the wiki for some starting points on how to use git. Ok sure. \nIn the future dont delete and re-create your fork. It is not necessary and the worst approach to \"fixing\" git problems. Just read the git manuals.... Thanks for the feedback :)\nSo what is this issue about, getting things merged?\n@vigsterkr seems like there is only one commit in there?\n. Thanks! :)\n. Yes, that will be done in a single step:\n1.  remove all math methods from SGVector\n2. Lots of compile errors\n3. Replace the calls with new linalg\nSo you can always get a feeling for what is missing by doing step 1\n. I wouldnt remove things for now. Let's finish the linalg branch, remove old linalg in there, replace calls, merge to develop. Before we touch SGVector\n. Needs rebase. Ok so lets do the minor cleanups and then merge this one. \nBTW it is not useful without a triangular solver, maybe this patch should already include it?\n. There is upsides and downsides with that.\nMaybe we can use \"cholesky_solve\", like scipy? One methods per factorisation .... I dont like passing enums on the decomposition types around too much\nhttps://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.linalg.cho_solve.html\nhttps://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.linalg.cho_factor.html. Let me know when this is ready to be merged. Ok let me know once this is done. Let me know how things go and whether you need help. I'll check this these days...\nIs everything else in here clean? Then I'll clone locally and try to come up with a fix...\nPing me if nothing happens within a week or two. Merge conflicts. There is still this inf error on travis. What was it again with that?. Yes it will be a specific version problem.\nWould you mind getting the same version that is running on travis (ubuntu 14.04, installed from the repository, see config/Dockerfile), and compile with that locally to find out whether you can reproduce the error locally?\nNext step: how to fix (I bet it has to do with the upper/lower interpretation changed or something)\nThen, if that is possible, we need to write a guard that fixes the problem for the particular eigen3 version. One way to go ahead here would be to disable the tests, and put a warning about the eigen3 version in cmake, see https://github.com/shogun-toolbox/shogun/blob/develop/src/shogun/CMakeLists.txt#L222\nThough it is better to fix things since these eigen3 versions are shipped.... Grep is your friend here\n. It should be in install already\n. This is not about changing readmes, but the cmake error message that you get when the data module is not present.\n. Fixed by #3551 . This is a good test, did you check it with valgrind for memory leaks?\nFor that, just run it isolated (using the gtest_filer argument of the unit test), and see whether all allocated blocks are also freed\n. Few minor style changes needed, then we can merge. Thanks! :)\n. Thats ok, leave the things as they are. Just for new ones we want to be consistent with you style.\nIf you feel inspired, feel free to send a style fix PR (low priority)\n. So in the added lines, there should be no camelcase anywhere.\nThe test names can be camel case (so exactly the opposite way of your update:) )\n. As I said above, please do not mix style fixes with adding new code in a single PR.\nPatch 1: your new test, with the correct naming of variables\nPathc 2(optional): changing the style in the other tests in the file\n. Yep but that's not what I wrote. Please don't change style in existing code in this patch, send a new one for such changes\n. Please read my above comments.\n. Well, write something that trains a models, serializes it, deserializes it, and then applying it to test data gives the same results. Then automatically make this happen for all models. Please read the \"getting involved\" guide in the wiki, about starting with Shogun. We can't hold your hands unfortunately. I suggest you pick something simpler if you don't know how to start here.. We have serialization working at the moment (at least in theory). The details under the hood will soon be replaced, but that doesn't matter for this task.\nTry to come up with a simple way to test serialization / de-serialization of all models.\nAlgorithm for supervised methods (CMachine subclasses)\n\nLoad training data\nTrain model on training data\nSerialize model to file\nDe-serialize model from file\nApply model to test data\n\nUnsupervised methods and other will work similarly, but let's start with the supervised ones.\nThe best thing was if we were doing this automatically for modes of similar classes (supervised, unsupervised, kernel machine, gaussian process, etc)\nYou can start via writing a unit test for a single model, as a prototype. However, eventually, we want to use some template engine to generate unit tests for all implementations we have of a particular class. We did a similar thing for serialization itself here, using jinja2. I would start writing a simple test for say a CLinearMachine and then make it automatically be generated for all subclasses.\nThe cmake file that generates the tests using jinja2 is here. The data should definitely be generated elsewhere. But in fact, we don't we just use the available datasets we have? See the meta examples, I  made sure that all datasets used in there do actually make some sense. I dont think you need to distinguish in the code between classification and regression. If you just used apply rather than say apply_regression, then you get a CLabels pointer, but serializing it actually should work.\n...of course this is modulo the data passed -- maybe we can have a class that you can give a CMachine instance and that returns you a pair of CFeatures and CLabels that correspond to the machine's problem type? Then the test code would be agnostic of the underlying problem type and therefore a but cleaner?. While the explicit list of things to be tested at the end of the file is great, I wonder whether we could apply this to all CMachine instances by default, and have a blacklist instead? For that, we need some code that works on generic CMachine pointers first (maybe using the data generation trick I mentioned above). Great start indeed, this is exactly what I had in mind. Want to send a PR for further discussion?. BTW I think we want a travis option to disable this thing as well, like -DTRAVIS_DISABLE_SERIALIZATION_TESTS. I would put it as tests/unit/base/trained_model_serialization_unittest.cc.jinja2. Yeah I was thinking that, but then I decided I think it should. Because this should work for all models. Dataset should be tiny though.... This is done, no? @micmn @vigsterkr . You know what is missing? Shall I re-open this to finish?. BTW I think we can also do these tests without jinja2, as done here https://github.com/shogun-toolbox/shogun/blob/develop/tests/unit/base/SGObjectAll_unittest.cc#L235. Tree Machines as well (they dont even serialize atm). The proper fix:\n- remove the extern \"C\" in the sg_print_function.cpp\n- only do when R version is more recent than 3.3.0 since previously it was necessary\n- for that, cmake needs to detect the R version\n. @vigsterkr thoughts?\n. Actually, I think we should just remove the extern C business.\nSee the bottom of\nhttp://www.hep.by/gnu/r-patched/r-exts/R-exts_97.html. Updated the fix to remove extern C -- the R manual says this should have never been done.\nThis will fix #3460 and should be backwards compatible. I guess the error message tells you what is wrong: you need ctags for this particular build configuration.\nYou can pass -DBUILD_META_EXAMPLES=OFF as an option to the cmake to avoid this.\nYou just wont have the examples then\n. The setup.py is oudated. Pls install shogun using anaconda.\nIf you cannot make sense of the build messages, it might make more sense to install a binary version of shogun (see again anaconda). I think we should put the content into the existing notebook\n. ill close for now, feel free to reopen if you have updates. A patch for the readme would be appreciated here as well :)\n. #3535 \n. Hi!\nWe currently have no readme for this. Any contribution is very welcome.\nHowever, Shogun does all this in a very standard way\n- Add your class\n- Run make to compile Shogun (it is automatically added, you might have to start a fresh build)\n- enable -DENABLE_TESTING=On in cmake\n- make test runs unit tests, you can add a unit test based on the existing ones\n- you can run a single selected unit test as well via executing build/tests/unit/shogun-unit-test --gtest_filer=YourTestName*\nAdding something like this to the unit testing page on the wiki would be helpful. Send a patch if you like\n. I just commented on a PR in the shogun-wiki repo. You can join efforts in there.\n. Thanks for that!\nCan you please run the QDA unit tests with valgrind and make sure there is no leak or other error introduced?\nSomething like valgrind ./shogun-unit-tests --gtest_filter=*QDA* in build/tests/unit\nGoogle for valgrind usage\n. Once that's all fine, and all cosmetic issues have been addressed, please merge all commits into a single one. Then good to merge\n. ASSERT compares the element to 0, which is not what you want here, you want to check that the underlying memory is not NULL. So (as I wrote!!!) it should be ASSERT(vec.vector), since vector member is the pointer to the data.\n. I agree, the valgrind trick should be in the testing readme. I will update the readmes soon (hopefully)\n. Now for the leaks you have, were they there before the changes you made? Please check\n. Changes are good otherwise. Thanks!\n. Changes are good otherwise. Thanks!\n. While you are cleaning this QDA mess up, another thing that should go is any method called from SGVector, e.g.\nSGVector<float64_t>::vector_multiply(col, col, col, m_dim);\nSGVector<float64_t>::scale_vector(1.0/(class_nums[k]-1), col, m_dim);\nThese should be done with eigen3, then you can also avoid storing the vector pointer above. Feel free to add (but first check the memory thing above, as it is harder to trace with more changes)\n. Yep I agree with you. Squash and we can merge. Thanks for the memory check!\n. Cool! I will merge once travis is green. Ping me when that happens\nBTW can you check which of the 5 tests is leaking memory, and why, and then file an issue for that? And then fix it? That's a very useful thing to do and lots to learn as well :)\n. We cannot merge with this commit message\n. Indeed\n. Nice, exactly this. Just a minor change and we are good (please suqash commits, or use git commit --amend on your branch)\n. As I said above, please squash commits\n. Guys can we please stop spamming this thread with this overly verbose style discussion? I very explicitly and clearly said how it has to be in my first response and then repeated it twice.  Shogun code is public to get inspiration and google is full of tutorials how to send clean patches that do nix mix up whitespace or style changes with newly added code.\n. We can merge this once it is squashed.\nBTW for the future, use git commit --amend  -- it avoids having all these commits and the need for squashing for smaller patches -- google its usage\n. updates here?. @OXPHOS This makes the cart tree example (aka using boolean vectors as parameters to Shogun methods) work. I am waiting for travis and might have to add some fixes, but generally this should work now\n3282\n. For the future, can you please update the pull request rather than opening a new one? You have to add -f to your git push command for that.\n. Thanks!\n. related to #3452\n. For now, the workaround is to compile Shogun with the cmake siwtch -DENABLE_TESTING\n. That is an idea.\nThough I don't Like the fact that jblas is turning everything into a double..... there must be an ndarray implementation for java no?. Yeah sure, as said, there is hundreds. This is still really important. Great! :)\nKeep in mind also that make meta_examples parses and translates all meta examples found by cmake (if you added a new one you have to rerun cmake), and that build_cpp_meta_examples builds the cpp versions of them, and finally, ctest -R generated_cpp-binary-linear_support_vector_machine runs your example. If you want more details on how it is executed, append a -V. There are still more examples to port as of early 2019: see examples/undocumented/python. There is a readme. We will need to port all examples in examples/undocumented/python/*.py to the new meta example system, using factories and base types rather than specialised types (i.e. using Machine m = machine(\"LibSVM\") rather than LibSVM m = LibSVM(), and using m.apply().get(\"labels\") rather than m.apply_binary().get_labels(). There is some hack script written by @tklein23 but it is old and unlikely to work.\nhttps://github.com/tklein23/shogun-partial-build\nFor now ccache effectively avoiding to re-compile things that haven't changed ... not optimal. Yes, let's not re-invent the wheel there ;). Can you send viktors patch in a separate first PR?. Nevermind about the seperate patch, this just refactors things, so merging. This needs a rebase with the orginal feature branch. Does that make sense?\nOr just drop things then?. Adding back ... thanks for letting us know. https://github.com/shogun-toolbox/shogun/wiki/Minimal-library-usage. @vigsterkr The cookbook preview does not show the right commit version for some reason. I noticed this before: the added cookbook is not part of the html renderer. @IOcodegeass can you squash the commits please. About the failing octave build, you read the error message. It might have told you that the build timed out, I restarted it :). About the d overloading, just remove the d for the distance. It is not used anywhere anyways. Just give the formula on the right hand side of the = sign. One more amend and ready to be merged. Cool I merged the data, so you can update the data version as well, see wiki for testing data howto. It is exactly that. Now we wait for travis to execute the integration test for your example. Ping me if it passed.. Nice one. Well done on first merged cookbook page :). Ah can you pls check to remove a potential cosine distance example from undocumented/python_modular folder. Yes! Thanks. Can you please not re-create pull requests? You can update the old one instead. Please update the old PR and dont send new ones. It is not high priority to change the others...but if you want to, go for it, cleanups are always good. Thanks!!. No idea what you mean....?. Check the history the error is not caused by you: http://buildbot.shogun-toolbox.org/builders/precise%20-%20libshogun. You got it! :) Very useful patch!\nCan you send a patch with the integration testing data to the data repository, and then once merged, update the commit here with the data submodule version? So that travis executes the integration tests for your new examples. Check the (compile) error: https://travis-ci.org/shogun-toolbox/shogun/jobs/180663257#L2350\nThis tells me that you didn't test the examples you wrote locally before you sent the PR. Please don't do this, it is a waste of CPU cycles in travis. Make sure to read our developing readme in the wiki.\nIt would be great if you could update the PR as we are very interested in having such examples in. It only takes very few seconds on mine (once shogun is compiled). Make sure to check the readme and find out how to run tests in an isolated way.. Good!. Good to merge otherwise!. Thanks!!. This will have to wait until after the xmas break ;). Thanks for reporting. Looks like some octave include file changes cause this.. If you update the data version that I just merged then the integration test will be executed. Commit message: \"Add Manhattan distance example and cookbook\". Great! If you could now please squash the commits into a single one, we are ready to merge it. No worries about the new pr. keep in mind though that you can always clone your fork. Never delete your fork!!. Sorry for the delay, I was on holiday.\nYes ping me in irc anytime if you need help. Now we are game :) Thanks!. I don't agree on the transpose. This flag should be to actually be able to avoid transposing at all, but rather perform the corresponding operation without changing the memory layout of the matrix.\nE.g.\n * https://www.tensorflow.org/api_docs/python/math_ops/matrix_math_functions#matmul\n * Numpy's transpose actually doesnt transpose but just creates a view\n * Eigen3's adjoint() creates a view as well and doesnt change the memory. Cool, so finally this can be merged :). Want to contribute one?\nWe might work on this early next year. Google is your friend here\n@vigsterkr can also point to our current ppa configuation files. BTW: check out git commit -amend. What is the state here now, I cannot access the old discussion anymore. .... I see, then we need different names for each. There is really no way of overloading mat-vec and mat-mat? Calling it mat_mul ?\nMaybe look out in other libs for some inspiration on consistent naming ...\n. Its good to go! Nice!. Good. Better no code than shit code !. Check https://github.com/shogun-toolbox/shogun-web2 . In paticular https://github.com/shogun-toolbox/shogun-web2/blob/master/templates/showroom.html and the main flask python script. This is because the data PR has not been merged yet. Doing that now. In fact, travis is fine. If you can update the data, then I can merge this. The data version is still not updated to your merged data PR. Nice!. Thanks for that!. Looks good, thanks for that. Ill merge as travis is fine\nDid you try it from say python?. Yes, these examples are partly outdated, there is a blacklist in the CMakeList.txt that shows the examples that are not part of the test.\nBTW feel free to port any of these examples to the new meta example systen, c.f. #3555 . I dont think the buildbot error is your fault, but thanks for pointing it out!. Cool thanks!\nLet's not port all the distance examples please :)\nThere are many other methods that would be better exposed.\nAlso, your data version is not yet merged (I need to do that first). Just merged the data commit, restarting travis. Ping me if green. Merge conflicts, needs a rebase. Otherwise good. Thanks @abhinavrai44 \nCan you please send integration test data and another PR here to update the data version. Testing for the distances are way more important than having more cookbooks....\n@vigsterkr I usually make sure the data is included before merging. BTW do you think we should merge more distance cookbooks? I would rather only have the code/test to avoid the cookbook being spammed with 100 distance examples?. What do you mean by \"but now its fixed\"?. ah cool :). You need to expose the modshogun.py (either the one that was installed to your system or the one in the build folder) to your PYTHONPATH environmental variable. Then Python will be able to see them and you can import modshogun. Thx. Thanks for reporting, should definitely be fixed. Yes, so let's just execute the compiled java examples using scala then.. Lets get this moving! :). How are things going?. Cool this looks good apart from some minor issues.... I checked travis and it executes the scala stuff\n@vigsterkr I guess we don't need integration testing of the meta examples for scala (since that is already done for java, and I trust them that the code does the same). Actually there is one more thing: the integration tests for java need to be executed using the *.dat files that java generated. As you are executing the java compiled examples in their directory, these are overwritten by the scala ones....\nSolution 1: make the java examples depend on the scala ones so that scala is executed before java.\nSolution 2: Replicate the example folder structure for scala and symlink the example binaries in there, then execute in the new folder. This also gives the option to do seperate integration testing, although I think we dont need that\n@vigsterkr thoughts?\nEDIT: Sorry this is nonsense, the examples are executed in meta/scala  .... my bad. BTW please also add a NEWS entry that says that we now have scala added to our build. The summary of my messy review: This is good to be merged once you removed the cookbook extension and changed the tab title, and the NEWS entry. Check for example http://shogun.ml/examples/latest/examples/binary_classifier/kernel_svm.html\nIt has these language tabs at the top. Cool then :) Nice work!. This definitely is in the right direction!\nTry sending smaller PRs, doing one thing at a time. Here you moved some code to a different file and also changed it. Avoid that, first PR is moving things to different files (without changing anything else). Second PR is then changing the code (SGVector replacement for example).\nSo this one can be split into multiple smaller patches.\nAs for your error, try to compile frequently when editing. When you add new files, it might be a good idea to delete the build dir and re-create the build due to obsolete cache errors.... No idea :(. Updates here?. Technically, we want everything to be serialized, needs to be sorted!\nBut for now we can force data to be on the CPU before serialization I guess. Have to do that before merging the branch. Good to merge after name change. Thanks!\nAnd never use constants in loop counters ;). The dsyev_ errors are LAPACK.\nThe other one is google protobuf, you can deactivate both in cmake I think.\nBut it would be good if we investigated what is going on here. You can use ccmake to see all options.\nLAPACK should be there (but I think it works without, many things will be unavailable). Protobuf is not so important. @vigsterkr I leave merging to you. Cool so this can be merged.\nDid you test it locally? I.e. did you run the scala executable with the detected path?. Good to go apart from minor things ...\n@vigsterkr ?. Not sure I understand the purpose of this PR ... just moving the function implementations to a separate file? Sorry maybe I was not clear earlier, but I was talking about doing the full refactoring in a separate patch, including adding a new base class for the solver and then making CKNN a specialization\nShall we maybe draw a little class diagram to make clear which parts of the code will be solver, which of them will be in the base class etc.\nAlso, please check the existing PR on KNN refactorinbg.. Just to check here.\nif you type make build_java_examples, and then again right after finishing, they are not compiled again?\nFurthermore, if you then modify only a single meta example source code listing in examples/meta/src, and type make build_java_examples, then only this example is re-compiled?\nIf so, we can merge. Submitted some comments, but this is good to go.\nMaybe consider a whitespace cleanup patch (always do there separate and don't merge with functional changes). I cannot merge due to merge conflicts, needs rebase and then is good to go. This is not the solution that I mentioned in #3553 \nCan you explain what you are doing here? Not 100% sure I get what you do ... build still passes though so it seems ok\nAlso, the warning thing with the flag, could you put that into another patch to separate concerns here?. Thanks for the good explanation :) I agree with the variable stuff. I should have realised this but I think I looked at this on my phone...\nVery nice work! Merging!. Hi Bert,\ncan you post some code that lets us reproduce your error?\nThanks!. Thanks! Will check soon!. Same question as for java here.\n\ntyping build_csharp_meta_examples twice only compiles them the first time\nmodifying the source code of a meta example followed by the above make only re-compiles the modifed one?\n\nIf so, good to go!\nTravis error is unrelated. Lets investigate what is going on here. See my comments in #3617 . Weird about the set_epochs ... did the python example work for you locally? Does the method exist?\nIf it exists and python works, then it is weird.\nAnyways, let's merge for now, travis did execute your example, see here\nWe now also want it to execute the integration test, but for that, I first need to merge your data PR, and then you need to modify this PR to use the latest data version.. Ping me once done. git status will always tell you whether you changed the data files. As you have commited (and I merged) the new file in the data repo, just make sure the submodule hash matches the latest commit hash of the data repo. . This is the hash: https://github.com/shogun-toolbox/shogun-data/pull/132/commits/c554e47b3838046b4653c666c96d2770ab1123a3. Check the developer readme for that! And potentially references therein\nBest\nH\nOn Fri, 10 Feb 2017 at 14:35, Sudarshan Konge notifications@github.com\nwrote:\n\nI think I screwed up somewhere. How do I\nmake sure the submodule hash matches the latest commit hash of the data repo\nI also can't\ncommit the updated version hash of the submodule (in the main shogun directory)\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/shogun-toolbox/shogun/pull/3617#issuecomment-278956194,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAqqv4KRX-_Ps7LXLCNepe5B1iOsSyfuks5rbHWugaJpZM4L2i2U\n.\n-- \nSent from my phone\n. Git amend is your friend, or git reset.. Also, all travis builds fail. Closing for now, feel free to re-open if making progress. Merge conflicts, otherwise good to go!. Travis error unrelated, merging. BTW please read our develop readme on amending commits, and use google to see how to update existing PRs (i.e. remove your debian folder thingi). Can you update the gist as well so I can see the rendered version?. You can update gists btw, so can always just update the old link rather than putting up a new one. Let's get this one going, should be merged soon and the changes are minimal ... :). Still some open issues, see above. There is still some cells that talk about \"with and without covertree\" which is not relevant anymore.\n. Fine from my side.\n@vigsterkr can you merge. Fine from my side\n@vigsterkr you go ahead. Closing as this seems to be unrelated to Shogun. See #3313 for an example for such a patch. Yeah do it !. Needs rebase. Nice precise patch! :)\nFine to merge once merge conflicts are resolved. Csharp?. Then I guess we are done! :) thanks!.  * Can you please put this in a gist and delete the super long message?\n * Wat happens if you disable nlopt in cmake?\n * Can you link other applications against your system's nlopt?. the docs tell you cmake -DENABLE_NLOPT=OFF\nccmake is a good tool for such things\n\nDid you actually try 3)?. import modshogun or \nfrom modshogun import *\nSee the examples and the readmes. Ok lets do complexity later. Use git amend to correct commits, now you need to squash them (we want only one). Can you please stop opening issues around you not being able to compile Shogun. Rather ask on the mailing list or on IRC. This is likely to be an issue with your local lapack installation. Check the Lapack setup and whether cmake detects things properly.\nAlso consider using a precompiled package .... Answer to your third point:\na=to_gpu(a) will only destroy the CPU vector if there is not other reference to it. If there is another thread working on it, it will have a reference which increased the reference counter and therefore the old SGVector will not be destroyed. In fact,  the memory will be intact and the thread will continue to work on the CPU version. I think .... @vigsterkr ?\nNot sure I understand the third point, maybe we should catch up on this in IRC?. Seems good to merge from my side. But let's leave this to @vigsterkr . So do it! :). This is not used anywhere right? :) So good to go. There are detailed instructions in the installation readme.\nThe error message says to install ctags (or disable meta examples)\nClosing as the issues here are not related to Shogun or cmake itself.\nIf you have trouble using cmake and related, please consider using the precompiled ppa. I added a more informative error message in 1a3b9d71c2daf48818e4ce1466a83aca5735b07a. The way to go here is to make the PRs minimal. Changing existing things goes into a PR, adding a new test goes into a PR, etc etc. Small steps. So just remove somethings and then we  proceed from there. It is great if you sport memory leaks as we need to fix them. Travis error unrelated, merging. Ok will close this one for now. Travis error unrelated, merging. setup is mac and gcc 6.3 iirc?. We're on it in https://github.com/shogun-toolbox/shogun/tree/feature/Cpp11. Can we make the c++11 guarding and fixing the errors make two PRs?\nIf this one is fine, why don't you squash the commits and we merge?. The variable you asked about simply is assigned as problem.l=m_labels->get_num_labels()\nSo what is the output of\nkernel.get_num_vec_lhs()\nfeats_trn.get_num_vectors()\nAlso, I cannot run the code as it is as I am missing the data files, so I have to guess here.... fixed in #3659 . Thanks!. Step one: Prototype a Shogun kernel that uses tensorflow to do computations. There is 1000 things to figure out and this covers quite a few of them.\nStep two: Compute the kernel gradients with tensorflow\nStep three: think about how we can apply this shogun - wide.\nQuestions:\n * Is linalg the right place in Shogun to integrate this into?\n * Do we want yet another API for graph computations?\n * XLS is very early alpha, shall we wait a bit longer?\n * Autodiff is still very  useful, which doesn't need XLA, but just an autodiff API\n * Serving is a totally different story, and even is a nice entrance task for GSoC. Should be done using the benchmarking framework by mlpack: https://github.com/zoq/benchmarks. MERGE it! :). Fine to merge from my side. Wanna do it yourself? :). Thanks for that! Ill merge the data PR, then you can update this PR here with the new data revision. Ping me once done. Ace, thanks!. Care to write a cookbook page?. ok sure. There are new errors in the scala build now. Repdoduce locally with -DENABLE_TESTING=ON\nSee https://travis-ci.org/shogun-toolbox/shogun/jobs/205716515#L4975. Thanks!. so travis didnt complain more than the current broken things\n@vigsterkr iirc the bugs in the parallel xvalidation with clone were caused by parallel issues that were fixed?. There is one\n@lkuchenb wanna have a go at that?. Next step -- don't clone the fucking data. This is madness in 2017 !!!. Yes, an overwhealming number of good reasons for that :)\nCome to IRC and we can discuss. Fine to merge from my side. @vigsterkr ?\n. @geektoni IRC hours depend. I am UTC 11am-7pm usually. Others might differ.. ~~Weird failures in travis, I just restarted to be safe.~~ Realised one is from multithreaded octave and the other one was a timeout anyways..... Yeah the big things should go in feature branches.\nYeah I see the changes are all related, so merge it.\nIt is just that it would be easier to review this in an incremental fashion. Travis error unrelated. Merging. Ok from my side.\n@vigsterkr you got thoughts on this?. Thanks!. @lambday will we have hsic back at some point?. Some failed:\nhttps://travis-ci.org/shogun-toolbox/shogun/jobs/207534368#L6795. @vigsterkr feel free to merge. I just added some clarifications, https://github.com/shogun-toolbox/docs/commit/619946795202190b35385888754325dca48e7a79. General point for discussion:\n@vigsterkr @lisitsyn \nSo the CMachine instances currently do serialize the features, and de-serializing loads all of the features back into memory. I am not sure we really want this? For the linear machines in this PR, this is not even necessary: the regression/decision function is simply f(w.T * x + b) and w is stored explicitly\nFor kernel machines, we would have f(x)=\\sum_i \\alpha_i * k(x, x_i), so we do actually need the data (though not all of it e.g. for SVMs). For now, I guess we would call store_model_features, and then only serialize those data which are needed in predictions. In general, it would be good to be able to have control over serializing data or not. What are your thoughts?\n@micmn I think you can validate my above points on linear machines by removing the feature reference in the trained model before serializing.\n. @vigsterkr I see. I mean it will be much less than the serialization tests (way less classes). But good point. Can't we just turn the auto-generated stuff off via cmake switch (for local developing?)\nQuestion also is whether we want to run this on CI or just buildbot. @vigsterkr ok agreed\n@micmn you have an idea of what we mean? . @micmn Did you set features and labels to zero after training?. So there are a few things about this failure:\n\nFor linear machines, resetting features and labels after training has to work. If it doesn't, this is a bug (and one of the reasons why we want to test this is finding those bugs!)\nThe error your described: [ERROR] In file /home/shogun/src/shogun/machine/LinearMachine.cpp line 81: Specified features are not of type CDotFeatures seems weird because the test features you provide in the test are CDenseFeatures which are indeed a subclass of CDotFeatures. This should not fail on dense features and needs investigation (separate PR!)\nIf you check the code in CLinearMachine::apply_get_outputs, you will see that the features field is overriden with the object passed. So everything should work once the above bug is resolved. Cool so we dont actually need to serialise the data, which is great!. Hey hey, any progress here?. yeah a sep PR would be good. Just suggest something sensible and then we can discuss around that. BTW we dont need to test this for other serialization types. This is about something else, serialization itself is tested elsewhere. Ok this is great!\nCan we have a seperate PR for the class_list changes and the json stuff?\nWay easier to review .... Sure no need to ask. BTW please use a better commit message:\n\"Clean up error messages, fixing #3680\" or so. Closing this for now, feel free to re-open. In the future, pls put whitespace changes to a separate commit. We ask external contributors to do this, so we do it as well. Your vim should not change existing code in my eyes when you save a file.. It is quite easy to put them in a separate commit. When you look at the diff and you realise there are whitespace changes, just undo your actual changes, save, commit, re-do the changes, commit again.\nJust 1 min of work for you and makes things so much cleaner. . Travis canceled?. I think the best idea here would be to explore overload the c++ cast operator so that CBinaryLabels can just be casted to CMulticlassLabels. And then rather than checking this label type enum, you do a dynamic cast of the CLabels pointer\n\nThe solution you suggest is really cumbersome to apply. Try to see whether you can make sense of what I wrote. Entrance task for detox project for sure :). First few down, but we need more (eventually all classes). This is still an open issue: to make algorithms observable, algorithms should only modify their members using put. @lisitsyn I think we do not need this anymore, right?. All this should be done in CMachine.h\nAlso, virtual bool train_locked(SGVector<index_t> indices) should also be guarded. I actually don't know. Would be good to check this out..... Thanks for the patch!\nFor the record, I checked the IPython notebooks, and none of them use the methods, so merging\n. Yep, thanks for pointing out . Do we actually have a unit test for this thing? Just to ensure nothing crashes?. I think a test that at least executes the stuff you added would be good. Correctness is tricky as it just prints out. Not sure it is worth the hassle to check this. But making sure it doesnt crash on corner cases would be useful definitely :). This one is fine to merge from my side. I think this branch needs a rebase (against what?)\ncheck out the commit about googletest that appeared. Shall we re-vive this?. No need to ask :). updates?. I see that the other way around. Performance wise it doesnt matter as the compiler optimizes this out anyways. Ill leave it to @lisitsyn But I think we should be consistent. Let's rebase and merge this. For the future, I would like to see separate PRs to reformat with the style checker, and those with the changes. It is impossible to see what has actually changed in the diff. Using github web interface editor to resolve merge conflicts :sunglasses: . Only style checker failed. @lisitsyn was fine with this as well. Merging. Lol yes I realised this\nWonder why they do this web interface thingi\nOn Sat, 23 Dec 2017 at 20:52, Viktor Gal notifications@github.com wrote:\n\nCan u plz not do this again... we are having now a merge develop commit in\ndevelop... super ugly\n\nOn 23 Dec 2017, at 16:29, Heiko Strathmann notifications@github.com\nwrote:\nUsing github web interface editor to resolve merge conflicts \ud83d\ude0e\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub, or mute the thread.\n\n\n\u2014\nYou are receiving this because you modified the open/close state.\nReply to this email directly, view it on GitHub\nhttps://github.com/shogun-toolbox/shogun/pull/3698#issuecomment-353747488,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAqqv57C5T-r4x8PIMOPWxaJHd1GeR5tks5tDWgCgaJpZM4McdYq\n.\n-- \nSent from my phone\n. Commit message: \"Clean up markdown header\"\n\n\"Minor fix for notebook\" is not a good commit message. These should always tell you what has been done and maybe why. Super minor comment, but keep in mind for next time. Thanks for reporting.\nNice entrance task for GSoC students. The entrance tasks are not really project related, especially not those easy ones. What about you run a linkchecker and then we close? :). I meant: you could run an automated tool that verifies the links in all the notebooks in the repository. If broken ones are found, you send another patch, otherwise we close this issue.. Thanks for that! Really useful to have that list!. Done by @lisitsyn in 6.0 release. Should be fixed in 91cce476cc39357fbd4fa7695d6a518933795a44. Ill merge once CI is green. I'm heikoS\nYou might need to try a few times, I'm more online under the week than on\nweekend\nOn Fri, 17 Mar 2017 at 17:26, Giovanni De Toni notifications@github.com\nwrote:\n\n@geektoni commented on this pull request.\nIn src/shogun/io/SGIO.cpp\nhttps://github.com/shogun-toolbox/shogun/pull/3705#discussion_r106703449\n:\n\n@@ -189,6 +196,7 @@ void SGIO::progress(\n          if (current_val >= max_val-1)\n          {\n              v = 100;\n+             last_progress=v-1e-6;\n\nYeah, I am still curious about them ;). Regarding IRC, I could be online\ntomorrow morning, will I find you @karlnapf https://github.com/karlnapf?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/shogun-toolbox/shogun/pull/3705#discussion_r106703449,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAqqv4CO-OIPkQ-OZmRy2fjjSJpx3KTlks5rmsJEgaJpZM4MeKpA\n.\n-- \nSent from my phone\n. Nice patch, thanks!. @vigsterkr @lisitsyn pls merge. This is a random error that is caused by an ill posed optimization problem so NLOPT reaches floating point epsilon. This should not cause an error but simply a warning... as it does.\nAs EP has random elements and is not guaranteed to converge, this error might happen occasionally. Think we should monitor  but not sure how to fix this (seed might help). Fine to merge from my side (if you tested the build with GPL turned off). Fixed in 8f9021fc9da57160d4d658ddeb9789740595a6f5. Pls dont open and close PRs, creates a lot of junk email. Thanks!. Yeah pls no re-opening of PRs. See https://github.com/shogun-toolbox/docs/blob/master/DEVELOPING.md point 10 of the git dev cycle. thx!. NOTE: You can edit your comments :). Not sure if that was recently fixed, my build is a week old dev branch. More care is needed!. /nfs/nhome/live/ucabhst/git/shogun/shogun_develop/shogun/src/shogun/statistical_testing/internals/DataManager.cpp:388. ok so I need to rebase. :). Thanks for that.\n@lambday you played around with subsets recently no? Bother having a look here?. Are they used in the GPL module? I guess not?\n\nAlso, I think one thing is missing is a low-rank update of the cholesky. Wanna add that and apply it to LARS @micmn ? (It is already there, just needs to be integrated in linalg). Sorry, I meant GP (Gaussian Process). As in: is the GP stuff using linalg already?. I think pair is fine. We can use tuple for more than two return parameters.\nAs linalg wont be expose to SWIG, that is fine (right?)\nBut please offer convenience methods so that the solver also can take a pair as input. if you used tuple, then the number of return arguments could depend on the enum value passed.\nAnd then we can also offer a wrapper. Thanks a lot for this!. Ouch! \nDefinitely\nThis is a good one for the detox2 project, or the usual suspects2 GSOC. Yes, and next time we need to be more careful with merging things like this\n@lkuchenb you want to send a revert patch?. We actually dont need more cookbook pages for distance. Just the meta example is fine\nActually, leave it in. This is a good to merge patch once travis is fine. One question. Do we have some tests of MKL results: unit/integration? Because if not it might be dangerous to merge this..... Happy to merge once registered parameter get default values. I would love to get this polished more:\n\nactually remove those doxygen comments everywhere, second commit did not cover all\nsquash commits\n\nBut fine to merge anyways as this actually helps. @lkuchenb do you have inspiration to fix the above two up? Then we can press the magic buttin. #3742 . Should be done in a similar way to https://github.com/shogun-toolbox/shogun/blob/develop/tests/unit/base/trained_model_serialization_unittest.cc.jinja2. Postponed for now. I vote for unitls. Let's merge then. A unit test for single a multi threads of a kernel machine would help. Yeah travis only checks single threaded.\nCan you at least memcheck this locally using multiple threads?\nIf that is ok, we can merge. Yes.\nJust run it locally with single thread and multiple ones and make sure your patch didn't change the output/result. Almost, run this with a memory checker that checks thread safety\nhttp://valgrind.org/docs/manual/hg-manual.html\nOh and please check whether the unit test uses the code you changed (i.e. runs in openmp parallel). What I meant is:\nCheck the unit test code. Does it run things single AND multithreaded.\nAlso, tell valgrind to use the helgrind tool, see above readme. It doesnt really matter how you do it, the point is that you need to be aware what you are checking there. And that needs things to be explicitly executed single and multithreaded. Any updates here?. I have faith in GSoC applicants :D. We are fixing the link atm. Check back in a few days. No it is the IRC chat logs, they are currently offline, but we might put them online again.\nIn order to build the system, you can use any other chat log on the web. Then, when you are done, and we have put our chat logs back online, you can just apply it to ours ..... Start with bag of words representations, see e.g. here: https://github.com/karlnapf/machine_learning_course/blob/master/classification.ipynb. Or train a NN ;). Everyone is free to try their ting.. Chat logs are labelled by construction. Could do that. But the task is to classify who of the core devs. And any ideas how to integrate this with tmdq :D. In other words: How does it look like when shogun is called from python. Can we maybe use it with the tmdq if available, so in ipython notebook we get similar kind of optics....not very important. Sure differente scope. But I got excited about it anyways.... to cite @oxphos : this is georgeous. Lets tidy it until @geektoni is pleased ;). Not sure how you installed, but\ngit submodule init\ngit submodule update\ndoes it when you are using git.. @vigsterkr pls merge. thx!. Can you comment on this? Why not rather fix it?. Quite excited about having all the subclasses of linear models checked soon! This will make Shogun a much better place\nSame will be ace for kernel machines (and reveal problems I am sure). thx!. Pls read our develop.md readme on how to send PRs. wikis are now editable for everyone. Give it a try :). Yep, exactly in that file.\nWith this description we won't be able to help you.. Hi!\nThis looks quite good. However, you realise that if you want all matrices (U, S, V) of an SVD, it computes the SVD three times?\nInstead, the method should return a triple with all matrices .... This needs a rebase. Please read the developing.md readme in the wiki. Without the exact steps you did and system specs, we cannot help you. Can we push this in?. This is a known bug. It will be fixed in the next release.. One final request: Could you squash the commits? (or using git amend and force push to your fork). Before you start coding: there is some serious design thinking required on how to cast the type free new Matrix (think LinalgMatrix), so the typed version. We don't know the best solution yet, but have an idea which we are happy to share. Ping me or @vigsterkr . I updated infra. Hi!\nThanks for taking this up, really appreciated!\n * modshogun -> shogun definitely! It should actually be import shogun as sg and then use sg.Bla() rather than importing everything into the namespace.\n * we don't want to use pylab but rather implicit imports such as import matplotlib.pyplot as plt\n * if you cannot replicate things with the standard scipy stack, make the parts that depend on non-standard packages optional, i.e. the notebook should execute smoothly, with no errors raised\n@vigsterkr comments?. Example: #3790 . All guarded files are moved, so this can be closed. As far as I can tell, there is no more GPL code in shogun/src, but others should check as well. Did another set of work to push this in #3946 . @sorig which ones?. Files that need guard or need to be moved to GPL repo and their use needs guarding\nSVMSGD.h SVMSGD.cpp\nlib/JLCoverTreePoint.h\nmulticlass/LaRank.cpp multiclass/LaRank.h\nFiles that need a new header (i.e. we own the copyright and can change it, or they never were GPL in the first place:\nDistance.cpp\nKernel.cpp\nKernel.h\nregression/svr/LibSVR.cpp\nAll the unit test *.cc files. Done a while ago. @lisitsyn now is your time, all GPL code has been moved to submodule and the build works with and without GPL enabled, http://buildbot.shogun-toolbox.org:8080/#/builders/4\nI suggest a git grep GPL and off you go :). #3946 \n5e6c6db08bac2a31e7f35d1c81c0b8f749c6199d. Done. Remove!\nAs well as all other switches of nightly none..... Done in 3560969. I think this is done. #3786 . cah. Thanks for noting that!\nNope, that code (the example itself) is not GPL (I mean it is but we can change that). This example should be guarded in the build instead.. I think auto will do the magic for you. Or even range\n@lisitsyn . I think all this can be done via the buildbot API in fact.\nCan copy to the webserver\nBest to discuss together with @vigsterkr on slack I think. I'm excited about this! :). restarted build. +1. open an issue and reference this pr! useful for later. picking up on this.\n@gbohner installed the mac brew package and then there are the old undocumented/libshogun example binaries in share/examples/libshogun\nI think what all packages of shogun should provide is compiled binaries of all c++ examples (both the old libshogun ones and the meta examples), and source code of the c++ meta examples.\nThen, any language binding package should have the source code (and potentially compiled) language examples as well ....\n. It is for the brew one, could be removed then I guess. Very sorry but we cannot really support this old version anymore, lack of manpower. I suggest to use the octave interface for the latest build. We will try to get the new Matlab bindings working at some point this year. Matlab static interfaces were removed in 5.0.\nWe are planning to get it back via SWIG. There is work on this, but somebody needs to take it up and finish. I cite the error message:\nPython module ply required for meta examples. Install or set\nBUILD_META_EXAMPLES=OFF. Nice to clean this up. I am always in favour of\n\nOOP to avoid duplicate code, i.e. one class per solver. The alternative is to do it like in the CPCA class where an enum is passed. This depends a  bit on the situation (what do you suggest here?)\nI think templating the algorithm class is not a good idea. Algorithms accept CFeatures which knows the type. CLeastAngleRegression (LARS) does some hacks inside to infer the type, but that solution is not optimal either. In order to solve this properly, a lot of work is needed, so for now I would replicate the thing in LARS. Thoughts @vigsterkr ?\nWhen you touch code like this, make sure to use linalg where possible. This will make it much easier in the future to maintain this code.\nMethods should not contain more than 10-30 lines of code (my opinion). whats the state here? neeeds more review?. Question:\nIs serialization of this covered somewhere? I think we should have an automagic test, but that only checks very basic cases, I think there should be at least one non-trivial test case. Not sure whether in here or somewhere else though....thoughts?. ?. I will create an entrance task thingi.\n. Check the examples and notebook on multiple kernel learning,\nhttp://www.shogun-toolbox.org/examples/latest/examples/regression/multiple_kernel_learning.html\nhttp://www.shogun-toolbox.org/notebook/latest/MKL.html. Quite excited about this. What does it need? Can I help?. I see. I think you will need to write a constructor in SGVector that will share the underlying SGReferencedData object, so that destroying the SGMatrix leaves the reference counter still at 1 (in the returned SGVector. Great!\n@OXPHOS please read carefully! :). ~~I think this will have to be split into multiple PRs later.\nBut we can start discussing here~~\nNevermind this. @vigsterkr ?. @geektoni can you write down a simple use case from an algorithm implementation perspective? Thx. The solution is to add a line like\nSG_ADD(m_current_values, ...\nto CLabels::init()\n\nWanna go ahead with this @olinguyen ?. Nice!. Yes in general an average is used. We will have to check the details though.. Done in #3954 . @durovo That would be a great addition to Shogun, such a welcome contribution!\nIt is a bit more tricky, make sure to share early drafts on this soon. Don't get into writing code before we have discussed a design draft. I suggest to read our corss-validation code before you start.. Yep this thread is totally fine. I would also be interested in hearing about the sklearn approach to see whether we can learn from it. I guess it would be best to re-use the x-validation code.. Any news on this?. No worries about the delay, your work and thoughts are still very much appreciated!\nI think your thinking makes sense, maybe we move on towards designing some header/API files?\nDon't worry about the parallel thing for now, we can address that once the design is there.. The first PR is just for discussion and we can first discuss the API before you write tests.\nHaving said that, minimal tests are essential, try to cover things like: preserving order, no leak of future data. Best thing is to do a manual split by pen&paper and then assert things in the unit test. Next thing is to think about corner cases .... Needs some clean up and then good to be merged. Once this is done (very nice initiative), let's re-design the Gaussians and mixture models a bit. I can help with that, let me know once you are ready with this stuff. Hi @MikeLing \nLet me know when you are done with the clean-up and I can help you re-designing the class.. Mmmh the unit test thing is quite concerning ....\nI think for simple re-factoring, we can merge without having tests, but generally, we should add some\nnotebook checks:\n * visually works\nunit tests:\n * compare with sklearn on a simple toy example with the same initialization as sklearn and only single iterations\ndomain/math tests\n * free energy increases in every iteration of EM (this is a really good sanity check)\n * all probability vectors always sum to 1 (logsumexp is 0)\n * when initialized with a good solution, it doesnt jump around but convergence quickly to the same solution as initialized. Ah ok.\nSo this test then must not change integration test results ..... travis seems ok. \ni think we can risk it then if we do a visual check ..... @OXPHOS Can you write me an email once this kinda works?. @lambday want to fix? :) We need your help .... I think we can just decrease here @lambday. @lamday ideas?. @lambday. Algorithms to add:\n * LeastAngleRegression\n * Kernel hypothesis testing (@lambday can guide what can be early stopped)\n * Most things in structure\n * Fernandos LMNN \n * many more, make sure to deploy this rather than just replacing SG_PROGRESS, it is very cool!\n(those can also get a progress bar btw).  * ModelSelection\n * Cross-validation. Run cmake with -DUSE_SVMLIGHT=on. Should be possible. Though we don't have an example. See the Ica notebook in the showroom, and have a look at the ruby examples on the website.\nI'll try to add an example for ica one of these days. I have sent a patch with meta examples for all ICA algorithms Shogun has. \nAfter this has been merged, they will appear on the website \"examples\", this includes ruby\nLoading the soundfile has to be done from ruby for now (Shogun cannot read soundfiles), best is to load the sound, store in vector form (csv or so), and then load using Shogun. No, there was a problem. But I will fix that now.... hopefully ;). Sent an update that should be merge-able within the next hour. http://shogun.ml/examples/nightly/examples/converter/ica_fast.html\nYou will have to load and convert to your desired sampling rate using something else. But once you have the time series representation of the sound, using shogun is easy. Let us know how it goes. Not sure I can help you there.\nDo you know of a nice standalone c++ library to do that? We could add a wrapper to Shogun ....\nthoughts @vigsterkr @lisitsyn @iglesias ?. For the make error, you either need to have eigen3 installed, or have an internet connection so that it can be downloaded. @arjunmenon make sure to read the installation and the examples readme. I cannot really help with the wav file loading, thats a job for searchengine. Same for javac and c-sharp I guess. I am still getting this\n[ 90%] Linking CXX executable multiclass_classifier/multiclass_linearmachine\n/home/heiko/git/shogun/build/examples/meta/cpp/meta_api/enums.cpp: In function \u2018int main(int, char**)\u2019:\n/home/heiko/git/shogun/build/examples/meta/cpp/meta_api/enums.cpp:20:6: warning: variable \u2018myVar\u2019 set but not used [-Wunused-but-set-variable]\n auto myVar = LIBLINEAR_SOLVER_TYPE::L2R_LR;\n      ^. #3881 . @vigsterkr can you restart the appveyor build?. Merging as AppVeyor was fine earlier (and just removed things). @vigsterkr Are the meta languages cpp included in the coverage?. This is now fixed as #3901 was merged, right? @micmn. The one in master branch has a bug (might give wrong results in some cases)\nThe one in develop branch was recently updated, fixing all errors, and running faster than sklearn now .... I am confused why the meta example tests all work as they kind of rely on global random, or did I miss something?. example for warning rather than error message #3885 . Everything where the main training method is a big loop with a stopping condition based on iterations and/or tolerance. no, as we don't know how long it will run. check git grep progress(range to find the places it is used and how. If we could extend the meta language so that it allows for overloading of a method of an object, then we could test director classes functionality without extending the current test infrastructure.\nAlso, these examples should illustrate why Director classes are cool.\nThe only problem is that we cannot really write code in the meta language, only Shogun API.\nI think one of the nicest things to overload is a method of CKernel, so maybe we go for CKernel::kernel and explicitly return something like the product of the indices (we would need product of basic types for that :/ )? The example then could call get_kernel_matrix, which is C++ code which then runs the kernel code from inside. Maybe there is a better example, it has to be catchy.\nSteps\n Read on Python's SWIG director classes here\n The first step would be to compile Shogun with Director classes enabled -DUSE_SWIG_DIRECTORS=On\n then try to overload a method in an explicitly written Python example, see if it works\n Then do it for other languages (including C++)\n* Think how we can modify the grammar.. https://travis-ci.org/shogun-toolbox/shogun/jobs/249789498#L3545. #3786\n. Fine to merge this despite the formatting errors? I will fix the cmake thing after having moved the files .... So what is next here?. sorry, that didnt come up locally when I tested it\nhttps://github.com/shogun-toolbox/shogun/commit/728a9f57a65d462c42be0e487dd26a26545db9de. Not sure \"create\" is the best name, but definitely better than the one before +1. Instantiate?. @MikeLing as for the \n296 - integration_meta_cpp-neural_nets-feedforward_net_regression (Failed)\n297 - integration_meta_cpp-neural_nets-feedforward_net_classification (Failed)\n320 - integration_meta_cpp-multiclass_classifier-random_forest (Failed)\n324 - integration_meta_cpp-clustering-gmm (Failed)\nI think all of those contain random elements, so the integration tests indeed need to be updated if you change the seed.. sigh, so what are we going to do about this?. The most annoying solution I can think of is using #ifdef to account for both systems .... I agree viktor, not using half of c++11 because of cross platform testing is not a good idea.\nI actually have an idea. What if we create a fake drop-in replacement for random (and all the distributions) that behaves deterministically. This would be purely for unit testing (not statistical testing) so it doesn't matter if the numbers are actually not \"correct\"\nBut just to ensure that random elements (forest for example) of algorithms are fixed accords platforms. Even if you fix the datasets, the algorithms might still behave different otherwise.\nThen we can have statistics tests (algorithm output \"makes sense\") which do not rely on fixed seeds. Maybe offer a flag for that? And so some clever auto settings of it..... I'd be keen to have a flag that improves speed substantially here, trading off with storing the dataset twice. Most of the time that is fine anyways.\nThoughts?. For now, I would just offer a flag \"copy_data_for_speed\" which would be appropriately documented, and enabled by default. Hi!\nYou can ask the mailing list on on IRC as well.\nleft hand side is training set\nright hand side is testing set.\nCheck the nowbooks in the \"showroom\" on the website for example on multiple-kernel learning, svms, etc. Good stuff indeed!\nCalls for a blog post! :). I like the minimal LICENSE thing way more. Check it!\nOn Thu, 7 Sep 2017 at 15:09, N Rajiv Vaidyanathan notifications@github.com\nwrote:\n\n@karlnapf https://github.com/karlnapf Has anyone already worked on\nthis? If not, I am willing to work on this.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/shogun-toolbox/shogun/issues/3922#issuecomment-327811042,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAqqvyBfpumW9RU-n0_QWMb8sdbe4zj1ks5sf_kegaJpZM4OTJiJ\n.\n-- \nSent from my phone\n. Merging as build that compiles unit test passed. Cool as f***!\n\n@geektoni I am not sure I understand each plots of the picture. So you are saying that each point in there corresponds to a single run for a fixed fold idx?\nIf so, this doesnt make sense, as the folds are iid, you can just group all of them.\nOne might be interested in the inter fold variance, i.e. how much variance is there in a single repetition. You want to have access to\n\nthe fold results for a single repetition (i.e. a time series of length k, for k folds)\nthe fold results merged for all repetitions (this is k*num_repetitions numbers). @vigsterkr as explained in irc, I think we should have some SWIG exposed observer that can access such things .... BTW the indentations can be checked locally, or even corrected locally, see the error message in travis for how to do that. Yeah good idea.\nI think a first step here should be to add tests that run preprocessors with all the models (no serialization yet), that might not work ;) so maybe one after the other?. Fine from my side. Actually, why don't you also add a thing to NEWS?. Just after the 6.1 line, like the cloud/windows/c++11 announcement back for 6.0. Done in #3945 .  #3944. Travis passed, I will merge if nobody complaints within today. Oh yes, SVMLight now needs to be guarded to only be available when USE_GPL_SHOGUN is set. I did not realize. Want to go ahead? The unit tests, meta examples, libshogun examples also need guarding then. Sure, but note it is not in the GPL repo but only in the main repo. And since it is not GPL code (i.e. not viral), it can stay there and just be disabled with a switch.\nFurthermore, SVMLight has a GPL dependency, so it needs guarding. Yes svmlight needs a gpl guard everywhere it is used in the main repo (code, tests, examples). grep for GaussianProcess in shogun/tests/unit. Hi @durovo and welcome! Great that you want to work on this\n\nThat would be ok. In fact, anything that reduces the bloated code.\n\na class that generates the data shared by all these tests\na class that sets up the GP pipeline that is used in many of the tests (e.g. one for each of the inference methods, one for regression/classification, etc, .... anything that is set up more than twice should be grouped this way\n\nLet me know if you have any questions. Closed in #3990 . Awesome!. Check the broken travis build. You can see the logs to see what parts don't compile /tests that fail, etc. Yes the failing tests are not your fault probably...ill double check. Yes remove redundant tests if they make no point. For the labels, I agree, score>thresh is the way to do them.\nMaybe you can find out where this constructor is called and then check why it is done this way?. Ok leave the tests in for now (as they are almost working and indeed making sense). If 0.5 sometimes fails (acceptable) then you can decrease the tolerance.\nRun 100 times or so and make the tolerance so that it passes all of them (and still makes sense). I think I am in favour of just deleting any readmes on linalg. The code speaks for itself and is not too messed up (mostly at least ;) )\nSo just maybe remove file and in wiki?. btw you committed changes to the data submodule and this is why the tests fail.\nNeeds to be removed before a merge\nThis is also why all builds fail. Ok lets wait for travis and then merge. yes, probably :). Yep, this has been moved. You will have to checkout the submodule if you want the GPL modules.\nWe have a build where all gpl codes are missing and LICENSE_GPL_SHOGUN=Off here\nThis might be broken (just happened recently), happy for any reports on failure cases as we can include them in the test build. Thanks for reporting, we will fix that. @olinguyen this will be useful for you once merged. Will review soon. Sorry for the delay! I think this was fixed now. If you rebase, what happens?\nShall we aim to get this in soon?. Check http://shogun.ml/examples/latest/quickstart/interfaces.html. Actually, @lisitsyn did changed to testsuite to use the build output in #4034 \nSo now it should work if you rebase against the latest develop\nLet's get this one done, it is very cool to have!. Great!\nSee also the recently opened issue on this #4049 . Maybe we can give this whole part of Shogun a facelift. If you are up for it :). @grig-guz any updates on that?. @gauravcr7rm feel free to start with this. But I would actually encourage you to take one of the other implementations, and modernize them a bit (use lingalg, SIMD, parallel loops, and better APIs). This is a good warm up to get a feeling for how those things are built within shogun. Then you can add a recurrent network. So green lights go ahead :). Closing this one as inactive. hi @olinguyen \nI will go through this now.\nFirst thing: could you remove all cell output from the commit? We are currently not merging notebooks with cell outputs.\nHowever, could you paste a version with the outputs on github gist and paste the link (so I can see how it looks?). Hi @olinguyen \nShall we get this one in soon? Would be great to have it. Yes a graceful error is preferred.\n@iglesias\nOn Wed, 6 Sep 2017 at 16:57, Viktor Gal notifications@github.com wrote:\n\n@ealtamir https://github.com/ealtamir great! although i think we should\nhave the ticket open to fix it with an assertation error instead of a\nsegmentation fault ;)\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/shogun-toolbox/shogun/issues/3975#issuecomment-327530023,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAqqvx1WWMBr9NTgt7kjQfoMuEf4OIpjks5sfsDTgaJpZM4PNvw5\n.\n-- \nSent from my phone\n. Ill make this an entrance task:\n\n\nput graceful error messages if users have set wrong paramerters or provide data that doesnt make sense\nfix the k=1 error\nmake sure than LMNN works. Sorry for the delay.\nWe would need an example here. Can't help with that description.\n\nAs for runtime, MKL is slow, especially when you ramp up the number of data and kernels. But if it is unreasonably slow, we should do something of course. Try importing as\nimport shogun as sg\nsg.SVMLight()\nOn Thu, 14 Sep 2017 at 07:01, nisarwani notifications@github.com wrote:\n\nThanks for the reply,\nI am only compiling ofr python interface. Here is the cmake command\ncmake -DINTERFACE_PYTHON=ON ../\nWhen I try to use SVMLight in c++, the code is compiling, without any\nerrors. But with python when I import Shogun.Classifier.SVMLight it gives\nme import error. If I run MKL without SVMLight, then it gives the above\nerror.\nI have downloaded shogun 6.0 from github\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/shogun-toolbox/shogun/issues/3978#issuecomment-329382182,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAqqv9hFqlQgZMZ6x1bFe0DjXH46X1w2ks5siMFNgaJpZM4PUIo5\n.\n-- \nSent from my phone\n. My hunch is this is the matrix decomposition bailing due to bad\nconditioning, rather than version issues.\n\nHave you tried more reasonable data?\nOn Thu, 14 Sep 2017 at 08:46, Viktor Gal notifications@github.com wrote:\n\n@ArtemiyFirsov https://github.com/artemiyfirsov ok i would first\nsuggest you to switch to nightly as that more mature actually than our\nstable release, so basically remove libshogun17 and python-shogun (apt-get\nremove...) and install the nightly with:\nsudo apt-get update\nsudo apt-get install libshogun18 python-shogun\nif you install stuff from source (see cmake etc.) then you dont need the\ndebian/ubuntu packages...\nanyhow the problem seemed to be with lapack, which is weird as that should\nbe one of the dependencies of shogun package.\nwhat's the output when you do:\ndpkg -l | grep atlas\nand when you do\ndpkg -l | grep lapack\n?\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/shogun-toolbox/shogun/issues/3979#issuecomment-329402043,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAqqv5xD0Estco1GYEDOkOXKCkf0hUqEks5siNnqgaJpZM4PVw99\n.\n-- \nSent from my phone\n. I mentioned this in the NEWS :)\nthanks!. Sorry what exactly is the question? :). any updates?. Can you share a minimal example of the code that caused this? . Is this still happening with the latest release/develop code?. The windows build we have is here: https://ci.appveyor.com/project/vigsterkr/shogun/branch/develop\n\nAFAIK this is without Python bindings. You are welcome to assist us extending this (we don't have windows machines). It seems like your lapack installation is broken. Can you post a gist of your cmake cache?. Post CMakeCache.txt on https://gist.github.com/. @Elluito we cannot help you with such vague problem descriptions. @Nirvan101 you will need to give us a more complete error description of what there is and what you did\n@ramamb25 try the nightly packages. @vermashresth you are using a different version to compile shogun than to run it. @kakshay21 your error seems to be caused by either missing files or an outdated cmake cache. Maybe try to start from scratch if you don't know what that means. Also consider using conda for installation. @jjmachan the dependencies are pretty minimal (almost all optional, except for say Python), but I think it would be a good idea to write them down somewhere. The cmake log tells you \"Test suite for meta examples does not exist! Please initialise  nd/or update the data submodule.\" You can do that (as indicated in the readmes) via git submodule init and git submodule update. Just changed the title slightly ;). your protobuf installation seems borken, or wrongly detected by cmake.\nuse -DENABLE_PROTOBUF=Off as cmake argument. This is due to a broken installation of your blas. I will close this issue now, as it is generating too much noise.. @Chetank99 this has nothing to do with shogun or git, but it is, as you reckoned, your network connection. Well the error message is that git cannot connect to the internet. You will have to check your setup. BTW if you just want to use shogun, just use conda. This is not the place to discuss this. Come join the IRC if you need help on those things. I am unsure of what you actually are trying to do there .... As the error message states, you need to init or update the data submodule, which is explained extensively in the docs.\ngit submodule init\ngit submodule update. There seems to be a missing cblas.h, but your error report doesn't really allow us to help you as it is a bit confusing. Looks like your compiler doesnt support C++14, or the compiler flag hasnt been set for a reason. You could have seen that via googling the error msg error: \u2018std::enable_if_t\u2019 has not been declared. Thanks Marcus!\nOuch, https://travis-ci.org/shogun-toolbox/shogun/jobs/288298933#L5120\nI will look into this!. Well done on your first Shogun patch! :)\nAny plans for next steps?. This is ace! It deserves a NEWS entry I think. Shall I create a build on buildbot?. Entrance task:\n\ninstall conda\nfind out how to compile with conda's MKL activated\nrun tests\n\nMost of the failed tests are probably due to slightly changes numerical values when using matrix decompositions.\n\nintegration tests: just don't store the failure causing value in a variable\nunit tests: investigate. Sure, that would be a cool integration to add.\nWe are currently not enough peeps to make this happen though, wanna help?. GSoC 2018 project...please see our suggested projects (once they are online)\nSome things are already in the wiki, following our hackathon 2017. ldd output for libshogun.so\nhttps://pastebin.com/v6MHVQxN\n\nand the python interface _shogun.so\nhttps://pastebin.com/QhZF5qh4\nThe error is \"Intel MKL FATAL ERROR: Cannot load libmkl_avx2.so or libmkl_def.so.\"\nThe current workaround is something in the lines of\nexport LD_PRELOAD=/opt/conda/lib/libmkl_core.so:/opt/conda/lib/libmkl_sequential.so. Note, not all of the data (too big), just the toy data necessary for the examples. are there benchmarks on the impact of this?\nEspecially with MKL?\nsklearn says on their website that the SVM solvers dont use MKL so no speedup....we could shine once again :). (and of course this is only true when not compiling against say anaconda's MKL blas/lapack. This should be done via the data repository installation. Sorry about the issue, was my mistake ... This is not Shogun related. Google for general solution. Thanks anyways! Generally, such patches are really appreciated. Maybe ping us in irc before sending the PR\nLet me know if you need any pointers.. This is great! Thanks!\nI will review this in detail tomorrow. Thanks!\nNo idea why the build would ever fail for those changes, but let's wait anyways. @sorig \nHey just saw that there might be some files missing:\nhttps://github.com/shogun-toolbox/shogun/blob/develop/src/shogun/features/iterators/DotIterator.h#L2. Unit tests also seem to not have been updated:\nhttps://github.com/shogun-toolbox/shogun/blob/develop/tests/unit/base/DynamicObjectArray_unittest.cc#L3. @sorig would you mind updating this? Also the script you used would be handy. https://github.com/shogun-toolbox/shogun/blob/develop/src/shogun/base/class_list.cpp.templ\nhttps://github.com/shogun-toolbox/shogun/blob/develop/src/shogun/base/class_list.cpp.py. actually, these are fine as they are BSD already, I was confused\nThe only ones left are\n./mathematics/linalg/ratapprox/tracesampler/ProbingSampler.h: * under GPL2+) for graph coloring related things.\n./mathematics/linalg/ratapprox/opfunc/RationalApproximation.h: * under GPL2+. See https://github.com/Froskekongen/KRYLSTAT.\n./mathematics/linalg/linsolver/CGMShiftedFamilySolver.h: * written by Erlend Aune, under GPL2+\n./mathematics/linalg/linop/SparseMatrixOperator.h: * under GPL2+\nI will take care of those, as the files actually are GPL and need to be moved. SHOEGUEN!!!!. Now we have python using the build and R using the system install?. +1. Whoops, indeed!\nThanks for reporting.\nThis is a lovely entrance task for those try to get involved in Shogun.. The R command also is not rendered properly, so that needs a mini fix as well. do it :). Nope, just fixing typos can be done in a single PR which consists of 1-2 lines. gnaaaaa. Ok, let's see what the buildbot says. Ah that is a known problem.\n@sorig didn't you recently check this?\nworkaround: remove that part of the example code, it is only for testing. We will try to fix this soon. If this persists after removing and re-creating the build directory, could you show how you run cmake to reproduce this?. I cannot confirm this, i.e. doens't happen on my system.\nI guess you can dive in and check it out. The class has guards and you can see whether they are set or not. \nBTW I am not sure what you mean when you say you run the example on the shogun cloud? Anyways, the cloud is built with testing disabled, so the class doesn't exist in there\nI will close this for now. Nice work, all tests pass so this is good to merge from my side.\n@vigsterkr @lisitsyn objections?\nWhile we wait for a reply, you could make the style checker stop moaning:\nhttps://travis-ci.org/shogun-toolbox/shogun/jobs/320567121#L727\nGreat entrance task, welcome to Shogun! :). (The WrappedObjectArray issue doesn't happen in travis). Ok let's kill him. Thanks!\nNeeds minor fixing and then good to go.\nWelcome to Shogun! :). Ace thanks!\nOne last thing (apart from my comment).\nThe style checker complains, https://travis-ci.org/shogun-toolbox/shogun/jobs/320637977#L745\nCould you fix that? We recently are trying to get a more consistent style in. lovely, thanks! :). #3840. Go for it. I will look into a fix for this after merging #4069 \nThis requires some changes in the meta grammar. Thanks for reporting the issues, we did indeed not foresee these problems. Very useful! Once I have sent a patch, you can continue\n  . @sorig . This is now possible to do, using the new put/get API\nSee https://github.com/shogun-toolbox/shogun/blob/7513cd68101ef3541390303aadadbc1ddfa5cbe1/examples/meta/src/base_api/put_get.sg or https://github.com/shogun-toolbox/shogun/blob/7513cd68101ef3541390303aadadbc1ddfa5cbe1/examples/meta/src/meta_api/kwargs.sg. I am closing this as it is part of transitioning to the new API and we have lots of examples here already. :heart: . I have written a little list of steps to identify which method to work on above. This is really useful info. Thanks for that! Curious to see what this leads to performance wise. Please don't comment here, but in the referenced issues. ho ho ho. Good example for time-series splitting based on simple iterator approach\nhttps://github.com/scikit-learn/scikit-learn/blob/ea6ea815e1071bfb9de18953fea98b136a7fa8ed/sklearn/model_selection/_split.py#L770. Go for it. An iterator would be a next step, much cleaner, yes. sure go for it :). Argmin should be defined. Check the cookbook readme it tell you where to put custom latex. If you find typos, make a separate pr. Closed via #4055 . Hi!\nThis might well be a bug, shall we try to create a minimal python script to reproduce this?. Or alternatively, can you share the data file and labels? Then I can try it locally. Dummy data is fine (if it reproduces the error).. Cool, thanks. I will look into that a bit later today.. Try to not set the feature types (I assume you want to do continuous features). There will be a warning, but it did not crash in my test (see above). This definitely is a bug. I will close this issue for now as we have #4054 in place. We can continue to discuss in here though. PS I made new code since yours was not standalone. But I think there is no qualitative difference. I am not sure what a weighted majority vote would mean for regression. Do you want to do classification?\nAs for the categorical features, yes that is definitely a bug. We need to investigate what is going on there.\nI might have some time to do that in the near future, but I am away the next week unfortunately. I put up the description so that maybe somebody else could pick that up. Bear in mind it is holidays :). Much appreciated thanks!. Cool! Good to go from my side (travis timed out). Thanks a ton for taking this up. The error message should be printed. Maybe it is Python? Did you manage to reproduce the same problem from c++?\nNot sure what is going on, you have an idea how to fix the problem itself. \nApproach:\n * Build unit test that fails\n * Fix the cause\n * unit test passes. Yes, for c++, you want init_shogun_with_defaults.\nFor Python, that should be the default @lisitsyn @vigsterkr \n@vermashresth thanks a lot for looking into this.\nNow, leaving the error message printing aside, let's get down with this RF bug :)\nWhy is it that there is a size mismatch? Isn't there one feature type per feature?. You are absolutely right, I don't know why I got confused here. So is there any problem after all? Doesn't seem like.. @vigsterkr @lisitsyn what's you opinion on that?. Great stuff, thanks for the patch!!!\nOne thing is that the data submodule needs to be updated first as I think all integration tests will fail otherwise. Or am I wrong?\nCheck the developing and examples readme on how to run those tests. I.e. each meta example generates output of the same filename as itself.\nThen the output is compared across the different languages and the c++ version.\nNow if you rename the files, our build simply doesnt run the tests anymore (check output of the c++ build). See here https://travis-ci.org/shogun-toolbox/shogun/jobs/322523257#L4853\nyou can see that only the tests for the not-renamed files were executed.\nYou will need to rename the test files as well, send a PR to the data submodule, and then update the submodule version in  this PR here. Then the tests will be executed and we can merge this.\nThanks so much for the effort!. Looks good to me! Thanks a lot for the cleanup. Well done on that! :)\nFeel free to do another one ... always welcome!. But this is already improving, let's iterate a few more times, and this will be good to merge.\nTHANKS! :). @lisitsyn ok it was green now apart form style, which I fixed. Let's wait for travis to be sure, but you can review now. And potentially merge. Ok baby, lets merge it, just fixed the style issues..... I take that back, windows is not happy, https://ci.appveyor.com/project/vigsterkr/shogun/build/1668#L16427. One unit test was actually broken (and under linux only passed accidentally with an invalid read memory error). Should be fixed now.. windows times out and travis is angry about style (even though I ran the checker, I think it is confused).\nI think that means merge ... Objections?. I'll just do it and fix tomorrow if there are problems. Do it. Make sure to read the developing readme before you start. yes, thats exactly it. make sure to send a pr soon to we can give feedback.\nKeep in mind we also want to get rid of independent jobs.\nBest would be if you used openmp instead for now. Everything in the computation folder should go. Done via #4103 . The style thing can easily be fixed via looking at the travis output, it gives you a command that formats all files corretly: https://travis-ci.org/shogun-toolbox/shogun/jobs/328007339#L720. Good stuff. Thanks!. https://travis-ci.org/shogun-toolbox/shogun/jobs/328667417#L718. This broke the windows build, due to removing the lapack guard\n[00:28:32]   C:\\projects\\shogun\\src\\shogun\\clustering\\GMM.cpp(424): error C2039: 'compute_eigenvectors': is not a member of 'shogun::SGMatrix<float64_t>' [C:\\projects\\shogun\\build\\src\\shogun\\libshogun.vcxproj]\nIf you want to remove that guard, you will need to use linalg eigenvectors. FYI: Your change did not change any of the outputs of the gmm example, see https://travis-ci.org/shogun-toolbox/shogun/jobs/329120177#L5035. Great, thanks for the patch.\nWant to have a look at LinearRidgeRegression next? I saw it is also guarded, but it in fact uses Eigen3 inside. will have to work all sorts of stuff out, don't look at this for now. I don't think this is a shogun problem.\n@dougalsutherland maybe we can ask for an hdf version of at least something, rather than exactly something?. @mlsgenaro google a bit to see how you can resolve package conflicts in conda. The error messages are quite clear. Should be something in the lines of (of the top of my head)\nimport moshogun as sg\ntrained_model = sg.LibSVM()\n...\nfile=sg.SerializableAsciiFile(\"serialized.dat\", 'w')\ntrained_model.save_serializable(file);\nfile=new CSerializableAsciiFile(\"serialized.dat\", 'r');\nCLibSVM* loaded_model=new LibSVM();\nloaded_model->load_serializable(file);\nfile->close();\nSG_UNREF(file);\nLet me know if that goes through, if not, I can write a concrete example\n. Great!. I think the LAPACK guard was still necessary.\nBTW if you remove such guards, make sure that Shogun compiles if the library that you guarded is not available. Really important.\nThe windows build doesnt have lapack installed, and here you go:\n[00:29:53]   C:\\projects\\shogun\\src\\shogun\\distance\\MahalanobisDistance.cpp(56): error C2039: 'inverse': is not a member of 'shogun::SGMatrix<float64_t>' [C:\\projects\\shogun\\build\\src\\shogun\\libshogun.vcxproj]\n[00:29:53]   C:\\projects\\shogun\\src\\shogun\\distance\\MahalanobisDistance.cpp(56): error C3861: 'inverse': identifier not found [C:\\projects\\shogun\\build\\src\\shogun. It is this line:\nhttps://github.com/shogun-toolbox/shogun/blob/develop/src/shogun/distance/MahalanobisDistance.cpp#L58\nBut actually this code is bad generally (not your fault). One should never ever store matrix inverse. Instead, it should store a cholesky factorization of the covariance matrix. Then, rather than multiplying the inverse covariance matrix to a vector in the CMahalanobisDistance::compute method, you should do a cholesky (triangular) solve. See (https://github.com/shogun-toolbox/shogun/blob/develop/src/shogun/classifier/LDA.cpp#L180 for an example how to do this in linalg. The matrix is PSD indeed, so LDLT would be more stable.\nhttps://eigen.tuxfamily.org/dox/group__TutorialLinearAlgebra.html\nI think adding an argument to cholesky would be good, so that the factorization can operate in both modes, something like or use_ldlt which by default is false. Also make sure to update the docs appropriately so it is clear what is used.\nGood catch!. You are absolutely right. In fact I now remember a discussion we previously had about this some years back. I think a clean solution would be to expose a new method and a few solver method as well. Everything else is too messy.. Could you pls split the PR into two: one for regression and one for the distance.\nYou still haven't checked whether shogun compiles with the guard remove and no lapack installed.\nPls don't remove guards without checking whether things still work: https://ci.appveyor.com/project/vigsterkr/shogun/build/1873#L6478. yep, you can just force update this one now. But you will need to fix the \"inverse\" problem (see above) first.. The release doesn't come with the gpl files anymore.\nEither clone https://github.com/shogun-toolbox/shogun-gpl into the src/gpl folder, or directly checkout from github and then do git submodule init && git submodule update\nLet me know if that solved it. If you download the file by hand and exatract it into external, that should do it. Google LD_LIBRARY_PATH and look out for how to set that in the documentation. It is not set correctly. Good work, thanks!. You are probably right. This means our random forests are not clonable. It has nothing to do with crossvalidation actually.. Actually I don't even understand why that clone is there in xvalidation. We removed the multicore version a while ago due to problems, so no clone is needed. puzzled. @vigsterkr any comments?. Why did you close?\nIf you figure out something is still wrong, just write that it is WIP as a comment, and then update your feature branch... We generate a lot of spam PR numbers otherwise. This is great! Thanks for the patch.\nI have one (annoying) re-structuring request: Could you send a PR where you store all those notebooks in your new format without doing any changes first? The diff is totally useless otherwise, and I cannot read through all of the notebooks in source code. Make sure to not loose your changes when doing this :). @iglesias now your notebook preview builder would be handy ;). Any updates?. The rebase needs to happen before I can review anything. This is just too big. Yes, but pls send that in a sep. PR that includes only deletions. DO IT. #4046 . > (18:34:46) zoq: Agreed, a simple preprocessing script sounds like a good idea, at the end, it's just a simple SQL query.\n(18:34:58) zoq: We could provide some simple scripts apart from the HTML/JS interface for the most common tasks.\n(18:35:12) zoq: I guess you could also write a simple Jupiter notebook? I think you could provide some input on that?. But some shogun algos are uncovered, so how can we know how it performs?. Thanks for reporting!\nWant to send a patch?. This one has a high impact: It brings the integration tests back: those are based on the CDynamicObjectArray and since the parameter was not registered, ::equals always returned true since e4a19a2519e3b76b281b88c025e054e9e5fcfa97. done via #4116 . Sounds like a type error. I'll check what's going on...\nIn the meantime, could you try the thing outlined in examples/meta/r/meta_api/matrix_types.R\n```\nmat <- RealMatrix(1000,10)\nuse loop to fill matrix, i.e.\nmat[1,1] = ...\n. I can confirm this is happening, as well for my suggestion above. actually\n\nmat <- RealMatrix(2,1)\nmat[1,1]=1\nError in .simpleInheritanceGeneric(fdef) : node stack overflow\nError in .allowPrimitiveMethods(primMethods) : node stack overflow\n\n$ R --version\nR version 3.2.3 (2015-12-10) -- \"Wooden Christmas-Tree\"\n```\nThis is develop, not the feature branch @vigsterkr mentioned. I just had a thought: we can also separate the openmp stuff into a new PR\nThis way, this one would just be refactoring without using the computation stuff, and then next one would be deleting the computation folder, finally, we would work on openmp separately from the refactoring (might be easier)\nI will leave that to you. What fails in the unit test?. Ok so why don't you remove all the omp calls then for now.. We now have a merge conflict as well.\n\n@lambday can you help with the aggregator?. @vigsterkr we had also split things a bit to make it easier to oversee\nfirst patch is to refactor existing code/tests to not use the computation framework.\nnext is deleting the now unsused code\nnext is using omp. I think we are good. Thanks for this monster effort!\nNext PR would be to continue with the other changes we discussed: removing all the classes that are now not used anymore, paralleling the current code using openmp.. @awild82 just asking: this solves the problem in #4098 for you?. You could write one for a linear svm or a random forest?. Xvalidation cookbook exists \nI suggest you start with porting an undocumented example from python to meta example, without a cookbook yet. There are dozens. Start with a simple one, eg kernel . yeah GPL off stopped working on my machine as well. Just turn it on for now, I will try to fix it later. I am pretty sure your code will violate the newly enforced style checker.\nRun git clang-format-3.8 --commit 7cfc4b6ffb18574fe5d76fc43c97b997ddbd0163 --binary /usr/bin/clang-format-3.8 with the git revision being the last one in develop before your PR. Windows is fine now. If you fix the style we can merge :). Still style errors:\nhttps://travis-ci.org/shogun-toolbox/shogun/jobs/337145184#L714\nBut windows passed, and all unit tests as well. Will merge. As @vigsterkr said, pls dont close PRs and open new ones. Just update the old one. Please don't close PRs if you are sending another one. Rather update it. I wrote in #3000 \n\"No need for cookbook rst pages for the remaining distance examples. A meta example is fine.\"\nWould you mind removing the cookbook page. The example itself it useful, Thanks for the patch!. Cool, the page is removed, but I don't see a PR for the data repo and you will also need to update the data submodule in this PR here. The PR has merge conflicts, you will need to rebase against the latest develop. wrong data submodule revision! Check the diff!. Tests passed, integration test included. Merging, and thanks! :). Welcome Paco, thanks for the patch! :). The example looks good. I have in the meantime merged you data PR, so you can now add the update to this PR as well. Thanks for the contribution! :). btw is there still an undocumented python example that can be deleted?. The error is not good. I'll make it an entrance task to put a better message in there.. @laurazh there is something wrong with you labels. Can you post the output of\nlabels.get_values(). Sorry, I meant, get_labels(). i.e.\nsg.MulticlassLabels(np.arange(4).astype(np.float64)).get_labels()\nor\nsg.MulticlassLabels(np.arange(4).astype(np.float64)).get_num_classes(). I just realised that you simply forget to train the knn instance. Add knn.train(). The task here (for GSoC applicants) is to add a meaningful error message.. http://lmgtfy.com/?q=update+existing+PR. Great, very useful!. @lisitsyn agree?. Please don't open multiple PRs. You don't need to close them but can easily update them. As said in the developer readme. Cool!\nBefore this can be merged, the data file needs update, as does the submodule revision in this PR. I think you can already change the submodule before it is merged....(not sure though)\nIt IS merged now however ;). Your data submodule is wrong in  here: https://travis-ci.org/shogun-toolbox/shogun/jobs/337352894#L455. Pls check the travis builds yourself to see these errors in the future.. Seems good now, thanks!\nYou can see here that the integration test is executed as well (using your data) https://travis-ci.org/shogun-toolbox/shogun/jobs/337548666#L4824. This is not a github issue, but a topic for the mailing list. Pls post it there. Or StackOverflow. @lisitsyn I need that clone of arrays, and of SG\n[DEBUG] Cloning parameter CloneEqualsMock::raw_matrix_basic of type shogun::Array2DReference<bool, int>.\nunknown file: Failure\nC++ exception with description \"Assignment not supported\" thrown in the test body.. Thanks! :). no windows-specific changes here, merging. It does :). @lisitsyn works with cpp, python, java, octave on my machine. Just merged #4154 if you wanna rebase. You will need to test that, I removed the nested call in the kwargs.sg. Thanks for the quick fix! :). Seems like ruby likes you :). lgtm thanks!. @vinx13 This broke the ViennaCL build, I guess this is due to come forgotten const ...\nWould be good to get this fixed asap. http://buildbot.shogun-toolbox.org:8080/#/builders/6/builds/316. Cool! Thx. Travis is green so let's try. LGTM! thx!. Pls don't spam out github via opening and closing PRs (and also not opening new ones when your old one didnt work, you can update* the old PR). Pushed this into the typedo feature branch. Yes! style fix and let's merge. Nice work!. :tada: . Thanks!\nWe will need to wait with merging this, as there currently is a problem with the internal tests that needs fixing first (and likely update of the data file).. The integration test fails if the listing produces different data than the one in the reference data. See the examples and the development readme. Shall we get this merged?. Sure, wanna go ahead with that?. There now also is a merge conflict, so you will need to update this PR. Thanks!. What is the status of this one then? :). ok ping me here once that is the case. typo fix and travis green and we can merge this :). restarted the hickup builds...should be fine soon. Hey. \nBefore you check out the meta examples, you will have to look into the typemaps.\nI suggest you modify one of the generated meta example code listings by hand and then execute them by hand to see how you will have to modify the grammar. But the meta examples are not the problem here, it is easy to update that once the typemaps use the native ruby matrix lib.\nCheck  swig_typemaps.i in interfaces/ruby\nIf the matrix class is immutable, then we will need to discuss this a bit .... @songxujing pls dont spam this thread which is about something else. Pls use the mailing list or open a new issue.. Added a few comments, the build seems to be fine. So tests passing.\nOnce we have cleaned up I think we can merge this.\nDid you check all ipython notebooks and meta examples for the new API?\n. Any updates here?. This is almost ready.\nJust my comment on the naming. What are your thoughts here?. Thanks for the udpate and ping! :)\nLooks good! No need to squash as github can do that these days\nOne thing I want to make sure.Did you run all unit and integration tests?. Thanks for the quick response :). Travis is happy now. Thanks!. Yes, exactly. Thanks!\nThe build is failing due to style errors (you can click it to see the log)\nhttps://travis-ci.org/shogun-toolbox/shogun/jobs/345852507#L729\nShows you how to fix it. LGTM\n@vigsterkr ?. https://travis-ci.org/shogun-toolbox/shogun/jobs/346871904#L712\nTo fix the errors automatically please run: \ngit clang-format-3.8 --commit 36406d48ed0183b83c92c79285040322d00f69e6 --binary /usr/bin/clang-format-3.8. This is good, just needs the correct style fix and we can merge it :). You will need to read the git manual, or use google. Keywords \"rebase\" \"merge conflict\". Don't we have some unit tests to cover this?\nSomething like. Asserts that residuals match requires tolerance on termination?. Otherwise fine from my side.\nIntegration tests should soon be back so then we can compare using that. Yeah would be good to add something in these lines to this PR. Great!. Once the tests pass this can be merged. :tada: . LGTM  ..... why not drop the CMath::log altogether in the PR?. Cool thx!. why another PR on this?. Pls don't open multiple PRs, but instead fix your old one. You can overwrite things with -f. Thx!!. Thx! :). Hi! Thanks for the patch!\nCould you pls send a PR that only contains changes in the lines that you acutally worked on? Otherwise I cannot review it. The stylechecker only checks lines that were changed by you so it will not fail if the rest of the file doesnt obbey the style. Pls don't close and re-open PRs. You can update your existing ones.\nBuild time is not your fault, if you ping us, we can restart the build. Could you pls send a PR where only the lines you changed are actually in the diff. There are some huge chunks of changes that are just due to formatting. The autoformater will only change your changes if you use it corectly. Looks good, thanks for the patch!. We would either need a full error message (use a github gist), or you can install the binary version of shogun if you have trouble compiling it from scratch. There is both a ppa and a conda package. We could also have a constructor in CBinaryLabels that takes a CLabels pointer and then does the same thing as this introduced as_binary method. Not sure that conflicts with a copy constructor in some way. I actually like the explicitness. @lisitsyn see updated version. I would like to merge this.\nBoth travis and windows timeout unfortunately.\nLocally, the tests pass.. Shall I contact travis to get more time on the builds?. Alternatively, we could disable the libshogun examples by default. removing old examples then?. Looking at the logs, this will not help unfortunately :( it is killen when compiling shogun itself. Alright, let's see what the buildbot says to this.... Great! Thanks for the patch. Will review tomorrow. Any updates here?. Hi there. Did you try fixing the seed?\nAlternatively, you could just not store the accuracy in a variable (all variables are used in integration tests, so if you dont store a number, it wont be used). Check the other cookbooks that depend on random states and how they fix the seed\n. CI is still failing. I merged the data PR in the meantime. Sure about the data.. What is the state of this?\n@FaroukY It would be cool if you could address suggestions asap, otherwise the whole thing gets stale and is more work to pick up again.. https://travis-ci.org/shogun-toolbox/shogun/jobs/382825416#L4800\nIntegration test fails. Make sure to reproduce it locally and potentially update the data file. always push things up, dont hold them back.\nThe labels problem should not block anymore now #4313 . and yes, pls always send a PR with the data file, otherwise we waste time looking at error messages. This should be fixed in develop, try rebasing and octave will pass\n(i.e. I reproduced and fixed the error with locally using this example). This needs a rebase as I fixed some missing SG_REF recently.\nShould be pretty minimal.. The liblinear test segfaults under clang.\nCould you pls run it with valgrind and make sure it neither leaks nor does uninitialised memory reads?\nhttps://travis-ci.org/shogun-toolbox/shogun/jobs/359675411#L4561\nPls check those things yourself next time, it is all there..... Windows build passed, but that is since it doesnt have LAPACK installed and you have the test still guarded so it is not executed. clang passed so the tests executed in there were fine. good.\nthe timeout is not your fault.\nI think this is OK to be merged soon.\nA made a few more comments inline. Ok cool! This is much better. Lets wait for the CI and then merge it!. Did run valgrind to check the memory usage of this test? No leaks, no uninitialized reads?. Great!\nThanks so much for this clean-up. Very useful. in #4204 the integration tests fail (i.e. the mean of the mixture component), not the unit test.\nI am not sure whether the cluster centres might be permuted if the sign changes, or if the results are simply different. Did anyone check whether some unit tests fail as well?. Ill dig into the GMM code and see whether we can make it invariant to the sign in GMM itself. Maybe it would make sense to do some postprocessing after the backend call.\nWe can enforce all eigenvalues have positive sign for example (moving the minus into eigenvector, or just drop if both are negative). Thoughs?. Alternative is to do that in the GMM .... and all the other algorithms where such problems appear. in this case ... good luck solving it :). Check the readme how to set the LD_LIBRARY_PATH. pip install ply\nor compile with -DBUILD_META_EXAMPLES=Off. you will nee to git grep CMath::is_inf to find all the occurences.\nRunning the tests locally will reveal forgotten ones, such as\nhttps://travis-ci.org/shogun-toolbox/shogun/jobs/356870589#L6244. Thanks for the PR. I guess currently no, but feel free to send a PR for this. Shouldnt be too hard.. Seems like a typo in the example to me. Sure you have the same listing as\nin the develop branch?\nOn Sat, 24 Mar 2018 at 23:23, Elfarouk Yasser notifications@github.com\nwrote:\n\nI just rebased against the new develop branch and built but got:\n[ 86%] Generating example multiclass-gaussian_naive_bayes\nTranslation of dependencies failed!\nCould not translate file /home/farouk/Desktop/shogun/examples/meta/src/multiclass/gaussian_naive_bayes.sg to .cpp.\nTraceback (most recent call last):\n  File \"/home/farouk/Desktop/shogun/examples/meta/generator/generate.py\", line 158, in \n    generatedFilesOutputDir=args.parser_files_dir)\n  File \"/home/farouk/Desktop/shogun/examples/meta/generator/generate.py\", line 81, in translateExamples\n    storeVars=storeVars)\n  File \"/home/farouk/Desktop/shogun/examples/meta/generator/translate.py\", line 822, in translate\n    storeVars)\n  File \"/home/farouk/Desktop/shogun/examples/meta/generator/translate.py\", line 244, in translateProgram\n    globalFunctions)\n  File \"/home/farouk/Desktop/shogun/examples/meta/generator/translate.py\", line 346, in dependenciesString\n    translations = list(map(self.translateDependencyElement, dependencies))\n  File \"/home/farouk/Desktop/shogun/examples/meta/generator/translate.py\", line 380, in translateDependencyElement\n    includePath = self.getIncludePathForClass(typeName)\n  File \"/home/farouk/Desktop/shogun/examples/meta/generator/translate.py\", line 416, in getIncludePathForClass\n    (' or '.join(variants)))\ntranslate.TranslationFailure: u'Failed to obtain include path for Clabels or labels or Clabels or labels'\nexamples/meta/CMakeFiles/meta_examples.dir/build.make:954: recipe for target 'examples/meta/cpp/multiclass/gaussian_naive_bayes.cpp' failed\nmake[2]:  [examples/meta/cpp/multiclass/gaussian_naive_bayes.cpp] Error 1\nCMakeFiles/Makefile2:3908: recipe for target 'examples/meta/CMakeFiles/meta_examples.dir/all' failed\nmake[1]:  [examples/meta/CMakeFiles/meta_examples.dir/all] Error 2\nMakefile:160: recipe for target 'all' failed\nmake: *** [all] Error 2\nThe build had no problems before the rebase, but it seems there is a\nproblem after the rebase. Any ideas?\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/shogun-toolbox/shogun/issues/4215, or mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAqqvxIyLkW7xPHSb5Eg5KCFG8Ko2_GFks5thsdNgaJpZM4S57Rk\n.\n-- \nSent from my phone\n. Quite cool to have stan in. Let's get going with this patch!\nThere are some open requests by viktor @FaroukY . Thanks for reporting.\nThe windows build is quite experimental.\nThere definitely is a problem with eigen not being able to detect your blas setup (second gist). @vigsterkr if you copy paste the gists, they work\n\nhttps://gist.github.com/fyoda/fca418aa6be771b9c9da5b4193326945\nhttps://gist.github.com/fyoda/f59cdefb4d8e62b2ce3bfcb4ac8c5fd4\nhttps://gist.github.com/fyoda/9c1ac9b2fdc418f9aff6910e2078ff85\n. Thx for the patch!. Sorry! As of now, you will have to check how we build shogun in appveyor and try to replicate that.\nAny patches fixing potential problems are more than welcome. Shogun is part of vcpkg\nhttps://github.com/Microsoft/vcpkg/pull/2977#issuecomment-371225760\nYou could try that and let us know how it went. Doesnt compile with ruby.\nCould you investigate a bit locally?\nhttps://travis-ci.org/shogun-toolbox/shogun/jobs/358916322#L1677. Sorry, I dont understand what exactly your problem is. Could you make a minimal example where you state what you expect and contrast it with what you get?. import shogun as sg\nsg.Math.init_random(1)\nThis will change but for now works. Overlooking the diff, this looks good.\nI dont understand why the build would break.\nYou are right that as is not available through the interfaces, but if you dont change the example code, then there should not be any problems (we will solve obtain_from_generic in the examples slightly differently). Are these ALL instances of obtain_from_generic in src/shogun ?. Ok cool! thx for this!. your protobuf setup is broken. You will need to investigate whether the library is in your dynamic linker path. The error is a standard error, so pls ask google. It could also be that shoguns cmake setup has to be fixed, in which case we can reopen this issue. Let me know. I googled a bit and found\nhttp://www.swig.org/Doc2.0/Java.html#Java_dynamic_linking_problems\nIt seems that the Shogun java lib is not visible to your java executable.\nwhat happens if you run the tests locally using cmake, i.e.\nctest -R generated_java-binary-multiple_kernel_learning -V\n. The CI errors might come from your test that fails to compile https://travis-ci.org/shogun-toolbox/shogun/jobs/362010308#L1948\nIt might be from the order of the includes in your test. Try including the googletest headers first (or last) dont remember, check other tests). Next thing, could you chunk the PR into multiple smaller ones? I.e. make a copy of your changes locally and update this PR with a smaller number of changes. It is better to send 5 small PRs than one big one (if possible). Try to separate individual additions. It makes things so much easier to review ( the reason why this wasnt reviewed yet is because it is so big). Next thing, could you chunk the PR into multiple smaller ones? I.e. make a copy of your changes locally and update this PR with a smaller number of changes. It is better to send 5 small PRs than one big one (if possible). Try to separate individual additions. It makes things so much easier to review ( the reason why this wasnt reviewed yet is because it is so big). Yes, definitely a lot of work. But hey -- the code is old, and old code requires love :)\nI realise you followed the structure and that is totally fine. However, lets not make the mess bigger. If we are going to extend this, lets at least get some improvements into place as well, to keep this maintainable and extendable. A lot of Shogun is like that: you wanna do something new, you first have to fix the mess (at least some parts of it)\nI agree it is a good idea to isolate some of the comments in issues. Do you want to go ahead with some of that?. We can discuss more details in IRC.. looks good to me, lets wait for the CI.\nthanks for the patch! :). Ill merge once CI is green. My motivation was: linking (even dynamically) against GPL code makes the code GPL. Therefore, I added some guards to prevent this. Correct me if I am wrong.\nDefinitely the LGPL things need to be removed.. Apparently the FSF sees dynamic linking as derivative work:\nhttps://en.wikipedia.org/wiki/GNU_General_Public_License#Linking_and_derived_works. Sure then lets leave it. @saatvikshah1994 go for it! :) Looking forward to seeing a patch here. well done :)\n. ah well, actually we also want to linalg-ize the code itself, not just the tests. go for it!. Definitely.. @iglesias we can now squash with github, so people can just keep on adding commits to PRs (better to see trajectory, history, and preserving discussions (at least for small changes that make sense to group into a single commit). All good the error is not caused by you. Sometimes, you will find quite bad style when looking around shogun code.\nHow did you solve it via your custom reduction?\nDid you do some research to see which to prefer over the other. thanks for the comment, I agree. Unfortunately this needs a fix before we can parallelise it, obviously.\nIdeas?\nOn Tue, 10 Apr 2018 at 16:14, Shubham Shukla notifications@github.com\nwrote:\n\n@shubham808 commented on this pull request.\nIn src/shogun/mathematics/linalg/ratapprox/logdet/LogDetEstimator.cpp\nhttps://github.com/shogun-toolbox/shogun/pull/4235#discussion_r180459962\n:\n\n    for (index_t j = 0; j < num_trace_samples; ++j)\n  {\n      SG_INFO(\n          \"Computing log-determinant trace sample %d/%d\\n\", j,\n          num_trace_samples);\n      // get the trace sampler vector\n      SGVector<float64_t> s = m_trace_sampler->sample(j);\n\n\n// calculate the result for sample s\nfloat64_t result = m_operator_log->compute(s);\n{\nsamples[i] += result;\n}\n}\n// calculate the result for sample s and add it to previous\nresult += m_operator_log->compute(s);\n\n\nI am having trouble in using const in compute due to this\nhttps://github.com/shogun-toolbox/shogun/blob/32a6746251e4864269feb08702786ed1214be0b6/src/shogun/mathematics/linalg/ratapprox/logdet/opfunc/LogRationalApproximationCGM.cpp#L64\nline..\nThe other method i.e. sample(j) is thread safe.\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/shogun-toolbox/shogun/pull/4235#discussion_r180459962,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAqqv5CgDYdKucWZGpuix3ao-MZHzhTnks5tnMxrgaJpZM4TJkk6\n.\n-- \nSent from my phone\n. Let's wait for CI and then merge it.\n\nJust being curious, did you try to run this code before and after parallelisation, and measure the time it takes?\nYou could try the ipython notebook for example...... Hey so whats the state of this?. Yes a Boolean flag should work. Then the solver knows that it has to put a\nminus, but it doesn\u2019t need to allocate a new memory block\nOn Sun, 13 May 2018 at 06:24, Shubham Shukla notifications@github.com\nwrote:\n\n@shubham808 commented on this pull request.\nIn\nsrc/shogun/mathematics/linalg/ratapprox/logdet/opfunc/LogRationalApproximationCGM.cpp\nhttps://github.com/shogun-toolbox/shogun/pull/4235#discussion_r187788650\n:\n\n{\n  SG_DEBUG(\"Entering\\n\");\n  REQUIRE(sample.vector, \"Sample is not initialized!\\n\");\n  REQUIRE(m_linear_operator, \"Operator is not initialized!\\n\");\n\n// we need to take the negation of the shifts for this case\n- if (m_negated_shifts.vector == NULL)\n- {\n-     m_negated_shifts = SGVector(m_shifts.vlen);\n-     Map shifts(m_shifts.vector, m_shifts.vlen);\n-     Map negated_shifts(\n-         m_negated_shifts.vector, m_negated_shifts.vlen);\n-     negated_shifts = -shifts;\n- }\n+\n+ SGVector negated_shifts(m_shifts.vlen);\nmmm that would have worked if we had a way to tell if a shift has been\npreviously negated. we need to remember something in compute (like if\nm_negated_shifts is null or if the flag has been previously set) but due to\ncompute being const now it will not allow this..... any ideas?\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/shogun-toolbox/shogun/pull/4235#discussion_r187788650,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAqqv9w7WeiAJ8T9HrD21wz26mGDt97Sks5tx7WJgaJpZM4TJkk6\n.\n-- \nSent from my phone\n. This broke the build.\nhttp://buildbot.shogun-toolbox.org:8080/#/builders/10/builds/430\nPls prioritise fixing this before other things.. @lambday any comments on this test and how it is affected by paralleisation?. I dont think they were executed on travis. Ping me in IRC and I can help you reproducing it. Nice, thanks of for the patch.\nThis is an incomplete refactoring though, see binary/kernel_support_vector_machine.sg for a fully refactored example (including loading the files etc). I think fully changing the examples will break the ruby tests though.\n (due to a potential bug in swig ruby)\nThat is fine, I have discussed this with @lisitsyn and we decided to disable the tests for ruby for now in order to move on with the refactoring .... Check this commit: https://github.com/shogun-toolbox/shogun/commit/c85feddc80340c61261f1a57328521adf889bc0e#diff-197df1e4436c2e76a5d4e179a2ee6ac6\n\nThe way to get around this problem is to replace the runtime type assertion with a conversion, i.e. replace something like\nASSERT(m_labels->get_label_type() == LT_BINARY)\nwith\nauto binary_labels = m_labels->as_binary();\n(and then use binary_labels downstream). Function defined here: https://github.com/shogun-toolbox/shogun/blob/develop/src/shogun/labels/BinaryLabels.cpp#L161. What being done? :). Totally, let me go through it. Ok to merge from my side once CI is green. Why did you reference the labels factory issue?. Travis error is unrelated (and fixed in HEAD). wanna move on by porting a legacy python example to meta example?. Fine to merge from my side occe CI is happy. Seems CI cannot be happy as this invalidates the cache .... nice! :). Actually this is quite high priority, so we will try to get this fixed asap. This is still open, there are still some\n\nconstructor calls ( to be replaced by factory calls)\nspecialized method calls (should be refactored to put/get or otherwise). it involves doing these changes and then checking whether they break anything. This involves reading the code, thinking, and running the tests. Please check here how to get started: https://github.com/shogun-toolbox/shogun/wiki/Getting-involved. The patch that really removes RealFeatures from the swig interfaces is 6475cf9956bc7e920e40b71836398161ec71ff5f\nIt is not included in here since the legacy python examples and the notebooks still rely on RealFeatures. Tada!. It does, ends up in the last case on the bottom. But we could maybe better\nput sg not implemented, you are right\n\nOn Thu, 19 Apr 2018 at 23:34, Fernando J. Iglesias Garc\u00eda \nnotifications@github.com wrote:\n\n@iglesias commented on this pull request.\nIn src/interfaces/swig/shogun.i\nhttps://github.com/shogun-toolbox/shogun/pull/4261#discussion_r182903712\n:\n\n    }\n\n\n\n// tag didnt match: either it was vector, or has different inner type\n\n// definitely a matrix, might need to convert values\nif (mat.num_rows>1 && mat.num_cols>1)\n{\n// TODO once needed\n\n\ntmp failure with error?\n\u2014\nYou are receiving this because you modified the open/close state.\nReply to this email directly, view it on GitHub\nhttps://github.com/shogun-toolbox/shogun/pull/4261#pullrequestreview-113800053,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAqqvxg67QbfEWd79WOJMwXMlDkvZbeYks5tqRDngaJpZM4TYxtS\n.\n-- \nSent from my phone\n. travis error unrelated. merging. travis is ok, so the autoformat didnt moan. Great!\nThis all looks good to me.\nBefore I merge, did you run all the notebooks? They run without errors?. thanks! :). Thx!. Travis finally is green, windows was OK before. You will need to run this before and after the changes, see if there is a difference.\nIf there is, pls identify the change that causes the difference and point me to it. Then I can help. Cool then!. Thanks!. I have the feeling the examples are not even executed, you have a lot of compile errors in here:\nhttps://travis-ci.org/shogun-toolbox/shogun/jobs/377248647#L1394. Nice one!. Great clean up patch!. This is a cool push!\n. This is OK from my side.\nOk to merge everyone?. just restarted appveyor and then we can merge this. style checker is not happy: https://travis-ci.org/shogun-toolbox/shogun/jobs/382682896#L726. Ok to merge @geektoni ?. If nobody objects, Ill merge this @vigsterkr @vinx13 . What is the state here?. Can be merged once CI is green. You need to make sure the meta example compiles in cpp when you change things:\nhttps://travis-ci.org/shogun-toolbox/shogun/jobs/382677577#L3451\n\nThis can be solved via replacing the assertion for regression labels with a conversion that uses regression_labels (see my PRs where I did this for some other classes). #3732 #3537. Yep, that is to be expected. You will need to use the conversion methods in there for now.\nSee #4297 for example. I am confused :) But I will ignore this while it is closed. amazing! thanks for this push!. #4334 . @vinx13 this is for you. Note: this is for later, no need to start doing this now. Just wrote it down since I just had a look at the code. I like the mixin, and also there wont be any of the instantiation problems (like we have for trees) because Perceptron is actually not templated..\nFor now, we left iteration() without parameters (as of yet, labels and features are stored in CMachine and CLinearMachine anyways). Once we refactor things, the design patterns can be applied here.\nAsking for a review @vigsterkr @lisitsyn @iglesias . great thanks!. I am sorry, by accident seem to have squashed your data commit, which changed the hash: https://github.com/shogun-toolbox/shogun-data/commit/6932819b75e66bae9ae4649c25a56c5cbce1432f. now there is a conflict, you will need to forcefully update your data to HEAD locally and then generate the listing data again. Sorry about that. Looks good to merge now. I'll dare ;). Thanks for this!\nAs said a few times before, let's not have more kernel cookbooks (after this one) as we already have tons of them. Rather, we should have cookbooks for algorithms that haven't been described yet. I restarted windows since it was failing. Alright, great! thanks!. restarting .... I'll leave it to @vigsterkr to merge this though. Could you add a data file for the new example?\nOtherwise good to go!. LGTM.\n@vinx13 did you change any of the API here? Do we need to change anything for that?. No idea why windows fails, restarted.\nTravis is OK so all code seems good. Ah ok that is a feature branch then.\nSo we will merge this, and then refactor upon feature branch merging. Very useful stuff, thanks! :). Hi!\nThis is due to the licensing of SVMLight (we cannot change that). You will have to compile shogun yourself if you want to use svmlight.\nWhat is the error for GMM? There is a LAPACK dependency there. @dougalsutherland is lapack enabled in the conda build?. This is due to a LAPACK dependency. We got rid of this in the latest development version, and this will be in conda with the next release (i.e. you can use GMM without having LAPACK installed)\nThough still it seems weird that lapack is not active in conda, will check that. (also need to compile shogun using the GPL compatible submodules if you want to use SVMLIGHT. No this is going to develop IMO, and yes of course we need a different name then. @shubham808 can you help me parsing the output...Put_perceptron/8192/1 means what exactly?. I dont understand why the put seems to take proportionally longer for larger dimensions. Is there any memory copying on?\n@lisitsyn . I created a feature branch feature/observer_benchmarks, pls resend the PR against this. This seems to be ready? I'll read one more time. Can we have the results here as well?. The difference seems pretty marginal to me.. Let's merge and then we can put @geektoni stuff in.\nIf we need changes, we can still do that as it is a feature branch only. idk, imo we only need the functions to returns stuff. But you are the guru here :D. https://ci.appveyor.com/project/vigsterkr/shogun/build/2756#L27945. All fixed? :D. add the lazyness flag you lazy flagger. Merging, as this allows us to continue adding more examples, and the string_features factory can very easily be grep replaced later once there is an appropriate factory.. Need to use binary_labels casting. Could you create a pr where you don\u2019t apply the autoformatter, otherwise it is very hard to see what you changed. No white spaces changes in the first instance please. absolutely!. windwos build is timeout, travis has a java problem\n```\n 8/131 Test  #68: generated_java-binary-newton_support_vector_machine ...................................***Failed    0.30 sec\nlda must be >= MAX(K,1): lda=0 K=0ldb must be >= MAX(K,1): ldb=0 K=0Parameter 9 to routine cblas_dgemm was incorrect\n```. This needs a rebase\n. Btw whenever you make something linalg, pls ensure there is some test coverage of the changes lines, so that we at least know he results didn\u2019t change in the integration test examples. Needs a rebase as another data patch was merged ... Good to merge otherwise. I think you need to rebase your data submodule. ah no\ngithub lag. Ill close this one as we solved this using CRTP instead of macros. This task requires a bit of C++ experience. First have a look at the code and see if you can find things you don't like. Then we can discuss. yes, please go for it\n\ntry to use SIMD where possible (linalg)\navoid redundant code\n\ntemplates are powerful. and please no manual template spezialization for different types of the dot products. . This is a good weekly task regarding examples, @shubham808 @vinx13 @FaroukY . This is still relevant and a subset of #4463. @lisitsyn your stuff in action\n@shubham808 @FaroukY @vinx13 here is how you can now port examples that use method calls in machines that are not part of CMachine's interface: you register then as lazy methods and then can just use get.. sure :). CI is not happy though. let me know once CI is happy and I will merge. restarted.\nBut I think this is good. Merging. That is a useful cookbook. Eventually, all this kind of stuff should rather be done using an AST parser rather than ctags, e.g. http://clang.llvm.org/docs/LibASTMatchersTutorial.html\nbut for now, this PR re-adds the classes to the test so we dont have error slipping through. yessss.....\nmind giving these automated tests a go with this lib?. let me know if anything came out of this. would be so much better than using clang. Indeed,\nBut we could exclude it from travis, and just run those tests on the buildbot if needed, @vigsterkr what do you think?. yes :( mmmh dont know what to do from here...\n@vigsterkr ?. ok cool, so let's change the type to generic features and then merge this  and move on ! :). Ok cool\nPR CI builds are currently not active, so you will need to check all tests locally\nIf you give your ok we can merge (and then closely look at the CI and buildbot outputs)\nShall I? :). Ok pls have an eye on the buoldbot and the new azure builds. azure seems fine: https://dev.azure.com/shogunml/shogun/_build?definitionId=2. Cool!\nNow what was next? :). I have a local branch with an initial attempt on this, but not complete. Will share it. this needs a data update, obviously. Can be closed. Does all this work actually?. Cool, can you make sure to extensively test the examples with those two classes (memory, results, etc) Hard to overview otherwise . I think that the MKL solvers are not available on windows atm.\nBut you could try vcpkg or conda. I think that is a good idea, depending on where this statement will be.\nThe alternative in such special cases is to use put<SGMatrix<T>>(\"feature_matrix\", mat). couldnt we make some friend classes instead of adding the set_store_model features to the interface. This is not meant to be an interface method (definitely not for SWIG, but not even for cpp). integration tests pass on travis, unit tests all passed locally, merging. Let me know if the factory creating worked..... rebase and merge :). Can you rebase data and then we can merge this as well!. Shall we get this in soon? :). Ah I remember now.\nWell the real question is whether Pipeline should be part of the modular interfaces (i.e. the base types) or not. . Ok some more thought:\nyou get the above error because put is strict with types: no subclasses allowed.\nSince that won't change, there are the options to (braindump)\n\n\nhardcode a pipeline in put that checks and casts to CMachine (ugly and not scalable, but does it need to be?)\n\nmake the builder return a CMachine and whoever wants to extract the elements of the pipeline needs to do .as_pipeline(). E.g. @gf712 doing the work on openml would have to use get_name() rather than sg.Pipeline to check the type first and then use the as. BTW as is something that we discussed at the hackathon in budapest as something that is needed anyways, see here. As most users don't want to inspect the pipeline they just built but rather call train I think this is a reasonable compromise (although not C++ style). (Veto'd by @vigsterkr but also liked by @iglesias )\ntemplate crossvalidation and use type traits to restrict to CMachine and CPipeline and whatever other learner we have (transformers maybe). Then have a factory for each that instantiates the corresponding xvalidation instance and then accepts pipeline or machine. (unnecessary complex)\nhave a factory for cross validation instances that either accepts machine or pipelines. This would allow the set the machine member, but put still doesn't work for `CPipeline.. Cool! I\u2019ll check it out soon!. Great that this now works !\nShall we address the other comments and then finally merge this?. ~~Something about include paths is broken in the example ...~~ sorry. Cool!\nThanks for the big amount of work. It is awesome to have this in now :). @iglesias actually the bias in the previous version was still wrong, I double checked things. Now it should work.\nI will also add a way to compute this for D>N for which the current method will fail (covariance matrix doesnt have full rank anymore). Travis times out, but all tests pass locally, merging. * Yes, this is how to reproduce the error!\nYes, the dimensionality changing is the issue, i.e. w contains one more component (this bias)\n\nI thought about the solution, and it seems to solve the problem.\nCan I suggest we have a PR, along with a unit test that compares this exact solver and bias configuration with say the one interfaced by sklearn? On a minimal problem (we have environments set up for testing such things, with trainig/test data etc, see some other unit tests, or ask here)\n. Nice catch! :). Welcome! :)\nYes totally, this is a good way to get involved with our testing system!\nWe would want to test the operations for all types that they are supported with. Sometimes that is only floats (some solvers for example), sometimes it is more (add) You can see the definitions here, DEFINE_FOR_ALL_PTYPE and the like. It is quite a mess with Macros and all that in there, but basically every operation is explicitly defined for a number of types. I think the first step would be to define the test types in the unit test locally and refactor, then define them globally somehow so we can re-use things.. The best thing would be to globally define the types for googletest to match things like all shogun ptypes, all floating point ptypes, all numerical ptypes, etc. No idea. In fact, we have been moving towards getting rid of CMath:: functions recently, in favour of using std:: directly. So there should be no need for powl anymore.\n@vigsterkr do you know why pow is still there, we have removed most others\n@vinx13 maybe you know?. Thanks!. Need to ignore the stylechecker since class_list.py assumes that classes are defined in a single line, so cannot apply this: https://travis-ci.org/shogun-toolbox/shogun/jobs/410368055#L746. We have virtual float64_t CDotFeatures::dot=0, which is a quite cool design, as independently of any feature type, we can write code and algorithms that work with these dot product features.\nThe problem, however, is the return type. Think about linear regression and cov, and 32bit features. We could of course write an iterative cov implementation that fills a 32bit cov matrix using the above API and elementwise casting. We cannot, however, do a batch version without copying the matrix. That is the first issue. The second issue is that the API is not const, due to the on-the-fly features that are computed in a non const inplace way.\nSo maybe we can build a second version of the API (keeping the old intact), that serves a similar purpose, but that is both const and templated. Thoughts?. @vinx13 do you have any idea on this error: https://travis-ci.org/shogun-toolbox/shogun/jobs/410945599#L2476. The thing is I cannot reproduce this locally or on the buildbot. Similar error occur when gtest.h is not included as the first include in the unit tests. Aaaah it is probably the include order .... Can you try moving it around?. ah yes of course, this was since I had some of the implementations in the header for some short time.\n@iglesias it is scary indeed. I think this is due to the non-std template stuff we are doing in CDenseFeatures.....it is definitely not a guard. Same with gtest.h btw, has to be included first otherwise it blows.\n@vinx13 thanks for the tip, I have updated, lets see :). At least a bit better:\n```\n        template \n        void\n        dger(T alpha, const SGVector x, const SGVector y, SGMatrix& A)\n        {\n            REQUIRE(\n                A.num_rows == A.num_cols, \"Matrix A (%d x% d) is not square!\\n\",\n                A.num_rows, A.num_cols);\n            REQUIRE(\n                x.vlen == A.num_rows, \"Length of vector x (%d) doesn't match \"\n                                      \"matrix A (%d)\\n\",\n                x.vlen, A.num_rows);\n            REQUIRE(\n                y.vlen == A.num_rows, \"Length of vector y (%d) doesn't match \"\n                                      \"matrix A (%d)\\n\",\n                y.vlen, A.num_rows);\n        for (auto j = 0; j < A.num_cols; ++j)\n        {\n            for (auto i = 0; i < A.num_cols; ++i)\n                A(i,j) += x[i]*y[j];\n        }\n    }\n\n```. Actually nevermind @vinx13 I'll include that in my latest PR. Looks good, how does it work?. Please also update to only using 2 classes (and then try less data and also pls post the runtime of the meta example). Happy to merge after the filename change. Tada :). fixes #4385 and prepares ground for a linear regression with bias update. Eigen3 can also do this, but it only updates the lower or upper part of a matrix. We might want to think about introducing types link symmetric matrices where only half of the memory is effectively used and the rest can be used a temp storage by algorithms, like eigen does. @OXPHOS btw is there a reason why we don't define the methods in linalg using a type trait \ntemplate <typename T, typename T2 = typename std::enable_if_t<std::is_arithmetic<T>::value>>\ntemplate <typename T, typename T2 = typename std::enable_if_t<std::is_floating_point<T>::value>>\ntemplate <typename T, typename T2 = typename std::enable_if_t<std::is_integral<T>::value>>\netc\nCould this replace the macros to define things for certain ptypes?. wow congrats! :). as if you need details. This seems very reasonable to me.\nWould you like to send a PR with an update.\nIn addition, we should maybe test some results against another library on a very simple example?. @braceletboy this is unrelated to this issue though, so pls let's communicate in another issue, or even better, in a PR from your side. Yep, just confirming that we don't want to allow modification of features. You can of course get a pointer to the matrix and modify memory directly, but we don't want to encourage that via the API. Note that in KMeansBase, the function does something rather than just returning the cluster centres. In this case it is rather pointless as it just extracts the feature matrix of the lhs features.\nWhat we eventually want is a way to extract/compute the cluster centres from the put/get API. The classes KMeans and Hierarchical will eventually be hidden from the interfaces so no methods will be available.. Thanks!. the data update is fishy. something is still weird with the data. No I mean all those removed files. could you try again?. Shogun natively compiles under windows, see our appveyor config and build\nWhether Minigw works is unclear to me. @vigsterkr ?. As said, the project compiles  natively without having to use mingw and that is the recommended solution here. All the information you need for that is above. Using anaconda, there are also windows binaries available (only the c++ core, no interfaces as of now AFAIK, patch welcome) https://anaconda.org/conda-forge/shogun\n. Could this work for any stl collection?. Discussed this quite a bit with @vigsterkr and we think that we do need vector, list, map, set (at least). But set list and vector would be a great start if map is hard. https://travis-ci.org/shogun-toolbox/shogun/jobs/426069785#L799. Soooo. what is the latest here? :). Yes go for it. Can be extended later on but then we at least have vector. @besser82. I guess the PR should go against my fork's branch, not the master. ah you commented, nw :). @gf712 it would be good to update the examples to extract the right values in a sensible way. Mind doing that?. So was there a problem in the end that needs fixing?. I think we should find out where those lines initially came from and then either confirm that those files were deleted (which commit, why), or find out the new name and then replace the filenames. We had renamed a few examples a while ago and I have the feeling that those for part of that. . Thanks for sorting this out guys!!. shogun is also part of vcpkg\nhttps://github.com/Microsoft/vcpkg/blob/master/CHANGELOG.md. What\u2019s the errors?. uff this seems like a python version problem. Can you reproduce this error locally? I bet it is some upgrade to Python 3. the windows variant error for sure is compiler version\nBut the Python error, what about that? We should get at least travis green. Great, thanks for the ping and let's merge. @vinx13 would you min checking the variant compiler error on windows? At least googling the source a bit?. Hi Gil,\nI am quite busy today, but I will try to get back to this PR tomorrow latest. \nThe error when running with valgrind seems strange, this usually happens when there is an uninitialized read somewhere in the code (something which valgrind shows). For the pinv, it might be worth checking how it is implemented and then dig in the code and eigen3 docs to see whether the way it is done works with long (eigen might bail actually). If it is unfixable, we will have to remove the definition for long. Hi Gil,\nI am quite busy today, but I will try to get back to this PR tomorrow latest. \nThe error when running with valgrind seems strange, this usually happens when there is an uninitialized read somewhere in the code (something which valgrind shows). For the pinv, it might be worth checking how it is implemented and then dig in the code and eigen3 docs to see whether the way it is done works with long (eigen might bail actually). If it is unfixable, we will have to remove the definition for long. It might be worth filing this as a bug report with the eigen3 guys, they are quick in fixing bugs. What's missing for this to be finished?. We should follow c++ is_arithmetic. Then remove the types that eigen3 cannot deal with, or those where it just doesn't make any sense. > Yes, I agree, in another patch could move the definitions to a header file with definitions that can be accessed by all tests?\nyes\n\nThe only thing missing here would be the complex type... But might have to write special test cases for these no?\nYou might have to for some. For now we can leave that and maybe investigate it later (e.g. when someone needs it.\n\nOk then, I will wait for a few more days for others to comment and then merge this\n. Just restarted the build. The patch looks good and IMO we can merge it :). How can we make examples to show this kind of stuff to users?. This seems ok to me now. I will wait for another day or so in case somebody else reviews, otherwise it's good to merge. Thanks a ton for the patch! Maybe we can test all linear models in this style once this is merged .... Ill put some links soon. Yes that seems to be the fix. Great! Thanks for looking into it\nOn Mon, 22 Oct 2018 at 10:38, Gil notifications@github.com wrote:\n\nThere is currently an issue building in one of the Travis environments (\nexample https://travis-ci.org/shogun-toolbox/shogun/jobs/443619544#L2902).\nThe issue gives an error like this:\n[ 58%] Generating trained_model_serialization_test.h\nTraceback (most recent call last):\n  File \"/opt/shogun/tests/unit/base/trained_model_serialization_test.cc.py\", line 169, in \n    config_file = sys.argv[3]\nThis is because the examples/meta/tags file is not passed to\ntrained_model_serialization_test.cc.py, and there are only two out of the\nthree expected args.\nThe issue is that CTAGS_FILE is defined in\nshogun/examples/meta/CMakeLists.txt, which is added in this line:\nhttps://github.com/shogun-toolbox/shogun/blob/e338426fc5777386b30eb0adee9e63bf61076e70/CMakeLists.txt#L516-L518\nBut the trained_model_serialization_test.cc.py command is defined in\ntests/meta, which is added before setting CTAGS_FILE to examples/meta/tags\n:\nhttps://github.com/shogun-toolbox/shogun/blob/e338426fc5777386b30eb0adee9e63bf61076e70/CMakeLists.txt#L502-L503\nSwitching the order of execution of the two blocks where these lines are\ndefined in the project CMakeLists.txt fixes the bug (on my machine) and\ndoesn't seem to break anything else.\nI'll submit a PR if this makes sense!\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/shogun-toolbox/shogun/issues/4408, or mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAqqv1TQva-8dk2J1hptY9G36APLDf4Kks5unZIygaJpZM4XzDD4\n.\n-- \nSent from my phone\n. Travis is alive again! :). Seems good to me. I will merge so that we have CI for the other tests. Thanks a lot for this. Thanks for the report\nCould you post a stand-alone example to reproduce this, minimally if possible. Thanks. But yeah, this will be interface related probably, not c++ lib related. BTW this passed all of our CI\nhttps://dev.azure.com/shogunml/shogun/_build/results?buildId=91\n\nTry to keep an eye on the output of this after any patch is merged (until we have the pre-merge builds back). Sorry what do you mean? :). ah I see now.\nGPL should compile, could you check/make sure?\nremoving these might cause a lot of compile errors, we should eventually do that (as I said, the modelselection parameters should be removed as well), but maybe for now let's focus on one thing?. cool thanks!. Hi!\nThanks for getting involved here.\nThere currently is neither a LOOP or a WHILE construct in the meta language... the language is supposed to be very simple and only just expressive enough for API examples.\nWe might change this at some point, but it is unlikely for now. What we usually do is to load datasets from files in case you want to generate training data. Have a look at the other examples to find inspiration.\nLet me know if you need further help. This issue is best discussed on the mailing list, irc, or on stackoverflow.. I saw this error before with an outdated build folder. If you start form scratch, it still happens?. We would have to dig a bit to replicate this error. I remember seeing it on some of our builds, but I dont know how or whether it was resolved. I think @vigsterkr might know more. @HaoZeke can you confirm?. @avramidis awesome! :) . Thanks for the report. This should have been fixed a while ago so I dont understand why this appears. We dont have a test build for archlinux though\nDo you desperately need ARPREC? That is only used by a few modules within shogun .... Thanks for the offer, I think long term maintenance of such a build would be most useful. @vigsterkr I'll leave replying to you. \nAs for the example problem. You could try turning them off using -DBUILD_META_EXAMPLES=Off, or, if you just want to disable the cpp meta examples -DTRAVIS_DISABLE_META_CPP=Off\n. That is great news. Patches would be very welcome here!\nLet us know if you need any help. As usual, let's keep an eye on azure and buildbot. Thanks!. Not sure what this error now means....\nBut thanks for the patch !. Ah ok so @vigsterkr already moved on.\nThis will be visible in the new MS build pipeline.\n@vigsterkr shall we merge this ?. You are right we dont really need any output here, as nothing really happens. THese outputs are for making sure that algorithms dont all of a sudden output different results.. Our CI (before PR) is down atm. Can you confirm this example runs smooth locally (say c++, python, and maybe java)?. Shall we update and merge this one?. Sure of course.\n. @avramidis we cannot really accept arguments in the watched functions. At least not with the current design, and this was a quite conscious decision. The way around this is to communicate with the classes via put get (to set parameters) and then call helper methods.\nIf that doesnt help, we might want to think about putting methods into the base interface.\nWhy does max_string_length need parameters?. Actually that is a method that we do not want to expose to SWIG, so rather just hide it.\nWe want feature objects to be immutable eventually, so changing is not allowed, rather construct a new one.\nBut in cases where we want things like this, it would be best added to the base class.. I think that would be quite hard as then we would need multiple get methods with different number of arguments as well or?. I also think our new API design doesn't need that (as far as I can see yet). I mean sure it is possible to offer this, exactly using those flexible templates. But it obscures the get API, which quite a cost for what is gained. IF we need such a thing (calling methods by a string, with multiple parameters), then we should add some other function that get.... something like run. But until we don't have to do that, I would be careful getting involved with such things.\nBTW the new API is all moving around. We are trying to figure out what is best still.\n. Any news here on this?. ah ok. Yeah just push stuff. It is easiest to discuss it here. Might save you some time if devs point out stuff. Shogun is a beast ;). it seems weird that this error shows up. Are you sure you are running the same shogun version as the source shows? You could find out via printing all parameter names using f.parameter_names()...if it is not there, then your change hasnt made it into the lib yet. Ah yes... I know what is wrong\ncome on irc to discuss more.\n. I reccommend you use pdb/gdb to check what the code is doing. I bet it goes here when you call f.get\nhttps://github.com/shogun-toolbox/shogun/pull/4441/files#diff-1029ddee9603f4c1a9a97d5ca5519f35R478\nSo there we see that get_string_list is not part of the list. That is actually because it doesnt exist. So we need to add it. This happens around here:\nhttps://github.com/shogun-toolbox/shogun/blob/develop/src/interfaces/swig/shogun.i#L233. Cool that it works for Python!! :). java error here\n2019-01-19T12:45:00.0405452Z /build/examples/meta/java/features/string_char.java:29: error: cannot find symbol\n2019-01-19T12:45:00.0405578Z DoubleMatrix features = f.get_char_string_list(\"get_features\");\napart from the mentioned DoubleMatrix CharVector issue, I don't understand why the java interface doesn't know the get_char_string_list method, as you instantiated those in swig (otherwise they wouldn't work in python)\nIn fact csharp has the very same problem, method is not defined see here\n2019-01-19T12:44:18.8254589Z string_char.cs(16,21): error CS1061: Type `Features' does not contain a definition for `get_char_string_list' and no extension method `get_char_string_list' of type `Features' could be found. Are you missing an assembly reference?\nAnd so does octave, although here it is a runtime error as obviously nothing is compiled, see here\n2019-01-19T12:46:14.2967680Z error: member not found\nSo all of those errors are related, we need to figure out why swig doesn't generate an interface for those langs while it does for python. NOTE: the r examples are disabled (static calls dont work), as are the ruby ones (overloading doesnt work). Maybe something trivial such as you added the methods inside SWIG_PYTHON guards? @gf712 you touched that recently, could you check.\n. Can you pastebin or gist the generated code? . It should be straight forward to change this btw, I\u2019ll ping @sorig tomorrow and describe the problem. @sorig we basically need to add a blacklist of types to avoid in the includes, something like\n\"Dependencies\": {\n        \"IncludeAllClasses\": true, \n        ....\n        \"SkipImport\": [\"StringCharList\"]. @avramidis curious, what happens if you replace in java.json the entry \"IncludeAllClasses\": true, with \"IncludeInterfacedClasses\": true,?\nBTW see translate.py for how the json is turned into code. what I mean explicitly setting it to False and adding the entry I pasted (interfaced classes) and setting it to true, did you try exactly that?. can you paste the code listing that is generated if you do that? Because I think the reason we have the interfaced classes entry is exactly this one you ran into here, but it was for another language ... . ah yes of course, thanks!. it is pretty easy to add a blacklist to the logic. You would check if the dictionary contains the blacklist key \"ExcludeImport\", and then assume you get a list of imports to exclude....\nwhen all the unions are taken in translate.py, you can then remove any blacklisted imports in there\nhttps://github.com/shogun-toolbox/shogun/blob/develop/examples/meta/generator/translate.py#L335. ```\nif \"ExcludeImports\" in self.targetDict:\n            dependencies.remove(self.targetDict[\"ExcludeImports\"]) # something in these lines, after all union. Hehehe, you will be surprised ;). Cool! Can you push so we can see what you did in the end?. For octave can you paste the generated code?. Cool, just checked the changes, all looks good. I will wait for the CI :). @avramidis octave failed. Could you pls post the listing of the char features example?. Error is explained here. And here we go: https://github.com/shogun-toolbox/shogun/blob/develop/src/interfaces/octave/swig_typemaps.i#L369. Have a look at the incoming typemap....looks like we need to write an octave string list typemap to make this work. It is actually quite good that we spotted that as it means one was never ever able to execute string based algorithms from octave ... :/\nLet me know if you need help with writing the typemap.. Very nice work! This now allows to port quite a few more examples. Ok cool, let me check this\n. Yes I agree!\nThis turned out to be a bit more effort than I first thought! ;)\nLet\u2019s keep your original function then and feel free to take a stab at the\nother or you mentioned!\nThanks for the nice discussion!\nOn Wed, 5 Dec 2018 at 08:18, Saatvik Shah notifications@github.com wrote:\n\n@saatvikshah1994 commented on this pull request.\nIn src/shogun/lib/SGMatrix.cpp\nhttps://github.com/shogun-toolbox/shogun/pull/4424#discussion_r238952255\n:\n\n@@ -227,6 +227,22 @@ void SGMatrix::zero()\n  set_const(static_cast(0));\n }\n\n+template \n+void SGMatrix::random(T min_value, T max_value)\n+{\n+ for (index_t idx = 0; idx < num_rows * num_cols; ++idx)\n@karlnapf https://github.com/karlnapf thanks this makes sense.\nI had one suggestion though - I feel that the entire random() and\nrandom_normal() impl should go in its own separate (issue + PR combo).\nReason being that (1) the implementation is less related to the immediate\nissue of minimizing the test redundancies and (2) the changes should\nideally be propagated to SGVector as well - overall then a significant\nportion of this PR would mix up two disparate issues. For this PR, I could\njust use the original NeuralLayerFixture::create_rand_matrix which would\nthen be replaced in the second PR.\nI'm up for whichever way you lean towards.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/shogun-toolbox/shogun/pull/4424#discussion_r238952255,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAqqvwNqOsXrptGYMdrmc9zITqg-OrTkks5u13M6gaJpZM4YyBmq\n.\n-- \nSent from my phone\n. > Once this is merged, I'll need to sift through the remaining Neural*Layer tests and apply similar changes.\n\ngreat!. Thanks for this.\nPlease keep an eye in the [CI}(https://dev.azure.com/shogunml/shogun/_build?definitionId=2) and the buildbot. If anything breaks that wasn't broken before, we should fix it first thing.\n. Yep CI is indeed fine. Buildbot, just check the builder list for cases that were green until we merged this PR (last build). They are all fine. The broken builds need fixing, most of them at least, with the recently broken ones having a higher priority. It is usually a tiny bit of work to figure out what causes errors, but we usually have logs to see which commit broke them.. This looks good.\nI am just confused, was there an integration testing file before? Usually we only have those if there is an example. \nWhat was the error when the k was in there?. for reference https://github.com/shogun-toolbox/shogun-data/pull/170. ok cool then good catch!. I'll merge, let's keep an eye on the CI output (as pre-merge CI is down atm). https://dev.azure.com/shogunml/shogun/_build/results?buildId=92. yes all interfaces passed. Thanks for the patch!\nBTW I thought again about those int enums. I think it slightly sucks as it is not typesafe. \nIt might be worth looking into this at some point, but it is slightly messy\nhttp://www.swig.org/Doc3.0/Java.html#Java_typesafe_enums\nhttp://www.swig.org/Doc3.0/CSharp.html#CSharp\netc\nI think before potentially doing anything here, one would try to fool put via placing an illegal int value that is fed into an enum, and then check. Technically, if we had proper handling of default cases when switching over the enums, all should be good (apart from potentially cryptic error messages). However, knowing the codebase, I am quite sure it will crash in some easy to spot situation ;)\nHaving said all that, I think it might be best to come back to this at a later point.\n. BTW here is one thing that is related to examples/API that requires ongoing efforts (not sure there is an issue or not).\nWe eventually want to get rid of all specialized types in all examples and notebooks. We in fact did this for CDesneFeatures aka (SWIG) RealFeatures. The factory/put/get API allows to create and modify objects so that neither constructors nor special public methods of such specialized classes are needed.\nSo you could pick any notebook/example where you find a specialized type, and then refactor the code to use the new put/get API. This will reveal more problems that then can be fixed on the fly.\nOnce all test code (apart from c++ unittests) is free of the use of the class, we can then remove it from SWIG (the .i definition files in src/interfaces/). This speeds up the compile time quite drastically.\nIt can be done one after the other\n\nremove all uses of special type in gpl/examples/notebooks\ngrep for the classname and all aliases and potentially goto 1\nRemove classname from swig definition\nEnsure all tests/builds pass\n\nAlternatively, one could also start by removing a class from the swig definition, and then use the errors (the type will be unknown to the say Python interface) to find which examples/notebooks fail.. @lisitsyn check this out. > Expected output:\n\nIt's a int\nValue 2\n[ERROR] In file /Users/ghoben/anaconda3/include/shogun/lib/type_case.h line 161: Type i is not part of the given typemap\n\nJust looking at the output. The error message is a bit cryptic. It should state \"Type int32_t is not part of \" or something in those lines\n. > Without much more code I can get the message Type int is not part of <long double, double, float>\n\nTo get \"Type int32_t is not part of \" we would have to map the demangled strings to those strings, i.e. {\"long double\": \"float128_t\"}\n\nThat is fine actually, no neep to map. > Getting this error from CI for the lint job:\n\n2018-12-04T13:38:11.4740116Z ##[error]/usr/bin/docker: invalid reference format.\n2018-12-04T13:38:11.4753526Z ##[error]See '/usr/bin/docker run --help'.\n2018-12-04T13:38:11.4754489Z ##[error]/usr/bin/docker failed with return code: 125\nIs that an issue with the script or is the format not correct in my code?\n\nI think @vigsterkr is working on CI stuff atm, so it might behave weirdly .... > Things seem to be working as expected, but the compiler generates so many warnings (all expected) because it looks at the function implementation of each type, due to the recursive nature of the check implementation. What is nice is that the way the static assert is written guarantees that the error doesn't get buried in warnings.\nis there a way to silence the warnings?. > > is there a way to silence the warnings?\n\nThere are two ways, either with compiler flags or add some pragmas (https://stackoverflow.com/a/7159392)\n\nok cool, might be worth looking into the pragma (later); we cannot change the flags. The NONE is not really necessary, as it is all the same: positional arguments.\nCheck what we did in put here:\nWe define a different put for objects and primitives / vectors, and even more involved for certain swig cases such as scalars, double matrices 1 double matrices 2 etc. The compiler then figures out which call to make, it is not c++17, pretty simple in fact. The internals are not that pretty, but the API is: just a put. I wonder whether this is possible with the lambda signature as well so that there is only one sg_any_dispatch which is overloaded for the different situations. I am by no means sure this goes, but it might be worth trying\n . Yes good idea to have the 3 lambda version be there in case we want to revert to it.\nWhy not keep it for now, and if you start another approach we can have another PR for that?\n. Yes, maybe open another PR with it? We can keep this one as is for now. Pretty happy here as well. This is definitely merge-able from my side. Clean API as well.\nLet's see whether the lambda type extraction gets you somewhere and if not, continue .... We can always change this later as the touch points with the rest of the code are super minimal and are actually easy refactors .... We should also move on from this soon actually :) Soon the exciting stuff begins. what about is_sg_primitive ? :D. No but honestly, I think defining such thins explicitly might be best actually. Yes, exactly. Just to have those types centrally defined so we can use them in other templated methods as well. Thanks!. I somehow thought I merged this already? Shall I, or was something still missing?. Ok shall I then? Just had another look and this is definitely something to use for a start\n. Ok let\u2019s watch ci. There might be compiler issues on some platforms with this modern stuff. Awesome!. That might be side effects from the changes. Keep in mind all tests are in one single executable and share memory so introducing memory errors in a test can make others fail, or adding a test could reveal existing memory problems due to layout changes. Merging, let's keep an eye on the CI. Definitely :)\nI thought we had updated the file ...\nWanna go ahead and clean this up a bit?. Really nice write-up!\nOne comment, 3. we can definitely change the api here ... towards a more standard random and random_normal, or even better: rand(), and randn()\n. That should be fine imo as the boundaries are a set of zero measure i.e. P(x=1)=0 almost surely. bringing the io in is awekward indeed. I think this issue is just for printing though, so despite the fact that we usually try to stick to the STL recently, I think you can do an explicit loop here. Or you just bring the io into the scope.\nBTW why do you want to print? This should not be necessary as most user API should just return strings (which is supported by swig). Just drop this. We can have the string returning method with a filter and potentially some ready packaged cases for things like hyperparamters. I think it is best if the caller prints for himself. Think IPython/swig where this happens anyways, and in C++ the caller can then choose where and how to print. Could even use sg_io but the point is that there is choice and we don't need to manage this so less code. In [3]: sg.GaussianKernel().parameter_names()\nOut[3]: \n('cache_size',\n 'combined_kernel_weight',\n 'lhs',\n 'lhs_equals_rhs',\n 'log_width',\n 'm_distance',\n 'm_precomputed_distance',\n 'normalizer',\n 'num_lhs',\n 'num_rhs',\n 'opt_type',\n 'optimization_initialized',\n 'properties',\n 'rhs'). Yes. I guess it would be the above, and then\nIn [3]: sg.GaussianKernel().help(\"log_width\")\nOut[3]: Sorry no description entered. Or Pythonic: [myobj.help(param) for param in myobj.hyperparameters()]. I think the mentioned method can be dropped.\nAs for the filter, I would just use the existing parameter_names method (or whatever the name is), and give it a filter parameter which has a default value. \nThen we can have special API sugar for things like hyper-parameters, things that might be needed frequently by users\nSince we are changing APIs all the time anyways atm (need to do a new release soon), we can definitely try to make this as clean as possible via dropping old things and chose appropriate names and patterns. I would drop print, see other thread.\nYeah just parameter_names ... and maybe hyperparameter_names?. Actually I would name them\n\nparameters(bitmask filter with default returning everything)\nmodel_parameters()\nhyper_parameters(). \"names\" is redundant when we return a string list\n. Drop :)\n\nOn Thu, 13 Dec 2018 at 13:25, Gil notifications@github.com wrote:\n\nAh ok! So drop the get_modelsel_names and have a filter-able\nparameter_names? What about the print_*, that is still useful right and\nshould be refactored no?\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/shogun-toolbox/shogun/pull/4432#issuecomment-446950692,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAqqv0aQIdcSuFJ6yO4EyylwdZEbswbSks5u4kdBgaJpZM4ZRc19\n.\n-- \nSent from my phone\n. I think there might be a problem as properties then might contradict, i.e. NONE and something else is set.\nThose summarising property states are better computed on the fly imo. We could maybe offer a method for this?\n. Sorry :)\nWhat I meant was: NONE is a summarising property which can be inferred from knowing all the others. And I think this is why we shouldn\u2019t encode this in a bit but rather compute it on the fly. I realise now that indeed this does not make any sense. But making NONE nonzero also doesnt make sense (due to semantical conflicts when NONE and other flags are set).\nI think the use case where a user wants parameters with no properties set OR some others set is not very realistic. Why not just drop the NONE from the API?. LGTM ... merge? :D. Thanks a lot for reporting!\nPatches welcome as this should be fixed before a release. Thanks for reporting!\nPerl is super experimental, but we would love to see this fixed. It is not part of our CI for some reasons, but it would be nice to put some work in that direction. If you are keen, I can give some pointers on how to start this.. As discussed, I think this is possible with put get, and potentially registering a lazy executed function with get, as done here and the corresponding example. @iglesias maybe you also have comments. Will merge this tomorrow if no more input from @lisitsyn @vigsterkr thx!. > @karlnapf Im currently blocked on the benchmark by #4452 .\n\nnoted! Looks like some sanitizer updates necessary....nice ! :). Really like the results table! Well well done! So useful and a great example of how we can make things faster with some clever thinking about low hanging fruits.. > > > @karlnapf Im currently blocked on the benchmark by #4452 .\n\n\nnoted! Looks like some sanitizer updates necessary....nice ! :)\n\nActually, I was blocked on Google benchmark not building on Mac/Ubuntu. This has been resolved and hence the analysis table above. The sanitizer problem is a bit tangential and Ive worked around it by using a VM/ @gf712 's docker image. I think this PR is ready to go unless there are some pending comments?\n\nYes well there are @vigsterkr comments about the seed. Was there any discussion regarding that? I am not updated to the latest I guess. Hi!\nWhile the concepts PR is great to get going on this new pattern, I still would like to get this one here merged soon. There is the issue with the seed. Any comments on this anyone?. Ok let's merge this then :) I will wait one more day and then do it if nobody else complains @lisitsyn @vigsterkr \nI think cool next steps would be:\n actually replace looped rng usage in shogun with this new API\n or write an entrance task for GSoC students to do this\nEDIT: actually reliased the randn is not yet included so I guess there will be little usage in loops as it is now ... We can do this once the normal random is in. I guess the effort is already done? So this is still better than what we have atm. We could stop pushing this forward then of course. But I think can still go in as an example of the overload dispatching?. Alright, agreed actually. @saatvikshah1994 maybe have a look at the feature branch to get a feeling for what @vigsterkr is talking about. @saatvikshah1994 thanks for the ping. The point beside the sees is that the whole design of the random class in shogun is flawed. This is why Viktor mentioned to not put any more effort into it. That said, we could of course still use your updates until we do a change, but the second point is that the uniform random number generator is not really used within shogun. This doesn't mean the PR is not useful. Quite the opposite is true, it is a really nice demonstration on how to gain speedups with vectorization as well as using better design patterns, so it is very useful for future reference. As said, for now, check the feature branch on how we can improve the random number generation in shogun in general. We should get the same ideas that you developed you in this PR into the feature branch and then put the efforts into pushing this forward rather than tuning deprecated designs.. As usual, let\u2019s watch out for the ci abd buildbot. Many thanks for reporting. Can you track down the exact change that started causing this?. again, if we want to find out what is going on here, it might be best to find out what commit (or even better line changed) started causing those issues ... whether they are real issues, and what the best solution would be. very interesting! This is a great investigation, appreciated.\n@gf712 is away for a week and a bit so it might take a while to fix it. But I will have a look at the commit and check/think a bit. Merging, since CI is happy. Let's nevertheless monitor the buildbot. I actually have a request for something that I think might be cool to have. The final goal would be to actually allow\nkernel = sg.kernel(\"GaussianKernel\", log_width=1)\nCurrently, we do that in the meta examples, but then translate it to \nkernel = sg.kernel(\"GaussianKernel\")\nkernel.put(\"log_width\" = 1)\nwhich becomes clumsy for multiple parameters.\nIf we could add to SWIG\ndef _internal_factory(self, name, **kwargs):\n    \"\"\"\n    A generic factory that accept kwargs and passes them to shogun via .put\n    \"\"\"\n    obj = getattr(shogun, name)() # \"shogun\" here is the module\n    for k,v in kwargs.items():\n        obj.put(k, v)\n        return obj\nand then monkey patch the factory methods, (here kernel) that would be much neater. I.e. have a single factory method in Python that passes things on to shogun and calls the factory method with the appropriate name.\nNow this doesnt work obviously\n_swig_monkey_patch(SGObject, \"kernel\", _internal_factory)\nas kernel is a global function. But I feel this could be easily modified to achieve what I described above? This would further simplify the python interface. Yes, we could use something like https://github.com/lisitsyn/stichwort later on, but for python since we wanna leverage the **kwargs we would need some python code. The key here is that we then only offer a single factory that then calls the other ones by the string and getattr in order to avoid having to add code for every single factory. I can do this, but I have little idea how to modify the monkey business to override global functions. @gf712 as this is API related, could you update the appropriate section in the NEWS file? (with a PR). Something like: added single get with variable return types for accessing fields in Python. Yeah let's just drop the whole print function and instead return an std::vector of strings. I think @gf712 you have prepared something for that somewhere right?. Lol .... remember you might maybe also use docker to get the same environment...did that a few times in the past to reproduce such errors. this has to do with your local setup not liking https connections\nyou could download the archive yourself and place it at the mentioned folder to skip the step. Nothing seems to be wrong on first sight. But you would need to post a fully stand-alone example that we can run to reproduce the error (use toy data). I am closing this due to a lack of a proper error report. Our CI indicates that all examples work. Feel free to re-open with a minimal, standalone example where you clearly highlight code, error messages, etc. Thanks. This looks fine. In order to help you, we would need a fully stand-alone example that isolates the error. Same as for #4446 . Great thanks. Let\u2019s see how this one goes once merged. sigh ..... Ah the commit does it? :D. Thx for this. Seems like the CI is working pre merge now. I will merge this anyways as only a minor change.. Would be cool to start playing with this, at least to develop some intuition for what we can do here.\nAs this didnt make it into C++17, we could maybe (as per suggestion by @gf712 ) use this  google lib that implements some 17/20 features in a c+11 compatible way .... ?. sure thing, looking forward to a patch! :)\nGet involved in IRC if you need help and make sure to read our getting involved guide. See discussion at the bottom of #4441 . @sorig Check it out we now have shogun API for my_kernel = sg.kernel(\"GaussianKernel\", log_width=1.0). One thing that would be really awesome was if the meta language examples would make use of this. Currently, it is not possible to modify say python.json in order to produce the correct output. E.g.\n\"KeywordArguments\": {\n            \"List\": \", $elements\",\n            \"Element\": \"$keyword = $expr\",\n            \"Separator\": \", \",\n            \"InitialSeperatorWhenArgs>0\": false\n        },\nproduces things like\nsvm = machine(\"LibSVM\"), C1 = 1.1, kernel = k\nAny change you could also have a quick hack on that? \nObviously, for the other langs, we don't want to change things (i.e. still go with the put cascades for now).. I think this is a simple matter of changing the generator a bit so that it can generate the python style kwargs .... let's see what esben makes of it :). Cool stuff, I should have thought that you did that :) thanks!. An example for #4463 . No it is because I changed the bandwidth of kernel (had to since its\nregistered in log\nDomain)\nI\u2019ll sort that\nOn Mon, 14 Jan 2019 at 16:07, Gil notifications@github.com wrote:\n\nI am not sure why it is failing... It seems like a error tolerance issue?\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/shogun-toolbox/shogun/pull/4462#issuecomment-454036734,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAqqvy9mJGe_DDgFv3cbD4ep47l2KPhsks5vDJ05gaJpZM4Z9Hrr\n.\n-- \nSent from my phone\n. Sigh ... the Gaussian kernel is pretty much the worst class to replace .... SVMs might be a similar amount of work ... will see. RIP sg.GaussianKernel, long live sg.kernel. @avramidis . #4462 is an example that requires some more work as the gaussian kernel is used all over shogun's history, other classes (such as machines) should be much smaller diffs\n\nThe main thing to keep the build intact is\n meta eamples\n undocumented python examples\n* ipython notebooks\nI have ignored for now (as not part of the build)\n applications\n graphical python examples. > I'll start working a bit on this. Btw what exactly are the undocumented python examples? I can start translating them to .sg files?\nexamples/undocumented/python\nEither delete (if it doesnt show something useful, some example were tests), or translate (which might need some c++ adjustments here and there. > How do you handle the GPL only classes? Is there a way to hide .sg files from the compiler/test with GPL only implementations?\nCheck cmake/FindMetaExamples.cmake, where you can blacklist things based on their dependencies. > Also need to come up with a way to get to the solvers when LibLinear.h is removed from Classifier.i\n\nshogun/src/shogun/classifier/svm/LibLinear.h\nLines 21 to 38 in f8a120b\nenum LIBLINEAR_SOLVER_TYPE \n { \n  /// L2 regularized linear logistic regression \n  L2R_LR, \n  /// L2 regularized SVM with L2-loss using dual coordinate descent \n  L2R_L2LOSS_SVC_DUAL, \n  /// L2 regularized SVM with L2-loss using newton in the primal \n  L2R_L2LOSS_SVC, \n  /// L2 regularized linear SVM with L1-loss using dual coordinate descent \n  // (default since this is the standard SVM) \n  L2R_L1LOSS_SVC_DUAL, \n  /// L1 regularized SVM with L2-loss using dual coordinate descent \n  L1R_L2LOSS_SVC, \n  /// L1 regularized logistic regression \n  L1R_LR, \n  /// L2 regularized linear logistic regression via dual \n  L2R_LR_DUAL \n };\n\nThat is a good point .... will need to continue to expose those to swig with the current design (could change later). Might need to think about this .... That is true\nWe still want to port them all (at least every class), keep in mind the\nexamples also serve as integration tests (if you add re data file, see\nreadme) so wenincrease coverage a lot (this has been helpful when\nrefactoring classes that have no unit tests.\nWhat you are saying is true though for the cookbooks: we don\u2019t need one for\nevery svm if they all have the same api. Just one well written that\nrepresents all does it here.\nOn Mon, 14 Jan 2019 at 17:36, Gil notifications@github.com wrote:\n\nEither delete (if it doesnt show something useful, some example were\ntests), or translate (which might need some c++ adjustments here and there\nI find that a lot undocumented examples are just slight variations of one\nanother.. for example the svms are very similar in terms of API\n\u2014\nYou are receiving this because you were assigned.\nReply to this email directly, view it on GitHub\nhttps://github.com/shogun-toolbox/shogun/issues/4463#issuecomment-454070707,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAqqv1zJaGSQRcO2xpMlNATwxsuGyYiiks5vDLH5gaJpZM4Z9Hyv\n.\n-- \nSent from my phone\n. @sorig let me know if you can do that, otherwise Ill add it as entrance task. Pls ask in irc or write a proper question on stackoverflow. Closing as not really an issue.. It is actually a really good idea to have a PR open with this. We can then all populate this over time...Let's keep it open and add things, usually it takes a few days to make the list .... Fixed a memory copying issue in SGVector that affected the correctness of a recent refactor of LinearRidgeRegression [Kirill Kolkir]. Yes, thanks a lot for addressing this so quickly. Checking now. As the CI passed, this means that the kwargs example passed ... ah no but there are no kernel instantiations in there. Did you try the behaviour a bit? If so, we can merge straight away and I can use it in #4462 . On my system (and on donbot):\n```\nTraceback (most recent call last):\n  File \"/home/heiko/git/shogun/examples/meta/generator/generate.py\", line 158, in \n    generatedFilesOutputDir=args.parser_files_dir)\n  File \"/home/heiko/git/shogun/examples/meta/generator/generate.py\", line 74, in translateExamples\n    ast = parse(file.read(), filePath, generatedFilesOutputDir)\n  File \"/home/heiko/git/shogun/examples/meta/generator/parse.py\", line 337, in parse\n    return parser.parse(programString, filePath)\n  File \"/home/heiko/git/shogun/examples/meta/generator/parse.py\", line 39, in parse\n    program = self.parser.parse(programString)\n  File \"/usr/lib/python2.7/dist-packages/ply/yacc.py\", line 331, in parse\n    return self.parseopt_notrack(input, lexer, debug, tracking, tokenfunc)\n  File \"/usr/lib/python2.7/dist-packages/ply/yacc.py\", line 1061, in parseopt_notrack\n    lookahead = get_token()     # Get the next token\n  File \"/usr/lib/python2.7/dist-packages/ply/lex.py\", line 393, in token\n    newtok = self.lexerrorf(tok)\n  File \"/home/heiko/git/shogun/examples/meta/generator/parse.py\", line 142, in t_error\n    raise TypeError(\"Failed to tokenize input. Unknown text on line %d '%s'\" % (t.lineno, t.value,))\nTypeError: Failed to tokenize input. Unknown text on line 8 '-1\n\nfloat is 32 bit and value has 'f' at the end.\nfloat float32_literal = 3.0f\nfloat float32_negative_literal = -10.0f\nreal is 64 bit.\nreal float64_literal = 3.0\nreal float64_negative_literal = -1.0\n``. Ah nevermind it was a caching issue (deleted build dir solved it, make clean didnt however). Not sure what python version the ci runs? (Mind checking?)\nOtherwise we merge and let the buildbot check it\nMerge?. No idea. Maybe it takes a moment. I managed to trigger the CI but this also triggered more changes requests ;). I know this sucks, but sadly no ... libshogun is only for C++ API, so we don't want to add anything there that is required from interfaces. Imagine you don't wanna use the interfaces but only C++, then these things obscure the API and type strings are not needed ever.. I had these intentions before as well, but this results in some really weird consequences about the codebase being different when compiled with or without interfaces .... It is definitely impossible to accessSelf?. Mmmmh maybe I need to rethink. My arguments actually also hold forparameter_namesright?\nI think, maybe add it (as a map, returned byparameters()` and we see how to deal with it?\nAlternatively, we could add some internal API to grant us access indirectly, e.g. via exposing a getter for the parameter properties or the Any or similar? Something that might be useful in general for the c++ API, but can also be used to access the type string?. Agree big time, \"parameter_names\" should be in SGBase.i.\nSo a getter for the map of the any's would be best c++ wise.\nI think then we can merge this thing here and have another PR that addresses the issues and also changes the error msg introduced here?. Let me know if you're ok with it and then Ill merge this one. Just had another look, definitely mergable.....so Ill do it now :). I actually do wonder how whether the parameter names method is used from\nwithin shogun. Might be worth checking this actually, and if so whether it\nis necessary. I have some vague memories\nIf it stays in, then we can only put the type string into swig\nOn Sun, 20 Jan 2019 at 18:43, Gil notifications@github.com wrote:\n\nyup, sounds good, I can work on a c++ AnyParameter getter and fix the\nerror message!\n\u2014\nYou are receiving this because you modified the open/close state.\nReply to this email directly, view it on GitHub\nhttps://github.com/shogun-toolbox/shogun/pull/4470#issuecomment-455886405,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAqqvz7pSTGhU7vrT98g9cV8q-CwwXQKks5vFKq0gaJpZM4Z-ecp\n.\n-- \nSent from my phone\n. Good idea! Readonly. Ok let\u2019s move the names into swig as well theb. We want something like this to work.\nk = kernel(\"GaussianKernel\", log_width=2)\nk.get(\"log_width\") # returns 2\nk.random_content() # randomizes all parameters\nk.get(\"log_width\") # returns a the randomizes log width\nk.random_content(\"log_width\") # just randomizes log_width\n\nIt requires some diving deep into the parameter framework, so maybe not the best first issue if you are not familiar with C++\nEDIT: names of the method to be decided and obviously this only needs to be available from C++, not Python (it is just easier to write what I mean this way). Note: this should not be implemented in a standard c++ shogun source, but in the swig extension. E.g. see https://github.com/shogun-toolbox/shogun/blob/develop/src/interfaces/swig/shogun.i\nEDIT: Or, since string distances are actually kinda useful, this can be added to shogun itself in the same way as CHammingWordDistance. Then shogun would use itself for generating error messages :sunglasses: . Completely overriding the default behaviour is tricky indeed.\nIn the perfect world we would have sg.kernel(\"G<TAB> only suggest from the kernel names. This would require some kind of nested namespacing though. But OK, let's maybe start with all shogun objects for now. There are other way to achieve this, e.g. via defining (string) constants with the sg module\nsg.kernel(sg.Gaus<TAB>\n@lisitsyn you had some ideas on this recently ...any thoughts?\nI think in general we need to play around to see whether we can arrive at something elegant @gf712 @AnirudhDagar \nGreat initiative this is though! :). Well anything that works nicely is more than nothing :). Me too. I am all for adding this and then improve it, go with the flow. Python changes all the time anyways and some smart kid will come along with a better solution in the future I am sure ... :). Reported as abuse with github. Awesome initiative!\nI will check this out a bit more soon, first need to read up on Concepts I guess ;). Reported as spam. > It seems like when SG_ERROR is called in CSGObject it tries to access self->map[BaseTag(name)], which creates a new entry in self->map with key BaseTag(name) if it doesn't exist. I am guessing this was not intended?\nLOL nope. Nice catch!. I'll wait for CI and then merge. Are these copy ctors actually used anywhere? Because if not I would almost rather remove them (if everything continues to work). where exactly?. Because if it was buggy then how did the code behave? why didnt tests fail? etc :). uh... quite essential interfacing code ... amazing\nSo yeah let's keep this fix :). Good to be merge from my side. Distance d = distance(\"EuclideanDistance\", lhs=features_train, rhs=features_train)\nbecomes (in cpp)\nauto d = wrap(distance(\"EuclideanDistance\"));\nd->put(Tag<decltype(features_train)>(\"lhs\"), features_train);\nd->put(Tag<decltype(features_train)>(\"rhs\"), features_train);. @sorig it would be cool if I could do the same thing for the \"MethodCall\" secion in cpp.json. But currently all arguments are jointly hidden in $arguments so I cannot extract the name/value pair that a put call would get.. @sorig in addition @lisitsyn would like to move all Tag occurrences to the beginning of the script :scream: \nlisitsyn 11:50:39\nHeikoS: I believe the next step is to move all the Tag<\u2026> to the top of the script\n@HeikoS 11:50:57\nThat will be tricky\nalthough\nmaybe not\nlisitsyn 11:51:12\nHeikoS: some postprocessing\nHeikoS: perfect would be to declare a namespace in the beginning\nnamespace tags { \u2026 }\nand then put them there. @lisitsyn actually I realise this does not make any sense as the normal put call is already templated and the template that matches the argument is used .... :/. What we need is something for the construction of objects, in c++ we want to use constructors while in swig we want to use factories .... but later .... We would need some more detailed description of what you are doing. Not sure this is even a shogun related problem here..... We are working on making it possible to use single algorithms without using the whole lib, but that will take quite some more time to be ready. Hi! Thanks for putting this up. Note that shogun does compile natively under windows.\nThis is interesting for others as well, so some thoughts on how to share what you did\n\nif you put up the source code, rather fork the repository, and locally push your changes in a single commit. Then people can see what you actually did. As of now, with the .rar file, I cannot without having to download it, then open and diff it manually, which is a higher barrier than just looking at a commit diff\nAs also mentioned in another PR, it would be actually useful if you described the process of what you did here in a blog post: what issues you faced, how you solved them, with lots of links and guidance for others. A tutorial style guide. Then we know a bit about the rationale of what you did. If it is nicely written, we could add it to our wiki for newcomers. Good addition definitely! Did you use this or how did it come about? CI is unhappy though. Then go for it :)\n\nOn Wed, 30 Jan 2019 at 15:53, Gil notifications@github.com wrote:\n\n@gf712 commented on this pull request.\nIn tests/unit/mathematics/linalg/operations/Eigen3_operations_unittest.cc\nhttps://github.com/shogun-toolbox/shogun/pull/4489#discussion_r252288948\n:\n\n\nint8_t, int16_t, uint16_t, int32_t, uint32_t, int64_t, uint64_t, float32_t,\nfloat64_t, floatmax_t, char>\nAllTypes;\n-typedef ::testing::Types<\nint8_t, int16_t, uint16_t, int32_t, uint32_t, int64_t, uint64_t, float32_t,\nfloat64_t, floatmax_t>\nNonComplexTypes;\n-typedef ::testing::Types RealTypes;\n-// TODO: add complex128_t type\n-typedef ::testing::Types NonIntegerTypes;\n-\n-TYPED_TEST_CASE(LinalgBackendEigenAllTypesTest, AllTypes);\n-TYPED_TEST_CASE(LinalgBackendEigenNonComplexTypesTest, NonComplexTypes);\n-TYPED_TEST_CASE(LinalgBackendEigenRealTypesTest, RealTypes);\n-TYPED_TEST_CASE(LinalgBackendEigenNonIntegerTypesTest, NonIntegerTypes);\n+TYPED_TEST_CASE(LinalgBackendEigenAllTypesTest, TypesGoogleTestWrapper::type);\n\n\nah actually less, I have a lot of things now that I don't need for this,\nso more like 100 loc\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/shogun-toolbox/shogun/pull/4489#discussion_r252288948,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAqqv0ynWi7t9ZmV6XvnsOdROPrvdKUJks5vIbHggaJpZM4aUF2Q\n.\n-- \nSent from my phone\n. Yeah I think adding is less important than removing as we can offer a reasonable selection of sets. Yes an issue would be good. The description should go into the wiki though (there is a section on dev documents somewhere), and then the issue can link to that.\n\nI am currently unsure about other places for typed tests. We want to test all machines of a certain type for particular things, but I think that will be done differently. It is something that you may find interesting though, see #4298 #3732 and #3537. The idea is that we use something like LLVM to find out what are the subclasses of a particular class (say IterativeMachine mixin) and then test the functionality for all implementations without having to add them explicitly. Ok waiting for the CI and then merging.\nLet's go to the typed testing at a later point, but I have the feeling you would like this kind of stuff :). CI is angry. I think it had a hickup, there was some \"we are working on fixing the problem\" earlier on the azure site. Alright, merging then :). In C++, this is not necessary in this case as this would suffice\nput_machine\": \"$object->put<CMachine>($arguments)\". Because of derivatives of the kernel for GP inference\n@votjakovr did that ages ago.. This error was a weird one, these kind of fixes (see relaxed tree class) we need to do when doing the transition. For now postponing the removal of LibLinear from SWIG due to enum problems. ok donbot now is happy for python legacy and meta...fingers crossed. omg finally. Either that or better port them to meta examples where possible\nThe legacy examples are causing a lot of work for quite some time, rather than touching them we should reduce their numbers.\nBut since that is not feasible, yes :)\nBTW you could also start with other classes and remove them from swig, we already did a big push for dense feats a while back.\nHaving said that, all efforts are welcome.... Really any non base class should be removed from the swig \u2018.i\u2019 files\nBest to do it one by one as there are always issues popping up... see my let few prs. Thanks for the report!\nWanna send a patch for this? Preferably with a unit test?. yes, though since the fix is simple, I don't expect any major headaches there.\nHave a look at the other unit tests for SGVector::as (if any) and then just add the case that was broken before. @vinx13 while you are at it, the error message (Could not convert CSGObject to X) is bad since it doesnt actually show the type of the instance it is converting from. It should instead use the class name using get_name. Mind updating that?\n. This can be done via a lazy get I think where you use watch_method or?\nOn Sun, 3 Feb 2019 at 10:24, Wuwei Lin notifications@github.com wrote:\n\n@karlnapf https://github.com/karlnapf how should I convert\nhttps://github.com/shogun-toolbox/shogun/blob/d1ca233df6a6e12b47788bc88852cab7528f0817/examples/meta/src/gaussian_process/regression.sg#L30,\nexposing another factory doesn't seem good\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/shogun-toolbox/shogun/pull/4498#issuecomment-460036164,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAqqv1zgiaT0f_eHfbEUe7ppe0TPONuSks5vJqrZgaJpZM4accKO\n.\n-- \nSent from my phone\n. > @karlnapf we can't use watch_method because this method has arguments\n\nas discussed, I think we can use the labels. Just checked the code and it is messy.\nI think in general, we should allow DenseLabels to have some kind of confidence/score field to be stored.\nthe GP could then have a boolean flag whether to fill those in predictions or not. Good point. Maybe you are right\nThe whole thing might need refactoring .... sigh. Maybe for now then your\nconfidence might be best.\nWhat about a new labels subclass just for Bayesian methods? Too much?\nWe can also postpone this as it seems some more work.\nWhat\u2019s best in your opinion?\nOn Thu, 14 Feb 2019 at 04:46, Wuwei Lin notifications@github.com wrote:\n\n@vinx13 commented on this pull request.\nIn src/shogun/labels/DenseLabels.h\nhttps://github.com/shogun-toolbox/shogun/pull/4498#discussion_r256682045\n:\n\n@@ -275,6 +275,8 @@ namespace shogun\n  protected:\n      / the label vector */\n      SGVector m_labels;\n+     / the confidence vector */\n\nI see, so I will store variance to CLabels::m_current_values. One thing\nI'm concerned about is that this may be confusing since higher scores\nusually mean higher confidence. But the variance is different. So when\nusers call labels.get_value they will get value with totally different\nmeaning. What do u think?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/shogun-toolbox/shogun/pull/4498#discussion_r256682045,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAqqvy8nY5sP5SWMYbOiueDK-kSixoquks5vNNwZgaJpZM4accKO\n.\n-- \nSent from my phone\n. > @karlnapf I think it is okay to have Bayesian labels, but then we need to subclass every label type (binary, multiclass, regression) because we have both GP classification and regression.\nAnd actually, condifence has the opposite meaning to variance\n\ntrue, not good. Mixin would be better, but that complicates things quite a bit as well ... and is out of scope\nOk then we just need to decide between using the existing field with the bad name or add another field that is useless for all other occasions... ?. https://github.com/shogun-toolbox/shogun/wiki/Getting-involved. Hi!\nPlease read this how to get involved: https://github.com/shogun-toolbox/shogun/wiki/Getting-involved\nCome and ask us in IRC if you have any specific questions. Sorry @sorig to pester you with all the meta example issues :). using it in #4494 . There is a description how to do this in one of the readme files\nOn Fri, 1 Feb 2019 at 12:23, Viktor Gal notifications@github.com wrote:\n\nYes plz do a pr there and update this pr to use that shogun-data commit.\n\nOn 1 Feb 2019, at 12:14, Kirill notifications@github.com wrote:\nIt should be a separate PR to shogun-data repository, right?\nYes indeed you should regenerate the data files for the lrr\n\u2026\nOn 1 Feb 2019, at 11:54, Kirill @.***> wrote: @vigsterkr There are\ntwo failed tests, they both depend on LinearRidgeRegression class. Am I\nunderstand right that I need to regenerate data files used for these tests?\nThey both depend on LinearRidgeRegression implementation which produced\nwrong results because of this bug in SGVector::as function. \u2014 You are\nreceiving this because you were mentioned. Reply to this email directly,\nview it on GitHub, or mute the thread.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub, or mute the thread.\n\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/shogun-toolbox/shogun/pull/4503#issuecomment-459691833,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAqqv9tl0vlfG2k7G4v_ZiflN5FBKXijks5vJCOtgaJpZM4ad3-W\n.\n-- \nSent from my phone\n. One thing that is missing is that you need to commit the data submodule in this PR. Simply git commit data This will tell the main repo to use the commit of the data repo that you sent the other PR for. Then the CI should pass. (and this can be done before we merge the data PR ... we will merge it when this CI passes). Cool, let's wait for the CI and then merge this\nNice one!\nJust curious, how did you find out about the error?. ah sorry there is a conflict in data now since I merged another patch, will need to rebase and force push into you feature branch now. no need to make a new pr.\njust simply rebase against develop and force push.\nalternatively, save your commits, and cherry-pick them on top of develop and then force push. Just had the time to properly understand what happened here. Really nice catch! Thanks again. could you rebase this, as there is a merge conflict right now (data was changed in another commit). rebase and merge?. CI now fails, not sure what is happening there, should be passing if you add the files. Maybe forgot to update the data submodule?. ah we wrote at the same time .... give the manual fix a try. But I bet this has to do with the eigensolver backend. We had sign issues with those before .... ah this is annoying, it is a known problem with the integration tests.... I am not sure we can even fix this. In the unit tests we make the tests invariant for sign...but meta examples we cannot do this.\nOne option is to make the CI pass and then use those as reference (while they fail for other archs)... maybe give it a try . If the manual fix doesnt work, don't loose sleep. Then we need to make the eigensolvers signs platform independent first. Ok let\u2019s see how this goes on the buildbot..... CI error is unrelated... shall we merge it?. Cool, and?. ah just saw this nevermind my other comment in #4519 . sweet so no changes, sounds good thanks for doing this!. Do it! :dagger: . Please dont send, close, and re-open PRs. This spams our inboxes. Rather update and existing PR using git force push. btw this is all in our readmes :D. From the installation readme\n```\nThe standard GNU/Linux tools and Python are minimal requirements to compile Shogun. To compile the interfaces, in addition to swig itself, you will need language specific development packages installed, see interfaces below.\n\nThere is a larger number of optional requirements. The output of cmake output lists optional dependencies that were found and not found. If a particular Shogun class is unavailable, this is likely due to an unmet dependency. See our docker configuration file for an example configuration used in our test builds.\n. The error message is pretty clear:\n\"Ctags required for meta examples. Install or set BUILD_META_EXAMPLES=OFF.\" . Thanks for reporting those. We are more than happy for direct edits to the wiki (it is public), PRs for the examples/cookbooks, and PRs to the website repository. no own apt server\ndebian packages are not up to the latest iirc....need to check. We have some ubuntu packages but not the latest....we always struggle to find people helping with the packaging in fact if you are looking for something to help :)\nI would leave the showroom for now as we are in the progress of updating the website again, so this should change soon. Nice, thanks!\nAs you can see, the CI is now [moaning](https://dev.azure.com/shogunml/shogun/_build/results?buildId=474&view=logs&jobId=089c709a-44eb-5f6e-96e7-15e9ee1ff5bf&taskId=5899c1e7-1112-57da-0a81-46bbe225a6c3&lineStart=127&lineEnd=128&colStart=1&colEnd=1) and you need to fix some examples\n.\nenum ENNOptimizationMethod\n{\n    NNOM_GRADIENT_DESCENT=0,\n    NNOM_LBFGS=1\n};\n``\nThis is defined inNeuralNetworks.h`, so you need to make sure SWIG sees this by either moving the enum to a file that is exposed to swig, or including the header within swig. I generally wonder whether we want neural networks as CMachine or as its own type?. The enum thing is a general problem that we are discussing how to solve atm, so don't spend too much time on it (for now just include the neural networks header and we can remove that later). yes. The other PR is merged, does that change anything with the links for this one? Otherwise Ill merge it as well! Thanks!. Nope, thanks!. This is a standard python path problem. You need to make sure the .py file of the shogun interface in your build folder is in the pythonpath. Best is to print it and compare . Thanks for the initiative.\nI am not sure this contains anything that is neither in our readme nor in any first item google search though. In particular the latter is really crucial if people want to get involved, so we usually do not put any instructions on how to install software, environments etc. Those also outdate at light-speed.\nThis is why I am slightly reluctant to put this into the main repository. One place where it could be useful was if you would put it into a blog-style permanent website.. Why guidance? :) You could just write a summary of the experience you had trying to compile it and show how you overcame them. Then you could put that into a github page or something similar. We would definitely put a link in our wiki. See for example the GSOC blogs we have there. Also, we could help you edit a bit before you make it public. . What are the results? :). What does this do/fix?\nSo now all classes need this export thing? . well that's great then!\nI am not super keen on having this export statement there, but hey if it makes stuff work why not :). Whats wrong with a strategy pattern? The swig thing we don't need necessarily if we offer sensible default choices (like the one here, and the median, which is much better for kernel machines btw). Then we can change the default value, but I am quite against having this type of hard-coded heuristics in places where they don't belong. At the least, we need a way to choose between more than one different heuristics. This one is not good in so many contexts.. Here is what I suggest:\n-Move the code to extract the parameter value away from the registration,\nlike somewhere centrally accessible, where all heuristics will live\n-Overload put to accept some way of instantiating the heuristic, for example\nkernel.put(\u201cgamma\u201d, auto_param(\u201cnum_features\u201d))\nkernel.put(\u201clog_width\u201d, auto_param(\u201cmedian_distance\u201d))\n-Inside this put, the same mechanism as outlined in this pr is used to\nregister the function\nOn Wed, 13 Feb 2019 at 22:11, Viktor Gal notifications@github.com wrote:\n\nThen we can change the default value\nand in your opinion for the degree what would be the good default value...\nand why?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/shogun-toolbox/shogun/pull/4522#issuecomment-463372694,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAqqv6hOamx0lsWgDj6iXxH2_qKYR6w8ks5vNH90gaJpZM4a0WXZ\n.\n-- \nSent from my phone\n. Gil what I meant was\n add a parameter property \u201cread only\u201d just like model parameter etc\n put throws a runtime error when someone tries to modify a read only parameter\n the left and right hand side of CKernel can be readonly\n users can only modify features with the init method, so the auto parameter init is always called. it looks like you are on the dev version, which doesnt have the compiled lib installed. Use the other version https://hub.docker.com/u/shogun. .. No, much better like this. My phone ate some other comments but not really important..... Lol, you cannot just delete this as shogun won't compile anymore. The CI output above shows you some errors as a starting point. But before you start refactoring/deleting all code that uses the MachineType type, pls make sure it is feasible to avoid running into a deadend. If the enum and code based on it is never really used, we can just safely delete it indeed. It needs some intelligent refactoring and hat depends on the code, and we will need to discuss different strategies. Nontrivial.\n\nReplacing them with int is not the way to do it.. Re the failing unit test, just remove the line that asserts the ref count. I never saw it in shogun, but I also don't really mind either way.. What do you mean?. Ah. Yes pls fix them, still dragging those along for coverage.... but maybe not first thing but only before merging the feature branch into develop. :rocket:. Thanks a lot!. Hi! It is quite unclear what you did, so it is hard to help you. Make sure to read up on how to ask for help, and provide details of what exactly you did. I\u2019m closing this for now as it seems a generic python setup issue. Best to google/understand what PYTHONPATH is. Minor: It would be cool if the visitor pattern that prints the instances would also look up the string for the enum so that users would see that rather than the int. Same with get actually, so that the enums would be strings only in the interfaces. I think given that we only ever have a few options, why not do the recursive manual macro thing somewhere hidden? Say for up to 5 for a start? It\u2019s not pretty but it\u2019s only at one place whereas the repeated parametername would be in dozens of places. Re asserting the correct type I think one way might be to actually combined the registration with adding the option. Then we can force the right enum type and inside hat method cast it to int, register ad parameter and then attach the options. See my point? . Re the get o think first checking the map and then the parameters might be ok no?. > > Re asserting the correct type I think one way might be to actually combined the registration with adding the option. Then we can force the right enum type and inside hat method cast it to int, register ad parameter and then attach the options. See my point?\n\nyup, I was thinking about something along those lines, but then will have to change SG_ADD a bit, and agree on a good signature.\nWhat about just calling sgadd from inside the option macro and remove the other sgadd call?\n\nRe the get o think first checking the map and then the parameters might be ok no?\n\nBut need to have a get that has accepts two types: the cast and then the return type, because in this case the value is casted to an int, but then we need to lookup in the map and return a string. this is easy with partial specialisation of get, but SWIG doesn't support that. what I had (and worked fine) was\n```c++\ntemplate ::value && std::is_same::value>* = nullptr>\n    std::string get(const std::string& name) const noexcept(false)\n    {\n        Tag tag(name);\n    if (m_string_to_enum_map.find(name) != m_string_to_enum_map.end())\n    {\n        return string_enum_reverse_lookup(name, get(tag));\n    }\n    else\n    {\n        SG_ERROR(\"There are no options in %s::%s\", get_name(), name.c_str());\n    }\n    return nullptr;\n}\n\n```\nand then for example in the to_string I had something like this:\nauto value = get<machine_int_t, std::string>(param);\nbut this can't be templated in shogun.i along with the other get (I tried %template() get) which I am guessing is related to this\nAh I see.... so what is the Second best option? I have the feeling one could do some hacks that involve adding a try error approach in python code, and do the mapping there in the lines how you made get return different types.... while that\u2019s again python specific code it would just be a small extension of what\u2019s there and the other Lang\u2019s have to live it int for now\n. Ah sorry phone editing is a pain. Option is more user friendly. Python users might not even know what an enum is :D. Cannot guard with swig as I changed the lib based on how it\u2019s compiled (bad) the method should rather live in the swig parts of is no used from inside. Ah of course. Ok then!. I\u2019m sorry for asking all these things, sounds painful. I don\u2019t have an idea how to do this in a better way right now.\nMaybe don\u2019t bother distinguishing the 3 and 4 argument add for options?\nIt\u2019s not the biggest problem if we didn\u2019t combine them in the end, my idea was morenin the \u201cnice to have if it\u2019s reasonable\u201d corner.\nMaybe let it sit for a day ti see whether a good solution comes to mind and if not we keep it as it is?\nAnything else that needs to be addressed still? A meta example would be ace.... to see this in action and to have it covered for all Lang\u2019s ci. Fixing in C++17 would be good! You could put a note or something and until then we go with some horrible copy paste monster?. No, just show how they are used. Ie set an option and retrieve an option. There is a base api example for such things. No need to change the tests. @avramidis this tests your new typemap as well, it worked locally when I ran the integration test in octave, which means that cpp and octave string lists dumped to the test file are the same. This covers both outgoing typemaps (the one that is returned by get) and ingoing ones (when the variable is passed to the testing container at the end of each example).. Maybe the typemap is buggy. Not even sure who wrote that... good to have it uncovered though!. It crashes haha.... ill also look into it locally a bit. I think it\u2019s probably the ingoing typemap as the example didn\u2019t crash before this pr when only the outgoing one was used.. Ok I found the error.\nThe csharp typemaps for strings are a pandorras box\n\n\nlength of array is written into first string in array using sprintf which is then parsed by csharp to allocate array space.... absolutely nuts\nthe error arose from Marshal.PtrToStringAnsi expecting a null terminted c-string (as the ANSI in the name suggests) but %typemap(out) shogun::SGStringList<char> did not provide that\nThe thing is generally a big mess\nThe typemap might have worked before if parts of the memory were initialized to 0 (it did for some strings). But it was never functional.\n\nSo here we go: string list support in and out for csharp!. C sharp passed!. In a next step we can add int strings to the examples/tests. Having looked at the typemaps, I expect this not not work.\nI\u2019ll have a go at this in the next couple of days.. Yes expose it.\nFunction calls either pls lazily or via registering them with get. The \u201cwatch_method\u201d registration. Yes. Sorry I wasn\u2019t clear earlier:\n\n\neither lazily. That is the method is called only in the public init function of the kernel when the argument is known. We have done this in many places to solve similar problems.\n\n\nor force the user to call it via registering the method and then have something like \u2018kernel.run(\u201cbla\u201d, arg)\u2019 This would require adding facility for that as watch_method currently doesn\u2019t allow this.\n\n\nI\u2019d prefer the first option as the second opens door to all sorts of hackery.\nSee what I mean now?\n. Then we might need the second option.... where is this used? Maybe there are other ways around this?. I think in this case the solution is easy. Give the kernel an additional member for the labels that are normally passed as the parameter of the method. Then inside init, you can put a runtime assertion that the labels have been set. So then users have to do\nk = kernel(\"AUCKernel\"\nk.init(lhs, rhs) # runtime error\nk.put(\"auc_maximization_labels\", my_labels) # maybe a different name?\nk.init(lhs,rhs) # works. Just looked at the method (didnt before, and didnt realise what it did)....\nNot sure how to solve this as this call is basically needed and not part of the kernel API.\nWe cannot add the call to the API neither. The only thing we could do is to add the mentioned void CSGObject::run(std::string) which runs a method without. Then we can make the labels that are needed for this method a member that users can set using put. Then usage would look like\nk = kernel(\"AUCKernel\", auc_maximization_labels=my_labels)\nk.run(\"setup_auc_maximization\") # replaces the std init call\nmat = k.get_kernel_matrix(). A bit hacky, but it was clear that such a problem would occur at some point (and that we need to run methods by strings) ... @lisitsyn ?. Yes would have to something new, inspired by the watch_method, or actually reusing that. Maybe it is just a matter of adding the run for registered methods with void return type. Ask @lisitsyn in irc he will have comments. I think always read only, never put able. And then we try to use factories.... let\u2019s see if that approach works at least. Can you mark feature matrix of dense feats also read only?. Exactly! That\u2019s the the point. Unfortunately no. It was my preferred solution as well but the any framework makes the compiler freak out if we do that (e.g clone, put) and then when I talked to @lisitsyn and he had some other reasons why not to do that. This pr here is the result of our discussion. CI problems?. > @shubham808 consistency_training_tests has that test that makes sure that two models: one with max_iterations = 1 and the other stopped after an iteration, yield the same results. This isn't true here, since the one with max_iterations=1 has different weights because end_training() method is called after training is finished, which changes the weights to the final form (performs averaging).\n\nFor this to work, end_trainig() has to be called when training is stopped for whatever reason. But this will require us to copy intermediate weights, so that training can be resumed with the intermediate weights instead of the averaged ones.\nEdit: My suggestion won't work since end_training() is also used to free used resources by other algorithms. So maybe something like pause_training()?\nEdit2: In this implementation of AveragedPerceptron, weights are summed across iterations, then averaged. Maybe this can happen online: mean_{n+1} = mean_{n} + (x - mean_{n}) / (n + 1)\n\nWe had this issue before. I dont remember how we exactly solved it. A running average seems to work here and seems like a very good idea. Make sure to pick a numerically stable one (might require some googling). Could you add a way to update a mean vector/scalar in linalg in a separate PR? Then you can rebase this PR here to use it. This would be better than just adding the explicit code in here and then it could be used in other algorithms (LARS comes to mind where it is done explicitly as well). Cool ci passed so this is almost good to go. I\u2019ll do one more detailed read through later and then we can merge. It depends a bit: if you only ever use the size to iterate over indices used for a std vector it is fine. If the indices are used in any other shogun data structure, it won\u2019t be\nWhy don\u2019t we just introduce a conversion tool that can be used in other places as well.... the problem popped up before in the testing framework. Flying now. Will check in detail later today . Ah whoops just saw Viktors comment. I think you should cherry pick it or something. Better to have a well behaved central solution. @vigsterkr it seems like deadbeef has everything we always wanted ;) thanks for pointing it out . Alright, before we merge, the CI errors needs to be addressed .... Closed?. Solution: move this to the linalg code. Sorry I didn\u2019t realise earlier that you defined this in the statistics module. Check out how things like the mean are defined in there. Yeah you are right of course. My term solution wasn\u2019t appropriate ;)\nI\u2019ll check into this later (on mobile atm), maybe @gf712 also has an idea.\nI think we should solve this in yet another pr to keep things separate.\nAnd Nevertheless, the averaging functions should live in the linalg module. Which also makes this pr independent of the complex problem. Thanks for spotting that btw :). Yes! That\u2019s annoying me as well\nAlso you can avoid building them via using the other make targets (libshogun, _interface_python, cpp-features-string, etc). you can just build one\nmake cpp-features-string. It generates all of them (that is the problem to be fixed), but it only compiles one.. Note convert is checking for overflows as well (some inline function). The name position needs discussion definitely. Don't go out too far for the gradients yet, I was more fishing for ideas of how all this would word. What is a good way to enable manual gradient implementations of parameters? I don't like this dictionary thing a lot. I think we should start from the algorithm dev perspective here. Maybe the pattern we use is the best we can come up with but since these changes require touching a lot of stuff I think it would be good to challenge that a bit.\nOn the other hand, if your approach works and all the old code stuff works, that is a way forward as well, and can/should well be merged in order to just remove the m_gradient_parameters. Maybe we can do two things: \n discuss all the comments raised in the PR to slowly move that forward\n draft an API to generally retrieve manually implemented gradients. > @karlnapf I don't know how the derivatives are calculated but I am assuming it's something done at the end of each iteration?\n\nCan we not do something like the auto parameter stuff? But in this case at the end of each iteration we call all the lambdas/functions to compute the derivative and update the respective parameters.\n\nyes that sounds good. So devs could then \"attach\" gradient computation methods to parameters? In the sense of\n// register gradient of `foo` wrt to `bar` which is a matrix\nadd_gradient(\"foo\", \"bar\", SGMatrix<float64_t> my_gradient_implementation);\nand then algorithms could call\nSGMatrix<float64_t> grad = obj.gradient(\"foo\", \"bar\");\nand if an algorithm needs gradients of all parameters it could do\nauto grad_params = obj.gradient_names(\"bar\"); // returns all parameters that have a derivative wrt to \"bar\"\nfor (param : grad_params)\n     step += param.gradient(\"bar\");\nIn those lines?\n. > Sorry for the late reply. I wasn't sure how I want to proceed. I propose to continue with \"discussing all the comments raised in the PR to slowly move that forward\". This PR needs a bit of work but at least we can get rid of the old m_gradient_parameters soon. After that we can start an issue to discuss and evaluate how we can change the gradient calculation mechanism. I would prefer not to be in a hurry to make large changes on that.\nyes I agree. I think it might be best to first refactor before changing the design. So let's proceed with that. No rush needed indeed. So what is needed for the old gradients to work with the new system?. actually can you clean up the PR a bit and make it smaller? It is super hard to review now. CI doesnt have to pass for now so you can hide using the new approach in all places. Just a single working example to start with is fine for discussion. Thanks for the patch!. It\u2019s your or so I think you have to do that. Seems I can merge anyways, will check it now. I actually do not know how this happened :(\nHow to I remove this?\n. I could,\nI did add this because of the below call of compute_feature_vector \nI added an SG_NOTIMPLEMENTED there because it sais in the comment that it is not implemented, and also it returns NULL by default, so the output would be strange anyway, because of the line\nfeat=compute_feature_vector(num, len, feat);\nfeat will then be used (eventually, if there are preprocessors and indirectly) in line 226\np->apply_to_feature_vector(SGVector(tmp_feat_before,tmp_len));\nIf there are no preprocessors, NULL will be returned.\nTo avoid this strange behaviour there, I added the SG_NOTIMPL. I think this part has to be revised somehow\n. I dont understand what you mean\n(comments removed, just wanted to avoid confusion here)\n. I just realised that the problem was that I did not realise that the method compute_feature_vector has to be overridden by subclasses.\nWill remove the NOT_IMPL\n. no.\nI also cannot figure out how to remove this :(\nWhen I click on \"view file\" I get an github http  404 error\n. plus there is no commit with the hash shown here\n. done\n. huh?, but this is done automatically. Or am I wrong?\n. My intend for the warning was another:\nWhen the parameter_version flag is being tried to load if it does not exist, there will be two warnings like \"could not load parameter_version from file blabla\".\nSo when a user wants to load an old file, he sees this warnings. Due to the way, parameters are loaded, these warnings currently cannot be avoided.\nTo avoid confusion from user-side, I added this warning, saying that everything is ok.\n. done\n. will do this in next patch\n. I dont know what this is. I never commited this.\nWe had this, I remember.\nI cannot revert the commit. \nany ideas?\n. fixed\n. Just a number to have some buffer space for a to string method.\nI could change this to fit exactly but this is fiddeling.\n. yes. Its to set this flag which enabled data freeing in the destructor.\nalternatives are public flag or friend class SGObject.\nBut I prefer this one. You dont?\n. fixed\n. typo, but the SGREF is ok right? :)\n. ok, merging\n. please dont use stdio but rather SG_PRINT SG_SPRINT\n. SG_PRINT\n. could you send these patches separately next time? though ok for now\n. if (titles)\n    delete titles\nbut usually we dont want to use delete but SG_FREE\n. mmh, why are you using std::map here?\n. without {}\n. without {}\n. without {}\n. this should not be included in a header file but in the .cpp\nalso, I think std classes should be avoided\n. without {}\n. without {}\n. without {}\n. without {}\nI wont comment the rest of these\n. if (..)\nwith space\n. Awsome!\nI did not take part in the discussion but i think it would be great to somehow incorporate probabilities for the labels.\n. All parameters need to be registered, not only these for model selection -- with MS_NOT_AVAILABLE.\nThis has to happen for every class and every variable in it. (m_labels, m_mean, etc)\n. our convention is: no braces for one line if/for, only if there are multiple lines, i.e.\nif (foo)\n    bar;\nif (foo)\n{\n    bar1;\n    bar2;\n}\nI know its annoying to stick to :) but thats how we do it everywhere\n. As far as I can see, this method searches the tree for a TParameter instance with the given name. Why do you need it?\n. Couldnt you break the the loop once you got true from the recursion?\nAlso, false is returned if the desired parameter is in a child node\n. Id prefer a friend class since this shouldnt be touched from outside\n. no {} here\n. Nice method!\nI have some worries: This stuff is really hard to overview due to all the recursions. On the long run, I would rather produce all combinations and choose a random one - simply because that reduces the chance of an implementation error. (The computational costs of producing all are not that large since parameter trees are small)\nThere are many cases of parameter trees to have in mind and now there are two places where one has to be extremely careful: this one and get_combinations. But for now, since get_combinations doesnt work, lets keep it :)\nThe rand function is ok btw\n. please use the SG_ADD macro with MS_NOT_AVAILABLE\n. Ill have a look in this once it is merged so that I can browse around in the code\n. character encoding clash\n. please put return false in a new line\n. There should be tests added for these new methods in Map. Many things depend on them\n. same here, please add a test for new methods at some point\n. The second error message wont be printed since shogun exits after the first :)\nalso: return SGMatrix(); is fine (0,0) is not needed\n. Please use SG_ADD and MS_NOT_AVAILABLE\n. Just a minor thing: sometimes you are using a=b, sometimes a = b\nWould be nicer to only have one version\n. please always call base constructor in sub-classes\n. please call base constructor\n. When methods return a new (SG_REF'ed SGObject), you need to remember to add that in the modular interfaces once you integrate them. Just a reminder (I always keep forgetting this and wonder about memory leaks) See the existing .i files to see what I mean\n. as already said: this may be done by the TSGDataType struct\n. Better do this with SGString than with the return output in one of the arguments. SGVector would also work\n. memcpy would be nicer here\n. this loop is the same in every of the case distinctions, could you refactor in such way that is there only once? The size is the only thing that is different for different types - also you can use TSGDataType again and then theres no need for a case distinction at all\nI would also use memcpy here and not iterator over all the bytes by hand. Thats much faster for large matrices! For casting, you need not to touch every byte.\nI think there is a semantic error: You in fact only copy the first element of the parameter data (bytewise). However, there are vectors and matrices, which you wouldnt cover here. I would get the size in bytes of the parameter data using TSGDataType and then just use memcpy to copy all. \n. yeah: so I think all of these distinction can be reduced to a few lines of code.\n. There might also be vectors/matrices of SGObjects, you need to iterate over all of them\nTSGDataType contains a field length_x length_y, (these are pointers so carefull, might be NULL)\n. you can use alpha.display_vector(\"Alpha Vector\"). Thats much shorter :)\n. minor thing: print \"asdasdads\" (without () ) does the job\n. I currently cannot see if you did this:\nAny method that returns a new object (like evaluate) which is SG_REFed needs to specify this in the modular interface to prevent memory leaks. Like for example %newobject create_merged_copy(CFeatures* other); in Features.i\n. please use a new line for {\n. sometimes, m_length_y is NULL. You have to check for that\n. I would prefer if you would add a wrapper in the CHash class, so that you dont have to iterate over the call IncrementalMurmurHash2, but just pass number of bytes to it. Makes reading a lot easier.\nAlso its faster since the function is only called once instead of num_bytes_of_matrix times\n. Same here: please put the loop into CHash class and just pass number of bytes\n. Otherwise: great work. This seems so much smoother than the old approach! :)\n. better make that SG_DEBUG\n. thats really cool!\n. why is this?\n. Careful here!\nIf feat==m_features and it has only one refcount, it is deleted and you get an error.\nalways first SG_REF\n. same here, first SG_REF\n. same here, first SG_REF\n. same here, first SG_REF\n. same here, first SG_REF\n. you have to be extremely careful with expressions like this new SGVector: Where is it deleted? Who handles this new vector on the heap?\nAlso, except for the serialization, this is done nowhere in shogun.\nIsnt there a way avoiding this? All pointers should  be already in memory and you just have to fill in the data.\n. please use SGVector here then you dont need the extra length field and you also can use SG_ADD to register parameters\n. all this memory stuff can be simplified with SGVector business with automatic reference counting etc\n. please alsways use {} if more than one line instructions after an if\n. I know that you adepted this from the other class and it might be annoying, but please again here, use SGVector stuff. This do_free flag is horrible :D\n. same here: SGVector\n. please use SG_ERROR instead ASSERT\nASSERTs are a nightmare for users, error messages tell them what they did wrong :)\n. SGVector all over the place ;)\n. {}\n. SG_ERROR\n. please put that { into a new line\n. same here\n. {}\n. {}\n. {}\n. {}\n. {}\n. why using static error here? you are in a non-static context\n. btw, if you init shogun with init_with_defaults() you dont need this method, take a look at other examples\n. could you put all the code into a method. Otherwise, shogun with trace-mallocs will complain about unfreed memory in the matrices\n. what about some documentation what you are doing here? :)\n. whitespace\n. Coding style, could you remove {} for one-line commands in loops/if ?\n. I would rather do this locally\n. please put a /**, otherwise doxygen produces a modular interface warning\n. Please provide some doxygen information on how this is computed\n. Please document the class. Say what it computes, point out references\n. please update doxygen\n. please initialise variables in here rather than in the constructor.\n. SG_MALLOC\nI understand this is a 3-dimensional array. I would prefer if you would use SGMatrix for this. This code is quite hard to read-\n. the set normalizer could go to init. Why exactly this normalizer?\n. Where is this code coming from?\n. We don't do this anymore, use the SGVector equivalent, then you also do not have to free\n. Please mention that Cholesky is used.\nOh and that this is for PSD matrices only\n. Just a minor thing: could you add more numerical accuracy here? Usually we do 1E-15\n. Could you do two separate test functions for the two tests?\n. indentation\n. This is a bit dangerous since log(det(M)) may fail.\nCould you rather use a fixed matrix and assert the results as above? (Solve in Matlab or whatever)?\nThe random case should only be done for very small  matrices - so maybe change the above and do the large one fixed\n. Rather use one single matrix here. You can do A=A*A.transpose().\nAs for the scalling, just use CMath's random normal to set every element. This way you get better conditions numbers\n. This causes the problem: You let eigen handle the matrix.\nInstead, allocate a shogun matrix SGMatrix S(dim, N) and then fill with randn.\nThen use eigen's Map\nAlso, please use a dimxN matrix where the vectors are column wise\n. please respect the 80 characters margin\n. shogun's coding standard is a=b (without spaces)\n. This needs a transpose if S is transposed. The result should contain vectors column wise\n. Margin again\n. Could you add some comments on how the sampling works? Cholesky factors etc...\n. please use a more descriptive comment or delete it :)\n. What does this mean?\nTry to avoid TODOs in code as they tend to be never solved\n. margin again ( I guess)\n. Please write proper error messages, using for example\nREQUIRE(cov.num_rows, \"CStatistics::sample_from_gaussian(): Number of covariance rows must be positive!\\n\") and so on....\n. Ah I see,\nplease do not explicitly invert matrices. Instead of computing inv(L)*x, do L\\x which is solving a system.\nHave a look in the ExactInferenceMethod code for an example how to solve systems involving Cholesky factors.\nAnother thing, please do not transpose matrices, compute the lower Cholesky if you want to transpose\n. yes, this is the way to resolve the memory issues :)\n. Not really needed, but they dont harm you\n. 15 is way to large. This has to be smaller than one even for this dimension\nProbably a wrong transpose or something\n. Have a look in what way the covariance matrix is computed. Whether it assumes that data is row-wise or col-wise\nThis might be the reason for the error below\n. These are very nice error messages! :)\n.  Always try to avoid inverting matrices. This is inaccurate in these high dimensions. I think what I would rather do is to multiply the empirical covariance with the given one and compare against the identity.\n. Thats a bit fuzzy. What about \"if true, samples from N(mu, C^-1)\", Ill merge but please change in the next PR\n. Why do you need to include the cpp? Can't we put interface in .h and implementation in .cpp. Or does that cause problems?\n. put your name here :) - in fact in any file you create\n(See other files on how to format, usually Written (W) 2013 Heiko Strathmann)\n. Could you add come notes on how the memory is organised? Are the entries copied? etc\n. @lambday I see the problem now, so if you want to create a single one, use the constructor. If you want to create an array of vectors, use SG_MALLOC. So what I said in IRC wasn't really correct. To answer again: yes, all SG structures that you dynamically create should be done with SG_MALLOC or new.\n. Please include a header with the GPL and your name\n. use init_shogun_with_defaults() Then you dont need to define this print_message stuff\n. newline\n. epsilon is not really needed for this example, just leave it. Then its set to the std value\n. you can also just leave this\n. the argument in the constructor can go\n. intendation. Maybe add see BinaryLabels documentation\n. why this here?\n. intendation\n. could you use a covariance matrix that is not invariant to permutation? This way you check whether the code permutes correctly\n. Yes, I think this would be good\n2013/4/19 Soumyajit De notifications@github.com\n\nIn src/shogun/mathematics/Statistics.cpp:\n\n\nREQUIRE(cov.sparse_matrix,\n\"CStatistics::sample_from_gaussian(): \\\nCovariance is not initialized!\\n\");\nREQUIRE(cov.num_vectors==cov.num_features,\n\"CStatistics::sample_from_gaussian(): \\\nCovariance should be square matrix!\\n\");\nREQUIRE(mean.vlen==cov.num_vectors,\n\"CStatistics::sample_from_gaussian(): \\\nMean and covariance dimension mismatch!\\n\");\n  +\nint32_t dim=mean.vlen;\nMap mu(mean.vector, mean.vlen);\nconst SparseMatrix &c=EigenSparseUtil::toEigenSparse(cov);\ntypedef SparseMatrix MatrixType;\n  +\n  +#ifdef EIGEN_YES_I_KNOW_SPARSE_MODULE_IS_NOT_STABLE_YET\n\n\n@karlnapf https://github.com/karlnapf I didn't add the extended\nSimplicialCholesky in the eigen3.h yet. We only need to handle it this way\nwhen we need to access the upper and lower factors. Shall I add this to\neigen3.h too?\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/shogun-toolbox/shogun/pull/989/files#r3876933\n.\n. You can add it to this one  :)\n. nice!\n. could you move this up to he other intis?\nalso please add to the parameters with SG_ADD\n\nedit: I think this method is not needed\n. this should probably take a boolean as input arugment?\n. Why is this method needed?\nI think it needs a getter to check whether it is supported and be initialised in the Machine implementation\n. exactly like this\n. indentation\n. please do proper indentation\n. @lisitsyn could you comment on this?\n. indentation\n. please do SG_ERROR(\"%s::decide_label(): Dimension  ...., get_name())\nto include context information, also provide a bit more information what this means \n. I think we should have multiple strategies here.\nCould you add a mechanism for specifying a multi-class probabilities method?\nAn enum should do it, can be set via a method and the rescale method checks which method to implement and gives an error message if not supported.\nWhat do you think of this?\n. I like error messages much more than asserts\n. See other comment on multiple strategies\n. indentation\n. newline\n. indentation\n. newline\n. comment indentation\n. could you rename the file to preprocessor_kpca_graphical.py?\nThis graphical example should include the plotting, the python_modular one should be without\n. could you rename the file to preprocessor_kpca.py?\n. This leaves the unit test unchanged right?\nCould you also check the integration tests?\ngo to tests/integration/python_modular\nand do python tester.py \n. could you close the unrelevant one?\n. what is the difference between preprocessor_kpca and preprocessor_kpca_modular?\nWe just want one graphical example right?\n. the graphical example should be in the graphical folder\n. tab\n. tab\n. wow, so you really ran all this with the GPML matlab toolbox?\n. obsolete comment\n. these while loops are not very elegant, this can all be done with vector operations.\n. please write this a bit more clearly, what is 42?, maybe use a variable for that\n. plt is not used here\n. please rename this file to generate_circle_data.py\nAlso, it should contain a function that returns the data points rather than a script.\nAnd please add a python documentation to that function so that we know what it does. Currently this is hard to guess if you just look at the code\n. all these plain numbers make the code very hard to read, consider using variables with descriptive names\n. please make example quiet, only output should be print('KernelPCA')\n. not needed as this is just a tool function\n. indexj, unused variable from here\n. indexj, unused variable from here\n. try zeros((2,50)), which is much nicer\n. use zeros\n. use zeros\n. nice with the variables! :)\n. again, zeros to create empty arrays\n. please avoid these loops\nyou can for example do:\ntemp=range(xmin_circle1, xmax_circle1+1)\ntemp=temp+temp\nThis makes code first much faster (not so important here, but in other places) and second much more readable\n. please replace by a one liner using numpy's vector/list operations as above, its too hard to read this way\nWhile loops should not be in here\nAlso, why don't you use the circle[0] data that you generated above?\n. what is this for?\nCan't you just import the file?\n. I see. But for the non-graphical you can just import\n. Check the other examples how to do this, e.g. classifier_svmocas_modular.py, which imports things from the tools folder\n. this can also be done as a vector operation\n. It works fine, but it looks horrible ;) and if its not needed I would rather like to avoid such constructs\n. tabstop\n. Why is this flag in the constructor?\nIt should be set manually if a developer has implemented probabilities, this should not be set by the user\n. no setter for this! It depends on what is implemented, not on what the user sets\n. This is the place where this flag is set, hardwired\n. and thats also why it doesn not have to be registered (sorry I was wrong here last time)\n. this should just return hard-wired \"true\" or \"false\", not a variable, in fact, the variable can be deleted\nThe base class should return false, and any subclass which implements probabilities should overload this method and return true\nIn fact I am just wondering, the flag should appear in the multiclass strategy rather than in the machine. The machine has nothing to do with it, it depends whether the multiclass strategy supports porbabilities\n. If no heuristic is set, then the distance from the hyperplane aka SVM score should be used/returned.\nIts easy for users to transform these to probabilities using the very same method then.\n. please don't resize vectors but rather allocate if differently given on the heuristic:\nSGVector tmp_output_for_i;\nif (get_prob_heuris()!=PT_NONE)\n  tmp_output_for_i=SGVector(m_multiclass_strategy->get_num_classes())\nelse\n  tmp_output_for_i=SGVector(num_machines)\n. so this call just does nothing if no heuristic is set right?\nI remember you talking about copies being made,....\n. I suggest a different naming scheme:\nRemove PT and HEURIS\nOVA_NORM\nOVA_SOFTMAX\netc\nPlease document in the enum what the different methods mean and provide references to either the class documentation (which contains references to literature) or to literature directly\n. Ah I see now,\nCopy by Value of SGVector copes again only rows/cols/ref-counts, not the data, so after this call, both vectors have the same data. This is good!\nBTW Could you do this without references (not needed)? Just have one vector as input and one as output\n. Please move all SG_ADD or m_parameters->add calls to one private methid called init() where you also set std values.\nCall this method from all constructors. We usually do it this way rather than using initialisers m_rejection_strategy(NULL)\n(This class did not do this in a clean way before so not your fault)\nAlso, please register the new variable. Ah shit, this causes the build to break. Please add a SG_WARNING(\"%s::CMulticlassStrategy(): register parameters!\\n\", get_name()) for now. We need to fix a bug before this can be added\n. in all doxygen comments, you have to provide @param for all parameters, otherwise we will get warnings later. Also makes sense :)\n. Please add a brief description and literature references of all method to class comment\n. This should be registered, but as I can see, none of the others are registered, so leave it for now. (Also again the bug problem)\n. Please provide more context for all error messages:\nSG_ERROR(\"%s::rescale_outputs(): Unknown OVA probability heuristic type!\\n\", get_name());\nAlso not that you need to add newline \\n\n. exactly like this!\n. TODO?\n. Same here, please add descriptions of all methods and where they come from to the class comments\n. @param missing\n. Please dont use references for SGVector, you can do copy by value\n. same here\n. Nice example!\nMake sure to valgrind it.\nAlso, could you add one for python_modular once done with the rest?\n. this should not be happening\n. this should not be there. You should not commit this\n@lisitsyn Do you have an idea how to resolve this?\n. As S\u00f6ren said, please do the documentation in the same style as the others\n. this should not be here\n. this should not be here\n. Hi Deepak\nas said a couple of times before, please do this file in the style of the other descriptions\n. This file should just contain the description for the python_modular example.\nNot the graphical one, not the circle file\nAlso the description should be a bit more informative (what data, what to expect, what can one see)\nThe header \"Documentation for ...\" should be removed\n. We do single if statements without the {}\n. if you return the same object, it needs to be SGREFed\n. I do not relly like to have this loop before you start.\nDo you think its possible to do everything in one loop (also pease add type checks for the kernel lists)\n. static casts are evil, use dynamic ones and check for NULL\nbtw for the kernel types, you can do get_kernel_type of CKernel (better)\n. without {} please\n. add index of the empty list\n. very nice that you thought of this :)\n. I find this one weird and unintuitive to read index starts at i which itself is being incremented.\nBut maybe it just doesnt match my way of thinking (recursively)\n. please not static cast\n. Please use DynamicObjectArray for CSGObject types\n. please SGREF\nAlso, in the modular typemap, you need to add a line that tells the iterface that a new object is returned\n. This SG_ADD might break things, better remove for now (comment out)\nSame with the others. Also, please bundle all SG_ADD calls in one private method in which you also initialise default values (dont do this in every constructor but just call the private method)\n. we do without {} if only one statement\n. here, since two lines, you need {}\n. same here {}\n. please comment out for now since this would break stuff\n. Please replace by SG_ERROR with a proper error message\n. Looks good!. Maybe comment on the circles\n. please remove all data commits, I will do them (the current ones are still not right)\n. { }\n. { }\n. { }\n. { }\n. this has to be dropped\n. this has to be dropped\n. this has to be dropped\n. this has to be dropped\n. this has to be dropped\n. looks good to me!\n@sonney2k @lisitsyn any comments on this? I am not a configure guru...\n. defines should not begin with underscores (there was recently a mail on the list on this)\nNot a severe thing, but maybe you could change this at some point?\n. haha :) I love those kind of expressions\n. I think we have to update our typemaps for this to be available from modular interfaces.\nComplex is put as a reference parameter in some functions ...\n@sonney2k @lisitsyn I never created new typemaps, how does that work?\n. we use #include  (with a space)\n. Please use SG_PRINT and SG_SPRINT (static) for prints\nSG_DEBUG and SG_SDEBUG for debug messages and \nSG_WARNING, SG_ERROR for warnings and errors\n. Every class in shogun needs a @brief doxygen comment to be merged.\nIn there, there should be a brief description of what the class does and which methods it uses (if possible in latex code)\nSee other classes for examples, e.g. CSVM\n. We have a SGVector datatype for vectors.\nAny class variable should be of this type\nAlso, please dont use vectors of vectors, but rather a matrix (SGMatrix)\n. all class members should be documented in doxygen, especially things like dfs (not clear what it is)\n. again, please use SGMatrix/SGVector here, otherwise we will run into problems in the modular interfaces.\nPlease also add a method documentation with @param and @return\n. class member variables usually begin with m_\n. we add {} in any for/if if there follow multiple lines\n. SG_ERROR(\"%s::findColoring: No coloring found, try biger maxColor\\n\", get_name())\n. a unit test for those would be great!\n. This will break all integration tests with this class involved\nSigh, I really have to fix this bug to allow changes of parameters.....\n. There is still a DataGenerator in here...\nCStatistics::sample_from_gaussian is the better alternative, I plan to remove DataGenerator at some point\n. Newly added parameters. But thats not your fault.\nThis only happens if there are existing integration tests before you added new parmeters - so if you change the example/implementation anyway, you have to update them anyway. In this case, its safe to add/remove parameters.\n. Ah I now understand what you had in mind here.\nI would do this slightly differently: the init method should not have any parameters, just private void init()\nIt initialises with default values and registers parameters.\nHowever, the constructors should have the REQUIRE checks, they are called by the user, so please do the checks in there.\n. this is fine like this - and exactly why we dont have public member variables but rather getter/setter\n. What is this supposed to do?\n. What is an implementation error here?\n. What does this mean?\n. What does this mean?\n. Why no strings of complex?\n. This might be a bit dangerous, could you rather keep SGOBJECT as it is?\n@sonney2k what do you think?\n. No you dont have to add it, but please change the error message to \"Parameters of strings of complex64_t are not supported\"\n. Yes, either save it to doubles, or change the error message (which I prefer - we will see when someone needs this)\n. I would just go for a proper error message. If someone needs this, he will tell us.\n. @sonney2k at some point, we should :)\n@lambday vectors and matrices are enough for now, but again, I think there should be error messages. In this case (and in fact also for SGObject, I think a warning should be printed before -1 is returned.\n. @sonney2k no idea why tests are fine, but lets not mess with this and just keep SGObject at 13\n. Ouch!\nUnit-tests! :)\n. this should be a warning\n. please mention what apply does for each linear operator implementation (latex code of Ax in this case)\n. I would rather not have the boolean flag and just init with 0\n. Please just ref the new one first, then unref old one, then overwrite. Best way. This one is indeed indeed weird and also not safe (object is estrogen if old and new are the same and recount is 1.\n. Please do this via constructor rather than setter. These things should only be set once. Setters for jobs and and results can be be removed in fact\n. Always SG unref, never delete on sgobjects\n. REF not really needed\n. Could you try to somehow avoid those whitespace changes? This blows up the diff and makes it much harder to parse\n. and there are soooo many of those\n. please dont do this since we tend to forget about documentation. When this is not there, we get at least a warning.\n. why do we need this at all?\n. btw written is enough :)\n. please always do REF before unref since it might be the same object with ref-count 1 (is deleted then)\n. we have to be really careful here about these two staying in synch. Isnt there another way of doing this? Can two references be avoided somehow?\n. doesnt check_members returns a boolean?\nIf not, why not? Should be unified with the supports_*** stuff\n. This works out pretty nice!\n. I dont really like this warning. Rather make it a proper check\n. please do these kind of things in seperate whitespace patches if possible (blows up diff once more)\n. maybe this should be long? Dont know, what do you think?\n. ouch, please only do this for LLT\n. does this call compute the eigenvalues or are the computed in the constructor? otherwise they are computed twice\n. return uninitialised memory if matrix is not full rank, that should not be.\nAlso Cholesky can be computed on non-full rank matrices, I think we should rather check psd here. Does LLT do that?\nPlease unit test these cases\n. ok cool!\n. Ah sorry I was confused. LLT needs a real diagonal to work, so when you fix the solver to be complex (which I maybe would change, what do you think) you dont need to deal with it\n. I think then just remove LLT from the solver, we wont need it anyways\n. could you add {} here? We always do them for statements over more than one line\nif (foo)\n{\nif (bar)\n   ...;\n}\n. why the cast?\n. deep copies are created with SGObject::clone() this is the copy constructor which does a shallow one (usually, you implemented it yourself here thoguh)\n. this class should have the possibility to accept precomputed shifts\n. really nice how everything is coming together now :)\n. I dont know about this. This means that we have to alter the script for new classes which is not nice.\nRather change codebase to include complex things. But maybe there is another way?\n. ok thats fine, agreed\n. could you add the math here? the cdf has a closed form integral expression\n. Please mention that log(normal_cdf) is called otherwise\n. please add documentation that this is the element wise p(y_i|f_i) and that one has to compute their sum in order to get full likelihood for all given f\n(for all implemented likelihood functions in shogun)\n. hi,\nif you want problems with clone (which is likely since it is not yet fully tested),please fill in bug reports with code.\nI will finish the cloning stuff after the workshop, but currently busy with preparing talks\n. I am not sure about this change. We should rather initialise things properly in a more explicit way.\nCould you come up with a nice way? :D\n. require maybe, users might get confused otherwise\n. to remove warnings?\n. why those int vectors?\nYou could easily store the probing vectors as floats. Is that a problem?\n. Always try to write out more information for require. Here you could add the lengths that are not matching. Makes it easier for the user\n. Why not have this under SGVector interface?\n. same here, why the pointer and length? We introduced SGVector to get rid of this :)\n. output sizes\n. nice! :)\n. good!\nAll these conditions should be carefully unit-tested btw (on very small toy datasets)\n. Its fine, just asking\n. The earlier ones are for the old coffin framework (I think at least) which is very old and can be changed ;)\n. Oh you got me wrong here. I was more after something like: \"Dimension mismatch! %d vs %d\"\n. these were really tricky ;)\n. those will be catched by automagic tests in the future :)\n. thats good, shogun takes ages to compile :)\n. nooooo.\nPlease dont do that. This is not the place to blacklist buggy classes. \nRather, this one is for classes that our framework does not allow to automatically test.\nCould you remove them and add a comment: \"Not for buggy classes. Only for framework problems\"\nOh, and maybe add a bug issue and send it to  me -or fix it :)\n. could you note here that this distribution is q(f|y) \\approx p(f|y) ?\n. could you give the exact form here? (including matrix inversion lemma version)\n. this seems some unnecessary operation to me. What about (python * is element wise multiplication)\n        # gp book 3.27, matrix inversion lemma on\n        # (K^-1 +W)^-1 = K -KW^0.5 B^-1 W^0.5 K\n        C = (K.T * w_sqrt).T\n        C = solve_triangular(L, C, lower=True)\n        C = solve_triangular(L.T, C, lower=False)\n        C = (C.T * w_sqrt).T\n        C = K.dot(C)\n        C = K - C\nApplying this to the identity might be unstable ?\n. interface in cpp file?\n. did the array stuff work out fine? Or were there any problems with it. I am just curious\n. Please give a detailed copyright for these functions if you took them from other implementations.\nWhere do all the magic numbers come from ? :)\n. please say where you got those from\n. maybe this is better as a protected method?\nSince users only want to integrate, but not specify intervals etc\nWhat do you think=\n. Ok, Then use DOXYGEN_SHOULD_SKIP_THIS\n. Yep I agree, maybe you can add that?\n. Please add this in the file header and state the projects copyright.\nIn the method dox, just add a note where it is from\n. ok then\n. Mmmmmh, is there maybe another source of those?\nSomething we can write in the header of the file? If not, maybe just state that it is easy to compute those or so ;)\n. Stuff?\n\"Added mini-framework for numerical integration in one variable. Implemented Gauss-bla-bla method\"\n. Minor comment: It is always good to print out the assertion parameters if they dont match. Helps a lot.\n. Yep fine. Though the pre allocated vector might be slightly faster. Memory is not important since it is a vector\n. These things have to be unit tested. In general, whenever you add new general purpose stuff, please write some tests. Same for algorithms\n. Why are you using strings here? These should be enums!\n. Please dont use ASSERT but REQUIRE with some meaningful error message giving the index and its bound\n. REQUIRE\n. Use a require here and again print the index and the allowed bounds\n. REQUIRE\n. again :)\n. nice!\n. REQUIRE\n. These are good error messages!\n. We carefully should think about whether this method can be empty\n. Yep\nat some point this we should clean up the examples since quite a few are in fact unit tests (I wrote most before we had googletest)\nSigh, work\n. maybe check for proper dimensions of the vectors?\n. where is this number coming from?\n. Cool!\nLooks good, I got a few comments but my phone wont let me comment.\nIll do this and merge later tonight.\nIndeed the precision is satisfactory :)\nOn 18 Aug 2013 11:51, \"Soumyajit De\" notifications@github.com wrote:\n\nIn tests/unit/mathematics/logdet/RationalApproximation_unittest.cc:\n\n+\n-   // clear all aggregators\n-   aggregators.clear_array();\n  +\n  +#if EIGEN_VERSION_AT_LEAST(3,1,0)\n-   // compute the trace of log(m) using Eigen3 that uses Schur-Parlett algorithm\n-   Map eig_m(m.matrix, m.num_rows, m.num_cols);\n-   float64_t trace_log_m=eig_m.log().diagonal().sum();\n  +#else\n-   float64_t trace_log_m=-11.51292546497021618279;\n  +#endif // EIGEN_VERSION_AT_LEAST(3,1,0)\n  +\n  +#ifdef HAVE_ARPREC\n-   EXPECT_NEAR(result, trace_log_m, 1E-13);\n  +#else\n-   EXPECT_NEAR(result, trace_log_m, 1E-07);\n\n@karlnapf https://github.com/karlnapf this was satisfactory :) CG-M\ngives one step convergence for basis vectors and we get equally impressive\naccuracy as that of direct solvers. will add more tests.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/shogun-toolbox/shogun/pull/1437/files#r5831285\n.\n. Would be good to output dim here too\n. In general, it would be nice to have some very basic math description of the method in the class description (of all classes)\nBut we will go through the doc at the end anyways\n. Pls no output in unit tests\n. Dynamic casts should be checked and the refcount might also do something here\n. Sweet, I like this one\n. agreed! :)\n. wait, num features here is number of dimensions, or am I wrong?\n. yep! why not do it? :)\n. sweet!\n. what about a cleaner way for this? setting reference counting to false?\n. These things should go to SGVectr SGMatrix as constructors (documented)\n. create\n. this is illegal, only \"\" and \".\" is allowed for parameter names (in the near future)\n. The name is fine here but a proper description would be good, copy paste bug probably\n. please initialise _all variables in here and not in the constructor. \nEven if you override the values in the constructor afterwards, doesnt matter\n. this should be done in init\n. please do REQUIRE here with a usable error message\n. also please do not name the parameters with \"m_bla\" but rather \"bla\". The \"m_\" doesnt belong to the parameter name but is rather a style thing we use for members in shogun\n. We willl soon have automagic methods for those, see #1348 \nBut keep this one for now\n. yep, agreed!\n. Do it different. Take the randn value to the power of difficulty. That produces something more challenging\n. Ok don't worry, that precision is fine for now if other things work.\nPlease move the sampler tests to a different place they do not belong here. They should go to trace sampler tests or so. This is just for the log det sampler\n. You had a fixed number of shifts in here. Since this constructor is not available anymore, I replaced with dummy accuracy (0) and set number of shifts to your old value by hand. Did this at a few places, see my last PR\n. The r\u00e4t app sampler will break if the min eigenvalues is negative. Therefore, there should be an assertion in a long the rat app class with a suggestion how to solve it.\n. Again here. You are testing the eigensolver, which does not belonging of this test but into the eigensolver tests\n. Move to trace sampler or colouring tests\n. the real result is available in closed form: sum(log(diag))\nThen the test runs faster\n. This gives a compile error on my machine\n. We has CSGObject::m_parameters first, which fixed it on my machine but did not work on the cluster I use with g++4.4\n. Very useful!\n. Should add some comments in the class description and a not about performance of this since people might abuse it\n. Maybe note that this is essentially and eigen3 wrapper. Also, does this use the permutation stuff by default? Should be noted\n. Good!\n. Nice!\n. Ah sorry I did not see that before.\nCould you rather zip the file directly and then open directly rather than having this archive stuff?\nhttp://docs.python.org/2/library/gzip.html\n. same here\n. some of these things depend on LAPACK, so you will have to catch ImportErrors here\nand execute/return only if import succeeds.\n\nSee other python examples\n. I think you can just try-catch an import Error which will cover all cases\n. SG debug is for debugging and information about algorithm convergence\nshould go under SG info\nOn 19 Sep 2013 10:10, \"Fernando Iglesias\" notifications@github.com wrote:\n\nIn src/shogun/structure/PrimalMosekSOSVM.cpp:\n\n@@ -132,8 +136,11 @@ bool CPrimalMosekSOSVM::train_machine(CFeatures* data)\ndo\n{\n-       SG_DEBUG(\"Iteration #%d: Cutting plane training with num_con=%d and old_num_con=%d.\\n\",\n-               iteration, num_con, old_num_con);\n-       if (m_verbose)\n-       {\n-           SG_SPRINT(\"Iteration #%d: Cutting plane training with num_con=%d and old_num_con=%d.\\n\",\n-                   iteration, num_con, old_num_con);\n-       }\n\nThis might be a good idea. Anyway, in the meantime, we should stick to\nSG_DEBUG :)\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/shogun-toolbox/shogun/pull/1607/files#r6456186\n.\n. it is very hard to see whether something changed here if you touch all lines, pls only change the files at places where you touched them\n. like here\n. Since this is an example, pls dont use this unit-test like style, but rather either generate random data or even load a classification dataset (see other examples)\n. It would be very nice to have the ARD illustrated somewhere, if it works. maybe this could even be the std kernel. But only if not too much effort\n. Same with this one. Should be an example for it and it should also go to the notebook later on. Maybe have a seperate notebook for regression at some point\n. Same here, students T regression with Laplace inference has to be illustrated somewhere\n. cool trick.\n@sonney2k @lisitsyn what do you think about this?\n. pls remove those comments /** TODO ... since they will silent the swig warnings about missing docs. Then we will never find them\n. pls remove any TODO doxygen comments\n. as said, pls handle other float sizes here.\n\nIn addition, pls output a warning if the parameter is no float. In fact, gradient parameters should only be allowed to be floats by design. Adding non-floats should give an error. Maybe this can be done in the macro? Doesnt have to be now. Finish the rest first\n. SG_NOTIMPLEMENTED for other cases, i.e. CT_VECTOR/CT_MATRIX and SGOBJECT,\n. Unit test should be added later. I agree on the rest :)\n. This should be done in base class to avoid reading Un in it memory. Also maybe the default value should maybe be something else than the optimum of Zero?\nsorry, just saw you do that, but there can be multiple solve calla....never mind the comment\n. Print a warning if residuals are not stored\n. Why this Change?\n. Ah yeah I see, SGVector doesnt support int64, its fine then\n. I like it more this way! Then we wont forget about them\n. at some point, this should plot the predictive distribution rather than the 95% error interval. See http://nbviewer.ipython.org/5982623 for code and example of what I mean\n. BTW it is so much nicer to do the modelselection now. Good work!\n. I am a bit concerned here that you only compare the ML, since it might be ambigous. What about comparing the found parameters directly? Should be easy to add\n. Same here, try to compare the found parameters also\n. A general note. Since the optimization is sensible to a lot of parameters, it should be tested with other inference methods, too. But that is also something for later\n. corrected, thanks!\n2013/10/27 Fernando Iglesias notifications@github.com\n\nIn doc/ipython-notebooks/clustering/GMM.ipynb:\n\n\n\"from modshogun import *\"\n],\n\"language\": \"python\",\n\"metadata\": {},\n\"outputs\": []\n},\n{\n\"cell_type\": \"code\",\n\"collapsed\": false,\n\"input\": [\n\"from matplotlib.patches import Ellipse\\n\",\n\"\\n\",\n\"# a tool for visualisation\\n\",\n\"def get_gaussian_ellipse_artist(mean, cov, nstd=1.96, color=\\\"red\\\", linewidth=3):\\n\",\n\"    \\\"\\\"\\\"\\n\",\n\"    Returns an allipse artist for nstd times the standard deviation of this\\n\",\n\n\nMinor typo: allipse -> ellipse\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/shogun-toolbox/shogun/pull/1737/files#r7237644\n.\n. minor: this will never happen due to how you generated the numbers right?\nIt is a bit of an awkward style in my eyes...\n. code style: we dont use { ... } for single line statements\n. as you already touched this, could you add a check that the indices in all folds cover all indices of the original labels? Thats just another few lines here....would be very helpful\n. see above, please avoid this case and remove this check\n. pls remove {...}\n. please dont use classLabels but class_labels\n. indentation\n. same here: could you check that the folds cover all indices?\nOr in fact what about this: add another test that checks this for all possible CrossValidation subclasses....avoid test code duplication\n. sorry about my previous wrong comment, this does check disjoint :)\n. pls use init_shogun_with_defaults() which omits the need of defining this print function (confuses people)\n. init_shogun_with_defaults()\n. Could we maybe move this to the base class? What do you think?\n. I know that this is based on the existing example, but that is a bit outdated.\n\nWe now do unit-testing to make sure things work and examples to illustrate how to use them.\nCould you move all the checks/assertions to a unit test (in the same way you did with the old ones)\nAnd then make the example just a few lines how to produce the folds?\n. pls no printing in the examples. just comment, people can uncomment if they want that\n. Should be commented out.\n. I know this comes from my example, but it is horrible ;)\nThose kind of checks should be in unit tests....\n. There needs to be a return NULL here to avoid compiler warnings and before that, pls put an SG_ERROR(\"View creation not supported yet\")\n. This needs a proper doxygen documentation before anything can be merged\n. So the copy constructor you are using might produce problems which could cause the memory leaks. We have to clean this up before we can merge.\nHave a look into the copy constructor and check\n- which parts are done copy by value rather than copy by reference/pointer\n- which parts are shared between the two objects (is SG_REF done properly?)\n- whether everything is initialised properly\n. is this really done in the subclasses, not in the base constructor?\n. could you use the new SGVector based get_feature_vector method here?\nMakes maintenance easier :)\n. SGMatrix ...\n. can use SGMatrix () operator here\n. and this can be removed once SGVector is used\n. same here, SGMatrix handles memory making code cleaner\n. Nice! documentation :)\n. Nice!\n. nice unit test!\n. again, why not use SGString or SGVector here? then memory handling is in autopilot.... Or do I miss something?\n. Dont copy matrices of the same type element by element, use memcpy\n. It should be possible to do this in place with Eigen3\n. data needs not to be updated since only the matrix is returned right?\n. this centring can be in place for sure with -=\n. realloc with smaller size will just free the remaining memory. So you can do that.\nSo you create an eigen view to the full matrix, which only accesses the submatrix that you want to return. Do multiplication into that one (previous centring can be done in place as said above). Then do a re-allocation of the underlying SGMatrix to the size of the corresponding eigen3 view. You  might have to experiment a bit with this as it depends on the way the matrix is stored in memory (column, or row wise) If all this doesnt work, you can also do the projection vector by vector (should not be much slower), as this will work for sure.\nLet me know if you have questions. And please do proper unit testing of this as its complicated and hard to foresee.\nhttp://stackoverflow.com/questions/9575122/can-i-assume-that-calling-realloc-with-a-smaller-size-will-free-the-remainder\n. I guess thats the transposing back here? Does this really work for all cases N=D, N>D, N<D ? Make sure to unit test all of those (sorry for being so pendantic). But good work! :)\nIf all cases are tested (!) I am ready to merge. @sonney2k  this is now in-place\n. Could you split those unit tests into as small parts as possible (dont worry about code redundancy, but we want to catch as precise errors as possible. Also try to give them clear names\n. This is a bug.\nWe do not use member initialisations in the constructor header.\nThe init function afterwards overwrites anything. The way to go is to initialise everything in init, and then assign constructor parameters after the init call. Also pls register the new member in the init method.\nThe latter will break the integration tests (since a new member was added), so you need to add another data PR, and update the data version in here.\n. Nice, this is a very elegant way now.\nOnly one thing about coding style (this is the last thing Ill complain about ;) .\nThere is quite some code redundancy here, could you simplify this a bit:\nMost of the commands here are shared between the two modes, so pull them out of the if-then-else:\n- processing feature matrix\n- set_feature_matrix call in the end\n- the matrix multiplication call by eigen3 (only transpose afterwards etc is different in modes)\n. Ok then, lets leave it like that\nWe also need unit tests for both cases though. But as soon as the init part is fixed, ready to merge.\n. ok then, sorry :)\n. ok this init one is the only thing missing\n. MS_NOT_AVAILABLE should this be\n. Nice work! :)\nSo, is there a unit test to make this waterproof? Since all tests were green before... ?\n. why the whitespace here?\n. the epsilon cannot be floatmax as this break the modular interface to it. also 64bit should be fine\n. we only do { ... } around a block if there is more than one line\nif (foo)\n    bar;\n. could you put the comment not after the \"else\" ?\n. I see, I in fact forgot that I did that :)\nAgreed!\n. no keep the floatmax if its in compare_ptype\n. haha nice, thanks! :)\n. ok, this is fine\n. please try to avoid those whitespace changes - makes the PR hard to read\n. This output should be removed as it is already displayed in the calling method.\n. Please add proper doxygen comments here\n/* Compares the value of two floats (handles special cases, such as NaN, Inf, etc).\n- Implementation inspired by [link to website]\n- @param a value first to compare\n- @param b second value to combare\n- @param eps threshold for values to be equal/different\n- @return true if values are equal, false if not\n  /\n. This comment is not necessary, as it is explained by the commented out set_loglevel :)\n. Nice those strtold tests!\n. could you add a float64_t eps=0.00001 in all of those tests, makes it easier to read\n. Could you change those here to REQUIRE(condition, \"PCA only works with dense features\") and \"PCA only works with real features\" while you are touching things?\n. Could you cast and stoire the feature pointer once in the beginning to avoid all those lengthy calls?\nCDenseFeatures* casted=(CDenseFeatures*)features\n. While you are on this, could you please implement the two different cases if N>D and D>N.\nThis covariance matrix is D^2 and this is impossible to store for very large D and small N.\nThe other way stores a matrix that is N^2 and is based on SVD\nOr do you want to do this later? I would do it all on the fly as you are touching the code/integration tests anyways...\nSee my issue description at #1876 \n. please dont do this as its going around Shogun's internal memory managment for SGVector, which causes leaks if users are not careful. Rather do \nm_eigenvalues_vector=SGVector(num_features);\n. against which implementation do you test here? Where do the numbers come from?\n. careful, here are tabs and spaces and python doesnt like that. The notebook gives me an unexpected intend error locally\n. Could you add some header for this?\n. In line 456, there is an evaluate call (just above this) which fails when I run the notebook locally. Because the C parameter is not given. Could you fix that?\nI just realised this is because you re-define the evaulate function. Could you either use a different name or fix this problem otherwise? Also, please make sure that you clear all outputs, restart the kernel, run all cellls (to check whether it works) and then remove outputs again before you commit a notebook?\nThanks\n. ok cool, could you add that reference in the header of the test? useuful for the future years when nobody remembers this conversation :)\nYou are right, BRML uses SVD always, but what I was after is the case distinction they have in there:\nfunction [Y,E,L,m,Xtilde]=pca(X,varargin)\n%PCA Principal Components Analysis\n% [Y,E,L,m,Xtilde]=pca(X,<M>,<usemean>,<return only principal solution>)\n% Inputs:\n% X : each column of matrix X contains a datapoint\n% M : the number of principal components to retain (if missing M=size(X,1))\n% usemean : set this to 1 to use a mean for the reconstruction\n% return only principal solution : set to 1 to return only the M leading eigenvectors and eigenvalues\n% Outputs:\n% Y : coefficients\n% E : eigenvectors\n% L : eigenvalues\n% m : mean of the data\n% Xtilde : reconstructions using M components\nif isempty(varargin)\n    M=size(X,1);\nelse\n    M=varargin{1};\nend\nif nargin>=3; usemean=varargin{2}; else usemean=1; end\nN=size(X,2);\nm = usemean*mean(X,2);\nX0 = X - repmat(m,1,N); % centre the data\n[E D]=svd(X0,0);\nL = (diag(D).^2)/(N-1);\nif M<N\n    Y = E(:,1:M)'*X0;\n    Xtilde = E(:,1:M)*Y + repmat(m,1,N);\nelse\n    Y = E'*X0;\n    Xtilde = E*Y + repmat(m,1,N);\nend\nif nargin==4; if varargin{3}; E=E(:,1:M); L=L(1:M); end; end\n. They do either always a SVD on X0, not on the covariance matrix. This is way more efficient than our version and the way to go. Then, depending on which dimension/N is larger, they reconstruct differently.\nDoes that make sense?\n. See 15.3 High Dimensional Data in the book\n. Exactly! The trick is to do a sensible guess in the code and then switch mode. In fact, you could also add an enum for the method to use, or auto which decides based on input data.\n. ah nice that you thought of these!\nthere are also the graphical python examples, in case you have not included them, but still going through the pr\n. We are moving to BSD, so please use the template in QuadraticTimeMMD.h :)\n. this one might be critical when there are different number of points, could you just use the number of points in the joint vector? Then this is already ready for the future\n. the comment is not really true anymore. nothing is merged, just the q samples are permuted. second part that you wrote is correct\n. Exactly like this! :)\n@sejdino do you confirm? \n. MMmmh this is a bit reduntant to TwoSampleTestStatistic. What about having another abstract base between test statistic and two sample / independence which is called two-distribution? Or rather have duplicate code and a simpler class hierarchy....I dont really know. \n@lambday @sejdino thoughts?\n. see #1919\n. Interface looks good!\n. ???\n. no merging\n. I think this one should be for both p and q, then range fill, then permute the first m.\n. Maybe dont re-write the math here but just add a link to base class where its described?\n. Could you change things to be consistent with Shogun style here?\nmemer variables have a m_ prefix and avoid these ugly underscores in the parameters of the methods.\nm_method=method;\nm_thresh=thresh; etc\n. pls say that d is dimension and n ist number of vectors, (Its clear but always good to state)\nMethod used for PCA should be \"Matrix decomposition method for PCA\"\n. please add a clear documentation in the doxygen \"@param\" here. Its really important to tell users what to do if there are that many options. Also, I think that the constructors should be splitted.\nthreshold is only needed if the method is actually threshold. So have one constructor where there is no mode parameter but only the threshold\n// mode is threshold\nCPCA(bool do_whitening=false, float64_t thresh=1e-6, EPCAMethod meth=SVD, EPCAMemoryMode mem=MEM_REALLOCATE);\n//mode is fixed number\nCPCA(bool do_whitening=false, EPCAMethod meth=SVD, EPCAMemoryMode mem=MEM_REALLOCATE);\nAlso I changed the order of method and memory mode since people want to change the first one more likely\n. And btw I would still have AUTO as default. Shogun users are used to eigendecomposition PCA, if n is large, so that should stay the same. \n. m_mem_mode=mem_mode would be best here :)\n. code style. Could you define the i variable locally in the loops rather than having the one in the front. That is a bit weird to read\n. we have a 80 character margin rule in shogun. could you line break those?\n. nice!\n. margin\n. This requires memory for the old matrix and the new matrix at once for a short amount of time.\n. I love this description! Nicely written\n. Could you add a comment on the do_whitening thing?\n. a few words what that actually does?\n. CPCA(bool do_whitening=false, float64_t thresh=1e-6, EPCAMethod method=AUTO, EPCAMemoryMode mem=MEM_REALLOCATE);\nspecifying the threshold should automatically set the mode to thresh. Maybe say this comment on top\n. mode is not a parameter here, no need for doc\n. could you say what whitening does?\n. Could you change the license of the unit test to the BSD one? see CQuadraticTimeMMD for a template\n. could you for all those unit tests add a small comment against which implementation you compare? \n\"Comparing against MATLAB XYZ implementation\"\n. No it cannot be put there as it all depends on having exactly to sets of points. This will for example not be the case in the three variable interaction tests. So I think I vote for another abstract base in between\n. for later: the %s::method_name is not needed anymore, we can do that automagically\n. just \"leaving\"\n. yep :)\n. just \"leaving\"\n. m_num_sample_iteration is a bit of a weird name. What about m_num_null_samples ?\n. all this %s::blabla stuff again. Sorry to annoy you with those cleanups ;)\n. haha, indeed, style matters!\nYeah switch would be better, I agree.\n. We do! But just block-wise :)\n. English grammar is \"has_parameter_hash_changed\" but I go for \"parameter_hash_changed\"\n. no need for %s::method_name.\nJust \"entering\"\n. just \"leaving\"\n. \"entering\"\n. \"leaving\"\n. style: one line blocks dont have { } in Shogun\n. no { } since only one line block\nbtw could you also respect the 80 char margin?\n. no {}\n. parameter_hash_changed\n. no {}\n. I am a bit concerned about memory here. If there are features attached to the kernel, they will be cloned too.\nThoughts on that?\n. same here. Memory blow up?\n. \"entering\"\n. \"leaving\"\n. { } around the inner for\n. btw. We are planning to change license to BSD. If you would like to, you can change all GP headers that just you wrote (like unit tests for example) into the template of CQuadraticTimeMMD.h\n. Great work! Important that preprocessors work with GPs\n. Yep. Pretty_function will do the rest using black magic :)\nOn 2 Mar 2014 21:34, \"Roman Votyakov\" notifications@github.com wrote:\n\nIn src/shogun/base/SGObject.cpp:\n\n{\n-   uint32_t new_hash = 0;\n-   uint32_t carry = 0;\n-   uint32_t length = 0;\n-   SG_DEBUG(\"entering %s::update_parameter_hash()\\n\", get_name())\n\njust replace with SG_DEBUG(\"entering\")?\n\nReply to this email directly or view it on GitHubhttps://github.com/shogun-toolbox/shogun/pull/1922/files#r10197629\n.\n. What about detaching the features from the kernel (keeping a reference to\navoid deletion), clone kernel and then Re-attaching them to the clone?\nAlso have you profiled the memory footprint of this? How long forbid the\ndouble men necessary?\nOn 2 Mar 2014 21:31, \"Roman Votyakov\" notifications@github.com wrote:\nIn src/shogun/machine/GaussianProcessMachine.cpp:\n\n@@ -63,7 +63,10 @@ void CGaussianProcessMachine::init()\n        feat=m_method->get_features();\n// get kernel and compute kernel matrix: K(feat, data)*scale^2\n-   CKernel* kernel=m_method->get_kernel();\n-   CKernel* training_kernel=m_method->get_kernel();\n-   CKernel* kernel=CKernel::obtain_from_generic(training_kernel->clone());\n\nI'm very open for discussion, but i still have no ideas how to avoid this.\n\nReply to this email directly or view it on GitHubhttps://github.com/shogun-toolbox/shogun/pull/1922/files#r10197612\n.\n. AFIK valgrind can do this. Like max amount memory used during runtime.\n\nBut you see the problem that cloning the data is not a valid solution?\n. Minor (for later, doesnt prevent merge): Could you write all math expressions (X, X', XX', UDV') in latex?\n. This is still weird, specifying a threshold in the constructor should automatically set the mode to THRESHOLD, no need for the mode paramter\n. But maybe I miss something here. It's not too important anyways.\n. pls no assertions in unit tests\n. I dont know why this is assert ,but should be expect\n. what about a default parameter value?\n. This should have a default value, and it should be properly documented that this is only for specific caces\n. sg_io->set_location_info(MSG_FUNCTION)\nobject.io.set_location_info(1)\nor\nsg_io->set_location_info(MSG_LINE_AND_FILE)\nobject.io.set_location_info(2)\n. Also, why is this assertion here?\nShould this not maybe be in a unit test rather?\n. Ah annoying.\nSo this call does not recompute the kernel?\nSGMatrix<float64_t> k_trts=kernel->get_kernel_matrix();\nI don't see this. Why do we have to clone the features. There must be a way of avoiding this somehow.\n. Could you rename m_num_features to m_num_data or m_num_points? its not really features...the name is misused in Shogun unfortunately \n. Also, do we really need to store that? I dont think so, as we can always ask the m_p m_q objects to tell us\n. Pls do that seperately and say \"Features p must not be NULL\"\n. same here, pls seperately and say that they should not be NULL\n. redundant comment: I think this is not needed\n. Or like that :)\n. While I agree it would be great to have an abstract class for Eigensolvers (in fact we have a base class for that), Lanzcos is not good here since it is iterative and more suitable for large sparse matrices (we dont have that here)\nHowever, I would love to see\n- an Eigensolver base class (general, the existing class in linalg would have to be modified slightly)\n- A subclass for Dense Eigen problems like here in PCA\n- that is used for all of Shoguns Eigenproblems (like PCA)\n- that can be changed easily to run problems on GPU with a global call (dont make this a parameter but a global flag for dense eigenproblems)\n. @vigsterkr will write an entrance task issue on that :)\nLooking forward to see this! :)\n. ok lets merge this for now and then discuss.\nI would just like to avoid cloning data in memory as then you need twice as much as before.\n. ok, great!\n. new CSerialComputationEngine();\n. ()\n. Agreed!\n. And document this in the constructor comment\n. Both of those libs are actually bundled (downloaded) via cmake automatically, so no need to install manually, and we should say that!\n. pls respect the 80 character margin here\n. Pls add a BSD copyright at the beginning of the file. There is a template in the file CQuadraticTimeMMD.h\n. in Shogun we use this naming convention for functions and methods \ngenerate_data_n_less_d\n. if (asdasd)\n(whitespace)\n. no {} for one line statements\n. @return ?\n. all parameters should be documented. And a sentence or two what the method does would be also helpful!\n. doxygen parameters\n. Could you try to be consistent with where you put the * for pointers?\n. pls document all parameters of all methods. Could you double check that for all new implemented methods?\n. this one should be ID3TREENODEDATA_H__\n. please dont register and initialise variables directly in the constructors but rather introduce a private method register_parameters, or init for this. It is easily forgotten otherwise if new constructors are added\nSee other classes for examples\n. Some more details are always good in class descriptions. What can the node do? What is the interface representing? How is it embedded?\n. Same here, more details would be very useful. \n. pls no camelCase but camel_case\n. ???\n. well, I agree, that slipped - although it's not really severe or do I miss something?\n. Same here, doesnt hurt anyone, in fact many unit tests have those.\n@dhruv13J could you fix those minor things? Thanks\n. it was commented out right? :)\n@lambday feel free to fix that one.\n. @mazumdarparijat You are only allowed to change the license of a file if it only contains my name. Otherwise you have to ask people. Though here you are lucky since Chiyuan already agreed on re licensing into BSD.\n. When printing error messages, always give information of what the wrong value is\n\"Number of children (%d) must be smaller or equal to 2\", m_children.size()\n. no {} for one liners\n. Same here, ginve information, and try to be consistent in your error messages\n. pls add a newline here\n. see other comments\n. no newline here\n. no {} \n. no newline\n. see other comment\n. no newline\n. yep, if doing so, be careful about reference counting. Getters in this array increase the count\n. @vigsterkr I checked the list, he is fine with us changing. Still we should not do this now, but later globally.\n. pls use BSD, see for example CQuadraticTimeMMD.h for a template\n. why would this solve a memory problem?\nAnd if it does, there is a bug, since set_features references the features from clone\n. Maybe the clone is to make sure that the original data was not modified?\n. Do you want to do this under BSD?\nLike in CQuadraticTimeMMD.h?\n. all members should be initialised (and registered) in the init() method. If constructors want to change something, they can to it after the init call.\n. Pls no asserts. REQUIRE(num_labels>=0, \"Number of labels (%d) must be non-negative\", num_labels)\n. pls dont do arrays of SGVectors if the number is fixed (as here)\nDo a SGMatrix, which makes code much easier to read and maintain (no new/delete, pointers, etc)\n. Ah I just realised they dont have fixed length. Maybe this is the best solution then. It's really annoying that we do not have a proper data-structure for such cases in Shogun\n@sonney2k @vigsterkr @iglesias @lisitsyn what do you think about that?\n. Pls no printf, but rather SG_PRINT\nPls no if (foo) SG_SPRINT but rather print in debug mode\n. REQUIRE with proper information to the user\n. REQUIRE\n. we dont use {} for one liners, and if we do it is\nfor (foo;bar;bla)\n{\n    asdasdasdasd....\n}\n. Can't we use a different data structure here?\nThe whole point of introducing SG-types is to get rid of those annoying double pointers and length variables\n. no {}\n. why this mclass_t type?\n. no {}\n. This should be done with SGVector, not with MALLOC/FREE.\nAlso SGVector has a method ::zero() which again makes code much more easy to read and shorter\n. pls put the { into a new line\n. REQUIRE - its user given input, so we need proper error messages\n. { into new line\n. pls respect the 80 character margin\n. REQUIRE\n. no {}\n. SGVector\n. no {}\nbtw your editor should do the formatting for you\n. SG_PRINT\n. we have serialisation methods for all registered parameters.\nPls dont introduce new methods for that but rather use Shogun types and register them, then serialisation comes automagically, in a tested, reliable, and unified way\n. pls avoid using STL headers where possible\n. same here, pls do not re-invent the wheel. This is pure horror to maintain\n. BSD?\n. no-go, either this does into DataType.h but I prefer to not use this at all\n. None of this double array stuff will work properly in the modular interfaces, so pls use a Shogun type\n. no TODOs allowed in merged code ;)\n. BSD? :)\n. please dont use strings to compare the context, have seperate methods with different names for that. Too messy to maintain otherwise\n. as said above, please have a sperate method for that\n. 80 character margin\n. pls no TODO :)\n. this can be deleted\n. SGMatrix? or something else? Not too important here as its a unit test, but we usually try to avoid this style\n. no {}\n. This unit test doesnt really test anything, but I guess checking this with valgrind is a good idea\n. Pls add some comment here that the input is not checked to be valid (since we dont want to spend all CPU time checking those things)\n. new line before comment\n. since this is streaming MMD, pls add the reference in class description too \n. ah its already there :)\n. mention that this is the std value somewhere\n. purely virtual method?\n. pls use CDenseMatrixExactLog::get_name() and similar here.\nAlso, the default should be sparse matrix, serial computation, probing sampler\n. Also mention the used solvers here\n. nice! but same, pls mention solvers and all default initialisations here\n. Don't do asserts in unit tests unless the program crashes if non-true\nEXPECT_TRUE\n. why do you put all the data if you dont test things at the end?\n. document them in doxygen style\n/* asdasdasdasd /\nm_tree_depth\n. and please add an m_prefix to all class members variables\n. same here, doxygen style comment pls\n. Just seeing those asserts,\ncould you pls make them a \nREQUIRE(mch, \"Instance of %s could not be casted to COnlineLibLinear\\n\", node->machine()->get_name())\nalso add a REQUIRE(node, \"Node must not be NULL\\n\")\nand REQUIRE(node->machine(), \"Node's machine must not be NULL\\n\")\nbefore the cast.\nJust makes things nicer for users\n. Would it be possible to split this very large test into multiple smaller ones?\nAlso make sure to cover all cases you can think of (including initialising with defaults, etc)\n. Agreed!\n. No I mean why not make it purely virtual. StreamingMMD will never be instanciated\n. Why do you do this by hand? Should be done with Shogun's PCA class\n. all those get methods should access the pca instance\n. Could this dataset be added to shogun-data?\nIt is not an option to download it by hand\n. We have a template for ipython notebooks, pls use the style suggested in there regarding headers, title, author list etc.\n. Pls use proper latex math (no * for example, no $const$ but $\\text{const}$, no dim but \\dim etc)\n. Could you write some more background on this. There are loads of excellent textbooks about this to take inspiration from. At least define the objects you are dealing with, and the problem we are solving, and its solutions\n. This is wrong grammar. Also, what about \"Let's make this work using Shogun\"\n. First of all, whats the toy data. You should describe that and the expectation on it\nSecond, this toy data is absolutely meaningless for PCA. If your goal for the 1D projection is to be this line, pls actually generate data from the line, and then add noise in the orthogonal directions. Then PCA will recover the original line. This dataset makes no sense, sorry about that\n. there are numpy functions for that btw, no need to redefine\n. There should be a story be told (in markdown cells, using mathematical descriptions of whats happening), including details about what Shogun does, how it does it, what are the involved classes etc.\nNot just dumping code. Have a look at the Gaussian Process notebook for example\n. doesnt PCA already do that?\n. pls describe the methods for PCA in the notebook text\n. Please make the title shorter, and in correct English\n\"PCA Projection of 2D data into 1D subspace\"\n. same thing here. The dataset makes no sense. Generate from a plan and add noise in a third direction and rotate to get something that makes sense\n. Like all those pancake plots in the references in the issue #1878\n. Could you state a reference here?\nAnd please be mathematically a bit more thorough. What are the objects we are dealing with here?\nWhat are we doing? The text is already good, but definitions are also important\n. Downloading data is not allowed ,should be in shogun-data, but see https://github.com/shogun-toolbox/shogun-data/pull/36\nwhich might be merged soon\n. again here, please tell a story of what you are doing in the markdown cells, not just in python comments\n. Nice plot! :)\n. I like the application, but again\n- details on what the objects are in math\n- what classes are you using, links to reference (see other notebooks, such as GP)\n- pls do proper typesetting\n. This constructor is not needed. Log-determinants of dense matrices will not be computed with this framework as it makes no sense at all. This is done with Cholesky decompositions of the dense matrix. If I can store the matrix, I can store a Cholesky. The point of this framework is that for sparse matrices, one can store the matrix, but not the factorization which is why we have to do all the approximation magic\n. could you remove the newline before #else\n. SG_REF (for all getters, important)\n. Please be consistent here and only use class names, also for the engine, and something like \"Using the default methods: \" in front of the list would be good\n. Pls make this \n/* @return trace sampler /\n. same for the other getter descriptions\n. seperate EXPECT TRUE pls\n. why that?\n. test can go\n. no {} for one liners\n. are you sure you want to use SG_MALLOC here?\nIt should either be used everywhere in liblinear or nowhere.\nIs SG_MALLOC used in other places?\n. Ah my bad, sorry I got confused ;) Nevermind\n. SG_MALLOC allows to trace all allocated memory blocks in shogun, which sometimes is very useful. everything should be SG_MALLOC, except for new CSGObject instances\n. You cannot put monica here and relicense. You have to ask her, or just rewrite all her stuff on your own\n. Which I think is fair\n. \"Data ..:\"\nAnd please add \"\\n\" to the end of all messages\n. No need to put this into a seperate file\n. I think this include is not needed\n. I agree on this!\n. Maybe even into CStatistics\n. forgot a \"\\n\" here\n. Forgot a \"\\n\" everywhere :)\n. Pls do some type checking here to make sure its really 64 bit. Either via dynamic_cast or via the feature type\n. same here, please ensure type safety of m_labels\n. these two nested while loops look dangerous to me (and hard to read)\nCould you replace this with something less scary? Like for loops? :D\n. whitespace before (\nsame below\n. pls check typesafety of casts everywhere to turn segfaults into run-time errors\n. Nice, I like this description. Will have to read it from html once merged :)\n. very nice test!\n. pls put the implementation into the .cpp file\nAlso, would you write a little unit test that ensures this function really does what it should? Always helpful\n. This one can be BSD. The old file is written by me and I agree :)\nBut my name should be there!\n. With all of those minor changes, are you sure that this really helps? c++ compilers can optimise those things quite efficiently.\nAnother point I am scared by is that things stop working - some of the methods are not explicitly unit-tested. Feel free to add such tests, although I know its lots of work and we have tests for final results (which all passed your patch)\n. Please use BSD license from now on, you can change all license in files that you touch that only me and Roman are listed in (so this particular file is not allowed to change yet, but Ill ask Jacob again). See QuadraticTimeMMD.h for a license template\n. newline before {\n. we do a whitespace before any { ( ...\n. As this class is written by you, it can be BSD.\nThe other names dont need to be in there\n. This should be updated to say a few words on BFGS with references and implementation references\n. all CSGObject members should be there as pointers\nThis means you have to\n- initialise them (add a private method void init() where you do that (and register them afterwards) and call this method from all constructors, see other Shogun classes for how to do that.\n- do reference counting\n. doxygen comments would be missing here if that was to be merged\n. Thats fine if things are private and only used internally.\n. please dont do comments in code lines, put them above\n. if (foo)\n{\n a=0; \n b=0;\n}\nelse\n{\n bar;\n}\nAnd we dont use {} for one line statements\n. Sweet!\nPls also add a SG_WARNING(\"Error during L-BFGS optimization, using original Newton method as fallback\\n\")\n. careful with cloning, that usually creates deep-copies (just saying)\n. license!\n. noooo, never to this.\nlog_sum_exp always should be done with the log sum exp trick. This will create underflows.\nSee CMath::log_sum_exp\n. switch would be nicer here. But not mandatory\n. Same as above. never exponentiate numbers that are probabilities. Always work in log-domain\n. and here\n. and here\n. Is this really the most effective way of doing this? There are so many loops, and those if statements inside are brutal for the performance. \n. license\n. not implemented? I am unsure why this is, could you explain?\n. same here ?\n. license\n. nice! :)\n. pls do the math in latex if you are already touching it, doxygen can read this\n. same here\n. just to annoy you a bit: This should be \"while (cp_ptr)\" so with space\n. As said, while its nice to have such scripts, its absolutely impossible to maintain this. We cannot include anything else than the correct numbers in our unit tests. Please delete the file. Maybe you can start a discussion on this on the mailing list, but current policy is just magic numbers.\n. Same here, just put a reference to the file, a link or so. We cannot maintain this.\nAlso, this is GPL, which we cannot include anymore\n. we do a newline before any {\nPlease update that everywhere (or update your script to do the formatting)\n. Can we use a Shogun type for the int here? Any public method should only use Shogun base types\n. double check that everything is initialised here.\nAlso, please SG_ADD all parameters, thats important for serialisation\n. your indentation is broken here (and other places) please fix that\n. indentation\n. this is still there\n. Could you use consistent comments?\n/* */   vs //\n. using namespace Eigen\nMassively clean up the code here. But only do this in .cpp files\n. formatting messed up\n. ok, but has to be before its merged\n. indentation, whitespace\n. please dont use initialiser lists, but a private init() methods which initialised and SG_ADDs.\nThen the initialiser list cannot be used\n. please dont do such things, commented out code, etc. Cannot me merged\n. docs\n. BSD please, also thats your name on here\n. indentation\n. please dont do this, just the numbers, as argued above\n. formatting.\nYou dont have to align the function parameters btw, just 1 indentation level\n. Can you change that in your formatting script? We usually dont do the {} in one-liners (although I think its better to always do them)\n. policy is to not have whitespace in between operands, apart from && || \n. grrrrrrrr (not your fault)\n. newline would be nice here\n. We will hopefully soon get rid of such annoying #defines via our linear algebra interface, which will do all of this in a central place and also compute eigendecompositions using multiple CPUs or GPUs :)\n. Could you check for failure here? Eigen3 migh freak out if the matrix's smallest Eigenvalue is slightly smaller than 0 on a float64 machine\n. Ok, will do that after merge since its impossible to read this here.\nI guess I will review everything with Dino once we dont change anymore\n. haha, you will be surprised there :)\nBut more on this later\n. whats all these newlines doing?\n. please use consistent filenames (with the rest of the examples)\n. We dont use this one, use Shogun functions instead\n. Sorry to say, but this file is a mess, and far from being mergable:\n-wrong formatting (See my other comments on style)\n-not well structured, what are all those function definitions doing, why do you even need them for simple operations such as plus/minus/times?\n-the bitmasking for the mode is not how things should be done, rather use Enums like in other Shogun classes\n-standard normal PDF thing should be in CStatistics, in fact, there already is something similar, same for cdf/etc. Those things should be put into some helper class in a way that they can be used from elsewhere easily. No need to re-invent the wheel all over again\n-assert should be REQUIRE, see other files\n-Nothing is documented\n-This is not an example, but rather a module that should go into some helper class, such as CMath, CStatistics or similar, we can discuss that\n-This is an example, examples are just to illustrate API usage, things such as EXPECT_NEAR should go into corresponding (simple) unit tests\n-many more problems here. Your work here is appreciated, but please clean this up first, and send it in a seperate PR after the previous one is merged. You seem to be adding new things to the PR rather than addressing the old issues. Let's keep things simple and get your previous codes merged before attempting this one.\n. no readmes please this is self explaining.\n. Plotting bounds is nice to have, but I would say the least important thing of all from the python codes ;)\n. This method does not really make sense to me. What do you want to do here?\nTo which part of MH does this correspond?\n. proposal points? Those are just samples from a distribution, no need for an extra method\n. The interface here doesnt make sense.\nWe want\n- a method to compute the log likelihood for a number of given data\n- if the distribution stores data internally, we want a method to compute the log-likelihood of all those data\n- See for example the apply() methods of CMachine\n. I guess thats taken from the original distribution class.\nLet us not include this for now and start with the basic things\n. ?\n. We use 4-character tabstops for indentation btw, see any other shogun file\n. All those tests should not be in a new file but rather in the existing GaussianProcessBinaryClassification_unittest.cc file, as its the same class you test. Just with different inference method, so the names of the tests can stay the same, but pls move them to the other file.\n. why that?\n. nono its fine, consistent is good! I was just wondering :)\n. why the small number here? I guess to prevent negative numbers - thats ok in principle but a bit ad-hoc.\nWhats the justification? Why can eigenvalues even be negative (should not happen, and if it does, should be prevented so that we dont need this here)\n. Now that I see this: \n- SGMatrix has the () operator implemented, so no need for this hard to read indexing\n- Never copy matrices element by element, inefficient, rather use memcpy (if possible). You can either copy vector wise if their dimension changed, and if not, you can do one memcpy call for the whole matrix\n. unused import\n. os.sep rather than \"/\"\n. where do those images come from?\n. os.sep, and there is also a cross-platform version of \"..\"\n. whitespace\n. we cannot really assume opencv to be installed. numpy can also read images\n. Okay, I see. Sorry I did not think about this properly before.\nOk then, do this, but make the ridge you add a class variable that one can set (not through constructor though, give it a default value). And then check whether some eigenvalues are too small, and then print a warning in which you suggest to drop that dimension via PCA.\n\"Covariance matrix has zero Eigenvalues at dimension %d. Consider reducing its dimension. Adding %f to avoid numerical problems\"\n. Ok, you are totally right then\n. I think this here can also be done with Shogun, right?\n. I dont really like these ############\nRather put a newline to seperate\n. please dont exceed the 80 character margin to much. \n. mention here that OpenCV must be installed\n. please use consistent comments\neither \n\"#comment\"\nor \n\"# comment\"\n. license is missing\n. Could you add some latex here?\n. The order of the doxygen parameters should be the same as in the code\n. \"Variational expectation not implemented for %s\", get_name()\n. You do not need to mention the parameter here. Use an error message similar to the above\n. this guard should be before the include\n. why do you use a reference here?\nAlso name it \"set_bound\"\n. nope thats not allowed :)\nRather not implement it at all then, the base class will return error anyways\n. same here\n. Start with capital letter \"Variational\"\n. Could you add a latex description of the maths behind the method here?\n. I am not sure why this is a subclass of LogitLikelihood. Is it really a likelihood model and will be used under this interface?\n. This is inheritet from base class, you dont need this again\n. Math and details please.\n. Same here\n. Its very important that you initialise the added variables\n. typo still\n. Could you please do all comments in the same way\nAlso, please do not do comments in code lines, thats ugly, use seperate lines for them\n. No need to mention GMM here I guess. \n. Training is done via the classical Llyods ....\nThe current formulation is a bit weird :)\n. a real world dataset. Furthermore, the effect\n. This is not really about density estimation, but rather to estimate likelihoods of such densities. This is an important difference.\n. unsupervised should not be capital\n. Please say that all algorithms are Kernel two-sample testing and are based on embedding probability distributions into Reproducing Kernel Hilbert Spaces.\n. Maybe even mention the Maximum Mean Discrepancy\n. ?\n. please make it BSD. See CQuadraticTimeMMH.h\n. passing SGMatrix around does not copy the memory, only the underlying structure that holds the memory. So we do \"copy-by-value\" for those (fast enough)\n. This is not our license template. Pls see the above file\n. haha nice :)\n. Could you seperate these unit tests? I know I wrote it and am the person to blame, but thats quite some time ago when we did not do things properly :)\n. maybe its worth the seperate those too? What are your thoughts on this?\n. Its not really Maximum likelihood, just evaluating/estimating the likelihood\n. SGMatrix instances are always created on the stack. \nm_bound=SGMatrix();\nAlso, SGMatrix instances are never stored as pointers (heap), but directly (stack)\nSGMatrix<T> m=SGMatrix<T>(10,10);\nAlso no need to reference them, that all done automagically.\nAnd when you SG_ADD them, you do not need to cast to anything (see other Shogun classes which have members of SGMatrix or SGVector\n. Thats nice since get_column_vector actually returns memory pointer. Please add a comment that this doesnt do any copying here\n. Note that once you change the type of m_bound from SGMatrix<T>* to SGMatrix<T>, all the method calls will be with \".\" rather than \"->\"\n. All this ugly stuff will also be simplified!\n. please never start method names with an underscore.\nAlso, I dont get the purpose of this method. You can do this check where you call the method, also why does it return the variance? Use a getter for such things\n. CMath::max ?\n. Sure you wanna do this locally?\n. I am confused here, do you plan to actually make people use this class as a likelihood instance? If yes, that its fine that its a subclass. If not, then we should re-think this\n. Yeah this should be in the code base. But again, this is just an operation on vectors matrices, you you can just use eigen3 here. No need to have a method. Later, this will be coded against shoguns linear algebra interface\n. this method should be in CStatistics, also I think it is already there\n. same here\n. this should not be a function, as said before\n. please choose better names for unit testing, also please add a few more meaningful cases\n. What about just adding the default parameters to the existing method? There seems to be some ambiguity here\n. Code style: newline before {\n. Such checks should not be called anyways. If the user provides data, then you should make sure you check that the data is good (i.e. positive)  But in this case, you are just checking your own code. Rather make sure the variance is always positive then checking it. Imagine a user gets this error \"Variance should always be positive\" and this is not caused by wrong input. This doesnt help at all and just confuses.\n. My opinon on this is the following:\nIf the Shogun user has to use your class as the likelihood model, then this should be a subclass. However, if this class is only a tool that is used from a certain inference methods with a certain likelihood function, then it should not be a subclass. How do you plan that this class will be interacting within our GP framework?\n. I do neither see the comment nor SG* datatypes on stack.....?\n. I do neither see the comment not SG* datatypes on stack .... ???\n. yes, eigen3 for now pls\n. This should be done via Shoguns representation of the Gaussian distribution. Instantiating the class might be a bit too much here, but you could for example add a static method to CGaussianDistribution\n. see log_pdf_multiple method\n. yeah I think so\n. Yeah, same thing. Let us use clear Shogun-style code here.\nPlease let us focus on getting this thing merged before talking about new things. Its good to have an idea for future stuff, but we shoiuld do things one by one. I agree that its quite a big task to use the bound for the KL method (that is also not really asked, right?)\n. yeah I agree actually\n. not to rewrite this for float and double is exactly the point of templates\n. Is this data version change for the movie data? Is this already in the shogun-data repository?\n. some comments on what you are doing here might be nice. Like a rough description of the data format and your  ways to read it\n. newline\n. newline\n. Could we try a different kernel width than one here? That ensures both implementations use the same parametrisation of the kernel. We had cases where this 1 width was produced by two different ones\n. is the same noise used in the matlab implementation?\n. why do you do the try catch here if you just print the error message?Not necessary\n. I think we dont need a main if eigen is not defined\n. This code allocates memory (which is freed once the vectors is not referenced anywhere anymore. Could we be able to re-use memory in between all those calls of this function? That will be much faster then\n. You could for example enforce that the matrix is already allocated, and then overwrite the old elements in every iteration\n. Most of the GP modules can be pulled out of the loop here. This avoids re-allocating them all the time\n. typo \"default\"\n. One question: Do you plan to call this method in a loop?\nThen it might be good to also have a vector version of it that computes log-pdf for an SGVector of Xs\n. No need to change the data version here\n. transpose().colwise() ?\nWhy not rowwise?\n(Just asking, maybe I dont get something here)\n. pls add a whitespace here, the strings are concatenated\n. I do not really get this check here.\nOnly true if name is \"mu\" or \"sigma\" or both ???\nYou should maybe put a comment here why you do this and give a better error message for users. This one might be confusing. Tell them that you only accept certain names (and the one that is given as you already do)\n. whitespace\n. pls put a comment here which case this is\n. Pls put a comment in the beginning of the method saying what these matlab code lines are supposed to do (where are they coming from, etc) just a small notice so that later developers dont get confused\n. remove this newline\n. Could you put a notice what this code is based on?\nYou have these Matlab comments in the implementation, it is worth mentioning where those are from etc for reference, not in the doxygen documentation and the license at the file header\n. Doxygen should contain paper and code reference\n. Please also document proivate elements in doxygen style with/* foo bar /. Very careful about chaning this. \nIt is fine to use {0,1} internally but always make this very explicit. To the outside of the class, the Shogun std should be used\n. Document this method (as mentioned above)\nClearly state what it does\n. pls remove the newline between the Map declarations here\n. no double newline\n. again, pls say in the beginning what this matlab code is. Just makes later developers life much easier\n. no double newline\n.CMath::abs(...)==CMath::INFTY` reads a bit nicer\n. I never agree on taking exp of probabilities. This might cause underflows. Either make sure (in the Math) that this never happens (for example if you know you have a prior avoiding small probabilities) and explicitly state why underflows will not appear, or check for them. But usually implementations do not need to take exp of probabilities. If you do, also explicitly state why (in a comment). Important to scale things up later on\n. I just realised this INFTY business is your check here. Maybe double check my above comment anyways :)\n. {} for the outer for loop\n. Are you sure this is a good idea? inverting matrices usually can and should be avoided. Rather precompute a decomposition (QR, Cholesky, SVD) and then solve system with those. Much more stable and also faster\n. positivity check for v? Or not needed? If not, state in a comment\n. again this transpose().colwise() that confuses me\n. pls try to avoid double newlines\n. Could you also comment on the memory requirements of this class?\nThere are quite a few matrices stored .....\n. And runtime etc, or point to reference\n. double newlines\n. This could be a helper function in CMath, then it doesnt need to be redefined in all the unit tests\n. pls add a comment where those numbers come from\n. double newline\n. which matlab code, pls mention this somewhere or point to class documentation\n. no newlines\n. So the variational distribution is a Gaussian? I know this is a kind of stupid question, but pls make these things clear\n. And whats the complaint? I mean its fine like this, I am just curious\n. I cannot see anything you mark as done here. ???\n. So if it is Gaussian here, mention this in the doc\n. Pls don't do this \"done\" thing. That just blows up my emails. Just do all changes, update the PR. I will be notified by this. Never send PRs where not all comments have been addressed, and if, then start a discussion. We want to keep communication clean.\n. I think both should be fine. Since transpose is done without changing memory, we dont loose speed here. So OK for first version\n. We could do this later maybe, if you want, you can file an issue for a discussion with @emtiyaz \n. As said, all methods (also private) should be documented via doxygen. Even precompute and init\n. as said, pls also put a proper citation here\n. You have to put a NULL check for the strings, otherwise the error message will cause a segfault\n. ASSERT is only for internal checks, if there is the slightest possibility that a user sees this, it should be REQUIRE with an error message what happened (and how to avoid)\n. no newline here\n. pls also in the doxygen\n. what about a newline between every of those blocks of similar code?\n. ah I am sorry, of course\n. Agreed!\n. About the issue with 0. Please put a source code comment here. (as you commented: \"Usually we use pdf in log-domain and the log_sum trick to avoid numerical underflow in particular for IID samples.\")\nSuch comments are very helpful later on\n. what about having only one transpose on the result (swap the order to do that) easier to read (no efficiency gains)\n. Please write down the math of whats happening here. This is kind of not saying anything. API documentation is very important!\n. This is good! But pls always separate things in PRs. You can always send a whitespace PR. I know this might sound a bit ridiculus here since its only one change, but its good practice in general and allows to merge faster.\n. This should be documented in doxygen (including the math in latex)\n. initialisations should go into the init method (as well as registering parameters)\n. the num_vectors should be set in the set_feature_index\n. pls make naming consistent, i.e. feature_index\n. this copy constructor should copy the vector, this uses the same memory which is not what you want there. In fact, you can just avoid copy constructor for now\n. Is this method really required?\nIf yes, the its good to use the copy constructor, or even the clone method\n. reasoning here?\n. note in the documentation that this creates new memory if there is a subset and returns vector using existing memory if no subset if there\n. again, please do consistent naming. Member variables should start with \"m_\", and the parameters of methods should be named similar\n. ??\n. You can also put your name here (and nickname also) if you want\n. that have contain? Thats not proper English.\nAlso, you should write a bit more why this class is there, what it can do etc\n. this doesnt fit, pls be a bit more careful with those things\n. always document what the thing does (copy vs referencing)\nAlso, think whether this method is really needed\n. doxygen @param!\n. better documentation and pls document return value and parameters\n. ????\n. doxygen (I wont comment this from now on, pls change everywhere)\n. not really in-place but \"returns the actual index vector memory, a newly allocated vector otherwise\"\n. vlen?\n. document also private methods\n. m_feature_index\n. pls use shogun types. index_t, int32_t\n. this is in-efficient (doesnt matter here but we want to do good style). Get once and then access in loop\n. same comment as above\n. - <= is $\\leq$\n- text in math mode should be done as $|v_\\text{true}|$\n. but this F_INT thing?\n. yes, but this should be stated in the doxygen docs of this constructor\n. what does it do exactly? write that\n. what happens here? document!\n. its not a deep copy. This is what I meant earlier. The new instance will use the same memory for the index vector as the old one. This should not happen in fact. So I suggest to get rid of the copy constructor here and use the clone method (deep copy) instead in the duplicate method\n. Again, could you explain why the feature type here is INT?\nThese features do not have a type really\n. please make those require with proper error messages for user (stating what the required class is and what the given one is)\n. \"the init function adds a subset\" - please be more careful with your english\n. whitespace after if\n. what if l or r are NULL? this segfaults then\n. pls use some fixed data here that you type in by hand. This is too much other things in the unit test\n. EXPECT_NEAR is your friend here. And in fact, this should be equal, but near is fine\n. No please dont use them! :)\n. I wonder how much overhead this generates?\nWe could maybe turn those checks off in the global interface optionally?\n@lisitsyn @lambday \n. start with capital letter in English\n. latex what the dot product does would be nice here\n. line break pls\n. here too, might be worth putting the things in variables for better readability\n. mmmmh copying?\nCant we use subsets here?\n. cast is never NULL?\n. bool\n. this is always guaranteed to terminate? maybe add comment in source code\n. maybe some comment whats happening in the following lines?\n. pls more details!\n. could you just send the PR for shogun-data? You can do this separately. Then this one will work and I dont have to download stuff (which is a bit of a pain to do)\n. some comments on those helper functions might be nice\n. REQUIRE with error message (expected vs current)\n. REQUIRE again, expected vs provided\n. docs\n. REQUIRE everywhere pls\n. whitespace. pls check this for all files\n. newline before and after loop\n. what is this for? comment!\n. line break\n. line break\n. inverting matrices is never a good idea. Rather solve linear systems when you multiply this matrix to something\n. this is done above I think\n. newline before and after if\n. main function should just init shogun, call something, and then exit. Rest should be in helper methods\n. could you pls make this a single loop which changes the filenames?\n. I mean this handling all datasets\n. do we need a main function if eigen3 is not there? why not just an empty file?\n. pls remove the data commit here, we dont need that\n. how many subsets will be added through this loop?\nThe handling internally is done very efficiently, but just curious\n. maybe some implementation details here? Like that subsets are used, how its implemented etc\n. Also, as subsets tend to mess up things sometimes, pls double check that everything here works fine when this is used for classification later on :)\n. very nice with the subsets btw, much more efficient than before\n. Yeah it should be fine.\nI just checked, the way they work is that every new thing that is added causes a new index mapping vector to be computed. So even if you have a large stack of things, the lookup will only be in one level. So should be fine.\nThis stuff is not thread-safe however, so you should mention that in the implementation docs and comments. I want to change this at some point (maybe you can actually)\n. In general, I agree. However, users want to know which algorithm is used, which computational complexity it has. And for that, it is very useful to have both: what is done, and some details on how its done\n. I just thought about this again. There is a problem. The subset stack is not designed to have too many subsets in a row. For every newly added subset, it internally stores the latest index vector for fast removing of subsets. If you add like 10000 of them, that might be a problem.\nWe have to deal with this. I think a solution would be to have a special mode of the subset stack, where you can only add subsets. Internally it then just replaces the current active index vector with another one, (and forgets the previous, rather than remembering the old ones as now), so that removing subsets is not possible anymore (gives an error). Could you add that with tests etc? Thats important for this pruning to be memory efficient\n. This should be done first, the current implementation is evil.\nSee also #2136\n. get_name print is not necessary in error messages, as this can be activated globally\n. same below\n. Pls use SGVector here, and not CALLOC. If you need to access pointers, you can do that still.\n. such hard-coded convergence criteria are bad, rather make all those things a parameter which has a default value, but that can be changed\n. pls dont have such booleans for printing.\nUse debug messages\n. same here.\nThis is SG_INFO or SG_DEBUG\n. if you used SGVector, you dont need to free later on, done automagically\n. SGVector\n. not get name everywhere\n. Should be documented in the class description that this only works with dense real 64 floats\n. pls always in error messages give the information what is expected and what you have gotten.\n\"Number of classes (%d) must match networks's number of outputs (%d)\"\n. there is a space missing here.\n. space missing. No get name of current class. For labels, its ok\n. yes, we have SG_INFO for this. Don't use boolean flags and SG_PRINT\n. Thanks! If things dont work, double check that the data version is the latest one of shogun-data. \n. I see that this is done in the Matlab code. But still I do not agree on computing/storing matrix inverses. You can always solve the corresponding linear system A^-1 * b, which is better than storing A^-1 and multiplying it to b.\nSee the solve() method of the decomposition base class of Eigen3.\nAlso, why did you choose this particular decomposition? See http://eigen.tuxfamily.org/dox/group__TutorialLinearAlgebra.html\nSuch choices should be documented, explained etc.\n. Note that you dont really have to do such changes, this is optimised away by the compiler. Just saying.\n. pls do NULL checks for the features here, you will get segfaults in the REQUIRE calls if they are.\nSame for labels below\n. very nice!\n. snprintf is fine as long as this is in an example.\n. we dont really use { } for one liner if then else. Minor :-P\n. doxygen documentation with @return\n. the latter with clone. Remove the copy constructor for now\n. pls remove the copy constructor for now\n. indentation is messed up here\n. please do a separate check for r and l\n. indentation\n. should be equal here, no need for an epsilon. But I am not sure whether this causes trouble ... pls try\n. no {}\n. no comments in the same line as code (please :) )\n. I like that\n. Don't we have some deprecated flag? @sonney2k @lisitsyn \n. { should be in new line\n. we do not use white-spaces for assignment operators\n. noooo, pls dont use initialiser lists, these just cause problems (people forget things)\nRather have a private init method which initialises and registers. The constructors then can call it and maybe override values with default ones\n. does this change anything?\n. this compute_eigenvectors method should just be deleted. \n. MALLOC!\nOr are they assumed to be zero afterwards?\n. this doesnt make sense, if you set to zero you should use MALLOC\n. btw very careful here, none of this is unit tested.\n. whats this?\n. SG_ADD (maybe also quickly change the others?\n. not in one line pls\n. pls no TODOs in new code\n. in desctructor?\n. note this might segfault if p is NULL\n. yeah, I will file an issue with this\n. Maybe not, I sometimes dont realise you copied\n. maybe keep for now :)\n. sorry!\n. yeah :)\n. ok, but before it was not initialised either, or was it?\n. No, super nice, thanks!\nIn fact just filed an issue of getting rid of those ugly methods :)\n. Just thinking loudly here :)\n. I think I used this somewhere, lets see what the unit tests say\n. They cause Shogun developers to forget to initialise members and caused lots of time wasted on fixing those things (as you are doing currently). Its just Shogun, they are not bad in general\n. true. Have you tried removing the method? Centering should not be done via matrix multiplication anyways. I remember I added the method, so if no-one else uses it, just delete\n. Not exactly, but almost. I did not know this little fact here:\nhttp://stackoverflow.com/questions/7546620/operator-new-initializes-memory-to-zero\n. haha, indeed! :)\nBut better fix less but in a way that one doesnt have to touch this again. No use if you fix uninitialised memory now, and then I have to do it again in two years because of a forgotten member in the initialiser list.\n. This description should go into the descriptions file as we use this on the website. Minor change\n. I checked, its only called from there. This is an abuse of the concept anyways, should be done via get_feature_vector, so I can blame @lisitsyn  here ;)\n. haha, just laughed loudly ;)\n. those are not necessay. SGMatrix members are initialised automatically\n. could you guys change this to SGVector? Then no free is needed\n. Nice!\n. I dont agree on this one. All builds should be able to run this if the data is present. This is a hack. You should be able to use relative paths from the shogun data submodule. Correct me if I am wrong.\n. pls make this a separate example. I know I wrote the old one and I know I did the testing in there, but that was before unit testing, just a new file with the plain example is ok\n. yeah, pls rather do a small example and just add the subset, no need to do the checks (thats what unit tests are for) its just about demonstrating API, you can add a comment that now the custom_kernel_matrix is the permuted version of the gaussian_kernel_matrix.\nAlso, pls do a real subset, i.e. the matrices will have different sizes. Just to make clear that this is also possible.\nA small matrix (say 3x3 and submatrix 2x2) is ok\n. yep thats all you need to expose those to the modular interfaces\n. you can add your name here\nSee other files\n. ah here it is, pls put on top! :)\n. I think this is not really needed as eigen3 will check, but it probably doesnt hurt\n. But if you have this, pls do\n\"Dimension of vector 1 (%d) is not equal to that of vector2 (%d)\" ...\n. on top\n. pls write down what the dot product is mathematicall, i.e. $\\sum_{i=1}^d a_ib_i$ where $a,b$ are $d$-dimensional vectors\n. top\n. @lisitsyn how much does this blow up the compile time?\n. really need both of those here?\n. math pls (in all the methods)\n. SG_UNREF\n. Capital \"No\"\n. \"No\"\n. \"No\"\n. \"Computes\"\n. \"For ...\"\n. Not commenting on the capitals anymore, also its super minor ;)\n. Dont need to mention KernelTwoSampleTesting here, this is useful in  general\n. In that case, please use SG_MALLOC/SG_NEW and SG_FREE\n. pls BSD and your name here :)\n. you can just print the kernel matrix itself here\nfor example custom_kernel_matrix.display_matrix(\"subset\")\n. yep, thats nice :)\n. In order to add this to the intergration tests, could you also return km_sub_kernel at the end of the function?\nThis will break the python_modular integration tests (since you changed something)\nSo you will have to re-generate the test with the generator.py file in the integration test directory (ask on IRC if unclear), and then do a shogun-data update (first the update, then update this PR with the corresponding data version)\n. I agree, its better to be explicit. \n. I agree, did not realise this before, sorry\n. I dont really get why you manually SG_FREE/SG_MALLOC SGVector instances. The point of this data structure is to not do this.\nYou can just do m_vector=SGVector<T>(len) (if the length is different to before) which does exactly what you are doing with setting the length and SG_MALLOCing the memory\n. This should be explained a bit in the class doc with maybe a link to a reference\n. same here, pls use the automagic memory managment of SGVector\n. i also should be greater than 0\n. this should not be in here, rather ask @vigsterkr to install the networkx (which is installed AFAIK)\n. love the visualisation! :) great work\n. pls no comment in the same line as code\n. could you add a little description in the descriptions.txt so that a documented example is generated?\n. Add a comment that you are doing random subsets here for the training\n. Actually, dont do a loop. But please make sure that training and test data does not overlap. You are essentially (doing cross-validaiton here. But Shogun can already do that. So no loop needed)\n. this might overlap\n. either use your file search or have a look at examples/descriptions\n. The copy constructor might be overloaded by subclasses .... just a comment\n. whats the thinking here?\n. Capital \"S\"\n. why error? file is not closed then ...\n. formatting!\n. just \"Entering\"\n. \"Leaving\"\n. What is the line of thoughts here?\n. Pls document what is used for comparison, i.e. equality operator == for discrete types and floating point accuracy for float types\n. nice! much cleaner\n. actually, we have to be super careful about floating point numbers here.\nThere is a recently added method CMath::fequals that can deal with all cases. Pls use that (or modify it to work), its already unit tested, so your unit test really only need to deal with matrix positions differing, the actual difference (NaN, inf, etc) do not need to be checked again.\n. ok!\n. as already said like 100 times, newline before {\n. in python\nGaussianKernel().io.set_loglevel(1) (or any other SG object)\n. sorry, thought you added that ;)\nWait, you added that. Come on, its not hard to obey these things\n. it should if you use the method, so just need to check differences in various matrix positions\n. I am not blaming you, sorry if it seems like that. I am just asking you to respect the standards we have agreed on. Everyone else does too. It doesn't slow down contributions, only discussing this every time again does. It annoys me too to have this discussion, but note that I (and everyone else) ask all contributors to do this.\n. I think these \"false\" unit tests are not really good. Could you write one where exactly one element is different (by maybe very small and larger value), and then put this one element into all possible matrix positions in a loop? This way we check all possible cases for errors. The current one immediately exits since all elements are different.\n. dont get why you test for dbl min and max.\nBut they can stay.\nWhat about also testing for old_val+epsilon\n. The line I was talking about was not in there before your patch. But yes, let's not continue. Rather soon have a hook for this.\n. pls add a newline here, also \"\\n\" is missing at the end of the message\n. CMath::min\n. wait a minute, why do you use pointers here and not SGVector?\n. wow I did not know that those streaming (<<) operators are define on the macros for the unit tests\n. Nooo, please dont have such writer methods in classes in the Shogun tree. See the massive discussion on the UAIFile. There should be a separate reader for all file formats that we do.\nPlease put this into a separate class that is also tested separately. Or let me know why we dont want to do that. @vigsterkr might also have comments\n. I would actually prefer if you used SGVector for this (cleaner interface!) (without reference counting obviously, but be careful on corrupted pointers on freeing the vectors, as usual)\nThis is all possible without views already\n. Dont worry! But yeah, we want to do things properly (and not as done in CHMM which is basically a giant hack)\nHave a look at the readers and writer for other data formats. This is useful so we want to have it as general as possible\n. You can comment every line of source code\n. epsilon and train_func should both be registered as parameters using\nSG_ADD(&m_epsilon, \"epsilon\", \"Convergance parameter for Gauss-Seidel\", MS_NOT_AVAILABLE);\nSG_ADD((machine_int_t*) &m_train_func, \"train_func\", \"blablabla\", MS_NOT_AVAILABLE);\nThis will make those parameter serialisable. If you do this, the integration tests for KRR will fail (and have to be updated, but lets do that in the next iteration)\n. maybe one unit test for each getter/setter pair?\n. usually, its possible to forward declare things in headers and put the includes into the .cpp files. Could you do that here?\n. I dont really like references on SGVectors since it is not really intuitively clear what happens if memory gets freed. Rather do copy by value\n. minor: { return foo; } \n(with whitespaces)\n. what about an SGVector here?\n. pls do academic referencing in addition to weblinks always\n. isnt there some operator that you could use here?\n. pls no dead code in patches\n. same here\n. whitespace\n. ok as long as you dont start passing this around in shogun, this is fine\n. no should be fine, sorry\n. no need for the ()\n. Why do you want DEBUG outputs in the notebook?\nProbably you want INFO. But in fact that wont help anything at all since we dont render the output anywhere\n. what is this again for?\n. pls post the nbviwer link whenever you send a notebook patch. Even if its not yet finished, I cannot review this otherwise\n. also, please always send notebooks in separate patches. Try to keep the PRs to the minimum size.\n. I know it is not your fault but \nif (verbose && SG_UNLIKELY(sg_io->loglevel_above(MSG_DEBUG))) is pure pain. Printing should never be done via variables. If one does costly computations in an algorithm, the results should just be saved to a vector which has a getter. Then SG_INFO can be used. SG_DEBUG is for other things.\nCould you clean that up in a patch?\n. No MSG_INFO in the notebook as things are printed to the terminal, not the notebook.\nAnd the verbose variable (and code) should be changed such that residuals (or whatever its computing/printing) are stored rather than printed. Then there should be a getter for those stored things that one can print in the notebook.\nSee what I mean?\n. almost\n- pls rename the verbose variable to something like m_store_train_error\n- Just do SG_INFO rather than this if statement at the end\n. I know its a minor thing, but variable names should correspond to what they stand for. This is not verbosing but storing residuals/errors during the algorithm, which in fact costs computation. So verbose is clearly the wrong word for this. Dont touch other classes, just this one. Again, this is not your fault, its just a nice clean-up on the fly (the way to go (tm) in maintaining open-source :) )\nThe information that is printed is clearly not debug information, but algorithm iteration information, therefore it should be SG_INFO. You can even replace \"add_debug_info\" by something like \"On iteration %d: error: %f, primal: %f\", etc.\nAnd there should be no if statement at all for the SG_INFO or SG_DEBUG. Moreover, never use SG_PRINT from within algorithms, that is the whole point of these different log-levels.\nSummary:\n- SG_DEBUG: Things liker \"entering\", \"leaving\" (see QuadraticTimeMMD for examples). Or \"doing this and that now\"\n- SG_INFO: Things like iterations residuals while algorithms are running. Something that can be printed every iteration\n- SG_GCDEBUG gargabe collector debug, things like reference counting\n- SG_PRINT not to be used from within Shogun algorihtms, this is for users\nAll those macros are just called, no if statement needed.\n. members should be initialised to default values in the init method\n. please avoid includes in header files where possible, you can use forward declarations.\n. Why do you call it \"Exact\" likelihood?\n. What about just removing the \"exact\" ?\nThis description you wrote there should go into the class doc\n. documentation is wrong here. Pls update. Also please indicate the computational costs for storing training information, and also mention this method in the class description\n. This will break integration tests (as serialised instances do have a different name for that member)\nThats a good opportunity to learn about them :) You basically just have to generate the test files using tests/integration/python_modular/generator.py and test with tester.py\nLet me know if you have questions\n. the verbose has to go, just always call SG_INFO\n. same here, verbose has to go\n. this one is good now! :)\n. this should go, as said above, just always do SG_INFO, people can disable that\n. yeah, thats a problem.\n@yorkerlin @votjakovr To what extend is the existing code inspired by octave's ? Just the interface, or also implementation details? If its just another implementation of a well-know algorithm under the octave interface, we should be fine\n. +1\n. GPML is bsd3 so thats fine.\nIf this replaces the old integration methods (given we can do that), we can get rid of the Octave one\n. All python examples return something at the end. All those things are serialised and then compared to a stored file (if there exists a python example for that class). Adding a new member to the class will let this comparison fail, therefore one has to update the file. If there is not intergration test for this class you added a member to, then all is fine\n. pls no prints in the examples, pollutes the screen\n. pls dont use iostream but SG_SPRINT\n. please_dont_do_camel_case_but_underscores\n. not needed\n. code formating is wrong, pls see the guidelines\n. no asserts in code pls, rather REQUIRE macro\n. pls no std::vector in interfaces, we have our own structures for that\n. Please document properly\nWhat does the class do, where is the implementation coming from, what are the costs, what is the math? etc etc\nAlso, please document all methods and members in doxygen (also properly)\n. no code in header pls\n. whitespace\n. I agree with @vigsterkr all the namespaces should have proper names, or a hierarchy\n. Yeah I think its safe to BSD it\n. thanks for the answer @votjakovr \n. pls try @iglesias suggestion\n. shogun::linalg::implementation now right?\n. I guess you mean \"square\" :)\n. I guess there is no way to check whether a matrix is symmetric in a cheaper way than just summing up. So I am not sure whether this is really needed? Is it really that much cheaper to compute sums in this symmetric way? I would be surprised. But it cannot hurt to have this.\n. when writing those error messages, always include the actual dimension in the message, i.e.\n\"The length of node array (%d) and weight array (%d) should be the same\\n\"\n. same here\n. and here\n. minor: we do not use whitespaces around operators such as *\n. - members have to start with \"m_\" prefix.\n- this member is not registered as parameter in the init method\n. doxygen\n. pls fill this\n. please be consistent with the whitespaces. No whitespace anywhere except for && and ||\n. Capital letters at sentence starts, please give the length of the elements to make it easier for users\n. newline is also missing here\n. this is true for all the REQUIRE calls\n. maybe provide some comment that gives an insight why you have this case distinction here? Useful for later developers\n. These have to be done properly\n. documentation\n. proper information please\n. this class is fine. But some of the things in there can be used elsewhere, (logdet) So please put the functions at the place they belong to. All the very GP specific things can stay here\n. Details!\n. NONE?\n. whitespace\n. yep, thats good!\n. detailed description here please :)\n. Please print dimensions in that case\n. newline missing and whitespace before coma\n. newline\n. what about putting those cases into methods to make the code more readable?\n. Yeah I think that should be done, this method is quite long ;)\n. Please use REQUIRE here!\n. And everywhere\n. So this is GPL code?\nWe would like to avoid including any more GPL code into Shogun since it block conversion to BSD...\nCould you investigate this and make sure we are allowed to relicense?\nI am merging for now anyways since license in header is still GPL\n. would be good to do a comment here what these case distinctions exactly do, maybe add in another PR\n. again the GPL issue here, pls make sure we can re-license this later\n. for this, it would be really nice to interface against boost   sigh\n. ok!\n. yes!\n. Could you document the enums?\n. I think thats fine!\n. These sum calls, might they be faster via the linalg interface and some of eigen3s simd magic?\n. Please print the values of m and n here.\n. and here\n. This might also be good to be documented\n. hehe, well......\n. ok\n. CSGObject::clone should also be able to do this, did you try?\n. whitespace after for\nno whitespace before <\n. only one newline here\n. SG_ERROR for those\n. might I suggest to store both sides of the * operator in variables first?\nThis is hard to read\n. Providing an interface for what? Pls say one or two sentences\n. your forgot the SG_REF of the given lik here.\nDo it before the UNREF\n. These wrappers here are not too nice, but I think thats the only point where this has to be done in the current design right?\n. Could you talk about the design pattern here? That one has to provide a likelihood instance?\n. sorry.\nWell if clone doesnt work, than you should overload it.\nBut in fact, rather let's try to fix the problem :)\n. I think we should stay general. So option 1 in my opinion.\n. Why empty here?\n. Very good that you say this!\n. Is this function exposed to modular interfaces?\n. I see, makes sense to not throw an error but to return empty vectors!\n. yep, thats good\n. Thats ok!\n. I think its ok. Though computing the variance is not that much work, right?\n. yep! (We should merge soon in my eyes)\n. @sejdino ?\n. print statistic type! to inform user\n. good!\n. really well documented! I like that\n. Do you have that in your contract?\nIf not, I personally would prefer to only have authors in the license (after a long discussion at the MLOSS workshop at NIPS). What difference does having this make?\n. minor: Hessian is capital H\n. Could you give a reference (and link) what the KL method is here?\n. Yep I know, but think about the people who read just doxygen documentation and not the source code\n. Please provide full details in the doxygen for people who read documentation (also in the base class)\n. the comments here should be inside the function rather than outside\n. minor formatting: you can put those here into one line\n. This returns true if the passed argument is NULL, which is not what we want \n. please do a seperate PR for the matrix operations class updates\n. Please dont leave placeholders\n. Ok I am fine with having this class of helper methods for GPs.\nBut if we do have, please document the mathematical operations of all methods properly.\nAnd again, please send a seperate PR, so much easier to review!\n. Maybe state component and value ?\n. Note that this is in CStatistics already. This is such a general operation that it should not be in the MatrixOperations class. I know you are waiting for the linalg framework by @lambday but if we put this kind of stuff into Shogun before, it should be at the correct position! Let me know your thoughts on this\n. maybe put inside function?\n. newline not necessary\n. indentation\n. Is this possible to get wrong as a user? If yes, then this should be REQUIRE, if not, thats ok\n. same here. please put a comment if this is not breakable by user\n. comments inside function (wont notice that anymore, please do everywhere)\n. Again, please put paper reference and some details inside the doxygen\n. Also where code is from, thats very useful information\n. seperate PR and see my other comments. This will not be possible to merge if the class is in all PRs.\nSo send the matrix operations thing first\n. Could you in all unit tests you do mention that you compare results against a reference implementation in matlab?\n. fine to say that in every test case in fact. just helps\n. Cool, so why not put that as a comment in the c++ code. Makes things way easier to re-construct later\n. You could even put a gist link into the code (as comment) But more important, state what you are comparing with\n. I see, thanks for the explanation. I suggest we change this and you come up with a different pattern to solve this problem. This is not really nice this way, confusing.\n. when would this be not positive?\n. see my other comments on log-determinants\n. It seems to be that you only need the diagonal of the inverse matrix. Why compute the full thing then? In particular, storing a matrix inverse is always very unstable. I am sure there are better ways of computing the trace of a matrix inversion that are faster and numerically stable\n. As said often before, please put all the reference into the doxygen. Nobody will read them if they are just in the code. Reference paper, reference code, etc!\n. one liners please.\nPlease also fix those things in your other (merged) patches\n. Does this really have to be a member? Can't you just stored the factorization?\n. Again, please clearly state where those numbers come from. If possible, give reference code in a gist link. If thats not feasible, at least say what you did\n. I know from experience that kernel matrices quickly end up being numerically not psd (very small negative Eigenvalues) In particular when they grow large: For example for Gaussian kernel, the Eigenvalues decay with an exponential rate, which is problematic. Adding this ridge usually helps. Why don't you check for this here and add if necessary? Also, usually tried to avoid computing Cholesky of the kernel matrix itself, but of kernel + noise on diagonal. In this case, we can safely assume that its psd.\nThoughts?\n. Hi!\nThanks for these details, but they should go into the unit test (as a comment)\nIf they are just on github, they are basically lost. If you put this inside the unit test file, people can look at them.\nAlso, please use gist links, as they are fixed - in contrast to your repository.\n. Just realised I was not very clear about that earlier, sorry about that. But should be now right? :)\n. I think this can be larger, but might be wrong. Lets aim for say 1GB with 64 bit\n. I think the performance gain saturates at some point. It might already be at 1000 examples, so this is fine\n. Whitespace at the beginning of string should not be there\n. this should be a class parameter that one can change with a setter! Never hard-code such things.\nIt doesnt have to appear in the constructor though\n. I think you do double computation here.\nisPositive already has to look at the Eigenvalues of the matrix, so it computes a factorization. This could be in fact used to compute the log-determinant, but you are calling another method there.\nSorry for not having spotted this earlier, but now with the loop it became more apparent.\nDo you see what I mean? I think you should just compute the log-det from the m_ldlt class directly. Or not check it this way.\n. Very nice now! :)\n. Seems like there is a lot of redundant code in this class and the above: All parameter are registered in both classes. Would it make sense to pull them out into a base class? Let me know what you think\n. same here, no hard-coded numbers please. But also consider putting this into a base class\n. same comment as above\n. Please comment here why it is more stable than the other method and in which case one should call which.\n. Can you put this into each of the test cases seperately? I know its kind of dull to do, but it makes sure that people dont confuse things when say new test cases are added. Thanks!\n. same here!\n. all test cases should be in a seperate unit test case\n. so cut here.\nIf you want to re-use the matrix, pull its generation into a helper function\n. Problem with those things is that they are not within Shogun's parameter framework.\nI suggest you precompute the factorisation, and store the triangular matrix rather than the m_ldlt object. You can then do triangular solves from the matrix. Here is some code\nL.triangularView<Lower>().solve(b);\n. You are right, (even though we do not care about sparse matrices here) computing the diagonal is as expensive as computing the full inverse (aka factorisation). My comment on the ldlt still holds\n. A minimal API example would be nice\n. Minor. Please be consistent with Capital letters at the beginning of sentences.\n. This is not a good unit test.\nRather you should compare numerical results, i.e. floating point numbers. This test can pass in all sorts of ways while LDA contains bugs. Would you mind doing a proper one against some Matlab/python impementation?\n. And please note that the unit test in fact should come before you update the implementation. Only way to make sure that before and after is the same. Could you remove the LDA patch and only add a proper unit test (or more) and then update the implementation afterwards?I know thats some work, but well worth it\n. please no unnecessary newlines\n. no {} here\n. dont like these global defines, why not make this local variables in the test?\n. rather use fixed data for the test\n. whitespace\n. Shoguns style isnum_neg<=0 || num_pos<=0`\n. no {}\n. please use SGVector instead\n. please dont use asserts when users can cause them to fail. Rather REQUIRE with a proper error message that  tell what is wrong (with details, i.e. numbers here) and how they could be fixed\n. computing matrix inverses is unstable, slow and should be avoided if possible. Rather, precompute matrix factorisations and use them to solve the A^-1 * b system with those. Let me know if you have questions\n. no {}\n. I know its not your fault, but could you get rid of this annoying DEBUG_LDA flag?\nUse SG_DEBUG and SG_INFO and print sensible information instead. Or, just remove this stuff, should not be in the code-base.\n. These tests usually break if things are not properly initialised when the class is created with empty constructor\n. When you do clean ups of classes, it is always a good idea to give it a general treat.\nFor example: Replace all ASSERTs by REQUIRE that give user readable error messages and context information. Would you mind doing that?\n. Is this cast always guaranteed to work, if a user passes non binary labels? Should be checked either by dynamic cast or the enum for label type\n. whitespace. Also please print both numbers and put a newline\n. we use a whitespace after if\n. Please put a proper error message.\nAlso, this check should not be necessary, as binary labels make sure there are only +1/-1. Please check and then remove the error message, or, if you keep it, write a proper one\n. This should be checked elsewhere (before, maybe even different class). And please clean up the error message\n. careful of integer division here\n. Question: As in PCA, there are probably multiple ways to do this. Computing the covariance matrix is fine if data is not high-dimensional, but if it is, then computing matrix is impossible. Could you investigate a bit and come up with a fix for that case?\n. The matrix you are dealing with is positive definite. Therefore, using Cholesky for solving is much faster and more stable.\n. never ever multiple explicitly inverted matrices to vectors. There is a \"solve\" methods for this.\nComputing matrix inverses is slow, unstable, and evil. Rather solve the linear system that this corresponds to directly (in Matlab language: never do inv(M)x, but rather M\\x )\n. Please dont use placeholders for member documentation, Say what it is.\n. Please start all members with \"m_\"\n. Please also implement the std constructor and call init from there\n. We dont use whitespace for the \"=\" operator\nindex_t a=1;\n. These empty matrix/vector inits are not really needed\n. Print the (wrong) numbers\n. You need to put newlines at the end of each REQUIRE\n. This cast is not safe. Check it before, or do a dynamic cast\n. Whitespace after coma, newline\n. redundant cast. \n. I think you can do this vectorized with eigen3, via iterating over corresponding indices. But thats cosmetic\n. same here, vectorized?\n. Same comment, what if dimension grows super large?\n. Maybe you can investigate this problem a bit in general, in PCA this is overcome via doing SVD on the data matrix rather than the covariance matrix\n. same here\n. Use Cholesky, the matrix is psd. Whitespace\n. Also, please do an in-place solve here. You at this point have three matrices of size dimensiondimension in memory. But you just need to have two\n. complex eigenvalues? Sure about that? This should not happen\n. no whitespace. We also have a qsort in CMath\n. indentation.\nAgain, this real() business is not really nice\n. no whitespaces\n. we have a matrix(row, col) operator, use it, it avoids bugs\n. whitespace cleanup please :)\n. This case is not safe. please make it safe by either checking feature enum, or dynamic cast.\nProvide a proper error message via REQUIRE\n. Dont do these redundant casts\n. class description with math, and method, computational costs, and a reference (barber book) please\n. no empty doc strings please\n. whats happening mathematically? document!\n. document! whats going on?\n. document @return \n. same here, what is this object mathmatically, document @return\n. typo \"constraints\"\n. \"calls\" with s\n. Does doxygen really support \\left \\right?\n. It might be good to document the members of the enums as well\n. Please say why the numbers are wrong and what they are\n. isn't this implemented in get_num_vectors()\n. Could you put something as \"%s::copy_dimension_subset() is not yet implemented!\", get_name()\n. Is this really the way to do this? Just wondering?\n. same here, proper message\n. what is this NULL thing supposed to do?\n. use a googletest macro please\n. typo init\n. just seeing this, could you update the error message to print the given number? Good for users\n. yep, that is much nicer!\n. please make a mental (and physical) note that this SVD and the above LLT is replace by internal linalg framework once this is ready. This will give GPU/multicore speedups\n. And update once we merge the feature branch\n. I think you should distinguish two cases here (like in CPCA). If N are the number of points and D are the number of data then\nSVD will be more efficient when D>>N\nand the previous version will be more efficient for N>>D\nThere should be an enum to force a particular version, and there should be an automatic option.\nThat is just some logic to add now (there should be unit tests for both cases)\nYou can really follow the PCA imeplementation and unittest logic here. @parijat might also have comments.\nThanks for your great work here!\n. I mean there are these in-place ways of doing preprocessors. See doxygen. I don't really know, just asking\n. Sorry about being annoying, but in any error message, also state the (wrong) value of the variable you are checking. And, as you did, the allowed values\n. Mmmh. You are right PRETTY_FUNCTION will do things when not implemented. Sorry. Just always want to make sure there are no cryptic error messages for users that try stuff :)\nThis warning things and returning NULL is not good. An error should be thrown. Returning NULL will likely result in a crash\n. And such behaviours also should be unit tested. We only do unit testing for numerical results, but in fact such I/O checks are also very useful\n. Each of those enums should be documented. At least providing a link towards where the information about them is in doxygen. So that users can easily click through this stuff\n. Please also print the given numbers for above require\n. Definitely, there should be a check on p_and_q!= NULL and then you can print the get_name() of the given (wrong) instance\n. yep\n. good!\n. doxygen please\n. Can you make these enums clickable. You do that via\n::S_UNBIASED_DEPRECATED\n. Also when you reference the enum type itself\n. same here\n. same here\n. and here\n. please add a null check and print the name of the m_likelihood instance\n. i might have overseen this earlier, but  every public getter in Shogun that returns a SGObject instance has to increase the ref-count. Consequently, you have to decrease it in this method here.\nPlease add both everywhere, very important for modular interfaces\n. sgref, see above\n. sgref, see above\n. sgref, see above\n. sgref, see above\n. Can this be set wrongly by a user?\n. maybe print lab->get_name() in this message\n. why the clone?\n. ah i see got it\n. grammar in the method name is wrong\nI suggest\ndual_parameters_valid as name\n. very nice documentation !\n. please also document std constructors\n. very nice implementation this class! :)\n. sgref, see above\n. sgref, see above\n. sgref, see above\n. sgref, see above\n. sgref, see above\n. sgref, see above\n. sgref, see above\n. sgref, see above\n. sgref, see above\n. sgref, see above\n. sgref, see above\n. please add a null check and print the class name of the given one if wrong\n. require with message pls\n. sgref, see above\n. sgref, see above\n. sgref, see above\nwhitespace\n. needs this?\n. why clone here?\nour usual standard is to return the vector to the same memory, and if people want to modify, they clone themselves.\n. same here, clone\n. this setter should also SG_REF the given instance\n. also docment please\n. clone here is fine\n. Do you think these unit tests cover all cases in the code? Just asking. If you say yes, I am fine with them\n. copy paste error\n. I wonder whether this number needs to be stored when one can always ask the feature object\n. Probably my fault back then :D\n. I am fine with this!\n. I second this, initialiser lists are evil in the way we register parameters in Shogun :)\n. please print the get_name of the labels to help users find whats wrong\n. Please start all error messages with a capital letter\n. overwriting the method internally is not good for a subtle reason:\nimagine a user changes the data for which the other case is better. then the method is not automagic anymore. i would rather do this in if-else-logic\n. Really how you do this here! Guess that was also quite insightful? At least for me :)\nOne thing. Could you try to not allocate so many matrices? Uses lots of memory.\nRather, try to have only a few matrix objects, and then you re-use the memory once you don't need it anymore?\nSee what i mean?\n. can you put a :: before the enum? then it becomes clickable in the doxygen.\nDo this whenever you reference enum values or the enum itself\n. nice!\n. yep, nice\n. ok, seems like there is a problem somewhere else?\n. these two parameters are redundant.\nwhat about just storing for internal use and converting the others in setters?\n. should be mentioned in class documentation. memory requirements, etc\n. newline here\n. wow super nice documentation!\n. ah sorry, i did not see that.\nNo for protected method, we can do whatever we like. Though, I have to say, I still would like to the the SG_REF to stick to our general style.\n. ok cool.\nas you already know, eventually, we want to cover all non-trivial code with tests\n. yep, like that, but always SG_REF before the SG_UNREF, otherwise an object might be deleted if the same object is passed to setter\n. ok!\n. ah I see, then this is a neat solution i guess :)\n. ok, thanks for the explain. It is not expensive, so fine to keep\n. ok, assert is fine then\n. ok, but then why store them locally?\n. i guess it doesnt matter, but yes, that avoids anything not being synchronised\n. nope. just dont overwrite it internally\nif its set to auto, leave it at auto. do the algorithm distinction with || in the if then else\nif(m_method == FLD_LDA || (m_method == FLD_AUTO && num_vec>num_feat))\n. print name of the given one.\nnull check before\n. just out of curiosity. are there any update on the linalg stuff for solving linear systems? @lambday \n. nice!\n. maybe print name of features get_name\nand important, null check before error message\n. i actually like initialised more\n. memcpy can do this,\nwhy do you convert into matrix? sorry its late ;)\n. create_transormed_copy\n. cant you rather compute the integer that corresponds to the percentage?\n. and then use above code\n. super nice!\n. whops, yes you are right\n. yep thats good!\n. no whitespace\n. one more thing (sorry for continuing to annoy you with requests :) )\nI suggest you put these two cases into seperate protected methods, this one is very long and hard to read.\nThis will keep things more compact\n. please make the enum doxygen clickable via adding a \"::\" or a \"#\" when mentioning it.\nmake doc compiles the class documentation locally and then you can see whether it worked\n. same here and at other places\n. same here, please make the enums clickable\n. there should be a few more sentences on the intuition here. why do we need this class, how is it used, etc\n. why do we need serialisation here?\n. we should where possible try to avoid converting float64 into float32 matrices. any thoughts on that?\n. thats fine\n. good unit tests!\nplease make sure you test all added code, even trivial things, as this is very low level so has to be waterproof\n. is this include really needed btw?\n. you have to initialise the members (m_method, m_gamma) here, otherwise unit tests will have memory errors.\nif std constructor is called, they are currently not set to anything\n. other classes may also be LT_BINARY, so just say that labels should be of type LT_BINARY\n. the space here is actually usually done, but thats super minor\n. sorry, ignore that. LT_BINARY is only true for CBinaryLabels\n. please put both cases into separate protected methods to make the code easier to read\n. indentation screwed up here\n. and here too\n. no newline\n. yeah, why not add another datatype like SGMatrix, though its painful to implement that.\nin fact, i think the overhead of sgobject is neglect able compared to sizes that matrices usually have, so fine from my side\n. sweet\n. sure this thing even needs to be exposed to modular ?\n. could we try to reproduce an example from the paper in the notebook for this?\n. maybe the class header of both of those should mention the memory requirements additional to the HSIC computation itself?\n. technically, this would be another unit test - one should always try to make them as small as possible.\nhaving said that, it doesnt really matter here ;)\n. mmmh, no hiding in in such a specialised class is not a good idea. then rather keep it public and expose it. i just though nobody might ever call this, so rather hide to not confuse people. but maybe actually somebody wants to call it, so keep this stuff, sorry for the confusion :)\n. ok !\n. yes, much better than zero mean!\n. have a look at the other python examples, try to copy their style.\n- bsd license on top\n- no docstrings (we do this via the descriptions)\n- a function that does the method, returns the predictions and possibly inference objects\n- the function takes some parameters, different combinations of those parameters are provided in a list\n- results of function are used for integration testing, you can put all possible inference methods in there\n- maybe a rename to variational_classifier would be useful?\n- see for example the libsvm classifier example\n. could you do a normal formatting in python scripts? eclipse has an auto-formatter that obeys standards. see again the other scripts\n. you could put all inference methods in a list and then do the below things in a loop (but better they come as a paramter as described above)\n. I dont like this, could you make this explicit and put a comment whats going on there?\npythonic way is try-catch\n. I forgot but is zero mean now a special case of const mean?\n. good!\n. this is a bit off-topic, but it would be great if you could make sure the CrossValidation framework works with your methods. In fact, I even suggest a unit test for this\n. bsd please. think ive mentioned that before\n. line breaks would be good after each list element\n. but thats exactly what i was after, nice!\n. line breaks also\n. thanks for you comments, @vigsterkr \nseems like your getting active again :)\n. reason?\n. why do you move this?\n. in shogun things are slightly different.\n- model selection parameters are those that can be changed via the grid-search framework (in shogun), they just provide memory access\n- gradient parameters are those for which the gradient is defined and they can be optimised via gradient based optimization\nnone of those correspond to hyper-parameters, latent parameters, or similar, but are just way to give general model selection classes access to the memory the variables are stored in.\nSure you still want to remove them?\n. so the latent parameters can be (if gradient is available) put into the gradient parameters\n. seperate PR for those renaming please\n. same here, another PR for this\n. base class does not need to be exposed to swig i think? In any case, please also do adding things to the interface files in a seperate PR (name changes of course not)\n. the changes in this file are beyond renaming, code is changed, so please put them in a seperate PR\n. give the user some information!\n. quite weird that this did not exist before?\n. doc updates also in a seperate PR please (you should have a couple of branches locally for those fixes)\n. adding parameters to classes also if possible in a seperate PR.\nThese changes will break any existing python examples where this class is used, its easier to backtrace them in that case\n. variable renamings also seperate PR. This breaky python integration tests (if examples exist)\n. also seperate PR here\n. @votjakovr thanks a ton for your comments - that is really appreciated ( as I am swamped with email from shogun /gsoc/workshop).\n@yorkerlin please discuss things with roman - lots of potential nice synergistic effects here\n. maybe (in another pr) add the comment that all laplacian inference methods would inherit from this class\n. as said before, this should be bsd\n. same here\n. could you pls make this a seperate example?\n. could you also return the gp class?\nthis will break the python integration tests which you have to update then.\nBut it will catch lots of errors\n. ok\n. this is in CStatistics already and is so general that it should not be hidden in here.\n. all asserts should be a require that give the user/dev information on whats wrong\n. eventually this should be done via the linalg framework\n. same here.\nin particular, this SGMatrix eigenvector method is crap - should use eigen3 or better linalg\n. as said in an earlier PR, it would be good if you returned the gp instance here too.\nThis will break the python integration tests, but we will also be able to catch a lot of problems with this.\n. in latex, you should always use \"\\log\" rather than \"log\" when in math mode.\nSame goes for all function names in fact -- could you address this in another (cosmetics) patch?\n. meanf in math mode will also look weird\n. typo\n. You should be consistent with starting all error messages with a capital letter. Here its small l, above its capital L\n. could you investigate a big (using debug messages) whether things are computed multiple times?\nWe had the situation with @votjakovr where the GP inference was effectively computed twice since some minor parameters changed and then update re-computed all matrix inversion. \n. as @vigsterkr said, no eigen3 includes in the header please.\nI will merge this (as most comments are minor), but please remove all eigen3 incldues from all header files in another patch before GSoC end.\n. what I find weird here is that a sum operation should have at least 1e-7 accuracy. Why is this?\n. I remember, that's why I asked :)\nIt might have been Re-introduced. Better double check such things\nOn 16 Aug 2014 14:41, \"Roman Votyakov\" notifications@github.com wrote:\n\nIn src/shogun/machine/gp/MultiLaplacianInferenceMethod.cpp:\n\n+}\n+\n+SGVector CMultiLaplacianInferenceMethod::get_diagonal_vector()\n+{\n-   if (parameter_hash_changed())\n-       update();\n  +\n-   get_dpi_helper();\n  +\n-   return SGVector(m_W);\n  +}\n  +\n  +float64_t CMultiLaplacianInferenceMethod::get_negative_log_marginal_likelihood()\n  +{\n-   if (parameter_hash_changed())\n-       update();\n\nHi @karlnapf https://github.com/karlnapf! Remember, I've fixed this\nproblem :)\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/shogun-toolbox/shogun/pull/2484/files#r16326105.\n. REUIQRE and a proper error message please\n. REQUIRE(len_array>0, \"Length of array (%d) must be greater than 0\\n\", len_array)\n\nPlease make sure that you do this kind of error message in all your code from this summer. Really important.\n. same here and everywhere else\n. You dont need to print the SGNDArray::get_value(): \nalso type dimension\n. and missing newline\n. Please double check all your requires. These are really important as the users only see these messages if something wrong. They dont see the source code. Its super frustrating if one gets a bad error message\n. No class name, method name please. Also please give the number of dimensions\nREQUIRE(curr_index.size() == num_dims, \"Dimension of current index (%d) does not match internal number of dimensions (%d)\\n\",  curr_index.size(), num_dims);\nalso note the typo\n. and so on and so forth, all your requires need to be changed I think\n. What about some unit tests that check very basic functionality such as the produced features have the right dimension?\n. assert?\nSee https://github.com/shogun-toolbox/shogun/wiki/Assertions\n. and here\n. and here\n. Any particular reason for this?\n. I am just wondering whether this should be a SG_NOTIMPLEMENTED rather than nothing?\n@iglesias thoughts?\n. if you do this, the method can be removed from the EM interface\n. REQUIRE(values.vlen>0,\"Number of values (%d) must be positive.\\n\")\nis a better error message, see https://github.com/shogun-toolbox/shogun/wiki/Assertions\n. the line above here is not purely virtual because otherwise, the class would be abstract, which for some reason was avoided. But with the new approach, this can also be empty I guess?\n. GP stuff all has hard dependence on eigen3. Why not use eigen3 here? It vectorises things and might be faster? Same for the other places\n. remove. This wrapper is not needed at all and its cleaner to remove it\n. Yes, this will be faster. However, probably not noticeable here. So just leave it like this!\n. I assume you just moved this around, but not actually edited it? (Just to be sure)\n. ehm, what is going on here?\n. is this deletion related?\n. missing newline\n. Could you transform this into a REQUIRE with a proper error message?\n. Could you use the new SGMatrix::() operator here? We did not have it back then.\n. I am curious, what does this flag do?\n. REQUIRE please (I know its not your fault, but we are touching the code already anyways\n. here too\n. and here\n. isnt this also something that might be better handled via a polymorphic approach?\n. SGMatrix::zero() reads easier\n. shouldnt this be protected?\n. We could actually keep the pattern with the Impl suffix also for the new implementation.\nThe idea is to hide internal helper methods in those objects, see for example @iglesias LMNNImpl class\n. the MALLOC does not belog here, just return NULL to avoid compiler warnings\n. just to be safe, can you make this a 128 bit float?\n. can you add a test where 64bit numbers overflow?\n. Yes, we want to avoid _any duplicate code in both of the classes. If there are batches of the same/similar code, move it into a helper method in the base class. Let me know if it's not clear to you\n. you want a seperate test case with a name that roughly describes it instead. Keep unit tests as atomic as possible.\nAlso, I suggest you allocate a vector of float64 numbers, which are all set to the maximum that float64_t can take. And then compare whether their average is also that maximum 64bit float number\n. yep like that :)\n. yep, that looks good!\n. I see, so it makes sense to have it in the base class. Thanks!\n. Base class does not need to be exposed via swig (I think)\n. if you got the patience, we usually dont do CamelCase but camel_case variable naming, so this would be x_size. (not too severe, but its an easy find-replace\n. Maybe add some more documentation here: Wikipedia link, some (minimal) math describing what it does\n. whats that?\n. Aren't we missing some SG_UNREF here?\n. can you do a typecheck here with a REQUIRE?\n. ClList -> cl_list\n. thats ugly\n. this seems to be a heuristic (that should be documented somewhere in the class doc)\n. remove those\n @iglesias agree?\n. Please do a proper error message, see https://github.com/shogun-toolbox/shogun/wiki/Assertions\n. https://github.com/shogun-toolbox/shogun/wiki/Assertions\n. type check? if not, put a comment that its not needed\n. such things should be documented in class doc\n. and this one maybe too? Or is it part of the std kmeans++ ?\n. distance -> m_distance?\n. please proper error message. This \"No. \" is hilarious! :)\n. I guess this needs to be updated.\nAlso I think you can move it to the KMeans class, the base class is a hidden internal one, it needs documentation for developers (base class for KMeans algorithms, these are the methods that have to be overridden, etc)\n. remove\n. line break\n. no camel case please\n. BSD please\nand also add the other authors\n. I think just overloadind CSGObject's method to avoid that save and load work\n. I like these clean ups!\n. I guess DotFeatures?\n. the require is not necessray because this is a private method?\nOr do you simply check at the places where the user defines the features?\n. Can you please make sure we re-use the computational tricks from the Gaussian kernel?\nBest would be to directly use the code.\n. where is width?\n. so it is DotFeatures, then you can change that above\n. good that you re-use code here. Please also consider what I said above about re-using code from the GaussianKernel\n. Please check LHS and RHS sperately. Good for error message\n. If you are checking for NULL you should maybe also check types here?\n. Please add the numbers so that the user knows\n. this can go to linalg, it is a very common call.\n@lambday ? thoughts?\n. DotFeatures?\n. Capital S, but thats a minor\n. Maybe re-use some existing code rather than adding this here?\n. Dot?\n. Please add a unit test for the Gaussina kernel against some matlab or python script with a length scale different to 1, just to make sure results dont change\n. Proper error message please\n. I dont like the fact that the kernel base class contains GP related stuff.\nCan we change this?\n. @yorkerlin consider to start using such things, as they become available\n. @yorkerlin might be useful for the multiclass logistic GPs\n. I guess my point was more to use the logit and softmax implementation from linalg in the CLogitLikelihood and CSoftMaxLikelihood. So can change existing implementations to use linalg\nFITC for multiclass might not work so well, we have to check, but lets discuss somewhere else as off topic here.\n. Computing pairwise distances is the same though.\nIn fact, what about the following:\nRather than having different classes for ARD kernels, why not add an enum to the existing class.\nKT_SCALAR, KT_DIAG, KT_FULL\nfor scalar (same as std), diagonal (your ARD kernel), and full pairwise (a symmetric matrix) weights?\nThen all these seperate classes would just go. Let me know what you think?\n. Why would that be a problem? The gradient method than just checks the enum and does compute different gradients?\nAutomatic would be good I agree, but thats for the future for now ;)\nAnyway, I think this refactoring makes a lot of sense, check out GPy, they also do this, though they just distinguish two cases: normal and ARD. We should do the same via a flag. I think in fact we can use a boolean, too. The pairwise scaling thing is infeasible anyways, so no need to worry about it, sorry that I brought that up.\nhttp://gpy.readthedocs.org/en/latest/tuto_GP_regression.html\n. Some more explanation here? :)\n. nice!\n. why have this variable?\nMaybe remove in a future clean up patch\n. Mmmmh, we should definitely have helper methods that you can give the intuitive representation that then wrap the call to this method. Could you add that in some next patch?\n. as I said below, please have helper methods for this to not confuse users by how to set the weights.\nShould be: Scalar, vector, matrix -- as in c++ type\n. Please print the number of columns and rows in the message\n. @lambday say is matrix product parallelised with openmp?\n. code formatting is not 100% correct here\n. Make sure to document this properly. Having all these different Gaussian kernels might get confusing otherwise.\nI would still prefer a single GaussianKernel which can enable ARD via a flag and which also containts all the stuff needed for FITC inference. Though it might be good so seperate due to seperation of concerns rule. So let's leave it this way.\n. I would really like to get rid of this and instead do a openmp version at some point.\n@lambday another one for you :)\n@lisitsyn or you? :)\n. these variabvles are not really needed.\nThe assert is also not really nice style.\nI know both not your fault, but could you maybe quickly fix them in some later patch?\n. I added an entrance task on this, see\nhttps://github.com/shogun-toolbox/shogun/issues/2702\n. math mode latex?\n. good!\n. please also add convenience methods here and mention them.\nHide the actual method, make it protected\n. we want to remove such methods from SGVector, please do not use them\n@lisitsyn @lambday @iglesias this includes the scale and add, right?\n. add a negative version? mmmmH, we should add this. Can you ping Rahul\n@lambday ?\n. @lambday I put up a trap to get you a GSoC applicant for this: #2717\n. If this is the cost of not having c++, I definitely vote for c++11. This is too messy to read (at least for me)\n. No, please put the idx. Provides help what is going wrong ofr the user\nhttps://github.com/shogun-toolbox/shogun/wiki/Assertions\nCould you send another PR for this?\n. \"Provided CFile pointer is NULL\\n\"\n. see above, please display index, and it is spelled \"cannot\", but it should really be \"must not\"\n. Good idea. Nice to have.\nBut maybe we can do this after the patch is merged? Also example translation is next step I would say, as well as adding more targets.\n. That is actually true. Could you add this to the CMake stuff and update the patch?\n. There is some standard that say #defines should not start with underscores. \nhttp://stackoverflow.com/questions/6756059/c-using-c-code-using-double-underscores-in-defines-and-identifiers\nnot a big thing but maybe good to clean up in the next PR?\n. we dont use { } if the statement is only one line. But thats minor here\ni.e.\nelse if(m_ARD_type==KT_FULL)\n    linalg::matrix_product(m_weights, ltmp, left_transpose);\nrather than\nelse if(m_ARD_type==KT_FULL)\n        {\n            linalg::matrix_product(m_weights, ltmp, left_transpose);\n        }\n. thanks!\n. yeah true, otherwise the compiler might complain\n. can you use the m_ variable here? No need to create a new one\n. Could you use m_evaluation_criteria rather than m_list_evaluation_criterion\n. Pls update the name here too, and the descrption\n. Pls check how we do whitespace formatting in Shogun\nWe also have a charater limit per line\n. The REQUIRE is bad, please see https://github.com/shogun-toolbox/shogun/wiki/Assertions\nIt can segfault, and please do a proper error message that helps the user rather than confusing him\n. Please dont do return by value for Shogun classes.\nRather use pointers, also make sure to increase the refcount, and also tell SWIG that this method returns a new object.\nSee Evaluation.i\n. Please update the documentation\n. update docs here and say that this is for convenience\n. here too, also please update the same as said above\n. ? looks fine to me\n. indentation\n. indentation and name please\n. I guess hiding the whole class is better than just hiding parts?\n@lambday what about the c++ being turned on. I really would like to avoid these guards\n. please get rid of this method\n. Should be documented at least with one sentence\n. Some references would be good here:\n- other classes that can use this\n- paper that explains inducing points? Maybe the JMLR ?\n. static casts without type checks....\nCan this fail from wrong user input? if so, please put a type check in a require\n. Please require them seperately for user to fix things easier\n. please dont do empty error messages\n. I see.\nCould you maybe open an entrace task issue about this?\n. should go on top\n. all includes on top please\n. no {}\n. No {}\n. The error message should be more clear. \"Unsupported ARD type, see \\\"\n. REQUIRE\n. REQUIRE\n. REQUIRE\n. no {}\n. hopefully soon via linalg?\n@lambday how are the linear solves going?\n. please document\n. If a user can trigger it, then yes. See the wiki entry.\n. I see. Leave the guard for now\nI think @lambday is pushing to require c++11 to build Shogun. From then, we can just delete all guards.\n. Maybe reference the class where this is used from?\n. nops thats fine, but please put a comment for future developers to see\n. m_evaluation_criteria = new CDynamicObjectArray();\nm_evaluation_criteria->push_back(evaluation_criterion);\n. same here\n. \"List of used evaluation criteria\"\n. please use a pointer here\n. indentation\n. put a comment that this is type safe by construction and thats why you dont check\n. remove \"kept for convenience\"\n. indentation\n. also remove the \"kept for ....\"\n. Did you write this or take it from somewhere?\n. nice!\n. great!\n. scary ;)\n. This should return NULL when given NULL\n. Same here I think\n. Does this segfault if  param->m_name is not set?\n. These are API changes and have to be documented in the NEWS file since serialised versions wont be compatible anymore\n. @lambday sorry for constantly pinging here, but how are the matrix factorisations doing? :)\n. ok cool!\n. typo: brief\n. These descriptions should to to the documentation of the methods themselves. In the brief section, you should rather point out the high level usage rather than details. \nWhat is more important would be to write down the math of the HMM model here, in latex\n. Also, why do you replace the old description. It is good, just needs to be extended with some details\n. should we maybe consider renaming the latent features to \"inducing variables\" and also reference the JMLR paper \"a unifying view on sparse GPs\" ?\n. Oh, and the math of the model would be good here, including the inducing features\n. It would be good to have the math in the model for such things, and then explicitly reference what in the model this represents?\n. please reference a method to check whether the kernel supports FITC inference, the enum, a method, or whatever\n. maybe consider renaming \"latent\" to \"inducing\" globally?\n. A verb is missing \"can different types\"\n. state math of log marginal likelihood?\nAlso maybe say that it is the approximate log marginal likelihood\n. there might be a memory leak here as \nm_evaluation_criteria->get_element_safe(0); increases the reference counter by one (I think)\nIf it does (check code), then you have to do SG_UNREF(temp_eval); in order to avoid leaks\n. check reference count\n. SWIG needs to know that this method returns a new object, see Evaluation.i in interfaces/modular\n. As we kind of agreed on c++11, lets DO it :)\n. This require statement is bad, see\nhttps://github.com/shogun-toolbox/shogun/wiki/Assertions\n. same here\nhttps://github.com/shogun-toolbox/shogun/wiki/Assertions\n. I know its off topic for the patch, but still. We should keep those clean, to avoid user frustration\n. Think how the documentation is used. People will read the summary first, just interesting in what the class actually implements, which type of HMM in this case (math & model is important for that reason). So this should be high level, talk about what can be done in principle and maybe which methods are important. But then the method documentation should be in front of the methods because it would be too long for people scanning over things otherwise.\nHave a look at Shoun's well documented code, for example the linalg framework, or the MMD framework, or the linear solver framework\n. hey\njust curious, can't there be a convenience method for the case where both arugments are the same?\n. for example here, this is bad due to the pointer de-referencing --- imagine this is an expensive operation\n. thanks for that.\nBTW there were some plans to provide obtina_from_generic for all Shogun classes automagically. Interested in doing this at some point? :)\n. Could you also add the \"new\" news section? This is the one of the old release...\n. Wait I am confused. Why did you add this here? Looking at the file doesnt seem it has the correct structure now\n. please make doxygen reference the enum definitions, see\nhttp://stackoverflow.com/questions/2335091/how-do-i-get-doxygen-to-link-to-enum-defintions\n. Which variables? Be explicit here, transition and emission probabilities\n. You dont have to say this. Remove\n. \"Ex\" ?\n. dont reference CMath. Otherwise nice to say.\nBTW: WHY do we use bubble sort?\n. Maybe say that we assume that those matrices are normalised?\n. please make all references in such way that doxygen can create links, you can create the pages locally via running \nmake doc\n. Can you write all math in latex?\n. Please use the standard Shogun style\n/** Constructor\n * @param p blabla\n.....\n*/\n. Rather describe this in word?\n. please use latex\n. line break issues\n. I know it is not your fault, but we have to get rid of such error messages.\nThe \"No can be removed\", and please provide proper information in the message, see https://github.com/shogun-toolbox/shogun/wiki/Assertions\n. Could you document all parameters here?\ndoxygen will complain otherwise?\nMaybe reference the math somewhere?\n. same here\n. and here\n. @param dmu the explicit gradient ....\n. With all those vectors around, it might get a bit confusing what is what.\nI suggest you write down the math of the thing somewhere, with well defined expressions, and then you can reference this. I.e. \"this method computes the $L_R$ matrix of expression XYZ\"\nWhat do you think?\n. No pointer here?\n. C* instances should never be on stack\n. Minor style: Shogun doesnt do { ... } if there is only one expression inside\n. Put a REQUIRE(param, \"param not set\\n\") here to avoid segfaults in the next check\n. same here, a NULL check is good\n. param NULL check\n. Could you explain me this lock business here?\n. Need to be careful about deadlocks when using this.\nWhere is the parallelisation happening?\n. Lets keep the lock to  be stable in terms of memory. I dont think the speed gain is too massive?\nThanks for explaining nicely\n. so you remove those checks everywhere.\nIs the idea that if param is set, then all those are redundant? Or are these things checked elsewhere anyways?\n. newline missing, and no space at the end pls\n\"Factors not set.\\n\"\n. newline\n. ah great, very good style!\n. why that?\nWe dont want that I think\n. I know it is very minor. But could we do\n\"Provided index (%d) is larger than number of rows (%d)\\n\"\nCapital letters at beginning of sentences, uniform style across Shogun. This is what our users see and it should be as clean as possible.\n. like this overloading\n. Do these base classes need to be exposed via swig?\n. Slight change:\n\"Inference method %s does not support FITC inference\\n\"\nMore easily understandable for user\n. @lambday could you do something here?\n. their code is deleted?\n. please return NULL in case you get NULL\n. this might break python integration tests, did you check whether it does?\nIf not, then it is not covered by python examples, which is also bad\n. @lambday we need something as\nhttp://docs.scipy.org/doc/scipy/reference/generated/scipy.linalg.cho_solve.html\nor at least a triangular solve in linalg\n. ?\n. ?\n. technically, there should be seperate tests for all of those variables.\nUnit tests should be as small and atomic as possible\n. same here, the test is a bit too long.\nI suggest that you code up some set_up method that initialises everything and then the unit tests are very short and seperate for the gradients?\n. \"Matrix dimension (%d) does not match vector dimension (%d)\\n\"\n. \"Operator \\\"%s\\\" is not of type DenseMatrixOperator.\\n\", op->get_name()\n. Some math details would be good, as well as a wikipedia link.\nAlso, state the assumptions to the operator\n. Capital letters in sentence starts in all exposed documentation  please\n. We changed to BSD, see other patches\n. BSD\n. same comments as above\n. Maybe with capital.\nLLT solver\nMaybe? What is the reason? Can't we ask eigen3?\n. please no _ at the end\n. math? wikipedia?\n. Solve\n. D\n. BSD\n. wrong order of numbers (num_vectors, i_col)\n. wrong order\n. mmmh\nshould not be. @lisitsyn any ideas about this?\n@sonney2k \n. Yeah I see but why the commented out code?\n. I think putting the naive method in the base class (with a warning on costs in documentation) and then in the ARD kernels overload with the efficient version. Like you said, its a good approach I think\n. okok\n. ok!\n. Do you know of \\Vert ?\n. Does doxygen class docs reference the method if you write it like this? So that users can click their way through the methods to check things?\n. Sorry I did not spot this earlier, but \"Index\" is ambiguous. Please use row or column index\n. too negative? Maybe some details how to avoid? Keep in mind users dont know what W is\nAlso nlZ is not something users might know?\n. Add a warning if max number of iterations reached without convergence?\n. check for param==NULL\n. param==NULL check\n. These comments should be visible to users, dont you think?\n. maybe print the difference here?\n. and also the m_tolerance level\n\"Max iterations (%d) reached, but convergence level (%f) is not yet below tolerance (%f)\"\n. Could you do seperately for both?\n. zero?\n. what about the default value?\n. same here, what does the default do?\n. our equal method has a precision parameter. But it is very very slow\n. should be ok mostly they will be the same pointer\n. This should be documented\n. Shouldn't this b absolute value?\n. BSD\n. could you use int32_t here, or index_t, whatever you feel is appropriate?\n. pointer?\n. wouldnt this be an enum?\n. enum?\n. because? please document\n. Could you do doxygen readable comments? with two asterix?\n. enum please, and also put this in description that its an enum\n. int32_t ? int64_t ?\n. Good that you document this!\n. See the other latest patches for the exact text\n. BSD\n. Most of those general includes do not need to be there actually, try removing most of them. The base class should theoretically do it\n. a link would be good here\n. could you offer a constructor that hides the cache size and sets it to a default value. I always thought it was annoying to choose it ;)\n. Like this one but with no features\n. always document default value if there is one\n. whitespace\n. our naming convention is \"m_\" prefix for member variables\n. thats done I think\n. This comes from old code. We are now using SGVector (no pointer, this is on the stack). It automatically handles memory. No need for any SG_FREE SG_MALLOC anymore\n. The whole method can go once you use SGVector rather than float64_t*\n. We try not to use ASSERT anymore, see here how to write error messages:\nhttps://github.com/shogun-toolbox/shogun/wiki/Assertions\n. buf = SGVector<float64_t>(num_vec);\n. the pointer in the method header should be an sgvector too, or maybe the whole idea of passing a pointer that is filled can be simplified?\n. @lambday can we do some linalg here?\n. this can go I guess\n. We try to use the functions from CMath, not from libc (this way we can debug if we want)\n. Is this cast safe?\nIf not, a dynamic_cast with a check and a REQUIRE message would be good\n. Could you require those seperately? Users then have the information which side is NULL\n. @lambday @lisitsyn Could we do some parallelisation here? openmp?\nKernel matrices are parallelised but gradients are not.\n. the sq_lhs etc are not registered? maybe they should\n. whitespace issues, please reformat\n. Could you remove those prints? they pollute the std output when running things on the buildbot\n. exactly how I meant it!\n. Minor:\n\"Left-hand-side features must be of type CDotFeatures\"\nIts not always a matrix and the term \"cast\" is unknown to many high-level language users\n. where are these sin/cos coming from?\n. formatting issue\n. In place would be good in general, but keep in mind this is just a vector operation (and the vector is not large)\nIn-place is more crucial when we create copies of the kernel matrices (which are the main memory bottleneck of GPs)\n. base class needs to be exposed to the interfaces? Users will never use it right?\n. newline missing\n. XSize should be called x_size\n. could this work with the new SGVector methods? rather than bool do_free and pointers?\n. this can go then too\n. can we make dists a matrix?\n. no need to loop, the distance class should be able to compute all distances itself (and in parallel)\n. same comments as above\n. formatting\n. This should be possible via linalg operations\n@lambday comments?\n. formatting\n. is this documented?\n. why would this method not be purely virtual?\n. Capital N and print the given number. Also it is iterations, not number of clusters\n. This doesnt make sense to me\n. should be documented\n. memcpy would be better here\n. always print the given number, and dont use \"No.\" but \"Number\"\n. eventually those should be renamed to \"m_\" but lets do later to not break tests\n. A wikipedia link and references to subclasses would be good\n. document what kmeans++ is in the class description and reference it\n. cl_list_pat\n. BSD please\n. Please make sure your error messages are proper. This one has a typo and doesnt print the given number\n. not sure you need all those includes, probably not\n. +1\n. Please mention that Shogun does COL-major matrix storage\n. for (index_t i=0 ...\n. 80 line character limit (roughly)\n. linalg, for add/scale/ and dot product\n. @lambday could you share which methods are here. Example call\nMaybe you can help @sorig a bit on how to use linalg, he probably will request a few more things (i.e. element wise sine/exp/cos, etc)\n. @lambday like here, we can make massive computational gains via using element wise operations on matrices rather than direct loops. Maybe even GPU?\n. This seems like an element wise matrix operation.\n@lambday can we do something about such operations with linalg? Could do it on GPU\n. Maybe have the proper license as for the other files here?\n. thats ok and standard. Will be catched by the REQUIRE statements in the later code\n. Could you be more explicit here?\nI.e. state the numbers which dont match, this can be done via multipe REQUIRE statements\n. \"Features not set\" is enough\n. seems like this could be done via linalg more efficiently too (openmp, GPU, etc)\n@lambday \n. this is already in lingalg\n@lambday can comment on syntax\n. this too\n. element wise operation on matrix, again maybe there is something better than a naive loop?\n. ah I just only saw all those comments.\nAmazing!\n. Ah yes, that is exactly what I had in mind.\nHow much work do you think this is?\nI see three major options:\n1. naive loop with openmp\n2. eigen3 (has some unary operations implemented efficiently using sse2/MKL etc)\n3. opencl (put everything on GPU and go from there, good if the matrix sits on GPU anyways, as these squared features would)\n. explicit please\n. TODO :)\n. Maybe we should start putting references to the kernel cache. But I dont really know the mechanics of this \n@lisitsyn do you?\n. Minor. But since the documentation is in English, sentences should start with capital letters. I know this is done nowhere, but we might start doing it ;)\n. There sometimes is a standard index?\n. @lisitsyn can you comment on exposing abstract base classes via the modular interfaces? \n. yep!\nBut careful about freeing things\n. minor: no { }\n. \"supports\" with an s\n. whitespace\n. @lambday I would actually try to block any such implicit conversion. What do you think?\n. Typo \"projec \"\n. \"we\" with small w\n. full stop \".\" missing at the end of sentence\n. remove c++. This holds for all modular languages. Just Shogun's API\n. I really really like this TOC. And it nicely showcases the massive amount of work that you have put into this\n. \"References\", with an s\n. \"It is not hard to exend the presented methodology ...\"\n. btw is therea reason why you use double backspace to escape latex math?\n. \"Following sections coveres some detailed\"\nmake it\n\"The following sections cover detailed\"\n. \"The GP prior ($p(\\text{f})$) and\"\ndont but brackets around the math, just coma\n. It also increases numerical stability\n. Good point!\n. It is ....\n. Before the code start. Could you add a sentence on what type of things you are going to do?\nSomething as \"We now define code to train a GP classifier using Shogun.\"\n. Why the name extract1?\nAlso, some text explaining what this does would help too\n. btw why do you call the filename inf?\n. Since this is parsing code, it pollutes the notebook reading flow a bit. \nWhat about adding a parsed file to shogun-data instead, that you can just load straight away? Maybe seperate files for labels and covariates?\n. Could you avoid doing comments in the same line as code? It makes thing look messy. Also add a couple of newlines here to decrease the density of characters\n. love the plots! nice\n. train2 since train was already taken?\nWhat about some suffix, such as \"train_large_scale\"?\n. A comment here helps people understand what is going on. You should add something\n. same here. More informative naming is good!\n. An intro here would be nice. Say that you are reproducing the same experiment with large scale GPs ... not just theory, also intuition and high level roadmap\n. where is this defined? Also would be good to add a comment what this call does\n. sorry I found it! Ignore :)\n. Add some conclusions!\n. overflow risk with 32  bit and product?\n. This is good!\nBTW do we have both cases covered in our builds @vigsterkr @besser82 @lambday ?\n. I have to admid, I just learned about this construct ;)\n. Not sure I understand this way of constructing strings to tell the GPU what to do\n. I see, so there is a number of predefined operation strings that we provide, but users also can put their own ones.\nHow does a wrongly typed operation's error look like?\n. Amazing, I saw the patch.\nOnce it is merged, @sorig can use it in here.\n. ok!\n. why is the field called \"is\" ?\nwhat about \"m_compute_gradients?\nMinor \"Whether gradients are computed\" (capital W)\n. super minor: we usually call the variable in setters the same as the member, but without the \"m_\" prefix\n. This seems weird to have an error in this method?\n. I see, so this is more \"assert_compute_gradients\"\n. if you have to change this everywhere, shouldnt this be \"false\" by default?\n. haha, awesome!!! :)\n. ok cool\nand overhead?\n. Yeah I agree, we should be as explicit as possible\n. btw do we also have these unary operations inline?\n. ehm inplace I mean\n. please send a seperate PR for this fix, and add unit tests that make sure the rationale is explicit\n. you want to use log2? any reason for that? Its likely to be forgotten, dont you think?\n. typo\n. renaming variables will break integration tests.\nCheck whether the ones broken are caused by this renaming, it is likely to be the reason\n. you dont use the get_scale() trick here as you did above for get_width. Any reason?\n. I personally dont like these *2.0 lingering around everywhere here......\n. What about doing this in the .cpp file instead? Saves a header include\n. Any reason why you are alternating error messages?\n\"must be greater than zero\"\n\"must be positive\"\n. thanks!\n. reason for this?\n. ok\n. I quite liked your get_sigma() get_width trick above as it makes the code look nicer.\nAny reasons why not ?\n. I agree, but the get_scale thing is much more easy to read. Doesnt matter though\n. Cool!\n. Compiler optimisers take care of such things\n. OK!\nUniform is good. Feel free to change things that you dont like on the fly\n. I see, but why commenting out rather than removing?\n. Ah yeah of course. The stability is important\n. maybe lat_type should be changed to inducing_feat_type at some point\n. Do we start like this?\nNot rather a random subset of the training data? Or is this done later?\n. please add row/col to the error message\n. Those parameters should be documented in the class docs\n. Maybe print name of kernel? \n. can't this be done in a single memcpy call?\n. Set with capital S\n. set_optimize_inducing_features(bool optimize_inducing_features)\n. much cleaner   +1\n. ah nice! finally\n. I guess the motivation for this renaming is that we now not only have FITC inference but many sparse approximations?\n. For the future. Pls do the renaming in a different patch, this is very  hard to review otherwise\n. Is all this deleted code just refactoring?\nAgain, it would be good to decompose huge patches like this one a bit. I cannot really see whether this stuff is now deleted or just moved as changes are hidden among the many others\n. Could you comment on how much you changed of the deleted file?\n. not reviewing all this as I guess its just refactored, right?\n. Some references to other classes would be good here, i.e. what it can be combined with. Doxygen will show subclasses anyway thought, so only orthogonal references needed\n. Great to have this in now!\n. worth precomputing CMath::exp(m_log_scale*2.0) to make code more readable?\n. For all those complicated looking blocks, a one sentence comment on what happens on a high level might be good for future developers to understand this\n. Does second loop depend on first? if not one could replace by a single one?\nParallelise? Or doesnt make sense?\n. GPU operations here since iterating over columns of a matrix?\n. Argument param not set\n. High level description of what this does and involves maybe?\n. Very nice overall documentation here! I like it\n. We should probably at some point get someone to proof-read all the math\n. great\n. Pls send renaming patches sperately in the future\n. Do you want to write to Jacob and ask him whether he agrees to re-license his code under BSD?\n. awesome that we are  using this now!\n@lambday ping\n. Is this controlled by the user?\n. a bit weird this overloading, but I realise it's not your choice\n. pls use \"\\exp\" when in latex math mode\npls use \"\\Vert\" rather than \"||\"\n(also grep for it in the docs and change all others if possible)\n. \\exp\n. I think it would be better to be explict about parameters here rather than using default values?\nAlso, can we get rid of this annoying kernel cache size parameter? This should be set to default instead\n. Ok good\n. Cool, so then we can change all GP code to BSD\n. np.exp is better I guess\n. I like this!\n. Start sentences with capital letters.... it's English after all ;)\n. Why virtual?\n. Ah sorry, forget about this, needs to be virtual in fact\n. The documentation should be a bit more clear here.\nE.g.\nReturns whether this instance supports dot products /whatever with the feature class specified in the argument\n. Can you elaborate?\n. I feel all those should be a REQUIRE rather than an assert since the user can make them fail\n. Maybe put in a reference to the \"get_feature_class_compatibility\" method called? So that a user can check .... or if there is a bug, it can be identified more easily\n. seems a bit large for a unit test....but well\n. we now have auto\n@lisitsyn \n. What about having a Macro to compare matrices with EXPECT/ASSERT_EQUAL?\nWhere would we add this?\n@lisitsyn do you have an opinion on this?\n. Could some of those imports be forward declared instead?\n. Get with capital \"G\"\n. No need to include the CMap include here, this can be forward declared\n. @lisitsyn elaborate pls\n. +1 for minimize\n. @lisitsyn  ??\n\"Does\" with capital D\n. I feel this should not be a CSGObject ..... \nWill this object be exposed to SWIG ?\n. forward declare\n. This is bad.\nAlways first reference the new one before de-referencing the old one. If m_fun == fun and the object is not referenced elsehwere (counter=1), then this will cause deletion of the object, which is a bug\n. @yorkerlin It would be good if you documented this a bit better. If @lisitsyn does not get the purpose, no-one will ;)\n. +1\n. \"Help function\" with capital H\n. could you be more precise and also more neutral here: \"our representation\", \"classical\" are not well-defined terms\nI guess you mean column-major (Shogun) and row-major. None of those are \"classical\" btw.\n. see above.\n. whitespace is not Shogun style here (minor)\n. auto!\n. Error message should contain index\n. What if var is NULL?\n. auto\n. pls put a source code comment on the reasoning here\n. auto\n. g=NULL?\n. That's a no-go!\n. again, pls put a comment\n. BTW I feel like there should be std-libraries to do this conversion for us. E.g. Eigen should have one.\nThis is so much code for such a simple and redundant operation. It is also not minimization specific, which makes this bad code-design. That is why I actually suggest to\n- either remove this code and use a library here\n- or put those operations to a more general class that can be used elsewere in Shogun\n. see comments from above\n. REF before UNREF, this is a bug\n. whitespace redundant\n. this class needs a work-over before merge\n. forward declare\n. Capital letters in sentence starts!\n. Rather put the doxygen readable class reference here.\n. good doc!\n. pls use Shogun types here\n. REF before unref\n. It seems a bit of an overkill to implement objective functions in this way....don't you think?\n. To me it feels that this code should go into a base class?\n. TOO MUCH stuff for a unit test.\nRather put these classes into Shogun code base as minimal examples how to build such cost functions, and reference them in the doxygen so that people can look things up.\nBut in fact @lambday what are your thoughts on making all this simpler? It is so cumbersome to define cost functions this way....\n. Good, pls make public and reference in documentation from base class\n. descent-based\n. As usual: Capital letters: \"Returns the name \" ....\nMakes the doxygen look much nicer\n. Usually?\nDetails?\nShall we not enforce it?\n. \"Learning rate\" capital L\n. This error message does not make sense\n. Why would there be a seperate class for learning rates?\nJust creates tons of overhead: C++ code of the base class, etc etc, this seems to be just a float.\n. Classes with just one virtual methods should not inherit from CSGObject in my eyes. See my comments in the other PR. \n@lisitsyn can you suggest a better way here? You know all the good patterns.\n. See also the LearningRate.h class\n. Yeah this CMap thing seems a bit of an overkill. Are there any computational implications?\n. minimize() \n. supports_batch_update\n. first REF then UNREF\n. This method is not require to be visible from SWIG, right?\n. Capital C\n. auto\n. The\n. whitespace\n. Should this really be another class? CPenalty?\n. I understand\n. Could you put a BSD license here and add your name?\nSee any recent Shogun file for a template\n. License like here\n. three times the same?\n. parameter should be called \"mean\" not \"cov\"\n. Good!\nThese now need documentation, but we can add that later\n. This should be updated and explain the different parametrisations possible here, and what the matrix contains in the case and which dimensions it has\n. Sorry just saw the different type\n. These need documentation eventually that explains what parametrisation it corresponds to\n. Shogun starts indexing at 0\n. Maybe we can improve the error message a bit to\n\"Mean dimension (%d) and covariance dimension (%d) do not match\"\n. see above for better error messages\n. This should go to a private/protected method\n. The error is only printed if Eigenvalue computation was a success. \nI suggest to print the smallest eigenvalue in a warning, and then throw the error afterwards, independent of the eigenvalue solver\n. need a new field here with the m_cov_type\n. m_dimension should be initialised to 0 here\n. calling the base constructor here doesnt work for initialising the dimension.\nThis is since the init() method is called afterwards. So you need a m_dimension=mean.vlen statement after init();\n. pls use enum, not string\n. put the enum ECovType here, see Gaussian.h\n. I think Wu Lin was also in here\n. see below\n. minor: maybe \"covariance (diagonal) dimension (%d)\"?\n. enum, and I think it should be called COV_DIAG\n. base constructor issues as above\n. exactly like that!\n. minor: could you make this m_cov(0,0), its cleaner\n. I think this copies the vector, \nhttp://shogun-toolbox.org/doc/en/latest/SGMatrix_8cpp_source.html#l01088\nbut it would be better to return the same memory (this should be documented in the getter doxygen)\nShogun stores vectors in column-major (fortran) format, which means you can extract subsequent columns of matrices without copying. So get_column_vector is your friend here:\nhttp://shogun-toolbox.org/doc/en/latest/SGMatrix_8h_source.html#l00115\nJust realised there is no version of it which returns SGVector, so you could add that for convenience. Ask me if unclear\nSummary:\nWe dont want to copy here, but return a reference to the memory. And this should be stated in the documentation\n. Should throw an error if m_cov is not COV_SPHERICAL\n. Should throw an error if m_cov is not COV_DIAG\n. Should throw an error if m_cov is not COV_FULL\nThis is wrong as the m_cov stores a cholesky factor. So you need to allocate a new matrix here, compute LL^T, and return that.  Also the documentation should state that this gives a copy.\nI also suggest to add another methods get_cov_full_cholesky() which returns m_cov if m_cov_type == COV_FULL\n. I think linalg can now do this, could you check?\n. This is the way to sample if m_cov_type == COV_FULL.\nThe other  covariance types work differently, so you need to do a switch(m_cov_type) here and impleent the corresponding behaviour (which is computationally cheaper)\n. This should be a warning, and only say \n\"Smallest Eigenvalue is %f.\\n\", ev[0]\"\n. no else here, the error should be thrown in any case\n. Since this method does not return anything, I would call it:\n\"update_cholesky\"\n. the whole method is only correct for m_cov_type == COV_FULL.\nI suggest (as above) to add static methods to the class that compute things based on what the covariance is, and then add a switch over m_cov_type here and call the helper methods\n. Can be done with eigen\n. I think this can be done with Eigen without a loop, using matrix expressions\n. or even more clever, with dot products and linalg\n. add WU\n. Computational costs should be stated here for differenc cov types\n. I think it would be good to add static methods for:\nsampling:\n- full covariance\nlog-pdf:\n- full_covariance\n- diagonal covariance\n- spherical covariance\nand then call these in the class methods\n. \"The\" base class \"does\" the following job:\n. \"is to simply use the gradient, or XXX\"\n. I think doxygen cannot parse this if there is no space after the *  -- but not sure.\nIn any case. \"Update\" with capital U\n@yorkerlin I know this is minor and it must be annoying that I constantly moan about this, but let's just use proper English for the interfaces ;)\n. why is this called \"variable_reference\"?\n. undocumented?\nWhat would this do?\n. yep, as we discussed. Thats good now, we can add as many of those classes as we like\n. great!\nThis \"variable reference\" thing is what exactly? Should be documented, I find it confusing\n. Please elaborate a bit more here. I don't understand what this class does without reading code. The class description should at least give an idea of what it is used for and what the inferface allows\n. No reference counting here, but a NULL check maybe?\n. I see. Thats actually a good way to do this!\n. \"Set\"\n. what about the REQUIRE?\n. If the class does not inherit from CSGObject, please remove the \"C\" prefix.\n@lisitsyn do you agree?\n. Should the m_fun not be deleted? Or something? How do you plan do manage such references?\n. @vigsterkr @lisitsyn the auto keyword breaks travis. What to do?\n. parallelise this loop maybe?\n. OK for this class. Just the issue of memory managment remains (the not-Shogun classes)\n. Doing what exactly? Document\n. Please elaborate what this does from a high-level perspective, the example is good!\n. \"Get\" ... \n. Document!\n. ?\n. Again the question of reference counting.\n. See #2880\n. Please mention the other classes that would use this class and how, using doxygen references\n. very good idea!\n. Also pls explain the context mechanism here, referencing the methods\n. Good!\nBut also here: pls reference all the classes that play together with this one\n. References to related classes, explain the interface, referencing the obtain_variable_context method\n. I like the new description, very helpful.\nAgain it would be good to put the class in context and reference other classes via doxygen links\n. What do you think about having a Shogun base class without serialisation and all the stuff. ONLY reference counting.\nThen all your classes could be subclasses of that one and users would not have to do things.\nSee #2880\n. How you did it here is fine, but it is a source of memory leaks\n. question mark at the end\n. why the if?\n. good!\n. See my other comment and #2880\n. Class references pls, otherwise good\n. Pls explain the interface, referencing the main methods of this class\n. Tell people what a first order stochastic cost function is (they might not know)\n. good !\n. Very nice description.\nCould you make sure doxygen produces links to the other classes you mention (google the syntax to reference other classes)\n. I have the feeling we should discuss this issue once more on IRC.\n. This class implements L2 regularisation within the CLASS_REFERENCE framework\n. Again, pls reference other classes that use this one and explain the high level interface\n. Elaborate, recferences\n. minor: we do not use {} for single line loops/if\n. I would go for the more efficient one.\n. So this makes our NNs dependent on linalg, right?\n@lisitsyn do you object?\n. @yorkerlin ???\n. minor: should go to a newline\n. shouldnt this part of the code now be removed? since linalg convolve comes with a NATIVE implementation?\n. I see.\nI support this idea in general. However, making the linalg::convolve interface depend on autoencoder parameters is not a good idea. I think you want to add a boolean flag and call it something else. Then I am fine with the idea. \nWould be good if @lisitsyn and @lambday could comment briefly\n. Even though we kind of wanted to do that.......\n. I mean everything else is stupid. Having such ifdef madness is not desirable. We rather want to use the linalg interface everywhere and rather not hard depend on things like eigen3 .... maybe we can revive the discussion\nagain, I'd like to head @lambday on this\nalso @besser82 and @vigsterkr \n. Thanks for the feedback, @iglesias :)\n. minor: Can you use the enum definitions here please?\n. what is the a reason for NATIVE here?\n. This could be easuer to read if we overloaded the *= operator of SGVector\n. here we have the + operator, why not use it?\n. @besser82 @vigsterkr how to do this in the proper was of using the g++ environment set in cmake?\n. I need some way to ensure that linshogun.so is already linked when running this with make -j. How?\n. @sorig you mentioned the explicit imports in python. We need the same thing for c++.\nWe have a problem here though: the directory structure of shogun. Basically, we somehow need to infer the include path from the type. Any ideas for doing this in an elegant way?\n. I think we need to distribuish constructors for SGObject (with Cprefix) and other objects\n. Same here. I had to comment out the LongRealVector output = test_labels.get_values() below. What are your thoughts on this problem?\n. This does not make any sense for cpp unfortunately, as the print functions do not take SGObjects or vectors.\nWe can of course just remove this, or define a print function for vectors only maybe?\n. This works via \"copy\" above, so I cannot use it as the C is not prefixed\n. Just saying: add_executable does not work. The .cpp files do not exist when cmake is at this point. Maybe via some dependency construction this might be changed. One could maybe have two loops: one that generates all the .cpp listings, and a second (that depends on the first) that compiles them via add_executable....But I dont know, need you guys input\n. @lisitsyn and away are the memory leaks! Good stuff I forgot about that. I do not need to SG_UNREF ever?\n. No ok, I think we just remove the print.\nDo you think we should remove it from the grammar all together?\n. Done in https://github.com/shogun-toolbox/shogun/commit/c220dac005387df3a32fd374198649053a57f1fe\n. 1. Could you do this without a do-while?\n2. Something is weird with the condition. You create random numbers until \"hitting\" something???\n3. Please when changing things, make sure they are formatted according to our guidelines (even if the KMeans class itself is not)\n. why this?\n. The header file also had the compact mentioned in its class description. Please update as well\n. whitespace before this\n. Please never send PRs with TODO in them. Just fill it in.\n. we usually use 4-space tabs for indentation\n. please respect the 80 characters margin\n. @lambday yes I agree here. But for now, CMath is fine if this does not yet exist as it is scalar valued.\n. @yorkerlin Hey Wu, please don't do such checks in the future. They are really hard to automatically re-factor. Rather use two lines with #ifdef\n. we use /* comment */ i.e. with spaces\n. we usually name variables with lower cases.\nAlso I suggest to call it random_inds\n. can you replace the get_feature_vector and free_feature_vector calls with the SGVector version. This is old code before we had SGVector, and the new version just works as SGVector<float64_t> vec = lhs->get_feature_vector(Cl) and doesnt require cleaning up afterwards\n. SGMatrix has the () operator which you can give row and column, could you use this here? makes the code easier to read. You see you have to clean up other peoples mess ;)\n. vfree can go then too\n. we have a new feature in the code that does \nauto lhs = ....\nYou can use this here to make the code easier to read. It is mostly syntax sugar\n@lisitsyn we can use this in Shogun right? Any checks needed....?\n. minor: no whitespace here\n. Minor: English sentences start with capital letters. This also goes for class comments.\nI know nobody does it in Shogun, but we could start at some point ;)\n. Can you document both parameters seperately?\n. Can you register these parameters using SG_ADD? See the other classes\n. Is there any reference to cite where this is coming from and where one can find evidence that it is useful?\n. Wx+b can be latex as well, as well as all the other math symbols in here\n. No need for this, SGVector has the [] operator\n. Ah sorry. I see what you are doing here. But could you still use a SGVector for this? No pointer? You can tell SGVector to not delete the data it received in the constructor\n. Can be used .... Capital C\nCan you also put a reference to a paper here, where there is some evidence that this is useful? Thanks\n. Same as before, pls dont use pointers\n. SGMatrix has the () operator\n. Same as before, these unit tests are too big! \nTry to test much smaller units .... split more tests. Unit tests should mostly just cover a single method, not the output of a full chain of operations as here. Try to isolate things more \nYou can keep the big one as well though ....\n. minor: we dont use { ... } if there is only a single line of code inside\n. I think this should be the default, not an option. Don't you agree?\n. please remove these bools. We should always precompute the norms, no?\n. if you make sure that the rhs norms are always computed when the rhs is changed, these are always up to date, so no need for booleans\n. open mp can be used here. Might be good to check that\n. combine this and the next line\n. Can you investigate the free_feature_vector mechanism a bit and see whether this call is really needed?\n. all these methods should go\n. good test, I guess we just need one of them if the bools are removed\n. so many loops .... what about some eigen3/linalg dot products and or openmp?\n. The reference should be included in here, not in the PR description. Nobody reads PR descriptions ...\n. Nice. Yeah if you find unregistered parameters, do add them. They are what makes Shogun classes serialisable.\nAlso make sure that they are initialised properly, otherwise the automatically generated serialisation unit tests will fail in travis\n. These constructor initialisations have caused troubles in the past.\nCan you\n- Initialise all members to default values in the init method (before SG_ADD)\n- call init from constructor\n- overwrite default values the were passed as parameters\n. can you call this thing different?\nmanual_bias?\ncompute_bias?\n. minor: we usually have the same name as the member variable in the parameter of a method, i.e. bias_flag when member is m_bias_flag\n. Please always state the current values in such error messages. See\nhttps://github.com/shogun-toolbox/shogun/wiki/Assertions\n. remove the %s@%p This is added automatically\n. we use 4 space tabs\n. See here https://github.com/shogun-toolbox/shogun/wiki/Shogun-development-guidelines\n. linear models do not need to store model features. Only kernel machine do need to do that (I might have introduced this nonsense years ago ;) )\n. remove\n. proper naming of the argument please. And please also doxygen document properly. Use CAPITAL letters in sentence starts, use @param to document parameters\n. whitespace missing\n. this can go\n. I dont get this comment. Maybe re-think it :)\n. Set with capital S\n. Please do proper comments. The added information of this one is zero.\n/* If true, bias is computed in ::train method /\n. comment unnecessary as the code totally explains it. Only write comments if they are helpful\n. No need for a loop here. You can compute the covariance matrix as a matrix-matrix product. Use linalg\nSee e.g. http://web4.cs.ucl.ac.uk/staff/D.Barber/pmwiki/pmwiki.php?n=Brml.Software\n. If you find those, can you replace them with something proper on the fly (this one can be deleted anyways)\n. same with this guy. Matrix vector product\n. The covariance matrix is positive definite, you can use LLT here.\n. No need for a loop here.\nSee e.g. http://web4.cs.ucl.ac.uk/staff/D.Barber/pmwiki/pmwiki.php?n=Brml.Software\n. Where is the ridge added to the diagonal?\n. need a REQUIRE that this pointer is not NULL\n. See https://github.com/shogun-toolbox/shogun/wiki/Assertions\n. ehm, could you use this operator properly? mu(j,i) ?\n. OK! Great catch.\nEven better would be to fix this and then use linalg\n. But the results will not change at all, this is just precomputing things\n. I see your point here.\nIn fact, I think it does not make sense to precompute the rhs squared norms at all. Distance is only computed once no?\nMaybe only precompute lhs?\n. What about:\nstoring the lhs norms always\nnot storing the rhs norms\nOffer a method that computes all pairwise distance between lhs and rhs, and precompute the rhs norms in there?\nH\n. Actually I had another thought about this.\nIt should be like this (and this is close to what you did initially)\nBy default, nothing is precomputed (we dont know which distances user wants to access)\n and both the rhs and lhs norms are just empty vectors\nThere is an optional method \"precompute\" (plus two method precompute_lhs and precompute_rhs that precomputes both sides\nWhenever rhs or lhs are changed, the norm vectors are reset.\nThere is a method to compute the distance for all pairs of lhs/rhs. This one obviously calls precompute inside\nThen you can manually call the precompute in KMeans and/or ask for many distances\n. No I mean to have a method for each rhs and lhs, and one wrapper for both\n. The wrapper just calls both methods, so that you dont have to call them manually. You can also just skip the wrapper. Not important.\nI dont know where the rhs/lhs are changed, you have to check that. But you can definitely overload these methods, reset the norm vectors and then call the base class method\n. This has a reason I guess?\n. we use 4-space tabs\n. Can you be more informative here: No features provided and no featured previously set\n. please respect the 80 char margin\nAlso, please writter better error message, see the development guidelines in the wiki.\nThe point is to say \"Number of training vectors (%d) does not match number of labels (%d)\" so that users get a hint on where they made a mistake\n. BTW I think linalg has mean computation in it. Could you check that and use if the case.\nIf not, can you open an issue that is to implement mean computations in linalg?\n. remove the %s, we do this automatically now\n. 4 space tabs\n. This doesnt make sense. You computed the mean above, and now you might not store it???\n. why the src_ prefix?\n. This technically needs doxygen comments\n. no need for this space\n. Good!\n. See my comment about error message above. 80 char margin\n. Expected Dense Features (%d) but got (%d)\n. looks good.\nPotential for linalg operations, i.e. covariance matrix computation.\nIf you are up for this, feel free to open an issue to add that to linalg\n. please use the () operator in matrix for this\n. can you add the ridge after you have computed the covariance below ? Makes more sense to me\n. Your name here?\n. I have to run now, will comment this a bit later\n. I would rather use if(m_rhs_squared_norms.vec)\n. Can you investigate whether this is a problem or not?\nIf de-activated, liblinear should document this, and maybe even overload the setter and throw an error telling the user that this is not allowed\n. I guess this method should be void and set the bias internally, rather than returning it\n. This documentation has zero information content. State what the parameter does\n. proper docs please\n. explain rather than stating useless things ;)\n. whitespace missing after coma, this also happened above\n. useless comment :)\n. btw this is also useless code, the matrix is overwritten below anyways\n. just set it here rather than adding\n. the index is wrong, it is just the diagonal. You can also do this via eigen without writing a loop btw\n. set rather than add\n. and if eigen says that it cannot factorize the matrix, this method should return false, after printing a warning that the matrix was not positive definite. This should also be unit tested if possible\n. Haha, happened again. Took me too long to comment above.... :) Will have a look at the unit tests later today.\nLots of comments -- but this patch is getting there. Keep the updates coming, they are good!\n. can you open an issue to add mean computation to linalg?\n. Could you put comment on this in the class description of liblinear?\n. Minor: capital T in sentence starts\n. This comment should be on top of the description\n. This comment can go, as the code makes it clear.\n. Exactly like this! :)\n. His comment can go.... And this allocates btw not get\n. I think Eigen can also take a scalar here, rather than a vector\n. Remove. See below\n. Make this a warning inside an if block and return false\n. Haha again have to stop here ;) soon!\n. ah noooo\nThis is something I really want to avoid ... double code.\nRather assume that linalg lib is available always.\n@lambday @vigsterkr what about just requiring this? We had the same thing for eigen3 and it was a pain -- also lots of bugs in the manual code ...\n. Minor: we dont do {} if there is just a single line\n. @lisitsyn if we enforced c++11 here we can to auto everywhere. What do you think?\n. need to register the squared norm members here as well\n. but a big warning here that the user has to track these calls when features are changed\n. another warning should be put here\n. Should contain instructions that this needs to be called in case of user changes the features\n. This error message does not help anyone.\nBUT I just realised the cast will always work as CEuclideandistance is type safe\nYou can make it an assert, or just remove it\n. can you format this properly?\n. The filename is not conform to the way we do it in Shogun.\n. Can you use BSD?\nSee here\nPut your name. Check other files for how to do it\n. This is neither a list, nor do we use CamelCases names.\nJust call it classes\n. weights_set is not a good name either\n. Make this 3 separate checks\n. this will likely cause memory errors.\nJust unref clustering (which takes care of the rest)\nRun  with valgrind to check for leaks\n. OR, just use some and auto. \n. why two files?\n. can you rename the file to gaussian_naive_bayes.rst?\n. @curiousguy13 we have to discuss this a bit. Will get back to you\n. Capital S in \"squared\"\n. no need for this second comment, also we dont really do comments in same line\n. this comment can go, the code speaks for itself\n. I suggest you keep the guards for now. But please get rid of this NATIVE thing, we will drop that anyways and eigen3 should be the default. \n. HAVE_LINALG was not enabled previously, which was an error\n. HAVE_LINALG is the way to do it\n. needs to be guarded\n. guard, also below\n. @vigsterkr whats your suggestion here?\nIgnore linalg, make a double implementation, or make this class unavailable for non c++11 ?\n. just guard the whole algorithm with HAVE_LINALG globally\n. that does the job for data\n. yes, that is a bug in doxygen. For now we cannot link against CDenseFeatures. But that is easy to fix via grep later\n. either use \\cdot or eve better w^\\top x_i (also below)\n. never reference function names in examples. Let the code speak for itself. Also function names can change, and we dont want to the rst file to be invariant to such name changes.\nOn the other hand, the LeastSquaresRegression should be a sgclass reference\n. I added a point on that here\n. Please don't compute the error in this cookbook page. Stay local -- this example is about using the regressor, not about evaluating it. Evaluation will be a different page.\n. check the KNN example. I just produce the test labels there\nAlso, using appply here gives you CLabels which might hide some of the CRegressionLabels methods. Use apply_regression. I added a note on this in the readme\n. Maybe you want to mention a potential bias term here?\n. Definitely mention the possibility to set a bias term here.\n. See #3044 which added this possibility\n. please dont do booleans, but just call EXPECT_FALSE a few times.\nBTW there is EXPECT_FLOAT_EQ ...\nSee https://github.com/google/googletest/blob/master/googletest/docs/AdvancedGuide.md\n. This is a unit test for KMeans, and not for random initialisation. The file should be named accordingly\n. 80 character margin wants to be respected :)\n. this method is private.\nYou can either test it like this\nor you test it through the public interface, i.e. you make the KMeans to run the random center, and then check the centers, extracted via the public interface\nThe second way is much preferred\n. btw here you used ^T, which is not consistent with the above. Also in LaTeX, this is ^\\top\n. Might be good to unit test this. We forgot about it initially ;)\n. dont really like the new name, since it has a typo, but that might be just me being OCD ;)\n. thanks for the correction!\n. see the updates I made to knn.rst, and also to the Readme. They are minor, but could you please mimic them?\nNot printing here. But rather say what comes out of the precition method\n. there is no such constructor in the class, that is why is doesnt compile\n. can you use the SGVector version of get_feature vector here?\n. if you start touching things, we dont use Capital variable names...Also please give this a name that documents itself, i.e. \"class\"\n. matrix has () operator, which you should use. Why += ?\n. can you make this a require, and also provide proper error messages\nhttps://github.com/shogun-toolbox/shogun/wiki/Assertions\n. please rename to something else, this is not a list and \"cl\" should be \"class\"\n. this is not a good idea.\nSorry I know we had a lengthy discussion, but I did not realise this.\nThe KMeans class should not use any specific features of the distance class specialization. Rather but a virtual method \"precompute\" in the base class\n. completely useless comment ;)\nplease remove\n. can you make this a for loop and break out of it??\n. Can you make this message understandable?\n. see above\n. this might be memory costly. Is it necessary?\n. we have CMath::max and min\n. unit tests help for this. Ill write more in a bit\n. Can you remove the print?\n. can you print 10 times during the max iterations?\nI.e. if max iterations is 100, you should print every 10\nIf it is 1000, you should print every 100\n. please dont do probabilistic unit tests\n. Unit tests are a quite different idea. These here are too big and complicated, if they fail, they dont tell more of what has failed than an integration test that fails due to a changed result\nYou should test:\n- corner cases\n- on very small datasets\n- Random initialisation: just test that every center is exactly on one data point, and no two centers are the same. How many points and k do you need to test that? Try two cases: k=2, n=3   and k=3, n=3\n- test modules or the code, not chains of multiple modules\n- Dont use random data, unless it leads to guaranteed pass\n. There is another unit test for the random centers, maybe coordinate yourself with that guy, he already put in some work as well\n. sorry you got me wrong here. I meant, rename the example to be one for CLinearRidgeRegression (including the math, which is almost the same), and then mention that LeastSquares is a special case -- rather than the other way around. Get what I mean?\n. this variable should not be called accuracy, but mse.\nAnd your mse should be called evaluation\n. please mention the type of labels the apply_regression returns. See the knn.sg \n. this can be removed, just say the squared empirical error. Keep in mind that the API examples are...well API examples. Math should be explained elsewhere\n. No need to say that we can differentiate and solve. Just state the form of the solution and say: One can show that the solution can be written as ...\n. We can evaluate the :sgclass:CMeanSquaredError\ndont mention accuracy, it is not accuracy here. Also try not to reference variable names in the text\n. This should go, see my earlier comment\n. For the special case when :math:\\tau = 0, a wrapper class :sgclass:CLeastSquaresRegression is available.\n. just mention that you pass \\tau. See knn http://buildbot.shogun-toolbox.org/cookbook_pr/cbdbb335010353edc98a37ebc2713fc0bdf00762/examples/classifier/knn.html\n. Optionally the bias can be disabled to avoid redundant computation.\n(shorter, cleaner)\n. remove \"also\"\nFinally, we can   (coma)\n. newline in between the wiki links please\n. Also, you can add a snippet that shows how to set this bias by hand\nImagine, we know the bias term. We can set it as\n[code] ...\n. E(w) is not defined and should not be used. Just say \"minimizes the squared loss plus a :math:L_2 regularisation term\"\n(also remove: by finding appropriate w)\n. and \\tau>0 scales the regularization term\n. you can add your name here :)\n. no newline\n. I think latex doesnt have \\mean....use \\frac{1}{N^2}\\sum_{i,j}, or \\frac{1}{N(N-1)}\\sum_{i,j} if diagonal is avoided\n. I guess you should also check the columns here?\n. latex\n. I like it that you re-use the sum module here :)\n. we dont do such type of comments usually\n. sorry it is row-wise I did not realise\n. Can you state that you checked the number of columns here? Also above\n\"Number of matrix columns (%d) must not be 0\" % m.num_cols\n. whitespace error\n. can you remove such outcommented code?\n. whitespace\n. whitespace\n. latex. Make sure to search for it in the file :)\n. do we really need to totally re-implement this?\nI guess so.\n@lambday  ?\n. no exploitation of symmetry here?\n. Please in error messages always give full information.\nMatrix has size %dx%d, which is not square\nSee the dev guidelines on this here:\nhttps://github.com/shogun-toolbox/shogun/wiki/Assertions\n. name :)\n. Distance feature type (%d) should be REAL (%d)\n. Capital L\n. Capital L.\n. this method has to get a different name.\nI suggest precompute_lhs ... imagine other distances might not be based on lhs norms but something else\n. Capital K\n. ....without having converged. Terminating.\n. ok, thanks for the clarification\n. precompute_rhs\n. openMP please :)\n. (but in a next patch)\n. No I mean min rather than this if block\nmin_dist=CMath::min(min_dist, dist)\n. we use 4space tabs\n. reset_precompute might be a better name\n. Dont say this that it is only implemented in euclidean distance\n. Dont say not implemented.\nSay this is an empty method that might be overloaded in a sub-class\n. same here. And the name change\n. Can you send a seperate patch for adding this method to the base class?\n. Then I can merge it first and this patch here gets smaller. It is already quite large\n. can be removed\n. minor: in c++ you can cast.\nThis might return a NULL pointer if you do it this way. I guess our static code analysis might complain here.\nSo better put in an ASSERT_NE\n. this unit test is much better :)\n. cast and null check\n. we dont continue to provide the NATIVE interface anymore. No need to add it. Eigen3 is the default now\n. This needs a bit more documentation.\nPut the warnings you have in Euclidean distance here as well. Also mention this method in the class description on top.\n. Good point :) I agree\n. It is not 100% possible -- some algorithms change the feature matrix. We had a long discussion here. If you have a solution, share it!\nThough we should definitely add this calls whenever we know that the features are changed \n@Saurabh7 can you take care?\n. We dont know when feature matrix is changed....\nFor the cases we know, we can (and should) just call reset.\nBut you are right it is not yet optimal. I leave it to @Saurabh7 to come up with a better solution \n. Kernel ridge regression is a non-parametric form of ridge regression. The aim is to learn a function in the space induced by the respective kernel $k$ by minimizing a squared loss with a squared norm regularization term\n. remove the linear part completely\n. use \\mapsto rather than \\rightarrow\n. \"from input space into a feature space \\cal F is used\n. please put \\langle ... \\rangle_\\mathcal{F} around the Phi product\n. Say that the learned function can then be evaluated as $f(x)=\\sum_{i=1}^Nk(x,x_i)$\n. KRR doesnt have a bias term\n. Remove this, KRR should not even offer this method\n. Actually, please remove Phi here completely. We dont need it in an API example, it doesnt matter for that\n. the alpha_i are missing here .. sorry I forgot above\n. The solution can be written in closed form as\n. I think this should go...we rather should fix #3088 \n@vigsterkr agree?\n. oh this is a patch I recently sent. \nYou have to\ngit pull --rebase upstream develop\nto avoid this showing up\n. typo: cannot\n. on a GPUVector\n. If it is not supported, then why put the implementation ?\n@lambday what is the motivation for this?\n. ehm, please never change file permissions in pull requests.... dont know what happened here\n. This should be\nHAVE_LINALG_LIB instead\n. these can go\n. this is not a good unit test.\nRather have values 0,1,2,....,9 in the vector and check the sum.\nThis can pass for all sorts of weird bugs (just taking the mean over a subset for example)\n. same here\n. no need tu put empty tests\n. this has to be removed\n. Can you remove \"classification\" here?\n. can you use \\exp rather than e^{} ... looks cleaner in the rendering\n. you can maybe put an argmax here and a c in front?\n. yep :)\n. @yorkerlin ???\n. this should stay\n. this should stay\n. should stay\n. should stay\n. should stay\n. should stay\n. La Rank did not converge after the maximum number of %d iterations\n. definitely\n. Also, something more well informed. How many iterations does this usually take until convergence? What does the literature say here?\n. the best thing is to do a first patch with the bugfix, and a second with the whitespace changes\n. Currently it is not clear what actually changed\n. Ah I see, sorry I did not spot that. You are totally correct :)\n. this should not be there, HAVE_LINALG_LIB is only 1 if c++11 is enabled\n. whitespace\n. can you use the standard shogun for loop formatting?\n. more whitespace problems here\n. the harmonic series is a good idea, but for a unit test, simpler is better. So why not just do a[i]=a[i-1]+1\n. please put this into a seperate unit test, so that each test is as small as possible\n. see comments above.\nAlso there are whitespace issues\n. max_iteration < 0 ???\nThis should not be there, if a user wants to run it long, just give a large number\n. We try to not do such implicit limits in Shogun.\n. No need for stating the method here. Just say \"LaRank did not converge after the maximum number of %d iterations\\n\"\nIt is pretty clear that increasing this number will then help\n. you forgot () for the exp\n\\exp \\left( inside-the-exp \\right)\n. Please always give full information\n\"Max iteration (%d) must be positive\"\n. The class docs need to mention this. What is a good way to select?\n. this comment is useless -- it is just the variable name :)\nI know it is quite self explaining here, but something like\n\"Maximum number of iterations before training is stopped\"\nis much more polished if users/developers read the doxygen docs\n. Just \"Default constructor\" is fine here\n. no need for that\n. newline before this.\nAlso, please start sentences with Captial letters -- proper English\n. This should be in the class description maybe?\n. we usually put all implementations into the .cpp file rather than in header\n. no {} for single line if statements.\n. I think this check should not be necessary. In fact, the algorithm (or the base class have to check) should make sure that the kernel is initialised with the training data on both sides.....\n. why a copy?\n. these checks should be done in the constructor/setter/ or even better in the base class. This method should assume that all is set up (you should put these assumptions in the doxygen docs of it)\n. can you do this via a loop on the diagonal?\nAllocating more memory is not necessary here.\n. dont mention eigen3 here.\nJust say it is done via Cholesky\n. btw can you change the interface a bit.\nThis method should be protected and just return the alphas.\nThen the train method itself should prepare everything, call the method, and then set the returned alphas.\nThis way, we can change the solver only without having to re-write all the checks etc\n. no need for that, just store the linear solve result in m_alphas straight away\n. could you make a seperate unit test for every case so that they are really minimal.\nAlso, add another test for when start!=0\nNegative values unit tests would also be good\n. These infos should be in the brief class description ....\n. Initialise in base class once the training features are passed\n. What about using a setter for this, that is also called from the constructors ...The setter might also initialise the alphas\nIF you want them here, take all checks out of the matrix inversion method at least\n. btw we are also using TABS, not spaces.\n. take alpha vector as input\n. minor: we usually use tabs in Shogun, not spaces. Not too severe, but definitely should be udpated at some point\n. Minor: always give tests (and all functions and variables in fact) names that explain what they do. Avoid numerical suffixes\n. what about int64?\n. Why a new method for this?\nThis can be just done in the base class CKernelMachine and in the setter for features of it\n. If the alphas are already prepared, this method can just fill them and doesnt need to taken them as an argument\n. BTW and for future comments: the vector here it is not initialized, but pre-allocated\n. I thought about this again.\nJust allocate m_alphas in the train method before you call the solve method. The solve method assumes that memory is already allocated .... done\n. I see\n. this is not really needed, you can just have another eigen3 map\n. eigen_alphas = llt.solve(eigen_y), no need to copy above.\nWe want to de-spagetthify the code here ;)\n. good!\nmaybe comment \"allocate alpha vector\"\n. Assumes that m_alphas is already allocated\n. you are still using spaces.....not that much of a big deal, but we really are using tabs. Keeping this style keeps the code clean\n. update this please\n. Minor: we dont use {} if there is only one statement in loop or if-statement\n. Is still says addition\n. Missing \"The\"\n. in terms of math, please only state the loss function that is minimized\n. Avoid Lagrange multipliers. Keep in mind this is not to explain the algorithm or math, but just show how to use the API in Shogun. You can say \"The solution takes the form ....\"\n. You could also write the minimization problem. But we only want one major equation\n. No need to be this lengthy. Just say that the alphas are sparse and thats it\n. C is coming from the loss function, so you can refer to it in math mode. :math:Cepsilon is the residual convergence parameter of the solver, state that.\nAlso state that there are other solver types, referring to the class docs of CLibLinear\n. L2R_L2LOSS_SVC stands for something, state that here as well\n. The second sentence should be covered above\n. rather than extracting b, one would extact the alphas here.\nIs the b not always set to zero for svm?\n. we should cite the Libliear paper here and reference the website\n. Mention that this is Liblinear specific\n. Interesting.....We definitely want the alphas for liblinear -- it is an svm after all. Where are they stored? _are_ they stored somewhere? Or just w?\nCan you investigate and open an issue if we cannot extract them?\nBut fine for the cookbook here\n. remove publisher\n. Can you abbreviate first names. See the bibtex entry I put in initially\n. usually pages are separated with double dash, --\n. this is good\n. The csharp error comes from the fact thatenum LIBLINEAR_SOLVER_TYPE.L2R_L2LOSS_SVCis translated toL2R_L2LOSS_SVC. If this was changed toLIBLINEAR_SOLVER_TYPE.L2R_L2LOSS_SVC, then csharp compiles. Any ideas how to fix this @sorig ?\n. you need to say what the \\xi is, i.e. hinge loss\n. where\n. dont explicitly mention the L2R_L2LOSS_SVC here but just say \"We here set the solver type to L2 regularized classification. There are many other solver types in :sgclass:CLibLinearto choose from.\"\n. Can you make this a link and do something like\n[LibLinear website](http://www.csie.ntu.edu.tw/~cjlin/liblinear/)\n. Please abbreviate first names\n. please use the same style as the other entries\n. no isbn please\nonly the things that appear on the website\n. useapply_regressionas otherwise the strongly typed interfaces get a type error\n. For gp, the mariginal likelihood should also be evaluated\n. @yorkerlin do we have a simple 1-call method to optimize all hyperparameters using ML2?\nIf not, @Ialong you could add that. Definintely, the meta example should explain how to learn hyperparameters using ml2\n. We only allow for 1 subtopic in fact. (CMake details)\n. Really like the cookbook page!\nI think we should shorten it slightly. See below\n. I would remove this equation\n. I think this should be p(y_i|f_i)\n. Maybe even mention Chapter 2 (I think regression is chp 2?)\n. I would put the prediction integral that you have below up here.\n\"Given training data, the label for a new point can be predicted as....\"\n. Please dont explicitly mention the chosen parameters in the text. If they change for some reason, the text gets outdated. The code speaks for itself -- just mention the parameters are set\n. remove concrete values, just mention the parameters (same argument as above)\n. \"However, we can optimize\" -> \"We can also learn these parameters via optimizing the marginal likelihood\" \n. remove model evidence, keep in mind this is an API example, not a statistical intro\n. nle_out? what does nle stand for?\n. not needed, all Real variables are stored by default\n. @yorkerlin it would be good if this would be a one-liner.gp_regression.optimize()c.f. GPy etc\n. Since you menation the C matrix here, maybe add code below to access it from the trained GP?\n. Then we can train the model and evaluate the predictive distribution. We get predicted :sgclass:CRegressionLabels. @yorkerlin I am mostly thinking about a simple convenience method that does all the above calls, and hides them behind a singleoptimize` call, which always optimizes all variables via ML2. I think this should already be done in order to minimize obscure code.\n. what in particular?\nThis should be 1/N^2 \\sum_{i,j=1}^N m_{i,j} (if mean is included)\n. Ah yes, C is not interesting. Nevermind my comment :)\n. These guys can be deleted. They are useless anyways :)\n. the whole file should go :)\n. the comment should be changed to \"to have less noise\"\n. I think these guys should be checked via REQUIRE before, but then the error codes can basically be ignored right?\n. btw we should always print the value of the given numbers as in the REQUIRE elsehwere\n. can you remove such temp comments in PRs? Can always re-add later\n. REQUIRE please.\nSee https://github.com/shogun-toolbox/shogun/wiki/Assertions\n. This can be done using SIMD in eigen3. No loop please\n. REQUIRE and proper message please\n. we put { in new line\n. please avoid such comments (but just send a patch :) )\nBUT as sbove, can you please do this with eigen without writing loops\n. please remove any commented out code in PRs\n. why not just 2x2? makes an easier to read unit test\n. I am pretty sure \\mean is not a latex macro\n. minor: we dont use { } for single statement for or if\n. latex\n. btw it would be nicer if one could \"read off\" the matrix in such unit tests -- so that I can directly check what is going on. This loop is hard to decipher.\n. yeah eigen3 simd would be better here. is that not possible?\n. Can you use the CMake function VERSION_LESS\nSee e.g. examples/meta/cpp/CMakeLists.txt\nIt is a bit cleaner\n. What about using latex for the doxygen?\n. I would vote for default: lower\n. This should be an error I think\n. I don't get the comment.\n. new improved is a bit vague... can you be concrete like \"fixed bug in KMeans++\" or \"Improved efficiency of KMeans++\"\nIt is also not a new feature but an efficiency improvement, there is a separate category for that\n. These two should go together\n. BTW can you please send a separate patch for the readme update?\n. what happens if linalg is not available?\n. yeah maybe just remove it. \nIt is by definition that Cholesky only works on sym psd matrices\n. Proper error message. State why it cannot do that, what is given and what is expected\n. Why put the warning in the method implicitly?\nI would rather REQUIRE(m_obj->m_fully_sparse,\"Can not compute gradient, the provided kernel (%s) does not support to optimize inducing features\\n\", kernel->get_name()); to be fully explicit\n. I think this one slipped in by accident. Has to be removed\n. this should not be in the code file. It should only be put there when the script is called from the cmake test\n. this is basically crap ;) so definitely remove it\n. does the base class need to be exposed via swig?\n. This is very good!\n. For Nystrom, the point is not to compute the full kernel matrix, but only the subset you subsample below. Otherwise there is no gain in memory usage\n. Choosing the indices for the random sub-matrix should come from a separate method. Then we can overload it and use different ways to do this: Incomplete Cholesky, uniform (as you do here), and leverage scores. All with a simple one-liner in a new class\n. Is computing the Eigendecomposition really the way to go?\nCheck eq5 of this paper: http://arxiv.org/pdf/1507.04717v6.pdf\nThis is how I would do it ....\n. If you do it this way, the alphas you get will be m-dimensional, which makes predictions cheaper\n. rather than writing examples, I think we should start with unit tests\n. this is good for testing internally though.\nYou could also do a meta example instead ...\n. Sorry, I just realised you are exactly doing the Moore Pensrose pseudo-inverse. It is a pitty that eigen3 doesnt have that in natively\n. You need to tell Shogun what the \"support vectors\" of this solution are then, the alpha vector will be sparse. It is a bit like in SVM, where the alpha vector is sparse as well. This needs to be carefully unit tested in order to make sure it really produces the correct results\n. we use the BSD license these days, pick from any existing file\n. this comment is useless, we try to avoid useless source code comments in shogun :)\n. we have range_fill\n. this should not be added on the fly I guess, but on the mm matrix see again http://arxiv.org/pdf/1507.04717v6.pdf\n. not a useful comment :)\n. Yes, so  the comment can go :)\n. bsd please\n. For later iterations: please make this doxygen comformant, so that class docs can be generated automatically, see http://www.shogun-toolbox.org/doc/en/3.0.0/classshogun_1_1CKernelRidgeRegression.html\n. Please add a description with the basic math in the class description.\nSee http://www.shogun-toolbox.org/doc/en/3.0.0/classshogun_1_1CKernelRidgeRegression.html\n. I think it should be CKRRNystrom\n. Sorry this one renders correctly:\nhttp://www.shogun-toolbox.org/doc/en/latest/classshogun_1_1CKernelRidgeRegression.html\n. See also http://eigen.tuxfamily.org/index.php?title=FAQ#Is_there_a_method_to_compute_the_.28Moore-Penrose.29_pseudo_inverse_.3F\nBTW you need to set a reasonable precision here as well, see https://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_pseudoinverse#Singular_value_decomposition_.28SVD.29\n. BTW for the meta example, have a look here for inspiration\nhttp://shogun.ml/cookbook/latest/examples/regression/kernel_ridge_regression.html\nThe two files from which is generated are here:\nhttps://github.com/shogun-toolbox/shogun/blob/develop/doc/cookbook/source/examples/regression/kernel_ridge_regression.rst\nhttps://github.com/shogun-toolbox/shogun/blob/develop/examples/meta/src/regression/kernel_ridge_regression.sg\n. And the test file for integration testing is here:\nhttps://github.com/shogun-toolbox/shogun-data/blob/master/testsuite/meta/regression/kernel_ridge_regression.dat\nThis is a separate repo and has to be sent in a separate patch.\nSee the Readme of the meta examples\n. I think this one here samples with replacement.\nWhy not just create a range_fill [0, ...., n-1] and then permute?\n. maybe a better name is subsample_indices?\n. For training, the lhs is equal to rhs\n. Can you use REQUIRE here and write a proper error message?\nhttps://github.com/shogun-toolbox/shogun/wiki/Assertions\n. You can index matrices like K_mm[row, col] with a double nested loop here.\nBut actually it would be better if you set a column subset on the features, and then just ask the kernel to give you the K_nm kernel matrix (which will be of the correct size, and computed using multiple cores). Then you simpy copy the values to K_mm rather than re-computing it.\n. I think you should set a tolerance level here, have a look at the wikipedia entry where they describe how it is say done in numpys pinv\n. this is fine, no need to split\n. Maybe state the Big-O complexity here?\n. Maybe state that there are multiple ways to select columns. And that overloading the \"subsample_indices\" method can alter that\n. And that we are uniformly sub-sampling in this class\n. This shoud be inherited from the base class no?\n. this is not classification and it is not pure ridge regression. You can add a new type if you want here. Or just remove this method as we can also just use dynamic_cast\n. This should be protected as well: sub-classes might want to read it\n. This method needs to be virtual btw\n. Ah there we go :) Nevermind the above comment\n. yep! See also above comment\n. Minors: Capital K please. Also, can you avoid the math in the header (only there)\n. please use \\argmin, which is already defined\n. no () around the <=\n. typo: please run a spell checker before you send cookbook pages\n. Try to be a bit more concise (that is: less text) here. Just stating the objective should be fine\n. This needs an illustration of\n- apply method (I guess cant be done right now as discussed in the thread)\n- evaluation (based on above)\n- How to run the mini batch version\n. can you do\nSGVector<index_t> temp(n);\ntemp.range_fill();\nCMath::permute(temp);\nSGVector<index_t> col(m_m);\nfor (auto i=0; i<m_m;i++)\n  col[i]=temp[i];\n. minor, we dont do {} for single lines\n. Can you actually use the kernel->get_kernel_matrix() interface for this? It is all parallelised etc.\nYou can set a subset to the features using your col variable before\n. We should have a method pinv in CMath....\nI wrote the same in a feature branch and it is not good to have duplicate code.\nI will take care of this later, no action required here for now\n. please always allocate Shogun objects on the heap, i.e. use new and SG_REF/SG_UNREF\nAlternatively, you can use some\nauto kernel = some<CGaussianKernel>();\nFor that, you need to include it and guard the code with #ifdef HAVE_CXX11\n. you can also use C++11 and some here if you want, allows for easier reference managment\n. this should be 1e-5 at least in my experience if all of the matrix is used (only difference should be the numerics of using the SVD to invert the kernel matrix)\n. minor { } is always in a new line, when there is only one line, we dont use it\n. name the test so that it states that it compares using all columns\nThen, there should be another test that uses only say half of the columns (with a lower accuracy)\n. not needed if you use some\n. BTW sorry for only bringing this up now. But maybe the name \"m_num_rkhs_basis\" is better than m. Minor though\n. Can remove the \"CParameterCombination::obtain_from_generic(): \".\nWe add that automatically now\n. \\argmin is not part of standard latex, but as mentioned in the readme, we have custom macros defined here that you can use and extend\n. this should just be swig/doxygen ignored\n. yep  // SWIG should skip this part\n. Typo: Built-in\n. Mention that this implementation uses the EM algorithm which is executed until some convergence criterion (which) is met\n. Need to mention / reference what SMEM is\n. Can you maybe use a multiclass dataset here?\n. I wouldnt use \\boldsymbol   but it is fine if you want that\n. I wouldnt use \\boldsymbol   but it is fine if you want that\n. Never put plain text in math mode.....\n:math i^{th}\nThe \"th\" can go after the math\n. Never put plain text in math mode.....\n:math i^{th}\nThe \"th\" can go after the math\n. \":math:\\cal{N} denotes normal distribution\"\nput N(\\mu,\\Sigma) denote a multivariate normal .....\n. In the code, you extract all \\phis, not just phi_i\n. no need for that, all Real* variables are stored anyways\n. this really should be in a separate patch btw\n. I am not sure I understand the logic of this preprocessor if then else.\nDon't we just want to hide things from swig? Why the else case?\n. Yes you can avoid the SG_REFs in that case\n. Check the method signature:\nSGMatrix<int32_t> get_cluster_pairs();\nThis make sthe csharp fail on travis (strongly typed)\n. Should this be in \"clustering\" or in \"classifier\"?\n. We apply a \"bottom up\" approach: each observation starts in its own clister, and pairs of clusters are subsequently merged.\nCleaner language\n. watch your English :)\nUse something as \"We start by constructing a pairwise distance matrix. Then, the clusters of the pair with closest distance are merged iteratively.\"\n. use sgclass for CDistance.\nNever mention method names or variable names in the text please, let the code speak\n. Remove \"And\"\n\"while\" -> well\n. if you put this example in \"classifier\" please also show how to extract predicted labels\n. remove \"in machine learning and statistical classification\"\n. remove \"or events\"\n. remove \", so y will be decided based on:\" and replace be \"i.e.\"\n. again, please dont explicitly mention variable names or methods in the text. Just say that there are parameters, and what they are for\n. Remove \"then\"\n. Can you put the david barber citation for the QDA chapter her?\nAlso I think there is a wiki entry on QDA\nYou can also relate to LDA\n. You can say that it is a generalisation of LDA\n. dude, it is this line\nhas to be \"int\"\n. yeah put them under clustering then, there are no labels given ....\n. yes it is used in the other pages, e.g. knn.\nThere is a free online version, just reference the corresponding chapter like I did in KNN\n. I think you want to parallelise over both folds and runs\n. OK\nI suggest for now: CSGObject::clone, which gives a deep copy.\nTry that and see how bad it is ;)\n. We copy the feature matrix this way, but that is OK for problems that are not too big. Let's see whether this works.\nWe can then start optimising further\n. don't mention dense features here, but only below. This is to explain what the algo does\n. LDA attempts to find a projection matrix that maximally discriminates the provided classes.\n. This is no sentence.\n. Please only mention algorithm basics here. Implementation details such as how the thing is computed should go below\n. Please dont use varialbe names, method names, or enum names in the text, but just let the code speak for itself. In the text you can just say that and SVD can be used .. see what I mean? We want to avoid the examples becoming outdated when code changes\n. By default, Shogun automatically chooses the decomposition method based on  :math:{N<D}\n. LDA learns a classifier via finding a projection matrix.\nThe classifier doesn't attempt to find a matrix\n. you should define the objects in this equation, otherwise they are not useful\n. Remove, and rather say: The project matrix is computed as\n. Remove \"Singular value decomposition\".\n. This condition, however, is violated when there ....\n(Never start a sentence with however)\n. In this case, Shogun computes the SVD as\n. again, define the terms used\n. N<=D  or N>D\n. \"that assumes that data are generated from a finite mixutre of Gaussians with unknown parameters\" is better\n. The model likelihood can be written as\n. This should be p(x|theta) (and you should define \\theta below\n. for the Gaussian N, please write it as N(x | \\mu_i, \\Sigma_i)\n. can we use \\pi_i for the weights?\n. \"The expectation maximization (EM) algorithm is used to learn parameters of the model, via finding a local maximum of the likelihood\".\n. We start by\n. We initialize GMM, passing the desired number of mixture components\n. All your sentences are missing the subject ;)\n\"We provide training features\"\n. Please mention the split merge version at the end, the first part should just be standard EM\n. How can it be used for classification? \nEither show or remove\n. separate patch/branch\n. you should rebase this branch against develop to not clutter the diff with these things. Makes it hard to read\n. you did not rebase against develop -- this is my patch\n. what does this flag do?\nIn general, use variable names that are self explaining :)\n. keep in mind that cloning the machine above also will clone the features (including the matrix), if the features are set.\nI would make use of that and not separately clone the features here.\nyou could do\nm_machine->set_features(features);\nCMachine* machine_clone = m_machine::clone();\nCFeatures* features_clone = machine_clode->get_features()\nfeatures_clone->add_subset( ...)\nI get it that you want to be efficient here, but let's postpone that for a bit and get the other things right first.\n. same as with features, use the cloned machine\n. keep in mind that your x-validation output currently is shared across the threads.\nThat is fine actually (updating it is fast), but you might want to do the update of it atomic so that the different threads dont conflict here\n. I think this should be done on the cloned features\n. Actually, I think this is wrong anyways --- for now remove all the preprocessors from the xvalidation code, we can do that later. I guess I did this wrong back when I wrote this ;)\n. is this call thread safe? Should be, but we better make sure it is\n. let's avoid this one for now (save it for later)\n. this should be covered by \"clone\"\n. Number of subsampled columns (%d) must be less than number of data (%d).\nI.e. give the user the information what exactly is wrong\n. Either make sure this cast is valid (dynamic cast, check for NULL), or put a comment that this is ensured somewhere else\n. #pragma omp parallel for\nmakes this run on multiple cores\n. exploit symmetry here, for index_t j=0; j<=i; j++) and then set both K_mm(i,j) and K_mm(j,i)\n. if you flip the loop (i.e. you traverse the matrix column-wise rather than row-wise), it will be more CPU-cache friendly and therefore significantly faster\n. great description :)\n. Maybe also put the keyword here, so that I wont moan like 10 more times because I forgot you changed the base class ;)\n. Usually, our naming scheme is a bit different, see the other files.\nMinor ...\n. if you really want to test those, you should call the getter afterwards and ensure it returns the same you set\n. use apply_regression which will directly give you regression labels and you can avoid the cast\n. mixture model (Only names are capital in English, like \"Gaussian\")\n. theta includes the weights \\pi_i as well\n. Can you use \\mathcal{N} for the Gaussian (maybe misunderstood me earlier, I just wanted the x in there)\n. Sorry, my mistake, it should be\n\"... via finding a local maximum of a lower bound on the likelihood\"\n. minor: space after \"G.E.\" missing (surname is not separated)\n. Minor: Technically, this is not safe. Think of possible overflows.\nYou could rather cast the vlen to unsigned int and assert for positivity first.\n. yes, that is it! :)\n. Ah, not 100%.\nThis hash points to a PR by somebody else that is more recent than yours.\nYour hash was a different one. You can either revert the data repo to your hash and then update the subproject again, or manually put your hash in.\nIt doesnt really matter here as we will merge all the data and cookbooks anyways, but you need to make sure that you are in controll of this\n. This one doesn't work -- see output of travis\nEDIT: data revision needs to be under the \"file\" called \"data\" and not \"shogun-data\".\nNo idea how you  managed to change that, but it is causing all the build error.\nCan you change it to \"data\" and to the correct hash of your merged data patch?\n. Does cause a build error?\nThe SWIG #ifdefs are not really used in compilation mode, but only for the swig wrapper (that should not need this type anyways)\n. For later:\nUse BSD license and your own name\nSee https://gist.github.com/karlnapf/24763634d5142f92f5213d282cf67e0f\n. OK, I just merged #3219 which includes your hierarchical data.\nThis means you can remove the data version at all, and we can merge.\nHowever, make sure to understand this to have an easier life for the next cookbook ;)\n. I see. The problem with cloning the features by hand is that if they had already been assigned to the machine in some way (via kernel e.g.), then they will already have been cloned. But OK, let's postpone that as well.\nCan you add all these things to a TODO list so that we wont forget?\n. Definitely no shallow copy here for now\n. Another good reference here is the book \"Learning with kernels\" By Sch\u00f6lkopf and Smola\n. please use the same bibtex style as the other entires, i.e. abbreviate first anems\n. no need to initialise the kernel here, svr does that\n. This is OK.\nI would add the objective function that is minimized. You can find it in the \"learning with kernels book\"\n. I think the x_i should be \\bf{x}_i i.e. the i should not be bold\n. Also please define the quantities in this expression\nx_i - training data\nx - new data point\nN - number of training data\nk - kernel function\n. \"The ID3 algorithm is a decision tree based algorithm for multi-class classification. It begins with the original ....\"\n. \"till\" -> unitl\n. Not sure we need the math S here, you never really use it. So just talk about the training set rather than using math S\n. I think this is (or should be) defined in CMath ... it is not portable this way. Might be a reason for an error\n. btw can you rename all the files to their long name?\nSorry, but this is something that just came up in chat. Includes the data files (unfortunately)\n. Too much detail. The cookbook examples should only cover basics (in particular for kernels) i.e. only one equation or so. Preferably only use math symbols for things that are later used in the code.\n\"..... This kernel is an inner product in the feature space generated by all sub-sequences of length n. The sub-sequences are weighted by the exponentially decaying factor \u03bb. More weight is given to those occurrences that are nearly contiguous. The implementation uses an efficient dynamic programming strategy.\"\n. ehm, this is empty. Did you notice?\n. if this is called maxlen, this kind of is contrasting the math that says subsequences of exactly n\n. minor: just name this create_kernel, why have \"appropriate\" in the tag\n. labels_test doesnt seem to be defined anywhere\n. Dont see it\nI mean this\nREQUIRE(m_num_rkhs_basis <= n, \"Number of sampled columns (%d) cannot be larger than number of data (%d).\\n\", m_num_rkhs_basis, n);\n. Decay (capital D)\n. hey what about this issue with things not being CSGOBject?\nShall we revert now that plugins are coming?\n. Let us focus on the non-locked case for now.\n. lets only parallelise the folds for now. This comes back later\n. wouldnt this also be applied if not threads are active?\n. why SG_REF here and not below?\n. I suggest you only unref if num threads >1\n. Then you can avoid SG_REFing the m_machine (which doesnt really make sense)\nAnd then instead, you can ref the output of the clones\n. can you put this into an external function of the unit test?\nThere is a setup mechanism for that\n. no confidence intervals, that is copy paste waste\n. cool!\n. usually, we try to avoid whitespace changes in such patches, and would send a separate patch\n. make it 3 runs to save time\n. make 3 folds to save time\n. why is this 1 by default?\n. why not num_cpus?\n. UPDATE sorry I got it now\n. @Saurabh7 this is the call you will need\n. minor: newline for {\n. this seems OK\n. no {} for single instructions please\n. You probably want to REF them here\n. and REF\n. no {}\n. wrong tab indent. This automatically REFs, no need to decrease refcounter at the end\n. you are playing with fire here as you did not ref things when they were cloned (only the labels) ;)\n. please no prints in tests\n. can you make this noisy? Most methods behave themselves better for gaussian like input\n. this method could return a std::pair with newly allocated data, so this code gets slimmer\n. use the new cache less constructor #3237 \n. no need to set epsilon here\n. set C to 1. Makes the solvers life easier :)\n. so this is valgrind clean?\nIf so, then I think we can merge it as a first go\n. You do need them. Just because it worked doesnt mean it will always work. Basically, behaviour is undefined if the thing is not refed and you pass it around. Example\nbar=foo->clone(); // ref-counter is 0\nmethod_call(bar) // assume this method increases the counter once at the beginning, and then decreases afterwards -- it becomes 0 and the object is deleted\nbar->method() // crash\n. Cool -- if it is clean. Then please polish the PR and we can go on.\nBefore I merge, Can you put some benchmark numbers?\n. sorry, can you explain why this is necessary after cloning but not without cloning? machine should be exactly the same as m_machine here....or am I getting this wrong?\n. Should really print out both m_num_rkhs_basis and n here. Otherwise users might be confused.\nAlso, the check should be done in the constructor and setter rather than here (this is not a method called by a user, it is better if such errors pop up from explicit user calls)\n. Question:\nCan I set any tag name on any object? No restricutions? \"Training data\" seems a bit too much freedom ;)\n. set to 3 manually here\n. I like this idea, but please use relative links? I.e. remove the http://shogun.ml/cookbook/latest/\n. We \"extend\" ..... \n\"datasets\" (plural)\n. love this, but the \"linear_svm\" name is not the nicest, can you just alias it as \"see the linear SVM cookbook\", where \"linear SVM\" is the link?\n. Just leave these parameters set to default values (this is already covered in the other cookbook), and don't talk about them here\n. pls dont call them map_* but rather eigen_*\nEDIT: actually, it is fine\n. nice -- much easier to read\n. maybe  put a newline here?\n. These are updates of Cholesky factors. There are eigen3 methods for this...\nHow they are embedded within pseudo code, an example is here:\nhttps://github.com/jbowlan/pyl1ls/blob/master/lars.py\nEigen3 methods are here:\nhttps://eigen.tuxfamily.org/dox/classEigen_1_1LLT.html#a67b49e986da81c3c90fe1793681e70ad\nIt would be good if those would be put into linalg (later). But definitely not do this manually .... \n. rankUpdate\n. Logistic regression is a binary classifier that assigns class probabilities as\n. You have to say what these letters mean, otherwise the math is not helpful.\nAlso, if you could state the loss function that is minimised, that would be great\n. I think it is not worth it as the loop is ultra cheap (but maybe investigate!)\n. Yes, and mention this in the class interface docs\n. in that case either add it, or use eigen3 to the rescue\n. It only allows for SWIG interfaces, so everything that you can do from python, you can do from meta examples\n. I assume this works in Python?\nIn that case, it is the grammar, which doesn't allow subsequent method calls.\nCheck if it works if you do this in a sequence of steps:\nDynamicObjectArray all_layers = layers.input(num_feats)\nall_layers = all_layers.linear(50)\nall_layers = all_layers.softmax(2)\nall_layers = all_layers.done()\n(or similar)\n. good!\n. please no URLs in ourbibtex\n. can you use the other datasets that have ground truth available?\n(see other examples)\n. yeah you need method calls here. Does this kind of stuff work from swig? If so, we should change the grammar @sorig\n. this example can definitely be translated to meta examples .... nothing that the grammar doesnt support. Just have to flatten the chained method calls\n. remove \"basically\"\n. whats this false thing?\nWhat is it for?\n. we also want the regularisation path extraction\n. avoid \"is about\", but rather say \"MKL is based on convex combinations of ...\"\n. This is for classification, you should say that \"For example, for a kernel SVM, the model is\"\n. Actually, I would not give this line here, this is already mentioned in the kernel svm cookbook (you could reference that like @OXPHOS did in #3242 ).\nRather the math line should be the k=\\sum_i k_i ..... \n. Are the algorithms in Shogun the ones in this paper?\n. I think this cookbook should solely focus on the MKL part\n- settup up multiple kernels\n- running mkl\n- extracting kernel weights\nAnd refer to the kernel svm cookbook for all other things\n. The point of this is to show that one can combine combined kernels with other ones. This doesnt really shine in this way. Maybe start by saying that one can combine all shogun kernels, and that you will combine a custom, a gaussian, and a poly.\n. why a new kernel for testing?\n. No an error is fine, but it needs a newline at the end\n. newline missing\n. why not swap the loop order here too for cache friendliness?\n. You are doing redundant computation here:\nI suggest\n- first compute K_nm\n- then copy the data into K_mm using memcpy and SGMatrix::get_column_vector\n. Can you maybe mention the other possible strategy here (using sgclass) ?\n. See equation 17 for the actual loss minimised.\nYou can translate that to this problem.\n. there is not feature space here, just the d -dimensional point itself.\nRather than \"map\" use \"assign\"\nI guess we need one p per class.\n. this is the reason why I want get rid of those lapack calls, they totally hide such things.\n. Sure :)\nNo worries if this takes a bit. It is complicated.\nThe math is not quite synchronised. Maybe this one helps? link1 link2\nYou basically need to state two things\n1) How the probability for a data x to be in class y is computed p(y_i=k | x_i)\n2) The above will involve a parameter vector \\theta. You need to state the loss that is minimised over \\theta (not over the data x itself btw). \n3) The loss will be regularised, which has different implications. The slep manual I posted contained the way it is done here.\nCurrently there are some problems.\n- min is over x, which doesnt make sense, it should be over \\theta\n- the p in first line is only for one class\nJust combine all pieces of information and then we will have it running :)\n. It is actually true multiclass. No one-vs-rest strategies\n@lisitsyn \n. for classes maybe use  K for the number of classes and k as index\n. use RealMatrix\n. not sure \"train\" and \"test\" is the best name here. You never train or test with a model.\nWe want to illustrate two things: Evaluating a symmetric distance, and one between different data.\nTherefore, just call them features_a, features_b maybe\n. Can you also show the interface to call the distance() method itself (that returns a float)\n. I know, it is a good point, but for now we want to keep the cookbooks a bit more minimal (they are API illustration after all)\n. ping me once you filled it\n. Part of this task is to clean up mistakes that others did in the past ;)\n(so if you find anything ugly or wrong, just correct)\n. always make sure your meta examples are executable locally before sending a PR with them.\nIf it bothers you to build the modular ones, at least the cpp should compile and run\n. I guess we would call this RealDistance to follow our old pattern of calling everything float64_t real\n. i.e. RealFeatures\n. due to the name clash, I suggest\nRealDenseDistance\n. We also do not want eigen in the interface.\nEven if it is impl class, put a SGVector, etc\nCreating Map is free\n. +1\n. why not create a Map inside and transpose it?\n. remove \"clustering\"\n. can you add david barbers book as reference as well?\n. is to minimize:\n. remove k= at the beginning\n. something is very weird here. Why all these changes in the diff?\nI expected single line changes...ideas?\n. yes, like this is what the notebook diff should look like\n. these all look good\n. please dont use \"this\" pointer\n. mmh.\nUsually, members in shogun have an \"m_\" prefix, which avoid the need for underscores in setter variables....\nBut well, its ok for now\n@lisitsyn \n. getters should be const\n. this x in the beginning should not be there :)\n. maybe mention (or remove in code) this \"fixed_centers\"?\n. the model, not the dataset\n. Dont start sentences with \"And\"\n. btw if you locally run make cookbook then you can look at the rendered html output (See readme).\nIf you do this, you can spot latex errors in this one (it is not rendered properly), as also visible here\n. remove P here\n. use \\log and \\exp latey macros\n. use ^\\top for transposed vectors\n. same here for exp and log\n. please be consistent with boldface math. This a is not bold, whereas the above one is\n. Great, this is much better now. It can stay like that once you cleared up these minor things\n. You have that l_1/l_q from the manual I guess?\nDo they say more about that? If so, also mention it below\n. I guess regularization constant is lambda? Mention it\n. I would put newlines after }, but that is mostly taste\n. \"which is identified\"\n. remove \"contains\"\n. I dont understand this, can you make it a bit clearer\n. remove \"a\"\n. remove contains\n. More user information!\nIf this is something that can be caused by a user, it should have a better error message, stating all infromation provided, and why it doesnt make sense\n. \"is identified\"\n. Make clearer, just like above\n. Wrong grammar.\n\"true if the parameter exists\" should be enough also\n. same as above\n. and here, will not comment on similar issue again, please sort out all of them :)\n. No documentation here?\n. not docs here?\n. this comes from @besser82 s take of the license, he wanted that. I dont care\n. maybe hide from SWIG?\n. Dont mention own classname in brief, just redundant\n. Maybe put an example?\n. I like newlines\n. This code is hard to read, should be unit tested 100% coverage. Can you ensure that?\n. is this change in parameter combination really necessary here?\nThis stuff is only every used in model selection code. I think it should be changed in a second patch\n. Why is this change needed?\n. good tests.\nCan we please have 100% coverage of the new stuff in SGObject?\n. call this one distance_matrix_ab\n. this is the same as above, so both of your distance matrices will be the same.\nActually, why don't we just remove everything after this line, and only compute the distance_ab.\nIt is pretty obvious how to do other combinations.....\n. I don't like it if we would have to put this error message in every implementation.\nMinor: No space after period.\n. not Eigen3.\nI suggest to remove the library from the doc. Also above\n@lambday thoughts?\n. I would remove \"with Eigen3\" here\n. we dont use {} for single line statements\n. Why was there an error message in the sum implementation if there is already one here?\nAlso, can we just call some method that throws the error? We dont want to re-write this all the time.\nFinally, can you please provide a better error message? Like \"Tried to call GPU method %s without registering a GPU backend first. Register as blablabla\"\n. -1 ?\n. This is not an optimal unit test.\nWhy don't you use range_fill, then at least we guard against a few more cases\n. this needs to be guarded, no? UPDATE: I just saw it is, sorry\n. I suggest you keep the GPU backend test in a different file, or at least move registering the backend to a method. To avoid having to register it for every test\n. this guy now lives on CPU memory if no GPU is available?\n. the k guys are not kernel matrices, but just kernels\n. I think we should not mention the linear SVM coobkook here, but a LibSVM (kernel svm) cookbook\n. and {x_i}_i are the training data\n. The combined features are for the case where different kernels have different features.\nYou dont need to do that here, but mention it. Also you combined features only get two copies of the training features here. Is that because one kernel is precomputed?\n. Maybe say \"the SVM coefficients \\alpha\"\n. put (precomputed) before CCustomKernel\n. \"because it was precomputed\"\n. Can you also compute accuracy (and make sure it looks reasonable)\n. I wonder whether we should say one sentence about how the MKL solver works here.\n. make it \"(over potentially different domains)\"\n. Remove \"One could make a combination of kernels like:\"\n. can you avoid abbreviations in the filenames. Use multiple_kernel_learning_binary_classifier.rst\n. please send a separate patch for this. But thanks and well spotted\n. why mention the return type in docs, it will be printed anyways\n. use range_fill please, not constant. Covers many more cases where things can go wrong\n. is this both Python 2 and 3 compatible?\n. \"a data-separating hyperplane in a Hilbert space induced by a positive definite kernel. The hyperplane is chosen to maximize the margins between the two classes.\"\n. I am getting a latex error for the math, so cannot read, please always run and check locally before sending PR\n. uses LibSVM [citation]\n. remove k(x,x_i) (this is not the training sample )\n. Say that epsilon is optional\n. @sanuj check the swig interface file (dont know how it is called, but you need to tell swig that a method returns a new object in the occresponding .i file)\n. please use 0\n. \"we show\"  -- present tense\n. Codebooks\n. For dense codebooks\n. Wrong grammar. Dont start sentence with while\n. There is no keyword, just mention the other class (so that it is also linked)\n. EDIT:\nthe 0 seed in Shogun causes a random seed to be generated. I didnt know that\nSo you have to use 1.\nAlso, can you send a mini PR that adds a comment in the CMath interface documentaion?\n. this should be a parameter\n. this parallel helps?\n. yes\n. you you either use spaces or tab stops (minor)\n. why using vector here?\n. this line has to go\n. you can do this with memset, no loop needed\n. same here, memset with zeros is super fast\n. some of the matrices can be const references\n. please no eigen3 includes in .h files (even if this is impl)\n. this one should go as well\n. Please align\n. please use \\bf{w}^\\top x\n. Say: It does not converge if the data is not linearly separable.\n. please use a different name, this is a python keyword\n. maybe us 100 rather than 1000\n. Shall we maybe put the math for the updates so that we have a math analogue to the learning rate\n. Can you put our updated description?\n\"Unified and efficient Machine Learning\"\nhttps://github.com/shogun-toolbox/shogun/wiki/Project-description\n. +1\n. comment removed?\n. Did you push? Dont see the update here\n. Start with \"The\". And please dont capitalise algorithm names apart from when they are using names, such as \"Gaussian\"\n. The sentence doesnt make sense to from after \"by exploiting\"\n. This is coming from where?\n. This should be explained/ellaborated a bit more on\n. Please remove Classifier in the title (It is listed under classifier)\nRename to relaxed_tree.rst\n. you need to add it to the cpp json, generator/targets/cpp.json in the same style as RealFeatures etc\n. ??\nPlease read the docs (or the code) for what this thing does. It should be apparent.\nAnd then mention it in the cookbook. No need to take min here, you can just decide on any number (that works for this dataset)\n. remove \"also\"\n. \"codebook, and to predict\"\n. usually, these are called \"mock\" or so, but ok\n. publication. Just wanted to know, all good then\n. the booktiitle seems a bit weirdly formatted though :)\n. ?\n. This should have some math and the statement that this is an abstract base class\n. I guess we dont need this....\nTags will make this redundant anyways I think\n. I would do the cache solely via setter\n. would this call even be allowed without a distance set? Does it have a meaning?\n. REQUIRE\n. Maybe not mention the method here (but just THAT distance has to be providced) as these things tend to change.\n. Remove \"Please contact ...\"\n. why compute the full matrix if only triangular is used?\n. not sure about this....any error will stop execution anyways\n. ehm.....this might be a bit long, no? :)\n. we never want eigen3 includes in header files, messes up things quite a bit\n. also, no stdlib includes in header files.\nIf you do this namespace thing here, it will be set for all of shogun, which is not wanted\n. this might segfault as there is nothing passed to %d\n. Should always state what was given by the user, see our wiki on error messages\n. I think there should be a flag degrees of freedom that can be used to change normalisation to 1/n, 1/(n-1) , etc\n. these tests would rather be at the same place as all the other linalg tests\n. yeah just some subset of features. You can just say that you pick some number of data here\n. you are right +1\n. can we remove the setter method name in the message here?\n. \"The distance instance is not initialized, please provide one.\"\n. yep, phew :)\nIs this unit tested?\n. No model selection here pls\n. translation invariant\n. Start like this:\n\"Base class that represents kernel functions that only depend on the difference of its inputs, i.e.\"\n. would rather say. The kernel can be written as\nk(x,y) = k(x-y)\n. TranslationInvariantKernel is a better name\n. googlemock for the rescue! :)\n. any reason to mix some with new?\n. some can avoid these unrefs\n. the std namespace will be active everywhere you include this file (which is for example happening in the class list, so everywhere)\n. Can we have a constructor that just creates an empty structure on the GPU? So that I can allocate giant heaps of memory directly in GPU without the need to transfer it?\n. Actually for both vectors, matrices, etc\n. This is a dangerous operations, as expensive. Remove.\nFor unit tests, we can just transfer the full structure back to main memory.\nThere is no need to access single elements of gpu structures, just do linalg operations on them\n. same here\n. what about 64 bit?\nSome backends dont support it I guess? Does yours?\nIF the backend supports, it should be offered here as well\n. Please write clearer API docs. What does this mean? :)\n. This one is good\n. Remove\n. Remove\n. Needs better documentation (can be done later) in the @brief\nIn particular, what is this thing used for and maybe an example of a typical case\n. Be consistent with Capital letters at sentence starts\n. Better comment\n. @return\nalso elsewhere\n. No need to specify \"read only\", it has the const keyword\n. BTW, we dont use transferToGPU but transfer_to_gpu\nThis is true for all of this patch\n. Can you put disclaimers at these methods that the calls are expensive\n. The naming here is not really good, what does \"release\" mean? Free memory on GPU?\n. Should go away. If one needs elementwise access, one should cast this guy to SGVector\n. remove\n. not sure about all these aligns\n@vigsterkr  can you enlighten me?\n. If we do things like this, we need 100% unit test coverage (that is checked for mem errors)\n. why do we need this transfer? It seems wrong\n. this does deep copy only if vector is on CPU?\nWhy?\nNo deep copies via copy constructor. There can be a clone or so for that\nUse the same memory here\n. Shallow copy for this one\n. There is a problem here:\nWe definitely dont want the data to be copied here. We want the same data to be re-used. Otherwise using linalg operations will always cause a memory copy.\nTherefore, if copying from a SGVector, we need to increase the reference counting in order to avoid problems. This means that Vector here and the base class SGReferencesData need to share the ref counter business.\nNOW, that is a problem if we are doing all this in a separate class (no idea how to resolve atm) \nA further problem is the alignment of the data, because we need to copy data in order to re-use memory from SGVector\n@vigsterkr @lambday discussion needed\n. If we did not align things, then the shared reference counting would be easier. We can just make this Vector a sub-class of SGReferencedData and good.\nThere are also further questions of how reference counting behaves when data is transferred to GPU\n. why this release call?\n. why this?\n. Ok some more thoughts, more explicit\n- Definitely no copying. It has to be possible that we pass two SGVectors to linalg::dot which causes an operation that is just like doing the dot-product with say eigen3\n- This means there is a problem with shared memory managment between SGVector (used to create this Vector) and this Vector. Deleting the SGVector might free memory and Vector causes a segfault. Dont want that. So we need to share the reference counting. This means that Vector has to be a subclass of SGReferencedData. \n- Finally, there is a problem if data is transfered to GPU, because now, the original SGVector memory can safely be freed. So once we transfer data, reference counter can be decreased. This means transferring data \"gives up\" on data in main memory. Transferring back creates a new SGVector that is again reference counted. Maybe we should call the transfer in fact move_to_gpu (as original data might be freed)\n  - Need to think about alignment. Might have to be remove for now (and then later introduce when we align SGVector as well. I have limited understanding of this though.\nMy two cents. Let me know\n. On the question whether to have a separate linalg::Vector as compared to putting things into SGVector, the fact that we really dont want to have element-wise access for GPU data speaks for having a separate class.\n. dont use the && here and it should be fine\n. you can just use a new line for this :)\n. Say \"This example uses LibSVM as backend, Shogun has many more SVM implementation, see CSVM\"\n. shift\n. , i.e. whose values does not change if the inputs are shifted by the same amount\n. remove\n. CustomDistance\n. Please put this into a new PR\nAlso, if you are touching this already, please do\n\u00b4\"Number of non-zero features (%d) cannot be larger than number of features (%d) in the data\\n\"`\nI.e. pass the information to user\nFurthermore, you need to check that the result of the dynamic cast is not NULL, in which case you would segfault here\n. We usually say \"must be set\". But thats minor :)\n. not sure I get this REQUIRE and then static case.\nWhy not dynamic cast?\n. Maybe dont say feature space here, as that has a meaning in kernel land, and it is not the dimension of the input data.\n\"Dimension mismatch (left, right)\" should do it\n. maybe in parallel?\n. parallel?\n. I know this is re-factored, but maybe you can use the SGVector version?\n. give length (sorry I know its re-factored)\n. dont get these whitespace changes?\n. ?\n. Do \"...during training. It predicts by using...\"\n. remove classification here, no need\n. Can you maybe phrase all this in the way that there is a default configuration, and then optionally one can put in these decision rules. See what I mean?\n. \"out of bag error\" ... use \"\"\n. need a newline\n. pls dont call this eval, it is a python keyword\n. can we have some references, e.g. on the first page here: http://civil.colorado.edu/~balajir/CVEN6833/lectures/cluster_lecture-2\n. Can you remove \"for binary classification\"\n. yeah, different patch definitely. Always try to separate things.\n. remove\n. I think this might increase the ref counter, you have to unref later\n. I think you just want to reference the base class here. Not all the implementations\n. if you cast twice, why not store the result and only cast once?\n. remove: regression\n. do we have a SVM regression cookbook that we can reference here?\n. no precomputed kernel here? :)\n. MKL regression needs an SVM solver as input. We here use SVMLight. \nPut this further down\n. Put the SVMLight as SVM solver sentence hre\n. call this binary_svm_solver\n. remove \"for multiclass classification\"\n. Same as for the other cookbook, reference to binary case would be good\n. Basically all comments for  the other cookbook are also valid here\n. whats the value of MISSING?\n. This check does not provide information to the user\n. maybe this one should print a warning if things dont work. And then the SG_ERROR is thrown afterwards\n. might be good to check when and if this helps (later)\n. maybe use self->name here rather than CGaussianKernel?\n. openmp maybe?\nBut this is not the point here I guess\n. ?\n. The\n. BTW. Does RF work if no combination rule is supplied?\nIt should by default, and then you should say here that changing it is optional\n. The test will fail if you dont fix seed I think\n. So you might need to resend the integration test data with a fixed seed\n. there are still two dynamic casts in here.\ndynamic_cast<CDenseFeatures<float64_t> *>(m_features) This is what I meant, not the num_features (though that can stay also)\n. space after CHAID\n. Start with \"The\"\n. techniques -> technique\n. reference!\n. why not set it to NAN?\n. not sure I get the reasoning here: why should set return things that were there previously?\n. why not a bool to indicate that there was something before?\n. Actually, should be void return type\n. why do we have this string comparison? Why not just compare by hash? Every tag has its own hash right?\n. remove string comparison, it is costly, this operator is called on every get\n. this should be called m_name\n. We need doxygen @param and @return for all public methods\n. int32_t please\n. Minor: always split multiple test cases into multiple tests\n. this is not good, users should not be able to set/create arbritary tags for classes.\nRather they should only be able to modify existing tags. The public set should in fact throw an error if this tag is not existing in the map. There can be a protected set that can add new tags to the map, called from constructor just like the current SG_ADD\nYou can test that with a mock object on top of the unit test.\n. Here is a draft, alse here\n``` c++\nclass CMockObject : public CSGObject\n{\npublic:\n    CMockObject()\n    {\n        init_params();\n    }\nconst char* get_name() const { return \"MockObject\"; }\n\nprotected:\nvoid init_params()\n{\n    //SG_ADD(m_vector); // converted into the below, in fact we can just change the macro (LATER)\n    m_vector = SGVector<float64_t>();\n    set(\"vector\", m_vector);\n}\n\nSGVector<float64_t> m_vector; // this can stay for now\n\n}\nTEST(SGObject, tags_set_sgvector)\n{\n    auto obj = some();\n    auto vec = SGVector(1);\n    vec[0] = 1;\nobj.set(\"vector\", vec);\nEXPECT_THROW(obj.set(\"foo\", vec), ShogunException);\n\nauto retr = obj.get<SGVector<float64_t> >(\"vector\");\n\nEXPECT_EQ(retr.vlen, vec.vlen);\nEXPECT_EQ(vec[0], retr[0]);\n\n}\n``\n. same as above, we need mock and different test cases\n. As mentioned in the unit test comment, we need tosetOne for _adding_ elements to the map, called from constructor/register_params(just like SG_ADD).\nAnd one for _setting_ values of elements.\nFirst one needs to be protected, i.e. from outside, users cannot add members\n. Yes we will require C++11 after GSoC.\nSo can be removed\n. Why this?\n. data copying implicitly is not really good in my opinion....\nI would do a shallow copy on operator=\nEDIT. This is shallow copy. I think \"copy_data\" is a bit of an unfortunate name, but ok\n.@return. I would call these guysto_gpuandfrom_gpu. +1\n. Is this now clear @OXPHOS?\n. yep\n. iostream can go anyways\n. no cout pls\n. SG_SDEBUG actually\n. Please give more informative error messages\n. Better error messages! newline also :)\n. newline\n. these comments should go in the class docs\n. call the namespace linalg, not linalgs\n. put a using shogun::linalg here\n. the test name is meaningless, in the test you do vienna, doesnt match. please clean up\n. NO!\nthe ifdef should only be around the call for setting the backend.\nOtherwise the test is still executed (on CPU)\n. pls do systematic names.\ngpu_transfer_vienna_backend\n. Yeah this is not good. But not the point of this PR and has been there before.\nThis patch is just some mini changes that we need for research code.\nchris can deal with this, he is almost done with LARS\n. I would put this check into a method that you call everywhere.\nJust say: You called a direct memory access method when data is in GPU memory.\n. THen you just put a single method call everywhere where you dont want people to do things\n. please dont do whiutespace changes\n. any news about these macros @vigsterkr ?\n. remove \"here\"\nAlso, please make really clear what things are here. Are you talking about subsets of data or of components?\nThirdly, is the user able to cause this error message? \n. Can you ellaborate a bit here?\n. Note that if you permute here and then later loop over idx, then your memory access will be random.\n. the mock object does not live in Shoguns main src.\nIt lives in the same directory as your unit test\n. why do you call the \"constructor\" of float64_t here?\n. indentation\n. link to knn cookbook maybe?\nAlso, please dont capitalise, but write \"Largest margin nearest neighbours is a  ...\"\n. Can you say that this distance is one example that can be used?\n. No need to provide the math, just sgclass\n. sgclass knn\n. please dont abbreviate filenames. Call it \"largest_margin_nearest_neighbours.rst\"\n. dont mention parameter names in the text (they might change),\njust say that you provide the number of neighbours as parameter\n. can you add a bibtex\nThere is a nice JMLR paper on this\n. please dont call this \"eval\"\n. can you remove \"regression\" from filename\nAh crap, need to change data filename as well, didnt realise earlier\n. pls dont call this eval\n. Lol, sorry we are miscommunicating.\nSo here you have adynamic_cast*>(m_features)`\nand a few lines below you have m_fea = dynamic_cast<CDenseFeatures<float64_t> *>(m_features)\nI am talking about only casting once, not about the int\n. can you put this under the GP folder? Not under multiclass?\n. reference to williams and barber, there is a chaper on multiclass laplace approximation in there, the pdf is free online\n. this kind of stuff is also not slower/memory intense now?\n. and this is also still fine?\n. uh these fuckers, we should really aim to get rid of those interfaces ;) LATER\n. Can you test this with a really high dimensional and really sparse set of vectors?\nTo make sure it doesnt blow up memory?\nLike 10mio dims with 100 components nonzero\n. call this guy\n\"assert_not_on_gpu\"\n. @vigsterkr you commented that you might allow for those guys.\nI kind of think this is dangerous, and would like to enforce to use an explicit transfer back before allowing to access the data. This way, we make sure people are aware that this is expensive.\nThat said, all methods of SGVector should go away in my eyes, and be moved to linalg. SGVector should just be memory wrapping and interfaces, nothing more, in particular no numerics or linear algebra.\nBut we can do that in a later patch @OXPHOS \n. this interface is weird.\nIt should take a GPUMemoryBase and just create a newly allocated copy of the data.\nThen, when you call it from SGVector, you should just pass the GPUMemoryBase pointer and length\n. I dont like passing *this here. This requires GPUMemoryBase to know about SGVector.\nRather directly pass the pointer and length\n. Just line SGVector::clone_vector returns T*\n. pls dont add random newlines\n. techically no need for (...)\n. Phrase indirect\n\"Direct memory access not possible when data is in GPU memory.\\n\"\n. see other comments\n. I dont understand this method\n. woudnt this also have to_cpu?\n. why the additional \"get\" call?\nDont think we need that\n. this comment should be \"de- @vigsterkr - ified\" ;)\n. for your later TODO list\nWe want to remove all the methods that you put this check in here.\nAnd then we want to make them part of linalg\n. this crashes if I pass SGMatrix, no?\nIf so, this was a bug. Could need a fix there\n. please only do such whitespace changes in separate patches\n. \"on GPU\"\n. assert_not_on_gpu\n. to_cpu also needed\n. I would only accept GPUMemoryViennaCL here and force the cast to be done by caller\n. why do we need to store the length again?\nIt is already stored in SGVector, no?\n. Why do we need these constructors?\n. why is this constructor needed? I dont think it is.....or?\n. interface as already mentioned\n. can't you make pairs of backend and memory class friends?\n. I see now why you need the length.\nI would maybe rather make them parameters of the call then\n. ?\n. why is this typedef defined here?\nShould be outside of the class\n. no need for \"get\"\n. please dont use this constructor but to things here\n. Say where vectors are in the error message\n. why would this be public?\n. why not \"linalg\"\n. please no const vectors in unit tests\n. I guess dot unit tests should be in its own file?\n. this is not how we designed the API in the file I sent you. There we created a vector and then called \"to_gpu\" on that vector, returning a new vector. And then this vector is passed to linalg::foo\n. This both contains a grammar error \"it's -> its\", and also doesnt make any sense to me :)\nrather remove, or replace with something meaningful. Recall that users of Shogun dont know what SWIG is. Rather say that every methods returns a reference to the same object or so\n. Please structure your unit tests a bit more:\n- group by linalg operations and distinguish between linalg::math_call and linalg::memory_call type of calls\n- Make sure you unit test things as the flags before and after gpu transfer\n- Make sure every line of the low level code is covered by a unit test\n. the name here would be\nto_gpu_viennacl\nyou can also add a test\nfrom_gpu_vienna_cl\n. there is no backend set here\n. More things to test.\n- on_gpu method returns the right thing\n- all gpu operations also work if there is no GPU backend active\n. m_sorted_transposed_feats maybe?\n. memset!\n. is this NULL at this point? It should be\n. parallel! :)\n. minor: newline after this pls\n. I hope active_indices is sorted? \n. wait, why are you still traversing row wise?\nThis outerloop should be over columns\n. dupes is sorted?\n. AFAIK, the current implementation cannot do that. \nThe Random forest interface can, but the CartTree inside assumes float64_t.\nShould definitely be changed, see email I wrote. \nWe can (and should) update this cookbook once Shogun supports these things\n. I  meant: please dont use vectors like [1,1,1,1,1] in unit test. Choose something like [1,2,3,4,5] or random\n. No it is ok :)\n. Can you pls print the provided  features\n\"Provided Labels (%s) are of type (%d) where they should be regression labels (%d)\\n\", m_labels->get_name(), m_labels->get_label_type(), LT_REGRESSION)\n. See above\n. See above, please print the numbers in there\n. What is data is NULL? Segfault? Needs a check I guess\n. no commented out code pls :)\n. yesss! \n. This creates a copy if the type doesnt match ST?\n. Can't you re-use the existing code in the sense that you generate the float64 data, and then manually copy it elementwise to float32 structures?\n. I dont think this should be templated.\nThe type should always be inferred from the provided features.\nSo maybe accept CDenseFeatures here and get T from there?\n. pls dont do that (yet)\nsome older compilers dont like it\n. no template here pls\n. I wouldnt template this\n. please dont mix CamelCase_with_underscores\n. great that this works now.\nNow let's clean up the interface a bit in the sense that we dont have templated train methods.\n. There is a couple of options here. But this PR is about speeding the existing implementation up.\nSo lets merge this while it is small, and then open a new one for the ordinals\n. maybe call this different than params and see what csharp says\n. maybe yeah\n. @c4goldsw @vigsterkr \nCrhis, here is another one of those float64 casts, which shouldnt be there.\n@OXPHOS just fixed a minor thing here, but we should systematically get rid of such casts everywhere. Can probably grep for them and have a list somewhere\n. see also other comment\n. see the udpated directory structure for classifier and multiclass\n. @lisitsyn thoghts?\n. Can you phrase all this a bit different. Say that SVMs can be generalise to multiple classes via different strategies. Then below, regerence the CMulticlassSVM and the different strategy classes. This can be done with any SVM.\n. Also, dont capitalise things, and start the above sentence with \"The\"\n. Need some reference here\n. say that epsilon is optional. \n. Please show how to use different strategies , ovo, ovr, etc\n. Pls replace this by http://en.cppreference.com/w/cpp/algorithm/iota\n. can you memset to zero manually here? We dont want to use this interface really\n. try to avoid such whitespace changes\n. needs newline at the end\n. index_t\n. Minor. Period at end of sentence\n. And remove this \"Tree root not present. Just say that machine not yet trained\"\n. Actually, so the inner loop definitely can go, since it accesses a column of the indices.\n. Minor. Period after sentence.\nCan the user cause this?\n. period\n. A random forest\n. do this change, removing the space between the > >\n. I am actually not sure about the most elegant way at this moment.\nWe might have to do some explicit checking of types (somewhere under the hood of shogun)\n@vigsterkr whats you suggestion?\n. Gaussian processes\n. processes\n. Const mean\n. not class, it is an object\n. cane sugar :)\n. pls  dont call this eval but different\n. pls use the same style as the other bibtex files. I.e. abbreviate first names\n. The ShareBoost algorithm\n. pls put that somewhere central.\nOr even add a method to SGVector that turns it into an eigen3 map (if not yet there)\n. I think these guys should also be doxygen documented with @param and @return\n. Maybe you can even use this documentation in the macro above. And then here, you can just reference to that, doxygen can generate links to other methods\n. get_name should always return the class name without C prefix, but only so if the class is a subclass of CSGObject. Every CSGObject has a C prefix, other classes dont need that\n. pls say which one is on GPU and which one is not in the error message\n. THIS is the documentation that users see (they dont see any of the internals).\nI would put all effort of documenting things to this place. All internal methods can just reference to this one and say they provide a specific implementation. Dont mention wrappers, backends or anything internal here, this is the place where it says what the methods does mathematically\n. Same as above.\nAlso, say\nTransfers vector to GPU memory. Does nothing if no GPU backend registered.\n\"From CPU to GPU\" doesnt really make sense, it is just our internal jibberish ;)\n. It is not \"running\" GPU code, but transferring to GPU memory\n. Dont talk about wrappers, say what it does, see above.\nAlso dont say \"to CPU\" as above, but rather: \"Fetches data from GPU memory\"\n. just call it dot_unittest\n. cpu_backend\n. no const vectors pls :)\n. gpu_dot_without_gpu_backend\n. no const\n. you can use these guys to create eigen3 maps\n. please doxygen\n. maybe put a comment about the alignment and its purpose?\n. Say what it supposed to do, doxygen will state that it is purely virtual\n. Say what it is used for, link to SGVector\n. doxygen @param\n. please user CAPITALS for macros\n. Description of what is supposed to represent and how it is used\n. Actually, for this to work with plugins, we need to be inheriting from CSGObject.\nBut for now, just leave it like this (you can remove the get_name)\n@vigsterkr I guess we can easily change that later? Or even @sanuj can make this the first plugin\n. You can reference the base interface here and say this is the implementation wrapper of that. Note: This doc is never read by users, only by developers, so it should give the appropriate information\n. I think we should have a better error here.\nCreate a method in the base class that says \"Linalg method is not (yet) implemented in the registered backend \\\"Vienna\\\". For that, you would need get_name. So can think about making the backends a subclass of CSGObject (in contrast to what I said above)\n. maybe rather subclass CSGObject?\n. CAPITAL\n. reference main interface\n. reference the other method, only need to write the math in the main interface\n. CAPITAL\n. reference to main interface, as other comments\n. I think this check should be in a method since it will be called from many places.\n. until\n. useless comment. Tell other devs what it is!\n. can you do using namespace linalg?\nSo that the simple API is showcased?\n. Also LinalgDot seems not the correct name here, we always use the class name\nHere that would be LinalgBackendEigen no?\n. this method should be remove of SGVector and be part of linalg.\nPut that on your todo list :)\n. no need for != NULL\n. test left name not appropriate\n. flags?\n. using namespace ...\n. no const\n. This is how it is written ;)\n. no need for multiclass in name\n. not title\n. This is no sentence :)\n. cool!\n. SG_SERROR(\"Cannot operate with first vector gpu (%d) and second vector gpu (%d).\\n\", a.on_gpu(), b.on_gpu());\n. The left name should always be the class it tests\n. ??\n. +1\n. \"vector\" is a bad description.\n\"Pointer to memory where vector data is stored\"\n. @return, @param\n. good!\n. As said, please reference the method in the linalg main user interface here.\nNo need to re-write all the @param and @return here, we only want to write this down once. Only comments that dev specific should go here. Like \"Override in backend specializations to support this operation\"\n. Good!\n. Say that this needs to be overriden for any GPU backend\n. Actually, here is another thing.\nCan you add an additional abstract base class for LinalgBackendGPUBase. And put the to_cpu from_gpu methods there and make them purely virtual? This way we enforce that GPU backends implement them\nFurthermore, you can have different pointer types for gpu_backend and cpu_backend. Adds more type safety\nSorry should have said that before.\n. No need to reference base class here as doxygen does that already automatixcally\n. thats good\n. You can use the copy constructor of SGMatrix\n. seems good like this\n. please dont do such whitespace changes\n. copy constr\n. BTW for both of those methods: You either need to hide them from SWIG or put a %newobject directive in the interface file. I prefer hiding from swig\n. are the changes in this file related to parallel? If not, pls separate pr\n. Use iota on data.matrix. One line of code\n. we have a permutation methods\nrange_fill (or iota) and then permute\n. Need more tests.\nPls add some tests where you ensure that the objects can be used within threads. \nI.e. modifying the shallow_subset copy doesnt change another one...\netc\n. guarding would have done it as well, but this is OK\n. Cool I guess this helped?\n. Nono, wait.\nThe linalg operations should not be purely virtual (we allow for incomplete backend implementations)\nOnly the to_gpu should be purely virtual as this is really needed when implementing a GPU backend\n. Saves -> stores\n. saves to GPU\nclone data (no GPU here, English doesnt make sense otherwise)\n. not purely virtual\n. good!\n. Good! this file\n. I suggest that we group the unit tests by backend implementations.\n. definitely group by backend implementations and not by operation\n. minor, pls dont call this \"eval\" \nCan send a mini pr to fix\n. pls dont call eval\n. Ok then\n. do it\n. void setter +1\n. enlighten me why you do this :)\n. Tag not const?\n. please use a better name for this method, even though it is not public interface\n. const tag?\n. same here, better name\n. this one is a no-go. We cannot have STL headers in any shogun header\n. either forward declare or change the signature of the parameters method\n. inline can go, even though it doesnt hurt\n. Think that this is public API, called by shogun users. So the doc should be taylored to them, they dont need to understand the tag framework, but just want to call setters.\n\"Setter for a class variable, identified by a tag. Throws an error if that the class does not have such a variable. See ... for finding out which class variables exist\".\nAnd then doxygen reference a way to find out which parameters are existing.\n. no need for static error here\n. useless comment\n. this call should document itself, then you dont need the comment, this is why renaming is good\n. see above\n. no static error\n. Better error message, this is for users. Orient it on the thing I wrote in the doxygen above\n. See comments above\n. identified by a tag\n. no static\n. This is expensive. Please dont do it. Throw a SG_ERROR straight away from recall_type\nEDIT: Nevermind, leave the try catch. We can profile this later\n. You should say what happens if parameter doesnt exist here.\nI think this should reutrn an error and not an empty any object.\nAgain, this is called by users, not devs\n. See above\n. better error message that gives information of what is going on\n. dont do it inline and put in .cpp file\n. Write doc for users, not developer\n\"Returns a list of all variables available for this class. These can be modified using [reference to get/set)\"\n. This is protected, so the docs are different.\nYou should tell the developer here that he can use this method to register member variables for access through get/set and serialization\n. see above\n. Better names pls\n. actually, remove the vim part\n. I think the formatting is different for authors ,no?\nPls check other classes\n. Either hide from doxygen, or document using doxygen tags (I prefer the second)\n. the way to doc this: just Returns -> @return\n. pls doxygen doc that class too\n. Also say that it uses void pointers\n. missing space after \"to\"\n. this checks pointers of storage, right?\n. Tags are used in the public interface, so they need docs for users.\nExplain the user (who doesnt yet know about tags at all), what a tag is, what it can be used for, and reference the methods that the user will call with the tag (these methods now also have good docs)\n. Hash is never visible to the outside interface, therefore the public interface should not talk about it\n. I dont understand how the name can be different when the hash is the name.\nIs it because of collision?\n. This is visible to users, pls adjust for that case\n. BTW the filenames differ from what we usually do for shogun......\n@lisitsyn can you comment on that?\n. not sure you want to do this?\n. pls explain\n. no relative path here pls\n. can you pls register member varialbes here\nthats the whole point of doing the mock object, simulating registering members\n. I mean this what you do here should also be possible, so pls check both cases\n. this add could be called different.\nregister maybe? It should be obvious that this registers a parameter within the object (and it cannot be called from outside) Add is more internal API\n. now this looks quite nice! :)\n. this test tests set and get, pls rename accordingly\n. maybe add test for wrong type get call\n. name properly\n. add some exception tests here too\n. proper name pls\n. no commented out code pls\n. pls dont define these, but put them inline, just confuses this way\n. just remove this method for now\n. thanks!\n. can you name this \"register\" ?\n. remove \"param\" from name\n\"set_with_base_tag\"\n. you only call this class parameter here, did not so in the above docs\n. What does that mean? Only checks for name and not type?\nCan you somehow tell the devs when they would use this?\n. the docs are much better now!! :)\n. Can we make this doc a bit better?\nWhat is a \"policy\" here, what is a \"value\" here. Define these things.\n. More infos please as any is at such core of this new framework.\nReference the classes it is used in\n. @param, @return please\n. everywhere here\n. I don't understand why converting to Any means erasing. Can you explain that in the docs?\n. what does \"recall\" here mean. Say that in the docs\n. No need to mention sub-classes.\nSay something like \"Base class for all tags\"\n. this is a nice description!\n. ok!\n. In such simple methods, no need to descrivbe anything, just do the @return field and thats it\n. same here\n. maybe put a source code comment saying that\n. good!\n. give this guy a mini @brief so that other devs know why you create this class here.\nI gues there are two points\n- testing registering member variables in tag framework\n- avoiding member variables using tag framework\n. what type of file is this?\n.js ?\n. Testing memory transfer to GPU should go to the corresponding backend test\nGroup everything by backend. \n- one test file for each backend base (generic methods like gpu transfer)\n- one test file for each class of operations under a subfolder of the particular backend\nSomething like this\nunit/mathematics/linalg/SGLinalg_unittest.cc\nunit/mathematics/linalg/eigen3_backend_unittest.cc // generic tests, no operations\nunit/mathematics/linalg/viennacl_backend_unittest.cc // generic tests, no operations, include to_gpu\n...\nunit/mathematics/linalg/operations/eigen3_operations_unittest.cc // contains dot, etc\nunit/mathematics/linalg/operations/viennacl_operations_unittest.cc\n. I would then put the fallback operations in some base unit test, as it is not really testing a particular backend\n. Lets discuss tomorrow\n. No need for that INFO message\n. Pls dont use classnames in error messages.\nJust say \"Feature type (%d) must be C_DENSE (%d)\\n\", .... \n. whats the i_i suffix supposed to mean?\nI mean I get the idea of calling the templated method, but why \"_i\"?\n. ehm???\n. You basically need to switch over all types that CFeatures can have here, and throw an error message otherwise\n. @vigsterkr \nCan't we somehow do these type of checks in a base class? This will appear over and over again if we start porting more algos to using multiple memory widths\n. these tabs are not necessary\n. can you check your PR to avoid such whitespace changes, in particular if you just add random tabs to lines\n. so now we need tests\n- for all other float types\n  *`and for all other types (checking that errors are thrown)\n@vigsterkr can we have a unit test that checks the train method for multiple feature types? And then we can just add the the classes that support multiple to that test, and specify which types it should work on\ntest_features_train_type(lars_instance, {float32_t, float64_6, floatmax_}) // checks that lars works with all those types and throws an error otherwise\n. or we could even have a template for unit tests that just exchanges the features in train and results.\nLike automating the test that @c4goldsw wrote here based on the 64 bit version\n. does this doxygen link?\n. select_policy\n. I meant more like\n/** @ return name of tag */\n. Ah then it is fine..\n. this is still a blocker\n. why this?\n. ah because you fixed it.\nIs is valgrinded?\n. This should maybe be unit test covered?\nAlso, is this memory clean?\n. typo get_global_linalg\n. is this yet used?\n. ah cool, so this works now?\n. pls no commented out code\n. SGVector's destructor, just like it frees the CPU memory\n. this test should be part of vienna backend test file\n. why not call the file LinalgBackendViennaCL_unittest.cc just like all other tests are named\n. no need for the \"cpu_backend\" here anymore, just dot\n. just call the test \"dot\"\n. actually, not sure whether I get this test\n. I dont understand this here\nVectors dont have a backend, it is the state of the global linalg\n. you can't do that!\n. ah I see now....name is misleading\nget_appropriate_backend would be better, or something else\n. I was wrong, actually this is good, as it can be re-used in other places.\nNice!\nWhat you should do is do a null check, and throw an error in that case\n(this is when data was transfered to GPU and then the gpu backend was set to null)\n. but do the check inside the get_backend method\n. check if null and throw an error otherwise\n. yes, this test doesnt really make sense after our discussion as an error will be thrown\n. :dart: \n. Such changes should be in separate patches, always try to split \"operations\" from \"backend changes\"\n. maybe alpha and beta should have default arugments 1\n. ah they are set to 1 already, good!\n. maybe we want a private helper method for this?\n. why this?\n. @lambday so this kernel is compiled everytime we call this operation?\nIs that good?\n. should be ok but @lambday needs to comment as well\n. why delete and not SG_FREE?\n. pls dont mix camelCase with camel_case\n. no need for \"eigen3_backend\" in test name as this is already in the right file\n. Can also change that for the already merged tests and in the other backends\n. this is not good as the data is actually different then\n. add again?\n. add?\n. btw do you actually try the operations with the complex type? or the int types?\n. as said, it would be good to make a private helper method for this and use it everywhere\n. what about malloc then?\nAlso, details please.\nBTW new in shogun is overloaded I think (or it used to be)\n. absolutely, but do it from the next PR on\n. \"Vector memory on GPU but no GPU backend registered. This can happen if the GPU backend was de-activated after memory has been transferred to GPU\"\n. from_gpu should still work without gpu backend!\nSo that we can run GPU code without a GPU, important!\n. ok then! thanks for checking\n. is this a prototype or why is this kernel only?\n. pls dont do those. send separate patches for whitespace changes\n. the shared-pointer-like\n. great this works!\nThis needs to be tested for all shogun classes.\nSee the unit tests for clone and equals, which we test for all classes as well\n. Why the cast?\n. Okok\nOn Thursday, 7 July 2016, Esben S\u00f8rig notifications@github.com wrote:\n\nIn src/shogun/distributions/kernel_exp_family/impl/Base.cpp\nhttps://github.com/shogun-toolbox/shogun/pull/3355#discussion_r69956349:\n\nreturn result;\n}\nfloat64_t Base::log_pdf(SGVector x)\n {\n    set_test_data(x);\n-   return log_pdf(0);\n-   return ((const Base*)this)->log_pdf(0);\n\nFixes the ambiguity warnings. On my Mac it didn't compile because of this\n\u2014\nYou are receiving this because you modified the open/close state.\nReply to this email directly, view it on GitHub\nhttps://github.com/shogun-toolbox/shogun/pull/3355/files/b45bdc86ebc800e8c2cfbf1a6268b91fee09bc9b#r69956349,\nor mute the thread\nhttps://github.com/notifications/unsubscribe/AAqqvy2iJLqpsG39Zb7mTNYY38nzZj44ks5qTUA0gaJpZM4JG80d\n.\n\n\nSent from my phone\n. can you remind me why we need this?\n. did we use shared in vector? Then all good!\n. I like gpu_ptr more, nice!\n. I like @lambday s idea on not expicitly writing all types all the time\n. Lets have a re-factor patch separately for vector and change this PR to the new style\n. can we pls split this patch and apply the renaming first in a separate patch?\n. indentation\n. This will also be available for both vector and matrix I guess?\n. Cool idea actually\n. what does \"temporary\" here mean. That is is removed before a merge?\n. why would this be comitted if it is automatically generated?\n. docstring documentation please\n. Can we please integrate this into cmake properly rather than printing python output and then not failing?\n. same comments as in other patch\n. pls no random whitespace changes\n. We have to have a discussion on in-place vs allocating new memory\n. updates?\n. dont put parallel in name of this method pls, rather \"batch\" or so\nBut I support the idea of putting that in\n. pls hide from SWIG\n. Pls do REQUIRE\n. is this cache friendly when running on multiple CPU?\n. same comments as above\n. I kind of dont like having this separate (though it is ok for first round)\nCan we put this into linalg eventually? Might be useful across shogun.\nAlso, we have eigen3 which might be able to to this SIMD and more cache friendly, as it is a common operation.\nThis should be investagated a bit eventually,\n@vigsterkr thoughts?\n. pls reference the classification cookbook\n. can you make 1 and 10 explicit variables and mention them?\n. can you align this?\n. pls abbreviate first names\n. can you pls send a separate patch that adds the nn examples under regression and classification in the meta examples as well (so they appear multiple times), also for the gp examples. \n. pls use the \"doc\" style of referencing.\nAlso pls do that for all other cookbooks you did (see my email, separate patch)\n. remove \"some\"\n. pls not gpl\n. can we pls do this properly?\n. I think you will get a warning that nothing is returned here, otherwise good\n. some thing might not compile with complex type, just a warning for now ;)\n@lambday \n. yeah I saw and I think this is ok\n. return NULL\n. whats the ratio here?\n. these hard-coded choices are from the original code?\n. shogun naming woul dbe lower case, if this is taken from liblinear code, it is fine\n. Memory, I.e. Vector or matrix\n. Can you document this method a by more explicit and day what it actually does?\n. See above\n. Typo first\n. Indentation\n. Indentation\n. What about a few more points?\nAlso, can you comment on the rationale behind this test?\n. The test name is not really useful, make it more explixit\n. Yeeeeah :)\n. Yeah I think we can should dare to do it now\n. trais is ok\nprecise buildbot will, see here\n@vigsterkr I guess precise can go offline as of this merge now\n. can you give a 2 sentence rationale for having this class?\n. SG_INFO, or SG_DEBUG, see the different loglevels in SGIO.h\n. MANIFESTO :dancer: \n. Try to test every line of code with tests as minimal as possible\n. I see where this is going.\nWill the API stay like this?\n. I vote that for now, we always do these type of operations in place.\nAnd if a developer doesnt want that, he can copy the vector first\n. Can we have a separate patch for the cmake changes ?\n. This seems ok but what we really need is a patch that serialises the parameter map of the tag framework....thoughts?\n. pls never use const vectors in unit tests, just as in linalg\n. this is a well tested shogun class?\nI remember either kdtree or cover tree was buggy\nAlso, are both implementation BSD compatible?\n. can we pls remove these horrible things? :)\n. This thing is actually quite good to have. We should put this into other algos.\nWanna send a snother patch for that?\nI think it doesnt work for openmp though\n. We should aim for mimicing sklearns setup and default parameters, as I think it is really well tuned\n. can we make this d-dimensional rather than n?\n. Can we not call these training and test features? But just features_a, and features_b?\n. indentation\n. yep thats exactly how I imagined it :)\n. I think no pointer here, but not sure\n. ah so this works\n. might be the SG_ADD with pointer is wrong\n. can we please have row-wise and column wise sum here as well?\nShould that be in a new patch or not?\n. this can go I guess?\n. all of them can go I guess?\n. I guess eventually, we just want to call this \"save\" and \"load\"\n. just wondering now: are these called by users? If so, who creates the Archive?\n. why unref?\n. I agree with this change, but it should be not in this PR, as it makes it harder to review\n. indentation\n. Give more information in such error messages\n. indentation mess :)\n. you should start using a proper texteditor, one that doesnt override things in files\n. why is this happening?\n. this is basic testing I guess....but the user API should look a  bit different, right?\n. yep!\n. Mmmh I see,\nNot sure we would do that in  here. Where is the circular thing biting you?\n. can you remove this change?\n. This should not be here but in SGVector\n. we dont want eigen3 to be in header files if possible\n. whitespace cleanup needed\n. Yeah you are right!\n. Weeeeellll.\nI mean so internally OK.\nBut for shogun users, can we please have functions that only take filenames and an enum that tells the file type? SWIG compatible etc.\nI\n. We dont want that in Shogun's interface to the outside world (SWIG)\nWe want the save method to take a filename string instead (and an enum for the type for serialization the user wants)\n. this is quite a big change.\nRationale?\n. unnecessary!\n. needs update\n. haha great test :)\n. pls remove this change\n. why this?\n. No need to say it is templated, the code speaks for itself, and the interface\n. is the \"t\" suffix the right one  here?\nWhy not \"_i\"?\n. Start sentences with capital letters\n. temove templated\n. see other comments\n. no need for this change\n. useless comment\n. typo: should\ntypo: regression labels.\nMissing period\n. \"No features provided.\\n\"\n. say what we got instead\n. say what we got, give more details on what was given.\nThese error messages are for users, so dont make their life hard :)\n. remove this comment, put in another else if, and bail on else\n. why the suffix?\n. this whitespace is unnecessary\n. I wont comment on more of those, but please dont do such changes\n. ah now I get the template suffix thing, it is a different variable\n. ?\n. Better error message!\n. Just say this is a templated specialization of the train method\n. remove!\n. remove\n. nice and covering all cases\n. Can we have a link to the docs for this in a comment?\n. How long does this take?\n@lamday ?\n. Good idea that this is in its own file\nCould you document the high level idea a bit?\n. As said above, I wonder how much overhead it is to generate these things on the fly\n. No it's good, I just got confused ;)\n. @Saurabh7 probably know a good one\n. What the class is used for -- for fellow future shogun hackers.\n. The viennaCL docs should talk about this API change somewhere. Where do you know this from otherwise?\nI just want a simple HTTP link in a source code comment here\n. a REQUIRE would be better here, with a proper error message for users\n. you can do a memcpy here, no need for a loop\n. index_t for the loop counter pls\n. index_t and memcpy\n. is this method hidden from SWIG?\n@Saurabh7 it contains a std::vector which we usually try to avoid in interfaces\n. REQUIRE saves the if\n. use REQUIRE rather than if and SG_ERROR\n. The sentence seems to be starting weirdly \"An interface method to -- this is called ...\"\n. No need to explicitly mention the base class name here (i.e. remove CLinearMachine)\n. pls doxygen explicitly reference methods you refer to, so that doxygen generates links in the html ouput\n. try to avoid explicitly mentioning the supported types since they easily get outdated. I.e. here oyu talk about 32 and 64, but not about the recently added floatmat. So better completely avoid such things\n. typo LARS\n. The comment is also way to wordy:\n@data training data\n. Ah this is annoying. I guess we can fix that with the hash in the new any.\n@sanuj @lisitsyn  ?\n. This is great, so we can now finally have this as part of BSD shogun?\n. Again, I think all add and scale methods should be inplace. If a user wants a new matrix where things are added, he can allocate it himeself\n. Can we do this less verbosy?\n\"Number of rows of matrix a (%d) must match matrix b (%d)\"\n. Can you explain what you mean by this?\n- de-serializatzion directly allocates member variable memory and loads the data there\n- setters (if exist) might allow to change members\n- otherwise no access\n. GPU matrix?\n. We need to resolve the refcounting thing here before merging\n@vigsterkr ?\n. can we please stay with Shogun's method naming convections? No camelCase\n. Not sure, I would find a single method that takes an enum cleaner here. But that is my taste\n@vigsterkr any comments?\n. nice and clean :)\n. This is _user documentation\nPlease tune it more. Don't talk about wrappers or anything, just tell the user that this serializes the SGObject to a binary file. You can reference to other methods in doxygen for details\n. same for all other methods\n. Needs to be hidden from SWIG.\nAlso please put more information for c++ users, and also for Shogun hackers in here\n. can we have to patch to any separately, just as the one for SGVectpor\n. its long to do it this way, but I don't really see another way, and it at least is very clear to read (in contrast to before where we have the same type of case distinction, but hidden deeply inside the spagetthi code.\nSo plus one!\n. BTW it would be cool if the whole serailization framework would have debug message that are very clear to read. They should include the parameter name, the type, etc\n. same here, please put debug messages in\n. Can we also at some point have a benchmark of how fast it is to store classes with the new vs the old framework?\n. ping!\n. no const in tests!!!\n. why the {} ?\n. remove \"_pair\"\n. call it load_equals_saved\n. better name for test please (see above)\n. yeah, ok!\n. I think that makes it a bit messy interface wise. Why performance?\n. @OXPHOS I discussed with @vigsterkr \nFor all operations, that we might want to do inplace, we want something like this:\nSo that we can do the three cases described in the gist.\nWhat would be even better was if we would not manually write the wrapper method for automatic allocation, but have an automatic template/macro way to generate it. That should be possible with c++11 templates, or even with the macro you have.\nIf we can do a proof of concept in this PR, then we can update the remaining operations. Ping us if you have questions\n. @OXPHOS Please add to your TODO list (in addition to deplying these updates) that we want to document things also in a readme, in particular the inplace example by @vigsterkr above should be in there to make the point that we can do inplace, but it will be slower, so we only wanna do it if memory blows up otherwise (e.g. element wise operations on gigantic matrices or so)....\n. pls dont do 2 variables per line\n. Sparse Gaussian Process Approximations\n. Just refer to the GP cookbook for saying what a GP is.\nI.e. This cookbook illustrates how to use sparse approximations to Gaussian processes\n. The inducing points can either be a subset of the training data or being optimized free form\n. Need a better reference, i.e. http://www.jmlr.org/papers/volume6/quinonero-candela05a/quinonero-candela05a.pdf\n. FITC is a crappy approximation, I suggest we use the Titias here?\n@yorkerlin ?\n. can't we learn the inducing points and only specify their number here?\n@yorkerlin whats the cleanest interface way to do this?\n. class link!\n. You can't. My bad :)\n. Google it, I dont know by heart\n. I agree this is an awkward interface, suggestions?\n. is that true?\n. remove the () after on_gpu maybe?\n. XOR for victory! :)\n. I mean inplace should be avoided where possible (for speed reasons), and only applied in particular cases where the memory cannot be afforded. So in that sense I dont care if the interface is awkward. But still would be nicer to do this somehow nicer (i.e. with a flag or so)\n. missing space\n. technique\n. chi 2 test should be here\n. link for that ANOVA is necessary\n. It should print every parameter (name and type) it serializes/loads. For Vectors even the size etc\n. still\n. pls remove this\n. ok!\n. yes!\n. minor: space after a and b\n. indentation\n. please dont use contant content vectors for tests\n. typo\n. Can this be documented a bit neater?\n. hide from SWIG (or is it already?)\n. updates?\n. Please write proper error messages\n. @lisitsyn how does your range thing work?\n. you only need to do that in the header\n.  false?\n. newline missing.\nAlso, is this a message for the user? Then it needs to be more concrete, i.e. how to solve it?\n. Cannot\n. newline missing\n. yes, but remove the newline in between\n. Did you check this for potential segfaults?\n. can you send a separate patch for those?\n. I dont understand why false\n. yeah imagine you already have a big matrix where you want to put things for some reason (I sometimes do in my code)\n. structure is good!\n. @sorig can you guide this a bit while I am away?\n. ?\n. whats the motivation here?\nalso for all changes in meta examples, always ping @sorig \n. Can we use REQUIRE here and give the user information of what is wrong?\nhttps://github.com/shogun-toolbox/shogun/wiki/Assertions\n. this is a good idea\n. if you do static casts, be more explicit, i.e. put a comment that this is ok, or do the cast just below the assertions above and store the result in a variable\n. Sorry to be picky here:\n- We need a line break (>80chars)\n- There is a newline missing at the end of the string.\n- The message should give information what went wrong (i.e. the GIVEN label type and also WHICH labels are bad)\n\"Given predicted labels (%d) must be binary (%d).\\n\", predicted->get_label_type(), LT_BINARY)\n. can you demangle this a bit?\n. see above\n. ace\n. is this always safe?\n. minor: use rand_num_feats  -- style consistency\n. same as above\n. line wrap\n. rand_num_feats\nMaybe a better name is appropriate as well: num_random_feats, num_rand_feats, or so\n. Avoid mentioning class names. Just \"Machine cannot be changed.\\n\"\n. This message will only confuse users, who dont know m_machine, NULL, or similar. Also dont use class names in error messages.\nAlso, REQUIRE in a getter doesnt make sense\n. Same as above\n. Same as above\n. Please be more careful with error messages.\nREQUIRE(rand_featsize>0, \"Feature subset size (%d) should be greater than 0.\\n\", rand_featsize)\n. can you use a better variable name?\n. as above\n. \"No machine provided!\\n\"\n. Reference machine? Again, please use good error messages that do not confuse users\n. cast safe?\n. At least you should put in a comment why\n. no {} for single statements\n. you could use auto here, or even @lisitsyn 's range\n. Can you put in two more sentences in here?\nAlso explain that this only works with certain type of base machines!\nAll the caveats that might produce error messages\n. Capital C\n. Capital C, also below\n. remove \"get name\", just \"@return\"\n. Capital M.\nAlso please be clearer in your language here\n. newline here\n. proper error message please!\n. please dont dynamic cast all the time. Store a typed pointer!\nAlso is the cast safe?\n. we abbreviate first names in our bibtex\n. \"reducing computational load\"\n. via replacing the kernel matrix with a low rank approximation\n. Maybe remove the last sentence, not related to Shogun :)\n. use math for n and m\n. I think it is O(m^3 + m^2n), and you should also say that this is for computation.\nI would also give memory requirements, and then compare both with the original KRR costs in memory and computation\n. Does the order matter here? Or are the specific rules always preferred?\n. Lets fix and merge :)\n. please respect shoguns code style\n. Please dont mention method names or technical terms such as \"constructor\" in error messages for users.\nJust say \"Features have not been provided.\\n\"\n. remove specified and add space before (%d)\n. I wonder whether we can macro this somehow @vigsterkr ?\n. why the rename? This might be better in another patch\n. newline before {\n. I think this is fine as this single loop is fast.  It would be more code slickness ... so maybe have a method?\n. space before /=\n. please dont do this. If you edit a file, stick to its style, if you wanna change the style, send a separate patch\n. it would be cool if we could have some snytatic sugar to make this shorter.\n@lisitsyn ?\n. please dont use such methods on SGMatrix - they are obsolete.\nUse linalg CMath or eigen\n. Best to add it to linalg and use that\n. either that or leave it as is ....\n. doxygen\n. superflous comment\n. superflous comment -- the next method is called train ... no need for that :)\n. Can we use a macro here?\nOr doenst work with the googletest macros?\n. You can stay within the style of a particular source file. Whitespace changes should always be in separate patches. The optimal thing is to clean up whitespace first and then do your own changes.\nMixing things is never good as it is hard to trace bugs this way.\nI hope to integrate a style checker soon so these discussions are of the past.\nIn terms of style. Just dont mix adding spaces before and after operators with not adding spaces, i.e. dont do a+= b or a +=b\n. sorry, my bad\n. yeah definitely good utitlity method. Linalg could do this\n. yep!\n. Dont do this yet. We will do this systematically soon\n. Nevermind actually, this is fine!\n. I get it! Good! The name could be better, or a source code comment would help\n. dont need the \"test_\" prefix in the name\n. superflous comment\n. we dont do camelCase in Shogun\n. we use {} in loops and if then else if there is more than a line afterwards\n. Can you please cherry pick this from the develop branch?\nOtherwise we will have a merge conflict\n. remove \"method of\"\n. Can you comment (in source code) on the possible outcomes of the eigen3 info?\nAnd can you properly translate this into a shogun error message?\n@vigsterkr what about some proper exception handling where we forward such numerical errors as numerical errors rather than just standard exceptions that make shogun stop?\n. cherry pick\n. please also check for correct dimensions of output\n. same here, please check for dimensions\n. Can we please have a boolean flag (or a separate method with separate tests) for LLT and LDLT. \n. please cherry pick the google mock, separate patch for that, before all the others\n. no more special thanks?\n. That should be somewhere else though. I mean Evangelos wrote it on purpose and it is nice to thank his mentors.\n. you need to make sure this works when executed from its folde ron our servers. See the other notebooks for the relative path of the usps data\n. Rather than putting all content into a single cell, split it into multiple ones.\nThen dont use source code comments to explain what you are doing, but the notebook markdown in separate cells\n. please define functions in their own cells\nAlso give every function a proper name, and put a minimal docstring\n. please no commented out code in patches\n. pls do\nimport modshogun as sg and do all imports in a single cell\n. every plot needs a title and axis labels.\nIn addition, it should have some description of what we can see\n. can you pls remove the camel case in variables?\n. That is a good question, so it cannot be the same method, has to be another one. So let's do that later. Another PR I guess\n. no space in the template please\n. this variable is not used anymore I guess?\n. this is not needed anymore, or needs to be modified to check vec.vec. EDIT: vec.vector\n. as above\n. as above\n. typo \"It is \" -> \"it is\"  ... even better would be \"which is\"\n. Make this: \"Please initialise and/or update the data submodule.\"\nIt is never a good idea to put explicit commands/names/etc into messages for users as these things outdate at lightning speed.\n. Yes, read above\n. Decition tree learning is based on trees as predictive models. (Remove the second sentence). There are two types of decision trees:. Or even better: There are decision trees for both classification (integer-valued) and regression (real-valued).. The classification and regression tree (CART) algorithm is an umbrella method ..... In this example, we show how to apply CART to multi-class datasets.\nRemove the rest and fix typos. These words you can also use above when you talk about regression/classification values. A reference to a wiki article would be good here. thinking about this. Most ndarray libraries that might support higher order tensors accept an axis for mean computation (rowwise would be 1, column wise would be 0), etc. Might be something to keep in mind for unification .... can you remind me of the discussion we had here on preallocated memory for results?. Can you clearly state here that it is the callers responsibility to pass an appropriately allocate memory matrix!. And also if/whether it is possible to pass one of the operands arguments (A or B) as a result? (since you do that in unit test). style, please use A.num_rows * A.num_cols here. Same everywhere else. and this works if a==result?\nWhat about eigens aliasing?. good!. Sure, or in the first cell that is not used for description. Absolutely do that :). that was my question. I have a hard time believing that it is an inplace operation though, as eigen3 would need the *= operator for that, no?. Mmmh, I think this is missing the point a bit.\nWe want a method that takes a cholesky factor and then solves the system here, it should just be the triangular solve. \nSo that I can precompute the Cholesky factor and then solve multiple systems with it without having to re-factor again. \nSee what I mean?. Can you use $x^\\top x$ for vector dot products?. and $d$ for the dimensions. The distance matrix. We can compute non-symmetric distances via. I just rendered the math here. It needs some minor tweaks (sorry for not checking earlier)\nthe _i subscript should not be in the bf macro, i.e. \\bf {x}_i\nCan you make write the denominator as \\Vert \\bf{x}\\Vert_2 \\Vert \\bf{x'}\\Vert_2 and then say \" where :math:\\Vert \\cdot\\Vert_2 is the Euclidean norm.\". now you can actually remove the \"d  dimensional\" -- d is never used anywhere. Just say in the first sentence\n\"The Cosine distance for real valued features x and x' is the similarity as measured by their angle.\" And then remove the \":math:\\bf x and :math:\\bf x' are :math:d dimensional feature vectors.\". Minor: Capital \"S\". no need for inline, and also no need for a bool return. Same as above. we should actually assert positivity here, but since this will change in the future, nevermind :). Nono, eigen3 can do triangular solves when you already have a cholesky. The LLT class in fact should be able to do so\nI think you can create a self-adjoint-view of the factorized matrix and then just call solve (which will do the same as calling llt.solve(x) when llt is already factorised internally. Check the eigen3 source code and just mimic it.\nEDIT: I just checked and eigen3 just calls solve on a particular view of the matrix to do the triangular solve ... so you can do that.. Ah and keep the current way of just computing a cholesky based solve once. In addition, there should be the option of providing a precomputed cholesky factor\n```\nL = cholesky_factor(X);\nalpha1 = chol_solve(L, a, true);\nalpha2 = chol_solve(L, b, true);\n// and the direct solve (re-factors the matrix)\nalpha3 = solve(X, b, ST_CHOLESKY);\n```\nsomething like this. It is dangerous whenever you are overwriting memory that is used for the computation.\nThis is why eigen3 often allocates temp memory. Make sure you really understand the mechanics here. Array is just a view onto matrices that allows for element wise operations (different operator * overloading for example). I mean if the test works, then I am fine with it, but pls memory check it.\nI recently wrote a readme for that in the developing.md in the wiki. rebase for this. btw do you have a gpu so that this code is actually tested on your machine?\nRemember that our new implementation falls back to eigen3 if it is not available ...... minor. character line limit. This needs to be tested. Is it tested on your machine? Travis cannot check it.... I am a nagger, i know.\nBUT we should not offer such a method. It is extremely cache unfriendly -- matrices are stored in column major format in Shogun. This is why if you change a single element of all rows, this totally screws up the prefetchers hard work.\nDevs should be forced to set the columns to constant values, and then potentially transpose afterwards.\n@vigsterkr @lambday thoughts?. Can you also ASSERT dimension equality before checking the memory content?. To avoid a crash if the clone method didnt work correctly. everywhere in fact. Not sure this is the clean way to do this. Maybe rather in eigen3.h\n@vigsterkr ?. This will have to do with different eigen3 versions. The way to solve this is to use guards on the eigen3 version, grep for #if EIGEN_VERSION_AT_LEAST(3,0,93). As we want shogun to work with all the versions ... Check the docs for when this type was defined etc.. is the absolute difference (singular). between components of two data points. Please also say that is is L1 distance, and add a wiki reference below. Yes, as otherwise it might segfault when you try to read the matrix at these indices, (say if the create matrix was smaller for some reason). ehm?\nThis needs more ellaboration :). I am unsure about the name \"apply\" here. Why not matmul (like tensorflow), or just dot?\n@lambday you probably have thoughts on this?. why are these changes in here?. Ah I get it, so you just moved them up.\nMMh, maybe this can be done in a sep patch? It kinda messes what is happening here. Yes definitely.\nAlso I am unsure about such a method... it is potentially slow, likely to allocate and copy anyways, problems with multiple references, etc. Is there a definite reason why we need it?. I guess you could simplify via just saying that the dimensions don't match and then printing the given ones plus the transpose information, then you just need one message. But this verbose version has advantages from a user perspective as well..... leaving it to you. That's true, thanks for pointing it out.\nMaybe then the name dot as in numpy? I don't like apply too much.... Cool!. If it is really not used, kill it.\nOtherwise, having a column version of it is ok, but make sure to use memset in there. Minor: unneeded comment it's clear from the code. Equality check on floating point numbers are not portable and randomly might fail on different systems, use expect_near macro. Btw why do you do things twice here?\nOn my phone currently so hard to see tyat. Shouldn't these be const references?. This doesn't look like eigen3 code. Also this really badly needs multicore acceleration \nOpenmp, did you ever use that?. Really hard to read. Is this tested?. This is crazy code\nDid the original state where it's coming from? Is it tested? Used?. Just say matrix, not image. Goes everywhere. I like these wrapped methods and the possibility to pass preallicated mem :). Please dont change any existing tests (in this commit), but definitely use EXPECT_NEAR for floating points in your new ones. Please use \\top for transpose. Please dont define D, just give the formula. As said above, this won't be merged with equality checks for floating point numbers in your added code. grep for convlution here:\nhttp://eigen.tuxfamily.org/index.php?title=3.3\nWe can take code from there, this one should not be in, it is slow and unreadable. I know it is not your fault, but if we have unit tests it should be easy to update the code...... ok, so we need to make sure we test this properly after a merge. So our NNs then implement this operatilon manually?\nIf so, then I suggest to remove this entirely, and then open an issue to put a proper implementation as an entrance task.\nAgree?. We dont you just allocate the matrix in different size rather than resizing it in the first bit of code?. ```\nSGMatrix result;\nif (transposed_A)\n    result = SGMatrix(rows, cols);\n....\n``. I think we could also add an SG_INFO message for the iteration here if you want.....but seperate PR. Maybe just the iteration number.\nIf there is some residual that is checked to set theconvergedflag, you can also print that. Please also print the maximum iteration number.SG_INFO(\"Iteration number: %d of max %d\\n\", iter, max_iter);`\n(also watch whitespace and spelling). This I wouldnt expose. In fact for the perceptron I would not write anything in info, just the iteration number. I guess\nassert_on_cpu is the right name here? :). renaming should be in a single patch that just does that. These changes should be in their own patch. filename would be KNNSolver. This should have a C prefix, otherwise many things in shogun will stop working. And this does a triangular solve now? Cool!. Please dont enumerate the methods. You think it is better to directly call eigen3 here? Then some overhead is avoided.... (storing the lower factor). It would be cool if we had: QR, LU, and SVD.\nBut next PR I guess :). in lars, we also update the cholesky.\nThis is a method linalg should also have, put it on the list :). This is so that you can use the java files I guess. Good. copy paste typo. I think we don't need another json here, just use the existing java json, the syntax is example the same. this has to be detected by cmake\n(send another PR for detecting scala). same here, do that in a sep patch. I think the environmental variables here are mixed up with java a bit?. Minor: no space after the colon. But I will merge anyways. Did you write this file? Or did you find it somewhere online? If so, please put a comment with the link.\nIf you wrote it, double check existing solutions for inspiration. Filename:\nKNNSolver.cpp. Can you make this the BSD license (migrate the old contributor names). Can you put something like \"inspired by [link]\" as a comment in this patch?\n. I think this should be selfAdjointView btw. because we know the factor is square. so?. This needs to be a better message. Imagine the confusion caused by this one :). Same here, please provide a better error message. maybe name this COMPILED_JAVA_EXAMPLE ?. indentation here is a bit weird .... yes I think I like this. maybe you can put that to the place where you set the flags rather than down here?\nAlso, the indentation is a bit weird. indentation not respecting the previous one, makes reviewing much harder for me btw. which needs a sep PR, I suggest do modify docker config first (it auto-updates the image when we merge a PR to the config file), and then this travis change in a single PR.\nThen the scala stuff you made in a next PR. As said before, this file can go. We just want to execute the already present java files. on travis x:cal is infinity. this needs to be ASSERT_EQUAL technically otherwise the loop below can segfault, same below. But nevermind. Nope, no element access for GPU structures (forces user to transfer back to cpu first). Nope, no element access for GPU structures (forces user to transfer back to cpu first). The point is that element-wise product on the GPU is done in vectorised form. Watch your English here :) Punctuation, grammar etc.\nSuper minor!. Try to write cleaner English:\nKNN predictions are costly: predicting a new point's class requires computing distances to all training samples. While Shogun exploits multi-core infrastructure, KNN quickly becomes infeasible for large datasets. . We can see it been significantly speed up by Cover Trees solver, but the result of KD-Trees solver is not so ideally. \nThis sentence doesnt really make sense, re-formulate. \"So, let's do a more systematic comparison. For that a helper function is defined to run the evaluation for KNN:\"\nbetter: We now do a more systematic comparison of Shogun's KNN implementations. We begin by defining a helper function\n. there are logic problems with this. what if both flags are True ?. better to use a string, as usually done in Python. indentation!. generally avoid whitespace changes, but I guess there here are OK because there was junk there before. mixing tabs and spaces. please put an info message telling the user that java is enabled because scala needs that. I thought we don't generate scala code? But rather run compiled java examples using scala ?. \u1e81e should maybe then create these folders by hand. Definitely no json file please. no scala pls. please dont compile scala examples, you just need to run the compiled java examples. this should stay in as we definitely want to integration test the scala output. minor: The \"===\" are longer than the text, which will cause warnings in the pipeline. \n. you can do that with cmake btw. \"of different\". You just need to say what you mean in a clean and neat way, and also avoid grammar errors (ideally is an adverb where you want to use an adjective). For example.\nWe can see that the cover tree solver is faster than standard standard KNN. In contrast, the kd-tree solver is much slower. This is likely to be caused be the high-dimensional data that we used.. Maybe you can also add an investigation of runtime as a function of dimension? This would be nice as it would confirm or reject your hypothesis above.. We can see that the Cover Tree is faster than standard KNN --- should be\nWe can see that the cover tree solver is faster than standard KNN\nThis is likely to be caused be the high-dimensional data that we used. --- should be\nThis is likely to be caused by the high-dimensional data that we used.\n. Please move any text to text cells, not in python comments. All the algrotihms are super fast, which makes timing very inaccurate. Can you maybe use a larger dataset where things take at least order of a second?. please dont capitalize words that are not names \"standard\" \"tree\", etc. remove the part \"which prove the KD-Tree is more suitable for low-dimensional data\"\nThis is not true (nothing is proved here), and it is also redundant with the first part of the sentence. Dont mention subclass names in the base class. Hard to maintain. dont capitalize words that are not names (solver). it is not \"will include\" but \"includes\". Please pay a bit attention to grammar.. The KNNSolver class will include most of Variables which been used all the other Solver and methods been used to choose\nIn fact this sentence doesnt make any sense. Remove it and just say that this is the base class for all KNN solvers. If you want to say something here, mention important methods (just say what they are intended to do). On the other hand, if the methods are well documented, no need to say a lot (apart from that this is the base class). No need to talk about inheritance, this is clear for the API docs automatically. . It will use -> It uses. It will use brute way to classify the object which mean compare the object against all training objects for each prediction. -- should be\nTest points are compared to all training data for each prediction.. Please adjust as in my other comment. typo: detailed. It uses cover trees to speed up the nearest neighbour search (typo and grammar mistake).\nMake it two sentences: For more information, see [link]. same as above comments for class docs. These files need BSD headers (unless they are GPL, in which case we cannot use them). can you pls use the one I wrote in the comment? \n\"Test points are compared to all training data for each prediction.\"\n. No need for the first sentence (also in the other places). Mistakes:\n * It uses (third person verbs have an s!)\n * It uses cover trees, or it uses the cover tree algorithm\n * .... to speed up the nearest neighbour search process (not searching process) ... Also it is not a search process, so rather write: to speed up the nearest neighbour computation\nIt uses cover. why the square brackets around the link?. see above. This stuff needs change as well. \"Virtual base class for all KNN solvers\"\nNo need to put the class name in the description of a class, the name is obvious from the API. No need for this,. Can you make this unified with above references\nFor more information, see .... This one is good. So you wrote this thing? Just wondering whether we should put credits for someone else?. Prepend: \"Standard KNN solver. \". Cover tree solver. It uses cover trees .... KD-tree solver. I uses kd-trees to speed up..... LSH solver. . space between LSH and (. No that is fine, I was just checking on whether it was copied from somewhere.\nOk last question: there is no standard c++ function to do this thing without warning?. Maybe we can leave the cookbook out for now, and add scala there in a second attempt (we can just rename the tab to \"java/scala\" in fact). Rather change \"Java\" to \"Java/Scala\". why this?. this target doesnt make sense if there is not scala compiled code right?. cool! this works on your machine?. Actually, I think we don't need integration tests for scala, see other comment. OK! thanks for the explain. Finally, I changed my mind again, seeing how nicely travis executes the integration tests. So leave it in, it is redundant but I think its ok to have it. Cool thanks for the explain. This one can be removed. This is the tab title thing. You need to put just one line in the 5.1 here...no need to create a new release ;)\nAlso just say that we added Scala interface to the build, it's not like we added a new language .... Yep!!. Sentences start with capital letter. The second sentence misses a subject. I also dont understand what that is supposed to mean. remove \"the\". Maybe just remove. why this?\ndistances are symmetric. good catch!. I suggest rather than copy pasting all this code, you add a set_up method that prepares all the data and instances, and then each unit test just chooses the solver type and checks output. same here. is it a problem that CLock only has PTHREAD locking stuff?\nWe might need to add openmp, or is openmp locking implied by pthread?\nhttp://stackoverflow.com/questions/2396430/how-to-use-lock-in-openmp\nWhile openmp implementations are likely to be based on pthread, the specification does not talk about this --- I think we should better be safe than sorry\nhttp://stackoverflow.com/questions/5661830/using-lock-in-openmp-and-pthreads. I see, thanks!. Either put it on stack, or clear up memory afterwards. get_labels. get_feature_matrix. class. since you call this method in only one place, why not just paste it in here?. can you please carefully check which KNN solvers depend on LAPACK (i.e. the source code is guarded with this macro), and then only guard the corresponding unit tests?. @yorkerlin would be cool to get this stuff in in a nicer way.... I merged and updated the example, see examples on our website. Did you check whether any of the KNN classes (or includes) do call lapack?. You have to check the code, just removing warnings wont help. Even if it works on YOUR system, it might not for others. lets not do that. so I guess you added these lines for a reason... Not sure why you would remove them, as the only failing unit test related to this is the one you added in the feature branch..... I know this was there before but I just reaslied this will cause tons of warnings for gpu code being executed without a gpu present. I wonder whether that is good?. Please use C++11 here. . std::thread. why the rename?. Such refactoring should go into a separate mini PR please. fixing missing UNREFS should go to a separate mini patch that clearly shows whats going on.. missing null pointer check for cast. can you use SGVector here, then there is no need to free later on. seems convoluted. As above, please address issues one by one in smaller PRs. not necessary. please adress LAPACK guarding in a separate PR. adding unit tests should be different PRs than fixing memory leaks\n(unless the test you added covers a bug you fixed). this test data class (and its usage in the unit tests) should be a sep. refactoring PR . separate patch pls. please dont disable, but guard with the eigen3 version that makes it fail.\nPut a warning message similar to https://github.com/shogun-toolbox/shogun/blob/develop/src/shogun/CMakeLists.txt#L222 in the cmake configuration. this should be a sep. PR. why do we need a variable for the number of threads?. @vigsterkr I am confused here, wouldn't we use Parallel to get the number of threads?. @vigsterkr can you comment on the use of Parallel here?. I wonder whether the distance matrix computation should be part of CDistance and then we just call that method from here?. can you please do every class in a separate PR?. remove \"from above comparison\" -- redundant. BTW I did not have that in the sentence I suggested.. BTW the cell below needs update: \"Evaluate KNN with and without Cover Tree. This takes a few seconds:\"\n\"Evaluate KNN using all possible solvers. This takes a few seconds\"\nIt is also a bit weird that you go back and forth in dataset.\nYou should mention that you are going back to the initial dataset now. btw the legend of this plot is screwed up. Man, it doesn't make sense. Above you say that KD-trees are slower due to high dimensionality of the data, and now you run things on a low-dimensional dataset, KD-tree is still slower than cover tree.\nYou have to say something about this.\n\"We can see that the KD-tree solver now is faster than standard KNN on this 16-dimensional dataset. The cover tree solver is still fastest.\"\nOr a bit more elaborate, hope you get the point. @vigsterkr I just checked a bit, and most of shogun is not thread safe at all. So I wonder whether we really should care about this too much at this point in time. I mean we should definitely have a go on making operations thread safe (along with proper const-ing), but for now I feel that, yes, it is user responsibility. Even for GPU transfers.\nI mean we can still keep this patch here, but I think there should be some more systematic effort to make shogun base lib thread safe.. Yes data structures.. True.. fine with me this hack. ~~why renaming the variable?~~ Ah sorry I just realised..... I meant: there are six lines but only two items in the legend. I mean this fixes the error, but I find the line that is guarded incredibly discomforting tbh. So my thoughts are\n\nif this assertion could possibly ever fail, then there is some serious problem in the create_shallow_copy above.\nIf this assertion never fails, i.e. the above line guarantees something that is documented in its interface and maybe even unit tested, then why put it?. Does clone actually return a zero a one refcount?\nIf it is zero, it should be SG_REFed here as well, same for features. ok thanks!. I wonder whether the features of the machine are cloned here? I guess they will be. Then they will be overwritten with the manual clone below. Maybe it is good to remove any features from the CMachine instance before cloning to avoid doing this twice. Alternatively, set the features, clone machine, and then extract them from there.\nRequires some checking of the existing code. @vigsterkr we really should have some proper iterators for our structures ..... I think get_first_element also increases the ref-count?. Next patch: Don't clone the features :). Both yes!\nAnd actually the Shogun policy is to return new objects with ref-count 1. +1 this is cleaner!. I guess this comment can be removed now, as it is a bit nicer. I think this function should be named differently, as it is about cookbook pages, not meta examples themselves. good to put that comment. rename as said above. one thing to consider here: Some meta examples do not have cookbook pages. So they do not need to be excluded technically. Not sure whether that causes problems, but it is definitely cleaner to only work on the existing cookbook pages. don't capitalise words that are non-names, like \"averaged\". Don't explain what classification is ;) This kind of stuff is more for the notebooks / longer examples. This is supposed to be a minimal.\nThe averaged Perceptron is an online binary classifier.. Make it simpler: \"It is an extension of the standard Perceptron; it uses the averaged weight and bias.\". Given a vector :math:\\mathbf{x}, the predicted class :math:c is given by. Do you actually use the c anywhere? If not, remove the name altogether. Grammar error, this is either the same sentence, or a new one (no capital W)\n\nBetter:\n\"Here, w is the average weight vector and b is the average bias. and \\theta is a step function:. Perceptron. Remove \"secondly\". what kind of labels does it return? I guess BinaryLabels, should be that sgclass. We can also extract the average weight w and the bias b.\n(reference the letters introduced above), shorten (no \"now\"). Finally, we can evaluate the performance, e.g. using :sgclass:CAccuracyMeasure.. choose_class should accept SGVector as well. This should work on SGVector as well. chapter, not Chapter. BSD please, see more recent headers. did you valgrind check this for leaks?. is this all that is needed? If so this is a neat solution. we usually have no space before pointer . no whitespace before . this might well be. want to add a test? :dancing_men: . ah great, you are a star!. no whitespace before *. did you valgrind it?. No need for c here if you dont use it downstream. averaged weight. can this be automated, i.e. filled in via jinja as well?. can we load datasets instead of generating them?. I think the feature pointers should be the same for regression and classification. Not sure I like these globals..... what about a factory for labels, given the CMachine instance?\nFeatures are the same type, so no need to distinguish I think. superfluous comment. superfluous comment. I think you can just use equals, so no need to actually access the labels. All done via polymorphism. superfluous comment. I think you can use jinja to instantiate using the default constructor here\nThe new_sgserializable is quite ugly inside, so I would like to avoid using it. And you know the classname anyways here. no comments that are self-explained with the two following lines of code pls :). use CSGObject::equals. Why would this method not be in CFeatures\nIf it should be eventually, could you create an issue or a TODO item for this?. shallow_subset_copy?\nWhy not shallow_copy and if there is a subset, it is copied?. Not sure why we need this new class?. leaving: . (colon). leaving: \n(no equals). This message contains a bug, where is the %d populated\n(not your fault but you can fix on the fly here). \nno method name needed. Just \"Entering\". If you are keen, you could fix the punctuation on such messages: \nSG_DEBUG(\"Entering.\\n\");. SG_DEBUG(\"Leaving, other object is NULL.\\n\");. for the keen: Add a \".\" at the end of sentence. \"Entering\"\n(capital E and not \"evaluate\"). \"No machine provided.\\n\". \"No features provided.\\n\". This %s should stay in actually. \"Leaving.\\n\". \"Entering.\\n\". capital S and period at end if you are keen. no method name. \"No machine provided.\\n\". \"Number of positive labels (%d) must be positive.\\n\", pos_count. see above. We generally try to avoid method names in errors. This is not your fault here, but you could clean it up:\n\"Subsets need to be built before accessing them.\\n\". see above. no method name. provided error information, see https://github.com/shogun-toolbox/shogun/wiki/Assertions. Print the information of what is given and what is expected. same as above. not method name. no method name. no method name. no method name. Actually, I just realise: Just remove all the \"Entering\" and \"Leaving\" debug messages, they are pointless. No actually I realise now why this was done. Makes sense I think. \"shallow_subset\"-copy\nLets leave it for now. this debug message should go down in the else branch.. Or just remove it actually ..... Not a fan of this. Double memory usage.\nEither create a method that extract the subset matrix into a pre-allocated one, or copy things by hand here.. So now you changed behaviour: get_feature_matrix now always returns a copy?. Not sure about this. There has to be a getter for the matrix itself (REQUIRING that there is no subset).\nNote you can also be careful with const here. I would prefer is this method would not get involved with any re-allocation business (sep. of concerns), but rather only REQUIRE that the sizes match.. what does deep copy mean here? It is just a copy no?. Sorry you are right, of course it still returns ....\nOK so let's leave that for now (old behaviour is stil ugly but not your concern here, can do sep patch, maybe create an entrance task?). rename and merge then :). Ah no, the resizing/re-allocating, I think that shouldnt be there. ~~so it does need to be cloned in the end?~~\nMaybe there should be a comment on the clone call, you got an idea why it is needed?. this is a new call?. This shouldnt be called in the setter?. put a comment here: \"to avoid serialization of the data\". superfluous comment. these guards should not be hard coded, but rather come from the cmake setup. \nIn fact, this whole thing should not be done in a python script I think\n@vigsterkr will have comments on how to organise this. check our class_list thing -- it does something very similar. No need for a comment, just @return is fine. Question: Does it need to be virtual? I know the other one is, but are there cases where we want this? And ... const?. Number of rows of given matrix (%d) would be more inline the messages we have. But that is very very minor. \"null\" is not something that should appear in an error message that can be triggered by SWIG.\n. Same here, make it user understandable.\n\"Provided object's type (%s) at index (%d) in list must match own type (%s). entrance task: write iterators for our data structures ? Thoughts?. doxygen @param docs are missing. If this can be caused by SWIG interface (i.e. user), should not contain the word NULL. ASSERT_EQ\notherwise you can get a memory read error in the subsequent loop. ASSERT. whitespace changes to separate commit pls\nThe patch is very large already and this makes it harder to parse. why this? Also should print the number. I hope this stuff is portable?\n. pls dont use \"int\". ah yes good. These copy paste messages all need clean up, wont comment the other ones. What is this sentence supposed to mean?. style: we do { in a new line. style. maybe put the utility method and then make an entrance task for people to port other tests to using that. mmmh I am not too sure about progress bars in evaluation methods, this will take less 1s for every case that your memory can handle ;)\nAnd actually this should be vectorized, i.e. shouldnt even be a loop ;). These strings EVALUATING MUTUAL INFORMATION:\nCan't we use pretty function for that? Not so sure we should write this manually every time. Should be the CClassName::method_name or so\n@vigsterkr @lisitsyn . why this?. why do you rename?. seems ok for me to do it this way. typo in warning message. Yeah I mean like accuracy is just an average of a vector difference so there is no loop needed for doing that but we can just use a linalg or eigen3 call which will SIMD it. we usually dont do extra {} for single lines. superflous comment. if you do that, why not initialise it with 0 above rather than -1?. @return last time progress was registered\nI.e. needs doxygen and a human understandable description (this is API doc). pls split positive and negative into separate tests. tests should be as small as possible. same here, split. I think notebooks these days support markdown, i.e. you can do links as [link name](address) or even just plain test (check the github renderer for how it looks). No need for this ugly aref anymore\n. Now that we are here.\nCan you split every sentence you touch in the notebook into separate lines? The diff is hard to read otherwise.\nJust put a new line after every period.. I think markdown also supports mail adresses. Try and check. @lambday had something similar. Yeah it doesnt, just a tiny bit cleaner....your call. Sorry for being picky, but these things are never fixed otherwise.\nYou can just remove the text here, and only have the @ in a single line. A* :). yep exactly like this! . email link doesnt work in the github preview. Does it when you render it locally?\n. Hehe ;)\nWe gotta make everything as good as we possibly can -- chances that somebody touches code again are pretty low, so better get it correct (and neat) in the first place.\nHey are you in IRC at some point, I remember you asked about dependencies ..... minor, new line for {. this loop is never executed. (float64_t)0 can be just 0.0 But in fact that is not even necessary.\nWhat you want to use here is EXPECT_NEAR which compares floating point numbers, where you can set the epsilon to machine precision or so. same here, sorry I didnt spot those earler.\nCannot check floats for equality....it is not portable. yeah thats github preview. \nif it works locally with jupyter then all good. Sure! Didn't know about that guy actually. Yep!!. Need the full BSD license in here (I know it is annoying). Don't get the filename, why not just DynamicObjectArray?. superflous comment, just call it \"base_array\" (the name do_array doesnt help a lot). name it \"cloned\" and remove comment. superflous comment. use index_t. I would rather say why you are doing this here. It is not immediately obvious.. Maybe this should be \"Feature converters\". F. Wickelmaier. We need a better reference here. Maybe David Barbers book?. Abbreviate first names pls. watch you bibtex style, this needs a conference name (check the others to see how we name them). This cannot be done in the meta language.\nCould add an Evaluation class for this.\nBut actually, unit tests are supposed to test for such correctness (you want to add one in a different PR?)\nThe meta examples will store the real matrix, so this is already good as an integration test. Just remove the line. Please put every sentence in a new line. This is a new requirement we added that makes later diffs easier to read.. \"Strain_D\" is strange. If you want to use text in math mode, use \\text. Also, please dont define objects if you dont use them later. In the sums, please also put the largest number of the index in (^N or ^D). Transpose is ^\\top in latex.\nPlease double check the math and make sure every quantity is defined, and used somewhere.. Is this demonstrated in here? If so, say so, if not it can be removed. use sgclass for Euclidean distance here. a lot of whitespace here. move these to calculate_distances_before and calculate_distances_after\nNo need for its own section. This would need some docs.\nAnd a SWIG ignore\nalso @lambday and @vigsterkr should comment. no need for comment if the method is documented. same here. This is a dangerous change that might have a lot of consequences ..... needs full lincense. filename ..... same comments as in other PR also apply here. this needs to be a in a register_params method that is called from every constructor. Everything that is not registered will not be part of serialization and clone... is it fine with those here?\nWhat about initialization of the memory of them?. this can have its own issue .... \nWe dont have CPLEX anywhere so we cannot actually check. Would be good to update this at some point ....\n. Documentation needs the type of solver somewhere. Eigen has multiple ones which are at different speed and stability. Maybe an option?. this is not the correct conversion. Actually, sorry, it is in most cases.\nBut this needs checks and user readable error messages. unit test pls. space after if. no {} for single line commands. I suggest you initialize all registered values in here to default parameter. Otherwise problems are likely. you cannot use the initializer in here if you call register/init parameters after (works as it is now, but would be better to init everything in the registration method). same comments as above. superflous comments in source code, the variable names are self explaining. Or should be renamed so that they are. technically, whitespace changes should be in separate patches. But dont worry for now, just for future prs. Why not SG_ADD?. reuse in the line above. SGVector (I also realise now why you use add_vector)\nIf it is too much, you can also skip this. is this method even used anywhere?. after our discussion I think it would be best to have cloned objects have a different allocated sizes. But equals should still return true. Can be achieved by not registering the parameters and treating them in post loading?. This is ugly, rather remove ;). @lisitsyn suggested that this might work implicitly, i.e. without the explicit cast to SGMatrix, but with an implicit one.\nThe casting back to eigen should happen in the same line I think. We cannot possibly accept this. This will blow up at the next oppurtunity...\nIf this parameter should not be part of equals, it should not be registered (and made transient in the future). So again: \n * we don't register the size of allocated memory parameter\n * serializing/deserializing will change it. Storing only stores the used part of the array, loading only allocated as much as needed, and re-sizes when something is added next time\n * it is not part of equals as it is not registered. registered parameters need to get default parameters. Lots of problems incoming otherwise.\nIt is a different story for existing code where parameters were already registered, but as you are adding that here, you need to put default values for all of them in this method. Consequently, need to replace all those init fields in the constructor with explicit assignments. this fix is ok for me. Yes there really should be a patch for this soon. We want to offer the other variants in eigen3 as well. At least the more stable ones with pivoting.\nWe can do that via an enum for each solver type.....\nSee e.g. https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.linalg.qr.html\nThey also offer different methods. Yep thats ok. I guess we call these things \"converter\" internally.\nMaybe just leave it as is now, easy to change later. I would do a sep. enum for each solver. There are only a handful for QR and they wont grow in numbers. We can also copy the scipy interface and offer a boolean for pivoting..... and a string parameter kind=\"stable\" just for @vigsterkr . use \"d\" for dimension please. use \\left ( and \\right ). please dont put 3 in the text. It is already in the code.\nJust say \"We choose the order k\". aaaah, so much nicer :). think you need to rebase here ..... I would explicitly demostrate this in a second step in the example. First without and then with, mentioning it. license missing. I think this should be added in a sep. PR.\nAlso, why is this just for SVM? This is generally linearly separable data with binary labels..... pls dont put such nop comments. This function creates features and labels. (the kind of data should be described in the @brief doxygen field). superfluous comment. I have the feeling this test will memory error ..... can you check?. use apply_binary then you dont have to cast. this will fail if the gaussians overlap.\nRather check the accuracy or so .... no m prefix pls, this is not a member. no prefix\n. @lisitsyn: this is nuts! Storing the variables locally, obscures the code!!!. minor style comment. Put { to new line. I think the notebooks render links as clickable when you just put the link itself.\nFor cases where the link text and the link are the same, you could try that. To avoid having the string there twice. [http://github.com/karlnapf] should be without the square brackets. why put this in variable, cant we just use the getter?. all of those dont have to be in a variable, no?\n@lisitsyn we discussed this in irc. @lisitsyn @vigsterkr I guess we could also use ctags here, just as for the meta examples?. can you check the meta example include directory discovery mechanism we do using ctags. https://github.com/shogun-toolbox/shogun/blob/feature/cereal/examples/meta/CMakeLists.txt#L23. https://github.com/shogun-toolbox/shogun/blob/feature/cereal/examples/meta/generator/generate.py#L20. ? still?. you can put your name to the authors at the beginning of the file\nas well as changing the license to BSD (grep for BSD license in the source tree). minor style:\nWe do new lines for {\n. next (small) PR: transpose actually changes the matrix memory, which is horrible. adjoint is the method we want to use here. These needs minimal doxygen documentation saying that they alter the state (transformation matrix).\nAlso potentially could say that EVD is based on an eigendocoposition of the covariance matrix, while SVD does an SVD on the data matrix.. gnaaa. no need for .vector. docs!. muy bien. I meant there is no license text in the file, and you should add it (BSD) :). Yes, everything that is registered will be part of serialization and clone (and equals). LOL :). This is all horrible code\nThere should be no loops in the kernelPCA algorithm. not sure.\nJust realise how bad this kernel PCA thing is though.\nI mean it should just be a couple of linear in high level land, no?. Rather than transposing matrices, I think it is a nicer idea to set a flag.\nEither this can be added to the data structure (to the new linalg::Matrix, not the SGMatrix), or the linalg methods can have flags that take into account the transpose.\nCheck out eigen3 adjoint vs transpose. Always print the information that is there.\nMatrix A (%dx%d) is not square. @vigsterkr (and everyone) I feel like we should put all those checks into functions so that we can unify their formatting.\nAny thoughts on that @micmn ?. return type is void?. man this code ... :D. and all those tests match sklearn you say?. You can actually put a link to this gist as a comment in the c++ unit test. You can do SGVector<float64_t>(result.matrix, m_target_dim, false);\nBUT I am not sure you want to do this. Who is responsible for this memory?. Actually, I don't understand what you are after here? Make a SGVector from SGMatrix? I think there is a constructor for this....\nWhat do you mean by \"data gets allocated\"?. can we put this \"generate filename\" into a helper function please?. remove such comments, they are redundant (code speaks for itself). please name this test better \"serializable_array\" is not a good description. first assert that the number of elements are the same, otherwise this might segfault\nUse a variable for that (not just constant 5 in the code above). There is a few places where we use temporary filenames. All of those should use the same helper function. In general, I would name the test after the function I test, here save_serializable. minor style: why is this a variable? No need imho. API docs please. Not sure these edits belong into this patch. why does this number change?. this will fail sporadically (even if rare). this should be indeed removed. Caller responsibility. camel case?. camel case?. another sporadic break. this varialble name should be \"num_data\". members should have \"m_\" prefix. this variable could get a better name that explains what it means\n. this is cryptic, rather untangle into multiple lines, but more importantly, structure it so it is clear what it is doing. can you make this loop readable? Maybe split it into two?\nAlso it is entirely unclear how the classes relate to the data\ni,j should be called something like \"counter_pos\" \"counter_neg\"\n. comment redundant due to next line. and put all those integer divisions into varialbes that have names. same as above, this is very cryptic. biggest question here: how do the labels correspond to the data? Not obvious from the code..... cool! That is indeed the easiest solution for what we have in mind\nThere is this issue:\n * Imagine you have some rather cheap operations (small dim add)\n * imagine you call this all the time, i.e. in a loop\n * Then the switch might have some run-time drawbacks\nThere is two competing mindsets here:\n * We can say \"we are not in rush to compute\" (@lisitsyn) and argue that looped linalg calls should rather be put into a vectorized linalg method that doesnt do the switch\n * We can add a mechanism that kind of \"tells\" linalg that the next calls are all say float64_t (and the switch is ignored)\n * the above could work via registering pointers to the implementations\nThese have to be traded off. I rather tend to the first solution. I guess a key headache here is mixed types (we probably wont allow them right?). I think this would be the existing impl methods we have in linalg rather than new ones?. ace interface. Type free algo implementations :dancer: . yeah nice to do a typed test.\n(offtopic: this should be done in lots of other places in shogun tests as well). I like the minimal interface.\nThere should be in and out wrappers for SG of course (later). yeah much nicer to parse ctag ouput rather than source code.\nOne question, isnt there a python package that does things like you do here?. no I mean that transposing a matrix (i.e. changing the order in memory) is often not necessary, but one can just mark them as transposed and the subsequent operation realises that\nNothing to do with complex adjoints. Is this an overloaded method? It doesnt make sense indeed....in particular since the code can be shared no? (The kernel matrix is all we need). Maybe this is actually a copy-paste error? :D. is there anything that catches numerical errors in here?. Shall we move CMath to linalg as well?. this API is a bit messy maybe? Reminds me of cblas ;)\n. this loop could be in linalg I guess?. put into linalg?. I think we should from now on avoid any fix data creation code in unit tests, and always move it to function, fixture, or whatever. What do you think?. on stack?. this doesnt leak?. A batch label conversion method, so that there is no loop in here, would be nicer. You could make this more readable via pulling the m_method == AUTO_LDA && num_vec <= num_feat out into a variable and call it lda_more_efficient . whats the 1?. please no loop. Can we put the two branches of this if else branch into methods?\nMakes the whole thing less bulky. idx_pos is a better name imho. this might give a numerical error?. formatting out of controll.... ;). this comment should maybe go to the @brief class API docs. remove this line ?. are you carefully thinking about the memory footprint here?. please no todos :). no todo pls. is this design fixed in the way that no matter what the input features are, Q is float64? I am ok, just asking. maybe can have a method count_threshold_exceed(T thresh, bool upper)?. @vigsterkr @micmn we could have the new Matrix class have a boolean flag that denotes transpose, so that these ugly true/false parameters of the matrix_prod call are not there? Thoughts?. no todo pls. todo. Yes, add it. Need to tell the SGVector to not free memory at the end though, and add a clear warning in the API doc that it is non-owning. +1 for the lazy, though quite a bit of API work needed for that. Linalg currently cannot do that.\nSo whats the best way forward right now in here?. yes. yep, a broadcasting like add. Please  use that method but linalg.\nI think a self adjoint eigensilver is in there now\nMichele will know or can help. Same here. This will make buildbot fail. \nNeed to update the infra repo as well. RLang. Can we please name those \"INTERFACE_PYTHON\" etc, since we are touching them already.\nThere is an issue for that somewhere. and that is more inline with the other switches, they are also grouped when ordered alphabetically and autocomplete is easier. why is a new object created here? Rather than using m_rng ... just asking. why would that 12345 be there?. what is the thinking here? Not necessary?. this seems a bit weird to me, can you explain?.  * none of the links work\n * kernel width has to be chosen to something. 1 would be a better choice than 0.25 I agree arbitrary values are bad style\n * I don't like the default seed parameter at all\n * also, why not let the seed be arbitrary when not specified? And then some of the logic here has to be rewritten to work with a local rng. @lambday can you advide here?. I think this is not needed. smart pointer?. I have to say, these are very nice test to ensure correctness of randomness :D. why not use this above as well?. why change the seed?. it is not* please revert this. see above. superflous comment, and pls remove this. of course, thx!. accurate what?. maybe a better name? that explains what is accurate?. You with a capital Y\nAlso, better to ommit \"you\" in error messages. is this all safe?. same question as above. What happens when this is the case? How is the user informed? Error message? Or automatic fallback?. is this described in the \"@brief\"?. nice!\n@MikeLing this is what I had in mind as one of the things in label refactoring. Capital C in constructor. Also no need to say that it is constructor, so rather remove. The same thing could be done for other solvers, such as cholesky (LLT, LDLT, ...). no newline  \\n after first sentence. General question: Is this multiclass LDA? If so, why does it assert binary labels?\nI think there should be no distinction (class wise) between binary and multiclass LDA. the indexing here seems a bit over-complicated. @micmn thanks for doing that.\nBTW we should remove such code from Shogun, allocation of memory, memset, all that. We have more high level constructs that do all this\n@MikeLing might be a nice one for you to change such code to using SGVector. Can we please have these type of license headers shogun wide? Rather than that long and ugly bulky text in every file?\n@iglesias @lisitsyn @vigsterkr . cant this loop be linalg?. This could also be linalg.\nGenerally, I wonder (offtopic) whether we shouldnt have some Covariance class that has an interface to compute incrementally and in batch version. And then have some kind of iterator access in situations like this one.\nThoughts?. I think there were some problems with that, but maybe I am wrong?. I think quite a few other projects do this no?. What happens if you put the get_mean() output into a local variable first? It is a bit nicer to read anyways. Well done!!! :). I dont think this change is OK.\n0.0 essentially means that the test always works\n0.8 means it almost never works.\nWhy would that come from a change of seed?. The same goes for all the others.. can you please refactor these two methods as well?. auto please. doesnt linalg have an in-place add? That would be cleaner here. this seems to be a matrix product, and it would be better to use no loop here. matrix multiplication!. If you re-run this a couple of times locally without a fixed seed, does the p-value fluctuate a lot?. @vigsterkr actually, this might be ok, since index_t num_null_samples=10;\nThis means that the test is repeated only 10 times, so there probably is a huge variability in the results. But also something might be screwed up in the setting of the folds etc.\n@lambday can you check this please. yeah I understand .....\nI mean this test is YOLO anyways....\nIll try to catch rahul to see what we can do here. Not sure I follow. Why would the p value be ever different when the seed is fixed?\nIn that case it seems this is just an execution and there should be now result assertion no?\nI also don't understand the code comment (local machine result)\nI mean is this just a unit test where you put the result of running the to be tested method in???. I suggest to remove the assertion (or even the whole test). I would remove this -----. can you give an example output of this?. needs doxygen @param. Wrong grammar (minor) This is a parameter, not a method. as said, we don't need to distinguish per fold but rather per run. Don't think this \"there will be many\" is instructive. Rather name them self-exlaining. This is the job of an integration test, which we have. So I suggest to just drop these tests... Would you mind marking them? So @MikeLing can drop. Yes absolutely!\nWhat about creating a fixture class with a very simple and small fixed dataset, no randomness.\nAnd then compute all the statistics and assert the results. Then put a link to a python notebook in there for reference. I did something like that here: https://github.com/karlnapf/shogun/blob/feature/kernel_exp_family/tests/unit/distribution/kernel_exp_family/impl/Nystrom_unittest.cc\nand here\nhttps://github.com/karlnapf/shogun/blob/feature/kernel_exp_family/tests/unit/distribution/kernel_exp_family/impl/DataFixture.h\nAs for the permutation test p-values. I would unit test the mechanics independently of ranodm data (if we have the statistics to be covered, we only need to test that the permutation test compares/computes the right things, i.e. test indices)\nI think this would really benefit the maintainability of the framework!. nevermind for now.\nLet's move on with this. What is missing?. First patch. Add meta example integraiton tests (minimal!) for all the unit test cases here. Then remove the unit tests. can you provide some insight here? :). cool!. Usually, solvers that compute few eigenvalues are slower than those computing all of them.\nAre two different solvers used when kREQUIRE(idx < get_num_feature_obj() && idx>=0, \"Feature index (%d) must be within [%d, %d]\", idx, 0, get_num_feature_obj()-1);. I think this is bad, an error should be thrown if the index is bad (either from the array, or by this class). \nAlso if you do bound checks, also assert for >=0. can you explain this?. maybe that link should go into the code? But doesnt have to. yes!. I would make this a REQUIRE. There is the thing that kernel machines only need to store the data which has non-zero support vector (alpha coefficients). The method store_model_features sub-samples the training data and then just keeps the relevant ones.\nThis then makes the kernel machine, when serialized, store less data.\nI think a fixture test that does the serialization with and without that call might be useful here, as it is for example used in cross-validation\nWhat do you think @micmn @vigsterkr . Ah I see.\nThe old code does this horrible thing ;)\nIn that case, I think it is OK to do this here. (Though a patch that cleans this is always welcome). Mmmmmh. I don't think that is a good API to offer.\nIf you compute all of them anyways, you can just return all, and then the caller picks the ones he wants.\nThere is also iterative methods that are much faster when only computing a few, and those are the ones that should get this API here, with a warning label (\"slower for computing all\"). I see ...\nBut the test is not very meaningful then, as KRR's point is to use less than all.\nMaybe change the default to something like \"half of the data\" if m=0?. Exactly! This also doesnt dump all of the data (for some kernel models at least)\nAce! For GP the API is not that stable, you can test regression classification, but also make sure to check the variance things that the GP can produce..... Yes! Ace!. first last I would call col_start and col_end. Exactly that I had in mind. REQUIRE pls or no bound checking at all. Actually, I think this can even stay like this.\nTherefore, remove the TODO. should be else if. for classification you can use laplace, for regression maybe use exact\nbut the other approximations should also be tested, FITC, etc. See the notebooks. I think GP probably deserve their own treatment as the are so modular ?. Minor: We try to use single lines per sentence. make sure your text editor doesnt hide formatting issues from you. In fact, one of the checks we run is a style checker, see its output how to convert the files to the right formatting and how to run locally. it somehow annoys me that we treat binary and multiclass differently when the former is just a special case of the latter\nNot your fault though, just generally a thing that we should think about @MikeLing @geektoni @micmn @vigsterkr . ace!. not sure why this needs to be const, but ok. you can use the C++ keyword auto' pretty much everywhere where you store a variable that is returned from a function. mention that no combination rules are yet applied. try to avoid such whitespace changes in the future, but nevermind for now. very nice improvement!. please no commented out lines in PRs :). if you usesomein the unit tests, then you don't have to UNREF at the end. can't this first bit also be put into a common place. e.g. hierarchy of fixtures? so it is not replicated below?. why?. function (for the people that add tests after you). or import from other file?. why pointer?. in other words, please let this guy live on stack. formatting .... this will leak, you have to UNREF or make this a some. This reminds me.\nPlease test your new unit tests using valgrind to check for memory errors\nThe dev readme has sections on thatvalgrind ctest -R unit-test_name*. check whether this method is supposed to increase the ref counter for the returned object or if the caller is supposed to do that?. this cast is not safe, so you need to store the casted pointer in a variable and then do aREQUIRE(labels_multiclass, \"Labels are not multiclass\\n\");or even better, try to convert them to multiclass (there are constructors to e.g. turn a binary labels into multiclass), and then say \"Could not convert labels to multiclass\". formatting!\nUse the style checker in travis to get the command that converts all your changes to the correct formatting, or do it manually. \"load\" not an appropriate name here. Rather \"generate_toy_data\". auto. such assertions should appear in the test name (if you write the test yourself)\ni.e. output_mutliclass_probs_sum_to_one. name change suggestion: just \"submatrix\". (maybe even rename the existing \"get_subvector\" for consistency. But nevermind for now. minor: \"mockData\"->\"mock_data\". mock_data. this compiles?\n\"Some\"->\"some\". This comment confuses me. I understand that these classes are ignored.\nBut I wonder whether they are tested elsewhere, what custom initialization means, and the C<=0 error seems to be a bug that should be referenced with a github issue?. comments should probably be put begind the lines with the individual classes below rather than all stacked up on top. I am confused!.srcis not a good name, a better one would bescores. somehow cleaner is:SGVector labels(src.num_rows). or confidences. this is not necessarily a probability matrix right? Can be arbritary scores. You could say from a matrix that contains scores for each class for each point. Those could be probabilities but do not have to be normalized. good!. you could putASSERThere for the pointer not being NULL, and for the size of the vectors to be both equal to N. as said, this dont have to be probabilities. can you check whether and where this constructor is called. You are changing behaviour of existing code here, so need to account for that. You should use it if you created an object that is not passed to any other method.\nIf you create sayCBinaryLabelsand then pass it to a methodset_labelsthis will increase the ref-counter by one, and the object that you calledset_labelson will later on decrease the counter, which means the object will be deleted.\nIf you never pass it around, or increased the counter yourself, you need to unref later. If you don't see leaks, then nothing to worry. Did you write this guy from scratch?. where do these numbers come from?. where do these numbers come from?. generate_nm_data is not a good name for a function. It doesnt tell me what \"nm\" means. where do these numbers come from?. where does this number come from?. why single threaded?. again: where do these come from?. ok!. minor: missing period at end of sentence.\nBetter error messageREQUIRE(labels_multiclass, \"Labels (%s) are not compatible with multiclass.\\n\", m_labels->get_name());(This needs aREQUIRE(m_labels, \"Labels not set.\\n\");to avoid segfaults for NULL labels. this should be index_t (or auto). auto i. you can also userangehere, see range.h. pls dont fix the seed in here, this method is not random at all. But you actually DID write this particular test here, no?\n. so I guess these come from your manually created function and this just asserts that the RF is working correctly, right?. dont think you need a seed for this test. not sure I get what this test does.\nAlso it needs a better (self-explaining) name. yeah this double free thing doesnt seem right. this will increase the counter definitely. think we need an unref here. I think also maybe this could be anautoor asomerather than using the member. why isnt this set from the other's active subset?. much more explicit!. not sure this is needed, but ok. wait a sec, is this function now defined multiple times?. better to reference an explicit commit hash in the link (master will change). Also can you put a gist link?. defined this next to the data so it is clear that it is coming from the gist. Think twice about this check11e-1==1.1 ` which is basically passing for any returned probability.\nYou will need to explore the output a bit here to make it consistent. It might be that the number of trees will have to be larger for this to stabilize ..... dont think we should set a seed here. thats better!\nBut I suggest in fact to\n\ngenerate the data randomly (CMath::randn_double)\ngenerate the labels randomly, sign(randn())\nincrease the dataset size\n\nassert for 0.5. Put the function in a shared file and then include it. Ah the data is fixed, sorry, then just leave it. Ok 0.1 is acceptable.\nIf you run this 100 times, how many times does it fail?\nYou can do this quickly via some bash magic, something like\nfor i in {1..100}; do ./shogun-unit-test --gtest_filter=TestName; done. thx!. No wait, I am confused here. What do you mean by addressed in a next pr?. We don't add these type of examples anymore, if it is possible to express the functionality in the meta examples\nSee here how to write one. I don't understand this doc, can you make it a bit more explicit?. Can you mention the types of parameters that have to be set here, and maybe give an example of a splitting?. Please start docs with capital letters (they are sentences)\neven if this is not the case in other parts of shogun. isn't this inherited from the base class? If not, it should be I think\n@vigsterkr . Please use proper punctuation and proper English. please dont use default initialization but do that in an init method\nAlso, h is not a good member name. Name it so that it is clear what it does. This should be done in the base class. can you introduce a method init and default initialise members in there? Thats shogun style.\nIf you do that, you cannot use initialization fields anymore but have to overwrite num_subsets and labels after calling init. this for loop needs some serious demangling. The message should be \"Number XXX (%d) has to be at least %d\". make this a REQUIRE. giving warnings and then taking implicit decisions (h=1) is harmful. this unit test is way too long\nCan you split it into multiple ones?. Also each of the unit tests should have a name that explains what is tested. superfluous comment. all of those comments are superflous. this method is a bit messy. I guess this can be done a bit cleaner?. mmmh this cannot be translated into a meta example....\nMaybe we can add an overly simplified one at least?\nThis one should go into the notebook on x-validation, we want to get rid of all language specific examples eventually. ?. @micmn wanna do some magic to create a static cast method for all Shogun classes?\nSeems like you would know how to do that fast and then we could delete all the manual ones (and increase coverage) Thoughts?\n@vigsterkr . should be inside one of the tests rather than in the outer file. fixture class for test data pls, this is redundant code. I guess \"get_observations\" would be more appropriate?. ASSERT(run);. \"build_subsets()\" is not defined? How is that related to the rng?\nAnyways, would you mind moving this to the base class and inheriting from there?. Any updates? :). yes. I am not sure I understand what you mean here. I definitely had TEST and TEST_F in the same file before ...\nAlso not very pretty :/\n. We usually don't put link to external notebooks in ours. \nMaybe we can either remove this, or merge some parts of it?. Notebook is long, what about a TOC?\nSee the GP notebooks. is this how we do the data path these days?. pls comment here that you are using a pre-written sql statement that is long and messy :). pls use a BSD header, see e.g. https://github.com/shogun-toolbox/shogun/blob/develop/src/shogun/statistical_testing/BTestMMD.h. (in all the files). all of this can be done in a vectorised fashion using linalg, see e.g. https://github.com/shogun-toolbox/shogun/blob/develop/src/shogun/preprocessor/KernelPCA.cpp#L92. I am also confused, why is this necessary?. what is LAPACK needed for here?. rather than writing unit tests here, a good sanity check would be to reproduce the plots in http://scikit-learn.org/stable/_images/sphx_glr_plot_calibration_curve_002.png\nYou could do that via say Python bindings. Once those work, then we can start adding more unit tests ...\nLet me know what you think. I suggest you use setUp to set up your cross estimator, and then have multiple tests that test different characteristics.\n\n\njust calling pred (without asserting anything)\n\nthe sum probability thing you did\nother things that are meaningful. also we can auto that. I would still start the loop at 0 for readability. You can still get the first element to do that.\nAnd then you should ensure that all of the features have the same number of elements. Wait, isnt that just get_num_vectors ?. yep just remove. Whats up with these changes?. nice! :). ah of course!!. great thanks!\n\nOne thing. If one of the provided pointers is NULL ,this segfaults.\nSo you additionally need something like this for both arguments\nREQUIRE(predicted, \"No predicted labels provided\\n.\"). Also, you are missing a \"\\n\" at the end of the error msg. ground_truth -> ground truth. ground_truth -> ground truth. ok well it shouldnt .... lol\nwell ... anyways :dancer: . party. could be moved outside the if/then/else. :D. I understand we don't need this anymore? Is that why it is removed?. Minor: missing white space, \"Default\" should be lowercase. Minor: Capital \"C\". I think this class should not accept labels ... labels are not needed to build the splitting. Could we talk about \"folds\" and \"fold size\" here, rather than subsets?\nAlso, I think this method should be called different \"num_steps_future\" for example. Can we please move this one into the base class?. Could you create a mock class here, or at least a function that generates all the code that is shared across the different tests? See e.g. tests/unit/FisherLDA_unittest.cc on how to do that. please use a for loop\nfor (auto i=0; i<runs; i++)\nor even better\n(auto i : Range(3, 10)), see base/range.h. minor \"at least\". superflous comment, clear from the code, remove. I am missing a test where you do things/checks by hand\n\nmanually specify some simple data\nmanually explicitly check the folds (since you know the data)\nreally small example, yet very useful. again, all this should be moved to some helper function. the test is bloated and hard to read right now. this is not what you check. You check that the vector is sorted.\nI am not too keen on this test. rather, you should check that every element in the training set is smaller than every element in the test set.\nThe manual test mentioned above would check that in a much simpler fashion, so I would just remove this one here. why not put this into a variable to make things cleaner?. missing newline \"\\n\" at end of message. are you sure this never suffers from integer division problems?. Can you make this docs a bit more intuitive.\n\nSee https://github.com/scikit-learn/scikit-learn/blob/ea6ea815e1071bfb9de18953fea98b136a7fa8ed/sklearn/model_selection/_split.py#L676 for a very good example. why this? No need. this reminds me. We currently have two maps: one for registering all paramters, one for modelselection available only.\nI suggest we don't keep this design with tags, but rather have one and somehow mark them otherwise ?. Yep I agree. I think this is a place to include the domain bounds, information about gradients, sampling callbacks (?), priors, etc\n. That will absolutely do it.\nBUT, as I said, I think the best thing would be to write down an explicit case example that you can solve on paper.... @lisitsyn let me know if this is the right place to add those checks. you could do auto gaussian = some<CGaussian>(mean, cov, FULL). no need for this comment. no need for comment. float64 is 16 digits, not just 8. i would rather compute the mean by hand and check whether it is the same here.. see comments above, also hold here. you can put your name here :). good? not good?. good? not good?. gotcha!. good? not good?. all the magic happens in this single line, awesome!. I would actually like to print the differences here (also for vectors which is currently shown as ...), as that was useful in unit-testing in the past. @lisitsyn I had to disable this until we fix the registration of that partly used array parameter. @vigsterkr this is fucked up, but the way it was before was even more so. We should just get rid of all of this accuracy business and rather fix the serialization, which should not be lossy (in my eyes). @vigsterkr here the beauty is. here it comes .... @lisitsyn @vigsterkr I disabled this for now. With this patch, DynamicObjectArray is not serializable anymore. We need to fix the registration of the underlying array (not all of it, only used parts). I'd suggest future_offset  but that is minor. This is a tag for the cookbooks, not free form comment. So pls make it super short (see other examples). Missing newline. auto k : Range(indices.vlen) but it is minor. max_future_steps\nor better max_future_offset. pls remove the (subset_inverse) here. This is documented in the base class\nThe newlines are awkwards, could you remove them?. in latex math, you don't use * for multiplication but \\cdot or just nothing. labels = some(num_labels); and then you dont have to SG_UNREF later. yes this is a better test in fact!. why do you need a fixed seed when you test for the estimation? Should be ok everytime no?. for auto i : range(sample_size) ... minor. I dont like this variable names. What about something meaningful? mean and covariance?. mu and Sigma?. or \"mean_est\" or so. @vigsterkr don't we have ways to fill arrays with random gaussian numbers?. you are right, this should not be in here\nI think there are linalg methods. Oh wow so this happened? :dancing_men: . you might want to add you own name here :). name. Is Calibration the best name? Could be ambiguous... CalibratedClassifier? . This class needs a @brief description to tell users what it is doing\n * it is a meta class that wraps a classifier\n * it performs x-validation to...\n * ... calibrate the probability outputs\n * this is expensive\n * what method is used to calibrate\n * etc. Description!. For all comments: start English sentences with an upper case, and end them with a period i.e. \"Constructor.\". I find it a bit weird that both CCalibration and CCalibrationMethod both inherit from CMachine. this should be documented in the class docs, i.e. this is for binary problems\nIf that is the case, you should probably also change the name BinaryCalibration or something. another CMachine inherited class.... not a fan of that. In my opinon, this is the only machine class you need. This one is the wrapper for CMachine and this one takes a machine and hides it behind another CMachine interface, wraps the calls, does x.validation etc. this is still needed. I think we have something like SigmoidParameters. After API is frozen. This still needs to be done. pls dont copy code comments around. this replicates the data as well .... slow. Just realised we really don't want to copy/replicate this buggy x-validation codes .... auto i : range(num_vectors). needs doxygen docs. auto...range.... thx for spotting. lol ... thx :). good!. I think these are incredibly useful.\nCan't you move the implementation to the .cpp and keep them?. cool thx for that. Can use it to write the big clone method. we are slowly getting quite complex with this stuff .... :D. ++. I think we are missing SG*.clone usage, since that cannot be assigned by value.. pls stick to the style we use. auto clustering = some<CGMM>(num_components, FULL) and then you dont need to SG_UNREF anything later. where do these numbers come from?. didn't know we had that, cool!. @lisitsyn this is what I meant in my other comment. This assertion here fails, because the pointer is the same. This fails as well. this does not fail since ->clone was called. I'll do it :). good point. #4083 . I see.\nWhile this is ok-ish, even better would be to compute them somehow otherwise. How do we know it was correct before. And actually for self consistency of results, we have the integration tests. So I would suggest to remove this unit test, and add a meta example +test data on GMM first (see developing and examples readme), and then do the change you did\nAlternatively, we could test against sklearn, pen&paper, or something that ensures absolute correctness in some way. good!\nMaybe we should also offer an API for just eigenvalues?\nBut that could be done later. we want to keep that, there might be other examples. please keep the block, so we can add things there that pop up in the future, just don't append anything. ah so this guard was just outdated?\nI think we should use linalg rather than eigen3 directly inside though. we also need a test for the pointer comparison. Sorry I didnt spot that before. leave it as it is for now (you can already change the rest)\nThen you could create this method in linalg, and then send another PR\n. Or, you could just add a loop for now.. could you do this test more explicit, i.e. just write down the matrices by hand (no loops)\nOtherwise good.\nAlso make sure to use EXPECT_THROW for the case of invalid input arugments (the REQUIRE)\nFinally, test some corner cases (empty matrix doesnt give memory error, etc). I guess I would call this add_diag rather than add_diag_vec. typo. Although I really like that you though of this case, I think this is a test for the copy constructor and not for ==.\nWould remove it. this one should probably contain Vojtech Franc.\nTricky situation as he is not in the git log. Can you add him manually?. Vojtech. vojtech. vojtech. vojtech. Either use Std::vector,or (since you know the size in advance) sgvector. I would like to not see these changes in the PR. They are unrelated.. This is an example of a change that should be in the other PR, as it changes usage of the rational approx framework. you have some serious git issues in here ;)\nNeed to read on how to rebase and how all of git works really. Never commit merge conflicts into a PR. this can come in a second patch. great that you use omp here.\nPull this out into the separate patch then I can review it smoothly. what is the purpose of this contraption?. use CParallel. is this method thread-safe (const)?. thread safety. definitely not thread safe. But if you use a vector as mentioned above, this should be ok. no need to have that comment. don't think you need barriers. The loop is simple and you can just use #pragma omp parallel. why don't you remove the aggregators variable and directly store in samples?. there is so much overlapping code here. Could we maybe refactor this to avoid that?\n@lambday what are your thoughts here?. No worries at all. Next time, just make sure to have a look at the diff of your PR in github yourself before you press \"send\".. I think a better would be to look at something like this:\nhttps://github.com/shogun-toolbox/shogun/blob/develop/src/shogun/regression/LeastAngleRegression.cpp#L251. typo: identity. needs to be specialized for floats I guess?. In fact need to check whether the elements support \".equals\" or \"->equals\". this is pretty cool!. every raw array, which was previously registered with add_vector should also be registered with watch_param. which dynamic array?. typo. use reduce here as well. type \"Then\" should be \"then\". you have a lot of whitespace changes not sure whether that is due to the auto-formatter.\nIn any case, search for \"style\" in the first travis job output to get the comment how to fix formatting. remove this comment, doesnt add any valu\nalso, you can just use linalg::dot. compute?\nAlso, make sure to start the comment with a capital M. \"Method to ...\". in all error messages that you touch, remove the ClassName::method_name, this is outdated. \"Purely virtual method that ...\". \"Method that solves ...\". or better \"computes\" and call it compute. \"* Method that computes ....\". maybe this can be parallell as well?\nor is it inside the other parallel loop?. maybe parallel as well?\nGenerally, I would put the omp into the outmost loop. I think the eigen3 includes can go once you use linalg. could you use nullptr here?. Delete this, not needed. is this exposed to swig?. I think you don't need to change anything here.. remove this for now. (the class will be rewritten soon). I don't think this can work, these static casts don't make any sense\nCould you remove you added watch_param statements here?\nThen, in another PR, could you even comment out the add_vector calls?. don't put those comments in, they don't add information :)\n(as I said, we can postpone the openmp into a new PR to make things easier to handle if you wanted to). it seems you are not changing anything here ... don't understand why the test fails.. you will have to carefully backtrace the changes.\nIf staring at the code doesnt help, you can run the two implementations next to each other in a debugger and ensure that the same number is added in each step. Ping me if you need help with this, I am in IRC for another hour. there are changes from another PR, you need to rebase.\nAlways check the diff !. Actually, we just realised that we probably want to keep the comment as well.\nBut that requires a minor change to the registration method .... Let me cook something up and ping you. This replaces the old clone equals tests. Can obviously also be done for the serialization tests.. I think you shouldn\u2019t clear the subsets here.\nThis should happen from outside (and only the last one) as otherwise you might remove stuff a user has set previously. Most things in there were written in demand if one specific use case, so probably no.\nI think everything should be consistent between normal kernels and custom kernels.\nSo maybe we keep it like this.\nMemory check is important before merging though. Sorry. I meant do it the same way for custom kernels as for normal kernels (clearing the stack). Not sure about this capitalisation underscore mix. Missing newline. What size? Should tell the user . I think I would prefer if you put explicit values in here, 0,1,2,3,4,.... \nAnd then later in the notebook, you can extract feature vectors/matrices get_feature_vector(0) etc. add some extraction illustration, i.e. how to get the vectors back from Shogun. same comments as above. Filename should be just \"features\". Make this: \"Passing data to Shogun\". Suggestion: \"Note that data is stored in column major format, i.e. each column of the matrix corresponds to an observation / feature vector, where each vector consists of a number of variables that is equal to the number of rows of the matrix.\"\nI.e. add the wikipedia link for explanation and for the feature vector. And remove the other sentences. This is a good point. Maybe it should be phrased \"Note that features are type-safe, e.g. you can only pass 64 bit floating point numbers to RealFeatures.. good, also put the column major thing. I in fact suggest to remove the vector example. why two kernels?. Real example. why is this in here? I think this test is simply about the uneven ratios no?\n. I would just put EXPECT_NO_THROW around this and then don't do more.. @guruhegde  I have one more request to keep this cleaner:\nCan you make the second two parameter of the AnyParameterProperties constructor have default values set to false.\nAnd then don't put MS_NOT_AVAILABLE every time, but only the descriptions .... This way, things will look way more neat. same here, pls let's not have this MS_NOT_AVAILABLE, GRADIENT_NOT_AVAILABLE everytime, but instead hide it, and only write it out if things are available. could you send a PR to data with the integration testing data for this example? And then update this PR with that new data revision? . if you require this, you could have asked for those types in the signature of the method, no?. we actually don't do \"CLASSNAME::METHODNAME():\" anymore. this segfaults if the caller passes nullptr. why dynamic cast if you don't check whether it worked?. usually, we try to not have comments like this, a bit superflous, the code tells the story :). same as above. and same again. this is a good comment. this can go. if you use auto gen = some<CMeanShiftDataGenerator>(0,2);, then you dont need to SG_UNREF later on. EXPECT_NEAR would be much cleaner. why not put the 10 into a variable with a meaninful name?. better name: feats_combined  (good names avoid the need for code comments). these names are confusing me. I think SGMatrix has an equals method. I think we should have a constructor that only takes the description and sets the other two to false (they are not used most of the time). typo: \"tht\". also remove \"upper or lower\", just say \"triangular LDLT (default: lower)\". Should technically write: \"Matrix dimensions (%dx%d) are not square.\\n\". Do a separate PR since that is very small and minimal. Then rebase this one here. Can't we make this a bit nicer here, so that any number of things that have the .on_gpu(), .on_cpu() method implemented is checked?\n@lisitsyn can you do some magic ?. We could add a @see factor (or however it is called) here. i wonder why there is \"SGMatrix\" in the test name?. \"Distance not set.\\n\". same as above. give the numbers:\n\"Number of training data (%d) does not match number of labels (%d)\\n\". \"Provided training labels are empty\". \"Number of classes is zero\" is not helpful here.\nI would rather say \"Machine is not trained\". \"Distance not set.\\n\". same as above. use a consistent style for things not being set\n\"no X provided\"\n\"X not set\"\nor something like this. But make it the same everywhere. Say what is expected rather than what you got\n\"Number of labels (%d) must be at least K (%d)\". no need for this. ah, I had overseen this.\nCan you pls send another PR where you  move the perceptron to the \"binary\" folder, it does not really follow the NN structure in Shogun. Same for the data file.\nAlso could you remove the python example in the same PR?. let's name the file consistently with the others... just perceptron.sg (and same for the data). @lisitsyn we need some systematic and compact way to do these checks. The amount of lines of code for such error messages is crazy. I would prefer a test that is more self explaining.\nLike you do\nx = A*b\nthen you solve b2= A^-1 * x\nand then you check that b==b2\nBut this one is OK as well. BTW why this reformatting?\nPls don't put this into a PR that changes small bits as this one. Makes everything much harder to review. nice much cleaner. why did you change the indentation? Is this the style cheker?. We changed the license header recently, to something much shorter. Check other files for the template .... just call it: cross_validation_support_vector_machine.sg. this can be in a single line. no need as this is not a kernel machine? Or does it produce warnings?. evaluation_criterion. the comments are only necessary if you write a cookbook page (you can leave them for now though). this call covers all the checks, so we don't need them again in this call (just convolutes the code)\n@vigsterkr both REAL and DENSE is checked. That was the whole point of having the base class here iirc. I would do a cast after the call and add a comment. this is reliable due to old code design principles by s\u00f6ren. yeah this is much much from a numerical perspective!. haha I wrote the same thing above ;) sorry didnt see. @vigsterkr I mean this whole type system in the features is messed up. I would just rely on dynamic_cast everywhere. The bool is also pointless. And the enums are even worse (too many types, I would make those strings or even better drop them). no opinion on the second. I just think building and executing them in these interface builds is a waste of cpu. floats still don't work in octave.... @sorig we will need to block this, ruby doesn't like it. @lisitsyn since it is only octave that cannot decide between float and int, maybe some of this could be done in swig....though I don't know how that would work..... +1 for all of this, but I think that would be another PR. https://www.gnu.org/software/octave/doc/v4.2.0/Integer-Data-Types.html. can also put the scalars in there :) we have some hackery that covers it now. but of course not necessary. I would maybe give a little more info:\n\"Getting parameter parameter CLASSNAME::PARAMNAME failed. Requested type is X, but internal type is Y\". no agreed, var should be this one, I meant the other stuff. Couldn't we have this without the AnyParameterProperties? but just string?\n@lisitsyn . could you use the new (shorter) bsd header? see some other files. but this is slower than the old version :(\nMaybe introduce this macro as a function in linalg? trace_dot or so?. I think this method could be pulled into linalg as it is useful for other situations as well.. actually yes definitely pull it out to hide this code here, messy. Is this the same as before? Doesnt seem to me but maybe I am blind :). is transpose matrix changing the memory (it shouldnt)\nAlso, the API seems inconsistent, for some methods we pass flags for whether matrices should be transposed. And for others, this flag doesnt exist and we have to transpose matrices from the outside ?. yep!. fixme?. EHM!!!\nWhy did the tests pass? Or are there none?. I see now! makes sense\nOk, this definitely should be tested somehow (at least in a unit test)\nWanna do send another PR for that?. this method cannot be const, it changes members. cannot be const .... I think it is a good idea to make this method const (if things are precomputed)\nHowever, there is a \"build_subsets\" call below, which cannot be const, so the current const doenst make sense. I like this renaming. so you make this method private. Why not call it in the constructor then? Then we can make the train/validation method const... I see now what your thinking was.\nI don't think this is the best style, making it mutable and the other things const.\nThere are simpler ways of reaching this goal imo. yes. we can also use the small license here. pls no todo in patches. . we dont do this ClassName::MethodName(): anymore. Just the message. where is this code coming from?. if this code is copy pasted, fine. Otherwise, can we rename this variable? :). I think this should suffice for now. could you remove this file? it makes the build fail as the method is not implemented.\nI will use things in the feature branch\n. why the copy paste? why doesnt one method call the other?. yeah let's modernize a bit :). maybe leave it so it is clear where the code is coming from. Can you tell doxygen that \"fit\" is a method here?. minor: no need to state the type in return, c++ tell you that already. missing newline. I woudl also suggest to change the error msg\n\"Empty confidences, which need to be allocated before fetching.\\n\"). could you remove those?. where do the numbers in the tests come from?. thats good! Could maybe be added as a comment in there. But nevermind for now. is this deleted anywhere? SG_REF?. deleted somewhere?. why display?. Think that should be OK as a workaround for now.. rather than the comment, I would name the variable probabilities and then you dont need it. Minor of course. no need for this comment. I would never write the type of a variable in a comment, but minor. good catch!. what will happen if a user forgets to call fit first?. no type in the comments or doxygen, it is clear from the code. \"Provided scores are empty.\\n\". remove. yes, can't this be done a bit more modern?. why all the empty lines?. I see. Not your fault of course. Would still be better removed :)\nIn fact, what would be best was to remove the example altogether (another PR) and have a meta example for that. Yes, but that is really me nitpicking. Just for the future. not sure why we want the return type to be const?. the method seem to be called in init, why call it again?. so it is built twice?. no need for const on index_t. could you remove all these superfluous comments?. also, does this test share code with other tests? If so it should be pulled out and shared. 1234?. use some, then you don't need to unref. cant we store the \"trans\" in a varialbe and then use that in the if-then-else. Then we don't copy the whole code block 3 times .... integration tests are back, so now let's see :). typo. cool!. Let's try to avoid this. Maybe you can shift things around that it is only built ones .. ?. I think it should be ok. pls rename the file to chi_square. these changes are unrelated, could you remove them?. @lisitsyn re-introduced wrap since we will get memory errors otherwise (things are not ref'ed and then deleted by objects). @vigsterkr same problem as for put: the check for SGMatrix/object bails. (so I blacklisted it for now). labels from file will be CDenseLabels which are then converted in the classes (needs some refactor in algorithms, see below). IMO we need std::iostream for our logs, then we could just overload the << operator and dont have to introduce all these to_string methods ... . @lisitsyn @vigsterkr this is how labels could be converted in algorithms, if they need an interface to CBinaryLabels for example. this was causing a segfault in python legacy examples every now and then. I gave labels copy constructors to the sake of converting them, see as_binary below. I made lots of getters const. @lisitsyn made the class non-abstract, for the sake of loading from files (one doenst know what type of labels it is at that point, only when the algorithm tries to convert to say binary labels we know). mentioned conversion method to convert label types into each other. great error message for users. meta examples now use some so cannot have a unique ptr inside shogun and expose the raw pointer via SWIG. @vigsterkr @lisitsyn this is the stuff behind the factory API. fixed in develop? I don't remember us fixing that SWIG bug? Did I miss it?\nhttps://github.com/shogun-toolbox/shogun/issues/4177. Yep absoltely agree. But I think maybe let's do that for all meta examples in another PR?\nI can do that next. Yeah I thought about this. The reason why I made it a new instance was the fact that in the as_binary below, all other cases return a new instance, so to be consistent.\nAnd also then the as_binary method can be const. On the other hand, as_binary could return const pointer ...\nIf you think it is better to just cast, I can do that. Not sure I get what you mean with the move and rvalue. I know but I can overload the << thing, no? Then I dont need the function calls when I write error messages, but can collect them somewhere else?\nTotally up for doing a new logging backend. GSoC project part?. discussed with him, he was fine with this change. Alternative would be to always return a new instance in the ::multiclass() method (which currently returns the raw pointer). yep!. how do you want to deal with converting say multiclass labels (that only contain two classes) to binary? For this, the actual vector needs to be changed, so a new instance is needed.. For converting the same datastruct, I see the point. Check the history of that line, it changed back and forth IIRC. if it is a shogun array, we cannot rely on SG_MALLOC SG_FREE?. this implicitly calls equals?. i.e. compares content?. The point here is if you do\n'labels = svm->precict'\nthen Shogun returns a raw pointer. Now if you pass this to another object that increases the ref-counter, and then decreases it, the object is destroyed. So calling a method on it subsequently crashes. This is why I re-added it, just like when you introduced it in the first place.\nOr you mean something else?. I would actually prefer if this thing gave you a read-only labels object (we wanna have immutable labels anyways)\nNote that memory is not copied here.\nAlso, after reading on it, I think the move constructor stuff doesnt really help here. will update. I would definitely also evaluate the test error, then you can say in the notebook that xvalidation can be used to estimate the test error. THis requires loading test data. I wouldnt use combined features, this is more multiple kernel learning. pls don't math mode \"mean\" and \"stddev\" but just say mean and standard deviation. \":math:k-fold cross-validation\". Pls start every sentence in a new line. \"This process is repeated  ... , of which we compute the average and standard deviation.\"\nRemove: and assign that as the accuracy of this mode -- not clear what it means. Pls remove \"in this tutorial\". Rather say \"We do perform k-fold cross-validation with a linear SVM model.. Simplify: \"We use a linear SVM model, see ....\". these are helper methods, could also move the contents directly into the switch below. it copies indeed, which I actually didnt realise. Why does it copy? Doesnt seem to make sense to me. And agree, it should not. yep!. no this is only internally, didnt put it in the header.\nYou think it should be just inside the switch?. browser didnt reload ;). Could you put every sentence in a new line. This makes later patches easier to review as the diff is smaller. \"Cross validation (lower case v) is a technique to estimate an algorithm's performance on unseen data\" would be a bit more concise. remainder -> remaining data. Could you split the sentence? Lots of \"ands\". Also watch your grammar\n(sorry need to be a bit picky here as this is user documentation). remove \"a performance for the final predictive model\". * Remove \"For instance\"\n replace \"you\" with \"we\"\n Please just use \"n\" data, not 10n. remove \"might for instance\". just say \"each of equal size\". replace acuracy with \"performance measure\". , of which we compute mean and standard deviation. Don't switch times\n\"We perform\". (linear SVM). linear ridge regression (no caitalization). Just \"Example\". punctuation problems. Also same comments as above regarding capitalization. remove the \"in this case\", this is obvious from the code and is easier to maintain in case the code changes. from. the second sentence repeats most of the first sentence, pls merge them. Repeated use of \"finally\". remove stddev\nremove \"the\" in \"the evaluation results\". We now show. no capitals, \nThe notebook should have an intro sentence: something like \"In this example, we illustrate how to use cross-validation with multiple kernel learning (MKL).\". pls remove these empty comment lines. this is autoformater, I didnt change that line. ah no actually I did\nsure can change that. though I'd rather do that globally in a separate PR ( i think there even is a PR for that). Maybe, but I was afraid that this will lead to all sorts of other required changes in places that depend on the fact that labels copy the data.\nI can look into it, though I think the patch is already big enough. I don't insist, I am just following the rest of the file, also it doesnt have any shogun includes. but let me remove the keyword. I agree, and will do it. lapack can still go imo, no?. I would remove the comment and let the code speak for itself :) minor. while I like this test, I think we should have something super simple. Like the gradient of the gaussian kernel wrt to bandwidth, left/right argument. Something that we know in closed form.\n. this might fail randomly if the data is not separable.. use N and D. auto. But please de-prioritise the test until all the cmake stuff is correct. superfluous comment. let's not do this string argument to determine the behaviour of the function. Just split it a bit further into helpers.. as said this big function should be split into multiple helpers and pls not string arguments to determine behaviour. could you pls check whether this class is guarded with lapack, and if not, remove the lapack guard around the test?. the formatting of this comment is weird. Also the comment is superflous: if you name your variables nicely (they are) then the code is self explaining and we dont need this comment as it doesnt add any information. you can initialise SGVector in the same way btw. this is quite cryptic, why not do a transpose_matrix call?. all instances you are creating yourself could be created with some and then you dont need to REF and UNREF them.. minor: pls put comments not inline but in a separate line above. dont remove these methods for now (the examples might stop working). as you can see, the current obtain_from_generic methods are inconsistent: some of the increase the reference counter, and some of them dont. This one doesnt.\nas does NOT increase the reference counter. This explains all the segfaults in the CI.\nUnfortunately, if you replace a obtain_from_generic that increased the reference counter, you will have to add a SG_REF after the call.\nHowever, I actually think that neither as nor obtain_from_generic should increase the reference counter at all, so the best would be just to modifz the code downstream to not decrease the reference counter downstream. here is an example of an increased reference counter. another increase. I think the ones that do the increase are the minority. are these things covered by other tests? I mean the numerical results ... If not, we need to do that!. ah yes I know this problem. leave it for now then. why removing the old one?. also CDenseFeatures<ST>::get_transposed(). minor: could you make those 3 lines one line?. I think you dont need an own method for this, just use the above call. pls remove this comment, it doesnt help. what about removing the _simple suffix?. while we are at it, could you make the license BSD? just copy it from anothoer file and maintain the old authors (add yourself). the comment doesnt help.\nAlso formatting issues. for (auto i : range(t_w.vlen)) ... but this is minor. why this wrapper, you could just call the thing inside :). same here, the wrapper doesnt help. formatting is all over the place here. remove this. the comment ... but it is minor. just add yourself as author in the standard way...makes later processing of headers easier. why the unref here?. the comment is useless ... super minor :). I guess I would just delete the old one from the same scope that you created it in (tearDown or whatever), but this is also minor.\nMake sure to run the test with valgrind to see whether the memory is fine btw. I dont think it does .... better!\nEven better would be to use a single linalg call to compute this, rather than the loop.\nThe advantage also is that it would be SIMD and therefore faster. Typo, Can with capital C. name the hidden state, i.e. The hidden state $h_t$ at time step $t$ is computed ...\n(also pls use proper math mode for all math variables). the subscript s_t_-1 is wrong\n$s_{t-1}$\nPls take extra care when writing math in the doxygen, you can render it locally via make doc and then check index.html it in doc. Could you explain to me why the reccurent layer inherits from the linear layer?\nI am not too familiar with the NN architechure we have but this puzzles me a bit?. \"Default\". The new Shogun has generic set and get methods that can change any parameter that was registered using SG_ADD. Therefore we dont need to add any getters or setters ...\nPls give those variables default values, and avoid \"must be called before ...\". What do you mean with \"The layer should fill the given arrays .... \"? This is a function call and the comment should describe what it does. I would call this \"Boolean mask\". Capital S. A few comments\n\n\"random\" is not a verb. Should be be explicit about what actually is done\nWhat if I want non-Gaussian noise to be added? Shouldnt this be more modular?. \"results should be stored\" should be \"results are stored\"\nHowever, this is public user API? Then there is not need to talk about internal states (this are hidden from the user)\nIf the method is an internal helper, it should be protected/private and then the docs can talk about internal states .... I realise many of these comments are regarding the general design of the NN classes, which are not your fault. \nHowever, we can still discuss these things a bit to make it better.. your formatting here is all over the place. pls dont do such vanilla implementation of linear algebra. Use linalg calls and no loops. for (auto j : range(A_ref.num_cols)) is the modern way to do this. should be ASSERT_EQ, comparing the content will segfault if this is false. there is a lot of copy paste code in those tests.\nUse Fixtures and templates  to avoid this (see e.g. the recently refactored LibLinear test merged PR). pls avoid TODOs in PRs. auto and range pls. I commented that we want to remove any getters and setters and use the generic ones.\nThis means that the code that pre-allocates memory in here need to be moved somewhere else, preferably into helper method that is always called before training. I am sure the other NNs have similar concepts, and this would be a nice PR that improves the general design. I think there is a way to generate a whole vector of random numbers\n\nAlso, would be cool to have this modular and user other random streams (separate PR). indeed! :). pls use linalg and not eigen. pls no raw pointers. auto. this should definitely be modular!. ?. it a actually is PD, I got this one wrong. yep you are right this we can link against. will remove. I just realised we can also move back the CNLOPTMinimizer class to the main repo, which is very good as then all the GP codes are still in the BSD release!. why all the whitespace?. could you try:\nresult = cross.evaluate()\nreal cv_mean_accuracy = result.get_real(\"mean\")\nMight have to check under which name the mean is registered. too much whitespace. for a binary classifier\nor\nfor binary classification. the === should fit the heading. remove \"is a type of model validation technique\" and replace with\n\"allows to\". replace \"essentially\" with \"is based on\". grammar error make it\n\"and all validation results are combined, e.g. by averaging\". remove \"known as\". remove whitespace after \"k-\". inconsistent N and n,\nno whitespace after n pls. say \"with approximately equal size\"\nand remove the (where n=...). remove whitespace, make this one paragraph. Dont use tenses like \"would be trained\", just say \"is trained\" (also in the other places). why a newline?. more grammar errors in here\nreplace \"at which we can\" with\n\"and then we can compute\". Also you are between direct and indirect formulations.\n\"the model is trained\" (indirect) and\n\"we train the model\"\npls be consistent within this cookbook. pls match the heading. Replace period with coma and remove parenthesis. so space after k-. missing period. pls dont make paragraphs with single sentences. for this section, I have the same comments as above. @geektoni this seems a bit verbose. Isnt there a shorter version?. CSingleFITCLaplaceInferenceMethod::obtain_from_generic method increases the reference counter, but your replacement doesnt.\nDo you see what I mean here? You have written that you have dealt with this, but that is not true here. ah sorry, you in fact remove the UNREF below ...\nall good!. this can just be \"apply\". I think this might break the integration tests (did you check?)\nIf so, pls generate a new test data file. Sorry! I just only realise this is the advanced way of using it. @geektoni Could we have a way to pass lambda functions that does not require to pass the string and the encoding?\nI.e. something as compact as the for (auto i: progress(range(0, 10))) but with a custom stopping condition?. just realised a minor inconsistency. Should be called \"training\" if the counterpart is called \"validation\"\nBut I would even prefer \"train\" and \"test\" ... this comment seems weird to me. see comment above. why the double initialisation?. just another question.\nIs this compute method threadsafe, i.e. const\nIf not, we need to make it so to avoid the race condition analyser moaning. Actually, all of the methods called in this parallel loop should be const. pls use the kwargs construct. Check meta_api/kwargs.sg for how to, or the kernel svm example I posted. You cannot just remove such a check, this will cause problems.\nThe reason why we have this check is that we cast the labels to binary later on.\nE.g. there is a line\nnew CMulticlassLabels(static_cast<CBinaryLabels*>(m_labels))\nKeep in mind m_labels here is of type CLabels*.\nIf you remove the check doing a static cast might cause memory errors. You will need to replace any static cast with a conversion, as in the commit I posted earlier.\nFurthermore here, you could directly convert to multiclass labels, rather than going via the binary ones (though it is not clear whether that works yet, you will need to experiment)\nPls add a unit test where you put in labels of the wrong type and make sure that an exception is thrown (rather than the program crashing). This will also make sure that you understand what is going on. What if some elements of the array are null?. maybe SG_FORCED_INLINE?. @sunalbert could you rebase and use the new function?. m_labels has type CLabels* and you cannot change that, so you need to convert whenever you want to use the CBinaryLabels* interface.\nMulticlass conversion is implemented, we just assumed [0,1,2,3,...] for now.\nPls add a test where you pass both wrong labels, binary labels, and multiclass labels and make sure nothing crashes (and run the tests with a memory checker).. great!\nfew comments\n\ncan we avoid all this redundant code via re-use the code from other tests?\ncould you test other label types as well (like to test the multiclass conversion)?. pls lets not do this, but directly convert to multiclass. There is a problem here:\nThis will only match if the pointer type is CSGObject but not for any subclass (discovered this in my own tests when writing clone). See below how to reproduce\nThe thing is: we decided to register the base classes directly, e.g. CKernel, without casting them to CSGObject\nI think we will have to use the decltype trick you did for cloning single objects ....\nOr maybe a type trait as we have done for put and get?. if you make a test where the pointer type is Object**, this will go into the std::copy branch above. Shared my tests here: c526de9543c62c5f9ab002c67091b175b6f0a7de. Actually already the simple basic_checks test reveals it: double free since the object wasn't cloned:\nhttps://github.com/shogun-toolbox/shogun/blob/c526de9543c62c5f9ab002c67091b175b6f0a7de/tests/unit/base/SGObject_unittest.cc#L192. You, all you need to do is to add a case in multiclass_labels, which deals with the case when LT_BINARY is provided. The conversion will be the same as in the constructor you mentioned (just using std::transform, and some checks). What is good about this is that if the user passed multiclass labels in the first place, there would no conversion. If the user passes binary labels, everything is as before (nonsense conversion, but we can leave that for now as we are working on a different topic here)\n\nI don't understand what you mean re the contiguous labels.. I am not sure I totally follow you but all I am saying is to replace \nnew CMulticlassLabels(binary_labels(m_labels))\nwith\nmulticlass_labels(m_labels) (and removing all REQUIRE statements that assert the label type before.\nObviously, multiclass_labels will need to be modified.\nIt doesnt make sense to first convert something to binary and then to multiclass when we can convert to multiclass directly, or?. few comments here:\n\nif there is no conversio needed, then no conversion should be done (i.e. leave the check in and only convert if that is false)\npls do only copy the label vector once, currently I think you are copying things multiple times (your loop, set_int_labels, ...)\nwhy do you call set_int_labels? I would rather just convert the label vector in here and then pass that vector itself to some<CMulticlassLabels>, rather than modifying the given label\nwe might in fact want to make the passed orig pointer const to avoid changing the original structure ....\npls use std::transform rather than custom loops. This constructor is duplicate code with the thing you wrote above, could you unify that?\n\nEDIT: I think if you use my PR version of the to_multiclass, this might work. I would like to not use these old (shitty) constructors .... actually, I am just working on this anyways, so let me do the fix and then you can use my code....stay tuned. #4251 once it is merged you can rebase against it.,\nIn fact, you can git cherry-pick the commit and then already update this PR if you wanna press on. this will break integration tests, so you will need to update the test data file.\nSee the developing.md readme for details. If it doesnt break the integration tests, then no change is needed .... actually nevermind. as long as you change all the example code listings, this is not a problem. ouch!!!\nbut I guess this works .... since you are touching this, could you use a factory here:\nEvaluation eval = evaluation(\"AccuracyMeasure\"). kernel bandwidth changed from 15 to 1, need to regenerated integration testing data. example refactoring is OK!. Why did the integration test data change?. ah the order!\nOK!. i think gamma should be model selectable. formatting pls. formatting.\nAlso please make this\n\"Number of classes (%d) must be 2\"\n. cant this check be pulled out and only done once?. +1. this is very indirect and makes the code hard to read. Why not create some really really simple test data to make the point with the exception?. maybe put a comment on what is expected to fail here. cant you use auto and some everywhere?. yep that is better, only one conversion don here. thats a first step!. can you use put here?. for the sake of minimal changes, i would not delete this and do inline below, but rather replace the line with the factory. in order to avoid all these unrelated changes in the diff, could you first send a PR where do dont change anything, but rather just save the notebook, and then do a second Pr with your changes.\nBut only do it next time, this one is OK. not sure about this. as you can see, this diff here is everything. Can you send a separate PR that just stores the nb in the new format and then update this one here to just have your changes? thx. due to the changed gaussian kernel parameters, i needed to re-generate the integration testing data.. This is done as we want to register members by base type.\nHere is an issue with this: users can now set any machine type to mkl.\nThis will lead to a runtime error, when the machine is used as an svm. Yes exactly, this is inherited from the design of put. . ok!. thx for the patch.\nUnfortunately, the problem is that master branch and development branch might be different. So either we use relative links or no link. good call, will move them!. because SWIG confuses it with the double templated put (for non-objects) and fails. \n%template(put) CSGObject::put<CMachine, CMachine, void>;\n. this will be changed definitely.\nI dont know exactly what you have in mind there. Have an example?. need to have this as put/add are now templates.\nAlso explains why I need 3 template parameters for put (there already is one with 2). this doesnt happen anymore now as we have the type information. need this as cannot include the dynamic object array header in the SGObject.h. could you use the new API (see other examples). you now need to do this as we dont do the obtain_from_generic anymore. i guess you can just delete the file?. @vigsterkr @lisitsyn can we start using multiple inheritance in the c++ framework? Or is there still a blocker?. does this work without further downstream changes?. is this registered now?. ah here we are ! :). i see so it is just passed on. Cool! :). I think this is a bit inefficient.\nEverytime this method is called, a new vector is allocated, content is copied and it is negated.\nCompute is called many times.\nI guess this is why negated shifts was a member.\nBut we can easily avoid this with a boolean flag in the solve_shiftedweightedor so?. minor: no paragraph break, combine with previous. no paragraph break. Colon after sentence. pls start new sentences in a new line, makes later editing much easier. period instead of coma after \"N\", and remove \"then\". I would remove the last examplek as you already said twice what is happening (\"Finally we train the model again from scratch on :math:P_1, P_2, ...,P_{n-2}, P_{n}and test it on :math:P_{n-1}`\"). pls newline after each sentence end. \"Example for a binary classifier\"\n(no capital B, of->for). one sentence per line, no linearbreaks within sentences (I wont comment again, this is globally :) ). Example for regression. Do not use -> \"Stratified cross-validation only makes sense for classification problems\". pls dont copy paste, many of those things were said above already. I.e. you can be briefer in the second example. pls use the new API everywhere\nFile f_feats_train 0 csv_file(..... use Labels and the new API. pls use kwargs in the meta example, not put. ah we need to add factories for this ... it is OK to use the old API here for now. new API and factory pls, see other examples. pls no obtain_from_generic, you dont need it, as you are using get downstream. pls use apply and Labels (new API). for this example, same comments as for the above. This is still unaddressed. I wonder whether we shouldnt do this in the base class? I.e. CMachine::train ?\nThis way we need to register every machine specialization. Or do I see this wrong?. *= -1. ok, as discussed, let's try to move this into the base class and for the spezializations that overload the train method can just call the base class (e.g. in CLinearMachine). could you adjust your editor so it doesnt cause all this noise?. Is this a user facing error?\nIf no, pls say \"No X provided.\\n\" (also missing newline). see above, and also for all other error msgs. Pls always print the provided number\n\"Number of training parameters (%d) must be positive.\\n\"\nhttps://github.com/shogun-toolbox/shogun/wiki/Assertions. minor: for (auto i : range(n)) for all range based loops. can you use linalg here pls, rather than those vanilla loops. I dont understand this warning thing. Could you make it more clear?. whats ONALINE?\nPls try to do slightly more elaborate test names :). add suggests to add them to a stack of callbacks. So how would this be tested with existing machines?. maybe those two can be renamed to something more descriptive. superflous comment :). could we rename k to something that indicates it is the \"train-loop\" counter.\nLike \"num_iterations_train\". MockModel. Ok I guess your idea here is that we only test things for the Mock_model once and not again for all the subsequent machines?. ?. these kind of messages can go. I am not sure I understand the purpose of this method?. the docs don't match the method. +1 for this change!. sorry for being picky, but there is a parameter missing in the docs.\nAlternatively, just dont document it, as the base class has a docstring already and doxygen will pick this up\nOr, you can mention that this precomputes the kernel matrix and stores it. We could make this `REQUIRE(!is_data_locked(), \"Data needs to be locked for training, call data_lock()\\n\"). ok got it now. This is a sentence.\nThis is the next sentence.\nA new paragraph starts with a newline in between.\nAnd then goes on here.\nAnd here.\nAd here.\nRather than\nbreaking in the middle of sentences. Or continuing in the same line for new sentences. I think this is clear from the first two examples. If you disagree we can also leave it in.. wrap? This should not be in the .sg file, it is automatically generated, see https://github.com/shogun-toolbox/shogun/blob/develop/examples/meta/generator/targets/cpp.json#L14. Pls only comment if you have questions or comments. No need to say that you are changing this in the next commit, just send the commit :) (trying to keep the noise in my inbox down). It does that! (Or at least it should do that :) ). we don't use put in meta examples, you would do\nMachine svm = machine(\"LibLinear\", labels=labels_train, liblinear_solver_type = enum LIBLINEAR_SOLVER_TYPE.L2R_L2LOSS_SVC). see kwargs.sg. could you send a separate PR with this change and then rebase against it?. As said in #4299 you will need to patch the .cpp file and convert the labels to the right type rather than asserting it.. minor: missing period. maybe add \", for example sgclass:Accuracy, or sgclass:RMSE\" (sgclass meaning linking to the class). link to the classes. missing whitespace. as said before, pls dont copy paste sentences from above. This gets to verbose. Either adjust the sentences \"Here, we use CMeanSquaredError as an evaluation metric\", or just remove this at all.. Same here, pls dont copy paste the text. I think most of this can be removed, and you can just group all the snippets in one and just say something like \"Here is an illustration how to do this with a regression algorithm\". +1!. not sure this comment is relevant?. evaluation_cirterion = evaluation(\"AccuracyMeasure\"). could you patch factory.h to have the cross-validation splitting as part of it? We wanna use the new API everywhere if possible. as said, I would block most of this into a single snippet and then simplify the example above. pls dont mix camelCase with camel_case. . points_on_a_line. is there a chance of re-using this test data or the general setup? If so it would be good to put it into a function or a fixture. It also makes the test smaller. @geektoni @shubham808 does the progress bar work gracefully from within omp?. MockModel. missing newline. Also pls always print the probided number\n\"Number of training parameters (%d) must be positive.\\n\". can't this be done without a loop using std::transform or something?. as abovek would be better to do this without loop (if easy)\nIf not, at least do a for (auto i : range(n)). could we maybe alias things like Matrix<var, Dynamic, 1> into something more readable?. why is this SAMPLE? rather than sample. not sure what the warning is supposed to mean, I commented this before but maybe I missed your reply?. average. apply, rather than apply_regression. shall we put this into a helper, this kind of stuff is used in multiple places in the tests. could we template the features as well?. I like the new API here!. as said many many times before, pls put this into the kwargs\nMachine svm = machine(\"MKLClassification\", kernel=combined_kernel, interleaved_optimization=False, svm=libsvm). I changed this in #4313 to illustrate. You will have to rebase. minor: no newline here\nminor2: pls document parameters even if you over-riding the docstring. No need for the obtain_from_generic. Just dont use the specialized type\nThe method signature is virtual CEvaluationResult* evaluate();\nso you can do\nEvaluationResult result = cross.evaluate() and then you use get downstream.\nWith any of those changes pls try them before you send a PR as this back and forth is quite time consuming.... EvalidationResult result = cross.evaluate()  should work. dont we want this to be const parameters?. i think this could be done via DotIterator. i would prefer to remove this and udpate the state vector itself inside the main loop.\n@shubham808 can elaborate as he is doing similar stuff for the perceptron. why not as? would be a bit cleaner. Training is expensive, so the additional costs shouldnt matter or?. maybe we should throw an exception rather than this boolean stuff?. why the change? just by editor accident?. this is bad.\nYou over-write the previous w in any case, which kills the feature of users being able to initialise themselves.\nThe setting should go into the else branch of the if statement.. set const to 1.0 / num_feat and them remove the add below. Sure but I mean, shouldn't the train method itself just throw an exception?\nWe don't get any context information here. If train would throw an exception, we could catch it here and then say \"training failed for reason X\". No?. I see, sure\n(though get_feature_matrix can be made const without problem). ah yes, correcting copy paste errors. nice!. The allocation should be moved into the if. i.e. just define it here. actually, why dont you just do m_w.resize(num_feats)?. ok cool. But still you can move the allocation inside the if, you dont want to allocate memory if the user has provided intitialization. keep this line here, and change the above to a definition. yep this seems to make sense. EvaluationResult result = cross.evaluate() didnt work?. this checks for ==, can you make it equals?. EXPECT_TRUE(perceptron_initialized->get_w().equals(weights))\n== is pointer equality (should fail). I am not sure whether default arguments should be here. @shubham808 did you check whether shogun has any other %newobject cases where a templated object is returned? I wonder whether this needs some special treatment (and idk). in fact we dont even need the parameter names, just the types. just checked, and there is no case of %newobject in shogun that is templated.\n@shubham808 i think let's just do it for float64 for now .... float64_t. maybe add sgclass to link to the API docs. could you start every sentence in a new line? I.e. newline after each \".\", this makes subsequent diffs way easier to review. \"stage\" of what? Maybe add \"stage of training\". +1. can this be done with the generic get?. Could you maybe make this \"Last element in pipeline is %s\", and then put the machine name there?. same story, pls print the name of the last added machine so the user knows what she did wrong. Could you be more elaborate here as well\n\"Pipline cannot be trained without an added machine. Last element is %s\". as above. user facing error messages need to be more explicit. print the index as well\n\"Requested index (%d) out of bounds \" .... Could you write some minimal class docs?. great set of tests!. since you are using x and x' downstream, pls put them into (inline) math mode here as well . n is undefined. +1!. I like this more as well! users can then print the strings themselves. not sure I understand. why swapping it?. yeah +1 as we discussed. since this is a test for pausing, maybe we should refactor the test file a bit and either create another file for those tests, or rename the test to something like \"all_models_tests\" (or some better name).. so we are stopping after 1 iteration? seems ok, but can you explain why that choice?. this seems quite clean now, nice. ok just so I understand. This trains for 1 iteration, then applys and remembers results, then saves-loads and applys, ensures results are the same. is StoppableMachineTypes instantiated somewhere? cant find it. ok!. can we rename to inverse ... without transform ?. I see. \nOk, one of the next things I want to do is to register callback functions with get so that we can lazily evaluate things, like in this case. lets have PROGRESS and SPROGRESS (static version). The static version accepts a string as the first argument, callers have to provide the class name manually in those cases. actually, no static macro needed, just call the function. why the io passing here?. why the this->io? can't we do this via the macro as well?. did you yet rebase against micheles merged patch? Surprised to still see this in here. I dont understand, there is no this in a static context. . could you pls check your editor and make it not do such newline character changes? They pollute the diff. why the additional?. SPROGRESS would be used in static contexts only .... Actually, this was part of the \"user experience\" GSoC project. So yes makes total sense imo. This is for the user chossing to serialize during pause right?\n. why do we need to add this? wasn't there something similar already?. I have another Q here: rather than whitelisting stoppable machines, we should backlist the ones that we want to exclude from testing. Or? Otherwise new machines will not be added to this list, and thus, not be tested. sure there will be different blacklists for different tests, but that is fine. Some machines will be serializable but not stoppable and vice versa. As said before, if we have to explicitly add new machines to this list in order to have them tested, this will be forgotten and so test coverage will suffer. Think 3 years ahead. We can maybe have a global ignore list and a test specific one to avoid too much redundancy. yep, the name is misleading, just call it \"serialize\".. second @geektoni comment\nAlso agree with @vigsterkr The ascii format should not be compulsory, as discussed in another thread. Same goes for filename. Also some error handling will be needed. why not for (auto j : SG_PROGRESS(range(num_feat)))?. same here, why not put it into the loop?. I wonder whether having only an outer progress would be better?\n@geektoni ?. nice!. if omp is used, I assume the pb definition needs to happen before (and cannot be done in the for loop?) @geektoni ?. Sorry for being picky here, but I realised that this is not nicest.\n(This doesnt block a merge though, we can change this later)\nMaybe use the filename (without suffix) instead? @vigsterkr @geektoni thoughts?. Could use __FILE__ and something like \nstd::string remove_extension(const std::string& filename) {\n    size_t lastdot = filename.find_last_of(\".\");\n    if (lastdot == std::string::npos) return filename;\n    return filename.substr(0, lastdot); \n}. let's do it in all the places then (apart from the mp case where I dont know whether that will work). Use case: user runs algorithm and pauses it. Then she want to serialize the intermediate state AND continue training afterwards. Not possible via explicitly calling serializing as that requires training to be stopped fully, and thus it cannot be continued afterwards. So serialization during a pause has to be an option for the user (in addition to do nothing, continue). Furthermore, we dont want to interface with the user to ask for filenames and formats in the pause. This is why there needs to be a way to specify potential intermediate training serialization format and name for the user before training starts. IMO this can be a member method as it will depend on the state of the stoppable object, namely the file that might be used for dumping the temporary state. you can, but ,as said above, you have to stop training for that. because we dont have a bytestream right now. Once we have a generic interface for serializing into whatever, we will use it here. yes it should be CFile. @shubham808 has not updated the PR yet after our discussion.. BTW @shubham808 the file should not be automatically serialized on pausing. Just if the user decides to.. Can we have some updates here pls? :). It works. The idea here was more that, in a typical use case, a user is just living in one thread. So the idea was to make his life easier and offer him a button to press that dumps the intermediate machine somewhere. It is a smallish detail . this is still an open question. There is no object for SG_SPROGRESS. We can either make the caller of the macro pass something or use the FUNCTION. But I think just using \"Progress\" is not the best or?. cool!\nHow does it look like with this? Can you post and example?. for Perceptron this should be ok as we are touching all data in every iteration. So there is no problem with continued algorithms starting to touch the data from beginning again. this shouldn't be in swig. yes, let's just remove those ctors. keep in mind this will blow the integration test. protected! both of them. Let's follow shogun's design for now which is to take features here. (and change this later). This needs more documentation about when it fails and what it is expected to do. Furthermore, as discussed, it should be in CMachine, virtual, and throwing an error (user facing, so \"This model doesn't support continuing training\"). as this class is abstract, no need for this. I think no need for \"register params\" actually, as we only have one ctor. SG_UNREF(m_features) (once those are added). continue_train is actually a better name! :). you need to set the iteration counter to 0 here, and the completed flag to false\n. this is not supposed to indicate completeness, but success.\nSo and \"continue\" should return a bool. if you are overloading the base class method I think you need to use CFeatures* here. type in description.\nmodel selection should only be available for max_iterations, not for the other two flags. why this?. pls use the base class method here (this means you need to manually set labels before training). no, pls use another member. We want to move towards not storing features in other classes. Call it m_continue_features or so. needs investigation!. on it. i usually prefer to avoid this, but it is ok here as well. max_iterations if you rename it (plural). i was actually wrong, if the number is the same this is fine. pls dont mention method names here (these will go) but just say externally setting weights and bias. plural pls. ah this is nice, as it uses get_name, this will still print the right string. Pls start sentences with capital letters (minor). Only possible if a previous CMachine::train call was prematurely stopped. Throws an error otherwise.\n(Keep in mind this is user facing docs, so need to be a bit more specific yet user friendly). outdated docs. outdated docs. Explain the purpose of the method, I.e. Can be overloaded by sublcasses for custom initialisation purposes. maybe this should be int64. Just \"Training continuation not supported by this model.\\n\". whenever you store a reference to a CSGObject, you need to  SG_REF it, (and SG_UNREF it in the destructor if it is a member). I think you dont want to delete this. I.e. the new member (continue features) we only want to use it for continuation, but we dont want (yet) to change any of the class structure underneath. This is CLinearMachine specific code and we dont want to alter it. yes, you are here re-allocating the vector in every iterations. This is not good and we need to find a way to recycle things. As we said we will \"communicate\" over member variables, I guess this needs to be moved there.. could we set this to false up here and then set it to true if it is converged rather than the other way around?\nWhy does this even have to be a local variable? Can't we use the member directly? (and modify it if needed only). I think since we only have a single (default) constructor, there is no need for this init method, but rather copy its contents into the ctor directly. pls only document these methods in the base class (properly for developers, with instructions that overloading this methods allows for certain things). No need to re-document them here. same here, we would only overload the docs if something special happens inside the method (which it doesnt for perceptron). what would you reset here?\nI think there is room for a method like this (would call it \"reset_training\"), but I would call this in \"train\" before training starts, i.e. the purpose would be a model where the user prematurely stopped training, and then calls \"train\" again rather than \"continue\". this seems copied from base class, no need to do that as doxygen will do it automatically. some docs describing that this class implements a model whose training can be stopped, and in particular resumed, at any time. train_machine is protected btw. Be careful when inheriting to respect the base class structure!. good doc!. \"To be overloaded by subclasses to implement a single iteration in the main training loop.\".... it is mandatory ;). put can be extended easily, but maybe you are right and 2^31 is enough ;). This need proper documentation pls here as well, this is user facing.. there is no setter? (fine if no). @lisitsyn @vigsterkr somehow the API didn't get simpler with all the latest additions :D. IMO, I would keep the c++ typed API in the unit tests.\nwrap(perceptron->apply_binary(test_features))->get_labels()\nAt least simpler than old school (SG_UNREF etc). dont we need to set a number of iterations here?. I see. Maybe rename and document. I am confused why this would be the same: one perceptron was stopped after one iterations, and the other one was trained until convergence no?. or maybe I should rather read :). minor whitespace error at the end. The converged flag can be set to true by the iteration method. So there is no need to have a variable. Just set it to true when it is appropriate, and leave it unchanged otherwise. I see. So this is about testing that one can stop a machine, then continue, and then the results match the process that didnt stop in the middle. OK!\nBTW I think the EXPECT_FALSE(results.equals(results_itermediate)); should be remove as this is not always necessarily the case. The last check is the one we care about.\nYou can check the status of the flags\nm_converged == false\nm_current_iteration == whatever. pls name the test appropriately\ncontinue_equals_nonstop or something :). i dont understand, see below. there is no need for this statement. just put an else branch and set converged (the member) to true in there. done. no need for a variable. ok as discussed, ignore my comments on this :). could you print a warning here if max iterations is reached but the complete flag is not yet set (like the one in perceptron). I would remove \"algorithm\". did not converge after the maximum number of %d iterations. Question: I guess we dont want the warning to be shown if the user stopped prematurely. Only if the algorithm actually did that number of iterations, see what I mean? . and/or clean up temporary states. you need to keep the reference here and UNREF it, otherwise you will have a memory error\n(most getters in shogun increase the reference counter on returning objects). This means, only showiung the warning if complete is false and m_current_iteration == max_iterations. as an example for the end_training method, you could overload it here and then free the vector...i.e. overwrite it with empty vector. no need for a 0 btw. also reset the continue features here. reset continue features here if algorithm completed naturally. (and store the given features). I think we should have one. But that should be different. It should just be an SG_INFO message that logs that the user chose to stop X prematurely after Y iterations. Yeah or that. Just something that informs the user that he aborts after X iterations .... you need to ref when you store a reference, and unref when you forget it. sgref data is not needed if you dont store the pointer (subclasses or base classes might and will take care of this)\ncalling train machine should first forget old continue features, so the unref seems valid\nYou will have to try all cases in unit tests to really figure it out, it is sometimes hard to foresee. but the rule is simple: \n always ref if you store a pointer\n always unref in dtor and when you overwrite it\n* if overwriting a pointer, first ref the new one and only then unref the old one (in case the obj is the same you avoid a deletion of the obj this way). Not so sure about this thing as we already have a cookbook for ridge regression\nhttp://shogun.ml/examples/latest/examples/regression/linear_ridge_regression.html. minor: be resumed. There should be no 8 in here. We will have to make this work otherwise somehow. Rather than defining this here, couldn't we just add this this-> to the original SG_ADD?\n@vigsterkr @lisitsyn @iglesias . What is cool is that we avoid the usual mixin factory macro mess since we only have a default constructor anyways. @vigsterkr would we have an own type of exception here?. progress bar is missing. I dont understand why we would repeatedly call pause_computation here. Can you explain?. I.e. explain what the method does (from a user perspective)\nSomething in the lines of:\n\"Continue training.\nThis method can be used to continue a previously prematurely stopped training call.\nIt throws an error if the machine does not support this feature.\n\"\nNo talking about overloading (that would be developer facing). Should we have our own exception type for such things?\nNotImplementedException\n@vigsterkr \nSG_NOTIMPLEMENTED is more for \"sorry not YET implemented, i.e. incomplete implementation)\", as opposed to. \"This machine doesnt support this feature\". why an if here?\nyou could just make the above an ASSERT_FALSE. you mean making a global method? where would that sit?. this would be in CLinearMachine. In CIterativeMachine actually!. @lisitsyn viktor wanted to keep the SG_ADD for now in order to make refactoring easier.\nAlso as serialization still doesnt work with the new framework, we need to register both old and new parmaters....\nSoooo, can we add the \"this\" to the original macro?\n@shubham808 The 8 has to go (as in the other macros the nubmer refers to the number of parameters). same question as for the SG_ADD, can't we just add the \"this\" to the macro?. Minor: pls start new sentences in a new line. I would prefer a lowercase k, unless K is used elsewhere. oh wait, we didnt have kernels yet? ;)\nOk then this is quite good to have :). is this really the best reference? where do you have it from? can't we use some of the references we already used (like the kernel book by sch\u00f6lkopf?). pls do proper references. These pages here are definitely not correct. Could you talk about the fact that a distance is used for this kernel?. acutually no. Just talked with @vigsterkr and we agreed that we should just add the \"this\" in the orginal definition of the macro. I see, maybe we can remove them then?. ok cool. When citing it, put [Section 3] in there (if possible). Maybe shorten the sentence slightly.. pls adjust to text. Is there a key equation that we could show here? You could shop around the web to see what other libraries use to describe this. cool thx!\nDid you render this locally (the html output) and check that it all works ?. Capital W. We create \"a\".\nAlso pls start new sentences in new lines. this should be said in the intro. remove \"the\"\nAlso, please spell it \"diffusion maps\". pls remove the cache size. Sorry didnt spot this earler. I have a final question here:\npredicted_label is a scalar, and it seems that the previous iterations ouputs are not used in the current iteration. Why do we need to store the vector of them?\nCouldnt we just not have the m_output and instead just define a local variable in the iteration method?. this is a bit misleading: \n\"Flag that determines whether hyper-plane is initialised by the algorithm, or not. The latter allows users to initialize the algorithm by manually setting weights and bias before training.\". You can remove the class name\n\"Mix-in class that implements an iterative model whose ...\". maybe >= just in case. Could you remove this?\nsubclasses will inherit the docstring .... remove docstring here so that the one of CMachine is used (and the CMachine one should be polished a bit, see my comment). we can leave it as is for now. or @shubham808 you can have a look at what is the apprpriate exception\n@vinx13 could you explain how to use custom ones?. \nPerceptronP? What does the P stand for?. i guess the output here can be remove just like in the other PR. can we make this a parameter, both N and D should be parameters. I think we have some random normal matrix method somewhere ... so no loop is needed. can be dine in a single line using std::for_each or so. it is only if sublcasses dont write a new docstring.\nBut as said, I would remove this here. shall we try to test a slightly more elaborate example of registering a member function that returns a constant or something?. Also, we need to keep in mind equals/serialization should exclude lazy computations.. this should be move to outside the loop over the data. i guess we can make learn rate to constant 1 and bias to constant 0 to simplify things. wasnt the idea that we dont want a string_features factory, buy instead somehow deal with this inside the features factory?. i.e. we need some kind of markup. Otherwise these things will be equaled/cloned/serialized as well....or is that intended? Costs computation and I am not sure it is worth it. ah, clone_from will be a problem as it attempts to write into the member. Only the ones that are important for the model being applied to new data.\nProbably only the weights.\nYou could add this to the serialised model test to find out what makes it work, that\u2019s what the test is for .. cant we use operator() here?. ASSERT_EQ to avoid segfault. offtopic: I would love to move all those general checks to base classes so that they dont get repeated in every specialization....\nany thoughts on this. i think this is good!. maybe call this \"perceptron_baseline\". @vigsterkr will have comments here, or? :). can i suggest to put line breaks in between the Ranges and then add an inline comment on what this addresses? make it easier to read. I guess we could get away with static casting here as well, but I am always more comfortable using a dynamic cast as done in  as. you want train_machine to be public? why?. the error message should change and say that.\n\"Training %s with %s is not supported.\\n\", get_name(), data->get_name(). keep in mind data might be nullptr.\nThis actually raises a problem: as we cannot dispatch in that case ...\nI guess we need to add a \"default\" option for training if features are not provided.\nmessy.\nOr we just enforce the user to pass features from now on (API change that will have an impact all over shogun)\n@vigsterkr @lisitsyn @iglesias thoughts?. @shubham808 I think for now you can add a \"train_without_features\" that could be overloaded by subclasses. We can then remove that later. nice, so we shaved off all this code in every class that dispatches the templated type for training. As of now, I am quite puzzled how we could combine this idea with iterative machine, since making the iteration method virtual and templated is not an option, but neither is to do the dispatching inside that method :(. I think then in clone and equals, I would just skip all registered function anys. typo \"DISPATCHER\". Could we make the macro in a way that we can use it do overload the train_with_dense to dispatch the template type, but also that we can use it to overload iterate_dense ?. maybe an easier name would be train_dense. needs a default case . the code is essentially the same, so why not try to create one macro for both.\n\naccept the to be overloaded name as a parameter, here train_with_dense\naccept a variable number of parameters that are passed on to the definition of the overloaded method and the call of the typed implementation\n\nThen DENSE_DISPATHER(train_with_dense, train_templated) would expand to\nvirtual void train_with_dense(CFeatures* data) { ... train_templated(data); ... }\nand DENSE_DISPATHER(iteration, iteration_templated) would expand to\nvirtual void iteration(CFeatures* data) { ... iteration_templated(data); ... }. nope, but it would be cool to use the same macro. this is something that could be done in the factory, or? (taling a sneak preview). out.set_cont(1.0). +=\nmake sure to run the Integration test  before updating the PR! it should have failed as this changes the results. sorry nevermind this comment I was wrong. why noy move the multiplier in front of the linalg::dot. i think this method can be removed if it doesnt do anything or?. yep!. Btw this can be a linalg call as well. Can we make the argument a const reference?. Put a comment that the last element is the gradient wrt the bias. I changed my mind, let\u2019s just remove all of the debug code. We can do lots of efficiency improvements here.... but later. i would prefer a copy ctor here. this doesnt belong into CFeatures imo, this is conversion code between std::vector and SGVector and therefore should sit in SGVector\nfeatures->view(SGVector<float64_t>(my_std_vector)). or even done implicitly (if possible). why have this method why not just use the copy ctor?. same comment as above. why all these changes? shouldnt we focus on the view first?. pls avoid such whitespace changes in PRs.\nThey should be in a sep PR. It just pollutes the diff (i.e. hard to review, takes longer). can you explain the reasonaing here?. again, I would prefer those changes to happen in a separate minimal PR. let's get rid of this import via only using linalg. i think there should be a check/assertion that features are dense float64 somewhere. doesnt this create the same problem with Some?. ok,\nI technically would prefer first adding the view and then have a second Pr to deploy it.\nBut I think others see that differently, so ok leave it in :). ah sorry I see now, totally justified!. But the definition is\nvirtual float64_t dense_dot(int32_t vec_idx1, const float64_t* vec2,\n            int32_t vec2_len);. lol :D. 42?. can you explain ?\nWe need to use the base class interface in things like xvalidation ... ah sorry nevermind I didnt see the arugment was the std. instead: yes ! :). i wonder, do we want to clone the subset?\nI think actually we shouldnt\nAnd also I think that add_subset should accept a const vector. but maybe you have a different opinion?. maybe we can actually systematically test all access methods of labels and features somehow?\nI would also assert that the view's data pointer is the same as the original one (i.e. no copy happened). sweet!. continue!. actually, why not move the check after the debug msg below, then you dont need to print the name and stuff again, but can just say \"Skipping as not cloneable\". same here, i would move this down and only say \"skipping\" to avoid duplicate code. This doc is not entirely correct, as this is the self adjoint version, it uses an Eigensolver if the provided psd matrix, not an SVD.\nAs suggested before, I would put a flag in self_adjoint that defaults to false.\nThis would then use the SVD to compute the singular values (the rest of the code is identical for non-self-adjoint).\nIf we dont do this, there will be lots of redundant code (basically copy paste for the non self-adjoint version). (and then rename it to pinv). pls check how scipy/numpy pinv behave\nThey do work for non symmetric matrices.\nBest thing is you do unit tests\n\nsymmetric psd matrix, using self_adjoint=true\nsquare matrix that is not PSD, using self_adjoint=false\nnon-square matrix (more rows that cols), using self_adjoint=false\nnon-square matrix (more cols than rows), using self_adjoint=false\n\nI suggest using some 3x3 matrix, and then do a unit test that compares against pinv in numpy. ah and also tests that assert for raised errors when self-adjoint=true but a non-square matrix is passed. Actually, let's keep both methods in.\nSo instead of the flag, lets add another method called pinv\nBut make both methods share code via a helper method, they only compute the decomposition themselves. dont understand the message.\nPerhaps it is unmutable for CLASSNAME?. one thing: as CIterativeMachine is not available from swig, you wont be able to instantiate NewtonSVM from Python, so you can actually remove newton SVM from the interface files, see #4354. Question: Can we re-use this memory across iterations? So we dont have to allocate-free-allocate-free it all the time? What is worse is actually that we set it to 0 in a loop everytime it is allocated. Could maybe be a member element and then be re-used?. could you use matrix notation here? i.e. operator()\nIn fact, it would be better to use sg_memcpy instead of the loop over dimensions...\nsg_memcpy(XSv.data(), sgv.data(), x_d). same as above, can this memory be re-used?. pls use .data(). why not w0.clone()?. so this is called but NewtonSVM doesnt implement that or?\nIt could though if we recycle the variables mentioned above. great!. is this happening anywhere?. I see ... ok!. ah but is it getting smaller or larger?\nIf just ever getting smaller, we could use resize .... which is might be more efficient?. the memcpy option is better, see my old comment. I think there should be a linalg call for this nested loop. memcpy. not sure why the multiply with identity is happening. linalg pls. .clone() possible here?. .zero() method pls. linalg::. clone? if possible without messing up the structure.... as said sg_memcpy, and otherwise would need to check the operator() implementation to see what it does. OK, agreed. @lisitsyn there is the question whether we really want to allow this for non-const methods ---- they might alter the state of the object, which probably is not what you would expect from a get. but for this example, due to some weird design, get_cluster_centers is actually non-const and it cannot easily be made const since it calls get_lhs on the internal distance object, which increases the reference counter, which cannot be const. gna!\nObject counter inside SGObject are not really ideas. ... is there maybe a way to fix this nicely?. it will still have to be linalg, as we are planning to remove the SGVector interface soon.\nCreating SGVector instances on stack is essentially for free so no worries.\nYou can even do them inline\nlinalg::dot(SGVector<...>(...., false), SGVector<...>(...., false))\nno need to give them a name. yes!. this can be memcpy as well or?. so this is O(logn) rather than O(n) ?\nGood call!. why the change here?. lol, this method should rather be deleted (or return a list of strings or so). it is not used?. lol another joker method :D. this should just be in the matrix itself, i.e. create a new feature instance using the reshaped sgmatrix. maybe we can rename this to merge  at some point later?. absolutely, the instance should be tied to the matrix, and otherwise users can create a new instance. matrix, not Matrix\nalso no need to print A. the memcpy that is part of clone is technically unncessesary here, i.e. the memory initialization of the result. I think you should offer the solver enum as a parameter in this method, just like svd does. size checks?. maybe this line can be shared somewhere between both pinv versions? it is the same code after all. pls remove this. This is not sufficient as an only test. There needs to be a comparison to scipy explicitly, so I suggest you keep this loop here, but also explicitly check the B matrix contents. what about naming B as A_pinv?. name C as I_check or so, so it is clear it is supposed to be the identity. same comments for the test above regarding names, and explicitly checking the output. for the SVD case can you also pls test a 4x2 matrix and a 2x2 (or the one from above)\nAlso please test that wrong dimensions raise error messages. btw you also need to test the other interface where the result matrix is not passed, but generated (here you also need to check that the result has the correct size). you could use the other interface here if you re-allocate the inverse in every iteration (no need to do 2 lines for that)\nBut in fact since the size is constant, why dont you share the memory of that matrix across iterations. Then it would be a bit more efficient.... Another commen there:\nLet's remove the ping from SGMatrix (might requirte using linalg in a few more algorithms). the formatting here is a bit all over the place ;). name this A_pinv_scipy or so, so it is clear where this comes from. A comment also will help. is there no API for \nauto I = linalg::identity(3);?. checking against result is missing. as said, pls test all cases here: square, and both rectangular setups. pls use std::min. Result with capital R. i am still moaning about the clone, which has a sg_memcpy that is not necessary. i dont think this can work . \"or pass the operand argument A as a result\" mean?\nI dont think that can work here as the dimension doesnt work out or? It would only work for square matrices\nBut you need to add a test for this as well\nlinalg::pinvh(A, A)\nand make sure there is no memory error. i guess we dont talk about specialized label classes anymore, just CLabels, but that is totally minor. mmh that is really weird. mmh that is really weird. ok kill it then :). i guess data type issues, ok leave it as it it. btw there is ASSERT_DOUBLE_EQ. name it pinvh after the method\nand then split this into two tests, one for each method. You could use a fixture to share the data. i take this back. lets just leave it like this. btw you should add a test for each method that complains if wrong dimensional result matrices are passed.\nSo that we have coverage of the REQUIRE statements. typo. i think the name is not the best. I know we used this in the gist, but maybe we can call this FeatureDispatcherMachine (or something better). So this class is supposed to JUST work with dense features? Then we need one for string and sparse, etc as well..... formatting clash here. what is this here? This is inherited from the base class I guess? . if this class is just for dense, then such checks should be one level up in the hierarchy since we dont want to copy paste them around. formatting all over :). wow didnt expect we need such things, but yes makes sense if i think about it\n. lets do this in a spearate PR ok?. i think the problem here is that we force the subclasses to implement the templated train method. So if we add string here, we will get a compile error when it is not implemented for strings....or?. not so sure about that.\nAdd a case for string and you will see the problem (see above). but whose features are this?\nCLARS right? So this means that your implementation here depends on the fact that the first template parameter has a member called features. We dont want to enforce that actually.. my point is that this implementation assumes that T or P has a member called features, i.e. it wont compile otherwise.\nI would just enforce features to be passed here tbh.\nWe wanna to this anyways soon, so all the mixins can already require features to be passed, so they dont need to be members. also, maybe a different name might be nice.. i see, thx!. CMachineFeatureTypeCRTP would be more descriptive, but also more ugly ;). could you paste the compile error that happens when train_machine_templated does not exist in CLARS? Curious how it looks. could you pls use the short version. no memory leaks/erriors in here?. you could actually call the clone method of SGObject here, then you do not need the get_labels() and can also use apply rather than apply_binary above. until. This is not the best unit test.\nBetter would be to compare the solver against liblinear or some reference implementation of the newton svm. \"embedded\". this is user facing doc, so you need to make it a bit more user friendly. this is actually pretty good ...\nJust need to make sure in the docs of the CRTP class that we mention that implementing this method is mandatory. could you try to make this only available for floating point numbers?\ntemplate <class ST, class = typename std::enable_if<std::is_floating_point<ST>::value>::type>\n. assert for non null pointer.  the check should happen inside CMachine, otherwise we have to repeat it. I think auto should be replaced with the real type here to make reading the code easier. what about a switch here?. the class name should match the filename please.\nOr is your plan to put all the dispatcher classes into one file?. could we introduce something that translates the feature type enum to a string and print the strings here instead?\nE.g. std::string feature_type(EFeatureType) that returns the actual string, which we then can use to print such things in a human readable way?. I would not print all options here, think if one type is added, then we need to change all error messages.\nJust saw that \"Training with dense features of provided type %s is not possible\". you see, this is copy pasted code that can go to the base class. remove comment. there we go, this is why I dont like to list options in error messages, the text is wrong now.\nJust do the msg as above. I had to look it up as i was unsure about the rhs but ok. just only check it if features (and labels) are provided in the base class, we will soon require them to be present anyways. This doesnt make sense: train_machine is overloaded by lots of shogun algorithms, so you cannot use that. You have to do the feature class dispatching in train, and it also needs to be optional (for the machines that are not CRTP), and the default option should be to just call train_machine??. supports_feature_dispatching. I am afraid we need one of such checks for each feature class to avoid problems when passing for example passing string features to LARS. we need some tests for the dispatcher in a mock class environment, to test that\n\nfeatures are correctly dispatched, also for multiple feature types\nerrors are thrown if the \"wrong\" type is passed. working with friend classes is infeasible here?\n. Much better. One final thing, could you make doxygen link the class method here?. on \"a\" pipeline. maybe link to some cross-validation cookbook. There is some way to link to notebooks, I think MKL does this. ha it is just below, nevermind :). Slightly weight English:\nWe demonstrate a pipeline consisting of a transformer ..., and LibLinear for binary classification.\n(maybe also link to the liblinear cookbook). I know this is copy pasted, but this is now outdates when we use factories.\nJust say: \"We create CFeatures and CLabels via loading from files\". This sentence basically repeats the intro, so I would just remove it. We create a Cpipeline, and chain the transformer and the classifier.. see CEvaluation. last word model (singular). actually, I would mention that \"we first chain all transformers, and then finalize the pipeline with the classifier\" (since you use \"then\"). Please also mention something like \"The pipeline instance behaves just like a machine and this can be directly passed to CCrossValidation\". dont we have factories for those things?\nI would prefer if all newly added examples would fully use the new api so we dont have to refactor later. Same here, could you pls use a factory for this. We dont want to use constructors in the examples. factory if possible (this might be more tricky). \"Cannot train %s using %s./n\". get_name(), data->get_name(). No features provided. pls also assert this->labels, you call a method on them below. of provided type (%s) ..., ptype_name(data->get_ptype) (i dont know how to get the ptype but i think it is possible). F_DREAL is hardcoded!. you need a method\ntemplate <class T> EFeatureType CDenseFeatures::get_feature_type()\nthat you can call here instead of F_DREAL, i.e. replace it with\ndata->get_feature_type(). I have doubts now. y. unnecessary. will do autoformat once this works, thx. Would you mind creating one? That should be pretty easy!. yes exactly, though I would just name it pipeline. could you pls use enable_if_t ?. typename U = typename std::enable_if_t<std::is_floating_point<ST>::value>>. exactly like this! :). BTW why is the C++ ype not just called Pipeline and the Machine called PipelineMachine?. check the factory machine in factory.h, you just have to add some macro lines\nthe call should be splitting_strategy(\"StratifiedCrossValidationSplitting\", labels=labels_train, k=2). TBH, I dont really agree, I think then should actually return CMachine (since the subsequent object will be used in this fashion) ... why do we need to know that a machine is a pipeline if it behaves as a machine?\nAnd then Pipeline is the thing that builds the stuff.\nMakes for a cleaner API imo\n\n@vigsterkr @lisitsyn @iglesias what are your thoughts?\n. Sorry I wasnt clear ;)\nWhat I suggested is this: CPipeline inherits from CMachine, so it is and behaves as a machine. Therefore I don't see a need to expose a separate class (say to swig). All we do is to call train/etc. I would call this thing CPipelineMachine and hide it from the outside world.\nNext, the class that constructs the above machine (whose name is now hidden from the API) could be called CPipeline now, the thing where one can add stages etc and the finalise as a CMachine. It has a method CMachine* CPipeline::then(CMachine)\nDoes it make any sense?. So CPipeline -> CPipelineMachine, CPipelineBuilder ->CPipeline, then returns CMachine. yes the stages thing is a problem. got it! thx for clarifying.\nI still would slightly prefer different names (CPipelineMachine, CPipeline) but it is only a minor difference, also no strong feelings. Can leave it as it is. did you try the friend class thingi? would be nicer to  not have this method public. minor though. newline pls. cant this sit inside the mixin?. we would need an integration (or unit) test for this class with string data before we can merge this.\nCould you send an example in a separate PR?\n@vinx13 what is the state of string feature meta examples?. this will fail if you pass anythign else then uin8_t string features. You will need to assert the above if you want to rely on this: https://github.com/shogun-toolbox/shogun/pull/4373/files#diff-2a3c04af057ddaaf9cfa245feef22d22L167. ++. I think a more appropriate name would be \"this_casted\". Training with %s of provided type %s is not ....(move type and use get_name of features). as above. Could you be more explicit here: \"Training %s with %s is not supported\", get_name(), data->get_name(). could this not sit in the mixin?. good!. MockMachine. pls accept the value of the field in the ctor and make it protected. m_expected_feature_type. no need to put data into the features, can use default ctor (your mock doesnt use it!). you need to REQUIRE the labels as well. Actually, CDenseRealMockMachine. could you rename the rest to \"TrainDenseReal, train_wrong_feature_type\". this should be typed as well (i.e. tried for all feature types for string) or?. why did you chose those specific 3 types? . maybe template it (with default parameter being char). you would need to make this templated to make the test templated. I think we do to enter the branch in CMachine::train or?. @vinx13 would you mind adding an example for the CWDSVMOcas ? :) (no cookbook needed for now). ah yes, this will be moved into the method that is called from outside, i.e. build_subsets ... putting it into a helper method makes sense. I think it would be even cooler if we moved those methods to CDotFeatures and then specialize them in here for batch computation if not subsets are set (otherwise call CDotFeatures::cov which has an iterative implementation). the idea if this patch is that inside the algorithm, we only work against a very high level interface of CFeatures (deals with subsets etc).\nFor labels, we always get the vector (which creates a copy for subsets). I think since it is just a vector, this is fine, although we could also think about adding an interface to CDot/DenseFeatures that computes the dot product with a CDenseLabels which then is batch without subsets and iterative(no copy) if there are....probably too complicated for the marginal benefits of avoiding a single vector copy. also generalise LRR for cases where D>N. make bias compitation optional which is inline with other shogun methods. the link doesnt work, where is this?. ok!. wasnt this defined already?. actually nevermind, this should never be executed anyways. as said, I think you dont need data here, but can just use the default ctor or?. I changed this to CDenseFeatures::mean (locally, will push soon)\nbut the point here is: labels are directly used via SGVector (with a copy happening if there is a subset), while for the features, we dont touch the feature matrix. If you derive it via differentiating the squared loss wrt to w and b, you will get that\ne.g. https://are.berkeley.edu/courses/EEP118/current/derive_ols.pdf. mmh this is not what I meant ... the formatting is messed up now .... \"Training %s with %s of type %s is not supported.\", get_name(), data->get_name(), feature_type(data->get_feature_type()). as above. ok!. I made a comment on the commit how this error msg should look like, . this is not enough as it is only checked if training_requires_labels() returns true. Your code here can segfault if no labels are provided....you need to enforce labels if dispatching is enabled ...\nSince we will enforce labels to be passed soon, you can do that for the case of dispatching. sorry, we cannot do this unfortunately. sorry, my bad, this is exactly what we wanna do :D. I am not a fan of error messages that contain things that might change in the future, i.e. a new class is made viewable.\nI would just state: Class is not viewable. The compiler error will provide the T and also the static assert which will tell the caller what is viewable and what is not. Or?. could we make viewable const?. is this used anywhere?\nHow are we going to do this without multiple inheritance?\nMaybe a mixin approach would be better, see IterativeMachine. I just realised that I had a mistake in my thinking. This is just a global templated function so all good, nevermind!\nI like this idea of having the method with the static assert!. You are right! the expression for w was wrong, it assumed no bias. I am fixing it right now. I think no. . Yes please! If you pass Some you should get Some. I still vote for const T. ++. This is pretty much the same thing that got me confused about the builder vs the pipeline.. @iglesias voila! :)\nIf you could do the math and confirm this, that would be neat! :). what about adding a factory for it? So users can call\nPipelineMachine(machine)? \nAlternatively, for the c++ folks, there is as. apply. Neural network is a machine, so this could be instantiated using machine\nThis means that the below methods set_layers, quick_connect, and initialize_neural_network all would have to be changed.\n set_layers can be done via put (already now!) just needs a dispatcher for the CDynamicObjectArray in put.\n quick_connect, and initialize_neural_network could just be called automatically inside the train_machine method which should be a minor refactor. apply. Would rather not have a network factory, but instead use machine. nice!. I wonder, could we go with the simpler name layer?\n. exactly like this!. yep!. although .... \nwhy would there be multiple calls? Due to multiple \"train\" calls?. also would need to make sure that these things are set to false if the network architecture is changed.\nSo actually, it might be that always* calling these might be a better idea. The cost is neglectible.\nThoughts?. btw instead of using the array type here explicitly, we should be able to use SGObject::add, which allows to append to an internal array.\nThe call should be machine.add(\"layers\", input); etc. This means you would first create the network instance, and only then add the layers.\nIf this works, we dont need to add the array type to shogun's base classes, which I would prefer. see the multiple kernel learning example where the same thing is done. does this overloading of var name and the factory work? Just asking. I like the flow of this, so good to merge from my side. This needs a better description.\n\"Whether the network is automatically connected and initialized on training.\". pls refer to the doxygen methods that are called here. yes this is much nicer. Should have thought about it straight away :). if you change these references you will need to change the cookbook ones as well! It refers to these snippets!. Is this taken care of?. ah yes it just returns the DynamicObjectArray so if we set that instead using add, all should be good. you need to unref this in the destructor. the f is not necessary. could you open an issue with this?. wasnt this method remove in develop?. If Pipeline inherits from CMachine, why cant the method be protected?. I think we could probably do this via the parameter framework clone, with a few additions for std::types. The class is not serializable anyways.\n@lisitsyn what do you think?. yeah I know, just so that it is documented. This is a nice entry task. sorry of course you are right. friend class?. @iglesias the old dget routine was a desaster, I added a new one, pls double check. @iglesias it created a temp matrix, called a multiply method, and then added it.\nThe new one doesnt create a temp matrix. It only works for x=y, though can be easily extended and we didnt need x!=y in any calls of the dger. @lisitsyn @iglesias @vigsterkr our template style for features doesnt allow me to use type traits to define member functions of templated features, as all template types are instantiated. This is why I added this runtime check for floating point numbers. We can remove that once the features are cleaned up a bit. It for now just stops callers from doing nonsense (compute mean on bools or ints). thanks to @shubham808 s work, we can now delete such redundant codes, while making algorithms fully templated. @shubham808 tada! :). this is what we are paying for mixing templated and non templated types (here features and labels). this is what we are paying for mixing templated and non templated types (here LinearRidgeRegression and features). copying the labels vector to get the correctly typed version of y (consequence of float64 labels vs templated algorithm impl). copying the labels vector to get the correctly typed version of y (consequence of float64 w vs templated algorithm impl). Good point!\nSo the idea is that features are a quite different type of object than matrices and vectors. Even in the case for dense features, e.g. think of an active subset. This is why such an interface for CDenseFeatures (can be moved up later) makes sense: via only offering an iterator interface and some convenience functions (as opposed to direct matrix access), one can keep things clean.\nlinalg on the other hand works on linear memory blocks, such as SGMatrix.\nThe PR is to exemplify these concepts. this is more like a TODO, i need to write down the math....it is coming. I avoid the static_cast or c-cast now using this explicit thing. Makes it more readable indeed\nFor the numerical things (1.0/N) I prefer the C cast tbh. I am confused by the \"linear\" in the filename. \nAlso I think it should still say \"mnist\"\nWhat about mnist_3class_256d_features_train.dat. what about also covering floatmax_t?. call this \"Dense feature types\", remove the LDA. Be more general.\n\"Dense features, like those read from a sgclass(CCSVFIle), can be read into memory in multiple primitive types, e.g. 32 bit or 64 bit floating point numbers.\nDownstream algorithms can often deal with multiple of such feature types, and will carry out required computations in the provided primitive type.\nReducing from 64 bit float to 32 bit float can be beneficial if for example very large matrices have to be stored.. missing sglcass missing newline after period. We create an instance of a shogun algorithm (here sgclass LDA). Group all of the feature calls together. And say \"We can pass any of the feature type to the algorithm for training\". I am not sure I understand why you want that change? This effectively bypasses any type checking that swig code could do at runtime (passed a valid enum type). dispatching not Dispatching. sgclass CDenseFeatures. Algorithms that inherit from ... downstream can deal with all of such types. Well feats->cov() kind of does something in these lines.\nIf there is no subset set, it does a speedy matrix multiplication. If there is a subset set, it first copies the matrix into a continuous one and then does the product. An improvement would be to do this without the copy, i.e. an iterative procedure like outlines in CDotFeatures::cov. However, DotFeatures are not typed and always return float64, and I am puzzled how to move the mean, cov, gram methods into that class ..... pls also extract the matrix here (even if it is not shown in the cookbook) for the sake of integration testing .... minor: newline after period. ++. Since I first met @lisitsyn I am in favour of short function names.\nI was thinking about giving it a boolean flag for centring at first but then didnt do it. Could re-introduce it in a case where this is needed.\nGenerally, I would prefer to move towards making clear cuts of functionality, i.e. have a data centring module come first, and then doing the cov.\nA preprocessor could also be responsible for this (at the cost of speedy bulk operations), then the code becomes less convoluted (mean is computed/removed at many places in the code)\nWhat do you think>?. this can be removed . Well it computes the covariance, just for a certain case of input.\nIt is also not called covariance_for_column_major_matrix ;)\nIf you feel strong, I can change it, otherwise I would just outsource mean computations into different code..\nOn another note, I wanna mimic the dot API of DotFeatures, but just templated. What are your thoughts on that?. good!. pls include gtest.h first, they use some magic macros that cause problems with some compilers otherwise. typo in comment. this is still true, also for the above label comparison. ?. this is still unaddressed. i think double and float is comfused here\ndouble is 64 bit. I understand now. results_complete->equals(results). (and then use apply instead of apply_binary. multiple. sure, we can register and we can do equals, and what happens is that the == operator of std::vector is called. This one uses the == operator on all elements, https://en.cppreference.com/w/cpp/container/vector/operator_cmp\nIt will not work recursively, i.e. std::vector> will also use the == operator of the elements, which in shogun is pointer equality and not .equals. Sweet. wait, did we want == operator for Any? Maybe I forgot this .... ;)\nI think we can merge and then I add a vector parameter to our big shogun class test. this is not the right fix here imo\nThis list is excluded meta examples (in this case for GPL)\nInstead, you need to add the same names to the SVMLight exclusion section.\n. I see. I guess they were renamed?. If so, we should just change the fname. IF they were deleted, then this can go..... but if they were just renamed the build should fail as the examples with the new name are not excluded from the tests and then produce a failure due to missing svmlight. yep!. @vigsterkr what's your opinion in making those things globally visible so that we wont have to re-declare them?. What is the motivation to have the \"check_\" prefix here?. there are some minor formatting issues here.\nYou an actually automatically format this properly... see the output from the first travis job here:\nhttps://travis-ci.org/shogun-toolbox/shogun/jobs/441713579#L723. This would also make the diff of this PR much smaller, as there are fewer whitespace changes. uh I didn't reallise that some tests actually operated with floating point number constants and we now have to change all those numbers. Quite some work! Thanks for going into this!. did you calculate these numbers?. I think we might have something similar in SGVector.h. as an idea, we could use a templated function get_epsilon<T> that looks up the appropriate epsilons?. minor style: we do comments in a line above usually (+ a whitespace line above that). What is the motivation for putting these one liners in here rather than the test code directly?\nHow does it look when a test fails? Is it easy to exactly identify where the program is coming from? Sometimes with googltest, it is hard to figure out which case the tester is exactly in. Or in other words: Why not put the test code in there straight away and use TypeParam instead of ST?. not sure the method is even defined for int types?\nGenerally, we try to avoid TODOs in the codebase (they never get addressed ;). maybe prob.use_bias instead? as it was already extracted?. while this text is nice, we have an environment set up to test synthetic data tasks. \nYou can either use that or, if you want the linear data, set up another one.\nThis approach cloggs up the tests, imagine every test defines its own data.\nCheck LinearTestEnvironment.h in tests/environments. I think there might be none for linear regression data, so feel free to add your 3x+2 function in there, it will be helpful for other places as well. pls extract labels to variable. another environment here\nWe can later one try to test all linear regression methods on both cases automatically (something planned for some time). In fact, once this test here is done (sanity checking for simple cases), we can try to generalise it next. these whitespace changes are polluting the diff a bit. Would you mind removing them?. well, next time!. is this coming from the autoformater? Because it shoulnt actually. Could you pls use the shorter header that we started using recently?\nCheck e.g. Gaussian.h. Could we move this to some class documentation?. superfluous comment ;). superfluous. We don't do pointer to SG* structures. They all live on stack.\nIf you want an array of SGVector, you can either use SGMatrix (if all vectors have the same size), or just use a std::vector. usually we do CamelCase for classes only. variables and function names are doing_this_kind_of_naming. superfluous. can this be made explicit in the code rather than hardcoded?. no need for the comment. no need for comment. same here :). and here :) you get the point .... I wonder, souldnt we pull out the shared code to avoid this heavy redundancy?. Any chance you can make this more compact using https://en.cppreference.com/w/cpp/types/numeric_limits/epsilon. again the question: if I grep for check_SGVector_add<, I only find this occurrence of the call, why not move the code in here? Did I miss a response for this anywhere?. does a call to this method with int type even compile?. usually, clang only formats the changes lines, i.e. if you don't touch it wont format. Did you use the correct commit hash for the format? I mean it doesnt matter too much tbh.. minor: no colons needed in doxygen style docs. this seems unnecessary, as we can do get_coefficients()[i] or?. I think taking the machine epsilon and then dropping two orders of magnitude is a reasonable idea. I was just asking what was the motivation here: why put the code into a call rather than moving it into the test directly? If there is a good reason it is fine of course, but if there is none, then maybe we should move it :). well as long as the eigen3 call goes through it makes sense to offer it :)\nBut I am surprised .... ok then, let's keep it, no big deal. actually, thought about it again, and I think it would be better to move the code down (for those instances where the function is not re-used anywhere else) to keep the spaghetti factor down a bit. These things tend to be annoying years in the future when refactoring stuff.. Check this\nhttps://stackoverflow.com/questions/4920783/combining-enum-value-using-bitmask. Pls don\u2019t put such things into the docs, they outdated at light speed.\nBetter describe that there are attributes, maybe an example, but not losing them all. You can also use the << operator to shift the 1 a desired number of digits\nSee the stack overflow post. at the end of the day, it is style. You can either define the mask as you did with hexadecimal as you did there. Or, instead, you can generate those masks on the fly using a shift operator, and then the HYPERPARAMETER constant is the number of bits that you shift, i.e.\nmy_mask &&\n(1 << HYPERPARAMETER) &&\n(1 << GRADIENT_AVAILABLE) &&\n The latter has the advantage that somebody reading the code never is exposed to hexadecimal numbers, nor powers or two (which can be confusing). On the other hand, masking operations (as what I wrote above) becomes a bit more notation heavy. Idk what is best to be honest. I think the shifting is more standard and therefore I think we should follow that.\nAh one thing I think is good about the shifting approach is that you can change the size of the mask easily. You are using a 32bit int above, but you only write down the last 8 bits, which is weird. Imagine you now changed the mask to 64 bit. Then the hexadecimals would be quite long, whereas the shifts wouldnt be.\n@lisitsyn @vigsterkr ?. Minor: sentences start with capital letters. I would remove that. Until we haven't removed it, it is not legacy.. what about calling it \"attribute_mask\" ? :). if you add doxygen docs, pls document all parameters\nminor: capital M. wouldnt this work?\nm_attribute_mask = \n(MODEL_PARAM & model_selection) |\n(GRADIENT_PARAM & gradient) |\n(HYPERPARAMETER & hyperparameter)\n. or in the new style\nm_attribute_mask = \n((1<< MODEL_PARAM) & model_selection) |\n((1<< GRADIENT_PARAM) & gradient) |\n((1<<HYPERPARAMETER) & hyperparameter). I would call this GRADIENT_AVAILABLE. can we keep the naming self-consistent\nHYPER_PARAM\nor\nHYPER_PARAMETER\nor name the other one\nMODELPARAMETER\nor rather\nMODEL_PARAMETER\nActually, in favour of short and concise names, why dont we use\nHYPER\nMODEL\nGRADIENT\n?\nAnd then the constant itself can have a doxygen string to say what that means?. superflous doc, as the method name explains it, i would just remove it. minor what about returning a const string reference. same here, would remove the comment. instead of doing this here, I think it might be better to offer a constructor that does not take any properties, and then set the default values inside there. We need to discuss clever default values as well btw. Same with the description, I would offer a ctor that does not ask for one, and then set it to the empty string or \"No description given\" in the ctor, but not here in SGObject.h. If the default is 0, then we don't need that.\nI would be explicit about a non-zero default, and add a private init method that sets it and that is called by all ctors.. I think it is pretty clear since the class is ending with \"Parameter\", so everything in there is one .... . yes, please don't define them globally, but as a static member. yes, but the first rule of the C++ committee is:\n\"The C++ language does not prevent people from shooting themselves in their feet\" ;). like!\nMaybe also mention descriptions in the first sentence \" ... of parameter meta information, such as properties and descriptions\". ah wait. the thing I had in mind was \nstatic const int HYPER = 1;\nand then\nmask = (1 << HYPER) | (1 << MODEL);\nThis is now difference, as you are effectively storing masks themselves in these const members, not the number of bit shifts. Or am I wrong? Let me stare a bit more to understand\n. I was more thinking\n(1 << HYPER) & model_selection | (1 << GRADIENT) && gradient etc. and here I was thinking\nm_attribute_mask & (1<<HYPER). wouldn't we want != 0. Also, this is not very explicit ... I know sergey suggested, but I need to think a bit here to understand this so not sure that is the best solution. fine as well. not sure :) haha\nwe can discuss in irc. yes, but start at 1. sure, the thing you wrote is (or should be, not sure what happens when you shift a boolean true) a neater version if what I wrote above.. Not sure what you mean, but what I have seen (and been doing) would result in\nAnyParameterProperties(\"My description\",  \n                       (1 << AnyParameterProperties::MODEL) |\n                       (1 << AnyParameterProperties::GRADIENT))\nIt is not as neat, you are totally right!. BUT it can be hidden quite easily either.\nI mean, it is your code, Ill leave it to you. Not sure whether it is possible to say what is better now, we will have to commit to something and then see if it works :) So go ahead with the above solution and then we see where we get. ah, no that was nonsense (just needed to make sure that you never get an all zero mask)\nBTW I also realised why the micro-controller people store the number of bits rather than the full masks: it uses less memory (which is not really relevant here, but still good to think about it for a moment).\nbyte HYPER = 2\n...\nbyte BLA = 255\nso with an 8bit integer, you can do 255 attributes if you generate the masks on the fly a la (1 << HYPER). If you wanted to store the mask for each attribute instead (i.e. HYPER = 0x00000010 (each mask is 8bit), you would have to store 255 * 8bit, which is quite a difference , especially if you have only 4k memory as on an AVR ;)\nHow many parameters are registered during a shogun run? Let's say 100 is realistic. Then we have 100 byte usage if we store only the offset, and 2.5kB if we use the masks. In a data heavy environment, and since we won't register many many more parameters this doesn't matter. For 1000 parameters it is still ok, 10^6 will result in some pretty big differences.. yeah actually good point! :). You could put \"modified by G.F\" here if you want. the whitespaces seem weird here. I would name it has_property. not sure whether it is good to test this .. imagine we want to change it -- need to change more than one test.. imagine we get more properties ... this test then needs to be touched which makes maintenance noisy.\nWhat about just asserting that no parameters are set. i.e. the mask is zero everywhere. yep here we can/should test. I would just write a test that ensures that the default parameter properties are 0. As said, putting them explicitly means we have to update the test when new properties arrive.\nWhy not define ParameterProperties::DEFAULT = 0x0 and then assert the default value is that?. For that we would need a method equals which returns true iff the masks exactly match. this doesnt make sense. You cannot check that a property is not set using has_property -- it is AND'ed with the given properties so if both are zero the and is still zero. Need the mentioned is_property (or something similar). as above. get model is not part of the old API or? Just realising this. yes, this does it.\nAnd then the next 3 checks can go. I think here we should check that the mask only has exactly the two (mode, gradient) set, but nothing else (without saying what the \"else\" is)\nRather than explicitly testing that bits are not set (again think about adding properties and we would have to extend the test). Weclome to SGObject ;). Mmmh. This is not nice, it would be better if this default was done automatically.\nI didn't realise this before, but I think it means we have to have SG_ADD3 (default properties) and SG_ADD4(user specified properties) as macros cannot do default parameters.. While it would be good to get rid of this thing, I think we still need to keep it (see below on default properties). this is quite nice now. again, would be good if default was taken if just nothing was provided. this is HYPER. hyper. hyper. things like this would be MODEL. model. model, you get the hang :). nice!. I think this would be covered in the test below where we check that nothing is set after construction.\nThe old api test is more to make sure we didnt change the behaviour of the code, so remove it here.. Yes, that would do it.. oh, and also, actually it could be operator== instead of equals, or?. mmh couldnt this even be added to bitmask_operators so that == is available for all bitmasks that are defined using this?. No I mean put an operator= in the enum class, the one defined via enableEnumClassBitmask(ParameterProperties);, not the AnyParameterProperties. Or doesnt that make sense?. yes. We dont need to fix all the wrongly added parameter in this PR though, just put a few examples in. yes this is good. yep, much cleaner. missed one here :). and here. Not sure ... Why would we want to have that?. Note for later: we can already start to get rid of the model_selection_parameters field and instead make the code work with any. Definitely a sub-task in all this refactoring ... but more on it later. same for those gradient parameters. I think this would need to be CFeatures, or? Why would we restrict to CDotFeatures?\nSorry it has been a while, we might have discussed this before ;). nullptr. let me understand this again. Is continue_train called from train? I.e. can we make sure the continure features are set on training start?. Or where exactly are those set?. i am not sure about this. equals should always just be true if really everything is semantically equal. So that would include the description and the name. Maybe a rename?. as said before, can we not explicitly test that those are not set, but in fact that none* are set. Otherwise we will have to modify tests when we add new attributes. everything else should be false. And that should be tested. Not just these two, same reasoning as above. same here. minor indentation glitch. ++. see my other comment. (I agree we need a method), but equals not maybe not the best name.\nWhat about a getter instead? Then the caller can just use ==. good catch!. and this does not break other examples? I cannot remember why exactly this case was here, but I remember putting in there for a reason...\nctest -R generated_cpp should bring clarity. was there a compile error before ?. we usually don't add such specifics to the gitignore. Devs do that locally. can't we move way more of the shared code into the fixture?. this code is so specific, I suggest we move it into a helper file within neuralnets. I think this is not a function we should have in neuralnet unittests.\nRather, this should be somewhere where other random number generators sit, like Random.h or something more appropriate. Random normal matrices are used in a lot of places in shogun. actually, there should also be a more efficient way to general an array of random normals. Check the array filling procedures in Random.h. reg_params. it is nice to pull this kind of stuff out! . this fixture is almost identical to the other one. Is there a reason to have this second one?. first one is definitely useful, so let's have that\nSecond one I dont know whether we need this? But if we do, then all fields need to be compared of course.. Yeah that happens on force push. I keep the notification emails to not loose comments when github hides them.. we should use the short style license here that is used in most places in shogun. see my earlier comment: this is not nn specific but should be a global shogun option. why this?. we will need to discuss whether we allow this...\n@vigsterkr @lisitsyn ?. e.g. we could have a random method in SGMatrix (because we have one in vector already)\nthis code is a bit too much noise for my taste, especially in a test. what about a better name than i\nAlso the typenames could me more meaningful. what about just type_finder? Or even something else?. for_each_type? not appropriate anymore?. make sure to udef all the macros after using them. We don't want to pollute the scope too much. sg_types\nSG_TYPES\n?\nAlso, those are only scalar types. What about adding a matrix type to start seeing what happens in those cases?\nWe will also need std::vector and friends.....which might show the issues with this approach: combinatorial explosions. \nI guess we could argue that the number of types are limited.. the PT_ prefix comes from somewhere else?. not sure these are still supposed to be in here?. uuuh, actually we might need to add all base types here as well....sigh\n@lisitsyn any thoughts?. yes but in order to compile the shogun tests we add the include path via -I so it by definition is a system include. Check out the other headers ... we are doing this everywhere :). Question, could we explicit here with the variables passed on as reference?. Is there a specific reason for 0 and 1? As opposed to something else?. Nice this will be usefil. like that yes, so that callers are forced to think about the scope. mmmh, we could throw an exception (recently added customized exceptions might be worth checking this out). If not, a boolean might be better than the int. question is whether there will be situations where no type is found but the caller doensnt know that....\nIf this does not make sense, exception would be best (with a nice error string) imo. nope, this just checks that one is not set.\nyou want to check that none are set. So you need to create an \"empty\" mask (all zeros) and then call compare_mask. but this empty mask should be created via not explicitly listing the available types.\nAgain, this is to avoid having to touch all these tests in the case when a type is added. not sure what you mean, but what about something like\nEXPECT_TRUE(params.compare_mask(ParameterProperties::EMPTY)) or similar. ehm ....it rings some bell, but I can't remember right now... stackoverflow ;). i see, what about somehow re-using those from DataType.h. ok great, this is quite redundant anyways, I like the C++ updates :). @vinx13 could you advise how to best use your new expections?\n@gf712 try SG_SERROR for static contexts\nGenerally, we want to move towards better exceptions, with types. But this is especially true for errors that can be caused by users. This is more an internal error that is caused by a developer...so maybe the SG_SERROR is ok as well\n. you can, and of course it is the same. But then it wouldnt test that the default properties are indeed empty, so the test would not be of an use: It would just test that if you create two properties with the default ctor, they have the same parameters. If you would check for EMPTY (or maybe we call it NONE (and define it to be 0x0), then you also assert that the default ctor generates empty parameters, which is exactly what we want here. If people then later start messing around with the ctor (e.g. change the default), the test will fail, so it provides some protection against devs going wild. I think it is fine to let people look at examples and then use the same style. Documentation just outdates, and this is only for developers (who know how to browse the codebase) anyways.\nWhat happens if a different signature is passed. Compile error I guess so that would be fine.. Well we don't want to support all the containers. Also the container definitions are part of the old type system and probably will be removed soon (nobody needs strings of SGVectors).\nWhat is more likely is that we support matrix/vector of the ptypes, and then strings...but let's do that later when it is needed.. sorry one more thing. Could we make the naming in a way that it is clear that this is a shogun utility function.\nLike adding a prefix sg_ to all functions and global variances that have to do with this\n. so this collects all types that we support going from any to compile time type right?\nSo if we added SGMatrix, it would be added here or?. sg_ prefixes would be good I think. pls, instead of a loop, use the CRandom::fill_array* functions that in some situations can be much much faster. you could store this in a variable as it is not optimized out or anything anyways. do you use anything from this?. sorry I just mean to not nest linalg calls for readability. minor. 1. yes you are exactly right, this needs template specialization, and a loop fallback is fine imo\nLet's do it for SGMatrix::random in this PR\n2. I suggest we keep the interface without arguments, i.e. (SGMatrix::random() and SGMatrix::random_normal), and then the caller is responsible for scaling things to the appropriate bounds, using linalg\nvery nicely thought trough! :). minimal evasion, so only make something as visible/complex as needed, not more. yes. yes that is even more explicit. the last three are nop now. these last 3 as well. wait, didn't we have a function like this somewhere already?. ah sorry, we actually need it here, nevermind my above comment. one thing: could you write a clear docstring for this function? So that it will be immediately clear to other devs how to use it (not so much how it works). is this function used anywhere else, apart from sg_each_type? If not, then we should hide it from the global scope and only define/use it locally in the dispatcher. can we have one vector/matrix type for illustration?. I think now that this works nicely and looks good, we should also have some minimal unit testing that ensures very basic functionality of the for_each thing. One test that makes sure dispatching works, one that checks failure cases caused by ignorant devs, and maybe one for corner cases .?. any way to re-use some redundant code?. especially the guard/macro bit....and only if it doesnt cause more trouble than it solves. for example.\nIt might even go somewher else, as I dont think it is any specific. yes doxygen docstrings have a descr. for the function and then one field for every parameter\n/** This is the description, describes the behaviour.\n@param foo first parameter, and potential details\n@param bar second parameter\n@return the result\n*/. yes move it to the other.\nIDK whether they should be nested or not .... I guess so, would there be any case where we would use the latter on its own?. @saatvikshah1994 \nwe have two options here:\n1. Make a loop over the CRandom::random_normal insice `SGMatrix::random (since CRandom doesn't support SIMD random normal generation \n2. Implement a random_normal_array which\n a. Has a default implementation with a loop for now\n b. First uses fill_array and then re-uses the box muller trainsform code that is already present (would need some refactoring)\nI suggest we keep things simple for now:\n offer CRandom::random_normal_array and use a loop for now\n if you want: make another PR once this is merged where you do option 2b from above\nWe definitely only want SGMatrix::random_normal to be a wrapper, i.e. no transforms or other complicated things in there.. Yes I think this is a good thing to document (we will see whether it outdates). yep, or an own test for the type_finger (whatever the name is)\nTo add tests, just add another cpp file and re-run cmake to detect it. this means the caller needs to know what container type she is dealing with ... mmh\nOk curious what you come up with. On the other hand that might be ok as in shogun, we usually only template API function according to the primitive type, not the container type (apart from some Any things). No I think it is good to offer functions to deal with vector/matrix/std::vector separately... we do this specialization inside Any as well (see equals/clone). mmh this name is a bit of a monster now ;)\nI am thinking loud ... sg_dispatch_any, sg_dispatch_any_vector, sg_dispatch_any_matrix, sg_dispatch_any_stl\nDon't know if that is even better. PT stands for \"primitive type\" in the old parameter framework. So keeping that in here doesnt make sense, maybe we need a new prefix?. do we need to set the seed? would prefer not to (portability). those should suffice for now, so nice :). what about T_. CT_ ?. if there is another \"container type\" enum, I don't really get why we then also need to state the containers here?\nHaving both seems redundant...? Maybe I just don't get it. this is not nice and quite long....\nBUT the good thing is that we (shogun framework devs) do this once, and not people who write higher level framework code. . maybe is_sg_vector ?. I feel this should not be a runtime warning?. since you have the vector thingi now in there, maybe also refactor the histogram bit?. I think you might kill me, but maybe \"dispatch\" should be part of the function name? sg_any_dispatch comes to my mind now.... but not sure what is better. What are your toughts?. is this used anywhere?. minor\nSGVector<float32_t> a_vector = {a_scalar} is a tiny bit shorter ;). this API is not the most elegant....need to discuss this. ++ for static_assert, always nicer. ah ... I already wrote something earlier....\nsg_any_dispatch is my fav atm ... but as said this might change. could we maybe change the signature to figure out whether the to be dispatched object is primitive/vector/matrix?\n. this is a nice idea, but it is actually not different from having 3 different functions, one for each case. I.e. the caller needs to decide what he wants to dispatch and then write something different for each use case. (function name vs lambda arguments). Different environments -> different random numbers for the same seed\nIf all the tests pass without it, I would prefer without ... let me know.\nunit tests are one binary, so if you change the state of libshogun, all other tests might be affected. Even if not, it is better practice to only ensure something if it is needed.\nYes, all approved otherwise! :). I think we could get away with a static cast and rely on the fact that the algorithm knows the feature type is correct (inside iteration), since it was checked before the algorithm started (by your last piece of gsoc work). ok thanks!. mmh phew ...\nI would like to think it is possible to refactor this to SGVector?\nOn the other hand, it might be nice to support std:: structures as well. I like such static assertions a lot!. no just the big stack. But it is OK, as it is better than before. Let's think. The feature of going from any to typed will be used in\n logging (only model parameters, numerical stuff)\n modelselecton (again, mostly model parameters, here we will need SGObject as well ...)\n* not sure what else?\nSo with all that in mind, I think those types should be ok. And we can extend on request .... like done for any. I wonder: Can't this be implicitly be done by forcing the lambda to have a different signature (including SGVector/matrix) ....\nIf not, I think having three functions is the best bet, at least API wise. They all could call the function you wrote here internally to avoid redundant code. lol ... let's hope this is the last :D. It is clear that the caller needs to know what he wants to dispatch to (primitive,vector/matrix).\nBUT, what if the call is the same for all, and then the compiler uses the signature of the lambda to decide. This means we would have to change the signature of course.\n\ndefine lambda (with SGVector in signature somehow)\npass to sg_any_dispatch\ncompiler sees lambda and knows that it is a vector and does the appropriate call\n\nIf that is not possible, I think 3 different functions is the best compromise. Or we ask @lisitsyn if he has an idea (though he is busy at nips until next week). > I think that could be possible, assuming that the compiler can implicitly convert SGVector and SGMatrix specialisations, right?\nWhat do you mean by that? BTW not sure if that helps, but It is definitely possible to extract inner types from templates, see this ugly thing for example\n\nLike in the summaryValue->set_simple_value(float value), I think all scalars can be interpreted as float right?\nThat is true, but the compiler will not implicitly convert float to SGVector\n\nBtw seems like std::is_scalar is quite a broad definition, will have to find a more strict definition like std::is_arithmetic (potentially combined with a std::decay). \nYes definitely, I would have a typedef for that that checks, see here\n\n. > Alright, got 1 static assertion working! I didn't work on anything else yet as this took a while to figure out. I also wanted to add a test for static_assert with gtest, and that wasn't easy but now its working! :D\nnice one!. > Should I replace the warning with an exception? The idea of the warning was that there could be situations where no lambda implementation exists of the expected type on purpose and an exception could abort a ML run without need.\nI would say yes, with an appropriate string describing what is wrong. quite nice!. this just as an internal filter method?\nSounds like a good idea to me!\nMerge?. stuff like this should though be hidden from outside (as done here). we can just drop this function. actually, we can just drop the whole cpp example ... to be replaced with a meta example\nActually \n@gf712 could you try to write a meta example for base_api where you show how to extract model selection parameters at some point? mayber after the monkey business is done. this we need to keep working somehow as it is used for the GP framework ... so a replacement needs to be written\nThe signature will probably change as TParameter will be removed and we will use the dispatching code that @gf712 wrote. this is the big fish to replace . Could you, instead of removing the method call, just keep it and place a runtime error (SG_NOTIMPLEMENTED) inside the method?. same here. The danger with just removing this line is that we forget to re-add it later when we have a replacement. So rather than changing behaviour of code, let's just put a runtime error. and here. Looking at the test, this can probably already be replaced with get. in all of those actually, get should do it. cool!. so now, we can hide get from the swig python and replace it with this method right?. what about get that returns SGObject?. shall we maybe hide this one from outside?. idk...eventually, we just want \"get\" to be visible to the outside....so I would even prefer not to have the dummy method but just \"get\". definitely, but let's first start with python to see how it goes\nSuch a nice API simplification.\nBTW once this is merged, you can change the json that does the get_real_vector names and unify it to a simple get. Then we have only moved the place where \"real\" \"real_vector\", etc are explicitly stated, rather than added this at another place in the codebase. I wonder whether this is the best place to put this?. Minor: start sentence with capital letter. I.e. \"Fill matrix ... \". No need to state the disabled as this is obvious from the signature. Cute!\nI am wondering whether we should maybe move all the template magic out of SG* since that is supposed to only be a wrapper class. Maybe Random itself.... or linalg?. +1 for the rename...\nActually, we might even go explicit for now as as there are not THAT many combinations...I like it on the other hand, so could also keep it as a generic tool within the lib (under a different name). std::for_each ?. remove. would there be a away to check it against the individual calling of random with some seed fixing magic?\nIf that's a pain, dont bother, this test is more to check that no memory errors happen I guess. add your name if you like. const possible?. Check shogun.i where all those methods are defined. We can modify things there. Yep!. Ah cool!. Totally right!\nIf you want to have a stab at doing some of this in a better way, feel free to submit a draft PR for discussing it! Fixing the equals would be a start. yeah don't worry then. i wonder whether doxygen correctly parses the above docstring if this comment is here?\ncould even just remove it. one thing: we usually allocate sgobjects using new for several reasons. It is a style thing but let's adopt that here as well. whitespace issues. maybe sg_is_any_of ? To make clear these are shogun traits?. btw I think I defined is_sg_base somewhere...we could move that into this file. ah crap yes, the ref counting is non-const....\nok so we need to postpone the const making for now until we have another way to do the reference counting. I think handling those asserts happens (should happen?) in the base class?. you are using this below I assume when changing the signatures of some methods?. any thoughts on this @saatvikshah1994 ?. would still be good to rename or at least discuss. should definitely be grouped a bit, did you have a look at the is_sg_base?. very nice that you are fixing those things on the fly as well.\nIt might be worth grepping for those in the ipython notebooks as well. omg how lovely :D. what is the perspective on this bit here?. why two errors? What is the context of each?. I would prefer if we would somehow connect the error msg with the ones present in the c++ lib, where this error already exists. Further, if done as here, the user should be informed that this error is within the interface rather than core shogun.\n. You know why that is?. And also the question: is that good? Maybe we should change that?. Hope you had a good vaccation, and happy new year :). yes, thx for that, maybe (if not yet done) put a comment that indicated that this can be removed/replaced . nice!. ok, but seperate PR. @Saurabh7 this would be something for you to check/figure out? I think we have seen and solved something similar before. Unless you have ideas @vinx13 ?. I am not sure I agree with this idea of making a class for modelselection parameters....we will have to discuss a bit here.\nBut actually, as discussed a few day back in person, let's maybe start with the gradient parameter API refactoring. Yes I remember this issue somehow with the same reproducibility issues. Maybe @Saurabh7 remembers. maybe add a comment that says \"Instantiate object via shogun factory, called by string\"...this might be something where other stumble over ... or maybe it is self explaining. What do you think?. is this python2 and 3 ompatible? I remember something changed in iterating over dictionaries .... ah great catch.\nSo how does this work? sg.kernel docstring now is taken from the actualy \"kernel\" method? If so that is super nice and was something I was a bit worried about. ah yeah, dont worry about it. Could we also move the docstring from sg.kernel to be the one of the c++ method kernel in factory? Or is that hard?. maybe we can add some of the basic things that one would do with string features?\nLike extract the string list?. nono, I mean this one CKernel* kernel(const std::string& name); in factory.h\nWhich is .... non existing... but generally we would like to forward those .... do you think it would be possible to have a single list of those factory methods that is then used? This way we have everything twice? Or is that not nice to read?. I just spend quite some time trying to register get_features with watch_param. Tricky one!\nYou will first need to make the method const (both of them as one calls the other). The compiler error you see when trying to register it comes from const, not from overloading (although it looks like that).\nThen you will hit another compiler error, which complains about bool shogun::any_detail::compare not matching with the call (among others). This comes from the fact that anything that is ever any'ed in Shogun needs to be comparable and cloneable for SGObject::clone/equals. This hasn't yet been done for SGStringList. You will have to add equals and clone to SGStringList (see SGMatrix/SGVector/ for inspiration). Then those compiler errors should vanish. I don't know what happens then, but it will bring you one step closer to making the get work for the list of string features.. @lambday it would be so great if we could port the stat testing stuff to the new API ... it causes problems all the time. I think everything to do that is in place now, it is really just a matter of refactoring the code. Yes exactly. Could this stuff also be done using a list? Might be more tricky but would be better. so this already does what @sorig described in #4128 ... great!. why is this necessary to make it work?. as in \"what breaks otherwise?\". I am being picky here, but what about just having to write this once, e.g. just \"distance\" and then \"_distance\" is automatically generated?\nOr will there be a case where we want to divert from this pattern?. I suggest to add some docs/comments at 3 places\nOne for overview of what this is doing (code is slightly cryptic), here.\nBoth functions that are called in the loop should have an interface doc (Python style or whatever) intended for devs looking at this in a few years time when all of us are long gone. ehehe indeed :D. Maybe write it in a way that it makes it easy to customise the hidden name but default is to use the _ prefix. Mmmh the error doesn't make sense or? distance WAS provided....\nMaybe this is worth investigating or would you say let's just move on? :D\nIm on irc atm btw :). btw can we re-use code from the non-const version in here? Like one calls the other?. long time ago this comment. But basically we want to start updating model states inside the loops for iterative machines\n@shubham808 comments?. what is the story with this change?. why this?. @shubham808 you should also review/discuss this as some of your classes are changed (better API). I assume this works now?. thx. note that clone and equals are implemented in SGObject, so no need to override them here, unless there are problems with the parameter framework (in which case it would be better to solve them). as above. sematic error: this returns after having compared the first string in the list. I guess you wanna return false of one string is different.... \"iff alphabets or feature vectors\". also the alphabet is not really compared in the method .... we would want a test for SGStringList ... all shogun objects are tested for clone and equals in here. There is an issue here, which @avramidis ran into: If none of the getters above lead to a result, then the method will claim that the parameter doesnt exist. But in his case, this is since there was not internal get_char_string_list method defined yet, thus none of the methods here matched. What happened then is that obj.parameter_names() showed a name, but obj.get(name) claimed that the parameter didn't exist. I wonder what is the best (most user/dev friendly) solution for this: Write down the types that were tried? Bad since those could be many and this confuses the user. The current way? Confuses people if a getter is missing. I would be afraid that this pops up a few years in the future (someone adds a new type) and then someone has to spend hours figuring this out. Any suggestions?. One idea I have is that we have some kind of API that can be queried for the string type information of a parameter. Then we could print something like: \"Could not get parameter {}, there is no getter for the type {}\".format(name, obj.param_type(name)) or similar.\nI wonder whether that would work/help?. or what?. yes this is cleaner!. > In that case can add a check name in obj.parameter_names(), and if it is say that there is no getter in the Python API that can be used for it?\nyes that would be a minimal solution at least. Then a dev will immediately be able to figure this out. But actually, being able to ask what type a parameter is (in string form) might also be something that could be useful or?. Is it possible with that? I\u2019m not sure. But at least what you wrote above is good: two messages, either saying Param doesn\u2019t exist (and actually then no need to try to dispatch) or say that python api doesn\u2019t support getting it.\nThis is in fact independent of whether we can get a type string, which would just make it easier to track the source. For now, yes\nWe might add a version with random content soon. Yes let's put that in. I think it should be part of the swig code, not the libshogun (not useful there). i.e. in the same way as the c++ code done in shogun.i and friends. for reference, we discussed this in irc and it comes from using explicit ctors AND kwargs, which for now has to be avoided. @avramidis this is the new way of listing getters and here is where the get_char_string_list has to be added (once this is merged of course). Cool this is much nicer now!. do we actually ever use floating point strings?\nI only were aware of bool, char, and maybe uint32. in general, pls avoid adding such whitespace changes to your commit. I usually use some fancy tool like gitkraken to selectively stage edits. The API is a bit awkward with this double \"get\". But not sure what would be the best way to avoid this apart from plain renaming. @lisitsyn you have some ideas as our API master?. indentation problems. copy constructor should be shallow copy, yes. \nAnd yes, good catch, when we use explicit initialization and then call init, everything is overridden. This is why we usually don't do it, but rather do all the initialization manually after the init call\n@gf712 yes if the memory is freed, then this is undefined behaviour. This is the whole motivation why we added SGVector .... back in 2012, this used to be float64_t* and int32_t vlen as a member variable and had the same issues. We should probably get rid of those raw memory things for strings as well .... just somebody has to do it :). maybe we should register this under \"num_vectors\" and \"features\" to avoid the double \"get\" mentioned above?. I think it should be possible to forward declare this or?. this was forgotten earlier and led to problems?. why this?\n. indentation problems. The CI build will give you a command to automagically fix this. btw why is the cast to SGString<T> necessary?. maybe one thing to add would be if the pointer address and the number of strings is the same, then we can return true straight away. minor: \"Equals method\" (no need to write the type). doesn't swig take care of this automatically? I.e. is this strictly necessary?. is this safe?. This test is kinda bad. Maybe we should just delete it and work on the non-empty tests for all sgobjects instead?. Same here (remove this). I would however love to see a test for clone/equals of SGStringList as those are very low level operations. And we should probably do these tests for all legal template arguments using the googletest magic.\n@gf712 can help here. General question. Does any of this affect the core PR changes? Because in general, I would prefer if we did those fixes in a different PR and keep this one strictly for adding the get_features of string features .... just to keep it a bit more tidy. I think java.json needs to be updated to that it doesn't translate CharVector to DoubleMatrix ... java swig in fact has typemaps for SGStringList, i.e. it is able to translate this straight to a java data structure, done here\nNoticed that when staring the the CI error here. ah yes, here we have the offender. You are getting the CI errors since these methods are only defined within  #ifdef SWIGPYTHON. @gf712 can give instructions where else to add this.\nBTW once #4470 is merged this will lead to a merge conflict in here. Depends on what is done earlier, but it should be easy to resolve it. \u4f60\u662f\u5bf9\u7684\uff0c\u5bf9\u4e0d\u8d77. Sorry I actually meant to ping @shubham808 ;)\nBut yes @Saurabh7 we are intending to do that. Ah yes. It is actually awkward as \"features\" is the registered raw array of SGString ... sigh.\n\"get_features\" is actually a bad name as it simply converts the array to StringList. \"string_list\" might be the best name.\nEventually, it would be best to somehow refactor/unify all this. But not for now. Now is just for adding the string list to the API\n. That\u2019d be ace!. \u201cOf type\u201d maybe?. I\u2019m unsure about this....\nWe would want simple c++ code that is then exposed to swig which does the mapping. In principle.\nMaybe we should rather work on getting the typemaps to work?. Also much less code..... I\u2019ll need to check where that is and whether it can be changed.\nMaybe we\u2019ll have a snowball effect of moving c code to swig. It depends on whether swig actually has typemaps for maps. And I also remember that sometimes swig was a diva with matching those, eg we had to move stuff to shogun.i as otherwise it didn\u2019t match (some order problem)\nEven if there are no typemaps for map I think it would be better to just write those ourselves and keep the code general here.. typo \"not\". yep. agreed. Changing strings is a whole big other story. @shubham808 maybe have a read of those changes since you wrote the original code. one option. The printed thing looks ugly indeed. we actually discussed that the parameter names is not part of any libshogun API and thus not really needed from C++ perspective (it used to be when we didnt have non-owning any policies).\nSo, following the idea that interface code should not part part of libshogun, I think we should have both as part of swig. And there, yes I think we should have both the map and the vector. However, I am against using CMap as this thing should not even be there. Instead, we can have a getter for single parameter descriptions (this is fine with your intended use as well). \nFurthermore, we could then either have Python/specific lang extensions that turn it into a local datastructure (such as an explicit typemap for std::map in Python) or whatever if we want.\nI just think it would be so awkward to actually get a CMap instance from the Python interface when asking for descriptions. Then rather have a list comprehension [obj.get_param_descr(name) for name in obj.parameter_names()] or something.\nOh and finally another solution could be to just put the descriptions as a vector in the same order as the parameter names.\nAll these solutions are ugly :(. hide it from swig as well #ifndef SWIG // SWIG should skip this part. update.\nJust saw you already wrote a cmap typemap for python, so I guess this already works well.\nI am still a bit skeptical as this makes the interface between python and the other langs (that dont have the cmap typemap) different, which is something we usually want to avoid. Having a single getter avoids this problem at the cost of the loop necessary if users wanna get all descriptions .... Not that bad in interactive environments like Python.\nAt the end of the day, this is a question of taste I guess. just curious, I always thought that emblace is more for cases where one calls a constructor when placing the element to avoid creating a temp variable. Is there a benefit here as well?. Not sure I understand this error message, the second part. ah maybe \"Not a shogun object base type\" or something like this?. doxygen string would be good, like what is this method doing.. ah sorry I didnt see. i would just saw shogun as swig users might not know what sgobject is :). agreed!. I am not so keen on this one....but did you actually use it?. should also move the base types here,\nhttps://github.com/shogun-toolbox/shogun/blob/develop/src/shogun/base/base_types.h. some unit tests would be good here. Ah! Of course, you can actually even use this as the \"as_machine\". this will not work as we cannot extract the arguments yet and you would need to pass the second argument to the machine factory. we can do this once #4490 is solved. This type here should be \"Pipeline\" as that is what the builder returns (and now Pipeline is part of swig as well). change to cross.put(machine(pipeline)) and it should work\nwe can change that to put_machine later once #4490 is in\n. OT: Can I suggest that we don't provide features and labels to crossvalidation but instead as parameters of CMachineEvaluation::evaluate(CFeatures*, CLabels*) ? Different PR though\n@lisitsyn @vigsterkr @iglesias . that would be great... if you do that also make sure to use it from at least one other test, and then maybe even create an entrance task for people to make use of this in all shogun tests ..... mixing CamelCase and under_scores. unsure about the naming as \"typemaps\" usually is used for the swig ones within shogun. but that is quite minor. this is sooo nice now!\nshould have an entrance task to do more of this!. yes definitely!. from compile time safety to runtime safety ... :/. we could also add the CBaseMulticlassMachine to the shogun  base classes, but I am not sure what we get API wise?\n@lisitsyn you wrote that, you have any opinions?. I\u2019d like to pass it as function arguments if the evaluation function rather than as fields before that..... We don\u2019t need this anymore now right?. Not needed abymore. Can\u2019t this be in factory?. Is that needed?. @lisitsyn pls comment on API suggestions for this part, as your wrote the thing IIRC :). @lisitsyn also would appreciate your input on the changes in this file. :laughing: . exactly. mmmh....not sure if it is worth it then?\nAs we can alway declare a new list in case it is required ?\nHow much code and how messy are we talking? :). minor. the ==== should be as long as the text. This is not exactly true anymore, as we need to call the factory now. I suggest we just avoid this and say\n\"We next create a cross-validation instance and pass the generated pipeline.\". @gf712 would you be up for making the definitions of the factory methods a bit neater? :). Do we need to implement this for every enum type? :scream: ?. @sorig @lisitsyn any ideas how to make ctags read function names constructed using the double ## in preprocessor, see here ?. @vinx13 I removed this as it is not really needed, one can use machine (I renamed to as_machine in this PR). is log defined?. ++. something is wrong with your git now, as this is from another patch. mmmmh....\ngenerally I would prefer import shogun as sg and import numpy as np\nBut dont worry about it in the old school examples. why this?. not sure why this is needed?. Actually, just checked, not sure factory.h can even be made that much simpler?. ok. could you name this \"fisher_lda\"?. rename to \"kernel_pca\". rename to \"log_plus_one\" ... you get the point of how we want the filenames to look like :). random_fourier_features.sg. Actually, I have a hard time believing that there was no single test that ran the kernel constructor before ...\nEDIT. I just checked, none of the base classes register a parameter called width, so why would this error appear?. I have doubts here as well.... the CI never complained about this but we actually have some test for fisher LDA somewhere so it was compiled before\nEDIT: Just checked, the class is used but not included. Could you forward declare it instead?. maybe briefer: is_sg_base(const std::string& param_name)? Or is that confusing?. ++. @lisitsyn comments?. actually, could we make this a function in shogun namespace, rather than a member?\nAlso, would it be possible to do that with templates then? :D. pls don't commit such whitespace changes. In general, never git commit -a but always add changes one by one and check what you are sending.. pls dont include these. I don't think you need the parenthesis here.\nAlso, are we really sure that the features are float64 bit?. auto. Pls remove debug statements within loops, they spam the output. I think there should be a SGVector based API for feature vectors that you could use here. This pointer based one is deprecated. I don't think the code to compute something should be in here. This is just a method to store a \"smaller\" version of the features, namely only those needed to apply the model to new data. If you need to compute things in order to do that, it should happen in a helper method. pls use std::copy or similar for those operations, it is faster. you should add a unit test that\n1. Trains the clustering\n2. Applies to test data\n3. Calls store_model_features,  asserts that now the features are different\n4. Now applies to test data again and assert that results didnt change. you should leave a comment on what type of algorithm/approach is used here to store/compute model features. did you check whether this eigen issue was solved recently?. mmh does this make sense to guard this here?. ah nevermind. not sure I was thinking straight when I wrote this but I had in mind an overloaded function that you pass something and it will give you a bool and we specialize the template for sg base types. However, I think with swig this will just give headaches, so maybe the current thing is best. KMeans is another algorithm. I just checked Hierarchical.cpp and it does not fix the features to be 64 bit float, but rather just calles the distance method of CDistance. So we will need to think a bit more here in order to not make the algorithm only work for those 64bit dense feautres. What about 32 bit? What about sparse features?What about string features? But let's address the other points first, this one will take some time and effort. are you sure this is a good idea?\nBecause binary labels already have this mechanism. Why wouldnt we re-use the same memory?. this shouldn't be on by default imo, as it is computationally expensive to compute the confidences. I think a boolean flat would be best.. what about \"gp_likelihood\", or just \"likelihood\"?. gp_mean?. yes, but that is fine for now I think. thanks!. i am not sure this was working before. What is the rationale here?. yep good point! :). we generally want a relative link here, not to the latest version. also it would be master branch (release) ... but since all that is complicated, we prefer the relative link. why not install.md? . same here, relative links to the INSTALL.md of the branch are preferred. thanks!. Like a method with one argument and bool return. It takes a templated argument and returns false. And then it is spezialized for shogun base classes and then returns true. Not sure that would work :)\nIf it is only you who needs this the current approach is probably fine as well or?. I remember writingCBinaryLabels::scores_to_probabilitiesages ago which used the \"distance from hyperplane\" to compute probabilities/confidences for SVM predictions. So it stored the scores along with the actual label. This is the mechanism I meant. maybe the field can be renamed? \"array\" is a bit dull. yeah looks nicer doesnt it? :). so the pattern you used for pipeline also works here then? That is nice. maybe we should rename this into \"Differentiable\" and \"as_differentiable\" in the future? What do you think?. whether predictive variance is computed in predictions. ++\nJust the \"confidence\" field vs the one in binary labels needs sorting out. just asking, is this covered by any tests that would detect changing results?. yeah nevermind, I was just fantasizing :). lol well I guess nevermind then :). Just he one more look.\nThere is the thing that only numerical values/vectors etc are serialised for the testing, not the objects themselves. So in this example here, the transformed features (matrix) are not part of the test. But I think they should. So if you extract the feature matrix, it will be serialised. Still todo?. Does this maybe cause problems? The original string is destroyed?. ++ for this one!. could we name it \"init\" to avoid clashes between brits and US? :)\n. rather than calling it here, why not call it just after the lambda was attached to the parameter. Then it would be like \"normal\" parameters being initialised, usually in the private \"init\" method where they all receive defaults and are registered .... I thought about this a bit more. Would it be possible to make this a normal function that is defined somewhere else (inside the same file)?\nI think putting the lambda here is a bit messy, as then we mix registration and computation of auto values, where those probably are better separated in the code or?. also we could avoid the clunky formatting?. we need to think about a way to document what \"auto\" here means. In particular, I think the strategy/function that computes the default value should come with a description string that can be queried by the user. Any ideas for that?. sorry I have to take this back, obviously what I wrote doesn't work as the features are not available.. so devs need to be careful and put this everywhere where the member that the value depends on would be changed ... for kernel that is clearly \"init\" so all good. Maybe we should make it impossible to change \"lhs\" and \"rhs\" via put in that case.... thoughts?. in a perfect world this would come in a different PR. But leave it in for now, just mentioning for next time. Another reason (and suggestion): it would be good to havepoly_kernel.put(\"degree\", auto_param(\"num_feats\")` as mentioned in my other comment. For that we would need\n\nthe function being isolated somewhere so it can be accessed\nhave an auto_param factory so that users can instantiate those strategies, which results in marking the parameter as auto as you did here. So the auto parameter function would need to be registered somewhere so that it is available to users.\nthis means that there needs to be a base type to represent all auto strategies that could be used\nif I get a string representation of the parameter, say in Python, it would write PolyKernel(degree=auto_param(\"bla\"))\noverload put to accept the type that is returned by the \"auto_param\" factory, and then register the instantiated function/pointer\n\nObviously, this is some work. Although it is not that bad. Doesnt all need to happen in this PR, but you see now why I don't like the lambda in there too much? Shall we discuss it a bit in here?. ++\nNo need to have a marker (get_output) as this is not referenced in a cookbook, but no need to change now. One thing is shouldn\u2019t we use a typed interface to modify those here? As we know the type?. Rather than writing code here, I would always just say that the values are stored in the scores/values vector of the predicted labels. no Leaks from refactoring this?. no I mean: if you would have a typo in \"current_values\" this would lead to a runtime error (bad). So why not use a typed setter or the current values or modify them otherwise using the Dense/Binary Labels API that is available here?. unclear :) You can also merge it as it is... no big change later. I think actually a parameter property that is \u201cread-only\u201d and then a generic check in put might be a good idea. Then we can \u201cforce\u201d users to use the unit method if they wanna use a kernel (and the auto parameters are safe). This is a pattern we had used before but with constructors and the lack of setters. Thoughts?. I mean a description for how the auto is computed. I think the description should live at the same place as the function that computed it so It can also be queried. Yes (no need for a separate file though as the auto strategy can be part of this class/plugin)\nI don\u2019t really understand why a lambda is necessary, why not just a function but that is a detail I guess\nMy main point is that things need to be structured in a way that later on one can easily add user access to change those strategies and documentation . Does this even work atm? :D. k_name?. we reserve camel case for class names. minor. this enum won't be available here anymore in the future..... so maybe we should already use ->as()?. I think here you can use the actual getter for lhs (if it exists) as the kernel type will always be known. This way we know for sure there is no runtime error if the parameter is renamed or a typo happens or whatever.. CKernel::get_lhs. I guess costs always are in a context. In this context (kernel machines) they are neglectable. But actually you are right if feature classes stay in the main framework (as opposed to being plugins) we can just keep the enum here.. I think I\u2019d remove this and just print and error if the requested parameter is not an enum type. This method seems good like this. ++. Would remove and just keep the below error. Same as above,  only confuses users, it\u2019s more an internal thing. Thinking this further: I wouldnt even talk about enum mappings, but just \u201coptions\u201d. Users don\u2019t need to know that there is an underlying enum. Use the enum value here rather than the int to make it readable, and they are available here. Actually, since this happens at runtime now, we could rather have one options map for each class, stored locally, and then a method \u201cadd_option(string, int)\u201d that is called this method here. Or even better\n\u201cadd_options(string param, map)\u201d which then can be called by directly writing out the map as {\u201cfoo\u201d: SOLVER::FOO, \u201cbar\u201d: SOLVER::BAR} in a single call\nI didn\u2019t realise yesterday that the map doesn\u2019t need to be gobal nor static. Mixing using override and not using it causes warnings iirc. Yes at some point. I would just leave this for another or or the detox gsoc project. Or maybe even: Do you think using a macro, we could avoid having to write the enum value name AND a string ?\nLike\nADD_OPTION(param_name, FOO); which adds the mapping \u201cFOO\u201d->FOO where FOO is one option of the enum?\n. This means we\u2019d have to write things one by one but on the other hand it avoids the redundancy of explicitly naming the option string. Typo. yes, c++ lacks quite some things ... :scream: . super minor: maybe \" ..for parameter %s::%s\" and print classname. long live the preprocessor :D. it is so funny that this old school stuff still is necessary/useful in places. pls undefine your helper macro after having used it. could you rather say something like \"illegal option %s for parameter %s::%s\" ... ?. whitespace going wrong here. sorry about this, but what about actually merging all those lines into one with a multi-argument macro?\nSG_ADD_OPTION(\"liblinear_regression_type\", L2R_L2LOSS_SVR, L2R_L1LOSS_SVR_DUAL). I see, well ok. In c++ it would have to be int as the enum definitions go hide in the plugins. So quite unsafe actually. No harm in keeping the get for that for now but also offer the one that returns a string.... need to see how it plays out. How does the output look like?. Sure thing. This doesn\u2019t look too bad now! :). I think we should also do a runtime assertion that the parameter was actually registered, so that only options for already registered parameters can be added. \nAnother point would be to assert that the registered parameter is in fact a machine_int_t so the we cannot register options for other types of parameters. No need to have the cache size... but don\u2019t bother changing... just for next pr. Is this public?. This would be removed or?. Yes, to further remove redundant code. Just realising. We might at some point have string parameters..... Looks good!. Should put a comment saying that both are the same. Yes should be. Just via changing example and the target Json filed to potentially replace some method call strings. ah these are smart, didnt see them before. i think it is fine, we can do that if needed. I think there might be a bug here: when the columnwise std-dev is computed, the column-wise mean would have to be substracted instead of the global one. Or am I confused?. minor: might be worth to adding a filtering method to those enums, so that we can ask questions like (\"is this a CDotFeatures subclass?\"), then these checks are  a bit less verbose. nice! :). You can use a linalg::add variation here . Pls separate line. This needs to happen in every ctor. Most importantly the default one. So best to move those to a method. @shubham808 is this necessary in subclasses?. Linalg here as well pls. No need to add those ctors really as it all will be done via default ctor and then put calls. I don\u2019t think it\u2019s good as then we\u2019d have the same code in every subclass. Plus if someone forgets to do this.... can\u2019t this be something that is done automatically?. Could we maybe name those two a bit better?. Some white space issues gere. nope, but we also don't need to delete those (yet). But for sure we dont need to add new ones. Re the rest. Good that you have added it. One thing I don't really like is that this is almost copy pasted from the another test, i.e. we have lots of redundant code. The other test was more a proof of concept how to test those things. I would much rather have a single test that covers all these classes, either templated (with a googletest type list that it is instantiate for), or via runtime iteration through the appropriate models (faster). Have a look at SGObject unittests for all sgobjects (there is an iterator), and also have a look at the serialized model unit tests.\nI think we should sort this out in this PR here ... . Let's use the test as is for now, since it allows us to fix the error you mentioned, until we have solved the mean issues, and then once that works we can update it.. yes!. rhs should also be read-only. could we remove this method and instead make one that returns std::vector<std::string> like we recently did for parameter names?. why print and not error? also why not REQUIRE?. couldnt we use some environment for regression? Then we'd have less code here. ah sorry this is libshogun example. What about actually loading regression data from a file?. Thinking even further, this example could be a meta example in fact! All of this is possible with the swig API or? Because if so, I would much prefer doing that to a libshogun example (we would like to get rid of as many as we can). in order to make this meta example compatible we would ne a factory for observers is factory.h, have a look, it is pretty self explaining. And then ParameterObserver needs to be exposed to swig (the other sublcasses shouldn't for now). i think the method name is a bit awkward. What about just subscribe?. here you would use put, see other meta examples. no need to have this in the example, irrelevant or?. const?. what is up with the SG_OBS_VALUE_TYPE here?. @gf712 any ideas?. we won't have these ctors available in the meta examples since it will be instantiated with a factory which calls the default ctor. So instead, you would use the add api (see the multiple kernel learning examples for how it works) to add strings to an internal vector of parameter names. we do! see for example #4537 where @gf712 changed it to have nice strings for enum/options. For now, this printing is fine, but generally, we won't to have an abstraction layer for where the generated string is going (we can do that later). \"This class implements a logger which prints all observed updates\" .. much simpler. Keep in mind registering parameters happens at runtime....so in fact you could only register the parameter in the observer once you receive it? Kinda crazy but might work :) Might require some changes. Let me know if you see what I mean. But we had this discussion before no?: there is no reason why the type of continue features is DotFestures. It should be just Features. The reference should be stored automatically in the base class, and the subclasses can check/assert the correct type at runtime in init_model (but not store them here, just check) and then the iteration method can do a static cast as that\u2019s safe then. This copy pasting of ref/unref code is not good, error prone.\nSee what I mean?. Maybe I\u2019m confused about this. Need to check he code my computer ... will do soon. But pls also see if the above makes sense. Moving averages imply that something is modified so I think is good to just modify the given vector wih the new datum. Not even sure we need the result parameter. Would just call the method \u201cupdate_mean\u201d and then have only 3 parameters: the mean, the new vector, and n. Just checked. The continue features are of type Features. This means that all that needs to happen here is the runtime assertion (as is fine, though there also is the enum, which is faster) that the features have the correct type. The code for storing the reference should be in IterativeMachine. Might need some refactoring for the perceptron as well .... No need for the as cast btw here as the property is already asserted above. The way to do that is via \u2018get\u2019 which is available for all registered parameters. Not sure. We know that all observed values have the same type or? So could template this? Not sure.... so The objects that are exposed to the api need to have parameters registered with the appropriate type. Seems like we are pushing the problem around, need to think a bit about this. Do we ever need to put a parameter observer?. Typo. Also could you use a more standard error msg?. Users should never receive an any, it is internal. Rather, they can access parameters with get. Lol yeah that\u2019s possible. But I think it is awkward indeed. Having the get compute something lazily is ok imo as the user wants to have something. But here the user wants to execute something so I think there should be a special method for that, hence \u201crun\u201d which indicates a void method. Ah I didn\u2019t think about the function being non const of course. Another reason to have a special directive (run) as it might change the state of the object.. The pointer already is CLabels or?. @lisitsyn you have an idea how much work it would be to add void functions as parameters?. Do we really need those methods? As there is already a filter method.... If you need them they should be private at least. same here, really needed?. I am unsure about this. Then we would have a double representation of the name. I see you do this in order to pass parameter information around (previously done via TParameter)\n@lisitsyn what are your thoughts?. I think we should probably think again about this method and approach to building gradients ... or?. this is some expensive code!. also readonly!. sorry. I was distracted by the name. What about observable_names ... which is inline with recent changes for normal parameters. Minor though. for that to work, you need to template the class (best is subclass as discussed) and then register the typed parameter.. I think in our discussion, we discussed the first option you described. I think it might work!. Check the definition of the macro\nBut I think we can add something to use string here as well. In what sense? \nActually there is a pr by Sergey that Viktor said he wants to merge so that should allow vectors . Minor I suggest removing the \u201c_param\u201d. Data submodule changes?. What\u2019s the error?\nYes getting strings should work! And t should be easy to make it work. Are this macros still needed with the any dispatch?. Std string ?. I think printing on screen is generally a bad thing to do. Rather print to a stream or shogun io or something..... Magic! :). Pls use a more descriptive name. Docs for devs are needed here. What about storing the any and returning a reference? Not sure about overhead .... could you use the new kwargs style here?\nKernel k = kernel(\"ANOVEKernel\", cardinality=2). these names are a bit odd. What about K_train_train and K_train_test. no need to put those in the examples. what about just saying what it is? w_averaged etc?. actually this name is fine imo. sweet, much nicer to read!. could we make this happen in the IterativeMachine class without the dev having to specify this in every use of the mixin?. @gf712 do we have a central map for these?. Id prefer offering two constructors here, one for each case, but that is a minor detail. good idea moving this into a new file!. ok! This is definitely better than before. I have a question: If a test fails, is it easy to see which class is the offender?\nFiltering the tests obviously is not possible via this though. Compile time is an advantage to the typed version of this we had before (we define a bunch of types and then essentially template the test). This then allows for filtering. Maybe in a follow up PR, we can do something about that. I think the current blocker was that it is hard to find all subclasses automatically. However, if we simply define a type list (as you have done here with strings), that is also acceptable for now.\n@gf712 might also be interesting for you. there should be yet a typed setter for this or?. since this helps to make the tests for reliable (no runtime error when setting things as here), we can keep the setter (even though it will be hidden from users in the future). Cool add the scoping!\nYes we can do this extension, but for now the explicit list is fine.\nCompiled types can be helpful but for now this is good!. Yes there was a discussion about this in the pr that added it. Solution: check the options first and if they don\u2019t work just call the standard out method.\n@gf712. Let\u2019s not touch the old parameter code. Instead, don\u2019t use sgadd here but rather register with the watch method used inside sgadd. This means that the parameter will not be serialised for now but that\u2019s fine for these observer classes imo.\nYou can register strings with the new framework. Not quite (the pr disables testing so I\u2019m not sure what will happen with it) . But what you could do is to cherry pick Sergey\u2019s commits that add std vector support in your pr here.\nOr in fact we can just push those to develop independently of this pr or the other pr. let me do that in fact. Oh and then also here: don\u2019t use sgadd but directly register the std vector. Solution: don\u2019t use sgadd but register with new framework directly. Can you mark left and right hand side parameters as read only? And then we force users to use the unit function, just like done in CKernel.\nSorry I didn\u2019t say before we just added this option . Cool! Does it work?. minor: formatting whitespace issues. minor: I wonder whether this additional implementation for bool could be avoided via wrapping the memory block with SGVector<T>(vec1, vlen, false) and then using linalg::dot? (since that might have the dot product for booleans implemented, but not sure actually. +1. So what is the replacement here?. Don\u2019t expose some to swig yet. We can do that soon but it\u2019s not ready. You will need to use the old skool approach for now . So just observed value raw pointer in the api. I think this loop could be replaced with a one liner from the std lib. What about renaming make_observation to observe (shorter, neater). Isn\u2019t NONE the default? If not shouldn\u2019t it? @gf712. Did you check the overhead of this\n@lisitsyn thought?. Make this shorter: \u201cTemplated specialisation of ObservedValue that stores the actual data.\u201d. This is not good. The class won\u2019t be available here anymore in the near future. Need to do either visitor pattern or virtual methods but no explicit casting. Please use the shogun int32_t to avoid sign warnings (need to explicitly cast and take care or overflows). Good point :). Class definition bit within scope. But actually that is not true, sorry.\nHowever I generally preferd if you used the to string method of the respective class, or a new method that prints this info, better style than having all the classes listed in here. Need some checks here as sizet is unsigned\n. Add a virtual method bool CSVM::supports_mkl() that returns false by default, and then CSVMLight overrides this returning true. Then you can all that method. A simple solution would be to add print_machine_information to CMachine. But then again, we have to_string actually. Of course there could be a curated version, for machines that return only the relevant fields. Whatever we do, I think we certainly should not dispatch machine types in here.. size_t is unsigned so it can represent numbers that cannot be converted to int32_t. That is minor as there likely won't be that many observations. But the compiler will generate warnings about such implicit conversions. No matter how the direction is, we will need to add something to the API/parameters of those classes to communicate. See also my other comment. The issue is that we want to go for a plugin architecture of shogun soon. This means that the specialization of classes are not available/visible to the base classes anymore. Only base class interfaces will be. Therefore, this stuff will need to be communicated via the API or get calls. This type should be hidden from swig / interfaces or?. IMO, we souldnt add this many new types to the API. Instead can't we use put/get to retreive this info?\nmkl_obs.get_observation(0).get(\"value\").get(\"fold\", 0)\nEven better if the get(\"value\") wouldnt be there but the values would be directly accessible from the value\nmkl_obs.get_observation(0).get(\"fold\", 0)\n@lisitsyn we need a get for array elements, similar to the add we have. Maybe we can just do this as above, with the additional int parameter in get?. ok then remove this here. as said above, I think we should try to hide this type. Exactly, new SVMs would just have to serve the base class API. This is the reason why we cannot have central structures for those plugins/classes anymore -- they will only be known at runtime. ah I am not so sure about this. I know it works but it is asking for trouble. As far as I can seem the SVM is never explicitly casted to LibSVMOneClass (or am I wrong?) So I would really prefer some method in CSVM that indicates support for whatever the MKL class wants to do/check.. if you want to inline things use SG_FORCE_INLINE. I see thanks!. my bad didn't see it. The reasons why this assertion is in here seem dubious to me. What you say makes sense, checking that the SVM is one class. So let's just add that method to CSVM.. just remove the obtain_from_generic method completely. Wherever it was called, replace the call with ->as<CMKLClassification>(). i think there are some whitespace issues here. pls also use your interleaved optimization method here, not the name. minor maybe change to \"one-class or not\"\n(since we can have binary and multi class). why this?. this is not used, so rather remove it. no that would be quite different ... . I cannot find it in the diff. since the features are only returned by base class in the future, we don't really need this anymore\nI guess there is a question here: Are feature types part of the core shogun or will they be plugins @lisitsyn @vigsterkr ?\nThe current approach in the new examples handles all features by CFeatures only, but I now just started to wonder whether we really want to do that? Certainly reduces the number of types ... (RealFeatures, FloatFeatures, ... for dense, string, sparse each). Actually, I just rememberd that we have an enum for this: EProblemType defined in CMachine.h  ... this can be reused here rather than adding some of the methods (interleaved needs to stay I guess). will that work if the type SVMLightOneClass is not known? (plugins). any idea why we have PT_BINARY and PT_CLASS?. it is what EProblemType is for I think. The dependency is kinda unnecessary through the ctor only if you look at it (and should be removed imo). In general, the MKL stuff should work with pretty much any SVM solver as long as there is some interface defined. . User facing error msg pls. I.e. in the lines of REQUIRE(\"%s must contain less than two unique labels, contains %d. Cannot convert to binary labels.\\n\", orig->get_name(), unique.size());. I wonder whether we always should print a warning with the conversion table in case we convert (especially if we change the representation). As doing it implicit might lead to confusion. Thoughts?. Why assert that it is NOT regression? Rather than asserting it is a certain type?. @gf712 will have some comments. Not sure we need this one?. This is a property that shouldn\u2019t be serialised as it remains unchanged during the instance life. So we can have a property for that maybe?\n@gf712. \u201cShogun needs to be compiled with svmlight enabled in order to use %s\u201d. Very good idea. Actually the best thing would be to just not register it I now realised. No get/set/clone/equals/save needed. Ok sure, but let\u2019s not forget about it. But why does the specialised class need be exposed and not the base class? Get is sgobject. My point is: the CV storage type is not needed in the interfaces, just a getter for the base type and then the user can use \u2018get\u2019. Yes we actually need to do this. Tag framework is the way to go. Reason is all these things will be plugins at some point and we also want to make swig api stable. Yep let\u2019s add it. I\u2019ll be travelling tomorrow and will have a look.. Force it inline. Exactly in those lines! Would be good to print some debug info on the actual values maybe?. Sorry the storing any comment should have been here. Can you change to: no kernel provided. do you need this, median of features rather than the matrix?. this is not quite precise. I would actually ditch the log in the conversion and rather convert from $-(x-y)^2/\\gamma$ parametrisation to $(-(x-y)^2 / {2\\sigma^2})$ (and back), i.e. there must be a square root and a division by two. Then add a log around this explicitly. The log parametrisation has different reasons, and also putting/removing a log is trivial and doesnt twist your brain.\n. Oh and whatever it does, make sure to add the math in the doxygen so it is clear. technically might would be possible to compute the median of upper/lower triangular values without copying them. But I am not so sure that is worth it? . However, I think what I would like to see is some analogue to the scipy way of computing pairwise distances, with a function that returns the distances as a vector, and then an \"expansion\" function. This way you can call the vector function here and then sort that directly, and you save yourself from ever storing the other half of the matrix. This also solves Viktor's comment above.. for (auto i : range(lhs->get_num_bvectors())) ?. It would be cool if we could not have explicit loops for traversing triangular matrices everywhere, but rather do this via a utility iterator or something. See comment on the pdist as vector above. some food for thought on fast median computation: https://www.stat.cmu.edu/~ryantibs/papers/median.pdf. I would generate a set of numbers with multiples to be a more \"realistic\" case, but I guess that is minor. this should also be hidden from swig or?. return value or reference?. Store value or reference?. do you need to try catch that here? We could just make the exception string in viktors code more verbose (printing the provided value and the ranges or something? Not sure I get why the error msg is here ...?. This is in fact quite a bit neater. We should have thought of this in the first place :). Ok good, but this now deals with the issue properly which is always better than adding more and more hacks. Actually, technically we only need one unique label. For training that never makes sense. But I think evaluation methods might complain since they also convert. Think test labels all are positive (totally reasonable). So this needs a modification.. Ok so this brings up another issue: What if the training labels are somewhat different than the testing labels...will the conversions be the same? If not, what do we do?. I think the grid-search thing would need to convert only once at the beginning anyways as otherwise  things will be inefficient ... and also might be buggy (see comment on conversion mapping invariance). So for those one off cases, I would rather warn the user or?. going with making them plugins for now .... this is a bit annoying that SWIG needs the template instantiation in order to realise base class hierarchies, but we can delete this once the API conversion is completed. this is a long list of code (in swig) that will not be used anymore.....directly editing feature objects in shogun in the style of\nf = RealFeatures(mat)\nf[:,1] = np.array([1,2,3], dtype=np.float64)\nwont work anymore. But we want to make feature immutable anyways .... this is a temporary add so I can make the examples work with sparse files. We would later on just give a filename/file and the factory figures out the type automatically so that we just have sg.file. Mmmh\nI wonder whether we could do the conversion on the base class just like we did the dense feature type dispatcher.\nBecause you are of course right . Yes. In fact the method for the full matrix would use the condensed form and expand ir. Yes just wanted to share it. Probably fine. think so. Ah ok it is a warning, so just leave this, nevermind my comment :). Actually, I think the ctor should also accept a description of the observation, rather than just putting it \"Value of the observation\". So when observations are made (especially for the custom observations like the CV one), users can actually get a one-liner what this is about? What are your thoughts on that?. yep! copy paste error (didnt compile yet). yes, CDynamicObjectArray takes care of that. We might want to catch the expection and make a pretty printed version since this is user facing though. Will add that. ",
    "haipengwang": "Thanks for the comments.\nI made some small updates on GMM.h:\n1) Add the missing \"public\".\n2) Define some values to keep the EM training normal in logarithmic scale.\nAs for the , I think it is still useful since the GMMModel class contains read and write functions: GMMReadIn and GMMWriteOut, while SG_WARNING() and SG_ERROR() should be very useful when outputing error or warning information.\n. Thanks for the comments.\nI made some small updates on GMM.h:\n1) Add the missing \"public\".\n2) Define some values to keep the EM training normal in logarithmic scale.\nAs for the , I think it is still useful since the GMMModel class contains read and write functions: GMMReadIn and GMMWriteOut, while SG_WARNING() and SG_ERROR() should be very useful when outputing error or warning information.\n. Yeah.\nIt indeed strikes me too.\nPossibly the covariance for our machine learning world is not big enough, and thereby the probability for we machine learning people to consider similar thing is relative high:-)\n. Dear Soeren Sonnenburg,\nThanks for the comments.\nI will try to incorporate CKernelMachine in the implementation.\nActually I think gaussian-kernel based parzen window estimator has very straight-forward formulation, and I think it doesn't cost too much to direct write a simple function to calculate gaussian-kernel distance, so in the beginning I don't consider incorporating CKernelMachine in the implementation.\nBut I agree with you that using CKernelMachine will make parzen window class more generalizable, and it is easy to generalize from gaussian kernel to other kernel functions.\nBesides, could you please tell me more about the naming convention in SHOGUN? Thank:-)\nHaipeng Wang\n. On the other hand, I want to remind that the window functions used in parzen window has a strong requirement:\nthe integration of the window function should be equal to 1.\nOnly a few functions can be used as parzen window function, such as rectangular function, triangular function, gaussian function, and so on.\nAnd many kernel functions usually used in classifiers (SVM or ANN or some other classifiers) is not suitable for parzen window.\n. Actually gaussian kernel function can be used for parzen window function.\nBut parzen window function isn't equivalent to kernel function.\nSo I am thinking whether it is really necessary to do this generalization using CKernelMachine.\nOr we can define another window function class which provide these suitable windows functions, such as rectangular function, triangular function and gaussian function.\nThanks:-)\n. But anyway, I will change the coding style of \"if else\" and modify the line \"dis = dis/m_fGaussianConst\" to \"dis = dism_fGaussianConst\".\nSorry that I am not at my personal computer, and I will correct them and git push them asap.\n. Oh, yes.I should use shogun features rather than just use the raw feature vector pointers.\nAs far as I know, shogun CSimpleFeatures and CDotFeatures classes provide many flexbile interfaces to utilize feature vectors.\nJust for using shogun features in the parzen window class, we can use the function \"get_feature_vector\" to get feature vector pointers.\n. Thanks for the advice, I have updated the comments type.\nThanks.\n. Sorry that I didn't notice the naming rules in shogun.\nActually I always followed some naming rules. For example, integers are named as n_, real numbers are named as f_ and pointers are named as p_* or pp_**.\n. You are right. Thanks for the comments:)\n. Thanks for the suggestion.\nActually in the beginning, I was a little confused whether to derive ParzenWindow from an existing class or not.\nParzen window estimates densities or distributions in a nonparametric way, so there are actually no parameterized model and there are no explicit model training process. Possibly some model-related member functions in CDistribution are not suitable, such as train(), get_num_model_parameters() and get_num_relevant_model_parameters.\nOn the other hand, KNN is also a kind of nonparametric density estimation method. But KNN can also be considered as a kind of nonparametric classifier, so in SHOGUN, KNN is derived CDistanceMachine, which I think is not suitable as the base class for ParzenWindow.\n. I will use a new branch.\nThanks.\n. Yes. dis = dis*m_fGaussianConst.\nSorry for this mistake. I will correct it and git push them asap.\n. ",
    "Tankiit": "Previous one had errors...\n. j1 is the bessel function of first order \n. ah the one i used essentially did not change the alpha.....just saw the commit by Ziyuan...thanks \n. j1 is Bessel function of first order \n. Hi essentially they are the same if you do some quick derivations. j1,j2 etc represent the order (http://en.wikipedia.org/wiki/Bessel_function#Bessel_functions_of_the_first_kind_:_J.CE.B1)\n. ",
    "lionelc": "Hi sonney2k,\nThanks for the comments. Now did the same thing for more classes in libshogun/kernel, and thinking about the following:\n1. For complex types such as CKernel subkernel (as in TensorProductPairKernel.cpp), they are not supported yet using the exiting functions in libshogun/base/Parameter.h ... Should there be a general function m_parameter->add(...) that supports such types?\n2. Another issue is: for a kernel which inherits from another kernel, the original parameters in the parent class also form a part of the parameter system. But such parameters can't be explicitly added for serialization support because they inherit from another class... I think this should be solved by doing something in base/Parameter.h and re-write the register_params(...) function.\nRegards,\nLei\n. Hi sonney2k,\nThanks for the comments. Now did the same thing for more classes in libshogun/kernel, and thinking about the following:\n1. For complex types such as CKernel subkernel (as in TensorProductPairKernel.cpp), they are not supported yet using the exiting functions in libshogun/base/Parameter.h ... Should there be a general function m_parameter->add(...) that supports such types?\n2. Another issue is: for a kernel which inherits from another kernel, the original parameters in the parent class also form a part of the parameter system. But such parameters can't be explicitly added for serialization support because they inherit from another class... I think this should be solved by doing something in base/Parameter.h and re-write the register_params(...) function.\nRegards,\nLei\n. I was wrong... This can work. It actually also appears in DistanceKernel.cpp.\n. I was wrong... This can work. It actually also appears in DistanceKernel.cpp.\n. For your minor comments about the curly brackets (in a new line) and indention problem, they have been addressed and complied thereafter. Please let me know if there's any other coding standard. \nCurrently there is no \"SG_UNSTABLE\" under the directory libshogun/kernel. But for a formal task, the same coding style w.r.t. serialization support should be applied to all the kernel classes and/or feature classes. \n. For your minor comments about the curly brackets (in a new line) and indention problem, they have been addressed and complied thereafter. Please let me know if there's any other coding standard. \nCurrently there is no \"SG_UNSTABLE\" under the directory libshogun/kernel. But for a formal task, the same coding style w.r.t. serialization support should be applied to all the kernel classes and/or feature classes. \n. I believe it can compile now after fixing an error due to the local version conflict. Let me check for whitespace issues.\nAnd yes, I meant to add such a pure virtual method as you mentioned. This may not be the best way but at least it can allow another parameter modular (as the proposed CParameterSetting and CSettingGenerator) to interfere and operate on parameters on top of kernel/features.\nTowards a better solution, I am thinking about the following way (you may let me know if it is feasible):\ne.g. in a kernel class such as SpectrumRBFKernel, the kernel coders just need to add.\nvoid  register_params() \n{ \n    m_parameters->register(var1, type1, var2, type2 ... );\n    m_parameters->register_parent(parent_class);\n}\n. I believe it can compile now after fixing an error due to the local version conflict. Let me check for whitespace issues.\nAnd yes, I meant to add such a pure virtual method as you mentioned. This may not be the best way but at least it can allow another parameter modular (as the proposed CParameterSetting and CSettingGenerator) to interfere and operate on parameters on top of kernel/features.\nTowards a better solution, I am thinking about the following way (you may let me know if it is feasible):\ne.g. in a kernel class such as SpectrumRBFKernel, the kernel coders just need to add.\nvoid  register_params() \n{ \n    m_parameters->register(var1, type1, var2, type2 ... );\n    m_parameters->register_parent(parent_class);\n}\n. ok... it would be more clear if I can draw a tree showing the dependencies between kernels/features/normalizers.\n. ok... it would be more clear if I can draw a tree showing the dependencies between kernels/features/normalizers.\n. checked the codes and fixed the whitespace issues\n. checked the codes and fixed the whitespace issues\n. ",
    "bordesa": "Hey Siddarth,\nThank you for the patch.\nAs Soeren, I would be interested in knowing of you have actually tested your patch on the datasets of the original SGDQN paper.\n. ",
    "Siddharthk": "Thanks for pointing out the mistakes.I will correct them including the copyright section.Will also test the classifier on the dataset of the original SGDQN paper.\n. Thanks for pointing out the mistakes.I will correct them including the copyright section.Will also test the classifier on the dataset of the original SGDQN paper.\n. ",
    "mcopik": "Ok..  So, this problem is about files with vector and matrices? Because I\ncan create a patch, which only add code for nd-array and doesn't change\nanything in vector/matrix functions.\n2011/4/24 sonney2k \nreply@reply.github.com\n\nThanks for your patch. It has one (or two) issue(s) though: If you have a\nplain ascii file that just contains\n1 2 3\nYour could previously load it via (python modular code):\nfrom shogun.Library import \nfrom shogun.Features import \nf=AsciiFile(\"simplefile.ascii\")\nlab2=Labels()\nlab2.load(f)\nprint lab2.get_labels()\nsame for matrices\n1 2 3 4 5\n6 7 8 9 10\nWe absolutely have to support these plain files (as many people provide\ndata like that). I haven't checked your changes for the binary format - but\nI guess the same issue (though breaking here is not as bad)\n\nReply to this email directly or view it on GitHub:\nhttps://github.com/shogun-toolbox/shogun/pull/66#issuecomment-1050993\n. Ok..  So, this problem is about files with vector and matrices? Because I\ncan create a patch, which only add code for nd-array and doesn't change\nanything in vector/matrix functions.\n\n2011/4/24 sonney2k \nreply@reply.github.com\n\nThanks for your patch. It has one (or two) issue(s) though: If you have a\nplain ascii file that just contains\n1 2 3\nYour could previously load it via (python modular code):\nfrom shogun.Library import \nfrom shogun.Features import \nf=AsciiFile(\"simplefile.ascii\")\nlab2=Labels()\nlab2.load(f)\nprint lab2.get_labels()\nsame for matrices\n1 2 3 4 5\n6 7 8 9 10\nWe absolutely have to support these plain files (as many people provide\ndata like that). I haven't checked your changes for the binary format - but\nI guess the same issue (though breaking here is not as bad)\n\nReply to this email directly or view it on GitHub:\nhttps://github.com/shogun-toolbox/shogun/pull/66#issuecomment-1050993\n. I made a commit. It doesn't look pretty, some parts of code are very\nsimilar, but I should be working properly.\n\n2011/4/25 sonney2k \nreply@reply.github.com\n\nYes that would be better. This way there is no incompatible change and I\ncan immediately apply your patch.\n\nReply to this email directly or view it on GitHub:\nhttps://github.com/shogun-toolbox/shogun/pull/66#issuecomment-1051036\n. I made a commit. It doesn't look pretty, some parts of code are very\nsimilar, but I should be working properly.\n\n2011/4/25 sonney2k \nreply@reply.github.com\n\nYes that would be better. This way there is no incompatible change and I\ncan immediately apply your patch.\n\nReply to this email directly or view it on GitHub:\nhttps://github.com/shogun-toolbox/shogun/pull/66#issuecomment-1051036\n. it*, not I - of course.\n\n2011/4/25 Marcin Copik mcopik@gmail.com\n\nI made a commit. It doesn't look pretty, some parts of code are very\nsimilar, but I should be working properly.\n2011/4/25 sonney2k \nreply@reply.github.com\n\nYes that would be better. This way there is no incompatible change and I\ncan immediately apply your patch.\n\nReply to this email directly or view it on GitHub:\nhttps://github.com/shogun-toolbox/shogun/pull/66#issuecomment-1051036\n. it*, not I - of course.\n\n\n2011/4/25 Marcin Copik mcopik@gmail.com\n\nI made a commit. It doesn't look pretty, some parts of code are very\nsimilar, but I should be working properly.\n2011/4/25 sonney2k \nreply@reply.github.com\n\nYes that would be better. This way there is no incompatible change and I\ncan immediately apply your patch.\n\nReply to this email directly or view it on GitHub:\nhttps://github.com/shogun-toolbox/shogun/pull/66#issuecomment-1051036\n. \n\n",
    "sploving": "removed the init_shogun / exit_shogun in ruby\n. I took a mistake. I thought that after defining the new type SGVector, we could use it in JAVA too. But what you mean is: SGVector is just a C++ class, not a Java class. In java, we still use double and in C++ we use SGVector, Am I right? Another thing, could you add some python example so that I could understand your new type well. Thanks\n. OK. I will check it again after making vector typemap work well\n. ",
    "npinto": "Interesting. I guess this has to do with mpi support in my hdf5 library. I'll check my USE flags and report back. Thanks for the prompt answer.\n. ",
    "vigsterkr": "@FlorianSchulze id3 tree just has been merged into repository. but C4.5 and C5.0 is still missing... so there's plenty of room for new stuff ;)\n. @FlorianSchulze if you want to start i suggest you start by checking out the base class for a tree classifier: TreeMachine and TreeMachineNode\n. oh and one of my favorite is missing as well: CART (Classification And Regression Tree)\nand example implementation can be found here:\nhttps://sites.google.com/site/rtranking/ \n. @FlorianSchulze which of those 3? :) i'm just asking because maybe somebody would like one of the those 3 options as well...\n. done\n. done\n. done\n. i've started this one using librf, which is an open-source GPL 2 implementation of Random Forest.\nIf anybody feels contributing to it don't hesitate to start sending PRs to this branch:\nhttps://github.com/shogun-toolbox/shogun/tree/feature/RandomForest\n. closing this in favour for feature/windows branch with appveyor :)\n. ah yeah sorry about that! just committed the change!\nthnx!\n. yeah i was discussing this with somebody yesterday on #shogun channel on freenode (irc). and i've checked as well the code of histogram intersection kernel, and for me i couldn't spot any differences between the two ways :(\nanyhow will put some dummy java code into the gist soon then\n. heheh well it has to be faster, but havent' benchmarked it. but it's basically 2 division vs 1 multiplication. so, even if division and multipl. operand would be the same processing time it's half the operand per iteration. but as we know division takes more time than multiplication... so it has to be faster.\nbut i guess you want to see numbers ;)\n. bitshift would work if it would be integer, but it's a double value...\ni'll run a simple test just to make sure\n. ok so here's a very dummy example, just to get the running time straight. with using -O3 optimization flag in g++:\nhttps://gist.github.com/1826653\ni'm getting 4864us (JS1) vs 4795us (JS2) on average after 4 different runs.\nthe reinitialization of the vector is required as otherwise cpu caching kicks in so there's no way to sample it realistically that way...\n. Hi Phillippe,\n::: is there a particular reason why you want to do the openCL implementation based on viennaCL? i mean i guess viennacl has some very good wrapper functions of opencl, but on the other hand opencl is a standard API that is maintained by the same organisation as opengl for example. and viennacl adds just another abstraction on top of that. but when you will want to use shogun with opencl support you'll always have to depend not only on having opencl (which would be implemented in various ways, depending on the vendor, but the API is fixed at least) but on having viennaCL as well. which i guess is not that hard to compile, but still having an extra abstraction+library that is 'not so standard' as opencl, just brings in more complications when you think about porting to various distribs and OS...\n. @pluskid oh yeah just that it's written here as well if you haven't checked the irc logs. i've tested yesterday night your patch with using cross-validation on MulticlassLibSVM and it works smoothly! thanks a lot for the patch. let's see if you'll need to remove the STL dependency...\n. Committed the requested changes\n. i do the auto-indent with vim (shift-v and then =), so IMO it should be right.\nalthough this is in my .vimrc:\ntabstop=2\nsofttabstop=0\nshiftwidth=2\n. i do the auto-indent with vim (shift-v and then =), so IMO it should be right.\nalthough this is in my .vimrc:\ntabstop=2\nsofttabstop=0\nshiftwidth=2\n. fixed the spacing issue.\ni've tried using --amend but that way i couldn't push the commit to my remote repo, and it asked me to merge :S\n. fixed the spacing issue.\ni've tried using --amend but that way i couldn't push the commit to my remote repo, and it asked me to merge :S\n. yes to command to run the uni testing is: make unit-tests\nif the unit test is syntactically ok but semantically not then gtest will print you the exact error where it happened and why.\njust try for yourself: add a line in SGVector_unittest.cc that will fail for sure, e.g.:\nSGVector x(11);\nEXPECT_EQ(10, x.vlen);\nand then you'll see how the error looks like.\n. yes to command to run the uni testing is: make unit-tests\nif the unit test is syntactically ok but semantically not then gtest will print you the exact error where it happened and why.\njust try for yourself: add a line in SGVector_unittest.cc that will fail for sure, e.g.:\nSGVector x(11);\nEXPECT_EQ(10, x.vlen);\nand then you'll see how the error looks like.\n. There's class method example as well like line 20 in SGVector_unittest.cc:\na.set_const(3.3);\nand it works.\nBut if you want more complex ways to test some class methods - and i think we want - then we will need to use mocking framework as well, see a good example what you could do with mocking here:\nhttp://code.google.com/p/googlemock/wiki/V1_5_CheatSheet#Using_Mocks_in_Tests\n. i've looked around for that but all in all unit testing is not meant for finding memory leaks.\nI've told to heiko yesterday that there's a way to find memory leaks within unit testing when using m$ compiler... but i couldn't find any other way.\nafaik valgrind is the way to find mem leaks.... unit testing is really about having checking that functions/classes are behaving as expected.\n. ok i've moved unit tests and regression tests under ./tests as we've agreed on irc.\ni'll add google mock framework check into configure script and a basic example as well.\n. So i've changed to use Google C++ Mocking FW. it comes with a bundled Google C++ Testing FW, but it allows us to do some more funky unit testing. See the LatentSVM test.\n. ok, i've applied the requested changes, any other wishes/comments?\n. 1:38 < wiking> sonney2k: eeeey!!!\n01:38 < wiking> sonney2k: just realised why submodule will be really bad for unit testing\n01:39 < wiking> sonney2k: it's gonna be way too complicated to keep the unit testing in synch with shogun itself\n01:39 < wiking> sonney2k: say you want change some classes in shogun, you push it\n01:39 < wiking> then you have to push separately in the submodule\n01:40 < wiking> since if you not, and most of the developers won't as they will not be able to put in 1 PR the new code + the unit tests for it\n01:40 < wiking> the unit testing will keep on failing...\nany input on this before getting along on the submodule-train? :)\n. change = rename it, since let's say you restructure a bit the code.\nmy concern is here more about the whole method of how to add new code + corresponding unit tests.\nthat's going to be at least 2 commits + 2 pushes and one extra pulls to get in sync with the submodule...\n. on the other hand we can agree that eventually the unit testing will be part of the main shogun repo, just that until the unit testing hasn't grown into a full blown unit testing system of shogun we keep it in submodule, i'm totally fine by having a testing submodule for a while and merge the whole repository into the shogun main repo as soon as most of the classes in shogun are covered by unit testing.\nbut this could be done by simply keeping a utest branch as is for a while and try to finish up with all the unit testing and just merge it in the master branch when we feel it's ready.\nif we go with the latter case just close this PR and we'll do another PR sometime in the future when the utest branch is ready to be merged\n. argh,SERGEY!!!\nok let me fix it!\n. @lisitsyn i have no idea about the internals of eigen... would it be ok to move that include into Math.cpp?\ni guess not since you are using Eigen:: in SGVector.cpp.\nshould we move it there, or?\n. this patch should do that\n. The -1 should be removed as otherwise it fails with a segfault.\nWhen the -1 is removed the unit test that i've defined for SVMOcas and MulticlassOcas runs smoothly! ;)\n. i've tried with +1 the unit testing failed, the labeling was shifted by +1\n. first time the unit testing actually was useful ;)\n. added a small description into the header file with proper syntax\n. :D fixed the author in copyright header \n. mmm made a mistake with the branches thus an unintended commit landed here as well :S\n. @karlnapf this will actually half solve your problem. but as you'll see this still won't detect gmock on your ubuntu, because for some reason in the last 2 release of Ubuntu they do not provide libgtest0 package only the libgtest-dev package that only contains the header files of Google C++ Unit Testing Framework. So you'll still need to compile & install libgtest by hand and let the ./configure script know where libgtest.so resides. e.g. ./configure --libs=\n. fix possible memory leaks, reported in:\nhttp://maeth.com/shogun-report/\n@uricamic please could you check on this patch, just to be sure that i'm doing it in a good way? and approve it for merging if it's ok!\n. fix possible memory leaks, reported in:\nhttp://maeth.com/shogun-report/\n@uricamic please could you check on this patch, just to be sure that i'm doing it in a good way? and approve it for merging if it's ok!\n. hehehe yeah! i've realised today that i might be doing a double-free.\nbut yeah if allocation of 'map' fails on line 171 in libp3bm.cpp then cp_list will not be deallocated, as that loop for freeing the whole structure will not be executed because of goto...\n. can we merge it then?\n. @lisitsyn or @sonney2k can u plz merge it ;)\n. @iglesias you owe me a beer ;p\n. yeah we figured that there's something else wrong with it as actually there was -pthread flag for c++... but still for some reason it couldn't find pthread_atfork function in the libpthread.so :S\n. I've just attached the Portfile for shogun 2.0 to the macports ticket mentioned in the bug. \nI've tested it locally on a 10.8 osx and works fine, but please test it and let me know if something is wrong on your machine.\n. a nightly packaging bot target is required on buildbot.\nand of course a place to upload them to... maybe on the buildbot page?\n. done via creating ppas. one, fast possibility to implement this to use the BSD licensed crfsuite library:\nhttp://www.chokkan.org/software/crfsuite/\n. @sonney2k have a look at the last patch... looks good?\n. ignore the travis build status as the .travis.yaml is only available in my master branch ;)\n. mmm Latent was never installed for python, so let's add this as well.... :P\n. no unfortunately not. the accelerator framework has some catlas_* functions defined, but not all of them that would be required for HAVE_ATLAS.\nthe ones set_const is using though are all available...\n. i reckon this should be fixed differently. since the compiler flags set by ./configure should be good - but apparently they are not :(\n. mmm the problem with the duck-tape fixes in general that people tend to forget them, that they were temporal fixes and soon they'll become permanent fixes and then maybe one day it hits back from an unexpected angle. :(\n. while we are at it, before doing a final decision on this i think it's worth to look around in this areas. I.e. we already have one kernel approximation method implemented in shogun (see HomogeneousKernelMapping by Vedaldi).\nthe group of Vedaldi have done some more work in this, see:\nSparse Kernel Approximations for Efficient Classification and Detection\nhttp://www.vlfeat.org/~vedaldi/assets/pubs/vedaldi12sparse.pdf\n. @karlnapf i think we can close this one. as the option for per module valgrinding is in the repo.\nand the leaks are reported already as issues per module\n. fixed in the HEAD of develop branch\n. fixed by 807d8ca02d81f46f3e87a1216c110e1708b4095c\n. fixed by 807d8ca02d81f46f3e87a1216c110e1708b4095c\n. fixed\n. fixed\n. @saketbharambe please create a new PR against 'develop' branch.\nas this one is against 'master' i'm closing it.\n. @deerishi please consider closing this (and pull request #998) as we don't accept any more PRs for the master branch. submit PRs for the develop branch.\n. closing as it's created against the master branch\n. closing as it's created against the master branch\n. worksforme\nsorry this was an invalid bug\n. totally my fault... :( called it on a wrong matrix :(\n. this patch fixes #943 right @sonney2k?\ncan we merge it and close that issue?\n. Should be fixed with this patch\n. hi Monica,\ni think you didnt really got how git works, e.g. commiting a zip file in this repo is a big nogo!\nplease read manuals about git like gitbook\n. @karlnapf i'll move this under multiclass/tree using TreeMachine\n. closing this one as it hasn't been touched for 10 months...\n. the problem is that we NEED gmock sources just as in case of gtest. hence, having only the header with precompiled shared/static libraries are not enough.\nsee:\nhttps://groups.google.com/forum/?fromgroups#!topic/googlemock/gFd0ZsHAvcw\nor\nhttp://code.google.com/p/googlemock/source/browse/trunk/README\n(### Generic Build Instructions ### part)\n. @grilomoto could you test this now with the new cmake build system\nclone the develop branch of shogun.\n. @grilomoto thanks for getting back to us so soon!\nunfortunately the build instructions are still outdated in the INSTALL for the new cmake system.\nso what you would need to run cmake build with the develop branch:\ngit clone https://github.com/shogun-toolbox/shogun.git\ncd shogun\nmkdir build\ncd build\ncmake -DJavaModular=ON -DBUILD_EXAMPLES=ON ..\nmake\ncould you copy-paste the output of this if there's still an error\n. do you have the full java SDK installed?\nfrom this error message i suspect that cmake couldn't find your JAVA_HOME.\nfor example in case of openjdk you would need to set the JAVA_HOME environment to point to your SDK root:\nJAVA_HOME=/usr/lib/jvm/java-6-openjdk-amd64 cmake -DJavaModular=ON -DBUILD_EXAMPLES=ON ..\nbut of course in your case i don't know where the JAVA_HOME actually is\n. where's your libawt.so and libjvm.so ? or i guess it would be libjvm.dll and libjvm.dll on windows...\n. hehe seems nobody is up for the task. i'm taking over this bug and happy to cherry-pick anybody's repo ;)\n. @van51 i think you don't understand cmake...\naaand that you are not testing this with the feature/CMake branch....\n. @van51 note that that branch is not up-to-date, i.e. it has been forked of develop branch a while ago and i haven't got around to rebase it yet\n. cmake has been merged into develop branch. see commit 175c88c521100867482efc7ba283d7567cc62fe1\ni'm closing this bug now, and just open a new issue if there's a specific problem with cmake build\n. i'm sorry to comment it so late, but i was piled up with work.\nFrankly i don't like this patch. the reasons:\n- we agreed to move both Features and Labels to StructuredModel\n- risk() shall be part of StructuredModel because if you want to use a BMRM, which solves basically anything optimization problem in the form of: min 1/lambda w^2 + R, where R equals the risk() return value. And with this patch what you actually gonna force to make everybody to do (if one wants to use BMRM to solve an SO-SVM problem) is to do an inheritance of DualLibQPBMSOSVM and re-implement there the risk() function, instead of doing this only at one place, i.e. in the StructuredModel.\n- passing around CDualLibQPBMSOSVM* for the different BMRM solvers is really cumbersome and i feel that it's completely misleading instead of actually passing StructuredModel.\nmy 2 cents\nviktor\nps: i'll review the patch again, and send some more comments if i find something else.\n. ... i did not want to close it, just comment it sorry....\n. @hushell as said earlier risk() calculates R in \nmin 1/lambda w^2 + R\nwhere R is anything you would like it to be. hence the implementation of risk() pretty much depends on your primary objective. and there are different formulation of SO-SVM. like 1-Slack formulation, n-slack forumlation etc.\n. btw: you haven't said anything about removing Labels from StructuredOutputMachine...\n. and how do you want to choose between the different risk implementations? enum or...?\n. no it's not fine... we should remove the Labels dependency in ctor of StructuredOutputMachine...\nand yes it is very bad to pass CDualLibQPSOSVM* as it really doesn't make any sense semantically...\n. should be fixed with that patch...\n. noup. and no...\n. fixed.\n. @karlnap\nnot necessary, why don't we just use the good old std::isalnum method?\ni mean if it's alphanum then it's ok otherwise raise an error, or?\n. http://www.cplusplus.com/reference/cctype/isalnum/\n. i see no better way...\n. fixed in SerialUTests which now merged into develop\n. come on there's like milion of 'standard' JSON serialization libraries out there.. why don't we just use that.\ni already have an msgpack feature branch in my local git repo. imho we'd better off with that:\nsee, some examples:\nhttp://avro.apache.org/\nhttp://msgpack.org/\n. unfortunately the source of error is not this... i dont know yet what it is... checking it and i'll get back to you!\n. ok as far as i can tell your code actually does everything good.\nthe ./src/shogun/base/class_list.cpp.py script should be changed that it know to check for HAVE_COLPACK as well.\ni'm working on this fix, and i'll let you know as soon as i'm ready with it. until then this PR will be pending. sorry\n. @lambday  ok so you are probably going to kill me but the problem with class_list.cpp.py that it does not allow at it's form #if defined(...) && ... for blacklisting class declarations... this would require some quite some amount of effort to change. basically what i'm saying is that could you please revert those changes that i asked you previously.\ni'm really sorry man, but there's just too much stuff to fix now with shogun before the next release and changing class_list.cpp.py behaviour is quite a low priority.\ni'll add now a quickfix for detecting #ifdef HAVE_COLPACK and blacklisting it for class_list.cpp if HAVE_COLPACK is not defined...\n. why do we care about these?\n. ok then instead of removing can we just move them under CMakeFiles/CMakeTmp ?\n. http://buildbot.shogun-toolbox.org/memcheck/\nhas the valgrind output of ctest.\nit's in xml but it's quite easy to parse (just search for Memory Leak) and you'll see which tests are currently leaking...\nsome of the above mentioned are fixed but there are some new ones...\n. http://buildbot.shogun-toolbox.org/memcheck/\nhas the valgrind output of ctest.\nit's in xml but it's quite easy to parse (just search for Memory Leak) and you'll see which tests are currently leaking...\nsome of the above mentioned are fixed but there are some new ones...\n. we have still some leaking unit tests but opening separate issues for them...\n. seems to be fixed finally ;)\nhttp://buildbot.shogun-toolbox.org/builders/cyg1%20-%20libshogun/builds/1554\n. disabled gmock tests on freebsd...\n. @sonney2k \nthe /usr/local/bin/update-notebooks.sh script on 7nn.de needs an extra line after the notebook is compiled/generated:\nipython nbconvert $notebook\nthis will generate the static html that we can refer to on the website.\nthis command requires some extra packages installed on that machine: apt-get install pandoc\n. @sonney2k \nthe /usr/local/bin/update-notebooks.sh script on 7nn.de needs an extra line after the notebook is compiled/generated:\nipython nbconvert $notebook\nthis will generate the static html that we can refer to on the website.\nthis command requires some extra packages installed on that machine: apt-get install pandoc\n. ok great @sonney2k has set up the script, so the nightly notebooks are being generated to:\nhttp://shogun-toolbox.org/static/notebook\ne.g.\nhttp://shogun-toolbox.org/static/notebook/gp_regression.html\nhttp://shogun-toolbox.org/static/notebook/gp_regression.ipynb\nnow we only need to figure out how to reference these html/ipynb files\n. related to #1678 \nbasically some of the notebooks relies on data in the shogun-data repository. hence we need to fix how the notebooks are generated nightly. they need to be able to read data from data repo\n. done\n. done\n. done\n. done\n. as a starter i've added a new bot target:\nhttp://buildbot.shogun-toolbox.org/builders/debian%20wheezy%20-%20memcheck\nthis runs on the new machine.\nthe output is not the best... but it produces an full xml report that we could transfer and visualize somewhere.\nbtw: if we would use jenkins this is already solved...\n@karlnapf @lisitsyn @iglesias @sonney2k \nit currently only runs the valgrind memcheck on libshogun examples and unit tests. i wonder if we should do this for all interfaces as well?\n. as you can see the memcheck for libshogun examples and the unit tests ran for about 45 minutes:\nhttp://buildbot.shogun-toolbox.org/builders/debian%20wheezy%20-%20memcheck/builds/1\nHence i would only run the memcheck build target only daily and not only after every commit...\nmy 2 cents\n. @karlnapf check the output (running times included):\nhttp://buildbot.shogun-toolbox.org/builders/debian%20wheezy%20-%20memcheck/builds/1/steps/test/logs/stdio\n. coverage is a problem...\nanyhow, closing this bug. reports are available here:\nhttp://buildbot.shogun-toolbox.org/memcheck/\ni'll add an xslt to visualize somehow the xml of ctest\n. this will be solved by a custom script with some help from fedora ppl\n. this is fixed, but as one can see from the travis logs class_list.cpp.py has to be fixed for python 3.3:\nhttps://travis-ci.org/shogun-toolbox/shogun/jobs/11091502#L690\nopening a new issue #1546\n. this is fixed, but as one can see from the travis logs class_list.cpp.py has to be fixed for python 3.3:\nhttps://travis-ci.org/shogun-toolbox/shogun/jobs/11091502#L690\nopening a new issue #1546\n. we used to have one, clang's scan-build, but unfortunately the bot that was creating the static-analysis checks went down... but now with the new build server we'll get it back soon. but thnx!\n. done. the results are available here:\nhttp://buildbot.shogun-toolbox.org/static_analysis/\nclosing this issue and opening some other's as there's a lot to fix according to the report\n. afaik we did not have this in ./configure. do we need it? btw: what is complex64?\n. i've just did this in shogun source root:\ngit grep HAVE_COMPLEX64\nand apparently there's no file in the repository that has this macro in it...\n@karlnapf are you sure it's HAVE_COMPLEX64 ?\nand btw: we dont have HAVE_FLOAT64 ... only USE_FLOAT64 which is only used in modular interface files...\nand no USE_COMPLEX64 is not used anywhere either...\n. i've just did this in shogun source root:\ngit grep HAVE_COMPLEX64\nand apparently there's no file in the repository that has this macro in it...\n@karlnapf are you sure it's HAVE_COMPLEX64 ?\nand btw: we dont have HAVE_FLOAT64 ... only USE_FLOAT64 which is only used in modular interface files...\nand no USE_COMPLEX64 is not used anywhere either...\n. we need complex64_t -> numpy's complex64 mapping.\nshouldn't be that hard... but let's bump up this task's priority so that we have lambday's work in modular interface as well\n. needs testing...\n. needs testing...\n. this is done.\n. added dependency for all .i files for generating the .cxx file, hence from now on any change in the modular interface swig files will trigger a regeneration of .cxx which will trigger a recompile of the .cxx\n. added dependency for all .i files for generating the .cxx file, hence from now on any change in the modular interface swig files will trigger a regeneration of .cxx which will trigger a recompile of the .cxx\n. @karlnapf do you want this enabled on travis as well?\nnote that if it's not soooo necessary i would avoid in order to not to exceed the soft limit for buildtime on travis. we can enabled them on buildbots...\n. this is done :100:\n. this is done :100:\n. from CMakeLists.txt:\nIF(BUILD_EXAMPLES OR ENABLE_TESTING)\n    add_subdirectory(examples)\nENDIF()\nhence if you have BUILD_EXAMPLES=OFF but ENABLE_TESTING=ON then it'll still build the examples as they are tests mostly.\nif you want really just to build libshogun:\nmake shogun\n. i've fixed a little thing, but other than that it was good.\n@karlnapf do you want this enabled on travis as well?\nnote that if it's not soooo necessary i would avoid in order to not to exceed the soft limit for buildtime on travis. we can enabled them on buildbots...\n. it's done... ;)\n. it's done... ;)\n. almost good.  we need to fix this in a way to support detecting the latest stable mosek 7.0\nalthough of course we still need to fix the interface for mosek 7.0 in shogun, but detecting 7.0 would be a good starting point.\n. @hushell yes, but changing this code into detecting 7.0 wouldn't be that hard as far as i can see....\n. fixed but another error has just appeared... see issue #1547\n. fixed but another error has just appeared... see issue #1547\n. fixed\n. fixed\n. most of them fixed by @sonney2k \non travis only the following 3 fails:\n- integration-python_modular-tester-serialization_svmlight_modular (Failed)\n- python_modular-serialization_svmlight_modular (Failed)\n- python_modular-serialization_string_kernels_modular (Failed)\n. fixed\n. that's done, see for example clone_unitttest.cc.py:\ntry:\n    import jinja2\n    outputText = entry(TEMPLATE_FILE, class_list_file)\nexcept ImportError:\n    print(\"Please install jinja2 for clone unit-tests\");\n    outputText = ['''#include \nTEST(Dummy,dummy)\n{\n}''']\n. that's done, see for example clone_unitttest.cc.py:\ntry:\n    import jinja2\n    outputText = entry(TEMPLATE_FILE, class_list_file)\nexcept ImportError:\n    print(\"Please install jinja2 for clone unit-tests\");\n    outputText = ['''#include \nTEST(Dummy,dummy)\n{\n}''']\n. remove REQUIRED for jinja2 in CMakeLists.txt\n. remove REQUIRED for jinja2 in CMakeLists.txt\n. this aint gonna happen fully as we wanna have at least LAPACK for eigen... for the rest of the codebase we are converging anyways..... removed by @sonney2k \n. you've forgot to clean ./src/shogun/\n. @karlnapf let's do something about this ... plz\n. note, this unit test sometimes fails... it's most probably due to this bug\nhttp://buildbot.shogun-toolbox.org/builders/precise%20-%20libshogun/builds/47/steps/test/logs/stdio\n. fixed\n. the output is good although the StreamingDenseFeatures still failing, but imo that's another issue.\nthnx for the fix!\n. REALLOC is there for a good reason, i.e. making SGSparseVector buffering possible...\nhence an extra if condition (size > 0) would be much better than switching back to malloc.\n. what's the example or unit test where u get this segfault?\nso that i can test it fast if i manage to get a fix for it...\n. perfect! thank you!\n. perfect! thank you!\n. ^ fixed\n. ^ fixed\n. closing as it's a duplicate of #1415 \n. virtualenv\ni've agreed with @sonney2k that i'll provide one for ourselves for testing and then we'll make a decision based on that.\n. up and running mailed the core developers for testing... will be public after tests/feedbacks\n. done.\nhttp://shogun-cloud.maeth.com\n@sonney2k please start the procedure to move shogun-toolbox.org domain somewhere where all of us can have a control over it's sub-domains. if there's no other option i'm happy to host the DNS resolvers. i've got a web based interface for admining sub-domains with several different users... but i'm fine with anything just that we can add sub-domains without waiting longer than 2 minutes...\nas well as it would be great to get somekind of an admin access over shogun-toolbox github user... it's almost the same as dns.... to manage the hooks, applications etc.\n. btw closing...\n. yeah because with the modular interface we are doing builds separately.\nshall we maybe have an extra build job, where examples and unit tests are being built?\nbtw: maybe we should do a trick on the bots that the required sources like gmock/gtest and other bundled stuff are stored somewhere separately hence downloading is not necessary?\n. yes yes duplicate of #1594 \n. it's not features_string_sliding_window_modular but regression_svrlight_modular.cs that fails... renaming bug.\n. why 3.1? :D\n. about ruby_modular: that path is supplied by ruby and not me... so i don't really know what i should do about this...\n. same for lua....\n. ok this is rather cumbersome, as i think this is required for Fedora and alike distros, but on debian like distros there's no /usr/lib64 so i don't really know what to do about this... \n. libshogun and lua should be fine now...\n. great. could you please add an\ninclude \nto the files you've changed... (as that's required for unlink function)\n. or the whole thing could just go into StreamingFile via adding libarchive to IOBuffer and then all our troubles are solved. the only thing is that this way we would have compression support for streaming files. which actually does have sense as why else would one compress a csv/libsvm file other than because it's big. if it's big then it needs a lot of memory, hence there's a high possibility that it wouldn't even fit into the memory. i.e. one would use streaming features anyways. ;P\n. fixed\n. fixed\n. fixed\n. i think it'll fail with mosek 6 as well... but i've tested with 7, but as you see from the error it's not related to mosek\n. i think it'll fail with mosek 6 as well... but i've tested with 7, but as you see from the error it's not related to mosek\n. milestone it plz next time...\n. we can close this then...\n. mmm i think then the order of includes should be changed in cmake will try/investigate\n. increase is not allowed on travis. they have a soft time limit of 20 mins and that's it... \n. this is because of the gdb magic by @lisitsyn \nunfortunately it's not doing exactly what it should hence one gets around 1 minute (~50secs) running time for a simple integration test...\n. see for example:\nhttp://buildbot.shogun-toolbox.org/builders/deb3%20-%20modular_interfaces/builds/1850/steps/test%20python%20modular/logs/stdio\nit took 72.34 sec to run all the python_modular tests... now it's 2581.15 sec which is ridiculous...\n. no it's not a 'few' it's all the python modular test that are using tester.py...\nand if u wanna see why actually it takes 50 seconds each use -V for ctest.... you'll see and will have a facepalm moment ;P\n. basically i need to fix the gdb thing that has been started by @lisitsyn \n. integration-python_modular-tester-modelselection_grid_search_krr_modular is just running and running but does not want to return.... 10+ mins\n. and of course same goes for\npython_modular-modelselection_grid_search_krr_modular\n@karlnapf ideas about what could be wrong...?\n. finally back to green...\\o/\n. yes we know this (or at least I) and this has been known by ubuntu developers as well (there's even a bug about this on lunchpad). there's not too much we can do about this... in newer ubuntus this has been fixed.\ni'd close this bug... \n. mmm @karlnapf  what exactly this bug is about...?\nimo this problem is covered by issue #1482 or?\ni wouldn't create a rendering service, just render the existing notebooks (that are in the repo) on our website nightly and for each release + have the cloud based notebook....\nor this is something else that i dont understand?\n. yeah sure...\ntry:\nipython nbconvert notebook.ipynb\nand as you will see a static html file is being generated that u can embed into any website...\nclosing...\n. this is done.\nthere are quite some bogus mathematical formulas in some of the headers, e.g.:\nLatentModel.h, QuadraticTimeMMD.h etc.\nthese should be checked/fixed\n. currently only one is failing\n```\nrunning /home/buildslave/nightly_default/build/doc/ipython-notebooks/metric/LMNN.ipynb\nFailed to run cell:\nfrom modshogun import LMNN\nnumber of targer neighbours in LMNN, here we just use the same value that was used for KNN before\nk = 3\nlmnn = LMNN(ape_features, ape_labels, k)\nlmnn.set_diagonal(True)\nlmnn.set_maxiter(1200)\ninit_transform = numpy.eye(ape_features.get_num_features())\nlmnn.train(init_transform)\ndiagonal = numpy.diag(lmnn.get_linear_transform())\nprint('%d out of %d elements are non-zero.' % (numpy.sum(diagonal != 0), diagonal.size))\n  1 cells failed to complete\n 25 code cells from notebook\n\n```\n. 90 mins???\nthe last time it was running successfully was 25m1.033s\n. or to introduce somewhere a mutex ;)\n. why not just bundle those two libraries? they are both 'bundleable'\n. i'm getting now these errors with the logdet notebook. seems like some files are missing....\n@lambday any insights?\n```\nhome/buildslave/nightly_default/build/doc/ipython-notebooks/logdet/logdet.ipynb\nrunning /home/buildslave/nightly_default/build/doc/ipython-notebooks/logdet/logdet.ipynb\nFailed to run cell:\nfrom scipy.sparse import csc_matrix\nm = mmread('../../../data/logdet/west0479.mtx.gz')\ncomputing a spd with added ridge\nB = csc_matrix(m.transpose() * m + identity(m.shape[0]) * 1000.0)\nfig = plt.figure(figsize=(12, 4))\nax = fig.add_subplot(1,2,1)\nax.set_title('B')\nax.spy(B, precision = 1e-5, marker = '.', markersize = 2.0)\nax = fig.add_subplot(1,2,2)\nax.set_title('lower Cholesky factor')\ndense_matrix = B.todense()\nL = cholesky(dense_matrix)\nax.spy(csc_matrix(L), precision = 1e-5, marker = '.', markersize = 2.0)\nplt.show()\nIOError                                   Traceback (most recent call last) in ()\n      1 from scipy.sparse import csc_matrix\n      2 \n----> 3 m = mmread('../../../data/logdet/west0479.mtx.gz')\n      4 # computing a spd with added ridge\n      5 B = csc_matrix(m.transpose() * m + identity(m.shape[0]) * 1000.0)\n/usr/lib/python2.7/dist-packages/scipy/io/mmio.pyc in mmread(source)\n     66 \n     67     \"\"\"\n---> 68     return MMFile().read(source)\n     69 \n     70 #-------------------------------------------------------------------------------\n/usr/lib/python2.7/dist-packages/scipy/io/mmio.pyc in read(self, source)\n    296     #---------------------------------------------------------------------------\n    297     def read(self, source):\n--> 298         stream, close_it = self._open(source)\n    299 \n    300         try:\n/usr/lib/python2.7/dist-packages/scipy/io/mmio.pyc in _open(filespec, mode)\n    240                 if filespec.endswith('.gz'):\n    241                     import gzip\n--> 242                     stream = gzip.open(filespec, mode)\n    243                 elif filespec.endswith('.bz2'):\n    244                     import bz2\n/usr/lib/python2.7/gzip.pyc in open(filename, mode, compresslevel)\n     32 \n     33     \"\"\"\n---> 34     return GzipFile(filename, mode, compresslevel)\n     35 \n     36 class GzipFile(io.BufferedIOBase):\n/usr/lib/python2.7/gzip.pyc in init(self, filename, mode, compresslevel, fileobj, mtime)\n     87             mode += 'b'\n     88         if fileobj is None:\n---> 89             fileobj = self.myfileobj = builtin.open(filename, mode or 'rb')\n     90         if filename is None:\n     91             # Issue #13781: os.fdopen() creates a fileobj with a bogus name\nIOError: [Errno 2] No such file or directory: '../../../data/logdet/west0479.mtx.gz'\nFailed to run cell:\nop = RealSparseMatrixOperator(B)\neigen_solver = LanczosEigenSolver(op)\ncomputing log-det estimates using probing sampler\nprobing_sampler = ProbingSampler(op)\ncgm.set_iteration_limit(500)\nop_func = LogRationalApproximationCGM(op, engine, eigen_solver, cgm, 1E-5)\nlog_det_estimator = LogDetEstimator(probing_sampler, op_func, engine)\nnum_probing_estimates = 100\nprobing_estimates = log_det_estimator.sample(num_probing_estimates)\ncomputing log-det estimates using Gaussian sampler\nfrom modshogun import NormalSampler, Statistics\nnum_colors = probing_sampler.get_num_samples()\nnormal_sampler = NormalSampler(op.get_dimension())\nlog_det_estimator = LogDetEstimator(normal_sampler, op_func, engine)\nnum_normal_estimates = num_probing_estimates * num_colors\nnormal_estimates = log_det_estimator.sample(num_normal_estimates)\naverage in groups of n_effective_samples\neffective_estimates_normal = zeros(num_probing_estimates)\nfor i in range(num_probing_estimates):\n    idx = i * num_colors\n    effective_estimates_normal[i] = mean(normal_estimates[idx:(idx + num_colors)])\nactual_logdet = Statistics.log_det(B)\nprint 'Actual log(det(B)):', actual_logdet\nprint 'Estimated log(det(B)) using probing sampler:', mean(probing_estimates)\nprint 'Estimated log(det(B)) using Gaussian sampler:', mean(effective_estimates_normal)\nprint 'Variance using probing sampler:',var(probing_estimates)\nprint 'Variance using Gaussian sampler:',var(effective_estimates_normal)\n\nNameError                                 Traceback (most recent call last) in ()\n----> 1 op = RealSparseMatrixOperator(B)\n      2 eigen_solver = LanczosEigenSolver(op)\n      3 \n      4 # computing log-det estimates using probing sampler\n      5 probing_sampler = ProbingSampler(op)\nNameError: name 'B' is not defined\nFailed to run cell:\nfig = plt.figure(figsize=(15, 4))\nax = fig.add_subplot(1,3,1)\nax.set_title('Probing sampler')\nax.plot(cumsum(probing_estimates)/(arange(len(probing_estimates))+1))\nax.plot([0,len(probing_estimates)], [actual_logdet, actual_logdet])\nax.legend([\"Probing\", \"True\"])\nax = fig.add_subplot(1,3,2)\nax.set_title('Gaussian sampler')\nax.plot(cumsum(effective_estimates_normal)/(arange(len(effective_estimates_normal))+1))\nax.plot([0,len(probing_estimates)], [actual_logdet, actual_logdet])\nax.legend([\"Gaussian\", \"True\"])\nax = fig.add_subplot(1,3,3)\nax.hist(probing_estimates)\nax.hist(effective_estimates_normal)\nax.plot([actual_logdet, actual_logdet], [0,len(probing_estimates)], linewidth=3)\nplt.show()\n\nNameError                                 Traceback (most recent call last) in ()\n      2 ax = fig.add_subplot(1,3,1)\n      3 ax.set_title('Probing sampler')\n----> 4 ax.plot(cumsum(probing_estimates)/(arange(len(probing_estimates))+1))\n      5 ax.plot([0,len(probing_estimates)], [actual_logdet, actual_logdet])\n      6 ax.legend([\"Probing\", \"True\"])\nNameError: name 'probing_estimates' is not defined\n      3 cells failed to complete\n     16 code cells from notebook \nwrote /home/buildslave/nightly_default/build/notebooks/logdet.ipynb\nreal    6m46.622s\nuser    6m33.172s\nsys 0m10.616s\nconverting to html\n[NbConvertApp] Using existing profile dir: u'/home/buildslave/.ipython/profile_default'\n[NbConvertApp] Converting notebook logdet.ipynb to html\n[NbConvertApp] Support files will be in logdet_files/\n[NbConvertApp] Loaded template html_full.tpl\n[NbConvertApp] Writing 348635 bytes to logdet.html\n```\n. i'm getting now these errors with the logdet notebook. seems like some files are missing....\n@lambday any insights?\n```\nhome/buildslave/nightly_default/build/doc/ipython-notebooks/logdet/logdet.ipynb\nrunning /home/buildslave/nightly_default/build/doc/ipython-notebooks/logdet/logdet.ipynb\nFailed to run cell:\nfrom scipy.sparse import csc_matrix\nm = mmread('../../../data/logdet/west0479.mtx.gz')\ncomputing a spd with added ridge\nB = csc_matrix(m.transpose() * m + identity(m.shape[0]) * 1000.0)\nfig = plt.figure(figsize=(12, 4))\nax = fig.add_subplot(1,2,1)\nax.set_title('B')\nax.spy(B, precision = 1e-5, marker = '.', markersize = 2.0)\nax = fig.add_subplot(1,2,2)\nax.set_title('lower Cholesky factor')\ndense_matrix = B.todense()\nL = cholesky(dense_matrix)\nax.spy(csc_matrix(L), precision = 1e-5, marker = '.', markersize = 2.0)\nplt.show()\nIOError                                   Traceback (most recent call last) in ()\n      1 from scipy.sparse import csc_matrix\n      2 \n----> 3 m = mmread('../../../data/logdet/west0479.mtx.gz')\n      4 # computing a spd with added ridge\n      5 B = csc_matrix(m.transpose() * m + identity(m.shape[0]) * 1000.0)\n/usr/lib/python2.7/dist-packages/scipy/io/mmio.pyc in mmread(source)\n     66 \n     67     \"\"\"\n---> 68     return MMFile().read(source)\n     69 \n     70 #-------------------------------------------------------------------------------\n/usr/lib/python2.7/dist-packages/scipy/io/mmio.pyc in read(self, source)\n    296     #---------------------------------------------------------------------------\n    297     def read(self, source):\n--> 298         stream, close_it = self._open(source)\n    299 \n    300         try:\n/usr/lib/python2.7/dist-packages/scipy/io/mmio.pyc in _open(filespec, mode)\n    240                 if filespec.endswith('.gz'):\n    241                     import gzip\n--> 242                     stream = gzip.open(filespec, mode)\n    243                 elif filespec.endswith('.bz2'):\n    244                     import bz2\n/usr/lib/python2.7/gzip.pyc in open(filename, mode, compresslevel)\n     32 \n     33     \"\"\"\n---> 34     return GzipFile(filename, mode, compresslevel)\n     35 \n     36 class GzipFile(io.BufferedIOBase):\n/usr/lib/python2.7/gzip.pyc in init(self, filename, mode, compresslevel, fileobj, mtime)\n     87             mode += 'b'\n     88         if fileobj is None:\n---> 89             fileobj = self.myfileobj = builtin.open(filename, mode or 'rb')\n     90         if filename is None:\n     91             # Issue #13781: os.fdopen() creates a fileobj with a bogus name\nIOError: [Errno 2] No such file or directory: '../../../data/logdet/west0479.mtx.gz'\nFailed to run cell:\nop = RealSparseMatrixOperator(B)\neigen_solver = LanczosEigenSolver(op)\ncomputing log-det estimates using probing sampler\nprobing_sampler = ProbingSampler(op)\ncgm.set_iteration_limit(500)\nop_func = LogRationalApproximationCGM(op, engine, eigen_solver, cgm, 1E-5)\nlog_det_estimator = LogDetEstimator(probing_sampler, op_func, engine)\nnum_probing_estimates = 100\nprobing_estimates = log_det_estimator.sample(num_probing_estimates)\ncomputing log-det estimates using Gaussian sampler\nfrom modshogun import NormalSampler, Statistics\nnum_colors = probing_sampler.get_num_samples()\nnormal_sampler = NormalSampler(op.get_dimension())\nlog_det_estimator = LogDetEstimator(normal_sampler, op_func, engine)\nnum_normal_estimates = num_probing_estimates * num_colors\nnormal_estimates = log_det_estimator.sample(num_normal_estimates)\naverage in groups of n_effective_samples\neffective_estimates_normal = zeros(num_probing_estimates)\nfor i in range(num_probing_estimates):\n    idx = i * num_colors\n    effective_estimates_normal[i] = mean(normal_estimates[idx:(idx + num_colors)])\nactual_logdet = Statistics.log_det(B)\nprint 'Actual log(det(B)):', actual_logdet\nprint 'Estimated log(det(B)) using probing sampler:', mean(probing_estimates)\nprint 'Estimated log(det(B)) using Gaussian sampler:', mean(effective_estimates_normal)\nprint 'Variance using probing sampler:',var(probing_estimates)\nprint 'Variance using Gaussian sampler:',var(effective_estimates_normal)\n\nNameError                                 Traceback (most recent call last) in ()\n----> 1 op = RealSparseMatrixOperator(B)\n      2 eigen_solver = LanczosEigenSolver(op)\n      3 \n      4 # computing log-det estimates using probing sampler\n      5 probing_sampler = ProbingSampler(op)\nNameError: name 'B' is not defined\nFailed to run cell:\nfig = plt.figure(figsize=(15, 4))\nax = fig.add_subplot(1,3,1)\nax.set_title('Probing sampler')\nax.plot(cumsum(probing_estimates)/(arange(len(probing_estimates))+1))\nax.plot([0,len(probing_estimates)], [actual_logdet, actual_logdet])\nax.legend([\"Probing\", \"True\"])\nax = fig.add_subplot(1,3,2)\nax.set_title('Gaussian sampler')\nax.plot(cumsum(effective_estimates_normal)/(arange(len(effective_estimates_normal))+1))\nax.plot([0,len(probing_estimates)], [actual_logdet, actual_logdet])\nax.legend([\"Gaussian\", \"True\"])\nax = fig.add_subplot(1,3,3)\nax.hist(probing_estimates)\nax.hist(effective_estimates_normal)\nax.plot([actual_logdet, actual_logdet], [0,len(probing_estimates)], linewidth=3)\nplt.show()\n\nNameError                                 Traceback (most recent call last) in ()\n      2 ax = fig.add_subplot(1,3,1)\n      3 ax.set_title('Probing sampler')\n----> 4 ax.plot(cumsum(probing_estimates)/(arange(len(probing_estimates))+1))\n      5 ax.plot([0,len(probing_estimates)], [actual_logdet, actual_logdet])\n      6 ax.legend([\"Probing\", \"True\"])\nNameError: name 'probing_estimates' is not defined\n      3 cells failed to complete\n     16 code cells from notebook \nwrote /home/buildslave/nightly_default/build/notebooks/logdet.ipynb\nreal    6m46.622s\nuser    6m33.172s\nsys 0m10.616s\nconverting to html\n[NbConvertApp] Using existing profile dir: u'/home/buildslave/.ipython/profile_default'\n[NbConvertApp] Converting notebook logdet.ipynb to html\n[NbConvertApp] Support files will be in logdet_files/\n[NbConvertApp] Loaded template html_full.tpl\n[NbConvertApp] Writing 348635 bytes to logdet.html\n```\n. @lambday could u send in a PR for the fix...? i'll wait for it until i rerun the notebook generation....\n. it's fixed thnx @lambday \n. mmm i would not do this... i mean i get your problem but it should be the Risk function implementation's responsibility whether the Risk itself should be normalized or not. i have some applications where i need normalization but sometimes i dont...\n. ? i mean the R/number of feature vectors you can do on the end of the risk calculation... that'd be the same. \n. no i mean in the risk function implementation just do a /number of fv \nright before returning the risk value.\nso don't modify anything and just leave up to the risk function implementation, whether the returned risk is normalized or unnormalized.\n. @hushell in your application they not supposed to, but in other structured application they are supposed to write it...\n. i'll try to check why R/nSamples within the Risk function is not working.\nanyhow, if we really really need this patch, then at least make it choose-able by the user with a setter/getter function + a boolean. i'm really against hard coding this into the library itself\n. thnx!\n. i reckon this is obsolete so i'm closing it.\n. @iglesias @mazumdarparijat for future reference. you can use ctest's -R command line flag. \nit's basically a regexp where you can define which tests you would like to run\n. @iglesias @mazumdarparijat for future reference. you can use ctest's -R command line flag. \nit's basically a regexp where you can define which tests you would like to run\n. what's the status of this?\n. well you could just have it w/o deriving from anything but then we would loose a lot of features, like serialization :S\n. i reckon we can close this one, or?\n. afaik we've never tried to compile with mingw64...\nthanks for the report we'll try to find a solution for it, but we might need your help to test the solution on mingw64 as none of us core developer has it... stay tuned!\n. i've just committed 17babf2deb5c022396d0503a48d7e7de93b02fc6 to develop branch to fix this issue.\nif you want to test it just clone the git repository and try to compile it. hope it fixes your problem. let me know about the outcome.\n. b5e701cbbd79468ce7472076518fdc2f0b9fed4c commit should fix your problems.\ni'm closing this bug as the initial issue has been fixed. feel free to issue a new ticket if there are more problems\n. this is very strange. as all those macros that your compiler is complaining about should be defined in config.h\ncan u confirm that you have a file here:\nC:\\shogun-develop\\src\\shogun\\lib\\config.h\nif so could you please share the content with us?\n. ok now it all makes sense.\nwhat were the steps exactly before running mingw32-make ?\n. i see.\ni would need some kind of a debugging output of cmake (logs anything) because I do not understand how is it possible that neither MACHINE nor LINKFLAGS is defined in your config.h\nbtw you should consider enabling BUNDLE_EIGEN option via cmake gui, as you will miss a lot of good stuff otherwise. or install eigen on your machine.... whichever fits you the best\n. ok i've checked now some sources.\ni understand the problem of MACHINE and most probably i know what might be the problem with the LINKFLAGS as well, but you should definitely consider installing some of these libraries:\nBLAS\nLAPACK\nGLPK\nEigen3 (required version >= 3.1.2)\nNLopt\nHDF5\nAtlas\n. i've just pushed commit 04ce4adf278412ef80567b07d3b19da5d81a23c4 which fixes the MACHINE problem.\nbefore you try to install any of those libraries maybe you should check out the last version of shogun and enable these two options in cmake:\nBUNDLE_EIGEN\nBUNDLE_NLOPT\nlet me know how that worked out for you\n. try now\n. ok cool. can u try adding\n```\nifdef MINGW32\ninclude \nendif\n```\nright before where the\nint gettimeofday(struct timeval* tp, void* tzp)\nfunction starts?\nbasically to line 32 in src/shogun/lib/Time.h\nit might still complain on the end to an undefined reference to timeGetTime but that can be fixed by adding -lwinmm to the linking flags\n. mmm ok thnx i think i must try to install and compile myself with mingw\n. what's your mingw version?\n. this should be fixed in latest develop after 8fdf278dca78f98d4e06058a5ce3defd560d697d. @karlnapf why me? i know square about matlab static interface :(\ni've asked several times how to generate .mex file but i never got an answer...\n. @karlnapf why me? i know square about matlab static interface :(\ni've asked several times how to generate .mex file but i never got an answer...\n. @sonney2k which?\n. @sonney2k which?\n. @yage99 great idea! would you be that someone who does it? since shogun is an open-source project anybody can and highly recommended to contribute!\n. @yage99 great idea! would you be that someone who does it? since shogun is an open-source project anybody can and highly recommended to contribute!\n. fixed!\nthnx again @yage99 \n. yeah w/o jblas there's no SGMatrix etc.\n. doh never seen this before.. i'll try to check it as soon as i have some time\n. am i right that this is obsolete?\n. how does it effect 3.0.0 if it's after the release? :)\n. it looks rather scary that there's an exception around malloc 8)\ni'll try to check on it... can't it be in another thread?\n. good news! new shogun-python-modular is out:\nhttps://ftp-master.debian.org/new/python-shogun_3.1.1-1.html\n. some more info:\n```\nrunning /home/buildslave/nightly_default/build/doc/ipython-notebooks/structure/FGM.ipynb\nFailed to run cell:\nimport matplotlib.pyplot as plt\nfig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12,4))\nprimal_bmrm = bmrm.get_helper().get_primal_values()\ndual_bmrm = bmrm.get_result().get_hist_Fd_vector()\nplot duality gaps\nxs = range(dual_bmrm.size)\naxes[0].plot(xs, (primal_bmrm-dual_bmrm), label='duality gap')\naxes[0].set_xlabel('iteration')\naxes[0].set_ylabel('duality gap')\naxes[0].legend(loc=1)\naxes[0].set_title('duality gaps');\naxes[0].grid(True)\nplot primal and dual values\nxs = range(dual_bmrm.size-1)\naxes[1].plot(xs, primal_bmrm[1:], label='primal')\naxes[1].plot(xs, dual_bmrm[1:], label='dual')\naxes[1].set_xlabel('iteration')\naxes[1].set_ylabel('objective')\naxes[1].legend(loc=1)\naxes[1].set_title('primal vs dual');\naxes[1].grid(True)\n\nValueError                                Traceback (most recent call last) in ()\n      7 # plot duality gaps\n      8 xs = range(dual_bmrm.size)\n----> 9 axes[0].plot(xs, (primal_bmrm-dual_bmrm), label='duality gap')\n     10 axes[0].set_xlabel('iteration')\n     11 axes[0].set_ylabel('duality gap')\nValueError: operands could not be broadcast together with shapes (0) (210) \nFailed to run cell:\nfig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12,4))\nprimal_sgd = sgd.get_helper().get_primal_values()\nxs = range(dual_bmrm.size-1)\naxes[0].plot(xs, primal_bmrm[1:], label='BMRM')\naxes[0].plot(range(99), primal_sgd[1:100], label='SGD')\naxes[0].set_xlabel('effecitve passes')\naxes[0].set_ylabel('primal objective')\naxes[0].set_title('whole training progress')\naxes[0].legend(loc=1)\naxes[0].grid(True)\naxes[1].plot(range(99), primal_bmrm[1:100], label='BMRM')\naxes[1].plot(range(99), primal_sgd[1:100], label='SGD')\naxes[1].set_xlabel('effecitve passes')\naxes[1].set_ylabel('primal objective')\naxes[1].set_title('first 100 effective passes')\naxes[1].legend(loc=1)\naxes[1].grid(True)\n\nValueError                                Traceback (most recent call last) in ()\n      4 \n      5 xs = range(dual_bmrm.size-1)\n----> 6 axes[0].plot(xs, primal_bmrm[1:], label='BMRM')\n      7 axes[0].plot(range(99), primal_sgd[1:100], label='SGD')\n      8 axes[0].set_xlabel('effecitve passes')\n/usr/lib/pymodules/python2.7/matplotlib/axes.pyc in plot(self, args, kwargs)\n   3891         lines = []\n   3892 \n-> 3893         for line in self._get_lines(*args, kwargs):\n   3894             self.add_line(line)\n   3895             lines.append(line)\n/usr/lib/pymodules/python2.7/matplotlib/axes.pyc in _grab_next_args(self, args, **kwargs)\n    320                 return\n    321             if len(remaining) <= 3:\n--> 322                 for seg in self._plot_args(remaining, kwargs):\n    323                     yield seg\n    324                 return\n/usr/lib/pymodules/python2.7/matplotlib/axes.pyc in _plot_args(self, tup, kwargs)\n    298             x = np.arange(y.shape[0], dtype=float)\n    299 \n--> 300         x, y = self._xy_from_xy(x, y)\n    301 \n    302         if self.command == 'plot':\n/usr/lib/pymodules/python2.7/matplotlib/axes.pyc in _xy_from_xy(self, x, y)\n    238         y = np.atleast_1d(y)\n    239         if x.shape[0] != y.shape[0]:\n--> 240             raise ValueError(\"x and y must have same first dimension\")\n    241         if x.ndim > 2 or y.ndim > 2:\n    242             raise ValueError(\"x and y can be no greater than 2-D\")\nValueError: x and y must have same first dimension\nFailed to run cell:\nfig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12,4))\nterr_bmrm = bmrm.get_helper().get_train_errors()\nterr_sgd = sgd.get_helper().get_train_errors()\nxs = range(terr_bmrm.size-1)\naxes[0].plot(xs, terr_bmrm[1:], label='BMRM')\naxes[0].plot(range(99), terr_sgd[1:100], label='SGD')\naxes[0].set_xlabel('effecitve passes')\naxes[0].set_ylabel('training error')\naxes[0].set_title('whole training progress')\naxes[0].legend(loc=1)\naxes[0].grid(True)\naxes[1].plot(range(99), terr_bmrm[1:100], label='BMRM')\naxes[1].plot(range(99), terr_sgd[1:100], label='SGD')\naxes[1].set_xlabel('effecitve passes')\naxes[1].set_ylabel('training error')\naxes[1].set_title('first 100 effective passes')\naxes[1].legend(loc=1)\naxes[1].grid(True)\n\nValueError                                Traceback (most recent call last) in ()\n     13 axes[0].grid(True)\n     14 \n---> 15 axes[1].plot(range(99), terr_bmrm[1:100], label='BMRM')\n     16 axes[1].plot(range(99), terr_sgd[1:100], label='SGD')\n     17 axes[1].set_xlabel('effecitve passes')\n/usr/lib/pymodules/python2.7/matplotlib/axes.pyc in plot(self, args, kwargs)\n   3891         lines = []\n   3892 \n-> 3893         for line in self._get_lines(*args, kwargs):\n   3894             self.add_line(line)\n   3895             lines.append(line)\n/usr/lib/pymodules/python2.7/matplotlib/axes.pyc in _grab_next_args(self, args, **kwargs)\n    320                 return\n    321             if len(remaining) <= 3:\n--> 322                 for seg in self._plot_args(remaining, kwargs):\n    323                     yield seg\n    324                 return\n/usr/lib/pymodules/python2.7/matplotlib/axes.pyc in _plot_args(self, tup, kwargs)\n    298             x = np.arange(y.shape[0], dtype=float)\n    299 \n--> 300         x, y = self._xy_from_xy(x, y)\n    301 \n    302         if self.command == 'plot':\n/usr/lib/pymodules/python2.7/matplotlib/axes.pyc in _xy_from_xy(self, x, y)\n    238         y = np.atleast_1d(y)\n    239         if x.shape[0] != y.shape[0]:\n--> 240             raise ValueError(\"x and y must have same first dimension\")\n    241         if x.ndim > 2 or y.ndim > 2:\n    242             raise ValueError(\"x and y can be no greater than 2-D\")\nValueError: x and y must have same first dimension\n      3 cells failed to complete\n     26 code cells from notebook \nwrote /home/buildslave/nightly_default/build/notebooks/FGM.ipynb\nreal    5m57.060s\nuser    5m27.796s\nsys 0m0.848s\nconverting to html\n[NbConvertApp] Using existing profile dir: u'/home/buildslave/.ipython/profile_default'\n[NbConvertApp] Converting notebook FGM.ipynb to html\n[NbConvertApp] Support files will be in FGM_files/\n[NbConvertApp] Loaded template html_full.tpl\n[NbConvertApp] Writing 365019 bytes to FGM.html\n```\n. some more info:\n```\nrunning /home/buildslave/nightly_default/build/doc/ipython-notebooks/structure/FGM.ipynb\nFailed to run cell:\nimport matplotlib.pyplot as plt\nfig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12,4))\nprimal_bmrm = bmrm.get_helper().get_primal_values()\ndual_bmrm = bmrm.get_result().get_hist_Fd_vector()\nplot duality gaps\nxs = range(dual_bmrm.size)\naxes[0].plot(xs, (primal_bmrm-dual_bmrm), label='duality gap')\naxes[0].set_xlabel('iteration')\naxes[0].set_ylabel('duality gap')\naxes[0].legend(loc=1)\naxes[0].set_title('duality gaps');\naxes[0].grid(True)\nplot primal and dual values\nxs = range(dual_bmrm.size-1)\naxes[1].plot(xs, primal_bmrm[1:], label='primal')\naxes[1].plot(xs, dual_bmrm[1:], label='dual')\naxes[1].set_xlabel('iteration')\naxes[1].set_ylabel('objective')\naxes[1].legend(loc=1)\naxes[1].set_title('primal vs dual');\naxes[1].grid(True)\n\nValueError                                Traceback (most recent call last) in ()\n      7 # plot duality gaps\n      8 xs = range(dual_bmrm.size)\n----> 9 axes[0].plot(xs, (primal_bmrm-dual_bmrm), label='duality gap')\n     10 axes[0].set_xlabel('iteration')\n     11 axes[0].set_ylabel('duality gap')\nValueError: operands could not be broadcast together with shapes (0) (210) \nFailed to run cell:\nfig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12,4))\nprimal_sgd = sgd.get_helper().get_primal_values()\nxs = range(dual_bmrm.size-1)\naxes[0].plot(xs, primal_bmrm[1:], label='BMRM')\naxes[0].plot(range(99), primal_sgd[1:100], label='SGD')\naxes[0].set_xlabel('effecitve passes')\naxes[0].set_ylabel('primal objective')\naxes[0].set_title('whole training progress')\naxes[0].legend(loc=1)\naxes[0].grid(True)\naxes[1].plot(range(99), primal_bmrm[1:100], label='BMRM')\naxes[1].plot(range(99), primal_sgd[1:100], label='SGD')\naxes[1].set_xlabel('effecitve passes')\naxes[1].set_ylabel('primal objective')\naxes[1].set_title('first 100 effective passes')\naxes[1].legend(loc=1)\naxes[1].grid(True)\n\nValueError                                Traceback (most recent call last) in ()\n      4 \n      5 xs = range(dual_bmrm.size-1)\n----> 6 axes[0].plot(xs, primal_bmrm[1:], label='BMRM')\n      7 axes[0].plot(range(99), primal_sgd[1:100], label='SGD')\n      8 axes[0].set_xlabel('effecitve passes')\n/usr/lib/pymodules/python2.7/matplotlib/axes.pyc in plot(self, args, kwargs)\n   3891         lines = []\n   3892 \n-> 3893         for line in self._get_lines(*args, kwargs):\n   3894             self.add_line(line)\n   3895             lines.append(line)\n/usr/lib/pymodules/python2.7/matplotlib/axes.pyc in _grab_next_args(self, args, **kwargs)\n    320                 return\n    321             if len(remaining) <= 3:\n--> 322                 for seg in self._plot_args(remaining, kwargs):\n    323                     yield seg\n    324                 return\n/usr/lib/pymodules/python2.7/matplotlib/axes.pyc in _plot_args(self, tup, kwargs)\n    298             x = np.arange(y.shape[0], dtype=float)\n    299 \n--> 300         x, y = self._xy_from_xy(x, y)\n    301 \n    302         if self.command == 'plot':\n/usr/lib/pymodules/python2.7/matplotlib/axes.pyc in _xy_from_xy(self, x, y)\n    238         y = np.atleast_1d(y)\n    239         if x.shape[0] != y.shape[0]:\n--> 240             raise ValueError(\"x and y must have same first dimension\")\n    241         if x.ndim > 2 or y.ndim > 2:\n    242             raise ValueError(\"x and y can be no greater than 2-D\")\nValueError: x and y must have same first dimension\nFailed to run cell:\nfig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12,4))\nterr_bmrm = bmrm.get_helper().get_train_errors()\nterr_sgd = sgd.get_helper().get_train_errors()\nxs = range(terr_bmrm.size-1)\naxes[0].plot(xs, terr_bmrm[1:], label='BMRM')\naxes[0].plot(range(99), terr_sgd[1:100], label='SGD')\naxes[0].set_xlabel('effecitve passes')\naxes[0].set_ylabel('training error')\naxes[0].set_title('whole training progress')\naxes[0].legend(loc=1)\naxes[0].grid(True)\naxes[1].plot(range(99), terr_bmrm[1:100], label='BMRM')\naxes[1].plot(range(99), terr_sgd[1:100], label='SGD')\naxes[1].set_xlabel('effecitve passes')\naxes[1].set_ylabel('training error')\naxes[1].set_title('first 100 effective passes')\naxes[1].legend(loc=1)\naxes[1].grid(True)\n\nValueError                                Traceback (most recent call last) in ()\n     13 axes[0].grid(True)\n     14 \n---> 15 axes[1].plot(range(99), terr_bmrm[1:100], label='BMRM')\n     16 axes[1].plot(range(99), terr_sgd[1:100], label='SGD')\n     17 axes[1].set_xlabel('effecitve passes')\n/usr/lib/pymodules/python2.7/matplotlib/axes.pyc in plot(self, args, kwargs)\n   3891         lines = []\n   3892 \n-> 3893         for line in self._get_lines(*args, kwargs):\n   3894             self.add_line(line)\n   3895             lines.append(line)\n/usr/lib/pymodules/python2.7/matplotlib/axes.pyc in _grab_next_args(self, args, **kwargs)\n    320                 return\n    321             if len(remaining) <= 3:\n--> 322                 for seg in self._plot_args(remaining, kwargs):\n    323                     yield seg\n    324                 return\n/usr/lib/pymodules/python2.7/matplotlib/axes.pyc in _plot_args(self, tup, kwargs)\n    298             x = np.arange(y.shape[0], dtype=float)\n    299 \n--> 300         x, y = self._xy_from_xy(x, y)\n    301 \n    302         if self.command == 'plot':\n/usr/lib/pymodules/python2.7/matplotlib/axes.pyc in _xy_from_xy(self, x, y)\n    238         y = np.atleast_1d(y)\n    239         if x.shape[0] != y.shape[0]:\n--> 240             raise ValueError(\"x and y must have same first dimension\")\n    241         if x.ndim > 2 or y.ndim > 2:\n    242             raise ValueError(\"x and y can be no greater than 2-D\")\nValueError: x and y must have same first dimension\n      3 cells failed to complete\n     26 code cells from notebook \nwrote /home/buildslave/nightly_default/build/notebooks/FGM.ipynb\nreal    5m57.060s\nuser    5m27.796s\nsys 0m0.848s\nconverting to html\n[NbConvertApp] Using existing profile dir: u'/home/buildslave/.ipython/profile_default'\n[NbConvertApp] Converting notebook FGM.ipynb to html\n[NbConvertApp] Support files will be in FGM_files/\n[NbConvertApp] Loaded template html_full.tpl\n[NbConvertApp] Writing 365019 bytes to FGM.html\n``\n. fixed\n. mmm can u copy-paste here the output of this:file /usr/local/Cellar/hdf5/1.8.12/lib/libhdf5.dylib. mmm can u copy-paste here the output of this:file /usr/local/Cellar/hdf5/1.8.12/lib/libhdf5.dylib. @dpo do you still get this error with the latestdevelop` branch?\n. mea culpa for that, but the seed was really set in a bad way :(\n. hi! i don't want to be a smartass here, and maybe you guys deliberately created such a toy example but i'd rather go with a more 'complicated' toy example, where you create your data grid with gaussian blobs and then do a kmeans over that.\nother than that the notebook looks great! kudos for that!\nmy 2 cents \n. is this related to #1701 ?\n. @lisitsyn will hack a bit more the xslt and there will be an output. until then you'll have to run the test on your own to get the exact errors\n. @karlnapf it's a hdf5 bug...\n. @karlnapf it's a hdf5 bug...\n. @PirosB3 we would like you to send a PR (pull request) instead of asking people to check on your forked repository... it is essential that you start working with PRs as that's how we do development during the whole GSoC.\neven if your code is not ready it's ok to send a PR as we'll discuss things in that PR and then you can change and add more commits to the PR obviously...\n. @karlnapf have you checked the code of stan? it's a horror. i mean distribution wise. the repository contains full src of boost, eigen (and something else i cannot remember now). we _do not want to bundle this into shogun. it would like bloat shogun's footprint by another 500megs or so....\n. @karlnapf have you checked the code of stan? it's a horror. i mean distribution wise. the repository contains full src of boost, eigen (and something else i cannot remember now). we _do not want to bundle this into shogun. it would like bloat shogun's footprint by another 500megs or so....\n. @karlnapf i reckon this can be closed, or?\n. @karlnapf how's the status of this one?\n. @karlnapf are we done with this?\n. noup this should not happen\n. @hushell i cannot agree with CFactorGraphFile. why dont we create an CUAIFile class that can handle UAI file in general, i.e. define all the right data structures and APIs and then when we have that ready, create maybe a CFactorGraphFile\nUAI has the following syntax:\n```\n\n\n```\nso in other words we need to define a function in CUAIFile that can parse any <Preamble> and then another function that can read the <Function tables> based on the preamble information.\nand then of course define the API in CUAIFile that supports reading these information...\n. @hushell same goes for CFactorGraph::convert_to_UAI(CFactorGraphFile* file, CFactorGraphFile* uai_file)... why would you have a file formatting/conversion function in CFactorGraph?\nit's purely IO related story.... it's like you would put csv to libsvm format conversion in LibSVM... why?\n. @aroma123 in short: not a good idea\n. @aroma123 let's concentrate on UAI itself now... but either it'll have to be a part of CUAIFile or a separate class. although since they are related they should be somehow coupled.. same goes for result file...\n. @aroma123 i'm not so sure if i understand the direct connection between a virtual function and an unimplemented functionality... as said earlier let's concentrate first on UAI itself.\n. @hushell this design pattern is completely flawed\nvoid CUAIFile::save(CFactorGraph* fg);\nit should be CFactorGraph::save(CUAIFile*) and CUAIFile should have interim variables to represent an uai file...\n. @Jiaolong currently we don't support multi-labelled libsvm files. but one of the entrance task (issue #1987) is to extend our current LibSVMFile file reader to support multilabels.\nit would be great if you could check/work on that issue!\n. @karlnapf mmm not really :( i've tried to help but we would really need an instance where i could do myself the debugging...\n. obsolete. one who is considering to work on this should really check out the new Pseudo-random number generation with c++11:\nhttp://en.cppreference.com/w/cpp/numeric/random\n. @lambday is there a particular reason you are storing the linear operator, i.e. m_linear_operator in EigenSolver? as personally i think it would be better just to have a simple constructor for the eigen solver and we would change the compute function to EigenSolver::compute(CLinearOperator<float64_t>* linear_operator). this way for example the eigen solver is reusable if you want to solve eigen problems for 2 different matrices. what's you opinion?\n. i've just ran magma's eigen solver test. example output:\nnote that the GPU i'm using is kind of old and simple... but still very good results. the CPU version is a simple lapack eigensolver...\n```\nMAGMA 1.4.1 , compiled for CUDA capability >= 1.0\ndevice 0: GeForce 320M, 950.0 MHz clock, 252.8 MB memory, capability 1.2\nUsage: ./testing_dsyevd [options] [-h|--help]\nN   CPU Time (sec)   GPU Time (sec)\n\n=======================================\n 1088      0.75             2.19\n 2112      5.52             2.16\n 3136     18.08             2.26\n 4160     43.85             0.03\n 5184     80.08             0.05\n 6208    136.36             0.05\n 7232    221.71             0.10\n 8256    333.74             0.08\n 9280    421.67             0.10\n``\n. @lambday there's GPU version of Lanczos solver both in MAGMA and in ViennaCL... we should introduce those as well, once we have this going nicely....\n. @lambday should we really differentiate betweencompute_all(...)andcompute_extremal(...)?\n. btw i've just seen that there'sSGMatrix::compute_eigenvectors(...),SGMatrix::compute_few_eigenvectors(...)ifLAPACKis available... shouldn't we get rid of these once the unifiedEigenSolver` is ready?\n@sonney2k according to git you've added those functions, what do you think?\n. @lambday but this is not true in case of CDirectEigenSolver, that computes all the eigen values...right?\n. @lambday lanczos works with sparse matrices only or not?\n. @lambday lol... well it's ok to keep DirectEigenSolver solver as we not always want to use Lanczos solver.... maybe the name of the DirectEigenSolver should be changed into some more meaningful? btw: can you come around irc instead of discussing it here in the issue? :)))\n. @lambday so here's my initial patch, but as you've mentioned on IRC maybe we should rework a bit the base EigenSolver class.\nshould we have then maybe:\nvirtual SGVector<float64_t> compute_all(CLinearOperator<float64_t>* linear_operator) = 0;\n/**\n * @param il low index of requested eigenpairs (1<=il<=n)\n * @param iu high index of requested eigenpairs (1<=il<=iu<=n)\n */\nvirtual SGVector<float64_t> compute_few(CLinearOperator<float64_t>* linear_operator, int il, int iu) = 0;\nand maybe the corresponding functions for SGMatrix<float64_t> and SGSparseMatrix<float64_t>? and just do an SG_NOTIMPLEMENTED or invalid operation for eigen solvers that does not support sparse or dense matrices?\nshould we remove the storage of the eigenvalues/eigenvectors? or we want to have them stored in the class for reusing (although i dont really see the point).\nMaybe we should return something else than a SGVector<float64_t> as we want to both return eigenvectors and eigenvalues... any ideas what would be the best data structure for that?\nor just follow what eigen (the library) does and store the eigenvalues/eigenvectors and with a function call they can be returned... and the object always stores the last eigen solution's eigenvector/values...\n. and the other thing i've thought about how we should deal with the different backends:\n a) we create for each backend a separate class, like for example for DirectEigenSolver there would be DirectLapackEigenSolver, DirectEigenEigenSolver (ok i know this name is horrible..), DirectMagmaEigenSolver etc.\nb) we have only one DirectEigenSolver and and in the header we create an enum based on the available backends and then the user can choose from which backend is being used. and then here with a shogun global parameter we can actually set (think about sg->parallel->num_threads())  globally which backend's eigensolver is being used. in case a) the same functionally can be only achieved via a eigen solver factory class as far as i can see now.\nthoughts?\n. and btw if we manage to do this with SVD as well then for example we can completely remove eigen dependency for PCA as it will be solved with whatever there is available...\n. closing this one as it has been fixed\n. please note that before you start reinventing the wheel there are some libraries that extensively worked with this topic:\n- http://www.cs.ubc.ca/research/flann/\n- https://code.google.com/p/nanoflann/\nnanoflann is great because it's like eigen, i.e. header only based and from the statistics seems to be better than flann itself...\n. @iglesias @mazumdarparijat this is done, or?\n. @iglesias @mazumdarparijat this is done, or?\n. no the question was is kd-tree implemented in shogun?\n. no the question was is kd-tree implemented in shogun?\n. yes it is. fixed\n. yes it is. fixed\n. this introduced a leak it seems #1989\n. @emaadmanzoor could you please wait half a day while i clarify with @karlnapf what actually he thought to do here...\n@karlnapf is this something similar that has been done with PCA ? if so i reckon we should rather go the same way as i started in #1930 \n. @emaadmanzoor could you please wait half a day while i clarify with @karlnapf what actually he thought to do here...\n@karlnapf is this something similar that has been done with PCA ? if so i reckon we should rather go the same way as i started in #1930 \n. @iglesias @mazumdarparijat this is done, or?\n. @iglesias @mazumdarparijat this is done, or?\n. @pl8787 thanks! about this last commit: please avoid code redundancy... there's a lot of lines of code that are shared between the unit tests like train and test set generation. this can be done by a function and then just call that function from the unit tests....\n. @pl8787 thanks! let's wait for travis...\ncould you a bit elaborate why 51fdd04 commit is the right way, i.e. why L2R_LR_DUAL should not be prob.n = w.vlen + 1 when use_bias is true? apart from the fact that it's going to fail? :)\n. @karlnapf would have loved that you dont merge this without triple check the LibLinear.cpp patch... you've merged something now about we are not so sure...\n. @pl8787 you've introduced some memory leaks: please fix them with a new PR... \nsee issue #1945 \n@karlnapf see a where fast merge leads us?\n. @karlnapf for the nth time :) travis has a soft limit of 20 minutes running of a job, and currently valgrinding all the unit tests and examples takes 6200+ seconds and that's really only the valgrind part not the compile which is another 15 minutes at least...\n. the problems with this patch is actually are:\n- CID3Classifier is implemented twice: once within the library and once within the examples. which i don't understand why would anybody do this?\n- a yet another tree is introduced to the library: there's really no need for that as there's already like 3 tree implementations in within the library. If the current multiclass/tree/TreeMachine.h is not generic enough for you, just extend/refactor it as you need, but please don't introduce a new class....\nthank you for the patch. contrary to @karlnapf comment, i think you could continue this pull request, as soon as you remove all those things i've mentioned above, the PR will be much much less, and easier to review\n. @karlnapf yeah like the LibLinear patch right? :) ok i'm just pulling your legs, don't take it personally...\n. @mazumdarparijat thanks! as i've mentioned in your previous PR, it would be great if we wouldn't have a yet another tree machine in the library, so please try to work on extending/refactoring the TreeMachine that it fits your/ID3 needs as well...\n. @mazumdarparijat i see 2 ways to overcome this problem:\n- well why not extend TreeMachine/TreeMachineNode to support N children per node instead of 2? and then change all the corresponding functions...?\n- rename TreeMachine/TreeMachineNode to BinaryTreeMachine or and then inherit that machine from GenericTreeMachine...\ncurrently GenericTreeMachine and TreeMachine are structurally almost the same, apart from one supporting 2 children per node the other N... i see this as a code redundancy, and we don't want that in Shogun as there's already a lot of line of codes in shogun...\n. mmm i've just checked again TreeMachine and TreeMachineNode.\nso there's NO way to do a simpler implementation of tree machine like in TreeMachine. That implementation has no limitation of the number of children a node can have...\nwhere the limitation comes in is actually in TreeMachineNode, but that you can easily generalise into an N children per node implemenation... and add a BinaryTreeMachineNode or similar inheritance from that general node and make sure all the current implementation of TreeMachine/TreeMachineNode are ported to support this, e.g.   ConditionalProbabilityTreeNodeData, BalancedConditionalProbabilityTree.h etc. will still work...\nso actually what i suggest you to do is to work on a generalisation of TreeMachineNode that supports N children per node and use that for implementing ID3 and then make sure that the other tree machines are still working...\n. and btw: unit tests are failing: https://travis-ci.org/shogun-toolbox/shogun/jobs/20268381#L8076\nthis is due to the fact that the you haven't added GenericTreeMachine to the ignore list in that unit test.\nnevermind, first let's try to refactor this PR and then take care of the unit tests...\n. @mazumdarparijat see my comments.\n. @mazumdarparijat regarding left() and right(): yes i saw the REQUIRE lines, but still those functions are just bad semantics as a general TreeMachineNode with N children has no left or right. hence those functions should not be the part of TreeMachineNode only of a class where there are only 2 children allowed per node, like an inherited class BinaryTreeMachineNode which can only have 2 children.\n. @mazumdarparijat i've just restarted the osx job since it didn't finish, so let's see how it goes...\n. @mazumdarparijat great job! and thanks for all the requested changes!\nas @iglesias it would be great to add some very basic unit tests for TreeMachineNode and BinaryTreeMachineNode so that we can even valgrind it to make it sure that it doesn't leak.\n. @mazumdarparijat as you can see there was just a glitch in travis, as now it is all green ;) so just add some simple unit tests and check for memory leak and then we can merge it!\n. as well as travis fails with this patch:\nhttps://travis-ci.org/shogun-toolbox/shogun/jobs/20278864#L4768\n. @tklein23 i reckon we are good with this or?\n. have you ran valgrind on this unit test?\nas far as i can see the Conditional jump or move depends on uninitialised value are actually in LibLinear.cpp that we inherited from the original liblinear implementation. \nthose errors are because the function does something with a variable that has not been initialised initially. Based on the error report you can check where the error happens and make sure that those variables are actually initialised when they are defined in the function.\nhere's some extra explanation and help how you can track these errors:\nhttp://stackoverflow.com/questions/2612447/pinpointing-conditional-jump-or-move-depends-on-uninitialized-values-valgrin \n. great! ok as soon as travis finishes the test on PR and it returns with OK i'll merge it and the other uninit errors can be fixed in an other PR as they are really liblinear related problems, it was only revealed but not introduced by your unit tests.\n. @karlnapf the remaining issues are available at our builtbot's page ;)\n. i've merged the #1957 PR, let's see on our buildbot how's the memory leak goes, but should be fine.\nCurrently on 2 of our bots the LibLinear unit tests are failing, and i suspect that this is due to the uninitialised memory errors... @pl8787 do you think you could work on fixing those? of course we can help out in it, just tell us if you want to work on it and where are you stuck.\n. @pl8787 those log outputs will not help you. our buildbot site is here: \nhttp://buildbot.shogun-toolbox.org/waterfall\nwhat needs to be fixed is actually all the valgrind errors, e.g. Conditional jump or move depends on uninitialised value that are in LibLinear.cpp\n. @pl8787 there seems to be some leaks still:\nhttp://buildbot.shogun-toolbox.org/memcheck/20140308-1105.html#idp235072\n. ok this is fixed\n. @pl8787 no need, it's available at our buildbot's site... dont waste your time on it...\n. @pl8787 i've restarted the failed job and seems now it passes. but anyhow SVMOcas is unrelated to LibLinear so it's safe to merge!\nThanks a lot!\n. STLs are fine if they are used in the implementation, i.e. .cpp\nregarding loading/storing: SVMlight format does support multilabeling? i.e. 1,2,3,7: 0:1, ...? if so then we'll need to fix our implementation, but none the less directly doing file manipulation here is just 'scary' :P\n. @tklein23 what do you think of just completely removing the load/save functionality (not just disabling it) from this PR and just put that code into a gist and have a reference to it in #1987 ? this way the only thing that will block the merging this PR would be #1972\n. @karlnapf mmmm travis is acting up a bit lately, but you can just restart the job that failed... usually that works out :P (btw you should be logged in to travis with your github account to be able to restart jobs or builds)\n. @karlnapf btw i dont see now travis failing?\nare you referring to the coverage comment? :)\n. great! but please make sure that the output of the notebook is always clear, we don't want to store the generated output of the notebook in the repository.\n. just one more thing: .ipynb_checkpoints directory or anything under it should not be added to repository\n. just one more thing: .ipynb_checkpoints directory or anything under it should not be added to repository\n. @ahcorde i'll close this PR as it's against the master branch and not against the develop branch. you should do development work in develop branch and send your PR against shogun repository's develop branch\n. this should be merged, but let's wait for travis to see it can really go, although i dont see any reason why it should fail... let's hope that everything goes ok and we can merge!\nthnx a lot for the patch!\n. this should be merged, but let's wait for travis to see it can really go, although i dont see any reason why it should fail... let's hope that everything goes ok and we can merge!\nthnx a lot for the patch!\n. @pl8787 travis works just there's a lot of PR currently and they all are being checked by travis:\nhttps://travis-ci.org/shogun-toolbox/shogun/pull_requests\n. @pl8787 travis works just there's a lot of PR currently and they all are being checked by travis:\nhttps://travis-ci.org/shogun-toolbox/shogun/pull_requests\n. @pl8787 yeah it's possible, which exact build number you'd like me to stop?\n. @pl8787 yeah it's possible, which exact build number you'd like me to stop?\n. @pl8787 sorry but you haven't answered the question.... which build do you want me to stop?\n. @pl8787 sorry but you haven't answered the question.... which build do you want me to stop?\n. @PirosB3 i reckon it's just sitting and waiting for a patch ;)\n. fixed\n. fixed\n. @sandeepkc yes but it'd be ideal to discuss your plan/ideas before you seriously start to work on this.\n. @sandeepkc think about what exactly?\n. @sandeepkc the only person who can tell this is yourself.\n. @sandeepkc that's great! but as i stated earlier before you start to put serious coding work into this it would be great to discuss how do you plan to solve this problem as serious design decisions have to be made during this task.\n. @sandeepkc yeah that's alright take your time..\n. @akpraharaj that's great! would be great to share some of your ideas... before starting do to serious coding...\n. @mazumdarparijat entropy is just -\\sum p_i ln(p_i) so which is a quite basic mathematical function. i.e. shouldn't be tied with multiclass. it should be part of CMath.\nyou can just add it straight to CMath as suggested in my comment where the probabilities are stored in a vector.\nhow to calculate those probabilities is a completely different story. that you can leave there as it is... maybe we could add later an entropy function for MutliclassLabel... but since ID3 is now the only one to use this, let's keep it only here.\n. @mazumdarparijat oh great, then why don't we use that in the first place? moreover there's mutual_info there as well which you could apply to calculate the information gain, right? or at least the relative_entropy part ...\n. @mazumdarparijat well why don't you just store the probabilities right away in an array, SGVector<float64_t> and just pass the underlying array to the entropy function on the end?\n. @mazumdarparijat the advantage is that we don't have code/functionality redundancy in the shogun's codebase, as it's already quite huge. moreover let's say we do some sorts of optimization for a given function, then we just have to do that in one place of the codebase and not all-over, which is getting more and more impossible with the codebase increasing by the time...\nfor this a very good example could be issue #1930 \ncurrently eigen's eigensolver is all-over the code and if we want to apply a new/faster eigensolver we would need to do that one by one everywhere in the code instead of just changing it in one place.\n. @mazumdarparijat the advantage is that we don't have code/functionality redundancy in the shogun's codebase, as it's already quite huge. moreover let's say we do some sorts of optimization for a given function, then we just have to do that in one place of the codebase and not all-over, which is getting more and more impossible with the codebase increasing by the time...\nfor this a very good example could be issue #1930 \ncurrently eigen's eigensolver is all-over the code and if we want to apply a new/faster eigensolver we would need to do that one by one everywhere in the code instead of just changing it in one place.\n. @karlnapf so here we go ;)\n. @karlnapf so here we go ;)\n. @karlnapf i was referring to our discussion about eigenizing things... basically this issue is exactly the start of that what you want to solve by eigenizing things. i.e. matrixmatrix, dot product etc.\n. @karlnapf i was referring to our discussion about eigenizing things... basically this issue is exactly the start of that what you want to solve by eigenizing things. i.e. matrixmatrix, dot product etc.\n. @lambday could you share your benchmarking code? as i'm really afraid that CPU caching kicks in (e.g. pipeline caching) hence we would really need to come up with a good way to do benchmarking... i'm sure someone on the 'internetz' investigated this whole thing how one could do true benchmarking of linalg operations. i mean the worst option here is to count the number of assembly operations :P but that i'd consider as a last resort\n. @lambday could you share your benchmarking code? as i'm really afraid that CPU caching kicks in (e.g. pipeline caching) hence we would really need to come up with a good way to do benchmarking... i'm sure someone on the 'internetz' investigated this whole thing how one could do true benchmarking of linalg operations. i mean the worst option here is to count the number of assembly operations :P but that i'd consider as a last resort\n. @khalednasr i reckon #2418 is covering this issue?\n. @khalednasr i reckon #2418 is covering this issue?\n. @khalednasr can we close this?\n. @khalednasr can we close this?\n. @vperic yep closing.... ;)\n. @akpraharaj to be honest i don't really understand what you would like to achieve here.\nbut the truth to be told now we have 7 commits in this PR and we are at a place where the PR cannot even be compiled.\ni suggest that you clear up a bit your PR and mind about what you would like to do here.\n. @akpraharaj to be honest i don't really understand what you would like to achieve here.\nbut the truth to be told now we have 7 commits in this PR and we are at a place where the PR cannot even be compiled.\ni suggest that you clear up a bit your PR and mind about what you would like to do here.\n. @abinashpanda there was a quite extensive discussion about this on the mailing list. see my comments here:\nhttp://article.gmane.org/gmane.comp.ai.machine-learning.shogun/4453 \n. @abinashpanda there was a quite extensive discussion about this on the mailing list. see my comments here:\nhttp://article.gmane.org/gmane.comp.ai.machine-learning.shogun/4453 \n. @tklein23 i don't really see a point to 'invent' a new format (especially when there are already more than a dozen) if one of them by default supports the functionality this issue requires. moreover, currently our libsvm file implementation is actually, because of this has a bug (not supporting multilabelling). So why not to fix this bug in libsvm and solve this issue at once.\nand to be honest to do this modification in LibSVMFile is more than trivial.\n. @Jiaolong yeah sure, but remember to send it to the shogun-data repository!\n. @Jiaolong no need to tell me i get an autogenerated email from github that you've sent a PR :)\nbut yeah thanks a lot! and i've merged it as you see\n. @sunil1337 yes, that could be most definitely the cause of it... you always have to make sure that you SG_UNREF an object that you created on heap.\n. @sunil1337 i've just commented the PR #1940 \nbut you should check the referenced valgrind output, it obviously points to one line in the unit test:\n==21977==    by 0x14D61F4: LogDetEstimator_Sparse_sample_constructor_Test::TestBody() (LogDetEstimator_unittest.cc:88)\ninstall valgrind on your machine and try to fix it locally and check the output of valgrind locally to see if your modification fixes the memory leak or not.\n. fixed\n. i'm sorry but this PR is not even near about what #1969 should be about\n. @nikolis yes, the question is the printed out variables are correct or not... if not they why not...\n. @nikolis yes, the question is the printed out variables are correct or not... if not they why not...\n. @gsomix awesome! thanks a lot!\n. @tklein23 any ideas why that shit happens with 20k lines? (see the bug report)\n. @oxphos dont waste your time on this i'll push tonight some major changes in a branch\n. @iRmantou yeah but actually i've picked it up yesterday... so sorry.. @hwl596 would have been better to send in a patch that actually removes the inline from that function. anyhow i've already did this.... thanks a lot anyways! \n. @sonney2k i think we should add an octave 3.8 build somewhere (travis or buildbot)\n@UsmanSyed i'll have to reproduce somehow this locally as otherwise it's going to be very hard for me to help....\n. mmmm what seems to be the problem, as i really don't think that this is a proper error report.\n. i'm closing this one as there is no proper description how to reproduce the bug...\ncheck the doxygen documentation for documentation and tutorials:\nhttp://shogun-toolbox.org/doc/en/latest/\n. regarding SFMT: no... we've just added SFMT. before even suggesting something like that, it'd be better to investigate the performance between SFMT/dSFMT and c++11\n. yeah we had a discussion about this with @cameo54321 on IRC. i've tried to suggest that it would be good to check out other libraries (e.g. stan, boost) to get some good ideas how our probability distrib API should look...\n. heheh this has been done a long time ago in f0fd43b9edc19f859849c99c32f5c44c5c0c81e0\n. @mazumdarparijat what's the status of this issue?\n. @dhruv13J if so then actually it's not so big changes so we can actually do the bundling.\nwhich is going to be pretty exciting for you (or not) since you'll have to do some fair amount of cmake hacking.... do you think you are up to that, or rather do something else? you can choose to do something else of course... i'll finish then the cmake hacking.... just let us know what you would like to do... of course i can give a hand with cmake hacking.\n. @abinashpanda there are at least 100+ other classes in shogun. please try to use your common sense\n. @abinashpanda and how about trying to do what i'm trying to say for the last 9 days about this issue: https://github.com/shogun-toolbox/shogun/issues/1913#issuecomment-37388247\n. @mazumdarparijat please try to follow a general rule: when you add anything anywhere check if there's anything there before and if yes then try to find and follow a pattern you see in the new thing you try to add instead of following a random way....\n. mmm this one killed a lot of tests :)\n. @frank0523 i reckon you can find a better name for the folder than deeplearning for neural networks... like for example neuralnets...?\n. @frank0523 i'm sorry i had to stop commenting your code...\nFrom me it seems like from this patch that you haven't got too familiar with the codebase of shogun:\n- you re-implement things that are unnecessary as they are already implemented in shogun\n- you use namespace much too much\n- you don't follow any of the styling that we require for code\n- you don't follow any of the design patterns that are in Shogun\n- you don't rely on any other parts of the Shogun codebase\n- and last but not least i truly believe that currently you don't know how the build system works of shogun.\nplease do not take this as an insult, but you really need to dig deeper into Shogun's codebase because this PR needs a full refactor\n. @grecocd mmmm... the next you are sending in a PR try to make is sure that the code actually compiles. see travis error:\nhttps://travis-ci.org/shogun-toolbox/shogun/jobs/21123065#L2181\n. @grecocd and before sending in the new commit please read this documentation:\nhttp://shogun-toolbox.org/doc/en/latest/developer.html\nespecially the code formatting part...\n. @Jiaolong please fix the travis error:\nhttps://travis-ci.org/shogun-toolbox/shogun/jobs/21126509#L4827\n[ERROR] In file /home/travis/build/shogun-toolbox/shogun/src/shogun/io/File.cpp line 66: Error opening file '../../../../data/multilabel/yeast_test.svm'\n. @Jiaolong you applied a lot of changes that were not necessary at all. i.e. the format changes. please revert them.\n. i know... the PR you've sent for shogun-data has been merged.\nnow your task is to change the reference to the HEAD of the shogun-data in shogun repository. this should be done withing this PR.\nin other words read this:\nhttps://github.com/shogun-toolbox/shogun-data/blob/master/README.md\n. great! there are some minor indent problems...\nmy only concern here is actually that currently with this it's not the file (and it's content) that predetermines the labeling (multilabel per line or single label) but the API.\nso you can actually call:\nvoid get_sparse_matrix(SGSparseVector<float64_t>*& matrix_feat, int32_t & num_feat, int32_t & num_vec,  SGSparseVector<float64_t>*& matrix_label, int32_t & num_classes);\non a single labelled file and vice-versa...\nthis design will just generate a lot of false issues, where people will complain when they feed different type of libsvm file to one of our examples.\nso what i'm actually trying to say is that multilabelling and 'single labelling' should be handled by the same API... the file's content should determine how to parse it.\n. @Jiaolong maybe we should refactor a bit CFile... since we can always represent single labaling with multilabeling. but i have to think about this a bit... but i would say that \nvoid get_sparse_matrix(SGSparseVector<float64_t>*& matrix_feat, int32_t & num_feat, int32_t & num_vec,  SGSparseVector<float64_t>*& matrix_label, int32_t & num_classes);\nAPI can handle both single and multilabeling.\n. @kprah honestly i don't know how to write a comment on this that i don't feel that i'm being rude. I know that you are working hard to try to solve this problem, and this is your 3rd PR, which i really appreciate, but.....\nbut as the last 2 times i've told you it would have been much better to actually do some planning, i.e. have some discussion about how and what to do and then execute it. You've told me that you gonna do this this time, but again you've disappeared for couple of days and now you came back with this PR....\nabout the PR itself:\nmy main concern is not that you are not following any of Shogun's code and design patterns (which already a problem), but you don't follow any object oriented design pattern, which is very essential when you use C++ for coding. for example in all your classes everything is defined public, you create classes not for representing entities, but for representing functions...\ni'm sorry if i offended you in any way, i'm really trying to do some constructive criticism here...\n. @pl8787 transcribing a matlab code into c++ is usually a very bad idea... and as i suspected you've used some base code and basically it has been transcribed into c++ and now it's like an untraceable implementation of something. sorry to be harsh but you need to refactor this PR \n. i reckon the problem is that data is not updated to HEAD in shogun repo\n. @karlnapf we'll soon merge a PR that'll fix the data version\n. @karlnapf we'll soon merge a PR that'll fix the data version\n. fixed\n. fixed\n. @frank0523 i believe that it might be better that you find another entrance task to solve as there's another PR already trying to solve this issue and it is in a far better state than this:\nhttps://github.com/shogun-toolbox/shogun/pull/2016\n. @Jiaolong we need to at least TODO the previous discussion: i.e. let the file content decide whether it's multi- or single-labelled, not the API... this means to mark in the the file as well not just a PR comment that is being lost as soon as you delete/merge it.\n. @Jiaolong mmm yeah, exactly that's what i wrote yesterday....\nbut we need to give helper functions so that one can convert SGSparseVector<float64_t>*& matrix_label into float64* if it's a single labelled.... so that actually one can supply that to CMulticlassLabels or CBinaryLabels\nand no that helper function should not be implemented in LibSVMFile...\n. honestly i don't really get what's point of having a class that has only static functions, see Elmat...\n. @pl8787 can you tell me what Elmat stands for?\n. @pl8787 one more thing: the next time you send in a PR please make it sure first that it compiles on your machine...\n. @pl8787 well that's the point... don't try to transcribe it.\n. @pl8787 well if there's a common format for recommendation then it would be good to have actually an IO class that can read such format.\n. @dhruv13J i'll check it in the next 12 hours...\n. @dhruv13J no... you need add it to include list. it's just that i reckon it doesn't have to be a system include, rather a normal include. see other examples in CMakeLists.txt\noh and btw you need to add HAVE_NANOFLANN to config.h.in as well. see the file for how to do that as well\n. @khalednasr awesome, thnx! i'll close #2098 as soon as this PR is being merged...\n. @khalednasr awesome, thnx! i'll close #2098 as soon as this PR is being merged...\n. @armanform you would need to refactor this PR as it's just not at all being integrated into Shogun. i suggest you to check out other class implementation (where the implementation reside within the library, where and how testing is being implemented).\n. no worries! thanks for taking care of it...\n. no worries! thanks for taking care of it...\n. sure go ahead!\n. sure go ahead!\n. @grecocd first a general question: why have you chosen to use KNN for calculating nearest neighbors? i mean strictly speaking KDE should rather be done on a binary search on a sorted array to find the 'nearest neighbor'... i'm just wondering what reference were you using to implement KDE?\nthe other problem: currently this implementation leaks memory. so please make sure that you release the memory/object that you've allocated.\n. oh and one more thing: you should register the class parameters to the parameter framework. see other implementation (hint SG_ADD)\n. btw: what are all these pure virtual functions standing for?\n. @grecocd i don't really see why would it make more sense to be inherited from CDistributions if you have to define all those pure virtual functions. if those functions are useless, i.e. doesn't make sense then it's a good sign that KDE should not be inherited from CDistributions.\n. @grecocd 'if I inherit from SGObject I will also have to define a lot of virtual functions for it to compile.'\nreally? a lot? mmm please recheck this, because i dont know what counts to be a 'lot' for you but the last time i've checked there's only one function you have to overload when you inherit from SGObject...\n. @ahcorde mmm the previous comment just got deleted... anyhow once more: have you used the template ipython notebook that we've provided in the repository for creating the notebook? if not please make sure that you use it...\nand please clear all the output cells before adding the notebook to the repository\n. @pickle27 gist?\n. btw are we sure that a simple knn would be the best classifier to tackle this classification problem?\nwe might as well use FGM (see http://shogun-toolbox.org/static/notebook/current/FGM.html) or neural nets for solving it? or maybe all of them? and compare their accuracy? ;)\n. anyhow kudos @ahcorde for the notebook. it's going to be great for sure!\n. actually, i have more comments but it'd be great to have the notebook without the output cells as that way we could directly comment the patch line by line.\n. @karlnapf wtf man?! you've just merged data into repository. please clean up this mess..\n. @ahcorde please prepare a new PR that removes the jpg from the repository and put that into shogun-data repository. and _again do _NOT commit notebooks with generated output. so pretty _PLEASE_ clear out all the output cells of the notebook....\n. closing this PR as it very much seems that did not find the root cause of the trouble and anyways it's trying to import files into the repository where they do not belong...\n. @tklein23 well it's not being used within the library but it's a good format for serialization\n. @ahcorde in other words do this:\ncd data && git checkout master && git pull && cd ..\ngit add data && git commit -m \"updating revision of data submodule\"\n. unfortunately this change was not good:\n- tests were failing in after_script because of the cd .. in after_success which is executed prior after_script.\n- anything that fails in after_script does not make the build job to be failed. which is bad since we want the build to be failed in case a test fails.\nhence i reverted this change...\n. @tklein23 no, let's merge it!\n. please test your code before submitting a PR\n. please test your code before submitting a PR\n. @sunil1337 it was segfaulting on travis\n. @sunil1337 it was segfaulting on travis\n. @lambday do we really want to do this compile time?\ni mean it would be more desirable for me to be able to switch the backend during runtime. at least when one calls init_shogun_with_defaults() \n. @kislayabhi yeah you would need to have opencv installed on travis machines.\n. btw is there a plan for how to support cv::gpu::GpuMat to shogun::CGPUMatrix\n. @pickle27 'We don't do any gpu stuff in shogun so there wouldn't be anything to directly convert to.' ???\nshogun::CGPUMatrix has been merged recently into develop branch. so most certainly we do gpu stuff in shogun... hence please do review the possibility to add support for it.\n. isn't this done by another PR that has been already merged?\n. isn't this done by another PR that has been already merged?\n. no-like :)\nwill get back with a more detailed comment within 24 hours\n. @jondo oh yeah, it should be. it can be updated via:\nhttps://docs.google.com/spreadsheet/ccc?key=0Aunb9cCVAP6NdDVBMzY1TjdPcmx4ei1EeUZNNGtKUHc&hl=en#gid=0\nwould really appreciate the update. \n. @jondo oh yeah, it should be. it can be updated via:\nhttps://docs.google.com/spreadsheet/ccc?key=0Aunb9cCVAP6NdDVBMzY1TjdPcmx4ei1EeUZNNGtKUHc&hl=en#gid=0\nwould really appreciate the update. \n. i would rather not have those classes/functions defined at all if the backend library dependency is missing.\n. fixed\n. fixed\n. ok so what's happening on my end is the following:\nwhenever a shogun object is being de-referenced in python i get a segfault with the following stacktrace:\n* thread #1: tid = 0x8898ce, 0x00000001000c2b0e Python`PyArg_UnpackTuple + 126, queue = 'com.apple.main-thread', stop reason = EXC_BAD_ACCESS (code=1, address=0x8)\n  * frame #0: 0x00000001000c2b0e Python`PyArg_UnpackTuple + 126\n    frame #1: 0x00000001032b0674 _modshogun.so`_wrap_delete_CSVFile(_object*, _object*) + 84\n    frame #2: 0x00000001032ac758 _modshogun.so`_wrap_delete_CSVFile_closure(_object*) + 56\n    frame #3: 0x0000000100065e92 Python`tupledealloc + 98\n    frame #4: 0x00000001000b1120 Python`PyEval_EvalFrameEx + 19904\n    frame #5: 0x00000001000ac052 Python`PyEval_EvalCodeEx + 1538\n    frame #6: 0x0000000100037b6c Python`function_call + 364\n    frame #7: 0x0000000100012033 Python`PyObject_Call + 99\n    frame #8: 0x00000001000af980 Python`PyEval_EvalFrameEx + 13856\n    frame #9: 0x00000001000ac052 Python`PyEval_EvalCodeEx + 1538\n    frame #10: 0x00000001000aba46 Python`PyEval_EvalCode + 54\n    frame #11: 0x00000001000d5884 Python`PyRun_FileExFlags + 164\n    frame #12: 0x00000001000d5401 Python`PyRun_SimpleFileExFlags + 769\n    frame #13: 0x00000001000eae98 Python`Py_Main + 3096\n    frame #14: 0x00007fff8e8345c9 libdyld.dylib`start + 1\n. ok so what's happening on my end is the following:\nwhenever a shogun object is being de-referenced in python i get a segfault with the following stacktrace:\n* thread #1: tid = 0x8898ce, 0x00000001000c2b0e Python`PyArg_UnpackTuple + 126, queue = 'com.apple.main-thread', stop reason = EXC_BAD_ACCESS (code=1, address=0x8)\n  * frame #0: 0x00000001000c2b0e Python`PyArg_UnpackTuple + 126\n    frame #1: 0x00000001032b0674 _modshogun.so`_wrap_delete_CSVFile(_object*, _object*) + 84\n    frame #2: 0x00000001032ac758 _modshogun.so`_wrap_delete_CSVFile_closure(_object*) + 56\n    frame #3: 0x0000000100065e92 Python`tupledealloc + 98\n    frame #4: 0x00000001000b1120 Python`PyEval_EvalFrameEx + 19904\n    frame #5: 0x00000001000ac052 Python`PyEval_EvalCodeEx + 1538\n    frame #6: 0x0000000100037b6c Python`function_call + 364\n    frame #7: 0x0000000100012033 Python`PyObject_Call + 99\n    frame #8: 0x00000001000af980 Python`PyEval_EvalFrameEx + 13856\n    frame #9: 0x00000001000ac052 Python`PyEval_EvalCodeEx + 1538\n    frame #10: 0x00000001000aba46 Python`PyEval_EvalCode + 54\n    frame #11: 0x00000001000d5884 Python`PyRun_FileExFlags + 164\n    frame #12: 0x00000001000d5401 Python`PyRun_SimpleFileExFlags + 769\n    frame #13: 0x00000001000eae98 Python`Py_Main + 3096\n    frame #14: 0x00007fff8e8345c9 libdyld.dylib`start + 1\n. ok so i've just compared the two cxx sources that were generated by swig2.0 and swig3.0.\nbasically what happens is the follow: swig3.0 adds the following line into the beginning of each wrapped method:\nif(!PyArg_UnpackTuple(args,(char *)\"new_CSVFile\",0,0)) SWIG_fail;\nso for example in case of CCSVFile, for the constructor it adds:\nif(!PyArg_UnpackTuple(args,(char *)\"new_CSVFile\",0,0)) SWIG_fail;\nand for the destructor it adds:\nif(!PyArg_UnpackTuple(args,(char *)\"delete_CSVFile\",0,0)) SWIG_fail;\nand in case of the destructor the passed args variable is  NULL, hence the segfault.\n. ok so i've just compared the two cxx sources that were generated by swig2.0 and swig3.0.\nbasically what happens is the follow: swig3.0 adds the following line into the beginning of each wrapped method:\nif(!PyArg_UnpackTuple(args,(char *)\"new_CSVFile\",0,0)) SWIG_fail;\nso for example in case of CCSVFile, for the constructor it adds:\nif(!PyArg_UnpackTuple(args,(char *)\"new_CSVFile\",0,0)) SWIG_fail;\nand for the destructor it adds:\nif(!PyArg_UnpackTuple(args,(char *)\"delete_CSVFile\",0,0)) SWIG_fail;\nand in case of the destructor the passed args variable is  NULL, hence the segfault.\n. and it seems that this is something of a python feature of swig3.x as according to @matthuska the r modular interface generated by swig3.x works without any problems.\n. and it seems that this is something of a python feature of swig3.x as according to @matthuska the r modular interface generated by swig3.x works without any problems.\n. i'm sorry but WTF is this?!\nyou do\n```\nifdef HAVE_EIGEN3\n...\nendif\n```\nbut _CLEARY_ in LDA.h:\n```\nifdef HAVE_LAPACK\n...\n``\n.LDA` was not ported to eigen... it is still using lapack.\n. indeed.\n. i've already pushed the fix....\n. @xiaoleiw that has nothing to do with it. your problem is that the system cannot find where libshogun.16.so has been installed. please specify the right path....\n. @xiaoleiw that has nothing to do with it. your problem is that the system cannot find where libshogun.16.so has been installed. please specify the right path....\n. @mazumdarparijat i wonder why the shogun version is a) slower b) lower accuracy :S\ni reckon we should rather still concentrate on getting new methods in shogun within your GSoC task, and concentrate later on the speed of the implemented algorithms. could you share the benchmarking code somewhere so we can later go back and work on optimisation...?\n. @lambday maybe because travis is a vps with 1 vcpu + 3 GB free ram...?\n. @lambday maybe because travis is a vps with 1 vcpu + 3 GB free ram...?\n. @abinashpanda please always make sure that when you add a notebook to clear all it's output cells. as the current one you want to merge has some of its output fields already set, i.e. not empty.\n. OOB can be taken as crossval. as it measures the error on the elements that are not in bag (just like crossval). In other words if you have N samples in your training set, and the bag is M < N, then the OOB will be measured on the N-M (previously not seen) samples. It's pretty much the same as crossval, as there as well you take out N-M elements for testing... the leftout number of element number in case of crossval will depend on the fold size.\nlong story short, OOB should vary between 2 runs, and it's as good as x-val for estimating the accuracy of the model.\n. @khalednasr please fix all your neuralnets notebooks according to this bug (see my patches)\n. @karlnapf in a way it breaks things but we've managed to get our notebooks into such a horrific state, that we need to clean them up, i.e. fix them ASAP. i think now we only have 1-2 broken notebooks left. which of course still a problem as till we don't get the notebooks fixed, the notebook page (http://shogun-toolbox.org/page/documentation/notebook) will be empty. \nshit coding + merging:tm:\n. @karlnapf 'travis hook on the buildbot'? you mean github hook on the buildbot, right? well i hope somebody will have time for that...\n. i reckon this is done\n. just my 2 cents: should we create a new namespace for gpu based stuff?\ne.g. like opencv does it: cv::gpu::, in our case it would be shogun::gpu:: ?\n. @mazumdarparijat wouldn't ball-tree be enough? i mean do we need kd-tree AND ball-tree as well?\n. mmm i reckon this has something to do with SSE2\n@postoroniy what's the arch/cpu in that machine where you want to compile shogun?\n. mmm i reckon this has something to do with SSE2\n@postoroniy what's the arch/cpu in that machine where you want to compile shogun?\n. @postoroniy what's your swig version? :)\nsee #2313 \n. @abinashpanda @tklein23 what's the status of this one?\n. @dpo you can run ctest -R unit which will basically run the unit tests. as far as i remember they are not depending on the data repository itself.\n. yeah first we need to get those work. since now all the bots/travis are properly broken\n. yeah first we need to get those work. since now all the bots/travis are properly broken\n. done by using ppa repositories\n. fixed\n. the actual problem is not really in cpp but in the generated csharp code, i.e. DeepBeliefNetwork.cs, namely:\n```\n public virtual double[,] get_weights(int i, double[] p) {\n        IntPtr ptr = modshogunPINVOKE.DeepBeliefNetwork_get_weights__SWIG_0(swigCPtr, i, p.Length, p);\n    if (modshogunPINVOKE.SWIGPendingException.Pending) throw modshogunPINVOKE.SWIGPendingException.Retrieve();\n        int[] ranks = new int[2];\n        Marshal.Copy(ptr, ranks, 0, 2);\n    int rows = ranks[0];\n    int cols = ranks[1];\n    int len = rows * cols;\n\n    double[] ret = new double[len];\n\n    Marshal.Copy(new IntPtr(ptr.ToInt64() + 2 * Marshal.SizeOf(typeof(int))), ret, 0, len);\n\n    double[,] result = new double[rows, cols];\n    for (int i = 0; i < rows; i++) {\n            for (int j = 0; j < cols; j++) {\n                    result[i, j] = ret[i * cols + j];\n            }\n    }\n    return result;\n\n}\n```\nand\n```\n  public virtual double[,] get_weights(int i) {\n        IntPtr ptr = modshogunPINVOKE.DeepBeliefNetwork_get_weights__SWIG_1(swigCPtr, i);\n    if (modshogunPINVOKE.SWIGPendingException.Pending) throw modshogunPINVOKE.SWIGPendingException.Retrieve();\n        int[] ranks = new int[2];\n        Marshal.Copy(ptr, ranks, 0, 2);\n    int rows = ranks[0];\n    int cols = ranks[1];\n    int len = rows * cols;\n\n    double[] ret = new double[len];\n\n    Marshal.Copy(new IntPtr(ptr.ToInt64() + 2 * Marshal.SizeOf(typeof(int))), ret, 0, len);\n\n    double[,] result = new double[rows, cols];\n    for (int i = 0; i < rows; i++) {\n            for (int j = 0; j < cols; j++) {\n                    result[i, j] = ret[i * cols + j];\n            }\n    }\n    return result;\n\n}\n```\n. @khalednasr almost, neural nets python example still have some errors:\nStart 412: python_modular-neuralnets_simple_modular\n411/562 Test #412: python_modular-neuralnets_simple_modular ............................................................................................***Failed    0.30 sec\nNeural nets\nTraceback (most recent call last):\n  File \"/home/travis/build/shogun-toolbox/shogun/examples/undocumented/python_modular/neuralnets_simple_modular.py\", line 29, in <module>\n    neuralnets_simple_modular(*parameter_list[0])\n  File \"/home/travis/build/shogun-toolbox/shogun/examples/undocumented/python_modular/neuralnets_simple_modular.py\", line 24, in neuralnets_simple_modular\n    network.train(feats_train)\nSystemError: [ERROR] In file /home/travis/build/shogun-toolbox/shogun/src/shogun/machine/Machine.cpp line 60: NeuralNetwork@0xc45ab0: No labels given\nhopefully the C# modular interface gets fixed by this PR ;)\n. @yorkerlin perfect, works thnx! as for future reference please start referring the issue number like #2407 in your commit descriptions. github will automatically detect it will put a reference to the given commit in the given issue's page.\n. @yorkerlin perfect, works thnx! as for future reference please start referring the issue number like #2407 in your commit descriptions. github will automatically detect it will put a reference to the given commit in the given issue's page.\n. fixed by #2404 \n. @iglesias on bsd bot.\n@mazumdarparijat you won't be able to test it like this\n. @mazumdarparijat neither BLAS nor LAPACK is not available at this moment on the bsd bot, i could try installing lapack but again the problem here is that it doesn't work as it supposed to be. you could try disabling lapack on your own machine and try to run like that the PCA unit test. that way you can make sure whether it's because missing LAPACK, i.e. wrong way of handling missing libraries or because of something else.\n. @mazumdarparijat any ideas for solution. if you want me to test anything on that bot send it to me as i have shell access to that machine so i can try to compile/run anything there.\n. @Jiaolong as it says it's for tracing memory allocations within the library. i reckon the problem is in your code, as no other part of shogun has a problem with compiling with TRACE_MEMORY_ALLOCS enabled.\n. @Jiaolong actually why aren't you following the coding standard of shogun and using sg_malloc directly?!\nplease use SG_* macros for memory management, i.e. in case of GraphCut.cpp:102:\nSG_MALLOC(Node, m_num_nodes)\nplease apply this to all the code you've written and used sg_malloc function instead of the previously mentioned MACRO...\n. @tklein23 one is part of the integration test the other is simply running the python modular examples. \n. i've just restarted some jobs regarding this PR on travis.\nbtw: i reckon even if we install viennacl on travis that would help nothing as i reckon a travis node doesnt have any hw backend that could actually be used by viennacl\n. i've just restarted some jobs regarding this PR on travis.\nbtw: i reckon even if we install viennacl on travis that would help nothing as i reckon a travis node doesnt have any hw backend that could actually be used by viennacl\n. @mazumdarparijat travis had for sure lapack ;)\n. @mazumdarparijat travis had for sure lapack ;)\n. http://buildbot.shogun-toolbox.org/builders/bsd1%20-%20libshogun/builds/2444/steps/configure/logs/stdio\n. @lambday here are some more information for you.\ni've displayed the following matrices and vectors in the unit tests in the given order: data, labels_vec, selected_data\nboth on freebsd bot (where the unit tests fails) and on a linux machine where it passes.\nfreebsd bot result:\n54: [==========] Running 1 test from 1 test case.\n54: [----------] Global test environment set-up.\n54: [----------] 1 test from BAHSIC\n54: [ RUN      ] BAHSIC.apply\n54: matrix=[\n54: [   0.0250000000000000014,  0.225000000000000006,   0.424999999999999989,   0.625,  0.824999999999999956],\n54: [   0.0500000000000000028,  0.25,   0.450000000000000011,   0.650000000000000022,   0.849999999999999978],\n54: [   0.0749999999999999972,  0.275000000000000022,   0.474999999999999978,   0.675000000000000044,   0.875],\n54: [   0.100000000000000006,   0.299999999999999989,   0.5,    0.699999999999999956,   0.900000000000000022],\n54: [   0.125,  0.325000000000000011,   0.525000000000000022,   0.724999999999999978,   0.925000000000000044],\n54: [   0.149999999999999994,   0.349999999999999978,   0.550000000000000044,   0.75,   0.949999999999999956],\n54: [   0.174999999999999989,   0.375,  0.574999999999999956,   0.775000000000000022,   0.974999999999999978],\n54: [   0.200000000000000011,   0.400000000000000022,   0.599999999999999978,   0.800000000000000044,   1]\n54: ]\n54: vector=[0,1,0,1,1]\n54: matrix=[\n54: [   0.0500000000000000028,  0.25,   0.450000000000000011,   0.650000000000000022,   0.849999999999999978],\n54: [   0.0749999999999999972,  0.275000000000000022,   0.474999999999999978,   0.675000000000000044,   0.875],\n54: [   0.149999999999999994,   0.349999999999999978,   0.550000000000000044,   0.75,   0.949999999999999956],\n54: [   0.174999999999999989,   0.375,  0.574999999999999956,   0.775000000000000022,   0.974999999999999978]\n54: ]\n54: /home/wiking/shogun/tests/unit/preprocessor/BAHSIC_unittest.cc:95: Failure\nlinux bot results:\n8: [ RUN      ] BAHSIC.apply\n8: matrix=[\n8: [    0.0250000000000000014,  0.225000000000000006,   0.424999999999999989,   0.625,  0.824999999999999956],\n8: [    0.0500000000000000028,  0.25,   0.450000000000000011,   0.650000000000000022,   0.849999999999999978],\n8: [    0.0749999999999999972,  0.275000000000000022,   0.474999999999999978,   0.675000000000000044,   0.875],\n8: [    0.100000000000000006,   0.299999999999999989,   0.5,    0.699999999999999956,   0.900000000000000022],\n8: [    0.125,  0.325000000000000011,   0.525000000000000022,   0.724999999999999978,   0.925000000000000044],\n8: [    0.149999999999999994,   0.349999999999999978,   0.550000000000000044,   0.75,   0.949999999999999956],\n8: [    0.174999999999999989,   0.375,  0.574999999999999956,   0.775000000000000022,   0.974999999999999978],\n8: [    0.200000000000000011,   0.400000000000000022,   0.599999999999999978,   0.800000000000000044,   1]\n8: ]\n8: vector=[0,1,0,1,1]\n8: matrix=[\n8: [    0.0749999999999999972,  0.275000000000000022,   0.474999999999999978,   0.675000000000000044,   0.875],\n8: [    0.149999999999999994,   0.349999999999999978,   0.550000000000000044,   0.75,   0.949999999999999956],\n8: [    0.174999999999999989,   0.375,  0.574999999999999956,   0.775000000000000022,   0.974999999999999978],\n8: [    0.200000000000000011,   0.400000000000000022,   0.599999999999999978,   0.800000000000000044,   1]\n8: ]\n8: [       OK ] BAHSIC.apply (1 ms)\n8: [----------] 1 test from BAHSIC (1 ms total)\nas you can see data matrix and labels_vec are exactly the same, but selected_data matrix is somehow shifted by row or something, i.e. row 1 of  selected_data on freebsd bot is exactly the same as row 0 on linux bot.\ncould you please elaborate how this could happen?\n. @lambday you mean this (running on the freebsd bot):\n```\n[wiking@sandbox ~/shogun/build]$ ctest -R unit-BAHSIC -VV\nUpdateCTestConfiguration  from :/home/wiking/shogun/build/DartConfiguration.tcl\nUpdateCTestConfiguration  from :/home/wiking/shogun/build/DartConfiguration.tcl\nTest project /home/wiking/shogun/build\nConstructing a list of tests\nDone constructing a list of tests\nChecking test dependency graph...\nChecking test dependency graph end\ntest 54\n    Start 54: unit-BAHSIC\n54: Test command: /home/wiking/shogun/build/tests/unit/shogun-unit-test \"--gtest_filter=BAHSIC.\"\n54: Test timeout computed to be: 9.99988e+06\n54: Note: Google Test filter = BAHSIC.\n54: [==========] Running 1 test from 1 test case.\n54: [----------] Global test environment set-up.\n54: [----------] 1 test from BAHSIC\n54: [ RUN      ] BAHSIC.apply\n54: matrix=[\n54: [   0.0250000000000000014,  0.225000000000000006,   0.424999999999999989,   0.625,  0.824999999999999956],\n54: [   0.0500000000000000028,  0.25,   0.450000000000000011,   0.650000000000000022,   0.849999999999999978],\n54: [   0.0749999999999999972,  0.275000000000000022,   0.474999999999999978,   0.675000000000000044,   0.875],\n54: [   0.100000000000000006,   0.299999999999999989,   0.5,    0.699999999999999956,   0.900000000000000022],\n54: [   0.125,  0.325000000000000011,   0.525000000000000022,   0.724999999999999978,   0.925000000000000044],\n54: [   0.149999999999999994,   0.349999999999999978,   0.550000000000000044,   0.75,   0.949999999999999956],\n54: [   0.174999999999999989,   0.375,  0.574999999999999956,   0.775000000000000022,   0.974999999999999978],\n54: [   0.200000000000000011,   0.400000000000000022,   0.599999999999999978,   0.800000000000000044,   1]\n54: ]\n54: vector=[0,1,0,1,1]\n54: [DEBUG] Entering!\n54: [DEBUG] entering DenseFeatures::clone()\n54: [DEBUG] constructing an empty instance of DenseFeatures\n54: [DEBUG] cloning parameter \"properties\" at index 0\n54: [DEBUG] entering TParameter::copy()\n54: [DEBUG] Comparing datatypes without length\n54: [DEBUG] leaving TSGDataType::equals_without_length(): data types without lengths are equal\n54: [DEBUG] CT_SCALAR\n54: [DEBUG] entering TParameter::copy_stype()\n54: [DEBUG] ST_NONE\n54: [DEBUG] entering TParameter::copy_ptype()\n54: [DEBUG] after copy of ptype PT_UINT64: source 1, target 1\n54: [DEBUG] leaving TParameter::copy_ptype(): Copy successful\n54: [DEBUG] leaving TParameter::copy(): Copy successful\n54: [DEBUG] cloning parameter \"cache_size\" at index 1\n54: [DEBUG] entering TParameter::copy()\n54: [DEBUG] Comparing datatypes without length\n54: [DEBUG] leaving TSGDataType::equals_without_length(): data types without lengths are equal\n54: [DEBUG] CT_SCALAR\n54: [DEBUG] entering TParameter::copy_stype()\n54: [DEBUG] ST_NONE\n54: [DEBUG] entering TParameter::copy_ptype()\n54: [DEBUG] after copy of ptype PT_INT32: source 0, target 0\n54: [DEBUG] leaving TParameter::copy_ptype(): Copy successful\n54: [DEBUG] leaving TParameter::copy(): Copy successful\n54: [DEBUG] cloning parameter \"preproc\" at index 2\n54: [DEBUG] entering TParameter::copy()\n54: [DEBUG] Comparing datatypes without length\n54: [DEBUG] leaving TSGDataType::equals_without_length(): data types without lengths are equal\n54: [DEBUG] CT_SCALAR\n54: [DEBUG] entering TParameter::copy_stype()\n54: [DEBUG] ST_NONE\n54: [DEBUG] entering TParameter::copy_ptype()\n54: [DEBUG] entering DynamicObjectArray::clone()\n54: [DEBUG] constructing an empty instance of DynamicObjectArray\n54: [DEBUG] cloning parameter \"array\" at index 0\n54: [DEBUG] entering TParameter::copy()\n54: [DEBUG] Comparing datatypes without length\n54: [DEBUG] leaving TSGDataType::equals_without_length(): data types without lengths are equal\n54: [DEBUG] CT_VECTOR or CT_SGVECTOR\n54: [DEBUG] entering TSGDataType::equals()\n54: [DEBUG] leaving TSGDataType::equals_without_length(): data types without lengths are equal\n54: [DEBUG] leaving TSGDataType::equals(): datatypes are equal\n54: [DEBUG] entering TSGDataType::equals()\n54: [DEBUG] leaving TSGDataType::equals_without_length(): data types without lengths are equal\n54: [DEBUG] leaving TSGDataType::equals(): datatypes are equal\n54: [DEBUG] length_y: 0\n54: [DEBUG] leaving TParameter::copy(): Copy successful\n54: [DEBUG] cloning parameter \"num_elements\" at index 1\n54: [DEBUG] entering TParameter::copy()\n54: [DEBUG] Comparing datatypes without length\n54: [DEBUG] leaving TSGDataType::equals_without_length(): data types without lengths are equal\n54: [DEBUG] CT_SCALAR\n54: [DEBUG] entering TParameter::copy_stype()\n54: [DEBUG] ST_NONE\n54: [DEBUG] entering TParameter::copy_ptype()\n54: [DEBUG] after copy of ptype PT_INT32: source 128, target 128\n54: [DEBUG] leaving TParameter::copy_ptype(): Copy successful\n54: [DEBUG] leaving TParameter::copy(): Copy successful\n54: [DEBUG] cloning parameter \"resize_granularity\" at index 2\n54: [DEBUG] entering TParameter::copy()\n54: [DEBUG] Comparing datatypes without length\n54: [DEBUG] leaving TSGDataType::equals_without_length(): data types without lengths are equal\n54: [DEBUG] CT_SCALAR\n54: [DEBUG] entering TParameter::copy_stype()\n54: [DEBUG] ST_NONE\n54: [DEBUG] entering TParameter::copy_ptype()\n54: [DEBUG] after copy of ptype PT_INT32: source 128, target 128\n54: [DEBUG] leaving TParameter::copy_ptype(): Copy successful\n54: [DEBUG] leaving TParameter::copy(): Copy successful\n54: [DEBUG] cloning parameter \"use_sg_malloc\" at index 3\n54: [DEBUG] entering TParameter::copy()\n54: [DEBUG] Comparing datatypes without length\n54: [DEBUG] leaving TSGDataType::equals_without_length(): data types without lengths are equal\n54: [DEBUG] CT_SCALAR\n54: [DEBUG] entering TParameter::copy_stype()\n54: [DEBUG] ST_NONE\n54: [DEBUG] entering TParameter::copy_ptype()\n54: [DEBUG] after copy of ptype PT_BOOL: source 1, target 1\n54: [DEBUG] leaving TParameter::copy_ptype(): Copy successful\n54: [DEBUG] leaving TParameter::copy(): Copy successful\n54: [DEBUG] cloning parameter \"free_array\" at index 4\n54: [DEBUG] entering TParameter::copy()\n54: [DEBUG] Comparing datatypes without length\n54: [DEBUG] leaving TSGDataType::equals_without_length(): data types without lengths are equal\n54: [DEBUG] CT_SCALAR\n54: [DEBUG] entering TParameter::copy_stype()\n54: [DEBUG] ST_NONE\n54: [DEBUG] entering TParameter::copy_ptype()\n54: [DEBUG] after copy of ptype PT_BOOL: source 1, target 1\n54: [DEBUG] leaving TParameter::copy_ptype(): Copy successful\n54: [DEBUG] leaving TParameter::copy(): Copy successful\n54: [DEBUG] leaving DynamicObjectArray::clone(): Clone successful\n54: [DEBUG] leaving TParameter::copy_ptype(): Copy successful\n54: [DEBUG] leaving TParameter::copy(): Copy successful\n54: [DEBUG] cloning parameter \"preprocessed\" at index 3\n54: [DEBUG] entering TParameter::copy()\n54: [DEBUG] Comparing datatypes without length\n54: [DEBUG] leaving TSGDataType::equals_without_length(): data types without lengths are equal\n54: [DEBUG] CT_SCALAR\n54: [DEBUG] entering TParameter::copy_stype()\n54: [DEBUG] ST_NONE\n54: [DEBUG] entering TParameter::copy_ptype()\n54: [DEBUG] entering DynamicArray::clone()\n54: [DEBUG] constructing an empty instance of DynamicArray\n54: [DEBUG] cloning parameter \"array\" at index 0\n54: [DEBUG] entering TParameter::copy()\n54: [DEBUG] Comparing datatypes without length\n54: [DEBUG] leaving TSGDataType::equals_without_length(): data types without lengths are equal\n54: [DEBUG] CT_VECTOR or CT_SGVECTOR\n54: [DEBUG] entering TSGDataType::equals()\n54: [DEBUG] leaving TSGDataType::equals_without_length(): data types without lengths are equal\n54: [DEBUG] leaving TSGDataType::equals(): datatypes are equal\n54: [DEBUG] entering TSGDataType::equals()\n54: [DEBUG] leaving TSGDataType::equals_without_length(): data types without lengths are equal\n54: [DEBUG] leaving TSGDataType::equals(): datatypes are equal\n54: [DEBUG] length_y: 0\n54: [DEBUG] leaving TParameter::copy(): Copy successful\n54: [DEBUG] cloning parameter \"num_elements\" at index 1\n54: [DEBUG] entering TParameter::copy()\n54: [DEBUG] Comparing datatypes without length\n54: [DEBUG] leaving TSGDataType::equals_without_length(): data types without lengths are equal\n54: [DEBUG] CT_SCALAR\n54: [DEBUG] entering TParameter::copy_stype()\n54: [DEBUG] ST_NONE\n54: [DEBUG] entering TParameter::copy_ptype()\n54: [DEBUG] after copy of ptype PT_INT32: source 128, target 128\n54: [DEBUG] leaving TParameter::copy_ptype(): Copy successful\n54: [DEBUG] leaving TParameter::copy(): Copy successful\n54: [DEBUG] cloning parameter \"resize_granularity\" at index 2\n54: [DEBUG] entering TParameter::copy()\n54: [DEBUG] Comparing datatypes without length\n54: [DEBUG] leaving TSGDataType::equals_without_length(): data types without lengths are equal\n54: [DEBUG] CT_SCALAR\n54: [DEBUG] entering TParameter::copy_stype()\n54: [DEBUG] ST_NONE\n54: [DEBUG] entering TParameter::copy_ptype()\n54: [DEBUG] after copy of ptype PT_INT32: source 128, target 128\n54: [DEBUG] leaving TParameter::copy_ptype(): Copy successful\n54: [DEBUG] leaving TParameter::copy(): Copy successful\n54: [DEBUG] cloning parameter \"use_sg_malloc\" at index 3\n54: [DEBUG] entering TParameter::copy()\n54: [DEBUG] Comparing datatypes without length\n54: [DEBUG] leaving TSGDataType::equals_without_length(): data types without lengths are equal\n54: [DEBUG] CT_SCALAR\n54: [DEBUG] entering TParameter::copy_stype()\n54: [DEBUG] ST_NONE\n54: [DEBUG] entering TParameter::copy_ptype()\n54: [DEBUG] after copy of ptype PT_BOOL: source 1, target 1\n54: [DEBUG] leaving TParameter::copy_ptype(): Copy successful\n54: [DEBUG] leaving TParameter::copy(): Copy successful\n54: [DEBUG] cloning parameter \"free_array\" at index 4\n54: [DEBUG] entering TParameter::copy()\n54: [DEBUG] Comparing datatypes without length\n54: [DEBUG] leaving TSGDataType::equals_without_length(): data types without lengths are equal\n54: [DEBUG] CT_SCALAR\n54: [DEBUG] entering TParameter::copy_stype()\n54: [DEBUG] ST_NONE\n54: [DEBUG] entering TParameter::copy_ptype()\n54: [DEBUG] after copy of ptype PT_BOOL: source 1, target 1\n54: [DEBUG] leaving TParameter::copy_ptype(): Copy successful\n54: [DEBUG] leaving TParameter::copy(): Copy successful\n54: [DEBUG] leaving DynamicArray::clone(): Clone successful\n54: [DEBUG] leaving TParameter::copy_ptype(): Copy successful\n54: [DEBUG] leaving TParameter::copy(): Copy successful\n54: [DEBUG] cloning parameter \"subset_stack\" at index 4\n54: [DEBUG] entering TParameter::copy()\n54: [DEBUG] Comparing datatypes without length\n54: [DEBUG] leaving TSGDataType::equals_without_length(): data types without lengths are equal\n54: [DEBUG] CT_SCALAR\n54: [DEBUG] entering TParameter::copy_stype()\n54: [DEBUG] ST_NONE\n54: [DEBUG] entering TParameter::copy_ptype()\n54: [DEBUG] entering SubsetStack::clone()\n54: [DEBUG] constructing an empty instance of SubsetStack\n54: [DEBUG] cloning parameter \"active_subset\" at index 0\n54: [DEBUG] entering TParameter::copy()\n54: [DEBUG] Comparing datatypes without length\n54: [DEBUG] leaving TSGDataType::equals_without_length(): data types without lengths are equal\n54: [DEBUG] CT_SCALAR\n54: [DEBUG] entering TParameter::copy_stype()\n54: [DEBUG] ST_NONE\n54: [DEBUG] entering TParameter::copy_ptype()\n54: [DEBUG] leaving TParameter::copy_ptype(): Both SGObjects are NULL\n54: [DEBUG] leaving TParameter::copy(): Copy successful\n54: [DEBUG] cloning parameter \"active_subsets_stack\" at index 1\n54: [DEBUG] entering TParameter::copy()\n54: [DEBUG] Comparing datatypes without length\n54: [DEBUG] leaving TSGDataType::equals_without_length(): data types without lengths are equal\n54: [DEBUG] CT_SCALAR\n54: [DEBUG] entering TParameter::copy_stype()\n54: [DEBUG] ST_NONE\n54: [DEBUG] entering TParameter::copy_ptype()\n54: [DEBUG] entering DynamicObjectArray::clone()\n54: [DEBUG] constructing an empty instance of DynamicObjectArray\n54: [DEBUG] cloning parameter \"array\" at index 0\n54: [DEBUG] entering TParameter::copy()\n54: [DEBUG] Comparing datatypes without length\n54: [DEBUG] leaving TSGDataType::equals_without_length(): data types without lengths are equal\n54: [DEBUG] CT_VECTOR or CT_SGVECTOR\n54: [DEBUG] entering TSGDataType::equals()\n54: [DEBUG] leaving TSGDataType::equals_without_length(): data types without lengths are equal\n54: [DEBUG] leaving TSGDataType::equals(): datatypes are equal\n54: [DEBUG] entering TSGDataType::equals()\n54: [DEBUG] leaving TSGDataType::equals_without_length(): data types without lengths are equal\n54: [DEBUG] leaving TSGDataType::equals(): datatypes are equal\n54: [DEBUG] length_y: 0\n54: [DEBUG] leaving TParameter::copy(): Copy successful\n54: [DEBUG] cloning parameter \"num_elements\" at index 1\n54: [DEBUG] entering TParameter::copy()\n54: [DEBUG] Comparing datatypes without length\n54: [DEBUG] leaving TSGDataType::equals_without_length(): data types without lengths are equal\n54: [DEBUG] CT_SCALAR\n54: [DEBUG] entering TParameter::copy_stype()\n54: [DEBUG] ST_NONE\n54: [DEBUG] entering TParameter::copy_ptype()\n54: [DEBUG] after copy of ptype PT_INT32: source 128, target 128\n54: [DEBUG] leaving TParameter::copy_ptype(): Copy successful\n54: [DEBUG] leaving TParameter::copy(): Copy successful\n54: [DEBUG] cloning parameter \"resize_granularity\" at index 2\n54: [DEBUG] entering TParameter::copy()\n54: [DEBUG] Comparing datatypes without length\n54: [DEBUG] leaving TSGDataType::equals_without_length(): data types without lengths are equal\n54: [DEBUG] CT_SCALAR\n54: [DEBUG] entering TParameter::copy_stype()\n54: [DEBUG] ST_NONE\n54: [DEBUG] entering TParameter::copy_ptype()\n54: [DEBUG] after copy of ptype PT_INT32: source 128, target 128\n54: [DEBUG] leaving TParameter::copy_ptype(): Copy successful\n54: [DEBUG] leaving TParameter::copy(): Copy successful\n54: [DEBUG] cloning parameter \"use_sg_malloc\" at index 3\n54: [DEBUG] entering TParameter::copy()\n54: [DEBUG] Comparing datatypes without length\n54: [DEBUG] leaving TSGDataType::equals_without_length(): data types without lengths are equal\n54: [DEBUG] CT_SCALAR\n54: [DEBUG] entering TParameter::copy_stype()\n54: [DEBUG] ST_NONE\n54: [DEBUG] entering TParameter::copy_ptype()\n54: [DEBUG] after copy of ptype PT_BOOL: source 1, target 1\n54: [DEBUG] leaving TParameter::copy_ptype(): Copy successful\n54: [DEBUG] leaving TParameter::copy(): Copy successful\n54: [DEBUG] cloning parameter \"free_array\" at index 4\n54: [DEBUG] entering TParameter::copy()\n54: [DEBUG] Comparing datatypes without length\n54: [DEBUG] leaving TSGDataType::equals_without_length(): data types without lengths are equal\n54: [DEBUG] CT_SCALAR\n54: [DEBUG] entering TParameter::copy_stype()\n54: [DEBUG] ST_NONE\n54: [DEBUG] entering TParameter::copy_ptype()\n54: [DEBUG] after copy of ptype PT_BOOL: source 1, target 1\n54: [DEBUG] leaving TParameter::copy_ptype(): Copy successful\n54: [DEBUG] leaving TParameter::copy(): Copy successful\n54: [DEBUG] leaving DynamicObjectArray::clone(): Clone successful\n54: [DEBUG] leaving TParameter::copy_ptype(): Copy successful\n54: [DEBUG] leaving TParameter::copy(): Copy successful\n54: [DEBUG] leaving SubsetStack::clone(): Clone successful\n54: [DEBUG] leaving TParameter::copy_ptype(): Copy successful\n54: [DEBUG] leaving TParameter::copy(): Copy successful\n54: [DEBUG] cloning parameter \"combined_weight\" at index 5\n54: [DEBUG] entering TParameter::copy()\n54: [DEBUG] Comparing datatypes without length\n54: [DEBUG] leaving TSGDataType::equals_without_length(): data types without lengths are equal\n54: [DEBUG] CT_SCALAR\n54: [DEBUG] entering TParameter::copy_stype()\n54: [DEBUG] ST_NONE\n54: [DEBUG] entering TParameter::copy_ptype()\n54: [DEBUG] after copy of ptype PT_FLOAT64: source 1.000000, target 1.000000\n54: [DEBUG] leaving TParameter::copy_ptype(): Copy successful\n54: [DEBUG] leaving TParameter::copy(): Copy successful\n54: [DEBUG] cloning parameter \"num_vectors\" at index 6\n54: [DEBUG] entering TParameter::copy()\n54: [DEBUG] Comparing datatypes without length\n54: [DEBUG] leaving TSGDataType::equals_without_length(): data types without lengths are equal\n54: [DEBUG] CT_SCALAR\n54: [DEBUG] entering TParameter::copy_stype()\n54: [DEBUG] ST_NONE\n54: [DEBUG] entering TParameter::copy_ptype()\n54: [DEBUG] after copy of ptype PT_INT32: source 5, target 5\n54: [DEBUG] leaving TParameter::copy_ptype(): Copy successful\n54: [DEBUG] leaving TParameter::copy(): Copy successful\n54: [DEBUG] cloning parameter \"num_features\" at index 7\n54: [DEBUG] entering TParameter::copy()\n54: [DEBUG] Comparing datatypes without length\n54: [DEBUG] leaving TSGDataType::equals_without_length(): data types without lengths are equal\n54: [DEBUG] CT_SCALAR\n54: [DEBUG] entering TParameter::copy_stype()\n54: [DEBUG] ST_NONE\n54: [DEBUG] entering TParameter::copy_ptype()\n54: [DEBUG] after copy of ptype PT_INT32: source 8, target 8\n54: [DEBUG] leaving TParameter::copy_ptype(): Copy successful\n54: [DEBUG] leaving TParameter::copy(): Copy successful\n54: [DEBUG] cloning parameter \"feature_matrix\" at index 8\n54: [DEBUG] entering TParameter::copy()\n54: [DEBUG] Comparing datatypes without length\n54: [DEBUG] leaving TSGDataType::equals_without_length(): data types without lengths are equal\n54: [DEBUG] CT_MATRIX or CT_SGMATRIX\n54: [DEBUG] entering TSGDataType::equals()\n54: [DEBUG] leaving TSGDataType::equals_without_length(): data types without lengths are equal\n54: [DEBUG] leaving TSGDataType::equals(): length_y=8 while other's length_y=0\n54: [DEBUG] changing size of target vector and freeing memory\n54: [DEBUG] allocating memory for target vector\n54: [DEBUG] 320 bytes are allocated\n54: [DEBUG] entering TSGDataType::equals()\n54: [DEBUG] leaving TSGDataType::equals_without_length(): data types without lengths are equal\n54: [DEBUG] leaving TSGDataType::equals(): datatypes are equal\n54: [DEBUG] length_y: 8\n54: [DEBUG] length_x: 5\n54: [DEBUG] copying element 0 which is 0 byes from start\n54: [DEBUG] entering TParameter::copy_stype()\n54: [DEBUG] ST_NONE\n54: [DEBUG] entering TParameter::copy_ptype()\n54: [DEBUG] after copy of ptype PT_FLOAT64: source 0.025000, target 0.025000\n54: [DEBUG] leaving TParameter::copy_ptype(): Copy successful\n54: [DEBUG] copying element 1 which is 8 byes from start\n54: [DEBUG] entering TParameter::copy_stype()\n54: [DEBUG] ST_NONE\n54: [DEBUG] entering TParameter::copy_ptype()\n54: [DEBUG] after copy of ptype PT_FLOAT64: source 0.050000, target 0.050000\n54: [DEBUG] leaving TParameter::copy_ptype(): Copy successful\n54: [DEBUG] copying element 2 which is 16 byes from start\n54: [DEBUG] entering TParameter::copy_stype()\n54: [DEBUG] ST_NONE\n54: [DEBUG] entering TParameter::copy_ptype()\n54: [DEBUG] after copy of ptype PT_FLOAT64: source 0.075000, target 0.075000\n54: [DEBUG] leaving TParameter::copy_ptype(): Copy successful\n54: [DEBUG] copying element 3 which is 24 byes from start\n54: [DEBUG] entering TParameter::copy_stype()\n54: [DEBUG] ST_NONE\n54: [DEBUG] entering TParameter::copy_ptype()\n54: [DEBUG] after copy of ptype PT_FLOAT64: source 0.100000, target 0.100000\n54: [DEBUG] leaving TParameter::copy_ptype(): Copy successful\n54: [DEBUG] copying element 4 which is 32 byes from start\n54: [DEBUG] entering TParameter::copy_stype()\n54: [DEBUG] ST_NONE\n54: [DEBUG] entering TParameter::copy_ptype()\n54: [DEBUG] after copy of ptype PT_FLOAT64: source 0.125000, target 0.125000\n54: [DEBUG] leaving TParameter::copy_ptype(): Copy successful\n54: [DEBUG] copying element 5 which is 40 byes from start\n54: [DEBUG] entering TParameter::copy_stype()\n54: [DEBUG] ST_NONE\n54: [DEBUG] entering TParameter::copy_ptype()\n54: [DEBUG] after copy of ptype PT_FLOAT64: source 0.150000, target 0.150000\n54: [DEBUG] leaving TParameter::copy_ptype(): Copy successful\n54: [DEBUG] copying element 6 which is 48 byes from start\n54: [DEBUG] entering TParameter::copy_stype()\n54: [DEBUG] ST_NONE\n54: [DEBUG] entering TParameter::copy_ptype()\n54: [DEBUG] after copy of ptype PT_FLOAT64: source 0.175000, target 0.175000\n54: [DEBUG] leaving TParameter::copy_ptype(): Copy successful\n54: [DEBUG] copying element 7 which is 56 byes from start\n54: [DEBUG] entering TParameter::copy_stype()\n54: [DEBUG] ST_NONE\n54: [DEBUG] entering TParameter::copy_ptype()\n54: [DEBUG] after copy of ptype PT_FLOAT64: source 0.200000, target 0.200000\n54: [DEBUG] leaving TParameter::copy_ptype(): Copy successful\n54: [DEBUG] copying element 8 which is 64 byes from start\n54: [DEBUG] entering TParameter::copy_stype()\n54: [DEBUG] ST_NONE\n54: [DEBUG] entering TParameter::copy_ptype()\n54: [DEBUG] after copy of ptype PT_FLOAT64: source 0.225000, target 0.225000\n54: [DEBUG] leaving TParameter::copy_ptype(): Copy successful\n54: [DEBUG] copying element 9 which is 72 byes from start\n54: [DEBUG] entering TParameter::copy_stype()\n54: [DEBUG] ST_NONE\n54: [DEBUG] entering TParameter::copy_ptype()\n54: [DEBUG] after copy of ptype PT_FLOAT64: source 0.250000, target 0.250000\n54: [DEBUG] leaving TParameter::copy_ptype(): Copy successful\n54: [DEBUG] copying element 10 which is 80 byes from start\n54: [DEBUG] entering TParameter::copy_stype()\n54: [DEBUG] ST_NONE\n54: [DEBUG] entering TParameter::copy_ptype()\n54: [DEBUG] after copy of ptype PT_FLOAT64: source 0.275000, target 0.275000\n54: [DEBUG] leaving TParameter::copy_ptype(): Copy successful\n54: [DEBUG] copying element 11 which is 88 byes from start\n54: [DEBUG] entering TParameter::copy_stype()\n54: [DEBUG] ST_NONE\n54: [DEBUG] entering TParameter::copy_ptype()\n54: [DEBUG] after copy of ptype PT_FLOAT64: source 0.300000, target 0.300000\n54: [DEBUG] leaving TParameter::copy_ptype(): Copy successful\n54: [DEBUG] copying element 12 which is 96 byes from start\n54: [DEBUG] entering TParameter::copy_stype()\n54: [DEBUG] ST_NONE\n54: [DEBUG] entering TParameter::copy_ptype()\n54: [DEBUG] after copy of ptype PT_FLOAT64: source 0.325000, target 0.325000\n54: [DEBUG] leaving TParameter::copy_ptype(): Copy successful\n54: [DEBUG] copying element 13 which is 104 byes from start\n54: [DEBUG] entering TParameter::copy_stype()\n54: [DEBUG] ST_NONE\n54: [DEBUG] entering TParameter::copy_ptype()\n54: [DEBUG] after copy of ptype PT_FLOAT64: source 0.350000, target 0.350000\n54: [DEBUG] leaving TParameter::copy_ptype(): Copy successful\n54: [DEBUG] copying element 14 which is 112 byes from start\n54: [DEBUG] entering TParameter::copy_stype()\n54: [DEBUG] ST_NONE\n54: [DEBUG] entering TParameter::copy_ptype()\n54: [DEBUG] after copy of ptype PT_FLOAT64: source 0.375000, target 0.375000\n54: [DEBUG] leaving TParameter::copy_ptype(): Copy successful\n54: [DEBUG] copying element 15 which is 120 byes from start\n54: [DEBUG] entering TParameter::copy_stype()\n54: [DEBUG] ST_NONE\n54: [DEBUG] entering TParameter::copy_ptype()\n54: [DEBUG] after copy of ptype PT_FLOAT64: source 0.400000, target 0.400000\n54: [DEBUG] leaving TParameter::copy_ptype(): Copy successful\n54: [DEBUG] copying element 16 which is 128 byes from start\n54: [DEBUG] entering TParameter::copy_stype()\n54: [DEBUG] ST_NONE\n54: [DEBUG] entering TParameter::copy_ptype()\n54: [DEBUG] after copy of ptype PT_FLOAT64: source 0.425000, target 0.425000\n54: [DEBUG] leaving TParameter::copy_ptype(): Copy successful\n54: [DEBUG] copying element 17 which is 136 byes from start\n54: [DEBUG] entering TParameter::copy_stype()\n54: [DEBUG] ST_NONE\n54: [DEBUG] entering TParameter::copy_ptype()\n54: [DEBUG] after copy of ptype PT_FLOAT64: source 0.450000, target 0.450000\n54: [DEBUG] leaving TParameter::copy_ptype(): Copy successful\n54: [DEBUG] copying element 18 which is 144 byes from start\n54: [DEBUG] entering TParameter::copy_stype()\n54: [DEBUG] ST_NONE\n54: [DEBUG] entering TParameter::copy_ptype()\n54: [DEBUG] after copy of ptype PT_FLOAT64: source 0.475000, target 0.475000\n54: [DEBUG] leaving TParameter::copy_ptype(): Copy successful\n54: [DEBUG] copying element 19 which is 152 byes from start\n54: [DEBUG] entering TParameter::copy_stype()\n54: [DEBUG] ST_NONE\n54: [DEBUG] entering TParameter::copy_ptype()\n54: [DEBUG] after copy of ptype PT_FLOAT64: source 0.500000, target 0.500000\n54: [DEBUG] leaving TParameter::copy_ptype(): Copy successful\n54: [DEBUG] copying element 20 which is 160 byes from start\n54: [DEBUG] entering TParameter::copy_stype()\n54: [DEBUG] ST_NONE\n54: [DEBUG] entering TParameter::copy_ptype()\n54: [DEBUG] after copy of ptype PT_FLOAT64: source 0.525000, target 0.525000\n54: [DEBUG] leaving TParameter::copy_ptype(): Copy successful\n54: [DEBUG] copying element 21 which is 168 byes from start\n54: [DEBUG] entering TParameter::copy_stype()\n54: [DEBUG] ST_NONE\n54: [DEBUG] entering TParameter::copy_ptype()\n54: [DEBUG] after copy of ptype PT_FLOAT64: source 0.550000, target 0.550000\n54: [DEBUG] leaving TParameter::copy_ptype(): Copy successful\n54: [DEBUG] copying element 22 which is 176 byes from start\n54: [DEBUG] entering TParameter::copy_stype()\n54: [DEBUG] ST_NONE\n54: [DEBUG] entering TParameter::copy_ptype()\n54: [DEBUG] after copy of ptype PT_FLOAT64: source 0.575000, target 0.575000\n54: [DEBUG] leaving TParameter::copy_ptype(): Copy successful\n54: [DEBUG] copying element 23 which is 184 byes from start\n54: [DEBUG] entering TParameter::copy_stype()\n54: [DEBUG] ST_NONE\n54: [DEBUG] entering TParameter::copy_ptype()\n54: [DEBUG] after copy of ptype PT_FLOAT64: source 0.600000, target 0.600000\n54: [DEBUG] leaving TParameter::copy_ptype(): Copy successful\n54: [DEBUG] copying element 24 which is 192 byes from start\n54: [DEBUG] entering TParameter::copy_stype()\n54: [DEBUG] ST_NONE\n54: [DEBUG] entering TParameter::copy_ptype()\n54: [DEBUG] after copy of ptype PT_FLOAT64: source 0.625000, target 0.625000\n54: [DEBUG] leaving TParameter::copy_ptype(): Copy successful\n54: [DEBUG] copying element 25 which is 200 byes from start\n54: [DEBUG] entering TParameter::copy_stype()\n54: [DEBUG] ST_NONE\n54: [DEBUG] entering TParameter::copy_ptype()\n54: [DEBUG] after copy of ptype PT_FLOAT64: source 0.650000, target 0.650000\n54: [DEBUG] leaving TParameter::copy_ptype(): Copy successful\n54: [DEBUG] copying element 26 which is 208 byes from start\n54: [DEBUG] entering TParameter::copy_stype()\n54: [DEBUG] ST_NONE\n54: [DEBUG] entering TParameter::copy_ptype()\n54: [DEBUG] after copy of ptype PT_FLOAT64: source 0.675000, target 0.675000\n54: [DEBUG] leaving TParameter::copy_ptype(): Copy successful\n54: [DEBUG] copying element 27 which is 216 byes from start\n54: [DEBUG] entering TParameter::copy_stype()\n54: [DEBUG] ST_NONE\n54: [DEBUG] entering TParameter::copy_ptype()\n54: [DEBUG] after copy of ptype PT_FLOAT64: source 0.700000, target 0.700000\n54: [DEBUG] leaving TParameter::copy_ptype(): Copy successful\n54: [DEBUG] copying element 28 which is 224 byes from start\n54: [DEBUG] entering TParameter::copy_stype()\n54: [DEBUG] ST_NONE\n54: [DEBUG] entering TParameter::copy_ptype()\n54: [DEBUG] after copy of ptype PT_FLOAT64: source 0.725000, target 0.725000\n54: [DEBUG] leaving TParameter::copy_ptype(): Copy successful\n54: [DEBUG] copying element 29 which is 232 byes from start\n54: [DEBUG] entering TParameter::copy_stype()\n54: [DEBUG] ST_NONE\n54: [DEBUG] entering TParameter::copy_ptype()\n54: [DEBUG] after copy of ptype PT_FLOAT64: source 0.750000, target 0.750000\n54: [DEBUG] leaving TParameter::copy_ptype(): Copy successful\n54: [DEBUG] copying element 30 which is 240 byes from start\n54: [DEBUG] entering TParameter::copy_stype()\n54: [DEBUG] ST_NONE\n54: [DEBUG] entering TParameter::copy_ptype()\n54: [DEBUG] after copy of ptype PT_FLOAT64: source 0.775000, target 0.775000\n54: [DEBUG] leaving TParameter::copy_ptype(): Copy successful\n54: [DEBUG] copying element 31 which is 248 byes from start\n54: [DEBUG] entering TParameter::copy_stype()\n54: [DEBUG] ST_NONE\n54: [DEBUG] entering TParameter::copy_ptype()\n54: [DEBUG] after copy of ptype PT_FLOAT64: source 0.800000, target 0.800000\n54: [DEBUG] leaving TParameter::copy_ptype(): Copy successful\n54: [DEBUG] copying element 32 which is 256 byes from start\n54: [DEBUG] entering TParameter::copy_stype()\n54: [DEBUG] ST_NONE\n54: [DEBUG] entering TParameter::copy_ptype()\n54: [DEBUG] after copy of ptype PT_FLOAT64: source 0.825000, target 0.825000\n54: [DEBUG] leaving TParameter::copy_ptype(): Copy successful\n54: [DEBUG] copying element 33 which is 264 byes from start\n54: [DEBUG] entering TParameter::copy_stype()\n54: [DEBUG] ST_NONE\n54: [DEBUG] entering TParameter::copy_ptype()\n54: [DEBUG] after copy of ptype PT_FLOAT64: source 0.850000, target 0.850000\n54: [DEBUG] leaving TParameter::copy_ptype(): Copy successful\n54: [DEBUG] copying element 34 which is 272 byes from start\n54: [DEBUG] entering TParameter::copy_stype()\n54: [DEBUG] ST_NONE\n54: [DEBUG] entering TParameter::copy_ptype()\n54: [DEBUG] after copy of ptype PT_FLOAT64: source 0.875000, target 0.875000\n54: [DEBUG] leaving TParameter::copy_ptype(): Copy successful\n54: [DEBUG] copying element 35 which is 280 byes from start\n54: [DEBUG] entering TParameter::copy_stype()\n54: [DEBUG] ST_NONE\n54: [DEBUG] entering TParameter::copy_ptype()\n54: [DEBUG] after copy of ptype PT_FLOAT64: source 0.900000, target 0.900000\n54: [DEBUG] leaving TParameter::copy_ptype(): Copy successful\n54: [DEBUG] copying element 36 which is 288 byes from start\n54: [DEBUG] entering TParameter::copy_stype()\n54: [DEBUG] ST_NONE\n54: [DEBUG] entering TParameter::copy_ptype()\n54: [DEBUG] after copy of ptype PT_FLOAT64: source 0.925000, target 0.925000\n54: [DEBUG] leaving TParameter::copy_ptype(): Copy successful\n54: [DEBUG] copying element 37 which is 296 byes from start\n54: [DEBUG] entering TParameter::copy_stype()\n54: [DEBUG] ST_NONE\n54: [DEBUG] entering TParameter::copy_ptype()\n54: [DEBUG] after copy of ptype PT_FLOAT64: source 0.950000, target 0.950000\n54: [DEBUG] leaving TParameter::copy_ptype(): Copy successful\n54: [DEBUG] copying element 38 which is 304 byes from start\n54: [DEBUG] entering TParameter::copy_stype()\n54: [DEBUG] ST_NONE\n54: [DEBUG] entering TParameter::copy_ptype()\n54: [DEBUG] after copy of ptype PT_FLOAT64: source 0.975000, target 0.975000\n54: [DEBUG] leaving TParameter::copy_ptype(): Copy successful\n54: [DEBUG] copying element 39 which is 312 byes from start\n54: [DEBUG] entering TParameter::copy_stype()\n54: [DEBUG] ST_NONE\n54: [DEBUG] entering TParameter::copy_ptype()\n54: [DEBUG] after copy of ptype PT_FLOAT64: source 1.000000, target 1.000000\n54: [DEBUG] leaving TParameter::copy_ptype(): Copy successful\n54: [DEBUG] leaving TParameter::copy(): Copy successful\n54: [DEBUG] leaving DenseFeatures::clone(): Clone successful\n54: [DEBUG] Entering!\n54: [DEBUG] Entering!\n54: [DEBUG] entering CKernel::remove_lhs_and_rhs\n54: [DEBUG] leaving CKernel::remove_lhs_and_rhs\n54: [DEBUG] entering CKernel::remove_lhs_and_rhs\n54: [DEBUG] leaving CKernel::remove_lhs_and_rhs\n54: [DEBUG] leaving CKernel::init(0x808e62f40, 0x808e62f40)\n54: [DEBUG] returning kernel matrix of size 5x5\n54: [DEBUG] Entering\n54: [DEBUG] Entering\n54: [DEBUG] Leaving\n54: [DEBUG] using custom kernel of size 5x5\n54: [DEBUG] entering CKernel::remove_lhs_and_rhs\n54: [DEBUG] leaving CKernel::remove_lhs_and_rhs\n54: [DEBUG] leaving CKernel::init(0x808c3aac0, 0x808c3b9c0)\n54: [DEBUG] num_vec_lhs: 5 vs num_rows 5\n54: [DEBUG] num_vec_rhs: 5 vs num_cols 5\n54: [DEBUG] Leaving\n54: [DEBUG] entering CKernel::remove_lhs_and_rhs\n54: [DEBUG] leaving CKernel::remove_lhs_and_rhs\n54: [DEBUG] entering CKernel::remove_lhs_and_rhs\n54: [DEBUG] leaving CKernel::remove_lhs_and_rhs\n54: [INFO] Kernel deleted (0x808eb13c0).\n54: [DEBUG] Leaving!\n54: [DEBUG] Initial number of features 8!\n54: [DEBUG] Entering!\n54: [DEBUG] Entering!\n54: dims=[1,2,3,4,5,6,7]\n54: [DEBUG] Leaving!\n54: [DEBUG] Entering!\n54: [DEBUG] Leaving!\n54: [DEBUG] entering!\n54: [DEBUG] entering!\n54: [DEBUG] entering CKernel::remove_lhs_and_rhs\n54: [DEBUG] leaving CKernel::remove_lhs_and_rhs\n54: [DEBUG] entering CKernel::remove_lhs_and_rhs\n54: [DEBUG] leaving CKernel::remove_lhs_and_rhs\n54: [DEBUG] leaving CKernel::init(0x808eb6180, 0x808eb6180)\n54: [DEBUG] returning kernel matrix of size 5x5\n54: [DEBUG] leaving!\n54: [DEBUG] entering!\n54: [DEBUG] returning kernel matrix of size 5x5\n54: [DEBUG] leaving!\n54: [DEBUG] Number of samples 5!\n54: [DEBUG] leaving!\n54: [DEBUG] statistic = 0.070779!\n54: [DEBUG] Leaving!\n54: [DEBUG] Entering!\n54: [DEBUG] Entering!\n54: dims=[0,2,3,4,5,6,7]\n54: [DEBUG] Leaving!\n54: [DEBUG] Entering!\n54: [DEBUG] Leaving!\n54: [DEBUG] entering!\n54: [DEBUG] entering!\n54: [DEBUG] entering CKernel::remove_lhs_and_rhs\n54: [DEBUG] leaving CKernel::remove_lhs_and_rhs\n54: [DEBUG] entering CKernel::remove_lhs_and_rhs\n54: [DEBUG] leaving CKernel::remove_lhs_and_rhs\n54: [DEBUG] leaving CKernel::init(0x808eb6240, 0x808eb6240)\n54: [DEBUG] returning kernel matrix of size 5x5\n54: [DEBUG] leaving!\n54: [DEBUG] entering!\n54: [DEBUG] returning kernel matrix of size 5x5\n54: [DEBUG] leaving!\n54: [DEBUG] Number of samples 5!\n54: [DEBUG] leaving!\n54: [DEBUG] statistic = 0.070779!\n54: [DEBUG] Leaving!\n54: [DEBUG] Entering!\n54: [DEBUG] Entering!\n54: dims=[0,1,3,4,5,6,7]\n54: [DEBUG] Leaving!\n54: [DEBUG] Entering!\n54: [DEBUG] Leaving!\n54: [DEBUG] entering!\n54: [DEBUG] entering!\n54: [DEBUG] entering CKernel::remove_lhs_and_rhs\n54: [DEBUG] leaving CKernel::remove_lhs_and_rhs\n54: [DEBUG] entering CKernel::remove_lhs_and_rhs\n54: [DEBUG] leaving CKernel::remove_lhs_and_rhs\n54: [DEBUG] leaving CKernel::init(0x808eb6180, 0x808eb6180)\n54: [DEBUG] returning kernel matrix of size 5x5\n54: [DEBUG] leaving!\n54: [DEBUG] entering!\n54: [DEBUG] returning kernel matrix of size 5x5\n54: [DEBUG] leaving!\n54: [DEBUG] Number of samples 5!\n54: [DEBUG] leaving!\n54: [DEBUG] statistic = 0.070779!\n54: [DEBUG] Leaving!\n54: [DEBUG] Entering!\n54: [DEBUG] Entering!\n54: dims=[0,1,2,4,5,6,7]\n54: [DEBUG] Leaving!\n54: [DEBUG] Entering!\n54: [DEBUG] Leaving!\n54: [DEBUG] entering!\n54: [DEBUG] entering!\n54: [DEBUG] entering CKernel::remove_lhs_and_rhs\n54: [DEBUG] leaving CKernel::remove_lhs_and_rhs\n54: [DEBUG] entering CKernel::remove_lhs_and_rhs\n54: [DEBUG] leaving CKernel::remove_lhs_and_rhs\n54: [DEBUG] leaving CKernel::init(0x808eb6240, 0x808eb6240)\n54: [DEBUG] returning kernel matrix of size 5x5\n54: [DEBUG] leaving!\n54: [DEBUG] entering!\n54: [DEBUG] returning kernel matrix of size 5x5\n54: [DEBUG] leaving!\n54: [DEBUG] Number of samples 5!\n54: [DEBUG] leaving!\n54: [DEBUG] statistic = 0.070779!\n54: [DEBUG] Leaving!\n54: [DEBUG] Entering!\n54: [DEBUG] Entering!\n54: dims=[0,1,2,3,5,6,7]\n54: [DEBUG] Leaving!\n54: [DEBUG] Entering!\n54: [DEBUG] Leaving!\n54: [DEBUG] entering!\n54: [DEBUG] entering!\n54: [DEBUG] entering CKernel::remove_lhs_and_rhs\n54: [DEBUG] leaving CKernel::remove_lhs_and_rhs\n54: [DEBUG] entering CKernel::remove_lhs_and_rhs\n54: [DEBUG] leaving CKernel::remove_lhs_and_rhs\n54: [DEBUG] leaving CKernel::init(0x808eb6180, 0x808eb6180)\n54: [DEBUG] returning kernel matrix of size 5x5\n54: [DEBUG] leaving!\n54: [DEBUG] entering!\n54: [DEBUG] returning kernel matrix of size 5x5\n54: [DEBUG] leaving!\n54: [DEBUG] Number of samples 5!\n54: [DEBUG] leaving!\n54: [DEBUG] statistic = 0.070779!\n54: [DEBUG] Leaving!\n54: [DEBUG] Entering!\n54: [DEBUG] Entering!\n54: dims=[0,1,2,3,4,6,7]\n54: [DEBUG] Leaving!\n54: [DEBUG] Entering!\n54: [DEBUG] Leaving!\n54: [DEBUG] entering!\n54: [DEBUG] entering!\n54: [DEBUG] entering CKernel::remove_lhs_and_rhs\n54: [DEBUG] leaving CKernel::remove_lhs_and_rhs\n54: [DEBUG] entering CKernel::remove_lhs_and_rhs\n54: [DEBUG] leaving CKernel::remove_lhs_and_rhs\n54: [DEBUG] leaving CKernel::init(0x808eb6240, 0x808eb6240)\n54: [DEBUG] returning kernel matrix of size 5x5\n54: [DEBUG] leaving!\n54: [DEBUG] entering!\n54: [DEBUG] returning kernel matrix of size 5x5\n54: [DEBUG] leaving!\n54: [DEBUG] Number of samples 5!\n54: [DEBUG] leaving!\n54: [DEBUG] statistic = 0.070779!\n54: [DEBUG] Leaving!\n54: [DEBUG] Entering!\n54: [DEBUG] Entering!\n54: dims=[0,1,2,3,4,5,7]\n54: [DEBUG] Leaving!\n54: [DEBUG] Entering!\n54: [DEBUG] Leaving!\n54: [DEBUG] entering!\n54: [DEBUG] entering!\n54: [DEBUG] entering CKernel::remove_lhs_and_rhs\n54: [DEBUG] leaving CKernel::remove_lhs_and_rhs\n54: [DEBUG] entering CKernel::remove_lhs_and_rhs\n54: [DEBUG] leaving CKernel::remove_lhs_and_rhs\n54: [DEBUG] leaving CKernel::init(0x808eb6180, 0x808eb6180)\n54: [DEBUG] returning kernel matrix of size 5x5\n54: [DEBUG] leaving!\n54: [DEBUG] entering!\n54: [DEBUG] returning kernel matrix of size 5x5\n54: [DEBUG] leaving!\n54: [DEBUG] Number of samples 5!\n54: [DEBUG] leaving!\n54: [DEBUG] statistic = 0.070779!\n54: [DEBUG] Leaving!\n54: [DEBUG] Entering!\n54: [DEBUG] Entering!\n54: dims=[0,1,2,3,4,5,6]\n54: [DEBUG] Leaving!\n54: [DEBUG] Entering!\n54: [DEBUG] Leaving!\n54: [DEBUG] entering!\n54: [DEBUG] entering!\n54: [DEBUG] entering CKernel::remove_lhs_and_rhs\n54: [DEBUG] leaving CKernel::remove_lhs_and_rhs\n54: [DEBUG] entering CKernel::remove_lhs_and_rhs\n54: [DEBUG] leaving CKernel::remove_lhs_and_rhs\n54: [DEBUG] leaving CKernel::init(0x808eb6240, 0x808eb6240)\n54: [DEBUG] returning kernel matrix of size 5x5\n54: [DEBUG] leaving!\n54: [DEBUG] entering!\n54: [DEBUG] returning kernel matrix of size 5x5\n54: [DEBUG] leaving!\n54: [DEBUG] Number of samples 5!\n54: [DEBUG] leaving!\n54: [DEBUG] statistic = 0.070779!\n54: [DEBUG] Leaving!\n54: measures=[0.0707790147959238974,0.0707790147959239807,0.0707790147959240085,0.0707790147959238281,0.0707790147959238697,0.0707790147959241611,0.0707790147959241056,0.0707790147959237448]\n54: argsorted=[7,3,4,0,1,2,6,5]\n54: [DEBUG] Entering!\n54: selected feats=[1,2,5,6]\n54: [DEBUG] Entering!\n54: [DEBUG] Leaving!\n54: [DEBUG] Leaving!\n54: [DEBUG] Current number of features 4!\n54: [DEBUG] Leaving!\n54: matrix=[\n54: [   0.0500000000000000028,  0.25,   0.450000000000000011,   0.650000000000000022,   0.849999999999999978],\n54: [   0.0749999999999999972,  0.275000000000000022,   0.474999999999999978,   0.675000000000000044,   0.875],\n54: [   0.149999999999999994,   0.349999999999999978,   0.550000000000000044,   0.75,   0.949999999999999956],\n54: [   0.174999999999999989,   0.375,  0.574999999999999956,   0.775000000000000022,   0.974999999999999978]\n54: ]\n54: /home/wiking/shogun/tests/unit/preprocessor/BAHSIC_unittest.cc:95: Failure\n54: The difference between data(inds[i], j) and selected_data(i, j) is 0.024999999999999994, which exceeds 1E-15, where\n54: data(inds[i], j) evaluates to 0.074999999999999997,\n54: selected_data(i, j) evaluates to 0.050000000000000003, and\n54: 1E-15 evaluates to 1.0000000000000001e-15.\n54: /home/wiking/shogun/tests/unit/preprocessor/BAHSIC_unittest.cc:95: Failure\n54: The difference between data(inds[i], j) and selected_data(i, j) is 0.025000000000000022, which exceeds 1E-15, where\n54: data(inds[i], j) evaluates to 0.27500000000000002,\n54: selected_data(i, j) evaluates to 0.25, and\n54: 1E-15 evaluates to 1.0000000000000001e-15.\n54: /home/wiking/shogun/tests/unit/preprocessor/BAHSIC_unittest.cc:95: Failure\n54: The difference between data(inds[i], j) and selected_data(i, j) is 0.024999999999999967, which exceeds 1E-15, where\n54: data(inds[i], j) evaluates to 0.47499999999999998,\n54: selected_data(i, j) evaluates to 0.45000000000000001, and\n54: 1E-15 evaluates to 1.0000000000000001e-15.\n54: /home/wiking/shogun/tests/unit/preprocessor/BAHSIC_unittest.cc:95: Failure\n54: The difference between data(inds[i], j) and selected_data(i, j) is 0.025000000000000022, which exceeds 1E-15, where\n54: data(inds[i], j) evaluates to 0.67500000000000004,\n54: selected_data(i, j) evaluates to 0.65000000000000002, and\n54: 1E-15 evaluates to 1.0000000000000001e-15.\n54: /home/wiking/shogun/tests/unit/preprocessor/BAHSIC_unittest.cc:95: Failure\n54: The difference between data(inds[i], j) and selected_data(i, j) is 0.025000000000000022, which exceeds 1E-15, where\n54: data(inds[i], j) evaluates to 0.875,\n54: selected_data(i, j) evaluates to 0.84999999999999998, and\n54: 1E-15 evaluates to 1.0000000000000001e-15.\n54: /home/wiking/shogun/tests/unit/preprocessor/BAHSIC_unittest.cc:95: Failure\n54: The difference between data(inds[i], j) and selected_data(i, j) is 0.074999999999999997, which exceeds 1E-15, where\n54: data(inds[i], j) evaluates to 0.14999999999999999,\n54: selected_data(i, j) evaluates to 0.074999999999999997, and\n54: 1E-15 evaluates to 1.0000000000000001e-15.\n54: /home/wiking/shogun/tests/unit/preprocessor/BAHSIC_unittest.cc:95: Failure\n54: The difference between data(inds[i], j) and selected_data(i, j) is 0.074999999999999956, which exceeds 1E-15, where\n54: data(inds[i], j) evaluates to 0.34999999999999998,\n54: selected_data(i, j) evaluates to 0.27500000000000002, and\n54: 1E-15 evaluates to 1.0000000000000001e-15.\n54: /home/wiking/shogun/tests/unit/preprocessor/BAHSIC_unittest.cc:95: Failure\n54: The difference between data(inds[i], j) and selected_data(i, j) is 0.075000000000000067, which exceeds 1E-15, where\n54: data(inds[i], j) evaluates to 0.55000000000000004,\n54: selected_data(i, j) evaluates to 0.47499999999999998, and\n54: 1E-15 evaluates to 1.0000000000000001e-15.\n54: /home/wiking/shogun/tests/unit/preprocessor/BAHSIC_unittest.cc:95: Failure\n54: The difference between data(inds[i], j) and selected_data(i, j) is 0.074999999999999956, which exceeds 1E-15, where\n54: data(inds[i], j) evaluates to 0.75,\n54: selected_data(i, j) evaluates to 0.67500000000000004, and\n54: 1E-15 evaluates to 1.0000000000000001e-15.\n54: /home/wiking/shogun/tests/unit/preprocessor/BAHSIC_unittest.cc:95: Failure\n54: The difference between data(inds[i], j) and selected_data(i, j) is 0.074999999999999956, which exceeds 1E-15, where\n54: data(inds[i], j) evaluates to 0.94999999999999996,\n54: selected_data(i, j) evaluates to 0.875, and\n54: 1E-15 evaluates to 1.0000000000000001e-15.\n54: /home/wiking/shogun/tests/unit/preprocessor/BAHSIC_unittest.cc:95: Failure\n54: The difference between data(inds[i], j) and selected_data(i, j) is 0.024999999999999994, which exceeds 1E-15, where\n54: data(inds[i], j) evaluates to 0.17499999999999999,\n54: selected_data(i, j) evaluates to 0.14999999999999999, and\n54: 1E-15 evaluates to 1.0000000000000001e-15.\n54: /home/wiking/shogun/tests/unit/preprocessor/BAHSIC_unittest.cc:95: Failure\n54: The difference between data(inds[i], j) and selected_data(i, j) is 0.025000000000000022, which exceeds 1E-15, where\n54: data(inds[i], j) evaluates to 0.375,\n54: selected_data(i, j) evaluates to 0.34999999999999998, and\n54: 1E-15 evaluates to 1.0000000000000001e-15.\n54: /home/wiking/shogun/tests/unit/preprocessor/BAHSIC_unittest.cc:95: Failure\n54: The difference between data(inds[i], j) and selected_data(i, j) is 0.024999999999999911, which exceeds 1E-15, where\n54: data(inds[i], j) evaluates to 0.57499999999999996,\n54: selected_data(i, j) evaluates to 0.55000000000000004, and\n54: 1E-15 evaluates to 1.0000000000000001e-15.\n54: /home/wiking/shogun/tests/unit/preprocessor/BAHSIC_unittest.cc:95: Failure\n54: The difference between data(inds[i], j) and selected_data(i, j) is 0.025000000000000022, which exceeds 1E-15, where\n54: data(inds[i], j) evaluates to 0.77500000000000002,\n54: selected_data(i, j) evaluates to 0.75, and\n54: 1E-15 evaluates to 1.0000000000000001e-15.\n54: /home/wiking/shogun/tests/unit/preprocessor/BAHSIC_unittest.cc:95: Failure\n54: The difference between data(inds[i], j) and selected_data(i, j) is 0.025000000000000022, which exceeds 1E-15, where\n54: data(inds[i], j) evaluates to 0.97499999999999998,\n54: selected_data(i, j) evaluates to 0.94999999999999996, and\n54: 1E-15 evaluates to 1.0000000000000001e-15.\n54: /home/wiking/shogun/tests/unit/preprocessor/BAHSIC_unittest.cc:95: Failure\n54: The difference between data(inds[i], j) and selected_data(i, j) is 0.025000000000000022, which exceeds 1E-15, where\n54: data(inds[i], j) evaluates to 0.20000000000000001,\n54: selected_data(i, j) evaluates to 0.17499999999999999, and\n54: 1E-15 evaluates to 1.0000000000000001e-15.\n54: /home/wiking/shogun/tests/unit/preprocessor/BAHSIC_unittest.cc:95: Failure\n54: The difference between data(inds[i], j) and selected_data(i, j) is 0.025000000000000022, which exceeds 1E-15, where\n54: data(inds[i], j) evaluates to 0.40000000000000002,\n54: selected_data(i, j) evaluates to 0.375, and\n54: 1E-15 evaluates to 1.0000000000000001e-15.\n54: /home/wiking/shogun/tests/unit/preprocessor/BAHSIC_unittest.cc:95: Failure\n54: The difference between data(inds[i], j) and selected_data(i, j) is 0.025000000000000022, which exceeds 1E-15, where\n54: data(inds[i], j) evaluates to 0.59999999999999998,\n54: selected_data(i, j) evaluates to 0.57499999999999996, and\n54: 1E-15 evaluates to 1.0000000000000001e-15.\n54: /home/wiking/shogun/tests/unit/preprocessor/BAHSIC_unittest.cc:95: Failure\n54: The difference between data(inds[i], j) and selected_data(i, j) is 0.025000000000000022, which exceeds 1E-15, where\n54: data(inds[i], j) evaluates to 0.80000000000000004,\n54: selected_data(i, j) evaluates to 0.77500000000000002, and\n54: 1E-15 evaluates to 1.0000000000000001e-15.\n54: /home/wiking/shogun/tests/unit/preprocessor/BAHSIC_unittest.cc:95: Failure\n54: The difference between data(inds[i], j) and selected_data(i, j) is 0.025000000000000022, which exceeds 1E-15, where\n54: data(inds[i], j) evaluates to 1,\n54: selected_data(i, j) evaluates to 0.97499999999999998, and\n54: 1E-15 evaluates to 1.0000000000000001e-15.\n54: [DEBUG] entering CKernel::remove_lhs_and_rhs\n54: [DEBUG] leaving CKernel::remove_lhs_and_rhs\n54: [DEBUG] entering CKernel::remove_lhs_and_rhs\n54: [DEBUG] leaving CKernel::remove_lhs_and_rhs\n54: [INFO] Kernel deleted (0x808eb1280).\n54: [DEBUG] Entering\n54: [DEBUG] Entering\n54: [DEBUG] Leaving\n54: [DEBUG] entering CKernel::remove_lhs_and_rhs\n54: [DEBUG] leaving CKernel::remove_lhs_and_rhs\n54: [DEBUG] Leaving\n54: [DEBUG] entering CKernel::remove_lhs_and_rhs\n54: [DEBUG] leaving CKernel::remove_lhs_and_rhs\n54: [INFO] Kernel deleted (0x808eb1640).\n54: [  FAILED  ] BAHSIC.apply (2 ms)\n54: [----------] 1 test from BAHSIC (2 ms total)\n54:\n54: [----------] Global test environment tear-down\n54: [==========] 1 test from 1 test case ran. (2 ms total)\n54: [  PASSED  ] 0 tests.\n54: [  FAILED  ] 1 test, listed below:\n54: [  FAILED  ] BAHSIC.apply\n54:\n54:  1 FAILED TEST\n1/1 Test #54: unit-BAHSIC ......................***Failed    0.04 sec\n0% tests passed, 1 tests failed out of 1\nTotal Test time (real) =   0.08 sec\nThe following tests FAILED:\n     54 - unit-BAHSIC (Failed)\nErrors while running CTest\n``\n. @lambday update? as this has been 3 days ago....\n. @tpokorra thanks a lot for digging into this! i'll try to see what i can do about it and get back to you asap\n. @kislayabhi dunno. installclangon your local machine and reproduce the same error and usegdborlldbto trace the stack.\n. this broke static interface\nhttp://buildbot.shogun-toolbox.org/builders/deb2%20-%20static_interfaces/builds/2257/steps/test%20cmdline/logs/stdio\n. you broke it you fix it :)\n. @karlnapf noup, unfortunately we are having that flag since march apparently and we are running into troubles still on travis\n. @karlnapf noup, unfortunately we are having that flag since march apparently and we are running into troubles still on travis\n. @lambday as always i'll point my finger on gstreamer\n. @lambday noup, as it's actually a media library, but they are thoroughly tested as it's being shiped with a lot of set-top boxes, browsers etc. and their plugin framework is just awesome.\n. @lambday no not at all, it's just a good reference point how we should do things.\nas that one has a very good pipelining method that basically makes sure that the pipeline you've defined, i.e. load_feature -> preprocess -> train -> test -> evaluate -> save\na) makes sense at all\nb) it automagically loads all the requried dlls\n. btw the problem with travis is actually with the swig interface code compilation itself, e.g.modshogun_PYTHON.cxx. we should try again to modularize the swig interface, as that file takes more than 4gigs of ram to compile.\n. @iglesias i suppose it should be among the commits, but i've just heard from @sonney2k that once it was done but that it was very unstable that's why it was dropped on the end. :S\n. fixed\n. fixed\n. @khalednasr any idea about this error?\n. @lambday @khalednasr could you tell me what's the version of viennacl that you are using?\n. as well as the fullcmakeparameters you are using when you want to compile thefeature/linalgbranch?\n. this is the bot's cmake linecmake -DENABLE_TESTING=ON -DLinAlgBackend=VIENNACL ..`\n. @lambday it's the same actually what u've written earlier...\n. @lambday @khalednasr i guess it might be the bug in viennacl with \nopencl-1.2-base                 4.4.0.117-2                   amd64        OpenCL* installable client driver loader\nopencl-1.2-intel-cpu            4.4.0.117-2                   amd64        OpenCL* runtime for Intel\u00ae CPU device\nwhat opencl driver are you guys using for testing the viennacl backend?\n. @khalednasr i've build the examples as well, they ran without any problems...\n. @khalednasr or you mean the actual viennacl library examples, not the ones in shogun?\n. @khalednasr ok i've managed to get the branch working by switching to amd's opencl icd driver instead of inte's\n. i'm talking about for example the sum_symmetric_block function eigen implementation part\n. i've just merged the feature/linalg branch into develop\n@lambday just one more question in case of having both viennacl and eigen available but not setting explicitly the linalg backend, what's going to be the default linalg backend, eigen or viennacl.\nsee the logs here:\nhttp://buildbot.shogun-toolbox.org/builders/trusty%20-%20libshogun/builds/6/steps/configure/logs/stdio\n. good, then the buildbots are just acting fine.\nlet's close this issue then :)\n. it's already in\n. @kislayabhi then i wonder why there's no requirement in the CMakeLists.txt for opencv version? after all you are the one who actually wrote that patch. please try to find out which is the earliest opencv version you would need and update the CMakeLists.txt according to that and send in a PR for it\n. @kislayabhi no we obviously cannot build opencv from source on travis.... but this is a completely different issue!\ncurrently the problem is that we have a notebook that according to cmake script we can work with any version of opencv, whereas this is not true at all. and this is totally unrelated what version ubuntu supports or not. you've committed and @pickle27 merged a code into shogun that is broken (see the notebook) due to the fact that you don't check the version of opencv.\nagain, find out the minimum version you would need and either add this version requirement into the CMakeLists.txt or, since i believe that actually for the integration you could support any 2.x opencv and the actual version problem is with the notebook, do a proper opencv version checking in the notebook itself. either way i cannot stress it how important it is that this is fixed ASAP\n. i feel like you are stating tautologies. i understand why you need it. i'm just saying that now if one builds shogun with eigen (note header only library!) if we build an app based on shogun it'll require eigen as well.... no matter if the class itself is not dependent on eigen as u guys just made both sgvector and sgmatrix fully dependent on eigen. this is just wrong. this needs to be fixed one way or another. if we cannot come up with a sane solution asap or.... so let's try to come up with a good solution instead of just stating the obvious.\nOn 09/08/2014, at 14:44, khalednasr notifications@github.com wrote:\n\n@vigsterkr In SGMatrix.h the function template  SGMatrix(Eigen::PlainObjectBase& mat) needs to be defined in the header file, since it's not possible to list all the possible values for Derived. Same goes for GPUMatrix.h, SGVector.h and GPUVector.h.\n\u2014\nReply to this email directly or view it on GitHub.\n. @khalednasr regarding linalg's header: it's not a problem till those headers are not being exposed in the 'public' headers of shogun.\n. @lambday the problem will be that once you compile shogun with eigen, you won't be able to compile any application which uses shogun (let's say LibLinear) without explicitly supplying the include path for eigen as well.. it'll just fail if it cannot find eigen headers, since you've put eigen headers into SGVector/SGMatrix headers... see what i mean?\n. @yorkerlin it wasn't actually a suggestion, it was a humble way to say that do not do this anymore.\n. @yorkerlin no... they do not include eigen3.h because i've fixed it in commit 4b47860 as i've mentioned earlier. no you should NOT remove #ifdef HAVE_EIGEN3 statements...\ni suppose you've misunderstood what's the problem here: you've included eigen3.h in your headers (which i've mentioned and i've fixed most of them) but without any reason. having eigen3.h included in the header was not necessary at all, since you were not using any of it's definitions in the header file per se, but only in the corresponding .cpp file. in other words, you should really think twice what headers you include in the headers and the implementation. only include things that are really necessary...\n. @karlnapf i've just did a git grep eigen3.h and the only headers that include eigen are headers of src/shogun/mathematics/linalg which are not part of the public interface.... am i right @lambday ?\n. @lambday thnx closing...\n. @abinashpanda note you could just use the referencing in the commit message itself, this way you could save yourself writing comments here like above.\n. @iglesias noup. travis fails mostly nowadays because of insufficient memory, i.e. the modular interface of shogun has grown too big....\n. @iglesias can we merge that PR?\n. aweeeeesome! let's see the reaction of travis\n. ok let me see if we get swig 2.0.5 or later from a ppa ;)\n. @khalednasr for me it looks good. \ni just had a silly idea now: maybe we could do a trick for eigen, that we actually generate a header file that basically creates the typedefs. this way if you had eigen backend then you will have the typedefs in the headers available. it's basically the same thing what you've done, it's just more automated?\n. it's too fucking long. even if i managed now to run them parallel.\n. yeah we could do that.\nthere are couple of notorious notebooks that takes more than 30 mins to render. LMNN (dunno how now the new one works) took more than an hour to be rendered.... but mmd notebook is taking long as well. logdet is the same.\n. @karlnapf @iglesias just sent in a fix for it today\n. @iglesias the notebooks that are successfully generated are uploaded. so that should be alright if it runs w/o error\n. @karlnapf this is more complicated than that. it's as if you want somebody to have shogun fixed on debian sid although it's broken because some package is currently being broken on sid. FCRH is a rolling unstable constantly upgraded distro. it cannot be assured that builds are always going to be successful\n. this is due to #2460 \n. obsolete. this is due to #2460 \n. as well as opencv notebooks.\n. lol yes it is in apt, just not the version you would like to have.\nwe already had this discussion... you need an opencv that's not available for debian wheezy in a prepacked format, and i'm not willing to install opencv from source... ring a bell? :)\n. this is a really fucked up error. basically the architecture is too new for the old gcc in precise, since we are using -march=native compiler flag.\n. this won't be easy... been trying to see a good fix for it... dunno yet how we can do it.\n. fixed\n. eh...?\nsorry but this doesn't make to much sense.... for independent job we should start using mesos\n. yeah i'd be great if people would start using [ci skip] tag in their commit descriptions if it's obviously not affecting the src of libshogun or the test, i.e. a change in a notebook will not have any effect on travis builds.\n. @khalednas it's not viennacl related for sure. as viennacl doesn't have template definition for unsigned long long or long long, see for example viennacl::ocl::type_to_string. i'm just wondering why this does not cause an error in our linux buildbot :S\n. ok this is actually fixed a longer time ago :)\n. we should resurrect this bug as starting with 8fdf278dca78f98d4e06058a5ce3defd560d697d we natively support MS Windows build.\ni reckon we should create a package and publish it on (nuget)[https://www.nuget.org] using appveyor. done. mmm this has been encountered just now by somebody else... :S looking into it, but interestingly enough travis-ci builds are done with ubuntu 14.04 docker images and have not encountered this before there :(\n. i reckon in case this is because the ipython on the builder server was a bit outdated. i've just updated it and now regenerating the notebooks. let's see...\nin case of doxygen generated documentation, i'm really surprise what mathjax does since the script url is this:\n\nhttp://www.mathjax.org/mathjax/MathJax.js\nand the webserver returns a 403 :( \nalthough according to mathjax help one should use the following cdn:\n<script type=\"text/javascript\"\n  src=\"http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML\">\n</script>\ni'll check if there's and option in doxygen to change the mathjax script url \n. ok it was error in the Doxygen_*.in files... fixed in cc1b966\n. great, but have you seen this maybe:\nhttp://cloud.shogun-toolbox.org/\ncurrently i'm working on generating debian and redhat packages nightly so that we can generate nightly docker images as well. as pulling a git repo into a docker image and build there the shogun from the source is really not how docker was meant to work with. but of course i guess you've seen this Dockerfile before:\nhttps://github.com/shogun-toolbox/shogun/blob/develop/configs/Dockerfile\nif you wanna help out in building an official always up to date docker image let me know, there are several tasks pending for this to finally happen.\n. a quick update: daily built shogun packages for trusty are available here:\nhttps://launchpad.net/~shogun-daily/+archive/ubuntu/ppa\nso basically once i have the python-shogun package as well generated daily, the generation of a latest docker image will be much easier. in other words it'll be available here:\nhttps://registry.hub.docker.com/u/shogun/shogun/\nstay tuned. in the meanwhile if you are having troubles with the daily packages let me know!\n. a quick update: daily built shogun packages for trusty are available here:\nhttps://launchpad.net/~shogun-daily/+archive/ubuntu/ppa\nso basically once i have the python-shogun package as well generated daily, the generation of a latest docker image will be much easier. in other words it'll be available here:\nhttps://registry.hub.docker.com/u/shogun/shogun/\nstay tuned. in the meanwhile if you are having troubles with the daily packages let me know!\n. @abhinavagarwalla i'm sorry but where did you get the first 2 steps? that shouldn't be there at all.\nplease refer to the manual:\nhttps://github.com/shogun-toolbox/shogun/wiki/QUICKSTART#compile-and-install-shogun-toolbox-into-home-directory\n. @abhinavagarwalla i'm sorry but where did you get the first 2 steps? that shouldn't be there at all.\nplease refer to the manual:\nhttps://github.com/shogun-toolbox/shogun/wiki/QUICKSTART#compile-and-install-shogun-toolbox-into-home-directory\n. partly it's done... but i believe we should just drop sooner or later the whole CMath, as now everything that's there is available in std:: since c++11. the location of is not set correctly usps_resampled.mat:\n```\n/usr/lib/python2.7/dist-packages/scipy/io/matlab/mio.pyc in _open_file(file_like, appendmat)\n     76             raise IOError(\"%s not found on the path.\"\n     77                           % file_like)\n---> 78         return open(full_name, 'rb')\n     79     # not a string - maybe file-like object\n     80     try:\nIOError: [Errno 2] No such file or directory: './usps_resampled.mat'\n```\n. this hasn't been touched for a long time... closing. a) use pastebin.com if you are thinking about pasting more than 20 lines\nb) this is not by far any proper issue description... i don't even understand what is this about, what's the problem... please read this: https://issues.apache.org/bugwritinghelp.html\n. Thats for sure that u have not used this conmand and got that missing lua error...\n\nOn Nov 15, 2014, at 6:26 PM, yalcinm notifications@github.com wrote:\n@sorig,\n/home/user/shogun/build\ncmake -DPythonModular=ON -DCMAKE_INSTALL_PREFIX=\"$HOME/shogun-install\" ..\nI also tried;\ncmake -DPythonModular=ON\n\u2014\nReply to this email directly or view it on GitHub.\n. Its impossible to have that lua error withthe cmake options you provided. I know because i wrote the that part if the cmake file.... \nOn Nov 15, 2014, at 6:51 PM, yalcinm notifications@github.com wrote:\n@vigsterkr,\nDid you even check the error log? Honestly?\nCheck this screenshot:\nhttp://imgur.com/c950ALm\n\u2014\nReply to this email directly or view it on GitHub.\n. Fyi for sure you have been using n combination of cmake arguments and now LuaModular is turned on. You will see it with ccmake ..\n\ndo an rm -rf * in build dir and re-run that cmake command with PythonModular....\n\nOn Nov 15, 2014, at 6:51 PM, yalcinm notifications@github.com wrote:\n@vigsterkr,\nDid you even check the error log? Honestly?\nCheck this screenshot:\nhttp://imgur.com/c950ALm\n\u2014\nReply to this email directly or view it on GitHub.\n. @yalcinm before sending in your next question please take some time an read this article:\nhttp://ben.balter.com/2014/11/06/rules-of-communicating-at-github/\n. unfortunately this is a new feature.\nyes swig3 creates some segfault when generating the python modular interface, but the compiler in yosemite has some new things going on :S\n. as i said in my previous post that error was not due to swig but rather compiler problem.... you could try using g++ (never tried but i reckon it'll work). but still remain with swig2.x as swig 3.x generated python interface needs fixing....\n. fyi read this http://ben.balter.com/2014/11/06/rules-of-communicating-at-github/\n. obsolete.... @thirdwing thnx but we don't want to have shogun depending on a bloated library like boost\n. some? :)\n. fixed\n. @vrishank97 sure. oh, then this is obsolete! :). :two_hearts: \n. couple of problems:\n- why all those indentation changes... please dont do that, makes bisecting incredibly hard.\n- dont change file permission, i.e. 644 -> 755\n\nplease fix those before further reviewing.\n. i would go for it! :)\n. tbis is definitely fixed. first things first... let's start with adding support of libarchive and then clean up the mess with not being able to parse more than 15k lines of csv :hurtrealbad: \n. @hrw hi! any ideas how i could quickly debug it? shall i use qemu or do you have maybe better suggestion?\n. @hrw i had once access to linaro, but it took a long time to get access :) so qemu it is :P\n. @hrw ok so finally i've managed to put together a qemu env for this ! \\o/ so yeah i can confirm this.... it puzzles me a bit why those specific errors, but looking into it :)\n. @hrw btw what are the cmake/compiler flags you are using to compile shogun for aarch64 as i've just seen that Release had -mfpmath=sse which is a no-go for anything none x86 :P\n. yeah no... ;) the problem is that we need a lot of extra packages that we cannot install w/o sudo....\nand travis does not support custom docker image... so till either of them is fixed we cannot switch to that...\n. lalala :) i haven't seen that thingy before... i'll push a pr to see how that works out.\n. @erip that's already solved in the feature branch... the problem currently is that there are some outdated packages in that container based lala and i would need to add other ppa-s that are not in the whitelist.... i requested for adding them to the whitelist but it'll take a while till it gets in ... until then this endeavor of ours is on hold...\n. @erip that's already solved in the feature branch... the problem currently is that there are some outdated packages in that container based lala and i would need to add other ppa-s that are not in the whitelist.... i requested for adding them to the whitelist but it'll take a while till it gets in ... until then this endeavor of ours is on hold...\n. @erip as u can see in that build, the coveralls build just runs fine... pip installed and all....\n. @erip as u can see in that build, the coveralls build just runs fine... pip installed and all....\n. @erip noup, these are still missing:\nhttps://github.com/travis-ci/apt-source-whitelist/issues/53\nhttps://github.com/travis-ci/apt-source-whitelist/issues/121\n. why is it a bug that the hash can return a negative value? i mean the function defintion is int32_t hash(const K& key) which means that the return value is signed, i.e. half of the time it should actually return negative values...\n. on the other hand using that as an index is a pure choice as it's used for indexing...\nbut then again, revisiting the function implementation:\nint32_t hash(const K& key)\n    {\n        return CHash::MurmurHash3((uint8_t*)(&key), sizeof(key), 0xDEADBEEF) % hash_size;\n    }\nbecause of modulo hash_size the return value should never be less than 0\n. @yorkerlin no you don't need to: sizeof(int32_t) = sizeof(unit32_t) except that the MSB is in one case part of the number while the other case it's the sign value...\n. and please note the modulo hash_size... i  mean by default the hash is a bucket of 41 different values...\n. use conda-forge package for windows:\nconda config --add channels conda-forge\nconda install shogun. dude, this is duplication of #2900 and currently the ball is at the travis people, i.e. this effort is being blocked by them...\n. dont care about your excuses! :P :dancer: (joking)\nmmmm not really, i've pushed all the requirements to their repos, but the PRs still just hanging there..\n. kill it! :)\n. come to irc\n. no bueno! we should fix the typemaps in the swig definition of csharp\n. why after and not before? :)\n. well let's at least give it a go before just signing off of not doing it :)\n. Sooooo\nmerry xmas (both roman catholic and orthodox) and new year!\ncan i help with this in any ways?\nviktor\n\nOn Dec 22, 2015, at 12:57 PM, Heiko Strathmann notifications@github.com wrote:\nAs we have now enough \"critical mass\" to do the license change, here is the list of steps TODO\n\u2022 create a blacklist of files that cannot be changed to BSD (integrated libs as SVMLight, alglib code in CStatistics, ...)\n  \u2022 Add a new LICENSE file with the BSD we use\n  \u2022 Remove GPL license header from all files\n  \u2022 Replace with a reference to LICENSE file and put all authors from git history in\n  \u2022 Discuss blacklist with team\n  \u2022 Once everyone is happy, change the main license and announce\n@lisitsyn I'll assign you as discussed\n\u2014\nReply to this email directly or view it on GitHub.\n. @kno10 yeah indeed when i started some benchmarking of shogun, i've started with k-means and currently i do agree that's it's in a rather horrible state. :(\n. @shark-S shogun is a c++ library that has a python interface (as well). if you want to work on shogun you should get familiar with c++\n. @shark-S you cab test in pyhon but write it in only in c++\n. @shark-S you cab test in pyhon but write it in only in c++\n. @youssef-emad great work! we would really like to have a more maintainable and reproducable way of doing benchmarking, so it'd be great if you could port this idea to this framework:\nhttps://github.com/zoq/benchmarks\n\nas this in this current form is just a snapshot of each library... and it'll be hard to re-run this in newer releases... as well as you'll see that there are many other libraries we should include in the comparison\n. @youssef-emad great work! we would really like to have a more maintainable and reproducable way of doing benchmarking, so it'd be great if you could port this idea to this framework:\nhttps://github.com/zoq/benchmarks\nas this in this current form is just a snapshot of each library... and it'll be hard to re-run this in newer releases... as well as you'll see that there are many other libraries we should include in the comparison\n. @Eejya there's this https://hub.docker.com/r/shogun/shogun-dev/~/dockerfile/\nfirst we need a way to nicely build .deb images...\nbut if you create the docker images derived of fedora, than there the packages are nicely updated thnx to @besser82 . we could ask him how to create shogun nightly packages for FH.\n. @Eejya there's this https://hub.docker.com/r/shogun/shogun-dev/~/dockerfile/\nfirst we need a way to nicely build .deb images...\nbut if you create the docker images derived of fedora, than there the packages are nicely updated thnx to @besser82 . we could ask him how to create shogun nightly packages for FH.\n. @ellesec can you tell us more about which version from the source, and which version of R and where is it located on your system?\n. @ellesec can you tell us more about which version from the source, and which version of R and where is it located on your system?\n. @karlnapf remove the eclipse stuff....\n. @karlnapf for future reference: please do not PR a feature branch, it doesn't make any sense and it makes travis work twice.\ni hope you did not intended to merge this branch via PR hitting the merge button on github page.\n. macports based shogun is pretty outdated :(\ntry to use either brew or from src\n. @abhinavmoudgil95 i'm not so sure how you've managed to get this error, as i really dont get how you've managed to get modshogun depend on libJPEG. could you share your cmake command?\nFYI i'm using shogun on OSX without any problems.\n. symlink between libraries? what exactly do you mean by that?\n. symbolic links between files? a symbolic link to a file, maybe... btw thanks for the link about symbolic link.\nbut i have a hard time to understand how that would that be the root cause of getting linking errors for your library\n. still dont understand how did modshogun got the linking dependency for libJPEG.\n. but man do you understand what does it mean to have a linking dependency to libJPEG? and how did you manage to do that?\n. no it has nothing do with that. i suggest you to read more about shared libraries and linking libraries because the things you've mentioned here makes no sense at all.\n. 1. there's liblinear in shogun and it's outdated\n2. it needs updating\n3. it'd be better to be updated by the multicore version\n. 1. there's liblinear in shogun and it's outdated\n2. it needs updating\n3. it'd be better to be updated by the multicore version\n. what are you doing?!\nthsi has been already merge man\n. @arasuarun the neuralnet part of shogun was written when the linalg package was starting to emerge. no wonder that it doesn't use all of the linalg backend.\n. @arasuarun the neuralnet part of shogun was written when the linalg package was starting to emerge. no wonder that it doesn't use all of the linalg backend.\n. why to do things tomorrow when you can do it today? :)\n. @arasuarun noup openmp is not for gpu: https://en.wikipedia.org/wiki/OpenMP\ngpu is via linalg + viennacl\n. this is a vanilla implementation. please consider using our linalg library and openmp at the least.\n. 3.1.0\n. yes, to which version?\n. so we want 4.x?\n. just upgraded to ipython (4.1.2), but we dont have buildbot so wont be able to test notebook related things until we dont resurrect buildbot\n. thanks a lot for the analysis!\ni find it a bit weird/interesting & worth to investigate why does does SVD so slow.\nespecially these two examples are worth to look into:\n| D | N | Shogun | scikit-learn |\n| :-: | :-: | :-: | :-: |\n| 500 | 100 | 0.293 | 0.077 |\n| 500 | 1000 | 0.261 | 1.852 |\nas even though there are 10 times less vectors the runtime is almost the same...\n. first of all thanks for the benchmark, but it would be much better to do it as part of:\nhttps://github.com/zoq/benchmarks\nthis framework already has support for PCA benchmarking. this way the results are easily reproducible and as well as it's part of a framework.\n. first of all thanks for the benchmark, but it would be much better to do it as part of:\nhttps://github.com/zoq/benchmarks\nthis framework already has support for PCA benchmarking. this way the results are easily reproducible and as well as it's part of a framework.\n. regarding the changes: if you did make install you'll have to do it again...\n. regarding the changes: if you did make install you'll have to do it again...\n. @abhinavmoudgil95 see the referred benchmarking framework.. that's the preferred way.\n. @abhinavmoudgil95 see the referred benchmarking framework.. that's the preferred way.\n. this'll need some love because i've just rebased the sphinx branch...\n. this'll need some love because i've just rebased the sphinx branch...\n. @yorkerlin sounds great! as there's already emerging some benchmarks on PCA, shouldn't we try to put this into one framework? i.e. keep the code somewhere and have the possibility to update the benchmarking results with the updated libraries?\n. @yorkerlin we should use/extend this framework:\nhttps://github.com/zoq/benchmarks\ni can setup a buildbot to generate the output...\nyou can see some of the results here:\nhttp://www.mlpack.org/benchmark.html\nthe shogun that was used was version 3.2.0...\n. @yorkerlin AWESOME++\nwe should convert this to a http://github.com/zoq/benchmarks/ task, that way we could re-test this anytime we want :)\n. @abhinavmoudgil95 plz dont do this, you can do this change post PCA in the benchmarking code (see the framework i've mentioned). all shogun class follows the (num_dim, num_vectors), so this change would make thing inconsistent within shoung\n. @abhinavmoudgil95 plz dont do this, you can do this change post PCA in the benchmarking code (see the framework i've mentioned). all shogun class follows the (num_dim, num_vectors), so this change would make thing inconsistent within shoung\n. and please read a bit https://git-scm.com/documentation as that merge remote-tracking branch... commit is definitely something you dont want to have in a PR.\n. and please read a bit https://git-scm.com/documentation as that merge remote-tracking branch... commit is definitely something you dont want to have in a PR.\n. @karlnapf any reason why not to merge this?\n. i've merged and updated the buildbot params for the viennacl job, i'm getting the following output from cmake\n-- ViennaCL set as default global linear algebra backend library\n-- Eigen3 set as default core module (linalg) backend library\n-- Eigen3 set as default reduction module (linalg) backend library\n-- Eigen3 set as default eigen solver module (linalg) backend library\n@karlnapf why Eigen3 is still the eigen solver module?\n. @sanuj there's something else with this because i've re-ran the gcc task and it's still timing out.\n. i told you not to merge this....\n. @curiousguy13 could you please squash these changes into one commit:\nhttps://git-scm.com/book/en/v2/Git-Tools-Rewriting-History\n. when where? :)\n. @OXPHOS thnx for the patch but lets rather fix the unit tests... we really would like to have eigen as a hard dependency.\n. ok so you were having problem when you included eigen headers? could you point me to an error, i.e. copy paste to pastebin or something?\n. where's the problem with EIGEN3? i let's try to fix that instead of adding it back to the macros\n. yea but there should not be any #ifdef HAVE_EIGEN3 anymore anywhere\n. @OXPHOS ok i see what you mean i just did a:\ncd tests/unit\ngit grep HAVE_EIGEN\nall those #ifdef HAVE_EIGEN3 should be out...\n. @karlnapf is this fixed or what else needs to be done?\n. @Xbar what all these unrelated doxygen changes? i mean it's great but it shoudl be part of separate PR, not about fixing LaRank\n. @Xbar what all these unrelated doxygen changes? i mean it's great but it shoudl be part of separate PR, not about fixing LaRank\n. @lambday ok so can i ask a simple question: why do we need a yet another CUDA backend?\n. and btw all these things should be runtime optimized and not user wise optimized... it makes everything much more easier for the user. so that the user asks for GPU type or CPU type is a foobar... this should be done under the hood, taking into consideration the cycles needed for moving around data from CPU to GPU mem...\n. i completely disagree with you @karlnapf about being extremely difficult: magma does this pretty good.\nand what does it mean that CUDA is better than ViennaCL? :)\n. statically compiling in a library options is so fing 90s ;) it's 2016 now, nobody wants to create 10 different compilation just because the lab has 10 different machines in the cluster. you want to have one blob (or blobs if we do shared libs fw one day) and that's what you wanna distribute among the machines in your cluster/lab/toasters \nthis is a reflection on @lambday's -DTURN_OFF_GPU idea\n. @lambday it should be neither.. it should be decided based on simple rules.\n. those conscious/well informed decisions are actually can be compiled into rules.\nthe rules are as good as the one who defines them :) (just to be an ahole here, and i couldn't let this hanging).\nthe turn off gpu should be in worst case an env/config option, but not a compile time option\n. and the reasoning for doing semi-optimal solutions just because of not enough manpower i would assume never ended up in a nice place... and this (shogun) we all do (in our spare time) because we are for some magical reason enthusiastic about it (still)... doing a half/semi-optimal solution that you do because of feelings.... well that is just a broken dream.\n. closing.... @Saurabh7 yeah sounds like a plan. see ELKI's k-means family: http://elki.dbs.ifi.lmu.de/wiki/Algorithms\n. no need. for the record: this is a good tool to call cmake from setup.py: https://github.com/scikit-build/scikit-build\n. this is still broken. let's keep it open... in the meanwhile you can install shogun python interface with conda:\nconda install -c conda-forge shogun. this is actually a regression in Eigen 3.3+: http://eigen.tuxfamily.org/bz/show_bug.cgi?id=1229\n. @karlnapf 3.3+ is dev branch, i.e. not yet stable. 3.2.x (which works and is bundled now) is the stable branch. i would bundle stable version and not development version.\nwe'll have to do similar hacks that i did with viennacl version 1.6 and newer vs earlier version. MACRO hacks to the rescue caramba! :dancers: \n. @karlnapf yes but one day the dev will become stable and there's gonna be a longer period where both 3.2 and 3.3 will be around... so we'll need to have the macro hack...will look into it asap.\n. @arianepaola good, but would be better if we could actually solve this by actually have a solution for 3.3+ :) because now as far as i understand this will just not use eigen 3.3+, or?\n. @karlnapf no temp merge. i hate temp merges :dancer: \n. i mean i really appreciate the effort here! i do! but let's try to fix it so that 3.3+ can play as well. should not be that hard\n. btw i should have written after the :dancer: caramba!\n. CARAMBA! :do_not_litter: \nbe more patient :) i'll add patches here and then let's merge it plz.\n. @sanuj @karlnapf is this for graphical examples?\nwe really want them to be run as tests?\n. i mean it's just a docker image and it will not hurt anybody to have matplotlib in the container :)\n. @karlnapf @yorkerlin hence the suggested gsoc project: KKT framework; where one could address most of these issues\n. @jucor our linalg finally supports it (just merged), but none of the svm's linalg part has been ported yet to be using the new linalg framework.... so not yet.. lets address the comments + rebase with the latest develop and see how travis behaves :)\n. ALL GREEN! good job @Saurabh7 let's merge it!\n. cool stuff @cfjhallgren !\ni only wonder about that virtual other than that we could merge this!\n. ok, great! let's see what travis says and then i'll merge it!\n. i dont see anywhere swig definitions defined for src/shogun/optimization/*.h in shogun/src/interfaces/modular\n. according to git grep machine/gp in shogun/src/interfaces/modular, yes it's defined in GaussianProcess.i and GaussianProcess_includes.i...\nanyways if you wanna have classes defined in src/shogun/optimization available via swig interfaces, please create a shogun/src/interfaces/modular/Optimization.i and shogun/src/interfaces/modular/Optimization_includes.i and follow the pattern that are in the other files in shogun/src/interfaces/modular\n. @karlnapf ok so the problem is that at the time of 4.1.0 release there was a bug in doxygen config, hence it is 404. as you can see in case of latest it's fixed:\nhttp://www.shogun-toolbox.org/doc/en/latest/installation.html\n. @karlnapf should we maybe port back that fix into current master?\n. i'd say let's concentrate on the new release... :)\n. @OXPHOS sg_rand->set_seed(1), but what is actually the problem with CMath::init_random(1)? it's a static function you should be able to call it like that\n. duplicate of #3178\n. @lambday i did comment in the beginning :)\n. @ilovejs which version of shogun are you trying to compile?\n. @ilovejs please clone the git repository and check out the develop branch as that one is confirmed to compile on ubuntu 14.04, see the output of the buildbot:\nhttp://buildbot.shogun-toolbox.org/builders/trusty%20-%20libshogun\nlet me know how it goes, so whether i can close this issue or not. thnx!\n. ppa is available at: https://launchpad.net/~shogun-toolbox/+archive/ubuntu/stable. do we know why c# and java modular tests for all the generated examples fails?\n. @OXPHOS yeah this is better )\n. @OXPHOS in the gist you've created you had a cmake/external/cereal.cmake. could you add that here as well?\n. @OXPHOS cool so now that this works, it's time to squash your commits into 1 big commit :)\nyou can read about how to do this here: https://ariejan.net/2011/07/05/git-squash-your-latests-commits-into-one/\nif you having problems how to do it feel free to ping me.... once you are done we can continue on adding support for cereal serialization :)\n. @yorkerlin i have a fix for this, coming already to develop, so can we close this?\nwhich issue do you refer to?\n. this is interesting.... as the buildbot is working. compiler version?\n. do you see any differences:\nhttp://buildbot.shogun-toolbox.org/builders/xenial%20-%20libshogun/builds/15/steps/configure/logs/stdio\n. this is waaaay tooo weird :) lemme reproduce this in a docker with ninja :)\n. @ljw3351639 could you tell me the specs of your OS?\n. @ljw3351639 i see... still would be good to know what is your setup so i could reproduce and possibly fix this error :) thnx. @karlnapf lot of updates are still pending... :) we'll get there sooner or later \n. @yorkerlin there's already some vw integration within shogun\n. @arianepaola let's get this moving...\n. ok so couple of things:\n ... does this require it to run cmake prior to run setup.py, because if so it's a problem... and why do we think that that's required\nwhen i checkout a clean repo and do python setup.py --version\ni'm getting this:\n```\nshogun wiking$ python setup.py --version\nRetrieving Shogun version\nVerifying Shogun preconditions\nShogun build environment completed tasks: cmake: [False] -  make: [False] - make install: [False]\nBootstrapping Shogun\nRunning CMake\nCMake arguments: -DPythonModular=ON -DENABLE_TESTING=OFF -DCMAKE_INSTALL_PREFIX=install\nCreating build directory: /Users/wiking/backup/shogun/build\n-- The C compiler identification is AppleClang 7.0.2.7000181\n-- The CXX compiler identification is AppleClang 7.0.2.7000181\nCMake Error at /usr/local/Cellar/cmake/3.5.2/share/cmake/Modules/Platform/Darwin.cmake:76 (message):\n  CMAKE_OSX_DEPLOYMENT_TARGET is '10.10' but CMAKE_OSX_SYSROOT:\n\"\"\nis not set to a MacOSX SDK with a recognized version.  Either set\n  CMAKE_OSX_SYSROOT to a valid SDK or set CMAKE_OSX_DEPLOYMENT_TARGET to\n  empty.\nCall Stack (most recent call first):\n  /usr/local/Cellar/cmake/3.5.2/share/cmake/Modules/CMakeSystemSpecificInformation.cmake:36 (include)\n  CMakeLists.txt:27 (project)\n-- Configuring incomplete, errors occurred!\n``\n. @Saurabh7 thnx for the benchmarks... any idea(s) why that diff between SG and SK in case ofN=5000andD=1000? or that's the cost of that two extra iteration?\n. well, it's time to merge! :)\n. useRealMatrix. just closed #3257 because it's a duplicate... let me know if you don't agree\n. http://www.stack.nl/~dimitri/doxygen/manual/docblocks.html\n. @OXPHOS fix the random seed :)\n. you meanmath.init_random(123)? ormath.init_random()?\n. @OXPHOS ok but then that should be investigated why it kept giving different results, as then it's a bug. setting the seed to a fix value should make everything deterministic. can you isolate maybe the code part somehow?\n. @OXPHOS WAIT A FUCKING SECOND.\nso did you trymath.init_random(1)ormath.init_random(0)`.\nin https://github.com/shogun-toolbox/shogun/pull/3280#issuecomment-225344176 you said math.init_random(1) hence my surprise\n. perfect! let's see what travis says... and then merge\n. @Saurabh7 move it to here plz!\n. @Saurabh7 waiting for the test to be added here....\n. @Saurabh7 can you just put this test into the other branch PR, that way travis will test everything at once...\n. one more thing. please make sure that the following errors are not present in the updated code:\n- http://buildbot.shogun-toolbox.org/static_analysis/2016-06-26-005840-32416-1/report-9779d7.html#EndPath\n- http://buildbot.shogun-toolbox.org/static_analysis/2016-06-26-005840-32416-1/report-6ac918.html#EndPath\n- http://buildbot.shogun-toolbox.org/static_analysis/2016-06-26-005840-32416-1/report-2e32eb.html#EndPath\n. @Saurabh7 great work! i'd happily merge this but i prefer to have it as one commit (i.e. could you please squash the commits into one) as well as address those minor comments from @karlnapf \nplz ping me when it's done. thnx!\n. @Saurabh7 great! i'll wait for travis to finish and then i'll merge it! great work!\n. in the shogun src dir:\ngit fetch\ngit rebase upstream/develop\ngit submodule init\ngit submodule update\n. 4a79db623b97434b1e9038cb1d0c0ff2997d6855 is the data commit version in develop so i think that develop just hasn't got updated to use the latest data \n. ping ping ping let's finish and merge!\n. hellooooo who are you? :D let's see \n. @c4goldsw good stuff!\ni've added couple of comments, but there's a major problem: why did you have to move the whole implementation out of the .cpp into the header?\n. @c4goldsw check how it is done in case of DenseFeatures\n. but openmp actually uses pthreads so.....? :)\n. @OXPHOS \ud83d\udc83 getting there! just let's be consistent with HAVE_CXX11 macro and that's all.\n. there is!\n. http://buildbot.shogun-toolbox.org/cookbook_pr/9a0a43e470ea8cc38e849d38c62c5dc3494ec510/\n. click on show all checks\n. for me only default is there = PR buildbot\n. cant we just simply have mean that returns always float64_t?\n. i.e. i dont really get why we really need that enum story.\n. in that case you could use something like:\n```\ntemplate < typename = typename std::enable_if< std::is_same::type >::type >\ncomplex_t mean(T);\ntemplate < T >\nfloat64_t mean(T);\n``\n. you cannot have this:\nhttps://gist.github.com/OXPHOS/8f2d048e71df1dfc98bb7e2f0953c30e#file-linalgnamespace-h-L10\n. @OXPHOS pingu\n. @OXPHOS what happening?\n. and the output diff there was?\n. or rather the euclidean distance between the 2 trained Ws?\n. @karlnapf PRNG... it's ok :)\n. @Saurabh7 seems ecoc and multiclass meta examples are failing. do they have actually some margin of error set? :)\n. @Saurabh7 what's happening with this?\n. @sanujRMudular` just works fine: http://buildbot.shogun-toolbox.org/builders/deb3%20-%20modular_interfaces/builds/2933/steps/test%20r%20modular/logs/stdio\nif the changes are the one that causes this then your changes are unfortunately not acceptable as it's unacceptable to have RModular disabled.\n. @lisitsyn up until it's in a feature branch i dont care... just saying that before merging this anywhere near develop, one should think twice... having RModular lost is a mayor thing.\n. since travis doesn't have a build for this let's merge it, or have you tested with: -DUSE_SVMLIGHT=OFF and creating/compiling the meta examples locally? if not could you please do it and report it back here.\n. that's ok let's see what the bb says....\n. all green! @lisitsyn wanna hit that merge button? \ud83d\udc83 \n. i'm fine with this, but: let's have the change in cmake then that it'll throw an exception if C++11 is not available, see unordered_map etc.... oooor keep the macros around...? @karlnapf \n. ok lets have travis green and mergoooo this bazdmeg! :)\n. @yorkerlin before jumping in alone, could we have like a design session (irc,gdocs, gist) where we can agree on the optimization framework refactoring?\n. my pain points:\n- kkt solver\n- use the linalg backend so we can use various linalg backends.\n- maybe instead of directly addign TMAC (looks great) work on a more general design?\n. @OXPHOS you still missed one of my comments where the cereal header needs to be eliminated:\n```\n/opt/shogun/src/shogun/lib/common.h:32:29: fatal error: cereal/cereal.hpp: No such file or directory\n#include \n```\nnote the rest of my comments that in order to be able to do that you'll have to do some forward declarations.\n. @OXPHOS SOOOOOOOOOOOOOOOOOOOOOOOOOOOOoooooooooooooooOOO? squash & merge?\n. @OXPHOS nono it's fine! just squash it into one commit :)\n. @OXPHOS ok, great! let's wait for travis to be green again (just to be on the very safe side) and then i'll merge! \\o/\n. @OXPHOS btw just as a side-note here, we'll have to do more serialization implementation, see SGMatrix etc. i.e. none SGObject base classes :)\n. @Saurabh7 quite big diff on the same data set (smaller dim) in case of SK... how many time do you run the tests? just to see what's the (mean, std) of runtimes....\n. @Saurabh7 great! can you please squash this so i can merge it?\nand in the next PR send in the falconn based update?\n. @Saurabh7 ok cool as soon as it's green we can merge.\n. @sanuj i've restarted the travis jobs that failed... they failed again....\n. @sanuj maybe a rebase with current develop would help...?\n. @OXPHOS rebase? what's the status here?\n. @Saurabh7 what's the update on this one?\n. forget jdk < 1.5... already 1.7 is EOL last april... @sorig can you do the change as part of this PR?\n. @OXPHOS rebase plz\n. @lisitsyn ping! any insights as i stole your idea? need this because those header includes of external libraries are giving the user a hard time, when trying to use MKL.\n. @c4goldsw thnx heaps for the patch! lets merge it and see what the memchecker does\n. @cfjhallgren lemme know if you need help how to fix the problem that is indicated by travis\n. @OXPHOS please rebase :)\n. @OXPHOS this is complete now, right? if so i'll merge\n. @sanuj i reckon we've agreed that we are not going to do this, right? should i close this one then...?\n. yeah baby grooooovyyyyy!\n. yeeey! \\o/\n. yeeeeey! huge imports FTW.\n@Saurabh7 do you have some performance #?\n. @Saurabh7 i've restarted travis, it fails because of in the eigen_wrapper.h the direct include of Eigen/Dense. plz patch the original code that it uses the shogun eigen wrapper and before you commit run the unit test for knn\n. @Saurabh7 i've just explained you what is causing this in my previous comment. the root cause is that eigen wrapper there... and plz if you are not so sure use the docker image for testing (the same one as the travis: https://hub.docker.com/r/shogun/shogun-dev/)\n. @Saurabh7 squash & go?\n. @OXPHOS mmmm it's just waaaaaaaaaaaaay toooo verbose... template to the rescue?\n. @sudk1896 do you know if there's a linux mint docker image out there somewhere so i could test it for you and see what could be the problem? (tried looking around in hub.docker.com but couldn't find anything)\n. could you please provide the OS and octave version you are using in this case?\n. @shogun-tester-AT any chance you've tested whether this bug occurs in develop branch of the git repo? \n. @entalent could you share the following details:\n- which version of shogun you are using\n- which version is the JDK you are having on your system\n. mmm based on the version you are using the git repo...? is it the develop branch?\nit's just weird because with the very same properties (jdk, ubuntu etc.) the java modular interface builds and tests successfully, see:\nhttps://travis-ci.org/shogun-toolbox/shogun/jobs/152507031 \n. @entalent could you maybe provide the full cmake output please?\n. lets close this and go with #3435 \n. @Saurabh7 FYI i just rebased the original feature/linalg_refactor so you'll have to update your branch as well\n. @Saurabh7 i suppose this is down in the drain as hasn't been touched for 4 months now... . @bhanratt could you please tell us your OS (and version).... and how did you install/get libnlopt?\n. basically the problem is rather with nlopt than shogun.... although i wonder why id doesn't try to link with the shared library of nlopt, i.e. libnlopt.so or libnlopt.dylib\n. @bhanratt ok so first of all can you remove the manually built NLOPT or build it with -fPIC CFLAGS/CXXFLAGS as currently that's your problem that shogun is trying to use that manually built library that has not compiled with -fPIC.\nbtw this is a docker image (ubuntu 14.04) which has all the required/optional dependencies for shogun that we use for travis-ci. if in case you just wanna test shogun i reckon it's a much better way to get started:\nhttps://hub.docker.com/r/shogun/shogun-dev/\n. @bhanratt if you dont mind i'll close this bug as it's really unrelated to shogun. but feel free to open it again if the problem remains after you've fixed the problem with nlopt.\n. oh this is rather interesting! could you please provide the full cmake output as well (when you ran it the first time). thnks\n. wait i thought you ran:\ncmake .. -DPythonModular=On -DPYTHON_INCLUDE_DIR=/usr/include/python2.7 -DPYTHON_LIBRARY=/usr/lib/python2.7/config/libpython2.7.so (see your first message).\nbut anyhow i would need the full output of what you get on your screen after running cmake.\n. yep this is the one!\ni suppose you are using all the default packages of ubuntu. in that case could you please just do the following:\nrm -rf build\nmkdir build\ncmake -DPythonModular=ON ..\n. @Corllll oh ok great! lemme look into it and get back to...\n. mmmm @elmasee i would really suggest to use the octave_modular interface instead of the static one.\n. @elmasee any particular reason you wanna use the old static command line interface?\n. @fysong86 thanks for reporting but we are not supporting shogun 3.0 anymore...\ncan you paste the errors that happens with shogun 4.1 on either of the ubuntu versions?\n. ok this is not what i meant.\nso when you run the first time cmake there's quite some output on stdout. i would need that to be able to tell what could be the problem.\nbtw: is there any particular reason why you wanna use the python static interface?\nif not the please try it with python modular interface: cmake -DPythonModular=ON ..\n. @fysong86 please look at the python modular examples how to use the modular interface:\nhttps://github.com/shogun-toolbox/shogun/tree/develop/examples/undocumented/python_modular\n. @lkuchenb thnx! there was an error actually in the evaluation_thresholds_modular script, which i think is actually an error that should be investigated as it just popped up now. just to be sure i'm re-running now the python modular test... let's see what's the outcome. \n. @lkuchenb ok so it's definitely an error caused by this change\n. @karlnapf let's merge feature/window :) and then it's all good... although it still has 2 errors... soooo? :)\n. you odnt need windows to fix them :) that's why you have appveyor :)\n. @souravsingh until we dont release a new version you'll have to check out the src from github and then run within the directory the setup.py\n. started to pop up on our ppa builds: https://launchpadlibrarian.net/306118784/buildlog_ubuntu-xenial-amd64.shogun_5.1.0+1SNAPSHOT201702120915_BUILDING.txt.gz. i'm wondering whether an os.walk is always alphabetical - couldn't find out in the manual - or the output is actually not fixed and it's maybe even using inode to do the traversing?\ni'm referring to this bit in class_list.cpp.py:\nfor root, dirs, files in os.walk(basedir):\n        for f in files:\n            if f in class_headers:\n                result.append(os.path.join(os.path.relpath(root, basedir), f))\ni reckon an alphabetical sort on result wouldn't hurt... that way we are sure that the include order is always the same.\n. BINGO: http://stackoverflow.com/a/5667552/2798875. @besser82 @jgomezlopez  okey, the sorting of that result array solved the problem in the ppa build... any of you want to confirm if that's the case on your side as well?. ok great thnx, closing!. done. @karlnapf in general this could be closed, or?. done in 8fdf278dca78f98d4e06058a5ce3defd560d697d \\o/. i've updated in one fix the data commit hash in the repo :)\n. ok this needs rebase, because of my refactor of cmake but as well to see how it actually works now that our shogun-dev has scala in it and the scala detector is already merged.... @piyushgoel997 sure, PRs are welcome!. @piyushgoel997 in that case nothing. in the other case you can see that pthread is being used to run parallel code. you should convert that using OpenMP. all these are well documented libraries, so please read about them and then try to convert the code and send a PR. it's not a problem if it's not working 100%, but without an attempt on the code it's really hard to help.. @piyushgoel997 create a PR and it's gonna get tested and i can review it as well. @micmn cool stuff. i've added there some comments.\nit very well be that there's a bug in the code fixes are more than appreciated.. missing crucial parts... explained what is needed on irc. waiting for fix.. thnx @abhinavrai44 !\ncould you now rebase your #3597 with the latest develop, since that way we'll see if all the things fit together on travis ;). could you please unify the use-case of num_threads =1 and num_threads=2 or more? it should be quite easy..... @MikeLing good idea to have 1 class per solver type... i would create 1 file per class defintion, that way it's much more clearer than this when you have defined several classes in one file. @MikeLing please check other implementation and you'll see that we always have an init() function of a class and that there's an SG_ADD(...), which adds the parameter(s) to the parameter framework... which actually is needed for SGObject.clone (and many other things).. you should SG_ADD params that you want to have serialized... and you should always init the params in the init to their default values.... ok, please address the rest at least.... still many places int32_t for indexing instead of index_t.... and the general design of KNNSolver needs a bit of more attention (see the const-ness of functions....) and dont swallow virtual functions..... @MikeLing before asking for a review make sure that your code actually compiles and passes all the tests with the CI; in your case appveyor is obviously failing:\n[00:17:35] c:\\projects\\shogun\\src\\shogun\\multiclass\\lshknnsolver.cpp(104): error C4716: 'shogun::CLSHKNNSolver::classify_objects_k': must return a value [C:\\projects\\shogun\\build\\src\\shogun\\libshogun.vcxproj]. @MikeLing ok cool! so let's see how the CI are acting, and once done we could merge! would you mind actually doing a squash of commits? :). @MikeLing ok so i've just checked there are still 3 minor changes that i've requested but you havent changed them:\n * init function to be private\n * some of the functions should be private\ncould you send in a minipatch to address those? no need for squashing... thxn. ok cool lets see now the ci and then we merge..... @geektoni i've just tried to reproduce this under 16.04. i have all those packages installed you've mentioned, but i cannot get the error.... could you do a dpkg -l > package.list and send it to me?. @geektoni could you try to reproduce this with the latest develop?. great! then i'm closing this. this should definitely work, we'll look into it!. @mohammadAdnan for these type of questions the mailing list is more appropriate as an issue (hence i'm closing this). currently there's no example showing how to use RBM for recommendation, but certainly you can use it... depends how you actually formulate your problem.\nhere's a paper that talks about how to use RBM for CF:\nhttp://www.machinelearning.org/proceedings/icml2007/papers/407.pdf. what do you exactly mean by deep version... but again subscribe to the mailing list and ask your question there.. ah yeah i guess it could be... max it requires some patching in worst case :). cool, if all the CI (except the octave modular) passes we can merge it.. yeah but isn't this part of #3597 already? if so it's fine there... let's just have that rebased that way everything gets tested there in that pr and close this one..... @palashahuja would really need more information than this... \nlike are you building yourself the docker image?\nor just launching a docker container based on shogun/shogun-dev and try to compile shogun in it?. ok so can you check your cpu load?. @Nicolas99-9 xenial packages in ppa has been only added just recently and we have not released shogun since then. try the nightly ppa, it has the precompiled python package of shogun as well:\nhttps://launchpad.net/~shogun-toolbox/+archive/ubuntu/nightly/. import modshogun. and have you actually done:\napt-get install python-shogun\n?. which code?. should be\n```\nfrom modshogun import sg\n````\nhere are some examples: http://shogun.ml/examples/nightly/index.html. mmm that sg() is quite old... so that will not work...\nbut if you replace these lines like:\nfrom shogun import Classifier,Features,Kernel,Distance\nwith\nfrom modshogun import LibSVM, RealFeatures....\nand then remove the prefixes, it should work :)\ni'm talking about this code:\nhttp://attributes.kyb.tuebingen.mpg.de/new-attributes.py. have fun... let us know if you have any problems!. @Nicolas99-9 here's an example how LibSVM works:\nhttp://shogun.ml/examples/nightly/examples/binary_classifier/kernel_svm.html\nbut please check the examples here as well:\nhttps://github.com/shogun-toolbox/shogun/tree/develop/examples/undocumented/python_modular\nand of course here's the whole API documentation:\nhttp://shogun.ml/api/latest/index.html. please stop pasting here huge debug content, use public services like pastebin.com. this is obviously would not ever run currently:\ncmake -DENABLE_NLOPT=OFF ..\ncmake   cmake -DPythonModular=ON\nmake\nbefore you continue this, please remove the created build directory and just run\nmkdir build\ncmake -DENABLE_NLOPT=OFF -DPythonModular=ON ..\nmake\n. and btw why do you want to build it from the source? if you just want on your ubuntu/debian system python modular interface use the ppa: https://launchpad.net/~shogun-toolbox/+archive/ubuntu/nightly. ok so my apologies it should have been:\nmkdir build\ncd build\ncmake -DENABLE_NLOPT=OFF -DPythonModular=ON ..\nmake\n. yes just add the nightly, and it's not related to your 68% but in case you just want to test shogun with python then that's more than enough for you..... cd /home/anelmad/shogun/build\ncmake -DENABLE_NLOPT=OFF -DPythonModular=ON ..\nmake\nif that's the output then you are clearly doing something very wrong sorry.... and i really cannot help you as this has nothing to do with shogun or cmake...\n. can you copy the content of this file somewhere:\n/home/anelmad/shogun/build/src/shogun/lib/config.h. btw you hand compiled and installed that nlopt..\neither remove it or fix it... you have not compiled that nlopt adequately...\nyour nlopt is in /usr/local/ prefix.... you can remove it or not as you wish.\nclearly this is not a bug of shogun but your system - as state from the errors:\n/usr/bin/ld.gold: error: /usr/local/lib/libnlopt.a(mt19937ar.o): unsupported reloc 23 against local symbol\n/usr/bin/ld.gold: error: /usr/local/lib/libnlopt.a(mt19937ar.o): unsupported reloc 23 against local symbol\n/usr/bin/ld.gold: error: /usr/local/lib/libnlopt.a(qsort_r.o): requires dynamic R_X86_64_PC32 reloc against 'nlopt_qsort_r' which may overflow at runtime; recompile with -fPIC\nuntil you dont fix this error of your custom nlopt shogun will not be able to compile.\ni'm closing this issues as it is clearly not shogun's error\n. @OXPHOS \nHere comes the first problem. a = to_gpu(a) destroys the original CPU vector\nis this somewhere explicit or implicit? if the former, then could you please point to the lines of code?. @OXPHOS yes it is true that in most of the cases our data structures were not thread safe at all... like sgvector and sgmatrix and it was the user's responsibility to make it like that. but in case of that on_gpu i felt that that should be really atomic instead of an if(pointer != null).. @OXPHOS great, now you can merge yourself ;). @OXPHOS how about following up on this with eigen developers first? maybe it's a bug and then they'll fix it in upcoming eigen 3.3.x release. they are quite responsive :). one could use c++11 feature for this:\nhttp://en.cppreference.com/w/cpp/types#Runtime_type_identification. @dracarys09 just start working on it and send a pr..... dunno what's happening with travis but i think this one is really safe to merge..... @clover978 could you provide the script itself as a gist plz?. @clover978 oh great! do you maybe have the data set available as well for sharing?. is it from here http://attributes.kyb.tuebingen.mpg.de/ ?. XLA https://haosdent.gitbooks.io/tensorflow-document/content/resources/xla_prerelease.html. @karlnapf started a gluing lib with tf serving, i guess later using the same idea one can do tighter integration with tf.\n. @lkuchenb cool! let's see what CI says about this :). @karlnapf i think it was completely other stuff :) only thing we need to test that i vividly remember that for some in one of the ipython notebooks the CV stuff was failing with this .... so somebody should try to run the notebook using this patch (imo there's a CV notebook, or?). @lkuchenb i'll try to finish up the input parser story in cpp11 branch so we can merge and then we can rebase this one to be sure about the MSVC et.al.. wooohooo, getting a parallel CV, how awesome is this in 2017! \ud83d\udc83 . @abhinavrai44 could you rebase with the latest develop. @abhinavrai44 could you rebase with the latest develop?. the comments regarding the arrays being used natively instead of SGVector has not been addressed by your fix.\none more thing:\nplease used index_t type for indexes like for example here:\nint32_t idx_r_start = idx_a1;\n       int32_t idx_start = idx_a1;\n       int32_t idx_stop = idx_a2  + 1;. yey great thnx!. @tdjogi010 sure thing, just send a PR when you have something. yeah pretty much that's the story there.. go. go!. @karlnapf @OXPHOS although why compile time? why not runtime parameter? like env var?. ok here's where it should be implemented, and there you can see how it's done for 2 different env vars:\nhttps://github.com/shogun-toolbox/shogun/blob/develop/src/shogun/base/init.cpp#L191-L222. ahahah yeah that var SHOGUN_LINALG_LIB_WARNINGS is pretty long :). :) thnx for the constructive feedback. Please note that we are an opensource project, i.e. we just dont have the resources to handle every request, but as you can see we build and test all modular interfaces that we support on travis and our buildbot farm as well (including octave). @beew on the same note i would highly suggest to read this: https://opensource.guide/how-to-contribute/#communicating-effectively. if it's a branch no need to pr as it's gonna then make the CIs double work as you can see :). boooyaaaa. @cullengao i guess you are using osx... which is completely normal. .so is for linux.. @cullengao this is very basic compiler flags. and i would rather suggest you to use -L<path to the lib folder where you installed shogun> instead of LIBRARY_PATH....\nbut anyhow if you feel that this is something that is missing there's the https://github.com/shogun-toolbox/docs repo and send in a PR . and btw on mac there is no LD_LIBRARY_PATH... it is DYLD_LIBRARY_PATH. @cullengao http://lmgtfy.com/?q=gcc+library+flags. @karlnapf i'm still not convinced that this should go in to the unit tests... and actually not because of the fact that this is going to take a while to run, but because of linking time of shogun-unit-test. try to see how it takes to link that with or without the generated tests (see serialization tests there). i'd actually be happy to move all these slow and long linking time generated stuff into a another executable... any other ideas?. @karlnapf this cmake switch story is getting out of hand... we have so many cmake switches that i rather just build a separate binary and if you dont want it just dont build it. or if you just want really unit tests then you do make shogun-unit-test ... and if you want the serialization tests then you generate that one.... and run that one.... @micmn currently this is how we populate which files to be used for creating the huge shogun-unit-test executable:\nhttps://github.com/shogun-toolbox/shogun/blob/develop/tests/unit/CMakeLists.txt#L82\nnow what we would like to have is that actually you create another executable, just like here:\nhttps://github.com/shogun-toolbox/shogun/blob/develop/tests/unit/CMakeLists.txt#L84\nonly difference that there's a defined set of files one would like to have compiled for that (let's call it set A). \nand then basically the sources for shogun-unit-test = {all unit test sources - set A}\n. @yangliangquanyoung shogun/lib/config.h is generated based on your system using shogun/lib/config.h.in. the config.h will be installed along with the rest of the headers of shogun to prefix location you've supplied. @lambday could we use std::memcpy instead of std::copy?. @lambday and actually could you wrap std::memcpy with shogun::memcpy in memory.h and use that? just to be able to easily plug in a different memcpy method..... @MikeLing as previously, just start wokring and sending prs :). and let's make it totally YOLO! what about adding \ud83d\udc83 icon as a progress bar? :>\nok joking... but we could use utf8 chars for actually doing some progress update instead of oldschool ascii art ;). @geektoni yeah let's template the progress character!  \ud83d\udc4d @karlnapf \ud83d\udc83 is coming!. mmm actually is utf8 in src allowed....? :) or that's a java feature :D. @geektoni should be fine then... in worst case we break wind0ze :). @deveshnag1 mmm in a way yes but as you can see in the meanwhile this has been addressed by somebody else in the https://github.com/shogun-toolbox/shogun/pull/3753 pr. fixed in #3753. @karlnapf can you show me that compiler please? :). Can u plz not do this again... we are having now a merge develop commit in develop... super ugly\n\nOn 23 Dec 2017, at 16:29, Heiko Strathmann notifications@github.com wrote:\nUsing github web interface editor to resolve merge conflicts \ud83d\ude0e\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub, or mute the thread.\n. @deveshnag1 you need to rebase over develop and then squash... read about these in gitbook. @deveshnag1 there's no need to open a new PR. just fix your branch and that'll appear here.. @deveshnag1 this is the reason why we require all students to do entrance tasks. to get familiar with the development environment and that we see how they adapt to it. there are plenty of git manuals out there that can help you in this matter.. @MikeLing i'm not so sure if i understand you... those libraries support specifying as a config variable to print the logs into a file instead of stdout/err. @MikeLing still not sure if i understand you.... i mean what does that have to do where the logs file is being put in the context of this task? that should be up to the user of shogun... it's a runtime configuration of either glog, spdlog or any of the logging library one chooses to use. @iRmantou i think you mixed up a commit in this pr... it's great that you've tried to fix the KernelMachine and use openmp, but do that in a separate pr.. @iRmantou for the future reference... you should just force push stuff into your branch to reset the pr... no need to open a brand new pr.... @elenanst ok those errors on travis are actually real errors with this patch.. ~~@elenanst please check the errors you are getting from CI and try to fix them. a good advise: try to run the tests before sending a PR~~\nnevermind, i should merge first the data prs..... @karlnapf i'd merge this as the current version is wrong.... soooo?. @karlnapf so... merge?. @micmn cool, but you've just copy pasted a bunch of cmake code :) could you maybe define a function/macro for those things and execute it both on shogun-unit-test and shogun-serialization-unit-test... :) thnx!. @micmn i'm invariant regarding this :))) i just wanted to avoid copy-pasting. thnx for the patch!. @lambday just ignore the error... it's already fixed. . @lambday ah and just for the record... there's no swig interface compilation being done on windows :). CI obviously :) but yeah i never ever tried building swig interfaces with MSVC.... and i would be a bit surprise if somebody ever did it... although would be cool :). @sam0410 i reckon your apt is still not trying to fetch from the ppa, see the list of packages of the stable ppa:\nhttps://launchpad.net/~shogun-toolbox/+archive/ubuntu/stable/+packages\n\nlibshogun17 is there. try using the -t flag for apt-get command and specify the repo you wanna use.\nmm i think CARTree was not yet part of libshogun16. @sam0410 or you are not using an amd64 or i386 architecture?. Uuuuhoooh! I just realised, that the xenial ppa support i've added after 5.0.0 release so currently there's only trusty packages in the stable ppa. Lemme check if i could generate them now (xenial packages)\n\nOn Mar 22, 2017, at 18:23, Samikshya Chand notifications@github.com wrote:\nHi @vigsterkr !\nI am using amd64\nI get this when I do sudo apt-get update\nsamikshya@samikshya:$ sudo apt-get update\nIgn:1 http://ppa.launchpad.net/shogun-toolbox/stable/ubuntu xenial InRelease\nHit:2 http://in.archive.ubuntu.com/ubuntu xenial InRelease\nIgn:3 http://ppa.launchpad.net/shogun-toolbox/stable/ubuntu xenial Release\nHit:4 http://in.archive.ubuntu.com/ubuntu xenial-updates InRelease\nIgn:5 http://ppa.launchpad.net/shogun-toolbox/stable/ubuntu xenial/main amd64 Packages\nIgn:6 http://ppa.launchpad.net/shogun-toolbox/stable/ubuntu xenial/main i386 Packages\nIgn:7 http://ppa.launchpad.net/shogun-toolbox/stable/ubuntu xenial/main all Packages\nIgn:8 http://ppa.launchpad.net/shogun-toolbox/stable/ubuntu xenial/main Translation-en_IN\nIgn:9 http://ppa.launchpad.net/shogun-toolbox/stable/ubuntu xenial/main Translation-en\nIgn:10 http://ppa.launchpad.net/shogun-toolbox/stable/ubuntu xenial/main amd64 DEP-11 Metadata\nIgn:11 http://ppa.launchpad.net/shogun-toolbox/stable/ubuntu xenial/main DEP-11 64x64 Icons\nIgn:5 http://ppa.launchpad.net/shogun-toolbox/stable/ubuntu xenial/main amd64 Packages\nIgn:6 http://ppa.launchpad.net/shogun-toolbox/stable/ubuntu xenial/main i386 Packages\nIgn:7 http://ppa.launchpad.net/shogun-toolbox/stable/ubuntu xenial/main all Packages\nIgn:8 http://ppa.launchpad.net/shogun-toolbox/stable/ubuntu xenial/main Translation-en_IN\nIgn:9 http://ppa.launchpad.net/shogun-toolbox/stable/ubuntu xenial/main Translation-en\nIgn:10 http://ppa.launchpad.net/shogun-toolbox/stable/ubuntu xenial/main amd64 DEP-11 Metadata\nIgn:11 http://ppa.launchpad.net/shogun-toolbox/stable/ubuntu xenial/main DEP-11 64x64 Icons\nIgn:5 http://ppa.launchpad.net/shogun-toolbox/stable/ubuntu xenial/main amd64 Packages\nIgn:6 http://ppa.launchpad.net/shogun-toolbox/stable/ubuntu xenial/main i386 Packages\nIgn:7 http://ppa.launchpad.net/shogun-toolbox/stable/ubuntu xenial/main all Packages\nIgn:8 http://ppa.launchpad.net/shogun-toolbox/stable/ubuntu xenial/main Translation-en_IN\nIgn:9 http://ppa.launchpad.net/shogun-toolbox/stable/ubuntu xenial/main Translation-en\nIgn:10 http://ppa.launchpad.net/shogun-toolbox/stable/ubuntu xenial/main amd64 DEP-11 Metadata\nIgn:11 http://ppa.launchpad.net/shogun-toolbox/stable/ubuntu xenial/main DEP-11 64x64 Icons\nIgn:5 http://ppa.launchpad.net/shogun-toolbox/stable/ubuntu xenial/main amd64 Packages\nIgn:6 http://ppa.launchpad.net/shogun-toolbox/stable/ubuntu xenial/main i386 Packages\nIgn:7 http://ppa.launchpad.net/shogun-toolbox/stable/ubuntu xenial/main all Packages\nIgn:8 http://ppa.launchpad.net/shogun-toolbox/stable/ubuntu xenial/main Translation-en_IN\nIgn:9 http://ppa.launchpad.net/shogun-toolbox/stable/ubuntu xenial/main Translation-en\nIgn:10 http://ppa.launchpad.net/shogun-toolbox/stable/ubuntu xenial/main amd64 DEP-11 Metadata\nIgn:11 http://ppa.launchpad.net/shogun-toolbox/stable/ubuntu xenial/main DEP-11 64x64 Icons\nIgn:5 http://ppa.launchpad.net/shogun-toolbox/stable/ubuntu xenial/main amd64 Packages\nIgn:6 http://ppa.launchpad.net/shogun-toolbox/stable/ubuntu xenial/main i386 Packages\nIgn:7 http://ppa.launchpad.net/shogun-toolbox/stable/ubuntu xenial/main all Packages\nIgn:8 http://ppa.launchpad.net/shogun-toolbox/stable/ubuntu xenial/main Translation-en_IN\nIgn:9 http://ppa.launchpad.net/shogun-toolbox/stable/ubuntu xenial/main Translation-en\nIgn:10 http://ppa.launchpad.net/shogun-toolbox/stable/ubuntu xenial/main amd64 DEP-11 Metadata\nIgn:11 http://ppa.launchpad.net/shogun-toolbox/stable/ubuntu xenial/main DEP-11 64x64 Icons\nErr:5 http://ppa.launchpad.net/shogun-toolbox/stable/ubuntu xenial/main amd64 Packages\n404 Not Found\nIgn:6 http://ppa.launchpad.net/shogun-toolbox/stable/ubuntu xenial/main i386 Packages\nIgn:7 http://ppa.launchpad.net/shogun-toolbox/stable/ubuntu xenial/main all Packages\nIgn:8 http://ppa.launchpad.net/shogun-toolbox/stable/ubuntu xenial/main Translation-en_IN\nIgn:9 http://ppa.launchpad.net/shogun-toolbox/stable/ubuntu xenial/main Translation-en\nIgn:10 http://ppa.launchpad.net/shogun-toolbox/stable/ubuntu xenial/main amd64 DEP-11 Metadata\nIgn:11 http://ppa.launchpad.net/shogun-toolbox/stable/ubuntu xenial/main DEP-11 64x64 Icons\nReading package lists... Done\nW: The repository 'http://ppa.launchpad.net/shogun-toolbox/stable/ubuntu xenial Release' does not have a Release file.\nN: Data from such a repository can't be authenticated and is therefore potentially dangerous to use.\nN: See apt-secure(8) manpage for repository creation and user configuration details.\nE: Failed to fetch http://ppa.launchpad.net/shogun-toolbox/stable/ubuntu/dists/xenial/main/binary-amd64/Packages 404 Not Found\nE: Some index files failed to download. They have been ignored, or old ones used instead.\nsamikshya@samikshya:$ gksu nautilus\n(nautilus:10539): GLib-GIO-CRITICAL **: g_dbus_interface_skeleton_unexport: assertion 'interface_->priv->connections != NULL' failed\nsamikshya@samikshya:~$ sudo apt-get update\nIgn:1 http://ppa.launchpad.net/shogun-toolbox/stable/ubuntu xenial InRelease\nIgn:2 http://ppa.launchpad.net/shogun-toolbox/stable/ubuntu xenial Release\nGet:3 http://in.archive.ubuntu.com/ubuntu yakkety InRelease [247 kB]\nIgn:4 http://ppa.launchpad.net/shogun-toolbox/stable/ubuntu xenial/main amd64 Packages\nIgn:5 http://ppa.launchpad.net/shogun-toolbox/stable/ubuntu xenial/main i386 Packages\nIgn:6 http://ppa.launchpad.net/shogun-toolbox/stable/ubuntu xenial/main all Packages\nIgn:7 http://ppa.launchpad.net/shogun-toolbox/stable/ubuntu xenial/main Translation-en_IN\nIgn:8 http://ppa.launchpad.net/shogun-toolbox/stable/ubuntu xenial/main Translation-en\nIgn:9 http://ppa.launchpad.net/shogun-toolbox/stable/ubuntu xenial/main amd64 DEP-11 Metadata\nIgn:10 http://ppa.launchpad.net/shogun-toolbox/stable/ubuntu xenial/main DEP-11 64x64 Icons\nIgn:4 http://ppa.launchpad.net/shogun-toolbox/stable/ubuntu xenial/main amd64 Packages\nGet:11 http://in.archive.ubuntu.com/ubuntu yakkety-updates InRelease [102 kB]\nIgn:5 http://ppa.launchpad.net/shogun-toolbox/stable/ubuntu xenial/main i386 Packages\nIgn:6 http://ppa.launchpad.net/shogun-toolbox/stable/ubuntu xenial/main all Packages\nIgn:7 http://ppa.launchpad.net/shogun-toolbox/stable/ubuntu xenial/main Translation-en_IN\nGet:12 http://in.archive.ubuntu.com/ubuntu yakkety/main Sources [903 kB]\nIgn:8 http://ppa.launchpad.net/shogun-toolbox/stable/ubuntu xenial/main Translation-en\nIgn:9 http://ppa.launchpad.net/shogun-toolbox/stable/ubuntu xenial/main amd64 DEP-11 Metadata\nIgn:10 http://ppa.launchpad.net/shogun-toolbox/stable/ubuntu xenial/main DEP-11 64x64 Icons\nIgn:4 http://ppa.launchpad.net/shogun-toolbox/stable/ubuntu xenial/main amd64 Packages\nIgn:5 http://ppa.launchpad.net/shogun-toolbox/stable/ubuntu xenial/main i386 Packages\nIgn:6 http://ppa.launchpad.net/shogun-toolbox/stable/ubuntu xenial/main all Packages\nIgn:7 http://ppa.launchpad.net/shogun-toolbox/stable/ubuntu xenial/main Translation-en_IN\nIgn:8 http://ppa.launchpad.net/shogun-toolbox/stable/ubuntu xenial/main Translation-en\nIgn:9 http://ppa.launchpad.net/shogun-toolbox/stable/ubuntu xenial/main amd64 DEP-11 Metadata\nGet:13 http://in.archive.ubuntu.com/ubuntu yakkety/main amd64 Packages [1,222 kB]\nIgn:10 http://ppa.launchpad.net/shogun-toolbox/stable/ubuntu xenial/main DEP-11 64x64 Icons\nIgn:4 http://ppa.launchpad.net/shogun-toolbox/stable/ubuntu xenial/main amd64 Packages\nIgn:5 http://ppa.launchpad.net/shogun-toolbox/stable/ubuntu xenial/main i386 Packages\nIgn:6 http://ppa.launchpad.net/shogun-toolbox/stable/ubuntu xenial/main all Packages\nIgn:7 http://ppa.launchpad.net/shogun-toolbox/stable/ubuntu xenial/main Translation-en_IN\nIgn:8 http://ppa.launchpad.net/shogun-toolbox/stable/ubuntu xenial/main Translation-en\nIgn:9 http://ppa.launchpad.net/shogun-toolbox/stable/ubuntu xenial/main amd64 DEP-11 Metadata\nIgn:10 http://ppa.launchpad.net/shogun-toolbox/stable/ubuntu xenial/main DEP-11 64x64 Icons\nIgn:4 http://ppa.launchpad.net/shogun-toolbox/stable/ubuntu xenial/main amd64 Packages\nGet:14 http://in.archive.ubuntu.com/ubuntu yakkety/main i386 Packages [1,218 kB]\nIgn:5 http://ppa.launchpad.net/shogun-toolbox/stable/ubuntu xenial/main i386 Packages\nIgn:6 http://ppa.launchpad.net/shogun-toolbox/stable/ubuntu xenial/main all Packages\nIgn:7 http://ppa.launchpad.net/shogun-toolbox/stable/ubuntu xenial/main Translation-en_IN\nIgn:8 http://ppa.launchpad.net/shogun-toolbox/stable/ubuntu xenial/main Translation-en\nIgn:9 http://ppa.launchpad.net/shogun-toolbox/stable/ubuntu xenial/main amd64 DEP-11 Metadata\nIgn:10 http://ppa.launchpad.net/shogun-toolbox/stable/ubuntu xenial/main DEP-11 64x64 Icons\nErr:4 http://ppa.launchpad.net/shogun-toolbox/stable/ubuntu xenial/main amd64 Packages\n404 Not Found\nIgn:5 http://ppa.launchpad.net/shogun-toolbox/stable/ubuntu xenial/main i386 Packages\nIgn:6 http://ppa.launchpad.net/shogun-toolbox/stable/ubuntu xenial/main all Packages\nIgn:7 http://ppa.launchpad.net/shogun-toolbox/stable/ubuntu xenial/main Translation-en_IN\nGet:15 http://in.archive.ubuntu.com/ubuntu yakkety/main Translation-en [582 kB]\nIgn:8 http://ppa.launchpad.net/shogun-toolbox/stable/ubuntu xenial/main Translation-en\nIgn:9 http://ppa.launchpad.net/shogun-toolbox/stable/ubuntu xenial/main amd64 DEP-11 Metadata\nIgn:10 http://ppa.launchpad.net/shogun-toolbox/stable/ubuntu xenial/main DEP-11 64x64 Icons\nGet:16 http://in.archive.ubuntu.com/ubuntu yakkety/main amd64 DEP-11 Metadata [654 kB]\nGet:17 http://in.archive.ubuntu.com/ubuntu yakkety/main DEP-11 64x64 Icons [368 kB]\nGet:18 http://in.archive.ubuntu.com/ubuntu yakkety-updates/main Sources [86.0 kB]\nGet:19 http://in.archive.ubuntu.com/ubuntu yakkety-updates/main amd64 Packages [224 kB]\nGet:20 http://in.archive.ubuntu.com/ubuntu yakkety-updates/main i386 Packages [220 kB]\nGet:21 http://in.archive.ubuntu.com/ubuntu yakkety-updates/main Translation-en [99.8 kB]\nGet:22 http://in.archive.ubuntu.com/ubuntu yakkety-updates/main amd64 DEP-11 Metadata [146 kB]\nGet:23 http://in.archive.ubuntu.com/ubuntu yakkety-updates/main DEP-11 64x64 Icons [86.8 kB]\nFetched 6,159 kB in 1min 17s (79.9 kB/s)\nReading package lists... Done\nW: The repository 'http://ppa.launchpad.net/shogun-toolbox/stable/ubuntu xenial Release' does not have a Release file.\nN: Data from such a repository can't be authenticated and is therefore potentially dangerous to use.\nN: See apt-secure(8) manpage for repository creation and user configuration details.\nE: Failed to fetch http://ppa.launchpad.net/shogun-toolbox/stable/ubuntu/dists/xenial/main/binary-amd64/Packages 404 Not Found\nE: Some index files failed to download. They have been ignored, or old ones used instead.\nCan you please tell me what to do next?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub, or mute the thread.\n. @sam0410 so i've tried, but unfortunately because of some stupid reasons i wont' be able to create you the stable xenial package... but dont worry, just use the nightly package, it is actually much better (has many fixes) than 5.0.0 :). @sam0410 we are more than happy to have contributions regarding fine-tuning our library :). @geektoni have you seen this error:\nhttps://travis-ci.org/shogun-toolbox/shogun/jobs/238246972#L493\n\nand the warnings:\nhttps://travis-ci.org/shogun-toolbox/shogun/jobs/238246972#L1541. ?. @micmn can we pick this up sometime? :). @micmn could you rebase this one and see what happens on travis? :) would love to get this in ASAP. @micmn one thing: would it be possible that we avoid using files from data and use the fixtures created by @MikeLing see: https://github.com/shogun-toolbox/shogun/pull/3812. hi @mpi-pebert thnx for reporting this. in case you just wanna use shogun on your jessie i'd suggest using the apt repository with the nightly packages of shogun: https://github.com/shogun-toolbox/docs/blob/master/INSTALL.md#debian-\nthe python interface is precompiled as well: simply install the python-shogun package.\ni'm successfully using these binaries with a jupyter notebook server on http://cloud.shogun.ml (which i suggest you to try in case you just want to use shogun in a python project); here's the docker image for it:\nhttps://hub.docker.com/r/shogun/cloud/\nand here's the Dockerfile:\nhttps://github.com/shogun-toolbox/shogun-cloud/blob/master/docker/cloud/Dockerfile\n. @mpi-pebert mmm okey... in that case you have still some options:\n 1. do you have the dependencies (not the build but the runtime dependencies) for the libshogun17 and python-shogun package on the system? because in that case you can just simply unarchive the deb packages and use the shared libraries inside :)\n2. i suppose you are missing some lapack dependencies... so in that case i'll have to investigate a bit. or probably the lapack detection needs some love in shogun. @mpi-pebert if i were you i'd use the packages in the nightly ppa... trust me it's really that much better :) + stable does not have python-shogun package yet\nregarding 2.) would it be possible that you send me the output of your dpkg -l on your system? drop it via mail and i can replicate things in a docker container and maybe even fix the cmake. @mpi-pebert btw just realised, you mentioned Trusty? why trusty? isn't it debian jessie 8.7 that you are using atm? if that's the case (debian jessie) use the apt.shogun.ml repository. @mpi-pebert and another btw: have you actually tried to compile develop branch of shogun? if not could you please try it and let us know if it causes the same bug?. @mpi-pebert thnx. i'll look into the dpkg list but the\nSGBase.i:311: Error: Unable to find 'swig_typemaps.i'\nis very alarming because that's basically saying that you are missing a file that is in the repository :(\ncould you actually check whether the file exists in src/interfaces/python_modular/swig_typemaps.i. mmm yeah the lapack/cblas problem with release 5.0.0 are actually known... i was just hoping that we got it right finally in current develop. the current develop is inches away to be the next 6.0.0 release so i'm closing this now.\nbut i'll open a bug regarding the make clean story. thnx. @tingpan so you replaced the relative path to the data dir, with a relative path to a script... i mean this is the same thing in different wrapping. the point was that there should be no relative path... @tingpan great! have you tested 1-2 of these notebooks that it works ?. ok perfect merging. @deveshnag1 most probably... then again the task is to check and solve this issue :). @deveshnag1 i'm not so sure how you've tested that notebook as although the particular error is gone from the notebook but there's other now:\n```\n[NbConvertApp] ERROR | Error while converting '/home/buildslave/nightly_default/build/doc/ipython-notebooks/gaussian_process/variational_classifier.ipynb'\nTraceback (most recent call last):\n  File \"/usr/local/lib/python2.7/dist-packages/nbconvert/nbconvertapp.py\", line 357, in export_single_notebook\n    output, resources = self.exporter.from_filename(notebook_filename, resources=resources)\n  File \"/usr/local/lib/python2.7/dist-packages/nbconvert/exporters/exporter.py\", line 165, in from_filename\n    return self.from_file(f, resources=resources, kw)\n  File \"/usr/local/lib/python2.7/dist-packages/nbconvert/exporters/exporter.py\", line 183, in from_file\n    return self.from_notebook_node(nbformat.read(file_stream, as_version=4), resources=resources, kw)\n  File \"/usr/local/lib/python2.7/dist-packages/nbconvert/exporters/html.py\", line 65, in from_notebook_node\n    return super(HTMLExporter, self).from_notebook_node(nb, resources, kw)\n  File \"/usr/local/lib/python2.7/dist-packages/nbconvert/exporters/templateexporter.py\", line 200, in from_notebook_node\n    nb_copy, resources = super(TemplateExporter, self).from_notebook_node(nb, resources, kw)\n  File \"/usr/local/lib/python2.7/dist-packages/nbconvert/exporters/exporter.py\", line 130, in from_notebook_node\n    nb_copy, resources = self._preprocess(nb_copy, resources)\n  File \"/usr/local/lib/python2.7/dist-packages/nbconvert/exporters/exporter.py\", line 302, in _preprocess\n    nbc, resc = preprocessor(nbc, resc)\n  File \"/usr/local/lib/python2.7/dist-packages/nbconvert/preprocessors/base.py\", line 47, in call\n    return self.preprocess(nb,resources)\n  File \"/usr/local/lib/python2.7/dist-packages/nbconvert/preprocessors/execute.py\", line 145, in preprocess\n    nb, resources = super(ExecutePreprocessor, self).preprocess(nb, resources)\n  File \"/usr/local/lib/python2.7/dist-packages/nbconvert/preprocessors/base.py\", line 70, in preprocess\n    nb.cells[index], resources = self.preprocess_cell(cell, resources, index)\n  File \"/usr/local/lib/python2.7/dist-packages/nbconvert/preprocessors/execute.py\", line 176, in preprocess_cell\n    raise CellExecutionError(msg)\nCellExecutionError: An error occurred while executing the following cell:\n\nan example for the sonar data set\ntrain_size=108\n(x_train, y_train, x_test, y_test, B)=extract(os.path.join(SHOGUN_DATA_DIR, 'uci/sonar/sonar.all-data', train_size))\ninference_methods =[\n                  KLDualInferenceMethod,\n                  SingleLaplaceInferenceMethod,\n                  SingleLaplaceInferenceMethod,\n                  KLDiagonalInferenceMethod,\n                  #KLCholeskyInferenceMethod, #this method takes too long to run\n                  #KLCovarianceInferenceMethod, #this method takes too long to run\n                  ]\nlikelihoods =[\n            LogitDVGLikelihood(), #KLDual method uses a likelihood class that supports dual variational inference\n            LogitLikelihood(),      \n            LogitVGLikelihood(), #KL method uses a likelihood class that supports variational inference\n            LogitVGLikelihood(), #KL method uses a likelihood class that supports variational inference\n            #LogitVGLikelihood(), #KL method uses a likelihood class that supports variational inference\n            #LogitVGLikelihood(), #KL method uses a likelihood class that supports variational inference\n            ]\nminimizers =[\n            KLDualInferenceMethodMinimizer,\n            None, #using default minimizer\n            LBFGSMinimizer,\n            LBFGSMinimizer,\n            #LBFGSMinimizer,\n            #LBFGSMinimizer,\n            ]\nlinesearches=[\n            BACKTRACKING_ARMIJO,\n            None, #using default line search method\n            BACKTRACKING_STRONG_WOLFE,\n            BACKTRACKING_STRONG_WOLFE,\n            #BACKTRACKING_STRONG_WOLFE,\n            #BACKTRACKING_STRONG_WOLFE,\n            ]\ncol_size=8\nlscale_min=0.0\nlscale_max=5.0\nlsigma_min=0.0\nlsigma_max=5.0\ndelta=0.5\nscale=5.0\nlscale_list = np.arange(lscale_min, lscale_max, delta)\nlsigma_list = np.arange(lsigma_min, lsigma_max, deltascale)\nlScale, lSigma = np.meshgrid(lscale_list, lsigma_list)\nwidth=len(likelihoods)/2\nf, plots =plt.subplots(width, 2, figsize=(col_size2,col_size*width))\napprox_bit_plot(inference_methods, minimizers, linesearches, likelihoods, x_train, y_train,  x_test, y_test, plots, lScale, lSigma, B)\nplt.show()\nAttributeError: 'int' object has no attribute 'startswith'\n. can lah!. @karlnapf is this done fully or still there are parts that needs to be cleared up?. @geektoni great! let's see the CIs. great stuff! just one minor thing: do you mind adding a mini `if` branch to this:\ndocker exec -t devenv /bin/sh -c \"cd /opt/shogun/; ./scripts/check_format.sh\"\n``\nthat this docker command is only executed whenCC == gcc&-z $OctaveModularso that the formatting check is only executed in one of the jobs... as it's really unnecessary to have it run in all the jobs.. awesome, merging!. @MikeLing this is a pretty simple task, mostly it requires you to switch adatafile to generated features..... @gbohner i'm not so sure if i understand what do you mean byonly the old binaries are shipped`?\nwe are not generating the meta examples using brew...\ndocumentation is not being generated at all.... @gbohner would be great if you could get back to us what would you like to see in the shogun package... as soon we're gonna have a small patch release and would like to get things right in the next brew update. much appreciated!. @karlnapf this is not even true for our debian package... :) . In order to be able to use MKL you need to have svmlight, which we are not allowed to distribute hence for MKL you need to compile shogun yourself from source. That should solve your problem.\n\nOn Jun 23, 2017, at 16:45, romainviard notifications@github.com wrote:\nOk thanks a lot for the link...I will probably test old version but that does not seem to be a solution for the future ....Maybe have a look elsewhere ? Are you pleased of obtained result ?\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub, or mute the thread.\n. @romainviard you should be able to use with shogun 6.0.0 as well... just make sure you compile shogun yourself.... @romainviard what does crash mean exactly? could you give some debug output?. mmm i wonder why the ocas test fails on gcc but not on clang :S. @MikeLing the changes in other tests (which uses binary data in one way or another) gonna be done in another PR?. @duhduhdan oh yeah, science tap should be specified! thnx a lot!. @MikeLing cool let's see what the CI does.. mmm travis tend to do these things lately:\n```\nThe command \"docker exec -t devenv /bin/sh -c \"cd /opt/shogun/build; ctest --output-on-failure -j 2\"\" exited with 0.\n\nDone. Your build exited with 1.\nwhich seems to be due to\nclang: error: unable to execute command: Killed\nclang: error: clang frontend command failed due to signal (use -v to see invocation)\nUbuntu clang version 3.4-1ubuntu3 (tags/RELEASE_34/final) (based on LLVM 3.4)\nTarget: x86_64-pc-linux-gnu\nThread model: posix\nclang: note: diagnostic msg: PLEASE submit a bug report to http://bugs.debian.org/ and include the crash backtrace, preprocessed source, and associated run script.\nclang: note: diagnostic msg: Error generating preprocessed source(s) - no preprocessable inputs.\nmake[2]: *** [tests/unit/CMakeFiles/shogun-unit-test.dir/clone_unittest.cc.o] Error 254\nmake[1]: *** [tests/unit/CMakeFiles/shogun-unit-test.dir/all] Error 2\nmake[1]: *** Waiting for unfinished jobs....\n[ 87%] Building CXX object tests/unit/CMakeFiles/shogun-serialization-unit-test.dir/base/main_unittest.cc.o\n[ 87%] Building CXX object tests/unit/CMakeFiles/shogun-serialization-unit-test.dir/utils/Utils.cpp.o\nLinking CXX executable ../../bin/shogun-serialization-unit-test\nDiscovering Tests in shogun-serialization-unit-test\n[ 87%] Built target shogun-serialization-unit-test\nmake: *** [all] Error 2\n``\ni reckon we are running out of mem?. @geektoni i've just restarted the windows based one. @MikeLing indeed it's std::vector that we want here. good that the CIs are all passing! :) lemme just quickly look at the code. @MikeLing is this just the beginning of this PR? i mean why isn'tsrc/shogun/base/DynArray.hdropped in the commit?. yeah, but i'd do it commit-by-commit within this pr (dont need to rebase) because i'm a bit afraid that suddenly with one change we will face some swig related problem. so let's have here all the changes... regarding DynArray. i've meant: you dont need to squash the commits... it's fine to have it in separate commits we'll squash it on the end automagically . yes. @micmn after merging #3830 this requires a rebase. @micmn let's rebase :). @OXPHOS if you by any chance happen to have some time to give a glimpse and blessing on this that'd be appreciated! . @micmn hehe yeah i reckon we can omit the style refactor in this case :P. or of course there's an option to actually not return the array itself but only an iterator... and refactor some parts of shogun to use iterator to copy CDynamicArray... this of course still does not solve our issue with serialization.. based on gdb ofGaussianProcessClassification.apply_preprocessor_and_binarythe problem is of course is inshogun::CDynamicArray::set_element; it seems it goes into an infinite recursive callstack as there are more than 20000+ set_element calls on the stack.... please try to copy-paste code without **ERRORS**. @MikeLing one more thing. please dont even push any commits until you dont havectestrunning successfully locally.... @MikeLing would be great to add unit-tests in the other PR to cover these errors, so that next time if there's a refactor the problem is easily caught. @MikeLing just for the record make sure that you create for each set_element interface a separate unit test (as with the deque refactor that's where things went bad). @MikeLing interestingly unit test on travis fails ;). @xxnyufengzhe you can use either... we don't have regular tests for 32 bit only for 64 bit, none the less it should work on 32 bit as well.. @xxnyufengzhe please read the manual carefully....\nif you want data repository then you should clone it:git submodule update --init`. @micmn let's rebase :). @micmn so there are 2 things\na) we need to modularize our linalg header as this is unscalable like this; not only because of compiler running out of memory but because of the fact that the LoC is 1000+ is just insane imo\nb) unit tests are missing\n. yeah is there any reason why we wouldn't do that ASAP?. @micmn could you try to rebase on latest develop this as i'm hoping that with the latest appveyor.yaml the out of heap space thing will not occur (fingerscrossed). argh it still fails with #3844 :((( reached out to appveyor, in case they have an idea..... 140a8eb77cfcd523711274789d4e5d6b5f30bd52 is hopefully fixing this... let's see couple of builds. but basically disabled VS2017 for the time being (as it takes way too much time to run one job) & disabled parallel build, this way hopefully there's more memory left for building the given source file.. feel free to reopen if you start to see this again ;). \ud83d\udc83 did not help:\nhttps://ci.appveyor.com/project/vigsterkr/shogun/build/952#L3499. ok this should be done as @micmn divided up the files.... . @lisitsyn hunting bears :). seems gcc fails due to clang-format errors, and clang job failed because there was a hiccup of github:\n```CMake Error at GoogleMock-stamp/download-GoogleMock.cmake:27 (message):\nerror: downloading\n'https://github.com/google/googletest/archive/release-1.8.0.tar.gz' failed\n. @micmn once you have the factored macro header just push it so then the gcc/clang gets retested. do we wanna fix now the clang-format? or that should be done later as part of another refactor? :).   148 - unit-Signal (OTHER_FAULT). a bit of a more information\nNote: Google Test filter = Signal.*\n[==========] Running 2 tests from 1 test case.\n[----------] Global test environment set-up.\n[----------] 2 tests from Signal\n[ RUN      ] Signal.SIGINT_test\n/opt/shogun/tests/unit/lib/Signal_unittest.cc:26: Failure\nDeath test: { CSignal tmp; tmp.enable_handler(); int on_next_v = 0; int on_complete_v = 0; auto sub = rxcpp::make_subscriber( &on_next_v { on_next_v = 1; }, & { on_complete_v = 1; }); tmp.get_SIGINT_observable().subscribe(sub); tmp.get_SIGINT_observable().connect(); switch (0) case 0: default: if (const ::testing::AssertionResult gtest_ar_ = ::testing::AssertionResult((on_complete_v == 1))) ; else ::testing::internal::AssertHelper(::testing::TestPartResult::kNonFatalFailure, \"/opt/shogun/tests/unit/lib/Signal_unittest.cc\", 26, ::testing::internal::GetBoolAssertionFailureMessage( gtest_ar_, \"on_complete_v == 1\", \"false\", \"true\").c_str()) = ::testing::Message(); exit(0); }\nResult: died but not with expected exit code:\n\n        Terminated by signal 6 (core dumped)\n\nActual msg:\n[  DEATH   ] terminate called without an active exception\n[  DEATH   ] \n[  FAILED  ] Signal.SIGINT_test (286 ms)\n[ RUN      ] Signal.SIGURG_test\nterminate called without an active exception. it can be closed it's fixed and it's in develop branch. @karlnapf for this would you use https://github.com/shogun-toolbox/shogun/blob/develop/src/shogun/ensemble/MeanRule.h\nonce RF can output probabilities for classification?. ok this should pass on CIs.. but let's be 100% sure about it and wait for them once it's green then we merge. @MikeLing adding a test for this (serialization test) would be good so that we know it works :P. can you paste here the content of the `serialization-asciiCBinaryLabels.XXXXXX` file?. @MikeLing you are testing the wrong thing in you test... you should check `get_value(i)` equaling...\n    for (int32_t i = 0; i < new_labels->get_num_labels(); i++)\n    {\n        EXPECT_EQ(\n            labels->get_value(i), new_labels->get_value(i));\n}\n```. @geektoni ok sorry i did here the review of both commits :S. @MikeLing if you wanna insist on these changes then let's switch as well all the CMath methods and other SGVector/SGMatrix methods to linalg plz! :) lemme know if you need help there which one and how!. @MikeLing i dont see the point of doing halfway things in this case. if you started this then plz do it properly if not then close this PR. the change of Malloc to SGVector is really not enough here... that can be done with a regex script automatically...\n. @MikeLing ok there are some debug stuff that should be removed, otherwise it's looking good. once those debug things are removed and some of those minor problems address we should merge this... later as @karlnapf mentioned we might wanna do a refactor and then get rid of that lapack dependency.... @MikeLing my only concern is that GMM is tested nowhere automatically only in a notebook :S. @MikeLing i'm not aware of anybody planning to add unit test for GMM ;)\none easy way that you manually try to run the ipython notebook locally and check if it's the same output as here: http://shogun.ml/notebook/latest/GMM.html. @MikeLing oh yeah that's should at least as well assure that we are still having the same output :))). @MikeLing ok lemme know when you've done with the notebook test as this seems to be ok so i'm gonna merge once you are done with double-checking.thnx. @karlnapf this has nothing to do with the new random.. @MikeLing can we please finish up this one by friday? what is missing is couple of matrix multiplications instead of using for loops... ping me if you need help, plz! just let's have this finally merged!. @MikeLing @micmn . @MikeLing . @MikeLing is this fixed?. @MikeLing can you valgrind that test? as our memtest output is having some problem :(. @MikeLing . @micmn appveyor likes it :). travis not so much.... @OXPHOS i'm just updating the travis build image so that you have libcereal-dev available :) once it's done i'll let you know. although yeah including cereal in .h is not the best thing :P. @OXPHOS could you fix the clang-format errors? btw do we wanna merge this directly into develop now?. @OXPHOS cool still formatting issues :). https://www.reddit.com/r/cpp/comments/30w7cs/inconsistency_in_c_random/. ok so i mean we are not thaaaaaaaaaaat bad (although of course note that good either). so basically i've just tested and as it is specified in the standards the random engines (mt19937, mt19937_64, minstd_rand etc) are generating the same values with the same order on the same seed :)\nof course it's annoying now that we'll have to provide our distribution mappers (like uniform, normal etc) but still we can do that - would be nice though to use the std api - and then have that as our random. still we can this way through out the external random generators that i've pulled in couple of years ago and still we are good with removing the global random :) . @mikeling :) cool console!\nbut what i have tested was this:\nstd::mt19937_64 rng(100);\n    for(int32_t i=0 ; i<15; i++)\n    {\n        std::cout << rng() << std::endl;\n}. based on the standard this should be the same regardless of the compiler/os/etc.\nthese random engines are all generating uniform distributed integers. now the way you generate a given interval of uniform distributed elements or other distributions based on these engines is actually implementation specific. but that we can provide ourselves :) see in CRandom how we use a  PRNG that generates uniform distributed random to get samples of a normal distribution with a given mean and variance.. so bascially the idea is that we can throw out src/shogun/lib/external/SFMT and src/shogun/lib/external/dSFMT/ and use c++'s own random engines, but provide our own implementation of std::uniform_int_distribution and std::normal_distribution etc.\nit's a bit unfortunate as it would have been great to be able to use standards all the way, but.... :) on the other hand i'm just saying that we only require these distributions having the very same order of the very same values because of our tests, right? conceptually the model itself suppose to work correctly if it's getting the samples of the distribution it expects (regardless of the actual values of those samples... that's why it uses a prng and not a fixed set of integers in the first place)... \nit should not require to have a specific value, just that the samples are from the right distribution... \nit's only ourselves, who wanna be able to test that the model is producing the very same thing with the same samples (when setting the seed)... only thing that our fixtures in the tests are for a given sample-set of a distribution on a given OS/distribution... \nin other words if we could address the fact that the sample-set with a fixed seed of a random variable is different based on the compiler than we actually could use the standard all the way. i think it would be a rather big sacrifice to say that just because our tests methodology we wanna throw out half of c++11's features for random... or?\nwhat do you think @karlnapf ?\n. i've just realised that say we use c++11's random features all the way in the implementation and we somehow solve the problem of testing. i'm suspecting that of course one could still argue that if we do this the problem could be that one would end up with different models using the same seed depending on the compiler OS, or? question of course is that would those model be sooooo different? on the other hand i think it's a fair presumption from the user that regardless of your compiler/system etc for the same (data, prng seed) pair you wanna end up having the same model, or?. @MikeLing mmmm not really... actually using CMeanShift and others would be in a way better solution than using mock data... as mock data either you have for each and every different platform/compiler... in case of random generator you can rely on the fact that they are consistent within the platform :)\nbtw: have you checked whether that function i pasted above generates the same set and order of elements on windows? on osx and linux it's:\n1791095845\n4282876139\n3093770124\n4005303368\n491263\n550290313\n1298508491\n4290846341\n630311759\n1013994432\n396591248\n1703301249\n799981516\n1666063943\n1484172013\n. @MikeLing ok wait this is weird. as they not only have different value but the decimal point as well... we should check on this because based on c++11 standards the random generators like std::mt19937_64 should actually be the same.. okey! it's me who has been the idiot here! the output that i've pasted was for std::mt19937 rng(100);  :D i've tested with std::mt19937_64 rng(100);. i've got the same as you @MikeLing \n17739577640593830518\n5969071622678286091\n17952737041105130042\n2376730765988821965\n16631841681740081638\n9150857099769749596\n17628273237363548829\n12758643431150887824\n15745505721093787712\n16772453002033882614\n18026061444052040848\n3392054037201815393\n9583663648522241488\n8357967283887551697\n10686319701058721126. @MikeLing are we ready with porting everywhere using std::uniform_int_distribution and std::normal_distribution? and it's only the unit tests that are failing now?. @MikeLing have you tried changing the seed of the prng to something else than now?\nbut then again this seems to be really unrelated somehow to prng itself, there's something else...\nok so now we have then everything using c++11 random, right?\nbased on travis apart from the neural net errors the following tests fail:\n301 - integration_meta_cpp-converter-ica_fast (Failed)\n    319 - integration_meta_cpp-multiclass_classifier-random_forest (Failed)\n    323 - integration_meta_cpp-clustering-gmm (Failed)\ni suppose these differences/errors are based on differences in random implementation? have you debugged this @MikeLing ?. @MikeLing is there a chance that you could test them on a linux machine using gcc? like docker on osx?. @MikeLing ok so those errors are because of the discovered difference between the different implementations of the sampling functions, like std::uniform_int_distribution, std::normal_distribution etc. so now basically we should come up with an integration test how to overcome this. . @MikeLing i'm not so sure if i understand ;). ok so since this goes into a feature branch i'm merging this and rebasing it... then i'll try to come up with a solution for the fixtures. @dougalsutherland the reference commit should fix your problem with using the right lapack... but the usage of LAPACK by Eigen is still experimental ENABLE_EIGEN_LAPACK as it fails to compile.... @dougalsutherland yeah that'll require substantial work to get it work... i've tried getting MKL as well but the problem is that the current way we detect MKL is not good with anaconda... anyhow this should solve your initial main problem :). @MaximilianLomardo how do you install shogun on your machine?. @MaximilianLomardo the problem with SVRLight and MKL is that they depend on SVMLight, which we are not allowed to distribute per license, hence if you wanna use those you'll have to compile shogun from source.\n@besser82 am i right that this is the case as well on fedora?. @MaximilianLomardo try http://shogun.ml/install there's a section how to compile it manually. @micmn do you fancy to look into this? :). i started off fixing in feature/eigen-lapack branch. @dougalsutherland i reckon i'm done with this.. i'll do more tests but then i'll merge the feature/eigen-lapack into develop. mmm it fails on osx... it requires further guarding in lapack.h to avoid cyclic definition of functions. just merged feature/eigen-lapack branch into develop, which should enable blas/lapack to be used by Eigen by default. @dougalsutherland oh that's weird although as you can see we never wondered these territories :) lemme try to check.... but i think we need to fiddle maybe with cmake :P. ooooooooooooh now i understand!\nthese things come up when you compile the examples:\n\"C:\\bld\\shogun_1499556275470\\work\\shogun-shogun_6.0.0\\build\\examples\\undocumented\\libshogun\\transfer_multitaskleastsquaresregression.vcxproj\" (default target) (217) ->\n  shogun.lib(Compressor.obj) : error LNK2019: unresolved external symbol __lzo_init_v2 referenced in function \"public: void __cdecl shogun::CCompressor::compress(unsigned char *,unsigned __int64,unsigned char * &,unsigned __int64 &,int)\" (?compress@CCompressor@shogun@@QEAAXPEAE_KAEAPEAEAEA_KH@Z) [C:\\bld\\shogun_1499556275470\\work\\shogun-shogun_6.0.0\\build\\examples\\undocumented\\libshogun\\transfer_multitaskleastsquaresregression.vcxproj]\n  shogun.lib(Compressor.obj) : error LNK2019: unresolved external symbol lzo1x_decompress referenced in function \"public: void __cdecl shogun::CCompressor::decompress(unsigned char *,unsigned __int64,unsigned char *,unsigned __int64 &)\" (?decompress@CCompressor@shogun@@QEAAXPEAE_K0AEA_K@Z) [C:\\bld\\shogun_1499556275470\\work\\shogun-shogun_6.0.0\\build\\examples\\undocumented\\libshogun\\transfer_multitaskleastsquaresregression.vcxproj]\n  shogun.lib(Compressor.obj) : error LNK2019: unresolved external symbol lzo1x_1_15_compress referenced in function \"public: void __cdecl shogun::CCompressor::compress(unsigned char *,unsigned __int64,unsigned char * &,unsigned __int64 &,int)\" (?compress@CCompressor@shogun@@QEAAXPEAE_KAEAPEAEAEA_KH@Z) [C:\\bld\\shogun_1499556275470\\work\\shogun-shogun_6.0.0\\build\\examples\\undocumented\\libshogun\\transfer_multitaskleastsquaresregression.vcxproj]\n  shogun.lib(Compressor.obj) : error LNK2019: unresolved external symbol lzo1x_999_compress referenced in function \"public: void __cdecl shogun::CCompressor::compress(unsigned char *,unsigned __int64,unsigned char * &,unsigned __int64 &,int)\" (?compress@CCompressor@shogun@@QEAAXPEAE_KAEAPEAEAEA_KH@Z) [C:\\bld\\shogun_1499556275470\\work\\shogun-shogun_6.0.0\\build\\examples\\undocumented\\libshogun\\transfer_multitaskleastsquaresregression.vcxproj]\n  shogun.lib(Compressor.obj) : error LNK2019: unresolved external symbol compress2 referenced in function \"public: void __cdecl shogun::CCompressor::compress(unsigned char *,unsigned __int64,unsigned char * &,unsigned __int64 &,int)\" (?compress@CCompressor@shogun@@QEAAXPEAE_KAEAPEAEAEA_KH@Z) [C:\\bld\\shogun_1499556275470\\work\\shogun-shogun_6.0.0\\build\\examples\\undocumented\\libshogun\\transfer_multitaskleastsquaresregression.vcxproj]\n  shogun.lib(Compressor.obj) : error LNK2019: unresolved external symbol uncompress referenced in function \"public: void __cdecl shogun::CCompressor::decompress(unsigned char *,unsigned __int64,unsigned char *,unsigned __int64 &)\" (?decompress@CCompressor@shogun@@QEAAXPEAE_K0AEA_K@Z) [C:\\bld\\shogun_1499556275470\\work\\shogun-shogun_6.0.0\\build\\examples\\undocumented\\libshogun\\transfer_multitaskleastsquaresregression.vcxproj]\n  shogun.lib(Compressor.obj) : error LNK2019: unresolved external symbol BZ2_bzCompressInit referenced in function \"public: void __cdecl shogun::CCompressor::compress(unsigned char *,unsigned __int64,unsigned char * &,unsigned __int64 &,int)\" (?compress@CCompressor@shogun@@QEAAXPEAE_KAEAPEAEAEA_KH@Z) [C:\\bld\\shogun_1499556275470\\work\\shogun-shogun_6.0.0\\build\\examples\\undocumented\\libshogun\\transfer_multitaskleastsquaresregression.vcxproj]\n  shogun.lib(Compressor.obj) : error LNK2019: unresolved external symbol BZ2_bzCompress referenced in function \"public: void __cdecl shogun::CCompressor::compress(unsigned char *,unsigned __int64,unsigned char * &,unsigned __int64 &,int)\" (?compress@CCompressor@shogun@@QEAAXPEAE_KAEAPEAEAEA_KH@Z) [C:\\bld\\shogun_1499556275470\\work\\shogun-shogun_6.0.0\\build\\examples\\undocumented\\libshogun\\transfer_multitaskleastsquaresregression.vcxproj]\n  shogun.lib(Compressor.obj) : error LNK2019: unresolved external symbol BZ2_bzCompressEnd referenced in function \"public: void __cdecl shogun::CCompressor::compress(unsigned char *,unsigned __int64,unsigned char * &,unsigned __int64 &,int)\" (?compress@CCompressor@shogun@@QEAAXPEAE_KAEAPEAEAEA_KH@Z) [C:\\bld\\shogun_1499556275470\\work\\shogun-shogun_6.0.0\\build\\examples\\undocumented\\libshogun\\transfer_multitaskleastsquaresregression.vcxproj]\n  shogun.lib(Compressor.obj) : error LNK2019: unresolved external symbol BZ2_bzDecompressInit referenced in function \"public: void __cdecl shogun::CCompressor::decompress(unsigned char *,unsigned __int64,unsigned char *,unsigned __int64 &)\" (?decompress@CCompressor@shogun@@QEAAXPEAE_K0AEA_K@Z) [C:\\bld\\shogun_1499556275470\\work\\shogun-shogun_6.0.0\\build\\examples\\undocumented\\libshogun\\transfer_multitaskleastsquaresregression.vcxproj]\n  shogun.lib(Compressor.obj) : error LNK2019: unresolved external symbol BZ2_bzDecompress referenced in function \"public: void __cdecl shogun::CCompressor::decompress(unsigned char *,unsigned __int64,unsigned char *,unsigned __int64 &)\" (?decompress@CCompressor@shogun@@QEAAXPEAE_K0AEA_K@Z) [C:\\bld\\shogun_1499556275470\\work\\shogun-shogun_6.0.0\\build\\examples\\undocumented\\libshogun\\transfer_multitaskleastsquaresregression.vcxproj]\n  shogun.lib(Compressor.obj) : error LNK2019: unresolved external symbol BZ2_bzDecompressEnd referenced in function \"public: void __cdecl shogun::CCompressor::decompress(unsigned char *,unsigned __int64,unsigned char *,unsigned __int64 &)\" (?decompress@CCompressor@shogun@@QEAAXPEAE_K0AEA_K@Z) [C:\\bld\\shogun_1499556275470\\work\\shogun-shogun_6.0.0\\build\\examples\\undocumented\\libshogun\\transfer_multitaskleastsquaresregression.vcxproj]\n  C:\\bld\\shogun_1499556275470\\work\\shogun-shogun_6.0.0\\build\\examples\\undocumented\\libshogun\\Debug\\transfer_multitaskleastsquaresregression.exe : fatal error LNK1120: 12 unresolved externals [C:\\bld\\shogun_1499556275470\\work\\shogun-shogun_6.0.0\\build\\examples\\undocumented\\libshogun\\transfer_multitaskleastsquaresregression.vcxproj]\nlet's close this ticket and open a new one that tries to handle this case on windows. great stuff! the mentioned unit tests would be super appreciated (see codecov's complaints). @karlnapf indeed we are having one this week :). @micmn this one needs a rebase. could you please do it? \ud83d\udc4d . @geektoni . BOOOOOOOOOOOOOOOM!. @karlnapf how do you imagine this to work/demonstrate in a notebook? note that TB is a separate JS app. @karlnapf yeah you dont want TB actually.... but observable structures all around. @karlnapf FYI https://github.com/shogun-toolbox/shogun/blob/develop/src/interfaces/swig/ParameterObserver.i. thnx for the fix!\nthere are some codestyle errors: https://travis-ci.org/shogun-toolbox/shogun/jobs/253777248#L749\n. @HeyItsDavid heheh there's still one more:\n```-        SG_SERROR(\"feature index out of bounds (%d >= %d)\\n\", idx, get_num_feature_obj());\n\n\nSG_SERROR(\n\n\n\"feature index out of bounds (%d >= %d)\\n\", idx,\n\n\nget_num_feature_obj());```. @HeyItsDavid i know it must be pain but could you fix the last remaining indentation error in the patch:\nhttps://travis-ci.org/shogun-toolbox/shogun/jobs/253919606#L750\n\n\nthnx. @OXPHOS can you rebase as i think @geektoni fixed some of the clang-format errors.... @OXPHOS merge material?. @micmn . @dougalsutherland this is like the best thing since.... EVER! :) thank you sooooo much for your efforts!. Forgive my ignorance here but svmlight != gpl afaik\n\nOn Aug 2, 2017, at 18:36, Heiko Strathmann notifications@github.com wrote:\nOh yes, SVMLight now needs to be guarded to only be available when USE_GPL_SHOGUN is set. I did not realize. Want to go ahead? The unit tests, meta examples, libshogun examples also need guarding then\n\u2014\nYou are receiving this because you modified the open/close state.\nReply to this email directly, view it on GitHub, or mute the thread.\n. @qcrist this is invaluable information!!! thank you soooooooooo much! as nobody yet tried to use swig on MSVC :) lemme try to address some of these bugs. @Galaxy-Fish do you have libshogun.so in library path? like LD_LIBRARY_PATH?. @lacava it's not trivial at all it's just that all of us are a bit busy. :( sorry that nobody looked into this yet... i'll try to look into it and see what could be done or at least what's the main issue. sorry again and thanks heaps for this awesome bug report!. needs rebase :). ++ for wiki!. @MikeLing i did a rebase and small changes in [wip] 2nd round so you maybe wanna remove those two commits from your branch in order to resolve this merge conflict now :). @Shenggang if you wanna use custom kernel and you are using python you could simply implement your kernel in python using director class.\nsee the example in examples/undocumented/python/kernel_director_linear.py for details. @AlexBinder did you download release zip or latest version of develop?. @evertes i'm closing this one, as @dougalsutherland should fix your problem. de ha van valami gond nyugodtan nyisd ki a ticketet :). @dougalsutherland thnx for clarifying. i hope you enjoyed down under :). @grig-guz you mean like ndarray?. @grig-guz there is actually SGNDArray but if the matrices are of the same dimensions you are probably better off storing it in an std::vector<SGMatrix<Type>> or something. The latest release is part of homebrew.\nIf you want to compile it from source follow this manual:\nhttp://shogun.ml/install#manual\n\nUsually if there's such error with make after cmake, that means that there was an error running cmake. Can you put your full cmake output to pastebin.com and put the link here, please?\n\nOn Aug 24, 2017, at 13:41, Rudrani Angira notifications@github.com wrote:\nI am trying to compile the shogun project manually on MacOS. I created a folder \"build\" , executed the \"cmake ..\" command.\nBut it gives me error when I try to use \"make\". I see that there is no \"Makefile\" in the build folder. I also tried doing \"./configure\" but I always get \"No such command found\".\nCould you please share how I am supposed to use \"make\" especially on mac.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub, or mute the thread.\n. ok so as you can see from the output of cmake:\n-- The ctags found is not suitable for meta examples.\n-- Could NOT find Ctags (missing: CTAGS_EXECUTABLE)\n-- The ctags found is not suitable for meta examples.\n-- Could NOT find Ctags (missing: CTAGS_EXECUTABLE)\nCMake Error at examples/meta/CMakeLists.txt:11 (message):\n Ctags required for meta examples.  Install or set BUILD_META_EXAMPLES=OFF.\n\nnamely if you use osx you need to have ctags installed (one way is to install it via brew: brew install ctags). or of course you can skip the building of the meta examples by running cmake:\ncmake BUILD_META_EXAMPLES=OFF ... @Jiewen2017 i guess you are doing something similar or: http://shogun.ml/install#manual-windows ?. closing this one; anybody attempts to use shogun on windows should try it with using conda package starting 6.1.3 there's a binary windows package available. @ealtamir great! although i think we should have the ticket open to fix it with an assertation error instead of a segmentation fault ;). we are trying to use this if possible https://software.intel.com/en-us/articles/a-new-linking-model-single-dynamic-library-mkl_rt-since-intel-mkl-103. btw this is only when ENABLE_EIGEN_LAPACK is enabled. @vermashresth yes go ahead. take a look how it is done in case of MKL. @vermashresth noup that's not a good idea as you need to consider the case when it's an atlas version of LAPACK/BLAS.... basically you need to figure it out that it's openblas based LAPACK/BLAS and if that's the case then you need to do a search for cblas.h. @vermashresth mmm it's not necessary to create a FindOpenBLAS file to detect the BLAS/LAPACK type.. of course you can do that as well... it's up to you. only thing is that you should be able to figure out whether the detected LAPACK API is openblas... and in case it is then it should be able to find the cblas.h or error if there's no such header available..... how do you mean that the path name in case of openblas is not always containing openblas keyword? can you show me a distro/installation where this is the case? . first of all looking into blas is foobar, as we only detect LAPACK... i know that blas is a dependency of LAPACK in this matter but we do not do BLAS detection directly.\nsecond of all if you check this: https://github.com/xianyi/OpenBLAS/wiki/OpenBLAS-Extensions\nyou realise that there are some openblas specific functions that could be used to detect whether the detected library is openblas, or something else.. people, there is a good reason why we have releases of the shogun-data as well, see for example:\nhttp://shogun.ml/archives/shogun/data/shogun-data-0.11.tar.bz2. @karlnapf dot product is done ;) and already merged into develop :P. I have just monitored the unit test changes in runtime. I havent benchmarked it properly. The mkl is definitely has a positive impact on the runtime (again i have only confirmed this by looking at unit tests runtimes)\n\nOn 4 Dec 2017, at 19:16, Heiko Strathmann notifications@github.com wrote:\nare there benchmarks on the impact of this?\nEspecially with MKL?\nsklearn says on their website that the SVM solvers dont use MKL so no speedup....we could shine once again :)\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub, or mute the thread.\n. @orivej thnx for reporting and writing a possible fix. could you PR this plz?\n@sorig are you happy with the fix?. just merged #4008 that fixes this. @karlnapf this is a duplicate of #3996 closing... as said this is great but this is a general git related thing so it shoulnd't be part of our manual that should be as simple as possible and concentrate on things that are related to the library itself. @krc3004 please read about the shogun formula a bit in homebrew, i.e. brew info shogun, you will see that there are many options for the formula that are turned off by default... one of which is --with-r that will build the R interface for you.. great! lets see what CIs say :). @sorig indeed... just restarted the ruby job, should run quickly, if it's all good i'll merge! :)\nthnx both @sorig and @orivej to get this fixed!. great, all's good! merging!. @durovo yes more or less that's the idea. as soon as you fix those the travis builds should be green. the reason they fail now is because you miss SG_ADD. @durovo its fine to do the openmp part later... the point was rather that you should do it in other part of the code as well as use linalg:: when possible.. @durovo make sure that the next time you submit that the unit tests are passing, see:\n53 - unit-CrossValidatedCalibrationTest (SEGFAULT)\n\non travis CI. :O wonder what has happened... i think it's about the python2->3 switch :(. thnx for the patch but, ctags is not a python package, hence this requirements will always fail. @salonirk11 yes, certainly. make sure that before you send in a PR both all the examples and tests are passing on your machine locally.. @lisitsyn the reason for the compilation slowdown is because you've changed a .h that basically being used everywhere hence the ccache-ed objects cannot be used... hence travis needs to compile everything from scratch... see in case of appveyor the compilation time is always 40+ mins ;). @Sahil333 any update on this? . @lisitsyn great that you've tracked down this one... only thing: dont' we wanna switch to static linking of the modular interfaces with libshogun eventually? would that actually fix this buck per se?. @xristos91 just put the v4.0.0.tar.gz under third_party/rxcpp. @salonirk11 sure feel free! go ahead and open a PR :). @Red-devilz yeah indeed the idea is that we somehow separate automated changes - see v3->v4 - from actual code change, i.e. bug fixes. fixed with b4e9249003d1ba1320ec726b9e133489b46833da. @xristos91 can you try the same thing but using the feature/meta_example_r_fix branch... there are many R related fixes there.. @shubham808 how is it going with the PR? could you rebase it over the latest develop?. @karlnapf whatever is fine... just not the current state of omp. this needs rebase :S. @guruhegde thanks for the patch. i skid through the patch looks good... i've restarted the failed travis jobs, lets see the output and based on that we can merge it \ud83d\udc4d . @guruhegde as you can see there some formatting issues in the patch:\nhttps://travis-ci.org/shogun-toolbox/shogun/jobs/332627900#L740\nyou can use clang-format locally and check all the errors before sending in a fix.\nsee https://github.com/shogun-toolbox/shogun/wiki/Code-style for more details. thnx for the fix!. https://travis-ci.org/shogun-toolbox/shogun/jobs/334467279#L745\n:(. I reckon a .i include missing in our swig descriptors for typemapping std::set\n\nOn 29 Jan 2018, at 19:17, Esben S\u00f8rig notifications@github.com wrote:\n@lisitsyn Here's an issue related to the SWIG interfaces and the parameter framework. In python, I can't see what the parameters of an object is:\nIn [1]: from shogun import GaussianKernel\nIn [2]: k = GaussianKernel()\nIn [3]: k.parameter_names()\nOut[3]:  *' at 0x11408e458>\nWhich means I have to dig around in C++ code to find the parameter names. Can this be fixed?\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub, or mute the thread.\n. @sorig i've just tried to do that what i said above.... it's not working... i'll let u know if i have found a solution. @sorig ok i've managed to do this:\n\n```\n\n\n\nimport shogun\nk = shogun.GaussianKernel()\nk.parameter_names()\n('cache_size', 'combined_kernel_weight', 'lhs', 'lhs_equals_rhs', 'log_width', 'm_distance', 'm_precomputed_distance', 'normalizer', 'num_lhs', 'num_rhs', 'opt_type', 'optimization_initialized', 'properties', 'rhs')\n```\n\n\n\ni guess this is what you would like to have right? :). @sorig lemme push it to develop. 3cc0e3740c5de1e0f31aebbb427615d8ef0e4d07 is the patch that gets u the above mentioned functionality. we have actually a problem: std::set isn't supported in the following interfaces: lua, java, set, r, octave, c#.\nin other words wrapper for std::set is only available in case of python, ruby, in the rest of the case the api will not be available... which is far from optimal so we should actually return our own CSet :D yaaay for custom things :(. @sorig if we wanna switch we should then switch the interface to use std::vector<std::string> as that's the only typemap that is available in all of the languages that we support (std::list is not available in c# and lua). @lisitsyn what do you think? custom CSet<std::string> or std::vector<std::string> as return value for parameter_names()?. @lisitsyn do you have any clue why\n%include \"std_vector.i\"\nis surrounded by #if !defined(SWIGJAVA) ?\ni.e. why no java typemaps for \n%template(IntStdVector) vector<int32_t>;\n    %template(DoubleStdVector) vector<float64_t>; \n?. commit 5423ead4de9a0304087112c027b7795fb0c0fa14\nAuthor: Christian Widmer cwidmer@tuebingen.mpg.de\nDate:   Fri Jun 15 17:29:53 2012 +0200\nincluded std_vector typemap while avoiding java compile error\n\none would hope that there's no more compiler error, or? :D. @sorig 532a8d45770e4e82ba8291c35f3e0985beec1930 should be the thing for you :). @syashakash thnx for the patch but there needs to be way more improvement to get the setup.py working.... . @syashakash this needs to be rewritten from scratch, this was just an attempt that was never working. . totally. @lambday appveyor, i.e. msvc fails with: https://ci.appveyor.com/project/vigsterkr/shogun/build/1818#L16654\nThe following tests FAILED:\n    245 - unit-KernelSelectionMaxMMD (Failed)\n    247 - unit-KernelSelectionMaxCrossValidation (Failed)\nmore details:\nNote: Google Test filter = KernelSelectionMaxMMD.*\n[==========] Running 4 tests from 1 test case.\n[----------] Global test environment set-up.\n[----------] 4 tests from KernelSelectionMaxMMD\n[ RUN      ] KernelSelectionMaxMMD.linear_time_single_kernel_streaming\n[       OK ] KernelSelectionMaxMMD.linear_time_single_kernel_streaming (8 ms)\n[ RUN      ] KernelSelectionMaxMMD.quadratic_time_single_kernel_dense\n[       OK ] KernelSelectionMaxMMD.quadratic_time_single_kernel_dense (2 ms)\n[ RUN      ] KernelSelectionMaxMMD.quadratic_time_single_kernel_dense_unequal_train_test_ratio\nC:\\projects\\shogun\\tests\\unit\\statistical_testing\\KernelSelection_unittest.cc(143): error: The difference between selected_kernel->get_width() and 0.25 is 0.1875, which exceeds 1E-10, where\nselected_kernel->get_width() evaluates to 0.0625,\n0.25 evaluates to 0.25, and\n1E-10 evaluates to 1e-10.\n[  FAILED  ] KernelSelectionMaxMMD.quadratic_time_single_kernel_dense_unequal_train_test_ratio (2 ms)\n[ RUN      ] KernelSelectionMaxMMD.linear_time_weighted_kernel_streaming\n1.000000 \ne 1.892112\n[       OK ] KernelSelectionMaxMMD.linear_time_weighted_kernel_streaming (8 ms)\n[----------] 4 tests from KernelSelectionMaxMMD (20 ms total)\n[----------] Global test environment tear-down\n[==========] 4 tests from 1 test case ran. (22 ms total)\n[  PASSED  ] 3 tests.\n[  FAILED  ] 1 test, listed below:\n[  FAILED  ] KernelSelectionMaxMMD.quadratic_time_single_kernel_dense_unequal_train_test_ratio\n 1 FAILED TEST\n        Start 246: unit-KernelSelectionMaxTestPower\n246/369 Test #246: unit-KernelSelectionMaxTestPower ................................   Passed    0.03 sec\n        Start 247: unit-KernelSelectionMaxCrossValidation\n247/369 Test #247: unit-KernelSelectionMaxCrossValidation ..........................***Failed    0.05 sec\nNote: Google Test filter = KernelSelectionMaxCrossValidation.*\n[==========] Running 2 tests from 1 test case.\n[----------] Global test environment set-up.\n[----------] 2 tests from KernelSelectionMaxCrossValidation\n[ RUN      ] KernelSelectionMaxCrossValidation.quadratic_time_single_kernel_dense\nC:\\projects\\shogun\\tests\\unit\\statistical_testing\\KernelSelection_unittest.cc(327): error: The difference between selected_kernel->get_width() and 0.25 is 0.21875, which exceeds 1E-10, where\nselected_kernel->get_width() evaluates to 0.031250000000000014,\n0.25 evaluates to 0.25, and\n1E-10 evaluates to 1e-10.\n[  FAILED  ] KernelSelectionMaxCrossValidation.quadratic_time_single_kernel_dense (3 ms)\n[ RUN      ] KernelSelectionMaxCrossValidation.linear_time_single_kernel_dense\n[       OK ] KernelSelectionMaxCrossValidation.linear_time_single_kernel_dense (26 ms)\n[----------] 2 tests from KernelSelectionMaxCrossValidation (29 ms total)\n[----------] Global test environment tear-down\n[==========] 2 tests from 1 test case ran. (31 ms total)\n[  PASSED  ] 1 test.\n[  FAILED  ] 1 test, listed below:\n[  FAILED  ] KernelSelectionMaxCrossValidation. @yashdusing thxn for the PR, try to create your patch on top of HEAD of develop please. @yashdusing you dont need to. just get your own branch yashdusing:develop in shape. this will get synced to your changes. you should get your own branch in shape... no need for a new pr. for further such question please consult google and search for github manuals. still an issue... the above fix is still not good enough, see:\nhttps://jenkins.brew.sh/job/Homebrew%20Core%20Pull%20Requests/17517/version=sierra/console. as you can see from the ppa's description it only has packages for trusty and xenial not for zesty. we'll add support for the next LTS (18.04). @bmurauer thnx for reporting! which version are you using?. @bmurauer just checkd with python 2.7 + shogun and i'm getting a nicer\n```\n\nNotImplementedError                       Traceback (most recent call last)\n in ()\n     19 x_train, y_train, x_test, y_test = train_test_split(x, y)\n     20\n---> 21 features_train = StringCharFeatures(x_train, RAWBYTE)\n     22 features_test = StringCharFeatures(x_test, RAWBYTE)\n     23\nNotImplementedError: Wrong number or type of arguments for overloaded function 'new_StringCharFeatures'.\n  Possible C/C++ prototypes are:\n    shogun::CStringFeatures< char >::CStringFeatures()\n    shogun::CStringFeatures< char >::CStringFeatures(shogun::EAlphabet)\n    shogun::CStringFeatures< char >::CStringFeatures(shogun::SGStringList< char >,shogun::EAlphabet)\n    shogun::CStringFeatures< char >::CStringFeatures(shogun::SGStringList< char >,shogun::CAlphabet )\n    shogun::CStringFeatures< char >::CStringFeatures(shogun::CAlphabet )\n    shogun::CStringFeatures< char >::CStringFeatures(shogun::CStringFeatures< char > const &)\n    shogun::CStringFeatures< char >::CStringFeatures(shogun::CFile ,shogun::EAlphabet)\n    shogun::CStringFeatures< char >::CStringFeatures(shogun::CFile )\n```\nlemme check why do you have a sigsegv with py36 :(\nstill of course this doesn't solve your problem :(. @bmurauer ok so i've managed to fix the problem in py27... currently the kernel blew up my memory :)\ni'll try to test it with a smaller dataset to first see what could be the problem then i'll check what's the problem on py3. btw for the record there's a bug in your code:\nx_train, y_train, x_test, y_test = train_test_split(x, y)\nshould be:\nx_train, x_test, y_train, y_test = train_test_split(x, y). ok i've managed to reproduce the error finally... lemme see if i can get a fix for it today. ok so it's basically because of this in the typemaps:\nPyBytes_AsString(PyUnicode_AsASCIIString(const_cast<PyObject*>(o)));\none of the strings fails with PyUnicode_AsASCIIString and since there's not return value check (sic!) \nthe PyBytes_AsString(NULL) will cause a SIGSEGV :) i think we should just simply convert it to UTF8 string and use that as feature. :). @bmurauer feature/numpy1.7 branch should have a fix for your problem.. i'll see if travis CI jobs passes in case yes i'll merge this into develop. if you could test is - as i haven't tested yet the python3 version - i would really appreciate it.. ps: the plz don't try with on the whole dataset as that'll just take all your ram in the machine :). @bmurauer sorry about it i was coding blindly as i really haven't tested it on py3. i'll do it now and force push. once tested/done i'll ping u here :) . @bmurauer ok now the HEAD of the feature branch (note i've force pushed) contains the fix that worked for me with python35. lemme know how does it work for you.. @bmurauer cool thnx for testing! let's keep the issue open until i merge the feature branch into develop :). fix merged, we can close. great stuff! although LMNN some parts could get some openmp stuff but that could be the next pr. i've just restarted the travis jobs to see whether that was a hiccup or there's an actual bug now :(. doh.... that's definitely that :((( see the output of our buildbot\nhttp://buildbot.shogun-toolbox.org:8080/#/console\n@karlnapf @lisitsyn d1763b83fbc029bae1487201ef56977efdfe43a0 broke builds. @syashakash could you rebase this with the latest develop as the integration test is back.. @naoa thnx for the report and the example code, we'll look into it!. @bmurauer thnx for the bug report, indeed bug is coming from somewhere there :) it's a bit painful to look at that code, lemme try if it's easy to fix.. @bmurauer ok so a very quick fix is that you change the labeling to 0 base \ud83d\udc83 \nbut yeah this shoudl work with any arbitrary labeling... i'm working on it but that'll take a bit longer to get in.. @krishnaw14  could you give details about your setup? like the version of the python you are using?. @krishnaw14 if you just wanna use shogun there's a homebrew package available that has the latest stable version of shogun (6.1.3). it's weird that your python does not have PyUnicode_AsUTF8AndSize but maybe just an include missing somewhere. have you installed that python via brew or?. mmm i've just realised what's the problem... i'll need 10 minutes to see if i can get a fix for it. @krishnaw14 i've just pushed the fix to develop... so you should pull the latest changes from git and it shoudl work. @naoa thnx for the great bug description. unfortunately neither SparseFeatures nor  CRandomFourierDotFeatures (#4169 that you reported as well) has support for subsets; that's why the error. of course it should be a none critical error;. as the error suggest you either checkout the GPL code (git submodule update --init) or simply disable to compilation of the GPL part with -DLICENSE_GPL_SHOGUN=OFF.\nplease next time read more carefully the error message before firing an issue. @goldturtle in case you just want to use shogun out of box you can simply use the conda-forge package or the ppa from here: https://launchpad.net/~shogun-toolbox/+archive/ubuntu/stable\notherwise i'll try to reproduce this error but based on our buildbot 16.04 build should be working:\nhttp://buildbot.shogun-toolbox.org:8080/#/builders/16. Do you have somewhere else shogun already installed?\n\nOn 20 Feb 2018, at 12:18, goldturtle notifications@github.com wrote:\nLooks like this happened to other people too:\n2841\nReading the last comment, though, I wouldn't know how to change the include path in a proper way.\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub, or mute the thread.\n. oh we never tried C# on windows. would be great to get it working. as far as i can see our .NET cmake script is waaaaay too old. lemme look into it a bit.. @armgong thnx for the report and sorry for the late reply... i'm currently getting this sorted. note that no need to replace the SDK version as you can just specify via '-DCSHARP_DOTNET_VERSION=' the version of the .net you wanna use. @yyanwcy521 if you just want to use shogun from python why dont you use either the ppa package or the conda package?. @yyanwcy521 on the other hand can you paste the content of the build/src/shogun/lib/versionstring.h file here . If u need to have new mem of course u need a new instance but thats not always the case. Hence have both of them and the compiler will figure it out for u\nOn 7 Mar 2018, at 17:22, Heiko Strathmann notifications@github.com wrote:\n@karlnapf commented on this pull request.\nIn src/shogun/labels/BinaryLabels.cpp:\n\nusing namespace shogun;\n\nCBinaryLabels::CBinaryLabels() : CDenseLabels()\n {\n }\n+CBinaryLabels::CBinaryLabels(const CBinaryLabels& orig) : CDenseLabels(orig)\nFor converting the same datastruct, I see the point\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub, or mute the thread.\n. Btw now thinking of this i\u2019m not so sure we need this things explicit... as it makes the whiole thing less smooth.... afaik you could have the whole thing implicit with the right ctors\nOn 7 Mar 2018, at 17:22, Heiko Strathmann notifications@github.com wrote:\n@karlnapf commented on this pull request.\nIn src/shogun/labels/BinaryLabels.cpp:\n\nusing namespace shogun;\n\nCBinaryLabels::CBinaryLabels() : CDenseLabels()\n {\n }\n+CBinaryLabels::CBinaryLabels(const CBinaryLabels& orig) : CDenseLabels(orig)\nFor converting the same datastruct, I see the point\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub, or mute the thread.\n. lets try to make the CIs pass as this anyways is going to be a huge problem with GSoC. i've tried dropping some heavy compilation tasks (serialization) but seems it is still not enough. imo that's not going to work as that's a global counter and they wont change that unless you actually start paying for their services.... and since we anyways still would have problems with appveyor i'd rather try to lower our compilation time. that's any easy fix just remove from the all target... that's a good question whether that actually would solve the compilation time problem. but it's worth a shot. but that means that you should remove the libshogun examples from ctest as well... as they won't be built by default. what's your cmake version?\ncould you copy-paste src/shogun/lib/versionstring.h content? it's in the build directory.... could you pull as this is a develop branch from beginning of 3rd of February and test it again, please. @geektoni so i'm guessing the reason it is currently not doing anything is because of this:\nhttps://github.com/shogun-toolbox/shogun/blob/develop/src/shogun/lib/Signal.cpp#L17\n\nsince in the handler:\nhttps://github.com/shogun-toolbox/shogun/blob/develop/src/shogun/lib/Signal.cpp#L36\ndo you know why it is by default turned off? :). @geektoni i would rather have it by default active as currently if you run a model and wanna cancel it the only way to do is to kill the process :) which is not the most user friendly behaviour :P. @FaroukY could you paste here the output of this command:\nctest -R integration_meta_cpp-clustering-gaussian_mixture_models -VV. oh this is rather interesting :)\ncould you maybe paste here the output of your cmake ?\njust create another directory (instead of build) and run cmake the same way the way you did before and paste here the full output... i guess you are running this on x86 arch right ? :). mmm i can get the very same problem with integration_meta_cpp-clustering-gaussian_mixture_models on freebsd where there's not lapack/blas backend. i'm thinking that actually maybe we are currently having some very different results from eigen for the eigenvectors/values when it's not having a LAPACK backend.\nstill would be great if you could get us your cmake so i can confirm this, i'll look into the above mentioned heuristics. yes yes it's options and it seems this is a bug.. @FaroukY @vinx13 @shubham808 these errors are actually blockers for the new release. the only way to reproduce this is actually to disable the LAPACK backend to be used by eigen (ENABLE_EIGEN_LAPACK) and compile shogun with it... it'd be great if you could somehow figure out why this happens and help in fixing these tests. @FaroukY as the cookbooks are actually automatically used as integration tests y, ou should add the generated integration tests files to the https://github.com/shogun-toolbox/shogun-data repository.\nsee details here: https://github.com/shogun-toolbox/shogun/blob/develop/doc/readme/DEVELOPING.md#integration-testing-of-results. @FaroukY regarding the data submodule you have to do the same thing as with the shogun repo. basically fork it prepare a patch and create a pull request. try to rebase now and tie the data submodule to the HEAD of master branch... that should make the unit tests pass. there's an integration error namely:\n```\n[DEBUG] Own parameter Serializable::value=0.46875 different from provided Serializable::value=1\n[DEBUG] Own parameter DynamicObjectArray::array= different from provided DynamicObjectArray::array=\n[DEBUG] For details, run: diff generated_results/cpp/evaluation/cross_validation.dat reference_results/evaluation/cross_validation.dat\n```\nplz fix this asap. @FaroukY you should reference the new data commit so that actually things are being checked against the right integration dat file. note i've merged your PR into the master of shogun-data. i wonder why does octave fail in this case :(. only happens in case of octave:\n[DEBUG] Own parameter Serializable::value=23.5918 different from provided Serializable::value=7.89437. @FaroukY ? this is taking unreasonably long to get it merged. closing as this has been fixed. @FaroukY yeah i think you forked your branch feature_fix_lib_linear_doc not from develop but from the other fix' branch. just checkout develop pull the latest changes and then simply cherry pick 505f6abd9c17a66a0cbebfdffc8a0b93f1e13003 into that and forcepush it to your remote feature_fix_lib_linear_doc, this way you dont even need to create a new PR ;). @FaroukY ok in order to be able to merge this you need to remove the unrelated commits in this PR. yeah unfortunately currently our clang/gcc jobs are timeouting... i'll try to check on it but as @geektoni said it shouldn't be related to your patch.. @grig-guz could you rebase your branch so we can resolve the conflict? thnx!. @grig-guz mmm seems there's still some problem... in general you should not merge develop into your branch but rather rebase it.... please read about git in general here https://git-scm.com/doc. mmm unfortunately not... it contains commits from the develop that is done by me. you need to create a pull request that only contains commits that are your patches only. @vinx13 great! thnx for checking this! so yeah then our unit test for GMM is not invariant for such case... which it should be as they are actually the same eigenvector. @karlnapf unit test? oh yeah mistyping... meant integration... and the whole solution is fsck-ed there... so that integration test with different EVs ends up having all different params... . different EVs = opposite direction. ---- i.e. dontlike. fyi here's the generated output when LAPACK isn't available\n<<_SHOGUN_SERIALIZABLE_ASCII_FILE_V_00_>>\narray Vector<SGSerializable*> 7 ({Serializable int32 [\nvalue int32 3\n]}{VectorSerializable float64 [\nvalue SGVector<float64> 2 ({-2.135073443612682}{3.141044590471833})\n]}{Serializable int32 [\nvalue int32 1\n]}{VectorSerializable float64 [\nvalue SGVector<float64> 2 ({0.7827734860786125}{-1.76419104479616})\n]}{MatrixSerializable float64 [\nvalue SGMatrix<float64> 2 2 ({3.392643915170048}{0.6720138121871488}{0.6720138121871488}{1.244541368311431})\n]}{VectorSerializable float64 [\nvalue SGVector<float64> 3 ({0.1969275849684337}{0.533330685448861}{0.2697417295827056})\n]}{VectorSerializable float64 [\nvalue SGVector<float64> 4 ({-16.95532013642032}{-3.130118784821983}{-31.06361716282668}{-3.130117794465724})\n]})\nresize_granularity int32 128\nuse_sg_malloc bool t\nfree_array bool t\ndim1_size int32 1\ndim2_size int32 1\ndim3_size int32 1. @karlnapf even more so ----. in this case imo this shows how some of our integration tests are actually not the best. :). same story comes up with the randoms. great! there are some minor improvements needed. @FaroukY lemme check why do the CIs fail. @FaroukY does this work for you locally? maybe because boost is in your include paths? as you can see from here this does not compile:\nhttps://travis-ci.org/shogun-toolbox/shogun/jobs/359680795#L1970\nstan misses boost... lemme try to pull these commits and see locally what's happening and get back to you. indeed the problem is that - i've just tested this with a machine that has boost installed and it worked fine... i'll test and try to come up with a fix in a clean docker container. @FaroukY i've use the same docker image that is being used for travis CI jobs: shogun/shogun-dev. @FaroukY basically the problems are the following:\n 1) you are using a system where you already have libboost installed (probably under /usr/include...)\n 2) you have not tested the cmake script with the given GIT_TAG (the boost versions are different)\n 3) the way you handle STAN_INCLUDE_DIR*... those are not going to automagically install themselves under include/shogun/..... you need to take care of that.. @FaroukY any update?. @FaroukY there's still a major part missing: installation of the Stan headers in the right folder. the tests are working because you rely on the fact that everything is available in build time. but once you ship this, the stan related directories are not going to be available as you dont copy them with make install. @FaroukY ok so when you do make install do you have files under /usr/local/include/shogun/lib/external/Stan ? because i doubt so... as there's nothing instructing cmake to install those files anywhere.. @FaroukY yeah first we need a rebase over develop and then fix those things i've mentioned i'll think about how to fix travis limitations.. @FaroukY the segfault is unrelated, there's currently a bug in develop branch, so we can ignore that. @FaroukY please check/fix the errors that clang-format reports, see: https://travis-ci.org/shogun-toolbox/shogun/jobs/375012262#L722. @FaroukY perfect! merging!. @karlnapf :D\n@fyoda i cannot open any of the linked gists... but as you can see on appveyor MSVC 2015 + shogun with bundled eigen just works fine... so unless you can give me some logs i'll close this bug as this seems to be your error. @fyoda i'm closing this one as this is really some local setup problem.... in case you think that it's not then plz post the full cmake output not just a part of it.. @fyoda oh! x86 build was never tested... hence it was never intended to work. buuuuuut even in that case this seems to be an eigen related problem actually, not shogun. @fyoda if you jump in gitter or irc we could get your problems fixed quicker :). looks better... let's see what the CI says. @grig-guz great! there are still some formatting errors. you can fix those by using clang-format; for details check the 8th point in https://github.com/shogun-toolbox/shogun/blob/develop/doc/readme/DEVELOPING.md. @grig-guz i'm not so sure what you mean by the cropped image but this is clearly a formatting problem:\nhttps://travis-ci.org/shogun-toolbox/shogun/jobs/359204608#L760\nor\nresult=1.0-CStatistics::normal_cdf(statistic, std_dev);\nshould be\nresult = 1.0 - CStatistics::normal_cdf(statistic, std_dev);. @fyoda you dont need to clone vcpkg just do: vcpkg install shogun. @fyoda could you actually provide the full version of your msvc?. @fyoda and if for some strange reason vcpkg does not work you can still opt to user the windows binary packages in conda-forge. @fyoda could you actually post the linker errors as that seems to be pretty strange...  since you the python module (shogun package in conda) itself is linked to that .lib... so i guess you are missing some linker flags.\non the other hand i'm not so sure what command yields you to the output of:\n[657/659] cmd.exe. mmm you are building an octave interface? :)\nor this is an error of nlopt that for some reason is trying to build something with octave?. @fyoda this is a vcpkg issue unfortunately :(. i see you've already sent in an issue, let's have this one track it: https://github.com/Microsoft/vcpkg/issues/3207. @vchen30 appreciate it!. @ypic honestly i have never used shogun from eclipse, i can provide you a CLI for how to use shogun from JVM. ok so here's an example from our buildbot:\n/usr/bin/java \"-Xmx1024m\" \"-cp\" \"/usr/share/java/jblas.jar:/home/buildslave/deb3_-_interfaces/build/build/src/interfaces/java/shogun.jar:/home/buildslave/deb3_-_interfaces/build/build/examples/meta/java:/home/buildslave/deb3_-_interfaces/build/build/examples/meta/java/binary\" \"-Djava.library.path=/home/buildslave/deb3_-_interfaces/build/build/src/interfaces/java\" \"averaged_perceptron\"\nbasically you need to have the shogun.jar and jblas.jar in the classpath and have the libshogun.dylib (or .so if you are under linux) in java.library.path. you can actually test this yourself whether it works or not if you enable BUILD_META_EXAMPLES cmake flag. yeah but in case you use it as a plugin, then it never violates the gpl.. so, in this particular case if src/shogun/lib/Compressor.cpp would be written in a modular fashion and the lzo decoder would be a plugin then all this can be ignored... for the time being i'd rather suggest to drop LZO as a whole.... as anyways this should not effect anything in the base library  - once modular.. and i dont really see anybody complaining of not being able to decompress string features that are lzo compressed... and because of this doxygen comment:\nNote that besides lzo compression, this library is thread safe.. so on the end only GLPK would need this if we call dynamic linking a derivative work, but as soon as there's plugin support this would need to be reverted..... @fyoda i'm not so sure how do you end up here.... could you maaaybe reproduce the commands of appveyor or what's in the manual for windows, namely:\n```\ncmake -G\"Visual Studio 14 2015 Win64\" -DCMAKE_BUILD_TYPE=Debug -DBUILD_META_EXAMPLES=OFF -DENABLE_TESTING=ON ..\nmsbuild \"C:\\projects\\shogun\\build\\shogun.sln\" /verbosity:minimal /t:Clean /p:Configuration=Debug /p:Platform=x64\n. @fyoda what? :) i mean if you do not want GPL sure thing that you can disable it... but that work as it works on appveyor. you are missing the submodules:\ngit submodule update --init\n``. @fyoda i'm not so sure that DLL should be linkable to a simple example - like the minimal code. for that you should compile with the static lib... if you want to use the dll you need to load it into memory withLoadLibrary`; but correct me if i'm wrong. it's not the same as in case of *NIX systems shared libraries..... @fyoda modifying direct.h? you mean dirent.h? none the less you have to actually use the right include flags for shogun (check exported target) before you wanna compile against the lib.... otherwise of course you are going have troubles compiling your code.\non the other hand i'd actually link with the static lib, which i dont get why wouldn't has any of those symbols in case of a Debug mode.... that's only adds debug information to the obj not removing functions. are you sure that you are having the right linker flags to find the debug mode library?. there's no such thing as shogun.a in windows... it's shogun.lib. @fyoda but the libshogun (c++ examples) compiles fine?. can you run the tests? meaning ctest? because those errors are just warning you that no BLAS/LAPACK is found, but that should still work - eigen does not require to have lapack/blas, it's optional... mmm it's not a solution.. .it's part of cmake.... as you have ran cmake with -DENABLE_TESTING=ON that will enable tests including unit tests and some other c++ examples.. please read this before further questions:\nhttps://github.com/shogun-toolbox/shogun/blob/develop/doc/readme/DEVELOPING.md\n. @fyoda if unit tests are passing that it definitely managed to link with shogun.lib.\nBUILD_EXAMPLES cmake flag is by default turned on that means all the libshogun examples ought to be compiled&linked as well, i.e. if you run ctest -R libshogun you should have some test that pass.. @fyoda submodules? i'm not so sure if i follow... moreover, if the libshogun examples are successfully compiled and linked then it means that your custom linking/compilation is broken... \nmeaning if you compile it with:\n```\ncmake -G\"Visual Studio 14 2015 Win64\" -DCMAKE_BUILD_TYPE=Debug -DBUILD_META_EXAMPLES=OFF -DENABLE_TESTING=ON ..\nmsbuild \"C:\\projects\\shogun\\build\\shogun.sln\" /verbosity:minimal /t:Clean /p:Configuration=Debug /p:Platform=x64\n```\nor\n```\ncmake -G\"Visual Studio 14 2015 Win64\" -DCMAKE_BUILD_TYPE=Release -DBUILD_META_EXAMPLES=OFF -DENABLE_TESTING=ON ..\nmsbuild \"C:\\projects\\shogun\\build\\shogun.sln\" /verbosity:minimal /t:Clean /p:Configuration=Release /p:Platform=x64\n```\nif you are referring to the fact that when you run once in the cmake say with -DCMAKE_BUILD_TYPE=Release and then switch in your MSVC application the type to Debug (or vice versa) then i think this is not fully supported by cmake generated solution, but i might be wrong. anyhow this is strictly cmake related issue, nothing to do with shogun. in case the above mentioned commands separately work fine (meaning that you run cmake in a clean build directory). mmmm i have to check with dirent.h but io.h won't work... it needs maaaybe some fixing.... lemme make an issue about that.\ni'm not so sure if i understand why would this happen - Debug not working in case of MSVC & shogun. i'm using cmake to generate the solution files, so i dont understand what's missing or how. ideas?. @fyoda have you maybe checked whether adding\n```\npragma comment(lib, \"winmm\")\n```\nto https://github.com/shogun-toolbox/shogun/blob/develop/src/shogun/lib/Time.cpp#L92. if you could add & test that'd be really great thnx!. @fyoda actually i've just checked, MSDirent is being installed to shogun/lib/external/MSDirent so there should be no problem with dirent.h. . can you actually paste me a compile command and the corresponding error when you observe problems with dirent.h?\n. @fyoda ok so based on this i dont see any of the compiler flags that are being exported to ShogunTarget.cmake. how do you add shogun to your project?. @fyoda yeah unfortunately that's not enough :) you use qmake? in case of cmake i could tell you how to use shogun, but i'm not so sure yet how to that with Qt build framework. @fyoda yes that's the problem,... but that's actually specified in the target cmake, i.e. ShogunTarget.cmake if you'd include shogun like that into your project.. @fyoda yes that was the idea..... thxn!. although it always fails on zesty and yakkety builds: http://buildbot.shogun-toolbox.org:8080/#/console. formatting issues :)\nhttps://travis-ci.org/shogun-toolbox/shogun/jobs/369770930#L736. @fyoda indeed this was an error in the previous release... now it's fixed in develop, basically any third party library will be available under shogun/third_party folder. for the time being you either start using the latest version of shogun or add the eigen headers under shogun/lib/external/eigen manually. @fyoda you should be able to use eigen 3.3.4 and put there manually.. .note that you should use -isystem gcc/clang flags when you compile shogun. btw which header includes #include <shogun/mathematics/eigen3.h>?. @fyoda oh so you are having this error while you are building shogun? i thought when you are using it. if it's build time problem, then there's a bug indeed. i'm just checking the headers and none of the external headers include the mentioned eigen3.h so you shouldn't have this problem while using shogun externally. but of course the build problem (when you try to build the library) should still be fixed... i remember that you were having some custom build mechanism. note the generated MSVC 2015 works by default - see our appveyor builds.. @fyoda can you provide me the compiler flags for the shogun.sln?. ok so basically this:\nC:\\Users\\myUser\\Downloads\\shogun\\build\\src\\shogun\\lib\\external\\eigen\nshould make sure that eigen is available. that dir is being created by cmake and by adding eigen to your project. check if eigen is actually there.. mmm that's weird as this should be autogenerated by cmake. are you sure you are checking it under build dir and not the root of the repo?. for the sake of argument, could you create a new build directory, say build-test and run cmake and try to build shogun.sln. mmm that is super interesting.... so then i guess this is a bug - that i couldn't reproduce yet... could you put there (C:\\Users\\myUser\\Downloads\\shogun\\build\\src\\shogun\\lib\\external\\eigen) manually the eigen 3.3.4? it should fix it.. doh... i dont really get why it cannot find unsupported/Eigen/MatrixFunctions. @fyoda this should be in C:\\Users\\myUser\\Downloads\\shogun\\build\\src\\shogun\\lib\\external\\eigen. mmm i'm not so sure where the linking error is coming from :(. @fyoda if you wanna use yourself Eigen for some reason in your code you need to provide the eigen for yourself. meaning: if you just simply use shogun classes you should not have problems of finding eigen, as it's actually only used during compile time of the library...\nok so basically the problem here is mostly about things that got already fixed imo - in develop. i'll test it during the weekend on my windows VM just to make sure, i.e. will leave the ticket open till then. @fyoda have you made sure that the X is column oriented?. @fyoda ok so i've downloaded the test file you've provided and there's a little problem with the file.\nif i do the following:\nauto f_feats = some<CCSVFile>(\"/home/wiking/test_data.txt\");\nf_feats->set_delimiter('\\t');\nauto features = some<CDenseFeatures<float64_t>>(f_feats);\nstd::cout << features->get_num_vectors() <<  \"x\" << features->get_num_features() << std::endl;\nthen i get: 30000x17, which is causes an error... because you have some a trailing tab in that file. after removing the trailing tab (sed -i -e 's/\\t*\\r*$//g' test_data.txt ) i get 30000x16 and the mixing matrix is:\nmatrix=[\n[   459.302725151999823,    324.939757943409973,    -206.930203346325669,   1017.6765139928907, -58.0979393438466403,   -381.568365213744585,   217.523521145939213,    -79.9385590493880329,   -254.004756568155386,   -468.951457589427889,   -64.3714023285542254,   -53.3471353453238137,   -322.769737185549161,   10.4312714615465651,    -284.228486095187293,   456.738708433941497],\n[   -83.1364200154806383,   -117.207455748656329,   55.4977299658408469,    -160.560277786274298,   157.52352419411767, 691.491304900098612,    -252.059091482141184,   -93.5160578296839731,   1515.64395131807714,    25.8788366823451348,    -33.6253372772045438,   -24.3965959312319463,   -83.4267234705427825,   -29.9555487682868318,   96.4996640631641185,    -77.1748872480455219],\n[   -57.8618273939855001,   -548.274854141138462,   86.8585873482053472,    -342.56011744591757,    61.8817110224018663,    589.150773078839165,    -532.574807786675251,   -2.14209966954531339,   -8.41489422277665433,   73.3352412269832996,    -261.425611286262949,   39.8323122908655805,    -89.6964620533470196,   -81.7295113589595275,   29.5481489764944314,    -114.166893069771049],\n[   21.3599520161364964,    -376.75621734141464,    44.7593815775305046,    -753.571166228988659,   72.3280446365675687,    325.230512170213217,    -372.947631512687792,   134.821175529931651,    -56.5330221901809935,   157.190788128249437,    415.647330062130834,    156.464445054564607,    2.70498505152730484,    -64.5503652898074307,   118.703241503753446,    -155.139369059597243],\n[   -67.7192428592761644,   -73.4881324550593718,   138.827651755174799,    -1170.92052373783008,   -8.12911755723775187,   179.161293738259218,    -67.6918570623005138,   91.5063868248935677,    -53.2687111679189087,   147.390703810336134,    26.2933404886696671,    8.28269811298511982,    -11.3214993846664331,   13.9987594822388672,    -17.173399350005802,    -300.558087376719982],\n[   -9.14277339091895236,   -209.826065040396401,   59.6882978033597809,    -865.925170181829685,   -16.9144940772986168,   27.9932639594260273,    22.6413932081600855,    130.023207887449104,    -75.0213363950374941,   342.004751950698278,    149.017486668001851,    149.385985009198976,    28.1396531326203139,    70.8531296148169076,    576.331462035490176,    -335.157055010095007],\n[   -419.864451447823853,   1.24706942740782045,    359.713445008593339,    -715.029493092204689,   53.402540534139753, -36.6069550566170818,   166.832360103551622,    35.6204041819167045,    -94.0391877258035009,   1027.41210680681365,    -41.181370562864366,    -47.7772015703591393,   -28.5450954012937252,   44.2969638211486441,    258.107331881284153,    -829.515024818142365],\n[   -1265.90040965219691,   -100.938912819393025,   59.0262481921335009,    -532.517984150775305,   62.6199802554108018,    -37.722549249178897,    86.8554821389344482,    33.7063295705434101,    -77.0111815834729327,   281.397291131026179,    93.3073070447557029,    25.8584445809246759,    -14.678253768952926,    71.312218418434739, 307.808068547446453,    -190.493231140804994],\n[   127.892914042979541,    -91.4729572491203839,   -95.1657527354946779,   -441.043808798078373,   704.148885021666615,    514.0449544432538,  301.990619308058172,    -215.608844518767995,   -56.0322940335786299,   -23.6555486226941021,   12.8787785942320312,    12.2929957177424214,    29.8207682879462759,    -471.721324279551368,   48.0851979807887346,    -47.843000169100975],\n[   -39.6638240714827077,   -148.887942158667812,   69.4619595937587633,    -355.010119528478697,   81.8847243060490797,    943.665881242620458,    131.518549654620927,    -210.717595896010948,   -29.1166019063875652,   5.24511013893199696,    72.6729236070638365,    -19.560151377895199,    0.920368405880367835,   -219.552899771694513,   4.46575466343028893,    -97.2548002026153995],\n[   -25.2536675741741625,   -145.991621200015544,   133.774756888803637,    -890.538992809660499,   5.61615144957117707,    318.138161514753563,    15.3355774430056222,    -506.017651106639335,   -28.5436078578585324,   47.4097258740880179,    93.8259267261292678,    -13.2086506178009024,   -9.73718674652855753,   -384.999302148472452,   -58.6515390501503191,   -214.33892641222053],\n[   -39.1834784746292257,   84.2488206953238006,    67.4062815242083815,    -646.263140596811581,   90.3569387446241734,    772.106548037443531,    -572.631752941114769,   -196.548126679407261,   -63.5653362382161333,   57.0499513374768554,    54.9738260676114479,    -20.1651174316595601,   -25.2597540619072873,   -192.655243162199071,   120.346469000616551,    -135.229313319615301],\n[   -60.5514695934585845,   -124.93251647799022,    112.266974437984857,    -756.616262963239819,   7.31704133818625557,    497.030382177899753,    0.542294392144153869,   -340.788760733810022,   -29.3840902819780752,   30.154836625350768, 89.4024666752783332,    -2.47779031968064389,   -12.4347016164837392,   -497.586396304999369,   -13.8433110782603439,   -181.758597895030078],\n[   -11.4556752656113741,   -78.1977780245274232,   158.900685058072952,    -996.353532550819864,   34.4765576118060864,    42.4611892763164533,    105.840349282357835,    -229.166733998809264,   -24.6567698338292978,   12.2149735791556537,    -62.9946404796934232,   -1017.78609173103814,   -48.1161218349577879,   -94.8385909551253263,   34.3435032368932553,    -540.156170102262877],\n[   -463.857023858344576,   22.5026860321452311,    386.745104699103535,    -745.43566018694878,    91.0003540422158892,    -42.5684290218087327,   172.47180509174774, -38.1987491821352947,   -46.8425401287464851,   -18.759944622556457,    -37.2207909621812192,   -34.0057094335685193,   -25.1374946578959992,   -7.85391194113015345,   148.39931622923919, -1655.54980841543147],\n[   -938.366692003676235,   30.5495102645555221,    1714.98363114869244,    -622.068451146463076,   145.474108556670615,    -6.15412390933587616,   201.425898227764748,    -71.2183402479302856,   -46.5639953573880874,   -180.742507875424707,   -11.3125995087097326,   -55.5817326808586643,   -27.1750328266777146,   -96.1199363360699266,   108.188351891238341,    -762.448110169664346]\n]. @fyoda could you provide the mixing matrix of the matlab script?. @fyoda note that FastICA has a random initialization step of the mixing matrix, so if you remove the setting the seed of the prng, you would get different mixing matrix each time you run train. say this is a mixing matrix from FastICA on the same data you have provided:\nmatrix=[\n[   -87.2978335039415612,   -201.463456013413605,   339.58493373907902, -71.6063604364714337,   225.571915866636346,    -309.447907624276354,   29.1364932782461281,    -468.417553611608867,   97.8838683645657568,    203.263286986708181,    464.540009327409734,    60.8850041879687964,    -458.782983476663048,   1065.14013548990579,    233.691503896411092,    277.917993346222318],\n[   -73.1984960604591208,   204.631113421825233,    -694.202215752602456,   -19.0473059216718248,   -95.2790619169751523,   -84.5523311352435627,   -4.5860439459642377,    27.0729951642764597,    -128.746126637953239,   -55.5902206489373611,   -85.3444928058000016,   -158.866315231654852,   77.7118135909123851,    -220.822269237200146,   -1511.57356716045865,   -95.5120794712073149],\n[   16.4487643714886218,    419.72456107668728, -606.035842165492681,   53.8983521372015346,    -224.937895760246505,   -104.706896095000801,   -11.2132772563124909,   75.4910228900917701,    -582.986006071040265,   -85.7417075696760236,   -59.9751853043090861,   -46.7670992118446165,   118.307437015745208,    -431.017513388928819,   23.3899839887034702,    -9.35461780229014295],\n[   129.717202799833927,    316.882613844285459,    -309.462033415035648,   194.612444214138122,    -514.862385633695681,   -24.0713453476844101,   8.93748665866728231,    142.826246080562186,    72.6751388743763016,    -44.9596728164457886,   18.0733321402292511,    -85.4863716522489625,   157.326096917194178,    -811.68144058792177,    77.0663573164143969,    -80.9314676023926012],\n[   99.7753944434963103,    126.294369874719621,    -113.29355574297243,    12.4917917342906843,    55.5859861316284807,    -9.21611167362344652,   -99.2205283002264906,   155.12258513744635, 76.0127170797922673,    -138.018857804625753,   -69.8840482197684594,   5.64586616262148588,    302.560725042760794,    -1166.37391029924629,   63.2288930829788356,    4.41364650024145089],\n[   139.019233065779332,    -11.5027563517878288,   13.8723472566533044,    164.070492212886165,    -169.553273654213513,   16.3847155367003978,    45.4269163519248949,    339.089267156691392,    59.0570199560900519,    -59.1814810015897521,   -12.7784527213764765,   6.37214142939990325,    335.665250638675957,    -887.293986051706611,   93.0557018609188731,    -570.051530714842102],\n[   46.5385068692608215,    -105.155543884505619,   87.3975390566480144,    -48.7952136197621087,   94.2011825617821756,    -26.2908512020329042,   -47.2165721538473449,   1033.26619480890167,    74.6790634451353696,    -357.783262937315783,   -426.078133069625267,   -61.5607940817891333,   827.661215569575916,    -694.36351815387286,    101.562868798192738,    -277.593816346441145],\n[   39.1004398163977029,    -64.687830516056394,    67.3215200891403498,    35.5828732713231375,    -60.1075126200665082,   -20.1399302475343305,   41.2774073968330626,    276.843261294547347,    66.6097325143525723,    -48.2139478804430155,   -1268.48590528831551,   -71.2276864803398126,   191.285580470088291,    -540.03336453185068,    88.2548519546780739,    -306.297260946290464],\n[   -227.058302893605941,   -252.096757939489152,   -468.607972255747882,   30.8476102492593078,    -47.7121189614876329,   28.6291814564966671,    -538.231764176717661,   -21.6066129836426661,   -10.4055292191268656,   93.8100377654720035,    126.631289780267451,    -705.849036597945314,   49.6327838234193877,    -442.415426893965389,   59.0765126903661013,    -65.7239253915353032],\n[   -225.523285527431824,   -129.166715995491728,   -918.389647418932782,   -9.12857511522297393,   -89.3912454811458588,   -6.06983590378054227,   -253.389983195448423,   7.19074757532500541,    19.1959864699752636,    -69.062307018039462,    -41.8371237833168266,   -93.1368256579046374,   98.425875033263452, -409.337767612172172,   36.6057147341662414,    -15.0993645693435958],\n[   -521.623967799879665,   38.9898608532696329,    -262.615338217006411,   2.05812428215153176,    -122.634331222663462,   -26.6950750074126937,   -421.136185386370528,   51.2212798617564502,    29.116414258618132, -133.743692978713511,   -27.3597345640707665,   -16.6655955961661313,   216.645687862741113,    -890.103676312148082,   34.0523923623950111,    41.5325865676463479],\n[   -165.504131423128428,   629.36239652028496, -733.843506939228519,   -16.9146742902455003,   -29.3456719546536675,   -29.8476246398971732,   -259.85653756386364,    59.5550335671633135,    79.859043838159181, -67.8271616918597431,   -40.2067414936038716,   -80.6091813515407267,   136.981523782454104,    -624.515261491631122,   55.5765729535027546,    -129.77927072781543],\n[   -358.874156130984773,   39.8194980224920698,    -444.951706354340672,   10.3947246461165133,    -158.50538276526936,    -28.0166090560245102,   -529.652036311735287,   32.4604178690485412,    24.1702757642914641,    -112.007398475161907,   -62.387617408643095,    -10.8474754464223864,   183.909280620761876,    -755.211881798640093,   32.4158087718220216,    2.17102605268478133],\n[   -236.110064662454221,   -40.7413748839802921,   19.6900608024038917,    -1012.61505407125378,   43.8258482757076635,    -52.3318999574951391,   -180.25962884980882,    21.5480405161474948,    2.07883731025328578,    -158.034111895500246,   -13.6035738606832748,   -62.4068500131283486,   542.250326531642372,    -993.918422777186265,   34.6071442284775657,    -53.2084684652367272],\n[   -31.6372025149047573,   -102.770754366443271,   95.3444096435627131,    -35.2773927470484239,   143.326511019605249,    -20.2030545794051619,   -109.960715533120691,   -8.9067503076971839,    78.9489942616323646,    -381.659773116285237,   -466.685541557876263,   -92.6926099430361461,   1657.03065015783022,    -721.511096757360406,   50.8476549204709585,    -169.25056605908344],\n[   -70.1607127627456606,   -135.590943171781902,   54.756057347384818, -54.7349569358214936,   110.325528610443882,    -23.3639881002416097,   -184.432762122255696,   -179.863061582244882,   81.0269610382579089,    -1705.92663333199357,   -952.624726137958987,   -145.943423645620413,   765.107799125946485,    -599.717282723846893,   50.1293561410527531,    -126.560849882023362]\n]. see https://github.com/shogun-toolbox/shogun/blob/develop/src/shogun/converter/ica/FastICA.cpp#L116-L125. here's some example matrix if you turn off whitening (it's on by default):\nmatrix=[\n[   0.375940005500138175,   -0.0494363584957610985, -0.340477794610175022,  -0.107615255155795508,  -0.117077857420152573,  -0.239703982383044051,  0.0127189713681975895,  -0.569207305433260702,  -0.0781336012534851049, -0.146132728906183756,  -0.0131814787100458988, -0.147874555363335936,  -0.328708923599884351,  -0.327216914966432681,  0.0983732836273789879,  0.243098736864453713],\n[   -0.248526747920617574,  0.00188935111391768306, -0.142187772136837892,  -0.300726812188067127,  -0.2762648162087884,    -0.272345132955633507,  -0.351565342295555194,  0.0304048716875823799,  -0.0119614813640933678, 0.322464260470538,  -0.347089282262279031,  0.107873342954605822,   0.286595072742869217,   -0.354325715195810687,  0.250934410242899975,   -0.213716426249467656],\n[   0.0302416097813612596,  -0.218242039887050543,  -0.44395684270437813,   0.144207486543423635,   0.367472915620601803,   0.0150912785481002575,  -0.278578417298657233,  0.163465945688120479,   0.220281353464369861,   -0.00553940885108976031,    -0.286466327363290918,  -0.259883955214308349,  0.262144738990786164,   -0.0162670319018741426, -0.359769746774179777,  0.312082203098220545],\n[   -0.162456672899444682,  0.641215355762007611,   -0.0909729438343435254, -0.484934245666478747,  0.0446586617207307829,  0.217769499531890121,   0.165986860739622544,   -0.168308332964604707,  -0.0655971386082358221, 0.0506092751760189377,  -0.206285287320152638,  -0.00492350371099625934,    0.0784183607208768368,  0.174667295977518316,   -0.205268670447954105,  0.292370722420021545],\n[   -0.114837274858251639,  -0.0862139764928357977, -0.274298150952630715,  0.384618687966544137,   -0.340084583742264135,  0.187879244820246483,   0.13470671855439692,    0.0184278569914821735,  -0.318218441627887816,  0.175782393078900756,   -0.35563167533059209,   -0.0928338689158359348, -0.0898807978906191046, 0.392665512961117846,   0.342572685461930926,   0.199728989866258061],\n[   0.0284964366080672667,  -0.245128294964123722,  0.31905745895714005,    0.113377555766306759,   -0.149309636824201286,  -0.0910617893190493188, 0.256946357735698072,   -0.198264117250969152,  0.156059748937075971,   -0.0661848677107223921, -0.239619610018749818,  0.52916103243286694,    0.322113996130820368,   -0.0884633570849552991, -0.112630519799324438,  0.445083551742947447],\n[   0.0392413354275384235,  -0.216565399223962896,  0.260919468245620734,   -0.296565292949707071,  -0.0781432991786215059, -0.0934712113136992179, -0.200327808676423247,  0.0963918283961005679,  -0.438923791228678972,  -0.616260821416048876,  -0.281915335977738268,  -0.190174163400448154,  0.0880436576151241851,  0.178401678752905329,   -0.0544224733166008903, -0.0254780624428163627],\n[   -0.258962399172312618,  -0.186625165537526005,  -0.0341399892357397289, -0.0790018344777431991, -0.214786063997460003,  0.604867071949218627,   -0.448592014715432974,  -0.119863031224837657,  0.157966696440277249,   -0.14726676284186177,   0.0405636261862047168,  0.253489971245581702,   -0.351009172368297206,  -0.104402540787491158,  -0.111160814978967626,  0.0639993628120597985],\n[   -0.113733044496894795,  -0.155569761415299934,  0.18928464710594245,    0.0198356915924177979,  -0.466194842336279447,  0.0585606484947633205,  0.155570511234983111,   -0.0297363078059857643, -0.151207868735745848,  0.291277438946045353,   0.146738489926682203,   -0.469053925618663103,  0.0676816361654835191,  -0.213305357985891397,  -0.528926400335942959,  0.0411588013448692044],\n[   -0.178668023693690642,  -0.0994035056112626547, -0.288455122259268781,  -0.134552087608625581,  -0.265526881273047854,  -0.137367758873393919,  -0.106289196658777507,  -0.165651977285262764,  0.132403386595788541,   -0.171989024280530062,  0.579331732039321645,   -0.0193635827493389596, 0.422297411576990078,   0.37056960849684295,    0.116179309285670204,   0.128483485208678583],\n[   0.0182092140749589709,  -0.546255056105720005,  0.047990378214621425,   -0.550363330635053272,  0.280122861702694181,   0.173334686499780621,   0.240086797668649182,   0.0490986787581948261,  -0.0174316565891627674, 0.343370207006582495,   0.0605007750982772818,  -0.0865248417666873854, -0.0845305859470525944, 0.0891811925020431773,  0.24593170771735931,    0.150232887829400252],\n[   -0.013169552824989942,  -0.0804491948752832919, -0.0798708321928426751, -0.00308530373326860617,    0.143810311886571257,   -0.327386238159558807,  -0.262509422246661739,  -0.0763566948183721783, -0.460184251237477693,  0.325924141302777026,   0.129659589785901475,   0.409920972503573899,   -0.217570170785871375,  0.280292015985338039,   -0.394021303455675209,  0.0191007781170050711],\n[   0.114722059371360374,   0.121432876574512447,   -0.150928142877857163,  -0.109624011833771254,  -0.21292647060729844,   -0.0880288420212235329, 0.034561690144070463,   0.698461747329667926,   -0.127364326881151929,  -0.0908713279773054605, 0.223935879976690289,   0.15319776588423617,    -0.156916398925001338,  -0.266696988123281076,  0.0649759392429476029,  0.443957294417570147],\n[   -0.283756222848293105,  -0.108167344370075025,  -0.371891400159077679,  0.0695846233686856358,  0.209813612954870582,   0.219488694284851016,   0.370239258911199354,   -0.0571025839344206157, -0.436164675385795064,  -0.218691693511820079,  0.104602835225251961,   0.196935800177192144,   0.223743998037759073,   -0.372169979062061396,  -0.066446385951737838,  -0.226548029523464894],\n[   0.696553020769398512,   -0.0318624608285026714, -0.224307670487180005,  -0.150990322692758333,  -0.263683983142127765,  0.304049094483425375,   0.0429393347158738906,  0.0953115779553692505,  0.0158374497875309536,  0.0770842321617730142,  -0.0685109984684282858, 0.210683718543903803,   0.229108808971641181,   0.133882914705502343,   -0.171703374041904389,  -0.335576766761977452],\n[   -0.250536330217281, -0.152001644436053657,  -0.27034829080977596,   -0.161190869170795781,  -0.192011485130373, -0.30762119006487787,   0.378657177090990671,   0.141291665709262937,   0.372182758601614372,   -0.186406026825159804,  -0.215205507070351437,  0.0856181274053062075,  -0.357178119638385305,  0.187002021203652474,   -0.251614596157469228,  -0.25970290023189041]\n]\nmatrix=[\n[   -0.0273673029949879285, 0.122828190043583849,   0.0387335790918273484,  0.156790393344593326,   0.017510589679586637,   -0.258409568587110394,  0.24596507346531063,    -0.257903001814161648,  -0.00134947065793650039,    -0.315381519257484755,  -0.430123704076080915,  0.0133853177682814479,  -0.0481527309544233992, -0.133092444463867282,  0.281841213863933238,   -0.616332866121246714],\n[   0.275688923108470429,   -0.195916243965636977,  0.380522145112254739,   0.0786924407776366103,  -0.13477190219723853,   0.199147515804758507,   0.304175613208891304,   0.381315945617447472,   0.233597486110479896,   0.153715944178553693,   -0.420596071908452329,  -0.117680698320479865,  0.238423285066784035,   -0.27820832927891781,   -0.185522440093141477,  0.0355625441856539729],\n[   0.29444942777897376,    0.3746472561715114, 0.332789257296516372,   -0.318488734930577944,  -0.0257207754182269821, 0.323348569858415313,   0.205637085498533906,   -0.311979166540460262,  -0.109672685349826146,  -0.367635650163358307,  -0.0260343681743106063, 0.22054376670988049,    -0.173782422496978228,  0.101437048732767029,   -0.21273660674663647,   0.183503833885667378],\n[   0.300977571956793699,   -0.221187860422281418,  -0.0826476048769393123, 0.04635376534789825,    0.40690570518706376,    0.195452185834820213,   0.112852402835624255,   -0.390400796746898537,  -0.0184031741248725754, 0.430062769966508163,   -0.127666686126735485,  -0.160911139879544973,  0.0353393359440084751,  0.448875635056356026,   -0.1133531861032087,    -0.198786032744556973],\n[   -0.0591351363896648746, 0.167074718095756691,   0.0261151214838551901,  -0.168947088582148658,  -0.159889834049031176,  0.14111341197627425,    -0.15139817097304048,   0.319889968126818081,   6.14193435508331701e-05,    -0.172954985657228971,  -0.301533510496752866,  -0.306913395178073145,  0.13486966159021796,    0.623860573135953334,   0.378171897321837647,   0.0565478449606348285],\n[   0.171156837363729913,   -0.0510202490571698974, -0.207128132609958882,  -0.186956867747464256,  0.366906389943486289,   -0.250389388391200951,  0.201762828471859168,   0.0752194301929479947,  0.0383428764959678828,  -0.212982654912619557,  -0.0813326197676723928, -0.5526928777762663,    -0.371112665735523983,  -0.196818033573717688,  0.0425642601773834409,  0.33072061646268075],\n[   -0.100105232294114929,  -0.349407254744824924,  -0.270694092005037468,  0.0324956746712533814,  -0.212510725455927391,  -0.108168764344130794,  0.480983996239841782,   0.168590398434897593,   -0.279752878724648879,  -0.331226013894509064,  0.108412964720531454,   0.0362719876742525482,  0.097184795836472132,   0.313200998993912572,   -0.40292803724299403,   -0.0774629487552986745],\n[   0.228073124882870987,   0.0777602331523198076,  0.266471640347570149,   0.297894376269683847,   0.353993429923279701,   -0.0203791743181723287, 0.0346957706709548827,  0.0676733520318807125,  -0.180430927322945356,  -0.286686055476456148,  0.449301285606381551,   -0.174836163360027913,  0.518937757693773594,   -0.029415658906934733,  0.181537278173448419,   -0.019407708524773512],\n[   -0.456855449054868801,  0.173562865758856599,   0.278868350467851245,   -0.394224442302132605,  0.305160456488951626,   -0.0238782604327776454, -0.0854951325283272306, 0.162381283258202752,   -0.29099236353499941,   0.130899428570931259,   -0.0396818587784305843, -0.224409166297407259,  0.0760784803058325715,  -0.0859221337354963699, -0.369836169839829032,  -0.311309664979357803],\n[   -0.484011445919525718,  -0.148367959870524257,  0.0823458330047445453,  0.117296677914862951,   0.0696553296349280565,  0.00339225977056915092, 0.100801983891267821,   -0.436951945064391001,  -0.113346692977113556,  -0.0461928401262297061, -0.330542801612720205,  -0.0157655864310586348, 0.336705696379399566,   -0.0539634839890512086, 0.0682544056467153937,  0.521343755163684919],\n[   -0.0828499217731216292, -0.650727499279906585,  0.124280150688932486,   -0.363403332349980679,  0.154953623304468185,   0.261966144891110642,   -0.220563469185690614,  -0.0608164168891405371, 0.262613207752497357,   -0.365327796020035511,  0.0896481783781945263,  0.0989850475714320194,  -0.0142709936572459565, -0.066586118591309057,  0.158745916162201683,   -0.164607831224580642],\n[   0.300414059413382617,   -0.246885817380803119,  0.033371765970940416,   -0.159374431861007526,  -0.383055626052964504,  0.00691044861861690903, -0.220145393891152008,  -0.15240976004962753,   -0.683158837503475169,  0.104123542038410857,   -0.0618757981071339919, -0.209335507547437266,  0.0077335477202740803,  -0.226453451404089262,  0.162513642711320488,   -0.029384852722923311],\n[   0.151966844791324107,   0.0795446153233403258,  -0.109228570979151576,  0.0341519095829613886,  -0.234705357669789172,  -0.182353172293009375,  -0.440496091072232865,  -0.280756206902009919,  0.31850593144673256,    -0.241663510124295294,  -0.069255007277699912,  -0.335495082369280606,  0.271005786874648713,   0.00459957600892515594, -0.491867589463062915,  -0.0606782937690361485],\n[   0.0326568093154042602,  0.130352189947319619,   -0.521157179130609549,  0.190638787950670408,   0.265223661607208561,   0.523679310132755482,   -0.208604860713546253,  0.182381526193762883,   -0.210675350398219885,  -0.204132295112606305,  -0.280601992414124979,  0.121835826117791146,   0.0771381108793734588,  -0.230299501517936073,  -0.112805157864200853,  -0.0358129999711991817],\n[   0.284791949791431021,   0.0203228212121650091,  -0.189051475809389885,  -0.444797671307044229,  0.211722473653677717,   -0.455141108730549693,  -0.0528527551033435988, 0.112443078015518547,   -0.0244074369433283332, 0.0594084502678837445,  -0.206931818185392635,  0.448393233812404479,   0.396794533714681263,   0.0149001470120983231,  0.0377260666313224899,  0.101906212892259665],\n[   0.0759283042719920837,  -0.195776027229388916,  0.357593387544854036,   0.387762075514833104,   0.21369435868524797,    -0.268669782033159832,  -0.375297419420714928,  0.168516704920479321,   -0.202866628217939743,  -0.14982050581982076,   -0.246780039320884909,  0.217271063902396505,   -0.344192211712587903,  0.216604796667049876,   -0.196921982882068136,  0.121272744120245801]\n]. @fyoda ? what do you mean very different exactly? i mean, please note that FastICA is randomized so every time you run it if you dont set the seed you will get different results.... . could you elaborate what exactly do you mean by the first estimation of the mixing matrix?. @fyoda on the other hand if you are definitely sure that there's some bug, unfortunately there's no way around but debug and compare the two implementations.. @fyoda can you provide code/data for the shogun test? that way debugging it would be simpler. @fyoda the interesting thing that the unit test of FastICA actually does something similar:\nhttps://github.com/shogun-toolbox/shogun/blob/develop/tests/unit/converter/ica/FastICA_unittest.cc#L25-L30\nand it seems to be passing :). @fyoda i'm just testing atm the data you've provided. with whitening turned on i'm getting this:\n\n. @fyoda i reckon this is the expected signal no?. @fyoda ok so i'm closing this ticket then :). @fyoda that's a good question. the negative coefficient is probably coming from the different eigen solver - namely that the eigenvectors are pointing to the opposite direction..... the amplitude difference is a good question though.... is the amplitude difference is consistent? i mean it looks like, meaning they are all scaled with the same factor, or?. i reckon this can be closed because of 02ee9a54554a8b6a50f502f58ffe1c05a0a66a32 right @shubham808 ?. @rayrapetyan ok i can reproduce this error on linux... interesting enough the same is not true on OSX :( i'll get back to you ASAP when i figured out what's going wrong.. @rayrapetyan i've already found out what's the root cause of the error.... there's a buffer overflow, so whatever osx returns is garbage, or it should be. currently i'm figuring out where's the logic error with the buffer overflow.. @rayrapetyan note that row = feature in case of shogun, i.e. that's it column oriented (each column is an example). @rayrapetyan ah yeah, no the overflow is in CARTree itself. your code is fine.. @rayrapetyan btw regarding the runtime: do you have categoricals? i.e. where you set ft[i] = true;? because if so, then its slow because of this: https://github.com/shogun-toolbox/shogun/blob/develop/src/shogun/multiclass/tree/CARTree.cpp#L683\nin case your problem is a binary classification or regression, today i'll push a patch that'll speed-up things significantly... if it's multiclass i am afraid that you'll have to wait longer for a speedup heuristics.. @abhishek-1991 sure thing, since this is a rather large effort, just start with a file where you find this macro and send in patches.. @abhishek-1991 sorry that i'm only getting back to you now... this error is now fixed in latest develop branch so just pull the HEAD of develop and try to recompile it again. it should work.... lemme know if you still face some problems.. have u seen these @vinx13 \nThe following tests FAILED:\n     84 - python_legacy-distance_canberraword (SEGFAULT)\n     86 - python_legacy-distance_hammingword (SEGFAULT)\n     87 - python_legacy-distance_manhattenword (OTHER_FAULT)\n    137 - python_legacy-kernel_comm_ulong_string (SEGFAULT)\n    138 - python_legacy-kernel_comm_word_string (SEGFAULT)\n    180 - python_legacy-kernel_weighted_comm_word_string (Failed)\n    207 - python_legacy-preprocessor_sortulongstring (SEGFAULT)\n    208 - python_legacy-preprocessor_sortwordstring (SEGFAULT)\n    214 - python_legacy-serialization_string_kernels (SEGFAULT)\n    227 - python_legacy-tests_check_commwordkernel_memleak (SEGFAULT)\nErrors while running CTest. @shubham808 could you rebase your branch over latest develop to get this PR properly tested! thnx!. @shubham808 could you rebase your branch over latest develop to get this PR properly tested! thnx!. miju :) since we can bundle eigen and afaik Stan actually has a 3.3.3 bundled eigen, i reckon the best would be just to drop as is the whole system detection of eigen and use Stan's eigen bundled.... @FaroukY . @vinx13 i'll ping you when you should rebase this branch over the latest develop so we can get an actual good picture of the CI status... sorry it is my fault :(. @vinx13 could you please rebase your branch over latest develop and push it so that this PR gets properly tested. thnx!. plz run clang-format on your code prior pushing into a PR. the bc2c9e50ace0eb64e52ecc49087c2822bcbf0d4d commit will hurt later... that merge should not be merged anywhere... @shubham808 plz fix it. @micmn from appveyor:\n[00:49:40]     15>c:\\projects\\shogun\\tests\\unit\\base\\trained_model_serialization_unittest.cc(127): fatal error C1083: Cannot open include file: 'trained_model_serialization_unittest.h': No such file or directory [C:\\projects\\shogun\\build\\tests\\unit\\shogun-unit-test.vcxproj]. @karlnapf currently discussing this already on #irc thnx. this pr is mergable... its only that gcc actually times out... i've just restarted the job but this should be fine to merge!. thnx for the patch! let's see what CI says and then i merge it. @vinx13 would be great to add a simple xval cookbook where we use a CPruneVarSubMean transformer before doing PCA and applying k-means on a toy data :D. + adding unit test would be great. aaaah and another thing: getter methods for the pipeline elements would be desirable. i.e. get the trained transformer etc.. @lisitsyn @karlnapf so as trying to add pipeline to the swig interface it came out that with is a reserved keyword in python. see the design from 2017 ws: https://github.com/shogun-toolbox/shogun/wiki/Hackathon-2017-base-api#pipeline\nhow about changing Pipeline.with(Transformer) to Pipeline.over(Transformer), since add and as are kind of taken by SGObject, imo as could still fly but add would be matched. i'm wondering whether then would be a reserved keyword in any languages we support?\nand another idea here: imo we should have a Pipeline that is inherited from CMachine, but the builder should be a separate base class, PipelineBuilder.\n@vinx13 i would still like to have a builder that supports adding a list of elements. something like: add_stages([transf1, transf1, machine]). @karlnapf we are getting travis problems here as well..... this branch is rebased over HEAD od develop, but octave is throwing this error\n```\nerror: in method 'Evaluation_evaluate', argument 2 of type 'shogun::CLabels *' (SWIG_TypeError)\nerror: called from\n/opt/shogun/build/examples/meta/octave/multiclass/cartree.m at line 39 column 10\n\n```\n```\nerror: in method 'Evaluation_evaluate', argument 2 of type 'shogun::CLabels *' (SWIG_TypeError)\nerror: called from\n/opt/shogun/build/examples/meta/octave/binary/linear_support_vector_machine.m at line 42 column 10\n\n```. @karlnapf maybe @shubham808 could take a go on this. this is a blocker for 6.2.0. Have u made sure that all the hacks are being in place for memleak detection of python tool with valgrind, eg https://stackoverflow.com/a/45248015\n\nOn 4 Jun 2018, at 16:17, Shubham Shukla notifications@github.com wrote:\nvalgrind says 0 leaks for all meta examples in python\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub, or mute the thread.\n. @vinx13 \n[ 95%] Generating pipeline/pipeline.exe\npipeline.cs(31,1): error CS0246: The type or namespace name `PipelineBuilder' could not be found. Are you missing an assembly reference?\npipeline.cs(32,1): error CS0841: A local variable `builder' cannot be used before it is declared\npipeline.cs(33,1): error CS0841: A local variable `builder' cannot be used before it is declared\npipeline.cs(34,1): error CS0246: The type or namespace name `Pipeline' could not be found. Are you missing an assembly reference?\npipeline.cs(38,1): error CS0841: A local variable `pipeline' cannot be used before it is declared\npipeline.cs(39,25): error CS0841: A local variable `pipeline' cannot be used before it is declared\nCompilation failed: 6 error(s), 0 warnings. and in java\n[ 97%] Generating pipeline/pipeline.class\n/opt/shogun/build/examples/meta/java/pipeline/pipeline.java:12: error: cannot find symbol\nimport org.shogun.Pipeline;\n                 ^\n  symbol:   class Pipeline\n  location: package org.shogun\n/opt/shogun/build/examples/meta/java/pipeline/pipeline.java:13: error: cannot find symbol\nimport org.shogun.PipelineBuilder;\n                 ^\n  symbol:   class PipelineBuilder\n  location: package org.shogun\n/opt/shogun/build/examples/meta/java/pipeline/pipeline.java:51: error: cannot find symbol\nPipelineBuilder builder = new PipelineBuilder();\n^\n  symbol:   class PipelineBuilder\n  location: class pipeline\n/opt/shogun/build/examples/meta/java/pipeline/pipeline.java:51: error: cannot find symbol\nPipelineBuilder builder = new PipelineBuilder();\n                              ^\n  symbol:   class PipelineBuilder\n  location: class pipeline\n/opt/shogun/build/examples/meta/java/pipeline/pipeline.java:54: error: cannot find symbol\nPipeline pipeline = builder.then(kmeans);\n^\n  symbol:   class Pipeline\n  location: class pipeline\n5 errors\nexamples/meta/java/CMakeFiles/java-pipeline-pipeline.dir/build.make:62: recipe for target 'examples/meta/java/pipeline/pipeline.class' failed\nmake[2]: *** [examples/meta/java/pipeline/pipeline.class] Error 1\nCMakeFiles/Makefile2:5049: recipe for target 'examples/meta/java/CMakeFiles/java-pipeline-pipeline.dir/all' failed\nmake[1]: *** [examples/meta/java/CMakeFiles/java-pipeline-pipeline.dir/all] Error 2\nMakefile:160: recipe for target 'all' failed\nmake: *** [all] Error 2. \ud83d\udc4d . The idea here was about sklearnish approach\nOn 4 Jun 2018, at 16:45, Heiko Strathmann notifications@github.com wrote:\n@karlnapf commented on this pull request.\nIn src/interfaces/swig/Transformer.i:\n\n@@ -5,7 +5,8 @@\n  */\n\n/ These functions return new Objects /\n-%newobject shogun::CTransformer::transform(CFeatures);\n+%newobject shogun::CTransformer::transform(CFeatures, bool inplace=true);\n+%newobject shogun::CTransformer::inverse_transform(CFeatures*, bool inplace=true);\ncan we rename to inverse ... without transform ?\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub, or mute the thread.\n. Min max scaled target (regression) for which u learn a model but then u wanna actually map back the prediction into the \u201coriginal scale\u201d\nOn 4 Jun 2018, at 16:47, Heiko Strathmann notifications@github.com wrote:\n@karlnapf commented on this pull request.\nCould you let me know some examples of where such a thing is used? Just curious ... can't think of any (apart from the ICA one, I see that ... )\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub, or mute the thread.\n. great! but i wonder if actually we could fix this by not hacking the compiler flag but fixing the warning itself?\nmeaning: i dont remember having these errors for a long time, i've only seen them recently, so probably/maybe a patch that we added caused this...? would be good to figure it out. an tidious way to figure this out is to check the travis python CI jobs in the past or http://buildbot.shogun-toolbox.org:8080/#/builders/37 and see where this tons of warnings starts to appear...\nas you can see a build from 2 months ago hasnt got these warnings:\nhttp://buildbot.shogun-toolbox.org:8080/#/builders/37/builds/306. @besser82 could you jump in here because that was your commit that has caused this. i had to fix some errors in the feature branch... it should be good now, as soon as things gets green in CIs i'm merging this and then i merge the whole feature branch into develop. @daifeng2016 are you using this on windows by any chance?. @daifeng2016 yeah unfortunately the windows version has no lapack backend... :( as said by @karlnapf this particular error is fixed in develop, but has not been released yet. @vinx13 yep cool! this way the c++ code became much much cleaner!. Working on it.\n\nYou can actually use precompiled llvm and libtooling\n\nOn 25 Jul 2018, at 19:57, Heiko Strathmann notifications@github.com wrote:\nyes :( mmmh dont know what to do from here...\n@vigsterkr ?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub, or mute the thread.\n. We always have these multipurpose classes where multiple functionality/concept is crammed into one class... here\u2019s a clear cut between a builder pattern and cmachine\nOn 27 Jul 2018, at 11:20, Heiko Strathmann notifications@github.com wrote:\n@karlnapf commented on this pull request.\nIn examples/meta/src/evaluation/cross_validation_pipeline.sg:\n\n+File f_labels_test = csv_file(\"../../data/classifier_binary_2d_linear_labels_test.dat\")\n+\n+#![create_features]\n+Features feats_train = features(f_feats_train)\n+Features feats_test = features(f_feats_test)\n+Labels labels_train = labels(f_labels_train)\n+Labels labels_test = labels(f_labels_test)\n+#![create_features]\n+\n+#![create_pipeline\n+Transformer subMean = transformer(\"PruneVarSubMean\")\n+Machine svm = machine(\"LibLinear\")\n+\n+PipelineBuilder builder = pipeline()\n+builder.over(subMean)\n+Pipeline pipeline = builder.then(svm)\nwhat about adding a factory for it? So users can call\nPipelineMachine(machine)?\nAlternatively, for the c++ folks, there is as\n\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub, or mute the thread.\n. You need the builder for building, but u would like to query a trained pipeline for various stages of it as well. Or what do you mean?\nOn 27 Jul 2018, at 14:35, Fernando J. Iglesias Garc\u00eda notifications@github.com wrote:\n@iglesias commented on this pull request.\nIn examples/meta/src/evaluation/cross_validation_pipeline.sg:\n\n+File f_labels_test = csv_file(\"../../data/classifier_binary_2d_linear_labels_test.dat\")\n+\n+#![create_features]\n+Features feats_train = features(f_feats_train)\n+Features feats_test = features(f_feats_test)\n+Labels labels_train = labels(f_labels_train)\n+Labels labels_test = labels(f_labels_test)\n+#![create_features]\n+\n+#![create_pipeline\n+Transformer subMean = transformer(\"PruneVarSubMean\")\n+Machine svm = machine(\"LibLinear\")\n+\n+PipelineBuilder builder = pipeline()\n+builder.over(subMean)\n+Pipeline pipeline = builder.then(svm)\nI understand that there might be mutability reason justifying two types. Why both part of the interface though?\n\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub, or mute the thread.\n. @lisitsyn @karlnapf can i cherry-pick rebase and merge this into develop?\n. see #4443\nplz try with VS 2017 or later. mmm the random (seeds etc) is for sure different so that for sure will end in slightly different results.\n\nafaik in case of sklearn the default is L2 reg with l1-loss right? and it's solving the dual? just because that's the default for LibLinear in case of shogun... i.e. make sure that they are the same solvers.\nbut again PRNG should give you different results - as expected.. @cyberyu thnx heaps for your detailed issue description! i'll try to look at it and get back to you ASAP. mmm wait, the submodule is being tagged... so in master branch the right commit should be marked from the gpl repo, so in a way it's tagged.. but you are right it would be more clear if we would have a tag.. which we actually should be a do now, as we know the exact commit hash that is for 6.1.3.\nbtw how do you get this error? :) when you checkout master branch do you do git submodule update?. ok just pushed a tag: https://github.com/shogun-toolbox/shogun-gpl/archive/v6.1.3.tar.gz this should make things work ;). Hold your horses plz needs some changes....\n\nOn 4 Jan 2019, at 17:22, Heiko Strathmann notifications@github.com wrote:\nWill merge this tomorrow if no more input from @lisitsyn @vigsterkr thx!\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub, or mute the thread.\n. This is the output of google\u2019s microbenchmarking tool. :)\n\nAnd yes the library runs the benchmark codes several times....\n\nOn 11 Jan 2019, at 13:47, Fernando J. Iglesias Garc\u00eda notifications@github.com wrote:\nFor SGVector: taking a vector of N size as denoted below. Seems like the gains of SIMD start showing only after 1024 elements.\nFor SGMatrix: taking a matrix N x N of the N size denoted below.\nI'm not very familiar with the algorithm used by dSFMT, but seems like tput proportionally improves as the number of elements increase until saturation at ~64K elements.\nIn the final cases there is always a drop - my guess is due to first going L1->L2/L3->Memory seemingly.\nCPU Caches:\n  L1 Data 32K (x4)\n  L1 Instruction 32K (x4)\n  L2 Unified 262K (x4)\n  L3 Unified 6291K (x1)\n\nBenchmark                                Time           CPU Iterations\nBM_SGVectorSimpleForRng/256          10686 ns       4824 ns     146526   50.6128MB/s\nBM_SGVectorSimpleForRng/1024         31256 ns      19961 ns      33535   48.9237MB/s\nBM_SGVectorSimpleForRng/4096        103190 ns      80179 ns       8864    48.719MB/s\nBM_SGVectorSimpleForRng/16384       476489 ns     332658 ns       2109   46.9702MB/s\nBM_SGVectorSimpleForRng/65536      1731183 ns    1341493 ns        533   46.5899MB/s\nBM_SGVectorSimpleForRng/262144     7684643 ns    5338349 ns        129    46.831MB/s\nBM_SGVectorSimpleForRng/1048576   28698992 ns   21948063 ns         32   45.5621MB/s\nBM_SGVectorSimpleForRng/4194304  124266202 ns   91083250 ns          8   43.9159MB/s\nBM_SGVectorSimpleForRng/8388608  183002067 ns  149759250 ns          4   53.4191MB/s\nBM_SGVectorSimdRng/256               28129 ns      23222 ns      29822   10.5134MB/s\nBM_SGVectorSimdRng/1024              22501 ns      21000 ns      33497   46.5021MB/s\nBM_SGVectorSimdRng/4096              37701 ns      30281 ns      25527       129MB/s\nBM_SGVectorSimdRng/16384             61909 ns      51976 ns      13567   300.621MB/s\nBM_SGVectorSimdRng/65536            137449 ns     125798 ns       5051   496.827MB/s\nBM_SGVectorSimdRng/262144           520684 ns     453210 ns       1561   551.621MB/s\nBM_SGVectorSimdRng/1048576         1951931 ns    1801798 ns        410   555.001MB/s\nBM_SGVectorSimdRng/4194304         8058928 ns    7512247 ns         93   532.464MB/s\nBM_SGVectorSimdRng/8388608        15801137 ns   15328644 ns         45   521.899MB/s\nBM_SGMatrixSimpleForRng/64           70855 ns      64603 ns      10725   60.4652MB/s\nBM_SGMatrixSimpleForRng/128         255030 ns     249715 ns       2717   62.5713MB/s\nBM_SGMatrixSimpleForRng/256        1022532 ns     996520 ns        712   62.7183MB/s\nBM_SGMatrixSimpleForRng/512        4473191 ns    4014367 ns        177   62.2763MB/s\nBM_SGMatrixSimpleForRng/1024      19003502 ns   16312333 ns         45   61.3033MB/s\nBM_SGMatrixSimpleForRng/2048      67421134 ns   64075364 ns         11   62.4265MB/s\nBM_SGMatrixSimpleForRng/4096     309963506 ns  303301000 ns          2   52.7529MB/s\nBM_SGMatrixSimpleForRng/8192    1427790785 ns 1366948000 ns          1   46.8196MB/s\nBM_SGMatrixSimdRng/64                22065 ns      21745 ns      31635    179.64MB/s\nBM_SGMatrixSimdRng/128               38247 ns      37770 ns      18664   413.689MB/s\nBM_SGMatrixSimdRng/256              108321 ns     106161 ns       6338   588.728MB/s\nBM_SGMatrixSimdRng/512              378871 ns     369052 ns       1817   677.412MB/s\nBM_SGMatrixSimdRng/1024            1519232 ns    1493987 ns        473    669.35MB/s\nBM_SGMatrixSimdRng/2048            6198822 ns    6065946 ns        129   659.419MB/s\nBM_SGMatrixSimdRng/4096           31814041 ns   31293111 ns         18   511.295MB/s\nBM_SGMatrixSimdRng/8192          280005706 ns  274623000 ns          2   233.047MB/s\nVery nice analysis and results! Nice to see the simd versions achieve up to about 10-fold improvement.\nJust to be sure, the results take into account random variation, yes? That is, the code is run many times for each Simple/Simd and size scenario.\nAlso, can you relabel or clarify the names of the columns? :-) Actually just clarifying the difference between the two time columns should be enough. I am guessing the last column should just be something like 8 bytes times N (for SGVector, N^2 for Matrix) divided by the time in one of the other two columns.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub, or mute the thread.\n. @karlnapf i wouldnt put more effort into this for the time being as we wanna drop some of the PRNG stuff as they are flawed by design atm. see feature/random-refactor. a) where is this code being used? meaning the SGVector/Matrix.random()\nb) how do you set the seed of this?. closing as adding support for 2015 in develop would be way too much work.\n@kikish if you want to compile with VS 2015, please try using master branch (latest release). noup. If u have ideas for a fix send me patches and i can check it locally on a windows machine... the azure job takes 1hour 20 mins to check any changes...\nOn 4 Jan 2019, at 17:07, Gil notifications@github.com wrote:\nThat thing was already a bit hacky to make clang and gcc work, the auto in the lambda makes it harder to determine the number of arguments required\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub, or mute the thread.\n. @saatvikshah1994 mmm the 3rd part is a bit interesting as i've added the benchmarking part to the codebase and i did actually wrote that and tested it on a mac :). ok so i've just ran this:\ncmake -DLIBSHOGUN_BUILD_STATIC=ON -DCMAKE_BUILD_TYPE=Debug -DCMAKE_INSTALL_PREFIX=/Users/wiking/shogun-build -DBUILD_BENCHMARKS=ON ..\nmake -j2 HashedDocDotFeatures_benchmark\n\nworked fine.\ni've tried make RandomFourierDotFeatures_benchmark and worked as well.\ncould you please give more info about your macos setup?\nbtw i know that i should compile with -DCMAKE_BUILD_TYPE=Release to get the right results of the benchmark, but i dont have ccache for that and didn't want to wait that long to get you the results.. Can you add your cmake output here as there\u2019s definitely something wrong with the benchmark detection... but it relies on gbenchmark\u2019s cmake config scripts so even in that case its a bug somewhere there.\nWhats your benchmark library version?\n\nOn 10 Jan 2019, at 07:19, Saatvik Shah notifications@github.com wrote:\n@vigsterkr Im running on OSX(10.14.1) and XCode Developer Tools 10.1.\nI checked ld -v -lbenchmark which does have /usr/local/lib as a search path and /usr/local/lib/libbenchmark.a exists - which makes the error a bit weird. Even still I added /usr/local/lib to LD_LIBRARY_PATH but the error persists.\n@gf712 yes, this does work! I guess I'll set it up on Ubuntu then.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub, or mute the thread.\n. Oh this looks nice! PR is welcome to replace our internal solution ;)\nOn 12 Jan 2019, at 10:17, Saatvik Shah notifications@github.com wrote:\nWith respect to the sanitizers, one thing to try could be https://github.com/arsenm/sanitizers-cmake?\nThis also prevents things like trying to run incompatible sanitizers together.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub, or mute the thread.\n. i've killed this CI job... coz the windows build would have taken ages :) but it should be fine to merge. i'll merge it as soon as the #4456 is done. @saatvikshah1994 can you please test this on your mac setup?. Its in cmake...\nOn 14 Jan 2019, at 13:39, Gil notifications@github.com wrote:\nHow do you handle the GPL only classes? Is there a way to hide .sg files from the compiler/test with GPL only implementations?\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub, or mute the thread.\n. @inglada thnx for the links!. export LD_LIBRARY_PATH=\"/usr/local/lib:$LD_LIBRARY_PATH\"\n\nas it's a variable that holds paths and not filenames.\n. this broke somehow windows CI. @gf712 any ideas why suddenly this error?. @gf712 yep i've got that but then somehow after merging the NewtonSVM started to cry.... and before it wansn't... dunno how this is possible of course... mmm its strange that there's no lapack as mkl is there... lemme check maybe something changed with anaconda env. on the other hand i've just checked NewtonSVM and it seems it doesn't use lapack anymore.... but lemme check again. @gf712 yeah something changed with the anaconda env as cmake now only detects BLAS but not lapack from MKL part :S. @gf712 https://github.com/shogun-toolbox/shogun/commit/d78717b8bfdcafe37b61e0fc7516f3e4ee6b2e3d should fix the problem, but i have to see what has happened with anaconda env :(. @mrkarna try to give it a go and send in a pr. @muhannad35990 you should stick with the precompiled anaconda version... today i'll test the import of the shared libs into a qt project... note it's gonna be on linux, but one would hope that it shouldn't be such a big difference....\nand actually could you create the output of qmake (errors) and paste it on http://pastebin.com and provide the link here. thnx!. @karlnapf btw why do we store the log_width instead of the width itself? :). @Kolkir great catch and thnx for the patch! the CI should be happy, but just in case i'll wait for it and then we can merge!. Yes indeed you should regenerate the data files for the lrr\n\nOn 1 Feb 2019, at 11:54, Kirill notifications@github.com wrote:\n@vigsterkr There are two failed tests, they both depend on LinearRidgeRegression class. Am I understand right that I need to regenerate data files used for these tests? They both depend on LinearRidgeRegression implementation which produced wrong results because of this bug in SGVector::as function.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub, or mute the thread.\n. Yes plz do a pr there and update this pr to use that shogun-data commit.\nOn 1 Feb 2019, at 12:14, Kirill notifications@github.com wrote:\nIt should be a separate PR to shogun-data repository, right?\nYes indeed you should regenerate the data files for the lrr\n\u2026\nOn 1 Feb 2019, at 11:54, Kirill @.***> wrote: @vigsterkr There are two failed tests, they both depend on LinearRidgeRegression class. Am I understand right that I need to regenerate data files used for these tests? They both depend on LinearRidgeRegression implementation which produced wrong results because of this bug in SGVector::as function. \u2014 You are receiving this because you were mentioned. Reply to this email directly, view it on GitHub, or mute the thread.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub, or mute the thread.\n. it's unrelated... merging!. closing in favour of #4525. @fyoda in develop we dropped support for MSVC 2015 :( try 2017. @fyoda that's a good question i think sometime before this commit: b45243cf5afadcea5dee1df21284c2442ee01e98. @fyoda but you could use the conda artifacts. @lisitsyn @karlnapf finally this fix is soon to be finished (the integration tester seems to be having some issue as well some unit tests...) all in all plz take a look at this pr, and lemme know if you have any objections to merge it as soon as the tests are passing.. windows build... as currently it was barely working.... and yes we need export for all the classes you intend to expose to the outside world..... there's no other way to create a valid dll on windows.... @karlnapf imo having that amount of flexibility would be cool but that's a lot of additional work, that currently we dont have bandwidth for.... especially adding option for this via SWIG etc.... this is a good enough addon that actually even helps @vinx13 to continue with a blocker part of the weighted degree kernel.... hopefully. @karlnapf this solution is way better than anything we have now.... if there's time we can refactor it the way you like it but currently having those values (degree = 0.0) is just worse than anything else. what do you mean dont belong? polykernel has a variable that could be autoinited based on the input... why wouldn't it belong here... yes you could add more ways to init it - nobody does this, see other libs - but again... having this is a good start and later can be refactored. > This one is not good in so many contexts.\n\nr u for real?. > Then we can change the default value\nand in your opinion for the degree what would be the good default value... and why?. Ok will do in the next iteration after this is merged\n\nOn 14 Feb 2019, at 09:31, Heiko Strathmann notifications@github.com wrote:\nHere is what I suggest:\n-Move the code to extract the parameter value away from the registration,\nlike somewhere centrally accessible, where all heuristics will live\n-Overload put to accept some way of instantiating the heuristic, for example\nkernel.put(\u201cgamma\u201d, auto_param(\u201cnum_features\u201d))\nkernel.put(\u201clog_width\u201d, auto_param(\u201cmedian_distance\u201d))\n-Inside this put, the same mechanism as outlined in this pr is used to\nregister the function\nOn Wed, 13 Feb 2019 at 22:11, Viktor Gal notifications@github.com wrote:\n\nThen we can change the default value\nand in your opinion for the degree what would be the good default value...\nand why?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/shogun-toolbox/shogun/pull/4522#issuecomment-463372694,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAqqv6hOamx0lsWgDj6iXxH2_qKYR6w8ks5vNH90gaJpZM4a0WXZ\n.\n-- \nSent from my phone\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub, or mute the thread.\n. closing because i've cherrypicked all the commits into the feature branch. Add one to azure :)\n\nOn 14 Feb 2019, at 10:24, Gil notifications@github.com wrote:\n@gf712 commented on this pull request.\nIn src/shogun/converter/ica/SOBI.cpp:\n\n@@ -66,8 +66,11 @@ void CSOBI::fit_dense(CDenseFeatures features)\n  MatrixXd M0 = cor(EX,int(m_tau[0]));\n  EigenSolver eig;\n  eig.compute(M0);\n- MatrixXd SPH = (eig.pseudoEigenvectors() * eig.pseudoEigenvalueMatrix().cwiseSqrt() * eig.pseudoEigenvectors ().transpose()).inverse();\n- MatrixXd spx = SPHEX;\n+ MatrixXd EVMsqrt = eig.pseudoEigenvalueMatrix().cwiseSqrt();\nIs there a buildbot for MKL backend btw?\n\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub, or mute the thread.\n. Yep but use clang as its faster compiling;)\nI\u2019ll b on irc in a bit\nOn 14 Feb 2019, at 10:33, Gil notifications@github.com wrote:\nAdd one to azure :)\n\u2026\nOK! Just for GCC should be fine right? Might need some help setting it up.\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub, or mute the thread.\n. closing in favour of #4529\n@gf712 right?. @gf712 cherry pick the commits of that branch into this one and we'll see right away ;). @karlnapf any concerns there .. We need to get rid of these due to the plugin based arch... there we cannot rely on these enums... but indeed before you push a pr plz make sure that it compiles locally.\nOn 20 Feb 2019, at 14:11, Heiko Strathmann notifications@github.com wrote:\nLol, you cannot just delete this as shogun won't compile anymore. The CI output above shows you some errors as a starting point. But before you start refactoring/deleting all code that uses the MachineType type, pls make sure it is feasible to avoid running into a deadend. If the enum and code based on it is never really used, we can just safely delete it indeed\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub, or mute the thread.\n. Its pretty standard to define static vars with camelcase and prefix it with k\nOn 20 Feb 2019, at 14:07, Heiko Strathmann notifications@github.com wrote:\nRe the failing unit test, just remove the line that asserts the ref count\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub, or mute the thread.\n. Yeah we dont really use these standard notations... but maybe it\u2019s time to change it... especially that as you can see this is not exposed to anything.\nOn 20 Feb 2019, at 14:29, Heiko Strathmann notifications@github.com wrote:\nI never saw it in shogun, but I also don't really mind either way.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub, or mute the thread.\n. We could use std::weak_ptr within the autoinit class for storing reference to the kernel and not increment the ref counter of kernel...\nOn 20 Feb 2019, at 15:29, Heiko Strathmann notifications@github.com wrote:\nI never saw it in shogun, but I also don't really mind either way.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub, or mute the thread.\n. @karlnapf i'm just wondering in case of DenseFeatures the feature_matrix could be defined as a const and that should have the same effect... or?. just needs a rebase... pushed that little code to develop\n. @vinx13 afaik by using target_link_libraries that should be taken care via cmake.... . @theartful can you please point us to the situation when the error you are trying to fix came up.... a test or dev env. @jyotirmoy1997 you are trying to build with MSVC 2015... that won't work... other than that you seem to have some permission errors\n. yes. CMake Error: Could not open file for write in copy operation C:/Users/nEW u/Desk                                                                                            top/shogun_stable/shogun/build/CMakeFiles/3.13.4/CMakeSystem.cmake.tmp. sorry but this is your dev env... and clearly you have a problem with permissions... i cannot help you more than this\n. probably CI will fail on all integration tests.... expected. @karlnapf yes, thank you - there's a good reason why i asked review from @lisitsyn. Sure cherry pick it as this one is already rebase fixed for develop... though i would carve out the clone support as well as without that cmp works but not clone...\nOn 12 Mar 2019, at 15:00, Heiko Strathmann notifications@github.com wrote:\nDo you guys mind if I push the commit by @lisitsyn into develop?\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub, or mute the thread.\n. closing in favour of #4573. @shiyi001 looks good! :) if nobody has any objections i'll squashmerge this. @theartful great effort but here's some comment:\ninstead of the is_* function, could you please do the following:\nvirtual uint32_t supports() const\nin the unsigned int we store bitmask, that describes the machine in question.\n\nsomething along this line\nenum Input {\n DenseFeatures 1 << 1;\nSparseFeatures 1 << 2;\n...\nsimilar to the machine type and then just use bitmask operations to set the unit32_t\n. But its in .cpp ;)\n\nOn 16 Mar 2019, at 20:55, Gil notifications@github.com wrote:\n@gf712 commented on this pull request.\nIn src/shogun/io/fs/FileSystemRegistry.cpp:\n\n@@ -0,0 +1,130 @@\n+#include \n+#include \n+#include \n+#include \n+\n+using namespace shogun::io;\n+using namespace std;\nusing namespace std; :O\n\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub, or mute the thread.\n. @lisitsyn ok this is almost done.... now only thing i need is to change the meta example integration tester and drop the whole Serializable* (ascii and hdf5) and corresponding interface.\n\none thing is that SparseFeaturesTest fails but that's because the SGSparseMatrix is registered with the tags framework in a wrong way..... @Macr0Nerd remove the functions and wherever it is being used use the mentioned .as function. it's part of SGObject. For me CMath::qnorm seems to have a little bug:\nin CMath::qsq the summation is done in the following way:\n                    result+=CMath::pow(x[i], q);\nwhile it should be:\n                    result+=CMath::pow(fabs (x[i]), q);\naka an absolute value is missing there....\n. use the naming convention of shogun: basically call it CLARS instead of LARS...\n. maybe these checks should be done in the beginning of the function call?\n. why do you need ?\n. @pluskid i thhink what @sonney2k means there is that he doesn't like std:: ;) so you try to implement it without using STL....\n. although i guess @sonney2k will only take the patch if you remove STL based implementation, you could use here ::const_iterator instead of ::iterator\n. i need it for:\nchar dirname(char path);\nsee man 3 dirname\n. even if it's commented out? i mean that line will be taken out, because nobody understands it IMO just me...\n. grrr. i might add that this is not consistent to me. if we say that functions should be called like foo() and not foo () then since if() is a function as well it should be if() and not if () or? :)\n. that's coming from libocas, but i can change it in shogun\n. i don't know.... i guess we should ask vojtech\n. yes that's the multiclass version and in the prev version (0.95) it was \ny2 = (uint32_t)data_y[i]-1;\nas well while in shogun it was y2 = (uint32_t)data_y[i]; so i guess that's a difference between the libocas and shogun, or?\n. did this really ever compiled on your system?\n. @lambday yes, it does not compile:\nmathematics/Statistics_unittest.cc:85:29: error: variable length array of non-POD element type 'SGSparseVector'\nwhat compiler do you use?\n. this is BAD!\n $(COMPFLAGS_GTEST_CPP) $(COMPFLAGS_GMOCK_CPP) covered this, it was totally unnecessary to do this!\n. to be able to mimic this functionality (dynamic_cast) in other interfaces like python, r etc. please provide a static function called 'obtain_from_generic'. see examples for it in Labels.\n. this should be rather checked with an ASSERT() or REQUIRE() call.\nedit Heiko: please use REQUIRE(\"%s::set_degrees_freedom(): Number of degrees ....\\n\", get_name());\n. use ASSERT or REQUIRE for these type of checks.\nalthough it's pretty nasty to throw exceptions from a constructor....\nedit Heiko: I think its fine here, just make sure that the standard constructur doesnt throw some.\nand please call the function below here to avoid double code\n. @monicadragan i'm currently doing a RandomForest implementation and i'll be needing decision tree implementation for that.\nThere's a templated tree implementation in shogun already, see shogun/multiclass/tree/TreeMachine.h\ni reckon maybe the ID3 implementation should be done based on that...\n@sonney2k what do you think? \n. a classifier and it's implementation should be under src/shogun/ and not part of an example. Examples are strictly for small examples where you 'show' how one can use a given module of shogun\n. i'd remove CStructuredLabels* as well... bot from CPrimalMosekSOSVM but as well from StructuredOutputMachine.\nStructuredOutputMachine::set_labels() would be just a call to StructuredModel::set_labels() so i don't really see why it's worth to keep that function....\n. plz use the macros. i.e. HAVE_CXX11 and so....\nalthough this is only handled well in cmake branch, not in develop. but merge is just coming sooooo...\n. @hushell this is automagically detected (c++11 or c++0x) by the new cmake build system. it will define you HAVE_CXX11 or HAVE_CXX0X macros that you check if it's defined in your code.\n. this should be:\nif defined(HAVE_CXX0X) || defined(HAVE_CXX1X)\n. same here: #if defined(HAVE_CXX0X) || defined(HAVE_CXX1X)\n. sorry man i'm a total idiot, the proper way should be:\nif defined(HAVE_CXX0X) || defined(HAVE_CXX11)\nsorry!\n. same here:\nif defined(HAVE_CXX0X) || defined(HAVE_CXX11)\nsorry for telling you the wrong macro :(\n. please do this with one #ifdef\nif defined(HAVE_COLPACK) && defined(HAVE_EIGEN3)\n. same here\n. same here\n. the 'rm /config.status &&' can be removed...\n. yes, but you need to define this in src/shogun/lib/config.h.in\ncmakedefine HAVE_CURL 1\n. no, do this instead:\nLIST(APPEND INCLUDES ${CURL_INCLUDE_DIRS})\nnote that it's CURL_INCLUDE_DIRS and not CURL_INCLUDE_DIR\n!\n. not needed at all.... as CURL_LIBRARIES is defined by FindCURL\n. LIST(APPEND DEFINES HAVE_CURL)\nis missing...\n. and don't forget to add\ncmakedefine HAVE_CURL 1\ninto ./src/shogun/lib/config.h.in\n!\n. this should be set by the notebook generator\n. remove this.. i mean the include of pthread.h\n. FreeBSD Project -> Shogun Toolbox\n. FreeBSD Project -> Shogun Toolbox\n. i know that this has been already merged and i'm sorry that i'm only joining so late the discussion, but if possible it would be great to set the eigen solver as a parameter of the class.\nthis would for example enable us to set a GPU based eigen solver for PCA-ing. See viennaCL's lanczos eigen solver:\nhttp://viennacl.sourceforge.net/doc/lanczos_8hpp.html\n@karlnapf @mazumdarparijat what do you think?\n. please note that that class_list.cpp is actually generated by class_list.cpp.py and as you can see in that python script we have config_tests where the macros are defined and check_is_in_blacklist is actually checking for #ifdef but not for #if defined. this might not be related to this, just saying that it's actually the best is to use #ifdef and not #if defined if possible....\n. @mazumdarparijat on one hand of course it makes sense to go with the eigen solvers that are already within shogun, i.e. mathematics/linalg/eigsolver. But on the other hand what I really would like to see here is to finally have support in shogun for GPU based calculation (or rather say OpenCL based ones), and that is currently not supported by those implementations. Maybe we should investigate of course how we could achieve that there, but the main idea here would be that the eigen solver in PCA is actually a parameter, so that the user of the PCA class can choose what eigen solver is being used for calculating PCA.\n. unit tests should really cover only one given functionality. although first i've liked this idea of covering everything at once, but actually it'd be better to split each of these into a separate unit test, so something like:\n- TEST(LibLinearTest, train_L2R_LR)\n- TEST(LibLinearTest, train_L2R_L2LOSS_SVC_DUAL)\n- TEST(LibLinearTest, train_L2R_L2LOSS_SVC)\n  ... and so on.\nbecause this way we can right away identify which method failed.\nYou can do this actually by writing a generated unit test. see for example tests/unit/base/clone_unittest.cc.jinja2 and the corresponding tests/unit/base/clone_unittest.cc.py and the tests/unit/CMakeLists.txt\n. let's hold on with this patch. i reckon the problem is somewhere in the setting prob->n the right way...\n. fix indenting... and this is for the whole file.\nmost editor has a script for this nowadays\n. this whole example needs refactoring...\nwhy is informational_gain_attribute function in the example?\nthis should be in the implementation of ID3\n. same here... CID3Classifier being implemented in the example? why?\n. we have a CTreeMachine in shogun. if that does not fit your requirements that's ok, just extend it and refactor it so that it works for you, but do not implement a new tree machine....\n. so if you have the informational_gain_attribute implementation here, why do you have it in the example?\n. header file among examples? why?\nthis file should not be here (examples) at all...\n. SG_SPRINT, SG_SDEBUG, but not fprintf\n. this example should rather be a unit test... and not an example\n. as i've mentioned in your previous PR:\na yet another tree is introduced to the library: there's really no need for that as there's already like 3 tree implementations in within the library. If the current multiclass/tree/TreeMachine.h is not generic enough for you, just extend/refactor it as you need, but please don't introduce a new class....\n. same as in case of GenericTreeMachine.h... try to refactor the multiclass/tree/TreeMachine.h that it fits your requirements...\n. fix indentation\n. if you do using namespace Eigen; as above why do you still used Eigen:: namespace scope?\n. you are not allowed to change the licensing of this file unless you have Chiyuan Zhang's consent\n. you are not allowed to change the licensing of this file unless you have Chiyuan Zhang's consent\n. avoid using STL in headers.\n. you can solve this with other data structures that are part of shogun, like DynamicObjectArray\n. @karlnapf you should not have merged this one...\n. @karlnapf same here...\n. why would you remove the empty line before EOF\n. no it will not break the universe that's true ;)\n. which unit test change the global loglevel of shogun?\n@dhruv13J no need to fix as i've already fixed it in 3ca11915acb0d482a6ddc0f90de5d83ef970ad93\n. @karlnapf the only unit test that's actually using SG_SDEBUG is mathematics/linalg/RationalApproximationIndividualJob_unittest.cc which indeed should have never been merged ;)\n. yeah but that's not the point: the point is why are these changes are there at the first place. moreover why there's a commented out line being added to the source...\n. @dhruv13J yes i understand but no. this should not be in any way here....\n. well this class it self is not visible in modular interface AFAIK, but all the inherited classes of CTreeMachine are visible via the modular interface, as they are not templated anymore ... ;)\ndo we want CTreeMachine to be visible in modular interfaces? if so then it needs some hacking in the .i files... + director classes ;)\n. indeed\n. @iglesias sorry... what? :)\n. any tree will be derived from this general TreeMachine. hence those classes will not be template classes anymore so we can just add it to modular interfaces as any other class.\none good example for this (using TreeMachine as a base class) CConditionalProbabilityTree which is added in src/interfaces/modular/Multiclass.i to the modular interface the usual way.\n. why would you need to implement a new BinaryTreeMachine?\nif you inherit the BinaryTreeMachineNode from this TreeMachineNode then you'll be able to still use it in TreeMachine, since it only stores a CTreeMachineNode<T> *m_root;\n. it's not that... it's because the LibLinear machine was actually never SG_UNREF-ed... hence the cloned features were not unrefed either...\n. @iglesias @mazumdarparijat these functions are introduced earlier in TreeMachine so i reckon we can leave them as is, as it would take quite amount of patching of other the other trees that were already there...\n. indent error\n. extra newline...\n. this should be done via CFile, moreover do we know which standard formats support multilabelling?\n. this rather should be something like:\nTEST(TreeMachine, ID3TreeNodeData)\n. this rather should be something like:\nTEST(TreeMachine, full_test)\nsame as above about the naming of the test... something more specific than full_test...\n. TEST(BinaryTreeMachineNode, full_test)\nalthough a something more meaningful name than full_test would be better, perhaps\nTEST(BinaryTreeMachineNode, build_tree) or something similar...\n. as @lisitsyn said maybe we could find a better name, like env_value or even log_level_env\n. environmental variable detection should be moved to a separate function and not just added here in the init part... as i suspect that later on we'll have other env variables as well...\n. even if you rewrite things that does not allow you to do licensing change...\nthe rules are quite clear on this one....\n. mmm i seriously thought that we have an entropy calculator in CMath.\nbut seems not, but it'd be great to have one in a form where you give in the Pi-s and it'll give you back the entropy, something like:\nfloat64_t CMath::entropy(SGVector<float64_t> probabilities)\n. please initialise the variable, i.e.: char* env_log_val = NULL;\n. could you please apply the changes according to my previous comments, i.e. add a new method called entropy to CMath that does entropy calculation on an SGVector where each element of a vector is a probability:\nfloat64_t CMath::entropy(const SGVector<float32_t> probabilities)\nfloat64_t CMath::entropy(const SGVector<float64_t> probabilities)\n. as @karlnapf says it's really no need to put these tests into a separate file.\nThe idea is that for each component/class in shogun there's a unit test set (i.e. several different tests) in the file called <classname>_unittest.cc. see the developer manual:\nhttp://shogun-toolbox.org/doc/en/latest/developer.html\nso i suggest to merge the 2 files for LibLinear unit tests, moreover keep the same unit test naming. Namely when you define a unit test in a file it should be:\nTEST(<Class name that is being tested>, <some meaningful test name for the test scenario>)\nSo for example TEST(LibLinearSimSetTest,train_L2R_L2LOSS_SVC) or TEST(LibLinearTest,train_L2R_LR)\nare not following this idea since we know that it's a test, i.e. you do not need to have the Test suffix in LibLinearTest or LibLinearSimSetTest. It should be simply LibLinear.\nso all LibLinear test should be something like:\nTEST(LibLinear, <some meaningful test name for the test scenario>)\nand if for example they are both doing training, you can do then:\nTEST(LibLinear, simple_set_train_L2R_L2LOSS_SVC)\nTEST(LibLinear, train_L2R_LR)\n. actually according to the definition of entropy it should be log and not log2\n. @mazumdarparijat entropy is a standard information theory definition:\nhttp://en.wikipedia.org/wiki/Information_theory#Entropy\nthere's no 'decision trees' version of it...\n. @mazumdarparijat you mean to CStatistics::entropy, right? although maybe it's worth introduce there an entropy calculation that doesn't expect the p be in logspace. @karlnapf any thoughts on this?\n. no need for checks like this if you use SG_MALLOC, see:\nhttps://github.com/shogun-toolbox/shogun/blob/develop/src/shogun/lib/memory.cpp#L193\n. why do you add this comment? no need for such things\n. wrong indenting...\n. LIBBMRM_CALLOC = SG_CALLOC\nwhere return of NULL is being checked in SG_CALLOC itself.\n. fix indentation\n. mmm r u sure about removing all these 283 lines? as i don't see where this functionality is being implemented that you've removed here. i.e. half of the file is missing, hence it cannot be even compiled...\n. ref++\n. ref++\n. ref++\n. ref++\n. i don't see any unref of any of those here...\n. why would you create a new DualLibQPBMSOSVM2.h/cpp? why not just patch the original one?\n. why would you still want to keep //MOD comment?\n. this should be implemented in the .cpp as already mentioned\n. this is the same as in case of BMRM\n. why?\n. mmm not really good idea. especially that you are doing this change because of some other library that you are just porting. should it happen the other way around?\nalthough get_* functions should be usually const, but since SG_REF actually changes the value of the class, it's really debatable...\n. ????\n. indent...\n. indent\n. fic indent\n. ???\n. no.... sorry but:\n a) why do you need this typedef. in other words you dont need it\n b) use SGMatrix\n. ???\n. why do you need this include actually here?\n. your indents are just wrong\n. ???\n. no.... this should be inherited from CMachine or CMulticlassMachine or some other machine but not CSGObject\n. if it's not implementet why do you return anything?\nFYI: there's a macro for this SG_NOTIMPLEMENTED so please use that and don't return anything...\nand btw: have you heard of pure virtual functions in c++?\nif not then i suggest to read this:\nhttp://stackoverflow.com/questions/2652198/difference-between-a-virtual-function-and-a-pure-virtual-function\nand refactor this code.\n. if you want to add a modular example, then:\n- try to follow the file naming (check the others in the directory).\n- check out the structure of those files as they all follow one template..\n. why new line?\n. missing empty line before EOF\n. please try to come up with a more descriptive name + add it within the shogun namespace....\nbut it would be even better to use some sorts of a dynamic array...\n. as said, why not a dynamic array here?\nlike simple char * ?\n. this is too few. there's a lot of class parameters/attributes that are not being set here and not being added to the parameter framework, i.e. SG_ADD\n. directly fscanf...?!\nplease try to use LineReader and Parser classes for doing the whole parsing... see LibSVMFile as a good example.\n. add a private: here as well... just to separate attributes from functions\n. @Jiaolong it would be much better to add this functionality to LibSVMFile and use MutlilabelLabels since we just merged that patch recently.... see issue #1987\n. perfect! thank you!\n. :) awesome, but please try to use macros at least.\nand since i don't see neither mode 0x3 nor mode 0x6, why a simple enum and an if (mode == ENUM_VAR) is not good enough to use here?\nbtw: is this code based on some other code or the whole thing is written from scratch?\n. what?\n. what's this for?\n. please discover a bit the functionality of the library and try to use already implemented functionality.. i.e. this whole class is unnecessary\n. why would you need an extra namespace for 'deeplearning'\n. and for enums?\n. example? in the library?\n. Makefile...? seriously?\n. no .... there's already one...\n. extra namespace for maths? what?\n. indent...\n. a class from nowhere, without inheritance... ?\n. namespace... seriously?\n. no way... we do have already a matrix implementation...\n. read my global review... you should fully rework this PR. that's why i've closed it\n. nono, please don't do this. keep the code format as it was. too much unnecessary line changes... so revert all these sorts of changes in your patch\n. again format change... keep it as it was.\n. why virtual?\n. this has nothing to do with a consistency of other functions...? it's a c++ thing. have you inherited this function? no\nare you going to derive from this class? no... then....\n. but this is not the main problem...\nsee my comment ....\n. please try to follow the naming convention, i.e. find a much more meaningful name than test\n. license part?\n. why do you need this? all the other forward declarations?\n. why do you need all these forward declarations?\n. you can do regression as well with an NN\n. a short description of this class would be really good to have\n. why virtual and why train_all?\nplease investigate the API of CMachine\n. same as above\n. same as above\n. same as above\n. this will never compile.... SGMatrix is a templated class...\n. this will never compile.... SGVector is a templated class...\n. i feel it a bit too restrictive that we say that for simple neural network one needs to have eigen. ok i know eigen is just a collection of header files but still....\n. can we at least add an #else part where the same arithmetics is being done... it's really simple without eigen ...\n. would it really hurt to add #else here as well?\n. not only...\nyou can do binary and regression as well with a NN\n. no, but before proceeding please read my global comment....\n. please clean all the output cell of the notebook, we don't want to store things in the repository that can be reproduced....\n. SG_ADD is still missing... please check other shogun class implementation...\n. this is wrong design concept...\nbecause this way what you are implying is that once we have another model that you want to save in uai file format we'll have to add\nvoid CUAIFile::save(COtherModel* m)\nand implement it here...\ni don't know if you can recall the reason why we wanted you to create a separate CUAIFile: to decouple it from a given model. now with this you are doing the same thing just in another class.\nwhat you need to do actually is to design a CUAIFile that can represent an UAI file with some standard data structures... and in FactorGraph for example you would set these interim variables in CUAIFile in the desired way.\n. still not fixed...\n. still using flags instead of equation...\n. i was just wondering if we could maybe have the CNeuralLayer templated, like:\ntemplate <class N> CNeuralLayer : public CSGObject\n{\n...\n}\nwhere N is the type of neuron being used in the layer?\n@lisitsyn ?\n. why do you keep this still in the code?\n. no need to #endif here if the implementation depends on eigen\n. no please don't do this... use SGMatrix\n. who came up with the name bsxfun?\nit is a function, can't it be just binary_singleto_expansion ?\n. what does Elmat stands for?\n. asdfQwertyO1234 please try to use more descriptive names than then lapack-like ones\n. why does exactly Berling Institute of tech gets credit here?\n. this is really from ALGLIB?\n. completely unnecessary.\nuse @author in the class description, which is atm missing\n. @khalednasr well, let's say like why not?\ni mean if you design the class N the good way then you dont have to have\ncompute_activations a pure virtual function. catching my drift?\n. perfectly in line with c++ standard, but in shogun variable initialization is in init() function, that you happened to have anyways...\n. refcount++\n. refcount++ for both layer i and i-1\n. refcount++\n. ok i was trying to refer to the fact that when you call DynamicObjectArray::element(.) it'll raise the reference count by one... in other words every time you call get_layer(i) in your code you will raise the reference count by one, but you never decrease it by calling SG_UNREF... hence this will lead to memory leak\n. that's why i suggested that you should create unit tests. as they'll right away such errors if you valgrind it.\nand anyways we require to have unit tests for new classes.\n. strncmp\n. please avoid the includes of STL headers in headers.\n. public? why?\n. that would be an SGMatrix, SGNDArray or?\n. does it have to be a system include for sure?\ni mean eigen has to have that, but i'm not so sure about nanoflann.... have you just copy-pasted this part or nanoflann requires -isystem include as well?\n. this class description is not true at all..\n. @tklein23 see CFile.h\n. there's really no need for this variable.\n. you instantiate a CKNN each time you call compute_nn why?\n. same as for CKNN, you instantiate a CGaussian and destroy each time when one calls compute_pdf, why?\n. i do understand what you are trying to do with that variable but still it's unnecessary...\n. what's this for?\n. what exactly would you need this for?\n. why do you define all these pure virtual functions?\n. if these two are not used please remove them.\n. why not to use simple SG_DEBUG instead of using these macros...?\n. same as for BENCHMARK_KNN why don't you check what's the log level with a branch and if it's MSG_DEBUG then execute the loops..\n. you need to include shogun/lib/config.h before you can actually check for this macro\n. i reckon this should be changed to something like:\n```\nenum EKNNMode \n{ \n  BruteForce\n  , CoverTree\nifdef HAVE_NANOFLANN\n, KDTree \nendif\n};\n```\nas KDTree mode is only available if nanoflann is available or?\n. as i've commented for the enum, i'd rather hide KDTree option if cmake did not find nanoflann...\ni mean this is why we actually have configure script to detect whether nanoflann is available... it's rather weird to say that we know that you don't have nanoflann but still 'support' that mode by returning runtime error....\n. @dhruv13J well that's what cmake (configure) script is for...\n. cmake should fail here if Eigen3 is not available. instead of failing during compilation with a static_assert\nbad design decision here. you should use:\nFIND_PACKAGE(Eigen3 3.1.2 REQUIRED) which will fail cmake itself if eigen is not available and you want to set SetLinAlgLib to be eigen.\nbut again i have very big concerns having linalg backend set during configuration/compilation\n. same as above... use FIND_PACKAGE(... REQUIRED)\n. as mentioned above\n. same problem\n. same problem as above\n. same problem as above...\n. again the same... btw don't you feel that you are basically copy-pasting functions that could be defined by a cmake macro/function\n. again....\n. very bad namespace naming....\n. a possible collision of namespace naming as it's highly likely that ever somebody will actually use the same namespace name...\n. you should not even have an enum defined if the supporting backend is not available...\n. and what happens with eigen3 is not available? no linalg backend at all?\n. bad macro naming....\n. seriously? this is why we have cmake/configure at the first place to detect this....\n. again very bad design decision here...\n. i'm sorry but this header is a horrible hack of macros.... this should be seriously refactored.\n. afaik it should be with $\n. i would set num_bags = 10 by default as anyways num_bags = 0 wouldn't do anything... \n. use clang as it's faster...\n. why do you need the following packages:\n- libqt4-dev libgtk2.0-dev \n- build-essential you should already have all the packages to build things with c++\n- python-dev python-numpy for what?\n. is this actually a bad copy-paste?\ndo you know what -DTRAVIS_DISABLE_UNIT_TESTS=ON -DTRAVIS_DISABLE_LIBSHOGUN_TESTS=ON stands for?\n- -DTRAVIS_DISABLE_UNIT_TESTS=ON disables gtest based unit tests, hence you don't have unit testing....\n- -DTRAVIS_DISABLE_LIBSHOGUN_TESTS=ON disables the building and running libshogun (c++) based examples.\n. this i find very-very bad way to do things.\n. noise factor is not being used at all in NumericalVGLikelihood for sure?\n. yes but in CVariationalGaussianLikelihood m_noise_factor is defined as private so you won't be able to access it from CNumericalVGLikelihood unless you define it protected....\n. but of course if you know that m_noise_factor of CVariationalGaussianLikelihood will never be used directly from the derived classes then you it's safe to keep it private.\n. i don't see any update... what update you mean?\ni just see only one commit in this PR\n. sweet thnx! :)\n. what's this left in comment?\n. why?\n. @iglesias you mean 0xDEADBEEF ? :) that's a good seed, no? at least easy to remember :)\n. yeah try to use std:string instead of simple char*\nyou can always convert it by .c_str()\n. is this math.h pow? because in that case this should be corrected....\n. exp is from math.h? again this is wrong if that's the case...\n. mmm let's not do this... :)\ni.e. remove the whole option BUNDLE_EIGEN and when NOT EIGEN3_FOUND then just simply include the external eigen...\n. this error doesn't make too much sense... :)\n. doesn't make much sense anymore, because previous it has been either found or bundled.\n. same here as above...\n. i'm not 100% if this is set when you bundle, needs to be doublechecked...\n. same as above.\n. same as above\n. again...\n. further more c has a nice ternary operator, i.e.:\nresult_multiplier = (result_multiplier<=0) ? 0 : pow(result_multiplier, power);\n. calculating exp(-result) in case of result_multiplier=0 is totally unnecessary, i.e. wasted cpu cycles....\n. CMath::pow and CMath::exp, but i'm hoping that the linalg backend would have a function for it as well.\n. that exp calc is still unnecessary if result_multiplier==0, right?\n. you can check the generated asm whether it is optimised, but i'm pretty sure that it wont.\n. btw you can switch back to\n```\nif (result_multiplier<=0)\n   return 0\nreturn CMath::pow(result_multiplier, power)*CMath::exp(-result);\n``\n. i reckon the indentation here is not really following what we use in shogun.\n. it's not 80s anymore. please use appropriate SIMD operations to do these things... or just use eigen.\n. specify openmp pragmas\n. linalg or sgvect not cmath plz\n. parallelize\n. parallelize\n. parallelize\n. certainly simd\n. what are all these indentation changes?\n. isn't this c++11 only?\n. add extra empty line as github says\n. yes because i dont see anifdef` for it\n. why do you need these?\n. @karlnapf linalg requires c++11... imo it's a bit too strict requirement \n. yeah, still dont do this on old code... just on the code you add.\n. This is not improving this is breaking history of the code... Which makes traceability of a given change quite cumbersome. \n\nOn Mar 12, 2016, at 15:50, Kunal Arora notifications@github.com wrote:\nIn src/shogun/features/DotFeatures.cpp:\n\n@@ -358,19 +365,19 @@ void CDotFeatures::benchmark_dense_dot_range(int32_t repeats)\n            dense_dot_range(out, 0, num, alphas, w, d, 23);\n#ifdef DEBUG_DOTFEATURES\n-    CMath::display_vector(out, 40, \"dense_dot_range\");\n  @vigsterkr any particular reason why we should not try to improve existing code, even if it is just indentation? Am I missing something here?\n\n\u2014\nReply to this email directly or view it on GitHub.\n. yes yes this should never come back :)\n. mmm why a static, why not a settable property of LaRank?\n. please refrain from whitespace refactorings (see this whole unittest patch)... it just makes git bisecting more complicated. just make sure that your patches are following the codestyle requirements\n. indentation on this is wrong\n. same here\n. 2 problems:\na) fixing the cereal version\nb) should do this here :)\n. why do we need this? we require EIGEN, so we eliminated this... same below\n. why?\n. why std::vector?\n. please avoid committing indent changes... when the code isn't yours\n. this is a NOP, plz avoid doing changes like this\n. \u2b55 \n. this should rather be like when looking for Eigen..... if it's available on the system use the system one, if not, then download it.\n. remove this....\n. once you are done with debugging remove this\n. no need for this\n. please do check the content of this directory...\n. CMAKE_BINARY_DIR please\n. so this should work\nSET(CEREAL_INCLUDE_DIRS ${CMAKE_BINARY_DIR}/Cereal/src/Cereal/include)\n. no need for this\n. you can remove this...\n. no need for this\n. this should be generated somehow....\n. i reckon we require much more python packages than numpy.... although maybe if every testing is turned off then it's not\n. coulnd't we omp pragma this?\n. shouldn't we at least allow a little bit of error/epsilon from 0.0 ?\n. mmm this is basically a scaling of the centers_i column with weights_set[i];\nyou should use a vector operation on this.\n. i know that this is just the code moved from the original implementation, but what's the use of including pthread? couldn't find any pthread calls...\n. what happens if we dont have linalg lib?\ni mean in that case we should at least warn the user that kmeans++ wont be used or something...\n. dont forget the documentation of that method\n. please use variables instead of magical constants like 6 and 4...\nso lie define:\n\nint32_t mat_rows = 6;\nint32_t mat_cols = 6;\n...\nafter that you can refer to those variables + changing the unit test from 6 to 60 would be easy :P\n. we should add it if it isn't added yet, but centers is an SGMatrix right? you can easily just define the region of area where the column vector is and do the scaling, or?\n. this transposition of the input matrix should be handled by eigen or linalg... doing this with a double nested loops is soo... 101 computer science\n. so here basically we X^T^T, i.e. double transpose the original input..... so we could just use the input itself right? :)\n. that map_X.transpose() = Xorig (the original input)\n. but that lapack thing wasn't even doing this... it was that manual nested loop in the beginning of train_machine that created this :P\n. DoubleMatrix is RealMatrix, see above...\n. why don't use just have 2 maps?\nmap_X and map_Xt\n. can't we just pass around SGVector/SGMatrix instead of going full into eigen?\n. @Saurabh7 maybe this is the reason why in the initial implementation there was that transposition of the input matrix. but it'd be good to actually use a more optimal solution for matrix transposition than nested loops.\nall lapack/blas functions has TRANS function arg that will let you do the operation on the original or the transposed version of the matrix. would be interesting to see the cache misses there ... :)\n. forgive my ignorance but if we have CPUVector and GPUVector then why do we need onGPU function. since the class could identify the storage or?\n. well instead of dynamic casting see the SGObject get_name virtual function. that's a more general way to identify the type of the object without dynamic casting.\n. yeah that would be more ideal indeed.\n. why do we generate at all GPUBackend class if there's no gpu backend? i'm sorry but i'm very very confused what's happening here...\nyou should implement a GPUBackend using for example viannacl IF it's available. if not you just don't create one. like many other models, that if LAPACK was not present, the implementation was not built. \n. why close here the #ifdef HAVE_VIENNACL ?\ni mean if you don't have viennacl this whole implementation is just foobar.\ndo it like\n```\nifdef HAVE_VIENNACL\n... the whole GPUBackend implementation...\nendif\n``\n.NaNin best case....\n. not sure if i understand :)\n. ah but if it's an int type why do you force a type cast with0.0? :)\n. @Saurabh7 are you sure it's an int type:SGVector weights_set?\n. is2^31 = 2147483648i.e. 2.1 billion points enough? :) i mean in case of a big data set it could be more than that or?\ni'm just suggesting that maybe usinguint32_tor ratherint64_toruint64_twould be better choice, or?\n. this should be rather++weights_set[min_cluster]`\n. just as above:\n--weights_set[cluster_assignments_i]\n``\n. i know that it should be a different PR, but maybe it'd be good to think about whether we really only wanna support vectors ofSGVector, i.e. why notSGVectororSGVectoretc.\n. only thing: a/operator (division) is usually much more costly than multiplication...\nso precomputingx = 1/weights_set[min_cluster]` and then just do\nvec[j]-x*centers(j, min_cluster))\nof course it'd be good to check what does the compiler do when you do -O3\n. see above (diff between / and * operation)\n. isn't weights_set[i] = the number of elements assigned to the ith centre?\n. ah ok yeah then rather use int64_t\n. this sounds like a SGVector<float64_t> no?\n. shouldn't this be then map_Xr*map_y?\n. mmm use a macro or a static const for magical constant values \n. again why not to use map_Xr*map_mu here instead of map_X.transpose()*map_mu?\n. do we really need this?\n. doxygen is still missing\n. any reason for example why LARS shouldn't be able to work on SGMatrix<float32_t> ?\nbut yeah this should be addressed in a consequent PR ;)\n. why the double section?\n. @lambday why wouldn't it be working? you are using malloc; the alignedness is only about the size nothing else...\n@OXPHOS please use SG_FREE and not direct memory functions like free\n. why is thins ifdef still here???????\nonly check whether gpubackend is registered.... nothing else\n. wie so?\n. have fun reading:\n- https://nativecoding.wordpress.com/2015/06/19/multithreading-multicore-programming-and-false-sharing-benchmark/\n- http://stackoverflow.com/questions/18236603/cache-lines-false-sharing-and-alignment\n- http://protheus.net.au/cache-in-a-multi-core-environment/\nand of course http://lmgtfy.com/?q=false+sharing+cache#\n. do we need this? i mean wasn't the idea for post GSOC stuff was that we will require c++11? @karlnapf \n. weird alignment... and shouldn't we alignas?\n. nooooooooooooooooooooooooooup. c++11 has waaay better templating engine to avoid this shitty macro ;) ping me on irc\n. miiiijaaaauuuuuu no\n. \ud83d\ude3f \n. nooooooooooup :)\n. i guess you knoooow what comes here... ? :)\n. indent is where my cat is ? :)\n. indent to the SKYYYYY\n. ping pong pang!\nso indent better basically ;P\n. a header defined + declared function is inlined by default... i.e. that inline is just redundancy\n. why doesn't this class have a destructor? :)\n. release me when we die \ud83d\udc08 \n. mmm this header (LinalgBackendEigen.h) will never be included anywhere outside of linalg implementation right?\n. we do have SG_ERROR or SG_SERROR or SG_PRINT... instead of std::cerr... just sayin'\n. same as above\n. why does it only can be a float64_t? and not a float32_t or a floatmax_t?\n. for fun and profit: http://stackoverflow.com/questions/10552280/fast-exp-calculation-possible-to-improve-accuracy-without-losing-too-much-perfo/10792513#10792513\n. mmm will clang/gcc in -O3 optimize this into 1/get_width() being a const and just do a CShiftInvariantKernel::distance(idx_a, idx_b)*<that const>?\n. see below about fastexp\n. actually why not? :)\ni know why in a way but is it really something that wouldn't like to have? :P\n. or possibly just for being consistent it might be better to remove the ::zero() method and have it in the linalg::<backend>::zero(SGVector<T>& v);\n. same thoughts as above.\n. we are still doing zeroing... as @karlnapf said this interface should slowly die.\n. is this like c++ <memory>? why/how/where?\n. this should be implemented in a different file i reckon... like SGLinalg_gpu...\n. hardcoding that a random forest can only work with CDenseFeatures<float64_t>\nwhich in case of a decision tree is very very crippling effect as they should handle categorical values easily.... \n. why this extra whitespace?\n. please make it sure that we can handle CombinedFeatures where we can use both Dense and String features... remember it's a tree!\n. why this whitespace change? :)\n. you could define a typedef for this, like CDenseFeaturesEigenMap:\ntypdef Eigen::Map < Eigen::Matrix< ST, -1, -1, 0,-1,-1>, 0, Eigen::Stride<0,0>> CDenseFeaturesEigenMap<ST>\nand shoudn't we return a reference?\n. i reckon this is typedef-ed in SGVector:\nhttps://github.com/shogun-toolbox/shogun/blob/develop/src/shogun/lib/SGVector.h#L40\nyou could use that from there, it's a public definition.\n. use uint32_t please\n. why the template param if you dont use it below?\n. use index_t\n. i'm not sure if i understand. get_feature_type() and get_feature_class() functions will give you the exact type of features you are dealing with.\nthis should be a rather easy patch to add support for other types than DenseFeatures\n. yep yep we've talked about this with @Saurabh7 \nwe should finish up with this PR as a performance fix, and then work with further improving trees/forests.\n. yeah but even if it's a map the content is copied if it's not a reference.... you dont want to copy anything since you just want the one obj to be passed around, or?\n. @c4goldsw :+1: for that\n. mmm why isn't this virtual if in the parent class it is virtual?\n. @OXPHOS if you removed HAVE_CXX11 in init.cpp why keep it here? :)\n. same as above... just be consistent and that's all.\n. no need for that little space there.\n. use float64_t\n. int32_t\n. and there we go with C++11 full throttle? :D\n. you mean int32_t?\n. float64_t ?\n. same as above\n. same as above\n. please try to use the types that we use throughout in shogun...\n. it's the user's choice... we can create helper functions if we want, say saveJSON, saveXML etc. but the idea here is that the user could define it's own Archive type as well.\n. so are we sure that the mean of a SGVector<int32_t> should be expressed as an int32_t and not at least a float32_t? because that's not really a mean... \n. i wouldn't do it like this, as said i think in a chat or somewhere; i would still expose the cereal_save with template but as well give implementations like this:\nvoid saveJSON(char* filename) const;, void saveXML(char* filename) const; etc. and have the corresponding pairs for loading. and you yourself create there cereal::JSONOutputArchive etc. so this way the simple user can just choose from one of the save* functions, whereas the more advanced user would still have the option to use the template...\nthose wrappers you can implement in the SGObject.cpp\n. yeah but why not just:\nvirtual float64_t mean(const Container<Type>& a) const?\n. great! so in the same manner, lets add support for all the default cereal output formats: xml, binary.\n. double include.... guessing by mistake\n. @karlnapf that was my first idea as well, but then think about all those interpreted languages, like python :)\nfrom modshogun import Class, EnumForSerialization\na = Class()\na.save(\"to_file.json\", EnumForSerialization.JSON)\nseems a bit more shitty than\nfrom modshogun import Class\na = Class()\na.save_json(\"to_file.json\")\nor?\n. true but think about performance as well... what about having the option for both?\n. so think about\nfloat64_t x[2048];\nfor (size_t i = 0; i < 2048; ++i) {\n   x[i] = function(x[i]);\n}\nthis is kind of the logic of inplace right? when you do this you do a read x[i], write x[i] and then read [i+1] so forth and so on... since you do a write x[i], it'll have to go into the RAM that will take:\n a) time\n b) will invalidate the L1 cache\nso yeah it's gonna take much more time than:\nfloat64_t in[2048];\nfloat64_t out[2048];\nfor (size_t i = 0; i < 2048; ++i) {\n   out[i] = function(in[i]);\n}\n. first, what is the weird whitespace between and and first.self->classes\nsecond i would assure parenthesis, like\nreturn (first.self->description == second.self->description)\nand (first.self->classes == second.self->classes);\nand have you checked that the conditions for bool operator== of undorder_map holds for Any: http://en.cppreference.com/w/cpp/container/unordered_map/operator_cmp\n. you should try to use gdb or lldb and add a breakpoint here and step through it, to see why it doesn't actually gets true.\n. no you are getting this a bit wrong. no need for casting or anything, every interface (complex vs rest) should just call the right function. so in other words when you have:\ntemplate <typename T, template <typename> class Container>\ncomplex128_t mean_impl(const Container<T>& a) const;\nthen it should actually call:\ncomplex64_t sum_impl(Container<complex128_t>& a)\nor use enabled_if, but don't enable this for every T.\nyou should not define a generic:\ntemplate <typename T, template <typename> class Container>\ncomplex128_t mean_impl(const Container<T>& a) const;\nit should only be defined for T = complex128_t.\n. hohohoooooo! a goooood one :P welcome back amigo :)\n. please make sure that you delete the cereatltest_sgvector.cereal file before exiting this function:\n- anticipate IO exceptions...\n- please use temporary files (see the other serialization unittests) to avoid any possible inconsistent state...\n. btw dont forget to decamelize these ones :)))\n. use EXPECT_EQ as the ref_count is an int32_t\n. check the ref_count value\n. i would add a test that creates a vector call SG_REF on it 2-3 times, serialize it and then check the deserialized vectors ref_count value...\n. do we really care the name of the file? no\n. - use it as in the example of the api i gave you:\nstd::string filename = std::tmpnam(nullptr);\n. when an exception happens, the std::remove will not be called, so you will need to call it in the catch section as well.\nno need to narrow down to io exceptions only, catch all of them by:\ncatch (std::exception& e)\n. change here the filename to std::tempnam as well, as well as wrap it around with try/catch\n. btw end here the try block... dont catch the GTest related exceptions...\n. using std::string would have the advantage of dtor => no mem-leak :)\n. please try to be consequent about naming variables: USE_* or HAVE_*... and why would you need this macro in C?\n. this just wrong... why to test it when you dont compile the tests or the modular interfaces?\n. no need for this. you already have a check for modular to detect swig...\n. and please check whether the jinja2 was actually found... and then set a variable.\n. @OXPHOS the 'necessity' of allocating the memory advance is because some people want to actually have full control of the mallocs happening within the code\n. ?\n. ok i understand this... buuuuut and here comes the pain part: in the top level CMakeLists.txt you do this:\nhttps://github.com/OXPHOS/shogun/blob/86425a27566deec4f8eb065af5526f23571a031c/CMakeLists.txt#L595\na.k.a. what we did for eigen. if you end up doing the internal dependency to have this working (above) you would need to ship cereal with shogun in the header directory... which is quite a problem.... only way to solve this if we could add all the cereal related implementations/definitions (declaration can go into headers as that can be easily solved) into .cpp\n. ok so i wonder how a (SGVector<T>)false cast is working out. but yeah we have a problem with the API itself here, because how can we return an SGVector<T> if the operation is not successful\n. @OXPHOS how many other places we do a trick like this?\n. false is good... better to be safe than sorry :P and anyhow it's almost as if you put it into else branch...\n. it's quite painful to say this but this is a good place for goto :P\nbut then again you wanna throw the exception :)\n. i know why... the question is do we have APIs that do:\nSGVector<T> some_api() {\n   if (some problem occures, hence we cannot return any sane SGVector<T>)\n}\n. you would need to move this as well :)\n. noup this should not be here :S\n. if ../ then please use \"CerealObject.h\"\n. boost? we dont have boost :)\nstd::optional is just a c++17 feature, that doesn't mean we cannot have it, question what are the compilers atm that support optional?\n. what do you mean not achievable? i mean that during your linalg refactor, have you had similar cases? could you maybe go through the changes pretty please? :)\n. the move of this was required because of the FIND_PACKAGE(Jinja2)?\n. i know that ${COMMON_MODULAR_SRC_DIR} has a trailing / when it's being set but having ${COMMON_MODULAR_SRC_DIR}shogun-base.i.py seen makes me feel... mmm so it's not looking good. does the  ${COMMON_MODULAR_SRC_DIR}/shogun-base.i.py really hurt?\n. you could just append shogun-base.i to the ${modular_files}, and call ADD_CUSTOM_TARGET once.\n. couldn't we use any random natural number?\n. random?\n. random size\n. random values\n. for relative includes please use \"\", but why do we include MockObject.h from (unit) tests into the code itself?\n. what's TagRealVector and RealVector are like? and why not the same? :)\n. you should rather fix this problem then in FindJinja2.cmake, i.e. put in the beginning of that script that for being able to run it you need FIND_PACKAGE(PythonInterp REQUIRED)\n. @OXPHOS what does SGVector<T> null mean? :) but ok if en exception is thrown the return value can be garbage... it's not that nice but acceptable\n. no need for this in .cpp... swig uses headers only\n. \"\" makes includes relative... sooooooo #include \"CerealObject.h\"\n. why do you need this? why the forward declaration was not enough?\n. this works atm just because you include cereal header in shogun/lib/common.h which should be removed.... so you need to do a forward declaration of Archive there instead of the include to make this work.\n. this is still included without guards for testing...\nand could you please use #include \"...\" instead of doing some magical relative path hacking for #include <>. note: read the manual of header include\n. does this actually work?\n. why do you need this?\nor rather, why dont we use the save_json and load_json methods?\n. our current kd tree only works with Densefeature<float64_t>?\n. yep yep here me being stupid again! :)\n. you can use Random.fill_array to fill a vector fast with random values (it's using SIMD instructions).\n. the rotation matrix is independent of the tree being used, right?\nin this case it'd be better to move the implementation into a higher class so that any rotation forest/tree implementation would be able to use it instead of re-implementing it.\n. btw: here according to the paper the random should be sampled from a standard normal distribution, i.e. mean = 0, stddev = 1. while CMath::random is just a sample from uniform random distribution\n. yeah i just realised later that it requires a std normal distr. random. Please use sg_rand or CRandom directly instead of CMath wrappers as they are gonna get deprecated sooner or later\n. @OXPHOS this is a method that we would like move to linalg backend; would be great if you could join in the discussion where to put it exactly and how\n. @Saurabh7 you are writing the exact same lines of code when applying the rotational matrix to the original data matrix in case of apply_regression and apply_multiclass, right? in this case it would make sense to have a function called apply_rotation or something as its unnecessary code duplication.\nthe other thing is that it would be great to port this part of code (transpose, multiplication of matrix etc) to the linalg backend. i believe everything you need for matrix manipulation is there already. right @OXPHOS ?\n. @OXPHOS wouldn't a rotational_matrix be a better name? as that api doesn't actually rotate a matrix :) but gives back an operator that'll rotate another matrix. :) other than that it's a good plan! @Saurabh7 can you start working on this?\n. yes we would like to have GPU part as well, it's not urgent though but yeah would be great to have a plan how to do it :)\n. if we'd to go with a:\ntemplate <typename T>\nvoid rotate_matrix(int32_t n, SGMatrix<T>& m);\n{\n    REQUIRE(n>0, '');\n....\n}\nwe could infer the backend or? :)\n. @oxphos just do the same as in case of eigen:\n 1) create A square matrix (n*n)\n 2) initialise with random values from a standard normal distrib\n 3) QR decompose this matrix\n 4) then let M be: \nM = Q * (qr.matrixQR().diagonal().array() < 0).select(-ones, ones).asDiagonal();\n5) depending on the determinant of the M flip the first column values.\n@Saurabh7 btw couldn't we do the the last step (negating) as a result of a scaling (with -1) of the given column? hence this could use some vector multiplication backend, i.e. SIMD?\n. do we really need iostream?\n. i would hope that there's an eigen matrix operator for doing this summation, or linalg by chance?\n. this definitely could be done simpler.. .like a memcpy? :)\n. you dont need this..... you can remove this if you dont use pthread anymore. because you dont use any openmp function in the code below.... have you actually tested this? as i'm afraid this is not going to work as it was previously, see the last item of params is never gonna be set.... please take more time to actually see through this patch and test it first locally (run the unit tests and integration tests).... @piyushgoel997 dont get me wrong here but you are sending code without even compiling? even long time developers of shogun would have a hard time adding new features like that, let alone a person who has never touched shogun's code.\nhere's a manual how to develop shogun in general: https://github.com/shogun-toolbox/docs/blob/master/DEVELOPING.md\nand yes that doesn't work for windows as till 3 hours ago shogun was not supported on windows. but still... you will need linux do get started with shogun... there are many ways to get a linux running under windwos, please use google to find out how to do it.\nyou can develop things on windows as well but you'll need Visual Studio for that... and some other things, like cmake. this aint' going to work until you dont add scala runtime to the shogun docker sdk. there's absolutely no need to do update and apt-get install scala; the latter would actually would install trusty's scala 2.9... . why the space between :: clas...?. man, not only the implementation should be in a separate file but as well the definition.\nmoreover, the naming of these different KNN solvers are very misleading, as CBRUTESolver is not a general brute force solver, it's just solves the KNN problem itself in a bruteforce manner... please incorporate this in the naming of the class as well the placement of the files.\nsame is true for the rest of the solvers.... why are you passing around float64_t* and int32_t when we have a wrapper class for vectors (SGVector) and matrices (SGMatrix)?. same here as above... + please use const where it's applicable.  see http://en.cppreference.com/w/cpp/language/cv for details. same as above. same as above. what's the point of this dummy implementation of the this function?\nplease use correct c++, i.e. in this case define the KNNSolver to be abstract => define the classify_objects function pure virtual. what's the point of this dummy implementation of the this function?\nplease use correct c++, i.e. in this case define the KNNSolver to be abstract => define the classify_objects function pure virtual. does this really need to be public?. instead of int32_t please use index_t. SGVector?. instead of int32_t please use index_t. instead of int32_t please use index_t. => index_t num_lab as well.... => index_t m_k as well...\n. use SG_DEBUG instead of DEBUG_KNN\nmake sure that before doing the nested for loop check the msg_level. index_t i. index_t i. index_t i. index_t i. SGVector. SGVector. index_t i. index_t i\n. what's that whitespace in the scope definition?. All these nested for loops in all of the classify_objects_k is:\n * essentially is the same across all the solvers, they are just depending on different inputs\n * really ugly; please think about refactoring it to use matrix/vector operations, that way the operator can be optimised later for parallel execution.. yeah i dont like these noob PRs... sorry that's my opinion. none the less there are many other things to be addressed within this PR.. @MikeLing you can implement there a NOP function if need be. or put a SG_NOTIMPLEMENTED macro there... the problem is that this way KNNSolver can be actually instantiated, which is wrong.... CBruteKNNSolver. why am i right that this function does not change the object itself (parameters of it)?\nbecause then you can define the whole function to be const.... i'm not so sure if i follow this description....\nand where are all the parameter descriptors?. same as above write a proper doxygen comment. what happened with the whitespacing.... whitespacing..... would a classification of input objects ever change the content of the model?\nif not then define this function to be const.. we might misunderstand each other, i meant why not to define the function itself const, like:\nvirtual int32_t* classify_objects_k(CDistance* d, const int32_t num_lab, int32_t* train_lab, int32_t* classes) const = 0;. couldn't this be:\nconst int32_t choose_class(float64_t* classes, const int32_t* train_lab) const;\n?. isnt this virtual?\nand couldn't the whole function be const:\nCMulticlassLabels* classify_objects(CDistance* d, const int32_t num_lab, int32_t* train_lab, float64_t* classes) const;. isnt this virtual?\nand couldn't the whole function be const:\nint32_t* classify_objects_k(CDistance* d, const int32_t num_lab, int32_t* train_lab, int32_t* classes) const;. index_t still.... this is still not fixed.. index_t. please read manual: http://en.cppreference.com/w/cpp/language/virtual. noup this is not correct... you wanna check the current log_level on runtime, not on compile time.... @MikeLing noup i didn't mean that at all.... and yes with your code that's the problem what you have just explained after buuuuuut that's why i said,\nyou wanna check the current log_level on runtime, namely:\nif (io->get_loglevel() == MSG_DEBUG) {\n/* debugging code */\n}. you could actually use:\nio->get_loglevel() <= MSG_DEBUG. virtual. virtual. the initialised solver is never released actually.... hence this will leak memory. what happened with the whitespace here?. this can be removed..... whitespace?. parent class' (KNNSolver) constructor is never called...... parent class' (KNNSolver) constructor is never called...... virtual?. parent class' (KNNSolver) constructor is never called...... parent class' (KNNSolver) constructor is never called...... virtual. parent class' (KNNSolver) constructor is never called...... virtual. parent class' (KNNSolver) constructor is never called...... whitespace.... parent class' (KNNSolver) constructor is never called...... parent class' (KNNSolver) constructor is never called...... virtual. guard it with HAVE_OPENMP. but you dont need it because for finding out num available threads you should call Parallel singleton.. parallel->get_num_threads(); instead of omp function.... what happens when you dont have openmp backend?\nhow will this function work at all when thread_num = 0 always, as you only have one thread...\n. these nested loops are a bit scary.... same story as above..... same again. mmm ok my bad as then step = total_num, right?\nif so just guard the code with HAVE_OPENMP, something like this:\n```\nifdef HAVE_OPENMP\nint32_t thread_num = omp_get_thread_num();\nelse\nint32_t thread_num = 0;\nendif\n. yes it is. `parallel->get_num_threads()` just returns the detected number of cores or what the user set it to.. yeah it was scary in the first place... i mean if there's no more elegant way to do it, of course leave it.. yeah you can do that... again that then requires some macro guarding.. fyi: most of the travis jobs failed because that version of clang has no openmp:\nhttps://travis-ci.org/shogun-toolbox/shogun/jobs/200335452#L1986. mmmm i'm not so sure if i follow.\nyou can simply manually run valgrind on the knn examples/unit tests and you'll see how it is leaking.. please call the ctor of `CKNNSolver`..... same, why dont you just call the default ctor in of `CKNNSolver` and call in that the `init()`, why explicitly like this?. same as in previous cases. same, why not the default ctor, like:\n        CBruteKNNSolver() : CKNNSolver()\n        { \n                      init ();\n                 }\n. @MikeLing this remains to be resolved. @MikeLing this remains to be resolved. @karlnapf why do you say that `CLock`? i mean yeah in this branch which is not rebased for ages it is still that but latest develop branch has better backend for locking. @karlnapf no because if you check Parallel it is active when either openmp or threading backend is available ;) and we want to activate this when openmp is available.. @karlnapf wtf you mean it's not thread safe at all?\n. it's one thing that datastructures are not thread safe... but this is something that has to otherwise things gonna be really fuckedup.... yeah so think about to_gpu being totally not threadsafe... that is something that you dont want to have on user code side... what you will mutex it constantly around the checking for on_gpu?. peeeeeeeeeeeeople `DISABLED_INTERFACES`. no return value, hence CI is failing. return value is missing.... please write more description in the doxygen comments, especially about the parameters.... why do you pass native `float64_t*` arrays, when we have `SGVector<float64_t>` for such things.... and please specify whether an input argument should be `const` or not... like most of the integers could be.... same as above. extra whitespace. extra whitespace. does this really need to be a pointer? you are not using lock as a function arg anymore anywhere afaik. #pragma omp critical section would be muc more adequate here, but i would rather refactor this in a lock-free way..... and use maybe `#pragma omp parallel for reduction(...)`. so now that you are doing this kind of changes, and you promised in the past PR of KNN refactor that you gonna do the changes i requested there please do it here:\nstart using `SGVector` instead of native arrays.. convert it to use SGVector. SGVector. SGVector. SGVector. SGVector. SGVector. this is missing it's `SG_UNREF` pair on the end of the for loop, when `(get_global_parallel()->get_num_threads()==1)`.\nm_on_gpu.store(false, std::memory_order_release);\n. same as above. same as above. same as above. same as above. same as above. again make sure about the order.\nm_on_gpu.store(((SGMatrix*)(&orig))->m_on_gpu.load(std::memory_order_aquire), std::memory_order_release);\n. move the lock part into the else branch.... move the lock into the else branch. index_t. index_t. indeed you are right!. this should be `index_t` just as any indexing variable....\nyou can do `++i` and `++idx_start`. this should be `index_t` just as any indexing variable....\nyou can do `++i` and `++idx_start`. all indices should be `index_t`. all indices should be `index_t`\n. if it's and index it's an `index_t`. why do you need this header?\ni might missed it but i dont see anywhere in the code omp API being used, or?. if you wanna get this inlined use `SG_FORCED_INLINE`, like:\nSG_FORCED_INLINE bool operator==(const SGMatrix& other) const\n. as previously mentioned could you wrap this with:\nshogun::memcopy(...)\n``\nand implement that insrc/lib/memory.h. @lambday i've seen mostly memcpy performing better than copy... (depends of course on the libc and compiler...). btw maybeshogun::memcpyis better ... just to be consistent. yeye go ahead with templating it!\nheader and inlined definitely... :). sounds like a plan but i would do this like on-demand... so implement create those wrappers when they are actually required.... or do you use somewhere copy_forward and copy_backward?. again if you have a use-case for it go ahead... meaning if you already use it somewhere then add it .... @abhinavrai44 i dont get it.. where? if you refer to#pragma omp...that's not an omp API per se... it's a preprocessor directive and for that you do not need to includeomp.h. i wish we would could put this into a utility part of the unit tests... would it be too much to ask to do it? we keep repeating these type of codes all around unit tests.. @lambday she merged linalg refactor yesterday into develop :) i think this slipped everybody :). @OXPHOS push the fix directly to develop and @lambday will rebase over that. :). @lambday no hurry... prefer the camelized notation over that weird mixture of camelized and not... . @lambday ok so i guess this needs rebase & replacing here the std::copy and then it's good to go!. @lambday for future reference: if you would be doing this in a feature branch of shogun repo i could do these changes and merge myself the feature branch... but like this unfortunately i cannot... the only option would be to cherry pick your repo's feature branch ....... so yeah please since you have write access on the main repo just do the feature branch within that.. so this logic is being copy pasted around the whole unit test codebase.\nit'd be great that if you could move this into a utility method and somehow either be able to refer to it as a fixture in the unit test, or at least have a method that would just return thefeatures_train,features_testandground_truthinstead of constructing it here in the code. not only but as well#ifdef USE_GPL_SHOGUN. why do we care about being single threaded?.index_t. a simple space should be enough. removeHAVE_CXX11as we now require the compile to support c++11 features.\nand please simply remove the pthread part...\ni.e. only leave the c++11 codebase in the implementation. do not leave commented code in... this is for all the commented out lines. i'm not so sure if i understand this tab here... please fix your whitespacing.. the while loop you check the value ofex_used[current_write_index]not in an atomic matter.. i mean yeah the value of your index, i.e.current_write_index` is atomic, but the ex_used could be changed in another thread... or?\nint32_t current_read_index = ex_read_index.load(std::memory_order_acquire); // for synchronisation\npart could be just simply substituted with a\nstd::atomic_thread_fence(std::memory_order_acquire);\nas you dont use current_read_index value before the loop anyways, or?. please check your whitespacing. these comments can go... we know from the git diff what has changed. no need for support of pthread. @tdjogi010 i really see no point... . @tdjogi010 cool, thnx... this is still an open question though.. data/matrix generators should go into a generator class in unit testing :). i reckon there should be a more elegant way to do this... this is kind of a bruteforce solution. btw have you profiled it, whether this actually results in a speedup (or maybe a slowdown?)\ndefinitely fences should be used since you dont use the values that you load. although i'm thinking whether somehow using exchange could be used here..... please add #error here.... just in case if one day somebody fixes the rest of the cplex code, but forgets this part :). why no nulllptr?. override?. i'm not so sure if i follow... clone is virtual and it needs to stay virtual to override... in c++11 it's override if you like, either way what you do now is not good :) imo. please do one or the other in order to be consistent with the rest of the codebase.... whoooooooooo although enum seems like a super simple solution, but i reckon having a separate function for it would be better.... think about how the function that handles the enum will grown and grow and grow :). yeah this is starting to be more elegant.... profile = speed benchmarking. you could use any benchmarking tool for that. i mean just to be clear the reason to switch here from the conditional var/mutex is to speed up the parsing procedure... . mmm no:) get a big file (generate one) and use it on the streamingfile\n@lambday any chance you have a big input file for streamingfile?. please read the manuals... i.e. os.getenv('SHOGUN_DATA_DIR', '../../../data'). os.path.join instead of simple concatenation of strings. os.path.join. here change the interpreter to:\n```\n!/usr/bin/env bash\n``\n. just removebashand make sure thatcheck_format.shis executable and let theenvfind out where bash is ... if available.. can you remove this and the one below :). there's no need for this anymore right?.BinaryLabelData, there's no plural form ofdata. since you are using gaussians for the data, this should be reflected in the name of the class i reckon. so something like:GaussianCheckerboard(num_samples, num_dims, num_labels);. why only when GPL is allowed?. why do you require LAPACK?. why do you require LAPACK?. mmm oh that's on the end of the file (sorry haven't seen it)... yeah we should change this :( although the problem is not only DataGenerator but as wellGaussian` distribution implementation as that one is required to have LAPACK as well.\nactually this should be a more pressing issue, i.e. convert Gaussian to use modern linalg backend instead of LAPACK. are you up for that @MikeLing ?. do we need this?. please use a different name, dont use cblas_dger. same here.... use a better name for the function cblas_dgemv. you  mean:\n@param alpha scaling factor for vector x. first word after the @param should be always the name of the variable in the function description. isn't this a matrix? (2nd parameter of the function). this should be\n@param x vector. same here as above... the doxygen needs to be fixed. ok so let's just have it without the cblas_ prefix as @lisitsyn suggested... so dgemv . i dont think we need this anymore, or?. where's the implementation of this?. this only works if num_labels = 2, i.e. one cannot have a multiclass dataset in this case, right?. @MikeLing no. first of all num_dim should be anything as the Gaussian will give you the right dimension (if you set it right).\nand GaussianCheckerboard should be able to generate a multiclass problem... so num_dim > 2 should be supported... it's only that you need to create and sample num_dim amount of gaussians, and each of them will have a different label... so in case of num_dim=2 and num_labels=10 it would look like a checker board (this is how the means of the gaussians should be set) where each 'square' has a different label. i've explained in my other comment why i don't agree with this, and would like to support multidim and multilabel. typo.... due to this as it was mentioned by @karlnapf in https://github.com/shogun-toolbox/shogun/pull/3738#issuecomment-289268657 that be great that our unit test somehow tests both multi and single threaded scenario.... same typo here.... this will not work if num_labels > 2. what if num_labels > 2. same as above. this should be CLabels as you would like to support multiclass as well.... pls copy here the bsd license. could you elaborate the part \"doesn't work so well\"?. mmm i think we can remove this as is.... i mean what is the terminal size when this will be a problem? . float64_t or?. same as above. license is missing :). the problem with these eigen mapped matricies that in case the linalg backend is on gpu these will all fail, as feature matrix is not on the CPU's mem. :(. we could address this later in a different commit, but we should at least mark here that it's a TODO. yep. if there's missing functionality it's worth to ask @OXPHOS how to go around implementing those.. operator++ works and it's atomic:\nhttp://en.cppreference.com/w/cpp/atomic/atomic/operator_arith. you need to initialize dim1_size dim2_size and dim3_size, otherwise it'll contain some random values and that's why the SGObject.equals_DynamicObjectArray_equal test will fail as those randoms are not the same. :)\nso please leave there those initialisation values.. by removing all these variables the parameter framework has no way to detect whether there is an element in the m_array or not... this way for example:\n```\nCDynamicObjectArray array1=new CDynamicObjectArray();\nCDynamicObjectArray array2=new CDynamicObjectArray();\narray1->append_element(new CGaussianKernel());\nEXPECT_FALSE(TParameter::compare_ptype(PT_SGOBJECT, &array1, &array2));\n```\nwill fail as the TParameter framework will not know anything about the size or the content of array1 or array2. i reckon the best course of action in this case is actually override the equals function in CDynamicObjectArray and check the size and content of m_array.. use SG_MALLOC. i would definitely create a different test each of the different set_array methods in CDynamicArray not having it in one test. because (10/5 +1)*5 = 15 :). see\nif (!exact_resize)\n            {\n                new_num_elements=((n/resize_granularity)+1)*resize_granularity;\n            }. in this case you can remove m_rng from CStratifiedCrossValidationSplitting as this is the only place it is being used in the class.. i wonder why do you change the return type of the function?\nand you kept the doxygen comment which is then now confusing.... please just keep the return value bool. could you use everwhere ++<variable> instead of <variable>++. same as in case of incrementing, could you use --<variable>. replace this function with:\ntemplate<class RandomFunc>\ninline void shuffle(RandomFunc&& r)\n{\nstd::random_shuffle(m_array.begin(), m_array.end(), r);\n}\nbecause we want to be able to fix the seed of the shuffle. by removing\nm_parameters->add_vector(&m_array.array, &m_array.current_num_elements, \"array\", \"Memory for dynamic array.\");\nbasically it means that the serialization of this object will not be correct... as the deque is not going to be serialized.\nit would be easy to add a vector element to m_parameters if we would use std::vector as that is a continuous memory block... unfortunately it's not the case of deque. but we still need a way to be able to serialize deque's content :S\n@lisitsyn btw how is this gonna be handled with tags? m_parameters->add_vector. this is why some of the python modular tests fails. i'm quite sure that this is not correct, as elements in deque are not contiguous in memory.. namely i dont think that this can be made work with deque, unless you copy the content from it to a vector temporarily and return that vector's memory region, but then how would you release the memory.... remove the whole idea of use_sg_mallocs... if somebody wants to use different malloc should set the SG_MALLOC parameter. i keep emphasizing that do not use in this case post operators but --array_size\nplease address these in the next change.... this would be totally broken if anybody ever would take use_sg_mallocs seriously, as what if use_sg_mallocs = false, SG_FREE would try to free a none owned memory region... but yeah please just remove the whole use_sg_mallocs logic. we do not want to have different API for shuffle... this is a very very bad approach!\nuse the unified one below.... please use auto instead of fixed int32_t array_size if we already have c++11 support fixed.. i know that this follows the clang-format (or i guess that's why it's formatted like this) but our class_list.cpp.py doesn't know how to handle this, i.e. template definition should be in one line:\ntemplate <> class CDynamicArray<bool> : public CSGObject. this is true for the real template definition as well, i.e. it should be:\ntemplate <class T> class CDynamicArray : public CSGObject\ninstead of\ntemplate <class T>\n       class CDynamicArray : public CSGObject\nthis will fix the problem of class_list.cpp. element shadows this.element:\n`../src/shogun/lib/DynamicArray.h: In member function \u2018bool shogun::CDynamicArray<bool>::insert_element(bool, int32_t)\u2019:\n../src/shogun/lib/DynamicArray.h:397:3: warning: declaration of \u2018element\u2019 shadows a member of 'this' [-Wshadow]\nplease just use a e instead of element. same as in case of insert_element:\n../src/shogun/lib/DynamicArray.h: In member function \u2018int32_t shogun::CDynamicArray<bool>::find_element(bool)\u2019:\n../src/shogun/lib/DynamicArray.h:466:3: warning: declaration of \u2018element\u2019 shadows a member of 'this' [-Wshadow]\nuse just e as function arg var.. this is a good test, but i'm thinking that we dont actually wanna do this (see my email to you @geektoni) but want to actually for example instead of \n%rename(GaussianShiftKernel) CGaussianShiftKernel;\nin Kernel.i we would only expose the Some<> version:\n%template (GaussianShiftKernel) shogun::Some<CGaussianShiftKernel>;\n. ok... anyhow we can refactor this in your next task as that one will heavily touch the random of shogun itself. but we'll discuss that later once DynArray removal is fully done.. rather std::vector.erase no?. i would use m_array.size() instead of num_elements for the size of vector, as you actually wanna have the same vector deserialized, right? meaning if you have m_array.size() = 15 but num_elements = 5, you still wanna have the same sized m_array deserialized...so\nm_parameters->add_vector(&head, &(m_array.size()), ..... i mean i'm not so sure how was this previously done... was the size of the array changed when deleted? if not then of course you need to nullify the element... nullptr only works if the stored element of DynamicArray is a pointer right? i.e. if it's index_t or other primitive time you have to come up with a different value. but then again --num_elements should be enough w/o touching the array itself as you shouldnt be able to read elements past who's index is index >= num_elements. i reckon we do not need these macros to be re-defined here if they are not removed from the LinalgBackendEigen.h, or?. argh...... have not seen that one... sooooooooo ugly, soooo redundant :(\nwe could put these macros into a separate header? and include it. yep.... indeed. i dont think it's a problem... i mean that the header would only be actually included consciously one would presume :) i.e. this is just to avoid copy-paste errors and actual copy-pasting things around.. @MikeLing could you add this on the very end:\nunlink(filename);. i believe that it should be enough to have:\n```\ninclude \"utils/Utils.h\"\n```\nor?. yeah this you can just pass directly to the generate_file_name. i would call this function rather\ngenerate_temp_filename. @MikeLing deprecated :) true.... ignore these comments :) the naming of the fucntion though should reflect the temp-ness :)\nbtw the return type should be void since the passed char* will actually contain the right filename :). so basically:\nvoid generate_temp_filename(char* filename)\nafter you call generate_temp_filename then the passed filename will contain the right path :). getMutiPleLabelData\ndo you mean multiclass?. this should be multiclassLabel. \nyep that sounds very good!\n. could we use with open.... as f:. here you should use (CBinaryLabels*)mockData->get_labels_test();. this whole unit test should be guarded with #ifdef USE_SVMLIGHT. ++. we could make this one a std::unique_ptr<CSignal> sg_signal(nullptr) since we are c++11 and not c++98. or Some<CSignal>? :). this as well could be part of #ifndef SWIG?\ndo we wanna expose signals to swig interfaces? @karlnapf @lisitsyn . miju.... caramba :) std::function plz. what if else?. why does this mostly whitespace changes have to do with this patch?. this is bascially linalg::scale(mean, mean, 1/alpha_sum). linalg::zero(cov_sum). linalg::add. this is basically linalg::dot(v,v). once a machine is trained (prematurely stopped or not), shouldn't we disconnect here and reset the cancel_computation state so if one wants to re-train the same machine could still do it?. 1.0/alpha_sum. debug ;). debug ;). debug. debug. 1.0/alpha_sum_sum. debug. is this the one and only function we use in GMM from LAPACK?\nand this is the reason why we require LAPACK?. oooh\nold linalg can take care of it: https://github.com/shogun-toolbox/shogun/blob/develop/src/shogun/mathematics/linalg/eigsolver/DirectEigenSolver.h\nbut ok it should be done if we have some time later.... not part of this pr. formatting... clang-format failed :). formatting... clang-format failed :). formatting... clang-format failed :). formatting... clang-format failed :). formatting... clang-format failed :). formatting... clang-format failed :). formatting... clang-format failed :). ok so here's my thing about the former story: lazy-evaluation (see eigen) or codegen/JIT (see XLA of TF). these are not orthogonal, meaning if we can have lazy evaluation, say:\nMatrix x, y;\nfloat64_t i;\nx.add(y).scale(i);\ni guess you get what i'm saying... this way you only need once to figure out the type of the matrix (and other elements' type) and then the rest is a straight line. of course this complicates things a lot, but to be honest if we look around ourselves this is the flow of every framework around (a bit of functional approach in the imperative world)\n. fyi: @karlnapf already done by @MikeLing in couple of tests :P. what's this?!. could you instead just make sure that you create an v = SGVector<float64_t>(labels->get_num_labels()) and set it random... and basically just do:\nlabels->set_values(v);\nlabels->set_labels(v);. here check both that values and labels are equal.. it's not... there shouldn't be a need for doing this.. based on standards this is not necessarily true, see from: http://en.cppreference.com/w/cpp/language/typeid\n```\nconst std::type_info& ti1 = typeid(A);\nconst std::type_info& ti2 = typeid(A);\nassert(&ti1 == &ti2); // not guaranteed\nassert(ti1.hash_code() == ti2.hash_code()); // guaranteed\nassert(std::type_index(ti1) == std::type_index(ti2)); // guaranteed\n. `DR` is a bit.... mmmm but yeah why not.. it's good but the error message should be rather: \"Features have not been provided.\\n\". i'd rather wrap it with unique_ptr:\nstd::unique_ptr m_rng;\n.auto m_rng = std::unique_ptr(new CRandom());. what happens with `seed`? is there a place where somebody calls `CMath::normal_random(<mean>, <dev>, <seed>)`?. what happens with the seed?. let's use more c++11 features:\nauto m_rng = std::unique_ptr(new CRandom());\n.auto m_rng = std::unique_ptr(new CRandom());.auto m_rng = std::unique_ptr(new CRandom());.auto m_rng = std::unique_ptr(new CRandom());```\nbut later we can add a static function to CRandom that wraps this :). this is double free...!. it's enough to do SG_UNREF in this case.... RLang is +1. do we need really public system? as this is really gonna be just statically linked into .so... so afaik PRIVATE should be enough, no?. same here i reckon a private would be enough...\n. linalg::scale(w_st, w, sign) ?. we could add here a test or REQUIRED for checking that\nREQUIRED(features->get_feature_class() == C_DENSE, \"LDA only works with dense features\"). in this case it could be prefixed:\n++m_class_count[c];. why __?. why would you import Random.cpp here?!. this you should drop. there's no need for this here... since you seed is a global thing.\njust in init as other things have\nset_global_seed and get_global_seed. the generate_seed function should not be part anymore of CRandom.... no i would put it into init.cpp as a static function. no need for the get_global_seed argument as the default ctor uses global seed anyways right?. no need for the get_global_seed argument. no need for the get_global_seed argument. no need for the get_global_seed argument. no need for the get_global_seed argument. no need for the get_global_seed argument. no need for the get_global_seed argument. no need for the get_global_seed argument. no need for the get_global_seed argument. no need for the get_global_seed argument. no need for the get_global_seed argument. no need for the get_global_seed argument. no need for the get_global_seed argument. no need for the get_global_seed argument. indentation ?. indentation. mmm i'm not so sure if an interface (abstract class) should actually contain this. @MikeLing do you agree that CRandom(sg_random_seed) and CRandom() will construct the same object with the same state? because it does...\nso there's really no need for\nnew CRandom(sg_random_seed)) it's enough to call new CRandom() right?. new CRandom(sg_random_seed)) -> new CRandom(). new CRandom(sg_random_seed)) -> new CRandom(). new CRandom(sg_random_seed)) -> new CRandom(). new CRandom(sg_random_seed)) -> new CRandom(). new CRandom(sg_random_seed)) -> new CRandom(). new CRandom(sg_random_seed)) -> new CRandom(). new CRandom(sg_random_seed)) -> new CRandom(). new CRandom(sg_random_seed)) -> new CRandom(). new CRandom(sg_random_seed)) -> new CRandom(). new CRandom(sg_random_seed)) -> new CRandom(). new CRandom(sg_random_seed)) -> new CRandom(). new CRandom(sg_random_seed)) -> new CRandom(). new CRandom(sg_random_seed)) -> new CRandom(). new CRandom(sg_random_seed)) -> new CRandom(). we can actually replace this complicated function with a simple c++11 class std::random_device... basically:\nuint32_t generate_seed()\n{\n    return std::random_device()();\n}. this implementation and the whole tfhistogram needs guard of HAVE_TFLOGGER. @karlnapf @lambday just need your ok here... i think it's fine the diff is just due to the fact that the prng behaviour changed, hence even setting the same seed will not give you back the same series of randoms..... @karlnapf @lambday same as above, need your explicit OK though. @karlnapf @lambday same as above, need your explicit OK though. @karlnapf @lambday same as above, need your explicit OK though. @karlnapf @lambday same as above, need your explicit OK though. @karlnapf @lambday same as above, need your explicit OK though. probably not because of change of seed then... hence my question here as i didn't want to dig into what is actually happening in this unit test.\ni.e. this part needs to be reverted and figure out where else things get not properly done with the random.. idk... but didn't want to merge it to the feature branch even w/o checking that the logic is correct or not.. rx will take care of this internally, no need to do ourselves:\nhttps://github.com/Reactive-Extensions/RxCpp/blob/master/Rx/v2/src/rxcpp/subjects/rx-subject.hpp#L185. we should fully remove this right?. does GMM really need an object level random?. do we really wanna use here an object level PRNG?. this is required the way we do GMM?. this is because we need an object level rng?. sorry for my ignorance, but why do we need an object level PRNG?. if we require an object level PRNG you should still explicitly define:\nm_rng = get_prng<std::mt19937_64>(); . this should be explicit which of the PRNG engine we wanna use... \nm_rng = get_prng<mt19937_64>();. explicitly specify the prng std::mt19937_64. explicitly define the required prng std::mt19937_64. this function should not be in math.... maybe we keep CRandom for that.... be explicit of the PRNG type. does it need to be a sorted map? :)\n. this is for debug purposes only?\nor is there somewhere we use type_name internally?. that empty string as m_name feels a bit \ud83c\udf76 \ud83d\udc11 . this could be a static function of ObservedValue, just like a factory function and returns a std::shared_ptr/some/*. SG_FORCED_INLINE. private virtual?. didn't we renamed this to YIELD_something? :). yeah you can use the utils part to add these type of generators/utility functions. any particular reason for the postfix operator++?. you can initialize them with 0 to make the compiler silent.... you can initialize them with 0 to make the compiler silent.... please dont do relative includes :(\ni know that there are places where it is done the same way but \n```\ninclude \"utils/Utils.h\"\nshould do it.. 0.5 is the error? :) so basically 0.1 or 1.0 is fine as well?. or this is something that it's going to be addressed in a next pr?. plz use the BSD license header from here (currently we are in transition to BSD based license): \nhttps://github.com/shogun-toolbox/shogun/blob/develop/src/shogun/lib/any.h#L2. plz add bsd license. you could use the prefix operator here... ++k. prefix operator `++i`. and of course `auto i = 0` should be good as well. use the bsd license plz. `namespace shogun` ?. @Sahil333 there's the IRC group or of course you can ask on the mailing list. we could use auto here no?. this whole part above should be rather done by a smart memcpy instead of setting the columns separately. needs a stride etc.. the problem with this is that this right away makes it inaccessible/usable from SWIG interfaces... we are having difficulties with exposing Some in SWIG. `sg_memcpy` ? as both std::vector and SGVector backed by a continuous memory chunk. the `i` here should be the type of `index_t` as you are comparing it with `coloring_vector.vlen` which is `index_t`. this should be `index_t` as well instead of `int32_t`. ok so i guess then it would be:\nEXPECT_THAT(coloring_vector, Each(0));\n. note from cmake manual: \"(execute_process) run while CMake is processing the project prior to build system generation.\"\ni'm not so sure we wanna have this.... because you wanna run this when you build the target, i.e. the meta examples :)\nfor that use `add_custom_target` or `add_custom_command` and make sure that whoever depends on the output of `parse.py` depends on this new target/command. dont worry cmake will make sure that this target will be only called once and not as many times somebody depends on it.. there are couple of examples within shogun cmake files how to use add_custom_target or add_custom_command, but if you are in doubt ask :). move this to the beginning of the file with the define as well.... this way the above includes will be included only once even if `shogun/evaluation/Calibration.h` included more than once. this explicit casting will cause a lot of trouble if the returned label is not binary of the machine :) you should either do a dynamic_cast to check for this or simply `get_label_type() == LT_BINARY`. same as above... do a safe typecheck. this could be simply:\nconfidence_values[j] /= temp_confidences[j];\n. ensure type. this method is almost the copy-paste of `CCalibration::apply_multiclass`. you can basically add the same parts to a common function and use that. nullptr. nullptr. you are missing the `SG_ADD` macros for the class parameters. check other objects for examples. move to the top of public. its a virtual right?\n. move this to the beginning of the file with the define as well..... move this to the beginning of the file with the define as well..... this could be substituted by a simple `linalg::scale`\nhas this code been borrowed from somewhere?. this could be done in parallel..... linalg::scale. almost copy-paste of `CCrossValidatedCalibration::apply_binary`. plz simplify. use `linalg::scale`. you can omp this one. omp?. almost the same as `CCrossValidatedCalibration::apply_multiclass` so plz define a template or a function where you pas the apply method. same as above.. omp?. linalg::scale. missing SG_ADD macros. m_a and m_b. need to init a and b variables and SG_ADD them. you could use REQUIRE. ok so then just change it to linalg::scale\nthe reason i've asked is because if it borrowed then we should change it there as well :). yep that's a good value as initial value. http://en.cppreference.com/w/cpp/language/nullptr. yes we do have the oldschool `NULL`, but since this is new code, it'd be good to follow higher std standard features. @durovo yeah as of this state the python interface will not contain these methods.. you'll need to add it to some definition files\nbut that should be another PR. for the time being you should write unit tests for this and test it from the C++ API. mmm oh yeah labelfactory, indeed its the best option. doxygen. please make sure at least that there's a brief description of the method itself. why from 1 and not 0?. same as above... why index from 1?. remove the whitespace after HAVE_ATLAS. to follow the logic <PKG>_FOUND please change this to `OpenBLAS_FOUND`. `Make sure that cblas.h header is available within the header search path in order to use OpenBLAS as BLAS/Lapack backend`. there's a typo here as well, i.e. it should be `path/to/r_example.r` and not `path/to/r_example.rb` (there's an extra `b` on the end). `auto` return is only c++14. but no worries just up the required version here:\nhttps://github.com/shogun-toolbox/shogun/blob/develop/CMakeLists.txt#L35. we have the ancient macros like USE_FLOAT64 etc. ). since we are c++11 use `nullptr`. `nullptr`. `nullptr`. `nullptr`. `nullptr`. `nullptr`. `nullptr`. `nullptr`. statements like these use preoperator and not the post operator. this could be reformulated instead of using critical using parallel for reduction...\ncheck for example:\nhttp://cs.umw.edu/~finlayson/class/fall16/cpsc425/notes/11-parfor.html. yeah no... i mean i was already about to comment above that why do we use eigen here directly... but now seeing that it's being used to do the dot product below.... plz use `linalg::` over SGVector/Matrix. use `linalg::` for linear algebra operations. instead of the explicit cast could we do a dynamic_cast here so that way we can clearly get a bad_cast exception in case of some weird `k`?. you can dynamic cast these into a var and then use them... one would of course suppose that the compiler would do this as well under the hood... but it might be easier to read the code...?. imo this will be a formatting error :(. by their angle?\nit's a weighted manhattan distance.... you mean something like:\nWe can create an instance of CanberraMetric by passing two Features objects for which features  we would like to measure the pairwise Canberra distances.. need to get the Docker image fixed for this on travis. while you are changing these lines, could you please replace these explicit casts to `dynamic_cast` at least plz? :)\nor even better do a `REQUIRED` assertation for the type. another casting. this could be auto or? :). imo this will never happen (unless l or r is null) since if the typecasting fails it'll throw a bad_casting exception.. same as above, see http://en.cppreference.com/w/cpp/language/dynamic_cast.\npragma omp parallel for reduction(+: samples[:num_estimates])\n. this should work with openmp 4.5. dont forget to remove the `critical` part. /me should really RTFM before saying RTFM. and actually this is implicitly tested in `CRealDistance::init`\nhttps://github.com/vinx13/shogun/blob/39e7dd2f390d13e665678be23b0b0221f1212033/src/shogun/distance/RealDistance.h#L34. instead of the REQUIRED sections it should be just check the return type of `CRealDistance::init(l, r)` as it checks for everything you need want....\n. i would actually change the design of this and have instead of init returning a boolean the actual typecasted values available... as i dont see really the point of an assertation failure that will just throw an exception and in case of success have a true :)\nthis function call should actually tell us that yes the types are cool and here you go this is what you wanted anyways. @vinx13 you can ignore most of this... just make sure that you do an ASSERT(CRealDistance::init(l, r)); and add a //FIXME: above with reference to the above comment. before merging, and since you are touching this could you move all these common parameters into a variable instead of keep growing this and copy-pasting around. thnx\nDTRAVIS_DISABLE_UNIT_TESTS=ON -DTRAVIS_DISABLE_LIBSHOGUN_TESTS=ON -DTRAVIS_DISABLE_META_CPP=ON\n``. btw should the libshogun/travis examples be actually part ofmake allat all?. maybe removing meta examples and libshogun all together frommake all` would make more sense and then just call the 'right' make for the right meta example you would like to have in the test phase, as that's actually more related to test and not the library testing itself.\nhttps://cmake.org/cmake/help/v3.5/prop_dir/EXCLUDE_FROM_ALL.html\nand \nhttps://cmake.org/cmake/help/v3.0/prop_tgt/EXCLUDE_FROM_DEFAULT_BUILD.html\nbacause of msvc. and maybe examples should have a common target... say make examples that actually collects all the required examples. that you would like to see with the given INTERFACE combination...\nthis is in context of trying to drop these TRAVIS_DISABLE_* params as they are rather hack from the past than solutions. another? i mean at least a var would be nice.... these are quite easy fixes in cmake files though. this seems to be an abstract class, as nothing is implemented.\nwhy dont you just drop the implementation and specify all the virtual methods pure virtual?. wouldn't storing this in an std::vector would be better?. or in worst case DynamicObjectArray, for assuring ref counting. as said i would rather use something more high level than this. isn't it enough to count the one of them as the other can be simply deducted?\ni would use something like std::count_if or std::transform. all these parameters should be set somewhere else... as you want to be able to control them without actually changing the code... i.e. there should be a getter/setter for them. same as above... too vanilla implementation. why are these all virtual? you will have a calibration method that is based on Sigmoid?. it should call the ctor of CCalibration. i still dont get why do we need default implementation with error. m_sigmoid_parameters is not registered... that'll make it impossible to clone this class. since currently registering std::vector is not possible you would need to use DynArray :(. the function arg can be const :). you can either use std::transform or at least use the fact that SGVector has iterator so it can actually do a range-loop. but that should be fixed in develop, so rebase over develop, otherwise i dont understand what's the problem. if you have just added the factory.sg could you change the relative path (../../) to an env var? i'm happy to add the support for that for every language \ud83d\udc83 . as if we install every these meta examples, the location of data should be somewhere very else :). although it looks shitty but operator << will not solve your problem to convert some objects to string.. that only works with primitive types. on the other hand having these macros is nice in a long run as we could switch finally our logging backend. maybe spdlog? that allows us to redirect the logs into any sink not only stdout/err. why do we need a copy? i mean it's over the same datastruct no? do we really want the as_binary be a copy? dont we wanna use c++11's feature of std::move and rvalue?. see above regarding move/rvalue. but this way you loose:\n 1) std::move the thing around - maybe not so important\n 2) enforce sole ownership of the resource.... @lambday was this intentional to have it as unique? i would presume yes....\n . i'm not so sure if i get what this has to do with constness...\ni'm talking about the fact that instead of copying the label, what you want  is the same memory/object & everything just as a different class, right? . new logging backend = just change the macro definitions.\nyeah you can offload the operator << but that means that you force to use std::iostream-like interface to be used, which is not the most liked IO back-end (for good reasons) in c++ community. on the other hand you can do the same thing, i.e. implement to_string() for that object where you wanna have operator <<, right?. so then use rvalue/move instead of this pattern.. sg_memcpy. same, sg_memcpy. are we sure we want exit(-1) ? i mean in this case of a REPL we would exit the REPL itself, or?. i wouldn't copy-paste this... some parts of it at least can be merged. for inline-ness plz use the macro: SG_FORCED_INLINE\nwhich will make it sure that it's really inlined. que?. afaik this copies the whole vvector itself.... do we really need to copy the data where it doesn't change at all..... que?\ndo we need to spell this one out? does this need to be expose to anybody apart from ourself internally?. ah this is in .cpp. nevermind. cpp only, nevermind. can we fixme/todo this... as this should not be a hard requirement, right?. i mean one would like to have something even like a: ['foo', 'bar', 'zelda'] multiclass labeling possible sooner than later. we could drop the != true part and just have a negation, right?. delete the file as well plz.. plz remove the temporary file on the end. see the 2nd comment... it's foobar, just ignore the first comment. you can use some in many of these cases, so you dont need to call SG_UNREF :)\nfor example:\nauto ll = some<CLibLinear>();. same here as above:\nauto eval = some<CContingencyTableEvaluation>();. this could be simply\nauto pred = ll->apply_binary(test_feats);. and then you can remove the declaration of CBinaryLabels* pred = NULL;. same as above... this code shares quite a lot of lines with train_with_solver... we could combine them :). use sg_rand->set_seed plz. i think this can go as this was required for the gaussian blob generator, but that's not LAPACK only anymore... . ?. std::?. wouldn't be better to actually drop those copy ctors that actually copy and fix it instead of this hack?. imo this is just nurturing the bad design so would be just better to drop that faulty copy-ctor.. doesn't matter based on your changes already going to create a problem for the other PR..... just replace it with the given macro above.. in case you insist on using inline... otherwise just drop the keyword inline. dont get it why is it better to remove instead of using a macro that ensures the inline-ess... hence make the whole code faster... \u00af_(\u30c4)_/\u00af. please avoid these type of hanges. why?. why is it a todo if it is the solution? if not then just add the todo but not the whole pragma.. have you tested the pragma itself?. why post operator here (i++) but prefix-operator below (++j). i'd stick with prefix. same story... prefix vs post.\nbtw this code seems to be a bit of a copy-paste from the one above. why the removal of the DEBUG-ing message?. you should fix the commit sha, instead of following the master HEAD. basically set\nGIT_TAG and set it to the last stable release of stan\n. what's the stan1 stan2 stan3 stands for?. who and what will install those the folder here?. who and what will install those the folder here?. who and what will install those the folder here?. this should be boost_1.64.0. i would actually start a new dir: include/shogun/third_party and start putting stuff there that are external... we have too many things in lib. this test is very fine, lets concentrate on the actual inclusion of the lib. i'm not so sure if this is the best way... namely that you are including stan like this.\nmeaning what if for some reason stan/math.hpp is in the include path already... probably the -isystem inclusion will take care of this, but maybe it'd be better that we explicitly specify in the include path that it's shogun's third-party include path... on the other hand probably this will actually break everything else within stan's headers.... \nit's more important actually that stan usage (or a model that is actually using stan) is being tested from a libshogun example. namely that the cmake target flags are set correctly when shogun is being used from a third-party lib, or just a simple CLI. note that it requires that either you:\n * either include stan only in cpp files of shogun, or\n * you expose the include flags in ShogunTarget.cmake; currently it's PUBLIC SYSTEM which should (? does it really? should be tested) reflect the correct exposure of include flags.... but i'm not so sure if we want to expose stan to the public, i would presume it's safe to think that we only use stan in the implementation of the models.. set the seed of random here, with:\nMath:init_random(1). you probably wanna reset the seed of the prng:\nMath:init_random(1). because liblzma is gpl?. because liblzo is gpl?. because nlopt is LGPL? note it's lgpl and not gpl. because lpsolve ig LGPL?. because colpack is LGPL?. can you please rebase this over latest develop and change this to include/shogun/third_party/Stan. can you please rebase this over latest develop and change this to include/shogun/third_party/Stan_Boost. can you please rebase this over latest develop and change this to include/shogun/third_party/Stan_Cvodes. instead of this you should actually copy these files into ${THIRD_PARTY_INCLUDE_DIR}/Stan. and what happens if it's std::vector? i mean the idea here is that you dispatch into whatever we support?. for the record: i dont like it... and its even worse than the put :). these type of dynamic dispatchers, along with the ones that are actually being generated by swig should go to the swig interface itself. i mean in some way or other we are just re-implementing here things manually that are actually being automagically generated by SWIG - see dynamic dispatching depending on function args.. remove these installs.... if you do a cmake copy thing then you will not have problems with the logs in travis.. check how COMMAND ${CMAKE_COMMAND} -E copy_if_different is being used in shogun. and the best place to do this copy would be ExternalProject_Add's INSTALL_COMMAND. great but i recon since here is everything a fwd declaration as SGObject.h is already quite cramped with stuff maybe it'd be better to factor this out somewhere else and just include that header in this header. why the Z?. i wonder whether actually we could generate from a constexpr or something as for a while i imagine that this template will be changed.... . tab?. this could be a pure virtual function in order to be an abstract class, or?. needs a quickfix to check whether it has been trained or not... :). auto. auto. sgvector could be used, or?. linalg::zero. this could go.... and declare it where its needed. linalg::scale?. sgvector?. auto?. auto. auto. auto. second @geektoni ... while(1) is usually a call for trouble :(. i'm not so sure if i get why do we have this #ifdef WIN32.... on non-windows we dont wanna allow controllers?. i'm not so sure if i get why do we have this #ifdef WIN32.... on non-windows we dont wanna allow controllers?. this should be ${VARIANT_RELEASE_VERSION} no?. yeah but they have tags as well, see https://github.com/mpark/variant/tree/v1.3.0. because unfortunately the branch HEAD will always contain the latest release, meaning you will always checkout the latest release which probably not going to be in future VARIANT_RELEASE_VERSION and it's a good idea in general to depend on a specific version of an external lib.. mmmm i've just checked that branch, sorry i totally misunderstood the concept this... it's pretty weird to me having a branch totally different content than the base branch :). ignore my comments above. you should use the shorter version of the license, see for example SGObject.h. should use the shorter version of the license. whitespace. we never include eigen directly in shogun headers, only in linalg but even then it's always via shogun/mathematics/eigen3.h. i guess this class is never gonna be part of an SGObject, hence we could make a pass over all the anti-shogun-patterns that you've used below. why not SGVector?. why not SGVector?. these args coudl be SGVectors. override/virtual?\n. indenting.... plz use clang-format. index_t. indent. use the shorter license. you have not added these to the param framework, nor this will be ever be able to be added... you need to use SGVector for that. same as above... SGVector. this currently is foobar... you wont be able to serialize reproducible this class. indent + problems with serialization. SG_ADD is not used anywhere in this class.... since it's || maybe it'd be better to have this as:\nreturn (m_callback) ? (m_cancel_computation.load() || result_callback()) : m_cancel_computation.load();\njust for the sake of lazy evaluation of or :). you either use the default (nullptr) valued set_callback, i.e. use\na.set_callback();\nor remove the default value of the function arg. i would opt for the latter. we need to figure out how we can store (if we want a none primitive typed std::vector, like stan::math:var)... hence the temporary story... just add a FIXME/TODO term there and then its good. these could be both SGMatrix passed as value.... there was an email from @karlnapf on the mailinglist not long ago about SGVector and SGMatrix and how they are passed around. just pass them as value. there's just no need for this variable on the stack... and just extra line of code. just do:\nif (m_index_of_sample >= get_sample_size())\n    return false;. auto. auto. auto. auto. auto. do we need this variable?. just do average_gradients += get_gradient(). second that this actually contaminates any file that will include this header. what's the story behind these being pointers instead of ref?. a more 'representative' naming would be more desirable :P than just features_train1. maybe a better name would be normalized_features_train/test. we could do before this an assertion for get_feature_class() == C_DENSE just to have a sane error message?. although .as has a quite good message as well.... ICAConverter has not been fitted.. if we know that this class is an abstract class for other classes that does conversion over CDenseFeatures<float64_t> then here it would be good to actually do the type mapping, meaning not to have this function pure virtual but implement something like this:\nvirtual void fit(CFeatures* features)\n{\n    REQUIRE(features->get_feature_class() == C_DENSE, \"ICA converters only work with dense features\")\n    fit((CDenseFeatures<float64_t>*)features);\n}\nand have a pure virtual protected member function that has a declaration:\nvirtual void fit(CDenseFeatures<float64_t>*) = 0;\nthis way the casting thing you need to do it in one place, all the other places you can already assume CDenseFeatures as input.. check the indenting... something is wrong here with the whitespaces. is there anywhere else a virtual function in this class? just make sure that in that case override is being used as otherwise clang is throwing a lot of warnings :). imo you could do this with one for loop:\nfor (auto& v: matrix)\n{\n    v = std::log(v + 1.0);\n}\nor?. used std::min. auto. auto. auto?. m_num_idxvec == num_featuresvec ?\nas now v_src and v_dst points to the very same memory region, before the patch idk. just checking.. auto. auto. std::min. std::max. this could be and std::copy no?. whitespace problem.. why move this into implementation ?. whitespace problem.. if this is pure virtual, then surely fit could be as well?\ni'm not sure if i follow the logic here why fit has a default implementation and why the apply does not or vice versa.. indent. why do we drop CDimensionReductionPreprocessor? :). gotcha!. ah yeah i remember now... sorry short memory :(. okie!. mmm actually i've realised... dont we actually wanna follow actual transformer api, meaning\n.fit and .transform ? :). shouldnt this be simply\nKernel k = kernel(machine_mkl.get(\"kernel\")). this assumes that one could explicitly cast CEvaluationResult to CrossValidationResult, which is not the case unfortunately.\nfor the time being there's no solution for this other than the one you had before:\nCrossValidationResult result = CrossValidationResult:obtain_from_generic(cross.evaluate()). i know that this does not come from this patch but in case of this unit test do we really need to have features_train to have CDenseFeatures<float64_t> type?. for emptyness check: X_new.size() > 0. oooootoooo, auto :). this could be a simple ternary expression:\n``\ntransformer->train_require_labels() ? transformer->fit(data, m_labels) : transformer->fit(data);\n. nullptr. auto. auto. sadly we wont be able to register this for a while :)\ni.e. no serialization :D. auto?. here you need to make sure that you convert it with obtain_from_generic :(.labs? please renamelabstolabelsotherwise this will fail... as it fails now. \\o/. i wonder if we should have a better name for this. as this is gonna be the name of the param when you do .put(...) and .get(). maybeiteration`?. require failed -> sg_io->msg(MSG_ERROR, \"blabla\") -> https://github.com/shogun-toolbox/shogun/blob/develop/src/shogun/io/SGIO.cpp#L125. my mistake, just testing the patch and pushing once done.. \nas far as SWIG goes it doesn't understand templates, before they are actually instantiated, say\n%template SGMatrix\n%template SGMatrix\nthat's the general guideline of SWIG... dunno how newobject works in this context but i would expect the same (consistent) behaviour?\n. how about renaming this to\nstd::string to_string() const;\nand basically return a string instead of directly writing SG_INFO.... that of course means that SGObject::to_string needs to be virtual. maybe instead of accessing the pipeline elements like this it'd be good to have a way to access them by name? see for example http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html#evaluation-of-the-performance-on-the-test-set\nof course this would mean to be able to add elements as a tuple, say\nPipline().with((\"customName\", ZeroMean).with....\nbut that could be that we support both, meaning either having\nPipline().with((\"customName\", ZeroMean).with....\nand\nPipline().with(ZeroMean).with....\nand in the 2nd case we use the get_name() string.... i wonder if we should start using typed exceptions, because for now EVERYTHING is ShogunException. things like MachineNotTrainedException etc....\n@lisitsyn @karlnapf mm?. note, that for now the only way to infer the reason of exception is actually parsing the exception message... that's not really 'nice' from an automation/developer's perspective. imo here std::invalid_argument be the more appropriate...\nbut in lot of the REQUIRE() case it's the same... namely a 'more specific type' of exception would be much more appropriate from a shogun user... if its getting integrated in a larger framework.. the need for a more specific type of exception came up in my head here atm, because expecting there ShogunException is too broad.... it could actually come from a transformer or the machine in the pipeline... right? @karlnapf @lisitsyn @vinx13 . i'm not so sure if get it why there's a specific interface serializing a \"machine\". especially when CStoppableSGObject is actually not even a machine, but an SGObject. i second what @geektoni says above.... why do you want to automatically serialize a machine when you pause a machine, and why that name and why in a format of ascii and why extension of .txt and what happens if the file already exists.... @shubham808 it's not about the name. it's more about that CStoppableSGObject is a CSGObject which already have a serialization functionality in it CSGObject::save_serializable CSGObject::load_serializable which should be sufficient to save any SGObject-like. mmm sorry but if i want to pause my training i dont want an object to touch my FS and do any sort of IO without my explicit request. i dont understand why a pause should trigger IO and serialization of the object itself. if one pauses a procedure, then it just pauses the procedure, having these implicit default actions like writing to file etc. is not something one would like to have - developer perspective. if she/he wants to serialize the paused object, she can do it by explicitly calling the function. it's there. if you want a helper method then plz dont add this to the object, rather add a function the shogun:: namespace that gets a CStoppableSGObject (or CMachine) and a CSerializableAsciiFile as arg and does the magic.. why wouldn't you be able to serialize a paused object/method?. why a file at all? why not a bytestream? i really dont get this concept atm. CFile* ?. ```\nm = new Machine();\nthread 1: m.train();\nthread 2: SIG to thread1 pause\nthread 2 calls m.save_serializable(...)\ndoes whatever\nthread 2 sends a callback to m to continue\n```\nwhy wouldn't this work?\n. smallish detail but this adds things to the places and functionality places where it does not belong. atm i really dont see this as a good design to go ahead with this as is.. i would define these in separate headers and cpp files :). and in the case of \"a machine is used before training\" i would call that exception MachineNotTrainedException as invalid state could be a lot of things and the not trained case i believe should be explicitly spelled out for the user :). still dont get why we need pointers.. \nyou could just simply wrap the EigenVectorXt with SGVector and not do cloning...\nnamely\nSGVector<float64>(gradients.data(), gradients.size())\nbut i reckon it'd be better that simply just create an\n```\nSGVector gradients(size);\nSGVector::EigenVectorXt mapped_gradients = gradients;\n/// now you can use mapped_gradients as an eigen vector\nreturn gradients;\n. plz use `linalg::scale()`. auto. mmmm this feels very very strange... as this definitely makes the whole cost function not thread-safe. i'm not saying that they should be, but i'm not convinced atm that this is actually required, or the only way to solve what you want. nitpicking, but this is the same as `res *= 0.5` except that the former is slower. same as above. these all could be auto right?. auto. auto. auto. if you return SGVector<float64_t> that should trigger a copy-ctor that should ++ the ref counter and hence shouldn't delete the data itself.. there should be many examples where this is used in shogun.... e.g. https://github.com/shogun-toolbox/shogun/blob/develop/src/shogun/machine/gp/GaussianLikelihood.cpp#L238. this needs fixing, because of reserved keyword. change it. yeah the wrapping is something that ought to go sooner or later ... one would hope :). no need for this if you are gonna throw away the obj on the end of the loop. plz dont use CMath's random interface for getting randoms. same here... dont use random interface of CMath to do this. is this really meant to be merged into develop? or rather have it in a feature branch?. shouldn't we use a different exception type other than ShogunException. shouldn't we use a different exception type other than ShogunException. i would rather go with the java like: `is_fitted` or `is_fit` depending which english we'd like to go with :) but let's stick to british (non north american english as in case of the exception type naming). let's register this param as is_fitted\n. @karlnapf are we talking now about PipelineMachine and PipelineBuilder, or why `then` returns Pipeline* ? :) imo those are different things or?. in case `then` would return CMachine, then the whole thing becomes opaque, and you would never be able to get out the elements of pipeline...\nunless we could register `std::vector<std::pair<std::string, variant<CTransformer*, CMachine*>>>  m_stages;` and then use getter... which is currently a nogo for multiple reasons :). imo builder is just clearer but i dont have any strong feelings about it...\nnote my second comment about extraction and observation of pipeline.. i.e. until we cannot register and cleanly extract stages from a pipeline this sort of explicit exposure is required :) otherwise the whole thing becomes totally opaque once built. since there's already a change here we could just port using `as<CDenseLabels>`. as agreed no need for this, drop it.... @karlnapf you mean about the duplciate method or what exactly do you mean by multiple inheritance?. k... note that needs extra love in case of SWIG interfaces... although it's a good question whether we actually wanna expose view mechanism for swig?. use `override`. use `override`. the problem with having CRandom created here is that you wont ever be able to set seed :(\nthere's a way to do this, check the usage of the global variable `sg_rand` in the codebase... but i rather would start reviving the `feature/random-refactor` branch and have then this fixed..... the rest of the patch has the same pattern and it'll be a problem as well there.... guard this for the time being with an\nifdef SWIGPYTHON\n%rename(_get) get(const std::string&); \nendif\n```\nas this will rename the get method in every interface.... that would require the dispatching implementation available in all the supported langauges. the std::map is not supported for all the languages we support in SWIG... that's why actually we have CMap in the codebase :( so either we have to go with our map or.... it's only std::vector that is being implemented in all of the language mappings. mmmm i would allocate it on the heap, and return a CMap<...>*. so basically\nCMap<std::string, std::string>* parameter_types() const {\n            auto result = new CMap<std::string, std::string>();\n            for (auto const& each: self->get_parameters()) {\n                result->add(each.first, each.second.get().get_value().type());\n            }\n            return result;\n        }. as far as i can remember std::string is mapped in every swig interface, so std::string should be preferred yes. ok so another thing is that maybe we wanna keep the parameter_names function and have an additional one that returns the map?. both... so that we still have just a function that returns the params :). use Some instead of shared_ptr... that you can use in any swig language :). sooo true \ud83e\udd26\u200d\u2642\ufe0f . ok so then it's a go... but in that case u could use std::map as well ;). what's the content of CMap value?. should be fixed in the commit below.... style problem... but i wonder why the clang-format job didn't catch this... need to check. tabber. this should be lhs->get_dim_feature_space(). this could move into CDotKernel::init(l,r). @lisitsyn i think adding here a REQUIRE() shoudl be enough or?. use the actual enum name plz instead of the value. use the actual enum name plz instead of the value. use the actual enum name plz instead of the value. use the actual enum name plz instead of the value. dunno if this is actually needed :). same as above... you could jsut check for the var itself. tasks are specialized steps.... so basically her eyou need first\n- script: conda install mkl-devel\n    displayName: asdf\n-  - task: CMake@1\n..... we might need to add -DBLA_VENDOR=Intel to force the usage of mkl. this should be ACCELERATE. no need for this imo. this should be:\n${{ if eq(variables['use_mkl'], 'true') }}:. we should use the public for shogun when mkl is used because then when you link with shogun.so you need to force that the user links against mkl as well. i think the includes could be PRIVATE as i dont think we expose mkl things in our headers.. or do we? i'm not so sure.... need to see if any of the headers of shogun exposes mkl in some indirect way..... but definitely libshogun needs that as well. i wouldn't be so sure about this.... and as is quite costly anyways... typeid hash would be more appropriate.... but again the feature class enum we might not wanna get rid of.. yeah.... and in case if colwise = true mean is the colwise mean. note i use the same container (mean) to get the std-dev.... avoid code redundancy and since i need to use add_to_dense_vector..... see https://github.com/shogun-toolbox/shogun/pull/4548/files#diff-e3b574b8a2ef609bae51ca0e7298680cR214. unlikely as we have #include <shogun/kernel/normalizer/KernelNormalizer.h> before the class :). yes yes... i'm more wondering just about the implementation of clone. DynProg.cpp ... shitty stuff. :)\non the end of the day if we could use some we could drop this whole thing and have std::vector> and not have this whole DynamicObjectArray.... i could keep the dim3 but never used and since on the end of the day we really would like to deprecate this object (see std::vector>), just wanna restrict as much as possible. i wonder why the clang-format job is not complaining....?\n@geektoni . SG_FORCED_INLINE plz. same here.... yes plz force it. @theartful plz never do this in a header file.... yes. it is a binary classifier. i would refrain myself for this type checking... you can use std::type_index(typeid). goes into supports. please dont do type checking like this.... supports(). supports_interleaving should go and add the described supports() function.... there we can add what sort of input type it supports etc.... way more extensible than having a simple function that returns a boolean for a certain small thing. and that interface should actually go into CMachine. this class already obviously depends on SVMLightOneClass so yes then it should work.... you can encode in one bit whether a machine is interleaving or not.... well mkl doesn't work with anything else but svmlight* stuff ... just saying.. @lisitsyn this is what you had in mind?. @karlnapf @lisitsyn check this commit and lemme know if you have objections, as then i'll just cherry-pick this one commit into develop..... do we need to copy this... i mean currently you already rely on sgvector being on cpu memory so just then take the eigen map vector of it.... ",
    "lianos": "uname -m actually wouldn't do the trick on OS X because OS X can boot in a 32-bit kernel, but use 64bit binaries, so uname -m can report i386 even though the user's R is running in x86_64.\nSomething like this could work:\nRARCH=`R --slave -e \"cat(R.version\\\\$arch)\"`\nBut actually, it looks like this is (again) OS specific:\n- On OSX, you'd have to do this to install the *.so info, for example, sg/lib/$RARCH/*.so.\n- It looks like *nix doesn't have architecture specific builds, so doing what you are doing now is correct.\n- I know that as of R 2.13 (or 2.14), windows supports 32 and 64bit R, but I'm not sure what its installation tree looks like. I can find out, though, (but maybe shogun doesn't build on windows anyway?)\n. ",
    "iglesias": "Sure, I will make nicer the non-graphical one I have been using for testing and try to make a graphical one using matplotlib.\n. I did the changes we talked about. How is it working right now:\nif rhs != lhs -> the covariance is computed using all the feature vectors and the distance is between a vector in rhs and a vector in lhs\notherwise -> the covariance is computed using the feature vectors of lhs and the distance is from a vector in rhs to the distribution lhs\nSo now both types of Mahalanobis distances (from a vector to a distribution and between two vectors of the same distribution) is implemented :-)\n. Here it is the new version. Let me know if I misunderstood you (again :P) so we can fix it\n. Here it is with the new changes we talked about. A quick summary:\n- I have moved the computations in apply that were only dependent of the \n  training data to train_machine. This implies a couple of new class members\n  and the removal of two class members that were used before in apply but\n  now they're not necessary.\n- For the serialization issue we discussed in IRC. SGVector* members are\n  now SGMatrix and SGMatrix* members, SGNDArray. I have extended a bit \n  these classes SGMatrix and SGNDArray with some methods that are useful \n  here. \nFinally, the serialization of SGNDArray members is pending since you\nsuggested to leave it this way and think more carefully how this should be done.\nPlease let me know about your opinion :)\n. Ok, but even if it is not strictly unit testing, I think it could be interesting to look for a way to make a check for memory leaks automatic and compulsory in tests.\nMaybe the output doesn't need to be as informative as the one that valgrind gives, but at least to point out what part of the code is not managing memory appropriately. \n. The idea of a separate branch for unit testing looks good to me, much cleaner to work with than the submodule.\n. OMG, I am completely stupid!\n@vigsterkr  thank you very much for finding it, I do owe you a beer :)\n. I have run this on arch with gcc version 4.7.2.\n. Yeah, I agree with you. On the other hand, it does compile in this case. If none is willing to\nfix ./configure soon-ish, this could be a temporary duct tape fix.\n. Looking forward to seeing it :)\n. Eigen 3 is fine\n. I can take a look to how you guys have done other unit tests and introduce one.\nIt is probably something good to know how to do :)\n. I am not entirely sure if removing the branches.only lines will lead to building all the branches;\nit makes sense that it would act like that, but it is not entirely clear from the doc above.\nAnyway, do we actually need that? I think it should be enough with having it in develop and master\nsince those are the branches target of pull requests.\n. I recognized it after building for python_modular and python_static. Let me check exactly\nwith what interface it appears\n. I think the patch looks fine. However a few comments:\n1) We are not accepting pull requests to master any more since we started using the\ngit flow development workflow. For more info see this entry in the mailing list\nhttp://article.gmane.org/gmane.comp.ai.machine-learning.shogun/3342/match=flow.\nAsk around at IRC or in the mailing list if you find troubles using it.\n2) Please consider doing commits that build correctly since later the buildbot works\ncommit by commit. It seems that a couple of the commits above are build failed.\n. I think this pull request should not be merged directly into master so you should prepare a new one\nfor the development branch using git flow.\nBut I am not in charge of merging things so maybe @lisitsyn, @sonney2k, @karlnapf or @vigsterkr \ncan answer you better.\n. This patch is nice but it has too many commits for the changes made. One single commit should suffice here.\nI am closing it for the time being.\n. I think it is looking very nice. Great effort indeed! I had in mind an implementation based\non eigen3 though.\nAny idea why does travil build fail? I have just checked and the only thing I see failing is the\nLLE test which doesn't have anything to do with this.\n. We indent with tabs -- no whitespace expansion. It could be that your gedit is configured to insert\nspaces instead of tabs. The tab width may affect as well. These coding conventions appear in\nsrc/README.developer.\n. @foulwall, @dvalcarce, I think this could be nice for a demo too. What do you think?\n. A couple of comments:\n1) We are not accepting pull requests against master any more since we started using the\ngit flow development workflow. For more info see this entry in the mailing list\nhttp://article.gmane.org/gmane.comp.ai.machine-learning.shogun/3342/match=flow.\nAsk around at IRC or in the mailing list if you find troubles using it.\n2) Since this is a graphical example, its place should be .../python_modular/graphical\nrather than .../python_modular.\n. @karlnapf, you already merged that yesterday I think :)\nSee https://github.com/shogun-toolbox/shogun/pull/1035 \n. @karlnapf, you already merged that yesterday I think :)\nSee https://github.com/shogun-toolbox/shogun/pull/1035 \n. Nice example! However, we are not accepting pull requests against master any more\nsince we started using the git flow development workflow. For more info see this entry\nin the mailing list \nhttp://article.gmane.org/gmane.comp.ai.machine-learning.shogun/3342/match=flow.\nAsk around at IRC or in the mailing list if you find troubles using it.\n. Nice example! However, we are not accepting pull requests against master any more\nsince we started using the git flow development workflow. For more info see this entry\nin the mailing list \nhttp://article.gmane.org/gmane.comp.ai.machine-learning.shogun/3342/match=flow.\nAsk around at IRC or in the mailing list if you find troubles using it.\n. Nice fixes. Thanks!\n. Ok good catch, although it is a funny thing to sort arrays with zero elements :)\nEDIT: As qsort is implemented recursively, I guess this case may arise even if\nthe original array to sort is not zero-length.\nHowever, we are not accepting any more pull requests against master since\nwe started using the git flow model. See\nhttp://comments.gmane.org/gmane.comp.ai.machine-learning.shogun/3342\nPull request against  develop and we will merge it.\n. Goot catch, thanks!\n. So this line is basically computing the dot product of the column of a matrix with itself. I understand your explanation. However, since the number of elements in the column of a matrix is actually the number of rows, I think I am missing something here.\n. I agree. It would be a good idea to use CMake.\n. I agree. It would be a good idea to use CMake.\n. I would be great to find someone with enough courage to make Shogun compatible with CMake!\n. You should create the build directory in the root of your shogun repository. The idea of using\ncmake is to avoid running ./configure.\n. This patch makes lot of sense definitely. I am actually unsure if it would make sense to use\nin PrimalMosekSOSVM a loss other than the hinge loss. Maybe in the future we can get also\nrid of that.\nMinor comment, I believe the first commit ('Merge remote-tracking [..]') shouldn't be there. Maybe\nyou did something weird when syncing your local develop branch with upstream develop? Anyway,\nI guess this time is ok and there will be no problem with merging it.\n. This patch makes lot of sense definitely. I am actually unsure if it would make sense to use\nin PrimalMosekSOSVM a loss other than the hinge loss. Maybe in the future we can get also\nrid of that.\nMinor comment, I believe the first commit ('Merge remote-tracking [..]') shouldn't be there. Maybe\nyou did something weird when syncing your local develop branch with upstream develop? Anyway,\nI guess this time is ok and there will be no problem with merging it.\n. I will ping Nico (last year SO project's mentor) and ask him about it -- he designed this IIRC.\n. I will ping Nico (last year SO project's mentor) and ask him about it -- he designed this IIRC.\n. Thanks! If you feel like it, try to amend the last commit (git commit --amend ...) when you\nintroduce the new change please.\n. Thanks! If you feel like it, try to amend the last commit (git commit --amend ...) when you\nintroduce the new change please.\n. @hushell, it is actually find to sync with upstream after finishing your work. You can for instance:\n- Sync with upstream.\n- Do your work, with some commits.\n- Finally, before issuing the PR, sync again.\nThis second time you sync, however, you may use git rebase, so that the commits you did locally\nare introduced after the new changes you get from upstream this second time you sync. This way\nwe avoid these merge commits.\nEDIT:\ngsomix sent a great mail with a list of the commands to do this using git flow. You can check it here:\nhttp://article.gmane.org/gmane.comp.ai.machine-learning.shogun/3538/match=\n. @hushell, it is actually find to sync with upstream after finishing your work. You can for instance:\n- Sync with upstream.\n- Do your work, with some commits.\n- Finally, before issuing the PR, sync again.\nThis second time you sync, however, you may use git rebase, so that the commits you did locally\nare introduced after the new changes you get from upstream this second time you sync. This way\nwe avoid these merge commits.\nEDIT:\ngsomix sent a great mail with a list of the commands to do this using git flow. You can check it here:\nhttp://article.gmane.org/gmane.comp.ai.machine-learning.shogun/3538/match=\n. Regarding the surrogate loss, from Nico's answer I understood it would actually make\nsense to use other losses different from the Hinge loss. In any case, for me it is fine\nif you decide to drop it.\n. Great! Then this PR is ready \n. I would rather do\nreturn SGVector(w, w_dim, false);\nsince it returns the reference to the w in the OnlineLinearMachine, as the\noriginal method.\n@tklein23 , do you find something wrong with that?\n. That is true. But it is the same that would happen with get_w even if the w member was a\nSGVector (provided that the member is directly returned, without copying, like in LinearMachine\nfor instance).\n. Let's merge this as it seems the safest option for the moment. It would be nice however\nto switch to SGVector.\n. @hushell, I think there was a possible memory leak with m_surrogate_loss in the last PR.\nHave a look at the change here and if you agree I will merge it.\n. Hey @pickle27! I just took a look at the commit. I think the design idea looks nice.\nImplementation-wise, I'd avoid implementing methods and including stuff like limits from\nthe standard library in the headers (the former to improve compilation time; the latter due\nto SWIG, although it may be no problem with SWIG in this case, I am not sure).\n. I think it is looking good, it is pretty close to be ready to get merged. Let's get it\nmerged ASAP before the next PR :)\n. It sounds good!\n. Hi @hushell!\nIt seems that one of your unit tests in failing on travis gcc. It works fine on travis clang though,\nwhich is weird. I took a look at the test and you are using no randomness so I am not sure\nwhy can it be caused. Any idea? \n. Good to merge! One comment, for the next time let's try to keep the patches a bit smaller\n(let's use a rule of thumb of 1000 lines of code max.) so they are easier to revise and merge.\n. @sonney2k, please let me know if the header is ok like this, with the std::vector, std::set and\nEigen stuff used in the private methods\n. @sonney2k, please let me know if the header is ok like this, with the std::vector, std::set and\nEigen stuff used in the private methods\n. @sonney2k, new version without including Eigen and std stuff in LMNN.h. I have created\na class LMNNImpl with static methods; these are the ones that used to be private\nin LMNN.\nI decided to go for static methods so that LMNN.h does not need to include LMNNImpl.h, as it\nwould if I had used the opaque pointer technique with a CLMNNImpl* attribute in CLMNN.\n. @sonney2k, new version without including Eigen and std stuff in LMNN.h. I have created\na class LMNNImpl with static methods; these are the ones that used to be private\nin LMNN.\nI decided to go for static methods so that LMNN.h does not need to include LMNNImpl.h, as it\nwould if I had used the opaque pointer technique with a CLMNNImpl* attribute in CLMNN.\n. I like the idea of using the library in shogun/src for make check-examples.\nIt makes sense to me doing\nmake && make check-examples\nwithout the need of installing in between.\n. Pretty cool to see these ones coming in @karlnapf. How can we see them? I am guessing\nthat for the moment we need to check out the code and reproduce them locally using ipython\nnotebook.\n. Hi Heiko, this can be a very good idea. I have a doubt about it however. As you said,\nit is very simple to do the grid-search by hand, but what about the separation of the\ndata into different folds? When I did model selection and cross-validation of some structured\noutput stuff this is indeed the part that took most of the time.\nI guess it is included in the cross-validation part, so I just wanted to make sure that\neven if we drop model selection we still have tools to generate folds from data.\n. The plan sounds good but I have one fear. We used unit tests to check individually very small pieces, using very simple inputs. However, I believe that the integration tests use larger data sets. I think it is safe to keep tests that are bigger than the unit tests. Maybe we could have unit tests doing equivalent things to the ones that are done currently in the integration tests? Including reading data from input files, etc. \n. Nice!\n. It should be all right @sandeepkc. Eventually all the issues here are waiting to be fixed, no matter they are labelled as entrance task or not. Also, to fix this you will need to get initiated with the Shogun dev cycle, which is pretty much what we want to see with the entrance tasks.\n. It should be all right @sandeepkc. Eventually all the issues here are waiting to be fixed, no matter they are labelled as entrance task or not. Also, to fix this you will need to get initiated with the Shogun dev cycle, which is pretty much what we want to see with the entrance tasks.\n. What about printing a warning message in the constructor?\n. Then there are two parts here, warn users and crate this list. I think that a #ifdef or #ifndef clause around the whole class (like the HAVE_EIGEN3 or DOXYGEN_SHOULD_SKIP_THIS we currently use) could work for this. \n. I merge it and we get done with it.\n. I think it might be caused if you don't have Mosek installed in your machine so it does not\nget to execute.\n. Not really to be merged (at least for the moment), just to debug why clang travis does not recognize C++11 option.\n. I have temporarily disabled these tests in #1311.\n. Sure, it looks fine!\n. Very nice, thanks!\n. Hmm, since the vector is made of chars, I think that %c was correct.\n. Did you happen to test it anyway? Making and SGVector of chars (like 'S','h','o','g','u','n') and checking it is correctly displayed?\n. Did you run valgrind with it? I find it weird that a missing unref provokes conditional jumps, it should only cause memory leaks. But I can be wrong too.\n. Perfect then! Let's wait for travis to build it and check that everything is fine and later I will merge it. Cool that you detected the bugs!\n. Cool that unit tests are already in. Probably you already took it into account,\nbut in a future future PR examples in libshogun, python_modular, etc, interfaces\nwill be good.\n. I don't use braces for one-liners, with the exception of if-else blocks where one branch is one-liner and the other is not. If one line (e.g. a string used in SG_ERROR or REQUIRE, a function call, etc) gets too long so that I want to break it in a couple of lines, then I use braces even if they are not required.\nI think that is the general style we use, but I am not 100% sure.\n. Could you check whether it is actually just a feeling or truly compilation time increased? To do this, you may use the time command together with make. Be sure to clear your compiler cache (in case you use one) and start from a new build and clean directory before performing the benchmark.\nThere are a couple of reasons why to do these changes. First, it is to centralise the use of all non-Shogun code in one place. Thus, if we decide to change the backend of i.e. a math operation (say we want to use FancyMathLibrary::sqrt(...) instead of std::sqrt(...)) we would only need to change it in  <shogun/mathematics/Math.h>.\nAnother motivation can be for Swig. We are always trying to reduce compilation time and memory consumption. However, I think this won't have much impact.\n@sonney2k, @karlnapf, other reasons?\n. Sure.\nOn 9 Dec 2014 19:59, \"Saurabh Goyal\" notifications@github.com wrote:\n\nJust after making the changes in files i compiled and it took time bit\nmore than usual which i think was because it was building up all the\nchanges but subsequently it is now back to normal compilation time.\nShould i commit the changes and push them ??\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/shogun-toolbox/shogun/issues/1328#issuecomment-66336908\n.\n. Added the unit test ensuring that the number of threads used is equal to one. I am not sure though why it should make a difference with symmetric or non-symmetric matrices. In addition, if the lhs features are different to the rhs features in a CDistance, then the distance matrix is not symmetric in general.\n. @hushell  I have not gone in detail through all of it yet. Do you think we could close this pull request and break it down into pieces and get it merged in several pull requests?\n\nFor example, I am thinking you could push first the changes you have introduced in the classes that you created in the first pull request. Then another one with the new classes, etc. The more and more minal pull requests (as long as the changes introduced are functional), the better\n. It sounds very good!\n. This is looking very nice! Can you please add unit tests and check the comments? Then we are done!\n. It looks like this PR cannot be automatically merged. Could you please rebase develop? If it is needed, force the push or delete your branch in origin and push it again.\n. You will probably get some conflict somewhere when you rebase, which I guess is why this cannot be merged automatically.\n. This looks nice! Please, have a look at the comments and questions and let me know about them.\n. It sounds good.\n. @hushell, did you include these cases (loopy, tree and forest graphs) in the unit test? That should be enough to test that find_set is fine.\n. Yeah, I compared in the configure line the directory included for eigen and\nthe path is different than for gtest. That seems funny.\nOn 6 Aug 2013 08:53, \"Soeren Sonnenburg\" notifications@github.com wrote:\n\nlooks like eigen3 is not found I guess you need to add some include dirs\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/shogun-toolbox/shogun/pull/1370#issuecomment-22161569\n.\n. Sure, this is not to be merged until it works. The idea was to revive Travis until cmake is ready.\n. @vigsterkr, do you mean a FindEigen.cmake or something to make what I am trying here work?\n. Looking good! Can we get unit test and example of use from python modular (or other interfaces if you prefer)?\n. Well, if you consider this is not worh exposing, then I guess it is fine. \n. Nice, thank you!\n. You are right. Clone is making a deep copy, while duplicate doesn't (as you said, the features matrices are shared for instance).\n. I inspected the case for CDenseFeatures and as I undertood it works like this:\n- Duplicate is basically a call to the copy constructor.\n- This copy constructor calls set_feature_matrix which boils down to an assignment (using the operator=) of the SGMatrix. I believe this just increases the reference count of SGMatrix, not copying the data.\n\nBut it can perfectly be that I got something wrong though!\n. The one with a CMatrixFeatures::duplicate is my fault. When I wrote that class I probably didn't understand what CFeatures::duplicate() was about.\n. But one thing I didn't understand well. Before it seemed to be good to keep duplicate as it doesn't copy the whole feature matrix, while clone does it (right?). Then, while should we substitute duplicate for clone now?\n. One detail that I miss is how will this class interact with the current StructuredModel and the solvers?\n. @hushell, thank you for the explanations!\n. Nice! Looking forward to seeing inference implemented in the next PR!\n. Hi Parijat. Great that are you are working on these issues. Please, go ahead!\n. It sounds good, please go ahead.\nOn 17 Oct 2013 20:49, \"mazumdarparijat\" notifications@github.com wrote:\n\nHi,\nI have managed to fix the working of the clustknb function in cases when\ninitial centres is supplied to it. Now the question is how to supply the\ninitial centres to it. Its presently being called inside the train_machine\nfunction as clustknb(false,NULL). Now we have to basically supply its\narguments based on whether user inputs initial centres (/uses KMeans++) or\nnot.\nFor this I propose that we add 2 additional members in CKMeans class:\nmu_initial for storing the initial centres supplied by user and mu_set flag\nwhich tells whether initial centers have been supplied. Then I can write a\npublic function for users to input their initial centers. Additionally I\ncan also overload the CKMeans constructor to accept initial centers.\nHow does this plan sound? Should I go forward?\nRegards.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/shogun-toolbox/shogun/issues/1378#issuecomment-26538028\n.\n. Maybe the wikipedia page on kmeans++ or even the article where it is presented can provide inspiration.\n. It sounds good. I would say you can implement this in a unit test but this will probably look cool also with a graphical example in Python.\n. Sure. Will do this evening.\nOn 13 Jan 2014 13:11, \"Heiko Strathmann\" notifications@github.com wrote:\nOf course, all PR are welcome at any time :)\n@iglesias https://github.com/iglesias coud you take care of this?\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/shogun-toolbox/shogun/issues/1378#issuecomment-32163847\n.\n. Sure. Will do this evening.\nOn 13 Jan 2014 13:11, \"Heiko Strathmann\" notifications@github.com wrote:\nOf course, all PR are welcome at any time :)\n@iglesias https://github.com/iglesias coud you take care of this?\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/shogun-toolbox/shogun/issues/1378#issuecomment-32163847\n.\n. Solved in #1817.\n. Solved in #1817.\n. Hi @ppletscher! I am not well familiarized with this algorithm for inference. Although I will revise the code all the same, would you mind taking a close look at it as well?\n. Sure, will do ASAP. Probably tonight, in about 4-5 hours\nOn 19 Aug 2013 20:36, \"Shell Hu\" notifications@github.com wrote:\n@iglesias https://github.com/iglesias Could you check the code again\nand see if it's good to merge?\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/shogun-toolbox/shogun/pull/1379#issuecomment-22894032\n.\n. Thank you!\n. I apologize, I should have noted that before merging it.\n. Thanks!\n. Good then, thank you!\n. It looks like it is using my gtest in /usr/include instead of the that it downloads. Any way to avoid that behaviour? \n. It is possible to restart complete builds or jobs from the travis GUI. You have to sign in with your github account and I guess that permissions for the repository are also needed.\n. Thank you for the fix!\n. It seems that there is some conflict with C# modular due to this commit, a couple of arguments being called the same in the wrapper. See https://travis-ci.org/shogun-toolbox/shogun/jobs/10279887#L2797 for instance.\n\nCould you have a look at it @votjakovr? Thanks!\n. It is looking good!\nHowever, travis is not happy with it yet. SGObject.clone_equals_JediDiag is failing. @pickle27, could you take a look at it? I guess it might be caused by some member not being initialized in some constructor.\n. It is cool you found the right way to fix this. Very nice job!\n. It is a pity we cannot go all the way with modern C++ :(\n. Very nice initiative! Thank you.\n. Awesome @hushell, I am actually also getting to know about how to do properly the initialization.\nSo just check that your classes are initializing all their member variables in their init methods. I just checked a couple of them and commented above, but didn't look to all of them ;)\n. Nice, ready to go! What's coming next?\n. @votjakovr, why was this actually causing the invalid memory read before? I don't quite see what was wrong.\n. I see. Funny thing is valgrind does not trigger in this example\nhttps://github.com/iglesias/tests/blob/master/c%2B%2B/IterateAndAccess.cc\nwhich I think does pretty much the same thing. I am probably missing something.\n. How do we want to make these ones available? A page in the website with links to nightly build of each day? Maybe we could even reuse the calendar we use for the IRC logs http://shogun-toolbox.org/page/contact/irclogs/\n. What about making a new entry in the doc page http://shogun-toolbox.org/page/documentation/information.\nIt could be for instance next to information. Something called \"Notebooks\". Once clicked on notebooks there are a few lines about how are we using them and them a list of topics and links to the notebooks.\nAlso, we should probably put something about them in home page. Maybe substitute the column with GSoC 2013 now. Some of the figures below could also be taken from notebooks and linked to them when clicked.\nAny thoughts?\n. So what is the state of this one? Is it already done taking into account the page with the notebooks @sonney2k showed us last Friday?\n. Just to be sure of it, what memory errors could rise in the interfaces and not in libhsogun? Providing the same code coverage, I guess that only the ones due to bugs in the SWIG bindings. It may quite possible though that we have better coverage in the examples of the interfaces.\n. Looking good! Just put a couple of words in the comments for Doxygen for the parameters of the setters. I know it is boring :( \n. @sonney2k, updated with CSVFile.\n. Updated with the semicolon in the first line. I am not sure if it makes a difference, most of the other examples do not have it.\n. Maybe I am completely wrong, but I think our quicksort is exactly O(N log N) for sorted arrays:\nhttps://github.com/shogun-toolbox/shogun/blob/develop/src/shogun/mathematics/Math.h#L767\n. I think we can hot-fix our quicksort actually. If we count the number of swaps and this is equal to zero after the outer-most while loop, then there is no need to call qsort for the other two halves.\nWith this, our quicksort goes down to O(n) for sorted arrays, provided that my reasoning is not wrong :)\n. @sonney2k, CSVFile should be ready here now. I also updated the KNN example.\n. All right, thanks! I did submit once the error trace to Viktor, but I don't remember if I needed to configure cmake with this option.\n. Hi @hushell! What about starting off by separating the CMake part from the rest? I think that each of them is worth a separate pull request and also their own commits.\n. Yep, you just need to submit them from different branches.\n. I've got a doubt regarding the labels and features of the FactorGraphMdoel. The labels used for the FactorGraphModel are structured labels where each of them is a FactorGraph, right? Also, the features are a set of FactorGraphs as well?\n. I've got a doubt regarding the labels and features of the FactorGraphMdoel. The labels used for the FactorGraphModel are structured labels where each of them is a FactorGraph, right? Also, the features are a set of FactorGraphs as well?\n. Could you also maybe paste in here the class diagram that you did for the google doc at the beginning? I think there have been some updates. I find it difficult to keep track of what each class should be doing :)\n. Could you also maybe paste in here the class diagram that you did for the google doc at the beginning? I think there have been some updates. I find it difficult to keep track of what each class should be doing :)\n. Aham! I understand more about it now. Thank you so much for the detailed explanation!\n. Aham! I understand more about it now. Thank you so much for the detailed explanation!\n. In the current develop I think the RubyModular issue in Travis has already been fixed, so just rebase this branch and force push.\n. In the current develop I think the RubyModular issue in Travis has already been fixed, so just rebase this branch and force push.\n. As far as I know, those directories contain standalone utilities (like the load matrix Soeren mentioned above). This task is about dropping those and using instead either tools within Shogun core or in the language at use.\nFor instance, drop within tools load matrix and make all the examples relying on it and use e.g. CSVFile.\n. As far as I know, those directories contain standalone utilities (like the load matrix Soeren mentioned above). This task is about dropping those and using instead either tools within Shogun core or in the language at use.\nFor instance, drop within tools load matrix and make all the examples relying on it and use e.g. CSVFile.\n. Good that you discovered this, nice! Why can it cause segfaults though?\n. Oh yes, I didn't consider m_model==model. However, I think there is no problem doing SG_UNREF(model) when model is NULL. I guess the problem is with having model pointing to something that has already been freed.\nI would argue that if ref_count()==1 and m_model==model then there is probably a bug somewhere already before (the same object is in two places, but only one place has claimed property of it).\nIn any case, I think the patch is cool, it makes the method safer.\n. Let's see that Travis is fine with it and I will merge afterwards.\n. Really cool! You have definitely got the grasp to implement ICA algorithms :)\nI just wrote a few minor comments; have a look please and, if you agree with the comments, address them.\nLooking forward to merge and see the result of the Python graphical example!\n. I am just always full of excitement :P\n. Nice, thank you so much for the patch!\n. I am merging it as the tests that are failing are Python integration tests. The SVM ones are unrelated to this patch and the LMNN test fails because the structure of LMNN has actually changed in this patch (new member added).\n. I am merging it as the tests that are failing are Python integration tests. The SVM ones are unrelated to this patch and the LMNN test fails because the structure of LMNN has actually changed in this patch (new member added).\n. @karlnapf, it sounds perfectly possible, sure. Moreover, it sounds like it would be a very nice feature, so this is definitely worth trying.\n. @karlnapf, it sounds perfectly possible, sure. Moreover, it sounds like it would be a very nice feature, so this is definitely worth trying.\n. @karlnapf I added a new member in LMNN, the integration test must be failing due to that\n. @karlnapf I added a new member in LMNN, the integration test must be failing due to that\n. I like very much this idea. I agree with S\u00f6ren, the security is a concern but I believe it should be possible to do without providing sudo rights to the users.\nIt should be enough with providing an ipython session in the form of SaaS in a machine where Shogun is installed. It should then be possible to disable imports of modules that could be dangerous security-wise (like sys I suppose).\n. I like very much this idea. I agree with S\u00f6ren, the security is a concern but I believe it should be possible to do without providing sudo rights to the users.\nIt should be enough with providing an ipython session in the form of SaaS in a machine where Shogun is installed. It should then be possible to disable imports of modules that could be dangerous security-wise (like sys I suppose).\n. It sounds like a nice idea. What do you suggest to use in order to generate the examples?\n. It sounds like a nice idea. What do you suggest to use in order to generate the examples?\n. Sorry, pressed close & comment heh\n. Sorry, pressed close & comment heh\n. It seems that the belief propagation unit test broke.\n. All right, so once you push the new data file updated, update the load in the notebook please and then we are ready to go!\n. Nice idea. The big question is, what about SWIG and Julia? Looking into this.. brb\n. Nice idea. The big question is, what about SWIG and Julia? Looking into this.. brb\n. So it seems that there is no SWIG for Julia (and according to Julia devs, it won't be). A little bit of talk about it https://groups.google.com/forum/#!searchin/julia-dev/swig/julia-dev/C_9ModNoiQU/OYVvTrKPh1kJ.\nTo me this seems that the option left would be an static interface, which I am not so fond of.\n. So it seems that there is no SWIG for Julia (and according to Julia devs, it won't be). A little bit of talk about it https://groups.google.com/forum/#!searchin/julia-dev/swig/julia-dev/C_9ModNoiQU/OYVvTrKPh1kJ.\nTo me this seems that the option left would be an static interface, which I am not so fond of.\n. Ooops yeah, I think I did some refactoring here and there. Is Mosek 7 what we are using now? I will install and fix it ASAP.\n. Ooops yeah, I think I did some refactoring here and there. Is Mosek 7 what we are using now? I will install and fix it ASAP.\n. Fix #1638\n. Fix #1638\n. @vigsterkr you are assigned too :)\n. @vigsterkr you are assigned too :)\n. Thank you @van51!\n. Let's write something about it in the install instructions. Maybe something like known issues and that to fix it one has simply to do cmake -DENABLE_CCACHE=OFF\n. Sorry for the delay in the answer @hushell.\nSee the buildbot's output at http://buildbot.shogun-toolbox.org/builders/nightly_default/builds/570/steps/notebooks/logs/stdio. There are two cells that are failing to complete in the structured output notebook. I bet this is because of the cells that are taking too long to complete. Note that it is taking approx. 50 minutes for the buildbot to create this notebook before it (presumably) times out.\nAlso, there is lot of debug info printed in the buildbot (the BMRM and SGD training algorithms, I guess). Could you please remove this one?\nRegarding the cells that are taking too long, do you have any suggestion so that the buildbot could create the notebook without problems?\n. The first easy thing to do I come up with would be to save the trained models somewhere (this is, serialize the machine after it has been trained) and basically in the notebook instead of training, you just load/deserialize this object. You could just write a few words in the notebook saying something like \"here instead of training, we will just load the trained model for timing reasons. If you would like to train the model instead, then you just need to blah blah\".\nI guess that the serialized machines could be uploaded to the data repository.\n@sonney2k, any thought about this? To sum up, use serialization to avoid times out in the buildbot creating ipython notebooks.\n. That is indeed the other option. Although that way we would probably lose the scope of the notebooks solving slightly complicated tasks closer to \"real-world\" data sets.\n@sonney2k, what pros do you see solving simpler stuff wrt saving trained models?\n. Or maybe you can just put to false whichever arguments you are setting to true in order to print debugging info.\nIs this verbose output used anywhere when running the notebook? If it is not, then we can safely disable it in the notebook :)\n. Yeah.. well, it is a reason :) All right, so then maybe the best solution is to load the whole data set but then use for training a small subset, saying that we do this for time constraints. I did this for the LMNN notebook.\n. I see. Then do as you suggested before: sg_print -> sg_debug.\n. Nice, thanks!\nOn 9 Oct 2013 18:36, \"Heiko Strathmann\" notifications@github.com wrote:\n\nsolved in #1714 https://github.com/shogun-toolbox/shogun/pull/1714\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/shogun-toolbox/shogun/issues/1713#issuecomment-25986492\n.\n. lgtm\n. Oops! Forgot the link to the preview http://nbviewer.ipython.org/7182635.\n. Done in #1744 \n. All right, this looks pretty much ready now! Please, just remove the commented SG_UNREF and if you can issue a PR with only one commit.\n. @yage99, could you please just write here in a comment very roughly the steps you followed to compile the mex file manually? I will try it out on my machine and, once I get it working, I will write the matlabinstall.md you suggest.\n\nA short description of how you did it will greatly help me!\n. Thank you so much for your detailed answer @yage99!\n. There are still some leaks related to the use of CDenseFeatures::duplicate in CKNN:store_model_features. Long story short, when using duplicate the integer that holds the reference count is copied, even though the duplicated features are not shared by the same objects as the original features, so in the end the duplicated are never freed.\nI did not find a way around this though. Any idea?\n. #1800 solves the leak on features creation in the example, though the ones on duplicate call are still there. @Saurabh7, run valgrind again and show me log please.\n. @karlnapf, I could use some help here. I don't understand why the features created in the example are freed, whilst the ones created on call to duplicate leak. It seems that the reference count object is shared (due to use of copy constructor) so according to my understanding they should be freed altogether.\n. All right! So the logic is looking good, but please address the comments above.\n. Thank you for your work! Not so much left to go :)\n. @sonney2k, have a quick look and merge if you agree please.\nBasically my point is that since the main function in the structure_multiclass_bmrm.py example has no return value, then the integration test does not actually check anything (right?). Thus, I opt for adding it to the blacklist and one example less failing.\nI didn't find out though why it actually fails. The exact error is \nstructure_mutliclass_bmrm.py setting 1/1                     EXCEPTION 'module' object has no attribute 'structure_mutliclass_bmrm'\n. Agree, will do something about it this evening probably.\n. I see three commits instead of just one here :)\nOnly yours should be the one to keep.\n. I see three commits instead of just one here :)\nOnly yours should be the one to keep.\n. You can extract patches from commits and then apply them with git. You could do that with your commit and apply it to a local clone of upstream develop. I don't remember right now the exact commands and options to extract and apply the patches but it should be easy to find in the git doc by keyword patch. Let me know if you don't find them.\n. You can extract patches from commits and then apply them with git. You could do that with your commit and apply it to a local clone of upstream develop. I don't remember right now the exact commands and options to extract and apply the patches but it should be easy to find in the git doc by keyword patch. Let me know if you don't find them.\n. I am not sure what you did :) But they should definitely not be there.\n. I am not sure what you did :) But they should definitely not be there.\n. Thanks!\n. Thanks!\n. Sure!\n. Sure!\n. I fixed a couple of things:\nshogun-toolbox/shogun@3ee2a0b1fb421556531028b801b50d0da670f0ba\nshogun-toolbox/shogun@e0a2c6acd16c962c72adee48bbcbc4b31d51ff7a\n. I fixed a couple of things:\nshogun-toolbox/shogun@3ee2a0b1fb421556531028b801b50d0da670f0ba\nshogun-toolbox/shogun@e0a2c6acd16c962c72adee48bbcbc4b31d51ff7a\n. I find the second one a little bit funny. @mazumdarparijat, how did you manage to run the example using Kmeans? Note the lower case m :)\n. I find the second one a little bit funny. @mazumdarparijat, how did you manage to run the example using Kmeans? Note the lower case m :)\n. I hadn't heard about these algorithms before so I cannot say much. But I see no reason why not having them so I say go ahead ;)\n. I hadn't heard about these algorithms before so I cannot say much. But I see no reason why not having them so I say go ahead ;)\n. It is not so easy regarding the buildbot. Long story short, the buildbot has not been complaining about tails (at least in integration tests) even if they were there.\nSee for instance test_python_modular of this build and find statistics_linear_time_mmd: http://buildbot.shogun-toolbox.org/builders/deb3%20-%20modular_interfaces/builds/2070\nAlso these IRC logs from 20:48: http://www.shogun-toolbox.org/irclogs/%23shogun.2014-01-17.log.html\n. Finally we have a winner!\nshogun-toolbox/shogun@b5e701cbbd79468ce7472076518fdc2f0b9fed4c\n. Let's do that indeed.\nOn 21 Jan 2014 05:50, \"Soeren Sonnenburg\" notifications@github.com wrote:\n\nargh but well done @iglesias https://github.com/iglesias! Now that we\nare back to green lets have a no-merge policy once any of the stuff is red.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/shogun-toolbox/shogun/issues/1819#issuecomment-32821333\n.\n. I will have a look soon. Could you please do a unit test? Have a look at tests/unit.\n. I will have a look soon. Could you please do a unit test? Have a look at tests/unit.\n. I had actually figured out that these other KMeans methods would come implemented in their own classes. Modularity is good. It could be easier to maintain as well.\n\nLet's wait for a second opinion though. @karlnapf?\n. I had actually figured out that these other KMeans methods would come implemented in their own classes. Modularity is good. It could be easier to maintain as well.\nLet's wait for a second opinion though. @karlnapf?\n. I believe that condensing several methods for KMeans computation in the same class is not the most practical option. Something modular divided into different classes would be much clearer.\n. It sounds reasonable to me!\nThink also about the interface you want to give for the user. For instance, at first sight I see two possibilities: (1) the user would create an instance of a different class depending on the KMeans method to use; (2) the only class visible from the outside is KMeans, and the particular method is selected using an enum variable.\n. I didn't check everything just yet. But will wait until the next batch of changes to continue. And sorry about the many comments, I guess I am sort of a picky person :)\n. The integration tests are probably failing because you have changed the KMeans class adding new attributes. These tests use the return values in the examples and compared them with the objects stored in files inside the data repository. You can re-generate locally these files using the generate script inside tests/integration/python_modular and see if the integration tests work well. If so, then you can commit the new files into the data repository.\n. I would prefer to see the functions where you have implemented mini-batch kMeans and Lloyd's kMeans inside a class. You could for instance do a KMeansImpl class where you put these methods. I implemented something similar for LMNN last summer using a class LMNNImpl.\n. Add also unit tests please. There should be a file with unit test for KMeans that at least test for each of the methods you are implementing and kmeans++. The idea behind the unit tests is to use simple data, you could for instance use the same rectangle you are using for the example.\n. Rebase upstream develop and try again the test, it should be fixed now with shogun-toolbox/shogun@15525454831a88ea9d7820520c85866ffa63c332.\n. @besser82, I thought the cmake change you did and we discussed about fixed this issue already. Any idea?\n. Nice job @mazumdarparijat. I am merging this now since everything is fine with Travis and there are just very minor things to fix. But please fix them when you find a moment ;)\n. Sure @mazumdarparijat. Those are indeed interesting ideas. Also, methods like OPTICS or hierarchical clustering could be nice as well.\n. @mazumdarparijat, actually I think it would be more interesting if you feel like doing an ipython-notebook with what you have done so far related to KMeans. Check the other ipython-notebooks we have done to get an idea. Basically, in these notebook we explain a little bit the theory of a method, put examples with Python code and show some nice plots.\nWith kmeans++, Lloyd and mini-batch method I believe there is enough material to do a nice notebook. Then, as more KMeans methods are added the notebook can be extended.\n. It would actually be better if you can submit these changes alone.\nSmall pull requests have some advantages. For instance, they are much easier for us to revise :-)\nSo please submit these changes in a single pull request without other new contributions.\n. Please rename also the lKmeans and mbKmeans header and implementation files.\nLooking good. Let's leave Travis working on it and I will merge it afterwards.\n. I think that you forgot to rename the files in the includes as well.\n. Let's try to avoid merging with commits that break the build, like the KMeans files renamed one.\nLuckily, you can do this pretty easily with git. Use git reset to go back to the commit that is broken. Apply the changes you have in the last commit (correct includes in KMeans) and introduce those changes in the broken commit by using git commit --amend.  Then just push again (note that you will need to force this push since you have rewritten the history of your local branch).\n. Nice work!\n. Good! Could you please upload the notebook somewhere so that we can read it before getting it merged? The easiest option is that you copy the contents of the ipynb file in a gist and then introduce the number of that gist here.\nNote that this method displays the notebook in a static way, so remember to copy in the gist the notebook with the outputs generated.\n. All right the, closing for now. Looking forward for a new one :)\n. Great Parijat. I didn't look at the references yet, but it looks like you could have finally hunted down the bug in LARS.\nThe unit tests comparing with standard implementations sound terrific. Please go ahead!\n. All right @pranet, I get it now. Thanks for the clarification!\n. @aroma123, so say that tomorrow I make a Markov network class in Shogun and I want to be able to load Markov networks from UAI files. Then I should make CMarkovNetwork::load_from_uai, CMarkovNetwork::save_to_uai which are going to be pretty much the same as CFactorGraph::load_from_uai and its brother.\nSee the point?\n. Hey @hushell, @abinashpanda , did the part CFactorGraph::save(CUAIFile*) finally happen? I see the CUAIFile implementation but no how it can be used from FactorGraph.\n. It is merged now @mazumdarparijat, let's see if everything is all right now.\n. It is merged now @mazumdarparijat, let's see if everything is all right now.\n. I just restarted the build (I should have just restarted the job though, sorry my bad). Let's see.\n. I just restarted the build (I should have just restarted the job though, sorry my bad). Let's see.\n. It has to be related to this\nCMLDataHDF5File* hdf = new CMLDataHDF5File((char *)\"australian\", \"/data/data\");\nThis Australian data is probably not available any longer for some reason. I wonder however where should it be located, if in the local machine or it is downloaded from somewhere using curl (although I don't see the complete url for the second case). @sonney2k, any idea?\n@mazumdarparijat, it might be that \"it works\" in your machine because you don't have installed either hdf5 or curl. See the www.github.com/shogun-toolbox/shogun/blob/develop/examples/undocumented/libshogun/library_mldatahdf5.cpp.\n. Understood, thanks!\n. @mazumdarparijat, do you want to apply that fix too? :-)\n. Issue open @ #1924.\n. Solved in #1945.\n. Solved in #1945.\n. Solved in #1938.\n. It is looking good to me!\n. Thanks!\n. Yes, that is probably right. We should guard the unit test with #ifdef HAVE_LAPACK. Please, feel free to do it and submit a pull request.\n. Solved in #1933.\n. Feel free to write your name below if you are working on this task! This way we avoid wasting manpower on the same algorithm.\n. Integrating nanoflann might be another possibility indeed. I am not familiarized with it at all so I will have a look, but feel free to go on your own and study whether we could integrate, bundle it, etc.\n. @armanform, could you please update #2095 instead? Just get rid of the previous commits in that pull request and issue the one you have linked to in your previous comments.\n. I think this took eventually a different path since we have the kd-tree but it is not usable as the cover tree is from knn. Still, it is possible to do knn classification with the kd-tree using an instance of the tree directly. Is that right, @mazumdarparijat?\nI didn't see any benchmark comparing cover tree and kd-tree.\n. I think this took eventually a different path since we have the kd-tree but it is not usable as the cover tree is from knn. Still, it is possible to do knn classification with the kd-tree using an instance of the tree directly. Is that right, @mazumdarparijat?\nI didn't see any benchmark comparing cover tree and kd-tree.\n. The cover tree implementation is under lib. JLCoverTree is the class.\n. The cover tree implementation is under lib. JLCoverTree is the class.\n. Neat solution @mazumdarparijat, thank you!\n. I just added this to the NEWS ;-)\n. Feel free to write your name below if you are working on this task! This way we avoid wasting manpower on the same algorithm.\n. #2383 \n. #2383 \n. This is almost ready to go @mazumdarparijat, great!\nHowever, please test thoroughly with valgrind the unit tests and maybe some other naive example of your own where you play around with the setters and getters of TreeMachineNode and BinaryTreeMachineNode so we are sure we are not leaking memory anywhere. If you push such a naive example as a unit test, that would be great :-)\n. I think we can do it like that @mazumdarparijat. We don't do ref counting of m_parent in TreeMachineNode.\nHowever, I think that the destructor should then go through the list of children and put their m_parent to NULL. Does that make sense to you?\n. Great job @mazumdarparijat, thanks!\n. Great job @mazumdarparijat, thanks!\n. Closing as something went wrong when updating the PR. Looking forward for the new one :-)\n. There is something a bit weird with the commits. Once you have opened a pull request, just add new commits to your local branch or amend the last commit (in the latter case you will need to force the push). Before pushing, remember to fetch upstream and rebasing against upstream/develop.\n. There is something a bit weird with the commits. Once you have opened a pull request, just add new commits to your local branch or amend the last commit (in the latter case you will need to force the push). Before pushing, remember to fetch upstream and rebasing against upstream/develop.\n. Hey @ahcorde, one commit is fine, don't really worry about the history of the previous commits. It would be nice though if the commit message is more suitable to the work you've done (something like eigenfaces example with OpenCV or so).\nIn case you decide to do this, you can do it very easily by\ngit commit -amend\nthen re-write the new commit message\ngit push -force origin develop.\n. Hey @ahcorde, one commit is fine, don't really worry about the history of the previous commits. It would be nice though if the commit message is more suitable to the work you've done (something like eigenfaces example with OpenCV or so).\nIn case you decide to do this, you can do it very easily by\ngit commit -amend\nthen re-write the new commit message\ngit push -force origin develop.\n. Nicely done Parijat! Do you think it is possible to get a couple of simple unit tests for train and apply? Something as simple that can be worked out by hand or compared to another implementation would be perfect. We can use them as well to ensure we don't have leaks here.\nI say we wait for the unit tests before merging if that's fine with everybody.\n. Yes, I think that sounds good!\n. Thanks for making the documentation more clear!\n. It must be something unrelated to this pull request. One of the jobs stalled for some reason. I will restart it now.\n. Sorry for the delay reviewing this @dhruv13J.\nI like the patch, it is starting to get in good shape. Please address the comments and/or answer the questions. Do not worry about that there are many comments, most of them are just simple issues regarding style.\nHowever, more important are the unit tests. Before we can merge we need to have at least a simple unit test checking that the kd-tree from nanoflann is indeed working good. This can be a very simple example, say a few points in 2D for which KNN can be easily worked out by hand or so.\nThanks your for work!\n. Great to hear that @dhruv13J!\n@vigsterkr, how will we tackle the non-constness of get_lhs()? Will we have to auto-magically apply during bundling the changes that Dhruv applied?\nI guess the option of refactoring so get_lhs() & Co. become const is not viable as they increase the reference of the returned features, making them non-const in fact.\n. Very nice job @mazumdarparijat! The example certainly looks more like something to include in a notebook than the typical examples we have for interfaces. But for me it would be all right to keep it like this for the moment (or maybe under graphical even? It doesn't contain plots but print on screens, but to me it certainly looks more like the typical illustrations we do in the graphical examples than an integration test/model of how to use the API).\nThere's one but though. I am not sure if the integration tests are failing just because the example is new, or because the example is just not following the the structure of the other integration tests and that messes up with the tester. @karlnapf, @vigsterkr, @sonney2k, @lisitsyn, do you know about this?\n. That's exactly what I meant with the example. Neat!\n. It looks like travis build will start sooner or later www.travis-ci.org/shogun-toolbox/shogun/builds/21108637.\nCould you run the integration test locally please? Make sure that the ID3 example you just added is actually run :)\n. Great! Tell me about the break lines for doxygen and we merge!\n. Then we are ready to go. Thanks for your work!\n. Ideally, we should wait until #2015 is merged before solving this issue.\n. Ideally, we should wait until #2015 is merged before solving this issue.\n. Closed without noticing. Just in case there was a misunderstanding, I meant that this issue should wait until we are done with the nanoflann integration.\n. I have to admit that this is far from being ready, but still as you mentioned it is just the first iteration :-)\nLooking forward to seeing something more complete.\n. Closing for the time being. Feel free to open a new pull request when you get more work done and do what Viktor and I have told you.\n. Just a small remark to what Thoralf said a couple of comments ago. In fact, thanks to the SWIG director classes, it is possible to create a structured output model from Python as well. For an example, see https://github.com/iglesias/linal/blob/master/graph/structure_grid_crf.py#L20.\n. Great @achintp! We are looking forward to seeing this!\n. Solved in #2063.\n. Very nice that you put this in. It will be ok that there are two commits this time, but please in the future it is enough with only one commit for this. For the second one you should just have amended the previous commit (with -amend). Note that this will require you to force (with -force) your push in case you had already pushed after the first commit.\n. Why do we actually need to include this header in all those files?\nAlso, there seems to be something funny with the commits in this pull request. I don't understand where three of them came from.\n. All right, I just saw the issue that Soeren opened, sorry about my question in the comment above :)\nIf you can just put the right commit and remove the other three, this should be ready!\n. Thanks!\n. Travis works commit by commit in the pull request I think. What I think you should do is to issue just one commit. I guess it makes sense since you are just doing one thing, but in a bunch of files.\n. Thanks @frank0523!\n. Solved in #2080.\n. Also, there should not be these many commits when we want to merge. For this work, just one commit is probably enough.\n. Cassio, I am sorry for the negative feedback but this is very far from a correct KDE implementation. Does this code even compile?\nI think you can benefit from reading other classes that are already implemented. In particular, I suggest you to start with Distribution and Gaussian. They are inside the distributions directory as well.\n. Nice one! Travis failed but it is not due to the changes here. I think it is my mistake actually, I merged a couple of commits to shogun-data for the nanoflann's PR (there is some changes in the KNN class) and the PR has not already been merged.\n. All right, Travis is back to green now!\n@mazumdarparijat, once you address the first comment by Heiko (which should be completely straightforward for you btw) we are ready to merge :-)\n. Lgtm! If @karlnapf agrees as well, it should be ready to merge.\n. All right, I merge now and take the responsibility ;)\nWhat's next @mazumdarparijat? :)\n. I say we could try to follow in the graphical examples a similar structure to what we follow in the non-graphical ones. For instance, we make two functions, one with the name of the file and another one with the same name and _plot, or _graphical appended. The output of the first function is given to the second one as input and, in addition, it is used for integration tests.\n. Three general comments:\n- Be consistent with how you capitalize Shogun. Use either shogun or Shogun, but do it consistently.\n- Although it is rather common to use it, dataset is not a real word I believe. I like better to use data set.\n- I think that 17 commits (one per notebook) is a little bit too exaggerated taking into account that these are minor changes. Maybe you could squash all the commits and just issue one. But wait for a second opinion on this if you want.\n. Sure, no problem, you are welcome! Thanks to you for taking care of doing it :-)\n. I think you shouldn't merge your local branch in develop as you do in 3.\nYou should go back to your branch and rebase against develop.\nOn 25 Mar 2014 02:17, \"kislayabhi\" notifications@github.com wrote:\n\nSorry for all these commits.\nI am doing something terribly wrong here. Please see to it.\nI had a local feature branch on which I worked and committed my\nchanges(the last one being named \"all notebooks abstracts revised 3rd\ntime\"). By the time I wanted it to merge with the develop, the\nupstream/develop has moved forward with one step. Methodically I did,\n1. git checkout develop\n2. git pull --rebase upstream develop.\n3. git merge local_branch_name\nthis resulted in lots of merge conflicts.\nI cleaned the develop branch staging and the working area using \"git reset\nHEAD\",\"git rm \" and \"git checkout --\" .\n4. I did a \"git commit -m \"3rd revision notebook abstracts\"\". I thought\nthis worked as\nperforming \"git merge local_branch_name\" shows output \"Already up-to-date\".\n5. I did a \"git push origin develop\". This has resulted in so many\npolluted commits shown above where there should only be 1. Moreover the\norder is also jumbled as the one I wanted to show you was the commit shown\nat the 2nd no. i.e \"all notebooks....\"\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/shogun-toolbox/shogun/pull/2080#issuecomment-38521310\n.\n. Thanks @kislayabhi!\n. Nope, no worries.\n. I believe this one is a duplicate of #2063 \n. Closing as this has been solved in #2063. Thanks anyway for the work @nikolis!\n. @grecocd, this is indeed looking better than before but, still, there's work to do.\n\nFirst of all and most importantly, why aren't you compiling locally the code before making pull requests? Do you have any problem doing that or don't know how to do it? This is really basic, the code that you send in a pull request should compile perfectly.\nAlso, note that in the final pull request you'd like to see merged, you will have to include only one commit (or at least less than the ones you are using here). It wouldn't be good to include 7+ commits for this work. The fact that they do not compile makes it worse actually.\n. So why is it that there are several lines with MultilabelLabels commented @Jiaolong?\n. I think there are still issues we commented in the previous PR that are unsolved, e.g. the unit tests do not really look like unit tests, they are still with GPL, etc...\n. @vigsterkr, it would be great if you could quickly check the cmake part :)\n. Well, for some reason this cannot be merged automatically from github. I guess there are some conflicts. Please rebase against current develop and resolve conflicts if need be. Also squash the commits, 13 commits for this is way too many.\n. Well, for some reason this cannot be merged automatically from github. I guess there are some conflicts. Please rebase against current develop and resolve conflicts if need be. Also squash the commits, 13 commits for this is way too many.\n. Well, for some reason this cannot be merged automatically from github. I guess there are some conflicts. Please rebase against current develop and resolve conflicts if need be. Also squash the commits, 13 commits for this is way too many.\n. Well, for some reason this cannot be merged automatically from github. I guess there are some conflicts. Please rebase against current develop and resolve conflicts if need be. Also squash the commits, 13 commits for this is way too many.\n. All right, looking forward. Hope you and your hand are completely recovered now.\n. There are a few comments not addressed in the unit test.\n. @vigsterkr, can you quickly review the cmake part?\n. All right @dhruv13J, let me check out your branch and test it locally. If everything works all right, I will merge it. I won't have time for this today, but I will do it during the weekend, most likely tomorrow.\n. Hi @dhruv13J,\nI have checkout your branch. Can you please solve this error?\n```\nInstall the project...\n-- Install configuration: \"\"\nCMake Error at cmake_install.cmake:36 (FILE):\n  file cannot create directory: /usr/local/lib/pkgconfig.  Maybe need\n  administrative privileges.\nmake[3]:  [install] Error 1\nmake[2]:  [nanoflann/src/Nanoflann-stamp/Nanoflann-install] Error 2\nmake[1]:  [CMakeFiles/Nanoflann.dir/all] Error 2\nmake:  [all] Error 2\n```\nI understand this is related to permissions to write under /usr/local/lib. However, the compilation with bundle Eigen enabled works well without requiring additional permissions.\nThere is also this warning you may want to fix:\n```\nCMake Warning:\n  Manually-specified variables were not used by the project:\nCMAKE_C_FLAGS\nNANOFLANN_INCLUDE_INSTALL_DIR\n\n```\nfor the second variable above.\n. @dhruv13J, ping.\n. It is a shame that you work you did turns out to be in vane. @mazumdarparijat, you compared our kd-tree with nanoflann and found the form to be more efficient; do you see any scenario where it could be useful to use nanoflann instead of your implementation?\n. Hi @grecocd. This is indeed improving and getting to a better state. See the comments above please.\nAlso, about your question concerning SGMatrix<index_t> and SGMatrix<float64_t>, I find it odd that what you have to use to compute the the PDF are the indices of the nearest neighbours. Could you please argue why is it that the case or double check with the reference implementation?\n. What would be interesting is if this can actually be used from Shogun. At least, there should be an integration with the KNN class. The test should also be done with proper unit tests.\n. The txt file is not necessary I believe.\n. Hmm I am a little bit concerned about the pseudocode not been correctly displayed. Let's see how it looks like when the buildbot renders it. In any case, have you tried looking for an alternative to write the pseudocode? Maybe it is possible to use latex for that?\nEDIT: Just did a quick search and didn't find any solution to write pseudocode in the notebook with latex. Don't worry much about this, hopefully it will work all right in the buildbot.\n. For the last exercise, could you mention something about the oscillatory behaviour of the error plot vs the training data set size?\nIn fact, I think that behaviour stems simply from randomness in the choice of the training data. I would expect to see that the error decreases as the training set becomes larger. Could you maybe repeat the experiment so that for a fixed training data set size, you repeat the process several times and finally average the classification error?\n. Nice notebook! Take some of the comments above as suggestions, not all of them are pointing out errors. Thanks for your work!\n. Great!\nInstead of hard-coding the 3 (the number of times you repeat the experiment for each training data size), use a variable please. It eases re-usability because it is less error prone since this 3 should be changed in two places if someone would like to use a different number of repetitions.\n. Nice! Thanks for your work.\nRemember to send another PR to update the tip of the shogun-data repository with the new files you added for this notebook.\n. BTW, I think you can start using another branch, pca is sort of a strange name for the ongoing work :-P Branches are for free and it just takes one second to create a new one\n. I don't see that many pure virtual methods in CDistribution. There are basically 5 methods: train, get_log_likelihood_example, get_num_model_parameters, get_log_model_parameter, and get_log_derivative. The two first ones must be implemented for sure. I am not sure about the usefulness of the third and the fourth, but I think they are in fact trivial to implement. The last one is the only one that I doubt whether it makes sense for every possible distribution-parameter combination.\nFor CSGObject the only pure virtual method is get_name. So I agree with @vigsterkr, I am not sure what you meant in your last comment, @grecocd.\n. Also @grecocd, as @karlnapf mentioned, please just update this pull request and do not keep on closing and issuing new ones. It makes it harder to track what the state of your work is.\n. Yeah, using Distribution for now sounds good to me!\n. Ok, let me check it now. But please write a more descriptive commit message than working.\n. Ok, let me check it now. But please write a more descriptive commit message than working.\n. All right, I am closing this for the time being then.\n. All right, I am closing this for the time being then.\n. At the beginning, it should be sudoku instead of suduko.\n. It is looking fancy!\n. @ahcorde, you should commit to this repository (shogun-toolbox/shogun) the new version of the shogun-data repository, which is a submodule of this repository. If you issue git status in the shogun repository, you should see something like new version of data.\n. For a single feature vector, or for an instance of CFeatures. In the latter, the data will be stored in the feature matrix.\n. ping @ahcorde :-)\n. I agree with @khalednasr comments! @armanform, have a look also at the error with the travis build please.\n. @mazumdarparijat, the nanoflann integration is still WIP.\n. This patch is still very preliminary @armanform. See the comments above (although I have not gone in depth through all the code). If you are still interested in pushing this, please do so and we will get this ready in a few more iterations.\nAlso, note that the patch did not compile in travis. Remember to build the code and test it locally before submitting pull requests.\nThanks for your efforts, we really appreciate it!\n. As mentioned before, this is a nice little program. Still, it cannot be merged for the reasons pointed out by Lisitsyn above. Thus, I am closing for the time being.\n@msahasrabudhe, feel free to submit another pull request if you want to see this in Shogun.\n. Thank you!\n. Thank you!\n. Nice pull request @mazumdarparijat. Just a few issues to attend (most importantly the subsets one). Also, I find this code nicely written, it's a pleasure to review it :-)\n. @mazumdarparijat, thanks for addressing the issues so promptly. It is ready to merge now!\nWhat's coming up next? The visualisation of the trees using graphviz or do you have something else in mind?\n. I understand the problem, and also think the solution you are suggesting is neat.\n@mazumdarparijat, do you want to work on this task? Once we have both modes for subsets, I think it could be fun to use a data set with many features in which the decision tree created is rather large, and benchmark the use of memory. My guess is that the difference might be not that big: consequent subsets will require less memory as less and less features are available.\n. I do not dislike the set_amnesic_mode(bool) idea.\nThe CFeatures::set_subset_stack(CSubsetStack*) idea could work as well I think. What advantages/disadvantages do you see wrt the current approach @tklein23?\nBy the way, one stupid question, why CFeatures::get_subset_stack() does not increase the reference count of m_subset_stack. What is it I am missing?\n. I do not dislike the set_amnesic_mode(bool) idea.\nThe CFeatures::set_subset_stack(CSubsetStack*) idea could work as well I think. What advantages/disadvantages do you see wrt the current approach @tklein23?\nBy the way, one stupid question, why CFeatures::get_subset_stack() does not increase the reference count of m_subset_stack. What is it I am missing?\n. With the current approach I meant how it is done at the moment, with features and labels instantiating the SubsetStack. I thought that was one of the points you wanted to revisit as you ask why the features were instantiating SubsetStack on their own. \n. With the current approach I meant how it is done at the moment, with features and labels instantiating the SubsetStack. I thought that was one of the points you wanted to revisit as you ask why the features were instantiating SubsetStack on their own. \n. Anyhow, I do agree with you. Using two different subset classes, one for each behaviour (with or without stack), does seem to be a well-designed solution.\n. Anyhow, I do agree with you. Using two different subset classes, one for each behaviour (with or without stack), does seem to be a well-designed solution.\n. There are some rules in there I didn't know about in fact. But good to know about them! LGTM\n. @tklein23, just open another file already written and follow its style. Doing that you are safe. You are writing a unit test, then open an already written unit test. You are writing a class, the same, etc. Just try to choose files that are relatively recent.\n. I don't get your point. You can have template methods in non - template\nclasses. See recent max/min argmax/argmin transition from SGVector to CMath\nby Sanuj.\nOn 18 Dec 2014 21:39, \"Saurabh Goyal\" notifications@github.com wrote:\n\nI am trying to move compute_eigenvectors to CMath but since matrix in\nSGMatrix is defined as \"T* matrix\" whereas while moving to CMath matrix\ncant be defined in this way as T does not name a type here. What default\ntype for matrix should i use in CMath??\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/shogun-toolbox/shogun/issues/2151#issuecomment-67552899\n.\n. I think that the concept is looking good!\n. @mazumdarparijat, see above.\n. @mazumdarparijat, see above.\n. Thanks @kislayabhi!\n\nBy the way, in the PCA notebook (http://shogun-toolbox.org/static/notebook/current/pca_notebook.html) the second plot in the section \"PCA on a 3D data\" (the one that is just before the eigenfaces stuff start) looks a bit weird. At least in the browser I am using now. Some of the points and red lines overlap with the figure title and the legend is misplaced as well. Could you please check if you see it like that too and fix it if that's the case?\n. Thanks @kislayabhi!\nBy the way, in the PCA notebook (http://shogun-toolbox.org/static/notebook/current/pca_notebook.html) the second plot in the section \"PCA on a 3D data\" (the one that is just before the eigenfaces stuff start) looks a bit weird. At least in the browser I am using now. Some of the points and red lines overlap with the figure title and the legend is misplaced as well. Could you please check if you see it like that too and fix it if that's the case?\n. The three other figures before the one I mentioned in the last comment have similar issues. Please, have look.\n. The three other figures before the one I mentioned in the last comment have similar issues. Please, have look.\n. @kislayabhi, \n400 : Bad Request\nWe couldn't render your notebook\nPerhaps it is not valid JSON, or not the right URL.\nIf this should be a working notebook, please let us know.\nThe error was: Error reading JSON notebook\n. Now the figures are looking prettier, definitely. The legends outside the frame and no overlap between plot elements and figure titles. Good!\n. Thanks for the prompt fix @mazumdarparijat\n. Thanks for the prompt fix @mazumdarparijat\n. Did you check with valgrind the test that Thoralf pointed out was leaking memory? From now on, please try to remember to check the examples and tests you push with valgrind.\nAnyway, it is normal that leaks slip through sometimes, so do not worry that this happened!\n. Perfect then @mazumdarparijat, ready to be merged!\nWhat are you planning to work on next? :-)\n. Thanks!\n. I like how this notebook looks like! @khalednasr, I wrote a few tiny comments/suggestions. Also, for the next time could you please upload to the nbviewer the notebook with the outputs? To our repository always clear the outputs before committing (as you have done) but the viewer is nicer with the outputs and better to review.\n. I like how this notebook looks like! @khalednasr, I wrote a few tiny comments/suggestions. Also, for the next time could you please upload to the nbviewer the notebook with the outputs? To our repository always clear the outputs before committing (as you have done) but the viewer is nicer with the outputs and better to review.\n. Dumb question, what do you mean with support? We have this SGNDArray.  However, IIRC, it lacks of SWIG type maps.\n. Thanks for applying the fixes. The visualisation looks fancy too. Thanks!\n. Thanks for applying the fixes. The visualisation looks fancy too. Thanks!\n. @tklein23, feel free to re-open if you get back to it :-)\n. @Jiaolong, make sure that you have the latest versions of the shogun-data submodule and the develop branch of shogun.\n. Make a commit inside the submodule, push, and then issue the pull request\nusing the web interface in github in the shogun-toolbox/shogun-data\nrepository instead of shogun-toolbox/shogun. If you haven't done so before,\nyou will need to fork shogun-data as well.\nOn 19 May 2014 10:42, Jiaolong notifications@github.com wrote:\n\nHi, I have run 'tester.py' on the related examples and all is well:\npython tester.py structure_discrete_hmsvm_bmrm.py structure_factor_graph_model.py structure_multiclass_bmrm.py structure_plif_hmsvm_bmrm.py\nstructure_discrete_hmsvm_bmrm.py setting 1/1                 OK\nstructure_factor_graph_model.py setting 1/1                  OK\nstructure_multiclass_bmrm.py setting 1/1                     OK\nstructure_plif_hmsvm_bmrm.py setting 1/1                     OK\nI also tried running generator.py before 'tester.py' which is supposed to\nbe the standard way, and one file in the shogun-data directory changed.\nI just wonder should I commit this changed file into shogun-data?\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/shogun-toolbox/shogun/pull/2189#issuecomment-43478208\n.\n. Make a commit inside the submodule, push, and then issue the pull request\nusing the web interface in github in the shogun-toolbox/shogun-data\nrepository instead of shogun-toolbox/shogun. If you haven't done so before,\nyou will need to fork shogun-data as well.\n\nOn 19 May 2014 10:42, Jiaolong notifications@github.com wrote:\n\nHi, I have run 'tester.py' on the related examples and all is well:\npython tester.py structure_discrete_hmsvm_bmrm.py structure_factor_graph_model.py structure_multiclass_bmrm.py structure_plif_hmsvm_bmrm.py\nstructure_discrete_hmsvm_bmrm.py setting 1/1                 OK\nstructure_factor_graph_model.py setting 1/1                  OK\nstructure_multiclass_bmrm.py setting 1/1                     OK\nstructure_plif_hmsvm_bmrm.py setting 1/1                     OK\nI also tried running generator.py before 'tester.py' which is supposed to\nbe the standard way, and one file in the shogun-data directory changed.\nI just wonder should I commit this changed file into shogun-data?\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/shogun-toolbox/shogun/pull/2189#issuecomment-43478208\n.\n. Lgtm. Only a minor remark, the indentation in the Doxygen of svm_bmrm_solver seems odd.\n. Make non-printable characters (I mean white spaces, tabs, etc) visible in your editor. Then you should see that there's something weird going on there. Maybe one line is using both tabs and white spaces to indent.\n. Done. Thank you @jiaolong.\n. How's it possible that we got this PR open for so long?\n. So, if I understand everything correctly, the idea is that we are able to export to graphviz any tree we may have around in Shogun, now that we want to do this for ID3. ID3 is just a type of tree, but a method very similar to the one you have done could be used to print pretty much any kind of tree.\n\nFor that reason, it is more convenient to have a class under io, for instance, CGraphvizFile. Let id3 be an instance of ID3, then we could just do something like id3->save(CGraphizFile*). Have a look at the io examples and how it is done for example with the features for details. \n. So, if I understand everything correctly, the idea is that we are able to export to graphviz any tree we may have around in Shogun, now that we want to do this for ID3. ID3 is just a type of tree, but a method very similar to the one you have done could be used to print pretty much any kind of tree.\nFor that reason, it is more convenient to have a class under io, for instance, CGraphvizFile. Let id3 be an instance of ID3, then we could just do something like id3->save(CGraphizFile*). Have a look at the io examples and how it is done for example with the features for details. \n. Hi @mazumdarparijat, apologise for the late answer, but I had to look more carefully into the io before answering your questions.\nAs a reference, I have been using this #2019. I see that in your code, export_to_graphviz_format is basically creating a DOT file. Similar to the UAIFile class (see the referenced PR), we need a DOTFile class. This class should contain the necessary attributes to write a file in the DOT format. Finally, a save method in the ID3 class should fill in the attributes of a DOTFile instance. The whole point is that the io only happens inside DOTFile.\nBefore continuing with the code, let me know what you think about this approach. I will have a look at other io classes and get back to you when I get a better picture of how should we proceed.\n. Closing this pull request after yesterday's conversation in IRC. This should be done following the io design.\n. @lambday, @karlnapf, @yorkerlin, what should be done about this one?\n. Cool, @yorkerlin. However, before adding new stuff, it is more relevant to take care of the existing features. At least, that is my opinion ;-) \n. @karlnapf, @yorkerlin, ping :-)\n. Hate being annoying, but for the next time amend the last commit instead of creating a new one. It is useful for some modifications that take place during the review of a PR.\n. Hey @mazumdarparijat, thanks! This is a long one so I will need to sit down and spend some time to review it. Tonight probably, from around 20h UTC I will be with it.\n. Need to get some rest now. Will continue tomorrow morning with the actual implementation file.\n. @mazumdarparijat, in the future, when do you commits like the last one to address reviews, please use git commit --amend instead. Note that you will have to force the push afterwards, but that's all right.\n. All right, once the minor comments left are addressed, we are ready to go. Good job!\n. I agree with @karlnapf's comments.. Let's get this by parts since it needs some work. Start by sending a header for KDTree and an implementation file with code stubs (no need to integrate it with current KNN at the moment, see the comment above in relation to #2089).\n. What I meant is that you could first work on the KDTree implementation, unit test it, and then go to its integration in the current KNN class in Shogun.\n. There are still a few old comments to address. Could you also please check why the SGObect unit test is failing? Check first if it does not fail locally on your machine. If it does it must because of something related to your changes, if it doesn't then we can restart the travis build.\n. Please see to the new comments (and also the old, unattended ones). I didn't go through everything yet but please address the current issues first.\nRun also locally the unit tests and the classifier_knn from the libshogun interface since there are errors detected by travis that I bet must be also appearing in your machine.\n. Closing for the moment as it has been not active for some time. @armanform, if you decide to work on this again, I will re-open the pull request, no worries!\n. Looking good!\n. Looking good!\n. Thank you for addressing the comments so promptly, @mazumdarparijat. I will merge now, but please have a look at the comment above regarding the return value of the prune method. If you agree, fell free to submit a minimal PR just with that change.\n. Thank you for addressing the comments so promptly, @mazumdarparijat. I will merge now, but please have a look at the comment above regarding the return value of the prune method. If you agree, fell free to submit a minimal PR just with that change.\n. I don't really mind the contributions page in the website. It is true that this info could also be in a markdown file in the github repo and put that content in the web.\n. I don't really mind the contributions page in the website. It is true that this info could also be in a markdown file in the github repo and put that content in the web.\n. Good that you remembered about the news!\n. Good that you remembered about the news!\n. Go ahead with the implementation.\n. Go ahead with the implementation.\n. I'll go on with it tomorrow, now ZzzZzz\n. It seems that this has broken the unit test of Bagging machine. Can you fix that, @mazumdarparijat?\n. Right about obtain_from_generic in labels. We have that method for features, for labels we have the class LabelsFactory.\n. I have updated now the sheet above with decision trees in Shogun. Didn't\nsee random forests in that list. Any idea how the information in the sheet\nis rendered in the website?\nAlso, http://www.shogun-toolbox.org/page/features/ seems to be sort of\nbroken now.\nOn 26 September 2014 13:26, Heiko Strathmann notifications@github.com\nwrote:\n\nMay I suggest to do a proper comparison with some other implementation,\nsay scikit-learn?\nBoth in terms of speed and accuracy. @tklein23\nhttps://github.com/tklein23 and me recently had a discussion and think\nwe should do this for any std method.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/shogun-toolbox/shogun/pull/2282#issuecomment-56949595\n.\n. They are more suggestions of changes than mistakes, really ;-) Thank you for fixing so promptly. I will wait for Travis and merge afterwards.\n\nWhat are you up next?\n. integration-python_modular-tester-classifier_averaged_perceptron_modular is failing on travis in this PR, but I cannot imagine how can it be related to the changes here. I am merging now so.\n. @mazumdarparijat, can you try to investigate why the integration test classifier_averaged_perceptron_modular is failing on Travis? If you need some suggestions on how to do this, please let me know.\n. It is not the first time it fails so it is probably not an isolated (or random) case.\nHave you locally rebased against latest develop and updated the submodules?\n. All right, it is still failing on Travis after the update. What about locally?\n. No idea what's wrong with Travis, then.\n. Updates in the notebook coming up next?\n. @kislayabhi, could you please paste somewhere some code to reproduce this error and link to it from here? Thanks!\n. Looking good in general, nice job!\n. Looking good in general, nice job!\n. Thanks for addressing the comments and no worries!\n. Thanks for addressing the comments and no worries!\n. Nice job with the notebook, I like it!\n. Nice job with the notebook, I like it!\n. It is looking good to me!\n. It is looking good to me!\n. Sure, sorry for the delay.\n. Good jog, @mazumdarparijat! Just a few tiny issues to address. Also, even if these losses are somewhat trivial, could you please add a few unit tests for them? They will be helpful to detect potential bugs due to changes that may come in the future.\n. Remember to let me know with a comment here once updated :)\n. Thanks for the update @mazumdarparijat. However, one of the tests is failing on Travis:\n[ RUN      ] LossFunction.exponential_loss_test\n/home/travis/build/shogun-toolbox/shogun/tests/unit/loss/LossFunction_unittest.cc:129: Failure\nThe difference between secondd[3] and 12639231.822633834 is 2.2351741790771484e-08, which exceeds epsilon, where\nsecondd[3] evaluates to 12639231.822633812,\n12639231.822633834 evaluates to 12639231.822633835, and\nepsilon evaluates to 1e-08.\n[  FAILED  ] LossFunction.exponential_loss_test (0 ms)\nIt might be that the rng in Travis generates different numbers than your computer. See the other comment above.\n. Still failing @mazumdarparijat ;-)\n. Did removing the SG_ADD make the error disappear?\n. I say we rather keep it. Revert the last commit and push again. I will take a look at the error and merge.\n. So the SG_ADD is definitely breaking the serialization unit tests. I have no idea why it happens though. I can try this evening to reproduce it locally but unfortunately we cannot merge this until the problem is solved.\n. Finally, so that must have been it. The first gcc is passing now. Thanks @karlnapf for pointing out the error!\n. Finally, so that must have been it. The first gcc is passing now. Thanks @karlnapf for pointing out the error!\n. It took some effort to get in this one :-)\n. It took some effort to get in this one :-)\n. Very nice job with the notebook!\n. Gradient boosting's MSE of 6.5 sounds pretty good compared to 21-25 of CHAID. Nice!\n. Yeah, please do. If CLDA doesn't have unit tests already, add them.\nOn 21 Jun 2014 20:03, \"Abhijeet Kislay\" notifications@github.com wrote:\n\nyeah!!!. Ok. This might be the problem from the start then!! I will see\nit.\nAnd should I add some unit-tests too.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/shogun-toolbox/shogun/pull/2325#issuecomment-46760435\n.\n. The unit test should cover both LDA methods. I believe that for the moment only FLD_LDA is tested.\n. restarting... the libshogun builds are failing because of the structure multilable example. The fail in modular interfaces was not expected though.\n. Looking good! @mazumdarparijat, see the few comments above. Nice job!\n. Thanks!\n. Thanks!\n. Done, @hushell!\n. Done, @hushell!\n. Ready to go.\n. Ready to go.\n. I have not checked thoroughly the code, but in principle I am doubtful about what new is implemented here that is not already covered by either LDA or MCLDA. There must be something I am missing!\n. Sure, I understand your point. The reason why MCLDA is \"obscure\" is\nprobably to make it scale well.\nOn 25 Jun 2014 18:22, \"Abhijeet Kislay\" notifications@github.com wrote:\n@iglesias https://github.com/iglesias , I think it was too obscure for\na simple Fisher's LDA as the algo suggested by this algorithm is too simple\nthan what's done in MCLDA class. However I am open to any suggestions. It's\njust that I worked on this a little bit and thought a simple dimensional\nreduction application of LDA would be good.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/shogun-toolbox/shogun/pull/2339#issuecomment-47124891\n.\n. Does valgrind complain of anything locally?\nOn 1 Jul 2014 06:55, \"Abhijeet Kislay\" notifications@github.com wrote:\nWhy the unit test gives me a Seg.fault in travis?. This one passes\nsmoothly locally.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/shogun-toolbox/shogun/pull/2339#issuecomment-47616922\n.\n. Try to fix them then. In principle, there should be none. I hope they are\nnot inside the testing library.\nOn 1 Jul 2014 07:38, \"Abhijeet Kislay\" notifications@github.com wrote:\nThere are many actually. I was just focusing on leaks.( output:\nhttp://pastebin.com/XJ4aa7Hg)\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/shogun-toolbox/shogun/pull/2339#issuecomment-47618836\n.\n. Probably unrelated to the changes here, but why are pretty much all the integration tests broken in Travis Python 3's build?\n. @mazumdarparijat, random forest and bagging machine unit tests are failing in the second Travis build.\n. Out of curiosity @mazumdarparijat, would you point out for me in the PR the changes that have impacted the most in going from 6 minutes to 3 seconds of training in the data set you mentioned?\n. @mazumdarparijat, have a closer look at the tests, maybe it is something related to the randomness. Perhaps running them several times locally helps.\n. Also, I say that first we get this PR merged once Travis is good. Then\nParijat can work in the notebooks until Monday as planned. After that, you\ndecide if you want to improve CART further or go on with the tasks on their\nschedule. Choose what you find most interesting/fun.\nOn 28 Jun 2014 12:47, \"Parijat Mazumdar\" notifications@github.com wrote:\n@iglesias https://github.com/iglesias btw, Travis is acting strange.\nThe all unittests are passing in 2nd build system now but not in 1st. Why\nthis difference?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/shogun-toolbox/shogun/pull/2347#issuecomment-47424282\n.\n. The* schedule\nOn 28 Jun 2014 13:56, \"Fernando J. Iglesias Garc\u00eda\" \nfernando.iglesiasg@gmail.com wrote:\nAlso, I say that first we get this PR merged once Travis is good. Then\nParijat can work in the notebooks until Monday as planned. After that, you\ndecide if you want to improve CART further or go on with the tasks on their\nschedule. Choose what you find most interesting/fun.\nOn 28 Jun 2014 12:47, \"Parijat Mazumdar\" notifications@github.com wrote:\n\n@iglesias https://github.com/iglesias btw, Travis is acting strange.\nThe all unittests are passing in 2nd build system now but not in 1st. Why\nthis difference?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/shogun-toolbox/shogun/pull/2347#issuecomment-47424282\n.\n. It makes sense. They key is \"each label in labels_vec has to be equal to one of the unique labels stored in ulabels\". However, then you could put a break inside the if, right? Or is it that a label in labels_vec can be equal to more than one of the unique labels and you want to keep the index of the last one found?\n. Please do. And let us know if performance changes somehow\nOn 29 Jun 2014 15:23, \"Parijat Mazumdar\" notifications@github.com wrote:\n\nOh no! Thats a mostake on my part. A big one considsring that we are\nbogged by performance issues. There should be a break! Let me add this.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/shogun-toolbox/shogun/pull/2347#issuecomment-47454457\n.\n. Please do. And let us know if performance changes somehow\nOn 29 Jun 2014 15:23, \"Parijat Mazumdar\" notifications@github.com wrote:\nOh no! Thats a mostake on my part. A big one considsring that we are\nbogged by performance issues. There should be a break! Let me add this.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/shogun-toolbox/shogun/pull/2347#issuecomment-47454457\n.\n. It is merged now @mazumdarparijat. Since you didn't mention anything about performance after the inclusion of the break, I am assuming it didn't change anything.\n. It is merged now @mazumdarparijat. Since you didn't mention anything about performance after the inclusion of the break, I am assuming it didn't change anything.\n. Sounds good!\n. Sounds good!\n. These ideas sound very exciting @hushell! \n. I don't understand all the code that is going on here. But I see there are unit tests, API examples, and the documentation is nice so I see nothing against merging it. Want to make the honours, @tklein23?\n. Nice job, @mazumdarparijat!\n\nConcerning the comparison with LMNN. The accuracy I obtained was 0.9967 (see http://nbviewer.ipython.org/gist/iglesias/6576096). I cannot find in this notebook any accuracy value reaching 0.99 :-)\nIn any case, I understand than in the experiment you do with RF and OOB error, the model is given all the data available for training, right? Note that this is not the same in the experiment I did with LMNN. At each of the runs done by the CrossValidation object, a portion of the features is used for training, while another portion is used for testing.\nBut experiment a bit more if you want, you should be able to beat that accuracy value with RF!\n. Definitely, try if whitening helps. Also, it may be good to reason beforehand whether the feature scaling and the decorrelation achieved by whitening will help with decision trees. You are the expert here, so you tell me ;-)\n. The most voted answer in the Quora thread linked below convinced me. To sum up, it seems that the OOB error gives a measure of the generalisation of the classifier as good as the one achieved by separating the data into training and test sets.\nHowever, I wonder if this OOB estimate varies amongst different runs of training RF + computing OOB error. This happens with the accuracy when doing cross validation with other classifiers and that's why we do many runs and take the average over the results (see set_num_runs in the LMNN notebook if you are not sure what I mean). What do you think, @mazumdarparijat?\nhttp://www.quora.com/Machine-Learning/What-is-the-out-of-bag-error-in-Random-Forests\n. Thanks for your explanations, guys.\n. That's an idea too. But take into account that we will have to run the code using latest scikit code (or potentially other libraries). This will probably require some hacking if we want to do it automatically.\nOtherwise, I would say that a good place to show these comparisons amongst libraries is a blog. We already talked about students having to write about their projects in personal blogs. This is definitely a topic worth an entry.\n. @mazumdarparijat, I see your point. However, we could still include nanoflann and just don't provide direct access to it from the interfaces. Then, there's no problem with headers including STL. In my opinion, this is not at all a limitation since the user would still be able to use the KD-tree through the KNN class.\nLet's be sure than there is in fact no much difference in both implementations. Please benchmark nanoflann and your implementation and show some results. Use at least a couple of data sets (they can be simulated data), one where the complexity lies in the number of data points and the other in the number of features. For accuracy, it would be nice to use a real set, somewhat well-known.\nAlso, the unit test SGObject.clone_equals_KDTree  in Travis fails. \n. I meant to use knn for the comparison. I think that should do to compare\nthe kd-tree implementations.  But your idea is good too! Go ahead with what\nyou like best.\nOn 5 Jul 2014 10:43, \"Parijat Mazumdar\" notifications@github.com wrote:\n\n@iglesias https://github.com/iglesias I'll fix the Travis failure.\nLet me do one thing then. Let me, in my repo, code up KDE dependent on\nnanoflann in one branch and KDE dependent on Shogun implementation of\nKD-Tree on another and then finally compare speed and accuracy. Whichever\nturns out better, we can merge that.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/shogun-toolbox/shogun/pull/2365#issuecomment-48081430\n.\n. Nice results @mazumdarparijat. I must admit the results of the second benchmark surprise me quite a bit, good job!\n\nHave a look at the unit tests. This is pretty much killing Travis in every way.\n. BTW @mazumdarparijat, what is the value of CMAKE_BUILD_TYPE when you are doing benchmarks?\n. Ouch, yeah. I should have remembered telling you this earlier. Benchmark always in release mode. While coding use debug mode since it is much faster for compilation, but when the time of benchmark comes, use always release.\nHave a look if scikit and nanoflann have also these two different build modes (they should) and update the results of the benchmarks, please (both for CART and the KD-tree).\n. Is the mid-point used by nanoflann the mean of the points?\n. @mazumdarparijat, release mode improves performance, significantly.\nPerhaps the compiler can optimise more heavily nanoflann code than your implementation (I don't want to be pessimistic, just considering the possibility).\n. Nice results! Did you check that the accuracy was good too?\nIf your implementation is better than nanoflann's, why do you think we should include the latter one?\n. Unit tests for Mosek would be nice, yeah!\n. So it looks good to me, @hushell!\n. So it looks good to me, @hushell!\n. Merging. By the way @hushell, you have rights to merge, is that correct?\n. Ah, sorry @hushell! I thought you did have it, my bad. No worries, I can continue merging PRs.\n. Hey @mazumdarparijat, this PR is way too long. I will start by reviewing the KNNHeap. Depending on the number of changes that may arise, it can be a better idea to split this PR into at least 2-3 smaller ones.\n. I am merging this for the moment to speed up things (sorry for taking long to review it). You can address the minor comments in your new PR.\n. @vigsterkr, please have a look in case this is very wrong.\n. @jmarrietar, @aveshd, could you try if the commit above by @lisitsyn fixes your problem?\n. Will continue looking tomorrow from kd-tree and below in the \"files changed\" tab. Now...ZzzZzz\n. No problem about the dual distances, as you mentioned, they are simple enough.\nComing back to the inline-ness, I think it is explained here pretty well: http://www.parashift.com/c++-faq/where-to-put-inline-keyword.html. I found really fun to read the NOTE at the bottom :-D\n. Just restarted the build Parijat. Let me know if it works well this time\nand I will merge it.\nOn 16 July 2014 08:37, Parijat Mazumdar notifications@github.com wrote:\n\nTo speed things up a bit, I'm going back to putting the inline code in the\nheader file.\nFor future reference, the travis error :\n../../../src/shogun/libshogun.so.16.1: undefined reference to\n`shogun::CNbodyTree::add_dim_dist(double)'\n../../../src/shogun/libshogun.so.16.1: undefined reference to\n`shogun::CNbodyTree::actual_dists(double)'\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/shogun-toolbox/shogun/pull/2383#issuecomment-49128844\n.\n. Your are right. It seems unrelated to this PR definitely. Merging this one then. Nice one, @mazumdarparijat! Looking forward to seeing the notebook showcasing KDE :-)\n. Great, thanks! How is the notebook going?\n. Good luck with your paper submission!\n. Good luck with your paper submission!\n. Just restarted the jobs that failed.\n. Where was this test failing, @mazumdarparijat?\n. Still not green\n. @mazumdarparijat, ready then.\n. Nice one!\nOn 24 Jul 2014 19:18, \"Parijat Mazumdar\" notifications@github.com wrote:\nwoohoo! solved.. :-) Check logs here\nhttp://buildbot.shogun-toolbox.org/builders/bsd1%20-%20libshogun/builds/2441/steps/test/logs/stdio\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/shogun-toolbox/shogun/pull/2420#issuecomment-50048955\n.\n. Nice one!\nOn 24 Jul 2014 19:18, \"Parijat Mazumdar\" notifications@github.com wrote:\nwoohoo! solved.. :-) Check logs here\nhttp://buildbot.shogun-toolbox.org/builders/bsd1%20-%20libshogun/builds/2441/steps/test/logs/stdio\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/shogun-toolbox/shogun/pull/2420#issuecomment-50048955\n.\n. Will continue checking the nootebook from \"classification using KDE\"\n. Looking nice! Just let me know when you address the minor comments above.\n. I am restarting the Travis job. I want to merge the PR once Travis is completely happy.\n. Just tried it locally. It seems these changes make the fix: f0d15c81c45c4b566acac8190abd9c7f0f0983d7\n. Letter data sets merged now.\n. Could you please rebase current develop? The PR cannot be merged without conflicts right now.\n. Fancy stuff, nice job! It is a pity that the accuracy of the classifier remains so low.\n. Fancy stuff, nice job! It is a pity that the accuracy of the classifier remains so low.\n. Arrrgh.\n\nNo idea why it fails at first sight, I would need to test it locally. I think I should be able to find time for this tomorrow.\nFeel free to have a look before that if you can/want.\n. I second that, @lambday.\n. @vigsterkr, could you quickly point out for me the previous attempt to modularize the swig interface, or somewhere I could start looking at?\n. Sure, done!\n. Hey guys. Are you implementing from scratch MPLP inference or are you using some other implementation as reference? Sorry if this is already cited in the code and I just missed it.\n. As long as there is no problem with the license, I guess it is all right.\n. Copying GPL code and making it BSD is not good, I think.\n. Looking nice to me! Just the very few minor comments above.\n. No problem at all ;-)\n. I am not sure why we need the separation between the classes MixtureModel and MixtureDistribution.\n. I am not sure why we need the separation between the classes MixtureModel and MixtureDistribution.\n. All right, so it seems that you are thinking about CMixtureDistribution as what in my head was simply CDistribution, and your CMixtureModel is what I thought about as CMixtureDistribution.\n- Why do you need to put another layer in the class hierarchy with CMixtureDistribution? In other words, how is CMixtureDistribution specialising the CDistribution class?\n. All right, so it seems that you are thinking about CMixtureDistribution as what in my head was simply CDistribution, and your CMixtureModel is what I thought about as CMixtureDistribution.\n- Why do you need to put another layer in the class hierarchy with CMixtureDistribution? In other words, how is CMixtureDistribution specialising the CDistribution class?\n. I understand. However, I think it doesn't make much sense to make CDiscreteDistribution and CGaussianDistribution inherit from CMixtureDistribution. Basically, a single Gaussian is not defined as a mixture (as far as I have seen at least). Maybe another name would be more suitable?\nIn addition, I am not sure whether adding another layer in the class hierarchy in order to support a single additional method is the best solution. Another possibility would be to make this method virtual in CDistribution and implement it by calling only SG_NOTIMPLEMENTED. Children classes may override it then.\n@karlnapf, any thoughts? (Sorry about the long comments :)\n. I understand. However, I think it doesn't make much sense to make CDiscreteDistribution and CGaussianDistribution inherit from CMixtureDistribution. Basically, a single Gaussian is not defined as a mixture (as far as I have seen at least). Maybe another name would be more suitable?\nIn addition, I am not sure whether adding another layer in the class hierarchy in order to support a single additional method is the best solution. Another possibility would be to make this method virtual in CDistribution and implement it by calling only SG_NOTIMPLEMENTED. Children classes may override it then.\n@karlnapf, any thoughts? (Sorry about the long comments :)\n. Sure. Let's way for Travis.\n. All good with Travis, ready to be merged.\n. The code looks all right to me. We might run into trouble because of the template to expose this class to interfaces though.\n@karlnapf, does the design make sense to you?\n. The code looks all right to me. We might run into trouble because of the template to expose this class to interfaces though.\n@karlnapf, does the design make sense to you?\n. I didn't know that was in your thoughts ;-) That's all right, then.\n. Ready to go (fail on OS X is not caused by this).\n. We let Travis finish and when all is green we merge it. @Jiaolong, ping us if you see Travis has finished and we haven't said anything new.\n. It shouldn't be caused by your changes, restarting the job...\n. Ready to go.\n. Travis jobs often fail. I think that most of the times it is because they take too much time. I am restarting the jobs that have failed for this PR.\n. @vigsterkr, thanks for the correction :-)\n. @Jiaolong, keep in mind to follow the notebooks template. I think this notebook is missing the abstract that will be shown in the webpage.\n. I like this notebook :-)\n. You can see the template here: http://nbviewer.ipython.org/github/shogun-toolbox/shogun/blob/develop/doc/ipython-notebooks/template.ipynb\n. @hushell, is this good and ready to go?\n. If the buildbot says it takes too much time, then we will have to reduce the running time. @hushell, @Jiaolong, could you please have a look once the next nightly build has finished if the notebook could be generated without any timeout?\n. I agree with @pickle27. The notebook is looking nice, but it would be even nicer to do classification (if that makes sense with the data you have) or maybe another task. In other words, try to show off a little bit more of Shogun in the notebook. For the moment, it seems that Shogun is only used for PCA.\nAnyway, nice work, I like it!\n. Any updates here, @kislayabhi?\n. A first easy step would be to address the comments above, @kislayabhi :-)\nAfterwards, what about using a publicly available face recognition data set and test some classifiers using your APM?\n. A first easy step would be to address the comments above, @kislayabhi :-)\nAfterwards, what about using a publicly available face recognition data set and test some classifiers using your APM?\n. I've just restarted the OSX job. The logs for the fail are here.\n. #2496 fixes it. Just run the KNN notebook locally without any trouble.\n. Fixed in #2496.\n. @karlnapf, it does not appear on the website because the notebook fails. There seems to be a hard-coded path to a data file (search for sonar.all-data in http://buildbot.shogun-toolbox.org/builders/nightly_default/builds/920/steps/notebooks/logs/stdio). I tried locally as well, same error.\n@yorkerlin?\n. After a quick search on Google, I understand it is not possible fork a repository and do local changes to the wiki of that repo and finally submit PRs as we normally do. IIRC, @karlnapf told me a couple of days ago this was possible though, so maybe I am missing something.\nIn any case, @kislayabhi, what you can do is just address the issues that are currently existing in this pull request here. I will then merge it, move the files to the wiki, and finally remove them from their current location.\n. I really like the job you did here, @kislayabhi. Really awesome. To increase the visibility, I have added a new etc. section in the wiki that links to your documents.\nI have currently gone through the k-NN one (see comment concerning accuracy above). I will continue going through the other documents asap.\n. Yeah, LMNN took yesterday about 6 minutes in my laptop with Shogun compiled in Debug mode.\n. @vigsterkr, will the LMNN notebook appear again automatically in the list (http://shogun-toolbox.org/page/documentation/notebook) or is there something else to do?\n. As this belongs to the linalg framework, I believe you are the right one to check this one out, @lambday. Could you do that, please?\n. I will merge this one since you guys are good with it :-)\n. Fixed since e27ed42db07eda176fc820e2546ad2aba7be6147.\n. http://buildbot.shogun-toolbox.org/builders/deb1%20-%20libshogun/builds/3480\nOther jobs probably have their own warnings as well.\n. http://buildbot.shogun-toolbox.org/builders/deb1%20-%20libshogun/builds/3480\nOther jobs probably have their own warnings as well.\n. We have now an updated project description.\n. I have now replaced the old information in the website.\nhttp://www.shogun-toolbox.org/page/about/project_description\n. I have now replaced the old information in the website.\nhttp://www.shogun-toolbox.org/page/about/project_description\n. Just realised @mazumdarparijat did not get back in this one but all the points are a yes if I am not mistaken.\n@mazumdarparijat, anything to add?\n. Any news on this, guys?\n@yorkerlin @mazumdarparijat @kislayabhi @khalednasr @Saurabh7 @abinashpanda @Jiaolong @lambday \n. Yeah, guys. The idea is that we would collect bits and pieces of your posts and use them (together with some other content) to write a longer post about GSoC 2014 in Shogun.\nThis post will then be published in the Google open source blog during fall. Your work and your names will be visible to lot of people from there! ;-)\n. http://blogs.impetus.com/other_technologies/technical_writing/GraphlabIntegrationWithApacheMesos.do\nnot sure why using mesos substitutes the Graphlab idea.\n. Merging as changes are not provoking Travis failures.\n. ### Contact\nEmail: fernando.iglesiasg@gmail.com\nirc: iglesiasg\nPersonal\nOccupation: Marie Curie research fellow at Thales Nederland and PhD student at the University of Twente, associated with the faculty of EEMCS.\nResearch interests: statistical signal processing and information fusion. Mainly, the application of Bayesian statistics and Monte Carlo methods to target tracking.\nOther interests: machine learning in education and open source, the amazing evolution of robotics, competitive programming as a tool to improve problem solving skills, fitness, travel, and video games.\nShogun\nFernando got involved in Shogun back at the beginning of 2012 with the goal of becoming a GSoC student, which he did, starting Shogun's framework for structured output learning under the supervision of Nico G\u00f6rnitz and Georg Zeller.\nAt the end of 2012 and during the first half of 2013, Fernando used Shogun for his undergraduate thesis work on structured output learning applied to label sequence learning and learning of general structured output models (aka graphs). The latter part was the precursor of GSoC 2013 project developed by @hushell and mentored by Patrick Pletscher.\nDuring the summer of 2013, Fernando implemented in Shogun the metric learning algorithm known as Large Margin Nearest Neighbours (LMNN) and applied it to a metagenomics data set. This work was performed under the supervision of Georg Zeller.\nAsk him about\n\nThe primal SO-SVM and its implementation in Shogun.\nk-NN and LMNN.\nUsing valgrind to catch memory leaks and profiling Shogun code.\nAnything else :-)\n. @khalednasr, is it possible to use the try-catch block in a more specific way? That is, instead of putting all the code of the examples within the try block, only the parts that might actually throw the exception.\n. Ping @khalednasr :-)\n. Merging as changes are not the cause of Travis errors.\n\nThanks @khalednasr!\nOn 4 Sep 2014 04:10, \"khalednasr\" notifications@github.com wrote:\n\n@vigsterkr https://github.com/vigsterkr made the notebook run faster,\nthis will probably fix #2536\nhttps://github.com/shogun-toolbox/shogun/issues/2536\nYou can merge this Pull Request by running\ngit pull https://github.com/khalednasr/shogun fix_rbm_notebook\nOr view, comment on, or merge it at:\nhttps://github.com/shogun-toolbox/shogun/pull/2537\nCommit Summary\n- fixed #2536\nFile Changes\n- M doc/ipython-notebooks/neuralnets/rbms_dbns.ipynb\n  https://github.com/shogun-toolbox/shogun/pull/2537/files#diff-0 (21)\nPatch Links:\n- https://github.com/shogun-toolbox/shogun/pull/2537.patch\n- https://github.com/shogun-toolbox/shogun/pull/2537.diff\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/shogun-toolbox/shogun/pull/2537.\n. @erip, is it inherited from any of the base classes in the hierarchy?\n. Should be fixed now 23810b2965ee75d0b8dc00293626d40ed43a289f.\n. Should be fixed now 23810b2965ee75d0b8dc00293626d40ed43a289f.\n. I think you are probably right, I don't see much reason of keeping this file in the repository either.\n\nI have just checked in the GitHub commit browser and the oldest commit in the Changelog file appears as well in the browser (https://github.com/shogun-toolbox/shogun/commits?page=386) so we should be able to drop this file without losing any information about past history.\n. I think you are probably right, I don't see much reason of keeping this file in the repository either.\nI have just checked in the GitHub commit browser and the oldest commit in the Changelog file appears as well in the browser (https://github.com/shogun-toolbox/shogun/commits?page=386) so we should be able to drop this file without losing any information about past history.\n. Thanks for the heads-up, @jondo. I have now deleted the file.\n. Definitely. Let's get rid of them. Waiting for your pr.\n. f9a5267f5a8dafa9dd0d1c04e582575de2539568\n. f9a5267f5a8dafa9dd0d1c04e582575de2539568\n. Not only in CombinedDotFeatures, but in pretty much all DotFeatures subclasses as the method is included in the ignores file.\nI am not sure why it is included in the ignores file, though. @lisitsyn, @karlnapf, @vigsterkr, @lambday, any idea?\n. Not only in CombinedDotFeatures, but in pretty much all DotFeatures subclasses as the method is included in the ignores file.\nI am not sure why it is included in the ignores file, though. @lisitsyn, @karlnapf, @vigsterkr, @lambday, any idea?\n. I can compile just fine on Ubuntu 14.04. Can you please post here what steps are you using (by the way, are you following the quickstart?) to get the source code, configure, and compile?\n. What branch of the repository are you compiling? I guess it is either\nmaster or develop. You should go for develop.\nOn 16 Oct 2014 01:05, \"johnny555\" notifications@github.com wrote:\n\nI initally tried to follow:\nhttp://www.shogun-toolbox.org/doc/en/current/installation.html\nbut by cloning the git repo instead. It seems that is out of date as there\nare is no ./configure file. When I noticed that there were cmake files I\ncreated a seperate build directory to run CMake in. I also chose to bundle\nARPREC, COLPACK, EIGEN, JSON, NLOPT.\nI configured CMake to build the python-modular library, I also had to\napt-get install SWIG\nFollowing that I configured, generated Cmake and then make -j4\nI didn't notice the QUICKSTART, but other than change the install prefix I\ndont think I did anything differently.\nhttps://github.com/shogun-toolbox/shogun/wiki/QUICKSTART\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/shogun-toolbox/shogun/issues/2550#issuecomment-59290904\n.\n. Sure, it is ready now.\n. @tklein23, I just added you to the developer profiles list. Feel free to create an individual profile page telling something about you if you like the idea.\n. @c3h3, I am very interested in getting to know more about the application where you used/plan to use Shogun's LMNN.\n. Thanks a lot for reporting!\n\nOn 26 October 2014 17:01, liquid notifications@github.com wrote:\n\nI pull the shogun-ipynb docker image, and try to import modshogun, but it\ndisplay the ipython kernel died\n[image: screenshot from 2014-10-25 18 54 41]\nhttps://cloud.githubusercontent.com/assets/3073346/4783480/f60fc056-5d27-11e4-8ced-5b845d4f5bb0.png\nso i remove the image, then git clone the Dockerfile from @c3h3\nhttps://github.com/c3h3 github project\nhttps://github.com/c3h3/docker-oblas-py278-data\nand re-build the c3h3/oblas-py278-data and c3h3/oblas-py278-shogun (from\ngit branch)\nnow it could import modshogun well :)\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/shogun-toolbox/shogun/issues/2553#issuecomment-60521879\n.\n. Very nice stuff, @c3h3. It is great that you mentioned Shogun and encouraged its use during the event.\n. @srgnuclear, a script that goes through the header files and reports everything included that is not <shogun/...> will definitely help.\n\nHowever, I think this is not so straightforward. We have some header files, like our entry point for eigen3 in mathematics which includes Eigen3 headers.\nMaybe we could start by checking that SWIG is not going through non-Shogun files. I am curious about whether this would make any impact in compilation time/memory. I think it won't.\n. @srgnuclear, a script that goes through the header files and reports everything included that is not <shogun/...> will definitely help.\nHowever, I think this is not so straightforward. We have some header files, like our entry point for eigen3 in mathematics which includes Eigen3 headers.\nMaybe we could start by checking that SWIG is not going through non-Shogun files. I am curious about whether this would make any impact in compilation time/memory. I think it won't.\n. On 31 October 2014 14:29, Saurabh Goyal notifications@github.com wrote:\n\n@iglesias https://github.com/iglesias, k i will look up the script. In\nthe meantime i manually went through header files in src and found some\ninclude apart from like < vector > ,< stdio.h > ,< glpk.h > ,< cmath > ,<\ncassert > ,< math.h > ,< limits.h > ,< ctype.h > ,< string.h > ,< stdlib.h\n\n,< time.h > etc do these also have to be removed because these are some\nbasic includes used frequently.\n\nI would say the following:\n\nIf possible, try to move these includes from header files to their\ncorresponding implementation (cpp) files. This will be possible to do when\n(i) they are not necessary because no method or data structure defined in\nthose included files is used in the header; (ii) the piece of code that\nrequires any of those includes can me moved to the cpp file. The latter\nmight be a bit trickier to solve when dealing with template classes, so I\nwould focus on non-templates firstly.\n\nAlso if this is not having considerable impact should i continue with it?\nWell, the thing is that we (at least I) don't even know what the impact\nthese changes might have on the resources (time and mem) used by make\nduring compilation. I understand that the main motivation of doing this is\nswig. Thus, what you can do to quickly get a feel for it is to guard all\nthese includes by #ifndef SWIG // SWIG should skip this ... #endif and\ncompare compilation resources before and after these guards are introduced.\n\nDoes that make sense to you?\n\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/shogun-toolbox/shogun/issues/2557#issuecomment-61259693\n.\n. On 31 October 2014 14:29, Saurabh Goyal notifications@github.com wrote:\n@iglesias https://github.com/iglesias, k i will look up the script. In\nthe meantime i manually went through header files in src and found some\ninclude apart from like < vector > ,< stdio.h > ,< glpk.h > ,< cmath > ,<\ncassert > ,< math.h > ,< limits.h > ,< ctype.h > ,< string.h > ,< stdlib.h\n\n,< time.h > etc do these also have to be removed because these are some\nbasic includes used frequently.\n\nI would say the following:\n\nIf possible, try to move these includes from header files to their\ncorresponding implementation (cpp) files. This will be possible to do when\n(i) they are not necessary because no method or data structure defined in\nthose included files is used in the header; (ii) the piece of code that\nrequires any of those includes can me moved to the cpp file. The latter\nmight be a bit trickier to solve when dealing with template classes, so I\nwould focus on non-templates firstly.\n\nAlso if this is not having considerable impact should i continue with it?\nWell, the thing is that we (at least I) don't even know what the impact\nthese changes might have on the resources (time and mem) used by make\nduring compilation. I understand that the main motivation of doing this is\nswig. Thus, what you can do to quickly get a feel for it is to guard all\nthese includes by #ifndef SWIG // SWIG should skip this ... #endif and\ncompare compilation resources before and after these guards are introduced.\n\nDoes that make sense to you?\n\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/shogun-toolbox/shogun/issues/2557#issuecomment-61259693\n.\n. Sure, @srgnuclear. If you remove includes from headers and those headers are relying on the includes to make use of classes or methods defined in those headers, then you are going to get compile errors.\n\nWhat you should check is if the headers really need to have these includes. It can be, for instance, that the dependency comes from the implementation of a method in a header. Then, you can just move the implementation to the cpp file (this does not apply, at least in a straightforward way, for template classes). Another case is that a header is just making use of a class in a method signature (as the return value or argument). Then forward declaration in the header should be enough.\nThis is not an exhaustive explanation covering every case but I hope that it at least you gives a better picture of what's going on.\n. Mini-batch k-means is actually mentioned in the documentation briefly. Anyway, I agree this class should be designed in a better way.\n@mallikarjun26, the description above by @karlnapf gives a pretty good idea of what needs to be done. Basically, the implementations of the two different k-means variations available in Shogun are all right (each one has its own implementation file, KMeansLloydImpl and KMeansMiniBatchImpl). However, the interface is maybe too cohesive (CKMeans is responsible for both Lloyd and mini-batch training). It would be better to have two different classes.\nA way to go might be to use inheritance. For instance, a class CKMeans could implement the standard algorithm while a subclass CKMeansMiniBatch would override the training method. There may better designs though. This was just an example. It is up to you to come up with a good solution and implement it :-)\n. I agree with @karlnapf because I do not quite see the advantage of using the factory method pattern for our use case. With the pattern we would do stuff like:\nCKMeans* mbkmeans =  CKMeans::make_kmeans(KMeansType::MB, ...);\nCKMeans* lloydkmeans = CKMeans::make_kmeans(KMeansType::Lloyd, ...);\nwhereas using just inheritance,\nCKMeans* mbkmeans = new CMBKMeans(...);\nCKMeans* lloydkmeans = new CLloydKMeans(...);\nAbove, CKMeans stands for a base class that contains common functionality and CMBKMeans and CLloydKMeans stand, respectively, for children classes with the particularities of mini-batch and Llody k-means.\nAgain, I am not sure what would be the advantages of using this design pattern here. Do you see any, @abhinavagarwalla?\n. I agree with @karlnapf because I do not quite see the advantage of using the factory method pattern for our use case. With the pattern we would do stuff like:\nCKMeans* mbkmeans =  CKMeans::make_kmeans(KMeansType::MB, ...);\nCKMeans* lloydkmeans = CKMeans::make_kmeans(KMeansType::Lloyd, ...);\nwhereas using just inheritance,\nCKMeans* mbkmeans = new CMBKMeans(...);\nCKMeans* lloydkmeans = new CLloydKMeans(...);\nAbove, CKMeans stands for a base class that contains common functionality and CMBKMeans and CLloydKMeans stand, respectively, for children classes with the particularities of mini-batch and Llody k-means.\nAgain, I am not sure what would be the advantages of using this design pattern here. Do you see any, @abhinavagarwalla?\n. @lisitsyn, why is it good not to use member variables w/o getters (and setters too, I guess) within the class where they are members?\n. I am not sure what flexible is here. Can you put an example, @lisitsyn? :-)\n. It sounds pretty awesome!\n. It sounds pretty awesome!\n. Thanks a lot for the nice description, @lisitsyn :-)\n. Thanks a lot for the nice description, @lisitsyn :-)\n. #2596\n. Travis builds are failing at random here. Merging, it should be all right.\n. Boring, painful stuff to do but necessary. Thanks a lot, @lambday!\n. @lambday, not sure if it can be useful, but I did a feature selection experiment using LMNN. It is in the notebook.\n. A little bit more of cleanup, nice stuff.\n. I forgot to mention that it is important to do this separately, both to ease the review and the cleanup process. That means that the best practice would be to send small and individual pull requests for each method.\n. See the wiki for more motivation why we want to do this.\n. According to what I wrote above, moving dot to CMath sounds good :-)\nI think that doing that one alone in a first pull request is good.\n. Sure. In my opinion, max, min, argmax, argmin, and linspace make sense in CMath. Mean might make more sense in CStatistics. About sorting I am not sure, I leave it up to you :-) But there might be already some sorting implemented in CMath (iirc).\n. We do a small trick. Look at the end of SGVector.cpp. There, we have defined the classes for which SGVector can be templated (which are basically primitive data types).\n. If they are not used anywhere, I'd say we can drop them. We can for sure drop the ones in SGVector. The CMath one can stay.\n. In my opinion, void CMath::qsort(SGVector<T>), SGVector<index_t> CMath::argsort(SGVector), and bool CMath::is_sorted(SGVector<T>) make sense.\n. In my opinion, void CMath::qsort(SGVector<T>), SGVector<index_t> CMath::argsort(SGVector), and bool CMath::is_sorted(SGVector<T>) make sense.\n. @curiousguy13, it sounds good to me, both range_fill and range_fill_vector should go out of SGVector. Do something nice with them (perhaps, simplifying to only one method) and move to CMath. Keep in mind unit tests. Looking forward to see the pull request.\n. It sounds good!\nOn 18 Feb 2015 10:26, \"Soumyajit De\" notifications@github.com wrote:\n\n@iglesias https://github.com/iglesias I think range_fill, zeros, these\nare perfect to be shifted to linalg instead.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/shogun-toolbox/shogun/issues/2582#issuecomment-74835157\n.\n. It sounds good!\nOn 18 Feb 2015 10:26, \"Soumyajit De\" notifications@github.com wrote:\n@iglesias https://github.com/iglesias I think range_fill, zeros, these\nare perfect to be shifted to linalg instead.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/shogun-toolbox/shogun/issues/2582#issuecomment-74835157\n.\n. This is when we get to the point of unit testing other languages in addition to C++. Do we need another testing framework for that?\n. I like better the first option. The typemaps are written in C++ (right?). Thus, testing the typemaps through C++ unit tests looks like a reasonable solution.\n. @tklein23, looking good to you?\n. Hi Abhinav, \n\nIt sounds a bit strange that there is nothing in your build directory. What instructions are you using exactly?\n. @abhinavagarwalla, the README_cmake is showing how you can configure some parameters before building Shogun (this listing is not exhaustive, you may want to use ccmake and see others). This does not mean you have to use several cmake instructions sequentially.\nIn general, create a build directory, cd into it, and issue cmake with the desired arguments to configure a build (with e.g. -Doption[=value]) and giving to cmake the path to the root CMakeLists.txt file. Then, the Makefile, along with other files, will appear in your current directory and everything will be ready to run make.\n. @abhinavagarwalla, the README_cmake is showing how you can configure some parameters before building Shogun (this listing is not exhaustive, you may want to use ccmake and see others). This does not mean you have to use several cmake instructions sequentially.\nIn general, create a build directory, cd into it, and issue cmake with the desired arguments to configure a build (with e.g. -Doption[=value]) and giving to cmake the path to the root CMakeLists.txt file. Then, the Makefile, along with other files, will appear in your current directory and everything will be ready to run make.\n. I have seen it actually twice already in #2581. I don't see how the changes there can be related to this failure though. @tklein23.\n. I have seen it actually twice already in #2581. I don't see how the changes there can be related to this failure though. @tklein23.\n. That is a point. However, I think that for rather common operations (like the sine) which are included in the C++ standard, we can just go along using the C++ ones. Is there any reason why we would like to use the sine implemented somewhere else instead of the standard one?\n. That is a point. However, I think that for rather common operations (like the sine) which are included in the C++ standard, we can just go along using the C++ ones. Is there any reason why we would like to use the sine implemented somewhere else instead of the standard one?\n. This small bug sneaked in #2581. Travis is probably not configuring the build w/ TRACE_MEMORY_ALLOCS=ON so it didn't fire any error. The buildbot got it though: http://buildbot.shogun-toolbox.org/builders/nightly_all/builds/800/steps/compile/logs/stdio\n. Hi @abhinavagarwalla, thanks a lot for your work! However, four commits for this is too much. One commit suffices for this work. Would you mind trying to squash them into a single commit? See https://ariejan.net/2011/07/05/git-squash-your-latests-commits-into-one/.\nAlso, I see that one of your commits is the result of merging upstream/develop in your local branch. It is preferable to rebase your commits (i.e. put your changes on top of -- after -- the current HEAD of upstream/develop). To do this, instead of doing git pull (which effectively does fetch + merge), simply do a fetch of the upstream repository followed by a rebase of the develop branch of upstream.\n. Great, @abhinavagarwalla. Thank you. I am merging since Travis failures are not due to the changes here.\n. Merging since the changes are not causing failures in Travis (g++ killed due to memory used, probably).\n. I agree with this idea. Great suggestion, Heiko!\nLet's find out whether we can modify the documentation for these tutorials from the wiki and have everything nicely synced.\n. @karlnapf @vigsterkr @lisitsyn @sonney2k @lambday, do you see any con? Anything against this?\n. Is the log det stuff already merged in develop? Test and examples were fine on develop after these changes. It is also good in Travis. Is there any dependence for the log det stuff that I and Travis might be missing and log det is not being tested?\n. @karlnapf @lambday\n. Is this just a test on updating wiki content from a fork? I didn't find content in the wiki of your Shogun fork.\n. @sanuj, see the entrance tasks, or one of the many recent suggestions in the mailing list. It would be great if you could help us out getting into shape for release. There's a recent thread in the mailing on what's need to be done before release. Anyway, this is not the right place to discuss this.\n. @sanuj, see the entrance tasks, or one of the many recent suggestions in the mailing list. It would be great if you could help us out getting into shape for release. There's a recent thread in the mailing on what's need to be done before release. Anyway, this is not the right place to discuss this.\n. It cannot be done for the moment because public attributes are not passed through typemaps.\n. An alternative may be to create a structure in the target interface from the SGVector, although I doubt whether it is possible to do it without copying the data.\nI think the right solution is not having public attributes, all of them should be accessed only through getters when it makes sense to make them part of the interface. However, I think it is quite rudimentary to do these getters by hand. An automatic way would be nice.\n. Thanks! And welcome to the world where continuous integration tools fire even if new changes only fix doc typos :-|\n. I will check it out later today at night.\nOn 17 November 2014 12:22, Heiko Strathmann notifications@github.com\nwrote:\n\nFine to merge from my side, but somebody else should also look\n@iglesias https://github.com/iglesias ?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/shogun-toolbox/shogun/pull/2616#issuecomment-63291218\n.\n. Thanks for your work, @abhinavagarwalla. This is getting to a good point. See the comments and get back to us if you have any doubt. Looking forward to seeing the updates.\n. I am coming back to this PR.\n. For get_vector, set_vector, get_matrix, set_matrix, get_string_list, and set_string_list keep the separate implementation for the bool data type and use the macro for the rest of data types.\n. Sorry about the long delay, @abhinavagarwalla. This is almost ready to go, just a very trivial update from your part and we are set to go. Thanks for your efforts!\n. @abhinavagarwalla, did you manage to compile this patch locally? See https://travis-ci.org/shogun-toolbox/shogun/jobs/52586025#L1377.\n. If compilation does not work without an optional library installed, then there is a bug. Probably, a class that depends on one of those libraries and lacks a proper #ifdef HAVE_LIBRARY guard.\n. @yalcinm, I am sorry that you are struggling to compile Shogun. However, I must add that it is not necessary whatsoever to install packages \"for hours\" to get Shogun ready. I am afraid you must be following a wrong approach.\n. @yalcinm, I am going to be available at the irc channel for some time now. Feel free to pass by if you want to get some more interactive help.\n. What is that? I have no idea what that error is. I have to run now, so I cannot look at it carefully.\n\n@yalcinm, however, I can suggest you the following quick workaround. That errors happens in EPInferenceMethod, which depends on eigen3, and you want to use Shogun for MKL, which afaik does not depend on eigen3. If you uninstall eigen3 from your machine (apt-get remove libeigen3-dev), and configure Shogun again from zero (i.e. remove build dir, create a new one, and issue cmake), then Shogun will be configure without eigen3, thus all the classes that depend on it (like the one where you get the error) won't be compiled.\nThis is just a random quick idea that nonetheless might work for your use case.\n. It is probably all right that Ubuntu \"freezes\" when compiling modshogunPYTHON_wrap.cxx. That file is extremely long. How long have you waited for it to finish?\n. For some reason, fdopen is not found, which is weird. You had also similar errors with strdup and something else, didn't you? There seems to be something funky going on in your system.\n. @yalcinm, please also keep in mind what Viktor has said above and use pastebin or gist if you want to share a long error, compilation, or whatever log. Do not use screenshots; it is a complete overkill to do it. If the log is short -- few lines -- copy paste directly here should be fine. \n. I am not sure at all whether this might help since I am not working nor have available a machine with os x. Anyhow, what version of swig are you using? We know there are atm some problems with swig 3.\n. Ah all right. Then that can be the issue. As far as I know, there is currently a problem with swig 3 and it is not yet supported. Swig 2.0.4 should be fine. Can you install that version on your os x and try?\nWhere did you see that Shogun 3.2. needs swig 3?\n. See answer in #2618 and please avoid duplicating the same question.\n. @svle, it is all right, no need to add more comments or descriptions. A quick search in stack overflow says that the error you are facing is resolved removing the object file.\nJust to be sure, could you please remove completely your Shogun build directory (i.e., rm -rf build), create a new one and run again cmake and make?\n. I am missing passing the Shogun library to the linker. Something like -lshogun.\n. I really doubt that the drastic solution consisting of removing the notebook is the right thing to do. I merge for now anyway since this was discussed already and agreed upon in the issue referenced above.\n@Ialong, can you please put a link to the notebook with the output (use http://nbviewer.ipython.org/) and another one with the ipynb file content (in gist) in a comment here so it is as easy as possible to keep track where it is?\n. I really doubt that the drastic solution consisting of removing the notebook is the right thing to do. I merge for now anyway since this was discussed already and agreed upon in the issue referenced above.\n@Ialong, can you please put a link to the notebook with the output (use http://nbviewer.ipython.org/) and another one with the ipynb file content (in gist) in a comment here so it is as easy as possible to keep track where it is?\n. Thanks, @Ialong!\n. Thanks, @Ialong!\n. I restarted the failing jobs; in one of them the compilation process had to be killed (probably it used up all avail. memory), in the other the test suite timed out.\n. I restarted the failing jobs; in one of them the compilation process had to be killed (probably it used up all avail. memory), in the other the test suite timed out.\n. So what's the state in this PR, @sanuj? Is it to be closed already or updated?\n. I agree with you, @sanuj. I rather have it in CMath than in SGVector. Even if this is not the final solution, it is still better compared to what we currently have. I would say let's either merge or close this PR soon and keep this moving. I don't like to see PRs open for ages.\n. Yep. That's the point I was trying to make. Ping me once it is rebased :-)\n. Thanks!\n. This seems to be doing pretty much the same work (maybe even in the same files?) that the pull request I just commented. Let's keep it centralise.\n. The ) was indeed missing. The comma was right though. Can you put it back?\n. Great, thanks for the fix.\n. This pull request needs a correct rebase and a new push to get rid of unwanted commits. Also, I suggest you to submit less changes (at least for the first time) since some of them do not seem to be clear (at least to me, see comments above).\n. I am closing this one for the time being, but please let me know if you prefer to update it and I will re-open it. Otherwise, you can issue a new one, as you prefer.\nLooking forward to seeing updates!\n. All right, merging even though Travis complains because errors are not related to this patch.\n@sanuj, if you want, you can try if type inference of the template works well for the changes here. That is, instead of calling the methods like CMath<T>::min(vec, vlen), I think it should be possible to only do CMath::min(vec, vlen) since T can be inferred from T* vec. Anyway, just for fun if you are curious about it. Thanks!\n. Sure.\n. Yes, @sanuj. In the second case (the sine wave example you mentioned), it should be something as easy as possible so that the same results given by the unit test can be obtained using pen and paper.\n. Hey, @sanuj. I just had a look at the unit test and you're definitely right; labels_test is not used. If I am not missing anything, the lines filling in the lab_test vector and creating the features from it can be removed. This test was probably created from another example where the test labels were used.\n@karlnapf will have to tell you about the easy script, I don't know about that one. As I understand it, it has nothing to do with easysvm but the results used in the test are the output of LibSVM.\n. @karlnapf, it seems we need to generate the data file for the integration test since Travis complains about this example failing.\n. Could you make only commit instead of the two here (check out git squash) and write a more clear commit message (such as replace math.h includes for Shogun math or so).\n. The commit message could be more descriptive, I am being rather picky here :-) But since you are getting some practice the development process, I think it can be worth to do the things properly.\nYou can rewrite your commit message by doing git commit --amend. Then just push again to your branch (very likely, you will have to force that push).\n. Merging as the failures in Travis jobs are not due to changes here.\n. Yes, if they are not used anywhere, let us remove max_abs and arg_max_abs.\n. By the way @sanuj, I just saw methods like binary_search_helper, qsort_index, and qsort_backward_index that, at first sight, I don't quite see the reason why they are part of the interface of CMath. It sounds like they should be only private methods. It may be something for you to look into afterwards if you like the idea ;-) There are very likely similar cases with other methods as well.\n. Remove them. Regarding doing it in this pull request or a new one, choose what you prefer :)\n. Never mind the errors detected by Travis so far, they are not caused by your changes.\n. Thanks, @sanuj! Also, if you feel like it, write properly formatted and complete Doxygen doc for arg_min and arg_max.\n. For some reason, GitHub does not allow us to merge this pull request automatically.\n. I will check this asap. Currently on the phone and the changes in the pr\nlook weird for some reason.\nOn 21 Dec 2014 05:06, \"Sanuj Sharma\" notifications@github.com wrote:\n\nI noticed I made some typos in this patch. I'll fix them in the next\ncommit. Shall I also remove floor_log and get_abs_tolorance?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/shogun-toolbox/shogun/pull/2645#issuecomment-67759448\n.\n. Thanks for this patch, @sanuj. It is not so fun stuff, so it's really appreciated :-)\n\nRegarding floor_log and get_abs_tolorance, I have no strong opinion whether to remove them. They are in CMath, which is not a template class, so it is not like having them in SGVector where SWIG would need to generate a different method for every data type.\nSo I leave it up to you if we want to remove them (because they are unused) or keep them. If we keep them, what would be nice is to rename tolorance to tolerance ;-)\n. Great, thank you!\n. I am merging for the time being as the code is fine and suggested changes are mainly style. If would be great however if you can submit a new PR with the suggested changes, @Saurabh7.\n. Would it be possible to get a unit test for linspace,  if there's none.\n. Let's just make a short one then that expects an SGVector as return value\nfor the new method. You may reuse the data of the other unit tests for this\none.\nOn 21 Dec 2014 13:01, \"Sanuj Sharma\" notifications@github.com wrote:\n\n@iglesias https://github.com/iglesias There is unit test written for\nlinspace in Math_unittest.cc. There are 3 types of linspace present in\nCMath, one has a unittest and the other two are dependent on the one\nwhich has a unit test.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/shogun-toolbox/shogun/pull/2649#issuecomment-67768213\n.\n. Thank you, @sanuj.\n. Thank you, @sanuj.\n. Merging as HDF5-related failures in Travis are unrelated and have already been solved in #2658. @sanuj, remember to rebase upstream/develop before updating or issuing a new PR ;-)\n\nThank you for continuing with SGVector's cleanup :-)\n. Oh, I didn't expect this typo fix would be this much work. Thank you!\n. I like to separate stuff into several files, so I would create a new file or a wiki page for this. In the README_developer could be a good place for it as well. It is just a matter of taste :-)\nIn any case, we should definitely link it from www.github.com/shogun-toolbox/shogun/wiki.\n. This comment does not help to solve your problem; I am going to have a closer look right away and let you know if I can add something which may be helpful. Anyway, why do you want to have both Octave modular and static interfaces enabled? If you can afford modular (i.e. having Swig) just use it and ignore static.\n. In my case (Octave 3.8.2), the second change is in the file comment-list.h, not in oct.h.\n. After making the changes suggested above, you may still run into the trouble\n$octave_root_dir/include/octave-3.8.2/octave/oct-hdf5.h:27:18: fatal error: hdf5.h: No such file or directory\n #include <hdf5.h>\nbecause it seems that octave sets HAVE_HDF5 if it finds hdf5 in your system, no matter the dev package is installed or not. Of course, you can solve this by installing the dev hdf5 package.\nIn any case, this is an Octave issue rather than a Shogun issue.\n. I think that's actually a very good point, @sanuj.\n@lambday, is it possible to refactor linalg so that C++11 is an optional dependency so we can have parts of it (e.g. dot product) that do not depend on it?\n. Yeah, I agree, @lambday. For me it would be fine to make C++11 a compulsory dependency. But I guess there might be some cons with that? @karlnapf @vigsterkr @lisitsyn @sonney2k \n. And also, it is just a matter of saying, Shogun v. x.x.x onwards requires a gcc v. > y.y.y., otherwise use an older version of Shogun. It is not like Shogun would not be usable at all in old systems.\n. It must be the integer variable called end in the binary_search routine.\n. As you prefer, this warning is not extremely important. In case of renaming, I would prefer another name different from c_end that has more sense, c_end with the comment is fine too.\n. Which ones are the unused variables?\n. I also think that warnings for unused variables in the examples like the one in kernel_combined_modular.cs are fine. This showcases some of the things that can be done from modular interfaces.\n. Can you fix the first link in your last comment above?\n. Regarding the warnings you mention above in lua modular, I think that sqrt and pow do not need to be exposed via swig, very likely all the languages we provide bindings for contain some math library with their own implementation of these functions so I'd say we do not need to provide them. Probably the same applies for random and normal_random, although these last ones might be arguable.\n. Merging as the first job failing in Travis (time out) is not caused by the changes here and the second job executes the unit tests without failures. Thanks!\n. Did you check for uses of it in modular examples and notebooks? Note that SGVector changes name in modular interfaces so grep for fequal, .fequal, or so, rather than for SGVector::fequal.\n. There would have not necessarily been an error if it were used in the notebooks and (maybe) part of the examples (e.g. graphical ones?) as they are not covered by tests.\n. It is looking good to me.\n. Part of the content at tapkee.lisitsyn.me, under Dimension reduction techniques (upper left corner) can be probably recycled for this.\n. Nope. You can submit a pr with changes in several files. It is good though to keep prs small and focussing on one task.\n. Exactly.\n. Travis didn't like some.h. Is it compiling with C++11 enabled?\n. I think they idea is that we have C++11 as optional dependency. Maybe some\nshould be guarded with HAVE_CXX11 as in the other places where c++11 stuff\nare used.\nOn 22 January 2015 at 13:07, Sergey Lisitsyn notifications@github.com\nwrote:\n\nHmm sorry didn't notice we still support C++ 98 :)\nCan we get rid of it? Any modern compiler supports C++11 now\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/shogun-toolbox/shogun/pull/2675#issuecomment-71011188\n.\n. The Isomap test failed in one of the Travis jobs. Isn't that ironic? xD\n. Let's merge it, then! @lisitsyn, feel free once you think it is ready -- I'm not doing it now in case you want to do something about Heiko's question above.\n. It is looking cool to me!\n. This looks like a pretty neat solution to me. From my side, feel free to merge ;-)\n. This looks like a pretty neat solution to me. From my side, feel free to merge ;-)\n. @lisitsyn, do you happen to know if there is any performance difference wrt to doing it with unique_ptr? I was just wondering whether this should wait until we decide if we take C++11 as compulsory dependency.\n. Perfect. Then, from my part, feel free to merge once you think it's ready\n;-)\nOn 19 Feb 2015 17:23, \"Sergey Lisitsyn\" notifications@github.com wrote:\n@iglesias https://github.com/iglesias no difference or faster. No need\nto use unique_ptr here anyway.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/shogun-toolbox/shogun/pull/2677#issuecomment-75082463\n.\n. Perfect. Then, from my part, feel free to merge once you think it's ready\n;-)\nOn 19 Feb 2015 17:23, \"Sergey Lisitsyn\" notifications@github.com wrote:\n@iglesias https://github.com/iglesias no difference or faster. No need\nto use unique_ptr here anyway.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/shogun-toolbox/shogun/pull/2677#issuecomment-75082463\n.\n. If it is going to be exposed to modular interfaces, use a Shogun structure. Maybe DynamicObjectArray or the one you mentioned.\n. If it is going to be exposed to modular interfaces, use a Shogun structure. Maybe DynamicObjectArray or the one you mentioned.\n. I have updated now the README_devleoper in the wiki with your suggested changes, Sanuj. Thanks! \n. We let Travis finish and then we merge it (everything should be fine anyway since #2254 was green).\n. Hello, @ghalghaz. I just trieda fresh install with current develop (Shogun 3.2.2) in Ubuntu 14.04 and the example worked fine.\n\nWhat exact version is the one you are using and what line did you use for cmake exactly?\nSmall remark: I didn't need to do sudo chmod +x ./so_multiclass_BMRM since the file had already executable permissions on. Also, I didn't need to export the LD_LIBRARY_PATH as indicated in the QUICKSTART.\n. I tried with the current develop branch of the repository. You can get that locally using git.\nI am currently trying with the version you mentioned.\n. @ghalghaz, I just tried with version 3.2.0 and the example worked fine as well. I did also a run with valgrind and it didn't detect anything abnormal.\nCould you run the program with gdb and provide more information about the crash? Note that in order to get more information from gdb you may need to compile Shogun in debug mode rather than release (which is the default). You can do that by configuring with the following line:\ncmake -DCMAKE_BUILD_TYPE=Debug ..,\nyou can also use the other options you were using before, i.e.,\ncmake -DCMAKE_BUILD_TYPE=Debug -DCMAKE_INSTALL_PREFIX=\"$HOME/shogun-install\" ...\n. Could you also please provide the information given by backtrace (bt). Also, since the logs are starting to get somewhat long, please use either gist or pastebin.\nOut of curiosity, why are you setting those two breakpoints?\n. Travis failed jobs are not due to changes here.\n. Thanks, @curiousguy13!\n. It's cool with me :)\n. I am not sure I'd say this is a bug. It might be bad usage of SGVector; you are returning a reference to some memory with local scope only in the function catchbug. \n. @yorkerlin, can you try with this version?\nSGVector<float64_t> catchbug()\n{\n    SGMatrix<float64_t> a(2,2,false);\n    a.set_const(-0.05);\n    SGVector<float64_t> tmp(a.matrix, a.num_rows*a.num_cols);\n    return tmp;\n}\n. What do you mean with class-sensitive? :-)\n. It doesn't count the reference in a class (that is, m_refcount is not a static member) but above when you are creating the first SGVector it has no way of getting to know the refcount object of SGMatrix.\n. So, in the case you cannot change the content of fun, I think the issue is that we would like to share the same RefCount object between a SGMatrix and a SGVector. I don't see how we can do this with the current API. We could introduce a new constructor in SGVector that takes an SGMatrix to do this but I am not sure this might bite us somewhere else.\n@lisitsyn, @lambday, what do you guys think? Maybe I am missing something, have a look at this issue please.\n. I don't have anything against introducing these convenience methods; but still we should keep it simple and do not add lot of clutter (many methods). Maybe one constructor for SGVector from SGMatrix and vice versa and that's all.\nBut also, I would like to knowthe use case, @yorkerlin. Why do you need to transform the SGMatrix to SGVector in your algorithm? Is it a nonsense to just keep the data in matrix form?\n. Definitely. That method should return a SGVector instead.\n. lgtm too.\n. @lambday or @lisitsyn, can you check if the openmp part (is just one line :) makes sense?\n. Perfect, thanks. Travis and merge.\n. Thanks for polishing the documentation. I will merge this but next time, please use only one commit for a change as small as this one. You can squash more than one commit into one after having done them.\n. Hey, @besser82. I see you can fix stuff related to osx, that's great :-) Do you think you could have a look to the broken osx job in Travis? https://travis-ci.org/shogun-toolbox/shogun/jobs/49594806\n. Never mind about the osx job, it has been broken for some time due to something unrelated. I have restarted the other two jobs since they failed due to compiler error (probably run out of memory, there may not work either next time). \n. Never mind about the osx job, it has been broken for some time due to something unrelated. I have restarted the other two jobs since they failed due to compiler error (probably run out of memory, there may not work either next time). \n. Thanks a lot for the patch @Jiaolong, and sorry for the wait.\n. Via #2706.\n. Via #2706.\n. Is SGVector::operator+ public? I guess it is. Then, if  a user happily tries to add two vectors of different lengths using the method, the assert will say hi.\n. I am sorry for the long delay, guys, but this pull request is HUGE :-)\n. This looks like pretty amazing work, @Jiaolong. Great! Do you think you would be able to showcase in your structure output learning notebook what can we do with it?\n. I did not see mplp mentioned anywhere in the notebook. Maybe I overlooked?\n. All right, got it.\nThen, what we need for this pull request is:\n1) Make unit tests simpler (probably just reducing input data) so we don't need to comment any of them, neither to wait long until they finish. If you want to have a larger test, then an integration one (that is, an example in Python modular) is the right place. Anyway, that would go to a future pull request.\n2) Check the function of this FGTestData. I really wonder whether it makes sense to have it in C++. But I need to know more about it first.\nThanks a lot for getting back so quickly, @Jiaolong. Really cool.\n. Then, it sounds like unit tests would be a better place for it, right?\n. I see your point. To use it from integration tests one could also just generate some data files and push them into the data repository. I admit this is not the most flexible solution though. I leave it up to you, then. If you want to keep the FG data class under structure, it is fine with me.\nI will check for other data generators we have in Shogun and see whether it would make more sense there. I will let you know.\n. There is this DataGenerator class inside the features directory. In my opinion, it is fine to keep this factor graph test data class here under structure, since it is quite specific. @karlnapf?\n@Jiaolong, I do suggest to rename it to FactorGraphDataGenerator, to follow the convention with other data generator classes.\n. Great, thanks a lot for the valgrind log. I see however the GEMPLP.sosvm is still taking too long. Can you reduce that time, please?\n. Oh, sure. Sorry, I did not take into account valgrind when I looked at the running time. 0.4 seconds should be fine, no need to worry more about it.\n. Perfect, we are ready to go, then. Thanks, @Jiaolong.\n. Feel free to send an update for the notebook when you have the time. It would be great if it includes as well some short discussion about gemplp.\n. I think it is referring to ifdef and other instructions to be taken care\nbefore compilation.\nOn 18 Feb 2015 18:11, \"Saurabh Goyal\" notifications@github.com wrote:\n\nhi i would like to take up this issue and subsequently i could complete\nother issues related to hmm which are listed along with it.\nI went through CHMM class in hmm.h. By precompiler flags do u mean inline\nfunctions or something else? I would like to know more as to how should i\nproceed with it.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/shogun-toolbox/shogun/issues/2712#issuecomment-74904003\n.\n. Easy solution would be to add attributes with setters.\n. You should try running it locally and check it\n. Shall we do a 5.0 release soon then so from pre-gsoc time people can start\nusing c++11 with no problem?\n\nWhat does Viktor think?\nOn 19 Feb 2015 18:59, \"Heiko Strathmann\" notifications@github.com wrote:\n\n+1 And also on the renaming, which is true.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/shogun-toolbox/shogun/issues/2719#issuecomment-75102381\n.\n. Remove the method in SGVector (therefore, change the calls to CMath) in the same PR.\n\nI also agree with Rahul that this should better go to linalg. In any case, I am also fine I you finish this PR putting it in CMath and then in a later one you move it from CMath to linalg. It would not be the most efficient move doing it in two steps, but your choice :-)\n. It sounds quite nice. I see it very useful to use for the output of feature selection algorithms (like LMNN).\n. Yes. See #2738.\nOn 25 Feb 2015 02:48, \"Jose Sotelo\" notifications@github.com wrote:\n\nHas this been taken? I'm trying to get familiar with the shogun toolbox\nand this looks like a good place to start.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/shogun-toolbox/shogun/issues/2730#issuecomment-75890211\n.\n. Yes. See #2738.\nOn 25 Feb 2015 02:48, \"Jose Sotelo\" notifications@github.com wrote:\nHas this been taken? I'm trying to get familiar with the shogun toolbox\nand this looks like a good place to start.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/shogun-toolbox/shogun/issues/2730#issuecomment-75890211\n.\n. @karlnapf can probably elaborate what else needs to be done here. Anyway, what I understand from his comment is that this list should not only be a list of class names but rather of ML methods/algorithms that are implemented by the classes.\n. Merging the bug fix now. @dkostka, it would be great if you could add the unit tests that Rahul has mentioned.\n. Nice job, @adit-39. Good decision to check the directories instead of the examples. Since you are on it, it would be also nice if you detect the classes that are missing unit tests and/or api examples. Those should be implemented so we can, for instance, start creating entrance issues for them.\n. A GitHub issue sounds reasonable to me. In the issue, you can have two separate lists: one with the missing unit tests, another one with the missing examples.\n\nAs they start gradually getting done, we can update those lists. Also, the PRs addressing missing unit tests and/or examples should reference the issue.\n. A GitHub issue sounds reasonable to me. In the issue, you can have two separate lists: one with the missing unit tests, another one with the missing examples.\nAs they start gradually getting done, we can update those lists. Also, the PRs addressing missing unit tests and/or examples should reference the issue.\n. https://travis-ci.org/shogun-toolbox/shogun/jobs/52040810#L1280\n. We can't merge this automatically, can you rebase?\n. It looks good to me, just very minor comments above. @lambday, ready to merge?\n. 1) Check in other places where linalg is used. You can find them fast by\nusing grep.\n2) Can they be substituted using SGVector? It is more convenient to use\nthem, less error- and memory leak-prone.\nOn 28 Feb 2015 05:59, \"Kunal Arora\" notifications@github.com wrote:\n\n[INCOMPLETE]\n@lambday https://github.com/lambday I had a couple of questions\nregarding this:\n1)Should I place a guard like #ifdef HAVE_LINALG_LIB before including\nlinalg.h in files?\n2)What would be the best way to deal with normal arrays like :\nfloat64_t* w= SG_MALLOC(float64_t, d);\nSGVector::range_fill_vector(w, d, 17.0);\nshould I make a separate method for this?\nYou can view, comment on, or merge this pull request online at:\nhttps://github.com/shogun-toolbox/shogun/pull/2745\nCommit Summary\n- added range_fill in linalg\nFile Changes\n- T src/shogun/kernel/GaussianARDKernel.cpp\n  https://github.com/shogun-toolbox/shogun/pull/2745/files#diff-0 (0)\n- A src/shogun/mathematics/linalg/internal/implementation/RangeFill.h\n  https://github.com/shogun-toolbox/shogun/pull/2745/files#diff-1\n  (116)\n- M src/shogun/mathematics/linalg/internal/modules/Core.h\n  https://github.com/shogun-toolbox/shogun/pull/2745/files#diff-2 (14)\n- M src/shogun/regression/KernelRidgeRegression.cpp\n  https://github.com/shogun-toolbox/shogun/pull/2745/files#diff-3 (6)\n- M src/shogun/statistics/IndependenceTest.cpp\n  https://github.com/shogun-toolbox/shogun/pull/2745/files#diff-4 (4)\n- M src/shogun/statistics/KernelIndependenceTest.cpp\n  https://github.com/shogun-toolbox/shogun/pull/2745/files#diff-5 (4)\n- M src/shogun/statistics/KernelTwoSampleTest.cpp\n  https://github.com/shogun-toolbox/shogun/pull/2745/files#diff-6 (4)\n- M src/shogun/statistics/StreamingMMD.cpp\n  https://github.com/shogun-toolbox/shogun/pull/2745/files#diff-7 (6)\n- M src/shogun/statistics/TwoSampleTest.cpp\n  https://github.com/shogun-toolbox/shogun/pull/2745/files#diff-8 (4)\n- M src/shogun/structure/DisjointSet.cpp\n  https://github.com/shogun-toolbox/shogun/pull/2745/files#diff-9 (4)\n- M src/shogun/structure/TwoStateModel.cpp\n  https://github.com/shogun-toolbox/shogun/pull/2745/files#diff-10 (4)\n- A tests/unit/mathematics/linalg/RangeFill_unittest.cc\n  https://github.com/shogun-toolbox/shogun/pull/2745/files#diff-11\n  (69)\nPatch Links:\n- https://github.com/shogun-toolbox/shogun/pull/2745.patch\n- https://github.com/shogun-toolbox/shogun/pull/2745.diff\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/shogun-toolbox/shogun/pull/2745.\n. Unit tests should pretty much cover all the code of the class. Have a look\nat other unit tests to get the idea.\n\nIs SGSparseVector under lib or base?\nOn 1 Mar 2015 07:23, \"Yingrui Chang\" notifications@github.com wrote:\n\nI would like to take a shot on this problem. What are the essential\nfunctions need to be tested (the file loaders?), and where the test file\nshould be placed (under test/unit/base/ ?)\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/shogun-toolbox/shogun/issues/2748#issuecomment-76580704\n.\n. It sounds reasonable, yes. Feel free to create a group of similar tests. For instance, access_by_index_square, access_by_index_more_cols, access_by_index_more_rows (or the names you prefer).\n. Hi @nginn,\n\nWhat I can see from the coverage analysis is that mainly there are two constructors, the load, and save methods that are not yet covered:\nhttps://coveralls.io/builds/4127565/source?filename=src%2Fshogun%2Flib%2FSGSparseMatrix.cpp\n. @souravsingh, I'd say get the system running locally as described in the readme in the link in the first comment. Have a look at how the examples already there work and then start creating new ones showcasing Shogun features. For the new ones, draw inspiration from the already existing Shogun examples.\n. I like the idea too.\n. Where did you find that tutorial, @neutralino?\nOn 2 March 2015 at 02:26, Don notifications@github.com wrote:\n\nThanks @lisitsyn https://github.com/lisitsyn! That works, turns out I\nwas also following a really old version of the tutorial.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/shogun-toolbox/shogun/issues/2753#issuecomment-76646220\n.\n. Also, I think it is easier to think about sparse matrices in terms of features and vectors instead of rows and columns.\n. Thanks for the new pull request, @yingryic. See the comments above and also have a look at the few comments regarding style left.\n. Merging for the moment since everything is fine. But it would be nice if you answer the comments above and address them if you like in new pull request.\n. I am at the phone with crappy connection now. Will continue reviewing later.\nOn 4 Mar 2015 19:47, \"Yingrui Chang\" notifications@github.com wrote:\ncreate a template to generate both sparse and dense matrix and made the\nrest of the test more concise. @iglesias https://github.com/iglesias\nYou can view, comment on, or merge this pull request online at:\nhttps://github.com/shogun-toolbox/shogun/pull/2759\nCommit Summary\n- modified sparse matrix unit test, made it cleaner\nFile Changes\n- M tests/unit/lib/SGSparseMatrix_unittest.cc\n  https://github.com/shogun-toolbox/shogun/pull/2759/files#diff-0\n  (105)\nPatch Links:\n- https://github.com/shogun-toolbox/shogun/pull/2759.patch\n- https://github.com/shogun-toolbox/shogun/pull/2759.diff\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/shogun-toolbox/shogun/pull/2759.\n. I am sorry that I did not detect that when reviewing the pr. Thanks a lot for fixing it, @lambday!\n. You should be using modshogun rather than init_shogun (see octave modular examples).\n. @yingryic, have a look. I also replaced the indentation with tabs instead of white spaces. In new files, white spaces are fine to indent but in an already existing file, please use the same characters that were already used. Configure your editor to make white space characters visible.\n. I even think we should remove the notion of features and vectors from these matrix classes. They should be treated as math objects and use more abstract terms (row and column) than feature and vector. The latter appear when we use matrices to represent feature matrices.\n. And the same for SGSparseVector. From my point of view, there should be no feature mention there.\n. Oh, I see. It is understood now. Thanks a lot for the explanation. This makes me wonder whether we should provide after all the operator() in SGSparseMatrix :D\n. They do provide that operator in other implementations (at least, I am somewhat confident Matlab does).\n. They do provide that operator in other implementations (at least, I am somewhat confident Matlab does).\n. I am really surprised to see that nothing breaks after this change! Does this mean that SGSparseMatrix is not really used anywhere? I would expect that all around where SGSparseMatrix::operator() is used, the features and vector indices should be swapped. Am I missing something?\n. I don't think there is any risk as the operator() is used nowhere but the unit test.\n. I don't think there is any risk as the operator() is used nowhere but the unit test.\n. In fact, I really wonder whether we need it if it is not used.\n. In fact, I really wonder whether we need it if it is not used.\n. What compiler and version are you using?\n\nOn 16 March 2015 at 01:05, vishalbelsare notifications@github.com wrote:\n\nI am compiling from the source tarball (SHOGUN Release version 4.0.0\n(libshogun 17.0, data 0.9, parameter 1) ) taken from Shogun's website. I am\ncompiling on Ubuntu 12.04. Things seem to go well, until the following\nshows up :\n[ 83%] Building CXX object\nsrc/shogun/CMakeFiles/libshogun.dir/io/protobuf/Chunks.pb.cc.o\n/home/vishal/Desktop/shogun-4.0.0/src/shogun/io/protobuf/Chunks.pb.cc: In\nmember function \u2018virtual int shogun::BoolChunk::ByteSize() const\u2019:\n/home/vishal/Desktop/shogun-4.0.0/src/shogun/io/protobuf/Chunks.pb.cc:392:9:\nwarning: declaration of \u2018data_size\u2019 shadows a member of 'this' [-Wshadow]\n/home/vishal/Desktop/shogun-4.0.0/src/shogun/io/protobuf/Chunks.pb.cc: In\nmember function \u2018virtual int shogun::Int32Chunk::ByteSize() const\u2019:\n/home/vishal/Desktop/shogun-4.0.0/src/shogun/io/protobuf/Chunks.pb.cc:614:9:\nwarning: declaration of \u2018data_size\u2019 shadows a member of 'this' [-Wshadow]\n/home/vishal/Desktop/shogun-4.0.0/src/shogun/io/protobuf/Chunks.pb.cc: In\nmember function \u2018virtual int shogun::UInt32Chunk::ByteSize() const\u2019:\n/home/vishal/Desktop/shogun-4.0.0/src/shogun/io/protobuf/Chunks.pb.cc:839:9:\nwarning: declaration of \u2018data_size\u2019 shadows a member of 'this' [-Wshadow]\n/home/vishal/Desktop/shogun-4.0.0/src/shogun/io/protobuf/Chunks.pb.cc: In\nmember function \u2018virtual int shogun::Int64Chunk::ByteSize() const\u2019:\n/home/vishal/Desktop/shogun-4.0.0/src/shogun/io/protobuf/Chunks.pb.cc:1064:9:\nwarning: declaration of \u2018data_size\u2019 shadows a member of 'this' [-Wshadow]\n/home/vishal/Desktop/shogun-4.0.0/src/shogun/io/protobuf/Chunks.pb.cc: In\nmember function \u2018virtual int shogun::UInt64Chunk::ByteSize() const\u2019:\n/home/vishal/Desktop/shogun-4.0.0/src/shogun/io/protobuf/Chunks.pb.cc:1289:9:\nwarning: declaration of \u2018data_size\u2019 shadows a member of 'this' [-Wshadow]\n/home/vishal/Desktop/shogun-4.0.0/src/shogun/io/protobuf/Chunks.pb.cc: In\nmember function \u2018virtual int shogun::Float32Chunk::ByteSize() const\u2019:\n/home/vishal/Desktop/shogun-4.0.0/src/shogun/io/protobuf/Chunks.pb.cc:1514:9:\nwarning: declaration of \u2018data_size\u2019 shadows a member of 'this' [-Wshadow]\n/home/vishal/Desktop/shogun-4.0.0/src/shogun/io/protobuf/Chunks.pb.cc: In\nmember function \u2018virtual int shogun::Float64Chunk::ByteSize() const\u2019:\n/home/vishal/Desktop/shogun-4.0.0/src/shogun/io/protobuf/Chunks.pb.cc:1736:9:\nwarning: declaration of \u2018data_size\u2019 shadows a member of 'this' [-Wshadow]\n[ 83%] Building CXX object\nsrc/shogun/CMakeFiles/libshogun.dir/base/class_list.cpp.o\nIn file included from\n/home/vishal/Desktop/shogun-4.0.0/src/shogun/kernel/normalizer/KernelNormalizer.h:16:0,\nfrom\n/home/vishal/Desktop/shogun-4.0.0/src/shogun/transfer/multitask/MultitaskKernelMaskNormalizer.h:16,\nfrom /home/vishal/Desktop/shogun-4.0.0/src/shogun/base/class_list.cpp:15:\n/home/vishal/Desktop/shogun-4.0.0/src/shogun/kernel/Kernel.h: In member\nfunction \u2018float64_t shogun::CKernel::kernel(int32_t, int32_t)\u2019:\n/home/vishal/Desktop/shogun-4.0.0/src/shogun/kernel/Kernel.h:210:21:\nerror: invalid use of incomplete type \u2018struct shogun::CKernelNormalizer\u2019\n/home/vishal/Desktop/shogun-4.0.0/src/shogun/kernel/Kernel.h:35:8: error:\nforward declaration of \u2018struct shogun::CKernelNormalizer\u2019\n/home/vishal/Desktop/shogun-4.0.0/src/shogun/kernel/Kernel.h:211:3:\nwarning: control reaches end of non-void function [-Wreturn-type]\nmake[2]: * [src/shogun/CMakeFiles/libshogun.dir/base/class_list.cpp.o]\nError 1\nmake[1]: * [src/shogun/CMakeFiles/libshogun.dir/all] Error 2\nmake: *** [all] Error 2\nvishal@goedel:~/Desktop/shogun-4.0.0/Build$\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/shogun-toolbox/shogun/issues/2772.\n. What compiler and version are you using?\n\nOn 16 March 2015 at 01:05, vishalbelsare notifications@github.com wrote:\n\nI am compiling from the source tarball (SHOGUN Release version 4.0.0\n(libshogun 17.0, data 0.9, parameter 1) ) taken from Shogun's website. I am\ncompiling on Ubuntu 12.04. Things seem to go well, until the following\nshows up :\n[ 83%] Building CXX object\nsrc/shogun/CMakeFiles/libshogun.dir/io/protobuf/Chunks.pb.cc.o\n/home/vishal/Desktop/shogun-4.0.0/src/shogun/io/protobuf/Chunks.pb.cc: In\nmember function \u2018virtual int shogun::BoolChunk::ByteSize() const\u2019:\n/home/vishal/Desktop/shogun-4.0.0/src/shogun/io/protobuf/Chunks.pb.cc:392:9:\nwarning: declaration of \u2018data_size\u2019 shadows a member of 'this' [-Wshadow]\n/home/vishal/Desktop/shogun-4.0.0/src/shogun/io/protobuf/Chunks.pb.cc: In\nmember function \u2018virtual int shogun::Int32Chunk::ByteSize() const\u2019:\n/home/vishal/Desktop/shogun-4.0.0/src/shogun/io/protobuf/Chunks.pb.cc:614:9:\nwarning: declaration of \u2018data_size\u2019 shadows a member of 'this' [-Wshadow]\n/home/vishal/Desktop/shogun-4.0.0/src/shogun/io/protobuf/Chunks.pb.cc: In\nmember function \u2018virtual int shogun::UInt32Chunk::ByteSize() const\u2019:\n/home/vishal/Desktop/shogun-4.0.0/src/shogun/io/protobuf/Chunks.pb.cc:839:9:\nwarning: declaration of \u2018data_size\u2019 shadows a member of 'this' [-Wshadow]\n/home/vishal/Desktop/shogun-4.0.0/src/shogun/io/protobuf/Chunks.pb.cc: In\nmember function \u2018virtual int shogun::Int64Chunk::ByteSize() const\u2019:\n/home/vishal/Desktop/shogun-4.0.0/src/shogun/io/protobuf/Chunks.pb.cc:1064:9:\nwarning: declaration of \u2018data_size\u2019 shadows a member of 'this' [-Wshadow]\n/home/vishal/Desktop/shogun-4.0.0/src/shogun/io/protobuf/Chunks.pb.cc: In\nmember function \u2018virtual int shogun::UInt64Chunk::ByteSize() const\u2019:\n/home/vishal/Desktop/shogun-4.0.0/src/shogun/io/protobuf/Chunks.pb.cc:1289:9:\nwarning: declaration of \u2018data_size\u2019 shadows a member of 'this' [-Wshadow]\n/home/vishal/Desktop/shogun-4.0.0/src/shogun/io/protobuf/Chunks.pb.cc: In\nmember function \u2018virtual int shogun::Float32Chunk::ByteSize() const\u2019:\n/home/vishal/Desktop/shogun-4.0.0/src/shogun/io/protobuf/Chunks.pb.cc:1514:9:\nwarning: declaration of \u2018data_size\u2019 shadows a member of 'this' [-Wshadow]\n/home/vishal/Desktop/shogun-4.0.0/src/shogun/io/protobuf/Chunks.pb.cc: In\nmember function \u2018virtual int shogun::Float64Chunk::ByteSize() const\u2019:\n/home/vishal/Desktop/shogun-4.0.0/src/shogun/io/protobuf/Chunks.pb.cc:1736:9:\nwarning: declaration of \u2018data_size\u2019 shadows a member of 'this' [-Wshadow]\n[ 83%] Building CXX object\nsrc/shogun/CMakeFiles/libshogun.dir/base/class_list.cpp.o\nIn file included from\n/home/vishal/Desktop/shogun-4.0.0/src/shogun/kernel/normalizer/KernelNormalizer.h:16:0,\nfrom\n/home/vishal/Desktop/shogun-4.0.0/src/shogun/transfer/multitask/MultitaskKernelMaskNormalizer.h:16,\nfrom /home/vishal/Desktop/shogun-4.0.0/src/shogun/base/class_list.cpp:15:\n/home/vishal/Desktop/shogun-4.0.0/src/shogun/kernel/Kernel.h: In member\nfunction \u2018float64_t shogun::CKernel::kernel(int32_t, int32_t)\u2019:\n/home/vishal/Desktop/shogun-4.0.0/src/shogun/kernel/Kernel.h:210:21:\nerror: invalid use of incomplete type \u2018struct shogun::CKernelNormalizer\u2019\n/home/vishal/Desktop/shogun-4.0.0/src/shogun/kernel/Kernel.h:35:8: error:\nforward declaration of \u2018struct shogun::CKernelNormalizer\u2019\n/home/vishal/Desktop/shogun-4.0.0/src/shogun/kernel/Kernel.h:211:3:\nwarning: control reaches end of non-void function [-Wreturn-type]\nmake[2]: * [src/shogun/CMakeFiles/libshogun.dir/base/class_list.cpp.o]\nError 1\nmake[1]: * [src/shogun/CMakeFiles/libshogun.dir/all] Error 2\nmake: *** [all] Error 2\nvishal@goedel:~/Desktop/shogun-4.0.0/Build$\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/shogun-toolbox/shogun/issues/2772.\n. Could you also check if there is #include\n in your\nshogun/kernel/Kernel.h? If so, I think it should be possible to remove\nsafely the forward declaration your compiler is complaining about.\n\nOn 16 March 2015 at 09:15, Fernando J. Iglesias Garc\u00eda \nfernando.iglesiasg@gmail.com wrote:\n\nWhat compiler and version are you using?\nOn 16 March 2015 at 01:05, vishalbelsare notifications@github.com wrote:\n\nI am compiling from the source tarball (SHOGUN Release version 4.0.0\n(libshogun 17.0, data 0.9, parameter 1) ) taken from Shogun's website. I am\ncompiling on Ubuntu 12.04. Things seem to go well, until the following\nshows up :\n[ 83%] Building CXX object\nsrc/shogun/CMakeFiles/libshogun.dir/io/protobuf/Chunks.pb.cc.o\n/home/vishal/Desktop/shogun-4.0.0/src/shogun/io/protobuf/Chunks.pb.cc:\nIn member function \u2018virtual int shogun::BoolChunk::ByteSize() const\u2019:\n/home/vishal/Desktop/shogun-4.0.0/src/shogun/io/protobuf/Chunks.pb.cc:392:9:\nwarning: declaration of \u2018data_size\u2019 shadows a member of 'this' [-Wshadow]\n/home/vishal/Desktop/shogun-4.0.0/src/shogun/io/protobuf/Chunks.pb.cc:\nIn member function \u2018virtual int shogun::Int32Chunk::ByteSize() const\u2019:\n/home/vishal/Desktop/shogun-4.0.0/src/shogun/io/protobuf/Chunks.pb.cc:614:9:\nwarning: declaration of \u2018data_size\u2019 shadows a member of 'this' [-Wshadow]\n/home/vishal/Desktop/shogun-4.0.0/src/shogun/io/protobuf/Chunks.pb.cc:\nIn member function \u2018virtual int shogun::UInt32Chunk::ByteSize() const\u2019:\n/home/vishal/Desktop/shogun-4.0.0/src/shogun/io/protobuf/Chunks.pb.cc:839:9:\nwarning: declaration of \u2018data_size\u2019 shadows a member of 'this' [-Wshadow]\n/home/vishal/Desktop/shogun-4.0.0/src/shogun/io/protobuf/Chunks.pb.cc:\nIn member function \u2018virtual int shogun::Int64Chunk::ByteSize() const\u2019:\n/home/vishal/Desktop/shogun-4.0.0/src/shogun/io/protobuf/Chunks.pb.cc:1064:9:\nwarning: declaration of \u2018data_size\u2019 shadows a member of 'this' [-Wshadow]\n/home/vishal/Desktop/shogun-4.0.0/src/shogun/io/protobuf/Chunks.pb.cc:\nIn member function \u2018virtual int shogun::UInt64Chunk::ByteSize() const\u2019:\n/home/vishal/Desktop/shogun-4.0.0/src/shogun/io/protobuf/Chunks.pb.cc:1289:9:\nwarning: declaration of \u2018data_size\u2019 shadows a member of 'this' [-Wshadow]\n/home/vishal/Desktop/shogun-4.0.0/src/shogun/io/protobuf/Chunks.pb.cc:\nIn member function \u2018virtual int shogun::Float32Chunk::ByteSize() const\u2019:\n/home/vishal/Desktop/shogun-4.0.0/src/shogun/io/protobuf/Chunks.pb.cc:1514:9:\nwarning: declaration of \u2018data_size\u2019 shadows a member of 'this' [-Wshadow]\n/home/vishal/Desktop/shogun-4.0.0/src/shogun/io/protobuf/Chunks.pb.cc:\nIn member function \u2018virtual int shogun::Float64Chunk::ByteSize() const\u2019:\n/home/vishal/Desktop/shogun-4.0.0/src/shogun/io/protobuf/Chunks.pb.cc:1736:9:\nwarning: declaration of \u2018data_size\u2019 shadows a member of 'this' [-Wshadow]\n[ 83%] Building CXX object\nsrc/shogun/CMakeFiles/libshogun.dir/base/class_list.cpp.o\nIn file included from\n/home/vishal/Desktop/shogun-4.0.0/src/shogun/kernel/normalizer/KernelNormalizer.h:16:0,\nfrom\n/home/vishal/Desktop/shogun-4.0.0/src/shogun/transfer/multitask/MultitaskKernelMaskNormalizer.h:16,\nfrom /home/vishal/Desktop/shogun-4.0.0/src/shogun/base/class_list.cpp:15:\n/home/vishal/Desktop/shogun-4.0.0/src/shogun/kernel/Kernel.h: In member\nfunction \u2018float64_t shogun::CKernel::kernel(int32_t, int32_t)\u2019:\n/home/vishal/Desktop/shogun-4.0.0/src/shogun/kernel/Kernel.h:210:21:\nerror: invalid use of incomplete type \u2018struct shogun::CKernelNormalizer\u2019\n/home/vishal/Desktop/shogun-4.0.0/src/shogun/kernel/Kernel.h:35:8: error:\nforward declaration of \u2018struct shogun::CKernelNormalizer\u2019\n/home/vishal/Desktop/shogun-4.0.0/src/shogun/kernel/Kernel.h:211:3:\nwarning: control reaches end of non-void function [-Wreturn-type]\nmake[2]: * [src/shogun/CMakeFiles/libshogun.dir/base/class_list.cpp.o]\nError 1\nmake[1]: * [src/shogun/CMakeFiles/libshogun.dir/all] Error 2\nmake: *** [all] Error 2\nvishal@goedel:~/Desktop/shogun-4.0.0/Build$\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/shogun-toolbox/shogun/issues/2772.\n. I know that it works with the gcc version that comes in Ubuntu 14.04.\n\n\nHave you tried removing the forward declaration I mentioned in the previous\nmessage?\nOn 16 March 2015 at 09:41, vishalbelsare notifications@github.com wrote:\n\nCompiler version quite dated at:\nvishal@goedel:~/Desktop/shogun-4.0.0/Build$ g++ --version\ng++ (Ubuntu/Linaro 4.6.3-1ubuntu5) 4.6.3\nYes, /shogun/kernel/Kernel.h has a include directive for\nWould upgrading the compiler help with this?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/shogun-toolbox/shogun/issues/2772#issuecomment-81506309\n.\n. Hey @yorkerlin. Hopefully, my last commit will fix the version problem. You may need to rebase.\n. It makes sense to me to refactor it so that it is const.\n. I must admit that looks very weird and unknown to tme :-) @lisitsyn, any idea?\n\nIf you can, @tengshaofeng, copy in pastebin, gist, or somewhere else the list of instructions you are doing to download Shogun, configure the build with cmake and trying to build it.\n. lgtm :+1: \n. Reopening until we are sure if we can make use of this solution.\n. It sounds great :-)\n. I only have the remark against using malloc and free. Feel free to merge if you feel it is right though.\n. I only have the remark against using malloc and free. Feel free to merge if you feel it is right though.\n. Unfortunately, we do not have much experience using Shogun in Windows. Perhaps someone who has tried it in the past might be able to help you. @vigsterkr, @lisitsyn ?\n. Unfortunately, we do not have much experience using Shogun in Windows. Perhaps someone who has tried it in the past might be able to help you. @vigsterkr, @lisitsyn ?\n. Any updates here @srsandeep?\n. iirc we were already using C++11. @lisitsyn, @vigsterkr, is that right?\n. I definitely agree with @lisitsyn. I haven't checked the version of the compiler myself, but it must support C++11. If it is not enabled by default, then we have to set the appropriate compiler flag.\n. Can you check if it is that you are running out of RAM when attempting to compile Shogun?\n. Closing due to inactivity.\n. Hey Daniel! Thank you for your interest in Shogun and the LMNN implementation. Note that the LMNN implementation in Shogun depends on Eigen3. Did you compile Shogun with Eigen3 support enabled?\n. cmake -DCMAKE_INSTALL_PREFIX=\"$HOME/shogun-install\" -DPythonModular=ON -DENABLE_EIGEN=ON ..\nshould make it (providing you have an active internet connection, the option above will download Eigen 3 and bundle it into your Shogun installation).\nWhen dealing with cmake in general, not only in Shogun, I suggest you to use ccmake when in doubt of the available cmake options or their exact name.\n. Oh, sorry about that. My mistake.\n. Solving issue.\n. Hi @iarroyof. Are you obtaining the weights using Shogun or from somewhere else? In case you are also using Shogun for the training, maybe using serialization solves your problem. See for instance serialization_string_kernels_modular.py or other serialization examples.\n. All right, let's try to go one part at a time.\n- I am not familiarised with the implementation of MKL so perhaps it is actually required to call train in an MKL object before you can call apply. Maybe some internal trained state is set upon call to train and it is checked before calling apply, failing the call if the trained state has not been set.\nWould cloning the MKL trained object solve your problem?\nThe C++ example you have cited above is not the same as what you are doing in your Python code. Note that in the C++ example tsvm has been trained before setting the kernel.\n- I am unsure about they way you are using save to serialize. The serialization Python examples and also in some code I used in the past I used pickle.\n. Keep us updated about your progress and feel free to close the issue once you consider it solved.\n. I have never user personally Shogun with Python 3 and Swig 3. Python 2.7 and Swig 2.0 should work, and Python 3 with Swigh 2.0 is building fine in Travis too. This is not a proper solution but if you can afford using those versions it might be the fastest workaround to get it to work.\n@vigsterkr, any known issue with Python 3 and Swig 3 together?\n. Would be nice if you can also fix indentation starting in line 100. The first level of indentation of the method has two spaces, which does not correspond to the first level of the other methods in the same file.\n. Merging. Travis failures not caused by this change.\n. You are definitely right. Thanks for the heads up!\n. Getting leaner :-)\nFor reference see #1253.\n. EDIT after reading that Lloyd's algorithm also admits maintaining the cluster centers fixed.\nI am not entirely sure I understand you correctly. To me fixed cluster centers would mean that the cluster centers are not moved, fixed from the beginning and not iteratively updated.\nCan you elaborate? \n. Hello again.\nfixed_centers is just an option we provide in the algorithm. You can use the algorithm with either the option set to true or false.\nWhy do you use that it obviously considers one case and ignores the other?\n. No problem at all :-)\n. No problem, @yorkerlin, you are welcome. Thanks to you for making all these cleanups.\n. lgtm. For next time maybe you can squash such commits into one (there is no need to have two separate commits for a change like this).\nIs the error in the second Travis job unrelated?\n. It makes sense to me.\n. If would be great if you can squash the indentation commits into the main one.\n. Thanks to you for getting back with the fixes swiftly :-)\n. Thanks!\n. @yorkerlin, any idea why the unit tests below failed in the clang job of Travis? Have you seen these tests failing other times?\nThe following tests FAILED:\n     48 - unit-GaussianARDKernel (Failed)\n     93 - unit-GaussianProcessClassificationUsingSingleFITCLaplacian (Failed)\n     95 - unit-GaussianProcessRegression (Failed)\n    218 - unit-FITCInferenceMethod (Failed)\n    228 - unit-SingleFITCLaplacianInferenceMethod (Failed)\n    229 - unit-SingleFITCLaplacianInferenceMethodWithLBFGS (Failed)\nErrors while running CTest\n. Hey @yorkerlin. You don't need a mac to use clang ;-) I think clang is the default compiler in os x (don't quote me on this one before double checking) but you can definitely use it in linux as well. \nIn fact, the clang job that is failing in Travis is running Ubuntu 12.04.\n. Why the kernel matrices you are getting means that the subkernels do not initialise for your data? What kernel matrices do you expect instead?\n. Why the kernel matrices you are getting means that the subkernels do not initialise for your data? What kernel matrices do you expect instead?\n. My point is that if you don't know what the values of the kernel matrix should be, why can you conclude that the identity matrix is not the correct one?\nAlso, I think that you might be inputting the feature matrix wrong when you create the RealFeatures object. Note that the kernel matrices that are displayed are rather large whereas if you only want to input three feature vectors the kernel matrices should be 3x3. Does that make sense?\n. Cool. I understand the issue is solved now.\n. Hey @iarroyof. No, I did not mean you need re-compiling. Coming back to one of the comments I made, if as you say the identity matrix is not the correct matrix for your kernel and data, what should the kernel matrix be instead?\n. Cool, thank you.\n. Hi @kno10, thanks a lot for your analysis and the comments.\nRegarding the first observation in your first comment, why do you say that mus is never used in the second loop (the for loop that starts after the block if (!fixed_centers)  { ... } ends)?\n. I think (perhaps I am getting it wrong) that mus is both read and written. Note that in lines 110 and 124 the operator used is -=. It is not probably not the most readable way of doing this :-)\nDoes that make sense to you?\n. It does, because fixed_centers can be true. However, I do agree with you that all the work done to update mus is a waste in case fixed_centers is false -- which may be the most common use case.\nI think we should break up this method into two methods, and use either of them depending on the value of fixed_centers.\nIt is great that you pointed this out! Thanks a lot.\n. @kno10, can you share the code you used for the benchmark results you mentioned above? A gist or similar would work fine.\n. Thanks for the patch! It needs a bit polishing here and there. Lookout for instances of the stuff we have pointed out even in places where we have not said anything explicitly.\n. Thanks! Since the line with the second change is actually a comment, can we just get rid of it?\n. +1 to Heiko's comments and it looks good to me too.\n. Hey, @Saurabh7. I think that the previous kmeans++ has indeed implemented the kmeans++ algorithm: https://github.com/shogun-toolbox/shogun/commit/05408fa731cda5f32e300b80f3ed6fcce1b944f9#diff-d738d5845382c912de5f74730c54329bR471.\nCan you please elaborate?\n\nPrevious kmeans++ was not kmeans++ ;) all it did was choose first k samples from data as initial centers. (which is worse than random init)\n. I think you are right. There is a small bug in the previous code, the first line in the loop above\n\nSGVector<float64_t> feature=lhs->get_feature_vector(c_m);\nshould be\nSGVector<float64_t> feature=lhs->get_feature_vector(mu_index[c_m]);\nDoes that make sense to you?\n. I say it is ready to merge from my side too. Thanks for the work and awesome you caught the bug!\nIs the benchmark mentioned above coming next?\n. Time is looking nice (btw, I am assuming that time is in the first table, measured in seconds).\nCan you provide more context about the second table (what are final sum and intertia)? Also, out of curiosity, why a couple of data sets of the first table are missing in the second?\nDoes it make sense to include these results in the benchmark framework?\n. All righty, thanks for the explanation :-)\n. Nice analysis and findings!\nOn 9 April 2016 at 11:46, Saurabh Mahindre notifications@github.com wrote:\n\n@karlnapf https://github.com/karlnapf @lambday\nhttps://github.com/lambday\nwow I just checked by changing the wrapper method to pass by reference too:\ntypename Vector::Scalar dot(Vector a, Vector b)\nchanged to\ntypename Vector::Scalar dot(Vector& a, Vector& b)\nalong with changing the compute to pass const reference :\nstatic T compute(const shogun::SGVector& a, const shogun::SGVector& b)\nand the results are now same as in CMath.\nSo looks like the problem was due to SGVector being passed by value in\nboth wrapper method and in addition in the compute method leading to\nadditional overhead.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly or view it on GitHub\nhttps://github.com/shogun-toolbox/shogun/pull/3145#issuecomment-207760068\n. I think @lambday can give you a good answer to that.\n. Hi @yorkerlin. I am wondering whether the cause may be something different to passing a non-SGObject to an SGObject via swig. CSingleLaplaceInferenceMethod extends the abstract class CInference, while Dummy does not extend an abstract class.\n\nWhat about changing your experiment so that Dummy extends an abstract class? The method register_minimizer would be the pure virtual in the parent class.\nI am wondering whether the issue may be caused by the generation of default constructors of a class that extends an abstract class.\nLooking forward to hearing from you!\n. Shame, I found it worth trying :-S\nIt may be interesting to find out why this happens though. Is it something we have configured on purpose for some reason? A bug? Etc.\n. Hey @yorkerlin, nice analysis. So, if I have interpreted correctly your results, the trigger is overriding register_minimizer. With the classes that do not override it we get the TypeError. Agree?\n. Yeah, I agree. I was just checking exactly that :-)\n. My question now is, is CExactInferenceMethod overriding all the pure virtual methods of CInference?\n. I think it does indeed. Otherwise, it should not be possible to instantiate it. So that makes sense at least.\n. @yorkerlin, does taking a look at the file generated by swig help? How does the code for register_minimizer for the different classes look like?\n. Looking forward to hear, @yorkerlin ;-)\n. Something related to inclusion of headers?\n. I think so, @yorkerlin. Have a look at interfaces/modular/GaussianProcess_includes.i and interfaces/modular/GaussianProcess.i.\n. I think those are in the recently included Minimizer.i and Minimizer_includes.i.\n. It looks like they are in develop\nhttps://github.com/shogun-toolbox/shogun/commit/e36cae74867969986776199178d821941748aa0f\n. Order with the includes can mess with anyone very badly! Cool that you detected the issue :-)\nWhy did you change the using namespace to the block? I thought we were employing the convention of using namespace in cpp implementation files.\n. Oh, now I realized you already merged this directly. It is all right, but please get back on the namespace query.\n. Should address the current error in nightly_all:\nhttp://buildbot.shogun-toolbox.org/builders/nightly_all/builds/54/steps/compile/logs/stdio. From Travis it seems that _unlink is not available in unix (maybe the right include is missing). It fixed the warnings in Windows build though.\nA good solution might be to wait until we use C++17 and change to std::filesystem::remove. . Self-reminder: if we merge this, remove from infra setting of this option.. Should the example python_modular/transfer_multitask_clustered_logistic_regression.py also moved under gpl?. Sure, I'll take care of it :+1:. I thought a bit further about the notebooks preview for PRs. For the actual previews/visualizations of the notebooks, I came up with creating a github repo where the notebooks with the cells' outputs will be pushed.\nPRs can be organized in branches or the dir structure of the repo. Once a PR is merged, the corresponding branch/directory is removed.\nThoughts on that? @vigsterkr @karlnapf . Yeah, using the webserver sounds good too.. Should we update the clang-format style file for this? @geektoni . @geektoni I don't have strong feelings about the location of the trailing '\\'s; though it looks indeed nice all aligned.\nRegarding the indentation between the first line with the macro signature and the following lines, I think it would be better to have everything with the same level of indentation since I believe that is more consistent with the current codebase.. I agree with you, @geektoni. I think 3 is the most suitable option as well.. Nice one, thank you!. Hey @kislayabhi. That seems to reconcile with the last paragraph in the first comment, correct? What do you think?. I think usually loading features into Shogun from files created by another program should be good.\nIf there's some application in mind where file i/o would not be suitable we could look into the wrapper idea.. Is there some link to that included file? Or do this include (and maybe others) need to be updated?. @vinx13, since you have already touched LMNN and are at least a bit familiar with it, would you like to pick this one up? :-). How is it going with this one, @vinx13? I wonder, no chase ;-). Thanks a lot for reporting @ealtamir.\nThe fix explains both issues (including the one with k=1). You were using a data set where one class had only 1 example, this is too few for the method's implementation. As an anecdote, I checked in the other public implementations of LMNN that I know of and they also have this \"bug\". At least now our implementation stops gracefully and points you at the error \ud83d\ude38 .  @FaroukY, the mentioned StratifiedCrossValidation is StratifiedCrossValidationSplitting, I guess.\nI think there are other points still open. I will comment in them for convenience.. @FaroukY, I folllowed-up in a few unadressed comments from Heiko. I am confused because apart from the StratifiedCrossValidationSplitting, all the other points were expected to be addressed. Am I looking at something wrong?. No problem, @FaroukY. Understood.\nRegarding the problem with StratifiedCrossValidationSplitting. Can it be solved with what Heiko mentioned in an earlier comment: patching the .cpp file and converting the labels to the right type rather than asserting?. @FaroukY, what is the state on this one? . Sorry that I did not mention it earlier, I have noticed only now the number of commits. Can you squash the 4 into 1? Thanks!. Adding a default constructor to RandomIterator. Let's see if that hopefully makes Windows build happy.. @lisitsyn , @vigsterkr, what do you think about RandomIterator's default constructor; any potential side-effect?\nIncluded because of https://ci.appveyor.com/project/vigsterkr/shogun/build/2335#L6154.. Argh the formatting again, didn't even think about this time since it was just editing. Anyways, I will set up a commit hook locally.. I have the feeling that @vigsterkr's first general comment is not addressed. @FaroukY, did you discuss with anybody about it?. How when it is needed, I think. We can sync in a few days about what\nattributes these cost functions should have, which ones make sense to make\npart of the public api.\nFor instance right now I see that the Interface class even contains the\ntraining data.\nOn 25 May 2018 at 09:33, Viktor Gal notifications@github.com wrote:\n\n@vigsterkr commented on this pull request.\nIn tests/unit/optimization/FirstOrderSAGCostFunctionInterface_unittest.cc\nhttps://github.com/shogun-toolbox/shogun/pull/4294#discussion_r190811707\n:\n\n\n*\n\n\nAuthors: Elfarouk\n\n\n/\n+\n+#include \n+#include \n+#include \"FirstOrderSAGCostFunctionInterface_unittest.h\"\n+#include \n+\n+using namespace shogun;\n+using Eigen::Matrix;\n+using Eigen::Dynamic;\n+using stan::math::var;\n+using std::function;\n+\n+/* This is a temporary fix. The variables are now variables\n\n\nwe need to figure out how we can store (if we want a none primitive typed\nstd::vector, like stan::math:var)... hence the temporary story... just add\na FIXME/TODO term there and then its good\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/shogun-toolbox/shogun/pull/4294#discussion_r190811707,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ABGrdqb2zbTIE2Z2t7vyIb87A8-3wD2lks5t17O6gaJpZM4UIEvl\n.\n. UNITTEST is good. It is about the FIRSTORDERSAG...\n\nOn Fri, May 25, 2018, 23:17 Elfarouk Yasser notifications@github.com\nwrote:\n\n@FaroukY commented on this pull request.\nIn tests/unit/optimization/FirstOrderSAGCostFunctionInterface_unittest.h\nhttps://github.com/shogun-toolbox/shogun/pull/4294#discussion_r191011586\n:\n\n@@ -0,0 +1,35 @@\n+/\n+ * This software is distributed under BSD 3-clause license (see LICENSE file).\n+ \n+ * Authors: Elfarouk\n+ */\n+\n+#include \n+#ifndef FIRSTORDERSAGCOSTFUNCTIONINTERFACE_UNITTEST_H\n\nNot sure to be honest. I looked at the other files and they all had\n_UNITTEST_H so I followed the convention. If there is no guideline I'll add\nmore _ between words\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/shogun-toolbox/shogun/pull/4294#discussion_r191011586,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ABGrdr1DEkwVl5kykUr3oKnV_b1tBTtpks5t2HT8gaJpZM4UIEvl\n.\n. No, the concept is much simpler :-) If you want to use std::function\nanywhere (header or implementation file), you just need to include the\nfunctional from the stl.\n\nOn Fri, May 25, 2018, 23:19 Elfarouk Yasser notifications@github.com\nwrote:\n\n@FaroukY commented on this pull request.\nIn src/shogun/optimization/FirstOrderSAGCostFunctionInterface.h\nhttps://github.com/shogun-toolbox/shogun/pull/4294#discussion_r191011757\n:\n\n+#ifndef FIRSTORDERSAGCOSTFUNCTIONINTERFACE_H\n+#define FIRSTORDERSAGCOSTFUNCTIONINTERFACE_H\n+\n+#include \n+#include \n+#include \n+#include \n+#include \n+#include \n+#include \n+#include \n+\n+using Eigen::Dynamic;\n+using Eigen::Matrix;\n+using stan::math::var;\n+using std::function;\n\nI thought that if you use using std::function; in a header, then you'd\nneed to access it from another file which has the include as\nFirstOrderSAGCostFunctionInterface::function no? In other words, shouldn't\nit be under the scope of the .h file?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/shogun-toolbox/shogun/pull/4294#discussion_r191011757,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ABGrdi0xavZtBk734HlIcclGP2jDPdQ8ks5t2HVEgaJpZM4UIEvl\n.\n. Making it a member attribute does not separate from the class, right?\n\nOn Fri, May 25, 2018, 23:37 Elfarouk Yasser notifications@github.com\nwrote:\n\n@FaroukY commented on this pull request.\nIn src/shogun/optimization/FirstOrderSAGCostFunctionInterface.h\nhttps://github.com/shogun-toolbox/shogun/pull/4294#discussion_r191014764\n:\n\n\n/* X is the training data in column major matrix format /\nSGMatrix* m_X;\n+\n/* y is the ground truth, or the correct prediction /\nSGMatrix* m_y;\n+\n/* trainable_parameters are the variables that are optimized for /\nMatrix* m_trainable_parameters;\n+\n/** cost_for_ith_point is the cost contributed by each point in the\n\n\ntraining data */\n\n\nMatrix, Dynamic, 1>* m_cost_for_ith_point;\n+\n/** total_cost is the total cost to be minimized, that in this case is a\n\n\nform of sum of cost_for_ith_point*/\n\n\nfunction)> m_total_cost;\n\n\n@iglesias https://github.com/iglesias Not silly at all! I actually\nthought about this, but I wanted to separate the logic of calculating the\ncost function from the class so that base class can support any cost\nfunction. We can also make a pure virtual method and let the user implement\nit. Both work.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/shogun-toolbox/shogun/pull/4294#discussion_r191014764,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ABGrdsejxOrMTsv2UnsowdjF5uPwrXcYks5t2HmIgaJpZM4UIEvl\n.\n. That makes sense of course, avoiding unnecessaey copies. But here nothing\nis passed, this is just the definition or declaration of the attributes of\na class.\n\nHaving these as pointers may make sense. It is of course not always the\nwrong choice. Although one should prefer using smart instead of naked\npointers.\nConceptually, whether an attribute should be a pointer, reference, or value\nis a question of ownership and life times.\nOn Fri, May 25, 2018, 23:38 Elfarouk Yasser notifications@github.com\nwrote:\n\n@FaroukY commented on this pull request.\nIn src/shogun/optimization/FirstOrderSAGCostFunctionInterface.h\nhttps://github.com/shogun-toolbox/shogun/pull/4294#discussion_r191015041\n:\n\n\n\n\nWARNING\n\n\n\n\nThis method returns\n\n\n\n\n\\f$ \\frac{\\sum_i^n{ \\frac{\\partial f_i(w) }{\\partial w} }}{n}\\f$\n\n\n*\n\n\nFor least squares, that is the value of\n\n\n\n\n\\f$ \\frac{\\frac{\\partial f(w) }{\\partial w}}{n} \\f$ given \\f$w\\f$ is\n\n\n\n\nknown\n\n\n\n\nwhere \\f$f(w)=\\frac{ \\sum_i^n{ (y_i-w^t x_i)^2 } }{2}\\f$\n\n\n*\n\n\n@return average gradient of target variables\n\n\n*/\nvirtual SGVector get_average_gradient();\n+\nprotected:\n/* X is the training data in column major matrix format /\nSGMatrix* m_X;\n\n\n@iglesias https://github.com/iglesias Just a convention (I try to pass\nthings by reference/pointer instead of value to avoid copying). But I agree\nI will get rid of SGMatrix's pointers since it already implements that\nunder the hood.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/shogun-toolbox/shogun/pull/4294#discussion_r191015041,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ABGrdu-0BhPwSEU-J71nDFwBP-J0IJyxks5t2HnygaJpZM4UIEvl\n.\n. It sounds like you may have been writing mostly C and using a C++ compiler\n:-)\n\nFairly common practice unfortunately. There are more expressive and just\nplain better ways of using C++.\nOn Sat, May 26, 2018, 00:50 Elfarouk Yasser notifications@github.com\nwrote:\n\n@FaroukY commented on this pull request.\nIn src/shogun/optimization/FirstOrderSAGCostFunctionInterface.cpp\nhttps://github.com/shogun-toolbox/shogun/pull/4294#discussion_r191025127\n:\n\n+/\n+ * This software is distributed under BSD 3-clause license (see LICENSE file).\n+ \n+ * Authors: Elfarouk\n+ /\n+\n+#include \n+#include \n+using namespace shogun;\n+using stan::math::var;\n+using std::function;\n+using Eigen::Matrix;\n+\n+FirstOrderSAGCostFunctionInterface::FirstOrderSAGCostFunctionInterface(\n+    SGMatrix X, SGMatrix y,\n+    Matrix trainable_parameters,\n\nI am not sure what is the convention in Shogun. But I always used pointers\nin my previous C++ coding (Just to avoid the syntax of reference). If you'd\nlike, I think we can change it to reference here.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/shogun-toolbox/shogun/pull/4294#discussion_r191025127,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ABGrdpx8eIDup6vB1-E0XcGLRGnDJBHgks5t2Iq6gaJpZM4UIEvl\n.\n. Probably not under zero, unless there's a nasty bug corrupting the memory.\nI see tht way of being over-careful about writing the error message a form\nof defensive programming.\n\nOn Sat, May 26, 2018, 00:51 Elfarouk Yasser notifications@github.com\nwrote:\n\n@FaroukY commented on this pull request.\nIn src/shogun/optimization/FirstOrderSAGCostFunctionInterface.cpp\nhttps://github.com/shogun-toolbox/shogun/pull/4294#discussion_r191025301\n:\n\n+\n+bool FirstOrderSAGCostFunctionInterface::next_sample()\n+{\n+ auto num_of_samples = get_sample_size();\n+ if (m_index_of_sample >= num_of_samples)\n+     return false;\n+ ++m_index_of_sample;\n+ return true;\n+}\n+\n+SGVector FirstOrderSAGCostFunctionInterface::get_gradient()\n+{\n+ int32_t num_of_variables = m_trainable_parameters->rows();\n+ REQUIRE(\n+     num_of_variables > 0,\n+     \"Number of training parameters must be greater than 0\");\n\nSure, but in this case, I'll just print: \"Number of training parameters\nmust be greater than 0, you provided 0\".... since you can't have\nnum_of_variables<0\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/shogun-toolbox/shogun/pull/4294#discussion_r191025301,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ABGrdoCL2i48HnB6FGTUE2boWNpmeUf8ks5t2IsCgaJpZM4UIEvl\n.\n. Hi @FaroukY! That's all right. Though I find that a ~2000 lines patch for a so-called MVP has a bit of over-kill smell.\n\nStill, I do suggest you to keep in mind the comments that have already been made. Just think that you don't want to build upon something that will need to be completely changed from its foundations. I am not necessarily saying that is the case here, but I believe you should keep it mind.. @FaroukY, still waiting here for you to get back to the review.\nOn 29 May 2018 at 22:20, Elfarouk Yasser notifications@github.com wrote:\n\n@vigsterkr https://github.com/vigsterkr Travis is green :)\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/shogun-toolbox/shogun/pull/4309#issuecomment-392930093,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ABGrdvr9wu3ObylGP03ZrOdMg0qfHhAiks5t3a2fgaJpZM4UOwrB\n.\n. Can RealMatrix distance_matrix_aa = d.get_distance_matrix() be ported to something like d.get('distance_matrix')? @karlnapf . Implementing the IterativeMachine concept with a mix-in seems like a reasonable idea. Nice job!. What I was wondering was why a user needs to use two different classes to\nuse a pipeline: e.g. the pipeline_builder to build/construct a pipeline\nobject, and then this pipeline object to do all the preprocessing,\ncross-validation, etc, etc, she wants.\n\nOf course I also understand your point. It is not clean if a class\nimplements many different concepts altogether. It gets confusing quickly,\nclasses get bloated, etc.\nIt is just a trade-off I think.\nTo clarify :-) These were not comments critizing any design choice.\nCompletely far from that! I asked to gain understanding.\nOn Fri, 27 Jul 2018 at 14:40, Viktor Gal notifications@github.com wrote:\n\nYou need the builder for building, but u would like to query a trained\npipeline for various stages of it as well. Or what do you mean?\n\nOn 27 Jul 2018, at 14:35, Fernando J. Iglesias Garc\u00eda \nnotifications@github.com wrote:\n@iglesias commented on this pull request.\nIn examples/meta/src/evaluation/cross_validation_pipeline.sg:\n\n+File f_labels_test =\ncsv_file(\"../../data/classifier_binary_2d_linear_labels_test.dat\")\n+\n+#![create_features]\n+Features feats_train = features(f_feats_train)\n+Features feats_test = features(f_feats_test)\n+Labels labels_train = labels(f_labels_train)\n+Labels labels_test = labels(f_labels_test)\n+#![create_features]\n+\n+#![create_pipeline\n+Transformer subMean = transformer(\"PruneVarSubMean\")\n+Machine svm = machine(\"LibLinear\")\n+\n+PipelineBuilder builder = pipeline()\n+builder.over(subMean)\n+Pipeline pipeline = builder.then(svm)\nI understand that there might be mutability reason justifying two types.\nWhy both part of the interface though?\n\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub, or mute the thread.\n\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/shogun-toolbox/shogun/pull/4380#issuecomment-408406930,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ABGrdgioF75DNVdSzkqZ7SjMgFAtjxv8ks5uKwoygaJpZM4VYFzE\n.\n. @karlnapf all right. I did not check the implementation of the equations 8-D. Great stuff!\n\nOn Wed, Jul 25, 2018, 19:40 Heiko Strathmann notifications@github.com\nwrote:\n\n@karlnapf commented on this pull request.\nIn src/shogun/regression/LinearRidgeRegression.cpp\nhttps://github.com/shogun-toolbox/shogun/pull/4384#discussion_r205200379\n:\n> -   linalg::matrix_prod(feats_matrix, lab, y);\n\nauto decomposition = linalg::cholesky_factor(kernel_matrix);\ny = linalg::cholesky_solver(decomposition, y);\n\nset_w(y);\n\nauto x_bar = linalg::colwise_sum(feats_matrix);\nlinalg::scale(x_bar, x_bar, 1.0 / ((float64_t)x_bar.size()));\nauto intercept = linalg::mean(lab) - linalg::dot(lab, x_bar);\nset_bias(intercept);\nauto feats = data->as>();\nauto y = regression_labels(m_labels)->get_labels();\nauto N = feats->get_num_vectors();\nauto D = feats->get_num_features();\n+\nSGVector w;\n\nYou are right! the expression for w was wrong, it assumed no bias. I am\nfixing it right now\n\u2014\nYou are receiving this because your review was requested.\nReply to this email directly, view it on GitHub\nhttps://github.com/shogun-toolbox/shogun/pull/4384#discussion_r205200379,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ABGrdgiQjyaOykYlntcul1WicNKaDNABks5uKK2qgaJpZM4VbIUW\n.\n. Even nicer exercise for gsocers ;-)\n\nIt would fit very nicely in a Jupyter nb.\nOn Thu, Jul 26, 2018, 20:38 Heiko Strathmann notifications@github.com\nwrote:\n\n@karlnapf commented on this pull request.\nIn src/shogun/regression/LinearRidgeRegression.cpp\nhttps://github.com/shogun-toolbox/shogun/pull/4384#discussion_r205561545\n:\n\n\nSGVector x_bar;\nT y_bar;\nif (m_use_bias)\n{\nx_bar = feats->mean();\ny_bar = linalg::mean(y);\n}\n+\nSGVector w;\nif (N >= D)\n{\nSGMatrix cov = feats->cov();\nlinalg::add_ridge(cov, m_tau);\nauto L = linalg::cholesky_factor(cov);\n+\nif (m_use_bias)\n\n\n@iglesias https://github.com/iglesias voila! :)\nIf you could do the math and confirm this, that would be neat! :)\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/shogun-toolbox/shogun/pull/4384#pullrequestreview-140857872,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ABGrdiQQ3jdP9ty22kixpgbJes1K-kX5ks5uKgyMgaJpZM4VbIUW\n.\n. Of course it is good, no strong feelings :-)\n\nI see the DenseFeatures::dot is a template already, and I am guessing the\ncall to linalg it does is to another template as well. What's your idea?\nOn Sat, Aug 4, 2018, 16:07 Heiko Strathmann notifications@github.com\nwrote:\n\n@karlnapf commented on this pull request.\nIn src/shogun/features/DenseFeatures.h\nhttps://github.com/shogun-toolbox/shogun/pull/4384#discussion_r207709447\n:\n\n+\n+ / Computes the empirical mean of all feature vectors\n+  * @return Mean of all feature vectors\n+  */\n+ SGVector mean() const;\n+\n+ / Computes the \\f$DxD\\f$ (uncentered, un-normalized) covariance matrix\n+  \n+  \\f[\n+  * X X^\\top\n+  * \\f]\n+  \n+  * where \\f$X\\f$ is the \\f$DxN\\f$ dimensional feature matrix with \\f$N\\f$\n+  * feature vectors of dimension \\f$D\\f$.\n+  /\n+ SGMatrix cov() const;\n\nWell it computes the covariance, just for a certain case of input.\nIt is also not called covariance_for_column_major_matrix ;)\nIf you feel strong, I can change it, otherwise I would just outsource mean\ncomputations into different code..\nOn another note, I wanna mimic the dot API of DotFeatures, but just\ntemplated. What are your thoughts on that?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/shogun-toolbox/shogun/pull/4384#discussion_r207709447,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ABGrdn5p_oUKHR9cEowA-E37dcZehphOks5uNaqHgaJpZM4VbIUW\n.\n. Sure, that sounds good. At the very least, to start experiment.\n\nOn Sun, 5 Aug 2018 at 14:30, Heiko Strathmann notifications@github.com\nwrote:\n\nWe have virtual float64_t CDotFeatures::dot=0, which is a quite cool\ndesign, as independently of any feature type, we can write code and\nalgorithms that work with these dot product features.\nThe problem, however, is the return type. Think about linear regression\nand cov, and 32bit features. We could of course write an iterative cov\nimplementation that fills a 32bit cov matrix using the above API and\nelementwise casting. We cannot, however, do a batch version without copying\nthe matrix. That is the first issue. The second issue is that the API is\nnot const, due to the on-the-fly features that are computed in a non\nconst inplace way.\nSo maybe we can build a second version of the API (keeping the old\nintact), that serves a similar purpose, but that is both const and\ntemplated. Thoughts?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/shogun-toolbox/shogun/pull/4384#issuecomment-410517083,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ABGrdgwOJKKOcrlNMKeSSuQfc9g4mi3Dks5uNuVPgaJpZM4VbIUW\n.\n. This thing of the include order is scary :-O\n\nAre we missing guarding some header?\nOn Tue, Aug 7, 2018, 17:14 Wuwei Lin notifications@github.com wrote:\n\n@vinx13 commented on this pull request.\nIn src/shogun/features/DenseFeatures.h\nhttps://github.com/shogun-toolbox/shogun/pull/4384#discussion_r208272163\n:\n\n#include \n+#include \n+#include \n\nthis can be moved to .cpp file so that the inclusion order problem can be\nfixed\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/shogun-toolbox/shogun/pull/4384#pullrequestreview-144046785,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ABGrdiyVTH469eYEPBUqJEACCKdoj0mvks5uOa7pgaJpZM4VbIUW\n.\n. Nice work @saatvikshah1994, thank you.\n\nI was wondering, would you like to experiment with concepts to implement what you have done here with traits and sfinae in sgvector and sgmatrix? If you're interested we can discuss further what would be the possible advantages and gains. This could be done in a separate branch and discussed in another pull request.. Great to know!\nLet me start an issue soon and I will mention you there. What I have in\nmind is still a work in progress idea ;-)\nIn the meantime, are you familiar with concepts already? Otherwise I can\nrecommend a video or two from latest cppcon to get you started.\nOn Sat, 5 Jan 2019 at 08:58, Saatvik Shah notifications@github.com wrote:\n\nNice work @saatvikshah1994 https://github.com/saatvikshah1994, thank\nyou.\nI was wondering, would you like to experiment with concepts to implement\nwhat you have done here with traits and sfinae in sgvector and sgmatrix? If\nyou're interested we can discuss further what would be the possible\nadvantages and gains. This could be done in a separate branch and discussed\nin another pull request.\nSure, I'm game! Do you want me to create an issue and we can discuss\nthere? Or on IRC first?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/shogun-toolbox/shogun/pull/4437#issuecomment-451636268,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ABGrdnei18yDc4nanGwoULbgnmrGB-lNks5vAFs5gaJpZM4ZU5h7\n.\n. Starters to get introduced to the main ideas:\nhttps://www.youtube.com/watch?v=HddFGPTAmtU\nhttps://en.wikipedia.org/wiki/Concepts_(C%2B%2B)\n\nOn Tue, 8 Jan 2019 at 19:30, Saatvik Shah notifications@github.com wrote:\n\nNot much no so would surely appreciate any starter tips.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/shogun-toolbox/shogun/pull/4437#issuecomment-452403255,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ABGrdr32PGKHTZl7Y6dndwY0SWC5o-tJks5vBOPYgaJpZM4ZU5h7\n.\n. > For SGVector<float64>: taking a vector of N size as denoted below. Seems like the gains of SIMD start showing only after 1024 elements.\nFor SGMatrix<float64>: taking a matrix N x N of the N size denoted below.\nI'm not very familiar with the algorithm used by dSFMT, but seems like tput proportionally improves as the number of elements increase until saturation at ~64K elements.\nIn the final cases there is always a drop - my guess is due to first going L1->L2/L3->Memory seemingly.\n```\nCPU Caches:\n  L1 Data 32K (x4)\n  L1 Instruction 32K (x4)\n  L2 Unified 262K (x4)\n  L3 Unified 6291K (x1)\n\nBenchmark                                Time           CPU Iterations\nBM_SGVectorSimpleForRng/256          10686 ns       4824 ns     146526   50.6128MB/s\nBM_SGVectorSimpleForRng/1024         31256 ns      19961 ns      33535   48.9237MB/s\nBM_SGVectorSimpleForRng/4096        103190 ns      80179 ns       8864    48.719MB/s\nBM_SGVectorSimpleForRng/16384       476489 ns     332658 ns       2109   46.9702MB/s\nBM_SGVectorSimpleForRng/65536      1731183 ns    1341493 ns        533   46.5899MB/s\nBM_SGVectorSimpleForRng/262144     7684643 ns    5338349 ns        129    46.831MB/s\nBM_SGVectorSimpleForRng/1048576   28698992 ns   21948063 ns         32   45.5621MB/s\nBM_SGVectorSimpleForRng/4194304  124266202 ns   91083250 ns          8   43.9159MB/s\nBM_SGVectorSimpleForRng/8388608  183002067 ns  149759250 ns          4   53.4191MB/s\nBM_SGVectorSimdRng/256               28129 ns      23222 ns      29822   10.5134MB/s\nBM_SGVectorSimdRng/1024              22501 ns      21000 ns      33497   46.5021MB/s\nBM_SGVectorSimdRng/4096              37701 ns      30281 ns      25527       129MB/s\nBM_SGVectorSimdRng/16384             61909 ns      51976 ns      13567   300.621MB/s\nBM_SGVectorSimdRng/65536            137449 ns     125798 ns       5051   496.827MB/s\nBM_SGVectorSimdRng/262144           520684 ns     453210 ns       1561   551.621MB/s\nBM_SGVectorSimdRng/1048576         1951931 ns    1801798 ns        410   555.001MB/s\nBM_SGVectorSimdRng/4194304         8058928 ns    7512247 ns         93   532.464MB/s\nBM_SGVectorSimdRng/8388608        15801137 ns   15328644 ns         45   521.899MB/s\nBM_SGMatrixSimpleForRng/64           70855 ns      64603 ns      10725   60.4652MB/s\nBM_SGMatrixSimpleForRng/128         255030 ns     249715 ns       2717   62.5713MB/s\nBM_SGMatrixSimpleForRng/256        1022532 ns     996520 ns        712   62.7183MB/s\nBM_SGMatrixSimpleForRng/512        4473191 ns    4014367 ns        177   62.2763MB/s\nBM_SGMatrixSimpleForRng/1024      19003502 ns   16312333 ns         45   61.3033MB/s\nBM_SGMatrixSimpleForRng/2048      67421134 ns   64075364 ns         11   62.4265MB/s\nBM_SGMatrixSimpleForRng/4096     309963506 ns  303301000 ns          2   52.7529MB/s\nBM_SGMatrixSimpleForRng/8192    1427790785 ns 1366948000 ns          1   46.8196MB/s\nBM_SGMatrixSimdRng/64                22065 ns      21745 ns      31635    179.64MB/s\nBM_SGMatrixSimdRng/128               38247 ns      37770 ns      18664   413.689MB/s\nBM_SGMatrixSimdRng/256              108321 ns     106161 ns       6338   588.728MB/s\nBM_SGMatrixSimdRng/512              378871 ns     369052 ns       1817   677.412MB/s\nBM_SGMatrixSimdRng/1024            1519232 ns    1493987 ns        473    669.35MB/s\nBM_SGMatrixSimdRng/2048            6198822 ns    6065946 ns        129   659.419MB/s\nBM_SGMatrixSimdRng/4096           31814041 ns   31293111 ns         18   511.295MB/s\nBM_SGMatrixSimdRng/8192          280005706 ns  274623000 ns          2   233.047MB/s\n```\n\nVery nice analysis and results! Nice to see the simd versions achieve up to about 10-fold improvement.\nJust to be sure, the results take into account random variation, yes? That is, the code is run many times for each Simple/Simd and size scenario.\nAlso, can you relabel or clarify the names of the columns? :-) Actually just clarifying the difference between the two time columns should be enough. I am guessing the last column should just be something like 8 bytes times N (for SGVector, N^2 for Matrix) divided by the time in one of the other two columns.. I didn\u2019t see an explanation there about the two results. Maybe I missed\nsomething.\nAnyhow, it is not important.\nOn Sat, 12 Jan 2019 at 11:36, Saatvik Shah notifications@github.com wrote:\n\n@iglesias https://github.com/iglesias the column explanations can be\nfound here: google/benchmark#397 (comment)\nhttps://github.com/google/benchmark/issues/397#issuecomment-305906871\nAnd yes, theres a fix needed in the bmark - I should be multiplying by\nsizeof(float64_t) in the SetBytesProcessed call everywhere.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/shogun-toolbox/shogun/pull/4437#issuecomment-453737102,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ABGrdkfFt8dUOlvk4Jqut4cnWZaKbf1jks5vCbqzgaJpZM4ZU5h7\n.\n. The two time* results.\n\nOn Mon, 14 Jan 2019 at 13:21, Fernando J. Iglesias Garc\u00eda \nfernando.iglesiasg@gmail.com wrote:\n\nI didn\u2019t see an explanation there about the two results. Maybe I missed\nsomething.\nAnyhow, it is not important.\nOn Sat, 12 Jan 2019 at 11:36, Saatvik Shah notifications@github.com\nwrote:\n\n@iglesias https://github.com/iglesias the column explanations can be\nfound here: google/benchmark#397 (comment)\nhttps://github.com/google/benchmark/issues/397#issuecomment-305906871\nAnd yes, theres a fix needed in the bmark - I should be multiplying by\nsizeof(float64_t) in the SetBytesProcessed call everywhere.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/shogun-toolbox/shogun/pull/4437#issuecomment-453737102,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ABGrdkfFt8dUOlvk4Jqut4cnWZaKbf1jks5vCbqzgaJpZM4ZU5h7\n.\n\n\n. About the seed, I think that was already solved in the latest commit in the PR: 6a97846. See also comments above from @saatvikshah1994 saying it was done.. @vigsterkr @karlnapf please guys have a look at the commit 6a97846. I think that one is addressing the seed concern.. Sure, we can check. Please leave the name of the lib or a link to it. Thank youuu :-). All right, it was abseil, sure.\n\nI suggest that at this moment we don't worry too much about this aspect(1) as this is really an investigation for us to start getting our hands dirty playing with concepts.\n(1) C++ standard support for concepts and the version of it used in Shogun.. Any preference to organize merging this and the other PR that triggered this experiment, @saatvikshah1994? I am guessing it may make sense to merge the other one first and then rebase the changes here onto it. Still it is fine by me to merge this one directly to the feature branch.. All CI tasks good. @saatvikshah1994, I can't merge from here due to the conflicts marked above. Would you mind doing the manual job to fix the conflicts? I will merge afterwards.. It seems like a reasonable fix.\nI don't have all the context though, this was introduced in d1571c4e80060ce5e753caf361889e285d1e9bc0, @vinx13?. Moved. Let me know if everything is all right now.\n. Added\n. What I don't see very clear then is what should be S? The covariance matrix of lhs or of rhs or something else? I think that for (x-y)' S^-1 (x-y) to be valid x and y must be from the same distribution.\n. Ok! Right now S is just the covariance of lhs, it is exactly the case (x-mu)' S^-1 (x-mu) in \nhttp://en.wikipedia.org/wiki/Mahalanobis_distance\nbeing mu and S the mean and covariance of lhs and x is a point of rhs. I will change it then \nso S and mu are computed over both lhs and rhs if lhs != rhs. In case lhs == rhs, should it\nbe like it is right now?\n. New private statement for members and all the members to private.\n. oh! I just proved that this code was taken from somewhere else\n. I didn't manage to get to work correctly the time functions in shogun so I have \nused these ones taken from the standard instead\n. So what about these parameters k and q, do they make sense \nas design parameters to tune with model selection?\n. If we want to include the thing with the parameter m_q for cover tree we\nshould compute the distances from the query point to the neighbors \nfound since JL's cover tree doesn't give them ordered by distance.\nDoes it pay off? My idea would be to do this if m_q != 1.\n. I modified barely nothing from the original implementation. Let\nme know if you want to see some of the things changed (e.g\nint -> int32_t, float -> float64_t, etc)\n. I remember S\u00f6ren said that for performance it is better to solve for a bunch of k values in a run. Still, could that be included in model selection?\n. ok, sorry about that. Is it ok this time or should I separate it in and do new \npull requests?\n. Fixed\n. sorry, stupid mistake :(\n. How is it that this needs to be included now but now before?\n. It would look better with the same indentation. There also other cases\nbelow in this file\n. nicely commented this file :+1: \n. See the line above, \"/** constructor\", it seems that it should be one tab to the left.\nIn line 67 for example, \"virtual CMulticlassLabels* ....\", I think that one is also\nmisaligned.\nI just quickly checked QDA.h, I didn't find these sort of wrong indentations. Anyway,\nin case there were any, they would be wrong :)\nIt is not that this is super-relevant either TBH, it just looks better well aligned.\n. IIRC there was still no DataGenerator class when I did the QDA example.\nIt would be great if you can update both of them.\n. Mmm I see. I haven't used the DataGenerator class myself either, but it makes sense that there should\nbe a way to do this.\n@karlnapf, any suggestion to generate this type of data using DataGenerator?\n. It is not very important here since this loop is likely not to have many iterations,\nbut in general it should be better to avoid loops. Something like\ncov = np.random.normal(size=(p,p))\nshould work fine here.\n. For the sake of avoiding more loops, you may construct the graph like:\np = sic.shape[0]\nX, Y = np.meshgrid(range(p), range(p))\ngraph = np.array((X[sic != 0], Y[sic != 0])).T\nI have checked that it works using your code but maybe I misunderstood\nsomething.\n. Thanks for your comments! What is exactly the part that would break the integration\ntests?\n. Is there a reason why complex64 is sometimes with small letters\nand sometimes with capitals in the test names?\n. This one is the one I meant in capitals\n. I am not sure if it makes sense to have CFeatures* CSOMachine::get_features()\nif there is already const CFeatures* CSOMachine::get_features() const. To me it\nlooks weird to keep both, and at first sight I prefer\nconst CFeatures* CSOMachine::get_features() const.\nWhy should we keep both?\n. As Patric has mentioned, remove these variables better please.\n. Yes, you are right, CTwoStateModel::simulate_two_state_data is now simulate_data\nand CStructuredLabels does not have any more obtain_from_generic as this is handled\nby LabelsFactory.\nIf you don't want to update it, I will do it once this PR gets merged.\n. @hushell, if there is no big reason to keep the non-const one, let's just keep the const version.\nLet me know if you come up with an scenario where the non-const function would be needed.\n. @hushell, sure, no problem\n. Why are the features being removed from the LinearSOMachine? If I understood it correctly,\nthe idea is that they will eventually be removed from the StructuredModel, so they will have\nto be in the LinearSOMachine.\n@hushell, @ppletscher, what do you think?\n. I'd rather see the methods implemented in the cpp better than in the header. Two reasons for this:\n1) Faster compilation time.\n2) Say we want to modify one of these methods. If they are in the header then it would take quite\nlong to re-compile, even if we only do a tiny little change. To modify a header forces re-compilation of\neverything.\n. I am not so sure that they should be in the StructuredOutputMachine anyway. See for instance the Machine,\nLinearMachine, and KernelMachine case. The LinearMachine has a pointer to the features, but that is not\nthe case of the KernelMachine, which gets the features through pointer to the kernel (i.e. the kernel has pointers\nto the features). Therefore, I think it makes sense that the StructuredOutputMachine does not contain the features\nand they are kept in the LinearStructuredOutputMachine.\n. I am not sure I understand the last point. It needs not to be always NULL though, in the constructor where\nthe StructuredModel is passed, you just do the pointer to the features in the LinearStructuredOutputMachine\nequal to the pointer to the features in the StructuredModel.\n. Done. I also introduced a choose class for multiple k method, similar to choose class,\nto avoid a little bit more of code duplication.\n. What does this TODO mean to say? A longer explanation?\n. Could you add maybe add a little bit of documentation for the class members above? To avoid\ndoc warnings at least.\nEDIT: Not only here, in general.\n. I am actually unsure if the code after SG_ERROR gets to be executed, @hushell did you test it?\n. Should the m_data_source be SG_REF'ed too? Also SG_UNREF'ed in the\ndestructor?\n. I think m_factor_type should be SG_REF'ed here.\n. A side note on style, I would go for\nreturn m_factor_type;\nwithout the brackets, so it matches better the rest of Shogun's code.\nAnyway, I guess this is a minor thing so if you prefer it like this, go ahead.\n. Yes, exactly after the get there should be then an UNREF once the factor_type is no longer\nneeded. Say that you have something like\nCFactor* factor = new CFactor(...);\nCFactorType* factorType = factor->get_factor_type();\nSG_UNREF(factor); // This does internally SG_UNREF(m_factory_type)\n// here we would have factorType whose memory has been freed if the get does not\n// increment the reference count of the CFactorType*\n. Yes, SG_REF also here.\n. Can this be removed? Dead code is normally not good.\n. CFactorGraphLabels I guess :)\n.  FACTORGRAPH_LABELS_H \n. No, we have to call it. We don't have automatic memory management with SGObject in general,\nonly with SGReferencedData such as SGVector, SGMatrix, etc.\n. I thought that \ninclude shogun/lib/config.h\nhad to appear before this. It compiles like this however, so I guess it is fine.\n. matrix that best diagonalizes? Maybe I am wrong though.\n. is this memory freed somewhere later?\n. All right, SGNDArray takes care of it later.\n. @tklein23 brought up a few days ago in this issue #1268 that reference counting fails\nwith objects from the stack and this seemed to be the normal behaviour. I don't understand\nthen why it is fine to use it here with this variable of type CFactorGraph.\n. Minor note on style, we separate attributes from methods, even if they have the same\nvisibility. In this case there should be two private blocks, the one on top with the method.\n. Either leave it not commented if it makes sense to have it there or remove it please.\n. Same here about methods/attributes separation.\n. Can you use SGString instead? Otherwise the methods that take an std::string\nas an argument won't work in target interfaces since there are no typemaps for\nstd::string but there should be for SGString.\nIn general the rule is that it is ok to use STL stuff in implementation files but\nnot in the headers, and if you really need to use it in a header then create an\nimplementation class that won't be visible to SWIG.\n. destructor?\n. Thoralf just clarified this for me on IRC, it should be fine to use the CFactorGraph object\nin the stack as you do here.\n. There is no real need to do SG_REF after new, but it is fine if you like to do it that way.\n. No need for IGNORE_IN_CLASSLIST, we just don't that class include in src/interfaces/modular/*\nand it won't be visible for target interfaces.\n. Why not to use SG_DEBUG (or SG_SDEBUG from non CSGObject classes)\ninstead?\n. Less whitespace after the =.\n. Use float32_t or float64_t better to follow Shogun's style.\n. In any case I notice that most of the stuff you are printing are Eigen structures. These\nare indeed more comfortable to print with cout, so for me it is fine to keep it in this way.\n. From SG_DEBUG and its friend is nice that you don't need to re-compile Shogun in case\nyou want to run with and without DEBUG output.\n. Could you also document these attributes please? I think doxygen will complain\notherwise.\n. Using SG_DEBUG: https://github.com/shogun-toolbox/shogun/blob/develop/src/shogun/metric/LMNN.cpp\n. Maybe there is something in Eigen to return a string that represents a matrix/vector.\nThen you could do something like:\nSG_DEBUG(\"Eigen matrix: %s\\n\", eigen_matrix.to_string());\nDo you know anything like that?\n. I can live with it :)\n. @pickle27, could you check if this method can be used from target interfaces (e.g. python_modular)?\ngsomix has told me there are typemaps for SGNDArray but I think the use of & would make this method\nnot usable from them.\n. Nice that you found they were these missing SG_UNREFs what were causing the leaks :)\n. If these delete should not be used, let's better remove them.\n. Extra semicolon here? I personally prefer much more having the methods\nimplemented in cpp files, I think overall compilation is faster. But it\ncan be also a matter of taste, so as you prefer.\n. Let's use int32_t and float64_t instead.\n. Extra braces.\n. REQUIRE(iter < itermax, \"Convergence not reached\\n\")\nmight be more concise\n. extra braces.\n. extra braces the innermost loop.\n. What does this comment mean? :)\n. Maybe these ones can be directly declared in the loops where they are used.\nAgain it is probably just a matter of taste so as you prefer eventually; I rather\nuse C++ style/techniques than C ones.\n. Extra semicolon.\n. braces\n. braces\n. braces\n. I am not sure how large can this T get, but I would rather\ndo p.reserve(T) before the loop and then push_back or\np.resize(T) and then p[i] = 1.0/T.\n. Actually since all the elements to introduce are equal,\nstd::vector p(T,1.0/T);\nshould work good.\n. Again, it might be a good idea to allocate this vector.\n. braces\n. braces\n. REQUIRE might look neater. \n. braces\n. braces\n. braces\n. Why the cast here? Isn't fortran_order_data an int32_t* already for its declaration?\n. @sonney2k, the test crashes in this line. If I understood correctly, get_distance_matrix should output a matrix equivalent to calling CDistance->distance() for every possible pair of indices.\n. Let us use SG_PRINT / SG_SPRINT instead.\n. Please, remove DEBUG info by default.\n. Let us try to minimize code in header files.\n. What do you think about separating MAPInference and MAPInferImpl into different files? I think it is a bit weird to put together the interface of MAPInference with the one of MAPInferImpl, since as an implementation class we would like to hide its details.\n. Why are these UNREF commented? Just leave them out if they are not used.\n. Same here.\n. Get rid of all thse braces for one-liners please.\n. Isn't #include  enough? If I am not wrong, compilation flags are set for C++11.\n. It sounds good, let's get rid of these additions if they are not used. \n. Didn't it work with unordered_map? :)\n. It is cool to check also for these, but maybe it can be neater using EXPECT_EQUAL or something similar\n. Ah ok, they are already below, so let's just remove these commented lines.\n. Please keep the external includes just in the cpp files.\n. Maybe it is better to move this class in its own file to make the code more readable.\n. element x and element y are?\n. if it has a circle / if there is a circle?\n. Nicely handled!\n. Since the default constructor does not initialize all its members, can there be problems with CDisjointSet similar to the ones found yesterday with the clone method in another class?\n. Can we be sure that as long as x is in [0,m_num_elements-1] then this recursion will end?\n. Does m_parent denote to which set in the disjoint set each element belongs to?\n. Please add some some unit test for this method covering several cases as it can be a bit dangerous if it the recursion goes on forever.\n. We can skip the braces for this one-liner if.\n. Does is_connected denote if all the elements in the disjoint set belong to the same subset?\n. Did you test it already? :)\n. I am not sure if this might give some problems with the reference counting. Anyway, doesn't this method look similar to what CSGObject::clone() does?\n. Please get rid of commented code if we are not going to use it.\n. What is it exactly a circle in a graph? A loop (an edge that connects a vertex to itself)?\n. Can you write a trivial sentence for doxygen? I guess that otherwise compilation with doxygen would generate a warning (maybe not as it is overriding the method in CSGObject).\n. Document these two as well please.\n. What if the features given are NULL? Is that handled somewhere?\n. It is cool to do Eigen::, and I like it better than using namespace blah blah. Can you choose any of the ways and use it everywhere instead of mixing both? \n. It looks like something happened with the indentation. It should be fine anyway.\n. All right, as you prefer.\n. Sure, make such a directory inside structure if you want. I just think that\nit is confusing to have two different classes that are non trivial in the\nsame header.\nIf you are only going to use it inside this FactorGraph another option is\nto make a private class nested in FactorGraph. In the header you would just\nforward declare this class and then you define and implement it in the cpp.\nOn 6 Aug 2013 20:45, \"Shell Hu\" notifications@github.com wrote:\n\nIn src/shogun/structure/FactorGraph.h:\n\n@@ -19,7 +21,107 @@\nnamespace shogun {\n-/* @brief Class CFactorGraph a factor graph is a structured input in general\n+/* @brief Class CDisjointSet data structure for linking graph nodes\n- * It's easy to identify connected graph, acyclic graph, roots of forest etc.\n- * please refer to http://en.wikipedia.org/wiki/Disjoint-set_data_structure\n- /\n  +class CDisjointSet : public CSGObject\n\nI have thought about this, but it only be used in CFactorGraph. Maybe we\nshould create a \"pgm\" folder, and put all graphical model related classes\nin, each with a single file.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/shogun-toolbox/shogun/pull/1369/files#r5611737\n.\n. And what about rank? What does it denote?\n. I meant if element x and element y are, not is :)\nOn 6 Aug 2013 20:37, \"Shell Hu\" notifications@github.com wrote:\nIn src/shogun/structure/FactorGraph.h:\n\n\n* @param xroot root of the set containing x\n* @param yroot root of the set containing y\n* @return new root\n*/\nint32_t link_set(int32_t xroot, int32_t yroot);\n  +\n/** link the roots of two sets containing x and y respectively\n* and return if they were linked\n*\n* @param x element x\n* @param y element y\n* @return if x and y were in the same set\n*/\nbool union_set(int32_t x, int32_t y);\n  +\n/** if element x and element y is in the same set\n\n\nshould be better explained :)\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/shogun-toolbox/shogun/pull/1369/files#r5611783\n.\n. If it is to denote the existence of an edge whose destination and origin\nvertices are the same, call it loop better please.\nOn 6 Aug 2013 20:36, \"Shell Hu\" notifications@github.com wrote:\nIn src/shogun/structure/FactorGraph.h:\n\nCDynamicObjectArray* m_datasources;\n+\n-   /* disjoint set /\n-   CDisjointSet* m_dset;\n  +\n-   /* if has circle in the graph /\n\nYes, a loop. Because 2 nodes in a factor graph means there is an edge\nconnecting them, if there is another path between them, certainly has a\nloop in the graph.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/shogun-toolbox/shogun/pull/1369/files#r5611961\n.\n. I agree with you. The SGVector has a T* member that could turn out\nproblematic if not initialized.\nOn 6 Aug 2013 20:26, \"Shell Hu\" notifications@github.com wrote:\nIn src/shogun/structure/FactorGraph.cpp:\n\n@@ -12,17 +12,156 @@\nusing namespace shogun;\n+CDisjointSet::CDisjointSet()\n-   : m_num_elements(-1)\n  +{\n-   SG_UNSTABLE(\"CDisjointSet::CDisjointSet()\", \"\\n\");\n  +\n-   register_parameters();\n\nLuckily, no problem for this one, since no pointers here, but it's better\nto initialize SGVector as well.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/shogun-toolbox/shogun/pull/1369/files#r5612037\n.\n. Is this something the user of CFactorGraph needs to know or is it something internal of the implementation?\n. Is this class supposed to be further specialized? Just wondering if this should be private.\n. I think there is a misunderstanding here :)\nI wondered why you decided to make the attributes protected, is it because you plan to inherit from this class in other classes?\n. Then let's write something about it in the doxygen of the header. Ideally, a user of a class should be able to use it without reading the implementation.\n. Let's remove this newline here.\n. Can we get a little bit more of description here? :) Since this is a public class it should be important for the user to understand what it does.\n. What about the opaque pointer? Shouldn't it be initialized to NULL or something as well?\n. It should be fine to do directly inference_str == \"TreeMaxProduct\" and the same with the rest, if you prefer.\n. If I remember correctly in the other classes you didn't initialize the members in register_parameters. Either there was an init function, or you initialized them in the constructors. Let's try to keep this consistent.\n. Is the CMapInferenceImpl member created somewhere?\n. I think you should SG_UNREF before this.\n. So how does this work exactly? I see that you call inference but the result is just stored in the energy member. I mean, the outputs do not seem to depend at all on the result of the inference, which looks a bit weird to me.\n. What do you plan to implement in CMAPInferImpl? Put it another way, why does CMapInfer need an implementation class?\n. Decide then a place and initialize everything in there.\n. My fault, I didn't realize before that the SGVector assignment can be changed when calling m_infer_impl->inference(assignment), I only considered it as an input parameter.\n. Aham! I think I get it. So what you plan to do is to have only one CMAPInference class, where the user selects in the constructor which type she wants to use (e.g. TreeMaxProduct). Then, the different methods to perform inference will be in the CMapInferImpl. Is that right?\n. Let's go with enums then. Thanks @karlnapf !\n. Why is m_fg not referenced and unreferenced in this CMapInference?\n. Can this be visible to SWIG interfaces even if it is just a function?\n. If you know more or less what is going on in the algorithm you have implemented below, document the steps with comments.\n. Use float64_t instead of double please.\n. float64_t\n. float64_t\n. That is expected behaviour. Remember #1268.\nSince FactorGraph is an SGObject, please use SG_REF and SG_UNREF and avoid using them from stack.\n. tr1/undordered_map was used when it hash tables where not yet included in the C++ standard. They are now, one just have to compile for C++11 support, and we do, so just include unordered_map, without tr1.\n. Sorry for not mentioning it before, but use REQUIRE better than if + SG_ERROR for these ones.\n. Is there any difference?\n. If everything is public, make an struct better. It also seems more consistent taking into account that you made the GraphNode an struct. \n. If you just don't write the classes in the corresponding .i files (Structure.i, Structure_includes.i) they won't be taken into account by SWIG. So no real need to do this I believe.\n. Does it make sense to have one example for tree graph and another for grid graph? Maybe the name of the example could reflect that as well.\n. I guess we can remove this include and the using namespace std since you are using SG_SPRINT below.\n. Leave the for (...) in one line please.\n. Let's remove braces for one-liners.\n. Is mi->second maybe more readable?\n. Please change this to a more informative REQUIRE.\n. for (...) in one line is better read.\n. for (...) in one line.\n. Are you using anything from iostream and anything from std without using std::? \n. @return instead of @param?\n. Write\n@return object name\nor similar to avoid doxygen warning please.\n. Let's write a more informative message with REQUIRE, as (I believe) this is a public method than can be called by a user.\n. Just remembered @karlnapf mentioned the parameters should not be named with the m_ in SG_ADD.\n. Then I guess the SequenceLabels will have to be moved under the labels directory as well.\n. If I understood Heiko correctly it is a good practice to avoid using the initialization lists. So these initializations here should be made in the init method...\n. ... and these ones here inside the constructor, after the call to init.\n. Is there a semantic difference between the init and register_parameters methods? It seems to me that both do the same but they are called in a different way depending on the class.\n. @karlnapf, please correct me if I am wrong :)\n. Yes please, change register_params() to init in your classes.\n. So then I think m_num_states initialize should be initialized somewhere in this method.\n. m_parent and and m_rank should be maybe initialized here as well?\n. True. Sorry, my mistake.\n. As we mentioned, let's rename this to init for the next time.\n. Please add here a couple of words for Doxygen's sake, something like:\n\n@ param features features to embed\n. The same for Doxyen.\n. param mixing_matrix blah blah blah\n. What is BUILD_DASHBOARD_REPORTS=ON doing?\n. Remove  commented code that is not used please.\n. I think as should be removed.\n. param fg to avoid doxygen warning.\n. Write a description for the return value too please.\n. Write a more informative REQUIRE please.\n. No nedd for the if, since you already ensure they are different from NULL in the line above.\n. REQUIRE.\n. REQUIRE is a more compact version for if+SG_ERROR.\n. I would rather make the cast safer using static_cast or dynamic_cast.\n. REQUIRE.\n. static_cast is safer maybe?\n. Let's try to keep the style consistent. Above there were no spaces for the template.\n. Same here.\n. REQUIRE.\n. REQUIRE.\n. REQUIRE.\n. Mmm I wonder why this was not causing a memory leak before.\n. CStructuredModel::get_features() does\nSG_REF(m_features)\nHow is it related to DynamicObjectArray?\n. m_features is CFeatures* I think.\n. Can we get a short description of it?\n. whether to whiten the data would do\n. Let's use REQUIRE instead with a short message since this method is public.\n. What is the source for this reference (and the other one below)? The paper by Hyvarinen et. al. in the header?\n. Should this be unref-ed?\n. Same here, just asking though.\n. haha nice! Keep it coming, I will do my best to stay alert :)\n. Does the method work well even if the unary factors are nor augmented?\n. No need for the backslash\n. Hmm, I think SG_DEBUG is actually more suitable than having a boolean member to control verbosity. Why do you prefer it with the boolean?\n. Same here\n. And here\n. Remove it please if it is not going to used.\n. Comment the prints please so the example runs clean, but we keep them for illustration.\n. Same here for the print.\n. and here\n. typo\n. I think it is better to push the notebook without output. They will be executed and rendered somewhere else. To remove the output do Cell > All Output > Clear.\n. I understand. However, I think this might be not a very nice solution. Think that we want to do this for each class. Then we would have every class with its own verboser member, etc -- not nice. What I have normally done when found myself in this situation is redirect the output to a file and then just grep the lines I am interested in.\n@karlnapf, @sonney2k, @lisitsyn, @vigsterkr, what do you think?\n. This might be a good idea. Anyway, in the meantime, we should stick to SG_DEBUG :)\n. Is something from std io used?\n. I'd rather keep the method in the labels class. Use get_labels + get_num_labels if so.\n. Minor style thing. If you want to keep all of these members public, let's remove the m_. Also, I would use an struct if all the members are going to be public. \n. Ok, where were you using fscanf? If you need to load data to features and labels, use CSVFile or similar.\n. Where would you do that? I know it is a pain to do SG_UNREF to get the number of labels (you don't need to do SG_REF though), but the other solution, to have small methods like these all over the place is not nice either. Like this, the classes get cluttered fast and it is difficult to keep track what the main job of each class is.\n. All right! Then std io is actually used in the file :)\n. Using LabelsFactory even better :)\n. Factorized representation or simply factorization maybe sounds better than factorization representation?\n. According to different factorizations (missing s :)\nMarkovian property is held\n. minor typo: chracter -> character\n. Can you mention here that this user provided loss function is typically the Hamming loss? I think it gives a better idea to the reader thinking about it in terms of something more particular like the Hamming loss than just \\delta.\n. Also, can we use capital instead of small delta? Just in order to follow the convention used in Doxygen.\n. Why does the +1 come into play? Does it represent the absence of a letter? Hmm, I guess it does not since it is since it is not included in the other factor types\n. Fix indentation here please\n. and here ;)\n. are using -> are used.\n. dualality -> duality ;D\n. statble -> stable\n. we don't need the gap becomes zero -> we don't need that the gap becomes zero / we don't need the gap to become zero\n. Nice that you wrote the instruction to install it. Would you mind making the import inside a try block and write in the except block print pip install ...? In case it is attempted to be run in a machine where it is not install, a nicer message than the error would be printed.\n. wanderful -> wonderful\n. Well, I still don't really agree with it. About efficiency, calling SG_REF and SG_UNREF compared to the rest of the stuff you do in the class is negligible. get_labels and set_labels are just the standard getter and setter. Recall also that in some conversation we had a the beginning, Patrick suggested that the labels and the features should not be inside the model, but just in the machine.\nThat said, keep it this way if you prefer. It is not a so important detail.\n. Minor thing: What about calling this variable features? Instances seems a bit too generic to me.\n. vc[0] = 2?\nJust to be consistent to how it is done above.\n. I have to look further into this change. @hushell, can you explain a little bit about it please?\n. Why keeping an individual random seed for this class?\n. IMHO, it sounds dangerous to modify the BMRM code in this way. Also, here we are doing it just so that the objectives among different solvers are comparable, so the price might not pay off the risk.\nIt sounds a very good idea to mail Michal about this, he is the bundle methods expert :)\n. If the random methods you are using are from CMath, would it be enough with using the seed of that class?\n. Let's make this more robust checking that labs->get_num_labels() is larger than zero before making the division. Also, it seems even more reasonable to me to do this in train_machine, since the same StochasticSOSVM can be trained several times with different training data (thus, with different number of training examples). \n. Yes, if all the random methods you are using are in CMath, then you should not need the m_rand_seed here and just use the seed in CMath, initializing it as you have mentioned. \n. Is that solved if you generate a different random number several times (in particular, one per gradient descent iteration)?\nAlso, what local minima do you refer to? If I am not wrong, the SO-SVM optimization is a QP, which is convex. The non-convex part, the inference or argmax, is solved apart in the structured model.\n. I guess only one of the two above is needed.\n. Let's keep the example running cleanly to follow the convention, with this line commented it should be fine.\n. I am still unsure about this. Why not controlling it through MSG_DEBUG or MSG_INFO?\n. @sonney2k, the idea was to make this cell of markdown type so that links (e.g. GitHub account) could be included. The abstract should still be easy to extract with a parser as it is just the second cell of markdown type, instead of the first one. Anyway, let me check tonight if there is a way to include links in heading cells (didn't really try it).\n. All right, I will update soon tonight.\n. make the example by default without output so it finishes cleanly as the other examples.\n. just realized it is a comment, so ignore what I just said :)\n. We are trying to follow the convention that the first markdown cell should be used for the abstract that will be rendered automatically in the website together with a link to the notebook. Can this cell be some type of heading then maybe?\n. Test error?\n. Minor typo: straight forward -> straightforward\n. Minor typo: allipse -> ellipse\n. Just add new commits to the branch in the pull request (your kmeans branch in this case).\nAn alternative, in order to avoid many commits for the same task is to add new changes to\nsame commit using git commit --amend. Note that in the latter case you will probably need\nto force the push to your branch in the GitHub repo.\n. fix indentation\n. remove the trailing space before the closing parenthesis please, just to be consistent with the rest of the style\n. The white space right after centers_i\n. It might be the configuration of the white space characters in your editor, maybe for some reason it inserted a tab (\\t) instead of spaces here or something similar. Have a look at the file enabling the option to make white space characters visible in your editor.\n. AFAIK with ctest it is possible to run all the tests, but not one in particular. From the mailing list about ctest in Shogun: http://permalink.gmane.org/gmane.comp.ai.machine-learning.shogun/3824\nTo run a test in particular what I do is to use the binary shogun-unit-test inside build/tests/unit together with the option --gtest_filter.\n. Please remove commented lines that are not used.\n. parsed -> parses\n. Minor on style, let us char* instead of char . Same for the rest of the file with pointers.\n. Minor on style again, open brace in new line.\n. Is there something weird going on with the indentation here and below?\n. No brace for one-liner.\n. Can we get error messages instead of ASSERT?\n. add the @return line in the method documentation for Doxygen please and also use in this line /* instead of /* at the beginning.\n. Could you explicitly set a default value for use_kmpp inside the init method? In this way we make sure there will be no problem at all due to uninitialised memory.\n. Could you use SGVector instead please?\n. So I understand you are using these features to populate them with the cluster centers and in the end returning them with the SGMatrix. What about using just an SGMatrix directly? It seems cleaner.\n. Please declare these variables where they are actually used.\n. And basically the same as the comment above for these tree variables as well.\n. Note on style, try to be consistent using spaces or not in assignment. Personally, I prefer spaces but better alway check the style that has been used in the file you are modifying.\n. I'd rather use a new variable with another name for the lhs sum here.\n. Neat! I like how you solved this part.\n. Break each of these two lines into two lines please, they are too long.\n. Use Written (W) 2014 Parijat Mazumdar instead please.\n. So we do not normally write the doc for the examples with the example itself, but in another directory (see examples/documented). However, I think this rule doesn't apply to graphical examples, so for me it is fine to keep it here.\n. I think this should happen when train is called actually. To me it makes sense that it should be possible to do in Python something like:\nkmeans = KMeans(...)\nkmeans.set_use_kmeanspp(False)\nkmeans.train() # this time the cluster centers are not initialized using k-means++\nkmeans.set_use_kmeanspp(True)\nkmeans.train() # this second time k-means++ is used\n. See the comments below. I'd like to see that it is possible to do the same using just one KMeans object.\n. Double check indentation, at first sight this looks too long.\n. Yes, because another constructor may be used.\nOn 14 Jan 2014 10:08, \"mazumdarparijat\" notifications@github.com wrote:\n\nIn src/shogun/clustering/KMeans.cpp:\n\n@@ -30,12 +30,13 @@\n    init();\n }\n-CKMeans::CKMeans(int32_t k_, CDistance* d)\n+CKMeans::CKMeans(int32_t k_, CDistance* d, bool use_kmpp))\n : CDistanceMachine()\n {\n    init();\n\nI have passed on default false to use_kmpp in the .h file . Is it still\nrequired to initialize in init() ?\nThere is error in Travis build here because of an extra ')' which might\nhave got added during my final touches. I have fixed it.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/shogun-toolbox/shogun/pull/1814/files#r8853561\n.\n. Here you are not using either SGVector or SGMatrix, but float64_t*.\nOn 14 Jan 2014 10:20, \"mazumdarparijat\" notifications@github.com wrote:\nIn src/shogun/clustering/KMeans.cpp:\n\n@@ -454,6 +458,65 @@ void CKMeans::store_model_features()\n    SG_UNREF(rhs);\n }\n+SGMatrix CKMeans::kmeanspp()\n+{\n-   int32_t cent_id, mu_1, mu_next;\n-   int32_t count=0;\n-   float64_t dist_temp;\n-   float64_t sum;\n  +\n-   CDenseFeatures* muspp=new CDenseFeatures(0);\n-   CDenseFeatures* lhs=(CDenseFeatures)distance->replace_lhs(muspp);\n-   int32_t num=lhs->get_num_vectors();\n-   int32_t dim=lhs->get_num_features();\n-   muspp->set_feature_matrix(SGMatrix(dim,k,false));\n- \n-   float64_t dists = SG_CALLOC(float64_t,num);\n\nSir, Actually my initial version used SGVector mainly because my earlier\ncommit of set_initial_centers() returned SGVector. But while rebasing I\nfigured that KMeans has be refactored (commit by sonney2k) during which\nSGVectors had been changed to SGMatrix. So I finally changed to SGMatrix.\nLets discuss this.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/shogun-toolbox/shogun/pull/1814/files#r8853802\n.\n. The features are already in the distance passed to KMeans on creation.\n\nJust keep track of the indices of the feature vectors that are selected as\ncluster centres.\nOn 14 Jan 2014 11:21, \"mazumdarparijat\" notifications@github.com wrote:\n\nIn src/shogun/clustering/KMeans.cpp:\n\n@@ -454,6 +458,65 @@ void CKMeans::store_model_features()\n    SG_UNREF(rhs);\n }\n+SGMatrix CKMeans::kmeanspp()\n+{\n-   int32_t cent_id, mu_1, mu_next;\n-   int32_t count=0;\n-   float64_t dist_temp;\n-   float64_t sum;\n  +\n-   CDenseFeatures* muspp=new CDenseFeatures(0);\n\nSir, I am actually trying to use the distance function to calculate\ndistance. Please see line 487. I guess this will not be possible without\ncreating features.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/shogun-toolbox/shogun/pull/1814/files#r8855267\n.\n. Fix indentation here please.\n. Could you also make this a const method please.\n. Fix indentation for the two lines above please.\n. Minor on style, no braces needed for one-liner blocks.\n. Did you check possible memory leaks with valgrind? If you do get_lhs the ref count should be incremented, thus you should use SG_UNREF when not making use of lhs.\n. Enable a option to see non-printable characters and you will notice the\ndifference.\n\nOn 15 January 2014 10:54, mazumdarparijat notifications@github.com wrote:\n\nIn src/shogun/clustering/KMeans.cpp:\n\n@@ -224,6 +225,10 @@ bool CKMeans::train_machine(CFeatures* data)\nASSERT(XSize>0 && dimensions>0);\n-   ///if kmeans++ to be used\n-        if (use_kmeanspp)\n-                mus_initial=kmeanspp();\n\nactually in my editor indentation looks correct at first glance! but\nactually there are spaces before these 2 lines and tabs before all other\nlines. Thats why it looks different here. Let me fix it.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/shogun-toolbox/shogun/pull/1814/files#r8890713\n.\n. SGVector and SGMatrix are automatically reference counted and memory-freed. Thus, no need for these two SG_FREE. The SG_UNREF was the only one needed ;)\n. There are some Python integration tests that fail. But as you said, unrelated to this PR.\n. Why not using init_shogun_with_defaults instead?\n. data.display_matrix(\"rectangle_coordinates\") looks more natural to me.\n. Fix indentation please.\n. Same as above regarding the use of display_matrix.\n. I guess the shogun:: is not necessary since you included the namespace.\n. Indentation.\n. Same comment about display.\n. Please check in other files how enums are defined and follow the convention.\n. Why protected? I thought in the end there was not going to be inheritance here. Also, we normally write public methods, private methods, then attributes, etc in that order inside the classes.\n. Maybe this constructor is not really necessary/practical?\n. set rather than switch\n. get rather than display and the @return is missing.\n. Make the method const please.\n. const method\n. It seems that this was already like this before but it would be great if you could use SGVector instead of the float64_t* and probably the same for C1List or an appropriate data structure.\n. If we are not planning to have inheritance here, then make everything private.\n. Doc?\n. Does the l in lKMeans stand for Lloyd? I would make it more explicit then.\n\n@sonney2k, anything against having functions out of any class? How will SWIG behave with it?\n. Documentation\n. Use REQUIRE with appropriate error messages instead of ASSERT. It seems that there were already others in this class, would be great if you fix them as well.\n. The same before, I would make the name more explicit at least once somewhere.\n. Use : CDistanceMachine() instead please, note the whitespace.\n. Same as above.\n. I would rather give actual well-defined values for these ones.\n. Why did this method change actually? My brain is already off ;)\n. All right\n. Please, be consistent with the whitespaces when you assign.\n. Remove newlines here please, one is enough.\n. Use CDenseFeatures::obtain_from_generic or a safer type of casting please.\n. Follow the convention for the enum please. See how it is done in other files.\nFor instance you could\nenum EKMeansMethod\n{\n    KMM_MINI_BATCH,\n    KMM_LLOYD\n};\n. Use a better name please. Maybe CKMeansLloydImpl. Use the Impl because we want to denote that this is an implementation class whose interface we don't really want to expose. \n. Please pick a better name, e.g. CKMeansMiniBatchImpl.\n. Typo: obsevations -> observations. Also, each observation (singular) or the observations.\n. The notebooks are included in the repository without output. In the iPython web session there is an option to clear all output.\n. Typo: Now that we have (remove the).\n. Typo: its -> it is or it's. Also check out for some \"lets\" instead of let's.\n. be initialized -> are initialized.\n. initial -> initialize.\n. There is something weird with the indentation in the last sentence.\n. typo: you're gave -> you have\n. I understand this quick start is better targeted on using Shogun as an external library rather than extending Shogun. Thus, I'd say that the the release build type and no testing are more suitable.\n. Arrgh! Nothing wrong here, but we should really rename all the so_* examples to structure_* to be consistent.\n. I guess one of them was supposed to be OFF :-) Also, there is a missing white space before or.\n. missing whitespace after features before (i.e.\n. remove the comma after since\n. separate \"atleast\"\n. and whitespace again after PCA and the start of the parenthesis :)\n. whitespace after features\n. Why is plotting in 3D not possible? I am not saying you need to make the plot though :)\n. typo: campare -> compare\n. minor on style, use a line break here please.\n. and here another one\n. I think this result is interesting. Could you try to argue here a little bit why does this (maybe counter-intuitive) behaviour happen? \n. Apologize if I am missing something obvious, but I don't quite get why commenting the abs method?\n. Is there any difference between these two below in order to compute the absolute value?\nelse if (a>0)\n      return a;\nelse\n      return -a;\nand\nelse if (a<0)\n      return -a;\nelse\n      return a;\n. I actually like the idea of the default auto that chooses depending on (D > N) ? But I didn't know about 'eig' being less accurate. So I am fine with either way as long as the user can choose the method to use.\n. And keeping on with the style... :-) One-liner blocks are ok without braces, except an if-else where one of the branches has more than one lines and the other just one (then braces in both).\nAlso, line breaks after a block are welcome.\nBTW, wouldn't this series of if (m_mode...) fit better with in a switch?\n. Write a white space after the dash please (I know it sounds stupid but for the sake of consistency).\n. White space after the dash.\n. According to this link http://gcc.gnu.org/onlinedocs/cpp/Defined.html, #if defined is equivalent to #ifdef, so I am not sure why compilation failed for Pang. Maybe missing parentheses for #if defined?\n. Awesome that you checked it without eigen3. Great job!\n. I guess this should be LARS instead of LAR? Also, you could just use lasso_ or lars_ at the beginning of the name of the test.\n. Write another private here please to separate methods from attributes.\n. This line should go out as well once std::vector is no longer used.\n. I think the usual style is with white spaces before and after the colon.\n. Without new line between if and else.\n. Here the braces are fine because the else has two lines ;)\n. I'd say same as above. If another branch has more than one lines, then all get the braces.\n. Regarding this class as well as CTreeMachine being templated. Will these classes be visible from SWIG interfaces? If yes, then I think we should be careful how we handle it and use the same pattern we have for other templated classes in Shogun like DenseFeatures. If not, then it is fine to do it this way but I would mark it somehow in the name of the class; e.g. TreeMachineImpl or so.\nAny thoughs on this @vigsterkr, @karlnapf?\n. I agree with Viktor, left and right subtree stuff makes no sense once we want it to be generic (i.e. N children per node where N can be larger than 2).\n. I 'm just being annoying here, but I think we can use lines of more than ~20 characters :-)\nIt will probably come sooner or later, but I guess none is coding in his/her phone yet :-P\n. Mmm I may be completely wrong here, but shouldn't m_root be unref-ed first?\n. And here the reference incremented?\n. SG_REF(m_root)\nI am probably misunderstanding something here. Why  there is no reference count management of m_root in most of the methods in this class?\n. I am wondering, is it possible to have these tree classes not visible via modular interfaces without sacrificing the flexibility of the decision trees?\n. Let me clear my thoughts :)\nSo the idea is that the decision trees  (ID3, C4.5, etc) use TreeMachine internally or are derived from TreeMachine, right?\nI was wondering if the fact of not being able to choose the template type in TreeMachine/TreeMachineNode somehow (e.g. as we currently do with feature classes) from the modular interfaces may limit the usability of the decision trees.\n. minor grammar thing: a pointer to its parent and a vector\n. get_parent.\n. I think you should still do SG_UNREF and SG_REF of the previous and new vector of children, respectively.\n. So why not just decrementing the reference to the old children vector and then assigning the m_children pointer to the children argument?\n. The reference to m_children should be incremented.\n. There is a newline not needed here.\n. Use children_vector or just children instead please (we don't normally camelize around here, although it's cool too -- I've got nothing against it).\n. Once get_children increments the reference count, remember that it should be decremented again due to this call.\n. See CDynamicObjectArray::get_element(int32_t), there's a call to SG_REF there.\n. Let's do as @vigsterkr says @mazumdarparijat. I am very sorry for the confusion.\n. I remove all the comments related to this get set renaming I suggested.\n. I think there is a typo in numpe\n. This part should be probably done with Shogun as well, see io/CSVFile\n. minor, annoying comment - white space before the brace\n. So, regarding style. What criteria do you actually use to break lines? These two lines look very weird to me. The second one is longer, the data type and the variable name are separated in two different lines.\nPersonally, in header files I put the declaration of the method in one line, as then you can get all the information using grep. You don't need to follow that rule though :-)\n. minor typo, lables should be labels.\n. These applies to the other methods as well, not just this one.\n. No new line needed here.\n. I would rather see the implementation of the methods in the same order as the declarations in the header. But take this as a suggestion, I don't think this is a real rule we are following.\n. You could also use SGVector::range_fill() to do this. Again, just a suggestion ;-) Keep it this way if you prefer.\n. Let's remove the braces for the one-liner.\n. Brace in the new line please.\n. No braces needed here.\n. I don't really follow the 80 character rule, IMHO it is a bit outdated taking into account the size of computer screens nowadays. I tend to use about 100 characters per line but it is not a hard limit, i.e. I have to write, say, 5-10 characters over 100 I do that instead of using a new line.\nBut this is something personal. Not a strict rule followed here.\nWhat I find weird here however is to break a long line into two lines and make the second one (notably) longer than the first one. In addition, writing a line break between the type and the name of a variable is odd in my opinion.\n. Anyway, just pick your style and be consistent. And if you don't do these two things I mentioned above then that would make me slightly happier. Otherwise, I will just live with it :-) It is not a big deal after all.\n. I believe that a different base in the log only changes the \"information unit\" of the entropy. If you are computing the entropy of a binary random variable, then it is probably more common to use the base 2 in the log.\nAnyway, as you have just mentioned, since we are finally making a decision based on a comparison between entropy values and not an absolute value, it should not really make a difference in this case.\n. Typo: shogun/exapmles should be shogun/examples.\n. By calling a git command. Also, please use Markdown syntax to link to the README_data.md.\n. What does \"you can make test of the examples\" actually mean? Maybe simply \"you can test the examples\" is more clear.\n. No problem! Typos pop up for everyone and everywhere :-)\n. Add please a dummy doxygen description (e.g. \"destructor\") to avoid an undocumented warning in Doxygen.\n. Please, write documentation for the methods.\n. I am guessing this is unfinished?\n. White space error at the beginning of the line.\n. Separate methods from attributes please.\n. We don't normally use the underscore in this fashion, like indist_. Neither for arguments, nor for attributes. Use m_dist for the attribute and just dist for the arguments.\n. What does pt stand for here? Point?\n. Align bool with template please.\n. I am unsure about the use of free_feature_vector here. Could you please argument why do we need it? Note that SGVector and SGMatrix have internal automatic reference counting, and it is normally not necessary to free their memory by hand.\n. Have a look to the other classes, we normally use another style for the name of enums. Something like EKNNMode and then KNN_BruteForce or so. Choose another one of your own :)\n. No need for white spaces in the empty line.\n. I would like a switch better than if-else for this.\n. Please the three variables above where they are used. So below float64_t* query = f->get_feature....\nDon't worry about performance for this, the compiler should take care of such things for you.\n. Please use SGVector instead for the two arrays above.\n. The way in which the line breaks are done above looks a little bit funny from here, but I guess I can live with that :-)\n. I am wondering, do we really need this one to be a pointer? If it doesn't need to, I'd say to just put a variable in the stack. We save us from using the delete below.\n. I think that you should check the value of dofree before doing this. Please check the Doxygen doc of get_feature_vector.\n. Use CDenseFeatures::obtain_from_generic(CFeatures* const) for this please. It's got a nice check and that way we reduce the number of explicit casts we do in the code.\n. Same as below here too, use CDenseFeatures::obtain_from_generic(CFeatures* const) instead please.\n. I am probably a bit rusty already and what I am going to say may be wrong, but isn't &query[0] the same as just query?\n. Just for the sake on continuing being annoying, if (... instead of if(....\n. Is this really worth having? Keep it if you want, just wondering..\n. Can you please say why is the comparison with lhs now instead of rhs?\n. Same as above about the declaration of the variables.\n. Same as above about using SGVector.\n. See the comment above about using variable in the stack instead.\n. Comment above regarding the use of query instead of &query[0].\n. See above concerning the check of dofree.\n. Hm all right, then please generate the doxygen documentation locally and try it out before pushing the code.\n. Then use point please, to be consistent with the name you have used in the method above.\n. You could pass the vector attribute of SGVector but that may mess around with the memory if Shogun thinks that there is no reference on it and nanoflann is still using it internally, so keep it this way.\n. Could you check this issue with the documentation yet?\n. My mistake, it is right to use it.\n. Can you argue why it is a good idea that the KDE class inherits from CKNN?\nA note on style also, for almost all the classes we include the prefix C (standing for class), so this should be CKDE\n. Then I would say to call point pt in the method above, just for consistency. But keep it like this if you want.\n. That sounds reasonable to me, thank you!\n. Please put the case statements at the same indentation level as switch.\n. and the break inside the case block\n. here too, correct indentation level\n. and break inside\n. indentation\n. and inside\n. no need for braces in the blocks inside a case\n. this one can go out out\n. no need for braces\n. this brace too\n. this one can go out too\n. and finally this one as well!\n. Use the new BSD license please, the same one you used for nanoflann_shogun.hpp. And I wrote no code here so no need to put my name :)\n. Aaah ok I understand now why my name was written, this is basically a copy-paste of the libshogun example :)\nSince I wrote that file it is all right to make the license change above since I approved it.\n. We can avoid loads of code duplication in these tests since you are pretty much doing the same thing in the three of them and there's only one line that changes. Create please a function in this file that makes at least all the data generation and then it is called from each test.\n. Hmm.. what sort of tests are these? The output is never checked...\n. Please read other unit tests so you get an idea what they are.\n. While you make this function, use default arguments or something else different to the defines I used above to define the number of classes, dimension, etc.\n. Note that since this is a unit test, we might want to fix the random seed so we know the same data is always generated and outliers won't make it fail.\n. The license is still missing.\n. This should probably be a SGObject.\n. Regarding style, we use new lines before opening braces.\n. The documentation of the methods is missing, we also use line breaks between methods, etc.\nSee other classes, preferably those that have been created or refactored recently and follow the style.\n. I am not sure I understand why the compute method would return an instance of CKDE.\n. If it is not used, then it should be removed.\n. The same, don't leave dead code.\n. something weird with the indentation here, as well as in the two lines below.\n. License missing.\n. Well, not necessarily dense features but the KDE class probably needs some features attribute. This is by the way where the data is stored for the class to use it.\n. This should be a CKernel probably.\n. I suggest to write either large scale or large-scale instead of largescale.\n. \"The CLibSVM class\" better, maybe?\n. I'd say \"training and test data are\" instead of \"training and testing data is\".\n. \"its initialization and training\", no \"it's initialization\", please.\n. I'd say \"and using the KMeans++ algorithm\", so remove that from.\n. I see you use capitals a bit randomly. Why Classical and Real dataset?\n. I would say classical approach instead of just classical.\n. I think that the last sentence is really strange. Could you rephrase it?\n. missing \"the\" before evaluation, I think.\n. See the capitalization of Support Vector machines.\n. a semi-formal introduction, no introductions. Also, see for the white space before the full stop.\n. instead of audio signals or an audio signal.\n. Also, I'd write BSS audio notebook instead of bss_audio.\n. introduces to, not into.\n. apply to, not over.\n. To overcome the limitations of KNN using the Euclidean distance. It is not a limitation of metric learning. In fact, metric learning is used to improve the performance of KNN using a distance learned from data instead of the common Euclidean distance.\n. the USPS\n. Further, the effect ...\n. I'd say a comparison with.\n. Maybe semi-formal is better.\n. Its, no it's. Twice\n. Link to SSVMs in wikipedia instead of SVMs.\n. Brief outline of factor graphs (in plural) is better maybe.\n. API instead of API's.\n. We are in the process of changing our license scheme from GPL to BSD. All new code should be BSD. Use the license in e.g. statistics/QuadraticTimeMMD.h\n. See same comment above regarding the license.\n. Include shogun/lib/config.h somewhere before ifndef KDE.h, for instance here.\n. Good! You wrote documentation for all the methods. However, note that the documentation should be in the right Doxygen format, with @param, @return, etc. \n. I would rather see names for the arguments.\n. typo in bandwidth\n. no need for private if there are no private members, but why do you prefer to keep the attributes above protected instead of private?\n. Put m_ at the beginning of the attributes. This should be m_points, the ones below, m_log_density, etc.\n. Write one attribute per line, because there should be Doxygen documentation for them as well. Something like,\n/** attribute documentation */\nattribute_type m_attribute_name;\n. Please, be consistent with the use of white spaces before and = in the assignments. Either you put them or not, but do the same all the time.\n. We can also remove all these verbose this->. m_num_rows=rows is enough.\n. It seems that the boolean worked is only used internally in the method. Thus, no need to talk about it in the API documentation (think about like you are providing an interface -- you need to explain what the method does and how people should use it, but you should not explain how the method does everything internally; those details can be written in comments within the implementation of the method in cpp file).\n. Use more modern style for variable declaration. The loop variable i should be declared in the for statement and I would rather see density declared closer to where it is actually used.\n. When is this memory freed? And there's no way this is compiling actually, pdf should be at least of time CGaussian* and probably the constructor called as CGaussian.\n. What cleanup method is being called here? I don't think it is defined anywhere.\n. Why is this a CRTP? Isn't a getter for a public member? Which in fact sounds sort of odd.\n. Please don't leave commented out code. If it is not used, just remove it.\n. The same here, why is it commented?\n. Put a newline afterwards please.\n. Use please the right syntax to write equations within the documentation. See for instance Gaussian.h.\n. This is not true any longer.\n. I still believe that it makes sense that CKDE inherits from CDistribution. At least, it should be an SGObject.\n. Something weird with the alignment of the .\n. The method description should be before the description of the parameters. See for instance Gaussian.h for this too.\n. Documentation for the return value is missing.\n. We normally use return instead of result, although according to the doc they are equivalent so this should be fine (I prefer consistency though :)\n. Just a minor remark, write the destructor after the constructors and also use proper doc which will generate Doxygen like /* Destructor */\n. That is obvious with the name of the attribute, what would be interesting is to say in very few words what the bandwidth is.\n. There should be no need for the two attributes above (m_num_rows and m_num_cols) because SGMatrix already contains these attributes (see the very end of the SGMatrix header file).\n. It makes sense to have a setter and a getter for this attribute, doesn't it? Currently, its value will always be equal to one.\n. For SGMatrix (and a couple of other two classes) we have automatic reference counting. Create this variable using \nm_points = SGMatrix<float64_t>(rows,cols,true); (without the call to new).\nRight now this way of doing it leaks memory in fact.\n. Where is the memory used by this variable freed?\n. += is handier in this case :)\n. and *= here.\n. Here you don't probably need rows and cols since the m_points matrix already contains this information.\n. Free this memory when you are no longer using it. Use SG_UNREF for this.\n. Corresponding SG_UNREF is missing.\n. SG_UNREF.\n. Why is the number of neighbours fixed to three?\n. Wrong indentation and below.\n. Wrong indentation, and below.\n. How does this test or KNN depend on lapack?\n. Use int32_t please.\n. Floats for indices in the loops?\n. int32_t for these guys.\n. Any comment about this @dhruv13J?\n. The license is missing. Also, where does this code come from? Have you used a reference implementation to draw inspiration from? An article? A tutorial on the internet?\n. I guess you can assign an integer to a float.\n. No worries!\n. Would it make more sense to rename the example file as well then?\n. I'd link the words \"decision tree learning\" (as the wiki article is called) in the second link. Very minor detail anyway, do it as you prefer.\n. typo: in different situations instead of is different situations\n. this sounds weird: the formation of these rules from training dataset. I'd say either from training data or from a training etc.\n. straightforward, one word.\n. add a comma between pre-defined and clearly distinguishable\n. typo: childeren\n. Neat!\n. \"The first step for that is [...]\". Remove the \"is\" after \"The first step\". \n. Where is the True outputted coming from? I'd rather not see it there. Very minor issue anyway.\n. It should be: \"do the predictions [...]\"\n. I guess you wanted to use although instead of through? Although or something similar.\n. This is the error rather than the accuracy.\n. Yeah, remove them. In a future pull request maybe you can actually do as Viktor has mentioned below and use the MSG_DEBUG and the log level instead of these DEBUG_KNN and BENCHMARK_KNN.\nIt should be a simple refactorisation. \n. Please correct sudoko, I guess it is a typo standing for sudoku.\n. use of simple image processing algorithms (you're missing the s).\n. using the K-Nearest Neighbors\n. divided into instead of in.\n. to complete this task it is necessary.\n. to identify.\n. This function returns (missing s).\n. These contours.\n. with the corners of the contours.\n. the sudoku puzzle has\n. Could you maybe say something in very few words about all the arguments passed to this function?\n. will map\n. maps a point\n. typo: into instead of intro\n. Use \\sum_{i=...}^... instead of \\Sigma.\n. The documentation should give a short description.\n. This is not right Doxygen documentation.\n. If you keep the pure virtual methods pure virtual in this class, then it won't be possible to make an instance of CKDE. How would it would possible to use KDE then?\nBy the way, I am wondering, have you tried testing your class?\n. It seems to me that what you are doing in compute_kde should be done in the train method. More or less.\n. I remember that Heiko told me once that the accented vowels were not showing correctly in his editor, so I stopped writing them in the files with my name. We will merge this for the time being but keep this in mind for the future. \n. @tklein23, why do we need SGIO.h (or some other headers below)? If it worked without them, what's the use of including them?\nI am not saying it is wrong, I just want to learn why :)\n. Isn't it actually better to keep this check? You could use the REQUIRE macro though.\n. Thanks for the explanation @tklein23, understood now!\n. Why is this method virtual? Do you plan to create classes that inherit from ID3?\n. Do you need an SG_UNREF(child) if you enter this if branch?\n. Now that I look at node = child and then there is SG_UNREF(node) it can be that everything is all right already!\n. We might even want to put some threshold with a small epsilon here. That is, even if we lose a tiny little bit of accuracy with the pruning, it might still be worth to do it (less nodes mean less decisions to make at test time and less prone to overfitting, I understand).\n. I agree, this should be possible to do with subsets and much nicer (we don't copy memory around and the code will be more readable).\n. and one more time, I agree :-) Just add a few comments here and there in a similar way as you have done in prune_tree_machine.\n. @karlnapf, actually, I don't think the goal of Doxygen documentation is to give a detailed description of how things are done in the methods, but to describe what they do; that is, useful information from the point of view of the user. Having said that, it is true that those details are useful for developers, and that probably most of the times developers are in fact the ones reading the doc. Just wanted two give my two cents.\n. But here the braces are in fact correct even according to our style. One of the branches of the if-then-else is not a one-liner.\n. There should be a newline here according to guidelines though :P\n. Between the brace closing the while and the return statement. In any case, since the return is only one line, I don't particularly think it is bad style either.\n. @tklein23, https://github.com/shogun-toolbox/shogun/blob/develop/doc/md/README_developer.md. Ctrl+f for \"one empty line\".\n. Leave out may.\n. So this method was only called from MultitaskLinearMachine.cpp? I had expected many more SG_UNREF once the SG_REF was included here.\n. We'll remove may in May then :-D\n. Stupid me, I just saw the other PR. Sorry about the bad joke..\n. Hmm, what does costom mean? :)\n. Please use either white spaces around the = operator or not, but do not mix it. Below you use one style, here another one..\n. Since you are doing these checks, it feels like a unit test would be a better fit. But let's see what @karlnapf thinks.\n. I actually don't see why this could cause before a memory leak which gets resolved using the code below instead. CTreeMahineNode::set_children(CDynamicObjectArray*) does not SG_REF its argument.\nI am not saying it is wrong, I just don't understand it. @mazumdarparijat, could you please motivate this change?\n. Ouch! I totally missed this at first. In general, try to avoid for loops in which you change the variable governing the loop inside. If you need to do that, use a while instead. Good that you refactored it though!\n. Ahm! I understand now. In fact, set_children is not really a setter but rather copies the elements in the given CDynamicObjectArray. I'd actually prefer if the method had another name (it is a bit misleading IMHO), or that it does what's normally expected from a setter. Anyway, Chiyuan had probably a good reason to make it in this way.\n. Use capital f in \"for more information\" as it is after a stop.\n. \"contains instances of NeuralLayer-based\" (missing of) \n. typo: \"and the compare\" should be \"and then compare\"\n. its parameters, instead of it's parameters.\n. @mazumdarparijat, if you are not sure about the design @karlnapf is suggesting, we can discuss it using a class diagram before going ahead with the code.\n. Since you already have to make other updates, it would be nice if the getters are const.\n. Any updates in this matter @armanform?\n. This is not necessary, please remove.\n. Use BSD license (see recently merged pull requests).\n. Doxygen documentation is missing in the methods.\n. Please leave a newline between methods.\n. Newline before opening brace\n. We don't normally use braces for one-liner blocks.\n. Is it necessary to use all this STL stuff? It gives complications with SWIG. There are probably classes already implemented in Shogun that you could use.\n. We don't use using in headers.\n. Use proper doxygen syntax for the API doc please.\n. We normally use underscores instead of \"camelising\".\n. What did you use as reference for your implementation?\n. Why protected? Are you thinking about some class hierarchy?\n. Can it be a problem the license change because of Octave's GPL license?\n. I believe you should be able to forward-declare SGVector instead, please try.\n. Now that I think about it, I am unsure since SGVector is templated.\n. I just tried it, it is possible to do it by forward declaring SGVector in the header as\ntemplate<class T> class SGVector<T>;\nand then of course including SGVector.h in the source file.\n. Write something here please. Even if you are planning to do something nice like the algorithm you did for ID3 but want to wait for doing that. At least write a few words and give a reference (to the ppt for instance if you like).\n. @ return is missing\n. no need for the\n*\n*/\nlines\n. same as above\n. We use something like that in the former configure script to check if libraries were present. I am not sure how are we currently handling this in cmake though. I thought this FIND_PACKAGE was taking care of this.\n. Use CDenseFeatures::obtain_from_generic(CFeatures*) or other sort of safe cast instead, please.\n. SGVector<T>::set_const(T).\n. Small typo, spcified should be specified.\n. set_const should be useful here as well (it just makes it look a tiny little bit nicer).\n. I don't think you need to include here MulticlassLabels.h and DenseFeatures.h.\n. I see you're using dynamic_cast below, so go for that one.\n. Use braces if you separate one line into two.\n. Will this do what is supposed to do if, after ordering, the last class in labels is most_label? Since you have if and else if, then I am afraid that you won't get into this block. I can be perfectly wrong, though.\n. SGMatrix with one row, why not using SGVector instead?\n. All right, because you want to to create DenseFeatures, it is all right then. Maybe write a short comment about that here, if you want.\n. Could you make a comment here that this threshold is used for non-categorical features only?\n. After training is finished, the training data in data will not be the same as before training because of this, right?\n. I understand you use this as a shorthand to avoid doing the long read (feats->get_feature_vector(i))[best_feature_index], isn't it? If so please write it in the comment (like shorthand for the features values of the best feature chosen, or so).\n. Is it possible to use SGVector<T>::unique(T*, int32_t) for this? That way we don't need to create best_feature_labels.\n. With opencv build I understand to compile opencv in a build bot. Is installing opencv through package manager an alternative solution? It should be faster and less error-prone.\n. Use SGVector<T>::clone() instead, please.\n. Is it possible that any of the elements in log_ratios is equal to zero? That is, can weight_count be zero?\n. Use one line for this comparison, it looks awkward.\n. What about this?\n. int32_t instead of int.\n. What about SG_ADD the m_mode attribute?\n. @yorkerlin, did you talk to @votjakovr about this? Have you gone through the code he implemented? What's your opinion? Is it just a copy-paste of octave's code or just a different implementation of the same algorithm?\n. Why did you remove this?\n. Please, don't use STL code in headers (it doesn't work well with SWIG). In the implementation files it is fine.\n. With one newline is enough.\n. Same here, no newline needed.\n. Since there is more than tree, the right way to do this is using an enum.\n@dhruv13J already solved this in #2089, you may take a look how it is done there. In fact, it would be good if you two guys synchronise somehow since you are both working on KNN. The ideal case would be if we can merge @dhruv13J's work soon.\n. No extra newline needed here.\n. @karlnapf, @lisitsyn, does this make sense to you? I don't know what this is for.\n. Being picky here, percentage of certainty\n. missing Doxygen for the new argument.\n. C45 instead of id3? In any case, prune decision tree should be just fine.\n. Can you mention in the doc something about in what cases pruning could not be successful?\n. I wonder if prune_tree_from_current_node is a more consistent name taking into account the naming you used for apply_multiclass and apply_multiclass_from_current_node. I understand you are using a similar design here.\n. Minor on style, white spaces between function arguments are missing.\n. I guess this check should be done the other way around? If current->data.attribute_id is -1, it can be bad to use it to index an array.\n. Use float32_t or float64_t, please.\n. Let's put a REQUIRE here for two children in nodes that are not nominal together with a nice error message.\n. As we just discussed at IRC, let's put a check at the beginning of this method for nodes that are leaves (i.e. with attribute_id == -1) and this method should return directly in that case. Then this second part of the && can be removed.\n. If either count_left == 0 or count_left == feature_matrix.num_cols, there would be something weird with the construction of the tree, wouldn't it? If I understand it correctly, that would mean that the threshold is either larger or smaller than all the attribute values of the data in the current node. Can that possibly happen?\n. What about using CMath::NOT_A_NUMBER instead for this? I guess CMath::MAX_REAL_NUMBER is still a valid feature value.\n. I think that an if block and do num_missing += 1 is more clear than this, but keep it as you prefer.\n. In fact, I think this method is always returning true, so the bool return value is not very useful. What about making it with no return value?\n. very minor thing, Gini with the first one in capitals when you are writing text (i.e. not code)\n. Minor things again, Gini and there's a typo in achieved.\n. Gini\n. Same thing\n. Why not MulticlassLabels for PT_MULTICLASS?\n. EProblemType has more than PT_REGRESSION and PT_MULTICLASS as possible values. Make this better a switch, deal with the specific problem types for which CART is defined (PT_MULTICLASS and PT_REGRESSION if I understand everything correctly), and make a default branch with an error to handle properly every possibility (even if the setter does not really allow to put other problem types, I don't know if that is the case though).\n. What is maxi supposed to contain? The index to the maximum value element  in the array? Then, I am not sure I get the logic here.\n. For regression, this might not be the best solution. Maybe you can to consider two floating point values for the same if their difference is very small. Could you check how they handled this in other implementations?\nIt is just an idea though, no need to change it right away, but please check what I mention above.\n. Is it possible to do this without copying feature vectors?\n. Hmm. I guess you don't need the ?true:false part.\n. Same here, I don't think you need ?true:false.\n. Something odd with the indentation\n. Use BSD license\n. BSD license\n. I think you need to unref this. Can you please check with valgrind?\n. Let's put a newline here.\n. Use one private for attributes and another one for methods, please.\n. It would be good to give a default value to the number of classes here to avoid potential errors due to uninitialised memory.\n. class index (%d) exceeds the length of the dense vector (%d), index, dense_dim\n. Typo in achieved.\n. The doxygen for the return value is missing.\n. Make getters const, please.\n. We normally write the attributes at the end of the class. Also, why protected and no private?\n. Why without the $? Just wondering.\n. I understand learning sample is data set. Is that right?\n. During the* tree growing process OR during tree growing.\n. For classification tasks\n. regression tasks\n. when the* dependent\n. it [cost-complexity pruning] yields*\n. using the* complexity normalised etc\n. I'd use however instead of but. If you want to keep but, then remove the commas between CNRE.\n. the* test dataset.\n. of the* best attribute, or attributes.\n. the* degree of closeness\n. Being picky here, why Left and Right with the initials in capitals but not in the discussion before?\n. double is\n. Nice documentation! :-)\n. Hey @karlnapf, @mazumdarparijat and I already discussed about this. See the comments above. I opened an issue for this as well (#2252). I plan to see to it asap.\n. Being picky about style: let's put here a newline since you put it always below and let's try to stick to that rule.\n. Maybe just return SGVector<float64_t>(m_alphas->get_array(), num); is more concise.\n. Also, be aware that by doing this you allow that m_alphas can be modified from outside. According to what I have seen in the rest of the class this should be fine since m_alphas is write-only within this class and only used in ::prune_tree().\n. is cut instead of it cuts\n. We need at least a link to the documentation you have linked to in the PR, although a description of the method here would be much better. With method I mean how does this pruning work. A short description suffices.\n. Spaces between all args, please.\n. This method is really long. Do you come up with a way of making it modular? For instance, separating it and using some private methods.\n. This is still missing.. fix it please in a minimal PR.\n. Then just mention that a description is given in the documentation of the class (just in case someone comes directly to read what this method does without reading the class doc first).\n. Make the method const.\n. remove float64_t \n. There's so much code duplication between the two branches of this if() ... else. They are basically the same except from two lines you use the constructor of/cast to node_t instead of bnode_t, right? We should be able to use a better design.\n. is above a threshold\n. using the p-value\n. the dependent variable\n. Nice documentation!\n. I think that in written English the correct term is Chi-squared. Minor detail, just being annoying here...\n. I think this comment is a bit misleading with the code since our implementation of incomplete_gamma already includes the division by gamma(k/2).\n. Similar comment here.\n. Use underscore as in the argument name. Otherwise, Doxygen will report a warning.\n. Maybe it could make more sense to enum instead of int for this?\n. Its -> it is..\n. It is \n. Yeah, I meant using a vector of enum for feature_types. Sorry for the short comment. I don't quite recall using SGVector with enum, but I guess it should be possible. \n. I am not sure if SGVector currently supports enum. But let's don't lose time with this right now.\n. Will these dynamic casts cause warnings in coverity like the ones we have previously seen? It looks to me like they may do it.\n. Should we make sure that folds is larger than (or equal to) zero?\n. Why going from nan to max float?\n. For features, I'd prefer if we use CDenseFeatures::obtain_from_generic(CFeatures*) to do the cast. It is not the same as for tree nodes because for the latter we don't have such methods.\n. Another possibility is to first count how many elements in m_feature_types are equal to 2, and then just use a SGVector instead of CDynamicArray for cont_ind. For large arrays, there might be a difference in performance taking into account the sequential access below.\n. Minor thing, I think there is a white space missing between the first and second lines.\n. in NULL, instead of in? And being annoying, the object is not actually a vector..\n. Use CLabelsFactory::to_multiclass(CLabels*) instead, please.\n. Same comment as above for the cast.\n. Same comment as above.\n. Is this part exactly the same as above? I can only see that the line with the cast is different.\n. If it is so, please do not duplicate code. Use a method that is called by both apply_regression and apply_multiclass, or merge these two methods into one and use the method get_label_type from CLabels to find out if you should use to_multiclass or to to_regression.\n. Take a look at the style, we put braces after a newline. See to this in the whole PR, please.\n. Fix indentation\n. Line breaks like this one (there are more below in this file) look weird to me.\n. knn looks like a more suitable name for this object, but it's fine.\n. We use underscores instead of camelising. See the other variables below as well.\n. I see now you have used this name already below. Why creating a new KNN object anyway? It should be possible to just change the mode, right?\n. It seems that this test is probably leaking since for instance this object is never freed. The same applies for the test labels and feature and the resulting labels below.\n. Should the mode you have passed as an argument be used here instead?\n. Documentation is missing.\n. set to the number\n. No idea if coverity may complain about the dynamic casts in this file done like this.\n. num_random_features sounds more natural to me\n. same as above\n. less than or equal\n. less than or \n. Missing documentation.\n. Why do you follow different styles in the documentation amongst distinct classes? Look for instance how you wrote this method for RandomForest. It is not something very relevant but I guess it is more natural to just stick to one style.\n. missing newline\n. less than the\n. From the beginning of this method up until here, this method is the same as the one that is overloading in its parent class :-) I think that only the REQUIRE in line 63 has been added here. Put common functionality in a method that can be called from both classes, please.\n. Something similar happens in this for loop, right? It seems that the whole body of the loop is the same in both methods and only the number of iterations changes. Any idea to avoid this much code duplication?\n. From here to the end of the method, I think it is the same as the method in the parent class. Let's avoid the duplication here as well.\n. Out of curiosity, why is this check implemented here but not in the method of the parent class?\n. For casting labels, use obtain_from_generic please. This way the actual cast is only written in one place and it is more practical if at some time we need to modify the way we are doing the casts.\n. Let's write some more informative message here, like Shogun and/or numpy installation not found. See other examples if you want to draw inspiration.\n. feats_labels sounds a bit funny to me, they are either features or labels, or? :-)\n. Can you write a very short comment saying what these False values represent?\n. It is ok to leave prints like this in the examples but please comment then so that tests by default are executed cleanly.\n. Maybe you can also print this one and/or return it. Just an idea.\n. Is it practical to have these nodes classes visible from the interfaces?\n. I mean, to me it seemed that these were internal details of the tree structures.\n. Hmm did this one slipped through? Put it as a with debug level or so if you want to keep it, please.\n. Nice fix!\n. Also, watch out for the syntax with print. I think this is breaking python3 in Travis. See in the other examples the use of print.\n. Same here, just wondering if we need C45TreeNodeData and ID3TreeNodeData.\n. Neither do I TBH. Try it out locally if you feel like it. It's not something relevant anyway so probably it's not worth to spend time with it.\n. I guess we just need this once.\n. Make the getter const, please.\n. const.\n. What about starting the loop from i=1?\n. The C4.5 algorithm\n. Can we get bigger markers in this plot? In the notebook the difference between the crosses and the circles cannot really be pointed out.\n. the trained tree or a trained tree. If you want, proof read and pay attention to this type of mistake, from now on I will skip it.\n. A link to the Doxygen for set_features_types would be nice.\n. I guess you already tried, but didn't this work fine using Shogun's CSVFile class?\n. It would be great if you used Shogun's CrossValidation framework for this kind of stuff.\n. Hmm I am probably misunderstanding something but if you do i=1 and num_vecs==1 then the body of the loop will never be executed because i<num_vecs evaluates to false from the very first moment.\n. Well done, then!\n. All right, got it!\n. the number of\n. is not the same as the number of features\n. the number of vectors\n. If I understand this correctly, the mean squared error obtained is not a relative error, but an absolute error. Then, only the 0.5 value does not tell much about the actual error made. That is, it depends on the range of values taken by the ground truth whether 0.5 is a large error or not. It would be nice if you could write a number to give an idea of these range of values, or use a relative error instead.\nIt might be that I am understanding something wrong though!\n. Look at the other protected and public sections, no new line needed here.\n. @mazumdarparijat, any thought about this?\n. The stop at the end of this line can be removed.\n. It would be nice to have Huber with the first in capital everywhere.\n. Everywhere that is not code.\n. const\n. missing doc (just to avoid warning by Doxygen)\n. Huber\n. doc\n. doc\n. I find it rather weird that you generate randomly the predicted and actual values first, and then hard-code here the test values.\n. I see two approaches:\n- Either you hard code numerical values everywhere and write a short comment so it would be possible to make sense out of them in the future.\n- You compute the test values in another way. For instance, you can write in this file a small function or macro that computes the square (for the squared_loss) of the difference between the predicted and the actual value. It should be possible to take a similar approach for the other losses. It is sort of funny since that is what the loss function class is for, but there are other reasons why they exist.\n. typo: categrical should be categorical.\n. One newline character at the end is wrong, it appears in the text.\n. Let's use topic or another word instead of thing ;-)\n. I'd say in the documentation, rather than at. But I am unsure about this one.\n. I see this is the exactly same code you use to generate the data set in C4.5 and CART (as you say in the description). Please, create a function with this code and just call it from the three places, instead of copy-paste and have the same code in three places.\nSorry for being so annoying :-P\n. You can pass a Boolean as an argument to that function to enable/disable the plotting (just an idea).\n. The function could return directly Shogun objects with the features and the labels.\n. between the original feature matrix\n. I'd use denoting instead of depicting here.\n. modifies the training data\n. I am not sure what is meant with in case continuous features are present, to return back the actual binned values used. Can you please rephrase?\n. You can probably do another function for the plotting of the results in C.45, CHAID, and CART as well. In any case, since this code is much shorter, it may be somewhat an overkill -- you choose.\n. So let's make another function for the regression data as well.\n. Minor on writing style. I'd using as usual twice so close to each other. You can maybe think about another linker which comes to say the same.\n. Minor: missing comma before etc.\n. Let us first read the dataset sounds more natural to me, but I am completely unsure about this one.\n. here we are also interested\ninstead of\nhere also we are interested\n. interested in finding out\ninstead of\ninterested to find out\n. Hence, (missing comma)\n. Aren't you using 20 bins below? :-) You can maybe just remove (10), it is understandable from the order of the arguments, IMO.\n. So 89.5% accuracy. I got 97% last summer with LMNN, in case you might find it a challenge to improve the accuracy with CHAID :-)\n. nitrous instead of nitous?\n. out of the 13 attributes, (missing comma)\n. let us load the dataset as our first step (remove in)\n. useful for.\n. I didn't know about this ptp function, nice. Thanks!\n. Nicely done! Do you happen to know the error that people have obtained using this data set with CHAID and other regression algorithms?\n. If you find something, put a link to it please.\n. suppied should be supplied.\n. A cross reference to CLossFunction doc would be nice here when you mention it.\n. randomly instead of ranomly.\n. missing doc (just to prevent doxygen from complaining)\n. Can you shortly say something about this gamma?\n. compute instead of cmpute\n. Same here as in the unit test we reviewed recently. If you use hard coded values here, then let's use them in get_sinusoid_samples as well.\n. Say something that this is the type of machine upon which gradient boosting will build the ensemble (i.e. the type of weak learners).\n. At first, I thought it should be possible that gradient boosting uses different types of weak learners but I see here that the implementation is fixed to one type. That can be all right, but it would be nice if it is mentioned in the class documentation.\n. @karlnapf, any idea why this line breaks SerializationAscii.HuberLoss, SerializationJSON.HuberLoss and another serialization test?\n. You are right, for the samples used in training it makes no sense to hard code them. But what about the test ones?\n. I don't see anything not initialised. Neither in this class nor in its parent. Can you try adding a default constructor @mazumdarparijat? I thought that wouldn't make a change since the current constructor has a default value but I suppose we better try.\n. There is some sort of fancy indentation all over the place in this file.\n. EDIT: @mazumdarparijat\n. Why this change?\n. Mmm isn't it tot equal to zero at this point? I mean, it looks like 0*least_squares_deviation\n. Oh I see. The method call is evaluated before the multiplication. Thus, tot has another value when the method returns.\n. I think that some of the methods below should be private instead of protected. It is a minor detail anyway, but do you see any class inheriting from CART or a friend class that will need to call them?\n. Fine. But if other few are really private, then I'd separate them.\n. It is not your fault, but the indentation is wrong here, I think.\n. This should be done in a private init method.\n. We avoid using the initialiser list and do this inside the init method instead.\n. I meant this for m_gamma(gamma).\n. Both comments above apply to this constructor as well.\n. I am a little bit concerned about the elements in simple_labels not being initialised. Since there is an if in the loop below, it is not straightforward all the elements in simple_labels will be initialised.\n@mazumdarparijat, will all the elements in simple_labels be initialised after the loop below? If they are not, is it guaranteed somehow that the non-initialised ones will never be accessed?\n. I guess grreater should be greater.\n. and is usually the square root of the total number\n. The purpose of the random subset sampling method\n. decorrelate the individual trees in the forest, (missing comma) thus\n. more generic; i.e. decrease (so use semicolon instead of comma an punctuation correctly with i.e.) \n. AFAIK there's only one English alphabet :-)\n. square root no dash\n. Could you write Majority Vote with a link to the documentation?\n. of all the individual tree outputs\n. i.e.\n. the predictions made by a single tree (remove from)\n. let us find out which one is better\n. As it is clear from the results above\n. Just print a few decimals for all the accuracy values (say something between three and four).\n. Nice results and the point is nicely made!\n. returns instead of return\n. to get the accuracy for different number of trees keeping the subset size constant\n. remove \"for the above case\" and keep a comma after \"plot qualitatively\"\n. impact on, I think\n. otained should be obtained\n. Nice plot btw!\n. Write out-of-bag (OOB) before the first time you start using the acronym. In fact, you could use out-of-bag just the first time and OOB the rest of the times.\n. As mentioned above, limit the number of decimals.\n. This should be Subset Size = 2, methinks.\n. Limit decimal digits.\n. I guess that now we can remove this TODO comment :-)\n. Hmm I am not sure why passing all these SGVectors by reference. @hushell, why?\n. Yeah, I don't think it is installed on Travis at all. It would be nice to install it in the buildbot, although it might be troublesome since Mosek is not free. I'd say that for the moment is ok with checking locally every time we modify some Mosek-related code.\n. Minor on style. We don't write braces for one-liners, except for if-else blocks in which one of the branches has more than one line (then all of them go with braces.)\n. Is this pretty much the same code as CPrimalMosekSOSVM::train_machine(CFeatures*)?\n. I would say we either need these setters or do the unit test in a different way (e.g. testing it through classes that use different configurations of lb and ub). I do not really as an option to copy paste in the unit test the code we want to test.\n. You are the leader now in the SO framework, so go on as you think it's best :-)\n. IIRC, we have =operator for SGVector. What is clone doing differently?\n. I think that there should be somewhere a message or an error if the length of lb or ub is different from M. Otherwise, the user can do the set, training happens without using the bounds and without the user being notified. Do you see my point, @hushell?\n. Missing class doc. A short sentence should suffice.\n. Arguments and return value docs are missing in this class. Is that on purpose?\n. I see that this KNNHeap class is only used from NBodyTree (why is it KNNHeap btw? Is it just the standard heap data structure?). I am wondering then if we could just use STL's heap functions so we don't need to code our own data structure.\nI see you have a private method that takes a heap as an argument. Would it be too cumbersome to change the design slightly so there's no STL in the header but we can still use the STL's heap functions in the implementation file? Then we don't need to implement and maintain ourselves a heap data structure. I think it should be all right, you could for instance pass the heap to the method as an SGVector and when you need to use heap operations in the cpp file you just give the pointer to the data in the SGVector to an std::vector (this operation might be dangerous though, check STL vector's doc if you decide to go for this design).\n. Did it work out using it instead?\n. It sounds like you are right, yeah!\n. Let's make this get and the one below const.\n. Ok, I just saw how these getters are implemented and they cannot be const. Disregard the comment above :-)\n. Although the m_ prefix is not used everywhere in Shogun for class attributes, let's try to at least to include it in the new code.\n. Minor issue: ike should be like?\n. This get and the one below can be const, I think.\n. Can we get a reference for the ball tree? Maybe the one you have used to understand this data structure?\n. A reference for the KD-Tree would be nice.\n. I think it is a common practice to explicitly write virtual in the destructor of children classes, although it doesn't change anything since the destructor of the parent class is already virtual. \n. Same as above, virtual is normally written, I believe.\n. \"the kernel density estimation technique\", or just \"implements kernel density estimation\".\n. Euclidean distance or Manhattan.\n. used is\n. doc for kernel and dist arguments is missing\n. Follow syntax conventions please and start enums with E (check an existing example before).\n. I'd rather have this code in the implementation file.\n. kernel bandwidth sounds more natural\n. Why did you decide to rename the initial from E to K?\n. I would call this kernel_type or something  else rather than kernel. IMHO, one would expect to find an actual kernel object in Shogun if a member has that name.\n. At some moment if would be nice that model selection is actually supported for this (as well as maybe other) attributes of this class.\n. as well as \n. Naming convention for enums?\n. as well as\n. Just in the case that code needs to be modified or tested some time, I normally prefer to write every implementation in the cpp files. Compilation after making even the smallest modification in a header takes time. Keep it here if you prefer though.\n. Conceptually, I am not sure why this include is needed here. As I understood it, KDE uses kd-trees and ball trees, which are specialised types of NBodyTree.\n. the same\n. Ok, this is a somewhat more important comment as it concerns the design of this class. Did you check whether it makes sense to make KDE a subclass of CDistribution and put it under the distribution directory instead of machine? IIRC, this was the idea I had in mind when we wrote the GSoC project idea.\n. Does it make sense for the other [pure] virtuals to be implemented in the KDE class?\n. You can dive in the doc and the code an have a look :-) Anyway, this is something you don't need to worry about just now.\n. Hmm all right. I cannot really tell if that is what makes most sense (at least to me) but I am fine if that's the design you have chosen!\n. This one is still missing :-)\n. Out of curiosity, did you remove the inline when you moved the implementation to the implementation file? I feel you prefer to keep them in the headers and I not very against that, so just keep them as they are ;-)\n. Minor annoying thing about style: can you please rename vec_id as well as the other attributes that don't have the m_ prefix?\n. This method is pretty long. Can you please write more comments so it is easy to follow what's going on?\n. \"thus demontrating the it's prowess as a non-parametric statistical method\" should be \"thus demonstrating its prowess as a ...\"\n. its samples\n. Let's denote the random variable differently from the independent variable of its density. That is, \"the pdf of a variable X given finite...\"\n. \"with the actual pdf\"\n. It could be cool to show the estimate using different number of samples and get a feeling that the more sample points you use, the better estimate pdf you get.\n. minor thing: let's introduce a new line for a bigger gap between the end of the item lists and the paragraph starting afterwards.\n. There is something weird with the last reference in the rendered notebook, it starts in the same line as the second reference.\n. Let's use references in a more standard way. Something like: \"It has been theoretically proven [3] ...\" and then you can remove the part \"a formal proof is presented ...\"\n. typo: and computes its descriptors\n. missing white space after Guassian in Difference of Gaussian(DOG)\n. do read, instead of Do read\n. Not sure why Documentation is with the first in capitals as well.\n. Likely? Is it non-deterministic how matplotlib does it? ;-)\n. Could you explain shortly in the notebook what the length of the radii of the circumferences represents? Also, the same for the angle of the radius that is plotted inside each circumference.\n. Instead of \"from the above 3 images\", write \"from the three images above\".\n. \"can be found using the [...] method\"\n. \"a few training images\"\n. \"We define here\" instead of \"we here define\" (word order and capitals for the first letter).\n. This is odd to me. Why does a DiscreteDistribution have to be a MixtureDistribution?\n. The same here as in CDiscreteDistribution.\n. What do you mean with belongingness values?!\n. I think you should sg_add the data attribute.\n. Out of curiosity, does it work doing directly\nCGaussian* outg=CGaussian::obtain_from_generic(comps->get_element(0));?\n. Why does free have to be explicitly called here? Is the automatic reference counting of SGMatrix not used here?\n. I am not sure about this design. I think these methods could be better in another part. This is something we can discuss.\n. Can we get some documentation for this struct? What is it for?\n. Might coverity complain about this if features  != NULL is not checked behorehand?\n. covariance\n. Why not making it a SGVector?\n. * the* implementation.\n. Dummy Doxygen to avoid warning.\n. Apart from checking values.vector != NULL, you should check that there's at least one element in the vector.\n. Maybe log_values is a better name for the argument, since it states clearly that values are expected to be logarithms.\n. I am a bit confused here, where does the +1 come from?\n. I could make sense to have this in CMath. Anyway, we can keep it here for the time being.\n. Dummy Doxygen.\n. Can we get a citation for the reference you have used to implement the EM algorithm here?\n. typo fucntion should be function.\n. \"it's energy\" should be \"its energy\"\n. an in \"an similarly\" should be a\n. handles should be handled\n. Let's remove this one. The constructor of CMapInferImpl already takes care of it.\n. Oh right, CDynamicObjectArray::get_element returns a CSGObject. It's fine. \n. All right. Thanks for your answer.\n. Hey @hushell. In principle, it makes sense to me, yes, and a better idea than having two different classes that both implement multi-dimensional arrays. However, I would check first if SGVector and SGMatrix contain methods that are similar to the ones you want to include. If they do, then it is perfect. Otherwise, we might need to think more about it. I remember there was some discussion and effort in the past in order not to make these SG* classes too cluttered with extra methods implementing math.\n. That is, we need (missing comma).\n. consists of finding\n. It aligns shapes\n. \"Analysis (GPA)\", missing white space before the acronym.\n. until the mean shape doesn't change\n. that have the largest variance\n. for the directions\n. It seems that more than three are used here :-)\n. @hushell, use your best criteria here. You know better than me what is needed here.\nHaving said that, I think there's probably not a very good reason to keep two different classes that implement the same multi-dimensional array data structure.\nRegarding the base sizes comment above, I see that there is still a m_base_sizes attribute in your multi-dimensional array class. Doesn't this mean that base sizes are being recorded here as well like you mentioned?\n. Yes @hushell, that was pretty much my point. I agree with you, it is a good idea to do this in another PR. Let me know if you want this one merged and then merge MDArray and SGNDArray, or the other way around. @Jiaolong, any preference from your side?\n. Oh, got it now. Thanks for the explanation!\n. Minor note on style, no need for braces here.\n. I wouldn't suggest this method inline as it contains an if and a loop (i.e. is not just a one-liner). Also, I prefer to keep implementation in the cpp files.\n. Can you reformulate the sentence here?\n. Because the block inside is just one line. Just a suggestion for this and the other one-liners for loops in the cpp file.\n. Let's make the asserts requires with nice error messages.\n. Maybe it would be better to pass a SGVector here.\n. What would happen if index is null or if it doesn't have num_dims elements? I think this method should make these checks. As it is right now it would terribly fail in those cases I believe.\n. And possibly other cases (that is, with the two I mentioned before I didn't pretend to be exhaustive).\n. Out of curiosity, why do you create a couple of Shogun objects in the heap and this one in the stack?\n. Fix the indentation.\n. Indentation.\n. Indentation.\n. What is being used from this header?\n. I think in SGVector and SGMatrix this is done with set_const. I suggest following the same interface.\n. I think it should be \"we lose\".\n. Here the newline is missing, while you are using it above. I am not sure whether we normally write REQUIRE with newline at the end. Maybe the style instructions say something about it. Otherwise, check how it is done in other files.\n. Minor thing, in text write Gaussian with the first letter in capitals.\n. Newline before start of block.\n. Cholesky with the first letter in capitals.\n. Shogun rep?\n. Please, do not use headings for sentences this long. I would rather make the line 1 the heading. In other words, remove the ### here and put it above.\n. Start the sentences with capitals, We.\n. between Shogun's implementation of neural networks and OpenCV's one on a ...\n. Our, start with capital.\n. Let us, or let's.\n. I would not use these horizontal bars all over the place, but that is probably a matter of taste.\n. I guess the > is not really necessary (just to be consistent with the rest of the document).\n. We get a pointer to a CvMat object.\n. The total number of features is [...].\n. @kislayabhi, any idea why our accuracy turns out to be worse? Both k-NN implementations used here should be exact k-NN search and they are using the exact same data (right?). Thus, the results should in principle be the same.\n. Pretty much all the information in this section (as well as in other paragraphs above and probably below as well) is duplicated from the kNN notebook. We should be able to make this in a better way.\nImagine that at some moment we have to update these files, it'd be a terrible pain to update the same things in each of them.\n. Nice results! So, the implementation we have in Shogun is both faster and more accurate in this classification task than OpenCV's implementation, right? It sounds too good :-)\n. I guess we do not need this newline here.\n. Completely agree! Note however that the method in CMath might have some differences -- I see that at least one difference; of the two REQUIRE above only one is present in CMath::log_sum_exp(...). Please update the method in CMath as well.\n. I agree. Sorry that I missed that. @abhinavagarwalla, would you mind to update?\n. Actually, CFile::get_vector(bool*&, int32_t&) is not just empty (https://github.com/abhinavagarwalla/shogun/blob/deeb9dcb4674dff5000434f5ec0dde1a66a38b2a/src/shogun/io/File.cpp#L79). I am not sure though why it is implemented here for bool data type and not for the rest.\n. Is it just a dummy implementation to avoid the class from being abstract? I am a little bit skeptical to believe that, otherwise why wouldn't it have been implemented with SG_NOTIMPLEMENTED or just an empty block? :-/\n. This p SGVector is an output argument. Anyway, one could use eigen to make the exp and then convert to SGVector. However, even if eigen3 is vectorised, do you think it would really be faster? I know that's true for languages like Matlab (I guess for Python too) but I think it might not be true for C++. I can do a small benchmark to test it.\n. That's fine. When an instance of one of the derived classes calls the method, then the one in the corresponding derived class is called (note that the method is virtual).\nPlease, put the\n{\n    SG_NOTIMPLEMENTED;\n}\nin the implementation (cpp files). IMHO, methods should be implemented in cpp files, not in the headers.\n. @abhinavagarwalla, could you please refresh my memory; why were these methods before with an empty implementation (i.e. only {}) in the header and now they have gotten a complete implementation in this file?\n. Is the STRING_LIST_GETTER macro including the set_string_list implementation as well?\n. Not sure why the change in this line, but that's fine.\n. I am being annoying here but one line for this is fine.\n. The same thing as above. If you want to use only one macro for both the setters and getters, that's fine for me. But I would rather use another name for the macro that either reflects that it includes both setter and getter, or does not seem to be bound to only one of them.\n. I would like to see the succinct macro strategy for get_vector, set_vector, get_matrix, and set_matrix as well.\n. Why not using the SG_NOTIMPLEMENTED approach there as well, then?\nOn 21 November 2014 07:17, Abhinav Agarwalla notifications@github.com\nwrote:\n\nIn src/shogun/io/File.cpp:\n\n@@ -193,6 +426,64 @@ char* CFile::get_variable_name()\n    return strdup(variable_name);\n }\n+#define STRING_LIST_GETTER(type)                                   \\\n+void CFile::get_string_list(                                       \\           \n\nBasically, these are just dummy implementation to prevent the class from\nbeing fully abstract.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/shogun-toolbox/shogun/pull/2616/files#r20700205.\n. Maybe I am getting something wrong here. I am talking about a few methods\nwhose implementation before this patch was empty, but for which you have\nnow added a dummy implementation.\n\nOn 21 November 2014 09:01, Abhinav Agarwalla notifications@github.com\nwrote:\n\nIn src/shogun/io/File.cpp:\n\n@@ -193,6 +426,64 @@ char* CFile::get_variable_name()\n    return strdup(variable_name);\n }\n+#define STRING_LIST_GETTER(type)                                   \\\n+void CFile::get_string_list(                                       \\           \n\nI tried that approach, and it also passed all the tests. I just kept the\ndummy ones considering that whoever wrote them, must have written them for\nsome reason.\nIMO, It may have been written for debugging the the class, maybe.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/shogun-toolbox/shogun/pull/2616/files#r20702511.\n. You are right! I didn't realise that it had a dummy implementation for bool\nand just empty for the rest of the types. I have no idea why it was done\nlike that, but I am good with the way you are doing it now with a dummy\nimplementation for all data types.\n\nOn 21 November 2014 09:16, Abhinav Agarwalla notifications@github.com\nwrote:\n\nIn src/shogun/io/File.cpp:\n\n@@ -193,6 +426,64 @@ char* CFile::get_variable_name()\n    return strdup(variable_name);\n }\n+#define STRING_LIST_GETTER(type)                                   \\\n+void CFile::get_string_list(                                       \\           \n\nSorry, I completely misunderstood.\nEarlier before this patch dummy function was implemented for only bool\ntype argument, but since I used Macros instead, the dummy definition was\nused.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/shogun-toolbox/shogun/pull/2616/files#r20702868.\n. Where's the get?\n. White space at the beginning of this line.\n. Fix indentation.\n. Leave a new line character after for, while, if, etc blocks. Unless it is to close the block with a right brace }.\n. Indentation.\n. Same as above regarding the new line.\n. Say that strings is of type SGString<int16_t>*. Is this statement allocating enough memory?\n. Can you please explain why this way of copying is correct? I see only 0 or 1 bytes are written but the content of strs might be different to that, right?\n. Configure your editor so that it shows you non printable characters. The\neditor probably inserted white space characters where we normally use tabs.\n\nOn 21 November 2014 09:45, Abhinav Agarwalla notifications@github.com\nwrote:\n\nIn src/shogun/io/File.cpp:\n\n+void CFile::get_string_list(                                       \\\n-       SGString*& strings, int32_t& num_str,                 \\\n-       int32_t& max_string_len)                                    \\\n  +{                                                                  \\\n-   SGString* strs;                                         \\\n-   get_string_list(strs, num_str, max_string_len);                 \\\n-                                                                   \\\n-   ASSERT(num_str>0 && max_string_len>0)                           \\\n-   strings=SG_MALLOC(SGString, num_str);                     \\\n-                                                                   \\\n-   for(int32_t i = 0;i < num_str;i++)                              \\\n-   {                                                               \\\n-       strings[i].slen = strs[i].slen;                             \\\n-                strings[i].string = SG_MALLOC(type, strs[i].slen); \\\n-       for(int32_t j = 0;j < strs[i].slen;j++)                     \\\n-       strings[i].string[j] = strs[i].string[j] != 0 ? 1 : 0;      \\\n\nIndentation is completely fine in my editor. I am not sure why git is\nmessing it up. Will look for a solution and update.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/shogun-toolbox/shogun/pull/2616/files#r20703820.\n. It seems you have completely indented the whole file in a new way. I am not sure why.\n. Can you please check why do we need this include? I had a quickly look at the file and I don't see the dependency.\n. Same as above.\n. The same. Pretty much check whether we need the stdio you have changed in this PR and let us know if any of them is actually necessary. Otherwise, just remove them.\n. Can this one be omitted here and just used from the cpp file?\n. I think it is weird making external, third party libraries we include depend on Shogun.\n. For this one, and the changes below (and possible others above). If you just have to change the include and everything compiles fine then there is something a bit funny. Why would the former include have been used? Also, you cannot rely on including CMath to make std math visible here. If CMath stops including std math then compilation of this file would break if it does depend on std math.\n. I'd say that now the unit tests for min and max methods make more sense in CMath rather than in SGVector, what do you think?\n. Agreed. Moving methods one-by-one eases review. However, pushing all commits here does not sound that good (it actually sounds kind of the contradictory to moving methods one-by-one).\n\nI just meant you could move the unit tests of the methods you have moved from SGVector to CMath from SGVector_unittest to Math_unittest (or whatever the unit test file for CMath is called :-) \n. I've just quickly browsed through this header file and I don't quite see why this include is needed at all (maybe I missed anything, though). Could you try to remove the include here? You may need to include it in the cpp file in case it is used in there.\nAlso, I think the includes for limits.h and SGIO are not needed here either.\n. Same as the comment below for this one. The header does not seem to require this include.\n. I am not sure, but I don't see why not. Do you see any advantage putting it inside? For some reason, it looks to me cleaner this way, although I always prefer to have methods implemented in cpp files.\n. I am not 100% sure whether they are the same, but it makes sense to me. I'd say that as long as compilation continues going smoothly is fine. Anyway, as we just discussed, these methods are going to be dropped probably as they are unused.\n. Does it work if you don't write NULL explicitly for the last argument and just make use of the default value defined in the method's signature?\n. If you are going to keep these methods in this PR, please move the implementation to the cpp file (and also the include lapack above).\n. See Travis errors in the first job. This change broke compilation.\n. There might be some Doxygen feature to do that.\n. only \"index of the maximum value\". The documentation of the interface does not need to contain the internal name of the variable for the return value.\n. The descriptions you gave make sense to me :-)\n. Why this new newline?\n. I'd definitely take advantage of these changes and use a better name for this variable, like  simply sum. This must be kind of a joke :-D\n. It uses kmeans++, which is an heuristic to obtain a good initial clustering.\nOn 18 Dec 2014 22:52, \"Heiko Strathmann\" notifications@github.com wrote:\n\nIn src/shogun/clustering/KMeans.cpp\nhttps://github.com/shogun-toolbox/shogun/pull/2648#discussion-diff-22074420\n:\n\n}\nbool CKMeans::get_use_kmeanspp() const\n {\n-   return use_kmeanspp;\n-   return m_use_kmeanspp;\n\nI am curious, what does this flag do?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/shogun-toolbox/shogun/pull/2648/files#r22074420.\n. Yeah, I guess also it stems for cumulative sum. In any case, I understand\nthe cumulative sum (cumsum, for short I'd use) is an array that behaves\nlike the cdf for a pdf. This is just a single value so sum makes more sense\nto me.\nOn 21 Dec 2014 09:32, \"Sanuj Sharma\" notifications@github.com wrote:\nIn src/shogun/mathematics/Statistics.h\nhttps://github.com/shogun-toolbox/shogun/pull/2649#discussion-diff-22146482\n:\n\n@@ -39,7 +41,16 @@ class CStatistics: public CSGObject\n     * @param values vector of values\n     * @return mean of given values\n     */\n-   static float64_t mean(SGVector values);\n-   template\n-       static floatmax_t mean(SGVector vec)\n-       {\n-           floatmax_t cum = 0;\n\n@iglesias https://github.com/iglesias yeah even i noticed this :-D but\nthen I thought it might mean 'cumulative' or something like that. Anyways,\nI'll change it and make it sum.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/shogun-toolbox/shogun/pull/2649/files#r22146482.\n. Indentation\n. This explanation does not seem to correspond to the implementation below.\n. Remove maxv and try to be consistent with the style in the documentation of all methods. If you use param vec Input vector (the first word starting with capital after the argument name, then you should do the same for all methods, e.g. param a Complex number above. Since most of the methods don't do this, I suggest you to just start with small letters here as well.\n. What does it do? :-)\n. Definitely. Are these load and save methods doing anything actually?\n. Oh, I see. The * in the lines below should be aligned with the first * of the first line with /**. At least, that's how I have always seen it. Not sure if Doxygen is happy with this style as well?\n. If we are doing changes here, I think it might be worth to change this to atan2(y,x) since it is much more common imo.\n. Can we get a short description for this parameter?\n. Minor suggestion, I would start directly with \"Extract the byte ...\".\n. Short description for this parameter.\n. Short descriptions for these ones.\n. Start with capitals (just to be consistent with the doc of the other functions).\n. Change to y input (numerator)\n. Change to x input (denominator)\n. change to atan2((double) y, (double) x);\n. http://en.cppreference.com/w/cpp/numeric/math/atan2\n. I think this is wrong, @abhinavagarwalla. CFile::set_vector(const type*, int32_t) for any type will call CFile::set_vector(const int32_t*, int32_t) which will call again itself and like that until the end of time.\n. Same comment as above.\n. Same as above.\n. Same as above.\n. The same here.\n. And here.\n. One line for the method header, please.\n. One line for the method header, please.\n. One line for the method header, please.\n. Should this comment be updated?\n. I'm afraid I am not completely understanding what initialisation of the last entry of the enum to 0 does. Aren't eigen and viennacl (if available) as well as native as default all getting the value zero?\n. I understood the enum above now, @lambday. Thanks for the explanation!\n. Hey, @lambday. A couple of comments regarding the linalg wiki:\n- When the Redux and Core modules are introduced here, maybe it would be nice to mention what is different between both? Just to give an idea where the names come from.\n- I saw the usage of linalg in CustomKernel.cpp. I wonder why the HAVE_LINALG_LIB guard is required. I guess it is because there is a cmake option that allows to switch on/off compilation of linalg. In that case, maybe that can be mention in the wiki as well.\n. It is very cool to have this documented in the wiki by the way. Awesome :-)\n. It sounds like we can get rid of HAVE_LINALG_LIB completely, right? With completely I mean everywhere and not only in CustomKernel.cpp. Operations in native will always be available so no guard required wherever they are used, and eigen3/viennacl stuff should always have their own have_eigen3/viennacl.\n. Oh, I got now the meaning of HAVE_LINALG_LIB; it refers to having an external library e.g. Eigen3 or ViennaCL. So far, I had read it as having the Shogun internal linalg library and that's why I got confused :-)\n. What about AppendToDefines(HAVE_FDOPEN) instead of this if? It is more succinct :)\n. The algorithm itself consists of\n. Compute the squared distances matrix\n. Break the three lines above to have a similar line width as the one used in the previous paragraphs (we don't follow any specific line width, but it is nice to be consistent at least within files).\n. Normalise these vector dividing (remove with)\n. Each eigenvector (singular)\n. with square root should be by the square root of its corresponding eigenvalue\n. A reference to the paper where Isomap was first introduced is a must as well :-)\n\nhttp://isomap.stanford.edu/\n. Mm I just saw there is already a reference for another paper at the beginning of the class description. I think the one in the link above is more suitable since it is an older paper but I am fine leaving it as it is as well.\n. Is it \"sparse\" Dijkstra's algorithm a special version of Dijkstra's algorithm or just the same? If the latter, I would just say Dijkstra's algorithm,\n. shortest (so-called) missing white space after shortest.\n. I think there's something weird with the indentation here.\n. Just for the sake of following our usual style, open and close braces in new lines. Here and below.\n. As we relying on the stl for doing the max, I suggest we just go and use std::max_element directly.\n. That is, that compute basically calls std::max_element.\n. If this can crash from the usage of a user, I think we should use REQUIRE with a proper error message.\n. We should also avoid SWIG from going through this file, right? I am not sure if that is already happening since there is no mention to this in any .i file or we should also use the guard.\n. Why this one?\n. Because of the std::unique_ptr use in the header. Also, this is to be used internally, not something to expose, right?\n. Can you display here idx as well? Maybe with Features index (%d) should...\n. Here and below maybe it is good to display n as well.\n. Nah, never mind about that one. It should be understandable enough to say that a negative value was passed.\n. @yorkerlin, I think you should be able to do it using scale and add. Please try.\n. We should be able to take out this one now?\n. Here the same?\n. Still needed? :-)\n. Print also the length and the size, please.\n. I think that your editor has messed around with the white spaces characters. Check if the file was initially using tabs or white spaces and stick to them.\n. Maintain style guidelines. Use a newline before opening a block.\n. We don't use braces for one-liner blocks unless if-else-if-else blocks where one of the branches has more than one line.\n. Apart from the indentation of the braces, this test has not been changed, right? The new changes are very difficult to parse like this.\n. Leave one empty line after you close this block.\n. Fix indentation in the comment.\n. Open the brace in a new line.\n. Should this be vecIndex<numberOfVectors?\n. Since you are using the same code to create the sparse matrix in several unit tests, why don't you use pull out that functionality into one function that then your tests call?\n. I am not sure I understand why the use of sort_features here. Would you mind to explain, please?\n. Write comments rather before, in their own line, instead of at the end of a line of code.\n. Why making all these const? They are passed by value anyway. It's fine, just wondering.\n. Double-check and fix it if so.\n. Can you call here GenerateMatrix as well so that this test only contains the EXPECT_EQ in the loops?\n. That way, we reduce lines of code.\n. Can you motivate a bit more? Fine that dense matrix uses (row,col) (or the other way around, do not remember now) and sparse the contrary (as you mentioned in the mailing list). But dense matrix is not obliged to have features in rows or columns, it is just a generic matrix.\n. Why cannot you make 1.?\n. I think you mean according, typo.\n. Why are all these const and passed by reference? I mean, the pass by reference is correct, but there is no gain here since the arguments are just primitive and very simple data types.\n. Why not creating a MatrixType variable in the method that is then returned? In my opinion, it is more intuitive and readable that way.\n. Keep one instruction per line, please.\n. I am not really a fan of this sort of indentation, but if you like it I guess it's fine.\n. Why is GitHub marking this in another colour? Something different with the indentation (using spaces instead of tabs) maybe? \n. This is what I mean:\n```\ntemplate\nMatrixType GenerateMatrixAccordingToSparseConvention(float64_t sparseLevel, int32_t m, int32_t n, \n              int32_t randSeed)\n{\n  CRandom randGenerator(randSeed);\n  MatrixType matrix(m,n);\n  for (index_t i=0; i<m; ++i)\n    for (index_t j=0; j<n; ++j)\n    {\n      float64_t randomNumber=randGenerator.random(0.0,1.0);\n      if (randomNumber<=sparseLevel)\n        matrix(i,j)=randomNumber*100;\n    }\nreturn matrix;\n}\n```\n. The idea is that when you call it for a dense matrix then m has the meaning of #features and n of #vectors (or vice versa, maybe), and when you call it for a sparse matrix, it is the other way around. It does not matter; they are just generic types for matrices, not actual feature objects!\nI haven't actually compiled and tested the code above, just write it here directly, but it makes sense to me and hopefully illustrates the idea. Use at your own risk ;-)\n. No. The for loops in my comment  made \u00a1 go from 0 to n-1 and j from 0 to m-1 (or vice versa) in any case.\n. I think we can skip the \\n.\n. Here as well.\n. Following the style used above, I would start the message with capital i and finish with a stop.\n. @Saurabh7?\n. If CKMeansBase is not needed, we can remove this include.\n. Only one newline is enough.\n. No newline.\n. No newline.\n. Separate both methods with a newline and write dummy documentation (e.g. class destructor) for the destructor.\n. Agree. Let's go for *_mb_kmeans_* or *_mbkmeans_* instead of *_mbKMeans_*.\n. Any reason why to keep this commented? If not, I say let's remove dead code.\n. Baum-Welch with capitals when writing text (vs. code), they are surnames.\n. Same as above. Leave white space before writing (BW_NORMAL).\n. Missing stop at the end of the sentence.\n. Typo: are initialized\n. Missing reference to algorithm.\n. All arguments should have Doxygen documentation. In this method and the ones below as well.\n. That seems to be true. I had this old commit in a local branch and pushed it overlooking the new structure.\n. What is the \"new\" news section?\n. I will move this into Cleanups in the section above. I am wondering though, how do keep track of news constantly if we put everything in a section after a release?\n. Can we make the data for the unit test more minimal so that the test is not time consuming? Unit tests should be quite minimal, using very toy and simple data sets.\n. I get a bit scared with all these changes with UNREF and REF. Did you run valgrind and ensure nothing is leaking?\n. Can you describe a little bit what is the function of this class?\n. Typo: factor graph.\n. Why are you using this macro here?\n. I am not saying is wrong though, just really asking :-)\n. Danger to have these includes if we want these headers to be public to modular interfaces!\n. All good with licenses here, I mean, is a new implementation what we have here inspired from the original?\n. If you already did it, then it should be fine ;-) I just wanted to be sure you did check it.\n. You don't need any macro to avoid swig from generating wrappers for a class. This is what you mean by generating Python interface. In fact, a class has to be included in swig files (.i files under interfaces directory) to be accessible to modular interfaces. We use macros when we make a class accessible to interfaces but we do not want some of its methods to be accessible via modular interfaces (but it is not this macro, see e.g. SGVector).\n. Thus, I suggest to remove this line.\n. What does it mean to follow at least 80%? Is it that 80% of the lines are copy & paste but not 100% of them? :-)\n. I understand, however, that we can use gemplp from Python since you have used it in your notebook, right? I guess because you use an enumerate or similar mechanism to select the type of approximate inference to use in your MAP class.\nThat is perfect, then! The interface, in this case, MAPInference class, is accessible to swig and through it one can define whether to use gemplp, etc.\n. Remember to remove the ignore here as well.\n. We are all right, then.\n. One-liner loop, no need for braces. Here and below.\n. Update the name here as well, please, and in the line below this one.\n. Hi Esben,\nIn principle, it is not only for model selection but for serialisation as\nwell. See https://github.com/shogun-toolbox/shogun/issues/869\nRemark: as mentioned in the issue, there seems to be something wrong going\non with serialisation though.\nOn 9 April 2015 at 13:52, Esben S\u00f8rig notifications@github.com wrote:\n\nIn src/shogun/kernel/PeriodicKernel.cpp\nhttps://github.com/shogun-toolbox/shogun/pull/2807#discussion_r28055118:\n\n\nderivative(j,k)=original_4.0_M_PI_dist_cos(trig_arg)_sin(trig_arg)/pow(period_width,2);\n}\nreturn derivative;\n}\nelse\n{\nSG_ERROR(\"Can't compute derivative wrt %s parameter\\n\", param->m_name);\nreturn SGMatrix();\n}\n  +}\n  +\n  +void CPeriodicKernel::init()\n  +{\nset_width(1.0);\nset_period(1.0);\nsq_lhs=NULL;\n\n\nCan you explain why they should be registered? Registered parameters are\nfor model selection or am I misunderstanding something?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/shogun-toolbox/shogun/pull/2807/files#r28055118.\n. Same as instead of same with, sorry for being annoying :-)\n. Even if it should be enough saying \"with refcounting\" as it is, I would perhaps remark here that no data is copied. Also in the method below.\n. +1 to supporting in-place too.\n. Something funny with the indentation here.\n. Indentation.\n. Try with SG_SWARNING.\n. A comment on why we are using this value for the third argument would be helpful.\n. Why does it make sense to have the specialisation for string explicitly in a new class?\n. Can we use higher level language features instead of this explicit memory management?\n. No idea it was just a seed :-) I suppose any value would work, then.\n. How is the memory management of the LearningRate* taken care of?\n. Missing question mark, for consistency with the comment below.\n. Leave a newline also here if so.\n. typo, eplison -> epsilon\n. No real need of Doxygen-type documentation for private methods (since they do not belong to the interface of the class). Especially when they are dummy placeholder descriptions such as this one.\n. Perhaps a more meaningful description which tells more than the attribute name?\n. Same as the comment in the other thread regarding Doxygen-docs of private methods.\n. I'd say let's try to be consistent with the doc in the other methods and remove this line.\n. New line to be consistent between this line and the next. There are similar cases to this case and the one above in other methods in this file and files below.\n. Maybe typo: epsilon? Same in the rest of this file.\n. Maybe you mean epsilon and not epslion? Almost there :-)\n. maybe epsilon.\n. No problemo, amigo, my pleasure :dancer: \n. I think you need to use this syntax to group the documentation of more than one member together.\n. You don't need the @'s here, I believe.\n. Can we get these equations with Latex? See for instance the doc of check_gradients in this file.\n. I am not sure why the change in this line. Anyway, maybe we can fix the > to >= as the actual require is doing :-)\n. I think this description should be maximum of the rate.\n. #2985\n. Sorry about joining late :-)\n\nTo avoid code duplication, I'd say it is worth to make an auxiliary method that computes the squared norms given the appropriate CDenseFeatures object.\n. I think that this warning only the doc is very dangerous. We should handle this is in the code as well.\n. Would be it possible using a precomputed boolean attribute? If set to true, the distances are precomputed, and updated whenever a feature matrix is changed.\n. What about a more concise method to input the matrix instead of one line per element? I find all these lines to initialise a small matrix an overkill.\n. estimator -> estimate\n. I would find it more consistent if the symbol for the vector in the equations (x) is the same as the variable name (a).\n. @param vec\n. The comments above regarding the method documentation also apply here and below.\n. According to the method documentation, should it be divided by vec.vlen or vec.vlen-1?\n. Let's use col_wise and row_wise for the test names for consistency with the implementation above.\n. I see there are other things in Redux.h called rowwise so consistency is a bit compromised already. I have no strong feelings about this any more, then.\n. Why keeping this method commented?\n. param m\n. Documentation for the second argument.\n. Hey @c4goldsw. I believe @karlnapf was referring to something along the line of what is discussed in the second link in your message.\nFor inspiration, have a look to how this is done in other places in linalg for other operations. I have checked out a couple for you:\n- Dot product between two vectors: https://github.com/shogun-toolbox/shogun/blob/develop/src/shogun/mathematics/linalg/internal/implementation/Dot.h#L102.\n- Element-wise product of matrices: https://github.com/shogun-toolbox/shogun/blob/develop/src/shogun/mathematics/linalg/internal/implementation/ElementwiseProduct.h#L123\nHope that helps!\n. What do you mean it makes no sense? The link you have provided is to compute the variance of a discrete random variable from its probability mass function. That is not the same as using data to estimate the variance of an underlying model. The latter is in fact what is described in the documentation of the compute methods here.\nIf you are interested in understanding why in variance estimation the N-1 makes sense, I suggest you a quick read: http://dawenl.github.io/files/mle_biased.pdf.\n. I think this name can lead to confusion because it is not distances that are pre-computed but the squared norms of the vectors. Why did the name change in #3073, @Saurabh7?\n. Minor remark: this newline is not needed here.\n. I see. Are there any other examples currently implemented where precompute is calculating something different to squared norms?\n. Is there a strong reason to keep implementation of these methods in the header? Also for the ones below.\n. Sure, like done here, it is a one-liner referencing to the LICENSE file, @karlnapf.. Nice catch @vinx13, thanks a lot for fixing. . Hey guys. Why isn't the same as before? I don't see it :-/ Or maybe this discussion is about a previous diff?. I checked and only found the three mentioned here: the original with TRACE plus the two in this diff.\n. Where is the difference between them, @vinx13?. Minor, for consistency, indent and break lines like in the doc reference you've cited.. Put this line and the two below at the same indentation than range, like in the doc we were referring to.. Of course, you are welcome!. These ones had to go because the members are T. These get_ methods wrap the T* into SGVector or SGMatrix.. Have a look at the output of the style checker ;-). Minor style fixing needed here as well.. For conciseness and better readiblity, write the variable declarations together with their initialization, for the ones  where it's possible.. For sure, then I won't be shouting about them \ud83d\ude4a . tmp failure with error?. Neat job documenting!. No need to update, just informational. A small improvement would be to query the matrix dimensions instead of hardcoding them in the loops.. I propose being more precise here. All the available data is divided into train and test subsets. Then it is in fact the train subset which is further divided to perform cross-validation. That is, you train using part of the train subset and validate using another part of the train subset (we could say, the \"validation subset\"). This is of course still simplified, but it does make clear that the data used for validation is part of the train data. And more importantly, it is also clear that the test subset is never used for any training.. Typo: and and.. It looks like there is a verb missing here, probably divide.. It would be nice to keep the hyphenation in cross-validation consistent.. Redundant: instantiate an instance.. Same as above regarding instance redundancy. Please check and fix if there are more instances (lol) of this.. Let's just do this for shogun namespace. This way it will also become consistent here in Eigen and Stan. If you really want to save typing some characters for any special symbol, just bring it to scope individually; for example, using Eigen::Matrix;.. Nitpicking a little bit on style. I would not use auto here. For one-liner scopes, I think the Shogun style used to be to omit the curly braces.. What about something like Eigen::MatrixXd X(2,3) instead?. Please remove the spaces inside the template parameters, nowadays it is all fine to have << or >> in templates :-). Ah, I see now. All right :-). ++ . What does this mean?. Is there some coding guideline preventing from using more underscores?. It does not feel good to have type aliases in headers. They also become effective in places where the header is included, no?. Style: newline (after implementing) and capitalization (implementing \\n The) are odd.. This is not informative doc :-) I'd remove very general comments like this one, they add noise.. Potentially silly question alert! Why having an attribute of this type instead of a method?. Why are all these members pointers?. As you are already touching this file, it would be nice to do a tiny extra effort and update the Copyright to the more modern shorter version.. It seems a bit odd that the interface inherits from the class.. Can you do the CombinedFeatures and BinaryLabels below with the factories?. Factory?. Getter instead of obtain_from_generic (ofg)?. Getter in place of ofg?. This is a good one indeed, but for the sake of going ahead with this PR, I suggest to do that independently. Are we keeping track of the stuff that is still missing factories somewhere, @karlnapf?. @FaroukY cross_validation, why crossReg?. Did you attempt doing this, @FaroukY?. What about this one, @FaroukY?. We need more clarity here :-) The distance extends the concept of the metric, what does this mean? I think distance and metric are commonly used interchangeably. Explain specifically what is meant by extending to negative values.. To some extent. What reference are you using for this classical definition?\nFrom I have read after checking shortly out there, a use case of this Chi-2 distance is to calculate distance between histograms. In that use case the values are positive of course. I suppose that people found other use cases where values are not necessary positive. I suspect that this dichotomy you have observed (definitions with and without the absolute value) may come from there.\nIt is fine for me if you want to leave the text as it is. I would not play with saying the distance extends the concept of metric to negative values since because I find it rather vague and potentially misleading.. I think it should be fine for defining n if you just put that the features belong to R^n the first time you introduce them. By the way, for extra mathy happiness, use \\mathbb with R :nerd_face: . Would it make sense to have it in a common place?. It is good to be consistent.\nAbove you are doing rate->set_const_learning_rate(0.01) and here instead you declare and define num_passes before passing it to the setter.. Can this StanVector* be a const StanVector& instead?. If kept as *, there must be a check for nullptr.. Nitpicking, it'd make sense to just have the lambda here.. Ok. So, I would like to understand this pattern based on declaring a function that returns a std::function, and the function's definition is a lambda. It feels like it is over-complicated.\nWhy not simply defining a function? Please explain.. No. That comment refers to all the lines below, where x and y are initialized.. Just use a function that returns a 'var' (instead of std::function). For the definition of the function just use the logic in the body of the lambda.. Not sure if this relationship between StanFirstOrderSAGCostFunction and FirstOrderSAGCostFunction makes sense.. I think that having a reference member makes default-constructability odd. This might be an issue for generic features such as the parameter framework, clone, etc.. FirstOrderSAGCostFunction does not have members for the training data. Why are they needed here?. I think a reference member should not be used but maybe it is ok.. No. I mean that the data should be defined only in one place.. Any potential side-effects adding it? Maybe we can add it and run the tests, examples, etc, to get a feeling of it.. The syntax to initialize this StanVector is a bit awkward. Can you do better? For example with something std::vector-like it could just look like std::vector<var>{x1, x2, x3}.. Refactor.. Use a smart pointer instead (either Shogun's Some with a SGObject or std::unique otherwise). Then the delete at the end goes away.. This is a case where auto really makes sense actually.. No deletion for this one and the ConstLearningRate?. Please have a look at initializing Eigen::Matrix in Eigen documentation.\nhttps://eigen.tuxfamily.org/dox/group__TutorialAdvancedInitialization.html. What do you mean?. As mentioned in another earlier comment from Heiko iirc, is \"NOT IMPLEMENTED!\" \"inherited\" in the sublasses documentation (where this method may be actually implemented)?. Perhaps it is only a naming issue, but saying that a LogisticLayer is a LinearLayer sounds odd.. Minor typo: constructor.. Missing Stan prefixed in the name?. Did you mean destructor of StanNeuralNetwork?. I am not sure these ones should be here. Let's check with @karlnapf or @vigsterkr. . This looks strange. The signature in the header is StanVector& get_layer_parameters(int32_t i);.. What do you think about a test case that asserts laziness?. Any plans to fix?. We must be careful and we must no add unnecessary relationships. If it turns out that the training data is a member both in the cost function, and in the neural network, and in etc etc, that is going to cause lot of usage confusion.\nIt does not sounds unreasonable that the tests have a facility packaged to prepare input data and avoid code duplication (e.g. inside a class such as the CRegressionExample you are mentioning). What did you find wrong with it?. Any news about this concern?. This must have slipped through.. Not necessary or was implicit include?. Minor: fix indentation of this block to rest of the function?. For consistency in the computations of features and labels, is it possible to do some linalg::mean of features?. So the model contains both w and b and what we are doing is first find w, ignoring that b exists, and then find b provided the found w.\nIs this standard practice in linear regression? I am a bit surprised that the equations to find w do not depend on the b since the gradient of the loss wrt to w has b.. Ok. My question was probably wrong. Even if b does not appear explicitly in the expression for w, that does not mean it is not in there. It has just been substituted.\nHowever, then the expression for w should be in principle different for the cases (1) w/ bias; (2) w/o bias. No?. Is it one as said above? Then change or remove \"all\".. Missing ]?. Why does the API contain these two builder and pipeline concepts? What about just a pipeline where steps are added (and of course the order of addition matters).. I understand that there might be mutability reason justifying two types. Why both part of the interface though?. Would a linalg::mean(feats) would make sense from the point of view of API happiness? :-P. Cast more solicit perhaps? If there has to be cast using T I think it should be more fair to do static_cast.. Explicit, not solicit..... Autocorrectors xD. Why the parentheses between T? :-). Maybe a specific error message pointing at the mistake? Bias not available for this feature dimension. Try using less number of features than data samples.. All righty! Do we have already some case using these feature iterators?. Completely minor: missing newline.. Would it make sense to call it noncentered_cov or something similar that makes the fact that is not centered explicit?. I think having a function called covariance that does not compute the covariance can lead to confusion :-P. Out of curiosity, having the local x is better than just using b[j] all over the method? Same question for update.. Minor: I think for this method the the A = LL^T can be left out.\n. Quite nitpicking: perhaps a different name for the L as they are not the same as the ones above. In wikipedia they use tilde analogously for A. It is completely subjective I think, whether we want the doc to be more math correct, or just reflect what happens in the code. I am happy either way :-). Just out of curiosity, std::is_same_v would make it here as well instead of is_any_of_v, yes? No need to update anything.. Nice questions, @saatvikshah1994 :-)\nI'd say the actual reasons are mostly due to history. Shogun has been around for long time, longer than what we consider today modern C++. Also, I personally think the focus and interests have also moved a bit from \"hardcore\" machine learning programming to software engineering.\nIt is of course always good to follow the latest trends and guidelines of C++. I've found however that as a project grows and gets bigger such redesigns and/or refactorings become too time consuming without having a clear value in terms of bringing new features. Technical debt.. About your comment on SGMatrix's API. Note that the main SG* guys, SGMatrix and SGVector are referenced data, so there's no need for handmade memory management with those ;-). About the initialization of CRandom that Heiko mentioned. It has to do with objects residing in the stack vs the heap. The rule-of-thumb I started using at some moment was that any SGObject has to be heap-allocated.\nI forgot the exact technical reason. Stepping through a minimal program with gdb could help understanding. But anyway, very vaguely from what I can remember: it had to do with the reference counting mechanism taking place as scopes end.\nAnyhow, I think should be able to follow the CPP core guidelines for this initialization: just wrap your SGObject inside a shogun::Some (aka std::shared_ptr).. Somewhat minor, does the doc propagate well to the functions below in the Doxygen?. Give it a pass with the style checker.. The template <typename U=T> part feels a bit redundant. Is there a way to do it without introducing the auxiliary U?. Let's do it at once with the approach to set the seed. I believe it is crucial or at least quite helpful for unit testing.. Of course here and in the other functions as well.. Nitpicking a bit. Maybe we can put the concepts in a separate sg_concepts.hpp file or rename to something else that includes traits and concepts. . Yeah, I find it a very nice advantage for programmer happiness. Great stuff for shorter and more clear compiler error messages as well.. Good stuff, thanks a lot for the work!\nFor my own understanding, do you have a specific reference to another concept that is defined similarly to this one you have created here (for example, a concept defined in the range-v3 library)?. This is a bit of a strange construct. You are passing a function argument to actually only used its type information (which in turn requires you to use the decltype). This is asking for a template.\nCan you use a template lambda?\nOtherwise, I'd prefer an auxiliary function outside the TEST body.. Looking forward to see it :-). Hey mate, I can't say much based on this snippet. What's your point? Also based on the last line, CrossValidation here seems to be a type of MachineEvaluation, no?. All right! Thanks for explaining :-). That seems interesting indeed. I wonder whether this is actually expected behavior or some (potentially current) limitation or bug. It could be worth opening a conversation about it, if there's none yet (just an e-mail should do or even a short blog post illustrating the issue).\nFor the purpose of this pull request, I say we keep it as you solved the problem. Very nice job debugging it and explaining here, thank you!. I think capturing this makes the code more self-explanatory. I'm curious, any reason to prefer this syntax? I wonder if this [&attribute = attribute] is documented somewhere actually, I only found it mentioned in stack overflow.. That's a bit the problem also, if it is not well documented, it is a bit trickier to figure out if it is captured by reference or value ;-) Anyhow, I guess that the leading & indicates it is by reference.. ",
    "jklontz": "Thanks for the very helpful feedback! I'm going to close this ticket for the time being as it's become evident I need to rethink some parts of the patch, primarily related to signals and BLAS integration, and I'm not sure when I'll have time to do so.\n. Thanks for the very helpful feedback! I'm going to close this ticket for the time being as it's become evident I need to rethink some parts of the patch, primarily related to signals and BLAS integration, and I'm not sure when I'll have time to do so.\n. Sorry this fell of my priority list, will resubmit the straightforward changes soon.\n. fixed!\n. fixed\n. oops, editor was using spaces instead of tabs. probably have many whitespace mistakes. will correct :)\n. fixed :)\n. Oops yeah, this is clearly wrong. Not sure how it compiled to begin with :) Will need to rethink this.\n. ",
    "gsomix": "Oh, no. I found a mistake of working with strings. I will correct it.\n. Done.\n. Done.\n. Done.\n. Done.\n. I just read this discussion here: http://www.mail-archive.com/numpy-discussion@scipy.org/msg29403.html\n. done\n. done\n. Hmm? Is that right?\n. forgot to freed m_refcount\n. done\n. forgot\n. fixed\n. fixed\n. fixed\n. fixed\n. fixed\n. done\n. PYPROTO_DENSEFEATURES?\n. I'm trying to figure out how avoid this temporary buffer.\n. ok, I'll check\n. ok, I'll check\n. this method is needed because of protobuf's streams works with posix file descriptors\n. only for int32_t now\n. needed because protobuf doesn't work with multiple messages\nhttps://developers.google.com/protocol-buffers/docs/techniques\nI think we need sth like \"splitted/full vector\" flag for load and save\n. only one chunk for now\nneeded strategy how split data (e.g. N 1MB sized chunks)\n. ok\n. ",
    "puffin444": "Ok, I'll think I'll go ahead and instead write up a basic implementation in shogun/regression. Thanks for the feedback!\n. 1. Is there an EYE function in shogun/lapack/blas?\n2. You mean use Cholesky to solve the system (K+I)x=(K)*y, yes? I can do this with the \n   clapack_dpbtrs function.\n3. I go ahead with the marginal variances. Testing all this may take a few days though.\n. How does this look? I changed a lot of stuff around. I am using the Cholesky decomposition-based algorithm for mean predictions and the covariance matrix from Rassmussen's textbook. I tested it with valgrind and I don't see any leaks. Please let me know if you find any formatting errors/design issues.\n. Oh no. Where does it say a \"Live Session\" submitted a revision? For a few days I developed using a usb key linux distro, but I thought I made sure the git settings had the user as \"puffin444\" and email as walke434@msu.edu\n. 1. Enums are now added for downcasting the EvaluationResults.\n2. Fixed copyright notices\n3. Used CMath function for random number generation.\n4. Overloaded set_parameter for different types.\n5. Added a simple GradientCriterion. Now we can maximiize and minimize. \n6. Fixed the get_name issue.\n7. I modified get_random_combination method to a method that may (or may not) return a randomized \n   tree based on an input parameter. If it is not random, it chooses the lowest possible value in the array. \n   This way I am able to customize the lower bounds for specific parameters.\n8. Maybe tests will reveal this valgrind wierdness for CMap. Do I add them in the examples folder? By the way Heiko, all of the examples that I could run using the framework had no memory leaks. Just to let you know there was one that did not run because the dataset was not available, and another example which appears not to be updated with the rest of shogun which the makefile does not compile. (The modelselection krr one). Other than that, everything leak wise looks fine.\n. Okay. Now the hashing uses the murmur incremental hashing function. There is no extra copying involved. I also added a fix for the python example.\n. Okay. So I found out that the current incrementalmurmur hashing implementation was not a true implementation\nof the murmur hashing algorithm. It was also architecture-dependent. I found a portable implementation of Murmur3, the latest version, on Smhasher: https://code.google.com/p/smhasher/source/browse/trunk/PMurHash.h. This implementation supports incremental hashing; the only requirement is that the total length of the data must be saved along with a \"carry\" number which is used to calculate the final result at the end of the hashing. It seems to work well. My only concern is in HashedWDFeatures and HashedWDFeaturesTranspose. I am unsure what the true intention for the incremental hash was in this program. It seems to be used to calculate some indices for vectors. Valgrind reports no memory leaks with regression_gaussian_process, but it does report 3 seemingly nonsensical read errors. \n1. It complains about an uninitialized value at CMap.h:355. This makes no sense because this pointer is set\n   to NULL and remains NULL by the constructor. This existed before the addition of Murmur3\n2. It complains about an uninitialized value at  SGObject.cpp:247. It is impossible for the two values being compared to be uninitialized. The first one is initialized in init(), and the second is initialized at the beginning of the function. What's weirder is that it fails to complain about the very next line, which uses both variables. \n3. Invalid read at get_parameter_incremental_hash(). I also investigated this, and I found that it whines when iterating through certain vectors/matrices. What makes no sense is that it seems to happen the first time the function is called, then on subsequent calls everything is fine. get_parameter_incremental_hash does not modify the parameters in any way. The data sizes it uses to compute the byte length of vectors are correct, and the strangest thing is that the fault occurs in the middle of the m_parameter array, not at the ends, and the number of faults differs whether I am outputting the data or not. \nWhat do you think may be causing these three bugs? Do you think there may be a problem with the way my OS/Hardware is handling memory?\n. Okay. Memory problems 2 and 3 have been fixed. \n. It appears that under TParamter, the m_parameter variable may either be a pointer or a pointer to a pointer depending on the actual type of the data being held. I originally was treating the variable always as a pointer.\n. It appears that under TParamter, the m_parameter variable may either be a pointer or a pointer to a pointer depending on the actual type of the data being held. I originally was treating the variable always as a pointer.\n. 1. Fixed the allocation stuff.\n2. Okay.\n3. The problem is that the InferenceMethod needs to use the feature matrix directly for some calculations.\n4. Okay.\nI took out some of the vector code from this pull request because as Heiko pointed out, it needs some polishing. It will be coming back though as now I need to use it for ARD in linear kernels. \n. 1. Fixed the allocation stuff.\n2. Okay.\n3. The problem is that the InferenceMethod needs to use the feature matrix directly for some calculations.\n4. Okay.\nI took out some of the vector code from this pull request because as Heiko pointed out, it needs some polishing. It will be coming back though as now I need to use it for ARD in linear kernels. \n. Okay, I cleaned up some of the ASSERTS/if statements and now I use SGVectors. It might be a good idea though later to consolidate this code through inheritance somehow, as the code between LinearARD and GaussianARD still near identical.\n. Okay, I cleaned up some of the ASSERTS/if statements and now I use SGVectors. It might be a good idea though later to consolidate this code through inheritance somehow, as the code between LinearARD and GaussianARD still near identical.\n. Okay. Cleaned some stuff up. What do you think?\n. Okay. Cleaned some stuff up. What do you think?\n. So get_kernel_matrix allocates a new matrix every time it's called? Oops sorry about that. I'll try to use valgrind to look for memory leaks. Does dsymm have a performance advantage over dgemm?\n. Oops I think that should be features->get_num_vectors() to make sure there are as many training labels as there are\ntraining examples. I'll try to fix that.\n. I need it to extract the initial values (and the lower bounds) for NLOPT.\n. How'd that become public? That should be private :)\n. Fixed all this in an update.\n. Cleaned this up.\n. I messed this up. I changed some things around so now it returns false correctly when\nthe parameter is not found.\n. Fixed.\n. Fixed.\n. Fixed.\n. Fixed.\n. Do you mean stuff in the examples folder?\n. Fixed.\n. Fixed.\n. Went through all my code and fixed this.\n. Fixed this.\n. Fixed this.\n. Thanks. I have not thought about modular interfaces yet.\n. Went through all my code and fixed what I found.\n. I hope to eventually encapsulate the mode selection code under here, so all the user needs to do is execute this\nfunction for hyperparameter learning.\n. Do you mean just do    CDenseFeatures* features=new CDenseFeatures(); ?\n. ",
    "harshitsyal": "i dont know why files ending with ~ are being added ,ex i made changes only to Classifier.i but i dont know why Classifier.i~ is being added.\n. i dont know why files ending with ~ are being added ,ex i made changes only to Classifier.i but i dont know why Classifier.i~ is being added.\n. Also in course of converting matlab code to shogun i made a lot of math functions that were not present in CMath.h like :\n->Creating diagnol matrix,given the vector to be placed on the diagnol\n->Creating a subset of matrix/vector given the indices of rows that needs to be in final matrix: does the job of Array(indices,:) in matlab.\nand a few more \nDo you want me to incorporate those in CMath  so that every one can use them ? \n. Also in course of converting matlab code to shogun i made a lot of math functions that were not present in CMath.h like :\n->Creating diagnol matrix,given the vector to be placed on the diagnol\n->Creating a subset of matrix/vector given the indices of rows that needs to be in final matrix: does the job of Array(indices,:) in matlab.\nand a few more \nDo you want me to incorporate those in CMath  so that every one can use them ? \n. Done.. and Please let me know if you want me to make some changes ..\n. Done.. and Please let me know if you want me to make some changes ..\n. Actually there is one concern with the code right now..\nthe value of line search in the last iteration doesn't matches the values in mablab implementation.And after hours of careful examination i think the problem is due to the precision of double.\nWhat i mean is that in the last iteration the values of h and g in line_search_linear becomes so small( about 1e-30)  that cblas_dgemm returns zero.\nBut problem is that we have to take the ratio of h/g which gives some value near 2-2.5 (because h and g are of same scale)\nOnly way to solve this problem is to use floatmax_t but cblas_dgemm doesnt operates on that .. Do you know any other way out ?\n. Actually there is one concern with the code right now..\nthe value of line search in the last iteration doesn't matches the values in mablab implementation.And after hours of careful examination i think the problem is due to the precision of double.\nWhat i mean is that in the last iteration the values of h and g in line_search_linear becomes so small( about 1e-30)  that cblas_dgemm returns zero.\nBut problem is that we have to take the ratio of h/g which gives some value near 2-2.5 (because h and g are of same scale)\nOnly way to solve this problem is to use floatmax_t but cblas_dgemm doesnt operates on that .. Do you know any other way out ?\n. sorren, Everything you told me to do is done, now only thing left is to use COFFIN framework functions rather than get_computed_feature_matrix,\nI am working on it,  but i am not sure whether i would be able to solve this problem . \n. sonney2k- sorry for some delay, Actually i didnt get time last 2 days,\nNow i have changed almost every code which was making use of whole feature matrix .. And as suggested by you I made use of only add_to_dense_vec and dense_dot only..\nCheck it out !\n. Is there any problem in code or output, That I can rectify ?\n. okay soeren, will commit new changes in a while,\nAlso do you want me to include the matlab example i prepared for testing on contypebianry_splice dataset?\nAnd on contypebinary_splice dataset it takes around 6-14 sec to train on whole dataset, Time varies on amount of memory available i guess.\n. Changed ! now you can merge it !\n. I think, its not the best place to say .But can you please give me some feedback on my gsoc proposal. I think its not perfect and needs a lot of modification !\n. hi blackburn and soeren,\nI am afraid, it has become really hard to debug TSVM.cpp.There seems to be some hidden problem embedded inside the the svmlin library in shogun..\nWhat I mean is that, I haven't changed ssl.cpp(file having real svmlin code in shogun) except adding 2 functions which according to me are functioning perfectly and also I am submitting the correct parameters to ssl.cpp through TSVM.cpp but still the final weights obtained are not exactly same as the weights obtained by original svmlin library.\nWeights from shogun and original svmlin are not exactly similar but have some similarity(like have same sign and magnitude).I dont know why that is happening, Maybe some internal error inside ssl.cpp which is pretty hard to debug.\nSo should I have a real check on it, as in, test it on real task and compare accuracy with its original counterpart.\nJust in that case if TSVM.cpp achieves similar accuracy can we integrate it in shogun(without the results matching from original library) ?\n. Hello soeren,\nI have checked the parameters a lot many times, I think there is no problem there. \nproblem seems to be inside ssl.cpp, which I am not able to figure out. The reason why i am saying that there is a problem in ssl.cpp is coz svmlin in shogun is also not giving exact same weights as orignal svmlin ( and I am pretty sure that parameters are same in both).\nI think ssl.cpp needs a lil review !\n. Done, didn't know about that earlier. :) \n. \" please document these paramters - what is l? what itr? \"\nDone..!\nyes C is 1/l but in matlab code it wasn't used anywhere so i didnt include that in my code, Now i have included C as well\n. Done\n. sry\n. sry\n. done ..\n. done\n. do you want me to include a function in math.h which can make a subset matrix/vector out of a matrix/vector given the indices of rows to be kept in final ?\n. sry .. Now i have indented my whole code .. have a look\n. okay but is that necessary, i mean i always followed the other style.\nbut for now i have done what you said\n. I have to use this matrix in various computations of cblas_dgemm,etc so i dont know how to do that in any other way.\nBut i'll work on it and will inform you if i find any other problem.\n. one problem:\nwhat value do i need to assign to CT_NEWTONSVM in machine.h . For now i have assigned it a random no.\n. thanks, for reminding about calloc i completely forgot about that..\n. Done .\n. I dont knw what's the problem here, in my editor it is perfectly fine.\n. ",
    "sridif": "Hi Sergey,\nSIFT is a local descriptor. The problem : every data point (image) has non equal number of attributes (SIFT descriptors). This is solved by the bagofwords approach. As a result of using this approach we will have equal number of attributes for every data point.\nSo the aim is to take in unequal features and give out equal features, as is your last GSOC idea \n\"Implementation of Boosting type learning method for non-constantly dimensional features defined as sets of fixed dim vectors with varying set size. \"\nThe code implements a simplified version of the following\nhttp://www.cs.unc.edu/~lazebnik/spring09/lec18_bag_of_features.pdf\nSo the aim of the code is to just create features. But the code lacks a rigorous implementation of theory as outlined in the pdf.\nAlso, Can u suggest me some ideas for another patch ? (I am sure this did not do the job ;) )\n. Hi Sergey,\nSIFT is a local descriptor. The problem : every data point (image) has non equal number of attributes (SIFT descriptors). This is solved by the bagofwords approach. As a result of using this approach we will have equal number of attributes for every data point.\nSo the aim is to take in unequal features and give out equal features, as is your last GSOC idea \n\"Implementation of Boosting type learning method for non-constantly dimensional features defined as sets of fixed dim vectors with varying set size. \"\nThe code implements a simplified version of the following\nhttp://www.cs.unc.edu/~lazebnik/spring09/lec18_bag_of_features.pdf\nSo the aim of the code is to just create features. But the code lacks a rigorous implementation of theory as outlined in the pdf.\nAlso, Can u suggest me some ideas for another patch ? (I am sure this did not do the job ;) )\n. Sergey,\nWill use a real dataset and use it for classification.. and get back to u. \n. Sergey,\nWill use a real dataset and use it for classification.. and get back to u. \n. Tat was exactly my idea.\n. Tat was exactly my idea.\n. Can u suggest me another patch idea ? \n. Can u suggest me another patch idea ? \n. Oh, The code also generates a random sift like data and displays the output at every stage. ;)\n. thanks for the feedback.\n. ",
    "pluskid": "Fixed the test data generation code as suggested by sonney2k.\nI also submitted another not-so-trivial patch for GSoC self-introduction, by adding the accuracy computing support for evaluating clustering result, as well as a Python code demonstrating how to use it.\nBTW: the munkres code for computing the best map is ported from a GPLv2 code.\n. I will close this pull request and create another with correct commit range\n. a feature? :p\nOn Sun, Apr 1, 2012 at 4:59 PM, Sergey Lisitsyn <\nreply@reply.github.com\n\nwrote:\nHowever it is not a bug actually :D\n\nReply to this email directly or view it on GitHub:\nhttps://github.com/shogun-toolbox/shogun/pull/407#issuecomment-4865104\n. Hi, currently the LARS is in a somewhat finished state. It includes LARS/LASSO with early stopping support. The python_modular interface is created and an example (with nice figure showing the LASSO path :p) is also included. Please review and merge this. Any comments is appreciated.\n\nBTW: I found strange thing in the shogun's ridge regression. I'm comparing LASSO with ordinary least square in the python example. The MSE of the lasso solution at the end of the path (all variables are selected) should be the same to that of OLE. However, when I run the program multiple times (random number seed is fixed), the OLE sometimes generate results with very high MSE. I thought it might be because of numeric instability of LeastSquaresRegression, but when I replaced it with LinearRidgeRegression, the problem was still there. Anybody has an idea why is it that? Anyway, I'll try to investigate the problem later.\n. I found the reason of weird OLS results: the matrices are not initialized in CLinearRidgeRegression.\nSo here comes a simple bug fix for CLinearRidgeRegression. Now the result becomes normal.\n. Hi, lisitsyn! Thanks for your fix! But concerning explicitly calling the default constructor of the base class, I don't think it is necessary. The C++ standard guarantee that when a derived class constructor does not explicitly call the base class constructor in its initializer list, the default constructor for the base class is called (implicitly). See for example wikipedia http://en.wikipedia.org/wiki/Default_constructor . You can have a try with your compiler in some simple test (e.g. http://snipt.org/uhZh6) if you don't believe it. :p\n. The variable is a constraint_generator for MKL. The code for MKL seems to require a CSVM * for constraint_generator. However, I don't know much about MKL, so I'm not sure whether multi-class SVM can be used as a constraint_generator in MKL. If it is true, we might also modify MKL to use CMachine and cast to either CSVM or CMulticlassSVM, but this could be ugly.\nAdopting similar ways as those used in features could be good solution. But I think that would be a bunch of refactoring work. Shall we open an issue on the issue tracker or a thread in the mailing list to discuss about this problem?\n. BTW, I scanned the IRC log, blackburn is right for why I'm making MulticlassSVM a friend of SVM: I need to access some protected variable of CSVM (actually, base class of CSVM), svs here. In a hurry, have to leave now, will discuss with you after I come back (or if I could find internet access there ;) )\n. @lisitsyn that was what I initially did -- and failed with some strange error of the SWIG python modular. However, I think later I figured out the reason is that the order of the \"include\" statement in the .i files matters very much. So I'll fix this again, tomorrow. Just back from the trip, too tired to work this night. :)\n. @lisitsyn oops, I didn't see the \"(for me)\", so I do the naming convention fixing. I also moved CMulticlassSVM to multiclass folder. But I didn't move its descendants. You can do that if you think necessary (after this is merged).\n. this is discard, see another pull request.\n. @lisitsyn I have rebased and made changes according to the new subset system just commited. check-examples runs OK at least in my local box.\n. @lisitsyn no, there shouldn't be zero in the final stage, it should be a bug. I'll check it today. Thanks for finding this! :)\n. @lisitsyn could you please tell me how to reproduce this? BTW: many python_modular examples are failing, errors are similar: TypeError: Cannot create new instances of type 'MulticlassOCAS'. It seems to be something wrong with SWIG again. Do you know how to fix this?\n. Hi, I refined the prototype to support confidence. Instead of \"probability\" as suggested by @karlnapf , I use \"confidence\" to indicate a more broader concept. For example, we might use the decision values (not necessarily within [0,1]) as \"confidence\".\nCurrently:\n- BinaryLabels embed the confidence value in the label (with float64_t), sign(val) is label, abs(val) is confidence\n- RegressionLabels do not support confidence. Is confidence useful in regression? If it is needed, we can store the confidence as an extra information.\n- MulticlassLabels store the confidence separately.\n. use size_t temporarily (will replace with index_t when std::vector is replaced with SGVector)\n. @sonney2k thank you for your demos, here's the updated code, making better use of CTime.\n. @sonney2k here comes!\n. Good idea!\n. yes, good idea, I will do this.\n. Hi, thank you, I have fixed the other two suggestions. But concerning replacing std::vector with SGVector, I do have some problems here.\nActually, I don't know how the SGVector object should be used. First of all, it is not ref-counted, so we should use it as ordinary containers. However, I'm a little confused about its design. For example, it sometimes allocate the memory for itself, and sometimes simply re-use the memory from the outside or another SGVector object. But in neither case, it do not release the memory at destructors. We have to call a function explicitly to release the memory, and should be very careful about where the memory comes from for each particular SGVector. So I'm very hesitate here to use SGVector, I'm wondering whether there is any conventions on how this object should be used to avoid common memory errors? If there's any, it would be very helpful for me (and potentially other developers). \nBTW: I also saw at the ideas page that SGVector is going to be re-designed with ref-counting support. I would be very interesting to know, if any of you could explain something about why it is designed this way, and in what way do you want the future SGVector looks like? Thank you!\n. OK, thank you! I have updated to use the built-in get_unique_labels()\n. Hi, thanks for the comment. That's on the TODO :p. This is only a working prototype, I'll make it more native to shogun in the following days. :)\n. Thanks, I have removed this. :)\n. Hi, why impossible? Is it a convention? I followed that of CMulticlassMachine, so shall we also change that in CMulticlassMachine?\n. Because multiclass_type is no longer a variable of CMulticlassSVM, but one variable of his base class CMulticlassMachine. I think it might be his base class's responsibility to write and load this variable. However, I'm not sure about this, so I commented out instead of deleting it. If you think it should still be read/write here, I can uncomment this and change \"multiclass_type\" to \"m_multiclass_strategy\". :)\n. Thanks! I've deleted these code.\n. OK, I will fix this tomorrow.\n. @sonney2k There's a small problem switching from SGVector to DynArray. It's mainly due to MulticlassMachine::register_parameters, who need to register the m_machines variable. However, DynArray is a more encapsulated class than SGVector: it's memory buffer and size variable is not directly acceptable from outside. I don't think making them public or making MulticlassMachine a friend of DynArray good ideas. Do you have any suggestion here?\n. @sonney2k thanks! CDyamicObjectArray seems to be a better choice, with automatic REF/UNREF. However, the problem is still there. How should this statement be re-written when we use CDynamicObjectArray for m_machines:\nm_parameters->add_vector((CSGObject***)&m_machines.vector,&m_machines.vlen, \"m_machines\");\nthis is in the register_parameters function of CMulticlassMachine.\n. Thanks, I have fixed this. :)\nNow please review again for merging.\n. Hi, I updated, and please review again.\nConcerning this and related problem, there is actually a minor issue. In order for the strategy to know num_machines, it should know num_classes. We have three options:\n1. tell the strategy num_classes in constructors: this is inconvenient because we do not know the number of classes when constructing classifiers, until we get the labels\n2. tell the strategy num_classes when training: this has the problem that several true multiclass machines do not use the strategy object to train, so the strategy object get no chance of knowing num_classes\n3. tell the strategy num_classes as a parameter: this is what I'm currently using, and I'm not very satisfied with this either. But it seems this is the current best solution.\n. @sonney2k hi, what is \"urgs\"?\n. here comes the non-STL version. @vigsterkr why CMap doesn't have a function like set_element(key, val) or so?\nbtw: I'm still wondering what \"urgs\" mean...\n. It's the config file for a vim plugin clang_complete. I added it to .gitignore to ignore it from git status. If you don't like it, I can remove this.\n. see binarylabels, it stores with float64_t (decision values) but returns with int32_t (+1 & -1, can even be made bool if you like).\n. Thanks, fixed!\n. @sonney2k for numerical types, static_cast is equivalent to type(x), and static_cast has the advantage that it is more identifiable in the code (e.g. via grep).\nThe vector passed for computing hamming distance should be guaranteed to only has value 0, +1 or -1. Or else the results are undefined. If the parameters are already invalid, adding Math::round only gives extra computation overhead but will not make the result right.\n. Thanks! I have fixed this.\n. @lisitsyn This class is more \"empty\", it allows more flexible control over how the multiclass machine is trained and applied. While CMulticlassMachines abstracts common patterns that includes OvR, OvO, ECOC, etc, we need more flexible control for CPT -- both tree-structure and online, quite different from previous MC machines in shogun.\n. fixed\n. fixed, thanks!\n. @sonney2k Hi, updated!\n. Because the label for the dataset is embedded in the feature file. I'm using the streaming features to extract the labels.\n. Hi, the problem is that I will have to create Kernels in the run-time. Since I do not know what the type of the kernel the user supplied, I have to use the kernelfactory. Or I do not know what to new. If you know any better solution here, please tell me.\n. Hi, I tried to modify this. But I failed because this will introduce circular dependency for CMath and SGVector. Any suggestion for how to fix this?\n. Theoretically, any SVM will work, or any binary kernel machine will work. But since I'm creating the machines myself (instead of using some machines from the user), I know I will not create any other kind of machine other than libsvm. If I want the flexibility here as to allow any kernel machine, I think I will face the same dilemma here as KernelFactory.\n. If CKernel has a clone() method, I can do this. Because I have to create multiple Kernel instances. \nYes, the built-in KernelFactory can only use default parameters. If the users want to customize parameters, he has to write a subclass of the KernelFactory himself. This is not an elegant solution, but I think this is currently the most straightforward solution. I have written an email to the list to explain my struggling for this. Hope I clarified the problem clearly. And I'd be very glad to see anyone come up with a better solution for the whole thing.\n. I'm afraid this won't work. Forward class declaration only tells the compiler there is a class named this, so that you can use some limited things like the pointer type (CMath *). But you cannot do things that depends on the content of the class definition. For example, calling CMath's method isn't allowed here because the compiler doesn't know what method CMath has now (by merely seeing a statement \"class CMath\").\n. Hi, I'm sorry for the typo. This is fixed now.\n. Now I updated to use the CMulticlassEvaluation's conf mat computing.\n. the Relaxed tree is formulated on top of binary SVM. So it requires the sub-machines to be SVM and it uses the SVM parameters in its own algorithm. That's why I'm making those parameters for the RelaxedTree machine.\n. the objective function in the paper (eq (1)) is really big and complicated. I tried to add some text description here and reference back to the paper. Hope this could be helpful.\n. Oh, sorry. I remember I fixed this, no idea why it is still there.\n. ",
    "shelhamer": "I have tried to do related areas as separate commits so that they can be checked independently. I would welcome any suggestions for this process, if the current way forward is in some way undesirable (I'm new to committing to the project after all, and don't want to make assumptions).\n. I rebased this feature branch again so that there should not be any conflicts merging. I took care of the m_parameters->add() calls noted by Soeren, replaced the remaining calls in kernels, and adapted the kernel normalizers as well.\n. ",
    "ostegle": "Great. \n1.   I am not aware of one, but probably worth creating. \n2.   All the inverses can be solved.\n   For example: \n   2.1 Mean prediction\n   \\hat{y} = K (K+1)^{-1} y \n   You can use the Chol (Keff) = chol (K+1) for most of these operations and then use the solve functions (DPBTRF).\n   For example:\n   Keff^{-1} y = alpha\n   <=> y = K * alpha\nYou can use:\nfile dpbtrs.f  dpbtrs.f plus dependencies\nprec double\nfor  Solves a symmetric positive definite banded system\n,    of linear equations AX=B, using the Cholesky factorization\n,    computed by DPBTRF.\ngams d2b2\n1. Marginal variances\n   Same for marginal variances. You can grab the exact details from my gp_base.py\n. The calculation inverse solves is needed more than once.\nAgree it is not the case in this example but for LML calculation and gradients you can reuse the same Chol several times.\nFine with creating Eye manually, but does not seem like pretty code.\nIn the next step we need a dedicates class for noise models anyhow, so probably no real practical issue.\n. Thanks, Green light from my side.\nLooks like you already did a good chunk of the work that was planed for the summer.\nExcellent.\n. ",
    "AlexBinder": "My opinion: the code should not be accepted. The code is very sloppy, has multiple severe faults. Does not scale beyond toy settings. Such a code was never used in a half-way realistic setting.\n1. Logical fault: the visual words are thrown away (line 175). Beginner's mistake.\n   For a new batch of data, e.g. testing data this code would recompute the visual words. Then for training and testing data the i-th dimension would correspond to another region is space and the i-th histogram's dimension cannot be compared between training and testing data or two batches. \n   For medium size data sets like VOC2011 the e.g. rgb sifts in realistic setups cannot not fit into memory, so there is no way to compute BoW features for such datasets with this code.\n2. memory infeasibility line 104: compute a matrix for many ten thousands or hundred thousands of descriptors? \n3. runtime infeasibility: off-the shelf clustering is computationally infeasible: 2000 images, 5000 RGB sifts per image = 10 Mio features (that is a low estimate) 3x128 dims per feature -> how long does that take to cluster? Months?\n   clustering needs control on the input size (memory) and the runtime (to finish after a max time). Can't think of anyone would iterate until small eps in a several hundred dimensional space.\n4. complete?? What about various ways to generate visual words? What about the many possible soft code book mappings? \n5. style comments line 77: integer division without check, code assumes anyway block structure but\n   if num_cols !=dim_descriptor, this is most likely an user error -> error prone\n. Hi Soeren I know about it by an email from Eric. I have even a version which fixes it - for 0.10.0 . when I have three days to get into the git stuff, then I will do it.\n. ",
    "ptillet": "Hello!\nWell, there are several reasons. First of all, ViennaCL is a very light dependancy as it is a header only library, and requires zero other dependancy (except OpenCL). It is released on MIT license which is as far as i know GPL Compatible , and it has been made and tested on MacOS, Linux and Windows :) \nAlso, Shogun massively uses linear algebra operations, and ViennaCL is precisely designed for that ( It also implements blas1,blas2 and blas3, iterative solvers. It works for dense matrices, sparse matrices, etc..., it would work with sparse-features out-of-the-box !). It is under active development : updating the ViennaCL version used in Shogun might improve the performance without requiring any efforts :D For now I am using my own OpenCL compute kernels, but in a few months ViennaCL will probably be able to generate the compute kernels I have written, and in the future enable custom_kernels on GPU for Shogun without requiring the user to know anything about OpenCL :)\nAlso, ViennaCL is part of the GSoC this year, and I think the project will be about Eigenvalues solver (dense or sparse), which would also help for hardware-accelerated dimension reduction! ;)\nAnother reason is that I am a contributor to ViennaCL and that I am familiar with the internals etc...\n. I agree .\nHowever, I don't really know how I would implement this, as I am adding methods to a class and as it is not possible in C++ to declare a class in multiple .h\nThe only option I see is to replace methods with function, which would make things pretty not intuitive ... Adding some other classes such as \"OpenCLFeatures\" would probably add a lot of complexity and might make the use of OpenCL not transparent :/\n. You are right. Should be cblas_dsdot\n. Shame on me :D Copy-pasting spotted. Fixing these comments.\n. Hmmm, I tend to use ++x , no particular motivation. I'll fix that.\n. You are right. I rewrote them without realizing it ... I'll fix that\n. in the CMachine code, there is :\nbool result = train_machine(data);\nif (m_store_model_features)\n    store_model_features();\nand in init() , set_store_model_features(true);\nIt seemed more semantically correct to put this code into store_model_features(), i've tested it and it seems to work.\nHowever, if you judge it better at the end of train(), then i'll put it back there\n. ",
    "koenvandesande": "Oh, or could it be that the function has been renamed to add_subset?\n. Looks like somebody beat me to it :)\nThe problem I was having was that my dtype was int64 - I thought it should be that one given that it was size_t. Now creating a subset works.\nHowever, there's still an issue with actually using the subset; the following 5-line example gives an error:\n```\nimport numpy\nfrom modshogun import *\nfeat=RealFeatures(numpy.array([[0,1,2,3],[0,1,2,3]], dtype=numpy.float64))\nsubset=Subset(numpy.array([0,1,2,3], dtype=numpy.int32))\nfeat.add_subset(subset)\nTraceback (most recent call last):\n  File \"\", line 1, in \nTypeError: in method 'Features_add_subset', argument 2 of type 'CSubset *'\n```\n. @sonney2k The build errors have been resolved.\n. For binary operators, if one is 32-bit and the other 64-bit, the compiler will automatically upcast the 32-bit one to be 64-bit as well.\n. Good question. That would be better here.\n. I've tried that, but then the compiler (clang and sometimes gcc) starts complaining that the operator[] is ambiguous when the argument is a uint32_t. My commit 6533afb above in the pull request was an attempt at this (multiple versions), but it failed.\n. And I think the code is more readable if I change the looping variable instead of adding 2-3 casts within the loop. The compiler automatically upcasts * and + etc to 64-bit as well, so the I thought it was quite practical.\n. Looking back at https://travis-ci.org/shogun-toolbox/shogun/builds/13077405 , with a third one for uint32, it needs a fourth one for \"long int\"... or we make the index type a template argument of the function? Or we go for 4 versions.\n. ",
    "uricamic": "I have changed the code according to your comments. Now I will try to do something with the algorithm, to make it faster.\n. I have noticed, I have done rebasing first and then resolved conflicts. In this pull request everything should be ok, I have moved all SO related stuff to folder shogun/structure. \n. The inactive cutting plane removal may add a few iterations to convergence, but significantly helps to reduce the time needed by the qp solver. \nFor example in the case of multiclass structured output svm trained on the MNIST OCR dataset, the time consumed by qp solver was reduced from 150s to 10s.\nMoreover the specified buffer is sufficient for more iterations when the inactive cutting planes are removed.\n. The inactive cutting plane removal may add a few iterations to convergence, but significantly helps to reduce the time needed by the qp solver. \nFor example in the case of multiclass structured output svm trained on the MNIST OCR dataset, the time consumed by qp solver was reduced from 150s to 10s.\nMoreover the specified buffer is sufficient for more iterations when the inactive cutting planes are removed.\n. I have done rebase, no complications appeared, hope it will be OK now.\n. Ahh, sorry, I haven't noticed the second commit :) \nBut thanks for the other memory-leaks fixes!\nI think it can be merged now\n. Yeah, you are right.\n. Yes, sure.\n. Hi, just to avoid any misunderstanding, the code for BMRM should remain as it is. Originally it expected R is already normalized. The reason why we used un-normalized risk was just to have comparable results for our BMRM modifications (namely PPBM and P3BM) and pure BMRM implementation. \nI think it is better to leave the decision of using the normalized or un-normalized risk on the designer of the risk function. And also maybe it will be better to use just normalized risk there, since I don't expect someone will be comparing those implementations.\n. Hi, sorry for the delay, I receive the e-mail notifications only sometimes.\nI know what is your point and it seems, that I have introduced an error while rewriting the original MATLAB code to C++. I will try to check the output on the testing data which I used to check the correctness of the functionality, since now I am a bit puzzled, how come, the results were OK. \nYou said that the results produced by your sequence are different than those produced by the old sequence, right? \nIf it is so, then, I should really check it, otherwise I think that it can be merged now.\n. If it produces the same results, I think it is completely ok. Thanks.\n. Indentation in both .h and .cpp should be fixed now.\n. ok, I will fix it\n. I will rename this parameter, it actually doesn't cope with computational thread, but with the number of cutting plane models\n. Does this order produce the same results and at the same speed? \n. beta is initialized by calloc (macro LIBBMRM_CALLOC), so beta[0] should be zero. The order here is then important for the QP solver initial solution.\n. Because we are just preparing beta array for the next iteration - we have set everything to the current nCP, and now, we are solving QP for the new cutting plane. That's why we add zero to the beta array and increase the nCP by one. \nThe beta array is then recalculated in QP call and  I, H, H_diag is resolved later with the help of newly computed beta.\nThe original code was correct, I have checked it to our original MATLAB code obtaining the same results. However, I agree that the boundary cases for BuffSize were potential risk.\n. ",
    "bittnt": "Thanks. That's very helpful.\nIn fact, i tried different ways to use Shogun. 1) I tried to use sudo port install shogun, it is supposed to install everything as well as the dependence softwares, but it doesn't. 2) I tried to download the shogun, and try to compile it , failed. 3) I tried to change some of the functions to make it to run on windows, failed. Some functions relied on the some unix system files. 4) I tried to use shogun on ubuntu linux, failed. Same problem. So I don't know how to use the shogun at this point.\n. Yes. I tried to get the latest git version as well as the version on the website. But they are all relying on some dependence software, while  i cannot find any tutorial about the step to install shogun and its dependences. Thanks a lot if you could provide me some links about the things you mentioned or any tutorials would help my problems. I just tried to install a ubuntu to play with shogun, but i didn't make it.\n. Could you give me some informations about your machine? (e.g. Osx, compiler and etc.)\n. Sorry. I tried already, still failed. \nshogun -I.. -o base/class_list.cpp.o base/class_list.cpp\nbase/class_list.cpp: In function \u2018shogun::CSGObject* __new_CDirectorDotFeatures(shogun::EPrimitiveType)\u2019:\nbase/class_list.cpp:402:98: error: expected type-specifier before \u2018CDirectorDotFeatures\u2019\nbase/class_list.cpp:402:98: error: expected \u2018:\u2019 before \u2018CDirectorDotFeatures\u2019\nbase/class_list.cpp:402:119: error: \u2018CDirectorDotFeatures\u2019 was not declared in this scope\nbase/class_list.cpp:402:120: error: expected \u2018;\u2019 before \u2018:\u2019 token\nbase/class_list.cpp:402:120: error: expected primary-expression before \u2018:\u2019 token\nbase/class_list.cpp:402:120: error: expected \u2018;\u2019 before \u2018:\u2019 token\nbase/class_list.cpp:402:128: warning: control reaches end of non-void function [-Wreturn-type]\nmake[1]: * [base/class_list.cpp.o] Error 1\nmake[1]: Leaving directory `/home/bittnt/Documents/shogun/shogun/src/shogun'\nmake: * [libshogun.so.12.0] Error 2\n. ",
    "amueller": "Thanks for getting back. Good luck with your release.\nWe'll release at about the same time :)\n. ",
    "ncray": "I think this is because Mountain Lion changed /usr/include/dirent.h\nSee https://github.com/mxcl/homebrew/issues/13649\n. Thanks for the hint.  I made this change, and it compiles now: https://github.com/ncray/shogun/commit/98c4423d39d14801f3624e330d0937c09595a452\nThe problem was that earlier versions of OSX (like 10.7) don't have __MAC_10_8 defined in Availability.h, so I went ahead and defined it.\nI'll make a pull request.\n. No, I have \ndefine __MAC_10_6      1060\ndefine __MAC_10_7      1070\ndefine __MAC_NA        9999   /* not available */\non my 10.7.4 machine.\nMaybe a better solution would be to have an #ifdef __MAC_10_8\n. Sorry, this still doesn't fix the problem (https://github.com/shogun-toolbox/shogun/commit/7a4c62ea33a4988e0f9a554b0805e993807cdcd1).\nSame issue: /usr/include/dirent.h:128:5: note: candidate function not viable: no known conversion from 'int (_)(const struct dirent )' to 'int (_)(struct dirent )' for 3rd argument;\nBefore Mountain Lion in /usr/include/dirent.h:\nscandir has as its 3rd argument int ()(struct dirent )\nIn Mountain Lion it is int ()(const struct dirent )\n. ",
    "besser82": "For me it works since 3.0.0 release without issues using Fedora19+.  I think this can be closed.\n. For me it works since 3.0.0 release without issues using Fedora19+.  I think this can be closed.\n. @karlnapf https://github.com/shogun-toolbox/shogun/pull/1608 should fix your issue  :)\n. @karlnapf https://github.com/shogun-toolbox/shogun/pull/1608 should fix your issue  :)\n. ruby_modular is affected as well, but even worse:\nInstalled (but unpackaged) file(s) found:\n   /usr/ local /lib64/ruby/site_ruby/modshogun.so\n. ruby_modular is affected as well, but even worse:\nInstalled (but unpackaged) file(s) found:\n   /usr/ local /lib64/ruby/site_ruby/modshogun.so\n. Should be fixed: https://github.com/shogun-toolbox/shogun/pull/1811\n. Should be fixed: https://github.com/shogun-toolbox/shogun/pull/1811\n. Allrighty  ;)  Let's wait for Travis.\n. size_t is always safe to use, as long you are dealing with unsigned 64-Bit integers.\n. Is int64_t a std-type defined in current CXX impl.?\n. I think this can be merged, since there is no real changes in code;  just some small fix-up in README-file.\n. Looks sane to me.  Thanks for the nice and careful work.  :D \n@karlnapf, @sonney2k:  This can be merged then?\n. Looks sane to me.  Thanks for the nice and careful work.  :D \n@karlnapf, @sonney2k:  This can be merged then?\n. Hi beew!\nI'm currently having the same problem with Fedora 21+.  It looks like octave-config3.8 has some changes, which break cli-compat to previous versions.  As soon as I have a solution, I'll cycle back to this and notify you.\nCheers,\n  Bj\u00f6rn\n. Hi beew!\nI'm currently having the same problem with Fedora 21+.  It looks like octave-config3.8 has some changes, which break cli-compat to previous versions.  As soon as I have a solution, I'll cycle back to this and notify you.\nCheers,\n  Bj\u00f6rn\n. np  :D\nLooks like comments and objections from https://github.com/shogun-toolbox/shogun/pull/1896 are solved, now. Let's wait for Travis to finsh and we'll merge.\n. @tklein23 can you please rebase onto recent master?  I'll re-check your changes afterwards and if it looks good, we can merge this one\u2026\n. The rest looks reasonable to me.  Please clarify / fixup my questions and we can merge this one\u2026\n. changes in 5aa2b89 shouldn't make the build fail\u2026  I'll merge.\n. @pickle27 and @kislayabhi: When Travis goes green on this, I'll merge\u2026\nThank you both for careful coding.  :D\n. @pickle27 and @kislayabhi: When Travis goes green on this, I'll merge\u2026\nThank you both for careful coding.  :D\n. @kislayabhi:  can you please rebasi onto recent develop, so we can be sure the build-fail is related to some outdated data-checkout in your branch?\n. @pickle27: I think so too. ^^\n. @kislayabhi, no but the rebase updated the data rev.  ;)\nTravis is passing, LGTM =)\n. This actually affects me on upcoming fc21 as well\u2026  Let's see if we can come up with a fix for this\u2026\n. LGTM =)  Many thanks for your work!\nFailures on TravisCI are not related to this.\n. LGTM so far\u2026  Let's wait to hear from Travisv and merge then\u2026\n. Build fails are not related to this\u2026  merging\u2026\n. @iglesias: Had a look over the changes.  Looks fine to me.  How do you feel about merging?\n. Just did a local run.  Saw no problems, no fails.  Merging.\n. I could solve the compilation issues with the following compiler-flags and removing -march=native from CMakeLists.txt:\nexport CFLAGS=\"-mno-avx -flax-vector-conversions -D'__has_extension(x)=0' -D'__has_feature(x)=0' -DvImage_Utilities_h -DvImage_CVUtilities_h\"\nexport CXXFLAGS=\"-mno-avx -flax-vector-conversions -D'__has_extension(x)=0' -D'__has_feature(x)=0' -DvImage_Utilities_h -DvImage_CVUtilities_h\"\nBut now the linker complains:\nduplicate symbol shogun::CEMBase<shogun::MixModelData>::get_name() const in:\n    CMakeFiles/libshogun.dir/base/class_list.cpp.o\n    CMakeFiles/libshogun.dir/distributions/EMMixtureModel.cpp.o\nduplicate symbol shogun::CEMBase<shogun::MixModelData>::iterate_em(int, double) in:\n    CMakeFiles/libshogun.dir/base/class_list.cpp.o\n    CMakeFiles/libshogun.dir/distributions/EMMixtureModel.cpp.o\nduplicate symbol shogun::CEMBase<shogun::MixModelData>::~CEMBase() in:\n    CMakeFiles/libshogun.dir/base/class_list.cpp.o\n    CMakeFiles/libshogun.dir/distributions/EMMixtureModel.cpp.o\nduplicate symbol shogun::CEMBase<shogun::MixModelData>::~CEMBase() in:\n    CMakeFiles/libshogun.dir/base/class_list.cpp.o\n    CMakeFiles/libshogun.dir/distributions/EMMixtureModel.cpp.o\nduplicate symbol shogun::CEMBase<shogun::MixModelData>::~CEMBase() in:\n    CMakeFiles/libshogun.dir/base/class_list.cpp.o\n    CMakeFiles/libshogun.dir/distributions/EMMixtureModel.cpp.o\nduplicate symbol shogun::CEMBase<shogun::MixModelData>::CEMBase() in:\n    CMakeFiles/libshogun.dir/base/class_list.cpp.o\n    CMakeFiles/libshogun.dir/distributions/EMMixtureModel.cpp.o\nduplicate symbol shogun::CEMBase<shogun::MixModelData>::CEMBase() in:\n    CMakeFiles/libshogun.dir/base/class_list.cpp.o\n    CMakeFiles/libshogun.dir/distributions/EMMixtureModel.cpp.o\nduplicate symbol typeinfo name for shogun::CEMBase<shogun::MixModelData> in:\n    CMakeFiles/libshogun.dir/base/class_list.cpp.o\n    CMakeFiles/libshogun.dir/distributions/EMMixtureModel.cpp.o\nduplicate symbol typeinfo for shogun::CEMBase<shogun::MixModelData> in:\n    CMakeFiles/libshogun.dir/base/class_list.cpp.o\n    CMakeFiles/libshogun.dir/distributions/EMMixtureModel.cpp.o\nduplicate symbol shogun::CEMBase<shogun::MixModelData>::get_name() const in:\n    CMakeFiles/libshogun.dir/base/class_list.cpp.o\n    CMakeFiles/libshogun.dir/distributions/MixtureModel.cpp.o\nduplicate symbol shogun::CEMBase<shogun::MixModelData>::iterate_em(int, double) in:\n    CMakeFiles/libshogun.dir/base/class_list.cpp.o\n    CMakeFiles/libshogun.dir/distributions/MixtureModel.cpp.o\nduplicate symbol shogun::CEMBase<shogun::MixModelData>::CEMBase() in:\n    CMakeFiles/libshogun.dir/base/class_list.cpp.o\n    CMakeFiles/libshogun.dir/distributions/MixtureModel.cpp.o\nduplicate symbol shogun::CEMBase<shogun::MixModelData>::~CEMBase() in:\n    CMakeFiles/libshogun.dir/base/class_list.cpp.o\n    CMakeFiles/libshogun.dir/distributions/MixtureModel.cpp.o\nduplicate symbol shogun::CEMBase<shogun::MixModelData>::~CEMBase() in:\n    CMakeFiles/libshogun.dir/base/class_list.cpp.o\n    CMakeFiles/libshogun.dir/distributions/MixtureModel.cpp.o\nduplicate symbol shogun::CEMBase<shogun::MixModelData>::~CEMBase() in:\n    CMakeFiles/libshogun.dir/base/class_list.cpp.o\n    CMakeFiles/libshogun.dir/distributions/MixtureModel.cpp.o\nduplicate symbol shogun::CEMBase<shogun::MixModelData>::CEMBase() in:\n    CMakeFiles/libshogun.dir/base/class_list.cpp.o\n    CMakeFiles/libshogun.dir/distributions/MixtureModel.cpp.o\nduplicate symbol typeinfo name for shogun::CEMBase<shogun::MixModelData> in:\n    CMakeFiles/libshogun.dir/base/class_list.cpp.o\n    CMakeFiles/libshogun.dir/distributions/MixtureModel.cpp.o\nduplicate symbol typeinfo for shogun::CEMBase<shogun::MixModelData> in:\n    CMakeFiles/libshogun.dir/base/class_list.cpp.o\n    CMakeFiles/libshogun.dir/distributions/MixtureModel.cpp.o\nld: 18 duplicate symbols for architecture x86_64\ncollect2: error: ld returned 1 exit status\n. @shubhamgoyal, @vigsterkr, @yarikoptic, @rieck:  Fix for building python_modular with mp-gcc-49 on Mac OSX 10.10 \"Yosemite\" is upcoming within the next two days\u2026  Will reference pull-request within this issue\u2026  ^^\n. @yarikoptic: For some reason github offered me your username for that topic\u2026 i dunno\u2026 sry\u2026\n. @karlnapf: not yet\u2026 compilation and linking dynlibs works, but - at least - Python_modular segfaults\u2026\n. Currently there is strange stuff going on with LLVM / Clang 6 + Accelerate-Framework on Mac OSX 10.10 Yosemite\u2026 :(\nWe are currently working on getting compilation running with mp-gcc-49, see https://github.com/shogun-toolbox/shogun/issues/2635\n. Build with mp-gcc-4X is fixed in PR #2647 \n. build errors are unrelated\u2026  looks like this can be merged then.  :D\n. should particularly fix #2640, too\u2026\n. Works for me either locally and on Fedora's builders\u2026\ncan you please give us the output of\nls -lah ~/Downloads/shogun/build/src/interfaces/\nI suspect issues with permissions, UMASK and/or Type of Filesystem\u2026\n. @beew: Can you confirm the referenced patch fixes this issue?\n. @beew: Can you confirm the referenced patch fixes this issue?\n. Issue resolved.\n. Issue resolved.\n. and besser82 did fix that mess :P\n. Failures on Travis are related to Octave.  Not related to changes in this PR.\n. Ruby- and Python-tests are fine on Travis, other fails are not related to these changes.  Merging\u2026\n. See my comment in https://github.com/shogun-toolbox/shogun/commit/caf0871a8223ae71a6003e84fca14e0140fdfe53\n-fPIC is already enabled for libshogun.  Can you please paste a log of the cmake-output and build-log of terminal on fpaste.org?  Don't forget to add DCMAKE_VERBOSE_MAKEFILE:BOOL=ON to your cmake-invocation\u2026\n. How about libcsv?  It's light-weight, straight forward to integrate, available on most distributions and LGPL'ed code.\n. @karlnapf I think libarchive affects or improves writing / reading all IO-formats in Shogun.  For that reason @vigsterkr wants to get that done first\u2026\n. @tandakun Mhh\u2026 Which OS, compiler, compiler-version and CMake-version are you using?  Looks a bit strange to me\u2026\n@karlnapf On it  ;)\n. @tandakun Mhh\u2026 Which OS, compiler, compiler-version and CMake-version are you using?  Looks a bit strange to me\u2026\n@karlnapf On it  ;)\n. @tandakun trying to reproduce with a CentOS / RHEL 6.5 VM and possibly digging out a solution.\n. @tandakun trying to reproduce with a CentOS / RHEL 6.5 VM and possibly digging out a solution.\n. Build fails are not related\u2026  Merging\u2026\n. Build fails are not related\u2026  Merging\u2026\n. This does not affect \"regular\" builds.  Merging\u2026\n. This does not affect \"regular\" builds.  Merging\u2026\n. @nachitoys Did you try recompiling after rm -f src/shogun/base/class_list.cpp?  That should fix the issue in 98% of the cases\u2026  Which compiler, CMake-version and OS you are using?\n. @hrm: Hi there!  Sorry for the late reply\u2026  I'm the package-owner in Fedora\u2026  Did you have a look at the spec-file I use, yet?  There are lot's of exceptions for failed tests for e.g. %ix86 and %arm, just because of this problem, currently\u2026\nI'll add the failing test to the excludes for aarch64 to the spec-file and ping pbrobinson to have him rebuild it.\nPossibly a rhbz# would be good to track this one\u2026\nCheers\n  Bj\u00f6rn\n. Should be fixed in packaging now.  See RHBZ #1222401.\n. @beew: Try to build using swig-3.0.7\u2026  This should at least fix the build\u2026\n. @beew: Using SWIG >= 3.0.7 and this PR: https://github.com/shogun-toolbox/shogun/pull/2882 should fix all issues regarding builds of octave-bindings.\n. @erip done\u2026\n. LGTM =)  Travis fails, because of the random issue with finding some unittest-executables\u2026  Not related to the changes.\n@lisitsyn Rdy 2 merge?\n. @lisitsyn Allrighty!  Let's wait for Travis.  If all goes fine, I'll merge?\n. @lisitsyn A'ight!  =)\n. @lisitsyn Travis was fine with the gcc / clang part\u2026  other stuff isn't related\u2026  Merging.\n. Travis-failures are not related.  Merging\u2026\n. The failure on Travis is unrelated\u2026\n```\nScanning dependencies of target octave_modular\n[100%] Building CXX object src/interfaces/octave_modular/CMakeFiles/octave_modular.dir/modshogunOCTAVE_wrap.cxx.o\n[100%] Building CXX object src/interfaces/octave_modular/CMakeFiles/octave_modular.dir/sg_print_functions.cpp.o\ng++: internal compiler error: Killed (program cc1plus)\nPlease submit a full bug report,\nwith preprocessed source if appropriate.\nSee  for instructions.\nmake[2]:  [src/interfaces/octave_modular/CMakeFiles/octave_modular.dir/modshogunOCTAVE_wrap.cxx.o] Error 4\nmake[1]:  [src/interfaces/octave_modular/CMakeFiles/octave_modular.dir/all] Error 2\nmake: *** [all] Error 2\n```\nThis indicates, the machine is running out of memory during compilation\u2026\nMerging\u2026\n. That should fix the bug mentioned in #2874.  Travis-fails are not related\u2026  Merging\u2026\n. LGTM  =)  Travis is not related\u2026  Merging\u2026\n. LGTM  =)  Travis is not related\u2026  Merging\u2026\n. No objections\u2026  Merging\u2026\n. No objections\u2026  Merging\u2026\n. Maybe this doc may come in handy for installing things not packaged in Ubuntu\u2026  =)\n. @jaelim When I build complete Shogun from sources, my machine get's slow as hell too\u2026  All eight CPU-cores are running @100%\u2026  How many RAM do you have installed inside your machine?  Terrible slow operation often is related to swapping mempages to HDD.  Building modular-interfaces (e.g. Python) takes serious amounts (~ 8 GByte) of RAM.\n@yorkerlin Currently we have two distros shipping Shogun in their native repos:  Fedora (v4.0.0 on <= 22, and git-snapshot in F23-beta / F-rawhide) and Debian (v3.20 in Debian Sid)\u2026  However both ship without svm^light, because of it's crappy license-terms\u2026\n. @yorkerlin Same specs for my laptop\u2026  Full build (all modular-interfaces and unit-test) takes ~45+ Mins.  RAM usage is avg. ~6 Gigs, too;  peak is 12 GBytes (during octave_modular).\n. LD_LIBRARY_PATH should point to the directory that has modshogun.so\nI think you're mistaken here\u2026  LD_LIBRARY_PATH should point to the dir libshogun.so is located in\u2026\n. @Arnei I'd suggest to remove all leftovers and rebuilding everything from scratch a second time\u2026\n. '-std=c++11' gets injected to build-flags automagically by an external CMake-script, if the compiler supports && needs that\u2026  Dunno why build go pop anyways\u2026  :(\n. Errors are not related\u2026  Merging\u2026\n@jondo Thanks!!!  =)\n. @iglesias: It's an issue caused by changes in Python 3.5 libs.  SWIG 3.0.7 is a confirmed to work version of SWIG and works fine with Python 2.7.X and Python <= 3.4.X builds.  The Fedora RPMs are built with that since Fedora 22.\n@qwertzdenek: Same issue for me on Fedora Rawhide builds.  Looking deeper into the issue next week.  Maybe there need to be adjustments on SWIG to be made\u2026\n. Some more research on this issue came up with another project having this problem with Python 3.5, too\u2026  :/\n. So this seems to be a known (and fixed in master) issue with SWIG and Python >= 3.5.\n. I re-built Shogun (recent checkout from develop) with a patched SWIG 3.0.7 with Python 3.5 on Fedora 24 and it works.  =)  So that issue should be gone with 3.0.8-release.  Will come up with a link to the build-logs, when Shogun-rebuild on Fedora's Koji is finished.\n\nI applied the following patches (in that order) to SWIG 3.0.7:\nhttps://github.com/swig/swig/commit/ef001de5240c1e05494e23b933b687f3f266045c\nhttps://github.com/swig/swig/commit/3e9854d308b56488e3bcae69acb4618253b49a94\nhttps://github.com/swig/swig/commit/4e8ea4e853efeca6782e905a75f83a5e704a5fb0\nhttps://github.com/swig/swig/commit/327b59a574c81437f79b168655feff04b12ce56d\nhttps://github.com/swig/swig/commit/c5322a9ecb2b9b13ad6300cf1675192a54f952c1\nhttps://github.com/swig/swig/commit/625a405b8e42f944fdc1a87e36725f03b8817a85\n. Glad to hear about!  =)\n. besser82's shogun-4.0.1-0.4.git20150913.d8eb73d.fc24 completed.\n. Ruby testsuite is passing; Python-errors are unrelated.  Merging\u2026\n. Going to merge #2955 after Travis is ready\u2026\n. @yorkerlin Which version of Fedora are you using?  All active maintained versions of Fedora ship doxygen >= 1.8.10.\nFor RHEL / CentOS / Scientific Linux, I can provide a recent version of doxygen through EPEL.\n. No objections from me, as said in #2959.  Debian Jessie ships doxygen 1.8.8, Ubuntu 14.04 LTS is on 1.8.6.  Fedora is >= 1.8.10, for RHEL, CentOS and SL I'll prepare are recent doxygen-compat package for EPEL (where Shogun will be shipped in soon, too).\n. Faling TravisCI is unrelated.  Merging, as this is confirmed to fix #2654.\n. Faling TravisCI is unrelated.  Merging, as this is confirmed to fix #2654.\n. Hey!\nIn Fedora's rpm-build, I simply cp -pr them into their destination directory after make install and pick them up into the package in %files.  That's all; no special magic.  This possibly might be an issue with CMake, too;  I'll dig into the build-scripts and try to figure that out\u2026\nCheers\n. Hey!\nIn Fedora's rpm-build, I simply cp -pr them into their destination directory after make install and pick them up into the package in %files.  That's all; no special magic.  This possibly might be an issue with CMake, too;  I'll dig into the build-scripts and try to figure that out\u2026\nCheers\n. Hello @beew!\nWe have a patched octave in the Fedora repos\u2026  The bugreport is from the maintainer of Fedora's ocatave-package\u2026\n. Hello @beew!\nWe have a patched octave in the Fedora repos\u2026  The bugreport is from the maintainer of Fedora's ocatave-package\u2026\n. Testsuite failures with Clang are not related.  Merging\u2026\n. Travis is not needed for this\u2026  No change in code nor behaviour of CMake.  Merging\u2026\n. Touched code passes unittests on Travis\u2026  Merging\u2026\n. Build fails are not related\u2026  Merging\u2026\n. No, the problem is:\u00a0 He's trying to link against a static python-lib, which is not compiled to link into dynamic-so...\nAm 20.02.2016 19:27 schrieb Sergey Lisitsyn notifications@github.com:I am not sure about this error, but as a general thing I'd recommend newer swig.\n\u2014Reply to this email directly or view it on GitHub.\n. Let's wait for Travis\u2026  If things are fine there, I'll merge this.\n. @karlnapf: No need for this\u2026  Reserving memory directly upon initialization of the vector is covered by C++-standard.  The previous code created a vector, alloc'ed memory, free'ed the mem and realloc'ed memory of the needed size.\nThe change in here just creates a vector and directly alloc's the needed amount memory.\n. Allrighty!  Since Travis looks good for libshogun itself, things are safe here.  Merging\u2026\n@yozw: Many thanks for your contribution!  =)\n. Same goes here\u2026  If Travis is fine with it, I'll merge this.\n. Same procedure here, too\u2026  ;)\n. Travis is fine\u2026 Merging\u2026\n. It's not our code\u2026  It's a simple limitation of LUA itself\u2026  See:  http://www.swig.org/Doc3.0/Lua.html#Lua_nn17\nSo foo(int a) always gets shadowed by foo(int64t)\u2026. @lisitsyn Will it be?  I mean, Any my_data = get(char *); or set(char*, const any&) would still be no problem\u2026\nPrint functions might be tricky, but for those we would just need to have mappings for the widest available type of any atomic type\u2026. Well, there is no real problem with Lua; the only thing is:  Using the Lua-Interface will be slower then others, just because Lua-typemapping will always use the widest type of the underlying atomic type, e.g bool or int will be int64_t, float will use double.\nImplementation-wise there is no real problem with Lua\u2026  It's just the fact, that Lua doesn't have a distiction between the width of the data-type used.. Anyways\u2026  If we really go down the road to a more modular approach, SWIG-interfaces need to be re-written from scratch - No way around it\u2026. @ellesec:  Can you please try to add -DR_EXECUTABLE=/opt/local/bin/R to the CMake-command and see, if it works?\n. @ellesec:  Can you please try to add -DR_EXECUTABLE=/opt/local/bin/R to the CMake-command and see, if it works?\n. So from my POV this is a good idea; I'm always open for splitting stuff and modularizing.  The only thing I would do differently:  Instead of having a sub-module in for this in Shogun, I'd transform the whole thing into a separate shared lib, so we can release it decoupled from Shogun and give improvements straight to the users.\nLooking in the future, we're planning to pluginize Shogun\u2026  So we could do with linalg, too and provide the corresponding different backends (Eigen3, ViennaCL, \u2026) as plugins\u2026\nIf we split out the linalg-thing and make it more versatile, I'd propose to give it a different name to avoid possible clashes with other projects\u2026  My proposal for that would be sth. like \"mathalicious\" or some other crazy name.\nDuring the split-out process we could also apply the new BSD-3 License to it, just to have that part done already.\n. @lambday Thank you very much! ^^\nAbout the plugin-stuff:  I think you got me wrong with the plugin thing\u2026  THe current linalg-thing won't be a plugin of Shogun\u2026  It will be a lib interfaced by Shogun.  The plugin-stuff in this context is more like:  Try to generalize / abstract the operations performed using Eigen or ViennaCL (and possibly other backends that may be there like ATLAS / BLAS(?)) and put them into plugins interfaced by our separated linalg-thing.\n. @karlnapf What in Shogun core is so specific, we cannot split into the new lib?  I mean: There's always a way to break inter-circular dependencies\u2026\nI can do the CMake-part, work along on the code changes neccessary and guide the people involved with code-changes and all\u2026\n. Sure ^^\nSmells like a br0k3n git clone\u2026  Please try to do a clean checkout and build inside it\u2026\n. Sure ^^\nSmells like a br0k3n git clone\u2026  Please try to do a clean checkout and build inside it\u2026\n. Travis is fine.  Merging\u2026\n. Travis is fine.  Merging\u2026\n. Fixed in #3039 \n. Fixed in #3039 \n. Thank you!  I've been quicker, already fixed that in a32b538b9df8620c142a4b1e5da9e4cc4f6b3ab3. ;)\n. Thank you!  I've been quicker, already fixed that in a32b538b9df8620c142a4b1e5da9e4cc4f6b3ab3. ;)\n. Hey!\nYou just need to install python-ply, using sudo apt-get install \npython-ply.  The other two (MOSEK, CPLEX) can be safely ignored.\nCheers\n   Bj\u00f6rn\nAm 08.04.2016 um 16:35 schrieb Alex Luya:\n\nHello,\nUnder a ubuntu 14.04 64bit,I tried to compile shogun from source,but \ngot error:\n...\n-- Could NOT find MOSEK (missing:  MOSEK_DIR MOSEK_INCLUDE_DIR MOSEK_LIBRARY)\n.....\n-- Could NOT find CPLEX (missing:  CPLEX_LIBRARY CPLEX_INCLUDE_DIR)\n.....\nCMake Error at cmake/FindPLY.cmake:24 (message):\n   ply import failure:\nCall Stack (most recent call first):\n   CMakeLists.txt:682 (FIND_PACKAGE)\nQuestions are:\nDo these two \"Could NOT find\"s cause this error?\nif there are,how to install them?(as you may knew,by default,much more \n\"Could Not find\"s will occur,and I have installed these missings by \ngoogling,but can't find a way to install these two)\nIs \"ply import failure:\" the only reason that caused this failure?If \nit is,how to fix it?It seems that no solution can be found by googling?\nIf \"ply import failure:\" do cause the problem,after fixed it,can I \nignore the two \"Could not find\"s to install?any consequence?\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly or view it on GitHub \nhttps://github.com/shogun-toolbox/shogun/issues/3149\n. Well, this happenes radomly\u2026  Have seen this in various builds, when (re-)building Shogun for Fedora.  I'm currently trying to build 5.0.0 for Fedora and I cannot get a successful build, because at least one out of six different arches fails due to this problem and makes the package-build fail in whole.\n\nAfaik, we need class_list.cpp for the SWIG'ed bindings\u2026. The madness continues\u2026  :(\n[ 53%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/base/class_list.cpp.o\ncd /builddir/build/BUILD/shogun-5.0.0/py2/x86_64-redhat-linux-gnu/src/shogun && /usr/bin/c++    -I/builddir/build/BUILD/shogun-5.0.0/py2/src -I/builddir/build/BUILD/shogun-5.0.0/py2/src/shogun -I/builddir/build/BUILD/shogun-5.0.0/py2/x86_64-redhat-linux-gnu/src -I/builddir/build/BUILD/shogun-5.0.0/py2/x86_64-redhat-linux-gnu/src/shogun -isystem /usr/include/eigen3 -I/usr/lib64/../include -I/usr/include/json-c -I/usr/include/libxml2  -std=c++11 -Wall -Wno-unused-parameter -Wformat -Wformat-security -Wparentheses -Wshadow -Wno-unknown-pragmas -Wno-deprecated -O2 -g -Wall -Werror=format-security -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches -specs=/usr/lib/rpm/redhat/redhat-hardened-cc1 -m64 -mtune=generic -Wno-misleading-indentation -fno-var-tracking-assignments -fopenmp -O2 -fPIC   -o CMakeFiles/libshogun.dir/base/class_list.cpp.o -c /builddir/build/BUILD/shogun-5.0.0/py2/src/shogun/base/class_list.cpp\nmake[2]: Leaving directory '/builddir/build/BUILD/shogun-5.0.0/py2/x86_64-redhat-linux-gnu'\nIn file included from /builddir/build/BUILD/shogun-5.0.0/py2/src/shogun/features/Alphabet.h:16:0,\n                 from /builddir/build/BUILD/shogun-5.0.0/py2/src/shogun/lib/BitString.h:15,\n                 from /builddir/build/BUILD/shogun-5.0.0/py2/src/shogun/base/class_list.cpp:15:\n/builddir/build/BUILD/shogun-5.0.0/py2/src/shogun/lib/WrappedSGVector.h: In member function 'void shogun::CWrappedSGVector<T>::register_params()':\n/builddir/build/BUILD/shogun-5.0.0/py2/src/shogun/base/SGObject.h:71:15: error: invalid use of incomplete type 'class shogun::Parameter'\n   m_parameters->add(param, name, description);\\\n               ^\n/builddir/build/BUILD/shogun-5.0.0/py2/src/shogun/base/SGObject.h:66:40: note: in expansion of macro 'SG_ADD4'\n #define VARARG_IMPL2(base, count, ...) base##count(__VA_ARGS__)\n                                        ^~~~\n/builddir/build/BUILD/shogun-5.0.0/py2/src/shogun/base/SGObject.h:67:39: note: in expansion of macro 'VARARG_IMPL2'\n #define VARARG_IMPL(base, count, ...) VARARG_IMPL2(base, count, __VA_ARGS__)\n                                       ^~~~~~~~~~~~\n/builddir/build/BUILD/shogun-5.0.0/py2/src/shogun/base/SGObject.h:68:27: note: in expansion of macro 'VARARG_IMPL'\n #define VARARG(base, ...) VARARG_IMPL(base, VA_NARGS(__VA_ARGS__), __VA_ARGS__)\n                           ^~~~~~~~~~~\n/builddir/build/BUILD/shogun-5.0.0/py2/src/shogun/base/SGObject.h:84:21: note: in expansion of macro 'VARARG'\n #define SG_ADD(...) VARARG(SG_ADD, __VA_ARGS__)\n                     ^~~~~~\n/builddir/build/BUILD/shogun-5.0.0/py2/src/shogun/lib/WrappedSGVector.h:84:3: note: in expansion of macro 'SG_ADD'\n   SG_ADD(&m_value, \"value\", \"Wrapped value\", MS_NOT_AVAILABLE);\n   ^~~~~~\n/builddir/build/BUILD/shogun-5.0.0/py2/src/shogun/base/SGObject.h:34:7: note: forward declaration of 'class shogun::Parameter'\n class Parameter;\n       ^~~~~~~~~\n/builddir/build/BUILD/shogun-5.0.0/py2/src/shogun/base/SGObject.h:73:32: error: invalid use of incomplete type 'class shogun::Parameter'\n    m_model_selection_parameters->add(param, name, description);\\\n                                ^\n/builddir/build/BUILD/shogun-5.0.0/py2/src/shogun/base/SGObject.h:66:40: note: in expansion of macro 'SG_ADD4'\n #define VARARG_IMPL2(base, count, ...) base##count(__VA_ARGS__)\n                                        ^~~~\n/builddir/build/BUILD/shogun-5.0.0/py2/src/shogun/base/SGObject.h:67:39: note: in expansion of macro 'VARARG_IMPL2'\n #define VARARG_IMPL(base, count, ...) VARARG_IMPL2(base, count, __VA_ARGS__)\n                                       ^~~~~~~~~~~~\n/builddir/build/BUILD/shogun-5.0.0/py2/src/shogun/base/SGObject.h:68:27: note: in expansion of macro 'VARARG_IMPL'\n #define VARARG(base, ...) VARARG_IMPL(base, VA_NARGS(__VA_ARGS__), __VA_ARGS__)\n                           ^~~~~~~~~~~\n/builddir/build/BUILD/shogun-5.0.0/py2/src/shogun/base/SGObject.h:84:21: note: in expansion of macro 'VARARG'\n #define SG_ADD(...) VARARG(SG_ADD, __VA_ARGS__)\n                     ^~~~~~\n/builddir/build/BUILD/shogun-5.0.0/py2/src/shogun/lib/WrappedSGVector.h:84:3: note: in expansion of macro 'SG_ADD'\n   SG_ADD(&m_value, \"value\", \"Wrapped value\", MS_NOT_AVAILABLE);\n   ^~~~~~\n/builddir/build/BUILD/shogun-5.0.0/py2/src/shogun/base/SGObject.h:34:7: note: forward declaration of 'class shogun::Parameter'\n class Parameter;\n       ^~~~~~~~~\nmake[2]: *** [src/shogun/CMakeFiles/libshogun.dir/build.make:17117: src/shogun/CMakeFiles/libshogun.dir/base/class_list.cpp.o] Error 1\nmake[2]: *** Waiting for unfinished jobs....\nmake[2]: Entering directory '/builddir/build/BUILD/shogun-5.0.0/py2/x86_64-redhat-linux-gnu'. This intercircular-header dependency-mess in the headers is so hard to fix\u2026  -_-. @vigsterkr I can confirm the patch works:  https://koji.fedoraproject.org/koji/buildinfo?buildID=859674. @karlnapf Silencing the compiler-warnings issued by SWIG-generated code would need to change the code-templates SWIG uses to generate the wrappers.  It's not our code, that causes these; it's just the way SWIG generates the code\u2026\nThere's nothing we can do about it, but contributing to SWIG directly\u2026. Know limitation of SWIG\u2026  See:  http://www.swig.org/Doc3.0/Ruby.html#Ruby_nn65. I can ask Mikio Braun, if he'd intend to update JBLAS supporting boolean arrays\u2026. That's an issue with SWIG itself.  See https://github.com/swig/swig/pull/875.  So we cannot fix it directly, but Octave 4.2 will be supported by SWIG 3.0.12.\nEither apply the patch from the linked PR to a custom-built SWIG or stick with Octave 4.0.X until the next SWIG-release\u2026  Sry.. After putting the missing files in place manually (related to https://github.com/shogun-toolbox/shogun/issues/3520):\n```\n[100%] Cookbook Sphinx building HTML\ncd /home/besser82/rpmbuild/BUILD/shogun-5.0.0/py2/x86_64-redhat-linux-gnu/doc/cookbook && /usr/bin/sphinx-build -q -b html -c /home/besser82/rpmbuild/BUILD/shogun-5.0.0/py2/x86_64-redhat-linux-gnu/doc/cookbook/_build -d /home/besser82/rpmbuild/BUILD/shogun-5.0.0/py2/x86_64-redhat-linux-gnu/doc/cookbook/_doctrees -D generated_examples_path=\"/home/besser82/rpmbuild/BUILD/shogun-5.0.0/py2/x86_64-redhat-linux-gnu/examples/meta/\" /home/besser82/rpmbuild/BUILD/shogun-5.0.0/py2/x86_64-redhat-linux-gnu/doc/cookbook/_build /home/besser82/rpmbuild/BUILD/shogun-5.0.0/py2/x86_64-redhat-linux-gnu/doc/cookbook/html\nException occurred:\n  File \"/home/besser82/rpmbuild/BUILD/shogun-5.0.0/py2/x86_64-redhat-linux-gnu/doc/cookbook/_build/sgexample.py\", line 155, in run\n    os.path.abspath(target_fname)))\nIOError: Generated listing /home/besser82/rpmbuild/BUILD/shogun-5.0.0/py2/x86_64-redhat-linux-gnu/examples/meta/python/regression/multiple_kernel_learning.py not found, it is expected to be created from the corresponding meta-example\nThe full traceback has been saved in /tmp/sphinx-err-of7eZe.log, if you want to report the issue to the developers.\nPlease also report this if it was a user error, so that a better error message can be provided next time.\nA bug report can be filed in the tracker at https://github.com/sphinx-doc/sphinx/issues. Thanks!\ndoc/cookbook/CMakeFiles/cookbook.dir/build.make:60: die Regel f\u00fcr Ziel \u201edoc/cookbook/CMakeFiles/cookbook\u201c scheiterte\nmake[3]: *** [doc/cookbook/CMakeFiles/cookbook] Fehler 1\n``. Well, any ideas about the fails on Travis?  Declarations didn't change\u2026 . You are trying to link against a custom compiled static-archive of bzip2\u2026  That's relly odd somehow\u2026  Try to removeusr/local/lib/libbz2.aand install your system'slibbz2-dev`.  Then rebuild Shogun in a fresh build-tree and all should go fine.\nUnfortunately I cannot assist you linking Shogun against a custom-built bzip2, since I do not know how it was compiled and all\u2026\nPlease report, if your problem was solved. Well, that would bloat the code in this header a bit too much\u2026. As the inlined code already shows up as a function.. Greaty!  Will merge this in after TravisCI has passed.  =). Everything should be fine now\u2026 Merging!. Yes!  =)  Just packaging v6.0.0 for Fedora  ;). Please check https://github.com/shogun-toolbox/shogun/pull/3801 first\u2026  ;). For me it those four tests fails on any platform, when building on RHEL or Fedora\u2026  :(. They are float() before.  Previous versions of numpy used to cast them to int() anyways\u2026. Okie, will do so  =). This is a sure fix; merging without Travis.. Travis fail is not related\u2026. Fail on Travis still not related\u2026  I've built successfully on fc27 and fc26 using this patch.  Merging\u2026. Simply comment it out:  #endif // USE_GPL_SHOGUN or #endif /*USE_GPL_SHOGUN*/ would be fine.. Yes, this has been implemented since Python 3.2. In the rebased commit I've changed it to except TypeError: to make it clearer.. Any objections on merging this?. You should set data to 0.12 as it needs to be bumped at any release.  Didn't happen during the few last releases, tho\u2026. ",
    "ptizoom": "it is just a start! and far from ready. but I would like this branch to be advertised in your shogun-toolbox/, \nthis might help bringing efforts!\nI have only tested on lunix 64:\n cd /usr/src/shogun\n make perl-conf\n make run-testsuite\n(only goes through classifier)\nanyway: I am trying to fix sparse objects and find compromise with PDL and SG structures.\nthe build mechanism is also flaky I wont mind a review LD__PATH hardcoded for test!)\n :: please remove those rm -rf $(somevariableset to /)... etc...\n. ok I totally agree to do this by steps... but for me this is going backward, I am already well in climbing my steps...\nat this stage (this branch) I can run tests, I have the perldl_modular target to compile and link (with your config system).\nI used a script (in contrib/ to transform the code in place.\nthen I started to modify the build chain... \nuntil I could compile and link my frame work,\nI can run the tests suites....(and it does!) and fixes functionalities and formats and more typemaps.\nnow I iterate this infernal work another time, and improve the package.\nso sg dense vector and matrix goes in and out PDL.\n(matrix cannot go out with this branch but soon will!).\nI did lots of \"unnecessary\" documentations twiks derived from  the auto transforms script.\nI worked this way too be able to learn on the fly the structure of SG (and PDL!).\nthe most important in this  branch  ptizoom/shogun/perl_swig_120921,\nis the perldl_mod stuff.\nultimately I have left lots of noise in the files, and no (c)... so one would need to cleansing the lot.\nI was thinking on cropping the  unnecessary (*.pl) later,\n as there are many stranger in this release due to the automated transform script.\n\"sonney2k\" if you like, you can also work on ptizoom/shogun/perl_swig_120921 to correct the \"config\" to your ways. and I had rather more, because these \"configure\" are really a pain.\n(I wonder why one parted from AM and \"autoconf\"...stuff?! one skilled in the art of shogun ain't in m4?)\nso I have sent you a working example here.... \nplease check it out, you have to meet me there, it is as good as it gets,\nand possibly cherry-pick the thingies you like to put on a \"shogun-toolbox/perl\" development branch,\ncheerio\n. at this stage dense vector/matrix with strings or numbers seems to be parsed in/out perl DL/SG (aka {string,matrix,vector}_{to,from}_pdl).\nthe tests/integration/perldl_modular is functional (make run-testsuite)\nbut it reveals that accuracy of the results is doubtful. so I gather the types parsing (memcopy) in swig_typemaps.i is buggy.\nalso I have reserves still with orientation of dimension...: dim[0] in PDL is columns number, and PDL vector() is dimension [n,1] by default. strings dimension in PDL::Char  are formatted likewise [strlen ~columns, rows, .. ,  1].\nand I am still suspicious of its mapping to SGObjects.\nthe examples in examples/documented/perl_modular are not perlified yet. it is a big task, maybe one need a machine to do the job?!\n. aye aye...captain...\nlimitations: amongst others...\n ../data/kernel/LinearByte_RAWBYTE_60.m , ../data/kernel/Linear_fnord.m  , ../data/kernel/LinearString_DNA_60.m  , ../data/kernel/SparseLinear_fnord.m , fail with * glibc detected * /usr/bin/perl: double free or corruption (!prev):...\nwhich did not used to on perl_swig_120921.\nbut I do not know when it was introduced by the perl  merge or core stuff since merged. may be you can help by telling ifit is a pure perl problem or not (and using \"--disable-reference-counting\" it will not make us wiser).\nthere are few other bugs, but the UNO MAJOR is that the tests results never fall in accuracy range????\nI think fixing this main issue will step up the stability and confidence in the perl-swig.\nI suspect at the root sordid problem of data format and alignments...where the simple memcopy() , ldx,mvx instructions... fails. I hope not to use a soft indexer to copy.\nfor the \"new style\" type of testing, unfortunately I have not bothered, and thinking of doing a ML translator python to perl. I might have touched in essence the meaning of tokenisation for (consuming) LR parser, which falls into distributing and factorising in some sort of ring with operators (or, is_just_after) (i thought!),\n and would not mind you give me directions and readings if you know people already worked on this parsing/translation tasks?\nI may let you the nice task of organising  makefiles and configs... I only know the use of the AM (autoconf macro) of the GNU, amazingly they are working  cross-platform and been there since the beginning....thus for a profound reason.\n. actually, there is a typo in resulting modshogun library...\nsrc/interfaces/perl_modular/modshogunso\n. ok, I hope these few patches let me come back to the  shoguna.\nit happened that it had fixed the \"double free or corruption \" problem. in fact there was a hardcoded path to the old perldl_modular.... which was still there.\n. the test results look like  this...:\nmake -C interfaces/perl_modular run-testsuite-perl_modular && true\nmake[1]: Entering directory `/usr/src/shogun/src/interfaces/perl_modular'\ncd ../../../tests/integration/perl_modular && (\\\n     LD_LIBRARY_PATH=//usr/local/lib PERL= PERLPATH=\"/\"     \\\n     LD_LIBRARY_PATH=\"/usr/src/shogun/src/interfaces/perl_modular:/usr/src/shogun/src/shogun\" \\\n     ./test_all.sh)\n** Testing in ../data/classifier/.m\n../data/classifier/LibLinear_0017_1en05_1_True.m        OK\n../data/classifier/LibLinear_023_1en05_1_True.m     OK\n../data/classifier/LibLinear_15_1en05_1_True.m      OK\n../data/classifier/LibLinear_30_1en05_16_True.m     OK\n../data/classifier/LibLinear_30_1en05_1_True.m      OK\n../data/classifier/SubGradientSVM_0017_1en05_1_05_False.m       OK\n../data/classifier/SubGradientSVM_023_1en05_1_05_False.m        ERROR\n[INFO] done. [INFO] converged after 59 iterations [INFO] objective: 4.333585 alpha: 1.162678 dir_deriv: 0.001568 num_bound: 2 num_active: 20 [INFO] solver time:0.490505 s /nclassified: 7.791946e-02 <--- accuracy: 1.000000e-04/n\n../data/classifier/SubGradientSVM_15_1en05_1_05_False.m     ERROR\n[INFO] done. [INFO] converged after 47 iterations [INFO] objective: 18.228042 alpha: 1.621445 dir_deriv: 0.158450 num_bound: 11 num_active: 9 [INFO] solver time:0.484756 s /nclassified: 7.477599e-04 <--- accuracy: 1.000000e-04/n\n../data/classifier/SubGradientSVM_30_1en05_1_05_False.m     ERROR\n[INFO] done. [INFO] converged after 7 iterations [INFO] objective: 212.868478 alpha: 0.000000 dir_deriv: 0.017614 num_bound: 12 num_active: 2 [INFO] solver time:0.568984 s /nclassified: 8.541129e-01 <--- accuracy: 1.000000e-04/n\n../data/classifier/SubGradientSVM_30_1en05_16_05_False.m        ERROR\n[INFO] done. [INFO] converged after 7 iterations [INFO] objective: 212.868478 alpha: 0.000000 dir_deriv: 0.017614 num_bound: 12 num_active: 2 [INFO] solver time:0.539777 s /nclassified: 8.541129e-01 <--- accuracy: 1.000000e-04/n\n../data/classifier/SVMLin_0017_1en05_1_True.m       OK\n../data/classifier/SVMLin_023_1en05_1_True.m        OK\n../data/classifier/SVMLin_15_1en05_1_True.m     OK\n../data/classifier/SVMLin_30_1en05_16_True.m        OK\n../data/classifier/SVMLin_30_1en05_1_True.m     OK\n../data/classifier/SVMOcas_0017_1en05_1_05_False.m      OK\n../data/classifier/SVMOcas_023_1en05_1_05_False.m       OK\n../data/classifier/SVMOcas_15_1en05_1_05_False.m        ERROR\n[INFO] Ocas Converged after 12 iterations ================================== timing statistics: output_time: 0.001151 s sort_time: 0.000038 s add_time: 0.000299 s w_time: 0.000063 s solver_time 0.002142 s ocas_time 0.003980 s /nclassified: 2.720155e-01 <--- accuracy: 1.000000e-04/n\n../data/classifier/SVMOcas_30_1en05_1_05_False.m        ERROR\n[INFO] Ocas Converged after 23 iterations ================================== timing statistics: output_time: 0.002831 s sort_time: 0.000065 s add_time: 0.000493 s w_time: 0.000187 s solver_time 0.059630 s ocas_time 0.064728 s /nclassified: 1.694003e+00 <--- accuracy: 1.000000e-04/n\n../data/classifier/SVMOcas_30_1en05_16_05_False.m       ERROR\n[INFO] Ocas Converged after 23 iterations ================================== timing statistics: output_time: 0.110068 s sort_time: 0.000086 s add_time: 0.000658 s w_time: 0.000168 s solver_time 0.070642 s ocas_time 0.182064 s /nclassified: 1.694003e+00 <--- accuracy: 1.000000e-04/n\n../data/classifier/SVMSGD_0017_1en05_1_True.m       $VAR1 = bless( {}, 'modshogun::SVMSGD' );\nERROR\n[INFO] lambda=2.673797, epochs=5, eta0=0.782020 [INFO] Estimating sparsity and bscale num_vec=22 num_feat=11. [INFO] using 22 examples. skip=16 bscale=0.282615 [INFO] Training on 22 vectors [INFO] Norm: 0.084089, Bias: -0.215520 /nclassified: 1.606063e-01, bias: 1.887768e-01 <--- accuracy: 1.000000e-04/n\n../data/classifier/SVMSGD_023_1en05_1_True.m        $VAR1 = bless( {}, 'modshogun::SVMSGD' );\nERROR\n[INFO] lambda=0.197628, epochs=5, eta0=1.499815 [INFO] Estimating sparsity and bscale num_vec=22 num_feat=11. [INFO] using 22 examples. skip=16 bscale=0.282615 [INFO] Training on 22 vectors [INFO] Norm: 0.708958, Bias: -0.131405 /nclassified: 8.379712e-02, bias: 1.135324e-01 <--- accuracy: 1.000000e-04/n\n../data/classifier/SVMSGD_15_1en05_1_True.m     $VAR1 = bless( {}, 'modshogun::SVMSGD' );\nERROR\n[INFO] lambda=0.030303, epochs=5, eta0=2.396782 [INFO] Estimating sparsity and bscale num_vec=22 num_feat=11. [INFO] using 22 examples. skip=16 bscale=0.282615 [INFO] Training on 22 vectors [INFO] Norm: 11.914167, Bias: 0.167660 /nclassified: 7.895516e-01, bias: 3.042243e-01 <--- accuracy: 1.000000e-04/n\n../data/classifier/SVMSGD_30_1en05_16_True.m        $VAR1 = bless( {}, 'modshogun::SVMSGD' );\nERROR\n[INFO] lambda=0.001515, epochs=5, eta0=5.068576 [INFO] Estimating sparsity and bscale num_vec=22 num_feat=11. [INFO] using 22 examples. skip=16 bscale=0.282615 [INFO] Training on 22 vectors [INFO] Norm: 473.630873, Bias: 1.745407 /nclassified: 4.640132e+00, bias: 1.126473e-02 <--- accuracy: 1.000000e-04/n\n../data/classifier/SVMSGD_30_1en05_1_True.m     $VAR1 = bless( {}, 'modshogun::SVMSGD' );\nERROR\n[INFO] lambda=0.001515, epochs=5, eta0=5.068576 [INFO] Estimating sparsity and bscale num_vec=22 num_feat=11. [INFO] using 22 examples. skip=16 bscale=0.282615 [INFO] Training on 22 vectors [INFO] Norm: 473.630873, Bias: 1.745407 /nclassified: 4.640132e+00, bias: 1.126473e-02 <--- accuracy: 1.000000e-04/n\n** Testing in ../data/clustering/.m\n../data/clustering/Hierarchical_EuclidianDistance_3.m       ERROR\n/nmerge_distance: 1.490951e-10, pairs: 1.800000e+01 <--- accuracy: 1.000000e-08/n\n../data/clustering/KMeans_EuclidianDistance_3.m     PDL: PDL::Ops::minus(a,b,c): Parameter 'b':\nMismatched implicit thread dimension 0: should be 6, is 3\n     at clustering.pm line 27\n    clustering::_evaluate('HASH(0x2a96848)') called at clustering.pm line 51\n    clustering::test('HASH(0x2a96848)') called at ./test_one.pl line 101\n    main::_test_mfile('../data/clustering/KMeans_EuclidianDistance_3.m') called at ./test_one.pl line 158\nERROR\n* Testing in ../data/distance/.m\n../data/distance/BrayCurtisDistance_fnord.m     ** glibc detected * /usr/bin/perl: free(): corrupted unsorted chunks: 0x000000000234abc0 ***\n======= Backtrace: =========\n/lib/x86_64-linux-gnu/libc.so.6(+0x76d76)[0x2b21028ebd76]\n/lib/x86_64-linux-gnu/libc.so.6(cfree+0x6c)[0x2b21028f0aac]\n/usr/src/shogun/src/shogun/libshogun.so.13(_Z7sg_freePv+0x18)[0x2b2107601db3]\n/usr/src/shogun/src/interfaces/perl_modular/modshogun.so(_ZN6shogun8SGMatrixIdE9free_dataEv+0x1c)[0x2b2106863d7c]\n/usr/src/shogun/src/interfaces/perl_modular/modshogun.so(_ZN6shogun16SGReferencedData5unrefEv+0x9f)[0x2b21067fedaf]\n/usr/src/shogun/src/interfaces/perl_modular/modshogun.so(_ZN6shogun8SGMatrixIdED1Ev+0x2b)[0x2b210681becf]\n/usr/src/shogun/src/interfaces/perl_modular/modshogun.so(_wrap_Distance_get_distance_matrix+0x35f)[0x2b210648f80f]\n/usr/lib/libperl.so.5.14(Perl_pp_entersub+0x58c)[0x2b2101f0a3cc]\n/usr/lib/libperl.so.5.14(Perl_runops_standard+0x16)[0x2b2101f019a6]\n/usr/lib/libperl.so.5.14(perl_run+0x3a5)[0x2b2101ea35b5]\n/usr/bin/perl(main+0x149)[0x400f89]\n/lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0xfd)[0x2b2102893ead]\n/usr/bin/perl[0x400fc1]\n======= Memory map: ========\n00400000-00402000 r-xp 00000000 fe:06 8499                               /usr/bin/perl\n00601000-00602000 r--p 00001000 fe:06 8499                               /usr/bin/perl\n00602000-00603000 rw-p 00002000 fe:06 8499                               /usr/bin/perl\netc....\n. Hi,\nsonne_:red concerns on formatting...I cannot promise a good style, but suggest to indent at the end of development and doc....but will try anyway!\n sonne_:there are still many big problems in perl....\n sonne_:sparse stuff is not tested at all! inf and nan values handling.\n sonne_: also, string_to_pdl does not work as expected...I was looking at that.\nexcepted that ML is fun!\nOn Wed, Nov 14, 2012 at 07:28:49AM -0800, Soeren Sonnenburg wrote:\n\nI have a couple of formatting concerns, like \nif (foo)\n(note the space between if and ( ) and then next statement on new line, { } brackets like\nif ()\n{\n}\nSGStringList<char>\nsome tab / whitespace stuff\nbut otherwise hurray! what is still missing?\n\nReply to this email directly or view it on GitHub:\nhttps://github.com/shogun-toolbox/shogun/pull/824#issuecomment-10370073\n\n\n\nDr. Christian Montanari, Consulting Engineer...\n. ",
    "dglodzik": "Thank you very much for fast response. I downloaded the newest source, and now get the following error:\nld: duplicate symbol shogun::SGMatrixList::copy_data(shogun::SGReferencedData const&) in ./lib/SGMatrixList.cpp.o and ./lib/SGMatrix.cpp.o for architecture x86_64\nDominik\n. ",
    "cesarfm": "Hello, I just added an example, also for my application to GSoC 2013. Thanks.\n. Hello, I just added an example, also for my application to GSoC 2013. Thanks.\n. Hello, sorry for the mistake with the branches, I had missed those instructions but I learned my lesson :-)\nI have made another pull request against the develop branch, so I will close this one. Thanks.\n. Hello, sorry for the mistake with the branches, I had missed those instructions but I learned my lesson :-)\nI have made another pull request against the develop branch, so I will close this one. Thanks.\n. ",
    "airanmehr": "Sounds great!\n. ",
    "theoryno3": "Which version of eigen, latest or backward compatibility with the all?\n. ",
    "tklein23": "Has been fixed by #1161.\n. The error was a bogus number of support vectors returned in debug output:\nNumber of SV: 19772 (including 140432545692465 at upper bound)\n. Failing tests are: mMCLDA.train_and_apply and mQDA.train_and_apply\n. Soeren told, that this method is correct.  So the only issue is documentation and/or readability.\nMy assumption was (j < len), but as far as I see, this was wrong.\n. Soeren told, that this method is correct.  So the only issue is documentation and/or readability.\nMy assumption was (j < len), but as far as I see, this was wrong.\n. That would work as well.  But if you're writing to (or even worse resizing) the returning vector, then the learners state gets corrupt.\n. Another problem appeared: If the learner object gets destroyed, then it destroys the weights vector.  But the returned  SGVector might still exist, but is corrupted now, since it's memory does not exist any more.  Thus, resulting in another SEGFAULT.\nSo the problem exists on both sides: Both objects claim the same memory.  We cannot decide which one should be allowed to free the memory and which not.\nSo imho there are two solutions:\n- (sonney2k): Switch to SGVector and remove (w, w_dim) - in this case SGVector cares about the memory\n- (tklein23): Copy the memory, so no object can cause (corrupting) side effects on the other object.\n. @lambday - I guess this segfault doesn't happen any more.\nIf you've got 5 minutes, please check and drop a commend (or close).\n. Removed pthread issue from pull request.  Now only contains fixes to running \"make check-examples\" without \"make install\".\n. So if it's totally forbidden to get references to SGObjects/SGReferencedData, why not add something like this?\n```\nSGVector * operator& ();\ntemplate SGVector * SGVector::operator& ()\n{\n    SG_SERROR(\"getting reference not allowed\");\n    return NULL;\n}\n```\nFirst it seemed to work, but it breaks some unit-tests.  For example CSubset::init():\nvoid CSubset::init() {\n    SG_ADD(&m_subset_idx, \"subset\", \"Vector of subset indices\",\n            MS_NOT_AVAILABLE);\n}\nObvously my proposition conflicts with our parameter framework.  Do you think there is some way to prevent getting references?  I'm just having the idea \"make impossible what shouldn't be done\".\n. A better idea is simply to increase ref count when getting a reference to the object:\n```\nCBinaryLabels * operator& ();\nCBinaryLabels * CBinaryLabels::operator& ()\n{\n    ref();\n    return this;\n}\n```\nThis fixes my issue and seems not to introduce a leak.  Any comments on this?\n(More general solution: Introducing this into SGReferencedData/SGObject could work, but seems to imply changes on every subclass of SGReferencedData/SGObject.)\n. @sonney2k: Could be even more magic (or idiomatic?) if I\n- renamed make target \"install-local-build\" to \"install-local\"\n- renamed directory \"local-build-tmp\" to \"install-local\"\n```\ninstall-local:\n   $(MAKE) DESTDIR=\"$(shell readlink -f \".\")\"/$@ install\nclean-local:\n   @rm -rf install-local\n%-local: install-local\n   $(MAKE) DESTDIR=\"$(shell readlink -f \".\")\"/$< $*\n```\nBut then the connection between the directory and the target would become {less obvious, more magic}.\nJust curious: Any thoughts on this?  More/less readable/idiomatic/error-prone/...?\n. @sonney2k: Did I eventually broke something by changing the output of \"display_vector\"? I changed the format string from \"%c\" to \"%d\", because the output was empty.\nNow I realized I only tested on 0 and 1 - which are unprintable characters.\n. I'll undo it.\n. Made a new PR #1308\n. With a different string, but yes.  Output was \"vector=[x,y,z]\"\nI'd even written a unit-test, but it's hard to test IO.  Any suggestions how to test IO with gtest?\n. I already fixed them (amongst others).\n. Fixed your objections.\nCould you maybe tell me the right indent options for your coding style?  Don't want to do that by-hand. :)\nWould \"indent --k-and-r-style\" be sufficient?\n. This issue has been fixed meantime.\n. Not ready for commit yet.  Forget actual regression test.\n. I updated my PR and shuffle(CRandom *) methods to several classes.  I found out that the parameter-less shuffle() is not consumed somewhere else, so I encourage to remove it.  Objections?\n. @sonney2k: Reverted my last commit, so your parameter-less (and non-thread-safe) methods are back.\n@karlnapf: I know good test coverage is important, but I currently can't do it.  Have a brief look on the complete diff.  I think unit-tests are not (strongly) required for the following reasons:\n- I simplified my changes so much that they only rely on modified versions of \"permute()\" or \"shuffle()\".\n- I added a note to \"shuffle()\" that it's not thread safe\n- Since m_rng is initialized with \"sg_rand\" by default, the behaviour did not change (unless you explicitly set m_rng)\n. I did a rebase & push --force to trigger travis.  It's (almost) green now.\n. @iglesias: Thanks for telling.  Didn't know that.\n. Fix confirmed by valgrind on affected code.\n. The example python_modular/features_io_modular.py (line 25) breaks because something is wrong with examples/undocumented/python_modular/fm_train_sparsereal.bin, but I can't tell what exactly.\nIt's loaded from SparseRealFeatures(fm_train_real.dat) and then saved back to disk.  When re-reading the binary, I get a\n38: SystemError: [ERROR] In file ./shogun/src/shogun/features/SparseFeatures.cpp line 236: \nsparse_matrix[0] check failed (matrix features 0 >= vector dimension 2)\nI guess the sparse matrix is not created correctly since \"num_features\" is set to zero, while the first vector is of dimension 2.\n. I care about because:\n- It's just inconvenient when doing a \"ls\" on the command line to have this useless output.\n- When browsing the directory you have to read more text before you can point to the right item.\nIt's not a real bugfix - rather a cleanup.\n. Yes, sure. :)\n. - To see the errors, just call valgrind ./shogun-unit-test --gtest_filter=$given_test_name inside build/tests/unit after compiling the tests with debugging information.\n- If you're gonna fix something, please add a comment to this ticket (avoiding people working on the same test)\n. We basically have 5 classes:\n- EPInferenceMethod: (@votjakovr: are you working on that?  Need assistance?)\n- RationalApproximationCGMJob: (@lambday: are you working on that?  Need assistance?)\n- Serialization: (@karlnapf?)\n- SGObject (clone_equals_*): I guess this should be split up:\n  - FeatureBlockLogisticRegression\n  - GaussianBlobsDataGenerator\n  - MAPInference\n  - DisjointSet\n  - DualLibQPBMSOSVM\n  - Factor\n  - MCLDA\n  - StringFeatures\n  - StringFileFeatures\n- MulticlassOCASTest: (@vigsterkr?)\nIt shouldn't be a big deal if we could distribute the affected classes to their \"owners\".\n. I will fix warnings from DualLibQPBMSOSVM.\n. I re-built and re-checked with a57b6a9 and got following output (sure you're up-to-date and on upstream/develop?):\n```\n$ valgrind --leak-check=full ./shogun-unit-test --gtest_filter=EPInference\n==29452== Memcheck, a memory error detector\n==29452== Copyright (C) 2002-2012, and GNU GPL'd, by Julian Seward et al.\n==29452== Using Valgrind-3.8.1 and LibVEX; rerun with -h for copyright info\n==29452== Command: ./shogun-unit-test --gtest_filter=EPInference\n==29452== \nNote: Google Test filter = EPInference\n[==========] Running 6 tests from 1 test case.\n[----------] Global test environment set-up.\n[----------] 6 tests from EPInferenceMethod\n[ RUN      ] EPInferenceMethod.get_cholesky_probit_likelihood\n==29452== Invalid read of size 4\n==29452==    at 0x60B9340: shogun::CEPInferenceMethod::update() (EPInferenceMethod.cpp:208)\n==29452==    by 0x60B820D: shogun::CEPInferenceMethod::get_cholesky() (EPInferenceMethod.cpp:99)\n==29452==    by 0xA00701: EPInferenceMethod_get_cholesky_probit_likelihood_Test::TestBody() (EPInferenceMethod_unittest.cc:68)\n==29452==    by 0xCFA5AA: void testing::internal::HandleExceptionsInMethodIfSupported(testing::Test, void (testing::Test::)(), char const) (gtest.cc:2078)\n==29452==    by 0xCE8EEE: testing::Test::Run() (gtest.cc:2151)\n==29452==    by 0xCE9001: testing::TestInfo::Run() (gtest.cc:2326)\n==29452==    by 0xCE91B7: testing::TestCase::Run() (gtest.cc:2444)\n==29452==    by 0xCE98F3: testing::internal::UnitTestImpl::RunAllTests() (gtest.cc:4261)\n==29452==    by 0xCFA03A: bool testing::internal::HandleExceptionsInMethodIfSupported(testing::internal::UnitTestImpl, bool (testing::internal::UnitTestImpl::)(), char const) (gtest.cc:2078)\n==29452==    by 0xCE8529: testing::UnitTest::Run() (gtest.cc:3875)\n==29452==    by 0xB1FB32: RUN_ALL_TESTS() (gtest.h:2288)\n==29452==    by 0xB1FA2B: main (main_unittest.cc:69)\n==29452==  Address 0xcb730a4 is 0 bytes after a block of size 20 alloc'd\n==29452==    at 0x4C2CD7B: malloc (in /usr/lib/valgrind/vgpreload_memcheck-amd64-linux.so)\n==29452==    by 0x6371A1A: shogun::sg_malloc(unsigned long) (memory.cpp:204)\n==29452==    by 0xB7C11F: int shogun::sg_generic_malloc(unsigned long) (memory.h:80)\n==29452==    by 0x63377CF: shogun::SGVector::SGVector(int, bool) (SGVector.cpp:79)\n==29452==    by 0x60B8A35: shogun::CEPInferenceMethod::update() (EPInferenceMethod.cpp:183)\n==29452==    by 0x60B820D: shogun::CEPInferenceMethod::get_cholesky() (EPInferenceMethod.cpp:99)\n==29452==    by 0xA00701: EPInferenceMethod_get_cholesky_probit_likelihood_Test::TestBody() (EPInferenceMethod_unittest.cc:68)\n==29452==    by 0xCFA5AA: void testing::internal::HandleExceptionsInMethodIfSupported(testing::Test, void (testing::Test::)(), char const) (gtest.cc:2078)\n==29452==    by 0xCE8EEE: testing::Test::Run() (gtest.cc:2151)\n==29452==    by 0xCE9001: testing::TestInfo::Run() (gtest.cc:2326)\n==29452==    by 0xCE91B7: testing::TestCase::Run() (gtest.cc:2444)\n==29452==    by 0xCE98F3: testing::internal::UnitTestImpl::RunAllTests() (gtest.cc:4261)\n==29452== \n[       OK ] EPInferenceMethod.get_cholesky_probit_likelihood (947 ms)\n[ RUN      ] EPInferenceMethod.get_negative_marginal_likelihood_probit_likelihood\n==29452== Invalid read of size 4\n==29452==    at 0x60B9340: shogun::CEPInferenceMethod::update() (EPInferenceMethod.cpp:208)\n==29452==    by 0x60B80A3: shogun::CEPInferenceMethod::get_negative_marginal_likelihood() (EPInferenceMethod.cpp:70)\n==29452==    by 0xA02BA3: EPInferenceMethod_get_negative_marginal_likelihood_probit_likelihood_Test::TestBody() (EPInferenceMethod_unittest.cc:147)\n==29452==    by 0xCFA5AA: void testing::internal::HandleExceptionsInMethodIfSupported(testing::Test, void (testing::Test::)(), char const) (gtest.cc:2078)\n==29452==    by 0xCE8EEE: testing::Test::Run() (gtest.cc:2151)\n==29452==    by 0xCE9001: testing::TestInfo::Run() (gtest.cc:2326)\n==29452==    by 0xCE91B7: testing::TestCase::Run() (gtest.cc:2444)\n==29452==    by 0xCE98F3: testing::internal::UnitTestImpl::RunAllTests() (gtest.cc:4261)\n==29452==    by 0xCFA03A: bool testing::internal::HandleExceptionsInMethodIfSupported(testing::internal::UnitTestImpl, bool (testing::internal::UnitTestImpl::)(), char const) (gtest.cc:2078)\n==29452==    by 0xCE8529: testing::UnitTest::Run() (gtest.cc:3875)\n==29452==    by 0xB1FB32: RUN_ALL_TESTS() (gtest.h:2288)\n==29452==    by 0xB1FA2B: main (main_unittest.cc:69)\n==29452==  Address 0xcb8dea4 is 0 bytes after a block of size 20 alloc'd\n==29452==    at 0x4C2CD7B: malloc (in /usr/lib/valgrind/vgpreload_memcheck-amd64-linux.so)\n==29452==    by 0x6371A1A: shogun::sg_malloc(unsigned long) (memory.cpp:204)\n==29452==    by 0xB7C11F: int shogun::sg_generic_malloc(unsigned long) (memory.h:80)\n==29452==    by 0x63377CF: shogun::SGVector::SGVector(int, bool) (SGVector.cpp:79)\n==29452==    by 0x60B8A35: shogun::CEPInferenceMethod::update() (EPInferenceMethod.cpp:183)\n==29452==    by 0x60B80A3: shogun::CEPInferenceMethod::get_negative_marginal_likelihood() (EPInferenceMethod.cpp:70)\n==29452==    by 0xA02BA3: EPInferenceMethod_get_negative_marginal_likelihood_probit_likelihood_Test::TestBody() (EPInferenceMethod_unittest.cc:147)\n==29452==    by 0xCFA5AA: void testing::internal::HandleExceptionsInMethodIfSupported(testing::Test, void (testing::Test::)(), char const) (gtest.cc:2078)\n==29452==    by 0xCE8EEE: testing::Test::Run() (gtest.cc:2151)\n==29452==    by 0xCE9001: testing::TestInfo::Run() (gtest.cc:2326)\n==29452==    by 0xCE91B7: testing::TestCase::Run() (gtest.cc:2444)\n==29452==    by 0xCE98F3: testing::internal::UnitTestImpl::RunAllTests() (gtest.cc:4261)\n==29452== \n[       OK ] EPInferenceMethod.get_negative_marginal_likelihood_probit_likelihood (43 ms)\n[ RUN      ] EPInferenceMethod.get_alpha_probit_likelihood\n==29452== Invalid read of size 4\n==29452==    at 0x60B9340: shogun::CEPInferenceMethod::update() (EPInferenceMethod.cpp:208)\n==29452==    by 0x60B81AB: shogun::CEPInferenceMethod::get_alpha() (EPInferenceMethod.cpp:91)\n==29452==    by 0xA0324B: EPInferenceMethod_get_alpha_probit_likelihood_Test::TestBody() (EPInferenceMethod_unittest.cc:198)\n==29452==    by 0xCFA5AA: void testing::internal::HandleExceptionsInMethodIfSupported(testing::Test, void (testing::Test::)(), char const) (gtest.cc:2078)\n==29452==    by 0xCE8EEE: testing::Test::Run() (gtest.cc:2151)\n==29452==    by 0xCE9001: testing::TestInfo::Run() (gtest.cc:2326)\n==29452==    by 0xCE91B7: testing::TestCase::Run() (gtest.cc:2444)\n==29452==    by 0xCE98F3: testing::internal::UnitTestImpl::RunAllTests() (gtest.cc:4261)\n==29452==    by 0xCFA03A: bool testing::internal::HandleExceptionsInMethodIfSupported(testing::internal::UnitTestImpl, bool (testing::internal::UnitTestImpl::)(), char const) (gtest.cc:2078)\n==29452==    by 0xCE8529: testing::UnitTest::Run() (gtest.cc:3875)\n==29452==    by 0xB1FB32: RUN_ALL_TESTS() (gtest.h:2288)\n==29452==    by 0xB1FA2B: main (main_unittest.cc:69)\n==29452==  Address 0xcba8ca4 is 0 bytes after a block of size 20 alloc'd\n==29452==    at 0x4C2CD7B: malloc (in /usr/lib/valgrind/vgpreload_memcheck-amd64-linux.so)\n==29452==    by 0x6371A1A: shogun::sg_malloc(unsigned long) (memory.cpp:204)\n==29452==    by 0xB7C11F: int shogun::sg_generic_malloc(unsigned long) (memory.h:80)\n==29452==    by 0x63377CF: shogun::SGVector::SGVector(int, bool) (SGVector.cpp:79)\n==29452==    by 0x60B8A35: shogun::CEPInferenceMethod::update() (EPInferenceMethod.cpp:183)\n==29452==    by 0x60B81AB: shogun::CEPInferenceMethod::get_alpha() (EPInferenceMethod.cpp:91)\n==29452==    by 0xA0324B: EPInferenceMethod_get_alpha_probit_likelihood_Test::TestBody() (EPInferenceMethod_unittest.cc:198)\n==29452==    by 0xCFA5AA: void testing::internal::HandleExceptionsInMethodIfSupported(testing::Test, void (testing::Test::)(), char const) (gtest.cc:2078)\n==29452==    by 0xCE8EEE: testing::Test::Run() (gtest.cc:2151)\n==29452==    by 0xCE9001: testing::TestInfo::Run() (gtest.cc:2326)\n==29452==    by 0xCE91B7: testing::TestCase::Run() (gtest.cc:2444)\n==29452==    by 0xCE98F3: testing::internal::UnitTestImpl::RunAllTests() (gtest.cc:4261)\n==29452== \n[       OK ] EPInferenceMethod.get_alpha_probit_likelihood (34 ms)\n[ RUN      ] EPInferenceMethod.get_marginal_likelihood_derivatives_probit_likelihood\n[       OK ] EPInferenceMethod.get_marginal_likelihood_derivatives_probit_likelihood (1 ms)\n[ RUN      ] EPInferenceMethod.get_posterior_approximation_mean_probit_likelihood\n==29452== Invalid read of size 4\n==29452==    at 0x60B9340: shogun::CEPInferenceMethod::update() (EPInferenceMethod.cpp:208)\n==29452==    by 0x60B82D5: shogun::CEPInferenceMethod::get_posterior_approximation_mean() (EPInferenceMethod.cpp:115)\n==29452==    by 0xA03E2D: EPInferenceMethod_get_posterior_approximation_mean_probit_likelihood_Test::TestBody() (EPInferenceMethod_unittest.cc:257)\n==29452==    by 0xCFA5AA: void testing::internal::HandleExceptionsInMethodIfSupported(testing::Test, void (testing::Test::)(), char const) (gtest.cc:2078)\n==29452==    by 0xCE8EEE: testing::Test::Run() (gtest.cc:2151)\n==29452==    by 0xCE9001: testing::TestInfo::Run() (gtest.cc:2326)\n==29452==    by 0xCE91B7: testing::TestCase::Run() (gtest.cc:2444)\n==29452==    by 0xCE98F3: testing::internal::UnitTestImpl::RunAllTests() (gtest.cc:4261)\n==29452==    by 0xCFA03A: bool testing::internal::HandleExceptionsInMethodIfSupported(testing::internal::UnitTestImpl, bool (testing::internal::UnitTestImpl::)(), char const) (gtest.cc:2078)\n==29452==    by 0xCE8529: testing::UnitTest::Run() (gtest.cc:3875)\n==29452==    by 0xB1FB32: RUN_ALL_TESTS() (gtest.h:2288)\n==29452==    by 0xB1FA2B: main (main_unittest.cc:69)\n==29452==  Address 0xcbc4074 is 0 bytes after a block of size 20 alloc'd\n==29452==    at 0x4C2CD7B: malloc (in /usr/lib/valgrind/vgpreload_memcheck-amd64-linux.so)\n==29452==    by 0x6371A1A: shogun::sg_malloc(unsigned long) (memory.cpp:204)\n==29452==    by 0xB7C11F: int shogun::sg_generic_malloc(unsigned long) (memory.h:80)\n==29452==    by 0x63377CF: shogun::SGVector::SGVector(int, bool) (SGVector.cpp:79)\n==29452==    by 0x60B8A35: shogun::CEPInferenceMethod::update() (EPInferenceMethod.cpp:183)\n==29452==    by 0x60B82D5: shogun::CEPInferenceMethod::get_posterior_approximation_mean() (EPInferenceMethod.cpp:115)\n==29452==    by 0xA03E2D: EPInferenceMethod_get_posterior_approximation_mean_probit_likelihood_Test::TestBody() (EPInferenceMethod_unittest.cc:257)\n==29452==    by 0xCFA5AA: void testing::internal::HandleExceptionsInMethodIfSupported(testing::Test, void (testing::Test::)(), char const) (gtest.cc:2078)\n==29452==    by 0xCE8EEE: testing::Test::Run() (gtest.cc:2151)\n==29452==    by 0xCE9001: testing::TestInfo::Run() (gtest.cc:2326)\n==29452==    by 0xCE91B7: testing::TestCase::Run() (gtest.cc:2444)\n==29452==    by 0xCE98F3: testing::internal::UnitTestImpl::RunAllTests() (gtest.cc:4261)\n==29452== \n[       OK ] EPInferenceMethod.get_posterior_approximation_mean_probit_likelihood (31 ms)\n[ RUN      ] EPInferenceMethod.get_posterior_approximation_covariance_probit_likelihood\n==29452== Invalid read of size 4\n==29452==    at 0x60B9340: shogun::CEPInferenceMethod::update() (EPInferenceMethod.cpp:208)\n==29452==    by 0x60B8339: shogun::CEPInferenceMethod::get_posterior_approximation_covariance() (EPInferenceMethod.cpp:123)\n==29452==    by 0xA04A05: EPInferenceMethod_get_posterior_approximation_covariance_probit_likelihood_Test::TestBody() (EPInferenceMethod_unittest.cc:313)\n==29452==    by 0xCFA5AA: void testing::internal::HandleExceptionsInMethodIfSupported(testing::Test, void (testing::Test::)(), char const) (gtest.cc:2078)\n==29452==    by 0xCE8EEE: testing::Test::Run() (gtest.cc:2151)\n==29452==    by 0xCE9001: testing::TestInfo::Run() (gtest.cc:2326)\n==29452==    by 0xCE91B7: testing::TestCase::Run() (gtest.cc:2444)\n==29452==    by 0xCE98F3: testing::internal::UnitTestImpl::RunAllTests() (gtest.cc:4261)\n==29452==    by 0xCFA03A: bool testing::internal::HandleExceptionsInMethodIfSupported(testing::internal::UnitTestImpl, bool (testing::internal::UnitTestImpl::)(), char const) (gtest.cc:2078)\n==29452==    by 0xCE8529: testing::UnitTest::Run() (gtest.cc:3875)\n==29452==    by 0xB1FB32: RUN_ALL_TESTS() (gtest.h:2288)\n==29452==    by 0xB1FA2B: main (main_unittest.cc:69)\n==29452==  Address 0xcbdee74 is 0 bytes after a block of size 20 alloc'd\n==29452==    at 0x4C2CD7B: malloc (in /usr/lib/valgrind/vgpreload_memcheck-amd64-linux.so)\n==29452==    by 0x6371A1A: shogun::sg_malloc(unsigned long) (memory.cpp:204)\n==29452==    by 0xB7C11F: int shogun::sg_generic_malloc(unsigned long) (memory.h:80)\n==29452==    by 0x63377CF: shogun::SGVector::SGVector(int, bool) (SGVector.cpp:79)\n==29452==    by 0x60B8A35: shogun::CEPInferenceMethod::update() (EPInferenceMethod.cpp:183)\n==29452==    by 0x60B8339: shogun::CEPInferenceMethod::get_posterior_approximation_covariance() (EPInferenceMethod.cpp:123)\n==29452==    by 0xA04A05: EPInferenceMethod_get_posterior_approximation_covariance_probit_likelihood_Test::TestBody() (EPInferenceMethod_unittest.cc:313)\n==29452==    by 0xCFA5AA: void testing::internal::HandleExceptionsInMethodIfSupported(testing::Test, void (testing::Test::)(), char const*) (gtest.cc:2078)\n==29452==    by 0xCE8EEE: testing::Test::Run() (gtest.cc:2151)\n==29452==    by 0xCE9001: testing::TestInfo::Run() (gtest.cc:2326)\n==29452==    by 0xCE91B7: testing::TestCase::Run() (gtest.cc:2444)\n==29452==    by 0xCE98F3: testing::internal::UnitTestImpl::RunAllTests() (gtest.cc:4261)\n==29452== \n[       OK ] EPInferenceMethod.get_posterior_approximation_covariance_probit_likelihood (48 ms)\n[----------] 6 tests from EPInferenceMethod (1114 ms total)\n[----------] Global test environment tear-down\n[==========] 6 tests from 1 test case ran. (1159 ms total)\n[  PASSED  ] 6 tests.\n==29452== \n==29452== HEAP SUMMARY:\n==29452==     in use at exit: 0 bytes in 0 blocks\n==29452==   total heap usage: 15,508 allocs, 15,508 frees, 1,232,510 bytes allocated\n==29452== \n==29452== All heap blocks were freed -- no leaks are possible\n==29452== \n==29452== For counts of detected and suppressed errors, rerun with: -v\n==29452== ERROR SUMMARY: 10 errors from 5 contexts (suppressed: 2 from 2)\n```\n. But if the vector is not sorted, the result will be lots of faster. ;)\nMoreover, is_sorted() is guaranteed to not change the vector (thus const) and can be used in cases where my sgvector is declared const.  I just need a formal guarantee that nobody changes my data. :)\nIt's no big deal if you don't want to accept it, since it's in fact hard to argue why we need it. :)\n. @iglesias is right - quick sorts best-case complexity is O(n log n)\n. You should ask @vigsterkr for details, but beside others -DBUILD_DASHBOARD_REPORTS=ON allows us to call ctest -D ExperimentalMemCheck to run the tests within valgrind.\nMoreover, it's possible to submit build errors to @vigsterkr's cdash (see CTestConfig.cmake).\n. I'm not quite sure.\nLet me guess: When m_model==model and ref_count()==1, then SG_UNREF(m_model) causes the destruction of m_model and sets the pointer to NULL.  But model points to the same (already freed) memory. Finally, calling SG_UNREF(model) does something on the freed memory, resulting in a crash.\n. No, didn't say SG_UNREF(model) will be a problem with model==NULL - in this case SG_UNREF will become a NO-OP.\nBut I agree that this bug only happens in connection with some other bug \"outside\".  Anyway, the bug can easily triggered with something like this:\nlabels = new Labels(); // ref_count == 0\nmodel->set_labels(labels); // allright, model now owns the labels\nmodel->set_labels(labels); // crash\nSo repeated calling of \"set_labels\" is not expected to harm. :)\n. Should be fine now.\n. Any suggestions for a dir/name?\n. Moved file to new configs directory: configs/valgrind.supp.\n. Pull request #1597 contains a unit test (currently disabled) to reproduce the segfault.  Hope this helps.\n. Pull request #1597 contains a unit test (currently disabled) to reproduce the segfault.  Hope this helps.\n. All files already include unistd.\n. All files already include unistd.\n. @iglesias: Looks fine.\n. memcheck doesn't see any warnings any more.  Can we consider this solved?\n. StreamingHashedDocDotFeatures currently extends CStreamingDotFeatures, but:\n- OnlineLibLinear requires CStreamingDenseFeatures<float32_t> to work:\n  SystemError: [ERROR] In file shogun/classifier/svm/OnlineLibLinear.cpp line 197: Expected streaming dense feature <float32_t>\n- OnlineSVMSGD requires \n  SystemError: [ERROR] In file shogun/features/streaming/StreamingDotFeatures.cpp line 83: Sorry, not yet implemented .\n- VowpalWabbit gives a Segmentation fault:  ```\n  #0  0x0000000000000000 in ?? ()\n#1  0x00007ffff4601cda in shogun::CVowpalWabbit::init (this=0xdf9b20, feat=0x944d00) at shogun/src/shogun/classifier/vw/VowpalWabbit.cpp:245\n#2  0x00007ffff460133e in shogun::CVowpalWabbit::train_machine (this=0xdf9b20, feat=0x944d00) at shogun/classifier/vw/VowpalWabbit.cpp:138\n#3  0x00007ffff459b35c in shogun::CMachine::train (this=0xdf9b20, data=0x944d00) at shogun/src/shogun/machine/Machine.cpp:67\n#4  0x00007ffff6078c84 in _wrap_Machine_train__SWIG_0 (self=0x96e458, args=0x7ffff7f18490) at src/interfaces/python_modular/modshogunPYTHON_wrap.cxx:570458\n#5  0x00007ffff6079169 in _wrap_Machine_train (self=0x96e458, args=0x7ffff7f18490) at src/interfaces/python_modular/modshogunPYTHON_wrap.cxx:570564\n\n. Any progress on that?  @van51?\n. http://nbviewer.ipython.org/urls/raw.github.com/tklein23/shogun/a8165dcf1948246f26bf1b46f0d2330505260e99/doc/ipython-notebooks/classification/StreamingSparseFeatures.ipynb\n. http://nbviewer.ipython.org/urls/raw.github.com/tklein23/shogun/a8165dcf1948246f26bf1b46f0d2330505260e99/doc/ipython-notebooks/classification/StreamingSparseFeatures.ipynb\n. Have a look on this: http://nbviewer.ipython.org/7188804\n. Have a look on this: http://nbviewer.ipython.org/7188804\n. If you have any suggestions what to show, just tell.  It was only a proof-of-concept that streaming sparse stuff works at all in python_modular.\n. If you have any suggestions what to show, just tell.  It was only a proof-of-concept that streaming sparse stuff works at all in python_modular.\n. It took me lots of time to find a working combination of (streaming sparse features) and (online learner).\nAn introduction into why using streamin/sparse stuff of beyond the scope of this notebook, since it's not introduction into how/why parse large data sets.\n. It took me lots of time to find a working combination of (streaming sparse features) and (online learner).\nAn introduction into why using streamin/sparse stuff of beyond the scope of this notebook, since it's not introduction into how/why parse large data sets.\n. For a minimal example and corresponding valgrind output, please have a look at:\nhttps://gist.github.com/tklein23/7323621\n. For a minimal example and corresponding valgrind output, please have a look at:\nhttps://gist.github.com/tklein23/7323621\n. Looks like everything can be tracked down to: shogun::CLinearMachine::apply_binary(shogun::CFeatures*)\n. Looks like everything can be tracked down to: shogun::CLinearMachine::apply_binary(shogun::CFeatures*)\n. PS: The loop is not necessary for the leak to happen.  it only becomes more obvious.\n. PS: The loop is not necessary for the leak to happen.  it only becomes more obvious.\n. Has been fixed by @sonney2k's commits: e2809507e15ceca22b8414f81263cf27e241b5e7 76f28aa64b64c3d216e0c17aeca05065065d1eda\nClosing ticket now.  Thanks for fixing.\n. Has been fixed by @sonney2k's commits: e2809507e15ceca22b8414f81263cf27e241b5e7 76f28aa64b64c3d216e0c17aeca05065065d1eda\nClosing ticket now.  Thanks for fixing.\n. Tested with shogun_3.0.0\n@karlnapf, @sonney2k: The preamble states that you are responsible.  Can you tell something about it?\n. In IRC we decided to \n- remove set_label(), because it's doesn't make much sense for StructuredLabels\n- to add API doc to describe what \"num_labels\" in the constructor does.  And maybe rename it to preallocate_size.\n. I just confirmed that it's still valid.  Seems to be a won't fix. or a glitch in the labels' API.\nFixing this will break other places.\nFixing this requires that we clearly define how add_label() and set_label() should behave.  If I may suggest:\n- set_label(i, ...) should assert that the labels are initialized big enough, i.e. i < n\n- add_label(...) always enlarges the labels object, even if the object has been created with n not-yet initialized elements.\n. Could be a good entrance task to learn about structured outputs and basic unit tests, but will require changes in quite a few places.\n. new CRealNumber(1); <-- Does 5 (!) allocations of 1024 Bytes\nshogun/base/SGObject.cpp lines 1066-1069\nEach new Parameter() creates a DynArray of 1024 bytes.  And struct label inherits from SGObject.\nIs there anything cheaper for CRealNumber to inherit from?\n. Hot-fixed by: 72f0bb15699f5b5abb90e49b045b97580ce803a9, 22ba728482a7adc4cef586e1b0cf84348a7ac900, ba1a8375d55524f20d353c1ca333931aa38d0833, d52da29673ed829b285ebee0dc32062ce6951484\nMemory usage for 1M labels is now 388 bytes/label:\n==25151==   total heap usage: 11,000,336 allocs, 11,000,336 frees, 388,034,382 bytes allocated\nStill too much, but we saved about 5KiB/label.  So the imminent problem is fixed.\n. I think we should close this issue and create a new one for changing the base class of StructuredData.\n. Agenda so far:\n- Removing base class SGObject from structured stuff.\n- Renaming subclasses so that they don't start with C (otherwise class_list.cpp will fail).\n- Replacing all SG_UNREF on labels by delete.\n- init the data with NULL in labels\n- also free it before the setter is doing the set\n. @sonney2k - I started refactoring the structured label classes, but as already supposed, I ran into problems with label ref counting:\nThe StructuredLabels/StructuredData is widely used in the SO framework and lots of code depends on Labels refcounting themselves.\nTo name one example, where it's not possible: In src/shogun/structure/MAPInference.h we're returning structdata references using get_structured_outputs(), which will be freed in the destructor.  But if we're passing these structdata to structlabels->add_label(structdata), then structlabels will try to free the labels as well.\nSo far it's not possible to take care of all labels properly without either introducing mem-leaks or double-frees.  Could you provide a thin base class, which does only the ref counting?\nI really think adding refcounting to StructuredData is the best way to keep the impact of this refactoring as small as possible.  Refcounting wasn't the problem anyway, so it's only adding minimal overhead compared to what we got so far.\n. I don't know a class ReferencedObject.\nBut SGReferencedData would not help, because ref()/unref() are protected.\nThe most pragmatic way for now seems to split up CSGObject into SGRefObject (does the refcount stuff) and CSGObject (inherits refcounting from SGRefObject).\n. @sonney2k: I prepared a PR #1770.  It's tested and working, no leaks, no double-frees.\nOnly drawback: I introduced a new class SGRefObject, which copies CSGObject, but without parameter stuff.  I think it should be possible to make it a super class of CSGObject to eliminate code-duplication.\n. Due to problems while refactoring, we now have obsolete classes in the code base: SGRefObject and SGDynamicRefObjectArray.  So the remaining task is to decide wheather to remove them or not. \nDespite this, I agree that this issue can be closed.\n. The top three commits are identical to pull request #1771 (the latter commits depend on the infrastructure changes).\n. I'm seeing no easy fix for the arised concerns.  Closing this PR.\nIf someone has got an idea, feel free to reopen.\n. @iglesias: Any suggestions?\n. memory consumption of testscript with 2M multiclass struct labels:\n- before this change: 136MiB\n- after this change: 17MiB\n. I did this by purpose.  And no, I don't think it's possible, because:\n- The model defines which labels to use, e.g. the multiclass model knows that it depends on the multiclass labels.\n- The structured machine cannot know which label class to use without asking the model. (If I knew the label class in advance, I wouldn't need a factory method, but ask for new someSOLabels directly.)\nAgree?\n. Seems to be fixed.\n. There is one last memory error: An uninitialized read in libhdf5.  This happens when using floatmax_t and should be fixed with libhdf5 >= 1.8.12.\n. Fixed with valgrind suppression file.\n. According to this thread, the remaining uninitialized read should disappear with libhdf5 version 1.8.12:\n- http://hdf-forum.184993.n3.nabble.com/Issues-with-H5T-NATIVE-LDOUBLE-td4026450.html\n. Fixed with valgrind suppression file.\n. @vigsterkr: Seems to be fixed.  Can we close it?\n. This PR mixes up a few things:\n- Added parametrized unit tests (now we can test the other solvers for free)\n- Removed individual BufSize variables in every solver -- instead declaring it extern in libbmrm.h\n- Fixing termination criteria to abort iterations if next run would exceeed cutting plane buffer (which holds up to BufSize CPs)\n- Fixing imports in libp3bm.cpp libppbm.cpp\n- Aborting on integer overflow in libppbm.cpp\n- Fixing off-by-one write to beta[ppbmrm.nCP]=0.0; by changing position of ppbmrm.nCP (see discussion in PR #1813 )\n. @sonney2k: The old PR was #1813 and has already been merged (with consent of @uricamic).\nThis PR contains a generalized unit-test and fixes for a bunch of small bugs I fixed on the way to get these tests running.\n. @besser82 -- Can you have a look on this one?\n. Working on this.  Stay tuned.\n. Thanks for your feedback.  Did some micro-optimisations, fixed errors and formatting, added a section where to get help when in trouble, but so far it seems to be enough...\n. In the first place it was meant to become the  \"Quickstart\" page linked on shogun-toolbox.org; mainly to help new users. But merging this won't hurt -- however, we should decide wheather this should be part of the shogun website or not.\n. Hey Dan!  Feel free to take this task!\nI totally agree that we should use standard formates.  I think we can go with SVMlighs format, like shown in data/toy/7class_example4_train.light.  No need to split it into two files.\nThe goal is simply to have something that can be applied easily to own data (without touching source).  Btw., you're not limited to Python - feel free to do it in C++ if you like to.\n. You wrote: Also, wouldn't it be better to have a single file that does training+evaluation? (like you said: evaluate_multiclass_labels.py, but without having the other two scripts) there is a lot of reusable functionality between the two.\nI think it's better to have individual scripts for training, predicting and evaluation.  Evaluation for example is not limited to a specific learning algorithm; you can use it to evaluate everything that outputs multiclass-labels.\nAnyway, if you see reusable code, try to use methods/includes/whatever-codeblocks for it.\n. A suggestion for the \"new\" language, in addtition to my comments on #1883:\n- if we don't use .sg for as meta-language, then we should obsolete the current .sg and replace it by the new one -- don't have another language to maintain. \n- in the first iteration of the new language, could we keep it as simple as possible and avoid using things like loops/branches?  Otherwise we quickly end up in writing another programming language\n. A suggestion for the \"new\" language, in addtition to my comments on #1883:\n- if we don't use .sg for as meta-language, then we should obsolete the current .sg and replace it by the new one -- don't have another language to maintain. \n- in the first iteration of the new language, could we keep it as simple as possible and avoid using things like loops/branches?  Otherwise we quickly end up in writing another programming language\n. As already stated in the project description, the \"new\" language will be very simple:\n- only sequential; loops and branches are not part of the language \n- each command on a seperate line\n- no functions/methods/blocks\nThere is already an example of a possible new language and how the outputs may look like:\n- https://gist.github.com/karlnapf/9079385\n. As already stated in the project description, the \"new\" language will be very simple:\n- only sequential; loops and branches are not part of the language \n- each command on a seperate line\n- no functions/methods/blocks\nThere is already an example of a possible new language and how the outputs may look like:\n- https://gist.github.com/karlnapf/9079385\n. As a start, we could have a \"compiler\" which converts examples/undocumented/cmdline_static/*.sg to a given language.  For example \"sg2python\", \"sg2r\", \"sg2java\" -- instead of inventing a new language?\nAs \".sg\" is currently very simply and doesn't support loops/branches, the \"compiler\" shouldn't be too hard.\n. As a start, we could have a \"compiler\" which converts examples/undocumented/cmdline_static/*.sg to a given language.  For example \"sg2python\", \"sg2r\", \"sg2java\" -- instead of inventing a new language?\nAs \".sg\" is currently very simply and doesn't support loops/branches, the \"compiler\" shouldn't be too hard.\n. No, we're still lacking coverage of these classes.\n. The script itself looks good.  I'm looking forward to the next scripts!\n. If you don't know how to evaluate multiclass labels, you could check the scikit-learn examples; their multiclass evaluation prints some metrics plus a confusion matrix and looks really neat.\nJust in case we're printing more than accuracy, we should consider reusing scikit-learn evaluation.  (Instead of re-writing it on our own.)\n. The rest of the script looks fine, but we should investigate this memory issue first.  Can you call valgrind or gdb on your script, so can see what happens?  The stack trace already tells us to look in SGMatrix::free_data.  Maybe there's some double-free happening...\n. Okay, I looked into the modular python examples and it looks like we're not doing exit_shogun -- did you check if the error still happens if you remove exit_shogun?\n. For me, this looks good now.  A better name for test_multiclass_svm.py would be predict_multiclass_svm.py.\n. Do you still have these memory issues?\n. Okay, lets assume you started your Script with python ./script.py param1 .. paramn and your python is in the path.  Make sure your SHOGUN has been compiled with debugging output.  Then do run it in the debugger (very brief introduction):\n```\n$ gdb python \n\nrun ./script param1 .. paramn ## running the script with all params in the debugger\nbt ## printing back trace\np x ## print suspect variable in current stack frame\n``\n. PS: Runningvalgrind(a memory checker) on your programm can be done:valgrind python ./script.py param1 .. paramn`\n\nPlease note, the you will also see stuff about python, so the output might be a bit messy.  Look for \"invalid read\" or \"invalid write\" or \"uninitialized value\" etc.\n. If you think you found something useful and don't know how to debug further:\n- it's best if you put it into a GIST\n- post it to the list or go to IRC\n- ask something brief and useful ;)\nFinally you will probably have found a bug and file your first issue.\n. Man!  Didn't expect so many comments for that.\n- License will be changed\n- About the formatting: I'm working with many languages, every one has different style.  Tell me a magic indent-invokation and I will apply it.  Making all this by-hand is too painful, sorry!  I refuse spending time on re-indentation! ;)\n- About the array-of-lists: Since it's all sparse, the lists will have different lengths.  SGMatrix is not applicable.\n- About save/load methods: Current shogun SVMlight loading/storing does only support numeric labels.  Multilabel requires something like \"1,2,3,7\", which is not supported yet.  I really spend much time finding out if there is a proper shogun way, but probably there isn't one.\n. Valgrind should be clean, but will check again.  TODOs will be removed.\n. Don't see why we should avoid STL headers.\n. On Saturday 08 March 2014 05:15:45 Viktor Gal wrote:\n\nregarding loading/storing: SVMlight format does support multilabeling? i.e.\n1,2,3,7: 0:1, ...?  if so then we'll need to fix our implementation,\n\nIn case of MultilabelLabels, I'm reading the labels from a seperate file.\nOne could think of an extension to SVMlight format, which simply provides\na comma-separated list of integers, which will then be read into a \nIntegerSet, which can then be added to the \"Multilabels\", but this would\nrequire a lot of API changes on existing classes.\n\nbut none the less directly doing file manipulation here is just 'scary' :P\n\nI know, and I'm totally sorry.  I tried different versions, but they \nall have been stopped by the current APIs.  I didn't want to change too\nmany places... as you know, each changed place will come up with a few\nobjections.\n. On Saturday 08 March 2014 05:15:45 Viktor Gal wrote:\n\nregarding loading/storing: SVMlight format does support multilabeling? i.e.\n1,2,3,7: 0:1, ...?  if so then we'll need to fix our implementation,\n\nIn case of MultilabelLabels, I'm reading the labels from a seperate file.\nOne could think of an extension to SVMlight format, which simply provides\na comma-separated list of integers, which will then be read into a \nIntegerSet, which can then be added to the \"Multilabels\", but this would\nrequire a lot of API changes on existing classes.\n\nbut none the less directly doing file manipulation here is just 'scary' :P\n\nI know, and I'm totally sorry.  I tried different versions, but they \nall have been stopped by the current APIs.  I didn't want to change too\nmany places... as you know, each changed place will come up with a few\nobjections.\n. @karlnapf - I think it's time for another iteration:\n- Formatting and all the other debugging/printing/ASSERT stuff is fixed\n- Changed constructors a bit (see init(...) method)\n- valgrind is still clean\nI did not change:\n- new[]/delete[] -- I think this is okay, since memory is allocated/freed properly\n- SG_MALLOC/SG_CALLOC -- I think this is fine as well, but feel free to annotate the places where you expect problems.\n- load()/save() -- because changing this requires more work on IO infrastructure (I could be an entrance task, but it a medium/hard one.)\n- Changing data structure -- It's very similar to what SGMatrix/SGSparseMatrix are doing\nAnyone who's interested/affected, feel free to annotate.\n. @karlnapf - I think it's time for another iteration:\n- Formatting and all the other debugging/printing/ASSERT stuff is fixed\n- Changed constructors a bit (see init(...) method)\n- valgrind is still clean\nI did not change:\n- new[]/delete[] -- I think this is okay, since memory is allocated/freed properly\n- SG_MALLOC/SG_CALLOC -- I think this is fine as well, but feel free to annotate the places where you expect problems.\n- load()/save() -- because changing this requires more work on IO infrastructure (I could be an entrance task, but it a medium/hard one.)\n- Changing data structure -- It's very similar to what SGMatrix/SGSparseMatrix are doing\nAnyone who's interested/affected, feel free to annotate.\n. On Sunday 09 March 2014 10:13:57 Heiko Strathmann wrote:\n[...]\n\n\nIndentation: sorry but that's your job, not the one of others ;) I do\n  this with eclipse sometimes, maybe ask on the mailing list for tools. Most\n  of the time, I just obey the rules when I write code. \n\n\nI'm changing between different projects with different coding styles.\nIt really makes me sick having to remember them all and not tool to\nfix it automatically.\nIt's not that I want you (or one of the others) doing this - I merely \nwant a consistent formatting rules.  And the only solution is having a\ntool for this.\n\n\nanother rule is: no\n  new/delete, except for new CSGObject instances. This is to ensure we\n  have control over allocated memory in shogun core. It makes things easier\n  to maintain. \n\n\nOkay, I already tried different versions in the past:\n- DynamicObjectArray does only work for CSGObjects, but SGVector isn't\n  one.\n- SGSparseMatrix does something similar, but it's not using parameter\n  framework.  So we have to care about everything (cloning, etc.) by\n  ourselves.\nMaybe there is something different?\n\n\nFile IO: It hurts Shogun a lot if we add \"on the spot\"\n  implementations of things like reading files that are basically just hacks\n  to provide a solution in one particular situation. They basically are\n  impossible to maintain and we had many cases in the past where no-one has\n  an idea how to fix a problem because of such solutions. Some proper IO\n  class to handle this format (that is well tested and documented) is the\n  only suitainable solution - so yes, this would be a great entry task.\n\n\nI could create an entry task for this, but\n- it doesn't solve the problem for this PR\n- I cannot supervise this entrance task, since I don't know the IO stuff\n  in shogun.\nAnd, think what happens if we merge this and nobody solves this entrance\ntask?  My PR will be pending forever and in six months nobody knows what\nto do about it.\nOne last solution for the IO: I'm removing the save/load code from the \nclass and everybody needs to full the structure by himself.  But then \nit's pretty useless for enabling new users to do multilabel stuff.\n. @karlnapf:\n- Indentation is fixed now (and we have a PR #1985 waiting for integration)\n- init() method contains disabled code for proper parameter-registration (can be enabled as soon entrance #1972 is solved)\n- Serialization code is disabled and waits for entrance #1987 to be solved\n. @karlnapf:\n- Indentation is fixed now (and we have a PR #1985 waiting for integration)\n- init() method contains disabled code for proper parameter-registration (can be enabled as soon entrance #1972 is solved)\n- Serialization code is disabled and waits for entrance #1987 to be solved\n. Done.  Ready to merge now. ;)\n. If #1958 has been merged, please check that the disabled parameter-init-code in init() will be enabled.\n. If #1958 has been merged, please check that the disabled parameter-init-code in init() will be enabled.\n. @karlnapf - According to the principle of \"seperation of concerns\" this is a good idea.  If the singletons are immutable, this is a very good idea, because it's indeed just encapsulating global data.  (Using global variables would be fine as well. ;))\nThe the variables are mutable (like random or parameter framework), we'll get problem with threads.\nIf you're really trying to put this stuff into singletons, then please make a first draft and lets check if it's obfuscating the code too much.\nputting this to issue #2112 to separate the discussion.\n. @karlnapf -- Added a reference of how to get astyle.  Ready to merge.  Travis shouldn't matter.\n. @vigsterkr - for my part, I would be happy to have a file like described.  Extending libsvm reader is nice, but IMO not neccessary.  Having a reader which simply returns a list of SGVector<intxx_t> would really be enough.\nIf we take care, that every label line corresponds a feature line, then reading current libsvm parsing is enough.  What's the problem with that?\n. @vigsterkr - I'm convinced.  Let's do it your way!\n. Fixed by #2073\n. Okay, the situation toally changed.  We (me+@lisitsyn) dived into it and what we found is crazy:\n- the problem happens with a file that contains exactly 5242880 lines of \"0\\n\"\n- this makes 10 x 1024 x 1024 == 10 MB\n- this means we've got a problem with files bigger than 10 MB\nWhere this come from?\n- https://github.com/shogun-toolbox/shogun/blob/4c4a5b277119ff700afa7ab876d88db1de3b66de/src/shogun/io/LineReader.cpp#L29\n- m_max_token_length=10*1024*1024;\nSo I think we should change the focus of this issue - not touching CCSVFile, but instead fixing CLineReader.\n. @sonney2k -- Each line has 1 byte + newline, so it's clearly a bug.  My test file looks like this:\n0\\n\n0\\n\n...\n0\\n\n5.242.880 lines in total.  Looks like a valid file to me.\n. Viktor, I guess it happens because the file has 12MB.  Reducing to 16k lines will remove the problem, because the file is small enough (less than 10 MB) again.\nAm 07.04.2014 um 10:49 schrieb Viktor Gal notifications@github.com:\n\n@tklein23 any ideas why that shit happens with 20k lines? (see the bug report)\n\u2014\nReply to this email directly or view it on GitHub.\n. @gsomix - Any progress in here?  Seems to affect other places as well. \n. Hey @gsomix - sorry for bugging again, but I want to write an example for the workshop (27th/28th of July).  This is currently a show stopper, because I cannot explain people that shogun cannot handle files bigger 10 MB.\n. @karlnapf: If you read a file bigger than the ring buffer, then there is an additional feature vector.  The ring buffer defaults to 10MB.\n\nIn other words:\n- A csv file with 10.000 lines with less than 10MB will produce 10.000 vectors.\n- A csv file with 10.000 line with more than 10MB will produce 10.001 vectors.\nThe bug itself seems minor.  But as we're checking dimensions of features and labels everywhere, it will produce \"unexpected\" errors on bigger files.\n. @karlnapf: If you read a file bigger than the ring buffer, then there is an additional feature vector.  The ring buffer defaults to 10MB.\nIn other words:\n- A csv file with 10.000 lines with less than 10MB will produce 10.000 vectors.\n- A csv file with 10.000 line with more than 10MB will produce 10.001 vectors.\nThe bug itself seems minor.  But as we're checking dimensions of features and labels everywhere, it will produce \"unexpected\" errors on bigger files.\n. Can you please do\n- a stacktrace with gdb as I described in #1954?\n- post valgrind errors for invokation of your script?\n. By the way - the stacktrace you posted already shows, that it happens when freeing a SGMatrix (probably the confusion matrix?):\n/usr/lib/libshogun.so.14(_ZN6shogun8SGMatrixIiE9free_dataEv+0xd)[0x7fb939c7781d]\n/usr/lib/libshogun.so.14(_ZN6shogun16SGReferencedData5unrefEv+0x53)[0x7fb939af7b63]\n/usr/lib/libshogun.so.14(_ZN6shogun8SGMatrixIiED1Ev+0x24)[0x7fb939c77ca4]\n. Closing as it's not happening any more.\n. Seems that the error only happens with:\n- LC_ALL=de_DE.UTF-8 ./shogun-unit-test\nbut not with:\n- LC_ALL=en_US.UTF-8 ./shogun-unit-test\n- LC_ALL=C ./shogun-unit-test\n. I can simply fix this by setting the locale manually (setlocale or SG_SET_LOCALE_C), but it's very fragile if such basic functions change their behaviour.\n@sonney2k - I suggest that we always SG_SET_LOCALE_C on startup to make shogun more deterministic.\nBtw.: man 3 setlocale states:\n- The locale \"C\" or \"POSIX\" is a portable locale; its LC_CTYPE part corresponds to the 7-bit ASCII character set.\n- On startup of the main program, the portable \"C\" locale is selected as default. -- Why do we override this?\n. It's used in only a few parts (grep for strtold, strtod and strtof):\n- Math.h (where it's wrapped)\n- SerializableXmlReader00.cpp\n- SGInterface.cpp\n- SGIO.cpp\n- Parser.cpp (and therefore in LibSVMFile, CSVFile)\nWhen using CParser directly, you have to take care about SG_SET_LOCALE and SG_RESET_LOCALE by yourself.\nCan't we use the locale-independent versions strtod_l, strtof_l, strtold_l instead?  I'm wondering if it's available on most platforms.\n. I cannot imagine why this should be the default, since it can be (with my patch) overridden using cmake -DCMAKE_C_FLAGS=-g.\nCurrently it's worse: if I don't want it, I cannot override it.\n. @karlnapf - the issue is not critical, so I don't mind waiting for @sonney2k.\nRegarding your comment \"and have the debug stuff as a separate flag\": This flag already exists, because you can always manually add -DCMAKE_C_FLAGS=-g -DCMAKE_CXX_FLAGS=-g when calling cmake.\n. Can we see the ipython notebook somewhere with outputs included?\n. Regarding failing travis: \nJust have a look on the travis log, it shows that there is some problem with git.  It seems like you updated \"data\" in your commit, but didn't push it upstream.  So it cannot fetch the reference and fails.\n. I used static code analysis and it was suggesting \"diffs\" on how to change the sources.  The automagical fix would do too much... consider this one to be a proof-of-concept and later we can check how to improve.\n. Maybe we get something similar with clangs next release, so we see it in the static analysis.\n. Code itself looks good to me.  I'm happy to see unit-tests for that.\nBut I'm really confused about 1000 lines for reading one file format.  (@abinashpanda - I know it's not completely your fault, but it's a good time to discuss this.)\nThe header file contains about 450 lines, the implementation 350.  Seems to be 90% boiler-plate-code.  Is this really necessary or did we just massively duplicate code?  Any suggestions how to improve it?\nAny objections against using function templates to reduce the size of the header file?  Shouldn't be a big deal - let me know if you need any assistance (i.e. documentation/examples) for this.\n. I'm convinced that it requires bigger changes.  I'd say it's beyond the scope of this issue.\n@hushell - as you created the issue, finishing this issue is up to you.  Please check if we're done here.\n. @abinashpanda - welcome back.  I hope you are fully recovered.\nWhat's the status of this PR?\nLast Travis built failed because SerializationXML.UAIFile is broken.  Do you know what happened here?\n. @abinashpanda - welcome back.  I hope you are fully recovered.\nWhat's the status of this PR?\nLast Travis built failed because SerializationXML.UAIFile is broken.  Do you know what happened here?\n. @abinashpanda - Congratulations.\nLet's wait for Travis and @hushell to merge. ;)\n. @abinashpanda - Congratulations.\nLet's wait for Travis and @hushell to merge. ;)\n. I'll try to remember the changes until next time. ;)\n. At least python-modular and libshogun are working locally.  I'll check it later.\n. @Jiaolong - please check what's missing here.  Last comment was 11 days ago.  Looking forward to hear from you.\n@hushell - are you in touch with @Jiaolong regarding this?\n. @kislayabhi, I was quite busy and not following the IRC.  Any update I missed?\n. @achintp, I seems you're stuck.  Do you need assistance on this issue?\n. It clearly is the duty of each developer.  But if you have a basic setup that is known to be working, it's easy for those who haven't seen C/C++ for a few years.  Just making it easier for those people to contribute.\nAren't researcher willing to do use every crazy language, every crazy framework, as long there is something to start from easily? Not to say they are lazy... just... comfortable. ;)\n. No new features, but less code, hopefully better tested code of better quality.\nUse?  None.  Sorry to disappoint you. ;)\n. About model selection again: Which classes do need the model selection framework and which don't?\n. It's about cleaning up things.  Making them maintainable.  If things are so tied together as they are right now, then this will bring us pain.\nCurrently I'm not clear what we can do easily and what we can't do.  But I see:\n- data classes, like features, vectors, matrices, streaming features, etc. are not using model selection, so why do they have members to do this?\n- file reader classes don't use (or don't need) parameter framework or model selection\n- SG_ADD takes care about parameter framework and model selection\n- etc.\nWhat is it that you're not sure about?\n. @karlnapf - I'm curious about replace_subset() as well.\n@mazumdarparijat - Do you suggest to add replace_subset() to CFeatures or to CSubsetStack?\n. @karlnapf - I'm not happy with your suggestion. Here is mine:\n- Why are CFeatures instanciating CSubsetStack on their own?\n- Why not add a CFeatures::set_subset_stack to features to set an user-defined instance of CSubsetStack?  (We can also call it replace_subset_stack, but the idea stays the same)\n- I see new CSubsetStacks in DenseFeatures and IndexFeatures as well, but we could use orig.m_subset_stack.clone() here\n. We should check something else first:\n- What's the size of the learning problem?  (# instances or # features or #iterations or # GiB or ...) \n- What's the overhead the current implementation introduces?\n. @iglesias - which is the \"current approach\"?\nIf you mean the set_amnesic_mode(bool):\n- In OOP you're not doing polymorphism by behaviour-switching flags.  I know we're doing it a lot in SHOGUN, but this shouldn't be a reason to continue this BAD practice.\n- And telling the CFeature class which SubsetStack to use shouldn't be hard at all (some way or another) \n- Beside this, I see at least one pragmatic way of doing what Heiko wants.\nStill, I think we should check the overhead/limitations of the current implementation.\n. Ah, okay!  Well, I don't have any problems with the currently implemented approach, but Heiko was complaining about it's memory efficiency.  When it came to \"switching behaviour\", it jumped in. ;)\n. Not instanciating the SubsetStack on their own would be one solution that wouldn't include the \"magic-different-mode\"-flag.  And it would be easy to implement as well, because two \"new\"s I found were just cloning the SubsetStack; the third one can be solved by replace_subset_stack.\n. @karlnapf - Cool, memcheck is a bit cleaner now.  Thanks to @lambday.\n. @lambday - It's like a curse!  Every time I try something simple (or try to fix something simple) it ends this way!\n. @lambday - In fact I like tracking nasty bugs down... but it also annoys me. ;)\n. @lambday - Something seems wrong with CList.  Can you please have a look on the buildbot logs?\nhttp://buildbot.shogun-toolbox.org/builders/nightly_none/builds/698\nIt produces tons of warnings and errors related to CList and refcounting:\nsrc/shogun/lib/List.h:121:15: warning: variable \u2018d\u2019 set but not used [-Wunused-but-set-variable]\nsrc/shogun/lib/List.h:388:15: warning: unused variable \u2018p\u2019 [-Wunused-variable]\nsrc/shogun/lib/List.h:427:19: warning: unused variable \u2018temp\u2019 [-Wunused-variable]\ntests/unit/lib/List_unittest.cc: In member function \u2018virtual void ListTest_get_first_element_ref_count_delete_data_false_Test::TestBody()\u2019:\ntests/unit/lib/List_unittest.cc:199:112: error: \u2018class shogun::CList\u2019 has no member named \u2018ref_count\u2019\ntests/unit/lib/List_unittest.cc:203:112: error: \u2018class shogun::CList\u2019 has no member named \u2018ref_count\u2019\ntests/unit/lib/List_unittest.cc:208:117: error: \u2018class shogun::CSGObject\u2019 has no member named \u2018ref_count\u2019\n. @lambday - you should be able to reproduce locally with\ncmake -DENABLE_TESTING=ON -DHAVE_LARGEFILE=OFF -DUSE_SVMLIGHT=OFF -DUSE_SHORTREAL_KERNELCACHE=OFF -DUSE_HMMPARALLEL=OFF -DTRACE_MEMORY_ALLOCS=OFF -DUSE_PATHDEBUG=OFF -DUSE_HMMCACHE=OFF -DUSE_BIGSTATES=OFF -DUSE_REFERENCE_COUNTING=OFF -DUSE_LOGSUMARRAY=OFF -DUSE_LOGCACHE=OFF -DCMAKE_BUILD_TYPE=Debug ..\n(I guess simply using -DUSE_REFERENCE_COUNTING=OFF would do it as well, but above is what buildbot used.)\n. Have a look at #2111 - it reminds me of your issue.  Can you check if upgrading gcc to >= 4.7.3 works?\n. To see if it worked out, please force nightly_all after merging this.  Thanks.\n. Fixed the buildbot.  No more actions required.\n. @lambday - Outch, it's more than I expected.\n. Does not solve the problem; closing.\n. Does not solve the problem; closing.\n. PS: I made the filter as specific as possible, so it shouldn't shadow any other memory problems.\n. Well, this multilabel code was partly developed while I was working on the Zuse-Institute, partly in my free time.  Thus, I cannot omit this notice.\n. Thanks.  The leak disappeared from the log.\n. @yorkerlin - pthread leaves some memory allocated.  This is a known issue and we have suppression rules for this.  Try running valgrind with --suppressions=configs/valgrind.supp (the config directory can be found in the shogun root).\n. > Looks pretty good to me, and you are right, the travis failures are unrelated.\n@karlnapf - I see this test failing from time to time.  Does someone work on a fix?\n. @abinashpanda, thanks for you PR.  Looks good so far, but we need to do some adjustments, otherwise we'll not be able to handle bigger input spaces.\n. Thanks for your PR.  Looks like we cannot remove psi_truth/psi_pred vectors from CResultSet, because we would introduce a big performance penality in PrimalMosek (correct me if I'm wrong).\nCan you please double check?\n. Fine.  Travis fails don't seem to be related to you.\nChanges are looking okay so far, but I think you changed a bit too many places.  I made a few suggestions how to make it less error-prone with less changes to the existing models.\n. Hey @hushell - crazy stuff.  I will take a few days to review this.  What's special about this solver?  What is it's main focus?\n. calling ASSERT(features[i].feat_index < dimension); in the loop\nis effectively the same as ASSERT(get_num_dimension() < dimension); outside the loop.\nAny preference regarding to style/readability?\n. No, later in your test you're using \"num_features\" as the number of non-zero-features, which is different from number of dimensions (i.e. max feature index plus one).  That's why I needed to differentiate.\n. I can remove the TODO comment if you like to - but I won't touch SGSparseMatrix because I'm afraid to break other tests/examples.\n. Didn't know that, I'll try.\n. These changes might look useless, since they're not functional.  But:\nWhile debugging it wasn't obvious to me that both branches are manipulating m_w, so I decided to make it more obvious.\n. I'm seeing that a break is missing here.  Will change soon.\n. SGSparseVector didn't use \"map\"\n. Yes, it should.  Just testing you. ;)\n. Will fix it.\n. Two options I've been thinking about:\n- We could ask Heiko if he could provide us something to register unserializable variables (e.g. SGRefObject)\n- Wrapping MAPInference::m_outputs into StructuredLabels, which leads to the next problem I did not realize in StructuredLabels::init():\n  SG_ADD((CSGObject**) &m_labels, \"m_labels\", \"The labels\", MS_NOT_AVAILABLE);\nPlease note, that CStructuredLabels::m_labels is no CSGObject any more.  So we lost serialization of StructuredLabels as well. So we need to decide about both...\n. The order does not produce the same results: IIRC, the old order was wrong and left beta[0] uninitialized, while writing to beta[BufSize].  So the index clearly exceeded the valid range 0..(BufSize-1).\n. The given change was neccessary, because beta[0] was never re-set, but beta[BufSize] exceeded beta's array bounds.\nAnyway, it seemed that this is an off-by-one, since diag_H and I have been set at position nCP instead of nCP+1.\nReverting this would introduce a memory violation - so maybe we can analytically create a toy example to show whats right?  Or show a pathological one to show when we're wrong?\n. Okay, let's analyze: nCP denotes the new cutting plane, we're going to initialize.  If beta's, I's, H's, H_diag's indices correspond, then we're setting the coefficient of the new curring plane to zero, which should be fine.\nMaybe you can explain, why we should be setting I, H and H_diag at position nCP, but beta at position nCP+1?\n. Well, I don't see, why we're using different indices for initializing beta and I, H, H_diag.  The index of the active beta is always one ahead.  We're initializing everything at index nCP, but instead of beta[nCP] we're touching beta[nCP+1].\nAfter looking at clean_icp() I'm very sure that the indices of beta, I, H and H_diag should correspond.  In this method we're moving memory to get one free cutting plane!  Suppose, we got one free cutting plane at index foo.  Then the next iteration initializes I, H and H_diag at index foo, but keeps beta[foo] untouched.\nWe're thus touching an unrelated beta[foo+1], but keep working with an uninitialized beta[foo].  Agree?\n. Any specific reason you decided to use GaussianKernel?  I would recommend using a LinearKernel first, but maybe you did it by purpose?\n. What is this @contextmanager doing?\n. What's track_execution()?\n. The initial thought was to provide an example script that is easy-to-use and is a working starting point for own stuff.  We should not try to mimic the complete libsvm/liblinear/svmlight API.  So I would recommend having at most 2 different kernels to keep it simple?\n. It doesn't seem to complicate things too much, so I don't mind if you keep this.\n. I see, thanks for explaining.\n. I don't think we should use test_feats while training.  Just doing kernel.init(train_feats, train_feats) in training should be fine; in testing use kernel.init(train_feats, test_feats)\nPlease check with other scripts/unittests/examples, how to initialize kernels properly.\n. Maybe we have a misunderstanding on the evaluate_multiclass script: I was intended to take two label-files and compute some metrics for them.  Nothing specific algorithms should be called in there.\nSimply reading two label-files (expected labels, predicted labels) and printing some metrics for it.\n. Don't forget to replace /tmp/model3 by something less static. ;)\n. ensure_valid was simply copied from the other label classes.  IIRC context is only used for printing.\n. I just moved it down so we know it's only needed for the both following methods, so we won't forget it.  But I will move it back. ;)\n. It would be cool if the test script only writes the \"predicted_labels\" to the output file, so we can read it by external programs.\nSecondly, if the evaluation code could be outsourced to an extra script evaluate_multiclass_labels.py, we can re-use it for everything that looks like multiclass data.\n. Please make sure that \"actual\" and \"predicted\" are stored in the same format on disk.  Simply make it the same format like in SVMlight, but without features: One label per line, text format.\nAnd please rename this file to \"evaluate_multiclass_labels\", because it has nothing to do with SVMs any more.\n. Something weird with indentation here.  Hit \"reformat\" in your favourite editor. ;)\n. You're almost done: Please save predicted_labels in the same (non-binary) format like the training data.  This allows us to view and manipulate the labels in arbitrary ways, because viewing/manipulating textual data is so easy on UNIX.\n. What's the meaning of \"width\" with linear kernels?\n. https://github.com/shogun-toolbox/shogun/blob/develop/doc/md/CodingStyle.md\n. I did some (semi-automatic) fixing of imports to make static analysis happy.  Sorry for missing this!\n. I suggest \"mat_label\" to \"labels\" to align naming in both functions.  Even better would be \"multilabel\", so the name is telling exactly what it will contain.\n. The error message is misleading: What does \"read multilabel with SGSparseVector type\" mean?  Which method call has to be changed if a user sees the message?\nSince an average user don't know what this means, we could say \"This is a multilabel file.  You are trying to read it with a singlelabel reader.\"\n(Still, the user doesn't known which file failed.)\n. This is really bad (I know, it's not your fault) - we should really use function templates here.  If you like C++ templates, then have a look on MultilabelLabels::to_dense.  Just a side note; no need to change it right now.\n. I guess LT_MULTICLASS_MULTIPLE_OUTPUT will not be needed any more.  Did remove it from the label-types enum?\n. Can we try to use function templates here?  IMO it whould really reduce the amount of duplicated code.\n. Can we try to use function templates here? IMO it whould really reduce the amount of duplicated code.\n. On Sunday 30 March 2014 00:52:18 Shell Hu wrote:\n\n\n+}\n+\n+CUAIFile::~CUAIFile()\n+{\n-    SG_UNREF(m_tokenizer);\n-    SG_UNREF(m_line_tokenizer);\n-    SG_UNREF(m_parser);\n-    SG_UNREF(m_line_reader);\n  +}\n  +\n  +void CUAIFile::init()\n  +{\n  +\n-    /** Can only be enable after this issue is\n  https://github.com/shogun-toolbox/shogun/issues/1972 +     * resolved\n-     * SG_ADD(m_factors_table, \"m_factors_table\", \"table of factors\",\n  MS_NOT_AVAILABLE);\n\nInteresting, in this way, it passed the clone tests. Use \"factors_table\" not\n\"m_factors_table\".\n\nNo surprise: No registration takes place, so it's simply serializing an\nobject without members.\nIf we're only missing support for SGVector, can't we just enable all\nother members and keep the affected ones disabled until #1972 is solved?\n. > indent issue\nIf you don't want to indent yourself:\n- https://github.com/shogun-toolbox/shogun/blob/develop/doc/md/CodingStyle.md\n. Including SGVector.h more than once? ;)\n. ~~I'm not sure if simply removing is a valid solution to the problem.  For me it seems more that num_lines++ is wrong only if skip_line() has been run.  IMO to make sure that the num_lines++ doesn't go together with skip_line(), we should move it inside the if-branch.~~\n~~Removing skip_lines() will slightly change the behaviour of the library, which is probably not desired.~~\n. Removed my last comment, because it was wrong.\n. Hey @iglesias - is indeed needed here, because the test uses SG_SPRINT and broke when I removed SGIO.h from SGObject.h.\nBefore it only worked because it was included indirectly (transitive): io_linereader.cpp included something, (that itself included something,) that included SGIO.h.\n. @abinashpanda -- For fixing the coding style I recommend:\n- https://github.com/shogun-toolbox/shogun/blob/develop/doc/md/CodingStyle.md\nIf you find that the style can be improved, please let me know.\n. I have a second PR in the queue regarding this issue - I'll do it there.  Thanks for checking.\n. You're so cruel!  My eyes are bleeding when I'm seeing if-else without {}.  (I'm was a perl guy, but the compiler design lecture broke me. ;))\nHave a look on this question and you know why I'm so unhappy about it:\nhttp://stackoverflow.com/questions/7644982/ambiguous-if-and-else-branches-is-the-behaviour-defined\n. Done.\n. Where exactly?  After the else {}?\n. Okay-okay.  Guilty as charged. ;)\n. I'm just copying things around to get rid of the warnings.  Sure I should fix all the style issues as well?\n. What's the difference?\n. What do you mean?\n. Okay, I'll remove it.  But then nobody will remember that this is a mess. ;)\n. No!  You're missing the 3 lines in between! ;)\n. I understand this for performance-critical parts, but this one is usually executed once and then never again.  I can take care of this in the future - but do you really want me to spend time on changing this?\n. Let me try to edit this in-line\n. Editing source-code on the web using github is great.\n. This memory is never initialized, so maybe we should keep calloc here to avoid uninitialized reads in valgrind.\n. Sorry for fixing. ;)\n. Yes.  Before, the method was a no-op.  Now it's really creating a centered matrix.\n. - I'm just following the style of this class (no init method)\n- Didn't know that initializer lists are bad.\n. Indeed.  I dived down to DynArray and this sadly seems to be true... let's hope that it's checked when setting preprocessors...\n. Well, this code cannot do anything: setting i=0 and then checking i<0 is clearly not walking over the matrix.\n. Not sure - I remember that in C++ new float will be zero initialized automatically.\n. Okay, hmm.  So... does it mean I have to carefully chose the files I fix; making sure the style is \"good enough\" so you don't let me fix the whole file? ;)\n. Thanks for sharing, good to know.\n. I'll remove it tomorrow.\n. @Jiaolong - Please always check if you have to SG_FREE/SG_UNREF the pointers you get.  (Commenting this because you edited this.  Just reminding.)\n. Simply removing them should work as well.  Don't you think initializing them explicitly is better?  (If so, I'll try online-exiting of patches again. ;))\n. @Jiaolong - No, please don't.  In this case SG_FREE is legitimate, because we don't have a solution for lists of SGVectors.\n. Can you please be a bit more verbose?  I don't see where it's wrong.\n. Sorry man.  As I said 100 times, don't force me to fix the formatting of code I didn't write.  I just fixed a bug.\n. I don't know what you're talking about.  Sure you want to slow my contributions down by blaming me for newlines?\n. This really annoys me.  And I don't believe that we're discussion about formatting.  It's about a newline!\n. Am 27.04.2014 um 16:09 schrieb Heiko Strathmann notifications@github.com:\n\nIn src/shogun/io/SerializableFile.cpp:\n\n}\n-   m_fstream = fopen(m_filename, mode);\n-   if (!is_opened()) {\n-       SG_WARNING(\"Error opening file '%s'\\n\", m_filename)\n-       close(); return;\n-   FILE* fstream = fopen(fname, mode);\n-   if (!fstream) {\n  I am not blaming you, sorry if it seems like that. I am just asking you to respect the standards we have agreed on. Everyone else does too.\n\nI don't know what you're talking about. I just want you to accuse me for things I didn't do again and again.  And I don't want so say that another ten times.\nIt doesn't slow down contributions, only discussing this every time again does.\nIt annoys me too to have this discussion, but note that I (and everyone else) ask all contributors to do this.\nAgain, please let us keep focused on the content. If I make mistakes, I'm always trying to fix them ASAP.\n\nAnd about formatting: I added a CodingStyle and adjusted it to the rules[tm].  And I fixed the classes I wrote as well.  So what else do you want beside blaming?\nIf we can focus on the technical parts, I'm fine. But I'm not fine spending just half a minute fixing formatting in code that I didn't commit.  \nEOT.\n. Be more specific: call it SDT_SPARSE_MULTILABEL.\nAfter fixing @iglesias suggestions, fix all files with this formatter:\n- https://github.com/shogun-toolbox/shogun/blob/develop/doc/md/CodingStyle.md\n. CSparseLabel should be called CSparseMultilabel instead.\n. We need some simple unit-tests here to make sure the vector is correct:\n- get_joint_feature_vector([...], [1]) == m\n- get_joint_feature_vector([1], [...]) == n\n- get_joint_feature_vector([...], [...]) == m*n\n- get_joint_feature_vector([1 0 2], [0 1 0]) == [0 0 0 1 0 2 0 0 0]\n- get_joint_feature_vector([1 2 3], [1 1 1]) == [1 2 3 1 2 3 1 2 3]\n. What about an easy unit test here?\n- loss([1 1 1], [1 1 1]) == 0.0\n- loss([1 1 1], [1 0 1]) == cost_fn\n- loss([1 0 1], [1 1 1]) == cost_fp\n- loss([0 1 0], [0 0 1]) == cost_fp + cost_fn\n. Please add simple tests here for both cases:\n- training == true\n- training == false\nTo keep it simple you only need to check if the predicted label in psi_pred is correct for some easy examples. (Only three labels, short weight vectors like [-1 +1 0], etc.)\n. I don't like this m_last_set_label, but I don't (yet) see a better solution.  We can improve this later.\n. This is exactly the stuff I want unit-tests for.  Suggested changes:\n- this code shouldn't happen in StructuredAccuracy, because now it cannot be reused for non-so-multilabels\n- we should add unit tests for a few examples\n. Instead of similar_lab we should use the well-known term tp\n. This is a fn\n. This is a fp\n. Avoid using dense vectors for features if possible - see my gist how to hash sparse vectors:\n- https://gist.github.com/tklein23/44f7b03be81743bd17ca\n. Why are you getting dense vectors here?\n. Can you declare int label = slabel_data[i] for better readability?\n. You know that you're writing at most c entries of this vector.  No need to use CDynamicArray for this; using of simple static-sized SGVector will avoid resizing vectors.\n. ~~(You can even use a sparse vector and fill it iteratively, so you save memcpy'ing manually.)~~\n. - do y_pred = SGVector(m_num_classes), set y_pred.vlen = 0\n- remember how many elements, for example using y_pred.vlen++\n- do new CSparseMultilabel(y_pred.clone()); or clone_vector(vector, len)\n. phi must be sparse.  If your feature space has 2**20 == 1M features, then this will always read 4 MB of data for a simple dot-product.  This is not necessary.  Use sparse features.\n. Now it's getting interesting.  Currently psi_pred is a dense vector.  This has to be changed... I'm suggesting of removing ResultSet.psi_pred and ResultSet.psi_truth completely.  \nYou'll have to do minor changes to a few places for this, but for me it seems like a minor refactoring.  (The calling code have to use model->get_joint_feature_vector(feat_id, labels) manually.)\n. We need another method here for creating a sparse joint feature vector.\n. What is C and does this assignment is still valid for hashed features?\n. Why did you remove the checks on slabel_1?\n. Same here: You can still check slabel_2\n. Same here.\n. Same here.\n. Seems that you're patch introduces additional overhead: It's computing joint vector twice: One time inside Model::argmax and one time in machine.\nI think the slack-computation should now be done inside the machine, not the model.  As this variable is only printed and doesn't contribute to the result, this shouldn't hurt.\n. If someone complains, we can put the printing inside the machine(s).\n. Is this necessary?\n. Why did you change this?  Is this necessary?\n. I now see what the problem is.  We need the joint_feature_vector inside the loss function.\nAre you sure that this is not breaking PrimalMosek solver?  From what I see, we're introducing big performance penalty for PrimalMosek, because it has to recompute feature vectors in every iteration.\nSeems that PrimalMosek forces us to keep the psi_truth/psi_pred vectors?\n. This should be the default, so nobody has to init them it explicitly:\nret->psi_computed_sparse = false;\nret->psi_pred_sparse = SGSparseVector<float64_t>(0);\nret->psi_truth_sparse = SGSparseVector<float64_t>(0);\nret->psi_computed = false;\nret->psi_pred = SGSparseVector<float64_t>(0);\nret->psi_truth = SGSparseVector<float64_t>(0);\n. No need for this initialization if you init CResultSet properly.\n. This should stay, but be changed like this:\nCResultSet* result = m_model->argmax(m_w, i);\n+    m_model->compute_dense_vectors(ret);\nnew_constraint.add(result->psi_truth);\nresult->psi_pred.scale(-1.0);\nnew_constraint.add(result->psi_pred);\n. This should be done in the model like this (naming of functions is up to you ;)):\npublic void compute_dense_vectors(CResultSet ret)\n{\n   if (!ret.psi_computed)\n   {\n      ret.psi_true = ...;\n      ret.psi_pred = ...;\n   }\n}\npublic void compute_sparse_vectors(CResultSet ret)\n{\n   if (!ret.psi_computed_sparse)\n   {\n      ret.psi_true_sparse = ...;\n      ret.psi_pred_sparse = ...;\n   }\n}\nThe idea is that sparse and dense psi are only computed on demand.  If you already computed the values once, don't do it again, because results are cached.\nOn demand computation is done in the model, so nobody has to manipulate CResultSet on its own.  Results are cached/stored in CResultSet.\n. You already have \"not implemented message\" in CStructuredModel::get_sparse_joint_feature_vector.  No need to override this method here.  Saving you a few code lines and benefiting from what you did in parent class.\nThat's why we have parent classes, to provide \"default behaviour\".\n. You already have \"not implemented message\" in CStructuredModel::get_sparse_joint_feature_vector.  You should be able to remove it here without problems.\n. Not initializing CResultSet manually - should be default.\n. You already have \"not implemented message\" in CStructuredModel::get_sparse_joint_feature_vector.  You should be able to remove it here without problems.\n. You already have \"not implemented message\" in CStructuredModel::get_sparse_joint_feature_vector.  You should be able to remove it here without problems.\n. You already have \"not implemented message\" in CStructuredModel::get_sparse_joint_feature_vector.  You should be able to remove it here without problems.\n. No, no, this is too much.  You should simply call m_model->compute_dense_vectors(ret); before accessing the dense vectors.  Then you're change will reduce to one line.\n. Same here: You should simply call m_model->compute_dense_vectors(ret); before accessing the dense vectors. Then the diff will be quite smaller.\n. Same here: Too much code for simply computing the psi... simply call m_model->compute_dense_vectors(ret); just before doing the SGVector::add.\n. Add another variable here which denotes if psi_truth/psi_pred have been initialized.\n. To make sure the boolean values are initialized properly, you could do (something like) this:\nboolean psi_computed = false;\nboolean psi_computed_sparse = false;\nSee: \nhttp://stackoverflow.com/questions/16782103/initializing-default-values-in-a-struct/16783513#16783513\n. This one is a duplicate.\n. You should add an assertion here if none is enabled.\n. This method will return nothing if both are false.\n. psi_i will not be changed if both flags are false.\n. Shouldn't you include SparseMultilabel.h here?\n. SparseMultilabel.h?\n. Please don't rely on implicit imports.\n. With this initialization, each labeling contains label_0.  Can you check this?\n. You have to remember how many labels you added using a counter variable (you can abuse y_pred_dense for this).\n. I don't see a reason for this +1.  Can you elaborate?\n. Why this?\n. Why messing around with dimensions here?\n. I don't see why this was removed.  I think this is needed for proper learning.\n. Come on.  I sent you a gist with my CLR-code I carefully wrote some time ago.  Please don't just put your name there.\n. I don't understand this assertion.  psi_1 shouldn't be that big - so what about checking it directly?\n. You're putting two different tests into one test case.  I suggest to split them.\n. I suggest something different here.  First create a bunch of label vectors:\n- labels_nil = {}\n- labels_23 = {2,3}\n- labels_34 = {3,4}\n- labels_1234 = {1,2,3,4}\nAnd then just do all the loss-computations in one place:\nEXPECT_EQ(xxx, model->delta_loss(slabel_12, slabel_23));\nEXPECT_EQ(xxx, model->delta_loss(slabel_23, slabel_12));\n...\nThis would make the tests more readable.\n. DIMS and NUM_SAMPLES are useless if you only set six values.  I suggest fixing them.\n. Here you set \"3\" examples, while above you used NUM_SAMPLES.  Decide which approach to take.  As we may want to test with different number of labels, examples, features, it's not good to fix it for all unit-tests.  Make it local to the test.\n. Please assert here that get_dim returns the right values.\n. Why so many dimensions?  Can you explain how you got to 12?\n. I think it would be good to have a separate unit test which simply tests constructor/initialization.\n. I don't understand the constants you're using here.\n. Why only \"EXPECT_GE\" here?  Can't we test something more useful like \"score(x, label1) >= score(x, label2)\"?  Also, I suggest to unroll the loop.\n. Why not initializing num_classes=0 and removing the else-branch?  Just to make it more compact. \n. \"Number of classes must be equal to taxonomy vector\"?\n. Something like this?\n\"parent-id of node-id:%d is taxonomy[%d] = %d, but must be within [0; %d-1]\"\n. What about resize_vector()?\n. What does get_attr_vector mean?  If it's what I guess, then you could call it get_label_vector or generate_label_vector.\n. int32_t offset = i*feats_dim;\n. Very good!  Can you please add some reference to HashedMultilabel and Multilabel as well?\n. This should be constants in the individual unit tests.  Don't abuse defines for that.\n. You write that it has to be within [0;n-1], but you only checked upper bound.\n. length of taxonomy vector\n. Use int32_t count = 0, because it's only used inside the loop.  Rule of thumb: Keep variables as local as possible.\n. root_node_count\n. Put simple initializations like this to the top.  (next to m_taxonomy)\n. remove \"if (num_classes)\" and add a \"REQUIRE(num_classes > 0, ...)\" to the top.\n. This check is not necessary I guess.  Just set it to 1 directly, because it was already initialized to 0.  On the other hand, if it was already set to \"1\", then setting it again doesn't hurt.  The only check that would make sense here:\nif (attr_vector[node_id] == 1) {\n   continue;\n}\n. This inner while loop wouldn't be necessary if the nodes are already sorted in pre-order or if we stored iteration order somewhere (because then it would be easy to start with the leaves and to guarantee that parents are always called after their parents ;)).\nNo need to change that.  Just saying.\n. Why don't do it simple this way?\nwhile (parent_id != -1) {\n   attr_vector[parent_id] = 1;\n   parent_id =  m_taxonomy[parent_id];\n}\n. Of even better (because less readable :)):\nfor (int32_t parent_id = m_taxonomy[node_id]; parent_id != -1; parent_id = m_taxonomy[parent_id]) {\n   attr_vector[parent_id] = 1;\n}\n. I think m_num_classes > 0 should hold always, regardless if we're in training-mode or not.  So remove else and keep the REQUIRE.\nForthermore, m_num_classes should be set within train() method and not be touched afterwards.  Isn't there a better way?\n. In my quick research I found:\n- set_labels is not virtual, so we can't use it without modification\n- init already knows about the labels and the taxonomy vector -- why not set m_num_classes there?\n- taxonomy.len == m_num_classes == multi_labels->get_num_classes()\nMy conclusion:\nWe should set m_num_classes in init() and add checks that above equation holds.  But we shouldn't modify m_num_classes in train() -- rather die if it doesn't match.\n. ",
    "van51": "Hello. I am interested in working on this one as an entry patch for the Google summer of code and I would like to know if it is still open and unassigned. \n. Ok, I will give it a go!\n. Ok, I made the changes you said. \n. Ok, after a few hours of messing around I managed to do it! Hope that admitting that, doesn't cost me any points :P Also, I added another change because when I re-tested it, it had some leaks. Don't know why they didn't show up before. Please, when you review the changes, get back to me :)\n. I made the change in SerializableFile as small as possible so that I wouldn't break anything in other classes. \nforth time's the charm! \n. I 've submitted a PR (#1059) for this issue. Please have a look when you 're available\n. Hello, thanks for your reply! \nI will go through your comments and adjust the code (and I thought I was careful)!\nIt is working however and it is memory-clean. I'll incorporate the example I have made in the class's unit test and\npost the output I get from valgrind (unless that can be done in testing as well somehow).\nAlso, I agree that it is a bit confusing to read so I will try to change it.\nI'll start working on it in a while and get back to you\n. The reason I have that first loop and ended up having the code written this way was because I wanted to avoid creating new CombinedKernels on the fly and then copying the already listed kernels from the old kernels to the newly created ones. I think that would also be the case in recursion. However it would probably be more intuitive and understandable as you said. Do you prefer it the second way? If yes, don't hesitate to tell me, it won't be any trouble to change it\n. I 've mades the changes you said. Also I've added a unit test and added a line in another one to fix a memory leak.\nI didn't add anything to the modular typemap, because I am not sure what to edit. Hopefully, that's all that's left and \nif you explain it to me, I can do it tomorrow. Unless you would prefer to have it in a recursion as well, which I am fine with.\nAnyway, I am looking forward to your input!\n. Hello! no worries about the delay, I hope you had a nice trip!\nI have made the changes you said and I will make a new PR.\nPlease review it when you have the time!\n. The w I get from octave is here : http://pastebin.com/hR2PHzez. \n. I made the change from SG_DEBUG to SG_ERROR at the appropriate lines\n. I have added a unit test file for this class. For the dot_products test I am missing the dot_range and dot_range_subset functions.\nI have also tried to test the iterator, but when it reaches the get_feature_iterator() method inside the CSparseFeatures class method, it throws a seg fault.Am I trying to use it wrong or could there be a bug there?\n. Update: Commented out the failing test\n. hello!\nSo, by doing \n\ncd src; ./configure && make\neverything works fine.\nIf instead I do this, \ncd src; ./configure; mkdir build; cd build; cmake ..; make\nI get the following : \nhttp://pastebin.com/KxwhSYLX\n\nAll the above commands are following a git clean.\nLet me know how I can help to figure out what's wrong :) \n. If I do it there, I get : \n\"CMake Error: The source directory \"/home/van51/shogun\" does not appear to contain CMakeLists.txt.\" \n. Indeed I wasn't working on the feature/CMake branch. I thought that some functionality was already integrated in develop. Everything seems to be working fine now. Sorry for the fuss :)\n. @vigsterkr OK, thanks for letting me know!\n. It was merged just a moment after I updated it. I just added 3 more cases to handle some special ctrl characters, like \\t,\\n,\\r, that are more probable to occur in a text. I'm just mentioning it to be covered!\n. I still want to add two more cases in the unit test for the dense_dot and add_dense. \n. I saw that the method get_computed_dot_feature_vector() uses internally add_to_dense so I don't think it needs an additional test case for that. Also, maybe the static method that is introduced now would fit better in the converter or even in CHash.\n. Added a unit test case\n. Added a unit test case\n. Fixed. If nothing else comes up on the review, it's good to merge.\n. Not good to merge yet. Found it has a bug when used with multiple threads (specifically in the apply methods of some classifiers). I'm in the process of trying to find out what it is\n. Changed the way I was creating the combined ngrams, by passing an already created CDynamicArray (to hold the combination indices) as an argument to the function generate_ngrams(), instead of creating a new CDynamicArray  each time in the method. It's not a pretty function call now, but it seems to solve the problem with mulithreading, at least in a minimal example. I plan on running the language detection demo tonight to make sure.\n. Update: created CRKSMachine as base class\n. I did it this way because that's what I had understood from the ideas page and I kind of went along. It shouldn't be hard to convert it and use it in the DotFeatures framework and I will get right on it, but there are still some things that I would like to discuss.\nFirst of all, what's the best way for the user to specify arbitrary functions? Will this function pointer approach work with the modular interfaces? \nSecondly, in the RFDotFeatures case we were \"lucky\", because the function phi (that was used there) needed to calculate the dot product between a feature vector and a parameter vector w and so we could have a simple dense_dot() call. For an arbitrary custom function phi we will probably need to get the feature vector and pass it to the function, which I think will be much slower.\n. The usage of the RandomFourier features through the RandomKitchenSinksDotFeatures class vs the direct implementation of the RF features in the dot() methods in RandomFourierDotFeatures.\n. Update: added unit-test. Waiting for travis\n. Yeah I was going to notify you as soon as travis was done. I was waiting because it had trouble compiling last time and I was trying to figure out why that was. I believe I've fixed it now however\n. Update: Fixed travis' errors. Had to do with KernelName:: not being a valid expression in pre-c++11 as an enum and with an inf that appeared in Json serialization.\nIt's good to be merged by my side.\n. I should switch this to SG_UNREF, right?\n. Should I check for nulls here? I have moved the assertion before the loop, but idk whether there could be null features inside the array..\n. seg fault here\n. I didn't go with what was suggested here. Instead I'm creating a SGSparseVector on the fly from the SGVector. I don't really like the two consecutive loops above, but since dim << feature space, I think the overhead won't be significant.\n. Initially I thought that the number of bits of the hash and the new feature space dimension would be two different parameters.\nHowever, if num_bits = log2(dim), I feel that line 108 above is redundant, since we reduce the dimension in the mod operation.. \n. Should this happen here from the user or inside the CHashedDocDotFeatures class?\n. Also here\n. cool! and I was looking for a way to work on the vector directly! got to work on my c++ skills as well! \n. Here I find the hashed index and later on I count how many times each index occured and that becomes the value of that index.\nBut from what I understand now, for numerical features, this should be switched to something like :\n h_index = MurmurHash( index, size, some_seed);\n h_vec[h_index] += vec[index];\n. It's 0 because in this method I make all the n-combinations (and for every ngrams >= n > 1) that start with the first token.\n. As I said before in this method with consider only combinations that start with the first token, and so the unigram of the first token is added in line 161.\n. indeed the method covers the unigram case.. I just thought it adds too much of an overhead in that case. Maybe it would be better to make different methods for these two cases like you suggested in your previous email.\n. The idea behind the skipping is that you when you combine multiple tokens, you allow it to skip some and consider some others further ahead. \nEg. if you have the tokens [\"a\", \"b\", \"c\", \"d\",] and you specify n_grams = 2 and skips = 2, you will get :\n[\"a\", \"ab\", \"ac\" (skipped 1), \"ad\" (skipped 2), \"b\", \"bc\", \"bd\" (skipped 1), \"c\", \"cd\", \"d\"].\nMaybe this example or something more elaborate should be added in the constructor definition.\n. Is it sufficient here to xor the hashed values or will consecutive xor's somehow diminish the randomness or limit the outcomes?\n. In the above while loop, if I just leave inside the following two lines:\nCDynamicArray* hashed_indices = new CDynamicArray();\nSG_UNREF(hashed_indices);\nI'm getting the following errors, on a multithreaded code : \"call init_shogun() before using the library, dying.\"\nSo, I'm guessing this rapid creation and unref'ing somehow manages to get things unref'ed when they shouldn't be?\n. If I don't include common.h, then gcc on travis fails with int32_t doesn't name a type.\nIf I include it then, on my machine with gcc and c++11 atomic, it shows a problem in linking \n\"[ 71%] Building CXX object src/shogun/CMakeFiles/shogun.dir/base/class_list.cpp.o\nLinking CXX shared library libshogun.so\n[ 71%] Built target shogun\nmake: *** [all] Error 2\"\nand also on clang on travis a similar issue\nWhat am I missing? \n. Before, the m_refcount was returned instead of count, which led me to think that it should return the current ref_count ( since m_refcount could have been increased/decreased again after the unlock() ), instead of the (variable) count in this method.\nThis is the expected behavior, correct?\n. This may need to change to return m_refcount->ref_count(), depending on the answer to my previous comment\n. Should copied SGObjects start their counter from 0, or from the value of the ref_counter the other object had? Depending on the answer, a minor modification may be needed in the copy constructor\n. Actually in SGReferencedData each copied object gets its own refcounter atm, that has the same value the original object's counter had (through the function copy_refcount()). Should that be changed and have them share it and ref/unref the same one? And furthermore, do something similar here?\n. Thanks for pointing that out. I will make sure they are fixed :)\n. A preliminary run with n=10 and num_vecs=10k got me thinking and I don't think that this comparison will be fair, because it is heavily dependant on the number of examples for the RFDotFeatures as well.\nThe RFDotFeatures needs to compute the projected vectors again and again for each dot product, while even the RFPreprocessor only does that once per example and then it's the straightforward dot product.\n. The RandomFourierDotFeatures don't require the feature vector above (line 119) and directly compute the dot product required and the additional operations in here instead of requiring an additional call to function phi (which is virtual). I 'm not sure what gets to be optimized with the compiler but I think these suffice to make it a tad slower.\n. If we just consider the Random Fourier specialization then yes, but the random kitchen sinks in general work with arbitrary phi's and not all phi's would require a dot product.\n. I thought of encapsulating the dot operation so that sub-classes can override this method and perform any pre-processing they need and then to be able to optionally call the base class method if they do require the dot operation.\n. Ok I will change the function.\n. The bug was here. len+dictionary.get_num_elements() is no longer the correct size of dic_weights, since the size of the dictionary itself has changed from the previous line. \n. Added the above method to be able to specify different values for p_num_elemenets and dim1.\n. @sonney2k yea I get that, however you would still want to access only the compact portion of the array and not all of it, so you would have to specify the current number of elements in addition to its entire size.\n. ",
    "elfring": "Would you like to delete any underscores from affected identifiers in your software library?\nHow do you think about to make your include guards not only standard-compliant but also really unique by appending a kind of UUID?\n. ",
    "hushell": "I also gave a try at #991. Hope it is what you want. The first part seems only need to call a function before getting output confidences. Do we need to create an example for this? Or just update an existing example?  \n. Oops! I forgot unit-tests. I'll commit later. \n. Thanks for the comments! Currently I only check the example I created, which is correct. I'll do valgrind and unit-tests ASAP. The compilation on my Mac is really slow, more than 20 mins :/ I'll setup a new shogun environment on a desktop. Then everything should be finished soon. BTW, how can I update the PR? since it's been closed. I updated something on my branch ae874347776b1f77d6c17e620a6e4ac29af6ef14\n. Sorry that I am still new to Shogun development! Some ideas seems not quite smart. \nWell, maybe a better solution can be like this: \n1) create a RescaleStrategy class, which includes all heuristics in Milgram's paper. \n2) Users can choose a rescaling strategy and register to MulticlassStrategy. This can be done via another construction or a setter. The default strategy is none, so this addition will not affect other existing parts that using MulticlassStrategy.\n3) The rescale_score(RescaleStrategyType res) method will be call in MulticlassStrategy::decide_label() at very beginning. \nIn this way, we don't need to change anything in Machine, except the is_prob_support(), which be an abstract function and be overloaded by subclasses. \nIn summary, add a new class CRescaleStrategy, an example, an unit-test case; Other changes will be at MulticlassStrategy, MulticlassOneVsOneStrategy and MulticlassOneVsRestStrategy. I'll finish these ASAP.\nRegarding valgrind, I checked it before commit. My added example and unit test are okay in my run. But the MulticlassOCAS failed in both test and valgrind check. Is it because of mine changes? \n. Hi Heiko,\nIn the end, I feel it's better not to introduce a new class, instead I added the probability estimation heuristics to MulticlassOneVsOneStrategy and MulticlassOneVsRestStrategy, which are routed by a switch-case. I implemented 4 heuristics, except the softmax function of OVA (i.e. Eq.(7)) which requires parameters A and B from score_to_probabilities(). \nAnother big change is in MulticlassMachine::apply_multiclass(), in which I call rescale_outputs() before decide_labels(), in default, it's just a copy, if specific strategy is chosen, the corresponding heuristic will be called. \n. No hurries! I'll add some unit tests once no major changes needed.\n. I tried to register the enum type varible in this way: \nSG_ADD(&m_prob_heuris, \"prob_heuris\", \"Probability estimation heuristics\", MS_NOT_AVAILABLE);\nbut I got errors like this:\nno matching function for call to \u2018shogun::Parameter::add(shogun::EProbHeuristicType*, ...)\nDid I do something wrong?    \n. I tried to register the enum type varible in this way: \nSG_ADD(&m_prob_heuris, \"prob_heuris\", \"Probability estimation heuristics\", MS_NOT_AVAILABLE);\nbut I got errors like this:\nno matching function for call to \u2018shogun::Parameter::add(shogun::EProbHeuristicType*, ...)\nDid I do something wrong?    \n. Hi Heiko,\nThanks very much for reviewing the code! There are still several issues need to be fixed. \n1) use SGVector new_outputs is fine when only modifying the content of new_outputs, but if I just copy like this: new_outputs = outputs; then it changes nothing after return. I think the implementation of \"=\" operation only pass the pointer, so in this case, change the pointer need pointer of pointer or reference of pointer. So I am still using rescale_outputs(const SGVector outputs, SGVector& new_outputs) . Please let me know if you have better way to do.\n2) There is one heuristic is unimplemented, because I think which need to modify score_to_probabilities(), specifically need output the parameters of logistic function. \n. Hi Heiko,\nThanks very much for reviewing the code! There are still several issues need to be fixed. \n1) use SGVector new_outputs is fine when only modifying the content of new_outputs, but if I just copy like this: new_outputs = outputs; then it changes nothing after return. I think the implementation of \"=\" operation only pass the pointer, so in this case, change the pointer need pointer of pointer or reference of pointer. So I am still using rescale_outputs(const SGVector outputs, SGVector& new_outputs) . Please let me know if you have better way to do.\n2) There is one heuristic is unimplemented, because I think which need to modify score_to_probabilities(), specifically need output the parameters of logistic function. \n. Hi Soeren,\nThanks for your comments! I fixed several things as you suggested. \n. Hi Soeren,\nThanks for your comments! I fixed several things as you suggested. \n. Hi Heiko,\nCould you add the part dealing with the A and B in scores_to_probabilities() ? Or just tell me a little bit how to make it. I remember you mentioned using CStatistics, but I still have no idea what is that. \nMy idea is changing the return type of the scores_to_probabilities function, so every time I call the function, I can store the A and B easily. I am not sure if this is a good solution. \n. Hi Heiko,\nCould you add the part dealing with the A and B in scores_to_probabilities() ? Or just tell me a little bit how to make it. I remember you mentioned using CStatistics, but I still have no idea what is that. \nMy idea is changing the return type of the scores_to_probabilities function, so every time I call the function, I can store the A and B easily. I am not sure if this is a good solution. \n. I know why I couldn't understand you. I haven't fetch upstream for a long time. Great changes! Now the fitting becomes independent of the svm machine. I'll update the code. If possible, let's merge the code tomorrow. This PR seems being alive too long.\n. I know why I couldn't understand you. I haven't fetch upstream for a long time. Great changes! Now the fitting becomes independent of the svm machine. I'll update the code. If possible, let's merge the code tomorrow. This PR seems being alive too long.\n. Hi,\nI made a singe commit, but I didn't roll back too much, since during the time, I merged from upstream/develop. In the newest commit you will see what I have changed. \nIt's a little strange to me, the travis build failed in python modular, c# modular and lua modular. How do I fix these problems? \n. BTW, when running valgrind, it detects memory leaks in base_load_file_parameters and base_load_all_file_parameters, but seems nothing to do with my part. \n. The first patch is more tough than my imagination. But I am patient person. I just afraid you run out of patience with me. :)\nI'll add unit tests right now. So you feel better using func(SGVector param) than explicit parameters return func(SGVector& param)? Since there is no resizing in the latest change, both forms are fine.\n. I added several test cases to compare these rescaling functions with manual calculations. Let me know if you want me to add more test cases for other functions in the multiclass strategy classes.  \nFor the performance, all heuristics get similar error rate: about 12-13 misclassifications out of 92 examples. The output labels matched the cases where none heuristics applied, there are very few 1-2 exceptions. But I don't think these points can be used for unit tests, because it's very hard to say how good the matching is.\n. Thanks Soeren! I am enjoying the process, from which I learn something basic that a Shogun developer should know. \n. I am very happy about the merge! How did you deal with the travis failure stuffs? I was worry about these, even after commenting out the parameter registration, seems something crashed. \nAnyway, there is no greater motivation like this, even this is an easy task. I'll move on to something serious now. :) \nBTW, I'll add an example to python modular asap. \nCheers! \n. Are you seriously adding this to NEWS? My PR is totally depends on your fitting_sigmoid :)\n. I turned on the tab expansion but forgot to change back :(\n. I see. It's better to use CStatistics::SigmoidParamters rather than two vectors, but usually users will not call rescale_outputs() directly, they only need to specify the type of heuristic. \nI'll modify these parameters this weekend. \n. I forgot to synchronize the python examples, so the Travis failed :(  I am doing it.\n. I forgot to synchronize the python examples, so the Travis failed :(  I am doing it.\n. Fernando, yeah, when I finish coding, I found there are new changes in upstream ... I should check it before coding :)\n. Fernando, yeah, when I finish coding, I found there are new changes in upstream ... I should check it before coding :)\n. Patrick, you think SSVM should only use Hinge loss as surrogate loss? In my opinion, structured model defines the input and structured output as well as prediction etc. But for training, we can have many different objective by using different surrogate loss. I agree then this would not be a SSVM anymore literally. \n. Patrick, you think SSVM should only use Hinge loss as surrogate loss? In my opinion, structured model defines the input and structured output as well as prediction etc. But for training, we can have many different objective by using different surrogate loss. I agree then this would not be a SSVM anymore literally. \n. @iglesias Thanks for the tips! Happy to learn it!\n. @iglesias Thanks for the tips! Happy to learn it!\n. @ppletscher I agree! I'll fix this in next update together with other points. Maybe we should wait Nico's reply. \n. @ppletscher I agree! I'll fix this in next update together with other points. Maybe we should wait Nico's reply. \n. @ppletscher : I couldn't come up with an idea moving out the init_opt() and auxiliary constraints etc. Because I found them are quite application related. Maybe we could do something better, but this has to touch too many classes. \n. Regarding the surrogate loss that I kept in PrimialMosekSOSVM last patch, Nico mentioned the quadratic loss is also applicable. I think he means the SVM2 in http://jmlr.org/papers/volume6/tsochantaridis05a/tsochantaridis05a.pdf\n@ppletscher Shall we keep this loss? Maybe we can set it a default argument to HingeLoss\n. @vigsterkr I think what risk() do is computing the subgradient and the total loss, which is R(w) = \\sum_i surrogate_loss(x_i,y_i;w) = \\sum_i [w^T \\psi(x_i,y) + \\delta(y,y_i)] - w^T \\psi(x_i,y_i), isn't it? So I don't think one uses BMRM need to re-implement this, all the things we need to care about are the surrogate loss and the delta loss. This is why I think risk() is model independent. @ppletscher reach again the surrogate loss, in the BMRM paper, I saw different loss functions have been tested. http://users.cecs.anu.edu.au/~chteo/pub/TeoLeSmoVis07.pdf\n. @vigsterkr You are right, what currently risk() is the n-slack formulation. But I still don't think risk() is model related. In my opinion, it is about different formulations of the training objective. For model, we should only care about joint_feature_map(), argmax() i.e. prediction method and application related loss(). \nSo how about this? In the StructuredOutputMachine, we have different training formulations:\n- 1-slack with margin rescaling and slack rescaling\n- n-slack with margin rescaling and slack rescaling\n  Totally 4 implementations of risk(), and when we doing training, we have chose one. \n. @vigsterkr Yes. I think enumeration is a good method, and user has to specify it when creating the StructuredOutputMachine. I didn't touch the labels, keep this in StructuredModel is fine. BTW, do you think pass CDualLibQPSOSVM* in svm_bmrm_solver() is bad?\n. @vigsterkr Shall we discuss on IRC? I am coding the multiple risk(), but I am not sure where to put them. And there are many details need to be specified.\n. Sorry for the delay! I got a bit busy in daytime. I changed this PR according to our discussions. Please let me know anything disagreement. \n. @iglesias The csharp problem solved :D but python still fails as before. I updated the csharp part of modshogun_ignores.i\n. @iglesias Yeah, I should unref this. Thanks!\n. @ppletscher I let FactorGraph as a derived class from CFeatures and will be treated as Structured inputs, while the FactorGraphObservation will be used as structured outputs.\n. @ppletscher I let FactorGraph as a derived class from CFeatures and will be treated as Structured inputs, while the FactorGraphObservation will be used as structured outputs.\n. @ppletscher I rewrite some parts but the basic design is still similar to Grante, I don't know if this will be problematic, but I found many libraries follow the same idea, such as Darwin's PGM module (http://drwn.anu.edu.au/group__drwnPGM.html). Now I don't have many cues what should be in the factor graph classes, maybe I should move to inference and see what functions are needed. \n. @ppletscher I rewrite some parts but the basic design is still similar to Grante, I don't know if this will be problematic, but I found many libraries follow the same idea, such as Darwin's PGM module (http://drwn.anu.edu.au/group__drwnPGM.html). Now I don't have many cues what should be in the factor graph classes, maybe I should move to inference and see what functions are needed. \n. @iglesias Thanks! I'll fix the minors in few hours. There are many things failed with Travis, I'd like to check these as well. \nPatrick would like to let me implement the max product from scratch, because this is easier for us to debug etc. So I have to work at that for several days before the new PR. But I have to make sure this being done by Friday. Otherwise the learning part cannot be worked out before mid-term. \n. @iglesias No idea :( It also runs fine in my laptop. But I modified that test case and committed. \n. @iglesias I'll fix ASAP.\n. Sure. I'll take a look later today.\nOn Tue, Feb 4, 2014 at 4:45 AM, Viktor Gal notifications@github.com wrote:\n\ncan we close this one?\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/shogun-toolbox/shogun/issues/1298#issuecomment-34055683\n.\n\n\n/\n- ============HuShell============\n  /\n. @iglesias I tried valgrind --leak-check=full ./shogun-unit-test --gtest_filter=FactorGraph.evaluate_energy_param_data_\nThere is no problem on my laptop. Shall we close this? \n. I have run valgrind, nothing wrong on my laptop. This is the output I got:\nhushell@hushell-U510:~/working/git-dir/shogun-issues/tests/unit$ valgrind --leak-check=full ./shogun-unit-test --gtest_filter=FactorGraph.\n==30457== Memcheck, a memory error detector\n==30457== Copyright (C) 2002-2012, and GNU GPL'd, by Julian Seward et al.\n==30457== Using Valgrind-3.8.1 and LibVEX; rerun with -h for copyright info\n==30457== Command: ./shogun-unit-test --gtest_filter=FactorGraph.\n==30457== \nNote: Google Test filter = FactorGraph.*\n[==========] Running 5 tests from 1 test case.\n[----------] Global test environment set-up.\n[----------] 5 tests from FactorGraph\n[ RUN      ] FactorGraph.compute_energies_data_indep\n[       OK ] FactorGraph.compute_energies_data_indep (71 ms)\n[ RUN      ] FactorGraph.evaluate_energy_data_indep\n[       OK ] FactorGraph.evaluate_energy_data_indep (22 ms)\n[ RUN      ] FactorGraph.evaluate_energy_data_dep\n[       OK ] FactorGraph.evaluate_energy_data_dep (20 ms)\n[ RUN      ] FactorGraph.evaluate_energy_param_data\n[       OK ] FactorGraph.evaluate_energy_param_data (21 ms)\n[ RUN      ] FactorGraph.evaluate_energy_param_data_sparse\n[       OK ] FactorGraph.evaluate_energy_param_data_sparse (22 ms)\n[----------] 5 tests from FactorGraph (170 ms total)\n[----------] Global test environment tear-down\n[==========] 5 tests from 1 test case ran. (224 ms total)\n[  PASSED  ] 5 tests.\nYOU HAVE 3 DISABLED TESTS\n==30457== \n==30457== HEAP SUMMARY:\n==30457==     in use at exit: 0 bytes in 0 blocks\n==30457==   total heap usage: 18,119 allocs, 18,119 frees, 941,465 bytes allocated\n==30457== \n==30457== All heap blocks were freed -- no leaks are possible\n==30457== \n==30457== For counts of detected and suppressed errors, rerun with: -v\n==30457== ERROR SUMMARY: 0 errors from 0 contexts (suppressed: 0 from 0)\n. I agree. Sorry that I was not familiar with frequent commit. Let's split the PR into 3 parts: 1) First factor graph changes with unit tests, then 2) MAPInference class 3) MaxProduct implementation. What do you think?\n. @iglesias I have to go out for few hours. I'll be back and update the PR. \n. @iglesias For the recursion in find_set(), I have no idea how to test it will not run forever. But I tested few cases, loopy graph, tree graph and a forest graph. It works fine. \n. @iglesias there will be a class FactorGraphModel, then I will implement argmax etc. based on the MAPInference, but of course we need change a bit for loss augmented inference. The FactorGraph is like your CSequence, and since the CFactorGraphObservation is derived from StructuredData, we can fill CResults easily. I'll hopefully finish the model part this week. I am very exciting about it :)\n. @iglesias update! :)\n. Hi guys, I'll go camping for 3 days. I hope it's not too late to finish this PR when I come back :D \n. @ppletscher , @iglesias Sorry for late! I was busy last week. I updated the PR according to your comments.\n@vigsterkr the cmake build system is great! To make sure the c++11 option, I only need use #ifdef HAVE_CXX0X in the code, right?\n. @vigsterkr Thanks! I updated the macro conditions. \n. @vigsterkr Not a problem! :) I didn't notice it as well. \n. @iglesias Could you check the code again and see if it's good to merge?\n. @ppletscher @iglesias Thanks for the comments! I think most of problems are solved. To change the m_fw_msgs and m_bw_msgs from std::vector to SGVector or Eigen::VectorXf need touch many lines, I suggest to keep it now and do it later for better understandable code. \n. @karlnapf @iglesias Thanks for the review! Happy to know the initialization and parameters stuffs! I'll keep these in mind :) \n. @iglesias Sure, I definitely need to do a double-check! :)\n. @iglesias Updated :)\n. @iglesias Thanks! I have to work on a workshop paper today :(  I'll keep on the factorgraph model PR on Friday.\n. Sounds great! So I send 2 PRs at the same time?\n. updated! :)\n. Sure. I'll draw a new diagram. Now I understand more, and many things have been updated. Let me briefly explain what's going on with FactorGraphFeatures and FactorGraphLabels and FactorGraph. \nFirst of all, a sample of FactorGraph is a structured feature, which should be stored in the FactorGraphFeatures (has an array m_samples). By concatenating data in FactorGraph, we get psi, the joint feature map. Here we assume that the FactorGraphModel has a linear parameterization. \nThen, the FactorGraphLabels store an array of structured outputs, i.e. m_labels. We defined a class FactorGraphObservation for representing the structured output. In our case, a structured output is vector of states (each node in factor graph has a discrete state). So in fact, no structure encoded in the FactorGraphObservation, all structured information are recorded in FactorGraph.\n. Sure. I'll draw a new diagram. Now I understand more, and many things have been updated. Let me briefly explain what's going on with FactorGraphFeatures and FactorGraphLabels and FactorGraph. \nFirst of all, a sample of FactorGraph is a structured feature, which should be stored in the FactorGraphFeatures (has an array m_samples). By concatenating data in FactorGraph, we get psi, the joint feature map. Here we assume that the FactorGraphModel has a linear parameterization. \nThen, the FactorGraphLabels store an array of structured outputs, i.e. m_labels. We defined a class FactorGraphObservation for representing the structured output. In our case, a structured output is vector of states (each node in factor graph has a discrete state). So in fact, no structure encoded in the FactorGraphObservation, all structured information are recorded in FactorGraph.\n. Updated! However Travis was unhappy about something. \n. Updated! However Travis was unhappy about something. \n. Yeah, be compatible with mosek 7.0 is a problem. It seems 7.0 changed a lot. I tried it, not working with current code. \n. @vigsterkr Sure. I'll give a try. \n. @vigsterkr Sure. I'll give a try. \n. updated! \n. updated! \n. @iglesias Thanks for merging! \n. @iglesias Thanks for the review! I'll also update the ipynb as HeikoS's suggestions and push it here then. \n. @iglesias All right! Updated!\n. I am working the relevant parts in ipython notebook. \n. I'll add unit-tests tomorrow. I just want to get one more thing done by the deadline, but it actually passed ;/ . \n. @sonney2k Sure. I'll be around. I am happy to involve in :) And there are a lot of things for the structured prediction have to be done. I hope we can have a relatively complete framework for 3.0.\n. Updated! Sorry for late! \n. updated!\n. @iglesias No idea why segmentation fault in SerializationHDF5.StochasticSOSVM. A CSGObject* cannot be NULL? Is it possible to gdb this test?\n. @iglesias Thanks for pointing out the dividing zero! :) I'll update tonight when I go back to home. \n. updated!\n. So I remove the libbmrm changes now, let's wait for Michal's response. \n. @vigsterkr I have tried normalize the risk in CStructuredOutputMachine::risk_nslack_margin_rescale(), but doesn't work, I think Michal said there is a particular reason not doing in the risk(). I forwarded the email to you. Do you have a better idea? \n. @vigsterkr you mean R=machine->risk(subgrad, W) / number ? e.g. in libbmrm.cpp? \n. @iglesias no worry, seems @vigsterkr has some experience on this point. And I don't think this is a tough problem. \n. @vigsterkr as I talked in IRC, I have tried do the normalization in our canonical risk function CStructuredOutputMachine::risk_nslack_margin_rescale(), but doesn't work -> BMRM stucks at the 2nd iteration. \nThe input risk has to be unnormalized for some reason, but I haven't check this in detail. @uricamic explained a bit in his email. \nAnyway, I just modified the primal objective computation, even risk in BmrmStatistics are unchanged, this should be application independent, unless you want to get unnormalized primal objectives. Furthermore, in our framework, user is not supposed to write the risk function. Only joint_feature_map(), argmax() and delta_loss are required. I don't see any problem for applications. \n. @uricamic @vigsterkr Sorry I didn't see such kinds of needs. Then it's not a good idea to force the risk to be normalized, if you really want to have unnormalized risk within optimization. The risk() has been moved to StructuredOutputMachine, since I was thinking this is model independent. Now seems we have to move it back to StructuredModel, isn't it? \n@uricamic @vigsterkr BTW, could you check why the normalized risk (i.e. return normalized risk from risk() function) is not working well with BMRM? I am not very familiar with these code, but I'll give a try as well. \n. Hi guys, \nWell, if you think the normalization stuff is not necessary, then we can close this PR. I still haven't figure out why BMRM get stuck when using normalized risk. Now seems the comparison between solvers is not possible, I have to delete this part in my ipython notebook :( \n. @iglesias No worry I just gonna delete that figure, I'll keep BMRM there. What you said make sense! Although objectives are the most informative values in comparison, but other values are important as well. Anyway, let's close this PR, I'll send another PR for my notebook after the morning meeting. \n. @sonney2k That's a good idea, you just reminder me this is why the SOSVMHelper class exists. We will in the end unify all solvers and do statistics in the same way. I was trying to avoid to include additional computations in the BMRM classes, since BMRM has its own statistics. But seems that plugin in some helper computations to BMRM classes will not touch the original code. Okay, let me get this PR in that way. \n. As @sonney2k 's suggestion, I add a SOSVMHelper pointer member to StructuredOutputMachine, such that all solvers will use the same way to compute statistics such as primal objectives etc. \n. updated! fixed a potential memory leak in libp3bm.cpp\n. Outputs are here: http://nbviewer.ipython.org/6865729\n. @iglesias Thanks! Nice to have a template :) \n. The computation of objectives and training errors will make the whole\nprogress 2x slower, but I think 1000 sec is still costly. What's the time\nlimit in the buildbot?\nOn Mon, Oct 7, 2013 at 2:58 PM, Fernando Iglesias\nnotifications@github.comwrote:\n\nAlso, I see in the notebook that there are a couple of things that are\ntaking a bit long (3000 or 444 seconds). I think this can be a problem for\nthe buildbot to generate the notebook, there was a similar problem before\nwith a part of the former LMNN notebook.\n@sonney2k https://github.com/sonney2k, @vigsterkrhttps://github.com/vigsterkr,\nhave you seen the structured output notebook failing in the buildbot due to\ntiming?\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/shogun-toolbox/shogun/pull/1710#issuecomment-25848975\n.\n\n\n/*\n- ============HuShell============\n  /\n. Now I am using about 600 training examples. I'll try to use less amount and see if we could demonstrate similar performance. \n@iglesias So you think the verbose outputs are not good? Yes, we could replace all SP_PRINT to SP_DEBUG. I'll send a PR for these. \n. Now I am using about 600 training examples. I'll try to use less amount and see if we could demonstrate similar performance. \n@iglesias So you think the verbose outputs are not good? Yes, we could replace all SP_PRINT to SP_DEBUG. I'll send a PR for these. \n. Okay, I'll test the amount of examples it needs. Hopefully limit the running time below 20 mins. \nThe verbose flag actually control both printing and logging (SOSVMHelper). Do you want to delete the SG_PRINTs?\n. Okay, I'll test the amount of examples it needs. Hopefully limit the running time below 20 mins. \nThe verbose flag actually control both printing and logging (SOSVMHelper). Do you want to delete the SG_PRINTs?\n. @sonney2k sure, let's see if any other problem coming out. I tested LabelsFactory::to_structured() in C++, which works pretty fine. \n. @sonney2k sure, let's see if any other problem coming out. I tested LabelsFactory::to_structured() in C++, which works pretty fine. \n. @aroma123 and @karlnapf : I was thinking just save/load the CFactorGraph, but it will be nice if we are compatible with UAI format. \n@aroma123 : try to reuse Sontag's MPLP code: http://cs.nyu.edu/~dsontag/code/README_v2.html if you decided to go with UAI format. \n. @aroma123 BTW, if you have time, you may also want to work on this: https://github.com/shogun-toolbox/shogun/issues/1916, I think @tklein23 will be happy to be involved :)\n. @aroma123 and @abinashpanda: Please read my updated description! \n. @vigsterkr Thanks for pointing this out. I was not familiar with IO part at that time. Now I find the correct way, an example is void SGVector::save(CFile* saver). I'll update the description right away :)\n. @iglesias Finally Throlf and I decided to let Abinash focus on multilabel classification. So this final step to make use of CUAIFile hasn't been done yet. Maybe @Jiaolong want to continue on this :) \nAnyway, If he doesn't have time, I'll do it in June. Now I am overwhelmed by homeworks and projects. \n. @Jiaolong test with scene or yeast dataset in [1] is enough. BTW, to create an example for python_modular is ok, maybe too small for a ipython notebook. \n. @Jiaolong : make sure only one commit for a PR. We'll check the code when you sending a new one. For data, I suggest you to convert txt files to csv files. \n. @Jiaolong Great job! Just one concern. How about the performance in real dataset? Comparing to the paper and the example in pystruct? You may send another PR to shogun-data and remove the random data testing in this example, if the performance of real data is not too bad. \n. @Jiaolong Great job! Just one concern. How about the performance in real dataset? Comparing to the paper and the example in pystruct? You may send another PR to shogun-data and remove the random data testing in this example, if the performance of real data is not too bad. \n. The code looks good! However, the performance is unsatisfying. I have tested using your code with BMRM and SGD. The training errors are pretty high in both cases. I guess this is because the random data is not a good one. Please test one of the added real dataset, since they have been merged. \n. Please git reset --soft to make a single commit for this PR. \n. @abinashpanda Good job! There are several issues, you may try to resolve. Make sure that function tables in UAI format are actually energy tables in our case. Please add unit-tests for the new class. \n. @abinashpanda Good job! There are several issues, you may try to resolve. Make sure that function tables in UAI format are actually energy tables in our case. Please add unit-tests for the new class. \n. Also, please git reset --soft to make only one commit for this PR. \n. parse() is great! But I think you missed how to construct a FactorGraph from the UAI file. Please think about how to do that. Something like SGVector::load(CFile*).\n. @abinashpanda Please don't launch another PR, since we have already discussed a lot on this one. Try to git reset --soft. This way people knows better what's going on here. \n. @abinashpanda I have updated the entrance task https://github.com/shogun-toolbox/shogun/issues/1913#issuecomment-37388247. Now I think things are very clear. I also removed the part for loading/saving factor types to ease your life :) \n. @abinashpanda Please add unit-tests if possible, otherwise it's hard to test your code. \n. Note that clone, serialization etc failed in Travis. These probably because you are not dealing with members registration correctly. \n. @tklein23 It looks great for me! Please merge for Abinash. \n. @Jiaolong Generally good! Maybe you don't want to do experiments with full connected graph here. But let's make this demo show both results for random data and real data (one dataset is enough). Fix minor issues, then we could merge for you! \n. It looks great to me! Note that there is an error using gcc:\nGraphCut.cpp:146:42: error: \u2018ETerminalType\u2019 is not a class or namespace\nTODO:\n- In maxflowlib3.0::maxflow.cpp::610, there is a function called test_consitency(), you may add this one for debugging. \n- Add a SO learning test case for simple grid graph.\n- Use GC and grid graph for multilabel classification (update your previous example).\n. @tklein23 @iglesias Could you merge this PR? I guess this will fix the bug we had yesterday. \nhttp://scan.coverity.com/projects/1763?tab=overview\n. @iglesias Thanks! Let's see if there are still bugs. \n. Thanks @tklein23! Actually this is part of work that I didn't finish last year. You may look at this for more detail: https://github.com/ppletscher/BCFWstruct. My code is inspired by this one. \nI have a brief plan to port several excellent existing SOSVM solvers to Shogun:\n- Frank-Wolfe batch algorithm (FW)\n- Block-coordinate Frank-Wolfe (BCFW)\n- Dual coordinate descent (DCA)\n- Online exponentiated gradient descent (EG)  \nSo in the end of this summer, very likely, we'll have several batch solvers and online solvers: \n- batch: primal QP, dual QP (BMRM) and FW\n- online: SGD, BCFW, DCA, EG\nSince @jiaolong is doing his part for approximate inference, we'll finally have a ipython notebook to compare all combinations of learning and inference under structured output framework. I think many people would like to see such an experiment. \nFor the SO framework, I think we'll have to refactor a bit the code, since currently we support only n-slack margin rescaling formulation. Besides, as more solvers introduced, we may have to reorganize our code. I don't like the current naming system: DualLibQPSOSVM, PrimalMosekSOSVM, StochasticSOSVM... A better way could be something like an interface, such that users can choose the solver based on theirs needs. What do you think? Would you like to discuss about this at some point? Well, I should open an issue to collect more thoughts. \n. @tklein23 Thanks! I'll open an issue for discussing about the refactoring stuffs. \n. @Jiaolong Would you like to test this solver in your notebooks? :) \n. I didn't check the large dataset. For the toy example in this PR, FW is\nsimilar to SGD, but slightly worse than BMRM, where the best is\nPrimalMosek.\nOn Mon, Aug 18, 2014 at 1:48 PM, Jiaolong notifications@github.com wrote:\n\nSorry, I am still working on other unmerged PRs and scene segmentation\nnotebook. I will send you the results once the comparison is done.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/shogun-toolbox/shogun/pull/2354#issuecomment-52552733\n.\n\n\n/*\n- ============HuShell============\n  /\n. @iglesias Thanks! I'll add unit-tests shortly for CMosek::init_sosvm() by creating dummy SOSVMs and check whether the learned w satisfy the constraints or not. \n. @iglesias unittests added! :)\n. @iglesias Updated! And luckily, no new commits merged before me :) \n. @iglesias ok! Now I added setters in PrimalMosekSOSVM. I think this way better, since now these bounds are only supported for primal QP formulation of SOSVM. \n. @iglesias Travis is happy now! \nBTW, is examples/undocumented/libshogun/so_multiclass.cpp disabled? It seems StructuredAccuracy has changed, this example now crashes because of this.\n. Great! Thanks @iglesias! Let's merge it! :) @Jiaolong will use this patch for his work.\n. Great! Thanks @iglesias! Let's merge it! :) @Jiaolong will use this patch for his work.\n. @iglesias Thanks man! No, I don't have merging permit. Could you add me as a contributor? I'll be careful :) \n. @Jiaolong IIRC, this was an old issue. But we decided do not support Mosek due to the license, so we didn't solve it. \n. Have you updated your old unmerged PR? We may close that one and include things to here. \n. Hey @jiaolong Are you still working on this PR? \n. @Jiaolong Hey man! How is going? I just checked Travis for your old commit. GraphCut failed for all unit-Serialization tests. It may be because of wrong in member registration or bad memory management of GraphCut. Could you check this class again? \n. Hey @tklein23 , I have checked this PR. Could you merge this PR if you feel no problems with it? Thanks! :) \n. @Jiaolong, Could you proposal a PR for adding python example for GraphCut? Since you added swig interface for it. \n. Thanks!\nOn Wed, Jul 30, 2014 at 10:58 AM, tklein23 notifications@github.com wrote:\n\nMerged #2385 https://github.com/shogun-toolbox/shogun/pull/2385.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/shogun-toolbox/shogun/pull/2385#event-147340681.\n\n\n/*\n- ============HuShell============\n  /\n. For some reasons, there are compile errors in python module. See line 2000 in the failed case. \n. Hey @tklein23 @iglesias, this PR looks good to me, could you merge it? \n. Thanks Fernando! \n. Hey @Jiaolong, sorry for late! It looks basically you followed the mplp package. That's great! We could integrate the code better a bit after. I'll check your code more carefully during the week. Just focus on your CV demos next week. \n. Hey Fernando,\nI think Jiaolong ported some code from\nhttp://cs.nyu.edu/~dsontag/code/mplp_ver1.tgz\nThis is the implementation from the original authors. The paper about this\ncode is: http://people.csail.mit.edu/tommi/papers/GloJaa_nips07.pdf\nWe have about 20 days left, right? I think it's ok to keep most of the code\nas the same as the original implementation. Let's refine the code a bit\nafter.\nBest,\nShell\nOn Mon, Aug 4, 2014 at 12:08 AM, Fernando Iglesias <notifications@github.com\n\nwrote:\nHey guys. Are you implementing from scratch MPLP inference or are you\nusing some other implementation as reference? Sorry if this is already\ncited in the code and I just missed it.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/shogun-toolbox/shogun/pull/2439#issuecomment-51024469\n.\n\n\n/*\n- ============HuShell============\n  /\n. Yeah, GPL is good right?\nOn Mon, Aug 4, 2014 at 1:12 AM, Fernando Iglesias notifications@github.com\nwrote:\n\nAs long as there is no problem with the license, I guess it is all right.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/shogun-toolbox/shogun/pull/2439#issuecomment-51028689\n.\n\n\n/*\n- ============HuShell============\n  /\n. I don't know much details about the license. But MPLP is declared to be\ndistributed/modified/copied free under GPL, since Shogun is under GPL 3, I\nthought it's fine. Is there a potential problem? Otherwise, Jiaolong, you\nhave to rewrite majorly.\nOn Mon, Aug 4, 2014 at 1:50 AM, Fernando Iglesias notifications@github.com\nwrote:\n\nCopying GPL code and making it BSD is not good, I think.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/shogun-toolbox/shogun/pull/2439#issuecomment-51031814\n.\n\n\n/*\n- ============HuShell============\n  /\n. Hey @Jiaolong class_list.cpp is generated dynamically.\nOn Sun, Aug 10, 2014 at 10:31 AM, Jiaolong notifications@github.com wrote:\n\n@tklein23 https://github.com/tklein23 Hi, I can't find the reason why\nit can not pass. I have done all the test on my PC for several times. It\nseems something wrong with class_list.cpp, which might need clean the\nbuild directory. Should I do a new PR?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/shogun-toolbox/shogun/pull/2453#issuecomment-51721174\n.\n\n\n/*\n- ============HuShell============\n  /\n. @Jiaolong I guess you have to update and repush. However, the clang fails at python module compilation again. This seems a warning that something is unstable. \n. @Jiaolong That's not the actual error, check line 1908, python module compiling error. \n. I think we need to hold back this PR for few days. Let's finish your earlier PRs first, especially the MPLP one. \n. @Jiaolong SGD: Average testing error is 0.4988. Is it correct? too high comparing with training error? \n. @iglesias This notebook looks pretty nice. Do you think it's ready to go? \n. @Jiaolong I remember you mentioned a running out of time issue. How did you handle it? \n. @Jiaolong So the code is based on MulDimArr? But it seems not functions are included in this PR. Are you sure MPLP also works with SGNDArray?\n. @iglesias I'll pay attention on this PR. Thanks for the comments! \n. @iglesias @Jiaolong It looks better! More importantly, Travis turns green now. After few changes, we'll be able to merge it. Cheers! \n. @Jiaolong Nice to have unit-tests! \n. @iglesias @Jiaolong Seems few issues unsolved, but it looks good to me. \n. Great! But again python module failed. Is it a problem now in Travis?\n. We do need to understand every detail in their code before merging to Shogun. I'll read their code carefully and come back to give comments. It may take a bit more time. \n. Sorry guys! Just see this post. I was distracted by other stuffs: moving,\ngraduation, projects, paper submission... Anyway, sorry for leaving this PR\nfor so long. I'll start work on it with Jiaolong this week.\nOn Mon, Oct 27, 2014 at 12:02 AM, Heiko Strathmann <notifications@github.com\n\nwrote:\n@hushell https://github.com/hushell @Jiaolong\nhttps://github.com/Jiaolong @iglesias https://github.com/iglesias\nwhats the ETA on this?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/shogun-toolbox/shogun/pull/2498#issuecomment-60556313\n.\n\n\n/*\n- ============HuShell============\n  /\n. @iglesias Sorry for late reply! I was busy for a final demo for another project, so this PR was suspended. Sorry that I missed this message as well. I am available again. I think I'll make the review done this weekend. \n. @Jiaolong: I just went over the MPLP code carefully. I found it's not as clean as what I expected, and its factor implementation (aka Region in their case) is quite different from ours. So I don't think it's easy to reuse our factor implementation and keep their core code only. My suggestions to finish this PR soon are listed as below.\n- Let's not just treat MPLP (max-product LP) as LP relaxation MAP algorithm. In fact, there are many methods can be called LP relaxation, the categorization was a bit wrong. So we can add a new macro, let's say MAX_PRODUCT_LP_RELAXATION. We'll do other types later. \n- We'll keep their inner data structure, i.e. Region. We don't change its name to EdgeFactor. It's helpful to try MPLP_ver1, which has simpler input/output. I can share my understandings for Region here: \n  m_region_inds, lists all nodes participated, e.g., (3,4) is an edge and 3 is a node index, they are both regions (factors/potentials may be more well known). \n  m_intersect_inds, lists all regions intersect with this region, stores region indices, e.g., region (3,4)'s index is 1, and region 3's index is 183, region 4's index is 184, then [1, 183, 184] will be stored. \n- UpdateMsgs implements basically the equation in Sontag's thesis (page 104). Note that messages are interpreted as dual variables in MPLP. But the update is different from their ealier conference papers. Here only messages passed from regions to variables, and Region can be viewed as a virtual concept comparing to the old update. \n- Let's replace their multidim_arr to SGNDArray. I think you have done this already. \n- You could keep all their STD stuffs, but should be hided well. Reference to BeliefPropagation.h\n- Let's delete their commented code when pulling into Shogun. But make sure copying their first few lines in their README. \nI'll add other thoughts if they came up. If you have time, let's restart this PR, and try to finish it in 2 weeks. Our goal this week is just to import their code and make minimum needed changes.\n. My experience is that not all inference algorithms can be used for sosvm, e.g., loopy belief propagation. So at very beginning of this project, I chose 3 inference algorithms to implement: Tree BP for tree structure, GraphCuts for binary submodular general graph and MPLP for general graph. These are proved to be nice subroutines for sosvm. Another important reason that we want to re-implement these 3, because all other inference libs have their own graph/factorGraph data structure. We didn't want to tie with any of them. So we implemented our own factorGraph and factorGraphModel for sosvm. Yes, it will be good to implement a converter from other libs to our factorGraph. \n. @Jiaolong I used darwin 2 years ago. I don't suggest to follow or import darwin's code. You'll find darwin's implementation is not as intuitive as it appears. If we want to support a more general inference lib. I'll suggest to go with OpenGM. \nMaybe we should think about how to deal with MPLP, as well as future support for other infernece algorithms. For MPLP, we can do either\n1. to wrap MPLP as a 3rd-party, and convert factorGraph to MPLP's Region, which is an idea similar to OpenGM (they have a wrapper for MPLP). Such a converter can be general enough for other libs. \n2. to extract the core inference code of MPLP, and reimplement using our factor representation. It's not very hard if you understand MPLP, which is just one iterative update step. You'll find the core part of MPLP is in UpdateMsgs function (less than 50 lines). I prefer to do this if you have time. \n@Jiaolong @iglesias What's your opinions? \n. @iglesias Yes, this is what we should do to support many other MAP inference algorithms. However, recall that this transform has to be done for each sample in each iteration of sosvm, since factors change as parameters update. For computational efficiency concern, I suggest to reimplement MPLP at least, such that for a general loopy graph, we don't depend on 3rd-party lib. As I said, we have implemented inference algorithms for tree case, and binary loopy graph case. \n. @Jiaolong So the purpose of this is refactoring the unit-test code? It looks good at first scan. I'll check it more carefully later today. \n. @Jiaolong So the purpose of this is refactoring the unit-test code? It looks good at first scan. I'll check it more carefully later today. \n. @Jiaolong Sorry man! Deadlines and midterms ruined my plan. Didn't expect delaying your progress. I think your changes are good. I just need to double check it. I'll do it tomorrow night. \n. @Jiaolong Sorry man! Deadlines and midterms ruined my plan. Didn't expect delaying your progress. I think your changes are good. I just need to double check it. I'll do it tomorrow night. \n. @Jiaolong I have gone through the code. It seems you just refactor the unit-test code, since many lines can be reused, am I right? If so, I am fine with your changes. Please double check valgrind to make sure no memory issues. Let's ask Fernando to get it merged. \n. @Jiaolong Good job man! Looking forward to see your new MPLP code. \n. @Jiaolong Good good! This one is much better than what I was thinking about. \nIt's nice to get rid of Region class in the original implementation (OI). I think you have fully understand the MPLP algorithm. I can tell from update_messages(), which is almost different from the OI. You are amazing, man! \nThe only thing that we should be more careful is the naming rule of variables, functions etc. I'll expand that in the code. \n. @Jiaolong Good job! I'll check your code out  and test it on my machine, then I probably give more comments later. \n. @Jiaolong Good to see these comparisons! As we expected, MPLP does a better job than GraphCuts in general graphs and similar to TreeBP in trees. Didn't expect MPLP is so much time consuming, but 460s = <8mins is acceptable? Since GraphCuts was not too bad, maybe better to set it as default. \n. \"the results from the MPLP V1.0 library\" is keeping their code as before? \nYou may want to timing each part of the code to see why the current CGEMPLP is slower. Also check if current CGEMPLP uses more iterations? \n. @Jiaolong @iglesias It's a great news this patch is merged! Sorry I couldn't help for the merge. I spent last fews day on traveling. Thanks for your hardworking! Let's plan our next steps :)\n. @Jiaolong @iglesias It's a great news this patch is merged! Sorry I couldn't help for the merge. I spent last fews day on traveling. Thanks for your hardworking! Let's plan our next steps :)\n. You are right! I removed this method and add a similar method in LinearMulticlassMachine.h. So every implementation wants to set up probabilistic support can have a public function for user. But the better way is done in the class constructor, e.g. I added a boolean param to CLinearMulticlassMachine(), so that we can decide at very beginning.\n. Sorry I was using tab expansion.\n. For OVO, MulticlassLabels::m_multiclass_confidences was allocated with size of num_machines, but actually the probability outputs only have num_classes elements, because those are posteriors wrt classes, see Eq.(14) in [1]. \n. This is a good proposal! I'll check if other multiclass methods have probabilistic nature. I am not familiar with every methods there we have. It's a good chance to learn :)  But it seems not every multiclass methods are controlled by MulticlassMachine. I'll let you know if I got a good idea. \n. Is it good to call scores_to_probabilities() in this way? if none of probability heuristic is set. \n. This heuristic need the parameters inside score_to_probabilities(). The implementation is easy but I need to talk to you and Sergey about how to modify that function if we decide to have the softmax heuristic. Please take a look at Eq.(7) in http://hal.archives-ouvertes.fr/docs/00/10/39/55/PDF/cr102875872670.pdf, maybe I am wrong and there is an easier way to do. Let's discuss on IRC if necessary. \n. This feature really good. I use \"const SGVector outputs\" to avoid unpredictable changes. \n. One thing I am concerning: is it good to resize a vector frequently? But in this way, the code seems much more clear than have an outputs and a new_outputs. \n. I think C++ allows overloaded functions, one for constant case and another one for operation on m_features. Keep one is enough?\n. Yes, I agree. I didn't keep our development in mind that time. \n. I just commented for all example code, because I was backing and forth this loss. \nBTW, @iglesias Is the so_hmsvm_mosek.cpp out of date? There are few functions I couldn't find in current code.\n. What if we want to prevent someone changing the contents pointed by m_features? IIRC, this is constant overloading, please see the corresponding section in this article: http://www.cprogramming.com/tutorial/const_correctness.html\n. @iglesias I think you are the best person to update this example. :)\n. Yes. But currently this m_features had not been used in anywhere, I was thinking it may be redundant, and m_features should be in StructuredOutputMachine anyway.\n. Good point! I update right now.\n. You are right. For kernel machine, only gram matrix is needed. The pointer to features should be only in linear struct machine. \n. But I think right now a feature pointer in linear machine doesn't contribute, since we have to register it even it's NULL. \n. @iglesias Thanks for code reviewing! You know much better the REF/UNREF :) \nIf we do SG_REF(m_factor_type) here, the place where it has been called would have to do an UNREF?\nAnd how about CDynamicObjectArray* CFactorGraph::get_factors()? Does REF needed here? \n. @iglesias Thanks for the explain. It make sense. But I remember we don't have to call SG_UNREF, if CFactor is derived from SGObject, right?\n. Okay, I am not familiar with SWIG, I thought std::string could be easier for user to call this function. \nSo you mean if a class is not provided to user I can use STL, and put something like this IGNORE_IN_CLASSLIST ahead?\n. forgot it :p\n. I like things in pairs. I remember once I unref something caused a segmentation fault. But maybe nothing to do with the ref stuff.\n. I'll remove these lines in next PR.\n. Sure tests are ongoing. But in the end this function has not been used. I found hiding implementation with STL could be better. I'll remove this code. \n. Yes, should be -std=c++0x\n. I have thought about this, but it only be used in CFactorGraph. Maybe we should create a \"pgm\" folder, and put all graphical model related classes in, each with a single file. \n. should be better explained :)\n. Yes, a loop. Because 2 nodes in a factor means there is an edge connecting them, if there is another path between them, certainly has a loop in the graph. \n. Luckily, no problem for this one, since no pointers here, but it's better to initialize SGVector as well.\n. In theory, after calling make_set(), in which set the parent of each element to itself, the recursion will be ended after at most O(num_element) steps. I am agree to rewrite it with no recursion and add some unit-tests.\n. yes, in find_set(), the parent[x] will be updated to the root of its set. \n. exactly. \n. will be done :) I am too lazy haha \nno problem comes from here so I didn't do the test. \n. because CFactorGraph is derived from CFeatures, the duplicate() has to be implemented, I referred to LatentFeatures, at LatentFeatures.cpp:36\n. This is called union by rank, i.e. depth in our case. If 2 roots are going to be merged, the higher ranked one will be the new root. \n. It's an efficient data structure for some graph theory, e.g. the minimal spanning tree. But do you think make it a general one and some one will use it?\n. yes, exactly, before there was a private, I think protected better for somebody wants to do more work :)\n. Users need to know, any suggestion to make it becomes safer?\n. I put these initialization into register_parameters(), I know this make things hard to read. \n. yes, this function need to be renamed to init()\n. Not now, but because I know we will create at least one instance, so I ref() here, this will not affect to NULL anyway. \n. Because I assume all the MAP inference all output 2 things: 1) energy value, 2) assignment of each variable node. Basically other outputs will not be used in structured output learning. \n. emm, I see. I forgot why I want to use string. I agree enums will be a better choice here. \n. At first, MAP inference can have many implementations. I was thinking two ways: One is let all implementations inherit from MAPInference, and user call derived class. Another way is keeping a CMAPInferImpl pointer in MAPInference, and create implementation instance according to user's input, i.e. \"TreeMaxProduct\".\nAnd because in implementation class, I may want to use STL, so I'd like MAPInfernece keep an opaque pointer CMAPInferImpl* , and hide implementation in other files. \nDo you think this is the right way to handle the implementation?\n. Because this will cause problems, when m_fg is not allocated dynamically. e.g. CFactorGraph fg; CMAPInference infer(&fg, TREE_MAX_PROD); \nI tried this will cause segmentation fault. Is there better way to deal with this case?\n. Interesting, so I have to use pointer for SGObject. Okay, I'll add the ref and unref for m_fg.\n. @iglesias I was trying to use std::unordered_map but it needs to enable -std=c++0x\n@vigsterkr How to set up -std=c++0x when ./configure? --cxxflags='-std=c++0x' doesn't work, but I add #ifdef HAVE_CXX11 in both .h and .cpp\n. @vigsterkr good! I'd like to try the cmake next Monday. \n. I try to make an example also run the MAP inference. So I slightly changed the graph construction, it becomes a tree graph, because I fixed x == 0 for vertical edges, i.e. if (x == 0 && y > 0). \n. yeah, I should run it with a large scale graph. \n. This is the output of union-find algorithm of disjoint set (for structure analysis). It's true that we can start from any variable node, but this way may be more informative. \n. yes, it is. \n. I also thought about this, but only 2 types of nodes, the same as message type. I think 0-1 is not too bad for an internal implementation. \n. you are right, std::stack is safer.\n. These are notations used in Sebastian's book. Making confusion here? \n. You mean operating summation on m_fw_msgs with std::transform() is not straightforward? \n. That's good point! I'll try to use SGVector or Eigen::VectorXf\n. I think after implementing loopy inference, we could have both cases in the so_factorgraph.cpp, but currently we can't do inference on grid graph. \n. I noticed this recently, could be easily done. \n. Nothing difference indeed. register_params() are old style. Shall I keep the name consistent with other classes? \n. Not really. Heiko said SGVector doesn't need to be initialized, since they are stored at stack. \n. I just realized this is useless, since in different variables can have different number of states. I'll delete it now. \n. Yes. I checked it. For example, in this file: https://github.com/hushell/shogun/blob/develop/src/shogun/structure/MAPInference.h\nat line 79, if you change this line to: \nvirtual void loss_augmentation(SGVector states_gt,\n    SGVector loss = SGVector());\ni.e. make it to 2 lines, \nthen the class MAPInference will not be listed in the generated class_list.cpp, the reason is it is detected as an abstract class. I went into the class_list.cpp.py, I found it detected the line 133 in MAPInference.h has \" = 0;\" . So it consider MAPInference as an abstract class, but actually it is not. It checks virtual functions outside \"{ }\" of MAPInference.\n. I guess previous models doesn't hold a DynamicObjectArray for features.\n. hmm, m_features is a DynamicObjectArray... m_model->get_features() is my old change from your code, I remember there was a get_features() in StructuredOutputMachine at beginning. But no idea, why no meak leak happens. \n. Then, I see the point, for multiclass and hmsvm, features are SGVector or SGMatrix, but in my case, m_features is FactorGraphFeatures, where I hold a DynamicObjectArray for storing FactorGraphs. \n. This is another way to augment loss, but it turns out the same, finding unary factors will take more time. If later we want to have a fast special case of factor graph, where only unary and pairwise potentials, this candidate may be useful.\n. Because I want to these information being printed during the training. Enabling SG_DEBUG will allow other debug information be printed as same time. In my experience, that's quite annoying. BTW, do you think we should have other members to track some statistics of PrimalMosek? such as primal objectives, number constraints. We may need these to have some plots. \n. I didn't decide that time. But turns out normalization is not necessary. \n. yeah, I agree. I don't like the way to control printing. If we can have something like a CDebugging class, an advanced version of SG_DEBUG, it has switches for all classes, such that only the desired classes all output debug information.\n. I got few warnings with fscanf(), the stdio was a try but doesn't help.\n. I just hate need a ref/unref to get the the number, so you think use this better? \ndynamic_cast(m_labels)->get_num_labels();\n. Yeah, m_xxx in python looks weird. I got some strange errors in swig, it doesn't allow mixture of private and public members. \n. It's old code, maybe we should refactor a bit? look at the get_rand_data()\n. Well, it's not a problem, I just want to get the number of samples, at StochasticSOSVM:90, int32_t N = m_model->get_num_labels(); I guess dynamic_cast(m_labels)->get_num_labels(); is also fine.\n. I am going to implement the BCFW this weekend. I think I will need it then. Let me fill out this function.\n. I'll rewrite some comments, make them as clear as possible. \n. I found StructuredModel::get_num_labels() is used in many places, so I think have this function may be useful. And we have get_labels(), set_labels() in this class, an additional function seems not a bad idea. I mean comparing with calling ref/unref in pairs could be more efficient. \n. Yeah, I was trying to discuss with you. Sorry I forgot. Look at Eq.(2) in this paper: http://users.cecs.anu.edu.au/~chteo/pub/TeoLeSmoVis07.pdf\nThe risk should be normalized, I just make it comparable to other solvers. However, it seems no problem without the normalization. \n. If we don't multiply the 1/m, it's not possible to compare the primal objectives with other solvers. In my experience, the normalization doesn't affect the convergence, even a bit faster. But to make sure this, shall I send an email to Michal?\n. Because we may need to get the same random sequence. I was using it for debugging objectives and slack variables. \n. If I am understanding correctly, CMath::random() will produce the same random sequence if the seed used for CMath::init_random() is the same. My purpose is, when I ran the train_machine() multiple times, I'll use the same order of examples for updating my model. Is there any better way to make this, if I don't record the seed? \n. Yes they are features, but instances may be more clear to emphasis that they are instances of factor graphs. \n. But I'll also need to set up different seeds so that we can sample in different ways to get rid of local minima. You mean I can do init_random(seed) in StochasticSOSVM::set_rand_seed(seed)? \n. Well, you are right, QP has only one global minima. But different sequences will result in different objectives (may not converge), since SGD stops at the limitation of number of passes. This is why we need different random seeds. \n. It is not used only to control printing, also for deciding whether new CSOSVMHelper() in train_machine(). We could change all prints to SG_DEBUG. \n. remove this line if it's not used\n. Have you sent the PR to shogun-data? .csv files may be better. \n. No need to reference for SGVector or SGMatrix. \n. I prefer to use SGVector instead of std::vector. \n. No reference needed. \n. Use SGVector? Since n_labs and n_feats are predefined. \n. indent issue. \n. Try to use CDynamicArray http://www.shogun-toolbox.org/doc/en/latest/classshogun_1_1CDynamicArray.html\nrather than std.\n. Factor types \\neq potentials.\n. Maybe tune a bit the lambda to boost performance. \n. Try to avoid to duplicate code. \n. indent issues.\n. Always make a new line when for-loops without brackets. \n. I don't think you want to name after gsomix :)\n. I prefer to use SG_UNREF/SG_REF, and make sure they are in pairs. \n. @iglesias Could we use new/delete with SGVector?\n. For object pointers, please keep in mind to use REF/UNREF in pairs, otherwise there will be mem-leaks. The same rule applies later for CFactor*.\n. What you need to store are not factor data, but the energy tables. Please use fg->evaluate_energies() to get them. \n. Maybe check fg is NULL or not, before using this pointer.\n. Why you call parse() in constructor?\n. SGVector<>* is weird, which may cause some memory problems. Try DynamicArray instead. \n. I am not sure CFile is functioning is this way. Such as LibSVMFile, only initialization performs in constructor. \n. Please use shogun/lib/DynamicArray.h\n. Note that /** @return energy table for the graph */  SGVector<float64_t> evaluate_energies() const;\nWhat we are going to store is this table, not factor->get_data(). \nAnd you need to compute them first. Please make a unit-test, you will understand what I mean here. \n. factor data in our case is just some features. \n. Try without reference, SGMatrix is just a wrapper of pointers. \n. Not sure if DynArray<SGVector<int32_t> > is the correct way. I prefer to use SGVector or DynamicArray. Note that SGVector is a wrapper of pointer. \n. SGVector has a function load(file). This may be what you need. \n. Right. Use lib/DynamicArray.h rather than base/DynArray.h. \n. Use lib/DynamicArray.h \n. Again, no need to use SGMatrix<>&, SGMatrix<> can bring changes out the function. \n. data structures in Shogun is a bit different from std, treat them as pointers, i.e., C style. It's better to find some examples in Shogun code base to learn how to use them.\n. okay, I see what's wrong in your code. You didn't give sizes to labels and feats in L249 and L250. This works, but not the best way. If possible, please move L40-L45 outside this function, and use SGMatrix in params without &. \n. All members should use prefix 'm_'.\n. IIRC, std::vector doesn't support elements with different sizes. \n. CDynamicArray cannot use SGVector as data type, but why don't you store all scope indices and the number of variables at beginning to CDynamicArray< int32_t>? \nWell, if you insist to use SGVector, try SGVector<>.\n. @abinashpanda Replied in the email. Your code seems good! Almost done. My suggestion is to use CDynamicArray or try to use SGMatrixList, then no need to add_vector by yourself. \n. Interesting, in this way, it passed the clone tests. Use \"factors_table\" not \"m_factors_table\". \n. Have you freed m_factors_table and m_factor_scope somewhere? \n. an additional space here.\n. why add a header in this example? \n. indent issue\n. It's not appropriate to pass a SGVector<float64_t>* to an function requires float64_t*.\n. indent issue. \n. Seems users have to take care of these pointers, should they be freed somewhere? Have you checked this example with valgrind? \n. I think float64_t* is enough, please check lib/SGVector.h to see how SGVector is implemented. \n. indent issue\n. space after load_labels\n. No need to add a space between CLineReader and . Well, leave the old code if they don't have problems. \n. directly set SGVector.vector, which is a public member. \n. Okay. I didn't know the context. Just be careful about SG_MALLOC and SG_FREE.\n. Maybe you need to set up tab shift in .vimrc, something like\nset shiftwidth=4\nset tabstop=4\nset smarttab\nOn Mon, Mar 31, 2014 at 3:25 PM, Jiaolong notifications@github.com wrote:\n\nIn src/shogun/io/LibSVMFile.cpp:\n\n@@ -48,10 +48,12 @@\nvoid CLibSVMFile::init()\n {\n-   m_delimiter=0;\n\nm_delimiter_feat=0;\nm_delimiter_label=0;\n\n\n@tklein23 https://github.com/tklein23 @tklein23https://github.com/tklein23I found the original code used tab for indent and for my newly added code,\nI used space. They were aligned well in my vim but why it displays\ndifferently in Github? What should I do to avoid this problem during coding?\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/shogun-toolbox/shogun/pull/2073/files#r11140242\n.\n\n\n/\n- ============HuShell============\n  /\n. SG_FREE is better. I mean use SG_ALLOC and SG_FREE.\n. Need a newline after a for-loop without brackets.\n. Could be const SGVector<>\n. Is there a special reason to make a separated test for factor tables?\n. It's better to remove the show data stuffs, apply for later. \n. How about we present both cases: random data and real data?\n. IIRC, for-loop with single line should be no brackets. \n. Is this a full connected graph? Why we have node index > 5? I mean shouldn't be like:\n[0 1; 0 2; 0 3; 0 4; 0 5; 1 2; 1 3; 1 4; 1 5; ...; 4 5]\nBTW, if this is a full connected graph, label_tree_index doesn't make sense. \n. For full connected graph, I think only one big factor needed, with 2^6 entries in the factor table. \n. Let's get rid of these SHOW_DATA code now.\n. Remove the code if not needed.\n. Remove the code if not needed. No need to ref/unref a pointer in a DynArray. \n. indent issue again.\n. same here\n. and here\n. Set verbose = false to speed up training, otherwise twice slower. \n. indent.\n. Add comments for every function and member variable. \n. Graph Cuts?\n. Do add comments even for private functions\n. It well be nice if we have some description like this: http://www.boost.org/doc/libs/1_38_0/libs/graph/doc/kolmogorov_max_flow.html\nI mean mention some important concepts, such as orphan, so people can read code easily\n. remove the additional comments\n. consistent comments needed\n. Could we use SGVector or DynamicArray here? \n. Could we use CList here? So that we could include this class to doxygen. \n. Error: ETerminalType is not a class or namespace\n. Not sure why add m_num_factors_at_order3. In fact, I am not sure how order 3 works here. Could you point out a reference for this?\n. Use sg_malloc?\n. Same\n. Remember to REF/UNREF this pointer!\n. copy or move this comment to header may be better. To where add_factor() is\n. Add a function build_st_graph() for the rest of code in this function?\n. Ok. I forgot this is mentioned in Kolmogorov's PAMI04. \n. make sure in general energy is negative. \n. summarize comments in header as well\n. Let's remove ITER_COND_MODE, MEAN_FIELDs, since they are not MAP inference. \n. Yes, just unref it when finished. \n. I mean we need to mare sure that m_map_energy is energy value or its opposite. Note that GC solves minimization, but SOSVM requires maximization. \n. here as well\n. need to add @param for doxyen\n. comments\n. This is a good idea!\n. pairwise?\n. max-product cannot used for full-connected? Have you tested it?\n. I guess you just need to modify FactorGraphModel:297\n. Sorry, I should explain this. \nLook at Mosek.cpp:84-89: https://github.com/shogun-toolbox/shogun/blob/develop/src/shogun/structure/PrimalMosekSOSVM.cpp#L84\nlb and ub are supposed to set up in init_primal_obj() and then input to Mosek solver at mosek->init_sosvm(). \nThis is why we'd like to use reference for lb and ub, as exactly the same as the regularization matrix C. \n. I am not sure Mosek is installed on Travis server, so I add this macro USE_MOSEK. But I tested on my computer with ./shogun-unit-test --gtest_filter=PrimalMosek* (also checked valgrind)\n```\n[==========] Running 1 test from 1 test case.\n[----------] Global test environment set-up.\n[----------] 1 test from PrimalMosekSOSVM\n[ RUN      ] PrimalMosekSOSVM.mosek_check_w_bounds\n[       OK ] PrimalMosekSOSVM.mosek_check_w_bounds (50 ms)\n[----------] 1 test from PrimalMosekSOSVM (50 ms total)\n[----------] Global test environment tear-down\n[==========] 1 test from 1 test case ran. (51 ms total)\n[  PASSED  ] 1 test.\n```\n. forgot about this :( I was copying pasting. Let me update it. \n. Yeah, it's not easy to set lb and ub eternally, since currently no models\nused lb and ub, and the only way is calling init_primal_obj(). We could\nconsider to have functions get_lb/set_lb and get_ub/set_lb, but I don't\nthink these functions will wildly used.\nOn Sun, Jul 6, 2014 at 11:29 PM, Fernando Iglesias <notifications@github.com\n\nwrote:\nIn tests/unit/structure/PrimalMosekSOSVM_unittest.cc:\n\n\ndPsi.zero();\nresult->psi_pred_sparse.add_to_dense(1.0, dPsi.vector, dPsi.vlen);\nresult->psi_truth_sparse.add_to_dense(-1.0, dPsi.vector, dPsi.vlen);\n}\nelse\n{\nSG_SPRINT(\"Should have either of psi_computed or psi_computed_sparse\"\n\"to be set true\\n\");\n}\n  +\nreturn ( mosek->add_constraint_sosvm(dPsi, con_idx, train_idx,\nmodel->get_num_aux(), -result->delta) == MSK_RES_OK );\n  +}\n  +\n  +SGVector dummy_mosek_sosvm(CFactorGraphModel* model,\nSGVector lb, SGVector ub)\n\n\nIs this pretty much the same code as\nCPrimalMosekSOSVM::train_machine(CFeatures*)?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/shogun-toolbox/shogun/pull/2372/files#r14582694.\n\n\n/*\n- ============HuShell============\n  /\n. Yeah, I understand. This way is ugly. But I don't want to add too many\nthings here, as I am just helping Jiaolong to add the bounding\nfunctionality. Actually he'll implement the setters in FactorGraphModel for\nlearning with GraphCut. Maybe we abort this PR and wait Jiaolong to do\nunit-test for thorough PrimalMosekSOSVM test?\nOn Sun, Jul 6, 2014 at 11:39 PM, Fernando Iglesias <notifications@github.com\n\nwrote:\nIn tests/unit/structure/PrimalMosekSOSVM_unittest.cc:\n\n\ndPsi.zero();\nresult->psi_pred_sparse.add_to_dense(1.0, dPsi.vector, dPsi.vlen);\nresult->psi_truth_sparse.add_to_dense(-1.0, dPsi.vector, dPsi.vlen);\n}\nelse\n{\nSG_SPRINT(\"Should have either of psi_computed or psi_computed_sparse\"\n\"to be set true\\n\");\n}\n  +\nreturn ( mosek->add_constraint_sosvm(dPsi, con_idx, train_idx,\nmodel->get_num_aux(), -result->delta) == MSK_RES_OK );\n  +}\n  +\n  +SGVector dummy_mosek_sosvm(CFactorGraphModel* model,\nSGVector lb, SGVector ub)\n\n\nI would say we either need these setters or do the unit test in a\ndifferent way (e.g. testing it through classes that use different\nconfigurations of lb and ub). I do not really as an option to copy paste in\nthe unit test the code we want to test.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/shogun-toolbox/shogun/pull/2372/files#r14582843.\n\n\n/*\n- ============HuShell============\n  /\n. You are more experienced than me :). I agree with you that copy/paste code is not good. But we do need unit-tests on the newly added code. I'll ask @Jiaolong to take over this PR. Let's close it? \n. @lisitsyn Yeah! This is a problem. We have a QP project this year, right? We could think about to have a general primal QP SOSVM, so that there are many choices of QP solver, rather than to stick on Mosek. \n. We have SGVector::operator= ? Is newly added?\n. Well, here what I want is PrimalMosekSOSVM only accepts lb with the same length as w, i.e., M, otherwise, w is unbounded. lb and ub are outputs of m_model->init_primal_opt(), so the responsibility is at m_model side. \nSo you would like to give some feedback? I actually use REQUIRE in Mosek:131, but checks can also be here. Give me few sec to change. \n. Direct \"=\" doesn't clone this vector. I have tried, which just passes the\npointer. So if lb changed, m_lb will be changed as well. This behavior may\nbe not correct for a setter, isn't it?\nOn Fri, Jul 11, 2014 at 6:02 AM, Fernando Iglesias <notifications@github.com\n\nwrote:\nIn src/shogun/structure/PrimalMosekSOSVM.cpp:\n\n@@ -299,4 +308,14 @@ void CPrimalMosekSOSVM::set_epsilon(float64_t epsilon)\n    m_epsilon = epsilon;\n }\n+void CPrimalMosekSOSVM::set_lower_bounds(SGVector< float64_t > lb)\n+{\n-   m_lb = lb.clone();\n\nDid it work out using it instead?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/shogun-toolbox/shogun/pull/2372/files#r14819113.\n\n\n/*\n- ============HuShell============\n  /\n. Why need define here? I feel only changes needed is moving infer_met here. \n. shouldn't be init_primal_opt()?\n. This condition is bad. Should be more general. I mean bounds can add to any factor types, isn't it?\n. FactorGraphModel should not be tied with GraphCut. Maybe check inference type? And call a special set up of bounds, e.g., CMAPInference::set_learning_constraints(), something like this. \n. I think no need to add swig interface for GraphCuts, since this class will never created by a python user. \n. how about using a switch-case? \n. how about checking cardinality of a factor? \n. Should we also make assumption that data are shift to be greater than 0? And maybe check this point as well?\n. I mean we should be prepared for future cases. \n. OK. So maybe add an example for GraphCut, so that people will know how to use GraphCut. A python example is fine. \n. REQUIRE better?\n. Check FactorType->m_cards\n. Last thing about this PR: how do we deal with the case where edge features are available? then w_dim can be 8 for example? when dimension of edge feature is 2. \n. Alright! Tag a TODO there, but change assert to REQUIRE, and say something like temporally GraphCut doesn't support edge features. \n. I think it's not a good idea to have a separated file for MDArray. First, we have SGNDArray. I know it's more convenient to use the implementation by the original code. I suggest to put this class inside MPLP.h\n. It's better to use mathematics/Math.h\n. Regions actually play the same role as factors, which could be viewed as a generalization of factors. Indeed, in the following paper, region is defined as a set of factors. Anyway, I think the explanation here is not very precise. BTW, The dual variables in dual LPR are messages. \nJonathan S. Yedidia et al. Constructing Free Energy Approximations and Generalized Belief Propagation Algorithms. 2004. \n. It's fine to introduce the CRegion at this moment. But I guess, it's possible to reuse our CFactor directly. \n. Comment out printing in python examples. This seems a convention. :/\n. I just went over the multi-dim array class. Do you think it's possible to combine this class with SGNDArray? For me, this class it's very general, can be viewed as a C implementation of Matlab's matrix. \n. Could you make this long line to two separated lines?\n. Same here. I guess we could do 80 characters per line. \n. Sontag's thesis p104 has nothing to do with this function. I suggest point people to Globerson et al. NIPS 07. \n. We don't actually tightening LPR in this code, so maybe ignore the first 2 references temporally. \n. between?\n. protected better? \n. messages summed to the intersect?\n. energy table? \n. This is the core function of the implementation. It will be good if you put the formula here, and write in latex form. I remember doxygen supports latex. See chap 6 of Globerson's NIPS 07 for the formula. \n. It seems lambda in the code is not the same meaning as in the papers. It's more common to denote as theta. Anyway, lambda here means potential function/table. \n. Would it better to use our factor graph and factors directly? Instead of copying every call of inference, which is time consuming. You can think region as factors. If pairwise factorization used (e.g. grid graph), intersections are variables. \n. BTW, it will be better to link people to mplp_ver2 code.\n. Why inline here? \n. Could you make a test for a grid graph, e.g., 10x10. \n. Yeah, I see. Let's discuss with other guys. \n. Hey @iglesias We have to implement a multi-dim array here- MDArray, which has some overloading operators specifically for float type. Do you think we should merge some code here to SGNDArray? \n. Yeah, if exactly the same functions needed for MPLP and GraphCuts, merging them is not a bad idea, for example rename to approx_infer_unittest.cc\n. Hey @iglesias We do need higher dimensional arrays to store messages involving 3 or more variables. It's possible to flatten the arrays, but then base sizes have to be recorded, which is like reinvent a multi-dim-array class. How about we keep Jiaolong's implementation here? And we open an TODO issue for future merging of these two classes? \n. Hey @iglesias, Yes, MDArray is more or less the same as SGNDArray, base_sizes is SGNDArray::dims. I just revised these two classes, they can be combined together. \n@iglesias, so your point is to implement the functions we needed to SGNDArray, right? If so, @Jiaolong, would you mind to send another PR for merging MDArray and SGNDArray? For *= +=, can be specifically for float data type, reference SGVector for hints.\n. @Jiaolong What do you think? Anyway, I agree that having 2 classes implementing the same thing is not a good idea. \n. @Jiaolong @iglesias I suggest to close this one first. \n@Jiaolong It's important to send an independent PR for SGNDArray. We need to make sure our change will not affect any other classes using SGNDArray. \n. same checking for curr_index, isn't it? \n. Maybe passing index_t* is not a bad idea, since the member dims has type index_t. \n.  if/for with only 1 line in the main body, remove {}, and start new line after if/for\n. Why a restriction on num_axes?\n. more comments? as well as the constructor after. \n. same here, since there is an argument. \n. should we use get_dims() instead? \n. no brackets needed in this case. \n. Are you sure for any other types, such as int8, this will not overflow? \n. Maybe more specifications needed? \n. why += -= are different from =? Should we also consider the case with a single number?\n. emphasis which is the first max element? \n. maybe put a newline after template?\n. keep this in mind :) It's fine to open an issue after this PR. \n. maybe this way better? the flatten length of the ND array?\n. a newline after for loop\n. should we use a unified name? I mean change all bases to dims. \n. Wouldn't be good to have wrapper for doing this?\n. because there are may be multiple max values. \n. Do we really need this header?\n. Another overloading function for clone using operator=? Because in the unit-test, arr = 0 looks a little bit weird. \n. Suggestion: \"The task is defined as a pairwise factor graph model with GraphCuts inference, where model parameters are learned by SOSVM using a SGD solver.\" \n. we loss all contex --> we loss all context\n. This Eq not quite right. \\phi_u(x,y_i; \\theta_u) and \\phi_p(x,y_i,y_j; \\theta_p) should be good, which means potentials parameterized by \\theta. \n. finit --> finite?\n. Are you sure there are different features for different labels? phi(x,yi,yj)? not different parameters for different labels?\n. IIRC, we are using theta_ij(yi,yj) in the code. \n. Yeah, you are right! Pairwise parameters are shared, so theta_p(yi,yj) *\nphi_ij, isn't it? Because phi is factor type tied.\nOn Sun, Aug 24, 2014 at 4:17 AM, Jiaolong notifications@github.com wrote:\n\nIn doc/ipython-notebooks/structure/Binary_Denoising.ipynb:\n\n\n\"\\n\",\n\"Given a noise black/withe image $\\textbf{x}$ of size $m \\times n$, the task of denoising is to predicted the original binary image $\\textbf{y}$. We flatten the image into a long vector $\\textbf{y} = [y_0, \\dots, y_{m\\times n}]$, where $y_i \\in {0, 1}$ is the value for pixel $i$. In this work, we aim to learn a model from a bunch of noise input images and their ground truth binary images, i.e., supervised learning. One may think about learning a binary classifier for each pixel. It has several drawbacks. First, we may not care about classifying every single pixel completely correct, i.e., if we misclassify a single pixel, this is not as bad as misclassify a whole image. Second, we lose all context, e.g., pairwise pixels (one pixel and its neighbore). The structured predition here is to predict an entire binary image $\\textbf{y}$ or a grid graph of $m \\times n$. Here, the output space $\\mathcal{Y}$ is all possible binary images of size $m \\times\n  n$. It\n  can be formulated as following:\\n\",\n\"\\n\",\n\"$$\\n\",\n\"\\hat{\\textbf{y}} = \\underset{\\textbf{y} \\in \\mathcal{Y}}{\\operatorname{argmax}} f(\\textbf{x},\\textbf{y}),    (1)\\n\",\n\"$$\\n\",\n\"\\n\",\n\"where $f(\\textbf{x},\\textbf{y})$ is the compitibility function, measures how well $\\textbf{y}$ fits $\\textbf{x}$. There are basically three challenges in doing structured learning and prediction:\\n\",\n\"- Choosing a parametric form of $f(\\textbf{x},\\textbf{y})$\\n\",\n\"- solving $\\underset{\\textbf{y} \\in \\mathcal{Y}}{\\operatorname{argmax}} f(\\textbf{x},\\textbf{y})$\\n\",\n\"- learning parameters for $f(\\textbf{x},\\textbf{y})$ to minimize a loss\\n\",\n\"\\n\",\n\"In this work, our parameters are pairwise and unary potentials and they can be written as:\\n\",\n\"\\n\",\n\"$$\\n\",\n\"f(\\textbf{x},\\textbf{y}) = \\sum_i \\theta_u \\phi_u(\\textbf{x},y_i) + \\theta_p\\phi_p(\\textbf{x}, y_i, y_j),    (2)\\n\",\n\n\nIn fact, since we use fixed size of images, we can define full-connected\nfactor graph model and use different parameters for different pairwise\nfactors, but it will make the computation very slow.\nI have tried it for 10x10 images, and it works very well but for images\nlarger than 20x20, it is extremely slow.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/shogun-toolbox/shogun/pull/2467/files#r16635443.\n\n\n/*\n- ============HuShell============\n  /\n. I understand this is what we are doing in code. Conventionally, people\ndon't use a joint \"node\" feature vector phi_u or a joint \"edge\" feature\nvector phi_p. Besides, if this is a dot product, may be better to use a\ntranspose.\nOn Mon, Aug 25, 2014 at 2:37 AM, Jiaolong notifications@github.com wrote:\n\nIn doc/ipython-notebooks/structure/Binary_Denoising.ipynb:\n\n\n\"\\n\",\n\"Given a noise black/withe image $\\textbf{x}$ of size $m \\times n$, the task of denoising is to predicted the original binary image $\\textbf{y}$. We flatten the image into a long vector $\\textbf{y} = [y_0, \\dots, y_{m\\times n}]$, where $y_i \\in {0, 1}$ is the value for pixel $i$. In this work, we aim to learn a model from a bunch of noise input images and their ground truth binary images, i.e., supervised learning. One may think about learning a binary classifier for each pixel. It has several drawbacks. First, we may not care about classifying every single pixel completely correct, i.e., if we misclassify a single pixel, this is not as bad as misclassify a whole image. Second, we lose all context, e.g., pairwise pixels (one pixel and its neighbore). The structured predition here is to predict an entire binary image $\\textbf{y}$ or a grid graph of $m \\times n$. Here, the output space $\\mathcal{Y}$ is all possible binary images of size $m \\times n$. It\n  can be formulated as following:\\n\",\n\"\\n\",\n\"$$\\n\",\n\"\\hat{\\textbf{y}} = \\underset{\\textbf{y} \\in \\mathcal{Y}}{\\operatorname{argmax}} f(\\textbf{x},\\textbf{y}),    (1)\\n\",\n\"$$\\n\",\n\"\\n\",\n\"where $f(\\textbf{x},\\textbf{y})$ is the compitibility function, measures how well $\\textbf{y}$ fits $\\textbf{x}$. There are basically three challenges in doing structured learning and prediction:\\n\",\n\"- Choosing a parametric form of $f(\\textbf{x},\\textbf{y})$\\n\",\n\"- solving $\\underset{\\textbf{y} \\in \\mathcal{Y}}{\\operatorname{argmax}} f(\\textbf{x},\\textbf{y})$\\n\",\n\"- learning parameters for $f(\\textbf{x},\\textbf{y})$ to minimize a loss\\n\",\n\"\\n\",\n\"In this work, our parameters are pairwise and unary potentials and they can be written as:\\n\",\n\"\\n\",\n\"$$\\n\",\n\"f(\\textbf{x},\\textbf{y}) = \\sum_i \\theta_u \\phi_u(\\textbf{x},y_i) + \\theta_p\\phi_p(\\textbf{x}, y_i, y_j),    (2)\\n\",\n\n\nSince theta_p is independent to i, j, I think we can just put\ntheta_p*phi_p(x, y_i, y_j). Here, phi_p(x, y_i, y_j) is in the form of\njoint feature.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/shogun-toolbox/shogun/pull/2467/files#r16648138.\n\n\n/*\n- ============HuShell============\n  /\n. Let's keep it temporally. \n. EdgeFactor is not a proper name. I think Region was a good one, as conventional. \n. the format of comments not good. \n. what do you mean var sets? intersection sets? which are intersection of variables with other regions. \n. trying to use original comments, and combine with our Factor\n. May need to explain more about these vars. \nobj_del = last_dual_obj - dual_obj\nint_gap = duality gap\nCorrect me if I am wrong. \n. I guess this is analogy to sepset for clique trees. However, actually messages are passed between clusters/regions which are not formed in a hypertree but in general a hypergraph. I don't know if seperator set is a conventional in the context or not. Besides, calling it intersection sets is not a bad idea. The more we follow the OI names, the easier for people to understand. \n. I suggest to rename to find_intersection_set_index\n. Suggest to rename to convert_energy_to_potential\n. Is it better to use std:set? since you don't want to have copies. I mean: vector< set > m_clique_separators. Then use set:insert, rather than this function. \n. = m_all_intersects?\n. It's nice to have members here since we don't have class Region. However, I suggest to rename separaters -> intersection_sets and clique -> region. Also applies to other members below. \n. cluster -> region\n. -> m_region_theta\nI know they call m_region_lambda, but lambda was used in the paper differently .\n. Now I think it's safe to remove other inference types, only keep TreeMaxProd (for tree), GraphCut (for binary loopy graph, better with submodular energy gurantee) and GEMPLP (general loopy graph), since we don't want to implement more methods. The default one choose GEMPLP. In a short time period, we don't want to support 3rd party inference lib.\n@Jiaolong @iglesias Thoughts? \n. BTW, since the factor graph is an input, we could check the graph type, and warn if the inference type is not a best fit. \n. Is this one the same as the eq in section 6 of [1]? \n. lam_minus need to multiply (1-1/num_sep) ? \n. Definitely a good idea! Some of functions in CFactorType CFactor for operating energy tables are hard to follow. You may open an issue for this. So either you or I can do it when we have time. \n. ",
    "kevinhughes27": "I can't seem to find an existing unit test for QDA can someone confirm that there isn't one? If this is the case then I'll look into writing a unit test for both these algorithms\n. @lisitsyn Unit test is up! (and tested on my machine) any more thoughts on this? I'd like to merge this and then start working on a PR for FFDiag.\nWould also be good to have Andreas take a look\n. @sonney2k @lisitsyn This should be all fixed now! Looks like it may have been a problem with my memcpy. I changed the is_permutation_matrix function to take an eigen3 matrix instead which actually made the unit test cleaner. Anyways everything just ran on Travis so this should be good to merge!\n. Looking forward to hearing your comments!\n. Yeah the implementations are a bit different, some of the ICA algorithms are pretty sensitive to very subtle differences I seem to remember. I think SOBI is the vanilla whitening. Use what is done in those classes as a basis to implement a vanilla whitener.\nAfterwards for bonus points try replacing the code in the ICA algs with the new shogun preprocessor and see if all the tests and demos still work! \n. Looks pretty cool, I agree with @karlnapf's points\n. @ahcorde yeah you need to do a rebase and \"fix\" your commits into one\n. git rebase -i HEAD~13\nfor 13 commits then follow the instructions to clean it up\n. oh and I also moved the download section above the license, seemed to flow better to me\n. yeah we can discuss changes to the website separately, I just thought we should add this to our readme!\n. we should only have one test file for LDA, can you rebase this off master. \n. @karlnapf want to take a look through this one now?\n. super cool!\n. also with this are we going to simplify the examples folder and consolidate the undocument and documented examples? I've always thought this was confusing\n. could be cool!\n. sorry could you clarify? I matched the style of QDA.h\n. Okay I will take a look. This example is exactly the same as the QDA example and this is how that example was done\n. Should I update both the QDA and LDA example?\n. Looking at the data generator it doesn't appear to do 100% of what we need here because I can't see how to simply create 2 separable distributions (what the +/- DIST is doing for us), sometimes the data works other times the classifier makes a lot of mistakes because the data isn't separable (I was just using generate_gaussians). \nObviously there are ways to use this module to generate the data and make it separable I'm just not sure its actually better than what we currently have, as in it would be roughly the same amount of code. Maybe I'm just not using the DataGenerator class properly yet? Or we could add this particular data generation functionality to the DataGenerator class.\n. I looked at the source code today and it seems like it should work as we need it to. I'll make some plots and see whats up\n. I tested the DataGenerator class by plotting its output and it looks like it should be working fine - I am not sure why neither LDA or QDA seem to work consistently with this data as opposed to the old function \n\n. okay I found an issue in my example program, should have it fixed shortly\n. how should I style this? should the debug print outs be fully left aligned with the #ifdef's\n. okay :( I guess I am kind of attached to my lapack version because I fought with that lib so hard to get it working lol. I'll take it out eventually but I want to run some speed checks for kicks after I've done more on the Eigen3 version\n. I don't think so - I only made a PR for the header there is a JADiag.cpp. I mainly wanted to discuss the interface but in hind sight its hard to fully see with the cpp file as well. I'll include the cpp when I make these other changes \n. Its a pointer because you don't need to pass V0. V0 is a guess and if you have a good guess the algorithm will converge quicker. So Its a pointer so I can test if it is NULL meaning it wasn't passed. There is probably a better way to do this. Have another look when I add the cpp file and it should make more sense.\n. @iglesias @lisitsyn Okay so without including limits how can I set the default value of eps to be the machine epsilon?\n. @lisitsyn thats what I was thinking initially, how would that look for the default argument? \n. @lisitsyn I changed it to <> and it wouldn't build - do I need to add something to the build file to make this work? I always though you used \"\" for local includes\n. do you want these <> again? see below.\n. right of course, silly mistake\n. The diff files for JADE don't really make sense FYI this should be treated as all new code\n. yeah that looks like a typo\n. I'll move it above just in case for the next PR\n. I didn't know about those, I just mimicked what I'd done in the past for QDA and MCLDA\n. Oh, I thought you meant the ifdef flag, yeah I was working with cout and just left it that way, I can switch if its prefered\n. yeah that might be handy - can you link any files where this is done?\n. quick look doesn't look like there is anything\n. how do you feel about leaving it as cout?\n. This code hasn't changed at all I just noticed that it was spaces not tabs so I re tabbed it, same goes for the rest of this file\n. I'm assuming this was a type so I fixed it\n. It would be great to dig in and figure out how to fix this line. It doesn't work because it returns a point to generic CFeatures object not a DenseFeatures object. The actually return type is a DenseFeatures cast back to a CFeatures. I looked into how trying to cast in Octave and it didn't look like it was possible. @sonney2k this might be a SWIG question?\nAnyways returning the mixing matrix and un-mixing below works for now but it would be good to fix, I'm open to suggestions. \n. the Jedi \"inner\" functions were directly ported from c inner functions written for the R jointDiag package and this variable was not used so I commented it out.\n. yeah I prefer that style too - again these functions were ported directly and I tried to not change them much\n. cool I had looked for a better way to do this but didn't find one. To be honest I'm not even sure why this is a vector and not a constant because its always used as one. When porting from R to C++ I didn't want to change anything until it was working \n. @iglesias I did, but this line didn't change so this comment is still here\n. I am not sure whats up with this diff - the function was just moved into the local anon namespace\n. looks like it isn't I'll add something\n. I think I have been consistent across all my code that I use the namespace, make a typedef for EMatrix and EVector. Then I only use these typedefs. I use the Eigen:: only for Map - otherwise Map is kind of ambigous at first glance\n. @lisitsyn Okay so both SOBI and FFSep need a function that does this same thing and it has the same name for now. I thought the best solution would be the anon namespace. The other solution would be to have the function as part of the class but then I would either need to make it take SGMatrices etc and then there would be a lot more Eigen::Maps in the code and it would be messier or I would have to include Eigen3 typedefs in the header which I also wasn't wild about.\nI thought anon namespace was the best solution, open to other ideas though!\n. @iglesias done\n. @iglesias added ASSERT(Features)\n. @iglesias I added a few comments\n. @iglesias Im going to leave this for the time being for consistency with the other ICA algorithms. I may switch to REQUIRE when I make commits for the final code review but for now lets stay consitent.\n. this is a great start but I think we would like to have this use a flag so user can build with OPENCV=ON. Right now this would just appear as a bonus if people have both but if for example someone wants OpenCV integration and they don't have OpenCV installed we won't warn them about it - makes sense?\nHave a look at the way we do it for python and python modular\n. we don't need anything about shogun in this first test we just want to have a sanity test that proves opencv was included in the build. We also use assertions in tests not prints.\nSince this test isn't testing anything probably just create a opencv data structure and that is it. I don't know/remember that much about testing in c++ is it cool to have a test that would fail by throwing a missing link exception or can we do something like assert_nothing_raised (thats the ruby in me lol) ping @iglesias \n. @iglesias yes cmake will take care of it configure wise. What I want is a simple test that uses OpenCV (we don't even need shogun at this point) to test our build bots with because we will want a build bot that includes the OpenCV flag so we can test all the future work @kislayabhi does.\nI want to see this build green which it will because it should skip all opencv tests at the moment and then we need to configure a bot to do an opencv build - does that make sense?\n. Yes the plan is to install via a package manager - I meant test our build environment I guess. I want to have travis and the others running a simple test that uses opencv so that when we add the more complicated conversion test we know that opencv is there and our framework is all setup right.\n. nice this is great and I like that you created a new block to put this kind of stuff in, this is getting pretty picky but I wonder if we want a shorter flag, like just OpenCV.\n@iglesias @lisitsyn @vigsterkr for opinions \n. picky again but lets capitalize it as OpenCV\n. so this main function shouldn't exist here and it might mess things up. Remove this and take all this code that uses the factory and create unit tests - make a test for each kind of conversion (ie CV_8U to double etc.) you might want to automate this or automatically generate this code.\n. I think the name of this object should imply that it takes OpenCV data structures. - unless we have more integrations planned that could re-use a generic factory.\nI'd go with CV2SGMatrixFactory\n. how about CV2FeaturesFactory\n. SG2CVMatFactory\n. bad indent\n. it might be better to make a lot of smaller unit tests. We have done something before using templating to generate tests like this.\n. I actually meant pass the default in the function call but this works too.\n. can we change these to use the actual opencv type def so it is more readable - like CV_32S or whatever it is\n. I'm really tempted to 1-line all of these templates here as its really just a mapping. One line per struct definition. \na single struct would look like this then. I don't know if this matches our style at all. What do you guys think?\ncpp\ntemplate<> struct SG2CVTypeName<float64_t> { static const int get() { return 6; } };\n. I don't like that we are repeating code here - lets do one of two things here:\n1. Make the conversions into private class methods and call them in the switch case\nor\n2. Provide a default value in the method signature for option\neg.\ncpp\ntemplate <typename T> cv::Mat SG2CVMatFactory::getMatrix(CDenseFeatures<float64_t>* A, SG2CVOptions option = SG2CV_MEMCPY)\n. yeah lots of indentation fixes need to happen here\n. can we name the test matrices better so that if a test fails its clear what conversion isn't working\n. There is also a lot of duplication here (duplication is bad because it means in the future we may need to change code in multiple places).\nI think you should take this whole test and make it into a function that takes option as an argument. Then in your main test function you will simply call this function three times, once with each option flag.\n. same comment here (make a function and call it for each option), sometimes this is referred to as making code DRY (Don't Repeat Yourself).\n. we use spaces on either side of an = right?\n. lets call this getSGMatrix\n. Also are these the same as the other templates? They look like it to me. We should move them to a separate hpp file and call them something like CV2SG_TypeMaps\n. lets call this getCVMat so it is clear what this is doing\n. @kislayabhi you are missing a line of code here and thats why this isn't working for mismatch type conversions. For example from my gist here is how to do a type change conversion using the constructor:\ncpp\nMat cvMat = Mat::eye(3,3,CV_32FC1);\ncvMat.at<float>(0,1) = 3;\ncvMat.convertTo(cvMat,CV_64FC1);\nSGMatrix<float64_t> sgMat((float64_t*)cvMat.data,3,3,false);\nYou should put the convertTo code inside an if statement and only use it if required, ie. detect if there is a type mismatch and if there is run the convertTo\n. so this conversion doesn't look right to me - in the sense that it is not very powerful. Why are we constrained to only converting CDenseFeatures<float64_t>* into cv::Mat? and why is the returned cv::Mat always a CV_64F?\nI think the template should be supplying the desired return type. I think you should just be passing CDenseFeatures without a template type and asking for the type inside this function.\nThe goal of this factory is to take any CDenseFeatures object and product the requested type of cv::Mat. Also for interests sake I think the main use case for pull data back into cv::Mat would be to visualize using highgui\n. as a note I think this also explains why the templates mapping opencv to shogun that are in both files look wrong to me. When you re-work this class I think you'll need a set of typemaps that are the opposite way.\nMy other comment stands though, lets have a new header file where we define these mappings\n. This is much better!\nI still think you should take this test function and make it a generic non test function that takes the option argument. Then we can have a single test file with 1 test function for each conversion type and cut our amoutn of code by 2 thirds.\nVery good though!\n. there is some wonky indentation going on here\n. this is getting closer to what I had in mind. There is some more weird indentation in this file too.\n. we shouldn't need to redefine this class in each test file - lets put it in its own file\n. there is a bunch of bad spacing here - what text editor are you using?\n. weird spacing\n. I think its our style to have spaces around = etc\n. why tab this in?\n. you might be able to get rid of some of these loops using more eigen methods, its fine this way though imo\n. :+1: for this - thats what I was getting at earlier in my comment\n. it would be cool do drop these cpp files as they have nothing interesting in them. I couldn't get it to work though when I removed this file and inlined the constructor, failed linking or something. @kislayabhi can you play around with this maybe @lisitsyn or @iglesias can help with why it failed to build shogun\n. thinking about this again, maybe this doesn't need to be its own class but rather just another method on the SG2CV_Factory. \nYes lets do that lets drop the matrix from the factory name and add a method called getDenseFeatures\n. again lets try and drop this useless cpp file\n. I don't understand how this works but it does - maybe a place for a comment? or can someone explain this to me\n. not to be confusing I am not suggesting any changes here I'd just like to learn :)\n. same comment, lets figure out how to not need this file\n. so this solution was hacky by me - we should test all the conversion methods all the time. I think the best bet is to actually copy and paste all these tests 2 more times and then have them run for each option. \nI'm open to other ideas but this way is the best I can think of - its a lot of similar code but its easy to see exactly what test failed.\n. the more I think about this the more I am convinced that this is a great idea! Also we don't have any tests for the dense features conversion right now so please add some once you move this functionality over to eh main CV2SG_Factory class\n. Also we should add the same thing to the reverse factory aka DenseFeatures -> cv::Mat\n. you are right that is the class layout we want (just those 2) and yeah it is super trivial to get dense features from matrices but everyone is going to need to do this all the time so we should include it.\n. It does cover all the possible conversions what it doesn't cover is each conversion using each technique (Manual, Memcpy ... etc)\n. we should have default args here\n. and here\n. missing indent I think\n. hmm at this point we are giving up any speed gains we might have had from this method. A few things - you mentioned the error was from OpenCV freeing our memory right? Is there any way to tell OpenCV hey don't manage memory on this object?\nYou might hate me for this but having multiple methods was kind of a fun experiment but I think we should drop all the methods except Memcpy for merging into master. Please add this as a new commit though and don't squash this history because I think this was neat and a worth while endeavor. \n. @lisitsyn I seem to recall you helped @kislayabhi come up with this, can you explain whats going on here for interests sake?\n. same thing lets drop the multiple methods, it was cool and worth experimenting with but there is no reason anyone would want to use something other than memcpy\n. Dropping the multiple methods for doing the conversion will also simplify the tests here and below\n. recognition is spelt wrong in this paragraph\n. the double and with the glory is awkward to read, at least change it to:\nall glitches, glory and prospects\n. dealing with\n. whats up with this line? something from python that I don't know about?\n. I like these steps but lets interlace the description of what you are doing with the images. So it will say grayscale followed by a gray image then remove noise followed by the smoother image - makes sense?\n. There is too many false positives!\n. We don't want the actual number plate to disappear!\n(drop the just)\n. This whole paragraph is weird it says the same thing 3 times. Lets drop this down into a single sentence saying that this has improved the results but we want to do better still.\n. that you might have used in MS Paint or other drawing programs\n. You mean that you used Matlab to generate this dataset? I would omit how you got this dataset from the notebook its not really relevant.\n. you might even be able to use a standard ocr data set\n. why is the output ['M', 'A', '1', '3', '9', '8', 'A', 'G']? Is the 1 coming from the false positive detection of a number near the bottom of the plate? We should fix this up so that we get a good result here.\n. okay cool I didn't know what this was doing is all\n. nitpick - bunch of unix new lines are missing\n. ",
    "alibezz": "I'll take care of this one.\n. ",
    "monalisag": "Eigen3 uses two-sided Jacobi Algorithm, which is computationally slow though it is the most reliable SVD. (http://eigen.tuxfamily.org/index.php?title=Reliability)\nAlso, I wont be able to implement the tests as I have my exams and will only be able to do so after 8th may and I will try to improve the quality of code further by using column wise operations and reducing the presence of loops.\n. @karlnapf I have made a unit test for QDA and wrote the wrong commit message by mistake. \n@pickle27 I will take a look at your example.\n. @karlnapf  Rebasing done.\n. I just changed the example by giving an input to apply\n. ",
    "monicadragan": "Done. Thanks for the feedback!\n. I'll keep that in mind. Thanks!\n. ",
    "votjakovr": "What is wrong? \nWhen I save file trailing whitespaces are automatically removed.\n. I totally agree with you! We need to take some time to debug/review this stuff.\nLet's discuss this on IRC.\n. Hi Heiko,\nSorry, I made a mistake in Classifier.i and Classifier_includes.i (https://github.com/shogun-toolbox/shogun/pull/1222/files) and now modular interfaces don't work, but if i delete my changes, everything is ok. I'm newbie in SWIG and i'd like to hear your advice about cleanup, which you mentioned.\nThank you in advance,\nRoman Votyakov\n. @karlnapf please have a loot at this draft\n. Sure :) Debug log can be found here.\n. Ok, thanks :) \nI've found, that GPs train twice, when we call apply(test). And I explain why: \nWhen we call apply(test), we need to compute K(train, test), but when we get kernel instance from an inference class and compute K(train, test), hash of the parameters of the kernel changes. And this forces the inference class to call update() method, when we want to call get_something() methods.\nI think, if we'll use clone of the kernel instance to compute K(train, test), then we'll fix this problem.\n. Now kernel->clone() works fine, since #1892. I think this issue might be closed.\n. @karlnapf i'll check a little bit later.\nAnyway, thanks @lambday for the fix :)\n. Thanks for your comments :)\nI've found what is wrong with travis. I will fix this and other things tonight.\nGPs graphical example is faster by at least 6 sec on my machine :)\n. Yep. I run unit-tests and integration tests. Every integration test is OK on my machine.\n. covariance is not defined\n. Ah, sure\n. Yes, I agree with you, it will be more informative.\n. sure :)\n. oh, yeah, sure\n. yep, we need to specify function class to integrate it and this functions is used only in particular implementation of integrate_quadgk method. That is why i put it here. Probably it's suitable not to generate documentation for these functions, since they are useless for users.\n. yep, but i didn't find copy method for array and use set_array() and unfortunately it reallocates array memory every call. It would be great, if we have an array copy method: which uses previously allocated memory for storing new array, if length of new array < length of array, and reallocates array memory otherwise.\n. evaluate_quadgk is adapted form Octave (quadgk.m).\nintegrate_quadgk is based on ideas from Octave (quadgk.m), but a little bit different and sometimes it is faster and more accurate than quadgk.\nGauss-Kronrod nodes and weights were taken from original QUADPACK fortran implementation\n. Gauss-Hermite nodes and weights were taken form there: http://keisan.casio.com/exec/system/1329114617 \n. i don't understand a little bit, when it might be useful. If user want just to evaluate integral, he call \nintegrate_quadgk(function, a, b), if user want to specify integrate_quadgk behaviour, he set can set parameters.\n. Oh, i agree! i'll fix it\n. oops, i'll fix this\n. unused private field\n. copy itself\n. uninitialized variables open_flags, mmap_prot, mmap_flags\n. unused private fields\n. compare bool and K_CUSTOM\n. compare bool and K_CUSTOM\n. get_ get_ twice :)\n. i think gpc object is unnecessary here\n. i think there is a very little issue with spacing\n. no features X were mentioned in above formula\n. Yeah, i totally agree\n. unused variable\n. incorrect order of arguments\n. empty statement\n. empty statement\n. shadows outer cycle counter\n. shadows outer cycle counter\n. shadows outer variable\n. shadows outer variable\n. shadows inner cycle counter\n. shadows inner cycle counter\n. shadows outer variable\n. shadows outer variable\n. So maybe add something like ASSERT(features->update_parameter_hash()) or let it be like it is (i mean without this strange line)?\nI prefer without this line, since i didn't find the good reason why it should be there.\nAnd you?\n. Ok, i added ASSERT. But i think, that in future we have to move ASSERT-like functionality from examples to unit tests. It is more convenient to check where something don't work with unit tests rather than examples.\n. different description and parameter: \"sample\" and \"single_sample\"\n. Doxygen shouldn't generate proper documentation for this method, since description and declaration are different: \"num_bits\" and \"dim\". Also, please check doxygen warnings, there are some of them, that you can fix :)\n. Doxygen should print a warning there since declarations and descriptions are different: \"shifts\" and \"shift\", \"weights\" and \"weight\"\n. I've scaled kernel there\n. Great! i'm very happy, that this issue is resolved :) I was confused there a little bit, but now everything is Ok.\n. oops:)\n. How could we better do this?\nFor example, SGVector has already constructor like this (i mean with the same signature):\n/* constructor to create new vector in memory /\nSGVector(index_t len, bool ref_counting=true);\n. Yeah, i saw that, but sorry can't wait when it is done... Anyway we can remove (or replace) this in future.\n. If you want to disable some test temporally, you could add DISABLED to its name.\nIt looks like:\nTEST(RationalApproximation, DISABLED_compare_direct_vs_cocg_accuracy)\n{ \n... \n}\nAnd this test will be compiled, but not executed.\n. Yeah, you are right there. But we and GPML have different optimization techniques and the difference is big (something about 1e-1). I plan add parameter's comparison (but little bit later), when problem with constrained parameters (i.e. non negative, etc) is fully solved, since for example l-bfgs and some other optimization algorithms has very bad results.\n. I've quick fixed this example to show, how to use model selection with GPs. And I didn't think about predictive distribution plotting, etc. But ok, i'll add it :)\n. I agree :)\n. Oops, sorry, I forgot about CLock.\nI don't understand the second question. Could you please explain a little bit? What do you actually mean?\n. ah, sure :)\n. I'm very open for discussion, but i still have no ideas how to avoid this.\n. just replace with SG_DEBUG(\"entering\")?\n. I'm not sure, that attaching/detaching features from the kernel will not change parameter's hash. But ok, I'll try to do so.\nNope i didn't profile the memory footprint. How can i do this?\n. SG_DEBUG(\"entering\") \njust prints:\n\"[DEBUG] entering\"\n. So when we detach features from kernel, and then attach them again. We will compute kernel matrix again.\n. I mean, if we detach training features from training kernel and clone it, then when we attach them again to training kernel we will recompute training kernel matrix or not? I'm confused a little bit.\n. I'll do this in separate PR.\n. Hi, guys! \nI'm sorry for delayed answer (I'm very busy now). This implementation is just based on Octave's one (some things are the same and some things are different). So probably, we can remove this lines from a header if needed.\n. Why should this field be there? If it's used only in multiclass case, probably it should be somewhere else. I think, that it's not good. For instance: CExactInferenceMethod class is inherited from CInferenceMethod class, but this field means nothing for CExactInferenceMethod class and it's not used there.\n. I don't fully understand, why you have added m_alpha, m_L and m_E to parameters of the CInferenceMethod class.\n. Hi @karlnapf! Remember, I've fixed this problem :)\n. ",
    "ozansener": "I will write some unit tests; however, I want to first make sure patch passes build test. It gets an error from Travis https://travis-ci.org/shogun-toolbox/shogun/jobs/6859829 I think error is related to python example; however, it is not included in the patch.\n. ",
    "crunchiness": "Sorry about that, fixed.\n. ",
    "lambday": "tests for load and save methods of TParameter has been added to cover all primitive types for scalar. Tests for vector and matrix types of float and complex have also been added ( #1162 and #1163 )\n. Heiko,\nThanks. I added a few tests for matrices and vectors for float64_t and\ncomplex64_t. yes dimension-empty case is not there. I'll add soon.\nOn Sat, Jun 8, 2013 at 11:30 PM, Heiko Strathmann\nnotifications@github.comwrote:\n\nVery nice work! The next step would be matrices and vectors. We probably\nwill not need all types covered for this since the code calls the same\nmethods as the ones for which you added the tests.what is important though\nis to test borderline cases where eg. one dimension is empty.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/shogun-toolbox/shogun/issues/1160#issuecomment-19152756\n.\n\n\nRegards,\nRahul\n+918879426396\n. Another thing, currently it always shows the function name it prints from, like, say, in SGVector:;display_vector(), or when I start the cmdline interface etc, [shogun::Version::print_version(), shogun::CCmdLineInterface::print_prompt()]\n. @karlnapf have a look when you're around :)\n. @karlnapf sending the next one then :P\n. @karlnapf clone works just fine :) please have a look. Will add new unit-tests soon\n. @karlnapf @sonney2k @lisitsyn please have a look :)\n. checked using valgrind, heap usage (for 1 time creation)\neigen3 : 264 allocs, 264 frees, 258,255,714 bytes allocated\nshogun: 3,000,236 allocs, 3,000,236 frees, 160,038,038 bytes allocated\n. updated with dense dot\n. @karlnapf I'm making the changes. I'll soon add a bunch of small unit-tests to cover all cases. Will move to COCG after that\n. @tklein23 sorry I totally missed this thread. RationalApproximationCGMJob is now fixed by #1565. This is what I got from valgrind \"--tool=memcheck\" \"--leak-check=yes\" \"--show-reachable=yes\" ./tests/unit/shogun-unit-test \"--gtest_filter=RationalApproximationCGMJob\"\n==3727== Memcheck, a memory error detector\n==3727== Copyright (C) 2002-2012, and GNU GPL'd, by Julian Seward et al.\n==3727== Using Valgrind-3.9.0.SVN and LibVEX; rerun with -h for copyright info\n==3727== Command: ./tests/unit/shogun-unit-test --gtest_filter=RationalApproximationCGMJob\n==3727== \nNote: Google Test filter = RationalApproximationCGMJob\n[==========] Running 0 tests from 0 test cases.\n[==========] 0 tests from 0 test cases ran. (32 ms total)\n[  PASSED  ] 0 tests.\n==3727== \n==3727== HEAP SUMMARY:\n==3727==     in use at exit: 0 bytes in 0 blocks\n==3727==   total heap usage: 29,110 allocs, 29,110 frees, 1,888,529 bytes allocated\n==3727== \n==3727== All heap blocks were freed -- no leaks are possible\n==3727== \n==3727== For counts of detected and suppressed errors, rerun with: -v\n==3727== ERROR SUMMARY: 0 errors from 0 contexts (suppressed: 0 from 0)\n. @karlnapf added a few more things. Please have a look\n. @vigsterkr I don't need any function pointers... :)\n. @karlnapf some problem with swig. Added computation framework to modular interface this morning. I'm looking into it.\nOh and since I added operator* for SGSparseMatrix, some problem with swig is also there, its asking to rename it.\n. @karlnapf alright.. I didn't figure out what java modular was complaining about! I can't get java modular compiled on my machine (not sure what's wrong, tried with -DJavaModular=ON but it didn't find jblas it seems, which I have in /usr/share/java/).\nCMake Error at /usr/share/cmake/Modules/UseJava.cmake:536 (if):\n  if given arguments:\n\"jblas\" \"jblas-1.2.0\" \"STREQUAL\" \"\"\nUnknown arguments specified\nCall Stack (most recent call first):\n  CMakeLists.txt:757 (FIND_JAR)\n. @vigsterkr @karlnapf I think this fixes #1628 \n. @sonney2k I'll do this\n. I got a reply from the ARPREC developers few days back regarding an issue I reported to them (https://gist.github.com/lambday/6775791) which was causing an issue. Sherry Li writes \n\"In the following sin/cos function, \nvoid mp_real::mpcssn(const mp_real& a, const mp_real& pi, mp_real &x,\n            mp_real &y, int prec_words)\nThere are 2 static tables declared, which would be allocated from heap. They are static because they shouldn't need to be recomputed every time the function is called.   The tables are filled with pointers to heap memory.  They are filled lazily as needed.  The heap allocations is the \"still reachable\" memory that valgrind finds.  This will not result in memory leaks over time using sin/cos over and over.\"\nI guess its not harmful then.\n. @karlnapf @sonney2k please have a look!\n. update - not ready for merge! I have some errors viewing the compiled notebook!\n. @karlnapf @sonney2k @vigsterkr please have a look!\n. valgrind reveals that we have leak while using DynamicObjectArray of KernelPCA and VwRegressor, rest works fine!\n. @karlnapf this fixes issue #1821 \nThanks for merging! I initially wanted to add TParameter tests only, but had some troubles. I will surely add more tests soon!\n. @votjakovr my pleasure! The example you had works fine locally :)\n. @karlnapf I agree with the class in between then. I already pushed the renaming PR :D and its kinda big :( next one will be smaller for sure!\n. @vigsterkr @karlnapf I think storing last call's solution is a good idea. There may be cases when we need to use last solved result more than once. So keeping the compute methods void and adding getters for eigenvalues and eigenvectors from last compute call would do - then we don't need to think of fancy data structures for returning both at once :P I agree with the enum (which we should set with some default value - may be the best candidate) but hiding the API from backend is necessary. As a user I would absolutely love the global backend setters (makes it really flexible), but also it shouldn't bother me if all I care is to compute eigenvalue and I don't know delta about magma/eigen3/lapack :)\nOh and as I mentioned in IRC, these eigen solvers work for SPD matrices (as you can see, all eigenvalues we deal with here are real). So, we might wanna rethink regarding storage as well - make it generic.\n. compute_few() sounds really good :+1:  compute_extremal would then just be a wrapper for ease of use\n. Btw, we should really keep this API ready for python modular to try new things out using directors, once the backend is solid :) :)\n. @vigsterkr @karlnapf @sonney2k @lisitsyn on another note, how do we like CRS for sparse matrices instead of LIL? leads to faster matrix vector multiplication [https://github.com/lambday/shogun/blob/develop/benchmarks/sparse_test.cpp], suitable for iterative linear/eigen solvers - also easy compatibility with eigen3.\n. @sunil1337 it looks good and clean (easy to fix the compilation error). One reason I wanted get_dot_functor() to be lightweight (and hence to avoid checks and object creation/deletion inside it) is because it will be called every time we call apply(), which probably will be a lot of times. But object creation/deletion will happen only once as per this, when we need it for different data type/backend. I think we can use typeid instead in this case (which is a O(1) operation) as we don't need the inheritance pattern. Then only one void* dot_functor would do. Let's see what @karlnapf @lisitsyn @vigsterkr say about this.\n. @karlnapf @lisitsyn @vigsterkr I was trying to think of some other way to deal with multiple linear algebra backend and global linalg backend setter problem for shogun other than using a factory class. As per our previous discussion on this thread, we went ahead with the factory class idea and @sunil1337 sent some patches. But recently I wanted to add different backend for Matrix::sum methods in the linalg module and just out of curiosity I tried a preprocessor based approach for setting backends globally in compile time instead to see how much it costs to leave things up to virtual methods for these tasks. I ran a benchmark for dot products (using Eigen3) with float type vectors of size 1,000,000, 1000 times, to measure mean/var statistics of runtime, with both these approaches, and the results are interesting -\n(Intel(R) Core(TM) i5-3230M CPU @ 2.60GHz with 8GB DDR3 @ 1600MHz)\nmean 0.000607864618301, var 0.000000004135587 [runtime] \nmean 0.000045088291168, var 0.000000000016706 [compile time]\nRuntime based approach is roughly 13.5x slower which seemed to me pretty large penalty. The macro based approach can be used to set backends for different tasks, dot, sum, eigensolver via compiler options. Its a complete different way to go and the only problem I think is with swig. As far as I thought, moving these stuffs to shogun-external as a header only linalg library can be a nice way out. In shogun classes we'll be using that linalg library instead of Eigen3 like we're doing now. The drawback is losing runtime flexibility for switching backends but for testing purpose we can always try things out with the external library. In shogun we forget about backends and code away - things are taken care in cmake.\nWhat are your views on this? @karlnapf @lisitsyn @vigsterkr looking forward to your comments and suggestions.\n@sunil1337 please don't kill me :(\n. @vigsterkr yeah here is it in this gist (please ignore all the garbage :D). I didn't even think of counting the number of assembly operations :D but I guess that clearly should be considered as a last resort - depends on a hell of other things. I simply used shogun's CTime class. Could you please try to run there? (compile the first one with -DUSE_EIGEN3 - horrible names, should be changed).\n. @karlnapf I think this script would do\nbash\ng++ -O3 -std=c++0x -lshogun -I/usr/include/eigen3 dot_factory_benchmark.cpp\n./a.out\ng++ -O3 -std=c++0x -lshogun -I/usr/include/eigen3 -DUSE_EIGEN3 dot_macro_benchmark.cpp\n./a.out\n. @karlnapf An initial draft. Please have a look. I will add some unit-tests along with this PR but I'm thinking of having the values gathered from my c++ implementation because the original MATLAB implementation doesn't cover these cases. The existing tests pass (except integration-python_modular-tester-statistics_quadratic_time_mmd  - due to the implementation change in spectrum null samples, two rv ~ N(0,1) instead of 1, rest is same for m==n).\n. Addresses #1918 and #1903.\n. @karlnapf hi I just noticed that a test in quadratic time mmd may be testing wrong. Please check https://github.com/shogun-toolbox/shogun/blob/develop/tests/unit/statistics/QuadraticTimeMMD_unittest.cc#L398. It should be testing results for normal vs precomputed. Fixing..\n. @sunil1337 This looks good :) Few points that I commented on -\n- Do not inherit from CSGObject for these classes. They don't need to be (I think @karlnapf @lisitsyn @vigsterkr will agree). And also, we don't need to add 'C' before classname.\n- Double check unnecessary includes. In many cases it would be managed by forward declaration in the header.\n- Please double check newlines and whitespaces (check other classes and maintain the same style)\nAnother thing. I guess you could use one single unittest file for all dotproduct backends instead of making a new file for each one (but always use different tests for them).\n. @tklein23 fixed by #2180 :)\n. @tklein23 thanks for pointing this out man! :)\n. @tklein23 lol no no :D \n. @karlnapf absolutely. I will rebase after you merge this and then try to fix the bug in streaming MMD. I need to rewrite that anyway. \n. thanks @karlnapf. I will take a look at this soon! :)\n. @karlnapf so its not just the streaming MMD classes? I will keep track.\n. @karlnapf hey could you please check if unit-test 2 and 4 should have CList* list=new CList(); instead of CList* list=new CList(true); in the beginning? May be I am missing something here :-/\n. @karlnapf these are really helpful :)\n. @karlnapf umm okay! But I think the info and debug messages are really helpful. What do you think? \n. @tklein23 umm I think a simple check around the CList unit-tests would do. I guess its guarded by some macro (see even CSGObject has no member named ref_count). Let me check. Regarding the warnings, I'm on it. Thanks for pointing this out! :)\n. @karlnapf yeah I will add the sum operations in lin-alg framework. May be I can add when the linear algebra factory class is added for the other task? I can write that but may be Sunil has written that already! And secondly, sum operations would work on MatrixOperators, right? Not in general LinearOperator.\nRegarding the tests, I'm checking multiple blocks of different size but the same sum operation on all - one TEST per sum method, but called multiple times. May be its better to separate these cases.\n. @karlnapf Well what I was thinking about is something like this: we can have a CMatrixSum base class having all these sum methods abstract - in fact we can have templated Matrix so that it can work directly with SGMatrix, Eigen3::Matrix and other future GPU matrices we may have -\nc++\ntemplate<class Matrix> CMatrixSum::sum(Matrix m);\ntemplate<class Matrix> CMatrixSum::sum_block(Matrix m, index_t block_begin, ...);\n//...\nThen Eigen3MatrixSu, for example, derives from this and defines these methods. CMatrixSum would have a global instance in the CLinearAlgebra which would be initialized in init() and can be set globally via set_backend as the other case. CCustomKernel would then just use that CMatrixSum* to compute these things and get rid of any particular backend dependency. Do you think its a good idea?\nAs you suggested, I think these unit-tests would have to be changed later anyway when I add this sum operation in linalg. So is it okay if I change these things (cutting down size of unit-tests here) in a later patch instead? Travis RubyModular failure was unrelated. I actually want to finish updating QuadraticTimeMMD after this which is half finished.\n. @karlnapf thanks for merging. Actually you're right! Let me take care of the LinearAlgebra stuff in the next patch instead otherwise I'll tend to jump off to the next topic! :D \n. @karlnapf I also noticed that there's a failure due to accuracy in buildbot for MMDKernelSelectionMax.select_kernel_quadratic_time_mmd due to this patch.\n[ RUN      ] MMDKernelSelectionMax.select_kernel_quadratic_time_mmd\n/home/buildslave/precise_-_libshogun/build/tests/unit/statistics/MMDKernelSelectionMax_unittest.cc:82: Failure\nExpected: (CMath::abs(measures[1]-0.000291185913881)) <= (10E-15), actual: 1.11042e-14 vs 1e-14\n[  FAILED  ] MMDKernelSelectionMax.select_kernel_quadratic_time_mmd (83 ms)\nI'll fix this in the next patch!\n. @tklein23 this solves the issue you mentioned in #2180. The warnings are harmless since they are caused due to empty SG_REF/SG_UNREF definitions in SGRefObject.h when reference counting is not used. Its not just in CList but many many places. However I see many IO related failures while running ctest using the configuration you mentioned.\n. @tklein23 to be exact, these are the tests which failed locally using \ncmake -DUSE_REFERENCE_COUNTING=OFF -DENABLE_TESTING=ON -DCMAKE_BUILD_TYPE=Debug ..\nThe following tests FAILED:\n     51 - unit-SGSparseMatrix (Failed)\n    144 - unit-UAIFileTest (SEGFAULT)\n    146 - unit-MLDataHDF5File (Failed)\n    147 - unit-LibSVMFileTest (SEGFAULT)\n    148 - unit-CSVFileTest (Failed)\nErrors while running CTest\n. @karlnapf The parts that I needed is done! (not currently sure if we need methods for mean, if we feel that we should, I'll add those - shouldn't take more than an hour). Currently only added Eigen3 backend for these methods. It would be easy to extend this for future backends following how things work for dot already. \nAlso, while replacing sum methods in CustomKernel we should use explicitly set EIGEN3 backend as of now (otherwise compilation fails if someone sets ViennaCL, for example, as global default backend via cmake). I will send this patch to feature/linalg branch only.\nI am trying to gear up a bit and finish MMD ASAP. Meanwhile when I get time I'll try to get other stuffs included to linalg as well (@yorkerlin can help me a bit in this may be?)\n. @karlnapf thanks for reviewing :) There are a few ideas for blockwise sum operations -\n- I thought that it would rather be better if we could directly use the sum method with blocks (instead of having a separate sum_block method with 4 extra params). It was fine as long as it was local with custom kernels but while putting this in linalg lib I think we can do better. So I thought of having a class Block which can wrap the matrix and have these block specific information. So, while using these sum methods for blocks we can just use it simply as\nc++\nlinalg::sum(linalg::block(mat,0,0,4,4));\n// we can use this block for different tasks in fact\nWhen required, we could pass around these blocks instead of matrices to various linalg methods which handles them internally. I think this would keep the API simple to use.\n- we need sum of the squared coefficients as well. But its not cleaner to have a different squared_sum method for that. Rather we can have transform methods returning an intermediate matrix and use like\nc++\nlinalg::sum(linalg::square(block));\n- for row-wise/col-wise operation, I am confused. In eigen3 we can have complex one-liners like\nc++\nmatrix.colwise().array().square().sum()\nbut for us to be able to do that we gotta pretty much rewrite what eigen3 does to achieve this. So may be we just have different methods sum_colwise() etc. I am not sure!\nLet me know your thoughts on these!\n. @karlnapf I am being a bit brave here and merging this to move forward. Square patch on the way! Please let me know if there's something you don't like here.\n. @karlnapf merging since travis is green. this works with blocks as well :) rowwise/colwise sum patch on the way!\n. @karlnapf thanks for your comments :) travis build is fine except for gcc libshogun builds where it took too long to produce output for linear mmd tests because of the default block size being 4. Trying with higher block size.\nAnother thing, as I mentioned in mail, with this patch it was being really tough to incorporate deprecated implementation inside this code because the class structure is modified a lot. I couldn't thought of any nicer way to do this than just copying/pasting huge portions of the code and having deprecated methods for pretty much everything. So currently I made these changes in a feature branch. Will have to discuss a bit regarding this.\n. @karlnapf updated the libshogun example and travis is green! Merging.\n. @karlnapf thanks. Alright I'm trying to remove it - if things are sane then will send a PR. Btw do you think that with the way Q matrix will be computed now (within-block permutation approach on pooled samples, patch on the way), the previous optimization formula holds fine? Just wondering :)\n. Was just checking the paper. Thanks :)\n. @karlnapf on second observation, I think this is more because of the randomness of the samples. I tried increasing the number of samples and it works every time. So I'm using a seed and a num-blocks that gives reasonable results for the python modular tests.\n. @karlnapf well this is basically how you did it originally. I just figured that it can work with new model as well :) sending the data PR now.\n. @karlnapf thanks for merging :) I'll add a python modular example soon.\n. @karlnapf actually some other tests are failing locally as well. Anyway I regenerated the data. The values seem sensible. After I updated the data locally, the failure for this particular test seems to be gone.\nThe following tests FAILED:\n    115 - integration-python_modular-tester-multiclass_c45classifiertree_modular (Failed)\n    132 - integration-python_modular-tester-multiclass_chaidtree_modular (Failed)\n    138 - integration-python_modular-tester-multiclass_cartree_modular (Failed)\n    159 - integration-python_modular-tester-regression_chaidtree_modular (Failed)\n    164 - integration-python_modular-tester-multiclass_randomforest_modular (Failed)\n    238 - integration-python_modular-tester-regression_cartree_modular (Failed)\n    315 - integration-python_modular-tester-regression_randomforest_modular (Failed)\n    322 - integration-python_modular-tester-multiclass_id3classifiertree_modular (Failed)\nSending a data PR.\n. @karlnapf restarting the travis build! fingers crossed!\n. @karlnapf travis is green again :) merging! :)\n. On second thought, on buildbot, only the mmd tests were failing! So I'll wait for you to review this and then merge!\n. alright!\n. @karlnapf added two unit-tests for large number of samples as well.\n. @karlnapf should I rebase this branch against develop before it can be merged with develop?\n. @sejdino I added a unit-test for b-test with 50,000 samples. locally all unit tests for b-test take about 0.7 secs to run (8GB DDR3, intel core i5). I don't understand what's taking so long on travis :(\n. @vigsterkr still, 50 mins instead of ~0.7 secs!!! maybe its not the best idea to use such a large test as unit-test then :/\n. @karlnapf @sejdino I updated the DEPRECATED versions. Now it uses the same samples as of corrected versions for computing statistic and variance, just the normalizing constant is changed. For variance, samples are not shuffled when using DEPRECATED. Please have a look.\nI also tried out a graphical example for B-test as mentioned in #1937 in this gist. And the corresponding plots are interesting.\n\nThis example could go inside the notebook. I'll send a PR for that once this is merged. Please note that integration test will fail in this PR due to usage of corrected methods instead of DEPRECATED. I will update the data according to new methods when this is merged with develop.\n. @karlnapf @vigsterkr somehow we need to disable these large tests on travis :(\n. @karlnapf travis fail for integration test is expected. however, libshogun tests on travis are taking long with new version - locally the problematic tests take really less time\n```\n[lambday@lambday.iitb.ac.in build]$ ctest -I 623,624\nTest project /home/lambday/Downloads/shogun/build\n    Start 623: libshogun-statistics_linear_time_mmd\n1/2 Test #623: libshogun-statistics_linear_time_mmd ........   Passed    0.16 sec\n    Start 624: libshogun-statistics_mmd_kernel_selection\n2/2 Test #624: libshogun-statistics_mmd_kernel_selection ...   Passed    1.48 sec\n100% tests passed, 0 tests failed out of 2\nTotal Test time (real) =   1.81 sec\n```\nI am trying to figure out what might be taking so long on travis (!).\n. @karlnapf with previous implementation, on develop I get this result locally\n```\n[lambday@lambday.iitb.ac.in build]$ ctest -I 623,624\nTest project /home/lambday/Downloads/shogun/build\n    Start 623: libshogun-statistics_linear_time_mmd\n1/2 Test #623: libshogun-statistics_linear_time_mmd ........   Passed    0.16 sec\n    Start 624: libshogun-statistics_mmd_kernel_selection\n2/2 Test #624: libshogun-statistics_mmd_kernel_selection ...   Passed    1.37 sec\n100% tests passed, 0 tests failed out of 2\nTotal Test time (real) =   1.70 sec\n```\nThe extra time taken can be explained by the recomputation of eta_i after shuffling samples within current burst with new implementation.\n. @karlnapf with WITHIN_BLOCK_DIRECT things take longer\n```\n[lambday@lambday.iitb.ac.in build]$ ctest -I 592,593\nTest project /home/lambday/Downloads/shogun/build\n    Start 592: libshogun-statistics_linear_time_mmd\n1/2 Test #592: libshogun-statistics_linear_time_mmd ........   Passed    0.49 sec\n    Start 593: libshogun-statistics_mmd_kernel_selection\n2/2 Test #593: libshogun-statistics_mmd_kernel_selection ...   Passed    3.44 sec\n100% tests passed, 0 tests failed out of 2\nTotal Test time (real) =   4.09 sec\n```\nI am moving these two tests inside unit-tests as discussed on irc and will disable them if necessary for travis.\n. @karlnapf 2 mmd tests fail because we'll have to generate data according to the new implementation. we have old data as per develop against which we're checking.\n. @karlnapf since this is in a feature branch I guess we can merge this. later when merging this with develop I'll send PR for data then.\n. @karlnapf thanks for your comments. I'll update the PR fixing the issues you mentioned :)\n. @karlnapf I will use just one variable m_num_remove instead of two as you suggested. Also, I think I should set the default value of this to 1. So in case someone forgets to set any particular number to remove in each iteration, its always safe to remove 1 at a time. Will update this PR changing this soon.\n. @karlnapf updated the PR. Please check.\n. @khalednasr @karlnapf @lisitsyn \n1) I agree with passing preallocated vector/matrices in this case. With dot operation it didn't seem to matter but for methods returning matrix/vector it is indeed a big drawback if we had to return only CPU vectors.\n2) Nice idea regarding having the code to be able to run for both cpu/gpu without having to copy data :+1:  But we need to ensure that we provide a uniform interface for both the cases, no matter what class the final matrix/vector class is derived from.\n3) Eigen::Dynamic is fine. But I completely agree with @karlnapf that we do in fact need to be able to support a backend of a user's choice, not just Eigen3. So having purely native Eigen3 dependent code in shogun is a bit restrictive (in fact in many cases we'd want to use our own implementation of something - for example, CG-M solver for solving shifted family of linear systems that Eigen3 doesn't have). Although it might be long till we enrich linalg so much that it suffices all our needs but that should be our long term goal.\nSo I think we should first ensure that our templated-backend derived class idea works out because that's gonna be the backbone of the whole linalg. So its better to start with that and make sure that it works for both matrices and vectors. Then with those classes we change the existing sum and dot methods. Then we move on and add other modules in the similar fashion.\n. @khalednasr agreed. good idea. So I am merging this now. What's the next plans for linalg?\n. @khalednasr +1 from my side. I don't see any obvious issues with merging this with develop. @vigsterkr  we need opencl/viennacl installed in the buildbots for these methods to be properly tested.\n. thanks @vigsterkr. working on it.\n. I just configured without lapack and it still seems to work locally. seems like its not lapack issue.\n. @vigsterkr could you please give me some details about the bsd bot? config-wise? Is there any difference in the RNG?\n. Travis failure is due to accuracy in unit-test on osx.. fixing next..\n. @karlnapf my apologies for the long delay. Just came back from a trip home (festive season in India).\nWhile working with KernelManager I discovered some issues with my previous plan. Initially I thought that we could have used a templated kernel manager class\nc++\ntemplate <class Kernel> class KernelManager\nBut to use this in a non-template hypothesis test base class we'd have to use a specialized kernel which is restrictive. Also for independence test we may use two different type of kernels. Similar argument can be given for DataManager as well - for example, we can use real dense features for one (samples from one distribution) and string features for another (from another distribution) in an independence test. Using a DataManager templated on Features would therefore restrict such use-cases.\nA few way outs I could have thought of -\n- make hypothesis tests wrapper classes (subclasses of CSGObject) templated : but this would bloat up the binary size as we'd have to instantiate for each kernel-type and feature-type - so not an option.\n- the other idea is to choose the type-specific tasks at the time of setting up the features and kernel. This could be done by (1) making CHypothesisTestSubclass::set_*() (features or kernels) methods templated or (2) leave it up to the DataManager and KernelManager to handle this.\nI'm in favor of (2) because making our wrapper's setters templated would require that we have to send it by the actual type and not via base pointers. Then we cannot use this to instantiate it inside a polymorphic method (say, inside virtual CFeatures* CFeatureSelection::apply(CFeatures* feats) the type information for the actual feature type is lost - no way in compile time we can know).\nHaving all these in mind, I thought of the following structure - In one side we have CSGObject wrappers and on the other side there are internal DataManager, KernelManager and Computation policies. These two layers will communicate with each other via TestType - a test type is sort of a meta-info for choosing appropriate policies for a particular test. The wrappers would just have a concrete test type and the internals would be instantiated with that. So the basic structure would look something like\n``` c++\ntemplate \nclass CHypothesisTest : public CSGObject {\n    typedef TestType test_type;\n    ....\n    DataManager data_manager;\n};\nclass CTwoSampleTest : public CHypothesisTest {\n    typedef TwoSampleTest test_type;\n    ....\n};\nclass CStreamingTwoSampleTest : public CHypothesisTest {\n    typedef StreamingTwoSampleTest test_type;\n    ....\n};\nclass CKernelTwoSampleTest : public CTwoSampleTest {\n    typedef CTwoSampleTest::test_type test_type;\n    ....\n    KernelManager kernel_manager;\n};\n// and so on...\n```\nThis way the internal details are perfectly hidden from the wrappers and rest is left up to the DataManager and KernelManager. In the updated implementation for the DataManager I have\nc++\ntemplate <typename TestType>\nstruct DataManager {\n    template <typename Features>\n    void push_back(Features* feats);\n    ....\n    vector<CFeatures*> samples;\n};\nThe idea is - based on feature-type, we'd set fetcher policy and permutation policy and for that we'd rely on the fact that push_back is called from CHypothesisTest::set_p() or CHypothesisTest::set_q() with the actual type. Therefore we'd have a heterogeneous container for features as well as their corresponding fetching+permutation policy all set and ready to be used. Fetching policies can be used to fetch all the data (non-streaming) or block-wise (streaming) - which policy to use is all decided by the TestType - TwoSampleTest or StreamingTwoSampleTest (same for Independence...).\nCouple of other ideas -\n- It can also be possible to remove dependencies on CFeatures completely for hypothesis test by using a container of Any instead that @lisitsyn is using in aer. This will allow us to specify some other feature/kernel type and write a few policies and we'll have hypothesis test ready for that type - e.g. if someone uses Eigen::SparseMatrix for sparse feature class he'd be able to make the whole thing work with minimal effort (no change in existing code required - just addition).\n- In the setter methods I need to down cast to the actual feature type from CFeatures* type. Using the enums for features and kernel its easy but there can be a better way where I can use some de-mangling stuffs for that, so that things don't break even when some feature type doesn't have those enums. What I am looking for is some sort of REGISTER_FEATURE or REGISTER_KERNEL facility which stores the valid names inside some static string array and while casting we rely on the object's de-mangled names. @lisitsyn I need your input on this.\nI haven't chalked up the kernel manager yet but I suppose it can work similarly as of data manager. There are some linking issues with the current poc code that I'll try to get rid of before our next meeting. Again sorry for the long post - just noting down the things related to the design decisions so that I don't forget.\nHope to talk to you soon!\n. @karlnapf Hi, hope you had a nice holiday. I updated a working example on the POC code here.\n- Run make inside src/ \n- add src/ to LD_LIBRARY_PATH (for libflash.so)\n- Run make in root dir.\nSome more testing needed. Before our next meeting I will try to code up KernelManager - will be good for the discussion if we have a working example ready. Should next Monday be fine then?\n. @yorkerlin thanks for the notes. maybe we can discuss regarding this sometime. I should open an issue on this so that we can move our in-place related discussions there.\n. Good to go! Merging :)\n. @iglesias yes. The task is then to replace existing SGVector scale calls with linalg one.\n. as well as remove scale from SGVector once everything is replaced. Links to SGVector cleanup issue #2582 as well :) \n. @vigsterkr @lisitsyn @besser82 ?? \nI'll vote for pre-gsoc. Its redundant to write useful things from scratch when we have it there already in c++11. Of course we can write those on our own but \n(1) they need to be properly tested for bugs which is tricky sometimes.\n(2) students will feel more comfy with existing c++ stuffs.\n. @curiousguy13 I think this should go in linalg instead of CMath. range_fill can work for SGMatrix as well. Please have a look at linalg README\nThe idea is to add it to Core module in linalg. \n1. You add a NATIVE implementation (see Add.h) and add a convenient global function in modules (see Core.h). You can simply use std::iota in our code.\n2. Write a unit-test for that\n3. grep for range_fill in shogun and replace those with your newly added linalg::range_fill()\n4. Remove range_fill from SGVector.\n. Done. Closing.\n. Travis errors unrelated. Merging.\n. @yorkerlin @karlnapf hey man this seems really weird. I can reproduce this locally. The funny part is, if I replace SG with CGPU counterparts, things work perfectly fine. So, I thought that it must be the constructor CGPUMatrix<T>::CGPUMatrix(const SGMatrix<T>& cpu_mat) that gets called when we pass those SGMatrices to matrix_product. I put some debug texts there and I get this\n[DEBUG] Constructing a GPU matrix from a CPU matrix!\nGPU matrix=[\n[   0]\n]\n[DEBUG] Constructing a GPU matrix from a CPU matrix!\nGPU matrix=[\n[   8.57765000000000022],\n[   4.57765000000000022]\n]\n[DEBUG] Constructing a GPU matrix from a CPU matrix!\nGPU matrix=[\n[   8.57765000000000022,    4.57765000000000022]\n]\nmatrix=[\n[   0]\n]\n(params parsed right to left). So things do seem fine up to this point - don't know what part is messing it up. Gotta dig deeper for this.\n. Oh I see! It stores the result into a temporarily created GPU matrix. So res (which is SGMatrix) is still empty. We need to provide a version that returns the result matrix.\n. @yorkerlin back then we added the result parameter in those methods because CPU < --- > GPU data transfer is costly. But we do need the standard versions that would simply return the result. Whenever using a linalg method with global backend (i.e. not specifying any backend), we should use those only. Will ping you when I add that method. Thanks for pointing out this bug! :) \n. @yorkerlin @karlnapf Added the matrix product method as discussed in the PR #2802. The following code works fine in my machine after this patch.\n``` c++\ninclude \ninclude \ninclude \ninclude \nusing namespace shogun;\nvoid debug()\n{\nSGVector<float64_t> left(2);\nleft[0] = 8.57765;\nleft[1] = 4.57765;\nSGMatrix<float64_t> left_mat(left.vector, 1, left.vlen, false);\nSGMatrix<float64_t> right_mat(left.vector, left.vlen, 1, false);\nSGMatrix<float64_t> res = linalg::matrix_product<linalg::Backend::VIENNACL>(left_mat, right_mat);\nres.display_matrix();\n\n}\nint main()\n{\n    init_shogun_with_defaults();\n    sg_io->set_loglevel(MSG_DEBUG);\n    debug();\n    exit_shogun();\n    return 0;\n}\n```\nResult\n[lambday@lambday.iitb.ac.in shogun]$ g++ -std=c++11 bug.cpp -I/usr/include/eigen3 -lshogun -lOpenCL\n[lambday@lambday.iitb.ac.in shogun]$ ./a.out\nmatrix=[\n[   94.530959045000003]\n]\n. @yorkerlin @karlnapf I will add similar things for other linalg methods. I've got an idea though. I remember discussing something regarding this with Khaled. The key point is, even though this approach makes things work, but the back and forth CPU-GPU data transfer really kills the performance. You see what we're doing here is, we're copying two CPU matrices to the GPU, compute the result and storing it temporarily in the GPU memory and then again transferring it back to CPU SGMatrix. Imagine doing that over and over (maybe inside a loop) for some ~1000x1000 matrices.\nSo I was thinking how about we have some cmake switch to make all matrices and vectors GPU ones shogun-wise ? So as devs we'll simply forget about this SG/CGPU matrices/vectors - we'll write same code but that cmake switch will decide which memory it should use. That way we won't be compromising with the performance that we can milk from GPU.\n. I agree that this decision should be left to the programmer - choose a particular matrix type and a particular backend if you want anything specific. I was thinking from the use-case that @yorkerlin is doing here. Without any backend specified, these operations will make the CPU-GPU transfer for every linalg call when ViennaCL is global backend.\nThe way I was thinking of achieving this is to make SGMatrix inherit from either of a CPU/GPU matrix based on a preprocessor flag that will be set using cmake. So same SGMatrix will work for both the envs - same interface just the internals changed. But yes when eigen3 calls are used so frequently, then I'm not sure if this plan will do any good. I don't know if Eigen::Map thing works with GPU/ViennaCL memory array. Maybe it is worth looking at! :) \nProfiling is totally possible though.\n. @yorkerlin \n- Default is eigen3 for everything. So no problem there\n- The flag thing is possible. However, we'll be having to copy the data, CPU to/from GPU. See, the flag you mentioned can very well determine which backend is used for the operations, even at runtime (one can easily write if-else/switch with those with specified backend). But for properly using GPU, one better rely on using a CGPUMatrix instead of SGMatrix. We can however choose to use SGMatrix everywhere - but that is where all the copying will take place if the flag specifies ViennaCL. I'm not very sure how bad that would be. Maybe a benchmark would help. I'll do that :) \nSorry my comments aren't really your algorithm specific but more generic (more from linalg point of view). I should really spend some time knowing the stuffs :( \n. @yorkerlin sounds like a great idea! Not sure about matrix-inverse though. Never used that :D Heiko?\nOkay I'll do these benchmarks. Buying one day time.\n. @karlnapf travis error seems unrelated. Merging this one.\n. @yorkerlin Could you please check if this patch helps you with the failures in ViennaCL build? Just use\nc++\nSGMatrix<float64_t> C = linalg::matrix_product(A, B);\ninstead of\nc++\nlinalg::matrix_product(A, B, C);\n. @karlnapf @yorkerlin Example run on my machine \n```\n[lambday@lambday.iitb.ac.in benchmarks]$ g++ -O3 -std=c++11 matrix_product_benchmark.cpp -I/usr/include/eigen3 -lshogun -lhayai_main -lOpenCL -o benchmark\n[lambday@lambday.iitb.ac.in benchmarks]$ ./benchmark \n[==========] Running 4 benchmarks.\n[ RUN      ] SGMatrix.matrix_product_eigen3(data.A, data.B) (10 runs, 1000 iterations per run)\n[     DONE ] SGMatrix.matrix_product_eigen3(data.A, data.B) (1011.571000 ms)\n[   RUNS   ]        Average time: 101157.100 us\n                         Fastest: 97959.000 us (-3198.100 us / -3.162 %)\n                         Slowest: 118535.000 us (+17377.900 us / +17.179 %)\n         Average performance: 9.88561 runs/s\n            Best performance: 10.20835 runs/s (+0.32274 runs/s / +3.26473 %)\n           Worst performance: 8.43633 runs/s (-1.44929 runs/s / -14.66056 %)\n\n[ITERATIONS]        Average time: 101.157 us\n                         Fastest: 97.959 us (-3.198 us / -3.162 %)\n                         Slowest: 118.535 us (+17.378 us / +17.179 %)\n         Average performance: 9885.61357 iterations/s\n            Best performance: 10208.35247 iterations/s (+322.73891 iterations/s / +3.26473 %)\n           Worst performance: 8436.32682 iterations/s (-1449.28674 iterations/s / -14.66056 %)\n\n[ RUN      ] SGMatrix.matrix_product_viennacl(data.A, data.B) (10 runs, 1000 iterations per run)\n[     DONE ] SGMatrix.matrix_product_viennacl(data.A, data.B) (12463.535000 ms)\n[   RUNS   ]        Average time: 1246353.500 us\n                         Fastest: 1197524.000 us (-48829.500 us / -3.918 %)\n                         Slowest: 1597167.000 us (+350813.500 us / +28.147 %)\n         Average performance: 0.80234 runs/s\n            Best performance: 0.83506 runs/s (+0.03272 runs/s / +4.07754 %)\n           Worst performance: 0.62611 runs/s (-0.17623 runs/s / -21.96474 %)\n\n[ITERATIONS]        Average time: 1246.353 us\n                         Fastest: 1197.524 us (-48.830 us / -3.918 %)\n                         Slowest: 1597.167 us (+350.813 us / +28.147 %)\n         Average performance: 802.34059 iterations/s\n            Best performance: 835.05633 iterations/s (+32.71574 iterations/s / +4.07754 %)\n           Worst performance: 626.10860 iterations/s (-176.23198 iterations/s / -21.96474 %)\n\n[ RUN      ] CGPUMatrix.matrix_product_eigen3(data.Av, data.Bv) (10 runs, 1000 iterations per run)\n[     DONE ] CGPUMatrix.matrix_product_eigen3(data.Av, data.Bv) (1439.195000 ms)\n[   RUNS   ]        Average time: 143919.500 us\n                         Fastest: 141417.000 us (-2502.500 us / -1.739 %)\n                         Slowest: 149050.000 us (+5130.500 us / +3.565 %)\n         Average performance: 6.94833 runs/s\n            Best performance: 7.07129 runs/s (+0.12296 runs/s / +1.76959 %)\n           Worst performance: 6.70916 runs/s (-0.23917 runs/s / -3.44213 %)\n\n[ITERATIONS]        Average time: 143.919 us\n                         Fastest: 141.417 us (-2.502 us / -1.739 %)\n                         Slowest: 149.050 us (+5.131 us / +3.565 %)\n         Average performance: 6948.32875 iterations/s\n            Best performance: 7071.28563 iterations/s (+122.95688 iterations/s / +1.76959 %)\n           Worst performance: 6709.15800 iterations/s (-239.17075 iterations/s / -3.44213 %)\n\n[ RUN      ] CGPUMatrix.matrix_product_viennacl(data.Av, data.Bv) (10 runs, 1000 iterations per run)\n[     DONE ] CGPUMatrix.matrix_product_viennacl(data.Av, data.Bv) (87.753000 ms)\n[   RUNS   ]        Average time: 8775.300 us\n                         Fastest: 6016.000 us (-2759.300 us / -31.444 %)\n                         Slowest: 13262.000 us (+4486.700 us / +51.129 %)\n         Average performance: 113.95622 runs/s\n            Best performance: 166.22340 runs/s (+52.26719 runs/s / +45.86602 %)\n           Worst performance: 75.40341 runs/s (-38.55281 runs/s / -33.83125 %)\n\n[ITERATIONS]        Average time: 8.775 us\n                         Fastest: 6.016 us (-2.759 us / -31.444 %)\n                         Slowest: 13.262 us (+4.487 us / +51.129 %)\n         Average performance: 113956.21802 iterations/s\n            Best performance: 166223.40426 iterations/s (+52267.18623 iterations/s / +45.86602 %)\n           Worst performance: 75403.40823 iterations/s (-38552.80979 iterations/s / -33.83125 %)\n\n[==========] Ran 4 benchmarks.\n```\n. Graphics card details\ndescription: VGA compatible controller\n             product: 3rd Gen Core processor Graphics Controller\n             vendor: Intel Corporation\n             physical id: 2\n             bus info: pci@0000:00:02.0\n             version: 09\n             width: 64 bits\n             clock: 33MHz\n             capabilities: msi pm vga_controller bus_master cap_list rom\n             configuration: driver=i915 latency=0\n             resources: irq:27 memory:f0000000-f03fffff memory:e0000000-efffffff ioport:5000(size=64)\n@yorkerlin could you please test this on your machine as well?\nThe result looks really interesting. Lot to discuss. Going to work now - will be back in the evening.\n. @karlnapf @lisitsyn please have a look!\n. The result for the benchmark which uses the same operation that Esben used in #2818. \n```\n[==========] Running 3 benchmarks.\n[ RUN      ] SGMatrix.elementwise (10 runs, 1000 iterations per run)\n[     DONE ] SGMatrix.elementwise (226079.371000 ms)\n[   RUNS   ]        Average time: 22607937.100 us\n                         Fastest: 21976420.000 us (-631517.100 us / -2.793 %)\n                         Slowest: 24043383.000 us (+1435445.900 us / +6.349 %)\n         Average performance: 0.04423 runs/s\n            Best performance: 0.04550 runs/s (+0.00127 runs/s / +2.87361 %)\n           Worst performance: 0.04159 runs/s (-0.00264 runs/s / -5.97023 %)\n\n[ITERATIONS]        Average time: 22607.937 us\n                         Fastest: 21976.420 us (-631.517 us / -2.793 %)\n                         Slowest: 24043.383 us (+1435.446 us / +6.349 %)\n         Average performance: 44.23225 iterations/s\n            Best performance: 45.50332 iterations/s (+1.27106 iterations/s / +2.87361 %)\n           Worst performance: 41.59148 iterations/s (-2.64077 iterations/s / -5.97023 %)\n\n[ RUN      ] SGMatrix.loop (10 runs, 1000 iterations per run)\n[     DONE ] SGMatrix.loop (539029.217000 ms)\n[   RUNS   ]        Average time: 53902921.700 us\n                         Fastest: 53814866.000 us (-88055.700 us / -0.163 %)\n                         Slowest: 54179313.000 us (+276391.300 us / +0.513 %)\n         Average performance: 0.01855 runs/s\n            Best performance: 0.01858 runs/s (+0.00003 runs/s / +0.16363 %)\n           Worst performance: 0.01846 runs/s (-0.00009 runs/s / -0.51014 %)\n\n[ITERATIONS]        Average time: 53902.922 us\n                         Fastest: 53814.866 us (-88.056 us / -0.163 %)\n                         Slowest: 54179.313 us (+276.391 us / +0.513 %)\n         Average performance: 18.55187 iterations/s\n            Best performance: 18.58223 iterations/s (+0.03036 iterations/s / +0.16363 %)\n           Worst performance: 18.45723 iterations/s (-0.09464 iterations/s / -0.51014 %)\n\n[ RUN      ] CGPUMatrix.elementwise (10 runs, 1000 iterations per run)\n[     DONE ] CGPUMatrix.elementwise (4143.123000 ms)\n[   RUNS   ]        Average time: 414312.300 us\n                         Fastest: 119611.000 us (-294701.300 us / -71.130 %)\n                         Slowest: 487104.000 us (+72791.700 us / +17.569 %)\n         Average performance: 2.41364 runs/s\n            Best performance: 8.36044 runs/s (+5.94680 runs/s / +246.38311 %)\n           Worst performance: 2.05295 runs/s (-0.36069 runs/s / -14.94377 %)\n\n[ITERATIONS]        Average time: 414.312 us\n                         Fastest: 119.611 us (-294.701 us / -71.130 %)\n                         Slowest: 487.104 us (+72.792 us / +17.569 %)\n         Average performance: 2413.63821 iterations/s\n            Best performance: 8360.43508 iterations/s (+5946.79686 iterations/s / +246.38311 %)\n           Worst performance: 2052.94968 iterations/s (-360.68854 iterations/s / -14.94377 %)\n\n[==========] Ran 3 benchmarks.\n``\n. @yorkerlin from the requirements, going by policy based design for the parameters sound reasonable to me, where the policies are different optimization policies. So laying down the requirements\n- We can useParameterorPropertyclass for normal/fixed parameters. PresentTParameterandParametercan be an inspiration for that. Getting rid of theADDthings would be nice. We don't have to specifyMS_AVAILABLEorMS_NOT_AVAILABLEevery time.\n- We need anotherModelParameterclass for parameters that participate in model selection. This can be a policy based class with default policy being the present model selection scenario. For variational parameters we'll use a separate policy implementing the optimization technique for them as required.ModelParameterwould haveenable/disablefeature. In general I like policies more since they go well with combinatorial scenario without having to enforce a _is-a-relationship_ .\n- We'll usepimpl[1][2] pattern the classes which helps the software be backward compatible and also reduce significant compilation time. Header inclusion should be minimal and we have to take that extra care. All the data would go inside theImplclasses and eventually will be saved inside theparameter_mapinSGObjectImpl`. Getting rid of unnecessary setters/getters would also be nice [3][4][5]\n[1] http://www.gotw.ca/publications/mill04.htm\n[2] http://www.gotw.ca/publications/mill05.htm\n[3] http://stackoverflow.com/questions/8447972/how-to-combine-auto-gettersetter-with-pimpl-design-pattern-in-a-public-api-inte#\n[4] http://www.idinews.com/quasiClass.pdf\n[5] http://www.javaworld.com/article/2073723/core-java/why-getter-and-setter-methods-are-evil.html\n. @karlnapf @iglesias @lisitsyn I am planning to make some changes aimed at reducing compilation time. This was on my list for a long time. \n. @lisitsyn it's a start.. Removing unused things that unnecessarily take up compiler's time could be useful at least. Next patches would include usage of pimpl and getting rid of all but mandatory header inclusions in the .h files and relying on forward declaration in most of the cases.\n. Alright. Travis errors seem unrelated. Merging this one.\n. LGTM :)\n. Added.. Please check.\n. Thanks Heiko, Sergey. Will look into the noalias. One more question. What\nwould be an efficient way to initialize a SparseMatrix (eigen3) from\nSGSparseMatrix? I was writing the log determinant for the sparse matrix\ncase, and was considering taking a SGSparseMatrix as input.\nOn Sat, Mar 30, 2013 at 1:00 AM, Sergey Lisitsyn\nnotifications@github.comwrote:\n\nIn tests/unit/mathematics/Statistics_unittest.cc:\n\n\n\n/* the cholesky decomposition gives m = L.L', where\n* L = [(2, 0, 0), (6, 1, 0), (-8, 5, 3)].\n* 2 * (log(2) + log(1) + log(3)) = 3.58352\n*/\nEXPECT_FLOAT_EQ(CStatistics::log_det(m), 3.58352);\n  +\n// TEST 2\n// create random positive definite symmetric matrix\nsize = 100;\nMatrixXd A(size, size); // random matrix\nMatrixXd M(size, size);\n  +\n// to make all the values between 0, 1\nA = MatrixXd::Random(size,size) * 0.5 + MatrixXd::Constant(size,size,0.5);\nM = A * A.transpose();  // positive definite matrix\n\n\nConsider using noalias() next time\nhttp://eigen.tuxfamily.org/dox/TopicWritingEfficientProductExpression.html\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/shogun-toolbox/shogun/pull/956/files#r3590345\n.\n\n\nRegards,\nRahul\n+918879426396\n. Sergey, thanks. That's what I am trying. I think it would be nice if we\nhave a toEigenSparseMatrix method in the SGSparseMatrix class itself :D\nOn Sat, Mar 30, 2013 at 9:34 PM, Sergey Lisitsyn\nnotifications@github.comwrote:\n\nIn tests/unit/mathematics/Statistics_unittest.cc:\n\n\n\n/* the cholesky decomposition gives m = L.L', where\n* L = [(2, 0, 0), (6, 1, 0), (-8, 5, 3)].\n* 2 * (log(2) + log(1) + log(3)) = 3.58352\n*/\nEXPECT_FLOAT_EQ(CStatistics::log_det(m), 3.58352);\n  +\n// TEST 2\n// create random positive definite symmetric matrix\nsize = 100;\nMatrixXd A(size, size); // random matrix\nMatrixXd M(size, size);\n  +\n// to make all the values between 0, 1\nA = MatrixXd::Random(size,size) * 0.5 + MatrixXd::Constant(size,size,0.5);\nM = A * A.transpose();  // positive definite matrix\n\n\nThe most efficient is to collect a vector of triplets (i,j,value) and use\nthe setFromTriplets method. It would not work with eigen older than 3.1.0\nthough.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/shogun-toolbox/shogun/pull/956/files#r3594835\n.\n\n\nRegards,\nRahul\n+918879426396\n. @lisitsyn wow. I'm gonna add this one then. Thanks man!\n. @karlnapf its about the matrix multiplication. Sergey said something about using noalias in matrix multiplication in the log_det method, which I needed to check. Thought I'd check and add later :)\n. @karlnapf I changed it to dimxN SGMatrix, but eigen's transposeInPlace after mapping to eigen is not working (Assertion `rows() == other.rows() && cols() == other.cols()' failed). So I have to use SGMatrix::transpose_matrix(..) instead. Plus, after computing I need to return a Nxdim instead, because if we had dimxN, this would produce a NxN covariance matrix when we'll try to compute empirical covariance using CStatistics::covariance_matrix(..).\nShould I just use Nxdim then?\nI added some ASSERTs too.\n. @karlnapf  will fix it in another PR, to keep things separate.\n. Ah, awesome! I should have thought of that. Fixing very soon :)\n. @karlnapf okay\n. @karlnapf since its template class, the compiler complains about types. Resolving it this way seemed easier. :)\n. Okay. I'm editing.\n. @vigsterkr yes. I just checked again. it compiles fine on my system. anything wrong?\n. gcc version 4.6.3 20120306 (Red Hat 4.6.3-2) (GCC). shall I make it const then?\n. @sonney2k okay. thanks. changing it asap.\n. @karlnapf @sonney2k but when I try using something like this\nSGSparseVector *vec=SG_MALLOC(SGSparseVector, size);\nI get memory leak, but when I do this\nSGSparseVector vec[size];\nits alright. :(\n. @sonney2k okay. yes it works. I thought its managed internally when we use SG_MALLOC. sorry for the mess :(\n. @karlnapf I didn't add the extended SimplicialCholesky in the eigen3.h yet. We only need to handle it this way when we need to access the upper and lower factors. Shall I add this to eigen3.h too?\n. @karlnapf I took a different matrix, the structure that shows the most fill-in without permutation when using cholesky. Please check\n. @karlnapf okay. yeah actually that would totally save writing version specific code. Will add it in the next PR :)\n. @karlnapf okay :)\n. @karlnapf okay :) will change it in the next PR\n. @karlnapf credit goes to krylstat :)\n. @karlnapf I haven't added support for SGString, that's why kept anything related to SGString this way. Shall I add it?\n. @karlnapf H5T_NATIVE doesn't have support for complex numbers, that's why kept it this way.\n. @karlnapf I wasn't sure about this. How should I represent complex numbers for Json? Kept it unimplemented for the time being.\n. @karlnapf I added only SGVector, SGMatrix, SGSparseVector and SGSparseMatrix support for complex64_t. Shall I add strings too?\n. @karlnapf @sonney2k I initially kept PT_COMPLEX64=14, but then changed it. Would it create problem somewhere?\n. I should probably change the message then?\n. @karlnapf @sonney2k okay. I'm checking.\n. @karlnapf @sonney2k should I then just change it back to 14?\n. @karlnapf @sonney2k shouldn't it be sinh(x) instead?\n. @karlnapf here it will do coloring and stuffs for the probing case\n. ya I should change it..\n. @karlnapf calling eigenvalues() just returns the values, eigenvalues are computed in the constructor only\n. @karlnapf tested with LLT and LDLT as well, don't seem to work for complex matrices. Tested with a small non full-rank matrix as well but failed in that too! :(\nIf LLT doesn't work for complex at all then should we remove it from the option totally?\n. @karlnapf sorry I didn't get it. LLT works for this matrix. I tested with another symmetric matrix whose diagonal had real coeffs but non-diagonal elements had imaginary coeffs too. LLT failed in that (and LDLT too). (and they did not raise any NumericalIssue in that case, just plain failed!).\nI fixed the solver type as complex cause template may cause unnecessary overhead in compile time, because we only need this for complex, so.. :-/ I'm confused!\n. yes :) I was really excited last night when this worked! :)\n. Alright, I'll add another constructor then, may be?\n. Shall I change it and remove the copy constructor? \n. oops! I think I used some other places also! will remove it.\n. @karlnapf I totally forgot to add that when I was adding complex64_t\n. @karlnapf new_sgserializable was returning NULL causing all the trouble because earlier\n- I made get_name() return the class name as \"CClassName\" instead of just \"ClassName\", so it didn't find it in the class list\n- for complex, I added NULL (in class_list.py) because few overloaded operators were not available\n- I didn't set the type generic for those\n. @karlnapf had to do it this way cause of a c++ issue I wasn't aware of :( that's why SG_ADD doesn't work and so doesn't IO stuff\n. @karlnapf Had to make it non-const cause I had to register it for cloning\n. @lisitsyn okay, I'll remove it. But this result looks reasonable, no? Earlier I was taking num_vectors=num_cols, so got confused. I guess this is what @sonney2k was talking about.\n. @karlnapf made this change while trying clone, it was giving uninilialized memory error in this line -\nhttps://github.com/shogun-toolbox/shogun/blob/develop/src/shogun/base/Parameter.cpp#L3748\n(although its just in the debug message)\nclone worked (as in, gave same results) but got some errors while unref-ing the cloned one. will work on it\n. @karlnapf okay I'll change it back then :D I'll test and submit the bug report. This error was only because of debug messages anyway :) I'll try to think of something, but currently I'm also too occupied with insti stuffs :(\n. @karlnapf undo-ing my previous change\n. @karlnapf actually, to get rid of compilation errors. since I added complex64_t support for this class (which was intended for making SGSparseMatrix easier to form in unit-tests, also @sonney2k said its good to have datatype support for all of these :D but didn't work though :( I'll check why), it was giving error complaining that it doesn't know how to use != operator for complex and int.\n. umm.. yes, but this gives us a bit more flexibility. May be it will be useful later for some other purpose :-/\n. alright.. I'll change these in next PR\n. Yes.. I actually followed the same format that was used for earlier definitions of dense_dot. I'll change these to use SGVector\n. hehe :D \n. @iglesias thanks for pointing this out, man :)\n. @karlnapf @sonney2k adding these again, since before I removed them it worked.\n. @karlnapf worst I got from running this 100,000 times was 1.9944846621\n. @lisitsyn no its for complex double :)\n. @karlnapf is it okay to use std::vector internally? I could have used SGVector instead but the length of this vector may or may not be equal to (although not exceeding) m_max_iteration_limit, and also we need two copies of these vectors. Seemed easier this way.\n. @karlnapf calling it twice for min and max, calling it once to get all the eigenvalues would do too, but this takes lesser time!\n. @karlnapf um yeah I can add that.. but that's already checked inside subsequent calls, e.g. solve_shifted_weighted. but yes its better to quit early if something is wrong.\n. ya I should change this.. adding a real comparison in next PR\n. @votjakovr err...silly! thanks man! :)\n. @karlnapf this was satisfactory :) CG-M gives one step convergence for basis vectors and we get equally impressive accuracy as that of direct solvers. will add more tests.\n. @karlnapf err yes that was a debug thing.. I will remove it...\n. @vigsterkr okay I am changing... but this is the source of the error?\n. @karlnapf okay... I'll change it\n. @karlnapf I noticed that Lanczos breaks when min eigenvalue is too close to 0. I would add a warning there.\n. @karlnapf I don't think we can manage more than 1E-3 if we don't use high precision stuffs. Currently arprec related things are not there in the cmake I guess.\n. @karlnapf two samples are not equal. I earlier made a mistake in naming which was confusing. Changed probing vector to coloring vector.\n. @karlnapf yeah I saw.. replaced them by accuracy :)\n. @karlnapf yeah it was quite painful :( easy to write tests now :)\n. @karlnapf then I guess its better to check for lapack, eigen3 and colpack all, right? these are all the hard dependencies we have!\n. @sonney2k what might be the reason that fixed this? I am still clueless :-/\n. @sonney2k weird that when I tried constructing a SGSparseMatrix using the earlier approach separately, it did not report any memory errors!\n. @karlnapf I used the iteration count as index of the SGVector that stores the residuals. using int64_t it was giving a stupid warning :(\n. @karlnapf yes that would be helpful. I will add :)\n. @sonney2k I meant as index! sorry for the confusion!\n. @karlnapf I remember you telling me not to use SGString.\n. @sonney2k oh I thought that was not needed when we use SGVector based one.. adding..\n. @sonney2k @karlnapf then replacing the SGVectors with non SG ones would be a safer idea?\n. @karlnapf based on the discussion with @sonney2k at irc I currently changed it back to the old one for now. I will work on handling it a cleaner way as he suggested and clean up these when that's done!\n. @karlnapf I initially thought of pushing the duplicated part into the base TestStatistic, unless there are other possible implementations of this base for which it would absolutely make no sense! What do you think?\n. @karlnapf I wasn't sure. Did the pretty function thing work?\n. don't worry I will clean these up :)\n. argh! it was meant to be num_permutation_iteration.. but I like the num_null_samples more, specially since for linear mmd we don't permute things.\n. @karlnapf yeah you're right - we don't need to store this.. just kept this for convenience - since for HSIC we need equal number of samples and all... removing it altogether would be better for future change I suppose!\n. I think the default should be probing sampler - so you could add a HAVE_COLPACK check on this constructor :) or may be around this line, in absence of which you initialize things with normal sampler.\nif possible, please add an info message regarding the default setting it uses, could be useful.\noh and normal sampler doesn't take these set of args :)\n. could you please document the parameters for doxygen and take care of whitespaces properly? :)\n. @karlnapf @vigsterkr ah I should have removed these. fixed in #1960.\n. err.. COLPACK :P\n. SG_REF has to be done for both the cases. I would put it after the endif.\n. you can use the get_name() methods of the classes to avoid writing it twice :)\n. @karlnapf should we use this default for only linear time MMD? for B-test, things would be different I suppose.\n. @karlnapf umm.. no. I think I should just put a warning here (and for the set_p_and_q) and return NULL (or do nothing for the other) instead of error. I just copied it from LinearTimeMMD :D\n. @karlnapf umm.. then we have to do this in the subclass anyway - both of them. I guess its better to put it here.\n. @karlnapf mostly whitespace stuffs in this file.\n. @karlnapf updated the documentation. Please check if the biased case is corrent.\n. @karlnapf might come handy for B-test (depends, though, on what they say!)\n. @karlnapf yeah I am adding this.\n. That would be great! I could easily have used our DirectEigenSolver (with convenient constructor, may be) if we had the interface for retrieving all eigenvalues instead of min/max. I will update this when that's ready :)\n. @tklein23 This was causing issues so I changed this. Things work locally.\n. I think this link should be against likelihood estimation (http://en.wikipedia.org/wiki/Maximum_likelihood) instead as @karlnapf mentioned in comment.\n. @karlnapf yes I should have separated these two. Makes it easier to find what's exactly not working.\n. @karlnapf umm.. since its just testing if everything is same with precomputed kernel.. as long as the other methods are individually tested, I think we can just have it this way here? I am not sure!\n. @sunil1337 I realize that its an irritating comment but you could have skipped the first space after '<' and used a space after the comma, like -\nCLinearOperator, SGVector >;\n(the last one you need).\n. @sunil1337 you'll need to update the documentation as well. With this change, this is no longer true :)\n. We do not need to inherit from CSGObject for this (and hence, do not make these classes 'C' classes - shouldn't start with C. See IterativeSolverIterator class for example). These are functor like classes with just compute method and I don't think should be made heavyweight.\n. Whitespace not needed.\n. Please add a newline\n. We don't need this :)\n. These are also not needed. In case something causes memory leak these come handy. But since we'll only have one instance it won't help much I think\n. Please add a newline\n. Please add newline\n. Not required if not derived from CSGObject\n. totally agree. Please use REQUIRE.\n. Please do not include headers that are not absolutely necessary. Use forward declarations.\n. @karlnapf Earlier build failed on travis with gcc because this was omp atomic update. I don't understand because it was compiling fine locally (gcc-4.8.2). @lisitsyn @vigsterkr any clue?\n. @karlnapf its already doing this in cleanup_custom().\n. @karlnapf yeah I am using newly added CMath::fequals() only :) So I guess all the checks in unit-tests are not needed. I just wanted to the symmetry test is working for different data types. Maybe Infty case isn't really needed.\n. @karlnapf ah now I see what you mean. I'll add the tests right now.\n. @karlnapf ah that's what I did but then read your comment once again - you suggested to check for really large/really small new values. I think its better to check old_val+2epsilon.\n. we can define it inside shogun namespace then. that's better in fact.\n. true. I'll take care of this.\n. yes. Its a bit longer but we won't ever have to type while using this API :D \n. @karlnapf well, I am not sure about this. I'll do some checks later.\n. the speedup would at max be 2x, unless caching kicks in. I'll check if this differs with super huge matrices.\n. This is because as of now MMD2_GAMMA works with previous deprecated implementation only (returning mMMD^2).\n. avoids the diag entries.\n. avoids the diag entries.\n. includes the diag entries.\n. Didn't require to copy into separate vectors. But it makes it easier for the later part of the code to read and debug. Let me know if its okay.\n. computes kernel values only once for doing both statistic and variance estimation at once.\n. includes diag entries.\n. avoids diag entries.\n. avoids diag entries.\n. return value is handled here instead for both these cases.\n. p-value remains same for both present and deprecated versions :)\n. had to use DEPRECATED here because gamma still doesn't work with new formula.\n. This is required here I think.\n. License missing I think. Please have a look at src/statistics/QuadraticTimeMMD.cpp.\n. same here.\n. Please do not use __ before this. Use something like CV2_FEATURES_FACTORY_H_. Makes it more readable.\n. If the body is empty you could have just put it here in the header I suppose.\n. Is this class going to be inherited? Just asking :)\n. Please do not use space between = and operands.\n. Not commenting on this tiny formatting thingi for rest of the code :)\n. License. Please take care of this in other files as well :)\n. No leading underscore. Please take care of this pt for the rest :)\n. I am not sure here, but do you think you could use the enum for PTypes here that we already have? and please avoid method body in the same line. Have it like\nc++\nstatic const int get()\n{\n    return PT_UINT8; // if you think you can use that\n}\n. you really sure about turning refcounting off here? Just asking :)\n. For multi-line for or if-else, please use braces :)\n. Also, no space between i and < and < and nRows. and could you please use num_rows instead of nRows or something like that? No CamelCase I mean :)\n. Same as mentioned in previous comment. using the enum here would be cleaner I think instead of using magic numbers :)\n. License.\n. Please take care of the indentation.\n. Just wondering, why you used the name like cvMatiii2?\n. braces.\n. indentation.\n. @karlnapf had to remove these checks because CCombinedKernel creates two new CCombinedFeatures object if other feature objs are provided. So even though those are same, lhs_equals_rhs flag is set as false in base CKernel::init call.\n. @karlnapf I could have used an instance of CQuadraticTimeMMD here for computing these as we discussed earlier. But that computes variance as well after latest revision, which is not required here. So I copied these lines. Not sure if that's okay.\n. Relying on eigen3 for matrix-matrix multiplication stuff. Will replace later when this gets inside linalg.\n. @karlnapf @sejdino please check if this formula is ok.\n. @sejdino ah thanks I got it. I already did this in compute_statistic_and_variance() but wasn't sure about computing Q here. I think we can just use the same strategy here - first compute the statistic on the pooled data and then randomly split that data into p and q in the same proportions (can be done via subsets similar to what I did for compute_statistic_and_variance() which adds a subset of randomly shuffled indices to access whole data from - then first B_x indices are considered to be from p and rest B_y are considered to be from q - mimics the effect of splitting it in the same proportions after random shuffling) and recompute these h terms for covariance.\nI'll send a patch for this next. Thanks a lot for your comment :)\n. well, it also depends on the blocksize as in - say if blocksize 250 is used for B-tests, then 250x1000x2 features can be a bit overhead :/\n. @karlnapf ah yes! I missed that! changing.\n. @karlnapf I think PRETTY_FUNCTION thing will take care of this class::method() thing, right? So a SG_WARNING should suffice.\n. @karlnapf mmh... you mean creating a new feature object with this matrix?\n. okay I'll take care of this!\n. as in, if apply_to_string_features() fails, we say with a NULL return that preprocessing failed! but I just checked that currently all existing string preprocessors just return true in all of these! So I think now there is no point to have the NULL thing.\n. oh I see your point. with get_name() we'll be able to generate proper warning messages for the subclasses as well!\n. @karlnapf @sejdino this should be m/2*MMD^2 I think!\n. this would be sB/4 instead! changing!\n. @karlnapf yeah. It anyway just wraps the same feature matrix around! But I'll check if I can manage doing it otherwise.\n. umm okay! actually I'm just checking the max and min here... but there may be other values which violate the acceptable range as well. So couldn't think of a proper error message. But I guess just printing max and min would be also useful for the user. Changing.\n. @karlnapf  Changing it to SG_ERROR then.\n. @karlnapf added the class dox links in the enum description which generates clickable links!\n. added clickable class dox link here as well! :)\n. taken from the old values we have in develop right now.\n. @karlnapf @sejdino added covariance computation without permuting as well. here we don't use DEPRECATED blockwise statistic computation though. Just for covariance the samples are not randomly shuffled when null variance estimation method is NO_PERMUTATION_DEPRECATED.\n. @karlnapf errr! changing.\n. @karlnapf I was getting a weird error with BAHSIC because m_num_features was not being set by these set_p/set_q calls and the statistic computed was coming out to be nan.\n. @karlnapf no, just assigning proper m_num_features value at the time of set_p/set_q fixed that problem. now locally results are fine.\n. yeah we don't need to. Shall I just remove it then?\n. @sejdino Well, using DEPRECATED blockwise statistic computation, the kernel is computed on different samples compared to present methods. So apart from the normalizing constant, the statistic changes too. Should I be using deprecated statistic for covariance computation? The results won't exactly be the same with earlier implementation based on the current implementation we have because when using compute_blockwise_statistic_DEPRECATED, we split the data in halves for each p and q for all the samples streamed in current burst. but for covariance, I am presently streaming twice as many blocks in each burst when in earlier implementation it just used to stream the same number of samples split in halves. with present implementation, this change didn't affect result because computation is done block-wise instead of burst-wise like before. I need to change it a bit then.\n. I think the covariance in present implementation is being computed wrongly. instead of computing covariance of mmd estimates between samples (p1, p3, q1, q3) and (p2, p4, q3, q4), pi/qi being the samples in current burst, currently its considering samples (p1, p2, q1, q2) and (p3, p4, q3, q4). I'll take a deeper look into this tomorrow.\n. @karlnapf well, if getting all the labels and using memcpy won't work if features and labels have subset initialized. This is a bit slower approach but it does index conversion and its called only once per apply(). Matrix is needed to create a CDenseFeatures instance. However, I think this memory can be freed when apply() is done. Therefore, a cleanup() call can used to take care of this. Since this feature object is not available for public API and for internal use, I guess that would be good.\n. Do we really need to put this under CSGObject? @karlnapf @lisitsyn thoughts?\n. @khalednasr could you please uncomment these (and the unit-tests)? I just restarted travis build. I don't think there should be an issue.\n. @khalednasr Is it possible to get the implementation into a cpp instead of header? Like SGMatrix? Should be, no? It just has one template arg T for scalar type which we already know what they're gonna be.\n. Ah I see. We need it for serialization. Sorry.\n. @khalednasr I think we'd soon be able to have shogun::linalg::prod which does it for all :)\n. @khalednasr  add a c++11 check here using HAVE_CXX0X or HAVE_CXX11in shogun. This will also get rid of travis problem.\n. SGObject has many other things like parallel, refcounting etc. I am wondering whether these may create problems!\n. @khalednasr c++11 check needed for variadic templates.\n. missing this in earlier patch for GPUMatrix didn't cause problem on travis because this code wasn't compiled due to HAVE_VIENNACL check which on travis isn't defined!\n. @khalednasr yes this is better. We know for sure that we gotta use Eigen::Dynamic here.\n. @khalednasr sorry for an irritating comment. But could you please use a newline here? next patch maybe?\n. @khalednasr this could have been sent to cpp as well, no? not very important though.\n. yeah. makes perfect sense for this.\n. @khalednasr Don't ViennaCL include headers define this macro when it finds OpenCL? I don't know, just curious.\n. Another irritating comment about fashion stuff. In shogun we use braces for conditions and loops when the operation spans multiline. So shogun code would look like\nc++\nfor (...)\n{\n    for (...)\n        foo();\n}\n. @karlnapf ummm.. well, its in public API of CFeatures.. so.. But since we have kept num_features thing out of CFeatures since it doesn't make sense for all feature types, maybe this method should not be here at all. Maybe a helper method in CFeatureSelection should handle it, like CFeatureSelection::get_num_features. What do you think?\n. @karlnapf the base class CDependenceMaximization doc already mentions the additional memory requirement. These two classes don't add anything extra on the top of that. \n. @khalednasr why should we use two different Derived here? will this enable us to call dot on vectors of two different scalar types or this is to be flexible about one fixed and one dynamic sized vectors? Via redux wrappers I think only a single vector type is handled. If it helps in terms of flexibility, could you please add a tiny unit-test for that?\n. elegant solution! :+1: :+1: \n. @karlnapf well I thought that since our last discussion with @vigsterkr and @sonney2k regarding this num_feature thing, having a copy_dimension_subset in CFeatures would ultimately result in this method being unimplemented in all feature classes except for CDenseFeatures and CSparseFeatures :(\n. The return type is a headache though!\n. ummm nah that doesn't work for, say, complex128_t and float64_t :(\n. yep that's what I was thinking! but it doesn't harm to keep it like this though. So no need to change it.\n. one can always directly call the linalg::implementation::dot if really necessary. But man will he be sorry for that :D \n. I guess this can only be avoided when we don't provide API for Eigen3 matrices at all. We always pass SG and then cast via eigen's maps, right?\n. So, if we pass a eigen3 vector, it uses SGVector's constructor with eigen3 vector param for that, then maps it back to eigen3 and computes dot, right?\n. cool! we can pass Eigen3 matrix here and assign the result to an Eigen3 matrix as well I guess because of the overloaded operators :+1: \n. @khalednasr can someone use it for, say, Block<Eigen::Matrix> block(eigen3_mat) by this without having to do deep copy via internally converting eigen3_mat to SGMatrix and storing that instead? via some overloaded operator trick? The point is, if we're able to pass eigen3_mat to those sum methods, we should be able to pass a block of eigen3_mat as well, without having to deep copy things. I guess that's possible, right?\n. Another thought - I guess now we can have this block method as a member for both SGMatrix and CGPUMatrix. So that just using mat.block(i,j,k,l) would do - simple as eigen3's construct. no need to pass the matrix for that. What do you think?\n. Could you please fix the indentation?\n. could you please add just a line for doxygen here? maybe just @param and @return?\n. @khalednasr this doesn't work anymore?\n. We can either sort the features as per their relevance or keep the selected features in the original order.. Since earlier I've kept it in the original order this is just to keep the similariry.. Do you think that we should keep them sorted as per relevance?\n. @khalednasr sorry for the long delay.. I didn't notice this PR.. Just one suggestion about this one.. since this specialization doesn't seem to be using Eigen3, do you think it would be okay if we keep this implementation inside the generic RandomMatrix class and not in some place that's specialized for Eigen3 backend? Then we'll just have to specialize for ViennaCL.. Similar thing for random vector as well..\nCould you please update the PR if you have just a few mins? Please let me know :) Thanks..\n. @sanuj SGVector has vlen within it. So that's fine. What I'm worried about is that presently we don't have HAVE_LINALG_LIB macro defined if none of the external linalg backend is found. And all the stuff inside linalg works only if the macro is defined. So no matter what native implementation is added to linalg, with present cmake setting it won't be visible if external linalg libs are not found. See the problem?\nI didn't get the non-vectors part. dot is only for vectors, right?\n. @srgnuclear I'm afraid if this is the only addition you've made for lapack backend support for linalg then it won't work. Notice that if cmake doesn't find eigen3 or viennacl, our friend HAVE_LINALG_LIB is still undefined (under which this whole thing works). You haven't taken care of that in the cmake. Maybe I'm missing something here? (any previous PR?)\n. - The other params are not documented\n- note that this struct is not directly used in our application, but these are rather internal (you can find how to use these dot methods in our dot unittests). To make it work, our function dot(a,b) HAS to be able to call a method named \"compute\". We don't want to write this function over and over again for all the backends we have. So this templated backend thingi takes care of that. You must keep the signature of compute method same as others. just dense dot for now.\n. @srgnuclear could you please explain why this change was required? (I don't remember exactly - we had redux earlier but then we added \"core\" module later to accommodate some other stuffs). me or Khaled may have missed something here.\n. Please note that there should not be any other params apart from the vectors.\n. I'll have to check this a bit. Will get back!\n. We just have to figure out what the other params are doing. I'm quite sure they are for length related information.\n. Also, the signature is\nc++\ndouble cblas_ddot(const int N, const double *X, const int incX, const double *Y, const int incY);\nSo passing shogun::SGVector<T> directly will cause disaster ! We gotta make use of SGVector 's vec and vlen here. Please test the code locally before pushing :-)\n. @srgnuclear check this link http://www.mathkeisan.com/usersguide/man/ddot.html\n. @iglesias umm I think no.. it is still true :D \n. @iglesias if both are available, then the enum would be defined as\nc++\nenum class Backend\n{\n    EIGEN3, // will be 0\n    VIENNACL, // will be 1\n    NATIVE, // will be 2\n    DEFAULT = 0 // therefore EIGEN3\n};\nIf eigen3 and viennacl is not found, the enum is\nc++\nenum class Backend\n{\n    NATIVE, // will be 0\n    DEFAULT = 0 // therefore NATIVE\n};\nWe're relying on the fact that enum by default starts from 0 and keeps on increasing by 1.\n. DEFAULT will be same as the first thing defined. So if both Eigen3 and ViennaCL are there, it is Eigen3. If only ViennaCL is there it is ViennaCL. If nothing is there it is NATIVE (which is limited to only a few operations that we'll provide native implementation).\n. @sanuj yep. that's pretty much it :)\n. @iglesias thanks for merging the convolution patch. I assume it is okay then if I rebase and merge this patch then.\nPlease check the wiki on linalg as well. I could really use your suggestions on that.\n. @iglesias thanks for your comments.\n- yes you're right, I'll add some comments on the module names. It is not very clear from those names. IMO redux is for those operations which takes a matrix/vector and returns a scalar, e.g. dot, sum etc. rest generic util things should go to core. I'll have to double check if that is what actually done here\n- actually the guard is not required. HAVE_LINALG_LIB is defined whenever we have HAVE_EIGEN3 or HAVE_VIENNACL defined. currently there is no cmake option to turn linalg off. I think only HAVE_EIGEN3 check will suffice there. I'll take care of that.\nThanks. I hope the wiki will be useful :) will finish it off by tonight.\n. @iglesias actually we don't want to implement everything natively, e.g. we most certainly don't want to write our own self adjoint eigensolver or SVD/QR/Cholesky. HAVE_LINALG_LIB guard is useful for those methods which are available only when we do have an external linalg lib. In linalg modules we're guarding those operations with HAVE_LINALG_LIB which do not have their native implementations (this modification is done in this patch only). I think it is still useful. A use-case is when we just want the result for some of these operations and don't wanna bother about which backend was used (i.e. relying on global backend setup).\nIn CustomKernel.cpp I'm explicitly using eigen3 backend because when I added this, ViennaCL specializations were not added till then for those methods. We can most certainly do native implementation for sum methods, and, in that case, then we can get rid of the guard in CustomKernel. Otherwise, as of now, we can get rid of HAVE_EIGEN3 guard and remove the specific backend Eigen3 in those sum methods - it will then use Eigen3/ViennaCL - whichever is available.\n. I think I should mention about proper usage of HAVE_LINALG_LIB in the wiki as well as the limitations while using NATIVE backend more specifically. Thanks for your inputs :)\n. I thought of putting a matrix there in the wiki - mentioning the currently supported operations and which backends we can use for that. I hope that would be helpful.\n. Yeah the meaning is not very clear. I should explain this macro properly in the wiki :)\nBTW I just rebased this branch. Shall we merge it after travis is finished?\n. agh there are some errors due to c++11 code being exposed.. will fix it.\n. @sanuj just addition of 4 lines. Sorry I just have been lazy P Will push by tonight.\n. @sanuj yeah I guess @iglesias is fine with it (once travis stops whining). I'll merge this after I push the changes. Glad to see that you're psyched to work on this :D \nGo linalg !\n. @karlnapf yep. this would be useful. I think Redux module is the right place for this. Also, if using Eigen3's SIMD powered operations this would be lot faster.\n. this can be done in parallel, openmp-ized :) but its fine for now.\n. @sanuj yes that's what I meant. There's no such rule of thumb for what operations we like to be executed in parallel - but it's worth noting down the scopes where we can. Also, I'm not sure whether we should use pthread or openmp. So maybe leave that for now - we can anyway add it later :)\n. nah shogun's math is fine.\n. Nope I think that's unnecessary overhead for this one.\n. yeah I believe that makes sense. I tried locally with/without the omp directive for dot and the assembly it generated was same - compiler takes care of the vectorization.\n. we can use std::accumulate here :)\n. @yorkerlin @karlnapf totally possible. As a matter of fact, it should just be one method with one [optional] functor param :D \n. @karlnapf umm I remember you suggesting something regarding the usage of underscores in the beginning of these. @yorkerlin is there any particular reason you changed this?\n. sorry but minor comment: if it is just one line, we don't use braces for if for etc things :)\n. @lisitsyn only up to 5 constructor params - 5 should suffice and rest should be initialized by setters but still variadic is good - why restrict ? I will also vote for c++11 one :P \n. why not check them in loop?\n. Errr. my bad! Shouldn't have merged.\n@curiousguy13 the header??\n. Some indentation issue is there.\n. This should be able to handle both SGMatrix and SGVector. We can have two methods\nstatic void compute(SGMatrix<T> A, SGMatrix<T> B, T alpha)\nand\nstatic void compute(SGVector<T> A, SGVector<T> B, T alpha)\nboth of these would call internally a\nstatic void compute(T* A, T* B, index_t num_elements, T alpha)\nwhere you pass the number of elements you want to scale. That would be consistent with the Eigen3 and ViennaCL specializations we have for scale.\n. extraneous template <>. I know we have it in linalg at many places but these are not really necessary. This is simply a partial specialization. We should aim at removing the existing ones. gcc doesn't complain but with clang we get a warning for this.\n. really useful :)\n. There should be a sanity check for size, in case the sizes of A and B differ.\n. This len parameter is not really required. In the specializations, we can extract them from the Matrix type itself.\n. Something wrong with indentation\n. We can just pass A.num_rows * A.num_cols here for len\n. The requires are not needed then\n. Same here\n. Just a NULL check here.\n. Default value of start is taken care of in the wrapper method itself. We don't have to put it here.\n. Here ^ (default value for start)\n. len not required here as well.\n. Something wrong with indentation\n. Something wrong with indentation\n. Something wrong with indentation\n. Would be so much neater if we don't have to pass vlen.\n. That's fine. The main thing is that these specializations handle both matrices and vector. For our native implementation we can add any helper method we need to achieve that purpose. This is practically a different class altogether. \n. @SunilMahendrakar well what I meant here was to have an additional method that just takes raw pointer types and the length of the consecutive memory array. \nc++\nstatic void compute(T* A, T* B, index_t len, T alpha)\n{\n    // REQUIRE on null check for both A and B\n    // the loop goes here\n}\nSo you write the loop there just once and make the SGMatrix and SGVector ones to call that method instead. That way, we'll be able to handle handle raw arrays as well (we can later simply add another wrapper which calls that with NATIVE backend). See what I mean?\n. Yeah. Keep the other requires there - they are useful. The null check REQUIRE is useful when people (hopefully mistakenly) pass scale(NULL, 1000, 0.5) in the raw ptr method.\n. @iglesias I personally like the \\n in the end - as per our wiki on Assertions -\n- Use proper English (Start with capital letters, write sentences)\n- Don't forget \\n at the end.\n- Make sure that you don't cause segfaults in your assertions, check bounds, pointer valid, etc\nAlthough it is just a matter of choice. \n. @SunilMahendrakar could you please add a couple of these tests (one on SGMatrix and another on SGVector) that does the scale operation in place? This way, linalg::scale(A, alpha) is also tested.\n. Actually, it makes sense to have unit tests for in place scale operation for all the backends - since we are exposing it for all of 'em.\n. @karlnapf sorry man I didn't follow. If only diagonal is required, couldn't we have used here CKernel::get_kernel_diagonal() instead?\n. @yorkerlin well, sorry I don't know how that algorithm works - if the working memory is the same as the matrix size, then I don't think adding a separate method for diagonal will gain something. But if you think you can manage with just an array of the size of the diagonal as the working memory (for computing the parameter gradient) - then it is useful.\nDoes this get_parameter_gradient work for all the kernel functions?\n. That's great! Although I'd make sure that adding something generic to the base is actually something that is useful. You can add extra methods in the sub classes (wherever appropriate) and at the time of usage you can just dynamic cast the kernel to call that method.. for things that are not supposed to be general, keeping them limited to the scope where they are actually used helps at times. I don't know much about the context but I hope that it makes sense :)\nGreat work by the way :) Hope to go through your additions soon!\n. Missed a few REQUIRES. Adding.\n. @curiousguy13 no, this has to be handled within the other struct only. Sorry :)\nCheck the linalg::add method implementation to see how it is done there. That can be helpful :)\n. Also, don't worry about the template argument being named Matrix for both matrix and vector. It doesn't matter. When you pass a SGVector there then Matrix can very well mean a SGVector. Does it make sense? You'll see the same for Add there.\n. You'll need a separate wrapper though for handling range_fill(T* A, index_t len). By wrapper I mean the methods that goes inside /modules, which in turn calls the compute methods in those implementation structs.\n. @sorig hey just a minor comment. Can you please follow the same notion when using #include here? As in, use #include <shogun/kernel/PeriodicKernel.h> instead. Also, it would be better to use the same license in the header. For example, check FeatureSelection.h. \n. @sorig Another very annoying minor comment. Although it is perfectly fine, but in shogun, multi-line (not necessarily multi-statement) blocks under if/for/while  etc usually goes within braces. For single line braces are not use. Brace (both opening and closing) takes an entire new line alone. Also, spaces are not used between operators and operands (I like it personally the other way though).\n. @yorkerlin ah due the usage of this, we're missing the in-place benefits here :(\nShall I add a param to specify whether you want this in-place or not?\n. @yorkerlin you need non-refcounted SGMatrix here because you're constructing it from a SGVector? I remember talking a couple of constructors in SGMatrix/SGVector to use it back and forth with refcounting on. Maybe adding this now would be a good idea.\n. @iglesias absolutely! that would surely be clearer. I'll update it.\n. Hehe no issues :D That indeed would be the right one. Will update :) \n. @yorkerlin I just merged the code. Two new constructors in SGMatrix are now available to share the same memory and refcount with an SGVector. Check #2814 (the usage is there in the unit tests added in that PR). Should be helpful.\n. @karlnapf yeah but presently these operations work the same way for matrices as well (in-place not allowed if we're not using an explicit backend). Maybe I'll think of something about how to handle this backend-independently.\n. @karlnapf Add scalar is not yet there in linalg. I will add that there, but I won't be able to do that until Monday (going on a trip).\nSo I think that we shouldn't keep this PR on hold for that. But @sorig can open as issue regarding replacing it and assign it to me for me to take care of that later?\n@sorig you can find most useful linalg methods (very few are there at this moment) in Core and Redux.\nThese methods take a backend that it will use to perform the computation, which currently are\n- NATIVE (our native implementation, this one is available only for a couple of operations as of now), \n- EIGEN3 and \n- VIENNACL. \nI'd suggest that when you use these methods, just \n- include <shogun/mathematics/linalg/linalg.h> in your cpp (NOT in header), \n- use explicit Eigen3 backend and \n- keep your code HAVE_EIGEN3 and HAVE_CXX11 || HAVE_CXX0X guarded. \nYou can check the README for linalg for some more details.Specially check the part for Shogun developers.\n. @karlnapf absolutely. I have been thinking about adding a handful of them in linalg. To make it generic I thought of having an interface\nc++\nlinalg::elementwise_operation<_UnaryOp>(Operand operand)\nlinalg::elementwise_operation<_BinaryOp>(Operand operand1, Operand operand2)\n// and so\nFor example, \n``` c++\n// the operation\nnamespace impl\n{\ntemplate \nstruct sin {\n    using return_type = float64_t;\n    return_type operator () (T val) { return CMath::sin(val); }\n};\n// specialize the above for complex128_t\n// the implementation\ntemplate  class _UnaryOp, template  class _Operand, typename T>\nstruct elementwise_operation {\n    static _Operand::return_type> compute(_Operand op) {      \n        // specialized for backend, Use CPU or GPU to compute things in parallel\n    }\n};\n}\n// the wrapper\n// provide separate wrappers for sin/cos/product/sq/sqrt - things like that,\n//  all using the same implementation struct\ntemplate ::backend, template  class Matrix, typename T>\nMatrix::return_type> elementwise_sin(Matrix operand) {\n    // not totally sure for the type thing here.. will have to check it\n    return impl::elementwise_operation(operand);\n}\n// usage\nlinalg::elementwise_sin(m);\n```\nand then we specialize for NATIVE and ViennaCL. For GPU, these can be done using custom OpenCL kernels. For eigen3, we can use the array thing.\nWrote the code just from the top of my head, it may not be like what it is. But I am sensing massive performance gain. Maybe we can provide in-place operations as well (when the scalar types agree to that).\nWill work on that next week. Maybe useful for @sorig :)\n. @karlnapf worked out a sample of how this should work. Posting the code in the gist. Can't access my laptop from office now so just ideone'ed a small poc - seems to work fine. The trick is\n- to have a container_type in our vector/matrix containers (for both CPU and GPU)\n- the operations will have a string for the opencl kernel as well, that will be used by viennacl to compute element-wise stuffs on gpu.\n. I thought a bit about this and it turns out that in-place is not possible if we allow implicit conversion to/from SG/CGPU. I need to figure out a proper way to do this. It is better that you use eigen3. Else if you do\nc++\nright_vec = linalg::add(left_vec, right_vec, ...);\nthis would technically work but you'll end up reallocating+freeing a chunk.\n. @karlnapf shouldn't be much work. I think openmp might turn out to be an overkill for native since modern compilers generate vectorized instructions for the loops. I have been using std::transform and std::for_all with lambda which I believe are reasonably fast. Maybe a benchmark will help.\nI am worried about implicit conversion to/from SG/CGPU structs. They are performance killers and they are obstacles for in-place operations as well. Not sure if we should keep those (or maybe handle in a different way). Will think about it a bit.\n. @karlnapf okay so this works. Unit-tests in my machine\n```\n[lambday@lambday.iitb.ac.in build]$ ./tests/unit/shogun-unit-test --gtest_filter=\"Elementwise_Sin.\"\nNote: Google Test filter = Elementwise_Sin.\n[==========] Running 14 tests from 1 test case.\n[----------] Global test environment set-up.\n[----------] 14 tests from Elementwise_Sin\n[ RUN      ] Elementwise_Sin.SGMatrix_NATIVE\n[       OK ] Elementwise_Sin.SGMatrix_NATIVE (1 ms)\n[ RUN      ] Elementwise_Sin.SGMatrix_NATIVE_inplace\n[       OK ] Elementwise_Sin.SGMatrix_NATIVE_inplace (0 ms)\n[ RUN      ] Elementwise_Sin.SGVector_NATIVE\n[       OK ] Elementwise_Sin.SGVector_NATIVE (0 ms)\n[ RUN      ] Elementwise_Sin.SGVector_NATIVE_inplace\n[       OK ] Elementwise_Sin.SGVector_NATIVE_inplace (0 ms)\n[ RUN      ] Elementwise_Sin.SGMatrix_NATIVE_complex128\n[       OK ] Elementwise_Sin.SGMatrix_NATIVE_complex128 (0 ms)\n[ RUN      ] Elementwise_Sin.SGMatrix_NATIVE_complex128_inplace\n[       OK ] Elementwise_Sin.SGMatrix_NATIVE_complex128_inplace (0 ms)\n[ RUN      ] Elementwise_Sin.SGVector_NATIVE_complex128\n[       OK ] Elementwise_Sin.SGVector_NATIVE_complex128 (0 ms)\n[ RUN      ] Elementwise_Sin.SGVector_NATIVE_complex128_inplace\n[       OK ] Elementwise_Sin.SGVector_NATIVE_complex128_inplace (0 ms)\n[ RUN      ] Elementwise_Sin.SGMatrix_EIGEN3\n[       OK ] Elementwise_Sin.SGMatrix_EIGEN3 (0 ms)\n[ RUN      ] Elementwise_Sin.SGVector_EIGEN3\n[       OK ] Elementwise_Sin.SGVector_EIGEN3 (0 ms)\n[ RUN      ] Elementwise_Sin.CGPUMatrix_VIENNACL\n[       OK ] Elementwise_Sin.CGPUMatrix_VIENNACL (69 ms)\n[ RUN      ] Elementwise_Sin.CGPUMatrix_VIENNACL_inplace\n[       OK ] Elementwise_Sin.CGPUMatrix_VIENNACL_inplace (59 ms)\n[ RUN      ] Elementwise_Sin.CGPUVector_VIENNACL\n[       OK ] Elementwise_Sin.CGPUVector_VIENNACL (0 ms)\n[ RUN      ] Elementwise_Sin.CGPUVector_VIENNACL_inplace\n[       OK ] Elementwise_Sin.CGPUVector_VIENNACL_inplace (0 ms)\n[----------] 14 tests from Elementwise_Sin (129 ms total)\n[----------] Global test environment tear-down\n[==========] 14 tests from 1 test case ran. (129 ms total)\n[  PASSED  ] 14 tests.\n```\nHave to take care of a few beauty related stuffs and documentation so didn't send this yet as a PR. Few things I want to point about this design\n- I wanted to turn off implicit type conversion between CPU/GPU types following our previous discussion. So for elementwise operations, I have turned off default backend. Devs must specify backend and use supported type to use these. For example, SGMatrix with ViennaCL not supported. In order to use ViennaCL, devs should transfer it to GPU first and then call the method on that (when that makes sense).\n- As a result of the above, in-place operations are available for NATIVE and  VIENNACL (Eigen3's array() thing doesn't work in-place I'm afraid). Separate wrappers for in-place and regular operation. In-place will work when the scalar types agree, static_assert fail otherwise (I thought it is better to give them a verbose info about why it failed during the compilation).\n- Matrices and vectors don't have to be treated differently. Made use of the templated type instead to avoid rewriting. Introduced standard data() and size() methods in our matrix/vector types to achieve that.\n- Future elementwise operation addition would be copy-paste (macros maybe)\nPreparing a benchmark now to see if we gain performance using GPU over CPU for these.\n. Benchmark result based on this (100x100 matrices), element-wise sin operation\n```\n[lambday@lambday.iitb.ac.in benchmarks]$ ./benchmark \n[==========] Running 3 benchmarks.\n[ RUN      ] SGMatrix.elementwise_sin_NATIVE (10 runs, 1000 iterations per run)\n[     DONE ] SGMatrix.elementwise_sin_NATIVE (3489.504000 ms)\n[   RUNS   ]        Average time: 348950.400 us\n                         Fastest: 342721.000 us (-6229.400 us / -1.785 %)\n                         Slowest: 371872.000 us (+22921.600 us / +6.569 %)\n         Average performance: 2.86574 runs/s\n            Best performance: 2.91783 runs/s (+0.05209 runs/s / +1.81763 %)\n           Worst performance: 2.68910 runs/s (-0.17664 runs/s / -6.16384 %)\n\n[ITERATIONS]        Average time: 348.950 us\n                         Fastest: 342.721 us (-6.229 us / -1.785 %)\n                         Slowest: 371.872 us (+22.922 us / +6.569 %)\n         Average performance: 2865.73679 iterations/s\n            Best performance: 2917.82529 iterations/s (+52.08849 iterations/s / +1.81763 %)\n           Worst performance: 2689.09732 iterations/s (-176.63947 iterations/s / -6.16384 %)\n\n[ RUN      ] SGMatrix.elementwise_sin_EIGEN3 (10 runs, 1000 iterations per run)\n[     DONE ] SGMatrix.elementwise_sin_EIGEN3 (3592.366000 ms)\n[   RUNS   ]        Average time: 359236.600 us\n                         Fastest: 357530.000 us (-1706.600 us / -0.475 %)\n                         Slowest: 363527.000 us (+4290.400 us / +1.194 %)\n         Average performance: 2.78368 runs/s\n            Best performance: 2.79697 runs/s (+0.01329 runs/s / +0.47733 %)\n           Worst performance: 2.75083 runs/s (-0.03285 runs/s / -1.18021 %)\n\n[ITERATIONS]        Average time: 359.237 us\n                         Fastest: 357.530 us (-1.707 us / -0.475 %)\n                         Slowest: 363.527 us (+4.290 us / +1.194 %)\n         Average performance: 2783.68073 iterations/s\n            Best performance: 2796.96809 iterations/s (+13.28736 iterations/s / +0.47733 %)\n           Worst performance: 2750.82731 iterations/s (-32.85342 iterations/s / -1.18021 %)\n\n[ RUN      ] CGPUMatrix.elementwise_sin_VIENNACL (10 runs, 1000 iterations per run)\n[     DONE ] CGPUMatrix.elementwise_sin_VIENNACL (129.061000 ms)\n[   RUNS   ]        Average time: 12906.100 us\n                         Fastest: 4283.000 us (-8623.100 us / -66.814 %)\n                         Slowest: 55755.000 us (+42848.900 us / +332.005 %)\n         Average performance: 77.48274 runs/s\n            Best performance: 233.48120 runs/s (+155.99846 runs/s / +201.33318 %)\n           Worst performance: 17.93561 runs/s (-59.54713 runs/s / -76.85212 %)\n\n[ITERATIONS]        Average time: 12.906 us\n                         Fastest: 4.283 us (-8.623 us / -66.814 %)\n                         Slowest: 55.755 us (+42.849 us / +332.005 %)\n         Average performance: 77482.74072 iterations/s\n            Best performance: 233481.20476 iterations/s (+155998.46404 iterations/s / +201.33318 %)\n           Worst performance: 17935.61116 iterations/s (-59547.12956 iterations/s / -76.85212 %)\n\n[==========] Ran 3 benchmarks.\n```\nGPU is way faster! Will try with custom unary operator (as required for this PR) as well.\n. Okay here is the plan. \n- I think using separate methods for vectors and matrix is a not a very good idea, leads to a lot of code duplication. Their underlying data is just a plain memory array. Also, in shogun, matrices are always column-major. So treating them in the same manner shouldn't be an issue. I admit that it is done in linalg in quite a few places but that's bad. We should change that.  I just added methods T* data() and index_t size() in both SGMatrix and SGVector (check PR #2830). So using these we can handle the underlying memory exactly the same way as one would do with raw array types (once that PR is merged).\n- Write one struct \nc++\ntemplate <enum Backend, class Container>\nstruct range_fill\n{\n    // you don't need to put anything here. Your code should go only with\n    // the specializations.\n};\nand partially specialize NATIVE. Access the data using container.data() and size using container.size().\n- Write two wrappers\nc++\ntemplate <Backend backend, class Container> \nrange_fill(Container container, typename Container::Scalar begin=0)\n{\n}\nand\nc++\ntemplate <typename T> // backend not required. raw pointers should work only with NATIVE\nrange_fill(T* memarray, index_t size, T begin=0)\n{\n    // construct a SGVector object with refcounting turned off\n    SGVector<T> v(memarray, size, false);\n    // instantiate the impl template with SGVector<T> type and NATIVE backend and call compute\n}\n- Since this supports NATIVE implementation, keep these above methods outside of the HAVE_LINALG_LIB guard (since they can be invoked even in absence of external linear algebra lib).\n. Yes I should change it to uint64_t or size_t.\n. Hehe. Just doing justification to my IRC handle :D There are lambda functions.\n. We have a dedicated ViennaCL build in buildbot. But that just sets the only the global backends to ViennaCL. I think we should get rid of global ViennaCL backend setups, because when that's done, one would take a massive performance hit due to all the data transfer in all the operations. Backends should always be specified explicitly. This static assert is an attempt to prevent from doing the such unintentional things.\n. Took me a while to come up with this one here :) \n. @karlnapf it gives a runtime error from OpenCL backend while compiling the kernel. If one follows the error description, it is not hard to figure out what went wrong.\nThere is not much magic regarding how the string should look like. It is plain C code, just written inside a string. This whole string that we're passing forms the body of a function, named operation, where the current element in the element-wise operation is called element. For example, here, the operation function looks like\nc\ninline float operation(float element)\n{\n    // the string that we provide gets replaced here\n    float outer_factor=-2*3.1415+sqrt(element)*pow(0.6, 2);\n    float exp_factor=exp(-2*pow(3.1415,2)*element*pow(0.2, 2));\n    float sin_factor=sin(2*3.1415*sqrt(element)*0.01);\n    return outer_factor*exp_factor*sin_factor;\n}\nAnd then this operation is used inside an OpenCL kernel\nc\n__kernel void kernel_some_unique_hash_float(__global float* vec, int size, int vec_offset, \n            __global float* result, int result_offset)\n{\n    int i = get_global_id(0);\n    if (i<size)\n        result[i+result_offset] = operation(vec[i+vec_offset]);\n}\nThe user would just have to provide the body of the operation. Everything else is taken care of inside the framework (thanks to Khaled's awesome code)\nThe only limitation is, the functions we can use inside those string must be supported by OpenCL. Like sin/cos/exp/pow/log etc are fine, but rand is not (so far I know).\n. @lisitsyn hey man could you please have a look at this? Didn't get much time to work on it earlier. This one is for the formatted solution.\nI spent some time earlier trying to work out things for the first solution but I don't think it can be done. Operators for basic types cannot be overloaded in C++. Maybe you have some ideas?\n. @karlnapf yes in-place versions are also there. \n. you have already written using namespace linalg, so you can omit the linalg:: here I suppose.\n. Remove the extra newlines.\n. same as above.\n. same as above.\n. same as above.\n. same as above.\n. same as above. remove extra newline.\n. same as above.\n. Not too relevant. But I am not sure why SGVector<T>* was used here. SGVector<T> is supposed to be used by value (mostly). Maybe @yorkerlin can comment since he has written the code.\n. same as above.\n. same as above. Also, a minor styling issue - in Shogun we don't use braces with if-else and loops if it is a single liner. For braces we follow the\nc++\nif (cond)\n{\n    // do stuffs\n}\ninstead of\nc++\nif (cond) {\n   // do stuff\n}\n. same as above.\n. same as above.\n. same as above.\n. same as above.\n. same as above.\n. same as above.\n. same as above.\n. same as above. Also, not a good idea to remove existing unit-tests. Just imagine, if the scale fails somehow, we'll get to see failed tests with misleading names (i.e. the later tests). Having specific unit-tests for small things serves just that purpose. It helps us to pinpoint the exact cause of a failure.\n. Ideally these tests should be put inside different TEST cases altogether.\n. yeah linalg in c++11 only. the macro HAVE_LINALG_LIB itself is not defined when we don't have c++11 supported compiler. the ifdef is taken care of in linalg.h itself. so only a check of ifdef HAVE_LINALG_LIB around the places where it is used would suffice.\n. as per the discussion in other threads (#2719, #2919), soon Shogun itself would be made c++11 only. It won't be an issue I guess in that sense.\n. Few points\n- I think this method should return float64_t or complex128_t. Imagine when T = int.\n- I think we should just create a map of the vector and call eigen3 mean().\n- We don't need two different methods for vector and matrices. Only one should be able to handle both. Let me know if you have questions about this :)\n. @karlnapf these should not be there. Implementing mean should not be too hard on viennacl. I'd rather remove this whole specialization as of now, and add this only when it has a working implementation.\nIMO the only other place we did is for block-wise sum (viennacl implementations absent). Those should be gone as well\n. Hi @OXPHOS. Thanks for sharing your thoughts.\n- Actually I think the return type should be whatever eigen mean returns for this specialization. But it should never be just T. If T is of any integer type, it give wrong results. Do you think it would be doable using any of C++11 magic? I am talking about that \nc++\nauto foo() -> decltype(...)\nthing. Something like this\n- You're right, there are just too many of them. Although all of them serve different purposes. But yes we'd like to clean things up a bit, hopefully, in this year's GSoC :)\n  About handling SGVector and SGMatrix, I'd say that we can just create an Eigen::Map of matrix type (the vector is just a matrix with 1 column) and then call mean() on that method. All we need to achieve this is to specify the data pointer, number of rows and cols. The method then can take a template instead. Kinda like this one here.\nThe reason I am excited to have just one method is that, while using the API, I won't need to specify linalg::vector_mean(...) or linalg::matrix_mean(...) each time. I'll just call linalg::mean(...) and expect that it knows how to handle this.\n. @OXPHOS yes it would look lame. What I was thinking that - say, have a trait.\nc++\ntemplate <typename T>\nstruct some_trait\n{\n    using Scalar = T;\n};\nand then specialize it as \nc++\ntemplate <>\nstruct some_trait<int32_t> // similarly for other int types\n{\n    using Scalar = float64_t;\n};\nso your method signature becomes\nc++\ntemplate <class T>\ntypename some_trait<typename T::Scalar>::Scalar mean(T op);\n// T = SGVector or SGMatrix, both have a Scalar defined\nYes we are not using decltype for this one. But this is one way we can achieve this - no ugly if-else. It still solves the return type problem for all the types without losing precision. Let me know if there are better/other ways you can think of to handle this.\nI am not sure whether this is the way to go though. Say, if I have a int vector with 1s and 0s, as long as there is a single 0, I'd get 0 mean every time if the return type is just int and not some floating type (which would be the case if we didn't do type-cast while /-ing). Is that the expected behavior? Not sure, just thinking aloud.\n. @yorkerlin I think since this is in the standard, we're good to go. Have a look at the discussion here and the implementation here. Looks like they take quite a lot of measures for different cases. I'll add the unit tests though.\nOne question : are these numbers (for the lnormal_cdf case)\nc++\nt[0]=0.197197002817524946749;\nt[1]=0.0056939283141627128337;\nt[2]=11.17348207854067787537;\nt[3]=0.513878566283254256675;\nt[4]=4.34696415708135575073;\nt[5]=-8.69392831416271150147;\nt[6]=-17.38785662832542300293;\nt[7]=-34.42874909956949380785;\nt[8]=-67.46964157081356461276;\nt[9]=-136.67410392703391153191;\njust chosen randomly while writing the unit-test or there is some source that I should be looking at?\n. @karlnapf there is a look-up table or sort for some predefined a, b and x in cdflib. We can use that. Also, maybe for  x < 0.5, we can use symmetry - will check if that is faster.\n. can we just hold a reference to the SGVector instead instead? We won't be passing this CPUVector around via any API (apart from linalg API which is internal), so an extra refcount is not really needed I think.\n. Drop the underscore '_' from the class name. CamelCase should be enough\n. We don't need this constructor. Remember, an instance of BaseVector is never really created. It is always either CPUVector or GPUVector.\n. Make this pure virtual. The base class doesn't really know whether the data is on CPU or GPU.\n. We gotta make this pimpl. No hard-dependency on ViennaCL in the headers :) Maybe when we split the header and the cpp, you can do that?\n. Just do what the present viennacl dot is doing.\n. Remember to keep the header free of this dependency when you split your code into cpp and headers. Only the cpp file should have this if-def checks, if any.\n. Why does this class needs to exist?\n. The compiler can deduce the type T. Can you try without specifying it explicitly.\n. Oh I see. My bad.\n. This is okay for now. But keep in mind that the creation of this object has to be done in init_shogun() call, since this object better be global. We don't want to create it every-time. We should be able to use this like sg_linalg->dot(..., ...), in a similar fashion we use sg_io etc.\n. get rid of the leading _. those are reserved for system headers. We should just use BASEVECTOR_H__\n. Move the implementation to the .cpp\n. This should not be public\n. Why the data commit?\n. Same as above\n. Same as above\n. Same as above\n. Indentation :)\n. Same as above\n. Same as above\n. Move the implementation to .cpp\n. What is this GPUptr? It's not declared or defined in GPU_Vector class.\n. Shouldn't it be working with the VCLMemoryArray shared ptr thing that it has?\n. No need to create the Linalg object here now. You have already added the global instance of it.\n. Use the global linalg object.\n. Why create std::vector first?\n. We need to benchmark the new code that you added, not the old code - we're gonna trash that anyway. That's the plan :)\n. Oh I see. You wanted to compare between the old code and the new one. I think this is not required.\n. Why std::vector and then memcpy? You can directly do that with SGVector.\n. We have to make the global sg_linalg work :)\n. This line could be problematic. Did you make sure that it works?\nAlso, since this squared result thing is not being returned by the API, it can just be a Eigen3 matrix that you use internally for this method. As long as the API doesn't take or return a non SG type, we're good. See what I mean?\n. Can you use eigSquaredResult.array().square() ? I think it does the same.\n. Same as above.\n. Same as above.\n. Impl?\n. Maybe we can avoid having the temporary matrix eigSquaredResult altogether.\n. Please no space around =. Except for logical operators, space is not used between operators and operands in Shogun.\n. Please remove space.\n. Please remove space.\n. Please remove space.\n. Please remove space.\n. In case the statement goes multi-line, use braces around if.\n. Please remove space.\n. Please remove space.\n. Please remove space.\n. Why these?\n. Please remove space.\n. Please remove space.\n. Please remove space.\n. Please remove space.\n. Might be a good idea to add some documentation for these methods.\n. Please add class documentation.\n. Please add class documentation\n. Please remove space\n. Please remove the above.\n. Shouldn't this be explicit?\n. Please remove space\n. Maybe it is better to implement this as friend - inline bool operator==(const BaseTag& first, const BaseTag& second).\n. Please add a operator!=(...) that uses the above one.\n. Newline\n. Remove above\n. Explicit?\n. Remove above\n. Oh I see. Don't bother for now. We're excited about this benchmark for now :D\n. indentation :)\n. You could have just created a init() method that does all those stuff, and then could have called that method from both the constructor. Anyway, not a big deal\n. Indentation\n. Indentation\n. Backend should be registered in the Data::init() itself. Also, I think there should always already be a CPU-backend (Eigen). We shouldn't have to set it every time.\n. Register the backend in Data::init(). Don't have to do it in each iteration.\n. No need to pass the template arg int32_t.\n. No need to pass the template arg int32_t.\n. Pass these as BaseVector<int32_t>*, as they are supposed to be used.\n. Maybe even BaseVector<int32_t> const * const.\n. Pass it using pointers. See comment above.\n. Pass it using const ref.\n. Indentation\n. Indentation\n. If you create this as a template, then we can easily check with the tests with different data-types. For example, the real use cases would mostly be float64_t and float32_t. can you please do that?\n. Just have a BaseVector<value_type>* Ac here. Same for others. Should be allocated in the heap.\n. We can directly pass data.Ac here then. This is how we'll use it in shogun code.\n. indendation went wrong again\n. Make the GPUVector live on the heap as well\n. newline :)\nc++\nBaseVector()\n{\n}\n. Please make similar changes in other places where you've used this\n. Usually in shogun the style is to put a newline after the template <typename T>. Make similar changes in other places where you've used this\n. newline\n. newline\n. newline\n. indentation\n. No underscores in class names :)\n. Nice! :)\n. newline\n. make it \nc++\ninline bool onGPU() const\n{\n    return true;\n}\n. indentation\n. newline\n. newline\n. newline\n. newline\n. remove the comment\n. newline before and after else\n. newline\n. newline\n. newline\n. remove these\n. unique_ptr :)\n. same as above\n. add test for GPUVector convert.\n. Use const method.\nc++\ntemplate <typename T>\nT dot(const CPUVector<T> &a, const CPUVector<T> &b) const;\n. c++\ninline bool onGPU() const\n{\n    return false;\n}\n. c++\ntemplate <typename T>\nT dot(const GPU_Vector<T> &a, const GPU_Vector<T> &b) const;\n. make it\nc++\ntemplate <class T>\nT dot(BaseVector<T> *a, BaseVector<T> *b) const;\n. can you please try using shogun's Unique here? if that works then we won't need c++11 guard around all these\n. probably then you won't need to create an instance here (if you use Unique)\n. a[i] = i in a new line please :)\n. Is it defined somewhere? I see definition of get_global_linalg() in the cpp but not this one. \nAlso, maybe this is not needed. Just remove this for now :)\n. make it const :)\n. oh for these methods that return something, you should use @return and then just write the description :)\n. const\n. Maybe put the data members at the bottom :)\n. Nice :)\n. data members at the bottom :)\n. If it is a struct, you don't need to declare anything public :)\n. const :)\n. data members at the bottom :)\n. remove these comments :)\n. data members at the bottom :)\n. Nice :)\n. This should not be required :)\n. Where is this defined? Maybe I am missing something here!\n. Maybe call it get_gpu_type. It is not very clear from the name that you want to convert it into gpu type. Also, we want this to be overloaded so that the users just use the same name while converting both matrix and vector. Maybe you have a better name in mind?\n. Well, when you change the behavior of this convert_vector thing (return gpu type if there is any, otherwise go on with cpu type), then this test should work even outside HAVE_VIENNACL guard.\n. well, this was decided because dynamic cast is thought to be slower compared to one virtual call + static cast. But I see what you mean. We probably won't need to have two different classes, only one class should suffice.\n. @OXPHOS what was the issue again with putting both these thing inside a single class? I think it is doable.\n. Okay thanks. But maybe we won't have to do the casting, if we can do what you discussed earlier in the gist @OXPHOS made - have it in the same class and call onGPU method. I can't seem to remember what was stopping us from doing that.\n. Agreed! The creation of the class itself should fail if viennacl is not found.\n. Agreed!\n. don't need to pass T in sum<T> I believe. \n. The comment // non-owning ptr, referring to the SGVector.vector is no longer valid, since this is now a deep copy. Can we please have shared-ptr for m_data here instead? \n. Does it work properly? Did you valgrind?\n. What about the case when data is on GPU? Say, you transfer the data on GPU once and then did a bunch of in-place operations. Then the data pointed by m_data is no-longer what the user would want. \nI think if the data is on GPU, it should copy that to m_data first. Then it should create a new SGVector, deep copy m_data to the vector of that SGVector and then return. This thing above will lead to segfault. For example:\nc++\nSGVector<float64_t> v;\nif (some_condition)\n{\n    // create a LinalgVector instance, vec, and do some linalg, finally assign that to the above SGVector\n    v  = vec;\n    // vec instance dies, along with its m_data\n}\n// try to use v, bam! dangling pointer, v.vector :(\n. Thread safety?\n. what happens to the data on GPU? Shouldn't it be copied to the m_data?\n. actually, when you fix the transferToCPU method, just call it here if onGPU() is true, before you create the SGVector.\n. Could you please call it Vector? Please change the file name also accordingly. LinalgVector is a bad name :D\n. Please use SG_FREE instead of free since you have used SG_MALLOC :)\n. Please use SG_FREE instead of free since you have used SG_MALLOC :)\n. If we do the transfer to CPU here, then the above std::copy is no more required :)\n. Please use SG_FREE instead of free since you have used SG_MALLOC :)\n. Please use SG_FREE instead of free since you have used SG_MALLOC :)\n. If we do the transfer to CPU here, then the above std::copy is no more required :)\n. This should return the data from GPU memory if onGPU is true :)\n. This should return the data from GPU memory if onGPU is true :)\nMaybe move these to cpp? Otherwise you'll have to include GPUVectorImpl here which is something we want to avoid.\n. err.. this should not be public. Unit-testing can be done via the following:\n- create an instance of Vector class, keep the data on cpu.\n- create another instance of Vector class, transfer it to gpu.\n- assert against values read index by index (will work once you fix the operator[]) \n. Curious about this line. What you wanted to do here?\n. See, we should not sync the data in everything that we do! GPU data, if present, should be copied to CPU only when (a) transferToCPU method is called explicitly or (b) an SGVector is created from this object. Otherwise we'll be ending up doing the transfer all the time! That's not very good!\nThe point of these data() methods were to access the CPU data by CPU backend (say, when you want to create an eigen map, which will only be used when no data is there in the GPU). The GPU backend anyway accesses the gpu data (it should not call these data() methods. See what I mean?\nJust make the data methods as it was before. In addition to that, let's make onGPU false explicitly when non-const data() is called. The reason is that, non-const access means that we're modifying it, in which case GPU data should become obsolete.\n. Try this\nc++\ntemplate<class T>\nT& Vector<T>::operator[](index_t index)\n{\n    REQUIRE(index>=0 && index<m_len; \"The index (%d) should be within range [0, %d]\\n\", index, m_len-1);\n    T value;\n    if (onGPU())\n        value = m_gpu_impl[index]; // viennacl::entry_proxy<T> to T transfer (GPU -> CPU happens here)\n    else\n        value = m_data[index];\n    return value;\n}\n. Oh it should not be T&. Maybe just add the read-only (const) version for now?\n. If a subclass (say, GaussianKernel) does not initialize a distance instance, then also it should work, no? For CDistanceKernel, this instance is mandatory.\n. Yeah triangular should be better. But the distance API currently doesn't have that.\nAlso, this is a bug. I should only use triangular matrix when lhs and rhs are equal. Fixing it.\n. yeah that's better. This is just to imitate the API we have for CKernel. I'll change it.\n. hehe tried that.. then realized that this is a static method :D\n. actually this is slow if precomputed distance matrix is provided. maybe I'll try changing the order in a separate patch (making sure that col-wise traversal is actually beneficial)\n. agreed\n. please make sure that you turn off operator[] and other methods (put a on_gpu check, if yes, throw error) for which things doesn't make sense when data lives on gpu.\n. we don't need it here as per the design we thought! to_gpu and from_gpu should be part of the backend.\n. why this is not virtual?\n. good point. it is coming from the fact that the distance we calculate is always float64_t no matter what. we should change that later. me and @karlnapf had a discussion on similar things a few days back. how about (just like in CKernel), we have a templated version of CDistance::distance, and then a non-templated version that (by default) returns float64_t.\n. good point. not sure whether it does that. better to be explicit.\n. euclidean distance class does this now also, i.e. precomputes the squared lhs/rhs norms.\n. yeah I'll add the test.\n. isn't std::fill faster?\n. do you get \"reading uninitialized values\" error on valgrind?\nhowever, for this unit-test's purpose, you can set the value const to 0 for the cpu vector, transfer to gpu, set the value to something else, transfer back and test. that will get rid of this error.\n. Can you please use something clever here to avoid writing these ptypes all the time? For example, something like the following should work. You can then just use the macro DEFINE_FOR_ALL_PTYPE to define all your methods for all the ptypes. It's cleaner and shorter (although does the same job under the hood).\n``` c++\ninclude \ninclude \nusing std::cout;\nusing std::endl;\nusing std::make_unique;\ntypedef float float32_t;\ntypedef double float64_t;\ntemplate \nstruct GPUMemoryBase\n{\n};\ntemplate \nstruct SGVector\n{\n};\ntemplate \nstruct SGMatrix\n{\n};\ndefine SG_SNOTIMPLEMENTED cout << \"Method not implemented\" << endl;\ndefine DEFINE_FOR_ALL_PTYPE(METHODNAME, Container) \\\nMETHODNAME(int32_t, Container) \\\nMETHODNAME(float32_t, Container) \\\nMETHODNAME(float64_t, Container)\n\nstruct LinalgBackendBase\n{\n    #define BACKEND_GENERIC_TO_GPU(Type, Container) \\\n    virtual GPUMemoryBase* to_gpu(const Container&) const \\\n    {  \\\n        SG_SNOTIMPLEMENTED; \\\n        return nullptr; \\\n    }\nDEFINE_FOR_ALL_PTYPE(BACKEND_GENERIC_TO_GPU, SGVector)\nDEFINE_FOR_ALL_PTYPE(BACKEND_GENERIC_TO_GPU, SGMatrix)\n\n};\nint main() {\n    auto backend = make_unique();\nSGVector<int32_t> a;\nauto gpu_vec = backend->to_gpu(a);\n\nSGMatrix<float64_t> b;\nauto gpu_mat = backend->to_gpu(b);\n\nreturn 0;\n\n}\n``\n. also, define theDEFINE_FOR_ALL_PTYPE` macro at someplace central to all linalg, since all the backends implementations can use this macro.\n. @OXPHOS imagine that in some shogun algo we write some code like this\nc++\nSGVector<T> a; //\nauto b = linalg::to_gpu(a);\nlinalg::invoke_method_foobar(b);\nauto c = linalg::from_gpu(b);\nbut say, some poor guy running shogun on his poor machine doesn't have a gpu backend.. for him, this above code shouldn't throw error.. \n. btw if you think that this is a better idea, can you please make your existing PRs work with this meta-macro instead? most of your PRs look fine. we can merge it then. \nMaybe add this to one of your PRs, let me or @karlnapf know that you want that to be merged first, get it merged, rebase your other PRs against it? sounds okay?\n. nice\n. creating a new shared_ptr? not sure about this. why not just directly assign the orig->gpu_ptr?\n. It can be forward declared I guess\n. isn't this much nicer :)\n. hah nice :)\n. why the whitespace?\n. indentation\n. indentation\n. Sorry I am bit late to comment on this but is it a good idea to put a hard REQUIRE in this method, when in the init(...) we only need the features to have DOT property active? Maybe have a fail-safe fallback mechanism for other features and add this Eigen3 specialization for dense features (interface should be the same for both). Also, this method should work for 32-bit float features also.\n. Hi @Saurabh7. The method get_distance_matrix doesn't have to be a template. It should always return a SGMatrix<float64_t> (even for sparse features). But internally it should be able to handle both CDenseFeatures<float64_t> and CDenseFeatures<float32_t>, along with other feature types that supports FP_DOT. You can have a template private method for that which handles both the cases - you do the Eigen3 stuffs inside that. Fallback works with dot method in the features. Sounds reasonable?\nAnother thing I missed - the present implementation creates two new feature matrices every time you call this (Please check CDenseFeatures::get_feature_matrix()). So, I think we should have two of these methods\n1. One that works without subsets (fail with SG_ERROR if subset is present, say that the user should use the other variant if they subsets). It should use CDenseFeatures::get_feature_matrix(int32_t&, int32_t&), then initialize the Eigen3 maps with that. No mem alloc/copy, no-ref-counting business, faster.\n2. One that works with subsets. This may give a SG_INFO when subsets are not present - saying that the users are better off using the other variant. This requires mem alloc/copy, ref-counting for SGMatrix but respects subsets.\nWe can do this by either having the method as virtual SGMatrix<float64_t> get_distance_matrix(bool subset_present=false) or having two different interfaces get_distance_matrix() and get_distance_matrix_with_subsets(). Thoughts on this?\nWhen we're done with the implementation, we should also have unit tests for all the feature types for which EuclideanDistance can be initialized. This way, if something doesn't work, we'll catch it.\nLet me know what you think. @karlnapf may have some comments.\n. Hey @Saurabh7! Making the float64_t version in the base virtual sounds like the way to go (adding this method as a virtual one in CEuclideanDistance anyway won't work if some other interface uses a CDistance * to call the get_distance_matrix() method on it - it will always end up calling the base version in that case).\nSo, regarding the data copy issue, we can have a check inside the overridden method for subsets, and use the strategy I mentioned above.\nFallback as the base template would work perfectly. Internally the distance methods call the dot for appropriate cases and works as one would expect.\n. extra newline\n. extra newline\n. extra newline\n. Was the casting necessary? Just wondering, does the += operator (in the map_result.col(row)wise() += casted_l(r)hs_norms expression) take care of the case when the operands are of different type? If it can, then we don't need that. Can you please double check?\n. I think you can have the map_result as double valued matrix from the beginning (take the matrix that you'll return in the end in the beginning itself, and then use the map as lvalue). So, if the lhs and rhs vectors are float32_t valued, you'll have just one cast (in this above expression). You won't need to cast the norm vectors. \nIn the present code, in addition to casting the map result, we're having to cast the vectors (== mem-alloc/copy for those entire vectors) as well. Also, in the end, we're having to copy all the values back.\nWhat do you think?\n. Shouldn't this be virtual?\n. Can you please use Some<T>? Won't need the SG_UNREF in the end then.\n. Some<T> ?\n. @Saurabh7 hey man! Yeah you're right. I missed that somehow.\n. Sounds good :)\n. You're right.. this is not necessary, but helps with readability and similar practice is followed everywhere in Shogun, so... :)\n. Aah.. anyway, let's minimize the number of casts needed in the worst case..\n. Can't we cast the result of map_l.transpose()*map_r to double? That way, you just cast a matrix of the size of the result matrix, rhs-lhs and norm vectors casting can both be avoided.\n. Hey @Saurabh7 sorry for being annoying again but just a minor comment. Isn't it possible to use the SGMatrix<float64_t> result matrix from here itself and then use an Eigen map for it for operations that follow? If operator = does a deep copy for Eigen (required in the expression map_result = (map_l.transpose()*map_r).template cast<double>();, then we're good, right? Saves an extra alloc and copy. Can you please check once whether it's doable?\n. Hey @Saurabh7 ! Sorry again for being a PITA (I should have commented on this before) but doesn't map_result = map_result.array().sqrt(); create a temporary matrix and then do a deep copy? In that case, would it be possible to have an in-place operation for element-wise sqrt for the result SGMatrix? Maybe use our linalg methods?\n. Just curious, what does Eigen cast<T> method do when the types are the same?\n. Hey @Saurabh7 sorry for delayed reply - was busy with something. Hacked a quick experiment to see for myself - turns out it does create memory even with -O3. We're better off using our own element-wise operator it seems.\n. @Saurabh7 should use a native implementation of element-wise operator. But I see that's not added in the new feature branch that @OXPHOS is working on, neither it is there in the develop branch but you can actually do it there.\nFor now, just use a \nc++\nstd::for_each(matrix.data(), matrix.data()+matrix.size(), [](T& value) { value = static_cast<T>(CMath::sqrt(value)); });\nor something similar, that complies with C++98\n. You can make the existing linalg work with your requirements, avoiding the error for float32_t by just passing a lambda to the existing element-wise operator\nc++\nlinalg::elementwise_compute_inplace(your_matrix, [](T value) { return static_cast<T>(CMath::sqrt(value)); });\nShould be fine since your patch is against develop - the old interfaces are not removed yet. And even in the new implementation, this operation should have the same API.\n. Just make sure you guard this thing with C++11 macros for now if you use C++11 stuff or linalg.\n. Hey @sudk1896, I still see CamelCases in variable naming here. So I think there has been a confusion in understanding what @karlnapf was saying about styling issue. What he meant was, if I am not wrong, to send your new test with the style we expect our code to be in. This patch is for adding a new test - so let's make that the purpose of this patch. Style change for existing code is another issue. Let's make a different patch for that purpose.\nDoesn't make sense to have a patch for new code addition in a style that we don't want, only to have the style reverted in a subsequent patch.\nPlease have a look at our coding style wiki.\n. Hey @Saurabh7! Did you happen to have a look at it? If not, I can merge this patch for now and we can change it later. Let me know.\n. Please have a look at NAMING CONVENTIONS: section in the above wiki.\n. No braces for single line loops/conditionals. Check styling guide.\n. { on a new line. Check styling guide.\n. @karlnapf it is internal to just check sanity. Users cannot screw this up, therefore no REQUIRE.. Agreed. So, I think \n- this ASSERT should itself be a unit-test (because if someone changes the impl of FeaturesUtil::create_shallow_copy later, e.g. SG_REF it before returning it, (s)he should be alerted cause it might break things)\n- Actually I should move this thing to be a part of the CFeatures API maybe. This functionality has no reason to stay within statistical testing hierarchy.\nLet me know what you think.. @karlnapf that's what this method does :) I think Saurabh added it last year.. For our present requirements, this method suffices. But it does not clone the entire subset and attach it to the shallow copy, rather just pushes the last working index into it. So, if someone wants to pop out the subsets in the copy in the same order, it can't be done. Should the need arise, will have to add that.\nAlso, maybe this method could use a better name. Maybe shallow_copy_with_active_subset() or so?. Agreed. A separate private method should simply accept a raw ptr (or maybe a SGMatrix) to copy things into. If the ptr (SGMatrix.matrix) is empty or doesn't have enough space to memcopy things into, it reallocates.\nThen this method as well as the other get_feature_matrix method can rely on that method.. Alright. Haven't changed the behavior. It returns the same underlying matrix when there are no subsets. Please check the above return statement that I haven't touched.. What I mean is, even if there are no subsets, it still creates a new SGMatrix and copies into it rather than returning the original one. But I agree, only copy would suffice to convey the same sense :). No clue. Without the clone, one of the unit-test fails. So kept the behavior just the same as before. Maybe Saurabh can fill this in.. Well, set_feature_matrix was already an existing method. Didn't change its behavior.. agreed. changed it accordingly.. was trying to set zero to an uninitialized memory.. was trying to set zero to uninitialized memory when actiavtion_gradients was not-initialized (happens when the layer is ~not~ an input).. two un-initialized matrices are equal to one another.. just allocating the size doesn't mean we're going to read from the memory. segfault otherwise.. yeah, nice idea. std::memcpy and std::copy has pretty much same performance in my experience but with this shogun wrapper, we can try different things. I'll do that. . Thanks. Will add that.. yeah makes sense. Should also make it templated I think. When using std::memcpy in the backend, will cast to void*, but then the backend can easily be replaced with std::copy as long as it is templated.\nShould this be in header and inlined BTW or we include memory.h in SGObject headers and things will blow up?. Cool. Also, ideally, I think we should have 3 such methods\nc++\nshogun::memcpy(...)         // no overlapping - supposedly fastest of all\nshogun::copy_forward(...)   // works with overlapping ..[s_first..d_first..s_last)..d_last\nshogun::copy_backward(...)  // works with overlapping ..d_first..[s_first..d_last..s_last)\nLet me know what you think. well, maybe just one for now would suffice... actually, copy_backward thing can be useful if we want to do dense feature subset in-place.. @OXPHOS :P did you change the name of the file recently?. should be a separate PR.. maybe an entrance task?. umm.. why not merge this PR instead as it fixes the issue? It will have to wait till another 8 hours before I could rebase and merge this PR.. although there is no hurry... haha true that :D. Will do the util thing in a separate PR.. @karlnapf cool.. will do that in a separate PR.... Yeah iterators would be really good to have. Will have to discuss with you about the scope.. This is a protected method :) Doxygen won't see this.. should replace with newly added shogun::memcpy here. Commenting just so I don't forget . here as well.. Hey there. I'm afraid I didn't make the task clear enough :)\n\nWe want to rename this memcpy to sg_memcpy (which you did) that in turn calls std::memcpy just as before (i.e., don't make it call shogun::memcpy inside - that won't exist anymore once you rename it)\nWe want all existing memcpy uses in Shogun to use this newly added sg_memcpy instead.\n\nSee what I mean?. haha sed all over :D.. love it ;). hi, @karlnapf @vigsterkr. yeah, this test is pretty meaningless... it's just there to ensure that compute_p_value isn't broken and gives the same result every time when running with same seed. so whatever p-value comes here, maybe we shouldn't read too much into it. It's just 10 null samples.. @karlnapf the idea was that if someone changes the implementation later and due to some (maybe unintentional) modification, the calculation gives a different result, then this unit test will be able to tell that.\nYes, you're right about the local machine thing, I took the value that I got after running this test with this fixed seed and put that value as the expected value for this unit-test.. @karlnapf I think the following tests follow similar pattern:\nTEST(QuadraticTimeMMD, perform_test_permutation_biased_full)\nTEST(QuadraticTimeMMD, perform_test_permutation_unbiased_full)\nTEST(QuadraticTimeMMD, perform_test_permutation_unbiased_incomplete)\nTEST(QuadraticTimeMMD, perform_test_spectrum)\nTEST(LinearTimeMMD, perform_test_gaussian_biased_full)\nTEST(LinearTimeMMD, perform_test_gaussian_unbiased_full)\nTEST(LinearTimeMMD, perform_test_gaussian_unbiased_incomplete)\nTEST(KernelSelectionMaxMMD, linear_time_single_kernel_streaming)\nTEST(KernelSelectionMaxMMD, quadratic_time_single_kernel_dense)\nTEST(KernelSelectionMaxMMD, linear_time_weighted_kernel_streaming)\nTEST(KernelSelectionMaxTestPower, linear_time_single_kernel_streaming)\nTEST(KernelSelectionMaxTestPower, quadratic_time_single_kernel)\nTEST(KernelSelectionMaxTestPower, linear_time_weighted_kernel_streaming)\nTEST(KernelSelectionMaxCrossValidation, quadratic_time_single_kernel_dense)\nTEST(KernelSelectionMaxCrossValidation, linear_time_single_kernel_dense)\nTEST(KernelSelectionMedianHeuristic, quadratic_time_single_kernel_dense)\nTEST(KernelSelectionMedianHeuristic, linear_time_single_kernel_dense)\nHowever, I think instead of dropping all of these, some of these could be rewritten with hard coded data and asserted accordingly. Let me know what you think.. @karlnapf yeah unit-test fixture is a good idea. I agree, that for permutation tests we just need to check the indices. If the internal classes don't provide required APIs to access those things, we can use mock objects to allow access to the unit-tests.. @karlnapf yeah that's better. I'll change it then.. @karlnapf I'll get in touch with you about this soon. I can manage some time for Shogun now after work.. ",
    "research2010": "I can not agree with you any more. But I find currently shogun is only compiled in-source.\n. Yes. You are right. I install the packages by apt-get.\nsudo apt-get install libatlas-dev\nsudo apt-get install liblapack-dev\nBut the problem is there.\n. Thank you, sir. I have downloaded a library which depends on shogun-0.7.3. So I downloaded it. I don't know if some functions have been modified in the latest version.\n. So, I use the 0.7.3. But I can't find the sg.mexa64. I found that in the subdirectory matlab of the src directory, there was no Makefile file, but there were Makefiles in the python and libshogun directory. So I doubted that if the source code compressed package had the right files.\n\n. Thanks, sir. I have downloaded the 3.2.0, and use CMake to generate the project. And I have selected the MatlabStatic=On, but it still has no the sg.mexa64.\n\n The bin directory is the compiled result. It just has the cmdline_static's result.\n. Problem solved! Thanks @vigsterkr , @iglesias !\nmex -v CXXFLAGS=\"\\$CXXFLAGS -std=c++0x\" -I/home/user/shogun-3.2.0/src/ ...\n    -I/home/user/shogun-3.2.0/include ...\n    -L/home/user/lib -lshogun ...\n    MatlabInterface.cpp  -o sg\nthen, exit matlab gui; \nin the terminal, type export LD_LIBRARY_PATH=/home/user/shogun-3.2.0/lib:$LD_LIBRARY_PATH\nthen open matlab in this terminal;\nthen type sg('help')\nthe correct information is shown.\n. ",
    "ppletscher": "I agree with Fernando, from what I can say there's no need to keep it in PrimalMosekSOSVM.\n. I agree with Fernando, from what I can say there's no need to keep it in PrimalMosekSOSVM.\n. Yes, exactly. SSVMs use the max-margin / Hinge loss, if they would use something different, they wouldn't be called SSVMs anymore.\nEven if you would want to train the model using log-loss as done in CRFs, then the PrimalMosek class would not make much sense, as you would not have any constraints and hence the optimization problem looks quite different.\n. Yes, exactly. SSVMs use the max-margin / Hinge loss, if they would use something different, they wouldn't be called SSVMs anymore.\nEven if you would want to train the model using log-loss as done in CRFs, then the PrimalMosek class would not make much sense, as you would not have any constraints and hence the optimization problem looks quite different.\n. Thanks for the patch, I like the direction.\n1. I don't know what Shogun's policy is on this, but: I would really prefer if instead of commenting all the old loss variables in the examples, you simply delete them.\n2. In my opinion, we don't have to keep the surrogate loss at all in StructuredOutputMachine. Even if in the future somebody would implement a ramp loss machine, this would probably require new solvers. But I'm ok with keeping it the way it is in your patch.\n. Wouldn't it make sense to have this in CLinearStructuredOutputMachine? I think most primal solvers will need this, especially the online solvers that you are going to implement in the second phase of gsoc 2013.\n. why this change? it's still grid, right?\n. maybe run for slightly larger instances\n. !is_connected\n. typo\n. !m_msg_order.empty() ?\n. why do you need this Boolean vector to store whether a node is the root?\n. !dset->get_connected()\n. false instead of 0?\n. num_vars is probably a better variable name, no?\n. (is_root[ni]), similarly for anything that might possibly follow\n. maybe consider using enums for node_type and parent\n. if it is a stack, why do you use a vector?\n. enums would hopefully increase the readability of this code\n. q_v2f? r_f2v below?\n. maybe consider having m_fw_msgs and m_bw_msgs as std::vector< Eigen::VectorXf > ? Would probably make some of the expression quite a bit more understandable. Or alternatively use the shogun vector alternatives\n. why m_logZ? If it's argmax, call it that way\n. typo\n. call differently, say min_energy, map_energy (check direction/sign!), but not m_logZ\n. it's ok, we can leave it like this. But I feel that we have vector implementations with + overloading for a reason.\n. ok, but then maybe include a short remark about it\n. just makes the ifs later on in the code hard to parse.\n. Do you need this function?\n. I would get rid of the copyright and possibly just mention that the code is inspired by our SGD matlab implementation.\n. Remove this function if you don't have a real implementation for it, also the comment uses a notation that is not really explained and probably mostly makes sense in the context of our BCFW paper. \n. Is this function actually used?\n. see above.\n. see above.\n. see above.\n. for sgd you probably don't compute the dual, so get rid of it, and it is hence probably only 2x slower.\n. ",
    "coveralls": "\nChanges Unknown when pulling f11468442c78d95adae9c447a7f6d74158ce2bf3 on lambday:feature/log_determinant into * on shogun-toolbox:develop*.\n. \nChanges Unknown when pulling 9977edbbdb3405ee9b1c5667f41b55f0d1ec540a on gsomix:feature/linereader into * on shogun-toolbox:develop*.\n. \nChanges Unknown when pulling d393aaf5b5004ee6926fdf49d74457ef9a1bbe43 on lambday:feature/log_determinant into * on shogun-toolbox:develop*.\n. \nChanges Unknown when pulling d393aaf5b5004ee6926fdf49d74457ef9a1bbe43 on lambday:feature/log_determinant into * on shogun-toolbox:develop*.\n. \nChanges Unknown when pulling bac3909813fbb5d6bb899c4fbc436e40949c0fe6 on zhengyangl:develop into * on shogun-toolbox:develop*.\n. \nChanges Unknown when pulling c64978c1cb0c45652907d3f93fc7c804d70d5bed on hushell:develop into * on shogun-toolbox:develop*.\n. \nCoverage remained the same when pulling e771a08190a013e951afeb8c4855ec04d4151413 on pl8787:develop into c6eb04c7c72b6f88a91cea501469663ff9eb4579 on shogun-toolbox:develop.\n. \nCoverage remained the same when pulling 27442a1ef06aea0051d6d476fe4c28227afb3bb6 on mazumdarparijat:pca into c6eb04c7c72b6f88a91cea501469663ff9eb4579 on shogun-toolbox:develop.\n. \nCoverage increased (+0.4%) when pulling b7cf615c5de6e33ffb017863c33fcb941824bac0 on Saurabh7:regressnb into 569c92e239edbf93b5e3344b417e4788d2682a61 on shogun-toolbox:develop.\n. \nCoverage increased (+0.0%) when pulling 358fe87eabdaefcaf0d8a82d53b4abd5481e4e56 on yorkerlin:variational-example into a5e9afeb3873f08b890825100b841bf82a7c3ff5 on shogun-toolbox:develop.\n. \nCoverage decreased (-0.04%) when pulling 851a0bea670fa93d7a8e174fd309ec9470805454 on yorkerlin:develop into a5e9afeb3873f08b890825100b841bf82a7c3ff5 on shogun-toolbox:develop.\n. \nCoverage increased (+0.03%) when pulling d8b345ac509034f8605201255658b728a52115c4 on lambday:develop into 40e6b267ba762ea92d9487d64ccff08e2397e7f9 on shogun-toolbox:develop.\n. \nCoverage increased (+0.05%) when pulling 2f9a7f70fd4db2cd878b207ae969a3d79f6654cd on abinashpanda:develop into 12dd1a39c5407204b5bc2aac2dd409cf140ea89e on shogun-toolbox:develop.\n. \nCoverage increased (+0.0%) when pulling c1550fcb06fe1b26e395f05947108cdd36f730d3 on lambday:feature/btest into c12795348f8b0d0d3e1e122fdc6c590a5e20c746 on shogun-toolbox:feature/mmd.\n. \nCoverage decreased (-0.02%) when pulling 26d8578f79ec70ce553c177d848a63f1bfa020a7 on mazumdarparijat:documentation into 8d319d5cebb7da19dc3aed86a4b25702ebc2c8be on shogun-toolbox:develop.\n. \nCoverage decreased (-0.02%) when pulling 6977ff89a3d402cc81baa20f868c67a6848aa66e on mazumdarparijat:rf into 8d319d5cebb7da19dc3aed86a4b25702ebc2c8be on shogun-toolbox:develop.\n. \nCoverage increased (+0.01%) when pulling 44f064bb9f957d289924703f2d0b6738e46bcbe8 on abinashpanda:hashed-mutlilabel into 31f5609f7a7345ca05b5c1f8c7425236da2270df on shogun-toolbox:develop.\n. \nCoverage decreased (-0.02%) when pulling e4bc1afdc4e8edd0bb6f5a64a6269ec2138b3908 on kislayabhi:eigenize_LDA into b4b96d1741e9c8dc841cab5930c7dbb451567df8 on shogun-toolbox:develop.\n. \nCoverage increased (+0.01%) when pulling a1a0a0d06e3bd92b116eba057416f4f01462e56a on kislayabhi:eigenize_LDA into 7b2e398d04aec3b7d0dd44440702707e3ac027fe on shogun-toolbox:develop.\n. \nCoverage increased (+0.01%) when pulling d28aea11007a5c2a8511d7f6d8f8791719a1d9f2 on Jiaolong:graph_cut_example into 7b2e398d04aec3b7d0dd44440702707e3ac027fe on shogun-toolbox:develop.\n. \nCoverage increased (+0.03%) when pulling e01094895efa62156d1c1d34e91fd293f58fd7c0 on mazumdarparijat:gd into 804bb9ffc3a741b61bdc324d4911160de9e0afd3 on shogun-toolbox:develop.\n. \nCoverage increased (+0.07%) when pulling 356fabd8901a32f7c9012d24de23dbee167cd8d8 on khalednasr:rbms into 764d19280d2e0d338f40e0ba7b771a0bc6b5bab1 on shogun-toolbox:develop.\n. \nCoverage increased (+0.01%) when pulling a686c4b73b0659a40d214c59c2b378d5258653a9 on mazumdarparijat:gd into b4b96d1741e9c8dc841cab5930c7dbb451567df8 on shogun-toolbox:develop.\n. \nCoverage increased (+0.01%) when pulling a686c4b73b0659a40d214c59c2b378d5258653a9 on mazumdarparijat:gd into b4b96d1741e9c8dc841cab5930c7dbb451567df8 on shogun-toolbox:develop.\n. \nChanges Unknown when pulling 9ee0841757b179169a46d32be91c2464aa974553 on lambday:feature/btest into * on shogun-toolbox:feature/mmd*.\n. \nCoverage increased (+0.02%) when pulling f5206d3d77fd8707e201ef9374017c39426416bc on hushell:bcfw into 9934205660da781c377a5715b58c240f54f2e3fa on shogun-toolbox:develop.\n. \nCoverage decreased (-0.05%) when pulling 224f700c61787bce098784279fe867688743499e on lambday:feature/selection into 7d792c99f58b5d3296b4533f0926b9f753d5e47d on shogun-toolbox:develop.\n. \nCoverage increased (+0.01%) when pulling df8c28afffc96a37bb9326892085761f4b9becc7 on mazumdarparijat:kdensity into 45fd6dbb1abbf3336e0384acf42b9fb05fc3f899 on shogun-toolbox:develop.\n. \nCoverage increased (+0.01%) when pulling e5259861827723c456325a16cffe4adfffe57519 on yorkerlin:develop into 45fd6dbb1abbf3336e0384acf42b9fb05fc3f899 on shogun-toolbox:develop.\n. \nCoverage increased (+0.0%) when pulling e5259861827723c456325a16cffe4adfffe57519 on yorkerlin:develop into 45fd6dbb1abbf3336e0384acf42b9fb05fc3f899 on shogun-toolbox:develop.\n. \nCoverage increased (+0.01%) when pulling 7861d9b93243d87b9737700ab38e6cc34828fcf0 on tklein23:undo_sgrefobject into e5c14dcb238c977810eedd1b0bda1875e5666041 on shogun-toolbox:develop.\n. \nChanges Unknown when pulling b37db3c7ebd71923b5b632271c957f9b07e56b83 on Ialong:notebook_fix into * on shogun-toolbox:develop*.\n. \nChanges Unknown when pulling 5644a5bc26c1519f33840e2a823411c17648559c on sanuj:develop into * on shogun-toolbox:develop*.\n. \nChanges Unknown when pulling b35b330f3b8afa21588e24c8057acba6ba75430b on sanuj:develop into * on shogun-toolbox:develop*.\n. \nChanges Unknown when pulling 28313ebcb4782eb9cd26bc6abb16fbca77a7750f on Saurabh7:kmeans into * on shogun-toolbox:develop*.\n. \nChanges Unknown when pulling 8a4d1a4fa14d954f8f91588a294f3593879e930b on srgnuclear:ioincludes into * on shogun-toolbox:develop*.\n. \nChanges Unknown when pulling 9b426d35b797213e1e920b0bc4634231768c1469 on sanuj:feature/SGVector_cleanup into * on shogun-toolbox:develop*.\n. \nChanges Unknown when pulling cfbad41055cb5e463032288dc7578759509f2d91 on Jiaolong:GEMPLP into * on shogun-toolbox:develop*.\n. \nCoverage decreased (-0.01%) to 74.85% when pulling bd309adc131913250674630c9f5ee187596b4b08 on yorkerlin:GPInference into 612bd40a0982679dfae62ae225440bf7de0f0da6 on shogun-toolbox:develop.\n. \nChanges Unknown when pulling aaaaaf3c1791c1dc7a5fde9ca3773cf2c2bd55f5 on lisitsyn:refactor/hashed_to_separate_dir into * on shogun-toolbox:develop*.\n. \nChanges Unknown when pulling b4454de3c6a0687a26e4c4dc7d8b0cc8aa92b71f on yorkerlin:develop into * on shogun-toolbox:develop*.\n. \nChanges Unknown when pulling b4454de3c6a0687a26e4c4dc7d8b0cc8aa92b71f on yorkerlin:develop into * on shogun-toolbox:develop*.\n. \nChanges Unknown when pulling 6fb5c02d8bf9e4fb9635d7e890c86f9b9b6d5a0f on yorkerlin:develop into * on shogun-toolbox:develop*.\n. \nChanges Unknown when pulling 7060724312e0e0da21a3cd93cfbbae89053e4f8f on yingryic:SparseMatrixUniteTest into * on shogun-toolbox:develop*.\n. \nChanges Unknown when pulling 2bcd224159afa766b19a1bc601b68d8f0fe800c6 on yorkerlin:develop into * on shogun-toolbox:develop*.\n. \nChanges Unknown when pulling 58afde5aadf125e43664b5e4bc468e8f315c353d on yorkerlin:develop into * on shogun-toolbox:develop*.\n. \nChanges Unknown when pulling 9e03a08c71f08eaaf4c8ffa25d3d5edc65427909 on yorkerlin:notebook into * on shogun-toolbox:develop*.\n. \nChanges Unknown when pulling 9e03a08c71f08eaaf4c8ffa25d3d5edc65427909 on yorkerlin:notebook into * on shogun-toolbox:develop*.\n. \nChanges Unknown when pulling 9e03a08c71f08eaaf4c8ffa25d3d5edc65427909 on yorkerlin:notebook into * on shogun-toolbox:develop*.\n. \nChanges Unknown when pulling b2e0409398f1350c821c54e1dc87e528ec3eb1d3 on yorkerlin:notebook into * on shogun-toolbox:develop*.\n. \nChanges Unknown when pulling b2e0409398f1350c821c54e1dc87e528ec3eb1d3 on yorkerlin:notebook into * on shogun-toolbox:develop*.\n. \nChanges Unknown when pulling b43b2d08f7abbf669a428cc66955c0f925ead1e5 on lambday:develop into * on shogun-toolbox:develop*.\n. ",
    "sdvillal": "I'm not trying anymore either, but here is what happened.\nJust download mosek 7 and try to compile shogun against it. Running configure we get this error:\n./configure-19368-25333.cpp:6:44: error: too many arguments to function \u2018MSKrescodee MSK_makeenv(void**, const char*)\u2019\n  MSK_makeenv(&m_env, NULL, NULL, NULL, NULL);\nIf we edit \".configure\" (substituting \"MSK_makeenv(&m_env, NULL, NULL, NULL, NULL);\" by   \"MSK_makeenv(&m_env, NULL);\") to account for the new mosek API, mosek is detected. But obviously shogun won't compile or work with mosek until the API changes are taken into account everywhere.\n. ",
    "mazumdarparijat": "Hi Sir,\nI have added the feature to initialize centers manually. I have sent a pull request as well. Actually I am contributing to open source for the first time. I hope that I have done everything the right way but chances are high that I might have done something wrong. Please have a look at the pull request and let me know about the reviews.\nRegards.\n. Hi,\nI have finally finished implementing kmeans++ and its graphical python example. Its been a long time now, can I still send PR?\n. Hi Sir,\nI don't see any folder for clustering in tests/unit/ . I plan on adding one and putting the unit test for kmeans in it.\nAlso for the unit test, I plan on using four points say (0,0) , (0,10) , (2,0) , (2,10) [ie. a rectangle]. Running the code for 2 cluster centers and initializing with a local minima say (0,5) , (2,5) [or with (1,0) , (1,10)] should give the same thing as result. Is this test ok?\nAlso, I think the int32_t dimensions is never set to the correct value (its always 0 as set in init() function). This also has to be addressed I think. \nRegards.\n. ok done.\nOn Mon, Nov 11, 2013 at 7:22 PM, Fernando Iglesias <notifications@github.com\n\nwrote:\nAll right, this looks pretty much ready now! Please, just remove the\ncommented SG_UNREF and if you can issue a PR with only one commit.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/shogun-toolbox/shogun/pull/1752#issuecomment-28200953\n.\n\n\nPARIJAT MAZUMDAR\nJUNIOR UNDERGRADUATE\nELECTRICAL ENGINEERING\nIIT DELHI\n. I have added the changes suggested. Please have a look at it and let me know if you have additional comments.\nOnce its final, I can issue a fresh PR with one commit. Regards.\n. I have updated the PR. Please have a look. Regards.\n. Let me issue a fresh PR with single commit now!\n. ok. I have created a new branch and sent a separate PR. But I am trying to understand how can someone else's commits be part of my PR. Is it because these were merges?\n. @iglesias this is the new PR. I created a new branch. This has just my commit. :)\n. Thanks to you actually! you had to work equally hard to point out big and small errors. hopefully next time I wont make most of them. :)\nAlso, I wanted to discuss about fast kmeans algos like mini-batch kmeans or elkan's kmeans. Do you think implementing these algos will be useful?\n. Oh God! Actually I worked out this example initially in the python terminal and then just put the commands down in a file. For testing the changes made in KMeans source I used an almost equivalent cpp test code which I made. Basically long story short, I never actually ran the python example file. Big mistake! \n. I mentioned about this to @iglesias but we didn't discuss much. \n. #1818 \n. yes sure. Already on it actually. Unit test is a bit tricky in this case because of the randomness of the algorithm. Let me try.\n. Hi, Sorry I couldn't follow the discussion. Are we supposed to create a new class for these fastKMeans methods ie this mini-batch kmeans and also elkan's kmeans which I have in mind?\n@karlnapf I have already tested by hand and results seem to be tending towards global minima always. The clusters formed are perfect but the cluster centers are never perfect (just tending towards perfect value). \nI feel, if we compute the mean of each of the clusters as an additional last step, we could do away with the small error in cluster center positioning. But this is just my opinion and not there in the paper.\n. @karlnapf I think this is the best way to do it (as of now atleast). The other way to do it would be to switch ref-counting off for the initial data matrix which is possible in C++ but not in python (i think). Actually what is happening is that due to ref counting, the entire memory block corresponding to data matrix is freed. But part of it was already freed due to reallocate. So, valgrind shows invalid free error. I also checked SGReferenced data, there is nothing to switch off ref counting after object is created (AFAIK). If you have any idea plz share.\n. @karlnapf Actually the SGMatrix member of CDenseFeature called feature_matrix, itself is a shallow copy (or copy by value) of original data matrix. Its because its copied over '=' sign (line number 300 in DenseFeatures.cpp)\nfeature_matrix = matrix; \nThe problem with shallow copy is that 2 pointers (matrix pointers in original SGMatrix and the feature_matrix SGMatrix in CDenseFeatures) point to same memory block. You use one pointer to deallocate (or reallocate) memory, the other pointer is left pointing to a deallocated memory causing problems later on!!\nI can change this to make a deep copy of original matrix (ie. it will create a local copy of memory block to store matrix) which would solve the issue. But if we are into creating a local copy, it would be better if we do it for the this PCA method only, like its done in kernelPCA (ie not in-place operation). Sure it uses more memory, but on the flip side it also preserves the original data matrix (which is helpful for user because he can reuse it in other places in the code). Just my opinion here! \nThis memory thing is awsome stuff! Got a lot of my concepts clear. :)\n. While the first option is an excellent idea, it would require a very major overhauling of the entire code I think! There are a lot of places where mat.matrix is used. \n. @karlnapf thanks a lot! :)\nGive me 2 days. I have project presentations day after tomorrow, so I need to focus on that a bit. Completing this is my top priority after that. Apologies in advance. \n. @karlnapf done! have a look please.\n. @karlnapf not a problem! :)\n. @karlnapf tuning PCA should be the next stop. I messed up a bit in this one I think, next one should be cleaner. :)\n. @karlnapf :  ya sure! let me first finish off this pca tuning task and I will update these things together.\n. @karlnapf @iglesias Try this gist to recreate the problem. Do you get my point? \n. @iglesias have a look please. My point is if number of features is greater than number of vectors, the stopping criteria should change. I have put links to other standard implementation of lars which I referred to.\nI can also add unit tests (for both cases n>p and n<=p) to compare shogun's solution with the solution provided by the standard implementations. Let me know if that is ok!\n. @karlnapf @iglesias thanks!!\nI will add the unit-tests then and also update the news.\n. @karlnapf @iglesias sorry for the delay! Had to be away from keyboard yesterday.\nI have added the unit-tests. I used the example_lars.m from the MATLAB implementation. I copied the states of input matrices and compared the states of output matrices at every step. Please have a look at it. \n. lmnn_modular is still failing in python3. Thats wierd. Let me check!\n. @karlnapf @iglesias I think lmnn_modular is still failing because one of the lines in data got corrupt while updating last time. I have sent a new data PR. That should solve this failing test hopefully. Please merge the data PR and let me know. I will update the data submodule.\n. Thanks @iglesias let me update data submodule!\n. @iglesias @karlnapf https://s3.amazonaws.com/archive.travis-ci.org/jobs/19966205/log.txt this failure is probably unrelated to the changes I have made. How did it come in? Is it a random error or something?\n. Thanks @iglesias but I see it still fails. https://travis-ci.org/shogun-toolbox/shogun/jobs/19966205\nThe test passes locally though. I am not sure what is the problem here.\n. I think I have run out of ideas now. I could not find any link with the changes I have made.\n. Can't stop smiling! FINALLY!! Thanks @karlnapf @iglesias . Let me address the remaining comments and get back.\n. @karlnapf @iglesias Could you please check 'libshogun-library_mldatahdf5' test? Even buildbot cries that it couldn't open data repository 'australian' !! This error got fixed in Travis on  its own.\n. @iglesias We shall make it possible to access kd-tree/ball-tree from knn. I will have to look at the knn code. Once the integration is done, we can compare kd-tree and cover tree. I see that cover tree implementation is infused with the knn implementation. There is no separate class.\n. ohk! I will look at this after Aug 5. Not as part of GSoC though! ;-)\n. @karlnapf yup I see. But on the flip side its also important to put as complete picture as possible. I think there is a tradeoff b/w the two. :)\n. @iglesias @vigsterkr Travis should be green now! Ready to merge?\n. GREEN!\n. @iglesias thanks! let me send in the other part (ie the id3 algorithm itself) as a different PR. :)\n. @karlnapf Actually the starting point had been @monicadragan 's earlier PR. I had to re-implement most of the things (actually almost everything except maybe the method names!). But still her PR was a good starting point atleast to get an idea of the algorithm. Lets wait for her reply maybe for sometime? \n. @iglesias @karlnapf There is this pretty standard example on 'whether to play tennis' that's used everywhere. Find it here. I plan on putting a unittest based on that. That should serve the purpose?\n. Thats what I meant in the first place. My point was that I don't see any advantage in calling the entropy function than writing 1 line math here. But I went ahead and updated this anyways. Sending in the updated commit now. \n. Also the log is updated from base 2 to base e. :)\n. I see you have a very good point! :) I'm satisfied.\n. @karlnapf Thanks! Let me address your comments. \nThere is actually 1 case missing in this unit-test code which I think I can tackle by adding 1 or 2 training vectors more to this. Let me try. About the notebook, I already have a nice real life example in mind related to gene/genetics. Lets see if it works out. :)\n. @karlnapf @iglesias I have added all your comments hopefully. I also went ahead and updated the news (yay!). If you have additional comments please let me know. \n. @vigsterkr Stochastic gradient boosting (CStochasticGBMachine) is already in. adaboost can also be accessed through the the same class just by supplying exponential loss function. But I think it will be good to have a derived class for adaboost. This is not done yet. Benchmarking is also not done yet.\n. @iglesias @karlnapf Please have a look!\n. @iglesias @karlnapf I added method for decision tree pruning (reduced error pruning specifically). Please have a look!\n. @karlnapf I couldn't see the failure log here. Let me try and figure out whats wrong.\n. @karlnapf won't just adding a method called replace_subset()  in the present class serve the purpose? Or am I missing something?\n. Well, I was thinking about adding it in CSubsetStack but from my limited perspective I see no harm in adding it in CFeatures as well if its more useful in some way.\n. @iglesias Completely my mistake!! sending in the fix in a moment. \n. @iglesias this fixes the leak. Apologies again.\n. @karlnapf thanks! :-)\n. @iglesias There are actually 2 problems that I see here. One is that I need the tree structure to be able to print the tree. In io its not accessible I think. How about creating a treeIO class derived from CTreeMachine? But again this brings us to the 2nd problem that for ID3 I am using ID3TreeNodeData structure and its members are printed. For other tree algorithm, some other structure for data may be used. So, I cannot think of a generic way to implement the print function that would work for all kinds of tree algorithms. Did I make some sense?\nSorry for late reply!\n. @karlnapf Ya you said same thing once before as well! From next time I will try to keep PRs as short as possible. In fact, I have already sent a new PR and its quite small. Please have a look at it :-)\nThanks Heiko.\n. @iglesias I have just added missing values thing. This is a short PR and hope its easier for you than last PR. :-)\nAs usual, I am hoping for valuable suggestions from you.\n. @iglesias I also added the tree pruning feature to this PR itself. Hope its still not too messy to be reviewed yet. But in case you feel so, let me know. I can do away with the last commit temporarily. \n. @iglesias I updated the PR. Hopefully I have addressed all your comments so far. I also merged multiple commits into 1.\n. @iglesias updated C4.5.\n. @iglesias I have added empty class structure for CART. Please have a look at it. I can go ahead with implementation once this is merged. Thanks!\n. @iglesias I have added training method for CART. Please have a look at it. This PR is not merge-able because it doesn't have unittests. However, once this part is ready, I plan on adding apply methods and along with that unitests.\n. @iglesias I have fixed the existing unittest! I must say that though the fix is very simple, but I really got tired finding the problem. Phew! ;-) \nThe mock test was also a straw man, so I added another unittest to it. Please have a look!\n. @iglesias Please have a look!\n. @iglesias I have addressed your comments. Please have a look!\n@karlnapf I have updated the viewer link to include ID3 outputs as well. (Pardon me, earlier I created only the C4.5 outputs and not the ID3 outputs to keep things compact.) Please have a look! \n. @iglesias Please have a look. Thanks!\n. @iglesias Pardon me for getting back late and for my apparently really bad understanding of the usage of articles!  I have addressed your comments please have a look. Thanks!\n. @iglesias Please have a look!\n. @iglesias Please have a look. Thanks! \n. I have also sent a corresponding PR to shogun-data. Once you merge that, I can update data submodule.\n. @iglesias thanks for merging the data. I have updated the submodule. :-)\n. @karlnapf @iglesias Thanks! :-)\n. @iglesias Please have a look. Thanks!\n. @iglesias @vigsterkr  Sorry for the delay. Let me update this now.\n. Travis, what say you now?\n. @iglesias tests finally pass! :-)\n. Yes! And still I am not entirely convinced about why would an overloaded constructor cause such a failure. Anyways!\n@iglesias @karlnapf Thanks for helping!\n. @iglesias Updated! :-)\n. @iglesias Please have a look. Thanks!\n. Oh no! Thats a mostake on my part. A big one considsring that we are bogged by performance issues. There should be a break! Let me add this. \n. @iglesias No. I'll keep trying in between. \nI'm trying to complete the Random Forest notebook by tonight. We can then start kde from tomorrow.\n. @iglesias smallest PR ever! :-)\n. @iglesias single tree kde is also ready. But I had to make a few changes in the NbodyTree class. So, once you merge this, I will have to resolve some rebase conflicts locally before sending in the new PR. Shouldn't be very difficult though! \n. @iglesias I got involved in some personal work for the past 2-3 days. I have a paper to submit by 30th actually.I have started working on it today. Apologies for the delay!\n. @iglesias Thanks a lot! I actually need good wishes on this. :-)\nI have started working on the notebook today. I will try to complete it by end of tomorrow. Then GMM!\nAug 5th is also just about 2 weeks away.\n. @vigsterkr @iglesias @karlnapf This should fix the failing PCA unittest in bsd bot!\n. @vigsterkr seems like travis as well as my machine both have lapack installed. That's why the test was not failing in any of these.\n. Had I known my machine was also using LAPACK, we could have solved this a bit faster!\n. we have the consent of travis!\n. woohoo! solved.. :-) Check logs here\n. @iglesias @karlnapf Could you please have a look at the class structure? It's more or less based on what we discussed. So, basically there is a CMixtureDistribution base class which is for all distributions compatible with mixture model. The speciality of classes inheriting from this class is that they have an additional needed method update_params_em which, when supplied with belongingness values and data points, updates the parameter values of the component distribution as part of the maximization step in EM.\nThere is a CMixtureModel class which takes in a dynamic array of CMixtureDistribution objects and their weights and computes the mixture model.\n. @iglesias Please restart Travis. There is some weird clang error. \n. @iglesias I CMixtureDistribution is a special part of CDistribution which is compatible with CMixtureModel because the member classes of this special part have an additional required method update_params_em(). This method is called during the maximization step in EM for mixture model. Since not all CDistribution members like HMM, KDE etc require this additional method, I created this special part called CMixtureDistribution. So, what do you think?\n. @iglesias SG_NOTIMPLEMENTED is also a valid way to do this. Let's wait for @karlnapf 's comments for sometime.\n. @iglesias updated! good to merge?\n. @iglesias @karlnapf I have added  the base class for EM, please have a look. The main idea is to store the all the data required by the expectation step and maximization step in a structure data.\nI will inherit a class called EMMixtureModel which will specialize this class for mixture models. In this specialized class I will specify the template structure.\nThere may be other good designs like defining the expectation step and maximization step in the target class itself and then passing that class as a parameter to the EM iteration function (which will be static is which case). Let me know what do you think!\n. @iglesias are we going to expose EM directly? I thought it is only for implementation in mixture model or other classes....\n. Hi Sir,\nThis might be a very silly question but everytime I make these changes do I\nhave to send a new pull request? Is there anything in github that allows us\nto modify the old pull request?\nRegards.\nOn Mon, Nov 4, 2013 at 2:16 PM, Soeren Sonnenburg\nnotifications@github.comwrote:\n\nIn src/shogun/clustering/KMeans.cpp:\n\n@@ -63,7 +79,8 @@ bool CKMeans::train_machine(CFeatures* data)\n    for (int32_t i=0; i0) clustknb(true, mus_initial);\n\nspace between if and ( and the next statement on a new line please\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/shogun-toolbox/shogun/pull/1748/files#r7395307\n.\n. ok thanks.\n. ok sure. Let me try. Thanks.\n\nOn Mon, Nov 4, 2013 at 9:55 PM, Soeren Sonnenburg\nnotifications@github.comwrote:\n\nIn src/shogun/clustering/KMeans.cpp:\n\n@@ -63,7 +79,8 @@ bool CKMeans::train_machine(CFeatures* data)\n    for (int32_t i=0; i0) clustknb(true, mus_initial);\n\nfor the final PR please provide us one commit only. you can use\ngit reset --soft \nto get all your commits back to non-committed. Then just git commit -a\neverything\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/shogun-toolbox/shogun/pull/1748/files#r7404479\n.\n\n\nPARIJAT MAZUMDAR\nJUNIOR UNDERGRADUATE\nELECTRICAL ENGINEERING\nIIT DELHI\n. Dear sir,\nI have written unit test for manual initialization of centers. Please have\na look at it at ;\nhttps://gist.github.com/mazumdarparijat/7360379\nI am not very confident if its correct.\nRegards,\nParijat\nOn Mon, Nov 4, 2013 at 10:31 PM, Parijat Mazumdar <mazumdarparijat@gmail.com\n\nwrote:\nok sure. Let me try. Thanks.\nOn Mon, Nov 4, 2013 at 9:55 PM, Soeren Sonnenburg \nnotifications@github.com wrote:\n\nIn src/shogun/clustering/KMeans.cpp:\n\n@@ -63,7 +79,8 @@ bool CKMeans::train_machine(CFeatures* data)\n   for (int32_t i=0; i0) clustknb(true, mus_initial);\n\nfor the final PR please provide us one commit only. you can use\ngit reset --soft \nto get all your commits back to non-committed. Then just git commit -a\neverything\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/shogun-toolbox/shogun/pull/1748/files#r7404479\n.\n\n\nPARIJAT MAZUMDAR\nJUNIOR UNDERGRADUATE\nELECTRICAL ENGINEERING\nIIT DELHI\n\n\nPARIJAT MAZUMDAR\nJUNIOR UNDERGRADUATE\nELECTRICAL ENGINEERING\nIIT DELHI\n. I am sorry, I did not get exactly which space. If its the one in between 2 constructor definitions (which is line 42 in the above code) , I think I see that in between all successive definitions of constructors and functions.\nRegards.\n. ok. My bad!!\n. the file commited to my repo does not show indentation problem. But it does here in the diff. What might be the problem. Have a look here please:\nhttps://github.com/mazumdarparijat/shogun/blob/kmeans/tests/unit/clustering/kmeans_unittest.cc\n. ok yes. there were some issues with tabs and spaces. I made it uniform.\nAlso, is there any way I can run unit test for kmeans only and see if its working as expected? Maybe using ctest or something?\n. I have passed on default false to use_kmpp in the .h file . Is it still required to initialize in init() ?\nThere is error in Travis build here because of an extra ')' which might have got added during my final touches. I have fixed it.\n. Sir, Actually my initial version used SGVector mainly because my earlier commit of set_initial_centers() returned SGVector. But while rebasing I figured that KMeans has be refactored (commit by sonney2k) during which SGVectors had been changed to SGMatrix. So I finally changed to SGMatrix. \nLets discuss this. \n. ohk! alright.\n. Oh! I inferred something else altogether! Let me change it.\n. Sir, I am actually trying to use the distance function to calculate distance. Please see line 487. I guess this will not be possible without creating features. \n. actually in my editor indentation looks correct at first glance! but actually there are spaces before these 2 lines and tabs before all other lines. Thats why it looks different here. Let me fix it. \n. ohk! Alright I will correct this. \nPlease have a look at other parts of code as well\nbtw, why is travis ci build failing? I tried to debug using logs but found that tests are failing at places completely unrelated to KMeans. \n. noted already!\n. Actually these parameters have to be set by users compulsorily. In case the user forgets to do so, these values will ensure that an exception is raised. Unless these values might cause some problem somewhere, I think its ok to keep it that way?\n. 2 reasons:\n1. I wanted to pass distance in its original form (without its rhs replaced) to minibatch and lloyd training methods. IMO, this makes inputs to these methods more logical. \n2. rhs_mus was not required to be passed. I created a local rhs_mus inside the set_initial_centers method.\n. @iglesias maybe because with increasing dimensions, the mixing of data points of 2 classes of Iris increases (near the boundary).  This mixing is least along the principal dimension chosen by PCA. While this is a plausible explanation, I am not very sure if this is the right one!\nShould I put this explanation in the notebook?\n. @karlnapf @sonney2k Yup I can do away feature_matrix_centered. That should resolve the memory issue? or what else would you want me to get rid of?\n. Hi @karlnapf, \nNo this is not transpose. Let me explain with an example. Suppose m was 4x50 (4 num_features and 51 num_vectors). Let our target dimension be 2. So we want a 2x51 matrix. But in the array m.matrix matrix elements are arranged in a column major format. So, in the final matrix we need the 1st 2 elements and then we dont need the next 2 of m.matrix array. Then again we need the next 2 elements and dont need then next 2 of them. In this part of code, I rearrange the array in such a way that all the elements needed are blocked together in the left part of array and then cut out the right part of array. Now that I have the array with needed elements only, I just change num_rows and num_cols of m to get the final matrix. This works for all cases!\n. @karlnapf the last commit was not tested by travis. Its in grey! I think its waiting for the PR in shogun-data to be pushed first. It should be green this time. After all the mess I have already created, I checked very thoroughly this time. \nOnce you push the PR in shogun-data, please force travis build on this again. Thanks.\n. yes sure!\n. Actually I intentionally did this because its easier to read code this way. Never mind, let me change it now. :)\n. @karlnapf I realized this while trying to change this part. Only processing feature matrix part is redundant. matrix multiplication call by eigen3 are different for the two (one uses block on same matrix m other strores result in a new matrix ret). Similarly, set_feature_matrix call is also different (sets m in one case ret in other). Only the processing feature matrix part is common but taking it out puts too many if-elses in between. So, IMO there is no point to make the code complex just to reduce this single redundancy.\nPlease let me know without any worries if I am missing something!\n. @karlnapf I already added unittests for both cases in the earlier commit. :) \n. @karlnapf  sorry for quitting irc chat in between. I have a very unstable connection here now.\nI have changed this but I still dont get error locally!! Maybe I am missing something.\n. @karlnapf sorry forgot to mention it before. I tested it against Matlab implementation. I also validated the results using the brml code that you pointed to. BRML code uses SVD always but results match with Matlab implementation and our Shogun implementation.\n. To add to the previous comment, the input matrix is randomly generated in matlab and its values are copied manually.\n. @karlnapf AFAIK eigenvalue decomposition and SVD have their own advantages one is order N^3 and other is order D^3 . So its better to implement both. I will confirm this with references.\nThe 2 cases make sense. Actually when D>N, PCA gives meaningful results for target dimensions less than N because rest eigenvalues are 0. I plan on adding a REQUIRE condition on target_dims to check if this constraint is satisfied.\n. Thats exactly what I am planning to do! :)\n. @karlnapf I think I will skip doing the AUTO thing.\nPrincipal component algorithm that pca uses to perform the principal component analysis [...]:\n'svd' -- Default. Singular value decomposition (SVD) of X.\n'eig' -- Eigenvalue decomposition (EIG) of the covariance matrix. The EIG algorithm is faster than SVD when the number of observations, n, exceeds the number of variables, p, but is less accurate because the condition number of the covariance is the square of the condition number of X\nThe above is a note from MATLAB documentation. I think its better to stick to this convention (ie SVD by default) ?\n. @karlnapf There are actually 2 modes which make use of this thresh value, VARIANCE_EXPLAINED and THRESHOLD. So we can't do away with the EPCAMode input in this default constructor. So a user has to specify Mode as well as thresh value.\nThis constructor also comes in handy when user wants to set nothing and go with the AUTO method in FIXED_NUMBER mode (which is most likely). All he has to do is CPCA() or CPCA(true) [if he wants to whiten results as well].\nThe next most likely thing that a user would do is change between SVD/EVD. For that he has to do just CPCA(SVD)/CPCA(SVD,true) [the other constructor is for these calls].\n. @iglesias I have changed this to switch case. Have a look please!\n. @karlnapf Like I said before (it got hidden because of outdated diff thing maybe!), the thresh parameter is used in 2 Modes VARIANCE_EXPLAINED and THRESHOLD, so we cannot automatically choose THRESHOLD mode when thresh is supplied. We still need to ask which mode is the thresh for. Makes sense?\nPlease correct me if I am thinking of something else altogether. Happens with me often! :)\n. okay. Let me update this with the other minor things left.\n. @vigsterkr Its not late actually. I am still touching up things in this. :)\nI  just now saw that there are already some files of LanczosEigenSolver and DirectEigenSolver in mathematics/linalg . But it seems that not all methods are implemented there. As of now, we can access only max eigenvalue and min eigenvalue. We could work on adding all methods there and then use them here. But in PCA we would still like to store just the transformation matrix and eigenvalues vector as parameters.\nBut this is all gibberish from the mind of an inexperienced chap (ie me :))\nLets wait for input from @karlnapf and @iglesias on this.\n. yup! the parenthesis. Plus there were issues in other places as well like no guard in pca_unittest, Dense matrix object unittest etc. \nActually, I ran make and then went on correcting files which caused errors until there were no more errors. :)\n. my bad!\n. @vigsterkr Oh! I see. So now we know. Thanks\nBTW, I checked making w/o eigen3. No errors now. :)\n. @iglesias I will address this.\n. @vigsterkr @iglesias This was actually done to avoid implementing new Node class and Tree class. If I remove these methods, I will have implement another node class BinaryTreeMachineNode and another tree class BinaryTreeMachine. I understand that you want to go with implementing these 2 inherited classes.\n. haha.. :P \n. @iglesias actually I personally prefer this get_ and set_ thing, but it was already this way from before so I let it stay the same. More so because these methods are already used in so many different trees already. \nBut since I am already touching the other files too, let me go ahead and change the method names.\n. @karlnapf I will include your comments.\n. ok. Let me fix this.\n. I checked the patches submitted by monica. Those don't have any license. Just saying. :)\n. @iglesias I was thinking of the 80 character rule actually. umm.. is it waived in header files?\n. changed it. Single line suits me too! :)\n. @vigsterkr please see my comment/question below.\n. umm... I checked again everywhere in decision trees I find log2 is used and they still call it entropy. I guess in decision trees this is the standard?\n. @vigsterkr check here however. http://en.wikipedia.org/wiki/Decision_tree_learning#Information_gain\nBut I am no expert on this and probably it hardly makes a difference because we are subtracting two logs in final gain calculation. Should I go ahead and change to log base?\n. @vigsterkr I checked the code. it uses exp(p)*p . So p should be supplied in log. Might be that it helps for bigger numbers\n. Oh! very sorry. Let me update this.\n. @iglesias my bad! Let me update this in the next commit.\n. @karlnapf I put these to get break lines between 2 subsequent lines in brief. Currently, the ID3 documentation is weird (the pseudo-code appears as a paragraph). I don't know if this is the standard way to introduce line breaks in between.\n. @karlnapf no its not for negative. Its to stop dividing by zero when some eigenvalues are zero (or to stop final value from blowing up when some eigen value is very very low [ie 0 theoretically]) Adding this epsilon prevents these 0/0 kind of situations during whitening. :)\n. @karlnapf ahh.. I remember you pointed this last time as well and we had some discussion! :)\nThe point here is that because of the problem with in-place thing, its not convenient to use () operator (m(row,col) points to m.matrix[col*num_features+row] not m.matrix[col*num_dim+row]). Plus element by element copying is also necessary because I am accessing spread out elements in the matrix array and stacking them to the left side of the same array. I can't access them in blocks. \n. @karlnapf max number of subsets at any time will be equal to the max depth of our decision tree (<= num_features). So, it can be quite high depending on dataset (few hundreds I am guessing!). Is there an upper limit to the stack size for subsets?\nI cross-checked subset things with documentation. There shouldn't be any error. Plus, unit-test added by me is passing. The solution is indeed more elegant with subsets. I didn't know about this feature before. :)\n. Yup, it doesn't ref count the argument. So, I think, the newly created CDynamicObjectArray was never destroyed in the earlier case. Hence the leak!\nWhats weird is that I fixed leak before as well (using valgrind differently actually), but this didn't show up then. I checked using the command which was pointed out by Thoralf and this showed up.\n. @karlnapf ok sorry! let me check the discussion on UAIFile. But I saw some writer methods in CHMM class as well, and hence moved ahead with this design. \nI can create a separate class but that will have just these writer methods in it. Nothing else!\n. i quickly searched for one earlier didn't find any. But now again on looking for it, I found a fill method to do this. Let me address this and your other comments as well.\n. my mistake! I will wait for your other comments and then fix all of them together.\n. @iglesias I have added the link to documentation in brief at the end of pruning part. Should I put the link here as well? \n. yes I agree! In fact I copy pasted the top part and changed node_t to bnode_t thats it. Actually I did this part very quickly while debugging so didn't think much. Let me address this.\n. @iglesias I used int to keep things same with the feature_types vector. If I could create a vector of enum, that will be very helpful in this case!\n. @iglesias SGVector of enum gives error. undefined reference to SGVector<shogun::TFeatureType> where TFeatureType is the enum name. There should be some other way to create a vector of enum?\n. @iglesias you are right! essentially the entire train method is same. But this random part (starting at line 180) is different. And because of that the loop block gets modified slightly. Number of iteration changes and every place where I used i earlier, I use idx[i] here. But if I could create a method just for this loop part, then I could do away with the duplicity in the code blocks above and below this loop block atleast. Let me try. \n. @iglesias This condition is reached only if all the attributes we have checked for splitting this node have identical values for all available training vectors. In the parent case, we were checking all available attributes to so this case was never possible. In this case we are checking only a few attributes using which it might not be always possible to make a split (ie atleast one of the remaining attributes which we are not checking could split this node but we are not using that and hence just returning the node w/o splitting).\n. I don't think it should complain because m_machine is set in CRandomForest::init() and is never unset in between!\n. @iglesias there is no obtain_from_generic method for CDenseLabels. I have added check for NULL pointer in my latest commit, that should solve the possible coverity defect.\n. @iglesias Actually all the other Node data structures are made visible to API, so I added this one as well.\n. omg yes! It slipped through.\n. thanks!! :-)\n. @iglesias I am not sure but I think this include is for swig to make sense of TreeMachine<C45TreeNodeData> used in multiclass.i . Its not for exposing this structure to the API. Is that the case? I actually don't have much idea about swig.\n. Careless me!\n. @iglesias yes. Actually the iris CSV file contains both string and float in each line. I am not sure if I did it the right way, but I think CSVFile could not handle a mixture of floats with strings. \n. @iglesias I thought of that. But again I had to demonstrate the pruning method which wouldn't have been possible if I used the CCrossValidation class (train and apply happens internally). Maybe in case of CART I can demonstrate the use of CCrossValidation class.\n. @iglesias :-) Actually if  I start the loop from i=1, I need to put an if block around it for the case when num_vecs==1 (we would have to skip this code segment in which case). \nUsing my strategy, we no longer need the extra if block. If you feel that this looks to much 'hack-y' I can change it right away? \n. @iglesias Please ignore my comments! I had  a big confusion thats why all this fuss. I have updated the PR\n. @iglesias here the number of training samples is 100. It would be very long to write all 100 samples. Overall, since I am setting the seed, its achieving the same thing as hard coding. What do you say?\n. I was doing just that! Sending in the PR now.\n. Actually with MAX_REAL_NUMBER I can directly sort the features without the need to traverse all the data points twice which was earlier needed to separate the nan features out. This in-turn speeds up the algorithm a bit, because number of data points can be very large (in thousands).\n. yes. CRandomCART uses few of them. \n. @iglesias This is  a slightly modified version of heap which is specifically designed to keep the least k values seen so far. Firstly, this is a fixed size heap. So, when I call push in it, it doesn't add a node to the heap as a std heap would do. It checks if the new value belongs to the least k values' set and accordingly replaces the worst previous entry. \nSecondly and most importantly, since this heap deals with only a specialized case, it does the replace operation in a single pass, which any std heap would do in 2 passes (pop and then push). So, I am slightly inclined towards keeping this heap implementation.\nIdeally, this heap class should be private to CNbodyTree because it's specific to that. But I didn't see any such design (i.e. private class within class) done earlier in Shogun. So that's why I created a separate class outside.\n. @iglesias missing class docs are not on purpose. Actually implementing this heap was a diversion from the actual tree implementation, so I did it in a hurry and left the docs to be done later. As you and I can see, I obviously forgot to add docs later! let me add them.\nAlso, please tell me if I should divide the PR into parts.\n. @iglesias I can move this to CDistribution. In fact, this is a distribution indeed that we are trying to estimate. But, like here, there also I will be using just the train method from the interface. Others I will have to put as SG_NOTIMPLEMENTED? \n. No, I think not!\n. @iglesias  I have no idea about model selection. What needs to be done to support model selection apart from the obvious change to MS_AVAILABLE?\n. @iglesias I have put the kernel evaluations as static methods in KernelDensity that's why there is a need to put header in this implementation file.\n. @iglesias I just discovered that it works the other way for inline functions. You write inline keyword only during defining in implementation file. Not in the header file while declaring. Sending in the updated PR now.\n. @iglesias This is already corrected in the last commit. I actually made the correction after generating the view and then didn't update the view.\n. @iglesias belongingness values are the $$y_{i,j}$$ values here. It basically represents the probability that jth data point belongs to ith mixture component in a way. Hence the nomenclature \"belongingness\". I don't know if it is a standard name for it?\n. is it possible to sg_add a templated data attribute? I think not but maybe I don't know the right way.\n. @iglesiasg sorry couldn't reply earlier. I have very limited access to\ninternet. I will get back to addressing your comments as soon as possible.\nSorry for yhr delay again\n On 7 Aug 2014 03:22, \"Fernando Iglesias\" notifications@github.com wrote:\n\nIn src/shogun/distributions/EMMixtureModel.h:\n\n\n* of the authors and should not be interpreted as representing official policies,\n* either expressed or implied, of the Shogun Development Team.\n/\n  +\n  +#ifndef _EMMIXTUREMODEL_H__\n  +#define _EMMIXTUREMODEL_H__\n  +\n  +#include \n  +#include \n  +#include \n  +\n  +namespace shogun\n  +{\n  +template class CEMBase;\n  +\n  +/* @brief This is implementation of EM specialized for Mixture models.\n\n\nCan we get a citation for the reference you have used to implement the EM\nalgorithm here?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/shogun-toolbox/shogun/pull/2448/files#r15923397.\n. I dont think so. The CGaussian method takes CDistribution as input. I can\nchange it to CSGObject and then it might start working..\nOn 7 Aug 2014 02:09, \"Fernando Iglesias\" notifications@github.com wrote:\nIn tests/unit/distribution/MixtureModel_unittest.cc:\n\n\nSGMatrix cov2(1,1);\ncov2(0,0)=3;\nCGaussian* g2=new CGaussian(mean2,cov2,DIAG);\n  +\ncomps->push_back(g1);\ncomps->push_back(g2);\n  +\nSGVector weights(2);\nweights[0]=0.5;\nweights[1]=0.5;\n  +\nCMixtureModel* mix=new CMixtureModel(comps,weights);\nmix->train(feats);\n  +\nCDistribution* distr=CDistribution::obtain_from_generic(comps->get_element(0));\nCGaussian* outg=CGaussian::obtain_from_generic(distr);\n\n\nOut of curiosity, does it work doing directly\nCGaussian* outg=CGaussian::obtain_from_generic(comps->get_element(0));?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/shogun-toolbox/shogun/pull/2448/files#r15921044.\n. No.. thats why i said there is a need to refactor old cgaussian code... For\nnow its explicitly mentioned in the doc by the past author to free this\ncovarance matrix manually.. you can check that in the get_cov method doc.\n\nOn 7 Aug 2014 02:11, \"Fernando Iglesias\" notifications@github.com wrote:\n\nIn tests/unit/distribution/MixtureModel_unittest.cc:\n\n\nweights[0]=0.5;\nweights[1]=0.5;\n  +\nCMixtureModel* mix=new CMixtureModel(comps,weights);\nmix->train(feats);\n  +\nCDistribution*\n  distr=CDistribution::obtain_from_generic(comps->get_element(0));\nCGaussian* outg=CGaussian::obtain_from_\n  generic(distr);\nSGVector m=outg->get_mean();\nSGMatrix cov=outg->get_cov();\n  +\nfloat64_t eps=1e-8;\nEXPECT_NEAR(m[0],9.863760378,eps);\nEXPECT_NEAR(cov(0,0),0.956568199,eps);\n  +\nSG_FREE(cov.matrix);\n\n\nWhy does free have to be explicitly called here? Is the automatic\nreference counting of SGMatrix not used here?\n\u2014\nReply to this email directly or view it on GitHub.\n. Umm.. I convinced myself this way that since mixture model is solved using\nonly EM we might have methods to control em parameters in the mixture model\nclass itself. I am not rigid about it and would gladly accept suggestions.\nIf it makes a bit of a difference i can change the method name to\nset_training_parameters which can be used to set num iterations and epsilon\nOn 7 Aug 2014 02:15, \"Fernando Iglesias\" notifications@github.com wrote:\nIn src/shogun/distributions/MixtureModel.h:\n\n@@ -129,7 +130,31 @@ class CMixtureModel : public CDistribution\n         * @param index index of component\n         * @return component at index\n         /\n-       CDistribution* get_component(index_t index) const;\n-       CDistribution* get_component(index_t index) const;\n  +\n-       /* set max iterations in EM\n\nI am not sure about this design. I think these methods could be better in\nanother part. This is something we can discuss.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/shogun-toolbox/shogun/pull/2448/files#r15921158.\n. Ok yes my bad. It's for storing data required for em in mixture model.\nOn 7 Aug 2014 02:17, \"Fernando Iglesias\" notifications@github.com wrote:\nIn src/shogun/distributions/MixModelData.h:\n\n{\n-   float64_t expect_cur=CMath::MIN_REAL_NUMBER;\n-   float64_t expect_prev=0;\n-   int32_t i=0;\n-   bool converge=false;\n-   while (i<max_iters)\n-   struct MixModelData\n\nCan we get some documentation for this struct? What is it for?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/shogun-toolbox/shogun/pull/2448/files#r15921200.\n. the max value from the array is first removed, all elements are divided by the removed max element and then exponentiation is done. +1 is the contribution from the removed element in the sum. This sum is basically scaled sum (ie all elements are divided by X0). So after taking log, we add X0 again (which is basically scaling back by multiplying the sum with X0). \n. @iglesias I have worked out the math for the case for generic EM by hand. There is no particular reference for this. I can cite the pattern recognition book that you sent maybe?\n. Because it's matrix in case covariance type is FULL. I have mostly copied this part of code from GMM.cpp max_likelihood method. I start refactoring the gaussian class, I will refactor this part as well.\n. The require condition after should suffice. if features is NULL dotdata will be NULL too.\n. @karlnapf yes, I will work on replacing all this will linalg now.\n. @karlnapf actually CStatistics uses log-sum-exp with min element. I found out that it was causing some numbers in the array to become infinity, thus giving very wrong result. Using max number for log-sum-exp solves the problem here.\n. \n",
    "erip": "Not sure I understand this one -- still valid? I found examples/undocumented/{octave, python}/tools/*, but I'm a bit confused about the issue in general.\n. @karlnapf Any word on this? I've noticed that shallow_copy is used (a bit) in the code. deep_copy, on the other hand, is both unused in its current state and is used as clone. How should I proceed?\n. @karlnapf I suppose this is still open then?\n. I can't seem to find a clone method in any of the kernel classes?\n. Google has the operations research tools repo. Might be nice.\n. Should Tapkee initialize methods be renamed as well? I wouldn't imagine so because there should exist no interplay between the libraries and Ruby. Further, I don't think it's allowed as per the LICENSE file.\n. Recommend closing this. \n. This is so annoying... Travis can't handle the build, so it fails.\n. @lisitsyn When Travis builds, some of the tests will pass. When I rebuild (no changes in code), other tests will pass. \n. Ah, sorry. I'm new to OSS, so I didn't know this. My apologies. test 12 seems to fail always, and I've created an issue in rudix to address it. In this latest build, all tests passed apart from test 12.\n. Merge?\n. Yes, there's plenty of documentation. The only thing I couldn't seem to find a mention of was how to do something like sudo pip install .... I'm sure it's somewhere, but I couldn't find it. I'm no travis expert, though, so it might be easy. Definitely \"normal\" custom linux packages are easy to take care of, however.\n. I spoke with someone at TravisCI and this is what they told me in response to the sudo pip install foo question:\n\nswitch to pip install foo --user and it'll install and be available in the travis user's import path\n. \n",
    "thereisnoknife": "Does anyone had a look on this ?\n. One more push ? ^^\n. Hi Heiko, don't worry, I am about to solve this, and keep you inform. The\nthing is that the functionality I was looking is not support yet in shogun.\nI have to write it myself.\nCheers!\n2014-02-19 12:37 GMT+01:00 Heiko Strathmann notifications@github.com:\n\nAh yeah, definitely! I will put an entrance label - maybe some GSoC\nstudent solves it. I also want to look into it, but so many things to\ndo.....\n\nReply to this email directly or view it on GitHubhttps://github.com/shogun-toolbox/shogun/issues/1589#issuecomment-35489658\n.\n. Hi Heiko, don't worry, I am about to solve this, and keep you inform. The\nthing is that the functionality I was looking is not support yet in shogun.\nI have to write it myself.\nCheers!\n\n2014-02-19 12:37 GMT+01:00 Heiko Strathmann notifications@github.com:\n\nAh yeah, definitely! I will put an entrance label - maybe some GSoC\nstudent solves it. I also want to look into it, but so many things to\ndo.....\n\nReply to this email directly or view it on GitHubhttps://github.com/shogun-toolbox/shogun/issues/1589#issuecomment-35489658\n.\n. \n",
    "prthkms": "Hello, I was looking into fixing this issue. I found that the implementation of whitening in FastICA, jade and SOBI are slightly different. I am using http://theclevermachine.wordpress.com/2013/03/30/the-statistical-whitening-transform/ as a reference. I'm a newbie. I intend to apply for GSOC and get myself acquainted with the code base. Any help would be appreciated .. \n. ",
    "jondo": "Please add Random Forest to the feature matrix.\n. So shouldn't the shogun feature page be updated?\n. The link https://github.com/shogun-toolbox/shogun/blob/develop/doc/md/INSTALL.md is broken, too.\n. ... and many (or all) other links.\n. I also cannot do it - but please keep this open as a (possibly low priority) feature request.\n. I'm afraid no. Doesn't the undetected corruption show that they can just be removed?\n. ",
    "josepablog": "Yes, I encountered the problem.\nI don't know if we need the strict validation.   I think the relaxed validation is what always should be done. For example, training a binary classifier without negative examples.   The classifier is trivial, but should not fail in this case.  Right?\n. Maybe my use case is \"bizarre\", but I need to train 1,000s of classifiers.  Some of them are trivial, most of them are not.  Shogun should not break for trivial classifiers.  I think it should alert me, and allow me to proceed. \nPerhaps a flag to avoid validation, then? \n. ",
    "perimosocordiae": "Nope:\n```\nIn [1]: from modshogun import LMNN\n\nImportError                               Traceback (most recent call last)\n in ()\n----> 1 from modshogun import LMNN\nImportError: cannot import name LMNN\n```\nHowever, other classes import just fine:\n```\nIn [2]: from modshogun import RealFeatures\nIn [3]:\n``\n.PCAworks,Isomap` doesn't. I guess that means Eigen is the culprit.\nEDIT: I'm building again with the bundled Eigen.\n. Thanks, using the bundled Eigen worked for me.\nFor future reference, this is on a CentOS 6.4 machine, and I had eigen3-devel-3.0.6-1.el6.noarch installed via yum.\n. ",
    "yage99": "Exploring the same problem. Have you got a solution?\n. Maybe someone in the project can build a .mex file for the users.\nBesides: the documentation really needs update, place the problem in the install.md to avoid making users wasting time trying cmake again and again.\nAlso, I have tried using mex to compile .mex file manually, it works. I think It\u2019s better to write something like matlabinstall.md to tell the users how to install step by step.\n\u53d1\u4ef6\u4eba: Heiko Strathmann [mailto:notifications@github.com] \n\u53d1\u9001\u65f6\u95f4: 2014\u5e742\u670822\u65e5 8:07\n\u6536\u4ef6\u4eba: shogun-toolbox/shogun\n\u6284\u9001: yage99\n\u4e3b\u9898: Re: [shogun] Shogun3.0 for matlab static didn't creat a sg.mexglx file (#1777)\n@vigsterkr https://github.com/vigsterkr  Viktor, any news on the matlab with cmake?\nFor now the quickfix (unfortunately) is to use a modular interface, such as octave.\n\u2014\nReply to this email directly or view it on GitHub https://github.com/shogun-toolbox/shogun/issues/1777#issuecomment-35786913 .\n. Haha, I\u2019m not sure, maybe I can write down my steps first. Since I don\u2019t have experiences working with others in github, I\u2019m a little afraid that my process maybe too slow.\n\u53d1\u4ef6\u4eba: Viktor Gal [mailto:notifications@github.com] \n\u53d1\u9001\u65f6\u95f4: 2014\u5e742\u670822\u65e5 16:05\n\u6536\u4ef6\u4eba: shogun-toolbox/shogun\n\u6284\u9001: yage99\n\u4e3b\u9898: Re: [shogun] Shogun3.0 for matlab static didn't creat a sg.mexglx file (#1777)\n@yage99 https://github.com/yage99  great idea! would you be that someone who does it? since shogun is an open-source project anybody can and highly recommended to contribute!\n\u2014\nReply to this email directly or view it on GitHub https://github.com/shogun-toolbox/shogun/issues/1777#issuecomment-35797427 .\n. ",
    "Saurabh7": "hi,I am sorry, i missed something in the workflow(non -fast forward updates)... anyways i sent a new pull request https://github.com/shogun-toolbox/shogun/pull/1795\nAren't the runs same as repeating the procedure??\nI was looking for something to contribute to machine learning and i have a lot of free time... :)\n. Yes that does sound interesting ! I would love contributing to anything including cross validation stuff. Thanks!!\n. I have added more tests...but i cant see how the build fail is related to my commit\n. Hi, Thanks...\nso should i do the renaming and splitting ? another branch?\nAlso it would be great if you could elaborate one of those you mentioned https://github.com/shogun-toolbox/shogun/pull/1793#issuecomment-30743803 ..so i can try...  :)\n. I dont have anything in particular.... I will have look at what you suggested till then..\n. @karlnapf , hello\nlittle tweaking of an example to cross validate knn (https://github.com/Saurabh7/shogun/blob/xvalexamples/examples/undocumented/libshogun/evaluation_cross_validation_knn.cpp).... gave results like https://gist.github.com/Saurabh7/8070273 .... does it look alright?? \n. valgrind leads to  https://gist.github.com/Saurabh7/8092342 . err...and i don't know what to make of the 'Unrecognised instruction' part.\n. https://gist.github.com/Saurabh7/8142658\nfixes the knn leak , the CDenseFeatures::duplicate one now remains...\n. @iglesias valgrind gives https://gist.github.com/Saurabh7/8146989 \n. well ..actually yeah ...the only difference seems that loo doesn't take a num_subset argument since that is same no of labels, right? \nso that would mean removing build_subset implementation from the new .cpp?(after inheriting standardsplitting)\n. @karlnapf I have removed the implementation part, example seems to work ok. \n. hi,\nI was looking at model-selection, grid search,etc...but ml isnt my major field of study now so eerrr at this point Its hard to come up with something. I have a week or so of free (boring) holidays so if you guys have anything substantial i can learn or help out with,pls do tell...\n. Still stuck due to the memory leaks leaks from the copy constructor of densefeatures ..\n. @karlnapf hi,\nthanks for the review,\nI made some changes, heres the new link :http://nbviewer.ipython.org/gist/Saurabh7/9777327\nI added the nb in classification directory w/o thinking, it won't be possible to add regression to this afterwards, what do you think?\n. http://nbviewer.ipython.org/gist/Saurabh7/4880fc90381afd420bcf\n@karlnapf Thoughts on this? I have tried to address most of the issues :)\n. Yes, addressed all other comments, but there is need to modify the example since we merged a different version of the dataset.\n. latest:http://nbviewer.ipython.org/gist/Saurabh7/05c250f925937dc09548\nThere is this warning i couldn't get rid of :\n-c:2: RuntimeWarning: [WARN] In file /home/saurabh/l/shogun/src/shogun/features/SubsetStack.cpp line 162: SubsetStack::remove_subset() was called but there is no subset set.\nwill try to remove it. most probably caused by get_feature_matrix\n. http://nbviewer.ipython.org/gist/Saurabh7/b781b4e9f8f59de90cd8 updated with some examples and grid search.\n. @karlnapf \nadded a section for SVR at the bottom : http://nbviewer.ipython.org/gist/Saurabh7/f6aa7c4aa974f6a62b72\nAlso since we have lot of sections, what about having a short table of contents on top with internal hyperlinks?\n. Yes I agree, unfortunately I had fit together most of the major parts together.\nKMeans is there in https://github.com/zoq/benchmarks ,  I ran it with some of the datasets in it and got better results in comparison with scikit with the newer version for bigger of the datasets .. \nalthough I am not sure everything is a fair comparison in it right now (parameters like tol need to be set to zero in scikit) and also initialization methods. @manantomar you could have a look if you are interested :)\n. w.r.t discussion in #2987 .\n- distance computed using dot products as dist(x, y) = sqrt(dot(x, x) - 2 * dot(x, y) + dot(y, y))\n- precompute lhs and rhs squared norms\n- unit tests\n. Done. btw does travis still check integration tests ? :)\n. Ok, so should i still update the integration files ? \ncant see why travis shows fail tho\n. hmm  - http://buildbot.shogun-toolbox.org/builders/precise%20-%20libshogun/builds/1140/steps/compile/logs/stdio . Is this because of  guards for linalg ?\n. Hmm I didnt know about this HAVE_LINALG change  @karlnapf \nhttps://github.com/shogun-toolbox/shogun/commit/7339764920061d35581287f2483c24cc6187901e#diff-af3b638bc2a3e6c650974192a53c7291L760\n so its set to 1 only if VIENNACL is found ??\n. Just updated with i believe all comments, \nif looks good i could squash and force push again :)\n. @karlnapf  Both minibatch and lloyd use the same code here.\nA new internal helper method ( initialize_training()  ??) could be added to CKMeansBase to handle the repeating code, which I then call in both MB and lloyd.\nIs this what u meant? if not pls give me some more hint. \n. Yes, both are part of the std algorithm http://en.wikipedia.org/wiki/K-means%2B%2B#Initialization_algorithm\n. I didn't really add any new code and the previous CKMeans didn't have any. I came across some new creation of objects in some methods but they were UNREF in the method itself. \nStill I'll run memcheck on the libhshogun example mayb?\n. distance is inherited from DistanceMachine which i haven't touched in this PR.\n. found a performance issue with linalg::dot and sgvector for smaller dimensions. https://gist.github.com/lambday/7cbd77eb0530ad41d9f5 . So we might have to use cmath::dot for time being till we have updates as this is inside compute()\n. I wasnt completely sure of implications on other algorithms so decided to start off by keeping it optional.\n. - Decided to make precompute_xxx_squared_norms() public since we don't know how often the rhs is changing.\n  for example: \n  - if rhs changes and we end up computing pairwise distances between say all vectors of rhs and lhs this is worth.\n  - But if rhs changes quite often and we  need pairwise distances using  only some  vectors , computing squared norms each time would be inefficient ?\n. We are not calculating the whole distance matrix. It is just between 2 vectors from lhs and rhs  distance (index a, index b)\nSo if we know that either lhs or rhs or both are fixed over some number of computations we can just use rhs_norm[ index b] instead of computing b.b for every computation. That was the idea. \n. Yes, i was thinking performance wise, but now that i think of it cant see a reason not to, will change it then.\n. Do you mean combining both precompute in one method ? That would be inefficient as in KMeans where one is fixed always but the other is changing over iterations (but fixed inside one iteration)\n. So the wrapper should call one of the or both methods depending on what has changed. Am i right ?\nIf i am right : the only way to keep track of which side changed is in replace_lhs / rhs which happens to be non virtual in CDistance\n. I believe this was actually completely wrong, should be +=\n. I had a feeling something was off here, thanks .. added it to base class\n. Its necessary, thats how the centers are updated in each iteration.\n. yes for that I have to store distances in a vector though. and min does similar loops inside.\n. aah just realized the order can be opposite too, since its random , will fix this \n. should be fixed now\n. just realized while doing this , need the if block for the cluster assignment anyways ...\n. yes will add this\n. There will be no initial centers and it will resort to random initialization.\n. @iglesias It was added to base distance to make it generic and avoid calls to specific distance specialization . Depending on the derived class the precomputations are handled specifically ( e.g. squared norms for EuclideanDistance)\n. Not currently i dont think so . But this may avoid adding functionality in code that depends only on one special class.\n. This compute_cluster_variances I cannot make sense of it. Its not part of the KMeans algo ,\nBut this is leading to additional computations since this is called at end of train_machine . and causes performance hit for some case like this for dataset corel-histogram shape=(68040, 32) for k=10\n2.77 s with compute_cluster_variances\n1.31 s without compute_cluster_variances\n. I cannot make sense of this computation nor find any reference. Any hints ?\n. Yes I had tried to do this initially but no there is no get_features or set_features in CMachine\n. Just using this to skip locked training temporarily and test the unlocked version.\n. clone creates different copy for each.\nAnd since clone happens after data lock, the assert fails. Had to do this.\n. Yes it clean. Hence wondering about the Ref comments.\n. Wondering why this worked.\nhttps://github.com/shogun-toolbox/shogun/blob/develop/src/shogun/base/SGObject.cpp#L721\nclone REFs the copy. So no REF needed ?\n. Changed this btw. Thanks for explaining.\n. I think you are right, since m_store_model_features is a param, it should be same for clone. Removed.\n. Going to add more.\nWanted to make them public and unit-test.\nCan change it back later too.\n. Unit tests are valgrind clean here. \n. Linalg didnt have colwise scale so had skipped this. \n. Then I have to transpose the matrix inside. As we loop over features in here. \n. Leads to additional overhead if I save the transpose. If I use map.transpose().col(i) too many cache misses. So options might be pass Eigen things or manually tranpose SGMatrix initally or any suggestions ?\n. Or mayb use directly, no helper method.\n. Develop version: output\nwhere lapack routines are used.\nOther than that we need the transpose to extract columns and take ddots\n. Yes, this whole updating part can be done with eigen operations. Wasn't so used to eigen back then :) changing ...\nEDIT: by whole I mean this update loop here\n. This is basically integer, fixed.\n. The weight vector should be int type. This is just checking if we don't divide by zero.\n. Its not right now. I am changing it, since its number of points assigned to cluster. \n. Yes in the case if 2.1 billion points happen to be assigned to one center, it might be short :) should i change to int64_t then ? \n. yes it is. \nBut again its starts off with all points assigned to center 0. So I guess this should definintely be increased...\n. Havent checked exactly how much it helps. Its just parallelizing the dot products.\n. its set to CMath::MAX_REAL_NUMBER , an optional routine in case missing attribute values\n. The current way gets information about feature types (nominal or not) through a parameter vector and handles each feature later. But yeah the supported type is only densefeature. \n. What I meant was cases like feature1 is nominal , feature2 is continuous.\nThis assumes that string values like 'hot', 'cold' are already encoded in numerals. \nSo the idea is we should support string features and do this conversion internally ?\n. yes\n. Inside a column here, needed to check which indices are active.\n. Couldn't find a way to memset this as we need to set rows to constant (not continuous in memory)\n. No it doesn't right now.\n. No, this is protected method.\n. @karlnapf Added this to set fixed splits in xval.\n. fixed\n. I think LMNN always returns CustomMahalanobisDistance. http://www.shogun-toolbox.org/doc/en/latest/classshogun_1_1CLMNN.html#a9f96703addfc52317b1f2bb97e20a7ee\nI have mentioned it as sgclass now.\n. Yes, I was curious about this too, how eigen3 likes would fare here. I have kept the authors way of doing things for now.\nyes this could be added in some way to linalg. Could be useful if one wants to reduce arrays/vectors after working with them in multiple threads.\n. Yes this is from original code.\n. Will change this name.\n@karlnapf there is already similar method in shogun, but it uses pthreads. I have used omp here.\nWe need to decide if replace it or add this newer version.\n. @karlnapf I have updated this test\n. I removed this using CRandom instead\n. There are tests, more might be needed I will check\n. \"Train model before updating model parameters\"  .. something like this sounds good ?\n. I can select the points randomly. Although I cant see a clean way other than adding subset in meta examples.\n. Its a std vector  of vectors  right now, can be copied into and sgmatrix and extracted. Will do that.\n. Yup looks like it.\n. I am removing this too, not absolutely necessary, can discuss later.\n. ok updated\n. ah crapp :)\n. I see, CMath::randn_double gives std normal can use that.  fill_array might not be suitable for std normal I guess.\n. Yes, its completely independent, will move it higher.\n. Ah sry missed these comments, yeah we could change it to rotate_matrix(int32_t n, SGMatrix<T>& m) \n. yeah we should change it to colwise scaling. will do that.\n. Yeah you are right, will fallback to the generic method for other feature types. Thanks!\n. Hmm, actually cannot support float32  as well with this design. As I cannot make the templated get_distance_matrix method from base class virtual.\n. @lambday\nget_distance_matrix is already templated in base CDistance class. \nI could make the float64_t  version virtual and use it here, if we only need one return type. And I was thinking about using the base class method itself as the fallback.\n. Hey @lambday , Doesn't get_feature_matrix already do what we need i.e. copy only when subsets are present : https://github.com/shogun-toolbox/shogun/blob/develop/src/shogun/features/DenseFeatures.cpp#L268   ?\nI have fixed other things.\n. It is in the base class, but I guess i should add it here too :)\n. Can't operate on different types with eigen, I tried. \n. yes that will reduce one cast, will do that .\n. @lambday  Actually that would mean I have to cast LHS and RHS feature matrix to double. I think just casting the norms is better (even if we have to cast the result matrix) , memory wise ?\n. Yep, I have updated with that now.\n. @lambday yep Its possibe, I have updated, can u stop previous travis maybe?\n. There shouldn't be any runtime implications (claimed in docs), not sure if a temporary object is used.\n. @lambday What exactly happens in our linalg with eigen backend ? cant find sqrt .. can you point me?\n. @lambday  ah sorry couldn't try this out yet, might do so on the weekend.\n. Looks like this is solved.\nVery Interesting PR btw ! So we are planning to change all APIs to .fit , .predict ?. ",
    "dpo": "```\n$ file /usr/local/Cellar/hdf5/1.8.12/lib/libhdf5.dylib \n/usr/local/Cellar/hdf5/1.8.12/lib/libhdf5.dylib: Mach-O 64-bit dynamically linked shared library x86_64\n$ ls -l /usr/local/Cellar/hdf5/1.8.12/lib\ntotal 15176\n-r--r--r--  1 dpo1  admin  2307908  1 Jan 19:09 libhdf5.8.dylib\n-r--r--r--  1 dpo1  admin  4190928  1 Jan 19:08 libhdf5.a\nlrwxr-xr-x  1 dpo1  admin       15  1 Jan 19:08 libhdf5.dylib -> libhdf5.8.dylib\n-r--r--r--  1 dpo1  admin     2441  1 Jan 19:08 libhdf5.settings\n-r--r--r--  1 dpo1  admin   333252  1 Jan 19:09 libhdf5_cpp.8.dylib\n-r--r--r--  1 dpo1  admin   604552  1 Jan 19:09 libhdf5_cpp.a\nlrwxr-xr-x  1 dpo1  admin       19  1 Jan 19:09 libhdf5_cpp.dylib -> libhdf5_cpp.8.dylib\n-r--r--r--  1 dpo1  admin   114848  1 Jan 19:09 libhdf5_hl.8.dylib\n-r--r--r--  1 dpo1  admin   153384  1 Jan 19:09 libhdf5_hl.a\nlrwxr-xr-x  1 dpo1  admin       18  1 Jan 19:09 libhdf5_hl.dylib -> libhdf5_hl.8.dylib\n-r--r--r--  1 dpo1  admin    19808  1 Jan 19:09 libhdf5_hl_cpp.8.dylib\n-r--r--r--  1 dpo1  admin     7272  1 Jan 19:09 libhdf5_hl_cpp.a\nlrwxr-xr-x  1 dpo1  admin       22  1 Jan 19:09 libhdf5_hl_cpp.dylib -> libhdf5_hl_cpp.8.dylib\n.\n$ file /usr/local/Cellar/hdf5/1.8.12/lib/libhdf5.dylib \n/usr/local/Cellar/hdf5/1.8.12/lib/libhdf5.dylib: Mach-O 64-bit dynamically linked shared library x86_64\n$ ls -l /usr/local/Cellar/hdf5/1.8.12/lib\ntotal 15176\n-r--r--r--  1 dpo1  admin  2307908  1 Jan 19:09 libhdf5.8.dylib\n-r--r--r--  1 dpo1  admin  4190928  1 Jan 19:08 libhdf5.a\nlrwxr-xr-x  1 dpo1  admin       15  1 Jan 19:08 libhdf5.dylib -> libhdf5.8.dylib\n-r--r--r--  1 dpo1  admin     2441  1 Jan 19:08 libhdf5.settings\n-r--r--r--  1 dpo1  admin   333252  1 Jan 19:09 libhdf5_cpp.8.dylib\n-r--r--r--  1 dpo1  admin   604552  1 Jan 19:09 libhdf5_cpp.a\nlrwxr-xr-x  1 dpo1  admin       19  1 Jan 19:09 libhdf5_cpp.dylib -> libhdf5_cpp.8.dylib\n-r--r--r--  1 dpo1  admin   114848  1 Jan 19:09 libhdf5_hl.8.dylib\n-r--r--r--  1 dpo1  admin   153384  1 Jan 19:09 libhdf5_hl.a\nlrwxr-xr-x  1 dpo1  admin       18  1 Jan 19:09 libhdf5_hl.dylib -> libhdf5_hl.8.dylib\n-r--r--r--  1 dpo1  admin    19808  1 Jan 19:09 libhdf5_hl_cpp.8.dylib\n-r--r--r--  1 dpo1  admin     7272  1 Jan 19:09 libhdf5_hl_cpp.a\nlrwxr-xr-x  1 dpo1  admin       22  1 Jan 19:09 libhdf5_hl_cpp.dylib -> libhdf5_hl_cpp.8.dylib\n```\n. With both the latest release and the develop branch, I'm now getting\nUndefined symbols for architecture x86_64:\n  \"ColPack::GraphColoring::GetVertexColors(std::__1::vector<int, std::__1::allocator<int> >&)\", referenced from:\n      shogun::CProbingSampler::precompute() in ProbingSampler.cpp.o\n  \"ColPack::GraphColoringInterface::Coloring(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >)\", referenced from:\n      shogun::CProbingSampler::precompute() in ProbingSampler.cpp.o\nld: symbol(s) not found for architecture x86_64\nHowever, CMake did find Colpack:\n```\n-- The following OPTIONAL packages have been found:\n\nGDB\nBLAS\nLAPACK\nGLPK\nARPACK\nEigen3 (required version >= 3.1.2)\nNLopt\nColPack\nDoxygen\nLibXml2\nHDF5\nCURL\nZLIB\nBZip2\nLibLZMA\nSNAPPY\nLZO\nSpinlock\nThreads\n```\n\nAny idea how I can resolve this?\nThanks.\n. The idea is to update the Homebrew formula for Shogun, and to depend on an external colpack.\n. @MartinHjelm Check that you brewed colpack without using --with-libc++. That seems to work for me (but the build fails in other places, related to the R interface, as I mentioned in separate issues).\n. @MartinHjelm Scratch that. It must have been a fluke. I'm not managing to build with ColPack. Unless you brew uninstall colpack, CMake will pick it up and compilation will fail as above, even if you specify -DBUNDLE_COLPACK=ON. There doesn't seem to be a way to disable colpack if it's installed already.\n. That seems to work. Thank you!\n. That seems to work. Thank you!\n. Ping.\n. Ping.\n. With both the ~~3.2.1~~ 3.2.0 release and the develop branch, building octave-modular against Octave 3.8.1 gives\n/tmp/shogun-4bCUNw/build/src/interfaces/octave_modular/modshogunOCTAVE_wrap.cxx:1518:24: error: virtual function 'map_value' has a different return type ('Octave_map') than the function it overrides (which has return type 'octave_map')\n    virtual Octave_map map_value() const {\n                       ^\n/usr/local/Cellar/octave/3.8.1_1/include/octave-3.8.1/octave/ov-base.h:568:22: note: overridden virtual function is here\n  virtual octave_map map_value (void) const;\n                     ^\n/tmp/shogun-4bCUNw/build/src/interfaces/octave_modular/modshogunOCTAVE_wrap.cxx:1194:46: error: no member named 'is_real_nd_array' in 'octave_value'\n      } else if (out.is_matrix_type() || out.is_real_nd_array() || out.is_numeric_type() ) {\n                                         ~~~ ^\n/tmp/shogun-4bCUNw/build/src/interfaces/octave_modular/modshogunOCTAVE_wrap.cxx:1753:24: error: virtual function 'map_value' has a different return type ('Octave_map') than the function it overrides (which has return type 'octave_map')\n    virtual Octave_map map_value() const\n                       ^\n/usr/local/Cellar/octave/3.8.1_1/include/octave-3.8.1/octave/ov-base.h:568:22: note: overridden virtual function is here\n  virtual octave_map map_value (void) const;\n                     ^\nThat's with Swig 2.0.8. If I use Swig 3, the errors are different:\nUndefined symbols for architecture x86_64:\n  \"MatrixType::MatrixType()\", referenced from:\n      _wrap_ShortRealSparseVector_get_dense__SWIG_1(octave_value_list const&, int) in modshogunOCTAVE_wrap.cxx.o\n      _wrap_ShortRealSparseVector_get_dense__SWIG_0(octave_value_list const&, int) in modshogunOCTAVE_wrap.cxx.o\n      _wrap_RealSparseVector_get_dense__SWIG_1(octave_value_list const&, int) in modshogunOCTAVE_wrap.cxx.o\n      _wrap_RealSparseVector_get_dense__SWIG_0(octave_value_list const&, int) in modshogunOCTAVE_wrap.cxx.o\n      _wrap_BoolSparseMatrix_load_with_labels__SWIG_1(octave_value_list const&, int) in modshogunOCTAVE_wrap.cxx.o\n      _wrap_BoolSparseMatrix_load_with_labels__SWIG_0(octave_value_list const&, int) in modshogunOCTAVE_wrap.cxx.o\n      _wrap_CharSparseMatrix_load_with_labels__SWIG_1(octave_value_list const&, int) in modshogunOCTAVE_wrap.cxx.o\n. Any chance of having a release soon? #2395.\n. Actually, I'm still having the same problem as in #2395 with R modular and the develop branch.\n. Any idea where #2395 might come from? I'm still seeing that on Mavericks.\n. ",
    "MartinHjelm": "@dpo I am getting the same colpack error on Yosemite. How did you solve it? I am installing colpack via homebrew. It seems that it is the python interface that somehow causes the problems. Or at least when I do not set the flag for modular python interface it compiles.\n. @dpo I am getting the same colpack error on Yosemite. How did you solve it? I am installing colpack via homebrew. It seems that it is the python interface that somehow causes the problems. Or at least when I do not set the flag for modular python interface it compiles.\n. Any updates on this? I am getting the same segfault. Using the Homebrew formula the 4.0 version compiles but python delivers the segfault. (Related https://github.com/Homebrew/homebrew-science/pull/1818)\n. Uhm. So this is the OSX error message I get when I just write\n    import modshogun \nin python. I put it in a gist here https://gist.github.com/MartinHjelm/c44ca567523f0fd048ec\n. @lisitsyn I am using the homebrew provided python install. So I am guessing Shogun should compile against that. I tried using the Apple provided Python and when I run import modshogun I get no segfault. Not sure what to make of that. I am gonna try running my shogun scripts later tonight and see if they work with the Apple python.\n. @lisitsyn  Oh I wish I had known that...but thanks for the help!\nAnyways, when I use Shogun with the Apple python version I can successfully run a couple of tutorial examples. \n. @lisitsyn But I also get things like this sometimes on the Apple python\nThread 0 Crashed:: Dispatch queue: com.apple.main-thread\n0   libshogun.17.dylib              0x000000010d055117 shogun::CLMNNImpl::find_target_nn(shogun::CDenseFeatures, shogun::CMulticlassLabels, int) + 1167\n1   libshogun.17.dylib              0x000000010d0528ea shogun::CLMNN::train(shogun::SGMatrix) + 368\n2   modshogun.so                   0x000000010bef52ab _wrap_LMNN_train(_object, object) + 326\n3   org.python.python               0x0000000108017d8c PyEval_EvalFrameEx + 14342\n4   org.python.python               0x000000010801ac82 _PyEval_SliceIndex + 902\n5   org.python.python               0x00000001080179a6 PyEval_EvalFrameEx + 13344\n6   org.python.python               0x0000000108014352 PyEval_EvalCodeEx + 1409\n7   org.python.python               0x0000000108013dcb PyEval_EvalCode + 54\n8   org.python.python               0x000000010803400e PyParser_ASTFromFile + 315\n9   org.python.python               0x0000000108033e2a PyRun_InteractiveOneFlags + 353\n10  org.python.python               0x0000000108033939 PyRun_InteractiveLoopFlags + 192\n11  org.python.python               0x00000001080337e3 PyRun_AnyFileExFlags + 60\n12  org.python.python               0x0000000108045437 Py_Main + 3051\n13  libdyld.dylib                   0x00007fff868a55c9 start + 1\n. @lkuchenb I installed via Homebrew science https://github.com/Homebrew/homebrew-science and using the OS python it runs or at least my code runs.\n. ",
    "opoplawski": "Ran into another octave issue:\nIn file included from /builddir/build/BUILD/shogun-3.1.1/build/src/interfaces/octave_modular/modshogunOCTAVE_wrap.cxx:154:0:\n/usr/include/octave-3.8.0/octave/oct.h:31:20: fatal error: config.h: No such file or directory\n #include \n                    ^\ncompilation terminated.\noctave is fixing this to be #include \"config.h\", I'm building a fixed octave package in Fedora to test with, but this will be an issue in general.\n. ",
    "beew": "Any update? Still can't figure out how to add path to INCLUDES according to sonnety2k's post 12 days ago.\n. Here is my workaround. I compiled a local copy of hdf5 (with parallel disabled) in $HOME/opt/hdf5 and  built shogun against it instead of the systen version with the command\ncmake -DCMAKE_INCLUDE_PATH=$HOME/opt/hdf5/include -DCMAKE_LIBRARY_PATH=$HOME/opt/hdf5/lib ..\nThis way shogun built successfully. I was able to build the python-modular and R-static interfaces, but octave interfaces (static and modular) still failed to build. I think that is because my copy of octave was compiled with hdf5-openmpi.\n. Hi,\nI know that it will install with serial but that is not a solution because I need the openmpi one for other applications.  Since hdf5 is optional for shogun there should be a way to simply disable it.\n. Well the comments on this thread are not very helpful. Since hdf5 is optional, not being able to disable it is not a Ubuntu packaging problem. That shogun is not able to work with hdf5 with openmpi enabled is not a Ubuntu packaging bug either.\nBut I solved the problem myself. Here is how, hopefully will be useful to others in same frustrating situation and not getting any help here.\nBasically compiled a local version of hdf5 at $HOME/opt (need to disable parallel) and then compiled shogun with cmake -DCMAKE_INCLUDE_PATH=$HOME/opt/hdf5/include -DCMAKE_LIBRARY_PATH=$HOME/opt/hdf5/lib  ..\nIt is not that difficult a workaround, but it would have been helpful if the experienced users on this thread have pointed it out to me earlier.\nP.S.\nOctave interfaces (static or modular) still don't build I think it is because my copy of octave was compiled with hdf5-openmpi \n. Any news here? I have encountered the same problem.  First I got\nUnknown MEX argument '-o'.\nThen after changing '-o' to '-output' in shogun/src/interfaces/matlab_static and rebuild got\nUnknown file extension '.1'.\n. Hi Matthuska, \nMy hdf5 is a local self build because shogun doesn't build with hdf5-openmpi, which I installed for the whole system as I need it for other things. \nSee this thread\nhttps://github.com/shogun-toolbox/shogun/issues/1856\n. Just updated my hdf5 to 1.8.12, the hdf5 warnings disappear but still cannot build R-Modular (shogun development from git) \n\nScanning dependencies of target r_modular\n[100%] [100%] Building CXX object src/interfaces/r_modular/CMakeFiles/r_modular.dir/modshogunR_wrap.cxx.o\nBuilding CXX object src/interfaces/r_modular/CMakeFiles/r_modular.dir/sg_print_functions.cpp.o\nIn file included from /usr/include/python2.7/numpy/ndarraytypes.h:1761:0,\n                 from /usr/include/python2.7/numpy/ndarrayobject.h:17,\n                 from /usr/include/python2.7/numpy/arrayobject.h:4,\n                 from /home/bee/Downloads/shogun/build/src/interfaces/python_modular/modshogunPYTHON_wrap.cxx:6318:\n/usr/include/python2.7/numpy/npy_1_7_deprecated_api.h:15:2: warning: #warning \"Using deprecated NumPy API, disable it by \" \"#defining NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION\" [-Wcpp]\n #warning \"Using deprecated NumPy API, disable it by \" \\\n  ^\ncc1plus: warning: unrecognized command line option \"-Wno-c++11-narrowing\" [enabled by default]\nLinking CXX shared module _modshogun.so\nCMakeFiles/_python_modular.dir/modshogunPYTHON_wrap.cxx.o: In function shogun_CSGObject___getstate__':\n/home/bee/Downloads/shogun/build/src/interfaces/python_modular/modshogunPYTHON_wrap.cxx:8953: warning: the use oftmpnam' is dangerous, better use`mkstemp'\n[100%] Built target _python_modular\nLinking CXX shared module modshogun.so\nGenerating modshogun.RData\nf=\"modshogun.R\"; fdata=\"modshogun.RData\"; source ( f ) ; save ( list=ls ( all=TRUE ) ,file=fdata, compress=TRUE ) ; q ( save=\"no\" ) \nError in file(filename, \"r\", encoding = encoding) : \n  cannot open the connection\nCalls: source -> file\nIn addition: Warning message:\nIn file(filename, \"r\", encoding = encoding) :\n  cannot open file 'modshogun.R': No such file or directory\nExecution halted\nmake[2]: * [src/interfaces/r_modular/modshogun.so] Error 1\nmake[1]: * [src/interfaces/r_modular/CMakeFiles/r_modular.dir/all] Error 2\nmake: *** [all] Error 2\n. Hi, here is the complete build log\nScanning dependencies of target version\nScanning dependencies of target ShogunVersionProtobuf\n[  1%] Scanning dependencies of target ChunksProtobuf\n[  1%] Scanning dependencies of target class_list\nGenerating version header\nRunning C++ protocol buffer compiler on ShogunVersion.proto\n[  1%] Running C++ protocol buffer compiler on Chunks.proto\n[  1%] Running C++ protocol buffer compiler on Chunks.proto\n[  1%] [  1%] [  1%] Built target ShogunVersionProtobuf\nBuilt target ChunksProtobuf\nRunning C++ protocol buffer compiler on Headers.proto\nScanning dependencies of target HeadersProtobuf\n[  1%] Built target version\nScanning dependencies of target r_doxy2swig\n[  1%] [  1%] Built target HeadersProtobuf\nScanning dependencies of target r_modular_src\n[  2%] Generating /home/bee/Downloads/shogun/src/shogun/base/class_list.cpp\nGenerating doxygen doc\n[  2%] copying SWIG files\n[  2%] Built target r_modular_src\n[  3%] Built target class_list\nScanning dependencies of target libshogun\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/ShogunVersion.pb.h:63: warning: Compound shogun::ShogunVersion is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Headers.pb.h:342: warning: Compound shogun::StringListHeader is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Headers.pb.h:237: warning: Compound shogun::SparseMatrixHeader is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Headers.pb.h:43: warning: Compound shogun::VectorHeader is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:476: warning: Compound shogun::Float32Chunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Headers.pb.h:135: warning: Compound shogun::MatrixHeader is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:304: warning: Compound shogun::Int64Chunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:390: warning: Compound shogun::UInt64Chunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:562: warning: Compound shogun::Float64Chunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:132: warning: Compound shogun::Int32Chunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:218: warning: Compound shogun::UInt32Chunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:46: warning: Compound shogun::BoolChunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/ShogunVersion.pb.h:39: warning: Member ShogunVersion_SGDataType (enumeration) of namespace shogun is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:32: warning: Member protobuf_AddDesc_Chunks_2eproto() (function) of namespace shogun is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:33: warning: Member protobuf_AssignDesc_Chunks_2eproto() (function) of namespace shogun is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:34: warning: Member protobuf_ShutdownFile_Chunks_2eproto() (function) of namespace shogun is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Headers.pb.h:32: warning: Member protobuf_AddDesc_Headers_2eproto() (function) of namespace shogun is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Headers.pb.h:33: warning: Member protobuf_AssignDesc_Headers_2eproto() (function) of namespace shogun is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Headers.pb.h:34: warning: Member protobuf_ShutdownFile_Headers_2eproto() (function) of namespace shogun is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/ShogunVersion.pb.h:33: warning: Member protobuf_AddDesc_ShogunVersion_2eproto() (function) of namespace shogun is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/ShogunVersion.pb.h:34: warning: Member protobuf_AssignDesc_ShogunVersion_2eproto() (function) of namespace shogun is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/ShogunVersion.pb.h:35: warning: Member protobuf_ShutdownFile_ShogunVersion_2eproto() (function) of namespace shogun is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/ShogunVersion.pb.h:46: warning: Member ShogunVersion_SGDataType_IsValid(int value) (function) of namespace shogun is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/ShogunVersion.pb.h:51: warning: Member ShogunVersion_SGDataType_descriptor() (function) of namespace shogun is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/ShogunVersion.pb.h:52: warning: Member ShogunVersion_SGDataType_Name(ShogunVersion_SGDataType value) (function) of namespace shogun is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/ShogunVersion.pb.h:56: warning: Member ShogunVersion_SGDataType_Parse(const ::std::string &name, ShogunVersion_SGDataType value) (function) of namespace shogun is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/ShogunVersion.pb.h:47: warning: Member ShogunVersion_SGDataType_SGDataType_MIN (variable) of namespace shogun is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/ShogunVersion.pb.h:48: warning: Member ShogunVersion_SGDataType_SGDataType_MAX (variable) of namespace shogun is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/ShogunVersion.pb.h:49: warning: Member ShogunVersion_SGDataType_SGDataType_ARRAYSIZE (variable) of namespace shogun is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:51: warning: Member BoolChunk(const BoolChunk &from) (function) of class shogun::BoolChunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:53: warning: Member operator=(const BoolChunk &from) (function) of class shogun::BoolChunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:58: warning: Member unknown_fields() const  (function) of class shogun::BoolChunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:62: warning: Member mutable_unknown_fields() (function) of class shogun::BoolChunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:69: warning: Member Swap(BoolChunk other) (function) of class shogun::BoolChunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:73: warning: Member New() const  (function) of class shogun::BoolChunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:74: warning: Member CopyFrom(const ::google::protobuf::Message &from) (function) of class shogun::BoolChunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:75: warning: Member MergeFrom(const ::google::protobuf::Message &from) (function) of class shogun::BoolChunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:76: warning: Member CopyFrom(const BoolChunk &from) (function) of class shogun::BoolChunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:77: warning: Member MergeFrom(const BoolChunk &from) (function) of class shogun::BoolChunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:78: warning: Member Clear() (function) of class shogun::BoolChunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:79: warning: Member IsInitialized() const  (function) of class shogun::BoolChunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:81: warning: Member ByteSize() const  (function) of class shogun::BoolChunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:82: warning: Member MergePartialFromCodedStream(::google::protobuf::io::CodedInputStream input) (function) of class shogun::BoolChunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:84: warning: Member SerializeWithCachedSizes(::google::protobuf::io::CodedOutputStream output) const  (function) of class shogun::BoolChunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:86: warning: Member SerializeWithCachedSizesToArray(::google::protobuf::uint8 output) const  (function) of class shogun::BoolChunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:87: warning: Member GetCachedSize() const  (function) of class shogun::BoolChunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:94: warning: Member GetMetadata() const  (function) of class shogun::BoolChunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:101: warning: Member data_size() const  (function) of class shogun::BoolChunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:102: warning: Member clear_data() (function) of class shogun::BoolChunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:104: warning: Member data(int index) const  (function) of class shogun::BoolChunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:105: warning: Member set_data(int index, bool value) (function) of class shogun::BoolChunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:106: warning: Member add_data(bool value) (function) of class shogun::BoolChunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:108: warning: Member data() const  (function) of class shogun::BoolChunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:110: warning: Member mutable_data() (function) of class shogun::BoolChunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:66: warning: Member descriptor() (function) of class shogun::BoolChunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:67: warning: Member default_instance() (function) of class shogun::BoolChunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:103: warning: Member kDataFieldNumber (variable) of class shogun::BoolChunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:137: warning: Member Int32Chunk(const Int32Chunk &from) (function) of class shogun::Int32Chunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:139: warning: Member operator=(const Int32Chunk &from) (function) of class shogun::Int32Chunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:144: warning: Member unknown_fields() const  (function) of class shogun::Int32Chunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:148: warning: Member mutable_unknown_fields() (function) of class shogun::Int32Chunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:155: warning: Member Swap(Int32Chunk other) (function) of class shogun::Int32Chunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:159: warning: Member New() const  (function) of class shogun::Int32Chunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:160: warning: Member CopyFrom(const ::google::protobuf::Message &from) (function) of class shogun::Int32Chunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:161: warning: Member MergeFrom(const ::google::protobuf::Message &from) (function) of class shogun::Int32Chunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:162: warning: Member CopyFrom(const Int32Chunk &from) (function) of class shogun::Int32Chunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:163: warning: Member MergeFrom(const Int32Chunk &from) (function) of class shogun::Int32Chunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:164: warning: Member Clear() (function) of class shogun::Int32Chunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:165: warning: Member IsInitialized() const  (function) of class shogun::Int32Chunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:167: warning: Member ByteSize() const  (function) of class shogun::Int32Chunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:168: warning: Member MergePartialFromCodedStream(::google::protobuf::io::CodedInputStream input) (function) of class shogun::Int32Chunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:170: warning: Member SerializeWithCachedSizes(::google::protobuf::io::CodedOutputStream output) const  (function) of class shogun::Int32Chunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:172: warning: Member SerializeWithCachedSizesToArray(::google::protobuf::uint8 output) const  (function) of class shogun::Int32Chunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:173: warning: Member GetCachedSize() const  (function) of class shogun::Int32Chunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:180: warning: Member GetMetadata() const  (function) of class shogun::Int32Chunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:187: warning: Member data_size() const  (function) of class shogun::Int32Chunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:188: warning: Member clear_data() (function) of class shogun::Int32Chunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:190: warning: Member data(int index) const  (function) of class shogun::Int32Chunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:191: warning: Member set_data(int index,::google::protobuf::int32 value) (function) of class shogun::Int32Chunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:192: warning: Member add_data(::google::protobuf::int32 value) (function) of class shogun::Int32Chunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:194: warning: Member data() const  (function) of class shogun::Int32Chunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:196: warning: Member mutable_data() (function) of class shogun::Int32Chunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:152: warning: Member descriptor() (function) of class shogun::Int32Chunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:153: warning: Member default_instance() (function) of class shogun::Int32Chunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:189: warning: Member kDataFieldNumber (variable) of class shogun::Int32Chunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:223: warning: Member UInt32Chunk(const UInt32Chunk &from) (function) of class shogun::UInt32Chunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:225: warning: Member operator=(const UInt32Chunk &from) (function) of class shogun::UInt32Chunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:230: warning: Member unknown_fields() const  (function) of class shogun::UInt32Chunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:234: warning: Member mutable_unknown_fields() (function) of class shogun::UInt32Chunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:241: warning: Member Swap(UInt32Chunk other) (function) of class shogun::UInt32Chunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:245: warning: Member New() const  (function) of class shogun::UInt32Chunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:246: warning: Member CopyFrom(const ::google::protobuf::Message &from) (function) of class shogun::UInt32Chunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:247: warning: Member MergeFrom(const ::google::protobuf::Message &from) (function) of class shogun::UInt32Chunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:248: warning: Member CopyFrom(const UInt32Chunk &from) (function) of class shogun::UInt32Chunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:249: warning: Member MergeFrom(const UInt32Chunk &from) (function) of class shogun::UInt32Chunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:250: warning: Member Clear() (function) of class shogun::UInt32Chunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:251: warning: Member IsInitialized() const  (function) of class shogun::UInt32Chunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:253: warning: Member ByteSize() const  (function) of class shogun::UInt32Chunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:254: warning: Member MergePartialFromCodedStream(::google::protobuf::io::CodedInputStream input) (function) of class shogun::UInt32Chunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:256: warning: Member SerializeWithCachedSizes(::google::protobuf::io::CodedOutputStream output) const  (function) of class shogun::UInt32Chunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:258: warning: Member SerializeWithCachedSizesToArray(::google::protobuf::uint8 output) const  (function) of class shogun::UInt32Chunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:259: warning: Member GetCachedSize() const  (function) of class shogun::UInt32Chunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:266: warning: Member GetMetadata() const  (function) of class shogun::UInt32Chunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:273: warning: Member data_size() const  (function) of class shogun::UInt32Chunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:274: warning: Member clear_data() (function) of class shogun::UInt32Chunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:276: warning: Member data(int index) const  (function) of class shogun::UInt32Chunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:277: warning: Member set_data(int index,::google::protobuf::uint32 value) (function) of class shogun::UInt32Chunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:278: warning: Member add_data(::google::protobuf::uint32 value) (function) of class shogun::UInt32Chunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:280: warning: Member data() const  (function) of class shogun::UInt32Chunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:282: warning: Member mutable_data() (function) of class shogun::UInt32Chunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:238: warning: Member descriptor() (function) of class shogun::UInt32Chunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:239: warning: Member default_instance() (function) of class shogun::UInt32Chunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:275: warning: Member kDataFieldNumber (variable) of class shogun::UInt32Chunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:309: warning: Member Int64Chunk(const Int64Chunk &from) (function) of class shogun::Int64Chunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:311: warning: Member operator=(const Int64Chunk &from) (function) of class shogun::Int64Chunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:316: warning: Member unknown_fields() const  (function) of class shogun::Int64Chunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:320: warning: Member mutable_unknown_fields() (function) of class shogun::Int64Chunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:327: warning: Member Swap(Int64Chunk other) (function) of class shogun::Int64Chunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:331: warning: Member New() const  (function) of class shogun::Int64Chunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:332: warning: Member CopyFrom(const ::google::protobuf::Message &from) (function) of class shogun::Int64Chunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:333: warning: Member MergeFrom(const ::google::protobuf::Message &from) (function) of class shogun::Int64Chunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:334: warning: Member CopyFrom(const Int64Chunk &from) (function) of class shogun::Int64Chunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:335: warning: Member MergeFrom(const Int64Chunk &from) (function) of class shogun::Int64Chunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:336: warning: Member Clear() (function) of class shogun::Int64Chunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:337: warning: Member IsInitialized() const  (function) of class shogun::Int64Chunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:339: warning: Member ByteSize() const  (function) of class shogun::Int64Chunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:340: warning: Member MergePartialFromCodedStream(::google::protobuf::io::CodedInputStream input) (function) of class shogun::Int64Chunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:342: warning: Member SerializeWithCachedSizes(::google::protobuf::io::CodedOutputStream output) const  (function) of class shogun::Int64Chunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:344: warning: Member SerializeWithCachedSizesToArray(::google::protobuf::uint8 output) const  (function) of class shogun::Int64Chunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:345: warning: Member GetCachedSize() const  (function) of class shogun::Int64Chunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:352: warning: Member GetMetadata() const  (function) of class shogun::Int64Chunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:359: warning: Member data_size() const  (function) of class shogun::Int64Chunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:360: warning: Member clear_data() (function) of class shogun::Int64Chunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:362: warning: Member data(int index) const  (function) of class shogun::Int64Chunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:363: warning: Member set_data(int index,::google::protobuf::int64 value) (function) of class shogun::Int64Chunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:364: warning: Member add_data(::google::protobuf::int64 value) (function) of class shogun::Int64Chunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:366: warning: Member data() const  (function) of class shogun::Int64Chunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:368: warning: Member mutable_data() (function) of class shogun::Int64Chunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:324: warning: Member descriptor() (function) of class shogun::Int64Chunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:325: warning: Member default_instance() (function) of class shogun::Int64Chunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:361: warning: Member kDataFieldNumber (variable) of class shogun::Int64Chunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:395: warning: Member UInt64Chunk(const UInt64Chunk &from) (function) of class shogun::UInt64Chunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:397: warning: Member operator=(const UInt64Chunk &from) (function) of class shogun::UInt64Chunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:402: warning: Member unknown_fields() const  (function) of class shogun::UInt64Chunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:406: warning: Member mutable_unknown_fields() (function) of class shogun::UInt64Chunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:413: warning: Member Swap(UInt64Chunk other) (function) of class shogun::UInt64Chunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:417: warning: Member New() const  (function) of class shogun::UInt64Chunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:418: warning: Member CopyFrom(const ::google::protobuf::Message &from) (function) of class shogun::UInt64Chunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:419: warning: Member MergeFrom(const ::google::protobuf::Message &from) (function) of class shogun::UInt64Chunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:420: warning: Member CopyFrom(const UInt64Chunk &from) (function) of class shogun::UInt64Chunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:421: warning: Member MergeFrom(const UInt64Chunk &from) (function) of class shogun::UInt64Chunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:422: warning: Member Clear() (function) of class shogun::UInt64Chunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:423: warning: Member IsInitialized() const  (function) of class shogun::UInt64Chunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:425: warning: Member ByteSize() const  (function) of class shogun::UInt64Chunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:426: warning: Member MergePartialFromCodedStream(::google::protobuf::io::CodedInputStream input) (function) of class shogun::UInt64Chunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:428: warning: Member SerializeWithCachedSizes(::google::protobuf::io::CodedOutputStream output) const  (function) of class shogun::UInt64Chunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:430: warning: Member SerializeWithCachedSizesToArray(::google::protobuf::uint8 output) const  (function) of class shogun::UInt64Chunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:431: warning: Member GetCachedSize() const  (function) of class shogun::UInt64Chunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:438: warning: Member GetMetadata() const  (function) of class shogun::UInt64Chunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:445: warning: Member data_size() const  (function) of class shogun::UInt64Chunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:446: warning: Member clear_data() (function) of class shogun::UInt64Chunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:448: warning: Member data(int index) const  (function) of class shogun::UInt64Chunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:449: warning: Member set_data(int index,::google::protobuf::uint64 value) (function) of class shogun::UInt64Chunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:450: warning: Member add_data(::google::protobuf::uint64 value) (function) of class shogun::UInt64Chunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:452: warning: Member data() const  (function) of class shogun::UInt64Chunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:454: warning: Member mutable_data() (function) of class shogun::UInt64Chunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:410: warning: Member descriptor() (function) of class shogun::UInt64Chunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:411: warning: Member default_instance() (function) of class shogun::UInt64Chunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:447: warning: Member kDataFieldNumber (variable) of class shogun::UInt64Chunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:481: warning: Member Float32Chunk(const Float32Chunk &from) (function) of class shogun::Float32Chunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:483: warning: Member operator=(const Float32Chunk &from) (function) of class shogun::Float32Chunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:488: warning: Member unknown_fields() const  (function) of class shogun::Float32Chunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:492: warning: Member mutable_unknown_fields() (function) of class shogun::Float32Chunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:499: warning: Member Swap(Float32Chunk other) (function) of class shogun::Float32Chunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:503: warning: Member New() const  (function) of class shogun::Float32Chunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:504: warning: Member CopyFrom(const ::google::protobuf::Message &from) (function) of class shogun::Float32Chunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:505: warning: Member MergeFrom(const ::google::protobuf::Message &from) (function) of class shogun::Float32Chunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:506: warning: Member CopyFrom(const Float32Chunk &from) (function) of class shogun::Float32Chunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:507: warning: Member MergeFrom(const Float32Chunk &from) (function) of class shogun::Float32Chunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:508: warning: Member Clear() (function) of class shogun::Float32Chunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:509: warning: Member IsInitialized() const  (function) of class shogun::Float32Chunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:511: warning: Member ByteSize() const  (function) of class shogun::Float32Chunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:512: warning: Member MergePartialFromCodedStream(::google::protobuf::io::CodedInputStream input) (function) of class shogun::Float32Chunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:514: warning: Member SerializeWithCachedSizes(::google::protobuf::io::CodedOutputStream output) const  (function) of class shogun::Float32Chunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:516: warning: Member SerializeWithCachedSizesToArray(::google::protobuf::uint8 output) const  (function) of class shogun::Float32Chunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:517: warning: Member GetCachedSize() const  (function) of class shogun::Float32Chunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:524: warning: Member GetMetadata() const  (function) of class shogun::Float32Chunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:531: warning: Member data_size() const  (function) of class shogun::Float32Chunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:532: warning: Member clear_data() (function) of class shogun::Float32Chunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:534: warning: Member data(int index) const  (function) of class shogun::Float32Chunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:535: warning: Member set_data(int index, float value) (function) of class shogun::Float32Chunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:536: warning: Member add_data(float value) (function) of class shogun::Float32Chunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:538: warning: Member data() const  (function) of class shogun::Float32Chunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:540: warning: Member mutable_data() (function) of class shogun::Float32Chunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:496: warning: Member descriptor() (function) of class shogun::Float32Chunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:497: warning: Member default_instance() (function) of class shogun::Float32Chunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:533: warning: Member kDataFieldNumber (variable) of class shogun::Float32Chunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:567: warning: Member Float64Chunk(const Float64Chunk &from) (function) of class shogun::Float64Chunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:569: warning: Member operator=(const Float64Chunk &from) (function) of class shogun::Float64Chunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:574: warning: Member unknown_fields() const  (function) of class shogun::Float64Chunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:578: warning: Member mutable_unknown_fields() (function) of class shogun::Float64Chunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:585: warning: Member Swap(Float64Chunk other) (function) of class shogun::Float64Chunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:589: warning: Member New() const  (function) of class shogun::Float64Chunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:590: warning: Member CopyFrom(const ::google::protobuf::Message &from) (function) of class shogun::Float64Chunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:591: warning: Member MergeFrom(const ::google::protobuf::Message &from) (function) of class shogun::Float64Chunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:592: warning: Member CopyFrom(const Float64Chunk &from) (function) of class shogun::Float64Chunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:593: warning: Member MergeFrom(const Float64Chunk &from) (function) of class shogun::Float64Chunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:594: warning: Member Clear() (function) of class shogun::Float64Chunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:595: warning: Member IsInitialized() const  (function) of class shogun::Float64Chunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:597: warning: Member ByteSize() const  (function) of class shogun::Float64Chunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:598: warning: Member MergePartialFromCodedStream(::google::protobuf::io::CodedInputStream input) (function) of class shogun::Float64Chunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:600: warning: Member SerializeWithCachedSizes(::google::protobuf::io::CodedOutputStream output) const  (function) of class shogun::Float64Chunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:602: warning: Member SerializeWithCachedSizesToArray(::google::protobuf::uint8 output) const  (function) of class shogun::Float64Chunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:603: warning: Member GetCachedSize() const  (function) of class shogun::Float64Chunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:610: warning: Member GetMetadata() const  (function) of class shogun::Float64Chunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:617: warning: Member data_size() const  (function) of class shogun::Float64Chunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:618: warning: Member clear_data() (function) of class shogun::Float64Chunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:620: warning: Member data(int index) const  (function) of class shogun::Float64Chunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:621: warning: Member set_data(int index, double value) (function) of class shogun::Float64Chunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:622: warning: Member add_data(double value) (function) of class shogun::Float64Chunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:624: warning: Member data() const  (function) of class shogun::Float64Chunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:626: warning: Member mutable_data() (function) of class shogun::Float64Chunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:582: warning: Member descriptor() (function) of class shogun::Float64Chunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:583: warning: Member default_instance() (function) of class shogun::Float64Chunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.h:619: warning: Member kDataFieldNumber (variable) of class shogun::Float64Chunk is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Headers.pb.h:48: warning: Member VectorHeader(const VectorHeader &from) (function) of class shogun::VectorHeader is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Headers.pb.h:50: warning: Member operator=(const VectorHeader &from) (function) of class shogun::VectorHeader is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Headers.pb.h:55: warning: Member unknown_fields() const  (function) of class shogun::VectorHeader is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Headers.pb.h:59: warning: Member mutable_unknown_fields() (function) of class shogun::VectorHeader is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Headers.pb.h:66: warning: Member Swap(VectorHeader other) (function) of class shogun::VectorHeader is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Headers.pb.h:70: warning: Member New() const  (function) of class shogun::VectorHeader is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Headers.pb.h:71: warning: Member CopyFrom(const ::google::protobuf::Message &from) (function) of class shogun::VectorHeader is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Headers.pb.h:72: warning: Member MergeFrom(const ::google::protobuf::Message &from) (function) of class shogun::VectorHeader is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Headers.pb.h:73: warning: Member CopyFrom(const VectorHeader &from) (function) of class shogun::VectorHeader is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Headers.pb.h:74: warning: Member MergeFrom(const VectorHeader &from) (function) of class shogun::VectorHeader is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Headers.pb.h:75: warning: Member Clear() (function) of class shogun::VectorHeader is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Headers.pb.h:76: warning: Member IsInitialized() const  (function) of class shogun::VectorHeader is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Headers.pb.h:78: warning: Member ByteSize() const  (function) of class shogun::VectorHeader is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Headers.pb.h:79: warning: Member MergePartialFromCodedStream(::google::protobuf::io::CodedInputStream input) (function) of class shogun::VectorHeader is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Headers.pb.h:81: warning: Member SerializeWithCachedSizes(::google::protobuf::io::CodedOutputStream output) const  (function) of class shogun::VectorHeader is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Headers.pb.h:83: warning: Member SerializeWithCachedSizesToArray(::google::protobuf::uint8 output) const  (function) of class shogun::VectorHeader is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Headers.pb.h:84: warning: Member GetCachedSize() const  (function) of class shogun::VectorHeader is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Headers.pb.h:91: warning: Member GetMetadata() const  (function) of class shogun::VectorHeader is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Headers.pb.h:98: warning: Member has_len() const  (function) of class shogun::VectorHeader is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Headers.pb.h:99: warning: Member clear_len() (function) of class shogun::VectorHeader is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Headers.pb.h:101: warning: Member len() const  (function) of class shogun::VectorHeader is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Headers.pb.h:102: warning: Member set_len(::google::protobuf::uint64 value) (function) of class shogun::VectorHeader is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Headers.pb.h:105: warning: Member has_num_messages() const  (function) of class shogun::VectorHeader is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Headers.pb.h:106: warning: Member clear_num_messages() (function) of class shogun::VectorHeader is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Headers.pb.h:108: warning: Member num_messages() const  (function) of class shogun::VectorHeader is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Headers.pb.h:109: warning: Member set_num_messages(::google::protobuf::uint32 value) (function) of class shogun::VectorHeader is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Headers.pb.h:63: warning: Member descriptor() (function) of class shogun::VectorHeader is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Headers.pb.h:64: warning: Member default_instance() (function) of class shogun::VectorHeader is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Headers.pb.h:100: warning: Member kLenFieldNumber (variable) of class shogun::VectorHeader is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Headers.pb.h:107: warning: Member kNumMessagesFieldNumber (variable) of class shogun::VectorHeader is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Headers.pb.h:140: warning: Member MatrixHeader(const MatrixHeader &from) (function) of class shogun::MatrixHeader is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Headers.pb.h:142: warning: Member operator=(const MatrixHeader &from) (function) of class shogun::MatrixHeader is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Headers.pb.h:147: warning: Member unknown_fields() const  (function) of class shogun::MatrixHeader is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Headers.pb.h:151: warning: Member mutable_unknown_fields() (function) of class shogun::MatrixHeader is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Headers.pb.h:158: warning: Member Swap(MatrixHeader other) (function) of class shogun::MatrixHeader is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Headers.pb.h:162: warning: Member New() const  (function) of class shogun::MatrixHeader is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Headers.pb.h:163: warning: Member CopyFrom(const ::google::protobuf::Message &from) (function) of class shogun::MatrixHeader is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Headers.pb.h:164: warning: Member MergeFrom(const ::google::protobuf::Message &from) (function) of class shogun::MatrixHeader is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Headers.pb.h:165: warning: Member CopyFrom(const MatrixHeader &from) (function) of class shogun::MatrixHeader is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Headers.pb.h:166: warning: Member MergeFrom(const MatrixHeader &from) (function) of class shogun::MatrixHeader is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Headers.pb.h:167: warning: Member Clear() (function) of class shogun::MatrixHeader is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Headers.pb.h:168: warning: Member IsInitialized() const  (function) of class shogun::MatrixHeader is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Headers.pb.h:170: warning: Member ByteSize() const  (function) of class shogun::MatrixHeader is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Headers.pb.h:171: warning: Member MergePartialFromCodedStream(::google::protobuf::io::CodedInputStream input) (function) of class shogun::MatrixHeader is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Headers.pb.h:173: warning: Member SerializeWithCachedSizes(::google::protobuf::io::CodedOutputStream output) const  (function) of class shogun::MatrixHeader is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Headers.pb.h:175: warning: Member SerializeWithCachedSizesToArray(::google::protobuf::uint8 output) const  (function) of class shogun::MatrixHeader is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Headers.pb.h:176: warning: Member GetCachedSize() const  (function) of class shogun::MatrixHeader is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Headers.pb.h:183: warning: Member GetMetadata() const  (function) of class shogun::MatrixHeader is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Headers.pb.h:190: warning: Member has_num_cols() const  (function) of class shogun::MatrixHeader is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Headers.pb.h:191: warning: Member clear_num_cols() (function) of class shogun::MatrixHeader is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Headers.pb.h:193: warning: Member num_cols() const  (function) of class shogun::MatrixHeader is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Headers.pb.h:194: warning: Member set_num_cols(::google::protobuf::uint64 value) (function) of class shogun::MatrixHeader is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Headers.pb.h:197: warning: Member has_num_rows() const  (function) of class shogun::MatrixHeader is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Headers.pb.h:198: warning: Member clear_num_rows() (function) of class shogun::MatrixHeader is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Headers.pb.h:200: warning: Member num_rows() const  (function) of class shogun::MatrixHeader is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Headers.pb.h:201: warning: Member set_num_rows(::google::protobuf::uint64 value) (function) of class shogun::MatrixHeader is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Headers.pb.h:204: warning: Member has_num_messages() const  (function) of class shogun::MatrixHeader is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Headers.pb.h:205: warning: Member clear_num_messages() (function) of class shogun::MatrixHeader is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Headers.pb.h:207: warning: Member num_messages() const  (function) of class shogun::MatrixHeader is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Headers.pb.h:208: warning: Member set_num_messages(::google::protobuf::uint32 value) (function) of class shogun::MatrixHeader is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Headers.pb.h:155: warning: Member descriptor() (function) of class shogun::MatrixHeader is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Headers.pb.h:156: warning: Member default_instance() (function) of class shogun::MatrixHeader is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Headers.pb.h:192: warning: Member kNumColsFieldNumber (variable) of class shogun::MatrixHeader is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Headers.pb.h:199: warning: Member kNumRowsFieldNumber (variable) of class shogun::MatrixHeader is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Headers.pb.h:206: warning: Member kNumMessagesFieldNumber (variable) of class shogun::MatrixHeader is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Headers.pb.h:242: warning: Member SparseMatrixHeader(const SparseMatrixHeader &from) (function) of class shogun::SparseMatrixHeader is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Headers.pb.h:244: warning: Member operator=(const SparseMatrixHeader &from) (function) of class shogun::SparseMatrixHeader is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Headers.pb.h:249: warning: Member unknown_fields() const  (function) of class shogun::SparseMatrixHeader is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Headers.pb.h:253: warning: Member mutable_unknown_fields() (function) of class shogun::SparseMatrixHeader is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Headers.pb.h:260: warning: Member Swap(SparseMatrixHeader other) (function) of class shogun::SparseMatrixHeader is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Headers.pb.h:264: warning: Member New() const  (function) of class shogun::SparseMatrixHeader is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Headers.pb.h:265: warning: Member CopyFrom(const ::google::protobuf::Message &from) (function) of class shogun::SparseMatrixHeader is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Headers.pb.h:266: warning: Member MergeFrom(const ::google::protobuf::Message &from) (function) of class shogun::SparseMatrixHeader is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Headers.pb.h:267: warning: Member CopyFrom(const SparseMatrixHeader &from) (function) of class shogun::SparseMatrixHeader is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Headers.pb.h:268: warning: Member MergeFrom(const SparseMatrixHeader &from) (function) of class shogun::SparseMatrixHeader is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Headers.pb.h:269: warning: Member Clear() (function) of class shogun::SparseMatrixHeader is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Headers.pb.h:270: warning: Member IsInitialized() const  (function) of class shogun::SparseMatrixHeader is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Headers.pb.h:272: warning: Member ByteSize() const  (function) of class shogun::SparseMatrixHeader is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Headers.pb.h:273: warning: Member MergePartialFromCodedStream(::google::protobuf::io::CodedInputStream input) (function) of class shogun::SparseMatrixHeader is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Headers.pb.h:275: warning: Member SerializeWithCachedSizes(::google::protobuf::io::CodedOutputStream output) const  (function) of class shogun::SparseMatrixHeader is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Headers.pb.h:277: warning: Member SerializeWithCachedSizesToArray(::google::protobuf::uint8 output) const  (function) of class shogun::SparseMatrixHeader is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Headers.pb.h:278: warning: Member GetCachedSize() const  (function) of class shogun::SparseMatrixHeader is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Headers.pb.h:285: warning: Member GetMetadata() const  (function) of class shogun::SparseMatrixHeader is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Headers.pb.h:292: warning: Member has_num_vectors() const  (function) of class shogun::SparseMatrixHeader is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Headers.pb.h:293: warning: Member clear_num_vectors() (function) of class shogun::SparseMatrixHeader is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Headers.pb.h:295: warning: Member num_vectors() const  (function) of class shogun::SparseMatrixHeader is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Headers.pb.h:296: warning: Member set_num_vectors(::google::protobuf::uint64 value) (function) of class shogun::SparseMatrixHeader is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Headers.pb.h:299: warning: Member has_num_features() const  (function) of class shogun::SparseMatrixHeader is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Headers.pb.h:300: warning: Member clear_num_features() (function) of class shogun::SparseMatrixHeader is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Headers.pb.h:302: warning: Member num_features() const  (function) of class shogun::SparseMatrixHeader is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Headers.pb.h:303: warning: Member set_num_features(::google::protobuf::uint64 value) (function) of class shogun::SparseMatrixHeader is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Headers.pb.h:306: warning: Member num_feat_entries_size() const  (function) of class shogun::SparseMatrixHeader is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Headers.pb.h:307: warning: Member clear_num_feat_entries() (function) of class shogun::SparseMatrixHeader is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Headers.pb.h:309: warning: Member num_feat_entries(int index) const  (function) of class shogun::SparseMatrixHeader is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Headers.pb.h:310: warning: Member set_num_feat_entries(int index,::google::protobuf::uint64 value) (function) of class shogun::SparseMatrixHeader is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Headers.pb.h:311: warning: Member add_num_feat_entries(::google::protobuf::uint64 value) (function) of class shogun::SparseMatrixHeader is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Headers.pb.h:313: warning: Member num_feat_entries() const  (function) of class shogun::SparseMatrixHeader is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Headers.pb.h:315: warning: Member mutable_num_feat_entries() (function) of class shogun::SparseMatrixHeader is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Headers.pb.h:257: warning: Member descriptor() (function) of class shogun::SparseMatrixHeader is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Headers.pb.h:258: warning: Member default_instance() (function) of class shogun::SparseMatrixHeader is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Headers.pb.h:294: warning: Member kNumVectorsFieldNumber (variable) of class shogun::SparseMatrixHeader is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Headers.pb.h:301: warning: Member kNumFeaturesFieldNumber (variable) of class shogun::SparseMatrixHeader is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Headers.pb.h:308: warning: Member kNumFeatEntriesFieldNumber (variable) of class shogun::SparseMatrixHeader is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Headers.pb.h:347: warning: Member StringListHeader(const StringListHeader &from) (function) of class shogun::StringListHeader is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Headers.pb.h:349: warning: Member operator=(const StringListHeader &from) (function) of class shogun::StringListHeader is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Headers.pb.h:354: warning: Member unknown_fields() const  (function) of class shogun::StringListHeader is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Headers.pb.h:358: warning: Member mutable_unknown_fields() (function) of class shogun::StringListHeader is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Headers.pb.h:365: warning: Member Swap(StringListHeader other) (function) of class shogun::StringListHeader is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Headers.pb.h:369: warning: Member New() const  (function) of class shogun::StringListHeader is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Headers.pb.h:370: warning: Member CopyFrom(const ::google::protobuf::Message &from) (function) of class shogun::StringListHeader is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Headers.pb.h:371: warning: Member MergeFrom(const ::google::protobuf::Message &from) (function) of class shogun::StringListHeader is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Headers.pb.h:372: warning: Member CopyFrom(const StringListHeader &from) (function) of class shogun::StringListHeader is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Headers.pb.h:373: warning: Member MergeFrom(const StringListHeader &from) (function) of class shogun::StringListHeader is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Headers.pb.h:374: warning: Member Clear() (function) of class shogun::StringListHeader is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Headers.pb.h:375: warning: Member IsInitialized() const  (function) of class shogun::StringListHeader is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Headers.pb.h:377: warning: Member ByteSize() const  (function) of class shogun::StringListHeader is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Headers.pb.h:378: warning: Member MergePartialFromCodedStream(::google::protobuf::io::CodedInputStream input) (function) of class shogun::StringListHeader is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Headers.pb.h:380: warning: Member SerializeWithCachedSizes(::google::protobuf::io::CodedOutputStream output) const  (function) of class shogun::StringListHeader is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Headers.pb.h:382: warning: Member SerializeWithCachedSizesToArray(::google::protobuf::uint8 output) const  (function) of class shogun::StringListHeader is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Headers.pb.h:383: warning: Member GetCachedSize() const  (function) of class shogun::StringListHeader is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Headers.pb.h:390: warning: Member GetMetadata() const  (function) of class shogun::StringListHeader is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Headers.pb.h:397: warning: Member has_num_str() const  (function) of class shogun::StringListHeader is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Headers.pb.h:398: warning: Member clear_num_str() (function) of class shogun::StringListHeader is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Headers.pb.h:400: warning: Member num_str() const  (function) of class shogun::StringListHeader is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Headers.pb.h:401: warning: Member set_num_str(::google::protobuf::uint64 value) (function) of class shogun::StringListHeader is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Headers.pb.h:404: warning: Member has_max_string_len() const  (function) of class shogun::StringListHeader is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Headers.pb.h:405: warning: Member clear_max_string_len() (function) of class shogun::StringListHeader is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Headers.pb.h:407: warning: Member max_string_len() const  (function) of class shogun::StringListHeader is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Headers.pb.h:408: warning: Member set_max_string_len(::google::protobuf::uint64 value) (function) of class shogun::StringListHeader is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Headers.pb.h:411: warning: Member str_len_size() const  (function) of class shogun::StringListHeader is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Headers.pb.h:412: warning: Member clear_str_len() (function) of class shogun::StringListHeader is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Headers.pb.h:414: warning: Member str_len(int index) const  (function) of class shogun::StringListHeader is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Headers.pb.h:415: warning: Member set_str_len(int index,::google::protobuf::uint64 value) (function) of class shogun::StringListHeader is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Headers.pb.h:416: warning: Member add_str_len(::google::protobuf::uint64 value) (function) of class shogun::StringListHeader is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Headers.pb.h:418: warning: Member str_len() const  (function) of class shogun::StringListHeader is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Headers.pb.h:420: warning: Member mutable_str_len() (function) of class shogun::StringListHeader is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Headers.pb.h:362: warning: Member descriptor() (function) of class shogun::StringListHeader is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Headers.pb.h:363: warning: Member default_instance() (function) of class shogun::StringListHeader is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Headers.pb.h:399: warning: Member kNumStrFieldNumber (variable) of class shogun::StringListHeader is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Headers.pb.h:406: warning: Member kMaxStringLenFieldNumber (variable) of class shogun::StringListHeader is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Headers.pb.h:413: warning: Member kStrLenFieldNumber (variable) of class shogun::StringListHeader is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/ShogunVersion.pb.h:115: warning: Member SGDataType (typedef) of class shogun::ShogunVersion is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/ShogunVersion.pb.h:68: warning: Member ShogunVersion(const ShogunVersion &from) (function) of class shogun::ShogunVersion is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/ShogunVersion.pb.h:70: warning: Member operator=(const ShogunVersion &from) (function) of class shogun::ShogunVersion is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/ShogunVersion.pb.h:75: warning: Member unknown_fields() const  (function) of class shogun::ShogunVersion is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/ShogunVersion.pb.h:79: warning: Member mutable_unknown_fields() (function) of class shogun::ShogunVersion is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/ShogunVersion.pb.h:86: warning: Member Swap(ShogunVersion other) (function) of class shogun::ShogunVersion is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/ShogunVersion.pb.h:90: warning: Member New() const  (function) of class shogun::ShogunVersion is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/ShogunVersion.pb.h:91: warning: Member CopyFrom(const ::google::protobuf::Message &from) (function) of class shogun::ShogunVersion is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/ShogunVersion.pb.h:92: warning: Member MergeFrom(const ::google::protobuf::Message &from) (function) of class shogun::ShogunVersion is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/ShogunVersion.pb.h:93: warning: Member CopyFrom(const ShogunVersion &from) (function) of class shogun::ShogunVersion is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/ShogunVersion.pb.h:94: warning: Member MergeFrom(const ShogunVersion &from) (function) of class shogun::ShogunVersion is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/ShogunVersion.pb.h:95: warning: Member Clear() (function) of class shogun::ShogunVersion is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/ShogunVersion.pb.h:96: warning: Member IsInitialized() const  (function) of class shogun::ShogunVersion is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/ShogunVersion.pb.h:98: warning: Member ByteSize() const  (function) of class shogun::ShogunVersion is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/ShogunVersion.pb.h:99: warning: Member MergePartialFromCodedStream(::google::protobuf::io::CodedInputStream input) (function) of class shogun::ShogunVersion is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/ShogunVersion.pb.h:101: warning: Member SerializeWithCachedSizes(::google::protobuf::io::CodedOutputStream output) const  (function) of class shogun::ShogunVersion is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/ShogunVersion.pb.h:103: warning: Member SerializeWithCachedSizesToArray(::google::protobuf::uint8 output) const  (function) of class shogun::ShogunVersion is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/ShogunVersion.pb.h:104: warning: Member GetCachedSize() const  (function) of class shogun::ShogunVersion is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/ShogunVersion.pb.h:111: warning: Member GetMetadata() const  (function) of class shogun::ShogunVersion is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/ShogunVersion.pb.h:145: warning: Member has_version() const  (function) of class shogun::ShogunVersion is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/ShogunVersion.pb.h:146: warning: Member clear_version() (function) of class shogun::ShogunVersion is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/ShogunVersion.pb.h:148: warning: Member version() const  (function) of class shogun::ShogunVersion is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/ShogunVersion.pb.h:149: warning: Member set_version(::google::protobuf::int32 value) (function) of class shogun::ShogunVersion is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/ShogunVersion.pb.h:152: warning: Member has_data_type() const  (function) of class shogun::ShogunVersion is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/ShogunVersion.pb.h:153: warning: Member clear_data_type() (function) of class shogun::ShogunVersion is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/ShogunVersion.pb.h:155: warning: Member data_type() const  (function) of class shogun::ShogunVersion is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/ShogunVersion.pb.h:156: warning: Member set_data_type(::shogun::ShogunVersion_SGDataType value) (function) of class shogun::ShogunVersion is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/ShogunVersion.pb.h:83: warning: Member descriptor() (function) of class shogun::ShogunVersion is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/ShogunVersion.pb.h:84: warning: Member default_instance() (function) of class shogun::ShogunVersion is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/ShogunVersion.pb.h:121: warning: Member SGDataType_IsValid(int value) (function) of class shogun::ShogunVersion is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/ShogunVersion.pb.h:131: warning: Member SGDataType_descriptor() (function) of class shogun::ShogunVersion is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/ShogunVersion.pb.h:134: warning: Member SGDataType_Name(SGDataType value) (function) of class shogun::ShogunVersion is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/ShogunVersion.pb.h:137: warning: Member SGDataType_Parse(const ::std::string &name, SGDataType value) (function) of class shogun::ShogunVersion is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/ShogunVersion.pb.h:116: warning: Member VECTOR (variable) of class shogun::ShogunVersion is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/ShogunVersion.pb.h:117: warning: Member MATRIX (variable) of class shogun::ShogunVersion is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/ShogunVersion.pb.h:118: warning: Member SPARSE_VECTOR (variable) of class shogun::ShogunVersion is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/ShogunVersion.pb.h:119: warning: Member SPARSE_MATRIX (variable) of class shogun::ShogunVersion is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/ShogunVersion.pb.h:120: warning: Member STRING_LIST (variable) of class shogun::ShogunVersion is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/ShogunVersion.pb.h:124: warning: Member SGDataType_MIN (variable) of class shogun::ShogunVersion is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/ShogunVersion.pb.h:126: warning: Member SGDataType_MAX (variable) of class shogun::ShogunVersion is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/ShogunVersion.pb.h:128: warning: Member SGDataType_ARRAYSIZE (variable) of class shogun::ShogunVersion is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/ShogunVersion.pb.h:147: warning: Member kVersionFieldNumber (variable) of class shogun::ShogunVersion is not documented.\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/ShogunVersion.pb.h:154: warning: Member kDataTypeFieldNumber (variable) of class shogun::ShogunVersion is not documented.\n/home/bee/Downloads/shogun/src/shogun/mathematics/Math.h:1810: warning: Member linspace(complex128_t start, complex128_t end, int32_t n) (function) of class shogun::CMath is not documented.\n/home/bee/Downloads/shogun/src/shogun/mathematics/Statistics.h:669: warning: Member mean(SGVector< complex128_t > vec) (function) of class shogun::CStatistics is not documented.\n[  3%] [  3%] [  3%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/ensemble/CombinationRule.cpp.o\nBuilding CXX object src/shogun/CMakeFiles/libshogun.dir/ensemble/MeanRule.cpp.o\nBuilding CXX object src/shogun/CMakeFiles/libshogun.dir/ensemble/WeightedMajorityVote.cpp.o\n[  3%] [  3%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/ensemble/MajorityVote.cpp.o\n[  3%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/metric/LMNN.cpp.o\nBuilding CXX object src/shogun/CMakeFiles/libshogun.dir/metric/LMNNImpl.cpp.o\n[  3%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/mathematics/Cplex.cpp.o\n[  4%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/mathematics/Math.cpp.o\n[  4%] [  4%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/mathematics/Statistics.cpp.o\nBuilding CXX object src/shogun/CMakeFiles/libshogun.dir/mathematics/SparseInverseCovariance.cpp.o\n[  4%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/mathematics/munkres.cpp.o\n[  4%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/mathematics/JacobiEllipticFunctions.cpp.o\n[  4%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/mathematics/ajd/FFDiag.cpp.o\n[  4%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/mathematics/ajd/QDiag.cpp.o\n[  4%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/mathematics/ajd/JADiagOrth.cpp.o\n[  5%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/mathematics/ajd/JADiag.cpp.o\n[  5%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/mathematics/ajd/JediDiag.cpp.o\n[  5%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/mathematics/ajd/UWedge.cpp.o\n[  5%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/mathematics/Mosek.cpp.o\n[  5%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/mathematics/lapack.cpp.o\n[  5%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/mathematics/eigen3.cpp.o\n[  5%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/mathematics/Integration.cpp.o\n[  5%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/mathematics/linalg/ratapprox/opfunc/OperatorFunction.cpp.o\n[  6%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/mathematics/linalg/ratapprox/opfunc/RationalApproximation.cpp.o\n[  6%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/mathematics/linalg/ratapprox/logdet/LogDetEstimator.cpp.o\n[  6%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/mathematics/linalg/ratapprox/logdet/opfunc/DenseMatrixExactLog.cpp.o\n[  6%] [  6%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/mathematics/linalg/ratapprox/logdet/opfunc/LogRationalApproximationIndividual.cpp.o\nBuilding CXX object src/shogun/CMakeFiles/libshogun.dir/mathematics/linalg/ratapprox/logdet/opfunc/LogRationalApproximationCGM.cpp.o\n/home/bee/Downloads/shogun/src/shogun/mathematics/Statistics.h:38: warning: argument 'values' of command @param is not found in the argument list of shogun::CStatistics::mean(SGVector< T > vec)\n/home/bee/Downloads/shogun/src/shogun/mathematics/Statistics.h:38: warning: The following parameters of shogun::CStatistics::mean(SGVector< T > vec) are not documented:\n  parameter 'vec'\n[  6%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/mathematics/linalg/ratapprox/logdet/computation/job/RationalApproximationIndividualJob.cpp.o\n[  6%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/mathematics/linalg/ratapprox/logdet/computation/job/DenseExactLogJob.cpp.o\n[  6%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/mathematics/linalg/ratapprox/logdet/computation/job/RationalApproximationCGMJob.cpp.o\n[  7%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/mathematics/linalg/ratapprox/logdet/computation/aggregator/IndividualJobResultAggregator.cpp.o\n[  7%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/mathematics/linalg/ratapprox/tracesampler/ProbingSampler.cpp.o\n[  7%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/mathematics/linalg/ratapprox/tracesampler/NormalSampler.cpp.o\n[  7%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/mathematics/linalg/linop/MatrixOperator.cpp.o\n[  7%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/mathematics/linalg/linop/DenseMatrixOperator.cpp.o\n[  7%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/mathematics/linalg/linop/LinearOperator.cpp.o\n[  7%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/mathematics/linalg/linop/SparseMatrixOperator.cpp.o\n[  7%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/mathematics/linalg/eigsolver/LanczosEigenSolver.cpp.o\n[  8%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/mathematics/linalg/eigsolver/DirectEigenSolver.cpp.o\n[  8%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/mathematics/linalg/linsolver/DirectSparseLinearSolver.cpp.o\n[  8%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/mathematics/linalg/linsolver/ConjugateOrthogonalCGSolver.cpp.o\n[  8%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/mathematics/linalg/linsolver/CGMShiftedFamilySolver.cpp.o\n[  8%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/mathematics/linalg/linsolver/IterativeShiftedLinearFamilySolver.cpp.o\n[  8%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/mathematics/linalg/linsolver/LinearSolver.cpp.o\n[  8%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/mathematics/linalg/linsolver/DirectLinearSolverComplex.cpp.o\n[  8%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/mathematics/linalg/linsolver/ConjugateGradientSolver.cpp.o\n[  9%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/mathematics/linalg/linsolver/IterativeLinearSolver.cpp.o\n[  9%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/mathematics/Random.cpp.o\n[  9%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/preprocessor/FeatureSelection.cpp.o\n[  9%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/preprocessor/PCA.cpp.o\n[  9%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/preprocessor/RandomFourierGaussPreproc.cpp.o\n[  9%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/preprocessor/KernelPCA.cpp.o\n[  9%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/preprocessor/HomogeneousKernelMap.cpp.o\n[  9%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/preprocessor/DensePreprocessor.cpp.o\n[ 10%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/preprocessor/FisherLDA.cpp.o\n[ 10%] [ 10%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/preprocessor/DependenceMaximization.cpp.o\nBuilding CXX object src/shogun/CMakeFiles/libshogun.dir/preprocessor/DimensionReductionPreprocessor.cpp.o\n[ 10%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/preprocessor/LogPlusOne.cpp.o\n[ 10%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/preprocessor/DecompressString.cpp.o\n[ 10%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/preprocessor/SumOne.cpp.o\n[ 10%] [ 10%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/preprocessor/NormOne.cpp.o\nBuilding CXX object src/shogun/CMakeFiles/libshogun.dir/preprocessor/SortUlongString.cpp.o\n[ 11%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/preprocessor/PNorm.cpp.o\n[ 11%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/preprocessor/KernelDependenceMaximization.cpp.o\n[ 11%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/preprocessor/PruneVarSubMean.cpp.o\n[ 11%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/preprocessor/BAHSIC.cpp.o\n[ 11%] [ 11%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/preprocessor/RescaleFeatures.cpp.o\nBuilding CXX object src/shogun/CMakeFiles/libshogun.dir/preprocessor/SortWordString.cpp.o\n[ 11%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/features/AttributeFeatures.cpp.o\n[ 12%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/features/RealFileFeatures.cpp.o\n[ 12%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/features/HashedWDFeatures.cpp.o\n[ 12%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/features/LatentFeatures.cpp.o\n[ 12%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/features/DenseFeatures.cpp.o\n[ 12%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/features/Subset.cpp.o\n[ 12%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/features/DummyFeatures.cpp.o\n[ 12%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/features/streaming/StreamingVwFeatures.cpp.o\n[ 12%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/features/streaming/StreamingDenseFeatures.cpp.o\n[ 13%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/features/streaming/StreamingFeatures.cpp.o\n[ 13%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/features/streaming/StreamingStringFeatures.cpp.o\n[ 13%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/features/streaming/StreamingHashedDocDotFeatures.cpp.o\n[ 13%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/features/streaming/StreamingSparseFeatures.cpp.o\n[ 13%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/features/streaming/generators/MeanShiftDataGenerator.cpp.o\n[ 13%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/features/streaming/generators/GaussianBlobsDataGenerator.cpp.o\n[ 13%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/features/streaming/StreamingHashedSparseFeatures.cpp.o\n[ 13%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/features/streaming/StreamingDotFeatures.cpp.o\n[ 14%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/features/streaming/StreamingHashedDenseFeatures.cpp.o\n[ 14%] [ 14%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/features/StringFileFeatures.cpp.o\nBuilding CXX object src/shogun/CMakeFiles/libshogun.dir/features/FactorGraphFeatures.cpp.o\n[ 14%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/features/PolyFeatures.cpp.o\n[ 14%] [ 14%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/features/DotFeatures.cpp.o\nBuilding CXX object src/shogun/CMakeFiles/libshogun.dir/features/SNPFeatures.cpp.o\n[ 14%] Generating modshogun_doxygen.i\n[ 14%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/features/MatrixFeatures.cpp.o\n[ 14%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/features/ImplicitWeightedSpecFeatures.cpp.o\n[ 15%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/features/Alphabet.cpp.o\n[ 15%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/features/CombinedFeatures.cpp.o\n[ 15%] [ 15%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/features/HashedDenseFeatures.cpp.o\nBuilding CXX object src/shogun/CMakeFiles/libshogun.dir/features/WDFeatures.cpp.o\n[ 15%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/features/HashedWDFeaturesTransposed.cpp.o\n[ 15%] [ 15%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/features/Features.cpp.o\nBuilding CXX object src/shogun/CMakeFiles/libshogun.dir/features/HashedSparseFeatures.cpp.o\n[ 15%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/features/DataGenerator.cpp.o\n[ 16%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/features/CombinedDotFeatures.cpp.o\n[ 16%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/features/HashedDocDotFeatures.cpp.o\n[ 16%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/features/LBPPyrDotFeatures.cpp.o\n[ 16%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/features/StringFeatures.cpp.o\n[ 16%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/features/DenseSubsetFeatures.cpp.o\n[ 16%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/features/ExplicitSpecFeatures.cpp.o\n[ 16%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/features/SparsePolyFeatures.cpp.o\n[ 17%] [ 17%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/features/IndexFeatures.cpp.o\nBuilding CXX object src/shogun/CMakeFiles/libshogun.dir/features/RandomFourierDotFeatures.cpp.o\n[ 17%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/features/TOPFeatures.cpp.o\n[ 17%] [ 17%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/features/FKFeatures.cpp.o\nBuilding CXX object src/shogun/CMakeFiles/libshogun.dir/features/SubsetStack.cpp.o\n[ 17%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/features/BinnedDotFeatures.cpp.o\n[ 17%] [ 17%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/features/SparseFeatures.cpp.o\nBuilding CXX object src/shogun/CMakeFiles/libshogun.dir/features/RandomKitchenSinksDotFeatures.cpp.o\n[ 17%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/modelselection/ModelSelectionParameters.cpp.o\n[ 18%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/modelselection/GradientModelSelection.cpp.o\n[ 18%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/modelselection/ModelSelection.cpp.o\n[ 18%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/modelselection/RandomSearchModelSelection.cpp.o\n[ 18%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/modelselection/ParameterCombination.cpp.o\n[ 18%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/modelselection/GridSearchModelSelection.cpp.o\n[ 18%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/loss/ExponentialLoss.cpp.o\n[ 18%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/loss/AbsoluteDeviationLoss.cpp.o\n[ 18%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/loss/SquaredLoss.cpp.o\n[ 19%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/loss/LogLoss.cpp.o\n[ 19%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/loss/SmoothHingeLoss.cpp.o\n[ 19%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/loss/HuberLoss.cpp.o\n[ 19%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/loss/LogLossMargin.cpp.o\n[ 19%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/loss/HingeLoss.cpp.o\n[ 19%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/loss/SquaredHingeLoss.cpp.o\n[ 19%] [ 19%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/statistics/MMDKernelSelection.cpp.o\nBuilding CXX object src/shogun/CMakeFiles/libshogun.dir/statistics/KernelMeanMatching.cpp.o\n[ 20%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/statistics/IndependenceTest.cpp.o\n[ 20%] [ 20%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/statistics/NOCCO.cpp.o\nBuilding CXX object src/shogun/CMakeFiles/libshogun.dir/statistics/LinearTimeMMD.cpp.o\n[ 20%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/statistics/HSIC.cpp.o\n[ 20%] [ 20%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/statistics/MMDKernelSelectionMax.cpp.o\nBuilding CXX object src/shogun/CMakeFiles/libshogun.dir/statistics/KernelSelection.cpp.o\n[ 20%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/statistics/KernelTwoSampleTest.cpp.o\n[ 20%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/statistics/MMDKernelSelectionCombOpt.cpp.o\n[ 21%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/statistics/MMDKernelSelectionOpt.cpp.o\n[ 21%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/statistics/MMDKernelSelectionMedian.cpp.o\n[ 21%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/statistics/MMDKernelSelectionComb.cpp.o\n[ 21%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/statistics/MMDKernelSelectionCombMaxL2.cpp.o\n[ 21%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/statistics/QuadraticTimeMMD.cpp.o\n[ 21%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/statistics/StreamingMMD.cpp.o\n[ 21%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/statistics/TwoSampleTest.cpp.o\n[ 22%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/statistics/KernelIndependenceTest.cpp.o\n[ 22%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/statistics/HypothesisTest.cpp.o\n[ 22%] [ 22%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/labels/RegressionLabels.cpp.o\nBuilding CXX object src/shogun/CMakeFiles/libshogun.dir/labels/FactorGraphLabels.cpp.o\n[ 22%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/labels/LabelsFactory.cpp.o\n[ 22%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/labels/BinaryLabels.cpp.o\n[ 22%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/labels/LatentLabels.cpp.o\n[ 22%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/labels/Labels.cpp.o\n[ 23%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/labels/MultilabelLabels.cpp.o\n[ 23%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/labels/StructuredLabels.cpp.o\n[ 23%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/labels/DenseLabels.cpp.o\n[ 23%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/labels/MulticlassLabels.cpp.o\n[ 23%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/regression/KernelRidgeRegression.cpp.o\n[ 23%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/regression/svr/MKLRegression.cpp.o\n[ 23%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/regression/svr/LibSVR.cpp.o\n[ 23%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/regression/svr/SVRLight.cpp.o\n[ 24%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/regression/svr/LibLinearRegression.cpp.o\n[ 24%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/regression/GaussianProcessRegression.cpp.o\n[ 24%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/regression/LinearRidgeRegression.cpp.o\n[ 24%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/regression/LeastAngleRegression.cpp.o\n[ 24%] [ 24%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/converter/EmbeddingConverter.cpp.o\nBuilding CXX object src/shogun/CMakeFiles/libshogun.dir/regression/LeastSquaresRegression.cpp.o\n[ 24%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/converter/ica/Jade.cpp.o\n[ 24%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/converter/ica/SOBI.cpp.o\n[ 25%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/converter/ica/FastICA.cpp.o\n[ 25%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/converter/ica/JediSep.cpp.o\n[ 25%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/converter/ica/FFSep.cpp.o\n[ 25%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/converter/ica/UWedgeSep.cpp.o\n[ 25%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/converter/ica/ICAConverter.cpp.o\n[ 25%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/converter/LaplacianEigenmaps.cpp.o\n[ 25%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/converter/HessianLocallyLinearEmbedding.cpp.o\n[ 25%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/converter/FactorAnalysis.cpp.o\n[ 26%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/converter/ManifoldSculpting.cpp.o\n[ 26%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/converter/MultidimensionalScaling.cpp.o\n[ 26%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/converter/HashedDocConverter.cpp.o\n[ 26%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/converter/LocalTangentSpaceAlignment.cpp.o\n[ 26%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/converter/KernelLocallyLinearEmbedding.cpp.o\n[ 26%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/converter/LocallyLinearEmbedding.cpp.o\n[ 26%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/converter/LocalityPreservingProjections.cpp.o\n[ 26%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/converter/StochasticProximityEmbedding.cpp.o\n[ 27%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/converter/DiffusionMaps.cpp.o\n[ 27%] [ 27%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/converter/TDistributedStochasticNeighborEmbedding.cpp.o\nBuilding CXX object src/shogun/CMakeFiles/libshogun.dir/converter/NeighborhoodPreservingEmbedding.cpp.o\n[ 27%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/converter/LinearLocalTangentSpaceAlignment.cpp.o\n[ 27%] [ 27%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/converter/Isomap.cpp.o\nBuilding CXX object src/shogun/CMakeFiles/libshogun.dir/transfer/domain_adaptation/DomainAdaptationSVMLinear.cpp.o\n[ 27%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/transfer/domain_adaptation/DomainAdaptationMulticlassLibLinear.cpp.o\n[ 27%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/transfer/domain_adaptation/DomainAdaptationSVM.cpp.o\n[ 28%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/transfer/multitask/TaskGroup.cpp.o\n[ 28%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/transfer/multitask/TaskTree.cpp.o\n[ 28%] [ 28%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/transfer/multitask/MultitaskClusteredLogisticRegression.cpp.o\nBuilding CXX object src/shogun/CMakeFiles/libshogun.dir/transfer/multitask/MultitaskL12LogisticRegression.cpp.o\n[ 28%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/transfer/multitask/MultitaskTraceLogisticRegression.cpp.o\n[ 28%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/transfer/multitask/MultitaskLogisticRegression.cpp.o\n[ 28%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/transfer/multitask/MultitaskLinearMachine.cpp.o\n[ 28%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/transfer/multitask/MultitaskLeastSquaresRegression.cpp.o\n[ 29%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/transfer/multitask/MultitaskROCEvaluation.cpp.o\n[ 29%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/transfer/multitask/LibLinearMTL.cpp.o\n[ 29%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/transfer/multitask/Task.cpp.o\n[ 29%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/io/streaming/StreamingFile.cpp.o\n[ 29%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/io/streaming/StreamingVwCacheFile.cpp.o\n[ 29%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/io/streaming/StreamingAsciiFile.cpp.o\n[ 29%] [ 29%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/io/streaming/StreamingVwFile.cpp.o\nBuilding CXX object src/shogun/CMakeFiles/libshogun.dir/io/streaming/StreamingFileFromFeatures.cpp.o\n[ 30%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/io/MLDataHDF5File.cpp.o\n[ 30%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/io/SerializableHdf5Reader00.cpp.o\n[ 30%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/io/SerializableAsciiReader00.cpp.o\n[ 30%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/io/CSVFile.cpp.o\n[ 30%] [ 30%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/io/LibSVMFile.cpp.o\nBuilding CXX object src/shogun/CMakeFiles/libshogun.dir/io/BinaryFile.cpp.o\n[ 30%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/io/SGIO.cpp.o\n[ 30%] [ 31%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/io/SerializableJsonFile.cpp.o\nBuilding CXX object src/shogun/CMakeFiles/libshogun.dir/io/SerializableFile.cpp.o\n[ 31%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/io/SerializableXmlFile.cpp.o\n[ 31%] [ 31%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/io/IOBuffer.cpp.o\nBuilding CXX object src/shogun/CMakeFiles/libshogun.dir/io/File.cpp.o\n[ 31%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/io/SerializableXmlReader00.cpp.o\n[ 31%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/io/SerializableJsonReader00.cpp.o\n[ 31%] [ 32%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/io/SerializableAsciiFile.cpp.o\nBuilding CXX object src/shogun/CMakeFiles/libshogun.dir/io/HDF5File.cpp.o\n[ 32%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/io/ProtobufFile.cpp.o\n[ 32%] [ 32%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/io/UAIFile.cpp.o\nBuilding CXX object src/shogun/CMakeFiles/libshogun.dir/io/Parser.cpp.o\n[ 32%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/io/SerializableHdf5File.cpp.o\n[ 32%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/io/LineReader.cpp.o\n[ 32%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/machine/LinearMachine.cpp.o\n[ 32%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/machine/LinearStructuredOutputMachine.cpp.o\n[ 33%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/machine/RandomForest.cpp.o\n[ 33%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/machine/KernelMulticlassMachine.cpp.o\n[ 33%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/machine/KernelStructuredOutputMachine.cpp.o\n[ 33%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/machine/Machine.cpp.o\n[ 33%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/machine/StructuredOutputMachine.cpp.o\n[ 33%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/machine/LinearLatentMachine.cpp.o\n[ 33%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/machine/OnlineLinearMachine.cpp.o\n[ 33%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/machine/KernelMachine.cpp.o\n[ 34%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/machine/StochasticGBMachine.cpp.o\n[ 34%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/machine/DistanceMachine.cpp.o\n[ 34%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/machine/BaggingMachine.cpp.o\n[ 34%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/machine/GaussianProcessMachine.cpp.o\n[ 34%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/machine/BaseMulticlassMachine.cpp.o\n[ 34%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/machine/gp/VariationalGaussianLikelihood.cpp.o\n[ 34%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/machine/gp/MultiLaplacianInferenceMethod.cpp.o\n[ 34%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/machine/gp/KLDualInferenceMethod.cpp.o\n[ 35%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/machine/gp/EPInferenceMethod.cpp.o\n[ 35%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/machine/gp/SingleLaplacianInferenceMethod.cpp.o\n[ 35%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/machine/gp/ConstMean.cpp.o\n[ 35%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/machine/gp/NumericalVGLikelihood.cpp.o\n[ 35%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/machine/gp/InferenceMethod.cpp.o\n[ 35%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/machine/gp/ZeroMean.cpp.o\n[ 35%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/machine/gp/StudentsTLikelihood.cpp.o\n[ 35%] [ 36%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/machine/gp/KLInferenceMethod.cpp.o\nBuilding CXX object src/shogun/CMakeFiles/libshogun.dir/machine/gp/DualVariationalGaussianLikelihood.cpp.o\n[ 36%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/machine/gp/LogitDVGLikelihood.cpp.o\n[ 36%] [ 36%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/machine/gp/LogitVGLikelihood.cpp.o\nBuilding CXX object src/shogun/CMakeFiles/libshogun.dir/machine/gp/ProbitLikelihood.cpp.o\n[ 36%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/machine/gp/SoftMaxLikelihood.cpp.o\n[ 36%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/machine/gp/MatrixOperations.cpp.o\n[ 36%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/machine/gp/VariationalLikelihood.cpp.o\n[ 36%] [ 37%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/machine/gp/KLCovarianceInferenceMethod.cpp.o\nBuilding CXX object src/shogun/CMakeFiles/libshogun.dir/machine/gp/LikelihoodModel.cpp.o\n[ 37%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/machine/gp/KLApproxDiagonalInferenceMethod.cpp.o\n[ 37%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/machine/gp/StudentsTVGLikelihood.cpp.o\n[ 37%] [ 37%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/machine/gp/SingleLaplacianInferenceMethodWithLBFGS.cpp.o\nBuilding CXX object src/shogun/CMakeFiles/libshogun.dir/machine/gp/KLCholeskyInferenceMethod.cpp.o\n[ 37%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/machine/gp/FITCInferenceMethod.cpp.o\n[ 37%] [ 37%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/machine/gp/ExactInferenceMethod.cpp.o\nBuilding CXX object src/shogun/CMakeFiles/libshogun.dir/machine/gp/GaussianLikelihood.cpp.o\n[ 38%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/machine/gp/LogitVGPiecewiseBoundLikelihood.cpp.o\n[ 38%] [ 38%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/machine/gp/LaplacianInferenceBase.cpp.o\nBuilding CXX object src/shogun/CMakeFiles/libshogun.dir/machine/gp/KLLowerTriangularInferenceMethod.cpp.o\n[ 38%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/machine/gp/LogitLikelihood.cpp.o\n[ 38%] [ 38%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/machine/gp/ProbitVGLikelihood.cpp.o\nBuilding CXX object src/shogun/CMakeFiles/libshogun.dir/machine/MulticlassMachine.cpp.o\n[ 38%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/lib/memory.cpp.o\n[ 38%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/lib/GPUVector.cpp.o\n[ 39%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/lib/tapkee/tapkee_shogun.cpp.o\n[ 39%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/lib/DataType.cpp.o\n[ 39%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/lib/slep/slep_mc_tree_lr.cpp.o\n[ 39%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/lib/slep/slep_mc_plain_lr.cpp.o\n[ 39%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/lib/slep/SpInvCoVa/invCov.cpp.o\n[ 39%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/lib/slep/tree/general_altra.cpp.o\n[ 39%] [ 39%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/lib/slep/tree/altra.cpp.o\nBuilding CXX object src/shogun/CMakeFiles/libshogun.dir/lib/slep/q1/eppMatrix.cpp.o\n[ 40%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/lib/slep/q1/epph.cpp.o\n[ 40%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/lib/slep/flsa/tesla_proj.cpp.o\n[ 40%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/lib/slep/flsa/flsa.cpp.o\n[ 40%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/lib/slep/flsa/sfa.cpp.o\n[ 40%] [ 40%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/lib/slep/overlapping/overlapping.cpp.o\nBuilding CXX object src/shogun/CMakeFiles/libshogun.dir/lib/slep/slep_solver.cpp.o\n[ 40%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/lib/external/brent.cpp.o\n[ 40%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/lib/external/gpm.cpp.o\n[ 41%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/lib/external/gpdt.cpp.o\n[ 41%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/lib/external/libocas.cpp.o\n[ 41%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/lib/external/shogun_libsvm.cpp.o\n[ 41%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/lib/external/libqp_gsmo.cpp.o\n[ 41%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/lib/external/gpdtsolve.cpp.o\n[ 41%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/lib/external/libqp_splx.cpp.o\n[ 41%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/lib/external/pr_loqo.cpp.o\n[ 42%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/lib/external/ssl.cpp.o\n[ 42%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/lib/SGReferencedData.cpp.o\n[ 42%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/lib/Tokenizer.cpp.o\n[ 42%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/lib/GPUMatrix.cpp.o\n[ 42%] [ 42%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/lib/SGNDArray.cpp.o\nBuilding CXX object src/shogun/CMakeFiles/libshogun.dir/lib/Data.cpp.o\n[ 42%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/lib/malsar/malsar_low_rank.cpp.o\n[ 42%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/lib/malsar/malsar_clustered.cpp.o\n[ 43%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/lib/malsar/malsar_joint_feature_learning.cpp.o\n[ 43%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/lib/Lock.cpp.o\n[ 43%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/lib/IndexBlockRelation.cpp.o\n[ 43%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/lib/computation/jobresult/VectorResult.cpp.o\n[ 43%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/lib/computation/jobresult/ScalarResult.cpp.o\n[ 43%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/lib/computation/aggregator/StoreScalarAggregator.cpp.o\n[ 43%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/lib/computation/aggregator/StoreVectorAggregator.cpp.o\n[ 43%] [ 44%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/lib/computation/engine/SerialComputationEngine.cpp.o\nBuilding CXX object src/shogun/CMakeFiles/libshogun.dir/lib/Time.cpp.o\n[ 44%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/lib/SGString.cpp.o\n[ 44%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/lib/IndexBlockTree.cpp.o\n[ 44%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/lib/NGramTokenizer.cpp.o\n[ 44%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/lib/CircularBuffer.cpp.o\n[ 44%] [ 44%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/lib/Compressor.cpp.o\nBuilding CXX object src/shogun/CMakeFiles/libshogun.dir/lib/StructuredData.cpp.o\n[ 44%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/lib/SGMatrixList.cpp.o\n[ 45%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/lib/DelimiterTokenizer.cpp.o\n[ 45%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/lib/OpenCV/OpenCVTypeName.cpp.o\n[ 45%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/lib/OpenCV/CV2SGFactory.cpp.o\n[ 45%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/lib/OpenCV/SG2CVFactory.cpp.o\n[ 45%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/lib/SGSparseMatrix.cpp.o\n[ 45%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/lib/SGVector.cpp.o\n[ 45%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/lib/SGMatrix.cpp.o\n[ 45%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/lib/Signal.cpp.o\n[ 46%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/lib/Trie.cpp.o\n[ 46%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/lib/Hash.cpp.o\n[ 46%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/lib/RefCount.cpp.o\n[ 46%] [ 46%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/lib/ShogunException.cpp.o\nBuilding CXX object src/shogun/CMakeFiles/libshogun.dir/lib/SGSparseVector.cpp.o\n[ 46%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/lib/SGStringList.cpp.o\n[ 46%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/lib/IndexBlock.cpp.o\n[ 46%] [ 47%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/lib/IndexBlockGroup.cpp.o\nBuilding CXX object src/shogun/CMakeFiles/libshogun.dir/kernel/SplineKernel.cpp.o\n[ 47%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/kernel/CombinedKernel.cpp.o\n[ 47%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/kernel/GaussianShortRealKernel.cpp.o\n[ 47%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/kernel/CauchyKernel.cpp.o\n[ 47%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/kernel/WeightedDegreeRBFKernel.cpp.o\n[ 47%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/kernel/MultiquadricKernel.cpp.o\n[ 47%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/kernel/RationalQuadraticKernel.cpp.o\n[ 47%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/kernel/ExponentialKernel.cpp.o\n[ 48%] [ 48%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/kernel/JensenShannonKernel.cpp.o\nBuilding CXX object src/shogun/CMakeFiles/libshogun.dir/kernel/BesselKernel.cpp.o\n[ 48%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/kernel/ConstKernel.cpp.o\n[ 48%] [ 48%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/kernel/WaveKernel.cpp.o\nBuilding CXX object src/shogun/CMakeFiles/libshogun.dir/kernel/Chi2Kernel.cpp.o\n[ 48%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/kernel/LinearARDKernel.cpp.o\n[ 48%] [ 48%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/kernel/GaussianARDKernel.cpp.o\nBuilding CXX object src/shogun/CMakeFiles/libshogun.dir/kernel/AUCKernel.cpp.o\n[ 49%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/kernel/HistogramIntersectionKernel.cpp.o\n[ 49%] [ 49%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/kernel/CustomKernel.cpp.o\nBuilding CXX object src/shogun/CMakeFiles/libshogun.dir/kernel/CircularKernel.cpp.o\n[ 49%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/kernel/PolyKernel.cpp.o\n[ 49%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/kernel/LogKernel.cpp.o\n[ 49%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/kernel/TStudentKernel.cpp.o\n[ 49%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/kernel/TensorProductPairKernel.cpp.o\n[ 49%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/kernel/PyramidChi2.cpp.o\n[ 50%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/kernel/DiagKernel.cpp.o\n[ 50%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/kernel/ANOVAKernel.cpp.o\n[ 50%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/kernel/GaussianKernel.cpp.o\n[ 50%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/kernel/Kernel.cpp.o\n[ 50%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/kernel/SphericalKernel.cpp.o\n[ 50%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/kernel/DistanceKernel.cpp.o\n[ 50%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/kernel/ProductKernel.cpp.o\n[ 50%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/kernel/LinearKernel.cpp.o\n[ 51%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/kernel/GaussianShiftKernel.cpp.o\n[ 51%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/kernel/PowerKernel.cpp.o\n[ 51%] [ 51%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/kernel/InverseMultiQuadricKernel.cpp.o\nBuilding CXX object src/shogun/CMakeFiles/libshogun.dir/kernel/WaveletKernel.cpp.o\n[ 51%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/kernel/SigmoidKernel.cpp.o\n[ 51%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/kernel/string/SparseSpatialSampleStringKernel.cpp.o\n[ 51%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/kernel/string/SalzbergWordStringKernel.cpp.o\n[ 52%] [ 52%] [ 52%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/kernel/string/DistantSegmentsKernel.cpp.o\nBuilding CXX object src/shogun/CMakeFiles/libshogun.dir/kernel/string/PolyMatchWordStringKernel.cpp.o\nBuilding CXX object src/shogun/CMakeFiles/libshogun.dir/kernel/string/PolyMatchStringKernel.cpp.o\n[ 52%] [ 52%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/kernel/string/CommWordStringKernel.cpp.o\n[ 52%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/kernel/string/CommUlongStringKernel.cpp.o\nBuilding CXX object src/shogun/CMakeFiles/libshogun.dir/kernel/string/LocalityImprovedStringKernel.cpp.o\n[ 52%] [ 52%] [ 53%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/kernel/string/RegulatoryModulesStringKernel.cpp.o\nBuilding CXX object src/shogun/CMakeFiles/libshogun.dir/kernel/string/WeightedDegreeStringKernel.cpp.o\nBuilding CXX object src/shogun/CMakeFiles/libshogun.dir/kernel/string/FixedDegreeStringKernel.cpp.o\n[ 53%] [ 53%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/kernel/string/WeightedCommWordStringKernel.cpp.o\nBuilding CXX object src/shogun/CMakeFiles/libshogun.dir/kernel/string/OligoStringKernel.cpp.o\n[ 53%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/kernel/string/SpectrumRBFKernel.cpp.o\n[ 53%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/kernel/string/SimpleLocalityImprovedStringKernel.cpp.o\n[ 53%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/kernel/string/SubsequenceStringKernel.cpp.o\n[ 53%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/kernel/string/LocalAlignmentStringKernel.cpp.o\n[ 53%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/kernel/string/LinearStringKernel.cpp.o\n[ 54%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/kernel/string/GaussianMatchStringKernel.cpp.o\n[ 54%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/kernel/string/HistogramWordStringKernel.cpp.o\n[ 54%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/kernel/string/WeightedDegreePositionStringKernel.cpp.o\n[ 54%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/kernel/string/SpectrumMismatchRBFKernel.cpp.o\n[ 54%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/kernel/string/MatchWordStringKernel.cpp.o\n[ 54%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/kernel/string/SNPStringKernel.cpp.o\n[ 54%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/optimization/liblinear/tron.cpp.o\n[ 54%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/optimization/liblinear/shogun_liblinear.cpp.o\n[ 55%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/optimization/lbfgs/lbfgs.cpp.o\n[ 55%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/evaluation/ica/AmariIndex.cpp.o\n[ 55%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/evaluation/ica/PermutationMatrix.cpp.o\n[ 55%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/evaluation/PRCEvaluation.cpp.o\n[ 55%] [ 55%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/evaluation/MeanAbsoluteError.cpp.o\nBuilding CXX object src/shogun/CMakeFiles/libshogun.dir/evaluation/MeanSquaredLogError.cpp.o\n[ 55%] [ 55%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/evaluation/SplittingStrategy.cpp.o\nBuilding CXX object src/shogun/CMakeFiles/libshogun.dir/evaluation/LOOCrossValidationSplitting.cpp.o\n[ 56%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/evaluation/ClusteringEvaluation.cpp.o\n[ 56%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/evaluation/ClusteringAccuracy.cpp.o\n[ 56%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/evaluation/MulticlassAccuracy.cpp.o\n[ 56%] [ 56%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/evaluation/CrossValidationSplitting.cpp.o\nBuilding CXX object src/shogun/CMakeFiles/libshogun.dir/evaluation/StratifiedCrossValidationSplitting.cpp.o\n[ 56%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/evaluation/MultilabelAccuracy.cpp.o\n[ 56%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/evaluation/CrossValidationMulticlassStorage.cpp.o\n[ 56%] [ 57%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/evaluation/CrossValidation.cpp.o\nBuilding CXX object src/shogun/CMakeFiles/libshogun.dir/evaluation/StructuredAccuracy.cpp.o\n[ 57%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/evaluation/MeanSquaredError.cpp.o\n[ 57%] [ 57%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/evaluation/CrossValidationPrintOutput.cpp.o\nBuilding CXX object src/shogun/CMakeFiles/libshogun.dir/evaluation/MachineEvaluation.cpp.o\n[ 57%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/evaluation/ContingencyTableEvaluation.cpp.o\n[ 57%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/evaluation/ClusteringMutualInformation.cpp.o\n[ 57%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/evaluation/GradientEvaluation.cpp.o\n[ 57%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/evaluation/DifferentiableFunction.cpp.o\n[ 58%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/evaluation/ROCEvaluation.cpp.o\n[ 58%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/evaluation/CrossValidationMKLStorage.cpp.o\n[ 58%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/evaluation/MulticlassOVREvaluation.cpp.o\n[ 58%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/base/Parallel.cpp.o\n[ 58%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/base/init.cpp.o\n[ 58%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/base/Version.cpp.o\n[ 58%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/base/ParameterMap.cpp.o\n[ 58%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/base/Parameter.cpp.o\n[ 59%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/base/SGObject.cpp.o\n[ 59%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/multiclass/MulticlassSVM.cpp.o\n[ 59%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/multiclass/MulticlassOneVsOneStrategy.cpp.o\n[ 59%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/multiclass/MulticlassLibLinear.cpp.o\n[ 59%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/multiclass/LaRank.cpp.o\n[ 59%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/multiclass/ShareBoostOptimizer.cpp.o\n[ 59%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/multiclass/ecoc/ECOCIHDDecoder.cpp.o\n[ 59%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/multiclass/ecoc/ECOCForestEncoder.cpp.o\n[ 60%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/multiclass/ecoc/ECOCStrategy.cpp.o\n[ 60%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/multiclass/ecoc/ECOCSimpleDecoder.cpp.o\n[ 60%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/multiclass/ecoc/ECOCRandomSparseEncoder.cpp.o\n[ 60%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/multiclass/ecoc/ECOCDecoder.cpp.o\n[ 60%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/multiclass/ecoc/ECOCOVREncoder.cpp.o\n[ 60%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/multiclass/ecoc/ECOCLLBDecoder.cpp.o\n[ 60%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/multiclass/ecoc/ECOCDiscriminantEncoder.cpp.o\n[ 60%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/multiclass/ecoc/ECOCRandomDenseEncoder.cpp.o\n[ 61%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/multiclass/ecoc/ECOCOVOEncoder.cpp.o\n[ 61%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/multiclass/KNN.cpp.o\n[ 61%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/multiclass/GaussianNaiveBayes.cpp.o\n[ 61%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/multiclass/ShareBoost.cpp.o\n[ 61%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/multiclass/MulticlassOCAS.cpp.o\n[ 61%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/multiclass/MulticlassLogisticRegression.cpp.o\n[ 61%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/multiclass/MulticlassTreeGuidedLogisticRegression.cpp.o\n[ 62%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/multiclass/GMNPLib.cpp.o\n[ 62%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/multiclass/tree/CARTree.cpp.o\n[ 62%] [ 62%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/multiclass/tree/RelaxedTree.cpp.o\nBuilding CXX object src/shogun/CMakeFiles/libshogun.dir/multiclass/tree/C45ClassifierTree.cpp.o\n[ 62%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/multiclass/tree/ID3ClassifierTree.cpp.o\n[ 62%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/multiclass/tree/KNNHeap.cpp.o\n[ 62%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/multiclass/tree/BalancedConditionalProbabilityTree.cpp.o\n[ 62%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/multiclass/tree/ConditionalProbabilityTree.cpp.o\n[ 63%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/multiclass/tree/KDTree.cpp.o\n[ 63%] [ 63%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/multiclass/tree/VwConditionalProbabilityTree.cpp.o\nBuilding CXX object src/shogun/CMakeFiles/libshogun.dir/multiclass/tree/RandomConditionalProbabilityTree.cpp.o\n[ 63%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/multiclass/tree/CHAIDTree.cpp.o\n[ 63%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/multiclass/tree/BallTree.cpp.o\n[ 63%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/multiclass/tree/RelaxedTreeUtil.cpp.o\n[ 63%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/multiclass/tree/NbodyTree.cpp.o\n[ 63%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/multiclass/tree/RandomCARTree.cpp.o\n[ 64%] [ 64%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/multiclass/MulticlassLibSVM.cpp.o\nBuilding CXX object src/shogun/CMakeFiles/libshogun.dir/multiclass/QDA.cpp.o\n[ 64%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/multiclass/ScatterSVM.cpp.o\n[ 64%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/multiclass/MulticlassOneVsRestStrategy.cpp.o\n[ 64%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/multiclass/MCLDA.cpp.o\n[ 64%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/multiclass/GMNPSVM.cpp.o\n[ 64%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/multiclass/MulticlassStrategy.cpp.o\n[ 64%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/neuralnets/NeuralConvolutionalLayer.cpp.o\n[ 65%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/neuralnets/NeuralSoftmaxLayer.cpp.o\n[ 65%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/neuralnets/NeuralLinearLayer.cpp.o\n[ 65%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/neuralnets/NeuralInputLayer.cpp.o\n[ 65%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/neuralnets/DeepAutoencoder.cpp.o\n[ 65%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/neuralnets/ConvolutionalFeatureMap.cpp.o\n[ 65%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/neuralnets/NeuralLogisticLayer.cpp.o\n[ 65%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/neuralnets/DeepBeliefNetwork.cpp.o\n[ 65%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/neuralnets/NeuralLayer.cpp.o\n[ 66%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/neuralnets/Autoencoder.cpp.o\n[ 66%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/neuralnets/NeuralRectifiedLinearLayer.cpp.o\n[ 66%] [ 66%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/neuralnets/NeuralNetwork.cpp.o\nBuilding CXX object src/shogun/CMakeFiles/libshogun.dir/neuralnets/NeuralLayers.cpp.o\n[ 66%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/neuralnets/RBM.cpp.o\n[ 66%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/ui/GUIDistance.cpp.o\n[ 66%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/ui/GUIPluginEstimate.cpp.o\n[ 66%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/ui/SGInterface.cpp.o\n[ 67%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/ui/GUILabels.cpp.o\n[ 67%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/ui/GUIStructure.cpp.o\n[ 67%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/ui/GUIFeatures.cpp.o\n[ 67%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/ui/GUIMath.cpp.o\n[ 67%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/ui/GUITime.cpp.o\n[ 67%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/ui/GUIHMM.cpp.o\n[ 67%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/ui/GUIConverter.cpp.o\n[ 67%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/ui/GUIClassifier.cpp.o\n[ 68%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/ui/GUIPreprocessor.cpp.o\n[ 68%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/ui/GUIKernel.cpp.o\n[ 68%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/latent/LatentModel.cpp.o\n[ 68%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/latent/DirectorLatentModel.cpp.o\n[ 68%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/latent/LatentSOSVM.cpp.o\n[ 68%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/latent/LatentSVM.cpp.o\n[ 68%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/structure/MulticlassModel.cpp.o\n[ 68%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/structure/PlifArray.cpp.o\n[ 69%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/structure/HMSVMModel.cpp.o\n[ 69%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/structure/MultilabelSOLabels.cpp.o\n[ 69%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/structure/FWSOSVM.cpp.o\n[ 69%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/structure/TwoStateModel.cpp.o\n[ 69%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/structure/MultilabelCLRModel.cpp.o\n[ 69%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/structure/MAPInference.cpp.o\n[ 69%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/structure/FactorType.cpp.o\n[ 69%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/structure/StateModel.cpp.o\n[ 70%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/structure/SOSVMHelper.cpp.o\n[ 70%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/structure/FactorGraphModel.cpp.o\n[ 70%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/structure/MulticlassSOLabels.cpp.o\n[ 70%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/structure/DirectorStructuredModel.cpp.o\n[ 70%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/structure/PlifMatrix.cpp.o\n[ 70%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/structure/libppbm.cpp.o\n[ 70%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/structure/GraphCut.cpp.o\n[ 70%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/structure/BeliefPropagation.cpp.o\n[ 71%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/structure/HierarchicalMultilabelModel.cpp.o\n[ 71%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/structure/SequenceLabels.cpp.o\n[ 71%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/structure/HashedMultilabelModel.cpp.o\n[ 71%] [ 71%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/structure/StructuredModel.cpp.o\nBuilding CXX object src/shogun/CMakeFiles/libshogun.dir/structure/FactorGraph.cpp.o\n[ 71%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/structure/SegmentLoss.cpp.o\n[ 71%] [ 72%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/structure/libbmrm.cpp.o\nBuilding CXX object src/shogun/CMakeFiles/libshogun.dir/structure/Factor.cpp.o\n[ 72%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/structure/libncbm.cpp.o\n[ 72%] [ 72%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/structure/MultilabelModel.cpp.o\nBuilding CXX object src/shogun/CMakeFiles/libshogun.dir/structure/IntronList.cpp.o\n[ 72%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/structure/CCSOSVM.cpp.o\n[ 72%] [ 72%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/structure/Plif.cpp.o\nBuilding CXX object src/shogun/CMakeFiles/libshogun.dir/structure/DualLibQPBMSOSVM.cpp.o\n[ 72%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/structure/PrimalMosekSOSVM.cpp.o\n[ 73%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/structure/DisjointSet.cpp.o\n[ 73%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/structure/libp3bm.cpp.o\n[ 73%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/structure/StochasticSOSVM.cpp.o\n[ 73%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/structure/DynProg.cpp.o\n[ 73%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/classifier/FeatureBlockLogisticRegression.cpp.o\n[ 73%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/classifier/LPM.cpp.o\n[ 73%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/classifier/NearestCentroid.cpp.o\n[ 73%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/classifier/LPBoost.cpp.o\n[ 74%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/classifier/svm/NewtonSVM.cpp.o\n[ 74%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/classifier/svm/LibSVMOneClass.cpp.o\n[ 74%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/classifier/svm/SVMSGD.cpp.o\n[ 74%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/classifier/svm/OnlineSVMSGD.cpp.o\n[ 74%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/classifier/svm/SVM.cpp.o\n[ 74%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/classifier/svm/GPBTSVM.cpp.o\n[ 74%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/classifier/svm/SVMOcas.cpp.o\n[ 74%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/classifier/svm/SVMLightOneClass.cpp.o\n[ 75%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/classifier/svm/QPBSVMLib.cpp.o\n[ 75%] [ 75%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/classifier/svm/GNPPLib.cpp.o\nBuilding CXX object src/shogun/CMakeFiles/libshogun.dir/classifier/svm/SVMLin.cpp.o\n[ 75%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/classifier/svm/SGDQN.cpp.o\n[ 75%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/classifier/svm/MPDSVM.cpp.o\n[ 75%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/classifier/svm/CPLEXSVM.cpp.o\n[ 75%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/classifier/svm/SVMLight.cpp.o\n[ 75%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/classifier/svm/WDSVMOcas.cpp.o\n[ 76%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/classifier/svm/GNPPSVM.cpp.o\n[ 76%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/classifier/svm/LibLinear.cpp.o\n[ 76%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/classifier/svm/OnlineLibLinear.cpp.o\n[ 76%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/classifier/svm/LibSVM.cpp.o\n[ 76%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/classifier/Perceptron.cpp.o\n[ 76%] [ 76%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/classifier/mkl/MKLMulticlassOptimizationBase.cpp.o\nBuilding CXX object src/shogun/CMakeFiles/libshogun.dir/classifier/mkl/MKLClassification.cpp.o\n[ 76%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/classifier/mkl/MKLMulticlass.cpp.o\n[ 77%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/classifier/mkl/MKLMulticlassGLPK.cpp.o\n[ 77%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/classifier/mkl/MKL.cpp.o\n[ 77%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/classifier/mkl/MKLOneClass.cpp.o\n[ 77%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/classifier/mkl/MKLMulticlassGradient.cpp.o\n[ 77%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/classifier/AveragedPerceptron.cpp.o\n[ 77%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/classifier/vw/vw_math.cpp.o\n[ 77%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/classifier/vw/learners/VwAdaptiveLearner.cpp.o\n[ 77%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/classifier/vw/learners/VwNonAdaptiveLearner.cpp.o\n[ 78%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/classifier/vw/VwParser.cpp.o\n[ 78%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/classifier/vw/vw_label.cpp.o\n[ 78%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/classifier/vw/VowpalWabbit.cpp.o\n[ 78%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/classifier/vw/VwEnvironment.cpp.o\n[ 78%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/classifier/vw/vw_example.cpp.o\n[ 78%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/classifier/vw/VwRegressor.cpp.o\n[ 78%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/classifier/vw/cache/VwNativeCacheReader.cpp.o\n[ 78%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/classifier/vw/cache/VwCacheWriter.cpp.o\n[ 79%] [ 79%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/classifier/vw/cache/VwCacheReader.cpp.o\nBuilding CXX object src/shogun/CMakeFiles/libshogun.dir/classifier/vw/cache/VwNativeCacheWriter.cpp.o\n[ 79%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/classifier/GaussianProcessClassification.cpp.o\n[ 79%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/classifier/PluginEstimate.cpp.o\n[ 79%] [ 79%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/classifier/LDA.cpp.o\nBuilding CXX object src/shogun/CMakeFiles/libshogun.dir/distance/ManhattanMetric.cpp.o\n[ 79%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/distance/CanberraMetric.cpp.o\n[ 79%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/distance/EuclideanDistance.cpp.o\n[ 80%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/distance/ManhattanWordDistance.cpp.o\n[ 80%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/distance/ChebyshewMetric.cpp.o\n[ 80%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/distance/JensenMetric.cpp.o\n[ 80%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/distance/MahalanobisDistance.cpp.o\n[ 80%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/distance/HammingWordDistance.cpp.o\n[ 80%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/distance/ChiSquareDistance.cpp.o\n[ 80%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/distance/CosineDistance.cpp.o\n[ 80%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/distance/AttenuatedEuclideanDistance.cpp.o\n[ 81%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/distance/CanberraWordDistance.cpp.o\n[ 81%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/distance/Distance.cpp.o\n[ 81%] [ 81%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/distance/GeodesicMetric.cpp.o\nBuilding CXX object src/shogun/CMakeFiles/libshogun.dir/distance/KernelDistance.cpp.o\n[ 81%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/distance/CustomMahalanobisDistance.cpp.o\n[ 81%] [ 81%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/distance/DenseDistance.cpp.o\nBuilding CXX object src/shogun/CMakeFiles/libshogun.dir/distance/BrayCurtisDistance.cpp.o\n[ 82%] [ 82%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/distance/TanimotoDistance.cpp.o\nBuilding CXX object src/shogun/CMakeFiles/libshogun.dir/distance/SparseEuclideanDistance.cpp.o\n[ 82%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/distance/CustomDistance.cpp.o\n[ 82%] [ 82%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/distance/MinkowskiMetric.cpp.o\nBuilding CXX object src/shogun/CMakeFiles/libshogun.dir/clustering/KMeansLloydImpl.cpp.o\n[ 82%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/clustering/Hierarchical.cpp.o\n[ 82%] [ 82%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/clustering/GMM.cpp.o\nBuilding CXX object src/shogun/CMakeFiles/libshogun.dir/clustering/KMeans.cpp.o\n[ 83%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/clustering/KMeansMiniBatchImpl.cpp.o\n[ 83%] [ 83%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/distributions/HMM.cpp.o\nBuilding CXX object src/shogun/CMakeFiles/libshogun.dir/distributions/Gaussian.cpp.o\n[ 83%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/distributions/classical/GaussianDistribution.cpp.o\n[ 83%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/distributions/classical/ProbabilityDistribution.cpp.o\n[ 83%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/distributions/MixtureModel.cpp.o\n[ 83%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/distributions/LinearHMM.cpp.o\n[ 83%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/distributions/Histogram.cpp.o\n[ 84%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/distributions/EMMixtureModel.cpp.o\n[ 84%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/distributions/Distribution.cpp.o\n[ 84%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/distributions/PositionalPWM.cpp.o\n[ 84%] [ 84%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/distributions/KernelDensity.cpp.o\nBuilding C object src/shogun/CMakeFiles/libshogun.dir/lib/external/dSFMT/dSFMT.c.o\n[ 84%] Building C object src/shogun/CMakeFiles/libshogun.dir/lib/external/PMurHash.c.o\n[ 84%] Building C object src/shogun/CMakeFiles/libshogun.dir/lib/external/SFMT/SFMT.c.o\n[ 84%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/io/protobuf/Headers.pb.cc.o\n[ 85%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/io/protobuf/ShogunVersion.pb.cc.o\n[ 85%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/io/protobuf/Chunks.pb.cc.o\n[ 85%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/base/class_list.cpp.o\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.cc: In member function \u2018virtual int shogun::BoolChunk::ByteSize() const\u2019:\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.cc:395:9: warning: declaration of \u2018data_size\u2019 shadows a member of 'this' [-Wshadow]\n     int data_size = 0;\n         ^\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.cc: In member function \u2018virtual int shogun::Int32Chunk::ByteSize() const\u2019:\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.cc:620:9: warning: declaration of \u2018data_size\u2019 shadows a member of 'this' [-Wshadow]\n     int data_size = 0;\n         ^\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.cc: In member function \u2018virtual int shogun::UInt32Chunk::ByteSize() const\u2019:\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.cc:848:9: warning: declaration of \u2018data_size\u2019 shadows a member of 'this' [-Wshadow]\n     int data_size = 0;\n         ^\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.cc: In member function \u2018virtual int shogun::Int64Chunk::ByteSize() const\u2019:\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.cc:1076:9: warning: declaration of \u2018data_size\u2019 shadows a member of 'this' [-Wshadow]\n     int data_size = 0;\n         ^\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.cc: In member function \u2018virtual int shogun::UInt64Chunk::ByteSize() const\u2019:\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.cc:1304:9: warning: declaration of \u2018data_size\u2019 shadows a member of 'this' [-Wshadow]\n     int data_size = 0;\n         ^\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.cc: In member function \u2018virtual int shogun::Float32Chunk::ByteSize() const\u2019:\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.cc:1532:9: warning: declaration of \u2018data_size\u2019 shadows a member of 'this' [-Wshadow]\n     int data_size = 0;\n         ^\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.cc: In member function \u2018virtual int shogun::Float64Chunk::ByteSize() const\u2019:\n/home/bee/Downloads/shogun/src/shogun/io/protobuf/Chunks.pb.cc:1757:9: warning: declaration of \u2018data_size\u2019 shadows a member of 'this' [-Wshadow]\n     int data_size = 0;\n         ^\n[ 86%] Built target libshogun\nScanning dependencies of target shogun\nScanning dependencies of target shogun-static\nLinking CXX static library libshogun.a\nLinking CXX shared library libshogun.so\n[ 86%] Built target shogun-static\n[ 86%] Built target shogun\nScanning dependencies of target base_load_file_parameters\nScanning dependencies of target base_load_all_file_parameters\nScanning dependencies of target balanced_conditional_probability_tree\n[ 86%] [ 86%] Building CXX object examples/undocumented/libshogun/CMakeFiles/base_load_all_file_parameters.dir/base_load_all_file_parameters.cpp.o\nBuilding CXX object examples/undocumented/libshogun/CMakeFiles/base_load_file_parameters.dir/base_load_file_parameters.cpp.o\n[ 86%] Building CXX object examples/undocumented/libshogun/CMakeFiles/balanced_conditional_probability_tree.dir/balanced_conditional_probability_tree.cpp.o\nLinking CXX executable base_load_file_parameters\nLinking CXX executable base_load_all_file_parameters\nLinking CXX executable balanced_conditional_probability_tree\n[ 86%] Built target balanced_conditional_probability_tree\nCMakeFiles/base_load_all_file_parameters.dir/base_load_all_file_parameters.cpp.o: In CMakeFiles/base_load_file_parameters.dir/base_load_file_parameters.cpp.o: In function test_load_file_parameters()':\nbase_load_file_parameters.cpp:(.text+0x76): warning: the use ofmktemp' is dangerous, better use mkstemp' ormkdtemp'\nfunction test_load_file_parameter()':\nbase_load_all_file_parameters.cpp:(.text+0x90): warning: the use ofmktemp' is dangerous, better use mkstemp' ormkdtemp'\nScanning dependencies of target base_map_parameters\n[ 86%] [ 86%] Built target base_load_file_parameters\nBuilt target base_load_all_file_parameters\nScanning dependencies of target base_migration_multiple_dependencies\nScanning dependencies of target base_migration_dropping_and_new\n[ 86%] Building CXX object examples/undocumented/libshogun/CMakeFiles/base_map_parameters.dir/base_map_parameters.cpp.o\n[ 86%] Building CXX object examples/undocumented/libshogun/CMakeFiles/base_migration_multiple_dependencies.dir/base_migration_multiple_dependencies.cpp.o\n[ 87%] Building CXX object examples/undocumented/libshogun/CMakeFiles/base_migration_dropping_and_new.dir/base_migration_dropping_and_new.cpp.o\nLinking CXX executable base_map_parameters\nLinking CXX executable base_migration_multiple_dependencies\nLinking CXX executable base_migration_dropping_and_new\nCMakeFiles/base_migration_multiple_dependencies.dir/base_migration_multiple_dependencies.cpp.o: In function test_migration()':\nbase_migration_multiple_dependencies.cpp:(.text+0x8d): warning: the use ofmktemp' is dangerous, better use mkstemp' ormkdtemp'\nCMakeFiles/base_map_parameters.dir/base_map_parameters.cpp.o: In function test_load_file_parameter()':\nbase_map_parameters.cpp:(.text+0x71): warning: the use ofmktemp' is dangerous, better use mkstemp' ormkdtemp'\n[ 87%] [ 87%] CMakeFiles/base_migration_dropping_and_new.dir/base_migration_dropping_and_new.cpp.o: In function test_migration()':\nbase_migration_dropping_and_new.cpp:(.text+0x1b3): warning: the use ofmktemp' is dangerous, better use mkstemp' ormkdtemp'\nBuilt target base_map_parameters\nBuilt target base_migration_multiple_dependencies\n[ 87%] Built target base_migration_dropping_and_new\nScanning dependencies of target base_migration_new_buggy\nScanning dependencies of target base_migration_type_conversion\nScanning dependencies of target base_parameter_map\n[ 87%] Building CXX object examples/undocumented/libshogun/CMakeFiles/base_migration_new_buggy.dir/base_migration_new_buggy.cpp.o\n[ 87%] [ 87%] Building CXX object examples/undocumented/libshogun/CMakeFiles/base_parameter_map.dir/base_parameter_map.cpp.o\nBuilding CXX object examples/undocumented/libshogun/CMakeFiles/base_migration_type_conversion.dir/base_migration_type_conversion.cpp.o\nLinking CXX executable base_parameter_map\nLinking CXX executable base_migration_new_buggy\nLinking CXX executable base_migration_type_conversion\n[ 87%] Built target base_parameter_map\nCMakeFiles/base_migration_new_buggy.dir/base_migration_new_buggy.cpp.o: In function test()':\nbase_migration_new_buggy.cpp:(.text+0x54): warning: the use ofmktemp' is dangerous, better use mkstemp' ormkdtemp'\n[ 87%] Scanning dependencies of target basic_minimal\nBuilt target base_migration_new_buggy\nCMakeFiles/base_migration_type_conversion.dir/base_migration_type_conversion.cpp.o: In function test_migration()':\nbase_migration_type_conversion.cpp:(.text+0x7ec): warning: the use ofmktemp' is dangerous, better use mkstemp' ormkdtemp'\nScanning dependencies of target classifier_featureblocklogisticregression\n[ 87%] Built target base_migration_type_conversion\n[ 87%] Scanning dependencies of target classifier_gaussian_process_binary_classification\nBuilding CXX object examples/undocumented/libshogun/CMakeFiles/basic_minimal.dir/basic_minimal.cpp.o\n[ 87%] Building CXX object examples/undocumented/libshogun/CMakeFiles/classifier_featureblocklogisticregression.dir/classifier_featureblocklogisticregression.cpp.o\nLinking CXX executable basic_minimal\n[ 87%] Building CXX object examples/undocumented/libshogun/CMakeFiles/classifier_gaussian_process_binary_classification.dir/classifier_gaussian_process_binary_classification.cpp.o\nLinking CXX executable classifier_featureblocklogisticregression\nLinking CXX executable classifier_gaussian_process_binary_classification\n[ 87%] Built target basic_minimal\nScanning dependencies of target classifier_gaussiannaivebayes\n[ 87%] Building CXX object examples/undocumented/libshogun/CMakeFiles/classifier_gaussiannaivebayes.dir/classifier_gaussiannaivebayes.cpp.o\n[ 87%] Built target classifier_featureblocklogisticregression\nScanning dependencies of target classifier_knn\n[ 87%] [ 87%] Built target classifier_gaussian_process_binary_classification\nBuilding CXX object examples/undocumented/libshogun/CMakeFiles/classifier_knn.dir/classifier_knn.cpp.o\nScanning dependencies of target classifier_lda\nLinking CXX executable classifier_gaussiannaivebayes\n[ 87%] Building CXX object examples/undocumented/libshogun/CMakeFiles/classifier_lda.dir/classifier_lda.cpp.o\nLinking CXX executable classifier_knn\nLinking CXX executable classifier_lda\n[ 87%] Built target classifier_gaussiannaivebayes\nScanning dependencies of target classifier_libsvm\n[ 87%] Building CXX object examples/undocumented/libshogun/CMakeFiles/classifier_libsvm.dir/classifier_libsvm.cpp.o\n[ 87%] Built target classifier_knn\nScanning dependencies of target classifier_libsvm_probabilities\n[ 88%] Building CXX object examples/undocumented/libshogun/CMakeFiles/classifier_libsvm_probabilities.dir/classifier_libsvm_probabilities.cpp.o\n[ 88%] Built target classifier_lda\nScanning dependencies of target classifier_minimal_svm\nLinking CXX executable classifier_libsvm\n[ 88%] Building CXX object examples/undocumented/libshogun/CMakeFiles/classifier_minimal_svm.dir/classifier_minimal_svm.cpp.o\nLinking CXX executable classifier_libsvm_probabilities\nLinking CXX executable classifier_minimal_svm\n[ 88%] Built target classifier_libsvm\nScanning dependencies of target classifier_mkl_svmlight_modelselection_bug\n[ 88%] Building CXX object examples/undocumented/libshogun/CMakeFiles/classifier_mkl_svmlight_modelselection_bug.dir/classifier_mkl_svmlight_modelselection_bug.cpp.o\n[ 88%] Built target classifier_libsvm_probabilities\nScanning dependencies of target classifier_mklmulticlass\n[ 88%] Building CXX object examples/undocumented/libshogun/CMakeFiles/classifier_mklmulticlass.dir/classifier_mklmulticlass.cpp.o\n[ 88%] Built target classifier_minimal_svm\nScanning dependencies of target classifier_multiclass_ecoc\n[ 88%] Building CXX object examples/undocumented/libshogun/CMakeFiles/classifier_multiclass_ecoc.dir/classifier_multiclass_ecoc.cpp.o\nLinking CXX executable classifier_mkl_svmlight_modelselection_bug\nLinking CXX executable classifier_mklmulticlass\nLinking CXX executable classifier_multiclass_ecoc\n[ 88%] Built target classifier_mkl_svmlight_modelselection_bug\nScanning dependencies of target classifier_multiclass_ecoc_discriminant\n[ 88%] Building CXX object examples/undocumented/libshogun/CMakeFiles/classifier_multiclass_ecoc_discriminant.dir/classifier_multiclass_ecoc_discriminant.cpp.o\n[ 88%] Built target classifier_mklmulticlass\nScanning dependencies of target classifier_multiclass_ecoc_random\n[ 88%] Built target classifier_multiclass_ecoc\nScanning dependencies of target classifier_multiclass_prob_heuristics\n[ 88%] Building CXX object examples/undocumented/libshogun/CMakeFiles/classifier_multiclass_ecoc_random.dir/classifier_multiclass_ecoc_random.cpp.o\n[ 88%] Building CXX object examples/undocumented/libshogun/CMakeFiles/classifier_multiclass_prob_heuristics.dir/classifier_multiclass_prob_heuristics.cpp.o\nLinking CXX executable classifier_multiclass_ecoc_discriminant\nLinking CXX executable classifier_multiclass_ecoc_random\nLinking CXX executable classifier_multiclass_prob_heuristics\n[ 88%] Built target classifier_multiclass_ecoc_discriminant\nScanning dependencies of target classifier_multiclasslibsvm\n[ 89%] Building CXX object examples/undocumented/libshogun/CMakeFiles/classifier_multiclasslibsvm.dir/classifier_multiclasslibsvm.cpp.o\n[ 89%] Built target classifier_multiclass_prob_heuristics\nScanning dependencies of target classifier_multiclasslinearmachine\n[ 89%] Built target classifier_multiclass_ecoc_random\nScanning dependencies of target classifier_qda\n[ 89%] Building CXX object examples/undocumented/libshogun/CMakeFiles/classifier_multiclasslinearmachine.dir/classifier_multiclasslinearmachine.cpp.o\n[ 89%] Building CXX object examples/undocumented/libshogun/CMakeFiles/classifier_qda.dir/classifier_qda.cpp.o\nLinking CXX executable classifier_multiclasslibsvm\nLinking CXX executable classifier_multiclasslinearmachine\nLinking CXX executable classifier_qda\n[ 89%] Built target classifier_multiclasslibsvm\nScanning dependencies of target classifier_svmlight_string_features_precomputed_kernel\n[ 89%] [ 89%] Built target classifier_multiclasslinearmachine\n[ 89%] Built target classifier_qda\nBuilding CXX object examples/undocumented/libshogun/CMakeFiles/classifier_svmlight_string_features_precomputed_kernel.dir/classifier_svmlight_string_features_precomputed_kernel.cpp.o\nScanning dependencies of target clustering_kmeans\nScanning dependencies of target converter_diffusionmaps\n[ 89%] Building CXX object examples/undocumented/libshogun/CMakeFiles/clustering_kmeans.dir/clustering_kmeans.cpp.o\n[ 89%] Building CXX object examples/undocumented/libshogun/CMakeFiles/converter_diffusionmaps.dir/converter_diffusionmaps.cpp.o\nLinking CXX executable classifier_svmlight_string_features_precomputed_kernel\nLinking CXX executable clustering_kmeans\nLinking CXX executable converter_diffusionmaps\n[ 89%] Built target classifier_svmlight_string_features_precomputed_kernel\nScanning dependencies of target converter_factoranalysis\n[ 89%] [ 89%] Built target clustering_kmeans\nBuilding CXX object examples/undocumented/libshogun/CMakeFiles/converter_factoranalysis.dir/converter_factoranalysis.cpp.o\nScanning dependencies of target converter_hessianlocallylinearembedding\n[ 89%] Built target converter_diffusionmaps\nScanning dependencies of target converter_isomap\n[ 89%] Building CXX object examples/undocumented/libshogun/CMakeFiles/converter_hessianlocallylinearembedding.dir/converter_hessianlocallylinearembedding.cpp.o\n[ 90%] Building CXX object examples/undocumented/libshogun/CMakeFiles/converter_isomap.dir/converter_isomap.cpp.o\nLinking CXX executable converter_factoranalysis\nLinking CXX executable converter_hessianlocallylinearembedding\nLinking CXX executable converter_isomap\n[ 90%] Built target converter_factoranalysis\nScanning dependencies of target converter_jade_bss\n[ 90%] Building CXX object examples/undocumented/libshogun/CMakeFiles/converter_jade_bss.dir/converter_jade_bss.cpp.o\n[ 90%] Built target converter_hessianlocallylinearembedding\nScanning dependencies of target converter_kernellocallylinearembedding\n[ 90%] Built target converter_isomap\nScanning dependencies of target converter_laplacianeigenmaps\n[ 90%] Building CXX object examples/undocumented/libshogun/CMakeFiles/converter_kernellocallylinearembedding.dir/converter_kernellocallylinearembedding.cpp.o\n[ 90%] Building CXX object examples/undocumented/libshogun/CMakeFiles/converter_laplacianeigenmaps.dir/converter_laplacianeigenmaps.cpp.o\nLinking CXX executable converter_kernellocallylinearembedding\nLinking CXX executable converter_laplacianeigenmaps\nLinking CXX executable converter_jade_bss\n[ 90%] Built target converter_laplacianeigenmaps\n[ 90%] Scanning dependencies of target converter_linearlocaltangentspacealignment\nBuilt target converter_kernellocallylinearembedding\nScanning dependencies of target converter_localitypreservingprojections\n[ 90%] Building CXX object examples/undocumented/libshogun/CMakeFiles/converter_linearlocaltangentspacealignment.dir/converter_linearlocaltangentspacealignment.cpp.o\n[ 90%] [ 90%] Built target converter_jade_bss\nBuilding CXX object examples/undocumented/libshogun/CMakeFiles/converter_localitypreservingprojections.dir/converter_localitypreservingprojections.cpp.o\nScanning dependencies of target converter_locallylinearembedding\n[ 90%] Building CXX object examples/undocumented/libshogun/CMakeFiles/converter_locallylinearembedding.dir/converter_locallylinearembedding.cpp.o\nLinking CXX executable converter_linearlocaltangentspacealignment\nLinking CXX executable converter_localitypreservingprojections\nLinking CXX executable converter_locallylinearembedding\n[ 90%] Built target converter_linearlocaltangentspacealignment\nScanning dependencies of target converter_localtangentspacealignment\n[ 90%] Built target converter_localitypreservingprojections\nScanning dependencies of target converter_multidimensionalscaling\n[ 90%] Building CXX object examples/undocumented/libshogun/CMakeFiles/converter_localtangentspacealignment.dir/converter_localtangentspacealignment.cpp.o\n[ 90%] Built target converter_locallylinearembedding\n[ 91%] Scanning dependencies of target converter_neighborhoodpreservingembedding\nBuilding CXX object examples/undocumented/libshogun/CMakeFiles/converter_multidimensionalscaling.dir/converter_multidimensionalscaling.cpp.o\n[ 91%] Building CXX object examples/undocumented/libshogun/CMakeFiles/converter_neighborhoodpreservingembedding.dir/converter_neighborhoodpreservingembedding.cpp.o\nLinking CXX executable converter_localtangentspacealignment\nLinking CXX executable converter_multidimensionalscaling\nLinking CXX executable converter_neighborhoodpreservingembedding\n[ 91%] Built target converter_localtangentspacealignment\nScanning dependencies of target converter_stochasticproximityembedding\n[ 91%] Built target converter_multidimensionalscaling\nScanning dependencies of target evaluation_cross_validation_classification\n[ 91%] Building CXX object examples/undocumented/libshogun/CMakeFiles/converter_stochasticproximityembedding.dir/converter_stochasticproximityembedding.cpp.o\n[ 91%] Built target converter_neighborhoodpreservingembedding\n[ 91%] Building CXX object examples/undocumented/libshogun/CMakeFiles/evaluation_cross_validation_classification.dir/evaluation_cross_validation_classification.cpp.o\nScanning dependencies of target evaluation_cross_validation_locked_comparison\n[ 91%] Building CXX object examples/undocumented/libshogun/CMakeFiles/evaluation_cross_validation_locked_comparison.dir/evaluation_cross_validation_locked_comparison.cpp.o\nLinking CXX executable converter_stochasticproximityembedding\nLinking CXX executable evaluation_cross_validation_classification\nLinking CXX executable evaluation_cross_validation_locked_comparison\n[ 91%] Built target converter_stochasticproximityembedding\nScanning dependencies of target evaluation_cross_validation_mkl_weight_storage\n[ 92%] [ 92%] Building CXX object examples/undocumented/libshogun/CMakeFiles/evaluation_cross_validation_mkl_weight_storage.dir/evaluation_cross_validation_mkl_weight_storage.cpp.o\nBuilt target evaluation_cross_validation_classification\nScanning dependencies of target evaluation_cross_validation_multiclass\n[ 92%] Built target evaluation_cross_validation_locked_comparison\n[ 92%] Scanning dependencies of target evaluation_cross_validation_multiclass_mkl\nBuilding CXX object examples/undocumented/libshogun/CMakeFiles/evaluation_cross_validation_multiclass.dir/evaluation_cross_validation_multiclass.cpp.o\n[ 92%] Building CXX object examples/undocumented/libshogun/CMakeFiles/evaluation_cross_validation_multiclass_mkl.dir/evaluation_cross_validation_multiclass_mkl.cpp.o\nLinking CXX executable evaluation_cross_validation_mkl_weight_storage\nLinking CXX executable evaluation_cross_validation_multiclass\nLinking CXX executable evaluation_cross_validation_multiclass_mkl\n[ 92%] Built target evaluation_cross_validation_mkl_weight_storage\nScanning dependencies of target evaluation_cross_validation_regression\n[ 92%] Built target evaluation_cross_validation_multiclass\n[ 92%] Scanning dependencies of target features_copy_subset_simple_features\nBuilding CXX object examples/undocumented/libshogun/CMakeFiles/evaluation_cross_validation_regression.dir/evaluation_cross_validation_regression.cpp.o\n[ 92%] Building CXX object examples/undocumented/libshogun/CMakeFiles/features_copy_subset_simple_features.dir/features_copy_subset_simple_features.cpp.o\n[ 92%] Built target evaluation_cross_validation_multiclass_mkl\nScanning dependencies of target features_copy_subset_sparse_features\n[ 92%] Building CXX object examples/undocumented/libshogun/CMakeFiles/features_copy_subset_sparse_features.dir/features_copy_subset_sparse_features.cpp.o\nLinking CXX executable evaluation_cross_validation_regression\nLinking CXX executable features_copy_subset_simple_features\nLinking CXX executable features_copy_subset_sparse_features\n[ 92%] Built target evaluation_cross_validation_regression\nScanning dependencies of target features_dense_real_modular\n[ 92%] Built target features_copy_subset_simple_features\nScanning dependencies of target features_subset_labels\n[ 92%] Building CXX object examples/undocumented/libshogun/CMakeFiles/features_dense_real_modular.dir/features_dense_real_modular.cpp.o\n[ 92%] Building CXX object examples/undocumented/libshogun/CMakeFiles/features_subset_labels.dir/features_subset_labels.cpp.o\n[ 92%] Built target features_copy_subset_sparse_features\nScanning dependencies of target features_subset_simple_features\n[ 93%] Building CXX object examples/undocumented/libshogun/CMakeFiles/features_subset_simple_features.dir/features_subset_simple_features.cpp.o\nLinking CXX executable features_subset_labels\nLinking CXX executable features_dense_real_modular\nLinking CXX executable features_subset_simple_features\n[ 93%] [ 93%] Built target features_subset_labels\nBuilt target features_dense_real_modular\nScanning dependencies of target io_libsvm_multilabel\nScanning dependencies of target features_subset_stack\n[ 93%] Building CXX object examples/undocumented/libshogun/CMakeFiles/io_libsvm_multilabel.dir/io_libsvm_multilabel.cpp.o\n[ 93%] [ 93%] Built target features_subset_simple_features\nBuilding CXX object examples/undocumented/libshogun/CMakeFiles/features_subset_stack.dir/features_subset_stack.cpp.o\nScanning dependencies of target io_linereader\n[ 93%] Building CXX object examples/undocumented/libshogun/CMakeFiles/io_linereader.dir/io_linereader.cpp.o\nLinking CXX executable io_libsvm_multilabel\nLinking CXX executable features_subset_stack\nLinking CXX executable io_linereader\n[ 93%] Built target io_libsvm_multilabel\nScanning dependencies of target kernel_custom\n[ 93%] Built target features_subset_stack\nScanning dependencies of target kernel_custom_index\n[ 93%] Building CXX object examples/undocumented/libshogun/CMakeFiles/kernel_custom.dir/kernel_custom.cpp.o\n[ 93%] Built target io_linereader\nScanning dependencies of target kernel_gaussian\n[ 93%] Building CXX object examples/undocumented/libshogun/CMakeFiles/kernel_custom_index.dir/kernel_custom_index.cpp.o\n[ 93%] Building CXX object examples/undocumented/libshogun/CMakeFiles/kernel_gaussian.dir/kernel_gaussian.cpp.o\nLinking CXX executable kernel_custom\nLinking CXX executable kernel_custom_index\nLinking CXX executable kernel_gaussian\n[ 93%] Built target kernel_custom\nScanning dependencies of target kernel_machine_train_locked\n[ 93%] Built target kernel_custom_index\n[ 93%] Built target kernel_gaussian\nScanning dependencies of target kernel_revlin\nScanning dependencies of target labels_binary_fit_sigmoid\n[ 93%] Building CXX object examples/undocumented/libshogun/CMakeFiles/kernel_machine_train_locked.dir/kernel_machine_train_locked.cpp.o\n[ 94%] [ 94%] Building CXX object examples/undocumented/libshogun/CMakeFiles/kernel_revlin.dir/kernel_revlin.cpp.o\nBuilding CXX object examples/undocumented/libshogun/CMakeFiles/labels_binary_fit_sigmoid.dir/labels_binary_fit_sigmoid.cpp.o\nLinking CXX executable kernel_machine_train_locked\nLinking CXX executable kernel_revlin\nLinking CXX executable labels_binary_fit_sigmoid\n[ 94%] Built target kernel_machine_train_locked\nScanning dependencies of target library_circularbuffer\n[ 94%] [ 94%] Built target kernel_revlin\nBuilt target labels_binary_fit_sigmoid\nScanning dependencies of target library_gc_array\nScanning dependencies of target library_dyn_int\n[ 94%] Building CXX object examples/undocumented/libshogun/CMakeFiles/library_circularbuffer.dir/library_circularbuffer.cpp.o\n[ 94%] [ 94%] Building CXX object examples/undocumented/libshogun/CMakeFiles/library_gc_array.dir/library_gc_array.cpp.o\nBuilding CXX object examples/undocumented/libshogun/CMakeFiles/library_dyn_int.dir/library_dyn_int.cpp.o\nLinking CXX executable library_circularbuffer\nLinking CXX executable library_dyn_int\nLinking CXX executable library_gc_array\n[ 94%] Built target library_circularbuffer\nScanning dependencies of target library_hash\n[ 94%] [ 94%] Built target library_dyn_int\nBuilding CXX object examples/undocumented/libshogun/CMakeFiles/library_hash.dir/library_hash.cpp.o\nScanning dependencies of target library_hdf5\n[ 94%] Built target library_gc_array\nScanning dependencies of target library_indirect_object\n[ 94%] Building CXX object examples/undocumented/libshogun/CMakeFiles/library_hdf5.dir/library_hdf5.cpp.o\n[ 95%] Building CXX object examples/undocumented/libshogun/CMakeFiles/library_indirect_object.dir/library_indirect_object.cpp.o\nLinking CXX executable library_hash\nLinking CXX executable library_indirect_object\nLinking CXX executable library_hdf5\n[ 95%] Built target library_hash\nScanning dependencies of target library_map\n[ 95%] Building CXX object examples/undocumented/libshogun/CMakeFiles/library_map.dir/library_map.cpp.o\n[ 95%] Built target library_hdf5\nScanning dependencies of target library_mldatahdf5\n[ 95%] Built target library_indirect_object\nScanning dependencies of target library_serialization\n[ 95%] Building CXX object examples/undocumented/libshogun/CMakeFiles/library_mldatahdf5.dir/library_mldatahdf5.cpp.o\n[ 95%] Building CXX object examples/undocumented/libshogun/CMakeFiles/library_serialization.dir/library_serialization.cpp.o\nLinking CXX executable library_map\nLinking CXX executable library_mldatahdf5\nLinking CXX executable library_serialization\n[ 95%] [ 95%] Built target library_map\nBuilt target library_mldatahdf5\nScanning dependencies of target library_set\nScanning dependencies of target mathematics_confidence_intervals\n[ 95%] [ 95%] Building CXX object examples/undocumented/libshogun/CMakeFiles/library_set.dir/library_set.cpp.o\nBuilding CXX object examples/undocumented/libshogun/CMakeFiles/mathematics_confidence_intervals.dir/mathematics_confidence_intervals.cpp.o\n[ 95%] Built target library_serialization\nScanning dependencies of target mathematics_lapack\n[ 95%] Building CXX object examples/undocumented/libshogun/CMakeFiles/mathematics_lapack.dir/mathematics_lapack.cpp.o\nLinking CXX executable library_set\nLinking CXX executable mathematics_confidence_intervals\nLinking CXX executable mathematics_lapack\n[ 95%] Built target library_set\nScanning dependencies of target modelselection_apply_parameter_tree\n[ 95%] Built target mathematics_confidence_intervals\nScanning dependencies of target modelselection_combined_kernel_sub_parameters\n[ 95%] Building CXX object examples/undocumented/libshogun/CMakeFiles/modelselection_apply_parameter_tree.dir/modelselection_apply_parameter_tree.cpp.o\n[ 95%] Building CXX object examples/undocumented/libshogun/CMakeFiles/modelselection_combined_kernel_sub_parameters.dir/modelselection_combined_kernel_sub_parameters.cpp.o\n[ 95%] Built target mathematics_lapack\nScanning dependencies of target modelselection_grid_search_kernel\n[ 95%] Building CXX object examples/undocumented/libshogun/CMakeFiles/modelselection_grid_search_kernel.dir/modelselection_grid_search_kernel.cpp.o\nLinking CXX executable modelselection_apply_parameter_tree\nLinking CXX executable modelselection_combined_kernel_sub_parameters\nLinking CXX executable modelselection_grid_search_kernel\n[ 95%] Built target modelselection_apply_parameter_tree\nScanning dependencies of target modelselection_grid_search_linear\n[ 96%] Building CXX object examples/undocumented/libshogun/CMakeFiles/modelselection_grid_search_linear.dir/modelselection_grid_search_linear.cpp.o\n[ 96%] Built target modelselection_combined_kernel_sub_parameters\nScanning dependencies of target modelselection_grid_search_mkl\n[ 96%] [ 96%] Built target modelselection_grid_search_kernel\nBuilding CXX object examples/undocumented/libshogun/CMakeFiles/modelselection_grid_search_mkl.dir/modelselection_grid_search_mkl.cpp.o\nScanning dependencies of target modelselection_grid_search_multiclass_svm\n[ 96%] Building CXX object examples/undocumented/libshogun/CMakeFiles/modelselection_grid_search_multiclass_svm.dir/modelselection_grid_search_multiclass_svm.cpp.o\nLinking CXX executable modelselection_grid_search_linear\nLinking CXX executable modelselection_grid_search_mkl\nLinking CXX executable modelselection_grid_search_multiclass_svm\n[ 96%] Built target modelselection_grid_search_linear\nScanning dependencies of target modelselection_grid_search_string_kernel\n[ 96%] Building CXX object examples/undocumented/libshogun/CMakeFiles/modelselection_grid_search_string_kernel.dir/modelselection_grid_search_string_kernel.cpp.o\n[ 96%] Built target modelselection_grid_search_mkl\nScanning dependencies of target modelselection_model_selection_parameters_test\n[ 96%] [ 96%] Building CXX object examples/undocumented/libshogun/CMakeFiles/modelselection_model_selection_parameters_test.dir/modelselection_model_selection_parameters_test.cpp.o\nBuilt target modelselection_grid_search_multiclass_svm\nScanning dependencies of target modelselection_parameter_combination_test\n[ 96%] Building CXX object examples/undocumented/libshogun/CMakeFiles/modelselection_parameter_combination_test.dir/modelselection_parameter_combination_test.cpp.o\nLinking CXX executable modelselection_grid_search_string_kernel\nLinking CXX executable modelselection_model_selection_parameters_test\nLinking CXX executable modelselection_parameter_combination_test\n[ 96%] Built target modelselection_grid_search_string_kernel\nScanning dependencies of target modelselection_parameter_tree\n[ 96%] Building CXX object examples/undocumented/libshogun/CMakeFiles/modelselection_parameter_tree.dir/modelselection_parameter_tree.cpp.o\n[ 96%] Built target modelselection_model_selection_parameters_test\nScanning dependencies of target neuralnets_basic\n[ 96%] Building CXX object examples/undocumented/libshogun/CMakeFiles/neuralnets_basic.dir/neuralnets_basic.cpp.o\n[ 96%] Built target modelselection_parameter_combination_test\nScanning dependencies of target neuralnets_convolutional\n[ 97%] Building CXX object examples/undocumented/libshogun/CMakeFiles/neuralnets_convolutional.dir/neuralnets_convolutional.cpp.o\nLinking CXX executable modelselection_parameter_tree\nLinking CXX executable neuralnets_basic\nLinking CXX executable neuralnets_convolutional\n[ 97%] Built target modelselection_parameter_tree\nScanning dependencies of target neuralnets_deep_autoencoder\n[ 97%] Building CXX object examples/undocumented/libshogun/CMakeFiles/neuralnets_deep_autoencoder.dir/neuralnets_deep_autoencoder.cpp.o\n[ 97%] Built target neuralnets_basic\nScanning dependencies of target neuralnets_deep_belief_network\n[ 97%] Building CXX object examples/undocumented/libshogun/CMakeFiles/neuralnets_deep_belief_network.dir/neuralnets_deep_belief_network.cpp.o\n[ 97%] Built target neuralnets_convolutional\nScanning dependencies of target parameter_iterate_float64\n[ 97%] Building CXX object examples/undocumented/libshogun/CMakeFiles/parameter_iterate_float64.dir/parameter_iterate_float64.cpp.o\nLinking CXX executable neuralnets_deep_autoencoder\nLinking CXX executable neuralnets_deep_belief_network\nLinking CXX executable parameter_iterate_float64\n[ 97%] Built target neuralnets_deep_autoencoder\nScanning dependencies of target parameter_iterate_sgobject\n[ 97%] [ 97%] Built target neuralnets_deep_belief_network\nBuilding CXX object examples/undocumented/libshogun/CMakeFiles/parameter_iterate_sgobject.dir/parameter_iterate_sgobject.cpp.o\nScanning dependencies of target parameter_modsel_parameters\n[ 97%] Built target parameter_iterate_float64\nScanning dependencies of target parameter_set_from_parameters\n[ 97%] Building CXX object examples/undocumented/libshogun/CMakeFiles/parameter_modsel_parameters.dir/parameter_modsel_parameters.cpp.o\n[ 97%] Building CXX object examples/undocumented/libshogun/CMakeFiles/parameter_set_from_parameters.dir/parameter_set_from_parameters.cpp.o\nLinking CXX executable parameter_iterate_sgobject\nLinking CXX executable parameter_modsel_parameters\nLinking CXX executable parameter_set_from_parameters\n[ 97%] Built target parameter_iterate_sgobject\nScanning dependencies of target regression_gaussian_process_ard\n[ 97%] Building CXX object examples/undocumented/libshogun/CMakeFiles/regression_gaussian_process_ard.dir/regression_gaussian_process_ard.cpp.o\n[ 97%] Built target parameter_modsel_parameters\nScanning dependencies of target regression_gaussian_process_fitc\n[ 97%] Built target parameter_set_from_parameters\nScanning dependencies of target regression_gaussian_process_gaussian\nLinking CXX executable regression_gaussian_process_ard\n[ 97%] Building CXX object examples/undocumented/libshogun/CMakeFiles/regression_gaussian_process_fitc.dir/regression_gaussian_process_fitc.cpp.o\n[ 97%] Linking CXX executable regression_gaussian_process_fitc\nBuilding CXX object examples/undocumented/libshogun/CMakeFiles/regression_gaussian_process_gaussian.dir/regression_gaussian_process_gaussian.cpp.o\nLinking CXX executable regression_gaussian_process_gaussian\n[ 97%] Built target regression_gaussian_process_ard\nScanning dependencies of target regression_gaussian_process_laplace\n[ 98%] [ 98%] Building CXX object examples/undocumented/libshogun/CMakeFiles/regression_gaussian_process_laplace.dir/regression_gaussian_process_laplace.cpp.o\nBuilt target regression_gaussian_process_fitc\nScanning dependencies of target regression_gaussian_process_product\nLinking CXX executable regression_gaussian_process_laplace\n[ 98%] Built target regression_gaussian_process_gaussian\nScanning dependencies of target regression_gaussian_process_simple_exact\n[ 98%] Building CXX object examples/undocumented/libshogun/CMakeFiles/regression_gaussian_process_product.dir/regression_gaussian_process_product.cpp.o\n[ 98%] Building CXX object examples/undocumented/libshogun/CMakeFiles/regression_gaussian_process_simple_exact.dir/regression_gaussian_process_simple_exact.cpp.o\n[ 98%] Built target regression_gaussian_process_laplace\nScanning dependencies of target regression_gaussian_process_sum\n[ 98%] Building CXX object examples/undocumented/libshogun/CMakeFiles/regression_gaussian_process_sum.dir/regression_gaussian_process_sum.cpp.o\nLinking CXX executable regression_gaussian_process_product\nLinking CXX executable regression_gaussian_process_simple_exact\nLinking CXX executable regression_gaussian_process_sum\n[ 98%] Built target regression_gaussian_process_product\n[ 98%] Built target regression_gaussian_process_simple_exact\nScanning dependencies of target regression_libsvr\nScanning dependencies of target serialization_basic_tests\n[ 98%] Building CXX object examples/undocumented/libshogun/CMakeFiles/serialization_basic_tests.dir/serialization_basic_tests.cpp.o\n[ 98%] [ 98%] Built target regression_gaussian_process_sum\nBuilding CXX object examples/undocumented/libshogun/CMakeFiles/regression_libsvr.dir/regression_libsvr.cpp.o\nScanning dependencies of target serialization_multiclass_labels\n[ 98%] Building CXX object examples/undocumented/libshogun/CMakeFiles/serialization_multiclass_labels.dir/serialization_multiclass_labels.cpp.o\nLinking CXX executable serialization_basic_tests\nLinking CXX executable regression_libsvr\nLinking CXX executable serialization_multiclass_labels\nCMakeFiles/serialization_basic_tests.dir/serialization_basic_tests.cpp.o: In function test_test_class_serial()':\nserialization_basic_tests.cpp:(.text+0x90): warning: the use ofmktemp' is dangerous, better use mkstemp' ormkdtemp'\n[ 98%] Built target serialization_basic_tests\nScanning dependencies of target so_factorgraph\n[ 98%] Built target regression_libsvr\nScanning dependencies of target so_fg_multilabel\n[ 98%] Building CXX object examples/undocumented/libshogun/CMakeFiles/so_factorgraph.dir/so_factorgraph.cpp.o\n[ 98%] Built target serialization_multiclass_labels\nScanning dependencies of target so_multiclass_BMRM\n[ 99%] Building CXX object examples/undocumented/libshogun/CMakeFiles/so_fg_multilabel.dir/so_fg_multilabel.cpp.o\n[ 99%] Building CXX object examples/undocumented/libshogun/CMakeFiles/so_multiclass_BMRM.dir/so_multiclass_BMRM.cpp.o\nLinking CXX executable so_factorgraph\nLinking CXX executable so_fg_multilabel\nLinking CXX executable so_multiclass_BMRM\n[ 99%] Built target so_factorgraph\nScanning dependencies of target splitting_standard_crossvalidation\n[ 99%] Building CXX object examples/undocumented/libshogun/CMakeFiles/splitting_standard_crossvalidation.dir/splitting_standard_crossvalidation.cpp.o\n[ 99%] [ 99%] Built target so_fg_multilabel\nBuilt target so_multiclass_BMRM\nScanning dependencies of target splitting_stratified_crossvalidation\nScanning dependencies of target statistics\nLinking CXX executable splitting_standard_crossvalidation\n[ 99%] [ 99%] Building CXX object examples/undocumented/libshogun/CMakeFiles/statistics.dir/statistics.cpp.o\nBuilding CXX object examples/undocumented/libshogun/CMakeFiles/splitting_stratified_crossvalidation.dir/splitting_stratified_crossvalidation.cpp.o\nLinking CXX executable statistics\nLinking CXX executable splitting_stratified_crossvalidation\n[ 99%] Built target splitting_standard_crossvalidation\nScanning dependencies of target statistics_hsic\n[ 99%] Building CXX object examples/undocumented/libshogun/CMakeFiles/statistics_hsic.dir/statistics_hsic.cpp.o\n[ 99%] Built target splitting_stratified_crossvalidation\nScanning dependencies of target statistics_linear_time_mmd\n[ 99%] Built target statistics\nScanning dependencies of target statistics_mmd_kernel_selection\n[ 99%] Building CXX object examples/undocumented/libshogun/CMakeFiles/statistics_linear_time_mmd.dir/statistics_linear_time_mmd.cpp.o\n[ 99%] Building CXX object examples/undocumented/libshogun/CMakeFiles/statistics_mmd_kernel_selection.dir/statistics_mmd_kernel_selection.cpp.o\nLinking CXX executable statistics_hsic\nLinking CXX executable statistics_linear_time_mmd\nLinking CXX executable statistics_mmd_kernel_selection\n[ 99%] Built target statistics_hsic\nScanning dependencies of target statistics_quadratic_time_mmd\n[100%] Building CXX object examples/undocumented/libshogun/CMakeFiles/statistics_quadratic_time_mmd.dir/statistics_quadratic_time_mmd.cpp.o\n[100%] Built target statistics_mmd_kernel_selection\nScanning dependencies of target streaming_from_dense\nLinking CXX executable statistics_quadratic_time_mmd\n[100%] Building CXX object examples/undocumented/libshogun/CMakeFiles/streaming_from_dense.dir/streaming_from_dense.cpp.o\n[100%] Built target statistics_linear_time_mmd\nScanning dependencies of target streaming_onlineliblinear_dense\n[100%] Building CXX object examples/undocumented/libshogun/CMakeFiles/streaming_onlineliblinear_dense.dir/streaming_onlineliblinear_dense.cpp.o\nLinking CXX executable streaming_from_dense\nLinking CXX executable streaming_onlineliblinear_dense\n[100%] Built target statistics_quadratic_time_mmd\nScanning dependencies of target streaming_onlineliblinear_sparse\n[100%] Building CXX object examples/undocumented/libshogun/CMakeFiles/streaming_onlineliblinear_sparse.dir/streaming_onlineliblinear_sparse.cpp.o\n[100%] Built target streaming_from_dense\nScanning dependencies of target transfer_multitaskleastsquaresregression\nLinking CXX executable streaming_onlineliblinear_sparse\n[100%] Building CXX object examples/undocumented/libshogun/CMakeFiles/transfer_multitaskleastsquaresregression.dir/transfer_multitaskleastsquaresregression.cpp.o\n[100%] Built target streaming_onlineliblinear_dense\nScanning dependencies of target transfer_multitasklogisticregression\n[100%] Building CXX object examples/undocumented/libshogun/CMakeFiles/transfer_multitasklogisticregression.dir/transfer_multitasklogisticregression.cpp.o\nLinking CXX executable transfer_multitaskleastsquaresregression\nLinking CXX executable transfer_multitasklogisticregression\nCMakeFiles/streaming_onlineliblinear_sparse.dir/streaming_onlineliblinear_sparse.cpp.o: In function main':\nstreaming_onlineliblinear_sparse.cpp:(.text.startup+0x8a): warning: the use ofmktemp' is dangerous, better use mkstemp' ormkdtemp'\n[100%] Built target streaming_onlineliblinear_sparse\nScanning dependencies of target variational_approx_example\n[100%] Building CXX object examples/undocumented/libshogun/CMakeFiles/variational_approx_example.dir/variational_approx_example.cpp.o\n[100%] Built target transfer_multitaskleastsquaresregression\n[100%] Built target transfer_multitasklogisticregression\nLinking CXX executable variational_approx_example\n[100%] Built target variational_approx_example\n[100%] Built target r_doxy2swig\n[100%] Swig source\n/home/bee/Downloads/shogun/src/shogun/lib/List.h:71: Warning 314: 'next' is a R keyword, renaming to '_next'\n/home/bee/Downloads/shogun/src/shogun/structure/GraphCut.h:51: Warning 314: 'next' is a R keyword, renaming to '_next'\n/home/bee/Downloads/shogun/src/shogun/structure/GraphCut.h:72: Warning 314: 'next' is a R keyword, renaming to '_next'\n/home/bee/Downloads/shogun/src/shogun/structure/GraphCut.h:93: Warning 314: 'next' is a R keyword, renaming to '_next'\nconstantWrapper   : int STRING_LEN = 256\nconstantWrapper   : char STRING_LEN_STR = 256\nconstantWrapper   : char CHAR_CONT_BEGIN = (\nconstantWrapper   : char CHAR_CONT_END = )\nconstantWrapper   : char CHAR_ITEM_BEGIN = {\nconstantWrapper   : char CHAR_ITEM_END = }\nconstantWrapper   : char CHAR_SGSERIAL_BEGIN = [\nconstantWrapper   : char CHAR_SGSERIAL_END = ]\nconstantWrapper   : char CHAR_STRING_BEGIN = [\nconstantWrapper   : char CHAR_STRING_END = ]\nconstantWrapper   : char CHAR_SPARSE_BEGIN = (\nconstantWrapper   : char CHAR_SPARSE_END = )\nconstantWrapper   : char CHAR_TYPE_END = \\n\nconstantWrapper   : char STR_SGSERIAL_NULL = null\nconstantWrapper   : char STR_IS_SGSERIALIZABLE = is_sgserializable\nconstantWrapper   : char STR_IS_SPARSE = is_sparse\nconstantWrapper   : char STR_IS_CONT = is_container\nconstantWrapper   : char STR_IS_NULL = is_null\nconstantWrapper   : char STR_INSTANCE_NAME = instance_name\nconstantWrapper   : char STR_GENERIC_NAME = generic_name\nconstantWrapper   : char STR_CTYPE_NAME = container_type\nconstantWrapper   : char STR_LENGTH_X = length_x\nconstantWrapper   : char STR_LENGTH_Y = length_y\nconstantWrapper   : char STR_GROUP_PREFIX = $\nconstantWrapper   : char STR_SPARSE_FPTR = features_ptr\nconstantWrapper   : char STR_SPARSEENTRY_FINDEX = feat_index\nconstantWrapper   : char STR_SPARSEENTRY_ENTRY = entry\nconstantWrapper   : char STR_TRUE = true\nconstantWrapper   : char STR_FALSE = false\nconstantWrapper   : char STR_ITEM = i\nconstantWrapper   : char STR_STRING = s\nconstantWrapper   : char STR_SPARSE = r\nconstantWrapper   : char STR_PROP_TYPE = type\nconstantWrapper   : char STR_PROP_IS_NULL = is_null\nconstantWrapper   : char STR_PROP_INSTANCE_NAME = instance_name\nconstantWrapper   : char STR_PROP_GENERIC_NAME = generic_name\nconstantWrapper   : char STR_PROP_FEATINDEX = feat_index\nconstantWrapper   : double M_PI = 3.14159265358979323846\nconstantWrapper   : int RNG_SEED_SIZE = 256\n/home/bee/Downloads/shogun/src/shogun/mathematics/Math.h:696: Warning 509: Overloaded method shogun::CMath::random(int64_t,int64_t) effectively ignored,\n/home/bee/Downloads/shogun/src/shogun/mathematics/Math.h:691: Warning 509: as it is shadowed by shogun::CMath::random(uint64_t,uint64_t).\n/home/bee/Downloads/shogun/src/shogun/mathematics/Math.h:691: Warning 509: Overloaded method shogun::CMath::random(uint64_t,uint64_t) effectively ignored,\n/home/bee/Downloads/shogun/src/shogun/mathematics/Math.h:696: Warning 509: as it is shadowed by shogun::CMath::random(int64_t,int64_t).\n/home/bee/Downloads/shogun/src/shogun/mathematics/Math.h:706: Warning 509: Overloaded method shogun::CMath::random(int32_t,int32_t) effectively ignored,\n/home/bee/Downloads/shogun/src/shogun/mathematics/Math.h:701: Warning 509: as it is shadowed by shogun::CMath::random(uint32_t,uint32_t).\n/home/bee/Downloads/shogun/src/shogun/mathematics/Math.h:701: Warning 509: Overloaded method shogun::CMath::random(uint32_t,uint32_t) effectively ignored,\n/home/bee/Downloads/shogun/src/shogun/mathematics/Math.h:706: Warning 509: as it is shadowed by shogun::CMath::random(int32_t,int32_t).\nconstantWrapper   : int DenseLabels_REJECTION_LABEL = shogun::CDenseLabels::REJECTION_LABEL\nconstantWrapper   : double DEF_PRECISION = 1E-14\nconstantWrapper   : int INFINITE_D = 1000000000\nScanning dependencies of target r_modular\n[100%] [100%] Building CXX object src/interfaces/r_modular/CMakeFiles/r_modular.dir/sg_print_functions.cpp.o\nBuilding CXX object src/interfaces/r_modular/CMakeFiles/r_modular.dir/modshogunR_wrap.cxx.o\nLinking CXX shared module modshogun.so\nGenerating modshogun.RData\nf=\"modshogun.R\"; fdata=\"modshogun.RData\"; source ( f ) ; save ( list=ls ( all=TRUE ) ,file=fdata, compress=TRUE ) ; q ( save=\"no\" ) \nError in file(filename, \"r\", encoding = encoding) : \n  cannot open the connection\nCalls: source -> file\nIn addition: Warning message:\nIn file(filename, \"r\", encoding = encoding) :\n  cannot open file 'modshogun.R': No such file or directory\nExecution halted\nmake[2]: ** [src/interfaces/r_modular/modshogun.so] Error 1\nmake[1]: * [src/interfaces/r_modular/CMakeFiles/r_modular.dir/all] Error 2\nmake: * [all] Error 2\nbee@localhost:~/Downloads/shogun/build$ \n. > Maybe we could add this to one of the Readmes. Feel free to send a little patch.\nLet us know how things go from now!\n\nGetting back a little late since I have not been using shogun's R interface \nI don't know how to fix the codes or make a patch but the problem I think is in\nshogun/src/interfaces/r_modular/CMakeLists  \nMore  precisely these lines\nADD_CUSTOM_COMMAND(TARGET r_modular\n    POST_BUILD\n    COMMAND echo 'f=\"modshogun.R\" \\; fdata=\"modshogun.RData\" \\; source( f ) \\; save( list=ls( all=TRUE ) , file=fdata , compress=TRUE ) \\; q( save=\"no\" ) \\;' | ${R_EXECUTABLE} --silent --no-save\n    WORKING_DIRECTORY ${CMAKE_CURRENT_BINARY_DIR}\n    COMMENT \"Generating modshogun.RData\"\n)\nWhat follow COMMAND echo are basically commands to be executed in R. When R  runs its work directory is given by the function getwd(), in my case this will return the value in ~/.RProfile, which is not my $HOME but a subdirectory of it. I can imagine others may want to set their default R work directory like this too instead of dumping R outputs all over their $HOME\nThe CMAKE option WORKING_DIRECTORY apparently cannot override the value in ~/.RProfie, so when the shogun build process executes R, it is looking for modshogun.R in WORK_DIRECTORY, which is it's default work directory (defined by .RProfie)  rather than ${CMAKE_CURRENT_BINARY_DIR}\nHere is my take on it, maybe someone who knows his C++ can just edit these lines so that people don't get tripped over on this little thing in the future (I tried but didn't work) \n.  Just saw the link to your change so I manually edited the file. It works. Thanks for fixing this.\n. Hello??\n. @lisitsyn \nThere is no  src/interfaces/octave_modular/CMakeFiles/octave_modular.dir/modshogunOCTAVE_wrap.cxx. It doesn't build. \n/home/bee/Downloads/shogun-4.0.0/build/src/interfaces/octave_modular/modshogunOCTAVE_wrap.cxx:1611:18: error: \u2018hid_t\u2019 has not been declared\n       save_hdf5 (hid_t loc_id, const char name, bool save_as_floats) {\n                  ^\n/home/bee/Downloads/shogun-4.0.0/build/src/interfaces/octave_modular/modshogunOCTAVE_wrap.cxx:1616:18: error: \u2018hid_t\u2019 has not been declared\n       load_hdf5 (hid_t loc_id, const char name, bool have_h5giterate_bug) {\n                  ^\n/home/bee/Downloads/shogun-4.0.0/build/src/interfaces/octave_modular/modshogunOCTAVE_wrap.cxx:1832:18: error: \u2018hid_t\u2019 has not been declared\n       save_hdf5 (hid_t loc_id, const char name, bool save_as_floats)\n                  ^\n/home/bee/Downloads/shogun-4.0.0/build/src/interfaces/octave_modular/modshogunOCTAVE_wrap.cxx:1836:18: error: \u2018hid_t\u2019 has not been declared\n       load_hdf5 (hid_t loc_id, const char name, bool have_h5giterate_bug)\n                  ^\n/home/bee/Downloads/shogun-4.0.0/build/src/interfaces/octave_modular/modshogunOCTAVE_wrap.cxx:1850:5: error: \u2018DECLARE_OCTAVE_ALLOCATOR\u2019 does not name a type\n     DECLARE_OCTAVE_ALLOCATOR;\n     ^\n/home/bee/Downloads/shogun-4.0.0/build/src/interfaces/octave_modular/modshogunOCTAVE_wrap.cxx:1853:43: error: expected constructor, destructor, or type conversion before \u2018;\u2019 token\n   DEFINE_OCTAVE_ALLOCATOR(octave_swig_ref);\n                                           ^\n/home/bee/Downloads/shogun-4.0.0/build/src/interfaces/octave_modular/modshogunOCTAVE_wrap.cxx:1910:18: error: \u2018hid_t\u2019 has not been declared\n       save_hdf5 (hid_t loc_id, const char name, bool save_as_floats) {\n                  ^\n/home/bee/Downloads/shogun-4.0.0/build/src/interfaces/octave_modular/modshogunOCTAVE_wrap.cxx:1915:18: error: \u2018hid_t\u2019 has not been declared\n       load_hdf5 (hid_t loc_id, const char name, bool have_h5giterate_bug) {\n                  ^\n/home/bee/Downloads/shogun-4.0.0/build/src/interfaces/octave_modular/modshogunOCTAVE_wrap.cxx:1921:5: error: \u2018DECLARE_OCTAVE_ALLOCATOR\u2019 does not name a type\n     DECLARE_OCTAVE_ALLOCATOR;\n     ^\n/home/bee/Downloads/shogun-4.0.0/build/src/interfaces/octave_modular/modshogunOCTAVE_wrap.cxx:1924:46: error: expected constructor, destructor, or type conversion before \u2018;\u2019 token\n   DEFINE_OCTAVE_ALLOCATOR(octave_swig_packed);\n                                              ^\nsrc/interfaces/octave_modular/CMakeFiles/octave_modular.dir/build.make:292: recipe for target 'src/interfaces/octave_modular/CMakeFiles/octave_modular.dir/modshogunOCTAVE_wrap.cxx.o' failed\nmake[2]: * [src/interfaces/octave_modular/CMakeFiles/octave_modular.dir/modshogunOCTAVE_wrap.cxx.o] Error 1\nCMakeFiles/Makefile2:110: recipe for target 'src/interfaces/octave_modular/CMakeFiles/octave_modular.dir/all' failed\nmake[1]: * [src/interfaces/octave_modular/CMakeFiles/octave_modular.dir/all] Error 2\nMakefile:137: recipe for target 'all' failed\nmake: * [all] Error 2\n. Hi,\nThanks for the reply. Maybe someone should update the Wiki with a link to the patch so that users will not waste their time unnecessarily? \n. ",
    "PirosB3": "Hi @tklein23 \nWhat type of format would you like the .data and .labels? I was thinking it would be a good idea to add something standard like svmlight. In this way, we reduce the files from 2 to 1 (svmlight contains both target and features) and provide a tool that is compatible with standard formats.\n .=.  : : ... : #\nWhat do you think?\nThanks,\nDan\n. @tklein23 I have started working on this new feature. When you have a second can you see my initial commit? it's no where near to a production version, but if there is something I am doing wrong please let me know.\nAlso, wouldn't it be better to have a single file that does training+evaluation? (like you said: evaluate_multiclass_labels.py, but without having the other two scripts) there is a lot of reusable functionality between the two.\nLet me know,\nDan\n. Perfect!\nI will send a PR\n2014-03-07 13:31 GMT+00:00 Viktor Gal notifications@github.com:\n\n@PirosB3 https://github.com/PirosB3 we would like you to send a PR\n(pull request) instead of asking people to check on your forked\nrepository... it is essential that you start working with PRs as that's how\nwe do development during the whole GSoC.\neven if your code is not ready it's ok to send a PR as we'll discuss\nthings in that PR and then you can change and add more commits to the PR\nobviously...\n\nReply to this email directly or view it on GitHubhttps://github.com/shogun-toolbox/shogun/issues/1868#issuecomment-37023878\n.\n\n\n\nPirosB3\nhttps://github.com/PirosB3 http://pirosb3.com\n. I am currently trying to fix this, once I have a solution I will write a description of how to solve it\n. Unfortunately I have currently solved by only compiling shogun (not the Python bindings). I will update when I have some progress on this.\n. Hi @hwl596 thank you very much for communicating this. Unfortunately my mac is 10.7.5, so 2 versions behind! My issue is related to gcc and clang being too behind, I think.\nThe strange issue is that I can compile Shogun without bindings fine! but when I start compiling the Python modular interface, all falls to pieces with that error :(\n. Yes, welcome to the club. I recommend updating Mac or use a VM.\nIf, by any chance, you don't necessarily need to use the Python bindings\nthen just make shogun_core\n2014-04-07 13:56 GMT+02:00 christopher-mohr notifications@github.com:\n\nI have a similiar issue on OSX 10.7.5 (gcc 4.7.3).\ncmake parameters\ncmake -DPythonModular=ON ..\nerror message\nLinking CXX shared library libshogun.dylib\n[ 85%] Built target shogun\nScanning dependencies of target python_modular_src\n[ 85%] copying SWIG files\n[ 85%] Built target python_modular_src\n[ 85%] Swig source\nsw/shogun-develop/src/shogun/lib/Hash.h:139: Warning 314: 'in' is a python keyword, renaming to 'in'\nsw/shogun-develop/src/shogun/lib/SGVector.h:58: Warning 503: Can't wrap 'operator long long' unless renamed to a valid identifier.\nsw/shogun-develop/src/shogun/lib/SGVector.h:58: Warning 503: Can't wrap 'operator unsigned long long_' unless renamed to a valid identifier.\nsw/shogun-develop/src/shogun/lib/SGVector.h:58: Warning 503: Can't wrap 'operator std::complex< double >' unless renamed to a valid identifier.\nScanning dependencies of target _python_modular\n[ 85%] Building CXX object src/interfaces/python_modular/CMakeFiles/_python_modular.dir/modshogunPYTHON_wrap.cxx.o\nsw/shogun-develop/build/src/interfaces/python_modular/modshogunPYTHON_wrap.cxx: In function 'PyObject wrap_SGIO_filter(PyObject, PyObject_)':\nsw/shogun-develop/build/src/interfaces/python_modular/modshogunPYTHON_wrap.cxx:24004:62: error: invalid conversion from 'const dirent_' to 'dirent_' [-fpermissive]\nIn file included from sw/shogun-develop/src/shogun/base/SGObject.h:24:0,\n                 from sw/shogun-develop/src/shogun/machine/Machine.h:18,\n                 from sw/shogun-develop/build/src/interfaces/python_modular/modshogunPYTHON_wrap.cxx:5157:\nsw/shogun-develop/src/shogun/io/SGIO.h:477:14: error:   initializing argument 1 of 'static int shogun::SGIO::filter(dirent_)' [-fpermissive]\nAt global scope:\ncc1plus: warning: unrecognized command line option \"-Wno-c++11-narrowing\" [enabled by default]\nmake[2]: *_* [src/interfaces/python_modular/CMakeFiles/_python_modular.dir/modshogunPYTHON_wrap.cxx.o] Error 1\nmake[1]: * [src/interfaces/python_modular/CMakeFiles/_python_modular.dir/all] Error 2\nmake: * [all] Error 2\n\nReply to this email directly or view it on GitHubhttps://github.com/shogun-toolbox/shogun/issues/1925#issuecomment-39721372\n.\n\n\n\nPirosB3\nhttps://github.com/PirosB3 http://pirosb3.com\n. @tklein23 thanks for the comments. I have added a simple version of the test script. Do you have any suggestions on what other framework I could test against?\nAlso, I have left some comments of parts I am unsure.\nDan\n. Great. I will have a look at how they do it.\nOn 7 Mar 2014 19:12, \"tklein23\" notifications@github.com wrote:\n\nIf you don't know how to evaluate multiclass labels, you could check the\nscikit-learn examples; their multiclass evaluation prints some metrics plus\na confusion matrix and looks really neat.\nJust in case we're printing more than accuracy, we should consider reusing\nscikit-learn evaluation. (Instead of re-writing it on our own.)\n\nReply to this email directly or view it on GitHubhttps://github.com/shogun-toolbox/shogun/pull/1954#issuecomment-37056966\n.\n. Hi @tklein23 I added some other changes here.\n- enter and exit functions\n- accuracy evaluation (for now we only have this)\n- confusion matrix\n- small fixes\n  Please let me know if I am missing something.\n\nUnfortunately there is a small issue when calling the get_confusion_matrix function from MulticlassAccuracy.cpp:\nWhen I call this function some times I get an invalid memory exception! (linked backtrace below). Could this be an issue related to SWIG's memory management? have you had these issues in the past? I would be happy to investigate more.\nhttps://gist.github.com/PirosB3/9450801\nDan\n. Hi @tklein23 more changes:\n- Changed test script to output predicted results only\n- Added linear kernel as an option on train script\n- Added evaluate script\n- Refactored and added common functionality utils.py script\n. Hi @tklein23 more changes:\n- Changed test script to output predicted results only\n- Added linear kernel as an option on train script\n- Added evaluate script\n- Refactored and added common functionality utils.py script\n. Hi @tklein23 \nOther changes: \n- Reindented all\n- Renamed evaluate script\n- Changed output from binary to SVMLight\n  Let me know what you think\n. Hi @tklein23 \nOther changes: \n- Reindented all\n- Renamed evaluate script\n- Changed output from binary to SVMLight\n  Let me know what you think\n. @vigsterkr gitignore has been modified.\n. @tklein23 I have just checked again. I was not having them when I wrote this, but it looks like I have triggered a memory leak again.\nDo you have any suggestion on how I can debug the Python binding?\n. - Added an issue: https://github.com/shogun-toolbox/shogun/issues/1999\n- Sent a mail on the ML\nThe only issue is that this branch exists on my repository. This means that, to test it out, users need to add my remote and checkout my branch. Wouldn't merging this PR make it easier for people to test out the bug? Obviously, if there is anything I need to add, please let me know.\nDan\n. 1) python train_multiclass_svm.py --dataset ../../toy/7class_example4_train.light --output /tmp/train_linear --kernel linear\n2) python predict_multiclass_svm.py --actual ../../toy/7class_example4_test.light --predicted /tmp/predicted_gaussian\n3) python evaluate_multiclass_svm.py --actual ../../toy/7class_example4_test.light --predicted \n. Has anyone started on this?\n. @tklein23 this is luckily not happening anymore. I will be posting a guide to train, predict and evaluate muliclass datasets. Once that is done, I will get other people to test and ensure the bug does not exist. Once we have asserted that, we can close this.\nBut it shouldn't be closed now.\n. Not at all, this is just an initial version, the refactored version will be able to choose from different kernels\n. It is an easy way to provide an enter() and exit() to functions. If you don't like it I can refactor. The reason it's in is because it makes the code more readable. \n. it is a handy way of checking how much time it takes to execute the functions in the inner statement, in this case: svm.train()\n. Is this the correct way of re-setting the parameters inside the kernel function?\n. both test_multiclass and train_multiclass can be imported and executed independently. This will be useful for other scripts (evaluate_multiclass.py)\n. Perfect. I will add Gaussian and Linear\n. oh! thats fine then. Thanks for the clarification\n. As the kernel was serialized together with the classifier, shouldn't kernel.get_lhs() return the train features? given that it was initialized in this way?\nfeats, labels = get_features_and_labels(LibSVMFile(dataset))\nGaussianKernel(feats, feats, width)\nI will be looking through examples and tests to understand better.\n. sounds great. on it now\n. unfortunately even after removing init/exit there is still a memory issue\n. This has been done. Please make sure it's okay\n. I added a utility file for reusability. train, test, evaluate scripts are much smaller now and easier to read\n. width is not used with linear kernels.\nlambda feats, width: LinearKernel(feats, feats)\nI just thought it would be good to have a common interface for all the kernels. In this way syntax is easier to read and it makes it simple to add/remove other kernels.\nIf you want I can change this\n. Oh sorry about that, it slipped in. Will be removed now!\nOn Thursday, March 13, 2014, Viktor Gal notifications@github.com wrote:\n\nIn .gitignore:\n\n@@ -128,6 +128,7 @@ configure.log\n /CMakeFiles/\n CMakeCache.txt\n /build\n+/build-osx\n\nwhy?\n\nReply to this email directly or view it on GitHubhttps://github.com/shogun-toolbox/shogun/pull/1954/files#r10560904\n.\n\n\n\nPirosB3\nhttps://github.com/PirosB3 http://pirosb3.com\n. ",
    "andbberger": "I'll do this. \n. I'll do this one! \n. ",
    "dhruv13J": "This patch also fails to call the new function from within the TParameter::compare_ptype() function... I will submit my patch soon\n. @pranet: check this - https://github.com/shogun-toolbox/shogun/pull/1896\n. @karlnapf: yeah, all tests passed locally, before i added the last commit; made a stupid typo\n. @vigsterkr: Is this okay?\n. @vigsterkr: I have no idea why the python_modular library_time.py should fail... is it unrelated to my patch?\n. @vigsterkr yeah\n. @iglesias, @vigsterkr I'll attempt the cmake hacking\n. @iglesias @vigsterkr Closing this and sending bundled nanoflann PR\n. @vigsterkr @iglesias Hey! finished with the nanoflann bundle. Please check for obvious mistakes.\n. Is this going to be merged? \n. was going to put the 'CMath::fequals(..)' function there, forgot to erase the whitespace...\n. accuracy was floatmax in TParameter::compare_ptype(..), so thought I should maintain the type\n. my bad here...\n. Do I change both to float64_t?\n. Not all unit tests added yet; will add the rest soon\n. Thought this would be helpful for other noobs like me ;-) !\n. The Max of relative error and diff is needed to handle cases with large eps values; eg: fequals(1.0, 1.11, 0.1) which needs the diff for comparison rather than relative error\n. The floatmax datatype was causing problems, so i changed it to float64 everywhere\n. the tolerant parameter allows less strict check for equality, based on the epsilon value given to the function. it has been added to all methods which can call TParameter::equals.\n. Nan's are unordered;\nhttp://en.wikipedia.org/wiki/NaN#Floating_point\n. okay :)\n. okay\n. @vigsterkr : Hey! I added that because it helps a lot while running individual tests when they fail... took me quite a while to figure out how to get the debug output while testing. I thought it would be helpful to those trying to correct failing unit tests :-)\n. @vigsterkr: But I see your point. I will try and avoid this in the future.\n. @vigsterkr: really sorry... this was my first PR, so i made a lot of weird changes... will definitely avoid this in the future.\n. @karlnapf, @vigsterkr: I added that for the commented out line below... thought it would be helpful while debugging.\n. Added const qualifier as nanoflann complains while using SGDatasetAdaptor\n. overlooked whitespace XO, will change soon\n. This is used frequently, and should be more efficient. Please advise if there is a better way to access the dim'th element of the idx'th feature vector.\n. I agree, I will avoid this\n. yes, I need to call these methods from within CKNN\n. This is a serious typo, will rectify soon\n. A point in the Feature Space, it is a part of the interface to nanoflann\n. okay\n. nanoflann expects ElementType (double over here) arrays for those two, so can't use SGVectors\n. okay, this i can change\n. okay\n. I was getting memory leaks with valgrind without it...\n. This is strange, there are extra tabs here than in my local files\n. Ah, it is... dunno what I was thinking ;-)\n. The freeing happens only if dofree was set to true while calling get_feature_vector, but yeah, I'll recheck these parts\n. classify_for_multiple_k initially made the assumption that init_distance() was called somewhere else, which was a bug. As you can see I've added that here, and was going to use a warning instead. For now, it is just a debug message, but I really think there should be a warning.\n. The lhs represents the Training set, and the rhs represents the query set, so m_k should be less than the number of training set vectors, rather than less than query set vectors.\n. Umm, the nanoflann::KDTreeSingleIndexAdaptor class expects a function called kdtree_get_pt to be implemented by the DatasetAdaptor.\n. This is the culprit for the failing python integration tests!! ;-)\n. okay\n. I used your example to make this, actually!!\n. @iglesias This is really unnecessary, I think I should remove it.\n. It doesn't, I forgot to remove it... sorry\n. those are not loop indices, but values to be used as features for the test.\n. Nope, I just copy-pasted that part... It's simply a header file, so I doubt it requires system includes. I'll remove that\n. I would say it's better to let users be aware that the KDTree feature is available if Nanoflann is installed, rather than telling them that it's not available at all.\n. @iglesias: Since I have not added this here, should I remove it? It would be much cleaner if I did.\n. @vigsterkr suggested it\n. ",
    "pranet": "Hi,\n@iglesias \nThis was what I had in mind when I commented out the zero. I may have been wrong though...The intention was to avoid direct comparison with zero\nhttp://stackoverflow.com/questions/2378628/can-you-compare-floating-point-values-exactly-to-zero\n@dhruv13J \nI completely missed that part . I'll try again\n. And how do I call the new function from TParameter::compare_ptype()? Do I have to use the existing one , or am I allowed to overload it as a template?\n. I see. thanks!\n. ",
    "vperic": "I'd be interested in this task - just to confirm, the files you have in mind are:\n- statistics_linear_time_mmd\n- statistics_quadratic_time_mmd.\n- statistics_mmd_kernel_selection\nand these should be added to ipython-notebooks/statistics/mmd_two_sample_testing.ipynb ?\nI'm still trying to get my bearing around the shogun codebase.\n. I'd be interested in this task - just to confirm, the files you have in mind are:\n- statistics_linear_time_mmd\n- statistics_quadratic_time_mmd.\n- statistics_mmd_kernel_selection\nand these should be added to ipython-notebooks/statistics/mmd_two_sample_testing.ipynb ?\nI'm still trying to get my bearing around the shogun codebase.\n. Shouldn't this be closed, that PR was merged already?\n. ",
    "punnu": "@iglesias: already working on it :)\n. @dhruv13j: sorry for messaging lately.. i started working on it last night. So, if you are ahead of me, i can move to another task. \n. ",
    "SunilMahendrakar": "@karlnapf ready for merge? :)\n. Will work on this :) \n. looks good! but if we could have a way to avoid having array of dot_functors one way is to create a new object in get_dot_functor and return it.. we could have some checking using dynamic_cast if we have already created a new object before..  something like\n```\ntemplate\nVectorDotProduct LinearAlgebra::get_dot_functor()\n    {\n        switch (backend)\n        {\n            case Eigen3:\n                Eigen3DotProduct functor; //or may be place it before switch\n                functor = dynamic_cast>(dot_functor);\n                if(!functor)\n                {\n                    delete dot_functor;\n                    functor = new Eigen3DotProduct();\n                }\n                dot_functor = functor;\n                break;\n            case Naive:\n                NaiveDotProduct functor;\n                functor = dynamic_cast>(dot_functor);\n                if(!functor)\n                {\n                    delete dot_functor;\n                    functor = new NaiveDotProduct();\n                }\n                dot_functor = functor;\n                break;\n            default:\n                break;\n        }\n        return reinterpret_cast>(dot_functor);\n    };\n```\nI am getting some compile time errors with this... \nthis would also avoid having to create all the unnecessary objects when set_backend is called\n. @lambday where are you? :p btw this is a lot better except for the lost flexibility  :)\n. will send a new request\n. @lambday thanks for the review. will take care about these. :)\n. @lambday @karlnapf sorry for the delay(had exams going on) please take a look :)\n. @vigsterkr shogun-unit-test executable works fine. I'll take a look\n. @vigsterkr okay :)\n. as per the discussions in #1973 I'm closing this.\n. @karlnapf  @lambday do I start working on this?\n. okay :) and it was cprobingsampler i changed it in the last minute hence the compilation error... will take extra care from next time :)\n. @lambday: typos :-/\n. @lambday din't knw they existed :p.. will do..\n. @lambday stupidity :p\n. @karlnapf okay :) will do... but the same is the default when sparse matrix(SGSparseMatrix is passed  this is the case when a SGMatrix is passed.. :) and yeah without COLPACK we cannot use probing sampler.\n. @karlnapf ah.. forget will do :)\n. but they're const right? lambday told its not required if they're const\n. Swig fails w/o the last one...sure will remove the first ones from all files..\n. @karlnapf since we're are not inheriting these classes from sgobject hence we need to delete. If we really want it we would have to create ref() and unref() methods in these classes :)\n. common.h can go in .VectorDotProduct.cpp my bad :/\n. Here VectorDotOperator should be CVectorDotOperator but the class_list.cpp.py script which generates class_list.cpp only supports single template argument. possible solution is to separate DotOperator for SGVector, SGSparseVector, and other GPU vectors or modify .py script to accommodate this but it cannot be generic as we don't know what the template arguments would be. need your help here.\n. @lambday do we make this a singleton or as usual a global? tried making it global(in last commit) but seems to give a segfault when constructor is called(ran gdb source seems to be sg_io variable will take a look.) but this way it works fine.\n. hi,\nWe'll have to add a simple method. I'll add it.\n. Agreed.\n. @lambday both Eigen3 and ViennaCL don't have static void compute(T* A, T* B, index_t num_elements, T alpha) in the spcializations we have. I was about to add it since it would be easy for moving other SGVector::scale calls to linalg::scale\n. @lambday yeah. so that we just have a REQUIRE there which calls this method to perform the actual computation.\n. sure!\n. @lambday this should be both (HAVE_EIGEN3 OR HAVE_VIENNACL) right? since we already check ( _FOUND and ENABLE_ ) on top and set( HAVE_ )\n. ",
    "emaadmanzoor": "I'd like to work on this, if noone is already.\n. I'd like to work on this, if noone is already.\n. ",
    "armanform": "Maybe I'm too late. But I'm also try to impelement it.\n. Ok, I'll try to fix all issues that you've mentioned.\n. Yes, of course I've used some theoretical material from wiki. And also this source http://www.stanford.edu/class/cs106l/handouts/assignment-3-kdtree.pdf   was good for implementing kdtree.\n. ",
    "pl8787": "@vigsterkr OK. Let me try again first. If I have some problems, I'll leave message here :)\n. @vigsterkr \nI fixed Conditional jump or move depends on uninitialised value and memory leak now.\nI think it's my fault.\nI use valgrind, it return no error now.\n. @vigsterkr \nI fixed Conditional jump or move depends on uninitialised value and memory leak now.\nI think it's my fault.\nI use valgrind, it return no error now.\n. [----------] 14 tests from LibLinearTest\n[ RUN      ] LibLinearTest.train_L2R_LR\n[       OK ] LibLinearTest.train_L2R_LR (118 ms)\n[ RUN      ] LibLinearTest.train_L2R_L2LOSS_SVC_DUAL\n[       OK ] LibLinearTest.train_L2R_L2LOSS_SVC_DUAL (21 ms)\n[ RUN      ] LibLinearTest.train_L2R_L2LOSS_SVC\n[       OK ] LibLinearTest.train_L2R_L2LOSS_SVC (46 ms)\n[ RUN      ] LibLinearTest.train_L2R_L1LOSS_SVC_DUAL\n[       OK ] LibLinearTest.train_L2R_L1LOSS_SVC_DUAL (19 ms)\n[ RUN      ] LibLinearTest.train_L1R_L2LOSS_SVC\n[       OK ] LibLinearTest.train_L1R_L2LOSS_SVC (53 ms)\n[ RUN      ] LibLinearTest.train_L1R_LR\n[       OK ] LibLinearTest.train_L1R_LR (53 ms)\n[ RUN      ] LibLinearTest.train_L2R_LR_DUAL\n[       OK ] LibLinearTest.train_L2R_LR_DUAL (73 ms)\n[ RUN      ] LibLinearTest.train_L2R_LR_BIAS\n[       OK ] LibLinearTest.train_L2R_LR_BIAS (29 ms)\n[ RUN      ] LibLinearTest.train_L2R_L2LOSS_SVC_DUAL_BIAS\n[       OK ] LibLinearTest.train_L2R_L2LOSS_SVC_DUAL_BIAS (19 ms)\n[ RUN      ] LibLinearTest.train_L2R_L2LOSS_SVC_BIAS\n[       OK ] LibLinearTest.train_L2R_L2LOSS_SVC_BIAS (20 ms)\n[ RUN      ] LibLinearTest.train_L2R_L1LOSS_SVC_DUAL_BIAS\n[       OK ] LibLinearTest.train_L2R_L1LOSS_SVC_DUAL_BIAS (18 ms)\n[ RUN      ] LibLinearTest.train_L1R_L2LOSS_SVC_BIAS\n[       OK ] LibLinearTest.train_L1R_L2LOSS_SVC_BIAS (39 ms)\n[ RUN      ] LibLinearTest.train_L1R_LR_BIAS\n[       OK ] LibLinearTest.train_L1R_LR_BIAS (46 ms)\n[ RUN      ] LibLinearTest.train_L2R_LR_DUAL_BIAS\n[       OK ] LibLinearTest.train_L2R_LR_DUAL_BIAS (32 ms)\n[----------] 14 tests from LibLinearTest (600 ms total)\nI added more 7 unit-test case without bias. \n. [----------] 14 tests from LibLinearTest\n[ RUN      ] LibLinearTest.train_L2R_LR\n[       OK ] LibLinearTest.train_L2R_LR (118 ms)\n[ RUN      ] LibLinearTest.train_L2R_L2LOSS_SVC_DUAL\n[       OK ] LibLinearTest.train_L2R_L2LOSS_SVC_DUAL (21 ms)\n[ RUN      ] LibLinearTest.train_L2R_L2LOSS_SVC\n[       OK ] LibLinearTest.train_L2R_L2LOSS_SVC (46 ms)\n[ RUN      ] LibLinearTest.train_L2R_L1LOSS_SVC_DUAL\n[       OK ] LibLinearTest.train_L2R_L1LOSS_SVC_DUAL (19 ms)\n[ RUN      ] LibLinearTest.train_L1R_L2LOSS_SVC\n[       OK ] LibLinearTest.train_L1R_L2LOSS_SVC (53 ms)\n[ RUN      ] LibLinearTest.train_L1R_LR\n[       OK ] LibLinearTest.train_L1R_LR (53 ms)\n[ RUN      ] LibLinearTest.train_L2R_LR_DUAL\n[       OK ] LibLinearTest.train_L2R_LR_DUAL (73 ms)\n[ RUN      ] LibLinearTest.train_L2R_LR_BIAS\n[       OK ] LibLinearTest.train_L2R_LR_BIAS (29 ms)\n[ RUN      ] LibLinearTest.train_L2R_L2LOSS_SVC_DUAL_BIAS\n[       OK ] LibLinearTest.train_L2R_L2LOSS_SVC_DUAL_BIAS (19 ms)\n[ RUN      ] LibLinearTest.train_L2R_L2LOSS_SVC_BIAS\n[       OK ] LibLinearTest.train_L2R_L2LOSS_SVC_BIAS (20 ms)\n[ RUN      ] LibLinearTest.train_L2R_L1LOSS_SVC_DUAL_BIAS\n[       OK ] LibLinearTest.train_L2R_L1LOSS_SVC_DUAL_BIAS (18 ms)\n[ RUN      ] LibLinearTest.train_L1R_L2LOSS_SVC_BIAS\n[       OK ] LibLinearTest.train_L1R_L2LOSS_SVC_BIAS (39 ms)\n[ RUN      ] LibLinearTest.train_L1R_LR_BIAS\n[       OK ] LibLinearTest.train_L1R_LR_BIAS (46 ms)\n[ RUN      ] LibLinearTest.train_L2R_LR_DUAL_BIAS\n[       OK ] LibLinearTest.train_L2R_LR_DUAL_BIAS (32 ms)\n[----------] 14 tests from LibLinearTest (600 ms total)\nI added more 7 unit-test case without bias. \n. That log is too big to upload.\nIt's 2AM now, I upload to my dropbox tomorrow.\n. The following tests FAILED:\n3 - integration-python_modular-SVMOcas_30_1en05_16_05_False (Failed)\nErrors while running CTest\nIs that what @karlnapf  mentioned in #1948 ?\n. @karlnapf  Yeah, I know what you mean. \nI need to generate some fixed data set and test both on liblinear and shogun, then compare them w and b one by one. \n. @karlnapf  I think it is possible to write another unit test file to test simple data set like:\n-1 1:-1 2:1\n-1 1:-1 2:0\n-1 1:-1 2:-1\n-1 1:-2 2:0\n-1 1:0 2:-1\n1 1:2 2:2\n1 1:3 2:1\n1 1:3 2:2\n1 1:3 2:3\n1 1:4 2:2\nin LibLinear_SimpleSet_unittest.cc\n. @karlnapf all code already pass valgrind check.\n. @karlnapf \nIn my comput it's valgrind clean.\nI use command valgrind --tool=memcheck --leak-check=yes ./shougun-unit-test > log\nIn log file there's no valgrind infomation around LibLinearTest.\nAnd I think the travis doesn't work now. \n. @karlnapf \nIn my comput it's valgrind clean.\nI use command valgrind --tool=memcheck --leak-check=yes ./shougun-unit-test > log\nIn log file there's no valgrind infomation around LibLinearTest.\nAnd I think the travis doesn't work now. \n. @vigsterkr Is it possible to stop the preview travis check, only check the latest e771a08?\n. @vigsterkr Is it possible to stop the preview travis check, only check the latest e771a08?\n. @vigsterkr yes\nThanks\n. @vigsterkr yes\nThanks\n. @vigsterkr  Sorry ,  3292\nonly need 3295\n. @vigsterkr  Sorry ,  3292\nonly need 3295\n. @karlnapf  and @vigsterkr\nAsk a question about git, how to organize the git branches in my fork version in order to enable multiple PR?\nOnce I PR one patch then the other patch commit will show in this PR.\nI think may be I need a branch maintain the develop branch in the shogun. And other branches come from this base branch. \nIs that the strategy you use?\n. @karlnapf  and @vigsterkr\nAsk a question about git, how to organize the git branches in my fork version in order to enable multiple PR?\nOnce I PR one patch then the other patch commit will show in this PR.\nI think may be I need a branch maintain the develop branch in the shogun. And other branches come from this base branch. \nIs that the strategy you use?\n. @coveralls  because it's the bug fixed version, no coverage improved.\nYou may compare the version has LibLinear_unittest.cc with the one w/o it.\n. @coveralls  because it's the bug fixed version, no coverage improved.\nYou may compare the version has LibLinear_unittest.cc with the one w/o it.\n. @karlnapf  Ok, It has passed Travis. \n. @karlnapf  @vigsterkr  ok I modify it now.\n. Have a try:)\n. @emtiyaz the return of the function ElogLik.m what's the meaning of gm and gv.\n. @ouceduxzk Thank you. And did you implement these already?\n. @ouceduxzk  I am confused too. \n@emtiyaz  Another problem here, in example.m I don't know where's the function call of ElogLik.m. \n. @karlnapf I have checked with valgrind.\nSorry for forget your name, because I just copy from the last unit test.\n. @karlnapf \nYou mean all unit test should be under BSD license?\nIs there any example of the BSD license we used?\n. @karlnapf I'll solve this in the coming pull request.\n. @karlnapf I'll solve this in the coming pull request.\n. @karlnapf sorry for late.\n. @karlnapf It's very kind of you. Thank you very much.\nAs you say it's just an example which is the target of the issue 1971.\nI implement it in a different way with @yorkerlin and just want to be familar with the GP.\nOK, I'll modify the code anyway.\nThere's some questions:\n1. ElogLik would be design to be a class, and this class deriver from which base class? And which forder should I put ElogLik?\n2. How small a patch is acceptable?\n. Thanks @lisitsyn I have rectifyed them.\n. @karlnapf ok, I understood. I'll do it later.\n. @karlnapf ok, I understood. I'll do it later.\n. @yorkerlin very impressive implement. Thanks a lot. Did you check the result of the ElogLik function?\nI think we can add bsxfun to the basic mathmatic lib. \n. Thank you.\n@vigsterkr I think it will be an static class just like CStatistics  in mathmatic.\n. @vigsterkr ok, I'm checking my code.\nGive me some advice about how to transcribing a matlab code into c++, but look not  that bad. \n. @vigsterkr \nhm = bsxfun(@times, a, bsxfun(@times, 1./v, bsxfun(@plus, l.^3, -l.^2*m + 2*l*v).*pl - bsxfun(@plus, h.^3, -h.^2*m + 2*h*v).*ph) + 2.*(ch - cl));\nfor example, this equation is hard to understand, and how to implement using c++?\n. @emtiyaz  You mean that we just use the GP already existed in Shogun. \nBoth use regression and classification to train and test the data.\n. @karlnapf @emtiyaz MovieLens data has string value like Movie Name, can I use other program like python pre-process the data?\nOr there are some useful io class i don't know?\n. @emtiyaz @karlnapf \nThere's some problems when I implement this example.\nDataset: Movielens 100k\nGenerate train feature matrix: 21_8000\nGenerate test feature matrix: 21_2000\nwhen I call function\nCExactInferenceMethod* inf = new CExactInferenceMethod(kernel,\n                          feat_train, mean, lab_train, lik);\n...\nCGaussianProcessRegression* gpr = new CGaussianProcessRegression(inf);\n...\nCRegressionLabels* predictions=gpr->apply_regression(feat_test);\nthe error occur:\nOut of memory error, tried to allocate 12800000000 bytes using malloc.\n. @emtiyaz Thanks a lot, I have realized that, actually I am still working on it.\n@karlnapf Yes I know it's too large for a normal PC.\n. @karlnapf  What's the meaning of work with string features? \nMake the stirng feature as a binary vector?\nFor example:\n\"a\" -> [1,0,0]\n\"b\" -> [0,1,0]\n\"c\" -> [0,0,1]\n\"a\" -> [1,0,0]\n. @yorkerlin  you have done the task 1971? So I should closed the pull request?\n. Valgrind check memory:\n==16218== LEAK SUMMARY:\n==16218==    definitely lost: 0 bytes in 0 blocks\n==16218==    indirectly lost: 0 bytes in 0 blocks\n==16218==      possibly lost: 640 bytes in 2 blocks\n==16218==    still reachable: 1,912 bytes in 3 blocks\n==16218==         suppressed: 0 bytes in 0 blocks\nand problem of here, I don't know why:\n```\n==16218== Invalid read of size 1\n==16218==    at 0x57C9B07: shogun::CDelimiterTokenizer::next_token_idx(int&) (in /home/pl/Develop/shogun/src/shogun/libshogun.so.16.1)\n==16218==    by 0x59947B4: shogun::CParser::read_cstring() (in /home/pl/Develop/shogun/src/shogun/libshogun.so.16.1)\n==16218==    by 0x5994494: shogun::CParser::read_real() (in /home/pl/Develop/shogun/src/shogun/libshogun.so.16.1)\n==16218==    by 0x403CAD: read_items_data(char const) (in /home/pl/Develop/shogun/examples/undocumented/libshogun/regression_gaussian_process_movielens)\n==16218==    by 0x40274E: main (in /home/pl/Develop/shogun/examples/undocumented/libshogun/regression_gaussian_process_movielens)\n==16218==  Address 0x139fbb0c is 1 bytes after a block of size 123 alloc'd\n==16218==    at 0x4C2B6CD: malloc (in /usr/lib/valgrind/vgpreload_memcheck-amd64-linux.so)\n==16218==    by 0x586C802: shogun::sg_malloc(unsigned long) (in /home/pl/Develop/shogun/src/shogun/libshogun.so.16.1)\n==16218==    by 0x55FD0CD: shogun::CCircularBuffer::detach_chunk(char, int, int, int, bool) (in /home/pl/Develop/shogun/src/shogun/libshogun.so.16.1)\n==16218==    by 0x55FD168: shogun::CCircularBuffer::pop(int) (in /home/pl/Develop/shogun/src/shogun/libshogun.so.16.1)\n==16218==    by 0x5976180: shogun::CLineReader::read_token(int) (in /home/pl/Develop/shogun/src/shogun/libshogun.so.16.1)\n==16218==    by 0x597622E: shogun::CLineReader::read_line() (in /home/pl/Develop/shogun/src/shogun/libshogun.so.16.1)\n==16218==    by 0x403AD1: read_items_data(char const*) (in /home/pl/Develop/shogun/examples/undocumented/libshogun/regression_gaussian_process_movielens)\n==16218==    by 0x40274E: main (in /home/pl/Develop/shogun/examples/undocumented/libshogun/regression_gaussian_process_movielens)\n```\nThe result of the program:\nMean Squared Error on Train:0.012712\nMean Squared Error on Test:0.030699\nLoss User Count in Test:3\nThere's some users can't be predict, because label preprocess 1->-1 , 5->1 and ignore others, some users have no training data, but have test data. \n. Failed because the movielens dataset need to be update.\nI have give a pull request on shogun-data now.\n. @emtiyaz I got the result:\nMean Squared Error on Train:0.012712\nMean Squared Error on Test:0.030699\nYour code still throw the error:\nundefined function 'setSeed'\nand I remove the function setSeed, another error occur:\n```\nError using feval\nUndefined function 'meanZero' for input\narguments of type 'double'.\nError in meanFuncMultiClass (line 43)\n\u00a0 \u00a0 mu(idx) = feval(meanfunc{:},\n\u00a0 \u00a0 hyp(k,:)', x);\nError in recommend0 (line 48)\nmu = meanFuncMultiClass({@meanZero}, 2,\n[], X);\n```\nMy matlab version is MATLAB R2012b\n. @emtiyaz Sorry for that, I am learning GP now, something not quite understand.\nIn file meanFunctionsMultiClass.m , it says:\n% Mean functions for multiclass GP. meanfunc is an array of functions as\n% listed in 'meanFunctions.m'. nClass is number of classes. Rest of the elements\n% are similar to the other GP covariance functions.\nbut meanFunction.m is missing.\n. @emtiyaz Sorry for that, I am learning GP now, something not quite understand.\nIn file meanFunctionsMultiClass.m , it says:\n% Mean functions for multiclass GP. meanfunc is an array of functions as\n% listed in 'meanFunctions.m'. nClass is number of classes. Rest of the elements\n% are similar to the other GP covariance functions.\nbut meanFunction.m is missing.\n. @emtiyaz  Thanks a lot.\nMy code is generate kernel user by user.\nFor example uid=1 he has 105 items. So It's kernel size is 105105.\nI don't know why The kernel matrix remains the same for all the users.\n. @emtiyaz  Thanks a lot.\nMy code is generate kernel user by user.\nFor example uid=1 he has 105 items. So It's kernel size is 105105.\nI don't know why The kernel matrix remains the same for all the users.\n. @emtiyaz \nCould I say the different user has different GP model?\nThe items they rating, should be the training data, and each of them has a genre feature vector.\nDot product of these feature vectors is the kernel K, we need to calculate m(x_{n+1})=k^T_C^{-1}t.\nwhere C=K+\\beta^{-1}.\n. @emtiyaz \nCould I say the different user has different GP model?\nThe items they rating, should be the training data, and each of them has a genre feature vector.\nDot product of these feature vectors is the kernel K, we need to calculate m(x{n+1})=k^T_C^{-1}_t.\nwhere C=K+\\beta^{-1}.\n. @emtiyaz  So my code's result is right, but I need to use the whole kernel and for each user get the sub kernel from it ?\n. @emtiyaz  So my code's result is right, but I need to use the whole kernel and for each user get the sub kernel from it ?\n. @emtiyaz\nI see some small error in your code\nIn file recommend0.m Line 61\n errTr = [errTe; Y(I,n) - pHat];\nshould be\nerrTr = [errTr; Y(I,n) - pHat];\n. @emtiyaz\nI see some small error in your code\nIn file recommend0.m Line 61\n errTr = [errTe; Y(I,n) - pHat];\nshould be\nerrTr = [errTr; Y(I,n) - pHat];\n. @emtiyaz \nAfter fix my bugs, I think my code is right.\nI modify your code read the same dataset as my code read.\nI print the kernel I made is just the same as yours.\nMy result is:\n```\nRoot Mean Squared Error on Train:0.326500\nRoot Mean Squared Error on Test:0.772123\n```\nYour result is:\n```\nRoot Mean Squared Error on Train:0.3472\nRoot Mean Squared Error on Test:0.7662\n```\n. @emtiyaz \nAfter fix my bugs, I think my code is right.\nI modify your code read the same dataset as my code read.\nI print the kernel I made is just the same as yours.\nMy result is:\n```\nRoot Mean Squared Error on Train:0.326500\nRoot Mean Squared Error on Test:0.772123\n```\nYour result is:\n```\nRoot Mean Squared Error on Train:0.3472\nRoot Mean Squared Error on Test:0.7662\n```\n. @emtiyaz \nM_items Matrix save the all the items feature, and with each user I will get the subset of it and generate the kernel.\nI don't know how to split a kernel directly.\n. @emtiyaz \nM_items Matrix save the all the items feature, and with each user I will get the subset of it and generate the kernel.\nI don't know how to split a kernel directly.\n. @emtiyaz \nThe kernel is the same because the kernel two hyper parameters are the same as the matlab.\nBut as your say the value of pHat is a little different\nIn matlab:\n0.967130655330889 -0.987889925624160 0.999098499609180 0.992744083696641 0.999098499609180 0.999760787117511 0.999098499609180 -0.989512663005208 -0.993819035392540 0.997549375585693 -0.989983022519339 0.992744083696641 0.996112025428189 0.997549375585693 0.996907430856761 0.993279092431305 0.999098499609180 0.999098499609180 0.999098499609180 0.993843803100296 0.996112025428189 0.999814777650095 -0.988167536236384 0.999760787117511 0.997549375585693 0.997549375585693 0.996907836966916 0.987406286617997 0.504037236782428 0.999098499609180 -0.982170913813507 0.999814777650095 0.999098499609180 0.999098499609180 0.992744083696641 1.01342959711848 0.996133973947022 0.981318987553670 0.994079216661293 0.999098499609180 0.996907430856761 0.996907836966916 0.990432040652404 0.504037236782428 0.994006322075223 0.999814777650095 0.986413597004904 0.999760787117511 1.00147670001090 -0.993819035392540 0.504037236782428 -1.00107050162625 -0.982170913813507 -0.965378628877540 0.504037236782428 0.992744083696641 1.00074170507600 \nIn shogun:\n0.967297403116997745,-0.987703268783877042,0.999099649894647279,0.992806413065643434,0.999099649894647279,0.999761407250696399,0.999099649894647279,-0.989513664064353926,-0.99382036318894984,0.997548975421378992,-0.989997334700105092,0.992806413065643434,0.997010601527225648,0.997548975421378992,0.99690625114721898,0.993279078818943506,0.999099649894647279,0.999099649894647279,0.999099649894647279,0.993842484125151304,0.991888709344075914,0.990099009900989868,-0.988169693659444959,0.999761407250696399,0.997548975421378992,0.997548975421378992,0.995452310272177621,0.987406977656590024,0.50403705044112701,0.999099649894647279,-0.990052899343959347,0.999723492970365224,0.999099649894647279,0.999099649894647279,0.992806413065643434,1.01333062327803591,0.996134704632474444,0.98131614846900439,0.99407992629644315,0.999099649894647279,0.99690625114721898,0.996579197226125646,0.99043217779795889,0.50403705044112701,0.994003785844135401,0.999723492970365224,0.986414518589334599,0.999761407250696399,1.00147844045408596,-0.99382036318894984,0.50403705044112701,-1.00103406052667587,-0.97077403542076568,-0.965392128060147292,0.50403705044112701,0.992806413065643434,1.00074127954487024\nyour parameter is sig2=0.01\nand I set the sigma=0.1 \nlik->set_sigma(0.1);\nBecause in GaussianLikelihood:\nCMath::sq(m_sigma)\nMaybe the little different sum over user by user lead to the rmse different.\n@karlnapf Let't me check it again.\nBy the way, why travis didn't work on my branch?\n. @emtiyaz \nFor every user iterator I calculate the sum of square error:\nThe square error is increase user by user.\nIn matlab:\nuser: 1\ntraint v_len 57\ntest v_len 49\ntrain error 3.005002\ntest error 28.854481\nuser: 2\ntraint v_len 11\ntest v_len 6\ntrain error 5.005890\ntest error 36.438401\nuser: 3\ntraint v_len 8\ntest v_len 6\ntrain error 5.007021\ntest error 46.793300\nuser: 4\ntraint v_len 9\ntest v_len 5\ntrain error 5.007180\ntest error 47.679637\nuser: 5\ntraint v_len 37\ntest v_len 31\ntrain error 10.015008\ntest error 71.884580\nuser: 6\ntraint v_len 21\ntest v_len 27\ntrain error 10.017555\ntest error 79.986748\nuser: 7\ntraint v_len 87\ntest v_len 88\ntrain error 23.226669\ntest error 114.097056\nuser: 8\ntraint v_len 10\ntest v_len 16\ntrain error 23.227305\ntest error 122.728586\nuser: 9\ntraint v_len 6\ntest v_len 5\ntrain error 23.228517\ntest error 126.333358\nuser: 10\ntraint v_len 28\ntest v_len 26\ntrain error 23.228897\ntest error 128.579372\nuser: 11\ntraint v_len 20\ntest v_len 9\ntrain error 23.230268\ntest error 131.724584\nuser: 12\ntraint v_len 10\ntest v_len 17\ntrain error 23.230396\ntest error 136.134262\nuser: 13\ntraint v_len 158\ntest v_len 108\ntrain error 89.544953\ntest error 232.297734\nuser: 14\ntraint v_len 21\ntest v_len 25\ntrain error 89.546437\ntest error 241.303654\nand In shogun:\nProcessing User:1\nTrain Vlen: 57\nTest Vlen: 49\nError Train: 3.005461\nError Test: 28.483049\nProcessing User:2\nTrain Vlen: 11\nTest Vlen: 6\nError Train: 5.006349\nError Test: 36.066968\nProcessing User:3\nTrain Vlen: 8\nTest Vlen: 6\nError Train: 5.007480\nError Test: 46.421868\nProcessing User:4\nTrain Vlen: 9\nTest Vlen: 5\nError Train: 5.007712\nError Test: 47.308173\nProcessing User:5\nTrain Vlen: 37\nTest Vlen: 31\nError Train: 10.015406\nError Test: 72.147741\nProcessing User:6\nTrain Vlen: 21\nTest Vlen: 27\nError Train: 10.017898\nError Test: 81.778323\nProcessing User:7\nTrain Vlen: 87\nTest Vlen: 88\nError Train: 23.226615\nError Test: 119.147137\nProcessing User:8\nTrain Vlen: 10\nTest Vlen: 16\nError Train: 23.227270\nError Test: 127.780375\nProcessing User:9\nTrain Vlen: 6\nTest Vlen: 5\nError Train: 23.228483\nError Test: 131.481039\nProcessing User:10\nTrain Vlen: 28\nTest Vlen: 26\nError Train: 23.228905\nError Test: 133.861541\nProcessing User:11\nTrain Vlen: 20\nTest Vlen: 9\nError Train: 23.230216\nError Test: 137.000420\nProcessing User:12\nTrain Vlen: 10\nTest Vlen: 17\nError Train: 23.230345\nError Test: 141.868105\nProcessing User:13\nTrain Vlen: 158\nTest Vlen: 108\nError Train: 80.568199\nError Test: 261.032326\n. @emtiyaz \nI mean that the error of train and test both increase user by user, for the case of the sum of the square error. In the end it'll reflect on  the second digit for total RMSE. \n. @emtiyaz \nFor data set generate:\nMatlab for training rating set:\nfileName = 'u1.base'\ndata = importdata(fileName);\nif display; fprintf('First 4 lines in %s should be the following:',fileName); end;\ndata(1:4,:)\nnUsers = 943;%length(unique(data(:,1)));\nnMovies = 1682;%length(unique(data(:,2)));\n% user ratings\nratings_train = sparse(data(:,2), data(:,1), data(:,3), nMovies, nUsers);\nMatlab for testing rating set:\nfileName = 'u1.test'\ndata = importdata(fileName);\nif display; fprintf('First 4 lines in %s should be the following:',fileName); end;\ndata(1:4,:)\nnUsers = 943;%length(unique(data(:,1)));\nnMovies = 1682;%length(unique(data(:,2)));\n% user ratings\nratings_test = sparse(data(:,2), data(:,1), data(:,3), nMovies, nUsers);\nMatlab for item features:\nfileName = 'u.item';\nout = importdata(fileName,'|');\ndata = out.data;\nif display; fprintf('First 4 lines in %s should be the following:',fileName); end;\ndata(1:4,:)\nmovieInformation = out.textdata; % title, release date etc\nmovieMetaData = sparse(data); % genres information\nIn the end save these value to file movielens100k.mat.\n. @emtiyaz \nThe GP Matlab code:\n```\n% A simple recommendation system\n% Written by Emtiyaz, EPFL\n% Modified on March 11, 2014\n%clear all\nseed = 1;\n% load data\nload('movielens100k.mat');\nY = ratings_train;\nYtest = ratings_test;\nX = movieMetaData;\n[M,N] = size(Y);\n% Make binary data from Y\n% We will treat ratings with 5 or 1 as binary labels,\n% and treat everything else as missing values\nY(Y==1) = -1;\nY(Y==5) = 1;\nY(Y>1) = 0;\nYtest(Ytest==1) = -1;\nYtest(Ytest==5) = 1;\nYtest(Ytest>1) = 0;\n% Set hyper-params for a GP model\n% There are three (right now set to a suboptimal value)\nlogScale = 0; % GP prior parameters\nlogSigma = 0;\nsig2 = .01; % likelihood variance\n% We will use zero mean and covSEiso Kernel\nmu = meanFuncMultiClass({@meanZero}, 2, [], X);\nSigma = covFuncMultiClass({@covSEiso}, 2, [logScale logSigma], X);\n% Gaussian process regression\nerrTr = [];\nerrTe = [];\nfor n = 1:N\n  I = find(Y(:,n));\n  Ite = find(Ytest(:,n));\n  if size(I,1)==0 || size(Ite,1)==0\n      continue\n  end\n  yn = Y(I,n);\n  G = inv(sig2eye(length(I)) + Sigma(I,I));\n  e = G(yn - mu(I));\n  % prediction and errors\n  pHat = mu(I) + Sigma(I,I)e;\n  errTr = [errTr; Y(I,n) - pHat];\n  pHat = mu(Ite) + Sigma(Ite,I)e;\n  errTe = [errTe; Ytest(Ite,n) - pHat];\n  fprintf('user: %d\\n',n);\n  fprintf('traint v_len %d\\n',size(I,1));\n  fprintf('test v_len %d\\n',size(Ite,1));\n  fprintf('train error %f\\n',sum(errTr.^2));\n  fprintf('test error %f\\n',sum(errTe.^2));\nend\nfprintf('Train Error %.4f Test Error %.4f\\n', sqrt(mean(errTr.^2)), sqrt(mean(errTe.^2)));\n``\n. @emtiyaz \nYou are right for user 1, the kernel matrix isA = sig2*eye(length(I)) + Sigma(I,I)`\nwhich condition number is very large:\nc = cond(A)\n c=1.8556e+03\n. @emtiyaz \nI increase the sigma parameter to a very large number like 100 (So in matlab sig2 will be 10000).\nThe kernel matrix will have low condition number.\nc = cond(A)\nc = 1.0019\nbut the different between Matlab and my code is still on the second decimal.\nI check in the shogun line by line, compare every matrix computation. I found that\n```\nIn ExactInferenceMethod.cpp\nIn function update_alpha()\na = L.triangularView().solve(a);\n```\nproduce the error.\nI'm sure that steps before this one perform well (I checked element by element in matrix/vector). \n. @emtiyaz \nAnd I found that the larger of the kernel matrix is ,the larger result different.\nFor example:\nUser1  kernel matrix 57_57 it's about 0.05 differ in Sum of square error.\nUser2 kernel matrix 11_11 it approaches to 0.\n. @emtiyaz  I don't know why increase sig2 does not work? \nAs you suggestion I will try it tomorrow.\nThanks a lot.\n. @emtiyaz  @karlnapf \nSorry to say that the problem is in the code of reading item features.\nThe parameter need to set to false, because if it set to true, it will ignore consequence delimiters.\nCDelimiterTokenizer *tokenizer=new CDelimiterTokenizer(false);\ntokenizer->delimiters['|'] = 1;\nNow the rmse is just the same as the matlab code result. Sorry again for my mistakes.\nResult\n```\nIn matlab:\nTrain Error 0.3438\nTest Error: 0.7659\nIn my code:\nRoot Mean Squared Error on Train:0.343765\nRoot Mean Squared Error on Test:0.765903\n```\nAnd I check the memory via valgrind the ==16218== Invalid read of size 1 problem before has gone.\nWhy travis didn't compile my code ?\nAnd may I continue with the example of GP binary classification?\n. @emtiyaz \nok\nI use original matlab code and data (fixed the Train rmse bug) the result is:\nTrain Error 0.3638 Test Error 0.7226\nTest on u1.base and u1.test\n| logScale | logSigma | sig2 | train_rmse matlab/my code | test_rmse matlab/my code |\n| --- | --- | --- | --- | --- |\n| 0 | 0 | 0.01 | 0.3438 / 0.3438 | 0.7659 / 0.7659 |\n| log(0.1) | log(0.1) | 0.01 | 0.5183 / 0.5183 | 0.8731 / 0.8731 |\n| 0 | 0 | 1 | 0.4840 / 0.4840 | 0.7175 /\u00a00.7175 |\nthe other u_.base and u_.test got the similar result.\nI don't know why the test rmse is wrong?\n. @emtiyaz  OK, I make a pull request now.\nIn matlab:\nDataset U1\nTrain Error 0.3438 Test Error 0.7659\nDataset U2\nTrain Error 0.3551 Test Error 0.7599\nDataset U3\nTrain Error 0.3488 Test Error 0.7618\nDataset U4\nTrain Error 0.3593 Test Error 0.7517\nDataset U5\nTrain Error 0.3629 Test Error 0.7504\n```\nIn mycode\nDataset U1\nTrain Pairs: 9046\nTest Pairs: 5841\nLoss User Count in Test:3\nRoot Mean Squared Error on Train:0.343765\nRoot Mean Squared Error on Test:0.765903\nDataset U2\nTrain Pairs: 14426\nTest Pairs: 5593\nLoss User Count in Test:3\nRoot Mean Squared Error on Train:0.355103\nRoot Mean Squared Error on Test:0.759953\nDataset U3\nTrain Pairs: 19318\nTest Pairs: 5264\nLoss User Count in Test:2\nRoot Mean Squared Error on Train:0.348809\nRoot Mean Squared Error on Test:0.761892\nDataset U4\nTrain Pairs: 20794\nTest Pairs: 5249\nLoss User Count in Test:6\nRoot Mean Squared Error on Train:0.359278\nRoot Mean Squared Error on Test:0.751786\nDataset U5\nTrain Pairs: 20642\nTest Pairs: 5299\nLoss User Count in Test:12\nRoot Mean Squared Error on Train:0.362865\nRoot Mean Squared Error on Test:0.750463\n```\n. I extend my code to Binary classification.\nAccuracy on Train: 91.056821%\nAccuracy on Test: 83.889745%\n. @emtiyaz \nrmse using gaussian regression:\n```\nDataset U1\nRoot Mean Squared Error on Train:0.343765\nRoot Mean Squared Error on Test:0.765903\nDataset U2\nRoot Mean Squared Error on Train:0.355092\nRoot Mean Squared Error on Test:0.759884\nDataset U3\nRoot Mean Squared Error on Train:0.348799\nRoot Mean Squared Error on Test:0.761820\nDataset U4\nRoot Mean Squared Error on Train:0.359270\nRoot Mean Squared Error on Test:0.751713\nDataset U5\nRoot Mean Squared Error on Train:0.362857\nRoot Mean Squared Error on Test:0.750392\n```\nAccuracy using binary gaussian classifier\n```\nDataset U1\nRoot Mean Squared Error on Train:0.659515\nRoot Mean Squared Error on Test:0.766503\nDataset U2\nRoot Mean Squared Error on Train:0.660438\nRoot Mean Squared Error on Test:0.755709\nDataset U3\nRoot Mean Squared Error on Train:0.653237\nRoot Mean Squared Error on Test:0.766492\nDataset U4\nRoot Mean Squared Error on Train:0.652584\nRoot Mean Squared Error on Test:0.763884\nDataset U5\nRoot Mean Squared Error on Train:0.655351\nRoot Mean Squared Error on Test:0.770729\n```\n. @emtiyaz  sorry I realize the problem, I fixed it now. And I will update the data result.\nSee above result, the test rmse is really close, but the train rmse is a little different.\nok, I will check it tomorrow.\n. CEPInferenceMethod & CLaplacianInferenceMethod\nCLogitLikelihood & CProbitLikelihood\nwhich one should I choose to do this task.\nthey are a little different.\n. @emtiyaz \nAccuracy using binary gaussian classifier (CEPInferenceMethod + CProbitLikelihood)\n```\nDataset U1\nRoot Mean Squared Error on Train:0.557569\nRoot Mean Squared Error on Test:0.729119\nDataset U2\nRoot Mean Squared Error on Train:0.561247\nRoot Mean Squared Error on Test:0.722258\nDataset U3\nRoot Mean Squared Error on Train:0.555730\nRoot Mean Squared Error on Test:0.731702\nDataset U4\nRoot Mean Squared Error on Train:0.557041\nRoot Mean Squared Error on Test:0.727075\nDataset U5\nRoot Mean Squared Error on Train:0.560331\nRoot Mean Squared Error on Test:0.730113\n```\nThe test error is better than the regression but the train error is worse.\nI have no idea of what's matter of my code, it's just replace the regression method to classification method.\n. @karlnapf \nPls check my code again, thanks.\n. @karlnapf \nOK, I'll check it tomorrow.\nYes, that's what I'm interested in.\n. @karlnapf \nSorry for late reply.\nIn File GaussianProcessMachine.cpp\n- In function: get_posterior_means\n  line 73: SGMatrix k_trts=kernel->get_kernel_matrix\n  Then Map this kernel Matrix to the eigen_Ks, and compute Ks=Ks_scale^2 eigen_Ks_=CMath::sq(m_method->get_scale()). \n  So kernel matrix has been modified.\n- In function: get_posterior_variances\n  line 129: SGMatrix k_tsts=kernel->get_kernel_matrix\n  Have the same problem in above.\n- In function: get_posterior_variances\n  line 139: SGMatrix k_trts=kernel->get_kernel_matrix\n  Have the same problem in above.\n. @karlnapf \nI think we can use eigen_Ks*=1/CMath::sq(m_method->get_scale()); after all kernel works have done to rewrite its value back to the origin.\n. @karlnapf \npls check this code. \nIf everything is ok ,let's move on to the next PR :) feel a little exciting.\n. @karlnapf \nok I got it.\n. @karlnapf \nYes I test it.\nI think the memory leak is comes from ID3ClassifierTree.\n==22100== \n==22100== HEAP SUMMARY:\n==22100==     in use at exit: 20,934 bytes in 344 blocks\n==22100==   total heap usage: 8,613,033 allocs, 8,612,689 frees, 1,188,888,490 bytes allocated\n==22100== \n==22100== 320 bytes in 1 blocks are possibly lost in loss record 221 of 236\n==22100==    at 0x4C2ABB4: calloc (in /usr/lib/valgrind/vgpreload_memcheck-amd64-linux.so)\n==22100==    by 0x4012598: _dl_allocate_tls (dl-tls.c:296)\n==22100==    by 0x64C07B5: pthread_create@@GLIBC_2.2.5 (allocatestack.c:579)\n==22100==    by 0x6CAC2D: SGObject_ref_unref_Test::TestBody() (in /home/liqiang/shogun/shogun/tests/unit/shogun-unit-test)\n==22100==    by 0x9072D9: void testing::internal::HandleExceptionsInMethodIfSupported<testing::Test, void>(testing::Test*, void (testing::Test::*)(), char const*) (in /home/liqiang/shogun/shogun/tests/unit/shogun-unit-test)\n==22100==    by 0x8F6358: testing::Test::Run() (in /home/liqiang/shogun/shogun/tests/unit/shogun-unit-test)\n==22100==    by 0x8F6436: testing::TestInfo::Run() (in /home/liqiang/shogun/shogun/tests/unit/shogun-unit-test)\n==22100==    by 0x8F6574: testing::TestCase::Run() (in /home/liqiang/shogun/shogun/tests/unit/shogun-unit-test)\n==22100==    by 0x8F6A0F: testing::internal::UnitTestImpl::RunAllTests() (in /home/liqiang/shogun/shogun/tests/unit/shogun-unit-test)\n==22100==    by 0x906E59: bool testing::internal::HandleExceptionsInMethodIfSupported<testing::internal::UnitTestImpl, bool>(testing::internal::UnitTestImpl*, bool (testing::internal::UnitTestImpl::*)(), char const*) (in /home/liqiang/shogun/shogun/tests/unit/shogun-unit-test)\n==22100==    by 0x8F5AEE: testing::UnitTest::Run() (in /home/liqiang/shogun/shogun/tests/unit/shogun-unit-test)\n==22100==    by 0x58E07E: main (in /home/liqiang/shogun/shogun/tests/unit/shogun-unit-test)\n==22100== \n==22100== 448 (328 direct, 120 indirect) bytes in 1 blocks are definitely lost in loss record 222 of 236\n==22100==    at 0x4C2CD7B: malloc (in /usr/lib/valgrind/vgpreload_memcheck-amd64-linux.so)\n==22100==    by 0x53FD4FB: operator new[](unsigned long) (in /home/liqiang/shogun/shogun/src/shogun/libshogun.so.16.1)\n==22100==    by 0x5406CDD: shogun::SGVector<double>* shogun::sg_generic_malloc<shogun::SGVector<double> >(unsigned long) (in /home/liqiang/shogun/shogun/src/shogun/libshogun.so.16.1)\n==22100==    by 0x584DFED: shogun::CLibSVMFile::get_sparse_matrix(shogun::SGSparseVector<double>*&, int&, int&, shogun::SGVector<double>*&, int&, bool) (in /home/liqiang/shogun/shogun/src/shogun/libshogun.so.16.1)\n==22100==    by 0x584F439: shogun::CLibSVMFile::get_sparse_matrix(shogun::SGSparseVector<double>*&, int&, int&, double*&, bool) (in /home/liqiang/shogun/shogun/src/shogun/libshogun.so.16.1)\n==22100==    by 0x533238D: shogun::SGSparseMatrix<double>::load_with_labels(shogun::CLibSVMFile*, bool) (in /home/liqiang/shogun/shogun/src/shogun/libshogun.so.16.1)\n==22100==    by 0x5A1A92: SGSparseMatrix_io_libsvm_Test::TestBody() (in /home/liqiang/shogun/shogun/tests/unit/shogun-unit-test)\n==22100==    by 0x9072D9: void testing::internal::HandleExceptionsInMethodIfSupported<testing::Test, void>(testing::Test*, void (testing::Test::*)(), char const*) (in /home/liqiang/shogun/shogun/tests/unit/shogun-unit-test)\n==22100==    by 0x8F6358: testing::Test::Run() (in /home/liqiang/shogun/shogun/tests/unit/shogun-unit-test)\n==22100==    by 0x8F6436: testing::TestInfo::Run() (in /home/liqiang/shogun/shogun/tests/unit/shogun-unit-test)\n==22100==    by 0x8F6574: testing::TestCase::Run() (in /home/liqiang/shogun/shogun/tests/unit/shogun-unit-test)\n==22100==    by 0x8F6A0F: testing::internal::UnitTestImpl::RunAllTests() (in /home/liqiang/shogun/shogun/tests/unit/shogun-unit-test)\n==22100== \n==22100== 2,015 (152 direct, 1,863 indirect) bytes in 1 blocks are definitely lost in loss record 234 of 236\n==22100==    at 0x4C2CD7B: malloc (in /usr/lib/valgrind/vgpreload_memcheck-amd64-linux.so)\n==22100==    by 0x53FD3EB: operator new(unsigned long) (in /home/liqiang/shogun/shogun/src/shogun/libshogun.so.16.1)\n==22100==    by 0x59BCCE8: shogun::CID3ClassifierTree::prune_tree_machine(shogun::CDenseFeatures<double>*, shogun::CMulticlassLabels*, shogun::CTreeMachineNode<shogun::id3TreeNodeData>*, double) (in /home/liqiang/shogun/shogun/src/shogun/libshogun.so.16.1)\n==22100==    by 0x59BCF0D: shogun::CID3ClassifierTree::prune_tree(shogun::CDenseFeatures<double>*, shogun::CMulticlassLabels*, double) (in /home/liqiang/shogun/shogun/src/shogun/libshogun.so.16.1)\n==22100==    by 0x7119EB: ID3ClassifierTree_tree_prune_Test::TestBody() (in /home/liqiang/shogun/shogun/tests/unit/shogun-unit-test)\n==22100==    by 0x9072D9: void testing::internal::HandleExceptionsInMethodIfSupported<testing::Test, void>(testing::Test*, void (testing::Test::*)(), char const*) (in /home/liqiang/shogun/shogun/tests/unit/shogun-unit-test)\n==22100==    by 0x8F6358: testing::Test::Run() (in /home/liqiang/shogun/shogun/tests/unit/shogun-unit-test)\n==22100==    by 0x8F6436: testing::TestInfo::Run() (in /home/liqiang/shogun/shogun/tests/unit/shogun-unit-test)\n==22100==    by 0x8F6574: testing::TestCase::Run() (in /home/liqiang/shogun/shogun/tests/unit/shogun-unit-test)\n==22100==    by 0x8F6A0F: testing::internal::UnitTestImpl::RunAllTests() (in /home/liqiang/shogun/shogun/tests/unit/shogun-unit-test)\n==22100==    by 0x906E59: bool testing::internal::HandleExceptionsInMethodIfSupported<testing::internal::UnitTestImpl, bool>(testing::internal::UnitTestImpl*, bool (testing::internal::UnitTestImpl::*)(), char const*) (in /home/liqiang/shogun/shogun/tests/unit/shogun-unit-test)\n==22100==    by 0x8F5AEE: testing::UnitTest::Run() (in /home/liqiang/shogun/shogun/tests/unit/shogun-unit-test)\n==22100== \n==22100== 4,568 (1,600 direct, 2,968 indirect) bytes in 10 blocks are definitely lost in loss record 235 of 236\n==22100==    at 0x4C2CD7B: malloc (in /usr/lib/valgrind/vgpreload_memcheck-amd64-linux.so)\n==22100==    by 0x53FD3EB: operator new(unsigned long) (in /home/liqiang/shogun/shogun/src/shogun/libshogun.so.16.1)\n==22100==    by 0x6C9246: stress_test(void*) (in /home/liqiang/shogun/shogun/tests/unit/shogun-unit-test)\n==22100==    by 0x64BFF8D: start_thread (pthread_create.c:311)\n==22100==    by 0x71FCA0C: clone (clone.S:113)\n==22100== \n==22100== 11,607 (128 direct, 11,479 indirect) bytes in 1 blocks are definitely lost in loss record 236 of 236\n==22100==    at 0x4C2CD7B: malloc (in /usr/lib/valgrind/vgpreload_memcheck-amd64-linux.so)\n==22100==    by 0x53FD3EB: operator new(unsigned long) (in /home/liqiang/shogun/shogun/src/shogun/libshogun.so.16.1)\n==22100==    by 0x7119AE: ID3ClassifierTree_tree_prune_Test::TestBody() (in /home/liqiang/shogun/shogun/tests/unit/shogun-unit-test)\n==22100==    by 0x9072D9: void testing::internal::HandleExceptionsInMethodIfSupported<testing::Test, void>(testing::Test*, void (testing::Test::*)(), char const*) (in /home/liqiang/shogun/shogun/tests/unit/shogun-unit-test)\n==22100==    by 0x8F6358: testing::Test::Run() (in /home/liqiang/shogun/shogun/tests/unit/shogun-unit-test)\n==22100==    by 0x8F6436: testing::TestInfo::Run() (in /home/liqiang/shogun/shogun/tests/unit/shogun-unit-test)\n==22100==    by 0x8F6574: testing::TestCase::Run() (in /home/liqiang/shogun/shogun/tests/unit/shogun-unit-test)\n==22100==    by 0x8F6A0F: testing::internal::UnitTestImpl::RunAllTests() (in /home/liqiang/shogun/shogun/tests/unit/shogun-unit-test)\n==22100==    by 0x906E59: bool testing::internal::HandleExceptionsInMethodIfSupported<testing::internal::UnitTestImpl, bool>(testing::internal::UnitTestImpl*, bool (testing::internal::UnitTestImpl::*)(), char const*) (in /home/liqiang/shogun/shogun/tests/unit/shogun-unit-test)\n==22100==    by 0x8F5AEE: testing::UnitTest::Run() (in /home/liqiang/shogun/shogun/tests/unit/shogun-unit-test)\n==22100==    by 0x58E07E: main (in /home/liqiang/shogun/shogun/tests/unit/shogun-unit-test)\n. @iglesias Thanks.\n. ok\n. Is there any rules about using SG_MALLOC and new T[] ?\nMay be I will change it back to the new T[].\n. I see all unit test file have includes, and I think I will simplify the statement at the file head.\n. @vigsterkr well, I correct these latter.\nvery useful suggestion.\n. @emtiyaz \nThese parameters are not support in CGaussianKernel, the default value is 1 too.\nCGaussianKernel only has two parameter one for cache size and one for width.\nk({\\bf x},{\\bf x'})= exp(-\\frac{||{\\bf x}-{\\bf x'}||^2}{\\tau})\n\\tau is the width\nbut in matlab:\n% k(x^p,x^q) = sf^2 * exp(-(x^p - x^q)'*inv(P)*(x^p - x^q)/2)\u00a0\nsf  &  ell are the hyper parameters\nthe P matrix is ell^2 times\nthese two parameters are not define in shogun CGaussianKernel, and the equivalent form is to set them both 1.\n. @emtiyaz \nif (V_train.vlen==0 || V_test.vlen==0)\nmeans if there's no user records in train or test set, we should not add these the rmse.\nFor that case, if V_train.vlen==0 but V_test.len!=0 which means cold start user, I ignore these user too.\nI add these logical in your matlab code too.\n. @karlnapf  yes, I add the movielens data to the shogun-data, and change the link file above.\n. @karlnapf \nFor different user, the kernel matrix will be different size, it depends on the rating records count in the training set. So it's hard to pre-allocating the matrix.\n. @karlnapf  @emtiyaz \nok only CZeroMean and CGaussianLikelihood which not depends on kernel can pull out of the loop.\n. @karlnapf  yes\n. @karlnapf I do the same thing as the CDenseFeatures do.\n```\ntemplate CDenseFeatures::CDenseFeatures(const CDenseFeatures & orig) :\n        CDotFeatures(orig)\n{\n    init();\n    set_feature_matrix(orig.feature_matrix);\n    initialize_cache();\nif (orig.m_subset_stack != NULL)\n{\n    SG_UNREF(m_subset_stack);\n    m_subset_stack=new CSubsetStack(*orig.m_subset_stack);\n    SG_REF(m_subset_stack);\n}\n\n}\n```\n. @karlnapf If you set subset then the num_vectors will be change according to the size of the subset.\n. It just call the copy constructor, so return the copy of this instance.\n. I use clone method in copy constructor. Is that ok?\nOr I simply remove the copy constructor in IndexFeature and new a instance in duplicate method?\n. I have try, but EXPECT_EQ didn't work, so I still use EXPECT_NEAR.\n. @iglesias \nThanks a lot. Wait for @karlnapf response.\n. @karlnapf \nMy default python env is python2.7, and install python3.3 respectively. but I don't know how to install shogun for python3.3. So the integration test files for python3.3 don't generated.\n. @karlnapf \nok\nI solve it.\n. Where's the descriptions.txt ?\n. ",
    "kislayabhi": "@karlnapf , @vigsterkr  : I will get it done! Thanks for evaluating it. About the data, I sent a PR yesterday to the shogun-data with my dataset updated. Did I do something wrong there?\nAgain thanks. You have given me a direction to work on!! loving it :)\n. updated :) (http://nbviewer.ipython.org/gist/kislayabhi/9431770)\nPhew, it took me some time.\n Sorry for the delay\n. Exams over :)\nthe nbviewer link is as follows:\nhttp://nbviewer.ipython.org/gist/kislayabhi/9431770\nI have taken care of most of the issues that you pointed out leaving the Eigenspectrum one. I will try to include that in my next commit. \nFew doubts that came \n-  I checked the 2d data example again. For some reason the required eigenvector(the blue one) is given by the 2nd element only.\n-  In performing  test_proj = np.dot(E.T, test_f), you asked to work it using CPCA only. I managed to correct all the rest of it but this one is different. Here the test image is centralized by subtracting the mean that we get from the training images. I may be wrong though  !!\n. @tklein23 , hey. I am trying to load the data from a libsvm data file using the CLibSVMFile class. \nmy implementation is as follows:\nsay the data is : \n1 1:85.0 2:92.0 3:45.0\n-1 1:93.0 2:81.0 3:48.0\nmy implementation:\nClibSVMFile* svmfile = new CLibSVMFile(filename);\nfloat64_t* feat;\nint32_t num_feat=3;\nint_32 num_vec=2;\nsvmfile->get_matrix(feat, num_feat, num_vec);\nI thought this might load my data from the file into thefeat. What am I doing wrong and any other way to do this.\n. @karlnapf , I have sent a PR in shogun demo . Issue #35 \n. @iglesias, yeah! it really looks weird. Will see to that.\n. @vigsterkr  yeah. It's done.\n.  Yeah this has prolonged long enough. Hopefully the last commit is correct. it does passes one of the integration test. \n. @karlnapf . python tester.py classifier_lda_modular.py passes on this branch locally. Did you check my last commit. Is it to be like that?\n. Why the unit test gives me a Seg.fault in travis?. This one passes smoothly locally.\n. @karlnapf , See if this is okay for merging. The data files are here #65 in shogun-data.\n. @vigsterkr , I reproduced the error. Here is the gdb output of the 1st unit-test ( WeightedMajorityVote_unittest )that fails: \n```\nkislay@shek:~/gsoc/$ gdb WeightedMajorityVote_unittest\nGNU gdb (Ubuntu/Linaro 7.4-2012.04-0ubuntu2.1) 7.4-2012.04\nCopyright (C) 2012 Free Software Foundation, Inc.\nLicense GPLv3+: GNU GPL version 3 or later http://gnu.org/licenses/gpl.html\nThis is free software: you are free to change and redistribute it.\nThere is NO WARRANTY, to the extent permitted by law.  Type \"show copying\"\nand \"show warranty\" for details.\nThis GDB was configured as \"x86_64-linux-gnu\".\nFor bug reporting instructions, please see:\nhttp://bugs.launchpad.net/gdb-linaro/...\nReading symbols from /home/kislay/gsoc/WeightedMajorityVote_unittest...done.\n(gdb) run\nStarting program: /home/kislay/gsoc/WeightedMajorityVote_unittest\n[Thread debugging using libthread_db enabled]\nUsing host libthread_db library \"/lib/x86_64-linux-gnu/libthread_db.so.1\".\n[==========] Running 3 tests from 1 test case.\n[----------] Global test environment set-up.\n[----------] 3 tests from WeightedMajorityVote\n[ RUN      ] WeightedMajorityVote.combine_matrix\n[       OK ] WeightedMajorityVote.combine_matrix (0 ms)\n[ RUN      ] WeightedMajorityVote.binary_combine_vector\n[       OK ] WeightedMajorityVote.binary_combine_vector (0 ms)\n[ RUN      ] WeightedMajorityVote.multiclass_combine_vector\n[       OK ] WeightedMajorityVote.multiclass_combine_vector (0 ms)\n[----------] 3 tests from WeightedMajorityVote (1 ms total)\n[----------] Global test environment tear-down\n[==========] 3 tests from 1 test case ran. (1 ms total)\n[  PASSED  ] 3 tests.\nProgram received signal SIGSEGV, Segmentation fault.\n0x00007ffff659ebc3 in ?? () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6\n(gdb) backtrace\n0  0x00007ffff659ebc3 in ?? () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6\n1  0x00007ffff659ec13 in std::basic_string, std::allocator >::~basic_string() () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6\n2  0x00007ffff7b6d0a4 in std::vector >::~vector() ()\nfrom /usr/lib/libopencv_ts.so.2.4\n3  0x00007ffff5f65d1d in __cxa_finalize (d=0x7ffff6a49e10) at cxa_finalize.c:56\n4  0x00007ffff68182a6 in __do_global_dtors_aux () from /usr/lib/libgtest.so.0\n5  0x00007fffffffdbe0 in ?? ()\n6  0x00007fffffffdfc0 in ?? ()\n7  0x00007ffff683aa81 in _fini () from /usr/lib/libgtest.so.0\n8  0x00007fffffffdfc0 in ?? ()\n9  0x00007ffff7de992d in _dl_fini () at dl-fini.c:259\nBacktrace stopped: previous frame inner to this frame (corrupt stack?)\n(gdb)\n``\n. Hey! Can you guys give me some ideas here on using more Shogun. I can resume. (I am still young)\n. Hey @iglesias . I have completed the above mentioned. But how to contribute to a wiki sub-module. Can't I just fork it and push my changes there and send a pull request. That is what I think I did earlier for sub modules related to shogun-data \n. I have followed the pattern that you guys were following in the wiki sub-module. i.e\n- created a page namedOpenCV-integration-examplesand linked it in Home.md\n- In the pageOpenCV-integration-examples, I have just listed all the benchmarks and examples.\n. @iglesias  I had difficulty to get much out from this notebook in short duration, so please see if the abstracts structure is similar to others. \n. No. I have no intentions for it. I will remove thevirtual.\n. I see. This is inherently implemented in CBinaryLabels base class so will remove this .\n. So you mean to completely remove the AUTO_LDA part, right?\n. oops!\n. okay.\n. @pickle27 , That means that we need to just have 2 class.\n-CV2SG_Factory.\n-SG2CV_Factory`.\nEach one of them will comprise of 2 methods each. i.e\n- getDenseFeatures and getSGMatrix\n- getCVMat_from_dense_features and getCVMat_from_SG_matrix.\nI was thinking of another simpler way out where we completely leave the dense features part from the following. that is:\n- CV2SG_Factory with the getSGMatrix part only.\n  This is so because, once we have the cv::Mat converted into SGMatrix, It is too trivial to convert it into DenseFeatures which can be done by the user itself.\nWhat you suggest!?\n. But yours a bit cleaner as now I remember that we need to mostly use OpenCV's Highgui and this would be more straight forward.\n. Hey @pickle27 , I think this test covers all possible conversions.  What's left?. We have covered all the 64 conversions that are possible here.\n. I was thinking of iterating over the enums. But then the problem that may be there is that we won't be super clear about at what point the test breaks. (  I don't know if there's a way in google testing framework to modify the TEST Name for a specific iteration.). So for now sticking to what you have suggested here.\n. Sorry. I get it now. I made these changes very early. I will correct them.\n. These are not needed. Will amend them.\n. Some time back I read that we should not load pylab inline with notebooks. (http://nbviewer.ipython.org/github/Carreau/posts/blob/master/10-No-PyLab-Thanks.ipynb). So in this line I have specifically loaded the inline version of matplotlib\n. Yeah @iglesias . I think they are the same. Might be, I am doing something wrong. Let me recheck it then.\n. Yes. While writing, I felt the redundancy. What @karlnapf has suggested for making it via doxygen will help here?\n. ",
    "ahcorde": "Okey, In this case close this pull request and I am to synchronize with the autor and maybe we can add my stuff.\nI just update the source code,  I forgot to push the code in the repository.\nI apologise for the confusion \n. Hello Heiko,\nI have been looking the IPython notebook about eigenfaces of Kislayabhi. I think he has done a good job.\nI don't know if my example is going to be included in the source code of Shogun. Do you think I have to finish this task or maybe start another one?\nThanks.\n. Hey,\nsome days ago I push some new code. I don't know if someone has had the opportunity to review it.\nI have another pull request ready but I would like to close this first.\nRegards\n. Hi Heiko,\nThank you for your comments.  Now I use os.sep ('/') and os.pardir ('..') and I use the face data of shogun-data.\nI use OpenCV because the GSOC's idea consist in integrate Opencv and Shogun. I use this library to read the image, resize and visualize. Do you think is a better idea to use numpy?\nRegards\n. Here is the PCA implementation [1]. In Shogun is possible to calculate projection with the function apply_to_feature_vector or  apply_to_feature_matrix ( first subtract and then multiply).What I am doing in reconstruction is the opposite operation\"( first multiply and then add). Maybe I can add this functionality to PCA class. \nI have changed all the comments. Now I'm consistent :)\nI'm working in the develop branch. I think this is the latest Shogun version, isn't it?\n[1] http://www.shogun-toolbox.org/doc/en/3.0.0/PCA_8cpp_source.html\n. Yes, It was a path problem. I had a problem with a hyperlink in my computer. \n. I have changed the licence and I have cleaned the output\n. Hello Heiko\nI have tried to merge all commits into one, but I don't know if I have done it well\n. thank you @pickle27!\n. Hello Heiko,\nI have added a window for each thing. One for subset of data image, mean face and another one for reconstruction. \nThe size of the image could be changed with the global variable: IMAGE_WIDTH and IMAGE_HEIGHT. But I have added nearest interpolation.\nRegards\n. I'm going to try to use matplot to present the results. Because OpenCV highgui module is very simple\n. Hello Heiko,\nNow all the images are visible in one window. \nI have done something strange with the commit. I think I have removed the \"history\" of the previous commits.\n. Hello Heiko,\nNow all the images are visible in one window. \nI have done something strange with the commit. I think I have removed the \"history\" of the previous commits.\n. Thank you @iglesias!\n. Thanks for all the comments.\nI'm including and fixing all the suggestions. I hope this evening update a new version.\nI'm searching a new dataset for \"print numbers\" and I'm trying to improve the classification problem.\n. Hello\nI have added some of your comments. I have to continue working on it. I have changed the GIST[1]\n[1] http://nbviewer.ipython.org/gist/ahcorde/9944212\n. Hi Fernando,\nThanks you. I have to be more careful with the english grammar.\n. Hello Heiko,\nI don't know exactly what you refer. I have commited the sudoku image to shogun-data and then I have committed the ipython notebook. Am I missing any step? Can you explain me?\nThanks you.\n. Hello Heiko,\nI think is a great idea ;). Do you think \"restore_feature_vector\" is a good name?\n. Hello Heiko,\nI think is a great idea ;). Do you think \"restore_feature_vector\" is a good name?\n. when you say \"for a full feature instance\".You mean for a matrix?\n. Hi,\nthis afternoon I'm going to update. I have had a days off.\n. I removed the folder but I don't know how to recover the original folder.\n. ",
    "ouceduxzk": "Hello everyone, I would like to join this family\n. ",
    "yorkerlin": "@emtiyaz \nI have some question about your Matlab code.\nAccording to the Appendix provided by you,\nIn your Matlab code\npl = pdf(l')/sigma  ph=pdf(h')/sigma\ncl = cdf(l') - 0.5       ch = cdf(h') - 0.5\nex2 =  v.((l+m)._pl - (h+m)._ph) + (v+m.^2).(ch - cl); (in your Malab code)\nIt does not follow the equation 4 in the appendix\nIf my understand is correct,\nex2 should be \nex2 =  v.((l-m)._pl - (h-m)._ph) + (v+m.^2).(ch - cl); \nwhere m is mu, v is sigma^2\npdf is standard_normal_pdf, cdf is standard_normal_cdf\nCorrect me if I understand wrong\n. Another points confuses me is\ngm = gm + bsxfun(@times, c, pl-ph); (in your Matlab code)\nIf my understanding is correct, gm should be\ngm = gm + bsxfun(@times, c, bsxfun(@times, pl-ph, 1./sqrt(v));\nCorrect me if I am wrong\n. Yes, I note that.\nHowever, according to  the equation 5 in the appendix,\nit should be ( pdf(l') - pdf(h') )/ sigma^2 where\nl' = (l-mu)/sigma and h' = (h - mu) /sigma \nNote that pl = pdf(l')/sigma and ph=pdf(h')/sigma \n. I need to use the result generated by the Matlab code to do unit test.\nPlease correct me when you  investigated it.\n. Ok. I will follow the code.\n. done in https://github.com/shogun-toolbox/shogun/pull/2106 and https://github.com/shogun-toolbox/shogun/pull/2161\n. I include the Matlab code based on the original   infLaplace.m from the GPML package, which I used to generated result for the unit test of LaplacianInferenceMethodWithLBFGS.\nNote that the minfunc from http://www.di.ens.fr/~mschmidt/Software/minFunc.html does not exactly correspond to the build-in LBFGS provided at Shogun API. \n. Note that the LaplacianInferenceMethodWithLBFGS will pass the unit test given the relative tolerance=1e-2, while the LaplacianInferenceMethod does not pass the unit test given the same tolerance. The result used for the LaplacianInferenceMethodWithLBFGS unit test comes from the Matlab output  (format long)\nIt is possible that the result used for the  LaplacianInferenceMethod unit test has not enough precision if we use the default format (format short) at Matlab.\n. @karlnapf  My last commit locally passed all the unit tests. \nIf the last commit pass at travis-ci, please let me know so that I will change the coding style and the copyright.\n. I do the code clean up right now.\n. @emtiyaz  and @karlnapf \nCould I go further to extend this entrance task by implementing the KL method as a new inference method based on the idea of Nickisch and Rasmussen 2008 for GPs?\nReasons:\n(1) @pl8787 is working on another entrance task and I do not want to duplicate his work.\n(2) This entrance task is not hard to be extended to the KL  method and it closely related to the first scheduled method during the Gsoc period. \nLet me know your opinion.\n. @emtiyaz \nIn your code, you set the randseed is 1 and I think the mvnrnd and rand is use the same random seed.\nHowever, in Shogun if I use the provided class, I can not ensure the corresponding mvnrnd  and rand use the same random seed.\n. @emtiyaz \nSo, I assume we use the C style approach to write some example programs so that it is similar to the Matlab code.\nCorrect me if I am wrong. \n. @emtiyaz \n\"Second, if you compare my matlab code to your C++ code, there is perhaps a huge gap in readability. I don't expect C code to be as compact as Matlab, but it should more or less take very similar form.\" \nDo you mean the example program or the ElogLik class?\nFor the example program, I can do it for you.\nFor the ElogLik class, I do not think so since Shogun is  object-oriented and you asked us to efficiently write the math part from the appendix.\nBTW, for the ElogLik class, I have completed unit tests for it  just like other classes in the GP framework. \nIf you look at the unit test code for other classes in the GP framework,  you will find out most of them use one dataset to verify their result.\nOf course, I can do it for you if you are in doubt.\n. @emtiyaz\nCurrently the example code is a potential code for implementing the KLMethod.\nI will make some change so that the example program will be similar to your Matlab code.\nFor the KLMethod, I think once I have done for this entrance task, I will go on to implement the KLMethod. \nBTW, implementing this KLMethod is just for me to get familiar with the GP framework.\nHopefully, it will be a future feature of the GP framework beyond the Gsoc schedule. \n. @emtiyaz \nPlease see the code and let me know if you have any suggestion.\nI have tested using the following six datasets and you can generate or download from here:\nFor download \nhttps://github.com/yorkerlin/VariationalApproxExample/tree/master/variational\nFor generate\nhttps://github.com/yorkerlin/VariationalApproxExample/blob/master/example.m\nPlease save the dataset in shogun/data/variational directory\nI also print out the m vector.\nLet me know if you need to see the result of m.\nIn general, the relative error between each elements of m is less than 0.00001.\nThe result of these six datasets are:\nFor dataset 1\nnum_samples:1000 num_dimensions:20\nlogLik from Shogun =106.5732516380 from Matlab =106.5732512165\nFor dataset 2\nnum_samples:500 num_dimensions:10\nlogLik from Shogun =111.6166380205 from Matlab =111.6166377623\nFor dataset 3\nnum_samples:800 num_dimensions:40\nlogLik from Shogun =65.1112812045 from Matlab =65.1112808901\nFor dataset 4\nnum_samples:200 num_dimensions:100\nlogLik from Shogun =15.5989340083 from Matlab =15.5989338981\nFor dataset 5\nnum_samples:100 num_dimensions:5\nlogLik from Shogun =13.3364206234 from Matlab =13.3364205385\nFor dataset 6\nnum_samples:20 num_dimensions:5\nlogLik from Shogun =3.9788173074 from Matlab =3.9788172933\n. @emtiyaz \nYou mentioned to sure the code is correct and readable.\nI have added some comments, added unit tests and changed the example program so that it is similar to your Matlab code.\nNow, I have followed your suggestion.\nPlease let me know if you have further suggestion so that I can move on. \n. @karlnapf  Let me know once this is merged so that I can send another PR for the example program.\n. @karlnapf  Let me know your suggestion.\n. See here\nhttps://github.com/shogun-toolbox/shogun/issues/1971\nYou need to do the following two main tasks:\n(1) Write a function similar to ElogLik.m for logit likelihood\n(2) Interface the optimization in example.m using Shogun's LBFGS implementation.\nThe LogitPiecewiseBound class corresponds to the first part of the task.\nThis example program corresponds to the second part of the task.\n. Another point is about the matrix inverse.\nThe reason why we use L-BFGS instead of Newton method is to reduce the time complexity.\nNaively doing LU factorization will hurt the performance of L-BFGS.\n I will exploit the eigen3 API to reduce the time complexity of each L-BFGS's iteration.\n The key idea is to compute ONCE for the pivoting information. \n. The LogitPiecewiseBound class will be useful for later implementation.\nThe example program servers a starting point/practice for implementing variational method.\nMy previous implementation of the example program, https://github.com/shogun-toolbox/shogun/blob/53d675a81e789a8a1521295abfad83f8284d6787/examples/undocumented/libshogun/variational_approx_example.cpp, will be a reference code for implementing a variational method.\n. Honestly I do not know how to do unit test for an example code since I do not want to include useless code in the library.\nWhere should I document the intuition and big-picture?  Like a brief introduction doc to a Shogun class?\n. I have to include cstdio header file to use the C's snprintf function.\nLet me know the corresponding Shogun build-in function so that I can replace the snprintf function.\n. @karlnapf \nFixed the matrix inverse issue. see the following comparison.\nElapsed seconds for the dataset one (run five times) using matrix inverse is:21.976011\nElapsed seconds for the dataset one (run five times) using FullPiov(LU) is:19.028877\nElapsed seconds for the dataset one (run five times) using LDLT is::9.281486 (since sigma is always Positive definitive)  (best choice) (ref:http://eigen.tuxfamily.org/dox/group__TutorialLinearAlgebra.html)\n. So I need to rebase this branch against develop and send another PR, right? If so, I will close this PR first. Then I will send another PR with additional document.\n. I am testing  the c++ code for KL method and will soon post the\nnotebook link and PR for the c++ code.\nOn Monday, May 5, 2014, Heiko Strathmann notifications@github.com wrote:\n\nCould you post the notebook link?\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/shogun-toolbox/shogun/pull/2206#issuecomment-42252827\n.\n\n\nbest,\nwu lin\n. @karlnapf \ngot it.\nI will send a PR for the notebook and another PR for the Laplace method today.\n. Working on it\nOn Tuesday, October 28, 2014, Fernando Iglesias notifications@github.com\nwrote:\n\nCool, @yorkerlin https://github.com/yorkerlin. However, before adding\nnew stuff, it is more relevant to take care of the existing features. At\nleast, that is my opinion ;-)\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/shogun-toolbox/shogun/pull/2212#issuecomment-60726790\n.\n\n\nbest,\nwu lin\n. I will first clean up the existing GP codes in order to use the\nShogun matrix operations while implementing the FITC Laplace method for\nbinary classification.\n@karlnapf \nWill do refactor the SingleLaplace class and the SingleLaplaceWithBFGS tomorrow and clean up the MatrixOperations class. \n@lambday \nYes. GP uses a lot of features of Eigen3. I will try to use the Shogun's linear algebra classes when implementing new features instead of using Eigen3.  Do you have any suggestion to use the Shogun's linear algebra classes?\nOn Wednesday, October 29, 2014, jaster yorker yorkerjaster@gmail.com\nwrote:\n\nWorking on it\nOn Tuesday, October 28, 2014, Fernando Iglesias notifications@github.com\n<javascript:_e(%7B%7D,'cvml','notifications@github.com');> wrote:\n\nCool, @yorkerlin https://github.com/yorkerlin. However, before adding\nnew stuff, it is more relevant to take care of the existing features. At\nleast, that is my opinion ;-)\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/shogun-toolbox/shogun/pull/2212#issuecomment-60726790\n.\n\n\nbest,\nwu lin\n\n\nbest,\nwu lin\n. @karlnapf\nCould u take a look at my design (both variational likelihood and integration) and give me some feedback so that I can update the kl method which depends on these likelihood model?\n. @karlnapf \nI am working on writing the document since it is scheduled to write some python notebooks and demonstrate the API according to the Gsoc plan.  Right?\nI implemented and refoctored the first KL method using the provided code and the second method based on the Cholesky factorization. \nThe base KL inference method class will be sent Moday.\nThen the first and second sub-class KL method will be sent separately once the base class is merged.\nThe third KL method using variational diagonal co-variance matrix will be implemented if time allowed, which is NOT \"exact\" inference compared to the previous KL methods.\nNote that the first KL method is kind of slow due to time complexity O(n^3) of each iteration of L-BFGS's update.\nSince the time complexity of each iteration of L-BFGS's update is O(n^2), the second KL method is fast  although the whole time complexity in general  is still O(n^3).\nFor the third KL method,  the time complexity of each iteration of L-BFGS's update is O(n), although the whole time complexity in general is still O(n^3).\nLet me know your suggestion.\n. @karlnapf  and @tklein23 \nTake a look at this.\nIt seems there is still some error reported by valgrind\nThe command I use is\nvalgrind --tool=memcheck --leak-check=full --track-origins=yes --num-callers=50 --error-exitcode=1 ./examples/undocumented/libshogun/variational_approx_example\nIt seems this error comes from Eigen3\n==32016== \n==32016== HEAP SUMMARY:\n==32016==     in use at exit: 2,616 bytes in 5 blocks\n==32016==   total heap usage: 91,332 allocs, 91,327 frees, 249,624,074 bytes allocated\n==32016== \n==32016== 704 bytes in 2 blocks are possibly lost in loss record 3 of 4\n==32016==    at 0x4A06B6F: calloc (vg_replace_malloc.c:593)\n==32016==    by 0x3CF2411A28: _dl_allocate_tls (dl-tls.c:296)\n==32016==    by 0x3CF2C088BC: pthread_create@@GLIBC_2.2.5 (allocatestack.c:570)\n==32016==    by 0x3CFB808CDD: ??? (in /usr/lib64/libgomp.so.1.0.0)\n==32016==    by 0x415AB9: void Eigen::internal::parallelize_gemm<true, Eigen::internal::gemm_functor<double, long, Eigen::internal::general_matrix_matrix_product<long, double, 0, false, double, 1, false, 0>, Eigen::Map<Eigen::Matrix<double, -1, -1, 0, -1, -1>, 0, Eigen::Stride<0, 0> >, Eigen::Transpose<Eigen::Map<Eigen::Matrix<double, -1, -1, 0, -1, -1>, 0, Eigen::Stride<0, 0> > const>, Eigen::Matrix<double, -1, -1, 0, -1, -1>, Eigen::internal::gemm_blocking_space<0, double, double, -1, -1, -1, 1, false> >, long>(Eigen::internal::gemm_functor<double, long, Eigen::internal::general_matrix_matrix_product<long, double, 0, false, double, 1, false, 0>, Eigen::Map<Eigen::Matrix<double, -1, -1, 0, -1, -1>, 0, Eigen::Stride<0, 0> >, Eigen::Transpose<Eigen::Map<Eigen::Matrix<double, -1, -1, 0, -1, -1>, 0, Eigen::Stride<0, 0> > const>, Eigen::Matrix<double, -1, -1, 0, -1, -1>, Eigen::internal::gemm_blocking_space<0, double, double, -1, -1, -1, 1, false> > const&, long, long, bool) (in /home/yorker/gsoc/project/shogun/build/examples/undocumented/libshogun/variational_approx_example)\n==32016==    by 0x40788A: run(char const*, char const*, char const*, char const*, char const*) (in /home/yorker/gsoc/project/shogun/build/examples/undocumented/libshogun/variational_approx_example)\n==32016==    by 0x40AE2B: test_datasets() (in /home/yorker/gsoc/project/shogun/build/examples/undocumented/libshogun/variational_approx_example)\n==32016==    by 0x40274D: main (in /home/yorker/gsoc/project/shogun/build/examples/undocumented/libshogun/variational_approx_example)\n==32016== \n==32016== LEAK SUMMARY:\n==32016==    definitely lost: 0 bytes in 0 blocks\n==32016==    indirectly lost: 0 bytes in 0 blocks\n==32016==      possibly lost: 704 bytes in 2 blocks\n==32016==    still reachable: 1,912 bytes in 3 blocks\n==32016==         suppressed: 0 bytes in 0 blocks\n==32016== Reachable blocks (those to which a pointer was found) are not shown.\n==32016== To see them, rerun with: --leak-check=full --show-reachable=yes\n==32016== \n==32016== For counts of detected and suppressed errors, rerun with: -v\n==32016== ERROR SUMMARY: 1 errors from 1 contexts (suppressed: 2 from 2)\n. @karlnapf \nPlease restart travis again.\n. @karlnapf \nPlease restart travis again.\n. @karlnapf  Travis is green now.\n. @karlnapf \nLet me know once it is merged so that I can send the  KL methods.\n. @karlnapf \nTake a look at this.\nIf everything is fine, I will work on the fourth KL method. \n. @karlnapf \nI update some lines of code in order to pass the json test.\nLet's see whether it works or not.\n. @karlnapf \nplease restart it\n. Travis is green now.\n. @karlnapf \nLet me know once it is merged so that I can send another PR\n. @karlnapf \nTake a look at this.\n. @karlnapf \nit seems the bug in CExactInferenceMethod::update_cov() will change the result in  CExactInferenceMethod::get_posterior_covariance()\nCExactInferenceMethod::get_posterior_covariance() is ONLY called by\nCInferenceMethod::get_marginal_likelihood_estimate(\n        int32_t num_importance_samples, float64_t ridge_size)\nHowever, the unit tests of CExactInferenceMethod do not include the test of this method.\n. @vigsterkr \nsee this https://github.com/shogun-toolbox/shogun/pull/2409\n. I will pay full attention to the notebook in the following two weeks. \nthe Laplace wrapper will be implemented once the notebook is done\n. @karlnapf \nthis PR should solve the following issues.\n #1901 \n1900\n.  Since CGaussianProcessBinaryClassification is extended to deal with multi class, I rename it to CGaussianProcessClassification \n. Note that CGaussianProcessClassification is generic.\nHowever, CGaussianProcessMachine for multi-class is NOT generic.\nSince I did not use  multinomial probit before, the current implementation in  CGaussianProcessMachine for multi-class works ONLY for soft-max.\nThe MultiLaplaceInference also works ONLY for soft-max\n. @karlnapf \ntake a look at this. \nLinearARDKerenel and GaussianARDKerenel are completely re-written.\n. @karlnapf \nLinearARDKerenel and GaussianARDKerenel support three version of ARD kernels, which you mentioned before.\nI use the Shogun's linear algebra lib in the implementation of these kernels.\nSince I want to support LinearARDKerenel, I do not use the GaussianKerenel as the base class for  GaussianARDKerenel.\n. @karlnapf \nIt seems that some Travis's tasks failed due to missing the linalg lib. Do you know how to fix this issue?\nOne error is\n /home/travis/build/shogun-toolbox/shogun/src/shogun/kernel/LinearARDKernel.cpp:86:4: error: \u2018linalg\u2019 has not been declared\n. @karlnapf \nI included the linalg in LinearARDKernel.h.\nI will remove the header file in LinearARDKernel.h and add it in LinearARDKernel.cpp to see whether travis complains or not.\n. @karlnapf \nThere may be another the issue in if in different situations. \nLet me know if it is an issue and I will fix it\n. @karlnapf  or @iglesias \nplease restart the failed travis' tasks\n. @karlnapf \nthere are three layers in the class hierarchy for fitc inference.\nThis class is in the first layer, which is used for general fitc inference. (regression, binary classification and multi-classification). \nFor inference methods for multi-classification (including Laplace method) , currently the design only focuses on the single kernel scheme, which means each class uses the same kernel. (this base class for fitc inference is not implemented since fitc may not be roust for multi-classification )\nOnce this class is merged, I will send a base class for single fitc inference (regression and binary classification)\n. @karlnapf \nI am writing the theory part for the new notebook. Only the final results of the derivations are include in the notebook.\nWill update this PR tonight. \n. For the  \"lock\" approach, this PR should be final\nOn Monday, March 16, 2015, Heiko Strathmann notifications@github.com\nwrote:\n\nI saw you sent a patch, is this the final one? Travis was ok except for\nsome other errors\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/shogun-toolbox/shogun/pull/2767#issuecomment-81966384\n.\n\n\nbest,\nwu lin\n. @karlnapf \nPlease fix the bug in version.cpp and restart travis since there is a unrelated error in version.cpp\n. @karlnapf  @iglesias \nwhy  TParameter *  param ->m_datatype.get_num_elements() is not a const method ?\n. Completed the implementation of the first method at  https://github.com/shogun-toolbox/shogun/issues/2779\n. Travis is done but the flag is still yellow.\n@karlnapf\n. @karlnapf \ntake a look at this.\n. this PR implements the second method  at https://github.com/shogun-toolbox/shogun/issues/2779\n. @karlnapf \nlet me know whether this PR is merge-able \n. @karlnapf @lambday \nLet's first test the linalg using the FITC inference.\nIn my opinion, at least for GP modules  without any backend specified, eigen3 should be the default backend unless eigen3 is not available.\nFor kernel modules (eg, GaussianARD kernel), ideally, we may want the following things \n- developers use global linalg methods without any backend specified\n- users pass a parameter flag to the global methods so that users can decide whether GPU is used or not in running time. (assuming  ViennaCL and eigen3 are available). \n- if any one of the libs (ViennaCL and eigen3) is missing, ignore the flag when the global methods are called. \nWhy running time, for kernels, the number of samples and the number of features are determined in running time.\nI am not sure whether the three things above can be achieved at the same time.\n. @lambday \nMaybe we can do some benchmarks to determine when GPU should be used.\nFor example, a graph is about the relationship between  GPU speedup and size of a square matrix, where y-axis is the speedup (ViennaCL vs eigen3) when GPU is used and x-axis is  the size of a matrix (assuming the matrix is square first) n for the following cases:\n- matrix-by-matrix product\n- matrix-by-vector product\n- matrix-inverse (base on LU?)\n- matrix decompositions (LU, QR, Chol, SVD, and others)\n- others supported operations\nWe may consider float64_t and float32_t are different cases if float32_t will be used.\nI think these benchmarks can help developers to determine when GPU should be used. \n. @lambday \nsee the updated comments.\nLooking forward to your benchmarks :) !\n. @karlnapf \nmatrix-inverse (base on LU?)\n. In the case of matrix-inverse-by-vector product or matrix-inverse-by-matrix product, matrix-inverse is forbidden.\nIf we need to compute the matrix-inverse alone, we may need to use LU? (eg, we use precision matrix as a permeameter instead of covariance matrix)\n. @karlnapf \nthe error is fixed now\n. @karlnapf \nplease merge this PR first.\nI am going to send two PRs about model selection followed up this PR\n. @karlnapf \ntake a look at the updated notebook.\nThe following things are added\n- Overview\n- Roadmap\n- plot and python code\n. http://nbviewer.ipython.org/gist/yorkerlin/e69926dbea6d47946d79\n. https://github.com/shogun-toolbox/shogun-data/pull/75\n. @karlnapf \nI have an issue to generate testsuite/python2-tests/  for\nexamples/undocumented/python_modular/mkl_regression_modular.py\n```\n!/usr/bin/env python\nfrom numpy import *\nparameter_list=[[20,100,6,10,0.5,1, 1], [20,100,6,10,0.5,1, 2]]\ndef mkl_regression_modular(n=100,n_test=100, \\\n        x_range=6,x_range_test=10,noise_var=0.5,width=1, seed=1):\nfrom modshogun import RegressionLabels, RealFeatures\nfrom modshogun import GaussianKernel, PolyKernel, CombinedKernel\nfrom modshogun import MKLRegression, SVRLight\n\n# reproducible results\nrandom.seed(seed)\n\n# easy regression data: one dimensional noisy sine wave\nn=15\nn_test=100\nx_range_test=10\nnoise_var=0.5;\nX=random.rand(1,n)*x_range\n\nX_test=array([[float(i)/n_test*x_range_test for i in range(n_test)]])\nY_test=X_test\nY=X+random.randn(n)*noise_var\n\n# shogun representation\nlabels=RegressionLabels(Y[0])\nfeats_train=RealFeatures(X)\nfeats_test=RealFeatures(X_test)\n\n# combined kernel\nkernel = CombinedKernel()\nkernel.append_kernel(GaussianKernel(10,0.1))\n#kernel.append_kernel(GaussianKernel(10,3))\n#kernel.append_kernel(PolyKernel(10,2))\nkernel.init(feats_train, feats_train)\n\n# constraint generator and MKLRegression\nsvr_constraints=SVRLight()\nsvr_mkl=MKLRegression(svr_constraints)\nsvr_mkl.set_kernel(kernel)\nsvr_mkl.set_labels(labels)\nsvr_mkl.train()\n\n# predictions\nkernel.init(feats_train, feats_test)\nout=svr_mkl.apply().get_labels()\n\nreturn out, svr_mkl, kernel\n\nif name=='main':\n    print('MKLRegression')\n    mkl_regression_modular(*parameter_list[0])\n``\n. If I run pythonmkl_regression_modular.pyorpython generator.py mkl_regression_modular.py, the result ofout` is\n[ nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan]\nHowever, if I run ctest -R  integration-python_modular-tester-mkl_regression_modular, the result of out is \n [ 0.11012167  0.16832948  0.23321433  0.30361707  0.3780979   0.45499184\n   0.53247986  0.60867088  0.68168855  0.74975591  0.81127137  0.86486966\n  0.90946325  0.94426115  0.9687649   0.98274338  0.98619065  0.97927277\n  0.9622706   0.93552598  0.89939807  0.85423533  0.80036654  0.73811175\n   0.66781151  0.58987032  0.50480826  0.41331348  0.31628804  0.21487977\n  0.11049461  0.00478553 -0.10038323 -0.2029952  -0.30096032 -0.39220729\n -0.47478236 -0.54694664 -0.60726419 -0.65467389 -0.68853974 -0.70867614\n  -0.71534695 -0.70923916 -0.69141408 -0.66324068 -0.62631632 -0.58238116\n  -0.53323221 -0.48064239 -0.42628919 -0.37169637 -0.31819054 -0.26687347\n  -0.21860958 -0.17402719 -0.13353134 -0.09732544 -0.06543924 -0.03776006\n  -0.01406511  0.00594727  0.02262901  0.03635741  0.04751449  0.05647092\n   0.06357425  0.0691411   0.07345276  0.07675364  0.0792518   0.08112103\n   0.08250397  0.08351574  0.08424779  0.08477164  0.08514242  0.08540201\n   0.08558181  0.08570501  0.08578852  0.08584454  0.08588172  0.08590614\n   0.08592201  0.08593222  0.08593872  0.08594282  0.08594537  0.08594695\n  0.08594792  0.0859485   0.08594886  0.08594907  0.08594919  0.08594926\n  0.0859493   0.08594933  0.08594934  0.08594935]\n. @karlnapf \nI think the result generated by ctest  is correct. Do you know how to get the ctest result by using python generator.py mkl_regression_modular.py\nI guess something wrong with the mkl modular for regression.  \nthe ctest and python generator.py generate the same results for mkl_multiclass_modular and mkl_binclass_modular. However, they get the different results for mkl_regression_modular.\nAny idea?\n. ```\nclass CMyClassInstanceFirstOrderCostFunction;\nclass CMyClassInstance(): public CMyClass\n{\nfriend class CMyClassInstanceFirstOrderCostFunction;\npublic:\n    CMyClassInstance();\n    virtual void set_minimizer(MinimizerFlag flag)\n    {\n        switch (flag)\n        {\n        //enum type\n        case GradientDescent: //FirstOrderMinimizer\n            CFirstOrderCostFunction cost=new CMyClassInstanceFirstOrderCostFunction(this);\n            //CFirstOrderMinimizer will delete cost\n            CFirstOrderMinimizer opt=new GradientDescent(cost);\n            //CMyClass will delete opt\n            register_minimizer(opt);\n            break;\n        //case Newton: //SecondOrderMinimizer\n            //break;\n        }\n    }\nprivate:\n    float64_t get_cost();\n    SGVector get_variable();\n    SGVector get_gradient();\n};\nclass CMyClassInstanceFirstOrderCostFunction: public CFirstOrderCostFunction\n{\npublic:\n    CMyClassInstanceFirstOrderCostFunction(CMyClassInstance *obj)\n    {\n        m_obj=obj;\n        ASSERT(m_obj); //REQUIRE\n    }\n    virtual float64_t get_function_value()\n    {\n        return m_obj->get_cost();\n    }\n    virtual SGVector get_variable_value()\n    {\n    return m_obj->get_variable();\n}\nvirtual SGVector<float64_t> get_variable_gradient()\n{\n    return m_obj->get_gradient();\n}\n\nprivate:\n    //do not delete the pointer\n    CMyClassInstance* m_obj;\n}\n``\n. @karlnapf \nany comment about this.\nif you agree such design, they will be used  in GP.\n. which classes should useCSGObjectas the base class?\n. @karlnapf \nsome useful minimizers (unconstrained cases)\nsee sectionOptimizer overview` in http://climin.readthedocs.org/en/latest/\n. Such design (the cost function part) is to implement the java interface in c++\nUsage:\n1. Laplace approximation using newton method or lbfgs (need refactoring)\n2. (Main usage: online/streaming inference) Stochastic gradient update schemes (gradient descend, gradient descend with Nesterov momentum, gradient update with annealing\ntechniques, gradient update with variance reduction techniques)\nOn Friday, June 12, 2015, Heiko Strathmann notifications@github.com wrote:\n\nHi\nso see what you are after here is to have a way to exchange optimization\nmethods in Shogun classes.\nLooks cool and elegant. So +1 in general. My questions are about\nnecessarity\n- What does this gain us in general?\n- Most models in Shogun are fixed, and so are many inference\n  algorithms. Is there really the option to change optimisers under the hood?\n  Is that something the user should decide?\n- In GPs, we use all sorts of opt. methods, I guess you want to put\n  the optimiser in the base class?\n- Then the question is: what is the gain as compared to doing this in\n  the subclasses directly\n- Can you maybe come up with an example where this is used?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/shogun-toolbox/shogun/issues/2853#issuecomment-111411985\n.\n\n\nbest,\nwu lin\n. Any feedback on the class design?\nIf you think so, the streaming/online GP will use the design.\nFor stochastic gradients, the key components are\n1. subset selection to create a mini batch  (scheduler)\n2. Gradient update schemes (minimizer)\nMy idea is to use the design so that users or developers only need\nto change schedulers and minimizers to implement new inference methods\nbased on stochastic gradients in the future.\nOn Friday, June 12, 2015, yorker lin yorkerjaster@gmail.com wrote:\n\nSuch design (the cost function part) is to keep the base\nUsage:\n1. Laplace method using newton method or lbfgs (refactor)\n2. Stochastic gradient update schemes (gradient descend, gradient\ndescend with nesterov momentum, gradient update with annealing\ntechniques, gradient update with variance reduction techniques)\nOn Friday, June 12, 2015, Heiko Strathmann notifications@github.com\n<javascript:_e(%7B%7D,'cvml','notifications@github.com');> wrote:\n\nHi\nso see what you are after here is to have a way to exchange optimization\nmethods in Shogun classes.\nLooks cool and elegant. So +1 in general. My questions are about\nnecessarity\n- What does this gain us in general?\n- Most models in Shogun are fixed, and so are many inference\n  algorithms. Is there really the option to change optimisers under the hood?\n  Is that something the user should decide?\n- In GPs, we use all sorts of opt. methods, I guess you want to put\n  the optimiser in the base class?\n- Then the question is: what is the gain as compared to doing this in\n  the subclasses directly\n- Can you maybe come up with an example where this is used?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/shogun-toolbox/shogun/issues/2853#issuecomment-111411985\n.\n\n\nbest,\nwu lin\n\n\nbest,\nwu lin\n. CMap does not support serialization. However, CMap is a subclass of SGObject.\nAt least I want CStringMap (that is CMap<std::string, SGVector<float64_t> >) is serialable. \nref\nhttps://github.com/shogun-toolbox/shogun/issues/2903\n. The error is not related to this PR.\nI merge this.\n. The error is not related to this PR.\nMerge\n. @nachitoys \nCould you tell me which version of shogun you used in your experiments  (release version or developer version)\nWe moved the internal representation of width in log domain in developer version 4.1. I am not sure whether the change affects MKL.\nIf you use a developer version later than May 6, 2015, could you try MKL using an older version? \nIf you get the different result using an older version, please report it.\nIf you use a developer version earlier than May 5, 2015 and get the same result, you may consider the following reason:\n- The width used in Shogun may be different from the standard width (see wikipedia).\nFor example,\n\\sigma^2  (standard width)\n2 \\times \\sigma^2 (shogun's width)\nreference\nhttp://www.shogun-toolbox.org/doc/en/latest/classshogun_1_1CGaussianKernel.html\nhttps://en.wikipedia.org/wiki/Radial_basis_function_kernel\n. Since I only changed comments, I merge this.\n. @karlnapf \n@lisitsyn \n@iglesias\nany suggestion?\n. @erip \nPlease try not to do the open and close operations.\nWe will receive an email every time you (re-)open or close this PR.\nTravis will fail due to the memory issue.\nYou can use the following trick:\nFor example,\n- In the first commit, all tests are passed except test 1 and test 2  are failed.\n- then do the git force update trick without changing the codes\n- In the second commit, if test 1 and test 2 are passed, you may consider all tests  are passed as long as errors reported by Travis are not related to this PR. \nthe git force update trick is:\ngit reset --soft \"HEAD^\"\ngit commit -m \"your comments\" (the `hash id` will be changed)\ngit push origin +your_branch (force to update your branch even with no change in codes)\n. BTW, @lisitsyn \nIt seems currently test 3 always fails due to the memory issue.\ntest 12 also fails all the time.\n. @lambday \n@lisitsyn \n@vigsterkr \nIt seems CMap (including CStringMap) is not serializable.  \nFrom DataType\nenum EContainerType\n{\n    CT_SCALAR=0,\n    CT_VECTOR=1,\n    CT_MATRIX=2,\n    CT_NDARRAY=3,\n    CT_SGVECTOR=4,\n    CT_SGMATRIX=5,\n    CT_UNDEFINED=6\n};\n. @lisitsyn \nSince CMap and CStringMap do not support serialization, I disable  serialization for MinimizerContext.\n. @lambday @lisitsyn \nCurrently, most of classes in the minimizer framework are not subclasses of SGObject.\nIf in future we allow all these classes in the minimizer framework become  subclasses of SGObject,\nthe serialization issue will be solved.\nIf not, I have to use CStringMap (a subclass of CMap) for serialization. \nIn that case, do you guys plan to support serialization for CMap?\n. Errors are not related to this PR.\nI merge  this.\n. @lisitsyn \nCMap (including CStringMap) does not support serialization. \n. The issue is solved.\nI close this\n. @iglesias \nLet me know once you think it is good to merge\n. errors are not related to this PR.\nI merge this PR.\n. Errors are not related to this PR.\nI merge this.\n. it seems the renaming will cause a warning.\nI revert this this PR.\n. it seems errors are not related to this PR.\n@arasuarun  thanks! \n. I merge this since only docs are changed.\n. Yes. Let's do benchmarks against sklearn, gpy, and GPflow under the\nframework.\n. I am not sure whether this change (O(n)) will improve the performance or not.\nI will do a test to estimate the efficiency of the change I made. For example, run the same line 100 times and compute the average elapsed  time. \n. I will add it later.\n. This code is based on the original LaplacianInference class.\nMaybe, I will refactor the original class later once I pass the unit test. \n. Got it\n. It seems that c++ compilers do optimise those things quite efficiently.\nI agree with you that we use the original code.\nHowever, my implementation explicitly optimizes these cases.\n. @karlnapf \nI have added the  references and implementation references. Please see the code.\n. I have fixed the indentation and let me know if it still has. \n. Ref:http://www.shogun-toolbox.org/doc/en/current/structshogun_1_1lbfgs__parameter__t.html\nIn the document it says\nint max_linesearch\nThe maximum number of trials for the line search. This parameter controls the number of function and gradients evaluations per iteration for the line search routine. The default value is 20. \nfloat64_t delta\nDelta for convergence test. This parameter determines the minimum rate of decrease of the objective function. The library stops iterations when the following condition is met: (f' - f) / f < delta, where f' is the objective value of past iterations ago, and f is the objective value of the current iteration. The default value is 0. \n. I just try to make the code and the document consistent \n. According to the l-bfgs implementation, orthantwise_end  should be bigger than orthantwise_start. So I change it from -1 to 1.\nBecause orthantwise_start is 0.\n. Of course, I can change the document. \nmax_linesearch from 20 to 40. (can keep the code unchanged by changing the document)\ndelta from 0 to 1e-5.  (can keep the code unchanged by changing the document)\ndelta value be will essential if past > 0(BTW, the default value of past is 0)\nHowever, orthantwise_end  should be > 0. (which means the code has to be changed) for this value.\n. In fact, it means I will complete this part today.\n. In fact, it means I will complete this part today.\n. I will do it once all methods in the class are completed.\n. Same here\nI will add it once all methods in the class are completed.\nMaybe, Wednesday. \n. This is closely related to the Math part I am writing.\nIf I complete two methods today, I will add the Math part tomorrow or tonight.\n. Actually I plan to add another enum value, called LT_LOGIT_PWB.\nAlso it closely related to the Math part.\n. avoid to copy the object?\n. Let me know if there are some constraints that we should pass a object rather than a const reference.\n. the enum _my_bsxfunOp is temporarily added for unit test. \n. Because SGVector s2 should be always positive, I use eigen's unaryExpr(function_pointer) to check each element in s2 should be positive. Of course, I can use a for-loop to check this. \n. I will think about it again.\nHowever, Since we can have different variational bounds (classes) for one likelihood (for example, logit), C++ users will use these classes when they want to use variational inference for GP.\nAnother way to do implement is to make the specific likelihood as a data member of the variational bound class and public some methods in the likelihood.\nDo you have any suggestion?\n. done.\nI have add const for all vector from get_column_vector.\nShould I need to comment them?\n. done\n. Good point!\n. I am totally fine with that.\nHowever, without this function, some implementation will be redundant.\nLet me know if you think we should directly use eigen3 instead of calling this function.\n. No, the CDF is included in CStatistics but PDF is NOT if this, http://www.shogun-toolbox.org/doc/en/current/classshogun_1_1CStatistics.html, is what you are saying.\n. So I assume you want me to use for-loop on SGVector instead of Eigen's unaryExpr.\n. Let me know if you think we should use a for-loop on SGVector.\n. What is more, I need to apply the transformation to all elements on a SGVector.\nLet me know if you think we should directly use for-loop instead of eigen's unaryExpr with function pointer.\n. same situation of  standard_norm_pdf.\nLet me know your suggestion so that I can make the change and continue to work on implementing the KL method.\nBTW, it turns out I will implement the KL-method of Nickisch and Rasmussen 2008 for GPs using the piecewise bound provided by Emti during this week and next week, although I think this is beyond the entrance task.\nDo u have any suggestion about my plan?\n. I am still waiting for your comments for the function pointer part.\n. I now submit a version which uses function pointers\n. @karlnapf \nCurrently my plan is to implement the ElogLik method this week.\nIn fact, I am almost done and I will soon submit a clean code.\nAnd next week I will focus on the L-BFGS method mentioned in the #1971 and use this bound to do inference following the idea of the KL-method of Nickisch and Rasmussen 2008 for GPs. \nBTW, the original method of Nickisch and Rasmussen uses Newton method without any variational bound and the time complexity is quite high.\nIt turns out that the implementation is similar to my L-BFGS method for Laplacian.\nThe reason why I want to implement a complete method is that I can do complete unit test and do some observation.\nWhat is more, pl8787 is working on another entrance task that I do not want to duplicate his work. I think extending this entrance task to a complete one may be a good choice.\n@karlnapf  and @emtiyaz \nWhat do you think?\n. done\n. remove it\ndone\n. done\n. done for the _norm_cdf_minus_const\n. I will change this later when I am writing the code for the KL method next week so that I can think this thoroughly.\n. Eigen3 will complain this if I change it to  rowwise()\n. done\n. param only can be \"mu\" or \"sigma2\" because this is a normal distribution, which can be specific by mu and sigma2.\nHere, we take the derivative wrt to mu or sigma2\n. added some comment.\ndone\n. done\n. done\n. done\n. done\n. done\n. For the LogitPiecewiseBound class, the distribution is Gaussian.\nFor other variational methods, I am not so sure.\n. I will submit it  later once all changes are made based on your suggestion.\n. done\n. The matlab code is used 0 and 1 as labels therefore the implementation internally uses there labels.\nSince the following code \n SGVector m_lab;\nand \nm_lab = ((CBinaryLabels*)lab)->get_labels();\nare passed by value, I think such transformation ONLY affects internally the class.\n. done\n. done\n. done\n. done\n. done\n. The meaning of vector inverse() is\n[2 2 2].inverse() == [1/2 1/2 1/2]\n. v is defined in Matlab code. \nIn my code it is called s2.\nsee here\n    for(index_t i = 0; i < s2.vlen; ++i)\n        ASSERT(s2[i] > 0.0);\n. done\n. done\n. done\n. done\n. done\n. eigen_l is a column vector (VectorxXd)\n. Usually we use pdf in log-domain and the log_sum trick to avoid numerical underflow  in particular for  IID samples.\nIn this class, I just follow the Matlab code and it is impossible to deal with 0 in log domain.\n. @karlnapf \nDo you think which one is better\n    eigen_l2_plus_s2 = (eigen_s2.replicate(1,eigen_l.rows()).array().transpose().colwise() + eigen_l.array().pow(2)).matrix();\nor\n    eigen_l2_plus_s2 = (eigen_s2.replicate(1,eigen_l.rows()).array().rowwise() + eigen_l.array().transpose().pow(2)).transpose().matrix();\nBoth codes work! I choose the first one.\n Let me know if you think the second one is better.\n. BTW, eigen3 requires \nmatrix.rowwise()  OP vector\nor\nmatrix.colwise()   OP vector.\nYou cannot reverse the order such as  (at least currently I do not know how to do it)\nvector OP matrix.colwise()\nor\nvector OP matrix.rowwise()\n. The memory requirement for each matrix is n-by-m, \nwhere \nn is the size of sample, which is the size of mu or sigma2 \nm is the size of the pre-defined bound, which is num_rows of the bound. (In the Matlab code, m is 20) \n. done.\nadded some comment \n. see above.\n. I will send another PR for this.\n. done\n. done\n. In fact, there are some author discussed this. \nFor example, the second-order delta method and Laplace method in fact use the Gaussian distribution.\n. BTW, I can write scalable methods  by using block matrix. (chopping the mu and sigma2 vector into sub small vectors)\nOf course, this is a trade-off between time complexity and space complexity.\nLet me know if you think this is a good idea. Currently I just follow the Matlab code.\n. The LikelihoodModel.h is the base class.\nI have mentioned in the LogitPiecewiseBound class\n/** set the variational NORMAL distribution given data and parameters\n *\n * @param mu mean of the variational normal distribution\n * @param s2 variance of the variational normal distribution\n * @param lab labels/data used\n *\n */\nvirtual void set_distribution(SGVector<float64_t> mu, SGVector<float64_t> s2, const CLabels* lab);\n. These functions are used to test the correctness of this program.\nOnce you think this program is correct, I will remove all these  LOAD functions and use the commented code.\nTherefore, I do not think adding REQUIRE with error message is necessary.\nLet me know you suggestion.\n. I follow the Matlab code since Emti asked for writing similar C++ code based on his Matlab code.\nWhat is more, we need to multiply this matrix to a updated vector at each L-BFGS iteration.\nDoing naive LU factorization (time complexity O(n^3)) for each iteration is not a good idea compared with matrix-vector multiplication  (time complexity O(n^2)). \nMaybe there is one way to remember the matrix structure (pivoting)  when performing Gaussian elimination (LU) so that we can do LU factorization at each iteration.\n. In fact, loading these datasets is just for testing the correctness.\nOnce you think the program is correct, I will remove all these dataset.\n. I have to  include cstdio header file to use snprintf  function.\nLet me know the corresponding build-in function. \n. I have send a PR for shogun-data.\n. I guess if eigen3 is NOT installed, without the #else statement during the MAKE process there will be  a linking error.\nI removed the #else statement based on your previous suggestion.\nLet me know your suggestion if you think we should keep the  the #else statement.\n. it means we do not use any variational bound (piecewise bound) or local method to approximate the expectation.\nwe use directly numerical integration instead, although it still approximates the expectation.\nI do not know how to come up a good name for this class. maybe you can give me a good name. \n. @karlnapf \nthe methods adapted from quadgk.m is written by @votjakovr, which is used in CLogitLikelihood::get_log_zeroth_moments().\nI guess the implementation is a well-know algorithm. see cf: http://en.wikipedia.org/wiki/Gauss%E2%80%93Kronrod_quadrature_formula\n. @karlnapf and @iglesias \nLet me know your suggestion so that I can make some change for the licences.\n. @iglesias  and @karlnapf \nLet me know how to solve this issue so that we can get this PR merged.\n. In fact, variational Gaussian Likelihood and variational Likelihood can be merged into one class if we  use variational method ONLY in GP, which means the variational distribution must be Gaussian. \nIf so, we can merge these two class.\nIf we want to use other variational distribution(s) in the future, we may keep these two classes.\nLet me know your suggestion.\n. As we discussed before, you suggested not adding methods in the likelihood model class and creating a new interface class---variational likelihood.\nSince \n- (1)we do not do multiple inheritance in Shogun\n- (2)some KL method(s) may use some methods in the original likelihood class\n- and (3) the interface of the Inferecne method base class, which only allows ONE CLikelihoodModel* as argument, is  \nC++\n    CInferenceMethod(CKernel* kernel, CFeatures* features, \nCMeanFunction* mean, CLabels* labels, CLikelihoodModel* model);\nI have to use these wrappers.\nLet me know if you have a better idea for this.\n. see the above comment.\nLet me know your suggestion.\n. get_first_derivative_wrt_hyperparameter\nonly available for student's T  (df and sigma. Note that sigma here is defined in student's T)\nlogit and probit do not have hyper-parameters\n. variational parameters (mu and sigma) in my implementation are m_mu and m_s2\n. I think it is listed in the beginning \n* Code adapted from \n * http://hannes.nickisch.org/code/approxXX.tar.gz\n * and Gaussian Process Machine Learning Toolbox\n * http://www.gaussianprocess.org/gpml/code/matlab/doc/\n * and the reference paper is\n * Nickisch, Hannes, and Carl Edward Rasmussen.\n * \"Approximations for Binary Gaussian Process Classification.\"\n * Journal of Machine Learning Research 9.10 (2008).\n. The reference result is based on GPML 3.4\n. Returning true when NULL is to fix the error happened before.\nNote the If new KLInferenceMethod() without any argument, the default m_model==NULL, in this case the function should return true, otherwise the error will happen again.\nNote that KLInferenceMethod() without any argument and KLInferenceMethod(arg1, ...) with arguments will call init().\nMaybe I can design another way to solve this issue. \nHowever, all implemented Inference (eg: EP, Laplacian) methods have the similar issue if CInference() without any argument is called because m_model is set to NULL.\n. kernel should be positive definite.\nIf not, one trick is adding a K+lambda*eye(n) to avoid some numerical issues, where lambda is a small scalar.\nWhat do you think?\n. the first KL method\nhttps://github.com/yorkerlin/approxKLVB/blob/master/approxLogKLWithLBFGS.m\nthe second one\nhttps://github.com/yorkerlin/approxKLVB/blob/master/approxCholeskyWithLBFGS.m\nthe third one\nhttps://github.com/yorkerlin/approxKLVB/blob/master/approxDiagonalWithLBFGS.m\n. I will clean-up the Matlab code.\n. Some sample code\n``` Matlab\nclear all; close all;\nx1=[0.8822936\n-0.7160792\n0.9178174\n-0.0135544\n-0.5275911];\nx2=[-0.9597321\n0.0231289\n0.8284935\n0.0023812\n-0.7218931];\nx=[x1 x2];\ny=[1\n-1\n1\n-1\n-1];\nhyp.cov = log([2; 2]);\nhyp.lik=[];\ncov = {'covSEiso'};\nlik = {@likLogistic}\n[alpha, sW, L, nlZ, dnlZ] = approxCholeskyWithLBFGS(hyp, cov, lik, x, y)\n [alpha, sW, L, nlZ, dnlZ] = approxLogKLWithLBFGS(hyp, cov, lik, x, y)\n [alpha, sW, L, nlZ, dnlZ] = approxDiagonalWithLBFGS(hyp, cov, lik, x, y)\n```\n. Result from the first KL method\nK =\n4.0000    2.5759    2.6816    3.2229    3.0980\n2.5759    4.0000    2.6419    3.7605    3.7153\n2.6816    2.6419    4.0000    3.2955    2.2812\n3.2229    3.7605    3.2955    4.0000    3.6244\n3.0980    3.7153    2.2812    3.6244    4.0000\nalpha =\n    0.4610\n   -0.2721\n    0.4321\n   -0.3693\n   -0.2929\nsW =\n0.4362\n0.4009\n0.4315\n0.4384\n0.4093\nL =\n1.3271    0.3395    0.3804    0.4645    0.4168\n     0    1.2360    0.2653    0.4072    0.3788\n     0         0    1.2368    0.2739    0.1163\n     0         0         0    1.1456    0.2363\n     0         0         0         0    1.1329\nnlZ =\n3.6869\ndnlZ =\n0.6462\n0.0924\n. Result from the second KL method (note that sW, W, L are different in this Method)\nalpha =\n    0.4610\n   -0.2721\n    0.4321\n   -0.3693\n   -0.2929\nsW =\n[]\nL =\n-0.1338    0.0103    0.0205    0.0260    0.0236\n    0.0103   -0.1263    0.0146    0.0313    0.0292\n    0.0205    0.0146   -0.1290    0.0305    0.0067\n    0.0260    0.0313    0.0305   -0.1528    0.0288\n    0.0236    0.0292    0.0067    0.0288   -0.1305\nnlZ =\n3.6869\ndnlZ =\n0.6462\n0.0924\n. result from the third KL method \nalpha =\n    0.4465\n   -0.2486\n    0.4151\n   -0.3619\n   -0.2760\nsW =\n[]\nL =\n    2.2548    5.9200    0.9113   -5.9762   -2.4383\n    5.9200   12.4309    2.5805  -14.6102   -4.5406\n    0.9113    2.5805    1.4335   -5.1419    0.8100\n   -5.9762  -14.6102   -5.1419   20.5124    2.5059\n   -2.4383   -4.5406    0.8100    2.5059    3.3635\nnlZ =\n5.9829\ndnlZ =\n    3.9600\n   -0.7521\n. see the reference result from GPML 3.4 and GPML 2.x\nhttps://github.com/yorkerlin/approxKLVB/blob/master/README.md\nNote that approxDiagonalWithLBFGS is a special case of approxCholeskyWithLBFGS.\nYou can find out the difference by comparing the implementation of approxDiagonalWithLBFGS\nhttps://github.com/yorkerlin/approxKLVB/blob/master/approxDiagonalWithLBFGS2.m (naive version)\nand\n approxCholeskyWithLBFGS\nhttps://github.com/yorkerlin/approxKLVB/blob/master/approxCholeskyWithLBFGS.m\n. agree!\n. It seems it is not so easy.\nsee \nhttp://onlinelibrary.wiley.com/doi/10.1002/nla.779/full\nand\nhttp://math.stackexchange.com/questions/18488/diagonal-of-an-inverse-of-a-sparse-matrix\ndiag(K\\diag(w)) is what I need. \nHowever, K\\diag(w) will lead to O(n^2) at each L-BFGS update since vector w will be changed at each time\nIf we want O(n) at each L-BFGS update, we may have to lose some accuracy by pre-computing K^{-1}.\nAny suggestion?\n. I will send a PR to correct this.\n. yes.\nCKLApproxDiagonalInferenceMethod::update_chol()\nCKLApproxDiagonalInferenceMethod::get_derivative_related_cov(MatrixXd eigen_dK)\nwill use it.\n. for example\n    eigen_InvK_Sigma=m_ldlt.solve(eigen_Sigma); in update_chol()\n. Note that the method is protected but NOT public.\n. this is defined in class. \nit cannot be set by user.\n. without this line it will work efficient but memory checker will complain!\n. we only store lambda in data member because alpha can be computed.\n. cf: CLogitDVGLikelihood\n``` c++\n    / get the upper bound for dual parameter (lambda)\n     \n     * @return the upper bound\n     /\n    virtual float64_t get_dual_upper_bound() const{return 1.0;};\n/** get the lower bound for dual parameter (lambda)\n *\n * @return the lower bound\n */\nvirtual float64_t get_dual_lower_bound() const{return 0.0;};\n\n/** whether the upper bound is strict\n *\n * @return true if the upper bound is strict\n */\nvirtual bool is_dual_upper_bound_strict() const {return true;};\n\n/** whether the lower bound is strict\n *\n * @return true if the lower bound is strict\n */\nvirtual bool is_dual_lower_bound_strict() const {return true;};\n\n``\n. Note thatCVariationalGaussianLikelihood* get_variational_likelihood()` is protected.\nLet me know if you think sgref is needed\n. In fact, this is a internal (private) cast.\nadded anyway\n. the reason is \nc++\neigen_Kernel_LsD.triangularView<Lower>()=Kernel_L*Kernel_D.array().sqrt().matrix().asDiagonal();\nthis line only init the lower Triangular part of the matrix. The memory checker will complain that it uses uninit-ed memory space.\n. this can be change.\nbut get_mu_dual_parameter() will work different from  get_variance_dual_parameter().\nmy implementation only stores lambda since alpha = lambda - data_label\nIf do so, user can change lambda get_variance_dual_parameter()  but cannot change alpha since alpha  get_mu_dual_parameter()  is computed based on lambda, which means there two methods work differently.\n. see above\n. cf:\n``` C++\nvoid CVariationalLikelihood::set_likelihood(CLikelihoodModel * lik)\n{\n    SG_UNREF(m_likelihood);\n    m_likelihood=lik;\n    SG_REF(m_likelihood);\n}\n``\n. not really.\nbut it passes other unit tests depend on it.\nI will add more later.\n. yes\n. CNumericalVGLikelihood is derived from CVariationalGaussianLikelihood.\nthe base class CVariationalGaussianLikelihood, will set the noise factor\n. only use for correcting the s2 inCVariationalGaussianLikelihood::set_variational_distribution().\n. the base class, CVariationalGaussianLikelihood::set_variational_distribution, usesm_noise_factorto correct them_s2.\n. seeLogitVGPiecewiseBoundLikelihood.cpp` in the PR\n. @vigsterkr \nany feedback about this? I have no idea of why such function call is a bad way? can you point it out?\nThe idea is LogitVGPiecewiseBoundLikelihood and NumericalVGLikelihood are direct sub-classes of VariationalGaussianLikelihood.\n m_s2 is a protected data member in  VariationalGaussianLikelihood.\nm_noise_factor is used to ensure m_s2 is positive. \nthe function VariationalGaussianLikelihood::set_variational_distribution will ensure m_s2 is positive.\n set_variational_distribution in a sub-class will do additional class-specific work.\n. The job of model selection is to optimize hyper-parameters  of a model.\nIn Shogun, it is done via m_gradient_parameters.\nHowever, variational parameters and dual parameter(s) are NOT  hyper-parameters  of a model. These variational parameters and dual parameter(s) are optimized during inference process. In order words, they are latent variables.\nThis also explains the below question.\n. see above\n. another way to do it is create a m_latent_parameters in SG_Object\nand SG_ADD these parameters into LATENT_AVAILABLE\n. gradient parameters are those for which the gradient is defined and they can be optimised via gradient based optimization.\nFor gradient based optimization, does Newton method or LBFGS method count?\n. I try to set \nSG_ADD(&m_lambda, \"lambda\",\n\"Dual parameter for variational s2\",\nMS_NOT_AVAILABLE, GRADIENT_AVAILABLE);\nBut it fails.\nI guess that the reason is\nC++\ndouble nlopt_function(unsigned n, const double* x, double* grad, void* func_data)\n{\n...\nCMap<TParameter*, SGVector<float64_t> >* gradient=gradient_result->get_gradient();\n    CMap<TParameter*, CSGObject*>* gradient_dictionary=\n        gradient_result->get_paramter_dictionary();\n...\n}\nwhich means all parameters in m_gradient_parameters will be tuned when GradientModelSelection is called.\nHowever, variational parameters cannot be directly tuned via GradientModelSelection.\nThat is why I remove them from m_gradient_parameters\n. Please let me know if you have any question about this.\nIf not, please merge it so that I can send the nb.\n. the second iteration of nb can be found at\nhttp://nbviewer.ipython.org/gist/yorkerlin/0da0f3c23ec66e9261ab\n. @votjakovr \nThanks for you feedback.\nAccording to @karlnapf 's suggestion, it seems every data member should be added into the parameter list via SG_ADD.\n. thanks for this point.\nIn fact, all I need is the virtual SGMatrix<float64_t> get_multiclass_E(); method in CInferenceMethod. This method is used in CGaussianProcessMachine class.\n. I will update this in a new PR since the diff cannot identify the renaming thing if I directly change it in this PR. \n. forgot to remove it. \nupdated now.\n. same here\n. The width is used in float64_t CGaussianKernel::distance(int32_t idx_a, int32_t idx_b).\nLocally, I have unit tests for FITCInferenceMethod, which uses the GaussianKernel and the kernel width is not 1.\nI will add some tests in tests/unit/kernel/Kernel_unittest.cc, which tests the GuassianKernel\n. All issues about CDotFeatures are\nCGaussianKernel is a subclass of CDotKernel() and \nin the following function\n```\n       virtual bool CDotKernel::init(CFeatures l, CFeatures r)\n        {\n            CKernel::init(l,r);\n        ASSERT(l->has_property(FP_DOT))\n        ASSERT(r->has_property(FP_DOT))\n        ASSERT(l->get_feature_type() == r->get_feature_type())\n        ASSERT(l->get_feature_class() == r->get_feature_class())\n\n        if ( ((CDotFeatures*) l)->get_dim_feature_space() != ((CDotFeatures*) r)->get_dim_feature_space() )\n        {\n            SG_ERROR(\"train or test features #dimension mismatch (l:%d vs. r:%d)\\n\",\n                    ((CDotFeatures*) l)->get_dim_feature_space(),((CDotFeatures*) r)->get_dim_feature_space());\n        }\n        return true;\n    }\n\n```\nMaybe explicitly asking users to pass CDotFeatures are better.\nWhat is more, maybe we should change the CDotFeatures() too\nCDotKernel::CDotKernel(CFeatures* l, CFeatures* r)\n. Yes, I may use in near future once the four FITC methods are implemented.\nThere are two additional inference methods (all about Emti's KL dual inference method) to be added in the multiclass logistic GPs.  I will do it once I get time.\n. BTW, @karlnapf  in theory, it seems FITC could work for the multiclass logistic GP. Not sure whether it is robust or not. Maybe, I take a look at this later.\n. CARDKernel is a weighted GaussianKernel while the existing CGaussianKernel  does not have the weights\n. Yes. It could be done. \nThe trick part is gradients wrt to parameters (eg, weights). ( in fact, in the future, automatic differentiation may be a good choice for gradient stuff when hand-crafted gradients are not available)\nUsually, the full pairwise matrix should be symmetric positive definitive.   (Not sure whether the linear algebra back end supports such checking since Eigen cannot be used in core API)\nI am working on it. ( will soon be available next week). ( I am looking at codes about the linalg) \n. I am working on unit tests of the FITC Lapalce method.\nWill soon update the kernel stuff.\nOn Sunday, February 1, 2015, Heiko Strathmann notifications@github.com\nwrote:\n\nIn src/shogun/kernel/GaussianARDKernel.cpp\nhttps://github.com/shogun-toolbox/shogun/pull/2676#discussion_r23900579:\n\n@@ -122,21 +107,10 @@ SGMatrix CGaussianARDKernel::get_parameter_gradient(\n        {\n            for (index_t k=0; k avec=\n-                   ((CDenseFeatures*) lhs)->get_feature_vector(j);\n-               SGVector bvec=\n-                   ((CDenseFeatures*) rhs)->get_feature_vector(k);\n\nREQUIRE(avec.vlen==bvec.vlen, \"Number of right and left hand \"\n\n-                       \"features must be the same\\n\");\n\nfloat64_t result=0;\nfloat64_t result=distance(j,k);\n\n\nWhy would that be a problem? The gradient method than just checks the enum\nand does compute different gradients?\nAutomatic would be good I agree, but thats for the future for now ;)\nAnyway, I think this refactoring makes a lot of sense, check out GPy, they\nalso do this, though they just distinguish two cases: normal and ARD. We\nshould do the same via a flag. I think in fact we can use a boolean, too.\nThe pairwise scaling thing is infeasible anyways, so no need to worry about\nit, sorry that I brought that up.\nhttp://gpy.readthedocs.org/en/latest/tuto_GP_regression.html\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/shogun-toolbox/shogun/pull/2676/files#r23900579.\n\n\nbest,\nwu lin\n. I need a vector operation to support subtraction. \nI only find add in the existing linear algebra examples.\n. @karlnapf \nLet me mention another reason why not use CGaussianKernel as the base class.\nThe reason is for the scalar version of Gaussian ARD kernel, we can use the dot operation in CDotFeatures, which is implemented in the exiting CGaussianKernel.\nThe dot operation can support sparse dot operation.\nHowever, for the vector and matrix ARD kernels, I have to first get Dense vectors and then use them to compute kernel values. \n. Added the math\n. No. Let me know if this is an issue.\n. It is the if... else if... else statement. Let me know if this is an issue\n. ok\nI will change it in the next PR.\nBTW, one exception of this rule should be macros (e.g. REQUIRE) right?\n. This may be a issue.\nsee https://github.com/shogun-toolbox/shogun/pull/2676\n@lambday suggested that it is better not to touch class_list.cpp.py\nThe following is my question related to this issue in the previous PR.\nIt seems that src/shogun/base/class_list.cpp will automatically  create CLinearARDKernel and CGaussianARDKernel objects.\nHowever, when HAVE_CXX11 is not defined, these two ARD kernels are empty, which leads to a error in travis. \nI know there is a trick to create empty CLinearARDKernel and CGaussianARDKernel objects in   class_list.cpp  to address this error.\nHowever, do you know how to exclude  these two kernels in  class_list.cpp  when HAVE_CXX11 is not defined? In order words,  class_list.cpp will NOT create CLinearARDKernel and CGaussianARDKernel objects\n. Let me know which approach is better (empty instances or excluding these classes).\n. add the reference first since there are several fitc methods to be implemented \n. This kernel is a subclass of CDotKernel.\nCDotKernel requires CDotFeatures as input.\nLet me know if you think explicitly checking is better\n. see this\nhttps://github.com/shogun-toolbox/shogun/issues/2735\nI will remove it\n. done \nsee\nhttps://github.com/shogun-toolbox/shogun/issues/2735\nFor now, I will remove this function.\n. next PR?\nFocus on the math part first\n. it is done in the base class for single fitc inference (regression and binary classification) since in the future this class may support multi-kernels.\n. in the introduction of the class.\n. updated\n. The CInferenceMethod will compute the gradients wrt  hyper-paramter in parallel (pthread).\nThe existing FITCInferenceMethod only evaluates  the kernel function once when CFITCInferenceMethod::get_derivative_wrt_kernel is called. In that case, we do not need the lock.\nHowever, if we want to compute derivative_wrt_inducing_features and derivative_wrt_inducing_noise in parallel, we have to protect (mutual lock) the kernel object.\nThe reasons are below:\n- inducing_noise, inducing_features and scale are like hyperparamters of the kernel function, and we have to evaluate  the kernel function to obtain the gradients.\n- in FITC inference, we have to initialize  the kernel object three times (\\Sigma_n,  \\Sigma_m and \\Sigma_{nm} in order to evaluate the kernel function.\n. Another approach:\nTo avoid this lock and gain speed-up, we need three kernel objects via copy.  In this case, these three kernel objects will be initialized only once (\\Sigma_n, \\Sigma_m and \\Sigma_{nm}).\nThis is the time-space trade-off. \nFor a large-scale problem, the lock approach requires less memory but runs quite slow. On the hand, this new approach requires much memory but runs fast.\nLet me know your suggestion.\n. For computing gradients wrt hyper-parameters in cov, induing points, and scale, in the lock approach we have to initialized the kernels  (\\Sigma_n, \\Sigma_m and \\Sigma_{nm}).\nThe extra time complexity of the lock approach is 3_n^2  + 3_m^2 + 3_m_n , where n is the number of  input points and m is the number of inducing points.\nFor the copy approach, we do not have this extra init operations since we have init the three kernel objects in advance.\n. @karlnapf \nIf you think the copy approach is more reasonable, I can  send a new PR for this.\nI am not sure that whether the existing base class CKernel has a copy() to do the following operation. (Or all sub-classes have a copy() to do so)\nCKernel * ptr=new GaussianARDFITCKernel();\nCkernel * new_ptr=obj.copy() //where new_ptr is a pointer to a new instance of GaussianARDFITCKernel\n. see updated PR\n. see updated PR\n. currently we check the name of the provided kernel to determine whether the kernel can compute the derivatives wrt inducing_features.\nThe reason is the base class CKernel does not have  is_support_FITC().\nThe wrapped kernel approach is hard to implement since the static template methods are used in CKernel.\n. Indeed, if a kernel(left_features, right_features) can compute the gradients wrt left_features, the kernel supports FITC  operation.\nHowever, the existing CKernel::get_parameter_gradient(const TParameter* param, index_t index=-1) cannot handle this operation, since left_features are not hyper-parameters of the kernel.\nDo you have any suggestion about this? Maybe we can fix this issue in the future.\n. see here\nint64_t TSGDataType::get_num_elements()\n{\n    switch (m_ctype)\n    {\n        case CT_SCALAR:\n            return 1;\n        case CT_VECTOR: case CT_SGVECTOR:\n            /* length_y contains the length for vectors */\n            return *m_length_y;\n        case CT_MATRIX: case CT_SGMATRIX:\n            return (*m_length_y)*(*m_length_x);\n        case CT_NDARRAY:\n            SG_SNOTIMPLEMENTED\n        case CT_UNDEFINED: default:\n            SG_SERROR(\"Implementation error: undefined container type\\n\");\n            break;\n    }\n    return 0;\n}\nSince the method is done the job, I remove the redundant code. \n. if seems that without adding them, some compiler(s) may throw some warning.\n. It seems that Roman used Jacob's framework.\nIn fact, I changed most of the code in FITCInferenceMethod\n. The reason is the base class has to the same checking job since m_inducing_features is a protected member of the base class.\n. As mentioned before, we have two posteriors.\n- the true posterior p(f_n|y) \\propto p(y|f_n)p(f_n), where p(f_n) is the FITC equivalent prior\n- FITC approximated posterior\n  p(f_n|y) = \\int_{f_m} { p(f_n|f_m)p(f_m|y) df_m }\nIn general, only when f_m=f_n, the FITC approximated posterior  becomes the true posterior.\n. see above\n. @lambday \nYes. we can use CKernel::get_kernel_diagonal()\nHowever, I may do some similar tricks when the diagonal elements of derivatives wrt hyper-parameter are required. say get_parameter_gradient_diagonal(param, i) if needed.\nsee deriv_trtr=m_kernel->get_parameter_gradient(param, i).get_diagonal_vector();\n. @lambday\nget_parameter_gradient is a method in CKernel\nI may add a method called get_parameter_gradient_diagonal in CKernel, which will naively call get_parameter_gradient(param, i).get_diagonal_vector()\nIn ARD kernels, It seems  get_parameter_gradient_diagonal can be efficiently implemented. In other words, the working memory is an array of the size of the diagonal elements. \n. @lambday \nAs long as we can speed up the performance of computing diagonal elements of kernel matrix, we can gain the same speed for computing the diagonal elements of gradients wrt hyper-parameter(s).\n@karlnapf  your opinion?\nHowever, for now I will implement efficient get_parameter_gradient_diagonal() ONLY for the ARD kernels. \n. yes since GaussianARD kernel is\nexp(\\frac {({\\bf x}-{\\bf y})^T \\Lambda^T \\Lambda ({\\bf x}-{\\bf y})} {w})\nfor diagonal part of gradients wrt (w or \\Lambda) , note that x==y, which implies the kernel value is const (1 in this case)\n. Btw, i will overwrite the method related to computing the diagonal elements of kernel matrices for ARD kernel. \n. @karlnapf \nIt is a bug :( if the left_features and right_features are different.\nI will update this PR and add a unit test.\n. if the param type is a vector or a matrix, the framework do have a non-negative index value.\nif the param type is not a vector or a matrix, the framework use the default index value.\nHowever, the default value does not be used in this case. \n. if the left features and right features are the same object, we can use this trick.\n. if  left features and right features are the same object, we can use this trick to efficiently compute diagonal elements.\n. the old codes do not consider the asymmetric case of a kernel matrix such as kernel->init(features, inducing_features) \n. CGaussianKernel is a special case of CGaussianARDKernel.\nI use the result from CGaussianKernel to test the correctness of CGaussianARDKernel.\n. asymmetric case\n. symmetric case\n. if   left features and right features are not the same object but is equal, we cannot use this trick.\nWe can do extra equal checking in order to use this trick when left features and right features are not the same object.\nHowever, I do not want to do the checking. Reasons are:\n- since this case is rare, doing the checking may hurt the performance. \n- floating point equal checking is tricky\n. yes, if possible\n. yes. Let me know once they are available.\n. Since I need to update the result in place, I have to use eigen3.\nNote that the global back in linalg for now does not support in place operations.\n. we can obtain the feature_vector in place if hs is CDenseFeatures.\n. ok. I will do that\n. @karlnapf\nThe previous code has a bug. The false must be used to avoid multiple deletions. The bug did not be detected since in previous examples/tests, parameters are scalars. The bug did appear when GaussianARDKernel is used since the weights parameter in the kernel is not scalar.\n. This will be addressed in follow-up PRs which will take account of parameter constraints and parameter transformations.\n. @karlnapf \nthe bug is mentioned at\nhttps://github.com/shogun-toolbox/shogun/issues/2838\n. There are many local modes.\nThe old result of sigma EXPECT_NEAR(sigma, 4.25456324418582e-04, 1E-2); is ignored in the unit test.\nThe new input setting will test all output.\n+   EXPECT_NEAR(scale, 3.778912324292096, 1E-3);\n+   EXPECT_NEAR(CMath::sqrt(width/2.0), 2.024939917854716, 1E-3);\n+   EXPECT_NEAR(sigma, 1.862122012902403, 1E-3);\n. the error message is updated \n. the width defined in CGaussianKernel is width=wid*wid*2.0, where wid is the parameter used in GPML and log_width = log(wid)\n. logarithm domain?\n. It seems that the integration tests are generated by some scripts.\nI have to modify the scripts to pass the test.\nCould you tell me where are the the scripts? I do not know the location of these scripts.\n. the fact is\nCMath::sq(get_scale()) is equivalent to  CMath::exp(m_log_scale2.0)\n. it seems\nCMath::sq(get_scale()) has two function calls while  exp(m_log_scale*2.0) has one call.\n. the error message is not written by me.\nI prefer to the \"must be positive\" choice.\n. when all positive parameters are moved into log domain, it becomes a unconstrained problem. \nFor now, I comment out these two lines of code.\nOnce parameters in GuassianARDKernel are moved into log domain, I will delete these two lines.\n. CMath::sq(get_scale()) has two function calls while exp(m_log_scale2.0) has one call.\nwhat is more, it seems operations in log domain are more numerical stable.\n. Working on moving parameters in GuassianARDKernel to log domain\n. I found the tests\nignored it\n. In fact, we can choose difference optimization algorithms.\n. yes.\nNow, we have varDTC and FITC.\n. yes.\nFor now on, we assume the matrix weight L is a triangular matrix with positive diagonal elements.\nThe reason is W=L*L'.\n- We want W is positive definitive. \n- What is more, if diagonal entries of L are positive, the decomposition is unique, which is good for model selection. (indeed, I store them in log domain since they are positive)\n. waiting for his respond \n. Jacob agrees to move the license under BSD.\n. the old Kernel class only supports computation between instances of a features class\n. the new Kernel class now can supports computation between different features classes if the features in left hand side support compatible computation.\n. see comments in DotKernel\n. convert the representation used in GP to the classical (continuous) vector representation\n. copy function copy function (from the classical vector representation to  the representation used in GP)\n. copy function (from the representation used in GP to the classical vector representation)\n. the classical vector representation used in many existing minimizers/libraries\n.  the representation used in GP\n. if CMap<TParameter*, SGVector<float64_t> >* m_variable only contains one parameter (that is, one SGVector), we let SGVector<float64_t> m_variable_vec be the reference of the SGVector, which means a in place mapping.\nIf the relationship is in place, we do not use these copy/convert functions. \n. this cost function (a friend class of CPiecewiseQuadraticObject2) can read/expose internal variables of  CPiecewiseQuadraticObject2.\nIn practice, we only allows CPiecewiseQuadraticObject2 to create an instance of CMyCostFunction2.\nFor example, one way is\n- private constructor of CMyCostFunction2\n- CPiecewiseQuadraticObject2 is a friend class of CMyCostFunction2 to call the private constructor \nSince this is a unit test, I just use it in a simple way.\n. See https://github.com/shogun-toolbox/shogun/pull/2874\n. @lisitsyn\n classical vector representation means SGVector<float64_t> or float64_t * (continuous memory space/ array)\nthe representation used in GP means CMap<TParameter*, SGVector<float64_t> >* (not continuous memory  space)\n. Many existing libraries/minimizers take continuous array as input (for example, SGVector<float64_t>.\nThe representation used in GP is CMap<TParameter*, SGVector<float64_t> >* (not  continuous memory).\nThis class contains helper functions to convert  CMap<TParameter*, SGVector<float64_t> >*  to SGVector<floa64_t>\n. @lisitsyn \n Ruby uses initialize as the name of its constructors. ref:https://github.com/shogun-toolbox/shogun/issues/2765\nUsing init should be fine. \nAm I correct?\n. any example?\n. this implementation also offers one way to deal with the issue https://github.com/shogun-toolbox/shogun/issues/2859\n. @lisitsyn \nI do not understand what you said.\nPlease give me an example.\nIn general, this is an interface (abstract class). \nUsers have to implement a subclass of this class. \nI can show you an example how to use this class. \nPlease take a look at the unit test. \n. @lisitsyn \nthis is an example about how to use the CFirstOrderCostFunction class\n. @lisitsyn \nplease also take a look at this.\nThis is another example about how to use the CFirstOrderCostFunction class\n. Classical vector representation is the contiguous memory allocation.\n for example,float64_t * and SGVector<float64_t>\nThe  representation used in GP is not contiguous.\nfor example\nCMap<TParameter* SGVector<float64_t> > var \ncontains\n- \"x1\" is a SGVector vec(3)\n- \"x2\" is a   SGVector vec(2)\nparameter x1 is a vector with length=3\nparameter x2 is a vector with length=2.\nHowever,var is not contiguous.\n. @karlnapf \nOne issue have to solve is\nfor example\nCMap<TParameter* SGVector<float64_t> > var\ncontains\n    \"x1\" is a SGVector vec(3) \n    \"x2\" is a SGVector vec(2)\nCMap<TParameter* SGVector<float64_t> > grad \ncontains\n    \"x2\" is a SGVector vec(2)\n    \"x1\" is a SGVector vec(3)\nIf the order of variable (var:x1,x2) and its gradient (grad:x2,x1) are not the same, we have to take care of their parameter order.\nIf we use SGVector (classical vector representation or  contiguous memory representation) , this issue is gone since we assume the order is matched.\n. @karlnapf \ntake a look at this.\nIt also related to the cost function design\n. the different penalty types (L2 and L1) can be plugged in\nfor L1 penalty, extra work should be done to deal with non-smoothness. (ref:http://arxiv.org/abs/1312.5799)\n. this is class for  sample-specific cost functions (it also related to the cost function design)\nthe type of cost functions can be written as\nf(w)= \\sum_i{f_i(w)}\nwhere i is the index of samples.\nf_i(w) usually is convex and smooth/differentiable.\nFor example,\nlease squares loss\nf(w)=0.5 * \\sum_i{ (y_i-w'*x_i)^2 },\nwhere (x_i,y_i) are features and a label  for the i-th sample.\n. We can also provide static helper function to do the conversion\nSGVector<float64> copy(CMap<TParameter* SGVector<float64_t> > input) \nin some ultility file (eg Math.h)\n. there is why I put the if(m_fun!=fun) here \n. @karlnapf \nOne approach:\n- if we use SGVector<float64_t> instead of CMap<TParameter*, SGVector<float64_t> >*, all issues related to WrappedMinimizer in https://github.com/shogun-toolbox/shogun/pull/2872 are gone.\n- We can provide some static helper functions (eg in CMath.h) to do conversion between  SGVector<float64_t> and CMap<TParameter*, SGVector<float64_t> >*,\nLet me know your suggestion.\n. @karlnapf \nthis is an example about how to use this framework.\nfor now,  I assume the cost function is defined by users.\n. step1 define/implement one stochastic cost function (that is a finite sum of sample-specific cost function)\neg:\nf(w)=0.5* \\sum_i{ (y_i-w'*x_i)^2 }, where (x_i,y_i) are features and a label for the i-th sample, w is the objective variable. \n. step2 add a penalized cost function (decorator pattern) (optional))\n. L2 penalty \n. step3 register penalty  (optional))\n. learning rate\n. step5 register learning rate (or step size/length)\n. gradient updater\n. step6 register gradient updater\n. step 4 which minimizer to be used\n. note that\nfor batch update, step5 can be viewed as linear search methods\n. reference result\n. this is a problem we try to solve.\nsince we do not do multiply inheritance, we need to define its cost function separately via friend class.\n. we can cache some results if samples are read from a hard drive\n. this step can be paralleled (either synchronous or asynchronous)\nfor now it is just a simple implementation. \n. @lisitsyn \n@karlnapf \nfor first order gradient based minimizers,\nwe have \n- penalty (L1, L2 and elasticnet)\n- learning rate (const, many line search methods, inverse scaling)\n- gradient update (gradient descend, momentum update,  rprop)\nif we want combination of all three things, class should be a good choice.\nwe can use static function pointers to do so. \n. @lisitsyn \n@karlnapf \nfor gradient updater, sadly,  function pointer is not a good choice.\nReasons:\n- for momentum update, we need to store a corrected_gradient (vector) as data member for gradient updater since corrected_gradient will change every time we call  update_variable(...)\nif we use function pointer, the corrected_gradient will be a global variable. (bad, I think) \n. for learning rate, function pointer is not a good choice too.\nsee above\n. @karlnapf \n@lisitsyn \nfor learning rate, function pointer is not a good choice too.\nReasons:\n*for inverse scaling learn rate, learning_rate = eta0 / pow(t, power_t), where t is the times we call get_learning_rate(...), eta0 and power_t must be given by users or default value.\nfor class design, we can store t, eta0 and power_t as data members.\n. @karlnapf \nhow can I use REQUIRE and SG_WARNING if the class is not a subclass of SGObject \n. @karlnapf \nNow, the cost function is not a subclass of SGObject.\n. @karlnapf \n@lisitsyn \nwe return a CMinimizerContext (which is a subclass of SGObject) to store the mutable variables.  (serialization)\n. @karlnapf \n@lisitsyn \nwe load from a CMinimizerContext (which is a subclass of SGObject) to get the mutable variables.  (deserialization)\n. @karlnapf \nnow, we use SGVector instread of CMap.\nOnce we agree on this framework, I will update the PRs https://github.com/shogun-toolbox/shogun/pull/2874 and https://github.com/shogun-toolbox/shogun/pull/2872\n. @karlnapf \ntravis complains about the auto keyword.\nref\nhttps://travis-ci.org/shogun-toolbox/shogun/jobs/73320014\nIn fact, we can use the linalg here.\n. For target variable (for minimization), we need the in place modification for these variables.\nFor the corresponding gradient, we do not need the in place requirement. \n. @karlnapf \nI know. \nCurrently, the goal of this PR is about class design.\nI will update the document.\n. in place modification \n. currently, I focus on class design.\nComments will be added soon.\n. For now, users/developers have to delete these points outside.\n. users/developers have to delete all pointers.\n. see the updated comments \n. see updated comments\n. see updated comments\n. see updated commnets\n. see updated commnets\n. see updated comments\n. @karlnapf \nI added a new gradient updater. \nPlease take a look\n. It seems the function is supported since C99\nref\nhttp://www.cplusplus.com/reference/cmath/log1p/\n. @lisitsyn \nOther useful functions can be added in CMath\neg, expm1\nhttp://www.johndcook.com/blog/2010/06/07/math-library-functions-that-seem-unnecessary/\n. @lisitsyn \n@karlnapf \n@iglesias \nAny idea how to use SG_WARNING if the class is not a subclass of SGObject?\n. I compared the result with s2gd package\n. I also compared the result with the official released svrg package\n. @lisitsyn \nYes.\nFor NesterovMomentumCorrection, the return value is different from delta.\n. Yes.\nIf we allow to return a vector, we do not need to use reference.\nHowever, the delta is not a corrected_descend_direction.\nwe need the delta for adadelta method.\n. I use the content instead of the memory address to generate a hash value\n. @iglesias \nI just use the same seed from CMap\nAs long as the seed is fixed, any value should work then.\n. @iglesias \nCMap uses the memory address/pointer to generate a hash value.\nI need a Map which uses the content instead of the memory address to generate a hash value.\nI need the StringMap for this, which is related to serialization and  deserialization:\nhttps://github.com/shogun-toolbox/shogun/issues/2903\nSimply put, I need a Map which supports serialization and  deserialization\n- given a key as string (eg, \"className::varName\"), the map returns a type T value (eg, SGVector<float64_t>).\n-  In order to do so, a Map should use the content instead of the memory address to generate a hash value.\n. The CMap uses the memory address to generate a hash value while StringMap uses the content.\nI need different memory addresses to test StringMap.\n. I think std::string key=\"name\"; std::string key2=\"name\" should work too.\nI want to ensure that key and key2 use different memory addresses.\n. Once we use some C++11 compiler, we can use std::shared_ptr.\nLast time, the buildbot (travis) complained about the auto keyword.\nSo, for now I use raw pointer and users have to new/delete pointers in the main.\n. thanks\n. it is for L1 penalty and Elastic Net (a combination of L1 and L2)\n. For smooth penalty (eg, L2), we return the gradient related to the penalty.\nFor non-smooth penalty, we can return a sub-gradient.\nFor sparse penalty (eg, L1), a sub-gradient does not guarantee sparsity.\nFor L1, we do the following things:\n- return 0  in  get_penalty_gradient(...)\n- use update_sparse_variable(...) to enforce the sparsity constraint\n. For elastic net (a linear combination of L1 and L2),  we can the following things:\n- return gradient wrt L2 penalty in get_penalty_gradient(...)\n- use update_sparse_variable(...) to enforce the L1 sparsity constraint\n. @lisitsyn \n@karlnapf \nI think the minimizer framework is able to reproduce the same results (L1,L2, and elastic net)  generated by sklearn (SGD_classifier).\nIf we scale up the framework (eg, extend to support spare vectors, parallel speedup (openMP ?), GPU speedup), we may become good alternative to sklearn.\n@lambday \n Does the linalg support switching from CPU computation to GPU computation for dense vector ?\nHow about sparse vector?  Does the linalg support switching from CPU computation to GPU computation for sparse vector ?\n. see the update\n. thanks!\ncorrected the typo at \nhttps://github.com/shogun-toolbox/shogun/pull/2956\n. Using gist for the wiki thing?\nOn Wed, Dec 23, 2015, 17:15 Heiko Strathmann notifications@github.com\nwrote:\n\nIn src/shogun/neuralnets/ConvolutionalFeatureMap.cpp\nhttps://github.com/shogun-toolbox/shogun/pull/2961#discussion_r48383563:\n\n@@ -283,6 +287,32 @@ void CConvolutionalFeatureMap::convolve(\n            outputs.matrix+i*outputs.num_rows + outputs_row_offset,\n            m_output_height, m_output_width, false);\n+#if defined(HAVE_LINALG_LIB)\n\nI would go for the more efficient one.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/shogun-toolbox/shogun/pull/2961/files#r48383563.\n. Sorry. Wrong email\n\nOn Wed, Dec 23, 2015, 17:24 Heiko Strathmann notifications@github.com\nwrote:\n\nIn src/shogun/neuralnets/ConvolutionalFeatureMap.cpp\nhttps://github.com/shogun-toolbox/shogun/pull/2961#discussion_r48384037:\n\n@@ -283,6 +287,32 @@ void CConvolutionalFeatureMap::convolve(\n            outputs.matrix+i*outputs.num_rows + outputs_row_offset,\n            m_output_height, m_output_width, false);\n+#if defined(HAVE_LINALG_LIB)\n\n@yorkerlin https://github.com/yorkerlin ???\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/shogun-toolbox/shogun/pull/2961/files#r48384037.\n. I do not add this example.\nI have no idea why 0 is here\n. Make sure that you do not re-introduce the same bug I fixed before.\nhttps://github.com/lambday/shogun/commit/b7e98db219d909ee0c61b359ee83e1687a79368e\n. please add  the unit tests at https://github.com/lambday/shogun/commit/b7e98db219d909ee0c61b359ee83e1687a79368e\n. @lambday \nI found a bug when I worked on Gaussian processes.\nThese inputs are used to re-produce the bug. \n. If we are going to implement the plugin parameter framework, it should be easy to do this together. \nIt should look like  gpr.model_selection( criterion )\n\nFor now, I want to leave it alone since it is about model selection.\n. Btw, from version 0.18, sklearn will use a new model selection framework.\nWe can learn from it.\n. @karlnapf \nwe can use other optimizers which support box constraints. \n. @karlnapf \nsee below\nvoid CSingleSparseInferenceBase::check_fully_sparse()\n{\n        REQUIRE(m_kernel, \"Kernel must be set first\\n\")\n        if (strstr(m_kernel->get_name(), \"SparseKernel\")!=NULL)\n                m_fully_sparse=true;\n        else\n        {\n                SG_WARNING( \"The provided kernel does not support to optimize inducing features\\n\");\n                m_fully_sparse=false;\n        }\n}\n. KLDualInferenceMethod requires  a bound constrained optimizer.\nI will send another PR (a build-in optimizer)  to address this.\n. See \nhttps://github.com/shogun-toolbox/shogun/pull/3167\n. @karlnapf \nIf I want to add nlopt in the i file, how can I guard nlopt when nlopt is missing/disabled?\n. @karlnapf\nIf I want to add nlopt in the i file, how can I guard nlopt when nlopt is missing/disabled?\n. @karlnapf \nNow, lbfgs and nlopt minimizers can be used in SingleLaplacianInferenceMethod.\n. The default Newton minimizer.\n. Ref\nhttps://github.com/shogun-toolbox/shogun/issues/2247\n. The i files about minimizers muse be defined before the i files about GP . \n. No. I will correct that and add comments\n. I am working on it from this week.\n. In fact, I will refactor the framework and add some new algorithms about coordinate descend. \nref: https://github.com/uclaopt/TMAC\nproject TMAC is written in C++11 and has template design.\nI plan to refactor the existing framework in shogun soon.\n. yes\n. This method solves the issue.\n. It seems that SG_CALLOC does not work. I have to use new.\nThis is the reason why delete is used later.\n. Since new is used, delete has to be used. (ref: template <class T> void CParseBuffer<T>::init_vector() )\nBTW, if SG_FREE is used instead of delete, there will be a memory leak since new is used\n. @karlnapf \nNote that CStreamingVwFeatures will automatically free train_file. (in this example)\nHowever, CStreamingStringFeatures will not automatically free train_file. (ref:  shogun/examples/undocumented/libshogun/streaming_stringfeatures.cpp)\nWe should make them work consistently.\n. this is a bug.\nworking_file is defined in the base class (CStreamFeatures).\n. this issue is solved since the bug is fixed at src/shogun/features/streaming/StreamingStringFeatures.h \n. I tried SG_MALLOC(T, 1);, it does not work.\nMaybe the size/length is wrong. Do you know how I can get the right size given a generic type T?\nsizeof?\n. I tried SG_MALLOC(T, 1), SG_CALLOC(T, 1), SG_MALLOC(T, sizeof(T) ), and SG_CALLOC(T, sizeof(T) ); and SG_FREE is used. All these attempts are failed.\nI have to use new and delete.\n. Does Travis not complain about the auto keyword ?\n. @karlnapf \nThe unit test of SGObject is based on get_name. \nIf a class is called CNAME, the get_name should return NAME. Otherwise, the unit test of SGObject will fail. \n. NO. let me give you an example\n. I use this trick to deal with circular reference.\n. Let me give you an example.\nCase A: in KLDualInferenceMethod::method(): (worse case)\nat time 1:  KLDualInferenceMethod: ref_count =0\nat time 2:  cost_fun->set_target(this);   (KLDualInferenceMethod: ref_count =1)\nat time 3:  SG_UNREF(cost_fun);  (KLDualInferenceMethod: ref_count =0 and will delete itself in its member method ) \nCase B: in KLDualInferenceMethod::method():\nat time 1:  KLDualInferenceMethod: ref_count =1\nat time 2:  cost_fun->set_target(this);   (KLDualInferenceMethod: ref_count =2)\nat time 3:  SG_UNREF(cost_fun);  ((KLDualInferenceMethod: ref_count =1 and it is fine)\n. I have to deal with these two cases. \nif(this->ref_count()>1) is a trick to deal with the issue\n. The following code will produce these two cases.\nCase A\nint main()\n{\nKLDualInferenceMethod* obj=new KLDualInferenceMethod()\nobj->method();\ndelete obj;\nreturn 0;\n}\nCase B\nvoid GPC::set_obj(KLDualInferenceMethod* obj)\n{\nSG_REF(obj)\nm_obj=obj;\n}\nvoid GPC::doSomething()\n{\nm_obj->method();\n}\n. a hack to deal with issues related to JSON\n. it will cause memory leak.\n. @karlnapf \nI remove SG_ADD to break circular reference. \n. @karlnapf \nUsers can use LBFGSMinimizer instead of NLOPTMinimizer to optimize inducing features.\n. The existing implementation in CSGObject::get_parameter_incremental_hash() cannot handle circular reference. \n. ```\nvoid CSGObject::get_parameter_incremental_hash(uint32_t& hash, uint32_t& carry,\n        uint32_t& total_length)\n{\n    for (index_t i=0; iget_num_parameters(); i++)\n    {\n        TParameter* p=m_parameters->get_parameter(i);\n    SG_DEBUG(\"Updating hash for parameter %s.%s\\n\", get_name(), p->m_name);\n\n    if (p->m_datatype.m_ptype == PT_SGOBJECT)\n    {\n        if (p->m_datatype.m_ctype == CT_SCALAR)\n        {\n            CSGObject* child = *((CSGObject**)(p->m_parameter));\n\n            if (child)\n            {\n                child->get_parameter_incremental_hash(hash, carry,\n                        total_length);\n            }\n        }\n        else if (p->m_datatype.m_ctype==CT_VECTOR ||\n                p->m_datatype.m_ctype==CT_SGVECTOR)\n        {\n            CSGObject** child=(*(CSGObject***)(p->m_parameter));\n\n            for (index_t j=0; j<*(p->m_datatype.m_length_y); j++)\n            {\n                if (child[j])\n                {\n                    child[j]->get_parameter_incremental_hash(hash, carry,\n                            total_length);\n                }\n            }\n        }\n    }\n    else\n        p->get_incremental_hash(hash, carry, total_length);\n}\n\n}\n```\nThe existing implementation is about tree traversal. It cannot handle loop. Please feel free to use graph traversal.\n. Yes. We can use VarDTCInferenceMethod for Gaussian likelihood.\n.  In practice, we can first group training points into K clusters using unsupervised algorithm (eg, Kmeans). Then centers of these clusters are used as K inducing points.\n. We could also randomly select K training points as initial inducing points. Then we optimize the approximated marginal likelihood to obtain good inducing points. \n. I second that.\n. I think this notebook is only for Gaussian likelihood.\nFor other likelihoods (eg, t distribution and Logistic distribution), we should use FITC Laplace method.\n. @Saurabh7 \nPlease check out the table at http://www.gaussianprocess.org/gpml/code/matlab/doc/ for choosing right inference method for one  likelihood.\n. ",
    "emtiyaz": "@yorkerlin that seems odd to me as well, at the first look. I will have to investigate more carefully.This code has been checked by many people over last 2-3 years, so I am confident. Why don't you go ahead with the current code, we can correct it later.\nThanks for letting me know about this.\n. For the second comment, note that when computing pl, we already divided by sqrt(v). So you don't have to do it again. Not the best implementation, but it was done like this for the first time, so we are stuck with this.\n. That is correct. I will trust the code since it has been checked against Maple. But let me pull out some old derivations and make sure that the code and the appendix are consistent. Thanks again. \nFrom now on, implement what's there in the Matlab code, and use the Appendix only as a reference guide to understand what is going on.\n. Please go ahead by assuming that the code is correct, and design the unit tests.\n. Nope, that is not necessary. I will suggest making sure, first, that your code is correct, and second that your code is readable. You have hard coded numbers right now in your code, so you have essentially checked your code only on one data setting. I will first make a unit test, where you can generate the same data as in Matlab code, and then check against at least 5 random datasets (also change the size of the datasets). A thorough comparison like this will make sure that there is no major bug.\nSecond, if you compare my matlab code to your C++ code, there is perhaps a huge gap in readability. I don't expect C code to be as compact as Matlab, but it should more or less take very similar form. Writing such code is important for us to be able to check your code, and also for other people to be able to use it later on.\n. There are always simple answers to questions like this. If I was you, I will generate few datasets in Matlab and save them and read in C. You can easily think of other ways too.\n. No worries! My code assumes that you are familiar with GPML written in Matlab (actually, Shogun's implementation is motivated by GPML as well, so it is always helpful to understand this). I just added the GPML files in the repository, so have a look at it. \nIt will be helpful to understand Algorithm 2.1 in Chapter 2 of Rasmussen's book. My implementation is almost the same as his algorithm (note that mean is zero for his model and mine too).\nhttp://www.gaussianprocess.org/gpml/chapters/RW2.pdf\nFeel free to ask more questions!\n. Well the Kernel matrix isMxM (where M is the number of movies). Each user choose to generate data only for a subset of the movie. Denote the ratings of n'th user by y_{On} where On are the subset of movies rated by n'th user. Then the GP model is y_{On} distributed by GP(0, K_{On,On}) where K_{On,On} is formed by taking a block of the Kernel matrix K (corresponding rows and column). Then you run GP regression on it. This way you don't have to form Kernel for every user.\nSo all users have same GP model, it is just that you observe a subset of the data for each of them.\n. I don't know if the code result's are correct, but you need to get the sub Kernel for each user. You are not using the same training and test data as my code right now, so i can't tell. It might be easy if you make some artificiel train-test pair for few users, and then check my code against your code.\nI might not be available for next few hours. But good luck.\n. We need to compare both algorithms on the same measure. You can do it in the following way. Let us say that p1 = p(y* = 1) is the prediction of a label being +ve, then redefine p1 to be p1 = 2_p1 -1. similarly, p0 = p(y_=-1) is the prediction of label being negative, then redefine it to be p0 = 2*p0 - 1. Now, compute the RMSE as you compute for the GP regression case (using (1-p1)^2 and (-1-p0)^2 and then average and square root). \nThis number is expected to be less than GP regression case (although you might have to play with hyperparameters later on). For now, lets see what you get.\n. I suspect there is an issue here. Can you make sure if GP regression is using (+1, -1) label or (0,1) labels in shogun? If it is the latter, you don't need to rescale the p1 and p0 by 2, simply do (1- p1)^2 and (0 - p0)^2. If this is the problem, you will get a consisitent both lower train and test errors.\nIf this is not the issue, modify Matlab code to do GP classification, and check the number against C++ code.\n. Cep preferably, but Laplace is ok too. Between probity Logit it does not\nmatter.\nI am away for rest of the day. Will respond\nTomorrow now.\nOn Tuesday, April 1, 2014, pl8787 notifications@github.com wrote:\n\nCEPInferenceMethod & CLaplacianInferenceMethod\nCLogitLikelihood & CProbitLikelihood\nwhich one should I choose to do this task.\nthey are a little different.\n\nReply to this email directly or view it on GitHubhttps://github.com/shogun-toolbox/shogun/pull/2082#issuecomment-39231727\n.\n\n\nEmtiyaz\nEPFL\nhttp://www.cs.ubc.ca/~emtiyaz/\n. I have few suggestions on that, but I am busy with a paper submission until\nAug. 4. When does GSoC end?\nemt\nOn Fri, Aug 1, 2014 at 4:36 PM, Wu Lin notifications@github.com wrote:\n\n@karlnapf https://github.com/karlnapf\ngot it.\nI will send a PR for the notebook and another PR for the Laplace method\ntoday.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/shogun-toolbox/shogun/pull/2212#issuecomment-50891345\n.\n\n\nEmtiyaz\nEPFL\nhttp://www.cs.ubc.ca/~emtiyaz/\n. Is 2 a hyperparameter here? Where is the other variance hyperparameter that you multiply the Kernel matrix with? In my matlab code, I set log of these values to 0, which means I set them to 1 (both of them). You should set both code to same values.\n. About hyperparameters. It see that the format is exactly same (for this Kernel). P is basically tau*I, which is second hyperparameter in my code. You can basically multiply the Kernel matrix with sigma_f, so you don't need to specify. I hope my explanation is clear for you. For now, since we both set it to (1,1), there is no worry there then (and you have made sure the gaussian variance parameter too), and it's all good.\nFor cold start, you don't have to ignore it. When you don't have any data, then the prediction is equal to the prior mean which is 0 in our case (you could specify other mean function, and then it will predict a non-zero value). This is regression based recommendation, which takes care of the cold-start problem.\n. I agree to this. My code does this only once outside the loop. @pl8787 has some problem doing this outside the loop, for which I don't understand the reason yet.\n. ",
    "iarroyof": "Hi all. Could you let me know if MKL can be performed in GPU? in case it is, there are examples or documentation about it?\nThank you..\n. Thank you for answer... \nIn that case, yes, Im definitely interested in looking into this with the help of someone. Personally I have not reviewed the code yet. Neither I have not more knowledge about GPU parallelization than some indications from Theano, e.g. \"in order to go to the GPU, variables must be declared as 'shared'...\". I've an idea why it is necessary, but I have never programmed nothing like that.\nFurthermore, I have performed some tests by using MKLMulticlass with lists of kernels. These tests were performed over the  fm_train_multiclass_digits corpus. Naturally it goes very slow, which becomes very difficult getting a good version of the code (it is thought even more difficult considering that Im also interested in to learn the widths of the MKLCombinedKernel, as well as higher level hyperkernels). The target dataset (text data) of my research is similar (in size) to the fm_train_multiclass_digits; that is why I think going to the GPU would be useful. \nSaid the above, how do you suggest to proceed? \n. Hi all. Could you let me know if MKL can be performed in GPU? if yes, there are examples or documentation about it? Im using the python modular interface.\nThank you...\n. Hi @lisitsyn, Thank you for answer. How I can add these changes to my installation?\n. Hi @lisitsyn, I think I compiled from sources, so I made this:\n$ cd shogun\n~/shogun$ ls\napplications  ChangeLog       configs            data      NEWS       src\nbenchmarks    cmake           COPYING            doc       README.md  tests\nbuild         CMakeLists.txt  CTestConfig.cmake  examples  scripts\n~/shogun$ git pull\nremote: Counting objects: 190, done.\nremote: Compressing objects: 100% (7/7), done.\nremote: Total 190 (delta 138), reused 138 (delta 138), pack-reused 45\nReceiving objects: 100% (190/190), 97.81 KiB | 80.00 KiB/s, done.\nResolving deltas: 100% (148/148), completed with 53 local objects.\nFrom git://github.com/shogun-toolbox/shogun\n   308e63c..2cbeed7  develop    -> origin/develop\nUpdating 308e63c..2cbeed7\nFast-forward\n benchmarks/elementwise_benchmark.cpp               |  26 ++--\n src/interfaces/modular/Kernel.i                    |  16 ++\n src/interfaces/modular/Kernel_includes.i           |  13 +-\n src/shogun/base/SGObject.cpp                       |   1 +\n src/shogun/io/SGIO.h                               |   2 +-\n src/shogun/machine/GaussianProcessMachine.cpp      |   2 +\n src/shogun/machine/gp/EPInferenceMethod.cpp        |  22 ++-\n src/shogun/machine/gp/EPInferenceMethod.h          |   5 +-\n src/shogun/machine/gp/ExactInferenceMethod.cpp     |  24 ++-\n src/shogun/machine/gp/ExactInferenceMethod.h       |   5 +-\n src/shogun/machine/gp/FITCInferenceBase.cpp        |  45 +++---\n src/shogun/machine/gp/FITCInferenceBase.h          |  18 ++-\n src/shogun/machine/gp/FITCInferenceMethod.cpp      |  20 ++-\n src/shogun/machine/gp/FITCInferenceMethod.h        |   2 +\n src/shogun/machine/gp/InferenceMethod.cpp          |  11 +-\n src/shogun/machine/gp/InferenceMethod.h            |   8 +-\n src/shogun/machine/gp/KLInferenceMethod.cpp        |  23 ++-\n src/shogun/machine/gp/KLInferenceMethod.h          |   5 +-\n src/shogun/machine/gp/LaplacianInferenceBase.cpp   |  18 ++-\n src/shogun/machine/gp/LaplacianInferenceBase.h     |   4 +-\n .../machine/gp/MultiLaplacianInferenceMethod.cpp   |   3 +-\n src/shogun/machine/gp/SingleFITCLaplacianBase.cpp  | 170 +++++++++++++++++++--\n src/shogun/machine/gp/SingleFITCLaplacianBase.h    |  84 ++++++++++\n .../gp/SingleFITCLaplacianInferenceMethod.cpp      |  34 +++--\n .../gp/SingleFITCLaplacianInferenceMethod.h        |   5 +-\n .../machine/gp/SingleLaplacianInferenceMethod.cpp  |   7 +-\n .../machine/gp/SingleLaplacianInferenceMethod.h    |   2 +-\n .../implementation/ElementwiseUnaryOperation.h     |   2 +-\n .../internal/implementation/operations/Parameter.h | 126 +++++++++++++++\n .../mathematics/linalg/internal/opencl_util.h      |  60 +++++++-\n src/shogun/neuralnets/NeuralNetwork.cpp            |  25 +++\n src/shogun/neuralnets/NeuralNetwork.h              |   2 +-\n tests/unit/base/SGObject_unittest.cc               |  90 +++++++++++\n tests/unit/machine/gp/InferenceMethod_unittest.cc  |  67 +++++++-\n .../linalg/ElementwiseOperations_unittest.cc       | 124 ++++++++-------\n 35 files changed, 905 insertions(+), 166 deletions(-)\n create mode 100644 src/shogun/mathematics/linalg/internal/implementation/operations/Parameter.h\n~/shogun$ cd\n~$ python mkl2.py\nTraceback (most recent call last):\n  File \"mkl2.py\", line 50, in <module>\n    kernels.append(BesselKernel(l = feats_train, r = feats_train, order = 0, width = 5, degree = 1))\nNameError: name 'BesselKernel' is not defined\nAs you see, the error remains. What Im doing wrong?\n. Thank you very much for your very fast answer @yorkerlin . Im using the release version from 4 months ago. I will review things you are suggesting and I will write you if any doubt emerges.\nThank you very much again\n. That was indeed the solution to this issue: 2\\sigma^2. There is total\nconsistence with the results mentioned both in the publication I mentioned\nabove and in the Sonnenburg's Large Scale MKL (2006).\nThank you very much.\n2015-08-21 10:49 GMT-05:00 Wu Lin notifications@github.com:\n\n@nachitoys https://github.com/nachitoys\nWhich version of shogun for your experiments? (release version or\ndeveloper version)\nWe moved the internal representation of width in log domain in developer\nversion 4.1. I am not sure whether the change affect MKL.\nIf you use a developer version earlier than May 5, 2015, you may consider\nthe following reason:\n- The width used in Shogun may be different from the standard width.\nreference\nhttp://www.shogun-toolbox.org/doc/en/latest/classshogun_1_1CGaussianKernel.html\nhttps://en.wikipedia.org/wiki/Radial_basis_function_kernel\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/shogun-toolbox/shogun/issues/2893#issuecomment-133469247\n.\n\n\nIgnacio Arroyo Fern\u00e1ndez\n. This is the output:\n```\n\n\n\nprint feats_train.get_num_features()\n9\nprint  feats_train.get_num_vectors()\n9\nprint feats_train.get_sparse_feature_matrix()\n(array([ 40.,  16.,  20.,  24.,   4.]), array([0, 4, 5, 6, 1], dtype=int32), array([0, 0, 0, 1, 3, 3, 3, 3, 4, 5], dtype=int32))\n```\n. Thank you for reply @iglesias. Probably I hope something different to the identity matrix (although the kernel indeed initializes, but it does it wrong). It is what I get regardless of the values of the input vectors. It does not happen for toy data, perhaps less than 100 dimensions.\n. \n\n\n",
    "khalednasr": "@vigsterkr yeah\n. @vigsterkr yup!\n. @vigsterkr \n- Reworked the notebook according to the template. http://nbviewer.ipython.org/gist/khalednasr/9921707\n. @lisitsyn done\n. Sorry about that, didn't know there was a template. I'll take care of it.\n. @vigsterkr \nis it alright if I just add the fix to my pending pull request https://github.com/shogun-toolbox/shogun/pull/2090 ?\n. - replaced SG_PRINT with SG_INFO and removed the print flag\n. - added null checking on the features and labels\n@karlnapf they're already tested. and thank you for your comments!\n. @iglesias Thanks for the comments! I'll take care of them in my next PR.\n. Thanks for reviewing!\nThe unit tests do cover the changes, and valgrind memory check passes.\n. @karlnapf I'll take care of that in my next PR.\n. @lisitsyn There seem to some problems with this PR, I'll close it for now and push it again when I fix the problems and implement the wake sleep algorithm\n. @lambday about returning new matrices/vectors, this will be problematic for methods that can accept both matrices and vectors (like add()), since it won't be obvious if a new matrix should be returned or a new vector. I suggest that we leave it with pre-allocated stuff for now, and we can later try to come up with an elegant way to handle returning new matrices/vectors.\n. @lambday Next up is convolution, followed by random number generation.\n@lambday @karlnapf @lisitsyn @vigsterkr  I need to start modifying neuralnets to make it use the library, so can we merge this with develop soon?\n. @karlnapf Thanks :)\n@lisitsyn Forgot about those :) added them now.\n. @lisitsyn Thanks! looks good apart from them minor comments above.\nI have no problems with dropping multiple inputs, they're not really useful.\n. @lisitsyn That's possible. But if we're gonna add multiple inputs through features, why remove them in the first place?\nI like that CNeuralNetworkOptimizer thing. makes things much cleaner :)\n. what's wrong here?\n. what's wrong here, other than the indent?\n. Why?\n. @vigsterkr It doesn't make sense to have a class that describes a single neuron, since it's more efficient to treat neurons layer-wise, that is, the activations of all the neurons in the same layer are computed by a single matrix multiplication.\n. @vigsterkr I thought I should only increase the reference count when I'm going to storing a reference to the object?\n. Oh I see, thanks for pointing it out\n. Will do!\n. opps :) I'll fix it.\n. Since shogun is a shared library, it doesn't make sense to have a main() function inside the source tree. You should probably rewrite this as a unit test and put it with the other shogun unit tests.\n. that should have probably remained unchanged\n. the printing here is not for debugging purposes, it has to be available in release builds as it is very useful to look at the error while training the neural network. \n. already documented\n. I did that because parameters here  points to a just a small part of the network's larger parameter array, the part where the parameters of this particular layer are stored. However, I'm planning to change this once we add support for views on SGVector.\n. Thanks for noticing that! I'll take of it in my next PR.\n. I guess it would be cleaner to do that. I'll take care of it\n. the pointer here is just shorthand for parameters.vector. It needs to be a pointer so that it can be passed to Eigen3 methods. This part will be changed anyway when the linear algebra backend is ready.\n. which operator?\n. ops! sorry about that\n. opps, thanks!\n. check_gradients() compares the gradients computed with backpropagation with ones computed using numerical approximation and returns the average difference between them, so if everything is implemented correctly it should always return a number that's close to zero.\n. different values for contraction_coefficient won't make a difference, if it works for one value it'll work for all values\n. This part and its corresponding unit test compile and run successfully on my machine, however it fails to compile on travis for some reason (log).\n. This constructor isn't absolutely necessary, but it's convenient to have a constructor similar to the one in CGPUMatrix.\n. Well, it would be nice to have, for example if you're storing the parameters of a model in matrix and want to save them when the class is serialized.\n. The conversion functions only work when the scalar types agree, it will only accept converting a float64 matrix into another float64 matrix.\n. nope, it uses the single threaded CPU backend unless one of these is defined: (VIENNACL_WITH_OPENCL, VIENNACL_WITH_CUDA, VIENNACL_WITH_OPENMP)\n. It might be better to leave it that way. I'm not sure what are the all the valid values for Derived here..\n. @lambday it was for flexibility, but I forgot to add another vector type to the redux wrapper. I'll take care of it in the next patch. \n. maybe we can just always return float64_t?\n. ahh I see. I guess we should restrict it to one vector type for now.\n. Initializing that with an Eigen3 matrix results in a deep copy of the matrix. Making m_matrix a reference instead, to avoid copying, results in weird runtime errors.\n. I think it would be better to keep inputs a matrix. If we gpu-ized things, and inputs was of type CFeatures,  since CFeatures has its data on CPU memory, we'd have to copy it into GPU memory in every iteration of training, which would be pretty slow.\n. Got a stray comment here :)\n. and here\n. I see. Well, as long as CDotFeatures will support GPU memory there won't be a problem.\n. comments. here and throughout this file\n. ",
    "ghost": "Howdy!\nI already pulled a request and further polished according to other's kind suggestions (https://github.com/shogun-toolbox/shogun/pull/1980). Feels great in this community!\n. Ah oh, my Travis build failed...\nI'm not sure whether I am doing something wrong, and I just modify the document there... Could anyone kindly give me a hand? Thanks in advance!\n. @iglesias For convenience in code review, I add the include line folder by folder. So that there're so many commits there... The Travis build will take much too long time. Could you please help me set Travis directly on the last commit? Thanks!\n. @lambday Sorry for the long pause. I have another bunch of questions about the add method:\n1) should I move both inplace operators += to SGVector.cpp in order to avoid including linalg.h into SGVector.h? Of course I understand that this will make them non-inline. That's why I'm asking this question.\n2) which backend for linalg::add should I use with SGSparseVector?\n. @lambday \n1) seems like I'm the one with the wrong idea of what's going on. I assumed that as we need to remove SGVector::scale and this issue is about both scale and add the same applies to the SGVector::add. In case when I don't remove SGVector::add but just replace those calls outside the SGVector with the linalg::add all is ok and my first question is not actual.\n2) I haven't found any issue about it. Can you please estimate the complexity of this task? I'm sure it will take me some time, but it's very interesting opportunity and I would definitely like to try. Until then I'll leave that one for now)\n. @lambday so, just to be clear on this, operators in SGVector continue to use SGVector:add and this is not gonna be changed?\n. @lambday \n1) I've created simple benchmark using hayai and it's results for inline and non-inline SGVector::operator+=(SGVector) didn't differ much.\nThis is result for inline operator+=:\n```\n[==========] Running 1 benchmark..\n[ RUN      ] SGVector.addoperator_SGVector (10 runs, 100000000 iterations per run)\n[     DONE ] SGVector.addoperator_SGVector (867014.567120 ms)\n[   RUNS   ]        Average time: 86701456.712 us\n                         Fastest: 86623911.212 us (-77545.500 us / -0.089 %)\n                         Slowest: 86746861.212 us (+45404.500 us / +0.052 %)\n         Average performance: 0.01153 runs/s\n            Best performance: 0.01154 runs/s (+0.00001 runs/s / +0.08952 %)\n           Worst performance: 0.01153 runs/s (-0.00001 runs/s / -0.05234 %)\n\n[ITERATIONS]        Average time: 0.867 us\n                         Fastest: 0.866 us (-0.001 us / -0.089 %)\n                         Slowest: 0.867 us (+0.000 us / +0.052 %)\n         Average performance: 1153383.15863 iterations/s\n            Best performance: 1154415.66423 iterations/s (+1032.50560 iterations/s / +0.08952 %)\n           Worst performance: 1152779.46202 iterations/s (-603.69661 iterations/s / -0.05234 %)\n\n[==========] Ran 1 benchmark..\n```\nAnd this is for non-inline operator+=:\n```\n[==========] Running 1 benchmark..\n[ RUN      ] SGVector.addoperator_SGVector (10 runs, 100000000 iterations per run)\n[     DONE ] SGVector.addoperator_SGVector (869310.618160 ms)\n[   RUNS   ]        Average time: 86931061.816 us\n                         Fastest: 86625677.516 us (-305384.300 us / -0.351 %)\n                         Slowest: 87580124.516 us (+649062.700 us / +0.747 %)\n         Average performance: 0.01150 runs/s\n            Best performance: 0.01154 runs/s (+0.00004 runs/s / +0.35253 %)\n           Worst performance: 0.01142 runs/s (-0.00009 runs/s / -0.74111 %)\n\n[ITERATIONS]        Average time: 0.869 us\n                         Fastest: 0.866 us (-0.003 us / -0.351 %)\n                         Slowest: 0.876 us (+0.006 us / +0.747 %)\n         Average performance: 1150336.80610 iterations/s\n            Best performance: 1154392.12561 iterations/s (+4055.31951 iterations/s / +0.35253 %)\n           Worst performance: 1141811.57600 iterations/s (-8525.23010 iterations/s / -0.74111 %)\n\n[==========] Ran 1 benchmark..\n```\nSo, I moved SGVector::operator+=(SGVector) to .cpp file and replaced call to SGVector::add with linalg::add.\nFor now I leaved SGVector::operator+=(SGSparseVector) as it is.\n2) Yes, new thread sounds good. I'm looking forward discussing these issues.\n. @karlnapf Hi.\nYes, I am definitely interested. Thanks for the information!)\n. @iglesias  Is there anything left to do in this issue since it's still open? I've looked through the SGSparseMatrix_unittest.cc and seems like there's missing only unit test for SGSparseMatrix::sort_features().\n. @karlnapf yes, I can using the same cmake 3.2.2\n. thank you for quick respond\nbut I get nothing at the folder   shogun-root/build/examples/undocumented/python_modular\nand yes, I do have some excutable file under  libshogun folder ,  but how can I call these func in python?\n. @yorkerlin \nthank u, I'll have a try\n. hi! can i try this?. Hi Viktor,\nthanks for the quick reply - sorry, I see that I have left out essential information:\nI have no admin privileges on the server (also, no Docker available), and I need to install Shogun with Python interface for a third-party tool for a somewhat large analysis task, so a (public) cloud setup is unfortunately not an option.\n+Peter. 1) well, it's worth a shot - I found libshogun17_5.0.0_amd64.deb in the Trusty PPA, but I don't seem to find python-shogun, where can I get the correct deb package?\n2) any hint on what might be missing is of course appreciated. I managed to get my hands on libshogun18_5.0.0+1SNAPSHOT201703060902-0_amd64.deb and python-shogun_5.0.0+1SNAPSHOT201703060902-0_amd64.deb using my laptop (where I am root, otherwise same Debian in-house installation). I extracted the contents and linked the libs in my environment (is that the correct procedure? I have never done it that way before...). I just tried the Python interface, which gave me this:\n```python\n\n\n\nimport modshogun\n_mod = imp.load_module('_modshogun', fp, pathname, description)\nImportError: [...]../lib/libshogun.so.18: undefined symbol: cblas_dgemm\n```\n\n\n\nAlso, I cloned the repo and tried to build the dev branch of Shogun - this is the CMake call\nbash\ncmake\n-DPythonModular=ON\n-DBUILD_META_EXAMPLES=OFF\n-DCMAKE_PREFIX_PATH=/TL/epigenetics2/work/pebert/conda/envs/comikl\n-DCMAKE_INSTALL_PREFIX=/TL/epigenetics2/work/pebert/conda/envs/comikl\n-DSWIG_EXECUTABLE=/TL/epigenetics2/work/pebert/conda/envs/comikl/bin/swig\n-DBUILD_EXAMPLES=OFF\nAgain, did not result in obvious problems, yet the make resulted in the following:\n```bash\nSGBase.i:311: Error: Unable to find 'swig_typemaps.i'\nsrc/interfaces/python_modular/CMakeFiles/_python_modular.dir/build.make:120: recipe for target 'src/interfaces/python_modular/modshogunPYTHON_wrap.cxx' failed\nmake[2]: *** [src/interfaces/python_modular/modshogunPYTHON_wrap.cxx] Error 1\nCMakeFiles/Makefile2:325: recipe for target 'src/interfaces/python_modular/CMakeFiles/_python_modular.dir/all' failed\n```\nI put the full log here: make dev error\nI'll send you the dpkg -l output via mail as requested.. Ok, I have identified a problem - I started again from scratch and confirmed that swig_typemaps.i exists; cloned the repo and compiled again; this time, it worked. That was a bit unexpected, so I played around and found that I can reproduce the Unable to find 'swig_typemaps.i' error by doing a make clean and then trying to rebuild the whole thing (i.e., swig_typemaps.i is now indeed missing from the folder, I presume that is expected behavior?)\nSo, technically, to get a running Shogun w/ Python interface:\n1. set up Conda environment as specified in gist\n2. clone repo\n3. build Shogun from source, set CMake paths as appropriate for environment\nIn case something needs to be changed\n\ndelete everything\nclone repo again\nbuild again\n\nThe error about the 5.0.0 release version is still unsolved.\nIn principle, this could be closed. But if you want to track down the source of the release version error, I am happy to run some more tests/provide some more info if you need.\n. ",
    "luckybuilding": "Hi, I have found this now and wanted to use it. Is this still available on shogun?! I don't see the link!\n. Hey @iglesias ,\nThanks for your reply. I was referring to the top link (https://github.com/shogun-toolbox/shogun/blob/develop/doc/md/INSTALL.md#matlab)\nI think the support for MATLAB has been removed on Shogun 6.0.0.\nI don't see anything about Matlab on the latest versions!\nCan you provide a instruction for compile and use of it on MATLAB?. Thanks. Isn't it going to be supported in next versions?\nBecause, I'm starting a line of coding for a project that would need to be\nupdated later and I don't wanna get stuck in an late unsupported version.\nOn Nov 21, 2017 11:49 AM, \"Fernando J. Iglesias Garc\u00eda\" \nnotifications@github.com wrote:\nYou don't see anything about Matlab on the latest versions because as\nmentioned above you have to use Shogun 4.0.0 or earlier to use Shogun in\nMatlab.\nIn a nutshell, the main instruction for compilation is to configure the\nbuild with the cmake option MatlabStatic set to ON. Now depending on the\nversion of Matlab you are using, such as if you are using a version more\nmodern than Shogun 4.0.0, you may need to do a couple of other updates in\nthe build files.\nIf you need help with this, I could help you interactively via irc in our\nchannel.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/shogun-toolbox/shogun/issues/2017#issuecomment-345949956,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAt0yq8ge10zOpwts0BoQCz6FpyVRax1ks5s4oeIgaJpZM4BqKCP\n.\n. ",
    "Jiaolong": "@hushell  Thank you very much for your comments. I will include them into the next version soon.\nCurrently, I only tested on scene dataset.  I haven't tuned the parameters. I will attach the training loss and testing error later.\n. @hushell \nSorry for the delay. I have finished the implementation of reading multilabel in LibSVMFile (see #2062 ), while haven't been merged.\nI have done local test with the scene dataset, here is the results:\n```\n./so_fg_multilabel \nNumber of the samples: 1211\nDimention of the feature: 294\nNumber of classes: 6\n\n\n\n\n\nSGD trained in   20.0292\nsgd solver: average training loss = 0.436829\n\n\n\n\n\nNumber of the samples: 1196\nDimention of the feature: 294\nNumber of classes: 6\nsgd solver: average testing error = 0.608696\n```\n. I have run pyStruct for scene dataset, here are the results:\nTraining loss independent model: 0.066612\nTest loss independent model: 0.111204\nTraining loss tree model: 0.059868\nTest loss tree model: 0.106048\nTraining loss full model: 0.049408\nTest loss full model: 0.097408\nOur example maybe wrong since the error rate is too high. I need to check it.\n. @hushell \nI have pushed the my latest code into my origin repository:\nhttps://github.com/Jiaolong/shogun/tree/fg_multilabel_cvc_2\n. @karlnapf OK, I see! :)\n. @hushell Thank you very much for your comments! I will update the code soon. Regarding to the full connected graph, I need more discussion. Do you mean one factor shared by all  nodes, like a star structure?\n. @kislayabhi Hi, have you loaded your data successfully?\n(1) You should use \nget_sparse_matrix(SGSparseVector<sg_type>*& mat_feat, int32_t& num_feat, int32_t& num_vec) \nas the function you were using is not implemented, \nvirtual void get_matrix(float64_t*& matrix, int32_t& num_feat, int32_t& num_vec) { };\n(2) No need to specify num_feat=3 and num_vec=2, they are returned by the function you called.\n. @hushell Could you elaborate a little bit more? Which modules in shogun are related to this issue?  What kind of test and experiments do you expect to perform?\n. @tklein23 I have submitted this work in #2088, please check it.\n. @tklein23 I got a Travis error:\nThe command \"git fetch origin +refs/pull/2073/merge:\" failed\nhttps://travis-ci.org/shogun-toolbox/shogun/jobs/22037649\nCould you help me to check it? I only changed the format issue and the previous versions have all passed the Travis test.\n. @tklein23 For the memory leak issue, I pasted the the valgrind output in gist\nhttps://gist.github.com/Jiaolong/9925271\nThank you very much!\n. Hi, I have run 'tester.py' on the related examples and all is well:\npython tester.py structure_discrete_hmsvm_bmrm.py structure_factor_graph_model.py structure_multiclass_bmrm.py structure_plif_hmsvm_bmrm.py\nstructure_discrete_hmsvm_bmrm.py setting 1/1                 OK\nstructure_factor_graph_model.py setting 1/1                  OK\nstructure_multiclass_bmrm.py setting 1/1                     OK\nstructure_plif_hmsvm_bmrm.py setting 1/1                     OK\nI also tried running generator.py before 'tester.py' which is supposed to be the standard way, and one file in the shogun-data directory changed. \nI just wonder should I commit this changed file into shogun-data?\n. @iglesias OK, thanks! I have done pull request in shogun-toolbox/shogun-data.\n. @hushell \nI just want to use the uai I/O to test MPLP in shogun, because there are some benchmark data in uai format. Now I think I should postpone this and focus on the main issues.\n. @iglesias @hushell Thanks for merging!\n. @hushell could you please help me to check this PR? The Travis error should not be related to my changes. This PR is part of the work for GEMPLP. Thanks!\n. @iglesias the travis failed, could you please help me to check it? @hushell the MPLP code is ready for PR but this patch should be merged first. The current performance can be seen from\nhttp://nbviewer.ipython.org/gist/Jiaolong/c25b694e3b0eb3093fac\n. @iglesias Thanks for your review! In fact, I have already prepared the notebook. An early version can be found here:\nhttp://nbviewer.ipython.org/gist/Jiaolong/95633b7efd64bbd096f2\nI will update it with more detailed explanations and probably with more experiments on other datasets.\n. Sorry, it was an old version. The only clue about MPLP is infer_alg = LP_RELAXATION. :p And now it should be infer_alg = GEMPLP.\n. The reason I created FGTestData is that the same testing data are used in multiple unit tests, e.g., GrapCuts, GEMPLP, ect, and probably in the future new algroithms. Thus there are too many redudant code. Another reason is that, we haven't implemented functions to read factor graph from files, so we have to run these code to construct factor graph data.\n. Yes, maybe we can move it into unit test folder. However, it can also be used in integral test.\n. All right, I will change the name.\n. I just updated the code with your comments. Regarding to the unit tests, valgrind tests are fine:\n```\nshogun/build/tests/unit$ valgrind ./shogun-unit-test --gtest_filter=\"GEMP\"\n==4564== Memcheck, a memory error detector\n==4564== Copyright (C) 2002-2013, and GNU GPL'd, by Julian Seward et al.\n==4564== Using Valgrind-3.10.0.SVN and LibVEX; rerun with -h for copyright info\n==4564== Command: ./shogun-unit-test --gtest_filter=GEMP\n==4564== \nNote: Google Test filter = GEMP*\n[==========] Running 7 tests from 1 test case.\n[----------] Global test environment set-up.\n[----------] 7 tests from GEMPLP\n[ RUN      ] GEMPLP.find_intersections_index\n[       OK ] GEMPLP.find_intersections_index (27 ms)\n[ RUN      ] GEMPLP.max_in_subdimension\n[       OK ] GEMPLP.max_in_subdimension (8 ms)\n[ RUN      ] GEMPLP.initialization\n[       OK ] GEMPLP.initialization (64 ms)\n[ RUN      ] GEMPLP.convert_energy_to_potential\n[       OK ] GEMPLP.convert_energy_to_potential (6 ms)\n[ RUN      ] GEMPLP.simple_chain\n[       OK ] GEMPLP.simple_chain (32 ms)\n[ RUN      ] GEMPLP.random_chain\n[       OK ] GEMPLP.random_chain (34 ms)\n[ RUN      ] GEMPLP.sosvm\n[       OK ] GEMPLP.sosvm (30277 ms)\n[----------] 7 tests from GEMPLP (30458 ms total)\n[----------] Global test environment tear-down\n[==========] 7 tests from 1 test case ran. (30498 ms total)\n[  PASSED  ] 7 tests.\n==4564== \n==4564== HEAP SUMMARY:\n==4564==     in use at exit: 0 bytes in 0 blocks\n==4564==   total heap usage: 7,657,410 allocs, 7,657,410 frees, 164,492,480 bytes allocated\n==4564== \n==4564== All heap blocks were freed -- no leaks are possible\n==4564== \n==4564== For counts of detected and suppressed errors, rerun with: -v\n==4564== ERROR SUMMARY: 0 errors from 0 contexts (suppressed: 0 from 0)\n``\n. The time shown above is measured under valgrind, so it is longer than running at realse mode. It is452 mson my PC. But anyway, I will try to reduce more. Currently, the SOSVM data has 4 classes and 16 samples.\n. Now the unit test time is reduced to 160 ms.[       OK ] GEMPLP.sosvm (160 ms)`\n. Many thanks! :D\n. Sure! I will try to polish the notebook and send it shortly. Thanks!\n. Thanks! I have corrected them.\n. Thanks!\n. SGVector is not a dynamic array. Note that for real data set case, the number of examples and the feature dimension are not predefined.  I may use DynArray in the future.\n. Yes, I will change it.\n. OK, I will change the comments.\n. I haven't PR data yet. I would like to take into account #1987 and use the libsvm format\nhttp://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multilabel.html\n. I used push_back later. Maybe it is better to use DynArray in shogun. \n. OK, I will improve it.\n. @tklein23 I tried it and it is awesome! However, I have to remove two options since I got some error:\nInvalid command line options: \nclose-templates\nalign-reference=name\nMaybe we should update the document? \n. Here the SGVector and SGMatrix have  to be reference type because they are initialized inside gen_data.\n. Yes, I agree. I am doing this work.\n. I have tried removing the &, while the data cannot be passed out the function. So I think we should keep the reference type.\n. To keep in consistent with the other functions. Actually, I am not sure. If LibSVMFile is not going to be further inherit, we can remove virtual. \n. OK, then I remove it.\n. Yes, I am thinking about your main concerns. So could you give me a pseudo-code about the file determined parser/reader? I am not clear about how is the top design. \nSomething similar to CSVFile? e.g., \nCLibSVMFile* svmfile = CLibSVMFile(fname);\nSGMatrix<float64_t> mat=SGMatrix<float64_t>();\nmat.load(svmfile);\n. Yes, I will change the naming.\n. Yes, it sounds better to output message \"This is a multilabel file. You are trying to read it with a singlelabel reader.\" \n. Yes, I have seen MultilabelLabels::to_dense. OK, we can change this later.\n. I haven't removed this type. Yes, it will not be needed any more, I will remove it.\n. @iglesias Yes, I agree. I will change it.\n. I think this was an old issue:\nhttps://github.com/shogun-toolbox/shogun/pull/2039\n. What is your suggestion about converting SGVector* to float64_t*?\n. SGVector<float64_t>* multilabel is the unified format for parsing labels from file. Single label is just a special case, so I converted SGVector<float64_t>* multilabel into float64_t* label if users explicitly call get_spars:matrix(..., float64_t* label)\n. SGVector<float64_t>* is a matrix for storing multilabel while float64_t* is for single label and it is a special case of multilabel, i.e. SGVector<float64_t>* with one column. So I don't think SGVector.vector should be used here.\n. @tklein23 Yes, actually I like the auto-format tool and I have tried but it can also change the original code style and a large amount of diffs will appear. So for LibSVMFile, I would like to fix the indentation issue manually. Later, for my other code, I will use your recommended tool.\n. @hushell  Thanks for your reminder. Yes, there were memory leaks, I have fixed part of them and still there are possible leaks as it reported as follows:\n==13700== LEAK SUMMARY:\n==13700==    definitely lost: 0 bytes in 0 blocks\n==13700==    indirectly lost: 0 bytes in 0 blocks\n==13700==      possibly lost: 1,991 bytes in 33 blocks\n==13700==    still reachable: 6,760 bytes in 86 blocks\n==13700==         suppressed: 0 bytes in 0 blocks\n. @tklein23 @tklein23 I found the original code used tab for indent and for my newly added code, I used space. They were aligned well in my vim but why it displays differently in Github? What should I do to avoid this problem during coding?\n. Do you mean change SGVector<float64_t>* multilabel into SGVector<SGVector<float64_t>> multilabel? This can be done.\nThen what about other pointers, e.g., SGSparseVector<sg_type>*& mat_feat? The thing is that the base class CFile defined all these types, e.g.,\nvirtual void set_sparse_matrix(\n            const SGSparseVector<float64_t>* matrix, int32_t num_feat, int32_t num_vec)=0;\n. All right! :)\n. Yes, actually, I am not sure about this change and I want some discussion here.\nHow to specify  the log level in the notebook example? I am not familiar with the setting of '''SG_UNLIKELY(sg_io->loglevel_above(MSG_DEBUG))'''.\n. OK, thank you very much!\n. This notebook plots the primal and dual values of the learning machine.\nFor SGD, setting verbose = true is enough. However, for BMRM, it requires debug mode. Note that the condition in this line\n. I prefer removing the condition SG_UNLIKELY(sg_io->loglevel_above(MSG_DEBUG)) if there is no harm, because it can avoid output the DEBUG INFO (in the terminal) which we do not really need, at least in this notebook. Removing it can speed up the computation of this notebook a lot.\n. OK, so I should do the changes as follows:\nif (verbose && SG_UNLIKELY(sg_io->loglevel_above(MSG_INFO)))\nand in the notebook\nfrom modshogun import MSG_INFO\nbmrm.io.set_loglevel(MSG_INFO)\nright?\n. OK, then I think in the notebook we only need to set verbose = true and in the libbmrm.cpp we can modify the code like this:\n```\nif (verbose)\n{\n        float64_t debug_tstart=ttime.cur_time_diff(false);\n    SGVector<float64_t> w_debug(W, nDim, false);\n    float64_t primal = CSOSVMHelper::primal_objective(w_debug, model, _lambda);\n    float64_t train_error = CSOSVMHelper::average_loss(w_debug, model);\n    helper->add_debug_info(primal, bmrm.nIter, train_error);\n\n    float64_t debug_tstop=ttime.cur_time_diff(false);\n\n    if (SG_UNLIKELY(sg_io->loglevel_above(MSG_INFO)))\n            SG_SPRINT(\"%4d: add_debug_info: tim=%.3lf, primal=%.3lf, train_error=%lf\\n\", bmrm.nIter, debug_tstop-debug_tstart, primal, train_error);\n\n}\n```\nIs that what you mean? \n. - But verbose has been used for storing the training outputs, e.g., here\n- Do you mean change the 'if statement' to if(sg_io->get_loglevel ()==MSG_INFO)? But the printed information is called debug_info. In debug mode, this information can not be displayed.\n. Thank you very much for elaborating all these details! I have made a new version.\nIn this version, I only focused on the LIBBMRM class, since you told me not to touch other classes. \n- However, I found the variable verbose has connections to CSOSVMHelper in many other places.  example_1\n- According to the definition of SG_DEBUG in your last summary, I am not sure if the function name add_debug_info should be changed. example_2.\n. I don't understand how it breaks integration tests. :(\nI have run the python scripts and the output of the testing is as in this gist\n. SG_REF is inside get_factors(), only SG_UNREF needs to be add. It seems SG_REF cannot be added twice.\n. No, the full-connected structure here apparently has cycles. I tried and max-product cannot be used. \n. Yes, you are right! I will change it.\n. Yes, I changed these lines for other tunings. I will change them back.\n. Yes, currently, only pairwise factor type is considered. I added TODO there is because w_dim == 4 can be problematic. For example, for a unary factor with status space of 4. \n. GraphCuts can be used for general s-t graph:\n```\ng = GraphCut(5, 6)\nadd termainal-connected edges\ni.e., SOURCE->node_i and node_i->SINK\ng.add_tweights(0, 4, 0)\ng.add_tweights(1, 2, 0)\n```\nhttp://nbviewer.ipython.org/gist/Jiaolong/483de7d03865a373a51a\n. Yes, but currently, it is only for GraphCuts.\n. OK\n. Yes, we need to check. How about adding ASSERT here? i.e., data term should be always greater than 0.\n. Yes, you are right!\n. The data term can not be accessed from factor type. It seems not possible to check data>0 here.\n. OK\n. @hushell If edge features are available, I don't think we can set any constrains on w to ensure submodularity. :(\n. I have thought about it. However, SGNDArray is a template class, and too general, operators e.g, '+=', '*=' cannot be implemented. Maybe an explicit class with float64 type would be better.\n. The same functions have been implemented in Graph Cuts unit test. There will be compiling errors if the same functions are put here. Make them inline here can avoid confusion with other files. Maybe I should combine two unit tests together?\n. Great comments! I will consider to use Factor directly.\n. Yes, they are actually energy table. I will consider to use energy table.\n. @iglesias hushell\nI agree to merge MDArray into SGNDArray. Thanks for your discussion above, now I see it is possible to implement them in just one class. I am currently re-factoring MPLP. Once refactor work is done, I will put MDArray into SGNDArray as soon as possible.\n. OK, I will send an independent PR for SGNDArray.\n. Yes, I agree.\n. yes, I will use SGVector<index_t>.\n. I have changed it to SGVector<index_t>, so num_axes can be removed. \n. For single value, +=, -= can be done like this:\nSGNDArray<T> arr_a(d, nd);\n    arr_a += (SGNDArray(arr_a.get_dims())=val);\n    arr_a -= (SGNDArray(arr_a.get_dims())=val);\n. Not for clone, it is a function of assignment. All elements are assigned the same value val.\n. Yes, yours looks much better.\n. Sorry for the typos.\n. I want to show the dot product of parameters and features. It is a linear function, right?\n. Thanks for pointing this out, I will follow the same interface.\n. In this experiment, we don't use different parameters for different pairwise factors and the edge features are the same. Maybe \\theta_p_{i,j} more general?\n. No, I think we only use theta_p for all pairwise factors in this experiment. Please check cell 11,\ndef define_factor_type(num_status, dim_feat).\nPlease also see the text under cell 10:\nFor binary denosing, we define two types of factors:\n- unary factor: the unary factor type is used to define unary potentials that captures the the appearance likelyhood of each pixel. We use very simple unary feature in this experiment, the pixel value and a constant value 1. As we use binary label, thus the size of the unary parameter is 4.\n- pairwise factor: the pairwise factor type is used to define pairwise potentials between each pair of pixels. There features of the pairwise factors are constant 1 and there are no additional edge features. For the pairwise factors, there are 2\u00d72 parameters.\nPutting all parameters together, the global parameter vector w has length 8.\n. In fact, since we use fixed size of images, we can define full-connected factor graph model and use different parameters for different pairwise factors, but it will make the computation very slow.\nI have tried it for 10x10 images, and it works very well but for images larger than 20x20, it is extremely slow. \n. Since theta_p is independent to i, j, I think we can just put theta_p*phi_p(x, y_i, y_j). Here, phi_p(x, y_i, y_j) is in the form of joint feature.\n. Yes,  int_gap = primal_obj_val - dual_obj_val\n. OK, I agree with you. I just followed DRWN. I will change the names.\n. Agree!\n. Sounds very good!\n. OK\n. OK\n. Yes, so I will keep theta.\n. Yes, I totally agree with you. \nI am also considering to re-factor energy table and use SGNDArray to store energies and messages.  \nTable factor operation class could also be implemented in the future because the current operation can directly done by SGNDArray which is not straightforward and hard to understand. Such ideas are just inspired from DRWIN.\nWhat do you think?\n. Yes, you are right. In fact, I am confused by this line of code as well. The original implementation does not  use lam_minus *= 1.0/num_separators;. I will check it later.\n. Yes, thanks, I will try to make the data smaller.\n. I have run valgrind before, it seemed everything is fine. I can double check it.\n. Yes, I will add the descriptions.\n. Is this macro for avoiding generating python interface? Maybe I am wrong. :)\n. Yes, currently, it can not be used for modular interfaces. :(\n. Our implementation is not exactly the same as the original one, I mean it is not a copy & paste. However, we followed at least 80% of the original implementation for the core functions, e.g., update_message(). The main difference is that our version  is more compatible with the factor graph framework in shogun. \n. OK, then I will remove it.\n. Yes, you are right! This trick was done by Shell. :)\n. No, I mean the algorithm structure, the logic. :)\n. I will do it.\n. Oops!\n. ",
    "abinashpanda": "CUAIFile implemented according to Issue #1913 \n. @tklein23 The travis build failed failed because of \n264 - integration-python_modular-tester-classifier_averaged_perceptron_modular (Failed).\nI have not written this test nor modified it, unable to understand why? If you could help me.\n. @tklein23 Created a fresh PR #2320 for CHashedMultilabelModel with refactorings in multiple structured output models and machines.\n. @tklein23 I think we should close this PR as we cannot remove psi_truth/psi_pred from CResultSet. Instead of this added psi_truth_sparse, psi_pred_sparse and psi_computed_sparse for supporting sparse joint feature vector (#2320)\n. Output of valgrind tests/unit/shogun-unit-test --gtest_filter=HashedMultilabelModel.* is : \n==32362== HEAP SUMMARY:\n==32362==     in use at exit: 0 bytes in 0 blocks\n==32362==   total heap usage: 57,344 allocs, 57,344 frees, 3,702,883 bytes allocated\n==32362==\n==32362== All heap blocks were freed -- no leaks are possible\n==32362==\n==32362== For counts of detected and suppressed errors, rerun with: -v\n==32362== ERROR SUMMARY: 0 errors from 0 contexts (suppressed: 3 from 3)\n. parse() method parses the UAI file to get the network type (m_net_type), number of variables(m_num_vars), etc, so that the get_net_type(), get_num_vars() could return the corresponding values.\n. @vigsterkr I wanted to create a array of vectors for storing the factors scope. For example\n```\nMARKOV\n3\n2 2 3\n2\n2 0 1\n3 0 1 2\n4\n 4.000 2.400\n 1.000 0.000\n12\n 2.2500 3.2500 3.7500\n 0.0000 0.0000 10.0000\n 1.8750 4.0000 3.3330\n 2.0000 2.0000 3.4000\n```\nIn the previous example: I wanted to create 2 SGVector<int32_t> for storing scope of first factor ([0, 1] and [0 1 2]) and finally store the 2 vectors in a list. I didn't find any data structure available in shogun that can create vector of SGVectors<int32_t> (tried out CDynamicArray, SGVector<SGVector<int32_t>>, SGNDArray<SGVector<int32_t>>). So finally I went along with std::vector. If you could suggest me any other data structure available in shogun for this.\n(Note: SGMatrix would not suffice the need as the size of factors' scope are variable)\n. @hushell @vigsterkr Should I go for implementation of factors_scope and factors_data in init as:\nSGVector<int32_t>* factors_scope = new SGVector<int32_t> [num_factors]\nSGVector<float64_t>* factors_data = new SGVector<float64_t> [num_factors]\nand \ndelete [] factors_scope\ndelete [] factors_data\nin the destructor ~CUAIFile. This is similar to the implementation of m_labels in https://github.com/shogun-toolbox/shogun/blob/develop/src/shogun/labels/MultilabelLabels.cpp. \nI don't think this would lead to any memory leak.\n. @hushell @vigsterkr Initially I tried to add m_factors_table and m_factors_scope parameters both of them are of type SGVector<>* using SG_ADD. The unit tests gave error (SGObject.clone_equals_UAIFile) and they cannot be added until https://github.com/shogun-toolbox/shogun/issues/1972 has been resolved. Any suggestions ?\n. @tklein23 This thing does not break the PrimalMosek solver. Yes, it introduces performance penalty as we have to compute join_feature_vector twice for each iteration.\n. @tklein23 I was thinking of something else. If we are going to convert these sparse joint feature vector into a dense one, then there was no need of creating them at first place. For example, if we are going to have 2**20 == 1M features, then a simple addition would be reading 4MB of data. Instead of that we can add a method in SGSparseVector similar to CSparseFeatures::add_to_dense() like\n\nSGSparseVector::add_to_dense(float64_t alpha, float64_t* dense_vec, int32_t dense_vec_dim)\n\nThis would not require to convert a sparse vector to dense and we can add them without needing any extra memory. Furthermore, it would also make the implementation more clean. Any suggestions? \n. SparseMultilabel is present in MultilabelSOLabels.h file.\n. class SparseMultilabel is defined inside MultilabelSOLabels.h. It is a type of StructuredData\n. Sorry for that. Actually, it is the default text set by my editor. I should have checked that.\n. As the features are set randomly, so we cannot check it directly.\n. ",
    "achintp": "Hey @tklein23 ,\nSorry my finals were going on and I couldn't devote much time to this. Understandably, I mised the GSoC deadline. I still want to complete it though, so I should have it in a couple of days\n. ",
    "cassiogreco": "@iglesias \n. @iglesias @karlnapf  I think i was able to squash all other commits\n. Oh yes, sorry about that. I have left the 5 virtual methods of CDistribution and fixed some other parts of the code\n. If it is false, an error will be returned\n. and I need a boolean as a return for the train(data) method\n. ",
    "wittawatj": "Hello,\ngcc --version gives\ngcc (Ubuntu/Linaro 4.6.3-1ubuntu5) 4.6.3\nlsb_release -a gives\nNo LSB modules are available.\nDistributor ID: Ubuntu\nDescription:    Ubuntu 12.04 LTS\nRelease:    12.04\nCodename:   precise\ncat /proc/cpuinfo outputs\nprocessor   : 0\nvendor_id   : GenuineIntel\ncpu family  : 6\nmodel       : 42\nmodel name  : Intel(R) Core(TM) i5-2450M CPU @ 2.50GHz\nstepping    : 7\nmicrocode   : 0x25\ncpu MHz     : 800.000\ncache size  : 3072 KB\nphysical id : 0\nsiblings    : 4\ncore id     : 0\ncpu cores   : 2\napicid      : 0\ninitial apicid  : 0\nfdiv_bug    : no\nhlt_bug     : no\nf00f_bug    : no\ncoma_bug    : no\nfpu     : yes\nfpu_exception   : yes\ncpuid level : 13\nwp      : yes\nflags       : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe nx rdtscp lm constant_tsc arch_perfmon pebs bts xtopology nonstop_tsc aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx est tm2 ssse3 cx16 xtpr pdcm pcid sse4_1 sse4_2 x2apic popcnt tsc_deadline_timer aes xsave avx lahf_lm ida arat epb xsaveopt pln pts dts tpr_shadow vnmi flexpriority ept vpid\nbogomips    : 4983.84\nclflush size    : 64\ncache_alignment : 64\naddress sizes   : 36 bits physical, 48 bits virtual\n. ",
    "chengsoonong": "homebrew defaults to 3.0.2 at the moment. Here is how to downgrade:\n$ brew versions swig\nWarning: brew-versions is unsupported and will be removed soon.\nYou should use the homebrew-versions tap instead:\n  https://github.com/Homebrew/homebrew-versions\n3.0.2    git checkout c484aac /usr/local/Library/Formula/swig.rb\n3.0.1    git checkout c75e79a /usr/local/Library/Formula/swig.rb\n3.0.0    git checkout d588f39 /usr/local/Library/Formula/swig.rb\n2.0.12   git checkout 89eafbe /usr/local/Library/Formula/swig.rb\n2.0.11   git checkout b51b660 /usr/local/Library/Formula/swig.rb\n2.0.10   git checkout 4392eba /usr/local/Library/Formula/swig.rb\n2.0.9    git checkout 0b50110 /usr/local/Library/Formula/swig.rb\n2.0.8    git checkout ce2b082 /usr/local/Library/Formula/swig.rb\n2.0.7    git checkout 4466cc1 /usr/local/Library/Formula/swig.rb\n2.0.6    git checkout 293be9d /usr/local/Library/Formula/swig.rb\n2.0.4    git checkout 0d8d92b /usr/local/Library/Formula/swig.rb\ngo to where the source is...\n$ cd $(brew --prefix)\ncheckout old formula\ngit checkout 89eafbe /usr/local/Library/Formula/swig.rb\nbrew install swig\nkeep repository clean\ngit checkout -- Library/Formula/swig.rb\n. ",
    "matthuska": "Just wanted to say that I'm using swig 3.0.2 with today's develop branch and the R modular interfaces and everything is working perfectly. gcc 4.8.2 in case that matters.\n. Travis is using Ubuntu 12.04, which only has swig 2.0.4. This is causing the build to abort when compiling with the r_modular bindings (we need >= 2.0.5).\n. This might do the trick (it has 2.0.7):\nhttps://launchpad.net/~swt-techie/+archive/ubuntu/swig\n. I just checked out the latest development version and it compiles fine, though I'm using Ubuntu 14.10.\nI don't know if it will make any difference but the HDF5 header in that warning (H5public.h) has been updated on my system so that I don't get the same warning you're getting. Here's my version:\n$ aptitude show libhdf5-dev\nVersion: 1.8.12+docs-1.1ubuntu1\n. I just checked out the latest development version and it compiles fine, though I'm using Ubuntu 14.10.\nI don't know if it will make any difference but the HDF5 header in that warning (H5public.h) has been updated on my system so that I don't get the same warning you're getting. Here's my version:\n$ aptitude show libhdf5-dev\nVersion: 1.8.12+docs-1.1ubuntu1\n. Just to rule it out, can you modify the H5public.h file so that it doesn't trigger that warning? The details are here:\nhttps://gitorious.org/kitware/itk/commit/f46f541516a33aa12be8e390b59f4b5871d57ab9\n--- a/Modules/ThirdParty/HDF5/src/itkhdf5/src/H5public.h\n+++ b/Modules/ThirdParty/HDF5/src/itkhdf5/src/H5public.h\n@@ -195,7 +195,7 @@\n #elif H5_SIZEOF_HADDR_T ==H5_SIZEOF_LONG\n #   define H5_PRINTF_HADDR_FMT  \"%lu\"\n #elif H5_SIZEOF_HADDR_T ==H5_SIZEOF_LONG_LONG\n-#   define H5_PRINTF_HADDR_FMT  \"%\"H5_PRINTF_LL_WIDTH\"u\"\n+#   define H5_PRINTF_HADDR_FMT  \"%\" H5_PRINTF_LL_WIDTH \"u\"\n #else\n #   error \"nothing appropriate for H5_PRINTF_HADDR_FMT\"\n #endif\n. Just to rule it out, can you modify the H5public.h file so that it doesn't trigger that warning? The details are here:\nhttps://gitorious.org/kitware/itk/commit/f46f541516a33aa12be8e390b59f4b5871d57ab9\n--- a/Modules/ThirdParty/HDF5/src/itkhdf5/src/H5public.h\n+++ b/Modules/ThirdParty/HDF5/src/itkhdf5/src/H5public.h\n@@ -195,7 +195,7 @@\n #elif H5_SIZEOF_HADDR_T ==H5_SIZEOF_LONG\n #   define H5_PRINTF_HADDR_FMT  \"%lu\"\n #elif H5_SIZEOF_HADDR_T ==H5_SIZEOF_LONG_LONG\n-#   define H5_PRINTF_HADDR_FMT  \"%\"H5_PRINTF_LL_WIDTH\"u\"\n+#   define H5_PRINTF_HADDR_FMT  \"%\" H5_PRINTF_LL_WIDTH \"u\"\n #else\n #   error \"nothing appropriate for H5_PRINTF_HADDR_FMT\"\n #endif\n. Are there any earlier errors? Perhaps related to swig? Maybe you can do a fresh build and post the entire build output into a gist so I can look through it for other possible problems. Swig should be generating modshogun.R and for some reason it doesn't seem to be happening.\n. Are there any earlier errors? Perhaps related to swig? Maybe you can do a fresh build and post the entire build output into a gist so I can look through it for other possible problems. Swig should be generating modshogun.R and for some reason it doesn't seem to be happening.\n. I don't see anything obvious in the build output. Can you give me the parameters you used when building hdf5? I'll build a local copy as well and see if I can reproduce your problem.\nAlso, what version of swig are you using?\n. I don't see anything obvious in the build output. Can you give me the parameters you used when building hdf5? I'll build a local copy as well and see if I can reproduce your problem.\nAlso, what version of swig are you using?\n. ",
    "sejdino": "it seems like we need to speed up the unit tests:\n87/299 Test  #86: unit-QuadraticTimeMMD ..............................................   Passed   74.95 sec\n        Start  89: unit-MMDKernelSelectionMedian\n 88/299 Test  #89: unit-MMDKernelSelectionMedian ......................................   Passed    1.34 sec\n        Start  90: unit-MMDKernelSelectionCombOpt\n 89/299 Test  #90: unit-MMDKernelSelectionCombOpt .....................................   Passed   37.14 sec\n        Start  91: unit-MMDKernelSelectionCombMaxL2\n 90/299 Test  #91: unit-MMDKernelSelectionCombMaxL2 ...................................   Passed   16.55 sec\n        Start  92: unit-MMDKernelSelectionMax\n 91/299 Test  #92: unit-MMDKernelSelectionMax .........................................   Passed   17.91 sec\n        Start  93: unit-HSIC\n 92/299 Test  #93: unit-HSIC ..........................................................   Passed   53.69 sec\n        Start  94: unit-BTestMMD\nI'm sorry but your test run exceeded 50.0 minutes. \nOne possible solution is to split up your test run.\n. Yes, there is no need to merge samples for the independence statistic (no kernel between entries from different samples is ever computed). You effectively only need to compute a kernel matrix K on {X_i} and a kernel matrix L on {Y_i} and then use the shuffled indices to access entries in K. \n. What does this do?\n. The formula for Q is correct, but one also needs to pool the X and Y data together within each block and then randomly split it (in the same proportions) when estimating variance (i.e., these h_{i,j}_{a,b}) would be computed on permuted data for estimating variance and on original data for computation of the statistic. I may be missing something but I don't see that happening here? See cheat sheet, Setup II (within-block permutation).\n. yes - it's awesome.\n. Hm, do not quite understand this. Does \"DEPRECATED blockwise statistic computation\" refer only to normalizing constants? NO_PERMUTATION_DEPRECATED refers to the old (incorrect) version of the variance/covariance computations - why is this different for kernel selection and for standard tests with a fixed kernel?\n. this one should be if(m_null_var_est_method!=WITHIN_BLOCK_DIRECT)\n. ",
    "postoroniy": "slava@slava-mint ~ $ uname -a\nLinux slava-mint 3.13.0-24-generic #47-Ubuntu SMP Fri May 2 23:31:42 UTC 2014 i686 i686 i686 GNU/Linux\nslava@slava-mint ~ $ python --version\nPython 2.7.6\n. HI @vigsterkr ,\nI compile for i686, compilation didn't show any error.\nlinux kernel as well as machine is mentioned here: Linux 3.13.0-24-generic i686 i686 i686 GNU/Linux\nShould I disable sse2?\nps gcc is\ngcc (Ubuntu 4.8.2-19ubuntu1) 4.8.2\n. ",
    "tpokorra": "we have a similar problem with Shogun 4.0.0 for Fedora with Mono 4.0.1 (https://bugzilla.redhat.com/show_bug.cgi?id=1223446):\n/usr/bin/mcs /t:library *.cs /out:modshogun.dll\nmodshogunPINVOKE.cs(7271,137): error CS0100: The parameter name `size' is a duplicate\nmodshogunPINVOKE.cs(17102,255+): error CS0100: The parameter name `size' is a duplicate\nmodshogunPINVOKE.cs(17888,190): error CS0100: The parameter name `size' is a duplicate\nmodshogunPINVOKE.cs(17891,190): error CS0100: The parameter name `size' is a duplicate\nmodshogunPINVOKE.cs(19190,192): error CS0100: The parameter name `size' is a duplicate\nIs there an easy fix for this?\n. I found http://www.shogun-toolbox.org/page/contact/irclog/2014-07-30/ which describes that ignoring the methods in modshogun_ignores.i should resolve the issue.\nIs this still appropriate?\n. based on the reply from the swig issue referenced above, it seems that shogun is using the inattributes in a way that cannot work if there are multiple parameters of type array:\n%typemap(ctype, out=\"CTYPE*\") shogun::SGVector<SGTYPE> %{int size_$1, CTYPE*%}\n%typemap(imtype, out=\"IntPtr\", inattributes=\"int size, [MarshalAs(UnmanagedType.LPArray)]\") shogun::SGVector<SGTYPE> %{CSHARPTYPE[]%}\nI tried to reproduce a test case with swig, but I have to admit I only got so far to see the existing tests for arrays.\ngit clone https://github.com/swig/swig.git\ncd swig\n./autogen.sh\n./configure --without-go --without-guile --without-java --without-javascript --without-ocaml --without-octave --without-perl5 --without-python  --without-python3 --without-r --without-ruby --without-tcl\nmake\nmake check\ncd Examples\nfind . -type f -name \\* -exec grep 'UnmanagedType.LPArray' {} \\; -print\nvi ./csharp/arrays/examplePINVOKE.cs\nvi ./test-suite/csharp/csharp_lib_arrays/csharp_lib_arraysPINVOKE.cs\nThere you can see how the tests do it. It seems that the original method already has an extra parameter for the array size.\nI think I will have to stop this investigation here. Please can someone with more knowledge about shogun pick up from here?\n. ",
    "flaxter": "Hi Wu. Is this question directed to me? (Happy to help, just need more\nguidance.)\nBest,\nSeth\nOn Sep 14, 2014 7:44 PM, \"Wu Lin\" notifications@github.com wrote:\n\nCould you reproduce the error so that I can run gdb to find the bug?\nI guess it should be related to computing the kernel matrix.\nOn Sunday, September 14, 2014, Heiko Strathmann notifications@github.com\nwrote:\n\n@votjakovr https://github.com/votjakovr @yorkerlin\nhttps://github.com/yorkerlin do you have an idea what causes this?\n\u2014\nReply to this email directly or view it on GitHub\n<\nhttps://github.com/shogun-toolbox/shogun/issues/2539#issuecomment-55542208>\n.\n\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/shogun-toolbox/shogun/issues/2539#issuecomment-55543531\n.\n. @yorkerlin @karlnapf any updates?\n. \n",
    "macumber": "Is it possible to build Shogun on Windows? Specifically the Ruby bindings\n. I have experience with SWIG and Cmake but I can't get a starting build of the non-Ruby components.  I tried initializing a build environment using Ruby's DevKit then generating Unix Makefiles but the build failed right away on Eigen.\n. Here is what I did to set up my environment:\n```\n\"c:\\Program Files (x86)\\Microsoft Visual Studio 12.0\\VC\\vcvarsal\nl.bat\"\n\"c:\\DevKit\\devkitvars.bat\"\nAdding the DevKit to PATH...\n\"c:\\Program Files (x86)\\CMake\\bin\\cmake-gui.exe\" # configure Unix Makefiles, Disable meta examples\n\"gcc --version\"\ngcc (rubenvb-4.7.2-release) 4.7.2\nSET PATH=%PATH%;C:\\Git-1.9.5\\Git\\bin\nSET PATH=%PATH%;\"C:\\Program Files\\TortoiseSVN\\bin\"\n\"c:\\Program Files (x86)\\Microsoft Visual Studio 12.0\\Common7\\IDE\\devenv.exe\" shogun.sln\n```\nHere is summary of configuration:\n```\nSummary of Configuration Variables\n-- The following OPTIONAL packages have been found:\n\nGDB\nOpenMP\nThreads\nDoxygen (required version >= 1.8.6)\n\n-- The following REQUIRED packages have been found:\n\nPythonInterp\n\n-- The following OPTIONAL packages have not been found:\n\nCCache\nMosek\nBLAS\nLAPACK\nGLPK\nCPLEX\nARPACK\nEigen3 (required version >= 3.1.2)\nViennaCL (required version >= 1.5.0)\nNLopt\nLpSolve\nColPack\nSphinx\nLibXml2\nHDF5\nCURL\nZLIB\nBZip2\nLibLZMA\nSNAPPY\nLZO\nSpinlock\nProtobuf\n\n-- The following REQUIRED packages have not been found:\n\nOpenCL\n\n===================================================================================================================\nEnabled Integration\n  OpenCV Integration is OFF      - enable with -DOpenCV=ON\n===================================================================================================================\nEnabled Interfaces\n  libshogun is ON\n  python modular is OFF      - enable with -DPythonModular=ON\n  octave modular is OFF      - enable with -DOctaveModular=ON\n  java modular is OFF    - enable with -DJavaModular=ON\n  perl modular is OFF    - enable with -DPerlModular=ON\n  ruby modular is OFF    - enable with -DRubyModular=ON\n  csharp modular is OFF      - enable with -DCSharpModular=ON\n  R modular is OFF       - enable with -DRModular=ON\n  lua modular is OFF     - enable with -DLuaModular=ON\n\nShogun will be built GPL3 compatible\nEnabled legacy interfaces\n  cmdline static is OFF      - enable with -DCmdLineStatic=ON\n  python static is OFF   - enable with -DPythonStatic=ON\n  octave static is OFF   - enable with -DOctaveStatic=ON\n  matlab static is OFF   - enable with -DMatlabStatic=ON\n  R static is OFF        - enable with -DRStatic=ON\n===================================================================================================================\nTo compile shogun type\n  make\nTo install shogun to C:/Program Files (x86)/shogun type\n  make install\nor to install to a custom directory\n  make install DESTDIR=/my/special/path\n  (or rerun cmake with -DCMAKE_INSTALL_PREFIX=/my/special/path) to just change the prefix\n===================================================================================================================\nConfiguring done\n```\nHere is the build error I get:\nE:\\shogun\\build>make\n[  0%] Built target Eigen3\n[  1%] Generating version header\n[  1%] Built target version\nScanning dependencies of target class_list\n**[  1%] Generating E;c:\\DevKit\\shogun\\src\\shogun\\base\\class_list.cpp**\n/bin/sh: C:/Users/dmacumbe/AppData/Local/Continuum/Miniconda2/python.exe: Bad fi\nle number\nmake[2]: *** [../src/shogun/base/class_list.cpp] Error 126\nmake[2]: *** Deleting file `../src/shogun/base/class_list.cpp'\nmake[1]: *** [src/shogun/CMakeFiles/class_list.dir/all] Error 2\nmake: *** [all] Error 2\nSeems like there is some code generation happening and the paths are messed up on windows:\n**[  1%] Generating E;c:\\DevKit\\shogun\\src\\shogun\\base\\class_list.cpp**\nWhat is the right channel for communication if I work on this further?  Make a fork of shogun and talk there?\n. Cool thanks, I tried building with MSVC and saw some totally different errors.  Code referencing a dead repo 'http://msinttypes.googlecode.com/svn/trunk' and other errors.  Do you have any idea how out of state MSVC builds are?  Is it best to try cross-compiling with gcc?\n. Taking a crack at it https://github.com/macumber/shogun/tree/build_windows\n. ",
    "ratsch": "Vipin (cc'ed) in my lab is maintaining easysvm.\nIs there anything broken?\nGunnar\nOn Oct 24, 2014, at 7:07 PM, Heiko Strathmann notifications@github.com wrote:\n\n@sonney2k @lisitsyn @vigsterkr @ratsch if you have no objections, I will delete this. We cannot maintain such things anyway.\n\u2014\nReply to this email directly or view it on GitHub.\n\n\nGunnar R\u00e4tsch\nSloan-Kettering Institute\nContact info: http://goo.gl/nnRkb\nhttp://twitter.com/gxr\n. ",
    "vipints": "I haven't checked with the recent release version yet, will look into that soon and let you know. \n. ",
    "c3h3": "I have tried to login it with github ... but ...\n\nand finally, I used google login, it's successfully! \nand .... I feel \n\"Wow ... it's awesome ! ... it's a cloud service !\"\n. Actually, I havn't seen the Dockerfile before. \nI just play with this docker image \nhttps://registry.hub.docker.com/u/vigsterkr/shogun-python/\nwith interactive shell mode. \nBecause I need LMNN and Neural Network package (and openblas numpy) , I built it by hands ... \n. For helping build shogun-dockerfile \nIf there is anything I can help, I can !!!\n. @sk413025 thanks for reporting and fixing it !!!\nI don't know why the image built by docker hub is running smoothly on my server and laptop but making a coredump error message on your laptop and AWS server.  (My friend also use this image on AWS and see the same error too ...)\nI am still finding the reason ......\n. @sk413025 thanks for reporting and fixing it !!!\nI don't know why the image built by docker hub is running smoothly on my server and laptop but making a coredump error message on your laptop and AWS server.  (My friend also use this image on AWS and see the same error too ...)\nI am still finding the reason ......\n. @iglesias \nI use Metric Learning (NCA and LMNN) along with DBSCAN to deal with some semi-supervised problems in real application. \nIn 2012, I met the product matching problem comes from e-commerce.\nIn my previous company,  We want to help our customers to track the price of their competitor's products which is also sold in customers' stores.\nSo, we separate out system into threee parts: Crawler + Matching + UI \nI designed the architecture of Crawler part and Matching Part, and finally spent most of time in Matching Kernel. \nTraditionally, these kind of jobs are all done by human labeling. \nIt is hard to work with a machine learning system without humans .... \nBecause if you have training data (well labeling data) , you needn't to learn ... (you just need to sell them and make money ... )\nTherefore, how to use machine learning skills to reduce the cost of human labeling jobs becomes the primary target of this system. \nSecondly, because there are always new products open-shelf and old products removed every day, so it's hard to model into a classification problem. Because once you trained your model, it is impossible to modify the total numbers of classes. \nTherefore, I starts with a clustering method --- DBSCAN \nalong with LMNN to learn the metric from the human labeling data. \nand after one iteration, system will ask labeler to label the outliers of DBSACN and support triples (just like support vector in SVM) in LMNN. \nThe iteration will stop when no data need to label.  \nActually, because the computational cost of LMNN and NCA are very expensive in large scale problem. So, when we are matching the producs' names, we would use a local term-document matrix instead the whole one. After learning metric locally, we would combine then into a global metric to compute the DBCAN clustering. \n. @iglesias \nI use Metric Learning (NCA and LMNN) along with DBSCAN to deal with some semi-supervised problems in real application. \nIn 2012, I met the product matching problem comes from e-commerce.\nIn my previous company,  We want to help our customers to track the price of their competitor's products which is also sold in customers' stores.\nSo, we separate out system into threee parts: Crawler + Matching + UI \nI designed the architecture of Crawler part and Matching Part, and finally spent most of time in Matching Kernel. \nTraditionally, these kind of jobs are all done by human labeling. \nIt is hard to work with a machine learning system without humans .... \nBecause if you have training data (well labeling data) , you needn't to learn ... (you just need to sell them and make money ... )\nTherefore, how to use machine learning skills to reduce the cost of human labeling jobs becomes the primary target of this system. \nSecondly, because there are always new products open-shelf and old products removed every day, so it's hard to model into a classification problem. Because once you trained your model, it is impossible to modify the total numbers of classes. \nTherefore, I starts with a clustering method --- DBSCAN \nalong with LMNN to learn the metric from the human labeling data. \nand after one iteration, system will ask labeler to label the outliers of DBSACN and support triples (just like support vector in SVM) in LMNN. \nThe iteration will stop when no data need to label.  \nActually, because the computational cost of LMNN and NCA are very expensive in large scale problem. So, when we are matching the producs' names, we would use a local term-document matrix instead the whole one. After learning metric locally, we would combine then into a global metric to compute the DBCAN clustering. \n. Wow ... it's cool ... shogun daily !!!\n. Wow ... it's cool ... shogun daily !!!\n. By the way, the day before yesterday, I joint the Docker Hackathon 2014 Event (Taipei branch), and I used Meteor and Docker to build a data mining tutorial platform ... \nhttps://www.youtube.com/watch?v=f1IAuMTUD2k\nIt's a little similar to shogun-cloud, but just for tutorial ... \nand I will use this platform to teach our community to use shogun !!! \n. Here is demo site: \nhttp://dockerhack2014.opennote.info/\n. haha ... because I am really a big fan of shogun !!! \nHere is the event ... (I joint Taipei branch, and also submit it to global hack day event)\nhttp://blog.docker.com/2014/10/announcing-docker-global-hack-day-2/\n. Here is the voting website ... you can see the shogun page and logo is in front of the video !!!\nhttps://www.docker.com/community/globalhackday/24/#18\n. I found its voting website has a lot of bugs ... in displaying names and the link of facebook likes ... \n. You need to replace forward_port as the port you want to forward to host ... something like 8080 or 8081 ....\nand replace outside_ipynbs as the path you want to mount in.\n. Windows and Mac are using boot2docker ... it's another story ....\nplease follow this: http://stackoverflow.com/questions/25560543/boot2docker-on-windows-cant-access-exposed-port\n. If you just want to try it, you can also try it online ...\nhttp://dockerhack2014.opennote.info/howToUse\ngo to the \"Study Material\" and click \"learning shogun\" ... \nBy the way, I add a new feature --- a real-time discussion (chat room) --- for every study material yesterday.\nSo, if you meet any problem on the website, you can leave a message there.\n. I usually use Multiple Kernel Learning for classification, ... it just like this\nhttp://www.shogun-toolbox.org/static/notebook/current/MKL.html\n. You could see this \nhttp://www.shogun-toolbox.org/doc/en/3.0.0/classshogun_1_1CMKLRegression.html\n. I have finished openblas numpy + opencv + shogun docker baed on ubuntu14.04.1\nHere it is: https://registry.hub.docker.com/u/c3h3/u1404-ocv-shogun/\n(Sorry, I will write the document later ... )\nand I also based on this image built shogun sample ipynbs and data \nhttps://registry.hub.docker.com/u/c3h3/learning-shogun/builds_history/75903/\nyou can use the following command to get it \ndocker pull c3h3/learning-shogun:u1404-ocv\nOR ... you can also try it on my platform \nhttp://dockerhack2014.opennote.info/\n( please login with meetup -> click Dockers on the top -> run c3h3/learning-shogun:u1404-ocv -> click \"show instance\" on the right ) \nand finally, here is the execution results generating from my docker images.\n\n\n\n\n\n\n. I have finished openblas numpy + opencv + shogun docker baed on ubuntu14.04.1\nHere it is: https://registry.hub.docker.com/u/c3h3/u1404-ocv-shogun/\n(Sorry, I will write the document later ... )\nand I also based on this image built shogun sample ipynbs and data \nhttps://registry.hub.docker.com/u/c3h3/learning-shogun/builds_history/75903/\nyou can use the following command to get it \ndocker pull c3h3/learning-shogun:u1404-ocv\nOR ... you can also try it on my platform \nhttp://dockerhack2014.opennote.info/\n( please login with meetup -> click Dockers on the top -> run c3h3/learning-shogun:u1404-ocv -> click \"show instance\" on the right ) \nand finally, here is the execution results generating from my docker images.\n\n\n\n\n\n\n. ",
    "srgnuclear": "hi.. I am working on this .. should i go through each header file or is their some other approach??\n. @iglesias, k i will look up the script. In the meantime i manually went through header files in src and found some includes apart from < shogun/... > like < vector > ,< stdio.h > ,< glpk.h > ,< cmath > ,< cassert > ,< math.h > ,< limits.h > ,< ctype.h > ,< string.h > ,< stdlib.h > ,< time.h > etc do these also have to be removed because these are some basic includes used frequently.\nAlso if this is not having considerable impact should i continue with it?\n. kk. I went through CHMM class and i found the following ifdefs:\nifdef USE_HMMPARALLEL_STRUCTURES\nifdef USE_LOGSUMARRAY\nifdef FIX_POS\nifdef USE_HMMDEBUG\nifdef HMM_DEBUG\nThese are present in config.h.in and cmakelists.txt. I want to know the flow process of how things work. \nLike in cmakelists the above flags are either set of or on in option function : so here is the flag is set to of then why is it defined as 1 in cmakedefine in config.h.in like here i am confused about the correlation between the two things. \nThen an if checks if the flag is set to 1 or not : so here is the value obtained from cmakedefine in config.h.in?\nFurther if the  value is true it is appended to a list : so here i want to know the context of the list.\nAbout the task : \n1) Can u provide me with a reference where options have been made part of the interface or may be solved by templating?\n. i think this may be a bug in MixtureModel_unittest.cc where a semicolon is missing at the end SG_UNREF(mix) .\nLike does the test run in this case ?\nEdit - link https://github.com/shogun-toolbox/shogun/blob/develop/tests/unit/distribution/MixtureModel_unittest.cc\n. should the description of all methods be provided for model class too or only CHMM class?\n. ohk.. ill take that into consideration.\nOn Sun, Mar 1, 2015 at 1:07 AM, Heiko Strathmann notifications@github.com\nwrote:\n\nall public methods (and classes) that are exposed via doxygen should be\ndocumented\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/shogun-toolbox/shogun/issues/2714#issuecomment-76541939\n.\n. in the first commit things were right in classifier but in second commit\nsmthn went wrong i am fixing it.\n\nOn Wed, Dec 10, 2014 at 6:07 PM, Heiko Strathmann notifications@github.com\nwrote:\n\nIn src/shogun/kernel/Kernel.cpp\nhttps://github.com/shogun-toolbox/shogun/pull/2629#discussion-diff-21600376\n:\n\n@@ -992,9 +992,9 @@ template  struct K_THREAD_PARAM\n    /* end (unit row) /\n    int32_t end;\n    /* start (unit number of elements) /\n-   int64_t total_start;\n-   int32_t total_start;\n\nehm, what is going on here?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/shogun-toolbox/shogun/pull/2629/files#r21600376.\n. Yes you are right we dont need these includes here i made the changes and compiled them.\n. Even for this one i compiled removing the #include  and everything seemed fine.\nShould commit these 2 changes and push?\n. Thats true also i went through the file again and their are instances which require the below cplex include too so i am changing both of them back to header file.\n. while transferring back the includes i may have skipped deleting this line so it showed up as modification.\n. I greped for all lapack dot calls in src and currently sdot and ddot have been used. I implemented it in linalg dot.h. \nI need reviews if its the correct way of doing so. Also if there are changes that need to be made please let me know.\n\nEdit: the semicoln in line 153 and 158 after b has been replaced by ',' in the squashed commit.\n. From lapack 2 dot product methods have been used ie cblas_ddot and cblas_sdot in shogun and also the arguments taken by these methods are different than that for the eigen3 dot methods. How can we express these 2 as static T compute( , ) so that all dot product calls can be replace by single compute method without any guards?\nI thought the way i implemented was correct but later i understood the whole picture of whats really to be done.\n. ohk i ll remove sdot from here and change compute_ddot to compute and also document other params.\n. Sorry i missed that i will make changes in cmake and add the commit.\n. No specific reason, as it was under core module i considered _core to be the correct declaration rather then _redux which has been defined separately below under redux module.\nIs it correct or should i revert the change here ?\n. lapack is using cblas_ddot and its defined with arguments other then 2 vectors so how can we pass only vectors?\n. ohk so if we need only 2 vectors as arguments should we implement the procedure as mentioned in the link to calculate the result instead of directly using cblas_ddot?(otherwise we ll end up using 5 arguments)\n. i ll correct that.\n. i havent replaced any of the old description just added a more detailed description in simple language for a newbie to understand. Ill put this description before the methods itself if you feel its correct.\nAlso i made this pr to know what was actually required so i ll make changes and add explanation of the math in this brief section along with what each method does.\n. ",
    "abhinavagarwalla": "In my view, the best way to approach this problem would be through using factory method pattern, i.e. by encapsulating the derived CKMeansMiniBatch and CKMeansLloydImpl into the base Kmeans.\nBy doing so, we would have the two different algorithms in two different classes, plus we can also use it by creating objects of KMeans only. And right now, that is used, which is I think is good when considering ease of the user.\nOne thing that can be done is that instead of using enum, we can encapsulate the derived class for better design.\n. Sorry, but I didn't get what you are trying to say. Would you please mind saying it again.\n. After cloning it, I followed the following steps from README_cmake:\n1. cmake -DBUNDLE_EIGEN=ON -DBUNDLE_JSON=ON\n2. cmake -DSWIG_EXECUTABLE=/usr/bin/swig2.0\n3. mkdir build && cd build\n4. cmake -DCMAKE_BUILD_TYPE=Debug -DENABLE_TESTING=ON -DTRACE_MEMORY_ALLOCS=OFF -DPythonModular=ON -DBUILD_DASHBOARD_REPORTS=ON -DCMAKE_INSTALL_PREFIX=\"$BUILDDIR/install\" ..\n5. cd ..\n6. make -j GoogleMock\n7. make -j all\n8. sudo make install\n9. make test\nThis is the gist for step 4 is here -\n https://gist.github.com/abhinavagarwalla/b161979f16e79a927c6f#file-cmake-build. \nWhat I think the problem is that, build files are written to ~/shogun instead of being written to ~/shogun/build. How should I correct it ?\n. Thanks ! It worked now, without those two steps.\nI had got the steps from here https://github.com/shogun-toolbox/shogun/wiki/README_cmake, it would be really nice if you could update the file.\n. Sorry, I was under the impression that it was good to go. I will make the checks and send the pull request again.\n. Sure.\nI had one thing to clarify, is it right to replace empty braces with SG_NOTIMPLEMENTED since, eventually in the derived classes, they are really being defined ?\n. I did this because by doing this, I would not have change the method call from where it has been called.\nIdeally, this method shouldn't be present in EM interface, but the patch serves the purpose of reusing the code addressing the issue.\nShould I stick with this, or completely remove the method from the EM interface ?\n. Basically, these are just dummy implementation to prevent the class from being fully abstract.\n. Yes, its better that way. I would update it.\n. Sure.\n. I tried that approach, and it also passed all the tests. I just kept the dummy ones considering that whoever wrote them, must have written them for some reason.\nIMO, It may have been written for debugging the the class, maybe.\n. Sorry, I completely misunderstood. \nEarlier before this patch dummy function was implemented for only bool type argument, but since I used Macros instead, the dummy definition was used.\n. Indentation is completely fine in my editor. I am not sure why git is messing it up. Will look for a solution and update.\n. The get implementation is there. Please recheck.\n. First of all, very sorry for the delayed response, as I have my exams going on.\nActually I incidentally converted all my tabs with spaces. Now, I have corrected it.\nSorry for the mistake.\n. I again checked my implementation. I think its completely right. Here, set_vector is a virtual function. So, when it's called the 2nd time, it will bypass the base class implementation and call the implementation of the derived class. Thus, there wouldn't be any recursion. \nMoreover, after looking more closely, I discovered these are in fact dummy implementation. In all cases, the derived class implementation will be called, unless base class implementation is called explicitly, which would hardly be the case practically.\n. ",
    "sanuj": "2645\n. There are three functions related to sorting in SGVector: qsort(), argsort() and is_sorted()\nqsort() looks like this\ntemplate <class T>\nvoid SGVector<T>::qsort()\n{\n    CMath::qsort<T>(vector, vlen);\n}\nSo if we move qsort to CMath then we will have to pass the vector as an argument or we can remove  SGVector<T>::qsort entirely and use the one mentioned in the above patch but in this case we'll need to pass the length as well.\nShould I also move argsort? (moving is_sorted() doesn't make sense to me)\n. What can I try next after UCI examples?\n. I have fixed the python_modular. Sorry missed it before. I noticed that errors in OctaveModular are independent of the datasets I moved. I can fix it but I'm not able to build shogun with -DOctaveModular=ON (some Ubuntu/Octave bug).\n. I also tried to install octave modular from these packages but they aren't available anymore.\nhttp://www.shogun-toolbox.org/doc/en/3.0.0/installation.html\nIs there any other way to install it.\n. @karlnapf I don't know why this failed now. Seems alright from my side. I guess it's not able to install properly.\nImportError: No module named 'modshogun'\n. @karlnapf I read the MKL notebook but it only had classification. I had a look at the unit tests for other regression algorithms: Gaussian Process Regression, LibSVR and Least Angle Regression. In the first two cases 1d noisy sine wave was used for training and testing and lars had some raw data I guess. How was the comparison data decided? I think in GPR, gpml toolbox was used to generate the data and in SVR some easy.py script was used (I found easysvm.py when I searched for it, not sure if this is what you used.) So I am not able to figure out how to go about this. Please suggest some reading resources or something else so I can write unit tests for this.\n. I assume I am not supposed to use the files you added in shogun-data because that's not hot it's done in  other regression unit tests and those files are supposed to be for examples.\n. No problem :)\n. @iglesias You're welcome :-)\n. @lambday @lisitsyn Do we need to add something before merge?\n. @lisitsyn I am interested in deep learning. I have read about neural nets but never really implemented them. Of what I know, rectifier is an activation function max(0,x) and leaky ReLU is when it has some small gradient for x<0. I have used the sigmoid function before but not this(but this should not be a problem I guess). I also had a look at the two papers that you mentioned (text understanding and image classification) and found out that CNNs are rather important for computer vision :) I have never used them but I will definitely read up about them. I know about word2vec that was mentioned in the text understanding paper from the kaggle beginner problem (sentimental analysis) :) So I am relatively new to deep learning but eager to learn.\nI chose this because it seemed easiest of the three. I'll implement the papers after this if no-one is doing that.\nWe already have a CNeuralRectifiedLinearLayer but I guess we will need a separate layer for leaky ReLU. Anything that I should know about before I start?\n. Should we go by adding a new class which inherits NeuralLinearLayer? Something like -\nclass CNeuralLeakyRectifiedLinearLayer : public CNeuralLinearLayer\n. @lisitsyn sorry for the delay. I had my end-semester exams. made the changes you suggested. Please review.\n. @karlnapf It works now after pulling the latest version. Although cmake -DBUILD_META_EXAMPLES .. is still not working. The cookbook looks pretty neat.\n. @karlnapf I'll start working on this. CKernelRidgeRegression already exists. Shall I just create a notebook by using CRandomFourierDotFeatures or is there more to it?\n. I'm cloning shogun-data right now. Will run metric_lmnn_objective.py and eigenfaces.py again after that.\n. I found these bugs while I was playing with KernelRidgeRegression python example and then decided to clean all the graphical examples. I must add, the interactive examples (interactive_svm_demo.py etc) are fun to play with :)\n. 5 tests which were not passing before are now passing.\nBefore:\nThe following tests FAILED:\n    330 - python_modular-evaluation_cross_validation_multiclass_storage (Failed)\n    338 - python_modular-structure_hierarchical_multilabel_classification (Failed)\n    346 - python_modular-preprocessor_fisherlda_modular (Failed)\n    366 - python_modular-modelselection_grid_search_kernel (Failed)\n    403 - python_modular-modelselection_grid_search_krr_modular (Failed)\n    458 - python_modular-neuralnets_simple_modular (Failed)\n    470 - python_modular-modelselection_parameter_tree_modular (Failed)\n    499 - python_modular-serialization_complex_example (Failed)\n    501 - python_modular-features_string_char_compressed_modular (Failed)\nAfter:\nThe following tests FAILED:\n    330 - python_modular-evaluation_cross_validation_multiclass_storage (Failed)\n    338 - python_modular-structure_hierarchical_multilabel_classification (Failed)\n    499 - python_modular-serialization_complex_example (Failed)\n    501 - python_modular-features_string_char_compressed_modular (Failed)\n. Interesting. All my changes in the second commit fail on travis but pass on my laptop. I have shogun 4.0.0 installed on my laptop so I think that's making this happen. I'll revert back the second commit.\n. @lambday  waiting for your comment :)\n. @karlnapf I'm almost done with this. I wanted to discuss some points before I send a PR:\n- All viennacl calls are in pseudo inverse method. Do we want to keep the pseudo inverse method, if yes then how can i replace the viennacl calls?\n- Will there be any case when the kernel matrix is not a square matrix? For LLT, I have assumed that kernel matrix will always be a square matrix.\nLLT in KRR is working, I have tested it. The integration test fail due to some slight changes in decimals of alpha so I need to update the data file. Also, there are no unit test for KRR (shall i add one?).\n. @karlnapf @lambday Dot in linalg currently only takes 2 arguments i.e. the input vectors. I am not sure how linalg::dot is going to work here because we would need the length of the vector also. Maybe we can add 3 arguments in native's dot? Or maybe we can use linalg::dot for all the vectors and CMath::dot for all the non-vectors.\n. What i meant by non-vector part is that if we pass pointers instead of\nvector objects(SGVector). If we pass pointers as it is done in this piece\nof code then we will have to pass another argument to the linalg::dot\nfunction i.e. Length of the vector explicitly.\n. Yeah i understand the cmake problem. Should we introduce HAVE_LINAG_LIB\nwhich would only make the native impementation visible. But then we would\nonly have dot and a couple of other functions only as it is not possible to\nimplement everything natively as you mentioned.\n. Agreed. I thought I would move them together when I move arg_max, max_abs, arg_max_abs. It would be faster and easier this way. I am moving the methods one-by-one because it would be easier to review and clean up as you mentioned. What do you think about me pushing all the commits here, moving individual methods with each commits and then you can merge after we have moved all the fuctions from SGVector to CMath?\n. @iglesias Instead of doing template function specialization, I have overloaded the functions. Is this fine?\n. @iglesias Why have you defined the template specialization outside the body of CMath? It could have been done inside the body too right?\n. @iglesias Here I had to explicitly call CMath::arg_max<float64_t> () instead of just CMath:::arg_max(), otherwise it was giving an error. Why is that?\n. No I was just curious :)\n. No, it's not working. Just tried doing that.\n. Oh sorry. It worked. Before I tried removing NULL in the method's signature.\n. I am not sure about the parameters of these 2 functions.\n. There are multiple overloaded pow functions, so do I need to copy-paste same doc before each of them or doing it only for the first one will be enough?\n. @karlnapf What do you reckon?\n. @iglesias What shall I add in the description of this function?\n. @iglesias yeah even i noticed this :-D but then I thought it might mean 'cumulative' or something like that. Anyways, I'll change it and make it sum.\n. @iglesias I have added these tests here 'cause you suggested to use the data available.\n. What's wrong with the indentation here?\n. Corrected it. :-)\n. I'm not sure what this parameter is doing exactly. Some help :) I don't seem to understand how the function is sorting. I know what is bubble sort. Not sure how the numbers are being swapped (vertically, horizontally or diagonally?)\n. Of what I understand,\n- if we have eigen and viennacl then \n  EIGEN3=0, VIENNACL=1, NATIVE=2 and DEFAULT=0\n  in which case Backend::DEFAULT is EIGEN3\n- if we have eigen and not viennacl then\n  EIGEN3=0, NATIVE=1, DEFAULT=0\n  in which case Backend::DEFAULT is EIGEN3\n- if we have viennacl and not eigen then\n  VIENNACL=0, NATIVE=1, DEFAULT=0\n  in which case Backend::DEFAULT is VIENNACL\n- if we neither have eigen nor viennacl then,\n  NATIVE=0, DEFAULT=0\n  in which case Backend::DEFAULT is NATIVE\n@lambday ?\n. @lambday How much work is left here?\n. Cool :) i have some holidays now so i would get time to work on the cleanup\nand native implementations. That's why i was asking. Hope you guys merge\nthis soon.\n. Do you mean by adding something like #pragma omp parallel for .... I can do that in this PR itself.\n. Also what all operations would we generally like to do in parallel? Do we always use openmp or something else as well? Just so that I take care of it in future :)\n. @iglesias Done :)\n. Should i instead include <math.h>?\n. @lambday Can we do this in parallel?\n. @iglesias switched to max_element\n. @iglesias Added the error check here and below.\n. @lisitsyn @lambday shall i remove this then? And is there still a need of benchmarking if we remove this?\n. @lambday removed it.\n. @lisitsyn done\n. Shall I typecast it to machine_int_t* like done below or make a new Parameter::add and a new entry in lib/DataType.h?\n. Is this enough or shall i make a new test for CNeuralConvolutionalLayer?\n. @lisitsyn This for loop can be made a private member function if you want as it is getting repeated in lbfgs also.\n. Alright, i'll do it.\nOn 10 May 2015 23:10, \"Sergey Lisitsyn\" notifications@github.com wrote:\n\nIn src/shogun/neuralnets/NeuralNetwork.cpp\nhttps://github.com/shogun-toolbox/shogun/pull/2840#discussion_r30004276:\n\n@@ -408,6 +420,19 @@ int CNeuralNetwork::lbfgs_progress(void* instance,\n        int n, int k, int ls)\n {\n    SG_SINFO(\"Epoch %i: Error = %f\\n\",k, fx);\n+\n-   CNeuralNetwork* network = static_cast(instance);\n-   SGVector gradients((float64_t*)g, network->get_num_parameters(), false);\n-   for (int32_t i=0; im_num_layers; i++)\n-   {\n-       SGVector layer_gradients = network->get_section(gradients, i);\n-       if (layer_gradients.vlen > 0)\n-       {\n-           SG_SINFO(\"Layer %i, Max Gradient: %g, Mean Gradient: %g.\\n\", i,\n\nFor convenience, can we output the layer name as well?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/shogun-toolbox/shogun/pull/2840/files#r30004276.\n. @karlnapf This is one way to take care of m_autoencoder_position but is inefficient. A better way would be to edit linalg convolution to tackle m_autoencoder_position like it's done below in the existing code. If we do the other way then I'll have to edit VIENNACL implementation as well.\n. @karlnapf I'm not sure if linalg is only enabled when c++11 compiler is detected. You can see here. We only need this if HAVE_LINALG_LIB is not always defined.\n. @karlnapf We have had this discussion before in Issue #2667 (in the end). Don't think C++11 was made a compulsory dependency for shogun.\n. gives import error so removing this.\n. NavigationToolbar2QTAgg replaced by NavigationToolbar2QT as the former gives import error.\n. shading is deprecated in matplotlib so removing this parameter. No visual changes in the plots.\n\n/usr/local/lib/python2.7/dist-packages/matplotlib/cbook.py:137: MatplotlibDeprecationWarning: The shading option was deprecated in version 1.2. Use edgecolors instead.\n  warnings.warn(message, mplDeprecation, stacklevel=1)\n. Putting :sgclass:CDenseFeatures gives link error.  It's not there in knn.sg also.\n. http://www.shogun-toolbox.org/CDenseFeatures redirects to http://www.shogun-toolbox.org/doc/en/latest/classshogun_1_1CDenseFeatures.html which does not exist. The correct link for CDenseFeatures is http://www.shogun-toolbox.org/doc/en/latest/singletonshogun_1_1CDenseFeatures.html\n. yeah, i was thinking the same but was not sure. Updating.\n. because i don't want to change m_labels by calling eigen_y = llt.solve(eigen_y); below, so i copied them in a new vector.\n. copy is inevitable since m_alpha is NULL.\n. adding this check here and not in constructor/setter/base class because\n- in constructor or base class labels or kernel may not be defined.\n- setters like set_kernel() and set_labels() can be called in any order so one of these variables can be NULL.\n- train_machine() does all the checking when it's required.\n. I'm calling this method here and not in the constructors because checks should only run when both labels and kernel are set which is not necessary in a constructor.\n. @karlnapf if you look at line 73, the first check can't go in setters/constructors for this use-case:\nkrr = KernelRidgeRegression()\nkrr.set_kernel(kernel)\nkrr.set_labels(labels)\nkrr.train()\nThe second check is explicit for krr. Last check can only be performed when m_labels is not NULL.\n. The matrix inversion method only has one check which is dependent on eigen's llt so can't be taken out of it.\n. @karlnapf There is no function to extract the alphas. CLibLinear is derived from CLinearMachine and get_alpha() is available in CKernelMachine. In the ipython notebook for SVMs w and b are extracted to draw the decision boundary and b is non zero.\n. to reference the website, shall i just put the link? I think that's not the correct way to do it.\n. We can uncomment this to build all the graphical examples but work needs to be done on the graphical examples to make them build without error. Currently only classifier_gaussian_process_binary_classification.py is tested.\n. @lisitsyn That is what I wanted to confirm. It calls fire fire_updated(tag_id); which is a virtual function. What is the purpose of fire_update(). I have taken this from aer.\n. @lisitsyn and they are defined in the header file here.\n. @lisitsyn this was commented in aer so i left it like that for you to see. tag_for uses anytag of Context which was not implemented in aer. Context has std::unordered_map<std::string, Any> m_tags;  but i don't thing anything is getting stored in m_tags as of now. I think anytag() is supposed to query m_tags.\n. I tried to do something like this but it was causing problems in translation:\nNeuralLayers layers()\nDynamicObjectArray all_layers =  layers.input(num_feats).linear(50).softmax(2).done()\nNeuralNetwork network(all_layers)\n. This is incomplete. If I uncomment the lines below then I get problems in translation. I think cookbook lacks the functionality to assign values to a class member variable.\n. If I want to use the same CombinedKernel then I would need to change the kernel matrix for CustomKernel by using set_full_kernel_matrix_from_full which is not possible with meta language as get_kernel() returns Kernel* and not CustomKernel*:\nCustomKernel k = combined_kernel.get_kernel(0)\nk.set_full_kernel_matrix_from_full(poly_kernel_mat_test)\nOther way is to delete the old CustomKernel and append a new one. @karlnapf do you want me to do this?\n. @karlnapf or i can also remove CustomKernel and only keep PolyKernel and GaussianKernel. This way I can just reuse the old CombinedKernel without any problems.\n. @karlnapf This paper is cited in MKL.h documentation.\n. @karlnapf i just started a PR for kernel svm #3281. We can change this once this gets merged.\n. @karlnapf Even after the changes by @lisitsyn, it gives segfault.\nauto num_feats = features_train->get_num_features();\nauto layers = some<CNeuralLayers>();\nlayers = layers->input(num_feats);\nlayers = layers->linear(50);\nlayers = layers->softmax(2);\nauto all_layers = layers->done();\nAbove is the cpp translation. Segmentation fault occurs in the 4th line.\n. @karlnapf if you find this appropriate, i can send a separate patch for this.\n. binary mkl cookbook?\n. @karlnapf MulticlassLibSVM only offers ovo. I tried to use MulticlassSVM but got this error. Error is because of line 20 in svm.sg from the gist.\n. I would like to reference multiclass svm cookbook here. #3314 \n. to initialize self, for dpointers\n. This was done to make Any.h independent of shogun.\n. yes\n. Hash is visible via public method hash() so can't remove this. User only has to see Tag and not BaseTag as methods using BaseTag are private.\n. register is a c++ keyword.\n. @karlnapf I tried testing it on local but I think docs are not generated for private methods. We may have to change the doxygen configuration file for this.\n. @karlnapf, @lisitsyn wanted this. I think it's a proof of concept. createX() will be used when we will have plugins. It will load the concerned plugin and return an object. Currently we don't have plugins so I used new_sgserializable. @lisitsyn wanted to see if it works in python, and it is working. I plan to include this in my tags-swig PR (#3332) because the SWIG interface will be similar to set/get/has.\n. Well, we are not going to keep CMockObject in shogun forever. Once shogun classes start using tags-parameter framework, we can remove this. But I think that will take some time, if you want I can remove the \"temporary comment.\n. This is the Jinja template for the automatically generated file. This comment is in the template so that it also appears in the auto-generated file. I can also use python code to make this comment in the generated file instead of mentioning it in the template.\n. Will add new line here.\n. Need to remove iostream. Do we have logging mechanism or its equivalent in shogun :question: \n. Will replace this by SG_ERROR.\n. need more ideas for testing.\n. @karlnapf @lisitsyn createX() calls loadLibrary() and other functions to return an object of X to the end user.\n. I can't understand why this fails as another_manifest and manifest are exactly same. Need help here in the implementation of operator==.\n. How to compare two Some<> objects as operator== is not defined.\n. @lisitsyn this still fails. manifest1 == manifest3 returns true.\n. @lisitsyn  This doesn't compile due to absence of operator== in Some. Shall I remove this check?\n. @lisitsyn If I add EXPECT_EQ(mock_class, another_mock_class), test will pass although these two are not exactly the same. This is because MetaClass has return true; in its operator==. I think this can be fixed if we compare this->instane()->get_name() in equaltiy operator of MetaClass. But this works only if every class has get_name().\n. @lisitsyn I have changed a bit of error handling here based on http://linux.die.net/man/3/dlerror.\n. yes\n. @sorig I'm trying on my local now.\n. @lisitsyn I wrote the above paragraph and realized then we are now going to use SG_ADD which only has register_member(). How will register_param() be used? Do we need it anymore? Will the new plugin classes also use SG_ADD?\n@OXPHOS This section is incomplete. Will complete it soon.\n. @OXPHOS I just realized that I need to talk about NonOwningValueAnyPolicy and PointerValueAnyPolicy, so will do that soon.\n. I think it should be [For SHOGUN developers](#for-shogun-developers) to make the link work.\n. You need to remove space :arrow_right: [Examples](#examples)\n. Similar points for all the above links in the table of contents.\n. no need of 'the' before easy: 'which allows easy and readable archive....'.\nSGObject can be CSGObject or SGObject.h everywhere. \n. If Cereal is not found, ....\n. here it will be: [Serialization methods inSGObject](#serialization-methods-in-sgobject).\n. CCerealObject class defined in tests/unit/io/CerealObject.h, is derived from CSGObject and used for Cereal serialization unit tests.\n. present tense and register typo.\nIn CCerealObject, we initialize a member SGVector<float64_t> m_vector and register it to the parameter list in constructors:\n. If you use \" cpp \" then there will by syntax highlighting.\n. use `register_member(\"test_vector\", &m_vector);` as m_vector is a member variable. See [tags readme](https://github.com/sanuj/shogun/blob/tag_readme/doc/tags_param.md#register_param--register_member).\n. \"cpp \"\n. S caps in serialization. Typo in sgobject.\n. ... and resets the parameter ...\n. list_param() is not available in the API because I faced some issues with SWIG while implementing it. Will update the readme when list_param() is merged in shogun.\n. Oh yeah, I forgot to mention this.\n. Yes, register_member() should be used because the Any object would use the same member variable's memory. So changing either one of them will automatically change the other one since they both point to the same memory location. This will not happen if we do register_param() because the Any object would use different memory.\n. I have explained this in Any's NonOwningValueAnyPolicy section so didn't mention it again.\n. Plugin architecture is still under development but I think we only plan to support C++11 and later versions. @lisitsyn ?\n. There would be a compiler error since the syntax would be screwd up.\n. I haven't linked these files since these are not yet merged in develop but live on feature/tags branch. But I'm confused, shall I link them now or later (after this gets merged in develop)?\n. This is for SWIG interface, to differentiate between has<T>(string) and has<T>(Tag). I had mentioned this in the docs earlier but heiko made me remove this since these docs are visible to shogun users, and they don't know what SWIG is. Guess I should add this in the readme.\n. blank line will be added when feature/tags is merged in develop.\n. will be automatically fixed when feature/tags is merged in devleop\n. ",
    "sorig": "I don't exactly understand the question either, but it seems like you're trying to compile shogun on Ubuntu 14.04 LTS. \nIn my experience, when compiling shogun with the modular python interface enabled on a clean ubuntu 14.04 LTS install, you have to install the following packages before following the QUICKSTART guide.\nsudo apt-get install the following packages:\ncmake\nbuild-essential\nswig2.0\npython-dev\npython-numpy\nlibeigen3-dev\nlibatlas3-base\nlibblas-dev\nliblapack-dev\nThen follow: https://github.com/shogun-toolbox/shogun/wiki/QUICKSTART\nRemember to run cmake with the argument -DPythonModular=ON\nFinally add the following lines to ~/.bashrc:\nexport LD_LIBRARY_PATH=\"$HOME/shogun-install/lib:$LD_LIBRARY_PATH\"\nexport PYTHONPATH=$PYTHONPATH:$HOME/shogun-install/lib/python2.7/dist-packages\nRestart terminal or run \"source ~/.bashrc\"\nYou should now be able to run the example files for the modular python interface.\n. @karlnapf Sure! Should I just add a section on how to compile specifically for Ubuntu 14.04? These are just the steps that got it working for me on this specific Ubuntu version. It might be different in other cases. \nAlso, currently the liblapack, libeigen, libatlas, and libblas packages are listed under \"Optional libraries to improve performance\" but I couldn't get compilation to work without them (even without the Python interface enabled). Do you have any idea why that is? \n. What arguments did you run cmake with? Please post the exact cmake command including all arguments you're using.\n```\nCMake Error at CMakeLists.txt:850 (MESSAGE):\n Lua could not be found!\nLua is required for compiling lua modular interface!\n```\nIt seems like you enabled the Lua interface. Cmake is complaining that it can't find Lua on your machine. Try installing Lua and running cmake again.\n. Thanks for the feedback everyone! I will get around to the suggested changes ASAP. \nI have been doing some testing with a larger number of example files and it turns out that the pyparsing library used in parse.py is a major bottleneck. I have tried out two alternative approaches: handwriting a LL(1) parser, and using the PLY (python lex-yacc) library instead (which is a LALR parser generator).\nBoth of these alternative solutions are fast (about equally fast), but the handwritten parser is not very maintainable as it relies on hardcoding a LL(1) parse table which requires a moderate amount of knowledge of parsing methods. On the other hand, the PLY library is not part of the standard python distribution like pyparsing is, so using this library introduces a new dependency.\nYou can see the parse.py reimplementation using PLY here\nBelow I've put timing comparisons of parsing and translating 100 example files on my machine. The first is using the current implementation that relies on pyparsing and the second is the reimplementation using PLY\n```\ntime python generate.py \nreal    0m38.380s\nuser    0m37.393s\nsys 0m0.676s\n```\n```\ntime python generate.py \nreal    0m0.665s\nuser    0m0.576s\nsys 0m0.081s\n```\nI would like to hear your opinion on what approach to go with - introducing a dependency on the PLY library or handwriting the parser without any dependencies. \n. Thanks for the feedback everyone! I will get around to the suggested changes ASAP. \nI have been doing some testing with a larger number of example files and it turns out that the pyparsing library used in parse.py is a major bottleneck. I have tried out two alternative approaches: handwriting a LL(1) parser, and using the PLY (python lex-yacc) library instead (which is a LALR parser generator).\nBoth of these alternative solutions are fast (about equally fast), but the handwritten parser is not very maintainable as it relies on hardcoding a LL(1) parse table which requires a moderate amount of knowledge of parsing methods. On the other hand, the PLY library is not part of the standard python distribution like pyparsing is, so using this library introduces a new dependency.\nYou can see the parse.py reimplementation using PLY here\nBelow I've put timing comparisons of parsing and translating 100 example files on my machine. The first is using the current implementation that relies on pyparsing and the second is the reimplementation using PLY\n```\ntime python generate.py \nreal    0m38.380s\nuser    0m37.393s\nsys 0m0.676s\n```\n```\ntime python generate.py \nreal    0m0.665s\nuser    0m0.576s\nsys 0m0.081s\n```\nI would like to hear your opinion on what approach to go with - introducing a dependency on the PLY library or handwriting the parser without any dependencies. \n. @karlnapf Using a C++ parser generator would introduce a similar dependency although it would probably be more cumbersome to install compared to a python library. Another option is using ANTLR, but the generated parsers it produces depend on the ANTLR runtime, so not much difference here either. \nOffering two versions is an option, but I'm thinking it's quite annoying to maintain two versions of essentially the same program. \nIf we are going to introduce a dependency on a parser generator then I'd say something like PLY is easiest to handle because it's just a matter of installing a python library. The handwritten parser is still a pretty good option although slightly more difficult to maintain. However, assuming that the grammar of the meta-language won't change much, this won't be a very big problem (it would still be easy to add new target languages and maintain the translation program).\nPersonally I'd say PLY is the best option while we're transitioning to using the meta-language as the main way of writing Shogun examples. Once we have a good number of examples in the meta-language and we're confident that the grammar won't change, we can always switch to a handwritten parser and remove the dependency. \n. Alright, let's have two versions then. If the grammar isn't changing much, I guess it's not a big deal anyways.\nHow should we handle this in terms of the build process then? In case PLY isn't installed, running cmake with GENERATE_EXAMPLES enabled will be be very slow. Should we just output a cmake warning telling users that installing PLY will speed up example generation?\n. I have now merged the two parser versions into a parse.py that detects wether PLY is installed and chooses the parser implementation based on that. \n. @karlnapf take a look again. Most of your comments should be addressed now.\n. Oh, I forgot to remove the unnecessary imports. They're gone now. @karlnapf should I squash all these commits?\n. @lambday These things are now fixed :)\n. Now squashed and added minor formatting changes (indentation)\n. I haven't added unit tests yet, but please comment.\n. @lisitsyn I've been busy with dissertation and exams the past month. I will try to implement @lambday's new linear algebra functionality into this PR by the end of next week. Bear with me :)\n. @yorkerlin I do not have time to work on this in the near future. Feel free to fix this work and get it merged :)\n. Can anyone tell me if there is an underlying problem with the container datatype or if my fix is adequate?\n. @lisitsyn The indentation issue is fixed now (the indentation in the original version of the file is also weird). The fix is simply to add an extra check for the container type being CT_VECTOR in the the if statement at line 320 in the original version of the file\n. This should fix it: #3126\n. I'm pretty sure Travis is going to fail for Lua, in which case the fix should be #3127 \n. Fixes problem with enum translations described in #3112 @sanuj @karlnapf \n. I had some trouble compiling on my machine (cmake can't find ctags although it is installed on my machine). \nDoes travis automatically run the examples in languages other than C++ yet?\n. I'm experiencing the same parsing issue with static calls. Looking into it now. Will keep you updated.\n\nOn 2 Jun 2016, at 03:38, OXPHOS notifications@github.com wrote:\nI did a little trick:\nMath math()\nmath.init_random(1)\nNow I can generate consistent data file. If you don't mind setting the seed = 1, I can just send in the integration test dataset generated with Math:init_random(1) for now.\nBut I do have problem working with : and 0..\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub, or mute the thread.\n. @sanuj \nThis Ruby error message is cryptic. You need to install the Ruby modular interface, take the generated ruby file and figure why it doesn't work. To me it doesn't look like it's a meta-language issue, but rather a problem with the SWIG interface.\n\nLet me know if it turns out to be a meta-language issue. In that case you may need to update the targets/ruby.json file.\n. @OXPHOS the meta language does not know what methods the classes have (and whether they're static methods or not). It therefore can't raise errors on mistakes like this. \nmath.init_random(1)\nIs grammatically correct to the meta language, but not semantically correct as the class is called Math and init_random is a static method. This must be checked downstream once the examples are run in the target languages. I.e. it's not enough just to check if the programs  parse - it's important to actually translate them, run them, and then inspect any error messages.\n. Let me know if you need help with this.\nJava error is caused by java.json#L14 not using the correct type. You need to figure out how BoolVectors are mapped to Java and use the appropriate type here.\nC# is also an error with mapped types. Take the generated C# file, figure out how to make it compile, and update the csharp.json file.\ncartree.cs(26,22): error CS1502: The best overloaded method match for `CARTree.CARTree(BoolVector, EProblemType, int, bool)' has some invalid arguments\n/opt/shogun/build/src/interfaces/csharp_modular/modshogun.dll (Location of the symbol related to previous error)\ncartree.cs(26,34): error CS1503: Argument `#1' cannot convert `bool[]' expression to type `BoolVector'\nThe errors in the other languages are probably similar, but need investigation.\n. @karlnapf ready for review when travis has finished \nThese changes include:\n- Support for element access and element assignment for SGVectors and SGMatrices\n- Refactor of translate.py: dependency extraction and automatic injection of variable storing is now mostly decoupled from translation. This should make maintenance easier in the future.\n- Support for native construction of SGMatrix/SGVector equivalents in target languages. This is implemented for Python. It is an optional feature so doesn't break the other languages.\nDoes not include:\n- Checking that variables being accessed (e.g. myMatrix[0,2]) are actually of type SGVector/SGMatrix\n. Here is a test file that I used. Would be nice to get a meta example using these features merged, so we can test all the languages.\n. @karlnapf All green. Ready to merge\n. The test file has both a vector and a matrix :)\n. I think this will be fixed when #3413 is merged\n. @sanuj This looks good! Just need to make the enum test work in Java and C#\n. Java error fixed in: #3408\nC# error fixed in: #3410\n. Note that you now need to specify the precision of floating point literals. E.g. 123.456 is a real (64 bit floating point) and 123.456f is a float (32 bit). \nWait for the two pull requests to be merged and then update your test to reflect this :)\n. @sanuj my fixes are merged. This should pass if you update the example with the updated float literal.\n. Sure!\nCould you also just add a few comments explaining the difference between the real and float literals and that example authors should use correctly typed literals? Would be great to have these meta-language tests also work as documentation for example authors\n. Just realised that I accidentally undid the changes that fixed the Java error. See #3416\n. C# should work if you update the example though\n. @sanuj It seems like SWIG is doing something weird when exporting the enums. Java seems to treat the enum values as integers rather than the associated enum type. I don't currently have time to to investigate this further. Could you have a look?\nThere are several possible solutions:\n- Figure out a way to make Java treat the enum values as the associated enum type rather than int (ideal solution)\n- If it turns out we HAVE to treat the enum values as int in Java then I see two possible solutions\n  - Find a solution where we just cast the values to the appropriate type. E.g. translate enum LIBLINEAR_SOLVER_TYPE.L2R_LR to (LIBLINEAR_SOLVER_TYPE)L2R_LR or something like that (you will have to play around with the enum translation rule and the import statements in Java).\n  - Create new syntax for enum variable instantiation. This is the last resort solution - would be quite ugly. \n. @sanuj @vigsterkr See: http://www.swig.org/Doc1.3/Java.html#enumerations\nAre we supporting JDK < 1.5? If not then we should switch the SWIG interface to use proper enums in Java.\n. @vigsterkr this is @sanuj's PR, but I have opened #3422\n. @sanuj OK, #3422 seems to work. I tried to send the change as a PR to your fork, but your fork doesn't show up in the list for some reason.\nInstead, can you cherry-pick my commit to this PR so these changes can be tested together?\nI will close #3422 once you've done that.\n. Travis is green, this can be merged :)\n. With this change we encourage example authors to use correctly typed literals. That is, write \nfloat C = 1.0f\nrather than\nfloat C = 1.0.\nWe need to do this because C# doesn't allow implicit downcasting - the latter statement does not compile in C#.\nAnother reason is that C++ auto keyword infers the wrong type which might get us in trouble once we enable integration testing for the other languages (? depending on how we do this @karlnapf)\nI have changed the current examples to use correctly typed literals. The integration tests fail because of this. Once shogun-toolbox/shogun-data#121 is merged, the tests should pass.\n. Maybe we should just stop serialising and testing native type scalars? Don't we have to get rid of these anyway once we start integration testing the other languages?\n. I have factored out the breaking changes (the ones that depend on shogun-toolbox/shogun-data#121) into #3413 to make this merge a bit easier.\n. OK, this one can be merged now\n. Ready to be merged\n. This is only a minor change of the include path heuristic. It would be a lot cleaner to get ctags to only output paths for classes/enums/typedefs that are available from the modular interfaces. Does anyone have an idea how to make that happen?\n. For each include path candidate we could check if the path exists in an %include directive in one of the .i files. Maybe that would work?\nCan we get SWIG to output us a list of all includes, so we don't have to parse the .i files in a hacky way?\n. Closing as this is now part of #3390\n. @karlnapf have a look :)\n. I'll update the type map now\n. @karlnapf that error is fixed now. Forgot to remove the old BoolVector initialiser translation rule.\nWe get this now. Seems like set_element and get_element aren't exposed in the swig interface. I checked the python interface and they aren't there either.\n. @karlnapf close this one\n. I don't think this is actually an issue with java. I found more files. Updates to follow. Can you write a bit more context on this? How does the language need to be extended?. I cannot reproduce this, but it sounds like a race condition caused by PLY caching the generated parser in the parsetab.py file on disk. If the parser generator is invoked by multiple processes, this could cause unpredictable behaviour. Potential solutions:\n\nDisable parser caching in PLY when using parallel builds (is it possible to pass a parameter controlling this from CMake?). This would cause PLY to re-generate the parser for each file being parsed - I don't know how big a performance hit this would be, but it would grow with the number of files we need to parse.\nMaybe we can name the cached parsers based on pid to avoid the processes using the same cache. Downside: a new parser file is generated per process and these are reproduced each time you compile.. @orivej Can you elaborate on how you reproduce this exactly? Does it occur just by\nmake -j meta_examples\n?. I can now reproduce the problem and I've confirmed that @orivej's fix works. Thanks for that!\n\nThe problem with this solution is that a new parser is generated for each example file which defeats the point of caching the parser. I've created an alternative solution which generates the parser before the parallel build is run in #4008. @vigsterkr Makes sense. I thought the other solution was a bit fishy. Have a look again :). The CI errors look unrelated. @karlnapf  I've edited my script with a custom author map that needs review here. All the entries that are '' need to be looked at. None means remove the author. I've added associated emails in a comment next to the author name.. i.e. I need comments on what the following authors should be called\n'Abhijeet': '', # abhijeetkislay@gmail.com\n    'Andrew': '', # tereskin@gmail.com\n    'David': '', # HeyItsDavid@users.noreply.github.com\n    'Dhruv': '', # durovo@users.noreply.github.com\n    'EdgeX': '', # tonmoysaikia@live.com\n    'F0Z': '', # akpraharaj@iitb.ac.in\n    'IOcodegeass': '', # shubhamskv10@gmail.com\n    'Monique': '', # monique@work.(none)\n    'Nightrain': '', # SayapinSasha@gmail.com\n    'Olivier': '', # nguyenolive@gmail.com\n    'Vipin': '', # vipin@cbio.mskcc.org\n    'Vipin T Sreedharan': '', # vipin.ts@tuebingen.mpg.de\n    'Xbar': '', # wishyx@gmail.com\n    'ZhengyangL': '', # liuzhengyang@outlook.com\n    'abhinavmoudgil95': '', # abhinavmoudgil95@gmail.com\n    'abhinavrai44': '', # abhinavrai44@gmail.com\n    'abinashpanda': '', # abinash.panda.ece10@itbhu.ac.in\n    'adit-39': '', # adit.39@gmail.com\n    'ahcorde': '', # ahcorde@gmail.com\n    'asia': '', # asia@asia.(none)\n    'cameron': '', # cameronjml2@gmail.com\n    'christopher': '', # christopher.goldsworthy@outlook.com\n    'cwidmer': '', # cwidmer@tue.mpg.de\n    'deerishi': '', # deerishi@gmail.com\n    'deveshnag1': '', # devesh.nag1@gmail.com\n    'dhruv13j': '', # dhruv13.j@gmail.com\n    'dkostka': '', # dkostka@users.noreply.github.com\n    'dmkorn': '', # daniel.m.korn@gmail.com\n    'foulwall': '', # liuzhengyang@outlook.com\n    'frank0523': '', # zhangyuyu2008@gmail.com\n    'hongguang guo': '', # thinkdeeper@hongguangdeMacBook-Pro.local\n    'hongguangguo': '', # ghghaut@gmail.com \n    'hwl596': '', # hiroshima596@gmail.com\n    'itsuper7': '', # itsuper7@gmail.com\n    'jacobw': '', # jacobw@localhost.(none)\n    'jey1401': '', # jey1401@gmail.com\n    'khalednasr': '', # k.nasr92@gmail.com\n    'kislayabhi': '', # abhijeetkislay@gmail.com\n    'lionelc': '', # ljiang@cct.lsu.edu\n    'liqiang': '', # liqiang036@126.com\n    'mickyaero': '', # mickydroch@gmail.com\n    'minxuancao': '', # michellecao95@gmail.com\n    'monicadragan': '', # monique@work.(none)\n    'parijat': '', # mazumdarparijat@gmail.com\n    'pl': '', # pl@pl-HVM-domU.(none)\n    'ptizoom': '', # ptizoom@gmail.com\n    'rahul': '', # rahul@localhost.localdomain\n    'ryan': '', # ryan@shoeshine.cc.gt.atl.ga.us\n    'sudk1896': '', # sudk1896@gmail.com\n    'theaverageguy': '', # yash14123@iiitd.ac.in\n    'thereisnoknife': '', # thereisnoknife@gmail.com\n    'tklein23': '', # tklein23@users.noreply.github.com\n    'trevor': '', # trevor.ballard@outlook.com\n    'vipints': '', # vipin@cbio.mskcc.org\n    'yorkerlin': '', # yorker.lin@gmail.com\n    'youssef': '', # youssef.emad.attia@gmail.com\n    'zhh210': '', # zhh210@lehigh.edu. I'll re-run the script on develop and push the changes here once the author renaming is done :). OK, I've added the final author name changes and re-run the script.\nA few files that were in the GPL->BSD list are no longer there. Looks like they have been removed in the meantime. These are the files:\nexamples/undocumented/libshogun/streaming_vowpalwabbit.cpp\nexamples/undocumented/libshogun/streaming_vwfeatures.cpp\nsrc/shogun/io/streaming/StreamingVwCacheFile.h\nsrc/shogun/io/streaming/StreamingVwCacheFile.cpp\nsrc/shogun/io/streaming/StreamingVwFile.cpp\nsrc/shogun/io/streaming/StreamingVwFile.h\nsrc/shogun/features/streaming/StreamingVwFeatures.h\nsrc/shogun/features/streaming/StreamingVwFeatures.cpp\nsrc/shogun/multiclass/tree/VwConditionalProbabilityTree.cpp\nsrc/shogun/multiclass/tree/VwConditionalProbabilityTree.h. Just added Vojtech to those 5 files :). I see. Those files weren't changed because the comments don't have the actual GPL license text in them. Maybe we need to change them manually.\nI guess we need to go through all files outside the gpl sub directory and check whether they have the BSD comment text or not.. @karlnapf Can we get a real example of this instead of my examples/meta/src/meta_api/kwargs.sg file? That would make it easier to change all the other target .json files (and see what edge cases I've missed). Yeah I know, but they would fail anyway since I've only added Python support for now.\nWill update with a real example and see how it goes. This piece of python code \nk2 = GaussianKernel()\nk2.put(\"lhs_equals_rhs\", True)\nThrows the following error\n386/392 Test #364: generated_python-meta_api-kwargs ........................................................***Failed    0.20 sec\nTraceback (most recent call last):\n  File \"/opt/shogun/build/examples/meta/python/meta_api/kwargs.py\", line 44, in <module>\n    k2.put(\"lhs_equals_rhs\", True)\nRuntimeError: [ERROR] In file /opt/shogun/src/shogun/base/SGObject.h line 349: Type for parameter with name \"lhs_equals_rhs\" is not correct.\nwhile this piece of C++ code is fine\nauto k2 = some<CGaussianKernel>();\nk2->put(\"lhs_equals_rhs\", true);\nlhs_equals_rhs is declared as a C++ bool in CKernel. Is this a problem with a type map somewhere?\n\n~~UPDATE: I changed the example to use integers instead of booleans. It looks like there is an underlying problem with booleans and some of the interfaces. This may need to be investigated.~~ I removed the boolean parameter from the example as changing to integer literals made the C++ version fail. Similarly, as the only language octave is complaining about:\nk = GaussianKernel();\nk.put(\"log_width\", 2.0);\nthrowing this error:\n[ERROR] In file /opt/shogun/src/shogun/base/SGObject.h line 349: Type for parameter with name \"log_width\" is not correct.\nerror: C++ side threw an exception of type shogun::ShogunException\nerror: called from\n    /opt/shogun/build/examples/meta/octave/meta_api/kwargs.m at line 37 column 1\nwhile the equivalent code in all other languages works fine. @lisitsyn Here's an issue related to the SWIG interfaces and the parameter framework. In python, I can't see what the parameters of an object is:\n```\nIn [1]: from shogun import GaussianKernel\nIn [2]: k = GaussianKernel()\nIn [3]: k.parameter_names()\nOut[3]:  *' at 0x11408e458>\n```\nWhich means I have to dig around in C++ code to find the parameter names. Can this be fixed?. @vigsterkr Amazing, thanks! Crucial functionality for people working from the swig interfaces. Yeah, it looks like those interfaces just broke with the change. Is it really necessary to have std::set exposed on the interfaces? It seems to me that parameter_names would be absolutely fine as a ~~std::list~~ std::vector. Are there examples in the code base where std::set is absolutely necessary?. Great, thanks a lot! . All green except for Octave (see error description above) which seems to be a bug with the octave interface and float types. I'm not currently able to install octave on my macine to figure out what the issue is. Is there a server somewhere where I can test all the interfaces?\nIt looks like there are issues with basic types in several of the interface languages. Would be good to write a thorough test case once this PR is merged.. OK, included last review changes. This is ready to be merged. @karlnapf just seeing if this passes. May solve the issue you were seeing. Just rebased and added the test. A cool! I think we actually anticipated this change when doing kwargs. I documented how to do it in the original PR #4128 \nI think the approach documented there still works :). Fixed in #4469 \nI have no idea how I missed that!. We could support statements like that but if you are suggesting that the meta-language should actually look up the parameter names in the Shogun API in order to get the positions of the named arguments, that would complicate things quite a bit (a lot actually).\nFurthermore, when the examples are translated, these kinds of named arguments would be lost in all target languages that do not support this feature.\nSay you have a meta-language example with the line (supposing the meta-language now supports this kind of statement):\nGaussianKernel kernel(size=10, width=2)\nThen this could potentially be translated to a python program as:\nkernel = GaussianKernel(size=10, width=2)\nBut in Java this would just be translated to:\nGaussianKernel kernel = new GaussianKernel(2, 10);\nSo for many target languages this wouldn't be very useful (keep in mind that most users will only look at the translations, not the meta-language versions). Another approach is to simply write the meta-language examples in a more explanatory way. E.g.\nint size = 10\nint width = 2\nGaussianKernel kernel(size, width)\nWhich would translate well to all target languages.\n. I guess this would be possible. I suspect there might be some complications casting the objects in some languages and target languages may handle string concatenation in different ways, but I think these issues should be manageable. \nMy only concern with this is that the translations may turn out to be more complicated than the meta-language version. One could imagine that for some target language a statement like print x,y,..,z would have to be turned into several print statements with string casts all over the place, which might not be desirable (I can't think of a concrete example of this off the top of my head).\nBut if you think this feature would be really useful, it should definitely be doable.\n. I'm not sure to be honest. I'm not familiar with this class. The typelist is simply scraped from the .i files in src/interfaces/modular as you can see in example-generation/types/gettypelist. \nPlease let me know if I need to do something smarter than that.\n. I copied from cmake/FindJinja2.cmake and adapted it to PLY. \n. Can you explain why they should be registered? Registered parameters are for model selection or am I misunderstanding something?\n. Ah, I see, thank you. Will add these now.\n. Alright. Is there any benefit of not constref'ing?\n. @lisitsyn You're right. I had already switched to argparse for parse.py and generate.py, but have for some reason forgotten about translate.py. Fix: #2821\n. Why are these native types and not e.g. SGVector<int32_t> like the translation rules in /src/interfaces/modular/Library.i?\n. Hmm these imports are annoying. We can define import statements in the \"Dependencies\" field, but I don't know where we'd get the path information from (like \"shogun/features/DenseFeatures.h\"). \nThe meta-language just knows the name of the classes that are being used, not where they're located. Looks like we need are more complex class scraper that includes that info. Would that be fragile to files being moved around in different releases of shogun? What do you think?\n. Well the easiest thing for now would be to just add a comment telling users to take a look themselves at whatever we want to print in the examples.\nIf we want to make a print statement for vectors only, we need to some more complex analysis of the parsed programs so we keep track of what type each variable has. I think it's possible, but would of course require some thought and work. Would any other language need this feature?\n. Oh sorry, just saw your comment about this. The github UI is a bit confusing\n. See #3120\n. There was a bug with the parsing error messages not counting comments as a line, so I've moved this rule into one with the side effect of incrementing the line count like the rule for the NEWLINE token.\n. I can't import some of these from the Python interface, so can't check what type they should have. Maybe they just need to be removed?\n. @karlnapf Here I'm making sure that each translated dependency is unique. I.e. if two different dependencies (an enum and a class) result in the same import translation, then they're squashed. \n. JavaScript. JSON does not support comments, so syntax highlighters make this hard to read. I could also change to markdown and wrap this in a code segment?\n. Fixes the ambiguity warnings. On my Mac it didn't compile because of this\n. @karlnapf this is the rules look now\n. Java and C# don't allow statements that have no side effects like this one.\n~~One solution could be to try putting it in a print statement print enum LIBLINEAR_SOLVER_TYPE.L2R_L2LOSS_SVC. The print statement doesn't work for arbitrary expressions because of C++, but I think it should work for something as simple as this. You may have to modify some of the example/meta/generator/targets/*.json files to get the this working (I think C++ needs an extra import statement for basic printing to work).~~\n~~The other option is to find the simplest/most stable class in Shogun that has a method that accepts enums. The downside to this is that it couples the test with whatever class you choose which is not super clean.~~\n. A third option (and probably the cleanest) is to try to get enum typed variables working. I.e. get the meta-language statement LIBLINEAR_SOLVER_TYPE myVar = enum LIBLINEAR_SOLVER_TYPE.L2R_L2LOSS_SVC to work. I actually don't think this would be very difficult - these types of statements already parse and translate. You just need to make sure that enum types like LIBLINEAR_SOLVER_TYPE are imported correctly in all the languages. That's just a matter of fixing the dependency translation rules in the target .json files. Try updating your example and see what travis says. Let me know if you get stuck.\n. Hey, I just had another look at this. I'm pretty sure the statement LIBLINEAR_SOLVER_TYPE myVar = enum LIBLINEAR_SOLVER_TYPE.L2R_L2LOSS_SVC is going to work in all languages without any modifications to the translation rules.\n. Please use 1.0. We currently use the auto type for C++ translations, so we need to make sure the literals are correctly typed to get the correct serialization for integration testing.\n. Please use 1.0. We currently use the auto type for C++ translations, so we need to make sure the literals are correctly typed to get the correct serialization for integration testing.\n. 1.0 please :)\n. Please update to:\nreal learn_rate = 1.0\nint max_iter = 1000\n. real len = 1.0\nreal decay = 1.0\nSee #3410\n. int on max_iter please :)\n. Order doesn't matter. Specific types have precedence over the generic rules.\n. You're right. Didn't read what was in the generator dependencies list properly. Thanks for feedback!. Saurabh7 -> Saurabh Mahindre. micmn -> Michele Mazzoni. vladislav.horbatiuk@gmail.com -> Vladislav Horbatiuk. Tiramisu 1993 -> Weijie Lin. in src/shogun/converter/ica only:\nKevin -> Kevin Hughes. Merge same authors. lambday -> Soumyajit De. Sanuj -> Sanuj Sharma. MikeLing -> Weijie Lin. pluskid -> Chiyuan Zhang. haha yeah you're right. Will make this consistent. Initial underscores t_ and p_ are enforced by the parsing library, that's why there is some confusion in this file. One of them had a boolean keyword argument earlier, but that fails in python. I\u2019ll revert it and comment it out. What does ruby say? I can disable nested global calls, but that may disable some nested calls that are actually useful.. I think we're just using the wrong syntax. See here. Testing in #4158. Do you think you would need something similar for StaticCall and GlobalCall? Can easily implement but haven't done it for now. ",
    "ianml": "Any updates regarding compatibility with the latest SWIG? Folks have been trying to get this packaged on Homebrew with clang/libc++. Don't have SWIG 2 available to test. The failures on Yosemite are here starting from:\ncd /tmp/shogun20150325-88679-1ri97ez/shogun-4.0.0/build/src/interfaces/python_modular && /usr/local/Library/ENV/4.3/clang++   -DDSFMT_MEXP=19937 -DHAVE_ARPACK -DHAVE_CATLAS -DHAVE_COLPACK -DHAVE_CURL -DHAVE_CXX11 -DHAVE_CXX11_ATOMIC -DHAVE_DECL_SIGNGAM -DHAVE_EIGEN3 -DHAVE_FPCLASS -DHAVE_HDF5 -DHAVE_ISINF -DHAVE_ISNAN -DHAVE_JSON -DHAVE_LAPACK -DHAVE_LARGEFILE -DHAVE_LGAMMAL -DHAVE_LINALG_LIB -DHAVE_LOG2 -DHAVE_MVEC -DHAVE_NLOPT -DHAVE_POWL -DHAVE_PROTOBUF -DHAVE_PTHREAD -DHAVE_PYTHON -DHAVE_SQRTL -DHAVE_SSE2 -DHAVE_STD_ISFINITE -DHAVE_STD_ISINF -DHAVE_STD_ISNAN -DHAVE_STD_UNORDERED_MAP -DHAVE_XML -DSFMT_MEXP=19937 -DSHOGUN -DSWIG_TYPE_TABLE=shogun -DUSE_BIGSTATES -DUSE_BOOL -DUSE_BZIP2 -DUSE_CHAR -DUSE_COMPLEX128 -DUSE_FLOAT32 -DUSE_FLOAT64 -DUSE_GLPK -DUSE_GZIP -DUSE_HMMCACHE -DUSE_INT32 -DUSE_INT64 -DUSE_LZMA -DUSE_LZO -DUSE_REFERENCE_COUNTING -DUSE_SHORTREAL_KERNELCACHE -DUSE_SNAPPY -DUSE_SPINLOCKS -DUSE_SVMLIGHT -DUSE_UINT16 -DUSE_UINT64 -DUSE_UINT8 -D_python_modular_EXPORTS -std=c++11 -stdlib=libc++ -O0 -g  -Wno-c++11-narrowing -fPIC -I/tmp/shogun20150325-88679-1ri97ez/shogun-4.0.0/src -I/tmp/shogun20150325-88679-1ri97ez/shogun-4.0.0/src/shogun -I/tmp/shogun20150325-88679-1ri97ez/shogun-4.0.0/build/src -I/tmp/shogun20150325-88679-1ri97ez/shogun-4.0.0/build/src/shogun -I/usr/include/python2.7 -I/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/numpy/core/include -isystem /usr/local/include/eigen3 -I/usr/local/include -I/usr/local/lib/../include -I/usr/local/Cellar/json-c/0.12/include/json-c -I/usr/include/libxml2    -o CMakeFiles/_python_modular.dir/modshogunPYTHON_wrap.cxx.o -c /tmp/shogun20150325-88679-1ri97ez/shogun-4.0.0/build/src/interfaces/python_modular/modshogunPYTHON_wrap.cxx\ncd /tmp/shogun20150325-88679-1ri97ez/shogun-4.0.0/build/src/interfaces/python_modular && /usr/local/Library/ENV/4.3/clang++   -DDSFMT_MEXP=19937 -DHAVE_ARPACK -DHAVE_CATLAS -DHAVE_COLPACK -DHAVE_CURL -DHAVE_CXX11 -DHAVE_CXX11_ATOMIC -DHAVE_DECL_SIGNGAM -DHAVE_EIGEN3 -DHAVE_FPCLASS -DHAVE_HDF5 -DHAVE_ISINF -DHAVE_ISNAN -DHAVE_JSON -DHAVE_LAPACK -DHAVE_LARGEFILE -DHAVE_LGAMMAL -DHAVE_LINALG_LIB -DHAVE_LOG2 -DHAVE_MVEC -DHAVE_NLOPT -DHAVE_POWL -DHAVE_PROTOBUF -DHAVE_PTHREAD -DHAVE_PYTHON -DHAVE_SQRTL -DHAVE_SSE2 -DHAVE_STD_ISFINITE -DHAVE_STD_ISINF -DHAVE_STD_ISNAN -DHAVE_STD_UNORDERED_MAP -DHAVE_XML -DSFMT_MEXP=19937 -DSHOGUN -DSWIG_TYPE_TABLE=shogun -DUSE_BIGSTATES -DUSE_BOOL -DUSE_BZIP2 -DUSE_CHAR -DUSE_COMPLEX128 -DUSE_FLOAT32 -DUSE_FLOAT64 -DUSE_GLPK -DUSE_GZIP -DUSE_HMMCACHE -DUSE_INT32 -DUSE_INT64 -DUSE_LZMA -DUSE_LZO -DUSE_REFERENCE_COUNTING -DUSE_SHORTREAL_KERNELCACHE -DUSE_SNAPPY -DUSE_SPINLOCKS -DUSE_SVMLIGHT -DUSE_UINT16 -DUSE_UINT64 -DUSE_UINT8 -D_python_modular_EXPORTS -std=c++11 -stdlib=libc++ -O0 -g  -Wno-c++11-narrowing -fPIC -I/tmp/shogun20150325-88679-1ri97ez/shogun-4.0.0/src -I/tmp/shogun20150325-88679-1ri97ez/shogun-4.0.0/src/shogun -I/tmp/shogun20150325-88679-1ri97ez/shogun-4.0.0/build/src -I/tmp/shogun20150325-88679-1ri97ez/shogun-4.0.0/build/src/shogun -I/usr/include/python2.7 -I/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/numpy/core/include -isystem /usr/local/include/eigen3 -I/usr/local/include -I/usr/local/lib/../include -I/usr/local/Cellar/json-c/0.12/include/json-c -I/usr/include/libxml2    -o CMakeFiles/_python_modular.dir/sg_print_functions.cpp.o -c /tmp/shogun20150325-88679-1ri97ez/shogun-4.0.0/src/interfaces/python_modular/sg_print_functions.cpp\n/tmp/shogun20150325-88679-1ri97ez/shogun-4.0.0/build/src/interfaces/python_modular/modshogunPYTHON_wrap.cxx:7694:38: error: expected member name or ';' after declaration specifiers\n    static bool check(PyObject *obj) {\n    ~~~~~~~~~~~                      ^\n/tmp/shogun20150325-88679-1ri97ez/shogun-4.0.0/build/src/interfaces/python_modular/modshogunPYTHON_wrap.cxx:7702:38: error: expected member name or ';' after declaration specifiers\n    static bool check(PyObject *obj) {\n    ~~~~~~~~~~~                      ^\n/tmp/shogun20150325-88679-1ri97ez/shogun-4.0.0/build/src/interfaces/python_modular/modshogunPYTHON_wrap.cxx:7709:36: error: expected unqualified-id\n  inline bool check(PyObject *obj) {\n                                   ^\n/tmp/shogun20150325-88679-1ri97ez/shogun-4.0.0/build/src/interfaces/python_modular/modshogunPYTHON_wrap.cxx:7781:35: error: expected member name or ';' after declaration specifiers\n    static bool check(PyObject *) {\n    ~~~~~~~~~~~                   ^\n/tmp/shogun20150325-88679-1ri97ez/shogun-4.0.0/build/src/interfaces/python_modular/modshogunPYTHON_wrap.cxx:8442:5: error: expected member name or ';' after declaration specifiers\n    {\n    ^\n/tmp/shogun20150325-88679-1ri97ez/shogun-4.0.0/build/src/interfaces/python_modular/modshogunPYTHON_wrap.cxx:8387:42: error: member initializer '_seq' does not name a non-static data member or base class\n    SwigPySequence_Cont(PyObject* seq) : _seq(0)\n                                         ^~~~~~~\n/tmp/shogun20150325-88679-1ri97ez/shogun-4.0.0/build/src/interfaces/python_modular/modshogunPYTHON_wrap.cxx:8392:7: error: use of undeclared identifier '_seq'; did you mean 'seq'?\n      _seq = seq;\n      ^~~~\n. I have a pretty functional formula and have tested the python interface, but one issue there is the result of how the OS X linker works. The path to the python used at buildtime is explicitly linked in the module, which causes a segfault if imported from another python. The fix for OpenCV was to set\nset_target_properties(${the_module} PROPERTIES LINK_FLAGS \"-undefined dynamic_lookup\")\nwith cmake (see https://github.com/Itseez/opencv/pull/3865), which I tried to steal but it didn't seem to have the desired effect. As-is, the formula makes the python bindings optional and --with-python triggers builds from source with the first available python in PATH. However, dynamically loading the python interpreter at runtime would make things more portable for Mac users and allow us to package the python bindings with the binary bottle.\n. ",
    "amroamroamro": "I'm commenting here because you referenced this issue in mexopencv, but I'm not very familiar with shogun... \nSeeing the last screenshot, you cant just pass a shared library shogun.so.16 to the mex command (it assumed a C/C++ source file); If you want to link against it, use the regular syntax of mex ... -L/path/to/libs -lshogun, the linker will find it on its own..\nThat would have been possible if you used a static library: mex ... ./file.cpp ./shogun.a\nRemember that mex is not a compiler itself, just a front-end.\nhttp://stackoverflow.com/questions/6578484/telling-gcc-directly-to-link-a-library-statically\n. I'm not a CMake expert, but the $<TARGET_SONAME_FILE:shogun> part is not right. This needs to be something that returns -L/path/to/libs -lshogun.\nAlso mex -o sg .. needs to be changed as mex -output sg .. for MATLAB. Octave seems to use the -o option instead.\n. ",
    "shubhamgoyal": "I am also facing the same issue. A fix would be really appreciated. Is their any other way to get this to work?\n. ",
    "yarikoptic": "\n@shubhamgoyal, @vigsterkr, @yarikoptic, @rieck: Fix for building\n   python_modular with mp-gcc-49 on Mac OSX 10.10 \"Yosemite\" is upcoming\n   within the next two daysa_| Will reference pull-request within this\n   issuea_| ^^\n\nthanks for the notice, but\nwhy was I supposed to care about the build on OSX?\n. ",
    "AlexLuya": "(Caution:my octave version is 3.8.1)\nIt may be an octave issue,after modifying two files:\n/usr/include/octave-3.8.1/octave/oct.h     line 31:#include <config.h>       ----->   #include \"config.h\"\n /usr/include/octave-3.8.1/octave/oct.h     line 28:#include <base-list.h>   ----->   #include \"base-list.h\"\ncompilation passed without error. \nCorrection:\n@iglesias is right,in octave 3.8.1,blow change should be made at file comment-list.h also.\n/usr/include/octave-3.8.1/octave/oct.h     line 28:#include <base-list.h>   ----->   #include \"base-list.h\"\n. Thanks,you are right,\"modshogun\"  works.By the way,the tutorial:\nhttp://www.shogun-toolbox.org/doc/en/3.0.0/modular_tutorial.html\ntells to use \"init_shogun\",is this a document error?\n. ",
    "aby0": "@karlnapf I was going through this issue and I think what @abinashmeher999 has proposed quite relevant except the use of map. I just want to know whether he is still working on this issue ? I just want to try it out. \n. @karlnapf If assertion is for internal variables then is there a reason to convert them into REQUIRE messages? I mean for user, these things are a black box and she didn't require a feedback for information which she didn't provided.\nExample:\n``` c\n/* ./src/shogun/lib/SGVector.cpp:268 /\ntemplate\nSGVector SGVector::operator+ (SGVector x)\n{\n    ASSERT(x.vector && vector)\n    ASSERT(x.vlen == vlen)\nSGVector<T> result=clone();\nresult.add(x);\nreturn result;\n\n}\n```\n. @iglesias I have quoted the irrelevant example here, but what I was asking is - if something which is not provided by the user then should we change that assertion as well?\n. ",
    "abinashmeher999": "@somya-anand Yup, I am on it. Sorry for the delay because of my midterm exams. Hey, you can review my PR.\n. @karlnapf Shall I use std::vector or shogun::v_array to pass the list?\n. Made the latest edits about indentation issues and returning a pointer.\n. A small question. Will we be making the packages public by setting up a web server using reprepro or using services like Launchpad? because from this link it seems like launchpad currently doesn't support Fedora, Arch Linux, etc. However, I came to know there's Fedora People.org for Fedora. \n. Finally, got to see a protected mode of derivation :relieved: . Curious to know the reason! :)\n. I am using vim. I rechecked it but this line starts just below */ in vim. Don't know why it shows like this here. I have tried to adhere to the coding style.\n. I think I shouldn't cout that the user is supposed to use the other method because he has provided multiple CEvaluation. Overloading the return type is not possible. If not REQUIRE, how should I tell the user to use the other method.\n. Thanks for pointing it out. I understand the logic. I did it because, the corresponding method for a single metric returned by value. If this is the case, I might have to modify the earlier method. Also the method CEvaluation::get_evaluation_direction() itself returns by value of which I am making a list and returning. So, even if I return a pointer, no future changes are going to be reflected.\n. Since, the user passed a CEvaluation, we will have to store it in a list. And the member variable(which is a pointer) that points to that list doesn't point to anything at the construction. I feel we will have to create a new one.\n. I was referring to the unusual alignment with the previous function. It's fixed now. My pre-existing settings in vim were replacing tab by 4 spaces. In the editor everything seemed properly indented. Commenting that out worked.\n. @karlnapf @lisitsyn any ideas regarding this?\n. ",
    "alishir": "@lisitsyn As I found in this blog post @zhangxiangxiao promised to release the Torch sourced code (GPU) in a few weeks.\n. ",
    "arasuarun": "Do we have a completely new Network class which has a Preprocessor (say DimensionReduction) member (sent via enum or object) and applies that on the given input, generating an output which is now the input for the rest of the network? This preprocessing has no part to play in the remainder of the training, right?\n. @lisitsyn (Please excuse the late response) I'm a bit confused in the implementation. I think it'd be better if I had a better view of why this would be used. Could you give me any links to check out? (cause I couldn't find anything related myself)\n. @lisitsyn The entire output loop could probably be put into a separate function and vectorized, too. It could possible be used elsewhere, too.\nWhat do you think? \n. Sorry for the late response @karlnapf, I was a bit preoccupied with exams and stuff. \nReg. this, considering we have PR #2973 (by me), which is a more general implementation, this is no longer required. I'll go ahead and close this. Thanks.\n. @yorkerlin What about this one? :) \n. Okay, I'll remove the enum and just keep a DescendUpdater object as a data member for NeuralNetworks. I think that'll easily wrap up this entire training algorithm.\n. @lisitsyn @yorkerlin I've now changed it to the user setting an object instead of passing an enum. \n. @yorkerlin @lisitsyn , please review. With the user passing the DescendUpdater object, it's not actually a [WIP] anymore, unless you guys would like to have anything else added.\n. @lisitsyn How about we start with HFO? It involves mostly linear algebra: conjugate gradient, R-operator. hfo could probably be a separate module in /optimization like lbfgs. A corresponding train_hfo method could be added to NeuralNetwork as well. \nI'll start a separate issue for this if you want. Else, we can just continue on this one. Thanks.\n. @lisitsyn? \n. @lisitsyn Really sorry for the super late response. I had mid semester exams and submissions for like, 2 weeks.\n1. Where directory should the R Operator (used in computing H*v http://www.bcl.hamilton.ie/~barak/papers/nc-hessian.pdf) be put? \n2. This entire HFO business is huge: should we start with a generic class for 2nd order minimizers first? A Conjugate Gradient minimizer method, maybe? \n. @karlnapf The third commit should've handled all your comments... expect the linalg/openmp one.\n@vigsterkr @karlnapf @lisitsyn To be honest, the entire neuralnets package is filled with for loops and (afaik) doesn't use the linalg library much, either. I think it can all be vectorized and refactored with openmp/shogun's linalg. What do you guys think?\nThanks. :) \n. @karlnapf I'm working on this and #3038. My second segment (a third of a semester) ends this weekend, so I have a lot of pending academic work. But I have all of next week off so I'll finish refactoring with linalg and openmp by then. I hope that's fine. :) \n. @vigsterkr I was wondering why you guys didn't use the linalg library for the logistic layer so I just stuck with the same implementation. I'll modify this one to use it now. Thanks.\n. I think all of those should be fixed now. My tab length in vim was too small and I didn't notice the tab/space issue.\n. Okay. I'll fix all of those right away. \n. I actually wanted to change it. But since it was completely unrelated to what I was actually doing, I decided to save it for another PR. \n. Grouping similar members together. Both of those are only used for RmsProp.  https://www.stack.nl/%7Edimitri/doxygen/manual/grouping.html#memgroup\n. Done. It's on #2973 \n. I edited this PR's description to include some sources. Should've done so at the beginning, sorry about that.\n. Done. I also changed the names from min_val and max_val to min_act and max_act. Thought it made more sense.\n. Done. :) \nBut I do notice that aside form the base NeuralLayer class, none of the other Neural*Layer classes have SG_ADD or init(). Should it be added for them, too? \n. Got it. A nice constructor from SGVector was all that it took.\n. Oh! I misunderstood your intention. Handled it. :P \n. ",
    "saketkc": "I have a pending PR open at homebrew-science for shogun 4.0: https://github.com/Homebrew/homebrew-science/pull/1818\nI have been a linuxbrew user for 6 months now and find it pretty helpful. homebrew-science formulae are also bottled which is essentially an OS, architecture specific packaged binary. The bottles are currently packaged only for Mac OS, however it is in pipeline for Linux distros[See: https://github.com/Homebrew/homebrew-science/issues/1795]\nI wanted to gauge the line of thought behind this issue. Is this not an acceptable way of distribution, given that i does not rely on sudo?\n. ",
    "sotelo": "Has this been taken? I'm trying to get familiar with the shogun toolbox and this looks like a good place to start.\n. ",
    "adit-39": "Will fix that in a couple of hours by taking every file in the src folder\nand its subfolders and only after that generate the list with classes\nmissing unit tests and examples! :)\nEvery *.h (except for template classes) in src is eligible to make this\nlist?\nOn 26 Feb 2015 7:55 pm, \"Heiko Strathmann\" notifications@github.com wrote:\n\nReopened #2730 https://github.com/shogun-toolbox/shogun/issues/2730.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/shogun-toolbox/shogun/issues/2730#event-242518144.\n. @iglesias do you want separate lists of those without unit tests, those without examples and those without both? \n. \n",
    "yingryic": "@karlnapf I checked before, actually the () operator is not used anywhere except for one unit test (ProbingSample test), where it constructed a symmetric matrix, thus the order actually doesn't matter.\n. fixed, take a look @karlnapf \n. take a look @karlnapf \n. changed\n. not sure why the indentation is like this, it appears fine before the pull reques\n. made a function to generate sparse matrix\n. I just follow the test already in the file. Seems we dont need, thus I delete it in the new commit.\n. just make it clear it is unchanged in the function. Pass in reference is better, but leave in this way wont cost much more, it is just ints and floats.\n. I have thought about for a while, I think as long as the sparse matrix and dense matrix use different interfaces, it will be very hard to make the code more elegant.\n1. we can not use the same (templated) function to generate both sparse and dense matrix\n2. we can write a function always generate two matrices (one sparse one dense), but it is not the best solution since some test only need one type of matrix.\nI guess I'll leave it here, if you want to make the two interfaces compatible, I could refine the test in the future.\n. got an idea\n. I use const & to denote the variable is not changed inside the function, * to denoting something is changed. If you prefer not to have the reference here, I'm fine with deleting that.\n. I don't think it is possible to create a sparse and dense matrix and fill in their value inside the function. since you need to use SparseMatrix(numberOfFeats, numberOfVects) but DenseMatrix(numberOfVecs, numberOfFeats) to initialize. Thus it is only possible to initialize the matrix outside the function and fill in their value inside the function.\n. I use spaces, not sure why this happen.\n. @iglesias  yes, I know what you mean, and I also think that is what we should do. \nBut the problem is if we initialize matrix as \"MatrixType matrix(m,n)\" and later use matrix(i,j) to access its contents. For sparse matrix, i goes from 0 to n-1, and j is from 0 to m-1, while dense matrix is i: 0 to m-1, j: 0 to n-1. \nThis is how sparse and dense matrix is currently implemented and what I mean the sparse and dense matrix use different interfaces.\n. fixed typo\n. In case you didnt notice my previous conversation @iglesias \nThe problem is if we initialize matrix as \"MatrixType matrix(m,n)\" and later use matrix(i,j) to access its contents. For sparse matrix, i goes from 0 to n-1, and j is from 0 to m-1, while dense matrix is i: 0 to m-1, j: 0 to n-1.\nThis is how sparse and dense matrix is currently implemented and what I mean the sparse and dense matrix use different interfaces.\n. changed to one line per instruction\n. changed to one line per instruction\n. SG dense matrix has no initialization upon construction, thus the template function cannot generate both sparse and dense matrix with zeros initialized. Thus I choose to initialize the matrix outside the function, and pass in it as an argument to set the nonzero entries.\n. ",
    "vishalbelsare": "Compiler version quite dated at:\nvishal@goedel:~/Desktop/shogun-4.0.0/Build$ g++ --version\ng++ (Ubuntu/Linaro 4.6.3-1ubuntu5) 4.6.3\nYes, /shogun/kernel/Kernel.h has a include directive for \nWould upgrading the compiler help with this?\n. ",
    "fernandorocha": "when I try to build the examples, basic_minimal is built, but got the same error for the others. I'm on a up-to-date ubuntu 14.04 LTS, installed shogun from repository.\n. ",
    "tengshaofeng": "when i make -j5, it still have error like \"/tmp/ccfDSvzN.s:899: Error: no such instruction: vfmadd312sd 232(%rsp),%xmm4,%xmm14'\n/tmp/ccfDSvzN.s:1158: Error: no such instruction:vfmadd312sd 232(%rsp),%xmm2,%xmm12'\n/tmp/ccfDSvzN.s:1182: Error: no such instruction: vfmadd312sd 240(%rsp),%xmm0,%xmm13'\n/tmp/ccfDSvzN.s:1263: Error: no such instruction:vfmadd312sd 232(%rsp),%xmm2,%xmm1'\n/tmp/ccfDSvzN.s:1779: Error: no such instruction: `vfmadd312sd 232(%rsp),%xmm10,%xmm8'\nmake[2]: * [src/shogun/CMakeFiles/libshogun.dir/preprocessor/HomogeneousKernelMap.cpp.o] Error 1\nmake[1]: * [src/shogun/CMakeFiles/libshogun.dir/all] Error 2\nmake: *** [all] Error 2\n\"\n. I cmake ,then i face the following error.Sould I install MOSEK,ATLAS and so on?\ntbq@tbq-All-Series:~/shogun-4.0.0/build$ cmake -DCMAKE_INSTALL_PREFIX=\"$HOME/shogun-install\" / -DMatlabStatic=ON ..\n-- Could NOT find CCache (missing:  CCACHE) \n-- Found SWIG: /usr/bin/swig2.0 (found suitable version \"2.0.4\", minimum required is \"2.0.4\") \n-- Using system's malloc\n-- Could NOT find MOSEK (missing:  MOSEK_DIR MOSEK_INCLUDE_DIR MOSEK_LIBRARY) \n-- A library with BLAS API found.\n-- A library with LAPACK API found.\n-- Could NOT find ATLAS (missing:  ATLAS_LIBRARIES ATLAS_INCLUDES) \n-- Could NOT find CBLAS (missing:  CBLAS_LIBRARY) \n-- Could NOT find GLPK (missing:  GLPK_LIBRARY GLPK_INCLUDE_DIR GLPK_PROPER_VERSION_FOUND) \n-- Could NOT find CPLEX (missing:  CPLEX_LIBRARY CPLEX_INCLUDE_DIR) \n-- Could NOT find Eigen3 (missing:  EIGEN_INCLUDE_DIR) (Required is at least version \"3.1.2\")\n-- Could NOT find OPENCL (missing:  OPENCL_INCLUDE_DIR) ATLAS\n-- Could NOT find ViennaCL (missing:  VIENNACL_INCLUDE_DIR OPENCL_INCLUDE_DIRS OPENCL_LIBRARIES) (Required is at least version \"1.5.0\")\n-- checking for one of the modules 'libColPack>=1.0.9;ColPack>=1.0.9'\n-- Could NOT find NLOPT (missing:  NLOPT_LIBRARY NLOPT_INCLUDE_DIR) \n-- Could NOT find LPSOLVE (missing:  LPSOLVE_LIBRARIES LPSOLVE_INCLUDE_DIR) \n-- Could NOT find ColPack (missing:  COLPACK_LIBRARIES COLPACK_LIBRARY_DIR COLPACK_INCLUDE_DIR) \n-- Could NOT find ARPREC (missing:  ARPREC_LIBRARIES ARPREC_INCLUDE_DIR) \n-- Could NOT find Doxygen (missing:  DOXYGEN_EXECUTABLE) \n-- checking for one of the modules 'libjson>=0.11;json>=0.11;json-c>=0.11'\n-- Could NOT find LibXml2 (missing:  LIBXML2_LIBRARIES LIBXML2_INCLUDE_DIR) \n-- Could NOT find HDF5 (missing:  HDF5_LIBRARIES HDF5_INCLUDE_DIRS) \n-- Could NOT find CURL (missing:  CURL_LIBRARY CURL_INCLUDE_DIR) \n-- Could NOT find BZip2 (missing:  BZIP2_LIBRARIES BZIP2_INCLUDE_DIR) \n-- Could NOT find LibLZMA (missing:  LIBLZMA_INCLUDE_DIR LIBLZMA_LIBRARY LIBLZMA_HAS_AUTO_DECODER LIBLZMA_HAS_EASY_ENCODER LIBLZMA_HAS_LZMA_PRESET) \n-- Could NOT find SNAPPY (missing:  SNAPPY_LIBRARIES SNAPPY_INCLUDE_DIR) \n-- Lzo includes and libraries NOT found. \n-- Spinlock support found\n-- Linear algebra uses c++11 features. Please use a supported compiler\n-- Found PythonLibs: /usr/lib/libpython2.7.so (found version \"2.7.3\") \n-- Found NumPy: version \"1.9.0\" /usr/local/lib/python2.7/dist-packages/numpy-1.9.0-py2.7-linux-x86_64.egg/numpy/core/include\nCMake Error at cmake/FindOctave.cmake:125 (if):\n  if given arguments:\n\"LESS\" \"3.8.0\"\nUnknown arguments specified\nCall Stack (most recent call first):\n  CMakeLists.txt:1082 (FIND_PACKAGE)\n-- Configuring incomplete, errors occurred!\nSee also \"/home/tbq/shogun-4.0.0/build/CMakeFiles/CMakeOutput.log\".\nSee also \"/home/tbq/shogun-4.0.0/build/CMakeFiles/CMakeError.log\".\n. ",
    "erogol": "that would be great \n. ",
    "mepatterson": "Okay, so using the latest from develop branch compiled and installed properly to my \"shogun-install\" directory (created 'include', 'lib', and 'share' directories). I cmake'd it with the ruby modular interface ON. But I'm having a disconnect on how to get the 'modshogun' file available? All of the tutorials and example docs suggest that in ruby I can just do require 'modshogun' but that is throwing a LoadError: cannot load such file -- modshogun ... where is this file created? How do I get access to it now that I've compiled the toolbox to my \"shogun-install\" directory.\n. Hi lisitsyn... I haven't had a chance to go back and try it yet. I'll let you know once I do.\n. ",
    "aagarwal-gtr": "Hello I have modified the QUICKSTART page in the wiki. The modified page can be found in my forked wiki repo https://github.com/aagarwal-gtr/shogun-wiki.git\nPlease merge.\n. ",
    "tandakun": "The problem was caused by the files located in \u2018examples/undocumented/libshogun/*\u2019 due to the lack of '-ldl' in link.txt file. I don't know whether it is necessary to explicitly add the library link, but the libshogun was successfully complied when I added a new command, \n\nSET(POSTLINKFLAGS \"-ldl\")\n\non line 21 in CMakeLists.txt\n. @besser82 \nthanks for your attention! the information you care are on the list below.\n\nCentOS release 6.5(Final)\nLinux version 2.6.32-431.el6.x86_64 \ngcc version 4.4.7 20120313 (Red Hat 4.4.7-4) (GCC) \ng++ (GCC) 4.4.7 20120313 (Red Hat 4.4.7-11)\ncmake version 2.8.12.2\n. \n",
    "hrw": "I had that feeling that I forgot something... \nBug reported: https://bugzilla.redhat.com/show_bug.cgi?id=1222401\n. Besser82: the solution is not to skip failed tests. The solution should be fixing of those tests to make them work.\nI hope to make a build on Power8 machine (where FPU follows IEEE754-2008) to check what kind of results it will give. AArch64 uses IEEE754 as well. x86 should use it too but it is x86 - architecture with such pile of mess that anything can happen there.\n. Besser82: this bug was not about Fedora packaging but about code. Please reopen.\n. Similar failures on PPC64LE:\nThe following tests FAILED:\n          1 - integration-python_modular-SVMOcas_30_1en05_16_05_False (Timeout)\n          3 - integration-python_modular-SVMOcas_30_1en05_1_05_False (Timeout)\n         15 - integration-python_modular-SVMOcas_15_1en05_1_05_False (Timeout)\n         16 - integration-python_modular-LibSVR_023_1_15_Gaussian_1en05_001 (Timeout)\n         17 - integration-python_modular-LibSVR_30_1_15_Gaussian_00001_0001 (Timeout)\n         18 - integration-python_modular-LibSVR_30_1_15_Gaussian_1en05_001 (Timeout)\n        114 - integration-python_modular-tester-kernel_poly_modular (Failed)\n        119 - integration-python_modular-tester-distance_sparseeuclidean_modular (Failed)\n        130 - integration-python_modular-tester-distance_cosine_modular (Failed)\n        138 - integration-python_modular-tester-distance_geodesic_modular (Failed)\n        147 - integration-python_modular-tester-classifier_multiclasslibsvm_modular (Failed)\n        168 - integration-python_modular-tester-converter_locallylinearembedding_modular (Failed)\n        173 - integration-python_modular-tester-kernel_sparse_poly_modular (Failed)\n        180 - integration-python_modular-tester-regression_gaussian_process_modular (Failed)\n        188 - integration-python_modular-tester-preprocessor_prunevarsubmean_modular (Failed)\n        193 - integration-python_modular-tester-kernel_sigmoid_modular (Failed)\n        195 - integration-python_modular-tester-converter_laplacianeigenmaps_modular (Failed)\n        216 - integration-python_modular-tester-kernel_salzberg_word_string_modular (Failed)\n        220 - integration-python_modular-tester-distance_jensen_modular (Failed)\n        221 - integration-python_modular-tester-converter_localtangentspacealignment_modular (Failed)\n        225 - integration-python_modular-tester-preprocessor_dimensionreductionpreprocessor_modular (Failed)\n        229 - integration-python_modular-tester-converter_hessianlocallylinearembedding_modular (Failed)\n        259 - integration-python_modular-tester-preprocessor_normone_modular (Failed)\n        277 - integration-python_modular-tester-classifier_svmocas_modular (Failed)\n        297 - integration-python_modular-tester-classifier_libsvm_modular (Failed)\n        298 - integration-python_modular-tester-converter_kernellocallylinearembedding_modular (Failed)\n        308 - integration-python_modular-tester-regression_libsvr_modular (Failed)\n        311 - integration-python_modular-tester-kernel_wave_modular (Failed)\n        325 - unit-DirectSparseLinearSolver (SEGFAULT)\n        331 - unit-LogDetEstimator (SEGFAULT)\n        352 - unit-Statistics (SEGFAULT)\n        457 - unit-BAHSIC (Failed)\n        574 - libshogun-converter_localtangentspacealignment (SEGFAULT)\n        626 - libshogun-converter_hessianlocallylinearembedding (SEGFAULT)\n        631 - libshogun-converter_kernellocallylinearembedding (SEGFAULT)\n        643 - libshogun-converter_laplacianeigenmaps (SEGFAULT)\n        652 - libshogun-converter_locallylinearembedding (SEGFAULT)\n        728 - python_modular-mathematics_logdet (OTHER_FAULT)\n        744 - python_modular-converter_locallylinearembedding_modular (SEGFAULT)\n        771 - python_modular-converter_laplacianeigenmaps_modular (SEGFAULT)\n        797 - python_modular-converter_localtangentspacealignment_modular (SEGFAULT)\n        801 - python_modular-preprocessor_dimensionreductionpreprocessor_modular (SEGFAULT)\n        805 - python_modular-converter_hessianlocallylinearembedding_modular (SEGFAULT)\n        873 - python_modular-converter_kernellocallylinearembedding_modular (SEGFAULT)\n        1025 - ruby-converter_hessianlocallylinearembedding_modular (OTHER_FAULT)\nErrors while running CTest\nBuild with disabled R support.\n. qemu is one way. Getting access to AArch64 or Power machine is other - but I do not know any publically accessible ways now.\n. https://www.linaro.cloud/ is one way. But I do not know how long you would wait.\n. http://arm.koji.fedoraproject.org/koji/buildinfo?buildID=362126 has build logs.\n. ",
    "wcalhoun516": "I tried:  \"-L/path/do/that/dir and the -lshogun flag should be used\" and it successfully compiles but still get a runtime error of \"Library not loaded: libshogun.17.dylib\"\n. On xcode I've set in my scheme that the environmental variable DYDL_LIBRARY_PATH is equal to my   -L/path/to/lbraries  (obviously without the -L). Still no luck.\n. Don't you just love how you can spend hours banging your head against a wall because you didn't recognize a two letter switch? That fixed it. I owe you a debt of gratitude.\n. ",
    "qwertzdenek": "It works! :) thank you!\n. ",
    "SergeySmith": "I've managed to solve the problem by replacing the default compilers (cc and c++) with GCC.\n. ",
    "youssef-emad": "@karlnapf I made this notebook for comparison between Least Square Linear Regression in sklearn and in Shogun.\nAs demonstrated in the notebook it seems that Shogun has no problem in speed but there is a problem with accuracy especially when the data shifted away from the zero ( e.g. the third example).\nThe reason is that the solution provided by Shogun's LeastSquareRegression is a linear method with bias 0. As appears in the train_machine method that the bias is not taken into consideration and has to be set manually through set_bias().\nThis can be modified easily by adding the bias as an additional feature and the value of this feature will be one along the observations.\nI searched CFeatures class but I didn't find a  way to add a new feature to a set of features.\nSo an implementation to this issue is to get the feature matrix from the input CFeature then copy it to a larger array with the bias then transform it into CFeature again but I think this implementation is a little bit dirty.\n. @karlnapf I made this notebook as a comparison between Shogun and SKlearn in multiple regression models ( Linear Ridge Regression, Lasso ,LARS , KRR).\n I used 4 datasets of different sizes and dimensions.\nDataset 1: size 400 , Dimensions 13\nDataset 2: size 700 , Dimensions 1\nDataset 3: size 17000 , Dimensions 20\nDataset 4: size 20000 , Dimensions 20\nObservations:\n1- Linear Ridge Regressionand KRR: Shogun is faster with approximately the same RSS.\n2- Lasso: Shogun is faster but It seems like there is something wrong as there was no weight set to zero and the resulted weights seemed close to what achieved using Ridge Regression.\n3- LARS and Lasso: The notebook kernel crashed at the third dataset. I think This happened because the dataset needed a very large number of iterations to converge. This was handled at SKlearn by setting a default maximum number of iterations = 1000. \nAs I understand from the docs that to have a Lasso Regression Model , We can use LARS with parameter lasso=True but the resulted weights are the same.Did I get it wrong ?\n. @vigsterkr yeah sure , I just have an assignment to deliver in a few hours so I'll finish it and check this out :smiley: \n. @karlnapf @vigsterkr I added new 4 benchmarks and made a pull request at the benchmarks repo\nI also made a notebook to compare the ease of use and accuracy between Shogun and Sklearn for 4 different classifiers ( Naive bayes , KNN,QDA,Logestic Regression) and to check large datasets with larger dimensions.\nIt seems Shogun have a problem with large dimensions and large number of classes\nNote: Sorry for the delay , I was stuck with some academic commitments.\n. @karlnapf The changes are easy to make and won't take much time. I was out all day and really tired right now so I will make the pull request tomorrow morning ASAP :smile: \n. @karlnapf check that\nLooking forward forward to your comments.\nnote: I used two instances from each classifier so I can used them at the final plot.\n. @karlnapf i used the new datasets. check it here\nlooking forward to your comments.\n. @karlnapf any updates ?\n. The typos :disappointed: \nok I'll fix them right now.\n. @karlnapf sorry but I don't understand your comment bout Naive Bayes.\n. @karlnapf sure , no problem.\n. @karlnapf The results are exactly the same for the second and the third datasets because they are fixed but the first is randomly generated each time so the results will change ( sklearn results also changes for the first dataset).\nFor the bias calculation,so I will make a function that computes the bias in the base class but it's not clear  to me how it will be called without modifying the train_machine in subclasses.\n. @karlnapf I got it but I have a suggestion. Instead of renaming the train_machine in all subclasses can I override the train() method inherited from the CMachine class and add the optional compute_bias call to it.\n. @karlnapf check this notebook for results.\n. @karlnapf I think I understand your comments and I will make the needed changes.\n. @lisitsyn I added a comment before compute_bias(), Is that what you meant ?\n. @karlnapf done :smiley: \n. yes the LibLinear test were failing with enabling compute_bias\n. sorry but I don't understand your comment. I am \"computing\" the bias only if m_compute_bias is true.\n. for your comments about the error messages , I just copied them from similar checks, I wanted the messages to be consistent with the others.\n. yeah sorry for that.\n. I checked linalg. I don't think there is mean computation.There is sum,max,dot,...etc but there is no mean.\n. From a quick look , I think because bias calculation is considered in LibLinear::train_machine() and is set internally. \n. Actually I started to feel that It won't get merged :smiley: \nI'll make the needed updates and inform you :smiley: \n. I don't think so, I tried it but it didn't work\nEigen supports scalar multiplication and division not addition and subtraction.\n. Actually Liblinear has a variable called use_bias to determine if the bias shall be used and it's mentioned in the comments. I can add a comment like prevent default bias computation\n. hahaha :smiley: \n. ",
    "bollu": "I'd like to pick this issue up. Tagging on this so I can find it later :)\n. I'd like to pick this issue up. Tagging on this so I can find it later :)\n. ",
    "ballardt": "Hmm, pretty sure I would have been online, but I'm using a stock Broadcom card that frequently acts up with Linux, so my connection probably went down at some point.\nBtw I've been busy with some school assignments, but I should be able to remove those unnecessary bits from the cookbook page either today or tomorrow.\u00a0\nThanks!\n```\n\n```\nFrom: Heiko Strathmann notifications@github.com\nSent: Thursday, March 10, 2016 7:20 AM\nSubject: Re: [shogun] Replace an undocumented example by a cookbook page (#3000)\nTo: shogun-toolbox/shogun shogun@noreply.github.com\nCc: Trevor Ballard trevor.ballard@outlook.com\nThis is just the link checker failing, are you online?\n Also pull the latest version, it works on my machine   \n\u2014\nReply to this email directly or view it on GitHub.                            \n. Whoops, didn't mean to leave the labels out of the constructor, my mistake. Thanks for being patient, I'll get to work on making these changes.\n. Ahh, ok. I had no idea submodules existed prior to my last PR, so I was kind of fuzzy on how they worked, but I get the concept now.\n. Btw, if it's possible, feel free to delete my other PRs. Prior to this I'd only ever used git for small projects, so PRs were new to me. I tried to resubmit a pull request after fixing some errors, but saw I wasn't able to (which makes sense: why would you try to merge 2 different versions of the same branch with upstream?), so I (wrongly) assumed you had to cancel the current PR before submitting a new one. I understand now, though, that a PR just references the literal branches. Even with a simpler task like this one I've managed to learn all kinds of stuff about git and software engineering in general :)\n. ",
    "Eejya": "@karlnapf , @vigsterkr I see, I had built a docker image of my forked repository since I wasn't sure what the task was. What would be the right way to begin with this? Also, is this the ppa we are talking about? - https://launchpad.net/~shogun-daily/+archive/ubuntu/ppa\n. ",
    "ellesec": "I'm trying to install shogun 4.1.0, and using R version 3.2.3 located at /opt/local/bin/R\n. If I run the cmake command with -DRModular=ON and -DR_EXECUTABLE=/opt/local/bin/R, I get the following error during make.\n/System/Library/Frameworks/Accelerate.framework/Frameworks/vecLib.framework/Headers/vDSP.h:9871:41: error: \n      redefinition of 'COMPLEX' as different kind of symbol\ntypedef DSPComplex      COMPLEX;\n                                        ^\n/opt/local/Library/Frameworks/R.framework/Resources/include/Rinternals.h:551:12: note: \n      previous definition is here\nRcomplex (COMPLEX)(SEXP x);\n                    ^\n1 error generated.\nmake[2]: ** [src/interfaces/r_modular/CMakeFiles/r_modular.dir/modshogunR_wrap.cxx.o] Error 1\nmake[1]: * [src/interfaces/r_modular/CMakeFiles/r_modular.dir/all] Error 2\nmake: * [all] Error 2\n. ",
    "manantomar": "Sorry for the delay.I have made the above changes and the doxygen documentation seems fine as it is to me.\nThanks for the feedback\n. Hi,\nFirst of all sorry for all the extra commits and the time delay.\nI have made the above changes and added a unit test for the same.\nAlso can you please give me some advice regarding how to run these builds locally (This might be a very silly question but i am quite stuck at this and any hint will be really helpful )\nThanks for the feedback and looking forward to successfully merging this PR. :)\n. Thanks for the comments . I will try the benchmarking now .Hopefully we can get the Kmeans running smoothly :)\n. Sorry but I didn't get what you meant by 3 checks .We have to check if both coordinates are not same at the same time, so how can we split it in 3 ?\n. ",
    "aelnouby": "@besser82 May you help me with this please ? \n. ",
    "amoudgl": "Hi @vigsterkr, I am going through source codes to identify this problem. But I have a silly doubt which is a bit off topic. It is as follows: \nI commented one method apply_to_feature_matrix from class CPCA in PCA.cpp, PCA.h just to test changes using python modular interface. I tried to make python modular interface again using command sudo make python_modular_src in build folder. However, it built python modular interface again but when I again call this method (apply_to_feature_matrix) in python, it doesn't raise any error as it may be using previously built libraries. If I do sudo make in build folder, it starts building all the modules again which takes time. Also I couldn't find any option in Makefile to build preprocessor modules like PCA again. \nSo, what should be general approach to test in python modular interface after making some change in source file say PCA.cpp? Can you please help? Please correct me if I am wrong somewhere. \n. @karlnapf Thanks for your feedback. I'd be happy to go ahead with this. I wrote a python script that can reproduce these results. How should I include these results in a maintainable way? Should I make a new notebook that compares the two toolkits and illustrates time and memory analysis? \n. @karlnapf @vigsterkr I am using the referred framework for PCA benchmarking but I am not able to get the output for shogun. Scikit was done with all datasets in about 50 minutes but Shogun is stuck at last dataset for about 4 hours and script is still running! I let the script running for about 5 hours last night but didn't get any output. So, I stopped the process and tried again today. I'll let you know if I get any results. \n. @rcurtin Yes you are correct. To test only PCA, I used the following command: \nmake run LOG=true BLOCK=shogun,scikit METHODBLOCK=PCA\n@zoq Script is still running. Since I wanted to save the output in database, I set LOG=true, so I am not expecting any results on stdout. Till now this is what I got on stdout. \nI think I'll remove this dataset from configuration file if I don't get output in an hour or so. \n. Well, let me guess. From what I know, PCA has time complexity of O(np^2 + p^3). This dataset has 515345 instances and 90 features. Total number of operations required for this dataset by above complexity is 4175023500. On average, assuming 10^7 operations per second carried out by machine, shouldn't it give the output in around 7 minutes? Correct me if I am wrong somewhere. \n. Yes, ignoring constants is not a correct idea but we can get rough estimate, right? In PCA, we don't do much extra (i.e those calculations which are constant and independent of N and D) calculations other than eigen decomposition, covariance matrix calculation and matrix multiplication to get new dataset. \nShogun uses SVD when number of dimensions(D) are more than number of instances(N). Here, it'll use eigen decomposition of covariance matrix. Earlier when I did the benchmarking, Shogun was much faster than scikit when N > D (results in this thread above). Scikit was done with all datasets in around 45 minutes, so I was assuming Shogun will take much less time. \n. @vigsterkr Okay, I'll take care of it next time. \n. ",
    "rcurtin": "Take a look at the config.yaml and see the block for shogun:\nhttps://github.com/zoq/benchmarks/blob/master/config.yaml#L1381\nThere are a lot of methods and a lot of datasets being run there.  You might want to try instead just running it for PCA (if you aren't already doing that) which you could do like this:\n$ make BLOCK=shogun METHODBLOCK=PCA\nMore information here:\nhttps://github.com/zoq/benchmarks/blob/master/README.md#benchmarking-a-single-libary-and-a-single-method\nI hope that helps.  It's also possible that you might want to remove a few datasets from the configuration so that it runs faster (some are very large).\n. Yeah, the yearpredictionmsd dataset is pretty large; 500k+ points in I think 90 dimensions.  So I'm not too surprised it's taking a long time.\n. The big-O notation leaves out constant factors, so, you can use big-O notation to talk about the scaling of the dataset as you increase its size, but you can't use it to predict how long a particular implementation will take.  I think also the shogun PCA implementation may use SVD on the data or it may calculate the covariance of the dataset and eigendecompose that, depending on the properties of the data.\n. ",
    "zoq": "Even if it runs for hours it should, at least, print some output to standard\nout. Can you show us the command that you used to start the benchmark?\nBtw. you can limit the execution time for each benchmark using the timeout\nparameter.\n. ",
    "OXPHOS": "Hi, \nI had a look at the code. Since linalg does all the matrix computation, I guess I need to implement a Mean module independent of all current modules in linalg.h, and calculate the mean.row and mean.col of matrices? Did I take this too easy?\nI had no experience in coding large projects and I really would like to learn! Thanks!\n. Yes thanks. Read some docs and it seems reading is helpful!\n. Oops..Sorry I had no experience with Latex and I just did a simple replacement.\nWill work on the code. Thanks!\n. I left an error message because I didn't know how to implement ViennaCL but wanted to set a reminder for people following-up. But I agree we can do MeanEigen3 and MeanViennaCL.\n. Sure. I'll add more tests.\n. - I thought about the first point but I was worried I would lose precision for some cases. Will do if this is not a concern.\n- I was wondering why we have so many Sum implementations but just to be cautious I decided to follow all the routines from VectorSum and Sum. I'm not sure about the methods of SGVector and SGMatrix but I can check out and try to squash both in one. \n. Oh yes decltype should work! But for integers if we just do / the mean would still be int right? All I can think of is add an if for whatever Int type and do some tricks but it looks lame..\n. Will change back.\n. I'll try this way. I think returning float mean from int matrix makes more sense.\n. I removed ifdef HAVE_VIENNACLbecause the structure before was like:\n```\nifdef HAVE_EIGEN3\n...\nifdef HAVE_VIENNACL\n...\nendif // HAVE_VIENNACL\nendif // HAVE_EIGEN3\nifdef HAVE_VIENNACL\n...\nendif // HAVE_VIENNACL\n```\nI feel like when HAVE_EIGEN3 is removed there's no need to separate two HAVE_VIENNACL\n. No idea where these weird whitespace come from. I think I need a new editor...\n. Not sure about the LaTeX expression here..\n. Do you mean \\underset{\\mathbf{S}}{\\argmin} ?\nThe online LaTeX editor I'm using return blank for \\argmin but \\arg\\min works...I guess it is somehow outdated?\n. Like:\n```\nifndef SWIG\n  /*..*/\n  template<..>\n  struct int2float {..}\n\nendif\n```\n?\n. Currently all clustering methods are under classifier entry..but it should be clustering\n. Do you mean: Bayesian Reasoning and Machine Learning?\n. @karlnapf aha sorry I missed this..Because int2float type is used in Redux.h and MeanEigen3.h several times as return type. If I hide int2float from SWIG, it's going to be undefined.\n. Sure.\n. Thanks for your comments! @lambday  I am using underscore because there's already CPUVector (or GPUVector or both) in shogun namespace. I'll look into the other points now.\n. I'll checkout. thx!\nBTW there's a similar step in max_product_benchmark, but there're some bugs..It copies an empty vector to SGMatrix and returns a deleted SGMatrix. Not sure whether you want to fix it or I should.\n. I can only think of doing it case by case in conf.py..\nlike :cookbook_linear_svm: '*' redirects to a fixed page\n. @karlnapf maybe you want to check again? This one is complicated..\n. get_gpu_type sounds like to return the name of the type and actually SGVector can be wrapped as either CPU type or GPU type..but I don't have a good name either.. just call it factory..? \n. This class should be able to transfer vectors from CPU to GPU via multiple GPU libraries (and use whatever GPU library the user assigns) in the future\n. The last argument should be min(fm_train_real.shape[0]-1, 30). I dont know where the 30 came from but as the training dataset has 80 rows I just wrote down 30 here.\n. RealSubsetFeatures cannot be identified, while it should be..\n. you mean the publication or the format..? \nThe format is from the BibTeX citing of google scholar..\nand publication is in relaxedtree.h\n. I was using (*m_gpu_impl)[index] to access GPU vector element in unit-tests.\nBut I made some changes here. Ideally, I want [] to return scalar type from GPU whenonGPU()==true and from CPU when onGPU()==false.\nSo, the m_gpu_impl can stay private while the unit-test can still access the element.\nHowever,\n- if I do it as the way I commented out, apparently the two types cannot be converted to each other easily.\n- if I do transferToCPU(), a - it's extremely slow; b - i cannot use it for operator[](index_t index) const; c - I cannot change the value of the element on GPU (may not required by users? I was changing the value in unit-tests).\nSo I don't have very good idea to implement the method here.\n. I have a copy constructor Vector<T>::Vector(Vector<T> const &vector), which requires this const method.\nHowever, the CPU and GPU cannot be synchronized within const method.\nThis might not be a big problem, because if the data in onGPU, the new Vector will have synced GPU and CPU vector. But the old vector won't and they'll be different. Also the const and non-const methods are incoherent. \n. It's nonzeros features but it didn't say where the number 30 came from. \nMy guess is this is the (empirical) number from the paper.\n. @vigsterkr I didn't get your idea here.. If I remove the #ifdef I cannot even finish compiling as m_gpu_impl is initialized here.\n. It calls LinalgBackendBase.dot()\n. There's no smart pointer in Linalg for now but I'm planing to switch them soon\n. @lambday  I have the from_gpu implemented in GPUMemoryViennaCLinstead of GPUBackendViennaCL because the operation requires several members in GPUMemoryViennaCL class.\n. I can but I feel like this is more cleaned up. Or maybe I am wrong.\n. I used smart ptr if that's what you mean\n. @karlnapf  I don't think set_const sets a const vector\n. Ohhhhhhh sry I missed that\n. There's no to_cpu or to_gpu methods in GPUMemoryBase or GPUMemoryViennaCL.\nAlso, as you mentioned, if we do all the transfer in LinalgBakcendViennaCL,  we don't need from_gpu either. I'll remove it.\n. Missing link?\n. I think I figured out..\n. Hey @vigsterkr  may I ask how/where/should this gpu_vec be deleted?\nI think it will be freed by SGVector as the pointer is passed to SGVector constructor but I'm not sure..\n. How do I refer to the vectors? Should I just say vector a and vector b? Or 1st and 2nd?\nWe can display cpu SGVector but not gpu SGVector..\n. I think the file can be named LinalgBackendEigen? We can add other methods to it too.\n. Never mind..we also have gpu_dot_without_gpu_backend in this file.\nWhat is the correct name for that? Also go with LinalgBackendEigen or just use Linalg for all?\n. YES!\n. @karlnapf If I group the examples by backend, I am wondering where this examples should go? Actually it is only testing for linalg namespace method on Travis now but not BackendEigen or BackendViennaCL.\nCan I \n- Leave the memory transfer example intact (or this is what you meant by \"without backend specific operations\"? )\n- Group other operations by backend\n  - Eigen3 \n  - ViennaCL with all tests guarded by #ifdef HAVE_VIENNACL\nEDIT: and remove the current gpu_dot_without_gpu_backend test in dot_unittests as it is supposed to be tested by memory_transfer_to_VIENNACL_without_VIENNACL in memory_transfer_unittests (which is not there yet)?\n. It doesn't work for SGVector but only here.\n. @karlnapf This one is testing the transfer_from_gpu method when there's no gpu backend available anymore. (Actually SGVector_from_gpu_viennacl_without_gpu_backend). So I have to put the vector on GPU first.\nThere's a SGVector_from_gpu_viennacl_without_gpu_backend test in vienna test file.\nDo you still want me to remove it?\n. I think this is the reason some travis tests kept failing..the typo was there since the very beginning..\n. Not sure whether this file needs to be moved around/refactored\n. gives error:\nerror for object 0x7ff2bc0b9af0: pointer being freed was not allocated\nin unit-test\n. i was using the int types. I didn't try complex types..\n. maybe i can send a separate patch after these patches? i'm afraid these minor changes will overload the travis.\n. Sorry I didn't get this..\n. This doesn't look very right..\nTo set a gpu vector, ideally an empty gpu vector with malloced space is provided.\nHowever, there's no way to initiate such gpu vector from the SGVector directly.\nNow, a cpu vector with unknown content is transferred to gpu and the gpu memory is reset by value.\nI didn't have better solutions for now.. @karlnapf @lambday @vigsterkr \n. @lambday right right my bad. I got this method mixed up with other ones. I'll fix this. Thanks!\n. I currently defined the DEFINE_FOR_ALL_PTYPE 3 times separately in LinalgBackendBase, Eigen and ViennaCL, because it is easy to control (undefine) and many types don't work for ViennaCL.\n. yes there's a warning..should i return -1 or it's okay?\n. Self is defined as class in SGObject.h\n. Missing line in several files...\nI just saw them yesterday and I can't help making them right..\n. SO MUCH BETTER\n. oh i was trying to use policy.type() which only gives int or double. I can be more specific if this strategy is okay\n. This test works and I got:\n{\n    \"value0\": {\n        \"value0\": {\n            \"value0\": 5,\n            \"value1\": 0,\n            \"value2\": 1,\n            \"value3\": 2,\n            \"value4\": 3,\n            \"value5\": 4\n        }\n    }\n}\nin which value0 is the length of the vector\nvalue1 - 5 are the entries of the sgvector\n. Edited:\nresult:\n{\n    \"value0\": {\n        \"value0\": {\n            \"value0\": 5,\n            \"value1\": 0,\n            \"value2\": 1,\n            \"value3\": 2,\n            \"value4\": 3,\n            \"value5\": 4\n        }\n    }\n}\n. I got \n{\"filetype\":\"_SHOGUN_SERIALIZABLE_JSON_FILE_V_00_\",\"vector\":{\"type\":\"SGVector<float64>\",\"data\":[]}}\nwhich doesn't seem right\n. yep..it'll be easy to do a grep with cereal_save\n. This is what the examples is like in cereal docs..like:\n```\nint main()\n{\n  std::ofstream os(\"out.cereal\", std::ios::binary);\n  cereal::BinaryOutputArchive archive( os );\nSomeData myData;\n  archive( myData );\nreturn 0;\n}\n``\n. like@see config.h` kind of stuff..?\n. Er..what is the high level idea? ;)\n. @vigsterkr updated - return float mean results from int vectors/matrices\n. I didn't find any available method in SGObject that can load parameters back to variables.\n. What I had here is calculating mean with only one method and cast the result to different type. But I realized that ViennaCL doesn't work that well with complex data and we'll have problem with colwise/rowwise mean operation.\nAnother way is to have independent complex_mean and float_mean in the backends implementation.\n. Can you give an example where they should be?\nWe can output the BaseTag(parameter names) in SGObject.cpp and output the types in each case clause.\n. With discussions in #3375.\nI think we need the unref() - because we allocated new space for loaded data and the old ones are gone.\nFor the same reason I think we need a new RefCount.\nHowever, I cannot do it now without appropriate reset method or serialization method in SGReferenceData.\nThe old load method was:\n```\n    REQUIRE(loader, \"Require a valid 'c FILE pointer'\\n\");\n    unref();\nSG_SET_LOCALE_C;\nSGVector<T> vec;\nloader->get_vector(vec.vector, vec.vlen);\ncopy_data(vec);\ncopy_refcount(vec);\nref();\nSG_RESET_LOCALE;\n\n``\n. @vigsterkr I addedtmpnamandstd::ios_base::failureto this test. Do you want to have a look? It doesn't look very pleasant but it's working at local (I'll try to figure out why travis cannot find cereal files)\n. Yeah I thought I could update this after the SGVector part is done because SGObejct requires the serialization of SGVector anyway. But that one is harder than I thought..\n. I would prefer do it likescale(a, scale, inplace=T)`, which is real \"inplace\".\nI don't see the necessity of allocating the memory in advance like:\nVector output(size);\nscale(a, scale, output);\ncompared to \nVector output = scale(a, scale, output);\n@karlnapf \n. It requires a boolean value to silence the warning..\n. I did this because I got warning when I used return NULL:\n/Users/zora/Github/shogun/src/shogun/mathematics/linalg/LinalgNamespace.h:338:11: warning: implicit conversion of NULL constant to 'bool' [-Wnull-conversion]\n   return __null;\n          ^~~~~~\n          false\nThough I do think NULL makes more sense..\nOr we can leave the warning there @vigsterkr \n. It's only in to_gpu and from_gpu - but they appear in every unittest file which calls the method\n. I can check. I guess it won't be easy to dig out...\n. okay I didn't find exact same scenario in Shogun  (because it seems not achievable) but I can make some changes:\n- SGVector<T>* from_gpu() { return NULL; }\n  or\n-   SGVector<T> from_gpu() { return SGVector<T>(); }\n  or\n-  Use boost::optional\n. The similar cases are all in this PR. I did return 0 for all SG_SNOTIMPLEMENTED macros\n. save_json only works for SGObject. Actually the serialization of sgvector and any are not exposed to users. I just did the internal tests of SGVector and Any objects with the naked(?) save/load methods. \n. I think we need to doc these methods and member since they're only in .cpp?\n. Why do we need this typename U=void here?\n. same typename U=void\n. same typename U=void\n. remove .\n. Default constructor might be better\n. remove {} for the if-else\n. blank line\n. blank line\n. blank line\n. this line can be removed\n. white space\n. split lines\n{\n    return \"MockObejct\";\n}\n. blank line\n. docs\n. I was told to use \"MockObject.h\"\n. blank line\n. minor... remove the * in the blank line and add one between include and */\n. blank line\n. return value of the parameter identified by the input tag\n. Maybe \"is not defined\" is better?\n. indentation\n. E.g.\n. Is it SGObject-based Shogun Class? I don't think structure like SGVector need this framework.\n. Maybe Tag<T> objects is clearly? As theoretically it's still sets<T>(Tag, value)\n. E.g.\n. ... can be used to check if a parameter exists with the given name and type.\n. Not sure whether you need to add comments here: tag name can be found by list_param();\n. It can also be gkernel.has(Tag_name); right?\n. @ref CSGObject::set() and @ref CSGObject::get()\n. links for Any_unittest.cc and Any.h.\n. remove \"using\"\n. I have a question here - \nsay we have registered a parameter as in MockObject.h, register_param(\"int\", m_integer);. We can modify m_integer and pass the modification to parameter list by mockObject.sets<int32_t>(\"int\", m_integer);. However, if once I modified the value (Any) in parameter list directly (such as serialization load), how could I synchronize the value in parameter list back to m_integer? Should we have such method?\n. Or as you mentioned, all members I'd like to synchronize between class this and param lists should be registered by register_member() and use NonOwningValueAnyPolicy?\n. MetaClass<T> is a template class\n. so C++11 is required to run plugin? I didn't see C++11 flags in Manifest.\n. Will there be errors thrown if users don't call the macros by order?\n. With link is better\n. I would suggest renaming the method to rotate_matrix and placing it between range_fill and scale methods.\nIf the method only supports CPU backend calculation, it should be registered in mathematics/linalg/LinalgNamespace.h:\ntemplate <typename T>\nSGMatrix<T> rotate_matrix(int32_t n);\n{\n    REQUIRE(n>0, '');\n    return sg_linalg->get_cpu_backend()->rotate_matrix(n);\n}\nand implemented in LinalgBackendBase.h and LinalgBackendEigen.h.\nThe Eigen3-map method should be called as typename SGMatrix<T>::EigenMatrixXtMap a_eig = a;\nOther Eigen3 methods will stay the same as below.\nAnd a unit-test in tests/unit/mathematics/linalg/operations/Eigen3_operations_unittest.cc is preferred.\nTo use the method, we can include shogun/mathematics/linalg/LinalgNamespace.h  and call linalg::rotate_matrix(n);\nIt's gonna be tricky to do GPU implementation, because there's no data structure passed to the method and the linalg cannot infer the backend to use. We can discuss if we'd like to have the GPU part. \nI can take a closer look once I submit my GSOC final report today : )\n. Oh I was wondering why there's no matrix ;D\nBut yeah it can infer the backend with SGMatrix in param list. So it would be just like other operations. \nI didn't see matrix rotation methods in ViennaCL with a quick search. So we have to start from scratch and maybe use kernels to do the rotation...?\n. @karlnapf I can not reconstruct something comparable to the original matrix with just L and LT from LDLT right? I am wondering if we just return L from the decomposition, how we can set up the unit-test checks EXPECT_. Or actually we should manage to return D as well?\n. @karlnapf I updated the comments information. There are just two possible outputs for llt.info( ). And you mentioned  translating the msg into a shogun error message, but I am not sure how that looks like. Otherwise this PR should be good to go.. @karlnapf we had discussions in #3387 and #3391. So it seems that two have two keep two interfaces. But I am now thinking maybe I can call refactor current Matrix func(Matrix a, Matrixb) to:\nalloc(result, num_rows, num_cols);\nfunc_in_place(a, b, result);\nreturn result\nSo the codes won't be duplicated.\nAlso I am thinking whether it's possible to have a wrapper method for all non-in-place methods, such as\n``\nContainer<T> wrapper(Container<T> a, Container<T> b, lambda x)\n{\n    alloc(result, size);\n    x(a, b, result);\n    return result;\n}. I am not sure I understand your question...It works ifaandresultpoint to the same memory. @karlnapf Do you mean that users explicitly callcholesky_factorand pass the matrix tocholesky_solver? I saw that's how scipy achievedcholesky_solver. But they calledlapack::potrsdirectly in the function, which can solveLu=b, while inEigen3, the solver is wrapped incholesky_factor. Here I just created a newlltinstance and it came with the triangular solver.  I think we will have to code back substitution from scratch if we don't calleigen3.llt.solve()`.. Quote:\n\nStatements like mat = 2 * mat; or mat = mat.transpose(); exhibit aliasing. The aliasing in the first example is harmless, but the aliasing in the second example leads to unexpected results. \n\nSo I think it's function-dependent. And this one might be the property of Eigen::Map<> or .array()? I did a quick search but didn't find where the .array() came from. Yes I am positive the codes are tested on GPU on my machine. @karlnapf I don't have to include <Eigen/Core> to make it work on my machine, and the codes failed on travis even with the header. It seems that  Eigen::UpLoType isn't taken as a known enum class on travis and I am not sure why.. I just realized the pre-allocating may cause memory leak. Are these the kind of ASSERT you were expecting?. @karlnapf Sorry I was wrong about it. I thought the operator= was not overloaded but when I was writing the comment I realized it was defined in the base class. So the old way was fine but I prefer the new one, as it avoids the unnecessary memory allocating and releasing.. @karlnapf Would you suggest to change it to set_col_const or just remove it? I didn't see any call on this method in Shogun. I need to call to_gpu in the methods below. Or I can just declare them here but something needs to be in the front.. @karlnapf matmul is matrix * matrix but I didn't see why we can't merge matrix * vector and matrix * matrix into one method. We can throw an error if type(B) = SGVector && transpose_B = true. These might need separate patch?. I don't like this part...but I didn't come up with any better idea.. @karlnapf The dimensions of the result matrix are determined by whether the input matrices are transposed. Currently there is no method to set matrices' rows and cols after declaration SGMatrix<T> mat;.\nNow I am trying to do:\n```\nSGMatrix result;\nif (transposed_A)\n    result.resize(rows, cols);\nif (transposed_B)\n    result.resize(rows, cols);\nelse\n    result.resize(rows, cols);\nif (A.on_gpu())\n     result = to_gpu(result);\nmatrix_prod(A, B, result, transpose_A, transpose_B);\nWithout the method, the codes will be like:\nif (transposed_A)\n    result.resize(rows, cols);\n    if (A.on_gpu())\n         result = to_gpu(result);\n    matrix_prod(A, B, result, transpose_A, transpose_B);\nif (transposed_B)\n    result.resize(rows, cols);\n    if (A.on_gpu())\n         result = to_gpu(result);\n    matrix_prod(A, B, result, transpose_A, transpose_B);\nelse\n    result.resize(rows, cols);\n    if (A.on_gpu())\n         result = to_gpu(result);\n     matrix_prod(A, B, result, transpose_A, transpose_B);\n```\n. This is not eigen3 and I didn't find convolve method in eigen. \nUnfortunately I haven't used Openmp. I think the problem is the code X.vcl_matrix() is now like X_gpu->data_matrix(X.num_rows, X.num_cols)\nEDIT: They're not tested. My machine doesn't support double precision computation on GPU.. Nope..They're not tested on my machine but there were ViennaCL convolution tests in unit-tests. I found only one application of convolution in shogun (see here) but it actually defines its own method (the same as the \"linalg\" one). So in a word the linalg::convolve method is not used.. Agree! Lemme remove the old codes in Shogun. @karlnapf Oh u r so right...my bad. Just updated. @karlnapf sry I missed this. I don't think I can overload dot method for both matvec, and vecvec in C++ since they have different return types.. I removed the whole generic solver since we're not enumerating the method. This was in Apply.h and it is required for GPU matrix element access.\nNOTE: current new GPUMatrix/Vector doesn't support element access. Maybe we can add it back.. @karlnapf @vigsterkr Regardless of the technical details, I am reshaping a = to_gpu(a) to to_gpu(a, a) and/or to_gpu(a) as the copy assignment a = xxx is not atomic.. I don't think the lock should be in the else branch. For example, assuming a is a GPU vector, thread1 is transferring gpu vector a from GPU back to CPU, and during which time a.on_gpu() == true. If thread2 calls to_gpu() when thread1 is working, without the lock in the beginning, the if clause will be true and nothing will be done. Thus, after thread1 and thread2 (from_gpu() and to_gpu()), a will end up in CPU. However with the lock, a will be transferred to CPU and moved back to GPU.\n. @lambday Sorry I believe it's a bug..It doesn't distinguish cases when I run the codes at local so I didn't notice... @vigsterkr Can I just fix it quickly by pushing it directly to develop branch? Or you want to wait till when I look into other problems in ViennaCL? Or @lambday just fix it here?. qr_factor. Let's just use SGMatrx and SGVector. The point was to hide Eigen from users. @karlnapf So the problem for replacing the solvers (including Cholesky solver) is that the matrices here are already mapped to Eigen objects. Now to call linalg::XX_solver(), we are casting them back to SGMatrix, and most likely they'll be cast again to Eigen inside linalg library. Do you think it makes sense? Or should we expect a huge refactoring to limit Eigen in linalg library only?\nIt is going to worth is if we use ViennaCL or XLA backends. But Eigen is the primary one now for sure.. I suggest adding a dimension check here. docs. Yep. I only see HouseholderQR in Shogun so I think we're good for now as long as we doc it.. identation\n. and elsewhere. space. Indent. non integer real.\nInterger doesn't really make sense here.. Non integer real. remove. Indent. .. delete. writes. Is this method used anywhere in Shogun? It is kinda specified. Oh never mind. I saw it.. Note: A lot of methods can be merged simply by renaming SGVector<T>::EigenVectorXtMap to SGVector<T>::EigenXtMapand SGMatrix<T>::EigenMatrixXtMap to SGMatrix<T>::EigenXtMap, if it won't cause any confusion @karlnapf @vigsterkr . Hi @geektoni \nIs there an appropriate way to fix this error?\nOr I'll probably try to change the target path to src/shogun as I did here.\n. It was modified from the old README but I was not sure where it was located and I don't want to edit it directly. IMHO, I'd shove all add_diag-related tests into one test unit for the neatness.\nOtherwise good to me :). ",
    "juancamilog": "I'm interested in getting started to work on this one. @yorkerlin, do you agree that the framework at https://github.com/zoq/benchmarks should be used? I could initially try testing GP regression exact inference, just  to get a hang of what's needed to setup the benchmarks.\n. ",
    "Xbar": "Changed max_iteration to be an option\nReverted most changes to the unit test\nMax iteration is hard to set, because morbid cases can always be made that will take very long to converge. In practice, if iteration exceeds a fairly large number, it may reflect some problems in the training set, or how SVM is initialized.\n. I was trying to do a rebase, but I didn't do it the right way. Somehow it merged other commits from other people.\n. @vigsterkr Could you close this PR?\nSorry but I really made a mess in my forked repo after an improper rebase.\nI deleted my repo and created a new PR.\n. The comments for CLaRank class or its constructors are almost empty. Should I add max_iteration to those comments? It might be more helpful to give just an overview of LaRank algorithm in those comments and then discuss the logistics behind max_iteration later in comments for relevant methods.\nLaRank algorithm usually converges in very few iterations, and max_iteration, a relatively large number, should be rarely exceeded and may not need adjustments for most use. Should I still include the theoretic upper bound (the formula) to explain how to select the parameter?\n. ",
    "arianepaola": "Here is a snippet from my working buildbot setup @karlnapf. It is building shogun 4.1.0 at the moment. It would be possible to create nightly rpms by specifying the tarball URL for the develop branch below.\n```\nfedoraRPMFactory = util.BuildFactory()\nfedoraRPMFactory.addStep(steps.Git(repourl='git://pkgs.fedoraproject.org/rpms/shogun.git', branch='master', mode='incremental', submodules=True, progress=True, retryFetch=True))\nfedoraRPMFactory.addStep(steps.ShellCommand(command=['wget', '-c', 'ftp://shogun-toolbox.org/shogun/releases/4.1/sources/shogun-4.1.0.tar.bz2']))\nfedoraRPMFactory.addStep(steps.ShellCommand(command=['wget', '-c', 'http://pkgs.fedoraproject.org/repo/pkgs/gmock/gmock-1.7.0.zip/073b984d8798ea1594f5e44d85b20d66/gmock-1.7.0.zip']))\nfedoraRPMFactory.addStep(steps.RpmBuild(specfile=\"shogun.spec\", dist='.f23'))\nc['builders'] = []\nc['builders'].append(\n    util.BuilderConfig(name=\"shogun-fedora-23_gcc-5_rpmbuild\",\n      workernames=[\"shogun-fedora-23\"],\n      factory=fedoraRPMFactory))\n```\n@besser82 @vigsterkr Is it possible to trigger a source download using rpmbuild instead of using the source archive specified in sources? I am getting a version mismatch, when I try to work with archives like the one below.\nMy setup can also download files according to https://fedoraproject.org/wiki/Packaging:SourceURL?rd=Packaging/SourceURL from GitHub.\nAn example URL is:\nhttps://github.com/shogun-toolbox/shogun/archive/9a75c9509cc5188ce522f4b6e79f3ee9257f8f3b/shogun-git-9a75c95.tar.gz\nThanks, Ariane\n. @karlnapf sure :-)\n. @karlnapf ok, I will have a look.\n. This is also referenced in https://bugs.debian.org/cgi-bin/bugreport.cgi?bug=809290\nDebian Bug report logs - #809290\nshogun: FTBFS: /MatrixLogarithm.h:340:54: error: 'Options' is not a member of 'DerivedEvalTypeClean'\n. @karlnapf yes\n. @karlnapf yes\n. @vigsterkr @karlnapf Thanks for the comments, they make sense. \nBut as an example Ubuntu 16.04 will be released this month and it is usual to release with beta packages. Unfortunately the Eigen 3.3 beta also uses a 3.2.x version numbering.\nI have modified my pull request to take out the version numbers from the source file and to rely only on one preprocessor definition. This moves the minimum Eigen version to a preprocessor definition and also resolves a mismatch between the source files (e.g. at least 3.1.0) and the CMake required Eigen 3.1.2.\nNow EIGEN_VERSION_DEPENDABLE surrounds the previous code block.\nIt is not a good idea to force the download of earlier Eigen versions, such as the defined 3.2.8 in cmake/external/Eigen3.cmake as it will hide bugs such as #3140 and will hinder testing shogun on newer versions.\nThe modified pull request will still trigger the error in #3140 and provide a temporary work around for #3142.\nAfter providing a final work around for #3142, the CMake IF clause can be modified to just use at least Eigen 3.1.2.\nThanks, Ariane\n. Thanks @iglesias @karlnapf \n. Ubuntu 16.04\ng++ --version\ng++ (Ubuntu 5.3.1-14ubuntu2) 5.3.1 20160413\n. Python modular is turned on.\ncmake -GNinja -DENABLE_TESTING=ON -DPythonModular=ON -DCMAKE_BUILD_TYPE=Release -DCMAKE_INSTALL_PREFIX=\"./install\" ..\nninja GoogleMock && ninja all && ninja install && ninja test\n. I have also tried to use -DUSE_ORIGINAL_LAPACK=ON\nThis gives an error that -llapack_atlas is required.\nAdding manually -llapack as work around seemed to work.\nThe installed libraries and tools on Ubuntu 16.04 are:\nsudo apt-get install git gcc g++ cmake ccache ninja-build doxygen swig python-dev python-numpy python-jinja2 libblas-dev libatlas-base-dev libglpk-dev libeigen3-dev libcolpack-dev libviennacl-dev libnlopt-dev liblpsolve55-dev exuberant-ctags python-ply libxml2-dev libhdf5-dev libcurl4-openssl-dev libbz2-dev liblzma-dev libsnappy-dev liblzo2-dev libprotobuf9v5 libprotobuf-lite9v5 protobuf-compiler\n. @vigsterkr @karlnapf @lgoetz \nAny update on this? I have tried a few workarounds, but the error still occurs.\n. @vigsterkr @karlnapf @lgoetz \nI will be updating the PR shortly.\n@karlnapf talked with me about the changes on IRC.\n. @vigsterkr @karlnapf @lgoetz \nAny update on this? Can my change be merged?\nI talked with @karlnapf about the change on IRC and it does not impact finding Sphinx on the system.\nShogun requires a newer sphinx version for the templates, which is not found in the default location. For example on Ubuntu 14.04, shogun requires a Sphinx install from PyPI.\n. @vigsterkr @karlnapf @lgoetz \nAny update on this? Can my change be merged?\nI talked with @karlnapf about the change on IRC and it does not impact finding Sphinx on the system.\nShogun requires a newer sphinx version for the templates, which is not found in the default location. For example on Ubuntu 14.04, shogun requires a Sphinx install from PyPI.\nOtherwise I have to rebase my cookbook pages against this PR every time to work on them and then undo the PR changes.\n. @vigsterkr @karlnapf @lgoetz I have mentioned on IRC that I saw the comments and will push an update shortly. Thanks!\n. @karlnapf How to add the latter part for ground truth labels?\n. @vigsterkr @karlnapf @lgoetz\nAny update on this? It is ready to merge and I am using Docker on top of Vagrant for development.\nThis could be helpful to other shogun users.\n. @vigsterkr @lisitsyn @karlnapf @lgoetz I have mentioned on IRC that I did strip down this version to a minimum to create a pull request.\n. @vigsterkr @lisitsyn @karlnapf @lgoetz Please review, comment and test. It is fully functional and requires only minor tweaks before merging.\n. @vigsterkr As discussed on IRC:\nIt is not needed to manually run cmake prior to running setup.py.\nsetup.py will run a bootstrapping process of  cmake, make and make install, as it requires the generated files / content for the Python package.\nAs Viktor mentioned, cmake was installed using brew. The setup.py script uses the default system cmake and might not have picked up the updated user's PATH.\n. @vigsterkr I squashed the commits.\n. @karlnapf Updated description.\n@vigsterkr ready to merge\n. @vigsterkr @karlnapf @lgoetz\nPlease review and merge.\n. @karlnapf Hmm, I am using \"`\". Don't know why the output looks strange.\n. this worked ```\n. @karlnapf's note on IRC:\nwhat about you put a try around it\nand in case of an exception, you print\nand then print the usual stack trace\nkeeping the output cleaner\n. @vigsterkr @karlnapf @lgoetz\nPlease review and merge.\n. @vigsterkr @karlnapf @lgoetz\nPlease review and merge.\n. @karlnapf The crash happens earlier, when loading json. So it does not get to print the file name.\nhttps://github.com/shogun-toolbox/shogun/blob/f2b6e3b0970c063124ee586488995e60c20f5853/examples/meta/generator/generate.py#L42\n. @karlnapf I updated this one.\n. @karlnapf squashed in latest PR update.\n. @vigsterkr @karlnapf @lgoetz\nPlease review and merge.\n. ok\n. @vigsterkr @karlnapf @lgoetz\nOk, also as noted by @vigsterkr \n. @vigsterkr @karlnapf @lgoetz\nPlease review and merge.\n. @vigsterkr @karlnapf @lgoetz\nPlease review and merge.\n. @vigsterkr @karlnapf @lgoetz\nPlease review and merge.\n. @vigsterkr @karlnapf @lgoetz\nPlease review and merge.\n. @vigsterkr @karlnapf @lgoetz\nPlease review and comment. I would like to improve and merge this cookbook.\n. Closing this PR for now and will reopen it soon.\n. @vigsterkr @karlnapf @lgoetz\nPlease review and comment. I would like to improve and merge this cookbook.\n. @vigsterkr @karlnapf @lgoetz\nPlease review and comment. I would like to improve and merge this cookbook.\n. Closing this PR for now and will reopen it soon.\n. @vigsterkr @karlnapf @lgoetz\nPlease review and comment. I would like to improve and merge this cookbook.\n. @vigsterkr @karlnapf @lgoetz\nPlease review and comment. I would like to improve and merge this cookbook.\n. Closing this PR for now and will reopen it soon.\n. @vigsterkr @karlnapf @lgoetz\nPlease review and comment. I would like to improve and merge this cookbook.\n. @vigsterkr @karlnapf @lgoetz\nPlease review and comment. I would like to improve and merge this cookbook.\n. Closing this PR for now and will reopen it soon.\n. @vigsterkr @karlnapf @lgoetz\nPlease review and comment. I would like to improve and merge this cookbook.\n. @vigsterkr @karlnapf @lgoetz\nPlease review and comment. I would like to improve and merge this cookbook.\n. Closing this PR for now and will reopen it soon.\n. @vigsterkr @karlnapf @lgoetz\nPlease review and comment. I would like to improve and merge this cookbook.\n. @vigsterkr @karlnapf @lgoetz\nPlease review and comment. I would like to improve and merge this cookbook.\n. Closing this PR for now and will reopen it soon.\n. @vigsterkr @karlnapf @lgoetz\nPlease review and comment. I would like to improve and merge this cookbook.\n. @vigsterkr @karlnapf @lgoetz\nPlease review and comment. I would like to improve and merge this cookbook.\n. Closing this PR for now and will reopen it soon.\n. @vigsterkr @karlnapf @lgoetz\nPlease review and comment. I would like to improve and merge this cookbook.\n. Closing this PR for now and will reopen it soon.\n. @vigsterkr @karlnapf @lgoetz\nPlease review and comment. I would like to improve and merge this cookbook.\n. @karlnapf Thanks for the comment. I will update this PR.\n. @karlnapf Thanks for the updated comments.\n. @karlnapf will be updating this PR.\n. @vigsterkr @karlnapf @lgoetz\nPlease review and comment. I would like to improve and merge this cookbook.\n. Closing this PR for now and will reopen it soon.\n. @vigsterkr @karlnapf @lgoetz\nPlease review and comment. I would like to improve and merge this cookbook.\n. Closing this PR for now and will reopen it soon.\n. @vigsterkr @karlnapf @lgoetz\nPlease review and merge.\n. Closing this PR for now and will reopen it soon.\n. @karlnapf ready to merge\n. I am trying to get the information from CMake when bootstrapping the version generation to get the string.\n. I know :smile: it is very minimal at the moment, but the dependencies will be added.\n. sure :+1: \n. Work in progress :smile: \n. @karlnapf I will update this part, but the original text and explanation comes from the Shogun implementation header file.\n. thanks :+1: \n. @karlnapf This was translated from the Python code.\n. @karlnapf I will rename it. It was in the example cookbooks.\n. Thanks :+1: \n. ok. RealDenseDistance :smile: it is\n. @karlnapf updated\n. @karlnapf This one?\ndistance(features_a, features_b)\n. @lisitsyn I have added them in a new version.\n. @vigsterkr Done in the new version.\n. @vigsterkr Let's update the dependencies. I have published a final version.\n. @karlnapf yes\n. ",
    "ilovejs": "the latest release on github v4.1.0\nsince ubuntu 14 don't have compiled package. I was trying to compile.\nI think there are some problems with that sample:\nexamples/undocumented/libshogun/balanced_conditional_probability_tree\nso, is there a way to have shogun core, shogun example binary in separate files ?\n. ",
    "rekado": "What follows are the cmake flags and the logs starting from the first error: \nhttps://gist.github.com/rekado/01fcfd74e3defe4288ae3bab86078f3e\n. I moved the error log from the previous comment to a gist.\nMy system is an x86_64 machine with 8GB RAM but this doesn't seem to be related to the error which is about conflicting declarations in included files.\nI'm building shogun in a chroot where only declared packages are available (that's a feature of GNU Guix).  There were no problems with the modular R interface in version 4.0.0.\n. It might be sufficient to #define NO_C_HEADERS before including R.h.  The R NEWS file says that R.h should not be included from within extern C blocks, but that's what swig appears to do.  With R 3.3.0 C headers are included when R.h is included.  This can be avoided by defining NO_C_HEADERS.\nI haven't tested this with Shogun yet, but it worked for SPAMS, which also includes bindings that were created with swig.\n. Nice, thank you!. ",
    "mitakas": "@vigsterkr Can you also setup the ppa to generate stable packages for xenial?. ",
    "arjunmenon": "Hey\nHas it been merged?. Cool! \nExamples are also in the pipeline?. Hey\nIf you can tell any ruby gem to do feature extraction of audio files. I can't find any.. Having slight trouble getting started.\nhttp://paste.ubuntu.com/25059799/\nmake is failing to download some dependency?\nBetter, if the executable have the latest commits? And how to create a ruby interface from there? That would be much easier.\nI tried a nightly thinking it would have the latest. After install, require 'modshogun' fails in IRB, which is obvious. I know some additional steps is to be done. But what flummoxes me. \n--\nWorking on Elementary os 0.3.2. 32-bit\n. As to the vector output, I found this gem - http://wavefilegem.com/examples.html\nLook under the third example - Reading a Wave File Into an Arbitrary Format\nIt outputs something like this - http://paste.ubuntu.com/25059826/\nWAV file - http://s000.tinyupload.com/index.php?file_id=00455761811235983910\n. ",
    "dougalsutherland": "Seems to work, thanks! (Just linking to openblas, not ENABLE_EIGEN_LAPACK which I haven't tried yet.). Cool! I'll probably wait for the next shogun release rather than backporting this into the conda package, though if you wanted to do a 6.0.1 with this, the lapack cmake changes, and those Windows fixes, I wouldn't complain. :). It's not just GLPK...after turning that off, got errors about bz2 and lzo (log).\nIs there something strange about my setup, or does anyone actually use shogun with any of the optional packages on Windows?. @karlnapf Sure....where do I put it? Between 6.0.0 and 6.1.0?. Cool, thanks!. (Sorry for slow reply, I was traveling.) I think it's just that you want to import from modshogun, not shogun, in the 6.0 release (which is what the conda package has). The docs on the website refer to shogun instead because they're for 6.1, where the interface has been renamed.. This error was fixed in https://github.com/shogun-toolbox/shogun/commit/d67be57eb50eb402b59d9892bfd317454061b6fc, which is present in shogun 6.1.*.\nHomebrew-science is now deprecated (https://github.com/Homebrew/homebrew-science/pull/6616), and so shogun 6.1 won't be available on it. You could try editing the formula to build 6.1.3, but the next time you brew update it'll go away.\nYou might prefer conda-forge builds.. @nilinswap: also, https://github.com/Homebrew/homebrew-core/pull/21444 would add shogun to homebrew-core, if you'd prefer to wait for that / get it from the pull.. This won't be fixed on conda-forge yet. @lisitsyn / @vigsterkr, should I push this out as a patch for 6.1.3, or will this be in a new release soon?. To add some details:\n\n\nIf you use a conda-forge build of python, e.g. conda create -n tmp -c conda-forge python=3.5 shogun, then this error won't happen, because conda-forge actually does dynamically link the python executable to (e.g.) libpython3.5m.dylib. This is why the conda-forge automated tests, and my use of the package, didn't crash.\n\n\nI've tested that with this patch, the conda-forge package of shogun will work (or at least, will import) with a conda defaults version of python.\n\n\nI've now released a new build with this patch (https://github.com/conda-forge/shogun-feedstock/pull/8; it's build number 1, e.g. shogun-6.1.3-py36_blas_openblas_1.tar.bz2).. I'm a little confused why this is happening, but the current conda-forge h5py recipe depends on a newer hdf5.\n@mlsgenaro It seems like you're somehow getting an old 6.0.0 build of shogun, when there's 6.1.3; this also has a fixed hdf5 dependency. Something in the conda dependency resolution is probably going wrong; try conda install -c conda-forge shogun=6.1.3 and you should probably get an error message that actually explains what's happening. (Likely it's conflicting with some other package you have installed.) As long as you don't need to use shogun and whatever package is conflicting at the same time, you can also definitely work around it by making a new environment:\n$ conda create -n shogun -c conda-forge shogun. Also @karlnapf, hdf5 doesn't maintain binary compatibility, so each build has to depend on a relatively tight version of hdf5.. Sorry I didn't see this last week (my github notifications are an endless swamp). Yes, there's no lapack backend on the Windows build yet (https://github.com/conda-forge/shogun-cpp-feedstock/issues/13).. Wow, totally screwed up the hub command line here. Wrong target..... ",
    "ealtamir": "After playing a bit with this issue, I realized that there was a class that only appeared once in the dataset, thus it conflicted with the k=3. After setting k=1 I got a new error:\npython3.5: /usr/include/eigen3/Eigen/src/Core/Block.h:123: Eigen::Block<XprType, BlockRows, BlockCols, InnerPanel>::Block(XprType&, Eigen::Index) [with XprType = Eigen::Map<const Eigen::Matrix<double, -1, -1, 0, -1, -1>, 0, Eigen::Stride<0, 0> >; int BlockRows = -1; int BlockCols = 1; bool InnerPanel = true; Eigen::Index = long int]: Assertion (i>=0) && ( ((BlockRows==1) && (BlockCols==XprType::ColsAtCompileTime) && i<xpr.rows()) ||((BlockRows==XprType::RowsAtCompileTime) && (BlockCols==1) && i<xpr.cols()))' failed.. I was able to make it work. The problem was related to the dataset, there weren't enough number of datapoints for certain classes so the k I selected wasn't feasible. Apparently k=1 also gives some problems. My solution was to clean the dataset so as to make sure there are enough samples for each class.. ",
    "orivej": "Yes, make -j meta_examples reproduces it every time, but this appears to happen only with a truly fresh build, rm -rf build; mkdir build; cd build; cmake ...; make -j meta_examples; make clean; make -j meta_examples is not enough.  Sorry if my estimation of the cause is not correct.. I have confirmed that the build does not write files outside the build directory, so in my case wiping it and unpacking the sources again vs make clean may just have an effect on the disk cache or similar that affects timings. I have also recorded (with fptrace) that e.g one generator writes\nbuild/examples/meta/cpp/binary_classifier/kernel_svm.cpp\nbuild/examples/meta/parser_files/parsetab.py\nbuild/examples/meta/parser_files/parser.out\nwhile another writes\nbuild/examples/meta/cpp/clustering/gmm.cpp\nbuild/examples/meta/parser_files/parsetab.py\nbuild/examples/meta/parser_files/parser.out\nso you can tell if this is fixed even without reproduction. Here is one way to fix it: https://github.com/orivej/shogun/commit/11d6bc3150da05fa0eef5451c8e21d3e196b2852. parse.py does not depend on any generator dependencies except ${CMAKE_CURRENT_SOURCE_DIR}/generator/parse.py.. ",
    "heytitle": "I'm taking a look at this issue. I've created a spreadsheet for the current coverage at https://goo.gl/Qv9vxU.\nI'll start go down the list, adding methods that are implemented in Shogun but aren't included in the benchmark. \n\nUpdated: An automatic way to get the results is using the cURL mentioned in https://github.com/shogun-toolbox/shogun/issues/4097.. Hi, \nI've figured out a way to get the results through the mysql_wrapper.php.\ncurl -X POST \\\n  http://www.mlpack.org/php/mysql_wrapper.php \\\n  -H 'Cache-Control: no-cache' \\\n  -H 'Content-Type: application/x-www-form-urlencoded' \\\n  -H 'Postman-Token: 175a469b-31bd-4921-bd23-170d8b9e171a' \\\n  -d 'request=<QUERY>'\nAnd <QUERY> is \nSELECT\n    *\nFROM (\n    SELECT\n        libraries.name as lib,\n        methods.name as name,\n        datasets.name as dataset,\n        results.time as time,\n        results.var as var,\n        libraries.id,\n        datasets.id as did,\n        libraries.id as lid,\n        results.build_id as bid,\n        datasets.instances as di,\n        datasets.attributes as da,\n        datasets.size as ds\n    FROM results, datasets, methods, libraries\n    WHERE\n        results.dataset_id = datasets.id AND\n        results.method_id = methods.id AND\n        methods.parameters = '' AND\n        libraries.id = results.libary_id AND\n        libraries.name = 'shogun' AND\n        results.time <= 0 # this can be remove if we want to get all results.\n    ORDER BY bid DESC\n) tmp;\nReturned JSON:\n[\n    {\n        \"lib\": \"shogun\",\n        \"name\": \"LinearRegression\",\n        \"dataset\": \"diabetes\",\n        \"time\": \"0\",\n        \"var\": \"0\",\n        \"id\": \"6\",\n        \"did\": \"8\",\n        \"lid\": \"6\",\n        \"bid\": \"33\",\n        \"di\": \"442\",\n        \"da\": \"10\",\n        \"ds\": \"0\"\n    },\n    {\n        \"lib\": \"shogun\",\n        \"name\": \"LinearRegression\",\n        \"dataset\": \"cosExp\",\n        \"time\": \"-2\",\n        \"var\": \"0\",\n        \"id\": \"6\",\n        \"did\": \"9\",\n        \"lid\": \"6\",\n        \"bid\": \"33\",\n        \"di\": \"200\",\n        \"da\": \"800\",\n        \"ds\": \"1\"\n    },\n...\n]. hi @zoq, \nI'm a bit concerned about the API though. With this interface, one might directly send malicious queries to the DB.\nI personally think we should find a better way to extract the result as well as making the API more restrict. \nOne way might be to generate benchmark results offline and write the results as JSON files. The frontend will just read those JSON files. . I was surprised too! \nCan you elaborate a bit about your approach? If I understood correctly, you would allow only certain IPs to send the POST request to the server. If that's the case, the benchmark page will be functional on for those IPs, right? . Hi @zoq & @karlnapf,\nGiven the command I provided previously and current coverage in https://github.com/shogun-toolbox/shogun/issues/4046#issuecomment-461455849, I think we basically have everything we need for this issue. \nWhat else should we do for this issue? . I've also created an issue at mlpack's benchmarks (https://github.com/mlpack/benchmarks/issues/133). It seems Shogun has an DTC algorithm but the   benchmark report doesn't have the the result. \n@zoq  do you have any idea why that happens?. @karlnapf I've created a PR (#4516) containing patches for these issues. \nNoting, I haven't updated the Debian installation yet because I'm not sure whether Shogun still has its own APT server. Could you please confirm? Otherwise, I'll just update the section with information from https://packages.debian.org.\nShould I create two new issues for fixing showroom pages and https as they might require further discussion. What do you think? . @karlnapf I can help you with Debian and Ubuntu packaging.. @karlnapf I've pushed new commits addressing your previous comments.  Regarding the relative url issue, the fix is related to Shogun-Web2's https://github.com/shogun-toolbox/shogun-web2/pull/43. \nCould you please review those changes and give me feedback again? . Nope, I mentioned the web PR because this PR depends on it.\nSo, please merge this one if you see no further change!. Currently, if you open [shogun.ml/examples][shogun-example], you find the JS error below:\nUncaught ReferenceError: MathJax is not defined\n    at mathconf.js:1\nThis error happens because mathconf.js is loaded before the core MathJax script, which contains the MathJax variable. Hence,  macros defined in mathconf.js, such as \\argmax,  are not defined.  \nIn order to solve this issue, we need to make sure that mathconf.js is executed after the core MathJax script is loaded. Including mathconf.js with type=\"text/x-mathjax-config\" is the solution: MathJax will automatically execute the script when it is loaded. \nI think the import below might also work and look nicer:\n<script type=\"text/x-mathjax-config\" src=\"_static/mathconf.js\"></script>\n[shogun-example]: http://shogun.ml/examples/latest/index.html. I wan't able to find the relative path. Is it http://shogun.ml/docs/DEVELOPING.md? \nThe link doesn't seem working properly. My browser recognises it as a raw file, not HTML. I also reported the issue at the website repository, https://github.com/shogun-toolbox/shogun-web2/issues/42.\nUpdated: With the PR for the web repo(https://github.com/shogun-toolbox/shogun-web2/pull/43), the relative path of DEVELOPING.md is docs/DEVELOPING.md.. ok. . ",
    "ChrisTX": "The Arch AUR package only builds the static ARPREC library, and that without PIC. As of such, it cannot be linked into libshogun.so, as that would require PIC. I've contacted the AUR maintainer with a fixed version of the package, that would enroll ARPREC as a shared library, which is how it should be on Arch.\nWith the fixed version of the package, I've been able to compile shogun with ARPREC support.. ",
    "alesis": "If we believe LAPACK, then\n*W       (output) DOUBLE PRECISION array, dimension (N)\n*If INFO = 0, the eigenvalues in ascending order.\nso we actually traverse over highest eigenvalues.\n. ",
    "xfrancv": "I guess there should be dis=dis*m_fGaussianConst, right?\n. ",
    "JohnLangford": "Many algorithms work with both supervised and unsupervised examples.  Consequently, it's useful to have get_next_example() which simply notes whether the example is labeled or not.  Otherwise, confusion and awkwardness will ensue.\n. Holding a write-once invariant throughout parsing is very helpful for performance in my experience.  Another thing which really helps is switching to 32 bit floats---1 in 10^6 precision is enough for machine learning in my experience and having the amount of memory used in your system is a substantial advantage.\n. It is possible, although difficult, to write a read-once parser.  This can be done using a case engine for VW's text format, although it's not there.  The binary format already prepends the length to read eliminating this issue.\nMy belief is that altering the text format so as to be parsed faster at the cost of human readability is undesirable.  For binary formats, this is entirely desirable.  Note that conversion from text to binary formats is trivially parallelized.\n. On 04/26/2011 12:50 AM, frx wrote:\n\n\n\nnumber_of_features = current_number_of_features;\n  +\n// Now allocate mem for buffer\nif (example_type == LABELLED)\n{\nexample_memsize = sizeof(LabelledExample) + sizeof(float64_t)*number_of_features;\ncurrent_example = (LabelledExample*) malloc(example_memsize);\nexamples_buff = (LabelledExample_) malloc(example_memsize_buffer_size);\ncurrent_feature_vector = (float64_t) ((char ) current_example + sizeof(LabelledExample));\n  +\n}\n  +\nelse\n{\nexample_memsize = sizeof(UnlabelledExample) + sizeof(float64_t)*number_of_features;\ncurrent_example = (UnlabelledExample*) malloc(example_memsize);\n\n\n\nIf I understand correctly, you are doing a 'malloc' for every example \nseen.  This is generally poor performance wise, because 'malloc' is not \na fast function, and because it means that the cache must be refreshed \nwith the new memory pool.\nThe way we handle this in VW is by allocating some memory for examples \nat the beginning, then reusing it.\n-John\n\n\nDo you mean a struct with a flexible array member?\nLike:\nstruct x {\nexample ex;\nfloat fv[ ];\n}\nIt still looks somewhat complicated (in my opinion) to do the allocation for\nthis using 'new' while ensuring that all example and feature vectors lie in\ncontinuous memory locations.. No direct way strikes my mind right away..\nCurrently, examples are stored as:\nexamples_buff =_buffer_size, and it is\nnecessary that the example and vector are continuous.\nI can change the code so that storage is like:\nexamples_objects_buff =_buffer_size\nfeature_vectors_buff =*buffer_size\nwhich doesn't depend on this continuity criterion. I can use 'new' for this\nvery easily as well.\nShould I make this modification?\nAnd btw, why do you recommend 'new' and not 'malloc'?\nOn Tue, Apr 26, 2011 at 12:41 AM, sonney2k\nreply@reply.github.comwrote:\n\n\nnumber_of_features =\n  current_number_of_features;\n  +\n// Now allocate mem for buffer\nif (example_type == LABELLED)\n{\nexample_memsize =\n  sizeof(LabelledExample) + sizeof(float64_t)*number_of_features;\ncurrent_example =\n  (LabelledExample*) malloc(example_memsize);\nexamples_buff =\n  (LabelledExample_) malloc(example_memsize_buffer_size);\n  +\n  current_feature_vector = (float64_t) ((char ) current_example +\n  sizeof(LabelledExample));\n  +\n}\n  +\nelse\n{\nexample_memsize =\n  sizeof(UnlabelledExample) + sizeof(float64_t)*number_of_features;\ncurrent_example =\n  (UnlabelledExample*) malloc(example_memsize);\n\n\nOn Mon, 2011-04-25 at 09:41 -0700, frx wrote:\n\nExamples are being stored in the memory as  followed\nby\n.. The example object only contains a pointer to the\nfeature\nvector, while the actual feature vector itself is next to the example\nobject\nin the memory.. Hence, I do a malloc combining the sizes of the\nexample\nobject+feature vector. I can use new[] but then I'll have to do new\nchar[...] because it is not a contiguous stream of objects of the same\ntype.\nYou could use a struct and then new - or am I missing something?\n\nSoeren\nFor the one fact about the future of which we can be certain is that it\nwill be utterly fantastic. -- Arthur C. Clarke, 1962\n\nReply to this email directly or view it on GitHub:\nhttps://github.com/shogun-toolbox/shogun/pull/52/files#r22610\n. On 04/27/2011 10:20 AM, frx wrote:\n\nnumber_of_features = current_number_of_features;\n  +\n// Now allocate mem for buffer\nif (example_type == LABELLED)\n{\nexample_memsize = sizeof(LabelledExample) + sizeof(float64_t)*number_of_features;\ncurrent_example = (LabelledExample*) malloc(example_memsize);\nexamples_buff = (LabelledExample_) malloc(example_memsize_buffer_size);\ncurrent_feature_vector = (float64_t) ((char ) current_example + sizeof(LabelledExample));\n  +\n}\n  +\nelse\n{\nexample_memsize = sizeof(UnlabelledExample) + sizeof(float64_t)*number_of_features;\ncurrent_example = (UnlabelledExample_) malloc(example_memsize);\n  I'm allocating memory for the buffer only when the first example is seen. (if number_of_vectors_parsed == 0)\n  This code has to be changed as the thought behind this was to assume number of features as constant, and assume this to be equal to the number of features in the first example seen. After this is known, adequate memory space is allocated for the buffer by using sizeof(float64_t)_number_of_features.\n\n\n\nI see.  In VW we double memory on demand with realloc() as needed---(See \nv_array.h).  This is pretty close to what the stl vector<> does.\n-John\n. Are you thinking about the memory allocation for the buffer or for the \nexample?  For the buffer, I just allocated a big chunk of memory, filled \nit, and if that was not enough doubled the size of the buffer.\nBut, the approach to take is dependent on whether proto buffers/avro is \nan adequate alternative.  What's the state of that?\n-John\nOn 04/27/2011 10:35 AM, frx wrote:\n\n\n\nnumber_of_features = current_number_of_features;\n  +\n// Now allocate mem for buffer\nif (example_type == LABELLED)\n{\nexample_memsize = sizeof(LabelledExample) + sizeof(float64_t)*number_of_features;\ncurrent_example = (LabelledExample*) malloc(example_memsize);\nexamples_buff = (LabelledExample_) malloc(example_memsize_buffer_size);\ncurrent_feature_vector = (float64_t) ((char ) current_example + sizeof(LabelledExample));\n  +\n}\n  +\nelse\n{\nexample_memsize = sizeof(UnlabelledExample) + sizeof(float64_t)*number_of_features;\ncurrent_example = (UnlabelledExample*) malloc(example_memsize);\n  That is a bottleneck in my code. The parser waits for the examples in the\n  buffer to be used rather than expanding the buffer to read more examples.\n  My technique assumes constant dimensionality, and won't work for sparse\n  features.. How do you recommend I do the memory allocation in this case?\n\n\nOn Wed, Apr 27, 2011 at 7:55 PM, JohnLangford\nreply@reply.github.comwrote:\n\n\n\nnumber_of_features =\n  current_number_of_features;\n  +\n// Now allocate mem for buffer\nif (example_type == LABELLED)\n{\nexample_memsize =\n  sizeof(LabelledExample) + sizeof(float64_t)*number_of_features;\ncurrent_example =\n  (LabelledExample*) malloc(example_memsize);\nexamples_buff =\n  (LabelledExample_) malloc(example_memsize_buffer_size);\n  +\n  current_feature_vector = (float64_t) ((char ) current_example +\n  sizeof(LabelledExample));\n  +\n}\n  +\nelse\n{\nexample_memsize =\n  sizeof(UnlabelledExample) + sizeof(float64_t)*number_of_features;\ncurrent_example =\n  (UnlabelledExample*) malloc(example_memsize);\n\n\nOn 04/27/2011 10:20 AM, frx wrote:\n\n\n\nnumber_of_features =\n  current_number_of_features;\n  +\n// Now allocate mem for buffer\nif (example_type == LABELLED)\n{\nexample_memsize =\n  sizeof(LabelledExample) + sizeof(float64_t)*number_of_features;\ncurrent_example =\n  (LabelledExample*) malloc(example_memsize);\nexamples_buff =\n  (LabelledExample_) malloc(example_memsize_buffer_size);\n  +\n  current_feature_vector = (float64_t) ((char ) current_example +\n  sizeof(LabelledExample));\n  +\n}\n  +\nelse\n{\nexample_memsize =\n  sizeof(UnlabelledExample) + sizeof(float64_t)*number_of_features;\ncurrent_example =\n  (UnlabelledExample_) malloc(example_memsize);\n  I'm allocating memory for the buffer only when the first example is seen.\n  (if number_of_vectors_parsed == 0)\n  This code has to be changed as the thought behind this was to assume\n  number of features as constant, and assume this to be equal to the number of\n  features in the first example seen. After this is known, adequate memory\n  space is allocated for the buffer by using\n  sizeof(float64_t)_number_of_features.\n\n\n\nI see.  In VW we double memory on demand with realloc() as needed---(See\nv_array.h).  This is pretty close to what the stl vector<>  does.\n-John\n\nReply to this email directly or view it on GitHub:\nhttps://github.com/shogun-toolbox/shogun/pull/52/files#r23491\n. \n\n",
    "mohataher": "I got your comments, I will work on it as soon as I get done with my midterm exams. \nThank you,\nmohamed\n. I have done all the modifications, should I send a new pull request ?\n. Don't think they would work. Through a regex on many classes, I figured out that developers use SGMatrix and SGVector as a referenced instance (not a pointer).  I changed them in the new commit.\n. ",
    "xiangyuwang": "thanks, I'll revise it.\n. no, the code is still under construction. We would add  at least cut_plane_algorithm()  and  CCCP()  function to make it complete. After that I would debug the code.  Thanks.\n. It could be character encoding problems. I'll fix it.\n. ",
    "saketbharambe": "No,\nIt doesn't need to be included. JLCoverTreePoint.h is already included in JLCoverTree.h. \nThe first two commits were a little messy due to other developments going on in the localhost. \n. ",
    "deerishi": "Hi Heiko\nSorry,just saw your mail.Had gone for dinner.\nwill change the name then again apply for a pull request.\ni sent 2 prs by mistake\nSincerely,\nDeepak Rishi\nOn Mon, Apr 22, 2013 at 10:45 PM, Heiko Strathmann <notifications@github.com\n\nwrote:\nIn examples/undocumented/python_modular/kpcatest_check.py:\n\n@@ -0,0 +1,57 @@\n+\n\ncould you rename the file to preprocessor_kpca.py?\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/shogun-toolbox/shogun/pull/1000/files#r3899809\n.\n. the generate_circle_data() is in the tools folder.inorder to import it ,from the graphical folder i need to add the path to sys.path.\n. again generate_circle_data() is in the subdirectory of python_modular.i tried running on my machine,but it turns out that sys.path does not by default search in the sub folders.so again i had to specify the path\n. but i checked this  this is working perfectly fine.\nbut i 'll also take a further look at the example you mentioned\n. I'll have to use 2 loops to find the square roots. math.sqrt() takes only single element as input\n. this file was already there.i did not create it\n. can you tell me the exact path of this particular file?also , is this the reason the travis build is failing?\n. As ,you want me to change the contents.the descriptions folder in examples\njust did this thing.also i am interested in working on Cross-Product for\nlists of kernels (MKL, CombinedKernel).\n\nSincerely,\nDeepak Rishi\nOn Thu, May 2, 2013 at 3:21 PM, Heiko Strathmann\nnotifications@github.comwrote:\n\nIn examples/descriptions/modular/preprocessor_kpca.txt:\n\n@@ -0,0 +1,7 @@\n+Documentation for Kernel PCA algorithm and its data generation\n\nHi Deepak\nas said a couple of times before, please do this file in the style of the\nother descriptions\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/shogun-toolbox/shogun/pull/1039/files#r4051927\n.\n. \n",
    "itsuper7": "Since Indices is also part of the parameter , So I pass it as reference and assign value to it in this \"prepare_tapkee_parameters_set\" funciton\n. I use http://homepage.tudelft.nl/19j49/Matlab_Toolbox_for_Dimensionality_Reduction.html to generate the resdata.\n. ",
    "zhh210": "I tried the SG_ADD macro but the compiling will fail. According to http://www.shogun-toolbox.org/doc/en/current/SGObject_8h.html#a1c8b5ed2a61b9e6e544c107c65b726c8 SG_ADD equivalents m_parameters->add. But don't we need the add_vector rather than add?\n. ",
    "vladislav-horbatiuk": "How did I missed 'norm' while looking through the documentation - no idea..:)\n. If you think it is 'legal' - then why not:) I thought that you won't like the idea of modifying the base kernel class - such things should always be done carefully.\n. ",
    "rostyboost": "@van51 Are you sure of this 0 index? Shouldn't it be s instead?\n. @van51 these loops don't seem to output the unigrams when n=1 and s=0... or maybe I missed something?\n. @van51 generate_ngram_hashes should handle the unigram case, so that you can avoid the switchcase here\n. @van51 please give more details about how the skipping work\n. @van51 from my experience, it's completely ok, as long as a base hasher is very good (MurMur3).\n. @van51 thanks. yes that would be great to add this little example somewhere in the documentation\n. ",
    "zhengyangl": "FIND_PACKAGE(CURL)\nIF (CURL_FOUND)\n  SET(HAVE_CURL 1)\n  LIST(APPEND INCLUDES ${CURL_INCLUDE_DIRS})\n  SET(POSTLINKFLAGS ${POSTLINKFLAGS} ${CURL_LIBRARIES})\nENDIF()\nIs that Ok? @vigsterkr \n. ",
    "lazyf0z": "I'll remove it.\n. ",
    "curiousguy13": "Oh, sorry about that!\n@lambday should i send a separate PR for this ?\n. @lisitsyn Thanks!\n. @lambday I made a new struct for range_fill_vec to handle float64* type of arrays.\nAlthough this seems to work , is there a better way to do this?\n. @lambday  first of all sorry for the late reply.\n\"Also, don't worry about the template argument being named Matrix for both matrix and vector. It doesn't matter. When you pass a SGVector there then Matrix can very well mean a SGVector. Does it make sense?\"\nyeah i know that Matrix can mean SGVector as well as SGMatrix but afaik the problem here was that Matrix cannot mean double* or int* type of arrays because we are using Matrix::Scalar in the range_fill struct  which throws an error even after calling it from a separate wrapper function in Core.h(as is expected) because double/int don't have scalar type. So to deal with these kind of arrays I made a separate struct to deal with these arrays.\nIn my code the vector in the second struct is not for SGVector but for these double_/int_ type of arrays.\n\"Check the linalg::add method implementation to see how it is done there. That can be helpful :)\"\nYeah, it was helpful but linalg::add doesn't deal with dynamically allocated arrays and neither does any other method in linalg. \nMaybe I'm still missing something. So can you tell me an elegant way to incorporate these arrays and vectors and matrix in a single struct.:)\n. I just converted all the spaces to tabs since that is what is mentioned in the coding guide.\nIt must have been spaces there before.\n. okay.\n. yes std::iota is available from c++11 onwards. would that be a problem?\n. As far as i know the linalg library is available only for c++11 onwards. @lambday  please correct me if i'm wrong here.\nSo, keeping that in mind i used #ifdef HAVE_LINALG_LIB and #else whenever linalg::range_fill is used in the code which makes the code compatible with older versions of C++ and C that do not support linalg.\n. I think these are to ensure that the test would run only in the presence of c++11 since that is what linalg is built for and skip the tests otherwise.\n. @vigsterkr any particular reason why we should not try to improve existing code, even if it is just indentation? Am I missing something here?\n. Alright. I'll update it\n. Hmm... fair enough. I'll revert it. Thank you :)\n. The matrix m used in the test case is a symmetric matrix because LLT does not work on non-symmetric matrices.\nShould I change/remove the comment?\n. ",
    "ajaybhat": "@lisitsyn Is use of the scope resolution operator :: for specifying type of the fields of the classes okay or is it something that you discourage?\n. @lisitsyn  i've fixed it.\n. I've also taken care of some compile errors that kept appearing.\n. ",
    "tonmoy-saikia": "Will do.\n. Sorry for that! Completely forgot to update it.\n. My vimrc had 2 space indents, I will update it.\n. I agree. Nice spot.\nIs <shogun/mathematics/Math.h> the correct header? If yes, I haven't included it.\nA git grep for math.h results in the following:\n\ngit grep \"math.h\" | grep Kernel\nsrc/shogun/kernel/BesselKernel.cpp:#include <math.h>\nsrc/shogun/kernel/TStudentKernel.cpp:#include <math.h>\nsrc/shogun/preprocessor/HomogeneousKernelMap.cpp:#include <math.h>\n. Yes that's true, but will this not be optimized by the compiler anyway? I am not sure.\nAlso, is this is less readable?\n\nresult = (result_multiplier<=0) ? 0 : CMath::pow(result_multiplier, power)*CMath::exp(-result);\nreturn result;\n. :+1:\n. ",
    "ialong": "C is the GP prior covariance. Do you want me to return the posterior from the trained GP?\n. also I am not sure there is a method (for CGaussianProcessRegression) to get back the full C matrix, only the prior or posterior variances. So if you want the prior covariance matrix I guess it would be easier to just evaluate the kernel independent of the GP object.\n. ",
    "c4goldsw": "@karlnapf , I searched around a bit for doing SIMD operations or coefficeint-wise operations in Eigen3 and came across this (I can't tell if any operation done here would fall into SIMD), though it doesn't seem this is appropriate as it doesn't allow for in place operations.  I also came across Eigen3 docs for coefficeint-wise operations.  Could you please point me in the right direction for carrying out coefficient-wise operations that are also considered SIMD operations in eigen3?\n. @iglesias  Thanks!\n. @iglesias I should have brought this up with someone else earlier - vec.vlen - 1 makes no sense, especially because the mean is computed using all values in the vector.    I had tested it as it is and it doesn't break current tests (though I'm wondering why previous tests passed with using vec.vlen -1).\n. Well, thank you for providing with that.\n. I don't know if there's a better way to do this (the loop starting on line 299) - I'm trying to make all of the columns of eigMean be equal to tempVec (which holds the rowwise mean for each row)\n. It works on some small unit test - what problem could arise?\nI\u0336n\u0336 \u0336r\u0336e\u0336g\u0336a\u0336r\u0336d\u0336s\u0336 \u0336t\u0336h\u0336e\u0336 \u0336s\u0336q\u0336u\u0336a\u0336r\u0336e\u0336d\u0336R\u0336e\u0336s\u0336u\u0336l\u0336t\u0336 \u0336m\u0336a\u0336t\u0336r\u0336i\u0336x\u0336,\u0336 \u0336a\u0336r\u0336e\u0336 \u0336y\u0336o\u0336u\u0336 \u0336s\u0336a\u0336y\u0336i\u0336n\u0336g\u0336 \u0336I\u0336 \u0336d\u0336o\u0336n\u0336'\u0336t\u0336 \u0336n\u0336e\u0336e\u0336d\u0336 \u0336i\u0336t\u0336?\u0336 \u0336 \u0336T\u0336h\u0336a\u0336t\u0336 \u0336m\u0336a\u0336k\u0336e\u0336s\u0336 \u0336s\u0336e\u0336n\u0336s\u0336e\u0336.\u0336 \u0336 \u0336B\u0336u\u0336t\u0336 \u0336w\u0336h\u0336a\u0336t\u0336 \u0336s\u0336h\u0336o\u0336u\u0336l\u0336d\u0336 \u0336I\u0336 \u0336r\u0336e\u0336p\u0336l\u0336a\u0336c\u0336e\u0336 \u0336i\u0336t\u0336 \u0336w\u0336i\u0336t\u0336h\u0336?\u0336 \u0336 \u0336I\u0336 \u0336t\u0336h\u0336o\u0336u\u0336g\u0336h\u0336t\u0336 \u0336a\u0336l\u0336l\u0336 \u0336M\u0336a\u0336p\u0336s\u0336 \u0336h\u0336a\u0336v\u0336e\u0336 \u0336t\u0336o\u0336 \u0336b\u0336e\u0336 \u0336i\u0336n\u0336i\u0336t\u0336i\u0336a\u0336l\u0336i\u0336s\u0336e\u0336d\u0336 \u0336w\u0336i\u0336t\u0336h\u0336 \u0336s\u0336o\u0336m\u0336e\u0336t\u0336h\u0336i\u0336n\u0336g\u0336 \u0336a\u0336n\u0336y\u0336w\u0336a\u0336y\u0336.\u0336 \u0336 \u0336  I get what to do now.\n. I had mentioned this above, but there currently is no implementation for a columnwise mean, so I'm leaving this unimplemented for now.\n. What do you mean by that?  What will be set for all of Shogun?  Just curious.\n. I realised that it's superfluous, fixed.\n. I had generated interface code and accidentally committed it, so I removed all the files from that directory, restored the ones from the head (that were there by default) and re-commited them.  But, I forgot to restore this file, and after struggling to get it from the remote repository, I just recreated it and did a cut and paste.\ntdlr Still learning about git.\n. Why should we return a reference?  If I think about it, it would be a good idea if we're dealing with a lot of features - the object could be allocated on the heap/free store and a reference could be passed back to LARs.  This would avoid having the whole thing made on the stack (as it currently is), just to have it copied on returning it.  Is this your line of thought @vigsterkr ?\n. The more I think about this, the more confused I am.  I also realise that everything I said above may not be correct - don't EigenMaps just wrap around SG structures?  What would the advantage be of getting a reference to the map?  Would I have to free that reference once it's no longer needed?  I've only learned C and I'm still learning C++, so I appreciate the help.\n. Do what?  This was already implemented.\n. I realised this was superfluous, taken out.\n. Will do.\n. That's already handled by lines 271 to 275 (tested too).\n. Still learning c++ : is there an elegant way to go about this (i.e. getting the type of CDenseFeatures and passing that type onto other things which rely off it)?\n. No - I realise that's what we discussed doing, I'll implement that now (and I believe you mean float64_t and not ST :) ).\n. @karlnapf Also, I won't be adding additional error info to this line or line 284 yet, since CFeatures can't adequately state get the feature / scalar type yet (I can do that in a later PR).\n. Why would we want to increment the values of a vector (as the entries change)?  Whoever implemented this is only using it to make vectors with all 0 entries, so I'll just replace those lines with a memset to zero.\n. \"_i\" was for \"implementation\", but I've changed it to \"_templated now\".\n. Will do.\n. Why though must I check for all types? If you look at lines 119 and 120, I throw and error if the types are not float32_t or float64_t. Isn't that sufficient, given those are the only types that make sense?\nEDIT: floatmax too\n. I'll try to template the test\n. I believe I had placed that in there whilst trying to use EigenMatrixMapXt - I just took it out as there's no longer .\nEDIT: No, I'm mistaken about that: https://github.com/shogun-toolbox/shogun/blob/develop/src/shogun/features/DenseFeatures.h#L26\n. \"_t\" is for \"_templated\".  Should I have something else there instead?  This is done to differentiate it between the templated and non-templated versions of the methods.\n. Do you also understand its usage within the context of the templated functions?\n. Left over when I was attempting to template it the train method, forgot to remove.\n. Any suggestions?\n. @Saurabh7 Thanks!\n. How am I meant to memcpy if the array types differ?\n. Same issue with memcpy as above.\n. I wouldn't be able to tell you, will discus in IRC.\n. Will have to ask about this too\n. @karlnapf would it be @see classname::method ?\n. I'm just including a return statement so that there are no compiler warnings.  This line will never be executed anyway, based off the preceding conditionals and checks.\n. I ensured that it executes using cout, but I'm not sure if that's sufficient - how could a segfault occur here?  The memory will still be there and will still have to be released.  I put this block into place when testing with non-float types because exceptions are thrown by train() in those cases.  Before I made this block, memory leaks were occurring because no cleanup was happening after train() threw its type exception.  Thus, I placed a try-catch block in so that cleanup can happen in the catch block.\n. My bad, I forgot to remove it.\n. How am I meant to do a memcpy if the elements are of different types?  Here's the man page.\nAn alternative I just found is something called std::transform.  Seems appropriate.\n. I'll look into that.\n. I'm not sure there's a way of getting around this.   We're only using a subset of columns to compute mean_neg (specified by classidx_neg[i]) that may not be adjacent.  The only thing I've found that lets you access a subset of a matrix is by using eigen's block operation, but the element's obviously have to be adjacent.\n. CDenseFeatures has subsets, (I'd have to look into that a little more), which we could use to set the values for mean_neg and mean_pos respectively.  But, when setting fmatrix, since it's an eigen matrix, I'd still have to use a loop to set it's values according to the indexes in classidx_neg.  If CDenseFeatures simply creates copies using loops, I think we should stick with loops.\n. CLinearMachine contains a weights variableSGVector<float64_t> w.  I need a templated variable to store the weights during the training before I copy the final weight vector back into w (line 265).  If I were just to use SGVector<ST> w inside of the code, I'd get shadowing warnings.  So, I think it's better just to use w_st.\n. Style question: should I also do this for anything of the form [operator]= (e.g. mean_neg+=)?\n. Hmm, I don't understand what the macro should do - should it compact the below into a single line?\nTEST(LDA, FLD_template_test_floatmax)\n{\n    check_eigenvectors_fld<floatmax_t>();\n}\n. I'll implement a method for this in a separate PR.\n. I'll implement this in a separate PR, since it requires a class/struct which contains the transformation operation.  It would be a good utility method.\n. I'm a bit confused - scatter is an an eigen map, so trace() is eigen's trace method (scatter_matrix is the SGMatrix that is wrapped by scatter).\n. Another style question: if there are inconsistencies with whitespace usage from a previous PR, should I remove the whitespaces?\n. @vigsterkr As far as I know, there is no VectorXt in Eigen - it'd be nice to have that type.  I can create that in another PR.\n. ",
    "cfjhallgren": "Changed this :)\n. It's gone :)\n. Good point :)\n. Good point\n. Added :)\n. I first sample with replacement then adjust by sorting and adding an increasing sequence of numbers from 0 to m-1. Maybe permuting is cleaner.\n. Yes sounds better :)\n. I removed it.\n. Good idea, added it.\n. Will do.\n. Will do.\n. Fixed.\n. Fixed.\n. Fixed.\n. Good name :)\n. Will try using 'some'.\n. Now using 'some' in the whole test file. I saw SG_REF/SG_UNREF is disappearing?\n. Great will check in CMath\n. Thanks :)\n. Done.\n. Good idea.\n. Cool\n. Done.\n. I removed the test.\n. Changed names a bit.\n. Great.\n. Added a check\n. Good point.\n. Maybe I should raise a SG_WARNING and return 'false' instead like below.\nAlso I saw this cast in a couple of other places, so maybe it is checked somewhere like you mentioned that I didn't see.\n. Done.\n. Added it to the constructor and setter instead (for the latter after checking \"kernel!=NULL\"). Also printed out the numbers.\n. Yes perhaps better to have the keyword here too.\n. Thanks I changed it.\n. ",
    "siebenHeaven": "Could you suggest a better name? I'll get an idea of how tests should be named then.\n. ",
    "amarlearning": "@karlnapf  That's wasn't much useful information! Your call, if you say I can add that again ?\nBut this looks good :)\n. @karlnapf Do you want any more changes ?\n. @karlnapf Is there any particular place where you want me to mention that .?\nAccording to me, this should be mentioned at the end of document.. ",
    "sudk1896": "@karlnapf Btw, all the other tests (in SGSparseMatrix) do not implement the style fixes you suggested (about camel case and the for loops). You can check. \n. @lambday: Thanks. Now I get it. So I only change the CamelCase in my newly added test and not in the other (older) tests. That is what you meant, correct ? Also, do you want me to change all the variable names (including the ones in the for loop) which are CamelCase ?\n. @karlnapf: Caveat here, I am following the lead of other tests in the same file. All of them use EXPECT_EQ macro and a select few use EXPECT_NEAR. I remember us having a looong conversation about keeping PRs modular (one change at a time). Also in the previous PR, all of the aforementioned changes weren't suggested. Its better I change the macro in another PR. I will squash and commit and remove the comment as you suggested.. If you have an mxn design matrix, m training samples and n features, it is checking for equality, the no. of features and no. of training samples in the original and transposed matrices. Btw, this is a square matrix, so m=n, which begs the question do we want to do it twice ? I think we should, better you do a check for training samples and the no. of features separately.. @karlnapf: I haven't changed anything else, this is good to merge since I implemented your suggestions (squash commits). . I will add the EXPECT_EQ_NEAR macro in this PR then. . ",
    "IOcodegeass": "u mean d instead of n for dimension of vector. But that would match with 'd' for distance measure. . Is this correct ?\nd({\\bf x},{\\bf x'})= 1-\\frac{{\\bf x^\\top x'}}{\\sqrt{\\sum_{i=1}^{d}{\\bf {x_i}^2}\\sum_{i=1}^{d}{\\bf {x'_i}^2}}}\n. Updated.. ",
    "abhinavrai44": "Yeah sure, but what should the message contain ? Which variables need to be highlighted... ok. Yeah, cause scala can directly use Java files. So I think we don't need another modular for Scala. Ok, right now I was hard coding it for testing it on my system. Sorry. https://github.com/frs69wq/simgrid/blob/master/buildtools/Cmake/Modules/FindScala.cmake. OK. So, I only need to keep the first 2 lines and it will be fine ?. If the json file is removed the corresponding folder binary_classifier, distance, etc in build/tests/meta/generated_results/scala are not created. And therefore the .dat files for scala for each category cannot be saved.. The message got edited in the update. Will put it back in. ok.. Ok. I'll work on some workaround. Got it. :-). Scala gives the error -  cannot access a member of class with modifiers public static. To solve this the class had to be made public. Yes it works. I'll just make the suggested changes and resubmit.. Ok.. Yeah. I'll remove this. @karlnapf The variable involved need to be computed only once as they will remain constant over all the threads. In parallel the same computations will be repeated over all threads. . ok.. Ok :-). omp is used in the function : get_negative_log_marginal_likelihood_derivatives. Sorry. My mistake.. @vigsterkr I didn't know that. Will make the chages. Thanks for pointing that out.. ",
    "MikeLing": "Oh Great! I will add this prefix, thank you :). oh, I just move thechoose_class() [1] from KNN to KNNSolver and change nothing due to karlnapf said we should do that(use SGVector and SGMatrix instead of point) in separate pr. \n[1] https://github.com/shogun-toolbox/shogun/blob/develop/src/shogun/multiclass/KNN.cpp#L578. yeah, but we actually haven't implementation for LSH  which inherited from KNNSolver, so at least classify_objects_k can't been defined as pure virtual. . oh sorry, I will change it to private. Hmmm, so, does CBruteKNN or CBruteKNNSolver sounds better?. hmmm, the output wouldn't been changed I think, but the SGMatrix looks like wouldn't accept a const argument in here(e.g [1]) . It will return:\n\nno matching constructor for initialization of 'SGMatrix' (aka 'SGMatrix') return SGMatrix(output,num_lab,m_k,true);\n\n[1] https://github.com/shogun-toolbox/shogun/blob/develop/src/shogun/multiclass/KNN.cpp#L535\n. sorry...I will pay more attention on the code style next time.. Hi vigsterkr, maybe I haven't get the point here, but why we need to define it as virtual? There is no other class inherit from BruteKNNSolver and redefine classify_objects_k. . Actually, I carefully avoiding to change any code which copy from the old knn.cpp (variable type, function type, etc) and thought we can do these improving in next patch. But I think I should drop this kind of stubborn and fix this and https://github.com/shogun-toolbox/shogun/pull/3608#pullrequestreview-20950822 in next commit , anyway we will do these  sooner or later :)\nApology for letting you point this problem again and again, I should fix them at first. Thank you for your help.. Oh, now I understand. Does it works like:\n\nSG_DEBUG(\"\\nJL Results:\\n\")\nfor ( int32_t i = 0 ; i < res.index ; ++i )\n{\n    for ( int32_t j = 0 ; j < res[i].index ; ++j )\n    {\n        SG_DEBUG(\"%d \", res[i][j].m_index);\n    }\n    SG_DEBUG(\"\\n\");\n}\nSG_DEBUG(\"\\n\")\n\n\nIs that mean it will run this for lop every time? even it wouldn't output anything if we aren't debugging? Maybe we should keep the DEBUG_KNN and just use SG_DEBUG instead of SG_PRINT?. Hi @vigsterkr , I got error while running Ctest[1] and it seems like there are a bunch of errors I haven't meet before[2]. BTW, I run all these test to virtualbox with ubuntu16.04 due to the valgrind doesn't full support OS X10.12.3[3], is that possible I got ViennaCL: FATAL ERROR is because I only give one kernel to virtual machine?\n[1] https://pastebin.mozilla.org/8979031\n[2] https://pastebin.mozilla.org/8979032\n[3] http://valgrind.org/downloads/current.html. sorry...I just forgot we also call CKNNSolve's init() in  its default ctor. >We can see it been significantly speed up by Cover Trees solver, but the result of KD-Trees solver is not so ideally.\n\nThis sentence doesnt really make sense, re-formulate\n\nHmmm, I say the KD-Trees' result is not so ideally is because it takes 3.0s+ to get things done. I know the more dimension the data have, the kd-tree will takes more time to get job done and it more easy to deal with the calculation in low data dimension. But it can't reflect from the result of last comparison, so I just say \"it's result is not so ideally\"\nDo you think it will be better to changed as \" the result of KD-Trees solver seems like not so ideally.\"? I'm not an native English speaker anyway, so please tell me if it's not a good idea, thank you :). It actually will get an out boundary error[1] if we don't switch it\n [1]In file /Users/mikeling/shogun/src/shogun/distance/Distance.cpp line 188: idx_a (0) must be in [0,111] and idx_b (103) must be in [0,36]\nunknown file: Failure\n. Sure thing!. How about create a class named KNNTestData and put all mock data in here? . Hmm, I found actually there is no need LAPACK for unite test. So let's just remove it. :). Hmm, at least no related error and warning comes out after we remove the lapack macro.. hmmm, but the purpose of this pr is about add kdtree test. The rest of code is about fix the error I got after add that test like memory leaking issue and out boundary error. So I think keep this change in this pr would be better.. I'm sorry, that's my fault...... I had stretch the figure so the lines wouldn't overlap with each other. See the latest gist in  https://gist.github.com/MikeLing/c57eef21a17c4371e2dcb3f4ce7505eb :). how about esp or linearepsilon. mmm, but it's named m_ linear_term as class variable  in https://github.com/shogun-toolbox/shogun/blob/develop/src/shogun/classifier/svm/LibLinear.h#L215 :). mmm, I think so. I compile it under the environment with openmp. I don't know why there's no error comes out. I will fix it right now :). Because the generate_gaussians in DataGenerator class require LAPACK. http://shogun.ml/api/latest/DataGenerator_8cpp_source.html. Because karlnapf said \"license missing\" in https://github.com/shogun-toolbox/shogun/pull/3711#discussion_r107633412. So I just assume all the file must been guarded by GPL. Or I should add license in the other way. I see! Thank you. @vigsterkr yes sure! Sorry for the late reply. :). @vigsterkr how about vvpm which means vector produce vector then plus matrix . I know it still looks weird, but I can't find a better name to represent this function does like vector produce vector then plus matrix . ah, thank you. . sorry, I forget to remove this one also. . :/ I forget to add the file after rename it. yes, you are right. We should confine num_labels and num_dim as 2.. I found the comment in https://github.com/shogun-toolbox/shogun/pull/3738#issuecomment-289267398 doesn't really helps to test multi threaded situation. omp_get_num_threads() always return 1 even I export OMP_NUM_THREADS as 4. How could I validate the openmp really works?. I just use parallel->get_num_threads() to get the number of threads, but the result stay the same (single thread) :/  . @vigsterkr yeah, I had notice this issue. And I had send an email in the email list to discuss how to combina CBinaryLabels and CMutilmutiLabels class. For now, I have no idea how to make mutilclass happened in here :(. @vigsterkr why?. @vigsterkr Hi, could you tell me something about set_generic<>()?\nI got bunch of error like\n```\n/Users/mikeling/shogun/build/src/shogun/base/class_list.cpp:815:110: error: use\n      of class template 'CDynamicArray' requires template arguments\n  ...g) { return g == PT_NOT_GENERIC? new CDynamicArray(): NULL; }\n                                          ^\n/Users/mikeling/shogun/src/shogun/base/DynArray.h:34:33: note: template is\n      declared here\n        template friend class CDynamicArray;\n        ~~~~~~~~~~~~~~~~~              ^\n/Users/mikeling/shogun/build/src/shogun/base/class_list.cpp:815:131: error: \n      expected ':'\n  ...g) { return g == PT_NOT_GENERIC? new CDynamicArray(): NULL; }\n                                                               ^\n                                                               : \n/Users/mikeling/shogun/build/src/shogun/base/class_list.cpp:815:104: note: to\n      match this '?'\n  ...g) { return g == PT_NOT_GENERIC? new CDynamicArray(): NULL; }\n                                    ^\n/Users/mikeling/shogun/build/src/shogun/base/class_list.cpp:815:131: error: \n      expected expression\n  ...g) { return g == PT_NOT_GENERIC? new CDynamicArray(): NULL; }\n``. I had remove all the use_sg_mallocs logic in this push :). mmm, but after look more into it, I found maybe useshuffle(sg_rand)in https://github.com/shogun-toolbox/shogun/blob/f7bccee6251d15f5cfee0514c6d22d3bcf5f7a40/src/shogun/evaluation/StratifiedCrossValidationSplitting.cpp#L155 is a better idea. Sorry for remove it before. Oh, I forget to restore these lines in here. I will do it after the rest of the pr works good.. Damn...This should been fixed :/. Mark, this should bem_array[num_elements - 1]. I guess this should be m_array[num_element -1] = NULL. Does this really broken the serialization? I really have no idea, will look into it after I address the another DynArray refactor pr . This one the another place may broken serialization. But it will broken SGObject if not comment it. Maybe we need a better way to register parameter.. OK! Thank you! I will try it before next push. I'm working on the DynamicObjectArray refactor, so I can test if all these implementation on DynamicArray can be implemented on DynamicObjectArray also. Also can helps me polish this pr at the same time. yes, you are right!. Hi @vigsterkr and @lisitsyn , what's the relationship between these parameters andSGObject::clone()? All these parameters will be clone  into generate a new Object? And serialization ?. mmm,m_parameters->add_vector(&head, &(m_array.size()), ....` doesn't work, and it broken the clone progress of DynamicArray. Here is how I implement it.\n```\n        virtual CSGObject clone()\n        {\n            CDynamicArray * cloned = (CDynamicArray) CSGObject::clone();\n            // Since the array vector is registered with\n            // current_num_elements as size (see parameter\n            // registration) the cloned version has less memory\n            // allocated than known to dynarray. We fix this here.\n            // cloned->num_elements = cloned->m_array.size();\n            return cloned;\n        }\nprivate:\n\n    /** register parameters */\n    virtual void init()\n    {\n        set_generic<T>();\n        T* head = m_array.data();\n                    // & doesn't accept a rvalue and it seems like auto will make 'size' unrecognisable for add_vector()\n        int32_t size = m_array.size();\n        m_parameters->add_vector(\n            &head, &size, \"array\", \"Memory for dynamic array.\");\n\n        SG_ADD(\n            &free_array, \"free_array\", \"whether array must be freed\",\n            MS_NOT_AVAILABLE);\n        SG_ADD(&dim1_size, \"dim1_size\", \"Dimension 1\", MS_NOT_AVAILABLE);\n        SG_ADD(&dim2_size, \"dim2_size\", \"Dimension 2\", MS_NOT_AVAILABLE);\n        SG_ADD(&dim3_size, \"dim3_size\", \"Dimension 3\", MS_NOT_AVAILABLE);\n    }\n\n```\nProgress stop in here\n\n[ RUN      ] DynamicObjectArray.array_SpectrumMismatchRBFKernel_clone_equals\nshogun-unit-test(91199,0x7fffe91903c0) malloc:  error for object 0x10637f125: pointer being freed was not allocated\n set a breakpoint in malloc_error_break to debug\nAbort trap: 6\n\nAnd here is some lldb debug message https://pastebin.mozilla.org/9024822. This head need been changed to GAUSSIANCHECKERBOARD_HPP. This method may not necessary because we are using accuracy to measure the result . The name of this PR is add binary Label fixture, but in order to prove the GaussianCheckerboard can works for multiplelabelclass, I add this global fixture and apply MulticlassOCAS_unittest to use it. Hope it won't confuse other people. And I will rename this pr after the rest of it is good to go :). Make sure we have at least two kinds of labels(otherwise we don't need all these things in here). Will remove it if unnecessary :). The array_size in here will indicate the size of this array like how many memory been allocated to this array(like m_array.size() == array_size). And num_elements means how many elements be used in this array. So they are not overlap actually.\nSorry for the wrong conclusion I made last time.\n  . this should been changed to the size of array. got it. just serialization maybe better?. mmm, these code only use once in here, do we need a helper function to do that?. where we should put that helper function? In Utils.h maybe?. oh, I somehow miss it. I will do it right away.. mmm, maybe I misunderstand something in here. If I pass \"serialization-asciiCDynamicArray.XXXXXX\" directly to generate_file_name, it will generate warning like:\n\n/Users/mikeling/shogun/tests/unit/lib/DynamicArray_unittest.cc:266:42: warning: ISO C++11 does\n      not allow conversion from string literal to 'char ' [-Wwritable-strings]\n        char filename = generate_temp_filename(\"serialization-asciiCDynamicArray.XXXXXX\");\n. mmm, I get Bus error if I change it with  char* filename = (char*)\"serialization-asciiCDynamicArray.XXXXXX\"; And it will happened in mkstemp. So I guess it's because I haven't assign a length of memory to that pointer. Anyway, I think we don't need to worry about it should be a pointer or not :) . But it's not only about label, it also had the test and train feature. How about MulticlassFixture?. :(. no, this compute_eigenvectors https://github.com/shogun-toolbox/shogun/blob/develop/src/shogun/clustering/GMM.cpp#L600 also require LAPACK. we need use linalg add function in https://github.com/shogun-toolbox/shogun/pull/3859/files#diff-b59cfc6c549dd52160d0ce73b6356b04R377. mmm, but I got error message like \nerror: no matching function for call to 'add' if I don't return reference. More error message in here https://pastebin.mozilla.org/9025837. no, I somehow forget to remove the seed in here.  If we want to init the seed, we should call CMath::init_random(seed). same as above, it will be removed in next push. sure, I will keep that in mind. mmm, I think we still need to free the array. Otherwise we will lost some memory https://pastebin.mozilla.org/9026080. \n\nAnd if i change it to \nvirtual void SetUp()\n{\n    array = SG_MALLOC(T, 5);\n    for (int32_t i = 0; i < 5; i++)\n    {\n        array[i] = (T)i;\n    }\n    wrapper_array = new CDynamicArray<T>(m_array, 5);\n    SG_FREE(array);\n}\n\nThe memory check will be ok https://pastebin.mozilla.org/9026081. Set a fix seed here is about to make CrossValidation in CQuadraticTimeMMD.select_kernel can choose 0.25 kernel width every time[1]. Let's see the CI result(ctest passed locally) and add find a way to set seed for it outside.\n[1] https://github.com/shogun-toolbox/shogun/blob/develop/tests/unit/statistical_testing/KernelSelection_unittest.cc#L292  . Hi  @karlnapf and @lambday, my question is like this:\nIn kernlelselect and other places related to CrossValidation, we need to set a specific random seed to make it can generate same result every time. Usually, before we remove sg_rand, it can been done like sg_rand->set_seed(12345). Then it will pass to the CrossValidations like here. \nBut after we remove the global random, it become a little difficult to make that happened, I think. Because in kernel selection test, we can't directly access its cv object like we do in here to set its seed. \nSo I add one more parameter which is the seed, and make it default return an fixed seed(12345) and pass it to  MaxCrossValidation like this. \nBut I'm not sure if it's appropriate to return a fixed seed a default when we do kernel select, and I'm kind of doesn't understand why we choose 0.25 as kernel width rather than others. \nThank you!. yes, you are right. It's no necessary in here..... Because we want to call m_rng->set_set_seed() in CSObject, so it's impossible to do that if CRandom still inherit from CSGObject.. sure, so basically as I comment in https://github.com/shogun-toolbox/shogun/pull/3888#discussion_r125174002, it's about to set default seed of random as sg_rand->set_seed(12345). And it's going to set a fixed seed in https://github.com/shogun-toolbox/shogun/pull/3888/files#diff-e76beae1ecab5719393a08a37a5a5ef8R101 for CrossValidationMMD. But it's impossible to do that (give a fixed seed to CrossValidationMMD inside), so that's only way I can pass a fixed seed as parameter to it from outside.. ok, I will do this in next push. yeah, you are right. so that's the problem I have when I'm working on the global random removal issue. Usually when we want to ensure all the module we call will have a fixed seed to ensure they have same output every time, we call sg_rand->set_seed(seed) But after we remove that:\n\n\nDo we want a fix seed for every module we used in the tests?(I guess yes :)\n\n\nHow to pass a fix seed to nested module? like when we do kernel selection, we need CrossValidationMMD inside, is that necessary to change some places just to ensure we unit test can passed every time?\n\n\nMaybe we have a better way to deal with these unit tests to make sure we get same result every time rather than give a fixed seed?\n\n\nMaybe I will have more questions about this, but please give me some guides above. Thanks a lot\n@karlnapf @lambday . sure, I will do it in next push. It will been removed in next push. . mmm, the logic in here is a bit of odd in here. I define the global random seed in init.cpp because here is where we define global variable. Then I do setter and getter function in SGObject in here due to we need to reference shogun/base/init.h every time if we want to call set_global_seed() or getter otherwise.. Hi @vigsterkr, for using global random seed in default CRandom ctor. If I write it like https://gist.github.com/MikeLing/c5d618c176a14aec06f3411ce1b30c5e I will get a undefined symbols for architecture x86_64 c++ error. But everything works if I use CSGObject::get_global_seed() instead of sg_random_seed instead. I don't know why this happened :(. that's a mistake, let me remove it right away! . > the generate_seed function should not be part anymore of CRandom...\nSo we need to move it to CSGObject? . Hi @vigsterkr and @lisitsyn , the linalg add need a reference rather than a value like add (SGVector< T > &a, SGVector< T > &b, SGVector< T > &result, T alpha=1, T beta=1) , I just don't know how to make this \nlinalg::add( components[1]->get_mean(), components[2]->get_mean(),  components[1]->get_mean(), alpha1, alpha2); works if I don't return a reference in here. Maybe I'm asking a dumb question, but I just don't know what should I do to make it works :(\nThank you\n. Hi @karlnapf ,   I already fix the missing parameter with global function call in here. Thank you for your help :). mmm, what's the in-place add of linalg you mean? . mmmm, I'm not sure why this doesn't work:\nalpha_sum = alpha_sum_v[i];\n        SGMatrix<float64_t> v=dotdata->get_computed_dot_feature_matrix();\n        auto column_vector = SGVector<float64_t>(alpha.get_column_vector(i), alpha.num_rows, false);\n        linalg::matrix_prod(v, column_vector, mean_sum);\nMaybe it's not about matrix product ? Or I misunderstand something here?. alpha.matrix[j * alpha.num_cols + i] is a element rather than a vector or matrix. So I think it's something like vector^{2}R(R is a real number) rather than matrix multiplication. Does it make sense to u? :). Hi @vigsterkr , I somehow miss this comment. Actually this function is a part of math before refactor, I just move its implementation to .h due to we need use template parameter in it. So do we need to move it to CRandom and keep it(the random class) ? . Hi @vigsterkr , I got a bunch of warnings in this LOAD macro like https://pastebin.mozilla.org/9029036. All of them are about'num_str' is uninitialized when used here. Do I need to give it a value like '0' or ?. @vigsterkr Hi, Does all these changes for Kernel.h make sense to u? I just want to make sure these are necessary changes so I can change other places related to kernel. Thank you :). Hi @vigsterkr , so the change in here is actually wrong, since const_cast from 'StorageIndex *' (aka 'int *') to 'index_t *' (aka 'long long *') is not allowed. But as you know the StorageIndex is actually more complex than int32_t which can't change directly. So what should I do next? Thank you!. Hi @vigsterkr and @karlnapf , so the int32_t replacement finally comes to the unit test refactoring. First of all, \nI'm not sure if these changes for RandomForest_unittes, KernelSelection_unittest and  RandomCARTree_unittest.cc make sense to you. But I remember we had discuss these test for random refactor and we need to change them somehow. So I just want to point it out in here :)\nSecondly, for CMath permute in here, I really don't know why the result changed. Does replace int32_t with index_t can make that change happened? I think I need to look into it later. And verify its result on Linux platform(I'm working on the OSX right now)\nthere still have some unit tests failed locally:\nThe following tests FAILED:\n    179 - unit-ProbingSampler (Failed)\n    285 - integration_meta_cpp-clustering-gmm (Failed)\n    294 - integration_meta_cpp-distance-cosine (Failed)\nAnd for ProbingSampler, it depend on colpack and looks like doesn't aceept index_t.(it can only use int, I think). I'm not sure if that's the reason why make the result different, will look into it later :). @vigsterkr mmm, so I keep this part due to we have this error https://pastebin.mozilla.org/9030480, it seems like AllOf require two parameters more than one?\n. so the unit test for ProbSampler still failed :(. ",
    "piyushgoel997": "can you tell why isn't it required?. Actually, I'm using windows so I'm unable to run the tests. I tried doing what is mentioned in the getting started guide but it doesn't work for windows. I tried the ways mentioned on the internet but couldn't get it working. The only solution I found online to do testing is to use Visual Studio, but since I don't have it installed on my laptop. So can you tell me is there a way to do it using the command line or any better way, or should I install Visual Studio. Thanks.. ",
    "micmn": "so I'll leave (guarded) #include omp.h; is it possible that parallel->get_num_threads() != 1 and openmp isn't available? in that case if I replace omp_get_num_threads() with that the step size would be wrong, do I need to consider that case?. they were like that in get_distance_matrix_helper https://github.com/shogun-toolbox/shogun/pull/3621/files/4d1f092534b2d9b1ba730ee19061994265d8ac2c#diff-7542e21aede27fe5307ee5008c99fa45L298. so isn't better using the actual number of threads spawned and in case of no openmp setting num_threads=1?. I've commented out this because CLabels doesn't have get_labels(). Which one? A square? A.num_cols == b.num_rows?. Cool, the implicit cast to SGMatrix works but, at least in this case, the other way around does not: EigenSolver<T> doesn't accept a Map type and I can't find a way to avoid the explicit cast.. I'm not sure I'm following here, currently the headers list is given by {$LIBSHOGUN_HEADERS}, instead of passing this list as an argument to the script (or headers_list.txt in case of MSVC) you want to pass the ctags file and in the script having the function parseCtags() from generate.py, then extract all the headers from this dictionary, something like:\ntags=parseCtags(arg)\nHEADERS=filter(lambda x: x.endswith('.h'), tags.values())\nAnd the remaining part of the script stays the same, correct?. yeah this is something i wanted to ask you, the idea is to port everything to linalg?. @vigsterkr are you suggesting to remove these undefs?. ok but in this case the header that contains the macros can't be guarded otherwise a second inclusion like in the cpp will not work.. Self-review: this is wrong.. @karlnapf I can't figure out why he does this (L110-117). @karlnapf This mean removal step from the kernel is not carried out, is this related with L110-117? Can't see how.... Will address these linalg issues in #3843 . I have a gist somewhere...here we go https://gist.github.com/micmn/f93f723b74db2a1eb5875f63d841bdc1. I'm having some trouble with this, in this way data gets deallocated, how do I pass to SGVector without copying?. Is there any reason to handle this differently? It's never used anywhere.... ok (should be done in the already implemented methods too).. Do you mean that we should take the conj transpose when T is complex?. yeah, could we add get_column(i) -> SGVector in SGMatrix that does this?. like a method Matrix.transpose() that flips the flag?\nthen every linalg method should check its status or we should find another way, first thought: when we cast to EigenMatrixXtMap we also check the flag. add(matrix, vector) that adds the vector to each column of the matrix?. Added to train_machine. The problem is that we cast ST to float64_t. check http://www.netlib.org/lapack/explore-html/d2/d8a/group__double_s_yeigen_gaeed8a131adf56eaa2a9e5b1e0cce5718.html#gaeed8a131adf56eaa2a9e5b1e0cce5718\n\nISUPPZ is INTEGER array, dimension ( 2*max(1,M) ). Eigen computes all the eigenvalues and dsyevr should too but for the latter I'm not 100% so I'll double check.. Maybe it's better \"compute all the eigenvalues and returns the top-k etc...\". This is to make it work with generated tests since I can't call set_num_rkhs_basis(), otherwise I could set by default m_num_rkhs_basis = 1 instead of 100  . Sure, the only difference is just calling store_model_features() before the serialization, right?\nI'll address this in the next PR after merging this one since I have already some work done for GP and Multiclass.. mmm actually it doesn't compile with \"some\". maybe wrap() is the correct one?\n\ncan't find any usage though. this is needed because:\n\n~DenseFeatures() calls SubsetStack->remove_all_subsets(), that unrefs m_active_subset\n~Features() unrefs m_subset_stack triggering its dtor ~CSubsetStack() that unrefs m_active_subset a second time\n\nso setting it to null avoids the second unref. one issue I see is that the method declaration cannot be generated since it has to be in the header. ",
    "geektoni": "I've added the BSD headers. Not sure if they are the right ones though. Tell me if you prefer a different license version.  . Yes, I wrote it. However, I need to admit that I took inspiration from some C++ guide (like this one http://www.cplusplus.com/reference/cstring/strstr/) and from some StackOverflow answers (for the lambda thing).\nShould I insert some links as acknowledgement?. The most suitable methods that I've found were mkstemp, mkdtemp and tmpnam. Unfortunately, none of them can be used because:\n mkstemp creates a new random file and return a file descriptor to it. It is safe, but some unit tests need only to have the filename to open a new file;\n mkdtemp creates a random directory and it is not what we need;\n* tempnam does create a random filename, but it suffers from the same problems of mktemp (http://en.cppreference.com/w/cpp/io/c/tmpnam);\nAlthough some of the classes used inside the unit tests (CSerializableAsciiFile, CSerializableXMLFile and CCSVFile) permits to create a file given its FD, the CStreamingAsciiFile class has not this ability. So, I opted to simply write this small function to resolve the warnings, without touching too much the unit tests code.\n. Adding the Scala module to DISABLED_INTERFACES will cause to not include examples/meta/scala/CMakeLists.txt. Only the Scala integration tests will be generated (and they will fail) and the flag -DScalaModular will become a bit useless (it will behave like an alias of -DJavaModular=ON). \nThis hack permits to create all the Scala tests anyway (generated and integration), but as I said before, this is just a workaround to prevent the continuous generation of the meta examples code.. By vectorized you mean \"vectorized by\" the compiler (using something like -ftree-vectorize), right? I understand your point. Maybe the progress bar can be more useful on longer evaluation methods (I suppose the only candidate here is cross-validation).. Are you referring to last_progress or v? If you're referring to last_progress I think of -1 as a value that means not initialized and I used it since there are cases in which last_progress is not set (for example when max_val-min_val<=0). Obviously, this is completely subjective. Using 0 instead of -1 should not make any difference. :). Core developer's will is law ;). Jokes aside, I will change it to 0. As I said, it is the same :+1: . argh noob fail. Can I use EXPECT_FLOAT_EQ? Seems to have the same behaviour as EXPECT_NEAR and also its meaning is more straightforward.. Same question as above\n. Yeah, I am still curious about them ;). Regarding IRC, I could be online tomorrow morning, will I find you @karlnapf?. Updated as requested.. Actually, I'm not really sure this line will work (since the check_format.sh script needs bash).  . This method is run each time we print the progress bar to account for the terminal size, which can change (for instance, if the user resizes or expands the window). However, I'm not so sure this is worth it (since it doesn't work so well). Should I remove this line? @vigsterkr . I was planning to throw an error if the user enables the progress bar, but the terminal size is too small to print it. This is currently disabled because it makes the unit tests crash. Do you think it is something we need? So that I can work on re-enable it without destroying the test suite. @vigsterkr   . The method gets the size of the terminal, which will be used to calculate how much space we will use to print the progress bar. If the user resizes/expands the window, this method will correctly get the new dimensions, but there are cases in which the progress bar will be still printed in a wrong way (even if the program \"knows\" that the terminal size has changed).  . It is calculated dynamically. There is a formula above which will calculate how much space we will use to print the progress bar. Sometimes the result can be 0 (or less), and that is a case in which we cannot print the progress bar.. @vigsterkr I've added a simple ParameterObserverInterface class, as you suggested yesterday, to implement the behavior of subscribe_to_parameters(). . A possible output is something like:\nList of CrossValidation's observable parameters\n* fold_result_0 [scalar]: Intermediate fold evaluation results. There will be many parameters fold_result_i, where i is between 0 and the number of folds.. Hi @OXPHOS\nThe PR #3934 should solve the issue. Once it gets merged, please rebase this PR on develop and remove the commit https://github.com/shogun-toolbox/shogun/pull/3932/commits/2fc8ab7b002981794bd13e80c4d6d3e3c2b5b020. After that, everything should become green on CIs. . Yes, there is a place where it is used https://github.com/shogun-toolbox/shogun/pull/3939/files#diff-9c3599c0d2090e493be261b079e9b63eR883. mmh apparently we didn't in the end. :P. I would say it's clang-format fault. Anyway, no, there are no particular reasons for the postifx operator instead of the prefix one.. It seems it is not needed to make things work. . I'm currently working on a simplified version (https://github.com/geektoni/shogun/blob/add_cross_validation_mkl_meta_example/examples/meta/src/evaluation/cross_validation_mkl_weight_storage.sg), which should show only the weights for one fold and one run (without cycling over all the observations).  . What do you mean here? :P Should I keep only the test and remove the meta example? Or it's better to port the meta example and keep also the test? . The user won't enter into REPL loop if m_active is set to false. In such a situation, the handler won't ask the user any inputs, it will just terminate the execution and it won't print any message to the screen either. So I decided to maintain a consisten behaviour between the Shogun's handler and the standard system handler when dealing with a SIGINT: exit and set the exit code != 0. . You could have used the compact version here, instead of the pb.print_progress/complete methods :) namely:\nfor (auto i : progress(range(0, max_num_epochs))). Sure! That's totally feasible. I should be able to make a patch for it later today.. #4240. m_cancel_computation.store(true); is an instruction that has to be written everytime we implement the on_next method in derived classes. Since developers could forget about it, we should add a new method called, let's say, on_next_impl() which will be the one that will be actually implemented and then write on_next as:\nvoid CStoppableSGObject::on_next()\n{\n    m_cancel_computation.store(true);\n        on_next_impl();\n}  . Same here, instead of the comment there should be on_pause_impl.. And same here, just for the sake of consistency.. From connect_to_signal_handler to on_complete they should be all protected methods since we do not want to expose them to the users.. Instead of while(1) we should try to use a more meaningful stopping condition. Here you could use this condition.. Same here. You could add as a condition iter<=num_iter and then move the warning message (\"Maximum number of Newton steps bla bla\") out of the loop. It is not super neat, but at least we do not have while(true). . Pay attention to the if indentation here :) It seems to be wrong from Github.. We were thinking about removing them, since they seem to be not useful (we\u2019ll see the outcome with the CIs).. I reckon here there should be the entire condition (training_time_clock.cur_time_diff()>get_max_train_time ())&&(get_max_train_time ()>0) to avoid to print messages when they are not needed.. @shubham808 I guess we can then remove the WIN32 guards.. @shubham808 Same here.. Do we need these headers (atomic, condition_variable, mutex)? I think we could delete them :) . The build won't fail without those headers because we already have included them in previous headers file, so could you please remove them? :) . It is already implemented into CMachine:train. However, there are some classes which implement their own version of the train method and therefore they do not have the connection with the signal handler.. This test shows more clearly what we are going to do in the future when testing the premature stopping feature.. Yes, it does :) . It seems a good idea ;). I think that is caused by the fact that some Shogun's class does not have the get_name method, because they do not inherith from SGObject. For those classes I think that using the standard progress bar could be enough. . As a common practice, shouldn't we name macros with uppercase names (so that we know it is a macro and not a function/method call)? It may be a pain to rename everything again though.  . This is good but in my opinion, the serialization should be placed into on_pause_impl() instead :) devs may want to provide other behaviours for algorithm instead of serializing them by default (and to do it now they would have to override the on_pause method, which is no good).. I agree. Stacked progress bars are not handled very well at the moment.. Yes, the pb definition has to appear before the #pragma.. I would prefer to see the actual object name, because if I had three progress bar inside three different train() method, I would like to see to which model (CPerceptron::train(), CAveragedPerceptron::train() etc.) they belong. So it is okay as it is done now :) . I guess that this guy should be protected, right?. You could call here, right before the end of continue_train, the method  reset_computation_variables (see here). So that each time the continue_train method finishes, you can call it again.  . We could enforce constness here with this new design, right? This can be done for all the method below (continue_train, iteration). @karlnapf . See my comment below :). Aren't we doing a sort of batch learning here? This iteration method will run over all the features so, if we premature stop continue_train and then we call it again, the iteration method will just run over all the features f from the start. This would work if we provided to iteration a set of new features/labels each time continue_train is called. @karlnapf @shubham808  . reset_computation_variables reset the flags used to prematurely stop the computations (see the StoppableSGObject class). If you do not call this and you have prematurely stopped the execution, the next time you will call train/continue_train those methods won't work because of COMPUTATION_CONTROLLERS (since the flag is still set to false). . Yes, I think this is fine here.. I see what you mean. However, the problem here is caused by the fact that those two files don't exist anymore, so even if I moved them into the SVMLight section I would face the same issue (cmake refuses to run because it doesn't find those files). . I did a bit of research around the repo and the only file that matches is an undocumented python exampled (examples/undocumented/python/graphical/svmlin.py). I also tried to search for those files inside git history, but I found nothing. If we look at the buildbot (http://buildbot.shogun-toolbox.org:8080/#/builders/4/builds/175) we see that three months ago everything was working. I tried to search for relevant changes in the last three months, but again I found nothing. I guess we can remove those files since they seem to have been not renamed.. The buildbot will spot errors (hopefully) if we missed something anyway.. The output of this example is the following:\nroot@2bec4daebb48:~/shogun/examples/undocumented/libshogun# ./example\n[%] Received an observed value\n[%] Received an observed value\n[%] Received an observed value\n[WARN] In file /root/shogun/src/shogun/regression/LeastAngleRegression.cpp line 357: Convergence level (0.458377) not below tolerance (0.000000) after 3 iterations.\nIdeally, each time an observed value is capture by the logger, it will print it on the terminal.. The original idea of this logger was to print on screen (or on file) the string representation of the observed value.\nHowever, I do not know if it is possible within Shogun (do we have a sort of to_string() method for all Shogun's objects?) or if it is feasible without using hacky ways.. The issue I'm facing here is how do we explain/show to the user that he can observe this value betas? I thought that we could register somehow some \"fake\" parameters inside the class which cannot be updated/obtained with put()/get(), but they can only be observed by the user (e.g. SG_ADD(&beta, \"betas\", \"Weights of the model while training\", ParameterProperties::OBSERVABLE);). @karlnapf what do you think?. As a side not, I am not able to include this header file at all (that's why everything down is commented out). I keep getting this strange linker error here:\n/usr/bin/ld.gold: error: CMakeFiles/libshogun.dir/lib/parameter_observers/ParameterObserverLogger.cpp.o: multiple definition of 'shogun::type_internal::print_map[abi:cxx11](std::unordered_map<std::type_index, shogun::TYPE, std::hash<std::type_index>, std::equal_to<std::type_index>, std::allocator<std::pair<std::type_index const, shogun::TYPE> > > const&)'\n/usr/bin/ld.gold: CMakeFiles/libshogun.dir/io/TBOutputFormat.cpp.o: previous definition here\n/usr/bin/ld.gold: error: CMakeFiles/libshogun.dir/lib/parameter_observers/ParameterObserverLogger.cpp.o: multiple definition of 'shogun::type_internal::get_type(shogun::Any const&, std::unordered_map<std::type_index, shogun::TYPE, std::hash<std::type_index>, std::equal_to<std::type_index>, std::allocator<std::pair<std::type_index const, shogun::TYPE> > > const&)'\n/usr/bin/ld.gold: CMakeFiles/libshogun.dir/io/TBOutputFormat.cpp.o: previous definition here\ncollect2: error: ld returned 1 exit status\nMaybe the solution is straightforward but I can't see it :(. Cool! For now, I will leave it just a simple debug observer which will be also useful to check that we are getting the correct values out of the machines. . I was thinking more about registering inside the machine directly the parameter we are going to observe. For LARS it could be something like:\n```c++\nvoid CLeastAngleRegression::init()\n{\n    m_lasso = true;\n    m_max_nonz = 0;\n    m_max_l1_norm = 0;\n    m_epsilon = CMath::MACHINE_EPSILON;\n    SG_ADD(&m_epsilon, \"epsilon\", \"Epsilon for early stopping\", ParameterProperties::HYPER);\n    SG_ADD(&m_max_nonz, \"max_nonz\", \"Max number of non-zero variables\", ParameterProperties::HYPER);\n    SG_ADD(&m_max_l1_norm, \"max_l1_norm\", \"Max l1-norm of estimator\", ParameterProperties::HYPER);\n    SG_ADD(&m_lasso, \"lasso\", \"Max l1-norm of estimator\", ParameterProperties::HYPER);\n    SG_ADD(&beta, \"betas\", \"Weights of the model while training\", ParameterProperties::OBSERVABLE);\n\n}\n``\nThebetasparameter will not be modifiable byput()\\get()` and it can only be captured using an observer. Therefore, the user may be able to see which kind of information can be extracted from the model at runtime using an observer. \nObviously, I do not know if SG_ADD is the right method, but I would like to signal somehow the presence of these emitted values.\nSee what I mean? . Okay then, I may try to fix it here in this PR.. Yes, it could be done. Actually, this example was taken directly from one of the unit tests, so it was not using any data files.. In principle, everything you see there is portable to a meta example. I'll try to see if I can convert it (maybe after we polish this one).. mmh cool. I'll add factories in another PR.. mmh it can be done. I have no strong feelings about it.. SG_OBS_VALUE_TYPE will be removed. I will update this when the other PR goes in.. Issue solved.. All the observer will store the values they capture. For now, they will be kept as Any values since they may be really different from each other (a scalar, a vector, etc.). Does it seem clean enough?. This works perfectly. However, some problems arises when trying to get an observation out of the observer:\nc++\nCrossValidationStorage obs = mkl_obs.get_observation(0)\nCrossValidationFoldStorage fold = obs.get_fold(0)\nThe call to mkl_obs.get_observation(0) extract one Any value. Is there any way to cast it somehow to its proper type by using the meta language? :/ @karlnapf any ideas?. Okay, so I've come up with another idea which I think it's the most viable option (and also it does not expose Any to the final users). Basically, the observers will have an std::vector<ObservedValue> parameter which will contain all the observation captured so far (we will not use the Any). If I want to access it from the interface I can do:\nc++\nParameterObserver pm = parameter_observer(\"ParameterObserverCV\")\nObservedValue obs = pm.get_observations(1)\nObservedValue will contain both the observed Any value and the name of it as registered parameters. Therefore, we could extract the value itself from the ObservedValue like:\nc++\nobs.get_real(\"value\") // if we know that the value is real\nobs.get_real(\"value\") // if it is a string...\nThe only possible issue is that inside ObservedValue we would need to register (with SG_ADD) an Any value, since we cannot know a-priori which will be the type of the observed value. \n. See above discussion.. Not sure. If we use the subscribe method of SGObject it may not be needed.. See above discussion.. In order to expose the observations (aka std::vector<ObservedValue>) we can:\n  * Use the current method (get_observations(int i)) which won't need std::vector<ObservedValue> to be registered with SG_ADD;\n  * Use get and add a new SWIG getter like %template(get_observation) CSGObject::get<std::vector<ObservedValue>, void>;. We would need also to add a new type called ObservedVector which will correspond to std::vector<ObservedValue>.. It is fine with me. It will be more consistent with the general Shogun's behaviour.. It is because we do not want to make the application fail if the received value is not of the requested type. Let's imagine that I have several parameter observers attached to the same machine, let's call them x and y. They may receive the same emitted values from the machine, let's say A and B (with a different type). However, x may care only about A and y may care only about B. If x receives B, I do not want to make it crash the entire application. It just have to ignore that emitted value an process the next one.\nSee what I mean?\nI can elaborate further in IRC if needed.. This guy here returns directly a vector of strings (with the name of the observable parameters), but I am not sure if this what you intended @karlnapf.. This should enable get for the value observed. However, I fear that this is not the solution either since we would just be able to \"get\" again an Any value.. Here name is actually a char *. From what I could see, std::string is not accepted by the parameter framework (?).. I was not able to get any std::string (or even char*) using get, therefore this line (and some others) causes an error. However, it should be pretty reasonable to do that or?. These tests are disabled because the parameter framework does not support the std::vector type.. The issue is caused by this definition of put here (https://github.com/shogun-toolbox/shogun/blob/develop/src/shogun/base/SGObject.h#L391). Basically, when we try to extract a string, the put method will check only the options added with SG_ADD_OPTION and it will not consider parameter which are of type string.. The class Parameter does not have any method to store an std::string object. I guess we could add it. Should it be straightforward, right? . Same here, the Parameter class does not have any add method to add std::vector objects. Moreover, Any still not support std::vector (however, this should be fixed by Viktor's PR).. No, they are not needed. Will remove them.. See issues above.. :dancer: :dancer: :dancer: . This is more of a convenience method to make some things works. I'm trying to see if I can remove it somehow :/ . It seems so. Still checking though.... It works indeed :D . SG_PRINT is fine? It should be okay since it uses the SGIO object which could be defined by the user. . I guess this method will need to stay here for a bit longer. The only place where is needed is here https://github.com/shogun-toolbox/shogun/pull/4552/files#diff-d4ce27ca628f4ba5cacbb66e265af3d0L114, but it is not completely trivial to remove it.. Since I need to expose Some to SWIG, I've tried to add it to the build. However, I keep getting these warnings:\n/root/shogun/src/shogun/base/some.h:71: Warning 508: Declaration of 'get' shadows declaration accessible via operator->(),              \n/root/shogun/src/shogun/base/SGObject.h:487: Warning 508: previous declaration of 'get'.                                                        \n/root/shogun/src/shogun/base/some.h:71: Warning 508: Declaration of 'get' shadows declaration accessible via operator->(),                      \n/root/shogun/src/shogun/base/SGObject.h:496: Warning 508: previous declaration of 'get'.                                                        \n/root/shogun/src/shogun/base/some.h:24: Warning 509: Overloaded method shogun::Some< shogun::ObservedValue >::Some(shogun::Some< shogun::Observe\ndValue > &&) effectively ignored,                                                                                                               \n/root/shogun/src/shogun/base/some.h:20: Warning 509: as it is shadowed by shogun::Some< shogun::ObservedValue >::Some(shogun::Some< shogun::Obse\nrvedValue > const &).                                                                                                                           \n/root/shogun/src/shogun/base/some.h:46: Warning 503: Can't wrap 'operator shogun::ObservedValue*' unless renamed to a valid identifier.\nMoreover, the entire compilations fails with this obscure and mystical errors from SWIG:\n/root/shogun/build/src/interfaces/python/shogunPYTHON_wrap.cxx: In function 'PyObject* _wrap_SomeObservedValue_put__SWIG_2(PyObject*, PyObject*)\n':                                                                                                                                      \n/root/shogun/build/src/interfaces/python/shogunPYTHON_wrap.cxx:837537:0: error: 'shogun_ObservedValue_put_scalar_dispatcher_Sl_int32_t_Sc_int32_\nt_Sg___SWIG_2' was not declared in this scope                                                                                                   \n         shogun_ObservedValue_put_scalar_dispatcher_Sl_int32_t_Sc_int32_t_Sg___SWIG_2((shogun::ObservedValue*)(arg1)->operator ->(),(std::string\n const &)*arg2,arg3);\nAny clue about what is going on? @karlnapf . I would prefer to leave it as it is, since it may cause confusion if we changed it to observe. Namely, observe emit a value, make_observation is a utility that creates an observable object.. What do you mean here by won't be available?. Yes, it is indeed the default value.. I guess this will need a bit more of discussion. It may also be a good example of a good entrance task though :). What kind of checks? This method should just return the total number of observations, or am I missing something? :/  . I think that it would be better to do this kind of task in another PR, since it is somewhat related to refactoring the observers (which it is not related to the ObservedValue). Plus, I would like to merge this soon such to keep working on the other tasks. . Okay, so, since std::vector::size returns a size_t value, I will keep also the return type of the function equal to size_t.. It is needed if we want to make the user able to extract it from the parameter map with get. . Sure! . That's because CrossValidationStorage has some methods to store/get objects (e.g. get_fold()) which are not present in SGObject. I guess that I could remove all those methods and use directly the tag framework. We could then remove get_CV_storage. . get(\"fold\", 0) would be super useful indeed.. @karlnapf were you thinking about something like this? . Updated.. @karlnapf This way we will store directly also the Any reference (I don't know why the formatting is messed up :/). . If this is an std::vector, then it won't have the element_at method, just at (if I understand correctly what it is happening here). . Is this throwing any errors to the user if the supplied index is out-of-bound/negative?. sooo is that try block okay? LoL. I would say by value because it is more self-contained. . Yes, it should be.. I guess that return by reference would be okay.. ",
    "lkuchenb": "Trailing whitespace. Trailing whitespace. Wrong format specifier %d. clone() SG_REFs the copy. If this fix is right then this code never worked for vectors of non-primitive types?. The downstream function TParameter::copy_stype() checks target_ptr->slen - so this occasionally has strange effects if not initialized I think... . I was hoping you guys could tell! There was that obscure comment in the code here, I can't say for sure the patch is enough. Certainly the code before wasn't all that was needed :P. Ha, didn't know DynamicObjectArray had a child... And it was forgotten whenever this beauty here was written.... Was thinking about that, too. I don't know how you guys want to go about this in the future. If you want to make use of this feature in future changes it implies changing more than needed for a fix in many cases, since compilers will give warnings when override is used inconsistently within one unit.. Writing virtual is just eye candy, it's virtual if it's virtual in base. The only specifier that would make an actual difference here is override. If you want the virtual here for code style, I can add it.. ",
    "hangg7": "I was thinking about the same thing for we already have the autogenerated context.. ",
    "deveshnag1": "It's acting strangely...\nIt works if I open it in new tab. Also working properly when opened using jupyter.\nOnly gives error when clicked normally in github preview.\nSuggestions?. ",
    "tdjogi010": "I thought reading all of it in one would be better than comparing using git diff for this commit. Will refactor things asap.. C++\nwhile (ex_used[current_write_index] == E_NOT_USED)\n         current_read_index = ex_read_index.load(std::memory_order_acquire); // for synchronisation\nThis is what I think:\nEven though the ex_used itself is not atomic, doing ex_read_index.load(std::memory_order_acquire);(soon will replace it by std::atomic_thread_fence(std::memory_order_acquire); ) in every iteration will synchronize every store before ex_read_index.store( ... ,std::memory_order_release); even the  non atomic ones.(as suggested here ) So changes to the ex_used will be reflected after the std::memory_order_acquire. \n@vigsterkr Let me know what you think.\n. Yeah... don't know what I was thinking back then.. How do I profile it using valgrind (or something else)? \nI am thinking of making the elements in ex_used atomic(like vector<atomic<E_IS_EXAMPLE_USED> > ex_used) then we can use something like \nC++\nwhile(ex_used[current_index].exchange(E_NOT_USED)==E_NOT_USED){}\n...//get the example\nex_used[current_index].store(E_USED);\nIf not can you elaborate more on how to use exchange for our case?. Sorry my bad. I meant to ask what test needs to be run for profiling(or any benchmark in benchmark folder)?. ",
    "iRmantou": "Ok, I have fixed them! : ) . Ok, I will fix these style problems, but I saw some source code that use {} for single line commands in BinaryLabels.cpp, so I Imitate them.. I have run shogun-unit-test before I sent this pr, I am sorry I dont know where I am wrong, you mean I need create a small test or example passing CBinaryLabels to CMulticlassClassifier?. ",
    "elenanst": "Glad to change it, but are you sure about it? I haven't come across the term \"Feature converters\" in the machine learning literature.. If you are referring to Bayesian Reasoning and Machine learning, there's no reference to MDS. Maybe Modern Multidimensional Scaling from Borg is ok? (also cited in the documentation of CMultidimensionalScaling). It is not demonstrated, but  this  converter.set_landmark(False) deactivates it. So, if I remove this paragraph I should write something like \"We deactivate an optimization technique\". I'm afraid that's a bit vague and I didn't find any relevant case in the cookbooks, so what's your approach? How about just dropping the second sentence?. I believe that the purpose of this boolean variable is to test whether the distances are preserved after the transformation, which is already ensured by the distance_preserving test in \"MultidimensionalSaling_unittest.cc\". So, no need for another one.. ",
    "tingpan": "Thanks for these comments. Just finished refactored and squashed all fixup commits:). ",
    "dhelekal": "I agree with you -- however the implementation of combined kernel seems to expect 0 being returned if its out of bounds, hence why i went with that.. Same thing as above, I tried preserving as much of the original code as possible (perhaps not the best choice). The original code seems to expect 0 being returned by convention if index is out of bounds when accessing data from multiple structures. However if you agree i'd be more than happy to rework it so that this isnt the case any more.. I operated under the assumption that the subset should be maintained in sync with the combinedfeatures used to init the combinedkernel. Normally, as per my understanding, the subset for the subkernels is obtained from the features used to init them. (The init(features l, features r) method of custom kernel in fact clears the subsets).\nAs the custom kernel has no features in the 1st place, I attempted to replicate what an init() would do if it had features.\nAlternative approach: I could instantiate CIndexFeatures using lhs_subset/rhs_subset, then init the custom kernel using these. This might actually be a better approach, on a second though. Update: There seems to be an inconsistency within CCustomKernel::init(CFeatures* l, CFeatures* r).\nNamely, if l & r are of type CIndexFeatures the normalizer doesnt get called, see following block\n```\n    if (l->get_feature_class()==C_INDEX && r->get_feature_class()==C_INDEX)\n    {\n        CIndexFeatures l_idx = (CIndexFeatures)l;\n        CIndexFeatures r_idx = (CIndexFeatures)r;\n    remove_all_col_subsets();\n    remove_all_row_subsets();\n\n    add_row_subset(l_idx->get_feature_index());\n    add_col_subset(r_idx->get_feature_index());\n\n    lhs_equals_rhs=m_is_symmetric;\n\n    return true;\n}\n\n/* For other types of CFeatures do the default actions below */\nCKernel::init(l, r);\n\nlhs_equals_rhs=m_is_symmetric;\n\nSG_DEBUG(\"num_vec_lhs: %d vs num_rows %d\\n\", l->get_num_vectors(), kmatrix.num_rows)\nSG_DEBUG(\"num_vec_rhs: %d vs num_cols %d\\n\", r->get_num_vectors(), kmatrix.num_cols)\nASSERT(l->get_num_vectors()==kmatrix.num_rows)\nASSERT(r->get_num_vectors()==kmatrix.num_cols)\nreturn init_normalizer();\n\n``\nIs this intentional or not?. I'm not 100% certain what do you mean re: consistency; could you please clarify it to me, perhaps on irc?. OK, so keep the current method, rather than providing dummy indexfeatures, and calling the customkernels init on those? (+ and fix the memory issues). oh that's odd, no clue why my editor suddenly switched to spaces instead of tabs. (. Yeah, sorry, i didnt want to commit that change.\nIt's just to make the style checker run locally. Working on reverting this.\n. As we're casting a pointer type, it should be NULL on fail afaik? From the website:If the cast fails and new_type is a pointer type, it returns a null pointer of that type.`. ",
    "olinguyen": "I'm a little confused when to use UNREF. When should it be used? There are some cases where new objects are created, UNREF is not run and there are no memory leaks.. It is called in apply_binary, but I set the default threshold value to 0, as it initially was, so it shouldn't be affecting the existing code. . I didn't write these tests, but by removing the number of threads set to 1, the assertion for the oob error fails (exceed 1e-6 tolerance).. Here I assumed that the previous test data values were correct, and just asserted that the probability had to be > 0.5 for a +1 output label. the RandomForest unit test also uses the same data. What's the best way to share data across unit tests?. The probabilities change though unless I increase the # of trees as you stated earlier. think it can stabilize the results a bit. woops, I meant to set it to 0.11 since it exceeded by 0.10000000000000003 . Sorry, since I'm using sklearn's toy data, what should be in this gist?. Is it appropriate to create a generate_test_data header and cpp file just for data generation? Should I place this in the tests/unit/utils folder? Sorry for asking a lot for this point, I just want to avoid re-changing it later.. it will be addressed in a future PR. we said we'd leave the tests as is and and iterate over them in another PR. Please correct my if there was a misunderstanding.. ",
    "Sahil333": "yes, it should be inherited but, without it, the compiler gives error \"build_subsets() is not defined\". So, I left it there and compiled successfully.  .  Thanks, I'll take care of that.. I'll change it to min_subset_size.. I'll initiate m_rng and m_min_subset_size in init()? . Peace, I'll mention it more precisely. . Okay, Let me see.. Right, I'll make it more explicit. Can you tell me where can I ask my specific questions about shogun? When I am stuck somewhere in shogun library.. I saw the FisherLDA_unittest.cc, great way. I will do like that. . Oh, I see. Instead of checking that every element in the training set is smaller than every element in the test set, we can check min in the test set is greater than max in the training set. Will it do the trick?.  It shouldn't, as I need the floor of (N/K) and that's what the division would return.. I need labels to know the number of indices needed to split. . Shall I remove both the test with a single manual test using the fixtures?. ",
    "durovo": "You are right.. I was under the impression that TEST and TEST_F cannot be used in the same file. Should I remove the empty class and change the corresponding TEST_Fs to TESTs? Also, the tests are the same as before.. I don't think that I borrowed this from somewhere. I'm not sure though... Should they be initialized to 0?. Should I use SG_ERROR to indicate that the machine should return binary labels when the labels are not binary?. That looks like a great idea. I will try doing that.. Yes, I'm currently trying to vectorize wherever possible.. Understood\n. I have now used a template function to hold the common code. I'll make a pr shortly. Should they not be initialized to NULL?\nI was following what was done here:\nhttps://github.com/shogun-toolbox/shogun/blob/15fdbcd7520191d27ee9ad69f196da0023c841e2/src/shogun/evaluation/MachineEvaluation.cpp#L81. The python interfaces for these files are generated automatically right? Or do I have to add these files to some list? I am asking because I am not able to access these (CrossValidatedCalibration, Calibration, etc) from the shogun module in Python.. Okay. Hello @vigsterkr, I have used the approach that you have mentioned (using get_label_type and REQUIRE). However, shogun seems to have a LabelsFactory class which has methods for doing the same thing. Is that the recommended way of doing this?. I borrowed this from another test. I will write proper test cases after we discuss the api. Or should I write them now?. Understood, I'll add the descriptions for all methods (and params) and classes.. The result from the first machine has already retrieved just before the loop. This is a dirty hack that I used because I couldn't find any method to get the number of samples in the features. . Understood. However, what is the standard way of getting the number of samples from a given set of feature? (i.e. how do I know how many labels will be there in the predictions before actually calling apply?). Okay.. Understood. . Understood.. CalibratedClassifier does seem like a better name. Okay :)\n. Yes, counting one of them would be enough. I using std::count_if would indeed be neat.. I am not the one who originally implemented fit_sigmoid, I merely made a few corrections in it. However, the code can certainly be improved. I'm on it.. I will replace this with std::vector... DynamicObjectArray seems to support only CSGObjects, it can't hold structs like CStatistics::SigmoidParamters. Please correct me if I'm wrong about that :). I was thinking of adding common fit and calibrate methods that would call fit_binary, fit_multiclass etc as required. Yes, there doesn't seem to be any point in the implementation. I'll just add those in the header file itself.. Understood. Probably not. Indeed there is no need to make them virtual.. Thanks for telling me about DynArray, I was looking for the standard shogun way of storing arrays of non-CSGObjects. . Yes, my bad. I'm not sure about making all methods pure virtual... suppose a calibration method does not support multiclass calibration, then the implementation will have to derive fit_multiclass and give error message manually. Would that be fine?. This one? /\n * This software is distributed under BSD 3-clause license (see LICENSE file).\n \n Authors: \n/. I have opted to use two SGVectors for now. Understood. Okay. It's all from the old fit_sigmoid. It is mostly copy-pasted. Should I rename it?. Okay. This code is mostly taken from the previous fit_sigmoid. Do you recommend changing all the messages?. Okay, that looks much better :)\n. Sure. I trained a linear SVC in sklearn, this is the decision function of that SVC. The expected output is the probabilities of the calibrated SVC.. Understood.. Okay. That would look much better indeed. I'm not sure myself. It was in the original code https://github.com/shogun-toolbox/shogun/blob/297f9a0c5d4771154515fef2fac865fdc1a1af5e/examples/undocumented/libshogun/labels_binary_fit_sigmoid.cpp#L22. Should it the first line be something like: \"Stores parameter A of sigmoid for each class\"?. Understood.. Yeah, it seems does seem very redundant. Will change this.. Okay. I missed this, sorry for that.. Not required? Will remove them.. Will do, thanks for the suggestion. The length of the parameter vector is checked in calibrate now and an appropriate message is displayed.. @vigsterkr I opted to use std::transform. ",
    "vinx13": "They are computed with shogun, before my modification.. There's already a GMM meta example and test data. So I removed this unit test.. Yes, just outdated. I will look into that to find the possibility of using pure linalg. . I found eigen_kernel_matrix.diagonal() += eigen_tau;. We are not able to implement adding a vector to a diagonal with SGMatrix or linalg. Any suggestions?. Removed. Please take a look. . Just followed other test cases like SGMatrix_cholesky_solver. Seems the style checker changed other codes than mine. I'm using command like git clang-format-3.8 --commit xxx , is it right ?. Just noticed that the original code is indented with spaces.. They are different. Actually I was wrong in #4167. Now it is the same as the original one.  https://github.com/shogun-toolbox/shogun/blob/431e56b67344e0662444c0efbd4aefde47990fa4/src/shogun/metric/LMNN.cpp#L113. transpose matrix creates a new matrix.\nThe API is indeed inconsistent, maybe we should add transpose flags to element_prod ?. Well, here the trace is used as the termination condition. Seems that it doesn't stop until reaching maximum number of iterations when trace is wrong in tests. . Yes. I'm testing against Matlab prototype here https://github.com/iglesias/lmnn-gsoc2013-review. @karlnapf I made another pr #4183 , with which I could make it clear here. code that creates features and labels could be shared with other tests, but this could make it difficult to modify test input. shall we do this refactor?. The original with TRACE is correct, the former one in this diff is wrong. . We are calculating trace of L' * L * gradient, which is sum( (L'L) . gradient' ), instread of trace( (L'L) . gradient) that appears in the previous commit. actually we can't, GMM is a  CDistrubition, where 'features' is not registered. some transformers actually do not need fitting, so i leave this a no-op. this is a branch for release, each version is a single directory. install with provided cmakelist requires a higher version cmake than ours\nhttps://github.com/mpark/variant/blob/29319715a1f0eb0980d380db8a2fda5af8d58feb/CMakeLists.txt#L8. @karlnapf yeah i realized this is wrong. we should update generator to automatically wrap the raw pointer here. @karlnapf unfortunately it doesn't :(\ngenerated cpp doesn't compile. features_train is Some, which is implicitly cast to raw pointer, and apply always return raw pointer here. the header is mixed with declaration and implementation, so i split into .h and .cpp as they are done in other template classes, e.g. DensePreprocessor. auto formatter did this :(. my mistake, thanks . CDimensionReductionPreprocessor is dropped from both cpp and interface.\nIt has fields like m_converter, m_kernel, m_distance but they are not used by subclasses it most cases (e.g PCA). So it is not very helpful as a superclass. And we don't need to wrap a converter with it now. there are many transformers that do not need fitting, we don't need to put a empty fit in every subclasses, but only one place here\nbut as for apply, it should always be implemented (except those abstract classes). we can provide more informative error message here, but as is as good. of course. unfortunately we can't use const for the time being because there are many non-const methods that should be const logically, e.g. get_feature_matrix. i'm going to just throw exceptions in fit. but for now train returns boolean, i keep this so that it is consistent that fit always throw exceptions when failure.. no, this is not a parameter :(. i found several links point to this paper, e.g https://github.com/gmum/pykernels/blob/c8afdc79a15197ad3be2a0db0118f5e948577f49/pykernels/regular.py#L132\nand I've checked that paper, the author gave a definition of it in sec.3. ICPR papers are not re-assigned a page number when they are published. This is based on the reference format provided in https://ieeexplore.ieee.org/document/4761717. Yes it works. you can use SG_THROW(exception_type, message...), but that's in feature/transformers branch that haven't been merged. i tried features(file) to load a string features into a dense one, no error happens until you actually use the features in some algorithms.. this is a virtual function that calls copy ctor of subclasses. this is taken from michele's pr, we need small refactor on this to deploy views. since view create a new instance, using smart pointers prevent ref/unref. actually no, this function is only used internally, so we know the static type of return value based on types of args . agree, we don't need clone here.\nadd_subset should accept a const vector, and we need to clone the vector in ctor of CSubset. we don't have a way to test different features/labels types, and different access methods now.\nand we can't check view's data pointer because that's a private member, calling get_labels will create a copy if subset is present. yes, and i think set_feature_matrix should be also removed (after some refactor). since we would like to drop setters in featuresset_num_vectors, set_num_features, we can't an empty features and then set feature matrix later, and here there is no need to store it in a member field. we don't have factory for CSplittingStrategy. could you explain how to use factory here?\nwe could create a factory PipelineBuilder* pipeline_builder(), or?. sure. @karlnapf since there are multiple subclasses of CSplittingStrategy, are we going to do some string comparison by name?. i think Pipeline and PipelineMachine might be a bit confusing, while PipelineBuilder can indicate its usage as a builder. so we need to do initialization in init() method, instead of the constructor\ne.g. https://github.com/shogun-toolbox/shogun/blob/1be1dbbf6fdce20e73901d205cee253f2d4cd873/src/shogun/evaluation/StratifiedCrossValidationSplitting.cpp#L21\nthis need refactor right?. with string feature factory, we can port meta examples now. allright . @karlnapf i'm not clear, could you explain the multiple inheritance problem?\ncurrently we expect T to be either features or labels, but we can also use viewable mixin in features and labels. @karlnapf @vigsterkr maybe we can just return Some<T> here?. the idea is to separate construction of pipeline into a single class so that the pipeline object is immutable, and then we don't need to verify elements of pipeline everytime. the second one, Pipeline can be Machine here because that's enough, we use it as machine in xval. however, if we want to get elements in the pipeline, we still need Pipeline type for now. yes this overload the factory name, the factory works in this case. thanks for letting me know.. it's a protected method from base class, you cannot access it of another instance in the derived class CPipeline. this comment was left before, i just copy-pasted to make the method public. this can be moved to .cpp file so that the inclusion order problem can be fixed. a problem here is that we can only call the default ctor from swig, then we may still need to call build_subset from the outside? @karlnapf . there are some style issues, we are using tab indent, see https://travis-ci.org/shogun-toolbox/shogun/jobs/442635134#L728. why w_size+1? if w has w_size+1 elements, the last element is w.vector[w_size]. Thanks for clarifying. The two code snippets indeed are equivalent, but I am still confused.\nWhen you set w_size to the size of weights (without bias), w.vector[0] ... w.vector[w_size-1] are weights, and w.vector[w_size] is bias. Maybe you have different idea?. Use SG_THROW or SG_STHROW to throw an exception with custom type. \nThe first argument is the type of the exception. The rest of the arguments are the same as SG_ERROR\ne.g.\nSG_THROW(ShogunException, \"...\"). There are many non-const methods algorithm internal methods. We can either use non-const arguments here, or use const_cast to drop const in internal methods. @karlnapf One problem ref counting is IterativeMachine. We need to increase ref count of features and labels in init_model. Agree, it should be in the base class. I will check if it is possible for now.. This is useful for const pointers (const CFeatures*). Although we decided to move to const methods later, I leave it here for possible future usage.. @karlnapf Got some linking issue\nUndefined symbols for architecture x86_64:\n2019-01-07T16:59:36.8868140Z   \"bool shogun::CLinearRidgeRegression::train_machine_templated<double>(shogun::CDenseFeatures<double> const*)\", referenced from:\n2019-01-07T16:59:36.8911800Z       shogun::CDenseRealDispatch<shogun::CLinearRidgeRegression, shogun::CLinearMachine>::train_dense(shogun::CFeatures*) in LeastSquaresRegression.cpp.o\n2019-01-07T16:59:36.9066190Z   \"bool shogun::CLinearRidgeRegression::train_machine_templated<long double>(shogun::CDenseFeatures<long double> const*)\", referenced from:\n2019-01-07T16:59:36.9110490Z       shogun::CDenseRealDispatch<shogun::CLinearRidgeRegression, shogun::CLinearMachine>::train_dense(shogun::CFeatures*) in LeastSquaresRegression.cpp.o\n2019-01-07T16:59:36.9263540Z   \"bool shogun::CLinearRidgeRegression::train_machine_templated<float>(shogun::CDenseFeatures<float> const*)\", referenced from:\n2019-01-07T16:59:36.9306660Z       shogun::CDenseRealDispatch<shogun::CLinearRidgeRegression, shogun::CLinearMachine>::train_dense(shogun::CFeatures*) in LeastSquaresRegression.cpp.o\nCLinearRidgeRegression::train_machine_templated is a template method, its definition is in LinearRidgeRegression.cpp and is not visible in this cpp file.. @karlnapf I only saw this error on CI. I cannot reproduce this locally with docker image.. yes, we need to explicit instantiate templates in cpp files to make it linkable. using SGVector can make it simpler, we can get rid of SG_MALLOC and SG_FREE. train_machine_templated returns void now (we should also do this to train later). @karlnapf I think set_w has been already called inside the iteration function?. no...but every other factory has this, do we need it?. yes there is np.* :). @karlnapf I feel that the list of factories is expanding :). use .as for type casting. using SGVector can make it clearer since you don't need SG_MALLOC / SG_FREE. use linalg::set_const as per https://github.com/shogun-toolbox/shogun/issues/2747. do u mean the mechanism in MulticlassLabels? BinaryLabels doesn't have such thing. agree, it will be simpler. I see, so I will store variance to CLabels::m_current_values. One thing I'm concerned about is that this may be confusing since higher scores usually mean higher confidence. But the variance is different. So when users call labels.get_value they will get value with totally different meaning. What do u think?. I guess the compiler can figure template parameter here?. are those setters gonna be removed in the future?. have you removed all HAVE_CXX11 macros? otherwise this will break other things. ",
    "shubham808": "Yeah I did make a lot mistakes sorry about that. In case if openmp is not supported this will work like a serial implementation i found a similar thing at shogun shogun/features/DotFeatures.cpp. I tried parallel but didnt do the trick maybe i will try again. i will use samples directly in the next patch. Interesting.... this is easier i will use that for sure.. I deleted it from all opfunc classes because it appears to be deprecated. is it okay ?. solved. solved\n. this is the other loop and i think we can keep it as is maybe ?. This was a style issue I have fixed it now... i think. typo error because we decided to not use any kind of aggregators and directly use samples. I have used linalg in all opfunc classes however the DenseMatrixExactJob class use mat.log() which is not in linalg so it still uses Eigen3. I have directly used the iterator i here which simplifies it.. Looks useful I will try to implement it.. yeah i am confused too... also the RationalApproximationCGM tests pass its the one with \nRationalApproximationIndividual that fail.. okay i am on it.. yeah this looks good.. must have been the style formatting. It was not being used anywhere... we could put it back in when we add progress bar to it (after this gets merged ). the pragma didnt work.... error: 'samples' does not have pointer or array type . That change was requested because it was not very informative. no test broken :). I am having trouble using const in compute due to this ..\nThe other method i.e. sample(j) is thread safe.. I understand.. i am replacing static casts with conversions like in kernel svm. That line looks like new CMulticlassLabels(binary_labels(m_labels)) the check can now be removed because we are already making sure of correct label type downstream (in binary_labels).\nAbout directly converting dense labels to multiclass that is not yet complete here. we will have to use binary till then.\n. okay so i will add 3 more:\none that passes binary labels, dense labels, and multiclass labels. but i think we will need to figure out our conversions to multiclass in LDA before this.. to covert directly to multiclass using multiclass_labels we will need to do two things first:\nAdd a case to convert binary labels here. We already have a constructor for that in CMulticlassLabels. \nsecondly because we expect labels like [-1, 1] (for example in lda unit test) we will need to take care of the require statement which expects contiguous labels. We could write a small loop to take in discrete values and turn them into contiguous integer labels. \nis this the correct approach here ?\n. yes we can take care of binary and multiclass easily however if we pass dense labels we will need the std::transform conversion (like in binary) too. That is do the same thing we do with binary labels (converting [-1, 1] to [0, 1])except now the labels are dense. This is not already present in multiclass_labels but instead we have a require statement that checks the dense labels to be of type [0, 1...] (this is why we needed to convert first to binary and then  to multiclass earlier) what we could now is replace the require statement with a loop that does exactly that with discrete valued dense labels. . will it not be better to add the regressionlabels exception test in multiclasslabels unit test ?. yeah i am aware of that :+1: . actually this pr needs some more work so i am holding style check off till the end :). i am not sure how.... I tried doing it at LDASolver but that messed up FisherLDA so i guess its better to do it here.. I have simplified it a bit more now :+1: . yes i am working on it. Fixed :+1: . see here this is a similar situation and the example compiles correctly. yes :+1: . mmm that would have worked if we had a way to tell if a shift has been previously negated. we need to remember something in compute (like if m_negated_shifts is null or if the flag has been previously set) but due to compute being const now it will not allow this..... any ideas?. yeah :+1: thats a good idea. yes we can :). I think we should keep them since the build is failing without it . see that its &max_iter and not &m_initialize_hyperplane. m_w is private so we will need to use get_w() everywhere instead... this causes type errors with v.add(gradient, w). i see. mmh... i thought that too but did not find any @vigsterkr any ideas ?. due to a class that use a progress bar being external i have commented class_name out for now. because the macro was having trouble infering type if we kept it in between or last. we dont always pass *this->io so i guess its better to pass it when we need to. this is list is limited by 2 things \n1. The algorithms that can be systematically tested for serialization\n2. The algorithms that do not converge before starting second iteration.\nFor now I have also not included any KernelMachine algorithms here . yes i did :) this only the part i want to add . yes but we could use it anywhere else too... easy serialization method. removes repetition of the same code this is a cleanup from the previous version :). i think its better to whitelist because blacklisting will mean having to blacklist the same algorithms over again for each test.. for example: we will have to add the algos in the IGNORE list here as well (since they cannot be initialized in the same way). I think just \"PROGRESS\" is the simplest version here :). the static version. we will need to discuss this :). @vigsterkr sure we can rename this :). @vigsterkr the idea is to serialize in Ascii as default when pausing but the user can always change that, infact even specify what type he wants before pausing :) I am still working on it atm. you are right i did miss that..... this can be moved in the loop sure. for error checking after loop ops maybe we should add an end_training() method to IterativeMachine. Done as a special case of LinearMachine for now (as discussed). this will need to be a method to revive computation controllers after cancel flag. after fixing this a test like here should work.. I have used the __FUNCTION__ macro here now :) . yeah that works... However there are more bugs so the test might be a while for now. i see yeah . this was temp.. we will remove it. because this was not being initialized earlier in the same way... we dont need it when we overload train_machine and use members instead of args for features. how about we just use the m_features member already present in LinearMachine ?. Suprisingly this works but doing .begin() in the same line does not.. we will need to merge this before we can run this test otherwise it will just time out in an infinite loop.. agreed. i think this is not so efficient. @karlnapf ? . i think int32_t should be fine or ? also swig has trouble in using \"put\" with int64_t. we can do directly with member but i think we talked about updating members in the end of the iteration ?. no there is not. right yeah. this is a different test. I train one perceptron for reference result then i trained another to stop after one iteration. the results_intermediate should not be equal to results (expects false) . but when we continue_train we will converge properly and results_complete will be equal to results.. @karlnapf there is a setter for this i have just not used it here. the loop here is such that i think we will need a local variable to check when m_complete needs to be changed( that is check if we never enter the if statement here) if that is so i think the converged bool works fine or?. with the end_training method or?. i used m_current_iteration here so that even if it is prematurely stopped the warning shows correct iterations.. we dont want warning in case of premature stop ?. but the CSignal will show something if we press CTRL+C. we do not need to unref when we leave continue_train() right ?. what we could do inside train_machine is: \nsgref(data)\nsgunref(m_continue_features)\nm_continue_features = data\ncontinue->train()\nif(naturally stopped)\nsgunref(m_continue_features)\nthis will cover our cases and we do not need to unref in destructor then.... will that work ?. the 8 is to diffrentiate between SG_ADD and the macro here... not anything meaningful really. yes I will need to use the progress object here. this is the implementation of COMPUTATION_CONTROLLERS macro but with this-> appended . yes! the results after this are:\n| Benchmark                         | Time     | CPU      | Iterations |\n|-----------------------------------|----------|----------|------------|\n| DataFixture/Put_perceptron/8/0    | 120 ms   | 119 ms   | 6          |\n| DataFixture/Put_perceptron/64/0   | 193 ms   | 193 ms   | 4          |\n| DataFixture/Put_perceptron/512/0  | 837 ms   | 837 ms   | 1          |\n| DataFixture/Put_perceptron/4096/0 | 3782 ms  | 3781 ms  | 1          |\n| DataFixture/Put_perceptron/8192/0 | 11831 ms | 11731 ms | 1          |\n| DataFixture/Put_perceptron/8/1    | 124 ms   | 123 ms   | 6          |\n| DataFixture/Put_perceptron/64/1   | 172 ms   | 172 ms   | 4          |\n| DataFixture/Put_perceptron/512/1  | 615 ms   | 614 ms   | 1          |\n| DataFixture/Put_perceptron/4096/1 | 5285 ms  | 5278 ms  | 1          |\n| DataFixture/Put_perceptron/8192/1 | 10290 ms | 10279 ms | 1          |. which ones should we register here ? :). @geektoni we can add the observer test similarly now :). this is a mistake...  no need for public ;). default option will also work for features classes not yet in the switch cases, train_without_features  makes more sense. i think we will need a new macro for iterate_dense. i think iteration need not be implemented everywhere we have train_dense or ? (since dispatcher will be used in for eg LDA which is not an iterativemachine). yeah it is not needed anymore . @karlnapf making const gives errors here. as discussed we will update this to linalg in the other pr. i have a q about the operator Xsv[ kx_d+j ] translates to Xsv( k, j ) right ?. actually size_sv changes every iteration so the size of memory is not constant over here. yeah newtonsvm does not implement it for now. it gets smaller than bigger... its better like this i think. as discussed check for dot features is enough. i left it like this because using linalg here will lead to allocating 3 more temp vectors each iteration of this loop and then a copy to temp1... vector_multipy seems more efficient here. yes yes :) will fix it. yes the dimensions would be incorrect. no i did not find any . it is only used in newtonsvm it seems :). we cannot use linalg::svd because it does not compute V matrix which we need to calculate inverse. yes i agree. we can add more cases like we did in macro version and then throw error if nothing matches. this would be CTypedMachine. yes.. will fix that. this will be general but i have only written the dense case. yes this is a bit tricky. actually it was already false we are taking it out of the constructor\n. features is CLinearMachine member CLARS do not store them locally... so i think all will have that member like CLDA because it is in machine. yes it should be const . how do i extract variable ?. ah i see :). /home/atom/shogun/src/shogun/machine/TypedMachine.h:49:98: error: \u2018class shogun::CLDA\u2019 has no member named \u2018train_machine_templated\u2019\n     return obj->template train_machine_templated<float64_t>(data->as<CDenseFeatures<float64_t>>()); \nalso here obj = this->as<P>() . yes i see it :). now both train_dense and train_string do not accept empty data so anything that uses dispatch will need to pass them in train. ran into a few errors with this... will clean this up in next push. yes since we are enforcing data should be passed during train. yes that is the plan. yes i agree :). this can go into the class doc when we write it. i have not tried yet. test for exception when trying to pass StringFeatures is missing . is this a good way to test it ?. actually we dont need it :) will remove altogether. travis was causing errors.. i think this should fix or ?. i think the check here is okay ? https://github.com/shogun-toolbox/shogun/pull/4373/files#diff-cc285e900e1599bd903dbb6d435f6089L52. making it protected caused errors with some<> also train_machine_templated need to be public so i will update it when i look into making it friend. actually this exception is generated in cmachine before we check for feature type in mixin so i think this is okay for a single case or ?. int, short int and char seemed okay... but we can add more to it. the same error as in CMachine here or ?. Machine.cpp line 51. no we need this here. this is cleaner since it will avoid multiple calls to initialize_neural_network. i think this is cleaner since there are no states now :) the code will work like it did before . right i will edit the cookbook too :). looks smooth :). yes that is a better name :). this is more flexibile and we also need it to pass primitives from swig. there should not be since i only introduced bias change but i will check again. It should be CFeatures but then we will need to check feature type in each iteration... for now we are only using IterativeMachine in LinearMachines which are DotFeatures.. yes it is called from within train_machine. this seems same as CMachine::train so it will not cause any problems but i see it is planned to be moved there after refactor. yeah init_model is taking features and labels as arguments which is more flexible :+1: . yeah we should do this in each subclass. yes this is better. i dont think we need this setter here @karlnapf  ?. the problem is we wont know what to cast data to as it is only known in subclass so doing this here makes more sense. we can make this new setter typed . ",
    "guruhegde": "I will undo the changes.. Ok. @karlnapf \nQuestions:\nWould you like me to comment out the add_matrix call having static_cast in parameter, i found one in ParameterCombination.cpp?\nIf so, shall i send single PR to address both(comment add_vector + add_matrix)?\nWhile commenting the code, should i leave a remark with any issue no.?\nI made necessary changes for #4110. will send PR soon.\nThanks. \n. I removed tag register here, by mistake i didn't remove new line. hope it's ok. . @karlnapf ok. That looks lot cleaner. Will modify and update.\nwould you like me to send a PR to set default argument in existing constructor of AnyParameterProperties? or Make new constructor(like you commented in #4132)\nI guess this PR can wait till above-mentioned changes get merged.. ",
    "syashakash": "Corrected.. Yes, I will. \nFirst I would add a commit to this  PR with removing the cookbook page and then add a commit for data revision. . ",
    "grg121": "Ahh ok, I thought it had to be with NN examples, I'll move it and remove the python example...\nAlso... I'm seeing that one check didn't pass which is weird because I just send the data file... don't know :/ \nWell, I'lll do that and send another commit! . another PR* xp. ",
    "luisffranca": "I've changed that, thanks.. In init() m_min_subset_size is set to 1, which is then used by build_subsets() (called by the constructor). If set_min_subset_size() is called, we should rebuild the subsets. I've found this due to a failure of timeseries_subset_linear_splits unit test.. build_subsets() you mean? It isn't called on init() in this class.. Done.. I've changed this according to the Issue description. I think this shouldn't be a problem, given the possibility to return it to const or non-const variables. Should I remove it?. One way to achieve this would be to allow m_min_subset_size to be passed as an argument in the constructor (default value 1) and remove the method set_min_subset_size(). Do you think that would be a problem?. I agree and will remove it.. ",
    "FaroukY": "Sure, I've now included into the example evaluating the test error.. Changed them to features_train and features_test respectively. . No Problem. Removed that. Done. Done. Done. Simplified the expression. Sorry for all the capitalizations (trying to get rid of that habit). I've tried to remove all capitalizations and make every sentence on a new line to reduce the diff. . Done. Sorry about the grammar. I was writing this at 2 am so I wasn't in my best form. Will make sure I keep it in mind. . Done. Done. Removed it and made the flow slightly better. . Done. Added it.. Added the sentence to the top of the file.. I used the string and vector in the tests, so I will only delete , is that okay?. Done. I did think about this, but when I did it, it had a lot of if statements and it wasn't very readable (for a test case). Are we sure we wanna do this?. Done. Done. Done. Done. I set up the GIT_TAG as required. Changed the names to be more descriptive. . Removed it and simplified the functions with extra helpers. Done. It doesn't use lapack, so I removed the guard. . Done, I replaced vector with SGVector and removed both includes for string and vector. Done, I replaced this with transpose_matrix after the training data is filled in.. I did use some in the beginning but it leads to several problems since some function calls that instantiate elements do a SG_REF with new, so in the end, when I UNREF some of them, sometimes they don't need SG_UNREF (because they used some) and some need SG_UNREF because they used new. This lead to a lot of segmentation errors so I just replaced all with new so that I keep track of what memory i need to free.. done, moved all inline comments to  separate lines.. Done. there is already initialize_train_and_test_and_ground() for the normal data, so I used the _simple suffix to indicate this is the simple data initializer (which is entirely different from the non simple one). Good point, did some renaming to get rid of initialize_train_and_test_and_ground() and initialize_train_and_test_and_ground_simple(). Actually I agree, I got rid of both initialize_train_and_test_and_ground and initialize_train_and_test_and_ground_simple. Thanks!. Done. Fixed. Fixed. Changed it.. I removed the transpose_feats_matrix() method and replaced it with the calls with get_transposed().. Done, changed it to BSD license. . Not sure which 3 lines. Do you mean the if statement?. Done. @karlnapf  Doesn't get_transposed() create a new transposed matrix? So I'm just deallocating the old matrix (that i transposed)? Or does get_transposed() act in place?. Fixed. Removed it :). Ohh I see, get_transposed() handles the memory deallocation inside of it. So no need for SG_UNREF :). Yea i got a bit confused. Since I store the transposed matrix always in train_feats, I need to deallocate the memory before I leave the function or else we leak. The TearDown will release the transposed matrix (which is not stored in train_feats). Will get rid of it.. Will change it.. I am not sure what you mean here. What's N and D? Do you mean the variable names?. Yes, but in this case, there is only one data point ( [1 1 1]^T) so the data is linearly separable (in an infinite number of ways) so it shouldn't fail no?. Sure, Fixed on the next commit. Fixed in the next commit. Sure, will be fixed in the next commit. \nSo by this, do you mean:\nSentence 1 .....\nSentence 2 .....\nor \nSentence 1....\nSentence 2....\n?. @karlnapf  Fixed in next commit.\n@iglesias Wouldn't \"partitions\" work here as a verb?. @karlnapf  In the last sentence, I use a_1, a_2, ..., a_n so I think leaving this sentence would be important so as to avoid confusion at where a_2, ..., a_n came from?. Fixed in new commit.. Fixed in next commit.. Fixed in next commit.. Fixed in next commit.. Changed in next commit. Fixed in coming commit. Now uses Labels. Fixed in the coming commit.. Fixed in next commit.. Done in next commit.. @karlnapf  I'm not entirely sure what you mean by this here. In the SVM example, it uses svm.put in this case. Is there an example of the kwargs?. @karlnapf  Other examples in cross_validation use AccuracyMeasure evaluation_criterion(), do you mean another example?. Fixed in next commit.. Tried changing to Labels but has problems with StratifiedCrossValidation (I opened an issue with details) so I'm leaving it BinaryLabels for now.. Sorry, meant that the points are on a line (ONALINE). @iglesias  As Viktor said. The original cost function needs a reference to an SGVector of the parameters. The parameters have now been changed in the cost function to SGVector so we're going to have to change the interface of the minimizers before returning a SGVector,. Not sure to be honest. I looked at the other files and they all had _UNITTEST_H so I followed the convention. If there is no guideline I'll add more _ between words. I thought that if you use using std::function; in a header, then you'd need to access it from another file which has the include as FirstOrderSAGCostFunctionInterface::function no? In other words, shouldn't it be under the scope of the .h file?. Actually you're right, I just forgot my c++! Including it in the header file does include it in other files as well. I'll get rid of it.. Sorry, I ran clang-format on it and didn't check the styling. Will fix the styling of the comments. @iglesias  Not silly at all! I actually thought about this, but I wanted to separate the logic of calculating the cost function from the class so that base class can support any cost function. We can also make a pure virtual method and let the user implement it. Both work.. @iglesias Just a convention (I try to pass things by reference/pointer instead of value to avoid copying). But I agree I will get rid of SGMatrix's pointers since it already implements that under the hood.. Hmm, how about FirstOrderSAGArbitraryCostFunction ??. I am not sure what is the convention in Shogun. But I always used pointers in my previous C++ coding (Just to avoid the syntax of reference). If you'd like, I think we can change it to reference here. . Sure, but in this case, I'll just print: \"Number of training parameters must be greater than 0, you provided 0\".... since you can't have num_of_variables<0. Didn't know this, but Eigen has unaryExpr which allows you to apply any function to all elements in the matrix. So I've removed the vanilla loop and replaced it with a call to that function.. This one is a bid harder to do without a loop. Since we're calling a function inside and passing an argument, Eigen::unaryExpr wouldn't work. I changed it to range(n).. Just wanted to emphasise it. Changed it to lower case.. @karlnapf It means we should return the partial derivative of the ith sample with respect to the variables, and not the partial derivative of the total cost with respect with respect to the variables. \nSo for example, if f_i(w) = (w^t*X[i]-y[i])^2 is the error of the ith data point, then this function returns the partial derivative of the current data sample with respect to w, rather than returning the derivative of Sum_i(f_i(w)) with respect to w.\nThis function (and get_average_gradient()) is from the base class (FirstOrderSAGCostFunction) and had the warning in its docs. Do you want me to change the warning in both files to something else, and if so, what do you want to clarify?. @iglesias  There is a problem with StratifiedCrossValidationSplitting class. If you update to the new API, it leads to an error. Viktor told me he needs to discuss this with Heiko first since it is a design problem, so for now any label which is used with StratifiedCrossValidationSplitting has to use the old API (BinaryLabels). For some reason, I tried the getter but it told me that the object is private. I will try again and see if I can do it. Same problem as above, tried the getter, but had problems with it. Will give it another go. @iglesias @karlnapf  The classical chi square distance is defined as:\n\nThis is for only positive values. However, the implementation in shogun is for both positive and negative values, since the denominator is (|x_i|+|y_i|) so it extends the chi square distance to the negative values too. Is this more clear?. Should I add \"for any natural number n\" ?. The whitespace is because I applied clang-format on this file, so it removed all the whitespaces according to clang-format. So this is not because my editor. @vigsterkr  But the data was being destroyed since the variable was local. I tried the second one but when I returned gradients, it just contained garbage values?. @iglesias Do you mean to put his name in a more obvious place? If so then I've already put his name in the header (copyrights). No plagiarism here :) . Just refactoring. I was using the same definition of a lambda twice in the file, so instead I just wrapped the lambda definition in a function: (cost_for_ith_datapoint) and used it in both places that were defining the lambdas so there is less copy paste.. Agreed, its only used once here so no need to define a function that wraps the lambda. \n. This is one of the things I'm discussing with you today.. @vigsterkr  Do I need to set the gpu backend before using linalg::scale ?. This should be addressed in the next set of commits. . Agreed. I've changed the parent class to be FirstOrderStochasticCostFunction in the next set of commits, since all these implemented functions are directly from it. (FirstOrderSAGCostFunction also inherits from FirstOrderStochasticCostFunction so it makes sense here to inherit from it as the Stan version is an alternative of FirstOrderSAGCostFunction).. I was just thinking of it as a design problem. As in aggregation vs composition. The cost function doesn't own the parameters, so I felt aggregation works better here (implemented in C++ as a reference) What do you think?. @iglesias In the unit test of FirstOrderSAGCostFunction, we implement an additional class called CRegressionExample that wraps FirstOrderSAGCostFunction and contains the training data. I've simply removed the necessity for that class since it just acted as a wrapper around the cost function and data, and have included the data in the loss function, or atleast a reference to the data.. @iglesias Do you mean you'd like the data to be in a more clear place (Like the top) rather than in the middle?. Hmm, well StanVector is just an alias for Eigen so I am not sure I can change the constructor unless I define a wrapper class for Eigen. . I wanted to cast here, since the output is not exactly:\nMatrix, Dynamic, 1>\n. @iglesias They're handled in the minimizer's destructor.. @iglesias Just a naming issue. Its just by Linear Layer, I mean a layer that has an activation of h(x)=x. Then, the logistic layer applies the sigmoid \\sigma to its output to get \\sigma(h(x))=\\sigma(x) or a sigmoid layer. So its just a specialisation of the linear layer. Will be refactoring the stan tests once the neural network is working. ",
    "sunalbert": "Ok, thanks. I will modify the code.. Thanks @iglesias . The style has been modified.. Ok, Thanks @iglesias @karlnapf . ",
    "grig-guz": "Ooops, you're right. That one should inherit from NeuralLayer.. Haha, not sure, these comments are back from 2014, taken from LinearLayer class. Will fix that!. ",
    "gf712": "The original issue described in #4382 is due to a mismatch of w_size and prob.x.num_features, the former included bias, whereas the latter did not. This results in an assertion error when performing vectorised operations with prob.x, i.e. prob->x->add_to_dense_vec(...).\nAs a fix I set w_size to the size of weights, excluding the bias term. \nif (prob->use_bias)\n    w_size = prob->n - 1;\nelse w_size = prob->n;\nTherefore, the bias is in the w_size+1 position. It seemed to be the cleanest fix! I could also have written something like this:\nif (prob->use_bias)\n{\n    prob->x->add_to_dense_vec(beta[i], i, w.vector, prob->n-1);\n    w.vector[prob->n]+=beta[i];\n}\nelse prob->x->add_to_dense_vec(beta[i], i, w.vector, prob->n);. Thanks! I have to switch my IDE indent!. Yes, that is right! I seem to have gotten confused, thanks for pointing that out. The bias worked before, but could lead to undefined behaviour.. I have fixed it now!. Yes that makes sense!. I have added this in the latest commit. It should work with linear regression problems with one or more features!. Hi, sorry about that!. Yes, this one comes from the clang format when I run it locally with the project format file.. I did find it odd!. I used it with my own clang format using shogun/.clang-format. In any case I'll reverse it!. The issue is that this does not take into account the floating point error propagation. That is why I picked the epsilon values through testing.. I got something like this working on my machine:\n```\ntemplate \nT get_epsilon()\n{\n    return std::numeric_limits::epsilon() * 100;\n}\ntemplate <>\nfloatmax_t get_epsilon()\n{\n    return 1e-13;\n}\n. I can move all the code to the TYPED_TEST if that makes it clearer? Sorry, I just noticed that all my other replies to your previous comments disappeared... Hi, it seems like my previous answer disappeared. There was no specific motivation for this. I just used other code in shogun as a template. The way it was written here is the same as in `shogun/tests/unit/classifier/LDA_unittest.cc`. The error traceback works fine when the test fails.. Yes, this compiles, but it might not make much sense...\nhttps://github.com/shogun-toolbox/shogun/blob/f6efd89467a7016be8a50bf3f7c0fdf32b075369/src/shogun/mathematics/linalg/LinalgBackendBase.h#L255-L262. I think it looks a bit neater to call a function inside the test, rather than writing all the code in there :). OK, I'll do that!. Ah that's neat! I'll implement that.. OK!. I had a look at the post, and the Enum with bit operations is pretty cool. I just thought that the hexadecimal representation is more readable than doing the bit shifts, no? . Yup, I just noticed that I switched the order.... True, that would be more concise. Is it worth writing a static member that is the default, i.e. 0?. OK! But hyperparameter is a term that could be misinterpreted if it written as HYPER, no?. The name MODEL causes conflicts if we use an enum, so will have to use something more specific, or just have these values as static class members of AnyParameterProperties.. Do you mean with a one liner like this?\nreturn static_cast((m_mask_attribute & GRADIENT_PARAM) > 0);\n. OK! I think you're right, I have changed the class members to use bit shifts, it is a bit cleaner.. Yes, I think we could do that. It seems to be pretty much standard now to return const references for std::string. It can sometimes lead to odd behaviours, but I don't think we need to worry about those.. \nhttp://thesyntacticsugar.blogspot.com/2011/09/evil-side-of-returning-member-as-const.html. would it make sense to use an enum within the class scope?. I thought about that, but then I thought it was cleaner to have the constructor \"know\" at what position the bit shift is required? Not sure if that makes sense?  . Do you mean:\nstatic const int HYPER = 0;\nstatic const int GRADIENT = 1;\nstatic const int MODEL = 2;\n. OK, if we have:\n  static const int HYPER = 1;\n  static const int GRADIENT = 2;\n  static const int MODEL = 3;\nbool model_selection = true;\n  bool gradient = true;\nint mask = ((1 << HYPER) & model_selection) | ((1 << GRADIENT) & gradient);\nThen the above mask will always be false/0 no?. Do you mean more like this:\nint mask = (model_selection << HYPER) | (gradient << GRADIENT)\nwhich is the equivalent of: shift bit at position X (1,2 or 3 in this case) if the function parameter is true.. Ah now I remember why I did it this way.. Basically if you want to provide an interface to create your own mask like this\nAnyParameterProperties(\"My description\",\n                       AnyParameterProperties::MODEL |\n                       AnyParameterProperties::GRADIENT)\nyou need to have the values of the static member with the shift. Unless I am missing something?. OK! Btw why does it need to start at 1, instead of 0? . Ah ok! I wasn't sure how it works with the boost software license!. Yup, and should change it to default to `false` instead of all `true`.. OK! So you mean have three tests, one for each parameter? The only thing that feels like is missing is an enum for zero, since we can't compare the `enum class` to an `int`, i.e. 0.. well I guess 0 and `ParameterProperties()` are the same, so never mind!. True!. The issue is that if you do `params.has_property(ParameterProperties::DEFAULT)` you end up with `0 & 0` which is `0`/`false`.. Yes, I can add a method with something like:\nbool equals(ParameterProperties other) {\n    return static_cast(m_attribute_mask) == static_cast(other);\n}\n. yup, I found it a bit odd when I wrote it. It seems unnecessarily convoluted.... It isn't, but the constructor of the old API still sets the ParameterProperties::MODEL bit, so I just thought it would be appropriate to add this check?. So just check:\nEXPECT_TRUE(params.equals(\n    ParameterProperties::MODEL | ParameterProperties::GRADIENT));\n``. Haha yes, I thought I would discretely add myself to the list :D. OK, I will bring this back and add a SG_ADD3!. but wouldn't that imply that all attributes would be equal, i.e. matching descriptions?. Both the weight and bias right?. yes, it does, it is actually better since it then uses the underlying type of the enum to do thestatic_cast. . but should still have theequalsmethod inAnyParameterPropertiesthat then uses theoperator==, no?. Because you still need a method to access it no? Unless you want to have a getter form_attribute_maskinAnyParameterPropertiesand then compare the returned value in the code to something. Wouldn't theequalsmethod be cleaner?. OK! I just think in most cases the description will differ, yet we only want to know if they have the sameParameterProperties`.\nCould just have:\n/** Compare masks */\nbool compare_mask(ParameterProperties other_params) const\n{\n    return m_attribute_mask == other_params ;\n}\nand then:\n/** Compare AnyParameterProperties properties */\nbool equals(AnyParameterProperties other) const\n{\n    return (description == other.description) && has_mask(other.get_mask());\n}\n. I did think I saw that comment in the previous PR, but the next day the comment disappeared... I think the second one is a bit of overkill and I cant think of a situation where it would be useful.. I'll just stick to the first one!. OK! I just gave them the most standard names because I'm not very innovative with variable names.. . Yes, that could work. Btw do you know why it is considered better to pass a lambda as a rvalue reference rather than as a copy, i.e. T&& func rather than T func. yes, that comes from shogun/tests/unit/base/SGObjectAll_unittest.cc. Also should change the file name.. It's called type_case because the first implementation used a switch... No, could be a boolean, or could be a void function. This is just a way to capture errors, like in my example above we could do this:\nif (!for_each_type(any, all_types, found_it_lambda)) {\n    std::cout << \"FAILED\" << std::endl;\n}\nOr could just throw an error from within the function if the type isn't found? Or allow a custom lambda to be executed in that situation?. Do you mean something like:\nauto write_summary = [value, summaryValue](auto type) {summaryValue->set_simple_value(\n            any_cast<decltype(type)>(value.first.get_value()));\n    };\nThat should be safer!. Or would it be:\nauto write_summary = [value, this](auto type) {this->summaryValue->set_simple_value(\n            any_cast<decltype(type)>(value.first.get_value()));\n    };. I am trying out SG_ERROR, but it won't work here because it seems to only work with classes that inherit from CSObject.. Can I just generate an exception inside the function like this:\nchar buffer[256];\nsprintf(buffer, \"Unsupported type %s\", any.type_info().name());\nauto msg = std::string(buffer);\nthrow ShogunException(msg); \nAlso I realised that if a type is not in the typemap we can throw an exception immediately. It's only in the case that the type is completely unknown, i.e. not in TYPES, that we will have to go through the list and then throw an exception.. btw it seems like it should be like this in C++ 14 (including this is C++11):\nauto write_summary = [&summaryValue=summaryValue, &value](auto type) {\n    summaryValue->set_simple_value(\n        any_cast<decltype(type)>(value.first.get_value()));\n};\nhttps://stackoverflow.com/a/42029220. OK! And the EXPECT_FALSE after, are they not necessary then? They should be covered by the empty mask.. Yup, now just need to come up with a way to make sure people use this signature... \nWould it be worth having some checks (not sure how)? Or just add somewhere in the documentation?. Ah cool! Thanks, works as expected!. Seems to be better to use rvalue reference if the lambda call is performed out of scope. But I don't see how that would happen, as the lambda never moves ownership. I cant tell if there is a downside to always use T&& func.. Ah seem to be the same! But will have to address the containers... Ah actually no, this gives me a segmentation fault:\nSG_SERROR(\"Unsupported type %s\", any.type_info().name());\nwhich I didn't have when I did this:\nchar buffer[256];\nsprintf(buffer, \"Unsupported type %s\", any.type_info().name());\nauto msg = std::string(buffer);\nthrow ShogunException(msg);. ah nvm, I just need to add init_shogun() at the start of my example code!. But EXPECT_TRUE(params.compare_mask(ParameterProperties::EMPTY)) is the same as EXPECT_TRUE(params.compare_mask(ParameterProperties())). I can add a EMPTY though to the enum. . OK! Btw, do you have any rule when to put or leave out functions/typedefs/enums from the namespace? . Yes, it should be added there. And then in the enum class TYPE and use SG_PRIMITIVE_TYPE to map the type to the enum and then to the appropriate map!. You mean sg_all_types right? (as opposed to the name of the macro). Ah ok! But then in that case it might make more sense to instantiate the m_attribute_mask class member in the constructors with ParameterProperties::EMPTY rather than ParameterProperties() no? I just used ParameterProperties() because it was the easiest way  to represent zero without any casting.. yes, it is similar, but it is a templated function that requires the type. Here we don't have the type, but have the std::type_info::name instead (and we cant infer type from that), so I needed this custom function that I hid in this namespace.. I will give it a go! I can add both implementations in Any.h. OK! Should I add information about the signature? I am assuming it will remain as it is now?. Good point, I can move it to the other namespace I created? Btw, should the other namespace be nested in shogun (i.e. shogun::type_internal), or completely separate?. I put it in Any.cpp as I was getting a duplicate symbol linker error.. Oh I meant if I should write the signature for the lambda in the description, sorry!. OK! No, I doubt it, it's very specific.. All right! I am guessing those tests fit in Any_unittest.cc?. Yes, I tried it out. It can cause some issues though.\n Let's say we add SGVector<float32_t> to the sg_all_types map. Everything works fine. But then if we use it in the TBOutputFormat.cpp we get a compile error, because the compiler tries to convert SGVector<float32_t> to float, because of the signature of summaryValue->set_simple_value(float) (error: no viable conversion from 'shogun::SGVector<float>' to 'float').\nWe will need to keep a second TypeList with SGVector and probably another one for SGMatrix. And then either have different sg_for_each_*_type functions, or come up with something fancier.. Working on it!. Might have to think for it for a while... I cant think of a way how the compiler could do such a check at compile time. Might have to stick to the more naive implementation for now. Ah ok! I do think I found a solution. Can basically have different parameters in the sg_for_each_type to execute lambdas for specific datatypes, and then with some SFINAE magic can execute the right lambda or throw an error (or do nothing). Could a have a function signature like this:\ntemplate <typename Lambda1 = std::nullptr_t, typename Lambda2 = std::nullptr_t, typename Lambda3 = std::nullptr_t>\nvoid sg_for_each_type(const Any& any, const typemap& typesmap,\n            Lambda1 scalar_func = nullptr, Lambda2 vector_func = nullptr, Lambda3 matrix_func = nullptr). Including \"any\" in the function name might make it easier to find the function in docs! So something along those lines would be better.. Could just leave without a prefix? Otherwise just use SG_ instead?. I got this logic working. Basically can pass a lambda to be executed if it is a scalar, SGVector or SGMatrix. If the type that is found is any of those but there is no lambda defined for that type I can throw an error, warning or even nothing (but would be hard to debug).. This could be good if you expect Any to have either of these types and can handle everything with a single call!. And then if the type wasn't registered we get the ShogunException(\"Unsupported type %s\") at runtime. Might be worth changing it to a static_assert to get a compile time error? . I was thinking about that, but it uses std::vector instead of SGVector? Is there some implicit casting going on?. I have been thinking that a lot of these things should be static asserts to make debugging easier... If everything compiles any developer might become more complacent.... I know.. The only other time that it might be used is if someone wants to create a custom map. Do you mean the list is long or the names?. No, sorry, I was going to try something else and then gave up and forgot to delete this. Do we need std::vector too?. Hmmm ok! Can have three different functions, it could make my life easier when specialising the helper function templates. Yes that sounds good! Was just waiting for a final decision before changing the name!. It is the equivalent to three functions, but could avoid repeating 3 almost equal lines of code with this call.. The nullptr is annoying, it's a shame you cant use python style function calls!. We could check the signature of the function to see if it is void and has one parameter at compile time.. I don't think so. Because the compiler knows what type is required by the function inside the lambda, i.e. summaryValue->set_simple_value() should be a float, but at compile time we don't know what the dispatched object is, so we compile functions for each possibility, including vector and matrix. That's why knowing if it's scalar, vector or matrix helps out a lot with SFINAE and avoid a compile error.. The only way would be to check the signature of all function calls inside the lambda and use that to figure out if it's scalar, vector or matrix, but I don't know if that is possible!. Got something like this working:\nstatic_assert(std::is_same<\ncheck_return_type<typename TypeList::Head, ScalarLambdaT, VectorLambdaT, MatrixLambdaT>, \nok>::value, \n\"All lambda definitions must be void and have the signature 'void f(auto type)'\");\nNot the most useful feature, but means we can write a test where the function signature is incorrect without compilation errors for the test! And gives the developer a \"gentler\" compile error.. I think that could be possible, assuming that the compiler can implicitly convert SGVector and SGMatrix specialisations, right? Like in the summaryValue->set_simple_value(float value), I think all scalars can be interpreted as float right?\nBtw seems like std::is_scalar is quite a broad definition, will have to find a more strict definition like std::is_arithmetic (potentially combined with a std::decay). I think the conversion to std::vector is necessary, because tensor board doesn't know about SGVector, like in this case:\nhttps://github.com/shogun-toolbox/shogun/blob/2f0b9cfe59d18fec38904fbc74ba5aa55a531e33/src/shogun/io/TBOutputFormat.cpp#L125-L135 . Alright, got 1 static assertion working! I didn't work on anything else yet as this took a while to figure out. I also wanted to add a test for static_assert with gtest, and that wasn't easy but now its working! :D. That should be doable! It's a matter of API design, functionally that is almost the same as we have now.. I added some utility structs to figure out if the type is SGVector or SGMatrix, so can use that to execute the correct lambda . Should I replace the warning with an exception? The idea of the warning was that there could be situations where no lambda implementation exists of the expected type on purpose and an exception could abort a ML run without need.. Yup, can use that. >I think that could be possible, assuming that the compiler can implicitly convert SGVector and SGMatrix specialisations, right?\nWhat do you mean by that?\nBasically if we have a lambda that takes SGVector it doesn't throw a compiler error when it sees the type SGVector and instead thinks that it can be converted.. Yup, should work fine, just need to write a test for it!. I am assuming the compiler complains that param isn't declared here right? . Did you try to remove m_gradient_parameters yet? Just curious as to how many things break... So is this the same as writing:\ntemplate <typename T>\ntypename std::enable_if_t<is_any<T, float64_t>::value, void>>\nrandom()\n{\n...\n}. Here you could write static_cast<U*>(matrix), num_rows * num_cols) instead, no?. OK! It is more readable indeed! Why do you need the template <typename U = T> though? Doesn't template <typename T> work?. I think so, you will need @karlnapf to get back to you on that one though. Btw I think you will need to rename is_any, because there is an Any class in shogun, which might cause some confusion. Is is_any_of too verbose? \nBtw if you always use the value of is_any, instead of its type, you could write a is_any_v as a short cut to is_any<...>::value. I think that is more conforming with the direction in which c++ is going. (see for example std::is_void_v). Yes, I can add that to #4441 . Yup, should be pretty straightforward to replace this, might be worth waiting for #4432 to be done. Yes, something similar might be possible with other interpreted languages right?. Ah true! Is the name for the method OK btw?. OK, we can \"hide\" the other getters by adding a _ prefix at some point? . It might be worth having a file with all the utility type_traits, no? Something like sg_type_traits.h. Ah true! Thanks, I'll have a look!. Why not use std::transform?\nstd::transform(matrix, matrix+(num_rows * num_cols), matrix, [&r](auto a){return r.random_half_open();});\nNot sure if this causes a performance penalty though.. I think the point of using STL over loops is that we are telling the compiler that we are never going to do something weird in the loop and break out of it. You could benchmark this and check the performance... I just tried this out on my machine, and both methods seem to take the same amount of time, I am not sure there is a way to speed up either methods. . And btw, I wouldn't recommend this, but you could do this with std::for_each:\nstd::for_each(matrix, matrix+N, \n    [&r, &matrix](auto a){\n        static int i = 0; \n        matrix[i] = r.random_half_open(); \n        ++i; \n    });. Or if you're not a fan of static members in lambdas you could do this:\nauto lambda = [&r, &matrix, i=0](auto a) mutable {\n        matrix[i] = r.random_half_open(); \n        ++i; \n    };\nstd::for_each(matrix, matrix+N, lambda);\nIn any case, I think std::transform is the best STL algorithm for this case.\nBtw, I don't know if you know this, but you can capture class members with a lambda like this &matrix=matrix. Thanks!. Yes, done!. Ah wait, that's a good point putting the loop in the try except block. self._get returns a SystemError and the other getters return a RuntimeError. self._get returns a SystemError and the other getters return a RuntimeError. I will have a better look during the week, but it seems that there are multiple definitions of how exceptions are handled in swig_typemaps.i. I will see if I can simplify this!. I think for this purpose it is fine, but in general I don't think it is advisable to change the behaviour of software at runtime... This just seems like the simplest solution without messing with the C++ code. . Yes, it's fine here. In Python 3 there is dict.iteritems(), which returns an iterator. It would be more efficient to use it if the user has python3, but the performance would probably not be noticeable with such a small dictionary.. . Yup!\n```\nIn [1]: import shogun as sg                                                                                                                                                                                 \nIn [2]: kernel = sg.kernel(\"GaussianKernel\", log_width=1.0)                                                                                                                                                 \nIn [3]: kernel                                                                                                                                                                                            \nOut[3]: GaussianKernel(cache_size=10,combined_kernel_weight=1,lhs=null,lhs_equals_rhs=false,log_width=1,m_distance=EuclideanDistance(...),m_precomputed_distance=null,normalizer=IdentityKernelNormalizer(...),num_lhs=0,num_rhs=0,opt_type=0,optimization_initialized=false,properties=,rhs=null)\nIn [4]: sg.kernel?                                                                                                                                                                                       \nSignature: sg.kernel(name, **kwargs)\nDocstring:\nkernel(name) -> Kernel\nkernel(obj) -> Kernel\nkernel(kernel_matrix) -> Kernel\nFile:      ~/anaconda3/envs/Python36/lib/python3.6/site-packages/shogun.py\nType:      function. But it doesn't make a lot of sense as the signature has changed, and it should probably be kernel(name, **kwargs) -> Kernel. That's why added the option to pass a docstring manually. Do you mean the one in Kernel.h?\n/** @brief The Kernel base class.\n *\n * Non-mathematically spoken, a kernel is a function\n.... Do you mean put all the names of the factory methods in a list and then pass it to a loop? . Yup, that's doable. I can push some code that does that. Basically _swig_monkey_patch checks if the first argument is a module, and then adds the object with the appropriate Python C-API.. Yup, should be possible. Let me give it a go!. Do we need to keep the old definitions, e.g. have the wrapped distance and also _distance?. What I wrote changes the behavior like this:\n```\nIn [5]: def main():\n   ...:     return \"Hello World\"\n   ...: \nIn [6]: main()\nOut[6]: 'Hello World'\nIn [7]: import sys\nIn [8]: sg._rename_python_function(sys.modules[name], \"main\", \"helloWorld\")\nIn [9]: helloWorld()\nOut[9]: 'Hello World'\nIn [10]: main()\nNameError                                 Traceback (most recent call last)\n in ()\n----> 1 main()\nNameError: name 'main' is not defined\n```\nSo should use it with care :D. I got this error from the CI https://dev.azure.com/shogunml/shogun/_build/results?buildId=279. Yes can do that. It was just to give some flexibility, but if we agree to the same pattern can just use _.. The generated code looks fine:\n```\n![create_machine]\nd = distance(\"EuclideanDistance\", lhs=feats_train, rhs=feats_train)\nkmeans = KMeans(distance=d, k=2)\n![create_machine]\nTried it out manually and get this for kmeans:python\nIn [11]: d = sg.distance(\"EuclideanDistance\", lhs=feats_train, rhs=feats_train)\nIn [13]: kmeans = sg.KMeans(distance=d, k=2)\nIn [14]: kmeans\nOut[14]: KMeans(cluster_centers={function},data_locked=false,dimensions=0,distance=null,k=3,labels=null,max_iter=10000,max_train_time=0,radiuses=Vector(0): [],solver_type=0,store_model_features=true)\n``\nthe issue isdistance=null. In that case can add a checkname in obj.parameter_names()`, and if it is say that there is no getter in the Python API that can be used for it?. But only if we reach the end of the checks. I think your original function disappears... I should add a check to prevent that from happening actually... Yes, but how would it work? With the C++ demangling?. OK, I added the check to raise an error if the param exists but can't be accessed with any of the known getters. \nIt would be possible to get the type string I think, just need to have a method that does it in CSGObject (if there isn't one already?). I added a check in both functions that raises an error if it destroys a function in the given scope . Should be even cleaner now and easier to add new factories and getters!. Isn't it better practice to do list initialisation, i.e. how it was before? Since this is a copy constructor all the class members should haven been initialised anyway. >The init() is called in the constructor which resets all the values to default. The unit test we have now doesn't catch his problem.\nAh ok! Sorry I didn't see that. \n\nAnd I am not sure if this is entirely correct because single_string is a pointer. How we have it now creates a shallow copy which I am not sure if it's what we want.\n\nTrue, that doesn't seem like a good idea. I am guessing if you delete the original CStringFeatures the copy has undefined behaviour?. So now you need to add the method name as a string here and not worry about it.\nhttps://github.com/shogun-toolbox/shogun/blob/13961a1d353414f301370425e124c18a4ce4c3b2/src/interfaces/python/swig_typemaps.i#L1348-L1354\nAnd when you recompile it should be accessible via .get in the Python interface.. But you still need the template, which should be %template(get_bool_string_list). Yup, for this you need to define the types you want to test:\nhttps://github.com/shogun-toolbox/shogun/blob/13961a1d353414f301370425e124c18a4ce4c3b2/tests/unit/mathematics/linalg/operations/Eigen3_operations_unittest.cc#L47-L50\nAssociate it to a (empty) class:\nhttps://github.com/shogun-toolbox/shogun/blob/13961a1d353414f301370425e124c18a4ce4c3b2/tests/unit/mathematics/linalg/operations/Eigen3_operations_unittest.cc#L26-L29\nhttps://github.com/shogun-toolbox/shogun/blob/13961a1d353414f301370425e124c18a4ce4c3b2/tests/unit/mathematics/linalg/operations/Eigen3_operations_unittest.cc#L59\nAnd then just pass the class to the TYPED_TEST macro\nhttps://github.com/shogun-toolbox/shogun/blob/13961a1d353414f301370425e124c18a4ce4c3b2/tests/unit/mathematics/linalg/operations/Eigen3_operations_unittest.cc#L64\nIn the test you can access the type with TypeParam. So this checks if either r->fill_array(container, len) or r->fill_array_co(container, len) are valid? Very cool! . This is definitely more readable!. oh wow, it does seem strange... I also need to to add PyString_FromString for python 2. I tried to template std::map but SWIG didn't return a dictionary... Which I thought it would?. I just realised that get_parameters() seems to be used a few times in shogun's code base, so will need to change this to something else. @karlnapf do you have any suggestions? parameter_map()?. yup, I had that at first but then for some reason sounded weird, but I agree that is better!. You mean return a map here and then write a %typemap(argout) for each target language that can't translate map to a builtin data structure?. I don't think there are many, there is one in neuralnets (and then DeepAutoencoders) and rbm, from what I can tell. So I had a better look into SWIG's std::map wrapper, and it behaves very similarly to a dict (i.e. has an item getter like behaviour, key and values getter). It just looks weird when printed, which could be odd for a user:\npython\nprint(kernel.parameter_types())\n<Swig Object of type 'std::map< std::string,std::string,std::less< std::string >,std::allocator< std::pair< std::string const,std::string > > > *' at 0x1181de490>\nSo should probably customise the map typemap right (at least for python)?. Ah ok! I tried with CMap but I get an error because the copy constructor is implicitly deleted.. I have this\nCMap<std::string, std::string> parameter_types() const {\n            CMap<std::string, std::string> result;\n            for (auto const& each: self->get_parameters()) {\n                result.add(each.first, each.second.get().get_value().type());\n            }\n            return result;\n        } \nHow do I get SWIG to copy CMap, when it wraps result with SwigValueWrapper?. so should this be single_string=std::string(orig.single_string) to get a copy?. Ah thanks! I got it to work :). Do you mean in libshogun or in SWIG?. I don't think I can because AnyParameter isn't derived from CSGObject, so would have to implement it's own ref counter... But this isn't meant to be exposed directly to SWIG anyway, so it should be fine no?. The idea is to then use this to extend SGObject in shogun.i to get some information about the parameters, i.e. name and type. The CMap is still needed for the SWIG interface in CMap<std::string, std::string> parameter_types() const, right?. the std::string is the demangled type for the param (key), which is then used for now in an error message in the Python API. Ah sorry, that's from me copying code to handle std::reference_wrapper, that should be push_back. Oh it's already there but I put the #ifndef SWIG bellow it. I guess it should go above the doxygen string?. \"Not a shogun CSGObject base type\"? Just to be more specific? . Or does it not make sense here?. It might be worth mentioning in the docstring that *args are passed to the object at instantiation?\nIt's a good idea to handle constructors with multiple arguments!. the issue is that the current msvc version doesn't seem to like this. I'm assuming it's because of the parameter pack deduction.. But I don't really understand the error trail from the CI. @karlnapf would it make sense to put this in a header that can be accessed by all tests? Together with the ::testing::Types typedefs below?. shouldn't that go to a sg_type_traits.h file?. Nevermind, all good!. in what way? :). oh right! it was just to make it unappealing for people to use out of context, but I can rename it. I will try to come up with something better. Just need to distinguish the runtime maps from the compile time structs.. I also wrote some structs to pop types from those structs. For example, you want sg_all_primitive_types, but you don't want to test for complex128_t, then you could do something like popTypesGoogleTestWrapper<sg_all_primitive_type, complex128_t>::type. Would that be useful to add? Either that or inside the test we can write type checks but that could be repetitive.... need to keep the equivalent of this. cool thanks!. The only thing is that I need to implement algorithms in compile time for that (append, reverse and pop). So it's a bit of a monster code, but it gets the job done and I have tests for that.... Yes, we can declare a new one. The idea of just being able to remove some in the spot is that redefining these types might lead to someone forgetting one, mostly when there are more than 10 primitive types...\nWe are talking about 200 lines of code, probably less if I aim for conciseness.. In terms of mess it depends if you like reading recursive structs.. . ah actually less, I have a lot of things now that I don't need for this, so more like 100 loc. clang, gcc and msvc should all remove the trailing comma if there are no __VA_ARGS__. sure! what do you have in mind?. But now there is no float64_t specialisation? Or is there now a faster symmetric eigensolver provided by Eigen?. Why did you change the clang-format version?. Why do you allocate memory on the heap here? Shouldn't it be SGMatrix<float64_t> null_matrix(num_features, num_vectors + merges);?. I think you could use SG_DEBUG instead? And then just set the global io to debug?. oh right, I think the 3.8 version is the one used in the Dockerfile, so I don't think you should change it here. You can use any other version for your own setup and then run it on a specific file using the .clang-format file in shogun :). Correct me if I am wrong, but the object should be on the stack and the data class member points to a region of the heap where the data lives, no?. Yea this doesn't work for gcc8 on ubuntu either. I will have a look tomorrow and see if I can fix it :). ah ok, I tried it a couple of months ago and this was causing an issue with gcc8!. No worries! I am not sure about it, lets see what @karlnapf says! :). you mean a function that takes an object and a string?. Ah ok! But what would the signature be? Can give it a go, this method is tailored for what I need rather than what a user would need, I think... It might not be worth adding all those templates just for this? Can always change in the future if it turns out to be useful.. ahh yes that makes sense. I had to capture this by value, instead of lhs by reference.. Casting lhs to CDotFeatures* and accessing get_dim_feature_space() led to a dangling pointer somehow... Do you know what the issue could be? Basically doing this:\n[&lhs=lhs]() {\n  return make_any(\n     1.0 / static_cast<double>(((CDotFeatures*)lhs)->get_dim_feature_space()));\n});. I thought this is captured by value? I thought that capturing the member by reference would be more efficient? The [&attribute = attribute] isn't well documented, I agree, I just found it in someones else's code a while ago and started using it.. . are you referring to this?. what is weird is that in my case the dangling reference error happened only when I cast the referenced value in the lambda . Which might be a problem now that I think about it?. Should be covered here. That's how I found these bugs! . Is there a buildbot for MKL backend btw? . @vigsterkr how do you capture env variables here? $(use_mkl) doesn't seem to do anything.... true, I just didn't know if it was needed to capture the env variable, not sure why it isn't working :(. @vigsterkr I think this is bug right?. So need to have\ntarget_include_directories(shogun PUBLIC ${MKL_INCLUDE_DIR})\ntarget_include_directories(libshogun PUBLIC ${MKL_INCLUDE_DIR})\n?. well can do a bit of trial and error... at least its working now. Maybe can merge #4523 to see if it works with this CI config?. so seems like libshogun can be private, but not shogun. Guess that was expected.. . haha sure!. Yup, sorry, I just needed an easy way to calculate the standard deviation. > so devs need to be careful and put this everywhere where the member that the value depends on would be changed ...\nYes, that is annoying but there is no other way I can think of\n\nI think actually a parameter property that is \u201cread-only\u201d and then a generic check in put might be a good idea.\n\nHmm I am not sure I understand what you mean. The parameter property is \"read-only\" no? And I added a check in put that verifies that a parameter has been initialised. Can you write a short pseudo code?\n\nThis is a pattern we had used before but with constructors and the lack of setters.\n\nCan you link it and I will have a look as I am not sure I understand what you mean?. OK, for this PR I can write the functions in a separate file. I could write a function wrapper that takes the references, i.e. the feature matrix, and then returns a lambda that has this captured and that can be called when the algorithm is initialised?\nAnd then in another PR we can write the put overloads using those functions? I mean the principle of your suggestion and what I wrote is similar right? In both ways we need a way to register that a parameter has to be initialised in the future.. Well the description can be access with AnyParameterProperties::get_description()? And then can be queried from the interface via SGObjet.parameter_description(name). Just need to improve the string description here.. I must have missed it!. Ah yes good point, I didn't quite like the fact that there was this static variable around! So add_options(string param, map<string, int>) just sits in SGObject, and then just need a mapping from param to string_enum_map, which makes it much simpler.. ah ok, but at some point would probably start adding override no? Doesn't it come at a cost when you just add more and more virtual class members?. You should be able to pass the literal variable name in a macro yes. It's a shame there isn't a universal name demangling across compilers, would be quite useful in this case.... I am not sure that would work though. Because you would then have a recursive macro, which then needs to be manually coded, i.e. SG_ADD_OPTION_1, SG_ADD_OPTION_2, SG_ADD_OPTION_3 right?. Unfortunately I can't :( the macro is needed by SG_ADD_OPTION, which is used later on. OK! Was just trying to follow the pattern from the other error messages. yes unfortunately this is the simplest way.. . @karlnapf is this fine for the object representation? It works fine. LibLinearRegression(C=1,bias=0,data_locked=false,features=null,labels=null,liblinear_regression_type=L2R_L2LOSS_SVR,m_tube_epsilon=0.01,max_iter=10000,max_train_time=0,solver_type=0,store_model_features=false,use_bias=false,w=Vector<double>(0): []). it seems like it might be possible to demangle it https://github.com/google/glog/blob/5c576f78c49b28d89b23fbb1fc80f54c879ec02e/src/demangle.cc#L1042, but would take a while to strip out the code we would need. @karlnapf I added these macros to this header because I need them for SG_ADD_OPTIONS. the problem with asserting if a parameter is of type machine_int_t is that we can't tell the difference from type int32_t, so the check will not work all the time... OK!. Yes, because I need it in SWIG. Can always do something like\n```cpp\nifdef SWIG\npublic:\nelse\nprivate:\nendif. You mean it would be removed if we merge SG_ADD and SG_ADD_OPTIONS?. Ah but that's fine. We can just check if the parameter is in the map, and if not dispatch it with CSGObject::put(const Tag<T>& _tag, const T& value) inside this function.. OK! Btw is it possible to test if get is working? I guess I need to add get_option to the parser?. I didn't want to use a macro but swig didn't like the templated functions I wrote so I just went with this... . I could move this to the library actually and then it is possible to this check in C++ as well?. actually could be a bit of a pain to do it with templating.... yup, I didn't realise that it could clash with C++ keywords.... Hmmm ok, there are two ways to solve this I think. Either move the definition of both those function into a translation unit (keep declaration where it is though), or mark them as inline in type_case.h (but inline is not enforceable though, so could break in some compilers/compiler versions). I can do that in a separate PR? It would be just good to check if that is the issue.. Actually inline is fine here because the linker will always see it as inline (and it won't be an ODR violation), whether or not the compiler decides to inline it in the machine code.. You should do a round of clang-format at some point to fix the indents!. Wouldn't it be better to return the result rather than modify the reference?. @lisitsyn is it OK to have a watch_method for non const functions?. @karlnapf it is a bit awkward that it isn't void.. is it ok?. with lambdas in general you should use auto instead of a function pointer with std::function. Here it doesn't matter too much as it is a test but in the library code use auto for lambdas when you can.. @vigsterkr Could this be an issue?. OK! just finding all the kernelnormalizer that were changed.... Yes, just change it to SG_ADD_OBJECT. yup, that's fine!. You mean the lambdas? I think it's better to leave them where they are. It's a zero cost abstraction anyway and improves readability. this could be results = self->filter(ParameterProperties::HYPER) and drop the get_parameters_by_property?. why is it in a try catch? . all the supported types should go here\nhttps://github.com/shogun-toolbox/shogun/blob/801bdbaee76c95ff77a243a8717f0ecefbf66dc0/src/shogun/lib/sg_types.h#L41\nand all the any supported types are here\nhttps://github.com/shogun-toolbox/shogun/blob/801bdbaee76c95ff77a243a8717f0ecefbf66dc0/src/shogun/lib/type_case.h#L118\n. maybe just type instead of stype? I think it would make it a bit cleaner. Thanks for that, it was a bug that I had missed before!. ah ok, maybe that should be changed too then. Imo it's better to have a consistent member type in these structs. this is an inline specifier for the linker, the code itself doesn't have to be inlined, so don't have to force it. yup, sorry need to change that. I think we need something like that. Because ::apply will return labels with either -1/+1, which is probably not what the user wanted... But then warnings can be easily missed or be too verbose if you do something like a gridsearch. static SG_FORCED_INLINE even no?. using namespace std; :O. shouldn't this be inlined to avoid linker errors?. ah yes sorry :D. This is really cool!. maybe we can write some sort of base class to handle the bit logical operations in shogun to avoid having to rewrite these?. and that can then be used inside AnyParameterProperties and here? Just thinking that at some point someone will make a mistake.. But these look fine. also I think the operation should be like here https://github.com/shogun-toolbox/shogun/blob/6434fe2df14bb0dbc34c27b2da4c7147f67b1159/src/shogun/base/AnyParameter.h#L136\nYou should use the complement. yes, that should be easy to add, just do it here:\nhttps://github.com/shogun-toolbox/shogun/blob/6434fe2df14bb0dbc34c27b2da4c7147f67b1159/src/shogun/base/AnyParameter.h#L34-L42\nand add the property in SG_ADD, i.e. SG_ADD(..., ParameterProperties::NOT_SERIALISABLE). @vigsterkr shouldn't this be final rather than override?. I changed this to use the Eigen version of standard deviation because it should be more efficient?. need to change this to std::log(0.5) / 2.0, like the default value in GaussianKernel. @vigsterkr I wasn't sure how to do a shuffle with SG_* so I just used the stl. could maybe add a static function in GaussianKernel to do the std::log(width / 2.0) / 2.0 conversion?. saving some precious clock cycles.... What do you mean? I still need to get everything from one of the triangles. The triangular view from Eigen just returns a matrix with 0s in the lower/upper triangle, I still need the non zero values?. Ah nice! If you want you can adjust the parameters around a bit? Thanks anyway!. I just shuffled the values around a bit to make it more realistic, but I can add a multiple there. . I guess we could add a function in CDistance that does that? And then add a static function that  can do the conversion to square form?. I wanted to add it, but then I realised that the iterators go along each training example, rather than column, so it became a bit of a pain without copying data.... I guess we can add the option when the exact value isn't required? I am just thinking that the stl implementation must be quite efficient already no?. and the new class member function would do this https://stackoverflow.com/a/13079806 ?. OK, I will add a warning. I think in openml this will generate lots of warnings when they do the learning for each fold, but I can disable warnings there. I will add the warning with a mapping.. do all the distances in shogun satisfy the triangle inequality and are symmetric? i.e. d(A,B) == d(B,A). nvm, they do, the base class has this written :D. ",
    "saatvikshah1994": "Sorry, this would be CLion specific will fix.. Yep will do so - I'll add a common fixture with the utility functions in NeuralLayerUtil right inside the unit/neuralnets folder. I believed that its standard convention - system files <> vs. project files \"\". Mainly useful since it helps differentiate project includes vs. system includes.\nI can revert if Shogun follows a different convention regarding this.\nI had made that change earlier when I had added NeuralLayerUtil. Decided to leave that in since I felt its a style improvement.. Oh ok, thanks for clarifying. I should also reverse the \"\" includes I've done in the other classes then.\nIll just revert my changes on this file then. If needed even the #pragma once can be added later.. This one?:\n/*\n * This software is distributed under BSD 3-clause license (see LICENSE file).\n *\n * Authors: ...\n */. no will remove. Not sure what you mean? add_vector itself stores its result in the position denoted by A_ref and returns void.. I was making it consistent with SGVector but sure - yes I noticed it uses SIMD instructions.\nI think SGVector::random should be changed too accordingly - not sure whether should be done in this or a different PR.. There's two catches here here though:\n1. SGMatrix::random should support the following types: int32/64; uint32/64,float32/64/128. fill_array* doesnt support int32/64 and float32/128. We will have to default back to the for loop implementation above for these types.\n2. The uint32/64 returns a random 32 or 64-bit integer(not within any range). The float64 returns a float  between [0, 1). Additional processing steps will be needed to scale them to [min, max]. Finally, the uint version only vectorizes random number generation if certain strict conditions are met, else it falls back to a for loop.. Thanks, though I'd like to clarify (2) again. I get the SGMatrix::random() function which would just fill the array with a uniform random distribution(using fill_array*). For random_normal I would need to further apply something like the Box-Muller transform right possibly using shogun::linalg ops.. @karlnapf thanks this makes sense.\nI had one suggestion though - I feel that the entire random() and random_normal() impl should go in its own separate (issue + PR) combo. Reason being that (1) the implementation is less related to the immediate issue of minimizing the test redundancies and (2) the changes should ideally be propagated to SGVector as well - overall then a significant portion of this PR would mix up two disparate issues. For this PR, I could just use the original NeuralLayerFixture::create_rand_matrix which would then be replaced in the second PR. \nI'm up for whichever way you lean towards.. Not clear to me why this makes it less portable? I kept it to ensure consistency on each run + it was being done earlier :). You've approved the PR so should I change it still?. separating declaration and definition would need explicit specialization of each template method signature - which would end up being really long. Since the methods are relatively short it seemed ok to keep both in the header file.\nThe only disadvantage is that an additional include of Math.h has to be added here. unneeded - will revert. Yep - Actually 3 versions are possible which mean the same:\n1. https://coliru.stacked-crooked.com/a/bc08035924d3582e\n2. https://coliru.stacked-crooked.com/a/7cc61a4adcdd5ded\n3. https://coliru.stacked-crooked.com/a/45dba741ddf2d4a0\nMine is a slightly more readable version of (2) :D. Yep I could. Will do so!\nYea I wanted to get the Thumbs up on this version. Adding it for SGVector shouldnt take too long on top of this. Dont think there's any other container right?\nAlso the type trait is_any <- I think this should go in some other file where it can be reused. Maybe common.h?. SFINAE only works on templated functions. A method inside a template class doesnt count. The function itself has to also be templated. As mentioned in CPPReference This rule applies during overload resolution of function templates. done. done. done. I think it should stay here - Ive noticed that macros have been used at come places to replicate this functionality which imho is a bad practice. eg. here. Keeping such examples would help push towards using SFINAE/tag dispatch for overload resolution.. std::for_each wont work with a stateful lambda which would be needed here [&r]{...r.random...}?. done. might be a bit of a chore to implement - especially because I'd have to based on the type use the random_half_open() vs. fill_array_oc version.. Btw I wrote up a short blog post on this topic - more as a point of reference but here it is in case anyone is interested :) . done. its fine - i didnt really add any logic just removed boilerplate. Yep, I'll rename it.\nApologies about the delay! Recently got back from vacation :) - I'll address all the comments this weekend.. done. Sure, I could look up and try to submit a few more once this is done.. Umm, sure, I was trying following the CPP Core guideline on this.\nAlso I guess instead of new I would need to std::make_unique since we want to avoid SG_UNREF?. Yep, std::transform is indeed equally fast(or even faster at times especially at lower optimization levels). This is done with std::transform - the for_each version looked a bit hacky :/. I'm also curious - why arent SG* objects implemented like STL style containers which have an inbuilt allocator(eg. like std::vector), thus avoiding the use of new. I'm not exactly sure about the complexity involved with that approach. Just curious as I imagine it would make API users especially for oft used containers like SGMatrix quite happy I presume.. done. Yep it would, I just kept it for consistency. Also is_same_v isnt part of STL till C++17. So I would have to add that too.. yep, ive moved it over. I added a sg_is_same_v for now and have used that. Guess it might be useful at other places too and can be replaced by the STL equivalent when you'll decide to bump up the C++ version.. ive addressed it for SGMatrix\nSGVector/CRandom are being included in one another, which is causing a problem when using sg_rand in the header file directly. I'll have a go at it tomorrow.. done. Yep it really is :). Most of the standard concepts I found in documentation/talks were not so specialized(even in use case) as this one is. \nI'll go through the ranges-v3 repo once - thats a good idea.. I'll look this up and correct as needed.. Will do.. Yes! there is - I was hitting a slight problem when trying to set it up though because of the explicit instantiations across all types that have been defined. I plan on breaking down the concepts one level more to:\n1. Integral/Floating\n2. Rngable\n3. RngVectorizable\nThis should ensure that only correct matches occur.. Not sure why it picked up the older commit without the sg_rand addition when I merged :/ . Will add that in.. Completely agreed - Will do so.. IIRC templates lambdas arent supported till C++20. Here Im using concepts with C++14 itself. I'll put an auxiliary function outside sure.. This is turning out way harder than I imagined :/\nSo the thing is concepts dont interfere with name mangling - But this also means that during explicit specialization to the compiler it looks like there are two duplicate templated member functions. Here is a minimal reconstruction: https://coliru.stacked-crooked.com/a/9d472eef7579adc9. So ranges-v3 uses C++ metaprogramming hacks to get a syntax resembling concepts but not exactly so. A few similar concepts as the one I've used here are: Integral which matches the ones I've setup and Swappable which checks that swap(x, y) is valid somewhat similar to RngVectorizable.. This seems to be valid following the style (1) mentioned here. ",
    "avramidis": "There was in my case. I use gcc version 7.3.0.. I believe you put that to make meta_api-enums work. Removing machine_int_t will cause the example to fail. However, to make the example I added to work you have to remove machine_int_t. I was hoping to see the CI results together and discuss how to proceed.\nI prefer to not have to convert the enum types.\nThis is the output I get:\nctest -R generated_cpp-meta_api-enums -V\nUpdateCTestConfiguration  from :/home/elavram/Workfolder/shogun/build/DartConfiguration.tcl\nUpdateCTestConfiguration  from :/home/elavram/Workfolder/shogun/build/DartConfiguration.tcl\nTest project /home/elavram/Workfolder/shogun/build\nConstructing a list of tests\nDone constructing a list of tests\nUpdating test list for fixtures\nAdded 0 tests to meet fixture requirements\nChecking test dependency graph...\nChecking test dependency graph end\ntest 47\n    Start 47: generated_cpp-meta_api-enums\n47: Test command: /home/elavram/Workfolder/shogun/build/examples/meta/cpp/meta_api/cpp-meta_api-enums\n47: Test timeout computed to be: 9.99988e+06\n47: [ERROR] In file /home/elavram/Workfolder/shogun/src/shogun/base/SGObject.h line 366: Cannot put parameter LibLinear::liblinear_solver_type of type int, incompatible provided type shogun::LIBLINEAR_SOLVER_TYPE.\n47: terminate called after throwing an instance of 'shogun::ShogunException'\n47:   what():  [ERROR] In file /home/elavram/Workfolder/shogun/src/shogun/base/SGObject.h line 366: Cannot put parameter LibLinear::liblinear_solver_type of type int, incompatible provided type shogun::LIBLINEAR_SOLVER_TYPE.\n47: \n1/1 Test #47: generated_cpp-meta_api-enums .....***Exception: Child aborted  0.16 sec\n0% tests passed, 1 tests failed out of 1\nTotal Test time (real) =   0.18 sec\nThe following tests FAILED:\n     47 - generated_cpp-meta_api-enums (Child aborted)\nErrors while running CTest. I think a solution to consider is to convert the all \"enum\" to \"enum class\" so there will be no automatic convert to int and it will be safer.. No, I didn't remove m_gradient_parameters. Mostly the model selection functionality breaks. . Good idea - I can do that.. I am stuck. I am trying to make get_features work. However, it is overloaded and it is used in many undocumented python examples. \nhttps://github.com/shogun-toolbox/shogun/blob/1c9ac30237f8b9ba5b2f9fe619fa9fabd7ee361f/src/shogun/features/StringFeatures.cpp#L1022\nhttps://github.com/shogun-toolbox/shogun/blob/1c9ac30237f8b9ba5b2f9fe619fa9fabd7ee361f/src/shogun/features/StringFeatures.cpp#L1030\nhttps://github.com/shogun-toolbox/shogun/blob/1c9ac30237f8b9ba5b2f9fe619fa9fabd7ee361f/src/shogun/features/StringFeatures.cpp#L1062\nI don't want to put them in the base interface because they are specific to string features.\nYou said earlier  \n\nThe way around this is to communicate with the classes via put get (to set parameters) and then call helper methods.\n\nput and get for watched parameters is fine but what helper methods you mean?\n. Oh yes!! Good point :P . There where no problems so I removed the ones I added. I've seen those. These are tests for constructors without arguments, right?. The init() is called in the constructor which resets all the values to default. The unit test we have now doesn't catch his problem.. And I am not sure if this is entirely correct because single_string is a pointer. How we have it now creates a shallow copy which I am not sure if it's what we want.. Yes, that's right. I will make another PR for that after the merge of this one.. I will have to try again the \"features\" because I was getting a double registration error. But I will let you know.. I added this in case we needed it for unit tests. But honestly, I am not sure if we really need it.. there was a warning because there is string named variable in the class and there was a warning. It is not really an issue and I can change it back.. You mean the \"this\" and \"other\" address is the same?. Your are correct - it is irrelevant to this PR. I should had written an issue for that.. Will do another PR.. Sorry about that. I will have a look at gitkraken.. I can change it to string_list for now but this will affect a few files. . See this reply https://github.com/shogun-toolbox/shogun/pull/4423#discussion_r249283822\nSo if we need shallow copy then this is fine. However, my understanding is that in the future we will use std::string instead of char* for strings? Is that correct @karlnapf ?. Thanks! I need to investigate more what single_string does.. I pushed one more change. I think we should not change the data type for this PR since it is just for fixing this particular issue.. I think it is fine since we know the specific kind of the object (i.e. EAlphabet).. It is a good idea, however, let's do that in another PR?. I removed this. The example works fine when I use SGStringList type to store the result of get_string_list (before it was get_features). It doesn't compile differently. The other factory methods use the same types I used.. Was getting and error that was saying it was double defined. Could not compile differently. I am not sure why. I have no idea what is the current status of the performance of the implementation. I was hoping for a quick fix my moving the problematic parts to the header but unfortunately it doesn't build for windows...\nThough, it would be good to investigate the status of the eigen bug. I will try to see what is going on on Windows to try to fix it.. It doesn't work on ubuntu gcc8? I tested it yesterday and it was working on my machine anyway.... OK, this PR fixes that but breaks the windows build :/. I am not sure. I will investigate. . Changed it back and now it works with \"width\". Honestly, I don't know why I had an error before.... ok, will do. Correct. I copied these from the other languages interfaces. I removed them.. @gf712 I was trying here to add an sg_type_map for Shogun objects. I am not sure if I am going to need it in the end. . Here I need the name of the object the parameter belongs to. AnyParameter doesn't have this information so I will have to think of a way to make it work.. @gf712 here too is my try to add objects. This works which allowed me to continue the change but obviously I will have to change it so it will be object type (or similar) and not primitive.. @gf712 Here too the object typemaps stuff.. This is needed many times so maybe we could move it in Any or somewhere else?. Sorry, I was testing something and I forgot about it. . ",
    "Kolkir": "Yes it's strange, these all not my code. Will try to find a way how to solve .. ",
    "braceletboy": "I tried to install 3.8 version on ubuntu 18.04 but couldn't find any way to do it. I did a while back and so I am not really able to recall the details now. So, I finally decided to change it to 3.9 version which is available in 18.04. I felt that if the num_features and num_vectors are too large, then it might be preferable to allocate the memory on heap. That's why I did that.. Okay. Will make the change.. Okay. I will undo the change. Thanks :+1: . Ohh. I made an oopsie. But, is it necessary to change it?. Made the change. Made the change. Looking into it :). Made the change anyways. Thanks for pointing it out :). Made the change :). Sorry about that, that was done automatically by vim when I saved the file. Will change it.. Will remove the parenthesis.\nI checked the KmeansBase file and used float64 bit. I felt that it made sense to make it float64 bit. What other type do you suggest?. Okay, makes sense.. Will do. Thanks!. Okay. Thank you!. Will do. Thanks :). This was supposed to not be there. Will remove it.. Okay. I get your point. Thanks!. ",
    "YuliZheng": "Haven't try yet because it is not recommended =_=. ",
    "shiyi001": "Yes. I have removed all the HAVE_CXX11 macros including the gpl submodule.. ",
    "theartful": "Current name is cached_bias and cached_w, which is still bad I think. I will think of something better.. It should be easy using SCOPED_TRACE. \nHard coding the linear machines may not be so scaleable. Can't the class_list script be extended to detect hierarchies? \nSorry I don't get your point about filtering, and why templated tests can be better.\n. @gf712 DataType.h seems to be obsolete, but I can't use sg_types and type_case yet since the class_list script (which has the create function) uses the old code.\nBtw, the implementation of these two files is very interesting!. Multikernel classes are the only classes that make use of the machine type. They use it for error checking.. I mimicked the ptype in sg_primitive_type. Dynamic casting would solve the problem, but I think it might not be the best solution since it breaks abstraction. Adding specific enum for SVMs would work, but it might defeat the point of removing machine type.. The problem here is whether MKL can make interleaving optimization using a certain svm class. I'm not sure, but this seems like information concerning the mkl class, not the svm itself, judging from the error message.. dynamic casting looks ugly to me. previous line is much cleaner\nC++ \nASSERT(svm->get_classifier_type() == CT_LIBSVMONECLASS). This sounds much cleaner! So this should allow people to create their custom SVMs for example using some factory interface?. I use the name instead of the enum now\nC++\nASSERT(std::string(svm->get_name()) == \"LibSVMOneClass\"). I think it checks that this is a one-class SVM. I can add a method that indicates one-class, but then SVMLightOneClass should also satisfy this, and it's not supported judging from this line ASSERT(svm->get_classifier_type() == CT_LIBSVMONECLASS). @vigsterkr this is to satisfy the one definition rule for the linker. should it be SG_FORCED_INLINEed?. ouch! sorry for that. it's used in MKLRegression. The is_classification should be used in MKLClassification I think?. I was thinking it maybe should be used in MKLMulticlass?. I thought I would ask first. Currently MKLClassification doesn't check that the SVM is not a regression one.. As I understand it, the one class SVMs, are used with the MKLOneClass, and similarily MKLMulticlass is used with the rest, so MKLMulticlass should check for this condition. If that's not the case, then it's useless. This can be replaced by EProblemType as well, since it has unary, binary, and multiclass.. not sure about this. does PT_CLASS means its one class classification?. this should go. I don't think supports is needed now, only \"supports_interleaving\" is left, and the rest comes from EProblemType. The CMachine is general purpose, but supports_interleaving only concerns MKL, is this ok? Also problem type and label type currently have their own enums (ELabelType and EMachineType), and they correspond. Sorry just want to make sure. PT_CLASS doesn't have its own apply version, and it makes tests fail if used.. in case a child class doesn't support a property of its parent class. sklearn uses 0.1, but the test won't pass using this value (error > 1e-5). memory leak?. @vigsterkr https://github.com/shogun-toolbox/shogun/blob/79fa98a85aed7eb88be30884349dd0666567ac5a/tests/unit/utils/SGObjectIterator.h#L9\nshouldn't it be namespace shogun?. "
}
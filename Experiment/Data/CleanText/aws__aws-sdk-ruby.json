{
    "mattyn": "Thanks for catching this!\n. Thanks for the patch, we'll check it out.\n. Thanks for the bug report!  We'd like to direct all bug reports through the AWS Developer Forums:\nhttps://forums.aws.amazon.com/forum.jspa?forumID=125&start=0\nIt would be great if you could post your question there so other AWS developers can see the answers.\n. Thanks for the patch!\n. Thanks again for reporting this issue!  This has been resolved with the latest release of the SDK.\n. Thanks for the patch!  I think this request covers the same issue.\n. Thanks for the patch!  We'll look into it.\n. It seems like a potential workaround would be to set the option on the HTTPartyHandler.  For example:\nAWS.config(:http_handler => AWS::Http::HTTPartyHandler.new(:ssl_ca_path => \"/etc/ssl/certs\"))\nDoes that help with your use case?\n. Thanks for the patch, we'll look into it!\n. I'm not able to reproduce the issue which this patch is meant to fix.  Can you post on the forums (http://forums.aws.amazon.com) with more details about where you are seeing this issue?  It would be great to know the ruby version and platform.  Also, when S3 can't verify the signature it returns an XML response including the expected string to sign.  If you can reproduce the problem under a non-critical bucket (e.g. one you create for this purpose) it would be great to see that XML response so we can pinpoint the problem further.\n. Thanks for the patch!\n. Thanks for the patch!  We'll take a look.\n. Hi,\nWe just released version 1.1.0 of the SDK, which should address this issue.  Let us know if you run into any more problems.\n. Thanks for the patch!  We'll take a look.\n. Thanks again for the patch.  We incorporated your first commit with version 1.1.4 of the SDK.  We've also made some sweeping changes to the way the library loads that should help to address the overall load time.  I'm going to close this request, but please let us know if you have any issues with the latest version.\n. Thanks for the patch!\n. Thanks again for the bug report and patch.  We've just released version 1.1.4 of the SDK, which should address this issue.\n. ",
    "trevorrowe": "Closing stale issue.  This should have been resolved a while ago.\n. Version 1.2.4 of the SDK should address all of the remaining issues from this pull request.\n. I merged in a different pull request that addresses this issue.  Should go out with the next release.\n. There are two issues at play:\n1. Times stored should be converted to UTC first\n2. When querying by DateTime or Time the value should be converted to UTC for correct comparison\nThese are indeed both issues that need to be fixed, but the pull request fixes these by adding a dependency on ActiveSupport extensions.  I would be inclined to accept the pull request if this dependency was removed.\n. We have purposely avoided extra dependencies, especially those that extend or modify core Ruby classes (like the ActiveSupport extensions).   There is definitely a good argument to make in the other direction, but I do not feel that someone who wants to build a simple command line application or Sinatra web app should be forced to install a large number of dependencies to use the aws-sdk.\nI have considered breaking the AWS::Record code away from the aws-sdk gem and making it a separate project.  In this case I would have a more favorable opinion of an ActiveSupport/ActiveModel dependency.\n. I should also add, that extracting AWS::Record into a separate project would likely not happen until a major version change of the SDK (i.e. version 2).\n. I'm closing this pull request for now.  Lets revisit this when we get closer to work on v2.  I think breaking this into a separate gem could be a good solution.\n. This has been released with version 1.2.6 of the aws-sdk gem.\n. Thank you for your submission!\n. Thank you for your submission!\n. Thank you for your submission!\n. I added a few questions to commit 7115e00.  Please take a look at those.\n. Thank you for reporting this issue.  I just released version 1.3.4 of the aws-sdk gem which addresses this issue in a broader sense (and added a few tests).\n. The nokogiri gem dependency was previously expressed as '>= 1.4.4', but this became an issue when 1.5.1 was released.  Version 1.5.1 was not compatible with Ruby 1.8:\nhttps://github.com/tenderlove/nokogiri/issues/632\nhttps://github.com/tenderlove/nokogiri/issues/631\nBoth of these issues have been resolved in 1.5.2; I will revisit this with the next release.\n. I'll be releasing shortly and rolling the dependency back to >= 1.4.4.\n. I haven't had a chance to take this for a spin yet, but it looks like an excellent start.  For integration testing, we can probably setup (and then latter tare down) and SNS topic and an SQS queue.  I'll try to take a look at the subscription confirmation issue.\n. You found a method that is no longer in use.  It was removed to AWS::Core::Configuration.  This method should have been removed (which is why it does not generate runtime errors).  I'll go ahead and just drop this method in the next release.  Keep up the good work!\n. If any of the service classes (e.g. AWS::S3, AWS::EC2, etc) ever return nil when they receive the message #client then there is a bug that needs to be resolved.  How often does this happen?  Do you have a simple case that you can reproduce this issue with?\n. It definitely sounds like a thread safety issue that needs to be resolved.  Is the client missing only at startup or are you experiencing this after things are underway?\n. Sorry for the slow response.  This is the correct place.  It looks like a good patch.  Let me merge it locally and take it for a spin.\n. Sorry for this taking a while.  I've merged this in.  It will go out with the next gem release.  Thanks for the submission!\n. Sorry for this taking a while.  I've merged this in.  It will go out with the next gem release.  Thanks for the submission!\n. Closing this pull request as this issue has been resolved here: https://github.com/amazonwebservices/aws-sdk-for-ruby/pull/51.\n. Go ahead and create a new pull request and reference it here.  I'll get back to you once I know if a new CLA is required.\n. Sorry for the long absence on this.\nRight now my primary concern with this feature is the dependency on HTTParty.  This gem is still expressed as a dependency to support a deprecated HTTP handler.  This handler will soon be removed as will the dependency on the gem.  \nA secondary concern would be for adding tests.  I added a while back a mock EC2 metadata server (https://github.com/aws/aws-sdk-ruby/blob/master/spec/mock_ec2_metadata_server.rb).  It shouldn't be very difficult to add support for mocking user metadata.\n. There is no explicit rescue in the code because there is little that can be done in most error cases.  The AWS::S3::Client examines the response and either returns a successful response or raises a runtime error based on the error code returned by S3.  Generally, all 300+ responses are raised as errors.\nIt is also worth noting that all 500+ responses from the service are automatically retried 3 times (for a total of 4 attempts) with an exponential back-off (this helps deal with throttling errors).  The error is raised only if all 4 attempts fail.\n. Sorry for the slow response.  I've been working on the approval process to accept a large contribution.  I'm going to send kadwanev a CLA.  I should be able to merge this in soon.\n. I've released version 1.5.8 which includes EMR support.  The JobFlow class was refactored to behave more like the rest of the SDK.\n. Looks good, will go out with the next release.\n. Sorry for the slow response.  I've been working on the approval process to accept large contributions.  I'm going to send yyuu a CLA.  I should be able to merge this in soon.\n. I'll be releasing an updated gem with support for Amazon CloudWatch support shortly.  I changed the names of a few classes, added documentation, split the MetricAlarmCollection class into 2 parts and added chainable filter methods to the collection classes to be more consistant with other collection classes.\n. Version 1.5.7 of the aws-sdk gem (with Amazon CloudWatch support) should now be live.\n. I can't believe it took so long, but I finally got back to this pull request.  I cleaned up the pending test, removed the HTTParty dependency and added a pair of integration tests.  These create a topic, create a queue, subscribe the queue to the topic, publishes a message to the topic, receives it from the queue and then uses AWS::SNS::Message to verify the authenticity of the received message.  It also tampers with the message to verify it fails.\nGood work @petemounce and thanks for your patience!\n. Currently, most services represented in the aws-sdk gem have a higher-level abstraction built on top of their client class.  We have been considering splitting the gem into 2.  The aws-sdk gem would remain as is (with higher level abstractions) and would rely on the aws-sdk-clients (or aws-sdk-core) gem.\nThat said, this is a rather large contribution.  I have not spent the time yet to see how we can get this merged.  I am confident we can though.\n. Thanks for the contribution!\n. I would like to merge this pull request, but it would be preferable to not have the gemspec added.  Is there a particular reason to have a gemspec file instead of running \"rake gems:repackage\"?\n. Thank you for the submission.  I've added this for the next release (the pull request can not be automatically merged).\n. Looks good, I'll make sure this goes out with the next release.\n. I'm going to be reverting a push I made that contained an RDS client so I can work in yours.  The affected commits I plan to revert are:\nc6beb0537b7bbe84190e6aeed1e73e19aaff2ea4\ncdabb8967b6c20cb4e2458d406688b47228d3c70\n5db2951ab8a24004631ef4b3fc25e3979bdfc89a\n. I pulled your commits into a local branch, added some unit and integration tests, and fleshed out a few more parts of the DBInstance and DBSnapshot classes.  I'll go ahead and close this pull request.\n. Looks good, thanks for the contribution!\n. I pulled in the second commit and removed the duplicate definition of the client operation, that way the additional method and test were added.\n. Looks good, thank you for the contribution.\n. Looks good.  Thank you for the bug fix/submission!\n. Sorry for not commenting on this pull request earlier.  It is a known issue that the SDK does not hold onto etag values from the list objects response.  This is problematic when you need the etag for objects you are enumerating.  \nYour pull request would resolve this issue, but it would be inconsistent with the SDK's strategy for caching data.  I was hoping to resolve this issue by making the S3Object extend AWS::Core::Resource.  This would make S3 behave like the other service interface and allow you to selectively hold onto volatile attributes.\nI'll be back in the office soon and I'll try to take a look at this.\n. The :metadata of an object stored in S3 is defined as the user-supplied hash of metadata.  I don't think mixing additional HTTP response headers into the user metadata hash would be the correct solution here.  \nUsing rspec syntax this is what I mean:\nruby\ns3.buckets['aws-sdk'].objects['key'].write(data, :metadata => meta)\ns3.buckets['aws-sdk'].objects['key'].metadata.to_h.should eq(meta)\nI agree that the additional http response headers should be accessible.  Would it be sufficient if all of the other header values were merged into the response data from #head_object?\nruby\nresp = s3.client.head_object(...)\nresp[:content_encoding] #=> '...'\n. I went ahead and added the headers identified in this issue to the #get_object and #head_object client responses.  I'm avoiding making other sweeping changes at this time because we are currently looking into more major changes related to the AWS::S3::Client that would accompany a major version bump (e.g. v2.0).  These changes would include a few backwards incompatible changes.  \nPrimarily I want to get rid of the hand-coded s3 client and replace it with a modeled client using a service model.  Similar service models are being used by the new Node.js SDK, the unified aws-cli, the new PHP SDK and boto core.\n. I pushed a commit that should address this issue (f1f45a7cf4).  This should go out with the next release.\n. The call to #bytesize is happening when building the HTTP request (while trying to specify the content-length header of the HTTP request.\nhttps://github.com/aws/aws-sdk-ruby/blob/master/lib/aws/core/http/request.rb#L159-L167\nThis was modified with the 1.6.7 release.  Previously it was using the #size of the HTTP body, this failed to correctly set the content length for some non-ascii payloads. \nCan you generate a simple example that demonstrates the issue you are experiencing?  Also, what version of Ruby are you using?  I'd like to reproduce this issue so I can resolve it, and then add tests to ensure this does not break in the future.\n. Ping.\n@frederickdubois Can you confirm is this is still an issue or if you are still using some sort of workaround?\n. Closing due to inactivity and inability to reproduce the issue.\n. I believe this issue has been resolved with 58ff1b923a27.  Can you verify if this issue is still a problem?\n. Thank you for reporting this issue.  There appears to be a thread-safety issue with how the service interfaces (e.g. AWS::SimpleEmailService) construct their client.  I will try to resolve this issue soon, but you should be able to work around this by constructing the ses interface outside the loop:\n```\nrequire 'aws-sdk'\nses = AWS::SimpleEmailService.new(...)\npool = Array.new(10) do |i|\nThread.new do\n  ses.send_email(\n    :subject => 'subject',\n    :body_html => 'body_html',\n    :body_text => 'body_text',\n    :to => 'success@simulator.amazonses.com',\n    :from => 'some@email.com',\n  )\n  end\nend\npool.each(&:join)\n```\nPlease watch this issue for updates.\n. Given the following example\n``` ruby\nrequire 'aws-sdk'\nses = AWS::SimpleEmailService.new # loading credentials from environment\npool = Array.new(10) do |i|\nThread.new do\n  ses.send_email(\n    :subject => 'subject',\n    :body_html => 'body_html',\n    :body_text => 'body_text',\n    :to => 'success@simulator.amazonses.com',\n    :from => 'some@email.com', # replace this with a verified email address\n  )\n  end\nend\npool.each(&:join)\n```\nI was able to execute this many times without issue.  If I moved the ses assignment inside the Thread.new block, then it would fail every time due to thread safety issues.  Is this what your test example looks like?\n. @desheikh I just pushed a commit that resolves an issue with AWS::Core::Configuration.  I then took your original example and added one line.  Between the two, this resolved the issue for me.\nruby\nAWS.eager_autoload!\nI realize this is not an ideal fix.  It forcibly loads all AWS classes up-front, which can be slow, but it is a one-time cost.  That said, this is not an issue in Ruby 2.0 and you can drop the call to eager_autoload! if you are able to upgrade.\n. Closing this issue as it should work properly if all classes have been eagerly auto-loaded.  Please re-open if you still run into issues.\n. Thank you for point this out.  The fix was pretty straightforward and can been seen here: 4978bb35ad -- the live documentation should get updated with the next gem release.  You can test this locally by cloning the git repo and running:\nyard server -r\nYou can view the generated documentation then at localhost:8808/docs/frames\n. Thank you for the pull request.  \nI just added support for s3 website configuration to the client in commit 27f1c26cd5ee4b8cbffd86b85aff199781847a84 and to the bucket class in 17cc9a1f08d9405e866e030f0ad1c42396e97f82.  The primary differences are the structure of the arguments in the client (I chose to map directly to the API, this helps when the API changes so we don't have to make backwards incompatible changes).  I also took a slightly different approach with the WebsiteConfiguration class.\n. Looks good, thanks for your contribution!\n. Unfortunately the stack trace you gave doesn't help resolve the issue.  The error indicates that the queue url is missing.  This could be reproduced with the following code:\nAWS::SQS.new.queues[nil].send_message('abc')\nWhat bothers me is how this could have happened.  In the example you gave above, the call to #first returns either a AWS::SQS::Queue object or nil.  Calling something like s#send_message on nil would have raised a very different error.  Your stack trace shows that you called #send_message on a queue object and that it had a nil url attribute.  The error was raised while validating the request parameters.\nI was not able to reproduce this issue myself.  It would be helpful to see the actual HTTP response body from the call to ListQueues (i.e. from calling AWS::SQS::new.queues.with_prefix(domain).first).\nWould it be possible for you to enable wire logging on the request?  You can do this when constructing the http request.\nsqs = AWS::SQS.new(:http_wire_trace => true)\nqueue = sqs.queues.with_prefix(domain).first\nBy default this will send it to your configured logger.  If you are running a Rails application, this is likely the Rails.logger.  If you do not have a configured logger, then output will be sent to $stdout. You can specify the logger with the :logger option.\nrequire 'logger'\nsqs = AWS::SQS.new(:http_wire_trace => true, :logger => Logger.new($stderr))\nThank you for reporting the issue.  I'd like to get this resolved!\n. The stack trace really helps.  Thank you for sharing.  I'm working with the SQS team to try to better understand and isolate the issue.  I'll update when I have more information.\n. We may have isolated the issue.  Please give it another try.\n. The issue you experienced previous was caused server side.  My understanding is this issue has been resolved and the SDK should work when listing queues.  \nThe issue you are reporting above appears to be unrelated.  It is difficult to tell what happened from the logger message.  The SDK uses Net::HTTP to send/receive AWS requests.  The status code 0 (zero) happened when the http handler tried to cast the http response status code from a string to an integer.  It appears that Net::HTTP returned an empty response (possibly a networking error).  \nThe only way to know for certain is to enable wire logging (see my example above).  This would allow us to see what example Net::HTTP returned to us.\nI'm guessing this raised an error in your application.  This should be improved so the net http handler triggers a retry on your behalf.  If this is correct, please open a new issue and put you debug message there so we can track it as a separate issue.\n. What sort of failure are you experiencing?  I'm concerned about the proposed solution as leading slashes in an object key are significant and should not be removed. Two objects with the keys \"foo.txt\" and \"/foo.txt\" are different objects in S3.\n. I'm guessing there is some other issue at play here.  Leading slashes are significant in Amazon S3.  The objected stored in a bucket with the key \"/path\" is different from the object stored at the key \"path\".  Are you using any other tool to put these objects in the buckets which may be removing slashes on your behalf?\n. I'm closings issue for now.  I'll be happy to revisit this issue if we can get a test case that fails.\n. A replacement interface has been added to the version 2 AWS SDK for Ruby here: https://github.com/aws/aws-sdk-ruby/pull/740\n. The AWS::Core::Data classes were added quite a while ago.  The client classes (like AWS::EC2::Client) used to construct responses where the values where dynamically created objects with methods punched onto them.  This proved to be a performance bottle neck.  To address the performance issues, the client response data objects were changed to plain old nested hashes and arrays.\nAWS::Core::Data and AWS::Core::Data::List were added to provide a backwards compatible interface for accessing response data.\n``` ruby\nresp = ec2.client.describe_regions\ndeprecated interface, currently handled via AWS::Core::Data\nresp.region_info[0].region_name\npreferred access style\nresp[:region_info].first[:region_name]\n```\nAccessing response data via #[] returns plain old hashes and arrays.  Accessing response attributes via named methods return these same structures wrapped. Method style access is deprecated and will not exist in version 2.  Unfortunately some sections of the SDK still access response data via the old interfaces.  The AWS::EC2::Instance#create method is one such section.  Unfortunately this abstraction leaked out because of the call to #map in this method.\nI agree with Loren, the core data abstractions can be improved (to prevent this issue).  That said, I consider this also a bug.  The #create method previously did return an array and the documentation should probably not change.  Ideally all such uses of response objects through method missing (that returns a core data object) should be removed.\n. Commit 79cbc73d16 should resolve the immediate issue of AWS::EC2::InstanceCollection#create not returning an actual array.  Changes still need to be made to the core data classes.\n. I pushed a branch with a proposed change to the core data classes (core-data-fix branch).  You can view the changes here: https://github.com/aws/aws-sdk-ruby/commit/bfc4dc6bb1f7cbf0a39c52a57ebc227022e98e8c. \n. Could you provide some additional information (version of Ruby, stacktrace, etc.)  I attempted a quick reproduction and was unable to trigger the error.  Here is what I did:\nruby\n1.9.3p327 :001 > require 'aws-sdk'\n=> true\n1.9.3p327 :001 > class TestModel < AWS::Record::Model; end\n => nil\n1.9.3p327 :002 > TestModel.create_domain\n => #<AWS::SimpleDB::Domain:TestModel>\n1.9.3p327 :003 > TestModel.first\n => nil\n1.9.3p327 :004 > Enumerator.new{|y| y << TestModel.first}.next\n => nil\n. Thanks for the contribution!\n. I pushed a commit earlier today that added support for CopySnapshot to the EC2::Client class (10ace102d6fe9d).   I have not added any higher level methods to AWS::EC2::Snapshot at this time.\n. Thanks for the contribution!\n. There appears to be a yard documentation issue causing the client method documentation to generate on the parent class.  I'll try to push out a fix shortly.\n. I've been able to resolve the issue locally with commit b76f81738b.  I'm going to rebuild and deploy the hosted docs next.\n. The hosted docs have been updated.  Thank you for reporting this issue.\n. I'm working on a patch that addes the :expires option to the S3 client #copy_object method and allows additional options to be passed in via S3Object#copy_from and #copy_to.\n. Thank you for the bug fix!\n. Thanks for the contribution!\n. Sorry for the long delay, but this should resolve the issue.  Not all values were getting properly escaped in the XML and certain characters would cause S3 to return an error about invalid XML.\n. I pushed a commit that should resolve this issue for now.  We are looking into a few options that would support adding arbitrary options to the request.\n. Thanks for the contribution!\n. I merged 8766e78 into master.  This should resolve this issue.  This will go out with the next release.  Thank you for reporting the issue.\n. Thank you for the pull request.  I've added a few integration tests as well.\n. Looks good!\n. Thanks for the bug fix!\n. I merged this in with a few minor edits (I trimmed trailing whitespace from the entire file, and squashed the commits).  You can view the commit here: 58943f647723574e75adfe42c257fe3d5c5dffb8.  Thanks!\n. I have concerns about adding this feature into the core SDK.  It would be very difficult to support this feature generically for DynamoDB.  Unless the attribute was inside the table schema, it would require a full table scan to properly validate the uniqueness of an attribute.  This is an ugly situation without additional indexes.  I'm going to close this pull request for now.  Thank you for making your contribution public.  This makes it possible for people who want to use this to opt in.  \nThank you for your contributions, please keep them coming.\n. I went ahead and removed the private tag from the AWS::IAM::Resource class.  This will allow the exists? method to show through in the docs.  This fix will affect all of the IAM resource classes, not just User.  Thanks for bringing this to our attention! (see 75319b2f6f7adbaa57d0c776676a0a3216dea3bf)\n. Thanks for the doc fix!\n. I reopened the issue so we can track this as a missing feature of the aws-sdk.  The SDK should be following 307 temporary redirects (and currently does not).  Yes, we would accept a pull request.\n. It is difficult to tell without more information from the HTTP response.  Can you enable wire tracing for the request?\n. I'm actually leaning towards removing the HTTParty gem dependency all-together at this time.  The AWS::Core::Http::HTTPartyHandler has long since been replaced as the default handler.  This would be a minor backwards-incompatible change (would break for users who explicitly replace the default handler with the deprecated HTTParty handler).\n. Closing in favor of 7490ef1 (removed HTTParty dependency and handler).\n. I was unable to reproduce this issue.  Are you using the latest version of the aws-sdk gem?  If the issue persists, it would be helpful if you could provide a full stack trace as well.  Thanks!\n. I think your code comment is useful.  Let me see what we can do about getting it into the source documentation that feeds into the client operation documentation.  If I merge your pull request, the comment will get clobbered the next time the client is updated.\n. I just submitted a pull request (https://github.com/aws/aws-sdk-ruby/pull/224) that adds better support for working with multiple regions and eliminates the need to consider endpoints for each service.\nPlease check this out and take it for a spin.  I would like some feedback on the interface.\n. Closing this issue now, you can track updates in #224.\n. Thanks!\n. @jmclachlan Are you calling AWS.eager_autoload! before you start your concurrency?  The SDK makes heavy use of autoload, which is not thread-safe.  The AWS.eager_autoload! method forcibly loads all library files across the SDK (this can be slow when they are not cached).  This generally clears up threading issues though.\n. I can not verify this yet (I will though), but it looks like DynamoDB is not returning the data at 'Responses'.  My guess is the response is returned with no data, and ALL of the request items are returned as unprocessed keys.  If this is the case, the fix is a one-liner ((response.data['Responses'] || {}).each_pair).\n. I should also add, this could be happening if you are getting throttled.\n. @karlfreeman Sorry for the long absence on this issue.  In both cases, it would be possible to coerce the nil value to an empty array.  I'm afraid by doing this, we could mask some other issue.  \nIf you are able to reproduce this issue somewhat reliably it would be SUPER helpful if you could enable http wire logging.  This would dump the entire http response body (that is getting JSON parsed) and would answer the question if the key is missing from the source, or if there is some issue in the SDK causing this buggy behavior. \nYou can enable http wire logging like so:\n``` ruby\nsend the log output somewhere more useful than stdout, like a file\nrequire 'logger'\nAWS.config(:http_wire_trace => true, :logger => Logger.new($stdout))\n```\nYou can reduce the scope of what requests get wire-logged by creating a ddb instance that logs instead:\nruby\nddb_without_wire_trace = AWS::DynamoDB.new\nddb_with_wire_trace = AWS::DynamoDB.new(:http_wire_trace => true, :logger => Logger.new(...))\nThanks!\n. Thank you for pointing out the error in the documentation.  I've pushed a fix.\n. We released an update (version 1.8.1) last night that addresses this issue.  Could you update and verify this issue no longer persists?\nThanks!\n. The :s3_origin_config is not marked as required because it is only needed when the origin is an S3 bucket. If the origin is outside Amazon S3, then you would need to pass the :custom_origin_config option instead.  The documentation you linked to is built directly from the public API documentation here: \nhttp://docs.aws.amazon.com/AmazonCloudFront/latest/APIReference/DistributionConfigDatatype.html#DistributionConfigDatatype_Elements\nIt can be generally difficult to document these conditionally-required parameters as the conditions that makes a param required can be complex.  Do you have a suggestion that you feel would improve this for the next user?\n. I will pass along the suggesting to the documentation team for Cloud Front.  I'm going to go ahead and close this issue as thanks,  Thanks for the feedback!\n. Thanks for the PR.  I'll take a look at the cause to this and check whats up with the test.\n. I merged this locally and pushed your commit (542f075b5f6205d6d7b1a67edb504eb79d96321e).  I was able to isolate the issue with the specs not failing (https://github.com/aws/aws-sdk-ruby/commit/793aa0d4f38e866db0d0bbe952bce791f46615ad#spec/aws/s3/multipart_upload_spec.rb#diff-1).  Thanks for the fix!\n. Thanks for the fix.\n. Thanks for the doc update!\n. The documentation on conditional put/delete for SimpleDB appears to be lacking.  You can add the expected options to the following SimpleDB methods:\n- AWS::SimpleDB::Attribute #set, #add, #delete\n- AWS::SimpleDB::AttributeCollection #put, #replace, #58 \n``` ruby\nitem = sdb.domains['domain-name'].items['item-id']\nadd values to an attribute if the attribute 'attr' contains 'value'\nitem.attributes['attr-name'].put(values, :if => { 'attr' => 'value' })\ndelete a list of attributes unless the attribute 'attr' exists\nitem.attributes.delete(attributes, :unless => 'attr')\n```\nDoes this help?  I would like to leave this issue open regardless, so we can track adding the missing documentation on these options.\n. Thanks for the doc fix!\n. No, it is not necessary.  The issue is the SDK does not know what or how many attributes you require.  In the abscence of the list of attributes to select, it will request only the table schema attributes.  In your example, this is \"id\" and \"date\".  The table schema attributes are required so the yielded Item object will know how to operate on itsel f (for example, what if you only wanted to delete each yielded item, you would need to know its schema attributes).\nIf you pass in a list of attributes to select, the block will instead yield an ItemData object (instead of an Item object).  \ntable.items.query(:select => [ATTRIBUTES_TO_SELECT], :hash_value => \"1\", :range_value => \"22\") do |item_data|\n    puts item_data.attributes #=> a hash of selected attributes\n    item_data.item #=> the AWS::DynamoDB::Item object for this attribute bag\nend\n. I will push the gem today.  I was unable to previously because the Rubygems.org API was in previously in read-only mode.\n. Version 1.8.1.2 is live now.\n. Thanks @qminhdo for answering the question.  Please re-open the issue @karlfreeman  if there is anything else this doesn't resolve.\n. Please let me know if this does not work, but it still passes our integration tests against AWS.\n. It looks good.  Could you add a few unit tests to cover the intended behavior?  I'd be happy to merge with tests.  Thanks for the good looking contribution!\n. I went ahead and removed the API version from the other operations and client documentation as well.  Thank you for the fix!\n. There are no current plans to add a flow like abstraction to the aws-sdk gem.  It is possible that something like this might get developed as a separate project (e.g. aws-flow).  I would stress this is not something we are currently discussing though.\n. I just pushed a commit (c67468a2037983b117b91804652d4542e446ab7a) that adds the missing operations.\n. I was hoping to get this released yesterday, but was not able to.  I just cut a 1.8.1.3 release.  Enjoy!\n. Thank you for reporting the issue.\n. Thanks for the commit!\n. A test would be good, If you would please.\n. Thank you for reporting this issue.\nThe API configuration is actually correct.  The bug you have was caused by a deficiency in the option validator. \nMap types and hash types are fundamentally different.  Hash types are more like structures, with a fixed possible set of keys.  Map types accept variable (user-defined keys).   I pushed a commit that should resolve this issue.  I'll try to cut a release shortly with this bug fix.\n. Thanks!\n. I really like the idea, but I'm running into issues using this on OS X 10.7.5 (the -w option does not exist).  My concern with merging this into the SDK is the long term support burdon.  Testing this on other OS's also becomes an issue.  \nI do think this would make a great plugin.  Thoughts?\n. A commit was pushed to master earlier this week (https://github.com/aws/aws-sdk-ruby/commit/cda7e7d3f28714e3a2175b1e91501db01909d26b).  Can you verify if this resolves your issue?\n. I see the issue.  The commit I referenced above resolved this issue for AWS::EC2::Instance class, but there is a related issue with AWS::EC2::Image (I miss-read the original issue). I'll try to get a fix posted shortly.\n. I just published a release (version 1.8.3.1).\n. You are correct, there is no dirty-tracking in the collection and resource classes.  That said, they are not object-relation-mappers.  They are more like fancy request builders.  It can be very difficult to correct track and manage changes intelligently.  It gets worse when you consider there is no way of optimistically locking your AWS resources.\nWe are actively looking at flipping the default cache behavior for version 2.0 (this would be a backwards incompatible change).  This would greatly help to reduce the chatty-ness of the SDK is many scenarios, but it would not directly address this concern.  As we get closer to work on v2, we will be blogging about it and looking for public input on the changes.  Please watch for updates on blog: ruby.awsblog.com\n. I've got a (shortly) incoming commit that adds support for following redirects.  I added this initially without adding support for 100-continue (per the S3 PutObject documentation).  As you pointed out, this would add latency to each operation for potentially little benefit (you can generally avoid the redirect simply by using the regional endpoint of the bucket).  As far as I can tell, S3 is the only service that responds to this expects header.  \nIf the 100-continue behavior ends up being important, I think I would prefer to make this strictly opt-in (via some configuration value).  This would allow the user to supply the timeout (how long to wait for 100-continue before sending the payload) and potentially some threshold (only use the expects header + 100-continue timeout if the payload exceeds some threshold, like 5MB).\nThoughts?\n. I have a work-in-progress change that provides the following new configuration options to AWS.config:\n- :http_continue_timeout - number of seconds to wait for a \"100-continue\" response before sending the request body.  This value has no effect if the \"expect\" header is not set on the request.  The \"expect\" header is populated when the request body exceeds the :http_continue_threshold (in bytes).\n- :http_continue_threshold - min size (in bytes) of the request body before the request will populate the \"expect\" header with \"100-continue\"\nRight now I'm running into a blocking issue.  There appears to be a bug with Ruby's Net::HTTP.  I've put together a reproducible test case that demonstrates this bug and then also a small patch to correct the behavior.  Without this patch, net http raises a runtime error (NoMethodError, attempting to call a method against a nil object).\nWhat I will probably do is push my change to a branch and include in that branch a monkey-patch that corrects the buggy net http method.  This would allow you to test the fix, share any feedback, etc.  I will also then submit the issue and patch to the core Ruby mailing list.\nI'll update this issue when I push the branch.\n. Sorry for the commit spam from re-pushing the branch.  \nThis feature has been merged and you can enable it like so:\n``` ruby\npatch buggy behavior in Net::HTTP around \"Expect\" and 100-continue\nAWS.patch_net_http_100_continue!\nonly :http_continue_threshold must be set, timeout defaults to 1 second\nAWS.config({\n  :http_continue_threshold => 2 * 1024 * 1024, # set \"Expect\" header when request body is greater than this\n  :http_continue_timeout => 1, # time to wait for 100-continue before sending payload\n})\n```\nPlease note, this only works with Ruby 1.9.3 and 2.0.0+ (Net::HTTP does not support 100-continue in 1.9.2 and older).\n. I have strong reservations against making AWS::Record case-insensitive.  Both SimpleDB and DynamoDB are both case-sensitive.  For this to work, the ids would have to be always normalized to lower-case strings.  As support is added for custom id attributes with AWS::Record, this could cause a number of issues.\nHave you considered filtering the incoming data from 3rd parties and normalizing the case as it is received from the source?\n. The plan was to make id a user-manageable attribute (this has not happened yet).  The default randomly generated UUID is meant to simulate how a RDS can create an id, but without being monotonically increasing (this is generally a bad idea in no-sql systems).  \nThe uuids are hexadecimal digits (0-9a-f), so it would be trivial to enforce that they are always lowercase (there is not plan to change this).\n. Yes, it will always return a lower case string (it will not return mixed case or upper case).  Your concern does make sense. I'm going to go ahead and close this issue for now, as there doesn't appear to be anything that needs to happen in SDK code.\n. Thank you for reporting this issue.  I was able to reproduce this only when using Ruby 1.8.7 (works in ruby 1.9 and 2.0).  I'll try to get a fix out shortly.\n. I just discovered I had an old version of the aws-sdk gem  installed with Ruby 1.8.7.  After updating to the latest, I was unable to reproduce the issue.  After some more digging I found this commit (https://github.com/aws/aws-sdk-ruby/commit/b36758593f64db349952d8911841dbaac8ac635a) that resolves this issue.  Please update your gem and verify this issue is resolved.  \nThanks!\n. On a side, feel free to upgrade to the latest 1.8.3.1 release.  We are not making backwards incompatible changes within the same major version (~> 1.0).\n. Thanks for reporting the issue.  I'll take a look at it today.\nTo answer your other question, yes.  There is work in progress that would update the client classes and core configuration.  As part of this refactor we are moving to a single configured :region parameter.  I had not considered pulling the region from the IAM instance profile.  Is this reported by the EC2 metadata service?\n. You are correct.  This is a tricky issue, as there are quite a few of these attributes that we would need to collect and copy along.  In additional to existing metadata, the ACL, :server_side_encryption and :reduced_redundancy one would also need to consider:\n- :content_disposition\n- :content_type\n- :cache_control\nThere are likely other attributes we would need to identify if we are going to do anything beyond merging existing metadata.\n. That is correct.  CopyObject will preserve those attributes only if NONE of them are set.  If you set any of them, you must set all.  The object ACL is never preserved by S3 when using the CopyObject operation.\n. At this point, I am inclined to add support for preserving those attributes in the original case (when updating metadata).\ns3.buckets['foo'].objects['bar'].metadata['key'] = value # preserve additional object metadata AND acl\nI am not inclined to make changes to the #copy_from method.  This currently very closely maps to the S3 CopyObject operation, and should probably stay as such.  Thoughts?\n. Hmm... I started work on this and I'm running into another issue.  It does not look like it will be possible to copy the ACL without 2 additional calls.  If the object does have a custom ACL, it can not be set via #copy_object, as that operation only accepts canned ACLs and ACL headers, not XML ACL documents.  The worst case is now 4 calls instead of 2:\n1. HeadObject (gets existing metadata)\n2. GetObjectAcl (to see if an ACL is set so we can preserve this)\n3. CopyObject (with the merged metadata)\n4. PutObjectAcl (only if step 2 returned an ACL)\nThis leaky abstraction is getting worse.  I wonder if there is a better way to handle this.\n. Closing this issue.  The public docs have a note on both #copy_to and #copy_from that indicates the ACL is not copied and must be re-specified.\n. Doc link for reference: http://docs.aws.amazon.com/AWSRubySDK/latest/AWS/S3/S3Object.html#copy_from-instance_method\n. The features are failing because you are missing a configuration file (this needs helpful error message).  You can add them to the integration tests, but it will cost you money to run them.   I recomend adding this to the unit tests (see spec/aws/auto_scaling/scaling_policy_spec.rb).\nIt appears this attribute is currently not tested.  I would update the tests for #update to ensure the proper value passes through to #put_scaling_policy and then I would also add a test for the #min_adjustment_step attribute to ensure it is extracted from the response correctly.\n. Looks good, thanks for the fix!\n. I am unable to reproduce this issue using your test script.  What version of the aws-sdk gem are you using?\n. Your rescue block is inside the polling block, so it would not be able to catch errors raised by #poll.  You should be able to flip the blocks like so:\nruby\nqueue = AWS::SQS::Queue.new(my_queue)\nbegin\n  queue.poll do |msg|\n    log \"Receiving message from queue #{msg.body}\"\n    received_message = parse_message(msg.body)\n  end\nrescue => e\n  log \"Exception: #{e.message}\"\n  retry\nend\nYou might want to track number of retries and stop if the #poll method continues to raise errors.  Another thing to note, the #poll method should only raise an error if it keeps failing (it defaults to 3 retries before raising the error).  You can get some more verbose logging by enabling a logger.  Each request will log the number of retries it made.\n``` ruby\nrequire 'logger'\nlogger = Logger.new($stdout)\nconfigure a global logger\nAWS.config(:logger => logger)\nor configure a logger only for one service interface\nqueue = AWS::SQS::Queue.new(my_queue, :logger => logger)\n```\n. The SDK internally uses a short exponential back-off for requests that get throttled or that return a 500 error.  AWS generally recommends an exponential back-off.\nAre you getting these 500 errors intermittently from existing queues, or are you only getting them when talking to a new queue?  I'm surprised that you would see these.\n. Sorry, my question probably wasn't very clear.  I was suggesting is should be very rare for an existing queue (that was not recently created) to return a 500 error when operating against it (send messages or polling for messages).  The SDK will automatically retry up to 3 times.  If you are seeing the 500 error raised, that means 4 attempts have already been made (over the last couple of seconds).\nIn this scenario, I would give it a healthy sleep, maybe 10 seconds before retrying.  One other thing to consider, you should probably have some terminal condition to your retry logic (don't just blindly retry).  I max retry count would work only if you are resetting the count after a certain amount of time (or after a success).\n. I closed this issue, but please feel free to re-open if you feel there are any lingering issues that need to be addressed.\n. @luccasmaso I've recently pushed a branch for the v2 SDK that has a significantly updated SQS polling abstraction. I would be interested if you could take a look at this and provide any feedback you might have before this makes it into the public SDK.\nhttps://github.com/aws/aws-sdk-ruby/pull/740\n. I am unable to reproduce this issue.  Your example runs for me.  The #add_authorization! method comes from the AWS::Core::Signature::Version2 module that is mixxed into AWS::EC2::Request in lib/aws/ec2/request.rb.  Has something else in your process defined AWS::EC2::Request before it gets auto-loaded by the aws-sdk gem?\n. Closing for lack of information.  If you can give steps to reproduce, I'll take a look into this.\n. @loopj Are you calling AWS.eager_autoload! and still seeing failures while the environment loads?\n. This is currently missing from the AWS::S3::Client.  It has not been purposefully omitted, just fallen off the radar.\n. I don't imaging I will have time to work on this until early next week.  If you do work on this, please feel free to contribute back with a pull request!\n. Closing as this feature was released with 1.10.0.\n. Currently the aws-sdk only supports getting the VPC id from a response (either CreateDbInstance or DescribeDbInstances).\nIf you call #describe_db_instances you will get a list of instances back with their VPC IDs.\n``` ruby\nget the vpc id for a single instance\nrds = AWS::RDS.new\nresp = rds.client.describe_db_instances(:db_instance_identifier => 'id', :max_records => 1)\nresp[:db_instances].first[:db_subnet_group][:vpc_id]\n```\nIt would be possible to expose this in the AWS::RDS::DBInstance class as well. The following monkey-patch should provide a #vpc_id method to each db instance.\nruby\nclass AWS::RDS::DBInstance\n  attribute :vpc_id,\n    :from => [:db_subnet_group, :vpc_id],\n    :static => true\nend\nIf this works for you, please feel free to put together a pull request and we can merge this into the SDK.\n. Thank you for reporting the issue.  Please try to use the following form:\nruby\nobject.write(data) # data should be an io-like object and respond to #size and #read\nThis is the preferred way to upload an IO-like object via S3Object#write.  The block form is deprecated.  That said, we do support Ruby 2 and I'll take a look at why this is failing.\n. Closing this issue for now.  Currently it appears AWS::S3 works fine with Ruby 2 (with the exception of this deprecated #write behavior).  Please re-open if you encounter other issues.\n. I am re-opening this issue.  A related issue (#215) trigger the same bug.  I'm going to be pushing a fix that should resolve both.  That said, I still recommend not using this block form for #write.\n. Each service behaves differently.  Some will block until the resource is in a state that you can describe it (the state may be pending or creating for example) and other will return straightway and you may have to guard against returned errors about the resource not existing for a period of time.  \nMost of our resources have #exists? methods you can call.  These generally make a describe call and then rescue s any missing resource exceptions that might get raised.\nI recommend using code like the following to wait for a particular state:\n``` ruby\ndef wait_for options = {}, &block\n  require 'timeout'\n  Timeout::timeout(options[:max_seconds] || 60) do\n    until block.call\n      sleep(options[:delay] || 10)\n    end\n  end\nend\nwait_for { instance.exists? }\n```\nThis helper method ensures you don't block indefinitely.  We do have plans to add additional helpers to the SDK that make it easy to wait for resources to hit particular states.\n. Thanks for the doc fix!\n. I ran a test with the SDK.  Here is the payload of the request sent:\nruby\nAWSAccessKeyId=REPLACED&Action=CopyImage&ClientToken=client-token-4&Description=copy-test-desc&Name=copy-test&Signature=x05mDJk%2FQrSvk8icNh7U3vEwczD6iftsPeqRfci3tHs%3D&SignatureMethod=HmacSHA256&SignatureVersion=2&SourceImageId=ami-54cf5c3d&SourceRegion=us-east-1&Timestamp=2013-03-12T23%3A05%3A29Z&Version=2013-02-01\nAs far as I can tell, the description has been serialized properly.  I tried to copy one of the Amazon Linux AMIs and I'm getting the same behavior as you, it seems to get stuck in pending.  It may be caused by the fact that I don't own the image I am trying to copy in this test.\nI did a quick search on the EC2 forum and I'm seeing a lot of related issues: https://forums.aws.amazon.com/search.jspa?threadID=&q=copy+image+pending&objID=f30&userID=&dateRange=all&numResults=15&rankBy=10001\nI'm going to close this for now, but please feel free to re-open the issue if it looks like the SDK is doing something wrong.\n. It is possible to have S3 verify the integrity of files you upload (to ensure the data was not corrupted over the wire).  If the file is uploaded as a single request (via put object), then S3 will verify the uploaded file if it has the \"Content-MD5\" header set.  If the file is uploaded in chunks via multipart upload, then S3 can verify the individual chunks if each chunk sets an appropriate \"Content-MD5\" header when calling add part.\nCurrently, the aws-sdk accepts a :content_md5 option to S3Object#write.  If the file is smaller than the multipart threshold then the content-md5 is sent along with the request and S3 will verify the upload.  If the file exceeds the multipart threshold then the content MD5 is ignored (S3 does not accept a single md5 for multipart uploads).\nWhat is missing from this story is the ability to instruct the SDK to compute checksums of parts on your behalf.  This would be helpful for both single and multipart uploads. \nThat said, addressing computing checksums on upload only verifies that S3 received the file as intended.  You may still want to verify the file at a later time when you download it back out of S3.  Currently the only way to do this is to store the content md5 of the entire file in the custom metadata and use that to verify the contents of the downloaded file.  Sometimes you can use the etag, but not for multipart-uploaded files (it is also impossible to tell how a file was uploaded which makes etag unreliable).\nA possible solution for this would be to use a standard metadata entry where the SDK could compute and store the md5.  It could then always trust this value as being the checksum of the entire file.\n. Good news, the v2 Ruby SDK supports computing the Content-MD5 on all Amazon S3 put operations by default.  If you haven't had a chance to take a look at the new project, you can check it out over here:\nhttps://github.com/aws/aws-sdk-core-ruby\nI'm going to close this issue as we are not doing any new major feature work on the v1 of the Ruby SDK.\n. The v2 SDK computes the Content-MD5 header by default on upload for calls to #put_object and #upload_part.  This ensures data integrity on upload.  You are correct though, that there is no current mechanism for getting a content md5 of the bits as they are downloaded.  One option would be to store the content md5 of the entire object in the object metadata.  This becomes accessible to the object for GET and HEAD operations.\n. The SDK does not currently support pre-signing operations besides GetObject, PutObject and pre-signed post.  Have you considered generating limited use credentials (using AWS::STS) that can allow the other ruby client to simply use the standard upload?\ns3 = AWS::S3.new(session_credentials)\ns3.buckets['bucket-name'].objects['key'].write(really_large_file) # auto splits large files into multipart uploads\nYou can generate limited use credentials using AWS::STS#new_session and AWS::STS#new_federated_session.\nhttp://docs.aws.amazon.com/AWSRubySDK/latest/AWS/STS.html\nHere is a quick example:\n```\ncreate temporary (1 hour) credentials that can call any s3 method\npolicy = AWS::STS::Policy.new\npolicy.allow(:actions => [\"s3:\"], :resources => '')\nsession = sts.new_federated_session(\"s3-user\", :policy => policy, :duration => 3600)\nsession.credentials\n=> { :access_key_id => '...', :secret_access_key => '...', :session_token => '...' }\n```\nYou can tailor the policy to limit the upload to a specific bucket and even object.  See the S3 docs for more information.\n. The aws-sdk is rather large, and eager loading all of its classes initially can be quite slow.  The opt-in option provides a mechanism for users that need a thread safe load of the SDK.  The approach we use is essentially the same as that used by Rails, and is generally well accepted.  At this time I would be not inclined to change the default behavior here, but I do understand your concern.\n. I just created a pull request that updates the aws-sdk to use vanilla autoload statements (https://github.com/aws/aws-sdk-ruby/pull/209).  I can verify there are still threading issues even without the register_autoload hooks.  That said, I am inclined to make the change for simplicity.\n. I don't know if this is your issue or just a typo in your example, but the correct endpoint is \"dynamodb.ap-southeast-2.amazonaws.com\" (missing db from the end of dynamodb).  Other than that, your example works great for me.\n. You can call #count and then enumerate if the count is greater than zero, but that requires 2 requests.  You can convert any collection into a vanilla array by calling #to_a.\nruby\nsnapshots = ec2.snapshots.to_a\nsnapshots.size #=> size now works\nsnapshots.each {|s| ... }\n. Can you give an example of when S3 returns the Forbidden response?  I am able to reproduce this only when I am calling #exists? against an object in a bucket that I do not own, otherwise, it always returns no such key.\n. If AWS::S3::Bucket#exists? returns true, that does not mean you have access to head objects in the bucket.  Bucket#exists? works by calling GET bucket versioning against the bucket.  The SDK will get one of the following three responses:\n- 404 NoSuchBucket (bucket does not exist)\n- 403 AccessDenied (bucket exists, but user does not have privileges to get the versioning state, the user may not own the bucket)\n- 200 OK (bucket exists)\nCalling AWS::S3::S3Object#exists? works by calling HEAD object.  This can generate the following responses:\n- 404 NoSuchKey (object does not exist)\n- 200 OK (object exists)\n- 403 Forbidden (maybe ????)\nIn the case of 403 forbidden it is no longer possible to tell if the object exists or not, only that you do not have privileges to call HEAD.  I we catch the 403 Forbidden and return false.  In this case S3 doesn't want to fold its hand and indicate if there is something there or not, only that you do not have privileges to know.  \nHave you tried the same request using your account credentials instead of an IAM user?  Also, does your S3 bucket have a special policy or ACL?\n. That is interesting.  I would post a question on the IAM forums (https://forums.aws.amazon.com/forum.jspa?forumID=76) and also the S3 forums (https://forums.aws.amazon.com/forum.jspa?forumID=24).  I would expect HEAD object to behave the same with both of those policies.  \nBack to the original issue.  While we can modify #exists? to return false in the case of a 403 response, I am afraid it could mask other issues (like poorly crafted policies, invalid bucket name, etc).\n. Thanks!\n. It looks like what is happening is the service is returning a InvalidAccessKeyId error which is being caught as a ClientError (all 400 level responses are client errors, 500 level responses are service errors).  This could be fixed if we instead of rescuing Errors::ClientError, we should rescue Errors::AccessDenied.\nThis change would allow the InvalidAccessKeyId error to raise up and would prevent the incorrect true value from being returned. \n. Thanks for the contribution!\n. Thanks for the fix!\n. I updated the #create method to accept the same format as is required by the service (the old format still works).  There was a chunk of code used to translate a hash of options into the list of block device mappings, and this was not flexible enough to correctly deal with :no_device -- you can now pass this through as an array of block devices.\nI updated the documentation for this option as well.\n. My understanding is that you should pass the device name (as a string) that you want to suppress.  Try this:\n{\n   :no_device  => '/dev/svdb',\n   :virtual_name => 'ephemeral0',\n}\n. I am not sure why, but Net::HTTP with Ruby 2.0.0 does not log the put payload in the http wire trace.  Because of this, we can not see what the XML of the request looks like.  Are you perchance using Nokogiri 1.5.7?  This release of nokogiri introduced a bug where the xmlns attribute gets duplicated on the root xml node.  This causes issues with XML requests like this.  I can't verify this because the PUT body is not in the wire trace.\nYou can fix this issue by upgrading nokogiri (or downgrading) from version 1.5.7.\n. I prefer to keep dependency versions reasonably loose.  Bumping it to > 1.5.8 can make things difficult for customers who have other libraries with nokogiri dependencies (or that use them themselves).  It would be ideal if we could simply exclude the single point release, but that is not possible to my knowledge.  \nAny thoughts on this @lsegal?\n. Feel free to submit a pull request with that and I'll merge it in.\n. I've found the issue, and created a fix.  I'm running integration tests to ensure everything continues to work in Ruby < 2.0.\n. It looks like this was a documentation issue.  You the :server_certificate option should be documented under :listeners.  The :server_certifcate option is just a convience option that accepts both a SSL Certificate ID or an AWS::IAM::ServerCertificate object.  You have always been able to pass the :ssl_certifcate_id as an option to the listener.\n. No problem!\n. Looks like a very good first start.  \nThere are only a few changes I would make to the AWS::S3::Client operation you added.  We are looking forward to a v2 release of the SDK where the AWS::S3::Client will use an API configuration just like the other services.  As part of this translation, the S3::Client operations will only accept the params defined directly in the S3 API documentation.\nIt looks like the current implementation accepts the following options, only two would need to be removed:\n- :bucket_name\n- :key\n- :upload_id\n- :part_number\n- :copy_source\n- :copy_source_range\n- :version_id\n- :first_byte ( not found in S3 API )\n- :last_byte ( not found in S3 API )\nThough not necessary, there are a handful of other options we could add to this method for completeness:\n- :copy_source_if_match\n- :copy_source_if_modified_since\n- :copy_source_if_none_match\n- :copy_source_if_unmodified_since\nIn addition to that, the method should be renamed to #upload_part_copy to match the naming pattern.  Please feel free to alias this to a more natural name.  Using the same name as the API documentation aids in discoverability.\nI'm still going through the code.  I'll leave some comments and questions inline there.\n. Thats looking pretty good! \nThe SDK already deals with failed requests and retries (by default up to 3 times per request) before raising the error.  Have you experienced errors that propagated out during a multipart copy?\n. Ideally we should create an integration test.  As part of the test we could upload a ~10 MB file and then perform a copy using the multipart copy.  Once we have that test then I am happy to merge.\n. If you have questions on how to add a cucumber feature, let me know.\n. I merged your pull request locally so I could add an integration test (93300dc).  Your commits should now be on master.  Thanks for the excellent work!\n. Thanks!\n. Thanks for the contribution!\n. Thanks!\n. The first error you are receiving is from Amazon EC2.  It could be more helpful, but it indicates the AMI does not exist in the region you are connecting to.  The 'ami-ad0a30d9' image exists in the eu-west-1 region and it looks like you are connecting to the us-east-1 region.\nThe second issue was a bug.  I went ahead and added #image_id to the AWS::EC2::Image class.\n. I like your documentation suggestion, but I'm going to just have to pass it on to the Cloud Front team.  The client-level documentation is extracted from the service API documentation and gets rebuilt from source when we update the client.  Any merge I make locally will get clobbered the next time there is any API update.  \nFor version 2 of the SDK we are looking into moving the API source into version control.  At that time we could provide a mechanism for merging additional documentation onto the clients.\n. The first failing example seems to be an issue with how the hash model class is detecting changes to the attributes.  Calling #add does not seem to trigger a change in the dirty attribute tracking (because it bypasses the attribute setter).  This is similar to how ActiveRecord fails to detect changes on a string attribute when you call #replace!\nYou can manually indicate the attribute has changed by calling ATTRIBUTE_will_change!\nmodel.my_strings_will_change!\nmodel.my_strings.add(\"s2\")\nmodel.save!\nAlternatively (I realize the above is not ideal), you can use the attribute setter:\nmodel.my_strings += ['s2']\nA better long-term fix might be to identify some of the methods on Set that mutate the set and track those for changes.  \nYou second question, how do you deal with changes made potentially by another instance of the object (potentially in a separate process even)... The only solution we have for this currently is optimistic locking.  You can enable this on your class like so:\nclass MyModel < AWS::Record::HashModel\n  optimistic_locking\nend\nThis causes HashModel to track changes via a attribute named version_id.  When making changes to the record in DynamoDB, this value will be checked as an update constraint to ensure it has not been modified.  If it is not the value it was when the record was loaded/hydrated then it knows the remote record has since change and it will raise an error. \n```\nm1 = MyModel['object-id']\nm2 = MyModel['object-id'] # same object\nm1.my_strings += ['s3']\nm1.save!\nm2.my_strings += ['s3b']\nm2.save!\n=> oops, raises AWS::DynamoDB::Errors::ConditionalCheckFailedException: The conditional request failed\n```\nIs this what you are looking for?\n. Thanks for reporting the issue.  The signer for AWS::CloudFront:: Request was not paying attention to session tokens (temporary credentials) like those returned from the IAM instance profile.\n. There is a #private_ip_address method on AWS::EC2::NetworkInterface.  It is documented on line 31 and is created on line 71 of lib/aws/ec2/network_interface.rb\nec2.network_interfaces.first.private_ip_address\nAlternatively, you can access it as part of the response data from a AWS::EC2::Client#describe_instances response.\n. I see the issue now.  When AWS::EC2::NetworkInterface was added, they only had a single private ip address.  It appears that EC2 now returns two attributes for the network interface that look like this:\n``` ruby\nresp = ec2.client.describe_network_interfaces\npp resp[:network_interface_set][0]\n{:groups=>[{:group_id=>\"sg-ce22e5a1\", :group_name=>\"default\"}],\n :tag_set=>[],\n :private_ip_addresses_set=>\n  [{:private_ip_address=>\"10.0.244.100\", :primary=>true}],\n :network_interface_id=>\"eni-26861c4a\",\n :subnet_id=>\"subnet-72861c1e\",\n :vpc_id=>\"vpc-b8811bd4\",\n :availability_zone=>\"us-east-1d\",\n :description=>nil,\n :owner_id=>\"469596866844\",\n :requester_managed=>false,\n :status=>\"available\",\n :mac_address=>\"1a:11:f4:10:a9:f6\",\n :private_ip_address=>\"10.0.244.100\",\n :source_dest_check=>true}\n```\nThe new :private_ip_address_set entry returns an array of addresses and notes which is primary.  \nThe commit I just pushed should address the missing attribute in the NetworkInterface class.\n. Using the #[] method on a collection does not make a request.  It returns an object that references something in AWS.  This object lazy loads attributes.  This makes it ideal when you don't actually need any of the attributes loaded for the object, but you want to operate on it (e.g. delete it, copy it, etc).  Most operations only require the ID which is provided to the #[] method.\n```\ndoes not make a request, just returns an object that references a resource in EC2 (but that object may not exist)\nimage = ec2.images['img-12345678'] \nthis will make a request and return true/false\nimage.exists?\n```\nBy default, instances AWS::EC2 are using the us-east-1 region.  You can create an instance of EC2 that references a different region, but that wont solve the issue here.  You are creating an Auto Scaling launch configuration (in us-east-1) and therefore must choose an AMI found in us-east-1. The error you are receiving indicates that the image you are attempting to use does not exist in us-east-1 (the image in question is actually in eu-west-1).  \nI you will either have to create the Auto Scaling launch config in the EU, or you will need to choose a classic region AMI.\n. To configure auto scaling and ec2 to use a non default region, you can configure then like so:\n``` ruby\nthe default region is us-east-1\nec2 = AWS::EC2.new(:ec2_endpoint => 'ec2.eu-west-1.amazonaws.com')\nau = AWS::AutoScaling.new(:auto_scaling_endpoint => 'autoscaling.eu-west-1.amazonaws.com')\nor you can configure them globally\nAWS.config(\n  :ec2_endpoint => 'ec2.eu-west-1.amazonaws.com',\n  :auto_scaling_endpoint => 'autoscaling.eu-west-1.amazonaws.com')\nec2 = AWS::EC2.new # now defaults to eu-west-1\nau = AWS::AutoScaling.new # also defaults to eu-west-1\n```\nYou can find a complete list of supported endpoints here.  In the next release (later this week) this will be greatly simplified and you will be able to use the region name like so (without needing the service specific endpoint option or the full http endpoint):\n``` ruby\nec2 = AWS::EC2.new(:region => 'eu-west-1')\nau = AWS::AutoScaling.new(:region => 'eu-west-1')\nor globally set the region\nAWS.config(:region => 'eu-west-1')\nec2 = AWS::EC2.new # eu-west-1\nau = AWS::AutoScaling.new # also eu-west-1\n```\nIt would also be much more helpful if you could provide backtraces with the error name and the code same that generated the error.  It is difficult to troubleshoot blind.\n. Thanks for the heads up.  I've fixed the regression and pushed a fix.  I'll cut a maintenance release shortly.\n. The fix is live now (1.9.1).\n. The issue is caused by VCR (or rather fakeweb).  VCR uses the fakeweb gem to mock out Ruby's Net::HTTP library.  Ruby Net::HTTP added a #continue_timeout= method in Ruby 1.9.3 which the SDK attempts to use when present (it uses a #responds_to? check).  fakeweb doesn't deal with this yet.\nI'll try to get a patch submitted to fakeweb, but until then you should be able to work around this like so:\nruby\nclass FakeWeb::StubSocket\n  attr_accessor :read_timeout # this one may not be needed\n  attr_accessor :continue_timeout\nend\n. I was tracking down the offending code in webmock and was going to submit a fix, but it appears it has already been fixed on master (https://github.com/bblimke/webmock/commit/32e3f65aea63477c116c201aab060c340a8acd99).  That said, it is not in the released version of webmock yet.  You can work around it for now with a similar monkeypatch:\nruby\nclass StubSocket\n  attr_accessor :read_timeout, :continue_timeout\nend\n. I'm going to close this issue for now as there are sufficient work-arounds and the bug lies outside of the SDK.  Thanks everyone for their contributions.\n. The two API versions are sufficiently different that the current AWS::DynamoDB abstraction can not be directly maintained without a few backwards incompatible changes.  I am looking at the higher-level interface we have today and trying to decide in what ways it can be improved.  It will likely get name spaced under AWS::DynamoDB2 (or something similar). \nYou can use the new API from the client right now.  In addition to the API changes, this client returns data with symbolized and snake_cased keys (e.g. 'TableName' would now be :table_name).  This makes it consistent with all of the other client classes in the SDK. \n``` ruby\nddb = AWS::DynamoDB::ClientV2.new\nddb.list_tables\n=> { :table_names => ['...', '...'] }\n```\nIf you have comments, suggestions or feedback, I would be interested to hear.\n. DynamoDB changed the shape of their requests and responses.  In most places, the changes were small.  In other places it requires you to change how you are calling the operation (e.g. Instead of passing a :hash_key_value you need to pass the name of the attribute mapped to the value).  These are backwards incompatible changes in the actual API.\nAdd that to the change in the key format in the client and its a good reason to re-evaluate the interface.  I think most of the existing interfaces will work, but there are a few rough spots I would like to change (mostly related to how ItemCollection#query and #scan work).\n. I'm going to close this issue for now as AWS::DynamoDB::ClientV2 fully supports the recent API changes.  We are still considering how to provide a helpful higher level abstraction for the new API.\n. How frequently are you getting these errors? Do you have any other information that might help (e.g. estimated file sizes, the region of the S3 bucket you were connecting to, etc)?\n. I noticed you closed the issue.  Was this transient, or is it a persistent issue you are running against?\n. Is there a side-effect of loading the railtie you want to avoid or modify?  Currently it does the following three things:\n- Attempts to load a configuration file from RAILS_ROOT/config/aws.yml (if one exists)\n- Adds :amazon_ses as an ActionMailer delivery option ( so you can do this: config.action_mailer.delivery_method = :amazon_ses)\n- Configures AWS to use Rails.logger (AWS.config(:logger => Rails.logger))\nIn a recent commit to master, lib/aws-sdk.rb was reduced down to two simple require statements.\nruby\nrequire 'aws/core'\nrequire 'aws/rails'\nlib/aws/core.rb registers the autoloads for the SDK and 'lib/aws/rails.rb configures the railtie.  Using this version you should be able to avoid the railtie completely by requiring aws/core directly:\n``` ruby\nin your Gemfile\ngem 'aws-sdk', '~> 1.0', :require => 'aws/core'\n```\nAgain, this only works on the latest code in the master branch.\n. Thanks for reporting the issue @desheikh!\n. @thinkgareth I have not found eager_autoload! to be necessary when using Ruby 2.0 (in my limited testing, things have just worked).\nCould you open a separate issue for tracking the request timeout issue you are experiencing?  It would be helpful to see some example code and one of the errors you are getting back.\n. @JoshMcKin I just published version 1.9.3 of the gem.  Sorry for the delay, I was preparing and traveling for RailsConf.\n. I ran a quick test locally where I launched an EC2 instance and then registered it with a load balancer.  I was not able to reproduce this issue.  Do you have a copy of the error returned?  It might help if you re-run the failure with HTTP wire logging enabled:\nruby\nelb = AWS::ELB.new(:http_wire_trace => true)\nThis should get us a detail log of exactly what was sent to ELB.\n. I'm going to close this issue for now.  Please re-open if this persists.  Please provide a stack / wire trace and steps to repo.  Thanks.\n. Thank you for reporting the issue.  The documented :comment option should work as intended now, as well as passing along the :hosted_zone_config. \n. Is this the entire script or are you doing anything else (threading perhaps)?\n. @ampedandwired The error message you are getting, Your socket connection to the server was not read from or written to within the timeout period. Idle connections will be closed. (AWS::S3::Errors::RequestTimeout) indicates that Amazon S3 is closing the http connection because it waited to long for data. This is caused when a request indicates the content-length will by X bytes, but then sends fewer.  S3 keeps waiting for the remainder bytes that aren't coming and then eventually kills the connection.\nYou mentioned you were able to reproduce this issue using strings (and not just files).  It also does not appear to be threading related (your example does not create multiple threads).\nI'm not able to reproduce this issue locally.  It could be helpful if you could enable some detailed logging so I can get some more insight into what is going on.  Can you try this:\n``` ruby\nwhen creating s3, enable a few loggers\ns3 = AWS::S3.new(:logger => Logger.new($stdout), :http_wire_trace => true)\n```\nI'm hoping this will help shed some light on what is happening so we can get this fixed.  Thanks!\n. I'm just spun up a clean EC2 Ubuntu instance and I'm going to try to replicate the issue again.\n. I'm still yet unable to reproduce the issue.  Here are the exact steps I took\n- Started a fresh EC2 Instance (AWS.ec2.instances.create(:image_id => 'ami-856f02ec', :key_name => '...', :security_groups => ['...'])) -- this is a 64 bit EBS backed instance running Ubuntu 12.04.2 LTS (Precise Pangolin)\n- SSH'd into the instance and ran the following commands:\n  - sudo apt-get update\n  - sudo apt-get install git\n  - sudo apt-get install build-essential libreadline6 libreadline6-dev libssl-dev libxml2-dev libxslt1-dev (readline-dev was unavaialble, so used libreadline6-dev)\n  - git clone git://github.com/sstephenson/rbenv.git ~/.rbenv\n  - git clone https://github.com/sstephenson/ruby-build.git ~/.rbenv/plugins/ruby-build\n  - echo 'export PATH=\"$HOME/.rbenv/bin:$PATH\"' >> ~/.bashrc\n  - echo 'eval \"$(rbenv init -)\"' >> ~/.bashrc\n  - bash -l\n  - rbenv install 2.0.0-p0\n  - rbenv local 2.0.0-p0\n  - gem install aws-sdk (this installed v1.10.0)\n  - exported my credentials to AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY\n  - cat > awstest.rb (pasted the following code)\nruby\nrequire 'aws'\ns3 = AWS::S3.new(:logger => Logger.new($stdout), :http_wire_trace => true)\nemr = AWS::EMR.new\nbucket_name = 'aws-sdk'\nputs \"Bucket exists: #{s3.buckets[bucket_name].exists?}\"\nemr.job_flows.each { |jf| puts jf.name }\nputs \"Uploading file\"\ns3.buckets[bucket_name].objects['foo'].write(\"I'm a little teapot\")\nI've run this 50+ times with a few pauses.  I will come back to this after I've let the instance idle.\n. @uberllama Thank you for looping us in.  I'll chime in on the paperclip thread.  After a quick scan of the other issue, they do not appear to be related, but I could be wrong. \n. @thinkgareth Could you try to test out the master branch and see if that resolves your issue (without the nested 4x retry block)?  I believe this might resolve your issue.\n. @dytsai Could you try out the master branch and see if that resolves your issues?\n. @kmcbride Thank you for the feedback!  This should go out with the next release.\n. @skalb what version of the Ruby SDK are you using?\n.  @skalb Can you give me an example stack trace of one of the timeouts?  Also, there is a newer version 1.28.0 that helps to eliminate this issue for some paperclip users.  You might also try updating and see if this resolves the issue.\n. @cdunn I'm reading through the code where request_transport is retried.  There appears to be a bug in their retry logic.  When they call req.exec the second time, no, attempt is made to rewind the body stream before.\nSimply put, the first request succeeds in sending some bytes, but not all before it encounters an error.  Net::HTTP decides to retry the request but fails to rewind the body stream, causing fewer than all of the bytes to be sent as indicated by the content-length header.  S3 keeps waiting for additional bytes, but never gets them, and then fails the request.\nThe SDK's retry logic always rewinds the body stream before it attempt to resend a request.  Net::HTTP make no such attempt.  I'm going to try to file this as a bug report and see if we can't get this fixed. \n. @pickerflicker Ruby Net::HTTP changed between 1.9.3 and 2.0 in a way that causes this error to occur.  In 1.9.3, when the connection fails or times out, the SDK retries the error.  In Ruby 2.0.0 Net::HTTP attempts to retry the failed request, but in a broken way.  It retries the PUT request without first rewinding the body stream.  This is okay if the request payload is a string, but the Ruby SDK uses IO objects for streaming requests.\nUsers of 1.9.3 are likely not reporting issues as their requests are succeeding with then SDK retries.  The timeout issue happens likely (this I have difficulty verifying) when S3 closes a long running HTTP session.  This may be due to congestion, or latent network issues, its hard to say.\nYou are not seeing this issue with fog probably for 2 reasons.  First, fog uses excon for http requests and not Net::HTTP.  Second, it does not use persistent HTTP connections by default.  A new connection is established (and closed) for each request.  You might experience the infrequent delay of s3 closing a connection if you enable persistent connections, but you likely would not experience the 2nd issue, as excon would not retry your request without rewinding the stream.\nThere are a number of possible work-arounds.  I'm trying to get a bug report and fix put together for Net::HTTP.  Until then, you can disable retries of PUT requests, following @cdunn's suggestion:\nruby\nNet::HTTP::IDEMPOTENT_METHODS_.delete(\"PUT\")\nAlso, it would be possible to put together a simple Excon handler for aws-sdk.  A simple implementation would not use persistent connections and should avoid the problems you are experiencing.   I'll try to update this thread with a sample implementation.\n. @pickerflicker Here is a sample http handler that uses Excon:\n``` ruby\nrequire 'excon'\nclass ExconHandler\ndef handle(req, resp, &read_block)\n    options = {\n      :method => req.http_method.downcase.to_sym,\n      :path => req.path,\n      :query => req.querystring,\n      :headers => req.headers,\n      :body => req.body_stream,\n    }\n    options[:response_block] = read_block if block_given?\n    connection = Excon.new(req.endpoint)\n    excon_resp = connection.request(options)\n    resp.status = excon_resp.status.to_i\n    resp.headers = excon_resp.headers\n    resp.body = excon_resp.body unless block_given?\n  end\nend\nAWS.config(http_handler: ExconHandler.new)\n```\nI would be interested to know if using this resolves your issue.  There are a few missing features from this implementation, that all could be addressed:\n- Custom SSL certs\n- Proxy support\n- Custom timeouts\nThese could all be addressed without much effort.\n. @pickerflicker What sort of frequency were you seeing the timeouts + retries in production previously?\n. We've been able to resolve these issues in the V2 Ruby SDK by patching Net::HTTP. These patches are:\n- Fix buggy expect 100-continue behavior\n- Fix retry logic for \"idempotent\" requests\nYou can see the patches in the v2 SDK here:\nhttps://github.com/aws/aws-sdk-core-ruby/blob/master/aws-sdk-core/lib/seahorse/client/net_http/patches.rb\n@mikhailov If you are interested to test out a patch, I'd be willing to backport these into a branch of the v1 Ruby SDK.\n. @mikhailov I've pushed a branch, s3-write-timeout-patch, which back ports some changes from the V2 SDK into this repo. Please feel free to give this a spin and let me know if it resolves the issue you are experiencing.\n. I should add a caveat that the fix only applies for Ruby versions 1.9.3+. Ruby 1.8.7 and Ruby 1.9.2 do not support HTTP Expect 100-continue and therefore will continue to have their connections closed by Amazon S3 under certain circumstances.\n. I found the issue that is preventing you from disabling the crc32 check.  There is a bug where the service-specific configuration options are not available until after their class is loaded.  You can work around this issue until the next release like so:\n``` ruby\nrequire 'aws-sdk'\nAWS::DynamoDB # forcibly load ddb, this makes the configuration option work\nthis will now work\nAWS.config(\n  :dynamo_db_crc32 => false,\n  :access_key_id => '...',\n  :secret_access_key => '...')\n```\nI have not yet been able to reproduce the crc errors, but I'm still looking.\n. Reopening.  The ability to disable CRC32 checks should be restored, but the actual issue causing the checks to fail is outstanding.\n. I spent some time today working on this.  I updated a test that was supposed to catch this (which wasn't triggering the gzip'd responses).  I also added the default accept-encoding, but I'm went with the shorter format (setting it to an empty value) to default to identity.  \nSee: http://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html\nThe \"identity\" content-coding is always acceptable, unless specifically refused because the Accept-Encoding field includes \"identity;q=0\", or because the field includes \"*;q=0\" and does not explicitly include the \"identity\" content-coding. If the Accept-Encoding field-value is empty, then only the \"identity\" encoding is acceptable.\nThank you for bringing this bug to our attention!\n. Thanks for reporting the issue and providing some insight on the bug.  I'm looking at this now.  I'm going to try to reproduce the issue and then test the fix.\n. Thanks for finding/fixing the issue!\n. The AWS::DynamoDB::ItemCollection class documents the #query method to accept a :select option.  This option allows you to specify the attributes to get.  \nThis method should probably also accept (and not ignore) the :attributes_to_get option.\n. I have not looked at the underlying API yet for RDS tagging to see how it might compare to the EC2 tagging API.  Having a similar API would be helpful.\nWe need to add a contribution guidelines to the project, but until then, checkout the blog post @lsegal wrote about contributing. http://ruby.awsblog.com/post/TxL3OOUOW45Q47/Contributing-to-the-AWS-SDK-for-Ruby\n. The SDK does not, primarily because there is not a great way to do it.  Here is a link to a discussion about this exact problem: https://forums.aws.amazon.com/thread.jspa?threadID=108012\nShort summary, you can use IAM to get the account ID IF you are not an IAM user with limited privileges.  Even then it is possibly with some hackery.\n. Thanks for the doc fix!\n. Is there a reason you closed this pull request?\n. The head request is likely failing due to eventual consistency (e.g. the object was recently uploaded but not all endpoints are aware of its existence  yet).  \nThe SDK retries all networking errors and ~ 500 errors, but it does not retry 400 level errors (400 level errors are considered to be client errors).  Amazon S3 is returning a 404 response from the head request.  While it is possible to retry these (and it seems like it would make sense sometimes), it could cause other issues.  Head object requests are commonly used to check for the existence of an object (e.g. bucket.objects['foo'].exists?) -- retrying here would be undesirable on a 404 HEAD object response.\nI am open to suggestions on how we could improve the interface.\n. I am currently exploring some options for general purpose helpers to deal with eventual consistency issues across the SDK.  I will go ahead and close this issue for now.\n. Thanks for the doc fixes!\n. Thank you for the deep dive on this.  If you would like to address @lsegal's feedback, then I'll be happy to merge.\n. I went ahead and made the minor changes suggest and merged this.  Thanks @Ryman!\n. The issue appears to be in the error extracting logic (failing with an empty response from the service).  I would have assumed an AWS::Errors::ClientError would have been raised, which is not happening.  I'll take a look at what is going on here.\n. It appears that the error object is an AWS::Errors::ClientError.  The actual error object instance is extending the module (which is why it is showing the inspect string from its actual class, Errors::Base).  \nYou should be able to simply rescue client error for now.\nruby\nbegin\n  do_something\nrescue AWS::Errors::ClientError\n  # rescues all 400 level errors\nend\n. I'm leaving this issue open.  We need to:\n- Provide a better inspect string\n- Model the 400 errors as something more helpful (e.g. AWS::S3::Errors::BadRequest)\n. @dkhofer One other quick note, when a service returns a 500 level response, the error extends AWS:Errors::ServerError.  You probably should not need to rescue these explicitly, as the SDK already retries these (up to 3 times by default).\nYou can specify the retry count with the :max_retries configuration option.  Some users have reported that some transient networking and server errors errors go away when they increase the max retry count.  \nThe SDK backs off exponentially between retries.  The default retry delays (in seconds) are:\n[0.3, 0.6, 1.2]\nAdding a fourth retry, the delays are:\n[0.3, 0.6, 1.2, 2.4]\nLastly, the SDK  only selectively retires 400 level errors (e.g. throttling errors).  The majority of 400 level errors are related to bad input and will never succeed when retried.\n. My only concern (besides adding tests and fixing failures) is that the message id should only be populated on  the raw message if it responds to the setter.  The raw message could be a simple string object.\n. I merged your pull request and added a test case and made one minor fix, see fd7a0ea.  Thanks for your contribution!\n. @redzebra I appreciate your thoughtful input on this issue.  I understand your frustration about needing to pass the essentially static us-east-1 region to each instantiation of AWS::Support when working outside us-east-1.\nRegarding the recent deprecations, we are currently evaluating ways to simplify and improve configuration.  One of the changes we are evaluating is grouping configuration options by service.\nImagine you want to log all AWS requests, but you need expanded logging for one service.  Using the proposed configuration change, you could do this:\nruby\nAWS.config(\n  :logger => default_logger,\n  :log_formatter => AWS::Core::LogFormatter.short,\n  :sqs => {\n    :log_formatter => AWS::Core::LogFormatter.debug\n  }\n)\nThis would also work as one solution for the regions issue:\nruby\nAWS.config(region: 'us-west-1', support: { region: 'us-east-1' })\nOur hesitation on forcing AWS::Support to 'us-east-1' is that this would be a breaking change for customers if the service ever regionalizes.  Changing the pattern at that time from 'support.amazonaws.com' to 'support.%s.amazonaws.com' would then send non us-east-1 region users to a new endpoint that potentially has none of their support requests.  I don't know if or when this would ever happen.  \nI want to continue this discussion, as I agree the current situation is sub-optimal.\n. I just ran the entire test suite in ruby 1.8.7-p371 without errors.  The travis test suite (https://travis-ci.org/aws/aws-sdk-ruby) is also showing green for all tested versions of Ruby (including 1.8.7).  All of the failures (except the last one) are related to shared example groups and they are failing from inside rspec. \nHow are you executing the tests?  I running bundle install from the clone of the repo and then bundle exec rspec. \n. I'm going to close this issue.  The tests are passing when using the proper version of rspec.  Please reopen if this is not the case.\n. Thanks for the fix!\n. Thank you for brining this to my attention.  It appears the related commit (https://github.com/aws/aws-sdk-ruby/commit/ec442d92d8a9a2df7750b87b52303898aeef5b90) was not pushed before the gem was cut.  I'm re-releasing this now (v1.11.2) with the corrected gem specification.\n. The release has been cut, and you can check out the updated dependency on the Rubygems page (https://rubygems.org/gems/aws-sdk).\n. I've dug through the Amazon S3 documentation (API, FAQ, pricing, etc) and haven't found any mention of increased cost.  Instead, I only found references to reduced redundnacy being cheaper (which is usable apparently with SSE).  I'll go ahead and remove that line from the docs.\n. Thanks for the heads up!\n. I removed the top require 'aws-sdk' instead of the second one.  I want to defer loading the SDK until after the $LOAD_PATH has been setup.\nI'm still chewing on how to name the executable.  I'm not crazy about the \".rb\" suffix.  I did some quick searching looking for some conventions for each of the target languages.\n- Node.js executable scripts tend to end with \".js\"\n- PHP executable scripts tend to end with \".php\" and use underscores in place of dashes\n- The AWS CLI (python) will likely use a name like aws-repl (omitting -sdk)\nI realize adding \".rb\" or \"-rb\" would be more explicit, I'm just trying to come up with something short, simple and sweet.  aws-sdk-repl.rb is a mouthful to type.  Thoughts?\n. The \".rb\" extension generally implies a library file.  Additionally, there exists a aws.rb library file within the SDK which could be confusing (I realize it wouldn't cause any conflicts).  Despite the suffix, aws-repl.rb feels a bit less confusing, probably because it uses the self describing term, repl.\nWhat about aws-rb-repl?  This allows for other expansions like aws-php-repl, aws-js-repl, aws-py-repl, etc.\n. Are you using the aws-sdk gem or the aws-ses gem?  The AWS SDK for Ruby (aws-sdk) supports keep alive by default.  It maintains a pool of persistent HTTP connections and only opens a new connection after it has been closed by the service or been idle for too long.\n. I'm going to close this issue as the SDK does support keep alive.  Please reopen if you find an issue with the current implementation.\n. @rainerborene I'm not sure what you are asking. The aws-sdk gem uses Net::HTTP as the default http handler. It also manages a pool of persistent connections, and these connections rely on Net::HTTP's support of keep alive.\n. @rainerborene Exactly one connection. If you replaced the 10.times block with something using threads to make requests concurrently, then there would be 10 connections. The internal pool is shared across clients and will grow to match the number of concurrent requests.\n. The 5f93be1 commit should resolve this issue and will go out with the next release.  You should be able to work around this issue until then by monkey patching the #console_output method.\nruby\nAWS::EC2::Instance\n  def console_output\n    output = client.get_console_output(:instance_id => self.id)[:output]\n    Base64.decode64(output) if output\n  end\nend\nThank you for bringing this to our attention!\n. I read through the linked fog issue and it appears that fog (for the time being) has decided to continue supporting Ruby 1.8.7.  They have also locked their nokogiri dependency to ~> 1.5.0, as 1.6+ requires ruby 1.9.3.  I think the most important outcome from this is they are looking more closely at how long they want to continue supporting 1.8.7.\nFor now,  think we are going to have to leave the nokogiri dependency locked where it is.  But lets use this issue as a place to discuss the future of supporting Ruby 1.8.7.\n. @benhamill @gtd I understand your points and agree with you.  That said, the current nokogiri version requirement was added as a stopgap.  I don't think it is fair to break existing 1.8 customers without a period of communication first.  I realize this make things difficult on the other end.  Ruby 1.9+ customers who want to use newer features of Nokogiri are negatively affected.  I still feel we owe existing users a grace period with some warning.\nThere are other paths to mitigate the issue.  We could potentially bump the major version of the SDK from 1 to 2.  This seems a bit drastic, but it follows the idea of semver.  Lifting the version constraint is technically a backwards incompatible change.  It would not help users that do not express specific version dependency on the SDK.  I'm not really a fan of this idea, just putting it out there for discussion.\n@lsegal What do you think about simply adding a note to the top of the README, posting a message on the blog about a pending change to the nokogiri requirement?\n. I posted this on a few of the pull-request issues, but I'll add it here as well.  I posted a blog post earlier this week with information about our plans and a timeline on when we will remove the Nokogiri upper bound.  You can read about them here:\nhttp://ruby.awsblog.com/post/Tx2T9MFQJK7U74N/AWS-SDK-for-Ruby-and-Nokogiri\nIn summary, in ~ 2 weeks the restriction will be removed.  The blog post outlines what users of Ruby 1.8 can and should.  Thanks everyone for being patient.\n. @smenor Yes, you should be able to use 2.0.0.rc1 now.  If you do encounter any bugs, please log an issue.\n. Version 1.27.0 of the Ruby SDK has been published.  This release removes the upper bound on the Nokogiri gem dependency.  See the release notes for more information.\n. I am going to close this issue for now.  To re-state, our end-of-life plan for Ruby 1.8.7 is to continue supporting 1.8.7 in the v1 SDK.  The v2 Ruby SDK only supports Ruby 1.9.3+, https://github.com/aws/aws-sdk-core-ruby.\n. Looks good!\n. Thanks for the doc fix!\n. Thank you for reporting the issue.  I've got a potential fix that should resolve the issue.\n. @lsegal Would you give this a review when you have a chance?  Thanks!\n. The previous commit had test failures with Ruby 1.8.  The Module.constants method does not accept a boolean argument as it does in 1.9+.  Additionally, constants returns strings instead of symbols.  \nLastly, I removed the proposed return value that would return all of the constants autoloaded by eager_autoload!.  This proved un-relaiable as one class may transparently trigger autoloading another and these were not getting counted.  This happened in cases such as one class inheriting from another one, or including a module.  The result is that everything gets loaded, but reporting was unreliable.\n. I was unaware that Yard has special treatment for a @return [void].  I'll gladly make that change.\n. @Flameeyes I'm also not able to reproduce this test failure locally.  Following your stack trace shows that the #data method is getting called on yielded response data and not on a response object.  I've seen numerous test regression from rspec and this looks like another.\nWhat version of rspec are you running locally?  The Gemfile specifies a hard dependency on '2.12'.  If you run bundle update and then rake does the issue persist?  If so, what does bundle show rspec return?\n. Interestingly, this is the approach we previously took.  The problem with whitelisting network errors was we never really got a full list of possible runtime errors and things crept through.  The scenario with JRuby is interesting though and something we will need to strongly consider.  I am not completely opposed to reverting, but this is a risky change.  Any network errors raised, but not listed, would go from getting retried to directly raised.  \n@lsegal do you have any thoughts?\n. Are you getting mistakenly getting errors trappred, or is this submission just a general improvement on how errors are handled?\nI am not fundamentally against reverting this behavior.  The current behavior was implemented in bcd98dbc579e9d520dadffe9911c64743a4ce082.  This change was prompted by a customer reported issue on the AWS forums (https://forums.aws.amazon.com/thread.jspa?threadID=115448&tstart=50).  It was an attempt to stop playing whack-a-mole trying to enumerate all the possibles errors raised by Net::HTTP.  If we do revert this, we would need to add the OpenSSL error to the list.\n. @iconara As I mentioned above, I am not against reverting this to how it used to work.  I was giving clarification on why the change was originally made so those considerations would not get lost.  I understand this is causing grief, so lets get this fixed.\n@grddev If you would add OpenSSL::SSL::SSLError to the list of errors to retry, then I think we should be able to merge.\n. Looks good, thanks for the contribution!\n. I will check these out locally and give them a try, but they look good from my initial work.\n. Your solution looks like it would work well to prevent retrying network errors.  I think we should merge this to address the immediate problem.  That said, I think it would be worthwhile to approach this problem from outside the HTTP handler.  Solving this inside the NetHTTPHandler forces other http handlers to solve the same problem.\nA longer term fix would be to have the core client class skip retries when a read block is present.  Additionally, it should inspect the headers for potential errors so that we could retry before sending the body XML error to the user.  Once this happens we could strip out this logic from the NetHttpHandler and improve the customer experience.\n@lsegal Do you have any thoughts on this?\n. I omitted a key point from my last comment.  The secondary problem exists when a HTTP response is received, but it contains a non-200 retryable error (some 400 level errors, 500 level errors and redirects).  Moving the retry disabling code down into the client would could solve this problem for more than transient network errors. \n. Merging this for now as it resolves an important issue.  We can work on generalizing this later.  Thanks for the great work @grddev!\n. @bpot You are correct, the CurbHandler would need to accept these options, as it does not support any of these.  Some of them won't make sense (like :http_idle_timeout as this is a detail of the connection pool used by the NetHttpHandler).  \nI think we should merge this request for now.  If you would like to expand CurbHandler's support for the additional HTTP options, that would be awesome.  If not, you could open an issue to track the missing features.  \nThoughts?\n. Please feel free to open a new issue for adding additional features to the curb handler or to submit a pull request.\n. Thanks for the patch!\n. The #with_dimensions collection accepts an array of dimension names and values, returning another collection.  Can you be more specific about what input params are getting ignored? An example would be very helpful.  Thanks!\n. Thanks for reporting the issue!  I've committed a fix and added tests.  Until the next release, you can work around this by using the #filter method.\n. Thanks for the update to the samples!\n. I don't like that the SDK currently accepts fewer than 6 characters and zero pads.  This feels like a bug that masks poorly formatted user input.  There is no way to know if the input was intentionally truncated or not and left padding zeros feels hackish.\nThe documented accept format is:\n# @param [String] code1 An authentication code emitted by the device.\n  # @param [String] code2 A subsequent authentication code emitted by\nIdeally, this function would have never attempted to accept integer inputs.  Given we have existing tests, I'm willing to go as far as fixing the bug and not introducing test regressions, but I don't want to go out of our way to support string input of fewer than 6 characters.\nThis would be my suggested change to the code:\ndef format_auth_code(code)\n  code.is_a?(Integer) ? sprintf(\"%06d\", code) : code\nend\nAs well as the extra test:\nit 'accepts input like \"099999\"' do\n  client.should_receive(:enable_mfa_device).with(hash_including(\n    :authentication_code_1 => '099999',\n    :authentication_code_2 => '098765'\n  ))\n  device.enable('username', '099999', '098765')\nend\nThoughts?\n. @ab Please do.\n. Scratch that, I just pushed the branch I had created locally with the code above. Thanks for your help @ab and @lsegal.\n. Thanks!\n. Thanks!\n. Thanks for the helpful contribution.\n. I merged in your changes locally and made a few additional updates:\n- HashModel.create_table now obeys the :hash_key attribute.\n- The hash key attribute is now a regular attribute, rather than a managed field, this allows the user to edit the attribute, add validations, etc.\nI made a few other related commits you can look at as well, just checkout the repo commit history.\nThanks for the submission!\n. I've corrected the documentation to reference #initialize.  This should remove some of the confusion.\n. I was thinking the same thing, and was going to update the pull to reflect.  seems like a reasonable default.\n. I addressed both feedback concerns (see above).\n. Another option would be to create a HTTP handler that uses Faraday for transport.  Customers could configure the Faraday apapter.\n. With the recently announced work on v2, we are going to put on hold requests requests like this to the basic client layer.  We are looking into how we can make these things more pluggable in the future and will consider this suggestion.\n. @p7r The v2 AWS SDK for Ruby supports pluggable HTTP clients. We currently only ship and support the Net::HTTP backed client because it is universally available. I can put together a proof-of-concept gist that should provide guidance on how to do this. \n. @p7r I've added a gist with a sample implementation here: https://gist.github.com/trevorrowe/ae55df222d45cb424366c98bdc5e1e4e\n. If you decide to go down that path and want to offer up a pull-request, reach out to me so we can talk about packaging. We may choose to not bundle it as part of the SDK, as we are trying our best to limit dependencies and ensure those we have are portable. I suppose we could ship it with a soft dependency, or potentially it could be made available as another gem, e.g. aws-sdk-typheous. I'm open to discussion on how that might work.\n. I like the idea of having a CredentialFileProvider that is initialized with a path to a file on disk.  This affords users the ability to be explicit if they want to use a file provider.\nYou could then merge the two approaches.  The ENV provider could still check for \"#{@prefix}_CREDENTIAL_FILE\" and then optionally construct a CredentialFileProvider.  Thoughts?\n. Great contribution!  Thanks!\n. Thanks!\n. Classes that extend from core resource currently only cache static attributes that are given to the constructor and status is not static.  This causes the SDK to make a request to get the current status each time this attribute is accessed.  You can cause the SDK to cache the status inside a memoization block, but the mechanism for this is a bit awkward.\nThe simplest approach is probably to use the SDK's built in response stubbing mechanism.  Your example could look like this:\n``` ruby\nAWS.stub!\nstack = AWS::CloudFormation::Stack.new('stack-name')\nstack.client.stub_for(:describe_stacks).data[:stacks] = [{\n  :stack_name => 'stack-name',\n  :stack_status => 'CREATE_COMPLETE',\n}]\nputs stack.name # stack-name\nputs stack.status # 'CREATE_COMPLETE'\n```\nBTW, this will change with v2 of the SDK where attributes will memoized by default.  You can read about some of the other upcoming v2 changes here: http://ruby.awsblog.com/post/Tx1KGN84F9CRCKM/Happy-Birthday-SDK-Now-Let-s-Celebrate-the-Future\nDoes this help?\n. I'm going to close this question for now.  Please re-open if you run into any issues.\n. Sorry for the slow response on this, but the fix went live a few weeks back.  As part of this, we moved from redcloth to rdiscount and we moved the docs from the source files into yaml files that are loaded by YARD while rendering docs.  The format of these files may change.  The current format makes them not very human readable without building docs.\n. Thanks for the doc fix!\n. The fix was quick, it appears when we updated the API version for AWS::CloudFront::Client with the 1.13.0 release, it had the side effect of requiring signature version 4.\nIn a different commit (bb6c01efc1e147d4ce3571cfd676319ef88469f0) I fixed a broken integration test that was also affected by the API change.\n. We will cut a bug fix release shortly (v1.14.1).\n. I found and fixed the issue with the REXML sax handler.  It is a really trivial fix.  I'll get this updated shortly.\n. Looks good.  We just need to add documentation around the new configuration option (see AWS::Core::Configuration and AWS.config).\n. Awesome!\n. Looks good, thanks for the contribution!\n. Thanks for the patch!\n. We currently do not provide last methods intentionally.  If a bucket in S3 contains a large number of objects, then it could require hundreds, or thousands, or more requests to get the last object.  S3 does not provide an API to get the \"last\" object, instead you must list objects 1k at a time until the response indicates there are not more items.  It is not possible to determine the number of requests required.\nWhile it is possible, it only works nicely for buckets with few objects or collections with few items.    It is possible we could consider this and adding a strong warning to the documentation to not call this for collections with potentially large number of objects.\n. There is currently a limitation that the higher level abstractions for AWS::DynamoDB will only work with the older API version.  Ideally, these interfaces should lock the client API version internally until they could be updated to be compatible with the backwards-incompatible newer API version.  \nIf you are okay using the AWS::DynamoDB::Client class directly then locking the API version will work fine.  \nI'm going leave this issue open, because the above should work without raising errors, even if it has to drop down to the older API version for now.\n. Sorry, closed this accidentally.  We still need to do the work outlined above.\n. I agree, the inconsistency is a bit ugly.  As @lsegal pointed out, renaming this outright would break users.  I am open to a pull request that renames the class and then aliases it under the old name.\n. Looks good!\n. I'm fine with the proposed fix.  Lets look at this again when we revisit the higher level interfaces with v2.\n. Looks good.\n. LoadBalancer#instances returns a collection with ELB specific logic.  If you are looking for a quick way to create an EC2::InstanceCollection, you could try the following:\nruby\nids = load_balancer.instances.map(&:id) # you may want to do this inside a memoize block\ninstances = EC2::InstanceCollection.new.filter('instance-id', ids)\nThis could be wrapped in a function like #ec2_instances.  Is this what you are looking for?  I'm going to close this issue, because we are not currently focused on v1 features.  Pull requests are accepted though.\n. It appears when you pass a configuration object directly into the client constructor, it is ignoring API version in the configuration object.  You can work around this by locking the version by passing the API version directly.\n``` ruby\nthis works\nAWS::DynamoDB::Client.new(api_version: '2012-08-10')\n=> #\n```\nThis will always work, and will override the AWS.config API version.  The bug here appears to be the client constructor is not peeking inside the passed configuration object under the service name.\n``` ruby\noops, buggy\nconfig = AWS.config.with(dynamo_db: { api_version: '2012-08-10' })\nAWS::DynamoDB::Client.new(config: config)\n=> #\n``\n. @mikeys You mentioned you were building a library on top of theAWS::DynamoDB::Client`.  You may be interested to know that we are working on a version 2 of the Ruby SDK.  As part of this update, the client abstractions are receiving a major overhaul.  Many of the changes make it easier for library authors to extend and modify client behavior.\nWe plan to publish the code here to GitHub shortly.  It will still be a work-in-progress, but we would love to get feedback on the new client interfaces.  It will be under a different namespace which allows users to run the existing SDK side by side with the new one.  I'll try to remember to post a note here once it is public.  You can also watch our blog where we will be making announcements (http://ruby.awsblog.com/).\n. I just pushed a fix for this issue.  Also, @mikeys, have you taken a look at http://github.com/aws/aws-sdk-core-ruby ?  It is still a work in progress, but it has a fully functional set of DynamoDB clients.\n. @rhossi AWS::Record was developed with the goal of being compatible with ActiveRecord/ActiveModel, but without a hard dependency.  We did not want users of the AWS SDK to force a hard dependency on such a large set of libraries.\nIn retrospect, AWS::Record should have been developed as a separate gem.  In this case, I think the ActiveModel dependency would be just fine.  \nWe are evaluating how things will change with the upcoming v2 release.  My current preference would be to extract AWS::Record into a separate gem, perform a major version bump and then we could bring in ActiveModel.  This shouldn't be that far off.\n. The documented return type is incorrect.  The current behavior is intentional.  I will update the docs to indicate that it returns a ObjectVersion when versioning is enabled on the bucket.  \nThat said, it is very easy to get the object from an object version:\nruby\n$ aws-rb\nAWS> obj_version = AWS.s3.buckets['my-versioned-bucket'].objects['key'].write('data')\n<AWS::S3::ObjectVersion:my-versioned-bucket:key:9MHtaVle8bWn1BtdFruEtu0IDoqA5fjy>\nAWS> obj_version.object\n=> <AWS::S3::S3Object:my-versioned-bucket/foo>\n. Also, if this is blocking you, you can work around this issue by specifying the full endpoint:\nAWS.config(:sts => { :endpoint => 'sts.us-gov-west-1.amazonaws.com' })\n. Looks good, thanks!\n. The REXML parser shouldn't be difficult to get into working order.  However, we also make use of Nokogiri's XML building interface.  These are not abstracted, and would have to be reworked to have a fallback.  That would be a decently large amount of work. A pull request would be welcome, but right now we are focused on v2.\nFor v2, we are building off multi_json and multi_xml to keep our dependencies more flexible.  We are also using the builder gem (pure ruby) for building XML.  I'm hoping to push v2 publicly soon so we can start getting feedback and moving things forward.\n. Sorry for the slow reply on this issue.  I am unable to replicate this with the latest version of the SDK.  It looks like it is using the 2012-08-10 API version for AWS::Record (which is not supported at this time).  A fix was done previously that locked the API version for AWS::Record to the client it knows how to use.\nIf this persists to be an issue with the latest SDK version, please re-open.\nThanks!\n. Thanks for the fix.\n. Thanks!\n. Thanks for the fix!\n. Thanks for the fix!\n. Thanks!\n. I am unable to reproduce this issue.  Are you passing the content-type and content-md5 to both URL for and along to your http request as headers?  Here is a sample I just ran.  Can you give this a try and let me know if it fails?\n``` ruby\nrequire 'net/http'\nrequire 'logger'\nrequire 'digest/md5'\nobj = AWS.s3.buckets['YOUR-BUCKET-NAME'].objects['sample.json']\nbody = '{ \"demo\": \"value\"}'\ncontent_type = 'application/json'\ncontent_md5 = Digest::MD5.base64digest(body)\ngenerate a pre-signed put URI\nu = obj.url_for(:write,\n  :content_type => content_type,\n  :content_md5 => content_md5,\n  :secure => false)\nbuild a PUT request\nput = Net::HTTP::Put.new(u.request_uri, {\n  'content-type' => content_type,\n  'content-md5' => content_md5,\n})\nput.body = body\nsend the PUT request\nhttp = Net::HTTP.new(u.host, u.port)\nhttp.set_debug_output(Logger.new($stdout))\nhttp.start\nresp = http.request(put)\nresp = [resp.code.to_i, resp.to_hash, resp.body]\nhttp.finish\nensure everything worked\nputs obj.head[:content_type]\nputs obj.read\n``\n. Content-Type and Content-MD5 are the only two that must be given to both#url_for` and passed in as headers.  This is because they are part of the computed signature.  They must both match in both places as well or you will get a signature error.\nI agree that the documentation in #url_for is lacking.  Would you be interested in submitting a pull request with expanded documentation?\n. Closing this issue as there is no code change required and the documentation has been updated.\n. Closing as a dup of #382.\n. I did some digging into this issue.  The bug is caused by how the SDK assumed the :limit parameter should be understood..  The SDK treated limit as \"don't give me more than N items\".  The service treats limit as \"we will look at no more than N items\" without a contract of how many it will return.  When the SDK sees an empty result, it asks for more, again ask, \"please give me no more than 1\" ...\nThe fix is simple enough.  I just need to add test coverage.\n. Shortly.\n. I took a quick look at this.  There is a quick sanity check that ensures you gave values for each of the placeholders.  In this case, it is a complete false positive and is raising needlessly.  I'll see if I can't create a fix for this shortly.\n. I fixed the bug and added a unit test to cover this behavior.  Thanks!\n. Thanks for the doc update!\n. Thanks for the contribution!\n. We will be removing the restriction in two weeks.  I just posted a blog post about this issue on our Ruby blog:\nhttp://ruby.awsblog.com/post/Tx2T9MFQJK7U74N/AWS-SDK-for-Ruby-and-Nokogiri\nThank you everyone for being patient.\n. I'm closing this issue now that we have released a version of the gem without the upper bound restriction on Nokogiri.\n. Thanks for the fix!\n. Thanks for the fix!\n. The SDK is only holding onto static attributes.  Calling #closed_after on a workflow execution will trigger a new request every time to see if the value has updated service side.  This is sub-optimal for tight loops like you have above.\nYou can work around this by enabling memoization:\nruby\nAWS.memoize do\n  # only one request required\n  workflow_executions.each {|ex| puts ex.started_at; puts ex.closed_at}\nend\nAll attributes are held inside a memoize block.  The block indicates it is okay to cache non-static attributes within a given scope.  I'm a not fan of this and we will remove the need to use memoize blocks in v2 of the SDK.\n. It can't be updated or specified, but it can go from nil, which means not closed to an actual timestamp.  Static attributes are those like a resource identifier or a creation timestamp that never change.  \nI'm going to close this issue for now.  I hope this helps.\n. There are two properties on the error that should have the information you require.  Take a look at #http_request and #http_response.  You may have to JSON parse the request body to extract request parameters, but it should all be there.\nruby\nbegin\n  ddb.list_tables(:limit => 10000) # generate an error returned by the servie\nrescue\n  puts error.http_response.headers['x-amzn-requestid'].first\n  puts error.http_request.body\nend\n. The http_request#param attribute is only for query-params, which is something DynamoDB does not use.  You only have access to the http request and response objects, but not the original request parameters.  You can determine the hash key by json parsing the http_request.body.\nAs for exposing the request id, I agree it would be a good idea, but there is not one canonical place for it across services.  Many/most expose it in the http response body (inside some response error XML).  Extracting this systematically, which possible would take some time to get right.  We are looking into this for v2 of the SDK though.\n. I did some digging.  The AWS::DynamoDB::Table#batch_get method does not support the :return_consumed_capacity because it has no place to return the value to.  The return value from #batch_get is an enumerable object that yields results from the batch operation.  If you skip this helper method, and go straight to AWS::DynamoDB::Client#batch_get you will have full access to the entire set of API inputs and outputs.\nIf you disable retries of throughput errors, then yes a ProvisionedThroughputExceededException may be raised.  That said, it is also possible for this to raise with retries enabled.  It will retry a maximum of 10 times, with exponential backoff before giving up and raising.  With this feature disabled, it will not retry and it will raise the error straightway.\nI am not 100% certain, but I don't believe DynamoDB returns ThrottlingExceptions.  This is the common error code given by most services when you are calling API operations with too much frequency.  I suppose DynamoDB might return this error if you were to call #describe_table too much, but it uses the provisioned throughput errors for throttling all of the data APIs.\nYes, you can raise any error class explicitly.\n. Closing this issue as there is nothing to be done from the SDK side.\n. Without more context I can't say what is going one.  Something worth noting is both of those calls to sleep shouldn't make much of a difference.  Neither of the preceding lines makes a request.\nruby\nstack = AWS.cloud_formation.stacks['somestack'] # makes no request\nresource = stack.resources['name'] # still makes no request\nresource.physical_resource_id # triggers the first request\nObjects are loaded from AWS lazily to prevent unnecessary calls.  There is no need to describe a resource if your only intent is to delete it, for example.  \nKeep in mind, every call to #physical_resource_id on the resource triggers a new call, unless you wrap this inside a memoize block.\n. If you enable logging you will see every request that is made.  You can setup logging globally via AWS.config or when you construct your cloud formation client:\n``` ruby\nrequire 'logger'\nglobally\nAWS.config(:logger => Logger.new($stdout))\nor for a single service interface\nAWS::CloudFormation.new(:logger => Logger.new($stdout))\n```\nAWS.memoize will only help if you are accessing multiple attributes of a resource or accessing attributes inside a tight loop and want to prevent multiple service calls.\n. I notice 9 different calls to #describe_stacks with the same stack name parameter.  It looks like you are accessing many attributes on the same stack and these are each triggering a different call.    This is likely part of the problem.  Do you have a larger code sample you can share so I can see your usage?\n. Another thing to consider.  We have released the new aws-sdk-core gem.  This is the first part of our v2 release.  It does not share the same higher level abstractions, but it does support the Cloud Formation API.  You may want to check it out:\nhttps://github.com/aws/aws-sdk-core-ruby\nI'm going to close this old/stale issue.  Please re-open if you continue to have issues.\n. Thanks for the fix, this should go out shortly.\n. Thanks!\n. The error indicates the HTTP session does not respond to #last_used.  This method is punched onto the HTTP session before it is used by the connection pool.  It is used by the pool to determine how long the connection has be idle and if it should be re-used or closed.\nMy guess is that delayed job doesn't play nice with objects that have been extended directly like this.\n. Thanks for the fix!\n. Thanks!\n. Thank you for pointing this out.  I believe the limit has changed since launch.  I think I'll just remove this note for now to prevent it from going out of sync again.\n. Thanks.\n. I just published a blog post letting our users know that we will be removing the upper bound restriction on Nokogiri in two weeks.  We can merge this in on the 19th.  Thanks for your patience.\n. This should be released shortly.\n. Version 1.27.0 of the Ruby SDK is now published.\n. Thanks for the contribution!\n. I just ran the following code without issue JRuby 1.7.6:\nruby\ns3.buckets['aws-sdk'].objects['foo'].write(data: 'TEST', acl: :public_read, content_type: 'image/jpeg')\nAre you still experiencing this issue?\n. Thanks for reporting the issue.  This should go out shorty.\n. Without seeing more debugging information I can not sure.  I ran your example without any errors.  Can you enable debug logging and then share the http wire log for a failed call?\ncf = AWS::CloudFront::Client.new(http_wire_trace:true)\ncf.create_distribution(...)\nYou may also want to take the opporunity to check out the new aws-sdk-core gem (github.com/aws/aws-sdk-core-ruby).  Its in a developer preview currently, but has support for CloudFront and provides other helpful features not present in the v1 SDK.  You can install it side-by-side, as it uses a different gem name and root module name.\nUsage is similar:\ncf = Aws.cloudfront(http_wire_trace:true)\ncf.create_distribution(...)\n. Your pastebin snippet shows that the request succeeded.  The error you got is from the SDK because the response structure in V2 for cloudfront differs ever so slightly.  Try this:\nruby\ncf = Aws.cloudfront(http_wire_trace:true)\nresp = cf.create_distribution(...)\nresp.distribution.domain_name\n. I don't quite understand.  It is not possible for the returned response to be nil.  What to you get if you call puts resp.data.inspect?\n. I'm closing this as I can't reproduce.  Please let me know if you are still having issues.  Error stack traces and HTTP wire traces are helpful.\n. I just responded on the forums with a question for additional debugging information.  I'll copy that response here and link to this issue from the forum post.\nI was unable to reproduce the issue myself.  I created a 200mb text file locally and uploaded it to S3 using a new key like so:\nruby\nmy_key = OpenSSL::Cipher.new(\"AES-256-ECB\").random_key\ns3.buckets['my-bucket'].objects['key'].write(Pathname('data.txt'), :encryption_key => my_key)\nThe upload succeeded and I was able to then download the same object to a file and the resultant file was identical to the source file.  Can you give me a code example of how you generate your key?  I would like to try to reproduce this issue so we can fix it.\nAlso, it would be very helpful to see some logging output.  Here is a small snipet that would generate some helpful information:\n``` ruby\nstart by running the interactive console\n$ aws-rb\nrequire 'pathname'\nconfigure a logger that truncates the data to a manageable size\npattern = \"[AWS :service :http_response_status :duration :retry_count retries] :operation(:options) :error_class :error_message\\n\"\nlog_formatter = AWS::Core::LogFormatter.new(pattern, :max_string_size => 10)\nAWS.config(:log_formatter => log_formatter)\ngenerate a key, please show an example of how you create the key\nmy_key = OpenSSL::Cipher.new(\"AES-256-ECB\").random_key\nreplace the bucket name, key and path as needed\nAWS::S3.new.buckets['aws-sdk'].objects['key'].write(Pathname.new('data.txt'), :encryption_key => my_key)\n```\n. Looking at the logging output I see the call to initiate_multipart_upload specifies the unencrypted content length as 202260596110 Bytes.  This is also reflected in the stream size of the opened file object.  That is 192,891 megabytes.  That is roughly 188 gigabytes.  \nYou described the file as ~ 200 MB, so I am guessing something is going wrong when it tries to determine the file size of your object.  That still doesn't explain why only 20 MB get uploaded before upload is completed.  \nDoes the problem go away if you disable the multipart upload and send the file in a single put request with encryption?\ns3obj.write(Pathname.new(path), :encryption_key => my_key, :single_request => true)\n. You are correct, I miss-read your original statement on the estimated file size.\nS3 is killing the upload because it knows it will not accept the full file.  It has a hard limitation of 5GB per PUT Object request.  Based on the Content-Length given, S3 is choosing to kill the upload before receiving all 200GB.  The SDK does have a feature you can enable that will wait for a 100-continue from S3 before sending the payload.  This can be when S3 may want to redirect or deny an upload based on the request headers.  However it also holds the payload for all requests until a response is received or a timeout expires.  In this case, we know why S3 is rejecting the upload.\nBased on the part size, I think I know where to look.  Give me a few mins and I will see if I can dig up the culprit.\n. Good news, I've narrowed down the bug to the AWS::S3::CipherIO class used to encrypt and stream the file.  You can see a repo of the buggy behavior from this gist: https://gist.github.com/trevorrowe/7574548\nI'll try to get this fixed shortly.  Thank you for your patience.\n. I've found the bug.  It will take a bit of rework on the internals of the CipherIO class to fix for the general case, but there is a really simply workaround.  If you specify the preferred minimum part size to a number of bytes that is evenly divisible by 16, then everything should work.\n``` ruby\nspecify the min part size as an even 20MB\ns3obj.write(Pathname.new(path), :encryption_key => my_key, :multipart_min_part_size => 20971520)\n```\nYou can specify this value as a global default via AWS.config\nruby\nAWS.config(:multipart_min_part_size => 20971520)\nLet me know if this resolves the issue for now.  I'll work on coding up a proper fix.\n. I already have that fix in place.  Currently the CipherIO class is only used internally and is marked as api private.  That said, this workaround/fix only works if the stream consumer, e.g. Net::HTTP, only requests chunks divisible evenly by 16 bytes.  This appears to always be the case.\n. @mfischer-zd I'm sorry.  The snippet I gave was incorrect.  The correct configuration is:\nruby\nAWS.config(:s3_multipart_min_part_size => 20971520)\nI left off the s3_ prefix.\n. We have a build/release tool that generates the change log.  If I merge this pull request, it will get clobbered the next time the changelog is updated.  My guess is the change log is ordered based on when the commits are merged onto the master branch, instead of when the commit was generated, in some topical/feature branch.\n. It looks like the change log tool was disabled.  It was part of an automated release process.  The change log is simply the git commit history, so you can use a git client to see the changes.  Additionally, there are usually release notes here: http://aws.amazon.com/releasenotes/Ruby -- but these have lagged as well.  I will make sure these get updated in the future.  \nSince January most of the effort has been put towards the v2 Ruby SDK (aws-sdk-core).  This repo should only be receiving API updates and bug fixes.\n. Thanks for the fix!\n. AWS::Record::HashModel does not current support tables with composite keys.  When it defines the table object, it currently limits itself to working hash-key only tables.  This is a known limitation that we plan to remove when this is migrated to the v2 SDK.\n. The #as_tree methods crawls your bucket one virtual folder at a time based on a delimiter (defaults to '/').  If you intend to crawl all objects in a bucket, this is not the best approach.  S3 truncates requests that contain a delimiter to objects that share the given prefix and delimiter.\nIf you want to generate the given example output for an entire bucket, you should use the AWS::S3::Client#list_objects method.  This returns a response with a hash structure of data about the contents of the bucket.  Here is a simple example that aggregates information about all of the files:\n``` ruby\nfiles = {}\noptions = { bucket_name: 'my-bucket-name' }\ndef process_keys(resp, files)\n  resp[:contents].each do |obj|\n    files[obj[:key]] = {\n      \"name\" => File.basename(obj[:key]),\n      \"is_file\" => true,\n      \"path\" => obj[:key],\n      \"meta\" => {\n        \"last_modified\" => obj[:last_modified].utc.strftime('%Y-%m-%dT%H:%M:%S+00:00'),\n        \"content_type\" => obj[:content_type],\n        \"content_length\" => obj[:size].to_i,\n      }\n    }\n  end\nend\ns3 = AWS::S3::Client.new\nresp = s3.list_objects(options)\nprocess_keys(resp, files)\nyou must request the next page of results until you have all objects\nwhile resp[:truncated]\n  resp = s3.list_objects(options.merge(:marker => resp[:contents].last[:key]))\n  process_keys(resp, files)\nend\n```\nThis will make exactly 1 request per 1k files.  Given the resultant hash of files, you should be able to use File.dirname and File.basename to nest them in directories with counts.  Is this what you are looking for?\n. I was mistaken.  It appears that content type is only available in the response from a call to get the object or head the object.  It is not returned from list objects.\nI went ahead and took a stab at this.  Your example didn't clearly indicate if you expected folders nested under folders but that is what this generates.  You can see the sample code here:\nhttps://gist.github.com/trevorrowe/7663726\n. Thanks for reporting the issue.  I've created a patch and added some additional tests to ensure this will not regress.  This will go out with the next release.\n. Thanks for the fix!\n. Thanks for reporting this!\n. Version 1.29.1 has been released with a fix.\n. I will need to verify the cause, but it looks like we are sending the wrong, older, API version.  This feature requires the latest API version.\n. Version 1.29.1 has been released with this fix. Thanks!\n. Version 1.29.1 has been released with a fix.\n. I'm looking into this issue and #423.  It appears these related issues were both regressions introduced when addressing issue #419.\nThe core issue is there are two operations that return a list of stacks, DescribeStacks and ListStacks.  One is filterable, other other is not.  When the the filtering code was enabled, this caused a switch to the other API operation that also includes deleted methods.\nThe memoization issue is caused by the Stack resource class does not define what attributes can be memoized from a list stacks call.\nI suspect the fix will involve rolling back to using describe stacks by default and then using list stacks only when a filter is present.\nLet me take a closer look and I'll update when I have more information.\n. The issue was what I suspected and the fix was pretty straight forward.  I made the following changes:\n- Reverted back to using #describe_stacks by default, now only using the #list_stacks operation when a filter has been provided.\n- Added memoization support for #list_stacks, so no matter which operation returns the list of stacks, the attributes can be memoized.\n- Added an integration test that covers the base case without filter, to compliment the filtered example.\nI will try to get this fix released shortly.\n. Version 1.29.1 has been released with this fix.\n. This is a known issue an limitation.  This will be resolved in the v2 SDK.\n. We've added support for the 2013-09-09 API version for RDS.  Could you try this again to see if this removes the restriction?\n. I think @molekilla is suggesting that you must connect to the target region to initiate the copy snapshot.  If I want to copy a snapshot from us-east-1 to us-west-1, I would connect to us-west-1:\nruby\nrds = Aws::RDS.new(region:'us-west-1')\nrds.copy_db_snapshot(\n  source_db_snapshot_identifier: 'arn:aws:rds:us-east-1:1234567890:snapshot:SOURCE-SNAPSHOT-ID'\n  target_db_snapshot_identifier: 'TARGET-SNAPSHOT-ID'\n)\nThe full ARN should be requried for the source db snapshot identifier.  The ARN should not be required for the target, only the identifier which has the following constraints:\n- Cannot be null, empty, or blank\n- Must contain from 1 to 255 alphanumeric characters or hyphens\n- First character must be a letter\n- Cannot end with a hyphen or contain two consecutive hyphens\nCan you share a working example using the CLI?\n. That cli command maps directly to the example I gave.  I'm glad it is working for you.\n. Actually, it looks like the CLI may be doing some magic to set the destination region by extracting it from the ARN.  Either way, I'm still glad it works for you.\n. Could your try running the above code again with http wire trace enabled:\nruby\nAWS.route_53(http_wire_trace: true).hosted_zones.each do |zone| \n  puts zone.delegation_set\nend\nThat should provide enough information to debug this issue.  Thanks.\n. Thanks, I was able to track down the issue.  The delegation set was returned in the client response at the root of the response, not inside the response[:hosted_zone] element.  That, and it is not returned from the list hosted zone calls, only the calls to create and get.\n. Unfortunately this will not work.  The higher level abstractions require the older API version.  We are looking at making some major changes here for the v2 Ruby SDK.  AWS::Record will be extracted to its own project, updated to use the latest API version, and get support for local and global secondary indexes, etc.  The bundled v1 AWS::Record will not be updated though.  \nWatch the Ruby SDK blog (http://ruby.awsblog.com/) for more information on what we are working on and when we start on the AWS::Record updates.\n. You are correct.  It appears the service signing name for version 4 is set incorrectly.  It is set as emr but should be elasticmapreduce.  As a quick work-around you can run the following:\nruby\nAWS::EMR::Client.send(:signature_version,:Version4, 'elasticmapreduce')\nI'll get a patch release out shortly to fix this.\n. Version 1.31.1 is out with the fix.\n. Thanks for reporting the issue.\n. Thanks for the doc fix!\n. I added to this a few tests and added back support for chaining the #with_status calls, see https://github.com/aws/aws-sdk-ruby/commit/d621f11ead605668a10999c4c85322a0df536c5f.\n. Thanks for the submission!\n. You say it does not seem to work.  How do you know it is working or not?  Can you also provide a code example of what you are doing?\n. Without a HTTP wirelog or reproduction case there is nothing we can do to resolve this issue.  If you can provide more information, then we can look into this.\n. Unfortunately, these appear to be limitations of the Auto Scaling API.  I went looking at the API reference to see if there were missing options that we could add to CreateLaunchConfiguration.  \nThe create launch configuration API ref:\nhttp://docs.aws.amazon.com/AutoScaling/latest/APIReference/API_CreateLaunchConfiguration.html\nLinks to the supported options for the block device mapping:\nhttp://docs.aws.amazon.com/AutoScaling/latest/APIReference/API_BlockDeviceMapping.html\n. I'm closing this issue as there does not appear to be any changes we could/should making in the SDK.  I would recommend making a feature request on the Amazon EC2 forums: https://forums.aws.amazon.com/forum.jspa?forumID=30&start=0.\n. Sorry for the slow response, thanks for the contribution!\n. Thanks for the contribution!\n. A quick question, in the code the following line shows that the lib directory has been removed from the FILE path.\n```\nFILE = jar:file:/opt/company/library.jar!/aws/core.rb\nFile.dirname(FILE) = jar:file:/opt/company/library.jar!/aws\n```\nIs this standard and is there a reason lib has been removed from the path when building the jar?\n. Have you tried passing :force_path_style => true to #url_for?\n. Also, you can pass :secure => false if you are okay with a vanilla HTTP request,\n. Thanks!\n. Thanks!\n. The error you gave indicated that on line 108, #name was being called on a StackResourceCollection.  The #stack should be a Stack, not a StackResourceCollection.  Can you give me a bit more of the context for your code that causes this error to happen?  I'd like to get down to the root cause.\n. Thank you for the details reproduction case.  This helped me to track down and resolve the issue.  I just pushed the fix that should be released shortly.\n. Hrmm..  Simply making the argument optional will prevent the error, but it will cause the #exists? method to always return true.  The get_resource method normally calls a get method.  There is no get signing certificate API operation.  The list_signing_certificates operation does not raise the NoSuchEntity error required to make exist work.\nI suspect we will need to provide a custom #exists? method that calls list_signing_certificates, paging if needed looking for a certificate with the same id.\n. We are not currently doing any new feature work on the v1 Ruby SDK.  We are accepting pull requests if you would like to submit something.\n. Closing for now, but do feel free to submit something.\n. Closing this as this functionality was added in 93c98359904917c19e0437d4ec67ce82adaf8900. \n. @ tmornini Looking at https://github.com/aws/aws-sdk-ruby/blob/master/lib/aws/core/lazy_error_classes.rb#L71, I think you are correct.  The mutex prevents multiple threads from defining the same constants, but inside the mutex it needs to ensure the const it wants to set has not been set while it was blocked.\nThe eager_autoload! does nothing to help this situation as those error classes are constructed at runtime.  The complete set of possible error classes is not known, so we can no pre-define them.\nThis should be a pretty simple fix.\n. The commit above should resolve the issue for you.  Please let me know if you keep seeing the warning with the above fix.\n. Its curious that it is generating a signature error.  Thank you for all of the details.  I'll see what I can do about reproducing on my end.\n. Sorry for the slow response.  I spent quite a bit of time reproducing and trying to resolve the issue from my end.  Here are some of my findings:\n- I am unable to reproduce this using MRI 2.0.0p353\n- I can reproduce this issue locally \"fairly\" consistently using rubinius 2.2.1\n- The service responds with the error \"The request signature we calculated does not match the signature you provided.\"\n  - The error contains a string to sign and canonical request as computed by the service\n  - I have patched my SDK to append the string to sign and canonical request to the error\n  - These values always match up 100% with those computed by the service\nThe authorization header is contains a signature that is computed roughly like so:\nruby\nOpenSSL::HMAC.hexdigest( OpenSSL::Digest.new('sha256'), derived_key, string_to_sign)\nI have verified I am working with the correct string to sign.  That leaves the possible sources of discrepancy to:\n- the derived key\n- the computed digest\nThe derived key is is calculated like so:\nruby\ndef derive_key(datetime)\n  k_secret = credentials.secret_access_key\n  k_date = hmac(\"AWS4\" + k_secret, datetime[0,8])\n  k_region = hmac(k_date, region)\n  k_service = hmac(k_region, service_name)\n  k_credentials = hmac(k_service, 'aws4_request')\nend\nGiven the intermittent failure modes, this leeds me to strongly suspect a thread-safety issue.  The v4 signer that computes the signature does not share state between calls to sign (a new digest is always created), except the read only credentials.  \nIs it possible that the OpenSSL::HMAC or OpenSSL::Digest modules are not thread safe?  I'm open to suggestions.\n. @YorickPeterse Yes, we should be able to test and isolate this further.  I simply ran out of time yesterday.  \nSome more background on the error message:\nThe two strings the service echos back in the error message are the:\n- canonical request\n- string to sign.  \nThe canonical request is a string compiled from the request uri, headers and a full sha256 checksum of the http request body.\nAfter generating the canonical request, you generate a \"string to sign\" which is comprised of a fixed string prefix, the current time, the credential scope (date/region/service name/fixed suffix) and finally a hexdigest of the canonical request string. \nIn my local test script (I will try to share this later), I am comparing the canonical request and string to sign that I generate with those returned by the service.  They match perfectly in every error.  Given the canonical request contains a sha256 checksum of the body, and the service checksum matches ours, I am positive the request is not getting corrupted over the wire.  \nThis points to a bad signature.  The final signature is created by computing a hex digest of the string to sign using a derived key (the derived key is your secret access key signed recursively with each portion of the credential scope). \nThe service validates the signature by computing and comparing signatures.  In the case of failure, it echos back the computed fingerprints (canonical request and string to sign).  It does not echo back the computed signature, but that is what fails to match.  This is why I feel like the error is happening while generating the signature.\n@JoshMcKin Not unhelpful, but I don't think that is the cause.  In my test example, I am creating a single sqs client (with a single credentials object).  That object is used to get my queue by name outside the concurrent sections of code.  This should guarantee my static credentials are ready to read/reuse.  You are correct, the non-static credential providers use a mutex to allow them to update.\nI won't much have time to look at this today, I'm heading out of town this afternoon.  I'll try to pick this back up on Monday though.\n. @YorickPeterse I have not been able to reproduce this error locally single switching over to OpenSSL::Digest.  Can you confirm?\n. The chunk signed stream class you linked to is currently not used by the SDK.  Each client constructs a single signer and reuses that across requests.  However, I don't believe any of the signer classes cache digest objects (except the un-used chunk signer).\nIt could be fairly easy to test.  The base client class defines a #sign_request method.  This method memoizes the request signer.  If this memoization were removed, the #sign_request method would construct a new signer for every request.\n#sign_request is defined here:\nhttps://github.com/aws/aws-sdk-ruby/blob/master/lib/aws/core/client.rb#L701\n. Each service can choose a different signature version / signer.  For this reason, there is a macro .signature_version that defines the #sign_request method.\nAs a quick monkey patch, the following should work:\nruby\nclass AWS::SQS::Client\n  def sign_request(req)\n    args = [credential_provider, 'sqs', req.region]\n    signer = AWS::Core::Signers::Version4.new(*args)\n    signer.sign_request(req)\n    req\n  end\nend\nThe code this is replacing looks like so:\nruby\ndef signature_version(version, service_signing_name = nil)\n  define_method(:sign_request) do |req|\n    @signer ||= begin\n      signer_class = AWS::Core::Signers.const_get(version)\n      signer_args = (version == :Version4) ?\n        [credential_provider, service_signing_name, req.region] :\n        [credential_provider]\n      signer_class.new(*signer_args)\n    end\n    @signer.sign_request(req)\n    req\n  end\nend\nNotice, the default implantation caches the signer to the client instance. The monkey patched version creates a new signer for each request.\n. Thats interesting. \nGiven the global mutex around the signing code seems to resolve the problem, that implies one of a few possible causes:\n- The credential_provider object shared by each signer\n- Code within the signer, e.g. OpenSSL\nThe v4 signer has 3 instance variables, which are the credential, the service name string and the region string.  What type of credentials object are you testing with? Can you dump out the credentials class name?\n. Just to rule out the credential provider chain, can you do the following:\n``` ruby\nuse your own account credentials here\ncredentials = { access_key_id:'akid', secret_access_key:'secret' }\ncredentials = AWS::Core::CredentialProviders::StaticProvider.new(credentials)\nAWS.config(credential_provider: credentials)\n```\nThen re-run your tests without the global mutex around the #sign_request method body.\n. I do not know any other solution. The signer is building the correct canonicalized string and string to sign. The only failure appears to be when it uses OpenSSL to generate the digest.\nIt is possible the SDK could manage a lock, but it would still need to be a global lock.  Im open to other suggestions.\n. It is not possible to change the client construction behavior without making breaking changes. The v2 Ruby SDK no longer shares client instances.\nThat said, the client objects should be thread safe. See my comment above: https://github.com/aws/aws-sdk-ruby/issues/455#issuecomment-41341972\nThe actual bug does not appear to be caused by shared state. the failing call looks like this:\nruby\nOpenSSL::HMAC.hexdigest( OpenSSL::Digest.new('sha256'), derived_key, string_to_sign)\nHave we ruled out that the hexdigest method is not thread safe?\n. I did not implement eager_auto! load in v2 as my understanding is that autoload in now thread-safe. We should explore this more.\nAs for the signature errors, the only fix I understand that we've tried that resolves the issue is to put a global mutex around the OpenSSL digest methods. This seems like it shouldn't be necessary. Thoughts?\n. @YorickPeterse In production, are you using static credentials, or are you using refreshing credentials, such as an IAM instance profile, assume role credentials, STS session credentials, etc?\n. The latest release (just happened minutes ago) contained a fix for this, but only in the context of RunInstances.  Thanks for tracking all the rest of these down.\n. You are correct.  The SDK currently only supports the configuration APIs, but not the document or search APIs.  This is something we are evaluating adding core support for in the v2 Ruby SDK.\nThere are a few community contributed gems that support the document and search APIs that you may want to check out:\nhttps://github.com/spokesoftware/aws_cloud_search\nhttps://github.com/duwanis/asari\n. Thanks!\n. Sorry for the very slow response.  We are not currently doing feature work on the v1 Ruby SDK.  We do accept pull requests, but we are currently focused on the v2 SDK (https://github.com/aws/aws-sdk-core-ruby).  I do think this could be a helpful feature to add.\n. Do you have any additional information to share about the failed request?  Is there anything unique about the bucket name, and could you share that?  I am unable to currently reproduce the issue.\n. I miss-read and failed to notice that it was the #exists? method on the object, not bucket class raising the error.  I should ask, is there anything unique about the object key in the failed examples?  Also, do you know if it fails consistently on the same keys?\n. We do not currently have any other related issues reported about this, but I agree the failures are discouraging.  It does not suprise me that a call to #content_length also fails.  Both #content_length and #exists? call the exact same AWS API, which is HEAD object.  If one fails, the other is sure to fail.\nSome questions that might help track down the issue:\n- Are you using AWS account credentials, or credentials from an IAM user or policy?\n- Do the requests fail consistently on the same keys, or is this intermittent?  If you added a simple retry to the rescue block, will the follow request succeed?\n- Is there anything special about the object keys that fail, like multibyte characters?  This should not be an issue, but its worth investigating.\nIt may also be helpful to see the HTTP wire trace from a failed request.  You can enable http wire tracing like so:\nruby\ns3 = Aws::S3.new(http_wire_trace:true)\nYou can pass an optional Logger object as :logger to specify where to send the wire trace.  It defaults to STDOUT.  Also, this logs everything about your http requests to S3, so the log may be large.\n. I pushed a commit a week ago related to the AWS::Core::CredentialProviders::EC2Provider related to how it detected expired credentials and rotated them, see: https://github.com/aws/aws-sdk-ruby/commit/a1c0197977560bc72d2eb0a876daa63648b2d985.\nCould you try running your code against the master branch, or perhaps apply this monkey patch:\nruby\nrequire 'aws-sdk'\nclass AWS::Core::CredentialProviders::EC2Provider\n  def credentials\n    if @credentials_expiration && @credentials_expiration.utc <= (Time.now.utc + (15 * 60))\n      refresh\n    end\n    super\n  end\nend\n. BTW, I just pushed v1.34.0 of the Ruby SDK with the EC2Provider fix.  I suspect this version will resolve your issue, especially considering the 5 hour window lines up with the standard expiration of EC2 IAM profile credentials.\n. @s2t2 Its not clear to me what you mean.  Using v2.0.41 I tried the following:\n``` ruby\nbucket I own, returns 200\nAws::S3::Bucket.new('aws-sdk').exists? #=> true\nbucket I do not own, returns 403, but still exists\nAws::S3::Bucket.new('demo').exists? #=> true\nnon-existent bucket, returns 404, does not exist\nAws::S3::Bucket.new('asdfasdfasdfasdfasdfa1111').exists? #=> false\n```\nCan you give me a code example of what fails?\n. @s2t2 Sorry, I was running my examples from pry inside the Aws module with a globally configured region.\nThe Http301Error indicates that you have received a permanent redirect. Would you open this as a new issue. It looks like there are some bucket dns redirect issues to resolve.\n. I think adopting this would be a good idea with the v2 Ruby SDK (https://github.com/aws/aws-sdk-core-ruby), but unfortunately, this breaks backwards compatibility with Ruby 1.8.7, which aws-sdk supports.\nI would not be opposed to a patch that adds a utility method for generating UUIDs, and uses SecureRandom.uuid when possible, falling back on UUIDTools when not, i.e. with Ruby 1.8.7.  Unfortunately, this does not grant us the ability to remove the gem dependency.\nThoughts?\n. Thanks for the contribution!\n. I pushed a commit last week that may resolve this issue.  It is on the master branch, but I have not cut a new release of the gem yet.  \nhttps://github.com/aws/aws-sdk-ruby/commit/a1c0197977560bc72d2eb0a876daa63648b2d985\n. Thanks!\n. Thanks!\n. Sorry for failing to respond to this issue sooner.  Has been continued to be an issue?  I agree it is very odd that you were able to successfully copy from an object to a different object, but then were unable to HEAD the source object.\n403 errors on a HEAD request can be tricky to debug.  The same operation called with GET would normally return an XML description of the error with the 403.  S3 does not return the xml error in response to HEAD to be compatible with the HTTP spec which says HEAD should not return a body.\n. Closing this stale issue, without further information I am unable to help debug the issue. Please re-open if you are able to reproduce with extra information.\n. It appear this feature was added a while ago by @awood45 in https://github.com/aws/aws-sdk-ruby/commit/8dd20ce4d2bc1f6bd22f9120fd85db641a706a2d.\n. The aws-sdk gem does not currently support the CloudSearch document and search APIs.  We are currently evaluating adding support.  I can not comment though on any timeline or if this will be included in the v1 Ruby SDK.\n. @quentindemetz V2 of the Ruby SDK supports #search, #suggest, and #upload_documents.  You can check it out here:\nrepo:\nhttps://github.com/aws/aws-sdk-core-ruby\ndocs:\nhttp://docs.aws.amazon.com/sdkforruby/api/Aws/CloudSearchDomain/Client.html\ngem:\naws-sdk-core\n. It appears this issue was dropped.  Have you been able to resolve this issue?  It appears the wire trace failed because Net::HTTP did not work with the Rails.logger.\n. Closing due to inactivity.  Please feel free to re-open if you can provide additional information.  We are unable to reproduce the issue ourselves.\n. Sorry for the very slow response.  I've added a patch for the regression that adds a #service attribute to the HTTP request which is now populated when the client builds a new request.\n. I am not 100% certain on this, but I don't believe the S3 policy language allows you to specify a maximum content-length for a put object.  I do know it is possible to limit the maximum size when using pre-signed posts, as you can configure a content-length-range, with a min and max size. \nYou may want to post your question to the Amazon S3 forums: https://forums.aws.amazon.com/forum.jspa?forumID=24\n. Sorry for the slow response.  Can you run your test again with HTTP wire logging enabled on your S3 object?\nruby\ns3 = AWS::S3.new(http_wire_trace:true)\nThis will send to $stdout (or you can also pass an instance of Logger as :logger => ...) the full http wire log.  This should give enough information to help debug why the error message might be blank.\n. Closing due to inactivity.  Please feel free to re-open if you can reproduce the error and provide additional information.\n. Thanks!\n. Sorry for the slow response.  I updated the documentation to match the intended behavior (which is to update the provisioned throughput).\n. Can you provide any additional information about the code that causes this error?  A test case that can reproduce the error would be great, but even an extract of your code where the error is happening.  Also, are you running with threads?\n. Is this code being run in a multi-threaded environment?  Also, what version of ruby are you using?  The aws-sdk gem uses autoload which is not thread safe in all versions of Ruby.  \nThere is a AWS.eager_autoload! method which will forcefully load every ruby file in the gem.  Its not fast, as there are a lot of files, but this usually resolves the issue.  Additionally, upgrading to Ruby 2.0+ or JRuby will resolve autoload thread safety issues.\n. Sorry for the very slow response.  Yes, it is a one-time cost at process start.  Have you applied this change, and has it resolved your issue?\n. Thats difficult to say.  I'll go ahead and close this issue then.  Please re-open if you continue to encounter the problem.\n. Thank you for the suggestions.  We are currently evaluating adding support for profiles to the Ruby v2 SDK in the same manor as supported by the CLI.  However, it is not likely this will be back-ported to the v1 Ruby SDK.  That said, if somebody is interested in doing the work, we do accept pull requests.\n. Sorry for the radio silence.  I have taken a quick scan, but I haven't reviewed it fully.  I'll try to get to that this afternoon.\n. I think the general idea is sound.  I agree that errors should not be used as flow control statements, and I understand the performance concerns.\nThat said, this patch changes the public interface of the Provider module such that it breaks anyone that replaces the default provider with any other provider.  For example, the SDK ships with an ENV credential provider that you can use to specify the ENV key prefix to use when searching for credentials:\nruby\nAWS.config(:credential_provider => AWS::Core::CredentialProviders::ENVProvider('MY_APP_'))\nThis would affect any class consuming Provider:\nruby\nclass CustomCredentialProvider\n  include AWS::Core::CredentialProviders::Provider\n  # ...\nend\nAWS.config(:credential_provider => CustomCredentialProvider.new)\nMight I suggest a small change to the pull request that would maintain backwards compatibility of the Provider interface and provide the same performance characteristics?\n``` ruby\nin the Provider module\ndef credentials\n  if set?\n    @cached_credentials.dup\n  else\n    raise Errors::MissingCredentialsError\n  end\nend\ndef set?\n  @cached_credentials ||= get_credentials\n  @cached_credentials[:access_key_id] &&\n    @cached_credentials[:secret_access_key]\nend\nin the default provider class\ndef credentials\n  providers.each do |provider|\n    if provider.set?\n      return provider.credentials\n    end\n  end\n  raise Errors::MissingCredentialsError\nend\ndef set?\n  providers.any(&:set?)\nend\n```\nThis isn't tested, but a solution similar to this should achieve the goal of not using errors as flow control and maintain backwards compatibility.  Thoughts?\n. Sorry, forgot to mention @kierarad and @wpc.\n. Thanks for the awesome contribution!  This will go out with our next release.\n. I am unable to directly reproduce this issue.  Here is my simple case:\n``` ruby\nobj = AWS::S3.new.buckets['aws-sdk'].objects['my key with spaces'].write(\"Hello\")\nobj.key\n=> \"my key with spaces\"\nputs obj.read\n=> \"Hello\"\nurl = obj.url_for(:read).to_s\n=> \"https://aws-sdk.s3.amazonaws.com/my%20key%20with%20spaces?AWSAccessKeyId=...&Expires=1396481909&Signature=...\"\n```\nPasting the resultant URL into a browser works just fine and I get the expected \"Hello\" string.  Can you produce a simple case that reproduce the error?\n. Thanks!\n. Thanks!\n. This looks like an issue in rubygems, not the aws-sdk.  Have you recently updated your install of rubygems?  Could you run gem env and share the output?\n. Sorry, this fell off the radar.  Has this continued to be an issue, or where you able to resolve it?\n. Closing this as a non-issue with the SDK.\n. I think the primary problem is that that enumerating security groups from a VPC correctly filters, but calling #security_groups[id] looses the context of the VPC.  Is that correct?\nThis is a current limitation of the SDK interface.  Anytime you pass an identifier to a #[] method, it is assumed that the resource can correctly be retrieved by that id directly, which is can.  That however fails to respect \"how\" you got to the resource, e.g. through a VPC.  \nA work around for now to determine if a security group exists within a VPC would be to filter like so:\nruby\nexists = !!ec2.security_groups.filter('vpc-id', vpc.id).filter('group-id', 'sg-2').first\nWe are currently working on resource interfaces in the v2 Ruby SDK and this should be much improved.  I don't know if there is much we can change without making breaking changes in the v1 SDK for now.\n. I spent a few minutes looking at this and I was unable to reproduce the error.  Here is what I did:\n``` ruby\ncertificate = AWS::IAM.new().server_certificates[\"RubyCert-1332393132.930061\"]\ncertificate.arn\nthe call to arn triggered a call to load attributes, the following was sent to my logger\n[AWS Core 200 0.615711 0 retries] get_server_certificate(:server_certificate_name=>\"RubyCert-1332393132.930061\")\n```\nThe AWS::IAM::ServerCertificate resource does specify the resource identifiers as such:\nruby\ndef resource_identifiers\n  [[:server_certificate_name, name]]\nend\nThat is what is responsible for sending the :name attribute to the client#get_server_certificate method.  \nCan you provide a simple example that reproduces the error?  Also, please let me know what version of the SDK you are using and what version of Ruby you are using.  Anything that helps me produce the error would be appreciated.\n. I'm glad you were able to resolve this.  I am adding a note that in the upcoming resources abstraction for the v2 SDK, that we will treat symbols and strings indifferently, across the board.  As for the option validation, the v2 SDK does what you have proposed.  It details what it expect and what it got, and gives the full context.\n. I was able to take a look at this and reproduce the issue.\nThe change to the ConnectionPool appears is not the bug, but removing the previous behavior of treating connection pools with similar options as singletons has revealed this bug.\nAs a quick work around, can you try the following:\n``` ruby\nrequire 'aws-sdk'\nperform what ever configuration you need\nAWS.config(...)\nthen call this method BEFORE constructing a service object\nAWS.config.http_handler\nnow run the process as normal:\nddb = AWS::DynamoDB.new\n...\n```\nFollowing the steps above should work around the bug.  If it does not, please let me know. \n. @mikestanley Thanks for your patience on the issue.  I've reverted the change in 1.36.0 that caused this issue.  We will look for another solution that fix was intended for.\n. Sorry for the slow response.  \nI did some digging into this, and you are correct.  The Ruby SDK should treat empty or self closing xml elements for strings as empty strings and not nil.  That said, making this change would be a breaking change.\nI would be interested in making this behavior change in the v2 Ruby SDK.  If you would like to submit a GitHub issue to https://github.com/aws/aws-sdk-core-ruby/issues it is a simple one line fix.  This is still in a release candidate period, so making that change now would be beneficial.\nAdditionally, you can install the v2 SDK in the same application as the v1 SDK (different gems and namespaces).  This would allow you to adopt the change right away.  The v2 SDK should be able to directly consume the responses from CloudFront and pass them back in as request params.  If not, we can fix these directly as bugs.\n. It appears that the documentation for AWS::CloudFront::Client is being built from the oldest API version supported, instead of the default API version.  Until this is corrected, you can find the correct documentation here:\nLatest version:\nhttp://docs.aws.amazon.com/AWSRubySDK/latest/AWS/CloudFront/Client/V20140131.html\n. The updated docs will be deployed with the next release.  Thanks!\n. Sorry for the slow response.  I just made a minor modification to eager_autoload! so that it returns all of the nested modules/classes it visits.\nIn a quick test, it does appear to be correctly autoloading AWS::SQS::Queue using the master branch:\n``` ruby\nAWS.eager_autoload!.map(&:name).grep /queue/i\nreturns the following:\n[\n \"AWS::SQS::Queue\",\n \"AWS::SQS::Queue::SentMessage\",\n \"AWS::SQS::Queue::PolicyProxy\",\n \"AWS::SQS::QueueCollection\"\n]\n```\nThe \"autoload\" statement for SQS (and every service) is at the very bottom of lib/aws/core.rb:\nhttps://github.com/aws/aws-sdk-ruby/blob/master/lib/aws/core.rb#L697-L700\nTo answer your question of \"how should we use eager_autoload!\" -- it should be invoked before entering any concurrent sections of your application.  Typically, this would be after requiring the aws-sdk gem but before using any of its classes.  \nIn theory you should not need to use eager_autoload! as JRuby 1.7+ have threadsafe implementations of autoload (https://github.com/jruby/jruby/wiki/Concurrency-in-jruby).  You may be able to work around this by accessing the Queue constant:\nruby\nAWS.eager_autoload!\nAWS::SQS::Queue\nAt this point, I'm not sure what is causing the issue.  You reported this issue a few weeks back, have you found a workaround?  Any additional information may help track down the root issue.\n. Sorry for the slow response.  The :version_id option to S3::Client::V20060301#head_object` is not marked as required in documentation.  I was also able to make a request using that method without the option:\n``` ruby\ns3.client.head_object(:bucket_name=>\"aws-sdk\",:key=>\"foo\")\n=> response ...\n```\nAre you getting some error back from the service or from the client when you omit :version_id?\n. Merged and released.\n. I just ran the following example:\nruby\nredshift = AWS::Redshift::Client.new(region:'us-west-2')\nredshift.enable_snapshot_copy(cluster_identifier: 'abc', destination_region: 'us-east-1')\nIt hit the service and returned the expected error,\u00a0AWS::Redshift::Errors::ClusterNotFound.  I ran this against v1.36.2 and v1.37.0.  What version of the SDK are you using?\n. I went looking at the SNS API documentation.  It appears the keys/values in your JSON message are not formed according to what SNS requires.\nJSON-specific constraints (I've removed the non-related ones):\n- Keys in the JSON object that correspond to supported transport protocols must have simple JSON string values.\n- Non-string values will cause the key to be ignored.\n- Keys that do not correspond to supported transport protocols are ignored.\nFrom the SNS API documentation:\nhttp://docs.aws.amazon.com/sns/latest/api/API_Publish.html\n. Thanks for your patience and the excellent bug report.  The commit above uses a single tag filter to resolve the issue.  Thanks!\n. It looks like the inline yard doc attributes were not making it into the API docs.  I moved them to the class and that seems to fix the issue.  The API docs will be updated with the next release.\nThanks!\n. This last commit and doc push seems to fixed the missing doc strings.  Thanks!\n. We do not currently have a client for Workspaces.  We are looking into adding a support, but I can not comment on a timeline.\n. I should clarify.  We do plan to support Workspaces once their API is public.\n. Thanks for your patience.  I added :static => true to the image_id and enabled memoization on the collection#each.\n. Using the AWS::EC2::Client#run_instances method, you can specify the :placement option directly.  The Client classes support the full API directly.  \nAdditionally, the InstanceCollection#run method accepts the :placement option as well.  You can pass any of the options that Client#run_instances accepts, and a few others to #run.\n. Closing as #504 resolves this issue.  Thanks!\n. Thanks for the contribution!\n. Thanks!\n. Can you provide a full stack trace?  It would also be helpful to see a sample of code that generates the error.  I suspect it is getting raised while you are accessing the response data.  Is this correct?\n. Your stack trace indicates this is happening in when testing (using factory_girl).  Is this also happening under actual usage?\n. Every response object from the SDK has a #data method that returns the response data as a vanilla hash.  If you call a method against the response object (i in your example) it proxies that method to the #data object, wrapping it in a Core::Data object.  My strong suspicion is that factory girl and Core::Data object are not playing together nicely.  One workaround would be to use the #data hash directly.\nThe simplest workaround may be to access the response.data hash directly, e.g.\nruby\ni = AWS.rds.instances.client.create_db_instance(db_name: options[:name], ...)\ni.data #=> returns a vanilla hash\nAnother option would be to install the aws-sdk-core gem.  Its part of the v2 Ruby SDK and can be installed in the same process as aws-sdk.  It resolves the issue by using Ruby Struct objects for response data.  This gives both hash-style access and method style access.  This structure should be api compatible with the response data methods returned by Core::Data but without the \"magic\".\nruby\nrequire 'aws-sdk-core`\ni = Aws::RDS.new.create_db_instance(db_name: options[:name], ...)\ni.data #=> returns a ruby structure\n. No plans to change the behavior in the v1 SDK at this time. The v2 SDK has a new implementation for stubbing responses that does not rely on a Core::Data pattern that should play nicely with factory girl.\n. You can specify the part of the S3 object to read currently by specifying a range:\n``` ruby\ns3.buckets['bucket-name'].objects['read'].read(range:0..1)\n=> reads first 2 bytes\ns3.buckets['bucket-name'].objects['read'].read(range:1..2)\n=> reads 2nd and 3rd bytes\n```\nI realize this has a different signature from IO#read.  Providing full compatibility with IO#read would imply keeping a marker so subsequent calls to #read would continue where it left off last time.  Also, IO#read accepts an optional second argument that is a target string buffer to replace when present.  \nI'm going to mark this issue as closed, as the current method signature is not likely to change from a hash of options.  Let me know if the range option is not sufficient.\n. Can you provide an example snippet of code that reproduces this error and a stack trace?\n. Thanks for the bug report.  The commit above should resolve this issue.  It should go out with the next release.\n. @troy I've left this issue open for quite a while now. You are correct about the limitations in the SDK for dealing with dotted bucket names and the classic S3 endpoint (s3.amazonaws.com). The SDK is unable to use DNS to resolve to the proper region because it cannot verify the peer certificate for buckets that contain dots. This makes the Amazon S3 error unhelpful, as it is sending us back to the domain name we are unable to verify. The only solutions I am aware of currently are:\n- Disable SSL (not recommended)\n- Make a get bucket location request to determine the proper region to send the request to\n- Make a request against the :s3_endpoint of s3-external-1.amazonaws.com. This is one of the public endpoints for the us-east-1 region.  Unlike other regions, this will return a temporary redirect instead of a permanent. This allows you to extract the actual region from the location header.\nI've used the 3rd strategy in the V2 Ruby SDK to help deal with the new sigv4 only region eu-central-1. In order to correct marshall requests to that endpoint, you must know the region name ahead of time. I use s3-external-1.amazonaws.com to determine the region as mentioned above.\nIt is not a pretty work-around, but its the only thing that I have found that works.\nI'm going to close this issue for now. I currently have no plans to modify the behvior for the v1 SDK at this time, but will consider adding this to the v2 AWS SDK for Ruby (https://github.com/aws/aws-sdk-core-ruby).\n. S3 requires the ContentType to be signed when sent.  The pre-signed url generator does not sign this unless you specify the content-type using the :content_type option.  My suspicion is @awood45 is correct and S3 is adding the \"ContentType\" header which it then expects to be part of the signature.\nIf you could give the full error message we may be able to debug this issue and confirm is this is the issue.\n. Closing stale issue. This does not appear to be an SDK issue.\n. I did some digging on this.  The where methods are only supported for AWS::Record::Model which is backed by SimpleDB, and not DynamoDB.  There appears to be a documentation issue where the methods documented on the FinderMethods module are being mixed into AWS::Record::HashModel.  \nIt is difficult to say how to achieve this.  If the \"name\" attribute is the hash key of the table, you can retrieve records using the Query API call.  Also, your table might have secondary indexes on that attribute.  If not, you have to scan, which is potentially very slow and costly on throughput.\nCurrently we are not doing any additional feature work on the v1 Ruby SDK.  We are focused on getting v2 out the door.  We plan to revisit AWS::Record in the future to address some of these missing features.\n. Thanks!\n. @lsegal is correct.  All of the AWS SDKs and Tools are standardizing around AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY.  That said, I realize there are legacy tools to consider.  I would be open to merging the pull request if you could remove the documentation example changes and move it down in the provider chain after ENVProvider.new('AWS').\n. Thanks!\n. I was looking at this today.  What do you think of the following:\n``` ruby\nprovider = AWS::Core::CredentialProviders::AssumeRoleProvider.new(\n  sts: AWS::STS.new(access_key_id:'AKID', secret_access_key:'SECRET'),\n  # assume role options:\n  role_arn: \"linked::account::arn\",\n  role_session_name: \"session-name\"\n)\nec2 = AWS::EC2.new(credential_provider:provider)\n```\nThe general idea is you need an STS client to make the assume role call.  You also need the assume role options to refresh the credentials with when they expire.  It would be fairly simple to implement this patterned after the EC2Provider (that consume instance profile credentials while auto-refreshing).\nThoughts?\n. @jud Yes, the entire :sts option could be optional and it would default to your current credentials (ENV, EC2 profile, etc).\n. First attempt.  Anyone want to take this for a spin?\n``` ruby\nmodule AWS\n  module Core\n    module CredentialProviders\n  # An auto-refreshing credential provider that works by assuming\n  # a role via {AWS::STS#assume_role}.\n  #\n  #    provider = AWS::Core::CredentialProviders::AssumeRoleProvider.new(\n  #      sts: AWS::STS.new(access_key_id:'AKID', secret_access_key:'SECRET'),\n  #      # assume role options:\n  #      role_arn: \"linked::account::arn\",\n  #      role_session_name: \"session-name\"\n  #    )\n  #\n  #    ec2 = AWS::EC2.new(credential_provider:provider)\n  #\n  # If you omit the `:sts` option, a new {STS} service object will be\n  # constructed and it will use the default credential provider\n  # from {Aws.config}.\n  #\n  class AssumeRoleProvider\n\n    include Provider\n\n    # @option options [AWS::STS] :sts (STS.new) An instance of {AWS::STS}. \n    #   This is used to make the API call to assume role.\n    # @option options [required, String] :role_arn\n    # @option options [required, String] :role_session_name\n    # @option options [String] :policy\n    # @option options [Integer] :duration_seconds\n    # @option options [String] :external_id\n    def initialize(options = {})\n      @options = options.dup\n      @sts = @options.delete(:sts) || STS.new\n    end\n\n    def credentials\n      refresh if near_expiration?\n      super\n    end\n\n    private\n\n    def near_expiration?\n      @expiration && @expiration.utc <= Time.now.utc + 5 * 60\n    end\n\n    def get_credentials\n      role = @sts.assume_role(@options)\n      @expiration = role[:credentials][:expiration]\n      role[:credentials]\n    end\n\n  end\nend\n\nend\nend\n```\n. This was added in 107a152bcb8b1eac20cb1ab20f05d9ccab1a6f76 and should be part of the current release.\n. The DynamoDB mid-level abstraction you are using is locked to the older API version.  You can access all of the features of DynamoDB using the client:\nddb = AWS::DynamoDB::Client.new(api_version: '2012-08-10')\nThe documentation for the 2012 API version client is here:\nhttp://docs.aws.amazon.com/AWSRubySDK/latest/AWS/DynamoDB/Client/V20120810.html\n. Your stack trace indicates this error is being retuned from SNS and is not a client-side validation error.  Given the message indicates the topic (or rather the topic arn) is invalid, my best guess is that you are connecting to one region and attmpeing to confirm a subscription to a topic in a different region.\nYou can verify this with the following:\n```\nmy guess at the cause of the error\nc.config.sns_endpoint\n=> \"sns.us-east-1.amazonaws.com\"\n```\nIf the endpoint returned has a different region than the topic arn that is likely the cause.  The arn above indicates you are using us-west-1.  The Ruby v1 SDK defaults to us-east-1.  You would need to create the client with the appropriate region (you may need to extract this region from the arn):\nc = AWS::SNS::Client.new(region:'us-west-1')\nLet me know if this helps or not.\n. It appears that the AWS::EC2::ImageCollection does not current accept the appropriate options for filtering images by id.  I've got an incoming patch that will address this.  Until this is fixed, you can work around this by using AWS::EC2::Client#describe_images.  This method accepts all of the options provided by the api, including :image_ids => [...].  However, it returns simply data, not AWS::EC2::Image objects.\nAlternatively, might I suggest taking a look at the aws-sdk-core (github.com/aws/aws-sdk-core-ruby).  This provides full api access to the API.  Higher level abstractions are incoming, but this version supports paging from the client, structured client responses and more.  Both gems can be used together in the same processes (core uses a different module namespace).\n`` ruby\nrequire 'aws-sdk-core\nec2 = Aws::EC2.new\nresp = ec2.describe_images(image_ids:['...','...'])\nputs resp.images.map(&:image_id)\n```\n. Oh, I missed something, you can filter images using the filter interface.\nruby\nec2 = AWS::EC2.new\nec2.images.filter('image-id', 'ami-fb8e9292').each do |img|\n  # ...\nend\nThis works by passing the following filtered options to the client #describe_images call:\n:filters=>[{:name=>\"image-id\",:values=>[\"ami-fb8e9292\"]}]\n. Version 1.40.3 has been released with this fix.\n. Closing as fixed.  Please re-open is the issue persists.\n. I am not aware of how to query the storage for a given AMI using the EC2 API, but might I suggest asking your question on the EC2 forums?\nhttps://forums.aws.amazon.com/forum.jspa?forumID=30\n. Thanks!\n. Sorry for the slow response on getting this merged.  I'll try to cut a patch release with this fix shortly.\n. Sorry for the delay on this.  These API definitions are generated and I've been working to ensure the source was updated.  I want to avoid clobbering this change next time I update from source.\n. I like your suggestion, I'm just not sure about the best mechanism to accept this contribution.  The endpoints.json file is downloaded from:\nhttp://aws-sdk-configurations.amazonwebservices.com/endpoints.json\nIf I merge this pull request, they will get clobbered on the next update.  Additionally, we are moving away from the endpoint.json document with the v2 SDK.  Instead, we are using are adopting a configuration file that has logic for expanding endpoints from region names.  This will eliminate the need to update the SDK when services add endpoints.\nWhat about instead creating a module inside AWS::Route53 that can provide known hosted zone ids?\n. Thanks!\n. Could you run your example again with wire logging enabled?  It seems that you are having an issue connecting to the endpoint.  \nruby\nrequire 'aws-sdk'\nAWS.config( :access_key_id => ENV['AWS_ACCESS_KEY_ID'], :secret_access_key => ENV['AWS_SECRET_ACCESS_KEY'])\nputs AWS.s3(http_wire_trace:true).buckets.first.exists?\nAlso, could you run the following and share the output?\nruby\nputs AWS.s3.config.s3_endpoint\n. It looks like you have a typo in your region name.  It appears to be \"us-east-1a\".  This should probably be changed to \"us-east-1\".\n. I've passed the request along to the team that maintains the Amazon S3 API reference documentation. Thanks for the feature request.\n. Thanks for the bug report!  The session token was simply not getting added to the url as part of the signature for v4.  The commit above should address the issue.\n. There was an update a while back where Client.new was updated to return a versioned subclass of client.\n``` ruby\nold behavior\nAWS::DynamoDB::Client.new\n=> #\nnew behavior\nAWS::DynamoDB::Client.new\n=> #\nnew behavior allows the SDK to support multiple major API versions\nAWS::DynamoDB::Client.new(api_verison:'2012-08-10')\n=> #\n```\nI suspect this is the change that affected your tests.  I would also guess that the rspec helper any_instance only stubs instances of the target class and not subclasses.\n. I would suggest something simple like AWS::S3::MultipartUpload::EmptyUploadError with a message like \"Unable to complete an empty upload.\"  Please feel free to submit a PR.\n. Thanks!\n. If you configure a default region where SES is not available, you can configure an alternative default region for SES:\nruby\nAWS.config(region:'us-west-2', ses:{ region:'us-east-1' })\nI agree the service specific region/endpoint options are not well documented for this case.  We can definitely expand the documentation here.\n. I'm not sure what method exists that returns all verified email addresses.  Can you point me at that?\nGenerally we are not doing additional feature work in the v1 Ruby SDK, our efforts are instead focused on v2 (https://github.com/aws/aws-sdk-core-ruby).  We do accept pull requests though, so something could be added.\nAs a side comment/question, have you considered using the v2 SDK?  It can be used in the same application or process, as it uses a different gem name and module namespace.  Also, the v2 SDK will managing paging automatically.\nSome examples:\n``` ruby\nrequire 'aws-sdk-core'\nses = Aws::SES.new\nIn the v2 SDK, the returned response is pageable. If you enumerate it,\nit will continue making requests, yielding responses, until complete.\nidentities = []\nses.list_identities(identity_type:'Domain').each do |resp|\n  identities += resp.identities\nend\nor simply\nses.list_identities(identity_type:'Domain').collect(&:identities).flatten\n```\nYour original example function could be written using the v2 SDK like so (note it is getting the verification status in batches, instead of one request per domain, SES throttles this call, see: http://docs.aws.amazon.com/ses/latest/APIReference/API_GetIdentityVerificationAttributes.html)\nruby\ndef list_all_verified_domains\n  ses = Aws::SES.new\n  ses.list_identities(identity_type:'Domain').inject({}) do |verified, page|\n    resp = ses.get_identity_verification_attributes(identities: page.identities)\n    resp.verification_attributes.each do |identity, status|\n      verified[identity] = status if status.verification_status == 'Success'\n    end\n    verified\n  end\nend\n. Thanks for the clarification!\n. Good catch.  I've reproduced the bug and reverted the Content-MD5 portion of that commit.  Expect a fix for this to be published shortly.\n. Thank you for the bug report.  You are correct, the configured timeout was not getting propagated to the http request.  This fix should go out shortly with the next release.\n. It looks like we were working on a solution at the same time.  I pushed a commit (5351edf302efb349568b4e5ddcfad3fed725b43e) that addresses this issue.  Thank you for the contribution though!\nSee #544\n. This has been fixed on master and is available now.\n. What version of the SDK are you using?  Using the latest release I get the following:\nruby\nAws.elb.client.methods.grep /modify/\n=> [:modify_load_balancer_attributes, ...]\n. Is there a reason you have not updated to the latest version 1.41.0?  We follow semver, so all minor point releases should be backwards compatible.  You should be able to safely express a dependency on '~> 1.x' of the Ruby SDK.\nWe update the SDK regularly to support new features of service APIs.  This accounts for the large minor (41 of 1.41.0).  It looks like the feature you require was added to the SDK in 1.36.0.\n. The v1 Ruby SDK ships with two versioned client classes.  You can construct the API clients like so:\n``` ruby\ndefaults to older API version for backwards compatability\nddb = AWS::DynamoDB::Client.new\nyou can give an API version, these are the same\nddb = AWS::DynamoDB::Client.new(api_version: '2011-12-05')\nddb = AWS::DynamoDB::Client::V20111205.new\nyou can create the latest API version like so\nddb = AWS::DynamoDB::Client.new(api_verison: '2012-08-10')\nddb = AWS::DynamoDB::Client::V20120810.new\n```\nPlease note, the mid-level abstraction only works with the 2011 API version.  DynamoDB local only works with the 2012 API version, so you will not be able to use the mid-level abstraction, and must drop down to the API client.\nruby\nddb = AWS::DynamoDB::Client.new(api_verison: '2012-08-10', endpoint: 'localhost', port: '8000', use_ssl: false)\nAlternatively, you can configure these as global defaults:\nruby\nAWS.config(use_ssl: false, dynamo_db: { api_verison: '2012-08-10', endpoint: 'localhost', port: '8000' })\nddb = AWS::DynamoDB::Client.new\nAll of that said, might I recomend taking a look at the v2 SDK, https://github.com/aws/aws-sdk-core-ruby.  This supports the 2012 API version by default and has much simpler configuration:\nruby\nrequire 'aws-sdk-core'\nddb = Aws::DynamoDB.new(endpoint: 'http://localhost:8000')\nIt has no mid-level abstraction currently, so the above example returns an API client.  You can use both versions of the SDK in the same process, as they come from different gems, aws-sdk and aws-sdk-core, and they use different namespaces.\n. The #tables method is only defined on AWS::DynamoDB. If you are using AWS::DynamoDB::Client::V*, then you should call the #list_tables client method.\n. It only supports the 2012-08-10 API version. You can create the client using one of the following two methods.\nruby\nddb = AWS::DynamoDB::Client.new(api_verison:'2012-08-10')\nddb = AWS::DynamoDB::Client::V20120810.new\n. Also, I would strongly recommend using the v2 Ruby SDK. You can install it in the same application, as it has an alternative name and namespace. The support for DynamoDB is improved and the documentation is better. It is available as the aws-sdk-core gem (stable) or the preview release of aws-sdk.\n``` ruby\nrequire 'aws-sdk-core'\nno need to specify API version\nddb = Aws::DynamoDB::Client.new(endpoint: 'http://locahost:8000')\n```\n. I agree with your statement.  That said, changing one of these would be a breaking change for existing users.\nIn the v2 SDK (github.com/aws/aws-sdk-core-ruby) we consistently use string values to avoid this problem.\n. The fuzzy version dependency of ~> 1.4 is equivalent to >= 1.4.0 && < 2.0.  Is that not sufficient?\n. Thanks.  It looks like this change was clobbered when updating the RDS API.  It appears this was not being tracked upstream.  I've corrected this now, so updating this API should keep this fix in the future.\n. Thanks!\n. Thanks!\n. There is a small issue with the #empty? check. Vanilla Ruby does not implement #empty? on nil.  A small change could fix this:\nruby\nid.to_s.empty?\n. @klausbadelt It looks like you are logging the number of bytes yielded. Does it look like you are actually receiving all of the expected bytes or are the files actually truncated? Also, how often do you receive this error?\nRuby Net::HTTP did change a lot of internals with 2.0. I wouldn't be surprised if this issue is related to an upgrade.\nYou could disable the client-side check. If it is a bug in how the SDK computes the file size / bytes yielded, disabling this would be a temporary workaround until we can identify and resolve the issue. However, if the files are actually getting truncated, then you will want to ensure you compare the downloaded file size against the file size returned in the response headers.\nruby\nAWS.config(verify_response_body_content_length: false)\nYou can manually verify the size of the object using the response from #read:\n``` ruby\nafter disabling the response body content length check ...\nFile.open(filename, \"wb\") do |file|\n  resp = inobj.read do |chunk|\n    file.write(chunk)\n    md5 << chunk\n    progress.log chunk.size\n  end\nif resp[:content_length] != file.size\n    # oops, file is trucnated\n  end\nend\n```\n. That is good news, and bad news.  The good news is that the verify content length check is not in error. :)  The bad news, obviously, is there appears to be some issue with the request completing but before all the bytes have been transferred.\nLet me do some research to see if I can dig up any possible causes for an early response termination\n. Going back to this issue. Based on the discussion, I'm going to close this old issue. The SDK is working as indented by raising an error when fewer than expected bytes have been received from the remote end. I realize the source of the unreliable connection may still be unknown, but there have been no reports or comments on this issue. Also, v2 has been stable for quite a while now.\nIf there is further actionable information, then we can re-open the issue.\n. Thanks for the contribution!\n. It looks like this patch had some unintended consequences. Existing metadata with 'x-amz' prefixes no longer are saved with the same keys, and are not accessible in the response via the key they were set by. This directly affects the client-side encryption headers we use to save the envelope encryption keys.\nThat said, you don't need this to work with the website redirect feature.  It appears the client already supported this via :website_redirect_location which is passed through, but not documented, by S3Object#write.\nruby\nbucket.object['foo'].write('', :website_redirect_location => \"http://example.com\" }, acl: :public_read )\n. What version of the aws-sdk gem are you using?  I ran the same code and got the following:\nruby\nArgumentError: missing required option name\n. I am unable to reproduce this issue with v1.42.0 of the aws-sdk gem.  I am able to successfully call the #create_platform_application operation from the client.  Could you verify you are loading v1.42.0 by accessing AWS::VERSION in your running code?\nGiven you are not blocked and that I can not reproduce this error, I'm going to close this issue.  Please reopen if you can provide more information.  Thanks.\n. I'm glad you found the issue.  Currently we allow the client to be constructed with any region.  If you configure a default region, for example 'us-west-2', and then construct a Support client then you will get a client with an endpoint that will not resolve.  We've considered attempting to validate the endpoint and and raise some sort of error or correct the endpoint for services in a single region but have decided not to.  Services expand frequently into new regions and client-side validation would prevent legitimate regions from working without SDK updates.  Additionally, the \"single\" region problem is complicated more when you consider the new China Beijing region and gov cloud where there may be additional endpoints, but require a new account to access.  \nWe are not currently doing feature work on the v1 SDK, as our efforts are focused on the v2 Ruby SDK (https://github.com/aws/aws-sdk-core-ruby).  We do accept pull requests thought.\nThanks.\n. Thank you for reporting the issue.  I was able to reproduce the issue and commit a fix.  The commit above also includes integration tests that ensure the copy snapshot succeeds.  I'll try to cut a patch release shortly with this fix.\n. Version 1.43.1 is now live with this fix.\n. I am not currently able to reproduce this issue.  Here is the example I used:\n``` ruby\ns3 = AWS::S3.new\nputs s3.buckets['aws-sdk'].objects['foo'].url_for(:put, content_type:'image/svg+xml')\n=> https://aws-sdk.s3.amazonaws.com/foo?AWSAccessKe....Zc%3D\n```\nThen I used the following curl command:\nbash\ncurl -v -H \"Content-Type: image/svg+xml\" -T Rakefile \"https://aws-sdk.s3.amazonaws.com/foo?AWSAccessKe....Zc%3D\"\nCan you share an example that fails for you?\n. Is the a reason you do not use the Ruby SDK to generate the signed url instead of creating your own signature?\n. Its pretty simple:\n```\nrequire 'aws-sdk'\ns3 = AWS::S3.new # you will need to configure credentials\nurl = s3.buckets['bucket-name'].objects['object+key'].url_for(:read)\nputs url\n=> https://bucket-name.s3.amazonaws.com/object%2Bkey?AWSAccessKeyId=...&Expires=1403735768&Signature=...\n```\nThe API documentation can be found here for S3Object#url_for\nhttp://docs.aws.amazon.com/AWSRubySDK/latest/AWS/S3/S3Object.html#url_for-instance_method\nI'm going to close this issue as this is already supported by the SDK.\n. Thanks for the issue report.  I will update this shortly with a fix for the 1.8 regression.\n. It appears there is an issue with how the docs are formatted for the CloudFront client.  The CNAME value is only needed for serialization.  There are two places in your call this affects, :aliases and :origins.  Your example reformatted would look like:\nruby\ncf = AWS::CloudFront.new\ncf.client.create_distribution({\n    distribution_config: {\n        caller_reference: cell_id.to_s,\n        aliases: {\n            quantity: 1,\n            items: [\"#{b.bucket_name}.cell.io\"]\n        },\n        default_root_object: '',\n        origins: {\n            quantity: 1,\n            items: [{\n                id: b_id.to_s,\n                domain_name: \"#{b.bucket_name}.s3.amazonaws.com\",\n                custom_origin_config: {\n                    http_port: 80,\n                    https_port: 443,\n                    origin_protocol_policy: 'match-viewer'\n                }\n            }]\n        },\n        default_cache_behavior: {\n            target_origin_id: b_id.to_s,\n            forwarded_values: {\n                query_string: false,\n                cookies: {\n                    forward: 'none'\n                }\n            },\n            trusted_signers: {\n                enabled: false,\n                quantity: 0\n            },\n            viewer_protocol_policy: 'allow-all',\n            min_ttl: 0\n        },\n        cache_behaviors: {\n            quantity: 0\n        },\n        comment: '',\n        logging: {\n            enabled: false,\n            include_cookies: false,\n            bucket: '',\n            prefix: ''},\n        price_class: 'PriceClass_All',\n        enabled: true\n    },\n})\nI'll leave this issue open until the documentation is corrected.\nMight I suggest taking a look at the v2 Ruby SDK (https://github.com/aws/aws-sdk-core-ruby)?  It is availalbe as the aws-sdk-core gem, and can be installed in the same process as the aws-sdk gem (it uses a different gem name and root namespace).  The v2 SDK documentation generates formatted examples of how to call the API methods.  Here is a link to the same method in the v2 SDK: \nhttp://docs.aws.amazon.com/sdkforruby/api/Aws/CloudFront/V20120505.html#create_distribution-instance_method\n. This has been resolved in the v2 Ruby SDK API reference documentation:\nhttp://docs.aws.amazon.com/sdkforruby/api/Aws/CloudFront/Client.html#create_distribution-instance_method\n. Thanks for reporting the issue and for the quick fix!\n. I would not think these should be retried.  Do you know what HTTP status code is returned with these errors?  The SDK retries 500 level errors (server errors), but does not retry 400 level errors (client errors) with the exception of throttling errors.\n. If its a 500, we should consider adding an exception to prevent the retries.  Another question, how long are your requests retrying for?  By default, I believe it retries over about 2 seconds with delays of [0.3, 0.6, 1.2] between each attempt (adding round trip time per request).  This delay grows significantly if you expand the number of retry attempts though:\nruby\nArray.new(config.max_retries) {|n| (2 ** n) * 0.3 }\n. Yes it does.  Also, 9 retries gives a total delay of ~153 seconds, that explains the long time required to give up.  Let me see about a patch to prevent this operation from being retried.\n. Thanks!\n. My understanding is that the CLI does not load anything besides credentials from ~/.aws/credentials.  Can you confirm this is where the CLI is loading the region from and not from some other file, such as ~/.aws/config or the ENV? An easy check would be to remove the region or change it to a bogus region name and run the CLI command.\nIf the CLI is loading region information from the credentials file, I don't believe this is intentional.  That said, I've been talking with the CLI develoers, discussion a best path forward to share additional configuration options.\n. Thanks!\n. I merged this locally and added the docs. Your commit is here: ab05afb500847d4e3f861ecf8259fd50743b52da; I'm not sure why this did not automatically close this PR.\n. Awesome.  I've been working around this using ENV, but this is much nicer.  I'm going to make a change to AWS::Core::Configuration so that a profile name can be configured.  This will remove the need to replace the default credential provider.\n. Thanks.\n. The Ruby SDK currently only supports loading credentials from the shared credentials file, typically found at ~/.aws/credentials.  This file does not support regions and also does not have the \"profile \" prefix on each section header.  Was this suggestion intended for the AWS CLI?\n. I'm guessing you are indicating that there are no methods on AWS::EC2::Subnet to view/modify this. They are accessible via the AWS::EC2::Client operations such as #create_subnet, #describe_subnets, and #modify_subnet_attribute. \nThese methods provide a 1-to-1 mapping the of the EC2 API and should have everything you require. Some of these features are absent from the higher level resource-oriented abstractions. You are welcome to submit a PR that pulls these forward, you can see lib/aws/ec2/subnet.rb where this would happen. Some of these are lagging as we have been focused on the v2 Ruby SDK. \nAs a side note, the design of the v2 resource abstractions eliminates the need to manually describe resource attributes which eliminates this maintenance burden.\n. Closing this issue as there are no plans to expand the v1 SDK at this time. Feel free to use AWS::EC2::Client or the v2 Ruby SDK.\n. It looks like you logged this against the wrong repo. You are probably looking for https://github.com/aws/aws-flow-ruby.\n. The Ruby SDK specifically chooses to not modify user supplied input before sending values to the remote service. This can cause lots of downstream issues where values are not formatted as expected.\nI suggest opening this as an issue with the flow framework project. They should be providing SWF friendly values.\n. Closing this issue for now as there is no SDK changes required.\n. Thanks for the bug fix!\n. Thanks!\n. There appears to be a bug in how the API reference is generated that causes the response values returned as headers to not get documented. The :upload_id should be present on the response, it is just not getting documented.\nUntil this is fixed, you could reference the Ruby SDK v2 documentation. I might also suggest using the v2 client to interact with glacier.  The v2 SDK ships with a tree hash class that makes it easier to upload multipart objects to glacier (see the integration tests for a sample of how it is used).\nV2 API docs:\nhttp://docs.aws.amazon.com/sdkforruby/api/frames.html\nV2 Glacier Client InitiateMultipartUpload docs:\nhttp://docs.aws.amazon.com/sdkforruby/api/Aws/Glacier/Client.html#initiate_multipart_upload-instance_method\n. I hadn't had a chance to look at the test failures in depth. I suspect they are failures to mock responses correctly.\n. I pulled in your change, and updated the tests. They were not correctly stubbing the response metadata that would be normally be there.\n. Can you share the patch that adds the 1 second sleep? I would like to track down the cause and improve the default experience if possible. You should not need to resort to work-arounds like this if at all possible.\n. This has been a long-standing limitation of the objects abstraction as currently implemented.  The same issue exists if you wish to access etags on each object.\nCurrently we are working hard on the v2 Ruby SDK (https://github.com/aws/aws-sdk-core-ruby). One of the major differences is that the resource classes cache attributes whenever available.  Unfortunately the resources branch with S3 support is not complete; We are working on it though.\nThat said, you can use the v2 SDK S3 client to generate this list in a fairly simple manor:\n``` ruby\nrequire 'aws-sdk-core'\nobjects = []\ns3 = Aws::S3::Client.new\ns3.list_objects(bucket:'bucket-name').each do |response|\n  objects += response.contents\nend\nlargest_object_size =  objects.sort_by(&:size).last.size\n```\nPlease note, calling each on a response will cause it to paginate responses. Each response can return up to 1k objects. The code above could be optimized to not track every object in memory if you only needed the largest object / size.\nThe v2 SDK is available as a separate gem and uses a separate namespace. This ensures you can install the v2 SDK in the same application as the V1 SDK. I hope this workaround helps.\n. Closing this issue as there is no plan to change this behavior in v1. The v2 pre-release of aws-sdk will be available shortly. This version caches all attributes of AWS resources by default.\n. Thanks!\n. Thanks!\n. Thanks!\n. The DescribeInstances API operation returns a list of reservation sets and each reservation set contains a list of instances. Are you maybe not enumerating all of the reservation sets?\nruby\nec2.client.describe_instances[:reservation_set].each do |reservation|\n  reservation[:instances_set].each do |instance|\n    puts instance[:instance_id]\n  end\nend\nUsing aws-sdk-core it would be:\nruby\nec2.describe_instances.reservations.each do |reservation|\n  reservation.instances.each do |instance|\n    puts instance.instance_id\n  end\nend\nI looked at AWS::EC2::InstanceCollection to ensure nothing special was going on, it performs the same enumeration as describe above.\n. I'm going to go ahead and close this issue for now. Its been stale for a while and it does not appear to be an issue with aws-sdk. \n. Thanks.\n. This does not appear to be documented well, but there is a base error class you can use, AWS::SQS::Errors::Base. There are also two other more specific error classes that each extend base.\n- AWS::SQS::Errors::ServerError (for 500 level errors)\n- AWS::SQS::Errors::ClientError (for 400 level errors)\n. Running the given example myself does not generate that warning. Have you configured an alternative HTTP hander? The default handler should gracefully yield chunks of data and not cause that warning to emit.\n. I suspect EMHttpHandler was written before the SDK supported block reads. The warning is there to alert users that the entire response object was loaded into memory by their http handler.\n. Thanks!\n. Yes it does.  The docs did not correctly get updated with our last release.  I just cut the v.1.52.0 release and the docs should be updated shortly to reflect the recent changes.\n. Thanks!\n. Thanks; Sorry for the slow response on this. It should go out with the next release.\n. Amazon S3 does not support transfer encoding chunked. You are required to specify the content length up front when calling PUT object or upload part.\nYou could approximate this using the multipart upload APIs. You can initiate an upload and then upload chunks one at time until you are complete. Finally call the complete multipart upload operation to finalize your upload. This has the following limitations:\n- There must be at least 1 part\n- There may be up to 10k parts\n- Parts must be 5MB-5GB each, except the last which only has a max size limitation\n- The file may not be smaller than 5MB total\nThere is a AWS::S3::MultipartUpload interface that helps with this. \n. Thanks!\n. If you run the following command, what does it respond with?\n``` ruby\npath = '/root/.aws/credentials'\ncreds = AWS::Core::CredentialProviders::SharedCredentialFileProvider.new( :path => path)\ncreds.get_credentials\n=> { ... }\n```\nIs the hash empty? If not, is there an :access_key_id and :secret_access_key (please keep the values of these secret and don't post them here)?\nThe credential provider will attempt to load from the given path, but if the file doesn't exist or is not readable, it will return an empty hash. This causes the client to raise the missing credentials error.\n. I've patched the faulty argument error. This will ensure you get a helpful message that indicates :path must be set.  Your final example:\nruby\nirb(main):001:0> iam = AWS::IAM.new\n=> <AWS::IAM>\nThis example will continue to to work as-is in Ruby 1.8.7. When you do not configure a credential provider, then the default credential provider chain is used.  The shared credential file provider is omitted from the chain for Ruby 1.8.7 as it will not work. This ensure the chain can continue to the next possible providers, such as the EC2 instance metadata service provider.\n. Support for tagging operations was added to the SDK with 40544cb7c826dafdd684634b44769edbfa86e069. For some reason, it appears the API docs were not updated to reflect this.  I'll take a look at why the hosted docs have not been updated.\nIf you would like to generate the API docs, you can git clone this repo, bundle install and then run rake doc.\n. I reworked this to add them as resource attributes.  \nAs a side note, in version 2 of the Ruby SDK, these attributes will be dynamic and will not require code updates to pull these through from the client response.\n. We are releasing a fix for this shortly. This is a duplicate of #604 and is resolved by ee43e63665d5f4074781f8781c53e8f18856f948 in the master branch. Sorry for the inconvenience.\n. Thats really curious. Version 1.52.0 of the aws-sdk gem does not install any binaries. It simply pulls in the new aws-sdk-v1 gem, which installs the binary. This sounds like the travis environment already had an older version of the aws-sdk gem installed.\nDo you mind sharing the relevant parts of your Gemfile?\n. Also, I am considering renaming the binary if this continues to be an issue. I want to track down the actual cause first.\n. It sounds like travis didn't do anything wrong, and you simply had code that gem installed outside of the bundle install. Looking at second_curtain, it depends on the fuzzy version 1.48, i.e. ~> 1.48. This means if you invoke gem install second_curtain it will pull in the latest version that falls between 1.48 and < 2.0 (not including 2.0).\nThis means you installed v1.50 of the aws-sdk (as specified in your Gemfile.lock) and then also 1.52 as a result of gem install second_curtain directly. 1.50 provides the aws.rb binary as does aws-sdk-v1 (which is installed by aws-sdk 1.52).\nThe solution would be to either update your locked depenency on aws-sdk or to not gem install outside the bundle install (which shouldn't need to do).\nI'm glad you were able to resolve the issue, and thank you for reporting it here. It doesn't sound like there is anything we should need to do for now, but we'll keep watching for related issues.\n. I am generally very in favor of this change, as this has been a long-standing issue. The primary issue is this is a non-backwards compatible change. For better or for worse, the existing implementation always calls #head_object and returns fresh/current values.\nMemoizing this values generally makes sense, but can introduce bugs in code that is relying on the non-static values.  For example, I might write a script that is polling for a change to last_modified, expecting some other process to change the data. If I updated SDKs and then my script broke, I would be very unhappy.\nGiven the SDK follows semver, users locked on version 1.x.y should be able to update within 1.x without breaking changes. I would be in favor of this change, if the default behavior could be maintained, and memoized attributes were opt-in.  For example:\nruby\ns3 = AWS::S3.new(s3_cache_object_attributes: true)\ns3.bucket['aws-sdk'].objects.each |obj|\n  # look ma, no head request!\n  puts obj.key + ' => ' + obj.etag\nend\nThis could be accomplished by registering (in lib/aws/s3/config.rb) a new configuration option and then using client.config.s3_cache_object_attributes to determine when to refresh or not.\nAs a side note, the v2 SDK has a very early preview of the S3 object interface available. V2 of the SDK works as expected out of the box and memoizes all resource data by default.\nruby\ns3 = Aws::S3::Resource.new\ns3.bucket('aws-sdk').objects.each do |obj|\n  # look ma, no head request!\n  puts obj.key + ' => ' + obj.etag\nend\nThe v2 aws-sdk-core gem is available now as a preview release, I'll be cutting preview releases of the aws-sdk-resources and aws-sdk gems shortly. The github project is here: https://github.com/aws/aws-sdk-core-ruby\n. Version 2 is different enough from v1 that is it not a drop-in replacement in a number of places. That said, they use different namespaces, so you can you use both gems in the same project. The lastest v1 release, 1.52.0 is available as the aws-sdk gem and the aws-sdk-v1 gem. This ensures you will be able to install aws-sdk ~> 2 and aws-sdk-v1 with only minimal Gemfile updates.\nTo answer your other question, I would be willing to merge this pull request given we can ensure backwards compatibility and the configuration option would be sufficiently simple enough for users to opt-in to use this feature. I have stopped feature work on v1, so I likely wont tackle this, but we welcome community contributions, and this seems like a good fit.\nIf you would find it useful enough to stop using your fork and have this merged into mainline, I'd be happy to see that happen.\n. Sorry, this fell off my radar. Looks good to me.\n. Thanks!\n. Thanks!\n. The Curb handler does not implement the async interface. If you use the default Net::HTTP based handler, it should work.\n. The error is being returned by the remote end as \"Invalid snapshot\".  That implies there might be an issue with the given :image_id. I have to say that does sound odd, especially if the previous calls have already succeeded.\nIt might be helpful to see a wire log of a failed request and a successful request:\n``` ruby\nthis will send details http wire logs to $stdout, you can also configure a :logger option\nAWS.ec2(http_wire_trace:true).instances.create(options)\n``\n. Closing this stale issue. Please re-open if you continue to have issues.\n. Theaws-sdk-core` gem actually supports the CloudWatch Logs API today.\n``` ruby\nrequire 'aws-sdk-core'\ncwl = Aws::CloudWatchLogs::Client.new\ncwl.operation_names\n=> [:create_log_group, :create_log_stream, :delete_log_group, ...]\n```\nYou can safely run aws-sdk v1 and aws-sdk-core in the same process/application without conflict. Is there a particular reason you would like to see this backported?\n. Thanks.\n. I would suggest using aws-sdk-core for this task.  One of the significant improvements between v1 and v2 is that the v2 client abstractions in core know how to paginate responses automatically:\n``` ruby\nrequire 'aws-sdk-core'\nr53 = Aws::Route53::Client.new\nwithout pagination, only the first 100 records\nresp = r53.client.list_resource_record_sets({:hosted_zone_id => thezone})\nwith pagingation, continues until all records have been returned\nr53.client.list_resource_record_sets({:hosted_zone_id => thezone}).each do |resp|\n  # ...\nend\nall response objects are PageableResponse objects and know how to get the next page\nresp = r53.client.list_resource_record_sets({:hosted_zone_id => thezone})\nresp.last_page?\nresp = resp.next_page until resp.last_page?\n```\nThis pagination is controlled by configuration. You can see the list of output and input paging tokens here: https://github.com/aws/aws-sdk-core-ruby/blob/master/aws-sdk-core/apis/Route53.paginators.json#L17\n. Are you referencing from the AWS::EC2::Instance class or from the AWS::EC2::Client class? The client class supports this already:\nhttp://docs.aws.amazon.com/AWSRubySDK/latest/AWS/EC2/Client.html#modify_vpc_attribute-instance_method\n. Sorry for the slow response. We've been nose down on the 2.0 stable release of aws-sdk-core and the preview release of aws-sdk-resources. This should go out with the next release.\n. I merged this locally and pushed your commits. I'm not sure why it didn't close this PR, but the commits are in master. I'm going to close this now. Thanks!\n. Thanks for reporting the issue. This should go out with the next release.\n. @jgauna What gem are you using and how are your requiring it?\n. @jgauna It appears that the error is caused by your dependency on the aws-sdk without specifying what version. Version 2 was recently released which uses a different namespace (Aws, instead of AWS). \nThe simple fix is to add a version in your Gemfile, e.g.\ngem 'aws-sdk', '< 2.0'\n. @jgauna Alternatively, you can replace aws-sdk with aws-sdk-v1, but then you might need to replace a require statement in code from require 'aws-sdk' with require 'aws-sdk-v1'.\nThis approach has the benefit of allowing you to add a gem dependency on version 2 of aws-sdk and use them in the same application. \n. Sorry for the slow response. The :target option is only supported by the v2 SDK, as available currently via aws-sdk-core. \n. Thanks!\n. Looks good, thanks.\n. The #poll method as implemented automatically deletes messages if the block returns normally. If you trap the error inside the block, then the message will be deleted. \nThis is intentionally, as we don't want the SDK to swallow unexpected errors. What is lacking here is a way for your to indicate that the message should not be deleted because you encountered an error that made it not possible to process. \nWhat if the SDK was updated so that throwing something like :failure would allow #poll to continue looping without deleting the message?\nruby\nq.poll do |msg|\n  begin\n    # process message\n  rescue => e\n    # we don't want this message deleted\n    throw :failed\n  end\nend\n. I'm going to close this issue for now. There are no plans to change this behavior in the v1 sdk, as it would be a breaking change. I do plan to address this as part of v2 when a polling abstraction is added to the Queue class. \n. Thanks!\n. I've added the option to the #create call for the launch configuration collection class.  You could also use the Client client to call #create_launch_configuration directly to access this option (https://github.com/aws/aws-sdk-ruby/blob/master/lib/aws/api_config/AutoScaling-2011-01-01.yml#L161).\nHope this helps.\n. Thank you for reporting this documentation bug. In the interim, might I suggest using the v2 Ruby SDK client for Kinesis? Besides some very minor differences in configuration, the client API should be the same. V2 is available at:\n- gem aws-sdk-core\n- Git Repo: https://github.com/aws/aws-sdk-core-ruby\n- Documentation: docs.amazonwebservices.com/sdkforruby/api/frames.html\n- Kinesis client docs: http://docs.aws.amazon.com/sdkforruby/api/frames.html#!Aws/Kinesis/Client.html\nThe v2 SDK clients are very stable and the documentation even provides formatting examples that are easy to follow. A non release-candidate 2.0.0 final will be published shortly.\n. Hmm.. this seems to be related to #633. I'm going to close this as the method has been removed.\n. Sorry for the confusion. I've pushed a commit that removes the description for this unsupported operation. The Kinesis API does not respond to a \"PutRecords\" call, only \"PutRecord\".\n. Thanks for this pull request! I'm looking now into porting this behavior to the v2 Ruby SDK.\n. @rb2k This seems reasonable. Open a new github issue and we can track that feature request.\n. Yes, there are multiple ways to filter EC2 instances. If you are using AWS::EC2::Client, you can pass filter options directly to the #describe_instances call:\nruby\nec2 = AWS::EC2::Client.new\nec2.describe_instances(filters: [{name:'filter-name', values:['filter-value']}])\nIf you are using the resource interface, you can use the #filter method:\nruby\nec2 = AWS::EC2.new\nec2.instances.filter('filter-name', ['filter-value']).each { |instance| ... }\nTo filter by tags, you use the filter name of \"tag:TAGNAME\". This works in both the client and resource interface.\n``` ruby\nfilter instances that have the tag name \"role\" and value \"web\"\nec2.instances.filter(\"tag:role\", \"web\").each { |instance| ... }\n```\nYou can get a full list of supported filters from the EC2 API reference documentation: http://docs.aws.amazon.com/AWSEC2/latest/APIReference/ApiReference-query-DescribeInstances.html\n. Thanks.\n. The inspect string on these classes are intentionally not expanded arrays or hashes. Many collections require pagination across multiple API calls and could represent millions of objects. For this reason, all collections are lazily enumerated. This also allows collection method chaining to filter a collection prior to making any requests. You can also call collection.to_a which forces the collection to expand if you are confident in the suitable size of the collection.\n. You should be able to do all of this if you drop down to use the AWS::RDS::Client interface. The current resource-based abstractions are limited. We are currently focused on the v2 SDK, so there are no plans to expand them in the v1 aws-sdk gem.\n. Can you provide an expanded stack trace?\n. I don't see a stack trace from the SDK. I would have expected lines in the stack trace with the Ruby SDK gem name.  Is this log truncated?\n. This has been merged into the aws-sdk-v1 branch.\n. Thanks for the issue report. I'll try to take a look at this later today. \n. While these classes do not exist, it is possible to use the AWS::IAM::Client operation to describe each of these things.\nBetter yet, the v2 Ruby SDK has these classes now. You can gem install aws-sdk with a --pre flag or you can install its dependent gem, aws-sdk-resources The v2 SDK uses a slightly different root namespace, so you can use it in the same project:\n``` ruby\nrequire 'aws-sdk-resources'\niam = Aws::IAM::Resource.new\niam.instance_profiles.map(&:name)\n=> [... list of profile names ...]\n```\nThe v2 repo is here and the v2 API docs are here. Currently we are focused on the v2 Ruby SDK work, so there is no plan to back-port these interfaces. Please note, the v2 SDK is not backwards compatible, but you can use them in the same application as pointed out above. This makes upgrades very simple.\n. This functionality exists when using AWS::DynamoDB::Client::V20120810.\n. Thanks for the contribution! Sorry for the slow response. This has been merged into the aws-sdk-v1 branch: d0916e194619b50aab236984f703ed8b760da7fa\n. Thanks for the tip. In the v2 SDK I've been using the following (not 1.8.7 compat) as an inject replacement:\nruby\nsource.each.with_object({}) do |(key,value), hash|\n  hash[key] = value\nend\n. This is an interesting suggestion, but we are currently feature locked on the v1 SDK. This would be a great suggestion for the v2 SDK: https://github.com/aws/aws-sdk-core-ruby\n. I added this to our public backlog here: https://github.com/aws/aws-sdk-ruby/blob/master/FEATURE_REQUESTS.md#progress-callbacks-for-amazon-s3-object-uploads\n. If you are using the classic endpoint of Amazon S3, which is {bucket-name}.s3.amazonaws.com, then the Ruby SDK defaults to an older signature version. If you have given the region name of 'eu-central-1' and not 'us-east-1', then the Ruby SDK will instead use the regional endpoint and the newer signature version.\nThat said, I have a pending commit that will deal with the response error, log a warning, determine the correct region, and then use the regional endpoint plus sigv4.\n. I'll be cutting a release shortly with the fix above for using buckets in eu-central-1 with the classic endpoint.\n. It is not in the v1 AWS SDK for Ruby. It is in the v2 Ruby SDK though:\n$ gem install aws-sdk-core\nThen:\nruby\nrequire 'aws-sdk-core'\nkinesis = Aws::Kinesis::Client.new\nYou can safely use both gems in the same application.\n. It does not. It is an API client only. The Kinesis client library is only, as far as I know, for Java and Python. It could be ported to ruby, but a port does not exist AFAIK.\n. If we were to build this today, it would depend on the v2 AWS SDK for Ruby (github.com/aws/aws-sdk-core-ruby) and it would probably be published as a separate repo and gem. I'm guessing it could be named something like aws-sdk-kinesis or aws-sdk-kcl.\nCurrently, I believe the Kinesis team owns each of the implementations, not the individual AWS SDK teams. That said, it would likely end up as a group effort.\n. There is not one yet. That said, we are considering adding this as a separate gem built on the v2 SDK. I can not comment on timelines though. Thanks for the suggestions.\n. There is no such feature for the Ruby SDK at this time. If one is added, it would be added to the v2 Ruby SDK which will be 2.0 stable shortly.\n. This has been resolved in the v2 SDK (which will be 2.0 stable shortly). There are no plans to change this behavior in v1. \n. This is correct behavior. You are specifying the key of the object as \"document+copy.pdf\".  To create a public_url that is equivalent they key should be \"document copy.pdf\" (no plus sign).\n``` ruby\nbucket = AWS::S3.new.buckets['bucket_name']\ns3_object = bucket.objetcs['files/document copy.pdf']\ns3_object.public_url\n=> #\n```\nNotice the %20, not %2B. The 2B is a URL encoded plus sign character, whereas 20 is a space character.\n. Sorry for the slow response on this. The url escaping is correct. The issue is that spaces need to be provided as space characters and not \"+\" in the key. \n. This has been merged into the aws-sdk-v1 branch: 437c6f16cd9b38138ea205f7014e0b0747afc0fd Thanks for the samples.\n. This has been merged into the aws-sdk-v1 branch: 362dd0aa9722be5d94f4e9911b7bcae2432294b1\nThanks for the contribution!\n. Thanks! I've merged this and also updated the Route53 model with some additional changes.\n. We've adopted a CHANGELOG for v2 of the SDK:\nhttps://github.com/aws/aws-sdk-core-ruby/blob/master/CHANGELOG.md\nCurrently the v1 SDK is on feature lock and only receives API updates and bug fixes. You point is however valid. I will add a CHANGELOG for any maintenance releases from now on.\n. This is a limitation of the EC2 API. When you associate a public ip address then a network interface is attached. This is not supported with an instance security group.\n. Thanks for the contribution. This should go out with the next release.\n. The v1 Ruby SDK requires that you configure the proper region for calls to eu-central-1. This is necessary because the eu-central-1 forces signature version 4 and the region name must be known to generate that signature.\nFor newly created buckets, you can work around this by configuring :region => 'eu-central-1.\n. This has been merged into aws-sdk-v1 60d1492d64d9f4588161fbdeb51ba9cac0d6f220\n. This is a limitation of the AWS APIs. If an API is added for discovering the ID, then this can be added to the SDK.\n. This is likely due to the DNS addressing used by the SDK. It will connect to an endpoint that contains the bucket name. My best guess is that you are heading multiple different buckets and objects and so these are going to different hosts. For example:\nbucket1.s3.amazonaws.com\nbucket2.s3.amazonaws.com\n...\nConnections are only pooled/re-used when they are the same endpoint, port, scheme, etc. Can you verify these requests are made against multiple buckets?\n. Sorry for the slow response. I've posted a fix for this to the aws/aws-sdk-core-ruby repository:\nhttps://github.com/aws/aws-sdk-core-ruby/commit/2ebebf94420dca8ebe31752f70c6b101fe05d956\nThis fix should be released shortly.\n. Thanks for the contribution! This has been merged into aws-sdk-v1 50300ce7e7bc6ee8898b3f5b14313558a6647f83\n. Thanks for the doc fix!\n. Thanks for the documentation fix.\n. Thanks for the fix! This has been merged into the aws-sdk-v1 branch by 4e22a77950b212a9dabfb6960eb82c3038cecda0.\n. In version 1.52.0 the gem was refactored from aws-sdk to two gems:\n- aws-sdk\n- aws-sdk-v1\nWhere aws-sdk has a runtime dependency on aws-sdk-v1. The reason is so that for versions >= 2.0 the aws-sdk gem can pull in the latest version while allowing users to install the v1 SDK in the same process (via aws-sdk-v1).\nThat said, I don't know why this should affect your usage of aws-sdk.\n. Might I also suggest that you replace your dependency from aws-sdk to aws-sdk-v1. This will avoid the need for your bundle install to pull in the depenency. The aws-sdk gem is now empty and is only there to load the v1 gem. I'm guessing this will help work around your issue. Just be certain to replace your require statements with:\nruby\nrequire 'aws-sdk-v1'\n. This is a result of the IAM API. When indexing access keys from the root IAM object you get the access keys for the \"current user\", which is either the root account or the iam user the configured credentials are scoped to. This is behavior as defined by the API.\nFor this reason, the SDK can only know the user for access keys when constructed with a user.\nThe following produces what you expect.\n``` ruby\niam.users['username'].access_keys['akid'].user.name\n=> 'username'\n```\n. Thanks for the fixes.\n. I ran the following code in the REPL:\nruby\nAWS> ec2 = AWS::EC2.new(:region => 'us-east-1', :max_retries => 1, :http_open_timeout => 1, :http_read_timeout => 0.01); ec2.client.describe_instances\n[AWS EC2 200 1.308121 1 retries] describe_instances() Net::ReadTimeout Net::ReadTimeout\nNet::ReadTimeout: Net::ReadTimeout\nfrom /Users/trevrowe/.rbenv/versions/2.0.0-p598/lib/ruby/2.0.0/net/protocol.rb:158:in `rescue in rbuf_fill'\nAWS>\nNotice that I modified the read timeout to a very short 0.01 seconds to force a failure. The call returned nearly instantly with the expect error. \nI notice that you are configuing the ec2 endpoint to ec2.us-east-1.amazonaws.com:7777. I wonder if this is causing an issue. There is a separate :ec2_port option that you can try. I'm wondering if this is causing a DNS lookup issue that is not related to the HTTP timeouts.\n. Closing stale issue. Please re-open if you have additional information.\n. I did some digging. It appears that the underlying API call for DescribeLoadBalancers requires pagination. I'll add a fix to cause the SDK to paginate.\nhttp://docs.aws.amazon.com/ElasticLoadBalancing/latest/APIReference/API_DescribeLoadBalancers.html\n. Merged into aws-sdk-v1, 97e4f9ac8008667bdfa695d177c0859c95576a56. Thanks!\n. The SDK, when loaded inside Rails will automatically load credentials from Rails.root/config/aws.yml. This behavior is being removed from the SDK as of v2.\n. The SDK does not attempt to override Ruby defaults as used by Net::HTTP. I do not know the Amazon S3 prefered settings. Might I suggest posting this question to the Amazon S3 forums here?:\nhttps://forums.aws.amazon.com/forum.jspa?forumID=24\n. I'm going to close this issue for now. I don't believe there is any change the SDK should be making to the default SSL context for now.\n. I'm sorry, but I do not know.\n. Thanks for reporting the issue. I've applied a fix that should go out with this week's release.\n. @KranthiKishore the grant_read option expects a value formatted to match the 'x-amz-grant-read' header. You can reference the Amazon S3 api docs for more information on the expected format. That said, it looks like what you really want is to make the object publicly readable. You can do this using the 'public-read' canned acl:\nTry the following:\nruby\nobject.upload_file('/home/kranthi/projects/hubkit/example_output.csv', acl: 'public-read')\n. The AWS::Redshift::Client API was not updated, so what you are seeing is the default parsing behavior. I'm going to update the model shortly, and that should resolve this issue.\n. Have you tried the following:\nruby\nassert_equal s3.client.stub_for(:put_object)\n. Closing this issue for now. We are not currently doing feature work in the v1 SDK. We can consider this as a feature request for v2.\n. By calling S3Object#head, you will receive the content encoding. The #metadata method only returns the \"x-amz-meta-*\" headers/metadata. The #head method returns both.\nYou can pass a block to #read to get both the data and the metadata:\n``` ruby\nread the object in chunks using block mode\nfile = File.new('target', 'wb')\nresp = s3.buckets['aws-sdk'].objects['key'].read do |chunk|\n  file.write(chunk)\nend\nfile.close\nthe returned response contains everything except the yielded data\nputs resp.inspect\n=>\n{:meta=>{},\n :restore_in_progress=>false,\n :content_type=>\"\",\n :etag=>\"\\\"ed076287532e86365e841e92bfc50d8c\\\"\",\n :accept_ranges=>\"bytes\",\n :last_modified=>2015-01-05 10:28:29 -0800,\n :content_length=>12,\n :data=>nil}\n```\nAdditionally, I would strongly recommend checking out the v2 AWS SDK for Ruby. The S3 client interface is much improved for getting objects.\n``` ruby\nv2 client only gem\nrequire 'aws-sdk-core`\nv2 sdk uses a different namespace so you can use both gems in the same process\ns3 = Aws::S3::Client.new\nresp = s3.get_object(bucket:'aws-sdk', key: 'foo')\nresp.body.read\n=> \"....\"\nresp.body.content_encoding\n=> \"...\"\nget object directly to a file\nresp = s3.get_object(bucket:'aws-sdk', key: 'foo', response_target: '/path/to/target/file')\n```\nHope this helps.\n. Sorry for the slow response. We've been very busy with the recent update to v2.  The token is sent to the endpoint. For email endpoints, a link is generated. Other endpoints have different formats. \n. @dandunckelman That is my understanding.\n. Closing in favor of https://github.com/aws/aws-sdk-core-ruby/issues/196.\n. This is a known limitation of the v1 AWS SDK for Ruby (aws-sdk gem). The 'eu-central-1' region requires signature version 4. When calculating this signature, the region name is part of the string to sign. The tricky part is the when using the endpoint of \"s3.amazonaws.com\", the SDK does not what region DNS will resolve to, and therefore cannot sign the request properly. Instead it is forced to guess, us-east-1.\nYou can work around this, by configuring your client to use the actual bucket region. When the SDK sees that it is talking to eu-central-1, will will automatically switch to sigv4, so there is no need to configure the signature version:\nruby\ns3 = AWS::S3.new(region: 'eu-central-1', access_key_id: '...', secret_access_key: '...')\nMight I also suggest checking out the v2 AWS SDK for Ruby? This version of the SDK will handle the error message returned, re-sign, and re-send the request with the proper region.\n. This has been merged into the aws-sdk-v1 branch: b61dd0d3d41cecdfbaf5e486b07aedf4ee43f4d1\n. BTW, thanks for the contribution!\n. I need to follow up on this. At the time, it was at the request of the service team.\n. I've added it back now and it should go out with the next release.\n. This has been merged into aws-sdk-v1.\n. You are correct. We plan to expand the S3 client to support requester pays as a parameter, but until this happens, you can use a custom handler to add this functionality.\n``` ruby\nclass RequesterPays < Seahorse::Client::Plugin\n  handler(step: :initialize) do |context|\n    if context.params.delete(:requester_pays)\n      context.http_request.headers['x-amz-request-payer'] = 'requester'\n    end\n    @handler.call(context)\n  end\nend\ndo this once, and all s3 clients will now accept :requester_pays to all operations\nAws::S3::Client.add_plugin(RequesterPays)\ns3 = Aws::S3::Client.new\ns3.put_object(bucket:'name', key:'key', body:'data', requester_pays:true)\n```\n\n. The 88edf93 commit should add support to all of the appropriate object operations that support requester pays. This should go out with our next release.\n. I went ahead and merged this, but I'm going to leave issue #694 open until this is addressed for the v2 SDK.\n. Adding #exists? methods is on a short list of features I'd like to address with resources shortly.\nFor the v1 SDK, the exists methods were all hand-coded and used different logic for each scenario. For example, with Amazon S3 buckets, the exists method would make a #get_bucket_versioning request. With a successful response, the bucket exists. If a NoSuchBucket error was raised (404), false was returned. If an AccessDenied error was raised, then true was returned (the bucket exists, you just can't get information about it).\nFor v2, I'm inclined to solve this using waiter methods. By adding a waiter for \"BucketExists\" and CloudFormation \"StackExists\" we can share the logic. The exists method would simply poll the waiter exactly once, and not keep waiting.\n. It may not be very clear, but the API reference documentation lists #each as an alias for #each_page:\nhttp://docs.aws.amazon.com/sdkforruby/api/Aws/PageableResponse.html#each_page-instance_method\nI prefer #each, but #each_page exists for consistency with the other methods such as #last_page?, and #next_page. I'm going to go ahead and swap the methods so the documented method is #each and #each_page is the alias.\n. Normally we do not accept pull requests or changes against the .api.json models. These are extracted from other documents internally and changes would be clobbered unless the changes are made upstream. The other *..json documents, such as pagainators, waiters, and resources and hand-maintained and we gladly accept changes here.\nTo address the inconsistent error information. Each of the AWS SDKs use the error information present differently. For Java and .NET, these are used to create error classes for the documented error types. All other errors returned by the service are raised as a generic service error. The Ruby SDK does not use this information at runtime, but only to generate API reference documentation to show what errors might be raised.\nI agree additional information here would be useful, but the Ruby SDK only vendors the API models, it does not \"own\" them.  I can definitely pass feedback along to the service teams that own the APIs.\n. I'm going to mark this as closed. Please feel free to re-open if you have any additional questions. Feel free also to ask questions in our gitter channel: http://gitter.im/aws/aws-sdk-ruby\n. I haven't given it much consideration to this point. I would be generally in favor of supporting this feature, but I am unable to comment on timeline. I'd have to see where this fits into our priorities.\n. The file needs to be at ~/.aws/credentials for Linux and Mac. For Windows it should be at %USERPROFILE%\\.aws\\credentials. It should be formatted like:\n```\n[default]\naws_access_key_id =YOURACCESSKEY\naws_secret_access_key=YOURSECRET\n[alternate-profile]\naws_access_key_id=YOURACCESSKEY\naws_secret_access_key=YOURSECRET\n```\nBy default, the SDK will use the credentials in the \"default\" profile. You can change the profile like so:\n``` ruby\ndefault for all services\nAws.config[:profile] = 'profile-name'\nor a per client profile\ns3 = Aws::S3::Client.new(profile:'other-profile-name')\n```\nI haven't heard any other issues reported about this feature. I use it myself regularly. Can you verify the paths and format and let me know if you are still having issues?\n. I'm glad you were able to resolve the issue.\n. > I can confirm that region is honoured if set in ~/.aws/credentials. The docs do look a little muddled here as they also say \"The AWS CLI will also read credentials from the config file\". It looks like credentials and config files can be used interchangeably with only a slight difference in format (eg. [name] vs. [profile name]).\n\nBased on this I think this PR is incomplete as ~/.aws/config should be inspected for a default region too.\n\nInteresting that the CLI loads a region from this file. I suspect this is a side-effect of sharing code for loading credentials & config as they used to be in the same file.\nI understand that it would be handy for the Ruby SDK to be able to share more configuration with the CLI, especially region. I am concerned that loading region from a credentials configuration file is mixing concerns. Credentials are not scoped to a single region. This also introduces some other possible complexity if we decide to adopt the CLI's ~/.aws/config file format for loading the region. Which file has precedence for setting the region? \nI'm going to continue the discussion over here with some of the other SDK and CLI engineers and I'll share what comes of this disucssion.\n. I spoke with one of the CLI engineers. He said it is not intended behavior for the CLI to load a region from the shared credentials file, but a side-effect of implementation. He also indicated that it might be subject to change in the future. Given the purpose of this file was to segregate AWS secrets from other configuration we are not inclined to support loading a region from this file in the Ruby SDK.\nThat said, I do understand the desire to share region configuration between the CLI and Ruby SDK. It would be worth investigating loading the region from the ~/.aws/config file directly.\n. I'm going to close this pull-request, as the current plan is to not support loading a region from the credentials file. If you'd like to discuss loading the region from the ~/.aws/config file, hop into the Gitter channel and shoot me a message.\n. Thanks! Test worked well locally for me.\nTo run the tests, git clone the repo, bundle install, and then run rake or rake:test.\n. Good question. Self throttling can prevent a single process from browning out access to these APIs. Another thing to consider is checking if the new AWS Config service returns information about stack summaries or events. I'm not sure if this has been added, but it might be a good solution. Lastly, you could possibly reduce to a single process that polls for information periodically caching it locally.\nI don't personally use the CloudFormation API much, so I may not be the best resource. I can also suggestion posting your question on the CF forum: https://forums.aws.amazon.com/forum.jspa?forumID=92\nSomething else to note, the AWS SDK for Ruby will automatically retry throttling errors up to 3 times before raising the throttling error. They do this with a default small exponential backoff. You can increase the number of retries and the backoff:\nruby\ncf = Aws::CloudFormation::Resource.new({\n  retry_limit: 10,\n  retry_backoff: Proc.new { |attemps| sleep(2 ** attempts) }\n})\nHope this helps!\n. The retry limit defaults to 3. Unless set otherwise, this should enable the default handler. \n``` ruby\ncf = Aws::CloudFormation::Client.new\ncf.build_request(:list_stacks).handlers.map(&:name).grep /retry/i\n=> [\"Aws::Plugins::RetryErrors::Handler\"]\n```\nAre you experiencing something specifically that indicates the retry handler is not applied or working?\n. Thanks for the fix. I'll merge this change and send the patch to the upstream source as well.\n. You've got the wrong ENV var for the id. Instead of AWS_ACCESS_KEY, try AWS_ACCESS_KEY_ID\nlambda = Aws::Lambda::Client.new(\n  region: 'eu-west-1',\n  access_key_id: ENV['AWS_ACCESS_KEY_ID'],\n  secret_access_key: ENV['AWS_SECRET_ACCESS_KEY'],\n)\nLet me know if this does not resolve your issue.\n. You are very close. You've constructed an API client, but you did not pass this client to the S3 object. \nruby\ns3 = Aws::S3::Client.new(credentials: credential, region: \"us-west-2\")\ntest_object = Aws::S3::Object.new(\"existing-bucket-name\", \"test-filename\", client: s3)\nresult = test_object.upload_file(\"/path/to/test/upload.txt\")\nThat said, I would recommend using the Aws::S3::Resource interface from the start.\nruby\ns3 = Aws::S3::Resource.new(region:'us-west-2', credentials: credentials)\nresult = s3.bucket('existing-bucket-name').object('key').upload_file('/path/to/file.txt')\nAlso, you can skip creating the credentials object, if you are going to use static credentials. You can reduce your original example to the following:\nruby\ns3 = Aws::S3::Resource.new(region:'us-west-2', access_key_id:'...', secret_access_key: '...')\nresult = s3.bucket('existing-bucket-name').object('key').upload_file('/path/to/file.txt')\nLastly, I also would recommend never putting static credentials in code. If you are loading them from a file outside version control, that is fine. If not, I would recommend moving them to the shared credentials file. If you create a file at ~/.aws/credentials that looks like this:\n``` ini\n[default]\naws_access_key_id =...\naws_secret_access_key=...\n[alternate-profile]\naws_access_key_id=...\naws_secret_access_key=...\n```\nThen the Ruby SDK will automatically load credentials for you from the default profile. You can also specify a :profile option with the name of what credentials to use, e.g. profile: 'aternate-profile'. Using shared credentials you can reduce your constructor to:\nruby\ns3 = Aws::S3::Resource.new(region:'us-west-2')\n. I just pushed a fix that will also improve the error message you receive. It will clearly state that you are calling without credentials instead of no method error on nil. Thanks for reporting the issue. Please feel free to re-open if you continue having issues.\n. I am unable to reproduce this. I sent an invalid request, rescuing the raised error. Here are its ancestors:\nruby\nerror.class.ancestors\n=> [Aws::EC2::Errors::InvalidInstanceIDMalformed,\n Aws::EC2::Errors::ServiceError,\n Aws::Errors::ServiceError,\n RuntimeError,\n StandardError,\n Exception,\n Object,\n JSON::Ext::Generator::GeneratorMethods::Object,\n PP::ObjectMixin,\n Kernel,\n BasicObject]\nYou should be able to rescue Aws::EC2::Errors::ServiceError or the even more generic Aws::Errors::ServiceError.\n. This one. I'll be closing the other repository shortly.\n. I'll see what I can do about reproducing this locally. Can you run the same test in Ruby 1.9.3?\n. Watching Ruby issue: https://bugs.ruby-lang.org/issues/10942. This appears not to be an issue in other versions of Ruby, might be a Ruby 2.2 bug.\n. Yup, it appears to have been fixed in trunk. I suspect this will go out with the next release of Ruby and will be back-ported into supported version. For users looking to work around this issue, see this related issue: https://github.com/aws/aws-sdk-ruby/issues/785\n. I should add that the work-around is given as an example to the v2 SDK, modifications would be required to make this work in the v1 SDK.\n. Thats for reporting the issue. This is not intended behavior. I'm going to look at this more, and hopefully get a fix shortly.\n. I've been looking into this. It appears, unlike other services, that cloud watch logs is using the marker to allow continuations, even when there are no more results at the current point in time.\nFor the SDK to handle this generically, the paginators would need to maintain the paging tokens from the previous response and compare them to the current ones to determine if there are any more results. I'm going to discuss this with a few team members and hopefully put together a solution. Thank you for your patience.\n. I have a working patch, but I'm doing some more research to ensure that echoing the same token is not a valid pattern elsewhere.\n. I re-worked the patch (e02eb99), this version is a much smaller code change, and is easier to follow. I'm much happier with the result and I think it satisfies the required change without breaking existing tests. The previous commit  had some test failures.\n. Closed by #730. This will be part of our next release. Thanks for submitting an excellent bug report and for your patience!\n. Good catch! Thanks for the fix!\n. Thanks for the fix!\n. The SDK will default to using REXML from the Ruby standard lib. It is not the most performant library, but it is always available and requires no native extension support. I personally recommend the Ox gem. It is faster than nokogiri, but less portable, so your own needs will dictate the solution.\nThis error, however does not look like an issue with REXML. Instead, I suspect the fake sqs server is not sending back the expected response structure. Can you enable the HTTP wire trace to help debug?\n``` ruby\nsqs = Aws::SQS::Client.new(http_wire_trace:true)\n...\n```\n. I'll try to take a look at this closer today. It is curious to me that the xml parsing logic is whitespace sensitive and that may warrant fixing in the SDK.\n. I was unable to reproduce the same issue locally. This gist, https://gist.github.com/trevorrowe/df78dd1f92cf02f19f76, gives the exact output you have in your example to the parser and it has no issues with the whitespace or xml formatting. More interesting, the actual error message indicates a different string being parsed:\n/home/quezacoatl/dev/work/sony/entrance/apps/worker/vendor/bundle/ruby/2.2.0/gems/aws-sdk-core-2.0.27/lib/aws-sdk-core/xml/parser.rb:33:in `block in structure': undefined method `key?' for \"\\n    \":String (NoMethodError)\nThis implies, that somehow the parser was receiving only a portion of the full xml body, and it was just whitespace.\nDoes this error persist on every request? If so, would you consider pairing over hangouts so that we can debug / reproduce this issue together?\n. @quezacoatl Thank you for the updated gist. I was able to reproduce the error locally and it know makes sense to me what is going on.\nIn the sample XML there is the following:\nxml\n<ReceiveMessageResult>\\n    </ReceiveMessageResult>\nWhen the ReceiveMessageResult xml element is present, it is expected to be a structure, i.e. a node with multiple child elements. Because none of these elements are present, the multi_xml gem is assuming that this is a string element and is returning the whitespace \"\\n    \" value. The parser blows up because it is expecting a Hash, not a string.\nThe simplest fix for this is to send one of the following XML results:\nxml\n<ReceiveMessageResult/>\n<ReceiveMessageResult></ReceiveMessageResult>\nIdeally, this would be reported to multi_xml as a bug. There should not be a difference in return value from MultiXml.parse based on the provider; I verified that Nokogiri works while REXML fails.\nGiven that the SDK works given expected input, I'm not inclined to make any changes in the SDK at this point. If I were to make a change, it would be to replace the multi_xml dependency. Parsing the entire document into a dom-like hash, just to translate the entire structure is less efficient than sax parsing directly into the desired structure.\nMy suggested work-around to you would be to patch fake_sqs at this point. Please, if you encounter any other issues like this, please let me know.\n. I've opened an issue with MultiXxml here: https://github.com/sferik/multi_xml/issues/46\n. The name is fine. It should match \"all\" of the requested instances are healthy. You control what all means by omitting the instance if list or providing one or more odd. Does that make sense?\n. Maybe my confusion is the intended use of the waiter. Do you intend to wait until any of the instances are \"InService\" and use this as a condition to indicate the health of the load balancer? If so, maybe its just naming confusion.\nMy suggestion:\n1. Rename the first waiter from \"InstanceHealthly\" to \"InstanceInService\". This makes it very clear what state is being polled for.\n2. Rename the second waiter to \"AnyInstanceInService\".\nExample:\n``` ruby\npoll until EVERY instance is in service\nelb.wait_until(:instance_in_service)\npolls until both instances are in service\nelb.wait_until(:instance_in_service, instances: [{ instance_id: 'id' }, { instance_id: 'id2'}])\npoll until ANY instance is in service\nelb.wait_until(:any_instance_in_service)\npolls until any of the two instances are in service\nelb.wait_until(:any_instance_in_service, instances: [{ instance_id: 'id' }, { instance_id: 'id2'}])\n```\nDoes that cover your use cases?\n. Thanks for the contribution!\n. thanks for reporting the issue. I agree this sounds like it should be fixed in the SDK. I'll try to take a look at this shortly.\n. I took a look into this and was unable to reproduce the issue:\n``` ruby\nsigner = Aws::S3::Presigner.new\nurl = signer.presigned_url(:get_object, bucket:'aws-sdk', key:'foo:bar')\n=> \"https://aws-sdk.s3.amazonaws.com/foo%3Abar?X-Amz-...\"\nr = Net::HTTP.get_response(URI(url))\n=> #\nurl = url.gsub('%3A', ':')\n=> \"https://aws-sdk.s3.amazonaws.com/foo:bar?X-Amz-...\"\nr = Net::HTTP.get_response(URI(url))\n=> #\n```\nI purposefully modified the generated url to use \":\" instead of the percent-encoded \"%3A\".  Both versions of the URL are accepted by Amazon S3. I suspect that the nginx to reverse proxy is adding headers that Amazon S3 expects to be signed, that aren't, or is modifying header values that were already signed.\nAre you able to run the example above?\n. As it stands, it does not look like a bug in the SDK. I would investigate the proxy to see if it is changing or adding headers to the request. Also, if you can provide a sample gist that shows the pre-signed URL can not be used, I'd be happy to dig into this more. Please feel free to re-open if you have more information.\n. This sounds related to #720. The v2 client, Aws::S3::Client, does not have a content_length parameter for #put. Neither the v1 or v2 client have a pre-signed post implementation. Only v1 provides a pre-signed post class, and this does not exist in v2. Please provide some more context in the other issue.\n. The v2 SDK does not provide a pre-signed POST utility at this time. Can you give a code example of what you are doing that generates this error?\n. The #presigned_post method has no equivalent in the v2 SDK. I'm going to mark this as a feature request. Until this has been ported to the v2 SDK, you can continue using the v1 presigned post utility. You can use v1 and v2 in the same application. \n. @modosc I've just submitted a pull-request to the main repository that adds pre-signed post support in v2. I would be interested in feedback: aws/aws-sdk-ruby#752\n. Thank you for reporting this issue. I'm looking into why this is being omitted.\n. Thanks!\n. Since version 2 has been released, we've been averaging about once a month of the v1 SDK releases. This is not a strict rule, just an observation. Releases happen generally when there is a bug or security fix, or an important API update for a service supported in v1. The last release was about 2 weeks ago.\n. The first line should never return nil:\nruby\nr = Aws::S3::Resource.new(region: 'ap-southeast-1', stub_responses: true)\n=> #<Aws::S3::Resource>\nWhat do you have in your Gemfile and how are you loading the aws-sdk gem?\n. Closing this as a non-issue with the SDK. I suspect there is an issue in your environment, perhaps some other gem or code is interfering with the SDK. The code you use above is verified by quite a few users.\nPlease re-open the issue if you can provide a small gist with a Gemfile and ruby script that demonstrate this bug. Thanks!\n. The purpose of the AWS SDK for Ruby :stub_responses => true feature is to eliminate the need for you to stub each of these individual classes. The SDK will exercise the entire code-base up-to but not including the HTTP request. It then returns an empty response.\n. The commit above will now send AssumeRoleWithWebIdentity calls without authentication.\n. This is an interesting issue. The NoEcho parameter is intended for CloudFormation to know when it should or should not return parameters back over the wire. From the SDK's perspective, the boolean has no meaning.\nOne option would be to add a customization to the log formatter that made it CloudFormation aware. I'm against this for a number of reasons. Instead, I think the better, long-term option is to make the Seahorse::Client::Logging::Formatter more flexible.\nA user, should be able to register a list of parameter names to black-or-white-list. These would be omitted from request parameter logging. If response data logging were added, this list could be observed there as well.  Filtering headers could be addressed in a similar manner, but with a different list.\nUntil a a feature is added to support this, you can work around it by either providing your own log formatter (which could potentially sub-class the existing one) or simply remove :request_params from the log pattern.\nThoughts?\n. This looks like a reasonable solution for your specific logging problem. I'll add a more generalized solution to the backlog of feature work.\n. Thanks!\n. Can you provide more information about how the file was uploaded? I suspect that is where the encoding was modified. The snippet of code you listed above should download to a new file using binary mode, e.g.:\nruby\nFile.open(file_path, 'wb') { |file| file.write(...) }\n. No problem!\n. The placement availability zone is a little bit different than the region. Each Amazon EC2 region has multiple azs, named with different letter suffixes, e.g. us-east-1a, us-east-1b, etc. The actual region name is \"us-east-1\".\nThe SDK does not attempt to validate the region name because:\n- New regions are brought online frequently and we don't want to force SDK updates to access them.\n- It can be helpful for testing purposes to fake a region\nAs a result, the SDK is following this pattern to build the endpoint:\nruby\n\"ec2.#{region}.amazonaws.com\"\nIf you prune the letter from the end of the region name, you should get what you are looking for. Please re-open if this doesn't resolve your issue.\n. Also, if you didn't know, the SDK will automatically load the instance profile credentials from the IMDS without extra configuration. You should not need to export credentials to ENV. The benefit of letting the SDK grab these on your behalf is that it will automatically refresh credentials from the IMDS before they expire.\n. The region is still required because there is no requirement that you must access AWS endpoints from the region your EC2 instance is running. The SDK will not ever provide a default region. If, by convention, you always access S3, IAM, or whatever service, in the same region as your instance, then pulling the AZ from IMDS is an option, but you do have to remove the suffix.\n. @awood45 Could you give this a look over?\n. Thanks!\n. To apply filters, you pass an array of hashes. Valid hash keys are :name, and :key. The filter :name is one of the filters documented here (click the request parameters tag for the valid filter list): http://docs.aws.amazon.com/sdkforruby/api/Aws/EC2/Client.html#describe_instances-instance_method\nTo filter by a tag name and value pair, its a bit tricky, because the filter name is the string \"tag:#{tag_name}\". For example:\nruby\nec2 = Aws::EC2::Resource.new\ninstances = ec2.instances(filters: [{ name: 'tag:app_type', values:['spark'])\nHope this helps!\n. Paging is only applied on operations that allow you to enumerate a list of resources without side-effects. Receiving messages from a queue has side-effects, removing messages from the queue. The only pageable operation in Aws::SQS::Client is #list_queues: https://github.com/aws/aws-sdk-ruby/blob/master/aws-sdk-core/apis/SQS.paginators.json\nCurrently, every operation is decorated with the pageable response. In theory, this allows customers to code against list operations that aren't pageable yet. Service teams could add paging and customers would start reaping benefits straightway.\nIn practice, I'm not sure this is worth the confusion, especially in this scenario. Receive messages is not a candidate for paging support.\n. You can call #receive_messages in a loop.  Eventually, the response will contain no results. \n. I should also add, that I've been considering building a SQS queue polling abstraction, but I don't know when it would be completed.\n. What version of aws-sdk-resources are you using? This was a limitation of older versions, but not in the current version. Enumerating objects in a bucket should yield ObjectSummary objects that are populated with all of the information returned in the list objects call.\n. To give an example, the following is from the aws.rb REPL. Notice there is only a single API request made to #list_objects, and no follow-up HEAD object requests.\nruby\nAws> s3 = Aws::S3::Resource.new\nAws> s3.bucket('aws-sdk').objects.limit(5).each { |obj| puts obj.last_modified }\n[Aws::S3::Client 200 1.127641 0 retries] list_objects(bucket:\"aws-sdk\",encoding_type:\"url\")  \n2015-01-20 22:04:03 UTC\n2012-12-06 18:32:58 UTC\n2012-12-06 18:32:58 UTC\n2012-12-06 18:32:59 UTC\n2012-12-06 18:32:59 UTC\nPlease feel free to re-open this issue if you feel there is still an error, but I suspect you just need to update.\n. @DanielRedOak The fix is part of the v2 SDK, not v1. Short of upgrading from v1 to v2, might I recomend using AWS::S3::Client from the v1 SDK instead of the mid-level abstraction. This will ensure  n+1 calls are not being made.\n. @DanielRedOak Good to hear. Would you mind sharing some of you experience upgrading from v1 to v2, publicly or privately?\n. Here would be fine.\n. Thanks for the suggestion. This should be part of the next release.\n. From the Amazon S3 API reference docs for GET Object:\n\nThere are times when you want to override certain response header values in a GET response. For example, you might override the Content-Disposition response header value in your GET request.\nYou can override values for a set of response headers using the query parameters listed in the following table. These response header values are sent only on a successful request, that is, when status code 200 OK is returned. The set of headers you can override using these parameters is a subset of the headers that Amazon S3 accepts when you create an object. The response headers that you can override for the GET response are Content-Type, Content-Language, Expires, Cache-Control, Content-Disposition, and Content-Encoding. To override these header values in the GET response, you use the request parameters described in the following table.\nNote\nYou must sign the request, either using an Authorization header or a pre-signed URL, when using these parameters. They cannot be used with an unsigned (anonymous) request.\n\nEmphasis added was mine. This means the #public_url method will not be able to provide what you are looking for. Instead, you should look at the #presigned_url method:\n``` ruby\ns3 = Aws::S3::Resource.new\nobj = s3.bucket('aws-sdk').object('key')\nobj.presigned_url(:get, response_content_disposition: '...')\n=> \"https://aws-sdk.s3.amazonaws.com/key?response-content-disposition=...\"\n```\nBecause this is a presigned url, it will expire. You can specify how long the URL is valid for. By default it expires after 900 seconds (15 minutes).\n. Thanks!\n. The implementation for this method would be fairly straightforward.  Internally it would yield a loaded object, refreshing self until the block returned a true value. The biggest question is how to ensure it has a consistent interface for max attempts, delay, etc to prevent polling indefinitely.\n. @danielsiwiec I like the general approach. I took your idea and refined it a bit. I was interested in a generic solution that would work for all resource types. I pushed a proof-of-concept branch. It lacks automated tests, but it passes all of the smoke tests I ran it through manually.\nhttps://github.com/aws/aws-sdk-ruby/commit/f9e33b4c8b0a3e26e95c61bb84ae046bd9f47355\nSome thoughts:\n- Its difficult to know what a sensible max attempts and delay values to use. Some resources would do better with shorter polling loops, where others would want longer ones. They can be provided as options, which is okay.\n- I opted to have the waiter return the resource object so it can be chained, instead of the response from the load operation: resource.wait_until(&condition).do_something\n- I am using the resource #reload method to force updates on the resource data. Unfortunately, not every resource can be loaded. This is not common, but the code would need to raise some sort of NotImplementedError when attempting to call #wait_until on a resource that can not be refreshed.\nThoughts?\n. I use the attempts counter to prevent the poller from reloading the resource when no more polling attempts will be made. Its more of an optimization. I could have achieved the same result be register a :before_wait callback and then reload from there. I did not do that because I don't want to interfere with the user's optional :before_wait callback.\nThe Client#wait_until returns the response because it has no concept of a resource. It polls an operation for a successful or failure state, returning the response that indicates success. Resource waiter methods have the extra context and return an instance object.\nThis brings up an interesting question. Calling Instance#wait_until_running will not internally reload the instance. Instead it uses the Client#wait_until(:instance_running) method to poll and then constructs and returns a new Instance object. If the response from Client#wait_until happens to return the appropriate data, it will populate the data on the returned instance. This makes it possible to chain calls like:\n``` ruby\ninstance.wait_until_running.state.name\n=> 'running'\n```\nHowever it also means the following is possible:\n``` ruby\nstopped_instance.state.name\n=> 'stopped'\nstopped_instance.start\nrunning_instance = stopped_instance.wait_until_running\nstopped_instance.state.name #=> 'stopped'\nrunning_instance.state.name #=> 'running'\n```\nThe intention was to never update data of a resource unless the user explicitly calls #reload. Does calling a waiter method express that direct intent and should therefore both forms of waiters update the internal data, or should be waiter forms chain a new copy of the resource with updated data. One is thread-safe, the other isn't.\n. Also, the #reload method is documented in the parent class for all resources:\nhttp://docs.aws.amazon.com/sdkforruby/api/Aws/Resources/Resource.html#load-instance_method\nI understand this is not the most obvious thing. I'm considering explicitly calling it out on each resource class.\n. Yes, it makes sense. Currently, I'm inclined to modify the sample to create a copy of the resource and load that instead of mutating the current instance. This would be consistent with the other #wait_until_n methods and it avoids thread safety issues.  Explaining when a resource changes state is now a very clear, \"only when you call #reload\".\n. Sure. It lacks a few of the discussed code changes, documentation with examples, and the test. If you'd like to submit these that would be great.\n. To answer your question about the interface. If I had a do-over, the other waiter methods would not yield the water to the block, there simply wouldn't be a block argument. Short of deprecating Client#wait_until in favor of a similarly named method, that cannot/will not change.\nI glanced through the code changes and they look good. Your copy method works. Does a vanilla dup not also work? I haven't checked this out locally yet, so I'll try to give a more complete review tomorrow.\n. Awesome. I'm going to close this issue, in favor of #744.  I'll hopefully review this tomorrow when I am in the office. Thanks!\n. This is an oversight. I thought I had merged it into master already, but it appears I had only merged from your branch into custom-waiters. I'll take care of that now.\n. It has been merged!\n. The CLI has a set of customized commands the bind multiple API calls to implement these. There is no \"CreateCluster\" and \"TerminateCluster\" API operation. In order to support these methods, we would need to crack open the CLI and see what it is doing. I'm not opposed to this, but it is not as trivial as adding support for an existing API call.\n. Thanks everyone for your feedback. I did speak with one of the CLI engineers and I am correct that these CLI commands are customizations. I'm passing this feedback to the EMR team. Currently there are no plans to implement these features in the AWS SDK for Ruby. Ideally these would be actual API calls eliminating the need to duplicate code logic between SDKs.\n. Thanks!\n. The version 2 SDK currently supports pre-signed PUT but not pre-signed POST. If you want to do a direct upload using a browser, you need a presigned POST. This functionality does exist in version 1 of the SDK. You can use both v1 and v2 together in the same application making it pretty easy to continue using the old-presigned POST utility.\nThe presigned POST docs from v1 are here: http://docs.aws.amazon.com/AWSRubySDK/latest/AWS/S3/PresignedPost.html\nJust add the aws-sdk-v1 gem to your project and then require 'aws-sdk-v1'\n. I should also add that porting the presigned post to v2 is on our backlog.\nhttps://github.com/aws/aws-sdk-ruby/blob/master/FEATURE_REQUESTS.md#amazon-s3-presigned-post\n. Looks good! I added one comment above about the documented namespace for the waiter errors. Feel free also to add an entry into the CHANGELOG.md at the repository root. Its important to keep the change log updated with new features. The chage log is consumed to produce release notes. Feel free also to pass on that, and I'll write the change log entry.\n. Thanks so much for driving this! This should go out with the next release of the SDK.\n. In the v2 SDK, there is a single :endpoint option for configuring all three.\nruby\nAws.config[:s3] = { endpoint: 'http://localhost:4567' }\nIf you prefer to not use the global default config, you can pass the same option to the S3 client or resource constructors:\nruby\ns3 = Aws::S3::Client.new(endpoint: 'http://localhost:4567')\ns3 = Aws::S3::Resource.new(endpoint: 'http://localhost:4567')\n. No problem!\n. I'm inclined to not merge this pull request. The examples are correct as it. They are loading credentials from one of the default locations documented previously in the readme. It is probably worth reviewing the readme to ensure this is more clear.\n. Loren's blog post is relevant only to version of the aws-sdk gem. Version 2 is built on a different code base, and it does not support Ruby 1.8.7 where autoloading was a thread safety issue. \nThe gem is thread safe and you should feel confident uploading file simultaneously using the v2 SDK. If you encounter any issues, we would treat it as a bug.\n. Thanks!\n. I was able to run your example without error. It looks like from the stack trace that you are using v1.39.0 of the aws-sdk. The linked documentation is for v1.63.0. If you upgrade to the latest version within the major version 1, then this should work.\n. Are you seeing Instance#state return the running state name or are you seeing this state from some other source, such as the console? Also, are you reloading the Instance object? I'm just trying to get a picture of how this scenario plays out.\n. I suspect this is simply an eventual consistency issue (which is common). Please update the issue with more information if you continue to run into unexpected issues.\n. @arunthampi The plan is to tag and release v2.0.34 on Thursday. I try to do a weekly update each Thursday unless there is a high priority security fix, bug fix, or API update.\n. Looking at the xml response, the object should be found at the given key from the XML: /uploads/9562e8df-0e1a-4043-ad53-930f96828dea/master.zip. Are you unable to HEAD or get this object after a successful POST?\n. I'm a bit confused. In your linked gist: https://gist.github.com/arunthampi/f4e47fa5a1d2ed1b7abc you show a 201 response. This indicates that your object was successfully uploaded to Amazon S3. If this is correct, then my suspicion is that you are extracting the key or location incorrectly from the response.\nWhen you say that it works with the v1 PresignedPost, can you share a gist of how you are using that? Also, then you say it has a valid response but the file is not uploaded, can you share a gist of how you are checking for the object post-upload?\n. I think I see the problem. In your v1 code you set the key as: \"uploads/#{SecureRandom.uuid}/${filename}\". In your v2 example, you use key(\"/uploads/#{SecureRandom.uuid}/${filename}\"). \nNotice in v2 there is an extra forward slash at the beginning of the key. Depending on the structure of your s3URL on line 37 you may have the incorrect number of slashes. This will really depend on if the S3 URL contains the bucket in the hostname or as part of the request URI.\n. In the project README under Credentials it gives a list of the accepted credential object classes with links to their documentation.\nI'll take a stab at clarifying the README and see about adding a more helpful error message for when credentials are not configured properly.\n. Thank you for your clarification. I'll push shortly an update that should help clarify configuration. Please check it out and let me know if you think it lacks something. There is a difficult balance between brevity/clarity and completeness. \nYes, all you have to do is set those two environment variables, and then you don't have to configure credentials at all.\n. Thank you for the feedback. Writing documentation isn't always an easy task when you are familiar with the tool. Its easy to forget what someone does and does not know.\n. @itsmrwave The SDK intentionally favors AWS_ACCESS_KEY and AWS_SECRET_ACCESS_KEY. These two ENV variables are what all of the AWS SDKs (Java, .NET, PHP, JavaScript, Ruby and CLI) have standardized around. If you use these, you will be golden. This is what is documented and preferred.\nThose same names are used inside the shared credentials file at ~/.aws/credentials, just without the prefix:\n- access_key_id\n- secret_access_key\nAs far as I know, those two are the only supported pairs for all of our SDKs again.\nAny other possible combinations, of which there are many, are not guaranteed to work. A few are called out in the default credential provider chain.  These are:\n- AMAZON_ACCESS_KEY_ID and AMAZON_SECRET_ACCESS_KEY\n- AWS_ACCESS_KEY and AWS_SECRET_KEY\nThese are there for compatibility with some of the legacy CLI tools, those pre-dating the newer unified CLI. Clearly there isn't support for ElasticBeanstalk's pair of AWS_ACCESS_KEY_ID and AWS_SECRET_KEY.  While we could add that pair, I suspect there are still others that we'd be missing.\nIt might be worth a minor refactor to have the credential provider chain search for the access key and secret key separably, instead of as a named pair, allowing more combinations. This would be an additive change and shouldn't affect any existing users negatively.\n. @itsmrwave I've made the change that will allow for alternative paris of the ENV variables: https://github.com/aws/aws-sdk-ruby/commit/d61484bd732fbba4bb16f0d124bb1e6df618e7fe\n. Thanks for reporting the documentation issue. I've removed the documentation for the non-existing features and corrected the names for :key and :description.\n. Have you tested this in your environment to see if it solves the problem? It looks like this will just move the error up to where it attempts to construct an error class from a nil error_code and raise there instead.\nI'm very interested in the situation that can cause the response to have a body, but the SDK is unable to extract an error code. Would you be willing to run a patched version of the SDK that would log the http response body in one of these situations so we can see what is the root cause?\nIf you are, let me know, and I'll put together some code that would collect the needed information.\n. I went ahead an made a small change that will at a minimum prevent errors while attempting to parse the error code and messages and will fall-back on a friendly http status code based error. This should allow us to identify when this happens.\n. Thanks for submitting the feature request. I've added this as a feature request to our public backlog here: https://github.com/aws/aws-sdk-ruby/blob/master/FEATURE_REQUESTS.md#awsecsclient-waiters\n. This has been merged into master.\n. Just a heads up. I hope to check this out later today. Theres quite a bit to review. Thank you for the contributions!\n. Thank you for the contributions!\n. Thanks!\n. I've updated the client class with PutRecords support and this has been released with v1.64.0.\n. Can you confirm this stack trace is from a get object request? The response from the GET request should not involve any XML parsing, unless the HTTP status code was a non ~200 response. If you can access the status code that would be helpful.\nAlso worth nothing, in the version that will be released shorty today, I replaced the XML parser/parsing logic. I've removed MultiXml, to avoid DOM parsing the data multiple times and replaced it with a single-pass SAX parser. This should be faster and may affect or resolve this issue.\nAny additional information you have would be helpful in trying to track down the cause of this issue.\n. The new version is out now. When you update can you please report back if this or any related issues persist?\n.  Can you confirm if this issue persists? If it does, it would be helpful to see the actual error class, message and a full stack trace.\n. I haven't heard anything back. No news is good news, right? :) Please feel free to re-open this issue if you encounter these errors again after the update.\n. Thank you for the bug report and the pointer to the cause. I've put together a fix and added a test to cover the expected behavior. This should go out with the next release shortly.\n. It looks like this is a deficiency of the current implementation. I look into a patch so that explicitly given nil values are preserved.\n. The fix above should got out with the next release. Thanks for reporting the issue!\n. @DanielRedOak I'm really interested in determining the cause and providing a fix. Would you be able to provide a complete XML document for the list object response that causes the failure? I would like to reproduce this locally, and am currently unable to. I'd like to get this patched asap.\n. I was able to produce a failure myself. No need to send the output. No clue what the cause is yet, but I'll update as I find out more.\n. If found the root cause of the issue. It appears that Nokogiri will send potentially multiple text events to the sax handler. For example, given the following XML element:\nxml\n<LastModified>2012-12-06T18:33:01.000Z</LastModified>\nIt will typically trigger a single text event with the value 2012-12-06T18:33:01.000Z as a string. For whatever reason, larger XML documents will cause nokogiri to start emitting two text events, e.g. 2012-12- and 06T18:33:01.000Z, splitting the XML text element up into multiple parts.\nThe fix is to have the stack frame collect all of the yeilded text values and then have them parsed when the result is accessed, joining the parts first.\nThis explains why other string values might be truncated (as only the last one would be returned). This appears to only affect the Nokogiri backend, but I'm going to test the others. I need to add a text case that triggers this an then post a fix.\nI hope to have this patched this afternoon. \nThank you for reporting the issue!\n. The fix was pretty straight-forward. In the sax-parser, when an element has a text value and that element was a known scalar value (e.g. string, timestamp, integer, boolean, etc) it was being type-cast in response to the text callback.  \nNow the text values are collected into a list and they are instead joined when the #result method is called on the stack frame. This allows xml parsers to emit the text in multiple parts and have them joined at the end when the full string is available.\nI added the ability to inject a parsing engine and the tests now exercise a dummy parser engine that splits the sample text into individual characters emitting each as their own text element. This initially broke all of the tests until the related change as made, causing the tests to go green again.\nI'm happy with the change and am inclined to ship this later today. Please feel free to share any feedback you have. Also, if you have a change to checkout the master branch to test the fix, that would be great.\n. This is available now in version v2.0.36.\n. Sorry for the inconvenience. There was a build issue with 2.0.35. It was fixed and patched shortly after release.\n. I am unable to reproduce this issue. Earlier versions of the aws-sdk gem did return Aws::S3::Objects, but this was changed a while ago. Off the cuff I'd say about 2 months ago. Are you certain you are testing with the version you gave? Can you add a puts statement into your example like this:\nruby\nputs Aws::VERSION\nresource = Aws::S3::Resource.new\nbucket = resource.bucket('bucket-name')\nbucket.objects.limit(10).each do |obj|\n  puts \"Obj.class: #{obj.class}\"\nend\nWhen running the example above, I get the expected Aws::S3::ObjectSummary class name and version. Also, a stack trace could be helpful.\n. I'm going to close this as a non-issue. I suspect there is an issue in your environment with how the SDK is loaded. Please do re-open this issue if you can share a failure test case.\n. Looking at the API reference documentation for Amazon EC2, you appear to be correct. There is no Filter by tag:Key=Value.  \nhttp://docs.aws.amazon.com/AWSEC2/latest/APIReference/API_DescribeSecurityGroups.html\nThis is a limitation of the API, but not the SDK. Might I suggest you open a question in the Amazon EC2 forum? https://forums.aws.amazon.com/forum.jspa?forumID=30\n. Thank you for reporting the issue. I verified the bug and just submitted a patch. The fix should be released shortly. Until the fix is available you can use the following workaround:\nruby\npolicy = Aws::IAM::Policy.new(arn, region:'us-east-1')\n. The DynamoDB API makes a distinction between lists and sets. DynamoDB does not accept empty sets. By accepting from the user an empty set and converting it to a list type we've changed the nature of the data structure. This prevents users from round-tripping an item (e.g. GET, modify, PUT) multiple times without possibly introducing bugs.\nI'm open to discussion on how best to handle an empty set. Another possible option is to drop the set.\n. I spent some time working on this. I started by working on a patch that would convert the simplified attribute value formats back into the serialized format with types. This would ensure the response stubbing plugin would succeed at validating the data. Followed by that I had to then also reverse the data format because the normal response handlers were not firing. This was a code-smell to me.\nThe response stubbing plugin correctly exercises all of the request handlers that build the request. Once it reaches the send handler it simply sets the response data and then returns. This prevents any of response handlers from  being exercised. \nThere was a related issue in the previous repo (https://github.com/aws/aws-sdk-core-ruby/issues/197) where the response stub skipped over the response handlers, preventing stubs that sets a response target from sending data to the target IO object.  This specific limitation was addressed then. I can certainly address this new bug with not supporting attribute values but then I feel like there will be something else in the future.\nI think the correct way to resolve this issue is to re-work the response stubbing to take the stub data and to marshal it down to an actual HTTP response with headers, status code and a body. By doing this all of the response handlers will fire as normally ensuring that all of the service customizations work as intended.\nI'm jotting down these notes here. I'm not sure I have time to do the feature work required right now and I want a log of what I discovered.\n. No problem! Thanks for being patient. The new response stubbing implementation should be much more reliable. Let me know if anything does not work as intended.\n. Thank you for the failure example. I've reproduced the issue locally and I'll try to see what the culprit is.\n. So the cause appears to be an interaction between capistrano and the SDK. In this case, I believe that the SDK is behaving properly, and that capistrano is doing a no-no.\nThe SDK is create a sub-class to Aws::Resources::Resource. When requiring the SDK with capistrano loaded, capistrano has done something to include extra base-classes:\nruby\nputs Aws::Resources::Resource.ancestors\nObject\nCapistrano::DSL\nCapistrano::DSL::Stages\nCapistrano::DSL::Paths\nCapistrano::DSL::Env\nCapistrano::TaskEnhancements\nKernel\nBasicObject\nWhen the SDK then attempts to add methods to the resource class it chooses to raise an error if there are method name conflicts. In this case, Capistrano::DSL::Env has a #delete method defined and this is conflicting.\nI would raise this as an issue against capistrano.\n. For demonstration, I modified the lib/a.rb file, adding the following:\nruby\nputs Object.ancestors\nThe output is:\nruby\nObject\nCapistrano::DSL\nCapistrano::DSL::Stages\nCapistrano::DSL::Paths\nCapistrano::DSL::Env\nCapistrano::TaskEnhancements\nKernel\nBasicObject\nFor a little more background, the resource classes are built at runtime from a shared definition. There is an intentional check to ensure that a resource action or association name does not re-define any method already present. This protects against the definitions defining methods like #method, #object_id or anything else in the base class (e.g. #reload).  The intention is to alert us of the conflict so we can provide renames for these conflicts.\nI would ask someone more knowledgeable about Capistrano is it is possible to run it without it monkeypatching the world.\n. I'm afraid that extending BasicObject would be problematic. There are lots of Ruby standard methods that are only available from Object:\nruby\n(Class.new.instance_methods - Class.new(BasicObject).instance_methods).sort\n:!~\n:<=>\n:===\n:=~\n:class\n:clone\n:define_singleton_method\n:display\n:dup\n:enum_for\n:eql?\n:extend\n:freeze\n:frozen?\n:hash\n:inspect\n:instance_of?\n:instance_variable_defined?\n:instance_variable_get\n:instance_variable_set\n:instance_variables\n:is_a?\n:itself\n:kind_of?\n:method\n:methods\n:nil?\n:object_id\n:private_methods\n:protected_methods\n:public_method\n:public_methods\n:public_send\n:remove_instance_variable\n:respond_to?\n:send\n:singleton_class\n:singleton_method\n:singleton_methods\n:taint\n:tainted?\n:tap\n:to_enum\n:to_s\n:trust\n:untaint\n:untrust\n:untrusted?\nI'm open to other options. I'll look around for some other possibilities and come back. As a temporary work-around, you can use aws-sdk-core only.\n. What if the safe_define method only happened during test?\nThe primary goal is to ensure that the resource operation names don't conflict with Ruby standard object methods, like #send. If these conflicts are exposed strictly in development / test environments, then I don't really care if someone has monkey patched object with new functionality. Its fair that my class will clobber or hide that method.\nThoughts?\n. I made a patch to test this and everything seems to work great. To test against my modified SDK, I added the following to the same repo:\n```\ndiff --git a/config/deploy.rb b/config/deploy.rb\nindex dda9215..9e5f52d 100644\n--- a/config/deploy.rb\n+++ b/config/deploy.rb\n@@ -1,6 +1,8 @@\n # config valid only for current version of Capistrano\n lock '3.4.0'\n+$:.unshift('~/aws-sdk-v2/aws-sdk-resources/lib')\n+$:.unshift('~/aws-sdk-v2/aws-sdk-core/lib')\nnamespace :problem do\n   task :a do\n```\nI'm happy enough with the solution, I'm going to push the fix to master and this should go out with the next release.\n. The purpose of #safe_define_method is to detect when the resource models are going to step on regular Ruby methods. If a module is mixed into Object that defines a #delete method and then a resource class needs to define #delete, I'm okay with that. The resource class will get the correct definition of delete. I'm not okay if a resource class for Amazon Simple Email Service were to define a #send method and clobber the default Ruby #send method from Object.\nI'm open to re-evaluate this if we run into further issues. Subclasses BasicObject seems like it would also be a suitable situation, I'm just afraid of the unknown edge cases we might encounter.\n. This is working as intended.  Calling Aws::SQS::Client#get_queue_url is documented to return a response object that has a #queue_url method. This pattern allows the SDK to support additional values when they are added by the service team. If the service team were to add the queue ARN to the response, the structure allows us to add an additional property in a backwards compatible manor.\nYou just need to adjust your code to do the following:\nruby\nsqs_client = Aws::SQS::Client.new(logger: nil)\nresp = sqs_client.get_queue_url(queue_name: ENV['AWS_QUEUE'])\nqueue_url = resp.queue_url\n. There are tests that should prevent regressions on that issue. Can you provide a code example that demonstrates the issue you are having? \n. Thank you for the example code. Can you try your example again after removing the #to_h call on the distribution config?\nruby\ncf.update_distribution({\n  id: resp.distribution[:id],\n  distribution_config: resp.distribution[:distribution_config,\n})\nAlso, the response data is a set of structures, so you don't need to hash-index the values. You can use Ruby methods:\nruby\ncf.update_distribution({\n  id: resp.distribution.id,\n  distribution_config: resp.distribution.distribution_config,\n})\nBy calling #to_h I got the same error as you. I'm pretty certain I know why. It has to do with the empty list values and if they should be serialized or not.\n. I'm not opposed to using the minor vs the patch for some of our update. That said, the majority of the changes to this repository are:\n- Service API additions.\n- Bug fixes.\nMost releases are not feature additions. Bug fixes belong as patch releases. The API updates can be argued as patches or features. The reasons I've been using patch versions for API updates is that they do not require code changes. We load a JSON document that describes the service API. The runtime does not need to change to support these updates.\nIs there a specific update or feature addition that was no backwards compatible that is causing you an issue? Again, I'm not opposed to doing bumps to the minor version when new features have been added, but upgrading issues should be treated as bugs/regressions. Please feel free to open a GitHub issue if you run into something.\n. Please feel free to still respond. I'm closing this as a non-issue. I am still interested in feedback on what updates may have caused problems.\n. I suspect you can remove your monkey-patch and simply remove the SQSQueueUrls plugin from the SDK while under test.\n``` ruby\nin your spec/test helper\nAws::SQS::Client.remove_plugin(Aws::Plugins::SQSQueueUrls)\n```\nThis will prevent the client from redirecting the request to the given queue URL. Let me know if this does not work.\n. This is a known issue that has been resolved here: https://github.com/aws/aws-sdk-ruby/commit/c02b5e7646319034e91fb9d9eca6c80d900ae214. This fix should be released shortly.\n. This ended up being a trivial fix. The Ox sax parser has a mode were values are decoded before being emitted. This should go out with today's release.\nThank you for reporting this bug!\n. Thank you for the feature request. I've added it to the public backlog here: https://github.com/aws/aws-sdk-ruby/blob/master/FEATURE_REQUESTS.md#paperclip-integration\n. The aws-sdk gem does not attempt to control the SSL version used by Net::HTTP. As far as I know, open ssl should negotiate TLS during the handshake.\n. Can you share a code example of how you are calling #create_job that triggers this behavior? This particular operation is not pageable and should not trigger page enumeration.\n. @StaymanHou The pageable response is a decorator of a vanilla response object. Many AWS services will truncate lists of results to limit response sizes.  A pageable response makes a response enumerable, allowing it to fetch the next page of results.  When enumerating all responses the page must yield itself first before fetching the next page.\n. Sorry for the slow response. I've been out of town at RailsConf. I took some time to reproduce this issue and to track the root cause. I'll try to update with more information tomorrow, but I have a good idea of the root cause for the leak and some possible work-arounds.\n. It appears that StringIO is the culprit from Ruby stdlib. The SDK expects that the http handler should write the http response body to an IO object. The default response target is a StringIO object. The following snippet when run from a script will demonstrate the leak.\n``` ruby\nrequire 'stringio'\ndef report\n  GC.start\n  memory = ps -o rss,vsz -p #{Process.pid} | tail +2.strip\n  leaks = leaks #{Process.pid} | grep -c Leak.strip\n  puts \"Memory: #{memory}; Leaks: #{leaks}; Heap: #{GC.stat[:heap_live_slots]}\"\nend\ndef leak(data)\n  io = StringIO.new\n  io.write(data)\nend\ndef no_leak(data)\n  StringIO.new(data)\nend\ndata = '.' * 1024 * 1024 * 10 # 10MB data\n20.times do\n  leak(data)\n  report\nend\n```\nBy removing the StringIO#write call, and replacing it with say StringIO.new the leak goes away. I'm going to be raising this with the Ruby core team. If I have more time tomorrow, I'll try to share a code snippet that can work-around this issue for now.\n. The only places a custom body is passed into response are:\n- When you provide once as the response target. This can happen when you use Aws::S3::Client#get_object:\nruby\n  s3 = Aws::S3::Client.new\n  File.open('target', 'wb') do |file|\n    s3.get_object(bucket:'name', key:'key', response_target:file)\n  end\n- When you pass a block to the same method, the response target is replaced with a block yielder.\nI was working on a drop-in replacement for StringIO. Here is what I have so-far. It implements all of the public interfaces of StringIO required by the SDK. It needs additional testing, but should be functional:\n``` ruby\nclass CustomIO\ndef initialize(data = '')\n    @data = data\n    @offset = 0\n  end\ndef write(data)\n    @data << data\n    data.bytesize\n  end\ndef read(bytes = nil, output_buffer = nil)\n    if bytes\n      data = partial_read(bytes)\n    else\n      data = full_read\n    end\n    output_buffer ? output_buffer.replace(data || '') : data\n  end\ndef rewind\n    @offset = 0\n  end\ndef truncate(bytes)\n    @data = @data[0,bytes]\n    bytes\n  end\nprivate\ndef partial_read(bytes)\n    if @offset >= @data.bytesize\n      nil\n    else\n      data = @data[@offset,@offset+bytes]\n      bump_offset(bytes)\n      data\n    end\n  end\ndef full_read\n    data = @offset == 0 ? @data : @data[@offset,-1]\n    @offset = @data.bytesize\n    data\n  end\ndef bump_offset(bytes)\n    @offset = [@data.bytesize, @offset + bytes].min\n  end\nend\n```\n. I was putting together a bug report for Ruby and found a related issue already opened and then resolved: https://bugs.ruby-lang.org/issues/10942\nIt was closed as fixed 7 days ago. I don't know when this will become available, but the fact the same issue was reported and then fixed is good news.\n. Thanks for posting the feature request. I'm out traveling for the week, but lets continue the conversation early next week.\n. @bilus I was previously looking at this. I have a local proof of concept branch which lacks tests. I got to the point where I needed a live bucket in S3 with a cname and I lacked one at the time. I need to circle back on this, but haven't had the opportunity too.\n. I'm not against exploring this. When this was implemented initially, I purposefully chose to not support types that DynamoDB did not explicitly support. For example, DynamoDB does not have a native date or time attribute value. If I had made a customization to marshal ruby Time objects to a string value under a certain format then I would not be able to migrate to the new DynamoDB time attribute value without forcing users to migrate data.\nAnother consideration is that you will not be able to round-trip a hash with symbolized values and get back an equivalent hash. The values will have been stringified. This is probably okay, but it might surprise some users. \nGenerally, I think that Symbol is safe from having DynamoDB implement a native attribute value type.\nThoughts?\n. Ping. Any comments?\n. Sorry for sitting on this for so long. I've decided to merge this in. I don't see any possible path that DynamoDB could add a type that would need to map directly to a Ruby Symbol, leaving this open for string values. The SDK will however never attempt to symbolize strings on the way back out. This is for connivence, on setting values as they will not round-trip as symbols.\nThanks for the contribution.\n. Thank you for reporting this issue. I'll try to take a look at this shortly.\n. I'll try to take a look at this shortly and see if I can reproduce this issue.\n. Are you getting this error for all AWS requests, or only for this operation with Route53? Also, could you verify your credentials are valid? Try the following:\nruby\nr53 = Aws::Route53::Client.new(region: 'us-west-1')\nputs r53.config.credentials.access_key_id\nputs r53.config.credentials.secret_access_key\nWithout sharing these values, can you verify they are the pair you expect?\n. Are you getting this error for all AWS requests, or only for this operation with Route53? Also, could you enable wire logging and share the output?\nruby\nr53 = Aws::Route53::Client.new(region: 'us-west-1', http_wire_trace:true)\n. It appears that you are sending an empty value for the hosted zone id. From the wire log:\nPOST /2013-04-01/hostedzone//rrset/\nI would have expected something more like:\nPOST /2013-04-01/hostedzone/someid/rrset/\n. The AWS SDK for Ruby requires a region for all services. This is necessary to prevent issues when services that have a single global endpoint regionalize. You can set the region once with Aws.config so that you don't need to pass a region to the client constructor.\nruby\nAws.config[:rotue53] = { region: 'us-east-1' }\n. @sferik @lsegal I deleted them as they weren't adding any value.\n. Continuing the discussion from: https://github.com/trevorrowe/jmespath.rb/issues/6#issuecomment-95875818\n\n\nGiven the small surface area required, I would like to explore removing the MultiJson dependency, while preserving the faster parsing options until at such time as Oj becomes standard.\n\nIf you\u2019re able to achieve this, I\u2019d encourage you to contribute this code to MultiJson.\n\nWhat I am considering:\n- Add a pair of non-public utility method to the SDK. Name these method very un-originally, something like Aws.load_json and Aws.dump_json. \n- Update the SDK to use these when marshaling and unmarhsaling JSON payloads and when loading the large API definitions. Use these for performance sensitive runtime code paths.\nA simple implementation of this module could be:\n``` ruby\nmodule Aws\n  # @api private\n  module Util\n    class << self\n  def load_json(json)\n    ENGINE.load(json)\n  end\n\n  def dump_json(obj)\n    ENGINE.dump(obj)\n  end\n\n  def oj_engine\n    require 'oj'\n    Oj\n  rescue LoadError\n    nil\n  end\n\n  def json_engine\n    require 'json'\n    JSON\n  end\n\nend\n\nENGINE = oj_engine || json_engine\n\nend\nend\n```\nThis is simplified version of what I do in place of using multi_xml: https://github.com/aws/aws-sdk-ruby/blob/master/aws-sdk-core/lib/aws-sdk-core/xml/parser.rb#L74.\n. @sferik I've addressed this in a branch that is being worked for v2.1.0 of the SDK. You can see my relevant changes here: https://github.com/aws/aws-sdk-ruby/blob/version-2.1-refactor/aws-sdk-core/lib/aws-sdk-core/json.rb#L13-L56\nThis provides the shim we need to allow users to leverage Oj for performance while maintaining only the minimum interfaces.\n. Do you know what API operation to poll in this case to verify when an instance it out of a load balancer?\n. Added the waiter. This should go out with the next release. Thanks for your patience.\n. This does not look to be intended behavior. It should support an IO object, such as a File. I'll try to look at this shortly.\n. I've been unable to reproduce this issue.  I can read the response into memory, or to a file using the response target path or IO object. Here is what I did:\n``` ruby\nrequire 'openssl'\nkey = OpenSSL::PKey::RSA.new(1024)\ns3 = Aws::S3::Encryption::Client.new(encryption_key: key)\ns3.put_object(bucket:'aws-sdk', key:'secret', body:'handshake')\nresp = s3.get_object(bucket:'aws-sdk', key:'secret')\nresp.body.read\n=> \"handshake\"\ns3.get_object(bucket:'aws-sdk', key:'secret', response_target:'./out')\nFile.read('./out')\n=> \"handshake\"\nFile.open('./out2', 'wb') do |file|\n  s3.get_object(bucket:'aws-sdk', key:'secret', response_target:file)\nend\nFile.read('./out2')\n=> \"handshake\"\n```\nCan you share an example of what is failing for the encryption client?\n. I added the block option to #get_object. Please note, using this form will disable retries of failed download. It is not possible for the SDK to know how these byes are consumed and it can not truncate this data sink like it can a file when it has to retry.\nPotentially in the future we could explore adding retries with a ranged get, but this is a much greater undertaking.\n. Can you please share an example of what you are doing that is triggering this error. I attempted to reproduce this with Amazon S3 and did not produce the failure:\nruby\ns3 = Aws::S3::Client.new(signature_verison: 'v4')\ns3.put_object(bucket:'aws-sdk', key:'empty', body:'')\ns3.put_object(bucket:'aws-sdk', key:'empty', body:nil)\n. Hmm... thats curious. I ran the following in my terminal\n$ touch empty\nAnd then the following from ruby:\nruby\nS3::Object.new('aws-sdk', 'empty-file').upload_file('./empty')\nAnd I received no error. Can you modify your application to rescue the error? I'd like to see the expanded error message and some of the request/response context:\nruby\nbegin\n  obj.upload_file(file_name)\nrescue => e\n  puts e.code\n  puts e.message\n  puts e.context.http_response.body_contents.inspect\n  puts e.context.http_request.headers.inspect\n  puts e.context.http_request.body_contents.inspect\nend\n. Simply replace the following line your shell script with the snippet above:\nruby\nobj.upload_file(file_name)\n. So I think I know what the issue may be. The bucket name you are using in your example is given as vikas-bucket/vikas. I realized that this is not a valid bucket name outside the classic region and you are using us-west-1 which requires bucket names to be DNS compatible. This prohibits the use of forward-slashes.\nI suspect if you change your bucket_name variable to vikas-bucket and your object key can contain the vikas prefix and as many slashes as you need.\nThe signature error happens as the SDK has to put the non-dns compatible bucket name into the path instead of as a subdomain. Amazon S3 computes a different canonical request which causes the error. At this point, the only way the SDK could deal with this is if attempted to validate the :bucket param for clients outside us-east-1.\n. Thank you for reporting the issue. The #exists? check works by making a HEAD request to the bucket. Because we do not know the actual region of the bucket, that may not exist at all, the HEAD request is being sent to the currently configured region for the Aws::S3::Client.\nIt looks like Amazon S3 returns 301 permanent redirects for existing buckets when you head it in the wrong region. The fix would be to treat this status code as a successful check. Yes the bucket exists.\n. The fix was very straight forward. I've added a test to cover regressions. This should go out later this week. Thank you for submitting the bug report and for your patience!\n. Looking at the linked issue, it appears that this is a limitation of Amazon S3, and not the SDK. Is my understanding correct? Similar to how you can request that the keys be URI encoded in a list objects response, you would need to specify the keys are URI encoded in a delete objects batch request so that S3 knows to decode them server-side.\nI can follow up with the Java team and see if they have raised this issue/question with Amazon S3.\n. The difference between Aws::S3::Client#delete_object and #delete_objects is that the key is sent in the request uri for the former and in the request body XML for the latter. The request uri is encoded allowing the special character to be correctly identified by Amazon S3. \nThese particular characters are not valid in XML and seem to be omitted service side. The list object response has the same issue. To work around this, Amazon S3 provides a request parameter for list objects that indicates the keys should be URI encoded inside the XML. The Ruby SDK in-fact does this for EVERY list object request and then decode the keys before returning them to the user. This ensures the XML parser will not raise.\nI'm using Builder from stdlib to build the XML. As far as I know, there are no valid xml entities for these special characters. The only known work-around at this time is to not send them in a delete objects request and use delete object instead. I spoke with the Java SDK engineers and they indicated this is still an outstanding issue. I can raise it with Amazon S3 and see what they suggest.\n\nThe trouble is that it silently fails. Is there something that can be done in the SDK so there is an actual failure?\n\nHave you checked the response to see if these keys are listed in the errors list? I suspect they are not, but it would be worth looking at. My guess is that Amazon S3 is ignoring these invalid XML characters and that results in a different key, potentially to an object that does not exist. I agree this is problematic.\nThe only way I can think of to detect this client-side would be to generate a list of special characters to scan for and send them individually.  Thoughts?\n. I think the best case scenario is that Amazon S3 adds the ability for API users to indicate they have encoded the keys, similar to the list objects solution. As mentioned, I'll raise this with them.\n. This exists in v2 on Aws::S3::Object.\n``` ruby\ns3 = Aws::S3::Resource.new\nobj = s3.bucket('name').object('key')\nobj.exists?\n=> true/false\n```\nLet me know if this isn't what you are looking for.\n. @pdrakeweb Thanks for the contribution. I'd like to understand a bit more about the situations that cause this error. I'm currently investigating an issue with Net::HTTP which I suspect may be the root cause of these errors. Do you have any more information about when these happen and perhaps a stack trace or any logs?\n. I spent a non-trivial amount of time attempting to reproduce this issue and I can across an issue in how Net::HTTP handles networking errors and automatic retries. The commit above should resolve this issue.\n. Yes. I have a write-up with suggestions, possible workarounds. I'm currently going through approval internally so that I can contribute these. There are related issues that affect the payload of a PUT request, stemming from the same issue. Lastly, there is a long-standing Expect 100-continue bug that I will be submitting a patch for. Three Net::HTTP issues in total.\n. @tdg5 If I recall correctly, the documentation on submitting issues indicated that I could do so via the issue tracker or via GitHub. I opted to go the GitHub route so that I could submit with suggested code-changes.  It did not seem to have received any attention and then it fell off my radar.\nI'll go ahead an ping those issues and if I don't get any response I'll open issues on the Ruby issue tracker.\n. Sorry for the confusion. I just pushed a commit that will correct the documentation. You can accomplish what you'd like this way:\nruby\ns3 = Aws::S3::Client.new(region:'...', credentials: ...)\ns3 = Aws::S3::Encryption::Client(client: s3)\n. I've added the ability for you to pass client construction options directly to the encryption client. This allows you to now do the following without using the :client option.\nruby\ns3 = Aws::S3::Encryption::Client(region:'...', credentials: ...)\n. Thanks for reporting the issue. I hope this update helps.\n. Thanks.\n. Closed as a duplicate of https://github.com/aws/aws-sdk-ruby/issues/800\nThe workaround it to construct a vanilla Aws::S3::Client and pass that in via :client. I corrected the documentation on this, and then expanded the initialize method to accept vanilla client options as well, such as :region.\n. Sorry for the confusion.\n. I've added this to my backlog and this will hopefully be part of the 2.1 release.\n. Thanks for reporting this issue. The client pagination configuration was missing an entry for the ListPolicies API operation. The commit above adds this and should be released later this week.\n. And yes, after the update, your code should correctly enumerate all policies.\n. I was able to gem install aws-sdk --version=1.64.0 and then from IRB require the library via require 'aws-sdk' and alternatively via require 'aws-sdk-v1'.  I then downloaded the gem file from RubyGems.org to see if the file is missing.  This is what I did:\n$ mkdir tmp && cd tmp\n$ wget https://rubygems.org/downloads/aws-sdk-v1-1.64.0.gem\n$ tar xzvf aws-sdk-v1-1.64.0.gem \n$ tar xzvf data.tar.gz\n$ ls lib/\naws     aws-sdk-v1.rb   aws.rb\nAs far as I can tell, the public gem artifact for v1.64.0 is not missing this file. Can you share what you are doing that causes this error?\n. That is intentional. The lib/aws-sdk.rb is only available in v1.64.0 of the aws-sdk gem, not in the aws-sdk-v1 gem.\nThis is what makes it possible to load v1 and v2 in the same process. If you replace your dependency on aws-sdk < 2.0 with aws-sdk-v1 your require statement needs to be require 'aws-sdk-v1'.\n. See the relevant line in the gemspec:\nhttps://github.com/aws/aws-sdk-ruby/blob/aws-sdk-v1/aws-sdk-v1.gemspec#L25\n. I haven't heard any reports of issues with pre-signed URLs expiring early. In the generated url, there should be an expires query parameter.\nhttps://bucket.s3.amazonaws.com/key?AWSAccessKeyId=AKIA...&Expires=1432106071&Signature=Atb47Bskjc9DAfqHbbDeTVGdNys%3D\nThe number is a unix timestamp (epoch seconds). Is that date correct for the URL you have generated?\n. Did you have a chance to verify if the URL work or had any issues?\n. I strongly suspect the expiration of the credentials used to sign the URL are the cause of the failure with the pre-signed request.\nI haven't dug into this, but I suspect you could give the instance profile role privileges to call the STS assume role operation with a longer expiration. The assumed role could be scoped to the appropriate S3 operations needed for the pre-signed URL.\n. There is a backlog item to improve these docs, but the option is documented in the client method linked to from the see also tags below: http://docs.aws.amazon.com/sdkforruby/api/Aws/EC2/Client.html#run_instances-instance_method\nYou pass a hash with a single entry, either :name or :arn. This is the IAM instance profile name or profile arn.\n. I was looking into this and unable to reproduce this error. Can you do me a favor an enable HTTP wire logging so I can see the XML response that is not being parsed properly?\nruby\n@client = Aws::CloudFormation::Client.new(\n  :region      => @region,\n  :credentials => @credentials,\n  :http_wire_trace => true\n)\n. I've attempt to recreate this failure locally and I've had no luck. Do you have an environment where this error is reproducible? If so, would be willing to sync up with me over Google hangouts to debug this together?\n. That is curious that it would match up to your Util module. My understanding is that it should have found Util inside Seahorse::Util first. If this were not the case, I would need to use fully-qualified namespaces everywhere to avoid possible name conflicts.\nI'll take a stab a reproducing this. Thank you for the update.\n. @rdark See that now. I'm glad you were able to resolve it!\n. Looking at the API reference documentation for GenerateDataKey it appears this API is meant for generating one-time use encryption keys. These keys should be used to encrypt data and then you store the encrypted data with the encrypted version of this key. You can then use your KMS key id to decrypt the data key later. This is very similar to the envelope encryption pattern used by the client side encryption. Unfortunately, it means this data key will not match that of the object. \nI think the best route here is to mimic Java's implementation of client-side encryption with KMS. I'm inclined to do this work, I'm just not sure when I'll be able to hop on this. I'm wrapping up some work on the 2.1 version of the SDK, and then I should be able to prioritize this.\nI'd be happy thought to give assistance to anyone trying to work around this until a generalized implementation can be added. \nBasically you need to HEAD the object to get the encryption envelope. In the metadata of the object (the material description), it should contain the KMS key id used to encrypt the envelope. Pass the encrypted envelope and the kms key id to Aws::KMS::Client#decrypt and you will get back the actual key that was used to encrypt the object. Now you can create a decryption cipher. Stream data from your encrypted object from S3 through the cipher and you should finally have your object plaintext.\nThis shouldn't be too hard to cobble together. Its a bit more work to augment the existing encryption client to simply accept a KMS key id and to do all of this. Primarly because KMS does not ever return the master key (only its id). This means you have to use #encrypt and #decrypt via Aws::KMS::Client.\n. I've pushed a work-in-progress branch of the SDK that adds support for client-side encryption with KMS.\nhttps://github.com/aws/aws-sdk-ruby/tree/s3-cse-kms\nHere is a quick example:\nruby\ns3 = Aws::S3::Encryption::Client.new(kms_key_id:'...')\ns3.put_object(...)\ns3.get_object(...)\n. I've got a working implementation with tests now available in the s3-cse-kms. If you'd like to check it out and take it for a spin, it should be ready. Otherwise, I suspect that I will be shipping this with a release later this week.\n. Sorry for the radio silence on this. This feature has been released, a few versions back actually. You should be able to update and use it now. Thanks for the patience.\n. The SDK documentation is pulled directly from the ECS API reference documentation. Might I recomend you leave feedback with the ECS documentation? There are \"Did this page help you links\" on all of the API reference documentation. Any changes made there will trickle down to the Ruby SDK reference docs.\n. That is an excellent suggestion. I'm currently working on a major update to the reference documentation. I'll see what I can do about created syntax/formatting examples for the resource interfaces as well.\n. Have you raised this issue with support or on the Amazon EC2 forums? This does not appear to be an AWS SDK for Ruby issue.\nEC2 forums:\nhttps://forums.aws.amazon.com/forum.jspa?forumID=30\n. I did the following and it worked as expected:\n``` ruby\nitems = (1..25).map do |n|\n  {\n    put_request: {\n      item: {\n        \"id\" => \"value#{n}\"\n      }\n    }\n  }\nend\nunprocessed_items = nil\n100.times do\n  # the target table I created with 1 write capacity units to ensure I will be throttled on batch write\n  r = dynamodb.batch_write_item(request_items: { \"aws-sdk-slow\": items })\n  if r.unprocessed_items.count > 0\n    unprocessed_items = r.unprocessed_items\n    break\n  end\nend\ndynamodb.batch_write_item(request_items: unprocessed_items)\n```\nCan you share an example of what you are doing that requires the customization above?\n. Can you pretty-print or inspect the response.unprocessed_items in your loop once so that I can see what they look like?\n. Yup, thats what I was looking for. I didn't notice the wide scroll. Have you potentially disabled parameter conversion when you construct the client?\nAlso, as a work-around, this should perform the nested conversion you require without having to white-list specific keys for the string maps.\nruby\nrequest_items = response.data.to_h[:unprocessed_items]\nI want to stress that this should not be required and if there is a bug when param conversion is disabled, then I'll want to address this.\n. I've been able now to reproduce the issue you are experiencing by disabling param conversion. If you re-enable the :convert_params (this is the default behavior) then your code should work without the hash conversion.\nParam conversion allows you to pass in a wider set of input values. For example, it allows you to pass in strings for dates and it will attempt to parse then into Time objects when necessary. When you disable param conversion, you are required to supply the proper type, e.g. Time. In this case, the param conversion is what was converting the structs down to simple hashes.\nThat said, the param validator should be updated to work with structs natively without conversion. I'll take a look at this. For now, my recommendation would be to set convert_params to true and you should be good.\n. The commit above is available in the version-2.1-release branch. This will be merged onto master and tagged as  v2.1.0. I'm not sure when this will happen. There are few other missing features for the 2.1 milestone. \nThe fix makes it possible to round-trip the response data structures as input without parameter conversion. Until then, enabling parameter conversion or calling #to_h on the response data will work around the issue.\nThank you for reporting the issue.\n. There was definitely a change to the pre-signed URL implementation were request headers were no-longer being hoisted to the query string.\nMy understanding is that these values are ignored by Amazon S3 if they are sent in the querystring and not sent as request headers. The change was to be a bug-fix not move them to query params and leave them as headers when generating the signature for the pre-signed URLs.\nAs a result of this, you must send those values as headers when you make the pre-signed request. I'm going to go back and verify this is correct and I'll update here with more information.\n. Thanks for reporting the issue. This fix will go out with our next release.\n. I just ran the following command locally:\nbash\n$ gem install aws-sdk --version=2.0.43\nFetching: aws-sdk-core-2.0.43.gem (100%)\nSuccessfully installed aws-sdk-core-2.0.43\nFetching: aws-sdk-resources-2.0.43.gem (100%)\nSuccessfully installed aws-sdk-resources-2.0.43\nFetching: aws-sdk-2.0.43.gem (100%)\nSuccessfully installed aws-sdk-2.0.43\n3 gems installed\nIt sounds like there was an issue while downloading the gem. Have you tried removing all of the files and then run gem install again?\n. I've received no other reports of a corrupt gem. I'm going to close this issue. Please feel free to re-open if you have additional information or if you can reproduce this.\n. I'll take a look at getting this fixed. My guess is merged changes did not get merged upstream. When @awood45 pulled the updated API files for Thursday's release, is probably when these got reverted. My apologies.\n. It looks like it was merged and shipped with v2.0.39 and then lost in v2.0.40. The offending commit is here: https://github.com/aws/aws-sdk-ruby/commit/f0fad0a038a4f81b0a697f7c367fd2735d9dd698. I've gone back and merged this upstream. The fix will be shipped today. I also audited all of the other changes made in your pull-request. The other changes have been merged up-stream and are intact.\nSorry for the confusion and inconvenience. We are currently looking at the process on how we update and maintain some of these shared metadata files. As the Ruby SDK is not the source, we need a better process to ensure we don't have regressions like this in the future. \n. Can you share a code example that reproduces the error? Also, can you share how you have configured your S3 client?\n. @pamio So the bucket did not exist and creating it allowed the upload file to work? If that is the cause, I'm suprised you didn't just get a 404 NoSuchBucket error.\n. It looks like you are checking for the existance of one bucket, and creating a different one when it does not exist:\nruby\nif !s3.buckets['gs-db-backups'].exists?\n   bucket = s3.buckets.create('backups')\nend\nYou reference gs-db-backups and backups. Is that intentional?\n. I responded with a pull request to your fork here: https://github.com/alkesh26/aws-sdk-ruby/pull/1 - This addresses the parser errors. The root cause is that a nil value is being passed into the #list and #map methods preventing the defaults to setup from being used.  Instead, nil values are used which don't respond to #[]= or #<<.\nIf you merge those changes, I'll be happy to these these.\n. Sure thing. When defining a Ruby method with a default value, that value is only used when the agument is not given.  For example:\n``` ruby\ndef example(value = \"default-value\")\n  value\nend\nexample\n=> \"example-value\"\nexample(nil)\n=> nil\n```\nThe JSON parser was always passing the target parameter. This prevented the default values of [] and {} from ever being used. The patch I sent to your branch modified the parser to not send nil values. The fix above also resolves this.\n. Thanks for the contribution!\n. Thanks for reporting the issue. The fix was a one-liner. The tests did not correctly catch this because they were asserting the wrong operation name was to be called. I've updated the test to not assert a specific method is called. Instead, I allow the client to receive and respond and to assert on the response.\nSorry for the inconvenience. The fix for this should be released later this week.\n. Its possible but there are few signifcant caveats currently:\n- You must provide the protocol structure for DynamoDB which I'll demonstrate below. Its a bit awkward.\n- Passing the structures expected by DynamoDB's protocol will bypass the initial issue, but it will also fail to return the simplified response values you probably expect.\n- Because the currently implementation is a bug, when it is fixed to work as expected, this work-around will stop working.\nWith all of those disclaimer out of the way, the following example demonstrates stubbing all of the support types by DynamoDB. Note that the list, :l and map, :m entries may be recursive as they support mixed types and can be nested arbitrarily deep. Essentially every value is wrapped in an object that decorates its type.\n``` ruby\nddb = Aws::DynamoDB::Client.new(stub_responses:true)\nddb.stub_responses(:query, items: [\n  {\n'string' => { s: 'value' }, \n'string-set' => { ss: ['a', 'b', 'c'] },\n\n'number' => { n: '10.0' },\n'number-set' => { ns: ['10', '11', '12'] },\n\n'binary' => { b: 'data' },\n'binary-set' => { bs: ['data1', 'data2'] },\n\n'boolean' => { bool: true },\n'null' => { null: true },\n\n# lists, unlike sets allow mixed types\n'list' => { \n  l: [\n    { s: 'value' },\n    { n: '10' },\n    { b: 'data' },\n    { bool: false },\n    { null: true },\n    { l: [] },\n    { m: {} },\n  ]\n},\n\n# maps\n'map' => {\n  m: {\n    'string' => { s: 'value' },\n    'number' => { n: '10' },\n    'binary' => { b: 'data' },\n    'boolean' => { bool: false },\n    'null' => { null: true },\n    'list' => { l: [] },\n    'map' => { m: {} },\n  }\n}\n\n}\n], count: 1)\n```\nThe response will not look like what you would expect because the simplified attributes plugin does not yet support stubbing. Here is what you could expect back. Notice the number attributes are always populated.\nruby\nddb.query(table_name:\"table-name\")  \n=> #<struct Aws::DynamoDB::Types::QueryOutput\n items=\n  [{\"string\"=>#<struct Aws::DynamoDB::Types::AttributeValue s=\"value\", n=\"NumberAttributeValue\", b=nil, ss=[], ns=[], bs=[], m={}, l=[], null=false, bool=false>,\n    \"string-set\"=>#<struct Aws::DynamoDB::Types::AttributeValue s=\"StringAttributeValue\", n=\"NumberAttributeValue\", b=nil, ss=[\"a\", \"b\", \"c\"], ns=[], bs=[], m={}, l=[], null=false, bool=false>,\n    \"number\"=>#<struct Aws::DynamoDB::Types::AttributeValue s=\"StringAttributeValue\", n=\"10.0\", b=nil, ss=[], ns=[], bs=[], m={}, l=[], null=false, bool=false>,\n    \"number-set\"=>#<struct Aws::DynamoDB::Types::AttributeValue s=\"StringAttributeValue\", n=\"NumberAttributeValue\", b=nil, ss=[], ns=[\"10\", \"11\", \"12\"], bs=[], m={}, l=[], null=false, bool=false>,\n    \"binary\"=>#<struct Aws::DynamoDB::Types::AttributeValue s=\"StringAttributeValue\", n=\"NumberAttributeValue\", b=\"data\", ss=[], ns=[], bs=[], m={}, l=[], null=false, bool=false>,\n    \"binary-set\"=>#<struct Aws::DynamoDB::Types::AttributeValue s=\"StringAttributeValue\", n=\"NumberAttributeValue\", b=nil, ss=[], ns=[], bs=[\"data1\", \"data2\"], m={}, l=[], null=false, bool=false>,\n    \"boolean\"=>#<struct Aws::DynamoDB::Types::AttributeValue s=\"StringAttributeValue\", n=\"NumberAttributeValue\", b=nil, ss=[], ns=[], bs=[], m={}, l=[], null=false, bool=true>,\n    \"null\"=>#<struct Aws::DynamoDB::Types::AttributeValue s=\"StringAttributeValue\", n=\"NumberAttributeValue\", b=nil, ss=[], ns=[], bs=[], m={}, l=[], null=true, bool=false>,\n    \"list\"=>\n     #<struct Aws::DynamoDB::Types::AttributeValue\n      s=\"StringAttributeValue\",\n      n=\"NumberAttributeValue\",\n      b=nil,\n      ss=[],\n      ns=[],\n      bs=[],\n      m={},\n      l=\n       [#<struct Aws::DynamoDB::Types::AttributeValue s=\"value\", n=\"NumberAttributeValue\", b=nil, ss=[], ns=[], bs=[], m={}, l=[], null=false, bool=false>,\n        #<struct Aws::DynamoDB::Types::AttributeValue s=\"StringAttributeValue\", n=\"10\", b=nil, ss=[], ns=[], bs=[], m={}, l=[], null=false, bool=false>,\n        #<struct Aws::DynamoDB::Types::AttributeValue s=\"StringAttributeValue\", n=\"NumberAttributeValue\", b=\"data\", ss=[], ns=[], bs=[], m={}, l=[], null=false, bool=false>,\n        #<struct Aws::DynamoDB::Types::AttributeValue s=\"StringAttributeValue\", n=\"NumberAttributeValue\", b=nil, ss=[], ns=[], bs=[], m={}, l=[], null=false, bool=false>,\n        #<struct Aws::DynamoDB::Types::AttributeValue s=\"StringAttributeValue\", n=\"NumberAttributeValue\", b=nil, ss=[], ns=[], bs=[], m={}, l=[], null=true, bool=false>,\n        #<struct Aws::DynamoDB::Types::AttributeValue s=\"StringAttributeValue\", n=\"NumberAttributeValue\", b=nil, ss=[], ns=[], bs=[], m={}, l=[], null=false, bool=false>,\n        #<struct Aws::DynamoDB::Types::AttributeValue s=\"StringAttributeValue\", n=\"NumberAttributeValue\", b=nil, ss=[], ns=[], bs=[], m={}, l=[], null=false, bool=false>],\n      null=false,\n      bool=false>,\n    \"map\"=>\n     #<struct Aws::DynamoDB::Types::AttributeValue\n      s=\"StringAttributeValue\",\n      n=\"NumberAttributeValue\",\n      b=nil,\n      ss=[],\n      ns=[],\n      bs=[],\n      m=\n       {\"string\"=>#<struct Aws::DynamoDB::Types::AttributeValue s=\"value\", n=\"NumberAttributeValue\", b=nil, ss=[], ns=[], bs=[], m={}, l=[], null=false, bool=false>,\n        \"number\"=>#<struct Aws::DynamoDB::Types::AttributeValue s=\"StringAttributeValue\", n=\"10\", b=nil, ss=[], ns=[], bs=[], m={}, l=[], null=false, bool=false>,\n        \"binary\"=>#<struct Aws::DynamoDB::Types::AttributeValue s=\"StringAttributeValue\", n=\"NumberAttributeValue\", b=\"data\", ss=[], ns=[], bs=[], m={}, l=[], null=false, bool=false>,\n        \"boolean\"=>#<struct Aws::DynamoDB::Types::AttributeValue s=\"StringAttributeValue\", n=\"NumberAttributeValue\", b=nil, ss=[], ns=[], bs=[], m={}, l=[], null=false, bool=false>,\n        \"null\"=>#<struct Aws::DynamoDB::Types::AttributeValue s=\"StringAttributeValue\", n=\"NumberAttributeValue\", b=nil, ss=[], ns=[], bs=[], m={}, l=[], null=true, bool=false>,\n        \"list\"=>#<struct Aws::DynamoDB::Types::AttributeValue s=\"StringAttributeValue\", n=\"NumberAttributeValue\", b=nil, ss=[], ns=[], bs=[], m={}, l=[], null=false, bool=false>,\n        \"map\"=>#<struct Aws::DynamoDB::Types::AttributeValue s=\"StringAttributeValue\", n=\"NumberAttributeValue\", b=nil, ss=[], ns=[], bs=[], m={}, l=[], null=false, bool=false>},\n      l=[],\n      null=false,\n      bool=false>}],\n count=1,\n scanned_count=0,\n last_evaluated_key={},\n consumed_capacity=nil>\nMy personal recommendation is use DynamoDB local, where everything works as expected, or wait for this to be fixed in master. Im looking at a fix as part of the 2.1 release.\n. I did open a follow up issue here: https://github.com/aws/aws-sdk-ruby/issues/823. Feel free to ask any questions you have, but the plan currently is to fix this for version 2.1.\n. All known response stubbing issues have been resolved as of v2.1.0 which is now public.\n. The aws-sdk gem does not provide this functionality. As far as I know, none of the AWS SDKs, except the CLI have a filesystem to S3 sync function.\n. Are you positive it was that call that trigger the throttling error? If so, it surprises me that the job flow was still created. This would be problematic as the response from that API call is supposed to return the job flow id so that you can reference it later.\nIf you haven't done so, might I recommend posting your question in the EMR forums?\nhttps://forums.aws.amazon.com/forum.jspa?forumID=52\n. Closing stale issue. Without additional information I wouldn't be able to help debug this. Please re-open if you see this again or have more context.\n. @nate These errors are being returned from the service. Amazon S3 validates the request and is responsible for both sets of errors. There isn't anything the SDK can really do about the error returned.\n. Yes, Amazon S3 returns these as error codes. The SDK converts them into error classes.\n. Looking at the S3 API documentation, I don't believe grant write is supported for PUT object:\n\nx-amz-grant-write Not applicable. This applies only when granting permission on a bucket.\n\nhttp://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectPUT.html\nThere is though x-amz-grant-write-acp, settable via :grant_write_acp.\n. As discussed on gitter, this is an API limitation.\n. Thanks for reporting the issue! I've corrected the unit test that was covering this and added an integration test to exercise this against Amazon S3.\n. Thanks!\n. Thanks for the fixes!\n. Closed by #832 \n. The change looks good. Maps and lists definitely are effected, but I suspect what you meant to say is that they are not tested?\n``` ruby\ndynamodb.put_item(\n  table_name: \"foo\",\n  item: {\n    id: 1,\n    contents: { # map\n      data: StringIO.new(\"bar\")\n    }\n  }\n)\n=> raises NoMethodError: undefined method `read' for \"bar\":String\n```\n``` ruby\ndynamodb.put_item(\n  table_name: \"foo\",\n  item: {\n    id: 1,\n    contents: [ # list\n      StringIO.new(\"bar\"),\n    ]\n  }\n)\n=> raises NoMethodError: undefined method `read' for \"bar\":String\n```\n. Do you have the ability to run this in Ruby 2.0 or newer? While it certainly is possible autoload is problematic, my understanding is that was resolve, but maybe not in Ruby 1.9.3.\n. @tmornini Ruby 2.0+ has a bug in StringIO wich leaks memory. See this issue for more information and possible workarounds:\nhttps://github.com/aws/aws-sdk-ruby/issues/785\nI'm considering creating a compatible replacement for StringIO and using that by default in the SDK to avoid this, but I have not done so yet.\n. Yes, the memory leak exists in the latest version of Ruby.\nAll of these initialized constant errors are going to be caused by the same issue with autoloading. I'll see about porting the eager autoload utility.\n. I've pushed a commit to the version-2.1-branch of the SDK that adds Aws.eager_autoload!\nhttps://github.com/aws/aws-sdk-ruby/commit/79f8170883e71ae5183d882c2dbe1a158402cf98\nI'm hoping to release 2.1 later this week or early next week.\n. This is now part of the 2.1 public release, available now.\n. @tmornini The eager_autoload! method is only loading constants under the service modules, which means utility classes are getting missed. That was an oversight. I've corrected it in the commits above. My apologies. This will go out with the next release.\n. I'll take a look at some of these.\n. I fixed quite a few of them last week and I just pushed another commit cleaning up more.\n. I finished the updates. The SDK no longer generates any of its own warnings when running the unit and integration tests. These changes should be part of the next release.\n. The Aws::Plugins::StubResponses plugin provides default credentials that should provide default static fake credentials. The issue is that the Aws::Plugins::RequestSigner plugin is providing an instance of the Aws::DefaultCredentialProviderChain and that is being resolved first. The default chain searches for instance profile credentials.\nOur tests are not picking them up because we apply the following in our spec helper:\nruby\nRSpec.configure do |config|\n  config.before(:each) do\n    # disable instance profile credentials\n    path = '/latest/meta-data/iam/security-credentials/'\n    stub_request(:get, \"http://169.254.169.254#{path}\").to_raise(SocketError)\n  end\nend\nThe solution is pretty simple. The SDK needs to apply the stub responses plugin after the request signing plugin so that it has precedence on setting the default credentials option.\n. I've fixed this in the SDK now. When you set stub_responses: true, it will no longer default to the credential provider chain to load credentials. Instead it will use a set of static fake credentials. This should go out with the next release, some time next week.\n. Thanks, this should be part of our 2.1 release shortly.\n. Sorry, submitted to soon. I'm fine with the name. It is description enough.  The waiter could be updated in the future if needed to be more specific with respect to the cluster status if that becomes a problem.  Something that we could investigate is making the path expression return the compound value.\nJSON\n{\n  \"state\": \"success\",\n  \"matcher\": \"pathAll\",\n  \"argument\": \"Clusters[].[ClusterStatus,RestoreStatus.Status]\",\n  \"expected\": [\"available\", \"completed\"]\n},\nI would have to discuss this change with the other SDK maintainers, but it seems sensible. This could future-proof the waiter against other cluster statuses with a restore status of completed, however unlikely.\nCurrently we do not integration test these waiters as the setup would be pretty difficult. Instead, when I merge these up-stream, there is a validator that will ensure the path expressions are valid and will resolve. We are still looking into alternatives to ensure that the desired behavior is achieved. \n. Thank you for reporting this issue. I'll take a look at this shortly.\n. Thank you for reporting this. The fix will go out with the next release.\n. The SDK uses Set for two primary reasons:\n- It semantically enforces unique values in the same way DynamoDB does\n- It is not ambiguous when compared to the DynamoDB list type\nThe first difference is convenient, but the 2nd reason is actually quite important. If item values received from DynamoDB were represented as arrays, it would not be possible to round-trip those values to the service and keep their current value. Using array for both lists, and binary/string/number sets would leave the SDK unable to know where those should end up when making a #put_item request. The current implementation guarantees no loss of precision for all data types (this is why we use BigDecimal for numeric attribute values) and the ability to get and put without changing the representation on the remote end.\nIts curious that the JSON library does not implement #to_json for Sets. I suspect this might be an oversight? Here are some of the current extensions:\nhttps://github.com/ruby/ruby/tree/2e4f0af00f85ca228bcf5fa919882359411c652a/ext/json/lib/json/add\n. The resource classes require a waiter they can poll once to determine if the resource exists. There were no waiters defined for Aws::IAM, so I added a user exists waiter to meet this need.\n. When IAM added the new attached policies API, the paginators did not get updated to configure the SDK on how to enumerate the new list operations. I've added them, and they will be part of the next release soon. Thank you for reporting this issue.\n. It looks like Enumerable was not getting added to the pageable response object in v2.1.0. Thank you for reporting the issue. I've added a test to cover the inclusion of the Enumerable to ensure this does not regress again. I'll try to cut a maintenance release soon.\n. Thanks for reporting the issue. I've fixed this regression and added a test case.  The fix should be released shortly.\n. You can configure a :http_read_timeout which flows though. It is documented here http://docs.aws.amazon.com/sdkforruby/api/Aws/Kinesis/Client.html#initialize-instance_method\n. There is also a :retry_limit option.\n. Thanks for the patch. I'm going to work on adding an integration test to ensure this does not regress.\n. Do these requests eventually succeed? Have you disable the default HTTP open or read time-outs? The table above implies that the time is spent with Net::HTTP which should have triggered default timeouts.\n. The SDK defaults the Net::HTTP read timeout to 60 seconds and the open timeout to 15 seconds. I'm surprised you haven't hit these. Also, Amazon S3 will often close stalled connections. If you stop sending bytes for ~ 30 seconds, they can close the connection.\nIs that example you show above timing the network time or the fully SDK client call?\n. The fact that it is taking so long, and yet succeeding suggests its time spent uploading, not opening an HTTP connection or waiting on the response.\nruby\ns3_object.write(File.open(file_path, 'r'))\nIs it possible the #read calls on your file are slow? Also, I noticed in your example that you are not closing the file, and you should probably open them in binary mode. You can do this yourself, or you can use the :file option and the SDK will manage the file.\n``` ruby\nblock mode auto-closes the file\nFile.open(file_path, 'rb') do |file|\n  s3_object.write(file)\nend\nor you can allow the SDK to manage the file\ns3_object.write(file: file_path)\n```\n. Those look like sample log entries from 5 slow uploads from one day. Is that correct? \nJun 29 11:40:22 :worker production.log:  [ 3:40:22.261013828] (8344) [WARN] /opt/rbenv/versions/2.2.2/lib/ruby/2.2.0/openssl/buffering.rb:326:in `syswrite'\nThis would imply the time is being spent waiting for OpenSSL to write to the socket. It does not appear to be getting stuck in SDK code. Short of instrumenting your code with monkey-patches that collect more fine-grained timing information with context (such as the age of the http connection), I'm not sure how to track this down. Thoughts?\n. The SDK users a model of the service API that is similar to a sort of WSDL. This document declares that parameter of type string which is correct for the given schema. The SDK does not attempt to perform any contextual translation from non-string values to strings. I'm not super familiar with cloud front, but I suspect that context can only be gained by reading and understanding the template. This is currently out of scope for what the SDK is aiming to provide for API compatibility. This sort of feature would be better served as a library that is built on top of the client. \n. Parameter validation is very important to eliminate the common errors in how request parameters are structured and typed. The validate params option is not unique to CloudFormation or specific to template params. It validates all request input parameters. It gives useful contextual information. For example:\n``` ruby\ns3 = Aws::S3::Client.new\ns3.put_object(bucket_name:'abc', key:'abc', body:123)\n=> raises ArgumentError: parameter validator found 3 errors:\n\nmissing required parameter params[:bucket]\nunexpected value at params[:bucket_name]\nexpected params[:body] to be a string or IO object\n```\n\nThis provides a layer of type-safety to the SDK. It can be disabled primarily for performance reasons. I suspect there is some confusion based on the name. It is not unique to template parameters.\n. It appears you are linking to documentation for v1 of the aws-sdk gem but are using v2. The documentation for #copy_from in v2 can be found here:\nhttp://docs.aws.amazon.com/sdkforruby/api/Aws/S3/Object.html#copy_from-instance_method\nYou should be able to invoke this like so:\nruby\ns3 = Aws::S3::Resource.new(region: ENV['AWS_REGION'])\ns3.bucket('example').object('dest').copy_from(copy_source: \"example/source\")\n. @dirkdk The URL is for the v1 documentation, and if you scroll to the top of the linked page, it is the v1 documentation as highlighted in red.\n. This works now because that feature was added as part of version 2.1.14:\nhttps://github.com/aws/aws-sdk-ruby/blob/master/CHANGELOG.md#2114-2015-08-12\n. Thanks for the fixes!\n. Have you tried #describe_subnets?\nruby\nec2 = Aws::EC2::Client.new\nec2.describe_route_tables(filters:[{name:'association.subnet-id', values:['subnet-id']}])\n. @thebravoman Btw, you should be able to upgrade and start using the version 2 SDK in the same application.  If you change your gem dependencies to look like this:\n```\nbring in both gems\ngem 'aws-sdk-v1'\ngem 'aws-sdk', '~>2'\n```\nNow you simply need to require both gems:\nrequire 'aws-sdk-v1'\nrequire 'aws-sdk'\nNow you have both versions running in the same application. Paperclip should continue working as normal.\n. You can get more information here: http://ruby.awsblog.com/post/TxFKSK2QJE6RPZ/Upcoming-Stable-Release-of-AWS-SDK-for-Ruby-Version-2\n. I did some initial digging. The SDK will rescue exceptions inside the Net::HTTP connection pool. Thisis intentional in the version 1 Ruby SDK because it needs to rescue Timeout::Errors from raised by Net::HTTP. In Ruby 1.8, which is supported by the version 1 SDK, Timeout::Error is not a subclass of StandardError, but rather Exception. This was changed in Ruby 1.9.\nSo while I am unable to change the rescue in version 1, it is reasonable now to remove it from the version 2 SDK as it no longer supports Ruby 1.8.\n. The SDK does not modify the values returned from Amazon EC2. I suspect there are some deficiencies in the API reference documentation, hence the discrepancy. I'll raise this issue upstream with the EC2 team. We intentionally do not add client-side modifications to the API results to ensure forwards compatibility with API updates. In the case of the console, they have additional business logic built into the web application.\nIf attach a gateway and it is still showing state as \"available\", I suspect it is just an eventual consistency issue. Has the status since updated?\n. This is very likely an issue with the documentation.  I can pass this along to the documentation team.\n. The implementation is correct, the docs are incorrect. Thanks for the heads up!\n. I am unable to reproduce this:\nruby\n $ aws.rb\nAws> Aws::VERSION\n=> \"2.1.2\"\nAws> elasticache.waiter_names\n=> [:cache_cluster_available, :cache_cluster_deleted, :replication_group_available, :replication_group_deleted]\nI was then able to invoke the :replication_group_available without issue. What version of the SDK are you using?\n. Those are simply defaults. You can set that to whatever values you prefer, including nil which disables them completely (poll forever, not recommended unless you customize the backoff via callbacks). Looking at the API reference documentation I can certainly understand why that would be non-obvious. Would you mind opening an issue to track an update to the wording to be more clear?\n. The :instance_exists waiter works by polling the Amazon EC2 DescribeInstances operation, waiting until it returns the instance in a response. Unfortunately this does not mean that the Amazon EC2 API is in a state ready for this resource to be tagged. Currently there is no API that the SDK can call to determine if the resource is taggable yet. My best suggestion would be to add in an artificial, short, delay after the waiter successfully returns to allow for the service to be eventually consistent.\n. The :instance_exists should ignore Aws::EC2::Errors::InvalidInstanceIDNotFound errors returned by the service. I verified this locally, shown below with some logging:\nruby\nec2.wait_until(:instance_exists, instance_ids:['i-12345678'])\n[Aws::EC2::Client 400 0.794136 0 retries] describe_instances(instance_ids:[\"i-12345678\"]) Aws::EC2::Errors::InvalidInstanceIDNotFound The instance ID 'i-12345678' does not exist\n[Aws::EC2::Client 400 0.459063 0 retries] describe_instances(instance_ids:[\"i-12345678\"]) Aws::EC2::Errors::InvalidInstanceIDNotFound The instance ID 'i-12345678' does not exist\n[Aws::EC2::Client 400 0.370968 0 retries] describe_instances(instance_ids:[\"i-12345678\"]) Aws::EC2::Errors::InvalidInstanceIDNotFound The instance ID 'i-12345678' does not exist\nThis makes me wonder if Amazon EC2 is sometimes returning a different error code. Can you rescue the Aws::Waiters::Errors::UnexpectedError, and then log the original error:\nruby\nbegin\n  # call waiter here\nrescue Aws::Waiters::Errors::UnexpectedError => unexpected\n  puts unexpected.error.class.name\n  puts unexpected.error.message\nend\n. The Aws::EC2::Instance#wait_until_exists method calls Aws::EC2::Client#wait_until(:instance_exists, ...) passing the ID of the current instance. They will both result in the same #describe_instances call on the client. These waiter methods should not be raising the errors your are describing:\nAws::EC2::Errors::InvalidInstanceIDNotFound\nThis error should automatically trigger another polling attempt. If the configured number of polling attempts lapses, then an Aws::Waiters::Errors::TooManyAttemptsError should be raised. This error is explicitly called out here in the waiter definition:\nhttps://github.com/aws/aws-sdk-ruby/blob/master/aws-sdk-core/apis/ec2/2015-04-15/waiters-2.json#L14-L18\nAws::Waiters::Errors::UnexpectedError\nThis error is reserved for service responses that contain an error which is not defined in the waiter definition. An error can trigger a waiter success, fail, or retry state. Any unexpected errors are wrapped in an instance of Aws::Waiters::Errors::UnexpectedError and then raised. You can access the original error by calling #error on the instance of Aws::Waiters::Errors::UnexpectedError.\nHow it should work\nBased on the expected behavior, your should not be getter either of these errors, and it should continue polling. Additionally, I am unable to reproduce the issue myself.  See the following examples:\nThis example makes up a fake instance ID and attempts to poll. This one polls the configured number of attempts and then gives up:\n``` ruby\nec2 = Aws::EC2::Resource.new\ninstance = ec2.instance('i-12345678')\ninstance.wait_until_exists do |waiter|\n  waiter.max_attempts = 2\n  waiter.delay = 1\nend\nraises Aws::Waiters::Errors::TooManyAttemptsError after 2 failed attempts\n```\nThis one starts a new instance and polls it straightway:\nruby\nec2 = Aws::EC2::Resource.new\ninstances = ec2.create_instances({\n  image_id: 'ami-1ccae774',\n  instance_type: 'm1.large',\n  min_count: 1,\n  max_count: 1\n})\ninstance = instances.first\ninstance.wait_until_exists\ninstance.terminate\n. Good catch. I've added logic to the :instance_running and :instance_state_ok waiters to retry the instance id not found errors. This should improve the experience for polling on newly launched instances.\nAs for the originally reported issue, it seems unfortunate that there is not an obvious way, via the Amazon EC2 API to know when an instance may be tagged. I'm going to play around with the DescribeTags API operation and see if that might provide more information.\n. At this point, I'm inclined to close this issue. I will certainly watch out for the opportunity to improve this, but currently there appears to be no deterministic way to wait until the instance is tagable without simply trying, rescuing the error and retrying until the system is internally consistent.\n. Good catch! Thanks for the patch.\n. I'm not very familiar with the Cognito Identity API, but I looked at the API reference documentation for a few of these operations and their documented examples use an authorization header. Are these strictly unsigned operations, or could/should they be conditionally signed?\n. Thanks! This should go out with the next release shortly.\n. This is reasonable, and simply an oversight.\n. This should go out with the next release, sometime this week.\n. Sorry for the slow response. Version 1 SDK updates have taken a back seat to version 2 work. I'll see about getting a release out with this change soon.\n. I pulled these changes locally, and made a few minor changes. Most notably, I renamed the waiters to InstanceProfileExists and AppExists to be consistent with other waiter names. I also added a resource exist waiter for Aws::IAM::InstanceProfile#wait_until_exists and Aws::IAM::InstanceProfile#exists?. The latter is automatically created when there is an exists waiter.\nThanks for the contribution! This should be released shortly.\n. Thanks for the heads up!\n. Any instance of Aws::DynamoDB::Errors::* represents an error returned from the service. I'm not familiar with this particular error. It also appears to have been truncated.\n. I'm not sure if you have resolved this, but the error is a validation error returned from the service. There is something it doesn't like about your request parameters. This does not appear to be an issue with the SDK. If you have any additional information or have been able to resolve, please feel free to re-open or comment on this. Thanks.\n. Can you share a code example of how you are invoking #copy_to. Also, do you receive this error periodically on large objects or is it consitently reproducible?\n. Good news, I was able to track down the issue here and I believe I have a fix. I'll publish a bug-fix release with this patch.\n. I haven't had a chance to fully review this yet.  One quick comment though, please feel free to remove the c2j prefixes. As an internal tool, this isn't meaningful in the public SDK. I would generally favor something more descriptive such as \"SharedExample\" instead of \"C2jExample\".\n. Thanks!\n. Thanks, I'll merge this then DRY it up.\n. Just a heads up. I'm looking into this. I believe the solution is going to invoke tracking down all of the known x-amz-* headers and determining which ones can, must, and must not be hoisted to the URL and which must be signed and sent as headers.\n. So this change was made so that x-amz- headers would be sent via the querystring eliminating the need for users to provide these values twice. Before the change you would have to specify these values to the presigner and then again to your HTTP client to send them as headers. This was necessary because signature version 4 requires all header values that start with x-amz- to be signed, including their expected values.\nWhen the change was made, it was intended to be a bug-fix so that a user could provide the values once to the presigner and have the URL querystring contain their values. This means the presigned URL no longer signs them as headers as they are part of the request URI.\nHere is an example showing how to create a presigned PUT url with server-side-encryption and KMS and then upload something with Net::HTTP:\n``` ruby\nrequire 'uri'\nrequire 'logger'\nobj = Aws::S3::Object.new('aws-sdk', 'secret')\nuri = URI(obj.presigned_url(:put, {\n  server_side_encryption: 'aws:kms',\n  ssekms_key_id: \"cb467d40-59be-44d7-813f-d0f281092da8\",\n}))\nhttp = Net::HTTP.new(uri.host, uri.port)\nhttp.use_ssl = true\nhttp.set_debug_output(Logger.new($stdout))\nreq = Net::HTTP::Put.new(uri.request_uri)\nreq.body = \"secret-body-#{Time.now.to_i}\"\nres = http.request(req)\nobj.ssekms_key_id # show that it is encrypted server-side\n```\nThis means you should be able to simply remove those headers from your HTTP request and everything should work. Clearly this was an unintentional side-effect of the bug-fix, and a breaking change. The tricky part is if this is reverted then newer users would be broken (those counting on the values to be part of the URI and not currently sending them as headers). This is a sort of catch 22 and I'm not sure what the best path forward.\nThoughts?\n. The final line of my example was there to demonstrate the object was encrypted:\nruby\nobj.ssekms_key_id # show that it is encrypted server-side\nThat method returns a nil value for non kms server-side encrypted objects. After uploading the object via the pre-signed PUT this returns the ARN for the kms key I used.\n. Hmm.. Given Amazon S3 accepts the upload with the server side encryption header as a querystring, it seems like that should also be checked when matching the policy. I'll try following up on this and see what the expected behavior is.\n. I spoke with an engineer on the S3 team. They are going to update their API reference documentation to reflect this limitation. As a work-around, I suppose we will want to add a #presigned_request that generates the presigned URL without hoisting any of the request headers. It could return a HTTP URI and a headers hash. This would allow you to make the request and know what the expected headers are.\nWould this resolve the issue?\n. Sorry for the confusion. The SDK expects the record to be a hash of string keys and string values. This is based on the expected values of the service. It could be possible to provide a client-side conversion of booleans to their appropriate string representation.\n. I've moved this to our feature request list.\n. I've added the missing paginator for Aws::IAM::Client#list_policy_versions which should stop this from raising. This fix should be released shortly. Thanks for reporting the issue!\n. Could you try applying the following patch to your code? That error would only happen if the value received from the instance metadata service for the \"expiration\" was the value \"Expiration\", i.e. Time.parse('Expiration') produces the error you are describing.\nruby\nclass Aws::InstanceProfileCredentials\n  def refresh\n    retry_count = 0\n    begin\n      json = get_credentials\n      c = Json.load(json)\n      @credentials = Credentials.new(\n        c['AccessKeyId'],\n        c['SecretAccessKey'],\n        c['Token']\n      )\n      @expiration = c['Expiration'] ? Time.parse(c['Expiration']) : nil\n    rescue ArgumentError => error\n      puts \"NO TIME INFORMATION: #{c.inspect} - #{json.inspect}\"\n      if retry_count < 3\n        retry_count += 1\n        retry\n      else\n        raise(error)\n      end\n    end\n  end\nend\nYou should replace the puts statement with something that can log to something more durable than standard out. This should:\n- Let us know if it is a temporal issue that resolves itself\n- Give us more information about the response that is generating this error\nEDIT: Modified the snippet\n. Any update on this? Has the error reoccured yet?\n. Hmm.. unfortunately, that error is unrelated to the other one. Without your patch applied, the current release of the SDK currently handles json parsing errors. There have been a few other changes to make parsing responses from the instance metadata service more robust. I'll be pushing another change shortly that will capture and retry the errors related to extracting the expiration time.\nThanks for collecting the information. \n. Thank you for reporting this issue.\nThere was a regression in serializing structure data types. The shared protocol tests exercised all of the code-paths of the param builder, but did not test with structure data types as input. This was was being tested for the other builder classes via their unit tests. I added a spec file for the EC2 param builder and added a test to cover this regression.\nSorry for the inconvenience. This fix should be released shortly.\n. Thank you for reporting this. I'm guessing that you meant that Aws::CognitoIdentity::Client#list_idenity_pools raises. The change that caused this regression comes from this pull-request:\nhttps://github.com/aws/aws-sdk-ruby/pull/862\nThe following operations were white-listed to be sent as unsigned requests:\n- GetCredentialsForIdentity\n- GetId\n- GetOpenIdToken\n- ListIdentityPools\n- UnlinkDeveloperIdentity\n- UnlinkIdentity\nI tested a few of these, but not all. Each of them do state quite clearly in their public API documentation the following:\n\nThis is a public API. You do not need any credentials to call this API.\n\nI've gone back and tested all operations and found two of them that do require authentication despite the claims in the API reference:\nhttp://docs.aws.amazon.com/cognitoidentity/latest/APIReference/API_ListIdentityPools.html\nhttp://docs.aws.amazon.com/cognitoidentity/latest/APIReference/API_UnlinkDeveloperIdentity.html\nI've corrected the code for these two operations and added integration tests to test these against live endpoints. Thank you for bringing this to our attention. I'll have a patch release out shortly.\n. I forgot to update, but 2.1.10 was released earlier today wight the patch.\n. Also worth noting, the API reference documentation has been corrected as well.\n. Thank you for reporting this limitation. This should go out with the next release.\n. Just a heads up. I've been working on adding support for API Gateway. This is coming, but I can not comment on when it will be ready.\n. Good news! We were able to release support for API gateway today!\n. There was an intentional change made to response paging with version 2.1. Prior to version 2.1 every operation was incorrectly decorated as a pageable response. This behavior however was buggy and could cause silent failures.\nFor response paging to work, there is some minimal configuration that indicates when results are truncated, and how to make a follow-up request to get more results. When this configuration is not present, then the pre-2.1 pager would simply say, I got all of the results. AWS services are frequently adding new operations and if the paging tokens were not added for a new operation, it would incorrectly report that all results were received in the first response and fail to get the rest.\nFor this reason, in 2.1+ a response will only be decorated with paging methods, such as #each when the configuration is present. This means users upgrading that were previously calling a pageable API call that did not have its configuration would receive a NoMethodError. Helps identify operations missing their configuration and it also means operations that should never be paged, like a PUT object to Amazon S3, will ever have the extra methods.\nThat said, I am unable to reproduce your issue. I checked and there is configuration for the DescribeStacks call. Here is the code that I ran to test this:\nruby\nopsworks = Aws::OpsWorks::Client.new\nopsworks.describe_stacks.each do |page|\n  # ...\nend\nCan you share the code you are running that produces this error?\n. Sorry for the very slow response on this. I've been working with the RDS team to ensure we get the proper service description. Once I get a full update, I'll merge yours in and then theirs on top to ensure we get everything.\n. Thank you for your patience. I was finally able to merge this and make the appropriate updates from upstream. This should be released shortly.\n. The SDK will throw a generic error such as this when it is not possible to extract the normal error code from the http response. This can happen for a handful of reasons. The most common reason is the body was not available or was corrupted. This can happen for responses to HEAD requests that don't have bodies or due to transient networking issues.\nIf you continue to see these errors, please feel free to share more details and we'll see if the SDK should be doing more in this instance.\n. Thanks!\n. Sorry for the slow response on this. It took a bit of digging to find out how I could load the details of a NetworkInterfaceAssociation given only its association ID. This should go out with the next release.\n. Thanks for the heads up!\n. Thank you for reporting the issue. I've found the root cause and I'm putting together a test to cover this in the future.\n. This should go out with our next release early next week.\n. I did some looking into this. I enabled a HTTP wire trace to see what was going on.\nruby\niam = Aws::IAM::Client.new(http_wire_trace:true)\niam.list_policies()\niam.get_policy(policy_arn:\"arn:aws:iam::aws:policy/AWSDirectConnectReadOnlyAccess\")\nIAM is only returning the description in the describe call. Is is not returned from the service in the list call. There is no flag that I am aware of that would cause it to be returned in both cases. I can raise this issue with the service team, but isn't anything that I can do from the SDK without sending a follow-up request which is not a suitable solution.\n. The IAM API reference documentation and the SDK documentation both mention that these values are only included in the output from a GetPolicy call:\nhttp://docs.aws.amazon.com/IAM/latest/APIReference/API_Policy.html\nhttp://docs.aws.amazon.com/sdkforruby/api/Aws/IAM/Types/Policy.html#description-instance_method\n. Can you provide some additional context to the issue. What service and operation are you making requests against? Can you share the policy?\n. Thanks!\n. This should be part of tomorrows release.\n. The original intention was to show that the return value might be an IO object or a File object. When it returns an array a different notation is used like:\nReturns:\n* (Array<String>) - Description of return value.\nThat said, I understand how this is not obvious or clear. I think I'll simply remove the File from the possible return types given that a File is considered an IO object.\n. Thanks!\n. I added a note to the README to address this. Thanks for the suggestion.\n. I'm curious what issue you had with :response_target. The following should work just fine:\nruby\ns3.get_object(bucket:'aws-sdk', key:'foo', response_target: '/path/to/file')\nThe following also works, but please note, the target option must be passed in a different hash. The first is the request parameters hash, the second is the request options hash:\nruby\ns3.get_object({ bucket:'aws-sdk', key:'foo' }, target: '/path/to/file')\nAs far as I can tell, the docs should be correct. Can you share the error you are having with :response_target?\n. Thanks!\n. Oops. I mean to merge a different pull request. Sorry, I'm going to revert this shortly.\n. Thanks for the corrections!\n. In your configuration, you gave the name of an availability zone where a region is expected. You should be able to simply replace \"us-west-2a\" with \"us-west-2\" and your code should work. The reason this gave you a networking error is because it was attempting to connect to ec2.us-west-2a.amazonaws.com which is not a valid domain.\nAnother note, it looks like you are trying to create an instance. Constructing an instance of the Aws::EC2::Instance class will only create a local reference to an instance by ID so that you can operate against it. For example, if I know the id of an instance I own, I can do the following to get status or to terminate it:\nruby\ninstance = Aws::EC2::Instance.new('i-12345678')\ninstance.state.name #=> \"running\"\ninstance.terminate\nIf you want to run a new instance you should use Aws::EC2::Resource#create_instances (http://docs.aws.amazon.com/sdkforruby/api/Aws/EC2/Resource.html#create_instances-instance_method) or Aws::EC2::Client#run_isntances (http://docs.aws.amazon.com/sdkforruby/api/Aws/EC2/Client.html#run_instances-instance_method):\nruby\nec2 = Aws::EC2::Resource.new(region: '...', access_key_id: '...', secret_access_key: '...')\ninstances = ec2.create_instances(instance_type: 'sample.type', image_id: 'sample-id', min_count: 2, max_count: 2)\nHope this helps!\n. The SDK has built in retry logic for 500 server errors and throttling errors. Are you encountering errors that you feel should have been retried but are not? Or perhaps are they not backing-off enough, or with sufficient number of retires?\n. I am unable to reproduce this locally. Can you try updating the the latest version and seeing if this persists? Also, what does the following produce?\nruby\nstubbed_s3_client = Aws::S3::Client.new(stub_responses: true)\nstubbed_s3_client.config.credentials\n. I'm little confused here. Do you mean when running the v2 SDK from within a Rails application, the precense of a RAILS_ROOT/config/aws.yml file causes stub_responses: true to stop working? As far as I know, the v2 SDK does not ever source these credentials. \n. This is correct. When credentials are present, they will be used to sign. I'm glad you were able to resolve this.\n. Thank you for reporting this issue. I definitely agree these should be filtered from the default logger output. I'm going to take a look at this today and see what we can do to resolve this.\n. I made a change to the SDK so that it will now filter sensitive parameters by default when logging request parameters.  The default list will be auto-updated by checking the service definitions as an automated release task.  You can add additional filters when constructing a log formatter.\n. You probably need to specify one or more of the response headers for content encoding. See the following blog post for more information. You can specify these values when you create the presigned url.\nhttp://www.bennadel.com/blog/2696-overriding-content-type-and-content-disposition-headers-in-amazon-s3-pre-signed-urls.htm\n. We do not currently support loading assume role credentials from the shared credentials file. You can use Aws::AssumeRoleCredentials from your Ruby script to manage refreshing role credentials:\nruby\nrole_credentials = Aws::AssumeRoleCredentials.new(\n  role_arn: \"linked::account::arn\",\n  role_session_name: \"session-name\"\n)\nec2 = Aws::EC2::Client.new(credentials: role_credentials)\nDoes this help?\n. I'm going to close this and add supporting assume role credentials from ~/.aws/credentials to the feature requests.\n. Thank you for the well formed pull-request!\n. Also, this should go out in tomorrows release.\n. I did some digging into this. I don't believe the SDK should actually retry these errors. The Net HTTP handler already rescues known networking errors and decorates them in a way that trigger retries. This means only runtime errors and signal errors should be causing a 0 status code.\n. Thanks for reporting the issue!\n. Thank you for reporting these errors and stack traces. I agree that it should be more resilient and attempt to retry these errors. Let me take a look at this.\n. I've put together a patch that will catch both of these scenarios and expanded the unit tests to cover these as well. Thank you for your help tracking down these issues!\n. Thank you for reporting this issue. I forgot to reference this issue from the commit that is the actual fix, fc89f16f04750936b96910bd4ecbb90287b1dca8.\n. When using the aws-sdk you should configure the region and your AWS credentials. The SDK will construct the correct endpoint for making API calls.  The S3 website endpoints are for using via the web, not the SDK. If you have a bucket website configuration, then you can use the web endpoint to access objects in your bucket via a normal browser.\n. The base resource class defines #exists? in such a way to raise the NotImplementError if the appropriate waiter definition does not exist. It relies on a waiter to check for the existence of the resource. I'll try to add a waiter that polls for the existence of a VPC peering connection and I'll share that shortly.\n. Thanks!\n. I responded to the stackoverflow issue. In short, you have not configured credentials. If you had made a request then it would have given a more helpful error message. \ns3 = Aws::S3::Resource::new(\n  region: region,\n  credentials: Aws::Credentials.new('access-key-id', 'secret-access-key')\n)\ns3.bucket(bucket).object(path).presigned_url(:get, expires_in: 604800)\n. What is your end goal? The refresh method is only for resource objects, not data objects returned by client operations. Can you describe what you are trying to accomplish?\n. From looking at the API for Auto Scaling, it looks like you would need to describe your auto scaling group, grab the list of instance IDs and wait until they are healthy. Looking at your initial question, it seems that for a period of time the #describe_auto_scaling_groups may return with an empty list of instances. Is this correct?\n. You can probably do something like the following. Be warned, I have not tested this myself:\n``` ruby\nas = Aws::AutoScaling::Client.new\ngroup_name = 'name'\nWait until the auto scaling group has a list of instances and\nall of the instances are in the desired state\nTODO : Needs a terminal case to prevent infinite loop\nbegin\n  resp = as.describe_auto_scaling_groups(auto_scaling_group_names:[group_name])\n  instances = resp.auto_scaling_groups.first.instances\nend while instances.empty? || instances.any? { |i| i.health_status != 'expected-status' }\n```\nSeems like a waiter would be helpful. I imagine something like the following:\nruby\nas = Aws::AutoScaling::Client.new\nas.wait_until(:instances_healthy, auto_scaling_group_names:['group-name'])\nI can add this to our backlog, but I can't comment on when we could get to it. We do accept pull requests for waiters if you'd like to take a stab at one.\n. I'm going to close this issue now. Please feel free to re-open or create a new issue if you run into trouble.\n. It looks like you opened the issue against the wrong repository. If you are using .NET then you should open an issue here: https://github.com/aws/aws-sdk-net\n. You can use the version 1 SDK, aws-sdk-v1 to generate v2 signatures. It uses a different gem name and module, so you can use them in the same application. Version 2 of the AWS SDK for Ruby intentionally does not provide a v2 pre-signed URL functionality. There are an increasing number of scenarios where these signatures will not work due to restrictions on new regions, server side encryption requirements, and more.\n. Closing this question as a non-issue for now. Please re-open if there is an issue with the v4 signatures.\n. EC2 requires the user data to be base64 encoded when sent over the wire. This does not affect the format it is made available in on the instance. This simply ensures it is safely encoded for the request. I suspect that AutoScaling is simply passing this requirement on. The SDK automatically base64 encodes data when using the EC2 API as part of the protocol. If AutoScaling had described this parameter as a binary value instead of string then we would have managed this over the wire as well.\nAt this point, I'm not sure there is much we can do. I would prefer if the SDK did this automatically as it does for EC2, but changing this now would be a breaking change for existing customers.\n. Off the cuff, I do not know what the cause would be. One very interesting piece of information comes from the second failed job linked. It was performing a multipart upload. It appears that all 7 parts failed with the same error.\nDo you know if these are temporal errors that resolve with a retry?\n. It is difficult to say if these are related, but an issue was found in the classic S3 signature code that would cause a signature does not match error when the bucket name is stuttered as part of the S3 object key. For example, if I attempted the following:\nruby\ns3 = Aws::S3::Resource.new\ns3.bucket('bucket-name').object('bucket-name/key').upload_file('path')\nThen it would raise an error when using the classic signature code, but not with signature version 4. The classic code is currently in use in most, but not all bucket regions. I'll put the fix up shortly, reference this issue. I suspect this will be released tomorrow. Hopefully this will resolve the issue.\n. The fix was released yesterday with v2.1.21. Let me know when you get the chance to update and test this version.\n. Any more news on this? Was the patch successful?\n. @BanzaiMan Is it possible to find out what configuration options were used when uploading? There is potentially related issue here https://github.com/aws/aws-sdk-ruby/issues/949. \n. @BanzaiMan Thanks. Lets open a new issue and reference this. Without more information I can't know if they are related, or simply different signature issues. I will need more information to debug this, so lets continue the discussion there.\n. For quite a while now, all feature work has been for version 2 of the SDK. We no longer update version 1 with new services, API updates for existing services, and we are not adding any additional functionality. We continue to accept and fix bugs and security issues. We do not currently have a date for when security and bug fixes will stop. I will start the conversation internally on picking an end of life date for version 1.\nThat said, we took a lot of effort to ensure that version 1 and version 2 could be used in the same project allowing users to upgrade at gradually and at will.\n. Thanks for the fix. This should go with the next release, possibly today.\n. As discussed in the gitter channel, we are actively looking into this. I can't comment on a timeline, but I'm inclined to add this functionality.\n. I've moved this to our feature request backlog. I also wanted to provide an update. We have been working on this internally. I'm fairly confident this feature will materialize. We are still trying to determine what the best mechanism would be to track the regions and keep the list current with each SDK release.\n. I'm afraid this is not technically possible. Removing the 1-week limit expiration will only permit the SDK to return invalid pre-signed URLs. Signatures extending beyond one week will not be honored by Amazon S3. This is a limitation of signature version 4 pre-signed URLs.\nVersion 2 of the SDK intentionally moved from v2 signatures to v4 signatures for security reasons. Additionally, v2 signatures simply do not work in an increasing number of scenarios, such as new Amazon S3 regions, operations involving encrypted objects and more.\n. I assume with the 60 days you are using the classic signature which does not have a limit on the expiration. My understanding is that 7 days is the limit for signature version 4, and that it does not vary by region.\n. As a status update, I've added some actual examples to the API reference documentation showing how to copy encrypted snapshots. I have not yet addressed removing the :destination_region from the documentation, or the behavior swap.\n. I just pushed a commit which fixes how the SDK was disabling pre-signing the copy-snapshot operation when the :destination_region was set. It looks like it was using that parameter keep from applying the copy_snapshot URL in a recursive loop (it calls #copy_snapshot to generate a presigned url).\nI believe the last step to improve this is to remove the :destination_region and :presigned_url parameters from the API reference documentation.\n. The updated documentation should go out with our next release, likely tomorrow.\n. Does the error message contain the expected string to sign and canonical request? If so, it would be possible, with a small patch, to get the SDK to save these values that it computed and then compare then to the expected values from the service.\n. I've put together a monkey patch that you can apply to your SDK that will capture the string to sign and canonical request. If you could add these and then rescue the signature does not match error, then we can compare the strings to see where things have gone wrong.\n``` ruby\nrequire 'aws-sdk'\nmodule Aws\n  module Signers\n    class V4\n      class << self\n        def sign(context)\n          signer = new(\n            context.config.credentials,\n            context.config.sigv4_name,\n            context.config.sigv4_region\n          )\n          req = signer.sign(context.http_request)\n          datetime = context.http_request.headers['X-Amz-Date']\n          context[:string_to_sign] = signer.send(:string_to_sign,\n            context.http_request,\n            datetime,\n            context.http_request.headers['X-Amz-Content-Sha256']\n          )\n          context[:canonical_request] = signer.send(:canonical_request,\n            context.http_request,\n            context.http_request.headers['X-Amz-Content-Sha256']\n          )\n          req\n        end\n      end\n    end\n  end\nend\n```\nWith this in place, you can do something like this:\n``` ruby\nbegin\n  # your code here\nrescue Aws::CloudSearchDomain::Errors::SignatureDoesNotMatch => error\n  # these are the values the SDK computed client-side\n  puts error.context[:string_to_sign]\n  puts error.context[:canonical_request]\n# this contains the values computed server-side\n  puts error.message\nend\n``\n. Sorry for the slow response. I was pretty swamped last week. I'll try to take a look at that shortly. I did get the email.\n. The:stub_responses` option causes the clients to empty responses. It does not attempt to setup fake state for AWS resources. If you desire the client to return a specific set of values, then you have to provide the values they should return:\n``` ruby\nAws.config[:ec2] = {\n  stub_responses: {\n    describe_instances: {\n      reservations:[{instances:[{state:{code:123, name:'name'}}]}]\n    }\n  }\n}\nAws::EC2::Instance.new('id').state\n=> #\n```\nThis level of response stubbing is most useful for applications or libraries that deal directly with the client classes. You are probably better off stuff the Aws::EC2::Instance#state yourself if that is the interface you are working with.\n. The Aws::S3::Errors::InternalError error indicates the service responded with a 500 status code. The SDK automatically retries these errors. It sounds like an object that generates this error will consistently generate this error. Is this correct? If you have a reproducible error case, then I would love to pair on debugging this. I have not seen this particular error myself.\n. @mattcook Thanks for for helping to reproduce the issue. I've got the wire log you sent me. Were you able to open an issue with Amazon S3?\n. Thats awesome. To be honest, I did not have a chance to raise the issue with S3. They must have seen the 500 errors coming from their service and resolved the issue. I'm going to close this issue, but please feel free to re-open if the issue persists.\n. The :region option is necessary for more than endpoint construction. DynamoDB requires version 4 signatures for authentication. A version 4 signature requires the region name. Its part of the canonical request, the derived key, and more. This validation message is intentional.\n. What region are you attempting to connect to? A list of supported OpsWorks regions can be found here:\nhttp://docs.aws.amazon.com/general/latest/gr/rande.html#opsworks_region\n. Looks good, thanks!\n. Good catch. I want to do some additional testing before I merge this. I suspect that http is only necessary if the bucket name contains dots. \n. Good point! The code is only used when virtual hosting, so using http seems to be the only sensible approach.\n. I've been considering how to deal with this issue. I am not generally opposed, but as you point out, I don't know if this should be the SDK's responsibility. It seems reasonable, so I'm inclined to explore how to make this happen.\nThe MD5 handler is not the only place that the file is read. There are three places that come to mind, in the following order:\n- The file is read & rewound to compute a MD5 hash\n- The file is read & rewound to compute a version 4 signature, this happens for certain endpoints.\n- The file is read as it is streamed over the HTTP connection.\nMD5 computation is optional, sigv4 is too. If the goal is to accept possibly closed files, we would need to do a closed check before each of these. Either that or we need a pre-check that happens up-front and let the other handlers continue to assume that the IO object is ready to be read.\nThoughts?\n. I was taking another look at this. I ran into an issue during testing. The #open method on File is private. I'm looking at another solution.\n. This should go out some time next week. I added logic to the incoming request parameter converter. If it receives a closed file or tempfile it will open a new file in read-binary mode. Thanks for your patience.\n. @Senjai This issue has cropped up multiple times. Yes, it should be reasonable to require an open file. There are however many different tools and libraries that use the SDK for uploading files. It is not always straight forward for a user to track down the tool that is \"responsible\" for closing the file. \nTo make things easier for the user, this patch makes it easier on the end user:\nhttps://github.com/aws/aws-sdk-ruby/blob/master/aws-sdk-core/lib/aws-sdk-core/param_converter.rb#L113-L121\nThe SDK will not attempt to open the closed file, instead is uses the file to get a path. This is the same as if the user had passed in the path to the file as a string. \n. Thanks!\n. Thank you for reporting this issue. Currently we generate our API documentation from the same source as the service API reference. This means that sometimes HTTP wire-level documentation sneaks into the SDK documentation.  The SDK will automatically base64 encode data that is to be treated as a binary blob.\nI'll see what we can do about improving the SDK documentation to not mention the encoding.\n. While not necessarily related to your issue or question, I notice that you are mixing usage for the version 1 SDK with version 2. Is this intentional? Version 1 uses AWS while version 2  uses Aws. You should be able to do the following:\n``` ruby\nrequire 'aws-sdk'\nThis should be called first; Clients constructed before calling\nthis method will not use the bundled certificate\nAws.use_bundled_cert!\nVersion 2 will automatically load the shared credentials\nfrom the default profile, no configuration needed\niam = Aws::IAM::Client.new\nuser_names = []\niam.list_users.each do |response|\n  user_names += response.users.map(&:user_name)\nend\nputs user_names.sort_by(&:downcase)\n```\nOr, you could use the resource interface, which will managing paging responses for you:\n``` ruby\nrequire 'aws-sdk'\nAws.use_bundled_cert!\niam = Aws::IAM::Resource.new\nuser_names = iam.users.map(&:name)\nputs user_names.sort_by(&:downcase)\n``\n. I am unable to reproduce your issue. The error message seems to indicate that theAuthorization` header is sending only the credential scope string. Try the example that I gave. If the problem persists, please feel free to re-open.\n. Thanks for reporting the issue. I'm looking into this.\n. Thanks for your patience. I've been able to resolve this error. I will try to get a release out shortly.\n. You are very close. Try this:\nruby\nAws.config[:s3] = {\n  stub_responses: {\n    list_buckets: Timeout:Error,\n  }\n}\n. The order you configure is significant. Anything you set inside the Aws.config hash is used as a set of defaults for constructing a client. You can either change the order of your statements above, or you can also pass the options directly to the client constructor:\n``` ruby\nAws.config[:s3] = {\n   stub_responses: {\n     get_object: Aws::S3::Errors::NoSuchKey.new(\"test\", \"test\")  \n   }\n}\nno need to specify :stub_responses option as it is set above in the defaults\ns3 = Aws::S3::Client.new\ns3.get_object(bucket: \"test\", key: \"test\")\nraises the no such key error\n```\nAnd the alternative:\n``` ruby\nconstructor options always take precedence over the Aws.config defaults\ns3 = Aws::S3::Client.new(\n   stub_responses: {\n     get_object: Aws::S3::Errors::NoSuchKey.new(\"test\", \"test\")  \n   }\n)\ns3.get_object(bucket: \"test\", key: \"test\")\nalso raises\n``\n. The default behavior for a copy object operation in Amazon S3 is to copy all of the metadata from the source object to the target object.  Metadata, such as content-type, content-encoding, the ACL (access control list), user-supplied metadata, etc are all ignored unless by S3 unless you set a special header,x-amz-metadata-directive: REPLACE`.  \nThe version 1 SDK inspects incoming options and attempts to decide if any options that would modify object metadata are present, and if so, would attempt a smart copy. This works for many cases, but proved to be a leaky abstract. First, an extra request is necessary to HEAD the object so that metadata not given in the copy could be maintained and not lost. Unfortunately, this didn't extend to all object metadata. HEAD object did not previously return the storage class, and it also does not return the ACL. If is the SDK performed the HEAD and GET Object ACL requests, Amazon S3 could add other metadata, that require other API calls in the future and then these would be silently dropped.\nTo avoid surprising behavior, we chose not do have the smart copy in version 2. You can modify the metadata for an object, but you must set metadata_directive: 'REPLACE', in your call to #copy_from. This preserves the S3 behavior. This does mean if you want to only change the content-type but you also want to preserve the object ACL, or user metadata, you either need to know those values and supply them to #copy_from or you need to HEAD and GET Object ACL yourself.\nCurrently there are no plans to port the smart copy behavior forward. We received many bug reports over the years for this functionality and it remains incomplete today.\n. Oh, and here is an example of what you need:\n``` ruby\nsource = Aws::S3::Object.new('aws-sdk', 'source')\ntarget = Aws::S3::Object.new('aws-sdk', 'target')\nsource.put(body:'text', content_type: 'text/plain')\ntarget.copy_from(source, content_type: 'application/json', metadata_directive: 'REPLACE')\ntarget.content_type\n=> 'application/json'\n``\n. I'm closing this as a non-issue, but please feel free to continue the discussion. I'm open to alternative ideas or view points. Feel free to re-open if you run into any issues.\n. Thanks for the reporting the issue.\n. Thanks!\n. Thank you for reporting this issue. We are looking into it now.\n. Do you know the region of the target bucket? Also, would it be possible for you to rescue theAws::S3::MultipartUploadError` for some additional debug information. The upload works by uploading parts using multiple threads. If any of those threads generate errors, then they are aggregated and raised. You can access the original errors from the MPU error:\nruby\nbegin\n  # ...\nrescue Aws::S3::MultipartUploadError => mpu_error\n  mpu_error.errors.each do |error|\n    puts error.message\n    puts error.backtrace\n  end\nend\nI think the issue is related signing the upload part requests. When using server side encryption with KMS the request must be signed with signature version 4. There is code in the SDK to rescue the error that indicates this and then attempts to resign and send the request. Its a bit tricky because the SDK must determine the region of the target bucket to generate the version 4 signature.\nMy best guess is that the code for determining the bucket region is failing to handle when the bucket location is not available as a header. A stack trace might help to confirm this.\nI'm also looking into some other alternatives for handling bucket region issues.\n. Just to clarify, what version of the SDK are you using?\nThe latest version of the SDK does support the full RDS API. I am not very familiar with the RDS API, so I can not comment if there is some other mechanism to load a snapshot into Aurora instance. It might be worth asking your question in the RDS forum: https://forums.aws.amazon.com/forum.jspa?forumID=60&start=0\n. You are running the latest version. Please feel free to re-open if you believe this is an SDK issue, and not just a limitation of the API.\n. You are close. The statistic_values option should have been nested inside the metric data.  For example:\n``` ruby\ncloudwatch = Aws::CloudWatch::Client.new\nresp = cloudwatch.put_metric_data({\n  namespace: \"Logstash Redis\",\n  metric_data: [\n    {\n      metric_name: \"Dev\",\n      dimensions: [\n        { name: \"Messages\", value: \"1.0\" },\n      ],\n      statistic_values: {\n        sample_count: 1.0,\n        sum: 1.0,\n        minimum: 1.0,\n        maximum: 1.0,\n      },\n    },\n  ]\n})\n```\nFrom the docs: http://docs.aws.amazon.com/sdkforruby/api/Aws/CloudWatch/Client.html#put_metric_data-instance_method\nAlso worth noting, the SDK will automatically pull the region from ENV['AWS_REGION'] and credentials from ENV['AWS_ACCESS_KEY_ID'] and ENV['AWS_SECRET_ACCESS_KEY']. You should be able to construct your client without any options if those environment variables are set.\n. Thanks for the contribution. I've merged this, but I'm not sure when our next scheduled release of version 1 will be. The version 1 SDK is primarily in maintenance mode at this point.\n. Thanks for reporting the issue. Can you reproduce this error reliably, or is it perhaps intermittent? Also, are you using the instance in a multi-threaded environment? Looking through the code, I'm trying to reason about how the #data method internally might return nil.\n. Ping. Just checking in to see if this issue persists and if you have any additional information.\n. To be honest, I don't recall the motivation for ignoring system proxies. Changing this at this point would possibly cause other problems, as some issue likely prompted the change.\nI suspect there are valid cases for using the system proxy to contact the instance metadata service and other times where using the system proxy is undesirable or problematic. The problem here is the SDK does not give the user any control over when to use or when to ignore the system proxy.\nWould it be sufficient if AWS::Core::CredentialProviders::EC2Provider#initialize accepted a :http_proxy option?\n. I also did some additional digging. The Ruby SDK is not the only SDK that disables environment proxies when connecting to the instance metadata service. I spoke with one of the CLI engineers and they do the same thing.\n. I'm going to move this to our feature requests.  Please feel free to continue commenting on this issue.\n. The stack trace isn't very helpful unless you look pretty deep. When you construct an instance of Aws::AssumeRoleCredentials, an Aws::STS::Client is constructed when one is not given. This client requires a region. The client class will attempt to load the region from ENV['AWS_REGION'] but it seems like a pass-through like you attempted would be more helpful.\nI've fixed the documentation so that the :client option will appear in the api docs. I'll see about making the region option (and others) pass through to the client constructor call.\n. I'm unable to reproduce the error you are experiencing. I ran the following code and it worked as expected:\n``` ruby\njson_string = '{}'\nAws.config[:sqs] = {\n  stub_responses: {\n    receive_message: [\n      { messages: [{message_id:'id1', receipt_handle:'rh1', body:json_string}] }\n    ]\n  }\n}\nsqs = Aws::SQS::Client.new\npoller = Aws::SQS::QueuePoller.new('http://queue.url', client: sqs)\npoller.poll do |msg|\n  puts '----'\n  puts msg.body\nend\n```\nPlease note, you do not need to set Aws.config[:stub_responses] = true and Aws.config[:sqs] = { stub_responses: ... }. It doesn't hurt, it just isn't needed unless you want every service to stub with empty responses and SQS to stub with non-empty responses.\nCan you provide a full stack trace so I can get some more information about the error?\n. It was not clear from your previous code examples, but it looks like you are attempting to stub poller responses. You are correct, that response stubbing is only for Aws::*::Client classes. Your choices are to:\n- Use the client to generate stubbed responses and let the poller operate on those. This is more of a functional test that exercises more of the system, but harder to write.\n- Use your testing library (looks like rspec) to stub the public interface of the queue poller to yield the messages or raise errors as you desire. You can still use the client to generate stub data using #stub_data.\n. I'm still not seeing the issue. Here is something I put together and ran through rspec:\n``` ruby\nbefore(:each) do\n  # you might want to define these in some sort of test fixture file\n  Aws.config[:sqs] = {\n    stub_responses: {\n      receive_message: [\n        {\n          messages: [\n            { message_id:'id1', receipt_handle:'rh1', body: '{}'}\n          ]\n        },\n        { messages: [] }\n      ]\n    }\n  }\nend\nafter(:each) do\n  # ensure no state leak between tests\n  Aws.config.clear\nend\nit 'processes queue messages' do\n  logger = double('logger')\n  expect(logger).to receive(:info).with('{}')\n  poller = Aws::SQS::QueuePoller.new('https://queue.url')\n  poller.poll(idle_timeout:0.5) do |msg|\n    logger.info(msg.body)\n  end\nend\n``\n. Availability zones are not the same as a region. The SDK requires a region when constructed so that I can produce a valid https endpoint. In the stack trace above it looks like you have configured the region asus-east-1c` which is an AZ, not region. You client construction should instead look something like:\nruby\nebs = Aws::ElasticBeanstalk::Client.new(region: 'us-east-1', ...)\n. Sorry, clicked close-and-comment instead of comment. It looks like the referenced forum gives the information necessary to get EB to launch your reserved instances. Please feel free to re-open if you have an issue with the SDK.\n. Currently the Aws::Signers::V4 class is marked @api private and has not been considered a public interface. This has allowed us to make interface changes on the signer as needed.\nA few months back the signer and credential interfaces were updated to resolve a race condition in the refreshing credential interface. The signer calls the following methods on the credentials object:\n- #access_key_id\n- #secret_access_key\n- #session_token\nUnfortunately, if a refreshing credentials object happens to refresh between invocations of those methods (some of which are called multiple times by the signer) then you make get miss-matched keys. To resolve this race condition, refreshing credentials added a #credentials method that returned the triplet of credential values for signing.\nThat said, I was discussing with some of the other SDK engineers yesterday. It seems like now would be an appropriate time to extract the sigv4 signer into a separate project to make it accessible, to document it, and to give it a stable interface.\nAs a temporary work-around, and I know this is non-optimal, you can construct a new signer and give it fresh credentials each time you sign.\n. I've moved this to our feature requests backlog. That said, I consider this a high priority and hope we can get this work done quickly.\n. Thanks!\n. The documented method signature is #copy_from(source, options = {}). The source param can be a String, Hash, or Aws::S3::Object. Yes, the implementation required you to pass two hashes if source was a hash.\nI went ahead and added the ability to merge the options into the source/target hash.\n. Thanks.\n. The issue is in your configuration. You specify the region with :region => \"us-west-2a\". This is the name of an availability zone, not a region. Try replacing this with us-west-2 and the networking error should go away. This happens because the region name is used to construct an endpoint, in this case ec2.us-west-2a.amazonaws.com, which is not a valid cname. \n. Since this was the second or third issue I've seen raised by users that were confused by the unhelpful error message returned from the networking stack, I've added some validation that will reject the region if it is an availability zone. This will go out with our next release. Thanks for reporting this issue.\n. I've sent a contribution up-stream. It appears this error is only raised when you attempt to create a bucket you already own outside of the classic (us-east-1) region. This will be pulled in with our next S3 update.\n. Error log :  \nSSL_connect returned=1 errno=0 state=SSLv3 read server certificate B: certificate verify failed\nThis error indicates that Ruby's OpenSSL was unable to verify the peer certificate while establishing an SSL connection to Amazon S3. This particular error is very common when your system does not have a correctly configured openssl library with certificate. For example, this happens on all Windows installation of Ruby.\nAs a work-around the SDK ships with an optional SSL cert bundle. Browser frequently bundle their own certs for the same reason. You can opt-in to use the bundled cert by calling:\nAws.use_bundled_cert!\n. This is currently a limitation of the #copy_from and #copy_to methods. As a workaround you use the Aws::S3::Client#copy_from method directly:\nruby\ns3 = Aws::S3::Client.new\ns3.copy_from({\n  bucket: 'target-bucket',\n  key: 'target-key',\n  copy_source: \"source-bucket/source-key?versionId=123456789...\"\n})\nYou need to ensure the source key is correctly escaped. You can call Seahorse::Util.uri_path_escape(key) to ensure the key is correctly escaped. You don't have to worry about the bucket name, S3 limits bucket names to characters that do not require escaping. You might have to call Seahorse::Util.uri_escape on the version id value. I don't know what characters might appear in the version ID, but escaping the ID should be perfectly reasonable. \nI'll mark this issue as a feature request to add support for copying versioned IDs to the resource interfaces as a feature request.\n. The AWS SDK for Ruby accepts all input parameters as Ruby style hashes with snake_cased symbolized keys. This is the convention in Ruby. The AWS CLI uses JSON with non-inflected keys. That said, they are not the same as what goes over the wire. This is especially true for AWS Query and REST-XML based services.  Similar to how the Ruby SDK standardizes the interface for Ruby, the CLI standardizes the input on JSON, regardless of the wire protocol.\nYour method will work most of the time. It does however lack the context of what type of data you are translating. There are some context sensitive hashes that do not use symbolized snake_cased keys. These are hashes that represent user data. If you know the name of the operation, it would be possible to provide a context aware translation that does this correctly using the service API model.\n. I did not mean to imply that the code exists, only that it would be possible to implement it correctly, but only if the operation is known. \nIt is not possible to stub the response correctly without knowing what response is being stubbed. The translation from the CLI requires knowledge about the shape of the data. Both tools share a definition of the API that could be used to guide the translation.\nWe can track this as a feature request and see what general interest there might be.\n. Some notes:\n- Add entries into the CHANGELOG and possibly the UPGRADING guides.\n- The CachedBucketRegionHandler in the S3 request signer plugin was previously forcing known us-east-1 buckets to use the s3-external-1 endpoint. We should consider if this is the correct endpoint or if the classic endpoint is to be used.\n- The comment block above the BucketSigningErrorHandler is not longer relevant. This handler now only deals with wrong region errors.\nLook for more code comments.\n. Yes and yes. This is what is expected, and we use the same escape in all cases. \n. This is a limitation of sigv4 presigned URLs. It is handled in the presigned as the error messages received from the api can be hard to decipher and frequently are consumed by users that did not generate the url.\n. Sorry for the slow merge. The fix looks good. We may be able to simplify the path expression and replace it with a pathAny matcher. Semantically it would be the same as the pathAny and pathAll matchers may not return empty lists.\nThis should go out with the next release.\n. I reproduced this myself like so:\n``` ruby\ndevicefarm = Aws::DeviceFarm::Client.new(region: 'us-west-2')\narn = devicefarm.list_devices.devices.first.arn\nresp = devicefarm.list_device_pools(arn: arn)\nresp.device_pools.first.type\n=> nil\n```\nLooking at the wire-trace, this is the response I received from #list_device_pools:\njson\n{\n  \"devicePools\": [\n    {\n      \"arn\": \"arn:aws:devicefarm:us-west-2::devicepool:082d10e5-d7d7-48a5-ba5c-b33d66efa1f5\",\n      \"description\": \"Top devices\",\n      \"name\": \"Top Devices\",\n      \"rules\": [\n        {\n          \"attribute\": \"ARN\",\n          \"operator\": \"IN\",\n          \"value\": \"[\\\"arn:aws:devicefarm:us-west-2::device:E9FA7276F0B4419BBB79B45886F6D211\\\",\\\"arn:aws:devicefarm:us-west-2::device:E6015EA88739458A8C6DD42D13637AFE\\\",\\\"arn:aws:devicefarm:us-west-2::device:D984258FE6044E1B94ED57E8D92B059B\\\",\\\"arn:aws:devicefarm:us-west-2::device:A2A04E0537B74149AED689F213D54C39\\\",\\\"arn:aws:devicefarm:us-west-2::device:1C0987A6E3A0441BA3A74320B2DFE8C9\\\",\\\"arn:aws:devicefarm:us-west-2::device:71574720B62A4A688B02A491C4A311AE\\\",\\\"arn:aws:devicefarm:us-west-2::device:5931A012CB1C4E68BD3434DF722ADBC8\\\",\\\"arn:aws:devicefarm:us-west-2::device:035b999777c0445d818566fdd49a2478\\\",\\\"arn:aws:devicefarm:us-west-2::device:9E515A6205C14AC0B6DCDBF3FC75BC3E\\\",\\\"arn:aws:devicefarm:us-west-2::device:BC44B6802F134918BDAB6FB4F38C37CC\\\"]\"\n        }\n      ]\n    }\n  ]\n}\nIt appears the API documentation is incorrect and that the service can omit the type. I don't know if this intentional or not, but it needs to be better documented by the service.  I'm going to close this issue. If you need help raising this issue with device farm, please let me know.\n. I took a look at this and came up with a fix. I'll be adding \n``` ruby\nrequire 'uri'\nuri = URI.parse('https://foo.com')\nputs \"SCHEME: #{uri.scheme} PORT: #{uri.port}\"\n=> SCHEME: https PORT: 443\nlets change the scheme\nuri.scheme = 'http'\nthis demonstrates the issue\nputs \"SCHEME: #{uri.scheme} PORT: #{uri.port}\"\n=> SCHEME: http PORT: 443\nthis unfortunate behavior is masking the issue, i would have\nexpected to see 'http://foo.com:443'\nputs uri.to_s\n=> http://foo.com\n```\nThe bug sneaks by our unit tests beause we are returning the result of #to_s from the URI which is shown to be buggy above. The reason for this is the string formatting for the URI differs based on the URI class. There is both URI::HTTP and URI::HTTPS. Changing the scheme on a URI::HTTPS to http will cause it to omit the port number that is expected when the scheme and port don't match. This causes the bug is singing.\nI've got an incoming fix for this shortly.\n. Another option is to use the aws-sdk-rails gem. This gem will automatically log to the Rails.logger. It will also add support for sending emails with Amazon SES.\n. Sorry for the accidental closure. A git commit referenced the wrong issue (998 instead of 988). I haven't looked at this pull request yet. I'll check with @awood45 to see if we can get this looked at.\n. @agperson @JonathanSerafini Sorry for the radio silence on this issue. This has not been intentional, but I understand why it sucks.\nThe frank reason why it has taken so long to approve this is that we are collaborating with the other SDK and tool teams on how to best to share configuration. The ~/aws/.credentials file is an important part of this. We receive frequent requests to support additional features available in the CLI and we want to make certain that we can support the same credential loading strategies across all of our tools (Java, .NET, PHP, Ruby, Python, JavaScript, Go-lang, CLI, etc).\nI agree this functionality is useful and I would like to merge it. I also would like to ensure that the format and behavior is what we are going to expect moving forward.  @awood45 has been reaching out to other SDK maintainers. What we are trying to avoid is a situation where each tool does something different, which is where we find ourselves largely today. We are currently having discussions on how to expand the shared configuration features, especially those around credentials.\nPlease feel free to re-open the pull request. While we have not merged it, the work is valuable. Our ignoring the pull request was not a good communication strategy. I apologize for this.\n. I merged this and corrected the default profile. This means a credential provider chain without config will still source the shared credentials file.\n. I've been able to reproduce the issue locally. It appears to be related to a recent update to the :instance_exists waiter where a non-empty list check was added. The failure is caused when the response data is nil which happens when you receive a non-200 status code. This is common for invalid instance ids or instance ids which do not belong to your account. The jmespath gem is simply saying you can not run a length check on a nil value.\nI'll try to submit a PR shortly to address this issue.\n. I'm not very familiar with that particular API, but my guess is that you would simply pass that has multiple times, each time with the key \"securityGroupIds\" and then different string values:\n[\n  {key: 'securityGroupIds', string_value: \"security-group-id-1\"},\n  {key: 'securityGroupIds', string_value: \"security-group-id-2\"},\n]\n. I've closed this issue and moved it to our feature requests.\n. @skippy I'm curious about your use case. Your original issue made it sound like you wanted to provide an IO object to the #put_object operation and it was rejecting this because it required a size. Your last comment makes it sound like you are trying to download a ~10 mb object from S3 and you down want to buffer it into memory on disk, but rather stream it out.\nAmazon S3 requires all calls to PUT object have the total content length specified as a header. This prevents the SDK from being able to stream without buffering the object to compute the total size. It is possible that the SDK could buffer at most 5MB into memory and then switch between a managed multiple upload of 5MB parts or a single PUT object request. However, this would be a new utility that would need to be written.\nHowever, if you are attempting to stream an object out of S3 you can simply call Aws::S3::Client#get_object or Aws::S3::Object#get passing in a :response_target option. This can be an IO object where bytes will be written to. If the response target IO object does not response to truncate then that will disable retries of failed GET requests.\n. The endpoint in your message above appears to contain some non-printable UTF-8 characters when I copied and pasted it. \n\"dyna\\U+FFE2\\U+FF80\\U+FF8Bmodb.\\U+FFE2\\U+FF80\\U+FF8Bus-ea\\U+FFE2\\U+FF80\\U+FF8Bst-1.\\U+FFE2\\U+FF80\\U+FF8Bamazo\\U+FFE2\\U+FF80\\U+FF8Bnaws.\\U+FFE2\\U+FF\"\nA few things to consider:\n- If you have copied/pasted this endpoint, e.g. from a web-page or word doc this could introduce some issues. \n- Also, I notice that it is looking for port 80. The SDK will never use HTTP unless you've explicity configured an HTTP (not HTTPS) endpoint.\nTo connect to dynamodb you should only configure the region name, you should not need to configure the full endpoint:\nruby\nddb = Aws::DynamoDB::Client.new(region: 'us-east-1')\nddb.list_tables\n=> #<struct Aws::DynamoDB::Types::ListTablesOutput table_names=[\"aws-sdk\", ...], last_evaluated_table_name=nil>\nHope this helps. Please feel free to re-open if you continue to run into issues.\n. @hakunin You should be able to simply Ignore the presigned-post #url and use localhost in your form. Is this not working?\n. I've verified the operation is present in the latest version of the SDK as well. It appears you have an issue with your application loading the older version of the gem. I would double check what version is in your Gemfile.lock.\n. Looking at your code example, I see what the issue is. I see how this would be confusing, but the Aws::Log::Formatter is not intended to be given as the formatter for the Ruby standard-lib logger. It is to be given to an client so that it knows how to format the message that it will send to the logger.\nFor example:\nruby\ns3 = Aws::S3::Client.new({\n  logger: Logger.new($stdout),\n  log_formatter:  Aws::Log::Formatter.new('[aws_operation=\":operation\"]')\n})\ns3.list_buckets\nThis will produce messages like:\nI, [2015-12-09T11:37:11.897427 #99733]  INFO -- : [aws_operation=\"list_buckets\"]\nNotice all of the prefixed information on my log line that wasn't part of my pattern. That was being appended by the std-lib logger. I can change this with the #logger:\n``` ruby\nlogger = Logger.new($stdout)\nlogger.formatter =  proc do |severity, datetime, progname, msg|\n  \"LEVEL: #{severity} TIME: #{datetime} MSG: #{msg}\\n\"\nend\ns3 = Aws::S3::Client.new({\n  logger: logger,\n  log_formatter:  Aws::Log::Formatter.new('[aws_operation=\":operation\"]')\n})\ns3.list_buckets\n```\nThis generates a log message like:\nLEVEL: INFO TIME: 2015-12-09 11:41:48 -0800 MSG: [aws_operation=\"list_buckets\"]\nHope this helps!\n. Thanks for reporting the issue and providing all of the context. Your hunch was correct. I've pushed a fix and added a test to prevent regressions. Thanks!\nI hope to release this fix shortly.\n. @bosskovic You do not need to check for an error code. The default behavior of the SDK is to raise an error if the request is not successful. Unless you have explicitly disable the raising response errors behavior, then you can simply call the #put_bucket_logging method and continue on, rescuing an error for failure cases.\n. I took the change and made the minor change myself. We should see about getting a maintenance release out with this fix so the affected Aws::CloudSearchDomain::Client users can pickup the work-around for the Content-Length bug.\n. I pulled this locally and made a small change to ensure this worked for all URIs by default. Your changes have been merged into mainline. Thanks for the PR!\n. I did some digging on this today. It appears that the SDK is doing the correct thing. Notice that it compacts the args list before splatting them into the HTTP.new call. This causes Net::HTTP to default p_addr to :ENV.\nI was able to change your example to working by removing the leading dot from the no_proxy environment variable:\nENV['no_proxy'] = 'amazonaws.com'\nHope this helps.\n. Support for AES/GCM/NoPadding should be in the public SDK now.\n. I've submitted a PR that addresses this issue and adds a supporting test.\n. This has been merged and will be part of our next release.\n. I've added a note to the README to clarify the use Aws.config.\n. I've added a code comment. I'd like to avoid buffering the object to get the auth tag from the end of the body by making a GET Object with range request. This should allow us to initialize the cipher with the auth tag straightway.\n. I've added https://github.com/aws/aws-sdk-ruby/pull/1204 which has an implementation my suggestion above.\n. I've merged these commits here: c3376f02c0490273674a37879a688438326bf84a\nThanks for the excellent contribution!\n. Closing in favor of 6acaac487c1a659e49da68e5f4694ada85fd7f33. Travis does not yet support using 2.3 as a Ruby version, but rather you must specify the explicit 2.3.0 version. This is an issue with an upstream dependency of theirs and they are waiting for it be resolved.\n. There is something going on in your environment that is causing this error. It is not your code sample. The Aws::ParamConverter module automatically mixes in the Seahorse::Model::Shapes module which is where FloatShape is defined.\nAn actual stack trace might be helpful. Also any other information you have, e.g. can you call other APIs, what is your $LOAD_PATH, etc. \n. I took a quick look at this. It appears to be very straightforward to provide localized options to Oj to avoid getting the system default of symbol keys. Here is what I put together to test. This prevents the example above from failing.\n``` diff\ndiff --git a/aws-sdk-core/lib/aws-sdk-core/json.rb b/aws-sdk-core/lib/aws-sdk-core/json.rb\nindex 3283139..77b4df7 100644\n--- a/aws-sdk-core/lib/aws-sdk-core/json.rb\n+++ b/aws-sdk-core/lib/aws-sdk-core/json.rb\n@@ -23,7 +23,7 @@ module Aws\n     class << self\n   def load(json)\n\n\nENGINE.load(json)\n\nENGINE.load(json, *ENGINE_LOAD_OPTIONS)\n       rescue ENGINE_ERROR => e\n         raise ParseError.new(e)\n       end\n@@ -40,19 +40,19 @@ module Aws\ndef oj_engine\n     require 'oj'\n-        [Oj, [{ mode: :compat }], Oj::ParseError]\n+        [Oj, [{ mode: :compat }], [{ symbol_keys: false }], Oj::ParseError]\n   rescue LoadError\n     false\n   end\ndef json_engine\n-        [JSON, [], JSON::ParserError]\n+        [JSON, [], [], JSON::ParserError]\n   end\nend\n# @api private\n-    ENGINE, ENGINE_DUMP_OPTIONS, ENGINE_ERROR = oj_engine || json_engine\n+    ENGINE, ENGINE_DUMP_OPTIONS, ENGINE_LOAD_OPTIONS, ENGINE_ERROR = oj_engine || json_engine\n\n\nend\n end\n```\nIf someone wants to put together a pull-request with this and add a unit test that sets the global default and clear it afterwards to ensure we don't regress, I'll happily merge it.\n. @jeffchuber What is your exact RUBY_VERSION?\n. I think this may be an issue with how the service's regional signing name is extracted from the endpoint:\nhttps://github.com/aws/aws-sdk-ruby/blob/master/aws-sdk-core/lib/aws-sdk-core/plugins/request_signer.rb#L53\nTo demonstrate:\n``` ruby\nclient = Aws::IoTDataPlane::Client.new(\n  endpoint: 'https://data.iot.eu-west-1.amazonaws.com',\n  region: 'eu-west-1',\n)\ncilent.config.sigv4_region\n=> \"io.eu-west-1\" -- oops, should be \"eu-west-1\"\n```\nThis shouldn't be to difficult to fix. Thank you for reporting the issue.\n. Looks good. We should add a unit test that uses a before hook to set the Oj default option and then disables it in an after hook (to ensure we don't leak configuration).\nAlso worth noting, the mode: :compat is necessary for both load and dump. Previously we only had this on dump. Without compat mode on, Oj attempts to convert identifiers that begin with \":\" into symbols which we do not want.\n. +1, these changes look fine to merge.\n. The bucket exists waiters is checking for a 404 status code, not a specific error code. The HTTP spec requires the the response to a HEAD request not contain a body. Since Amazon S3 normally returns error codes and messages in the body, the HEAD response becomes a simple HTTP 404 not found.\nThis is where things break down. The client response stubbing interface for errors cheats a little and assumes that the status code is a 400 for all stubbed response. This is largely because we don't have an exhaustive list of errors and their status codes. This could be improved with what we do know, but currently it does not. To make things more difficult, we do have have sufficient information from the status code to disambiguate between NoSuchKey and NoSuchBucket 404 errors. \nThat said, you can achieve what you require, but it is small bit of extra work.\n``` ruby\nAws.config[:s3] = {\n  stub_responses: {\n    head_object: { status_code: 404, headers: {}, body: '', }\n  }\n}\nAws::S3::Object.new(key: 'k', bucket_name: 'b').exists?\n=> false\n```\nHope this helps.\n. I've added more information about the given shape in the unsupported case.\n. I merged this locally, it has been pushed to master already.\n. Additional code documentation added.\n. I should add that v1 should support the longer EC2 instance IDs. The SDK does not attempt to validate the length or pattern of instance IDs. You should be able to receive and give the new longer instance IDs without issue.\n. The change looks good, but I would be more inclined to see a functional test for a specific service to exercise this. Try adding a test to the IoT data plane client spec that does this.\n. Look good, thanks for the contribution!\n. Thanks for reporting the issue. This has now been fixed and should be part of our next release.\n. The copy is correct. The intended word is ini, not init. This is describing the file format, which is ini. Thank you for the PR though! We do appreciate the help.\n. @whazzmaster Good insights.\n\nI believe that what needs to happen is that, in ObjectCopier#copy_object needs to pass in the source client explicitly (if it exists) to the options passed into ObjectMultipartCopier#copy\n\nYou are correct. What is needed is a client configured for the region of the source bucket. It should be possible to automatically extract this from the source object if it has been passed in as an instance of Aws::S3::Object or Aws::S3::ObjectVersion. \nFor situations where an object is not given, we could add the following options:\n- :source_region - Use this to construct a new client, using general configuration from the current client, overriding only the region.\n- :source_client - Exactly what you describe above\nI think what I'm still considering is should we continue attempting to copy from the same region when we are not certain? If so, I think we should also rescue the 301 error an raise a more helpful error, that we could not discover the size of the source object and that they will need to pass one of the above options.\nThoughts?\nSource client, or possibly source region.\nAlternatively it could pass in a source region. The SDK could construct a client for the appropriate region and then HEAD the object there. This has the additional benefit that it would allow the customer to pass in :source_region when the source object is a hash or string.\nAnother option is to support a HINT from the user about the size of the source object or the prefered\n. Thanks for reporting the issue. I've put together a fix that will work for the SSE CPK options and any other future options by inspecting the API model for S3 to know what options the call accepts. I've also added an integration test to ensure this continues to work.\n. @chupakabr Are you by chance testing your code in JRuby? Or rather, are you experiencing the original issue while using JRuby? I ask because there was recently an issue/regression with how classes reported their name, which causes this chunk of code to cause the error you describe.\n. Capitalizing the identifier is not sufficient to convert it into a class Name. Some services have multiple words. For example :dynamodb capitalizes to Dynamodb but would need to be DynamoDB.\nMy comment above was also highlighting that this is only an issue if your module does not correctly report its name, which is a feature of Ruby modules. There was a regression in JRuby that caused this not to work. I believe this has been corrected.\nLastly, I'm not happy about how this code works today. If I had a do-over, I would instead not generate the exception classes at runtime at all. Unfortunately this behavior can not change without a major version bump of the SDK, which is not currently planned.\n. Looking at your code, I don't see anything obviously wrong. To better understand what the root cause is, we will need to compare the authentication string-to-sign and canonical-request as computed by the SDK and by the service. This should give us an accurate picture of what has caused the signing error. \nCan you apply the following to your code, before you make the #upload_file request? It will cause the SDK to print to standard out the string-to-sign and canonical request that is computing.\n``` ruby\nAws::Signers::V4.send(:prepend, Module.new do\ndef string_to_sign(args)\n    result = super(args)\n    puts \"STRING_TO_SIGN: #{result}\"\n    result\n  end\ndef canonical_request(args)\n    result = super(args)\n    puts \"CANONICAL_REQUEST: #{result}\"\n    result\n  end\nend)\n```\nNext, we need to extract the same values from the error returned from the service. Wrap your call to #upload_file in a begin rescue block so we can access the http response body. The HTTP response body contains the string-to-sign and canonical request as computed by the service. \nruby\nbegin\n  obj.upload_file(fileName, {\n    server_side_encryption: 'aws:kms',\n    ssekms_key_id: ssekms_key_id\n })\nrescue => error\n  puts error.context.http_response.body_contents\nend\nCan you then copy and paste both outputs to this issue?\n. @drewacl I'm glad to see the additional debug output was sufficient to show the cause of the signature error. It looks like you closed this issue, were you able to discover the root cause?\n. Sorry for the slow response and thank you for the pull request. Currently I'm not inclined to merge this. The primary reasons you have already outlined:\n- Does not support single request PUT Object calls\n- Does not reflect bytes correctly when individual requests are retried.\nAdditionally, I would like to see the block yield two params:\nruby\ns3.upload_file(...) do |progress, total|\n  puts \"uploaded #{progress} / #{total} bytes\"\nend\nI'm going to close this issue. Its still on our public back-log. If you are so inclined, I'd be happy to discuss strategies for making the generic solution possible.\n. @newellista It appears that the linked jruby issue has been resolved. I'm not sure when they plan to release an update, but it doesn't appear the SDK requires any change currently. Please feel free to re-open if needed.\n. I took a moment to review this. The code looks good. I had one piece of feedback above we need to address and then I'm happy to merge this in. Good work!\n. Thanks for the contribution! This should go out with our next release. \nI'm going to do a small amount of refactoring on the tests. We've been moving away from unit testing the handlers in isolation and instead functionally test the client class they are applied to. This has the benefit of ensuring the plugin is applied correctly. Your tests are organized well enough that moving this should be pretty straightforward.\n. I just ran the following code without issue:\nruby\ns3 = Aws::S3::Client.new(http_wire_trace:true, region:'ap-northeast-2')\ns3.list_objects(bucket:'aws-sdk-ap-northeast-2')\nWhich generated the following HTTP wire trace:\nopening connection to aws-sdk-ap-northeast-2.s3.ap-northeast-2.amazonaws.com:443...\nopened\nstarting SSL for aws-sdk-ap-northeast-2.s3.ap-northeast-2.amazonaws.com:443...\nSSL established\n<- \"GET /?encoding-type=url HTTP/1.1\\r\\nContent-Type: \\r\\nAccept-Encoding: \\r\\nUser-Agent: aws-sdk-ruby2/2.2.17 ruby/2.2.2 x86_64-darwin13\\r\\nX-Amz-Date: 20160218T192931Z\\r\\nHost: aws-sdk-ap-northeast-2.s3.ap-northeast-2.amazonaws.com\\r\\nX-Amz-Content-Sha256: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\\r\\nAuthorization: AWS4-HMAC-SHA256 Credential=AKIAJUNH63P3WCTAYHFA/20160218/ap-northeast-2/s3/aws4_request, SignedHeaders=host;x-amz-content-sha256;x-amz-date, Signature=6c0f73f010cb260de922d5194305840f7d1d7759b332c4113dc43d4fef9596f6\\r\\nContent-Length: 0\\r\\nAccept: */*\\r\\n\\r\\n\"\n-> \"HTTP/1.1 200 OK\\r\\n\"\n-> \"x-amz-id-2: 73iBOoaEYCcsMQGt5fYC/Lde2l4YL9GB9lHE/TiFY6ARlONIF43puCH1bExXxfDLfjeRiVfmke0=\\r\\n\"\n-> \"x-amz-request-id: B780DC7D2C8A8819\\r\\n\"\n-> \"Date: Thu, 18 Feb 2016 19:29:33 GMT\\r\\n\"\n-> \"x-amz-bucket-region: ap-northeast-2\\r\\n\"\n-> \"Content-Type: application/xml\\r\\n\"\n-> \"Transfer-Encoding: chunked\\r\\n\"\n-> \"Server: AmazonS3\\r\\n\"\n-> \"\\r\\n\"\n-> \"118\\r\\n\"\nreading 280 bytes...\n-> \"<?xml version=\\\"1.0\\\" encoding=\\\"UTF-8\\\"?>\\n<ListBucketResult xmlns=\\\"http://s3.amazonaws.com/doc/2006-03-01/\\\"><Name>aws-sdk-ap-northeast-2</Name><Prefix></Prefix><Marker></Marker><MaxKeys>1000</MaxKeys><EncodingType>url</EncodingType><IsTruncated>false</IsTruncated></ListBucketResult>\"\nread 280 bytes\nreading 2 bytes...\n-> \"\\r\\n\"\nread 2 bytes\n-> \"0\\r\\n\"\n-> \"\\r\\n\"\nConn keep-alive\nI'm not sure why Net::HTTP is unable to open a connection to the endpoint. Can you ping s3.ap-northeast-2.amazonaws.com from the same host?\n. No problem. Please feel free to re-open if there is anything else.\n. Thank you for reporting the issue and for the excellent reproduction steps. Off the top of my head I'm not sure what the root cause it, but I'll take a look.\n. I found the issue and the fix was pretty straight forward. When stubbing response data with a hash, the data is used to generate an HTTP response. There was a bug in how we created the response body for Query based services that used flattened XML maps. The fix should go out with our next release.\n. I went ahead and added a pair of utility methods to the SDK:\nhttps://github.com/aws/aws-sdk-ruby/blob/master/aws-sdk-core/lib/aws-sdk-core/checksums.rb\nThe sigv4 signer and S3 MD5 plugin both use these methods. This should improve memory usage in these scenarios. There are a couple of outstanding issues:\n- Aws::Glacier::Client - Glacier requires the payload of an uploaded archive to be sent with a tree-hash that is computed from the archive in rolling 1MB chunks. Unless we can get OpenSSL::Digest to compute a digest of a file part, this will have to be done in Ruby. We could read the 1MB in smaller chunks to reduce memory usage though.\n- Aws::S3::Object#upload_file - Uses a wrapper around a file object to send to Aws::S3::Client#upload_part. These file part wrappers are not File or Tempfile objects themselves as they represent a slice of the entire file. I'm not sure how to get OpenSSL::Digest to compute a digest of a file part. If I can find this, then we can improve the memory usage here as well.  Alliteratively, we can also reduce the chunk size that we read and pass to the digest.\nPlease feel-free to re-open to discuss alternatives or also feel free to send a PR.\n. @manabusakai Your usage is correct. This bug was fixed in the previous release. You can see a related issue here: https://github.com/aws/aws-sdk-ruby/issues/1097\nIf you update to v2.2.18, then this should be fixed.\n. Good catch. I expanded the test to have multiple values and fixed the bug. This should go out with todays release.\n. @albertogg @jv-dan Thank you for reporting this issue. Looks like this was an unintended interaction between the default stubs plugin and the verify checksums plugin. The response stubbing is generating a fake response where the checksums are the string literals \"String\".\nAny easy fix would be to have the SQS MD5 plugin simply not attempt to verify checksums when response stubbing is enabled. Thoughts?\n. Okay, I've pushed a fix with some additional tests around this. I'll see if it is possible to get a patch release published tomorrow with the fix.\n. Normally the Aws::DynamoDB::Client.new method would raise an argument error indicating that a :region has not been specified. I suspect your environment has exported a region environment variable. Here is the list of ENV keys it searches:\n- ENV['AWS_REGION']\n- ENV['AMAZON_REGION']\n- ENV['AWS_DEFAULT_REGION']\nHope this helps.\n. I've merged the PR. This should go out with tomorrows release. Thank you for the thorough bug report and PR.\n. This looks good. I think the only, non-necessary prior to merging, additions we could make it to improve this are:\n- Add a :copy_source_region option where users can specify the region to allow the SDK to construct a client and HEAD the source object when needed.\n- Document :copy_source_client\n. Thanks for the contributions! I've pulled branch locally and added to it support for object version copies.\n. Sorry, it looks like master changed between when I pulled your branch so this pull request didn't cleanly merge when I pushed. Master does contain all of your individual commits, just not a merge commit from this PR.\n. Thanks for the bug fix!\n. Thanks for the PR.\n. As far as I can tell, this feature is working as expected. Client response stubbing will always returns the next element in the response stub list for an operation on subsequent calls. If the final response has been returned, it will continue to return the final response. For example:\n``` ruby\ns3_prefix = ''\nAws.config[:s3] = {\n  stub_responses: {\n    list_objects: {\n      contents: [\n        { key: \"#{s3_prefix}template-a/docker-compose.yml\", last_modified: Time.now.utc },\n        { key: \"#{s3_prefix}template-b/docker-compose.yml\", last_modified: Time.now.utc },\n        { key: \"#{s3_prefix}template-c/docker-compose.yml\", last_modified: Time.now.utc },\n        { key: \"#{s3_prefix}not-a-template/docker-compose.xml\", last_modified: Time.now.utc }\n      ]\n    },\n    head_object: [\n      { metadata: { \"instance\" => \"t1.tiny\", \"artifacts\" => \"art1:art2:art3\" } },\n      { metadata: { \"instance\" => \"t2.tiny\", \"artifacts\" => \"art2:art1\" } } ,\n      { metadata: { \"instance\" => \"t3.tiny\", \"artifacts\" => \"art3:art2\" } }\n    ]\n  }\n}\ns3 = Aws::S3::Client.new\nproduces what you expect\ns3.list_objects(bucket:'aws-sdk').contents.each do |obj|\n  key = obj.key\n  metadata = s3.head_object(bucket:'aws-sdk', key:key).metadata\n  puts \"#{key} : #{metadata.inspect}\"\n  # template-a/docker-compose.yml : {\"instance\"=>\"t1.tiny\", \"artifacts\"=>\"art1:art2:art3\"}\n  # template-b/docker-compose.yml : {\"instance\"=>\"t2.tiny\", \"artifacts\"=>\"art2:art1\"}\n  # template-c/docker-compose.yml : {\"instance\"=>\"t3.tiny\", \"artifacts\"=>\"art3:art2\"}\n  # not-a-template/docker-compose.xml : {\"instance\"=>\"t3.tiny\", \"artifacts\"=>\"art3:art2\"}\nend\nevery HEAD object request replays the last element\ns3.list_objects(bucket:'aws-sdk').contents.each do |obj|\n  key = obj.key\n  metadata = s3.head_object(bucket:'aws-sdk', key:key).metadata\n  puts \"#{key} : #{metadata.inspect}\"\n  # template-a/docker-compose.yml : {\"instance\"=>\"t3.tiny\", \"artifacts\"=>\"art3:art2\"}\n  # template-b/docker-compose.yml : {\"instance\"=>\"t3.tiny\", \"artifacts\"=>\"art3:art2\"}\n  # template-c/docker-compose.yml : {\"instance\"=>\"t3.tiny\", \"artifacts\"=>\"art3:art2\"}\n  # not-a-template/docker-compose.xml : {\"instance\"=>\"t3.tiny\", \"artifacts\"=>\"art3:art2\"}\nend\nstart over with a new client, produces what you expect\ns3 = Aws::S3::Client.new\ns3.list_objects(bucket:'aws-sdk').contents.each do |obj|\n  key = obj.key\n  metadata = s3.head_object(bucket:'aws-sdk', key:key).metadata\n  puts \"#{key} : #{metadata.inspect}\"\n  # template-a/docker-compose.yml : {\"instance\"=>\"t1.tiny\", \"artifacts\"=>\"art1:art2:art3\"}\n  # template-b/docker-compose.yml : {\"instance\"=>\"t2.tiny\", \"artifacts\"=>\"art2:art1\"}\n  # template-c/docker-compose.yml : {\"instance\"=>\"t3.tiny\", \"artifacts\"=>\"art3:art2\"}\n  # not-a-template/docker-compose.xml : {\"instance\"=>\"t3.tiny\", \"artifacts\"=>\"art3:art2\"}\nend\n```\nAlso worth noting, If you are making a HEAD request for each element in your list objects response, you have 1 more key than you have stubbed responses. It may be that you are sharing a client instance between each test execution and are therefore leaking state. If your code is public, I'd be happy to take a look at it for you.\n. Good catch. Thanks for the PR!\n. Can you provide a full stack trace for one of the failures? It is difficult to debug without more context.\n. @grosser The linked documentation uses on method to generate a presigned URL. The actual implementation relies on Aws::S3::Presigner which is part of aws-sdk-core. You can construct a presigner directly:\n``` ruby\nrequire 'aws-sdk-core'\npresigner = Aws::S3::Presigner.new(client: Aws::S3::Client.new(...))\nurl = presigner.presigned_url(:get, bucket:'bucket_name', key:'object-key', expires_in:3600)\n```\n@awood45 Is correct that most of our higher level abstractions live in the resources gem, but this feature is essential enough for S3 that it lives in core. Hope this helps.\n. Thanks for the feature request. Depending on what testing framework you use, there are methods to achieve this. For example, using RSpec, you can do the following:\n``` ruby\nclient = Aws::S3::Client.new(stub_responses: true)\nsuccess\nallow(client).to receive(:delete_bucket).with(bucket:'aws-sdk').and_return(client.stub_data(:delete_bucket))\nerror\nallow(client).to receive(:delete_bucket).with(bucket:'aws-sdk-not-empty').and_raise(Aws::S3::Errors::BucketNotEmptyError)\nall other requests will hit the default stubs\n```\nMoving this sort of functionality into the client stubbing interface could be helpful as well. The above example can be a bit verbose, and depends on using a mocking/stubbing library. Just sharing it in case it can help.\n. I agree. It also prevents the SDK from executing the request stack. Response stubbing very carefully only by-passes the network request. This ensures any handlers or plugins still trigger. The rspec #allow method will short-circuit all of these.\n. Thanks for the fix!\n. @mparramont Are you multithreading or using some other form of concurrency with the SDK? The AWS SDK for Ruby does rely on Ruby's #autoload which has been known to break down under certain scenarios. It can happen that a class is defined and accessed before it's methods have been added to the class.\nIf you see this error again in the future, you may consider adding something like this to your app and see if this resolves the issue:\n``` ruby\nrequire 'aws-sdk'\nAws.eager_autoload!(services: ['S3'])\n```\n. Thanks for the fix!\n. Thanks for the fix!\n. The quotes are actually part of the return value as given by S3. I agree they are awkward, but the etag does contain them. Removing them now would also be a breaking change for anyone who is attempting to remove them, so we can not do that.\nAre you accessing the etag to compare it against the md5 of the object you PUT? If so, you might be interested to know that the SDK automatically generates the MD5 and sends it with the object as a header which ensures the object was uploaded correctly.\n. This issue has been reported against by another user and we have a minimal test case to reproduce the issue. I'll update this issue with more information as we work to resolve it. In short there is a concurrency issue with how the XML parser is choosing an issue that can create this issue in a multithreaded environment.\n. Here is the snippet I am using to reproduce the issue:\n``` ruby\nrequire 'aws-sdk-core'\nrequire 'thread'\nsqs = Aws::SQS::Client.new\nthreads = 10.times.map do\n  Thread.new do\n    sqs.send_message(queue_url:\"https://sqs.us-east-1.amazonaws.com/469596866844/trowe\", message_body:'string')\n  end\nend\nthreads.map(&:join)\n```\n. I've just pushed a commit that fixes both the documentation issue and the resource model issue for this method. This should go out with our next release tomorrow.\n. Thanks!\n. This is something I've been considering adding to the SDK recently. I'm still considering what the mechanism would be to allow for unauthenticated requests.\n. @musiaht Do you mean stricter pinning against jmespath or in jmespath against the json gem?\n. @nybblr Are you looking for an interface like this:\nruby\nreq = s3.presigned_request(:get_object, bucket:'...', key: '...')\nreq.uri #=> \"https://...\"\nreq.headers #=> { ... } authorization in here\nIf so, this is something we have strongly considered and is already on our back log. If not, could you provide a clarifying example of what you are looking for?\n. The AWS SDK for Ruby uses Ruby's autoload functionality to limit what files/classes are loaded at runtime. If you are only using Aws::EC2, then Aws::S3 code is not loaded.\nIf you are seeing increased memory usage, there can be multiple causes. If you are using Amazon S3, then you maybe be loading objects into memory instead of streaming them, for example. Also, multiple versions of Ruby have memory leaks in StringIO which is used by the SDK (you may find this goes away with an update of Ruby).\nIn the future, we are investigating a modularized SDK, where each service ships as a separate gem. We have no formal timeline on when or if this will happen.\n. All resource collection / has many associations are wrappers around paginated API calls. The only accepted arguments are :limit and pass-through arguments to the client operation. This is strictly always a hash. This is per the resources spec.\n. See the following pull-request:\nhttps://github.com/aws/aws-sdk-ruby/pull/1163\n. Have you run into an issue with these patches? They are a series of fixes for Net::HTTP bugs. The issues being patched:\n- Net::HTTP retry of idempotent operations is broken when the request body is streaming.\n- Net::HTTP retry of idempotent operations is broken when the response body is being yielded\n- Net::HTTP's implementation of 100-continue causes runtime errors when a non-100 continue response is returned.\nThe first two issues are patched by simply disabling all retries at the transport error. Net::HTTP provides no facility to do this with modifying the constant is uses internally to track what http verbs are idempotent. I have an open pull request that does so more carefully by disabling retries only when streaming.\nThe third issue could only be patched by replacing the entire transport request method. The actual change is quite small to fix the bug. \nRelevant pull requests against ruby/ruby:\nhttps://github.com/ruby/ruby/pull/951 (still open, but see also https://github.com/ruby/ruby/pull/1019)\nhttps://github.com/ruby/ruby/pull/949 (merged)\nI may need to revisit the range of Ruby versions to apply these patches to. It is possible some of these fixes made it into 2.3.0. The patches will need to continue to live with this library until all supported versions of ruby have been fixed.\n. Also worth noting, we do not implement retries from Net::HTTP. Issue title updated to reflect.\n. Very likely. We will need to determine what version of Ruby that Net::HTTP has been patched and scope the patches down to the affected versions.\n@cjyclaire and @awood45 would be the best individuals to work with on this.. The multipart uploader uses Aws::S3::Client under the covers. The client will, by default, retry specific errors up to 3 times. The Aws::S3::MultipartUploadError you are receiving has a #errors method which  provides access to an array of all of the errors rescued during upload. Can you try accessing these and giving more information about the specific errors/failures?\n. @steventen Can you provide more context to why and what you were providing as :endpoint in your v1 #url_for call? Normally, you should not configure the :endpoint for an S3 client or resource object. The endpoint is constructed for you from the :region option. The endpoint option exists primarily for testing against non-s3 hosts, e.g. local testing against mock S3 implementations.\nThe :virtual_host option enables using the bucket name as the host. This is limited to http (no https) because of issues with verifying the SSL peer certificate.\n. @steventen Thats correct. Your bucket name must be the cname you register as the virtual host. In addition to that, the request would have to be HTTP, not HTTPs.\n. I'm going to close this as we don't currently have plans to change the behavior here. Certain resource objects can not be loaded because they are only available from the AWS APIs via enumeration. The #load method can not be removed for these resource classes because it is defined in the base class for every resource. That is why they raise NotImplementedError. The actual error could have been a NoMethodError, but that isn't what we are raising today.\nWe have received similar reports of this being confusing for other resource classes, so it is something we are aware of and are looking into.\n. I suspect I know what is going on here. A little background first.\nWhen using one of our tools, if you configure your S3 client for \"us-east-1\", the tool will construct an endpoint like \"https://s3.amazonaws.com/....\". This is important, because this particular endpoint (and only this endpoint) is global. It relies on DNS to redirect the request to the actual region where the bucket lives.  This means, I could have configured my client for \"us-east-1\" and actually been redirected to \"bucket-name.us-west-2.amazonaws.com\".\nThe reason this matters is because of signature version 4. The v2 SDK uses sigv4 to authenticate the request. Sigv4 signatures contain the region. This means if you configure your client for \"us-east-1\" but get DNS directed to \"us-west-2\", the signature is invalid. For most requests, the SDK will correctly detect this error, redirect, cache the actual region and then emit a warning so that the additional hops can be avoided in the future.\nIn the case of HEAD Object this is not possible today. S3 follows the http rfc that said responses to HEAD requests must not contain a body. For this reason, we have no information from S3 why the request failed.\nTo verify this, you can try converting the HEAD Object call into a GET Object call. If you enable HTTP wire trace logging you should see multiple requests. If this succeeds, then my suspicion is confirmed.\nThere are two possible work-arounds:\n- Configure your client for the proper region\n- Force the SDK to use an older signature version (one that does not need to know the proper region).\nTo force the SDK to use the older signature version:\nruby\ns3 = Aws::S3::Client.new(signature_version:\"s3\")\nTo enable wire trace for debugging:\nruby\ns3 = Aws::S3::Client.new(http_wire_trace: true)\n. Glad to hear this helped resolve your issue. Please re-open if you run into any further issue.\n. This doesn't appear to support any of the other possible shared configuration values from ~/.aws/config besides region and credentials. These should be released together as we don't want the opt-in flag to initially only add partial support and then later pull additional configuration.\n. I finished going though this today. I have some additional feedback from simply a code-organization perspective that should affect functionality. I can go over this with you in person. If you add @api private to the Aws::SharedConfig class, then I would say we are good to ship (given the feedback above has been addressed).\n. Ship it!\n. This has been merged and will ship with our next release. I made a minor change to add a VpcExists waiter. Aws::EC2::VPC now has both exists and available waiters available.\n. Also, thanks for the contribution!\n. If the x-amz-expires param is correct, then I suspect the credentials you are using to sign the request with are expiring. Are you using session based credentials that have an expiration?\n. Thats correct. The credentials you used to sign the request expire. The presigned URL is valid only as long as the credentials used to sign it. The same thing would happen if you create a presigned URL using root account credentials, and then you rotate your account credentials.\n. That looks great. Please submit the PR.\n. Closed by https://github.com/aws/aws-sdk-ruby/pull/1187 -- Thanks @darcy!\n. @ravi05cse The thread-safety issue should be resolve in current versions of the SDK now. You should be able to update from 2.2.12 to the latest version to resolve this issue.\n. The bug here appears to be the SDK's assumption that the expires header is a parseable time format. The CLI is passing the value through unparsed as a String. If we were to switch this behavior today, users that rely on it being an instance of Time would be broken. We may have to attempt to do the conversion and then fall back on returning a string. Another option is to add an additional field, such as #expires_string which does not convert and the #expires member would be nil when a valid Time value can not be parsed.\nThoughts?\n. I did some looking into this today. Amazon S3 will allow you to persist ANY string value into the Expires headers when making a PUT object request. I disable the SDK's validation on PUT and I sent the value \"abc\".\nSubsequent HEAD Object reqeusts respond with the exact value given during PUT. This means I got back the string literal \"abc\". This means to me that the SDK needs to be robust enough to handle any possible invalid expiration header.  As per the RFC quoted above (emphasis added is mine):\n\nHTTP/1.1 clients and caches MUST treat other invalid date formats, especially including the value \"0\", as in the past (i.e., \"already expired\").\n\nGiven the SDK is not attempting to interpret the expires header, it should probably do a best effort. What I'm proposing is the following:\n- Failures to extract time data from timestamp headers should simply result in nil values. This means the #expires member of the #head_object response would be nil in the error cases.\n- Add an additional #expires_string member to the response, that extract the header as is, without attempting to parse. This allows access to the raw value in every scenario.\nThoughts? \n. I've submitted a pull request with a proposed fix/work-around.\n. We are open to adding waiters to any/every service. The are no concrete plans for work on SSM waiters at the moment, but we can track this as a feature request. Pull requests are also welcome.\n. As far as I know, you can not control the order of attribute value keys. The SDK uses Ruby hashes, which have ordered keys and will preserve the order as they are sent over the wire. The order they are serialized into the HTTP response is controlled by DynamoDB.\n. There are legacy reasons why Seahorse is not namespaced under Aws.  Changing this now would be a backwards incompatible change.\n. I've run across this problem previously. The :instance_exists waiter is describing the instances by id and is getting a valid 200 response from EC2. It appears that EC2 will allow you to describe an instance some time before you are able to tag the instance. It would be helpful if the SDK could provide a :wait_until_instances_are_taggable waiter, but I do not know of a way to poll the EC2 API for this state.  Ideally, Amazon EC2 would allow you to provide the tags in the #run_instances call. \nYou should be able to work around this limitation with a short sleep before the #create_tags call.\n. Great addition! I added a few small comments.\n. To add some additional context. I have been able to reproduce this issue when using Nokogiri and Ruby 2.3.0. It does not appear to reproduce with other XML parsing engines.\n. I've found and patched the issue. I'm going to try to cut a patch release shortly.\n. I have not seen this error reported previously. Client objects should be thread safe. Clients share a pool of Net::HTTP connections. Connections are removed from the pool while in use and returned once the request is completed.\nIt would be helpful if you could provide a minimal test case that demonstrates this issue under concurrency so we can track down the root cause.\n. Have you stopped seeing these errors after adding this to the list of retryable errors?\n. I'm going to close this issue against the SDK. We consume the API reference documentation from Route53 when building SDK docs. Once the source docs have been updated, this will trickle into the SDK docs.\n. Merged into master here: c3376f02c0490273674a37879a688438326bf84a\n. @nickfrandsen Try removing the forward slash section from the bucket name and move it to the prefix:\n``` ruby\nAws.config.update({\n  region: 'us-east-1',\n  credentials: Aws::Credentials.new('my_akid', 'my_secret')\n})\ns3 = Aws::S3::Client.new\nresp = s3.list_objects(bucket: 'bcteam-1b299a00-4ea3-2f92-b048-03f6ca07e7f0', prefix:'15_minute_data')\n```\nBucket names are never permitted to contain a forward slash. Everything after the slash is actually part of the object key(s).\n. I reproduced this issue locally and confirmed it is an issue with the forward slash in the bucket name. I've added code to raise a helpful error message in this scenario.\n. I find it curious that each request takes a constant 5+ seconds. Have you have any more luck with this? I have not previously heard of any similar issues. If this is still a problem we should try instrumenting your client to track down where the client is spending time.\n. I was able to setup and run DynamoDB locally and did not experience any of the reported performance issues.\nruby\nAws> dynamodb(endpoint:'http://localhost:8000').put_item(table_name:'aws-sdk', item:{'id' => 'abc'})\n[Aws::DynamoDB::Client 200 0.069221 0 retries] put_item(table_name:\"aws-sdk\",item:{\"id\"=>{s:\"abc\"}})  \n=> #<struct Aws::DynamoDB::Types::PutItemOutput attributes=nil, consumed_capacity=nil, item_collection_metrics=nil>\nI'm using Java version 1.8.0_92 on my macbook with Ruby 2.3.0. I used the following command to start the most recent version of DynamoDB local:\njava -Djava.library.path=./DynamoDBLocal_lib -jar DynamoDBLocal.jar -sharedDb\n. It sounds like there is an issue in your environment. I'm going to close this issue as this does not appear to be an issue with the SDK. Please feel free to re-open this issue if you have more information or can provide a case to reproduce this with the SDK.\n. There is not a public interface for converting DynamoDB stream events to vanilla Ruby hashes and types. I think this would be a helpful interface though. We can track this as a feature request.\n. I've moved this to our public backlog.\n. Currently Aws::DynamoDBStreams::Client#get_records returns the DynamoDB attribute values as Ruby struct objects that have the union of possible types, i.e. #<struct s=nil ss=nil n=123 ns=nil ...>. The Aws::DynamoDB::Client has a plugin that automatically converts these attribute value structs to vanilla Ruby values (e.g. 123 from the previous example). It is possible to update this method to behave the like Aws::DynamoDB::Client operations, but it would have to be an opt-in feature to preserve backwards compatibility.\nAdding a utlity to parse the JSON document passed to the Lambda function would be pretty straight-forward. We have two options for this interface:\n- Return a vanilla Ruby hash as a result of a JSON parse, converting only the attribute values.\n- Return a struct object in the same format as #get_records with the attribute values converted to simple Ruby objects. \nThoughts?\n. > I kind of prefer the 1st option, so I can freely use the hash in the same way I use hashes returned from the DynamoDB queries, scans etc.\n@phstc Can you clarify what you mean? Aws::DynamoDB::Client today does not return a hash object in response #get_item, #query or #scan. These responses are primary composed of nested struct objects. Only when you reach the attribute values of the response, does it convert down to a hash.\nI think maybe the options I proposed above were poorly phrased. Given the following, completely made-up, JSON document as sent to a lambda function:\njson\n{ \n   \"awsRegion\": \"us-east-1\",\n   \"dynamodb\": { \n      \"ApproximateCreationDateTime\": 123456789,\n      \"Keys\": { \n         \"id\" : { \n            \"S\": \"abc\",\n         }\n      },\n      \"NewImage\": { \n         \"id\" : { \n            \"S\": \"abc\",\n         },\n         \"name\" : { \n            \"S\": \"new-name\"\n         },\n         \"size\" : { \n            \"N\": 456\n         },\n         \"enabled\" : { \n            \"BOOL\": true\n         }\n      },\n      \"OldImage\": { \n         \"id\" : { \n            \"S\": \"abc\",\n         },\n         \"name\" : { \n            \"S\": \"old-name\"\n         },\n         \"size\" : { \n            \"N\": 123\n         },\n         \"enabled\" : { \n            \"BOOL\": false\n         }\n      },\n      \"SequenceNumber\": \"seq-num\",\n      \"SizeBytes\": 1245,\n      \"StreamViewType\": \"view-type\"\n   },\n   \"eventID\": \"event-id\",\n   \"eventName\": \"event-name\",\n   \"eventSource\": \"event-source\",\n   \"eventVersion\": \"event-version\"\n}\nOption 1: Minimal conversion of attribute values\nJSON parses the document and convert only attribute values to their ruby equivalents. Other values, like the ApproximateCreationDateTime would not be converted and would remain numeric instead of being converted to a Time object.\nruby\n{ \n  \"awsRegion\" => \"us-east-1\",\n  \"dynamodb\" => { \n     \"ApproximateCreationDateTime\" => 123456789,\n     \"Keys\" => { \n       \"id\" => \"abc\"\n     },\n     \"NewImage\" => { \n       \"id\" => \"abc\",\n       \"name\" => \"new-name\"\n       \"size\" => 456,\n       \"enabled\" => true\n     },\n     \"OldImage\" => { \n       \"id\" => \"abc\",\n       \"name\" => \"old-name\"\n       \"size\" => 123,\n       \"enabled\" => false\n     },\n     \"SequenceNumber\" => \"seq-num\",\n     \"SizeBytes\" => 1245,\n     \"StreamViewType\" => \"view-type\"\n  },\n  \"eventID\" => \"event-id\",\n  \"eventName\" => \"event-name\",\n  \"eventSource\" => \"event-source\",\n  \"eventVersion\" => \"event-version\"\n}\nOption 2: Convert to SDK struct types + attribute values\nConvert the JSON into the types defined by the SDK. Attribute values would be represented with vanilla ruby hashes / values, not their struct type. Note that non-attribute value types would be converted (like the approximate_creation_date_time below). Also, structs use snake_case instead of the default casing.\n``` ruby\n<struct Aws::DynamoDBStreams::Types::Record\nevent_id=\"event-id\",\n event_name=\"event-name\",\n event_version=\"event-version\",\n event_source=\"event-source\",\n aws_region=\"us-east-1\",\n dynamodb=\n  #\"abc\"},\n   new_image={\"id\"=>\"abc\",\"name\"=>\"new-name\", \"size\"=>456, \"enabled\"=>true},\n   old_image={\"id\"=>\"abc\",\"name\"=>\"old-name\", \"size\"=>123, \"enabled\"=>false},\n   sequence_number=\"seq-num\",\n   size_bytes=1245,\n   stream_view_type=\"view-type\">>\n```\nThoughts?\n. Thanks for the fix!\n. I've confirmed that the order does not matter, but I've pushed a commit that makes sure they line up.\n. Looking at the API reference documentation, it looks like you can list all subscriptions, or list them by topic arn:\n- #list_subscriptions\n- #list_subscriptions_by_topic\n. If the list of possible subscriptions is either large or unbounded, then listing all subscriptions to simply filter client-side would be a poor solution. I'm not extremely familiar with the SNS API, but I do know the SDK represents all of the possible inputs and outputs for the service. You may need to keep an index of subscriptions in your application or in another data source (such as DynamoDB).\n. I'm going to close this question, as it does not appear to be an issue with the SDK. Please feel free to re-open if needed.\n. You didn't provide the error, but I suspect it is because your policy does not permit the proper actions. I believe it should s3:getObject instead of s3:get. Also, I believe it is s3:ListBucket. I'm grabbing these values from here:\nhttp://docs.aws.amazon.com/AmazonS3/latest/dev/using-with-s3-actions.html\n. Thanks for the feature request. You are correct, this is a service limitation. We update the SDK with each new service feature, so you can be certain if this functionality is added it will be made available through the SDK.\n. The AWS SDK for Ruby documentation for API operations are pulled directly from the service API docs. http://docs.aws.amazon.com/AWSEC2/latest/APIReference/API_AuthorizeSecurityGroupIngress.html \nIt appears that specifying a source security group by name is only supported for EC2 classic (which is non-vpc) and for the default VPC in an account.\nI think you will need to use the :ip_permissions option:\nruby\nresp = client.authorize_security_group_ingress({\n  group_id: \"...\",\n  # ...\n  ip_permissions: [\n    {\n      user_id_group_pairs: [\n        {\n          group_id: \"....\",\n          # ...\n        }\n      ],\n    },\n  ],\n})\nHope this helps.\n. Throttling errors are already being retried inside the waiter. By default, every client request is retried up to three times. You can increase this limit if you are being throttled. These retries happen with exponential backoff (e.g. 0.3, 0.6, 1.2, seconds etc). \nruby\nec2 = Aws::EC2::Client.new(retry_limit: 6)\nUnexpected errors returned by the service (after retries are exhausted for a single request) will stop the waiter from polling and will be wrapped in an Aws::Waiters::Errors::UnexpectedErrror. You can access the original error by calling #error on this object.\nhttp://docs.aws.amazon.com/sdkforruby/api/Aws/Waiters/Errors/UnexpectedError.html\nHope this help.\n. I can certainly use your script and reproduce the issue. Amazon EC2 is returning a successful 200 response with a volume id. Here is the output of a HTTP wire trace:\n-> \"HTTP/1.1 200 OK\\r\\n\"\n-> \"Content-Type: text/xml;charset=UTF-8\\r\\n\"\n-> \"Transfer-Encoding: chunked\\r\\n\"\n-> \"Vary: Accept-Encoding\\r\\n\"\n-> \"Date: Fri, 17 Jun 2016 23:08:51 GMT\\r\\n\"\n-> \"Server: AmazonEC2\\r\\n\"\n-> \"\\r\\n\"\n-> \"223\\r\\n\"\nreading 547 bytes...\n-> \"<?xml version=\\\"1.0\\\" encoding=\\\"UTF-8\\\"?>\\n<CreateVolumeResponse xmlns=\\\"http://ec2.amazonaws.com/doc/2015-10-01/\\\">\\n    <requestId>f066a84f-c33d-46bf-b63e-d8bccb6522e8</requestId>\\n    <volumeId>vol-93212943</volumeId>\\n    <size>10000</size>\\n    <snapshotId/>\\n    <availabilityZone>us-east-1a</availabilityZone>\\n    <status>creating</status>\\n    <createTime>2016-06-17T23:08:51.909Z</createTime>\\n    <volumeType>gp2</volumeType>\\n    <iops>10000</iops>\\n    <encrypted>true</encrypted>\\n    <kmsKeyId>my-fake-kms-key-asdf</kmsKeyId>\\n</CreateVolumeResponse>\"\nSubsequent API calls to describe the volume reveals the issue, and this does not provide information or context as to why the create failed, only that the volume does not exist. I'm not sure what the SDK can do. I can forward this issue to the service team, but I'm not sure there is anything actionable by the AWS SDK for Ruby.\n. Thanks!\n. I did some digging into this. The modeled enumeration is correct. If you HEAD and object it will not return a storage class of GLACIER. You may not specify this either when making a PUT Object call. This is only valid when configuring a transition lifecycle policy. Once the object has transitioned to glacier, you will not be able to HEAD the object, unless you restore from glacier first. During restoration it will receive one of the storage classes above.\nHope this helps.\n. I've done a quick review of this. I think we should discuss the method in which white-listed headers are provided. I'm of the opinion that this should not be done as client configuration options. Given the client will never consume these options, they increase the surface area of a client just to act as a convenient bag for grouping options. I suspect this was suggested because a Client instance is passed to the pre-signer.  Retrospectively, the Aws::S3::Presigner should not have ever accepted a client instance. Instead it should have accepted a credential provider and a region. We can not change that interface now, but we can deprecate it. Lets do a more in-depth review in person and we can update the pull-request. \n. Still looking at this, but I'm not sure its a bug. Locally I can verify that VERSION is being loaded when calling .eager_autoload! with or without a service filter. @abhi-patel Are you confident that .eager_autoload! is being called prior to entering concurrency? Also, are those two common errors the same after calling eager autoload or do the constants that cause errors change?\nIts worth noting that we are investigating removing the use of autoload completely from the SDK. It may happen in stages. The first thing we would need to do is modularize the SDK into service specific gems.\n. We are still investigating this issue for now. As a work-around, I was able to send that failing request by URI encoding filename in the example above. Some browsers, like Chrome, automatically decode the filename when downloading the object. Other browsers, like Safari, appear not to. I haven't tested other browsers.\n. There has been some ongoing discussion. In short this is a limitation of signature version 4 and header values. There isn't anything that can be done in the SDK. The best work-arounds are url encoding the values, or transliterating/pruning non-ascii characters. I'll be sure to update this issue if we hear anything else.\n. Calling the original method would definitely cause a stack overflow error, as it would be recursive. Looking at your test, it appears that you are trying to assert something about the value being sent to Aws::S3::Client#put_object, is that correct? \nIt sounds like you need to record requests sent to the client so you can assert about the values they receive. We do this in some of our integration tests. A simple way to do this is by registering a handler on the client.\n``` ruby\nrequests = []\nclient = Aws::S3::Client.new(stub_responses:true)\nclient.handle do |request| # register a handler in the request handler stack\n  requests << request\n  @handler.call(request)\nend\nnow make some requests\nclient.put_object(bucket:'aws-sdk', key:'key', body:'Hello World!')\nnow inspect the recorded requests\nputs requests[0].operation_name #=> put_object\nputs requests[0].params[:body] #=> Hello World!\n```\nIs this what you are looking for?\n. @erikogan I'm not sure I understand how the SDK would be able to reliably upload the contents of an unlinked file. \n. Closing this as a known issue/limitation of autoloading. We are investigating removing use of Ruby's autoload from the SDK in the future.\n. I've pushed jmespath 1.3.0 which should address this issue.\n. I haven't tracked down the root cause yet. The json_pure gem is no longer in our dependency tree, but I understand that it may still be causing problems with jmespath when loaded. I'm going to continue investigating this issue to see if I can narrow down the root cause here.\n. Version 1.3.1 of the jmespath gem was released yesterday. This includes a fix for environments where pure_json is already loaded into the environment. Can you update jmespath and see if this resolves the issue above?\n. I'm closing this issue here. The root cause has been addressed in our dependency. Updating jmespath to 1.3.1 should resolve this issue for anyone affected.\n. This has been resolved up-stream now. The docs will be corrected on our next release.\n. I'm flagging this as a bug. In previous versions this worked as expected, and I'm not sure what change causes this regression. \n. The fix for the unhelpful error will go out with our next release.\n. To add some context, the API reference documentation for each service is shared from the service API documentation. In this scenario, the service API reference documentation mentions how things are serialized on over the wire, which includes the querystring formatting of key/value pairs which the SDK does for you. This causes the confusion.  This sometimes happens as well when service documentation references names sent over the wire which are actually snake_cased in the SDK.\nI'm not sure what we can or should do here. I agree, the documentation is confusing at best and basically wrong for Ruby SDK users. Hand maintaining corrections in our library would be difficult at best, as we would need a way to keep this up to date. We could explore writing a documentation plugin that attempts to detect these situations and correct, but I don't think we would be able to catch 100% of miss-matches. A targeted patch/plugin for the EC2 tags documentation is possible, but it is difficult to guess how the source documentation may be changed over-time, potentially breaking said plugin.\nI'm open to suggestions.\n. I've also submitted an SDK specific example that will appear in the API reference documentation for #describe_instances. This should be published with our next release.\n. As far as I can tell from the stack traces above, this is unrelated to the jmespath gem issue. Can you update to the latest version and test again. This should not normally make any difference, but it appears almost as if you have a truncated version of the service resource definitions JSON document. A gem update would update this. If this does not resolve the issue, could you share a gist of your Gemfile.lock so we can see if there is maybe another library that is causing grief?\n. I agree that the issue is unrelated to JMESPath. @fedot Can you provide more context to when the error occurs?\n. I am unable to reproduce this error using v2.3.22 of the SDK or the most recent v2.4.0. Have you been able to resolve this issue?\n. Thanks for the update.\n. I've submitted a pull-request that wraps the socket error you received above with a helpful error that indicates the likely cause and what possible regions are.\n. To add to @awood45's comment, you can also use version 1 and version 2 of the SDK side by side (in the same application). This allows you to use newer v2 functionality without re-writing existing working code.\nSee this blog post for an example:\nhttp://ruby.awsblog.com/post/Tx2NJE86FP0HHXX/Announcing-V2-of-the-AWS-SDK-for-Ruby\n. I've added the missing paginator configuration. This should go out with our next release.\n. We do not have unit tests for this code, as it is not a public interface. It is tested functionally and in our integration tests.\nWhat is the expected behavior here if the File has been unlinked and then removed from the file system. It appears its going to move down one step to the next condition (respond to #read) and then fail there. Is that indented?\n. I've merged this and added a CHANGELOG entry and a test to cover the behavior.\n. If the response is empty, then the wait should not return. This indicates that the autoscaling group is being omitted from the response for some reason. Invalid id? Not created yet, eventual consistency? The waiter definition explicitly requires the number of autoscaling groups in the response to match those in the request:\ncontains(AutoScalingGroups[].[length(Instances[?LifecycleState=='InService']) >= MinSize][], `false`)\n. I've found the issue. This is related to how the jmespath expression from the shared waiter is translated into snake_case to work against the Ruby data structures. I'll be looking at this today.\n. I've submitted a pull-request to address this issue. We will likely have this merged in time for tomorrow's release. Thanks for reporting the issue!\n. Thanks for the correction!\n. Thanks for reporting this issue. We are having discussion about this now. I'll get back here shortly.\n. Ship it!\n. Thanks for contributing the missing paginators. I've submitted this up-stream as well.\n. I went HEAD the bucket. I was curious to know what region the bucket is in. The bucket seems to no longer exist. My guess is the bucket is not in us-east-1, but it is being signed as such. Normally the SDK would detect this region-missmatch, but S3 does not return sufficient information. Normally the error would be returned in the response body, but HEAD responses do not have a body.\nAlso, a HEAD bucket request would normally return an \"x-amz-bucket-region\" response header. Did you remove this from the error response when copy-pasting it?\n. While the data is not available from the returned route, you can call instance methods on that object, such as #replace_route and #delete. \n. Are you using the SDK in a multithreaded processes perhaps? The SDK makes heavy use of Ruby's autoload function which is not thread safe. When it fails, it is often with a name error like the one above. \nCan you try adding one of the following lines to your application after you've loaded the SDK?\n``` ruby\nthis loads every service\nAws.eager_autoload!\ncherry pick the service modules to load\nAws.eager_autoload!(services: ['S3', 'EC2'])\n``\n. I'm not sure what would be the source of the different error messages. My best guess is there is some difference between how the Ruby SDK and the CLI were configured. Both tools are returning an error as returned from the service.\n. You can initialize your client withhttp_wire_trace: trueand that will send a wire trace of the HTTP requests to standard out.\n. Thanks for the fixes!\n. Is this usingaws-sdk-rails`?\n. Can you provide a code example of how you are calling the SDK with a bcc where it is getting dropped?\n. I suspect you have an issue with how you are reading the PDF from disk, i.e. encoding. Can you share the Ruby code snippet of how you are using the SDK to upload the file?\n. Thanks for reporting this. I was releasing the SDK about an hour ago, and ran into multiple problems uploading the gem to rubygems.org. I was getting 504 errors from the API.  It eventually succeeded, but it appears that their index is broken.\nI'm going to try to yank the gem and re-upload.\n. The gem yank \"succeeded\" but then subsequent pushes fail. That said, gem installs should work now:\n```\n$ gem list --remote aws-sdk\n REMOTE GEMS \naws-sdk (2.5.8 ruby)\n```\nI'm going to try in a few minutes again to push v2.5.9.\n. @ibussieres Does your bucket name contain dots? The Ruby SDK will put the bucket name in the url path when it is not DNS compatible, or  if it contains dots. As @awood45 has indicated, this should work fine.\n. Thanks for the contribution!\n. I'm going to close this issue, as it is currently non-actionable by the SDK. That said, we will be pushing out a preview release shortly of the SDK where the services are modularized into individual gems. With this refactor, we have removed the use of Ruby's autoload which should also side-step this issue entirely.\n. I'm guessing you were able to resolve this issue. I'm responding in case others come across this issue.\nAmazon S3 does not allow you to create a bucket with a slash in their names. Often users will consider the bucket name to contain some portion of a key prefix. As they can often be run-together as part of a URI, this seems to make sense.  Unfortunately this will cause signature version 4 errors, it breaks DNS compatibility, and causes other issues. Bucket names do not contain slashes, it is simply not possible. \n. Just chiming in here. The preview release of the modularized SDK is coming shortly. With modularization, we have eliminated the use of autoload.\n. @rayway30419 Are you still experiencing the intermittent missing credential errors? Commonly this is caused by the aggressive defaults in attempting to load credentials from the instance metadata service. Once credentials expire, they must be refreshed, but if the metadata service fails to respond, a missing credentials error is raised. I've just push a small change that will go out with the next release where you can configure expanded retries and timeouts:\nruby\nclient = Aws::DynamoDB::Client.new(\n  region: Aws.config[:region],\n  instance_profile_credentials_timeout: 5, # defaults to 1 second\n  instance_profile_credentials_retries: 5, # defaults to 0 retries\n)\nThis change will be available in the next release.\n. There are a bunch of red herrings flying around here. This is not a concurrency issue at all. The stack trace is coming from code in the SDK when we are trying to build an XML document to send as a request. The error is not returned from the remote service.\n\nIt happens when using the S3 endpoint for deleting multiple objects at once (through the corresponding aws-sdk method).\n\nThere is a known issue with Amazon S3 and object keys as represented in XML. Object keys can contain characters that are invalid inside an XML document. Some of these, often non-printable characters, will even cause S3 to fail on responses. As a work-around, S3 allows you to send a boolean on all list object calls, requesting the keys be URL encoded before serializing them in the XML response. The SDK does this automatically, and then URL decodes them before returning them to the user. This happens transparently and avoids many errors.\nThe issue is that S3's Delete Objects api call accepts keys back in XML. If you have some of these object keys with invalid characters, the xml encode method raises. This is the error you are seing. These keys simply can not be represented in XML. As such, they can not be sent via DELETE objects. They must be sent as an individual DELETE object call where the key is sent in the URI where it is URL encoded.\nIf Amazon S3 adds the options to provide the keys to DELETE objects in a URL encoded form, then the SDK could smooth over this issue automatically. Until such a parameters/option exists, the only work-around is to DELETE the object(s) with the special characters individually.\n. I'm going to close this issue, as there is nothing the SDK can do at this time. Please feel free to re-open if you have additional questions.\n. Sorry, I made the change this afternoon, got distracted and failed to update this issue.\n. Thank you both for reporting this issue. I was able to reproduce the issue locally. There were two issues. One, it was not using the local tag data on the auto scaling group description causing multiple load calls. Second, it was not describing the tag with the appropriate filters, allowing it to return the wrong data.\nI've patched both, they will go out with our next release.\n. > I would expect that if r.respond_to?(X) == true then I can call r.method(X) to get the associated Method object.\nI disagree. The purpose of #method_missing is to provide behavior exactly when a method does not exist, hence, the method is missing. The purpose of #respond_to? is to indicate if an object will respond to a method/message, not if there is a concrete method.\n. I did some digging in the awesome_print code. Before simply calling inspect, it checks to see if an object responds to #to_hash. It does a bunch of sanity checks on the object, even as far as checking the interface of the object returned from #to_hash responds to certain hash methods. Failure from any of these checks causes it to return nil and fallback on inspecting the object. One of these is getting the #to_hash method and ensuring the arity of the method is 0 (it can be called without arguments).\nOne options would be to replace #respond_to? with calling #public_methods and checking to see if the returned list includes :to_hash. I can go add this to the issue on awesome_print.\nSeparately, I modified the response class to use the Ruby stdlib Delegator to provide access to the response data. This resolves the issue immediately with awesome_print, and it provides better formatting. It results in awesome printed structure data, instead of down-casting it to a hash. \n. I've submitted a pull-request to awesome_print that fixes the root issue there. It includes a test case demonstrating the issue along with the fix.\nhttps://github.com/awesome-print/awesome_print/pull/285\nEither way, we are now working around the issue by using Delegator in this SDK.\n. Great work! I've merged this into the code-generation branch.\n. It has simply not been implemented yet. I'm going to mark this as a feature request so we can add it to our backlog. \n. RDS does not accept a group of VPC security group ids to this operation. The API documentation is here: \nhttp://docs.aws.amazon.com/AmazonRDS/latest/APIReference/API_RestoreDBInstanceFromDBSnapshot.html\n. Good news, this was added a while back to v2. You can read the docs on the pre-signed post utility here:\nhttp://docs.aws.amazon.com/sdkforruby/api/Aws/S3/PresignedPost.html\n. This appears to be an issue with the server you are connecting to, not with the client. I'm not sure what is running on localhost:3128, but S3 handles this sequence of operations correctly. I'm not sure there is anything that should be done in the SDK, it sounds like something that needs to be fixed in the S3 mock implementation.\nI'm going to close this issue, but please feel free to re-open if you find this is actually an issue or limitation in the SDK.\n. I don't see an obvious care where we are adding keys to the hash during iteration.  The @pool hash is protected by a mutex and is only enumerated or indexed behind that mutex. The stack trace is terminating inside the hash's block that yields for missing keys. \nI can rewrite the code to avoid using the default block for assigning the list. This should be a simple change that I can ship with the next release so you can test the change.\n. This change should go out with our next release. Once this is live, can you update and let me know if this resolves the issue you've been experiencing?\n. @janko-m Constructing any resource with a nil identifier now raises an ArgumentError. This was originally the intention, but the code was checking if the options has contained the identifiers key, but not if it had a non-empty value. Thanks for reporting the issue!\n. Sorry for the slow response. You are correct. We should be able to modify the resource definition to make use of the v2 list operation. This should be backwards compatible from the resource interface. I'll add this as a feature request.\n. I'd be in favor of adding jitter to the retry backoff. Feel free to send a PR.\n. The default chain only loads credentials from ECS when ENV[\"AWS_CONTAINER_CREDENTIALS_RELATIVE_URI\"] is set:\nhttps://github.com/aws/aws-sdk-ruby/blob/master/aws-sdk-core/lib/aws-sdk-core/credential_provider_chain.rb#L86\nMy understanding is that this environment variable is automatically exported by ECS. Is this not happening?\n. The SDK uses Net::HTTPResponse#read_body which does not allow us to control the chunk sizes. I'm not familiar with a way to tune the chunk sizes that Net::HTTP returns. \n. I added the missing tests to the pull requests and merged it. Thanks for raising the issue!\n. Thanks for the contribution. I've merge the change and added tests that cover this behavior.\n. Can you share a copy of your Gemfile? Also, have you attempted to see where the time is spent? Is it time waiting for the response bytes (slow HTTP?) or perhaps in parsing/loading the JSON?\n. We ran into significant problems with a hard dependency on nokogiri and opted to make this optional. There are other optional XML parsers. If the ox gem is present, that will also be used and runs even faster than nokogiri. \nThere is a blog post, but it warrants adding this to the documentation in a few more places to aid in discovery.\n. I'm going to close this as there doesn't appear to be an issue. I've reached out to someone internally about getting the developer guide updated to add content about performance tuning. Thanks for the heads up.\n. The limit in the v2 SDK is not flexible. The version 2 SDK uses AWS signature version 4. Sigv4 has the hard requirement that signatures are never valid for longer than 1 week. The v1 SDK uses an older signature version that does not have this limitation. While it still works in many circumstances, there are more and more operations, regions, and parameters are invalid/unusable unless you use signature version 4. Choosing to use an older version may come back to bite you in the future.\n. Closing as a question, but please feel free to re-open if you have additional questions.\n. This is an issue with documentation. The error raised by the SDK is Aws::SNS::Errors:: EndpointDisabled. This is a known issue/limitation that is in our backlog to correct the documentation.. There are additional reasons that #upload_file will not support a StringIO object. The #upload_file avoids buffering multiple file parts into memory by open multiple file objects. I suppose that the method could be abstracted away from file by adding a wrapper interface that returns \"file parts\". \n. The error message is written by the service, and is simply returned and raised by the SDK. That said, the error object should respond to the method #context. The context object contains the request parameters.\nruby\nbegin\n  client.get_queue_url(queue_name: 'q-name')\nrescue Aws::SQS::Errors::NonExistentQueue => error\n  puts \"Queue #{error.context.params[:queue_name]} not found\"\nend. @papadeltasierra A couple of thoughts or suggestions.\n\n\nIt sounds like you are receiving an upload stream from the user which you are buffering into 6MB parts, so that you can upload using the multipart upload interface. This avoids the need to know the total size of the object and/or buffering the entire object into memory. Have you considered having the user upload directly to S3 and not pass the bytes through your rails stack? There are a handful of mechanisms to do this. If they are using a browser, you can create a pre-signed POST form that will upload directly to S3. If not using a browser, you can use a presigned URL and have then put the object to s3. Both of these mechanisms allow you to apply constraints on the object.\n\n\nGiven an Aws::S3::Client object you can use this to initialize any S3 resource class. For example:\n\n\n```ruby\nclient = Aws::S3::Client.new(...)\nusing the class constructors directly\nb = Aws::S3::Bucket.new('bucket-name', client: client)\no = Aws::S3::Object.new('bucket-name', 'object-key', client: client)\nusing the Aws::S3::Resource class\ns3 = Aws::S3::Resource.new(client: client)\nb = s3.bucket('bucket-name')\no = s3.bucket('bucket-name').object('object-key')\n```\n. The SDK is reading a handful of JSON service definition files at SDK load when defining a service module for the first time. These JSON documents are used to dynamically define the modules, classes, methods, etc for the service. This will be slow because it is generating classes at runtime as well as source potentially large JSON documents from disk to build these classes. It will also do a large amount of string inflections from the source JSON document to provide the snake_cased method and param names that are idiomatic in Ruby.\nThe good news is that we are currently in a preview stage of a significant update to the SDK. The SDK is being modularized so that each service is bundled into its own gem. In addition to modularizing the SDK, the runtime generation is replaced with generated source that is compatible with the existing SDK. I ran the benchmarks above locally and got the following results:\n```ruby\n\nrequire 'benchmark'; 5.times { require 'aws-sdk-sns'; puts Benchmark.measure { Aws::SNS::Client.new } }\n  0.000000   0.000000   0.000000 (  0.003313)\n  0.000000   0.000000   0.000000 (  0.000687)\n  0.000000   0.000000   0.000000 (  0.001271)\n  0.000000   0.000000   0.000000 (  0.001422)\n  0.000000   0.000000   0.000000 (  0.000663)\n```\n\nThere is obviously still some extra time spend during the initial require statement for the gem, but it is significantly faster. Aside from some very minor changes in your Gemfile and possibly a change in how you require the SDK, these should be backwards compatible. There are a pair of blog posts that highlight these new versions here:\n\nModularization\nUpgrading from v2 to v3\n\nHope this helps!. I failed to answer one of your questions. It is safe to use the SDK clients as singletons. They are immutable and safe to share across threads. . I just realized I made a significant mistake in my benchmark above and now I feel really silly. \n```ruby\n\nrequire 'benchmark'; 5.times { require 'aws-sdk-sns'; puts Benchmark.measure { Aws::SNS::Client.new } }\n```\n\nNotice that I required the aws-sdk-sns gem outside of the benchmark block. Moving inside I get similar results to what you are getting, even with the code-generated branch:\nruby\nrequire 'benchmark'; 5.times { puts Benchmark.measure { require 'aws-sdk-sns'; Aws::SNS::Client.new } }\n  0.180000   0.040000   0.220000 (  0.219647)\n  0.000000   0.000000   0.000000 (  0.000643)\n  0.000000   0.000000   0.000000 (  0.000673)\n  0.000000   0.000000   0.000000 (  0.000664)\n  0.000000   0.000000   0.000000 (  0.000687)\nI was surprised by this until I realized why. Version 2 of the SDK makes heavy use of autoload. Version 3 does not. I uses explicit require and require_relative statements. This means there is some cost paid up-front when the library is required. First client instantiation is significantly faster as there are no more files to autoload. This means there is still a pretty heavy startup time.\nIf you require the SDK at process startup, this should move the cost there. If you are using short-lived processes then this may not help. . Also worth noting is most of the time is not spent loading sns, rather is it is in loading its downstream dependencies:\nruby\nrequire 'benchmark'; 5.times { puts Benchmark.measure { require 'aws-sdk-core'; require 'aws-partitions'; require 'aws-sigv4'; require 'aws-sigv4' }}\n  0.150000   0.030000   0.180000 (  0.181079)\n  0.000000   0.000000   0.000000 (  0.000015)\n  0.000000   0.000000   0.000000 (  0.000012)\n  0.000000   0.000000   0.000000 (  0.000019)\n  0.000000   0.000000   0.000000 (  0.000011). Looks great. Make sure to port this over to the code-generation branch.. I would recommend that the :event_stream_handler option accept an instance of the EventStream class or a block that is used to configure the class.. Be sure to remove the pair of debug statements here.\n. Stylistically, I prefer the method invocation with parenthesis, i.e. option(:signature_version, 'v4')\n. This is a good opportunity to improve this existing error message. Add into the message the valid signature versions.\n. We should reword this documentation to indicate the :content_length is only needed when :multipart_copy is true.  The content_length is used for calculating the number of parts and their size.\n. This could also be expressed as:\nruby\noptions[:copy_source_client] ||= source.client if source.respond_to?(:client)\nThe ||= is less important, the respond_to? check makes this a little more future proof. Currently we have not added support for Aws::S3::ObjectVersion, but this is likely to happen. No need to change this unless you feel inclined. If you don't, we will make the change when adding object version support.\n. The #get_partition method could own the logic of returning 'aws' as the default partion if the region does not match the regex of any known partition. This removes the need for the nil check above.\n. Yes, that is only for the documented request syntax examples. Another option would be to do something like \"true || false\". The important part of this is to ensure the example shows that a boolean is provided. It should also be valid Ruby syntax.\n. Completely understood, not sure of a valid alternative. #create_bucket and #list_buckets are simply not supported. The reason is the s3-accelerate endpoint must have an accelerated bucket as the endpoint prefix. Create is creating a new bucket that does not previously exist and list buckets is outside the context of a single bucket. #delete_bucket was added on request from the S3 team. There is no deterministic characteristic of these that makes it possible to not hardcode.\n. You will want to remove byebug from the Gemfile and gitignore.\n. I would suggest replacing the constant with a method that memoizes. We had to make a similar change because of how autoload works. It is possible for a second threat to get a no such constant error under concurrency. The code below mitigates this. It is possible that multiple threads hit the method, but a single object will be memoized for later use.\nruby\ndef shared_config\n  @shared_config ||= SharedConfig.new\nend\n. Given there are three class level methods, I would move them inside the eigenclass:\nruby\nclass << self\n  def ini_parse(raw)\n  end\n  # ...\nend\n. Good catch on the empty ENV region string. I don't believe we check for this previously.\n. I would suggest moving the ENV['AWS_SDK_LOAD_CONFIG'] completely outside the shared config class. Loading from the shared config should be the default behavior of this class. It could instead accept a boolean at construction time to disable this feature. The code above that constructs the default shared config object could do the environment check. This ensures defaults are preserved and eliminates the need for a process wide environment variable in applications that construct this directly.\n. Rather than using assignments in the conditionals, I would suggest:\nruby\ncredentials = credentials_from_shared(p, opts)\ncredentials ||= credentials_from_config(p, opts)\ncredentials\n. Also, I haven't tested this, but I suspect this will be problematic if the credential keys exist in the document, but they have no values. The credentials object will exist, but it will be empty, and we should continue to the next check.\n. I understand this may still be in flux, but it does not appear that this is hooked up anywhere. It should probably return an instance of AssumeRoleCredentials automatically when the assume role profile is set.\n. The yard comments should be formatted like:\n@param [String] key_pair_id Then documentation goes at the end.\n. I would suggest:\n``` ruby\n@param [String] url\n@option options [Time, DateTime, Date, String, Integer] :expires\n@option options [String] :policy A JSON policy document.\ndef signed_url(url, options = {})\n  expires = options[:expires]\n  policy = options[:policy]\nend\n``\n. We can probably useURI.parse(string)here on the url, instead of parsing it ourselves.\n. You should add aprivatestatement here to keep these methods from being documented as part of the public interface.\n. In Ruby, onlynilandfalseevaluate tofalse` in a boolean context. You can remove the not nil checks here.\nruby\nelsif resource && expires\n. Inline your major assertion.\nruby\nsigner = UrlSigner.new(key_pair_id, private_key_path)\nexpect(signer).to ...\nTry to not hide the main code under test in a let expression.\n. We should add options here:\n``` ruby\n@param [String] private_key_id\n@option options [String] :private_key\n@option options [String] :private_key_path\ndef intilaize(key_pair_id, options = {})\n  private_key = options[:private_key]\n  # if both :private_key and :private_key_path are unset we should raise an argument error\n  private_key ||= File.open(options[:private_key_path], 'rb') { |f| f.read }\nend\n```\n. I would suggest DRY'ing up the code by having a single re-usable string for the got value (class) instead messaging. You can create a const string at the top of the class like:\nruby\nGOT_INSTEAD = \"got value %s (class: %s) instead.\"\nThen you can simply append the following to each error message\nruby\nGOT_INSTEAD % [values.inspect, values.class.name]\n. Also note, we should use the #inspect of the value for each error message instead of the default #to_s that results from passing the raw value through string interpolation. This makes for a better error message.\n. I'd like to see if we can improve the implementation to not buffer, seek, truncate and rewind the target IO object. I think this can be improved by making a 2nd GET request to the object with a range header that will retrieve only the trailing authentication tag. This can be used to initialize the cipher that we give to the IO decrypter which should eliminate the need for the extra reads.\nI've pulled this down locally, and I'm going to try this out.\n. I'm not worried about the interface change here. It has never been documented, and poses no general purpose utility. That said, I'm happy to simply revert this part of the change.\n. Yup. I wrote the code original in the one file for convience and failed to remove it when I added it to its own file. I'll clean that up.\n. Minor nitpick. I try to limit the top-level utility functions. A simple change would be to keep this function inside the Aws::SharedConfig module. You could name the method something like .default_config, e.g Aws::SharedConfig.default_config.\n. This method call will fail with an incorrect number of arguments error.  I would suggest something like:\nruby\ndef assume_role_credentials(options)\n  if Aws.shared_config.config_enabled?\n    profile = nil\n    region = nil\n    if options[:config]\n      profile = options[:config].profile\n      region = options[:config].region\n    end\n    assume_role_with_profile(profile, region)\n  else\n    nil\n  end\nend\n. We do this in multiple places, so this pattern is not new (inject or construct a client), but we do not actually require a client. We actually need a region, and a set of credentials. The client object in inconsequential. It seems like a better solution would be to have a public interface for getting the default region, and a public interface for getting default credentials. \nThe client configuration can be updated to use these interfaces, so the behavior remains consistent, and then the code can be reduced down to injecting an optional region and or credentials.. We can determine this correctly for every API operation by simply using the API object.\nruby\nAws::S3::Client.api.operation(method).http_method\nThis has the benefit of it will work for more than object operations.. This implementation of build_headers is limited to only support those params that have x-amz- prefixes. As such you will run into issues with things like x-amz-metadata- headers that are given as hashes normally, but this method will require them to be given as strings with metadata-{key} prefixes. \nIt would be ideal to preserve the same interface we use when making client calls.. Try to remove this from config and instead use context[:event_stream_handler]. ",
    "msavy": "I didn't realise there were separately documented extensions to Time offering the RFC methods (hence constructing the format manually!).\nThe other solution is better I think.\nCheers.\n. ",
    "geoffyoungs": "Yes it does - thanks - I'd missed that.  It's a much cleaner solution :)\n. ",
    "prashantrajan": "Version 1.0.3 seems to have fixed this issue for me.\n. ",
    "krishan": "With aws-sdk it is not yet possible to alter the content_type of an existing s3 object (without downloading and re-uploading it).\nThis patch enables the developer to do just that by supplying a new content_type when copying an s3 object, i.e. \nobj1.copy_from(\"bucket/key\", :content_type => \"image/png\")\nFor more info, see my question in the aws-sdk developer forums: https://forums.aws.amazon.com/thread.jspa?threadID=76578&tstart=0\nI'd be real happy if you could merge that into a new release.\n. ",
    "awendt": "+1\n. @trevorrowe Thanks for the quick response. And thanks for clarifying that you're not a fan of this, Github sent me an e-mail with a comment that didn't make sense, I guess that was before an edit :)\nAnyway, thanks for pointing me to AWS.memoize. I wasn't aware that closeTimestamp could be updated.\n. ",
    "kostia": "Updated to Tag 1.2.0\n. ",
    "jtarchie": "Any word on the support for this? It would be great to have this HTTP handler instead of using the TCPSocket EM replacement.\nOtherwise, can we pull this out into its own gem?\n. ",
    "JoshMcKin": "https://github.com/JoshMcKin/em_aws\nNeeds some clean up but using it in prod for a while now\n. Use https://github.com/JoshMcKin/em_aws if want this feature.\n. Opened a discussion on AWS to see it some else was suffering the same outages. It may be that SQS is sending an unexpected response.\nhttps://forums.aws.amazon.com/thread.jspa?threadID=111726&tstart=0\nOpened an issue with AWS and will update if anything pertinent is found.\n. Been running a script  with :log_formatter => AWS::Core::LogFormatter.debug but have yet to catch one. We've been doing some pondering and the C# guys are getting bad URI exceptions from their SDK. We believe that the initial queue url lookup might be return a nil or blank value for the queue's url or even a blank response entirely.\n. Below is a response we received from AWS when attempting to retrieve queue information using the SDK\n<?xml version=\"1.0\" encoding=\"utf-16\"?><ResponseMetadata xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns:xsd=\"http://www.w3.org/2001/XMLSchema\" xmlns=\"http://queue.amazonaws.com/doc/2009-02-01/\"><RequestId>edbece41-b384-5596-9d21-8701f347edc0</RequestId></ResponseMetadata>\n. Was finally able to catch one with the ruby logger. Looks like they're giving back some different XML when getting queue urls, usings a strange tag 'queueUrls' which should be 'ListQueuesResult'\nGOOD:\n| AWS us-east-1 SQS list_queues 0.222552 0 retries\n +-------------------------------------------------------------------------------\n |   REQUEST\n +-------------------------------------------------------------------------------\n |    METHOD: POST\n |       URL: https://sqs.us-east-1.amazonaws.com::443:/\n |   HEADERS: {\"content-type\"=>\"application/x-www-form-urlencoded; charset=utf-8\", \"content-length\"=>239, \"user-agent\"=>\"aws-sdk-ruby/1.7.1 ruby/1.9.3 x86_64-darwin12.0.0\"}\n |      BODY: AWSAccessKeyId=HIDDEN&Action=ListQueues&QueueNamePrefix=EMailQueue&Signature=HIDDEN&SignatureMethod=HmacSHA256&SignatureVersion=2&Timestamp=2012-12-10T20%3A34%3A46Z&Version=2011-10-01\n +-------------------------------------------------------------------------------\n |  RESPONSE\n +-------------------------------------------------------------------------------\n |    STATUS: 200\n |   HEADERS: {\"X_AMZN_REQUESTID\"=>\"d8777c33-558f-599c-822d-35731c28586d\",      \"CONTENT_TYPE\"=>\"text/xml\", \"CONTENT_LENGTH\"=>\"326\"}\n |      BODY: <?xml version=\"1.0\"?><ListQueuesResponse xmlns=\"http://queue.amazonaws.com/doc/2011-10-01/\"><ListQueuesResult><QueueUrl>https://sqs.us-east-1.amazonaws.com/CHANGED_JUST_INCASE</QueueUrl></ListQueuesResult><ResponseMetadata><RequestId>d8777c33-558f-599c-822d-35731c28586d</RequestId></ResponseMetadata></ListQueuesResponse>\n +-------------------------------------------------------------------------------\nBAD:\n+-------------------------------------------------------------------------------\n |   REQUEST\n +-------------------------------------------------------------------------------\n |    METHOD: POST\n |       URL: https://sqs.us-east-1.amazonaws.com::443:/\n |   HEADERS: {\"content-type\"=>\"application/x-www-form-urlencoded; charset=utf-8\", \"content-length\"=>243, \"user-agent\"=>\"aws-sdk-ruby/1.7.1 ruby/1.9.3 x86_64-darwin12.0.0\"}\n |      BODY: AWSAccessKeyId=HIDDEN&Action=ListQueues&QueueNamePrefix=EMailQueue&Signature=HIDDEN&SignatureMethod=HmacSHA256&SignatureVersion=2&Timestamp=2012-12-10T20%3A35%3A02Z&Version=2011-10-01\n +-------------------------------------------------------------------------------\n |  RESPONSE\n +-------------------------------------------------------------------------------\n |    STATUS: 200\n |   HEADERS: {\"X_AMZN_REQUESTID\"=>\"eaa8c1e2-ed52-52f2-9228-88a2a4e435bf\", \"CONTENT_TYPE\"=>\"text/xml\", \"CONTENT_LENGTH\"=>\"312\"}\n |      BODY: <?xml version=\"1.0\"?><ListQueuesResponse xmlns=\"http://queue.amazonaws.com/doc/2011-10-01/\"><queueUrls><QueueUrl>https://sqs.us-east-1.amazonaws.com/CHANGED_JUST_INCASE</QueueUrl></queueUrls><ResponseMetadata><RequestId>eaa8c1e2-ed52-52f2-9228-88a2a4e435bf</RequestId></ResponseMetadata></ListQueuesResponse>\nEDIT: added requests\n. Ill run the following script inside the Rails.console for a couple hours and if doesn't fail we should be good, if it does fail it will throw the exception and kill the loop and I'll get the log\nwhile true\n   puts AWS::SQS.new(:log_formatter => AWS::Core::LogFormatter.debug).queues.with_prefix(domain).first.approximate_number_of_messages\n   sleep 15\n end\n. Almost got the bad after running the test script for a couple an hour or so, no response at all.\n+-------------------------------------------------------------------------------\n |   REQUEST\n +-------------------------------------------------------------------------------\n |    METHOD: POST\n |       URL: https://sqs.us-east-1.amazonaws.com::443:/\n |   HEADERS: {\"content-type\"=>\"application/x-www-form-urlencoded; charset=utf-8\", \"content-length\"=>239, \"user-     agent\"=>\"aws-sdk-ruby/1.7.1 ruby/1.9.3 x86_64-darwin12.0.0\"}\n |      BODY: AWSAccessKeyId=HIDDEN&Action=ListQueues&QueueNamePrefix=HIDDEN&Signature=HIDDEN&SignatureMethod=HmacSHA256&SignatureVersion=2&Timestamp=2012-12-11T01%3A15%3A52Z&Version=2011-10-01\n +-------------------------------------------------------------------------------\n |  RESPONSE\n +-------------------------------------------------------------------------------\n |    STATUS: 0\n |   HEADERS: {}\n |      BODY:\n. I'm continuing to work the issue with AWS but we can be sure this not an SDK issue. I can post any more finds as AWS support continues working the ticket here if you like, but for now I will close the issue.\nThanks for you help.\n. Sorry to reopen this one, @trevorrowe, but is there any reason the SDK would print a status 0 with no body or headers?\n. Ok, running with :http_wire_trace => true\nThanks\n. Forgot to answer the rest. As of right now, I have not gotten an error in the application that relates to the empty response, due in large part to the fact most of with work does on in a delayed job and the job just retries if an exception is thrown. Was looking for a bit of advise as to the best approach in narrowing the search for the cause of the empty response and http_wire_trace sounds like the best place to start.\nThanks again.\n. Ran the test again for 4 hours at 15 second increments and no errors. I think we can safely call this a non-issue ;)\n. Get the same error when attempting to get content_type\n. bucket and dir are strings and I am able to retrieve the object, because prior to attempting to get the expiration_date I get a temporary url. Below is a better example of the code:\ns3 = AWS::S3.new \no = s3.buckets[bucket].objects.with_prefix(dir)\no.each do |f|\n  puts f.url_for(:read, :expires => 10*60, :secure => true) # => returns url as expected\n  puts f.expiration_date # => raises exception 'can't convert nil into String'\n  puts f.content_type # => raises exception 'can't convert nil into String'\nend\n. I should also state the all objects in dir use server side encryption, although it probably does not matter.\n. These are csv files and I'm setting the content_type. I'll create a test script that should reproduce the error, which I should have done in the first place ;-)\n. I understand completely. Expire date is set a bucket to expire all objects after 1 day, the expire for the url is just the expiration for that particular url. In the midst of some refactoring, should be done in an hour or so then i can create a test script. Thanks for your continued help.\n. This reproduces the error.\n```\nbucket_name = 'my-test-bucket-123321123'\ndir = 'temp/'\nfile_name = 'test.csv'\ncsv = \"1,2,3\\t\\n4,5,6\"\ns3 = AWS::S3.new\nbucket = s3.buckets.create(bucket_name)\nbucket.lifecycle_configuration.update do\n  add_rule dir, 1\nend\nbucket.objects.create(\"#{dir}#{file_name}\", StringIO.new(csv),:content_type => 'text/csv')\no = s3.buckets[bucket_name].objects.with_prefix(dir)\no.each do |f|\n  puts f.url_for(:read, :expires => 10*60, :secure => true) # => returns url as expected\n  puts f.expiration_date # => raises exception 'can't convert nil into String'\n  puts f.content_type # => raises exception 'can't convert nil into String'\nend\nbucket.delete!\n```\n. Well...looks like your suggestion to pull out some gems was the right one. We're using the em-aws driver which adds support for EM-Synchrony for making asynchronous requests and it looks like its not pulling everything out of the header or something.\n. @trevorrowe This issue is holding up a few of our threaded rails projects and the update of EmAws, another release would be greatly appreciated ;)\n. Thanks a bunch @trevorrowe\n. Hate to barge in with some possibly not very useful information. In the early development of EmAws I saw this error when I hadn't patched the Mutex with the Em.synchrony's fiber safe mutex. I 90% sure there use to be a mutex around fetching request credentials or a least a portion like the session_token but cannot be sure. I didn't create an issue in EmAws at the time because it was before it was ever released. \nThis was so long ago it might not be related but thought I'd mention it.\nEmAws Mutex patch: https://github.com/JoshMcKin/em_aws/blob/master/lib/em_aws/patches.rb\n. This may not matter but  rubinius 2.2.9 is set to ruby version 2.1.0 but the open ssl threadsafe patch is in MRI Ruby 2.1.1+\n. @YorickPeterse see: https://github.com/aws/aws-sdk-ruby/issues/525#issuecomment-41809120\n. You could try patching with http://www.ruby-doc.org/stdlib-2.1.1/libdoc/openssl/rdoc/OpenSSL/Digest.html if you are using 2.1.1.\nOpenSSL had a thread safety issue that was resolved in July for 2.1.1, but does not appear to have made it to 2.0 much less 1.9.\nhttps://bugs.ruby-lang.org/issues/8386\nhttps://bugs.ruby-lang.org/projects/ruby-trunk/repository/revisions/42135/diff/ChangeLog\n. Yeah if you correct and plain Ruby Digest is not threadsafe, I was wondering how the guys might go about fixing it. The best performing fix might be ugly but a simple mutex around hex digest would burdensome for MRI-2.1.1 folks or anyone with JRuby assuming its not an issue for them. \n. EM-aws supports streaming but has limitations: https://github.com/JoshMcKin/em_aws#streaming \n. ",
    "amscotti": "This is my first pull request on github, so if the code needs to be improved or if there is a more formal process let me know.\n. ",
    "rcrogers": "Thanks for these fixes.  I monkeypatched them into AWS in my app, because it doesn't look like they'll get included in the gem anytime soon.\n. My bad.  I'm using Ruby 1.9.3 p286.  Gem version is 1.7.1.  The backtrace is unfortunately useless:\nSystemStackError: stack level too deep\nfrom /usr/local/rvm/rubies/ruby-1.9.3-p286/lib/ruby/1.9.1/irb/workspace.rb:80\nHere's a gigantic trace generated by set_trace_func: https://gist.github.com/4322620\n. Upgrading to p327 solves this (wish I knew why!).  Thanks for testing it out and pointing me in the right direction.\n. ",
    "daniel-nelson": "Glad the patch is useful to you! We've been running it all this time, and it is working well for us.\n. ",
    "jagthedrummer": "Is there any reason that this can't be accepted into the library?  Trying to query based on a date seems to be completely broken in the main library at the moment....\n. Is there a reason that you're against adding the dependency?  It seems more maintainable/stable to add the dependency and to use code that already works as opposed to having to duplicate and maintain the code for time zone conversion as a new module.  Just curious about the philosophy at play.\n. Ah, yeah, I can see where you're coming from on avoiding libs that modify core classes.  That could be a nasty surprise to someone not prepared for it.\nThe idea of breaking AWS::Record out sounds interesting.\nThanks for the response.\n. I just packaged this set of fixes into a gem so that it can easily be included in a project.\nYou can just add this to you Gemfile to get these fixes:\ngem 'simple_date_fix'\nhttps://rubygems.org/gems/simple_date_fix\nhttps://github.com/Octo-Labs/simple_date_fix\n. Any chance of merging this in?  I'm running into the same conflict.\n. Yeah, I've tried the require 'aws-sdk' trick and it still does not prevent conflict with the Rightscale/Appoxy 'aws' gem.\n. The issue is that the 'aws-sdk' gem clobbers the 'Aws' namespace (used by the 'aws' gem) when it is added to a project, which makes it impossible to access any of the classes from the 'aws' gem that live in that namespace.\n. Excellent!  I'll check this out first thing in the morning.  Thanks for the help.\n. I've add a new commit (https://github.com/Octo-Labs/aws-sdk-ruby/commit/2134e04f200ba16aac8e7e905cb570d60b6e3618) in response to this comment from Loren. https://github.com/aws/aws-sdk-ruby/pull/125#commitcomment-2448789\n. Any chance of getting this pulled in?  (I'm trying to decide if should create a separate gem for this, or if it's going to get into the main library...)\n. Woo-hoo!  Thank you!\n. I was needing this in a couple of different projects, so I packaged it as a stand alone gem called simple_unique.  \nhttps://github.com/Octo-Labs/simple_unique\nhttps://rubygems.org/gems/simple_unique\nIf/when this one gets pulled in (or some other implementation of validates_uniqueness_of) I'll just deprecate simple_unique.\n. ",
    "professor": "Trevor,\nI'm glad to see that S3 does versioning, it's still not clear to me how to retrieve old versions and get them onto my filesystem. I'm probably missing something really obvious. I can get the ruby Object Version that I want, but then I need to .....?  I posted this question here: https://forums.aws.amazon.com/thread.jspa?threadID=85452&tstart=0 \n. ",
    "trobrock": "Just added a rake task for generating a gemspec for this project also so I can use the gem on my fork until this gets into the official version\n. I couldn't find a place in the tests that really tests any of these config points, am I missing something?\n. Those should be addressed here ecdea0040a1e997057f54e36c34167d33110f690 and 301a811a8bb656241703b55e7cc1679a923ec957\n. ",
    "ghost": "Great, thanks! I'll have a go of it.\n. Thanks!\n. Oh, what are valid values for price_class? 'PriceClass 100', for example? \n. I agree that that section of the guide was not really helpful when trying to stub when using a Resource instead of a Client.  One example that used a Resource would have been very helpful to me.  This is what I eventually came up with after seeing the example above:\n```\nec2 = Aws::EC2::Resource.new(stub_responses: Rails.env.test?)\nec2.client.stub_responses(:describe_instances, {\n  reservations: [{\n    instances: [ { instance_id: 'instance-42' } ]\n  }]\n})\nec2.instances.first\n=> #\n```\nIn hindsight it makes sense that to use \"Client Response Stubs\" requires using the client, but the documentation did not help me make that mental connection and I slogged through a few dead-ends before I found this thread.  Thanks  :-). ",
    "jjb": "ahh i see.\nsorry, i should have searched old tickets first.\nOn Mon, Mar 19, 2012 at 12:09 PM, Trevor Rowe <\nreply@reply.github.com\n\nwrote:\nThe nokogiri gem dependency was previously expressed as '>= 1.4.4', but\nthis became an issue when 1.5.1 was released.  Version 1.5.1 was not\ncompatible with Ruby 1.8:\nhttps://github.com/tenderlove/nokogiri/issues/632\nhttps://github.com/tenderlove/nokogiri/issues/631\nBoth of these issues have been resolved in 1.5.2; I will revisit this with\nthe next release.\n\nReply to this email directly or view it on GitHub:\nhttps://github.com/amazonwebservices/aws-sdk-for-ruby/pull/33#issuecomment-4576622\n. \n",
    "sikachu": "@trevorrowe 1.3.8 was released without the change in the gemspec. I saw it was released on March 16th while Nokogiri was updated on March 9th. (1 day after you release 1.3.7, dang!)\nAre you going to do another release soon? I think having something like ~> 1.5.1 would be better, though.\n. ",
    "mattmoskwa": "Thanks for this. I was about to open my own pull request :)\n. ",
    "davetchen": "Hi jfieber, I've been working on a Rails 3.2/ruby-1.9.3 app using appoxy/aws to manage EC2 instances Now I need to integrate paperclip with S3, which is looking for the aws-sdk gem. In my Gemfile I have:\ngem 'aws'\ngem 'aws-sdk', :git => 'git://github.com/jfieber/aws-sdk-for-ruby.git', :branch => 'appoxy-conflict'\ngem \"paperclip\", \"~> 3.0\"\nBut I get the following 'bundle' error.\n$ bundle\nUpdating git://github.com/jfieber/aws-sdk-for-ruby.git\nFetching source index for https://rubygems.org/\nCould not find gem 'aws-sdk (>= 0) ruby' in git://github.com/jfieber/aws-sdk-for-ruby.git (at appoxy-conflict).\nSource does not contain any versions of 'aws-sdk (>= 0) ruby'\nAny ideas? Thanks!\n-dave\n. There is already a pull request for doing the v2 upgrade: https://github.com/thoughtbot/paperclip/issues/1764\nDo you think it is on the right track?\n. ",
    "lsegal": "The biggest change in this PR is something we cannot accept without breaking a lot of existing users with a backward incompatible change to the SDK, so, unfortunately, we cannot merge this in.\nI do like the fixes to our documentation and tests so that users know to require the SDK by the correct library name, but these changes are superficial at best without actually removing the 'aws.rb' file, which, again, would break existing users.\nThis is something we will certainly make sure happens if we do a major version bump in the SDK, but this change will require a major version bump. Until then, we can update the README to point to the right gem name.\n. That's not exactly what is happening. Our gem actually uses the AWS namespace, which is compatible with the 'aws' gem's Aws namespace, since Ruby constants are case sensitive. The real issue happens when you require 'aws-sdk' before 'aws', RubyGems will throw the path to the gem in the load path, and any subsequent require for 'aws' will hit the load path before it does a full gem lookup. Bundler makes this a little more complicated, since it does loading in a different way.\nUnfortunately there's no way of fixing the issue inside the aws-sdk library until the next major revision (when we can make backward incompatible changes), but we're working on some potential workarounds that you can apply to your project.\n. The above commit should allow users to pass -raws to ruby commands in order to make the aws gem work with the aws-sdk gem. In the context of rails, this might mean running ruby -raws -S rails or exporting the RUBYOPT='-raws' environment variable.\n. @petemounce the build didn't fail, it simply had a pending test. I don't know of any pending tests in our current codebase-- have you tried running tests recently? As for integration tests, you should make use of your credentials provided through your environment variables, and never commit them into source control.\n. AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY, see @trevorrowe's recent series on credential management: http://ruby.awsblog.com/post/Tx15SMGKHNB9N28/Credential-Management-Part-1\n. Thank you for helping to improve the AWS SDK, @ohookins!\n. Thanks for the quick patch @natemueller!\n. Yes :)\n. Thanks for pointing this out!\n. Oops! Thanks again, @tedsparc!\n. To clarify: it seems like the underlying issue here is that calling #dup on Data::List will not perform a deep copy-- that and the fact that our docs are misleading in reporting an \"Array\" return rather than the proper Data::List class that is returned (which actually wraps the Array). I think we can fix both of those things, so that a) the docs don't refer to the returned instance data as a plain array, and, b) we can better handle dup to perform a deep copy so they work better when a user wants to mutate the original object.\n. Thanks for the fix!\n. Closing this since it seems to be a Ruby issue. Thanks for reporting-- let us know if this happens on a later version than p286!\n. Can you provide a backtrace for the above exception?\n. I'm going to close this issue since there has been no feedback in the last few months. @karlfreeman, if you are still having problems with DynamoDB in your application, please follow @trevorrowe's instructions and provide some more detailed logging of your requests so we can narrow down the problem. Thanks!\n. We don't have auto-loading of plugins, though this is typically solved in application code with Gemfiles, so there are external ways to achieve this without having any such kind of mechanism. You can list as an install step to add gem 'aws-sdk-osx-keychain' to their Gemfile and then require bundler/setup, for instance, to load your plugin. Is there a reason auto-load is necessary?\n. Okay let's close this. \nFor what it's worth, I think the scenario you're describing definitely lends itself to being outside core, and possibly outside a plugin (your aws-keychain-util project seems like a much better fit).\n. @styliii first, thanks for the pull request! I believe encouraging users to use the latest JSON gem is a great idea. However I don't believe we should be enforcing this from a library perspective, as this should be an application level decision, and there are better ways to manage this kind of a thing (a Gemfile.lock, for one).\nFrom a technical perspective, this patch addresses the Gemfile for local development but does not modify the gemspec, which is what we use to generate the gem, so we would still not be enforcing 1.7.7+ for users doing a gem install aws-sdk. I've just made a change in 080badd83387f8e9c70bf657f82fc46827e8acd2 to grab dependencies in our Gemfile from the gemspec so this duplication of dependencies is no longer a source of confusion.\nAs much as we want people to upgrade to 1.7.7+, I don't think the aws-sdk should force users onto these versions for the simple reason that certain applications might be locked down to a very specific version of the JSON gem prior to 1.7.x which would cause odd failures during bundle installs. Note that our dependency list is meant to mark the versions  that we are compatible with, so I think it's correct to keep it the way it is. I think that educating developers as to why they should upgrade their JSON gem is a more productive step than silently upgrading it for them in a gem install-- the latter hides the fact that there was a behavioural change in their application. I hope that makes sense.\n. Thanks!\n. Thanks for the fix!\n. Hey @jan,\nYes, the API returns from the call before it \"creates\" the instance. This is due to the fact that instance creation is an operation that can only be considered eventually consistent. Some services optimize for eventual consistency over atomicity for performance reasons, and occasionally some API operations require eventual consistency because they take a relatively long time to process and would otherwise end up tying up a socket connection instead of serving other requests. \nAs @trevorrowe pointed out, this differs per the needs of each service, though we do want to have better support for waiting on specific resource states for these scenarios. That said, you'll probably see an API closer to the one proposed by Trevor above, or perhaps something like wait_for(&:exists?) / wait_for(:created), instead of a separate block_until_exists method. This is because some resources have many different states (created, deleted, pending, etc.), so we would want an API that could express all of these different states in a consistent way.\n. I would suggest adding custom metadata to your object that includes the MD5 sum of the object. We were actually just recently kicking around the idea of adding this value on the user's behalf (patches would be accepted for this if you want to help out). Until that is added, you can do something like:\n``` ruby\nbucket = 'the_bucket'\nkey = 'the_key'\nfile = 'the_file_to_upload'\nConsider reading in N mb chunks for better memory performance\nrequire 'digest/md5'\nchecksum = Digest::MD5.hexdigest File.read(file)\ns3.buckets[bucket].objects[key].write file: file, metadata: {checksum: checksum} \n```\nThis does mean that you will effectively be reading the file twice, which is one of the reasons why the SDK should be doing this on your behalf-- but it will get the job done.\nYou can then read the metadata back out with:\nruby\ns3.buckets[bucket].objects[key].metadata[:checksum]\n. > So what are the limitations of recommending PresignedPost? We cannot send multiple files in one web form using this method?\n@schneems You can with some JavaScript-fu, i.e., creating a separate <form> for each file and/or intercepting the default browser form handler and sending the data via an XHR request. Once you have a pre-signed URL you can easily just $.ajax() the data into the bucket-- see aws/aws-sdk-js#251 for code, along with the File API to get at the payload. You technically don't need to use forms at all.\n. @schneems any time!\n. The real solution here is to upgrade your Ruby to 2.0, or at least 1.9.3. MRI 1.9.2 is an unsupported version of the 1.9 series-- it is not supported by Rails, and, as far as I know, it is not supported by the Ruby core team (the recent JSON vulnerability fix is not backported into 1.9.2, for instance). \nAutoload was made threadsafe last year and should be threadsafe in 1.9.3+, so the idea that autoloading is a \"dangerous mechanism\" is no longer a correct statement. See http://bugs.ruby-lang.org/issues/921 for more info. The problem here is that there is a lot of well SEO'd misinformation about autoloading in Ruby, so the correct path here is getting the word out about the fact that autoload has been fixed, not to move away from it.\nFWIW, 1.9.2p320 also looks to have the autoload fix, so you should at the very least be on the latest patchlevel of 1.9.2, but I would recommend against using that version in general. You should really be already testing out 2.0 for deployment, but if not, at least run 1.9.3, as it's stable and supported.\nIf you try this out on 1.9.3+ (or even 1.9.2p320) and still have this problem, we can reopen this issue, but until then I think we should close this out as we plan on continuing to support autoload for as long as it's supported in Ruby core.\n. There was a regression in nokogiri, the XML parsing library we used, that was released yesterday. The regression should only be present in 1.5.7 and was quickly fixed with a 1.5.8 release (also yesterday). You should upgrade to Nokogiri 1.5.8, either with gem install nokogiri or by doing a bundle update if you have a Gemfile. See: sparklemotion/nokogiri#868 and https://github.com/sparklemotion/nokogiri/blob/master/CHANGELOG.rdoc\n. @Dru89 Can you add AWS.config(http_wire_trace: true) to look at the data being sent by the SDK and verify that it is not duplicating the xmlns attribute as described in this bug?\n. That XML looks proper to me. Does that error occur every time you run your script? or perhaps it only happens occasionally?\n. Sorry for taking so long to respond to this,\nI don't think there's any real need for the SDK to provide uploading support for pre-signed URLs. It might make a good utility method, but ultimately, uploading data to a pre-signed PUT should be as simple as using any Ruby HTTP library to perform the put on the URL with the body payload provided. Here's what it looks like with core Net::HTTP-- the following script will upload itself to S3 into a bucket/key:\n``` ruby\nrequire 'aws-sdk'\nrequire 'net/https'\ns3object = AWS.s3.buckets[ENV['BUCKET']].objects[ENV['KEY']]\nurl = s3object.url_for(:write)\nputs \"Uploading to #{url.to_s}\"\nNet::HTTP.start(url.host, url.port, :use_ssl => true) do |http|\n  req = Net::HTTP::Put.new(url.request_uri)\n  req.body_stream = File.open(FILE)\n  req['Content-Length'] = req.body_stream.size\n  req['Content-Type'] = '' # Prevents Net::HTTP from adding content-type\n  response = http.request(req)\n  puts \"Status code: #{response.code}\"\n  puts \"Response body: #{response.body}\"\nend\nputs \"Data written:\"\nputs s3object.read\n```\nUse with:\n$ BUCKET=mybucket KEY=mykey ruby uploader.rb\nI think a utility like this (aws-sdk-presigned-uploader) would be great as a third party gem.\n. Can you try this with http_wire_trace => true?\nruby\ns3 = AWS::S3.new(:http_wire_trace => true)\ns3.buckets[:bucket_name].enable_versioning\nJust make sure to remove any sensitive data/headers (like your access key id).\n. Version dependencies are AND'ed together, not OR'd, so that dependency specifier would not work:\nResolving dependencies...\nCould not find gem 'nokogiri (>= 1.5.8, ~> 1.4.4) ruby', which is required by gem 'aws-sdk (>= 0) ruby', in any of the sources.\n. Thanks!\n. Looks related to #192, though @trevorrowe was unable to reproduce any issues with the block-less format. We will have to test this one out again.\n. Is the above code the minimal case? ie., is :encryption_key needed to reproduce?\n. That's good to know. Explains why @trevorrowe didn't reproduce it.\n. Thanks for the quick fix!\n. I'm going to close this since there hasn't been a response in a while, but using the above workaround (if it is even necessary) should be sufficient. Please re-open if you are still having an issue with Railties, but it would be great if you could also provide more information about what you are trying to accomplish, and what you think the AWS SDK for Ruby is doing wrong.\n. Thanks!\n. Since this seems to be specific to Ruby 2.0, I wonder if those who are experiencing this issue on p0 can also reproduce it on 2.0.0-p195 which was released last week. There may have been some related changes that could have fixed this issue.\n. I wonder what regions everybody else is using?\n. Closing this issue since this was released with v1.12.0. Please chime in here if you are still seeing write timeouts in S3 with Ruby 2.0.\n. @mikhailov unfortunately the only way to get the patch is by upgrading your gem. Why is upgrading problematic?\nThat said, keep in mind that this error is also very common when you are putting too much traffic through S3. Typically this error means that your connection is being throttled, and S3 is killing your connection mid-way through because it never received a 100 continue header. Enabling 100-continue won't stop you from being throttled, but it will provide a much less cryptic error on failures.\nI would recommend that everyone with this problem re-attempts reproducing the error with 100-continue enabled in the SDK. This should at least make it more clear if there is throttling going on or if there still is an issue in the SDK. To enable this feature, set the :http_continue_threshold on the global config or S3 object. Example:\n``` ruby\nAWS.config(http_continue_threshold: 1024 * 1024) # 1MB threshold\nor on the S3 object directly\ns3 = AWS::S3.new(http_continue_threshold: 1024 * 1024)\n```\nDocumentation for :http_continue_theshold can be found here.\n. @mikhailov giving :http_continue_threshold a should should be able to make things more clear about whether this is a throttling issue. If it is not, then I would definitely recommend looking at upgrading to the latest SDK which should include many more fixes than this.\n. @ab can you try with 100-continue support described above? This will help rule out non-SDK causes for disconnection.\n. @mikhailov sorry for missing this-- S3 throttling limits depend on a lot of factors. For all intents and purposes, you can count all API operations, including upload and download.\n. @mikhailov as I mentioned, the actual limit depends on many factors. It's something you will have to experiment with. That said, throttling limits should be scoped to each region-- you can test this out too.\n. @mikhailov you can use endpoint instead of s3_endpoint as mentioned in that blog post.\n. Thanks for the doc fix!\n. Closing because this should be resolved by the above commit and released in 1.10.0. Please re-open if you are still having this issue.\n. @kbacha @laurilehmijoki this is caused either because of packet loss (as you mentioned), or possibly because S3 is throttling the connections, if you are opening too many. In either case, there is little the SDK can do besides attempt to retry, though the real solution here is to lower the thread count of connections in your concurrent usage-- sending as fast as possible and hitting throttling or packet loss is probably going to be less efficient than sending at a slower but reliable rate. Realize that hitting EOFError means the SDK originally retried 3 times on your data, possibly sending up to ~10mb each time, on each file. This can add up to a lot of wasted bandwidth.\n. Answering in laurilehmijoki/s3_website#8\n. Version 1.5.8 is extremely old. Are you able to reproduce this in the latest version (1.9.5)?\n. Are you sure it hasn't retried? In my tests, we are capturing those exceptions and forcing a retry. Note that we do re-raise, but re-raising the exception is actually what causes the retry. Eventually, however, once we hit our max retry count, we will bubble that exception back up, and that is what you are seeing. By default that max retry count is 3, which will happen quite fast in the event of a \"no route to host\" exception.\nAdding :http_wire_trace => true or a logger will help show how many retries occurred:\n``` ruby\nrequire 'aws-sdk'\nrequire 'logger'\nconfig = {:http_wire_trace => true}\nlog to standard out, strip all of the Ruby logger standard prefixes\nlogger = Logger.new($stdout)\nlogger.formatter = proc {|severity, datetime, progname, msg| msg }\nconfig[:logger] = logger\ncolorize the logs\nconfig[:log_formatter] = AWS::Core::LogFormatter.colored\nAWS.config(config)\nCall your operations here...\n```\nNote that you can bump up your max retries by setting the following config option:\nruby\nAWS.config(:max_retries => 10) # retry 10 times\nThis is basically what you were doing, but by hacking the HTTP class instead.\n. Retries (can) happen on any service or network error. In other words, we should be handling Errnos too. I just used the above script to call operations on S3 while disconnected from the internet (shut off wifi) and was able to log retries.\n``` ruby\n\n\ns3.buckets['mybucket'].objects['key'].read\nopening connection to mybucket.s3.amazonaws.com...\nopening connection to mybucket.s3.amazonaws.com...\nopening connection to mybucket.s3.amazonaws.com...\nopening connection to mybucket.s3.amazonaws.com...\n[AWS S3 200 2.115574 3 retries] get_object(:bucket_name=>\"mybucket\",:key=>\"key\") SocketError getaddrinfo: nodename nor servname provided, or not known\nSocketError: getaddrinfo: nodename nor servname provided, or not known\n  from /Users/lsegal/.rvm/rubies/ruby-1.8.7-p371/lib/ruby/1.8/net/http.rb:560:in initialize'\n  from /Users/lsegal/.rvm/rubies/ruby-1.8.7-p371/lib/ruby/1.8/net/http.rb:560:inopen'\n  from /Users/lsegal/.rvm/rubies/ruby-1.8.7-p371/lib/ruby/1.8/net/http.rb:560:in connect'\n  from /Users/lsegal/.rvm/rubies/ruby-1.8.7-p371/lib/ruby/1.8/timeout.rb:67:intimeout'\n  from /Users/lsegal/.rvm/rubies/ruby-1.8.7-p371/lib/ruby/1.8/timeout.rb:101:in timeout'\n  from /Users/lsegal/.rvm/rubies/ruby-1.8.7-p371/lib/ruby/1.8/net/http.rb:560:inconnect'\n  from /Users/lsegal/.rvm/rubies/ruby-1.8.7-p371/lib/ruby/1.8/net/http.rb:553:in do_start'\n  from /Users/lsegal/.rvm/rubies/ruby-1.8.7-p371/lib/ruby/1.8/net/http.rb:548:instart'\n  from /Users/lsegal/aws/ruby/sdk/lib/aws/core/http/connection_pool.rb:301:in start_session'\n  from /Users/lsegal/aws/ruby/sdk/lib/aws/core/http/connection_pool.rb:125:insession_for'\n  from /Users/lsegal/aws/ruby/sdk/lib/aws/core/http/net_http_handler.rb:52:in `handle'\n```\n. What version of Rails are you using to test this, as well as what version of the Ruby SDK? I was unable to reproduce this issue in the latest version of the SDK (1.9.5) with Rails 4, so I don't think this was a recent change.\n\n\nIt's possible (but unlikely if you're testing in Pry) that this is related to autoload behaviour, which could be resolved by adding:\nruby\nAWS.eager_autoload!\nTo the top of your script / Pry session. \n. Okay, I can reproduce this now with deliver! and we have indeed found the culprit. Note however that you should be calling deliver, not deliver!, as per the docs:\n\n\"This method bypasses checking perform_deliveries and raise_delivery_errors, so use with caution\"\nhttps://github.com/mikel/mail/blob/master/lib/mail/message.rb#L237-L238\n\nThat said, we will fix this in the next release, but a simple workaround in the meantime would be to add an empty hash attribute \"settings\" to the SimpleEmailService class like so:\nruby\nclass AWS::SimpleEmailService; def settings; {} end end\nOr you can simply stick to deliver.\n. Weird, deliver should be able to find more issues than deliver!, since the latter actually skips a bunch of sanity checks. deliver! with a bang is not like save!-with-a-bang in that it does not raise more exceptions-- it raises fewer. What kind of issues was deliver! seeing that deliver was not?\n. Looks like the key checking only occurs in the where method, which means in the meantime you can workaround this by creating the ConditionBuilder directly:\nruby\nAWS::S3::PresignedPost::ConditionBuilder.new(presigned_post, 'x-requested-with').is('')\n. @awood45 can you take a look at this?\n. This is a third-party issue with paperclip. See this pull-request that fixes the issue: thoughtbot/paperclip#1241 as well as this related forum post that explains how to use the forked repository as a temporary workaround.\n. @rushingfitness Note that paperclip has fixed and released this issue in 3.4.2, so you should no longer need to use my fork. However, I imagine your Gemfile.lock still depends on a version of paperclip prior to 3.4.2. Take a look at the referenced paperclip issue above, since other users are reporting this too, and it seems to be related to sidekiq.\n. I'm going to close this issue since tracking this on paperclip is more appropriate (this is a third-party issue), and it looks like you were already there (sorry I didn't notice!). \nThanks for bringing this to our attention, everyone.\n. This is related to #250, see that ticket for a workaround (which you discuss in your description).\n. Closing this as a duplicate of issue 250. Let's track this there.\n. Hey @Ryman, I like this idea, and if @trevorrowe agrees, I think we should merge it, provided you can take a look at and fix up the (mostly minor syntax) notes I wrote above.\n. Have you checked the number of retries on these requests? It could have done so. If the SDK hits the maximum retry count (which is 3 by default) it will give up and raise the original exception. You can check the retry count by adding a logger when you create your S3 object:\nruby\nrequire 'logger'\ns3 = AWS::S3.new(:logger => Logger.new($stdout))\ns3_bucket = s3.buckets[OUR_BUCKET]\ns3_obj = AWS::S3::S3Object.new(s3_bucket, our_key)\ns3_obj.write(:file => our_file_path)\nWhat data does that give you?\n. Given the extremely short time delta on that request it looks kind of like it didn't even get across the wire. The exception might have been raised from internally on the SDK, which would not trigger a retry-- though it doesn't explain where the status code came from.\nDo you have similar logs from the 505's by the way?\n. What happens if you turn wire tracing on?\nruby\ns3 = AWS::S3.new(:http_wire_trace => true)\n. That backtrace is missing the exception class / message. Can you provide that for more information about what is blowing up?\n. Seems like ActiveSupport's BufferedLogger does not fully implement the Logger API, but it should work if you use the following:\nruby\nrequire 'logger'\ns3 = AWS::S3.new(:logger => Logger.new($stdout), :http_wire_trace => true)\nIf you want to log to a file you can use File.open('aws-requests.txt', 'w') instead.\n. Sounds good to me.\n. This seems to have caused a build failure. Please make sure the unit tests pass before submitting pull requests, as it makes it easier for us to decide what to merge. Also, adding test coverage for the code you've added gives us more confidence that the code works. It would be great if you could add tests for this new behavior.\nOtherwise I think this looks like an okay change, unless @trevorrowe has any reservations.\n. @loopj what other test failures were you seeing? You should see a green test suite in a fresh checkout. If you are not, those are test/build bugs we need to fix.\n. @loopj can you open a separate issue for those along with full backtrace on the exceptions?\n. As a general rule of thumb we would want to see unit tests for the added code, especially in this case when there is some subtle behavior going on here (with the respond_to? check). Covering all the cases of this new functionality in tests would be useful.\n. Thanks for the pull request @loopj! :+1:\n. Hi @redzebra. First off, thanks for the pull request!\nThe current behavior is the intended functionality of the SDK. The reason we allow regions to be configurable is so that as services add new regions (which happens fairly often), users can immediately access those regions without requiring an update to the SDK. For that reason, hardcoding us-east-1 as the \"only\" region would be the wrong move here, especially if the AWS Support API were to launch in new regions in the future. Basically, we allow the user to specify the region they want, and this usually makes the SDK much more extensible. I wouldn't call your scenario a \"break\", since you are explicitly making the choice to attempt to use AWS Support in the specified region. This is a lot like the ability to specify any arbitrary service :endpoint option, which can also fail if you specify an invalid endpoint. Anyhow, the above code might be a perfectly valid call in the future, it just isn't right now.\nThat said, allowing users to configure any region does lend itself to a little bit more confusion in these scenarios for newly launched services in limited regions. I think the right thing to do here would be to have the SDK emit a warning if the region is not recognized for the service. I would stress warning, and not exception, since we do not want to stop users from configuring arbitrary regions, otherwise they would be forced to update their copy of the SDK just to access services in newly launched regions. I would also point out that we would want a consistent solution across all services here, not just for a single service. A pull request that added this kind of behavior would be much closer to the track we want to follow going forward. @trevorrowe might have some extra thoughts on this, having just blogged about some of the additional work he's just recently done with region support in the SDK.\nHope that explains some stuff about how regions work in the SDK. I'm going to close this, because hard-coding us-east-1 in the Support endpoint is not something we want to do, but if you want to take a stab at the stuff I discussed above, we'd certainly love that!\n. Is this maybe a regression in RSpec? What happens if you gem install rspec -v 2.13.0 (the latest version)?\n. It looks like you've created the bucket object with a hash as the bucket name. The correct syntax for instantiating a bucket is:\n``` ruby\n\n\nAWS.config(key: '...', secret: '...')\nbucket = AWS.s3.buckets['test-bucket']\n=> #\nbucket.name\n=> \"test-bucket\"\n```\n\n\nNote that you should be seeing just the bucket name in the inspected value of the bucket object, not a hash. Seeing a hash means you might have passed a hash to the bucket name like so:\n``` ruby\n\n\nbucket=AWS.s3.buckets[{a: 1, b: 2}]\n=> #1, :b=>2}>\nbucket.name\n=> \"{:a=>1, :b=>2}\"\nbucket.url\n=> \"http://s3.amazonaws.com/{:a=>1, :b=>2}/\"\n```\n\n\nNote that the SDK doesn't try to protect the name from being a valid String, which is something we might want to do though.\n. This would be a typo in the Elastic Beanstalk console configuration panel, at least if you are referring to a Ruby environment. The AWS SDK for Ruby does not make use of any variables named AWS_SECRET_KEY, so setting this value would have no effect. The actual variable name should be AWS_SECRET_ACCESS_KEY, as you described.\nNote that you can set any environment variable on Elastic Beanstalk by using the eb CLI tool or with a .ebextensions file (the latter is not recommended if you are using git aws.push), not just the ones exposed in the console. The console simply exposes some common variables that users can set, but does not list every possible environment variable.\nNote also that you can also use IAM roles for EC2 instances to configure your credentials with Elastic Beanstalk. You can do this by assigning the correct permissions to an IAM role and setting it as the \"Instance Profile\" value in the Server tab of the configuration panel. Using IAM roles is much more secure, since you can create roles that have restricted credentials, meaning if the key gets leaked, the amount of damage done could be limited. You can also very easily revoke credentials for an IAM role without affecting any of your other applications, which is not as easy to do if you are using your master secret key. In addition, the AWS SDK for Ruby will automatically use an instance profile if present on the EC2 instance with zero configuration necessary on your part.\nHope that helps!\n. aws-repl.rb? aws.rb?\n. The script is failing on every version of the SDK since 12 hours ago? Can you log the EC2 requests with wire tracing on to see what the response data contains? (make sure to scrub out any private information!)\nYou can enable wire tracing by setting AWS.config(:http_wire_trace => true) before creating the EC2 instance.\n. So it looks like the <output> element is not in the response, which was my hunch. I assume that if it had been working 12 hours ago it means EC2 is no longer listing this element. Fortunately it seems like this would only be the case for instances with no output to display yet, since I've tested this on long running instances and console output seems to work there.\nA temporary workaround would be to wrap this code in a begin rescue block that retries the request in the same way that you wait for instance state to be :running. We will perhaps have to handle the case where output is missing.\n. @ab we can't give exact dates but we will definitely be cutting a gem for this fix fairly soon.\n. Adding a note for 1.8.x users was [also] my idea, and I don't think it's unreasonable to have 1.8.x users jump through just a few extra hoops to get a running aws-sdk now that 1.8 is EOL.\nThat said, and this is the important part, we cannot just drop 1.8.x support and break existing customers with no warning. I would not expect any responsible library maintainer to do that to their customers without a major version bump, and so we don't have much choice but to have some deprecation plan with sufficient time to notify customers of the upcoming change.\nNote that locking to <1.6 is only a temporary strategy as we figure out how to move forward here. @trevorrowe said this a few times. The goal is eventually to remove the version constraint.\n@gtd you make a lot of good points. The problem is that this change was driven by real customer pain, not hypotheticals. We're not talking about \"slim possibilities\" here, were talking about real customers with broken code. I agree that the fix is trivial (I had previously suggested adding the Gemfile notice), but we need to inform our customers first. We can't just make the change and tell our customers that we broke their code after the fact. This will take some time to unravel, but we will unravel it. I think the main thing here is to be patient. This is temporary, but we're all moving in the same direction.\n. Hey guys, we just announced plans for version 2.0 of the aws-sdk gem which will be dropping support for Ruby 1.8.7. This gives us a path forward without breaking our existing users. Questions and comments are welcome.\n. @pglombardo can you provide details on how you ran into the dependency block? Is there a specific library that you are using?\n. @craigmcnamara unfortunately #330 will not solve the problem-- I've commented in the commit as to why.\n. @eahanson we published the GitHub repository for v2 at https://github.com/aws/aws-sdk-core-ruby\n. @kellyfelkins those aren't recent updates. The version 1.11.1 is an older version that did not have the nokogiri version lock. The latest version of the AWS SDK for Ruby does lock to a version prior to 1.6.0 for Ruby 1.8 compatibility. As we move to V2 of the SDK, the nokogiri dependency will be relaxed (and optional).\n. Hey so,\nWe all have the same goals; let's not make this dramatic. We all want to see the Nokogiri version lock removed, we just have to make sure we do it responsibly and in such a way that breaks as few users as possible. \nWe have a plan in place and we will be sharing something fairly shortly. We asked for your patience a little while ago, and I'm now asking again to wait just a little longer until we get this all straightened out. Until then, let's let cooler heads prevail-- there's no need for any attacks on either end.\n@smenor to clarify, I do actually work for AWS, I do not volunteer, though I do try to spend quite a bit of time volunteering my help in Ruby's open source community, that might be what Ryan was referring to. As far as the RUBY_VERSION thing goes, we've had a few pull requests that we had to close (#330, #383, etc) because RubyGems doesn't support this mechanism. We don't have a Gemfile, we just have a gemspec.\n. Instead of nil it should be documented as returning \"void\". This gives us room to add a contract where it was previously (explicitly) undefined.\n. Great!\n. :+1:\n. We likely don't want to completely remove proxy support from the CurbHandler. Rather, we should update the handler to use the new proxy data. You're right that it's no longer in the Request, but it should have been moved to the options passed in during initialization of the handler object, so we can grab the proxy options from there. \n@trevorrowe would you want to take a look at this?\n. Awesome @ktzhu! :+1:\n. This causes an interesting failure in our test suite. Did you encounter this when running the tests? It seems like this change might have uncovered an unrelated bug in the MFA code.\n. It seems to be failing on all Ruby versions. Were you not reproducing this in 1.9 or 2.0? Just curious, because if not this might be an environment issue.\n. Note that we should have a validation layer that should have already checked all inputs, so if something was not passed in as an integer, that should have been caught. It might not be necessary to do extra validation, here.\n. Seems alright to me. Should we update the documentation as well? A @note would be helpful (\"this method is undefined for inputs less than 6 characters\"). Adding example tags would also clarify the docs.\n. Thanks!\n. Looks good.\n. I think \"quiet\" should default on when using -e. Users should specify -v (or -l) if they want logging on with -e. That would be a more natural interface to those using aws-rb on a one-off basis:\nbash\n$ aws-rb -e 'puts s3.buckets.map(&:name)'\nConversely, logging should only be enabled by default in \"interactive mode\". In a non-interactive mode, we should assume the STDOUT is reserved for the user and not pollute it unless asked to.\n. We already have support for multiple HTTP libraries via a pluggable interface. We previously supported httparty, and we currently still support curb, as separate HTTP handlers. A handler is simply a class that responds to #handle with an HTTP request and response object. You can see the implementations in lib/aws/core/http.\nYou can replace the handler by specifying the :http_handler option either to the global config or to individual clients constructors with your handler object:\n``` ruby\nclass MyHandler\n  def handle(req, resp) ... end\nend\nAWS.config(http_handler: MyHandler.new)\nor\nAWS::S3.new(http_handler: MyHandler.new)\n```\nSee the AWS.config documentation for more.\n. Thanks for the quick fix!\n. Thanks for the quick fix! \n. Can you re-submit this patch without all the whitespace/indentation changes to the other parts of the code? Just git push -f to your existing branch.\nAlso note that we use two space indentation for all code, which should apply to your newly written tests too.\nThanks!\n. Thanks for the pull request! Unfortunately, we can't accept this patch. Please see #273 for more information on our plans for moving forward with 1.8 support.\n. Thanks for the pull request, @drewda. We're actually making a bunch of changes to the way we generate docs, so I'm not sure we want to accept the patch, but you pointed out a number of extra fixes we should be making, specifically:\n- The markdown gem we are currently using isn't a fan of nested lists. Redcarpet has known issues with handling these lists, and that's why you have to hack it with extra indentation. I've been toying around with rdiscount instead of redcarpet, and that seems to solve it without having to modify the indentation. That's an easy change.\n- Using <Hash> doesn't seem to be markdown friendly, so we're going to have to escape these. Note that the Array<Hash> (and other similar type specifiers) is used in many more places than just CloudSearch, so we will want to fix them everwhere, not just in this file.\n- We're actually moving away from codegen'd documentation. This is the main reason we will opt out of accepting this. With the new API versioned clients, having codegen'd documentation requires a lot of duplication of docs, so we want to remove this extra duplication. Unfortunately all those doc files you've been editing are (a) automatically generated and (b) going to disappear very soon. I think removing the codegen'd documentation will actually help avoid these scenarios where awesome people like you spend time fixing stuff that we just can't merge in (because subsequent executions of our codegen will just bring the problem back).\nBut thanks again for bringing these issues to our attention with this patch! I don't think we will accept your code, but I'll keep this issue open to track our progress on making these fixes.\n. Bringing @trevorrowe in on this too.\n. Capybara does not require nokogiri 1.6.0, it requires >= 1.3.3; you can see this on the rubygems.org capybara gem page. Are you sure you don't have gem 'nokogiri', '= 1.6.0' somewhere else in your Gemfile?\n. Glad you got this sorted out @i-arindam.\n. I'm able to reproduce this. It looks like a regression that affects all CloudFront operations.\n. I don't think we should implement .last in the collection class due to the caveats @trevorrowe pointed out. I think a more than reasonable workaround would be to simply use the to_a method and call last on that, i.e., objects.to_a.last (ditto for buckets).\nI'm going to close this issue; feel free to reopen if you think we truly need a #last method.\n. Looks like we should be using #min and #max instead of #first and #last here:\nhttps://github.com/aws/aws-sdk-ruby/blob/master/lib/aws/s3/client.rb#L926\nThat is a fairly easy change to make.\n. Yes, I will be looking at this. I currently have a working fix, but you can't use #min and #max because they are not available in 1.8.7. Fortunately using #exclude_end? is just as trivial a change.\n. The above fix will go out with our next release. Thanks for opening this issue @kbullaughey!\n. @PeterBengtson due to our work on V2.0 of the AWS SDK for Ruby, we have no plans to put out new high level abstractions for this version of the SDK. In addition to the new version, we also want to make some significant improvements to the high level APIs to make them more idiomatic and easy to use. If you are interested, we would certainly be happy to look at a pull request that ports over the high level functionality of the DynamoDB client to the new API version.\n. Thanks for pointing out the inconsistency. We could alias this class but we couldn't rename it in case anyone is making use of that class name. I'm personally of the opinion that we shouldn't bother. @trevorrowe, @awood45, what do you guys think?\n. Can you provide the names of the libraries you are using that depend on Nokogiri 1.6.x?\n. The Ruby SDK supports this, but it's not (currently) available in the high level abstraction. You can still access it in the low level Client class:\nhttp://docs.aws.amazon.com/AWSRubySDK/latest/AWS/CloudFormation/Client.html#create_stack-instance_method\nruby\ncf = AWS::CloudFormation.new\ncf.client.create_stack(options)\nFeel free to submit a pull request if you want to take on adding this support to the high level abstraction!\n. Can you provide code that reproduces this error?\n. I can't reproduce this. Have you tried updating to the latest version of the SDK (1.15.0)?\nIf this still happens on the latest version, run the following command:\n$ aws-rb -v -e 'AWS.sns.client.list_topics'\nAnd provide the output (make sure to sanitize any private information)\n. Try the following:\n$ export AWS_ACCESS_KEY_ID='...'\n$ export AWS_SECRET_ACCESS_KEY='...'\n$ aws-rb -v -e 'AWS.sns.client.list_topics'\n(only list the output of the last command, aws-rb)\n. I was just writing this. Glad you got it sorted!\n. Can you provide more details on what doesn't work? If you are getting an error, please provide the full stack trace.\n. I'm reading through the Java sample application provided in the mobile push getting started guide, and it seems as though they set the MessageStructure parameter to 'json'. This might be required. See line 229 of the SNSSamples/src/com/amazonaws/sns/samples/mobilepush/SNSMobilePush.java file. I think you might want the structure to look like:\nruby\nmessage = { :default => \"Default message\", :aps => { :alert => params[:message]}}.to_json\nAWS.sns.client.publish :message => message, :target_arn => target, :message_structure => 'json'\nNote that you also need a \"default\" key at the root of your JSON object if you use the 'json' message structure. See the Ruby docs for #publish and the API docs for more info on this parameter.\nI can't test this myself, so let me know if this works for you.\n. Glad you got it working!\n. Try using aws-rb -v your_script.rb to see what the SDK is sending and receiving over the wire. Does that give you anything to work with?\n. I'm going to close this issue since there has been no response in the last week. Feel free to re-open this if you are able to provide a wire trace with more information about this issue.\n. This is causing Travis failures. Once we get those fixed it should be okay to merge.\n. You have to pass a \"default\" key if you use a JSON payload. See the docs and the example I gave you in the last issue you opened. \n. @goleador are you suggesting that the solution given in #336 is not working for you anymore? You had mentioned in that issue that it was. Is this issue different from the original one?\n. If the docs say it should be returning a value and its not that would be a bug. Are you sure the email is being sent?\n. In that case it seems like we are just missing this functionality. I agree that we should return data back. \n. No. I think we can add this in unless @trevorrowe or @awood45 have objections. \n. This should have been fixed with the latest release via #343. What version are you testing this on?\n. Looks good to me, thanks for putting this together!\n. Actually I noticed some issues with the patch, I will comment inline.\n. Thanks for the patch, Kyle!\n. @leelynne if you submit a PR it will create a new issue, we can reference this closed issue if we merge that. It's not necessary to leave this open given that we have no immediate plans to add support for this from our end. We certainly look forward to your pull request, though.\n. Looks good.\n. There is no explicit asynchronous support in the SDK, but you can make use of threading or sub-processes in Ruby to achieve the same result. Simply launch a new set of threads that will process these requests, or alternatively, just fork the process. There are also plugins like em_aws which implement asynchrony through EventMachine and might be what you are looking for, as well as more generic solutions like Celluloid that you can look at.\nDoes that help?\n. @mikhailov can you explain what you mean by high CPU consumption? The SDK is doing nothing client side, so there should be no reason for CPU usage to increase by any significant amount-- all of the time is being spent blocking on I/O waiting for S3 to return its response. Are you sure you are talking about CPU time and simply a roundtrip time?\nAs for the time data you posted, it is definitely the case that transfers inside of a region will run faster than inter-region copies. The time values you listed seem reasonable given the regions specified. Note however that those times don't represent any CPU usage, those are likely times waiting on I/O from S3.\n. I am going to close this since workarounds to use the SDK in an async manner are provided above, though I don't believe that asynchrony is going to solve your latency issue across regions. If you still believe that there is something that the SDK is doing incorrectly, and this is not related to inter-region latency, feel free to re-open-- but if you do, please make sure to provide more details about your problem.\n. This is by design. The high level abstraction for DynamoDB does not support the new 2012-08-10 API version. See #327\n. What version of the SDK are you using? AWS::VERSION should tell you that, or bundle show\n. The issue seems to be that :allowed_methods and :allowed_origins are required parameters, but the SDK is not enforcing this client-side. The following request works for me when I add those:\nruby\nAWS.s3.buckets['mybucket'].cors.set max_age_seconds: 3600,\n  allowed_methods: ['GET'], allowed_origins: ['*']\nAre you passing those in Fog? If not, maybe it is doing it for you through the use of defaults.\nThe bug here is that allowed_methods and allowed_origins are not required when they should be.\n. I'm reopening this because we still need to ensure that we fail client-side when those parameters are not supplied.\n. Hi @goleador. \nUnless you are reporting a specific issue/bug with the SDK, I would suggest posting on the SNS forums about any questions you have about using the API. The forums are a much better place to get support on using specific services, and they can point you in the right direction if you are passing any invalid values. I would also highly recommend reading the source for the sample application discussed in #336 as well as the SNS documentation to help debug your issue, since they explain how to properly use mobile push.\nI will close this issue since the SNS forums/support are the right place for this question, not the Ruby SDK. Feel free to reopen if you believe that there is a bug in the SDK.\n. What error are you getting from the service?\nPlease list the output running that script through aws-rb -v yourscript.rb\n. Also, have you looked at https://forums.aws.amazon.com/thread.jspa?threadID=134152&tstart=0 ? The thread uses the Node.js SDK, but you should be able to translate the call into the Ruby equivalent to get what you want. It seems this user was having a similar issue.\n. Judging by that forum thread, it seems that your payload has to include \"APS\" (uppercase) and/or \"APNS_SANDBOX\". This is also documented in the Mobile Push documentation.\n. See #327 and #355 for more information on the state of the higher level DynamoDB abstraction compatibility with the 2012-08-10 API version. Marking this as a duplicate of those issues.\n. I think the above commit satisfies all of our concerns in addition to making Bundler work correctly under these version conflict scenarios.\n. Can you provide more of a backtrace that shows at the very least control flow through paperclip, ideally through the aws-sdk gem? It's very hard to tell what's going on given the data here. At first glance I would imagine there is some kind of routing/dns/connection error going on, perhaps a firewall that is blocking your outgoing connections?\n. Unfortunately those logs don't give enough data about where the error is coming from. Are you behind a firewall/proxy by any chance?\n. Yes, this does look related, @scaryguy. @trevorrowe's been trying to nail down the issue, but it seems to be hard to find a root cause for. I would jump in on that ticket with whatever information you might have.\n. @scaryguy I spoke with @trevorrowe and he seems pretty confident that it is not related to the paperclip issue you linked to. It seems like a server configuration issue. We did some digging and found this StackOverflow issue (below) where a user on a new Ubuntu 12.x installation had the exact same problem you are having, and it turned out to be an IPv6 configuration issue:\nhttp://stackoverflow.com/questions/16040158/rails-mailer-netopentimeout-execution-expired-exception-on-production-serve\nThis sounds like it is more likely the underlying issue. I'm going to close this issue since I'm not sure there is anything the SDK is doing incorrectly here (we don't cause OpenTimeout errors, they are only caused by not being able to open a remote connection), but feel free to reopen if further research into the OpenTimeout error is getting you nowhere.\n. You're on a very old version of the SDK that does not support regions as described in that post. If you download the latest gem, everything should work as advertised.\n. You're certainly right in that we should make sure to add version numbers to our posts to clarify these going forward. \nFor what it's worth, I would also recommend reading these related posts about the new region/configuration features in the SDK since v1.8.x which might be useful to you:\n- More region functionality: http://ruby.awsblog.com/post/TxARY2076S09CA/Working-with-Multiple-Regions\n- The aws-rb executable: http://ruby.awsblog.com/post/Tx37CB1ZX8AGBQ5/Using-the-AWS-SDK-for-Ruby-from-Your-REPL\n. I would also recommend locking your aws-sdk gem to ~> 1.0, not 1.8.0, as we release new minor versions fairly regularly with bug fixes and new features. We also blogged about best practices for locking gem versions here.\n. @rbroemeling I completely understand the error-proneness of configuration. Adding validation to configuration is something we're looking at in V2 of the SDK (/cc @trevorrowe), I think it would be a pretty useful addition.\n. I'm marking this as a duplicate of #244. Opening too many concurrent connections can cause throttling or packet loss failures that force a retry. The default retry limit for the SDK is 3, so that will easily reach the maximum if you are getting throttled or have packet loss. Either increase the retry limit or lower your number of concurrent connections, the latter being much more recommended.\n. Looks good, thanks for the contribution!\n. I'm not sure this patch is the right way to go about compatibility for a couple of reasons:\n1. AWS::Record doesn't actually ever require rails; we assume it has already been loaded if someone is making use of this class (in fact, this code is designed to be used without Rails). In the same way, we should be assuming you've loaded active_model if you need it. This ends up being a weird user story, since you don't actually know it's needed, but that brings me to point number 2:\n2. Since AWS::Record doesn't have a hard dependency on Rails, I'm not sure why this method has a hard dependency on ActiveModel. It seems like we are only using it to detect a Rails version, something AM is not needed for. In fact, having this dependency looks like a bug, since it disables non-Rails users from using the Naming module unless they've included active_model on their own (granted this model_name probably never gets called outside of Rails, so users don't see this). I would rather see us remove this dependency than add an explicit one.\nI wonder what @trevorrowe thinks about this-- but I think the path forward here is going to either be:\n1. Move this conditional require call into the Railtie hooks in lib/aws/rails.rb instead. That should be the only place where we run Rails specific code at loadtime, and that should make sure Record (for the most part) continues to work without AM (this isn't exactly true due to reason number 2).\n2. The better path: remove the ActiveModel conditional code and replace it with a more robust Rails2 vs 3 check that can also work outside of Rails. That's what it looks like the code in question is really trying to do.\nNote that a temporary workaround for Rails 4 users right now would be to put gem 'active_model' in their Gemfile when using AWS::Record.\n. > As a suggestion to your Path 2, can't we just add dependency to active_model in gemspec and always require active_model in naming.rb? I checked Mongoid and they handle it this way.\nNo, it would not be appropriate to put active_model in the gemspec. That would mean the aws-sdk would depend on a Rails specific piece of infrastructure when in reality that code will almost never get called in most cases. The dependency does not look necessary to me, and we can pull it out.\n. Just a note, the REXML fallback isn't supported and doesn't seem to be working at all right now according to #318, so adding a fallback would (potentially) be quite a bit of work. We're already looking at multi_xml in V2 of the SDK to remove the required dependency.\n. I'm not sure if V1 is the right place for this since there may be some backward incompatibility issues. Bringing @trevorrowe in.\n. V2 has a very different architecture when it comes to internals like building / parsing XML, so unfortunately we could not write this in V2 and backport.\n. Yes. We're hoping to make it available soon though, stay tuned.\n. @loopj check it out: https://github.com/aws/aws-sdk-core-ruby\n. Thanks for the patch!\n. Have you reported this issue on the Amazon ElastiCache forums? Given that this is a change to how the the low-level API operations are structured, this is probably something that should be investigated at the API layer itself. \nI'm not sure the SDKs are the right place for this kind of fix, because (a) this same patch will need to be applied in every SDK separately (and all other tools/clients), and (b) this may cause even more confusion, since all of the operations provided in the Client class should be a 1:1 mapping of service calls in the ElastiCache API. Having a new call that does not map to any of the ElastiCache API operations in our low-level client will be odd for documentation and discovery.\nI think the right approach would be to engage the ElastiCache team on the forums and see if there are any ways to make this more clear in the underlying API.\n. Thanks for reporting this. Threading in the SDK is a known issue with the way autoload is used. We recommend using eager_autoload! as discussed above. I'm going to close this, but if you have any other related issue that you think might be bugs in the SDK feel free to report them!\n. Can you provide more information about this issue? What code did you use to reproduce the 400 failure? What is the exact output you are getting?\n. Can you enable wire tracing with AWS::SNS.new(http_wire_trace: true) and show the data that you are sending to the service?\n. That log shows a 200 response. Where are you seeing the 400?\n. Unfortunately we cannot accept this patch since this strategy of conditionally building the gemspec based on the RUBY_VERSION will not work. See #330 for more information on why, but in short, RubyGems does not have conditional dependencies.\n. Can you run the query with request logging enabled in the SDK? You can do this by adding the following to the beginning of your script:\n``` ruby\nrequire 'logger'\nAWS.config(:logger => Logger.new($stdout))\nnewsletter = Newsletter.find(286)\nnewsletter_schedule_id = newsletter.newsletter_schedules.first.id\nNewsletterStats.where(:society_id=>newsletter.society.id,:newsletter_id=>newsletter.id,:schedule_id=>newsletter_schedule_id.to_i).first\n``\n. It seems like the .first is forcing the SDK to set a limit of 1, at which point our paginator kicks in because it sees that there is more data to grab. We will look into this and see exactly what is going wrong.\n. It sounds like the SDK is doing the right thing here. I would suggest posting on the [AWS OpsWorks forums](https://forums.aws.amazon.com/forum.jspa?forumID=153) to see if they can provide any help with what's going on. I would also point out the lack of error message, since services should be providing a helpful error message in these situations. Just to make sure, though, try running withAWS.config(http_wire_trace: true)` to make sure there is no error being sent back in the payload that we are not parsing.\n. @mattbailey I would also recommend posting on the forums (as suggested to @ajsharp above) as the AWS OpsWorks team listens to feedback like this.\n. @ckarbass what gem are you using that requires nokogiri 1.6?\n. That's a tough one. It looks like that gem explicitly disables nokogiri < 1.6, but I'm not quite sure why it would. The commit that changed the behavior explains that nokogiri dropped support for 1.8 (which is true), but in the very same commit they add extra support for Ruby 1.8 in the gem itself (see counter_cache.rb). It sounds like they are incorrectly forcing a 1.9+ gem if they are still supporting Ruby 1.8. \nIf it's possible, you could try using v1.4.1 of the impressionist gem, which is the last version to not have the 1.6+ requirement, according to the commit above. That would work with the latest version of the aws-sdk.\n. @dblock we've already announced this going forward in V2 of the SDK.\n. @mtparet if the copy object API call is succeeding correctly (200 response) with the correct input parameters being sent, it sounds like this would be an issue with the service itself. I would suggest raising this issue in the Amazon S3 forums and see if they have any input.\n. I totally agree with that statement, @mtparet. However in this case, we got no error information from the API call itself, so all the SDK knows is that you asked S3 to copy an object, and S3 said that it did. We rely on the service to know if your call failed, which is why you should raise this in the S3 forums to make sure they correctly return us an error when a copy fails.\n. @eniskonuk keep in mind that we are flipping the memoization strategy in V2 moving forward because of behaviors like these. The SDK should certainly attempt to send a minimum number of requests for a given call, but it's not possible to completely isolate application code from being throttled.\n. Thanks!\n. Unfortunately a handful of similar pull requests were closed-- this method doesn't work, since RUBY_VERSION is only checked when the gemspec is built, not when it is installed. RubyGems does not have conditional dependency support. See #330 and #383.\n. No problem! Thanks for the contribution either way-- we know people are itching to help us fix this :)\n. For reference you can push to the same branch without having to close a pull if you make a mistake. You can also consider pull -f if you want to rewrite your mistake :)\n. There is a get_server_certificate method on the AWS::IAM::Client class that you can look at to get the certificate.\n. You should be able to call:\nruby\ncert = iam.client.get_server_certificate(server_certificate_name: c.name)\np cert[:server_certificate][:certificate_body]\n. @trevorrowe could we not just change the part size picker to choose a number that is divisible by 16? That would at least be a temporary fix in the SDK\n. Good catch! Thanks!\n. The reason is indeed for 1.8 support. In V2 of the sdk (http://github.com/aws/aws-sdk-core-ruby) we've moved to multi_json, since we don't support 1.8 in that version, but I'm not sure it would make sense for V1.\n. Thanks for the pull request!\n. I would honestly say: use composition over inheritance. The Delegator class is much cleaner to use for this kind of thing:\n``` ruby\nrequire 'delegate'\nrequire 'aws-sdk'\nclass MyS3Class < Delegator\n  def initialize\n    @s3 = AWS::S3.new\n  end\ndef getobj; @s3 end\ndef my_convenient_method(*args)\n    @s3.buckets.to_a\n  end\nend\ns3 = MyS3Class.new\np s3.my_convenient_method\np s3.buckets.to_a # you can call S3 methods directly\n```\nI'm not sure supporting inheritance has much value, especially since composition is generally a better way to provide extra functionality to an object. That said, if it were only a few lines of code to change in order to make this work, I wouldn't necessarily be opposed.\n. I think the issue here is that if lib/ is indeed being moved around in the bundle, that will potentially break other things in the SDK that rely on relative paths from lib/. For instance, we are currently fortunate that that api configs in V1 are located inside of lib/, but in V2, these are located above lib in ROOT/apis/. This solution would not be portable to the new SDK. If we were using this structure in V1, loading of clients would have been broken too. Note that where those configs live on disk is an implementation detail, so it's still possible we might one day make this internal change to the SDK, breaking your package once again.\nIt seems that rawr has a serious flaw in the way it packages libraries if it is messing with the directory structure. Packagers should not alter the directory structure of a project, since it is not acceptable to assume that only the lib directory is used by a gem (this directory exists by convention only). Many gems rely on assets outside of lib, so relative paths are important. I bring this up because, if the directory structure had remained intact, this should not have caused a problem.\nDoes warbler alter the directory structure as well? If not, perhaps it would be wise to fix this in rawr. An easy reproduction of another failing case would be:\nlib/foo.rb:\nruby\ngemspec = eval(File.read(File.dirname(__FILE__) + '/../foo.gemspec')) # I want to load my gemspec!\n. Thanks for the contribution!\n. How often are you seeing that warning? Just once? Multiple times?\nCan you provide code that reproduces the issue? I cannot reproduce this on MRI, though I'm not sure how you are using the SDK. Can you also provide more information on when you started seeing this?\n. Are you only reproducing this in JRuby? Also can you answer whether this warning is printed once or multiple times?\nI am Loren@AWS by the way!\n. Just a note: if we vendor UUIDTools, we have to make sure to pull in the license as well.\n. Can you provide an HTTP wire trace?\nruby\neb_client = AWS::ElasticBeanstalk::Client.new http_wire_trace: true\neb_client.describe_environments\n. Can you provide a few details about how you are using DynamoDB? Are you constructing multiple DynamoDB clients?\n. Thanks!\n. The Aws constant, as opposed to AWS, is for the Ruby SDK V2, which is located at aws/aws-sdk-core-ruby (the aws-sdk-core gem), and the API docs for that package are at: http://docs.aws.amazon.com/sdkforruby/api/Aws/EC2/V20130615.html\nThe V2 SDK is still in the works, and we don't yet have higher level abstractions like #instances collections. You can call #describe_instances instead, though. Or you can use V1 until these higher level abstractions are made available. The V1 SDK is the \"aws-sdk\" gem.\n. Thanks again!\n. This is expected behavior. The high level DynamoDB abstraction does not support the 2012-08-10 API. See #427 and #430\n. @mirakui yes, V2 supports the same versions V1 supports at the low level. Note however that there are currently no high level abstractions in V2 as we continue to build out the core infrastructure of the SDK.\n. The V1 SDK does not memoize responses by default. This is something we're changing in V2. That said, you can use the memoize block in V1 to make this work:\nruby\nAWS.memoize do\n  ec2.instances.each {|i| i.some_other_attribute }\nend\nirb code:\nruby\nAWS> ec2.instances.each {|i| p i.image_id }; nil\n[AWS Core 200 0.925014 0 retries] describe_instances()  \n[AWS Core 200 0.219826 0 retries] describe_instances(:instance_ids=>[\"i-272ace49\"])  \n\"ami-1624987f\"\n[AWS Core 200 0.224155 0 retries] describe_instances(:instance_ids=>[\"i-aa8c24e6\"])  \n\"ami-1624987f\"\n[AWS Core 200 0.278458 0 retries] describe_instances(:instance_ids=>[\"i-fa4ad4ca\"])  \n\"ami-bba18dd2\"\n...\n=> nil\nAWS> AWS.memoize { ec2.instances.each {|i| p i.image_id } }; nil\n[AWS Core 200 0.471397 0 retries] describe_instances()  \n\"ami-1624987f\"\n\"ami-1624987f\"\n\"ami-bba18dd2\"\n...\n=> nil\n. The api_version parameter is explicitly locked for DynamoDB, as the high level abstractions do not support the 20120810 API version. You can see #493 for a recent pull request closed for the same reason along with other issues that explain why this cannot be supported.\n. Neither RubyGems nor Bundler should resolve dependencies to pre-release versions unless you've provided the --pre option when installing the gems. This is what happens for me when installing aws-sdk:\nsh\n~$ gem install aws-sdk\nFetching: nokogiri-1.6.1.gem (100%)\nBuilding native extensions.  This could take a while...\nSuccessfully installed nokogiri-1.6.1\nFetching: aws-sdk-1.38.0.gem (100%)\nSuccessfully installed aws-sdk-1.38.0\n2 gems installed\nSame effect with Bundler:\nsh\n~/tmp$ bundle install\nFetching gem metadata from https://rubygems.org/........\nFetching additional metadata from https://rubygems.org/..\nResolving dependencies...\nUsing json (1.8.1)\nInstalling mini_portile (0.5.3)\nUsing nokogiri (1.6.1)\nUsing uuidtools (2.1.4)\nUsing aws-sdk (1.38.0)\nUsing bundler (1.5.3)\nYour bundle is complete!\nUse `bundle show [gemname]` to see where a bundled gem is installed.\n~/tmp$ bundle update\nFetching gem metadata from https://rubygems.org/........\nFetching additional metadata from https://rubygems.org/..\nResolving dependencies...\nUsing json (1.8.1)\nUsing mini_portile (0.5.3)\nUsing nokogiri (1.6.1)\nUsing uuidtools (2.1.4)\nUsing aws-sdk (1.38.0)\nUsing bundler (1.5.3)\nYour bundle is updated!\nI would recommend always using a Gemfile.lock if you are deploying applications to a production environment. Your lock file should guarantee the exact versions of your dependencies in your app and never try to update during a deployment.\nDo you have a Gemfile.lock in your application? If so, have you verified that the 1.6.2.rc1 is not listed in your Gemfile.lock?\n. @Jimflip unfortunately it is not possible for a gem specification to opt out of pre-release versions of a dependency, so there is no way we can adjust our dependencies to solve this issue from aws-sdk itself. This is a limitation of the specification format of RubyGems. That said, even if this were possible, we would not want to disable users who explicitly wanted to test pre-release versions of dependencies, as there is no inherent incompatibility of a pre-release version from the perspective of semantic versioning. The regression you ran into could have just as easily occurred in a regular point release of the library had it not been caught in a pre-release.\nIf you are picking up pre-release versions in your production environment, I would recommend auditing your Chef recipes to ensure that it never attempts to install any gems with the --pre switch. Pre-release versions should not be installed through normal usage of RubyGems or Bundler. Using --pre was the only way I was able to install Nokogiri 1.6.2.rc1. Even gem install aws-sdk --pre would not pull in the pre-release of nokogiri, I had to explicitly run gem install nokogiri --pre. I would suggest checking for that command in your Chef recipes.\nIn fact, I would actually recommend finding a way to explicitly lock the versions of all gems you are installing in your production environment. If your Chef recipes do not currently do this, you should explicitly install all your dependencies by the same version you have tested in your development environment (Bundler is the easiest way to do this). As mentioned above, library regressions can happen in regular releases as well (and can occur in any of your libraries, not just the AWS SDK), so you want to make sure the code you run locally is exactly what you will get in production. \nI am going to close this issue as a regression in a third-party library. If you have any other questions about how you could lock your dependencies feel free to comment on this issue and we might be able to point you in the right direction. Thanks for bringing this issue to our attention.\n. @kadwanev actually, the current standard environment variables are the ones already used by the SDK. The EC2 CLI tool is already in the process of being replaced by the new unified CLI, which uses AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY (see here).\n. Have you checked the time on your machine to make sure it is synced up properly? The error you got was a time skew error, which usually means that either your machine is ~5+ minutes out of sync, or AutoScaling is.\n. @cer yep. We're discussing that right now actually. A lot of these docs are shared across our SDKs and tools, so a change here will likely be pulled into our other docs too.\n. Are you mocking your filesystem in your specs by any chance? Also, do you have a backtrace that shows where the file read / open call is coming from?\n. Support for encrypted volumes was added in f44fa4a6b0fad86a7d95f4e4b9e32e3dbccc37be and released in v1.41.0. You can see the #describe_volumes method documentation for more information. Note that as we move to V2 of the Ruby SDK, we are only making updates to the client operations. If you want to help out by creating a pull request that moves this support into the high level Volume class, we would be happy to look at it.\n. @sferik unhelpful bot is unhelpful :(\n. @sferik I didn't delete anything, maybe @trevorrowe or @awood45 did?\n. This actually seems like an issue with the AWS SDK. You can reproduce this without awesome_print:\nsh\n$ ruby -raws-sdk -e 'r=Aws::IAM::Client.new.list_users; p r.respond_to?(:to_hash); r.method(:to_hash)'\ntrue\n-e:1:in `method': undefined method `to_hash' for class `#<Class:#<Seahorse::Client::Response:0x007fdc8ba8c770>>' (NameError)\nDid you mean?  to_s\n    from -e:1:in `<main>'\nI would expect that if r.respond_to?(X) == true then I can call r.method(X) to get the associated Method object.\nIt looks like aws-sdk does actually delegate respond_to? and method_missing, but doesn't delegate existing methods on Object like #method and the like, which it should probably do as well. I wonder if it would be a breaking change to inherit Seahorse::Client::Response from BasicObject instead of Object, so that it does not accidentally swallow methods meant to be delegated to @data.\n. Just FYI you could also have used respond_to_missing?, which resolves the expectation about method()-- in fact, you should always be using that callback when overriding method_missing to get correct expectations. Ruby will fill in the details for you if you do this:\n``` ruby\nclass Foo\n  def method_missing(sym, args) sym == :bar ? 'hello' : super end\n  def respond_to_missing?(sym, args) sym == :bar end\nend\nFoo.new.method(:bar)\n=> #\nFoo.new.method(:bar).call\n=> \"hello\"\n```\nThis removes the need for any hacks (on awesome_print's side). I would expect any implementation that overrides method_missing to include a respond_to_missing? implementation too, if they want to be 2.x-friendly.\n. ",
    "petemounce": "Cool.  I had a thought just now that given that this has no dependencies on the SNS class, perhaps I should instead wrap this up into a new class AWS::SNS::Message, which can then have AWS::SNS::Message#authentic? as well as maybe some future customisations for wrapping the different types of AWS-emitted SNS' like AutoScaling notifications (where the Message field of the SNS is itself nested JSON, so that could get parsed out the box and dumped into an OpenStruct or something).\nre: testing - I don't understand why we'd need an SQS queue...?  Is it because that's the most convenient end-point to set up within the test context...?\n. Yeah, the more I think about it as I write my SnsListener, the more I'd like to have a rich model for an AutoScalingSnsMessage with methods like #event (to return :ec2_instance_launch, :ec2_instance_terminate etc), #instance_id, etc.\n. I've made a bit of progress on the AWS::SNS::Message class that I need for my SnsListener; I'll push it in a while.\n. It's ready in and in production for us, but I need some permission before contributing it back.  Working on that.\n. I don't know why this pull-request got merged - I didn't think I had permission to do that, and I didn't mean for that to happen at all since 46 supercedes this one.\n. When I run bundle exec rake spec it bombs out with \"command line too long\".\nWhen I run rspec spec I get an error relating to the spec files not knowing about AWS::SNS::Message constant - I assume I've messed up somewhere along the line with includes.\n. Ping @trevorrowe can you please help me get my tests green, so I can try to go further and write some integration tests?\n. @trevorrowe Oh, damn, I've pulled in way more commits than I actually wanted to there.  I think I rebase'd when I should have merged, or something like that.  Would you like me to try again in a fresh pull-request?\nI fixed the failing specs that I had; see b6768ee and 18a5765.\nStill to-do:\n- integration tests as mentioned earlier\n- fuller coverage for the FromAutoScaling originator mixin\nComments welcome!\n. @trevorrowe if I re-create the pull-request, will I need to submit another contributor agreement?  Or, shall I just reference the new pull-request from this one and indicate that the new one supercedes this one?\n. @trevorrowe done.  Apologies for the mess :(\n. This replaces pull-request #46.\n. @trevorrowe ping?\n. @trevorrowe travis-CI wiring is awesome.  I've fixed a couple of failures; hadn't appreciated that the gem is backwards compatible (duh).\nAfter that, though, I now don't understand why this build is red:\n- I'm guessing it's the pending test - but this is pending in each of 8.1 through 8.4\n- I lack a linux box (at the moment; this is a great excuse to build a VM) to repro on - is it possible to download maybe a Vagrantfile to reproduce one of the travis-CI build-runner machines onto, locally, so I can repro using that...?\n. @pas256 I need a bit of help making the Travis CI build pass, and after that, figure out what to do about supplying credentials to some end-to-end integration tests without putting them into source control.\n. @lsegal I don't understand why 3 of the 4 jobs didn't care about the pending test, though, so I assumed it wasn't that.\nre: env - AWS_ACCESS_KEY_ID and AWS_SECRET_KEY, right...?\n. Speaking of which, the pending test is pending for want of not rolling my own version of retryable.  At justeat, I did robertsosinski/retryable#4; does that look like an acceptable thing to introduce, or is there already a similar implementation internal to aws-sdk?\nre: env - thanks, saw that, couldn't remember for sure the variable names without checking.\n. @trevorrowe awesome.  Thank you!  :smile: \n. ",
    "JasonLunn": "Looking forward to having this facility working for subscription and notification messages.\n. ",
    "kml": "I have problem with missing s3_client and I thought that using this method could help.\nI'm processing messages from queue. For each message a Job object is created and perform is invoked.\nSometimes I'm getting errors because there is no client. I think that this code can ilustrate my problem:\n```\nrequire 'retryable'\nclass Job\n  class MissingS3Client < StandardError; end\n  def initialize(..., options)\n    #...\n    retryable(tries: 3, sleep: 3, on: MissingS3Client) do\n      set_bucket(options[:bucket])\n    end\n  end\n  #...\n  def perform\n    # @bucket...\n  end\nprivate\ndef set_bucket(bucket_name)\n    # get an instance of the S3 interface using the default configuration\n    service = AWS::S3.new\n    raise MissingS3Client, \"There is no S3 client\" unless service.client\n    @bucket = service.buckets[bucket_name]\n  rescue\n     AWS.add_service('S3', 's3', 's3.amazonaws.com')\n     raise\n  end\n end\n```\n. @trevorrowe I'm using JRuby with TorqueBox, so maybe that's some thread-safety problem (one shared client - many threads).\nSimplified version of my code is here: https://gist.github.com/2508894\nI'm uploading ~1kk small images daily (thumbs).  During last 4 hours I've got 1 occurence of MissingS3Client exception (logged after set_bucket's rescue), but sometimes I've got thousands of them. AWS.add_service('S3', 's3', 's3.amazonaws.com') seems to help.\nCan I help somehow? I can log something after rescue, but I don't know what would be usefull.\n. I've started digging and I've found what was the problem.\nIt's old jruby-openssl gem.\nFix:\nbundle update jruby-openssl\nGemfile.lock diff:\n-    jruby-openssl (0.7.6.1)\n-      bouncy-castle-java (>= 1.5.0146.1)\n+    jruby-openssl (0.9.4)\n+      bouncy-castle-java (>= 1.5.0147)\n. ",
    "m-badov": "I'm having the exact same issue with Ruby 1.9. When uploading from multiple threads, client is sometimes nil. If it helps, it seems that the issue occurs precisely when a worker thread is used for a second time. Additionally, this problem does not happen with Ruby 1.8, which uses 'green threads'.\n. ",
    "bhuga": "Is this the right place to post pull requests, or are submissions preferred via another route?\n. ",
    "awood45": "This has been stale for several months, and we do not use HTTParty anymore. I'm going to be closing this, but there are a couple options as far as bringing this change up to date:\n- You can update this pull request for the current state of V1, and our current dependencies. We can review that PR. This may take longer for us to get to, as we are currently focused on new features for V2.\n- You can write this feature as a plugin for V2 of the SDK, or as a third-party gem you can later integrate with V2. If you'd like to go this route and have any questions, please feel free to reach out.\n. This has been stale for several months. I'm going to be closing this, but there are a couple options as far as bringing this change up to date:\n- You can update this pull request for the current state of V1, and our current dependencies. We can review that PR. This may take longer for us to get to, as we are currently focused on new features for V2.\n- You can write this feature as a plugin for V2 of the SDK, or as a third-party gem you can later integrate with V2. If you'd like to go this route and have any questions, please feel free to reach out.\n. Hello, and sorry for the rather late response.\nWe're currently working full speed on V2 of the SDK, and looking at higher level abstractions for that version only. As such, we would look at a pull request for this if you would like to take a crack at it, but we are unlikely to get around to this soon.\n. This has been stale for several months. I'm going to be closing this, but there are a couple options as far as bringing this change up to date:\n- You can update this pull request for the current state of V1, and our current dependencies. We can review that PR. This may take longer for us to get to, as we are currently focused on new features for V2.\n- You can write this feature as a plugin for V2 of the SDK, or as a third-party gem you can later integrate with V2. If you'd like to go this route and have any questions, please feel free to reach out.\nAdditionally, if you choose to resubmit this pull request, please consider breaking individual features out into individual pull requests.\n. Yes, you're correct that it's built on top of #put_object in the same manner.\n. We are currently working on Version 2 of the AWS SDK for Ruby. At this point, we are not planning to add additional higher level abstractions to V1 of the SDK, and are instead focusing on new features for V2.\nIf you would like to submit a pull request for this feature in V1 of the SDK, we would be happy to review it.\n. This issue has been stale for several months, so I am going to close it now. As we are currently working full speed on Version 2 of the SDK, we are not currently looking at adding further high level abstractions to V1 of the SDK.\nIf you would like to submit a pull request for the feature in V1, we are happy to take a look at it.\n. I took a look at this today. When the transfer encoding is 'chunked', then the body is an empty string, not nil. I have added a handler and test case which covers this case.\nThanks for reporting this issue! The fix will go out with the next release.\n. Thank you for bringing this to our attention. I've written a change that will resolve the issue.\n. I can't reproduce this on my end.\nCan you please tell me which version of the SDK you're using, and which version of Ruby you're using?\n. I am unable to reproduce the issue with those Ruby versions and the 1.11.3 code. I'd like to see the full rspec output to eliminate the possibility of test execution order effecting the outcome.\n. Closing - please feel free to reopen this or file a new issue if this recurs.\n. At this time, we have no plans to add high level abstractions to V1 of the SDK. We are happy to review a pull request for this feature if you would like to write one.\n. Bug fix release 1.14.1 is now out.\n. I'm having a bit of trouble reproducing this. I need a bit of extra information to try and debug:\n-Which version of the AWS SDK are you using?\n-Which version of RubyGems are you using?\n-What OS are you running this on?\nRubyGems should fail when you try and activate the aws-sdk gem without Nokogiri, how did you manage to load the gem without Nokogiri present?\n. Indeed, thanks for bringing this to our attention.\nI'm taking a look now, sorry for the delay in getting back to you.\n. Changes now in master branch, will go out with next release.\n. Sorry for the delay - I'm picking this up now.\n. I'm submitting a change that updates the documentation and marks ObjectMetadata#[]= as deprecated. The added documentation should make the risks of the []= function especially clear.\n. Going out with the next release.\n. I can see that this is missing from the Amazon S3 API, I'm looking into this.\n. This will go out with our next release, but is currently in our master branch.\n. Per Loren's comments, we're not going to merge this pull request.\nAs mentioned in the related issue, we are actively discussing how to safely release the nokogiri version lock without breaking our remaining 1.8 customers. Once we have come to a conclusion there, I will share the outcome in that thread.\n. We have no plans at this time to write any further high level abstractions for Amazon DynamoDB in version 1 of the SDK. As we approach the release of version 2 of the SDK, we will be providing the ability to create higher level abstractions in separate gems. If you want to be a part of the process of creating this and other abstractions, follow along on the blog as we release more details on the new SDK.\nIf you would like to implement this for V1 of the SDK, we're happy to look at pull requests.\n. This is a correct assessment.\nThe :expires value in your second line sets when the object is no longer cacheable:\nhttp://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectPUT.html\nhttp://aws-docs.integ.amazon.com/AWSRubySDK/latest/AWS/S3/S3Object.html#move_to-instance_method\nThe expiration date referenced is a bucket lifecycle attribute. For example, objects in a given bucket can be configured to be deleted after a year, rotated into Amazon Glacier after another fixed timespan, and so on. You can read more about that here:\nhttp://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketPUTlifecycle.html\nFeel free to reopen if this isn't clear or if you have more questions.\n. We'd be happy to take a look at a pull request for this. Thanks!\n. Closing per previous comments/lack of activity. We will still be happy to take a pull request on this. Feel free to reopen if you have additional information on this.\n. We're in the process of considering high level abstractions to include with the Version 2 release of the AWS SDK for Ruby. Thanks for letting us know what's important to you in that regard.\nFor the current version of the SDK, we're happy to consider a pull request for this feature.\n. Closing per previous comments/lack of activity. We will still be happy to take a pull request on this. Feel free to reopen if you have additional information on this.\n. You can open a new issue with the feature request, explaining what you're looking for, yes. We aren't adding anything to Version 1 anymore, however, which was the topic of this issue.\n. Hi Adam,\nHave you had a chance to test your script with wire tracing? Feel free to check back with us here if the wire traces bring up any additional questions.\n. The -v flag on aws-rb will allow us to see the raw requests and responses being sent over the wire. Seeing this can help reveal the exact cause of your issues.\n. Quick check, are you using the 1.16.0 release? I'm checking in case I made a mistake on the release.\n. I just managed to reproduce the issue myself. Taking a look now.\n. Testing a fix now.\n. A fix is in place at the tip of master. Closing this now, if you see any further issues or have any questions please feel free to reopen.\n. Final Note - I've released version 1.16.1 of the SDK, which includes this fix.\nThanks again for reporting this.\n. If you're having issues with Amazon SNS topics not working as expected, I'd recommend you post on the Amazon SNS Discussion Forum or contact AWS Support.\n. Thanks for catching this issue, and especially for sending a fix!\n. With a cursory glance at the ELB API I am not seeing any tagging operations. Could you clarify for me a bit more what you're thinking of doing?\nOr, if the pull request is a small one, just take a stab and we can review/iterate on it.\n. Have you tried AWS::EC2::Client#unassign_private_ip_addresses?\nAs for adding it to the higher level abstractions, we're currently focused on development for V2 of the SDK - we are happy to take a look at a pull request if you'd like to take a crack at adding this.\n. Can you share with us the exact error you're getting? We're not able to replicate this on our end, we'd need the wire logs from aws-rb -v to look into this any further.\n. We have managed to reproduce the issue - using your PR change can resolve a particular bundler issue when using our SDK.\nWe're a bit concerned with forcing all of our users into what is essentially a lock into Nokogiri 1.5.10. Our old dependency was 1.4 or greater, so we may have users still on 1.4 who would be force-upgraded by this change. There have been known issues with specific versions of Nokogiri, so some customers may be broken by a forced upgrade. We are discussing these risks, and alternatives such as a ~> 1.5.0 dependency. Watch this space.\n. Can you tell us which version of the SDK you are using, and, if known, which version you were using before?\n. Thanks for sending this to us!\n. This issue has been inactive for over a year, and V1 is end of life. Happy to continue to look at this, given all the effort in place so far, if we get more information. For now, closing. Again, feel free to reopen if more information comes in.. I think upgrading to version 2 of the SDK is the best path forward for that. We've dealt with these issues in V2 in a more thorough way. Plus, V1 is essentially deprecated, so I don't see non-critical work happening on it going forward.\n. I'd encourage them to upgrade, if there are any blocking issues we're happy to look in to them.\n. Thanks for bringing this to our attention - we indeed missed some input parameters in hand coding this operation. V2 of the SDK handles this by pulling in a service description file, but the issue remained in V1.\nI have created a pull request to resolve the issue and will keep this space updated.\n. I don't think this is likely to be an SDK issue, there are two possible scenarios I can see:\n1. Signature Version 4 has a time-based element, is a significant amount of time passing from the link generation to running it on the iPad?\n2. Is it possible the Safari is overwriting a header? It may be helpful if you could compare wire trace logs between the iPad and the other browsers.\n. So I can't replicate this using Ruby 2.1.1 and aws-sdk 1.38.0 (or the latest state of the master branch).\nWhat I would suspect is that, possibly, the api_config file for the EC2 client version you're trying to load is corrupted in some way.\nCan you look at the following file in your environment, and let us know if it looks obviously corrupted, or if not, attach it to this thread?\n/home/vagrant/.rvm/gems/ruby-2.1.1/gems/aws-sdk-1.38.0/lib/aws/api_config/EC2-2013-08-15.yml\n. Can you please provide some more context?\nA wire trace of the call you were attempting would be useful, as would more levels of the stack trace.\nThanks!\n. Trevor and I were looking at this today apropos to another issue outside of GitHub.\nIt looks like Net::HTTP is returning no content when yielding to a block, and we're not yet sure why.\nA workaround would be to load the file in full, rather than yielding - it isn't ideal, but it will get you working again while we investigate. Otherwise, we will keep you informed in this issue.\n. I've changed the issue title for our own tracking - if your code is NOT yielding to a block, please let me know and I'll investigate that as a separate issue. I am admittedly making an educated guess here.\n. We've seen multiple other people reporting a very similar issue in recent days, I do not think that your network speed is causing this error.\n. We were able to resolve this issue on our end by using the 'excon' gem as our HTTP handler, using the process defined in Trevor's post near the end of issue https://github.com/aws/aws-sdk-ruby/issues/241\nThis is a workaround you can use while we do our remaining investigative work and file a bug against Net::HTTP as appropriate.\n. This issue should now be resolved at the tip of the master branch.\n. Thanks for the extra notes here.. You can add this documentation to your pull request if you would like, the\ndoc strings are in the source code.\nOn Thu, Jul 10, 2014 at 1:49 PM, Alex Farrill notifications@github.com\nwrote:\n\nThanks in advance for this, I still think the API documentation should be\nupdated, e.g.:\nhttp://docs.aws.amazon.com/sdkforruby/api/Aws/DynamoDB/V20120810.html\nOptions Hash (options):\n:api_version (String) \u2014 default: '2012-08-10' \u2014 The API version to use for\nthis service. Valid values include:\n2011-12-05\n2012-08-10\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/aws/aws-sdk-ruby/pull/573#issuecomment-48662324.\n. If you'd be more comfortable with me pulling down your PR and adding the\ndoc updates that is also fine.\n\nOn Thu, Jul 10, 2014 at 1:56 PM, Alex Wood awood45@gmail.com wrote:\n\nYou can add this documentation to your pull request if you would like, the\ndoc strings are in the source code.\nOn Thu, Jul 10, 2014 at 1:49 PM, Alex Farrill notifications@github.com\nwrote:\n\nThanks in advance for this, I still think the API documentation should be\nupdated, e.g.:\nhttp://docs.aws.amazon.com/sdkforruby/api/Aws/DynamoDB/V20120810.html\nOptions Hash (options):\n:api_version (String) \u2014 default: '2012-08-10' \u2014 The API version to use\nfor this service. Valid values include:\n2011-12-05\n2012-08-10\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/aws/aws-sdk-ruby/pull/573#issuecomment-48662324.\n. Hi Enis,\n\n\nWe do not have plans to launch support for additional services in V1 at this time. You should use V2 of the SDK for Cognito support. You can use V1 and V2 together in the same project, so you could consider adding V2 for Cognito operations, and continuing to use V1 for other services until you're able to migrate fully to V2.\nLet us know if you have any questions about how to do any of this.\n. I'm taking a look at this now.\n. In Ruby 1.8.7, I am not aware of a reliable way to determine the home directory. We also don't want to go down the somewhat error prone path of poking around your file system to try and guess.\nThat said, a NoMethodError is not the right error to throw, so I'm going to craft an ArgumentError in this case that calls out the correct approach, and better handle this code path in the default credential provider. Watch this space.\n. This change is now in the master branch, and will go out with the next release.\nThanks for letting us know about this!\n. Sorry for not updating in this thread, but we have this feature in developer preview now.\nCheck out aws-record and give us feedback!\nThere's also an introductory blog post that you may find interesting.\n. I can't recreate this issue in 2.0.42.\nCan you tell me a bit more about what you are trying to do?\n. Have you taken a look at Presigned Post? If you're doing this via any kind of form, that could be a better choice. In any case, I can't identify anything that has changed and I seem to be able to successfully run presigned requests.\nIf you can isolate a small code example that breaks, that could help.\n. Thanks for this, I was able to reproduce and I'm taking a look.\n. I have a possible fix written which passes in your case, but there are a few other cases I want to check in review. Will push up my changes to a branch and PR.\n. Per our earlier discussion, Hashes and Arrays worked fine from the first change, the second use of #read was isolated to Set types only. This change fixes the appropriate test and code.\n. Can you provide a stack trace? I can't reproduce this error on my end.\n. This looks like a failure when trying to pull from the instance profile credentials, which I'm still not able to recreate. If you can tell me what version of the SDK you're using, that could be helpful. Taking a look in the meantime.\n. We can see that the stub_responses client is accessing the credential provider chain, which shouldn't happen at all. We're looking in to that.\n. Thanks for the update! If you do track this down to a particular MRI version, we'd love to hear about it in case it comes up again.\n. From the SDK perspective, we're returning the service response correctly.\nI think you have the right understanding, however. My guess is that, if that call returns nothing when you're specifying an association, the subnet uses the default route table.\nI'm going to raise this with our documentation team and follow up.\n. Hi, just to follow up: If no explicit subnet is returned, you can assume that it will be using the default route table. I believe they will be updating their docs to more clearly reflect this behavior.\n. Thanks for bringing this to our attention.\nV1 of the SDK is approaching end of life, and we are unlikely to do further releases except in the case of critical issues. Because of that, I'm closing this issue without applying a patch.\nIf you have any questions or confusion about that method, feel free to reopen and ask.\n. I found at least one example of this in Version 2 code, but for reference what version of the Gem are you using?\n. So, the one case in V2 where we rescue Exception is a valid case - we are cleaning up our connection pool, then re-raising the error. There shouldn't be any trapping of signals.\nCan you let me know the version of the Gem you're using, or any other details about what you are doing when this issue occurs?\n. On an initial pass the same appears to be true in V1 of the Gem as well.\n. Looks good to me, nice catch! \ud83d\udc4d\nI would be inclined to agree that this is the best way rather than special casing -1, but I'm open to changing my mind.\n. I can see where there could be confusion here, will take a look at how we can word this.\n. Thanks for bringing this to our attention. I've added the \"Default\" label to the table headers directly, and that will be reflected in the documentation after our next release.\n. Can you provide a wire trace for this? That should tell us if the service is returning these values, or if something is going on with the SDK.\n. The AWS CLI command you're using is a higher-level abstraction included in the CLI that wraps the #add_job_flow_steps API: http://docs.aws.amazon.com/sdkforruby/api/Aws/EMR/Client.html#add_job_flow_steps-instance_method\nWe don't have plans currently to add this hand-written abstraction to the Ruby SDK, but can look at it as a feature request.\nThat said, in my personal experience, you shouldn't have much trouble translating that to an API call, and if you do, feel free to let us know here as I have some experience using the API directly.\n. We go into more depth in a blog post here: https://ruby.awsblog.com/post/Tx2T9MFQJK7U74N/AWS-SDK-for-Ruby-and-Nokogiri\nThe short version is that you need to specify a Gemfile.lock for 1.8.7 support, as we made the decision to remove our own lock so that Ruby 1.9+ users could roll Nokogiri forward after 1.8.7 support was dropped by the Ruby core team.\nFeel free to reopen if anything in the post doesn't work for you.\n. The signature version hasn't been removed (the defaults and s3 signer error handling logic have, more details in UPGRADING.md).\nThe presigner in V2 of the SDK has always used Signature Version 4. As Trevor indicated, it would be difficult to properly support signature versions other than 4 for presigned URLs.\n. Update/Clarification: V1 of the SDK is deprecated (and has been for some time in a de-facto state), though we will release any future security updates that are needed. Official announcements will be posted later in the appropriate channels.. Going to close this out as this is definitely deprecated. Any future releases will only be for security issues when and if they arise.. Picking this up, there is a tiny difference in the canonical request we generated versus the one generated by the server.\nOn our end, this is in the request:\ncontent-length:0\nOn the server end, they expected:\ncontent-length:\nI'm going to investigate this further, see what the spec indicates should happen.\n. It could be helpful if there is any way you can capture a wire trace of this happening, to confirm that we are correctly signing the request (to determine if the problem is client side or server side).\nTo do this:\nruby\nclient = Aws::CloudSearchDomain::Client.new(http_wire_trace: true, endpoint: endpoint)\nI realize this is potentially very noisy in your logs, so I understand if you can't do it, but it's helpful if you can.\nI also think that the intermittent nature of this problem implies that adding retries around your call for this error should be effective. Let me know if it is not.\n. The related issue makes me suspect strongly that there is a deeper issue here (they had a non-zero content length that still had a blank content-length value expected server side).\n. So, if content-length is added after signing, then your request should work. Locally, my default handler order has content-length headers added after signing, which is why I can't recreate this locally.\nWhat I'd like to see is your handler ordering and priority, to see if your handler order differs from mine. Can you give me the output of:\nruby\nclient = Aws::CloudSearchDomain::Client.new(opts) # However you normally construct it.\nclient.handlers.entries.each { |h| puts \"#{h.handler_class}: #{h.step} #{h.priority}\" }; nil\nclient.handlers.to_a\nThis will help me debug.\n. Can you rerun and provide the other output too? Sorry, need both to confirm a suspicion.\n. Okay, I have some more clarity on this issue.\nWhat you're seeing is a two-stage error:\n1. Your initial request generates a 500 Error, for whatever reason. The SDK recognizes this and attempts to retry.\n2. On the second request only, we additionally sign the content-length header, which generates the signature error. We believe this to be a server-side issue.\nI don't know what is causing the 500 Error to begin with, but if you're adding retries around that successfully, then that's a viable workaround. If you're consistently seeing 500 Errors, then you should separately reach out to the service team via AWS Support to investigate that.\n. Glad to hear. I'm continuing to track this issue to resolution overall, but it sounds like we can close this one. Feel free to chime in issue #1014 (almost the exact same thing, different cause of the initial 500) if you have more issues.\nAlso, if the frequency of this is a problem for you, I definitely encourage you to reach out to the service team via AWS Support. If you're seeing this, you're probably not the only one, and your debug information has been helpful.\n. I managed to recreate this using a ~5 GB file. I'm looking in to it, but for reference here is my stack trace:\nundefined method `match' for nil:NilClass\n/Users/alexwood/.rvm/gems/ruby-2.1.1/gems/aws-sdk-core-2.1.30/lib/aws-sdk-core/plugins/s3_request_signer.rb:212:in `region_from_location_header'\n/Users/alexwood/.rvm/gems/ruby-2.1.1/gems/aws-sdk-core-2.1.30/lib/aws-sdk-core/plugins/s3_request_signer.rb:194:in `detect_region_and_retry'\n/Users/alexwood/.rvm/gems/ruby-2.1.1/gems/aws-sdk-core-2.1.30/lib/aws-sdk-core/plugins/s3_request_signer.rb:157:in `handle_region_errors'\n/Users/alexwood/.rvm/gems/ruby-2.1.1/gems/aws-sdk-core-2.1.30/lib/aws-sdk-core/plugins/s3_request_signer.rb:150:in `call'\n/Users/alexwood/.rvm/gems/ruby-2.1.1/gems/aws-sdk-core-2.1.30/lib/aws-sdk-core/plugins/s3_request_signer.rb:33:in `call'\n/Users/alexwood/.rvm/gems/ruby-2.1.1/gems/aws-sdk-core-2.1.30/lib/aws-sdk-core/xml/error_handler.rb:8:in `call'\n/Users/alexwood/.rvm/gems/ruby-2.1.1/gems/aws-sdk-core-2.1.30/lib/aws-sdk-core/plugins/s3_request_signer.rb:124:in `call'\n/Users/alexwood/.rvm/gems/ruby-2.1.1/gems/aws-sdk-core-2.1.30/lib/aws-sdk-core/plugins/s3_redirects.rb:15:in `call'\n/Users/alexwood/.rvm/gems/ruby-2.1.1/gems/aws-sdk-core-2.1.30/lib/aws-sdk-core/plugins/retry_errors.rb:87:in `call'\n/Users/alexwood/.rvm/gems/ruby-2.1.1/gems/aws-sdk-core-2.1.30/lib/aws-sdk-core/plugins/s3_md5s.rb:33:in `call'\n/Users/alexwood/.rvm/gems/ruby-2.1.1/gems/aws-sdk-core-2.1.30/lib/aws-sdk-core/plugins/s3_expect_100_continue.rb:21:in `call'\n/Users/alexwood/.rvm/gems/ruby-2.1.1/gems/aws-sdk-core-2.1.30/lib/aws-sdk-core/plugins/s3_bucket_dns.rb:31:in `call'\n/Users/alexwood/.rvm/gems/ruby-2.1.1/gems/aws-sdk-core-2.1.30/lib/aws-sdk-core/rest/handler.rb:7:in `call'\n/Users/alexwood/.rvm/gems/ruby-2.1.1/gems/aws-sdk-core-2.1.30/lib/aws-sdk-core/plugins/user_agent.rb:12:in `call'\n/Users/alexwood/.rvm/gems/ruby-2.1.1/gems/aws-sdk-core-2.1.30/lib/seahorse/client/plugins/endpoint.rb:41:in `call'\n/Users/alexwood/.rvm/gems/ruby-2.1.1/gems/aws-sdk-core-2.1.30/lib/aws-sdk-core/plugins/param_validator.rb:21:in `call'\n/Users/alexwood/.rvm/gems/ruby-2.1.1/gems/aws-sdk-core-2.1.30/lib/seahorse/client/plugins/raise_response_errors.rb:14:in `call'\n/Users/alexwood/.rvm/gems/ruby-2.1.1/gems/aws-sdk-core-2.1.30/lib/aws-sdk-core/plugins/s3_sse_cpk.rb:18:in `call'\n/Users/alexwood/.rvm/gems/ruby-2.1.1/gems/aws-sdk-core-2.1.30/lib/aws-sdk-core/plugins/param_converter.rb:21:in `call'\n/Users/alexwood/.rvm/gems/ruby-2.1.1/gems/aws-sdk-core-2.1.30/lib/seahorse/client/plugins/response_target.rb:21:in `call'\n/Users/alexwood/.rvm/gems/ruby-2.1.1/gems/aws-sdk-core-2.1.30/lib/seahorse/client/request.rb:70:in `send_request'\n/Users/alexwood/.rvm/gems/ruby-2.1.1/gems/aws-sdk-core-2.1.30/lib/seahorse/client/base.rb:207:in `block (2 levels) in define_operation_methods'\n/Users/alexwood/.rvm/gems/ruby-2.1.1/gems/aws-sdk-resources-2.1.30/lib/aws-sdk-resources/services/s3/multipart_file_uploader.rb:110:in `block (2 levels) in upload_in_threads'\n. Essentially, we can confirm that Trevor's gut instinct on this was correct. We're failing when we cannot identify the bucket region from a header. We'll move on to proposing a fix now.\n. As a workaround while we develop the fix for this, you can get this working by using a SigV4 client:\nruby\ns3 = Aws::S3::Client.new(signature_version: 'v4')\nIn my script reproducing your error, this solved the problem.\n. That's pretty much the full fix too, we're going to make SigV4 the default behavior for all regions with S3. The remaining work has to do with ensuring we don't create any regressions in doing so, but the functionality for your case will be identical.\nStill, leaving this open until the root cause is resolved and the fix released.\n. We've pushed the fix for this, and it will go out with the next release. Please let me know if you see a recurrence (unlikely if the workaround has worked for you).\n. We're taking a look at this, may not be related to the previous issue.\n. I'd recommend, if possible, using Signature Version 4 signing in these cases and seeing if that helps to resolve the issue. We're working right now on a 2.2.0 release that will make this the default behavior, but you can also do this right now:\nruby\nclient = Aws::S3::Client.new(signature_version: 'v4')\n. I'd strongly suspect that our most recent release (2.2.0) will resolve this problem. Can you confirm?\n. Do you have any information about those breakages?\n. Let me know if there's any information you can share - I haven't heard of breakages yet, would love to jump on any issues people are seeing immediately.\n. @BanzaiMan do we have any updates on 2.2.x helping this issue? I know in the other thread we seemed to be near a resolution to the Python packages issue, but have other customers seen this issue resolve with 2.2.x?\n. @BanzaiMan should I leave this open, or do we think this has brought a resolution to the problem?\n. @BanzaiMan - It's looking like this update has likely fixed the issue, and that this is just a hard one to confirm (understandably). I'm going to close this for now, but I'm still monitoring the TravisCI issue, and please feel free to reopen or to open a new issue if there is any sign that this issue persists (or has recurred in yet another form).\n. I'll look in to this.\n. So, I think it's safe for you to catch both - there's a small semantic difference, in that BucketAlreadyOwnedByYou only occurs when you're trying to create a bucket that exists && is yours, unlike the other error which is for bucket names otherwise taken.\nI'll work on the documentation update in the meantime, because I agree that could be spelled out in the docs for clarity.\n. Going to close this for now - please feel free to reopen if this doesn't work for you.\n. Added this to the feature request backlog.\n. I've added the feature request for this.\n. This is still available to you in V2. Try this:\nruby\nclient = Aws::SES::Client.new\nresp = client.send_email(options)\nresp.context[:request_id]\n. Glad to hear. From V1 to V2 the way we store some metadata has changed, I think that's what you were seeing. Let me know if you have more questions.\n. If you take a look at the documentation, you will see that the response is actually a struct.\nThe length is the number of response attributes, which can vary from operation to operation. Example:\nruby\nresp = autoscaling.describe_lifecycle_hook_types\nWill return: => #<struct Aws::AutoScaling::Types::DescribeLifecycleHookTypesAnswer lifecycle_hook_types=[\"autoscaling:EC2_INSTANCE_LAUNCHING\", \"autoscaling:EC2_INSTANCE_TERMINATING\"]>\nruby\nresp.length # => 1\nWhen you're accessing the [0] value of that array, you're fetching the same value you'd get if you called:\nruby\nresp.auto_scaling_groups\nWhen you're accessing the [1] value of that array, you're fetching the next_token part of the response, which is nil (indicating the response wasn't paginated.\nIf you have multiple auto scaling groups, you can array access them like so:\n``` ruby\nresp.auto_scaling_groups[0]\nresp.auto_scaling_groups[1]\nor\nresp.auto_scaling_groups.each { |group| puts group.auto_scaling_group_name }\n```\nHope this helps, we document the response structure for every operation in our documentation.\n. Our default credential provider will only wait for 1 second for the EC2 metadata service to return credentials before failing. If you're experiencing this often, you should explicitly construct the EC2 credential provider AWS::Core::CredentialProviders::EC2Provider with retries and a longer timeout. That should resolve this issue.\n. Looks good, the failing test is obviously unrelated and predates the change.\n. Thanks!\n. The conflict is probably just the changelog from the last release. You can rebase or just pull the changelog out (and we can add the item on merging).\n. Pulling this down, the waiter doesn't work as intended (it will bail on the first 400 response).\nI'm debugging this now.\n. Nevermind, it's working, I made a brain fart while testing. Pulling this in manually due to the merge conflict on CHANGELOG - thanks!\n. Thank you for your pull request. We're not taking new features for V1 of the SDK anymore, as it is on the deprecation path and we're doing bug fixes and security patches only.\nIf you'd like to add features to V1, I would recommend forking and vendoring the package.\nI would also recommend looking at V2 of the SDK, which you can use in the same project as V1 with the gem structure we have created. See the README for more information.\n. I'm able to recreate this, thanks for the report. Will take a deeper look.\n. I've found the issue, we just need to ensure that we've run require 'set' before we attempt to create a ParamFilter. This happens by coincidence when you create a service client (or do various other things), but should just be explicit in the file. Writing a quick fix that will go out with the next release.\nIf you need a fix sooner, just ensure you do a require ahead of the config call, like so:\nruby\nrequire 'aws-sdk'\nrequire 'set'\nAws.config.update({\n  log_formatter: Aws::Log::Formatter.new(Aws::Log::Formatter.default.pattern.chomp),\n  log_level: :info\n})\nSorry for the oversight, and thanks for reporting this.\n. If you can provide a wire trace, that would probably make it clear, but I have a pretty strong guess.\nLooking at the waiter definition, we are waiting until Aws::EC2::Client#describe_snapshots returns 'completed' for your given snapshot. It is possible that the console is using a different process than we are.\nNevertheless, this appears to be correct behavior.\n. You're seeing a difference in behavior when using the SDK because the EC2 API frontend sometimes caches the result of API calls. In the intervening period between the console showing the snapshot as complete and the waiter returning a successful result, you're seeing cached API call results.\nSo, when you see the snapshot showing as complete in the console, that is correct and the snapshot is complete. However, until #describe_snapshots returns a success value to the SDK, it won't terminate the waiter.\n. Unfortunately there is not a way that I am aware of. I can provide feedback to the EC2 API team about the behavior, but from an SDK perspective there's not much we can do. However, I can say that once the waiter returns successfully, the snapshot is complete.\nWhat operations are you trying to do that use the completed snapshot? I can try to help think of a workaround for you, if there is one.\n. I'll take a look at this.\nKeep in mind that at the end of the block, messages are automatically deleted, so if you're accessing from multiple places, there could be a contention issue (not that your code indicates this per se).\nI'll try to see if I can reproduce this.\n. I'm relieved to hear that!\nI'd personally recommend using a separate queue for any testing.\nAn option does exist to not automatically delete messages after the block completes, but figuring out how to apply that properly for testing would be a lot of complexity. Best to have a \"test queue\" which can, for example, have test messages sent to it to test whatever behavior you want to trigger.\nLet me know if you have any more questions.\n. Thanks for this, looks interesting!\nWe're looking at some ways to include examples like this, so we may take you up on this soon. Going to leave this open for a bit, because right now there isn't a perfect place fit for this yet. Watch this space.\n. We now have a developer guide that you can contribute to, if you would like.\nWe would welcome example contributions to that guide, and would be happy to help with crafting a PR. Thanks!\n. The presigner enforces the limit because that sigv4 limitation would otherwise cause errors (you could see this if you tried to manually generate a presigned request with a different limit.\n. It does look like the pagination definition document is missing, that's where you're running in to problems. I'll look in to this.\n. This is going to take some extended investigation on our end, since the API response doesn't give us all of the values we need to write a complete pagination definition easily.\nIn the meantime, you can check pagination manually. Consider the following (against the IMDB sample data):\nruby\nAws> resp = client.search(query: \"star\", query_parser: 'simple', return: \"title\", cursor: \"initial\")\n[Aws::CloudSearchDomain::Client 200 0.367792 0 retries] search(query:\"star\",query_parser:\"simple\",return:\"title\",cursor:\"initial\")\n=> #<struct Aws::CloudSearchDomain::Types::SearchResponse\n status=#<struct Aws::CloudSearchDomain::Types::SearchStatus timems=5, rid=\"requestid=\">,\n hits=\n  #<struct Aws::CloudSearchDomain::Types::Hits\n   found=85,\n   start=0,\n   cursor=\"cursorresponse\",\n   hit=\n    [#<struct Aws::CloudSearchDomain::Types::Hit id=\"tt1411664\", fields={\"title\"=>[\"Bucky Larson: Born to Be a Star\"]}, exprs=nil, highlights=nil>,\n     #<struct Aws::CloudSearchDomain::Types::Hit id=\"tt1911658\", fields={\"title\"=>[\"The Penguins of Madagascar\"]}, exprs=nil, highlights=nil>,\n     #<struct Aws::CloudSearchDomain::Types::Hit id=\"tt0086190\", fields={\"title\"=>[\"Star Wars: Episode VI - Return of the Jedi\"]}, exprs=nil, highlights=nil>,\n     #<struct Aws::CloudSearchDomain::Types::Hit id=\"tt0056687\", fields={\"title\"=>[\"What Ever Happened to Baby Jane?\"]}, exprs=nil, highlights=nil>,\n     #<struct Aws::CloudSearchDomain::Types::Hit id=\"tt0120601\", fields={\"title\"=>[\"Being John Malkovich\"]}, exprs=nil, highlights=nil>,\n     #<struct Aws::CloudSearchDomain::Types::Hit id=\"tt1674771\", fields={\"title\"=>[\"Entourage\"]}, exprs=nil, highlights=nil>,\n     #<struct Aws::CloudSearchDomain::Types::Hit id=\"tt2141761\", fields={\"title\"=>[\"The Blackout\"]}, exprs=nil, highlights=nil>,\n     #<struct Aws::CloudSearchDomain::Types::Hit id=\"tt0258153\", fields={\"title\"=>[\"S1m0ne\"]}, exprs=nil, highlights=nil>,\n     #<struct Aws::CloudSearchDomain::Types::Hit id=\"tt0397892\", fields={\"title\"=>[\"Bolt\"]}, exprs=nil, highlights=nil>,\n     #<struct Aws::CloudSearchDomain::Types::Hit id=\"tt0069945\", fields={\"title\"=>[\"Dark Star\"]}, exprs=nil, highlights=nil>]>,\n facets=nil>\nIf you see that the number of hits + the start index is less than the \"found\" part of the response, you know you have a truncated response and can continue to use the cursor to make additional queries.\nLeaving this open for a bit because I'd like to build in an answer to this problem.\n. From a pagination perspective, our spec doesn't (yet) have a way to handle the structure of the responses consistent with the pagination pattern.\nIt is possible, it's just going to be a matter of either writing a CloudSearch-specific customization, or enhancing the pagination spec.\nIn the meantime, you can do pagination, you just have to pass in a cursor or increment the start parameter by the number of objects in the hit array. Unfortunately, that does mean that you'd have to roll your own #each method.\nTracking this for now as a feature request.\n. Added this to the feature request backlog. Please feel free to reach out if you have any questions or difficulties using the response to page through your search results.\n. Actually, I would like to investigate that bit. The way the lack of a pagination spec is presenting isn't too helpful.\n. No, I think there's a bug in here to investigate, it shouldn't be treating this like a PageableResponse with no pagination config defined.\nDefining the pagination spec is a feature request, the bug is why it's presenting in this manner.\n. What appears to happen is that Aws::Plugins::ResponsePaging is a default plugin, whether or not pagination is defined for a given client. This is why PageableResponse appears in the ancestor list.\n. Aws::Pager meanwhile, is nil in this case, so we'll always hit this particular piece. That's the core issue.\n. I have a proposed patch for this that I'm getting reviewed. Hoping to push that tomorrow to go out with the next release.\n. This fix will go out with the next release, and won't make non-pageable operations appear pageable.\n. Can you provide an example to recreate this, or a stack trace?\n. That makes sense, this looks essentially like multipart download functionality.\nLooking at this as a feature request now.\n. Closing this issue, I've added the feature request and it looks like we have a workaround in the meantime (the feature would just be a high level wrapper around this same workaround).\nFeel free to reach out if you have any questions or further issues.\n. I'll take a look at this.\n. It looks like we're not updating the port correctly when applying the HTTP scheme, which means the request is signed against port 443, but run against port 80, which will be a mismatched signature. Working on a patch.\n. Could you give me an example of what you mean?\n. This is curious, because on the tip of master the presigned URL generated is using an HTTP scheme, but appears to be using port 443 when signing. I'll take a closer look as I construct the patch.\n. Looking a bit deeper, I'm suspecting this is an artifact of how the Ruby URI object is implemented. When we substitute the scheme value, it's still an \"HTTPS\" type:\n#<URI::HTTPS:0x123123123123 URL:http://blahblahblah>\nAs such, when you pull the 'port' from this, it's still 443. Not sure if this changed over time since you seem to see this working in the past.\n. I don't think printing a warning when no filters are passed is appropriate, because in some cases that's exactly what the caller is intending to do.\nSimilarly, while you have the ability to filter in a way that only describes your images using the owners: [\"self\"] option, changing the default behavior in any way is a backwards incompatible change, a customization, and produces behavior contrary to what the documentation describes.\nI can forward your feedback to the Amazon EC2 team, but I don't see a change here that I can make. I'd recommend using the available filters to help constrain the size of the response to your calls.\n. I don't see an option within the API to do that. I can give feedback to the Amazon EC2 team, but from a client perspective there's not much to be done.\n. You can work around this by making ranged requests to the large object and reassembling the parts.\nHere's one example:\nruby\nclient = Aws::S3::Client.new\npart = \"bytes=0-4999\"\nresp = client.get_object(bucket: bucket, key: key, range: part)\nThis will get 5000 bytes of the object, and you could do this (with larger blocks) sequentially for the full size of the object.\nThat's the workaround, I'm also working on putting something like this together as an SDK feature that compliments multipart uploads.\n. That sounds like a good idea, I'll see if there's any reason not to do that. If you'd like, feel free to open a pull request for this as well.\n. I've added this to the feature request list, let me know if you have any other questions or ideas!\n. The example you gave will work, yes. (It will return empty responses if you don't further specify response content - not necessarily valid, but stubbed.)\nWe appreciate the suggestion, and are currently working on a developer guide that will include information about testing.\n. Generally looks good, I'd like to make the change around default attempt counts before we can merge. While you're adding that option, could just as easily make the wait period configurable as well.\n. Can we add a couple tests around the options being consumed, for regression testing purposes? Otherwise looks good.\n. I think we're about set to go here. Last comment above, then I'll pull in to test and merge. Thanks!\n. That is unrelated to your work, that's right. It has to do with the 2.2.0 release.\nThanks!\n. Can you tell me a bit more about the operations you're performing where these issues are happening? It will help quite a bit to be able to reproduce what you're seeing.\n. Where are you running these? Are you using EC2 instance profiles?\n. Are you using the Aws::AssumeRoleCredentials class? If so, are you using EC2 Instance Credentials to call this, or static credentials?\nWe can take this discussion off GitHub if that's a sensitive question, but I'd like to investigate if this implies a bug in AssumeRoleCredentials.\n. I'll take a look at this with the new information - especially if this is intermittent it would usually indicate credential expiration, but I'll try and reproduce.\n. I can see the frustration with the error names - it can vary by service, and if there were a way to model this in a unified way (for example, a common ancestor exception class) that could be helpful. I can explore if there is a sensible way to do that.\nI'm going to explore with the service teams why these sessions are expiring earlier than you're expecting. I'll share whatever I can find.\n. Can you tell me a bit more about how you're caching the credentials on your end? Expiration of the credentials should be giving you a different error.\n@johnathanludwig - what error did you get for the case where you say \"These sessions definitely are intermittently expiring sooner then they should. I was in the middle of testing with one that was working and then it expired with 13 minutes remaining.\"? Was it these assorted signature errors, or a token expiration error?\n. So, if you're getting this brand of error is is most likely that your access key and secret key do not match.\n(Disclaimer: I am not a Redis expert.) It seems possible that, since you're accessing each component of the credential separately, that you could have race conditions in your caching. I would be curious to see what happens if you serialized the 3 credential components into a single stored object, and deserialized it together.\nIf that fixes the problem, then a synchronization issue is the culprit.\n. Glad to hear we might be on to something - please let me know if you have any more questions, happy to help further if I can.\n. Could you tell me a bit more about the request you're making? Would help me give a more complete answer.\n. I think I see what you're getting at - it would help if you could give me more detailed reproduction steps to create the error on my end (I can spin up an ES cluster of my own as needed to do this).\n. I think this is related to #929 and #1014 - investigating.\n. So, if content-length is added after signing, then your request should work. Locally, my default handler order has content-length headers added after signing, which is why I can't recreate this locally.\nWhat I'd like to see is your handler ordering and priority, to see if your handler order differs from mine. I'll figure out a code snippet to get that in your example, but you could also try this since it should use the same config:\nruby\nclient = Aws::ElasticsearchService::Client.new(opts) # However you normally construct it.\nclient.handlers.entries.each { |h| puts \"#{h.handler_class}: #{h.step} #{h.priority}\" }; nil\nclient.handlers.to_a\n. Based on other output I've seen, don't worry about my last ask. We believe that what you're seeing is a server-side issue around signing, specifically around the content-length header, and are continuing to investigate.\n. Pull request #1026 would resolve this issue by preventing any signing of the content-length header, even before this is fixed server side.\nThat fix will go out with our next release.\n. Can you provide a stack trace for this call? I would suspect you're not getting this value back from the service, as I don't know of any SDK customizations that would do this.\n. Just need to see what's going over the wire. You can do this by setting the http_wire_trace parameter when creating the client. So:\nruby\nrds = Aws::RDS::Client.new(http_wire_trace: true)\nrds.describe_db_snapshots(db_instance_identifier: dbinstance)\nFeel free to trim out identifying details, but my guess is that we aren't altering the service response in any way.\n. I am not sure off-hand, but that explanation sounds plausible. I can forward that question to the RDS team.\n. You're correct in observing that the API is returning the time that the snapshot creation completed. Nothing has changed on the SDK side, we're just presenting the result of the API call to the service. You could try repeating the call until the snapshot creation is completed or fails (which sounds like a new waiter that could be added).\nClosing this now, let me know if you have more questions.\n. The client is thread-safe, so if you share a single client across threads you will get request parallelism. Within a single thread, the client will block on the request completing, so you should be using threads to get parallelism.\nFeel free to let me know if you have more questions!\n. Can you give me a code snippet to recreate the issue?\n. No problem! It will be helpful as well if you can provide a wire trace in case it is not reliably reproducible. You can do this by adding the :http_wire_trace parameter to client construction.\nruby\nclient = Aws::RDS::Client.new(http_wire_trace: true)\n. Let me know if you find any more info about this, I'd be curious to dig in to this more.\n. Can you please explain a bit more about what you're trying to do?\n. Looking at the CLI source for this functionality, the ask appears to be for client-side decryption of EC2 passwords as a plugin.\nI'll add this as a feature request, and would also be happy to take a pull request for this.\n. Thanks for reporting this, I'll try and reproduce.\nWhat will help as well is if you can let me know what version of the SDK you're running, and if you could provide a wire trace. To get a wire trace, just add the :http_wire_trace parameter to client creation:\nruby\nclient = Aws::EC2::Client.new(http_wire_trace: true)\n. What Trevor showed should work - if not, can you give a full reproducing call?\n. This looks good, thanks!\n. Thanks!\n. Do you have any alarms configured for that metric, currently? What's an example of an alarm you're trying to see? (I'll try to recreate on my end.)\n. Can you include a wire trace for this? Try:\nruby\nclient = Aws::CloudWatch::Client.new(http_wire_trace: true)\nclient..describe_alarms_for_metric(namespace: namespace, metric_name: metric_name)\nclient.describe_alarms\nI'd like to separate service behavior from any SDK code.\n. No problem - feel free to leave this open if there's anything else we can answer.\n. The answer to your exact question is that the object is a representation of the STS API model, and doesn't have any higher level features decorated in.\nWhat I am thinking, however, is that there is a feature request to have a credential provider that uses STS in this manner, or that can easily consume this response to create an Aws::Credentials object.\nWhat are you trying to do? Asking to craft the right feature request.\n. We do provide an abstraction around AssumeRoleCredentials, but it may require some tweaking to handle a plain #get_session_token call. I'll toy around with this over the weekend.\n. Size matters because many operations require that the size is known in advance. What are you trying to do?\n. Size is required from IO-like objects used in our parameters. I don't know of any counter-examples to this. If you're trying to do something where using plain IO without size makes sense, reach out so we can consider the feature request.\n. I'm unable to recreate this, is it possible that you're having transient network issues on your end? Where are you making this call from?\n. How are you trying to do this with the fake-s3 gem? The presigned post method is primarily constructing headers, and does have a url parameter you can set.\n. @andretanguy Could you please open a separate issue for that? That looks like a possible separate bug.\n. @hakunin - I just merged pull request #1020 which plumbs endpoints through presigned posts created via S3 Objects as well as S3 Buckets within the resource interface.\nIf you're using our S3 resource abstraction, this should be all you need. If you're creating presigned post object directly, you should use the :url parameter provided.\nGoing to close this now, feel free to reach out if this isn't clear or doesn't fully cover what you need to do.\n@andretanguy I'd still like to dive in to what you're seeing as a separate issue.\n. That's an interesting feature request. I'll add it to the feature request backlog, and would look at a pull request for this if you want to take a swing at it.\n. I've added the feature request, though as an alternative to consider, you could also save these URLs somewhere for reuse. Depends on what your use case is, exactly.\n. Looks potentially related to #929 - I'll try and reproduce with this, it could help to get a wire trace as well by adding http_wire_trace: true to your client construction.\n. Thanks, that's very helpful. I'm communicating with the service team to see why we're calculating signatures differently, and I'll add this in. I'm not sure it's a client issue yet, but either the client or the service is making signature mistakes here.\n. I'm having trouble reproducing this on my end though, can you show me a bit more about the request you're making?\n. So, if content-length is added after signing, then your request should work. Locally, my default handler order has content-length headers added after signing, which is why I can't recreate this locally.\nWhat I'd like to see is your handler ordering and priority, to see if your handler order differs from mine. Can you give me the output of:\nruby\nclient = Aws::CloudSearchDomain::Client.new(opts) # However you normally construct it.\nclient.handlers.entries.each { |h| puts \"#{h.class}: #{h.step} #{h.priority}\" }; nil\nclient.handlers.to_a\nThis will help me debug.\n. Sorry, I had a bug in one line:\nruby\nclient.handlers.entries.each { |h| puts \"#{h.handler_class}: #{h.step} #{h.priority}\" }; nil\n. Thanks - this is curious because by this ordering, content-length should strictly be added after signing, which is the workaround for this issue. Looking more in to this.\n. Okay, I have some more clarity on this issue.\nWhat you're seeing is a two-stage error:\n1. Your initial request generates a 500 Error, for whatever reason. The SDK recognizes this and attempts to retry.\n2. On the second request only, we additionally sign the content-length header, which generates the signature error. We believe this to be a server-side issue.\nWhat is curious to me is that you're seeing consistent 500 errors, despite adding retries around the second error. That indicates to me a second issue, which I can only speculate on, around your use or the server's reading of your expr param. I'm going to see if I can recreate that.\nI'd encourage you, separately, to reach out to the service team via AWS Support to see why you're getting persistent 500 errors.\n. @traviskroberts I'm working on the signature error still, but I have some more information for you.\nThat 500 Error you're getting should actually be a 400 Error - it looks like the function \"log\" is not supported, only \"ln\", \"logn\", or \"log10\". This is why you're getting this error consistently rather than intermittently.\nOnce the service fixes the signature problem, you'll still see 500 errors. Once this is correctly translated to a 400 error, the request still won't work as-is.\nLet me know if this workaround helps you use expr without errors.\n. I have some examples for you, using the IMDB sample data. Let me know if this helps you avoid the 500 that causes this:\n``` ruby\n500 Error\nclient.search(query: \"star\", return: \"title\", expr: \"{ 'score': 'log(_score)' }\", sort: \"score asc\")\nSuccess\nclient.search(query: \"star\", return: \"title\", expr: \"{ 'score': 'log10(_score)' }\", sort: \"score asc\")\nclient.search(query: \"star\", return: \"title\", expr: \"{ 'score': 'ln(_score)' }\", sort: \"score asc\")\n```\n. Right, the signing error comes on the retry. If you look at the wire trace, a 500 Error (which really should be a 400 Bad Request, but that's a fix in progress) happens first.\nSo, if you can replace \"log\" in your call with \"ln\", \"logn\", or \"log10\" as appropriate, you should no longer have this issue.\n. Referring to the \"log distance\" bit in your request above.\n. I'm not sure what would be causing that, as that goes well into the service usage domain.\nHopefully this will help: http://docs.aws.amazon.com/cloudsearch/latest/developerguide/defining-expressions-in-requests.html\nI can try to dig around, but admittedly this is past my knowledge of CloudSearch's internal workings.\n. Thank you for your pull request. We're not taking new features for V1 of the SDK anymore, as it is on the deprecation path and we're doing bug fixes and security patches only.\nIf you'd like to add features to V1, I would recommend forking and vendoring the package.\nI would also recommend looking at V2 of the SDK, which you can use in the same project as V1 with the gem structure we have created. See the README for more information.\n. What version of the SDK are you using? Most likely, you're on an older version of the SDK that doesn't have that part of the client model.\n. Which version, exactly, of the SDK are you using? If you run gem list, what does it show for the SDK? #add-tags support was added in version 1.51.0. You wouldn't see an API version bump for some services unless there was a breaking change, such as new required parameters.\n. Relevant Change: https://github.com/aws/aws-sdk-ruby/commit/40544cb7c826dafdd684634b44769edbfa86e069\n. I can't recreate this on my own version of 1.64.0, which has the add_tags function. I did notice in your stack trace that you're running this in a Rails app - what is in your Gemfile.lock for that project? Are you possibly locked on an old version?\n. I'll leave this open while you poke around, but I'm pretty confident that this is a problem with your gem bundling for that project.\nTry, for example, running the aws-rb command, and then run:\nruby\nelb.client.add_tags\nThis should give you a parameter validation error, but will show that the API is present.\n. Thanks for the report, I'll try and reproduce. What would also help is if you can provide a stack trace of the error you're seeing.\n. Looks good, nice catch, and thanks!\n. I wouldn't call this erroneous, in the sense that the code works just fine (it would be invalid JSON, but not invalid Ruby). Stylistically though this is a cleaner look.\n. I'll go with the best practice on this one. Thanks!\n. Correct. It's a common misconception due to the fact that JSON will break, but it's a best practice to exclude the comma.\nAlso, I appreciate the thanks, hope the workshop was useful!\n. Related to #1012 \n. Looks good with a first pass.\n. Integration tests pass locally as well, and the behavior seems to match. Thanks!\n. I'm actually not able to reproduce this. Can you give me your stack trace from this error, and maybe specific parameters (if possible) that create this issue?\nAlso, which version of the SDK are you using?\n. Try modifying your code like so:\nruby\ns3 = Aws::S3::Resource.new(http_wire_trace: true)\nresult = s3.bucket(app_name).object(backup_file_name).upload_file(backup_file_location)\nGiven your custom endpoint situation, and common request, I suspect that a server-side issue is likely. A wire trace would help to confirm.\n. Without further information, or a reproducing example against a public S3 endpoint, this looks like an issue with the endpoint you're trying to call. The error you're getting is coming from the server, it's cast as Aws::S3::Errors::InvalidArgument because of the error code returned by the server.\nI'd recommend at this stage that you reach out to the endpoint provider and cut a bug report with them. Providing the wire trace may help in that regard. Feel free to reach out if we can be of any more help.\n. The resource model doesn't have a way to do this within its current design.\nYou can use the low level API Aws::S3::Client#list_objects, which returns both the common prefix array and the first level objects in the response.\n. For reference: Aws::S3::Client#list_objects.\n. I can't recreate this off-hand. Can you give me a repro example?\n. When I try to recreate this, the request appears to go over the wire, rather than this being a client side validation. I'm looking in to this more.\n. I can tell you that this does work when I pass just the name, rather than a full ARN. For example:\nruby\nclient.get_instance_profile(instance_profile_name:\"aws-opsworks-ec2-role\")\n...works, while this doesn't:\nruby\nclient.get_instance_profile(instance_profile_name: \"arn:aws:iam::123456789000:instance-profile/aws-opsworks-ec2-role\")\nWhat operation are you calling? I can attempt to repro there and see if there is a doc issue.\n. Next thing would be to provide a stack trace, if you can.\nruby\nec2 = Aws::EC2::Resource.new(http_wire_trace: true)\nec2.create_instances(opts)\nLet's see what this generates. I'm mainly interested in confirming that we craft the client request correctly with your particular options, and then we can try to recreate at the client level.\n. Feel free to redact anything you need to, the structure is more important than the exact content (probably).\n. No problem - can you please link to the issue you've filed with chef-provisioning-aws? I'd be interested to see where the confusion might be to see if there is something we can do better. Thanks!\n. Thanks for the link - if down the road it looks like there may be an SDK issue here, or even maybe a service issue, feel free to reach out again.\n. There's a failing test here, essentially because the client call within expects the :secure parameter to not be present.\n. I've pulled down and confirmed that this doesn't break our integration tests around presigning.\n. I like the idea, but I think this can be changed to be more generally applicable. I'm going to pull this down, modify it, and push.\n. Related to #703 \n. Current plan is to not support this (the related issue has expanded discussion), but could add this as a feature request and see how much interest there is.\n. We've discussed this a bit further, so here's a bit of an expansion on the discussion.\nWe're not going to add this to the ~/.aws/credentials file. The AWS region is not a credential item, and arguably the fact that the CLI loads the region from the credentials file is an implementation bug that they cannot now remove without causing a break for some customers.\nHowever, I can see that, with multiple discussions around this feature request, that there is a demand for something like this. We're exploring the idea of some sort of shared configuration file, and how we could do that without risking breaking behavior for any customers using existing ways to pull in configuration. Not sure what that will look like, exactly, yet (the details of avoiding breaking behavior with this are a bit tricky), but this is a discussion we can have.\nGoing to close this since we don't intend to support this functionality, but I hear you on the feature request.\n. No worries, I think that's how we're looking at it. There are other problems with supporting everything in the ~/.aws/config file, but that's part of why we need to discuss on our end how we can support a config file.\n. I haven't seen this before either. Try creating your client with the :http_wire_trace option added to confirm my suspicion that this error is coming from the service. We'll go from there.\n. Any update on this? Is this still happening for you?\n. Thanks for the report, I see what you're getting at.\nI'm going to work on a reproducing case so that I can test a proposed fix.\n. I'm not sure what you're asking here - I don't see any mention of the SDK in your stacktrace. How are you using the SDK?\n. I don't see anything here that relates to the SDK, and this issue has been silent for a while. Closing.\n. Feel free to reopen if there is an issue here to take a look at.\n. Thanks for the report - I see a commit I'm suspicious of, give me a moment to confirm.\n. This will be fixed in the next release. I'll be looking in to the mistake made on our end that allowed this regression to happen. The regression was introduced in 2.2.4, so version 2.2.3 will work for your code (as well as 2.2.7, when it is released).\n. Are you trying to download a very large object? I am not sure about all the timeouts your connection could be exposed to, but you may want to take an alternative approach if you're downloading a very large object.\n. This has been inactive for three weeks, so I'm going to close this.\nIf you'd like to discuss, happy to talk about possible causes of this and ways to work around them. We also have an open feature request to support multipart downloads.\n. Assuming you're referring to the SQS QueuePoller. Thanks for the report, I'll take a look at it.\n. I'm actually not sure how to re-word this, and I'm open to suggestions.\nWhat the docs currently say is that when the message processing block terminates, the message is deleted automatically. Throwing :skip_delete is a way to prevent this behavior if you wish for the message to be returned to the queue.\nThis does indeed imply that a long running poller will receive the message again, but in the context it feels clear to me.\nThat said, I'm interested to hear what details you feel are missing so that I can improve the docs.\n. Closing this, but if you have more thoughts on what details are missing I'm happy to take a look.\n. Thanks for the report, I'll take a look at the mismatch.\n. It looks like this error is coming from the service, based on a test with http_wire_trace: true on the client. Investigating.\n. Sorry, I think I see the confusion now. I believe that the AWS enum value represents AWS Lambda, based on reading around the documentation. I'll pass on the feedback that this isn't very clear.\nLet me know if this fixes the issue for you.\n. Honestly, I don't see that documentation either. I'll try and get an answer from the service team about how to do what you're trying to do.\n. I'd also encourage you to contact AWS Support about that.\n. Thanks for sharing! We are looking at developer guides, so sharing this is definitely useful!\n. Thanks for the report - just to help me reproduce, can you give me some code snippets of how you're attempting to decrypt?\n. Thanks for the info - I'm working on a reproduction now so that I can properly test a fix.\n. Will revisit this after the holidays, the full fix seems to be somewhat involved. Thanks again for the report!\nIf you end up patching this in the meantime and want to start a pull request, happy to help on that route as well.\n. 100% understood, of course.\n. I'm able to reproduce this, so your key is fine. I'm working on a fix, and I'll be able to confirm against my own setup once I can get that working as well.\n. Thanks for the report and details, will take a look at why this is happening.\n. Okay, pretty clear on things now, and can certainly see that the block is no longer being hit anywhere.\nI'm looking in to the right fix now.\n. I'm testing this now.\n. Looks good once the incorrect matcher is fixed.\n. Thanks!\n. It is there, under placement.\nFor example, this is what I see:\nruby\nec2 = Aws::EC2::Resource.new\nec2.instances.first.placement.availability_zone #=> \"us-east-1c\"\nSo, if you have an instance resource, run this to get the availability zone:\nruby\ninstance.placement.availability_zone\n. Resource abstractions are a part of the AWS SDK for Ruby V2, which has the Aws root module.\nIf you're using V1 of the SDK, it would have the AWS root namespace, but you'd use a top level class like AWS::S3 for the equivalent behavior.\nIt sounds like you're creating new code. I'd HIGHLY recommend using V2 of the SDK, as V1 is being imminently deprecated.\n. Going to close this, as it looks like there isn't an issue here.\nIf you need to use V1 and you'd like to see how to use Amazon S3, the docs are here.\nIf you are trying to use V2 and you'd like to see how to use Aws::S3::Resource, those docs are here.\n. Resource abstractions are new to V2 - if you're using V1, you'd be using the AWS::S3 class, rather than Aws::S3::Resource.\nI'd recommend upgrading to V2, because V1 is on the deprecation path and isn't receiving new features. You can also upgrade to V2 incrementally, rather than all at once. Just add an aws-sdk-resources dependency, and you can upgrade incrementally by moving code over to the Aws namespace. More advantages of V2 are explained on the blog.\n. I think there is some clarity that can be added to the documentation here.\nThat hash allows you to set the region globally, but the region you pass as an ENV variable is bound until a client is created. For example, I see this in my local environment:\nruby\nENV['AWS_REGION'] # => \"us-east-1\"\nAws.config[:region] # => nil\nAws::S3::Client.new.config.region # => \"us-east-1\"\nI can see where, if you expect the global config to be write and read, that this could be confusing.\n. Thanks for doing this, looks like a good start. I'm going to review through this after the new year, and I'm happy to help with getting this ready to merge in.\n. I'm going to take a deeper look at this now.\n. I can confirm this is passing all tests for us. Reviewing code again now.\n. This looks good to me and passes our existing tests - I would like to see unit tests though. Let me know if you need any guidance on this.\n. Very sorry for the delay - Trevor had code review comments he wanted to add.\n. Thanks for this - I agree that starting Ruby 2.3 test coverage is a good idea.\nIt does look like the Travis build is failing, however, so I'm going to look in to that before I can accept the PR.\n. We're discussing this on our end. Some of these configuration files were meant to be CLI-only, but we are looking to what we can and should support above and beyond just shared credentials (which was the original intention).\n. Additional configuration supported in #1178 \n. #1178 has been merged, this will be supported with the next release, which will be version 2.4.0.\n. Thanks for this!\n. This is an interesting feature request, will definitely take a look at this.\n. Adding to feature request backlog. Will also take a PR for this if you want to take a crack at it.\n. @basex A lack of waiters just means that none have been created yet, and that there is no waiters JSON file. We can still take PRs, for example, to add Route53 waiters. You'd just need to create the file in the pattern of other waiter files.\n. I'm unable to reproduce this. A couple questions:\n- Which version of the SDK are you using?\n- What is the value of BUCKET? Is it an Aws::S3::Bucket object?\n. Closing for inactivity. Feel free to reopen if you have additional information.\n. This is an interesting catch! I'm going to reproduce on my end and see what the right behavior should be. Tentatively looking at this as a bug.\n. I am able to recreate this in the following way:\nruby\nOj.default_options = {symbol_keys: true}\nAws::S3::Resource.new\nIs this how you're setting your key symbolization in Oj, or are you doing this in a different way?\n. I actually just wrote that and was about to send the PR. One moment...\n. We've merged in the fix, to go out with the next release.\n. I can't reproduce this currently (your example works for me)...a full stack trace may help to see if there's something being plumbed up the stack, but I'll keep trying to reproduce.\n. Closing for inactivity. Feel free to reopen if you have more information, such as a stack trace.\n. So, if I understand you correctly, you're relying on the signature to show that the cache-control header is not modified by your customer?\nIn any case, I'll re-check that it needs to be on the blacklist, although our direction is moving towards a whitelist of headers in the future, potentially.\nI think there's a feature request here to whitelist headers for signing. I think it's fair for you to say: \"Sign this header, I understand that I'm now responsible for ensuring it is not modified in transit to the server, and that there will be an error if it is.\"\n. Adding to feature request backlog. Will also take a PR for this if you want to take a crack at it.\n. I've added review notes to that PR, and we'll work on getting that in.\n. Pushed the fix, will go out with the next release.\n. I can't reproduce an error, here is what I'm seeing with the latest version (using aws.rb):\nruby\nAws> client = Aws::IoTDataPlane::Client.new(region: \"us-east-1\", endpoint: \"https://#{iot.describe_endpoint.endpoint_address}\")\n[Aws::IoT::Client 200 0.67704 0 retries] describe_endpoint()\n=> #<Aws::IoTDataPlane::Client>\nAws> client.config.sigv4_region\n=> \"us-east-1\"\nAws> client.publish(topic: \"foo\")\n[Aws::IoTDataPlane::Client 200 0.786854 0 retries] publish(topic:\"foo\")\n=> #<struct Aws::EmptyStructure>\nCan you give me a failing example to work off of? Your original failing example also appears to work fine for me.\n. That makes sense, I'm able to recreate that. Going to create a separate tracking issue since this may go beyond IoT.\n. Let's talk about a unit test strategy later today - most of the rest of our tests try to be unit test agnostic, and as you say, the global config change is more or less inevitable (an attempt to not leak configuration would likely just result in assuming the default behavior is always :symbolize_names being false, although that's a likely safe assumption.\n. Can you tell me a bit more about your issue? I'm happy to roll back the second phase of yesterday's fix if there is a regression in behavior we missed, but this wouldn't be the fix.\nPlease open an issue so we can pin down what regressed.\n. Any further information on this? I'm very curious about your use case because that regex should not be triggered for any endpoints that contain a region, only \"global\" endpoints.\n. So for IOT, you should be correctly getting your actual region, because setting the region is required in the client configuration. Can you open an issue with a failing code example?\n. Looks good on a first pass, I want to take a deeper look into querystring_param before merging.\n. Thanks! Happy to look at more of these too, if you find more warnings.\n. LGTM\n. LGTM, one comment on documentation.\n. On the first point, you can generate the docs from source by checking out the repo and running:\nbundle install\nrake docs\nThat will produce the same documentation, offline.\nAs for rubydoc.info, unfortunately that process doesn't support plugins, and so they will be unable to generate matching documentation.\n. Going to close this - feel free to let me know if you have any other questions about this.\n. This is an auto-generated file, so we can't take changes to it. In any case, this change wouldn't change any behavior.\nYou are right that data.iot isn't a real endpoint, and that each endpoint actually contains an identifier. Because of this, our Aws::IoTDataPlane clients require that you specify an endpoint explicitly.\nIf you've run into issues with that in our client, please open an issue. Thanks!\n. Good catch, thanks!\n. There are a couple things going on here, I'll break it down one at a time.\nFirst, IoT data plane uses an endpoint unique to you, which is the reason that we require you explicitly define an endpoint.\nConsider the following:\n``` ruby\nNote that you do not need to explicitly define an endpoint.\niot = Aws::IoT::Client.new(region: \"us-east-1\")\nThrows an ArgumentError\nAws::IoTDataPlane::Client.new(region: \"us-east-1\")\nValid\nclient = Aws::IoTDataPlane::Client.new(\n  region: \"us-east-1\",\n  endpoint: \"https://#{iot.describe_endpoint.endpoint_address}\"\n)\n```\n. The second part is a bug between the server and client that I am currently looking at. To help illustrate, let's expand on that last example:\nruby\niot = Aws::IoT::Client.new(region: \"us-east-1\")\nclient = Aws::IoTDataPlane::Client.new(\n  region: \"us-east-1\",\n  endpoint: \"https://#{iot.describe_endpoint.endpoint_address}\",\n  http_wire_trace: true\n)\nclient.publish(topic: \"foo\")\nYou'll see something like this:\n-> \"HTTP/1.1 200 OK\\r\\n\"\n-> \"content-type: application/json\\r\\n\"\n-> \"content-length: 2\\r\\n\"\n-> \"date: Tue, 19 Jan 2016 17:26:10 GMT\\r\\n\"\n-> \"x-amzn-RequestId: 0827dce3-d9ef-4463-b1f1-40a06d0ead44\\r\\n\"\n-> \"connection: Keep-Alive\\r\\n\"\n-> \"\\r\\n\"\nreading 2 bytes...\n-> \"OK\"\nread 2 bytes\nWe try to parse the response of OK as JSON, and that's where you get the error. The fix is going to be to make a customization for this service that explicitly checks for this and crafts the appropriate response.\n. We've taken a different approach on this than a customization, but we've confirmed a fix for this. Will go out with the next release.\n. No - V1 of the SDK is only being changed for major bugs or security vulnerabilities.\nHowever, do note that you can migrate to V2 piecemeal, so if you want to use V2 only for your EC2 operations, even moving over one at a time, you can do that.\nFor example, if your Gemfile includes V1 like so:\nruby\ngem 'aws-sdk', ~> '1.0'\n...you can add V2 to your project thusly:\nruby\ngem 'aws-sdk', ~> '1.0'\ngem 'aws-sdk-resources'\nSince the major versions have different namespaces (AWS for V1 vs Aws for V2), you won't have any naming collisions.\nHope this helps, feel free to reach out if you have more questions about this, or about upgrading.\n. Thanks for checking back on that! Glad things worked themselves out quickly.\n. Working on a repro now.\n. It actually looks like this error is coming from the service itself. Something that will help: Can you give me a wire trace for the working and non-working calls?\nTo do this, add http_wire_trace: true to your client construction.\nThis will help me differentiate between client changes, service issues, and user error. Feel free to redact anything sensitive from the wire trace output, or if that's difficult, you can send it to me over email at alexwood@amazon.com\n. Thanks for that - a contributing factor does seem to be the API version bump from CloudFront.\nI'm going to dive in a bit more to what changed with the service, and see if it's possible there is a client issue. I'll also reach out to the CloudFront team to see what's happening on their side.\n. Adding the \"bug\" tag since, with no change in here noted as backwards incompatible, there's likely a bug either with the client or the service.\n. We're investigating your issue with the service team now. In the meantime, you'll need to use SDK version 2.2.12 or earlier. The new API version in question was introduced in version 2.2.13.\n. Unfortunately, not much to report. However, you should be able to work around this by altering the response from #get_distribution_config, if this is the issue I think it is. If you're using ACM, remove from the :viewer_certificate portion of the call to #update_distributions the portions about the :iam_certificate_id.\nLet me know if that helps, if not, another wire trace would be helpful.\n. Thanks for the additional info, I'm passing this along to the service team.\n@matiaskorhonen - does this workaround help for your case as well?\n. I've worked with the service team and you should no longer see this problem. I've tested with the code example at the top of the thread and it is now working properly.\nClosing this, but please do not hesitate to reopen or to file a new issue if this recurs or if you're still having issues.\n. I'm not seeing this in the AWS CLI either, can you link me to what you're referring to or show me an example?\n. For clarity, these are not available in the CLI either. These functions do not currently have an API endpoint and are only available in the console.\nIf you can explain your use case, I'm happy to pass it on to the DynamoDB team for you.\nClosing this since there's nothing to do on the SDK side, but I'll still keep an eye on this and pass on your use case.\n. Presigned post and presigned url are somewhat different in purpose.\nPresigned post helps you build forms that can post directly to S3.\nPresigned URLs perform the same request build & sign process used by the SDK, but doesn't send the request - rather it returns to you a URL that represents the request we would have made.\nWhat are you trying to do?\n. Help me understand what your use case is in a bit more detail, because as far as I can tell, the client should already provide what you need.\nIf you're looking for a :content_type option, I think that :response_content_type, which is available in #get_object, should be what you need. If not, some more details about what exactly you're trying to do, and what exact behavior is differing between the Presigned URL and opening the object in the console, could help.\n. No activity on this issue for three weeks now, going to assume that the existing functionality worked. Feel free to reopen or create a new issue if you have more information.\n. The bug appears to be the fact that, for Aws::IoTDataPlane in particular (or any service where we require specification of an endpoint by the user in general, so possibly Aws::CloudSearchDomain as well), the \"prefix\" variable in Aws::Plugins::RequestSigner is nil, which means the \"us-east-1\" hardcoded endpoint is matched against /\\.amazonaws\\.com/. That would match just about any endpoint.\n. Apologies for the regression - I've tested this fix against that failing case and others, and it will go out with the next release.\n. Thanks for adding what you did to fix the issue, glad this appears to be resolved.\nWas there anything else we can still help with?\n. So, here is an example I created just now (in the REPL, but fundamentally the same):\nruby\nAws> b = Aws::S3::Resource.new.bucket(\"foo\")\n=> #<Aws::S3::Bucket name=\"foo\">\nAws> pp = b.presigned_post(key: \"bar\", success_action_status: '201', acl: 'public-read')\n=> #<Aws::S3::PresignedPost:0x007fdd414a21a0\n @bucket_name=\"foo\",\n @bucket_region=\"us-east-1\",\n @conditions=[{\"bucket\"=>\"foo\"}, {\"key\"=>\"bar\"}, {\"success_action_status\"=>\"201\"}, {\"acl\"=>\"public-read\"}],\n @credentials=#<Aws::Credentials access_key_id=\"AKIAMYKEYREDACTED\">,\n @fields={\"key\"=>\"bar\", \"success_action_status\"=>\"201\", \"acl\"=>\"public-read\"},\n @key_set=true,\n @signature_expiration=2016-01-22 11:30:39 -0800,\n @url=\"https://foo.s3.amazonaws.com\">\nAws> pp.fields\n=> {\"key\"=>\"bar\",\n \"success_action_status\"=>\"201\",\n \"acl\"=>\"public-read\",\n \"policy\"=> \"eyJleHBpcmF0aW9uIjoiMjAxNi0wMS0yMlQxOTozMDozOVoiLCJjb25kaXRpb25zIjpbeyJidWNrZXQiOiJmb28ifSx7ImtleSI6ImJhciJ9LHsic3VjY2Vzc19hY3Rpb25fc3RhdHVzIjoiMjAxIn0seyJhY2wiOiJwdWJsaWMtcmVhZCJ9LHsieC1hbXotY3JlZGVudGlhbCI6IkFLSUFKWEhFN1ZOQkk3UU5DUlFRLzIwMTYwMTIyL3VzLWVhc3QtMS9zMy9hd3M0X3JlcXVlc3QifSx7IngtYW16LWFsZ29yaXRobSI6IkFXUzQtSE1BQy1TSEEyNTYifSx7IngtYW16LWRhdGUiOiIyMDE2MDEyMlQxODMwNDJaIn1dfQ==\",\n \"x-amz-credential\"=>\"AKIAKEYREDACTED/20160122/us-east-1/s3/aws4_request\",\n \"x-amz-algorithm\"=>\"AWS4-HMAC-SHA256\",\n \"x-amz-date\"=>\"20160122T183042Z\",\n \"x-amz-signature\"=>\"24aaeb8f92d42c1cb42fd0396e71244405cffe162c27781c8f1175d67dc353a8\"}\nSo, :fields is certainly a hash, and my guess is the error depends on what exactly simple_form_for is trying to do with 'form_data'.\n. Can you give me a reproducing example of that error? Which version of the SDK are you using?\n. No activity here in three weeks, closing. Feel free to reopen or to create a new issue if you have more information.\n. Thank you for the feature request. We're looking at the best way to support some of these CLI configuration values going forward.\nIn the meantime, one option is to use the AWS_REGION environment variable, if you use the same region across clients.\n. This is correct, that's currently CLI-specific.\nGiven that there is a large ask for this, we are looking for the best way to support some of these features, without introducing unexpected behavior into the SDK.\n. Addressed by #1178 \n. #1178 has been merged, this will be supported with the next release, which will be version 2.4.0.\n. Good callout!\n. I'm not going to add this to upload_file, reason being that the functionality comes from an S3 plugin provided in core. I am looking at upgrading the S3 main docs to explain that this occurs across the service, for any request that includes a body.\n. Thanks for the report, will take a look at the pull request.\n. Working on a fix within the Glacier Checksum plugin that rewinds the body before attempting calculation. That would prevent this code path from being triggered.\n. Thanks for the report!\n. Can you provide a failing test for this? Would like to have the issue covered by our test suite, especially for regression testing purposes.\n. Thank you for the code contribution. Looking at the Glacier Checksum Plugin, we're going to take a different approach that will prevent the invalid call. I'll push that change up shortly and close the issue.\n. This sounds like an issue with the library depending the SDK.\nThere is possibly another way you can arrange your gems, since it appears (without being able to see the code) that it assumes 'aws-sdk' means V1.\nYou can lock aws-sdk to V1 in your gemfile, and then depend on 'aws-sdk-resources'.\nruby\ngem 'aws-sdk', '~> 1.0'\ngem 'aws-sdk-resources'\n. No response in two weeks, closing. If the gemfile tweaks didn't fix the issue, and you have more information, please feel free to reopen or to open a new issue.\n. Thanks for raising this to an issue, will discuss with Trevor.\n. Thanks for reporting this. I had a brief chat with their team, and it would be helpful if you could give me a full reproduction example, and a wire trace.\nTo enable wire tracing and get the wire logs:\nruby\nclient = Aws::APIGateway::Client.new(http_wire_trace: true) # plus whatever other options you use\nopts # what options are you using? feel free to strip out anything sensitive\nclient.put_integration_response(opts)\nFeel free to strip out anything identifying from the wire logs, but it may help the service team if we can keep the request ID. If you'd be more comfortable emailing me the wire logs, let me know.\n. Thanks, I've reached out to the API Gateway team to see what is going on.\n. After talking to the service team, this is a known bug in the service right now. So, as you discovered, for now you should make sure to specify that parameter.\nClosing as there's nothing we can do on our end. If I am made aware of a change to this I'll update the issue.\n. I'd consider raising an issue with AWS Support on this one, there isn't much that we can do ourselves on this. Will try to raise it up again with the API Gateway team.. This isn't a part of their API for that method, but can you describe what you're trying to do? May be able to help, and if not, will have more precise feedback to send to the Beanstalk team.\n. I understand what you're getting at.\nIt's been pointed out to me that the Elastic Beanstalk CLI does this:\nbash\neb labs cleanup-versions\nIf you'd like to go the API route, you can combine the output of #describe_application_versions with the version labels in #describe_environments to determine which versions are not in use.\nDoes this help?\n. I'll pass on that feedback. Let me know if you have any more questions!\n. There is no such restriction, so this is something we're going to investigate.\n. Thanks for the report, will reproduce and investigate.\n. It's not currently done, but we would certainly take a pull request to do it, or add it as a feature request.\nIf you want to take a crack at the PR, I'd recommend writing such a validation as a plugin, similar to our S3 MD5 checker.\n. The \"capitalize\" portion is causing multiple regressions in the test suite. May want to consider another approach. Will try to take a look at this on my end as well.\n. What version of the SDK are you using?\n. Do you have any information about how? Just headers?\n. Looks good, thanks for catching this!\n. You can do this, the semantics are slightly different. The concept to keep in mind is that we will resolve configuration in roughly the following order:\n1. Locally supplied configuration at client creation time.\n2. Globally defined configuration, scoped to the service.\n3. Globally defined configuration.\n4. Default values.\nOr, for a contrived code example:\n``` ruby\nclient = Aws::S3::Client.new(region: \"us-east-1\")\nclient.config.endpoint # => #\nAws.config[:s3] = { endpoint: \"https://bar.foo\" }\nAws.config[:endpoint] = \"https://foo.bar\"\nec2 = Aws::EC2::Client.new(region: \"us-east-1\")\nec2.config.endpoint # => #\ns3 = Aws::S3::Client.new(region: \"us-east-1\")\ns3.config.endpoint # => #\ns3_custom = Aws::S3::Client.new(region: \"us-east-1\", endpoint: \"https://my.example.endpoint\")\ns3_custom.config.endpoint # => #\n```\n. Thanks, the link is helpful for getting some initial context.\n. On the test failure, I agree that I don't think it's your change.\n. So, I want to make a master issue for this soon, but we're currently figuring out the correct approach for supporting these features across SDKs in a consistent way. There's probably going to be a delay in pulling this in while we are figuring it out, but we're not ignoring this.\n. We're closing in on how we want to approach this. One change will be that some of these changes may be gated by an environment variable - anything that would risk possible backwards incompatibility.\nWill be keeping this open in the meantime, the intention is to merge this or to otherwise implement this.\n. That's actually an interesting approach I hadn't considered. I'll definitely add that to the discussion we're having about broadening this support - I agree that also solves the problem. Would likely be implemented as ENV['AWS_CLI_CONFIG'] || use_in_sdk or similar.\n. Thank you for this. We're moving forward, but I'm going to use #1132 as the main PR for this since you're doing essentially the same thing, and that PR is slightly further along. Let's move the conversation there.\n. Sorry, help me understand. Are you getting that ArgumentError from the code above?\n. It does not appear that the #create_load_balancer API supports that parameter: http://docs.aws.amazon.com/ElasticLoadBalancing/latest/APIReference/API_CreateLoadBalancer.html\nThere's not much I can do here, I would suspect that the CLI is just providing a wrapper around running those two commands.\n. Actually, for clarity, what CLI command are you running that allows this?\n. I can pass that feedback on to the service team. The errors you're getting are from the service level, so there's no way for our client to alter this behavior.\n. If you'd like to open a PR, happy to look at it. I think that if the unit tests are based on an assumption that would be changing, it would be okay to modify them (and then we can look at the change as a whole).\n. (Accidental close.)\n. Thanks for the report - I'll reproduce and see what can be done about this.\n. The issue is a typo in the resource definition. Going to make a fix for this and test.\n. Fix is pushed and will go out with the next release.\n. Thanks for the report - will take a look at the PR as well.\n. This would not be by design for any reason I can think of, thanks for the report.\n. Certainly happy to see a PR for this. The unit tests would likely just confirm that the relevant headers are being lifted to the request (which would, at this point, be a failing test).\n. Thanks for the report, will take a look at this.\n. I'm going to check if there is any unexpected behavior from the waiters as written. I do know that some matcher types end up behaving in de facto the same manner, so it may not be an issue (even if not semantically optimal). I don't recall off the top of my head (returning from vacation too) which matchers are which, so I'll be looking at differences in behavior if any.\n. This is a good catch - the waiter will likely just cycle on false for a scalar with a pathAll matcher.\nI'm already going to be touching Auto Scaling resources to fix another bug, so I will test and push the fix for this - no need for a PR, unless it's already done and tested.\n. I've written a fix for this that will work in the case of requesting exactly one Auto Scaling group. Unfortunately, due to waiter/JMESPath limitations, if you have multiple Auto Scaling groups in a single request, something like :group_exists will return a success response if at least one group exists.\nYou can avoid this by always specifying a single group in your waiter requests.\n. Can you elaborate a bit more?\n. Closing for lack of activity - feel free to reopen if you have more context to add.\n. In general, higher level abstractions including Presigning and Object Upload were put into the resources gem. Is there a particular reason you don't want to or can't pull in the resources gem?\n. I can bring this up with the service team, and see if there is a model enhancement that can be made. I wouldn't be inclined to make this a client-side customization since it's making a validation assertion that, in theory, could change server side.\n. I agree and will look in to if a model-level validation is possible here. I don't want to go down the customization route, however.\n. I've pushed a fix, releasing shortly.\n. Working on the fix now, will push a new patch release afterwards.\n. @jasonschulte is correct, but we're going to push a fix to make that unnecessary (otherwise should be added as a gemspec dependency).\n. I've pushed a fix, verifying and releasing shortly.\n. Release is in progress. I'll close this issue once I confirm that 2.2.22 resolves the issue.\n. Gem version 2.2.22 is out. I have to wait a few minutes to test (uninstalling kramdown mid-release would not be my smartest move today). Can someone help confirm the problem is gone?\n. I can also now confirm no issues on my end. Closing this, but feel free to reopen if anyone is seeing this with 2.2.22.\nThanks everyone for the flood of reports - and I apologize for my mental mistake on this one. We'll do better.\n. ?\n. Can you try this again with a wire trace?\n``` ruby\nclient = Aws::Route53::Client.new(http_wire_trace: true)\nrun your calls\n```\nYou can scrub sensitive data of course, mainly interesting in seeing if the request is well formed by the client. Usage issue versus potential bug.\n. I agree this isn't a helpful error message. I'm reaching out to the service team for help on this, because as far as I can tell we're sending a well-formed request. I'll update this space.\n. For the non-working call, I'm hearing that :resource_records is required. A best practice shared from the service team is to take the result of #list_resource_record_sets and use that for the call into #change_resource_record_sets with whatever changes you want to make.\nLet me know if this resolves your issue.\n. Moved to the feature request backlog.\n. I've expanded and rewritten this into a tutorial that I am merging in. I'm happy to see contributions to the developer guide and look forward to more. Let me know if you have any comments or additions to the guide produced.\nThank you!\n. I'm unable to recreate this. Can you tell me a bit more about how you're calling this and setting up your environment?\n. Closing due to no activity in a month. If you have additional information, feel free to reopen and I'll be happy to help.\n. Can you provide a full stack trace for this? Will take a look.\n. This is interesting...I'm unable to recreate the issue on my end. Can you tell me which SDK version you were using at the time of the failure?\n. ...and, by chance, if restarting your servers would potentially involve rebuilding your bundle, and therefore silently upgrading?\n. Given the fact that this error was happening transiently, I think eager autoloading should solve the issue. If it doesn't, please feel free to reopen or to cut a new issue.\n. I'm unable to recreate this - I am able to call #put_object successfully on a dotted bucket. Can you tell me more about your use case?\n- What region are you calling?\n- What version of the SDK are you using?\n- Can you give me a failing example? Similar but not exactly the same bucket name is fine.\n. Thanks for being proactive on this!\n. This code is breaking a large number of tests.\n. I'm going to test this somewhat carefully to avoid any unexpected behavior change. What will help is, are you addressing a particular piece of missing functionality, or is change just being made \"in case\"?\n. Given that there isn't anything that hits this, going to close this PR. If you see a case where this would be hit that I can reproduce, feel free to reopen!\n. Thanks for this! Pulling down for local testing.\n. This looks good, thanks!\n. Primarily a matter of discussion on our end about the details of how we can best support this across SDKs (not just Ruby). I'm driving that now, working towards a sustainable and backwards-compatible solution.\n. Alright we're moving forward, here's what I'd like to see before merging (I'll pull this down and do these things next week, but if you'd like to tackle it first, go for it):\n- Unit Tests: Let's make sure that we're testing these code paths with mock config files.\n- Feature Flag: This change is not backwards compatible. Therefore, we're going to require that ENV['AWS_SDK_LOAD_CONFIG'] is set to do any of this. We'll need to add that before we can push this to production.\nAdditionally, I'm looking at MFA support and a bit more. But we can pull down this PR once we have tests and the ENV flag in place.\n. Closed in favor of #1178 \n. Thanks!\n. It would probably be an extra help if we can get a wire trace on the client.\nruby\nclient = Aws::SES::Client.new(http_wire_trace: true)\nI definitely would be interested in tracking this down. Is this a transient error, or is it consistently reproducible?\n. I'm not able to recreate this, trying to test the #send_email method isn't revealing any problems on my end. If you can provide a wire trace that includes the response you're getting that is having parse issues, that would be a huge help.\n. Have you been able to recreate the issue with a wire trace?\n. Closing for inactivity - feel free to reopen or to open a new issue if you are able to provide more information. Or, if you found another root cause, let us know!\n. Good catch, thanks!\n. Indeed, I can see where the docs aren't showing everything, despite it being in the resource description. Going to look more in to this, see how this is happening.\n. I'm even seeing, early on, that the resource call isn't working properly. This may not be a documentation issue.\n. Going to try and replicate this, then will work on an update. Also happy to take a pull request if you like.\n. Thanks for the report, I've added a compatibility fix for older Oj versions.\n. I see what you're saying about the :dry_run flag not being passed through, but what do you mean about the instance stopping? That code snippet above will only call Aws::EC2::Client#describe_instances.\n. To clarify, your example above is functionally equivalent to:\n``` ruby\n!/usr/bin/env ruby\nrequire 'aws-sdk'\nrequire 'json'\nec2 = Aws::EC2::Resource.new\nclient = ec2.client\nec2.instances.each do | i |\n  begin\n    client.wait_until(:instance_stopped, instance_ids: [i.id])\n  rescue Aws::EC2::Errors::DryRunOperation\n    puts \"DryRun No Changed Made\"\n  end\nend\n```\nThat code will only make an API call to #describe_instances. Adding wire tracing to your resource instantiation can show this:\nruby\nec2 = Aws::EC2::Resource.new(http_wire_trace: true)\nIncidentally, the following code should give you your intended behavior:\n``` ruby\n!/usr/bin/env ruby\nrequire 'aws-sdk'\nrequire 'json'\nec2 = Aws::EC2::Resource.new\nclient = ec2.client\nec2.instances.each do | i |\n  begin\n    client.wait_until(:instance_stopped, instance_ids: [i.id], dry_run: true)\n  rescue Aws::EC2::Errors::DryRunOperation\n    puts \"DryRun No Changed Made\"\n  end\nend\n```\n. Your point about us not plumbing the parameters to the waiter stands, at the very least it's not clear that this is the behavior.\nHowever, as a matter of helping with your use case, I'm not sure that passing :dry_run to #describe_instances is necessary in your case, if I'm understanding it correctly. To be clear, :wait_until_stopped DOES NOT stop the instance. It won't make changes, it will poll for changes.\nTo stop an instance, and block until the instance is in a stopped state, you'd do the following (this is obviously not a dry run):\n``` ruby\n!/usr/bin/env ruby\nrequire 'aws-sdk'\nec2 = Aws::EC2::Resource.new\nec2.instances.each do | i |\n  i.stop\n  i.wait_until_stopped\nend\n```\nHopefully this unblocks you.\n. I can see where the documentation could be more clear on this method. The instance ID of your resource instance is passed through on your behalf, you do not need to add it like in your earlier example.\n. I've added a pull request to fix the documentation. At this point, we don't plan to support adding extra parameters to resource waiter calls, as there isn't a case we know of where it is needed at this point. In your case, it does not appear that you need :dry_run as you're calling a side-effect free method.\nResolving this, feel free to reach out again if you need any other help.\n. Reviewed with Trevor offline.\n. Thanks for the feature request! I'll pass it on to the service team for you as feedback.\n. I'd also consider leaving feedback to the team directly, which you can do through the console. Click \"Feedback\" in the lower left hand corner. This is also helpful because they may reach out to you directly.\n. This should be working (nothing about a space character should break this), I'll try to reproduce on my end.\n. I'm able to replicate this issue on my end, thanks for the report and I'm looking at it now.\n. Specifically, it looks like we are URL encoding that space twice, which is leading to the 404 (we're calling #head_object on an object that doesn't exist).\n. I'm working through some edge cases in the proposed fix for other URL encoding cases. The referenced pull request code will fix your case, but it breaks for other URL encoding situations.\n. This should be resolved by that PR, and will go out with the next release. Thanks for the report!\n. This turns out to be the wrong approach, so I'm abandoning this PR, to be replaced by a new one later.\n. I can see what you're talking about, and taking this on as a feature request to add to the resource model.\n. If you'd like, we'd also happily take a pull request that adds this to the resource model.\n. Added to feature request backlog.\n. You're only going to get this error if the credentials you provide are nil. For example:\n``` ruby\nec2 = Aws::EC2::Resource.new(region: \"us-east-1\", credentials: Aws::Credentials.new(\"key\", \"secret\")) #=> #\nec2.instances.first # [Aws::EC2::Client 401 0.725211 0 retries] describe_instances() Aws::EC2::Errors::AuthFailure AWS was not able to validate the provided access credentials\nec2 = Aws::EC2::Resource.new(region: \"us-east-1\", credentials: Aws::Credentials.new(nil, nil)) #=> #\nec2.instances.first # Aws::Errors::MissingCredentialsError: unable to sign request without credentials set\n``\n. Thanks for the update. One thing to consider, if it's feasible in your environment, is to create a shared credentials file at~/.aws/credentials` - if you follow the format in the documentation, it will be loaded correctly for all clients.\nIn any case, glad you were able to determine the cause of the issue!\n. Added to feature request backlog.\n. Version 1.2.3 is out, this issue should provide more context: https://github.com/jmespath/jmespath.rb/issues/20\nLet me know if this helps.\n. Sorry about the trouble, glad the new release solves the issue. Closing this for now, let us know if you have any other issues or questions with this.\n. Issue #921 discusses this as well, but we only use Signature Version 4 for presigning. It's built in to the presigner itself.\nCan you tell me a bit more about how you're getting to the SignatureDoesNotMatch issue?\n. Closing for inactivity. Feel free to reopen or open a new issue with more information.\n. I believe that this authorization is fundamentally the same. The only signed header in that request is \"host\", so I don't think you would see signature errors within the expiration period if other headers changed.\nThe problem you're running in to is one of generating the presigned URL for each request, am I understanding that correctly?\n. You can change headers that aren't part of the signature on the request, but the entire query string is signed - there's no way around this that I am aware of.\nWhat are you trying to do?\n. Added to feature request backlog.\n. I'm not able to replicate your error through creating a fresh bucket/objects and running your code snippet. Can you tell me anything else about these objects that might be different?\n. That makes sense, and yes I can replicate the issue now. When we call #head_object to pull object data, we're getting a 404. I'll dig in to this a bit more.\n. So there is an issue here, but I'm not sure that there is a way to get your example above to work as-is.\nWhat's happening now, which appears to be wrong, is when you ask for the delete marker we're simply calling #head_object without a version ID. For example, my replicating case makes the following calls:\nruby\n[Aws::S3::Client 200 0.745526 0 retries] list_object_versions(prefix:\"test-versioned\",bucket:\"alexwood-versioned-test\",encoding_type:\"url\")\n[Aws::S3::Client 404 0.079272 0 retries] head_object(bucket:\"alexwood-versioned-test\",key:\"test-versioned\") Aws::S3::Errors::NotFound\nHowever, I can craft #head_object calls, with the version ID, that give me a more useful result for non-delete-marker versions:\nruby\ns3.head_object(bucket: \"alexwood-versioned-test\", key: \"test-versioned\", version_id: \"b71KNmKTx7kcMnaZQQK_9gglXpPdpJoQ\")\n[Aws::S3::Client 200 0.646074 0 retries] head_object(bucket:\"alexwood-versioned-test\",key:\"test-versioned\",version_id:\"b71KNmKTx7kcMnaZQQK_9gglXpPdpJoQ\")\nCalling #head_object on a version with the delete marker will give me a 405 coded error. So, I don't think you'll be able to get an Aws::S3::Object resource with delete_marker: true.\nI am looking in to what the best way is to do this, and if there's an improvement that could be made to the resource abstraction.\n. I think part of the issue is that there might be a limitation in the resource model - it could be hard to fix this in a complete and backwards compatible way.\nI'd suggest as a workaround that you use the #list_object_versions client API instead:\nruby\nclient = Aws::S3::Client.new\nresp = client.list_object_versions(bucket: bucket, prefix: key)\nresp.delete_markers # Has what you want.\n. I haven't heard back, so assuming that this workaround was helpful. Feel free to comment or reopen if you're still having issues.\n. Based on wire tracing, I don't believe unprocessed items is returned when the whole call comes as a ProvisionedThroughputExceededException error.\nRather, if you used #batch_write_items with an array of many items, and you get a partial success, you'll get a 200 response and unprocessed_items will include any items that were not written.\nIf you get a 400, you can assume all items were not written. All of this is server-side behavior.\nDoes this help?\n. That's correct, yes.\n. Going to close this, since it seems like we have a resolution here. Feel free to ask any follow up questions you have, or to open another issue if there are other issues you run in to.\n. I'm not seeing this error. Are you sure that the gem version is correct?\nruby\nresource = Aws::AutoScaling::Resource.new(region: \"us-east-1\")\nresource.group(\"Foo\") #=> #<Aws::AutoScaling::AutoScalingGroup name=\"Foo\">\nTry checking the output of the following within your own code:\nruby\nAws::AutoScaling::Resource.new(region: \"us-east-1\").methods.sort - Object.new.methods\nDoes that include :group?\n. The Aws::VERSION variable will tell you the version you have. I'd postulate that you are running an old version, given the missing method.\n. At least 2.2.21, but I'd recommend updating to the most recent version of the SDK in general - we do keep things backwards compatible.\n. Upgrading will resolve your issue, let me know if you have any further questions.\n. Glad that worked out! We do keep a CHANGELOG.md file in the root directory (and publish these release notes on the AWS web site) to help debug these kinds of things.\n. I'll check this with the team and doc writers, but we can't take pull requests to an auto-generated file. Thanks for the heads up!\n. I can say that this would not be intended behavior. Trying to repro now.\n. I'm unable to recreate this on my end. Can you provide me with a detailed stack track of the eventual error, and the output of:\n``` ruby\nbasic_creds = Aws::InstanceProfileCredentials.new\nsts = Aws::STS::Client.new(http_wire_trace: true, credentials: Aws::InstanceProfileCredentials.new)\ncreds = Aws::AssumeRoleCredentials.new(\n  client: sts,\n  role_arn: 'role-arn-to-assume',\n  role_session_name: 'role-session-name',\n)\nsts = Aws::STS::Client.new(http_wire_trace: true, credentials: basic_creds)\nsts.get_caller_identity # identity with assumed role\n```\nPlease scrub any sensitive data from the wire logs, I mainly need to see what calls we are making over the wire.\n. Glad to hear it got worked out!\n. I am able to replicate the regression, taking a look.\n. Interestingly, the exact choice of bucket name here matters. The bucket 'test-issue-s3' is in the ap-northeast-1 region. So, for example, this code works:\nruby\nAws::S3::Resource.new(region: \"ap-northeast-1\").bucket('test-issue-s3').exists? #=> true\nThe issue here comes from the 307 Temporary Redirect we're getting. I get a similar issue if I try to hit a bucket I own in the \"eu-central-1\" region from a client in \"us-east-1\".\n. Looking back at some previous versions, I don't think this is a regression, I don't think this particular case has ever worked. Looking at fix options. I don't think I want to add 400 Bad Request as a successful waiter case, however.\n. So, the fix here (as Trevor is wrapping up) is to check for the bucket region in headers as well as the body. That will resolve this issue and similar. It's not often encountered because this particular failure modality was not hit by most client methods.\n. Thanks for the report!\n. LGTM\n. LGTM, assuming my assumption within was correct.\n. Sanity Check: Is there any collection, anywhere, that behaves differently? Are we going to be able to ensure that all collections going forward behave in this manner?\n. Change itself looks fine w/r/t stated intent.\n. That support does exist now if you set the endpoint manually. There are a couple of open pull requests we're finalizing with further improved support.\n. Version 2.3.0 is out with this support.\n. Glad that was worked out quickly!\nFor reference (if others see this), if a hash value is nil, Ruby hashes will effectively treat that as the key not existing. That's what the validation is seeing.\n. Can you show me the context of how you're calling this? Having trouble reproducing this.\n. It works for me (examples from aws.rb):\n``` ruby\ns3.resource.bucket('foo').object('bar').presigned_url(:get) #=> \"https://foo.s3.amazonaws.com/bar?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAKEY%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20160427T163755Z&X-Amz-Expires=900&X-Amz-SignedHeaders=host&X-Amz-Signature=dfb94310e64c5352534891e574cba7ce6e0e740f649053b173820014d28f92a2\"\ns3.resource.bucket('foo').object('bar').presigned_url(:get, secure: false) #=> \"http://foo.s3.amazonaws.com/bar?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAKEY%2F20160427%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20160427T163803Z&X-Amz-Expires=900&X-Amz-SignedHeaders=host&X-Amz-Signature=22a965dd9cedbaf468e26d612c2dffa8586173860180747e39616a01f6cc0d6d\"\n```\nWhat version are you using? The confusion may be the fact that you're not seeing :secure in that method, because it isn't applied until we get to the client presigner. If that's the case, I could see the use of a doc change.\n. That's correct. I'm not sure off the top of my head why that was, let me get back to you on that.\n. #786 has the extended discussion on why you can't use HTTPS with virtual hosts. In short, because missing security certificates in those cases. Do you have a case where you need HTTPS for virtual hosts?\n. What is the content type of one of those objects if you try the following?\nruby\nclient = Aws::S3::Client.new(region: REGION)\no = client.get_object(bucket: BUCKET, key: key)\nputs o.content_type\nKeep in mind that S3 doesn't inspect the objects you're sending over the wire (for example, you could just as easily be encrypting them). I would suspect that the content type is set as you defined it.\n. Haven't seen any movement on this for a month, closing.\nFeel free to reopen or to open a new issue if you have more information.\n. You wouldn't get a NoSuchEntity error since we lazy load resources by design - we won't make a client call for you until we have to. However, adding an exists? method is a good feature request.\n. I've created an :alarm_exists waiter, and the corresponding Aws::CloudWatch::Alarm#exists? method. It's currently pending review before it can be pushed out to all the SDKs.\n. Feature will be included in the next release.\n. I see what you're saying about the fact that Aws::S3::ObjectSummary having a load method present but not implemented. Are you trying to use ObjectSummary as a resource equivalent to a #head_object call?\nFor example, I can see that a call like s3.resource.bucket(b).objects.first will return an ObjectSummary with the fields in question populated. Calling summary.object.load will populate all fields, but may not be desirable if the object in question is large.\nI'll take a look at the resource model for this, will also help to understand the use case here.\n. So, this actually may be complex to do (supporting an ObjectSummary#load properly), but I have a workaround.\nIf you create an Object resource, note that #load actually calls #head_object. We only call #get_object on an explicit get.\nFor example:\n``` ruby\ns3 = Aws::S3::Resource.new\nobject = s3.bucket(BUCKET).object(OBJECT)\nobject.load # calls #head_object\nLater, if you want...\nobject.get #calls #get_object\n```\nThis will actually work for your use case. I agree that the semantics can be confusing, so I'll look in to how to best improve/clarify this.\n. Can you provide a bit more context? Client-side or server-side encryption? Is the \"resp\" you're referring to from an S3 get_object?\n. Okay, I can see that the response is not Base64 encoded. This is expected behavior. You can Base64 encode the response with:\nruby\nresp = kms_client.encrypt(key_id: kms_key_id, plaintext: content)\nBase64.encode64(resp.ciphertext_blob)\n. Glad that worked for you! Closing, feel free to follow up if you have other questions or issues.\n. I'm not sure what this has to do with the SDK off hand - perhaps this was meant for the Oxidized repo?\n. Closing this as I'm pretty sure it's unrelated to the SDK. Feel free to add a comment or reopen if one of us missed something about that.\n. That is, indeed, very crisp. I approve!\n. @temujin9 This last commit is the end of the planning/development cycle. Once we finish reviewing and ensuring that the test harness is solid (since this touches a lot of code that's run by just about every single client construction) we're intending to release.\n. When we release this, you'll definitely see a note here and it will be a minor version bump, for e.g. to 2.4.0.\n. I've marked the Aws::SharedConfig class as API private.\n. Best way to test is to run it with your local client. In your development directory, can run ./aws-sdk-core/bin/aws.rb to get an interactive shell with your changes. Then test on actual AWS resources.\nI'm happy to do that for you with my account if you'd prefer, because this looks good at a glance. I'd revert the CHANGELOG change though, as that tends to cause merge conflicts. I'll happily add that for you (in fact, that will happen via our release scripts once I push this change up to the other SDKs).\n. Sorry about the delayed response here - fix the merge conflict issue and I'll get this tested and merged.\n. LGTM\n\nOn May 9, 2016, at 12:49 PM, Trevor Rowe notifications@github.com wrote:\nThe XML parser supports multiple backends. There is a race condition when loading the parser code where the default parser has not been chosen, but the first instance of the parser has been constructed. In this scenario, it raises with a undefined method error on nil. This is because the self.class.engine is returning nil.\nHere is a minimal test case that reproduces this issue. It reproduces reliably under Ruby 1.9.3:\nrequire 'aws-sdk-core'\nrequire 'thread'\nsqs = Aws::SQS::Client.new\nthreads = 10.times.map do\n  Thread.new do\n    sqs.send_message(queue_url:\"https://...\", message_body:\"string\")\n  end\nend\nthreads.map(&:join)\nThe work-around forces the constructor to set the default engine if this hasn't happened yet. There is an additional concurrency issue encountered with setting a default endpoint provider. This surfaced during testing. Similar issue and fix.\nFixes #1135\nYou can view, comment on, or merge this pull request online at:\nhttps://github.com/aws/aws-sdk-ruby/pull/1180\nCommit Summary\nFix for Aws::Xml::Parser concurrency issue.\nAnother threading issue addressed.\nAdded CHANGELOG entry.\nFile Changes\nM CHANGELOG.md (7)\nM aws-sdk-core/lib/aws-sdk-core/endpoint_provider.rb (11)\nM aws-sdk-core/lib/aws-sdk-core/xml/parser.rb (11)\nPatch Links:\nhttps://github.com/aws/aws-sdk-ruby/pull/1180.patch\nhttps://github.com/aws/aws-sdk-ruby/pull/1180.diff\n\u2015\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly or view it on GitHub\n. This is currently defined in the model itself, so default choices would come from that. It would be interesting to explore what it would take to allow configuration of HTTP verbs used by service calls.\n. Added to feature request backlog.\n. I can't really follow your code above. Are you actually setting :region to ?\n\nIf so, I can imagine where errors could pop up. What are you trying to do?\n. Quick hunch then - what happens if you add: \nruby\nAws.eager_autoload!\nIt's also very likely that an upgrade to the latest aws-sdk-resources version fixes this.\n. Added to feature request backlog.\n. Looks like an autoloading concurrency issue - temporary workaround would be to add this when you initialize your code:\nruby\nAws.eager_autoload!\nWe'll take a look.\n. Are you still seeing this? It looks like this was handled in the next version 2.3.3. It was a threading issue, and we've been cleaning up a few of these. If you cannot upgrade, the workaround above should also fix the issue.\n. I think part of this issue comes from the fact that the EMR API has changed as well - #describe_job_flows is now deprecated with intent to be removed: doc link. A straight port of #exists? from V1 wouldn't work in the same way because we're talking about different client methods.\n. Let's work through the current state, especially since there is a forced migration from #describe_job_flows to #describe_cluster - so even a future higher level abstraction needs to handle this behavior. Let's say I make a describe cluster request for a cluster that obviously does not exist:\nruby\nemr = Aws::EMR::Client.new(http_wire_trace: true)\nemr.describe_cluster(cluster_id: \"j-1foobar\")\nThe body of the 400 Bad Request response I get from the service (if pretty-printed) is:\njson\n{\n  \"__type\": \"InvalidRequestException\",\n  \"ErrorCode\": \"MalformedClusterId\",\n  \"Message\": \"Cluster id 'j-1foobar' is not valid.\"\n}\nI want to emphasize, again, that V1 of the SDK is going to have this same issue as #describe_job_flows gets deprecated (I can't even run it on my test account anymore to compare). We're building the exception type first from the __type value, and second from the ErrorCode value. This is the cross-service standard. We calculate the code in the same way. Unfortunately in this case, it clobbers the more descriptive ErrorCode value in the error response body, which is difficult to fix in a non-breaking way. In other words, even a higher level abstraction would probably have to resolve the situation in the same way you did, at least for the moment. Though something in the exception class that exposes the raw ErrorCode from the response seems like it could be helpful in cases like these without being a breaking change.\n. Messing around, you can actually expose this error. It's not the cleanest code (and not tested in detail), but while we figure out the best way to expose this, it illustrates how you could more reliably pull out the error code from the response body:\nruby\ndef exists?(cluster_id)\n  begin\n    emr.describe_cluster(cluster_id: cluster_id)\n    true\n  rescue Aws::EMR::Errors::InvalidRequestException => e\n    if JSON.parse(e.context.http_response.body_contents)[\"ErrorCode\"] == \"MalformedClusterId\"\n      false\n    else\n      raise e\n    end\n  end\nend\n. Added to feature request backlog.\n. As per issue #924 V1 of the SDK is end of life, and we're not making further API updates to it. Happy to work through issues with the V2 experience, but V1 changes are around major bugs or security issues only, should those arise.\n. Seems a duplicate of #1192?\n. How are you setting your credentials at client creation time? It's possible that there is an issue with the service, but most commonly this issue would happen because you're not getting the role/user you expect when creating a client.\n. At this point (given that you're explicitly setting your credentials so it can't be default credential provider chain confusion), it sounds like there's either a service issue, or some way you could be using the service differently. Have you tried opening a support ticket with the service team?\n. I can try to ask on your behalf, however debugging this with them will probably require sharing things that I wouldn't be comfortable posting in a GitHub issue, such as account IDs, access keys.\n. I'll reach out to Lambda and see what I can find out about this. You could try posting a wire trace of your call, stripping out sensitive values. To do that, just add http_wire_trace: true to your client construction.\n. I want to dig in a bit more to make sure you have the exact credentials needed for the call you're trying to make. Can you try the following two calls? Two choices, CLI or SDK. If you go the CLI route, make sure you're using the same credentials as with the SDK.\nCLI:\nshell\naws ec2 describe-security-groups --group-names sg-5613863 \naws ec2 describe-subnets --subnet-ids sub-131313\nSDK:\nruby\nopts = {} # However you construct your clients normally. Region, credentials, etc.\nec2 = Aws::EC2::Client.new(opts)\nec2.describe_security_groups(group_names: [\"sg-5613863\"])\nec2.describe_subnets(subnet_ids: [\"sub-131313\"])\nLet me know what those return.\n. Changing the param is fine - I was mostly interested in seeing if you got 403 errors for either call. I have discussions ongoing with the Lambda and EC2 teams and will let you know as those progress.\n. I've gotten some feedback from the service teams that you have two Lambda IAM roles, one of which has access to \"DescribeSecurityGroups\" and one that does not. You can email me at alexwood@amazon.com if you want to discuss details on that further.\nI do also note your last bit of correspondence as well. Can you give me a request ID for something you ran with that role you pasted above?\n. Alright, feel free to comment or reach out by email if you need help\ndebugging. Checking your IAM Users/Roles is probably the best path from\nwhat I've seen.\nOn Tue, May 31, 2016 at 12:32 PM, Prabhakar Chaganti \nnotifications@github.com wrote:\n\nMore debugging. Used the aws-cli as a sanity check. Getting the same error\nthere. So this is definitely not an issue with the aws-sdk-ruby, but an\nissue with the credentials or how we are calling the API. Will get this\ntracked down. Closing this issue for now.\nthanks again for your help with this!\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/aws/aws-sdk-ruby/issues/1192#issuecomment-222795245,\nor mute the thread\nhttps://github.com/notifications/unsubscribe/ACBDoQbcia0qngy1iwRbYfLLDMC-c2Wrks5qHIy2gaJpZM4Iknqq\n.\n. Immediate workaround would be to wrap the method calling the poller with something that catches this error and creates a new poller.\n\nWe already do retry networking errors, so if you're seeing that in your stack trace, multiple retries at the client level have already failed. Other than infinitely retrying, I'm not sure what else the code side can do.\n. One other option is that you can increase the :retry_limit in the client, to make this kind of error less likely and to increase the exponential backoff time. For example:\nruby\nclient = Aws::SQS::Client.new(retry_limit: 5)\npoller = Aws::SQS::QueuePoller.new(url, client: client)\npoller.poll do |message|\n  # ...\nend\nClosing as that's the most reasonable workaround I can think of (again, we can't infinitely retry networking errors, risks an infinite loop). Feel free to add any further questions you have, or reopen/create a new issue if there are other concerns.\n. I should have mentioned it sooner, but you'd get the same retry behavior, except with exponential backoff and for all retryable cases, simply by adjusting :retry_limit. Nothing stopping this, for example:\nruby\nclient = Aws::SQS::Client.new(retry_limit: 25)\npoller = Aws::SQS::QueuePoller.new(url, client: client)\npoller.poll do |message|\n  # ...\nend\nThe retry periods could get really long, but it would terminate and would retry with similar aggressiveness - per client call.\n. No, it is already retried. See the RetryErrors plugin.\nYou are only seeing the error after retries are exhausted.\n. I have a (loose) suspicion. What happens if you try this variant?\nruby\nsns = Aws::SNS::Client.new(access_key_id: ENV['AWS_ACCESS_KEY_ID'], secret_access_key: ENV['AWS_SECRET_ACCESS_KEY'], region: 'us-east-1', http_wire_trace: false)\nresp = sns.list_subscriptions\nresp.last_page? # Testing if pagination happened.\nresp.subscriptions.length # For reference.\nI'd also be interested in seeing the HTTP wire trace of your call and response. That will help determine why you're seeing a discrepancy (because the other possibility is that you're getting a legitimately different response between console and the service, which would be an issue I would raise with the service).\n. Yes. Try this:\nruby\nsns.list_subscriptions.each_page do |page|\n  page.subscriptions.each do |subscription|\n    # do what you want, but for example:\n    puts subscription.subscription_arn\n  end\nend\n. That's why I asked for last_page? - your value of \"false\" shows that the response was truncated. If that had returned true, I was hoping something in the wire trace response would have hinted to the cause (or to a broken pagination configuration). But, this looks to be a correctly behaving paginated response.\n. Closing this since it appears we've identified the cause. Feel free to tack on any follow-up questions, or reopen/open a new issue if we missed something.\n. I think it's fair feedback, and I'll pass it on to the service team.\nClosing as it appears the question is answered. Feel free to add on any follow-up questions, or to create a new issue/reopen this if there's more information or concerns.\n. While waiting on the repro case, research has also indicated (I think, need to confirm) that this exception hasn't always been defined in OpenSSL. To avoid breaking some customers, we'd want to ensure that we don't crash out when the constant isn't defined. (This is primarily a note to self.)\n. I understand the frustration with the fact that this API is large and has multiple use cases. I would agree that example beyond the auto-gen stubs in the docs would be especially helpful for this API.\nOne note is that :ttl cannot be marked as required, because it should be omitted for ALIAS records: Relevant Documentation.\n. Mostly minor comments, plus the in-progress resolution of test failure causes.\n. The QueuePoller class does provide the :idle_timeout option, which appears to handle your intended behavior. Is that sufficient?\n. I get what you're saying, we're on the same page now. Looking in to it.\n. It's really hard to say without seeing the nature of the exceptions you got - which I can understand you may not have. Short answer is that shouldn't be able to block infinitely, though it could block for a while (a couple retry layers exist).\n. Absolutely, feel free to reopen if you have more information!\n. Glad you got things working quickly!\n. I'm able to recreate the signature issue with a bucket with slashed names (for e.g., foo/bar). Trivial recreation example:\nruby\nclient = Aws::S3::Client.new\nclient.create_bucket(bucket: \"awood45/foo/bar\") # Raises 403 SignatureDoesNotMatch exception\nIncidentally, the same issue exists for 'us-east-1' and 'us-west-2', so I do not believe this is region-dependent.\n. At this point, looking at this as a possible bug in the signer. Possibly URL encoding? Will take a look.\n. I think I can also see why you're getting the error you're getting - given that S3's different commands are, at times, aliases to different REST queries against the endpoint, it could treat a POST to /foo/bar as a malformed put object to bucket 'foo' of the key 'bar'. Or so it appears from an early investigation.\n. I'll admit I'm a bit unfamiliar with some of this. Can you give me an example of the command you're trying to create, and where you feel like something is missing? If the EB CLI can do it, there should be a mapping with the API.\n. Anything the AWS CLI can do, the SDK can do as well. It's written over the same REST API description.\n. What is likely is that the --single parameter in the EB CLI is an alias for a package of parameters merged in to the resulting API call to the service itself, rather than a particular parameter itself. That's a good place to start looking, and I'm digging through the docs as well.\n. I'm sure it's possible, it's just a question of which API parameters need to be set. I'm going to try and get a hold of that team, or find the EB CLI's source code.\n. I think I found what you're looking for. The Elastic Beanstalk team keeps a list of option settings in their documentation.\nSo, in the #create_environment call, you'd include the following in your options hash:\nruby\nopts = {\n  # Other parameters relevant to your environment...\n  option_settings: [\n    {\n      namespace: \"aws:elasticbeanstalk:environment\",\n      option_name: \"EnvironmentType\",\n      value: \"SingleInstance\"\n    }\n  ]\n}\nThat is the relevant setting for what you're trying to do. I've passed this on as feedback to the Beanstalk team as well, to see if they can add a relevant example.\n. That should answer your question, feel free to follow up if you have any additional questions or issues. Thanks!\n. There's a mismatch in the comments about where ExpiresString goes in the hash order and where it is actually put. I'm not really sure why it matters though, at least in the context of the diff.\nIf we are deciding to care about this, we should probably add an assertion in the unit test that this order is ensured.\n. LGTM then\n. LGTM\n. Can you show me the exact code you're using to call SNS? The hash above mixes Ruby hashrocket syntax with JSON hash syntax, so it's hard to tell where the error is coming from.\n. Haven't heard back, can you show me the code you're using, or a similar piece of code to reproduce your error?\n. Closing - feel free to reopen if you have more information.\n. You can absolutely call any API from an EC2 instance. A best practice for this would be to have an IAM role for the instance which gives you the permissions you need to make the API calls that you need to make.\nThe SDKs (Ruby and the others) will look for credentials from the EC2 Instance Metadata service, so credentials configured in this manner will be picked up automatically.\nDoes this cover what you need?\n. That's possible so long as the EC2 instance has routing/security group permissions to hit the CodeDeploy API endpoint, and you have credentials configured.\nAre you trying to run a particular piece of code and having problems, or looking for advice to get started?\n. The SDK documentation goes through configuration options for getting credentials for your client, including from EC2 Instance Metadata. Closing, feel free to add on any follow-up questions you have.\n. Thanks for raising this, will check with the doc team.\n. The inner implementation appears to be reasonable, but how would an end user use this? I don't believe we have users creating signer classes directly, so we would need to wire this up to configuration in some way.\n. Some other notes after a deeper dive:\n- In the Aws::Signers::V4 class, this will need to work in the sign method, and the whitelisted headers will have to plumb down to the signed_headers method.\n- This does not necessarily involve the instantiation of a new signer instance, in fact, it usually does not.\n- You may be changing how the internal interface works, so after we get this done we may want to look at if this is a minor version bump.\n. Looking at this as a probable bug. You're likely correct about the nature of the bug as well, and we've made fixes to similar issues recently.\nA full Aws.eager_autoload! should serve as a workaround if needed in the meantime.\n. Can try to reproduce, but it would be strange if this is happening despite Aws.eager_autoload! being run ahead of entering concurrency. If it was run after threads had been generated, then the error is possible.\nI did check and you shouldn't be having a problem with the partial autoload.\n. I am able to recreate this, checking if this is an invalid input or if we should be able to handle it.\n. You can execute concurrent workflows with SWF, check out their documentation. I'm not an expert on SWF itself, so if you get stuck you can also reach out to them via support or perhaps the AWS forums.\nIf you have questions about SDK interaction with SWF, feel free to follow up with those questions.\n. Taking a look - have you tried something like byebug to find the line in your code where things start to loop?\n. Are you running your application under concurrency? Aws.eager_autoload! avoids concurrency loading issues with the autoload method.\nThat said, I'll investigate to see if we can make those values thread safe despite autoload.\n. It does, but you can call it with only a subset of services as well. For example: \nruby\nAws.eager_autoload!(services: %w(S3 EC2))\n. This appears to be related to a JMESpath/JSON issue that has come up recently. A fix is being worked on for the jmespath gem (which we also maintain) and I'll update this space once it is released.\n. I'll reach out to the S3 doc team to check the documentation consistency. Thanks for the report!\n. What version of the SDK are you using?\n. What specific gem version are you using? This is a relatively new method, if you're using an older version of the SDK, even within the major version of 2, you may not have this.\n. Yes, I agree that's not a helpful error. I'll take a look at it.\n. Thanks for the documentation feedback - I can see the formatting of the shared documentation is confusing here, and we might need something custom.\n. It may be helpful to see your JMESPath version and Ruby version, given an ongoing issue we've had there. Either way, I'll try to replicate your issue.\n. Related to #1237\nThis means you haven't configured a region. To configure a region, set the AWS_REGION environment variable, or pass it in as a parameter. For example:\nruby\nkey = OpenSSL::PKey::RSA.new(1024)\ns3 = Aws::S3::Encryption::Client.new(encryption_key: key, region: \"us-east-1\") # or the region you are using\n. It's an unhelpful error that I'm aiming to fix, though it's something I can't consistently reproduce either so it's been slightly elusive. Sorry about the unhelpful error message.\n. Working on reproducing this now.\n. I ran an example, copying an object with a space to another object with a space, identical to your example, and it works in 2.3.21. Checking if there was a bug previously patched.\n. There was a bug, which was fixed in version 2.2.32: commit fixing this issue.\nWe had a bug where we didn't properly URL encode certain characters that required URL encoding in this specific usage path. Your SDK version contains that bug, later versions do not. If you upgrade your SDK version, you should no longer see this issue.\n. Good catch, thanks!\n. I think there's missing context here, because I don't see syntax errors in the Resources code itself. Do you have any context about the code calling this, or reproduction steps for the aws-sdk-resources gem?\n. It's absolutely feasible, though you have to use a couple steps. For a command line tool, you probably want to look at libraries like optparser to cleanly handle arguments.\nFor your use case, keep in mind that the S3 API supports prefixes only for a filter, so the rest you'd have to do yourself as an enumeration block. For example:\nruby\nbucket = Aws::S3::Resource.new.bucket(BUCKET_NAME)\nselection = bucket.objects(prefix: \"path/to/\").select do |obj|\n  # Do your code-style filtering here - for example:\n  obj.key.match(/[.]gz$/)\nend\n. One last note is that the above code essentially uses Aws::S3::Client#list_objects and the :prefix parameter therein. The resource abstraction was used to simplify. What you do with the selection return set above is up to you depending on what you want your application to do.\nLet me know if you have any other questions!\n. You're seeing two different errors here.\n1. us-west-2a is not a valid region identifier.\n2. us-west-2 is a valid region, but if you're getting a Aws::S3::Errors::AccessDenied exception back from the service, it means you do not have permissions to perform that #put_object request. You should check the IAM role that you're using for your client to ensure it actually has permissions to make S3 calls and that it has permissions to write to that bucket.\n. You're writing V1 code with V2 of the gem. The top level namespace in V2 is Aws rather than AWS, leading to the particular error you saw.\nHere is what you should do, as a refactor of the above:\n``` ruby\nrequire 'aws-sdk'\ns3 = Aws::S3::Resource.new(region: \"us-west-2\")\nbucket = s3.bucket(ENV['S3_BUCKET'])\n```\nDo not hard code your credentials in your code. We have a variety of ways that you can get credentials into your client, including environment variables, a shared configuration file, and EC2/ECS credential providers. You should use whichever of those approaches is most appropriate for your application setup.\n. Correct, I do not see a pagination definition for that method, but the :next_token member is present in the response.\n. I'm definitely a fan - small comment on the messaging, but otherwise LGTM at a late night pass.\n. LGTM - I think I see some possible edge cases, though I don't think they're likely to ever happen in the wild. We can discuss that offline (and I may be wrong), but that won't block this change.\n. Thanks for the PR, we will take a look!\n. Yep, it will go out with the next release. Thanks!. If you're setting ENV within the process, it may not be recognized as we check at initial SDK load time.\nTo be clear, which failure mode are you seeing?\n1. Your client has AssumeRole credentials, but the role was assumed using the wrong profile credentials.\n2. Your client is using static credentials from your config file.\nI'm going to suspect 2, but want to be sure so I'm checking the correct use case.\n. Actually wait, I'm sorry, I misunderstood. I was looking at the behavior of the default credential provider chain, your example is something different. In your file above, you're not using the Shared Config logic for assuming a role, you're calling Aws::AssumeRoleCredentials directly and providing your own profile.\nIs the script above the exact (or approximate) script you're trying to debug?\n. Is there any reason not to just use the default credential provider chain? For your example above, if you ran Ruby with the ENV variable already set (not set in code), this should work:\nruby\nclient = Aws::S3::Client.new(profile: \"account2\", region: REGION)\nSubstitute S3 for whichever client you need. This will follow the config resolution path for AssumeRole, and will provide automatically refreshed credentials.\n. Starting with the base class used by this system: Let's say you wanted to assume a given role_arn with source credentials in profile account1. You'd make the following call:\nruby\nsts_client = Aws::STS::Client.new(profile: 'account1', region: REGION)\ncredentials = Aws::AssumeRoleCredentials.new(\n  client: client,\n  role_arn: role_arn,\n  role_session_name: \"mysession\",\n  serial_number: mfa_serial,\n  token_code: token_code\n)\ns3_client = Aws::S3::Client.new(credentials: credentials, region: REGION)\nIf you start from the basic usage pattern, sourcing values from config is a matter of parsing that config source. Currently, the method used by the default chain is behind a private API, so I can take it as a feature request to expose a method to directly create an MFA Assume Role call with config values.\n. Added to feature request backlog.\n. Three part answer:\nFirst, the workaround. Make sure that the AWS_SDK_LOAD_CONFIG environment variable is set, for example to \"1\", and your code should work. That's the feature flag environment variable, and setting it will get you back on track.\nSecond, why this happened in 2.4.2. The 2.4.0 and 2.4.1 releases had a code bug where we ignored the feature flag and always used the new credential resolution chain, which is why your code worked without a feature flag. Technically, the new credential resolution chain is a breaking change, hence the feature flag.\nThird, what might happen. We're discussing if we should remove the feature flag, since the breaking change surface area is low. That discussion is ongoing (and we're open to input).\nFor now - set the environment variable AWS_SDK_LOAD_CONFIG and you'll be working again.\n. Sure, I can leave this open.\n. #1263 creates an opt-out flag, and makes this behavior on by default. When merged, it will be released as version 2.5.0.\n. We're happy to take a PR for this, and will also consider as a feature request.\n. Added to feature request backlog.\n. We're happy to take a PR for this, and will look at this feature request.\n. What you're seeing is us wrapping errors from your HTTP client, which we then retry.\nThis error indicates that Ruby's OpenSSL was unable to verify the peer certificate while establishing an SSL connection to Amazon S3. This particular error is very common when your system does not have a correctly configured openssl library with certificate. For example, this happens on all Windows installations of Ruby.\nAs a work-around the SDK ships with an optional SSL cert bundle. Browsers frequently bundle their own certs for the same reason. You can opt-in to use the bundled cert by calling:\nruby\nAws.use_bundled_cert!\n. It sounds like you've had a discussion with Trevor about this, we'll take the retry on as a feature request.\n. Added to feature request backlog.\n. Thanks for the research on this. What you're doing appears on the surface to be correct, but I have to admit I'm not super familiar with CloudFormation details, so I'll review this with my team to make sure we're not missing something. It's very possible that this was just a design oversight.\n. I've merged the PR (thank you!) and it will go out with the next release. I'll push it upstream so that other SDKs can pick it up as well!\n. No changes on our end. I can't reproduce this, so it's hard to say why you're seeing this without a wire trace of the call. The bucket also doesn't exist (as of the time of writing), so I don't think it's the same issue we saw before.\n. If it's consistent over a period of time, run a wire trace of your call. For example:\n``` ruby\nrequire 'aws-sdk'\nresource = Aws::S3::Resource.new(region: 'us-east-1', http_wire_trace: true)\nbucket = resource.bucket('healix')\nbucket.exists?\n```\nThis will output a wire trace of the client calls made, and this would allow you to verify if we are making a well-formed request. If we are, then the issue is with the service. If we aren't making a valid request, that's a bug.\n. Trimmed parts of that - interesting that you got exactly that error, I don't see anything off-hand that's broken. However, I can take this to the service team to take a look.\n. After a discussion with Trevor - are you getting this error on a freshly created bucket?\n. It looks like you're getting some edge case behavior that has to do with recent creation/deletion of the bucket. If the bucket is recently deleted, but did exist in a different region, you'll potentially see the 400 instead of a 404.\nThis does raise an interesting question about if we can handle this edge case in the #exists? implementation (treat 400 + no x-amz-bucket-region), but it should be VERY rare, and may not always be semantically correct.\nI'm going to take this as a feature request (looking for a clean way to handle this edge case), but we may not be able to fix this particular issue. Let me know if you have any other questions or concerns.\n. Will take a look at that, thanks. Added to feature request backlog.\n. There's a failing test here, so I'll take a look when I get in the office on Monday - I want to get this fixed for the next release.\n. The root cause of the test failure was elsewhere - I've made a fix that I'll be pushing up shortly. Thanks for pointing this out!\n. Ok, I'm able to recreate this on my end. Taking a look, sorry for the initial confusion.\n. Okay, here's the story - this is working as intended, with an unhelpful error message.\nThe first example you give, is sourced from a call to Aws::EC2::Client#describe_route_tables, which gives us the full suite of information for the route table, which we then populate the object with. You will notice, however, that #load isn't defined for that object either, if you try to call it directly, we can just hydrate the object with the response.\nHowever, the second example calls Aws::EC2::Client#create_route, which does not give us a response body - we can only hydrate the object with the keys used to make it, and know it was created. There doesn't currently exist a way to describe a single route from a route table.\nThis means that you'll have to step back to describing the full route table again after creating the route, to get the full properties of the object. Happy to share feedback upstream that an individual route description API would be useful, but that's all we can do.\nClosing this as we've answered, but feel free to follow up with any questions you still have.\n. Thanks!\n. This file is automatically generated, so a PR would just get clobbered with the next release. I'll raise this with the S3 doc team. Thanks!\n. Another suspicion I'd like to investigate with a few questions:\n1. Did you omit anything from your CLI command example above, such as a profile you've specified?\n2. Do you have a ~/.aws/credentials file?\n3. Do you have a ~/.aws/config file? If yes, which version of the SDK are you using?\n4. Do you specify credentials for the SDK client, or use the default provider chain?\nI'm suspecting that somehow the SDK and CLI are using different credentials, and the details of how we handle default credential provider chains would be related to how you're seeing a difference. This is especially true if you have detailed IAM permission documents for your roles.\n. Have you had a chance to take a look at your credential setup?\n. Closing due to inactivity - feel free to reopen or let us know if you get a chance to look at your credential setup.\n. Considering if this would be a breaking change, but looks good at a first pass.\n. I've resolved to include this for aws-sdk-core 3.0, as there are already some semi-breaking changes involved there. Now considering merging it in sooner as the surface area for issues seems to be low.. We will review this against V3. The upgrade from 2 to 3 promises no code breaking changes, that still holds, so adding this is fine.. We will need to open this PR again against V3 code, however, since the structure is a bit different.. In this case, the first example is failing the \"DNS Compatible\" check (to see if we can subdomain it), giving the trailing slash - we added the \"bucket\" after the base domain. The example, for that reason, is a bit difficult to make much of.\nAre there any issues this is causing for you?\n. Why not do a URI comparison if you're having issues with the trailing slash? For example, I think this would work (it works locally):\nruby\nu1 = URI.parse(\"https://s3.amazonaws.com/\")\nu2 = URI.parse(\"https://s3.amazonaws.com\")\nu1 == u2 # => true\n. Looking at the aws-ses gem source, they don't actually appear to use the SDK at all, it's all custom-rolled by them. I'd recommend reaching out to the maintainer of that package if you want to figure out why it's having issues with BCC - I couldn't see at a cursory glance at their package what the issue might be, but I do see BCC issues discussed in their closed issue history - it's possible upgrading the aws-ses gem will help, but I can't be sure.\nYou could also take a look at the aws-sdk-rails gem, which should work. If you see issues with that delivery method, feel free to add a note here, and to open an issue there.\n. As an aside, I'd be curious if the to: email address received the email. I'm aware that Rails by default will drop delivery errors, which means that if you saw a failure for a permissions/configuration issue it may not be immediately obvious unless you change your Rails configuration to raise delivery errors.\nIf you did receive the email to the to: address, this thought doesn't apply.\n. This is looking at first glance like a bug somewhere in the resource definition - when I add wire tracing, the tag set does appear to be present in both calls. I'm looking deeper in to this now.\n. Okay, I take it back - it's a bit more complicated than that, it appears.\nYour first call maps to describe_network_interfaces(), which includes the tag set in the response.\nYour second call maps to describe_instances(), from which we extract the network interfaces in the response.\nIt doesn't look like there's an easy way for us to rewrite the resource interface to support this member being looked up in this manner. I can do to the EC2 team and see if this can be added to their response shape.\n. Okay, so a workaround does exist, involving an additional call:\nruby\nni = ec2.resource.instances.first.network_interfaces.first\nni.load # Calls describe_network_interfaces\nni.tag_set # Now present in the object\nClosing this, I'll send up feedback to see if they can include this in the initial response. Sorry for the bad experience with this.\n. This is a bug in the AutoScaling resource definition, which we've identified and I'm pushing up a fix (this bug would exist in all SDKs) to go out with the next release.\n. Closing this since it's been inactive for about a month. Feel free to add a comment or reopen if you have more info.\n. We've pushed 2.5.10 and later 2.5.11, as there were issues fixing the 2.5.9 version. The fix here is to upgrade, sorry for the inconvenience!\n. I'm unable to recreate this:\nruby\nkinesis.get_shard_iterator(\n  stream_name:\"mystream\",\n  shard_id:\"shardId-000000000000\",\n  shard_iterator_type:\"AT_TIMESTAMP\",\n  timestamp: Time.now\n)\nThis is returning an iterator for me. What version of the SDK are you using?\n. Some questions, as I still cannot reproduce:\n1. Are you using exactly Time.now for the :timestamp value?\n2. Are you using something other than standard library Time?\nCould you give an example of the exact value output by Time.now, and that same variable with :to_s called on it, in your system?\n. Glad you got this working!\n. Can you tell me how this problem is manifesting for you? If you're referring to CloudFront fetching from your bucket as the origin, it shouldn't have any effect.\nIf you're having a different issue, perhaps with headers, knowing exactly what problem you're having would help me debug this.\n. Good catch, thanks! I'll push this upstream as well.\n. Identical to issue #1058 - your instinct is correct on this one.\n. You can stub the status code as well, and doing that will give you the outcome you're looking for.\n. No problem, sorry for the confusion!\n. Can you give me a reproducing example of this? I'm unable to replicate this based on what you've given me. What version of the SDK are you using?\n. That makes sense, glad you got things working!\n. Do you have a more extensive stack trace for this? I can't reproduce this (running the same code in a substantially similar environment works fine).\n. I don't actually see the error you're getting in the stack trace, but I'm guessing that this is a 500 error of some sort, and we exhausted retries. Your unusual latency probably is related, as a 500 error would indicate some sort of issue on the service side.\n. Is there anything different about your system at the point where latency spiked? Were you at a higher load?\n. That's interesting - I'd be curious to see a wire trace, because errors with the Aws::Kinesis namespace should be associated with a 500 error response from the service itself. They could be related, but that's not certain. If you want to go this route:\nruby\nclient = Aws::Kinesis::Client.new(http_wire_trace: true)\n. You could catch the error and inspect the HTTP response details, but there isn't necessarily an easy way to do this for rare+transient errors.\nGiven that this looks like either a networking issue on your end, a service issue, or some combination of the two, reaching out to support is also still an option.\n. Looks good, thanks for the contribution!\n. Thanks for the investigation here - will dive in to this.\nI suspect that your guess around thread safety is the right one. In which case, calling Aws.eager_autoload! (which accepts individual services as parameters if you do not want to autoload the entire SDK) should fix the symptoms.\nIf you try this workaround and continue to see this error, please do let me know.\n. Interesting - thanks for spotting that!\n. I'm noticing the :to_json calls embedded at multiple points. Is this intentional? Perhaps you could pull out the generated JSON string (anonymizing as needed) to verify. This looks like either a usage issue or service issue on the surface.\n. Added to feature request backlog.. So you're saying that on an EC2 instance, you're seeing this periodically? How is your environment set up?\n. Is the Ubuntu version you listed your development environment, or an EC2 AMI? How are you configuring your clients in code?\n. Okay, it looks like this may be related to how Sidekiq initializes - you might need to do eager_autoload within your Sidekiq workers, rather than only in the Rails initialization. Can you try that?\n. For what it's worth, we're working on an overhaul of the SDK which should reduce or eliminate these autoload issues - message definitely received that they cause problems when threading.\n. It looks like this went out with a previous release. Let me know/reopen if you see this again after looking at configured retries/timeouts. Thanks!. It's hard to pin down without a wire trace - did this happen only a single time, or does this periodically recur?\n. Is this in a threaded setup, or deleting objects sequentially?\n. I followed the link to RubyGems and the Source Code link appears to point to this repo.\nIs this possibly for an older version?\n. It is unlikely to be a bug, it is more likely that they are older snapshots. The client is just making a request to the service and translating the response, so if you're concerned that you're getting an invalid response, it could be worth reaching out to the EC2 team via a support ticket.\n. The response isn't directly the collection of snapshots. You want to do this:\nruby\nresp = ec2.describe_snapshots.(owner_ids:['123456'])\nresp.snapshots.each do |snapshot|\n  p snapshot.snapshot_id\nend\n. Let me know if you have any further issues or questions!\n. For some clarification (I forgot this), the list of snapshots includes shared snapshots (notice the \"amazon\" value for :owner_id in those snapshots. That's where the other snapshots were coming from.\n. It looks like the stack trace (relating to your code) is out of date. Have you been able to isolate a smaller failing example?\nI'm looking at this in parallel.\n. Okay, I did get a smaller reproducing example:\nruby\nstub = Aws::ECS::Client.new(stub_responses: true)\nstub_service_response = {:services=>[{:service_name=>\"TEST_TARGET\", :service_arn=>\"arn:aws:ecs:us-east-1:1234:service/TEST_TARGET\"}], :failures=>[]}\nstub.stub_responses(:describe_services, stub_service_response)\nstub.wait_until(:services_stable, { cluster: 'cluster', services: ['job'] })\nWill take a look.\n. Okay, I got it. Your stubbed service response lacks the :deployments shape in the response. The waiter is raising because it doesn't know how to handle the malformed response. You need to modify your stub like so:\nruby\nstub_service_response = { services: [{ service_name: \"TEST_TARGET\", service_arn: \"arn:aws:ecs:us-east-1:1234:service/TEST_TARGET\", deployments: [{desired_count: 1, running_count: 1}] }], failures: [] }\nThat will work. I understand the confusion here, we don't tend to do validation on response shapes since the service doesn't require it - but it does open the possibility of confusion when stubbing. Sorry about that!\n. We validate that the fields you stub exist, but we do not validate that you have all fields needed - it's just not a concept we've modeled.\nThere is a way to track parameters in stubbed client calls, but it's a bit of a hack (I'm considering alternative approaches as a new feature). We do this in aws-record: Take a look at the :api_requests variable, and how it is used in these tests.\n. You can cut a ticket via AWS Support to see if a rate limit increase is possible.\nAs far as managing this, you can have your code rescue this exception, add a backoff period, and retry.\n. It's also worth noting that if you see a throttling error, it's also already been retried by the client. If you need to hit that API in parallel, you could modify the default :retry_limit in the client options: http://docs.aws.amazon.com/sdkforruby/api/Aws/ElasticLoadBalancing/Client.html#initialize-instance_method\n. Closing this for now, feel free to add a comment if you have any further questions on this.\n. We don't have a built in rate limiter class - if you get a throttling error after retries, it's recommended to have backoff logic in your app as appropriate. You can also increase your limits with AWS Support.. The default credential provider chain forks between the ECS Credential Provider and the EC2 Credential Provider. If putting credentials on the instance fixed the issue, my suspicion is that the ENV variable isn't present in the container.\nCan you double check that the credential provider feature flag is on?\n\nOn Oct 5, 2016, at 5:30 PM, Malek Ascha notifications@github.com wrote:\nI'm using the sdk to interact with s3. I have my application running in a container on ECS. When I assign an IAM role directly to the task that my ruby app is running on, it fails to detect the credentials and I get an access denied whenever I try to use the s3 client. It works fine when I bind the role to the EC2 instance that it's running on.\n\u2015\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub, or mute the thread.\n. Documentation is here.\n\nThe SDK is looking for the AWS_CONTAINER_CREDENTIALS_RELATIVE_URI environment variable. If you're able to use EC2 instance metadata credentials, then that ENV variable is not present.\n. Did this resolve your issue, or does the issue persist?\n. You're correct that :retry_limit exists with a default of 10, and is only overridden if you override it manually. I believe you are correct that the throughput error is only surfaced after retries have been exhausted.\n. Is your issue that you're having difficulty with testing the code, or just that you want to verify the behavior in the Aws::Plugins::RetryErrors plugin?\n. You can turn off retries entirely by setting :retry_limit to 0. The https://github.com/aws/aws-sdk-ruby/blob/master/aws-sdk-core/lib/aws-sdk-core/plugins/retry_errors.rb plugin source code shows the types of errors that we retry, and you may have to consider how you would handle those cases.\nYou can also get the number of retries from the response context of any call, much like the logger class does. See https://github.com/aws/aws-sdk-ruby/blob/master/aws-sdk-core/lib/aws-sdk-core/log/formatter.rb#L134 for how we pull that out of a client response.\n. Closing as it seems the question is answered. Feel free to reopen or add a comment if you have more questions!\n. I'm looking at this, and it seems to still make the extra calls when inspecting stack data. It doesn't look wrong, but does this address the linked issue for you?\n. Okay, I take that back, it looks like when I pulled down this branch to test, it wasn't actually taking in the diff.\n. In any case, that looks good then. I'll merge this now, and push it upstream. Thanks!\n. This looks like a direct response from the client, without us doing anything to the output. Can you try running with a wire trace?\nruby\nclient = Aws::AutoScaling::Client.new(region: 'us-east-1', http_wire_trace: true)\n. Looks good, but will need to rebase one more time due to the CHANGELOG updating.\n. Sorry, this looks good and I'll merge/push up the change.\n. I've integration tested this against an accelerate-enabled bucket that I own. It is hitting the correct endpoint and returning responses successfully.\n. So, I had some interesting results when trying to replicate this. :to_hash is definitely implemented for the response object, but it may be doing something clever which doesn't work. For example:\nruby\nresp = iam.list_users # gives the error NameError: undefined method `to_hash' for class `#<Class:#<Seahorse::Client::Response:0x007fc8832b5400>>'\nresp.to_hash # does return a valid hash\nHave you raised an issue with the awesome_print maintainers?\n. Thanks for this, merging in.. This sounds like it may be related to your networking settings. Have you tried switching to multipart uploads for larger files?\n. Well, if you can't upgrade Ruby and the AWS SDK (V1 is end of life), I'd recommend looking at multipart uploads for larger files, since the threshold seems to be important.\nClosing this as it seems like it's been resolved/nothing we can do. Feel free to comment if you have any other questions.\n. This is similar to #602 - you can directly call the #create_multipart_upload client method, and upload parts one by one with #upload_part (you will need to know the part size at each stage), and conclude with #complete_multipart_upload.\n. Sorry, you're right about this. I've made a fix such that the next release will re-introduce the fix (as well as a second PR that was also squashed upstream).\n. Added to feature request backlog.. It's interesting that the object implements, but cannot handle, #rewind. I appreciate the issue here. This comes up in retry scenarios mainly, so I'm going to look in to if we can just surface errors as a non-retryable exception.\nDo you see this intermittently, or constantly?\n. It looks like our docs failed to refresh. It does appear to be there now: Doc Link\n. I think it may be correct that both work now that more robust bucket redirect logic is in place, but we'd like to do some verification before accepting this PR. I do agree that if we find anything, that note should be updated, and removed if we don't find anything.. It sounds like this could actually be a question for the service team, as we're almost certainly not touching the pkey itself for a client call. I'm going to reach out to them.\n. I've talked to the SNS team, and they confirm that you will need to strip those parts of the pkey file out to get a successful API response. http://docs.aws.amazon.com/sns/latest/api/API_CreatePlatformApplication.html\nI have shared your concerns with them as feedback.\n. I hear you and I've pointed them to this issue. Nothing we can really do on the SDK side, as mentioned above this operation doesn't touch your inputs beyond wire protocol changes.\n. Going to go ahead and close this for now, but feel free to reopen if you see this again after upgrading to the latest version.. Let's reopen and take a look.... It's just interesting because Trevor's original comment about the mutexes seems to still apply. If you have some sort of scenario that can reliably recreate this (if possible) that may help.\nAdditionally, how frequently are you seeing this issue currently?. This is an issue where I'd encourage an upgrade to V3 of the SDK. It is generally better with concurrency, and while I can't guarantee it fixes this issue, it should make it a lot easier to narrow down with the more detailed stack traces.. Closing soon if this remains inactive, I'd love to know if V3 doesn't resolve this.. Let's reopen and take a look then. One other thing that may be relevant, can you tell us a bit more about how you're doing concurrency in your application? It may help us to attempt to reproduce your issue.. Added to feature request backlog.. I think you're right that a fundamental change to the algorithm as a default might be an issue (though it might not), we might be able to expose that in other ways. You can, of course, already bring your own backoff block.. Added to feature request backlog. Will definitely take as a PR as well.. The default credential provider chain will work, provided that you're using at least version 2.3.22 of the Ruby SDK. As Trevor pointed out, please do verify that you've turned on the ECS Credential Provider within ECS, because we only attempt to use those credentials if they're provided by ECS.. Closing for inactivity, feel free to reopen if you have more information.. Do you have permissions to the bucket? This error would indicate that the bucket exists but that you do not have access to it, or that the object exists and you don't have access to it.\nThere isn't a way to determine if an object you do not have access to exists.. Closing for inactivity, feel free to reopen if you have more information.. I'm able to recreate this, taking a look.. It looks like this changed with PR #1216. What's interesting is that the PR above seems to create the correct HTTP response, with metadata in the header instead of in the response body (which obviously would not exist with a #head_object call). I'm going to check if there's a deeper issue here.. Self Note: This doesn't appear to be broken for actual calls to the S3 endpoint (thankfully), checking for differences in stubbed and end-to-end responses.. The responses seem to be the same at most levels, the issues occurs somewhere deep in the XML parsing stack.. It appears that body parsing of the stubbed XML response for #head_object causes the issue here. It's clobbering the metadata value exclusively when body parsing happens.. Additionally, this issue only occurs for #head_object, and not for #get_object.. I've cut a PR that fixes the symptoms of this issue, but I'm going to have it reviewed before shipping, as there are a couple open problems. In any case, thanks for raising this.. We've pushed a fix for this that will go out with the next release.. I'm actually noticing as well that the method appears to be missing, despite it being present in the API model. I'm investigating, but since your comment implied \"no longer exists\", can you tell me what version you used where it did exist? In either case, investigating.. Okay, something else interesting is that this varied across machines for me, so that's another thing to check. I do see this method on a new install of the gem.\nWhat version of the gem are you using, and if you're in a Gemfile, what is your full set of current gems?. Docs are indeed generated/updated with each release. No worries, glad you got what you needed.. Okay, I see what's going on here after some investigation. The service is the source of these errors, and will throw this error if you provide that attribute name to a queue that does not have FIFO enabled.\nI'll pass your feedback on to the service team, but there isn't anything we can do from the SDK side (and indeed, we may not be able to change this now as it may be a breaking change to move from exceptions to returning false).. Added to feature request backlog.. Added to feature request backlog.. Can you tell me a bit more about the problem you're trying to solve, maybe with a reproducing example? Tests are breaking, but I can review this a bit better with more context.. We're working on this, there was a gem name collision.. This has been resolved, we have that gem now, and you should be able to install the RC gems. Please let us know if you see more issues!. That is a good callout - I see that the commits are there, but the tag as you say is in the wrong place. I'll fix this tomorrow with another release for the new features in 2.6.34, thanks for letting me know.. Version 2.6.35 is being released now and will resolve this issue. Thanks for the report!. Any particular reason we want to change the version here? I'm more inclined to add Ruby 2.4 as it comes out rather than extend into additional point changes unless there's a particular change to concern ourselves with.. I'm going to decline this PR, but I will make a change to try and remove the 2.3.0 point and test 2.3 instead. That should solve the problem you're mentioning and would be more consistent with how we test other Ruby versions. Thanks for bringing this to our attention!. Unfortunately, V1 of the SDK is deprecated and we are only addressing security vulnerabilities going forward. However, I would look at a PR for the issue in V2 of the SDK, if the issue presents the same way (I'll take a look at a repro test for that).. Could we add a failing test for this? If we're loading wrong, I'd like to also protect against regressions.. That test helps, and it does indeed error without your changes. Thanks!. Use the :user_agent_suffix option, like so:\nruby\ns3 = Aws::S3::Client.new(user_agent_suffix: \"example1\")\nec2 = Aws::EC2::Client.new(user_agent_suffix: \"example2\")\nIt will add your suffix to the end of the user agent string.. I can't reproduce this, the class definitely exists when I run this version of the SDK.\nHow are you trying to create a client?. Taking a look at the PR to raise it up to the cross-SDK code.. I've merged the PR and this should go out with the next release. Thanks!. Thanks!. Socket errors are generic to a number of root causes, this error message is generally intended to help with misconfigured clients hitting a non-existent endpoint or region.\nIn any case, this PR looks good. Thanks!. I was able to get your example to work with a slight change:\nruby\nclient_stub = Aws::S3::Client.new(stub_responses: true)\nclient_stub.stub_responses(:list_buckets, {\n  buckets: [{ name: 'my-bucket' }]\n})\nresource_stub = Aws::S3::Resource.new(client: client_stub)\nresource_stub.buckets # Note that this won't actually make a stubbed call, but buckets.first works.. I'll check the documentation to see if we could improve clarity, but let me know if this works for you or if you have further issues with stubbing resources.. So, consider the client calls being made by your resource calls. In this case, you would stub the response to :list_objects. It gets complex if you are checking multiple buckets, but if you'll only call it a single way, you could stub the response to :list_objects to similarly return those keys as you did for :list_buckets.. Thanks for the update, glad things are working well now. We do have an entry for this in the developer guide. Let me know if that would have helped as well (and we can make sure it's more prominently linked to).. Closing as the issue at hand is resolved, but still interested in your feedback on the guide.. One concern we have is that we can't do such a release if it breaks Ruby 1.8 support. While Ruby 1.8 is end of life, so is V1 of the SDK, so there's a fine line we have to walk.\nOne possible workaround while we look at this would be to just fork the V1 repo, make the changes you need, and vendor that gem or cut a purpose-built release. Since V1 is end of life, it doesn't really create a sync issue unless there is a security issue down the line (since the end-of-life concern does not apply to security patches).. In fact, it looks from the JSON gemspec that the most recent version requires Ruby 2.0, which means that for V1, dropping the JSON version lock would break a non-trivial number of customers if they upgrade.\nV2 gets around this by not requiring anything outside of the standard library to be present as far as JSON goes.\nSo, I don't think we can cut a new release, but happy to leave this open for a bit to discuss mitigation options.. We have too many Ruby 1.8 and Ruby 1.9 customers, as well as V1 being end of life, so I'm closing this as \"no plan to fix\". V2 should hopefully avoid this kind of problem (and does appear to) since it only depends on the Ruby standard library.\nFeel free to add any further questions you have, if you have questions or issues with mitigation strategies.. This change has numerous failing tests - I'll take a look at the issue w/r/t Ruby 2.4's release, but I can't merge this PR as-is.. I don't think I want tests to be detecting Ruby versions in this manner. The tests may only be passing because this code isn't hit, besides.\nI'm taking a look at Ruby 2.4 compatibility concerns and I'll keep an eye on this PR for guidance as I do so.. Thank you for pointing this out - I'm addressing this in a separate branch/PR to solve this (and any other issues I can find) for Ruby 2.4.. Looks good at a first pass - I'll run the integration tests and give a final review when I get back to work on Tuesday. Thanks!. I'll look in to that question since I'm now wondering if we have similar issues hiding elsewhere in the code.\nFor now, looks good and merging. Thanks!. Thanks for this!. Thanks for the suggestion - we need to look at if there would be any complications with other services, but I agree that experience isn't optimal.. Added this as a second reference to an existing Feature Request.. V2 approaches multipart uploads differently, so there are a couple of options.\nOne is to use the managed file upload helper.\nThe other is to use the relevant client operations directly:\n #create_multipart_upload\n #upload_part\n    * #upload_part_copy if using an existing S3 object as the source\n #complete_multipart_upload\n #abort_multipart_upload if needed\nI'd generally recommend using the managed file uploader, unless there's a specific reason it doesn't work for your use case. If it doesn't work for your use case, do let me know if there's a feature we can add that would help make multipart uploads easier for your use case.. I've passed that on to the doc team, I can see where that page would be confusing.\nI also see your point on the need for examples outside of file uploading. Seems like a dev guide candidate and I'll pass that on as well. Thanks!. Scrapping as this has gone off original intent. Will replace with a refactor to handle 2.4. You have the right understanding, and optimizing resource memory usage is something we could take a look at. I'd recommend for the #describe_images API that you use the :filters parameter to keep the response size down. The #images method on the resource supports these same params.. Added to the feature request backlog.. Sure - the EC2Provider is for EC2 Instance Metadata Credentials, and is the final step in the default credential provider chain. That IP is the local source of the Instance Metadata service on an EC2 machine.\nIf you're seeing an error involving that class, it is most likely that you created a new client with no credential source configured. How are you running in to this error?\nIt is also likely that you're writing new code - is there a specific reason to be using the deprecated V1 SDK in your use case?. Closing for inactivity. Feel free to reopen if you have more information or questions.. I believe that you should be able to run #upload_file in parallel just fine - you could have multiple threads each running #upload_file in parallel, in which case you'd be mostly constrained by your network throughput capacity.\nLooking at the Travis issue, it is correct that we don't currently have a sync command for uploading multiple files at once with a single call, but if it's just the performance question, you can parallelize multiple file uploads.. Let me know if this answers your question, or if you have more questions - thanks!. Closing for inactivity. Feel free to reopen if you have more information or questions.. Self-status update - all tests pass locally with the exact same gem set as the failing Travis tests, so next steps are to deep dive on possible platform differences.. So, this sounds like this boils down to adding custom signed headers to a presigned URL, that are unrelated to the S3 call itself, is that right?. Thanks for bringing this up - happy to take a PR, otherwise will look at altering the resource definition and passing that up to the other SDKs.. Please do also update us if you find something that fixes your local environment, in case this comes up again. Thanks!. Great catch! Would you like to open a PR, or would you like me to apply the fix? I think your fix is sound (though my PR review would be to check if there's any meaningful difference between #dump and #generate just to be safe).. Thanks for the PR, I've merged and this will go out with our next release.. I'll definitely take a look at this. It's a bit more involved to set up and repro than the usual issue, so it's taken a bit longer to set aside the time. Happy to take a PR in the meantime, of course, but I agree there should not be an inconsistency.. Indeed, that does not appear to be a waiter we have implemented. Adding as a Feature Request.. I see where it would be useful for your case, but we think it may involve more than just an extra parameter to allow you to force HTTPS, since this logic is used in other code paths. Can take this on as a feature request.. This looks good, thanks! Will add this to the next release.. So, keep in mind here that #public_url is a method we provide in the resource interfaces, as a convenience. Implementation is like so:\nruby\ndef public_url(options = {})\n  url = URI.parse(bucket.url(options))\n  url.path += '/' unless url.path[-1] == '/'\n  url.path += key.gsub(/[^\\/]+/) { |s| Seahorse::Util.uri_escape(s) }\n  url.to_s\nend\nThe thing to illustrate here being that there's no magic in constructing the URL, you could build this behavior yourself over client responses. There are two choices here:\nFirst, you could just create a resource object and use the method there:\n```ruby\nThis is the client operation you can that completed the multipart upload described above\nclient.complete_multipart_upload(bucket: bucket, key: key, upload_id: upload_id)\nobj = Aws::S3::Resource.new(region: region).bucket(bucket).object(key)\nobj.public_url # Use this\n```\nSecond, you could use the fact that you know your exact bucket name to do it yourself. Note that with this approach you may need to be aware of things like URI escaping:\nruby\nclient.complete_multipart_upload(bucket: bucket, key: key, upload_id: upload_id)\nurl = \"https://#{bucket}.s3.amazonaws.com/#{key}\"\nThe first approach might be cleaner if you're worried about edge cases, the second approach is mainly shown as an example of what is going on at a high level.. Does this help you get what you need? If not, I can adapt this approach based on how your code works. Adding this to the client directly seems potentially problematic.. We're providing what the service response is providing. It appears that :delete_marker is only ever populated for versioned objects. For example, in a versioned bucket:\nruby\ns3.put_object(bucket: bucket, key: \"test\", body: \"test\")\nresp = s3.delete_object(bucket: bucket, key: \"test\")\nresp.delete_marker #=> true\nThat said, nil is a falsy object, so you should be able to treat it as a boolean.. Good point, it does look like we had this in V1. Taking this as a feature request, I'll take a look at copying over the old solution.. As in the other issue, thanks for the PR and will review.. Thanks for already making the PR for this, will look at that.. #938 was the source of the line in question - making a note here as part of the regression testing.. Looking at the relevant commit and the current state of the code, it's possible that the setting of port 80 and the setting of the scheme to http may be redundant, which would mean the PR is ok. Still setting up a test environment.. This commit added the http scheme to the code.. Okay, there does appear to be a problem which makes this issue a bit more complicated than the included PR. Consider the original purpose of this code snippet, which is to switch the URI to http. For your use case of a non-standard port, your PR is fine:\nruby\nuri = URI(\"https://foo.bar:3000\") #=> #<URI::HTTPS https://foo.bar:3000>\nuri.port #=> 3000\nuri.scheme = \"http\"\nuri #=> #<URI::HTTPS http://foo.bar:3000>\nuri.port #=> 3000\nHowever, it doesn't work for the standard HTTP/HTTPS ports - that explicit setter is needed:\nruby\nuri = URI(\"https://foo.bar\") #=> #<URI::HTTPS https://foo.bar>\nuri.port #=> 443\nuri.scheme = \"http\"\nuri #=> #<URI::HTTPS http://foo.bar>\nuri.port #=> 443 - wrong\nThat doesn't mean we can't fix this issue - I'd like to support your ability to set ports, but we need to do a bit more. I'll copy this comment to the PR.. This appears to be redundant with #1403 - I'm going to close this and review that.. Update on this - I'm trying to get an environment set up to properly test this. I believe that the port 80 explicit mention may have been for a specific bug, and I would want to avoid a regression of the same. We only have an integration test around this currently.. Okay, there does appear to be a problem which makes this issue a bit more complicated than the included PR. Consider the original purpose of this code snippet, which is to switch the URI to http. For your use case of a non-standard port, your PR is fine:\nruby\nuri = URI(\"https://foo.bar:3000\") #=> #<URI::HTTPS https://foo.bar:3000>\nuri.port #=> 3000\nuri.scheme = \"http\"\nuri #=> #<URI::HTTPS http://foo.bar:3000>\nuri.port #=> 3000\nHowever, it doesn't work for the standard HTTP/HTTPS ports - that explicit setter is needed:\nruby\nuri = URI(\"https://foo.bar\") #=> #<URI::HTTPS https://foo.bar>\nuri.port #=> 443\nuri.scheme = \"http\"\nuri #=> #<URI::HTTPS http://foo.bar>\nuri.port #=> 443 - wrong\nThat doesn't mean we can't fix this issue - I'd like to support your ability to set ports, but we need to do a bit more. I'll copy this comment to the PR.. Indeed, a topic of discussion in our team right now is to potentially move a lot of this logic into a single class to avoid exactly these kinds of inconsistencies. Let's change the PR to use this kind of approach and then we should be good to go.\n\nOn Feb 1, 2017, at 8:35 AM, Klaas Jan Wierenga notifications@github.com wrote:\nThis is a better way of fixing this issue I think. The solution was there all along in sign_but_dont_send which applies exactly the same technique.\n\u2015\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub, or mute the thread.\n. I actually don't see this clearly documented, and I'm not sure we would want to expose it that way.\n\nCan you tell me a bit more about your use case here? What API version are you trying to lock to and why?. So, this would be a somewhat substantial feature request, as our current approach is to not keep older API versions around unless a breaking change exists (in which case there is a proper version update, such as what we did with ElasticLoadBalancingV2).\nI'd recommend a review of your testing approach to not be reliant on the API version value in the request body being consistent, if at all possible.\nI do appreciate the challenge here, so if you can tell me a bit more about your testing approach I may be able to provide advice or alternative approaches.. I'll take a look to see if I can try and reproduce. Are you seeing this on the SDK's automatic retries, or on retries within your code?\nIf the latter, you should look for anything where you might be mutating your items hash, as that seems the most likely culprit. I'll investigate just in case the SDK is doing something odd on retries, but this is unlikely.. Actually, I think I see it now. It looks like you're taking the raw unprocessed items hash, and plumbing it right back in, and the simple attributes helper it making it a map one level deeper.\nI'll work on an example of how you can avoid this later, but you can also avoid this by turning off simple attributes at the client level.. I'm taking a look at that now, because you should be right about that.. Okay, I ran code substantially similar to your example above, and had no issues - the items were put in the format you would expect (with simple attributes ON). Looking now for differences in behavior.. Question: Are you able to reproduce this on demand? If so, it would be interesting to see the response shape you're getting back from unprocessed items. For example, if you captured it in byebug to see if it matches what you would expect.. That's interesting, I can try to recreate the error by using the retry keyword.. So, I made a simple example to try and recreate this, and I'm not getting the same issue - retries are as expected:\n```ruby\nrequire 'aws-sdk'\nrequire 'securerandom'\nrequire 'byebug'\ndef writer\n  client = Aws::DynamoDB::Client.new\n  1000.times do\n    batch = []\n    25.times do\n      batch << {\n        put_request: {\n          item: {\n            \"uuid\" => SecureRandom.uuid,\n            \"int\" => rand(1000),\n            \"map\" => {\n              \"m1\" => { \"a\" => rand(25) },\n              \"m2\" => { \"b\" => rand(25) },\n              \"m3\" => { \"c\" => rand(25) }\n            }\n          }\n        }\n      }\n    end\n    @retry_flag = false\n    begin\n      if @retry_flag\n        byebug\n      end\n      resp = client.batch_write_item(\n        request_items: {\n          \"BatchModel\" => batch\n        }\n      )\n    rescue StandardError => e\n      @retry_flag = true\n      retry\n    end\n  end\nend\none = Thread.new { writer }\ntwo = Thread.new { writer }\nthr = Thread.new { writer }\none.join\ntwo.join\nthr.join\n```\nWhen this eventually fails and I hit the byebug component, batch is as expected and the retried call works.\nI can try to dive a bit deeper in to your code, but so far I'm still unable to recreate the issue.. We also have a bit of a hunch as to a possible source. Checking that now.. Okay, I do see a possible bug source here. The code works under normal circumstances, but it's possible you're getting an unusual error, like perhaps a network error, under production load and our plugin stack is consuming the mutated hash for a built-in retry.\nIn our Aws::Plugins::DynamoDBSimpleAttributes class, the param hash gets mutated. If we go through the full stack, this mutation happens in both directions and there's no problem, but it looks like in some cases we might mutate it twice.\nStill investigating.. Quick note - this is the top priority bug we have remaining, but it is proving rather hard to repro for failing test purposes. Still working on it.. If you have a small reproducing example, possibly with wire trace examples, that would help. Still investigating even if that is hard to get.. Sorry about the delay here - I couldn't repro with that script, but actively picking this up again.. Okay, I think we got it. Going to try and get this out with the next release.. Please feel free to reopen if you see any recurrence of this issue after pulling down version 2.9.3 (yet to be released, but that will be the next version number).. Sorry about the delays here, we've got bandwidth to catch up on PRs - and this looks good. I'm going to work on getting the merge conflicts handled to pull this in.. Merged via a parallel commit. Closing this - thanks!. We're going to add that paginator, hopefully to go out with the next release. I'll close this issue once that new paginator is in place.. We can use a separate issue to discuss that.. Interestingly, this appears to be related to whether or not a paginator is defined for a method. If a paginator is defined, we're populating nil, otherwise a stub value. I'm looking in to why this is.. That's very possible, though the inconsistency in stub behavior is interesting regardless.. This seems to be expected behavior (though we're looking in to it). It occurs because it's marked as a \"payload\" member, which leaves the response member as an IO type. I can see that the documentation isn't reflecting this, so at minimum that's something we should update.. This appears to be an error from the service, not from the SDK. It isn't immediately clear to me from CloudFormation's documentation if there's a distinction.\nIf you're stuck, it may be worth raising a ticket with AWS Support or asking a StackOverflow question. But as far as I can tell, this is a question for the service teams.. As you found, you'll get that error if the default credential provider chain doesn't find credentials. If you expect that no network calls should occur in your test environment, as for unit testing, stub clients won't look for credentials and won't ever make HTTP requests. You can make one like so:\nruby\nclient = Aws::S3::Client.new(stub_responses: true). Based on the relevant documentation, I don't think this is likely to be an SDK issue per se. One approach could be to validate (by printing the generated JSON in a test environment) that you're creating the JSON you expect, and that it lines up with APNS docs.\nOur Getting Help section of the README has some helpful links. If you think your JSON is well-formed, you could file a ticket with AWS Support. In either case, you might find similar issues and resolutions on Stack Overflow.\nIf you work through this and think there's a specific SDK issue, feel free to reach out or reopen.. You're right that we don't have the top level method for this in the resource abstractions yet, as far as I can see. We're happy to take a PR for it, otherwise can take on as a feature request to add.\nInterestingly, there may be a bug in the Aws::RDS::DBClusterSnapshot object, as it requires the cluster ID itself. I'll look further in to that as well.. I can pretty easily add a top-level Aws::RDS::Resource#db_cluster_snapshots method which accepts a snapshot ID to filter down to a collection of one, but it could be a breaking change to add the direct resource creation with only a single parameter. I'll make a branch for the WIP Resource PR.. I think what would be useful here is a wire trace. For example:\nruby\ncloudfront = Aws::CloudFront::Client.new(http_wire_trace: true) # plus other options\nMy suspicion if you're getting nil is that the service itself isn't returning the value, which becomes a different question.. There's some room to ask why XML bodies are smashing the header value, when the API model clearly says metadata comes from the headers. This list could also be extracted or generated via other means, potentially.. Which version of the SDK are you using, for reference? We do have tests to ensure encryption and decryption works across SDKs, so I'd like to reproduce your issue.. Thanks for that, we'll look in to this.. It looks like this may be an issue with the Java SDK, as that metadata doesn't seem to be used. We're tracking it (you don't need to file a new issue with the Java SDK repo), but just wanted to update.. We're working on a fix to the Java SDK currently. I'll update when there's a Java update to consume, then we can close this issue.. The AWS SDK for Java has released version 1.11.96 which fixes this issue on their end. Upgrade your Java SDK and you should no longer have this issue. If you do, feel free to reopen. Thanks for the report!. V1 of the SDK is end of life, so unfortunately we cannot accept further API updates to it as we won't be doing releases unless there is a security issue.\nKeep in mind, you can use V1 and V2 in the same project, and upgrade piecemeal for calls like this. Let me know if you have any questions about how to do that.. The waiter is built on top of #describe_stacks. It only accepts the :stack_name parameter, but you should be able to use stack_id as the value for that parameter per documentation.\nIf you still have problems, feel free to reopen.. This makes it look like you have an older version of the model, since these are client-side validation errors.\nWhat version of the SDK are you using?. This should work in version 2.6.24 and up.. I'm unable to recreate this locally. I'm actually thinking you may want to try reinstalling the SDK, just on a hunch.. I think that's most likely. I've seen this about one or two other times (different services/params each time), and that's always fixed it. There's possibly an investigation in to if JSON-based code gen makes this kind of issue more likely, but that would be mooted by the upcoming major version bump of the SDK.\nClosing this for now, if you see any kind of recurrence feel free to reopen.. Short answer is that the response stubbing plugin does not introduce latency, however, you can make a tweak in order to introduce your own simulated latency. Let me cook up an example.. You can create a custom latency handler, in which you have the latency behavior you desire:\n```ruby\nclass LatencySimulationHandler < Seahorse::Client::Handler\n  def call(context)\n    sleep(5) # Replace this with your latency logic, this would slow down each response by 5 seconds\n    @handler.call(context)\n  end\nend\nstub_client = Aws::S3::Client.new(stub_responses: true)\nstub_client.handlers.add(LatencySimulationHandler, step: :sign, priority: 0)\n```\nThis will introduce your delay to each call. You could also call out to some other logic that determines the latency behavior you want. Does this help?. Going to close this for now as I think it answers your question. Feel free to reopen if you have issues later on.. Glad you were able to sort that out. If you have issues with your test configuration, you may find resources like Stack Overflow in our Getting Help section of the README useful.. This isn't an SDK issue, Amazon S3 is seeing increased error rates in US-EAST-1, which is no doubt why you're seeing paperclip issues. https://status.aws.amazon.com/\nI'll leave this open until the issue is resolved in case others come to report the same thing.. This issue is resolved, so closing this. The related issue will be a separate discussion.. The main issue is that there is no good way to distinguish this error from other causes of networking errors. I'm open to ways we can wrap this that are non-breaking (since other types of network errors definitely have occurred for customers), but I'm not sure off the top of my head what that would look like.. Thanks for raising this. I think these requests make sense and I'm happy to either take a PR, and/or treat this as a relatively high priority feature request.. This will go out with the next release.. I'm not sure what this question refers to, can you elaborate?. This sounds like a service usage question, could you elaborate a bit more on what you're trying to do?. #927 and #983 talk more about this. SigV4 signatures can't go beyond 1 week. I have added a note to the relevant documentation.. Here's a repro assuming the service returns as described:\nruby\nec2.stub_responses(describe_instances: [{reservations: []}])\nr = Aws::EC2::Resource.new(region: \"us-east-1\", client: ec2)\nr.instance(\"i-123456789\").state.name\nThat returns the junk error.. I can see that the EC2 resource definition seems to take for granted that reservations cannot be empty. How we can wrap this with a more user friendly exception in a general way is an interesting question.. The #update_service response appears to have the value you need, would that make more sense?. Okay, I do see what you're saying now, and looked more into the documentation. I don't see a filter parameter for #list_tasks, so you may need to describe the tasks to check their definitions. Given that this is a service usage question, check out the getting help section of the readme - Stack Overflow users across multiple languages may have a preferred approach for this.. We are considering supporting this, it may also be worth noting that AWS_PROFILE works similarly and I believe is also supported by the CLI.. Adding to feature request backlog.. Those are stubbed values, but I do think this is a good case for an example snippet which would help remove this kind of confusion.. I'll reach out to the service team about this and get back to you. If you have an AWS Support account, I recommend opening an issue there as well.. It may also help if you can provide a code snippet showing what you used to do (feel free to anonymize details), and of what you're trying to do now. Thanks!. So the Aws::SNS::Client#set_topic_attributes method seems to be the only way I can see - especially where the CLI has the same issue. This is a feature request for SNS, and I'll let them know but you should also contact support.\nI would advise some caution about using #set_topic_attributes, in that it would make it pretty easy for you to mangle your \"Policy\" attribute and leave it unusable.. We haven't implemented region fetching from the EC2 instance metadata service yet, no. Adding this as a feature request.. Adding to feature request backlog.. Do you have a small code snippet to recreate this? May also be able to advise how to use :stub_responses for your use case.. This is related to #1664 root cause, and is a service issue that we are working on with them. As for differences in console behavior, it's possible the console is pre-populating default values for you. For example, if there is a prefix field, and it's left blank, a blank string is a valid prefix for put_bucket_logging - it is not the same as null. I do agree that this should be more clear in the API documentation and we're pushing the service team for that update as well.\nClosing this as well, since next actions are underway and outside the SDK. Sorry for the ongoing churn with this.. That is strange. Can you provide the wire trace for this, and provide a code snippet that recreates this for you? To turn on wire tracing, add this to client creation:\nhttp_wire_trace: true\nThis error message is in response to a particular kind of error thrown by the service, so I'd like to gather info.\n\nOn Mar 22, 2017, at 5:56 AM, Mathias Klippinge notifications@github.com wrote:\nHello,\nI am getting a bit confused by this warning:\nS3 client configured for \"eu-west-1\" but the bucket \"\" is in \"eu-west-1\"; Please configure the proper region to avoid multiple unnecessary redirects and signing attempts\nOne the one hand it doesn't make sense to warn when something is X and you expected X, as X == X. But I cannot easily ignore a warning. Any clue what might be going on here?\n\u2015\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub, or mute the thread.\n. This would be happening because our wrong_sigv4_region? check looks at the error code (which expects the 400 you're getting), and then for if the response includes region information, which your error response does.\n\nI think our next steps would be to look at if we can refine that check, or if we can in some other way expose the underlying error to avoid confusion.\nTo be clear: Your error is the expired token. Which could be a separate issue if you're using a refreshable credentials provider and having these issues.. Thanks for the heads up - this is indeed the condition under which we would perform a V1 release.\nI think given the extreme rarity of V1 releases, we can simply drop Ruby 1.8 support promises in a new version (and provide release notes guidance).\nInvestigating and will release once we come up with a plan.. Sorry, I've been pulled around a bit this week and haven't had a chance to do this yet. Going to work on this today/Monday.. We're actively working on this, and will release once we've tested on the new Nokogiri versions.. Here's the plan:\n\nWe're currently planning to cut a release on Monday.\n\nIn the meantime, if you fork the repo and change the Nokogiri dependency to ~> 1, that will work in Ruby versions after 2.1.0.. I've pushed the change to master, release pending.. Gem 1.67.0 is pushed, and removes the Nokogiri version lock.. Do you have multiple operations against CloudFormation occurring across processes/threads while running this? I can check what those API rate limits are, but we do generally work to account for that and this waiter actually has a much longer delay than some other waiters in the same waiter collection.. That would explain it - it's possible that between all of your processes, you're hitting the API limit. Unfortunately, the built-in rates can't account for the fact that API limits are across different call sources. There are a couple workarounds you can do:\n\n\nThe wait_until documentation shows how you can modify the polling period and maximum retries. You could set longer polling periods and find a rate that works for your usage rates.\n\nYou can also implement retries around your waiter calls, starting a waiter over (perhaps with an even longer polling period) when you encounter a throttling error.\n\nClosing because I think we've gotten to the root of it. Feel free to add any follow-up questions you have.. One potential issue is that silently wrapping that error at this point would be a breaking behavior change. Anyone relying on those errors to be surfaced would instead potentially punch through their throttle limits.\nIt is an interesting question to see if we could expose the ability to expand the set of errors we retry on an opt-in basis, but not by default.. Something to help clarify (I gave a confusing response earlier) - our standard retry handling already retries throttling errors. If you're seeing the error, we've already retried multiple times and failed our retry limit before surfacing the error. Hence the recommendation to handle the throttling error by expanding retry periods. You do have a couple more advanced options to consider in your client configuration:\n\nSet your own :retry_limit. Our default is three. Set as you desire, but I don't think we'd want to retry indefinitely as you could create a never-terminating waiter.\nYou can also provide a custom :retry_backoff lambda, if you want to bring your own retry backoff logic.. There are some discussions within the team as to how we can integrate the default credential provider chain with MFA - it's a bit of a difficult issue since it requires an interrupt or extra handler to consume the MFA token value.\n\nIn the meantime, Aws::AssumeRoleCredentials should work for MFA roles.. Adding to feature request backlog. I'm definitely open to a PR which adds a method that does this (and takes the MFA token as a param).. Looking at the API definition, the input shape for the next token is :marker, and the output token from the service is :next_marker. This does break the usual convention, but our pagination definitions seem to handle this:\n ElasticLoadBalancing Paginator\n ElasticLoadBalancingV2 Paginator\nYou should be set if you change the code above to:\nruby\n   params = {page_size: max.clamp(20, ELB_DESCRIBE_MAX), marker: marker}\nbegin\n  response = elb_client.describe_load_balancers(params)\nrescue => err\n  event.child_event(\"elbs_for_marker #{marker}: #{err.inspect}\", 'error')\n  return nil #stops future execution\nend. You could also use the built-in pagination functionality.\nClosing as I think we've got the answer here, feel free to reopen if that doesn't fix things for you.. I think we need some more information to help with this - there's nothing in the poller code inherently that would seem to be vulnerable to unplanned pauses.\nIn what stage is the poller stalling? Are you just not seeing processed messages over a period of time? How do you know it is stalling instead of just a period of time where no messages are received by the poll call?. I see what's going on here. The block is only triggered when messages are received. If you think that messages should be received but aren't, you could add wire tracing to your client. See the Aws::SQS::QueuePoller#initialize method.\nFor example, you could make your QueuePoller like so:\nruby\nclient = Aws::SQS::Client.new(region: REGION, http_wire_trace: true)\nqueue_poller = Aws::SQS::QueuePoller.new(QUEUE_URL, client: client)\nIf you're getting wire trace output during the \"down\" period, then SQS isn't receiving messages. If you're not getting wire trace output at all, that's more interesting. Try that and see which it is. But at a glance, there may not be an issue here.. We're happy to take a PR, but in any case this would be a helpful feature request to take on.. This looks reasonable, thanks!. Can you elaborate more on the case which \"works fine on all methods except ones with required options\"? Would like to document/test that if taking this PR.. Given the transient nature of this, I'm suspecting a cause other than the SDK. The SDK does fetch credentials on expiration or when we see an expired token exception (then it will retry).\nI'd recommend filing a ticket with AWS Support if you have ongoing concerns, but also feel free to reopen if this occurs again and we might be able to help.. Just to check, what do you get from calling Aws::VERSION in the same pry instance?. I definitely don't see anything in that usage example that should be an issue. Going to work on repro with Ruby 2.4.. This definitely looks like a side effect of #1487 - and the best possible workaround seems to be what you've already discovered, to require the rest of the cgi library as needed.\nHowever the exception is very unhelpful...considering if we should return the old require pattern.. Okay, given that this seems to be a Ruby 2.4 change in cgi, it's probably easiest to just patch that now be reverting to the full CGI import now, and figure out what they changed later.. On second thought, the bug label is inappropriate given that the weirdness seems to center on Ruby proper (consider opening an issue with them), but I'll change the import strategy regardless.. I've pushed a fix to this that will go out with the next release.. I'm sufficiently confident that this build failure is not related to this change, given that master has flipped from passing to failing independent of any change. While we pin down that issue, we can continue with this.. It would seem, despite the patch version bump, that ox is the most likely failure candidate due to the failures being in the XML tests. Investigating that first.. Local testing is failing now even with older versions. Expanding the investigation.. I can confirm that, code wise, this issue now propagates all the way back to 2.9.0 and 2.8.0. Going to need to keep searching for which dependency is now breaking, as older, passing builds don't have an obvious explanation in the gem set.. Nevermind, after revising some local testing settings, discovered the regression is in ox 2.4.13, or in how we consume it.. Opened an issue in the ox repo.. The reasoning for this is that we have a central source of these JSON files, and need to keep them in sync with service teams. In the case of api-2.json and docs-2.json, we cannot accept pull requests. Instead, we would reach out to the relevant owners for updates. That's what we've done here. I do appreciate your efforts and I'll push to get this supported ASAP.. Shared examples are written in a cross-SDK way, which wouldn't take simple attributes into account. We'll need to see how we can adapt those examples to look like they would in a call with the :simple_attributes setting on.\nThanks for the report!. Stay tuned on this one. I don't have a release date for this, but I'll pass on the request.. When you're using SharedCredentials directly, it expect the access key and secret key to be set. It's meant for loading static credentials from config only. You have two choices to get the behavior you want:\n\nUse the default credential provider chain. Aws::S3::Client.new(profile: 'dev', region: 'eu-west-1') should work with the credential files described above. The only caveat is that if you have defined environment variables with credentials, those would be loaded first using the default credential provider chain.\nUse AssumeRoleCredentials directly. Loading your credentials directly from shared config in this mode is an outstanding feature request, but it will call assume role on your behalf.\n\nI'd recommend the default credential provider chain approach.. That does sound like another vote for the \"integrate AssumeRoleCredentials with shared config\" feature request.. Can you provide a wire trace of your call? When you create a client or resource, just add a new parameter:\nruby\nclient = Aws::S3::Client.new(http_wire_trace: true)\nresource = Aws::S3::Resource.new(http_wire_trace: true)\nThat will help us diagnose the discrepancy.. I had to rebuild some of the commit history, and it looks like that may require a rebasing of this branch. Taking a look.. Fixed. Sorry about that.. How difficult would it be for you to reopen this PR against master? I'm actually having a hard time now with the fact that the diff has gotten big enough that GitHub essentially crashes when I try to view it. I am doing a PR cleanout now, so I will have bandwidth to speed through a review.. This is a limitation of Signature Version 4, you can't extend the signature expiration time out further than that. Here's the comment on this from an earlier issue #1345 \n\"The limit in the v2 SDK is not flexible. The version 2 SDK uses AWS signature version 4. Sigv4 has the hard requirement that signatures are never valid for longer than 1 week. The v1 SDK uses an older signature version that does not have this limitation. While it still works in many circumstances, there are more and more operations, regions, and parameters are invalid/unusable unless you use signature version 4. Choosing to use an older version may come back to bite you in the future.\"\nSee also #927 . Possibly? I think it depends on what you're trying to do, but I haven't spent a lot of time around that side of CloudFront personally. I'd recommend some of the resources in our Getting Help guide to answer this question.. My immediate guess would be that they are related, and the same workarounds apply. The modular SDK release is the long term fix.\nWill leave this open a bit to see if those workarounds work for you.\n. Going to go ahead and close this, but feel free to reopen if workarounds don't improve the situation.. Thanks for the heads up - going to write a patch for this.. Thanks for the report! This will be retryable with the next release.. Curious why you're double-escaping the \\n character - I haven't looked at email protocols in a while but if you're looking for a newline why not do this?\nruby\nresp = @ses.send_raw_email({\nraw_message: {\n    data: \"From: user@domain.com\\nTo: user@domain.com\"\n  }\n})\nI think your blockstring variant would encode that way.. Well if they're both bombing out, that's definitely interesting. Your mailtext string above just renders as \"From: user@domain.com\\nTo: user@domain.com\\n\".. Ok - glad that fix worked. Also, thanks for reporting because the documentation shouldn't be steering you wrong like that.. I think a bit more clarification is needed on what you're trying to do. I can see two approaches - one you can do today with some code, and one that is not possible:\nIf you want to get presigned URLs for everything in a bucket at a given point, that's not difficult:\nruby\nclient = Aws::S3::Client.new\npresigner = Aws::S3::Presigner.new\nurls = []\nclient.list_objects(bucket: BUCKET).each_page do |page|\n  page.contents.each do |obj|\n    urls << presigner.presigned_url(:get_object, bucket: BUCKET, key: obj.key)\n  end\nend\nIf you want a presigned URL that gives you some sort of navigable, updating interface, the problem is that that's stacking multiple operations, and isn't something presigned URLs can do - they are just a presigned request to S3. For example, you'd need to generate new :get_object URLs somewhere with credentials.\nIf you're trying to go the latter route, I'd recommend looking at tools that help make this easier, like the AWS SDK for JavaScript's browser SDK.. Given your requirements, it seems like the JavaScript SDK approach is the right one. Or you could make the S3 objects public and potentially reduce the amount of server-side URL generation you need to do, but I suspect that's not the goal here.\nWhat bringing in Cognito does for you is to help solve the authentication problem. Presigned URLs aren't really designed to do what you're trying to do, so server side generation would be the alternative.\nClosing since I think we've gotten to the answer here. Feel free to let me know if you have any follow-up questions.. I'm actually inclined to accept this. Normally we don't because we source this from shared models, but:\n\nFixing it is on the backlog.\nIt's just a documentation change, so no harm if it reverts.. This appears to be an issue with your RubyGems setup, rather than the aws-sdk gem. Likely related issue.\n\nI'd recommend you upgrade your version of RubyGems and see if that fixes the issue for you.. I think this may have to do with the general recommendation not to use sudo with RubyGems, but I'm not sure what is specific to your environment.\nI've found a Stack Overflow question which seems to address your exact issue. I'd look through those answers and see which options work best for you.\nClosing as I think we've found the exact cause of both issues, feel free to reopen if you go through troubleshooting and see something SDK-specific.. At first glance, looks like an oversight when adding the SharedConfig layer. Hence the bug-nature of this.. Releasing this fix as 2.10.0. It's a minor version bump since it has an effect on the default behavior for some customers, potentially.. A refactor is reasonable, can review as a PR.. I'm not finding a history of this type of issue with V1 of the SDK. One thing to keep in mind is that V1 is end of life, so while we can try to point you in the right direction, there won't be any further code updates to that library, even for bugs (except security issues that may arise, which we will still patch).\nIf you have any stack traces where an error is occurring, that would be helpful.. I may also recommend looking in to the Getting Help section of the wiki, as Stack Overflow is another source of support, if community members have seen this issue before.\nOf course, my biggest recommendation would be to switch over to V2 of the SDK, which is actively supported, to see if that resolves your upload issue.. This looks good, but before I can merge, a quick piece of administrivia:\nCan you confirm: \"By submitting this pull request, I confirm that my contribution is made under the terms of the Apache 2.0 license.\". Same administrivia on this one, while I rerun the failed test case can you confirm: \"By submitting this pull request, I confirm that my contribution is made under the terms of the Apache 2.0 license.\". I'm unable to reproduce this, and to be honest not sure what you're referring to. Can you refresh my memory on what you're expecting to find?. It would be useful to see the output of the following:\nruby\nclient = Aws::CloudFormation::Client.new(region: 'us-west-2')\nclient.describe_stacks(stack_name: 'dev-test-vpc')\nFeel free to anonymize as needed, the question is if the service is returning the values you're expecting. If it is, then we may need to look at the model. If it is not, the issue does not lie with the SDK.. That's definitely a possible issue, that would be along the lines of \"if the service is sending it over the wire but we don't render it...\" that was my leading guess.\nI'll try to get an update about what the service team is doing about this.. Actually, I do have an update and forgot to update this - this value should be present in the latest version of the SDK. Try upgrading and see if this works for you now.\nClosing since it's added to the API, if you continue to have issues after upgrading feel free to reopen.. I'd recommend going to Stack Overflow for help with how to use React with Rails. Take a look at our Getting Help section in the README.\nIn any case, it looks like your upload route is just rendering an existing object, which wouldn't make sense for a drag and drop upload. I don't have the React expertise to put that together, but Stack Overflow should.\nClosing in favor of that route, if you find an issue with the SDK in the process feel free to reopen.. This has been inactive - closing, feel free to reopen if you have wire trace info.. I don't see any issues off-hand in the kitchen-ec2 usage of the waiter. I don't have enough information from this though to provide an answer with certainty, I'd need wire trace info of what responses the service was providing.\nPerhaps some background would help: Was this spot request successfully fulfilled in the end, or did it fail?. If you have the error code, that's helpful, and yes, we can take a PR on the waiter.\nDid the spot instance request eventually succeed, despite the waiter failure? I'm also trying to understand what the final state of this waiter polling should have been.. There is a chance of regression depending on how the roles are configured. If an ECS container role is present, we will use it and not use EC2 instance role, even if the EC2 instance ECS is running on has a role.\nIf the roles are the same, the outcome will be the same. If the roles differ in permissions, then it is possible switching could introduce auth errors for operations you don't have permissions for in the ECS role.\nDoes that answer your question?. The other question, of course, would be which version of the aws-sdk gems you're using. The ECS container role feature would have to be present in your version of the SDK to be picked up.. Yup, I can't speak to the behavior of a custom credential provider chain. Thanks for following up!. The ones you've changed have a \"single source of truth\" outside this repo, to ensure the SDKs are in sync.. Thanks for the report on this. This is definitely not intended behavior, and at this point looking at this as a bug.. In your example, could you show me the output of r.inspect? I can't replicate this issue.. Could you try running the following, so we can try to debug the response you're getting?\nruby\nec2 = Aws::EC2::Client.new(region: 'us-west-2', http_wire_trace: true)\nec2.describe_host_reservations\nFeel free to strip anything sensitive out, but seeing what the service is returning will help us pin down where any issue may lie.. Actually the request ID would be super useful (and are unique to the request), in case we need to involve the service team. We don't need any of the other IDs. If you prefer, you can email the request ID to alexwood@amazon.com as well.. Yes, I have it. Looking to see if I can reproduce the issue.. Another question that came up - are you running this code in a threaded environment?. Additionally, can you provide your full Gemfile.lock? Wondering which XML parser you are using.. Great! That would potentially lend some credence to the idea that some autoload issues were taking place, since once of V3's big changes is to cut that off entirely. Thanks for the update!. Just to be clear, are we seeing a particular failing case, or is this just a concern that the error message implies that only symbols are accepted? Want to make sure we are fixing the right thing.. Thanks - if we don't have a behavior issue (as I believe Strings work as well as Symbols in this case), I'm going to prefer not making a code change. Feel free to reopen if there is an issue.. This looks like a networking issue. It's unclear from this alone if it's a networking issue from your side or the service side without more context. It may be worth reaching out to AWS Support in this case.. Though for some context, are you running on EC2 or from a local machine?. Going through and cleaning up old issues, this appears to have been a networking issue - the best avenue to continue to investigate this would be via AWS Support.. Thanks for the report on this, I'll investigate with the service team. I'd catch both exceptions as a workaround, sorry about this issue.. For some background, do you know when the exception type changed for you? It wouldn't be tied to an SDK version, it would be a particular date. Do you know how often (for example, through logging) you caught the previous exception type?\nI just want to be super clear that the exception we're getting has changed, and eliminate the possibility that you're encountering this exception for the first time.. Thank you, that's helpful.. Additionally, can you capture the exception(s) you're seeing currently using a wire trace? When we tried to reproduce we got Aws::EC2::Errors::SnapshotCreationPerVolumeRateExceeded, so if you're still seeing a different exception, that is very interesting indeed. To get a wire trace:\nruby\nec2 = Aws::EC2::Client.new(http_wire_trace: true, region: REGION)\nWith that setting, you'll output the wire logs, which we can use to see the response the service provided and in turn see how we generated the exception. If you're still seeing the new exception type, this will be super helpful. Thanks!. Of course, when posting wire logs, feel free to strip anything sensitive (we shouldn't need any of that to actually diagnose the issue). But, please do keep any request ID values, which we can use to directly see what happened on the service side.. Thank you for reporting this. The issue has been resolved on the service end. The correct exception is Aws::EC2::Errors::SnapshotCreationPerVolumeRateExceeded.\nSorry for the trouble, taking steps to help prevent it from happening again.. The Aws::Dax::Client documentation is correct, the SDKs broadly support the control plane APIs. Support for DAX as a database client to my knowledge only exists in Java today. I'll pass this on to that team as a feature request.. I don't know - the Dax team would certainly announce any new SDK launches, however.. Can you provide a wire trace for this?\nruby\ns3 = Aws::S3::Client.new(http_wire_trace: true). Thanks, and I've been able to recreate this issue as well. Investigating whether this is a client or service issue, since I don't think we've touched the relevant code in some time.. I've reached out to the service team for help debugging this as well.. Thanks for the note - I don't think that's a factor since I reproduced the error without a dash.. Sorry, no updates yet. Will revisit after reInvent, need to collaborate with a few parties on this one.. May have a breakthrough here, as it appears S3 sometimes throws these errors for general usage mistakes, rather than a malformed XML request. Checking.. Potentially related Issue #1670\nI've passed this information on to the S3 team and will look for a new update.. To be entirely honest, I'm not sure. The documentation doesn't indicate that the call is improper, but I can't eliminate that possibility either. As far as I can tell:\n\nThe Ruby SDK is properly formatting your request.\n\nThis same error has appeared elsewhere in the case of user error, though of a different kind.. @admorphit That's actually very helpful. Do you have a related issue, or a JS code snippet to reproduce that you can provide? Thanks!. Okay, I've identified the root cause. As suspected, it's user error, though granted, the requirements here are not well documented (which is feedback I'm pressing with the service team).\n\n\nThe :target_prefix parameter is required. This is why you're getting an error.\n\nYou'll also likely need to set grant permissions, but at least for that you'll get a much more user-friendly error and the way to do that is documented.\n\nSo, it's again much like the other issue, where MalformedXML is being returned in place of a user-friendly error message. This should unblock you - closing, but feel free to add any follow-up questions you have.. I agree with you entirely - documentation across the SDKs comes from the service teams, and I'm pushing for these updates across the impacted APIs currently. Once done, they'll be included in the standard release process.. > However, in the console, a prefix isn't required? Why is this an API limitation only for these SDKs and not the console? I'd like to be able to apply it to the entire bucket...\nIt is required. My guess is that leaving the field blank is translating to an empty target prefix, which is valid. A null target prefix is not valid. This should be better documented.. No worries, this is a very unhelpful error message so your frustration is understood and appreciated. We're pushing for a better user experience here.. That would be added in the caller - logstash. Unless logstash plumbs through your own parameters, in which case you would need to add it there.. @ramd123 I'd recommend opening an issue on the AWS SDK for Java repo - hard for me to debug any edge cases you may be running in to with that tool.. In this case, it is intended behavior that V2 and V3 cannot coexist. I do see where build automation processes breaking due to the same executable is an issue, and we can take a look at ways to alleviate that.. The executable conflict is not desired behavior, though also difficult to avoid.\nHowever, major versions 2 and 3 not existing in the same project is a tradeoff we've made consciously. The upgrading guide walks through the use cases, but essentially the major versions are code compatible, so you're only changing packaging to upgrade. Unfortunately, with modularization, if we allowed you to have V2 and V3 in the same project, there was no way to ensure the underlying implementations didn't collide and cause unpredictable runtime behavior, while still providing code compatibility.\nAs for libraries which lock you to aws-sdk ~> 2.3, can you point me to these libraries? We'd love to help provide an upgrade path for them.. The case of a library which is hard locking on old aws-sdk minor versions would raise other questions too, since that minor version is, I think, over a year old at this point and hasn't been receiving updates for new services and features for some time.. @alvises Can you tell me a bit more about that environment? I'm inclined at this point to rename the executable for V3 and end that issue.. Thanks for the info. Working on a fix now.. I've pushed a fix to rename aws.rb to aws-v3.rb and we are working on getting it out this morning. This issue has discussed two different topics (the executable conflict and the V2/V3 packaging decisions) but I'm going to focus on the executable question here.. The executable issue should be fixed as of aws-sdk-resources version 3.0.1, which has the rename. Closing this, but feel free to reopen if the issue persists.\nAs for the discussion of why V2 and V3 can't coexist in general, if we want to discuss that we should do so in a different issue to avoid conflating this specific issue with that discussion.. Correct, that will be in master soon, and it lives in this repo.. We're moving V3 to the master branch soon, we have to coordinate that change with some internal tooling. For now V3 lives in code-generation, correct.. Release tags are a more interesting issue now that we have modularized into 100+ gems. aws-sdk will probably be on 3.0.0 for a long time, it's the underlying service gems which will be updating. I'm open to ideas about how to add dozens of new GitHub releases/tags to this repo without it getting overwhelming.\nThe CHANGELOGs in the code-generation branch are current and will soon be in master. Similarly, the V2 CHANGELOG will continue to update once it has moved to a branch.. The use case you have will be interesting with V3. We actually will be having hundreds of CHANGELOGs, and users will be migrating to only using a subset of service gems relevant to their use cases. It's unfortunately impractical for us to create repos for each gem, so a \"single\" CHANGELOG doesn't fit well going forward. Happy to discuss what we do have, or if there are ways to make our multiple changelogs easier to consume.. Yep, that looks great. We're going to take on a task to add this to our generation process.. Version 3 is now on the master branch. Leaving open while we work on your PR.. I'm fine with this - I agree that we will delete this after we switch branches, but happy to bridge things for convenience. Let's keep the conversation going in the other thread as well.. This is awesome, thanks! We are going to look at adding this into our generation process, as you stated, so this PR may be on hold for a bit but I'm strongly inclined to do this in general.. I am able to reproduce this error. Taking a look.. You'll need to depend on the aws-sdk-cloudwatch gem rather than core, per the relevant section in the upgrading guide, then include require 'aws-sdk-cloudwatch. The aws-sdk-core gem in V3 does not contain service clients.. Closing this as the question is answered. Let me know if you have any follow-up questions or issues.. Just to help clarify, can you give me a reproducing code snippet?. In the case of the X-Ray clients doing automatic instrumentation, our team doesn't handle that. I'll pass on this feature request.. Looks good on a first pass, will wait for tests.. Aside: If you're actually referring to the aws-s3 gem, we don't maintain that. The aws-sdk-s3 gem is the official S3 gem we maintain.\nWith that gem, you can specify custom endpoints at your discretion, for example:\nruby\ns3 = Aws::S3::Client.new(endpoint: \"https://my.custom.endpoint\")\ns3.list_buckets # Or whichever operations you want to run.\nIf your endpoint is S3 API compatible, it will work.. Additionally, administrivia, can you confirm: \"By submitting this pull request, I confirm that my contribution is made under the terms of the Apache 2.0 license.\". I hear what you're saying, but unfortunately it's a bit difficult to automatically generate aliases on top of generated/shared resources, and maintain a strong backwards compatibility promise (name collisions are a risk, for example). You're right about the names varying slightly from standard Ruby idioms, but the fix is potentially more damaging than the problem.\nGoing to close this, let me know if you have any questions.. I'm going to echo the fact that I'm not sure we want to do this. Is it not feasible to use existing refreshable credential sources?. I'm seeing where the issue is here. This code reproduces the issue from scratch:\nruby\nsqs = Aws::SQS::Client.new(\n  region: \"eu-west-1\",\n  endpoint: \"https://eu-west-1.queue.amazonaws.com\"\n)\nqueue_url = sqs.get_queue_url(queue_name: QUEUE_NAME)\nsqs.send_message(\n  queue_url: queue_url,\n  message_body: \"Test\"\n)\nIt is true that, unless you're intentionally redirecting towards the legacy endpoint, you're not going to run into this issue unless copying the queue URL from the Python/CLI result when also choosing the legacy endpoint.\nThis would be a feature request to add the same sort of customizations to the Ruby SDK. There is probably a safe way to do it, but it would also not likely be at the top of the feature request backlog. I am open to reviewing a PR to add support for this.. Now that is definitely interesting. I'll investigate what's going on with this.. There is a shortcoming with MFA that you can work around by using Aws::AssumeRoleCredentials directly. What value are you setting for AWS_PROFILE?. Actually, the MFA case may still require client calls directly, it's on a feature request backlog across the SDKs.. It should actually give preference to the presence of the source_profile config value. For example, we have unit tests which specifically validate this.\nIt may help if you provide a redacted version of your shared configuration setup, and how you're constructing a client.. Interesting - I'm looking at the code now to see if there is an obvious fix, but it looks like this is hand-rolling its own credential provider chain. So what the SDK does or does not do with regards to role assumption wouldn't apply.. It also looks like the hand-rolled implementation here is using private APIs, and it's totally understandable that it would skip the SDK's later-updated logic around Shared Config completely. We did not implement assume role credential support in the SharedCredentials class.. @matt9949 are you also having this problem via kitchen-ec2, or directly? Trying to see if these are related issues.. That's fair, and the dev guide you linked should have a feedback form. I'm going to tag this as a Documentation issue for the moment so we can look at more clarity in our own docs.. It does look like kitchen-ec2 may have hand-rolled their credential chain to have AssumeRole support in a provider chain, before we implemented it (V2 did not ship with AssumeRole support in the credential provider chain, it was added later).. Closing for inactivity, please reopen if you have further information. Would be curious if this happens for other chef_gem installations as well, given what we see locally.. This looks like an issue where the underlying nature of the resource has been changed by API changes over time. Will take a look at this, the recommended workaround would be to use the client APIs directly.. LGTM. It's quite possible as well that this is a shortcoming with the resource model, and that we're unfortunately running in to it now. Migrating to client calls is the cleanest solution, though I definitely appreciate that this is a major overhaul, potentially. We're going to see if it's feasible to update the resource model, and if not, if it's feasible to customize this particular resource to match V2 behavior.. This has been resolved.. You're right, I misread this as being the option descriptions for API methods. My miss.. LGTM, thanks!. These changes look good, thanks!. It would be helpful to see how you've configured your custom endpoint in your client, to determine why you're seeing this issue. It looks like a standard networking error.. That was my instinct on this as well. The behavior the SDK exhibits is valid against S3 endpoints. Given that the issue here applies only to a custom endpoint setup, the workaround seems sufficient. Closing, let me know if you have any follow-up questions.. Looks good, some administrivia: Can you confirm \"By submitting this pull request, I confirm that my contribution is made under the terms of the Apache 2.0 license.\". Given that you're getting MessageAttributes correctly on other formats, it would indicate that the SDK is properly formatting and sending your request. Have you reached out to AWS Support? This sounds like a usage or service question, not sure what we can do on the SDK side.. The correct answer would probably be for the service to raise a more friendly error message, though we will work with them on that. The problem with adding this validation on the client level in general tends to be that if service limits change, you'd have invalid behavior, though given the limit size of 1000, we might be able to consider an exception in this case.. On further review, I think this is a case where the service needs to pass on more user-friendly errors, and we've given them feedback in that regard, especially due to related issues we've seen. Closing this as the action isn't something we can control, and when it happens, the SDK will behave differently automatically.. The problem is your code example has defined hsm twice, in different contexts. When you're calling it in the block, it's now a type shape, not the client. Here is code which will work:\nruby\nhsm_client = Aws::CloudHSMV2::Client.new(\n  :http_wire_trace => false,\n  :region => region, :credentials => @aws_cred, :logger => @aws_logger\n)\nbegin\n  resp_c = hsm_client.describe_clusters\n  resp_c.clusters.each do |cc|\n    cc.hsms.each do |hsm|\n      hsm_client.delete_hsm(cluster_id: hsm.cluster_id, hsm_id: hsm.hsm_id)\n    end\n    hsm_client.delete_cluster(cluster_id: cc.cluster_id)\n  end\nrescue => e\nend. You're correct that this is not yet supported. Definitely noting the interest here.. This work is in progress for version 3 of the SDK. Backporting to V2 will be prioritized depending on how simple/complex the refactor ends up being.. This has been added, and the follow-up PR for additional features beyond the scope of the issue may or may not be merged. Still, this is resolved.. Basically, that setting will tell you what we are sending over the wire in real time - we aren't keeping it for later display. However, you COULD inspect the HTTP request and responses after the fact. Consider:\nruby\ns3 = Aws::S3::Client.new\nbegin\n  s3.list_objects(bucket: \"nosuchbucket12345\")\nrescue Aws::S3::Errors::NoSuchBucketError => e\n  # You could look at the raw HTTP Request with:\n  e.http_request # This is a hash with body, headers, etc.\n  # And the raw HTTP Response with:\n  e.http_response # Again, a hash\nend\nThis should give you the ability to look in to a request which failed. If you want to change the http_wire_trace setting after the fact, for future requests, you can do so simply:\nruby\ns3 = Aws::S3::Client.new\ns3.list_buckets # No wire tracing\ns3.config.http_wire_trace = true\ns3.list_buckets # Will include wire tracing\ns3.config.http_wire_trace = false\ns3.list_buckets # Will not have wire tracing. This should cover what you're trying to do, and works the same in V3. Closing, let me know if you have any other questions.. We could also expose a way to pass options into Net::HTTP, for an alternative approach.. The request needs to be of the format shown in the documentation.\nIt looks like some of your request does follow the documented format, and other portions do not. In fact, I suspect as written your options would be ignored. Use symbolized, snake-case keys where called upon in the documentation (for known shapes - the arbitrary hash within \"options\" should use the format you already have).. Let me know if this helps, or if you have follow-up questions after reading the documentation.. It does look like, when I look at this on some other services as well, that the issue is validate_params expecting symbolized keys. It triggers on log_driver since that's a required param.. Just for a quick update, turning off validate_params is NOT a valid workaround per my testing. Essentially, the gem requires symbolized keys for parameters in both Versions 2 and 3. This does help with avoiding issues like duplicate keys for the same param, so figuring out a good way to work around this is not a quick fix.\nI'd be curious to know if you're actually getting a valid result when you're sending your requests over the wire with validate_params: false, or if the gem is accepting the input but the service is either missing your input or returning an error.\nTo do what the CLI is doing, we would likely need to create an explicit way to pass JSON parameters, rather than implying a non-symbolized hash which came from JSON is valid input to the client (which then opens us up to other issues since :log_driver != \"log_driver\").. What patch did you have in mind, if you were able to do that patching? I'd encourage them to symbolize keys which align with API parameters, since that's required behavior (and has been for the life of the SDK).\nI think the feature request to explore safe ways to make that more permissive is valid, but it's going to take some time to do safely.. There is a way we can do this, and it may work without param validation in some services, but not others, today. Depends on protocol. It's not yet something we support and validate with tests, so I'd use caution with turning off validate_params, or with using non-symbolized keys (using the JSON parser's ability to symbolize keys for you may be handy if consuming JSON).\nWe're looking at this as a feature request to look at how we could systematically support non-symbolized keys.. Yes, it should be.. To give some background, we have a task to investigate a safe way to consistently handle both symbolized and non-symbolized keys in our backlog. Likely, this would involve validation exceptions if the same key is provided twice.. If you want to pick up the region and credentials, you should try this:\nruby\nses = Aws::SES::Client.new(profile: \"automailer\")\nThat would pick up both your shared credentials and the region specification. The credentials object class is just that.\nIf you're unable to use the default credential provider chain (for example, because you also have environment variable credentials but don't want to use them here), you could specify the profile (for the region setting) AND set your credentials object directly as you did above.\nClosing, feel free to reopen if you have more issues or follow-up questions.. This is a pagination/prefix selection question. Do you have a snippet of Ruby code for what you're doing now?. We don't currently have a mechanism for including beta services in our SDK, we currently only support GA launches. Consider reaching out to the service team to see if they can provide you with anything for the preview.. gems/aws-sdk-s3/lib/aws-sdk-s3/resource.rb is a generated class, if you can take it out of the PR I can review (and add an item to improve the code generator). Good catch!. Thanks!. Looks like the underlying issue may have been fixed in subsequent ox versions (per tests passing), so this may now be OK. Going to try and confirm this.. Any updates on this? Would be especially interested in something long-term sustainable, as the :skip_white change itself was due to a patch version breakage.. I'm able to replicate this, working on a fix now.. Pending next V3 release.. You can do this:\nruby\nclient = Aws::IAM::Client.new(stub_responses: true)\nclient.stub_responses(:get_role, 'NoSuchEntity')\nclient.get_role(role_name: \"foo\")\nI'll leave this open while reviewing the documentation, but this should get you unstuck.. @Doug-AWS this could be a developer guide candidate (testing, stubbing, and error stubbing).. Closing due to inactivity, feel free to reopen when you have more information.. This is definitely an issue we're tracking, thanks for raising it here as well.. As a workaround, the equivalent documentation in V2 remains accurate as the versions are API compatible. I do agree that workaround is...not a long term answer.. This sounds reasonable, actually, as you would be going through the rest of the credential provider chain. I'd recommend setting dummy credentials, though I could also see this as a feature request to support unsigned requests.. Closing as the feature request of unsigned requests is already in the backlog.. This looks good, and per what's becoming routine, can you confirm: \"By submitting this pull request, I confirm that my contribution is made under the terms of the Apache 2.0 license.\". Can you explain more about what kinds of incorrect behavior you're seeing? These operations are just calls to a remote service, it would help to understand what you're seeing to know how to route this issue.. Can you show what the input is and possibly an example of what different languages you're seeing?. I'm curious what your usage pattern is for these objects. Are you getting this on your first request, or do you believe that these few-MB objects are just piling up and not being garbage collected to the point where you use up your process memory?\nIt's hard to determine from the information here if there's anything on the SDK end to do. It's especially interesting to see this be environment specific.. Try updating your version of the SDK, the latest versions will have that region's endpoint scheme.\nIf you do not wish to upgrade, you'll need to manually set the :endpoint of ec2.cn-northwest-1.amazonaws.com.cn as well as the region.. All looks good, we'll pull this in. Thanks!. Sorry for the silence, let me give an update. We're looking at this as a PR we intend to merge, however we're very limited on bandwidth at this point to give a proper review. It may be a week, or a few weeks, before we can review this. I can say it's fairly high on the priority list.. Now that I've been able to properly review this (deeply sorry about the difficult delays), I'm happy to accept this PR. Thank you again for having things documented and tested!. Not really, it seems to pass muster and to be honest a lot of the confidence comes from the integration tests (which I will run before formally merging to confirm). I think it's the right approach to offer this as a separate and new method.. As for the questions on memory usage, we've certainly had cases with our multipart support where performance tweaks were later needed. I'll take another pass with that in mind, but if it's functional and not causing regressions to existing methods, I'm more inclined to accept the PR.. Nothing is standing out as a red flag memory usage wise from what I can read of the implementation. Integration tests are passing for me, so I'm going to merge this and it should go out with our next SDK release!. Belay that, this is failing in JRuby (and not just the inability to install of the older version). I'm looking at why now.. Looks like this line is sometimes problematic in JRuby - I'm working on isolating why: https://github.com/aws/aws-sdk-ruby/pull/1711/files#diff-739272f1b1c14844fd5d13e3e702850aR108. Furthermore, in JRuby 9.1.6.0 where I was testing, this seems to be the offending line: https://github.com/jruby/jruby/blob/9.1.6.0/core/src/main/java/org/jruby/RubyIO.java#L4083. I'm going to take some time to try and debug this - but if I can't figure out the cause of the breakage, I'll need to revert this change, and create a new PR (or let you re-submit this PR).. There's some evidence from chatter about JRuby's implementation of IO.copy_stream which indicates it may be bugged.. https://github.com/jruby/jruby/pull/4093\nhttps://github.com/jruby/jruby/issues/5048. We do place importance on keeping old versions of Ruby/JRuby intact, we don't want to break customers or force upgrades whenever possible. I'll try dropping in that code equivalent - is there a reason for using IO.copy_stream which will regress from that equivalent code?. That equivalent code fails often with undefined methodbytesize' for nil:NilClass`\nI'm investigating, but will need to resolve that.. It does look like JRuby fixed this as of 9.1.16.0. So, we know that this is a JRuby standard library issue.. I'm not going to worry about reverting this in that case - it's new functionality, no regression has been introduced. But I don't want to trigger a release until we can get the right code working for older JRuby versions.. I think that's actually reasonable in this specific case, but if possible, I'd like to find a solution which is going to work on these versions. It's not quite a non-starter since there is no forced upgrade here (if you don't opt-in to use this new functionality, you don't break), but ideally we can avoid forcing end-users to upgrade solely to maintain compatibility with their AWS SDK dependency.. Your above posted change does seem to work, stand by.. I think this is the right approach - I don't see anything additionally that IO.copy_stream was giving us other than brevity. I'll link the new PR to this PR.. That's an interesting issue to consider. I'm going to look into that before merging the linked PR.. Just to check, but I think it's implied, is this an occasional/transient error? Is there a set of inputs which consistently produces this error?. Thanks, this looks then like it's likely a service issue, so I'll be opening a report with them internally. Some things that could help, if able:\n\nIf you can show me the inputs you're using, like a code snippet of what is causing this (feel free to remove any identifying info).\n\nIt will be difficult to try and get a request ID here, so I'll forward any questions the service team has back here.. The service team is repeating the request for your inputs. Can you share the call you're using that occasionally creates this error?. Understood - it's a little difficult to determine what the \"right\" approach would be for handling a generic malformed response from the service, which is essentially what this is. We potentially could, for example, add Aws::WHICHEVERSERVICE::Errors::ServiceError to the ancestry of the JSON::ParserError we raise, but past that, there are other ways that a response can be malformed and distinguishing between those generically is a difficult problem. Even adding that error would at least require some plumbing, since not all cases of a malformed response are associated with service errors.. Given how rare this is (haven't seen this type of issue before for any service), I'm thinking the best path forward is still to get the service to behave properly.. Sorry about the confusion, I can totally see where you're following the way the docs are written, and how the exception name you got back from the service is not helpful (upon re-reading it does seem to say the key \"exactly one\" provision, but I read it the way you did the first time).\nWe've passed this feedback on to the service team and will press for a change to the shared documentation for this method.. That aside, have you been able to get your calls working with this clarification?. Looks good, thanks!. :bucket is an option you'd pass into individual API calls, it isn't a client option either globally or for individual clients. What are you trying to do? Do you have a code snippet that explains at a high level what you're trying to run?. No worries, glad you were able to get to a working solution. Indeed it has some potential issues to treat that as global configuration, and we aren't wired for that, but there's nothing stopping you from building abstractions over your own client calls in your code which injects a bucket into calls that require one.. :http_open_timeout is related to how long we'll attempt to keep a connection open, it is not related to retry logic. @Doug-AWS will look at updating the wording of that example to be more clear.\nIf you want to expand time between retries, the current way to do that is by modifying the :retry_backoff option. So if you wanted a fixed retry spacing of 60 seconds, you could do the following:\nruby\nclient = Aws::APIGateway::Client.new(\n  credentials: role_credentials,\n  retry_limit: 4,\n  retry_backoff: lambda { |c| 60 }\n)\nNote that :retry_backoff is treated like a lambda, so even though you're looking for a fixed value, you still need to accept the input variable. Try this and let me know if it fixes your issue.. This is why I shouldn't try to answer issues late at night! Yes, I missed the Kernel.sleep part. Thank you for catching that.. Closing as I think we've resolved this, and the example has been updated. Feel free to reach out if you have more questions/issues.. How are you providing credentials for this project?. You're using a version of the SDK that predates support for shared credentials, and it's also an end of life version. Is this a new project or an old one?\nIf it's a new project, I'd recommend upgrading to V3 of the SDK, in which case a shared credential file created by the CLI is one valid option. Though if you're deploying to EC2 instances, I'd recommend using EC2 Instance Roles which the SDK with automatically pick up.\nIf it's an old project, I'd at very minimum recommend upgrading first to the latest version of V1 of the SDK, and coming up with a migration plan to V3. End of life means that even bug fixes are no longer considered for V1.. Best path forward is to upgrade. First to try and upgrade your V1 SDK version, then to V2/V3. https://github.com/aws/aws-sdk-ruby/blob/master/MIGRATING.md. Keep in mind, per that migrating guide, you can run V2/V3 in parallel with V1 of the SDK to allow a piece by piece migration, the guide shows you how. The aws-sdk-rails gem may also come in handy.. I don't think that is related, if the code is triggered that creates a client and you don't have credentials in place, you'll receive that error.. Closing this, I think the question has been answered. Feel free to update with any questions you have.. Which version of the SDK are you using, also?. Can you elaborate a little more on what you're trying to do?\nAre these updates to :user_data values on an EC2 instance? Or are you referring to Launch Configurations for Auto Scaling?. I see what you're getting at. I think you can actually get pretty close to this today. Consider this example:\n```ruby\nrequire 'aws-sdk-autoscaling'\nclient = Aws::AutoScaling::Client.new\ncopy_from_lc = Aws::AutoScaling::LaunchConfiguration.new(name: OLD_LC_NAME)\nopts = copy_from_lc.data.to_h\nopts[:launch_configuration_name] = \"NewLaunchConfigName\"\nopts[:image_id] = \"NewAMI\"\nopts.delete(:created_time) # Cleanup\nopts.delete(:launch_configuration_arn) # Cleanup\nclient.create_launch_configuration(opts)\n```\nThis should be pretty close to what you need - you may need to fill in a few extra values depending on the details, but give this a try and see if that gives you a useful head start to build around.. I'm able to reproduce this, looks like a valid use case as well. I have a guess as to where this regression came in as well.. We're working on some test failures in PR #1731 which is the proposed fix for this. Once we can sort that out, will go out with the next release.. Closing, this will go out with the next release. Will note the version number of core after releasing, and then feel free to reopen if issues recur.. This behavior is intentional. All the presigned URL generator is doing is creating a valid, presigned version of the :get_object request. To do what you're asking, we would have to add an additional call to :head_object, and that runtime/network overhead is not appropriate for many users.\nFor example, the presigner method doesn't check for bucket existence either. We will faithfully return a valid presigned request for any set of inputs. I would recommend that:\n\nIf you want to test for object existence before creating a presigned URL, call :head_object.\nIf you want to test that the :get_object call will be successful, have your API make a call to :get_object or whichever call you're presigning with the same params before building the URL.\n\nClosing, let me know if you have other follow-up questions or issues with these workarounds.. Taking a look at that test failure, but yes, I can also look at this as a feature request to the code generators to put this into generated classes. That seems like the correct path forward (for example, I'm not too concerned with adding it to the code generator classes themselves, they aren't shipped with the gem.. Can you show (feel free to redact sensitive information) the output of this call with wire tracing on?\nruby\nacm = Aws::ACM::Client.new(http_wire_trace: true)\ncert_detail = acm.describe_certificate(certificate_arn: cert.certificate_arn)\ncert_detail.certificate.domain_validation_options.each do |v_opts|\n  puts \"[#{v_opts.validation_status}] #{v_opts.domain_name} => #{v_opts.validation_method}\"\nend\nIt is most likely that this is a difference in what the service itself returns, but if I am wrong, wire tracing will reveal the issue.. Is it possible for you to add a (previously) failing test to the spec suite that reflects the error?\nIt otherwise looks good, and happy to help if you're not sure about the best way to do this.. I'd raise this with AWS Support or as feedback to the service teams. We consume the API definitions provided by services. We will also follow up on our end. It's only closed here because there's nothing we can do directly in the SDK (this issue would exist across SDKs and be resolved across them together).. I'd consider opening an issue with AWS Support with any differences or discrepancies you're spotting. May be more efficient on an ongoing basis.. We'll definitely pass it on.. Definitely don\u2019t go to that trouble, I think it was more to see if you knew of other issues. We\u2019ll continue to work with the service team, but that\u2019s all we can do as the SDK is just 1:1 transcribing server responses.\n\nOn Apr 15, 2018, at 11:34 PM, Dinesh Yadav notifications@github.com wrote:\n@cjyclaire To get all the diff, I will have to code and compare prices using the code and figure out the discrepancies, or I will have to manually check the prices. So, I guess this part if done at your service team end will be more fruitful as I do not know how you fetch the prices returned by the sdk. Do you hit the APIs, if yes there is some error in parsing those prices, if not the source where sdk takes the prices from may be buggy.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub, or mute the thread.\n. No updates at this time, will advise when the service team has more information.. The SDK itself is formatting your request to the api.pricing.us-east-1.amazonaws.com endpoint, and then translating the raw response back to code objects to present to you. The wire trace shows the SDK is doing both sides of that transaction correctly.\n\nTherefore, any discrepancies you're seeing in this case are outside the SDK.. I'll pass on the discrepancy, though they're already aware. I still do not see where the SDK itself is buggy, as we are correctly formatting your request (which is not to the JSON file endpoint you mention above of https://pricing.us-east-1.amazonaws.com/offers/v1.0/aws/AmazonEC2/current/index.json, but is api.pricing.us-east-1.amazonaws.com) and parsing the response.\nI'm sorry to hear about issues on your production servers from this issue, I would recommend contacting AWS Support to be able to directly engage with the service itself.. https://aws.amazon.com/contact-us/\nAWS Support is one choice, or if this is an issue with your own AWS bill,\nperhaps the billing support link in that page is appropriate.\nOn Fri, May 4, 2018 at 12:07 AM, Dinesh Yadav notifications@github.com\nwrote:\n\nI believed that both of them are same. Now if they are different, then\nboth the sources(one starting with https and another one with api) allowing\nprogrammatic access to the prices are buggy and SDK is fine.\nCan you tell me where can I file a bug issue for\nhttps://pricing.us-east-1.amazonaws.com/offers/v1.0/aws/\nAmazonEC2/current/index.json endpoint if at all it is possible?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/aws/aws-sdk-ruby/issues/1745#issuecomment-386521029,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ACBDoSu5jno5t-icT2wLHVFV5i8LJZZ6ks5tu_5IgaJpZM4TBu8W\n.\n. I have gotten a response on at least one case, the i3.xlarge instance - it sounds like the discrepancy was not including the \"preinstalledSw: SQL Web\" filter in your request, it's not a like-for-like SKU comparison.\n\nI think generally though, you're going to have a much more efficient time answering these questions with AWS Support - I'm not an expert in this service, and Support will have better SLAs for responses.. This feature request is being tracked elsewhere. Counting this as a +1 for that support.. It would seem then that the workaround is to install directly the service gems you need, which is a recommended approach. Is there any reason you'd prefer not to do this?. @jdblack This example is curious, I can't reproduce this anywhere. Are you running this in a gem environment with bundler present or as a standalone script? Does this error persist for you?. This is actually a change to a generated class, so it's not going to work as formed. I can look at this as a +1 to the requests to support unsigned requests for operations which accept them broadly (there are others in other services as well, as you probably know).. Closing due to inactivity.. Closing due to inactivity.. Can you share your Gemfile.lock? Just want to check something on that.. I'd have to explore your use case a bit more, but the main reason PresignedPost doesn't have :content_md5 applied is because you don't have a way to know what the MD5 is when you're presigning the post without a known body.\nCan you tell me a bit more about what you're trying to do? If you just want to give permissions for the browser to construct an S3 POST request and want to help it build a post with a checksum, it's possible the Javascript SDK may provide a useful solution, for example.. Okay that makes sense, so the use case is to provide a way to allow for more headers, without them being signed headers. I need to do a bit of research on this.. Just to check, is this still an issue? If so, I'd like to see your Gemfile specifically, and how the AWS SDK is being loaded in your Rails app.. This is probably something where we could expand context. While you are correct about the initializer if created directly, the documentation is correct for the most common use case - the default credential provider chain: https://github.com/aws/aws-sdk-ruby/blob/master/gems/aws-sdk-core/lib/aws-sdk-core/credential_provider_chain.rb#L27\nI'm happy to take this change if we draw that distinction, or I can take this as a task to update the documentation myself. Up to you!. So the question here is why the service is returning a zero fixed price for your reserved instances via the API, when presumably you paid more than zero? Just to be clear.. I'll pass this on the to EC2 team, given that the SDK is properly formatting requests and responses. You can also raise this with AWS Support.. Closing - the SDK is correctly dealing with the responses given, best way to continue to escalate this is via AWS Support.. V1 is end of life, we aren't even taking bug fixes for it (outside of security vulnerabilities). At this point, your options (outside of the very strongly recommended upgrade) would be:\n\nUse the low-level client in V1, which shouldn't have this issue.\nUse a build of your fork locally to gain this behavior.. I'll pass this on to the owners of the endpoint file. Are you finding that, for regions we currently support in the aws-partitions gem, that the experience works well? (To help me scope the request). This is actively being tracked and worked on.. Definitely taking a look at this - it may require some careful looks at our generators as well.. I've got a pending work item being tracked to add the warnings flag without breaking our test suite (it looks like just a few tests that are checking output in a way that doesn't play well with warnings), but the rest has been merged. I would happily take other warning-reducing PRs.. I'm going to hold off on this, because we're working on the correct fix on the service end and it isn't an Oj-specific issue. If the service fix falls through, we're going to be shipping a more general solution to this problem.. This should be resolved now on the service end. Are you still seeing this issue?. That's right, any credentials will do if you're not calling out to a service. I'll definitely pass on that feedback.. Attn: @Doug-AWS . Sorry for the delay here - let me work on a conflict resolution on the PR and this should be good to merge.. This broke our automated release process. I'm looking in to why that is\nbefore re-adding the warnings file.\n\nOn Tue, Oct 9, 2018 at 3:40 PM Danny Iachini notifications@github.com\nwrote:\n\nIt looks like at least a part of this was maybe undone here:\nf4b3251#diff-eba7bf5f9cb1f1ef94f03eeba1444966L24\nhttps://github.com/aws/aws-sdk-ruby/commit/f4b3251658ee1d95403d7a851ace8495ea4115af#diff-eba7bf5f9cb1f1ef94f03eeba1444966L24\nand here: aca0d7c\nhttps://github.com/aws/aws-sdk-ruby/commit/aca0d7cb31d1715b60711cbb907623fab498b595\n(I was updating the aws-sdk-s3 gem in one of my teams' projects, and was\njust hoping to get an understanding of why this was added and then removed?)\n\u2014\nYou are receiving this because you modified the open/close state.\nReply to this email directly, view it on GitHub\nhttps://github.com/aws/aws-sdk-ruby/pull/1781#issuecomment-428218969,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ACBDoZIT8eVSP9UWUrxEcRIyrnM9-wPYks5ujLVggaJpZM4T-xpZ\n.\n. This is a function of the API, because the policy is a streaming response object. In order to get the JSON payload, do the following:\n\n```ruby\nrequire 'aws-sdk'\nclient = Aws::S3::Client.new()\nresp = client.get_bucket_policy(bucket: \"api.docs.hack.cloudcoreo.com\")\njson = JSON.parse(resp.policy.read)\n```. Happy to pass this feedback on to the service team. You can also pass this feedback on to the service team via AWS Feedback channels.\nUnfortunately, there isn't a way for the SDK itself to get this extra information from the error type of Aws::KMS::Errors::MalformedPolicyDocumentException received from the service. If a message is added, it will be included automatically.. To help me track down an initial suspicion, does your application use threads heavily?. As a possible workaround, you could consider throwing a Mutex around that call. I'm not aware of a concurrency issue per se that would manifest in this manner, but it smells like I might find one.\nSee if that workaround helps you, we'll investigate in the meantime.. So far I've been unable to reproduce this. For some additional information, is this tied to any particular API call, or many different APIs?\nHow do you initialize and maintain threads in your application?. Any updates on the workaround or on my other questions?. Thanks for reporting this, I can see where this would be happening. We'll be working on this today.. The @raw_stream object is only used for event stream responses, so the fix here is going to be to ensure we are not collecting a copy of the streaming response in memory except when we are required to (in the case of an actual eventstream response).. We've confirmed that #1787 resolves the object space size explosion, and we're intending to release it today. Should be released as aws-sdk-core 3.21.1.. New release is out. Feel free to reopen if you continue to see this after upgrading, thanks!. This is kind of an interesting case, it sounds like we're expecting different behavior in 1.9.3 if adopting this change?. Ahh, I'm understanding the purpose a bit more. I am concerned though that this could catch some objects which weren't intended to be treated as hashes. Granted, we survive this now because of ordering, but I'm now considering if it's important to explicitly ask you to USE your hash transformation ability before calling low-level clients.. Your counterpoint is fair as well, though we'd have big problems if we implemented, for example, the :to_s check in the same way. Different because this is at the end of the switch stack, but just something I need to think about.\nNot worried about the 1.9.3 point anymore, I was confusing myself on the OpenStruct emphasis. No worries, you're correct on that count.. Otherwise looks good, we can make a plan to release tomorrow.. What version of the SDK are you using? It works for me using V2 of the SDK. If you're using a version older than 2.10.91, that API wouldn't exist yet and you should upgrade.. There isn't anything I know of which sets this without your knowledge. Can you run this with http_wire_trace: true set on your client, and paste the output from that? Feel free to redact anything sensitive.. What is the structure of your call that generates this?. Can you give me an example of a reproducing API call? I'm trying to compare what was sent over the wire to the client params you set.. I'm able to recreate the error, but the SDK itself isn't inserting anything you didn't specify. There's probably something different you need to do in your request, but I can see that it isn't immediately obvious. I'd recommend reaching out to AWS Support to get EC2's take on this.. With that said, I was able to get this working by explicitly setting the instance interruption behavior value explicitly, so I'd recommend that you go with that approach. My guess is (and it's only a guess) the implied default behavior is \"terminate\", and setting an explicit value is conditionally required when you're setting a persistent spot instance type. I'll flag down @Doug-AWS to encourage the EC2 team to improve their documentation around this, but my recommendation moving forward is that you explicitly set your termination behavior setting.. You'd have to set it to something other than terminate. As I understand it, the point of a \"persistent\" spot instance is to use behavior other than termination when the spot instance is dropped. Announcement post.\nWhat you seem to need to do is choose whether you want the instance to stop or hibernate, and to make sure your instance's storage solution is compatible.. I see - I think the support case is probably the best path forward then. From an SDK point of view, we're faithfully marshaling your request and parsing the service response.. I think that if you provide the key, it's going to expect a valid location constraint.\nWhy not do this?\nruby\ns3_client.create_bucket(bucket: BUCKET)\nDo you need to pass this configuration value at all?. You should install the aws-sdk-s3 gem, or include the aws-sdk-s3 gem in your Gemfile.. From the related issue #1711 - this issue is likely to increase memory usage by a decent amount, including outside of JRuby.. That's definitely possible. There's a few ways to go about this that I'm comparing right now. Most ugly might be to have this approach as a fallback, or gating on language versions (the approach I most want to avoid).\nI'm leaning towards the belief that punishing the vast majority of users with memory bloat just because JRuby's standard library was buggy is not the way to go. Question is how well we can support older JRuby versions.. Ok, verdict - I'm going to go with your implementation with a documentation note about the JRuby issues. If we get reports about it, my fallback plan (which I'd rather not do) is to fork the logic to the memory bloat approach if and only if the user is using an old version of JRuby by checking the Ruby version at runtime.. We can't accept this directly, as these are files we consume from an outside source. However, I am going to work to push this upstream, which would update the waiters for ALL SDKs, rather than Ruby alone.. You were correct on the error matcher concern by the way. The correct matcher type would be \"path\" for a singular argument like that. No big deal, I've changed it upstream.. This will go out with the next release after today. Thank you!. There's something missing here to help me reproduce, because we don't have autoloading in V3 except at the per service level. Where are your require lines, and what are they?. Yes! I'd strongly recommend changing your dependencies to point to the specific gems you're using. For example, based off what you configured:\nruby\ngem 'aws-sdk-s3', '~> 1'\ngem 'aws-sdk-dynamodb', '~> 1'\nOr if you're using a data mapper library for DynamoDB like ActiveRecord:\nruby\ngem 'aws-sdk-s3', '~> 1'\ngem 'aws-record', '~> 2' # DynamoDB gem included with this\nWith either of these approaches you're good to go. Otherwise, we autoload when the sub-gems are first referenced.. If you need to add new headers which require signing at the time of the request, then presigned URLs would not be appropriate for your use case. I'd recommend in such a situation that you perform the necessary signing on the server side. If you must do so on the client side, such as a webpage, you could also do so using the AWS SDK for Javascript in the Browser, and concepts like CORS to grant narrow permissions to the user directly.. Unfortunately, if the value of a header needs to be signed, you need that value available to you at the time you perform the signing. Otherwise the signature is rendered invalid. In the context you're describing, I think there are 4 options:\n\nUse only signed headers which have values available to you at signing time.\nPush more of what the user is doing back to the server side to make 1 cover the dynamic values.\nIf you're providing a mobile web product, use the browser SDK linked above.\nIf you're providing a mobile app, you may find the AWS Mobile SDK for iOS helpful.\n\nClosing as I think this is answered and there's nothing more we can do here, but happy to answer any follow-up questions.. Another possibility might be the PresignedPost class. It does have ways to point out metadata headers that will be added at build time as part of a policy, and may help to give you a workaround for this issue.. We'll take a look at this, but the recommended usage is to simply require the service gems you need directly. aws-sdk-resources and similar are provided as a migration convenience more than as a long term recommended path.. So, researching this I'm not sure how calling gem resolves this issue. That method does allow you to load a specific gem version, but it needs to be specified in code.\nI think that this is something we can note down as a known drawback of modularization - that if you pull in the omnibus gem and update all sub-gems frequently, it's going to be a lot of versions to traverse. My strong recommendation would be to scope down to just the service gems that you actually use, which would reduce the scope of this issue significantly.. I would be open to mitigations to reduce this issue in aws-sdk-resources, it's not the intended behavior to cause load time pain. But I don't see how this one works, unfortunately.. I also could be missing something about what exactly Kernel#gem is doing, documentation around it for the use case you're describing is sparse, unfortunately.. I think the feedback is well taken here that we should be putting the per-service gem caveat more prominently in front of the \"get started\" example there.. The README on this repo does have those warnings prominently, will queue up some work on the marketing page.. There are other benefits to it being a standalone credential provider - for example, it would allow users to invoke it directly, rather than as a shared configuration side effect.. So the last thing we need before merging is to add credential provider chain tests to cover what should and should not happen (making sure this is the end of the credential stack, ensuring it's a terminal state).. I can take another look at this Monday, but I suspect the path forward is to reach out to AWS Support for DeviceFarm. It looks like the SDK is correctly handling things, but you're looking for help with your hand-written HTTP traffic with DeviceFarm.. Thanks for the report - we'll engage with the service team here and take a look at possible options to move forward on this.. We've been able to recreate this now, and are trying to nail down where this is coming from.. What's interesting is, I can't recreate this in the latest version of the aws-sdk-s3 gem - looking for any relevant changes we've made.. Minimal reproducing example (hat tip to @srchase)\nCreate a file: dd if=/dev/zero of=fifteen_mb.txt count=1024 bs=15400\nRun:\n```ruby\nrequire 'aws-sdk-s3'\ns3 = Aws::S3::Resource.new(region: \"us-east-1\")\no = s3.bucket(BUCKET_NAME).object(\"mp_deadlock_test.txt\")\no.upload_file(\"fifteen_mb.txt\")\n``. I think this was actually fixed from version 1.16.0 onwards, but please do check back and let me know if an upgrade solves your problem.. Tentatively putting the bug label back on because at minimum we have really inconsistent performance for the working versions as well, which could indicate the potential for these issues with bigger files.. Okay, we've nailed this down exactly. #1545 predicted the possibility of certain file upload situations creating an endless loop in generating checksums, and that's exactly what you've encountered. That pull request fixed this issue, and was released asaws-sdk-s3version1.16.0. If you upgrade, the problem will end.. Investigating - I'm suspecting that this requires an endpoint to be provided manually and that we need to provide the proper error type for this. Stand by.. Okay, that's what is going on here. You need to provide an endpoint from the#describe_container` call in the AWS Elemental MediaStore API.. For example:\n```ruby\nmediastore = Aws::MediaStore::Client.new\nmediastore.create_container(container_name: CONTAINER_NAME)\nAfter a describe call shows the container is ACTIVE and has an endpoint...\nendpoint = mediastore.describe_container(container_name: CONTAINER_NAME).container.endpoint\nmediastore_data = Aws::MediaStoreData::Client.new(endpoint: endpoint)\nand then make your calls...\n```\nLeaving this open because in previous services like this we've removed the plugin which auto-fills the endpoint to make this requirement more clear, but hopefully this unblocks you :). Right - this is true of one or two other services as well, just need to drive consistency of behavior here.. Tests appear to be passing now, just reviewing for possible sharp edges. Thanks for submitting!. So for some background on this, since a DynamoDB numeric can be integers or float-type numbers, BigDecimal allows us to have a consistent return type for numeric attributes. Your solution for getting to the result you want is correct.\nLibraries like aws-record will do this for you, if you define your attributes as specifically integer or float type.. You need to stub out the #list_objects method. Your code as listed doesn't use #list_buckets, but assuming it's needed elsewhere, you could use a config line like this:\nruby\nAws.config[:s3] = {\n  stub_responses: {\n    list_buckets: {\n      buckets: [name: \"my-bucket\"]\n    },\n    list_objects: {\n      contents: [{key: \"mykey\"}]\n    },\n    get_object: {\n      body: tempfile = file_fixture('file').read\n    }\n  }\n}. Good catch, thanks!. The DAX team has, unfortunately, not provided an official Ruby client. We'll be happy to note this as a feature request and send your request on to them.\nThe AWS SDKs include support for the DynamoDB APIs, which includes control plane operations for DAX, but the data plane client is customized (note the import amazondax line, which relates to https://pypi.org/project/amazon-dax-client/ ).. This looks like it's most likely an issue with the service - are you able to capture a wire trace of any of the problematic calls?. I'd recommend reaching out to AWS Support. I don't see anything about the client (there are no customizations, for example) that would indicate this would happen, it's a plain API call. If we see a wire trace showing we double-counted the data key, for example, that would be interesting.\nIt may also require grabbing some data from the context.http_request object - do you have a way to reproduce this predictably?. It's important to note that the client itself is generated code, and this PR if accepted would be written over by the next STS release. If you'd like to expose presigner functionality for STS, I'm open to it, but I'd follow the pattern we used for S3 of having a specific presigner class.. I'm guessing you're running this in a multi-threaded environment - would that be right?. This is a known issue with version 2 of the SDK in multithreaded environments. You have a couple of options for resolving the issue. Essentially, version 2 of the SDK uses autoload which is not thread safe and can lead to race conditions in multi-threaded code.\nUpgrade to Version 3\nVersion 3 of the AWS SDK for Ruby is thread safe (it does not use autoload except for requiring whole services). It's also code compatible with V2 of the SDK, so it should only require changing your Gemfile and potentially some require lines.\nWorkaround\nThe second option is to run the Aws.eager_autoload! method somewhere in your configuration/startup code. That will eagerly load all of the SDK files (and can be scoped down to just the services you use), and therefore you'll avoid race conditions.. I think this is a known issue with our code and Ruby 2.5 - will take this as another note to clean some of these up.. That should work, you do NOT need credentials. I have questions:\n1: What version of the SDK are you using?\n2: What happens if you create a client in the following way?\nruby\n    Aws::S3::Client.new(\n      stub_responses: {\n        get_object: {\n          status_code: 200,\n          headers: {},\n          body: load_fixture('domain_data_set_string', 'csv')\n        }\n      }\n    )\n3: How are you creating your client that is asking for credentials?. What version of the SDK are you using?. This is a good callout, I hadn't thought of this. I'm happy to take a pull request for this as well.. So thinking about this, I'm a little concerned that changing this could break customers who have started to write test expectations against this. Would it meet your needs if we instead lifted whether or not the request was presigner based to the top level of the API requests object? Or, possibly, if we allowed you to filter them out with an optional param?. We should also carefully call out in release notes how tests could break when updating to this - if customers are relying on this to be collected for presigned requests, they'll need to adapt.. On second thought, hold on merging this. I don't want to cause surprise test breakage if there is another way to avoid this.. I deeply apologize for the mistake here, I'm going to fix this first thing today. A workaround is to hard upgrade aws-sdk-core but yes, NEEDING to do that isn't correct. I clearly made a mistake when updating the gemspec generation code.. I believe #1871 fixes this as planned, I need to look at the extent of this. Is dynamodb the only affected gem?. Okay, this was an oversight on my part. Here are the affected gems:\n\naws-sdk-appstream\naws-sdk-dynamodb\naws-sdk-elasticloadbalancing\naws-sdk-rds\naws-sdk-s3\n\nWe'll have new versions of all service gems out today, but I had added plugins to core, yet my update to the gemspec generation logic was put into a separate commit. That was a mistake on my part.. Here's a summary:\n\nI'll post an update here when we push our next release, these gems will be usable with the latest updates after we get that out.\nIn the meantime, you can use the latest versions of the current gems by also upgrading aws-sdk-core to the latest version. That is essentially what this pull request will do.. Update: The release for this is going through our release process now. Expect to be able to update in about 30 minutes from this post.. This is fixed in the latest versions of each gem. Sorry about the churn here.. What version of aws-sdk-ssm are you using? Are you seeing the same client error?. What happens if you upgrade to 1.25.0? It does look like 1.24.0 may have been impacted by this issue.. Also, before I can merge please confirm: \"By submitting this pull request, I confirm that my contribution is made under the terms of the Apache 2.0 license.\". Just need the confirmation on the licensing and we're good to go.. No worries! This will go out with our next release.. You can't stub the waiters directly, only the underlying client calls.. This example worked for me:\n\nruby\nrds = Aws::RDS::Client.new(stub_responses: true)\nrds.stub_responses(\n  :describe_db_instances,\n  db_instances: [\n    {\n      db_instance_identifier: \"test-db\",\n      db_instance_status: \"available\"\n    }\n  ]\n)\nrds.wait_until(:db_instance_available)\nAre you trying to do something beyond that?. To help debug this, can you show the response you received in wire trace? Turning on http_wire_trace: true on your Aws::Lambda::Client class should provide the wire trace. That will help us determine where the formatting is being applied.. The wire trace logs there show me that the triple-escaping happened before the response was sent to the Ruby SDK, we're faithfully returning what the service gave us.\nWhat I'd recommend doing is looking at your Lambda handler's implementation, and possibly reaching out to AWS Support. I don't know which runtime you're invoking, so I may not have the expertise to help debug your Lambda handler.\nClosing as there's nothing much we can do on the Ruby SDK end. Feel free to reach out if you have more questions or see further issues after the wire trace shows the output you want.. Key files are the test rake file, and the services.json file. Everything else is service regeneration noise.. Thanks for reporting this. Will check scope (I would wager this is not the only impacted gem) and push out a fix.. First is the mitigation, which is also going to be the fix: If you're broken by this, upgrade your version of aws-sdk-core to at least 3.37.0. That will unblock you. I'm going to be submitting a build fix which will update all .gemspec files to have that dependency scheme.\nI'll then work on an emergency release to update any gems that have released since that time.. https://github.com/aws/aws-sdk-ruby/commit/695ecc61b398066d7e165ec5a2f1d0557282429d will fix this, going to trigger a build soon. 33 gems were impacted in total.. Release is in progress. If you are using an affected gem, you should be able to upgrade in less than an hour (though if you're reading this, you could work around it right now by updating core, so do that first, but this won't recur at least).. Releases of the affected gems are complete. You should no longer see issues after upgrading to the latest service gems - please re-open with the affected versions if you do.. Changes look good in general. I'm currently verifying that the current setup is indeed broken in Ruby 2.6, and that this PR fixes the problem.. Confirmed on both counts. Before we merge though, need to do a bit of administrivia. Can you confirm:\nBy submitting this pull request, I confirm that my contribution is made under the terms of the Apache 2.0 license.. For background, the issue you're seeing tends to happen in multi-threaded environments due to autoloading, and was an issue fixed in major version 3 of the SDK. If you're required to use major version 2, you may want to require the aws-sdk or aws-sdk-core gem up front and call Aws.eager_autoload! to get around this.. Can you provide the http_wire_trace output (anonymizing specific values like the distribution is fine and even encouraged)? It's hard to tell from this if it's the SDK not properly marshalling the request, or if this is a quirk on the service side.. Given that the response to that API call is effectively an empty response, I don't think the class selection for the response is wrong, but I do think it's reasonable that the Aws::EmptyStructure class should respond to #to_h, even if the result would just be {}.\nIs it your standard practice to call #to_h on all your service responses?. s/Seahorse::Client::Response/Aws::EmptyStructure on that last thought.. I'm able to see what you're saying here, looking at this as a bug.. The issue here seems limited to Query APIs. The intention was for it to check for a nil/empty response, in which case it would provide a new Aws::EmptyStructure instance. However, the XML parser itself is returning Aws::EmptyStructure the class, which isn't nil.\nThe fix for this is pretty simple, though I'm also trying to evaluate the risk of a breaking change blast radius as far as applying the fix goes. The danger is that those who are using this client may be relying on the current state of the shape data.. Going to merge this in after we confirm tests are working.. This will go out with the next SDK release.. Definitely agree that the documentation example is not helpful, and I'm prioritizing a fix for this. The schema validation error is because  { s: 'test' } is a valid DynamoDB value, it just so happens to be a hash which will be wrong for any key type.\nIn any case, totally agreed that this is an issue and unhelpful documentation.. Unfortunately it is not quite that simple. The documentation files (and example files, the issue in this case) are autogenerated from a shared source. Simply put, the next release would overwrite such a change. I've taken on a work item in the next sprint to customize this example to fit the simple attributes behavior.. Leaving this issue open since I'd also like to fix these examples, but at minimum the broken examples are now removed. The \"placeholder\" examples are already correct.. This is now properly fixed. Examples should have been removed from the docs in today's release. The next release will restore the examples, this time in working form.. This is currently a WIP. Final resolution of #1972. I just had a silly error. This version should work.. Just to be clear, we have removed these patches for Ruby 2.5 - is the request here to narrow that exception for some other versions as well?. V1 of the SDK is end of life, we're unfortunately no longer accepting any pull requests for it. I would strongly urge you to use V2 or V3 of the SDK, which support all current regions.. Taking a look. @dgem I wouldn't recommend downgrading to V2 from V3 on this case, pinning to the previous version of aws-sdk-core (3.46.0) is a reasonable workaround while we sort this out.. It looks like what we are going to have to do here is to remove the http-2 gem from the gemspec, and require users to separately include that gem in order to use H2-related features.\nTesting that now in a branch.. Working on a fix in #1995 . Update on this issue:\nWhat is this issue?\nIf you are using Ruby versions before 2.1, you will be unable to update aws-sdk-core to 3.47.0 due to the new dependency on http-2. While these versions of Ruby are end of life, we're not intending to hard-break users of these Ruby versions, so we are treating this as a bug.\nHow can I work around it?\nIf you're using Ruby versions >= 2.1.0, this issue does not impact you and there is no action required at this time. If you're using an older version of Ruby, you'll need to pin aws-sdk-core to 3.46.0 until we get a fix out.\nWhat is the fix?\nOur current plan is to make the http-2 gem an optional dependency. Once we do so, to use the HTTP/2-based asynchronous APIs, you'll be required to provide the http-2 gem yourself, for example by including it in your Gemfile. If you do not, you'll receive an initialization error when you try to initialize any asynchronous client.\nThis is unfortunately a breaking change if you're using a new version of Ruby and quickly adopted the HTTP/2 clients, so I'm open to ideas if there's a better way. But on balance, this change would break fewer users.. I'm going to add that there's a good chance we're going to wait until Monday to deploy a fix. Reasoning:\n\nThe current workaround of pinning a version is reasonable for those impacted - you wouldn't be able to use HTTP/2 features anyways without upgrading, and we will be fixing this swiftly so the version pinning isn't a permanent state.\nThe only fix we can come up with is a breaking change for anyone early adopting the HTTP/2 features. We don't want to drop a breaking change on a Friday and possibly have customers break over the weekend. (I don't want anyone to break at all, but I can't come up with a solution that doesn't involve some sort of breaking change.)\n\nI do recommend that anybody using the HTTP/2 features currently get ahead of the changes by explicitly adding the http-2 gem to their dependencies now.. On the question of pinning, the gem you'd need to pin is aws-sdk-core to 3.46.0, though the fix release is likely imminent.\nIf you're using the command line, you'd add a -v option, such as:\nbash\ngem install aws-sdk-core -v 3.46.0. The PR for this is merged and pending release.. Looks like we just need to backport the relevant changes to V2. Adding to our backlog.. Best thing to do here would be to require the AWS service gems you need, rather than all ~150 services.\nFor example, if you use S3 and EC2, you could simply install aws-sdk-s3 and aws-sdk-ec2 and the number of gems you install will be cut drastically.. This is a fair callout, we can narrow the scope of this warning.\n\nOn Mar 18, 2019, at 5:50 PM, Stanis\u0142aw Pitucha notifications@github.com wrote:\nIssue description\nGem name aws-sdk-core 3.48.0\nVersion of Ruby, OS environment\nRuby 2.3, Ubuntu server\nCode snippets / steps to reproduce\nUpgrading from aws-sdk-core (3.47.0) to 3.48.0 resulted in the following message:\nUnable to load the http/2 gem.\nWhile it doesn't seem to break anything specifically, it spams the output with this message, making it hard to (for example) use cronic or something else which expects a clean output.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub, or mute the thread.\n. This change will go out with the next release, after which upgrading aws-sdk-core to 3.48.1 will clean up the error message spam.. This is a fairly lengthy retry period, I'd like to see this start with a tighter default (like 3), with an option that can be passed in to set your own retry period.\n. Add a comment around this to explain the magic number, at least. That way, it's a less confusing failure if the defaults are changed later. Or, move out to variables that are grouped together.\n. s/Status/State\n. s/Status/State\n. s/Status/State\n. s/Status/State\n. Optional: Could we output the actual type we received in this case?\n\nruby\nmsg = \"Only list of strings supported, was: #{member.shape.member.shape}\"\n. I understand what &unhandled_callback is for because of the full context of the change, but it could be hard to remember or not intuitive to someone reading. Can we add this to the docs above?\n. Let's take out any TODO comments, as a style matter - I agree we should add this (although that isn't a blocker to me for accepting this PR, given the bug related to this), but we can handle that via other means.\n. I don't want to encourage hard-coded credentials at all. Given that we have a default credential provider chain explained in the guide already, I'd stick with:\nruby\nAws::SQS::Client.new(region: region_name)\n. This if statement will always return true. You would instead write something similar to:\nruby\nstatus_code = context.http_response.status_code\nif status_code == 307 || status_code == 303\n  # do things\nend\n. This is just for doc examples, right? And I assume this is to avoid the default presumption of examples being \"accelerate on\"?\n. Long term nervous about hard naming these. Fine for now, should consider alternatives down the road.\n. I'd love the \"all everything in everywhere\" example to be present, since that's a use case some have. For example:\nruby\nAws.partitions.each do |partition|\n  partition.regions.each do |region|\n    region.services.each do |service|\n      # Do anything you'd want to do for all services in all regions.\n      puts \"#{service} is in #{region.name} in #{partition.name}\"\n    end\n  end\nend\n. I've removed it from the Gemfile, I still think it's fine in the .gitignore file unless we would intend on tracking some sort of byebug history in the repo at any point.\n. Moved to a method, updated tests and usage to reflect this.\n. Done. Also removed an unused method from the class (development artifact I neglected to remove).\n. You're correct, this hasn't yet been hooked up. Next change will hook it up (conditional to the env variable being set) to the default credential provider chain.\n. This is done with the most recent change.\n. Last diff doesn't wire this up yet since we're still resolving behavior questions. Will resolve that before merging.\n. I think that may also be the current behavior. I'm going to add some tests for this specific case so we can handle it properly.\n. I've added some specific tests and configuration cases around this behavior. It will now explicitly fall through shared credentials, then to shared config, then to returning nil if credentials are incomplete as defined by the Aws::Credentials#set? method.\n. Nitpick: Personally prefer non-realistic looking account IDs, such as 123456789012.\n. I know currently that we're doing this ranged get call due to the nature of GCM decryption, but a comment explaining why we're doing this (and why we're doing this instead of double-reading the object) would be useful for 1-2 years down the road.\n. Can we document these two options, even if very succinctly? Very helpful down the line when debugging and needed to reload context.\n. By previous convention, we may want to consider releasing this feature as 2.4.0. Arguably minor enough to not bother, but it's technically a breaking change of a private API.\n. Why is this defined twice? Copy error?\n. Is it possible for this to end up undefined, given the case switch below?\n. Technically this will make it show up BEFORE 'Expires' in my testing, not after.\n. I might be missing something, but why not just remove this whole block and use:\nruby\n@blacklist = BLACKLIST_HEADERS - whitelist_headers\nYou already set whitelist headers to be an array by default, so once we get to this point, I'm not sure the if-else block is doing anything.\n. This was fixed in the most recent commit.\n. Could be worth mentioning poorly formatted custom endpoints here, which is also a common cause.\n. Is ConertName intentional or a typo?\n. Same typo question for ConertLocation\n. Let's go ahead and link it - for example:\n* Come join the AWS SDK for Ruby [Gitter Channel](https://gitter.im/aws/aws-sdk-ruby). Let's provide a link to the tag. For example:\n* Ask a question on StackOverflow and [tag it](http://stackoverflow.com/questions/tagged/aws-sdk-ruby) with `aws-sdk-ruby`. I'd potentially add a note along the lines of:\n\nIf in doubt as to whether your issue is a question about how to use AWS or a potential SDK issue, feel free to open a GitHub issue on this repo.. Everything in this else block would be a good candidate for another helper method.. Curious: Why not have each thread's #get_object call write directly to a file? It seems like we're going to be keeping every single chunk in memory, which could be problematic.. Same comment here - parallel writing seems to be the best approach, no?. Now that we are creating large files, I'm a bit concerned that we might be leaking tempfiles with how our tests are written. We should add explicit cleanup steps and a shared tag for these tests like @large-file or similar.. Could we also integration test our cleanup of temp files? That would be a bad regression if it ever happened.. We could do a bit of refactoring here for clarity. Let's rename obj in the block to make it more clear it's the output file, something like output_file. Similarly, the inner file could become part_file. This will make it more clear what we are doing.. I'm concerned about error handling in this case. We may want to put this method's logic in a private inner method, and wrap it in a begin/ensure loop. For example:\n\nruby\ndef concatenate_files(fileparts)\n  begin\n    concatenate_parts(fileparts) # put the existing method logic in a private method here\n  ensure\n    fileparts.each do |part|\n      File.unlink(part) if File.exist?(part)\n    end\n  end\nend. The thought here is that if we have some sort of File issue partway through the write, we're going to leak temp files. The ensure step performs a cleanup of remaining part files.. I'd like to consider refactoring this, as the logic feels a bit fragile. Could we, for example, create the chunks and sort the chunks into thread batches ahead of time, then process the thread batches one at a time? I'm worried that a later logic mistake makes it really easy to hit an infinite loop here.. Happy to pair on this as well.. The = character is considered something to avoid in a filename. Let's change our naming scheme (possibly even in a private method) to something like: targetfilename.txt_part_0013 and so on. The other advantage is this is string sortable (reducing complexity elsewhere) when done right. We can also pair on this.. Should expand this to note that the method should return before making client calls in the child process. In theory, a threaded system might call this, start API calls before we clear, and have issues.. I'd consider removing integration test steps that have to do with implementation details. All we care about in an integration test is that the user behavior will result in the proper output. Unit tests can go into a deeper level of detail (such as for mocks).. That said, checking the implementation behavior here may be appropriate if we're promising certain behavior, like in this case.. I would consider renaming #get_range and #get_part as these function names are confusing elsewhere in code given what they do. Perhaps:\n\n#multithreaded_get_by_ranges\n#multithreaded_get_by_parts\n\nThat way, when you run in to these method names, you understand what's going on, especially with #get_part. This needs to have some safety checks added. As it appears to operate now, if you get an exception during processing, you risk having orphaned files because the exception from the client will bubble up as soon as we join a failed thread.\nSince this method creates the intermediate files, it should be responsible for cleaning them all up before completing.. Given that we're rescuing all of StandardError, perhaps we could simplify this to be an ensure block, which means we only ever need to call #clean_up_parts here whether the method succeeds or fails. It also eliminates the need to catch only to re-raise the same exception.. Lift this in to the begin/ensure block.. Make sure this can handle a nil value for parts, since an ensure block is going to call this method.. Not needed here if this is run within the outer begin/ensure block which handles cleanup.. Why the rename? Generally even for API private classes, a rename like this triggers a minor version bump (just in case).. s/these branches/this branch. I'd call this \"Major version 3 of the aws-sdk packages\" - emphasizes further that the code is backwards compatible.. I'd combine \"1\" and \"2\" here, since the recommendation to migrate towards service gems is the same.. Should show how require statements change as well. For example:\nruby\nrequire 'aws-sdk-core'\n...would become...\nruby\nrequire 'aws-sdk'\nWe may also add a section about how V1 users need to now explicitly move to using the aws-sdk-v1 gem and require line, rather than aws-sdk major version 1.. Given that this field is S3-specific (especially in case other such fields come up for other services in the future), perhaps a name like :s3_host_id would be appropriate.. This is probably something to revisit - we want this changelog at the top level, despite the disconnect in the dependency chain.\nI believe this is also related to the test failure this PR creates.. This isn't really right - we should reverse the code, but the release count is still the release count.. Let's add the actual credentials from the stubbed credential files here, otherwise LGTM. s/gem correspondingly/corresponding gem. Let's capitalize these.. Use the cleanup logic you stubbed above. If the test fails you won't clean up the resource as written now.. This seems like it's prone to an unusual, but possible, edge case where somebody makes something called \"httpservice\" or similar which would then be an invalid endpoint. We should be checking for http:// or https://.. This is possibly too specific - if regressions appear in new 'ox' versions, we will miss it with this version lock. If we want, this could be the \"new\" OLD_OX and we keep the unbounded dependency for later tests.. I think either here or in the near future we should pull this out of the API Key plugin - this wouldn't cause harm if the API Key plugin were removed, but it's turning this plugin into a catch-all for bringing along other configuration.. This is going to create a difficult to read user agent. I'd go with this:\nruby\nua = \"aws-apig-ruby/#{CORE_GEM_VERSION}\". I wonder if we can squeeze out more performance by going to a thread worker model with thread-safe queues.. Proposed wording change:\n**Shared configuration is loaded only a single time, and credentials are provided statically at client creation time. Shared credentials do not refresh.**. This is probably a place where comments are worthwhile, to explain why we nest this twice.. Generally against leaving TODO markers in code - is this just something still pending before release, or is there a trigger for picking this up?. Raise an appropriate typed error.. ?. Assuming this is coming before merging - correct?. We should discuss this a bit further. This might be rather broad in what it accepts.. s/a enumerator/an enumerator. \"chunk containing exactly one data message\" I think is what we're trying to say. \"chunk containing a partial message\" is what I think we're trying to say here, this one is more unclear. \"chunk data is saved in the decoder's message_buffer\". \"chunk containing more than one data message\". s/formating/formatting. Style point: Don't use inline if when the code is not a single line. Prefer a wrapping if statement.. This is insufficient for launch, as we would also need to ensure that service gems which require the presence of eventstream to have a minimum bound on their dependency on aws-sdk-core, otherwise bundler will accept invalid dependency graphs.. This is not right, we're going to lock customers on an old version of core. We need a bounded dependency like:\n\"~> 3\", \">= 3.21.0\"\nReference: https://guides.rubygems.org/patterns/#declaring-dependencies. I think this is a mental miss for me. When I wrote the plugin add-on code, I assumed perhaps we could take on the agent port on the fly, but never wired that up. I think we should require the port at client creation time.. I need to clean this up - in practice this hasn't been an issue.. I should add the # @api private tag, agreed.. One issue is that the logging could get noisy. I need to circle back on this offline.. Same as above comment, seeing the full implementation it should be an AND condition.. Yep that should be cleaned up.. I think so, I basically forgot to clear out some self-discussion comments before sending up the PR. Will clean up for second round.. Tracked in a different plugin, regardless of call success. The spec tests should reflect this.. Can you talk a bit about the motivation of this part of the change? I can follow what you're doing, but want to check about any possible unintended consequences.. This looks like we took a bit of a shortcut, knowing we could list the type as File because of the switch condition. If we're going to also log Tempfile, which makes sense, we should have that display here. For example, we could accept the class as a param:\nruby\nwhen Tempfile then summarize_file_io(value, \"Tempfile\")\nwhen File     then summarize_file_io(value, \"File\")\nruby\ndef summarize_file_io(io, classname)\n  \"#<#{classname}:#{io.path} (#{io.size} bytes)>\"\nend. I'd actually prefer if we test both paths, rather than just replacing with a tempfile. Better way to ensure we could catch regressions with either path. Though given that it is not critical path in this test, you could choose to change one of the two test files, and not the other and I think that would meet the requirement.. I'd make the \"else nil\" explicit here.. I don't think this is the way to go about this either - I think when this credential provider is created, you should be able to specify the process identifier directly. You can fetch that value in the credential provider chain approach and pass it in to this constructor.\nEssentially, we shouldn't strictly require that a shared config file even exists.. I'd use a SDK typed exception here, as above. Similar to https://docs.aws.amazon.com/sdk-for-ruby/v3/api/Aws/Errors/MissingCredentialsError.html but not that exception. Perhaps Aws::Errors::ProcessCredentialsError?. Again, we need to not source from shared config within this class. It's also static, so the place to set this is in the constructor.\nproc is also a reserved word, best not to be used as a variable name.. We should make the \"is process credentials enabled by shared config\" decision fully within this logic section. If so, we create a ProcessCredentials object and expect it to work. If not, we return nil and move on.. My last point is for us to take another pass on the process invocation choice. Backticks run this way are going to be unescaped, and while we're taking in the data from the credentials file, we may want to be cautious. Take a look at Open3 and let's see if Open3.capture2 works equally well.. Maybe just say:\n\"This method is only implemented for stubbed clients, and is available when you enable stubbing in the constructor with stub_responses: true.\"\nI think this wording is important because it can read like you're supposed to turn on stubbing, when really you're just using this if you know you need it, like for testing.\nI'd also raise a typed exception, perhaps NotImplementedError.. I think as discussed before, we should have a more complex object here, so long as we're actually providing this functionality as a first class member. Maybe create a dedicated class for this, even potentially a dedicated handler class which is inserted when we stub the client.\nBut for a dedicated class, we could also record more data about each request. We can record the parameters, keep the context object in full, record the method name, and provide convenient helper methods for each.. Let's actually verify the contents, and use an example operation with parameters to verify that users can do the same validations on a first-class basis as you can in the aws-record examples. This is especially important because if we make changes later that would break stubbing behavior, we want to exercise all of it (assertions on everything we expose) so that those changes would break the test. Right now, the test has no contract on what actually is in the api_requests object.. I think it would make sense to explicitly pull out and provide the params. See: https://github.com/aws/aws-sdk-ruby-record/blob/master/spec/aws-record/record/item_operations_spec.rb#L45\nConsider pulling out context.params as the member :params, context.operation.name as the member :operation, and then just dropping the :context object. I think that will focus on the most important parts, while still providing the full context.. I'd actually like us to add more tests. For example, service names with spaces, and possibly some service names which should fail by intention. This covers the basics, I'd like to get a bit creative on cases.. Let's use a different example, closer to what we would want to do. A hard-coded echo with hard-coded credentials would never be recommended. It's just meant to be an illustration, but some may take it literally.. We're close now, at this point we just need to make sure we're splitting and escaping the string in the same way Python/CLI does: https://github.com/boto/botocore/blob/25a6a4c7c8a8b1797fc78c6a34dc7bca3f0bfba1/botocore/credentials.py#L793-L796. Last bit here, I would also add some tests with this same profile where we ensure that static profile credentials, for example, are pulled in first. It's a bit thorough, but important for regression.. Is this change related to freezing string literals, or just an extra change you wanted to make added in?. Same comment on relation to the PR intention.. I actually imagined this as an option you could pass in to the api_requests method itself. Should be easy to replace the attr_reader with a method that takes an optional param to do this. That way, you don't need to decide at client construction time.. I would expand on this. What GitHub issue does it solve, or, what kind of error does this address? Helps users identify if they need to pick up this upgrade in particular.. It may be worth adding a comment explaining why this conditional is important for later reference if looking at this code months later.. Revised:\n* Issue - Fixes a bug in the `:response_target` plugin error callback. Under certain circumstances a special body object can be removed before its error callback is triggered, breaking retry logic.. This would need to be in aws-sdk-core, as far as I can see you have not modified the eventstream gem.. This isn't very descriptive...\n:endpoint_discovery_required?\n:endpoint_discovery_available?. Perhaps worth noting that not all clients support this?. Can we fix the formatting on this file? It was correct before.. s/enocode_headers/encode_headers. Add user-facing documentation if we are making this public.. Let's flesh out this documentation with documentation of parameters and the return value. This method is doing a lot of stuff (for one example, explaining how we're changing the encoding at the end with the unpacking).. indentation here is a bit hard to follow, I'd at least most the last ).read down to its own line.. Are there any partial event receive cases we need to handle here, or is that abstracted out before we get to this phase? Defensive coding here is a good idea.. Would this also be dependent on the sigv4 changes where it pulls in an eventstream dependency from #1946?. We need to determine the backwards compatibility story on this gem. It's possible we will need to narrow this dependency to avoid surprise breaking changes and relax only after careful testing.. Any reason this isn't async_client: true?. Is this ever a valid combination? It does seem like we should explicitly raise in this situation.. To explore this a bit more, is this just a documentation artifact? I don't see anywhere in which this impacts the code generation.\nChecking since most of the implementation challenge around bidirectional is the input signaling, so if it's more than documentation we should dig deeper.. I'm only seeing this impacting documentation, but checking intent :). This looks like we are leaking threads. How are we cleaning these up?. Can we resolve this TODO question?. And if it's an earlier Ruby version? We need to have a user-friendly error when client creation is attempted in an older version of Ruby.. How are we surfacing these defaults to user-facing documentation?. Let's resolve this TODO item.. Need these docstrings filled out. Should answer my question from above, though.. There needs to be a counter to this, where the RUBY_VERSION is < 2.3 and we raise an appropriate error.. Ahh yes, looks like i only fixed this in one place. Updating.. I'm not a fan of an || statement with unpredictable side effects. Is there a reason we potentially wouldn't want to delete params[:event_stream_handler] if params[:output_event_stream_handler] is defined?. I want to make sure we include a test that provides the :unsigned_headers value as well, ensuring that we do not sign those headers. Say, 'Bar2' or similar.\nThe situation I want to avoid is where we suddenly start signing banned headers, which then get modified by proxies, and then cause breaking changes down the line.. I feel like this particular one is pretty stable, but I agree this is the line that feels the most hacky in the entire change. I'm going to apply a small refactor.... ",
    "pmarreck": "(I work with alussos)\nI also wonder why there's no begin/rescue/end code in the event the put/write fails for some reason, or any check of the response to see what it actually is... It is nice to assume that S3 never goes down or errors on writes/puts, but we all know this is not the case :)\n. FYI this happens in conjunction with the Paperclip gem, version 3.0.2, in its \"flush_writes\" method.\nHere is a fuller stack trace:\n/our_web_app_path/shared/bundle/ruby/1.9.1/gems/aws-sdk-1.5.1/lib/aws/core/response.rb:175:in method_missing'\n/our_web_app_path/shared/bundle/ruby/1.9.1/gems/aws-sdk-1.5.1/lib/aws/s3/s3_object.rb:313:inwrite'\n/our_web_app_path/shared/bundle/ruby/1.9.1/gems/paperclip-3.0.2/lib/paperclip/storage/s3.rb:286:in block in flush_writes'\n/our_web_app_path/shared/bundle/ruby/1.9.1/gems/paperclip-3.0.2/lib/paperclip/storage/s3.rb:272:ineach'\n/our_web_app_path/shared/bundle/ruby/1.9.1/gems/paperclip-3.0.2/lib/paperclip/storage/s3.rb:272:in flush_writes'\n/our_web_app_path/shared/bundle/ruby/1.9.1/gems/paperclip-3.0.2/lib/paperclip/attachment.rb:197:insave'\n/our_web_app_path/shared/bundle/ruby/1.9.1/gems/paperclip-3.0.2/lib/paperclip/instance_methods.rb:17:in block in save_attached_files'\n/our_web_app_path/shared/bundle/ruby/1.9.1/gems/paperclip-3.0.2/lib/paperclip/instance_methods.rb:10:inblock in each_attachment'\n/our_web_app_path/shared/bundle/ruby/1.9.1/gems/paperclip-3.0.2/lib/paperclip/instance_methods.rb:9:in each'\n/our_web_app_path/shared/bundle/ruby/1.9.1/gems/paperclip-3.0.2/lib/paperclip/instance_methods.rb:9:ineach_attachment'\n/our_web_app_path/shared/bundle/ruby/1.9.1/gems/paperclip-3.0.2/lib/paperclip/instance_methods.rb:16:in save_attached_files'\n/our_web_app_path/shared/bundle/ruby/1.9.1/gems/activesupport-3.0.13/lib/active_support/callbacks.rb:436:in_run_save_callbacks'\n/our_web_app_path/shared/bundle/ruby/1.9.1/gems/activerecord-3.0.13/lib/active_record/callbacks.rb:273:in create_or_update'\n/our_web_app_path/shared/bundle/ruby/1.9.1/gems/activerecord-3.0.13/lib/active_record/persistence.rb:60:insave!'\n/our_web_app_path/shared/bundle/ruby/1.9.1/gems/activerecord-3.0.13/lib/active_record/validations.rb:49:in save!'\n/our_web_app_path/shared/bundle/ruby/1.9.1/gems/activerecord-3.0.13/lib/active_record/attribute_methods/dirty.rb:30:insave!'\n/our_web_app_path/shared/bundle/ruby/1.9.1/gems/activerecord-3.0.13/lib/active_record/transactions.rb:245:in block in save!'\n/our_web_app_path/shared/bundle/ruby/1.9.1/gems/activerecord-3.0.13/lib/active_record/transactions.rb:292:inblock in with_transaction_returning_status'\n/our_web_app_path/shared/bundle/ruby/1.9.1/gems/activerecord-3.0.13/lib/active_record/connection_adapters/abstract/database_statements.rb:139:in transaction'\n/our_web_app_path/shared/bundle/ruby/1.9.1/gems/activerecord-3.0.13/lib/active_record/transactions.rb:207:intransaction'\n. ",
    "copernicus": "This is really cool. +1 for EMR support in the sdk.\n. Wow. +1 for this too. Please add CW support to the SDK. Thanks!\n. Any chance this will get merged?\nthanks!\n. +1 for a release with the RDS support. Thx!!!\n. Trying to add this myself to my local copy of the SDK. I changed the ec2/client.rb to new API version, and added the CopySnapshot operation to a new EC2-2012-12-01.yml. When I try to use the EC2.client.copy_snapshot, I am getting back an error like this:\ncan't convert Symbol into String - (TypeError)\n/Library/Ruby/Gems/1.8/gems/aws-sdk-1.7.1/lib/aws/core/client.rb:479:in delete'\n/Library/Ruby/Gems/1.8/gems/aws-sdk-1.7.1/lib/aws/core/client.rb:479:inbuild_request'\n/Library/Ruby/Gems/1.8/gems/aws-sdk-1.7.1/lib/aws/core/client.rb:434:in send'\n/Library/Ruby/Gems/1.8/gems/aws-sdk-1.7.1/lib/aws/core/client.rb:434:inclient_request'\n/Library/Ruby/Gems/1.8/gems/aws-sdk-1.7.1/lib/aws/core/response.rb:180:in call'\n/Library/Ruby/Gems/1.8/gems/aws-sdk-1.7.1/lib/aws/core/response.rb:180:inbuild_request'\n/Library/Ruby/Gems/1.8/gems/aws-sdk-1.7.1/lib/aws/core/response.rb:108:in initialize'\n/Library/Ruby/Gems/1.8/gems/aws-sdk-1.7.1/lib/aws/core/client.rb:183:innew'\n/Library/Ruby/Gems/1.8/gems/aws-sdk-1.7.1/lib/aws/core/client.rb:183:in new_response'\n/Library/Ruby/Gems/1.8/gems/aws-sdk-1.7.1/lib/aws/core/client.rb:433:inclient_request'\n/Library/Ruby/Gems/1.8/gems/aws-sdk-1.7.1/lib/aws/core/client.rb:334:in log_client_request'\n/Library/Ruby/Gems/1.8/gems/aws-sdk-1.7.1/lib/aws/core/client.rb:420:inclient_request'\n/Library/Ruby/Gems/1.8/gems/aws-sdk-1.7.1/lib/aws/core/client.rb:316:in return_or_raise'\n/Library/Ruby/Gems/1.8/gems/aws-sdk-1.7.1/lib/aws/core/client.rb:419:inclient_request'\nWhat am I doing wrong?\nthanks for your help\n. Found the issue. It was an error with how I was passing the options.\n. Thanks! I will try this out.\n. That was it. Thanks for the quick response!\n. Found the issue. The billing alerts need to be enabled in the billing console in order to get the cloudwatch to work. \n. ",
    "kadwanev": "Client usage roughly described: http://stackoverflow.com/questions/10942410/can-you-programmatically-control-elastic-mapreduce-jobs-easily\n. Love it! Thank you very much.\n. When can we hope for a cut release including RDS?\n. @trevorrowe Any feedback here?\n. Hmm, seems like parallel options. The docs I referenced were fresh as of Feb 2014.\nHow about if I change the default language back but still have the gem accept the alternate values?\n. Jinks! sounds like a plan\n. done\n. ",
    "pas256": "I would love to see this get merged in. What is needed for that to happen?\n. Hey Trevor,\nCan you post the work around code we talked about at re:invent here please?\n. Just checking how it works in Python with Boto - this is their approach\napi_params (dict) \u2013 a dictionary of additional parameters to pass directly to the EMR API (so you don\u2019t have to upgrade boto to use new EMR features). You can also delete an API parameter by setting it to None.\nhttp://boto.cloudhackers.com/en/latest/ref/emr.html\n. ",
    "ariejan": "Nice, but I don't see this getting pulled in to the (low level) aws-sdk gem. It would be a great candidate for a wrapper gem, though. \n. @trevorrowe In that case, this gets a :+1: from me :-)\n. ",
    "moorecp": "The gemspec wasn't supposed to be included in the pull request. I did that later just to make bundler happy when used in another project and forgot github would include the change in the pull request. I have removed that commit from the pull request.\n. ",
    "ab": "@trevorrowe I see that this is listed as resolved in the 1.6.4 release notes, but unless I'm missing something it doesn't look a fix was actually included. Thanks!\nhttp://aws.amazon.com/releasenotes/Ruby/0311621489672106\n. This also underscores the need for tests here. Sorry I haven't found time to write any yet.\n. @trevorrowe I'm experiencing this issue in a Ruby 2.1.0 codebase running aws-sdk 1.32.0. Wire traces indicate that once every several attempts, the connection would hang for almost exactly 20 seconds, then retry and immediately succeed. Maybe some kind of stale connection in the pool? @maxfolley's pool.empty! workaround is working for me.\n. @lsegal @trevorrowe Was this actually fixed for ruby 2.1.0? I've been seeing the problem in aws-sdk 1.32.0.\n. Thanks for fixing quickly! Any ETA on when the new release will be cut?\n. Ah really sorry about that. I didn't test on Ruby 1.8. Will try to follow up with something better.\n. Ah, evidently I didn't test integer inputs either. It's hard to get something generic since Integer#to_i doesn't take arguments, and Integer() evidently doesn't take a base in Ruby 1.8.\nHow about something like this:\n```\ndef format_auth_code(code)\n  if code.is_a?(String)\n    unless code =~ /\\A\\d+\\z/\n      raise ArgumentError.new(\"Code must be numeric\")\n    end\n    code = code.to_i(10)\n  end\nsprintf(\"%06d\", code)\nend\n``\n. (This has the disadvantage that it will reject inputs like\" 123\"`, whereas previously it would accept them.)\n. Just for summary, it was failing for different reasons in 1.8 and 1.9:\nIn 1.8, Integer() doesn't accept the base argument.\nIn 1.9, I didn't test integer arguments, so it would have been sufficient to use sprintf(\"%06d\", Integer(code.to_s, 10)).\nYou can use String#to_i, but that of course doesn't do any validation.\n. Sounds good. I was confused about why it was doing left padding in the first place. Would you like me to commit something like that?\n. I think it's a 500, but can double check that.\n. This indicates an HTTP 500, yes?\nI, [2014-06-27T13:30:49.505368 #13534]  INFO -- aws-api: [AWS Core 500 155.226213 9 retries] run_instances(...) AWS::EC2::Errors::InsufficientInstanceCapacity Insufficient capacity.\nWe're running with a large number of retries because in the normal case where it's an HTTP 400 throttling response, we really do want to retry until it succeeds.\n. Thanks!\n. ",
    "Flameeyes": "Hrm, you're right, sorry.\n. Hrm, something went wrong, but at least the commits now seem correct.\n. The SDK is version 1.11.3 and the failure happens with\ndev-lang/ruby-1.8.7_p371\ndev-lang/ruby-1.9.3_p429\n. Ah interesting, we do tend to skip over harddeps especially for build dependencies are in 99% of the cases they are there for bogus reason, so it's very possible that is the cause. Feel free to close, I'll investigate on the rspec version issue on my side!\n. ",
    "natemueller": "No problem.  Are you guys planning to cut a release soon?\n. ",
    "evanwieren": "Thanks. I know there are a number of items to look at when it comes to this. \nThanks.\n. This would be a client side encryption.\ndef encrypt_content(kms_key_id, content)\n    kms_client = Aws::KMS::Client.new\n    begin\n      resp = kms_client.encrypt({\n        key_id: kms_key_id,\n        plaintext: content\n      })\n    rescue => e\n      # TODO: get the actual exception\n      Rails.logger.debug \"Error! AWS problem encrypting contents\"\n      Rails.logger.debug \"#{e.message} #{e.backtrace.join(\"\\n\")}\"\n    end\n    resp ? resp.ciphertext_blob : nil\n  end\n. Yes, that worked. I had attempted to decode64, I did not try encode64. \nThanks \n\ud83d\udc4d \n. Duplicate\n. ",
    "AlexanderEkdahl": "Any updates regarding this feature?\n. ",
    "courtsimas": "+1\n. ",
    "yhirano": "+1\n. ",
    "tedsparc": "Hi lsegal,\nThanks so much for the fast response here.  I thought I'd mention that I think the line after your change in README.rdoc needs to be also be modified slightly:\n$ git clone git://github.com/aws/aws-sdk-ruby\n-  $ cd aws-sdk-for-ruby/samples/\n+  $ cd aws-sdk-ruby/samples/\n. ",
    "ohookins": "In general, it would be nice to have access to any object metadata that does not begin with x-amz-meta-\nThis is where the filtering takes place:\nhttps://github.com/aws/aws-sdk-ruby/blob/master/lib/aws/s3/client.rb#L914\nThe fix is trivial, but requires Amazon to make a design decision. @trevorrowe ?\n. @trevorrowe Admittedly, there is an issue with mixed concerns here. Unfortunately the fact still stands that fields like Content-Disposition are valid user-supplied metadata that can be set on the S3 object and retrieved when you GET/HEAD that object in all other circumstances. In fact the console allows you to set more or less any header on the object for retrieval.\nI think what you say makes sense - it does seem logical to have a mechanism for retrieving x-amz-meta- only, keeping the standard (or other non-standard) HTTP headers as a separate concern.\n. Thanks! :+1: \n. I suspected as much, and would have been surprised if I was the first to ask for this. Great that it is being worked on! Thanks for the response and the workaround :)\n. ",
    "almegeddon": "Sorry, close/reopen on accident.\nI believe returning the other HTTP headers in the head_object response is the right approach and as you've both pointed out, keeps the non-HTTP S3 meta as a separate concern.\n. ",
    "gmcnaughton": "+1\nI was surprised that there was no easy way to access the Cache-Control header set on my objects.  It would be great if head_object returned all the header values that can be set via write or copy_to, otherwise you have some headers that are write-only!\n. ",
    "mbleigh": ":+1:\nI'm trying to compare local files that may or may not have been gzipped to their S3 counterparts and not finding a way to do so. Is there currently any way to fetch content encoding using the Ruby SDK?\n. ",
    "alexmarchant": "@mbleigh I'm also here for this reason. Would be very helpful to be able to read content_encoding.\n. ",
    "wkoffel": "Content-Disposition was mentioned in this thread, but I don't see it in the Feb. 26, 2013 update.  Was there a reason this was left out?  Like others, I can't figure out any way to read a content-disposition header using the Ruby SDK.\n. ",
    "kaluznyo": "My issue is fixed in paperclip gem (https://github.com/thoughtbot/paperclip/issues/1000)\nFor me you can close this issue\n. ",
    "desheikh": "@trevorrowe, thanks for the response, but please note that keeping the initialization out of the loop will still have the same issue.\n~/.rbenv/versions/1.9.3-p327/lib/ruby/gems/1.9.1/gems/aws-sdk-1.7.1/lib/aws/core/signature/version_4.rb:87:in `service': NotImplementedError (NotImplementedError)\n~/.rbenv/versions/1.9.3-p327/lib/ruby/gems/1.9.1/gems/aws-sdk-1.7.1/lib/aws/core/client.rb:499:in build_request': undefined methodadd_authorization!' for #AWS::SimpleEmailService::Request:0x007fdff13558d0 (NoMethodError)\n. @trevorrowe, I've been able to execute it multiple times without issue as well, but If I keep at it I do get the occasional add_authorization error. Havn't been able to reproduce the error at \"version_4.rb:87\" again though.\nMy test file is exactly the same except for the array size which I've set to 90 and of course the from email.\n. Hi,\nThanks, I just tested out sending mails out with the latest version of the sdk and so far have not experienced any issues yet even without the eager_autoload.\nAWS.eager_autoload! raises an exception though, created a separate issue for it.\n. Hi,\nI've started getting similar errors when running my rspec tests with vcr cassetes.\nundefined method `continue_timeout=' for #\nTested with version 1.9.2\n. ",
    "organicveggie": "Yeah, I just happened to be trying to launch an EBS Optimized instance and I had to check with support to figure out how to do it. Once I realized the support was already in place to accomplish this, it just wasn't documented... Seemed like the least I could do. :)\n. ",
    "joelhewitt": "We have fixed the problem.\nThanks for bringing this to our attention.\n. ",
    "irons": "I'm still seeing this error, though only when I send a message to a queue I've been handed, not when I acquire the queue. I updated the case at https://forums.aws.amazon.com/thread.jspa?messageID=425282 with a short sample reproducing the problem, which has now been in constant effect for almost two days.\n. I'm using aws_sdk 3.1.5, as reported in the stack trace. I'm seeing this error 100% of the time across four machines, all running OS X (10.6.8, 10.7.5, and two on 10.8.2), all with aws_sdk 3.1.5, all using ruby 1.9.3-p374 via rbenv. The test script continues to fail for me, as above.\n. Dammit. aws_sdk 3.1.5 \u2260 aws-sdk 1.8.3.1. After uninstalling the former gem and installing the latter, the test script does succeed. I see how it happened, one of the machines got updated to the bogus gem, and then the other machines followed suit in the course of troubleshooting the first one. What a mess.\n. ",
    "rubysolo": "When I store a file at a \"path\" with a leading slash, it does not show up in my bucket -- it only works if I remove the leading slash from the key.\nWhen I don't strip the slash from the source key, I get \"No such key\" errors.  When I don't strip the slash from the target key, it doesn't give me a failure, but the new file does not exist at the expected path.\n. ",
    "bensie": ":metal: \n. :+1: \n. Sure is nice how easy GitHub makes even the smallest changes!\n. I ran into this yesterday as well. Pretty sure server cert can only be applied to the listener, not to the ELB directly.\n. ",
    "erikh": "Sorry, gave you the wrong link: https://github.com/aws/aws-sdk-ruby/blob/master/lib/aws/core/data.rb#L230\nFundamentally this lies, because methods defined on Object will never hit your method missing proxy, and therefore in this case I've duplicated AWS::Core::Data::List, which of course does not duplicate the array that it's delegating to.\n. welp, seems I was doing it pretty wrong here -- #[] doesn't deal with names, and I was confused. sorry for the distraction.\n. ",
    "kchowhan": "when i use copy_snapshot is it copying them to the same region. I created a client for destination region and invoked copy_snapshot. am i missing anything? does copy_snapshot method work across regions?\n. ",
    "monfresh": "The with_prefix method takes a string as an argument. My guess is that you didn't enclose the prefix with quotes. I just tried it and it worked for me.\n. Also, when you say o = s3.buckets[bucket].objects.with_prefix(dir), \"bucket\" and \"dir\" have to be actual bucket and folder names, both enclosed in quotes. \n. From the \"content_type\" documentation: http://docs.aws.amazon.com/AWSRubySDK/latest/AWS/S3/S3Object.html#content_type-instance_method\n\"Returns the content type as reported by S3, defaults to an empty string when not provided during upload.\"\nHow did you upload your objects to S3?\nWhen you uploaded them, did you specify the content-type?\nWhat happens if you manually set the content-type in the S3 console and then run your block with just \"puts f.content_type\"?\nI have an S3 folder I use to test with, and when I ran \"puts f.content_type\" on a series of objects that did not have the content-type explicitly set on upload (even though the Content-Type key does appear in the S3 console - it was added automatically but with the wrong content type: image/jpeg for css files for example), then I just get empty strings returned. Once I set the Content-Type manually in the S3 console for one of the objects to \"text/css\", that's what was returned to me for that object.\n. I think you are misunderstanding the meaning of \"expiration_date\". When you call \"puts f.expiration_date\", it is reading the field labeled \"Expiry Date\" in the S3 console (in the Properties tab for the object). The \"Expiry Date\" is set at the bucket level on a series of objects via the \"Lifecycle\" section in the Properties tab for the bucket, as explained here: https://forums.aws.amazon.com/ann.jspa?annID=1303\nIf you click on one of your csv objects in the S3 website, then click on the Properties tab, and if the \"Expiry Date\" field is empty, then \"puts f.expiration_date\" will return an empty string.\nWhen you call \"f.url_for(:read, :expires => 10*60, :secure => true)\", it is NOT setting the \"expiration_date\" of the object. It is simply creating a URL that expires in 10 minutes.\nNow, the fact that you are getting an error instead of an empty string is another issue that I can't explain. My guess is that there might be a conflict with another gem. I would create a clean gemset with RVM that only contains the aws-sdk gem and see if you still get the same error.\n. Thanks for the script. I'm going to be away from my computer for the rest of the day, so I can't try it now. I also haven't played around with creating buckets and objects with the sdk yet, so I can't evaluate your code. \nIf you haven't already, I would troubleshoot this by manually setting an expiry date (via the s3 website) for objects in a new clean test bucket, then running only the portion of your script that reads f.expiration_date. If you get the expiry date back, then you know the issue lies with your script that sets the expiry date. Repeat with content type. \nOn Jan 2, 2013, at 13:56, \"Joshua T. Mckinney\" notifications@github.com wrote:\n\nThis reproduces the error.\nbucket_name = 'my-test-bucket-123321123'\ndir = 'temp/'\nfile_name = 'test.csv'\ncsv = \"1,2,3\\t\\n4,5,6\"\ns3 = AWS::S3.new\nbucket = s3.buckets.create(bucket_name)\nbucket.lifecycle_configuration.update do\n  add_rule dir, 1\nend\nbucket.objects.create(\"#{dir}#{file_name}\", StringIO.new(csv),:content_type => 'text/csv')\no = s3.buckets[bucket_name].objects.with_prefix(dir)\no.each do |f|\n  puts f.url_for(:read, :expires => 10*60, :secure => true) # => returns url as expected\n  puts f.expiration_date # => raises exception 'can't convert nil into String'\n  puts f.content_type # => raises exception 'can't convert nil into String'\nend\nbucket.delete!\n\u2014\nReply to this email directly or view it on GitHub.\n. \n",
    "vsespb": "also #122 \n. ",
    "jeffshantz": "I should add that I am using version 1.8.0 from the repository:\nruby\ngem 'aws-sdk', git: \"https://github.com/aws/aws-sdk-ruby\"\n. Looking in to this further, when I create an IAM user in IRB, it appears that the user's name is not set in the object that is returned:\nruby\nu = iam.users.create('testing')\n => <AWS::IAM::User name:> \nruby-1.9.2-p290 :004 > u.name\n => nil\nThis explains the first error above (expected string value for option user_name).  I reiterate that the user is created successfully, but the user's name in the returned object is not set.\nIn fact, it appears that user's names are never set (even when using each):\nruby\niam.users.each { |u| puts \"#{u.name} (#{u.inspect})\" }\n (<AWS::IAM::User name:>)\n (<AWS::IAM::User name:>)\n => nil\nAs you can see, both of my test users (which appear just fine in the web console) are being returned, but they appear to be empty when retrieved using each.\n. I tried creating the login profile manually:\n``` ruby\nruby-1.9.2-p290 :034 > u = iam.users.create(\"testuser\")\n => \nruby-1.9.2-p290 :036 > u = iam.users[\"testuser\"]\n =>  \nruby-1.9.2-p290 :037 > lp = AWS::IAM::LoginProfile.new(u)\n =>  \nruby-1.9.2-p290 :038 > lp.password = \"abcDEF123!\"\nAWS::Errors::Base: \n\nSender\nNoSuchEntity\nCannot find Login Profile for User testuser\n\n296e1d1a-60dc-11e2-8c9d-7786bfa02548\n\nfrom /Users/jeff/.rvm/gems/ruby-1.9.2-p290@2212/gems/aws-sdk-1.8.0/lib/aws/core/client.rb:318:in `return_or_raise'\nfrom /Users/jeff/.rvm/gems/ruby-1.9.2-p290@2212/gems/aws-sdk-1.8.0/lib/aws/core/client.rb:419:in `client_request'\nfrom (eval):3:in `update_login_profile'\nfrom /Users/jeff/.rvm/gems/ruby-1.9.2-p290@2212/gems/aws-sdk-1.8.0/lib/aws/iam/login_profile.rb:53:in `password='\nfrom (irb):38\nfrom /Users/jeff/.rvm/rubies/ruby-1.9.2-p290/bin/irb:16:in `<main>'\n\n```\nOk, no luck.  Next, I took a look at the code for password= and noticed that if the login profile does not exist, the code calls client.create_login_profile.  Hence, I tried doing the same using instance_eval on the LoginProfile object:\nruby\nruby-1.9.2-p290 :040 > lp.instance_eval('client.create_login_profile(password: \"abcDEF123!\")')\nArgumentError: missing required option user_name\n    from (irb):40:in `instance_eval'\nOk, let's add the user_name then:\nruby\nruby-1.9.2-p290 :043 > u = iam.users[\"testuser\"]\nruby-1.9.2-p290 :045 > lp = AWS::IAM::LoginProfile.new(u)\n => <AWS::IAM::LoginProfile user_name:testuser>\nruby-1.9.2-p290 :046 > lp.instance_eval('client.create_login_profile(user_name: \"testuser\", password: \"abcDEF123!\")')\n => {:xmlns=>\"https://iam.amazonaws.com/doc/2010-05-08/\", :login_profile=>{:user_name=>nil, :create_date=>nil, :must_change_password=>nil}, :response_metadata=>{:request_id=>nil}}\nruby-1.9.2-p290 :047 > u.login_profile.exists?\n => true\nSure enough, that worked.  I can now log in to the web console as that user.  Just thought I'd add this workaround in case anyone else is having the same problem.\n. My apologies.  I had reinstalled Ruby using RVM and forgot to reinstall all the gems (some of which had external libraries built against the old version of Ruby), so it turns out that that was what was causing this strange issue.  When I deleted my gemset and reinstalled all the gems within in, this problem went away.\n. Actually, it turns out that one can call exists? on this since User is a subclass of Resource; however, it is not documented.  Perhaps one could add the following to User (this was taken from IAM::Group):\n``` ruby\n(see Resource#exists?)\ndef exists?; super; end\n```\nThis way, the generated RDoc will show User as having an exists? method.\n. Sorry, see #130.  I'm a newb.\n. My apologies.  I had reinstalled Ruby using RVM and forgot to reinstall all the gems (some of which had external libraries built against the old version of Ruby), so it turns out that that was what was causing this strange issue.  When I deleted my gemset and reinstalled all the gems within in, this problem went away.\n. ",
    "ryantate": "It looks like, for purposes of my app, I can just have everyone use U.S. buckets no matter where they are. It appears aws-sdk will create buckets by default in U.S. no matter where the user is so it should be fine. Closing out.\n. ",
    "moorage": "+1\n. ",
    "xli": "Is this same issue?\nAWS::S3::Errors::TemporaryRedirect (Please re-send this request to the specified temporary endpoint. Continue to use the original request endpoint for future requests.):\n  WEB-INF/gems/jruby/1.8/gems/aws-sdk-1.11.3/lib/aws/core/client.rb:360:in return_or_raise'\n  WEB-INF/gems/jruby/1.8/gems/aws-sdk-1.11.3/lib/aws/core/client.rb:461:inclient_request'\n  (eval):3:in list_objects'\n  WEB-INF/gems/jruby/1.8/gems/aws-sdk-1.11.3/lib/aws/s3/object_collection.rb:297:inlist_request'\n  WEB-INF/gems/jruby/1.8/gems/aws-sdk-1.11.3/lib/aws/s3/paginated_collection.rb:29:in _each_item'\n  WEB-INF/gems/jruby/1.8/gems/aws-sdk-1.11.3/lib/aws/core/collection/with_limit_and_next_token.rb:54:in_each_batch'\n  WEB-INF/gems/jruby/1.8/gems/aws-sdk-1.11.3/lib/aws/core/collection.rb:80:in each_batch'\n  WEB-INF/gems/jruby/1.8/gems/aws-sdk-1.11.3/lib/aws/s3/object_collection.rb:253:indelete_all'\n. My workaround is adding the followings (we used the following 3 services in this app):\n  require 'aws/sqs'\n  require 'aws/dynamo_db'\n  require 'aws/s3'\nbefore AWS.eager_autoload!.\nAfter this change, I don't see similar error in my production environment anymore.\nIf you thought there is nothing more you can do, please close this ticket. I know it's very tricky to do autoload in JRuby.\n. ",
    "ravionrails": "After updating (to v 1.8.0) it's working now, thanks @trevorrowe \n. ",
    "karlfreeman": "I'm using it roughly like so:\n```\ndynamodb_query = dynamodb_table.batch_get(:all, feed)\nunless dynamodb_query.nil?\n  dynamodb_query.each { | activity |\n    parse_activity(activity)\n  }\nend\n```\n. Sure, not much to the back trace though. Thanks for the quick reply!\ngems/aws-sdk-1.8.0/lib/aws/dynamo_db/batch_get.rb:150 \u2022 each\ngems/aws-sdk-1.8.0/lib/aws/dynamo_db/batch_get.rb:169 \u2022 each_attributes\n/usr/local/api/releases/20130124112142/lib/unmagnify/me/activity/index.rb:51 \u2022 each\n/usr/local/api/releases/20130124112142/lib/unmagnify/me/activity/index.rb:51 \u2022 response\nbundler/gems/goliath-ea70864f86a9/lib/goliath/api.rb:227 \u2022 call\nbundler/gems/goliath-ea70864f86a9/lib/goliath/rack/async_middleware.rb:73 \u2022 call\nbundler/gems/goliath-ea70864f86a9/lib/goliath/rack/async_middleware.rb:73 \u2022 call\nbundler/gems/goliath-ea70864f86a9/lib/goliath/rack/simple_aroundware_factory.rb:77 \u2022 call\nbundler/gems/goliath-ea70864f86a9/lib/goliath/rack/validation/default_params.rb:39 \u2022 call\nbundler/gems/goliath-ea70864f86a9/lib/goliath/rack/validation/default_params.rb:39 \u2022 call\nbundler/gems/goliath-ea70864f86a9/lib/goliath/rack/params.rb:66 \u2022 block in call\nbundler/gems/goliath-ea70864f86a9/lib/goliath/rack/validator.rb:40 \u2022 safely\nbundler/gems/goliath-ea70864f86a9/lib/goliath/rack/params.rb:64 \u2022 call\nbundler/gems/goliath-ea70864f86a9/lib/goliath/rack/default_mime_type.rb:26 \u2022 call\n/usr/local/api/releases/20130124112142/lib/unmagnify/middleware/path_params.rb:18 \u2022 block in call\nbundler/gems/goliath-ea70864f86a9/lib/goliath/rack/validator.rb:40 \u2022 safely\n/usr/local/api/releases/20130124112142/lib/unmagnify/middleware/path_params.rb:15 \u2022 call\nbundler/gems/goliath-ea70864f86a9/lib/goliath/rack/validation/request_method.rb:29 \u2022 call\nbundler/gems/goliath-ea70864f86a9/lib/goliath/rack/async_middleware.rb:73 \u2022 call\n/usr/local/api/releases/20130124112142/lib/unmagnify/middleware/cors.rb:23 \u2022 call\nbundler/gems/goliath-ea70864f86a9/lib/goliath/rack/async_middleware.rb:73 \u2022 call\nbundler/gems/goliath-ea70864f86a9/lib/goliath/rack/async_middleware.rb:73 \u2022 call\nbundler/gems/goliath-ea70864f86a9/lib/goliath/rack/async_middleware.rb:73 \u2022 call\ngems/rack-1.4.3/lib/rack/content_length.rb:14 \u2022 call\ngems/async-rack-0.5.1/lib/async_rack/async_callback.rb:114 \u2022 call\ngems/async-rack-0.5.1/lib/async_rack/async_callback.rb:91 \u2022 block in new\nbundler/gems/goliath-ea70864f86a9/lib/goliath/rack/builder.rb:56 \u2022 call\nbundler/gems/goliath-ea70864f86a9/lib/goliath/rack/builder.rb:56 \u2022 block (3 levels) in build\ngems/http_router-0.9.7/lib/http_router.rb:165 \u2022 call\ngems/http_router-0.9.7/lib/http_router.rb:165 \u2022 process_destination_path\n(eval):178 \u2022 block (2 levels) in inject_root_methods\n(eval):168 \u2022 catch\n(eval):168 \u2022 block in inject_root_methods\n(eval):107 \u2022 block in inject_root_methods\ngems/http_router-0.9.7/lib/http_router/node/root.rb:68 \u2022 []\ngems/http_router-0.9.7/lib/http_router.rb:118 \u2022 block in call\ngems/http_router-0.9.7/lib/http_router.rb:118 \u2022 catch\ngems/http_router-0.9.7/lib/http_router.rb:118 \u2022 call\nbundler/gems/goliath-ea70864f86a9/lib/goliath/rack/async_middleware.rb:73 \u2022 call\n/usr/local/api/releases/20130124112142/lib/unmagnify/middleware/cors.rb:23 \u2022 call\nbundler/gems/goliath-ea70864f86a9/lib/goliath/rack/favicon.rb:26 \u2022 call\nbundler/gems/goliath-ea70864f86a9/lib/goliath/rack/async_middleware.rb:73 \u2022 call\ngems/rack-1.4.3/lib/rack/content_length.rb:14 \u2022 call\ngems/async-rack-0.5.1/lib/async_rack/async_callback.rb:114 \u2022 call\ngems/async-rack-0.5.1/lib/async_rack/async_callback.rb:91 \u2022 block in new\nbundler/gems/goliath-ea70864f86a9/lib/goliath/request.rb:163 \u2022 call\nbundler/gems/goliath-ea70864f86a9/lib/goliath/request.rb:163 \u2022 block in process\n. I thought perhaps #161 might fix this, however its close but no cigar. Still having\nNoMethodError: undefined method 'each_pair' for nil:NilClass\nat https://github.com/aws/aws-sdk-ruby/blob/master/lib/aws/dynamo_db/batch_get.rb#L150 sporadically.\n. Looking at that stack trace it doesn't appear to be related. However don't hold me to it :wink: \n. @trevorrowe Is there anything more I can do to help figure out why these nil's keep cropping in response.data['Responses']\n. There's a good chance that we're getting throttled every now and again so that could be the cause.\n. I'll also add I've just seen this happen on response.data[\"Items\"] too\n. Thanks for the clarification. Keep up the good work.\n. Thanks for the clarification. Keep up the good work.\n. I think this could also close #140 I'll update and check\n. ",
    "jmclachlan": "I wonder if this is all related. I randomly get this error quite a bit when using DynoDB with threads.\nundefined method tables' for <AWS::DynamoDB>:AWS::DynamoDB\n/task/__gems__/gems/aws-sdk-1.8.5/lib/aws/record/hash_model.rb:86:indynamo_db_table'\n/task/gems/gems/aws-sdk-1.8.5/lib/aws/record/hash_model/finder_methods.rb:28:in find_by_id'\n/task/__gems__/gems/aws-sdk-1.8.5/lib/aws/record/scope.rb:97:infind'\n/task/gems/gems/aws-sdk-1.8.5/lib/aws/record/hash_model/finder_methods.rb:77:in `find'\n. Nope. You are right. I think this is just a more general issue with the base client initialization and threads. My bad. \nhttps://github.com/aws/aws-sdk-ruby/issues/90\n. @trevorrowe Awesome. That fixes everything. Thanks a bunch.\n. ",
    "nelhage": "I've been running that overnight, and we look good so far. I'll reopen if the issue shows up later, but I think we're probably good. Thanks!\n. ",
    "dandunckelman": "Aww, I see. I had the bad assumption that it was NOT required, even if my origin is an S3 bucket.\nI suppose it would be more clear if it stated that it was required when the origin is an S3 bucket.\n. Great. Thanks for the help.\n. This should probably raise this error:  http://docs.aws.amazon.com/AWSRubySDK/latest/AWS/Errors/MissingCredentialsError.html\n. Nice!\nJust for reference, I tested it and got the following:\nruby\n/home/dldunckel/.rvm/gems/ruby-1.9.3-p194@sites/gems/aws-sdk-1.8.5/lib/aws/core/client.rb:339:in `return_or_raise': The AWS Access Key Id you provided does not exist in our records. (AWS::S3::Errors::InvalidAccessKeyId)\n    from /home/dldunckel/.rvm/gems/ruby-1.9.3-p194@sites/gems/aws-sdk-1.8.5/lib/aws/core/client.rb:440:in `client_request'\n    from (eval):3:in `get_bucket_versioning'\n    from /home/dldunckel/.rvm/gems/ruby-1.9.3-p194@sites/gems/aws-sdk-1.8.5/lib/aws/s3/bucket.rb:455:in `versioning_state'\n    from /home/dldunckel/.rvm/gems/ruby-1.9.3-p194@sites/gems/aws-sdk-1.8.5/lib/aws/s3/bucket.rb:443:in `versioning_enabled?'\n    from /home/dldunckel/.rvm/gems/ruby-1.9.3-p194@sites/gems/aws-sdk-1.8.5/lib/aws/s3/bucket.rb:506:in `exists?'\n    from create.rb:141:in `bucketExists'\n    from create.rb:343:in `uploadSiteContent'\n    from create.rb:276:in `uploadContent'\n    from create.rb:35:in `initialize'\n    from create.rb:391:in `new'\n    from create.rb:391:in `<main>'\nThanks!\n. Hmm, I just saw this:  https://github.com/aws/aws-sdk-ruby/pull/47\n. No problem! And thanks for the fix!\n. @trevorrowe not a problem.  I just moved a script over to v2 yesterday w/o much issue, so thanks for all the hard work on the new version.\nSo, my goal was to automatically confirm the subscription, but based on what you're telling me:  because the endpoint is an email, I can't programmatically confirm the subscription (outside of reading/parsing the email).  Is that correct?\n. @trevorrowe sounds good!\n. ",
    "qminhdo": "Thanks for clarify. As reference in Doc: Item objects do not have any attribute data populated. The Attributes#to_hash is the one that makes get_item request. \n. Actually, you can just pass additional options available in AWS::DynamoDB::Client into the #query e.g :limit, :scan_index_forward. Just make sure to add the :select option with [attributes_to_get] so you get ItemData object  (real data) right away; otherwise you will have Item object as return value without real data and will have to make addition get_item request using item.attributes.to_h (As mentioned in https://github.com/aws/aws-sdk-ruby/issues/151).\nruby\ntable.items.query(:select => :all, :range_greater_than=> \"1\", :limit => 2,  :consistent_read => true,\n                       :exclusive_start_key => {  # => this is ugly\n                            :hash_key_element => {:s => \"1\"},\n                            :range_key_element => {:s => \"11\"}\n                        }     \n) do |item_data|\n puts item_data.attributes #=> a hash of selected attributes\n item_data.item #=> the AWS::DynamoDB::Item object for this attribute bag\nend\n. ",
    "marceldegraaf": "Thanks! I already thought that was the reason it wasn't pushed yet so I thought I'd make a reminder here :-).\n. Sure! These are the relevant parts of the ProxyHealthChecker class. \nIt's used to check if a proxy server is up and running, by making a connection through it with HTTPI. As the proxy ElasticIP addresses can be changed quickly (as a means to prevent abuse), we store the current ElasticIP to proxy server mapping in SimpleDB.\n``` ruby\nrequire 'httpi'\nrequire 'logger'\nclass ProxyHealthChecker\n  attr_accessor :role\ndef initialize(role)\n    @role = role\n  end\ndef healthy?\n    perform_check\n  end\ndef perform_check\n    request.url = health_check_url\n    request.proxy = \"http://#{proxy_ip}:9999\"\n    response = HTTPI.get(request)\nif response.code.to_s =~ /2\\d{2}/\n  true\nelse\n  false\nend\n\nend\ndef proxy_ip\n    simple_db_domain.items.where(role: @role).first.attributes[:ip].values.first\n  end\ndef simple_db_domain\n    @simple_db_domain ||= simple_db.domains[\"mobile_proxy_eips\"]\n  end\ndef simple_db\n    region = ENV['REGION'] || Settings.health_check_region\n@simple_db ||= AWS::SimpleDB.new(region: region)\n\nend\nend\n```\nThe error occurs in the proxy_ip method, when calling simple_db.items. We first  thought we had a class name clash, as the error occurs when AWS-SDK calls ItemCollection.new. However, this doesn't seem the case as we do not seem to have another ItemCollection class somewhere.\nFor the sake of completeness, this is our Gemfile:\n``` ruby\nsource 'https://rubygems.org'\ngem 'sinatra'\ngem 'httpi'\ngem 'rake'\ngem 'rails_config'\ngem 'aws-sdk'\ngroup :test do\n  gem 'simplecov'\n  gem 'rspec'\n  gem 'rack-test'\nend\n```\nDoes this help?\n. This is in Sinatra, which runs on Ruby 1.9.3 behind Nginx and Passenger. So I don't think it's multi-threaded.\nIf we add AWS.eager_autoload! to our app.rb the issue should be solved? I assume the added slowness only occurs when starting the app, e.g. not on every AWS-related action?\n. Hi @trevorrowe, no problem. We actually haven't added AWS.eager_autoload! to our app, but the problem hasn't occurred lately. Could it be that changes to the SimpleDB API have solved the issue for us?\n. ",
    "kenn": "Thanks for the fix! This release saved my day.\n. Oh great! This certainly does the job. Thanks!\n. ",
    "tonyhb": "Thanks! Had error #146 occurring a lot in our code, too.\n. ",
    "cdanzig": "So after digging a bit deeper is looks like /lib/aws/api-config/ElasticTranscoder-2012-09-25.yml is defining codecOption rules the following way:\nyaml\nCodecOptions:\n  :sym: :codec_options\n  :type: :map\n  :members:\n    :sym: :map_value\n    :type: :string\nIt seems to me (although I am not familiar enough with the code) that this should be something like:\nyaml\nCodecOptions:\n  :sym: :codec_options\n  :type: :hash\n  :members:\n    Profile:\n      :sym: :profile\n      :type: :string\n    Level:\n      :sym: :level\n      :type: :string\n    MaxReferenceFrames:\n      :sym: :max_reference_frames\n      :type: :string\n. Thanks Trevor. I'll keep an eye out for the release.\n. NM.... I am an idiot and misspelled CONTROL\n(I swear I looked at it a dozen times)\n. ",
    "zwily": "I suppose a plugin would work. Is there any mechanism to automatically load aws-sdk plugins every time aws-sdk is required, so we don't have to ensure that all our scripts explicitly pull in the plugin? I don't see anything about plugins in the current SDK.\n. We use a lot of different scripts that use aws-sdk, some written by us, some not. Some are distributed as gems, some aren't. Adding something to the Gemfile for all those situations is just not really reasonable. I agree with you that this probably doesn't belong in core, but unless there's a way for me to have it automatically pulled in every time aws-sdk is pulled in, it's not all that useful.\nIt may not matter anyway though... The same day I did this pull request, I also started another little tool that helps keep AWS creds in the keychain and create shells with the appropriate environment vars: https://github.com/zwily/aws-keychain-util. \nI've been using that for the last week and it's been great, so I'm not even sure this pull request is all that important to me anymore. :smile:\n. This probably applies to other settings that are normally not copied with S3Object#copy_from as well, like :server_side_encryption and :reduced_redundancy.\n. When I first looked at the #copy_from method, I thought that the part that sets the :metadata_directive was attempting to preserve those. But I read it backwards - if :metadata is set, then it will use REPLACE, so everything gets reset to defaults.\nThat part is confusing too - it seems like if I don't set :metadata, :content_disposition, :content_type, or :cache_control in a copy, they will be preserved. If I set one of them, the rest will be reset (assuming I understand the S3 docs on that correctly.)\n. Agreed - #copy_from should maintain the semantics of CopyObject. (Although it would be nice if the #copy_from documentation made the point about \"if you set one you must set them all\" more clear.)\n. Ick, yeah that's messy. If it's too bad, then just updating the documentation to say that it uses #copy_from semantics is likely enough for now. That at least would have kept me from getting bitten by it.\n. ",
    "pmenglund": "Still seeing the error with the latest bits:\n```\nmartin@mbp[master]$ gem list aws-sdk\n LOCAL GEMS \nmartin@mbp$ git clone git@github.com:aws/aws-sdk-ruby.git\nCloning into 'aws-sdk-ruby'...\nremote: Counting objects: 9724, done.\nremote: Compressing objects: 100% (3011/3011), done.\nremote: Total 9724 (delta 7143), reused 8736 (delta 6542)\nReceiving objects: 100% (9724/9724), 3.42 MiB | 863 KiB/s, done.\nResolving deltas: 100% (7143/7143), done.\nmartin@mbp$ cd aws-sdk-ruby\nmartin@mbp[master]$ RUBYLIB=lib pry\n[1] pry(main)> require 'aws-sdk'\n=> true\n[2] pry(main)> AWS.config(:access_key_id => \"...\", :secret_access_key => \"...\")\n=> \n[3] pry(main)> ec2 = AWS::EC2.new\n=> \n[4] pry(main)> ami = ec2.images[\"ami-807cefe9\"]\n=> \n[5] pry(main)> ami.block_device_mappings\nNoMethodError: undefined method ebs' for {:device_name=>\"/dev/sdb\", :virtual_name=>\"ephemeral0\"}:Hash\nfrom /Users/martin/src/ruby/aws-sdk-ruby/lib/aws/core/data.rb:101:inmethod_missing'\n[6] pry(main)> ^D\n```\n. Thanks! Do you know when you'll release a new gem?\n. Thanks!\n. ",
    "nate": "Also, this introduces a bit of latency in the case where you know the endpoint is good, so in some cases it may make sense to disable the handling of redirects.\n. Also, normal retries do not help if they do not take redirects into account.\n. For reference, here is where RightAWS actually handles the 100 Continue response:\nhttps://github.com/rightscale/right_http_connection/blob/master/lib/base/net_fix.rb#L144-L176\n. I'm working with our customers to get access to a bucket where this occurs so I can both recreate the issue and confirm that this patch fixes it. Once I hear back in the affirmative I'll give it a shot.\nI know from much googling that most broken pipe errors with S3 libraries are due to not using a regional endpoint, especially the old issue with european buckets. The issue I'm seeking to address wasn't fixed by using the correct regional endpoint. Some buckets would still consistently get broken pipe errors unless you did the 100-continue trick and followed any redirects. We found this to be true both with fog and older versions of right_aws, and both at the time handled redirecting to the correct regional endpoints. I'm unsure of the nuances that are occurring that cause these issues as they're very hard to debug, but I'm hoping we can finally get to the bottom of it.\nI agree completely with the proposed 100-continue behavior, that sounds quite similar how right_aws implemented it, but they do it for every request. Another idea is to retry with 100-continue in the case of broken pipe errors.\n. Code to generate the errors:\n``` ruby\naccess_key = \"\"\naccess_secret = \"\"\nregion = \"us-east-1\"\nclient = Aws::S3::Client.new(region: region, credentials: Aws::Credentials.new(access_key, access_secret))\nkey = \"\"\nbucket = \"\"\nowner_id = \"\"\nclient.put_object_acl({\n  key: key,\n  bucket: bucket,\n  access_control_policy: {\n    grants: [\n      {\n        grantee: {\n          id: \"badid\",\n          type: \"CanonicalUser\",\n        },\n        permission: \"READ\"\n      }\n    ],\n    owner: {\n      id: owner_id\n    }\n  }\n})\nclient.put_object_acl({\n  key: key,\n  bucket: bucket,\n  access_control_policy: {\n    grants: [\n      {\n        grantee: {\n          id: \"mybad@emailthatdoesnotwork.com\",\n          type: \"AmazonCustomerByEmail\",\n        },\n        permission: \"READ\"\n      }\n    ],\n    owner: {\n      id: owner_id\n    }\n  }\n})\nclient.put_object_acl({\n  key: key,\n  bucket: bucket,\n  access_control_policy: {\n    grants: [\n      {\n        grantee: {\n          id: \"https://abadgroupuri.com/neverworks\",\n          type: \"Group\",\n        },\n        permission: \"READ\"\n      }\n    ],\n    owner: {\n      id: owner_id\n    }\n  }\n})\n``\n. So S3 returns an InvalidArgument error for invalid id's and uri's, but returns a UnresolvableGrantByEmailAddress for invalid emails? It's not the message but the error class that's the problem.\n. Ah, ok, bummer. Thanks!\n. Actually, [I think I found it](https://github.com/aws/aws-sdk-ruby/blob/master/aws-sdk-core/apis/s3/2006-03-01/api-2.json#L3488-L3507). The [contribution guidelines](https://github.com/aws/aws-sdk-ruby/blob/master/CONTRIBUTING.md#what-you-should-keep-in-mind) say I should submit an issue for this. Should I put together a pull request, too?\n. I think it's missing a few other places in that file as well.\n. Yup, thanks!\n. This could be fixed by changing the_http_response_bodyinstance method onAws::Log::Formatter` to this:\nruby\ndef _http_response_body(response)\n  if response.context.http_response.body.respond_to?(:rewind)\n    @param_formatter.summarize(response.context.http_response.body_contents)\n  end\nend. Alternatively, Seahorse::Client::Http::Response#body_contents could raise a specific exception in this case, something like UnreadableBodyError or similar, and that specific exception could be caught in Aws::Log::Formatter#_http_response_body.. The issue for us is that a log format can cause an exception to be raised. That shouldn't be the case, in my opinion. I can't optionally include exclude a field from the log format when the response is BlockIO.\nMaybe some background would help? We're wrapping an S3 client instance with our own code. This code creates the client with some defaults (including setting the log format and passing in the logger) and when executing operations it handles a few conveniences (extra logging, instrumentation, etc). Sometimes downloads are passed a block and we allow it.\nIf the log format can cause exceptions to be raised we will either need to write our own log formatter which would duplicate everything in Aws::Log::Formatter except catch an exception or we will need to keep two S3 client objects around and call one or the other depending on if a block is passed.\nIt seems much more sensible for _http_response_body to return nil or some sort of indicator that the body is irretrievable, since it's just a convenience method for logging. At a minimum a recognizable error for this issue would be nice, since RuntimeError could be so many other things. Specifically an error I can catch in a replacement Aws::Log::Formatter..     The reason why I'm raising error instead of letting it go silently is that the usage is actually taking in a wrong .body\nusing the block directly will lead to wrong body, and this is not a good practice\n\nI don't understand, isn't the block syntax supported by the API? The block syntax makes certain things possible that would be difficult without it.\nI get that you're resistant to catching the error and returning the wrong body, but how about adding a new log formatter field like :http_response_body_unless_block_io? Then at least people can opt into the behavior.\nI'm not sure how else I'd work around this issue except using two separate s3 clients initialized with two separate log formatters, one for use with block transfers and one for file transfers.. ruby\nclient.get_object(bucket: bucket, key: key){|_chunk| }\nThis will stream bytes to the block as they become available, right?\nruby\nclient.get_object(bucket: bucket, key: key).body {|_chunk| }\nAnd this will download the entire file into memory, right?\nDownloading the entire file into memory is unacceptable.. Thanks!. Excellent, thanks!. ",
    "ahannon-fiksu": "I definitely understand your reservations, and the reasoning makes sense.\nI guess my concern is that I am using HashModel, and I don't have easy control over the id. Is it guaranteed to always be lower case? I guess my concern is over this protected method, which I was planning on overriding in a subclass:\nruby\ndef populate_id\n  @_id = UUIDTools::UUID.random_create.to_s\nend\nHowever, I don't like the idea of setting @_id in my subclass. What if we had a separate protected method, called generate_id, with the default implementation looks like this, with the populate_id changed appropriately:\n``` ruby\nprotected \ndef generate_id\n  UUIDTools::UUID.random_create.to_s\nend\ndef populate_id\n  @_id = generate_id\nend\n```\nThis would allow me to override generate_id in my subclass, ensuring the same case all around?\nWould you be amenable to this change?\n. I guess my question is this: does UUIDTools::UUID.random_create.to_s always return an all lowercase UUID? If not, I need to manage the id on my own (basically, calling @_id = UUIDTools::UUID.random_create.to_s.downcase in populate_id), which is not desirable. I can always just .downcase the incoming UUID from 3rd parties if the generated id is guaranteed to be always be downcased.\nIf [0-9a-f] is in fact the class of characters \u2014 and not [0-9a-fA-F] \u2014 then downcasing the id from the 3rd party will work. However, I need to be sure that is the case, either through documentation or controlling it on my end. Does this make sense?\n. Thank you for the explanation.\nI ended up going in a different direction (using regular DynamoDB calls), since I prefer the call to MyModel.create! to fail if the item already exists, and I couldn't see how to do that with HashModel. However, if that requirement wasn't required in my current project, I think your solution would have worked for me...\n. ",
    "ansman": "Nvm me, I had an ancient version installed.\n. ",
    "rusanu": "Thanks for looking into it. Updating aws-sdk to 1.7.7 fixed the issue indeed.\n. ",
    "menno": "That's good to hear. \nThe availability-zone can be retrieved from http://169.254.169.254/latest/meta-data/placement/availability-zone (not the instance-profile, my mistake). In general I like the way the credential-providers are implemented. It keeps it very simple to configure the credentials in our test, development and production-environments across multiple regions.\n. Thanks! That seems to have solved the issue. \n. We've been experiencing a similar issue so I took a stab at implementing the retries in the EC2Provider.  You can find it here: https://github.com/menno/aws-sdk-ruby/tree/feature_ec2-credential-provider-retry\nWe've been testing this in production and through logging I did notice that the metadata-service is sometimes very hard to reach. See the http-debug-output-log below where it can take up to the 5th retry to get a response. \nThe sdk is configured with AWS.config(credential_provider: AWS::Core::CredentialProviders::EC2Provider.new(retries: 5, http_debug_output: logger)\nLog output:\nopening connection to 169.254.169.254...\nopened\n<- \"GET /latest/meta-data/iam/security-credentials/ HTTP/1.1\\r\\nAccept: */*\\r\\nUser-Agent: Ruby\\r\\nHost: 169.254.169.254\\r\\n\\r\\n\"\nConn close because of error Timeout::Error\n[aws-sdk] 2014-09-19 13:01:28 UTC -- Retrying after #<Timeout::Error: Timeout::Error> (5 retries left)\nopening connection to 169.254.169.254...\nopened\n<- \"GET /latest/meta-data/iam/security-credentials/ HTTP/1.1\\r\\nAccept: */*\\r\\nUser-Agent: Ruby\\r\\nHost: 169.254.169.254\\r\\n\\r\\n\"\nConn close because of error Timeout::Error\n[aws-sdk] 2014-09-19 13:01:30 UTC -- Retrying after #<Timeout::Error: Timeout::Error> (4 retries left)\nopening connection to 169.254.169.254...\nopened\n<- \"GET /latest/meta-data/iam/security-credentials/ HTTP/1.1\\r\\nAccept: */*\\r\\nUser-Agent: Ruby\\r\\nHost: 169.254.169.254\\r\\n\\r\\n\"\nConn close because of error Timeout::Error\n[aws-sdk] 2014-09-19 13:01:33 UTC -- Retrying after #<Timeout::Error: Timeout::Error> (3 retries left)\nopening connection to 169.254.169.254...\nopened\n<- \"GET /latest/meta-data/iam/security-credentials/ HTTP/1.1\\r\\nAccept: */*\\r\\nUser-Agent: Ruby\\r\\nHost: 169.254.169.254\\r\\n\\r\\n\"\nConn close because of error Timeout::Error\n[aws-sdk] 2014-09-19 13:01:38 UTC -- Retrying after #<Timeout::Error: Timeout::Error> (2 retries left)\nopening connection to 169.254.169.254...\nopened\n<- \"GET /latest/meta-data/iam/security-credentials/ HTTP/1.1\\r\\nAccept: */*\\r\\nUser-Agent: Ruby\\r\\nHost: 169.254.169.254\\r\\n\\r\\n\"\nConn close because of error Timeout::Error\n[aws-sdk] 2014-09-19 13:01:47 UTC -- Retrying after #<Timeout::Error: Timeout::Error> (1 retries left)\nopening connection to 169.254.169.254...\nopened\n<- \"GET /latest/meta-data/iam/security-credentials/ HTTP/1.1\\r\\nAccept: */*\\r\\nUser-Agent: Ruby\\r\\nHost: 169.254.169.254\\r\\n\\r\\n\"\n-> \"HTTP/1.0 200 OK\\r\\n\"\n. It's here: https://github.com/aws/aws-sdk-ruby/tree/code-generation/gems/aws-sigv4\nAppears that the code-generation branch has not yet been merged to master following the SDK V3 release.. After checking some branches it seems to be located in this branch: https://github.com/aws/aws-sdk-ruby/tree/code-generation. ",
    "jderrien": "Thanks for your help, trevorrowe.\nI've added the tests for the attribute, Hope all is ok.\n. Actually the class uses ListServerCertificates from the AWS API which only return the metadata not the certificate itself.\nGetServerCertificate returns the certificate but I'm not sure how I should implement this nor if I would be able to do this without spend a lot of time to dive into the underlying of the Ruby SDK.\n. @lsegal You're right. Thanks for your help!\nIn fact I haven't used the Client classes until now and I'm a bit confused. When should I use these classes? What are the purposes of these classes?\n. ",
    "luccasmaso": "Thanks for the answer @trevorrowe!!  =)\n. How much delay should have for an interval between each \"retry\" attempt?\nDo you think a fixed interval or a \"exponential_backoff\" would be better?\nThanks again\n. The 500 errors doesn't happens often, rarely, but it I need the script to be always alive and recovers it self.\nI got the errors from existing queues.\nThe retry Inside rescue should sleep some exponential time to begin again or did you mean that the queue = AWS::SQS::Queue.new(my_queue) will hold it itself?\nThanks\n. ",
    "loopj": "Looks like the same thread-safety issue as this:\nhttps://forums.aws.amazon.com/thread.jspa?threadID=96110\nI've seen this issue when using sidekiq to send emails in ruby 1.9. The first couple of emails fail while the aws environment is still loading.\n. AWS.eager_autoload! fixes this, but I wanted to make sure these two issues were linked.   \n\nJames Smith\nOn Tuesday, May 21, 2013 at 7:28 AM, Trevor Rowe wrote:\n\n@loopj (https://github.com/loopj) Are you calling AWS.eager_autoload! and still seeing failures while the environment loads?\n\u2014\nReply to this email directly or view it on GitHub (https://github.com/aws/aws-sdk-ruby/issues/186#issuecomment-18212218).\n. If I'm running the .deliver method on the mail object, it works fine, it only fails when I call .deliver!.\n\nVersion info:\n- Latest version of SDK (1.9.5)\n- Rails 3.2.13\n. Thanks Loren.  \nI use deliver! in some of my test suites, since it highlights some issues we've had in production that are missed by plain-old deliver.\nGlad you've been able to reproduce.\n- james\nOn Monday, May 13, 2013 at 5:06 PM, Loren Segal wrote:\n\nOkay, I can reproduce this now with deliver! and we have indeed found the culprit. Note however that you should be calling deliver, not deliver!, as per the docs:\n\n\"This method bypasses checking perform_deliveries and raise_delivery_errors, so use with caution\"\nhttps://github.com/mikel/mail/blob/master/lib/mail/message.rb#L237-L238\nThat said, we will fix this in the next release, but a simple workaround in the meantime would be to add an empty hash attribute \"settings\" to the SimpleEmailService class like so:\nclass AWS::SimpleEmailService; def settings; {} end end  \n\nOr you can simply stick to deliver.\n\u2014\nReply to this email directly or view it on GitHub (https://github.com/aws/aws-sdk-ruby/issues/246#issuecomment-17849159).\n. Thanks @lsegal that workaround fixed it for now, I'd definitely like to see this working in the next release too.\n\nI can't recall why we use deliver! in some tests, sorry.\n. Added responds_to? check as suggested, which also fixes the test failures.\nApologies for not spotting the test failures, there were so many other existing test failures that it wasn't clear.\n. 27 test failures on master, they look like ruby 1.8 failures.\nruby 1.8.7 (2012-02-08 patchlevel 358) [universal-darwin12.0]\nshell\nrspec ./spec/shared/record/abstract_base_examples.rb:77 # AWS::Record::Model it should behave like an aws record base class it should behave like record class #create! raises an exception when create returns false\nrspec ./spec/shared/record/abstract_base_examples.rb:97 # AWS::Record::Model it should behave like an aws record base class it should behave like record class #create new records delegates to #save of a newly created object\nrspec ./spec/shared/record/abstract_base_examples.rb:77 # AWS::Record::Model it should behave like an aws record base class it should behave like record class #create! raises an exception when create returns false\nrspec ./spec/shared/record/abstract_base_examples.rb:89 # AWS::Record::Model it should behave like an aws record base class it should behave like record class #create returns an invalid object if the attributes are invlalid\nrspec ./spec/shared/record/abstract_base_examples.rb:97 # AWS::Record::Model it should behave like an aws record base class it should behave like record class #create new records delegates to #save of a newly created object\nrspec ./spec/aws/s3/client_spec.rb:1203 # AWS::S3::Client#list_object_versions should add :delimiter as a \"delimiter\"\nrspec ./spec/aws/s3/client_spec.rb:1207 # AWS::S3::Client#list_object_versions should add :key_marker as a \"key-marker\"\nrspec ./spec/aws/s3/client_spec.rb:1211 # AWS::S3::Client#list_object_versions should add :max_keys as a \"max-keys\"\nrspec ./spec/aws/s3/client_spec.rb:1215 # AWS::S3::Client#list_object_versions should add :prefix as a \"prefix\"\nrspec ./spec/aws/s3/client_spec.rb:1219 # AWS::S3::Client#list_object_versions should add :version_id_marker as a \"version-id-marker\"\nrspec ./spec/aws/s3/client_spec.rb:1641 # AWS::S3::Client#list_objects should add :delimiter as a \"delimiter\"\nrspec ./spec/aws/s3/client_spec.rb:1645 # AWS::S3::Client#list_objects should add :key_marker as a \"key-marker\"\nrspec ./spec/aws/s3/client_spec.rb:1649 # AWS::S3::Client#list_objects should add :max_keys as a \"max-keys\"\nrspec ./spec/aws/s3/client_spec.rb:1653 # AWS::S3::Client#list_objects should add :prefix as a \"prefix\"\nrspec ./spec/aws/s3/client_spec.rb:1693 # AWS::S3::Client#list_multipart_uploads should add :delimiter as a \"delimiter\"\nrspec ./spec/aws/s3/client_spec.rb:1697 # AWS::S3::Client#list_multipart_uploads should add :key_marker as a \"key-marker\"\nrspec ./spec/aws/s3/client_spec.rb:1701 # AWS::S3::Client#list_multipart_uploads should add :max_keys as a \"max-keys\"\nrspec ./spec/aws/s3/client_spec.rb:1705 # AWS::S3::Client#list_multipart_uploads should add :upload_id_marker as a \"upload-id-marker\"\nrspec ./spec/aws/s3/client_spec.rb:1709 # AWS::S3::Client#list_multipart_uploads should add :max_uploads as a \"max-uploads\"\nrspec ./spec/aws/s3/client_spec.rb:1713 # AWS::S3::Client#list_multipart_uploads should add :prefix as a \"prefix\"\nrspec ./spec/aws/s3/client_spec.rb:1767 # AWS::S3::Client#upload_part sends :part_number as partNumber\nrspec ./spec/aws/s3/client_spec.rb:1727 # AWS::S3::Client#upload_part it should behave like accepts upload_id sends :upload_id as uploadId\nrspec ./spec/aws/s3/client_spec.rb:1727 # AWS::S3::Client#complete_multipart_upload it should behave like accepts upload_id sends :upload_id as uploadId\nrspec ./spec/aws/s3/client_spec.rb:1727 # AWS::S3::Client#abort_multipart_upload it should behave like accepts upload_id sends :upload_id as uploadId\nrspec ./spec/aws/s3/client_spec.rb:1948 # AWS::S3::Client#list_parts sends :max_parts as max-parts\nrspec ./spec/aws/s3/client_spec.rb:1952 # AWS::S3::Client#list_parts sends :part_number_marker as part-number-marker\nrspec ./spec/aws/s3/client_spec.rb:1727 # AWS::S3::Client#list_parts it should behave like accepts upload_id sends :upload_id as uploadId\n. Sure no problem\n. Let me know if you guys need any further changes to get this merged in.\n. Since I havent dug deep into the library or its tests, I have no idea what the appropriate test would be for this change. If you guys have suggestions on how I should write a test for this, I'm more than happy to oblige.\n. Hey guys, would love to have this merged, we've been using it on production for a few weeks now\n. Thanks @trevorrowe good catch on the responds_to check.\n. Doesn't look rspec related, same failures which the latest rspec:\nhttps://gist.github.com/loopj/5725906\n. Using multi_xml seems like a great approach, would you be open to a pull request for the multi_xml switchover?\n. It looks like the way your xml parsing is currently structured allows for a fairly modular switch-over. The current test suite should be pretty solid for checking for incompatibility, and we can add additional tests for any multi_xml specific stuff.\nWe can always have a branch/PR for the changes and choose to merge them later (eg, target v2)\n. I can't see any v2 related tags in the repo, is it private right now?\n. Thanks @trevorrowe, good to know. Looking forward to v2, definitely think the multi_xml route is the way to go.\n. Nice, thanks!\n\u2014\nSent from Mailbox for iPhone\nOn Thu, Oct 3, 2013 at 11:33 PM, Loren Segal notifications@github.com\nwrote:\n\n@loopj check it out: https://github.com/aws/aws-sdk-core-ruby\nReply to this email directly or view it on GitHub:\nhttps://github.com/aws/aws-sdk-ruby/issues/370#issuecomment-25660110\n. \n",
    "LeeGraber": "Thanks for the response, Trevor. Is this something that is coming soon or should I go ahead and try to hook it in from my on fork in the meantime. I am hitting this limitation and need to get around it\nThanks\n. Hey Trevor ... any chance to take a look. I got caught up in some deployment related issues so didn't get to look at this yet. Let me know. Thanks\n. Hi Trevor,\n   I have submitted pull-request 218 (https://github.com/aws/aws-sdk-ruby/pull/218). I am working through adding tests but perhaps you could take a look at the code. I have tested it manually in a variety of ways (on small files, varying the size threshold, and against files > 5GB) and it seems to be working just great. I tried to follow the coding conventions you had. Obviously I would like this in the main sdk so that I don't have to depend on my fork. Let me know what you think. Thanks\n. Trevor, I made changes as to our discussions, tested the calls, and adjusted my code to be responsible for the head call. I did re-upload a 5GB file (mine was deleted) so I have not added the internal rescue and retry. Please take a look and let me know if we are on the same page.\n. No problems experienced by me. It is working fine.\nThe only thing I haven't done, which you recommended, is if the user\ndoesn't specify :use_multipart_copy, and the file is >5GB, then AWS throws\nan exception. I am not catching that and switching to multipart\nautomagically. The caller would have to catch the exception. It is a nice\nfeature to have ... I just didn't implement it. :) Right now the code in my\napplication simply always checks the source length (we don't do copy that\noften for me to worry about the perf).\nLet me know what else you need before you are ready to run it through what\never you do before accepting the push.\nThanks\nLee\nOn Fri, Apr 12, 2013 at 1:25 PM, Trevor Rowe notifications@github.comwrote:\n\nThats looking pretty good!\nThe SDK already deals with failed requests and retries (by default up to 3\ntimes per request) before raising the error. Have you experienced errors\nthat propagated out during a multipart copy?\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/aws/aws-sdk-ruby/pull/218#issuecomment-16315655\n.\n. Okay ... I will take a look at your test code. :)\n\nOn Fri, Apr 12, 2013 at 2:04 PM, Trevor Rowe notifications@github.comwrote:\n\nIdeally we should create an integration test. As part of the test we could\nupload a ~10 MB file and then perform a copy using the multipart copy. Once\nwe have that test then I am happy to merge.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/aws/aws-sdk-ruby/pull/218#issuecomment-16317503\n.\n. Thanks Trevor. We look forward to switching back to the master branch :)\n\nOn Tue, May 14, 2013 at 10:12 AM, Trevor Rowe notifications@github.comwrote:\n\nI merged your pull request locally so I could add an integration test (\n93300dc https://github.com/aws/aws-sdk-ruby/commit/93300dc). Your\ncommits should now be on master. Thanks for the excellent work!\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/aws/aws-sdk-ruby/pull/218#issuecomment-17890543\n.\n. \n",
    "mvastola": "No probs. Hadn't seen that travis thing before -- pretty cool. The praise and reassurance was nice in that it provided automatic computer confirmation that really boosted my ego!\nTo think.. in changing one whole commented source, I was totally FLAWLESS in avoiding any/all compile errors and/or violations of your guys' acceptance specifications/tests! I'm pretty sure I essentially just won the internet!\nThanks again for merging. Gotta go celebrate now!!\n. ",
    "Vittly": "Also if i run\nec2.instances.create(\n:image_id => 'ami-1234',\n:key_name => 'abcd',\n:instance_type => 'm1.xlarge',\n:user_data =>\n<<-eos\n!/bin/bash\necho \"whoami in pwd\" >/tmp/loglog\nreboot\neos\n)\nI get no error about base64 and skipping user_data script as well (lack of \"#!\" at begin)\n. it is not a bug sorry. i have missed extra spaces before '#!' that i thought <<-eos would omit\n. Okey. What is \"region you are connecting to\"? I've wrote above:\n  lc = au.launch_configurations.create(launch_name, ec2.images['ami-ad0a30d9'], 't1.micro')\nit means that ec2.images had found the image by id, but launch_configurations was not\n. Okey. ec2.images['ami-ad0a30d9'].exists? returns true.\nAlso if i write\nimg = ec2.client.describe_images(:filters =>\n  [{:name => 'tag:Name', :values => 'worker_server'}]).data[:images_set][0]\nau.launch_configurations.create(launch_name, img[:image_id], 't1.micro')\nthis code fails. Problem is not about regions.\nOr if it is and I am wrong, please teach me how to specify for AWS::AutoScaling what region I want to use?\n. I am using EU-Ireland, so I add  :auto_scaling_endpoint => 'autoscaling.eu-west-1.amazonaws.com' and it works! Thank you\n. ",
    "chinshr": "So is this the correct way of doing it?\n``` ruby\nec2.instances.create(\n:image_id => 'ami-1234',\n:key_name => 'abcd',\n:instance_type => 'm1.xlarge',\n:user_data =>\n<<-eos\n!/bin/bash\necho \"whoami in pwd\" >/tmp/loglog\nreboot\neos\n)\n```\n. ",
    "jan": "Thanks for the quick and helpful response, @trevorrowe. \nFrom a user's perspective to the SDK, it is a a strange concept that after a create, the instance might not exist. Does the API return \"okay\" before even creating the instance description? This would mean, that every user of the SDK would have to code this timeout behavior after an instance create. And that really means, that it should be done on the SDK level in the create instance call, and meanwhile be mentioned in the documentation with blinking alert signs attached to it. Don't you think? \nI see that there might be a use case for cranking out instances and not using the returned instance object afterwards, but I would think that the majority of users does access the instance after creating it.\nOf course you cannot change the default behavior anymore. Maybe an option or optional second parameter block_until_exists? Or a helper method on instance itself with that name?\nruby\ninstance = region.instances.create({image_id: 'ami-myami'}).block_until_exists\ninstance.add_tag(\"Name\", value: \"trevorrowe\")\nAnyway, you'll figure it out and I know to solve it for my case. Thank you again.\n. ",
    "directionless": "I thought about that. I think it would only help me verify the integrity of a download, it wouldn't verify the upload. \nShort of downloading and comparing, I don't see any way to verify an uploaded file's integrity.\n. Ah, thank you for explaining how the :content_md5 option fits with the write.\nI'm currently using the sdk's write method and letting it handle splitting for me. It would be nice if when it split to multipart is handled the md5 checksumming. Otherwise I'll have to override a bunch of implicit behavior.\nFor the download, it seems pretty reasonable to cram an hash value into the header. But I've been focusing on the upload half.\n. ",
    "rdrey": ":+1: Would be great if the SDK could do :content_md5 checks for small files by default.\n. ",
    "messick": ":+1: \n. ",
    "zimbatm": "I had a look at the new SDK and it doesn't support setting the content-md5 on the object if it's uploaded via multipart. \nIt's a limitation of the API, not the client: http://docs.aws.amazon.com/AmazonS3/latest/API/mpUploadInitiate.html\nHaving the Content-MD5 set on the object is useful for clients who download the object and want to verify that everything was fine after the download.\n. ",
    "jkburges": "\nThe v2 SDK computes the Content-MD5 header by default on upload for calls to #put_object and #upload_part\n\nWhat about AWS::S3::Object#file_upload?  Presumably this is just a convenience over #put_object, but I got lost in the code trying to confirm this.\n. Thanks @awood45 - if it's any help, I arrived at this code because just from reading the doc, I was confused as to what the default retries is for default credential provider chain, so this was my attempt to clarify, but I didn't understand things properly as you point out.. Thanks @cjyclaire!\nDoes that info end up in the API doc somewhere? I couldn't find it.\nThere's a section Configuration Options which talks about setting config with a specific credential type, but as I understand it that would break the \"credential search chain\" or whatever the proper name is (i.e. the type of credential provider would become fixed to whatever you provide there).\nI ask because maybe I can submit a PR to clarify this bit of doc for use cases such as mine.\n. ",
    "SGospodinov": "Hello, can someone tell me what is expected behaviour if aws does not receive the expected checksum? I mean is it there some kind of an error which will be raised. If no how I can handle such behaviour?. ",
    "schneems": "Hello @trevorrowe @lsegal. I'm new to this crazy world of sending files straight to S3 (skipping the backend). I'm writing an article for how to do this for Heroku and so far it looks like the pre-signed post http://docs.aws.amazon.com/AWSRubySDK/latest/AWS/S3/PresignedPost.html is the way to go (either that or a JS library). I like that I can set limitations on the action more than with the JS method which seems more open to abuse. I found this issue while looking for potential limitations or problems with presigned-post. \n\nHow do we multipart upload from the client to the PresignedPost endpoint?\n\nSo what are the limitations of recommending PresignedPost?  We cannot send multiple files in one web form using this method? \n. @lsegal perfect, thanks :heart:  :heart:  :heart: ! Sorry for hijacking the thread :)\n. I understand. Please pass that info along. This lack of documentation cost me $50 in amazon developer support and a few hours of my life cc/ @lsegal \n. ",
    "iconara": "I'm sorry, I didn't mean that autoload in Ruby was a dangerous mechanism, just that the custom autoloading code in the AWS SDK gem probably was. There's more there than just Ruby's autoload and I assumed that that was what was caused the threading issues. If you don't think that is the case you are probably right.\nUpgrading to Ruby 2.0.0 is not really a possibility, just as with 1.9.0 and 1.9.1 at this point it doesn't feel like a stable release. But most of our other code runs 1.9.3 or JRuby in 1.9.3 compatibility mode, so if you think that there's nothing more in that code that's unsafe we should be ok.\nThanks for your help.\n. As it is, using S3 from JRuby is broken. We definitely see errors getting trapped by this code that definitely shouldn't. You really can't rescue all errors like this, it makes the library extremely brittle.\nYou say you want to avoid playing whack-a-mole listing all of the errors that are safely retry:able, but instead you've created a situation where you need to keep track of errors that are not retry:able, and that list is infinitely bigger.\n. ",
    "cwholt": "@trevorrowe -\ninitialize bucket\nirb(main):019:0> bucket = AWS::S3.new(:access_key_id => \"key\", :secret_access_key => \"secret\").buckets[\"bucket\"]\n=> #<AWS::S3::Bucket:bucket>\nexists?\nirb(main):020:0> bucket.objects[\"test.jsa\"].exists?\nAWS::S3::Errors::Forbidden: AWS::S3::Errors::Forbidden\n    from /<redacted>/bundle/ruby/1.9.1/gems/aws-sdk-1.8.4/lib/aws/core/client.rb:339:in `return_or_raise'\n    from /<redacted>/bundle/ruby/1.9.1/gems/aws-sdk-1.8.4/lib/aws/core/client.rb:440:in `client_request'\n    from (eval):3:in `head_object'\n    from /<redacted>/bundle/ruby/1.9.1/gems/aws-sdk-1.8.4/lib/aws/s3/s3_object.rb:294:in `head'\n    from /<redacted>/bundle/ruby/1.9.1/gems/aws-sdk-1.8.4/lib/aws/s3/s3_object.rb:271:in `exists?'\n    from (irb):20\n    from /<redacted>/bundle/ruby/1.9.1/gems/railties-3.2.13/lib/rails/commands/console.rb:47:in `start'\n    from /<redacted>/bundle/ruby/1.9.1/gems/railties-3.2.13/lib/rails/commands/console.rb:8:in `start'\n    from /<redacted>/bundle/ruby/1.9.1/gems/railties-3.2.13/lib/rails/commands.rb:41:in `<top (required)>'\n    from script/rails:6:in `require'\n    from script/rails:6:in `<main>'\nirb(main):021:0> bucket.objects[\"test.js\"].exists?\n=> true\nThis is on aws-sdk gem release 1.8.4, and it's likely worth noting I am using IAM, but cannot determine if there is a specific Action that I need to add to get this to work correctly. I have tried updating the IAM policy to allow for \"s3:*\" Actions, and even gone so far as using a \"superuser\" IAM policy that has access to all actions and resources, but the resulting behavior is still the same.\n. The IAM policy does seem to have permissions:\nirb(main):004:0> bucket\n=> #<AWS::S3::Bucket:bucket>\nirb(main):005:0> bucket.exists?\n=> true\nirb(main):006:0> bucket.objects[\"test.js\"].exists?\n=> true\nirb(main):007:0> bucket.objects[\"test.jsa\"].exists?\nAWS::S3::Errors::Forbidden: AWS::S3::Errors::Forbidden\nI have tested with the account credentials and everything works as expected (returns false when object does not exist).\nWhat I have determined after a lot more testing with IAM is that the API is returning 403 Forbidden on my request when the Resource is explicitly set to that of the bucket.\nThis works:\n{\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": \"s3:*\",\n      \"Resource\": \"arn:aws:s3:::*\"\n    }\n  ]\n}\nThis does not:\n{\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": \"s3:*\",\n      \"Resource\": [\n        \"arn:aws:s3:::bucket\",\n        \"arn:aws:s3:::bucket/*\"\n      ]\n    }\n  ]\n}\nSo it's likely this is not an issue with the Ruby AWS SDK, but a problem w/ IAM. It's also possible I just need another resource statement that I don't know about nor can find documented anywhere.\nFor now I will just monkeypatch this in my own codebase, but would like to know if you have any insight into the IAM issue.\nNote: in all cases, bucket is replaced with my actual bucket name.\n. That's fine, I think if the IAM issue got solved, this would also be fixed.\nAs I said, I'll monkeypatch until the cause of the 403 is determined.\n. see #201 for more details\n. ",
    "shipstar": "@cwholt I just came across this issue. As far as I can tell, specifying both resources in the IAM policy works now. The following works as expected.\n{\n\"Version\": \"2012-10-17\",\n\"Id\": \"MyID\",\n\"Statement\": [\n    {\n        \"Sid\": \"\",\n        \"Effect\": \"Allow\",\n        \"Principal\": {\n            \"AWS\": \"MyIAMUser\"\n        },\n        \"Action\": \"s3:*\",\n        \"Resource\": [\n            \"arn:aws:s3:::bucket/*\",\n            \"arn:aws:s3:::bucket\"\n        ]\n    }\n]\n}\nJust FYI!\n. ",
    "jpwynn": "the fyi from @shipstr worked very well for us as well - thanks!\n. ",
    "dmagliola": "Another \"vote\" for @shipstar's comment, for anyone reading this, that's definitely the problem.\n. ",
    "frankwallis": "works for me too, thanks @shipstar!\n. ",
    "vickeryj": "I had this problem when I specified an s3 path in my IAM policy (arn:aws:s3:::bucket/path/*). Changing the policy per @shipstar's suggestion to grant access to the entire bucket fixed this. What's a good way to bring this up with the IAM people?\n. ",
    "kgf": "Does anyone know what permissions are actually needed ? Would prefer to not just wildcard...\nWhat I currently have, but not enough yet.\n\"Effect\": \"Allow\",\n            \"Action\": [\n                \"s3:GetBucketLocation\",\n                \"s3:ListBucket\"\n            ],\n            \"Resource\": [\n                \"arn:aws:s3:::bucket-us-east-1\"\n            ]\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"s3:AbortMultipartUpload\",\n                \"s3:PutObject\"\n            ],\n            \"Resource\": [\n                \"arn:aws:s3:::bucket-us-east-1/*\"\n            ]\n        },\n. Found this link, testing now\nhttps://groups.google.com/forum/#!topic/fluentd/Vvok7FTVuq4\n. ",
    "coreystinson2": "tl;dr Adding s3:ListBucket instead of s3:* worked for me.\nSo this had been an ongoing issue with me for quite some time.\nI have a library responsible for uploading new objects to s3, you can see the code below. I generate bucket keys at the time of upload however first do a quick existence check to prevent any accidental overrides if we somehow generate the same hex\nruby\nkey = \"#{SecureRandom.uuid}#{extension}\"\nexisting = Aws::S3::Object.new(bucket, key)\nif existing.exists?\n  raise ExistsError, 'Please try uploading the file again.'\nend\nNow developing locally I use the following IAM permissions, I like keeping these down to the bare minimum for both my development and production policies.\njson\n\"Action\": [\n    \"s3:GetObject\",\n    \"s3:PutObject\"\n],\n\"Resource\": [\n    \"arn:aws:s3:::my-bucket/*\",\n    \"arn:aws:s3:::my-bucket\"\n]\nThe above policy works great, the upload library executes without problems and returns both true and false based on whether or not the file actually exists.\nNow we push this code to production on an instance registered to ECS and we start experiencing some weird behaviour. This exact same policy now starts throwing the following error\n/usr/local/bundle/gems/aws-sdk-resources-2.5.3/lib/aws-sdk-resources/resource.rb:134:in rescue in exists?'\nWhile the aforementioned fixes such as changing the policy actions to below, do in fact work - I'm super anal about giving my applications excessive permissions and think it's a great practise to ensure they only have what they need to have.\njson\n\"Action\": [\n    \"s3:*\"\n],\n\"Resource\": [\n    \"arn:aws:s3:::my-bucket/*\",\n    \"arn:aws:s3:::my-bucket\"\n]\nAnyway I spent ages going one by one through all the policies until I found the combination that worked. s3:ListBucket.\njson\n\"Action\": [\n    \"s3:GetObject\",\n    \"s3:ListBucket\",\n    \"s3:PutObject\"\n],\n\"Resource\": [\n    \"arn:aws:s3:::my-bucket/*\",\n    \"arn:aws:s3:::my-bucket\"\n]\nStill havn't figured out the difference between development and production in regards to exactly why production requires this new action. However I'm much happier using this as oppose to just straight up s3:*. ",
    "AlexMorreale": "Just ran into this myself. It's pretty crazy .exists? requires GetObject and not just ListBucket. +1. @cjyclaire thank you very much, i will try this and get back to you.. hmm so I ended up doing this \n```\ndef stubbed_asg\n  opts = [ # mocked calls ]\nAws.config[:autoscaling] = {\n    stub_responses: {\n      describe_auto_scaling_groups: {\n        auto_scaling_groups: opts\n      }\n    }\n  }\nend\n```\nand just calling this where ever i need the ASG stubbed. I suppose this will work.\n. It's not exactly what I wanted. \nI wanted to be able to make calls to non-existent ASGs and it return false for asg_resource.exists?\nI don't really have a means for passing a client on object creation so i can't choose when to pass the stubbed client.. ",
    "jacksonrayhamilton": "I ran into this too, but it was not even a real permission problem, my server\u2019s clock was off.  Using ntp fixed it for me: https://stackoverflow.com/a/49260424/1468130  Leaving this here in case it helps anyone else.. ",
    "pkoch": "Supposedly, just s3:ListObjects and s3:GetObject is sufficient. Source: https://aws.amazon.com/premiumsupport/knowledge-center/s3-troubleshoot-403/. ",
    "abargnesi": "gem install nokogiri (version 1.5.8) did the trick.  Thanks.\n. ",
    "Dru89": "Is this still a problem?  For some reason I still seem to be seeing it.  I had Nokogiri 1.5.9 installed and I even tried downgrading to 1.5.8 as seen below.\nHere's what my Gemfile looks like:\nsource 'https://rubygems.org'\ngem 'nokogiri', '= 1.5.8'\ngem 'aws-sdk', '~> 1.0'\ngem 'jekyll'\nAnd here's what my script looks like\ns3 = AWS::S3.new(\n    :access_key_id =>     ENV['AWS_ACCESS_KEY_BLOG'],\n    :secret_access_key => ENV['AWS_SECRET_KEY_BLOG']\n)   \nbucket = s3.buckets.create(BUCKET_NAME, :acl => :public_read)\nbucket.configure_website\nIt gives me this error whenever I run it:\nThe XML you provided was not well-formed or did not validate against our published schema\n/Users/ahays/.rvm/gems/ruby-2.0.0-p195/gems/aws-sdk-1.10.0/lib/aws/core/client.rb:360:in `return_or_raise'\n/Users/ahays/.rvm/gems/ruby-2.0.0-p195/gems/aws-sdk-1.10.0/lib/aws/core/client.rb:461:in `client_request'\n(eval):3:in `put_bucket_website'\n/Users/ahays/.rvm/gems/ruby-2.0.0-p195/gems/aws-sdk-1.10.0/lib/aws/s3/bucket.rb:320:in `website_configuration='\n/Users/ahays/.rvm/gems/ruby-2.0.0-p195/gems/aws-sdk-1.10.0/lib/aws/s3/bucket.rb:292:in `configure_website'\n/Users/ahays/Projects/blog/Rakefile:56:in `block in <top (required)>'\n/Users/ahays/.rvm/gems/ruby-2.0.0-p195@global/gems/rake-10.0.4/lib/rake/task.rb:246:in `call'\n/Users/ahays/.rvm/gems/ruby-2.0.0-p195@global/gems/rake-10.0.4/lib/rake/task.rb:246:in `block in execute'\n/Users/ahays/.rvm/gems/ruby-2.0.0-p195@global/gems/rake-10.0.4/lib/rake/task.rb:241:in `each'\n/Users/ahays/.rvm/gems/ruby-2.0.0-p195@global/gems/rake-10.0.4/lib/rake/task.rb:241:in `execute'\n/Users/ahays/.rvm/gems/ruby-2.0.0-p195@global/gems/rake-10.0.4/lib/rake/task.rb:184:in `block in invoke_with_call_chain'\n/Users/ahays/.rvm/gems/ruby-2.0.0-p195@global/gems/rake-10.0.4/lib/rake/task.rb:177:in `invoke_with_call_chain'\n/Users/ahays/.rvm/gems/ruby-2.0.0-p195@global/gems/rake-10.0.4/lib/rake/task.rb:170:in `invoke'\n/Users/ahays/.rvm/gems/ruby-2.0.0-p195@global/gems/rake-10.0.4/lib/rake/application.rb:143:in `invoke_task'\n/Users/ahays/.rvm/gems/ruby-2.0.0-p195@global/gems/rake-10.0.4/lib/rake/application.rb:101:in `block (2 levels) in top_level'\n/Users/ahays/.rvm/gems/ruby-2.0.0-p195@global/gems/rake-10.0.4/lib/rake/application.rb:101:in `each'\n/Users/ahays/.rvm/gems/ruby-2.0.0-p195@global/gems/rake-10.0.4/lib/rake/application.rb:101:in `block in top_level'\n/Users/ahays/.rvm/gems/ruby-2.0.0-p195@global/gems/rake-10.0.4/lib/rake/application.rb:110:in `run_with_threads'\n/Users/ahays/.rvm/gems/ruby-2.0.0-p195@global/gems/rake-10.0.4/lib/rake/application.rb:95:in `top_level'\n/Users/ahays/.rvm/gems/ruby-2.0.0-p195@global/gems/rake-10.0.4/lib/rake/application.rb:73:in `block in run'\n/Users/ahays/.rvm/gems/ruby-2.0.0-p195@global/gems/rake-10.0.4/lib/rake/application.rb:160:in `standard_exception_handling'\n/Users/ahays/.rvm/gems/ruby-2.0.0-p195@global/gems/rake-10.0.4/lib/rake/application.rb:70:in `run'\n/Users/ahays/.rvm/gems/ruby-2.0.0-p195/bin/ruby_noexec_wrapper:14:in `eval'\n/Users/ahays/.rvm/gems/ruby-2.0.0-p195/bin/ruby_noexec_wrapper:14:in `<main>'\n. Here's the XML that I'm seeing:\nxml\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<WebsiteConfiguration xmlns=\"http://s3.amazonaws.com/doc/2006-03-01/\">\n<IndexDocument>\n    <Suffix>index.html</Suffix>\n</IndexDocument>\n<ErrorDocument>\n    <Key>error.html</Key>\n</ErrorDocument>\n</WebsiteConfiguration>\nAs you can see it's not duplicated.  Any other possibilities that I can look into, though?\n. It was happening every time.  Switching to using\nruby\nbucket.website_configuration = WebsiteConfiguration.new({\n    index_document: { suffix: 'index.html' },\n    error_document: { key: 'error.html' }\n})\nseems to have solved my problems, though.  I'll do some more digging into the root cause tomorrow, if I have time.  It's not a huge blocker for me, though (and I don't know of anyone else that's affected), so this ticket can remain closed, I think, until I have some better evidence of what's happening.\n. ",
    "hynkle": "I was experiencing the same issue as @Dru89 above, and his snippet allows me to work around my problem.  I've discovered, however, that adding in a routing_rules key will reproduce the MalformedXML error:\nruby\nbucket.website_configuration = WebsiteConfiguration.new(\n  index_document: { suffix: 'index.html' },\n  error_document: { key: 'error.html' },\n  routing_rules: []\n)\nWhich is kind of funny, as that means that means if you use @Dru89's snippet it will succeed, but if you then follow that with bucket.website_configuration = bucket.website_configuration, it will fail, since routing_rules: [] will have been added into the WebsiteConfiguration's options as a default.\nUsing:\nruby 2.0.0p195\naws-sdk 1.11.3\nnokogiri 1.5.10\n. ",
    "aurels": "Hello,\nCan you please give me an example of what I'm supposed to pass to :no_device ?\n{\n   :device_name  => '/dev/svdb',\n   :virtual_name => 'ephemeral0',\n   :no_device    => ''\n}\nIf I pass a value like 'none', I get a \"AWS::EC2::Errors::InvalidRequest: The request received was invalid\".\nThanks\n. Yes that's what I had understood too but doing that raises an error \"you don't comply with the grammar\" or something.\n. ",
    "jshou": "This is what I got:\nopening connection to s3.amazonaws.com:443...\nopened\nstarting SSL for s3.amazonaws.com:443...\nSSL established\n<- \"PUT /bucket_name?versioning HTTP/1.1\\r\\nContent-Type: \\r\\nContent-Length: 175\\r\\nUser-Agent: aws-sdk-ruby/1.8.5 ruby/2.0.0 x86_64-darwin12.2.0\\r\\nDate: \nWed, 27 Mar 2013 00:55:54 GMT\\r\\nAuthorization: AWS ACCESS_KEY_ID:SECRET_KEY\\r\\nAccept-Encoding: gzip;q=1.0,deflate;q=0.6,identity;q=0.3\\r\\nAccept:\n */*\\r\\nHost: s3.amazonaws.com\\r\\n\\r\\n\"                                                                                                                                    \n-> \"HTTP/1.1 400 Bad Request\\r\\n\"\n-> \"x-amz-request-id: REQUEST_ID\\r\\n\"\n-> \"x-amz-id-2: REDACTED\\r\\n\"\n-> \"Content-Type: application/xml\\r\\n\"\n-> \"Transfer-Encoding: chunked\\r\\n\"\n-> \"Date: Wed, 27 Mar 2013 00:55:51 GMT\\r\\n\"\n-> \"Connection: close\\r\\n\"\n-> \"Server: AmazonS3\\r\\n\"\n-> \"\\r\\n\"\n-> \"133\\r\\n\"\nreading 307 bytes...\n-> \"<?xml version=\\\"1.0\\\" encoding=\\\"UTF-8\\\"?>\\n<Error><Code>MalformedXML</Code><Message>The XML you provided was not well-formed or did not validate against our published \nschema</Message><RequestId>REQUEST_ID</RequestId><HostId>HOST_ID</HostId></Error>\"                          \nread 307 bytes\nreading 2 bytes...\n-> \"\\r\\n\"\nread 2 bytes\n-> \"0\\r\\n\"\n-> \"\\r\\n\"\nConn close\nAWS::S3::Errors::MalformedXML: The XML you provided was not well-formed or did not validate against our published schema\nfrom /Users/jshou/.rbenv/versions/2.0.0-p0/lib/ruby/gems/2.0.0/gems/aws-sdk-1.8.5/lib/aws/core/client.rb:339:in `return_or_raise'\n. That was exactly the problem. After upgrading nokogiri to 1.5.9, the command worked fine. Thanks!\n. ",
    "kevinkarwaski": "FWIW, I was just banging my head against the same issue (MalformedXML) in attempting to restore s3 objects from glacier. Once I upgraded to nokogiri 1.5.9 everything worked... would it make sense to update the sdk gemspec to reflect this? Any reason to keep it pinned at >=1.4.4?\nhttps://github.com/aws/aws-sdk-ruby/blob/master/aws-sdk.gemspec#L13\nI'm happy to fork and submit a PR. Thanks!\n. Something like this might work:\nspec.add_dependency \"nokogiri\", \"~> 1.4.4\", \">= 1.5.8\"\n. ",
    "cheynewallace": "Thanks, I can repro this one pretty easily if need be.\n. Yes, it seems the :encryption_key is required to reproduce.  It wrote the file fine without it.\n. I also tried using a static 16 character string as the encryption_key for testing purposes.  Same problem\n. Great, so I wasn't doing anything wrong, it was a bug. Good to know!  I'll keep an eye out for the fix\n. ",
    "attili": "My pleasure. Thanks for merging it so quickly.\n. ",
    "davidpelaez": "@trevorrowe you mentioned that \"Calling #add does not seem to trigger a change in the dirty attribute tracking (because it bypasses the attribute setter).\" isn't this to be considered a bug? I don't have all the knowledge to understand why it may not, but I'm talking to support right now about this issue and not sure if it's ok to have that behaviour.\n. ",
    "ralph-tice": "This property should be a collection, not an individual property.  You can have more than one private IP address.\n. Thanks for the excellent turnaround, really appreciate it.\n. ",
    "sammarx": "Thanks!\n. ",
    "kyrylo": "I also experience the same problem with VCR cassettes.\n/opt/rubies/ruby-1.9.3-p392/lib/ruby/1.9.1/net/http.rb:647:in `continue_timeout='\naws-sdk (1.9.5) lib/aws/core/http/connection_pool.rb:127:in `session_for'\naws-sdk (1.9.5) lib/aws/core/http/net_http_handler.rb:52:in `handle'\naws-sdk (1.9.5) lib/aws/core/client.rb:238:in `block in make_sync_request'\naws-sdk (1.9.5) lib/aws/core/client.rb:267:in `retry_server_errors'\naws-sdk (1.9.5) lib/aws/core/client.rb:234:in `make_sync_request'\naws-sdk (1.9.5) lib/aws/core/client.rb:496:in `block (2 levels) in client_request'\naws-sdk (1.9.5) lib/aws/core/client.rb:376:in `log_client_request'\naws-sdk (1.9.5) lib/aws/core/client.rb:462:in `block in client_request'\naws-sdk (1.9.5) lib/aws/core/client.rb:358:in `return_or_raise'\naws-sdk (1.9.5) lib/aws/core/client.rb:461:in `client_request'\n(eval):3:in `put_object'\naws-sdk (1.9.5) lib/aws/s3/s3_object.rb:1669:in `write_with_put_object'\naws-sdk (1.9.5) lib/aws/s3/s3_object.rb:608:in `write'\nEDIT: However, I don't have this problem with v1.7.1. Sorry, I have no time to debug this right now, so I'll just stick with 1.7.1 for now.\n. For what it's worth, my application uses WebMock instead of Fakeweb.\n. Thank you, Trevor.\n. ",
    "pyu10055": "cool, thanks for the quick response.\ndo you mean because the response format, the new API is not compatible with existing classes?\nI usually use those higher-level classes (Table/Collections), not the client class directly. But I notice those higher-level classes rely on the client object instantiated in the configuration, so I hope there will be an option in configuration for specifying version.\nIf the high-level classes can stay the same would be great, maybe proxy the request/response parsing to other classes based on the configured version number.\n. ",
    "remon": "I am working on windows 7 and file size is less than 600kb my bucket region is in US standrad ,my firewall is off\n. sorry I closed it by mistake .these error was solved after changing the time zone on my lap.\n. @ElvinEfendi  I had a problem in my clock on my local machine so ..difference was days between actual time and time on my machine after adjusting it ...its working ...also check the time on your production machine \n. ",
    "ElvinEfendi": "@remon I am experiencing the same issue. According to what have you adjusted the time zone?\n. @remon thanks for quick reply. I did not change anything but it just started to work.\n. ",
    "parkr": "Thanks! I was trying to fall out of the standards and the railtie got in my way, and unfortunately I didn't know how to disable its automatic execution at the time! I ended up doing this:\nruby\ngem 'aws-sdk', require: false\n. ",
    "deckchairhq": "Confirmed here too..\nWhat is the current status re: thread safety? With Ruby2 and the latest version of this gem would eager_autoload be redundant?\nI am getting lots of request timeouts (around 500 failures per 15,000 jobs) whilst putting ~3mb files to S3.\nAs i'm uploading in a Sidekiq thread ensuring the library is threadsafe is my first task...\n. @trevorrowe Cheers, I've provided a bit more information about my issues in my comment here: https://github.com/aws/aws-sdk-ruby/issues/241#issuecomment-17160357 - everything is currently running smoothly but will be closely monitoring my failed jobs and will report back if anything odd pops up.\n. I've been receiving this error alot, it turned out for me it was threading (too many concurrent CPU heavy jobs in my worker queue).\nI do wonder if this may have had a hand in some of the errors I logged whilst only having one concurrent job queue (one thread per process) but they certainly weren't as common as you've mentioned. I'll be doing a big upload again very soon and will carefully monitor the failures.\nOne of the differences I can see between some of my code and yours is that i'm parsing in my own IO object (built on top of StringIO with some convenience methods to implement tempfiles if a path is requested). You could attempt to read the file into memory as a string if space allows and parse that in, which would remove any potential issues caused by the file from the equation, everything else in your case is vanilla, so if it's not your file it likely is the library.\nG\n. I've reverted to the following nested retries until we can get a fix. Retrying 4 times catches 100% of the failures (~20k uploads per day);\n``` ruby\n  # Dirty dirty dirty(S3Bug)..\n  begin\n    tempobject.rewind\n    obj = s3_bucket.objects[ uid ].write( tempobject )\n  rescue Exception => e\n    puts \"Upload Failed once..\"\n\n    begin\n      tempobject.rewind\n      obj = s3_bucket.objects[ uid ].write( tempobject )\n    rescue Exception => e\n      puts \"Upload Failed twice..\"\n\n      begin\n        tempobject.rewind\n        obj = s3_bucket.objects[ uid ].write( tempobject )\n      rescue Exception => e\n        puts \"Upload Failed three times..\"\n\n        tempobject.rewind\n        obj = s3_bucket.objects[ uid ].write( tempobject )\n      end\n    end\n  end\n\n```\n. ",
    "kyriacos": "Thanks Trevor.\nOn Sun, Apr 28, 2013 at 11:26 AM, Trevor Rowe notifications@github.com\nwrote:\n\nThank you for reporting the issue.  The documented :comment option should work as intended now, as well as passing along the :hosted_zone_config.\nReply to this email directly or view it on GitHub:\nhttps://github.com/aws/aws-sdk-ruby/issues/240#issuecomment-17139745\n. \n",
    "ampedandwired": "Actually it turns out that the failure is somewhat intermittent. After leaving my computer idling for a while the above script actually works a few times before starting to fail again. Once it starts failing it fails pretty consistently.\nNote that it always succeeds on ruby 1.9, so this is definitely something 2.0 related.\n. Hi Trevor, that's the entire script. This is on a Linux system if that's significant. I'll try and replicate it on another box when I get some time to see if it's a local oddity.\n. OK, so I can replicate this on an EC2 Ubuntu Server 12.04 instance set up as follows:\nsudo apt-get update\nsudo apt-get install git \nsudo apt-get install build-essential readline-dev libssl-dev libxml2-dev libxslt1-dev\ngit clone git://github.com/sstephenson/rbenv.git ~/.rbenv\ngit clone https://github.com/sstephenson/ruby-build.git ~/.rbenv/plugins/ruby-build\necho 'export PATH=\"$HOME/.rbenv/bin:$PATH\"' >> ~/.bashrc\necho 'eval \"$(rbenv init -)\"' >> ~/.bashrc\nbash -l\nrbenv install 2.0.0-p0\nrbenv local 2.0.0-p0\ngem install aws-sdk\nruby awstest.rb\nIt doesn't happen as frequently as it does on my local box, but it does happen. Most times it happens when I rerun after leaving the box idle for a few minutes.\nEdit: now I've managed to get it into a state where it's happening pretty much every time I run it.\n. The error still occurs even if you write a simple string instead of a file, so it doesn't seem to be related to the file being uploaded at all.\n. Here's a full trace from two consecutive tests, the first one succeeded the second did not:\n```\n$ ruby test.rb\nopening connection to xxxxx.s3.amazonaws.com:443...\nopened\nstarting SSL for xxxxx.s3.amazonaws.com:443...\nSSL established\n<- \"GET /?versioning HTTP/1.1\\r\\nContent-Type: \\r\\nUser-Agent: aws-sdk-ruby/1.10.0 ruby/2.0.0 x86_64-linux\\r\\nDate: Thu, 16 May 2013 03:38:02 GMT\\r\\nAuthorization: AWS XXXXXXXXX:xxxxxxxxxx=\\r\\nAccept-Encoding: gzip;q=1.0,deflate;q=0.6,identity;q=0.3\\r\\nAccept: /\\r\\nHost: xxxxx.s3.amazonaws.com\\r\\n\\r\\n\"\n-> \"HTTP/1.1 200 OK\\r\\n\"\n-> \"x-amz-id-2: xxxxxxxxxx\\r\\n\"\n-> \"x-amz-request-id: xxxxxxxxxx\\r\\n\"\n-> \"Date: Thu, 16 May 2013 03:38:06 GMT\\r\\n\"\n-> \"Transfer-Encoding: chunked\\r\\n\"\n-> \"Server: AmazonS3\\r\\n\"\n-> \"\\r\\n\"\n-> \"71\\r\\n\"\nreading 113 bytes...\n-> \"<?xml version=\\\"1.0\\\" encoding=\\\"UTF-8\\\"?>\\n\"\nread 113 bytes\nreading 2 bytes...\n-> \"\\r\\n\"\nread 2 bytes\n-> \"0\\r\\n\"\n-> \"\\r\\n\"\nConn keep-alive\nI, [2013-05-16T13:38:05.396659 #12764]  INFO -- : [AWS S3 200 2.933784 0 retries] get_bucket_versioning(:bucket_name=>\"xxxxx\")  \nBucket exists: true\nUploading file\nConn close because of keep_alive_timeout\nopening connection to xxxxx.s3.amazonaws.com:443...\nopened\nstarting SSL for xxxxx.s3.amazonaws.com:443...\nSSL established\n<- \"PUT /foo HTTP/1.1\\r\\nContent-Type: \\r\\nContent-Length: 19\\r\\nUser-Agent: aws-sdk-ruby/1.10.0 ruby/2.0.0 x86_64-linux\\r\\nDate: Thu, 16 May 2013 03:38:07 GMT\\r\\nAuthorization: AWS XXXXXXXXX:xxxxxxxxxx=\\r\\nAccept-Encoding: gzip;q=1.0,deflate;q=0.6,identity;q=0.3\\r\\nAccept: /\\r\\nHost: xxxxx.s3.amazonaws.com\\r\\n\\r\\n\"\n-> \"HTTP/1.1 200 OK\\r\\n\"\n-> \"x-amz-id-2: xxxxxxxxxx\\r\\n\"\n-> \"x-amz-request-id: xxxxxxxxxx\\r\\n\"\n-> \"Date: Thu, 16 May 2013 03:38:11 GMT\\r\\n\"\n-> \"ETag: \\\"8622b9718771d75e07734684d6efa1dd\\\"\\r\\n\"\n-> \"Content-Length: 0\\r\\n\"\n-> \"Server: AmazonS3\\r\\n\"\n-> \"\\r\\n\"\nreading 0 bytes...\n-> \"\"\nread 0 bytes\nConn keep-alive\nI, [2013-05-16T13:38:10.843515 #12764]  INFO -- : [AWS S3 200 3.394982 0 retries] put_object(:bucket_name=>\"xxxxx\",:content_length=>19,:data=>#,:key=>\"foo\")  \n$ ruby test.rb\nopening connection to xxxxx.s3.amazonaws.com:443...\nopened\nstarting SSL for xxxxx.s3.amazonaws.com:443...\nSSL established\n<- \"GET /?versioning HTTP/1.1\\r\\nContent-Type: \\r\\nUser-Agent: aws-sdk-ruby/1.10.0 ruby/2.0.0 x86_64-linux\\r\\nDate: Thu, 16 May 2013 03:38:12 GMT\\r\\nAuthorization: AWS XXXXXXXXX:xxxxxxxxxx=\\r\\nAccept-Encoding: gzip;q=1.0,deflate;q=0.6,identity;q=0.3\\r\\nAccept: /\\r\\nHost: xxxxx.s3.amazonaws.com\\r\\n\\r\\n\"\n-> \"HTTP/1.1 200 OK\\r\\n\"\n-> \"x-amz-id-2: xxxxxxxxxx\\r\\n\"\n-> \"x-amz-request-id: xxxxxxxxxx\\r\\n\"\n-> \"Date: Thu, 16 May 2013 03:38:16 GMT\\r\\n\"\n-> \"Transfer-Encoding: chunked\\r\\n\"\n-> \"Server: AmazonS3\\r\\n\"\n-> \"\\r\\n\"\n-> \"71\\r\\n\"\nreading 113 bytes...\n-> \"<?xml version=\\\"1.0\\\" encoding=\\\"UTF-8\\\"?>\\n\"\nread 113 bytes\nreading 2 bytes...\n-> \"\\r\\n\"\nread 2 bytes\n-> \"0\\r\\n\"\n-> \"\\r\\n\"\nConn keep-alive\nI, [2013-05-16T13:38:15.724986 #12863]  INFO -- : [AWS S3 200 2.919649 0 retries] get_bucket_versioning(:bucket_name=>\"xxxxx\")  \nBucket exists: true\nUploading file\n<- \"PUT /foo HTTP/1.1\\r\\nContent-Type: \\r\\nContent-Length: 19\\r\\nUser-Agent: aws-sdk-ruby/1.10.0 ruby/2.0.0 x86_64-linux\\r\\nDate: Thu, 16 May 2013 03:38:17 GMT\\r\\nAuthorization: AWS XXXXXXXXX:xxxxxxxxxx=\\r\\nAccept-Encoding: gzip;q=1.0,deflate;q=0.6,identity;q=0.3\\r\\nAccept: /\\r\\nHost: xxxxx.s3.amazonaws.com\\r\\n\\r\\n\"\nConn close because of error end of file reached, and retry\nopening connection to xxxxx.s3.amazonaws.com:443...\nopened\nstarting SSL for xxxxx.s3.amazonaws.com:443...\nSSL established\n<- \"PUT /foo HTTP/1.1\\r\\nContent-Type: \\r\\nContent-Length: 19\\r\\nUser-Agent: aws-sdk-ruby/1.10.0 ruby/2.0.0 x86_64-linux\\r\\nDate: Thu, 16 May 2013 03:38:17 GMT\\r\\nAuthorization: AWS XXXXXXXXX:xxxxxxxxxx=\\r\\nAccept-Encoding: gzip;q=1.0,deflate;q=0.6,identity;q=0.3\\r\\nAccept: /\\r\\nHost: xxxxx.s3.amazonaws.com\\r\\n\\r\\n\"\n-> \"HTTP/1.1 400 Bad Request\\r\\n\"\n-> \"x-amz-request-id: 9606878C35487466\\r\\n\"\n-> \"x-amz-id-2: xxxxxxxxxx\\r\\n\"\n-> \"Content-Type: application/xml\\r\\n\"\n-> \"Transfer-Encoding: chunked\\r\\n\"\n-> \"Date: Thu, 16 May 2013 03:38:40 GMT\\r\\n\"\n-> \"Connection: close\\r\\n\"\n-> \"Server: AmazonS3\\r\\n\"\n-> \"\\r\\n\"\n-> \"15c\\r\\n\"\nreading 348 bytes...\n-> \"<?xml version=\\\"1.0\\\" encoding=\\\"UTF-8\\\"?>\\nRequestTimeoutYour socket connection to the server was not read from or written to within the timeout period. Idle connections will be closed.9606878C35487466xxxxxxxxxx\"\nread 348 bytes\nreading 2 bytes...\n-> \"\\r\\n\"\nread 2 bytes\n-> \"0\\r\\n\"\n-> \"\\r\\n\"\nConn close\nI, [2013-05-16T13:38:40.555424 #12863]  INFO -- : [AWS S3 400 22.893523 0 retries] put_object(:bucket_name=>\"xxxxx\",:content_length=>19,:data=>#,:key=>\"foo\") AWS::S3::Errors::RequestTimeout Your socket connection to the server was not read from or written to within the timeout period. Idle connections will be closed.\n/home/charles/.rbenv/versions/2.0.0-p0/gemsets/awstest/gems/aws-sdk-1.10.0/lib/aws/core/client.rb:360:in return_or_raise': Your socket connection to the server was not read from or written to within the timeout period. Idle connections will be closed. (AWS::S3::Errors::RequestTimeout)\n  from /home/charles/.rbenv/versions/2.0.0-p0/gemsets/awstest/gems/aws-sdk-1.10.0/lib/aws/core/client.rb:461:inclient_request'\n  from (eval):3:in put_object'\n  from /home/charles/.rbenv/versions/2.0.0-p0/gemsets/awstest/gems/aws-sdk-1.10.0/lib/aws/s3/s3_object.rb:1704:inwrite_with_put_object'\n  from /home/charles/.rbenv/versions/2.0.0-p0/gemsets/awstest/gems/aws-sdk-1.10.0/lib/aws/s3/s3_object.rb:608:in write'\n  from test.rb:9:in'\n```\nThe code being run is as follows:\nruby\nrequire 'aws'\ns3 = AWS::S3.new(:logger => Logger.new($stdout), :http_wire_trace => true)\nemr = AWS::EMR.new\nbucket_name = 'xxxxx'\nputs \"Bucket exists: #{s3.buckets[bucket_name].exists?}\"\nemr.job_flows.each { |jf| puts jf.name }\nputs \"Uploading file\"\ns3.buckets[bucket_name].objects['foo'].write(\"I'm a little teapot\")\n. In terms of replicating the problem, did you try running it on an EC2 Ubuntu instance? I was able to replicate the problem there on a clean box (using the setup documented in my comment above), so hopefully you'll be able to duplicate my results that way.\n. Some further notes:\n- I can readily replicate the problem using the exact steps and test code posted by Trevor above. I am using AMI ami-04ea7a3e (Ubuntu 12.04 Server) on a t1.micro instance in the Sydney region.\n- The region of the S3 bucket seems to make a difference for me. I was only able to produce the error when using a bucket in the Sydney region. Other regions (standard, California, Singapore) worked fine.\n- Using Ruby 2.0.0-p195 gives the same result\n- Using a different AWS account and newly created bucket gives the same result\n. ",
    "schmielson": "Also experiencing intermittent failures. Currently just using the following workaround in place of calls to S3Object's write method:\n# path - The String path of the file to be uploaded.\n  # s3_obj - The S3Object handle to be written to.\n  def upload_to_s3(path, s3_obj)\n    retries = S3CONFIG[:upload_retries]\n    begin\n      s3_obj.write(File.open(path, 'rb', :encoding => 'BINARY'))\n    rescue => ex\n      retries -= 1\n      if retries > 0\n        puts \"ERROR during S3 upload: #{ex.inspect}. Retries: #{S3CONFIG[:upload_retries] - retries}\"\n        retry\n      else\n        # oh well, we tried...\n        raise\n      end\n    end\n  end\n. ",
    "psanford": "We also ran into this problem after upgrading to ruby-2.0.0-p0. We saw a significant increase in s3 upload reties immediately after the upgrade. It doesn't seem to be related to the data as our retries eventually succeed.\nThis test case will fail for me sometimes on ruby-2.0:\n``` ruby\nrequire 'aws'\ns3 = AWS::S3.new(\n  :access_key_id     => ENV['AWSID'],\n  :secret_access_key => ENV['AWSKEY'],\n)\nbucket = ENV['BUCKET']\npid = Process.pid\nsize = 1024 * 1024 * 25\n0.upto(100).each do |i|\n  name = \"#{pid}.#{i}\"\nputs \"generating #{name}\"\n  File.open('/dev/urandom', 'rb') do |urand|\n    File.open(\"/tmp/#{name}\", 'wb') do |out|\n      out.write(urand.read(size))\n    end\n  end\nputs \"uploading #{name}\"\n  File.open(\"/tmp/#{name}\", 'rb') do |f|\n    s3.buckets[bucket].objects[name].write(f,\n      :multipart_threshold => 1024102420,\n    )\n  end\nputs \"finished #{name}\"\nend\n```\n. ",
    "uberllama": "Hi guys. Not sure if this is related, or a separate issue: https://github.com/thoughtbot/paperclip/issues/751#issuecomment-18214277\nThose of us using Paperclip have been encountering timeouts caused the aws-sdk gem after version 1.6.2. This is a major issue, and one that's been around for some time. I don't know if there has been any coordination on fixing it yet.\n. Thanks for your hard work over such a long period of time on this! Very exciting.\n. Thanks Trevor, very helpful. So it looks like in this case (eventual consistency) I need to do the retries on my end. It may serve me to put a short sleep in my rescue\n. As far as suggestions go, I suppose it might be nice if there was an extra boolean param I could pass into head calls that include retries. The use case I'm presenting is increasingly common as folks use a front end direct upload process to get their files onto s3, then follow up with some additional back end processing. \nIn my case, the head operation has to happen immediately, but the actual copying of the file is deferred to a background process with its own retry handling. So that may be less of a common use case.\nRegardless, I know what's happening now. Cheers. :)\n. ",
    "endymonium": "I think I got the same issue, my test case is to upload a bunch (~40) of smaller files:\n- Heroku + Ruby 1.9.3: does work\n- Heroku + Ruby 2.0.0: does not work and gives a timeout, most of the time after the 2nd file\n- local + Ruby 2.0.0: does work\nI had the error also locally, but increasing the timeouts using\nAWS.config(:http_open_timeout => 120, :http_read_timeout => 120)\nhelped. The curious thing is, that on Heroku the error clearly comes before the timeout runs out.\nI'll now test what happens if I exchange aws-sdk with some other s3 upload gem (fog).\n. Ok, if I exchange aws-sdk with fog, it works for me also on Heroku + Ruby 2.0.0\n. ",
    "frogandcode": "We're seeing this bug as well, it's causing some of our backup scripts to fail on occasion. Like a few others have mentioned above, this is a script that has run without incident for a few years, but started failing in the past week. The failures coincide with our move to Ruby 2.0. (Also: Though we use Paperclip elsewhere, this script has nothing to do with Paperclip.)\nThe timeout errors are sporadic, and during a given backup, one or more files may be successfully copied while later a file fails to be copied due to the timeout error. They seem to happen when there are multiple files being transferred and a particular file in the succession of writes happens to take a bit longer. I've done some logging during these writes and though my sample size is a bit small, it appears that writes where the connection is open for 20 seconds are failing, even though our timeout thresholds are well above that. Some sample output from the script with instrumentation for debugging is included below. Notable is this section:\nWriting to S3: searches/production-searches-2013-06-05.txt.bz2\nElapsed: 21.825005663\nwhich is where the timeout error occurs. So far (admittedly, in a small sample of trials), this same script works fine as long as the elapsed time stays under 20 seconds. I'll do some more experimenting and post an update if we glean new info, but I'm wondering if there's some reason, in Ruby 2.0, the http_read_timeout (or is there another setting in play when doing writes?) is actually 20 seconds, even though it is configured at a value well above that.\nHere's the output:\nirb(main):001:0> Backup.new.backup_application_logs\nTotal start: 2013-06-06 11:58:10 -0400\nStart: 2013-06-06 11:58:10 -0400\nhttp_open_timeout: 120\nhttp_read_timeout: 120\nhttp_idle_timeout: 60\nhttp_wire_trace: true\nWriting to S3: ad_events/production-ad_events-2013-06-05.txt.bz2\nElapsed: 0.512415023\nStart: 2013-06-06 11:58:10 -0400\nhttp_open_timeout: 120\nhttp_read_timeout: 120\nhttp_idle_timeout: 60\nhttp_wire_trace: true\nWriting to S3: attachment_views/production-attachment_views-2013-06-05.txt.bz2\nElapsed: 2.644124356\nStart: 2013-06-06 11:58:13 -0400\nhttp_open_timeout: 120\nhttp_read_timeout: 120\nhttp_idle_timeout: 60\nhttp_wire_trace: true\nWriting to S3: profile_views/production-profile_views-2013-06-05.txt.bz2\nElapsed: 6.695234678\nStart: 2013-06-06 11:58:20 -0400\nhttp_open_timeout: 120\nhttp_read_timeout: 120\nhttp_idle_timeout: 60\nhttp_wire_trace: true\nWriting to S3: profile_clicks/production-profile_clicks-2013-06-05.txt.bz2\nElapsed: 0.631335565\nStart: 2013-06-06 11:58:20 -0400\nhttp_open_timeout: 120\nhttp_read_timeout: 120\nhttp_idle_timeout: 60\nhttp_wire_trace: true\nWriting to S3: project_views/production-project_views-2013-06-05.txt.bz2\nElapsed: 0.672840081\nStart: 2013-06-06 11:58:21 -0400\nhttp_open_timeout: 120\nhttp_read_timeout: 120\nhttp_idle_timeout: 60\nhttp_wire_trace: true\nWriting to S3: project_clicks/production-project_clicks-2013-06-05.txt.bz2\nElapsed: 0.167696896\nStart: 2013-06-06 11:58:21 -0400\nhttp_open_timeout: 120\nhttp_read_timeout: 120\nhttp_idle_timeout: 60\nhttp_wire_trace: true\nWriting to S3: searches/production-searches-2013-06-05.txt.bz2\nElapsed: 21.825005663\nTotal elapsed: 33.149053171\nAWS::S3::Errors::RequestTimeout: Your socket connection to the server was not read from or written to within the timeout period. Idle connections will be closed.\n    from /var/www/vhosts/dribbble.com/rails/shared/bundle/ruby/2.0.0/gems/aws-sdk-1.11.0/lib/aws/core/client.rb:360:in `return_or_raise'\n    from /var/www/vhosts/dribbble.com/rails/shared/bundle/ruby/2.0.0/gems/aws-sdk-1.11.0/lib/aws/core/client.rb:461:in `client_request'\n    from (eval):3:in `put_object'\n    from /var/www/vhosts/dribbble.com/rails/shared/bundle/ruby/2.0.0/gems/aws-sdk-1.11.0/lib/aws/s3/s3_object.rb:1704:in `write_with_put_object'\n    from /var/www/vhosts/dribbble.com/rails/shared/bundle/ruby/2.0.0/gems/aws-sdk-1.11.0/lib/aws/s3/s3_object.rb:608:in `write'\n. ",
    "gotwalt": "Experiencing this on Ruby-2.0.0p195. Same symptoms - intermittent massive write timeout.\n. ",
    "phstc": "I'm also experiencing this issue on ruby 2.0.0p195. It was working fine on 1.9.3p194.\nsh\nruby 2.0.0p195 (2013-05-14 revision 40734) [i686-linux]\n. Using dynamo_db_endpoint and dynamo_db_port:\nshell\n OpenSSL::SSL::SSLError:\n       SSL_connect returned=1 errno=0 state=SSLv2/v3 read server hello A: unknown protocol\nuse_ssl: false \"fixes\" the SSL problem above, but then I get: \"DynamoDB Local does not support v1 API.\".  Any suggestions?\n. Awesome @trevorrowe! \nI changed my code to use aws-sdk-core following your well detailed instructions and now DynamoDB Local is working in development - way faster than using the real one :horse_racing:  :horse_racing: .\n:beers: \n. > Is there any helper in the SDK that parses it?\nSomething similar to https://github.com/awslabs/logstash-input-dynamodb/blob/master/lib/logstash/inputs/DynamoDBLogParser.rb#L121-L161\n. Thanks @trevorrowe \n. Hey @cjyclaire \n\nI'm going to work on this feature request\n\n\ud83c\udf7b \nI have a stream connected to a lambda that enqueues DynamoDB records to an SQS queue. Then I process these messages using a Ruby worker, that needs this event parser.\nBut in the Logstash snippet I used as an example above, they are receiving the JSON from the DynamoDB stream directly.\n. > Return a vanilla Ruby hash as a result of a JSON parse, converting only the attribute values.\nI kind of prefer the 1st option, so I can freely use the hash in the same way I use hashes returned from the DynamoDB queries, scans etc.\n. @trevorrowe sure, I was thinking about response.items that returns an array with Hashes. But I understand your point.\nFor my specific use case, what I honestly need only is something that converts NewImage into a Ruby vanilla hash. But I have the entire event payload as well, so something that converts it entirely or NewImage is good enough for me. Regarding to 1st or 2nd, both work for me. The second option feels like more the aws-sdk-ruby way. \n. @trevorrowe I understand that, but I was wondering if the sdk could abstract that and raise up a more explicit message, so the clients don't need to rescue, in order to raise a more descriptive error message.. ",
    "tristandunn": "@frogandcode suggested trying to establish a new connection per upload, so I switched our backup to use the method below. We haven't noticed any issues so far and it appears some writes may even be faster than before. I wouldn't consider it the ultimate solution, but it might helpful if you're writing important backups.\nruby\ndef write(name, file)\n  @bucket.objects[name].write(file)\nensure\n  # Ensure the HTTP pool is emptied after each write.\n  AWS.config.http_handler.pool.empty!\nend\n. ",
    "esilverberg": "+1\nToday I just deployed a ruby 2.0.0p195 upgrade and immediately started encountering this issue with aws sdk 1.11.3. It is an intermittent error, but I do so many S3 writes that I see it every few minutes\n. Guys --\nI have been plagued by this issue for several months now. I though it was just me, relieved to hear I am not crazy and this is a wider issue (just found this page by Googling EHOSTUNREACH)\nYou can see my AWS case number here: 1551135111\nI appear to have a similar setup -- Ubuntu 14.04 LTS using aws-sdk. Identical stack:\n/usr/local/lib/ruby/2.2.0/net/http.rb:879 in initialize\n/usr/local/lib/ruby/2.2.0/net/http.rb:879 in open\n/usr/local/lib/ruby/2.2.0/net/http.rb:879 in block in connect\n/usr/local/lib/ruby/2.2.0/timeout.rb:88 in block in timeout\n/usr/local/lib/ruby/2.2.0/timeout.rb:98 in call\n/usr/local/lib/ruby/2.2.0/timeout.rb:98 in timeout\n/usr/local/lib/ruby/2.2.0/net/http.rb:878 in connect\n/usr/local/lib/ruby/2.2.0/net/http.rb:863 in do_start\n/usr/local/lib/ruby/2.2.0/net/http.rb:858 in start\n/gems/aws-sdk-v1-1.59.0/lib/aws/core/http/connection_pool.rb:327 in start_session\n/gems/aws-sdk-v1-1.59.0/lib/aws/core/http/connection_pool.rb:127 in session_for\n/gems/aws-sdk-v1-1.59.0/lib/aws/core/http/net_http_handler.rb:55 in handle\n/gems/aws-sdk-v1-1.59.0/lib/aws/core/client.rb:253 in block in make_sync_request\n/gems/aws-sdk-v1-1.59.0/lib/aws/core/client.rb:282 in retry_server_errors\n/gems/aws-sdk-v1-1.59.0/lib/aws/core/client.rb:249 in make_sync_request\n/gems/aws-sdk-v1-1.59.0/lib/aws/core/client.rb:511 in block (2 levels) in client_request\n/gems/aws-sdk-v1-1.59.0/lib/aws/core/client.rb:391 in log_client_request\n/gems/aws-sdk-v1-1.59.0/lib/aws/core/client.rb:477 in block in client_request\n/gems/aws-sdk-v1-1.59.0/lib/aws/core/client.rb:373 in return_or_raise\n/gems/aws-sdk-v1-1.59.0/lib/aws/core/client.rb:476 in client_request\nI can repro this issue if you give me about 1 hour, on any instance. It's clearly transient and appears to go away on subsequent calls.\nIn my case, the exception is: \nErrno::EHOSTUNREACH: No route to host - connect(2) for \"dynamodb.us-east-1.amazonaws.com\" port 443\n(not port 80)\nThere are no special chars as I am using the aws-sdk. Would love to get to the bottom of this.\n. ",
    "dytsai": "I have exactly the same issue here. I tried all different methods (extend timeout, empty http pool etc.) and seems that using File or Pathname instead of IO stream may help a bit and the error rate decreases. However, no matter what I did, the intermittent errors never disappear. I am using Heroku, Ruby 2.0, and latest Sinatra, Unicorn and AWS SDK.\n. ",
    "kmcbride": "@trevorrowe I was experiencing this issue as well, but switching to master looks like it may have fixed it.\n. ",
    "mgogov": "I was having similar timeout issues running a large paperclip image preprocessing job over 2TB of photos with a combo of aws-sdk 1.11.3, paperclip 3.4.2, ruby 2.0.0p195, storing on AWS S3, running the job from an AWS EC2 instance.\nI just updated to aws-sdk 1.12.0 and seems the job is running well so thank you for these fixes!\n. ",
    "jonhyman": "I just updated to aws-sdk 1.14.1 and paperclip 3.5.0 to test out if it was fixed. Attachments that don't have pre-processing are uploading fine, but attachments that do have pre-processing are timing out with\n\nYour socket connection to the server was not read from or written to within the timeout period. Idle connections will be closed\n\nIt could be that since the preprocessing is running advpng and optipng for compression that it's taking too long. I'll also post this in https://github.com/thoughtbot/paperclip/issues/751 but for me something is still broken, going to downgrade.\n. Yeah, they eventually succeed, though a few get killed by our process timeout checks and restart automatically and complete. We don't do anything other than this in one part of our code\nruby\n@s3 = AWS::S3.new(:access_key_id => @aws_access_key, :secret_access_key => @aws_access_secret)\nbucket = @s3.buckets[@aws_bucket_name]\ns3_object = @s3_bucket.objects[key_name]\ns3_object.write(File.open(file_path, 'r'))\nand another area where we see timeouts is uploading from the Paperclip gem. Is there a way to set the timeout myself?\n. I believe it's the network time, I'm getting that from a New Relic transaction trace. For example, here's one that just happened 2 minutes ago in some code which performs png optimization and uploads to S3. Based on the extremely short image optimization time (optipng spent ~2.3 seconds and advpng spent 169ms) this image is likely very small. The upload to S3 took 782 seconds to succeed, though. \n\n. But as I said, not all uploads take a long time, it's just about 1% of the time.\n. Upon further inspection, the image from the 782 second upload looks looks like it was 21kb.\n. Hm. I doubt it's our reads. We're on SATADOM flash storage.\nIn the example there with the image processing, the upload is handled by the Paperclip gem. We're on the latest version but looking at source it seems like it is conceptually similar.\n. Though I will make that change, thanks for the heads up.\n. @trevorrowe do you have any other suggestions for how I may be able to troubleshoot and figure out what is going on?\n. I've been attempting to repro and then do a thread trace. It looks like it hangs in the request in connection pool. I'll keep digging, perhaps a timeout isn't getting set properly somewhere.\n``\nJun 29 11:35:28 :worker production.log:  [ 3:35:28.476373370] (8344) [WARN] Thread TID-osussmirk \nJun 29 11:35:28 :worker production.log:  [ 3:35:28.476620388] (8344) [WARN] /opt/rbenv/versions/2.2.2/lib/ruby/2.2.0/openssl/buffering.rb:326:insyswrite'\nJun 29 11:35:28 :worker production.log:  /opt/rbenv/versions/2.2.2/lib/ruby/2.2.0/openssl/buffering.rb:326:in do_write'\nJun 29 11:35:28 :worker production.log:  /opt/rbenv/versions/2.2.2/lib/ruby/2.2.0/openssl/buffering.rb:344:inwrite'\nJun 29 11:35:28 :worker production.log:  /opt/rbenv/versions/2.2.2/lib/ruby/2.2.0/net/http/generic_request.rb:205:in copy_stream'\nJun 29 11:35:28 :worker production.log:  /opt/rbenv/versions/2.2.2/lib/ruby/2.2.0/net/http/generic_request.rb:205:insend_request_with_body_stream'\nJun 29 11:35:28 :worker production.log:  /opt/rbenv/versions/2.2.2/lib/ruby/2.2.0/net/http/generic_request.rb:122:in exec'\nJun 29 11:35:28 :worker production.log:  /var/www/platform/shared/dashboard/bundle/ruby/2.2.0/gems/aws-sdk-v1-1.64.0/lib/aws/core/http/patch.rb:27:inblock in new_transport_request'\nJun 29 11:35:28 :worker production.log:  /var/www/platform/shared/dashboard/bundle/ruby/2.2.0/gems/aws-sdk-v1-1.64.0/lib/aws/core/http/patch.rb:26:in catch'\nJun 29 11:35:28 :worker production.log:  /var/www/platform/shared/dashboard/bundle/ruby/2.2.0/gems/aws-sdk-v1-1.64.0/lib/aws/core/http/patch.rb:26:innew_transport_request'\nJun 29 11:35:28 :worker production.log:  /opt/rbenv/versions/2.2.2/lib/ruby/2.2.0/net/http.rb:1384:in `request'\nJun 29 11:39:24 :worker production.log:  [ 3:39:24.716904843] (8344) [WARN] /opt/rbenv/versions/2.2.2/lib/ruby/2.2.0/openssl/buffering.rb:326:in syswrite'\nJun 29 11:39:24 :worker production.log:  /opt/rbenv/versions/2.2.2/lib/ruby/2.2.0/openssl/buffering.rb:326:indo_write'\nJun 29 11:39:24 :worker production.log:  /opt/rbenv/versions/2.2.2/lib/ruby/2.2.0/openssl/buffering.rb:344:in write'\nJun 29 11:39:24 :worker production.log:  /opt/rbenv/versions/2.2.2/lib/ruby/2.2.0/net/http/generic_request.rb:205:incopy_stream'\nJun 29 11:39:24 :worker production.log:  /opt/rbenv/versions/2.2.2/lib/ruby/2.2.0/net/http/generic_request.rb:205:in send_request_with_body_stream'\nJun 29 11:39:24 :worker production.log:  /opt/rbenv/versions/2.2.2/lib/ruby/2.2.0/net/http/generic_request.rb:122:inexec'\nJun 29 11:39:24 :worker production.log:  /var/www/platform/shared/dashboard/bundle/ruby/2.2.0/gems/aws-sdk-v1-1.64.0/lib/aws/core/http/patch.rb:27:in block in new_transport_request'\nJun 29 11:39:24 :worker production.log:  /var/www/platform/shared/dashboard/bundle/ruby/2.2.0/gems/aws-sdk-v1-1.64.0/lib/aws/core/http/patch.rb:26:incatch'\nJun 29 11:39:24 :worker production.log:  /var/www/platform/shared/dashboard/bundle/ruby/2.2.0/gems/aws-sdk-v1-1.64.0/lib/aws/core/http/patch.rb:26:in new_transport_request'\nJun 29 11:39:24 :worker production.log:  /opt/rbenv/versions/2.2.2/lib/ruby/2.2.0/net/http.rb:1384:inrequest'\nJun 29 11:40:22 :worker production.log:  [ 3:40:22.261013828] (8344) [WARN] /opt/rbenv/versions/2.2.2/lib/ruby/2.2.0/openssl/buffering.rb:326:in syswrite'\nJun 29 11:40:22 :worker production.log:  /opt/rbenv/versions/2.2.2/lib/ruby/2.2.0/openssl/buffering.rb:326:indo_write'\nJun 29 11:40:22 :worker production.log:  /opt/rbenv/versions/2.2.2/lib/ruby/2.2.0/openssl/buffering.rb:344:in write'\nJun 29 11:40:22 :worker production.log:  /opt/rbenv/versions/2.2.2/lib/ruby/2.2.0/net/http/generic_request.rb:205:incopy_stream'\nJun 29 11:40:22 :worker production.log:  /opt/rbenv/versions/2.2.2/lib/ruby/2.2.0/net/http/generic_request.rb:205:in send_request_with_body_stream'\nJun 29 11:40:22 :worker production.log:  /opt/rbenv/versions/2.2.2/lib/ruby/2.2.0/net/http/generic_request.rb:122:inexec'\nJun 29 11:40:22 :worker production.log:  /var/www/platform/shared/dashboard/bundle/ruby/2.2.0/gems/aws-sdk-v1-1.64.0/lib/aws/core/http/patch.rb:27:in block in new_transport_request'\nJun 29 11:40:22 :worker production.log:  /var/www/platform/shared/dashboard/bundle/ruby/2.2.0/gems/aws-sdk-v1-1.64.0/lib/aws/core/http/patch.rb:26:incatch'\nJun 29 11:40:22 :worker production.log:  /var/www/platform/shared/dashboard/bundle/ruby/2.2.0/gems/aws-sdk-v1-1.64.0/lib/aws/core/http/patch.rb:26:in new_transport_request'\nJun 29 11:40:22 :worker production.log:  /opt/rbenv/versions/2.2.2/lib/ruby/2.2.0/net/http.rb:1384:inrequest'\nJun 29 11:41:58 :worker production.log:  [ 3:41:58.549290483] (8344) [WARN] /opt/rbenv/versions/2.2.2/lib/ruby/2.2.0/openssl/buffering.rb:326:in syswrite'\nJun 29 11:41:58 :worker production.log:  /opt/rbenv/versions/2.2.2/lib/ruby/2.2.0/openssl/buffering.rb:326:indo_write'\nJun 29 11:41:58 :worker production.log:  /opt/rbenv/versions/2.2.2/lib/ruby/2.2.0/openssl/buffering.rb:344:in write'\nJun 29 11:41:58 :worker production.log:  /opt/rbenv/versions/2.2.2/lib/ruby/2.2.0/net/http/generic_request.rb:205:incopy_stream'\nJun 29 11:41:58 :worker production.log:  /opt/rbenv/versions/2.2.2/lib/ruby/2.2.0/net/http/generic_request.rb:205:in send_request_with_body_stream'\nJun 29 11:41:58 :worker production.log:  /opt/rbenv/versions/2.2.2/lib/ruby/2.2.0/net/http/generic_request.rb:122:inexec'\nJun 29 11:41:58 :worker production.log:  /var/www/platform/shared/dashboard/bundle/ruby/2.2.0/gems/aws-sdk-v1-1.64.0/lib/aws/core/http/patch.rb:27:in block in new_transport_request'\nJun 29 11:41:58 :worker production.log:  /var/www/platform/shared/dashboard/bundle/ruby/2.2.0/gems/aws-sdk-v1-1.64.0/lib/aws/core/http/patch.rb:26:incatch'\nJun 29 11:41:58 :worker production.log:  /var/www/platform/shared/dashboard/bundle/ruby/2.2.0/gems/aws-sdk-v1-1.64.0/lib/aws/core/http/patch.rb:26:in new_transport_request'\nJun 29 11:41:58 :worker production.log:  /opt/rbenv/versions/2.2.2/lib/ruby/2.2.0/net/http.rb:1384:inrequest'\nJun 29 11:48:35 :worker production.log:  /opt/rbenv/versions/2.2.2/lib/ruby/2.2.0/openssl/buffering.rb:326:in do_write'\nJun 29 11:48:35 :worker production.log:  /opt/rbenv/versions/2.2.2/lib/ruby/2.2.0/openssl/buffering.rb:344:inwrite'\nJun 29 11:48:35 :worker production.log:  /opt/rbenv/versions/2.2.2/lib/ruby/2.2.0/net/http/generic_request.rb:205:in copy_stream'\nJun 29 11:48:35 :worker production.log:  /opt/rbenv/versions/2.2.2/lib/ruby/2.2.0/net/http/generic_request.rb:205:insend_request_with_body_stream'\nJun 29 11:48:35 :worker production.log:  /opt/rbenv/versions/2.2.2/lib/ruby/2.2.0/net/http/generic_request.rb:122:in exec'\nJun 29 11:48:35 :worker production.log:  /var/www/platform/shared/dashboard/bundle/ruby/2.2.0/gems/aws-sdk-v1-1.64.0/lib/aws/core/http/patch.rb:27:inblock in new_transport_request'\nJun 29 11:48:35 :worker production.log:  /var/www/platform/shared/dashboard/bundle/ruby/2.2.0/gems/aws-sdk-v1-1.64.0/lib/aws/core/http/patch.rb:26:in catch'\nJun 29 11:48:35 :worker production.log:  /var/www/platform/shared/dashboard/bundle/ruby/2.2.0/gems/aws-sdk-v1-1.64.0/lib/aws/core/http/patch.rb:26:innew_transport_request'\nJun 29 11:48:35 :worker production.log:  /opt/rbenv/versions/2.2.2/lib/ruby/2.2.0/net/http.rb:1384:in request'\nJun 29 11:48:35 :worker production.log:  /var/www/platform/shared/dashboard/bundle/ruby/2.2.0/gems/rest-client-1.6.7/lib/restclient/net_http_ext.rb:51:inrequest'\nJun 29 11:48:35 :worker production.log:  /var/www/platform/shared/dashboard/bundle/ruby/2.2.0/gems/newrelic_rpm-3.12.0.288/lib/new_relic/agent/instrumentation/net.rb:27:in block (2 levels) in request_with_newrelic_trace'\nJun 29 11:48:35 :worker production.log:  /var/www/platform/shared/dashboard/bundle/ruby/2.2.0/gems/newrelic_rpm-3.12.0.288/lib/new_relic/agent.rb:426:indisable_all_tracing'\nJun 29 11:48:35 :worker production.log:  /var/www/platform/shared/dashboard/bundle/ruby/2.2.0/gems/newrelic_rpm-3.12.0.288/lib/new_relic/agent/instrumentation/net.rb:26:in block in request_with_newrelic_trace'\nJun 29 11:48:35 :worker production.log:  /var/www/platform/shared/dashboard/bundle/ruby/2.2.0/gems/newrelic_rpm-3.12.0.288/lib/new_relic/agent/cross_app_tracing.rb:48:intl_trace_http_request'\nJun 29 11:48:35 :worker production.log:  /var/www/platform/shared/dashboard/bundle/ruby/2.2.0/gems/newrelic_rpm-3.12.0.288/lib/new_relic/agent/instrumentation/net.rb:23:in request_with_newrelic_trace'\nJun 29 11:48:35 :worker production.log:  /var/www/platform/shared/dashboard/bundle/ruby/2.2.0/gems/aws-sdk-v1-1.64.0/lib/aws/core/http/connection_pool.rb:356:inrequest'\nJun 29 11:48:35 :worker production.log:  /var/www/platform/shared/dashboard/bundle/ruby/2.2.0/gems/aws-sdk-v1-1.64.0/lib/aws/core/http/net_http_handler.rb:64:in block in handle'\nJun 29 11:48:35 :worker production.log:  /var/www/platform/shared/dashboard/bundle/ruby/2.2.0/gems/aws-sdk-v1-1.64.0/lib/aws/core/http/connection_pool.rb:131:insession_for'\nJun 29 11:48:35 :worker production.log:  /var/www/platform/shared/dashboard/bundle/ruby/2.2.0/gems/aws-sdk-v1-1.64.0/lib/aws/core/http/net_http_handler.rb:56:in handle'\nJun 29 11:48:35 :worker production.log:  /var/www/platform/shared/dashboard/bundle/ruby/2.2.0/gems/aws-sdk-v1-1.64.0/lib/aws/core/client.rb:253:inblock in make_sync_request'\nJun 29 11:48:35 :worker production.log:  /var/www/platform/shared/dashboard/bundle/ruby/2.2.0/gems/aws-sdk-v1-1.64.0/lib/aws/core/client.rb:282:in retry_server_errors'\nJun 29 11:48:35 :worker production.log:  /var/www/platform/shared/dashboard/bundle/ruby/2.2.0/gems/aws-sdk-v1-1.64.0/lib/aws/s3/region_detection.rb:11:inretry_server_errors'\nJun 29 11:48:35 :worker production.log:  /var/www/platform/shared/dashboard/bundle/ruby/2.2.0/gems/aws-sdk-v1-1.64.0/lib/aws/core/client.rb:249:in make_sync_request'\nJun 29 11:48:35 :worker production.log:  /var/www/platform/shared/dashboard/bundle/ruby/2.2.0/gems/aws-sdk-v1-1.64.0/lib/aws/core/client.rb:511:inblock (2 levels) in client_request'\nJun 29 11:48:35 :worker production.log:  /var/www/platform/shared/dashboard/bundle/ruby/2.2.0/gems/aws-sdk-v1-1.64.0/lib/aws/core/client.rb:391:in log_client_request'\nJun 29 11:48:35 :worker production.log:  /var/www/platform/shared/dashboard/bundle/ruby/2.2.0/gems/aws-sdk-v1-1.64.0/lib/aws/core/client.rb:477:inblock in client_request'\nJun 29 11:48:35 :worker production.log:  /var/www/platform/shared/dashboard/bundle/ruby/2.2.0/gems/aws-sdk-v1-1.64.0/lib/aws/core/client.rb:373:in return_or_raise'\nJun 29 11:48:35 :worker production.log:  /var/www/platform/shared/dashboard/bundle/ruby/2.2.0/gems/aws-sdk-v1-1.64.0/lib/aws/core/client.rb:476:inclient_request'\nJun 29 11:48:35 :worker production.log:  (eval):3:in put_object'\nJun 29 11:48:35 :worker production.log:  /var/www/platform/shared/dashboard/bundle/ruby/2.2.0/gems/aws-sdk-v1-1.64.0/lib/aws/s3/s3_object.rb:1765:inwrite_with_put_object'\nJun 29 11:48:35 :worker production.log:  /var/www/platform/shared/dashboard/bundle/ruby/2.2.0/gems/aws-sdk-\n```\n. It was all the same upload. Also it looks like it didn't succeed. I may be\nmisreading whether or not they succeed, as our code has some automatic\nretry logic after about 15 minutes.\nIll keep looking into this on my end. Thanks.\nSent from my mobile device\nOn Jul 1, 2015 1:15 PM, \"Trevor Rowe\" notifications@github.com wrote:\n\nThose look like sample log entries from 5 slow uploads from one day. Is\nthat correct?\nJun 29 11:40:22 :worker production.log:   3:40:22.261013828 [WARN] /opt/rbenv/versions/2.2.2/lib/ruby/2.2.0/openssl/buffering.rb:326:in `syswrite'\nThis would imply the time is being spent waiting for OpenSSL to write to\nthe socket. It does not appear to be getting stuck in SDK code. Short of\ninstrumenting your code with monkey-patches that collect more fine-grained\ntiming information with context (such as the age of the http connection),\nI'm not sure how to track this down. Thoughts?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/aws/aws-sdk-ruby/issues/847#issuecomment-117755651.\n. I don't think this is related to aws-sdk. https://github.com/mperham/sidekiq/issues/702#issuecomment-122429014\n. \n",
    "shaharz": "+1\n2.0.0p247 + Local works fine, but on Heroku constantly times out. Only solution was to go with a new connection per upload.\n. ",
    "skalb": "+1\nRuby 2.0.0p247 works fine local, but Ruby 2.0.0p353 on Heroku has intermittent timeouts.\n. @trevorrowe 1.21.0\n. @trevorrowe I'll give that a try and let you know, thanks! For reference, here's a series of requests:\n2013-11-26T18:21:16.237960+00:00 app[worker.1]: [AWS S3 200 0.360992 0 retries] put_object(:acl=>:public_read,:bucket_name=>\"...\",:cache_control=>\"max-age=315576000\",:content_length=>169248,:data=>#StringIO:0x007fcca0a3b780,:key=>\"...\")\n2013-11-26T18:21:16.237960+00:00 app[worker.1]:\n2013-11-26T18:21:16.268220+00:00 app[worker.1]: [AWS S3 200 0.034549 0 retries] put_object(:acl=>:public_read,:bucket_name=>\"...\",:cache_control=>\"max-age=315576000\",:content_length=>73479,:data=>#StringIO:0x007fcc9ec583d0,:key=>\"...\")\n2013-11-26T18:21:16.268220+00:00 app[worker.1]:\n2013-11-26T18:21:16.282839+00:00 app[worker.1]: [AWS S3 200 0.040105 0 retries] put_object(:acl=>:public_read,:bucket_name=>\"...\",:cache_control=>\"max-age=315576000\",:content_length=>63526,:data=>#StringIO:0x007fcca03f94f8,:key=>\"...\")\n2013-11-26T18:21:16.282839+00:00 app[worker.1]:\n2013-11-26T18:21:16.327017+00:00 app[worker.1]: [AWS S3 200 0.096753 0 retries] put_object(:acl=>:public_read,:bucket_name=>\"...\",:cache_control=>\"max-age=315576000\",:content_length=>123440,:data=>#StringIO:0x007fcc9c879cc0,:key=>\"...\")\n2013-11-26T18:21:16.327017+00:00 app[worker.1]:\n2013-11-26T18:21:16.889061+00:00 app[worker.1]: [AWS S3 200 22.10015 1 retries] put_object(:acl=>:public_read,:bucket_name=>\"...\",:cache_control=>\"max-age=315576000\",:content_length=>100250,:data=>#StringIO:0x007fcca0b99b90,:key=>\"...\")\n. ",
    "maxfolley": "Unfortunately I was forced into using an old version of this gem due to a dependency strictly requiring a version with this bug present. I was batch processing images in a rake task and uploading them to S3 using paperclip. I tossed the following after each save and it did the trick.\nAWS.config.http_handler.pool.empty!\n. ",
    "bravilli": "Is it fixed already?\nI also meet the same issue while uploading a file by s3.\njust Request-timeout issue, and really can not find the way.\n. ",
    "yangez": "Also happening for me in Ruby 2.0.0p353 with aws-sdk 1.33.0, but @maxfolley 's workaround works for me too.\n. ",
    "mikhailov": "we have got the same problems, has it been fixed? At the moment we use AWS-SDK 1.9.1, Ruby 2.0.0-p353. It's a bit problematic to update the gem version, is there patch available?\n. @lsegal we keep almost same Gemfile across internal apps (SOA based), but only one app is on Ruby 2.0.0, experemental branch. The exception was never happened on 1.8.7/1.9.3. For us to upgrade the certain gem takes a time, that has to be scheduled :) So is that issue common for all environments?\nThe second question what kind of traffic counts, just uploading?\nP.S. The issue happens primarily to copying files between regions (US->EU, EU-AU). We use different buckets for different purposes.\nNot sure if that helps, but our config file is the following: AWS.config(max_retries: 10)\n. Trevor Rowe said that is the uncommon issue, the test-case: \"S3 is waiting for additional bytes that aren't coming.\" http://stackoverflow.com/questions/13149240/ruby-aws-sdk-timeout-error\nThe syntax I use all the time is:\nruby\n    s3_object = s3_interface.buckets['bucket'].objects[file_name]\n    s3_object.write(:file => file_name, :content_type => 'video/mp4')\nMaybe I have to use binary format somewhere here?\n. @lsegal ok, thanks, do you know what kind of traffic throttles (100+ rec/sec), just uploading?\n. sorry guys, I have not got the answer on AWS S3 limit for throttling yet. I reviewed aws-sdk changelog v1.9.1..master and didnt' find any serious reason for upgrading. So how can this issue be solved, by downgrading to Ruby 1.9.3?\nThis issues is difficult to replicate so I'm pretty sure that happens because of throttling limit. Still not sure what is the limit and what counts: download, upload, both?\n. @lsegal ok, great, so 100 req/sec for all operations with S3 across all used regions?\n. The problem didn't solve yet, please let us know the progress please.\n. @trevorrowe that's great, we can test the patch, thanks! A question about s3_endpoint, we still use it, but it seems to be deprecated in favor of region. Can we still use s3_endpoint syntax with aws-sdk-ruby v2?\n. @lsegal we have no problems to running the process via async non-blocing way. The main issue is high CPU resources consumption and waiting while each file is copied from S3 bucket (eu-west-1) to S3 bucket (us-west-1). For instance:\nEC2 (eu-west-1) to S3 (eu-west-1): 0.5-1 sec\nS3 (eu-west-1) to S3 (us-west-1): 5-10 sec\n. AWS S3 SSL settings are not good, overall rating is C https://www.ssllabs.com/ssltest/analyze.html?d=s3-eu-west-1.amazonaws.com&s=54.231.132.8\n. Sorry, you are right, there is nothing SDK can do to make server-side better. Don't you know how long SSLv3 remain being available on AWS S3 endpoints?\n. AWS S3 settings are going to be changed as of 12:00 AM PDT April 30, 2015 https://forums.aws.amazon.com/thread.jspa?threadID=178739\n. ",
    "Bhushan30": "@lsegal : How we can upload throttling stream using S3 aws-sdk-ios.\nThe S3UploadInputStream was deprecated after the release of iOS6. So what is best solution to upload throttling stream to amazon server?\n. ",
    "cdunn": "We just upped to 2.1 from 1.9.3 and are experiencing the same issue on sdk 1.38.0.\nIntermittent 20second writes with 1 retry.\n<- \"PUT //test HTTP/1.1\\r\\nContent-Type: \\r\\nAccept-Encoding: \\r\\nContent-Length: 172188\\r\\nX-Amz-Server-Side-Encryption: AES256\\r\\nX-Amz-Acl: authenticated-read\\r\\nUser-Agent: aws-sdk-ruby/1.38.0 ruby/2.1.1 x86_64-darwin13.0\\r\\nDate: Mon, 31 Mar 2014 20:23:17 GMT\\r\\nAuthorization: AWS =\\r\\nAccept: */*\\r\\nHost: s3.amazonaws.com\\r\\n\\r\\n\"\nConn close because of error Broken pipe, and retry\nopening connection to s3.amazonaws.com:443...\nopened\nstarting SSL for s3.amazonaws.com:443...\nSSL established\n<- \"PUT //test HTTP/1.1\\r\\nContent-Type: \\r\\nAccept-Encoding: \\r\\nContent-Length: 172188\\r\\nX-Amz-Server-Side-Encryption: AES256\\r\\nX-Amz-Acl: authenticated-read\\r\\nUser-Agent: aws-sdk-ruby/1.38.0 ruby/2.1.1 x86_64-darwin13.0\\r\\nDate: Mon, 31 Mar 2014 20:23:17 GMT\\r\\nAuthorization: AWS =\\r\\nAccept: */*\\r\\nHost: s3.amazonaws.com\\r\\n\\r\\n\"\n...waits for 15 seconds...\n-> \"HTTP/1.1 400 Bad Request\\r\\n\"\n-> \"x-amz-request-id: \\r\\n\"\n-> \"x-amz-id-2: \\r\\n\"\n-> \"Content-Type: application/xml\\r\\n\"\n-> \"Transfer-Encoding: chunked\\r\\n\"\n-> \"Date: Mon, 31 Mar 2014 20:23:36 GMT\\r\\n\"\n-> \"Connection: close\\r\\n\"\n-> \"Server: AmazonS3\\r\\n\"\n-> \"\\r\\n\"\n-> \"15c\\r\\n\"\nreading 348 bytes...\n-> \"<?xml version=\\\"1.0\\\" encoding=\\\"UTF-8\\\"?>\\n<Error><Code>RequestTimeout</Code><Message>Your socket connection to the server was not read from or written to within the timeout period. Idle connections will be closed.</Message><RequestId></RequestId><HostId></HostId></Error>\"\nread 348 bytes\nreading 2 bytes...\n-> \"\\r\\n\"\nread 2 bytes\n-> \"0\\r\\n\"\n-> \"\\r\\n\"\nConn close\n...then retries and succeeds instantly\nI've tried some of the purposed ideas above to no avail. Anyone figured out anything new here?\nI don't believe it is throttling related as I switched back to our 1.9.3 branch and it does not appear to occur.\n. So I'm not sure if there is an easier (or safer way) to fix this but it looks like starting in 2.x, Net::HTTP is retrying transport_request for request methods it deems idempotent and it includes PUT in that list.\nSee http://rxr.whitequark.org/mri/source/lib/net/http.rb?v=2.1.0-p0 line 1426 and compare against versions < 2\nFor now I am removing PUT and it seems to have fixed the timeout issue.\nNet::HTTP::IDEMPOTENT_METHODS_.delete(\"PUT\")\nEDIT: or the request method should be updated to PATCH...\n. +1\n. ",
    "marleich": "I have got exactly the same problem. @cdunn 's workaround works for me.\n. ",
    "pickerflicker": "@cdunn @marleich I have the exact same problem, but the workaround doesn't work for me.  I still see intermittent connections taking exactly 20 seconds.\nCan you explain why the fix works?  I think by preventing Net::HTTP from retrying, it returns control to aws-sdk quickly and is able to use the \":http_open_timeout\" config option.  I've changed this timeout to 2 seconds, the default is 15 seconds.  But still no luck.  Any suggestions appreciated.\n. In this thread, an AWS rep is saying during heavy congestion, AWS servers will wait 20 seconds before closing the connection.  I believe this explains why changing the AWS config timeouts or applying the net::http workaround does not work for me: \nhttps://forums.aws.amazon.com/thread.jspa?threadID=149634 \n. I ended up using 'fog' gem to upload to S3 and I get no more timeouts or retries.\n. Thanks for clarifying @trevorrowe.  I wasn't exactly sure why Fog gem worked.\nI'm using Ruby 2.1.0.  My original issue was not an actual error, since the second PUT request would always be quick and successful.  My issue was the occasional 20s timeout is just too long and ruins user experience.  I've tried adding the net:http workaround into my Rails project initializers, but I still saw these occasional timeouts.\n. @trevorrowe Sorry, there's no good way for me to try this out now.  This timeout issue didn't show up in QA(probably not enough requests.) and I can't experiment on production.\n. @trevorrowe The timeouts would at least once every 2 hours.  I would see a long POST request take a little over 20 seconds in New Relic.  These requests all had the same signature: first PUT request timeout after 20 seconds, followed by one retry which was always successful and quick.\n. ",
    "tcassanego": "The workaround proposed by @cdunn works for me to remove the timeout, but I still get tons of retries.  They most often come from the EOF error.  Oddly enough, running on a single thread produces the most retries, while they decrease as I ramp up to 5 threads.  Seems like this would point to it not being congestion related...\n. ",
    "gonzedge": "\ud83d\udc4f\n. ",
    "justincase": "Ruby 1.8.7 is retired and 1.9.3 will be EOL soon.\n:fireworks: :ship: :+1: \n. ",
    "guyshechter": "I am also seeing many CRC32 errors when not limiting the number of responses in my query.\nmeasure_groups =  DataCosm.query( {:hash_value => user_id_device_id, \n                                  :scan_index_forward => true,\n                                  :limit => 30,   # The difference between CRC32 errors and not in >= 1.9.2\n                                  :range_greater_than => params[:time_opts][:start_date].to_i\n                                 })\nI found that by using aws-sdk-ruby v1.9.1\n https://github.com/aws/aws-sdk-ruby/tree/1.9.1 I was able to make the CRC errors go away.\n. ",
    "naberohs": "If the API response was gzip compressed, CRC32 checks seems to fail.\nNet::HTTPRequest (ruby 2.0.0) adds Accept-Encoding to enable compression.\n\"x-amz-crc32\" of API response header has the value generated from compressed body. On the other hand, CRC32 check use inflated body. As a result, CRC32 check has failed.\nI think an easy solution is to set the identity to Accept-Encoding. Response body is not compressed, but it does not change with 1.9.3.\n``` ruby\naws-sdk-1.10.0/lib/aws/core/http/net_http_handler.rb:89\n    def build_net_http_request request\n\n      # Net::HTTP adds a content-type header automatically unless its set\n      # and this messes with request signature signing.  Also, it expects\n      # all header values to be strings (it call strip on them).\n      - headers = { 'content-type' => '' }\n      + headers = { 'content-type' => '',  'accept-encoding' => 'identity;q=1.0' }\n\n```\n. ",
    "kbacha": "I've been running into the same issue while doing multipart uploads on different threads. Any help into why this is happening would be great.\n. I wrapped my code in some retry logic and can 'circumvent' this bug. But I'm not really satisfied by this solution. If I knew 'why' it was happening (like packet loss or something else) then I'd be more contented by retrying.\n. ",
    "mdub": "FWIW, I've been able to \"solve\" my problem by increasing the :max_retries option. The default is 3; I've cranked it up to 15, and seen no more EOFErrors.\nConclusion: sometimes you just need to try harder :-)\n. I was about to do the same, then realised/discovered that aws-sdk has retry (with exponential back off) built it.\nAnd in fact, if you enable logging, there's evidence that it retries a recovers:\n[AWS S3 200 0.265568 0 retries] get_bucket_location ...\n[AWS S3 200 3.844282 0 retries] list_objects ...\n[AWS S3 200 0.579139 1 retries] copy_object ...\nFor hints as to when/why it needs to retry, check out the #retryable_error? function in aws/core/client.rb.\ndef retryable_error? response\n    expired_credentials?(response) or\n    response.network_error? or\n    throttled?(response) or\n    redirected?(response) or\n    response.error.kind_of?(Errors::ServerError)\n  end\n. +1\n. Specifying an appropriate path in aws-sdk-core/apis/cloudformation/2010-05-15/resources-1.json seems to do the trick:\ndiff\ndiff --git a/aws-sdk-core/apis/cloudformation/2010-05-15/resources-1.json b/aws-sdk-core/apis/cloudformation/2010-05-15/resources-1.json\nindex 5fa2e6f..ffacfb1 100644\n--- a/aws-sdk-core/apis/cloudformation/2010-05-15/resources-1.json\n+++ b/aws-sdk-core/apis/cloudformation/2010-05-15/resources-1.json\n@@ -32,15 +32,16 @@\n     \"hasMany\": {\n       \"Stacks\": {\n         \"request\": { \"operation\": \"DescribeStacks\" },\n         \"resource\": {\n           \"type\": \"Stack\",\n           \"identifiers\": [\n             { \"target\": \"Name\", \"source\": \"response\", \"path\": \"Stacks[].StackName\" }\n-          ]\n+          ],\n+          \"path\": \"Stacks[]\"\n         }\n       }\n     }\n   },\n   \"resources\": {\n     \"Event\": {\n       \"identifiers\": [\nIs it as simple as that, or is there a good reason those resources don't have path defined?\n. Any further thoughts on this?\nWould it be helpful if I put a pull-request together, adding the relevant path declarations?\n. Fixed in #1311 - thanks @awood45.  But unfortunately it was reintroduced in 990c5be :-(\nI'll open another issue.\n. Hmm. Travis CI build appears to have been failing (one test) since 9ee5ac9. @awood45?\n. Looks all good in 2.6.14; thanks!\n. @cjyclaire: It's happening for newly launched instances. I'm running it (the waiter) on the new instance itself \u2013 waiting for ELBs to mark it healthy, before sending a success signal to CloudFormation.\nI think an \"error\" matcher needs to be added to the definition of the \"InstanceInService\" waiter. I'll send you a PR ... but imagine you'll need to get it fixed upstream (the waiter definitions are elsewhere, right)?\nAnyway, I've managed to work around it in the meantime, using a custom poller definition:\nruby\nPOLLER_CONFIG = {\n  \"operation\" => \"DescribeInstanceHealth\",\n  \"acceptors\" => [\n    {\n      \"argument\" => \"InstanceStates[].State\",\n      \"expected\" => \"InService\",\n      \"matcher\" => \"pathAll\",\n      \"state\" => \"success\"\n    },\n    {\n      \"matcher\" => \"error\",\n      \"expected\" => \"InvalidInstance\",\n      \"state\" => \"retry\"\n    }\n  ]\n}\n. ",
    "laurilehmijoki": "The s3_website gem users are also experiencing this end-of-file error: https://github.com/laurilehmijoki/s3_website/issues/34.\nAny idea what could cause this problem?\n. Thanks for answering! This problem has puzzled me for a long time.\nWhat concurrency level would you recommend?\nSome of the s3_website gem users have websites that contain hundreds of files. They would really benefit from a high level of concurrency.\n. ",
    "leondmello": "@lsegal I'm trying to find an option here where I can possibly change the thread_count used for multipart uploads.\nCan't seem to find it \ud83d\ude1e  . ",
    "rb2k": "A nasty workaround in the meantime:\nmodule Net\n  class HTTP\n    alias_method :real_start, :start\n    def start\n      retry_count = 0\n      begin\n        real_start\n      rescue SystemCallError => e\n        raise e if (retry_count +=1) >= 4\n        sleep 2\n        retry\n      end\n    end\n  end\nend\n. I think the behaviour is still the same. Socket exceptions on http.start don't seem to get rescued anywhere.\nWe didn't see this one a lot, but it happens every now and then (with about 4000+ instance, you start to see these little blips)\n. I'm not at the computer with all the source at the moment, but aren't those retries just for 4xx / 5xx errors?\nMy errors weren't for specific requests but when initially creating a connection in the connection pool (http.start)\n. Oh, shame on me :(\nI followed the whole call chain and this commit might have actually changed the behaviour:\nhttps://github.com/aws/aws-sdk-ruby/commit/bcd98dbc579e9d520dadffe9911c64743a4ce082\nThanks for helping!\n. Just a question after the fact: Since the SDK initializes the object without any params itself over at: https://github.com/aws/aws-sdk-ruby/blob/v1.63.0/lib/aws/core/credential_providers.rb#L128\nWould it be useful to just use AWS.configure for the default amount of retries? \n. Created one over here https://github.com/aws/aws-sdk-ruby/issues/717\n. Yeah, seen this happening too.\nWould appreciate a merge :-/\n. ",
    "eliaslevy": "The following quick hack got it working for me:\nclass AWS::S3::ObjectCollection\n  protected\n  def next_markers page\n    marker = page.next_marker or (last = page.contents.last and last.key)\n    if marker.nil?\n      raise 'Unable to find marker in S3 list objects response'\n    else\n      { :marker => marker }\n    end\n  end\nend\n. ",
    "markaschneider": "Thanks.  The branch in your link fixed the issue for me.\n. ",
    "danielpuglisi": "@lsegal thanks, your pull request fixed the issue for me too <3\n. ",
    "jordanmichaelrushing": "unfortunately this pull didn't help me. I'm still getting the error even while specifying, :git => 'git://github.com/lsegal/paperclip', :branch => 'remove-aws-sdk-version-check'. I'm still getting the uninitialized constant AWS::Core::ClientLogging when trying to create a new user.\n. My gemlock shows paperclip (3.4.2)\n      activemodel (>= 3.0.0)\n      activerecord (>= 3.0.0)\n      activesupport (>= 3.0.0)\n      cocaine (~> 0.5.0)\n      mime-types\n. ",
    "jimcroft": "Looks like the underlying RDS tags API is pretty straight forward and can be built to work similarly to EC2 but I've found a snag.\nThe AddTagsToResource, ListTagsForResource and RemoveTagsFromResource methods require a full ARN to identify the RDS instance. All the other methods work with the DBInstanceIdentifier instead.\nI can deduce all of the ARN components except the customer account number. Any idea if the SDK or the low level APIs expose this anywhere?\nReference:\nhttp://docs.aws.amazon.com/AmazonRDS/latest/APIReference/API_AddTagsToResource.html\nhttp://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_Tagging.html#USER_Tagging.ARN\n. Picking the discussion up over here... if I amend this PR so that Aws::SharedCredentials cred and region attributes are set following the same logic as the AWS CLI is that something that would likely be merged in?\n. Ok thanks @trevorrowe, will wait to see what feedback comes from your discussions. FYI where the CLI docs say \"the keys in the credentials file will take precendence\", I've just tested and it also seems to mean a region set in credentials takes precedence over a region set in the config file.\n. Hi @trevorrowe any update on this?\n. ",
    "dkhofer": "Argh, it looks like the markdown rules eliminated the status code part of the response, it was status=505 with an \"at\" symbol in front of status.\n. The sporadic errors are not just 505s, I saw at least one 400 too and most of the time these responses did not cause the problem we've been seeing.  I will try to narrow down the cause.\n. So I've seen at least two cases:\nStatus: 400 Body: \"\" (not nil, empty string)\nStatus: 505 Body: \"\"\nIn both cases, the error raised is an instance of AWS::Errors::Base.  With the 400, I think the S3 client should be raising a BadRequestError, but is not doing so because the body is an empty string and not nil.  Can you confirm?\nWith the 505, I'm confused because from reading the code it seems that it should be retrying the request but isn't.  I don't have an explanation for this but if there is anything you'd like me to try let me know.\nThis kind of problem happens about once in one thousand attempts.  AFAICT it is bogus each time, so I am implementing some simple retry code on my side which I believe will take care of the problem.  But I wanted to note this behavior in case anyone else sees it.  Please let me know if you have any questions.  Thanks!\n. I think we already have logging set up.  The only line related to the 400 that I can find is this (with filenames etc. manually obscured):\nproduction.log.5:[AWS S3 400 0.004178 0 retries] put_object(:bucket_name=>\"our-bucket\",:content_length=>1363,:data=>#,:key=>\"our-file\") AWS::Errors::Base\n. FYI, the request was coming from a us-east instance and going to a us-east S3 bucket.  Looks like 4ms for the time?  Is that a reasonable time delta for that scenario?\nI checked for 505 responses in the lines logged by the aws-sdk gem and I'm not seeing them.  I could have sworn I saw these yesterday but unfortunately I've lost the custom log files that had the lines I was logging to track these exceptions with more detailed information.  I will keep watching for them.\nSo I guess my one question for now is whether the 400s with a body of \"\" should be resulting in a BadRequestError instance being raised instead of an AWS::Errors::Base instance.  Thanks!\n. Just saw another 400:\nproduction.log:[AWS S3 400 0.101772 0 retries] put_object(...) AWS::Errors::Base\nThese happen maybe 5-10 times a day for us so it may be a day or two until I can tell if the 505s are really happening or not.  Sorry for my confusion.\n. I will make that change today and report back.\n. Unfortunately, the wire tracing itself caused errors.  Sample backtrace:\nundefined method `<<' for #ActiveSupport::BufferedLogger:0x00000005be63b0\n/home/rails/tracking/shared/bundle/ruby/1.9.1/gems/activesupport-3.2.13/lib/active_support/tagged_logging.rb:65:in `method_missing'\n/usr/local/rvm/rubies/ruby-1.9.3-p392/lib/ruby/1.9.1/net/http.rb:1410:in `D'\n/usr/local/rvm/rubies/ruby-1.9.3-p392/lib/ruby/1.9.1/net/http.rb:761:in `connect'\n/usr/local/rvm/rubies/ruby-1.9.3-p392/lib/ruby/1.9.1/net/http.rb:755:in `do_start'\n/usr/local/rvm/rubies/ruby-1.9.3-p392/lib/ruby/1.9.1/net/http.rb:750:in `start'\n/home/rails/tracking/shared/bundle/ruby/1.9.1/gems/aws-sdk-1.11.0/lib/aws/core/http/connection_pool.rb:301:in `start_session'\n/home/rails/tracking/shared/bundle/ruby/1.9.1/gems/aws-sdk-1.11.0/lib/aws/core/http/connection_pool.rb:125:in `session_for'\n/home/rails/tracking/shared/bundle/ruby/1.9.1/gems/aws-sdk-1.11.0/lib/aws/core/http/net_http_handler.rb:52:in `handle'\n/home/rails/tracking/shared/bundle/ruby/1.9.1/gems/aws-sdk-1.11.0/lib/aws/core/client.rb:238:in `block in make_sync_request'\n/home/rails/tracking/shared/bundle/ruby/1.9.1/gems/aws-sdk-1.11.0/lib/aws/core/client.rb:267:in `retry_server_errors'\n/home/rails/tracking/shared/bundle/ruby/1.9.1/gems/aws-sdk-1.11.0/lib/aws/core/client.rb:234:in `make_sync_request'\n/home/rails/tracking/shared/bundle/ruby/1.9.1/gems/aws-sdk-1.11.0/lib/aws/core/client.rb:496:in `block (2 levels) in client_request'\n/home/rails/tracking/shared/bundle/ruby/1.9.1/gems/aws-sdk-1.11.0/lib/aws/core/client.rb:376:in `log_client_request'\n/home/rails/tracking/shared/bundle/ruby/1.9.1/gems/aws-sdk-1.11.0/lib/aws/core/client.rb:462:in `block in client_request'\n/home/rails/tracking/shared/bundle/ruby/1.9.1/gems/aws-sdk-1.11.0/lib/aws/core/client.rb:358:in `return_or_raise'\n/home/rails/tracking/shared/bundle/ruby/1.9.1/gems/aws-sdk-1.11.0/lib/aws/core/client.rb:461:in `client_request'\n(eval):3:in `put_object'\n/home/rails/tracking/shared/bundle/ruby/1.9.1/gems/aws-sdk-1.11.0/lib/aws/s3/s3_object.rb:1704:in `write_with_put_object'\n/home/rails/tracking/shared/bundle/ruby/1.9.1/gems/aws-sdk-1.11.0/lib/aws/s3/s3_object.rb:608:in `write'\n. Sorry: \"undefined method `<<' for #ActiveSupport::BufferedLogger:0x00000005be63b0\"\nWe're on rails 3.2.13.\n. OK, that's working, I'll let you know what I find.\n. Here is an example 400:\n<- \"PUT /10138007-stderr HTTP/1.1\\r\\nContent-Type: \\r\\nContent-Length: 0\\r\\nUser-Agent: aws-sdk-ruby/1.11.0 ruby/1.9.3 x86_64-linux\\r\\nDate: Tue, 04 Jun 2013 15:39:33 GMT\\r\\nAuthorization: AWS AKID:baR5kfepEINAkGnHzK1IhVVQWms=\\r\\nAccept: */*\\r\\nHost: a-b-c.s3.amazonaws.com\\r\\n\\r\\n\"\n-> \"HTTP/1.1 400 Bad Request\\r\\n\"\n-> \"Transfer-Encoding: chunked\\r\\n\"\n-> \"Date: Tue, 04 Jun 2013 15:39:32 GMT\\r\\n\"\n-> \"Connection: close\\r\\n\"\n-> \"Server: AmazonS3\\r\\n\"\n-> \"\\r\\n\"\n-> \"0\\r\\n\"\n-> \"\\r\\n\"\nConn close\n[AWS S3 400 0.086687 0 retries] put_object(:bucket_name=>\"a-b-c\",:content_length=>0,:data=>#<File:/home/rails/tracking/releases/20130603182446/tmp/mnt/logs/(10138007-stderr) (0 bytes)>,:key=>\"10138007-stderr\") AWS::Errors::Base\nAt this point we have implemented retries in the code that calls into aws-sdk and have gotten 200s back consistently on later attempts with files that generated 400s.  So we've worked around this problem.  My only request is that the gem raise a BadRequestError instead of an instance of AWS::Errors::Base so that it's clearer what is going on.\nLet me know if there is anything else I can help with.  Thanks!\n. Great, thanks!  I will change our code to handle AWS::Errors::ClientError instead.\nThe title of this issue is now completely wrong.  I am thinking of changing it to something like \"Misleading error and message coming from empty-body 400 responses.\"  Is that OK with you?\n. Thanks!  I'll keep an eye out for the release.\nAlso belated thanks @trevorrowe for the info on errors and retries!\n. ",
    "redzebra": "Hi @lsegal,\nWhile I agree with the rationale as a general principal, I don't think it holds up in the specific case of the Support API. Perhaps I should have proposed support.amazonaws.com instead. I think this boils down to whether one views Support as a global service or as one released only in us-east-1; I take the former view as the Support API - particularly Trusted Advisor - is providing globally-aggregated data rather than region-specific data.\nConsider examining EC2 instances in an arbitrary region and checking for related Trusted Advisor flags. The issue is that, irrespective of EC2 region, Trusted Advisor results are always, and only, available from support.us-east-1. Ignoring my previous example with AWS.config, assume that EC2 region selection is accomplished using ENV['AWS_REGION']:\nruby\nec2 = AWS::EC2.new\nec2.do_stuff # does regional stuff if AWS_REGION is valid\nsupport = AWS::Support.new\nsupport.do_stuff # does global stuff if, and only if, AWS_REGION==us-east-1\nI consider this broken because if my scenario involved IAM instead of Support the behaviour would be different (and as I expect):\nruby\nec2 = AWS::EC2.new\nec2.do_stuff # does regional stuff if AWS_REGION is valid\niam = AWS::IAM.new\niam.do_stuff # does global stuff regardless of AWS_REGION\nThe reality with Support is that someone has to hardcode us-east-1 because it is the only option. Either the SDK handles it or, exacerbated by @trevorrowe deprecating the possibility of AWS.config(:support_endpoint=>'support.us-east-1'), us-east-1 must be explicitly stated on each and every use of AWS::Support.new. Either way, in the event of Support API becoming regional instead of global, updating copies of the SDK probably won't be the biggest problem.\n. ",
    "yhahn": "Thanks for the fast fix :+1: \n. ",
    "ojak": "Sorry, looks like I missed a bracket.  This is not a bug.\n. ",
    "sakuro": "Thank you for the clarification!\n. ",
    "pgibler": "How do I raise the issue of the typo env var to the Amazon Elastic Beanstalk team? It's highly confusing and took me a while to realize that it was the source of my issues.\n. ",
    "lime": "I contacted AWS Support about this, hopefully the information will reach the Elastic Beanstalk team that way. :)\n. ",
    "rainerborene": "@trevorrowe Sorry for bringing this up again. But I can't find any examples of this feature being used. How I would use it with Sidekiq for example?\n. Take for example this piece of code. How many connections the pool will hold?\n``` ruby\nclass MailWorker\n  def perform\n    @ses = Aws::SES::Client.new\n    @ses.send_raw_email(...)\n  end\nend\n10.times do |new|\n  MailWorker.new.perform\nend\n```\nWhat I am asking is how to ensure that persistent connections are being used by other threads in my application? How do I configure these pool of connections? \n. ",
    "gstib": "<- \"POST / HTTP/1.1\\r\\nContent-Type: application/x-www-form-urlencoded; charset=utf-8\\r\\nContent-Length: 242\\r\\nUser-Agent: aws-sdk-ruby/1.9.5 ruby/1.9.3 i386-mingw32\\r\\nAccept: */*\\r\\nHost: ec2.ap-southeast-2.amazonaws.com\\r\\n\\r\\n\"\n<- \"AWSAccessKeyId=#########&Action=GetConsoleOutput&InstanceId=i-9c1824a6&Signature=2Rs3l0%2BXVRfCXFlH8fOrh%2BYVatA3G12thdjmId2C3sQ%3D&SignatureMethod=HmacSHA256&SignatureVersion=2&Timestamp=2013-06-14T05%3A10%3A46Z&Version=2013-02-01\"\n-> \"HTTP/1.1 200 OK\\r\\n\"\n-> \"Content-Type: text/xml;charset=UTF-8\\r\\n\"\n-> \"Transfer-Encoding: chunked\\r\\n\"\n-> \"Date: Fri, 14 Jun 2013 05:10:46 GMT\\r\\n\"\n-> \"Server: AmazonEC2\\r\\n\"\n-> \"\\r\\n\"\n-> \"12a\\r\\n\"\nreading 298 bytes...\n-> \"<?xml version=\\\"1.0\\\" encoding=\\\"UTF-8\\\"?>\\n<GetConsoleOutputResponse xmlns=\\\"http://ec2.amazonaws.com/doc/2013-02-01/\\\">\\n    <requestId>b31dda62-18ca-475d-8838-05dfac353b79</requestId>\\n    <instanceId>i-9c1824a6</instanceId>\\n    <timestamp>2013-06-14T05:10:46.000Z</timestamp>\\n</GetConsoleOutputResponse>\"\nread 298 bytes\nreading 2 bytes...\n-> \"\\r\\n\"\nread 2 bytes\n-> \"0\\r\\n\"\n-> \"\\r\\n\"\nConn keep-alive\n. ",
    "benhamill": "Personally, I'd prefer to see 1.8 support dropped. Or, more specifically, I'd like to see this gem not go out of its way to support 1.8. If someone wants to use a newer version of this gem on 1.8, they can declare their own nokogiri version constraints. The way this new version dependency is declared, it prevents people who are using features of future versions of nokogiri (we're using 1.6.0.rc1 at work already) from also using aws-sdk. :disappointed: \n. ",
    "gtd": "It's one thing to keep supporting 1.8, but it's another thing to prevent using current versions.  IMO if you specify a '< xxx' declaration in a gem, it should only be because that version is incompatible.  It should not be a defensive move on behalf of legacy users who can trivially add an explicit requirement to nokogiri 1.5 to their application Gemfile and be done with it.\nIn other words, if the choice is between sparing 1.8 users a line in their Gemfile (and at worse a bit of confusion) and require Nokogiri 1.6 users to fork aws-sdk-ruby I think the choice is clear.\n. > I still feel we owe existing users a grace period with some warning.\nBut existing users would have a Gemfile.lock with a supported version.  There's nothing forcing an upgrade here, that would only happen on a new project (or at least a project adding nokogiri for the first time).  Projects in active development and certainly new projects will be on 1.9.x or later given that 1.8 is two major versions back and doesn't even have security support anymore.  In short, I think the possibility of anyone being affected by this is actually pretty slim (and I say this as someone who still has several apps on 1.8.7), and if so is trivially handled with a bit of documentation to add a line to your Gemfile.  Contrast with what will certainly be a growing number people requiring nokogiri 1.6 either explicitly or implicitly as a secondary dependency and facing the brick wall of irreconcilable version requirements when they try to bundle install.  I've been in this situation of having to fork 3rd party gems not because of any actual incompatible software, but simply because of overzealous version fencing and it's a bitter pill to swallow.\n. Okay, fair enough, thank you for the response.\n. @smenor As someone who shares your opinion (and argued quite a bit more objectively for it earlier in the thread), I need to point out that you are wrong.  You were being a dick, straight up, whether intended or not.  Don't try to go on the offensive against Ryan to defend yourself, just own up to it and move on.\n. ",
    "teeparham": "+1 to dropping 1.8 support and removing the nokogiri version restriction. It's worse to prevent users on 1.9+ from upgrading other gems that depend on the new nokogiri than it is to provide support to an old an officially unsupported ruby version.\nFor anyone else looking to update nokogiri, you'll need to use aws-sdk version 1.11.1, which is the last version that allows nokogiri 1.6:\ngem 'aws-sdk, '1.11.1'\n. ",
    "sblakey": "Not the LA FC or any of the new systems I designed - but that doesn't help\nmuch if customers can't buy stuff.\nOn Fri, Jul 12, 2013 at 11:56 AM, Tee Parham notifications@github.comwrote:\n\n+1 to dropping 1.8 support and removing the nokogiri version restriction.\nIt's worse to prevent users on 1.9+ from upgrading other gems that depend\non the new nokogiri than it is to provide support to an old an officially\nunsupported ruby version.\nFor anyone else looking to update nokogiri, you'll need to use aws-sdk\nversion 1.11.1, which is the last version that allows nokogiri 1.6:\ngem 'aws-sdk, '1.11.1'\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/aws/aws-sdk-ruby/issues/273#issuecomment-20897116\n.\n. \n",
    "kellyfelkins": "For now using hsadan fork, with nokogiri '~> 1.6.0'\nhttps://github.com/hsadan/aws-sdk-ruby\n. Hi Ryan,\nI had trouble with an earlier version of the gem. There have been\nrecent updates to the gem that allow you to run nokogiri 1.6. Here is a\nexcerpt from my gemfile.lock\nGEM\nremote: [1]https://rubygems.org/\nspecs:\n```\naws-sdk (1.11.1)\njson (~> 1.4)\nnokogiri (>= 1.4.4)\nuuidtools (~> 2.1)\nnokogiri (1.6.0)\nmini_portile (~> 0.5.0)\n```\nAs you can see I'm running aws-sdk 1.11.1 and nokogiri 1.6.\nI hope this helps.\n-Kelly\nOn Mon, Oct 7, 2013, at 05:37 PM, Ryan Bigg wrote:\nHaving a 2.0 version would be really helpful. We're seeing some\n  issues in some Spree stores using an older version of Nokogiri and\n  we believe that Nokogiri 1.6 fixes those issues. Good to know that\n  it's being worked on!\n\u2014\nReply to this email directly or [2]view it on GitHub.\n[rbZ29HBfavmFcTcrEnr3ahOvv-ua5tugRjpaIr3xJw2umh_MHw8Q3szsw7QVQKSt.gif]\n\nneighbors@justmyneighbors.com\nThe email address for your neighborhood.\nReferences\n1. https://rubygems.org/\n2. https://github.com/aws/aws-sdk-ruby/issues/273#issuecomment-25857105\n. Thank you for getting back to me. I'll try your suggestions and let you\nknow.\nOn Wed, Nov 14, 2018 at 10:31 AM Chase Coalwell notifications@github.com\nwrote:\n\n@kellyfelkins https://github.com/kellyfelkins,\nI haven't been able to reproduce this issue.\nCan you confirm whether or not the record event payload is correct or not?\nCan you also enable http_wire_trace on the S3 client and confirm if the\ndata being returned from S3 is valid or not?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/aws/aws-sdk-ruby/issues/1915#issuecomment-438768794,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AACT_0mWQIbVKKtQVpNyk_Cnzv7MSqp_ks5uvGGUgaJpZM4YajjX\n.\n\n\n-- \nneighbors@justmyneighbors.com neighbors@justmyneighbors.com\nThe email address for YOUR neighborhood.\n. ",
    "pglombardo": "I agree with @gtd and @benhamill .  The Gemspec add_dependency line \"Lists the gems that must be installed for this gem to work\" but that isn't the case here.  The case here is that nokogiri '< 1.6' is needed under a special usage case (for users running Ruby 1.8).\nAdding that dependency line split your user base into two in which both can never be satisfied.\nThe better route to have taken would have been a gem post-install/README message noting that nokogiri dropped 1.8 support and that it should be version locked in the Gemfile for Ruby 1.8 users.\nFor background, I was just trying to update aws-sdk for a recent fix and just hit the dependency block caused by the imposed nokogiri lockdown.\n. I went back a couple commits to try to figure that out and it seems that I already had nokogiri 1.6 with aws-sdk 1.11.1 in the Gemfile.lock.  \nOnly when I tried to update aws-sdk to the latest version for a Ruby 2/Request::Timeout fix I realized that I was locked out from newer versions.  I switched to git source for now (per @kellyfelkins above) and the timeouts are gone.  (Thanks BTW)\n. ",
    "craigmcnamara": "Wouldn't this fix the problems without breaking anyones support?\nhttps://github.com/aws/aws-sdk-ruby/pull/330\n. ",
    "eahanson": "Any news on the on the 2.0 release? I'm itching to use a newer version of Nokogiri because other gems I use depend on the newer version.\n. ",
    "radar": "Having a 2.0 version would be really helpful. We're seeing some issues in some Spree stores using an older version of Nokogiri and we believe that Nokogiri 1.6 fixes those issues. Good to know that it's being worked on!\n. @smenor I love your ironic use of obnoxiousness to clearly demonstrate how obnoxious you think still support Ruby 1.8.7 is. It was ironic, wasn't it?\nIf you want to rip this gem out of your project, then please: go ahead. However, I will not stand for anybody within the Ruby community acting obnoxious towards people who put their valuable time into maintaining projects for the community. These same people who have lives that are more than just about maintaining one gem. Please appreciate that small fact.\nThese people are volunteering their time to build this project and you go and crap on their hard work because of a decision that you don't agree with? Totally uncool. \nEither fork this project and make the changes you want and then use your fork, or go ahead and rip this out and use something else. The maintainers will not act based on your obnoxiousness. \n. Excellent news! Thank you @trevorrowe.\n. ",
    "vfonic": "@lsegal +1 for finally removing nokogiri < 1.6.0 dependency...\n. ",
    "smenor": "This is obnoxious - you're forcing a ton of extra work on a clean OS X install, just to support a deprecated version of Ruby.\nIf people want to stick with an old version, let them sort out their dependencies - don't punish the people who are actually moving forward.\nThis makes me want to rip out aws-sdk in our projects.\n. That was most definitely not meant to be ironic (or obnoxious) for that matter (merely direct). I'm also certainly not crapping on your hard work. I wouldn't have followed @lsegal on Twitter if I were either hostile or unappreciative.\nThis is not the only thread with people asking to drop the 1.8.7 dependency, and not to be a dick but I really don't like the idea of forking the project and then having to maintain sync just to change one line in the Gemfile (and I really don't like spending my morning figuring out why something is broken only to find that it all comes back to supporting people who should have moved on from a very old and outdated version of Ruby; I understand there's pain in switching from 1.8.7 to 1.9+ but we did it, people with large codebases have done it, and if someone has a code base that's so large they can't handle that aspect of maintenance, then maybe they at least have the resources to use a different branch).\nIt's great that you're doing what you're doing and my understanding was that you're working for Amazon not volunteering (very sorry for that misunderstanding on my part and that you've misunderstood my tone.. frankly, on yours).\nThat said.. that you take me expressing my opinion as obnoxious seems, to me as unironically obnoxious and quite uncool - so good day to you, sir.\n@lsegal - what about a conditional in the Gemfile to detect RUBY_VERSION and then use the old >~ nokogiri for people using 1.8.7 but the current release for people not living in the dark ages ? \n. As you like though I wasn't trying to defend myself (I'll certainly own up to going a bit on the offensive though - as I was).\nI'm way past moved on (and have been since yesterday) and just responded to his comment and now yours.\nLooking back at my comment, I'd say I was only directly expressing my opinion - which remains the same. It may not have been the most diplomatic approach.. but it beats the hell out of passive-aggressively stomping your feet or accepting what's honestly a silly situation. To borrow your very own words\n\u00abIn other words, if the choice is between sparing 1.8 users a line in their Gemfile (and at worse a bit of confusion) and require Nokogiri 1.6 users to fork aws-sdk-ruby I think the choice is clear.\u00bb\nNot only that but I offered a trivially simple solution that works for the 1.8 users /and/ for us with no forking and a couple extra lines in the project's Gemfile - which just as I was writing that I learned isn't an option. \nMy head is quite cool and I agree that we all want the same thing. It just irks me. Also this is not an issue for me at the moment as I patched for it yesterday. I'm just imagining that quite a few other people are facing the same frustration and unnecessary time sink that I did. \n. @kylev the \u00ab ~> 1.5 \u00bb in Fog is just shorthand for \u00ab '>= 1.5', '< 1.6' \u00bb so they're on the same page as aws-sdk (otherwise I completely agree.. though given the response to my comments, I suspect that my support may be detrimental to your cause :)\n. Ah @kylev - took me a bit of reading and re-reading to realize you're right but I finally got it - thanks / always good to learn something new - especially when it's realizing that I \u00abknew\u00bb something that was wrong :)\nIn that case, I'm completely on board with you - with the understanding that the team will get to it.. though reiterating your (and my own) frustration at an abuse of the gemspec.\n\u00abaws-sdk is not dependent on this old nokogiri\u00bb - if I have a legacy system with other / external dependencies (like pre 1.9 Ruby) then maintaining that should be my responsibility.. which brings me back to this being obnoxious and punishing the ~ 88% of people who've already moved forward to accommodate the ~ 12% who haven't.\nTo the team - I'd be sympathetic if you actually had to rewrite code to support the newer nokogiri but since we're talking about a minor change to one line in your gemspec, it's obstinate to break things for people who are making effort to keep up with the times (which here means using a Ruby that isn't ~6 years old) just to accommodate people who for some reason can't (or won't) move forward (and who could easily add part of a line to impose their own cap on nokogiri to < 1.6 elsewhere).\nI don't mind libraries being opinionated, but that opinion shouldn't be \u00abkids move so fast these days ! Slow down and take your time getting to something released in 2007 - it's only 2013 and the old stuff was only finally deprecated this year !\u00bb\n. .. and maybe part of the reason why people have been so slow to move away from 1.8.7 is that libraries like this bend over backwards to keep them from feeling even a tiny bit of (easily remediable) pain.\n. [like / thanks ! ] :D\n. Is aws-sdk-core-ruby --pre (what looks like the initial 2.0) complete and safe / stable enough to reasonably use yet or is it more an alpha stage ?\n. Never mind - I've just tried it - looks like RC1 and so far it seems to work great (and without insisting that I downgrade anything :)\n. Awesome - though if I would have noticed that before, I would have whined a lot less here :D \n. ",
    "manuelmeurer": "@gtd Word.\n. @brianbianco https://github.com/aws/aws-sdk-ruby/issues/273#issuecomment-27928188\n. ",
    "kylev": "It looks like the Fog folks have switched their Nokogiri dependency to be just \"~> 1.5\".\nI fear the version lock dependency within the aws-sdk gemspec is actually an incorrect approach to maintaining compatibility. If aws-sdk wishes to maintain 1.8.x compatibility, it should do so with its code, not dependencies. Simple FAQ note that indicates 1.8 users should lock their Nokogiri version to \"~> 1.5.0\" will allow such users to easily use aws-sdk on Ruby 1.8 while unblocking those on 1.9 and 2.0 to move forward.\nGemspec version dependencies are about interfaces expected to be present for your library to work. They should not care if those interfaces are implemented using new or old hash syntax. Until this is resolved, you'll be leaving users of more current Ruby versions in a bind, unable to update other dependencies or get access to current bug fixes in aws-sdk.\nI am stuck on aws-sdk 1.11.1 because I already use Nokogiri 1.6, and the value of keeping aws-sdk is declining since I'm hitting bugs.\nAgain, removing the \"< 1.6\" Nokogiri requirement doesn't not make aws-sdk itself unfriendly to 1.8 users, but it does make it \"gem hostile\" to anyone running 1.9.\n. It's worth noting that according to NewRelic's recent \"state of the stack\", 1.8.7 only accounts for 12% of applications they surveyed. Of that 12%, I'm willing to wager that 90% of them already know to not upgrade to Nokogiri 1.6 or will immediately figure it out the second they try to use it and get a syntax error.\nThe version restriction isn't really protecting this minority. They're already working around it. It is, however, causing frustration for the other 88% of the community that wants or needs newer versions of aws-sdk, nokogiri, or anything that depends on them.\nIt's been 5 months, this thread is the top result for \"aws-sdk nokogiri\", and there's a pull request sitting there with both the fix and documentation updates. Let's move forward! :smile: \n. @smenor That's actually incorrect, and a common cognitive mistake I see in gemspecs. The \"twiddle waka\" or \"~>\" allows the last dotted element element specified to be greater than or equal. So \"~> 1.5\" is shorthand for \">= 1.5, < 2.0\".\nSee here and here.\nIn this case, \"~> 1.5\" means run any of the 1.x series, never the 2.0. If they had specified \"~> 1.5.2\" it would mean \"any of the 1.5 series above patch level 2, never 1.6\". Again, it allows the last specified \"dot level\" to go up.\n. :+1: \n. Wahoo! Thanks, team.\n. Since it took me a while too, and I ran across this with my search terms, here's a multi-filter example that should find instances that have tag Active (any value), has Stage set to production, and is in state running.\nThis uses the v2 SDK.\nruby\nf = [\n  { name: 'tag-key', values: ['Active'] },\n  { name: 'tag:Stage', values: ['production'] },\n  { name: 'instance-state-name', values: ['running'] }\n]\nec2_client.describe_instances(filters: f)\nHope that helps.\n. ",
    "mdimas": "This dependency on an old version of Nokogiri is preventing us from upgrading aws-sdk and moving forward. Your gemspec should represent the dependencies your project has on other gems, and aws-sdk is not dependent on this old nokogiri. The gemspec for your project should not represent dependencies some set of users might have with their environment. If they're using 1.8.7 they should already have a line locking them to a compatible version of nokogiri.\nAsking the large majority of your users to fork in order to remove the nonexistent dependency you've created is really not very user friendly response.\n. ",
    "brianbianco": "What is the status of this issue?  I've just run into this rather annoying restriction myself.  Certainly it seems it should be up to 1.8.X users to worry about gem compatibility themselves as they are using and EOL version of ruby.\n. @manuelmeurer Thanks.  Not sure how I managed to miss that.  Looks like the restriction will be removed on Nov 19th for anybody else who happens upon this thread.  You should of course read the blog post like I did :)\n. ",
    "grddev": "Not retrying for edge cases which have (not yet) been identified is infinitely better than wrongfully retrying (without a possibility to override) for unanticipated errors. In general, an exception should never be caught unless it is known to be safe to do so. Furthermore, if an error that should have been covered escapes, there is still a simple workaround for the api consumer (namely to retry him/herself). When mistakingly catching a JRuby OutOfMemoryError and retrying as if this was nothing, there is very little an api consumer can do.\n. Implementation of this should be fairly straightforward.\nA possible alternative design would be to wrap the errors in something like UserError and BeyondRepairError (obviously with less colloquial names) to guarantee that handle never lets any 'networking' errors escape. That would probably be more convenient inside the the sdk, while less convient for api users as the original errors would be more hidden.\nOne could also argue that not every application would need the BeyondRepairError, as repeated output of the same chunks might be okay in some situations. An alternative design to this would be to include something like the current chunk offset in the callback (and allow offsets to be repeated), so that the consumer could decide whether to raise an error or to continue.\n. I rebased the solution on master, and implemented the specified behavior\n. As should be evident from the lack of activity, I don't have any time to pursue this. I confirm that my contribution is made under the terms of the Apache 2.0 license, so feel free to either integrate this into the code base or not.\n. ",
    "bpot": "It doesn't look like the CurbHandler accepts any options right now. It also doesn't appear to handle settings like http timeout, ssl verify, etc.\n. Sounds great to me!\n. ",
    "islue": "ruby\n      def with_dimensions *dimensions\n        dimensions = @filters[:dimensions] || []\n        dimensions += dimensions.flatten\n        filter(:dimensions, dimensions)\n      end\nAccording to the source code of #with_dimensions, the input params are just overwritten and never used.\n. You are right. Thanks.\n. ",
    "ktzhu": "First pull request \u2013 so exciting! Thanks for merging!\n. ",
    "stevecj": "Ah -- I had thought that HTTP handlers were only used for insertion and not for replacement. I suppose, if you can insert, you can replace though.\n. ",
    "p7r": "It would appear that in fact, this is now impossible in v2 and you actively decided to not support such a feature. \nWe are having really poor performance throughput using Net::HTTP which Seahorse insists on, and can't see an easy way to get Typhoeus via Faraday plugged in. When building a micro-service architecture and wanting to publish SNS messages as fast as possible, this is kind of a blocker.\nIs there any update on this issue?\n. @trevorrowe - thank you, that's really, really helpful. We could not see that at all, it wasn't obvious.\nI think we might line up a typhoeus adapter and offer it up on pull request - might be worth documenting this up and providing examples with the SDK. I'll talk to the team next week.\n. ",
    "bshelton229": "I really like that approach. I'll mock that up. Thanks!\n. ",
    "kyledayton": "OK I adjusted the indentation.\n. The Amazon SES API docs say it should return a value, while the rdoc for this gem says it should return nil. It sends the email without issue, but it would be useful to have the generated message ID from the API response, for bounce/spam complaint tracking. \n. Would it be a significant change to add this functionality?\n. ",
    "breerly": "To be honest that implementation feels a little awkward and makes my tests hard to understand and implement. \nI ended up using SimpleMock to do something like this.\n``` ruby\nstack = SimpleMock.new AWS::CloudFormation::Stack\nstack.expect(:name, 'some_stack')\nstack.expect(:status, 'ROLLBACK_COMPLETE')\nstack.expect(:template, \"{'name':'some_template'}\")\nstacks = [stack, stack]\ncloud_formation = SimpleMock.new AWS::CloudFormation\ncloud_formation.expect(:stacks, stacks)\n```\nThe only thing I can't figure out is how to use this with Factory Girl so that I can take advantage of sequences, etc...\nping @trevorrowe \n. At the end of the day this SDK uses so much global (static) state that I ended up avoiding AWS::Core::Resource at all costs. Instead, I ended up defining a tiny stub class that can be used with FactoryGirl and SimpleMock.\n``` ruby\nFactoryGirl.define do\nclass StubAWSCloudFormationStack\n    attr_accessor :name, :status, :template\n  end\nfactory :cf_stack, class: StubAWSCloudFormationStack do\n    sequence(:name, 1) {|n| \"stack-#{n}\" }\n    status 'ROLLBACK_COMPLETE'\n    template \"{'version':'1000000000'}\"\n  end  \nend\n```\nNow, in my tests, I can do something like so.\nruby\ndescribe StackService do\n  before do\n    stacks = FactoryGirl.build_list(:cf_stack, 10)\n    cloud_formation = SimpleMock.new AWS::CloudFormation\n    cloud_formation.expects(:stacks, stacks)\n    @test_subject = StackService.new(cloud_formation)\n  end\nend\nI hope this helps anybody else who is trying to write testable code against this API.\n. ",
    "drewda": "Thank you for the thorough explanation, @lsegal.\nI'll look forward to seeing the revamped docs. And in the meantime, I'll just read the source instead of the generated docs.\n. ",
    "i-arindam": "You are actually right @lsegal . Must have had that dependency from somewhere else.\nThough I was getting the mentioned error.\nSo I removed capybara and bundle installed aws-sdk.\nAfter your mentioning, I re-added capybara to bundle with the version and it did complete without erroring.\nLittle weird. \nThanks anyways. \nClosing now.\n. ",
    "balexand": "I've been getting this error today while trying to install updates with bundle install. I can't explain why, since Capybara only requires nokogiri >= 1.3.3 as @lsegal mentions. \n```\nBundler could not find compatible versions for gem \"nokogiri\":\n  In Gemfile:\n    aws-sdk (= 1.15.0) ruby depends on\n      nokogiri (< 1.6.0) ruby\ncapybara (= 2.1.0) ruby depends on\n  nokogiri (1.6.0)\n\n```\nBut upgrading from bundler 1.3.5 to 1.4.0.rc.1 (latest as of today) and expliciting including nokogiri in my Gemfile (gem 'nokogiri', '1.5.10') has fixed the issue! Also it looks like this might be related: https://github.com/bundler/bundler/issues/2596\n. ",
    "rnhurt": "This error does not occur in v1.12.0 but does in v1.13.0 and v1.14.0.\n. I can confirm that this fixed my problem.  Thanks for the super fast turnaround!!\nLater...\n  Richard\n. I just ran into this problem while building a Lambda function to update TransitGateway EC2 Routes.  My route table has an \"S3 Endpoint\" which doesn't have a destination_cidr_block:\n...\n     #<struct Aws::EC2::Types::Route\n      destination_cidr_block=\"0.0.0.0/0\",\n      destination_ipv_6_cidr_block=nil,\n      destination_prefix_list_id=nil,\n      egress_only_internet_gateway_id=nil,\n      gateway_id=nil,\n      instance_id=nil,\n      instance_owner_id=nil,\n      nat_gateway_id=\"nat-11111111111111111\",\n      transit_gateway_id=nil,\n      network_interface_id=nil,\n      origin=\"CreateRoute\",\n      state=\"active\",\n      vpc_peering_connection_id=nil>,\n     #<struct Aws::EC2::Types::Route\n      destination_cidr_block=nil,\n      destination_ipv_6_cidr_block=nil,\n      destination_prefix_list_id=\"pl-11111111\",\n      egress_only_internet_gateway_id=nil,\n      gateway_id=\"vpce-00000000000000\",\n      instance_id=nil,\n      instance_owner_id=nil,\n      nat_gateway_id=nil,\n      transit_gateway_id=nil,\n      network_interface_id=nil,\n      origin=\"CreateRoute\",\n      state=\"active\",\n      vpc_peering_connection_id=nil>],\n  ...\nEverytime I try to act on any routes in that table I get an ArgumentError: missing required option :destination_cidr_block error, no matter if I'm trying to touch that endpoint route or not.  At this point I'm completely stopped by this problem.  I don't know of any way of add/deleting routes from a route table with an S3 endpoint in it (or any other \"endpoint\").\nMy code is pretty simple and explodes on the \"table.routes\" before it even outputs the first route:\nec2 = Aws::EC2::Resource.new(region: 'us-east-1')\ntable = ec2.route_table('rtb-1111111111111111111')\ntable.routes\nArgumentError: missing required option :destination_cidr_block\nfrom /Users/my_name/src/my_project/vendor/bundle/ruby/2.5.0/gems/aws-sdk-ec2-1.72.0/lib/aws-sdk-ec2/route.rb:360:in `extract_destination_cidr_block'. Alex, I updated my code to use the Client API and it worked great.\nThanx!\nRichard. ",
    "sorah": "Confirmed at 2.0.0-p247, 1.9.3-p448\n. - AWS::SDK version: TBD, I know this was able to reproduce on HEAD as of 7/25/2013\n- rubygems: 2.0.3 (as of 7/25/2013)\n- OS: OS X 10.8.5\nBecause I've cloned nokogiri before and its on my $RUBYLIB env variable.\n(This is my fault, complied binary in my working copy was broken; But aws-sdk shouldn't ignore LoadError silently to notice something going wrong)\nSo this can't be reproduced with bundle exec. (bundle modifies RUBYLIB to load installed gem more prior)\n. For the background for this patch, we head this problem with File::TMPFILE.. \n\nMay I ask why the md5 spec tests need to be removed?\n\n@cjyclaire Testing \"without loading them into memory\" is impossible, and method calling was changed by this patch. But, hmmm... After I removed the test case, suite is missing tests for File and Tempfile objects. I'll add both later.\n\nI'm curious, why is the IO object being duplicated? Wouldn't the behaviour be the same if we read the original IO object and rewind it afterwards?\n\nOops, yes, this isn't necessary because dup-ed IO still shares their position with an original IO. Will fix later\n. Disregard my previous reply about dup-ing IO. this is required to set binmode on with leaving  binmode of original IO as is.. Rebased to the latest master, and added test cases for File, and Tempfile.. > checksum with files loading into memory,\nIs that possible? This patch takes a same approach with what Digest::*#file method doing.. :relaxed:\nHaven't measured but I believe this patch improves memory consumption for passing IO objects, not only Files.. what's happening on the diff?. Ping?. Note: #1516 contains a same change. > We'll take a look at this, but the recommended usage is to simply require the service gems you need directly.\nI agree with this, but I was not sure this recommendation is correct.\nWe had several scripts remain that requires aws-sdk directly. After we notice this issue, we migrated such scripts to load the service gems directly.\nBut as you're saying, causing slow load pain with require \"aws-sdk\"  is not intentional behavior (I understand you're agreeing with this).\n\nThat method does allow you to load a specific gem version, but it needs to be specified in code.\n\nNo. Kernel#gem is to activate gems, and optionally takes requirements. https://docs.ruby-lang.org/en/2.5.0/Kernel.html#method-i-gem\nVersion constraints are not required. \nThe problem I've explained earlier is: Activating aws-sdk gem causes unresolved dependencies (unresolved_deps) to the service gems . But they remains unresolved because aws-sdk-resources.rb just registers service gems for autoloading.\nRubygems may try to find a file from unresolved_deps (with trying all possible gem version combinations) when requiring new files. require \"aws-sdk\" with this situation triggers that code path. (Actually, it happens when requiring standard libraries such as cgi, in inside of aws-sdk-core.rb)\nThe code path becomes slow when an environment has many aws-* gem versions.\nThe solution is to resolve gems in unresolved_deps. One is to require them (which activates a gem inside require method), and alternate one is to just activate them using Kernel#gem method.\n\nOne more thing: As I explained require \"aws-sdk\" leds many unresolved_deps unresolved, any requiring standard libraries after require \"aws-sdk\" could be also slow.\n```\n$ irb -raws-sdk\n(slow)\n\n\nrequire 'cgi'\n(slow!)\n=> false\nGem::Specification.unresolved_deps\n=> [... so many  aws-* gems ...]\n``. I love writingrequire \"aws-sdk\"` in my irbrc. That is one of my (small) use case.. \n\n",
    "mfischer-zd": "How should a well-written client recover from a TruncatedBodyError exception that occurs during a multipart fetch of an encrypted file?  I'm currently trying to retrieve a 100GB+ file and I can't seem to retrieve the whole thing without getting one.\nMy current code:\nobj.read(:encryption_key => ENV['ENCRYPTION_KEY']) do |buf|\n  remaining_length = buf.length\n  until remaining_length == 0\n    remaining_length -= file.write(buf)\n  end\nend\n. Thanks Trevor.   For the purpose of this exercise, I using the same key generation method as you:\ns3obj.write(Pathname.new(path), :encryption_key => OpenSSL::Cipher.new(\"AES-256-ECB\").random_key)\nHere's the debugging output you requested (identifying details redacted):\n```\nI, [2013-11-20T20:50:37.697816 #23871]  INFO -- : [AWS S3 200 0.437816 0 retries] initiate_multipart_upload(:bucket_name=>#,:data=>#, @stream_size=202260596110, @orig_cipher=#, @cipher=#, @eof=false, @final=nil>,:key=>#,:metadata=>{\"x-amz-iv\"=>#,\"x-amz-key\"=>#,\"x-amz-matdesc\"=>\"{}\",\"x-amz-unencrypted-content-length\"=>202260596110})  \nI, [2013-11-20T20:50:42.560903 #23871]  INFO -- : [AWS S3 200 4.771689 0 retries] upload_part(:bucket_name=>#,:data=>#\\xB0\\r\\xFF\\xBB\\xDAI\" ... (20226060 bytes)>,:key=>#,:part_number=>1,:upload_id=>#)  \nI, [2013-11-20T20:50:42.759908 #23871]  INFO -- : [AWS S3 200 0.197626 0 retries] upload_part(:bucket_name=>#,:data=>\"![!\\xE1\\xFB\",:key=>#,:part_number=>2,:upload_id=>#)  \nI, [2013-11-20T20:50:43.275303 #23871]  INFO -- : [AWS S3 200 0.514918 0 retries] complete_multipart_upload(:bucket_name=>#,:key=>#,:parts=>[{:etag=>#,:part_number=>1},{:etag=>#,:part_number=>2}],:upload_id=>#)\n```\n. I think you misread; I said 200GB.\nWith respect to your single-request inquiry, that's not working either:\nI, [2013-11-20T21:50:37.116303 #25353]  INFO -- : [AWS S3 200 8.246672 3 retries] put_object(:bucket_name=>#<String \"XXX\" ... (18 bytes)>,:content_length=>202260596112,:data=>#<AWS::S3::CipherIO:0x0000000273d708 @stream=#<AWS::Core::ManagedFile:XXX>, @stream_size=202260596110, @orig_cipher=#<OpenSSL::Cipher:0x0000000273d578>, @cipher=#<OpenSSL::Cipher:0x00000002c01578>, @eof=false, @final=nil>,:key=>#<String \"2-1/201311\" ... (33 bytes)>,:metadata=>{\"x-amz-iv\"=>#<String \"wiLT6CzIMu\" ... (24 bytes)>,\"x-amz-key\"=>#<String \"1q0ZG6aDTW\" ... (64 bytes)>,\"x-amz-matdesc\"=>\"{}\",\"x-amz-unencrypted-content-length\"=>202260596110}) Errno::ECONNRESET Connection reset by peer\nI can't fathom why S3 is sending us an RST packet mid-upload, but tcpdump confirms it.\n. Interestingly, if I create a 200MB empty file, or even a 200GB empty file via dd if=/dev/zero bs=1M, I can upload the file successfully with both encryption and multipart enabled.\nSo there appears to be something about the content of the files themselves that is causing the problem.  They're gzip-compressed tar files containing MySQL backups, if that's relevant.\n. The workaround is effective, but only if I specify it in the s3obj.write call.  It does not appear to be effective if I specify it in AWS.config.\nThanks for the quick diagnosis and workaround!\n. +1 - having the APIs and CLI behave identically with respect to default configuration would reduce a lot of confusion and surprise.\n. Actually, it looks like the Ruby SDK doesn't support the modern CLI config file format at all ($HOME/.aws/config, as opposed to the legacy $HOME/.aws/credentials file).  See #1045\n. ",
    "geota": "AWS::DataPipeline.create_pipeline returns a Core::Response object. Call the data method on the response object to access the response as a hash. \n``` ruby\ndp = AWS::DataPipeline.new\npipeline_name = \"pipline_test\"\npipeline = dp.client.create_pipeline({\n  :name => pipeline_name,\n  :unique_id => pipeline_name\n  })\ndefinition = dp.client.get_pipeline_definition(pipeline.data)\n```\n. ",
    "kinman-enphase": "Thank you. I was misled by the return output. Documentation clearly states it's a Core::Response.\n. ",
    "schmich": "Perfect, thanks a bunch, Alex. The usage is much clearer with your updates.\n. ",
    "kbullaughey": "I was hoping it would be easy. Thanks for locating the relevant line. I'm not set up with a forked copy for submitting a pull request. Would you be able to make those changes?\n. Great. Thanks for the fix.\n. ",
    "takeshinoda": "I understood :smiley: . Thank you. \n. ",
    "PeterBengtson": "Any idea when the higher abstractions will use the newer 2012 client in their entirety? It would be a most welcome thing.\n. Hi Loren,\nThanks for the reply. I'd like nothing more than to contribute to AWS-SDK for Ruby; however, as I'm currently involved in writing a DynamoDB drop-in replacement for ActiveRecord, and nothing out there seems to use secondary indices due to the lack of higher-level abstractions in the AWS-SDK Ruby gem, I'd rather jump onto the 2.0 bandwagon instead. Any indications on when you expect to release something that would allow us to get going?\nThe gem is called ocean-dynamo and is part of ocean-rails, an open source project letting people set up Rails SOA:s in the Amazon cloud. (http://wiki.oceanframework.net) \n/ Peter Bengtson\n  System Architect, Odigeo\n10 sep 2013 kl. 03:51 skrev Loren Segal notifications@github.com:\n\n@PeterBengtson due to our work on V2.0 of the AWS SDK for Ruby, we have no plans to put out new high level abstractions for this version of the SDK. In addition to the new version, we also want to make some significant improvements to the high level APIs to make them more idiomatic and easy to use. If you are interested, we would certainly be happy to look at a pull request that ports over the high level functionality of the DynamoDB client to the new API version.\n\u2014\nReply to this email directly or view it on GitHub.\n. \n",
    "Mrjaco12": "Well it looks like these two similarly named fields are actually doing different things. I can close this if that is a correct assessment.\n. ",
    "goleador": "AWS.config({\n                            :access_key_id => '...',\n                            :secret_access_key => '...',\n                        })\nsns=AWS::SNS.new\nc = sns.client\nc.list_topics\n. Ran the command. It says it's unable to find my credentials but I do have aws-sdk.rb file with a call to configure it.\n. Actually I think that's because the command doesn't run the aws-sdk.rb. Seeing this now:\nWARNING: Nokogiri was built against LibXML version 2.9.1, but has dynamically loaded 2.7.8\nJust above the same error I had before.\n. Got rid of the warning but the error remains (note aws-sdk-1.15.0):\nAWS::SNS::Errors::SignatureDoesNotMatch: The request signature we calculated does not match the signature you provided. Check your AWS Secret Access Key and signing method. Consult the service documentation for details.\nThe Canonical String for this request should have been\n....\nThe String-to-Sign should have been\nfrom /Users/rodrigo/.rvm/gems/ruby-1.9.2-p320/gems/aws-sdk-1.15.0/lib/aws/core/client.rb:366:in `return_or_raise'\nfrom /Users/rodrigo/.rvm/gems/ruby-1.9.2-p320/gems/aws-sdk-1.15.0/lib/aws/core/client.rb:467:in `client_request'\nfrom (eval):3:in `list_topics'\nfrom (irb):1\nfrom /Users/rodrigo/.rvm/gems/ruby-1.9.2-p320/gems/railties-3.2.14/lib/rails/commands/console.rb:47:in `start'\nfrom /Users/rodrigo/.rvm/gems/ruby-1.9.2-p320/gems/railties-3.2.14/lib/rails/commands/console.rb:8:in `start'\nfrom /Users/rodrigo/.rvm/gems/ruby-1.9.2-p320/gems/railties-3.2.14/lib/rails/commands.rb:41:in `<top (required)>'\nfrom script/rails:6:in `require'\nfrom script/rails:6:in `<main>'\n. Same error. I just tried S3 and same thing happens:\nAWS.s3.buckets.each { |bucket| bucket }\nAWS::S3::Errors::SignatureDoesNotMatch: The request signature we calculated does not match the signature you provided. Check your key and signing method.\n    from /Users/rodrigo/.rvm/gems/ruby-1.9.2-p320/gems/aws-sdk-1.15.0/lib/aws/core/client.rb:366:in return_or_raise'\n    from /Users/rodrigo/.rvm/gems/ruby-1.9.2-p320/gems/aws-sdk-1.15.0/lib/aws/core/client.rb:467:inclient_request'\n    from (eval):3:in list_buckets'\n    from /Users/rodrigo/.rvm/gems/ruby-1.9.2-p320/gems/aws-sdk-1.15.0/lib/aws/s3/bucket_collection.rb:144:ineach'\n    from (irb):7\n    from /Users/rodrigo/.rvm/gems/ruby-1.9.2-p320/gems/railties-3.2.14/lib/rails/commands/console.rb:47:in start'\n    from /Users/rodrigo/.rvm/gems/ruby-1.9.2-p320/gems/railties-3.2.14/lib/rails/commands/console.rb:8:instart'\n    from /Users/rodrigo/.rvm/gems/ruby-1.9.2-p320/gems/railties-3.2.14/lib/rails/commands.rb:41:in <top (required)>'\n    from script/rails:6:inrequire'\n    from script/rails:6:in `'\n. Do I need to specify the region?\n. I added the region too ... didn't help.\n. I am an idiot! Mixed dev and prod keys. My sincere apologies, thank you for helping out!\n. I get the message on the phone but it's just the JSON. I expected it to play a sound and to display just the message not the json.\n{\n \"APNS_SANDBOX\":\"{\\\"aps\\\":{\\\"alert\\\":\\\"\\\"}}\"\n}\nBasically it displays the whole text above when I expected \n. I am trying this now still does not work:\nAWS.sns.client.publish :subject => 'test', :message => { :APNS_SANDBOX => {:aps => { :alert => params[:message]}}.to_json }.to_json, :target_arn => target\n. It works if sending from AWS Console\n. NICE! It worked. Thank you again @lsegal .\n. Ok. That made it not crash but the device doesn't get the message when the target is a topic that it is subscribed to. It does get a message when I send a message directly to the device's end point.\nAm I missing something fundamental here? I thought if I send a message a topic all devices (endpoints) subscribed to that topic would receive it.\n. @lsegal That was directly to a specific device not a topic.\n@batianusrey That was the issue thank you.\n. Thanks for your help @lsegal. Hope the forums can be as responsive as you. Unfortunately, if I don't get this issue resolved by tomorrow I'll have to go with a different service like Parse or UrbanAirship. Thanks again.\n. @lsegal I got it to work using the AWS console. I just don't know how to make this work using the SDK.\n. I finally got it:\napns_json = {\n          :aps => {\n              :alert => self.subject,\n              :sound => 'default'\n          },\n```\n      :payload => {\n          :notification_type => 'new_workout',\n          :workout_id => self.workout_id\n      }\n  }.to_json\nmessage = {\n      :default => self.subject,\n      :APNS => apns_json,\n      :APNS_SANDBOX => apns_json\n  }.to_json\nAWS.sns.client.publish :subject => self.subject,\n                         :message => message,\n                         :topic_arn => target_arn,\n                         :message_structure => 'json'\n```\n. Thank you!\n. ",
    "sunnycmf": "@isegal thanks you solve my problem! you rocks guy!\n. ",
    "ingscjoshua": "Hi i'm try to send  notifications whit sound and badge but only recibe default text \n\nmessage = { :default => \"Credito aprobado\", :APNS_SANDBOX => {:aps => { :alert =>'Test',:badge =>'1',:sound =>'default'}}}.to_json\n        AWS.sns.client.publish :message => message, :target_arn => topic_arn, :message_structure => 'json'\n\nIm not understand what is the problem? \nany ideas? \nRegards\n. you have one example for use IOS  im try copy format to console SNS but not Work \n. ",
    "rajuptb": "I use the following in PHP\n$title = 'My Test Message';\n$sound = 'doorbell.caf';\n$msgpayload=json_encode(array('aps' => array('alert' => $title,'sound' => $sound,)));\n$response = $sns->publish(array(\n    'TopicArn' => $TopicArn,\n    'MessageStructure' => 'json',\n    'Message' => json_encode(array(\n        'default' => $title,\n        'APNS_SANDBOX' => $msgpayload\n    ))\n));\n. ",
    "thebucknerlife": "@ingscjoshua did you resolve this issue? I'm having the same problem.\n. For anyone else who arrives here, the issue is very similar to the issue discussed in the aws-sdk-js library above. You have to encode your json twice (sigh). Working code in ruby for aws-sdk:\n``` ruby\nclient = AWS::SNS::Client.new\napns_payload = { \"aps\" => { \"alert\" => \"hey it worked!\", \"badge\" => 14 } }.to_json\nmessage = { \"default\" => \"this is the default\", \"APNS\" => apns_payload }.to_json\nclient.publish( message: message, target_arn: your_endpoint_arn, message_structure: 'json' )\n```\n. ",
    "jmstone617": "This is also incorrect (or at least has changed as of today)\nThe following works for me and successfully plays a sound:\napns_payload = { \"aps\" => { \"alert\" => @message, \"sound\" => 'default' } }.to_json\n message = { \"default\" => @message, \"APNS_SANDBOX\" => apns_payload }.to_json\n sns.publish(target_arn: device.endpoint, message: message, message_structure: 'json')\n. Failure/Error: post :create, user_id: @user.id, token: '28ab392bb32'\n NoMethodError:\n   undefined method `lines' for nil:NilClass\n # (eval):3:in `create_platform_endpoint'\n # ./app/services/register_device_token_with_push_service.rb:13:in `register'\n # ./app/controllers/api/v1/devices_controller.rb:26:in `create'\n # ./spec/controllers/api/v1/devices_controller_spec.rb:35:in `block (5 levels) in <top (required)>'\n # ./spec/controllers/api/v1/devices_controller_spec.rb:34:in `block (4 levels) in <top (required)>'\n # ./spec/spec_helper.rb:123:in `block (3 levels) in <top (required)>'\n # ./spec/spec_helper.rb:122:in `block (2 levels) in <top (required)>'\nThis is the 'register' method invocation in line 2 of the stack trace:\nresponse = sns.create_platform_endpoint(platform_application_arn: [REDACTED], token: @token)\n. That\u2019s the whole log \u2014 it\u2019s something inside the create_platform_endpoint call. Is something making invoking a method called :lines that is supposed to be on an object passed in when instantiating the client? Again, I am just passing in the access key and secret when creating the client.\n\u2014\nSent from Mailbox\nOn Tue, Oct 7, 2014 at 12:34 PM, Trevor Rowe notifications@github.com\nwrote:\n\nI don't see a stack trace from the SDK. I would have expected lines in the stack trace with the Ruby SDK gem name.  Is this log truncated?\nReply to this email directly or view it on GitHub:\nhttps://github.com/aws/aws-sdk-ruby/issues/639#issuecomment-58235949\n. Or something I should pass into create_platform_endpoint other than the application_arn and a token?\n. The problem was the test environment did not have the access keys set. That said, there should probably be a failure with a relevant exception message when trying to instantiate the AWS client, as opposed to crashing due to a no method on a nil class when trying to call a method on variables you assume have been set.\n. \n",
    "anoojr": "Suddenly started having SNS turn endpoints 'Enabled' to 'false' all by itself. Also I tried bulding the json by hand and still not working. Here is what I tried. Any help is appreciated. Thanks\nmessage = {\"default\" => 'Test text', \"APNS_SANDBOX\" => {\"aps\" => {\"alert\" => \"Test text\", 'badge' => 1, 'sound' => 'default'}}}.to_json\nclient.publish(target_arn: t_arn, message: message, message_structure: 'json')\ngives me a message_id and a request_metadata with request_id as subkey. \nIs there a place to look at the logs for these requests. At least on sandbox?\n. ",
    "Sailias": "@jmstone617, your solution works for me.  \nDouble to_json, and sound: 'default' seems to do it.\n. ",
    "jayd3e": "I'm use the AWS Java SDK, and this thread really helped me.  Apparently all SNS SDKs require the platform-specific payload to be a string and not a nested object.  Encoding my JSON twice fixed the issue.. ",
    "ColinHebert": "@awood45 Is it worth reopening this issue now?\n. Is it possible to reconsider the option of loading the region from the aws cli configuration?\n. ",
    "ahawkins": "Sure. What is wire tracing?\nOn 6 sep 2013, at 19:41, Alex Wood notifications@github.com wrote:\n\nHi Adam,\nHave you had a chance to test your script with wire tracing? Feel free to check back with us here if the wire traces bring up any additional questions.\n\u2014\nReply to this email directly or view it on GitHub.\n. K will do. \n\nOn 7 sep 2013, at 18:43, Alex Wood notifications@github.com wrote:\n\nThe -v flag on aws-rb will allow us to see the raw requests and responses being sent over the wire. Seeing this can help reveal the exact cause of your issues.\n\u2014\nReply to this email directly or view it on GitHub.\n. \n",
    "leelynne": "Yes. Before the 1.16 release the new parameter wasn't accepted to even make\nthe request. With 1.16 the parameter is accepted by but rejected by the\nservice.\nI didn't see the EC2 version API change with 1.16 release did it?\nQuick check, are you using the 1.16.0 release? I'm checking in case I made\na mistake on the release.\n\u2014\nReply to this email directly or view it on\nGitHubhttps://github.com/aws/aws-sdk-ruby/issues/344#issuecomment-23597270\n.\n. Specifically the LoadBalancer.instances returns a collection of EC2 instances.  I want to (and it feels natural) be able to filter those instances based on tags - e.g. lb.instances.tagged_values('awesome_instances').  I'll send you a CR.\n. Then please leave this open so I can submit a pull request on it.\n. Closing. Looked at old docs.\n. ",
    "batianusrey": "@goleador, I see that you are sending a message to a topic but you are passing it's ARN it to the target_arn: hash key instead of the topic_arn: hash key. Haven't tried it but that my be the reason.\n. ",
    "nenetto": "I leave the doc info here for future visits: \n\nbe a syntactically valid JSON object; and\ncontain at least a top-level JSON key of \"default\" with a value that is a string.\nYou can define other top-level keys that define the message you want to send to a specific transport protocol (e.g., \"http\").\n. \n",
    "mikeys": "@lsegal  Thanks for the answer, I was wondering though, If I'm using the DynamoDB::Client directly, is it possible to use a different AWS configuration than the global one (not AWS.config)?\nI'm asking this since I'm building a dynamo db related gem which once included in a project, SHOULD NOT alter the existing AWS.config in case the SDK is already used in the project.\nCurrently, the following applies:\n``` ruby\nCreating a new config object and passing it to the constructor DOESN'T affect the client version.\n\n\nconfig = AWS.config.with(dynamo_db: { api_version: '2012-08-10'  })\n=> \ndynamo_db = AWS::DynamoDB::Client.new(config: config)\n=> #\n\n\nAltering the global AWS config object and instantiating DOES affect the client version.\n\n\nAWS.config(dynamo_db: { api_version: '2012-08-10'  })\n=> \ndynamo_db = AWS::DynamoDB::Client.new\n=> #\n```\n\n\nThe above is probably caused by the fact that AWS::Core::Client#configured_version is defined the following way:\nruby\ndef configured_version\n  svc_opt = AWS::SERVICES[name.split('::')[1]].method_name\n  AWS.config.send(svc_opt)[:api_version]\nend\nInstead of something like this:\nruby\ndef configured_version(options = {})\n  svc_opt = AWS::SERVICES[name.split('::')[1]].method_name\n  config = options[:config] || AWS.config\n  config.send(svc_opt)[:api_version]\nend\n. @trevorrowe I'll be more than happy to assist in any way I can, from feedback and bug mashing to support features implementation. Thanks for the info.\n. To whoever it may concern:\nI think that the issue may be a bit different than what has been described and is a result of wrong thread handling:\nIn https://github.com/aws/aws-sdk-ruby/blob/aws-sdk-v1/lib/aws/core/credential_providers.rb#L48:\nruby\n        def set?\n          @cache_mutex ||= Mutex.new\n          unless @cached_credentials\n            @cache_mutex.synchronize do\n              @cached_credentials ||= get_credentials\n            end\n          end\n          !!(@cached_credentials[:access_key_id] &&\n            @cached_credentials[:secret_access_key])\n        end\nUsing EC2Provider - Every thread which tries to fetch credentials, basically resets @cached_credentials if near_expiration? is true:\nruby\n        # Clears out cached/memoized credentials.  Causes the provider\n        # to refetch credentials from the source.\n        # @return [nil]\n        def refresh\n          @cached_credentials = nil\n        end\nThat means that moderate amount of threads which call the credentials method concurrently near the credentials expiration time, will cause get_credentials to get evaluated a lot of times (since each one clears the cache of the current thread in the \"synchronize\" block) even though it's \"one-at-a-time\" which will cause bombarding of the meta-data endpoint, and I assume this is why the response takes more than a second.. ",
    "aceofspades": "I did, but later realized AWS won't allow removal of the primary address, which is what I wanted to do when shutting down instances (to manage IP reservations).\nThanks...\n. ",
    "jpablobr": "I'm using 1.8.0, but also tried with 1.17.0 and had the same issue.\n. This is also a 'rails', '~> 3.2.1' app.\n. That was it, thanks! I was just following the CORSRuleCollection#set specs and those params didn't seem to be required, neither in the fog tests,  but they always pass them.\n. ",
    "assembler": "thanks\n. Wow, thanks for the update! Glad you have figured out where the leak is. I've tested this against ruby 2.1.5, and there are no leaks. On ruby 2.2.0 and 2.2.1 there is severe leaking. That explains why the memory went wild after upgrading from 2.1.5 (which suffered its own memory issues) to 2.2.0 (which suffered even more :)\nI've changed this line:\nhttps://github.com/aws/aws-sdk-ruby/blob/v2.0.39/aws-sdk-core/lib/seahorse/client/http/response.rb#L60\nInto this:\nruby\n@body = StringIO.new(body_contents + chunk)\nAnd the leaking went away. I know that this is stupid and that its ruby responsibility to provide proper fix, but it will be probably months before ruby is fixed. Do you think I can safely monkeypatch locally aws-sdk until then? Do you see any problems with the patch above? I have searched through codebase to find a place where custom :body IO is passed to response, but I don't think there is.\nThank you so much for your help!\n. Awesome. So I can just put this in my code and it will all work without memory leaks:\n``` ruby\nmodule Seahorse\n  class StringIO\n    def initialize(data = '')\n      @data = data\n      @offset = 0\n    end\ndef write(data)\n  @data << data\n  data.bytesize\nend\n\ndef read(bytes = nil, output_buffer = nil)\n  if bytes\n    data = partial_read(bytes)\n  else\n    data = full_read\n  end\n  output_buffer ? output_buffer.replace(data || '') : data\nend\n\ndef rewind\n  @offset = 0\nend\n\ndef truncate(bytes)\n  @data = @data[0,bytes]\n  bytes\nend\n\nprivate\n\ndef partial_read(bytes)\n  if @offset >= @data.bytesize\n    nil\n  else\n    data = @data[@offset,@offset+bytes]\n    bump_offset(bytes)\n    data\n  end\nend\n\ndef full_read\n  data = @offset == 0 ? @data : @data[@offset,-1]\n  @offset = @data.bytesize\n  data\nend\n\ndef bump_offset(bytes)\n  @offset = [@data.bytesize, @offset + bytes].min\nend\n\nend\nend\n```\nOnce the patch is applied to ruby, I can safely remove this. Thank you for your help!\n. I've created a gem that can be included in project and which solves memory issues with aws until ruby 2.2.3 is released: https://rubygems.org/gems/aws_memfix\n. In the process of developing a gem, I've tried with SimpleDelegator implementation of StringIO. It turned out that all it takes to avoid memory leaks is this:\nruby\nrequire \"delegate\"\nmodule Seahorse\n  class StringIO < SimpleDelegator\n    def initialize(data = '')\n      @io = ::StringIO.new(data)\n      super(@io)\n    end\n  end\nend\nHave no clue why, but it works..\n. Thanks for your help. Yes, we're using the multithreaded environment (thread/pool). We'll try with eager_autoload and see how it goes. Thanks!\n. ",
    "scaryguy": "I was using 1.11.1 with Paperclip 3.5.1 and I didn't change it.\n. Hi,\nhere is my development logs:\n``` console\n[paperclip] saving articles_images/15/original/landscape-rainbow_1842437i.jpg\n[AWS S3 200 88.183825 3 retries] put_object(:acl=>:public_read,:bucket_name=>\"BUCKET_NAME\",:content_length=>63397,:content_type=>\"image/jpeg\",:data=>Paperclip::UploadedFileAdapter: landscape-rainbow_1842437i.jpg,:key=>\"articles_images/15/original/landscape-rainbow_1842437i.jpg\") Net::OpenTimeout execution expired\n(1.0ms)  ROLLBACK\nCompleted 500 Internal Server Error in 90941ms\nNet::OpenTimeout (execution expired):\n  app/controllers/admin/articles_controller.rb:70:in `create'\nRendered /home/scaryguy/.rvm/gems/ruby-2.0.0-p247@enews/gems/actionpack-4.0.0/lib/action_dispatch/middleware/templates/rescues/_source.erb (0.9ms)\n  Rendered /home/scaryguy/.rvm/gems/ruby-2.0.0-p247@enews/gems/actionpack-4.0.0/lib/action_dispatch/middleware/templates/rescues/_trace.erb (1.8ms)\n  Rendered /home/scaryguy/.rvm/gems/ruby-2.0.0-p247@enews/gems/actionpack-4.0.0/lib/action_dispatch/middleware/templates/rescues/_request_and_response.erb (56.3ms)\n  Rendered /home/scaryguy/.rvm/gems/ruby-2.0.0-p247@enews/gems/actionpack-4.0.0/lib/action_dispatch/middleware/templates/rescues/diagnostics.erb within rescues/layout (76.5ms)\n```\nStrange thing is it works in production/Herkoku. But doesn't work in development environment.\n. Not really. Does Ubuntu come with a built in and enabled firewall? \nI'm trying to update aws-sdk to latest version. I'll inform you.\nBtw, could this be related to https://github.com/thoughtbot/paperclip/issues/751 ?\n. Okay, thank you @lsegal .\n. ",
    "rbroemeling": "/me sighs.  I see.  Gemfile had '~> 1.8.0' in it.  Maybe the blog should include a minimum version number for the behaviour, as it is one of the top Google hits.  I'm not sure that v1.8.5 (released 13-March-2013) constitutes \"very old\".\nNevertheless, thanks for the help!\n. I wouldn't care so much, except that it fails completely silently -- there is no sign that the region parameter has had no effect whatsoever, until you double-check things in the AWS control panel or until you notice that all of your script-created/managed resources are muddled together instead of divided by region.  There's no error or other sign that the region parameter is being utterly ignored.\nThanks for the additional links; and I'll update the Gemfile lock as you suggest.  It is difficult to tell how each gem should be locked on a case-by-case basis, since handling of version numbers is arbitrary across different gems.\n. ",
    "rhossi": "As a suggestion to your Path 2, can't we just add dependency to active_model in gemspec and always require active_model in naming.rb? I checked Mongoid and they handle it this way.\nAnyway, meanwhile I'll work as per you suggested putting active_model in the gemfile.\nThanks\n. ",
    "mtparet": "Thanks a lot to have updated the documentation.\nHowever I still think this behavior add not necessary complexity. (dealing with two kinds of object returned from a same method/parameters)\n. Ok I will go to the forum but the aws-sdk should not failed silently. It is the responsibility of the aws-sdk to ensure all actions performed by itself are successfully performed. (and to raise an error if necessary)\n. Thanks @lsegal, I will let you know if I have answers.\n. There is indeed a bug, you're comparing dns name with an additional dot character:\nhttps://github.com/aws/aws-sdk-ruby/blob/master/lib/aws/route_53/resource_record_set.rb#L158\n``` ruby\n\ndetails[:name]\n\"toto-matthieu-test.*.\"\nname\n\"toto-matthieu-test.*\"\n```\n. It seems good to have the point at the end of the record. The fact that the sdk enable us to create dns records without the point at the end has led me to the wrong direction.\n. \n",
    "dfried": "Ah you're right. It's a shame the difference between govcloud and regular API endpoints is irregular. \nHow does that look?\n. ",
    "linktoming": "When double click then copy the secret key in Sublime Text, It turned out to only copied half of the secret key before the character \"/\"\n. ",
    "jessethegame": "Haha, thanks\n. ",
    "aleak": "Makes sense, I wasn't sure where the underlying API was defined.\nI'll pop over to the forums and create a post, thanks.\n. ",
    "arkadiyt": "You should use AWS.eager_autoload! instead, or upgrade to ruby 2:\nhttps://ruby.awsblog.com/blog/tag/threads\n. ",
    "grosser": "Thx for the pointers! :D\nOn Tue, Oct 8, 2013 at 11:15 AM, Arkadiy Tetelman\nnotifications@github.comwrote:\n\nYou should use AWS.eager_autoload! instead, or upgrade to ruby 2:\nhttps://ruby.awsblog.com/blog/tag/threads\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/aws/aws-sdk-ruby/issues/380#issuecomment-25914239\n.\n. any timeframe for this 2.0 ?\nhow about a minor version bump that just drops the dependency ?\n. :heart: for coming back and actually fixing it :)\n. @trevorrowe could you comment on mergability / what is missing ?\n. I'd suggest a credentials setting itself new_fancyness=true so everything\nstays inside that file\n\nOn Thu, Mar 31, 2016 at 9:55 AM, Alex Wood notifications@github.com wrote:\n\nWe're closing in on how we want to approach this. One change will be that\nsome of these changes may be gated by an environment variable - anything\nthat would risk possible backwards incompatibility.\nWill be keeping this open in the meantime, the intention is to merge this\nor to otherwise implement this.\n\u2014\nYou are receiving this because you commented.\nReply to this email directly or view it on GitHub\nhttps://github.com/aws/aws-sdk-ruby/pull/1092#issuecomment-204019878\n. no, from doing a create_load_balancer with :connection_draining set\n. ahhh ... I was using aws elb create-load-balancer with --cli-input-json but cli-input-json was not supported in the ruby wrapper, so I was thinking it was possible to do it directly ... \n\nlet's close this, I understand that this is not easy/possible ... but maybe supporting cli-input-json is ... not sure ... thx for the reply!\n. I can, but I'd prefer not to pull it in, to not have an additional gem /\nnot make other apis available and keep the app consistent.\nI was assuming the design philosophy is to have everything in core and\nresources is just a fancy wrapper.\nOn Tue, Mar 1, 2016 at 12:53 PM, Alex Wood notifications@github.com wrote:\n\nIn general, higher level abstractions including Presigning and Object\nUpload were put into the resources gem. Is there a particular reason you\ndon't want to or can't pull in the resources gem?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/aws/aws-sdk-ruby/issues/1113#issuecomment-190898712.\n. I was thinking, if there is nothing to delete, then ideally the client\nwould not make a call.\nAlternatively a nice api error message would also work ... but avoiding\nnetwork calls is always great :)\n\nOn Tue, Mar 1, 2016 at 12:57 PM, Alex Wood notifications@github.com wrote:\n\nI can bring this up with the service team, and see if there is a model\nenhancement that can be made. I wouldn't be inclined to make this a\nclient-side customization since it's making a validation assertion that, in\ntheory, could change server side.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/aws/aws-sdk-ruby/issues/1114#issuecomment-190900807.\n. random name seems a bit strange ... how about \"assumed-#{profile_name}-#{ENV['USER']}\"\n. \n",
    "seanarnold": "It is only some tokens. The length doesn't seem to cause it, however this token is being returned by my device and isn't working\n. Using this code here:\naws_opts = {\n      platform_application_arn: platform_application_arn,\n      token:                    (device_token || ''),\n      custom_user_data:         (user_id || '')\n    }\n...\nresponse = sns_client.create_platform_endpoint(aws_opts)\nWhere device_token is APA91bETzEWr7i-1yUGWpgPsu-bDe9kV_rPu1EnHL9oj0s9WDqxGnK5r2tm9x8VbE2yLBAzA6WqP8FhJPCvCKUx8XFhSCxrSzdql_t8KLNS7MFrc4k-RCfDj6TvOoQ2to4SG8RGbOJVS\n.\nThe response we get back is:\n{\"success\":false,\"error\":{\"code\":400,\"message\":{\"device_token\":[\"is invalid\"]}}}\nThis device_token works fine when manually adding to the SNS Management console\n. Here's a request that gives me a 400\n<removed>\n. Ahhhh sorrry,\nit was a problem with our implementation! Sorry, thanks for your help. \n. ",
    "UncleGene": "False alarm for the code, thanks for your example. \nI'd still keep this issue open for the documentation problem: it does not state anywhere which options are expected to be passed in the header and which are included in the generated url (as far as I can see in the code, content_type and content_md5 are the only header ones?).\nAnother missing piece of documentation is that signature is generated for PUT (it can be implied, but deserves explicit statement)\nAt current state it is practically impossible to construct a valid request from documentation only.\nThank you!\n. PR #387 \n. ",
    "maintux": "Hi, thanks for reply. Here the results:\n```\nWith limit:\n...\n[AWS DynamoDB 200 0.020943 0 retries] scan(:attributes_to_get=>[\"id\"],:exclusive_start_key=>{\"HashKeyElement\"=>{\"S\"=>\"71fdf53f-b2c7-4f4b-9ead-df21b0ae70ac\"}},:limit=>1,:scan_filter=>{\"newsletter_id\"=>{:attribute_value_list=>[{:n=>\"286\"}],:comparison_operator=>\"EQ\"},\"schedule_id\"=>{:attribute_value_list=>[{:n=>\"294\"}],:comparison_operator=>\"EQ\"},\"society_id\"=>{:attribute_value_list=>[{:n=>\"912\"}],:comparison_operator=>\"EQ\"}},:table_name=>\"production_ta_newsletter\")\n[AWS DynamoDB 200 0.012294 0 retries] scan(:attributes_to_get=>[\"id\"],:exclusive_start_key=>{\"HashKeyElement\"=>{\"S\"=>\"e341e7f5-65c9-4201-86bb-05ca3c9217e1\"}},:limit=>1,:scan_filter=>{\"newsletter_id\"=>{:attribute_value_list=>[{:n=>\"286\"}],:comparison_operator=>\"EQ\"},\"schedule_id\"=>{:attribute_value_list=>[{:n=>\"294\"}],:comparison_operator=>\"EQ\"},\"society_id\"=>{:attribute_value_list=>[{:n=>\"912\"}],:comparison_operator=>\"EQ\"}},:table_name=>\"production_ta_newsletter\")\n[AWS DynamoDB 200 0.017557 0 retries] scan(:attributes_to_get=>[\"id\"],:exclusive_start_key=>{\"HashKeyElement\"=>{\"S\"=>\"b09ca195-4368-4da2-9b41-1564dce66126\"}},:limit=>1,:scan_filter=>{\"newsletter_id\"=>{:attribute_value_list=>[{:n=>\"286\"}],:comparison_operator=>\"EQ\"},\"schedule_id\"=>{:attribute_value_list=>[{:n=>\"294\"}],:comparison_operator=>\"EQ\"},\"society_id\"=>{:attribute_value_list=>[{:n=>\"912\"}],:comparison_operator=>\"EQ\"}},:table_name=>\"production_ta_newsletter\")\n....\n```\n```\nWithout limit\n[AWS DynamoDB 200 0.0819 0 retries] scan(:attributes_to_get=>[\"id\"],:scan_filter=>{\"newsletter_id\"=>{:attribute_value_list=>[{:n=>\"286\"}],:comparison_operator=>\"EQ\"},\"schedule_id\"=>{:attribute_value_list=>[{:n=>\"294\"}],:comparison_operator=>\"EQ\"},\"society_id\"=>{:attribute_value_list=>[{:n=>\"912\"}],:comparison_operator=>\"EQ\"}},:table_name=>\"production_ta_newsletter\")\n```\nAs you can see with the limit attribute the gem makes a lot of scan operations, that even causes triggers of read limit! \n. thanks! :-)\nIl 16/ott/2013 20:28 \"Loren Segal\" notifications@github.com ha scritto:\n\nIt seems like the .first is forcing the SDK to set a limit of 1, at which\npoint our paginator kicks in because it sees that there is more data to\ngrab. We will look into this and see exactly what is going wrong.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/aws/aws-sdk-ruby/issues/385#issuecomment-26444370\n.\n. :+1: @trevorrowe good job! Thanks. @lsegal thanks to you too :)\nHave you any idea about next gem release?\n. \n",
    "maxmmurphy": "bump! This feature is so useful for those of us using vpcs and need to currently assign EIPs and maintain those.\n. ",
    "ajsharp": "Awesome, thanks for the wire trace tip. Will try that and post to the forums. \n- alex\n\nOn Oct 19, 2013, at 2:05 PM, Loren Segal notifications@github.com wrote:\nIt sounds like the SDK is doing the right thing here. I would suggest posting on the AWS OpsWorks forums to see if they can provide any help with what's going on. I would also point out the lack of error message, since services should be providing a helpful error message in these situations. Just to make sure, though, try running with AWS.config(http_wire_trace: true) to make sure there is no error being sent back in the payload that we are not parsing.\n\u2014\nReply to this email directly or view it on GitHub.\n. \n",
    "mattbailey": "This is actually a really crappy error response.   It is, in fact, invalidating your args, but not reporting that as the cause of the validation error. you want args: {'recipes' => ['clockworkd::default']}\nPosting this here since it's a first google hit, and I think it /should/ be a bug for a more useful exception.\nTook me forever to figure this out. See:\nhttp://docs.aws.amazon.com/opsworks/latest/APIReference/API_DescribeServiceErrors.html\n. ",
    "ckarbass": "wow, quick response.  Impressionist (https://github.com/charlotte-ruby/impressionist)\n. I'll give that a shot, thanks for your help.\n. Actually, that version doesn't work with Rails 2; I forgot I had already tried that. I'll see if they can add support for Ruby 1.8 or try forking and doing it myself potentially.\n. ",
    "amatriain": "I'm using the Feedzirra gem, which depends on the Sax-machine gem. Latest sax-machine version depends on Nokogiri 1.6.0. I cannot update to latest Feedzirra version because of aws-sdk dependency on Nokogiri < 1.6.0\n. ",
    "dblock": "Can we just drop support for Ruby 1.8.7 maybe moving forward? \n. ",
    "findchris": "Thanks for the response.\nSo I should be able to do something like following, right?\nruby\nbegin\n  ddb.batch_get(properties, keys, consistent_read: true)\nrescue  AWS::DynamoDB::Errors::ProvisionedThroughputExceededException => e\n  puts e.http_request.params['hash_key']\n  puts e.http_response.headers['x-amzn-requestid'].first\nend\nIt seems that making at least the request_id available as a first class attribute would be a valuable feature for debugging.\n. I see.  Thank you.  Just to be explicit, you mean:\nruby\nrescue  AWS::DynamoDB::Errors::ProvisionedThroughputExceededException => e\n  puts JSON.parse(e.http_request.body)['RequestItems']['my_table_name']['Keys']\n  puts e.http_response.headers['x-amzn-requestid'].first\nend\nOn a somewhat related note, I'd like to know about consumed capacity for a batch_get call.  In client_v2.rb, I see a reference to the :return_consumed_capacity option, but not in the #batch_get documentation (:consistent_read is the only documented option I see).  Is that option not documented for some reason?\n. In addition to my previous question about :return_consumed_capacity, I had another point I'd like to have clarified if you have the time.\nWill a ProvisionedThroughputExceededException be raised if you don't configure dynamo_db_retry_throughput_errors (defaults to true)?  I presume it will be raised if retry logic fails.\nFinally, what is the difference between a ThrottlingException and a ProvisionedThroughputExceededException, and can a ThrottlingException be explicitly rescued?\nThanks.\n. Thank you @trevorrowe.  Your communication and support has been top notch, and I very much appreciate that.\nOne last thing as I close out this ticket:  :+1: on making request_id more readily accessible ;-)\nCheers.\n. Is there are reproducible test case?  I'd like to get this reported to ruby-core, if this is an issue with 2.1.1.\n. @YorickPeterse Do you have a reproducible test case that uses aws-sdk?\n. That's great news. Thanks for the quick reply.\nIs there a sample app to demonstrate usage?\nOn Thursday, October 23, 2014, Trevor Rowe notifications@github.com wrote:\n\nClosed #650 https://github.com/aws/aws-sdk-ruby/issues/650.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/aws/aws-sdk-ruby/issues/650#event-182925230.\n. Does this wrap the \"Amazon Kinesis Client Library (KCL)\" like the Python library does?  The interface is different:  no process_records and no mention of checkpointing.\n\nPer the docs:  \"Although you can use the Amazon Kinesis API to get data from an Amazon Kinesis stream, we recommend using the design patterns and code for Amazon Kinesis applications provided by the KCL.\"\n. Would a Ruby port be outside the scope of aws-sdk-ruby?  If so, what entity within AWS would get such a project kicked off?\n. Can I :+1: this project, should you have the opportunity to push the conversation forward with your contacts at AWS?  I'd love to get this kicked off, and it shouldn't be terribly difficult to follow the Python library's example.  Since the Python and Java libraries are official, it seems silly for me to start a separate project if it is already on the AWS roadmap.\n. ",
    "eniskonuk": "@trevorrowe -- appreciate the quick reply. I'm not sure that I can give more context. The code prior to these lines does a create/update_stack and waits until the stack is either CREATE_COMPLETE or UPDATE_COMPLETE. No 'Rate Exceeded's in any of that section. Only when I get to this point. So, if the physical_resource_id is the only one that makes a call, why is it getting 'Rate Exceeded'? And 2-3 times and sometimes more.\nI tried AWS.memoize to no avail.\nThx\nEnis\n. I ran everything using the logger as you mentioned and here is a snippet of the log. Any ideas?\n.I, [2013-10-24T05:51:31.244727 #78134]  INFO -- : [AWS CloudFormation 200 0.477329 0 retries] describe_stacks(:stack_name=>\"cukeresque\")  \n.I, [2013-10-24T05:51:33.497100 #78134]  INFO -- : [AWS CloudFormation 200 0.250959 0 retries] describe_stacks(:stack_name=>\"cukeresque\")  \n.I, [2013-10-24T05:51:36.063670 #78134]  INFO -- : [AWS CloudFormation 200 0.564497 0 retries] describe_stacks(:stack_name=>\"cukeresque\")  \n.I, [2013-10-24T05:51:38.295203 #78134]  INFO -- : [AWS CloudFormation 200 0.229314 0 retries] describe_stacks(:stack_name=>\"cukeresque\")  \n.I, [2013-10-24T05:51:40.544130 #78134]  INFO -- : [AWS CloudFormation 200 0.246828 0 retries] describe_stacks(:stack_name=>\"cukeresque\")  \n.I, [2013-10-24T05:51:42.768423 #78134]  INFO -- : [AWS CloudFormation 200 0.222141 0 retries] describe_stacks(:stack_name=>\"cukeresque\")  \n.I, [2013-10-24T05:51:45.045025 #78134]  INFO -- : [AWS CloudFormation 200 0.274512 0 retries] describe_stacks(:stack_name=>\"cukeresque\")  \n.I, [2013-10-24T05:51:47.424044 #78134]  INFO -- : [AWS CloudFormation 200 0.376792 0 retries] describe_stacks(:stack_name=>\"cukeresque\")  \n.I, [2013-10-24T05:51:49.673840 #78134]  INFO -- : [AWS CloudFormation 200 0.247767 0 retries] describe_stacks(:stack_name=>\"cukeresque\")  \ncreate_stack update/create stack done - status: UPDATE_COMPLETE\nI, [2013-10-24T05:51:54.024935 #78134]  INFO -- : [AWS CloudFormation 400 4.306528 3 retries] describe_stack_resource(:logical_resource_id=>\"qwiklabResqueASGroup\",:stack_name=>\"cukeresque\") AWS::CloudFormation::Errors::Throttling Rate exceeded\nRate Exceeded\nI, [2013-10-24T05:52:08.494796 #78134]  INFO -- : [AWS CloudFormation 400 4.468372 3 retries] describe_stack_resource(:logical_resource_id=>\"qwiklabResqueASGroup\",:stack_name=>\"cukeresque\") AWS::CloudFormation::Errors::Throttling Rate exceeded\nRate Exceeded\nI, [2013-10-24T05:52:22.690939 #78134]  INFO -- : [AWS CloudFormation 400 4.194326 3 retries] describe_stack_resource(:logical_resource_id=>\"qwiklabResqueASGroup\",:stack_name=>\"cukeresque\") AWS::CloudFormation::Errors::Throttling Rate exceeded\nRate Exceeded\nI, [2013-10-24T05:52:36.726255 #78134]  INFO -- : [AWS CloudFormation 400 4.033627 3 retries] describe_stack_resource(:logical_resource_id=>\"qwiklabResqueASGroup\",:stack_name=>\"cukeresque\") AWS::CloudFormation::Errors::Throttling Rate exceeded\nRate Exceeded\nI, [2013-10-24T05:52:46.980818 #78134]  INFO -- : [AWS CloudFormation 200 0.25285 0 retries] describe_stack_resource(:logical_resource_id=>\"qwiklabResqueASGroup\",:stack_name=>\"cukeresque\")  \nThx\nEnis\n. I figured out what causes it. I think the sdk needs to take care of rate exceeded somehow. I had a conditional like \nIf stack.exists? and stack.status == \"CREATE_COMPLETE\" \nThis caused rate exceeded further down in the code. The sdk needs to insulate the application code from this I believe. Otherwise the higher level methods are useless.\nThx\nEnis Konuk\n978-760-0732\n\nOn Nov 21, 2013, at 12:45 PM, Trevor Rowe notifications@github.com wrote:\nAnother thing to consider. We have released the new aws-sdk-core gem. This is the first part of our v2 release. It does not share the same higher level abstractions, but it does support the Cloud Formation API. You may want to check it out:\nhttps://github.com/aws/aws-sdk-core-ruby\nI'm going to close this old/stale issue. Please re-open if you continue to have issues.\n\u0081\\\nReply to this email directly or view it on GitHub.\n. I agree that it's not possible to isolate application code 100% but the previous if statement I gave as an example should be handled in the higher level abstractions. Otherwise those higher level methods are not very useful. I ended up going down to the client level and writing my own abstraction method which is a waste. The SDK at the higher levels should slow the application down, if necessary, to handle rate exceeded errors like these. CloudFormation is one of the most aggressive services in this regard -- 0.5TPS is a ridiculously slow transaction rate. And why do I need to even know that when I'm writing application code? That's why there are all those abstraction layers.\n\nJust my 2cents.\nThx\nEnis Konuk\nenis@qwiklab.com\n978-760-0732\nOn Nov 21, 2013, at 10:48 PM, Loren Segal notifications@github.com wrote:\n\n@eniskonuk keep in mind that we are flipping the memoization strategy in V2 moving forward because of behaviors like these. The SDK should certainly attempt to send a minimum number of requests for a given call, but it's not possible to completely isolate application code from being throttled.\n\u2014\nReply to this email directly or view it on GitHub.\n. I will try and let you know if this clears the issue.\n\nEnis Konuk\nenis@qwiklab.com\n978-760-0732\nOn Jan 15, 2014, at 5:05 AM, vonkoch notifications@github.com wrote:\n\nCould you try v1.32.0? It should fix the issue.\n\u2014\nReply to this email directly or view it on GitHub.\n. It works now. 1.32.0 has fixed this issue.\n\nThx\nEnis Konuk\nenis@qwiklab.com\n978-760-0732\nOn Jan 15, 2014, at 8:30 AM, Enis Konuk enis@cloudvlab.com wrote:\n\nI will try and let you know if this clears the issue.\nEnis Konuk\nenis@qwiklab.com\n978-760-0732\n\nOn Jan 15, 2014, at 5:05 AM, vonkoch notifications@github.com wrote:\n\nCould you try v1.32.0? It should fix the issue.\n\u2014\nReply to this email directly or view it on GitHub.\n. OK. Thanks.\n. Use 1.56.0 of the SDK. We ran into the same problem and 1.56.0 seems to solve it.\n. @trevorrowe: We had the same problem and ended up using the describe_job_flows, terminate_job_flows API calls. The disturbing part is that the describe_job_flows documentation says that this call is deprecated and will be removed in the future. The trouble is that there doesn't seem to be an alternative unless the cluster_id and job_flow_id parameters are interchangeable or the same. Shedding some light on this would be very helpful in lieu of implementing what the CLI is doing.\n. Thanks for the note. Take your time. We're using our forked version of the gem at the moment and will switch over once everything is merged and released. There are probably some additional waiters that we'll add later.\n. @trevorrowe: Any update on this?\n. Awesome. Thanks for the merge. More to come in the future.\n. It does appear to be an S3 API issue but I'm not sure. I don't think it's an encoding issue as it works fine using the delete_object API call but not when you use delete_objects. The trouble is that it silently fails. Is there something that can be done in the SDK so there is an actual failure?\n\n\nThx\nEnis \nOn Apr 29, 2015, at 1:02 PM, Trevor Rowe notifications@github.com wrote:\n\nLooking at the linked issue, it appears that this is a limitation of Amazon S3, and not the SDK. Is my understanding correct? Similar to how you can request that the keys be URI encoded in a list objects response, you would need to specify the keys are URI encoded in a delete objects batch request so that S3 knows to decode them server-side.\nI can follow up with the Java team and see if they have raised this issue/question with Amazon S3.\n\u2014\nReply to this email directly or view it on GitHub.\n. The response errors list is empty. It does look like S3 just ignores the request. The other issue is that if the list has multiple objects and only one has invalid XML characters, the whole list is ignored. Your workaround is what we've implemented right now but that potentially increases the number of API calls by the number of objects. If nothing can be done, then we can close this issue.\n. @trevorrowe :100: Thanks\n. Thanks @trevorrowe \n. Thanks @trevorrowe. Really appreciate the fast response.\n. Yes. I saw it and tested already. We will deploy as soon as we can.\n\nEnis Konuk\nenis@qwiklab.com\n978-760-0732\nOn Jul 29, 2015, at 8:42 PM, Trevor Rowe notifications@github.com wrote:\n\nI forgot to update, but 2.1.10 was released earlier today wight the patch.\n\u2014\nReply to this email directly or view it on GitHub.\n. \n\n. Thanks Trevor. Saw that a few minutes ago. We're starting work using it immediately.\n\nOn another note, the SSM API change broke our existing code. I'm hoping an upgrade to 2.1.32 will automagically fix everything.\nThx\nEnis Konuk\nenis@qwiklab.com\n978-760-0732\nOn Oct 26, 2015, at 4:35 PM, Trevor Rowe notifications@github.com wrote:\n\nGood news! We were able to release support for API gateway today!\n\u2014\nReply to this email directly or view it on GitHub.\n. My initial assessment was yes but after debugging this, it turns out the issue is not related to the API change but to the new AWS-* documents in SSM that are not deletable. \n\nThx\nEnis Konuk\nenis@qwiklab.com\n978-760-0732\nOn Oct 26, 2015, at 5:15 PM, Trevor Rowe notifications@github.com wrote:\n\nDo you mean that a service change to the SSM API caused your application to\nbreak before upgrading the SDK? If so thats a bit concerning and should\nprobably be escalated.\nOn Mon, Oct 26, 2015 at 2:13 PM, eniskonuk notifications@github.com wrote:\n\nThanks Trevor. Saw that a few minutes ago. We're starting work using it\nimmediately.\nOn another note, the SSM API change broke our existing code. I'm hoping an\nupgrade to 2.1.32 will automagically fix everything.\nThx\nEnis Konuk\nenis@qwiklab.com\n978-760-0732\nOn Oct 26, 2015, at 4:35 PM, Trevor Rowe notifications@github.com wrote:\n\nGood news! We were able to release support for API gateway today!\n\u2014\nReply to this email directly or view it on GitHub.\n\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/aws/aws-sdk-ruby/issues/884#issuecomment-151286412.\n\u2014\nReply to this email directly or view it on GitHub.\n. :+1:  We would also benefit from this.\n. Any update on this? I am now stuck. I need 2.2.13 because of ACM and now can't get Cloudfront to work. If it's going to take a bit I will need to revert using ACM until this is fixed.\n. Unfortunately, I can\u2019t get this to work at all w/ 2.2.13. So, I\u2019m reverting back to 2.2.12 and taking ACM out. Here is a wire trace that would hopefully help.\n\n\nI, [2016-02-02T20:40:29.555058 #22148]  INFO -- : [Aws::CloudFront::Client 200 0.044627 0 retries] get_distribution(id:\"E3FGJN6RSDRJ1X\")  \n<- \"GET /2016-01-28/distribution/E3FGJN6RSDRJ1X HTTP/1.1\\r\\nContent-Type: \\r\\nAccept-Encoding: \\r\\nUser-Agent: aws-sdk-ruby2/2.2.13 ruby/2.1.7 x86_64-darwin15.0\\r\\nX-Amz-Date: 20160203T014029Z\\r\\nHost: cloudfront.amazonaws.com\\r\\nX-Amz-Content-Sha256: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\\r\\nAuthorization: AWS4-HMAC-SHA256 Credential=AKIAJAZK6AWGD4XJY5RQ/20160203/us-east-1/cloudfront/aws4_request, SignedHeaders=host;x-amz-content-sha256;x-amz-date, Signature=8b99e9ddb419f4d96729532a3ce895c4e5f4f38df50b4f1beaef19ef0887462d\\r\\nContent-Length: 0\\r\\nAccept: /\\r\\n\\r\\n\"\n-> \"HTTP/1.1 200 OK\\r\\n\"\n-> \"x-amzn-RequestId: 1b4f2bc8-ca17-11e5-a3d0-bdf971e8bbc4\\r\\n\"\n-> \"ETag: EYL9XOBT5VVS5\\r\\n\"\n-> \"Content-Type: text/xml\\r\\n\"\n-> \"Content-Length: 2690\\r\\n\"\n-> \"Vary: Accept-Encoding\\r\\n\"\n-> \"Date: Wed, 03 Feb 2016 01:40:29 GMT\\r\\n\"\n-> \"\\r\\n\"\nreading 2690 bytes...\n-> \"\"\n-> \"<?xml version=\\\"1.0\\\"?>\\nE3FGJN6RSDRJ1XDeployed2016-02-01T19:00:08.855Z0d3syxngdzbbsmh.cloudfront.netfalse0145435321412601ELB-qlstack2-ElasticL-1STWX54AV2QDB-1773988177qlstack2-ElasticL-1STWX54AV2QDB-1773988177.eu-west-1.elb.amazonaws.com080443match-viewer3TLSv1TLSv1.1TLSv1.2ELB-qlstack2-ElasticL-1STWX54AV2QDB-1773988177trueall1*false0allow-all02HEADGET2HEADGETfalse8640031536000false00falsefalsePriceClass_100truecloudfrontSSLv3truenone0\"\nread 2690 bytes\nConn keep-alive\nI, [2016-02-02T20:40:29.607404 #22148]  INFO -- : [Aws::CloudFront::Client 200 0.045243 0 retries] get_distribution(id:\"E3FGJN6RSDRJ1X\")  \nI, [2016-02-02T20:40:29.607490 #22148]  INFO -- : 231232953254:eu-central-1: Disabling cloudfront distribution: E3FGJN6RSDRJ1X: Deployed: true\n<- \"PUT /2016-01-28/distribution/E3FGJN6RSDRJ1X/config HTTP/1.1\\r\\nContent-Type: \\r\\nAccept-Encoding: \\r\\nUser-Agent: aws-sdk-ruby2/2.2.13 ruby/2.1.7 x86_64-darwin15.0\\r\\nIf-Match: EYL9XOBT5VVS5\\r\\nX-Amz-Date: 20160203T014029Z\\r\\nHost: cloudfront.amazonaws.com\\r\\nX-Amz-Content-Sha256: 24ac89e57665f04b9e25a3259bac6ee258b5cb6486217b24bc43990e0ed611d0\\r\\nAuthorization: AWS4-HMAC-SHA256 Credential=AKIAJAZK6AWGD4XJY5RQ/20160203/us-east-1/cloudfront/aws4_request, SignedHeaders=host;x-amz-content-sha256;x-amz-date, Signature=66d6824ba2829ff1daccac00c83fec3556f902470ca26336aca1039febd6e783\\r\\nContent-Length: 3123\\r\\nAccept: /\\r\\n\\r\\n\"\n-> \"HTTP/1.1 400 Bad Request\\r\\n\"\n-> \"x-amzn-RequestId: 1b58c8b9-ca17-11e5-a3d0-bdf971e8bbc4\\r\\n\"\n-> \"Content-Type: text/xml\\r\\n\"\n-> \"Content-Length: 283\\r\\n\"\n-> \"Date: Wed, 03 Feb 2016 01:40:29 GMT\\r\\n\"\n-> \"Connection: close\\r\\n\"\n-> \"\\r\\n\"\nreading 283 bytes...\n-> \"\"\n-> \"<?xml version=\\\"1.0\\\"?>\\nSenderMalformedInputUnexpected list element termination1b58c8b9-ca17-11e5-a3d0-bdf971e8bbc4\"\nread 283 bytes\nConn close\nI, [2016-02-02T20:40:29.664125 #22148]  INFO -- : [Aws::CloudFront::Client 400 0.054669 0 retries] update_distribution(distribution_config:{caller_reference:\"1454353214126\",aliases:{quantity:0,items:[]},default_root_object:\"\",origins:{quantity:1,items:[{id:\"ELB-qlstack2-ElasticL-1STWX54AV2QDB-1773988177\",domain_name:\"qlstack2-ElasticL-1STWX54AV2QDB-1773988177.eu-west-1.elb.amazonaws.com\",origin_path:\"\",custom_headers:{quantity:0,items:[]},s3_origin_config:nil,custom_origin_config:{http_port:80,https_port:443,origin_protocol_policy:\"match-viewer\",origin_ssl_protocols:{quantity:3,items:[\"TLSv1\",\"TLSv1.1\",\"TLSv1.2\"]}}}]},default_cache_behavior:{target_origin_id:\"ELB-qlstack2-ElasticL-1STWX54AV2QDB-1773988177\",forwarded_values:{query_string:true,cookies:{forward:\"all\",whitelisted_names:{quantity:0,items:[]}},headers:{quantity:1,items:[\"*\"]}},trusted_signers:{enabled:false,quantity:0,items:[]},viewer_protocol_policy:\"allow-all\",min_ttl:0,allowed_methods:{quantity:2,items:[\"HEAD\",\"GET\"],cached_methods:{quantity:2,items:[\"HEAD\",\"GET\"]}},smooth_streaming:false,default_ttl:86400,max_ttl:31536000,compress:false},cache_behaviors:{quantity:0,items:[]},custom_error_responses:{quantity:0,items:[]},comment:\"\",logging:{enabled:false,include_cookies:false,bucket:\"\",prefix:\"\"},price_class:\"PriceClass_100\",enabled:false,viewer_certificate:{certificate:nil,certificate_source:\"cloudfront\",ssl_support_method:nil,minimum_protocol_version:\"SSLv3\",iam_certificate_id:nil,cloud_front_default_certificate:true},restrictions:{geo_restriction:{restriction_type:\"none\",quantity:0,items:[]}},web_acl_id:\"\"},id:\"E3FGJN6RSDRJ1X\",if_match:\"EYL9XOBT5VVS5\") Aws::CloudFront::Errors::MalformedInput Unexpected list element termination\nThx\nEnis Konuk\nenis@qwiklab.com\n978-760-0732\n\nOn Feb 1, 2016, at 10:40 PM, Alex Wood notifications@github.com wrote:\nUnfortunately, not much to report. However, you should be able to work around this by altering the response from #get_distribution_config, if this is the issue I think it is. If you're using ACM, remove from the :viewer_certificate portion of the call to #update_distributions the portions about the :iam_certificate_id.\nLet me know if that helps, if not, another wire trace would be helpful.\n\u2014\nReply to this email directly or view it on GitHub https://github.com/aws/aws-sdk-ruby/issues/1067#issuecomment-178349595.\n. I think I have figured out where the issue is. Between 2.2.8 and 2.2.12 when I do a get_distribution API call on the same distribution id I get the following differences in the XML. You can see how two new sections that got added w/ the new API version. And one of them \u2018CustomHeaders\u2019 is malformed. I think that\u2019s the culprit.\n\n2c2\n< \n\n\n25,28d24\n<           \n<             0\n<             \n<           \n33,40d28\n<             \n<               3\n<               \n<                 TLSv1\n<                 TLSv1.1\n<                 TLSv1.2\n<               \n<             \n\nEnis Konuk\nenis@qwiklab.com\n978-760-0732\n\nOn Feb 2, 2016, at 8:48 PM, Enis Konuk enis@cloudvlab.com wrote:\nUnfortunately, I can\u2019t get this to work at all w/ 2.2.13. So, I\u2019m reverting back to 2.2.12 and taking ACM out. Here is a wire trace that would hopefully help.\nI, [2016-02-02T20:40:29.555058 #22148]  INFO -- : [Aws::CloudFront::Client 200 0.044627 0 retries] get_distribution(id:\"E3FGJN6RSDRJ1X\")  \n<- \"GET /2016-01-28/distribution/E3FGJN6RSDRJ1X HTTP/1.1\\r\\nContent-Type: \\r\\nAccept-Encoding: \\r\\nUser-Agent: aws-sdk-ruby2/2.2.13 ruby/2.1.7 x86_64-darwin15.0\\r\\nX-Amz-Date: 20160203T014029Z\\r\\nHost: cloudfront.amazonaws.com http://cloudfront.amazonaws.com/\\r\\nX-Amz-Content-Sha256: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\\r\\nAuthorization: AWS4-HMAC-SHA256 Credential=AKIAJAZK6AWGD4XJY5RQ/20160203/us-east-1/cloudfront/aws4_request, SignedHeaders=host;x-amz-content-sha256;x-amz-date, Signature=8b99e9ddb419f4d96729532a3ce895c4e5f4f38df50b4f1beaef19ef0887462d\\r\\nContent-Length: 0\\r\\nAccept: /\\r\\n\\r\\n\"\n-> \"HTTP/1.1 200 OK\\r\\n\"\n-> \"x-amzn-RequestId: 1b4f2bc8-ca17-11e5-a3d0-bdf971e8bbc4\\r\\n\"\n-> \"ETag: EYL9XOBT5VVS5\\r\\n\"\n-> \"Content-Type: text/xml\\r\\n\"\n-> \"Content-Length: 2690\\r\\n\"\n-> \"Vary: Accept-Encoding\\r\\n\"\n-> \"Date: Wed, 03 Feb 2016 01:40:29 GMT\\r\\n\"\n-> \"\\r\\n\"\nreading 2690 bytes...\n-> \"\"\n-> \"<?xml version=\\\"1.0\\\"?>\\nhttp://cloudfront.amazonaws.com/doc/2016-01-28//\">E3FGJN6RSDRJ1XDeployed2016-02-01T19:00:08.855Z0d3syxngdzbbsmh.cloudfront.net http://d3syxngdzbbsmh.cloudfront.net/false0145435321412601ELB-qlstack2-ElasticL-1STWX54AV2QDB-1773988177qlstack2-ElasticL-1STWX54AV2QDB-1773988177.eu-west-1.elb.amazonaws.com http://qlstack2-elasticl-1stwx54av2qdb-1773988177.eu-west-1.elb.amazonaws.com/080443match-viewer3TLSv1TLSv1.1TLSv1.2ELB-qlstack2-ElasticL-1STWX54AV2QDB-1773988177trueall1*false0allow-all02HEADGET2HEADGETfalse8640031536000false00falsefalsePriceClass_100truecloudfrontSSLv3truenone0\"\nread 2690 bytes\nConn keep-alive\nI, [2016-02-02T20:40:29.607404 #22148]  INFO -- : [Aws::CloudFront::Client 200 0.045243 0 retries] get_distribution(id:\"E3FGJN6RSDRJ1X\")  \nI, [2016-02-02T20:40:29.607490 #22148]  INFO -- : 231232953254:eu-central-1: Disabling cloudfront distribution: E3FGJN6RSDRJ1X: Deployed: true\n<- \"PUT /2016-01-28/distribution/E3FGJN6RSDRJ1X/config HTTP/1.1\\r\\nContent-Type: \\r\\nAccept-Encoding: \\r\\nUser-Agent: aws-sdk-ruby2/2.2.13 ruby/2.1.7 x86_64-darwin15.0\\r\\nIf-Match: EYL9XOBT5VVS5\\r\\nX-Amz-Date: 20160203T014029Z\\r\\nHost: cloudfront.amazonaws.com http://cloudfront.amazonaws.com/\\r\\nX-Amz-Content-Sha256: 24ac89e57665f04b9e25a3259bac6ee258b5cb6486217b24bc43990e0ed611d0\\r\\nAuthorization: AWS4-HMAC-SHA256 Credential=AKIAJAZK6AWGD4XJY5RQ/20160203/us-east-1/cloudfront/aws4_request, SignedHeaders=host;x-amz-content-sha256;x-amz-date, Signature=66d6824ba2829ff1daccac00c83fec3556f902470ca26336aca1039febd6e783\\r\\nContent-Length: 3123\\r\\nAccept: /\\r\\n\\r\\n\"\n-> \"HTTP/1.1 400 Bad Request\\r\\n\"\n-> \"x-amzn-RequestId: 1b58c8b9-ca17-11e5-a3d0-bdf971e8bbc4\\r\\n\"\n-> \"Content-Type: text/xml\\r\\n\"\n-> \"Content-Length: 283\\r\\n\"\n-> \"Date: Wed, 03 Feb 2016 01:40:29 GMT\\r\\n\"\n-> \"Connection: close\\r\\n\"\n-> \"\\r\\n\"\nreading 283 bytes...\n-> \"\"\n-> \"<?xml version=\\\"1.0\\\"?>\\nhttp://cloudfront.amazonaws.com/doc/2016-01-28//\">SenderMalformedInputUnexpected list element termination1b58c8b9-ca17-11e5-a3d0-bdf971e8bbc4\"\nread 283 bytes\nConn close\nI, [2016-02-02T20:40:29.664125 #22148]  INFO -- : [Aws::CloudFront::Client 400 0.054669 0 retries] update_distribution(distribution_config:{caller_reference:\"1454353214126\",aliases:{quantity:0,items:[]},default_root_object:\"\",origins:{quantity:1,items:[{id:\"ELB-qlstack2-ElasticL-1STWX54AV2QDB-1773988177\",domain_name:\"qlstack2-ElasticL-1STWX54AV2QDB-1773988177.eu-west-1.elb.amazonaws.com http://qlstack2-elasticl-1stwx54av2qdb-1773988177.eu-west-1.elb.amazonaws.com/\",origin_path:\"\",custom_headers:{quantity:0,items:[]},s3_origin_config:nil,custom_origin_config:{http_port:80,https_port:443,origin_protocol_policy:\"match-viewer\",origin_ssl_protocols:{quantity:3,items:[\"TLSv1\",\"TLSv1.1\",\"TLSv1.2\"]}}}]},default_cache_behavior:{target_origin_id:\"ELB-qlstack2-ElasticL-1STWX54AV2QDB-1773988177\",forwarded_values:{query_string:true,cookies:{forward:\"all\",whitelisted_names:{quantity:0,items:[]}},headers:{quantity:1,items:[\"*\"]}},trusted_signers:{enabled:false,quantity:0,items:[]},viewer_protocol_policy:\"allow-all\",min_ttl:0,allowed_methods:{quantity:2,items:[\"HEAD\",\"GET\"],cached_methods:{quantity:2,items:[\"HEAD\",\"GET\"]}},smooth_streaming:false,default_ttl:86400,max_ttl:31536000,compress:false},cache_behaviors:{quantity:0,items:[]},custom_error_responses:{quantity:0,items:[]},comment:\"\",logging:{enabled:false,include_cookies:false,bucket:\"\",prefix:\"\"},price_class:\"PriceClass_100\",enabled:false,viewer_certificate:{certificate:nil,certificate_source:\"cloudfront\",ssl_support_method:nil,minimum_protocol_version:\"SSLv3\",iam_certificate_id:nil,cloud_front_default_certificate:true},restrictions:{geo_restriction:{restriction_type:\"none\",quantity:0,items:[]}},web_acl_id:\"\"},id:\"E3FGJN6RSDRJ1X\",if_match:\"EYL9XOBT5VVS5\") Aws::CloudFront::Errors::MalformedInput Unexpected list element termination\nThx\nEnis Konuk\nenis@qwiklab.com mailto:enis@qwiklab.com\n978-760-0732\n\nOn Feb 1, 2016, at 10:40 PM, Alex Wood notifications@github.com> wrote:\nUnfortunately, not much to report. However, you should be able to work around this by altering the response from #get_distribution_config, if this is the issue I think it is. If you're using ACM, remove from the :viewer_certificate portion of the call to #update_distributions the portions about the :iam_certificate_id.\nLet me know if that helps, if not, another wire trace would be helpful.\n\u2014\nReply to this email directly or view it on GitHub https://github.com/aws/aws-sdk-ruby/issues/1067#issuecomment-178349595.\n. I got it to work w/ 2.2.12 by forcing custom_headers: { quantity: 0 } when I turn around the response from get_distribution to update_distribution\n\n\nThx\nEnis Konuk\nenis@qwiklab.com\n978-760-0732\n\nOn Feb 2, 2016, at 9:45 PM, Enis Konuk enis@cloudvlab.com wrote:\nI think I have figured out where the issue is. Between 2.2.8 and 2.2.12 when I do a get_distribution API call on the same distribution id I get the following differences in the XML. You can see how two new sections that got added w/ the new API version. And one of them \u2018CustomHeaders\u2019 is malformed. I think that\u2019s the culprit.\n2c2\n< http://cloudfront.amazonaws.com/doc/2016-01-13//\">\n\nhttp://cloudfront.amazonaws.com/doc/2015-09-17//\">\n25,28d24\n<           \n<             0\n<             \n<           \n33,40d28\n<             \n<               3\n<               \n<                 TLSv1\n<                 TLSv1.1\n<                 TLSv1.2\n<               \n<             \n\nEnis Konuk\nenis@qwiklab.com mailto:enis@qwiklab.com\n978-760-0732\n\nOn Feb 2, 2016, at 8:48 PM, Enis Konuk enis@cloudvlab.com> wrote:\nUnfortunately, I can\u2019t get this to work at all w/ 2.2.13. So, I\u2019m reverting back to 2.2.12 and taking ACM out. Here is a wire trace that would hopefully help.\nI, [2016-02-02T20:40:29.555058 #22148]  INFO -- : [Aws::CloudFront::Client 200 0.044627 0 retries] get_distribution(id:\"E3FGJN6RSDRJ1X\")  \n<- \"GET /2016-01-28/distribution/E3FGJN6RSDRJ1X HTTP/1.1\\r\\nContent-Type: \\r\\nAccept-Encoding: \\r\\nUser-Agent: aws-sdk-ruby2/2.2.13 ruby/2.1.7 x86_64-darwin15.0\\r\\nX-Amz-Date: 20160203T014029Z\\r\\nHost: cloudfront.amazonaws.com http://cloudfront.amazonaws.com/\\r\\nX-Amz-Content-Sha256: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\\r\\nAuthorization: AWS4-HMAC-SHA256 Credential=AKIAJAZK6AWGD4XJY5RQ/20160203/us-east-1/cloudfront/aws4_request, SignedHeaders=host;x-amz-content-sha256;x-amz-date, Signature=8b99e9ddb419f4d96729532a3ce895c4e5f4f38df50b4f1beaef19ef0887462d\\r\\nContent-Length: 0\\r\\nAccept: /\\r\\n\\r\\n\"\n-> \"HTTP/1.1 200 OK\\r\\n\"\n-> \"x-amzn-RequestId: 1b4f2bc8-ca17-11e5-a3d0-bdf971e8bbc4\\r\\n\"\n-> \"ETag: EYL9XOBT5VVS5\\r\\n\"\n-> \"Content-Type: text/xml\\r\\n\"\n-> \"Content-Length: 2690\\r\\n\"\n-> \"Vary: Accept-Encoding\\r\\n\"\n-> \"Date: Wed, 03 Feb 2016 01:40:29 GMT\\r\\n\"\n-> \"\\r\\n\"\nreading 2690 bytes...\n-> \"\"\n-> \"<?xml version=\\\"1.0\\\"?>\\nhttp://cloudfront.amazonaws.com/doc/2016-01-28//\">E3FGJN6RSDRJ1XDeployed2016-02-01T19:00:08.855Z0d3syxngdzbbsmh.cloudfront.net http://d3syxngdzbbsmh.cloudfront.net/false0145435321412601ELB-qlstack2-ElasticL-1STWX54AV2QDB-1773988177qlstack2-ElasticL-1STWX54AV2QDB-1773988177.eu-west-1.elb.amazonaws.com http://qlstack2-elasticl-1stwx54av2qdb-1773988177.eu-west-1.elb.amazonaws.com/080443match-viewer3TLSv1TLSv1.1TLSv1.2ELB-qlstack2-ElasticL-1STWX54AV2QDB-1773988177trueall1*false0allow-all02HEADGET2HEADGETfalse8640031536000false00falsefalsePriceClass_100truecloudfrontSSLv3truenone0\"\nread 2690 bytes\nConn keep-alive\nI, [2016-02-02T20:40:29.607404 #22148]  INFO -- : [Aws::CloudFront::Client 200 0.045243 0 retries] get_distribution(id:\"E3FGJN6RSDRJ1X\")  \nI, [2016-02-02T20:40:29.607490 #22148]  INFO -- : 231232953254:eu-central-1: Disabling cloudfront distribution: E3FGJN6RSDRJ1X: Deployed: true\n<- \"PUT /2016-01-28/distribution/E3FGJN6RSDRJ1X/config HTTP/1.1\\r\\nContent-Type: \\r\\nAccept-Encoding: \\r\\nUser-Agent: aws-sdk-ruby2/2.2.13 ruby/2.1.7 x86_64-darwin15.0\\r\\nIf-Match: EYL9XOBT5VVS5\\r\\nX-Amz-Date: 20160203T014029Z\\r\\nHost: cloudfront.amazonaws.com http://cloudfront.amazonaws.com/\\r\\nX-Amz-Content-Sha256: 24ac89e57665f04b9e25a3259bac6ee258b5cb6486217b24bc43990e0ed611d0\\r\\nAuthorization: AWS4-HMAC-SHA256 Credential=AKIAJAZK6AWGD4XJY5RQ/20160203/us-east-1/cloudfront/aws4_request, SignedHeaders=host;x-amz-content-sha256;x-amz-date, Signature=66d6824ba2829ff1daccac00c83fec3556f902470ca26336aca1039febd6e783\\r\\nContent-Length: 3123\\r\\nAccept: /\\r\\n\\r\\n\"\n-> \"HTTP/1.1 400 Bad Request\\r\\n\"\n-> \"x-amzn-RequestId: 1b58c8b9-ca17-11e5-a3d0-bdf971e8bbc4\\r\\n\"\n-> \"Content-Type: text/xml\\r\\n\"\n-> \"Content-Length: 283\\r\\n\"\n-> \"Date: Wed, 03 Feb 2016 01:40:29 GMT\\r\\n\"\n-> \"Connection: close\\r\\n\"\n-> \"\\r\\n\"\nreading 283 bytes...\n-> \"\"\n-> \"<?xml version=\\\"1.0\\\"?>\\nhttp://cloudfront.amazonaws.com/doc/2016-01-28//\">SenderMalformedInputUnexpected list element termination1b58c8b9-ca17-11e5-a3d0-bdf971e8bbc4\"\nread 283 bytes\nConn close\nI, [2016-02-02T20:40:29.664125 #22148]  INFO -- : [Aws::CloudFront::Client 400 0.054669 0 retries] update_distribution(distribution_config:{caller_reference:\"1454353214126\",aliases:{quantity:0,items:[]},default_root_object:\"\",origins:{quantity:1,items:[{id:\"ELB-qlstack2-ElasticL-1STWX54AV2QDB-1773988177\",domain_name:\"qlstack2-ElasticL-1STWX54AV2QDB-1773988177.eu-west-1.elb.amazonaws.com http://qlstack2-elasticl-1stwx54av2qdb-1773988177.eu-west-1.elb.amazonaws.com/\",origin_path:\"\",custom_headers:{quantity:0,items:[]},s3_origin_config:nil,custom_origin_config:{http_port:80,https_port:443,origin_protocol_policy:\"match-viewer\",origin_ssl_protocols:{quantity:3,items:[\"TLSv1\",\"TLSv1.1\",\"TLSv1.2\"]}}}]},default_cache_behavior:{target_origin_id:\"ELB-qlstack2-ElasticL-1STWX54AV2QDB-1773988177\",forwarded_values:{query_string:true,cookies:{forward:\"all\",whitelisted_names:{quantity:0,items:[]}},headers:{quantity:1,items:[\"*\"]}},trusted_signers:{enabled:false,quantity:0,items:[]},viewer_protocol_policy:\"allow-all\",min_ttl:0,allowed_methods:{quantity:2,items:[\"HEAD\",\"GET\"],cached_methods:{quantity:2,items:[\"HEAD\",\"GET\"]}},smooth_streaming:false,default_ttl:86400,max_ttl:31536000,compress:false},cache_behaviors:{quantity:0,items:[]},custom_error_responses:{quantity:0,items:[]},comment:\"\",logging:{enabled:false,include_cookies:false,bucket:\"\",prefix:\"\"},price_class:\"PriceClass_100\",enabled:false,viewer_certificate:{certificate:nil,certificate_source:\"cloudfront\",ssl_support_method:nil,minimum_protocol_version:\"SSLv3\",iam_certificate_id:nil,cloud_front_default_certificate:true},restrictions:{geo_restriction:{restriction_type:\"none\",quantity:0,items:[]}},web_acl_id:\"\"},id:\"E3FGJN6RSDRJ1X\",if_match:\"EYL9XOBT5VVS5\") Aws::CloudFront::Errors::MalformedInput Unexpected list element termination\nThx\nEnis Konuk\nenis@qwiklab.com mailto:enis@qwiklab.com\n978-760-0732\n\nOn Feb 1, 2016, at 10:40 PM, Alex Wood notifications@github.com> wrote:\nUnfortunately, not much to report. However, you should be able to work around this by altering the response from #get_distribution_config, if this is the issue I think it is. If you're using ACM, remove from the :viewer_certificate portion of the call to #update_distributions the portions about the :iam_certificate_id.\nLet me know if that helps, if not, another wire trace would be helpful.\n\u2014\nReply to this email directly or view it on GitHub https://github.com/aws/aws-sdk-ruby/issues/1067#issuecomment-178349595.\n. Very strange. The ping doesn\u2019t work for me. I get an ICMP timeout. If I try it from an EC2 instance it works fine. So, something is wrong w/ the routing somehow. The trace route shows that the request makes it to Korea but then dies. \n\n\n\ntraceroute to s3.ap-northeast-2.amazonaws.com (52.92.8.9), 64 hops max, 52 byte packets\n 1  router.asus.com (192.168.1.1)  2.384 ms  2.097 ms  1.977 ms\n 2  10.0.0.1 (10.0.0.1)  2.809 ms  2.940 ms  2.746 ms\n 3  96.120.69.61 (96.120.69.61)  11.261 ms  19.999 ms  11.987 ms\n 4  ge-6-1-ur01.woburn.ma.boston.comcast.net (68.87.158.197)  12.586 ms  11.783 ms  20.213 ms\n 5  be-117-ar01.woburn.ma.boston.comcast.net (68.85.106.161)  17.395 ms  16.265 ms  17.972 ms\n 6  he-0-11-0-0-ar01.needham.ma.boston.comcast.net (162.151.112.17)  15.197 ms  16.947 ms  17.525 ms\n 7  be-7015-cr02.newyork.ny.ibone.comcast.net (68.86.90.217)  21.266 ms  20.903 ms  23.006 ms\n 8  be-10103-cr02.ashburn.va.ibone.comcast.net (68.86.85.161)  27.581 ms  28.110 ms  29.739 ms\n 9  be-10114-cr02.56marietta.ga.ibone.comcast.net (68.86.85.10)  51.819 ms  51.451 ms  54.761 ms\n10  be-11424-cr02.dallas.tx.ibone.comcast.net (68.86.85.22)  68.878 ms  70.202 ms  74.977 ms\n11  be-11524-cr02.losangeles.ca.ibone.comcast.net (68.86.87.173)  99.990 ms  99.257 ms  95.023 ms\n12  he-0-0-0-0-pe01.losangeles.ca.ibone.comcast.net (68.86.85.14)  94.996 ms  109.233 ms  99.985 ms\n13  50.248.117.82 (50.248.117.82)  92.609 ms  94.291 ms  92.778 ms\n14  121.78.120.225 (121.78.120.225)  94.021 ms  96.099 ms  96.147 ms\n15  * * \n16  * * \n17  * * \n18  * * \n19  * * *\nI thought that it was an aws-sdk issue as everything seemed to work fine through the console. It looks like it\u2019s something else.\nThx\nEnis Konuk\nenis@qwiklab.com\n978-760-0732\n\nOn Feb 18, 2016, at 2:31 PM, Trevor Rowe notifications@github.com wrote:\nI just ran the following code without issue:\ns3 = Aws::S3::Client.new(http_wire_trace:true, region:'ap-northeast-2')\ns3.list_objects(bucket:'aws-sdk-ap-northeast-2')\nWhich generated the following HTTP wire trace:\nopening connection to aws-sdk-ap-northeast-2.s3.ap-northeast-2.amazonaws.com:443...\nopened\nstarting SSL for aws-sdk-ap-northeast-2.s3.ap-northeast-2.amazonaws.com:443...\nSSL established\n<- \"GET /?encoding-type=url HTTP/1.1\\r\\nContent-Type: \\r\\nAccept-Encoding: \\r\\nUser-Agent: aws-sdk-ruby2/2.2.17 ruby/2.2.2 x86_64-darwin13\\r\\nX-Amz-Date: 20160218T192931Z\\r\\nHost: aws-sdk-ap-northeast-2.s3.ap-northeast-2.amazonaws.com\\r\\nX-Amz-Content-Sha256: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\\r\\nAuthorization: AWS4-HMAC-SHA256 Credential=AKIAJUNH63P3WCTAYHFA/20160218/ap-northeast-2/s3/aws4_request, SignedHeaders=host;x-amz-content-sha256;x-amz-date, Signature=6c0f73f010cb260de922d5194305840f7d1d7759b332c4113dc43d4fef9596f6\\r\\nContent-Length: 0\\r\\nAccept: /\\r\\n\\r\\n\"\n-> \"HTTP/1.1 200 OK\\r\\n\"\n-> \"x-amz-id-2: 73iBOoaEYCcsMQGt5fYC/Lde2l4YL9GB9lHE/TiFY6ARlONIF43puCH1bExXxfDLfjeRiVfmke0=\\r\\n\"\n-> \"x-amz-request-id: B780DC7D2C8A8819\\r\\n\"\n-> \"Date: Thu, 18 Feb 2016 19:29:33 GMT\\r\\n\"\n-> \"x-amz-bucket-region: ap-northeast-2\\r\\n\"\n-> \"Content-Type: application/xml\\r\\n\"\n-> \"Transfer-Encoding: chunked\\r\\n\"\n-> \"Server: AmazonS3\\r\\n\"\n-> \"\\r\\n\"\n-> \"118\\r\\n\"\nreading 280 bytes...\n-> \"<?xml version=\\\"1.0\\\" encoding=\\\"UTF-8\\\"?>\\naws-sdk-ap-northeast-21000urlfalse\"\nread 280 bytes\nreading 2 bytes...\n-> \"\\r\\n\"\nread 2 bytes\n-> \"0\\r\\n\"\n-> \"\\r\\n\"\nConn keep-alive\nI'm not sure why Net::HTTP is unable to open a connection to the endpoint. Can you ping s3.ap-northeast-2.amazonaws.com from the same host?\n\u2014\nReply to this email directly or view it on GitHub https://github.com/aws/aws-sdk-ruby/issues/1096#issuecomment-185881031.\n. You're absolutely right @awood45. I feel like a dufus. . Got it.... That fixed it. I think this makes it a documentation problem for the initializer as the endpoint is now a required field.. \n",
    "antpaw": "sorry my fault libxml2 wasn't installed\n. ",
    "swapab": "Thanks @trevorrowe \nI realized that the connection was opened while pushing the job into queue.\nWhich can't be used during actual processing of the job. Re-initializing AWS::S3 did worked for me :)\nI think I can close this issue.\n. ",
    "nishiyamaosamu": "Sorry, not collect. Close this.\n. Thank you for the beginner\n. ",
    "ezkl": "Sorry, @lsegal. Neglected to search the closed issues.\n. ",
    "jhawthorn": ":+1: Awesome! Thanks.\n. ",
    "kirillsalykin": "Thanks for fast fix!\n. ",
    "sgessa": "Using aws-sdk-core:\nhttp://pastebin.com/raw.php?i=d1TKu1iP\nError: no member 'domain_name' in struct\n. second line raises an error so resp is nil\n. ",
    "miketheman": "@trevorrowe The ChangeLog looks to have stagnated since 2013 - and there's been some releases since then. Any chance for an update on what's going into these?\n. Thanks!\n. :+1: Thanks!\n. ",
    "afhammad": "That works like a charm, thanks Trevor.\nThe content type is not returned but I guess I can check the file extension. \n. @trevorrowe coming back to this, I'm struggling to find an efficient way of transforming the flat files array to a nested tree (similar to the originally posted one, however multiple layers of nesting) based on the paths. Any pointers appreciated!\n. I had to refactor it for my needs but that helped a lot, thanks Trevor!\n. ",
    "reiz": "OK. My fault :-) This is how it works: \nurl = AWS.s3.buckets['veye_projects'].objects.first.url_for(:read, :secure => true)\nSmall break + fresh air + coffee is always a good idea to solve a problem :-D \n. @trevorrowe Awesome! Thanks for the fast response! \n. ",
    "askreet": "Upon further investigation and code diving I've found that this was done on purpose 3 months ago because the new API didn't work with the abstraction layer that is provided to Ruby.  This is very inconvenient :-:.  I've already written my code using the abstraction layer so now I cannot use DynamoDB Local for testing.\n. Did this actually allow you to use the high level abstractions?  See https://github.com/aws/aws-sdk-ruby/commit/31569f635c3f2c4fa5a4556d9915fb5b0362cbf1 where this was put in on purpose because the high level abstractions did not fully work with the new API.\n. ",
    "demirhanaydin": "I opened a pull request about this issue, you can find here #430 \n. ",
    "cwarner-mdsol": "I'm still getting this limitation with aws-sdk-core-2.0.0.rc4 with the \"Only\nautomated snapshots can be copied.\" error.\n/Library/Ruby/Gems/2.0.0/gems/aws-sdk-core-2.0.0.rc4/vendor/seahorse/lib/seahorse/client/plugins/raise_response_errors.rb:15:in\n\ncall': *Only automated snapshots can be copied.*(Aws::RDS::Errors::InvalidParameterValue)\nfrom\n/Library/Ruby/Gems/2.0.0/gems/aws-sdk-core-2.0.0.rc4/vendor/seahorse/lib/seahorse/client/plugins/param_conversion.rb:22:incall'\nfrom\n/Library/Ruby/Gems/2.0.0/gems/aws-sdk-core-2.0.0.rc4/vendor/seahorse/lib/seahorse/client/request.rb:70:in\nsend_request'\nfrom\n/Library/Ruby/Gems/2.0.0/gems/aws-sdk-core-2.0.0.rc4/vendor/seahorse/lib/seahorse/client/plugins/operation_methods.rb:42:inblock (2 levels) in add_operation_helpers'\nfrom ./copysnapshots-api.rb:85:in block in copy_snapshots'\nfrom ./copysnapshots-api.rb:70:ineach'\nfrom ./copysnapshots-api.rb:70:in copy_snapshots'\nfrom ./copysnapshots-api.rb:123:in'\n\nOn Thu, Jan 16, 2014 at 1:26 PM, Trevor Rowe notifications@github.comwrote:\n\nWe've added support for the 2013-09-09 API version for RDS. Could you try\nthis again to see if this removes the restriction?\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/aws/aws-sdk-ruby/issues/428#issuecomment-32502355\n.\n\n\nChristopher Warner\nCloud Engineer\nMedidata Solutions Worldwide\u00ae\n350 Hudson St, 7th Floor New York, NY 10014\ncwarner@mdsol.com | direct: +1 212-659-1177 | main: +1 212-918-1800\n. Last time I checked it was still an issue but that was a couple maybe 2-3 months ago and I haven't been following lately. I came up with a work around using the the cli tools which use boto all of which have been working pretty well.\n. I should discern that when I state using cli tools I mean https://github.com/aws/aws-cli and not the amazon rds command line tools.\n. I'll give it a spin and update on my end.\n. This is still not working for me, how exactly are you getting this to work? I'm using valid arns.  So the following:\n```\nrds = Aws::RDS.new(api_version: '2013-09-09')\nsource_arn = \"arn:aws:rds:us-east-1:1234567890:snapshot:snapshot-id-here\"\ndest_arn = \"arn:aws:rds:us-west-1:1234567890:snapshot:copied-snapshot-id-here\"\nrds.copy_db_snapshot({:source_db_snapshot_identifier => source_arn,\n                                        :target_db_snapshot_identifier => dest_arn})\n```\nGets me:\n/Library/Ruby/Gems/2.0.0/gems/aws-sdk-core-2.0.0.rc9/vendor/seahorse/lib/seahorse/client/plugins/raise_response_errors.rb:15:in `call': Only automated snapshots can be copied. (Aws::RDS::Errors::InvalidParameterValue)\nAfter going through the updated library doc and digging through the code it seems this is ONLY available for automated snapshots. I'm just not sure why this is a constraint though; so following that this can be marked closed. I've found a work around as noted anyway but if you could outline how you got it working with rc9?\n. Unfortunately it doesn't, you're losing me.. why would authenticating in the target region matter? I'm just lost as what that has to do with the copy_db_snapshot method.. The arns are used to decipher and include my account info. The constraints as stand say the snapshot has to be automated but the http doc states that it can be manual or automated. It works with other tools which leads me to believe something is going on higher up the stack maybe in the serializer or somewhere else.. \nAre you actually doing this with rc9 as stated? I'd just like to be clear about the issue.\n. Works a champ thanks for the clarification Trevor, for the CLI I've been using:\nrds-copy-db-snapshot --source-db-snapshot-identifier source_arn --region dest_region --target-db-snapshot-identifier whatever_dest_id\n. Yeah the CLI braces for broken thinking with the dest_region flag. If I had fully read the API Version 2013-09-09 user guide I would have seen that.. Anyway thanks again and my bad on this one.\n. ",
    "molekilla": "Has this been fixed (copying manual to other regions) ?\n. I tested this and it seems to work with API 2013-09-09 if you do Copy a manual snapshot to other region. If you try the same region you do get 'Only automated...' error.\n. The trick here is authenticating in the target region\n``` ruby\nalso add the Target endpoint region, i.e. US WEST 1 endpoint\nrds = Aws::RDS.new(api_version: '2013-09-09')\nsource_arn = \"arn:aws:rds:us-east-1:1234567890:snapshot:snapshot-id-here\"\ndest_arn = \"arn:aws:rds:us-west-1:1234567890:snapshot:copied-snapshot-id-here\"\nrds.copy_db_snapshot({:source_db_snapshot_identifier => source_arn,\n                                        :target_db_snapshot_identifier => dest_arn})\n```\nHope that helps\n. ",
    "mtrimmer": "```\nopening connection to route53.amazonaws.com...\nopened\n<- \"GET /2012-12-12/hostedzone HTTP/1.1\\r\\nContent-Type: \\r\\nAccept-Encoding: \\r\\nUser-Agent: aws-sdk-ruby/1.29.1 ruby/1.9.3 x86_64-linux\\r\\nDate: Thu, 16 Jan 2014 19:08:37 GMT\\r\\nX-Amzn-Authorization: AWS3-HTTPS AWSAccessKeyId=****,Algorithm=HmacSHA256,Signature=******\\r\\nAccept: /\\r\\nHost: route53.amazonaws.com\\r\\n\\r\\n\"\n-> \"HTTP/1.1 200 OK\\r\\n\"\n-> \"x-amzn-RequestId: 9a7272fc-7ee1-11e3-9add-f35e9c3aa272\\r\\n\"\n-> \"Content-Type: text/xml\\r\\n\"\n-> \"Content-Length: 683\\r\\n\"\n-> \"Date: Thu, 16 Jan 2014 19:08:36 GMT\\r\\n\"\n-> \"\\r\\n\"\nreading 683 bytes...\n-> \"<?xml version=\\\"1.0\\\"?>\\n/hostedzone/Z1SZ5VH6ZA3NMEaws.fake.com.83BD3036-C8C1-502C-93DD-22ABFB2507B03/hostedzone/Z11W4T7OJA2AECfake.fake.com.BCD2F440-225D-0E1C-AADA-B433C6A6FE6FFor Testing2false100\"\nread 683 bytes\nConn keep-alive\nDelegation Set For aws.fake.com.\nDelegation Set For fake.fake.com.\n```\n. ",
    "danoyoung": "to fast for me...awesome!\nOn Fri, Dec 20, 2013 at 2:45 PM, Trevor Rowe notifications@github.comwrote:\n\nVersion 1.31.1 is out with the fix.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/aws/aws-sdk-ruby/issues/431#issuecomment-31044132\n.\n\n\nBet on Cowboys, Not Horses....\n. ",
    "nllong": "I was unaware of the V2 effort.  Sounds like that is the path to take as multi_json is exactly what i need and only need support for Ruby > 1.9.  Thanks!\n. ",
    "fukayatsu": "+1\n. ",
    "davidguthu": ":+1:\n. ",
    "vonkoch": "Could you try v1.32.0? It should fix the issue.\n. ",
    "tongueroo": "FYI, the build is failing due to a rubygems issue.  3 of the 4 builds are passing.  It also passes locally for me.\n. @trevorrowe thanks for the adding the specs!  I'll use that as an example of specs for next time. \n. @rdark I ran into the same error and did not know it was coming from lono.  Sorry about that! I've removed the extension as it no longer needed since ruby sorts the output of a the generated json now.\n. Interesting, the Resource api returns the right number of instances.\nAws> resource = Aws::EC2::Resource.new\n[Aws::STS::Client 200 0.586834 0 retries] assume_role(role_session_name:\"default_session\",role_arn:\"arn:aws:iam::994926937775:role/Admin\",external_id:nil,serial_number:nil)  \n[Aws::STS::Client 200 0.1684 0 retries] assume_role(role_session_name:\"default_session\",role_arn:\"arn:aws:iam::994926937775:role/Admin\",external_id:nil,serial_number:nil)  \n=> #<Aws::EC2::Resource>\nAws> resource.instances.count\n[Aws::EC2::Client 200 0.393117 0 retries] describe_instances()  \n=> 4\nAws>. @cjyclaire Thank you so much!  That was exactly the issue. I had a feeling that I was doing something silly.\nHere's an example of it working. \nsh\nAws> ec2.describe_instances.reservations.each do |r|\nAws|   r.instances.each {|i| puts i.instance_id}  \nAws| end; nil  \n[Aws::EC2::Client 200 0.277981 0 retries] describe_instances()  \ni-09482b1a6e330fbf7\ni-066b140d9479e9681\n=> nil\nAws>\nNOTE: I'm back down to 2 instances.. ",
    "FrankStienhans2": "I have provided to the team a cloud formation script for reproducing the problem.\nIt is my understanding that it is a confirmed issue.\n. OK - I will attach the cf template.\nAnd instructions.\nThanks,\nFrank\nOn Jan 17, 2014 8:20 PM, \"Trevor Rowe\" notifications@github.com wrote:\n\nWithout a HTTP wirelog or reproduction case there is nothing we can do to\nresolve this issue. If you can provide more information, then we can look\ninto this.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/aws/aws-sdk-ruby/issues/438#issuecomment-32674186\n.\n. I found the problem - I do not think it is a bug.\n. \n",
    "untidy-hair": "Hi, @lsegal. That's true! That totally slipped my mind... I was dumb. Thank you.\n. ",
    "lardcanoe": "Great question. Can't say for sure how standard this is, though I suspect it isn't.\nIf you aren't happy with the above, since I can't guarantee that it is standard, would you be ok with an approach that exposes a function to override the SRC var so that I can do that from our code, and not have to monkey patch that var?\n. I'm going to collect data overnight to see if it is a certain region.\n. Over 95% of these errors are in ap-southeast-2. Now why 2.1.36 reports this whereas 2.0.33 doesn't is beyond me, but I'm not gonna worry about it. Just another one to consider retryable.\nThanks.\n. ",
    "jumbalaya-wanton": "Fixed. The URL in the s3 form was pointing to s3 instead of s3-sa-east-1. The format must also be https://s3-region.amazonaws.com/bucket.name for this to work.\nPS: Setting secure: false wont matter since the OPTIONS request goes to the address you specify in the form.\n. ",
    "khadrin": "Sure, will try. Thanks!\n. Hi. Test case for you:\n$ bundle exec bin/aws-sdk-issue-448 testcase\nAWS::VERSION=(1.32.0)\nwaiting for cloudformation to create stack (testcase)\nwaiting for cloudformation to create stack (testcase)\nwaiting for cloudformation to create stack (testcase)\nwaiting for cloudformation to create stack (testcase)\nwaiting for cloudformation to create stack (testcase)\ndumping resources for stack ({#stack_name})\nAWS::EC2::SecurityGroup\nInstanceSecurityGroup\nundefined method `name' for <AWS::CloudFormation::StackResourceCollection>:AWS::CloudFormation::StackResourceCollection\n/home/stsmith/.rbenv/versions/1.9.3-p448/lib/ruby/gems/1.9.1/gems/aws-sdk-1.32.0/lib/aws/cloud_formation/stack_resource.rb:108:in `resource_identifiers'\n/home/stsmith/.rbenv/versions/1.9.3-p448/lib/ruby/gems/1.9.1/gems/aws-sdk-1.32.0/lib/aws/core/resource.rb:100:in `resource_options'\n/home/stsmith/.rbenv/versions/1.9.3-p448/lib/ruby/gems/1.9.1/gems/aws-sdk-1.32.0/lib/aws/cloud_formation/stack_resource.rb:112:in `get_resource'\n/home/stsmith/.rbenv/versions/1.9.3-p448/lib/ruby/gems/1.9.1/gems/aws-sdk-1.32.0/lib/aws/core/resource.rb:235:in `block (2 levels) in define_attribute_getter'\n/home/stsmith/.rbenv/versions/1.9.3-p448/lib/ruby/gems/1.9.1/gems/aws-sdk-1.32.0/lib/aws/core/cacheable.rb:63:in `retrieve_attribute'\n/home/stsmith/.rbenv/versions/1.9.3-p448/lib/ruby/gems/1.9.1/gems/aws-sdk-1.32.0/lib/aws/core/resource.rb:235:in `block in define_attribute_getter'\nbin/aws-sdk-issue-448:59:in `block in <main>'\n/home/stsmith/.rbenv/versions/1.9.3-p448/lib/ruby/gems/1.9.1/gems/aws-sdk-1.32.0/lib/aws/core/collection.rb:48:in `each'\n/home/stsmith/.rbenv/versions/1.9.3-p448/lib/ruby/gems/1.9.1/gems/aws-sdk-1.32.0/lib/aws/core/collection.rb:48:in `block in each'\n/home/stsmith/.rbenv/versions/1.9.3-p448/lib/ruby/gems/1.9.1/gems/aws-sdk-1.32.0/lib/aws/core/collection/simple.rb:74:in `_each_batch'\n/home/stsmith/.rbenv/versions/1.9.3-p448/lib/ruby/gems/1.9.1/gems/aws-sdk-1.32.0/lib/aws/core/collection.rb:80:in `each_batch'\n/home/stsmith/.rbenv/versions/1.9.3-p448/lib/ruby/gems/1.9.1/gems/aws-sdk-1.32.0/lib/aws/core/collection.rb:47:in `each'\nbin/aws-sdk-issue-448:56:in `<main>'\ndeleting stack (testcase)\n```\n$ cat bin/aws-sdk-issue-448                                                                                                       \n!/usr/bin/env ruby\nrequire 'aws-sdk'\nstack_name = ARGV.shift\nif !stack_name\n  puts \"Usage: cancel-stack-update STACK_NAME\"\n  exit 1\nend\nputs \"AWS::VERSION=(#{AWS::VERSION})\"\ntemplate = <<-TEMPLATE\n{\n  \"Resources\":\n  {\n    \"InstanceSecurityGroup\":\n    {\n      \"Type\":\"AWS::EC2::SecurityGroup\",\n      \"Properties\":\n      {\n        \"GroupDescription\":\"Allow 80 from anywhere.\",\n        \"SecurityGroupIngress\":[\n          {\n            \"IpProtocol\":\"tcp\",\n            \"CidrIp\":\"0.0.0.0/0\",\n            \"FromPort\":80,\n            \"ToPort\":80\n          }\n        ]\n      }\n    }\n  }\n}\nTEMPLATE\ncfn = AWS::CloudFormation.new\nstack = cfn.stacks.create(stack_name, template)\nstatus = ''\nbegin\n  loop do\n    break unless stack.status =~ /(CREATE|UPDATE)_IN_PROGRESS$/\n    puts \"waiting for cloudformation to create stack (#{stack_name})\"\n    sleep 5\n  end\nrescue AWS::CloudFormation::Errors::Throttling\n  puts \"API request throttled trying to get stack (#{stack_name}) status.\"\n  retry\nend\nbegin\n  puts \"dumping resources for stack ({#stack_name})\"\n  stack.resources.each do |resource|\n    puts resource.resource_type\n    puts resource.logical_resource_id\n    puts resource.last_updated_timestamp\n    puts \"---\"\n  end\nrescue\n  puts $!, $@\nend\nputs \"deleting stack (#{stack_name})\"\ncfn.client.delete_stack({'stack_name' => stack_name})\nexit\n```\n. Any update on this? Need any more info?\n. ",
    "acnalesso": "Hi there,\nI am having the same issue here as well.\nAny chance somebody help us?\nThanks\n. This has been fixed on version 1.34.1 .\nIt works like a charm now :)\n. ",
    "tmornini": "Hello, thanks for your quick response!\nThe issue happens occasionally, but but not infrequently. :-)\nWhen I see it, it seems to come in spurts, probably from being inside a retry handler...\nI believe I've seen this on both 2.1.0 and JRuby 1.7.10.\nI do not have a simple reproducible code sample.\nI'm using Celluloid, which may imply threading issues, though the way the code is written, it's highly unlikely that the same client would be operated upon by multiple threads.\nThe SDK is loaded via:\n```\nrequire 'aws-sdk'\nAWS.eager_autoload! AWS::Core\nAWS.eager_autoload! AWS::DynamoDB\nAWS.eager_autoload! AWS::SQS\nAWS.eager_autoload! AWS::S3\n```\nLoren@AWS on the AWS forum suggested that I try:\n```\nrequire 'aws-sdk'\nAWS.eager_autoload!\n```\nI did, and that made no difference.\nI took a look at the line referenced in the error.\nI'm not 100% clear on how that code works, but perhaps it should be guarded by a defined? check?\n. Hey Loren@AWS :-)\nI'm pretty sure I've seen it in 2.1.0, but not 100%\nWhen I see it, it tends to come in batches. All my AWS calls are wrapped in a retry handler, I've assumed it was due to that.\n. Here's another we just noticed, exact same situation.\n``\nNoMethodError: undefined methodsearch' for #\n    /Users/tmornini/.rbenv/versions/1.9.3-p551/lib/ruby/gems/1.9.1/gems/jmespath-1.0.2/lib/jmespath.rb:32:in search'\n    /Users/tmornini/.rbenv/versions/1.9.3-p551/lib/ruby/gems/1.9.1/gems/aws-sdk-resources-2.0.47/lib/aws-sdk-resources/builder_sources.rb:71:inextract'\n    /Users/tmornini/.rbenv/versions/1.9.3-p551/lib/ruby/gems/1.9.1/gems/aws-sdk-resources-2.0.47/lib/aws-sdk-resources/builder.rb:37:in block in build'\n    /Users/tmornini/.rbenv/versions/1.9.3-p551/lib/ruby/gems/1.9.1/gems/aws-sdk-resources-2.0.47/lib/aws-sdk-resources/builder.rb:36:ineach'\n    /Users/tmornini/.rbenv/versions/1.9.3-p551/lib/ruby/gems/1.9.1/gems/aws-sdk-resources-2.0.47/lib/aws-sdk-resources/builder.rb:36:in with_object'\n    /Users/tmornini/.rbenv/versions/1.9.3-p551/lib/ruby/gems/1.9.1/gems/aws-sdk-resources-2.0.47/lib/aws-sdk-resources/builder.rb:36:inbuild'\n    /Users/tmornini/.rbenv/versions/1.9.3-p551/lib/ruby/gems/1.9.1/gems/aws-sdk-resources-2.0.47/lib/aws-sdk-resources/operations.rb:87:in call'\n    /Users/tmornini/.rbenv/versions/1.9.3-p551/lib/ruby/gems/1.9.1/gems/aws-sdk-resources-2.0.47/lib/aws-sdk-resources/operation_methods.rb:19:inblock in add_operation'\n... through our code ...\n/Users/tmornini/.rbenv/versions/1.9.3-p551/lib/ruby/gems/1.9.1/gems/celluloid-0.16.0/lib/celluloid/calls.rb:26:in `public_send'\n/Users/tmornini/.rbenv/versions/1.9.3-p551/lib/ruby/gems/1.9.1/gems/celluloid-0.16.0/lib/celluloid/calls.rb:26:in `dispatch'\n/Users/tmornini/.rbenv/versions/1.9.3-p551/lib/ruby/gems/1.9.1/gems/celluloid-0.16.0/lib/celluloid/calls.rb:63:in `dispatch'\n/Users/tmornini/.rbenv/versions/1.9.3-p551/lib/ruby/gems/1.9.1/gems/celluloid-0.16.0/lib/celluloid/cell.rb:60:in `block in invoke'\n/Users/tmornini/.rbenv/versions/1.9.3-p551/lib/ruby/gems/1.9.1/gems/celluloid-0.16.0/lib/celluloid/cell.rb:71:in `block in task'\n/Users/tmornini/.rbenv/versions/1.9.3-p551/lib/ruby/gems/1.9.1/gems/celluloid-0.16.0/lib/celluloid/actor.rb:357:in `block in task'\n/Users/tmornini/.rbenv/versions/1.9.3-p551/lib/ruby/gems/1.9.1/gems/celluloid-0.16.0/lib/celluloid/tasks.rb:57:in `block in initialize'\n/Users/tmornini/.rbenv/versions/1.9.3-p551/lib/ruby/gems/1.9.1/gems/celluloid-0.16.0/lib/celluloid/tasks/task_fiber.rb:15:in `block in create'\n(celluloid):0:in `remote procedure call'\n/Users/tmornini/.rbenv/versions/1.9.3-p551/lib/ruby/gems/1.9.1/gems/celluloid-0.16.0/lib/celluloid/calls.rb:92:in `value'\n/Users/tmornini/.rbenv/versions/1.9.3-p551/lib/ruby/gems/1.9.1/gems/celluloid-0.16.0/lib/celluloid/proxies/sync_proxy.rb:33:in `method_missing'\n/Users/tmornini/.rbenv/versions/1.9.3-p551/lib/ruby/gems/1.9.1/gems/celluloid-0.16.0/lib/celluloid/proxies/cell_proxy.rb:17:in `_send_'\n/Users/tmornini/.rbenv/versions/1.9.3-p551/lib/ruby/gems/1.9.1/gems/celluloid-0.16.0/lib/celluloid/pool_manager.rb:41:in `_send_'\n/Users/tmornini/.rbenv/versions/1.9.3-p551/lib/ruby/gems/1.9.1/gems/celluloid-0.16.0/lib/celluloid/pool_manager.rb:140:in `method_missing'\n/Users/tmornini/.rbenv/versions/1.9.3-p551/lib/ruby/gems/1.9.1/gems/celluloid-0.16.0/lib/celluloid/calls.rb:26:in `public_send'\n/Users/tmornini/.rbenv/versions/1.9.3-p551/lib/ruby/gems/1.9.1/gems/celluloid-0.16.0/lib/celluloid/calls.rb:26:in `dispatch'\n/Users/tmornini/.rbenv/versions/1.9.3-p551/lib/ruby/gems/1.9.1/gems/celluloid-0.16.0/lib/celluloid/calls.rb:122:in `dispatch'\n/Users/tmornini/.rbenv/versions/1.9.3-p551/lib/ruby/gems/1.9.1/gems/celluloid-0.16.0/lib/celluloid/cell.rb:60:in `block in invoke'\n/Users/tmornini/.rbenv/versions/1.9.3-p551/lib/ruby/gems/1.9.1/gems/celluloid-0.16.0/lib/celluloid/cell.rb:71:in `block in task'\n/Users/tmornini/.rbenv/versions/1.9.3-p551/lib/ruby/gems/1.9.1/gems/celluloid-0.16.0/lib/celluloid/actor.rb:357:in `block in task'\n/Users/tmornini/.rbenv/versions/1.9.3-p551/lib/ruby/gems/1.9.1/gems/celluloid-0.16.0/lib/celluloid/tasks.rb:57:in `block in initialize'\n/Users/tmornini/.rbenv/versions/1.9.3-p551/lib/ruby/gems/1.9.1/gems/celluloid-0.16.0/lib/celluloid/tasks/task_fiber.rb:15:in `block in create'\n\n```\n. @trevorrowe This code base has severe memory growth issues on 2.0 or later.\n. @trevorrowe Thanks. Does that exist in 2.2.2 as well? Because we have the same issue in both, and I'm pretty confident the problem is in our code, and we leak with the AWS SDK v1 as well...\nHere's another spurious issue that we've only seen once. Including it here to be helpful, not pedantic. :-)\n``\nNameError: uninitialized constant Aws::S3::BUCKET_REGIONS\n  /Users/tmornini/.rbenv/versions/1.9.3-p551/lib/ruby/gems/1.9.1/gems/aws-sdk-core-2.0.47/lib/aws-sdk-core/plugins/s3_request_signer.rb:130:inuse_regional_endpoint_when_known'\n  /Users/tmornini/.rbenv/versions/1.9.3-p551/lib/ruby/gems/1.9.1/gems/aws-sdk-core-2.0.47/lib/aws-sdk-core/plugins/s3_request_signer.rb:122:in call'\n  /Users/tmornini/.rbenv/versions/1.9.3-p551/lib/ruby/gems/1.9.1/gems/aws-sdk-core-2.0.47/lib/aws-sdk-core/plugins/s3_redirects.rb:15:incall'\n  /Users/tmornini/.rbenv/versions/1.9.3-p551/lib/ruby/gems/1.9.1/gems/aws-sdk-core-2.0.47/lib/aws-sdk-core/plugins/retry_errors.rb:88:in call'\n  /Users/tmornini/.rbenv/versions/1.9.3-p551/lib/ruby/gems/1.9.1/gems/aws-sdk-core-2.0.47/lib/aws-sdk-core/plugins/s3_md5s.rb:33:incall'\n  /Users/tmornini/.rbenv/versions/1.9.3-p551/lib/ruby/gems/1.9.1/gems/aws-sdk-core-2.0.47/lib/aws-sdk-core/plugins/s3_expect_100_continue.rb:21:in call'\n  /Users/tmornini/.rbenv/versions/1.9.3-p551/lib/ruby/gems/1.9.1/gems/aws-sdk-core-2.0.47/lib/aws-sdk-core/rest_body_handler.rb:9:incall'\n  /Users/tmornini/.rbenv/versions/1.9.3-p551/lib/ruby/gems/1.9.1/gems/aws-sdk-core-2.0.47/lib/aws-sdk-core/plugins/user_agent.rb:12:in call'\n  /Users/tmornini/.rbenv/versions/1.9.3-p551/lib/ruby/gems/1.9.1/gems/aws-sdk-core-2.0.47/lib/seahorse/client/plugins/restful_bindings.rb:13:incall'\n  /Users/tmornini/.rbenv/versions/1.9.3-p551/lib/ruby/gems/1.9.1/gems/aws-sdk-core-2.0.47/lib/seahorse/client/plugins/endpoint.rb:31:in call'\n  /Users/tmornini/.rbenv/versions/1.9.3-p551/lib/ruby/gems/1.9.1/gems/aws-sdk-core-2.0.47/lib/seahorse/client/plugins/param_validation.rb:22:incall'\n  /Users/tmornini/.rbenv/versions/1.9.3-p551/lib/ruby/gems/1.9.1/gems/aws-sdk-core-2.0.47/lib/seahorse/client/plugins/raise_response_errors.rb:14:in call'\n  /Users/tmornini/.rbenv/versions/1.9.3-p551/lib/ruby/gems/1.9.1/gems/aws-sdk-core-2.0.47/lib/aws-sdk-core/plugins/s3_sse_cpk.rb:18:incall'\n  /Users/tmornini/.rbenv/versions/1.9.3-p551/lib/ruby/gems/1.9.1/gems/aws-sdk-core-2.0.47/lib/seahorse/client/plugins/param_conversion.rb:22:in call'\n  /Users/tmornini/.rbenv/versions/1.9.3-p551/lib/ruby/gems/1.9.1/gems/aws-sdk-core-2.0.47/lib/aws-sdk-core/plugins/response_paging.rb:10:incall'\n  /Users/tmornini/.rbenv/versions/1.9.3-p551/lib/ruby/gems/1.9.1/gems/aws-sdk-core-2.0.47/lib/seahorse/client/plugins/response_target.rb:18:in call'\n  /Users/tmornini/.rbenv/versions/1.9.3-p551/lib/ruby/gems/1.9.1/gems/aws-sdk-core-2.0.47/lib/seahorse/client/request.rb:70:insend_request'\n  /Users/tmornini/.rbenv/versions/1.9.3-p551/lib/ruby/gems/1.9.1/gems/aws-sdk-core-2.0.47/lib/seahorse/client/base.rb:216:in block (2 levels) in define_operation_methods'\n  /Users/tmornini/.rbenv/versions/1.9.3-p551/lib/ruby/gems/1.9.1/gems/aws-sdk-resources-2.0.47/lib/aws-sdk-resources/request.rb:24:incall'\n  /Users/tmornini/.rbenv/versions/1.9.3-p551/lib/ruby/gems/1.9.1/gems/aws-sdk-resources-2.0.47/lib/aws-sdk-resources/operations.rb:41:in call'\n  /Users/tmornini/.rbenv/versions/1.9.3-p551/lib/ruby/gems/1.9.1/gems/aws-sdk-resources-2.0.47/lib/aws-sdk-resources/operations.rb:87:incall'\n  /Users/tmornini/.rbenv/versions/1.9.3-p551/lib/ruby/gems/1.9.1/gems/aws-sdk-resources-2.0.47/lib/aws-sdk-resources/operation_methods.rb:19:in `block in add_operation'\n... through our code ...\n/Users/tmornini/.rbenv/versions/1.9.3-p551/lib/ruby/gems/1.9.1/gems/celluloid-0.16.0/lib/celluloid/calls.rb:26:in public_send'\n  /Users/tmornini/.rbenv/versions/1.9.3-p551/lib/ruby/gems/1.9.1/gems/celluloid-0.16.0/lib/celluloid/calls.rb:26:indispatch'\n  /Users/tmornini/.rbenv/versions/1.9.3-p551/lib/ruby/gems/1.9.1/gems/celluloid-0.16.0/lib/celluloid/calls.rb:63:in dispatch'\n  /Users/tmornini/.rbenv/versions/1.9.3-p551/lib/ruby/gems/1.9.1/gems/celluloid-0.16.0/lib/celluloid/cell.rb:60:inblock in invoke'\n  /Users/tmornini/.rbenv/versions/1.9.3-p551/lib/ruby/gems/1.9.1/gems/celluloid-0.16.0/lib/celluloid/cell.rb:71:in block in task'\n  /Users/tmornini/.rbenv/versions/1.9.3-p551/lib/ruby/gems/1.9.1/gems/celluloid-0.16.0/lib/celluloid/actor.rb:357:inblock in task'\n  /Users/tmornini/.rbenv/versions/1.9.3-p551/lib/ruby/gems/1.9.1/gems/celluloid-0.16.0/lib/celluloid/tasks.rb:57:in block in initialize'\n  /Users/tmornini/.rbenv/versions/1.9.3-p551/lib/ruby/gems/1.9.1/gems/celluloid-0.16.0/lib/celluloid/tasks/task_fiber.rb:15:inblock in create'\n```\n. @trevorrowe Thanks\n. This seems to be working better, but not yet perfect.\nUsing 2.1.0, and calling:\nrequire 'aws-sdk'\nAws.eager_autoload!\nwe're still seeing (at least) this one exception:\n``\nNoMethodError: undefined methodnew' for nil:NilClass\n    /Users/tmornini/.rbenv/versions/1.9.3-p551/lib/ruby/gems/1.9.1/gems/aws-sdk-core-2.1.0/lib/aws-sdk-core/xml/parser.rb:28:in parse'\n    /Users/tmornini/.rbenv/versions/1.9.3-p551/lib/ruby/gems/1.9.1/gems/aws-sdk-core-2.1.0/lib/aws-sdk-core/query/handler.rb:53:inparse_xml'\n    /Users/tmornini/.rbenv/versions/1.9.3-p551/lib/ruby/gems/1.9.1/gems/aws-sdk-core-2.1.0/lib/aws-sdk-core/query/handler.rb:29:in block in call'\n    /Users/tmornini/.rbenv/versions/1.9.3-p551/lib/ruby/gems/1.9.1/gems/aws-sdk-core-2.1.0/lib/seahorse/client/response.rb:43:incall'\n    /Users/tmornini/.rbenv/versions/1.9.3-p551/lib/ruby/gems/1.9.1/gems/aws-sdk-core-2.1.0/lib/seahorse/client/response.rb:43:in block in on'\n    /Users/tmornini/.rbenv/versions/1.9.3-p551/lib/ruby/gems/1.9.1/gems/aws-sdk-core-2.1.0/lib/seahorse/client/http/response.rb:137:inblock in on_success'\n    /Users/tmornini/.rbenv/versions/1.9.3-p551/lib/ruby/gems/1.9.1/gems/aws-sdk-core-2.1.0/lib/seahorse/client/http/response.rb:164:in call'\n    /Users/tmornini/.rbenv/versions/1.9.3-p551/lib/ruby/gems/1.9.1/gems/aws-sdk-core-2.1.0/lib/seahorse/client/http/response.rb:164:inblock in listener'\n    /Users/tmornini/.rbenv/versions/1.9.3-p551/lib/ruby/gems/1.9.1/gems/aws-sdk-core-2.1.0/lib/seahorse/client/http/response.rb:128:in call'\n    /Users/tmornini/.rbenv/versions/1.9.3-p551/lib/ruby/gems/1.9.1/gems/aws-sdk-core-2.1.0/lib/seahorse/client/http/response.rb:128:inon_done'\n    /Users/tmornini/.rbenv/versions/1.9.3-p551/lib/ruby/gems/1.9.1/gems/aws-sdk-core-2.1.0/lib/seahorse/client/http/response.rb:135:in on_success'\n    /Users/tmornini/.rbenv/versions/1.9.3-p551/lib/ruby/gems/1.9.1/gems/aws-sdk-core-2.1.0/lib/seahorse/client/response.rb:42:inon'\n    /Users/tmornini/.rbenv/versions/1.9.3-p551/lib/ruby/gems/1.9.1/gems/aws-sdk-core-2.1.0/lib/seahorse/client/response.rb:51:in on_success'\n    /Users/tmornini/.rbenv/versions/1.9.3-p551/lib/ruby/gems/1.9.1/gems/aws-sdk-core-2.1.0/lib/aws-sdk-core/query/handler.rb:27:incall'\n    /Users/tmornini/.rbenv/versions/1.9.3-p551/lib/ruby/gems/1.9.1/gems/aws-sdk-core-2.1.0/lib/aws-sdk-core/plugins/user_agent.rb:12:in call'\n    /Users/tmornini/.rbenv/versions/1.9.3-p551/lib/ruby/gems/1.9.1/gems/aws-sdk-core-2.1.0/lib/seahorse/client/plugins/endpoint.rb:34:incall'\n    /Users/tmornini/.rbenv/versions/1.9.3-p551/lib/ruby/gems/1.9.1/gems/aws-sdk-core-2.1.0/lib/aws-sdk-core/plugins/param_validator.rb:21:in call'\n    /Users/tmornini/.rbenv/versions/1.9.3-p551/lib/ruby/gems/1.9.1/gems/aws-sdk-core-2.1.0/lib/seahorse/client/plugins/raise_response_errors.rb:14:incall'\n    /Users/tmornini/.rbenv/versions/1.9.3-p551/lib/ruby/gems/1.9.1/gems/aws-sdk-core-2.1.0/lib/aws-sdk-core/plugins/param_converter.rb:21:in call'\n    /Users/tmornini/.rbenv/versions/1.9.3-p551/lib/ruby/gems/1.9.1/gems/aws-sdk-core-2.1.0/lib/seahorse/client/plugins/response_target.rb:18:incall'\n    /Users/tmornini/.rbenv/versions/1.9.3-p551/lib/ruby/gems/1.9.1/gems/aws-sdk-core-2.1.0/lib/seahorse/client/request.rb:70:in send_request'\n    /Users/tmornini/.rbenv/versions/1.9.3-p551/lib/ruby/gems/1.9.1/gems/aws-sdk-core-2.1.0/lib/seahorse/client/base.rb:207:inblock (2 levels) in define_operation_methods'\n... through our code ...\n/Users/tmornini/.rbenv/versions/1.9.3-p551/lib/ruby/gems/1.9.1/gems/celluloid-0.16.0/lib/celluloid/calls.rb:26:in `public_send'\n/Users/tmornini/.rbenv/versions/1.9.3-p551/lib/ruby/gems/1.9.1/gems/celluloid-0.16.0/lib/celluloid/calls.rb:26:in `dispatch'\n/Users/tmornini/.rbenv/versions/1.9.3-p551/lib/ruby/gems/1.9.1/gems/celluloid-0.16.0/lib/celluloid/calls.rb:63:in `dispatch'\n/Users/tmornini/.rbenv/versions/1.9.3-p551/lib/ruby/gems/1.9.1/gems/celluloid-0.16.0/lib/celluloid/cell.rb:60:in `block in invoke'\n/Users/tmornini/.rbenv/versions/1.9.3-p551/lib/ruby/gems/1.9.1/gems/celluloid-0.16.0/lib/celluloid/cell.rb:71:in `block in task'\n/Users/tmornini/.rbenv/versions/1.9.3-p551/lib/ruby/gems/1.9.1/gems/celluloid-0.16.0/lib/celluloid/actor.rb:357:in `block in task'\n/Users/tmornini/.rbenv/versions/1.9.3-p551/lib/ruby/gems/1.9.1/gems/celluloid-0.16.0/lib/celluloid/tasks.rb:57:in `block in initialize'\n/Users/tmornini/.rbenv/versions/1.9.3-p551/lib/ruby/gems/1.9.1/gems/celluloid-0.16.0/lib/celluloid/tasks/task_fiber.rb:15:in `block in create'\n\n```\nI say \"at least\" because this behavior has always been non-deterministic.\n. @trevorrowe After a few hours of testing, I can confirm that the exception above is very repeatable in my case, and is the only such issue we've seen in many iterations.\n. @trevorrowe Thanks, looking forward to next release.\n. @trevorrowe Oh, apologies, made a bad assumption that it was my responsibility.\n. The tests are green locally and it appears the failures are all gem install failures and not this PR's changes...\n. @awood45 Options added\n. @awood45 Good call, sorry for being lazy. I actually had the semantics of max_attempts wrong...\n. Rebased on current master\n. @awood45 Done. After the rebase, specs now log:\nDEPRECATION WARNING: called deprecated batch method#deleteon a batch ofAws::S3::ObjectVersionobjects; use#batch_delete!instead\nA few quick pokes make me 80% certain that is unrelated to this work.\n. ",
    "YorickPeterse": "To clarify, we've been running the same code base under MRI for a good 2 months now without any of these errors popping up. Last Friday I switched things over to Rubinius and they've since started to occur. It could be a bug in Rubinius but the stack trace doesn't show any clear signs of this sadly.\n. And it seems disabling checksum validation doesn't do anything, the error just re-occurred.\n. Extra info: the error seems to occur when a thread encounters an error, kicks off the error handling process and is then re-started (using a retry). I'll see if I can set up a way to reproduce this.\n. Managed to reproduce it using the following script on Rubinius:\n``` ruby\nrequire 'aws'\nrequire 'thread'\nAWS.config(\n  :access_key_id     => '...SNIP...',\n  :secret_access_key => '...SNIP...',\n  :sqs_region        => 'eu-west-1',\n  :sqs_endpoint      => 'sqs.eu-west-1.amazonaws.com'\n)\nAWS.eager_autoload!(AWS::SQS)\nAWS.eager_autoload!(AWS::Core)\ndef do_work\n  sqs   = AWS::SQS.new\n  queue = sqs.queues.named('test')\nqueue.poll do |message|\n    sleep 2 # Fake some work\n    raise 'Fake error to showcase restarting of threads'\n  end\nend\nthreads = []\nmutex   = Mutex.new\n10.times do\n  threads << Thread.new do\n    begin\n      do_work\n    rescue => error\n      # Don't mess up console output\n      mutex.synchronize { puts \"Error: #{error.message}\" }\n  sleep 2\n  retry\nend\n\nend\nend\nthreads.each(&:join)\n``\n. It's worth noting that the behaviour is rather random. In some runs I can't reproduce it, some other runs it happens basically right away.\n. Any news on this? I'll be performing some extra tests today on the latest version of Rbx but I believe the issue still persists. Not yet sure yet if it's an issue with the AWS SDK or Rbx though.\n. Extra note: this issue still persists on Rbx 2.2.6. Not sure yet if it's an Rbx or aws-sdk issue.\n. Any news on this? I'd love to somehow fix this in whatever way we can in Rbx. However, I'm not familiar with the inner workings of the aws-sdk and have never seen anything like this outside of it.\n. I'm not aware of any thread-safety issues withOpenSSL::Digest`. The code used by Rubinius for this is the same code as used in MRI with some changes to make it compatible with Rbx. Assuming this were thread-unsafe it should also affect MRI, though the GIL probably prevents it from occuring. Having said that, it could be that indeed OpenSSL is somehow not thread-safe.\nIs there a way this can be tested without the whole aws-sdk stack? Since you mentioned that the various signatures match it could be that something else other than OpenSSL is interfering along the line.\n. @trevorrowe I haven't tried this yet with using OpenSSL::Digest instead of plain Digest. I'll give this a try tomorrow.\n. Sorry for the late reply. I gave this a shot using aws-sdk 1.43.2 and have not yet experienced the error. I'll do some further testing in the coming days to be absolutely sure it's gone.\n. Upon further testing this problem does seem to still occur on Rubinius 2.2.10 using aws-sdk 1.46.0. In this case I'm getting errors such as the following:\n```\nAWS::SQS::Errors::SignatureDoesNotMatch: The request signature we calculated does not match the signature you provided. Check your AWS Secret Access Key and signing method. Consult the service documentation for details.\nThe Canonical String for this request should have been\n'POST\n/345153512707/shepherd\ncontent-length:171\ncontent-type:application/x-www-form-urlencoded; charset=utf-8\nhost:sqs.eu-west-1.amazonaws.com\nuser-agent:aws-sdk-ruby/1.46.0 rbx/2.1.0 x86_64-linux-gnu\nx-amz-content-sha256:0edb693803f329fa2976405b32df33f1bd0600f5aeea7b5d3cd15a3bec7ab9ab\nx-amz-date:20140703T151611Z\ncontent-length;content-type;host;user-agent;x-amz-content-sha256;x-amz-date\n0edb693803f329fa2976405b32df33f1bd0600f5aeea7b5d3cd15a3bec7ab9ab'\nThe String-to-Sign should have been\n'AWS4-HMAC-SHA256\n20140703T151611Z\n20140703/eu-west-1/sqs/aws4_request\n7447fe83fa48a596773ebecb9503237a2e4614988f6bb696bbe26fed8b6394ef\n```\n. Contents of the Gemfile:\n``` ruby\nsource 'https://rubygems.org'\ngem 'activerecord', '~> 4.0'\ngem 'mysql2'\ngem 'rollbar'\ngem 'daemon-kit'\ngem 'aws-sdk', '~> 1.0'\ngem 'logstash-file'\ngem 'oni', '~> 3.0'\ngem 'dotenv'\ngem 'json', '>= 1.8.1'\ngroup :development, :test do\n  gem 'pry'\n  gem 'pry-doc'\n  gem 'pry-theme'\ngem 'rspec', '~> 3.0'\n  gem 'simplecov'\n  gem 'rake'\nend\ngroup :yard do\n  gem 'yard'\n  gem 'kramdown'\nend\n```\nAnd Gemfile.lock:\n```\nGEM\n  remote: https://rubygems.org/\n  specs:\n    activemodel (4.1.4)\n      activesupport (= 4.1.4)\n      builder (~> 3.1)\n    activerecord (4.1.4)\n      activemodel (= 4.1.4)\n      activesupport (= 4.1.4)\n      arel (~> 5.0.0)\n    activesupport (4.1.4)\n      i18n (~> 0.6, >= 0.6.9)\n      json (~> 1.7, >= 1.7.7)\n      minitest (~> 5.1)\n      thread_safe (~> 0.1)\n      tzinfo (~> 1.1)\n    arel (5.0.1.20140414130214)\n    aws-sdk (1.46.0)\n      json (~> 1.4)\n      nokogiri (>= 1.4.4)\n    builder (3.2.2)\n    coderay (1.1.0)\n    daemon-kit (0.3.0)\n      eventmachine (>= 0.12.10)\n      thor\n    diff-lcs (1.2.5)\n    docile (1.1.5)\n    dotenv (0.11.1)\n      dotenv-deployment (~> 0.0.2)\n    dotenv-deployment (0.0.2)\n    eventmachine (1.0.3)\n    i18n (0.6.9)\n    json (1.8.1)\n    kramdown (1.4.0)\n    logstash-file (0.2.0)\n      json\n    method_source (0.8.2)\n    mini_portile (0.6.0)\n    minitest (5.3.5)\n    multi_json (1.10.1)\n    mysql2 (0.3.16)\n    nokogiri (1.6.2.1)\n      mini_portile (= 0.6.0)\n    oni (3.1.0)\n    pry (0.10.0)\n      coderay (~> 1.1.0)\n      method_source (~> 0.8.1)\n      slop (~> 3.4)\n    pry-doc (0.6.0)\n      pry (~> 0.9)\n      yard (~> 0.8)\n    pry-theme (1.1.2)\n      coderay (~> 1.1)\n      json (~> 1.8)\n    rake (10.3.2)\n    rollbar (0.13.1)\n      multi_json (~> 1.3)\n    rspec (3.0.0)\n      rspec-core (~> 3.0.0)\n      rspec-expectations (~> 3.0.0)\n      rspec-mocks (~> 3.0.0)\n    rspec-core (3.0.2)\n      rspec-support (~> 3.0.0)\n    rspec-expectations (3.0.2)\n      diff-lcs (>= 1.2.0, < 2.0)\n      rspec-support (~> 3.0.0)\n    rspec-mocks (3.0.2)\n      rspec-support (~> 3.0.0)\n    rspec-support (3.0.2)\n    simplecov (0.8.2)\n      docile (~> 1.1.0)\n      multi_json\n      simplecov-html (~> 0.8.0)\n    simplecov-html (0.8.0)\n    slop (3.5.0)\n    thor (0.19.1)\n    thread_safe (0.3.4)\n    tzinfo (1.2.1)\n      thread_safe (~> 0.1)\n    yard (0.8.7.4)\nPLATFORMS\n  ruby\nDEPENDENCIES\n  activerecord (~> 4.0)\n  aws-sdk (~> 1.0)\n  daemon-kit\n  dotenv\n  json (>= 1.8.1)\n  kramdown\n  logstash-file\n  mysql2\n  oni (~> 3.0)\n  pry\n  pry-doc\n  pry-theme\n  rake\n  rollbar\n  rspec (~> 3.0)\n  simplecov\n  yard\n``\n. Ok I did some testing and bothDigestandOpenSSL::Digest` instances are not safe to share across threads. On MRI this seems to magically work, probably due to the lack of proper threading. On Rubinius doing so will rather quickly result in either a Ruby exception or an entire VM crash when running the following:\n``` ruby\nrequire 'thread'\nrequire 'digest'\nrequire 'digest/sha1'\nThread.abort_on_exception = true\nexpected = '0beec7b5ea3f0fdbc95d0dd47f3c5bc275da8a33'\nthreads  = []\ndigest   = Digest::SHA1.new\n100.times do\n  threads << Thread.new do\n    loop do\n      got = digest.hexdigest('foo')\n  unless got == expected\n    raise \"#{got} is not #{expected}\"\n  end\nend\n\nend\nend\nthreads.each(&:join)\n```\nUsing https://github.com/aws/aws-sdk-ruby/search?q=%22OpenSSL+Digest%22&type=Code there seem to be some places where Digest instances are kept around. For example, https://github.com/aws/aws-sdk-ruby/blob/ee75b781dac33fae825b2f652d8aa6234b6ddf59/lib/aws/core/signers/version_4/chunk_signed_stream.rb#L149 stores an instance variable with a Digest instance.\nI'll do some digging to see if those particular bits are shared between threads.\n. @trevorrowe The code in particular is a tad confusing and I'm not sure what method exactly to override. That is, AWS::Core::Client does not seem to have the method signature_version nor sign_request as either an instance or class method (at least according to Pry). The API documentation (http://docs.aws.amazon.com/AWSRubySDK/latest/AWS/Core/Client.html) also makes no mention of said method.\nBasically my idea is to override the method so that it doesn't memoize the signer, then test if that works without throwing any signature errors.\n. After testing said patch it appears that removing the caching of the signer instance doesn't prevent the errors from occurring. A global lock however seems to do the trick so far:\n``` ruby\nclass AWS::SQS::Client\n  GLOBAL_LOCK = Mutex.new\ndef sign_request(req)\n    GLOBAL_LOCK.synchronize do\n      args = [credential_provider, 'sqs', req.region]\n      signer = AWS::Core::Signers::Version4.new(*args)\n      signer.sign_request(req)\n      req\n    end\n  end\nend\n```\nThis might suggest that the actual AWS client instance is shared between threads. This would be odd considering the application in question itself shares nothing AWS related between threads. Instead each SQS connection runs in its own, isolated thread with its own client. I'll try to see if the client is indeed accessed from multiple threads.\n. I applied the following patch to our codebase:\n``` ruby\nclass AWS::SQS::Client\n  GLOBAL_LOCK = Mutex.new\ndef sign_request(req)\n    id = Thread.current.instance_variable_get(:@thread_id)\n    puts \"Thread ##{id} accessing #{self.class} ##{object_id}\"\nGLOBAL_LOCK.synchronize do\n  args = [credential_provider, 'sqs', req.region]\n  signer = AWS::Core::Signers::Version4.new(*args)\n  signer.sign_request(req)\n  req\nend\n\nend\nend\n```\nRunning the application results in the following output:\nThread #12 accessing AWS::SQS::Client::V20121105 #165844\nThread #16 accessing AWS::SQS::Client::V20121105 #165844\nThread #11 accessing AWS::SQS::Client::V20121105 #165844\nThread #14 accessing AWS::SQS::Client::V20121105 #165844\nThread #10 accessing AWS::SQS::Client::V20121105 #165844\nThread #13 accessing AWS::SQS::Client::V20121105 #165844\nThread #12 accessing AWS::SQS::Client::V20121105 #165844\nThread #16 accessing AWS::SQS::Client::V20121105 #165844\nThread #11 accessing AWS::SQS::Client::V20121105 #165844\nThread #17 accessing AWS::SQS::Client::V20121105 #165844\nThread #13 accessing AWS::SQS::Client::V20121105 #165844\nThread #9 accessing AWS::SQS::Client::V20121105 #165844\nThread #8 accessing AWS::SQS::Client::V20121105 #165844\nThread #15 accessing AWS::SQS::Client::V20121105 #165844\nThread #10 accessing AWS::SQS::Client::V20121105 #165844\nThread #14 accessing AWS::SQS::Client::V20121105 #165844\nThread #12 accessing AWS::SQS::Client::V20121105 #165844\nThread #16 accessing AWS::SQS::Client::V20121105 #165844\nThread #13 accessing AWS::SQS::Client::V20121105 #165844\nThread #17 accessing AWS::SQS::Client::V20121105 #165844\nThread #11 accessing AWS::SQS::Client::V20121105 #165844\nThread #10 accessing AWS::SQS::Client::V20121105 #165844\nThread #8 accessing AWS::SQS::Client::V20121105 #165844\nThread #14 accessing AWS::SQS::Client::V20121105 #165844\nThread #15 accessing AWS::SQS::Client::V20121105 #165844\nThread #16 accessing AWS::SQS::Client::V20121105 #165844\nThread #12 accessing AWS::SQS::Client::V20121105 #165844\nThread #9 accessing AWS::SQS::Client::V20121105 #165844\nThread #11 accessing AWS::SQS::Client::V20121105 #165844\nThread #13 accessing AWS::SQS::Client::V20121105 #165844\nThread #14 accessing AWS::SQS::Client::V20121105 #165844\nThread #8 accessing AWS::SQS::Client::V20121105 #165844\nThread #15 accessing AWS::SQS::Client::V20121105 #165844\nThread #16 accessing AWS::SQS::Client::V20121105 #165844\nThread #12 accessing AWS::SQS::Client::V20121105 #165844\nThread #17 accessing AWS::SQS::Client::V20121105 #165844\nThread #9 accessing AWS::SQS::Client::V20121105 #165844\nThread #11 accessing AWS::SQS::Client::V20121105 #165844\nThread #15 accessing AWS::SQS::Client::V20121105 #165844\nThread #16 accessing AWS::SQS::Client::V20121105 #165844\nThread #13 accessing AWS::SQS::Client::V20121105 #165844\nThread #12 accessing AWS::SQS::Client::V20121105 #165844\nThread #8 accessing AWS::SQS::Client::V20121105 #165844\nThread #14 accessing AWS::SQS::Client::V20121105 #165844\nThe last number in each line is the object ID of the client in question. Perhaps I'm missing something but this would suggest that there's a single AWS::SQS::Client instance being shared across all threads.\n. Worth noting: the @thread_id variable is Rubinius specific and its a hack to get it out this way. As far as I know you can't get thread IDs reliably on MRI.\n. @JoshMcKin What threadsafe patch? It could be that we have to update rubysl-openssl accordingly.\n@trevorrowe Using this code:\n``` ruby\nclass AWS::SQS::Client\n  GLOBAL_LOCK = Mutex.new\ndef sign_request(req)\n    puts \"Region: #{@region.inspect}\"\n    puts\n    puts \"Service: #{@service_ruby_name.inspect}\"\n    puts\n    puts \"Credentials: #{@credential_provider.inspect}\"\n    puts '---'\n    puts\nGLOBAL_LOCK.synchronize do\n  args = [credential_provider, 'sqs', req.region]\n  signer = AWS::Core::Signers::Version4.new(*args)\n  signer.sign_request(req)\n  req\nend\n\nend\nend\n```\nThe output is as following:\n```\nRegion: \"eu-west-1\"\nService: \"sqs\"\nCredentials: # @static_credentials={} @cached_credentials={}>, # @suffixes={:access_key_id=>\"ACCESS_KEY_ID\", :secret_access_key=>\"SECRET_ACCESS_KEY\", :session_token=>\"SESSION_TOKEN\"} @cached_credentials={:access_key_id=>\"[CENSORED]\", :secret_access_key=>\"[CENSORED]\"}>, #\"ACCESS_KEY\", :secret_access_key=>\"SECRET_KEY\", :session_token=>\"SESSION_TOKEN\"}>, #\"ACCESS_KEY_ID\", :secret_access_key=>\"SECRET_ACCESS_KEY\", :session_token=>\"SESSION_TOKEN\"}>, #, #]>\n```\n. @JoshMcKin Discussed this with @dbussink, apparently we do not have said patch in rubysl-openssl but we do have similar thread-safety changes. We'll have to look into updating said Gem according to MRI's current setup.\n. @trevorrowe The error persists when using that snippet (minus the global lock).\n. It's been a while, so here's a little follow-up. This particular problem still occurs on Rubinius 2.3, although much less often so far. I'll be doing another testing round with the patches discussed above to see how things behave in that case.\n. Said application has since been running with the above lock with zero errors popping up from the SQS code of the SDK.\n@trevorrowe Do you know of any other way of solving this particular problem without a global lock? While it's rather clear the problem is with the request signing it would be great if this could be solved without having to resort to locks. \n. @trevorrowe Is it possible for the aws-sdk to not share client instances between threads, or is that part of the core design? If the latter is the case I think a lock is the only option.\n. @trevorrowe Rubinius 2.3 ships with an updated version of Ruby's OpenSSL library, which should contain a bunch of threading related fixes. I'll do some extra digging tomorrow to see if I can still reproduce the error discussed in https://github.com/aws/aws-sdk-ruby/issues/455#issuecomment-47962592 (but using OpenSSL::Digest instead of Digest).\n. I'm taking another look into digest/openssl errors without involving the AWS SDK. For this I cooked up the following repository https://github.com/YorickPeterse/rubinius-digest-threading. Locally I can't seem to trigger any digest errors so it's running on Travis now: https://travis-ci.org/YorickPeterse/rubinius-digest-threading/builds/42244201\nIf this particular error can't be reproduced on its own anymore there are two possible scenario's:\n1. The error just happens very, very rarely\n2. The above problems are caused by aws-sdk, not Rubinius, rubysl-openssl or rubysl-digest\nBoth cases are possible and at this time it's hard to pinpoint exactly what's the cause. \n. Using an AWS specific repro script I do get a bunch of signature mismatch errors. I'm using the following script:\n``` ruby\nrequire 'aws'\nrequire 'thread'\nAWS.config(\n  :access_key_id     => '...',\n  :secret_access_key => '...',\n  :sqs_region        => 'eu-west-1',\n  :sqs_endpoint      => 'sqs.eu-west-1.amazonaws.com'\n)\nAWS.eager_autoload!(AWS::SQS)\nAWS.eager_autoload!(AWS::Core)\ndef do_work\n  sqs   = AWS::SQS.new\n  queue = sqs.queues.named('test')\nqueue.poll do |message|\n    message.delete\n  end\nend\nthreads = []\nmutex   = Mutex.new\nThread.abort_on_exception = true\n100.times do |n|\n  threads << Thread.new do\n    mutex.synchronize { puts \"Starting thread #{n + 1}...\" }\nbegin\n  do_work\nrescue => error\n  # Don't mess up console output\n  mutex.synchronize { puts \"Error: #{error.message}\" }\n\n  sleep 2\n  retry\nend\n\nend\nend\nthreads.each(&:join)\n```\nAfter a while this fails with the following errors:\n```\nThe String-to-Sign should have been\n'AWS4-HMAC-SHA256\n20141126T221250Z\n20141126/eu-west-1/sqs/aws4_request\n4290b7aa82740ebf84a725b00abf1d30404ca131d5b9e55dfebafdb2d682e7e9'\nError: The request signature we calculated does not match the signature you provided. Check your AWS Secret Access Key and signing method. Consult the service documentation for details.\nThe Canonical String for this request should have been\n'POST\n/591910330996/test\ncontent-length:167\ncontent-type:application/x-www-form-urlencoded; charset=utf-8\nhost:sqs.eu-west-1.amazonaws.com\nuser-agent:aws-sdk-ruby/1.59.0 rbx/2.1.0 x86_64-linux-gnu\nx-amz-content-sha256:436afced8784b07a74aec44059b716b172abf6cfd470361f3b667aad4561922d\nx-amz-date:20141126T221452Z\ncontent-length;content-type;host;user-agent;x-amz-content-sha256;x-amz-date\n436afced8784b07a74aec44059b716b172abf6cfd470361f3b667aad4561922d'\nThe String-to-Sign should have been\n'AWS4-HMAC-SHA256\n20141126T221452Z\n20141126/eu-west-1/sqs/aws4_request\nc7ab82a37dfd6ccfb4ff3b986d88f66f7730e1ba44d01bca3660b35acea7a311'\nError: The request signature we calculated does not match the signature you provided. Check your AWS Secret Access Key and signing method. Consult the service documentation for details.\nThe Canonical String for this request should have been\n'POST\n/591910330996/test\ncontent-length:167\ncontent-type:application/x-www-form-urlencoded; charset=utf-8\nhost:sqs.eu-west-1.amazonaws.com\nuser-agent:aws-sdk-ruby/1.59.0 rbx/2.1.0 x86_64-linux-gnu\nx-amz-content-sha256:436afced8784b07a74aec44059b716b172abf6cfd470361f3b667aad4561922d\nx-amz-date:20141126T221452Z\ncontent-length;content-type;host;user-agent;x-amz-content-sha256;x-amz-date\n436afced8784b07a74aec44059b716b172abf6cfd470361f3b667aad4561922d'\n...\n```\nWhen this particular error occurs it seems to occur in batches of multiple threads failing at roughly the same time.\n. If it helps, while I haven't seen these particular errors on JRuby I did just get the following error:\nNameError: uninitialized constant AWS::Core::Signers::S3::Base\norg/jruby/RubyModule.java line 2723 in const_missing\n/usr/local/rvm/gems/jruby-1.7.16.1/gems/aws-sdk-v1-1.61.0/lib/aws/core/signers/s3.rb line 59 in signature\n/usr/local/rvm/gems/jruby-1.7.16.1/gems/aws-sdk-v1-1.61.0/lib/aws/core/signers/s3.rb line 53 in authorization\n/usr/local/rvm/gems/jruby-1.7.16.1/gems/aws-sdk-v1-1.61.0/lib/aws/core/signers/s3.rb line 47 in sign_request\n/usr/local/rvm/gems/jruby-1.7.16.1/gems/aws-sdk-v1-1.61.0/lib/aws/s3/client.rb line 58 in sign_request\n/usr/local/rvm/gems/jruby-1.7.16.1/gems/aws-sdk-v1-1.61.0/lib/aws/core/client.rb line 492 in client_request\norg/jruby/RubyProc.java line 271 in call\n/usr/local/rvm/gems/jruby-1.7.16.1/gems/aws-sdk-v1-1.61.0/lib/aws/core/response.rb line 175 in build_request\n/usr/local/rvm/gems/jruby-1.7.16.1/gems/aws-sdk-v1-1.61.0/lib/aws/core/response.rb line 114 in initialize\n/usr/local/rvm/gems/jruby-1.7.16.1/gems/aws-sdk-v1-1.61.0/lib/aws/core/client.rb line 203 in new_response\n/usr/local/rvm/gems/jruby-1.7.16.1/gems/aws-sdk-v1-1.61.0/lib/aws/core/client.rb line 490 in client_request\n/usr/local/rvm/gems/jruby-1.7.16.1/gems/aws-sdk-v1-1.61.0/lib/aws/core/client.rb line 391 in log_client_request\n/usr/local/rvm/gems/jruby-1.7.16.1/gems/aws-sdk-v1-1.61.0/lib/aws/core/client.rb line 477 in client_request\n/usr/local/rvm/gems/jruby-1.7.16.1/gems/aws-sdk-v1-1.61.0/lib/aws/core/client.rb line 373 in return_or_raise\n/usr/local/rvm/gems/jruby-1.7.16.1/gems/aws-sdk-v1-1.61.0/lib/aws/core/client.rb line 476 in client_request\n(eval) line 3 in put_object\n/usr/local/rvm/gems/jruby-1.7.16.1/gems/aws-sdk-v1-1.61.0/lib/aws/s3/s3_object.rb line 1760 in write_with_put_object\n/usr/local/rvm/gems/jruby-1.7.16.1/gems/aws-sdk-v1-1.61.0/lib/aws/s3/s3_object.rb line 612 in write\n/usr/local/rvm/gems/jruby-1.7.16.1/gems/aws-sdk-v1-1.61.0/lib/aws/s3/object_collection.rb line 85 in create\n/usr/local/rvm/gems/jruby-1.7.16.1/gems/opener-daemons-2.3.2/lib/opener/daemons/uploader.rb line 32 in create\n/usr/local/rvm/gems/jruby-1.7.16.1/gems/opener-daemons-2.3.2/lib/opener/daemons/uploader.rb line 18 in upload\n/usr/local/rvm/gems/jruby-1.7.16.1/gems/opener-daemons-2.3.2/lib/opener/daemons/worker.rb line 73 in upload_output\n/usr/local/rvm/gems/jruby-1.7.16.1/gems/opener-daemons-2.3.2/lib/opener/daemons/worker.rb line 46 in process\nWhile this is in S3 and not SQS I suspect this code is suffering from the same problem.\n. @quinn Can you reproduce it using the script mentioned in https://github.com/aws/aws-sdk-ruby/issues/455#issuecomment-64718616? You probably have to adjust the script a bit for V2 of the SDK though.\n. @quinn If possible, could you at least share the error output if you still have it?\n. @trevorrowe This particular problem also seems to occur when using V2 of the SDK and Rubinius 2.5.2:\nAws::S3::Errors::SignatureDoesNotMatch: The request signature we calculated does not match the signature you provided. Check your key and signing method.\n/var/www/kaf_processor/deploy-2015-04-24_15_08_47/vendor/bundle/rbx/2.1/gems/aws-sdk-core-2.0.40/lib/seahorse/client/plugins/raise_response_errors.rb\" line 15 in call\n/var/www/kaf_processor/deploy-2015-04-24_15_08_47/vendor/bundle/rbx/2.1/gems/aws-sdk-core-2.0.40/lib/aws-sdk-core/plugins/s3_sse_cpk.rb\" line 18 in call\n/var/www/kaf_processor/deploy-2015-04-24_15_08_47/vendor/bundle/rbx/2.1/gems/aws-sdk-core-2.0.40/lib/seahorse/client/plugins/param_conversion.rb\" line 19 in call\n/var/www/kaf_processor/deploy-2015-04-24_15_08_47/vendor/bundle/rbx/2.1/gems/aws-sdk-core-2.0.40/lib/aws-sdk-core/plugins/response_paging.rb\" line 9 in call\n/var/www/kaf_processor/deploy-2015-04-24_15_08_47/vendor/bundle/rbx/2.1/gems/aws-sdk-core-2.0.40/lib/seahorse/client/plugins/response_target.rb\" line 15 in call\n/var/www/kaf_processor/deploy-2015-04-24_15_08_47/vendor/bundle/rbx/2.1/gems/aws-sdk-core-2.0.40/lib/seahorse/client/request.rb\" line 70 in send_request\n/var/www/kaf_processor/deploy-2015-04-24_15_08_47/vendor/bundle/rbx/2.1/gems/aws-sdk-core-2.0.40/lib/seahorse/client/base.rb\" line 216 in define_operation_methods\n/var/www/kaf_processor/deploy-2015-04-24_15_08_47/vendor/bundle/rbx/2.1/gems/aws-sdk-resources-2.0.40/lib/aws-sdk-resources/request.rb\" line 24 in call\n/var/www/kaf_processor/deploy-2015-04-24_15_08_47/vendor/bundle/rbx/2.1/gems/aws-sdk-resources-2.0.40/lib/aws-sdk-resources/operations.rb\" line 41 in call\n/var/www/kaf_processor/deploy-2015-04-24_15_08_47/lib/kaf_processor/s3_uploader.rb\" line 41 in upload_json\n/var/www/kaf_processor/deploy-2015-04-24_15_08_47/lib/kaf_processor/processor/text_nodes.rb\" line 24 in process_without_trace_Custom_KafProcessor_Processor_TextNodes_process (process)\n/var/www/kaf_processor/deploy-2015-04-24_15_08_47/vendor/bundle/rbx/2.1/gems/newrelic_rpm-3.10.0.279/lib/new_relic/agent/method_tracer.rb\" line 343 in process_with_trace_Custom_KafProcessor_Processor_TextNodes_process\n/var/www/kaf_processor/deploy-2015-04-24_15_08_47/vendor/bundle/rbx/2.1/gems/newrelic_rpm-3.10.0.279/lib/new_relic/agent/method_tracer_helpers.rb\" line 81 in trace_execution_scoped\n/var/www/kaf_processor/deploy-2015-04-24_15_08_47/vendor/bundle/rbx/2.1/gems/newrelic_rpm-3.10.0.279/lib/new_relic/agent/method_tracer.rb\" line 341 in process (process_with_trace_Custom_KafProcessor_Processor_TextNodes_process)\n/var/www/kaf_processor/deploy-2015-04-24_15_08_47/lib/kaf_processor/worker.rb\" line 62 in run_processor\n/var/www/kaf_processor/deploy-2015-04-24_15_08_47/lib/kaf_processor/worker.rb\" line 40 in process\nkernel/bootstrap/array.rb\" line 76 in each\n/var/www/kaf_processor/deploy-2015-04-24_15_08_47/lib/kaf_processor/worker.rb\" line 39 in _process (process)\n/var/www/kaf_processor/deploy-2015-04-24_15_08_47/config/initializers/newrelic_tracers.rb\" line 16 in process_without_newrelic_transaction_trace (process)\n(eval)\" line 3 in process_with_newrelic_transaction_trace\n/var/www/kaf_processor/deploy-2015-04-24_15_08_47/vendor/bundle/rbx/2.1/gems/newrelic_rpm-3.10.0.279/lib/new_relic/agent/instrumentation/controller_instrumentation.rb\" line 353 in perform_action_with_newrelic_trace\n(eval)\" line 2 in process (process_with_newrelic_transaction_trace)\n/var/www/kaf_processor/deploy-2015-04-24_15_08_47/vendor/bundle/rbx/2.1/gems/oni-4.0.0/lib/oni/daemon.rb\" line 101 in run_worker\n/var/www/kaf_processor/deploy-2015-04-24_15_08_47/vendor/bundle/rbx/2.1/gems/oni-4.0.0/lib/oni/daemon.rb\" line 85 in process\n/var/www/kaf_processor/deploy-2015-04-24_15_08_47/vendor/bundle/rbx/2.1/gems/oni-4.0.0/lib/oni/daemon.rb\" line 174 in run_thread\n/var/www/kaf_processor/deploy-2015-04-24_15_08_47/vendor/bundle/rbx/2.1/gems/oni-4.0.0/lib/oni/daemons/sqs.rb\" line 35 in receive\n/var/www/kaf_processor/deploy-2015-04-24_15_08_47/vendor/bundle/rbx/2.1/gems/aws-sdk-resources-2.0.40/lib/aws-sdk-resources/services/sqs/queue_poller.rb\" line 413 in yield_messages\nkernel/bootstrap/array.rb\" line 76 in each\n/var/www/kaf_processor/deploy-2015-04-24_15_08_47/vendor/bundle/rbx/2.1/gems/aws-sdk-resources-2.0.40/lib/aws-sdk-resources/services/sqs/queue_poller.rb\" line 412 in yield_messages\n/var/www/kaf_processor/deploy-2015-04-24_15_08_47/vendor/bundle/rbx/2.1/gems/aws-sdk-resources-2.0.40/lib/aws-sdk-resources/services/sqs/queue_poller.rb\" line 405 in process_messages\nkernel/bootstrap/proc.rb\" line 20 in call\nkernel/common/throw_catch.rb\" line 30 in catch\nkernel/common/throw_catch.rb\" line 7 in register\nkernel/common/throw_catch.rb\" line 29 in catch\n/var/www/kaf_processor/deploy-2015-04-24_15_08_47/vendor/bundle/rbx/2.1/gems/aws-sdk-resources-2.0.40/lib/aws-sdk-resources/services/sqs/queue_poller.rb\" line 404 in process_messages\n/var/www/kaf_processor/deploy-2015-04-24_15_08_47/vendor/bundle/rbx/2.1/gems/aws-sdk-resources-2.0.40/lib/aws-sdk-resources/services/sqs/queue_poller.rb\" line 336 in poll\nkernel/common/kernel.rb\" line 510 in loop\n/var/www/kaf_processor/deploy-2015-04-24_15_08_47/vendor/bundle/rbx/2.1/gems/aws-sdk-resources-2.0.40/lib/aws-sdk-resources/services/sqs/queue_poller.rb\" line 331 in poll\nkernel/bootstrap/proc.rb\" line 20 in call\nkernel/common/throw_catch.rb\" line 30 in catch\nkernel/common/throw_catch.rb\" line 7 in register\nkernel/common/throw_catch.rb\" line 29 in catch\n/var/www/kaf_processor/deploy-2015-04-24_15_08_47/vendor/bundle/rbx/2.1/gems/aws-sdk-resources-2.0.40/lib/aws-sdk-resources/services/sqs/queue_poller.rb\" line 330 in poll\n/var/www/kaf_processor/deploy-2015-04-24_15_08_47/vendor/bundle/rbx/2.1/gems/oni-4.0.0/lib/oni/daemons/sqs.rb\" line 34 in receive\n/var/www/kaf_processor/deploy-2015-04-24_15_08_47/vendor/bundle/rbx/2.1/gems/oni-4.0.0/lib/oni/daemon.rb\" line 174 in run_thread\n/var/www/kaf_processor/deploy-2015-04-24_15_08_47/vendor/bundle/rbx/2.1/gems/oni-4.0.0/lib/oni/daemon.rb\" line 158 in spawn_thread\nkernel/bootstrap/proc.rb\" line 20 in call\nkernel/bootstrap/thread.rb\" line 396 in __run__\n\nThere is also a huge amount of autoloading problems occurring when using V2. On V1 we also had these problems but they could be solved by using AWS.eager_autoload!. Because such a method doesn't exist for V2 I had to hack it together myself, basically it would recursively refer to all constants, fixing the problem. I'll be looking into seeing if this is a problem with Rbx's autoload not being thread-safe.\n. @trevorrowe At least on JRuby 1.7 autoload is not thread-safe as far as I'm aware of. I believe it wasn't until JRuby 9000 that it was made thread-safe, but @headius can probably fill you in on that. By the looks of it autoload isn't thread-safe on Rubinius either, I'll do some digging to see if there's anything we can do on our end to fix that.\n\nAs for the signature errors, the only fix I understand that we've tried that resolves the issue is to put a global mutex around the OpenSSL digest methods. This seems like it shouldn't be necessary. Thoughts?\n\nIn comment https://github.com/aws/aws-sdk-ruby/issues/455#issuecomment-64717739 I discussed this and set up a standalone script that couldn't reproduce anything close to the problem discussed in this issue. The code we use for OpenSSL is pulled directly from MRI commit https://github.com/ruby/ruby/commit/5a58165520d5a429ab69f8d6d952a8ff645452bc. I still have to look into updating to the current version, I vaguely remember having problems last time I tried.\nEither way, in V1 the problem was that the client/signature signing instances were shared between threads without any synchronisation in place. Is such a pattern still the case with V2?\n. @headius see https://github.com/aws/aws-sdk-ruby/issues/455#issuecomment-71468196.\n. At least this script https://gist.github.com/YorickPeterse/2efb97451fd27c34aec7 fails on Rubinius with constant missing errors, haven't managed to get it to fail on JRuby yet. \n. In https://github.com/rubinius/rubinius/commit/b57399f8f44c9afbe5b2785abd85943a8f46ddd3 I took care of the autoloading problems (as far as I can tell). I finally managed to get my hands on a signature error using the scripts discussed in https://gist.github.com/YorickPeterse/2efb97451fd27c34aec7:\n```\nAn exception occurred running repro.rb:\n(Aws::SQS::Errors::SignatureDoesNotMatch)\nBacktrace:\nSeahorse::Client::Plugins::RaiseResponseErrors::Handler#call at /home/yorickpeterse/.gem/rbx/2.1.0/gems/aws-sdk-core-2.0.40/lib/seahorse/client/plugins/raise_response_errors.rb:15\n      Seahorse::Client::Plugins::ParamConversion::Handler#call at /home/yorickpeterse/.gem/rbx/2.1.0/gems/aws-sdk-core-2.0.40/lib/seahorse/client/plugins/param_conversion.rb:22\n                    Aws::Plugins::ResponsePaging::Handler#call at /home/yorickpeterse/.gem/rbx/2.1.0/gems/aws-sdk-core-2.0.40/lib/aws-sdk-core/plugins/response_paging.rb:10\n       Seahorse::Client::Plugins::ResponseTarget::Handler#call at /home/yorickpeterse/.gem/rbx/2.1.0/gems/aws-sdk-core-2.0.40/lib/seahorse/client/plugins/response_target.rb:18\n                        Seahorse::Client::Request#send_request at /home/yorickpeterse/.gem/rbx/2.1.0/gems/aws-sdk-core-2.0.40/lib/seahorse/client/request.rb:70\n              { } in Aws::SQS::Client#define_operation_methods at /home/yorickpeterse/.gem/rbx/2.1.0/gems/aws-sdk-core-2.0.40/lib/seahorse/client/base.rb:216\n                                      { } in Object#script at repro.rb:11\n                                                     Proc#call at kernel/bootstrap/proc.rb:20\n                                                Thread#run at kernel/bootstrap/thread.rb:356\n```\nThis indicates that the script does eventually reproduce the problem given enough time and luck.\n. In a production application the autoloading problems are indeed resolved, sadly the signature errors remain (when using V2). I'll be looking into this today to see if I can solve this problem for good.\n. @trevorrowe I've tested this both on an EC2 instance (which uses IAM credentials) and locally (which uses static credentials). I'm currently looking into rubysl-openssl as I suspect it has some problems that might trigger this particular problem. Running my repro script mentioned above with -Xcapi.lock (preventing concurrent C API calls) seems to prevent any errors from occurring, hence my suspicion.\n. We recently updated the OpenSSL code of Rubinius to match the code of the latest MRI version (rubysl-openssl is basically a 1:1 copy of MRI's OpenSSL code). Having looked at the code of V2 of the AWS SDK I couldn't find a point where mutable state was shared between threads, leading me to believe it might be OpenSSL that's broken.\nTo confirm/deny this I set up a new standalone script to try and trigger the problem:\n``` ruby\nrequire 'openssl'\nThread.abort_on_exception = true\ninput  = 'the cake is possibly a lie'\ndigest = OpenSSL::Digest::SHA256.new\ndigest.update(input)\nexpected = digest.hexdigest\nputs 'Starting threads...'\nthreads = 10.times.map do\n  Thread.new do\n    loop do\n      digest = OpenSSL::Digest::SHA256.new\n      digest.update(input)\n  got = digest.hexdigest\n\n  if got != expected\n    raise \"Expected digest #{got.inspect} to equal #{expected.inspect}\"\n  end\nend\n\nend\nend\nthreads.each(&:join)\n```\nOn MRI this will run fine for all eternity. On Rubinius on the other hand this pretty much instantly crashes with the following output:\n```\nStarting threads...\nAn exception occurred running /tmp/openssl_thread.rb:\nExpected digest \"70b9d40465992fa71a861e050481fb1a4bed5c8aa127272e7bf5838a3b0ab240\" to equal \"83103cff21a7a50e45eb90c29ca9d24204d95fe7369565ec1d5339750da386ab\" (RuntimeError)\nBacktrace:\n{ } in Object#script at /tmp/openssl_thread.rb:23\n       Kernel(Object)#loop at kernel/common/kernel.rb:511\n  { } in Object#script at /tmp/openssl_thread.rb:16\n                 Proc#call at kernel/bootstrap/proc.rb:20\n            Thread#run at kernel/bootstrap/thread.rb:356\n```\nInteresting enough the digest appears to always be \"70b9d40465992fa71a861e050481fb1a4bed5c8aa127272e7bf5838a3b0ab240\". However, when running Rubinius with a CAPI lock (using -Xcapi.lock) the produced digest is \"e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\", though the VM sometimes segfault in the OpenSSL code.\nTo cut a long story short, it seems the OpenSSL extension is beyond broken in multi-threaded environments or Rubinius does some weird things to it. I'll continue digging until I'm sure what's to blame. If it turns out to be unrelated to the AWS SDK itself I'll close this issue.\n. Per \"rideliner\" from the Rubinius Gitter channel, the above snippet is flawed. The local variable digest in the Thread.new block also overwrites the outer variable, leading to the race condition. Somehow I expected locals to at least be thread-local (even in this case). I shall get myself a dunce cap.\nEither way, the actual signature problem still persists. I'll continue my investigation, but at least it seems that both rubysl-digest and rubysl-openssl are not the cause for these problems.\n. @trevorrowe I literally just got this error on MRI 2.2 as well (though this is using V1), which seems to rule our Rubinius itself being the problem. The error/backtrace is as following (as taken from Rollbar):\n```\nAWS::SQS::Errors::SignatureDoesNotMatch: The request signature we calculated does not match the signature you provided. Check your AWS Secret Access Key and signing method. Consult the service documentation for details.\nThe Canonical String for this request should have been\n'POST\n/345153512707/qr_generator\ncontent-length:175\ncontent-type:application/x-www-form-urlencoded; charset=utf-8\nhost:sqs.eu-west-1.amazonaws.com\nuser-agent:aws-sdk-ruby/1.63.0 ruby/2.2.2 x86_64-linux\nx-amz-content-sha256:47236ab18c30db436023205b5b09bd472e527b24ade58bc3acc21ed4a0d6ad4c\nx-amz-date:20150716T181615Z\nx-amz-security-token:[REMOVED]\ncontent-length;content-type;host;user-agent;x-amz-content-sha256;x-amz-date;x-amz-security-token\n[REMOVED]'\nThe String-to-Sign should have been\n'AWS4-HMAC-SHA256\n20150716T181615Z\n20150716/eu-west-1/sqs/aws4_request\n8b1031f744460308b9295139a5b1c3992e3bf83e66b70b699fd3479fe2d25c19'\n```\nBacktrace:\nFile \"/var/www/barqr/deploy-2015-06-17_13_58_37/vendor/bundle/ruby/2.2.0/gems/aws-sdk-v1-1.63.0/lib/aws/core/client.rb\" line 375 in return_or_raise\nFile \"/var/www/barqr/deploy-2015-06-17_13_58_37/vendor/bundle/ruby/2.2.0/gems/aws-sdk-v1-1.63.0/lib/aws/core/client.rb\" line 476 in client_request\nFile \"(eval)\" line 3 in receive_message\nFile \"/var/www/barqr/deploy-2015-06-17_13_58_37/vendor/bundle/ruby/2.2.0/gems/aws-sdk-v1-1.63.0/lib/aws/sqs/queue.rb\" line 201 in receive_message\nFile \"/var/www/barqr/deploy-2015-06-17_13_58_37/vendor/bundle/ruby/2.2.0/gems/aws-sdk-v1-1.63.0/lib/aws/sqs/queue.rb\" line 303 in block in poll\nFile \"/var/www/barqr/deploy-2015-06-17_13_58_37/vendor/bundle/ruby/2.2.0/gems/aws-sdk-v1-1.63.0/lib/aws/sqs/queue.rb\" line 301 in loop\nFile \"/var/www/barqr/deploy-2015-06-17_13_58_37/vendor/bundle/ruby/2.2.0/gems/aws-sdk-v1-1.63.0/lib/aws/sqs/queue.rb\" line 301 in poll\nFile \"/var/www/barqr/deploy-2015-06-17_13_58_37/vendor/bundle/ruby/2.2.0/gems/oni-3.1.0/lib/oni/daemons/sqs.rb\" line 34 in receive\nFile \"/var/www/barqr/deploy-2015-06-17_13_58_37/vendor/bundle/ruby/2.2.0/gems/oni-3.1.0/lib/oni/daemon.rb\" line 180 in run_thread\nFile \"/var/www/barqr/deploy-2015-06-17_13_58_37/vendor/bundle/ruby/2.2.0/gems/oni-3.1.0/lib/oni/daemon.rb\" line 164 in block in spawn_thread\n. For the first time ever I also experienced this issue on JRuby 1.7:\n```\nAWS::SQS::Errors::SignatureDoesNotMatch: The request signature we calculated does not match the signature you provided. Check your AWS Secret Access Key and signing method. Consult the service documentation for details.\nThe Canonical String for this request should have been\n'POST\n/345153512707/opener-opinion-detector-basic\ncontent-length:192\ncontent-type:application/x-www-form-urlencoded; charset=utf-8\nhost:sqs.eu-west-1.amazonaws.com\nuser-agent:aws-sdk-ruby/1.64.0 jruby/1.9.3 java\nx-amz-content-sha256:90638f76a52ee75c5210c44a615ba9e75278e3e54da88972f50cb680501308d6\nx-amz-date:20150820T230324Z\nx-amz-security-token:[REMOVED]\ncontent-length;content-type;host;user-agent;x-amz-content-sha256;x-amz-date;x-amz-security-token\n[REMOVED]'\nThe String-to-Sign should have been\n'AWS4-HMAC-SHA256\n20150820T230324Z\n20150820/eu-west-1/sqs/aws4_request\n25907c2c04c518ad21ab5a3c66f1a31fa3eaacc87c62cc6aecf740cb44710c14'\n```\nBacktrace:\nFile \"/usr/local/rvm/gems/jruby-1.7.16.1/gems/aws-sdk-v1-1.64.0/lib/aws/core/client.rb\" line 375 in return_or_raise\nFile \"/usr/local/rvm/gems/jruby-1.7.16.1/gems/aws-sdk-v1-1.64.0/lib/aws/core/client.rb\" line 476 in client_request\nFile \"(eval)\" line 3 in receive_message\nFile \"/usr/local/rvm/gems/jruby-1.7.16.1/gems/aws-sdk-v1-1.64.0/lib/aws/sqs/queue.rb\" line 201 in receive_message\nFile \"/usr/local/rvm/gems/jruby-1.7.16.1/gems/aws-sdk-v1-1.64.0/lib/aws/sqs/queue.rb\" line 303 in poll\nFile \"org/jruby/RubyKernel.java\" line 1501 in loop\nFile \"/usr/local/rvm/gems/jruby-1.7.16.1/gems/aws-sdk-v1-1.64.0/lib/aws/sqs/queue.rb\" line 301 in poll\nFile \"/usr/local/rvm/gems/jruby-1.7.16.1/gems/oni-3.1.1/lib/oni/daemons/sqs.rb\" line 34 in receive\nFile \"/usr/local/rvm/gems/jruby-1.7.16.1/gems/oni-3.1.1/lib/oni/daemon.rb\" line 180 in run_thread\nFile \"/usr/local/rvm/gems/jruby-1.7.16.1/gems/oni-3.1.1/lib/oni/daemon.rb\" line 164 in spawn_thread\nRuby info:\njruby 1.7.16.1 (1.9.3p392) 2014-10-28 4e93f31 on OpenJDK 64-Bit Server VM 1.7.0_85-mockbuild_2015_07_20_19_47-b00 +jit [linux-amd64]\n.  Looking at this graph (showing these Digest errors grouped per hour) the errors don't seem to happen at specific times, instead they appear to occur fairly randomly.\n. Contents of the application's Gemfile (with trimmed Git URLs):\n``` ruby\nsource 'https://rubygems.org'\ngem 'rollbar'\ngem 'daemon-kit'\ngem 'aws-sdk', '~> 1.0'\ngem 'logstash-file'\ngem 'oni', '~> 3.0'\ngem 'json', ['>= 1.8.1']\ngem 'dalli'\ngem 'nokogiri', ['~> 1.6', '>= 1.6.1']\ngem 'httpclient'\ngem 'countries'\ngem 'mongoid', '~> 3.1'\ngem 'activerecord', '~> 3.0'\nGit dependencies.\ngem 'holidaycheck_api',\n  :git    => '...',\n  :branch => 'master'\ngem 'hotels_nl',\n  :git    => '...',\n  :branch => 'master'\ngem 'tripadvisor', :git => '...'\ngroup :development, :test do\n  gem 'pry'\n  gem 'pry-doc'\n  gem 'pry-theme'\n  gem 'bond'\n  gem 'webmock'\n  gem 'ansi'\ngem 'rspec'\n  gem 'ci_reporter'\n  gem 'simplecov'\n  gem 'rake'\n  gem 'rubocop'\nend\ngroup :yard do\n  gem 'yard'\n  gem 'kramdown'\nend\n```\nContents of the Gemfile.lock:\n```\nGIT\n  remote: ...\n  revision: 20dd108366b7d6fdaa7e6629fb42cab4db1e5ea8\n  branch: master\n  specs:\n    holidaycheck_api (0.3.1)\n      faraday\n      nokogiri\n      nori (~> 2.3)\nGIT\n  remote: ...\n  revision: b0be19e9a649cfb02801abb550f93f14540f9810\n  branch: master\n  specs:\n    hotels_nl (0.2.0)\n      addressable\n      faraday\n      nokogiri\n      nori (~> 2.3)\nGIT\n  remote: ...\n  revision: 02ab7a98e4915e99c790245ff7cbfec7bc6e4cfe\n  specs:\n    tripadvisor (1.0.1)\n      faraday\n      faraday_middleware\n      json\nGEM\n  remote: https://rubygems.org/\n  specs:\n    activemodel (3.2.17)\n      activesupport (= 3.2.17)\n      builder (~> 3.0.0)\n    activerecord (3.2.17)\n      activemodel (= 3.2.17)\n      activesupport (= 3.2.17)\n      arel (~> 3.0.2)\n      tzinfo (~> 0.3.29)\n    activesupport (3.2.17)\n      i18n (~> 0.6, >= 0.6.4)\n      multi_json (~> 1.0)\n    addressable (2.3.5)\n    ansi (1.4.3)\n    arel (3.0.3)\n    ast (1.1.0)\n    aws-sdk (1.36.2)\n      json (~> 1.4)\n      nokogiri (>= 1.4.4)\n      uuidtools (~> 2.1)\n    bond (0.5.1)\n    builder (3.0.4)\n    ci_reporter (1.9.1)\n      builder (>= 2.1.2)\n    coderay (1.1.0)\n    countries (0.9.3)\n      currencies (~> 0.4.2)\n    crack (0.4.2)\n      safe_yaml (~> 1.0.0)\n    currencies (0.4.2)\n    daemon-kit (0.2.3)\n      eventmachine (>= 0.12.10)\n      i18n\n      rubigen\n      safely (>= 0.3.1)\n      thor\n    dalli (2.7.0)\n    diff-lcs (1.2.5)\n    docile (1.1.3)\n    eventmachine (1.0.3)\n    faraday (0.8.9)\n      multipart-post (~> 1.2.0)\n    faraday_middleware (0.9.0)\n      faraday (>= 0.7.4, < 0.9)\n    httpclient (2.3.4.1)\n    i18n (0.6.9)\n    json (1.8.1)\n    kramdown (1.3.3)\n    logstash-file (0.2.0)\n      json\n    method_source (0.8.2)\n    mini_portile (0.5.3)\n    mongoid (3.1.6)\n      activemodel (~> 3.2)\n      moped (~> 1.4)\n      origin (~> 1.0)\n      tzinfo (~> 0.3.29)\n    moped (1.5.2)\n    multi_json (1.9.2)\n    multipart-post (1.2.0)\n    nokogiri (1.6.1)\n      mini_portile (~> 0.5.0)\n    nori (2.3.0)\n    oni (3.1.0)\n    origin (1.1.0)\n    parser (2.1.7)\n      ast (~> 1.1)\n      slop (~> 3.4, >= 3.4.5)\n    powerpack (0.0.9)\n    pry (0.9.12.6)\n      coderay (~> 1.0)\n      method_source (~> 0.8)\n      slop (~> 3.4)\n    pry-doc (0.6.0)\n      pry (~> 0.9)\n      yard (~> 0.8)\n    pry-theme (1.0.2)\n      coderay (~> 1.1)\n      json (~> 1.8)\n    rainbow (2.0.0)\n    rake (10.1.1)\n    rollbar (0.12.15)\n      multi_json (~> 1.3)\n    rspec (2.14.1)\n      rspec-core (~> 2.14.0)\n      rspec-expectations (~> 2.14.0)\n      rspec-mocks (~> 2.14.0)\n    rspec-core (2.14.8)\n    rspec-expectations (2.14.5)\n      diff-lcs (>= 1.1.3, < 2.0)\n    rspec-mocks (2.14.6)\n    rubigen (1.5.7)\n      activesupport (>= 2.3.5)\n      i18n\n    rubocop (0.19.1)\n      json (>= 1.7.7, < 2)\n      parser (~> 2.1.7)\n      powerpack (~> 0.0.6)\n      rainbow (>= 1.99.1, < 3.0)\n      ruby-progressbar (~> 1.4)\n    ruby-progressbar (1.4.1)\n    safe_yaml (1.0.1)\n    safely (0.3.2)\n    simplecov (0.8.2)\n      docile (~> 1.1.0)\n      multi_json\n      simplecov-html (~> 0.8.0)\n    simplecov-html (0.8.0)\n    slop (3.5.0)\n    thor (0.18.1)\n    tzinfo (0.3.39)\n    uuidtools (2.1.4)\n    webmock (1.17.4)\n      addressable (>= 2.2.7)\n      crack (>= 0.3.2)\n    yard (0.8.7.3)\nPLATFORMS\n  ruby\nDEPENDENCIES\n  activerecord (~> 3.0)\n  ansi\n  aws-sdk (~> 1.0)\n  bond\n  ci_reporter\n  countries\n  daemon-kit\n  dalli\n  holidaycheck_api!\n  hotels_nl!\n  httpclient\n  json (>= 1.8.1)\n  kramdown\n  logstash-file\n  mongoid (~> 3.1)\n  nokogiri (~> 1.6, >= 1.6.1)\n  oni (~> 3.0)\n  pry\n  pry-doc\n  pry-theme\n  rake\n  rollbar\n  rspec\n  rubocop\n  simplecov\n  tripadvisor!\n  webmock\n  yard\n```\n. The offending code is the following: https://github.com/aws/aws-sdk-ruby/blob/master/lib/aws/core/signers/version_4.rb#L195\nTo make things even weirder:\nDigest::SHA256.superclass # => Digest::Base\nPerhaps MRI pulls off some magic tricks but it's a bit odd that the SHA256 class inherits something that can't be inherited.\n. Seems this was reported in the past as well in https://github.com/aws/aws-sdk-core-ruby/issues/43\n. Looking at the C code, http://rxr.whitequark.org/mri/source/ext/digest/digest.c#494 seems super racy. If some other thread is modifying the same data structures it could potentially be the case that it ends up raising the error as the ancestor tree is still being set up.\n@knu any comments on the above? Is the Digest module supposed to be thread-safe?\n. The rate at which this error occur seems to vary depending on how fast data is being processed. For example, one service with a much smaller work load per job triggers this error much faster than a slower service.\n. We are running on MRI 2.1.1 so I don't think those changes resolve this particular problem.\n. Oh derp, I misinterpreted that as patching Digest, not OpenSSL::Digest. I was considering that or slapping a big mutex around the signing part but I haven't gotten to trying either method yet.\n. I deployed the following hack to one of our applications:\nAWS::Core::Signers::Version4::Digest = OpenSSL::Digest\nI've not seen the error pop up in said application since adding the hack. I'll give it a try with some other applications as well.\n. @findchris No, I haven't been able to set up a test case that doesn't use aws-sdk.\n. @findchris I vaguely recall having had one around but I can't seem to find it. You might be able to reproduce this particular error by using the code here https://github.com/aws/aws-sdk-ruby/issues/455#issuecomment-33472682. Having said that, since deploying the above fix (which is included in current aws-sdk releases) I have not experienced this particular problem.\n. @trevorrowe Are there any objections against merging this in? I suspect that this might also be related to #455 but I'm not fully sure.\n. Any update on this? We're seeing this problem across all of our applications that use the aws-sdk so having this patch (or an alternative fix) merged in would be much appreciated. \n. Thanks!\n. Yes, the URLs do include the Expires parameter. Here's an example URL pointing to a text file: https://s3-eu-west-1.amazonaws.com/storage.olery.com/aws-sdk-808-test.txt?AWSAccessKeyId=AKIAIGBLDAE2THGTJVQA&Expires=1432283350&Signature=442ZV51EMy5LojR36kWZq3R0dgI%3D\nI just generated this URL, I'll check back in a few hours to see if it's still valid.\n. Interesting enough said URL is still valid, leading me to believe it might be the content type (our expired URLs have the content-type set to application/pdf). To test this I generated the following URL:\nhttps://s3-eu-west-1.amazonaws.com/storage.olery.com/aws_808.pdf?AWSAccessKeyId=AKIAIGBLDAE2THGTJVQA&Expires=1432629378&Signature=7cwKkNguwRH9pW%2FZyL9U2kp0DTo%3D\nUsing the following code:\n``` ruby\nrequire 'aws-sdk-v1'\nb = AWS::S3.new.buckets['storage.olery.com']\nobj = b.objects['aws_808.pdf']\ncontent = File.read('/home/yorickpeterse/Downloads/aws_808.pdf')\nobj.write(content, :content_type => 'application/pdf')\nobj.url_for(:read, :expires => 604800)\n```\nI'll check again if this URL expires prematurely.\n. So I just realized something, which might explain the problem we're seeing. We're using IAM instance credentials to generate pre-signed URLs from our applications. Instance credentials expire after a little while, probably resulting in the URLs also expiring. Is there a way around this, or is the only option to re-generate signed URLs in such a case? Sadly using static AWS credentials is not really an option due to it being less secure than IAM credentials.\n. This indeed seems to be caused by IAM credentials expiring. I'll close this issue as there's little the AWS SDK can do about this. Sorry for the noise.\n. ",
    "codekitchen": "It probably goes without saying, but as another data point, the same problem affects other AWS::Services as well. I was experiencing very frequent AWS::S3::Errors::Forbidden errors after switching my multithreaded bucket-copying app to Rubinius, until I added the equivalent patch for S3:\n``` ruby\nclass AWS::S3::Client\n  GLOBAL_LOCK = Mutex.new\ndef sign_request(req)\n    GLOBAL_LOCK.synchronize do\n      signer = AWS::Core::Signers::S3.new(credential_provider)\n      signer.sign_request(req)\n    end\n  end\nend\n```\n. Ah, I searched for existing issues but I guess I didn't pick the right keywords to find that one. Thanks, that's a bit lower level but definitely will work.\n. ",
    "quinn": "i see the label \"Version 1\" on here, but I am seeing this with v2.0.30. I am using MRI 2.1.3p242. error happens after awhile using threads. \n. @YorickPeterse I think i was mistaken, i'm trying w/o threads and still seeing. sorry for the confusion \n. sure:\n/Users/lsu/.rbenv/versions/2.1.3/lib/ruby/gems/2.1.0/gems/aws-sdk-core-2.0.30/lib/seahorse/client/plugins/raise_response_errors.rb:15:in `call': The request signature we calculated does not match the signature you provided. Check your key and signing method. (Aws::S3::Errors::SignatureDoesNotMatch)\n    from /Users/lsu/.rbenv/versions/2.1.3/lib/ruby/gems/2.1.0/gems/aws-sdk-core-2.0.30/lib/aws-sdk-core/plugins/s3_sse_cpk.rb:18:in `call'\n    from /Users/lsu/.rbenv/versions/2.1.3/lib/ruby/gems/2.1.0/gems/aws-sdk-core-2.0.30/lib/seahorse/client/plugins/param_conversion.rb:22:in `call'\n    from /Users/lsu/.rbenv/versions/2.1.3/lib/ruby/gems/2.1.0/gems/aws-sdk-core-2.0.30/lib/aws-sdk-core/plugins/response_paging.rb:10:in `call'\n    from /Users/lsu/.rbenv/versions/2.1.3/lib/ruby/gems/2.1.0/gems/aws-sdk-core-2.0.30/lib/seahorse/client/plugins/response_target.rb:18:in `call'\n    from /Users/lsu/.rbenv/versions/2.1.3/lib/ruby/gems/2.1.0/gems/aws-sdk-core-2.0.30/lib/seahorse/client/request.rb:70:in `send_request'\n    from /Users/lsu/.rbenv/versions/2.1.3/lib/ruby/gems/2.1.0/gems/aws-sdk-core-2.0.30/lib/seahorse/client/base.rb:216:in `block (2 levels) in define_operation_methods'\n    from /Users/lsu/.rbenv/versions/2.1.3/lib/ruby/gems/2.1.0/gems/aws-sdk-resources-2.0.30/lib/aws-sdk-resources/request.rb:24:in `call'\n    from /Users/lsu/.rbenv/versions/2.1.3/lib/ruby/gems/2.1.0/gems/aws-sdk-resources-2.0.30/lib/aws-sdk-resources/operations.rb:41:in `call'\n    from /Users/lsu/.rbenv/versions/2.1.3/lib/ruby/gems/2.1.0/gems/aws-sdk-resources-2.0.30/lib/aws-sdk-resources/operation_methods.rb:19:in `block in add_operation'\n    from copy_assets.rb:8:in `block in <main>'\n    from /Users/lsu/.rbenv/versions/2.1.3/lib/ruby/gems/2.1.0/gems/aws-sdk-resources-2.0.30/lib/aws-sdk-resources/batch.rb:75:in `each'\n    from /Users/lsu/.rbenv/versions/2.1.3/lib/ruby/gems/2.1.0/gems/aws-sdk-resources-2.0.30/lib/aws-sdk-resources/batch.rb:75:in `each'\n    from /Users/lsu/.rbenv/versions/2.1.3/lib/ruby/gems/2.1.0/gems/aws-sdk-resources-2.0.30/lib/aws-sdk-resources/batch.rb:75:in `each'\n    from /Users/lsu/.rbenv/versions/2.1.3/lib/ruby/gems/2.1.0/gems/aws-sdk-resources-2.0.30/lib/aws-sdk-resources/collection.rb:18:in `block in each'\n    from /Users/lsu/.rbenv/versions/2.1.3/lib/ruby/gems/2.1.0/gems/aws-sdk-resources-2.0.30/lib/aws-sdk-resources/operations.rb:128:in `block in all_batches'\n    from /Users/lsu/.rbenv/versions/2.1.3/lib/ruby/gems/2.1.0/gems/aws-sdk-core-2.0.30/lib/aws-sdk-core/pageable_response.rb:77:in `each'\n    from /Users/lsu/.rbenv/versions/2.1.3/lib/ruby/gems/2.1.0/gems/aws-sdk-resources-2.0.30/lib/aws-sdk-resources/operations.rb:127:in `all_batches'\n    from /Users/lsu/.rbenv/versions/2.1.3/lib/ruby/gems/2.1.0/gems/aws-sdk-resources-2.0.30/lib/aws-sdk-resources/collection.rb:18:in `each'\n    from /Users/lsu/.rbenv/versions/2.1.3/lib/ruby/gems/2.1.0/gems/aws-sdk-resources-2.0.30/lib/aws-sdk-resources/collection.rb:18:in `each'\n    from copy_assets.rb:5:in `<main>'\nIt happens after awhile but always on the same object. I am batch copying from one bucket to another using Object#copy_from. \n. I was not url encoding the copy source param, a bit embarrassing. Still, it's a cryptic error message for this type of error. \n. ",
    "headius": "What is not thread-safe about autoload in JRuby 1.7?\n. Those errors do not necessarily indicate that JRuby's autoload is unsafe. Constants can end up missing during any concurrent require logic, regardless of whether autoload is involved, if code attempts to access classes while they're still booting. This can happen if, for example, defined? is used to determine the existence of a class. This is because class definition is not atomic.\nOf course it could also indicate a thread-safety problem in JRuby's autoload, but it's definitely not the first place I'd look.\n. Ok, looking at the code, there's an access of Base there, and Base is defined as an autoload...so there's a better chance this is a bug in autoload in JRuby. I'll see if I can come up with a case that reproduces it.\n. ",
    "palfrey": "Here's how to do it in your own code BTW\n``` ruby\npemPath is the file path of your PEM file\ninstance is an AWS::EC2::Instance\nec2 = AWS::EC2.new\nencrypted_password = ec2.client.get_password_data(:instance_id => instance.instance_id).password_data\nprivate_key = OpenSSL::PKey::RSA.new(File.read(pemPath))\ndecoded = Base64.decode64(encrypted_password)\npassword = private_key.private_decrypt(decoded)\n```\n. ",
    "gjastrab": "Yes, sorry I meant to put more info in after I had submitted it.\nNothing unique about the bucket name aside from having 1 underscore in it.  A customer is passing us a large amount of data to import into our system via S3, so we're running a process iterating over the bucket which has hundreds of thousands of objects.\nThe code to iterate through is essentially doing something like:\ntree = bucket.as_tree(prefix: 'some/prefix/')\ntree.children.each do |node|\n  if bucket.objects[node.prefix + 'somefile'].exists?\n    ...\n  end\nend\n\nThe first time I got this error the process had run for about 6 hours and the 2nd time it had run for 12 hours, so I wonder if there's some edge case that's causing it to occur only after doing many many thousands of requests.\nOther relevant information is it's authenticating via an IAM role, so not passing any credentials when instantiating AWS::S3 just doing:\ns3 = AWS::S3.new\nbucket = s3.buckets['mybucket_name']\nSorry for the initial lack of info, hope this helps narrow it down.\n. I do know on the 2 instances when it failed the object did not exist, so it should have returned false.  Also apologize for not mentioning that.\nThe format our customer is sending us data let's them optionally include a file at a predictable location within each \"folder\" so I need to do this exists? check to see if I can do that optional processing or not.\n. Hm, although tonight it just happened on an object that does actually exist in the bucket (after running for 5 hours).  Since yesterday I wrapped the exists? call in a begin/rescue:\ntree = bucket.as_tree(prefix: 'some/prefix/')\ntree.children.each do |node|\n  begin\n    if bucket.objects[node.prefix + 'somefile'].exists?\n      ...\n    end\n  rescue AWS::S3::Errors::BadRequest\n  end\nend\nbut there was another method I forgot about which also called exists? on an S3Object so the process failed again.  Odd that this time when I looked in the logs the object it failed on is one that exists, so it should have returned true unlike the earlier times it raised...\n. This is turning into a deal breaker for us.  Now it's happened when calling S3Object#content_length (the process had run for 5.5 hours when it encountered this):\nAWS::S3::Errors::BadRequest\n\".../aws-sdk-1.32.0/lib/aws/core/client.rb:368:in return_or_raise'\"\n\".../aws-sdk-1.32.0/lib/aws/core/client.rb:469:in client_request'\"\n\"(eval):3:in head_object'\",\n\".../aws-sdk-1.32.0/lib/aws/s3/s3_object.rb:293:in head'\"\n\".../aws-sdk-1.32.0/lib/aws/s3/s3_object.rb:317:in content_length'\"\nThe object it was being called on does exist and I was able to download it using the AWS CLI just fine.\nThis is getting to be quite a pain - I can't wrap every S3 interaction in a begin...rescue to prevent the script from bombing out.  Have you been able to find a way to reproduce it?  Something regarding it being run/authenticated via an IAM profile perhaps?\nLet me know if there's something more I can gather on my end that would help you track down the issue.  Thanks.\n. Will turn back on with the wire trace logging to hopefully get more specifics on the request/response for you to debug.\n- I said earlier these are servers launched under an IAM role.  That role has been given access to the bucket with the following policy:\n\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Sid\": \"Stmt1280939304000\",\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"s3:GetBucketAcl\",\n        \"s3:GetBucketLocation\",\n        \"s3:ListBucket\",\n        \"s3:ListBucketMultipartUploads\"\n      ],\n      \"Resource\": [\n        \"arn:aws:s3:::MY_BUCKET_NAME\"\n      ]\n    },\n    {\n      \"Sid\": \"Stmt1132939872000\",\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"s3:DeleteObject\",\n        \"s3:GetObject\",\n        \"s3:GetObjectAcl\",\n        \"s3:ListMultipartUploadParts\",\n        \"s3:PutObject\",\n        \"s3:PutObjectAcl\"\n      ],\n      \"Resource\": [\n        \"arn:aws:s3:::MY_BUCKET_NAME/*\"\n      ]\n    }\n  ]\n}\n\n\nI haven't tried a retry on the block, but I don't think it's a specific key issue.  On the most recent occurrence of this crashing, I looked in the log for the key where the exists? call had failed.  I opened up a new console on the same server and called exists? and content_length on that key and got back true and the appropriate content length.\nNo nothing special about the object keys\n. Sure, but I had already restarted the processes with wire tracing enabled, so the next time it fails I'll send you the log info and then restart it with this monkey patch.  Thanks.\n. Excellent, although a couple of times it was after 12 hours.\n. a1c0197977 in v1.34.0 seems to have solved the problem, thanks! :thumbsup: \n. \n",
    "s2t2": "FYI on gem version 2.0.41 a call to bucket.exists? throws rescue in exists?': Aws::S3::Errors::BadRequest only if the bucket exists. The bucket in question was created using bucket = resource.bucket(BUCKET_NAME). Please advise. \n. Thanks for your fast reply. I was using the following code, which is now behaving differently than it was yesterday, in that yesterday it was throwing error when checking for existence of an existing bucket, whereas now its sometimes returning true and sometimes throwing an error when checking for existence of a non-existent bucket. And resource.create_bucket was working yesterday but now is attempting to create the bucket in the wrong region.\nrb\ncreds = Aws::Credentials.new(KEY_ID, KEY_SECRET)\nresource = Aws::S3::Resource.new(:credentials => creds, :endpoint => ENDPOINT, :region => REGION)\nbucket = resource.bucket(BUCKET_NAME)\nresource.create_bucket(:bucket => BUCKET_NAME) unless bucket.exists? #=> Aws::S3::Errors::BadRequest\nUsing the code you suggested, I'm getting the following...\nrb\nS3::Bucket.new(BUCKET_NAME)\n=> NameError: uninitialized constant S3\nAws::S3::Bucket.new(BUCKET_NAME)\n=>  Aws::Errors::MissingRegionError: missing region; use :region option or export region name to ENV['AWS_REGION']\nbucket = Aws::S3::Bucket.new(BUCKET_NAME, :region => REGION)\n=> #<Aws::S3::Bucket name=\"testt\">\nbucket.exists?\n=> Aws::S3::Errors::Http301Error\nbucket = Aws::S3::Bucket.new(BUCKET_NAME, :credentials => creds, :endpoint => ENDPOINT, :region => REGION)\n=> #<Aws::S3::Bucket name=\"testt\">\nbucket.exists?\n=> Aws::S3::Errors::BadRequest\nI may just be using the gem wrong, so any guidance you can give is appreciated.\nEDIT: I have found more luck using the code below, but that is throwing region-related errors which are not applicable to this issue.\n``` rb\nresource = Aws::S3::Resource.new(:region => REGION, credentials: creds)\nbucket_names = resource.buckets.map{|bucket| bucket.name}\nresource.create_bucket(:bucket => BUCKET_NAME, :acl => \"private\") unless bucket_names.include?(BUCKET_NAME)\n> Aws::S3::Errors::IllegalLocationConstraintException: The us-west-1 location constraint is incompatible for the region specific endpoint this request was sent to.\n```\n. ",
    "MicTech": "@trevorrowe \nI have a same problem.\nversion 1.65.0\npolicy\n{\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": \"s3:ListAllMyBuckets\",\n            \"Resource\": \"arn:aws:s3:::*\"\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": \"s3:*\",\n            \"Resource\": [\n                \"arn:aws:s3:::bucketname\",\n                \"arn:aws:s3:::bucketname/*\"\n            ]\n        }\n    ]\n}\nruby\ns3 = AWS::S3.new(:access_key_id => 'xxx', :secret_access_key => 'xxx')\nbucket = s3.buckets['bucketname']\n```\nputs bucket.objects[source_key].acl\nAWS::S3::Errors::NoSuchKey: No Such Key\nputs bucket.objects[source_key].exists?\nAWS::S3::Errors::BadRequest: AWS::S3::Errors::BadRequest\n```\nrequest\n```\nHEAD /directory/filename.ext HTTP/1.1\nContent-Type: \nAccept-Encoding: \nUser-Agent: aws-sdk-ruby/1.65.0 ruby/2.2.3 x86_64-darwin15\nDate: Thu, 21 Apr 2016 13:51:44 GMT\nAuthorization: AWS xxx:xxx=\nAccept: /\nHost: bucketname.s3.amazonaws.com\n```\n. @awood45 That's correct and I agree with that, but unfortunately Paperclip doesn't support v2, at least not yet.\n. ",
    "anoldguy": "I realize this is an old issue, but I found that being explicit about the region to fix the problem for us.\nWe were getting the AWS::S3::Errors::BadRequest for every call to .exist?. Our bucket was in us-east-2, and was somehow working without specifying the region. Once we passed region into the S3 client, all was well.. ",
    "burke": "SecureRandom.uuid was added in 1.9. I guess we could make the dependency conditional on platform.\n. ",
    "wvanbergen": "We could include a simple backport of SecureRandom.uuid if it is not defined for 1.8.7. Here's a simple JS implementation of the algorithm as an example:\njs\n'xxxxxxxx-xxxx-4xxx-yxxx-xxxxxxxxxxxx'.replace(/[xy]/g, function(c) {\n    var r = Math.random()*16|0, v = c == 'x' ? r : (r&0x3|0x8);\n    return v.toString(16);\n})\nShould be easy to implement the same in Ruby. If this is OK I will update this PR.\nA little background: we're currently in the process of eliminating as many gems from our bundle, because bundler is really starting to slow down our development, tests and deploys. This is how I ran into this dependency.\n. Alternatively I can copy the algorithm for UUIDTools :)\n. I added a simple back port of SecureRandom.uuid based on the RFC: http://www.ietf.org/rfc/rfc4122.txt\nNot quite sure what the best place to put it is.\n. ",
    "EronHennessey": "Thanks for taking the fix so quickly! :)\n. ",
    "thewoolleyman": "Oops, didn't realize pull request would auto-open a new issue.  This is a fix for https://github.com/aws/aws-sdk-ruby/issues/464.\n. ",
    "quentindemetz": "I would love to see document upload as well\n. ",
    "zrisher": "This is great! Thanks for the note about availability in 2.0, saved me a lot of trouble.\nIt was a bit confusing to come from the CloudSearch API docs, which say \"We recommend using one of the AWS SDKs or the AWS CLI to submit search requests\", to this gem and not see any of the functionality needed to accomplish that. The \"Supported Services\" section of the Readme lists CloudSearch without any qualifications, but the documentation doesn't list any of the required methods.\nPerhaps you could mention near the top of the ReadMe the services that aren't fully supported, but that V2 is bridging those gaps? Or at least mention on the CloudSearch API pages, when they tell you to use the SDK for services V1 doesn't support, that you need to look at V2.\nOtherwise, thank you so much for these gems.\n. ",
    "mbailey": "We currently need to set AWS_DEFAULT_REGION and AWS_REGION when they both have the same meaning. This fix DRYs up the redundant config and makes aws-sdk-ruby\n. ",
    "nicolas-besnard": "I got a new error when using http_wire_trace :\nrake aborted!\nundefined method `<<' for #<ActiveSupport::BufferedLogger:0x007f8b0e2436c8>\n/Users/nicolas/.rvm/gems/ruby-1.9.3-p484/gems/activesupport-3.2.16/lib/active_support/tagged_logging.rb:72:in `method_missing'\n/Users/nicolas/.rvm/gems/ruby-1.9.3-p484/gems/aws-sdk-1.28.1/lib/aws/core/http/connection_pool.rb:301:in `start_session'\n/Users/nicolas/.rvm/gems/ruby-1.9.3-p484/gems/aws-sdk-1.28.1/lib/aws/core/http/connection_pool.rb:125:in `session_for'\n/Users/nicolas/.rvm/gems/ruby-1.9.3-p484/gems/aws-sdk-1.28.1/lib/aws/core/http/net_http_handler.rb:55:in `handle'\n/Users/nicolas/.rvm/gems/ruby-1.9.3-p484/gems/aws-sdk-1.28.1/lib/aws/core/client.rb:244:in `block in make_sync_request'\n/Users/nicolas/.rvm/gems/ruby-1.9.3-p484/gems/aws-sdk-1.28.1/lib/aws/core/client.rb:273:in `retry_server_errors'\n/Users/nicolas/.rvm/gems/ruby-1.9.3-p484/gems/aws-sdk-1.28.1/lib/aws/core/client.rb:240:in `make_sync_request'\n/Users/nicolas/.rvm/gems/ruby-1.9.3-p484/gems/aws-sdk-1.28.1/lib/aws/core/client.rb:502:in `block (2 levels) in client_request'\n/Users/nicolas/.rvm/gems/ruby-1.9.3-p484/gems/aws-sdk-1.28.1/lib/aws/core/client.rb:382:in `log_client_request'\n/Users/nicolas/.rvm/gems/ruby-1.9.3-p484/gems/aws-sdk-1.28.1/lib/aws/core/client.rb:468:in `block in client_request'\n/Users/nicolas/.rvm/gems/ruby-1.9.3-p484/gems/aws-sdk-1.28.1/lib/aws/core/client.rb:364:in `return_or_raise'\n/Users/nicolas/.rvm/gems/ruby-1.9.3-p484/gems/aws-sdk-1.28.1/lib/aws/core/client.rb:467:in `client_request'\n(eval):3:in `describe_environments'\n. ",
    "pwolanin": "I see also many or all of the removed classes had a service method which maybe was used in the past for these log messages?\nAWS::Core::Http::Request does not have that method defined\n. Thanks for the fix\n. ok, thanks\n. ",
    "andres99x": "That's what i concluded after some research.\nThanks anyways!\n. ",
    "jstultz": "Thanks, definitely an improvement, though I should note that the @return line is not quite correct. It says \nReturns the givenoptionshash.\nbut the function will always return a hash containing both :read_capacity_units and :write_capacity_units, even if only one of the two was specified in the input hash.\n. ",
    "i0rek": "I second that request. ~/.aws/config is the recommended way to store credentials. For us every dev has ~/.aws/config and the server has a iam role. Having a aws.yml or even credentials in the code is neither neccessary nor recommended.\n. ",
    "wpc": "Any one on this? We see a huge performance difference when signing 200 S3 urls on EC2 instance with EC2CredentialProvider.\n-- wpc\n. Hi, trevorrowe\nThank you for you suggestion. We have made a change to the pull request following it.\n-- wpc\n. ",
    "bunnymatic": "This is not my stack post, but it seems to be affecting other folks as well...\nhttp://stackoverflow.com/questions/8380193/special-characters-in-filename-affecting-aws-sdk-ruby-gem-url-for-method\n. saw your comments.  i will try.  i've been a bit underwater with work\nstuff, but i can try to get a sample together.\ncheers\nOn Wed, Apr 2, 2014 at 3:39 PM, Trevor Rowe notifications@github.comwrote:\n\nI am unable to directly reproduce this issue. Here is my simple case:\nobj = AWS::S3.new.buckets['aws-sdk'].objects['my key with spaces'].write(\"Hello\")\nobj.key#=> \"my key with spaces\"\nputs obj.read#=> \"Hello\"\nurl = obj.url_for(:read).to_s#=> \"https://aws-sdk.s3.amazonaws.com/my%20key%20with%20spaces?AWSAccessKeyId=...&Expires=1396481909&Signature=...\"\nPasting the resultant URL into a browser works just fine and I get the\nexpected \"Hello\" string. Can you produce a simple case that reproduce the\nerror?\n\nReply to this email directly or view it on GitHubhttps://github.com/aws/aws-sdk-ruby/issues/479#issuecomment-39393052\n.\n\n\nVisit http://bunnymatic.com\n. Yep - looks like you're correct.  That is, I can't seem to repro the case.\nWe ran into this while trying to send files through https://github.com/waynehoover/s3_direct_upload and now i'm wondering if it was related to spaces and possibly double html encoding somewhere along the way.  I'll close this issue and do some more research.  \nIf I can build a good test case, i'll submit a new issue.\nCheers\n. ",
    "nickborromeo": "@bunnymatic I am not sure if you agree but maybe simply calling string on key in this method will do the trick\n``` ruby\n    class S3Object\n  include Core::Model\n  include DataOptions\n  include ACLOptions\n  include AWS::S3::EncryptionUtils\n\n  # @param [Bucket] bucket The bucket this object belongs to.\n  # @param [String] key The object's key.\n  def initialize(bucket, key, opts = {})\n    super\n    @key = key\n    @bucket = bucket\n  end\n\n```\n. ",
    "luis-ortiz": "Sure!\nMocaltzi:ops-prodwest lfo$ gem env\nRubyGems Environment:\n  - RUBYGEMS VERSION: 1.8.23\n  - RUBY VERSION: 1.9.3 (2013-06-27 patchlevel 448) [x86_64-darwin12.5.0]\n  - INSTALLATION DIRECTORY: /Users/lfo/.rvm/gems/ruby-1.9.3-p448\n  - RUBY EXECUTABLE: /Users/lfo/.rvm/rubies/ruby-1.9.3-p448/bin/ruby\n  - EXECUTABLE DIRECTORY: /Users/lfo/.rvm/gems/ruby-1.9.3-p448/bin\n  - RUBYGEMS PLATFORMS:\n    - ruby\n    - x86_64-darwin-12\n  - GEM PATHS:\n     - /Users/lfo/.rvm/gems/ruby-1.9.3-p448\n     - /Users/lfo/.rvm/gems/ruby-1.9.3-p448@global\n  - GEM CONFIGURATION:\n     - :update_sources => true\n     - :verbose => true\n     - :benchmark => false\n     - :backtrace => false\n     - :bulk_threshold => 1000\n  - REMOTE SOURCES:\n     - http://rubygems.org/\n. ",
    "codwazny": "You are correct, the #security_groups[id] loses context of the VPC.  I wasn't sure if this was an bug or a design limitation.  It seems to be the latter.  I think your work around should be fine for my needs for now (and is significantly less terrible than mine).  I look forward to v2 when it is fully ready for prime time.\n. ",
    "zoltrain": "It was an issue with the type I was loading into the selection.\nThe variable I was passing into the [] accessor was a symbol so my error. It was being loaded from a YAML file and converted into a Symbol.\nShould've known programming late and making mistakes go hand in hand.\nThanks for helping though.\nWould it be worth adding the Class type to the error massage inside options_grammar?\nInstead of:\n`validate': expected string value for option server_certificate_name \nMaybe:\n`validate': expected string value for option server_certificate_name, got '{class name}'\nSo in my case:\n`validate': expected string value for option server_certificate_name, got 'Symbol'\n. ",
    "mikestanley": "Sure.\nwe have a multiple long running processes spread across multiple machines\nthat pull jobs off of queues (zeromq).  the process is intended to run\nforever.  the process is single threaded and handles a single job at a\ntime.  the dynamodb client is established once per process and use that to\nprocess requests.  a single job is several read/writes across several\ntables (basic usage, small <1k puts, and basic key lookup, with the\noccassional limit 1 on a key/range query).\ninitialization follows something akin to this:\nAWS.eager_autoload!(AWS::Core)\n  AWS.eager_autoload!(AWS::Record)\n  AWS.eager_autoload!(AWS::DynamoDB)\n  AWS.config(config['dynamodb'])\n  @dynamodb = AWS::DynamoDB.new(config['dynamodb'])\nin 5 minutes a single process probably handles ~700-1000 jobs.\nwith 1.36 each process climbs to about 1024 FD and starts through random\nerrors around DNS resolution, ssl verify, etc.  with 1.35 each process\nstays steady at around 40 open fds.\nOn Sat, Mar 8, 2014 at 12:36 AM, Loren Segal notifications@github.comwrote:\n\nCan you provide a few details about how you are using DynamoDB? Are you\nconstructing multiple DynamoDB clients?\n\nReply to this email directly or view it on GitHubhttps://github.com/aws/aws-sdk-ruby/issues/486#issuecomment-37089922\n.\n. Sorry accidentally closed this.  I reopened it.  This is still very much an issue (I just got confused by the two button choices on the mobile view of github ;-)\n. \n",
    "gogogehring": "Thanks!!\n. ",
    "mirakui": "Oops, I found the reason why it is. #343\nIs the compatibility problem still going?\n. I see. Thanks.\nDoes aws-sdk-core support the 2012-08-10 API?\n. I see. :+1: \n. ",
    "samwgoldman": "I see now that I should be using S3Object#url_for instead of PresignV4 directly.\n. ",
    "vireshas": "Thanks a lot for that update @samwgoldman!\n. ",
    "littlechu": "It's related to the Ruby Code for the client head_object functions\n```\n      # @overload head_object(options = {})\n      #   @param [Hash] options\n      #   @option options [required,String] :bucket_name\n      #   @option options [required,String] :key\n      #   @option options [String] :version_id\n      #   @return [Core::Response]\n      object_method(:head_object, :head) do\n    configure_request do |req, options|\n      super(req, options)\n      if options[:version_id]\n        req.add_param('versionId', options[:version_id])\n      end\n    end\n\n    process_response do |resp|\n      extract_object_headers(resp)\n    end\n\n  end\n\n```\nIf I understand the code correctly, it will only process version_id options. Since the API state that head_object should function like get_object without a response body. I would expect options like :if_modified_since, :range will work for head_object functions.\nIf you look for the get_object code, it handle theses cases\n```\n      object_method(:get_object, :get,\n                    :header_options => {\n                      :if_modified_since => \"If-Modified-Since\",\n                      :if_unmodified_since => \"If-Unmodified-Since\",\n                      :if_match => \"If-Match\",\n                      :if_none_match => \"If-None-Match\"\n                    }) do\n        configure_request do |req, options|\n      super(req, options)\n\n      if options[:version_id]\n        req.add_param('versionId', options[:version_id])\n      end\n\n      [\"If-Modified-Since\",\n       \"If-Unmodified-Since\"].each do |date_header|\n        case value = req.headers[date_header]\n        when DateTime\n          req.headers[date_header] = Time.parse(value.to_s).rfc2822\n        when Time\n          req.headers[date_header] = value.rfc2822\n        end\n      end\n\n      if options[:range]\n        range = options[:range]\n        if range.is_a?(Range)\n          offset = range.exclude_end? ? -1 : 0\n          range = \"bytes=#{range.first}-#{range.last + offset}\"\n        end\n        req.headers['Range'] = range\n      end\n\n    end\n\n    process_response do |resp|\n      extract_object_headers(resp)\n      resp.data[:data] = resp.http_response.body\n    end\n\n  end\n\n```\nThanks for looking into this.\n. ",
    "CallumD": "Forgive my lack of knowledge how would I find this?\nI tried the following:\nruby\nirb(main):002:0> AWS::VERSION\n=> \"1.8.5\"\nIs it the case I am just miles behind?\nThanks for your help!\n. Ok upgrading the Gem worked. Sorry to have wasted your time. Thanks again!\n. Confirmed works with the latest version. Thanks!\n. ",
    "syslxg": "the problem is still there.\n. ",
    "gtalton": "Thanks Trevor, that's all I was looking for at this point .. I'll keep an\neye on this gem for now. Thanks.\nOn Wed, Apr 2, 2014 at 4:45 PM, Trevor Rowe notifications@github.comwrote:\n\nI should clarify. We do plan to support Workspaces once their API is\npublic.\n\nReply to this email directly or view it on GitHubhttps://github.com/aws/aws-sdk-ruby/issues/500#issuecomment-39387992\n.\n. \n",
    "NZKoz": "You don't need to memoize responses though, couldn't something as simple as:\nruby\n      def each(&block)\n        response = filtered_request(:describe_instances)\n        response.reservation_set.each do |reservation|\n          reservation.instances_set.each do |i|\n            instance = Instance.new(i.instance_id, :config => config)\n            instance.send(:cache_static_attributes, :describe_instances, i)\n            yield(instance)\n          end\n        end\n      end\nSolve the commonest cases?  I'm running some tests locally with that change and it's much snappier and static attributes don't cause the n+1 query issue.\n. On another note, why is image_id not a static attribute?  The only git log for it is in a massive patch bomb (0d31588d6e80cfc0ec9bd53d48273c1b609fad8f) so it's hard to figure out if there's a strange case where that value can change?  that's the last of my performance issues wrapped up \n. Awesome, thanks folks!\n. ",
    "rpocklin": "I'm in the same boat - @smoi how about you give a reason for closing this?  Is there a way to do it in the Ruby SDK?\n. ",
    "haswalt": "Can anyone comment on this? I can't find out how to do this without monkey patching the SDK and this is closed without comment\n. ",
    "wormzer": "Ah I see. I was hung up on having an option in InstanceCollection#create (and I actually submitted a pull request with an implementation). If my pull request doesn't make it in I'll fall back to one of these two alternatives, thanks so much.\n. ",
    "eicca": "@lsegal thanks for notice, done.\n. ",
    "caerulean": "After a bit of messing around I found where it's localised to. I used caller at the start of the method where the stack error is thrown. Here is the stack trace:\n``\n[1/1] AwsInterfaceTest#test_terminate_rds_instance_worksC:/Dev/conformity-shrew/test/aws_test_helper.rb:296:inget_or_create_rds_instance'\nC:/Dev/conformity-shrew/test/factories/rds_instances.rb:19:in block (3 levels)\nin <top (required)>'\nC:/Dev/Ruby200/lib/ruby/gems/2.0.0/gems/factory_girl-4.3.0/lib/factory_girl/attribute_assigner.rb:48:ininstance_exec'\nC:/Dev/Ruby200/lib/ruby/gems/2.0.0/gems/factory_girl-4.3.0/lib/factory_girl/attribute_assigner.rb:48:in build_class_instance'\nC:/Dev/Ruby200/lib/ruby/gems/2.0.0/gems/factory_girl-4.3.0/lib/factory_girl/attribute_assigner.rb:13:inobject'\nC:in object'\nC:/Dev/Ruby200/lib/ruby/gems/2.0.0/gems/factory_girl-4.3.0/lib/factory_girl/strategy/build.rb:9:inresult'\nC:/Dev/Ruby200/lib/ruby/gems/2.0.0/gems/factory_girl-4.3.0/lib/factory_girl/factory.rb:42:in run'\nC:/Dev/Ruby200/lib/ruby/gems/2.0.0/gems/factory_girl-4.3.0/lib/factory_girl/factory_runner.rb:23:inblock in run'\nC:/Dev/Ruby200/lib/ruby/gems/2.0.0/gems/activesupport-3.2.11/lib/active_support/notifications.rb:125:in instrument'\nC:/Dev/Ruby200/lib/ruby/gems/2.0.0/gems/factory_girl-4.3.0/lib/factory_girl/factory_runner.rb:22:inrun'\nC:/Dev/Ruby200/lib/ruby/gems/2.0.0/gems/factory_girl-4.3.0/lib/factory_girl/strategy_syntax_method_registrar.rb:20:in block in define_singular_strategy_method'\ntest//aws//aws_interface_test.rb:133:intest_terminate_rds_instance_works'\nC:/Dev/Ruby200/lib/ruby/2.0.0/test/unit.rb:858:in run_test'\nC:/Dev/Ruby200/lib/ruby/2.0.0/minitest/unit.rb:1301:inrun'\nC:/Dev/Ruby200/lib/ruby/2.0.0/test/unit/testcase.rb:17:in run'\nC:/Dev/Ruby200/lib/ruby/2.0.0/minitest/unit.rb:919:inblock in _run_suite'\nC:/Dev/Ruby200/lib/ruby/2.0.0/minitest/unit.rb:912:in map'\nC:/Dev/Ruby200/lib/ruby/2.0.0/minitest/unit.rb:912:in_run_suite'\nC:/Dev/Ruby200/lib/ruby/2.0.0/test/unit.rb:657:in block in _run_suites'\nC:/Dev/Ruby200/lib/ruby/2.0.0/test/unit.rb:655:ineach'\nC:/Dev/Ruby200/lib/ruby/2.0.0/test/unit.rb:655:in _run_suites'\nC:/Dev/Ruby200/lib/ruby/2.0.0/minitest/unit.rb:867:in_run_anything'\nC:/Dev/Ruby200/lib/ruby/2.0.0/minitest/unit.rb:1060:in run_tests'\nC:/Dev/Ruby200/lib/ruby/2.0.0/minitest/unit.rb:1047:inblock in _run'\nC:/Dev/Ruby200/lib/ruby/2.0.0/minitest/unit.rb:1046:in each'\nC:/Dev/Ruby200/lib/ruby/2.0.0/minitest/unit.rb:1046:in_run'\nC:/Dev/Ruby200/lib/ruby/2.0.0/minitest/unit.rb:1035:in run'\nC:/Dev/Ruby200/lib/ruby/2.0.0/test/unit.rb:21:inrun'\nC:/Dev/Ruby200/lib/ruby/2.0.0/test/unit.rb:774:in run'\nC:/Dev/Ruby200/lib/ruby/2.0.0/test/unit.rb:366:inblock (2 levels) in autorun'\nC:/Dev/Ruby200/lib/ruby/2.0.0/test/unit.rb:27:in run_once'\nC:/Dev/Ruby200/lib/ruby/2.0.0/test/unit.rb:365:inblock in autorun'\n = 7.99 s\n  1) Error:\ntest_terminate_rds_instance_works(AwsInterfaceTest):\nSystemStackError: stack level too deep\n    C:/Dev/Ruby200/lib/ruby/gems/2.0.0/gems/aws-sdk-1.35.0/lib/aws/core/data.rb:194\nFinished tests in 8.012801s, 0.1248 tests/s, 0.0000 assertions/s.\n1 tests, 0 assertions, 0 failures, 1 errors, 0 skips\nruby -v: ruby 2.0.0p451 (2014-02-24) [i386-mingw32]\n```\nHere is the method, I have a feeling it has something to do with waiting for it to be created. The stack trace is usually thrown around that point in the code, I check the console and the db instance is still in its creating stage but my raise doesn't show in the output.\n```\ndef create_rds_instance(options)\n    puts caller\n    i = AWS.rds.instances.client.create_db_instance(db_name: options[:name], \n                                        db_instance_identifier: \"\", \n                                        allocated_storage: 5,\n                                        db_instance_class: \"db.t1.micro\",\n                                        engine: \"mysql\",\n                                        master_username: \"\",\n                                        master_user_password: \"\",\n                                        db_subnet_group_name: \"\",\n                                        publicly_accessible: false)\n    account_no = find_current_account_no\nif options[:tags]\n  options[:tags].each do | tag_name, tag_value |\n    i.client.add_tags_to_resource(resource_name: account_no, tags: [key: tag_name, value: tag_value])\n  end\nend\n\nwaited = 0\nwhile AWS.rds.instances[i.id].status == :creating \n  sleep(1)\n  waited += 1\n  raise 'Waited too long for RDS Instance to move out of creating state' if waited > 60\nend\n\nregister_rds_instance_to_be_removed(i.id)\ni\n\nend\n```\nNote: I have upgraded to ruby 2.0 since the first comment.\n. The create method is only used for testing the terminate method. When I run the test, it fails in the create method at some point (a db instance is still created), then I re-run the test and because it doesn't have to go through the create method, it runs fine and the test passes.\n. ",
    "sandstrom": "Thanks for the clarification, much helpful!\nI see some benefit in using the same signature as IO#read, but that's up to you. I understand that there are tradeoffs too, e.g. backwards compatibility.\n. ",
    "cbnoonan": "It looks like there is a Rails 4 issue with aws-sdk (1.36.1)\nBecause of the error message \"Credential should be scoped to a valid region, not 'us-east-1'\"\nI first checked out where my queue was located, which was Oregon (us-west-2). \nI changed this to a location in Virginia (us-east-1), but still I received this error message.\nI saw this link (https://github.com/aws/aws-sdk-java/issues/176) and tried downgrading. It worked!\nWhen I downgraded to 1.11.1 while using SQS in us-west, there was no error message with the same exact code.\n. Any idea when the release is scheduled for?  I just tried 1.38.0 and it\nstill doesn't work correctly.\nThank you.\nOn Fri, Apr 11, 2014 at 2:58 PM, Trevor Rowe notifications@github.comwrote:\n\nThanks for the bug report. The commit above should resolve this issue. It\nshould go out with the next release.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/aws/aws-sdk-ruby/issues/508#issuecomment-40257411\n.\n\n\nDare to be remarkable.\n. ",
    "leonsodhi": "Related to part 2 above, hopefully someone with more SSL & S3 related knowledge can comment on whether this is relevant, but according to the Wildcard certificate page on Wikipedia:\n\nIn the case of a wildcard certificate for *.company.com\nBecause the wildcard only covers one level of subdomains (the asterisk doesn't match full stops), these domains would not be valid for the certificate:\ntest.login.company.com\n\nIn this case, it looks like the AWS wildcard cert has a subject of *.s3.amazonaws.com, so that would be fine for bucket.s3.amazonaws.com but not for bucket.example.com.s3.amazonaws.com.\n. ",
    "troy": "+1 on @leonsodhi's comment. That makes me wonder whether the destination endpoint contained in a PermanentRedirect needs to be provided to the library differently, or the API expects callers to somehow infer the region and set that. I'd gladly pass in region instead of s3_endpoint if I knew it.\n. We ended up brute-forcing the region by trying each of them (AWS::S3.regions), then caching the one which works. It sucks but I haven't seen any other solution when you know but can't control the bucket name and neither know nor control the region.\n. Nice handling, @guss77.\n. Thanks for answering! I long ago gave up on ever seeing an update, so this is like Christmas in January.\n. ",
    "SimonBirrell": "I'm tearing my hair out. \nI need to use all three of the following:\n1. The Ruby SDK\n2. A bucket name with dots \n3. A region outside of us-east ?\nFor customer reasons I need to use sa-east-1.\nFor static website hosting I need to use a name with dots.\nEvery way I try to do it I get\nAWS::S3::Errors::PermanentRedirect: The bucket you are attempting to access must be addressed using the specified endpoint. Please send all future requests to this endpoint.\nThis can be worked around if the bucket name has no dots, but not if it has.\n. I did the same. Here's my brute force implementation of S3Object#copy_to\nAbsurd!\n```\n    def brute_force_copy_to(bucket, source_key, target_key)\n      success = false\n  if @last_region_that_worked.present?\n    success = attempt_to_copy(bucket, source_key, target_key, @last_region_that_worked)\n  end\n\n  AWS::S3.regions.each do |region|\n    break if success\n    success = attempt_to_copy(bucket, source_key, target_key, region.name)\n    @last_region_that_worked = region if success\n  end\n\n  raise RuntimeError, \"No endpoint worked!\" if !success      \nend\n\ndef attempt_to_copy(bucket, source_key, target_key, region)\n  success = true\n  s3 = AWS::S3.new(region: region)\n  source_object = s3.buckets[bucket].objects[source_key]\n\n  begin\n    source_object.copy_to(target_key)\n  rescue AWS::S3::Errors::PermanentRedirect => e\n    success = false\n  end  \n  success      \nend\n\n```\n. ",
    "guss77": "Here is my way of handling PermanentRedirect errors - try not to get them. Instead, check if the S3 bucket has location_constraint and if so re-login to that region:\ns3 = AWS::S3.new(awscreds)\n    if s3.buckets[bucket].location_constraint != awscreds[:region] then\n      # need to re-login, otherwise the S3 upload will fail\n      s3 = AWS::S3.new(awscreds.merge(region: s3.buckets[bucket].location_constraint))\n    end\n. Thanks for the merge. When do you plan to release an update to rubygems.org?\n. OK, thanks\n. Hi, any chance of shooting a gem release with this API change? \nI'm really dependany on this for my workflow and I would love to drop my private repo - it makes a mess with bundle installs.\nThanks.\n. Thanks. That's OK - currently I'm using a local patch to get my VPC peering connection fix. If there's a release every month or two, I'm fine with that and I'll just port features from my local branch to the official v1 as time permits.\n. ",
    "Jimflip": "I understand what your saying about Bundler, but this situation isn't quite so simple, it's an external  daemon process which doesn't support bundler installed via Chef.\n. ",
    "nonsense": "You're right, I tested it outside of my application and it works fine.\nI guess this must be due to some weird dependency... libxml2 or nokogiri version mismatch.\nI will try to debug further and find the issue and post it here.\n. This is really weird.\nIf I run the commands below, and review the output of YAML.load(File.read(path)) / api_config , the command succeeds and loads the contents of the path (for example .rvm/gems/ruby-2.1.1/gems/aws-sdk-1.38.0/lib/aws/api_config/EC2-2013-08-15.yml)\nHowever inside irb, the Hash keys are accessible (api_config[:operations] works fine), whereas if ran from within rails (for example rails c), the Hash keys are not accessible - api_config[:operations] is nil.\nI guess YAML is not working properly inside rails, but not sure how to fix it.\nSee attached screenshots for difference between irb and rails c outputs - the Hash is formatted differently when YAML.load is executed inside irb and rails c, which leads to the nil pointer exception.\nirb\nrequire 'aws-sdk'\nrequire 'pry'\nAWS.config (...)\nAWS.ec2\n// inspect api_config\nrails c\nAWS.config (...)\nAWS.ec2\n// inspect api_config\n\n\n. So when YAML.load(File.open(path)) is executed in rails, the Hash keys should be inside quotes:\napi_config[\":operations\"] works whereas api_config[:operations] is nil\nWhen the same command is executed in irb, the Hash keys work without quotes, just as the gem expected them to.\n. At the end it turned out to be a problem with gem 'bloggy' a dependency in my project.\nbloggy (0.3)\n        jekyll (~> 1.4.2)\n        rack-contrib\n        rails\n        rdiscount\ngem 'safe_yaml' provides an alternative implementation of YAML.load and makes AWS gem crash by its default settings.\n. ",
    "jmahowald": "I'm experiencing the same issue.  Is there a prescribed way if I am using safe_yaml to avoid the problem?\n. ",
    "mfamilia": "see safe_yaml gem documentation about configuration options since deserializing symbols is turned off by default.\nhttps://github.com/dtao/safe_yaml\n. ",
    "Tim-Scott": "Interested in this as well.  Were you able to figure it out?\n. ",
    "Jud": "@trevorrowe that looks perfect to me -- I assume that I could just leave the access_key_id and secret_access_key blank to have the STS client pull from EC2 roles?\n. I will take that for a spin this week. Exactly what I was looking for!\n. ",
    "d-smith": "Ah - did not think to check the time on my machine. I tried it again this morning and did not see the same problem.\n. I would like to retract/close this issue if possible.\n. ",
    "jwarchol": "Thank you, I think that was it exactly. What confused me was that I was able to list topics and saw the right number. Turns out I have two topics in us-east-1 that I'd never really used myself (SES related, and billing alerts) and it coincidentally matched the number in us-west-1. When I configured the client properly it all worked. \n. ",
    "knu": "Preloading Digest::SHA256, i.e. require 'digest/sha2' at the top level, may work as a cure in the meantime.\nI'll investigate it when I get the time, maybe this weekend.\n. ",
    "johanneswuerbach": "Thanks, I don't see the issue anymore after applying your patch :smile: \n. ",
    "Swimminschrage": "Thanks for the heads up around the clobbering.\nI'll take a look at creating a separate module for this.\n. So I decided to simply add the S3 hosted zone id strings as a Hash (keyed off of region) within the HostedZone class since they are related.  Couldn't decide on any better place to put them.\n. ",
    "jrab89": "I'm also getting the same result with ruby-2.0.0-p451\n. Running the following with ruby-2.0.0-p247 and aws-sdk (1.40.0):\nrequire 'aws-sdk'\nAWS.config( :access_key_id => ENV['AWS_ACCESS_KEY_ID'], :secret_access_key => ENV['AWS_SECRET_ACCESS_KEY'])\nputs AWS.s3.config.s3_endpoint\nputs AWS.s3(http_wire_trace:true).buckets.first.exists?\nResults in:\ns3.us-east-1a.amazonaws.com\nopening connection to s3.us-east-1a.amazonaws.com:443...\nopening connection to s3.us-east-1a.amazonaws.com:443...\nopening connection to s3.us-east-1a.amazonaws.com:443...\nopening connection to s3.us-east-1a.amazonaws.com:443...\n/Users/uraboje/.rvm/rubies/ruby-2.0.0-p247/lib/ruby/2.0.0/net/http.rb:878:in `initialize': getaddrinfo: nodename nor servname provided, or not known (SocketError)\n    from /Users/uraboje/.rvm/rubies/ruby-2.0.0-p247/lib/ruby/2.0.0/net/http.rb:878:in `open'\n    from /Users/uraboje/.rvm/rubies/ruby-2.0.0-p247/lib/ruby/2.0.0/net/http.rb:878:in `block in connect'\n    from /Users/uraboje/.rvm/rubies/ruby-2.0.0-p247/lib/ruby/2.0.0/timeout.rb:66:in `timeout'\n    from /Users/uraboje/.rvm/rubies/ruby-2.0.0-p247/lib/ruby/2.0.0/net/http.rb:877:in `connect'\n    from /Users/uraboje/.rvm/rubies/ruby-2.0.0-p247/lib/ruby/2.0.0/net/http.rb:862:in `do_start'\n    from /Users/uraboje/.rvm/rubies/ruby-2.0.0-p247/lib/ruby/2.0.0/net/http.rb:857:in `start'\n    from /Users/uraboje/.rvm/gems/ruby-2.0.0-p247/gems/aws-sdk-1.40.0/lib/aws/core/http/connection_pool.rb:321:in `start_session'\n    from /Users/uraboje/.rvm/gems/ruby-2.0.0-p247/gems/aws-sdk-1.40.0/lib/aws/core/http/connection_pool.rb:125:in `session_for'\n    from /Users/uraboje/.rvm/gems/ruby-2.0.0-p247/gems/aws-sdk-1.40.0/lib/aws/core/http/net_http_handler.rb:55:in `handle'\n    from /Users/uraboje/.rvm/gems/ruby-2.0.0-p247/gems/aws-sdk-1.40.0/lib/aws/core/client.rb:253:in `block in make_sync_request'\n    from /Users/uraboje/.rvm/gems/ruby-2.0.0-p247/gems/aws-sdk-1.40.0/lib/aws/core/client.rb:289:in `retry_server_errors'\n    from /Users/uraboje/.rvm/gems/ruby-2.0.0-p247/gems/aws-sdk-1.40.0/lib/aws/core/client.rb:249:in `make_sync_request'\n    from /Users/uraboje/.rvm/gems/ruby-2.0.0-p247/gems/aws-sdk-1.40.0/lib/aws/core/client.rb:511:in `block (2 levels) in client_request'\n    from /Users/uraboje/.rvm/gems/ruby-2.0.0-p247/gems/aws-sdk-1.40.0/lib/aws/core/client.rb:391:in `log_client_request'\n    from /Users/uraboje/.rvm/gems/ruby-2.0.0-p247/gems/aws-sdk-1.40.0/lib/aws/core/client.rb:477:in `block in client_request'\n    from /Users/uraboje/.rvm/gems/ruby-2.0.0-p247/gems/aws-sdk-1.40.0/lib/aws/core/client.rb:373:in `return_or_raise'\n    from /Users/uraboje/.rvm/gems/ruby-2.0.0-p247/gems/aws-sdk-1.40.0/lib/aws/core/client.rb:476:in `client_request'\n    from (eval):3:in `list_buckets'\n    from /Users/uraboje/.rvm/gems/ruby-2.0.0-p247/gems/aws-sdk-1.40.0/lib/aws/s3/bucket_collection.rb:140:in `each'\n    from foo.rb:5:in `first'\n    from foo.rb:5:in `<main>'\n. Yeah I'm dumb, thanks for your help!\n. After looking into this some more it turns out that only the host https://support.us-east-1.amazonaws.com/ exists. Getting the support client object from a different region object would cause the methods of the support client to try to make requests to hosts like https://support.us-west-2.amazonaws.com/, which don't exist.\nMaybe it would be a good idea to have some higher level methods in AWS::Support instead of only being able to use AWS::Support::Client?\n. ",
    "erupenkman": "just for anyone else, when running the opsworks register command, DO NOT change the region from us-east-1 to match the region your instances run in.\nI got the same error message and came here from search engine, hope it helps someone.\n. ",
    "flippyhead": "@erupenkman well that's funny -- your comment got me unstuck from quite a bit of confusion. Apparently opsworks command and control is us-east-1 only \n. ",
    "cer": "That's great. The same is probably true for SDKs for other languages and AWS APIs.\n. Cool. You are welcome!\nOn Fri, Jan 16, 2015 at 2:04 PM, Trevor Rowe notifications@github.com\nwrote:\n\nI've passed the request along to the team that maintains the Amazon S3 API\nreference documentation. Thanks for the feature request.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/aws/aws-sdk-ruby/issues/533#issuecomment-70330033.\n. \n",
    "repeatedly": "Great!\nIt helps this issue :)\nhttps://github.com/fluent/fluent-plugin-s3/issues/45\n. ",
    "shterrett": "Thanks for the quick response; that makes sense. I'll keep stubbing my singleton. \n. ",
    "LandonSchropp": "Ah, good call on the file system mocking. That seems to be the problem. Thanks for the help!\n. ",
    "pquery": "from both our gemfile and gemfile.lock we're using gem 'aws-sdk', '1.22.1'\nI will upgrade our gem and see if it fixes the issue, the gem has been working so well I didn't even think about it it being a gemfile.lock issue. \n. I have updated my gem to version 1.33.0, however I am still not able to modify the :access_log  attribute, which is what I was going for in the first place. \nI was able to based on the sample code of issue 450 to enable cross_zone_load_balancing, however on the modification options that is 1 of 3 modifications available. \n```\nirb(main):059:0>  eclient.modify_load_balancer_attributes(\nirb(main):060:1       :load_balancer_name => \"#{account_alias}\",\nirb(main):061:1       :load_balancer_attributes => {\nirb(main):062:2* :cross_zone_load_balancing=>{:enabled=>true}\nirb(main):063:2> }\nirb(main):064:1> )\n[AWS Core 200 0.652217 0 retries] modify_load_balancer_attributes(:load_balancer_attributes=>{:cross_zone_load_balancing=>{:enabled=>true}},:load_balancer_name=>\"vpnpoc\")  \n=> {:load_balancer_name=>\"vpnpoc\", :load_balancer_attributes=>{:cross_zone_load_balancing=>{:enabled=>\"true\"}}, :response_metadata=>{:request_id=>\"39505965-e661-11e3-9d89-a74c362d1586\"}}\n```\nWhen I added either access_log or connection_draining to the request however, the request would come back with - ArgumentError: unexpected key access_log for option load_balancer_attributes\nThis is the same error which would return if the only modification being passed was the access_logs. \nirb(main):065:0> eclient.modify_load_balancer_attributes(\nirb(main):066:1*        :load_balancer_name => \"#{account_alias}\",\nirb(main):067:1*        :load_balancer_attributes => {\nirb(main):068:2*          :cross_zone_load_balancing=>{:enabled=>true},\nirb(main):069:2*          :access_log=>{:enabled=>true,:s3_bucket_name => \"#{bucket_name}\"}\nirb(main):070:2>          }\nirb(main):071:1>        )\nArgumentError: unexpected key access_log for option load_balancer_attributes\n    from /home/adam/.rvm/gems/ruby-2.0.0-p353/gems/aws-sdk-1.33.0/lib/aws/core/option_grammar.rb:344:in `block in validate'\n    from /home/adam/.rvm/gems/ruby-2.0.0-p353/gems/aws-sdk-1.33.0/lib/aws/core/option_grammar.rb:342:in `each'\n    from /home/adam/.rvm/gems/ruby-2.0.0-p353/gems/aws-sdk-1.33.0/lib/aws/core/option_grammar.rb:342:in `validate'\n    from /home/adam/.rvm/gems/ruby-2.0.0-p353/gems/aws-sdk-1.33.0/lib/aws/core/option_grammar.rb:590:in `block in validate'\n    from /home/adam/.rvm/gems/ruby-2.0.0-p353/gems/aws-sdk-1.33.0/lib/aws/core/option_grammar.rb:586:in `each'\n    from /home/adam/.rvm/gems/ruby-2.0.0-p353/gems/aws-sdk-1.33.0/lib/aws/core/option_grammar.rb:586:in `validate'\n    from /home/adam/.rvm/gems/ruby-2.0.0-p353/gems/aws-sdk-1.33.0/lib/aws/core/option_grammar.rb:601:in `request_params'\n    from /home/adam/.rvm/gems/ruby-2.0.0-p353/gems/aws-sdk-1.33.0/lib/aws/core/query_request_builder.rb:37:in `populate_request'\n    from /home/adam/.rvm/gems/ruby-2.0.0-p353/gems/aws-sdk-1.33.0/lib/aws/core/client.rb:737:in `block (2 levels) in define_client_method'\n    from /home/adam/.rvm/gems/ruby-2.0.0-p353/gems/aws-sdk-1.33.0/lib/aws/core/client.rb:557:in `build_request'\n    from /home/adam/.rvm/gems/ruby-2.0.0-p353/gems/aws-sdk-1.33.0/lib/aws/core/client.rb:490:in `block (3 levels) in client_request'\n    from /home/adam/.rvm/gems/ruby-2.0.0-p353/gems/aws-sdk-1.33.0/lib/aws/core/response.rb:171:in `call'\n    from /home/adam/.rvm/gems/ruby-2.0.0-p353/gems/aws-sdk-1.33.0/lib/aws/core/response.rb:171:in `build_request'\n    from /home/adam/.rvm/gems/ruby-2.0.0-p353/gems/aws-sdk-1.33.0/lib/aws/core/response.rb:111:in `initialize'\n    from /home/adam/.rvm/gems/ruby-2.0.0-p353/gems/aws-sdk-1.33.0/lib/aws/core/client.rb:203:in `new'\n    from /home/adam/.rvm/gems/ruby-2.0.0-p353/gems/aws-sdk-1.33.0/lib/aws/core/client.rb:203:in `new_response'\n    from /home/adam/.rvm/gems/ruby-2.0.0-p353/gems/aws-sdk-1.33.0/lib/aws/core/client.rb:489:in `block (2 levels) in client_request'\n    from /home/adam/.rvm/gems/ruby-2.0.0-p353/gems/aws-sdk-1.33.0/lib/aws/core/client.rb:390:in `log_client_request'\n    from /home/adam/.rvm/gems/ruby-2.0.0-p353/gems/aws-sdk-1.33.0/lib/aws/core/client.rb:476:in `block in client_request'\n    from /home/adam/.rvm/gems/ruby-2.0.0-p353/gems/aws-sdk-1.33.0/lib/aws/core/client.rb:372:in `return_or_raise'\n    from /home/adam/.rvm/gems/ruby-2.0.0-p353/gems/aws-sdk-1.33.0/lib/aws/core/client.rb:475:in `client_request'\n    from (eval):3:in `modify_load_balancer_attributes'\n    from (irb):65\n    from /home/adam/.rvm/gems/ruby-2.0.0-p353/gems/railties-4.0.0/lib/rails/commands/console.rb:90:in `start'\n    from /home/adam/.rvm/gems/ruby-2.0.0-p353/gems/railties-4.0.0/lib/rails/commands/console.rb:9:in `start'\n    from /home/adam/.rvm/gems/ruby-2.0.0-p353/gems/railties-4.0.0/lib/rails/commands.rb:64:in `<top (required)>'\n. I've now updated, after looking at the versions listed on rubygems.org vs the versions on the release notes to 1.41.0 and the ability to turn on and off the access logs is now available. \n. ",
    "dhilsand": "What happened to the 'tables' collection that was defined on the client. Right now, I'm getting\n undefined method `tables' for #AWS::DynamoDB::Client::V20111205 (NoMethodError) \n. ",
    "aruprakshit": "@trevorrowe  Any idea which version of API Dynamo DB local does support ?\nArup-iMac$ irb\n2.1.2 :001 > require 'aws-sdk'\n => true\n2.1.2 :002 > provider = AWS::Core::CredentialProviders::SharedCredentialFileProvider.new(profile_name: \"crawler\")\n => #<AWS::Core::CredentialProviders::SharedCredentialFileProvider:0x00000102e55080 @path=\"/Users/shreyas/.aws/credentials\", @profile_name=\"crawler\">\n2.1.2 :003 > AWS.config(use_ssl: false, dynamo_db: { endpoint: 'localhost', port: '8000' }, credential_provider: provider)\n => <AWS::Core::Configuration>\n2.1.2 :004 > ddb = AWS::DynamoDB::Client.new\n => #<AWS::DynamoDB::Client::V20111205>\n2.1.2 :005 > ddb.list_tables\nAWS::DynamoDB::Errors::InvalidAction: DynamoDB Local does not support v1 API.\n    from /Users/shreyas/.rvm/gems/ruby-2.1.2@yelloday/gems/aws-sdk-v1-1.58.0/lib/aws/core/client.rb:375:in `return_or_raise'\n    from /Users/shreyas/.rvm/gems/ruby-2.1.2@yelloday/gems/aws-sdk-v1-1.58.0/lib/aws/core/client.rb:476:in `client_request'\n    from (eval):3:in `list_tables'\n    from (irb):5\n    from /Users/shreyas/.rvm/rubies/ruby-2.1.2/bin/irb:11:in `<main>'\n2.1.2 :006 >\n. This was an version issue.\n. ",
    "ttp": "have problems with connection to 'http://locahost:8000' (v2 client),\nresolved by changing it to 'http://127.0.0.1:8000'\n. ",
    "lreeves": "Oh okay, thanks Trevor!\n. ",
    "oggy": "You're totally right -- my fail.\n. ",
    "y13i": "@trevorrowe Thank you for your notice! Fixed.\n. ",
    "lanrion": "@awood45 \nHi: \n`` ruby\n/home/service/.rbenv/versions/1.9.3-p448/lib/ruby/gems/1.9.1/gems/bosh_cli-1.2175.0/lib/cli/release_comp\niler.rb:164:inrescue in find_in_indices': Blobstore error: Failed to fetch object, underlying error: #\n /home/service/.rben\nv/versions/1.9.3-p448/lib/ruby/gems/1.9.1/gems/aws-sdk-1.32.0/lib/aws/core/http/net_http_handler.rb:83:i\nn ensure in block (2 levels) in handle' (Bosh::Cli::BlobstoreError)\n/home/service/.rbenv/versions/1.9.3-p448/lib/ruby/gems/1.9.1/gems/aws-sdk-1.32.0/lib/aws/core/http/net_h\nttp_handler.rb:83:inblock (2 levels) in handle'\n/home/service/.rbenv/versions/1.9.3-p448/lib/ruby/1.9.1/net/http.rb:1323:in block (2 levels) in transpo\nrt_request'\n/home/service/.rbenv/versions/1.9.3-p448/lib/ruby/1.9.1/net/http.rb:2672:inreading_body'\n/home/service/.rbenv/versions/1.9.3-p448/lib/ruby/1.9.1/net/http.rb:1322:in block in transport_request'\n/home/service/.rbenv/versions/1.9.3-p448/lib/ruby/1.9.1/net/http.rb:1317:incatch'\n/home/service/.rbenv/versions/1.9.3-p448/lib/ruby/1.9.1/net/http.rb:1317:in transport_request'\n/home/service/.rbenv/versions/1.9.3-p448/lib/ruby/1.9.1/net/http.rb:1294:inrequest'\n/home/service/.rbenv/versions/1.9.3-p448/lib/ruby/gems/1.9.1/gems/aws-sdk-1.32.0/lib/aws/core/http/conne\nction_pool.rb:330:in request'\n/home/service/.rbenv/versions/1.9.3-p448/lib/ruby/gems/1.9.1/gems/aws-sdk-1.32.0/lib/aws/core/http/net_h\nttp_handler.rb:61:inblock in handle'\n/home/service/.rbenv/versions/1.9.3-p448/lib/ruby/gems/1.9.1/gems/aws-sdk-1.32.0/lib/aws/core/http/conne\nction_pool.rb:129:in session_for'\n/home/service/.rbenv/versions/1.9.3-p448/lib/ruby/gems/1.9.1/gems/aws-sdk-1.32.0/lib/aws/core/http/net_h\nttp_handler.rb:55:inhandle'\n/home/service/.rbenv/versions/1.9.3-p448/lib/ruby/gems/1.9.1/gems/aws-sdk-1.32.0/lib/aws/core/client.rb:\n246:in block in make_sync_request'\n/home/service/.rbenv/versions/1.9.3-p448/lib/ruby/gems/1.9.1/gems/aws-sdk-1.32.0/lib/aws/core/client.rb:\n275:inretry_server_errors'\n/home/service/.rbenv/versions/1.9.3-p448/lib/ruby/gems/1.9.1/gems/aws-sdk-1.32.0/lib/aws/core/client.rb:\n242:in make_sync_request'\n/home/service/.rbenv/versions/1.9.3-p448/lib/ruby/gems/1.9.1/gems/aws-sdk-1.32.0/lib/aws/core/client.rb:\n504:inblock (2 levels) in client_request'\n/home/service/.rbenv/versions/1.9.3-p448/lib/ruby/gems/1.9.1/gems/aws-sdk-1.32.0/lib/aws/core/client.rb:\n384:in log_client_request'\n/home/service/.rbenv/versions/1.9.3-p448/lib/ruby/gems/1.9.1/gems/aws-sdk-1.32.0/lib/aws/core/client.rb:\n470:inblock in client_request'\n/home/service/.rbenv/versions/1.9.3-p448/lib/ruby/gems/1.9.1/gems/aws-sdk-1.32.0/lib/aws/core/client.rb:\n366:in return_or_raise'\n/home/service/.rbenv/versions/1.9.3-p448/lib/ruby/gems/1.9.1/gems/aws-sdk-1.32.0/lib/aws/core/client.rb:\n469:inclient_request'\n(eval):3:in get_object'\n/home/service/.rbenv/versions/1.9.3-p448/lib/ruby/gems/1.9.1/gems/aws-sdk-1.32.0/lib/aws/s3/s3_object.rb\n:1366:inget_object'\n/home/service/.rbenv/versions/1.9.3-p448/lib/ruby/gems/1.9.1/gems/aws-sdk-1.32.0/lib/aws/s3/s3_object.rb\n:1085:in read'\n/home/service/.rbenv/versions/1.9.3-p448/lib/ruby/gems/1.9.1/gems/blobstore_client-1.2175.0/lib/blobstor\ne_client/s3_blobstore_client.rb:98:inget_file'\n/home/service/.rbenv/versions/1.9.3-p448/lib/ruby/gems/1.9.1/gems/blobstore_client-1.2175.0/lib/blobstor\ne_client/base.rb:50:in get'\n/home/service/.rbenv/versions/1.9.3-p448/lib/ruby/gems/1.9.1/gems/blobstore_client-1.2175.0/lib/blobstor\ne_client/sha1_verifiable_blobstore_client.rb:19:inget'\n/home/service/.rbenv/versions/1.9.3-p448/lib/ruby/gems/1.9.1/gems/blobstore_client-1.2175.0/lib/blobstor\ne_client/retryable_blobstore_client.rb:19:in block in get'\n/home/service/.rbenv/versions/1.9.3-p448/lib/ruby/gems/1.9.1/gems/bosh_common-1.2175.0/lib/common/retrya\nble.rb:21:inblock in retryer'\n/home/service/.rbenv/versions/1.9.3-p448/lib/ruby/gems/1.9.1/gems/bosh_common-1.2175.0/lib/common/retrya\nble.rb:19:in loop'\n/home/service/.rbenv/versions/1.9.3-p448/lib/ruby/gems/1.9.1/gems/bosh_common-1.2175.0/lib/common/retrya\nble.rb:19:inretryer'\n/home/service/.rbenv/versions/1.9.3-p448/lib/ruby/gems/1.9.1/gems/blobstore_client-1.2175.0/lib/blobstor\ne_client/retryable_blobstore_client.rb:18:in get'\n/home/service/.rbenv/versions/1.9.3-p448/lib/ruby/gems/1.9.1/gems/bosh_cli-1.2175.0/lib/cli/release_comp\niler.rb:151:infind_in_indices'\n/home/service/.rbenv/versions/1.9.3-p448/lib/ruby/gems/1.9.1/gems/bosh_cli-1.2175.0/lib/cli/release_comp\niler.rb:109:in find_package'\n/home/service/cf_nise_installer_services/nise_bosh/lib/nise_bosh/builder.rb:306:infind_package_archive\n'\n/home/service/cf_nise_installer_services/nise_bosh/lib/nise_bosh/bosh.rb:251:in get_source_package'\n/home/service/.rbenv/versions/1.9.3-p448/lib/ruby/gems/1.9.1/gems/bosh_agent-1.2175.0/lib/bosh_agent/mes\nsage/compile_package.rb:62:instart'\n/home/service/.rbenv/versions/1.9.3-p448/lib/ruby/gems/1.9.1/gems/bosh_agent-1.2175.0/lib/bosh_agent/mes\nsage/compile_package.rb:31:in process'\n/home/service/cf_nise_installer_services/nise_bosh/lib/nise_bosh/builder.rb:218:inrun_packaging'\n/home/service/cf_nise_installer_services/nise_bosh/lib/nise_bosh/builder.rb:208:in install_package'\n/home/service/cf_nise_installer_services/nise_bosh/lib/nise_bosh/builder.rb:194:inblock in install_pac\nkages'\n/home/service/cf_nise_installer_services/nise_bosh/lib/nise_bosh/builder.rb:192:in each'\n/home/service/cf_nise_installer_services/nise_bosh/lib/nise_bosh/builder.rb:192:ininstall_packages'\n/home/service/cf_nise_installer_services/nise_bosh/lib/nise_bosh/builder.rb:265:in install_job'\n/home/service/cf_nise_installer_services/nise_bosh/lib/nise_bosh/runner.rb:107:inrun_default_mode'\n/home/service/cf_nise_installer_services/nise_bosh/lib/nise_bosh/runner.rb:70:in run'\n./bin/nise-bosh:4:in'\n        from /home/service/.rbenv/versions/1.9.3-p448/lib/ruby/gems/1.9.1/gems/bosh_cli-1.2175.0/lib/cli\n/release_compiler.rb:121:in find_in_indices'\n        from /home/service/.rbenv/versions/1.9.3-p448/lib/ruby/gems/1.9.1/gems/bosh_cli-1.2175.0/lib/cli\n/release_compiler.rb:109:infind_package'\n        from /home/service/cf_nise_installer_services/nise_bosh/lib/nise_bosh/builder.rb:306:in find_pa\nckage_archive'\n        from /home/service/cf_nise_installer_services/nise_bosh/lib/nise_bosh/bosh.rb:251:inget_source\n_package'\n        from /home/service/.rbenv/versions/1.9.3-p448/lib/ruby/gems/1.9.1/gems/bosh_agent-1.2175.0/lib/b\nosh_agent/message/compile_package.rb:62:in start'\n        from /home/service/.rbenv/versions/1.9.3-p448/lib/ruby/gems/1.9.1/gems/bosh_agent-1.2175.0/lib/b\nosh_agent/message/compile_package.rb:31:inprocess'\n        from /home/service/cf_nise_installer_services/nise_bosh/lib/nise_bosh/builder.rb:218:in run_pac\nkaging'\n        from /home/service/cf_nise_installer_services/nise_bosh/lib/nise_bosh/builder.rb:208:ininstall\n_package'\n        from /home/service/cf_nise_installer_services/nise_bosh/lib/nise_bosh/builder.rb:194:in block i\nn install_packages'\n        from /home/service/cf_nise_installer_services/nise_bosh/lib/nise_bosh/builder.rb:192:ineach'\n        from /home/service/cf_nise_installer_services/nise_bosh/lib/nise_bosh/builder.rb:192:in install\n_packages'\n        from /home/service/cf_nise_installer_services/nise_bosh/lib/nise_bosh/builder.rb:265:ininstall\n_job'\n        from /home/service/cf_nise_installer_services/nise_bosh/lib/nise_bosh/runner.rb:107:in run_defa\nult_mode'\n        from /home/service/cf_nise_installer_services/nise_bosh/lib/nise_bosh/runner.rb:70:inrun'\n        from ./bin/nise-bosh:4:in `'\n```\n. @awood45 Thanks very much.\nNetwork speed we have  is very slowly. I want to know if it can cause that exception or not . \n. ",
    "klausbadelt": "Still experiencing this issue. We're reading big files (100 GB+) from S3. Here is our relevant code:\nruby\nFile.open(filename, \"wb\") do |file|\n  inobj.read do |chunk|\n    file.write(chunk)\n    md5 << chunk\n    progress.log chunk.size\n  end\nWe used this code for a bout a year before without the issue. Unfortunately hard to say when it started - could be when updating to ruby 2.0.\n. Thanks for re-opening. I added :verify_response_body_content_length temporarily to production; will report back. I had been checking file size vs. S3 object size.\nWe log progress by tracking each chunk.size. I'm not sure how many bytes to expect in each yield (can I actually set the chunk/block size?). If it helps you I can log the chunk size for each yield.\nWe get this error often every ~ 20-30 GB of downloaded data, which is every ~ 15 min. It seems to relate somehow to connectivity/transfer speed from S3 - just before this error S3-to-EC2 speed often goes down (for example, from 33 MB/s to 20 MB/s). We're on EC2 m1.medium and m1.xlarge instances within the same region of the S3 bucket.\nIt currently takes us 3-6 retries to get a 100+ GB file from S3, sometimes it fails all 8 retries we have set.\n. Adding :verify_response_body_content_length just makes the read exit \"successfully\" but leaves the read incomplete. Here is an excerpt of my logs. Note get_object() returns 200 but exits prematurely now. The IOError is raised by me if file sizes of S3 and local file mismatch.\ndelivery-worker.rb[30462]: Download progress 25092.9/121033.8 MB (20.7%) {:kbytes_s=>35615, :line=>217, :job_id=>\"5710\"}\ndelivery-worker.rb[30462]: Download progress 26406.2/121033.8 MB (21.8%) {:kbytes_s=>22409, :line=>217, :job_id=>\"5710\"}\ndelivery-worker.rb[30462]: [AWS S3 200 849.75637 0 retries] get_object(:bucket_name=>\"transcode.kinonation\",:key=>\"5711/kn3863_come_on_eileen_main.mov\")\ndelivery-worker.rb[30462]: Download file size mismatch (IOError) {:retry=>1, :job_id=>\"5710\"}\n. ``` ruby\nin_uri = URI in_url\nout_uri = URI out_url\nin_obj  = Kino.s3.buckets[in_uri.host].objects[in_uri.path.sub(/^\\//,'')]\nfilesize = in_obj.content_length.to_i\nCopy S3 -> S3\ndirect S3-to-S3 (incl. multipart) copy\nout_obj = in_obj.copy_to out_uri.path.sub(/^\\//,''),\n                         bucket_name: out_uri.host,\n                         content_length: filesize,\n                         reduced_redundancy: true,\n                         use_multipart_copy: true,\n                         acl: :bucket_owner_full_control\n```\nThe exception handler (which produces the backtrace above) looks like this:\nruby\nrescue Kino::NoRetryError, OAuth2::Error, StandardError => e\n  # manually move the message into the dead letter queue\n  Delivery.dead_letter_q.send_message(msg.body) if msg rescue nil\n  error = \"Delivery job #{job && job.id} failed (#{e.class}): #{e.message}\\n\\t#{e.backtrace.join(\"\\n\\t\")}\"\n  Kino.log.error error\n  Kino.notify error, \"Delivery error\"\n  job.update_job status: Kino::JOB_STATUS_FAILED, message: \"(#{e.class}) #{e.message}\" # rescue nil\nI did skip some non-related code (IMHO) but hope this shows the use. Pretty basic I think. Interesting to me that I'm catching an ArgumentError deep inside the SDK call stack, which to me seems outside my control.\nThis happens now practically every time on long running #copy_to (12+ hours) with file sizes around 50 GB and more. Smaller files (< 10 GB) seem fine.\n. ",
    "amw": "I've also started getting this error lately. Have you found out what's causing it?\n. Here's backtrace to the level of my app call.\nAWS::Core::Http::NetHttpHandler::TruncatedBodyError: content-length does not match\n/home/production/.rbenv/versions/2.1.2/lib/ruby/gems/2.1.0/gems/aws-sdk-1.51.0/lib/aws/core/http/net_http_handler.rb:83:in `block in handle'\n/home/production/.rbenv/versions/2.1.2/lib/ruby/gems/2.1.0/gems/aws-sdk-1.51.0/lib/aws/core/http/connection_pool.rb:129:in `session_for'\n/home/production/.rbenv/versions/2.1.2/lib/ruby/gems/2.1.0/gems/aws-sdk-1.51.0/lib/aws/core/http/net_http_handler.rb:55:in `handle'\n/home/production/.rbenv/versions/2.1.2/lib/ruby/gems/2.1.0/gems/aws-sdk-1.51.0/lib/aws/core/client.rb:253:in `block in make_sync_request'\n/home/production/.rbenv/versions/2.1.2/lib/ruby/gems/2.1.0/gems/aws-sdk-1.51.0/lib/aws/core/client.rb:282:in `retry_server_errors'\n/home/production/.rbenv/versions/2.1.2/lib/ruby/gems/2.1.0/gems/aws-sdk-1.51.0/lib/aws/core/client.rb:249:in `make_sync_request'\n/home/production/.rbenv/versions/2.1.2/lib/ruby/gems/2.1.0/gems/aws-sdk-1.51.0/lib/aws/core/client.rb:511:in `block (2 levels) in client_request'\n/home/production/.rbenv/versions/2.1.2/lib/ruby/gems/2.1.0/gems/aws-sdk-1.51.0/lib/aws/core/client.rb:391:in `log_client_request'\n/home/production/.rbenv/versions/2.1.2/lib/ruby/gems/2.1.0/gems/aws-sdk-1.51.0/lib/aws/core/client.rb:477:in `block in client_request'\n/home/production/.rbenv/versions/2.1.2/lib/ruby/gems/2.1.0/gems/aws-sdk-1.51.0/lib/aws/core/client.rb:373:in `return_or_raise'\n/home/production/.rbenv/versions/2.1.2/lib/ruby/gems/2.1.0/gems/aws-sdk-1.51.0/lib/aws/core/client.rb:476:in `client_request'\n(eval):3:in `get_object'\n/home/production/.rbenv/versions/2.1.2/lib/ruby/gems/2.1.0/gems/aws-sdk-1.51.0/lib/aws/s3/s3_object.rb:1366:in `get_object'\n/home/production/.rbenv/versions/2.1.2/lib/ruby/gems/2.1.0/gems/aws-sdk-1.51.0/lib/aws/s3/s3_object.rb:1085:in `read'\n/home/production/myapp/app/models/file_location.rb:117:in `block in cp'\nAlso possibly relevant is that I'm downloading files from S3 in threads.\n. aws-sdk-core-ruby has been moved back to this repository, but the issues have not been migrated. Maybe it's enough to reopen this issue? The other issue I've opened is aws/aws-sdk-core-ruby#194.\n. ",
    "bjeanes": "FWIW, I see this issue only when writing to file in block\nI'm on an old version still (1.40.3), so I know this is well into \"not supported\" territory, but I don't see this mentioned above explicitly.\nCompare writing to a StringIO vs a Tempfile (100% reproducible in each branch, for me):\n``` ruby\n\nf = Tempfile.new(['x', '.ris'])\n=> #\nobj.read { |chunk| f.write(chunk) }\nAWS::Core::Http::NetHttpHandler::TruncatedBodyError: content-length does not match\n    from /Users/bjeanes/.gem/ruby/2.2.6/gems/aws-sdk-1.40.3/lib/aws/core/http/net_http_handler.rb:83:in ensure in block (2 levels) in handle'\n    from /Users/bjeanes/.gem/ruby/2.2.6/gems/aws-sdk-1.40.3/lib/aws/core/http/net_http_handler.rb:83:inblock (2 levels) in handle'\n    [... snip ...]\ns = StringIO.new\n=> #\nobj.read { |chunk| s.write(chunk) }\n[AWS S3 200 12.151541 0 retries] get_object(:bucket_name=>\"[redacted]\",:key=>\"[redacted]\")\n=> {:meta=>{}, :restore_in_progress=>false, :content_type=>\"\", :etag=>\"\\\"2ba34e5ec00d3c718b104987f18c5069\\\"\", :accept_ranges=>\"bytes\", :last_modified=>2017-07-18 05:08:05 +0000, :content_length=>28541588, :data=>nil}\n```\n\nObviously, reading into a StringIO completely side-steps the advantage of chunked fetching, so this is merely illustrative.\nAgain, this is on an older version but I thought this may still be helpful if the issue is continuing in some fashion.. Some further info. The exception only happens if I write the chunk to the file. If I write something else, the exception does not occur:\n``` ruby\n\nobj.read { |chunk| f.write(\"0\") }\n[AWS S3 200 10.965613 0 retries] get_object(:bucket_name=>\"[redacted]\",:key=>\"[redacted]\")\n=> {:meta=>{}, :restore_in_progress=>false, :content_type=>\"\", :etag=>\"\\\"2ba34e5ec00d3c718b104987f18c5069\\\"\", :accept_ranges=>\"bytes\", :last_modified=>2017-07-18 05:08:05 +0000, :content_length=>28541588, :data=>nil}\nf.rewind\n=> 0\nf.read(100)\n=> \"0000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\"\n```\n\nI added a counter to see how many chunks it processes:\n``` ruby\n\nn = 0\n=> 0\nf.truncate(0)\n=> 0\nobj.read { |c| n += 1; f.write(c) }\nAWS::Core::Http::NetHttpHandler::TruncatedBodyError: content-length does not match\n    [... snip ...]\nf.rewind\n=> 0\nf.size\n=> 34212\nn\n=> 8\n```\n\nIn a completely un-scientific test of a \"handful\" of times, it always failed on the 8th chunk and always only wrote 34112 bytes, at least with my specific test file.\nI suspected the presence of a raise in ensure block of AWS::Core::Http::NetHttpHandler is masking the original exception. Indeed, after modifying my local copy of the gem to preserve log this exception, I discovered that the origin is an encoding exception (at least in my case): #<Encoding::UndefinedConversionError: \"\\xCB\" from ASCII-8BIT to UTF-8>\nA cursory glance at the implementation in the latest version (well, master) implies that this issue is no longer present due to no longer using performing the length check in the context of an ensure clause.\nSo, this probably is actually fixed. However, for people in older versions of the gem, perhaps ensure your file is open in binary mode first. This worked for me:\n``` ruby\n\nf.truncate(0); obj.read { |c| f.write(c) }\nAWS::Core::Http::NetHttpHandler::TruncatedBodyError: content-length does not match\nf.binmode\n=> #\nf.truncate(0); obj.read { |c| f.write(c) }\n[AWS S3 200 10.536865 0 retries] get_object(:bucket_name=>\"[redacted]\",:key=>\"[redacted]\")\n=> {:meta=>{}, :restore_in_progress=>false, :content_type=>\"\", :etag=>\"\\\"2ba34e5ec00d3c718b104987f18c5069\\\"\", :accept_ranges=>\"bytes\", :last_modified=>2017-07-18 05:08:05 +0000, :content_length=>28541588, :data=>nil}\nf.size\n=> 28541588\n```\n\nAlternatively, set the encoding to the chunk encoding:\n``` ruby\n\nf = Tempfile.new('')\n=> #\nirb(main):092:0> obj.read { |c| f.set_encoding(c.encoding); f.write(c) }\n[AWS S3 200 10.536865 0 retries] get_object(:bucket_name=>\"[redacted]\",:key=>\"[redacted]\")\n=> {:meta=>{}, :restore_in_progress=>false, :content_type=>\"\", :etag=>\"\\\"2ba34e5ec00d3c718b104987f18c5069\\\"\", :accept_ranges=>\"bytes\", :last_modified=>2017-07-18 05:08:05 +0000, :content_length=>28541588, :data=>nil}\n```. \n",
    "lukaszkorecki": "@trevorrowe my pleasure!\n. ",
    "Mistobaan": "version: 1.42.0\nfor now I solved by installing aws-sdk-core 2.0.0.rc8 and using it\n. ",
    "maitreya1975": "```\nrequire 'aws'\nrequire 'pp'\nrequire 'logger'\nAWS.config({\n    :log_level => :debug,\n    :logger => Logger.new($stdout)\n})\ndef aws_ec2_region(region)\n        AWS::EC2.new(region: region).client\nend\npp \"AWS Ruby SDK Version = #{AWS::VERSION}\"\nsource_region = 'us-east-1'\nsource_snapshot_id = 'snap-d7b44515'\ndest_region = 'us-west-2'\nnew_desc = 'New Description'\npp \"Creating a copy of snapshot #{source_snapshot_id} in region #{dest_region} from region #{source_region}\"\ndest_snapshot_id = aws_ec2_region(dest_region).copy_snapshot(source_region: source_region,\n                                                             source_snapshot_id: source_snapshot_id,\n                                                             description: new_desc\n                                                             ).data[:snapshot_id]\npp \"RubySDK: Created snapshot #{dest_snapshot_id}\"\ncommand = \"aws ec2 copy-snapshot --region '#{dest_region}' --source-region '#{source_region}' --source-snapshot-id '#{source_snapshot_id}' --description 'CLI #{new_desc}' --output text\"\ndest_snapshot_id=#{command}.chomp\npp \"CLI Command #{command}\"\npp \"CLI: Created snapshot #{dest_snapshot_id}\"\n```\n. ",
    "d3chapma": "Perhaps I'm taking a round about approach to signing my url.\n``` ruby\nobjectPath = CGI::escape(params[:s3_object_name])\nmimeType = params[:s3_object_type]\nexpires = Time.now.to_i + 100\namzHeaders = \"x-amz-acl:public-read\\nx-amz-server-side-encryption:AES256\"\nstringToSign = \"PUT\\n\\n#{mimeType}\\n#{expires}\\n#{amzHeaders}\\n/template-icons/#{objectPath}\";\nsig = CGI::escape(Base64.strict_encode64(OpenSSL::HMAC.digest('sha1', ENV['S3_SECRET_KEY'], stringToSign)))\nrender json: {\n   signed_request: CGI::escape(\"https://s3.amazonaws.com/template-icons/#{objectPath}?AWSAccessKeyId=#{ENV['S3_ACCESS_KEY']}&Expires=#{expires}&Signature=#{sig}\"),\n   url: \"https://s3.amazonaws.com/template-icons/#{objectPath}\"\n}\n```\nI'm using this method to create a signed request then on the client I am doing:\n``` javascript\nxhr = new XMLHttpRequest()\nif (xhr.withCredentials?) {\n  xhr.open method, url, true\n} else if (typeof XDomainRequest != \"undefined\") {\n  xhr = new XDomainRequest()\n  xhr.open method, url\n} else {\n  xhr = null\n}\nif (xhr) {\n  xhr.setRequestHeader 'content-type', file.type\n  xhr.setRequestHeader 'x-amz-acl', 'public-read'\n  xhr.setRequestHeader \"x-amz-server-side-encryption\", \"AES256\"\nxhr.send file\n}\n```\nDo you see any reason that this approach would raise an issue?\n. To be honest, I was following an example so I wasn't even aware that you could use the sdk for that. \n. ",
    "jufemaiz": "No worries! :+1: \n. :(. ",
    "trungpham": "already supported http://docs.aws.amazon.com/AWSRubySDK/latest/AWS/IAM/SigningCertificateCollection.html\n. http://docs.aws.amazon.com/AWSRubySDK/latest/AWS/IAM/ServerCertificateCollection.html\n. undefined method `handle_async' for #AWS::Core::Http::CurbHandler:0x007fea0b980678\n. Net::HTTP handler doesn't implement handle_async either.\nBut anyway, I was able to use the HTTP Handler wrapper to work with async.\nThis is code I am using. Please let me know if I'm doing something wrong: https://gist.github.com/trungpham/cb054ec2f5688dbf03ef\n. should check response_code == 0. if so, then set response.network_error = true\nhttps://github.com/aws/aws-sdk-ruby/blob/master/lib/aws/core/http/curb_handler.rb#L135\n. https://github.com/aws/aws-sdk-ruby/pull/619\n. merged https://github.com/aws/aws-sdk-ruby/pull/619\n. ",
    "masaomoc": "Oh, this seems to be my misunderstanding. Never mind about this issue.\n. ",
    "alexfarrill": "Oh cool, didn't realize... yeah, would you mind pulling and updating the docs?\n. Yes, the error message does include the Canonical String for this request and String-to-Sign\n. Thanks, I will patch it and get back to you\n. Hi Trevor, I emailed you earlier this week with the log output.  Let me know if there's anything else I can do to help debug.\n. Thanks Trevor, let me know if there's anything we can do to help.\n. Awesome, thank you\n. Thanks Alex, we're enabling the http_wiretrace and I'll email you the log output next time the error occurs.  I also added a retry, thanks for that info\n. Sure, here's the output from that:\n[Seahorse::Client::NetHttp::Handler, Seahorse::Client::Plugins::ContentLength::Handler, Aws::Json::ErrorHandler, Aws::Plugins::RequestSigner::Handler, Aws::Plugins::RetryErrors::Handler, Aws::Rest::Handler, Aws::Plugins::UserAgent::Handler, Seahorse::Client::Plugins::Endpoint::Handler, Aws::Plugins::ParamValidator::Handler, Seahorse::Client::Plugins::RaiseResponseErrors::Handler, Aws::Plugins::ParamConverter::Handler, Aws::Plugins::ResponsePaging::Handler, Seahorse::Client::Plugins::ResponseTarget::Handler]\n. Yep, sorry, missed that:\nirb(main):011:0> client.handlers.entries.each { |h| puts \"#{h.handler_class}: #{h.step} #{h.priority}\" }; nil\nSeahorse::Client::Plugins::Endpoint::Handler: build 90\nSeahorse::Client::NetHttp::Handler: send 50\nSeahorse::Client::Plugins::RaiseResponseErrors::Handler: validate 95\nSeahorse::Client::Plugins::ResponseTarget::Handler: initialize 90\nSeahorse::Client::Plugins::ContentLength::Handler: sign 0\nAws::Plugins::ParamConverter::Handler: initialize 50\nAws::Plugins::ParamValidator::Handler: validate 50\nAws::Plugins::UserAgent::Handler: build 50\nAws::Plugins::RetryErrors::Handler: sign 99\nAws::Plugins::RequestSigner::Handler: sign 50\nAws::Plugins::ResponsePaging::Handler: initialize 90\nAws::Query::Handler: build 50\nAws::Xml::ErrorHandler: sign 50\n=> nil\nirb(main):012:0> client.handlers.to_a\n=> [Seahorse::Client::NetHttp::Handler, Seahorse::Client::Plugins::ContentLength::Handler, Aws::Xml::ErrorHandler, Aws::Plugins::RequestSigner::Handler, Aws::Plugins::RetryErrors::Handler, Aws::Query::Handler, Aws::Plugins::UserAgent::Handler, Seahorse::Client::Plugins::Endpoint::Handler, Aws::Plugins::ParamValidator::Handler, Seahorse::Client::Plugins::RaiseResponseErrors::Handler, Aws::Plugins::ParamConverter::Handler, Seahorse::Client::Plugins::ResponseTarget::Handler]\n. Thanks Alex, I think this is a good workaround and the number of errors has actually dropped off since I first reported this.  I'm only seeing this error every few days now whereas before we were seeing it multiple times per day.  I'm not sure if that's because of a change in AWS or a change in our traffic patterns, but that in combination with the retries should be a good enough solution for now.  Thanks\n. +1 for this feature. thanks!. ",
    "matuag": "Line 145 of lib/aws/s3/client.rb is causing the problem. If name is a symbol (as in my case), start_with? is an undefined method.\n```\nProblem\nkey =  name.start_with?(\"x-amz-\") ?  name : \"x-amz-meta-#{name}\"\nPossible solution\nkey =  name.to_s.start_with?(\"x-amz-\") ?  name.to_s : \"x-amz-meta-#{name}\"\n```\n. Issue resolved through #578.\n. ",
    "cristim": "Ok, I just realized I used the wrong github repo...\nI will read about the credentials stuff and if it makes sense I will re-submit it to the AWS CLI repo.\n. ",
    "rantonmattei": "I doubt it is a Flow issue. Flow eventually calls the API with [ClassName].[MethodName] as name param (SimpleWorkflow service with RegisterWorkflowType action).\nIt is inherent to the SWF service, I guess. The main issue is that we cannot register WorkflowType with a colon in it (like \"MyWorkflow::HelloWorld.sayHello\"). Therefore, using Ruby namespaces is excluded.\nHowever, I was able to register a Workflow using backslashes in the name from the console. So that means you can register workflow classes within namespaces using Php. Unfortunately, there is no Php implementation of Flow...\nI guess a workaround could be implemented at the SDK level and replace those colons with another separator before registration. And then replace them back when retrieving the WorkflowTypes after polling for tasks.\nBTW, the same thing is happening when registering Activities.\n. ",
    "bosmanx": "Sorry for this, I will test it better next time.\n. I should check for empty\n  response[:response_metadata]\nbefore use\n  response[:response_metadata][:request_id]\nAny suggestions how to do in in \"Ruby\" style greatly appreciated\nMy first idea:\n  response_metadata = response[:response_metadata] || {}\nand then use\n  response_metadata[:request_id]\n. ",
    "smojtabai": "We added a 1 second sleep before the refresh and haven't seen it for a while. It seems the call for credentials can fail in some instances which is really frustrating.\n. So to be clear the sleep is in a retry block of our code:\nsleep 1\n@s3_client.config.credential_provider.refresh\nupload(bucket, dest_file_name, dest_path, file, retry_count + 1)\nI think the only way to address this issue is to specifically specify, hey we want to use the EC2CredentialProvider therefore it should retry. Where as now it is guessing which provider to use and only tries once, if it fails it assumes it is not the correct credential provider.\n. ",
    "scalp42": ":sake: \n. ",
    "pchaganti": "That is exactly what we are doing. Will dig into this more.\nthanks for the quick response!\n. You were right. We were not enumerating one of the reservation sets. Did not see this issue with calling ec2.instances as it does the enumeration itself.\nthanks!!\n. Thanks! This was for something that we did not want to currently change, but you are right. It is better we switch this over to aws-sdk-core.\nthanks again for your help!\n. Tried that. That will only work if there is an explicit association between the subnet and a route table. but if the subnet is using the default route table, there is no explicit association between the two and the above will not return any route table.\n. Ruby\n2.1.1 :009 > ec2.describe_route_tables(filters:[{name:'association.subnet-id', values:['subnet-fa0a80d1']}])\n => #<struct Aws::EC2::Types::DescribeRouteTablesResult route_tables=[]>\n2.1.1 :010 >\n\n. After experimenting with this, when the API returns 'available' for the attachment state, the EC2 console displays status of the gateway as attached to VPC. \nWhen the API returns a blank string for the attachment state, the EC2 console displays the status as detached.\n. The status still shows as available. This might be a doc discrepancy.\n. Thanks!\n. :+1: \n. Got it. Thx for the clarification!\n. You do not need to encode to base64. We are using \nRuby\nIO.read(path_to_zip)\n. Using sdk version 2.1.33. Could you please clarify what info you need?\nRuby\nrds.describe_db_snapshots(db_instance_identifier: dbinstance)\n. When a db copy snapshot API call is made, does the snapshot have the create time assigned immediately, or is that time assigned when the creation is complete. In our case we are making the API call to create snapshot, and then listing all the snaps. Maybe that is why the snap create time is nil? Cant really tell from the docs when the time is assigned.\nthanks!\n. Oops. Closing this.\n. The client is being created by passing in the keys to the constructor. Interestingly enough, I can update everything else for a function configuration, including the IAM role. The issue seems to come up only when I try to update the vpc configuration.\nthanks!\n. Also I can set the vpc configuration to the default Lambda managed vpc without any issue. To do that, I just pass in an empty list of subnet and security group ids. That works just fine. When I try to pass in a list of vpc sec group ids, I always seem to get back the error.\n. Does that require a support contract with AWS?\n. I understand. I can certainly provide any info they want over email perhaps if that works.\nthanks!\n. Removed security group id and credential info from this.\n```\nopening connection to lambda.us-east-1.amazonaws.com:443...\nopened\nstarting SSL for lambda.us-east-1.amazonaws.com:443...\nSSL established\n<- \"PUT /2015-03-31/functions/newnode/configuration HTTP/1.1\\r\\nContent-Type: \\r\\nAccept-Encoding: \\r\\nUser-Agent: aws-sdk-ruby2/2.3.7 ruby/2.3.0 x86_64-darwin15\\r\\nX-Amz-Date: 20160523T174650Z\\r\\nHost: lambda.us-east-1.amazonaws.com\\r\\nX-Amz-Content-Sha256: 40ca0a900d03f46da27d12fe528a36a6a6987430b783480977357d39c6dc11\\r\\nAuthorization: AWS4-HMAC-SHA256 Credential=CREDENTIAL/20160523/us-east-1/lambda/aws4_request, SignedHeaders=host;x-amz-content-sha256;x-amz-date, Signature=1c0ec9c00e92d58d5e4f6f00e83e184ccc2d79df88e0ad358d4cf00fb8\\r\\nContent-Length: 82\\r\\nAccept: /\\r\\n\\r\\n\"\n-> \"HTTP/1.1 403 Forbidden\\r\\n\"\n-> \"Content-Type: application/json\\r\\n\"\n-> \"Date: Mon, 23 May 2016 17:46:50 GMT\\r\\n\"\n-> \"x-amzn-ErrorType: AccessDeniedException:http://internal.amazon.com/coral/com.amazon.coral.service/\\r\\n\"\n-> \"x-amzn-RequestId: 5413c850-210e-11e6-8a7a-651d6e763795\\r\\n\"\n-> \"Content-Length: 258\\r\\n\"\n-> \"Connection: keep-alive\\r\\n\"\n-> \"\\r\\n\"\nreading 258 bytes...\n-> \"{\\\"Message\\\":\\\"Your access has been denied by EC2, please make sure your request credentials have permission to DescribeSecurityGroups for sg-xxxxxxxx. EC2 Error Code: UnauthorizedOperation. EC2 Error Message: You are not authorized to perform this operation.\\\"}\"\nread 258 bytes\nConn keep-alive\n```\n. When describing sec groups, group_names option will not work, as these are vpc sec groups. I will need to use the group_ids instead. Made the corresponding change in my call. The actual sg and subnet ids have been changed in the log I am attaching.\nthanks!\nopening connection to ec2.us-east-1.amazonaws.com:443...\nopened\nstarting SSL for ec2.us-east-1.amazonaws.com:443...\nSSL established\n<- \"POST / HTTP/1.1\\r\\nContent-Type: application/x-www-form-urlencoded; charset=utf-8\\r\\nAccept-Encoding: \\r\\nUser-Agent: aws-sdk-ruby2/2.3.7 ruby/2.3.0 x86_64-darwin15\\r\\nX-Amz-Date: 20160525T183519Z\\r\\nHost: ec2.us-east-1.amazonaws.com\\r\\nX-Amz-Content-Sha256: e96826e2714f9df161d6dcd0b68a5c49180b14bae6ddf01a8c53814e803b\\r\\nAuthorization: AWS4-HMAC-SHA256 Credential=CRED/20160525/us-east-1/ec2/aws4_request, SignedHeaders=content-type;host;x-amz-content-sha256;x-amz-date, Signature=c67a547267707792b0cef70b76a3e17056164a887632c82fd2821b35d8dd\\r\\nContent-Length: 70\\r\\nAccept: */*\\r\\n\\r\\n\"\n-> \"HTTP/1.1 200 OK\\r\\n\"\n-> \"Content-Type: text/xml;charset=UTF-8\\r\\n\"\n-> \"Transfer-Encoding: chunked\\r\\n\"\n-> \"Vary: Accept-Encoding\\r\\n\"\n-> \"Date: Wed, 25 May 2016 18:35:19 GMT\\r\\n\"\n-> \"Server: AmazonEC2\\r\\n\"\n-> \"\\r\\n\"\n-> \"588\\r\\n\"\nreading 1416 bytes...\n-> \"<?xml version=\\\"1.0\\\" encoding=\\\"UTF-8\\\"?>\\n<DescribeSecurityGroupsResponse xmlns=\\\"http://ec2.amazonaws.com/doc/2015-10-01/\\\">\\n    <requestId>8ab187ba-48f4-42bd-a46b-d1eedf6dff05</requestId>\\n    <securityGroupInfo>\\n        <item>\\n            <ownerId>6587193521</ownerId>\\n            <groupId>sg-5613863 </groupId>\\n            <groupName>default</groupName>\\n            <groupDescription>default VPC security group</groupDescription>\\n            <vpcId>vpc-16516512</vpcId>\\n            <ipPermissions>\\n                <item>\\n                    <ipProtocol>-1</ipProtocol>\\n                    <groups>\\n                        <item>\\n                            <userId>6587193521</userId>\\n                            <groupId>sg-5613863</groupId>\\n                        </item>\\n                    </groups>\\n                    <ipRanges/>\\n                    <prefixListIds/>\\n                </item>\\n            </ipPermissions>\\n            <ipPermissionsEgress>\\n                <item>\\n                    <ipProtocol>-1</ipProtocol>\\n                    <groups/>\\n                    <ipRanges>\\n                        <item>\\n                            <cidrIp>0.0.0.0/0</cidrIp>\\n                        </item>\\n                    </ipRanges>\\n                    <prefixListIds/>\\n                </item>\\n            </ipPermissionsEgress>\\n        </item>\\n    </securityGroupInfo>\\n</DescribeSecurityGroupsResponse>\"\nread 1416 bytes\nreading 2 bytes...\n-> \"\\r\\n\"\nread 2 bytes\n-> \"0\\r\\n\"\n-> \"\\r\\n\"\nConn keep-alive\nopening connection to ec2.us-east-1.amazonaws.com:443...\nopened\nstarting SSL for ec2.us-east-1.amazonaws.com:443...\nSSL established\n<- \"POST / HTTP/1.1\\r\\nContent-Type: application/x-www-form-urlencoded; charset=utf-8\\r\\nAccept-Encoding: \\r\\nUser-Agent: aws-sdk-ruby2/2.3.7 ruby/2.3.0 x86_64-darwin15\\r\\nX-Amz-Date: 20160525T183520Z\\r\\nHost: ec2.us-east-1.amazonaws.com\\r\\nX-Amz-Content-Sha256: ddd69d8825e07164f8394bfc4fa6053298f8642a82f4d0860d59d0bda10f2\\r\\nAuthorization: AWS4-HMAC-SHA256 Credential=CRED/20160525/us-east-1/ec2/aws4_request, SignedHeaders=content-type;host;x-amz-content-sha256;x-amz-date, Signature=d5f12626e9041ed8484b047a27dd2c032eaa34112cad4e8a5d6b163eb01144\\r\\nContent-Length: 68\\r\\nAccept: */*\\r\\n\\r\\n\"\n-> \"HTTP/1.1 200 OK\\r\\n\"\n-> \"Content-Type: text/xml;charset=UTF-8\\r\\n\"\n-> \"Transfer-Encoding: chunked\\r\\n\"\n-> \"Vary: Accept-Encoding\\r\\n\"\n-> \"Date: Wed, 25 May 2016 18:35:19 GMT\\r\\n\"\n-> \"Server: AmazonEC2\\r\\n\"\n-> \"\\r\\n\"\n-> \"355\\r\\n\"\nreading 853 bytes...\n-> \"<?xml version=\\\"1.0\\\" encoding=\\\"UTF-8\\\"?>\\n<DescribeSubnetsResponse xmlns=\\\"http://ec2.amazonaws.com/doc/2015-10-01/\\\">\\n    <requestId>e090247f-d5c8-440d-be31-a2020e3f306d</requestId>\\n    <subnetSet>\\n        <item>\\n            <subnetId>subnet-131313 </subnetId>\\n            <state>available</state>\\n            <vpcId>vpc-16516165</vpcId>\\n            <cidrBlock>10.0.0.0/24</cidrBlock>\\n            <availableIpAddressCount>248</availableIpAddressCount>\\n            <availabilityZone>us-east-1e</availabilityZone>\\n            <defaultForAz>false</defaultForAz>\\n            <mapPublicIpOnLaunch>false</mapPublicIpOnLaunch>\\n            <tagSet>\\n                <item>\\n                    <key>Name</key>\\n                    <value>socrates-subnet</value>\\n                </item>\\n            </tagSet>\\n        </item>\\n    </subnetSet>\\n</DescribeSubnetsResponse>\"\nread 853 bytes\nreading 2 bytes...\n-> \"\\r\\n\"\nread 2 bytes\n-> \"0\\r\\n\"\n-> \"\\r\\n\"\nConn keep-alive\n. UPDATE  Just for testing, updated the role assigned to the function whose vpc configuration is being updated to allow ALL ec2 ops. Still being returned the same access error. Attaching a screenshot of the policy simulator.\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n  {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"logs:CreateLogGroup\",\n        \"logs:CreateLogStream\",\n        \"logs:PutLogEvents\"\n      ],\n      \"Resource\": \"arn:aws:logs:*:*:*\"\n    },\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"ec2:*\"\n      ],\n      \"Resource\": \"*\"\n    }\n  ]\n}\n\n. I tried setting the vpc config from the AWS console and I was able to save the changes. I just cant seem to be able to do it via the API, even though I am using the same access keys for the user setting the same config via the API. I will go through and double check everything, and post back.\nthanks for your help!\n. More debugging. Used the aws-cli as a sanity check. Getting the same error there. So this is definitely not an issue with the aws-sdk-ruby, but an issue with the credentials or how we are calling the API. Will get this tracked down. Closing this issue for now.\nthanks again for your help with this!\n. Using a similar policy, except that the IAM policy of the user allows * from the sourceIP.\n. Removed the actual list of subs brought back as I do not want to post those details in a public post. Here is the rest. The length still comes back as 11.\nthanks!\n``` Ruby\n2.3.0 :006 > sns = Aws::SNS::Client.new(access_key_id: ENV['AWS_ACCESS_KEY_ID'], secret_access_key: ENV['AWS_SECRET_ACCESS_KEY'], region: 'us-east-1', http_wire_trace: true)\n => # \n2.3.0 :007 > resp = sns.list_subscriptions\nopening connection to sns.us-east-1.amazonaws.com:443...\nopened\nstarting SSL for sns.us-east-1.amazonaws.com:443...\nSSL established\n<- \"POST / HTTP/1.1\\r\\nContent-Type: application/x-www-form-urlencoded; charset=utf-8\\r\\nAccept-Encoding: \\r\\nUser-Agent: aws-sdk-ruby2/2.3.7 ruby/2.3.0 x86_64-darwin15\\r\\nX-Amz-Date: 20160530T214145Z\\r\\nHost: sns.us-east-1.amazonaws.com\\r\\nX-Amz-Content-Sha256: 0693efeb802ed7b95fd14be8698fa947dcdd642a4f1c46bb0830aea20d\\r\\nAuthorization: AWS4-HMAC-SHA256 Credential=CRED/20160530/us-east-1/sns/aws4_request, SignedHeaders=content-type;host;x-amz-content-sha256;x-amz-date, Signature=428ae3db3092d5106ae7656eee1a86c9f7532a624ad81fa119980a3cd6e7e\\r\\nContent-Length: 43\\r\\nAccept: /\\r\\n\\r\\n\"\n-> \"HTTP/1.1 200 OK\\r\\n\"\n-> \"x-amzn-RequestId: 69d4d65d-c6ff-5732-91b2-999b73268807\\r\\n\"\n-> \"Content-Type: text/xml\\r\\n\"\n-> \"Content-Length: 5415\\r\\n\"\n-> \"Date: Mon, 30 May 2016 21:41:45 GMT\\r\\n\"\n-> \"\\r\\n\"\nreading 5415 bytes...\n-> \"\"\nread 5415 bytes\nConn keep-alive\n => \n2.3.0 :008 > resp.last_page?\n => false \n2.3.0 :009 > resp.subscriptions.length\n => 11 \n```\n. This might be a pagination thing?\n. \ud83d\udc4d \n. \ud83d\udc4d . Sure. This is how we used to add a perm to allow trail notifications to new or existing topic previously. The AWS account ids are from the trail docs.\n```Ruby\nsns.add_permission(\ntopic_arn: topic_arn, \nlabel: \"CloudTrail Policy to publish Notifications of API Activity\", \naws_account_id: [\"903692715234\", \"859597730677\", \"814480443879\",                                                                                                                                                           \"216624486486\", \"086441151436\", \"388731089494\", \"284668455005\", \n\"113285607260\", \"035351147821\"],\naction_name: [\"Publish\"]\n)\n```\nThis still works, but CloudTrail will not deliver notifications with this perm. The new way apparently is to provide the service cloudtrail.amazonaws.com as the principal. The trail docs have also changed to show this - \n```\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [{ \n        \"Sid\": \"AWSCloudTrailSNSPolicy20131101\",\n        \"Effect\": \"Allow\", \n        \"Principal\": {\n            \"Service\": \"cloudtrail.amazonaws.com\"\n        },\n        \"Action\": \"SNS:Publish\", \n        \"Resource\": \"arn:aws:sns:Region:SNSTopicOwnerAccountId:SNSTopicName\"\n    }]\n}\n```\nThere does not seem to be a way to do this from the API. \n. Any further info on this?\nthanks\n. This q may have been answered in https://github.com/aws/aws-sdk-ruby/issues/1271\nWill try that out.\n. Not sure of the exact date when it changed. It is definitely in the last week or so that we have been seeing this exception. The errors raised previously have never had the Exception suffix. We have had this code running for the last several years and the error handling has not changed. \n. Some more info. This error happens when you try to copy an encrypted RDS cluster snapshot to another region. We are also using an IAM role and not access keys.\nThe aws-sdk for dotnet apparently had a similar issue which had to do with one of the parameters - https://github.com/aws/aws-sdk-net/issues/635\n. I just tried this and am able to do the copy from the AWS console using the exact same parameters, but not with the sdk.\n. Some more debugging. I checked the pre-signed URL that the AWS console is posting through the Chrome dev tools and compared the params in it to the URL generated by the RDS presigner plugin in this SDK.\nThe main difference I see is that the console is sending these three extra params. I am not sure what is needed for the signed URl but these are the additional three: \n&SignatureMethod=HmacSHA256\n&SignatureVersion=4\n&X-Amz-Content-SHA256=e3b0c44298fc1c149afbf4c89962427ae41e4649b934c5991b7852b855\nAlso the X-Amz-Security-Token generated by the console is a much larger string. It is almost twice the length of the token generated by the SDK. I dont know if that is significant.\n. I will check this. The role I am using has the admin policy with everything in it.\nThe db cluster was created encrypted. So the snapshots(manual/auto) are automatically encrypted. I cant tell if you are using an encrypted db cluster.\n. hmm. i will try to reproduce this in a completely different AWS account.\nthanks for your help !\n. Tried this in a completely different account with a different role, and it worked.\nNot sure what the issue is with the other role I am using but somehow somewhere it is missing something.\nThanks again for your help!\n. Got it. Thanks for the update!. This is NOT an issue with the SDK. Closing this ticket!\n. \ud83d\udc4d . \ud83d\udc4d . ",
    "zerowolfgang": "Roger that. It's more likely to be a derp on my part.\n. ",
    "ajvondrak": "Ah, you're right.  My project's configured to use AWS::Http::EMHttpHandler.new from the em_aws gem.  I'll have to check out their docs & see if it's a bug with them or if I'm using the gem wrong on my end.\nThanks so much for the help, and sorry for the confusion!\n. Got it, makes sense to be getting that warning then.\n. I'll close this issue out and take up any issues with em_aws.  Thanks again!\n. Excellent! Thank you for the speedy response.\n. ",
    "duane": "Is this no longer an option?\n. ",
    "Doug-AWS": "We've documented stubbing in the developer guide at http://docs.aws.amazon.com/sdk-for-ruby/v2/developer-guide/stubbing.html.\n. We've implemented a wait_until method on the S3 client. The Dev Guide has a section that describes the mechanism. See the \"Waiters\" section of the \"Programming with the AWS SDK for Ruby\" topic, http://docs.aws.amazon.com/sdk-for-ruby/latest/DeveloperGuide/aws-ruby-sdk-programming.html.\n. I strongly recommend you read the first chapter or two of the Developer Guide, http://docs.aws.amazon.com/sdk-for-ruby/latest/DeveloperGuide/, as it covers a number of areas where you could stumble. \n. I talked to the doc writer and he is going to update the docs to add more info that should help.\n. I don't see any issue in your code. You might try using the console to create a new access key and secret access key (services, IAM, select the user, user action->manage access keys, create new access keys). I recommend saving them in your HOME/.aws/credentials file so you don't have the values in any source file, even by accident, as some day you might forget and expose the values. See http://docs.aws.amazon.com/sdk-for-ruby/latest/DeveloperGuide/aws-ruby-sdk-getting-started.html for more info.\n. There's a known issue with us-east allowing bucket names that pose problems. See http://docs.aws.amazon.com/AmazonS3/latest/dev/BucketRestrictions.html. TL:DR => \"bucket names can contain any combination of uppercase letters, lowercase letters, numbers, periods (.), hyphens (-), and underscores (_)\". Note the absence of forward slash. \n. I just added an example of using tags to filter ECS instances with a specific tag value. See \"Getting Information about All Instances with a Specific Tag Value\" in http://docs.aws.amazon.com/sdk-for-ruby/latest/DeveloperGuide/aws-ruby-sdk-recipes.html#aws-ruby-sdk-ec2-recipes.\n. Try the code we include with the Aws.partition method, http://docs.aws.amazon.com/sdkforruby/api/Aws.html#partition-class_method.\nruby\nAws.partition('aws').regions.map(&:name)\nIt just returned:\nus-east-1\nus-west-1\nus-west-2\nap-northeast-1\nap-northeast-2\nap-south-1\nap-southeast-1\nap-southeast-2\nsa-east-1\neu-west-1\neu-central-1\n. You've discovered that the type RouteTable (http://docs.aws.amazon.com/sdkforruby/api/Aws/EC2/Types/RouteTable.html) is not the same as the class RouteTable (http://docs.aws.amazon.com/sdkforruby/api/Aws/EC2/RouteTable.html). You are accessing the type in your first example, the class in your second. If you want to just see the vales of all of your RouteTable objects in us-west-2, you can do something like the following:\n```\nrequire 'aws-sdk'\nec2 = Aws::EC2::Resource.new(region: 'us-west-2')\nec2.route_tables.each do |table|\n  puts \"ID:          #{table.id}\"\ntable.routes.each do |r|\n    puts \"  Gateway:     #{r.gateway_id}\"\n    puts \"  State:       #{r.state}\"\n  end\ntable.associations.each do |a|\n    puts \"  Subnet ID:    #{a.subnet_id}\"\n  end\nputs \"VPC:         #{table.vpc_id}\"\nend\n```\n. Does the route show up in the console?\n. You can't. The owner property is read-only. If you want some other means of identifying the bucket, such as a friendly name that you can display, you could use tagging.put on the bucket object and add a tag.\n. We recommend you always try the client head_bucket method to verify whether a bucket exists and you have access. See http://docs.aws.amazon.com/sdkforruby/api/Aws/S3/Client.html#head_bucket-instance_method. \nIf you call it with a bucket arg nil, it throws an ArgumentError exception, which is what you are requesting.\n. Except it would likely break a ton of other developer's code.\n. Your code isn't testing whether the bucket exists. It assumes the bucket exists. You need something like:\n```\ndef file_exists(bucket_name, file_name)\n  s3 = Aws::S3::Client.new(region: region)\n  res = Aws::S3::Resource.new(client: s3)\nbegin\n    # Throws an exception if we can't access bucket\n    s3.head_bucket({bucket: bucket_name, use_accelerate_endpoint: false})\nif res.bucket(bucket_name).object(item_name).exists?\n  true\nelse\n  false\nend\n\nrescue Aws::S3::Errors::NotFound\n    false\n  end\nend\n```\n. Try just describe_images. The response object should contain an array, image_details, from which you can get the push date from each entry's image_pushed_at property. . Thanks Alex, I'll add it to my list.. We already have instructions on creating credentials. Under Tutorial Prerequisites:\nSet up an AWS access key to use the AWS SDKs. For more information, see Setting Up DynamoDB (Web Service).\n. Sweet, I'll add this to https://docs.aws.amazon.com/sdk-for-ruby/v3/developer-guide/setup-config.html#aws-ruby-sdk-setting-credentials by EOD.. Done.. ",
    "rberger": "Yeah, I finally figured it out. I had to download the git repo and build the docs myself.\nI posted what I found as the answer to my original question of how to actually USE the aws-sdk to mange put_lifecycle_hook at https://forums.aws.amazon.com/thread.jspa?threadID=158781\n. ",
    "srri": "Looks good to me\n. ",
    "hakamadare": "@awood45 that sounds like a perfectly reasonable solution; i am equally unaware of a reliable way to determine the home directory :)\nin an ideal world i'd like to be able to do the following:\nruby\niam = AWS::IAM.new(\n  :shared_credential_file => '<SOME PATH>'\n)\nand be confident that\n- the default credential provider chain would go through its normal search algorithm, but would do the right thing when it got to the shared credential file provider (i.e. if i've passed :shared_credential_file, use that value, otherwise look in the homedir on Ruby 1.9+ or throw an ArgumentError on 1.8.7)\n- if i pass the path to a file that is unreadable for whatever reason, it'll throw some appropriate error\nthanks for digging into this!  btw i've opened support case 237696151 about this issue.\n. Maybe I'm not using the new code correctly, but this change didn't work out quite as I had hoped.  Here are three snippets, all run on a Debian Squeeze system using the stock Ruby 1.8.7, taken from within a /var/lib/gems/1.8/gems/aws-sdk-v1-1.52.0/bin/aws-rb session:\nirb\nirb(main):001:0> iam = AWS::IAM.new( :credential_provider => AWS::Core::CredentialProviders::SharedCredentialFileProvider.new )\nNoMethodError: undefined method `ArgumentError' for #<AWS::Core::CredentialProviders::SharedCredentialFileProvider:0x7f6a410fb768>\n        from /var/lib/gems/1.8/gems/aws-sdk-v1-1.52.0/lib/aws/core/credential_providers.rb:277:in `shared_credential_file_path'\n        from /var/lib/gems/1.8/gems/aws-sdk-v1-1.52.0/lib/aws/core/credential_providers.rb:293:in `initialize'\n        from (irb):1:in `new'\n        from (irb):1\nI see it trying to raise that error, but apparently it doesn't know how?\nirb\nirb(main):001:0> iam = AWS::IAM.new( :credential_provider => AWS::Core::CredentialProviders::SharedCredentialFileProvider.new( :path => '/root/.aws/credentials' ) )\n=> <AWS::IAM>\nAwesome, that worked fine.  Additional testing after this point confirmed that I was able to query IAM as I expected.\nirb\nirb(main):001:0> iam = AWS::IAM.new\n=> <AWS::IAM>\nLooks good... but then as soon as I try to query, I get the big \"this is how to pass credentials\" warning, instead of the ArgumentError I expected, which would have pinpointed the problem (namely that I cannot expect the shared credential file lookup to work on an old Ruby).\n. When I run that command on the same system, the hash is not empty; it contains the correct :access_key_id and :secret_access_key.  So that part looks like it's doing the right thing.\n. Outstanding!  It looks like this is fixed now.  Thank you for sorting this out!\n. ",
    "ashfurrow": "Certainly. I eventually tracked down the problem: Travis CI was invoking bundle install and installing our second_curtain tool with the version specified in the Gemfile.lock file. The version it was depending on was 1.48.0, if I understand correctly. Later, and due to my error, an explicit gem install second_curtain command was executed, which tried to install a newer version of aws-sdk, 1.50.0, which I believed caused the conflict. The relevant line of the Gemfile is \nruby\ngem 'second_curtain', '~> 0.2.0'\nWhile those of the Gemfile.lock file are \nGEM\n  remote: https://rubygems.org/\n  specs:\n    aws-sdk (1.50.0)\n      json (~> 1.4)\n      nokogiri (>= 1.4.4)\n    second_curtain (0.2.0)\n      aws-sdk (~> 1.48)\n      mustache (~> 0.99)\nLet me know if there is anything else I can do to help track down the problem. I'm not an experienced Ruby developer, so it's entirely possible that the problem is a result of an error I've made. \n. > or to not gem install outside the bundle install (which shouldn't need to do)\nThis is exactly what we ended up doing. Thanks for clarifying that it was the correct solution!\n. ",
    "bzanchet": "Hey Trevor,\nthanks so much for your quick feedback. I agree that this patch isn't backwards compatible and that's a problem - one simplistic solution would be to make \"force_refresh\" default to true - but it felt too hacky even for scratching my own itch :)\nAnyway, I like the idea of adding a new configuration - would you say it's worth doing? It shouldn't take more than a couple hours to make this patch rely on a new configuration and be 100% backwards compatible. Or should I and everybody else just start using the RC of the new version?\nBest!\nBruno\n. Hey Trevor, there you go - took less than I expected.\nI sticked with the configuration name you mentioned-- can you think of a better one? Also removed the \"force_refresh\" option for simplicity sake. I guess one can just create a new instance of the object if they want to refresh its attributes (Rails has a \"reload\" method on ActiveRecord instances-- I think if we're going down that path, a full \"reload\" - which would just return a new, empty instance of the same S3Object - is probably simpler/better than having a \"force_refresh\" in each one of the attributes.\nLooks better?\n. Also-- do you want me to squash all the commits? I couldn't find a way to do that without having to close this PR and creating a new one.\nAnd a neat tick: it's easier too look at the changes if you add \"?w=1\" on the github URL - does the same as the -w option on git (as far as I know this isn't documented anywhere); because my editor removes all trailing spaces and messes up the raw diff.\n. Hello?\n. ",
    "franciscodelgadodev": "Ooopps, nevermind guys. The correct way to subscribe a BAIDU device is:\nruby\nendpoint_params = { \n  platform_application_arn: \"baidu_app_arn\", \n  token: \"baidu_device_channel_id\", \n  params: { \n    \"UserId\" => \"baidu_device_user_id\" ,\n    \"ChannelId\" => \"baidu_device_channel_id\" \n  } \n}\nAWS::SNS::Client.new.create_platform_endpoint(endpoint_params)\nPlease check that \"UserId\" and \"ChannelID\" keys are \"camelized\" strings. \"ChannelId\" value should be the same as \"token\" value.\nJust in case someone is having troubles like me jeje.\nThanks!\n. Thanks @daniel-marschner ! it might help to someone else ;) thanks for sharing your problem and your solution!\n. ",
    "daniel-marschner": "Thanks for coming up with that, but I had issues with your documented solution. So, I went ahead and checked the Android SDK and found this line. It shows how they create the request body for the platform endpoint creation request and the :params key is actually called :attributes instead. So here is my solution:\nruby\nendpoint_params = { \n  platform_application_arn: \"baidu_app_arn\", \n  token: \"baidu_device_channel_id\", \n  attributes: { \n    \"UserId\" => \"baidu_device_user_id\" ,\n    \"ChannelId\" => \"baidu_device_channel_id\" \n  } \n}\nAWS::SNS::Client.new.create_platform_endpoint(endpoint_params)\nWith that payload it actually worked for me, but again, thanks for documenting that issue in first place!\n. ",
    "jimberlage": "Thanks for the explanation, if they can be used without conflict I'll simply use both.  I have no need for a backport, then.\n. ",
    "jzajac2": "I'm having a hard time getting this to work. Am I supposed to use both of these list_resource_record_sets calls to get all results?:\n```\nwith pagingation, continues until all records have been returned\nr53.client.list_resource_record_sets({:hosted_zone_id => thezone}).each do |resp|\n  # ...\nend\nall response objects are PageableResponse objects and know how to get the next page\nresp = r53.client.list_resource_record_sets({:hosted_zone_id => thezone})\nresp.last_page?\nresp = resp.next_page until resp.last_page?\n```\nOr is just one of the calls sufficient? And if just the first one is sufficient (the one with the .each do loop, what goes in the # ... ? Seems like I would need to append to a variable (or the resp object). . ",
    "srchase": "@jzajac2, \nAssuming you're using the current version of the SDK, pagination for that resource is handled by the SDK and documented here: Paging Response Data.\nThe examples in the documentation show how to let the SDK handle paging through a built-in enumerator and how to manually page the results.. @mujz \nDid you try the suggestion from bittrance?. @islemaster \nDo you have any additional details that would help us reproduce the issue?. Closing this issue.  Happy to re-open if there are additional details to reproduce.. A fix for this has been merged.  The documentation will get updated with the next release.. I've added the feature request for this.. I've added the feature request for this.. @toaster \nThe Service Team indicated a fix for this issue is in place and you should not be seeing this error.\nI'm going to close this issue, but if you experience a regression, let us know and we can re-open.. Our suggested best practice is to utilize individual service gems in lieu using require \"aws-sdk\".\nI'm going to close this issue now.  If there's additional information, we can revisit, but for now, we feel using individual service gems is the right approach for most users of the SDK.. @shibz \nSee this migration page: Migrating from Version 1 or 2 to Version 3 of the AWS SDK for Ruby.\nThe top-level of the documentation gives the general use case and is the quickest way to get started with the SDK.  If you look at the developer guide, we offer documentation on more nuanced uses of the SDK, which include specifying specific versions, or using the gems independently if you are not utilizing all of the AWS services.. I've added the feature request for this.. @raymondhoagland \nI'm going to close this issue.  If you have evidence that there is an issue with the way the SDK is handling this, let us know and we can re-open.. @peggles2 \nThin can be configured to run as threaded, but is not by default.\nIf you're using a configuration file for Thin, you could check to there: https://github.com/macournoyer/thin#configuration-files. There's required work to figure out the right way to handle types in the resource models.\nI'm closing this issue and we'll track it as a feature request.. @philsmy \nWere you able to get this resolved?  This sounds like an issue on the ActionMailer side, and not with aws-sdk-ses.. I'm going to go ahead and close this issue, since it likely pertains to an issue on the ActionMailer side.\nWe're happy to investigate further if you decide to take another look at it.. @JamesDullaghan \nWere you able to come up with a reliable repro for this issue?\nI attempted to recreate the issue, but did not encounter this behavior.. Closing for inactivity.. @tommybonderenka \nI'm going to close this issue for now.  If you are able to capture a wire trace, or have feedback from the service, let us know and we can re-open.. Hi @arianf,\nWould you be interested in submitting a Pull Request for this change?. Hello @allcentury,\nIs there a specific version and architecture of Alpine where you're seeing these transient issues?. @softwaregravy,\nThanks for attempting to reproduce the issue.  If you do come up with something consistent, let us know.. @vbalazs \nWhat version of Shoryuken are you using?\nDo you ever get this error where Shoryuken is not used?. @vbalazs \nHave you continued to see this error?  I've spent a bit of time trying to reproduce the issue, but have not had any luck.\nHas anything happened recently that would help me understand what specifically is causing this issue?\nThanks!. I'll go ahead and close this issue for now.  Let us know if anything else comes up that might help.. @petrgazarov \nThis looks like an issue with the way the file is being read.\nCan you try this instead?\n```\n\ufeff    s3 = Aws::S3::Client.new(region:'us-east-1')\ns3.put_object({\n  bucket: BUCKET_NAME,\n  body: File.open(file_path, 'rb').read,\n  key: some_file_key_with_extension\n})\n\n```. Closing this issue.  Let us know if you have any further questions.. @jemmyw \nAre there any other details you can provide that might help us narrow down what is causing this to happen?\nIs the xml always malformed in the same spot?. @lwoggardner \nThanks for reporting this issue.  We're looking at getting this resolved.. @mark100net \nYou can update the Content-Type like this:\n```\nclient = Aws::S3::Client.new(\n  :region => 'us-west-2',\n  :access_key_id => ENV['AWS_ACCESS_KEY_ID'],\n  :secret_access_key => ENV['AWS_SECRET_KEY'], \n)\ns3 = Aws::S3::Resource.new(:client => client)\nobject = s3.bucket(ENV['AWS_BUCKET']).object('mypath/myfilename')\nnew_metadata = object.metadata.merge(\"json\" => { \"name\" => \"afile.zip\",\n                                  \"model_class\" => \"Course\",\n                                  \"model_attachment\" => \"course_file\" }.to_json)\nsource = ENV['AWS_BUCKET'] + 'mypath/myfilename'\nobject.copy_from({acl: 'public-read', content_type: 'text/plain', metadata_directive: 'REPLACE', copy_source: source, metadata: new_metadata})\n```. @maryshirl \nWas the device previously confirmed?\nAre you able to get the list of devices by calling client.confirm_device?  Or can you see the device listed in the AWS Console?. We're working with the Cognito Identify Provider to clear up this issue.  They're looking into the documentation to provide clarity on when new_device_metadata should be returned with an auth call.\nWe'll pass along that information it is shared with us.. I checked in with the team again, but do not have an update to share yet.  I'll keep you posted as I hear back.. @maryshirl \nThe Service Team has indicated that their documentation is incorrect and needs to be updated.\nTheir explanation is that only auth flows that include RespondToAuthChallenge will return NewDeviceMetadata.\nAre you able to change to an auth flow that includes RespondToAuthChallenge?. @maryshirl \nI'm going to close this issue for now.  Happy to re-open if you suspect the SDK is not working as expected.. Hello @DenisUA \nYou have the correct usage for setting a DefaultSenderID.\nHave you confirmed that the location where you are attempting to send the message supports Sender IDs?\nSupport for Sender IDs is described here: Supported Regions and Countries. @DenisUA \nNot all countries and regions support Sender IDs.\nI'm not getting that error when executing the same code using the same versions of Ruby and SNS.\nAre you using a different Sender ID?  Is it valid?  \"Custom\" will work.  Requirements for Sender ID can be found here: Sending an SMS Message\n. @DenisUA \nI was able to successfully receive an alphanumeric Sender ID using your code.\nAs the SDK itself seems to be functioning properly, I'll suggest that you reach out to the SNS service team directly via the AWS Forums.. @jec \nWere you able to try this without specifying the SourceRegion?\nThe SourceDBInstanceIdentifier has the region embedded in it, so a separate SourceRegion parameter is not required.. @jeyraof \nCreating the presigned post does not create an object in s3.  The object is not created until a form is submitted to actually upload that file to s3.. @janko-m \nThanks for reporting this issue.  We're taking a look.. Hello @andrew-newell,\nThanks for reporting this issue. \nBoth of the issues you point to are for the prior version of the SDK.  Stubbing for the current version does not support writing the response_target to a file.\nI'm marking this a feature request.\n. @andrew-newell \nCan you give this a try instead?\n@s3_client = Aws::S3::Client.new(stub_responses: true)\n@s3_client.stub_responses(:get_object, body: 'this is a body')\n@s3_client.get_object(response_target: 'output.txt', bucket: 'myBucket', key: 'myKey'). You can check out this blog post by @awood45 for some additional reading on Advanced client stubbing in the AWS SDK for Ruby Version 3.\nYou'll likely be able to accomplish most things with stub_responses.  That allows you to set the data or errors that will get returned on the normal client calls you make elsewhere.. Hello @RUN-CMD \nI apologize for the delayed response.\nThe service is not returning the md5 of message attributes.\nIf you enable the http_wire_trace when setting up the SQS client, like this:\nsqs = Aws::SQS::Resource.new(region: 'us-west-2', http_wire_trace: true)\nyou can see the XML response and will see that the md5 you're looking for is not there.  When I ran the code, I did receive the message attributes though.\nI've opened a request with the service team to have them look into the issue.  I'll update you as I hear a response from that team.. Hello @RUN-CMD,\nCan you try this instead:\nqueue.receive_messages({message_attribute_names: [\"All\"]})\nWhen message_attribute_names is set, either to \"All\" or specific message attribute names, it will return all or specific message attributes along with an MD5 hash of the returned attributes.\nThe documentation should be improved to make that clear that message_attribute_names can take \"All\".. Hello @philippevezina,\nAcceleration is supported with the Presigner, but you need to pass in use_accelerate_endpoint on the presigned_url method:\n```\nrequire 'aws-sdk-s3'\nsigner = Aws::S3::Presigner.new()\nurl = signer.presigned_url(:put_object, bucket: 'my_bucket', key: 'my_key', use_accelerate_endpoint: true)\n```\nIs this what you were hoping to accomplish?. @bituinb \nThanks for raising this issue.\nWhere is this error being returned?  On the object.upload_file?. Thanks for following up!  I'm going to close out this issue.. Yes, the client is thread-safe.  See: https://github.com/aws/aws-sdk-ruby/issues/1001#issuecomment-157122330. @CESteinmetz \nFor a real wait_until call, you can supply parameters that will be used used for the describe_db_instances calls.  Those parameters can help if you want to wait for a specific instance (or group of instances through filtering) to reach the available state.  If you don't supply those parameters, the underlying describe_db_instances call will return all db instances.\nIn @awood45's example, the stubbed response for describe_db_instances, used by the wait_until call, is set to return the single instance, so it is unnecessary to add any params.. Closing this issue.  Let us know if you have additional questions.. Hello @wjaspers,\nThanks for opening this issue.\nIs the verify if bucket being set properly on the resource?\nCan you try this instead?\nbucket = resource.bucket('arn:s3:bucket')\nobject = bucket.object('path-to-file')\nobject.exists?\n. @wjaspers \nWhich form of encryption are you using?  There are some differences between how head_object works depending on if you are using KMS, S3-managed keys, or customer-provided keys.. Did the break happen to coincide with a change in version of the SDK?. @wjaspers \nCan you try changing your client to this?\n```\nclient = Aws::S3::Client.new(region: 'us-east-1', credentials: Aws::InstanceProfileCredentials.new)\n```\nThe S3 client can be instantiated with Aws::InstanceProfileCredentials.new directly.  You don't need to call STS as an interim step.. Just to be sure we're on the same page, is this the code you're using?\n```\nrequire 'aws-sdk-s3'\ncredentials = Aws::InstanceProfileCredentials.new()\nclient = Aws::S3::Client.new(region: 'us-west-2', credentials: credentials)\nresource = Aws::S3::Resource.new(client: client)\nbucket = resource.bucket('myBucket')\nobject = bucket.object('path-to-file')\nputs object.exists?\n```. @wjaspers \nThanks for sorting this out.  I'll go ahead and close this issue, but we'll take another look at that documentation.  Apologies for the confusion.. @Doug-AWS \nThanks for opening this issue.\nDid the customer have a specific question?. @Doug-AWS \nThe configuration documentation describes the way that the SDK automaticaly searches for credentials.\nTo use a shared credentials file directly, you can do the following:\n```\nrequire('aws-sdk-s3')\nshared_creds = Aws::SharedCredentials.new(path: 'my_path', profile_name: 'my_profile_name')\ns3 = Aws::S3::Client.new(region: 'us-west-2', credentials: shared_creds)\n```\nThe path or profile_name can be omitted to use the respective defaults of '~/.aws/credentials' and 'default'.\nRefer to the SharedCredentials documentation.\nYou can also use the shared_creds to update the global configuration (instead of using them for a specific client):\n```\nshared_creds = Aws::SharedCredentials.new(path: 'my_path', profile_name: 'my_profile_name')\nAws.config.update(credentials: shared_creds)\n```\n. @alinavancea \nThanks for opening this issue.\nThe AWS CLI ls command contains logic that combines S3 API calls to list all of the buckets and objects available to the authenticated sender of the request.\nThe Ruby SDK does not contain the same feature.  Instead, it gives you the underlying API calls that could be combined to list all of the objects in each bucket.\nYou could use list_buckets to fetch all of your buckets, and then call list_objects_v2 on each bucket.. @alinavancea \nThe Access Denied error indicates that the assume role you are using does not have permissions to list the buckets of the account.\nYou can refer to this documentation to make sure that your policy for that role is correct.\nThe AWS Developer Forums for S3 can be of help if you have further questions on configuring the correct permissions.. @alinavancea \nThanks for following up.\nDoes this accomplish everything you were needing?  Do you have any followup questions?. @alinavancea \nYou're welcome.  Closing out this issue.. @cgledezma1101 \nThanks for opening this issue.\nAre you able to share which third-party library you are using?. @cgledezma1101 \nCan you also provide a full wire trace?\nThat would point to where this error is being raised.. This would be an error passed through from the CloudFormation API for template validation.  That validation would happen on the creation of a stack or change set, or on a call to validate_template.\nThe validation is performed by the service and not in the SDK itself.  As there isn't an issue with the SDK, I'm going to close this issue.  We'll assist the Service Team in getting this resolved.. @mmattice \nThanks for opening this issue.  The Ruby SDK does not currently support using ca_bundle in config.\nMarking this as a feature request.. Tracking this as a Feature Request: https://github.com/aws/aws-sdk-ruby/pull/1904. @itmart \nThanks for opening this issue.\nContents defines a single response.  Can you give this a try instead?\nAws.config[:s3] = {\n  stub_responses: {\n    list_objects: [\n        'Errno::ECONNREFUSED', \n        { key: \"key/aws_file.pdf\" }\n    ]\n  }\n}\nCheck out the Advanced Client Stubbing blog post.. @itmart \nRather than define the s3 resource again, can you reuse it?  Or am I misunderstanding where you are having the issue?. @itmart,\nYou're calling download_file twice?\nIf you call that twice, it will be setting up a new s3 resource each time.\nIf you want to advance to the second stubbed response, you'll need to re-use that s3 resource rather than creating a new one.\n. This part:\nbucket_objects = bucket.objects.select do |item|\n    item.key.start_with? object_key\nend\nIs trying to iterate through the 'contents' returned in the list_objects call, where each item is a hash, with the key that you are trying to match on.  That error is being raised because it cannot find a key while attempting to iterate on the response you stubbed.\nYou can stub list_objects like this, so that the second list_objects call will get something that is meaningful to bucket.objects.select:\nAws.config[:s3] = {\n  stub_responses: {\n    list_objects: [\n      Errno::ECONNREFUSED,\n      { contents: [ {key: 'foo' }, {key: 'foobar'}, {key: 'bar'} ] }\n    ]\n  }\n} . @janko-m,\nThe Aws::S3::Object#data will return the equivalent head of that object.\nobj = Aws::S3::Object.new(bucket_name: 'foo', key: 'bar')\nobj.data_loaded? # => false\nobj.data # =>  #<struct Aws::S3::Types::HeadObjectOutput... with all read attributes of object\nobj.data_loaded? # => true (following last obj.data call which required loading data)\nobj.load # => reloads data, via head object call.\n. Tracking as a feature request: FEATURE_REQUESTS. Tracking as a feature request: FEATURE_REQUESTS. @kellyfelkins,\nI'm working to see if this issue can be reliably reproduced.. @kellyfelkins,\nI haven't been able to reproduce this issue.\nCan you confirm whether or not the record event payload is correct or not?\nCan you also enable http_wire_trace on the S3 client and confirm if the data being returned from S3 is valid or not?\n. @kellyfelkins \nI'm going to close this issue.  If you have time to investigate, we can re-open.. @waldyr \nThanks for opening this issue.\nThe copy_object documentation states that the copy_source parameter must be URL-encoded.\nFor simple keys, this may not matter, but if your keys can potentially include any special characters, you should encode them to avoid this issue.\nS3_CLIENT.copy_object(acl: 'public-read', bucket: S3_BUCKET.name, key: 'z2', copy_source: URI.encode_www_form_component(File.join(S3_BUCKET.name, s3_key)))\n. I appreciate the offer for a PR, but the other SDKs have the same behavior and we wouldn't be able to change the functionality that might break other customers with existing implementations. . I'll go ahead and close this issue for now.  Let us know if you have any further questions.. @bjonord \nI've marked this as a duplicate and will close.  You can track resolution for this dependency issue in #1917.. Thanks for opening this issue.\nJust to be clear, versioning is successfully enabled on the origin bucket, but just not in time for the put_bucket_replication call to succeed?. Do you see this error every time?  Or does put_bucket_replication sometimes succeed?\nAlso, are these newly created or existing buckets?\n. @mike-bailey \nThanks for following up.\nI have not been able to reproduce this issue, but the evidence indicates that the SDK is executing put_bucket_versioning and put_bucket_replication as expected.   S3's underlying consistency model is the likely culprit for what's happening on the service side.  Unfortunately, I don't have documentation from S3 that spells out the impact of enabling versioning on subsequent calls to add bucket replication.\nIf you capture the Request IDs, the S3 service team may be able to assist further on the AWS Developer Forums.  Optionally, you could retry setting up replication when you encounter this error, or make a call to confirm that versioning is enabled before doing the put_bucket_replication call.. Have you tried inserting a get_bucket_versioning call before the put_bucket_replication?\nIn my testing, that consistently returned \"Enabled\" (while the subsequent put_bucket_replication calls succeeeded).  I'm curious if you see that returning \"Disabled\" or not.\nDepending on the specifics of your app, you could implement doing a specific number of calls with get_bucket_versioning until \"Enabled\" is returned, before proceeding to put_bucket_replication.  That would be akin to the waiters implemented in the SDK and might provide a workaround while following up with the S3 Service Team.\nI can pass along the Request IDs for S3 to take a look to offer further insight on what's happening on the service side.. @mike-bailey \nThanks for replying.  The S3 Service Team will need the bucket name, and the following headers from the responses:\nx-amz-id-2\nx-amz-request-id\nYour screenshot only gives the partial requestID.. @mike-bailey \nDoes your organization have an AWS Support plan?\nYou could open a case with  AWS Premium Support.. @mike-bailey \nI'm going to close this issue for now.  Happy to re-open if there's any followup once you've engaged Support and the Service Team.. @mike-bailey \nWe can track this as a feature request to implement a waiter.. @coreyaus \nThanks for opening this issue.\nHow are you configuring your Secret?  As an RDS Database? Or a generic Database?. @coreyaus \nThanks for clarifying.  I suspect you'd see the same behavior in either case, but wanted to be sure.  I'm able to reproduce the issue and have escalated to the Service Team to get this resolved.. @coreyaus \nThe Service Team has indicated that only Aurora Clusters and Serverless Aurora types are supported at this time.\nAre you using an Aurora?. Please refer to this documentation:\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/data-api.html\nNote that the Data API is in beta for Aurora Serverless and is subject to change.. @coreyaus \nCheck with the AWS Developer Forums for RDS.   That team would be better equipped to answer questions regarding their plans for the Data API.\nI've opened a request with the Service Team to update the documentation to reflect which DB Types are supported and to make clear the limitations of the Data API at this point.\nSince that needs handled on their end, I'm going to close this issue for now.   Appreciate you raising this issue and your feedback.. @mheffner \nThanks for opening this issue.\nThe API Reference for the CostExplorer indicates that only MONTHLY and DAILY are supported for Granularity on GetCostAndUsage, GetReservationUtilization and GetReservationCoverage.\nI'll followup with the Service Team to get this discrepancy resolved.. @mheffner \nThe Service Team confirmed that HOURLY granularity is not supported at this time.\nThey are working to update the resource model, which will trigger an update on the SDK that will correct the documentation.. Closing this issue.  The Service Team is tracking resolving this issue on their end.. @kreynolds \nThanks for opening this issue.\nThe pipe object needs to be read.  Can you give this a try?\nIO.pipe do |r, w|\n  w << \"test\"\n  w.close\n  s3.put_object(\n    bucket: 'bucket',\n    key: 'object',\n    body: r.read\n  )\nend. @kreynolds \nSee this issue: https://github.com/aws/aws-sdk-ruby/issues/1010\nWithout a size, the SDK is not able to complete the PUT to S3 (nothing to set the content-length header).\nCould you help us with a few more details on your use case so that we might better understand how this might be improved?\n. @kreynolds \nAppreciate the candid feedback.   I've got a followup item to see if we can make this more clear to avoid further confusion.. @shoja \nAre you able to update to the latest version of aws-sdk-s3 and Sidekiq?\nThe aws-sdk-s3 version is fairly recent, but I understand that that version of Sidekiq is a bit older and upgrading might be more involved.\nI'm curious if you get the same behavior with a more recent version of Sidekiq.. Closing this issue for inactivity.. Thanks for opening this issue.\nAre you looking for something like this, but for use with Ruby? \nAmazon Kinesis Producer\nI've marked this as a feature request for the team to consider.. @peterwake \nThanks for opening this issue.\nThe move_to operation does not download the data to your server.  That operation uses S3's PUT Object - Copy, or Upload Part - Copy when multipart_copy is enabled, to copy the object within S3.  The body of the object is not actually downloaded.\nThe AWS CLI command you're invoking is less low-level.  It's using the same underlying S3 operations, but is doing more to abstract and actively manage moving (copying and deleting) objects from one folder to another.\nYou can add the http_wire_trace: true option when initializing your object (or S3 client) to see the underlying calls that are being made to S3.  That could help you compare what's happening with the AWS CLI vs the Ruby SDK.\n. @peterwake \nUsing multipart_copy is required for objects 5GB and larger.  For files less than 5GB, you can do the move with one operation, but you could also test using multipart_copy and vary the chunk_size.   The AWS Developer Forum for S3 may be able to provide better guidance on the right number of threads for moving multiple objects.. @peterwake \nI'm going to close this issue.  Let us know if there's anything further.. Thanks @peterwake!\nHelpful gist and information for anyone else attempting to do the same thing.. @atyndall \nThanks for opening this issue.\nCan you provide an example of how you are instantiating your signer?. Closing this issue for inactivity.. @dudo \nCan you provide an example of how you are stubbing get_queue_url?\nAre you doing something like this:\nsqs.stub_responses(:get_queue_url, { queue_url: 'foo' }). @dudo \nHave you referred to this blog post on Advanced client stubbing?\nThat might give you a few patterns to use.. Thanks for opening this issue.  We'll review your PR.. @brandoncc \nThat trace points towards an issue with carrierwave.\nAre you using aws-sdk directly?  Or only via carrierwave?  Did your gem version of carrierwave get changed with your Rails upgrade?. I'll close this issue for now.  Happy to re-open if needed.. @umanoda \nHave you referred to this Advanced Client Stubbing blog post?\nThe patterns there for stubbing individual clients may be of help for your use case.. @umanoda \nWhich 'default stub stetting' are you referring to?. @mnacos \nThanks for opening this issue.\nI've been unable to reproduce this issue as described.\nDo you get the same behavior using the CLI?. Opting to close this issue for inactivity.  We're happy to re-open when you can reengage. . @hrjaan \nThanks for submitting this feature request.\nOthers that are interested in seeing this feature can chime in on this issue.. @nikriek \nWas 'aws-sdk-s3 1.10.0' meant to be 'aws-sdk-sqs 1.10.0'?\nAre you using a specific version of aws-sdk-core?. @nikriek \nThanks for following up.\nI wasn't able to reproduce this issue using the SDK directly.   Are there any additional details that might help with a reliable repro?  Is there anything out of the ordinary going on with ENV?\n. @nikriek \nThanks for following up with how you got this resolved.. @pablogmorales \nHave you referred to the Configuring the AWS SDK for Ruby documentation page?\nThis is the suggested way to read from a specific path (using the SharedCredentials format):\nshared_creds = Aws::SharedCredentials.new(path: 'my_path')\nAws.config.update(credentials: shared_creds)\nIf you need to read from JSON, you need to perform an update on Aws.config:\ncreds = JSON.load(File.read('secrets.json'))\nAws.config.update({\n  credentials: Aws::Credentials.new(creds['AccessKeyId'], creds['SecretAccessKey'])\n})\n. @pablogmorales \nAre the formats of /etc/sensu/plugins/credentials and /root/.aws/credentials the same?\nI would also check that you have the proper permissions for that path.\nIf you need additional help, Stack Overflow would be a good place using the aws-sdk-ruby tag.. Hard coding credentials is not recommended.\nThis does work if needed:\nAws.config.update({\n  credentials: Aws::Credentials.new('fooAccessKeyId', 'fooSecretAccessKey')\n}). Closing this issue.  Let us know if there's a need to re-open.. @SirRawlins \nThis is caused by the CloudFront API returning a self-closed  tag in the get_distribution_config response:\n<OriginGroups>\n  <Quantity>0</Quantity>\n  <Items/>\n</OriginGroups>\nThat output is an invalid input for the the update_distribution_config operation.  I've opened a request for the CloudFront Service Team to investigate.. https://github.com/aws/aws-sdk-ruby/pull/1960\nWe're getting 2.6 added to Travis CI.. Thanks for submitting this issue and PR.  We're reviewing.. Hello @tfolk,\nDid you have an issue?  Or was this opened by mistake?\nI'll close it out for now, but happy to re-open if you do have an issue.. @ice799 \nThanks for raising this issue.\nupload_file is expecting a path.\nIt will work if the TempFile is open or closed.\nDoes this work for you?  Or is there a different you were wanting to do?\n```\nf = Tempfile.new('foo');\nf.write('foo');\nf.close\nobj = Aws::S3::Object.new('my_bucket', 'my_key')\nobj.upload_file(f.path)\n```\n. @ice799 \nHave you referred to this documentation? upload_file\n\nParameters: source (String, Pathname, File, Tempfile) \u2014 A file or path to a file on the local file system that should be uploaded to this object. If you pass an open file object, then it is your responsibility to close the file object once the upload completes.\n\nThat line in the documentation is generated from this line: https://github.com/aws/aws-sdk-ruby/blob/708140b8f5777e05ec8acaaad7059a711e8b13e4/gems/aws-sdk-s3/lib/aws-sdk-s3/customizations/object.rb#L287. @w00lf \nThanks for this feature-request and feedback.  We'll review and see if we can improve this area.. Hello @ms-ati,\nThanks for opening this issue.\nThe tilde operator sets the dependency to a major version, but will allow minor version upgrades.   It will not allow an upgrade to the next major version (in this case 4.0).\nSee this part of the README:\n\nThis project uses semantic versioning. You can safely express a dependency on a major version and expect all minor and patch versions to be backwards compatible.\n\nDoes that clarify the intent?. @satyashanmuka \nThanks for submitting this issue.\nYou can use tagging to track resources at a more granular level.\nRefer to this documentation on how that can be achieved: Using Cost Allocation Tags\nOnce you've tagged resources, you can add tags to your get_cost_and_usage call to return only the cost and usage for the specific resource (a specific instance in your case).\n. Yes, that is correct.. Good to hear.  I'll close out this issue as the SDK portion of this is working as intended.\nIf you end up having further questions, follow-up and we can re-open this issue if needed.. @jfharden \nThanks for raising this issue.\nThe AutoScaling service is not returning the next_token element in the response.  I've raised this issue internally with that team to resolve the discrepancy between the documentation and the API.\nIn case you're curious, you can add http_wire_trace: true in the client to see the underlying API calls:\nautoscaling_client = Aws::AutoScaling::Client.new(region: \"eu-west-1\", http_wire_trace: true)\n. @thapakazi \nThanks for submitting this feature request.\nThis is a feature that would best be implemented by the Transcribe Service Team, and not in the SDK itself.\nHave you reached out to the Transcribe Service with this request?. I'll go ahead and close this issue here, but let us know if you have followup questions.. @thapakazi,\nYou can engage the Transcribe Service through the AWS Developer Forums.. @rafaelfranca \nThanks for raising this issue.  Client-side encryption under v1 was a customization that was not carried forward to v2 and v3.\nThat behavior for v3 is noted here: Client-Side Encryption \nI've tagged this as a documentation issue to add a note regarding client-side encryption for v2 and v3.. @nurse \nThe next_token is not required and will not be returned from the EC2 Service if there are not additional pages of results.\nIt should be set to nil in this case.\nCan you provide an example of your code?. @PatrickKing \nThanks for raising this issue.  Removing credentials from as_json would be a breaking change and would require a major version.. ",
    "carlhoerberg": "Nice, was looking in the AWS::EC2::VPC class. But never mind then. \n. Ideally you should use Refinements instead of open monkey-patching when patching core libraries. . ",
    "thewatts": "Found the issue,\nOnly requiring aws/s3 doesn't include the AWS configurations.\nChanging to: require 'aws' fixed it.\n. ",
    "jgauna": "I'm getting this issue too. Changed that line as you said but the error persists\n. This is my gemfile:\n``` javascript\nsource 'https://rubygems.org'\ngem 'RedCloth', :require => 'redcloth'\ngem 'ancestry'\ngem 'aws-sdk'\ngem 'aws-ses', :require => 'aws/ses'\ngem 'backbone-on-rails', '~> 1.0.0'\ngem 'backbone-support'\ngem 'bourbon', '~> 2.0.0.rc1'\ngem 'cancan'\ngem 'covenant'\ngem 'delayed_job', '~> 2.1.4'\ngem 'doc_raptor'\ngem 'dynamic_form'\ngem 'ejs'\ngem 'flutie'\ngem 'formtastic', '~> 2.2.1'\ngem 'haml-rails'\ngem 'high_voltage'\ngem 'jbuilder'\ngem 'jquery-rails'\ngem 'multi_json', '~> 1.3'\ngem 'mysql2', '~> 0.3'\ngem 'nokogiri'\ngem 'oj'\ngem 'paperclip'\ngem 'rack'\ngem 'rails', '~> 3.2.13'\ngem 'rake'\ngem 'rubyvis'\ngem 'will_paginate'\ngem 'grocer'\ngem 'net-sftp'\ngem 'thin'\ngem 'urbanairship'\ngem 'zipruby', :platforms => :mswin\ngem 's3_direct_upload'\ngem 'mime-types'\ngem 'therubyracer', :platforms => :ruby\ngem 'libv8', :platforms => :ruby\ngem 'rails_config'\ngem 'acts_as_list'\ngem 'devise'\ngem 'devise-encryptable'\ngem 'doorkeeper'\ngem 'oauth2'\ngem 'faker', '1.2.0'\ngem 'rack-cors', require: 'rack/cors'\ngem 'bxslider-rails'\ngem 'pagedown-rails', '~> 1.1.3'\ngem \"time_splitter\"\ngem 'american_date'\ngem 'country_select'\ngem 'jquery-fileupload-rails'\ngroup :assets do\n  gem 'coffee-rails', '~> 3.2.1'\n  gem 'compass-rails', '>= 1.0.3'\n  gem 'haml_coffee_assets'\n  gem 'sass-rails', '~> 3.2.3'\n  gem 'select2-rails'\n  gem 'turbo-sprockets-rails3'\n  gem 'uglifier', '>= 1.0.3'\n  gem 'jquery-ui-rails'\n  gem 'jquery-modal-rails'\n  gem 'jquery-validation-rails'\n  gem 'jquery-timepicker-rails'\n  gem 'chardinjs-rails'\nend\ngroup :development do\n  gem 'parallel_tests', '0.15.4'\n  gem 'sextant'\n  gem 'pry'\n  gem 'active_record_query_trace'\nend\ngroup :development, :test do\n  gem 'factory_girl_rails', '~> 2.0.0.rc'\n  gem 'guard-jasmine'\n  gem 'jasminerice'\n  gem 'pry-nav'\n  gem 'pry-rails'\n  gem 'pry-stack_explorer'\n  gem 'rspec-rails', '~> 2.13'\n  gem 'simplecov', :require => false\nend\ngroup :production, :staging do\n  gem 'memcache-client'\n  gem 'newrelic_rpm'\n  gem 'rack-ssl', :require => 'rack/ssl'\nend\ngroup :test do\n  gem 'bourne', '~> 1.4'\n  gem 'capybara', '2.1.0'\n  gem 'chronic'\n  gem 'codeclimate-test-reporter', require: nil\n  gem 'cucumber', '~> 1.3.1'\n  gem 'cucumber-rails', require: false\n  gem 'database_cleaner'\n  gem 'email_spec', '~> 1.4.0'\n  gem 'launchy'\n  gem 'poltergeist', '1.4.1'\n  gem 'selenium-webdriver', '2.35.1'\n  gem 'sham_rack'\n  gem 'shoulda-matchers', '~> 1.0'\n  gem 'timecop'\n  gem 'treetop'\n  gem 'json_expressions'\nend\n```\nThe ful error log is:\nC:\\Users\\Juancito\\Desktop\\CMS\\vertical-server>rails s\n=> Booting Thin\n=> Rails 3.2.21 application starting in development on http://0.0.0.0:3000\n=> Call with -d to detach\n=> Ctrl-C to shutdown server\nExiting\nC:/Users/Juancito/Desktop/CMS/vertical-server/config/initializers/aws.rb:5:in `<top (required)>': undefined method `config' for AWS:Module (NoMethodError)\n        from C:/Users/Juancito/Desktop/CMS/vertical-server/gems/ruby/1.9.1/gems/activesupport-3.2.21/lib/active_support/dependencies.rb:245:in `load'\n        from C:/Users/Juancito/Desktop/CMS/vertical-server/gems/ruby/1.9.1/gems/activesupport-3.2.21/lib/active_support/dependencies.rb:245:in `block in load'\n        from C:/Users/Juancito/Desktop/CMS/vertical-server/gems/ruby/1.9.1/gems/activesupport-3.2.21/lib/active_support/dependencies.rb:236:in `load_dependency'\n        from C:/Users/Juancito/Desktop/CMS/vertical-server/gems/ruby/1.9.1/gems/activesupport-3.2.21/lib/active_support/dependencies.rb:245:in `load'\n        from C:/Users/Juancito/Desktop/CMS/vertical-server/gems/ruby/1.9.1/gems/railties-3.2.21/lib/rails/engine.rb:593:in `block (2 levels) in <class:Engine>'\n        from C:/Users/Juancito/Desktop/CMS/vertical-server/gems/ruby/1.9.1/gems/railties-3.2.21/lib/rails/engine.rb:592:in `each'\n        from C:/Users/Juancito/Desktop/CMS/vertical-server/gems/ruby/1.9.1/gems/railties-3.2.21/lib/rails/engine.rb:592:in `block in <class:Engine>'\n        from C:/Users/Juancito/Desktop/CMS/vertical-server/gems/ruby/1.9.1/gems/railties-3.2.21/lib/rails/initializable.rb:30:in `instance_exec'\n        from C:/Users/Juancito/Desktop/CMS/vertical-server/gems/ruby/1.9.1/gems/railties-3.2.21/lib/rails/initializable.rb:30:in `run'\n        from C:/Users/Juancito/Desktop/CMS/vertical-server/gems/ruby/1.9.1/gems/railties-3.2.21/lib/rails/initializable.rb:55:in `block in run_initializers'\n        from C:/Users/Juancito/Desktop/CMS/vertical-server/gems/ruby/1.9.1/gems/railties-3.2.21/lib/rails/initializable.rb:54:in `each'\n        from C:/Users/Juancito/Desktop/CMS/vertical-server/gems/ruby/1.9.1/gems/railties-3.2.21/lib/rails/initializable.rb:54:in `run_initializers'\n        from C:/Users/Juancito/Desktop/CMS/vertical-server/gems/ruby/1.9.1/gems/railties-3.2.21/lib/rails/application.rb:136:in `initialize!'\n        from C:/Users/Juancito/Desktop/CMS/vertical-server/gems/ruby/1.9.1/gems/railties-3.2.21/lib/rails/railtie/configurable.rb:30:in `method_missing'\n        from C:/Users/Juancito/Desktop/CMS/vertical-server/config/environment.rb:8:in `<top (required)>'\n        from C:/Users/Juancito/Desktop/CMS/vertical-server/gems/ruby/1.9.1/gems/activesupport-3.2.21/lib/active_support/dependencies.rb:251:in `require'\n        from C:/Users/Juancito/Desktop/CMS/vertical-server/gems/ruby/1.9.1/gems/activesupport-3.2.21/lib/active_support/dependencies.rb:251:in `block in require'\n        from C:/Users/Juancito/Desktop/CMS/vertical-server/gems/ruby/1.9.1/gems/activesupport-3.2.21/lib/active_support/dependencies.rb:236:in `load_dependency'\n        from C:/Users/Juancito/Desktop/CMS/vertical-server/gems/ruby/1.9.1/gems/activesupport-3.2.21/lib/active_support/dependencies.rb:251:in `require'\n        from C:/Users/Juancito/Desktop/CMS/vertical-server/config.ru:3:in `block in <main>'\n        from C:/Users/Juancito/Desktop/CMS/vertical-server/gems/ruby/1.9.1/gems/rack-1.4.5/lib/rack/builder.rb:51:in `instance_eval'\n        from C:/Users/Juancito/Desktop/CMS/vertical-server/gems/ruby/1.9.1/gems/rack-1.4.5/lib/rack/builder.rb:51:in `initialize'\n        from C:/Users/Juancito/Desktop/CMS/vertical-server/config.ru:in `new'\n        from C:/Users/Juancito/Desktop/CMS/vertical-server/config.ru:in `<main>'\n        from C:/Users/Juancito/Desktop/CMS/vertical-server/gems/ruby/1.9.1/gems/rack-1.4.5/lib/rack/builder.rb:40:in `eval'\n        from C:/Users/Juancito/Desktop/CMS/vertical-server/gems/ruby/1.9.1/gems/rack-1.4.5/lib/rack/builder.rb:40:in `parse_file'\n        from C:/Users/Juancito/Desktop/CMS/vertical-server/gems/ruby/1.9.1/gems/rack-1.4.5/lib/rack/server.rb:200:in `app'\n        from C:/Users/Juancito/Desktop/CMS/vertical-server/gems/ruby/1.9.1/gems/railties-3.2.21/lib/rails/commands/server.rb:46:in `app'\n        from C:/Users/Juancito/Desktop/CMS/vertical-server/gems/ruby/1.9.1/gems/rack-1.4.5/lib/rack/server.rb:304:in `wrapped_app'\n        from C:/Users/Juancito/Desktop/CMS/vertical-server/gems/ruby/1.9.1/gems/rack-1.4.5/lib/rack/server.rb:254:in `start'\n        from C:/Users/Juancito/Desktop/CMS/vertical-server/gems/ruby/1.9.1/gems/railties-3.2.21/lib/rails/commands/server.rb:70:in `start'\n        from C:/Users/Juancito/Desktop/CMS/vertical-server/gems/ruby/1.9.1/gems/railties-3.2.21/lib/rails/commands.rb:55:in `block in <top (required)>'\n        from C:/Users/Juancito/Desktop/CMS/vertical-server/gems/ruby/1.9.1/gems/railties-3.2.21/lib/rails/commands.rb:50:in `tap'\n        from C:/Users/Juancito/Desktop/CMS/vertical-server/gems/ruby/1.9.1/gems/railties-3.2.21/lib/rails/commands.rb:50:in `<top (required)>'\n        from script/rails:6:in `require'\n        from script/rails:6:in `<main>'\nAs you may notice i'm using Windows. Last thing is i'm not a ruby dev. Just trying to run locally an enterprise project.\n. ",
    "myoung34": "Actually looks like this is available in the version 2 api\n. ",
    "bmurtagh": "Can you provide an example where multiple filters are applied? I'm attempting to do this, but I'm failing horribly. Sorry for digging up a post from 2014 :(\n. @kylev Awesome. Thank you so much for the example. I really appreciate it.\n. Can you provide more details or sample code on how you ended up running into this error?\n. ",
    "brandonkboswell": "I'm also running into this issue. Any idea which version of the aws-sdk-ruby doesn't contain this regression or perhaps any other possible workarounds you may know of?\n. Thanks @BRMatt that patch worked for me.\n. ",
    "BRMatt": "We're using this as a temporary patch:\nAWS::S3::PresignedPost::SPECIAL_FIELDS << :secure\n. I'm afraid I don't have trace information from the client - does the aws sdk provide a way to get that? I do have the cloud trail log (I edited the original post earlier to include it) which shows the error code that caused the waiter to blow up. \nLooking at the waiter configuration it seems it might need a retry processor on that error code, similar to the NatGatewayAvailable waiter?. It seems the request succeeded because an instance was created. The spot request ended up in the request-canceled-and-instance-running state. It looks like it was valid for around 5 minutes. I'm not sure why the instance was still running after the request was cancelled, or indeed what cancelled the request.\nI'll see if I can submit a PR tomorrow.. I tried out this patch on our system, it seems to handle the error correctly and waits until the request is fulfilled:\n\n\n. Oh so the json files are auto generated?. Gotcha, makes sense. Any idea on when that might happen? Just so I know when we can stop using our fork.. ",
    "dsmithco": "Just to clarify I put..\nAWS::S3::PresignedPost::SPECIAL_FIELDS << :secure\ninside my aws.rb\n# config/initializers/aws.rb\n  require 'aws-sdk'\n  AWS::S3::PresignedPost::SPECIAL_FIELDS << :secure\n. ",
    "cha55son": "I'm working on a minimal test case so this will be easier to solve.\n. Here's what I found while trying to create the minimal test case. We were using require 'json' in our Gemfile (to load engines dynamically). This in turn was conflicting with aws-sdk somehow and was breaking the JSON serialization of AR instances. Once I reworked the the code to not require 'json' the problem went away.\n. ",
    "bmedici": "Hi there, as I'm integrating an S3 target in rest-ftp-daemon, I was wondering if upload progress was available through a callback method or passing a block, now ?\nAny news about this feature ?\n. Thanks @Doug-AWS for the pointer. It may be useful to provide a clean and easy way to get a method called with progress information on a Aws::S3::Resource upload_file transfer (callback).\n. ",
    "palexvs": "I use aws-sdk (1.56.0)\n. Has this feature already added? It seems like I can't run new instances without it, I get error message \"State transition reason: Server.InternalError: Internal error on launch\" each time. ",
    "witoff": "+1.  This really needs to be available in Ruby SDK.\n. ",
    "jondahl": "I'm interested in this as well. Any plans for bringing this (or something similar) to the v2 SDK?\n. ",
    "sebhtml": "+1\n. ",
    "fedenusy": "+1\n. ",
    "joshmosh": "+1. ",
    "aliismayilov": "It happens on CGI::escape https://github.com/aws/aws-sdk-ruby/blob/master/lib/aws/core/uri_escape.rb#L26\n. Let me know if I should fix ./spec/aws/core/region_collection_spec.rb:60 for the build to be green.\n. ",
    "winebarrel": "Thank you for marging!\n. Thank you for merge! :-)\n. ",
    "igrep": "\nCurrently the v1 SDK is on feature lock and only receives API updates and bug fixes. You point is however valid. I will add a CHANGELOG for any maintenance releases from now on.\n\n@trevorrowe I got it. Thank you! :+1: \n. ",
    "fallwith": "Can someone please take a look at this one?\nIt looks like every other instance of Digest::MD5 was already converted to OpenSSL::Digest::MD5, and this PR seeks to make that same change to the last remaining instance of Digest::MD5 in the AWS::SQS::Queue class.\nCurrently we're using multiple threads to listen to SQS queues (one unique queue per thread) and occasionally everything blows up when it hits the Digest::MD5 line. I researched it a bit and found that Digest::MD5 is not thread-safe and that using OpenSSL::Digest::MD5 solves the problem. Then I discovered that the aws-sdk-ruby project had already made the change to OpenSSL for all but one instance of Digest::MD5, so this PR was submitted to get that last instance changed as well.\n. Thanks very much for working on this, @cjyclaire - that makes sense to me. Are the upstream API models and examples documentation available somewhere that I could help contribute to (either to help document the clarity you provided me with here, or for future contributions)?. ",
    "artemaminov": "Figured out\ninitializer created\nAWS.config(\n    region: 'eu-central-1',\n    s3_signature_version: :v4\n)\n. ",
    "ZipoKing": "My code snippet: (s3 and localized_s3 are AWS::S3 instances where localized_s3 has :s3_endpoint parameter changed\nruby\nbegin\n          s3.buckets[a.bucket_name].objects[a.path].copy_to(new_key, :bucket_name => b.name,\n                                                            :acl => acl)\n        rescue AWS::S3::Errors::TemporaryRedirect, AWS::S3::Errors::PermanentRedirect\n          localized_s3 = s3_instances[b.location]\n          localized_s3.buckets[a.bucket_name].objects[a.path].\n              copy_to(new_key, :bucket_name => b.name, :acl => acl)\n        end\n. ",
    "colstrom": "It looks like you can pull this from the describe_db_snapshots method on an Aws::RDS::Client instance.\nIs db_snapshot_identifier (on each element of the collection returned by describe_db_snapshots) what you're looking for?\nhttp://docs.aws.amazon.com/sdkforruby/api/Aws/RDS/Client.html#describe_db_snapshots-instance_method\n. ",
    "n-nishizawa": "Thank you so much for your reply.\nThe requests are for the same bucket.\nBut they are for different objects(keys).\nSorry,,, I'm using sdk v2.\nShould I move this issue to v2's issue page?\n. Thanks for fixing it!!\nIt works well, and gets better performance.\n. ",
    "weyus": "This issue was caused by my Rails.root/config/aws.yml file being present. Even thought I was not calling AWS.config, and even though the format of my aws.yml file was different than database.yml, the keys and values were still being picked up. I've since removed my call to AWS.config altogether since it isn't necessary.\n. ",
    "cmather": "Sweet!\n\nOn Jan 14, 2015, at 4:14 PM, Trevor Rowe notifications@github.com wrote:\nClosed #684 via aws/aws-sdk-core-ruby@3e33d31.\n\u2014\nReply to this email directly or view it on GitHub.\n. \n",
    "KranthiKishore": "Am getting Aws::S3::Errors::InvalidArgument: Argument format not recognized error when uploading file  if I give grant_read option. Can you tell me why is this?\nirb(main):084:0> object\n=> #<Aws::S3::Object bucket_name=\"test_development\", key=\"test_sample_file.csv\">\nirb(main):085:0> object.upload_file('/home/kranthi/projects/hubkit/example_output.csv', grant_read: 'test_sample_file.csv')\nAws::S3::Errors::InvalidArgument: Argument format not recognized\n. ",
    "fatuhoku": "What's the syntax then? Currently I've got:\nruby\n    obj.put(file_name, contents, {}, \"authenticated-read\"). ",
    "roman-mazur": "Thanks a lot!\n. ",
    "sheldonh": "Perfect, thanks!\n. ",
    "protochron": "I'd also be interested in knowing the best way to handle this case. It's especially useful when looking at CloudFormation stacks since it means you don't have to iterate over the entire stack collection.\n. ",
    "philostler": "I was very surprised when I went looking for the #exists? method on Aws::S3::Object and couldn't find it. I think this is a pretty major omission tbh. I'd be interested when there is a nice solution available... atm i'm rescuing Aws::S3::Errors::NoSuchKey :disappointed: \n. ",
    "smeyfroi": "I've got this down to a simple test case, which suggests that the DynamoDB API isn't available using the SDKv2?\n```\nirb(main):010:0> client.list_tables\nopening connection to dynamodb.eu-west-1.amazonaws.com:443...\nopened\nstarting SSL for dynamodb.eu-west-1.amazonaws.com:443...\nSSL established\n<- \"POST / HTTP/1.1\\r\\nContent-Type: application/x-amz-json-1.0\\r\\nAccept-Encoding: \\r\\nUser-Agent: aws-sdk-ruby2/2.0.22 ruby/2.1.5 x86_64-linux\\r\\nX-Amz-Target: DynamoDB_20120810.ListTables\\r\\nX-Amz-Date: 20150210T104811Z\\r\\nHost: dynamodb.eu-west-1.amazonaws.com\\r\\nX-Amz-Content-Sha256: 44136fa355b3678a1146ad16f7e8649e94fb4fc21fe77e8310c060f61caaff8a\\r\\nAuthorization: AWS4-HMAC-SHA256 Credential=REDACTED/20150210/eu-west-1/dynamodb/aws4_request, SignedHeaders=content-type;host;user-agent;x-amz-content-sha256;x-amz-date;x-amz-target, Signature=d387e39fc11124a7c5788005437282573aafd22823528c521605b78afa19904a\\r\\nContent-Length: 2\\r\\nAccept: /\\r\\n\\r\\n\"\n-> \"HTTP/1.1 200 OK\\r\\n\"\n-> \"x-amzn-RequestId: C6UL0NHNURSHN9L8C8QTPQV753VV4KQNSO5AEMVJF66Q9ASUAAJG\\r\\n\"\n-> \"x-amz-crc32: 2003450131\\r\\n\"\n-> \"Content-Type: application/x-amz-json-1.0\\r\\n\"\n-> \"Content-Length: 98\\r\\n\"\n-> \"Date: Tue, 10 Feb 2015 10:47:09 GMT\\r\\n\"\n-> \"\\r\\n\"\nreading 98 bytes...\n-> \"{\\\"TableNames\\\":[\\\"test_cc_events\\\",\\\"test_cc_group_events\\\",\\\"test_cc_group_metrics\\\",\\\"test_cc_metrics\\\"]}\"\nread 98 bytes\nConn keep-alive\n[Aws::DynamoDB::Client 200 0.042491 0 retries] list_tables()\n=> {\"TableNames\"=>\n  [\"test_cc_events\",\n   \"test_cc_group_events\",\n   \"test_cc_group_metrics\",\n   \"test_cc_metrics\"]}\n```\n```\nirb(main):011:0> client.describe_table(table_name:'test_cc_events')\nopening connection to dynamodb.eu-west-1.amazonaws.com:443...\nopened\nstarting SSL for dynamodb.eu-west-1.amazonaws.com:443...\nSSL established\n<- \"POST / HTTP/1.1\\r\\nContent-Type: application/x-amz-json-1.0\\r\\nAccept-Encoding: \\r\\nUser-Agent: aws-sdk-ruby2/2.0.22 ruby/2.1.5 x86_64-linux\\r\\nX-Amz-Target: DynamoDB_20120810.DescribeTable\\r\\nX-Amz-Date: 20150210T104843Z\\r\\nHost: dynamodb.eu-west-1.amazonaws.com\\r\\nX-Amz-Content-Sha256: 6ddbdfcc57cff70175d9c8a0c05e6634f326473df31f155bf7c1d1879cd07788\\r\\nAuthorization: AWS4-HMAC-SHA256 Credential=REDACTED/20150210/eu-west-1/dynamodb/aws4_request, SignedHeaders=content-type;host;user-agent;x-amz-content-sha256;x-amz-date;x-amz-target, Signature=e5803521326ebb453f1bc67d9837dabd1541629b92c55d34da9dfc83bc4d1cea\\r\\nContent-Length: 31\\r\\nAccept: /\\r\\n\\r\\n\"\n-> \"HTTP/1.1 400 Bad Request\\r\\n\"\n-> \"x-amzn-RequestId: LKQDT6ECNS1KE7UMFN5U66A82RVV4KQNSO5AEMVJF66Q9ASUAAJG\\r\\n\"\n-> \"x-amz-crc32: 2965254985\\r\\n\"\n-> \"Content-Type: application/x-amz-json-1.0\\r\\n\"\n-> \"Content-Length: 143\\r\\n\"\n-> \"Date: Tue, 10 Feb 2015 10:47:41 GMT\\r\\n\"\n-> \"\\r\\n\"\nreading 143 bytes...\n-> \"{\\\"__type\\\":\\\"com.amazon.coral.validate#ValidationException\\\",\\\"message\\\":\\\"The paramater 'TableName' is required but was not present in the request\\\"}\"\nread 143 bytes\nConn keep-alive\n[Aws::DynamoDB::Client 400 0.035712 0 retries] describe_table(table_name:\"test_cc_events\") Aws::DynamoDB::Errors::ValidationException The paramater 'TableName' is required but was not present in the request\nAws::DynamoDB::Errors::ValidationException: The paramater 'TableName' is required but was not present in the request\n    from /geome/releases/d98388511cf5209a6d4e58bf34901899517bcd04/vendor/bundle/ruby/2.1.0/gems/aws-sdk-core-2.0.22/lib/seahorse/client/plugins/raise_response_errors.rb:15:in call'\n    from /geome/releases/d98388511cf5209a6d4e58bf34901899517bcd04/vendor/bundle/ruby/2.1.0/gems/aws-sdk-core-2.0.22/lib/aws-sdk-core/plugins/response_paging.rb:10:incall'\n    from /geome/releases/d98388511cf5209a6d4e58bf34901899517bcd04/vendor/bundle/ruby/2.1.0/gems/aws-sdk-core-2.0.22/lib/seahorse/client/request.rb:70:in send_request'\n    from /geome/releases/d98388511cf5209a6d4e58bf34901899517bcd04/vendor/bundle/ruby/2.1.0/gems/aws-sdk-core-2.0.22/lib/seahorse/client/base.rb:215:inblock (2 levels) in define_operation_methods'\n    from (irb):11\n    from /geome/releases/d98388511cf5209a6d4e58bf34901899517bcd04/vendor/bundle/ruby/2.1.0/gems/railties-4.2.0/lib/rails/commands/console.rb:110:in start'\n    from /geome/releases/d98388511cf5209a6d4e58bf34901899517bcd04/vendor/bundle/ruby/2.1.0/gems/railties-4.2.0/lib/rails/commands/console.rb:9:instart'\n    from /geome/releases/d98388511cf5209a6d4e58bf34901899517bcd04/vendor/bundle/ruby/2.1.0/gems/railties-4.2.0/lib/rails/commands/commands_tasks.rb:68:in console'\n    from /geome/releases/d98388511cf5209a6d4e58bf34901899517bcd04/vendor/bundle/ruby/2.1.0/gems/railties-4.2.0/lib/rails/commands/commands_tasks.rb:39:inrun_command!'\n    from /geome/releases/d98388511cf5209a6d4e58bf34901899517bcd04/vendor/bundle/ruby/2.1.0/gems/railties-4.2.0/lib/rails/commands.rb:17:in <top (required)>'\n    from bin/rails:4:inrequire'\n    from bin/rails:4:in `'\nirb(main):012:0>\n```\n```\nirb(main):012:0> client.describe_table(tableName:'test_cc_events')\n\n\n\n\n\n\nthe same result as above\n```\n. And to eliminate the eu-west-1 endpoint, here's a result from us-east-1:\n\n\n\n\n\n\n```\nLoading production environment (Rails 4.2.0)\nirb(main):001:0> DynamodbSchema.create_events_table\nopening connection to dynamodb.us-east-1.amazonaws.com:443...\nopened\nstarting SSL for dynamodb.us-east-1.amazonaws.com:443...\nSSL established\n<- \"POST / HTTP/1.1\\r\\nContent-Type: application/x-amz-json-1.0\\r\\nAccept-Encoding: \\r\\nUser-Agent: aws-sdk-ruby2/2.0.22 ruby/2.1.5 x86_64-linux\\r\\nX-Amz-Target: DynamoDB_20120810.CreateTable\\r\\nX-Amz-Date: 20150210T110216Z\\r\\nHost: dynamodb.us-east-1.amazonaws.com\\r\\nX-Amz-Content-Sha256: 06f89020d041495afe8ad5b8a35630d2229a7a378d6833c7903fb28a6194d76e\\r\\nAuthorization: AWS4-HMAC-SHA256 Credential=REDACTED/20150210/us-east-1/dynamodb/aws4_request, SignedHeaders=content-type;host;user-agent;x-amz-content-sha256;x-amz-date;x-amz-target, Signature=475ba875de4e6f35236c913738ee638af0aa3545b98866f89473adb06fcea682\\r\\nContent-Length: 762\\r\\nAccept: /\\r\\n\\r\\n\"\n-> \"HTTP/1.1 400 Bad Request\\r\\n\"\n-> \"x-amzn-RequestId: I9HPQ7ISHMHLDA5G31PU8JDIKBVV4KQNSO5AEMVJF66Q9ASUAAJG\\r\\n\"\n-> \"x-amz-crc32: 2965254985\\r\\n\"\n-> \"Content-Type: application/x-amz-json-1.0\\r\\n\"\n-> \"Content-Length: 143\\r\\n\"\n-> \"Date: Tue, 10 Feb 2015 11:01:15 GMT\\r\\n\"\n-> \"\\r\\n\"\nreading 143 bytes...\n-> \"{\\\"__type\\\":\\\"com.amazon.coral.validate#ValidationException\\\",\\\"message\\\":\\\"The paramater 'TableName' is required but was not present in the request\\\"}\"\nread 143 bytes\nConn keep-alive\n[Aws::DynamoDB::Client 400 0.348192 0 retries] create_table(table_name:\"test_cc_events\",attribute_definitions:[{attribute_name:\"resource_id_event\",attribute_type:\"S\"},{attribute_name:\"timestamp\",attribute_type:\"N\"},{attribute_name:\"organisation_id\",attribute_type:\"N\"}],key_schema:[{attribute_name:\"resource_id_event\",key_type:\"HASH\"},{attribute_name:\"timestamp\",key_type:\"RANGE\"}],provisioned_throughput:{read_capacity_units:1,write_capacity_units:1},global_secondary_indexes:[{index_name:\"gsi_orgid_timestamp\",key_schema:[{attribute_name:\"organisation_id\",key_type:\"HASH\"},{attribute_name:\"timestamp\",key_type:\"RANGE\"}],projection:{projection_type:\"INCLUDE\",non_key_attributes:[\"_class\"]},provisioned_throughput:{read_capacity_units:1,write_capacity_units:1}}]) Aws::DynamoDB::Errors::ValidationException The paramater 'TableName' is required but was not present in the request\nAws::DynamoDB::Errors::ValidationException: The paramater 'TableName' is required but was not present in the request\n    from /geome/releases/d98388511cf5209a6d4e58bf34901899517bcd04/vendor/bundle/ruby/2.1.0/gems/aws-sdk-core-2.0.22/lib/seahorse/client/plugins/raise_response_errors.rb:15:in call'\n    from /geome/releases/d98388511cf5209a6d4e58bf34901899517bcd04/vendor/bundle/ruby/2.1.0/gems/aws-sdk-core-2.0.22/lib/aws-sdk-core/plugins/response_paging.rb:10:incall'\n    from /geome/releases/d98388511cf5209a6d4e58bf34901899517bcd04/vendor/bundle/ruby/2.1.0/gems/aws-sdk-core-2.0.22/lib/seahorse/client/request.rb:70:in send_request'\n    from /geome/releases/d98388511cf5209a6d4e58bf34901899517bcd04/vendor/bundle/ruby/2.1.0/gems/aws-sdk-core-2.0.22/lib/seahorse/client/base.rb:215:inblock (2 levels) in define_operation_methods'\n    from /geome/releases/d98388511cf5209a6d4e58bf34901899517bcd04/lib/dynamodb/driver.rb:29:in create_table'\n    from /geome/releases/d98388511cf5209a6d4e58bf34901899517bcd04/db/dynamodb_schema.rb:65:increate_events_table'\n    from (irb):1\n    from /geome/releases/d98388511cf5209a6d4e58bf34901899517bcd04/vendor/bundle/ruby/2.1.0/gems/railties-4.2.0/lib/rails/commands/console.rb:110:in start'\n    from /geome/releases/d98388511cf5209a6d4e58bf34901899517bcd04/vendor/bundle/ruby/2.1.0/gems/railties-4.2.0/lib/rails/commands/console.rb:9:instart'\n    from /geome/releases/d98388511cf5209a6d4e58bf34901899517bcd04/vendor/bundle/ruby/2.1.0/gems/railties-4.2.0/lib/rails/commands/commands_tasks.rb:68:in console'\n    from /geome/releases/d98388511cf5209a6d4e58bf34901899517bcd04/vendor/bundle/ruby/2.1.0/gems/railties-4.2.0/lib/rails/commands/commands_tasks.rb:39:inrun_command!'\n    from /geome/releases/d98388511cf5209a6d4e58bf34901899517bcd04/vendor/bundle/ruby/2.1.0/gems/railties-4.2.0/lib/rails/commands.rb:17:in <top (required)>'\n    from bin/rails:4:inrequire'\n    from bin/rails:4:in `'\nirb(main):002:0>\n```\n. Got it. If I call the API as follows then the error goes away. Are the param names not being converted correctly?\n```\nirb(main):004:0> client.describe_table('TableName'=>'test_cc_events')\nopening connection to dynamodb.eu-west-1.amazonaws.com:443...\nopened\nstarting SSL for dynamodb.eu-west-1.amazonaws.com:443...\nSSL established\n<- \"POST / HTTP/1.1\\r\\nContent-Type: application/x-amz-json-1.0\\r\\nAccept-Encoding: \\r\\nUser-Agent: aws-sdk-ruby2/2.0.22 ruby/2.1.5 x86_64-linux\\r\\nX-Amz-Target: DynamoDB_20120810.DescribeTable\\r\\nX-Amz-Date: 20150210T111440Z\\r\\nHost: dynamodb.eu-west-1.amazonaws.com\\r\\nX-Amz-Content-Sha256: 174fd624232807655698d4c1b836fa07df4a1283e012bc3491b87f9672382831\\r\\nAuthorization: AWS4-HMAC-SHA256 Credential=REDACTED/20150210/eu-west-1/dynamodb/aws4_request, SignedHeaders=content-type;host;user-agent;x-amz-content-sha256;x-amz-date;x-amz-target, Signature=796d4d5e44996ebd9b1fc571aa56ccc938a2f6856b0eb7ffcc0b3895f695bd9a\\r\\nContent-Length: 30\\r\\nAccept: /\\r\\n\\r\\n\"\n-> \"HTTP/1.1 200 OK\\r\\n\"\n-> \"x-amzn-RequestId: 6R380JINGS5QBRF1C02KDPNAANVV4KQNSO5AEMVJF66Q9ASUAAJG\\r\\n\"\n-> \"x-amz-crc32: 3831511604\\r\\n\"\n-> \"Content-Type: application/x-amz-json-1.0\\r\\n\"\n-> \"Content-Length: 1040\\r\\n\"\n-> \"Date: Tue, 10 Feb 2015 11:13:38 GMT\\r\\n\"\n-> \"\\r\\n\"\nreading 1040 bytes...\n-> \"{\\\"Table\\\":{\\\"AttributeDefinitions\\\":[{\\\"AttributeName\\\":\\\"organisation_id\\\",\\\"AttributeType\\\":\\\"N\\\"},{\\\"AttributeName\\\":\\\"resource_id_event\\\",\\\"AttributeType\\\":\\\"S\\\"},{\\\"AttributeName\\\":\\\"timestamp\\\",\\\"AttributeType\\\":\\\"N\\\"}],\\\"CreationDateTime\\\":1.421443377033E9,\\\"GlobalSecondaryIndexes\\\":[{\\\"IndexName\\\":\\\"gsi_orgid_timestamp\\\",\\\"IndexSizeBytes\\\":1055808,\\\"IndexStatus\\\":\\\"ACTIVE\\\",\\\"ItemCount\\\":10979,\\\"KeySchema\\\":[{\\\"AttributeName\\\":\\\"organisation_id\\\",\\\"KeyType\\\":\\\"HASH\\\"},{\\\"AttributeName\\\":\\\"timestamp\\\",\\\"KeyType\\\":\\\"RANGE\\\"}],\\\"Projection\\\":{\\\"NonKeyAttributes\\\":[\\\"_class\\\"],\\\"ProjectionType\\\":\\\"INCLUDE\\\"},\\\"ProvisionedThroughput\\\":{\\\"LastDecreaseDateTime\\\":1.422099472762E9,\\\"NumberOfDecreasesToday\\\":0,\\\"ReadCapacityUnits\\\":1,\\\"WriteCapacityUnits\\\":1}}],\\\"ItemCount\\\":10979,\\\"KeySchema\\\":[{\\\"AttributeName\\\":\\\"resource_id_event\\\",\\\"KeyType\\\":\\\"HASH\\\"},{\\\"AttributeName\\\":\\\"timestamp\\\",\\\"KeyType\\\":\\\"RANGE\\\"}],\\\"ProvisionedThroughput\\\":{\\\"LastDecreaseDateTime\\\":1.422099472713E9,\\\"NumberOfDecreasesToday\\\":0,\\\"ReadCapacityUnits\\\":1,\\\"WriteCapacityUnits\\\":1},\\\"TableName\\\":\\\"test_cc_events\\\",\\\"TableSizeBytes\\\":4262581,\\\"TableStatus\\\":\\\"ACTIVE\\\"}}\"\nread 1040 bytes\nConn keep-alive\n[Aws::DynamoDB::Client 200 0.025969 0 retries] describe_table(\"TableName\"=>\"test_cc_events\")\n=> {\"Table\"=>\n  {\"AttributeDefinitions\"=>\n    [{\"AttributeName\"=>\"organisation_id\", \"AttributeType\"=>\"N\"},\n``\n. And to insult to injury, specifyingsimple_json: false` in the dynamodb client config gives you this:\n[6] cloudchief \u00bb  client.describe_table('TableName'=>'cc_events');\nArgumentError: parameter validator found 2 errors:\n  - missing required parameter params[:table_name]\n  - unexpected value at params[\"TableName\"]\nfrom /usr/local/ruby/ruby-2.1.5/lib/ruby/gems/2.1.0/gems/aws-sdk-core-2.0.22/lib/seahorse/client/param_validator.rb:24:in `validate!'\nMy workaround for now is to change table_name: 'blah' into 'TableName' => 'blah' when hitting the API. Be nice to fix this inconsistency in the SDK though.\n. I think this is the simple_json: parameter. It must be false if you want to adopt the parameter conventions in the docs.\n. I started seeing a significant memory leak after upgrading to v2, but I also upgraded to ruby2.2 and a bunch of other stuff at the same time, so take that observation with a pinch of salt. It's on my list to investigate further.\n. Hmmm that's odd. OK here's the error, which includes output from the code below (excuse the mess: you can see the working version in the stuff that's commented out):\n``` ruby\n    # Note AWS API limit of 25 puts per request so we batch the put\n    # and add a delay between batches\n    # TODO: limit of 1MB per request in total not enforced here\n    def self.put_items(table_name, items)\n      __start_time = Time.now if DEBUG\n      in_batches_of_25(items) do |items_batch, last_iteration|\n        put_requests = items_batch.map do |item|\n          {\n            put_request: {\n              item: item\n            }\n          }\n        end\n        request_items = {\n          table_name => put_requests\n        }\n        loop do\n          response = client.batch_write_item(request_items: request_items)\n          sleep NICENESS_PAUSE unless last_iteration\n      break if response.unprocessed_items.size == 0\n\nrequest_items = response.unprocessed_items\nrequest_items = unprocessed_items_to_request_items(response.unprocessed_items)\nRails.logger.info(\"request_items\")\nRails.logger.info(request_items)\n      sleep NICENESS_PAUSE  * 2  # sleep again: we're not managing to store all items\n    end\n  end\nensure\n  ::Rails.logger.debug(\"put_items #{table_name} (#{items.length}): #{(Time.now() - __start_time)*1000}ms\") if DEBUG\nend\n\ndef self.unprocessed_items_to_request_items(unprocessed_items)\nunprocessed_items.as_json.deep_transform_keys do |key|\nif %w{ item put_request n s b ss ns bs m l null bool delete_request }.include? key\nkey.to_sym\nelse\nkey\nend\nend\nend\n```\n2015-05-15T08:03:48.914Z 23049 TID-ox45wrunc CreateHistoricGroupMetricsWorker JID-25a46a9675cc7fc03ec44445 INFO: request_items\n2015-05-15T08:03:48.914Z 23049 TID-ox45wrunc CreateHistoricGroupMetricsWorker JID-25a46a9675cc7fc03ec44445 INFO: {\"test_cc_group_metrics\"=>[#<struct put_request=#<struct item={\"timestamp\"=>#<struct s=nil, n=\"1431406800000.0\", b=nil, ss=nil, ns=nil, bs=nil, m=nil, l=nil, null=nil, bool=nil>, \"_class\"=>#<struct s=\"GroupMetric\", n=nil, b=nil, ss=nil, ns=nil, bs=nil, m=nil, l=nil, null=nil, bool=nil>, \"interval\"=>#<struct s=nil, n=\"900\", b=nil, ss=nil, ns=nil, bs=nil, m=nil, l=nil, null=nil, bool=nil>, \"group_id\"=>#<struct s=nil, n=\"73\", b=nil, ss=nil, ns=nil, bs=nil, m=nil, l=nil, null=nil, bool=nil>, \"group_id_interval\"=>#<struct s=\"73:900\", n=nil, b=nil, ss=nil, ns=nil, bs=nil, m=nil, l=nil, null=nil, bool=nil>, \"organisation_id\"=>#<struct s=nil, n=\"4\", b=nil, ss=nil, ns=nil, bs=nil, m=nil, l=nil, null=nil, bool=nil>, \"config_resource_statistics\"=>#<struct s=\"{\\\"1\\\":{\\\"probe\\\":{\\\"l\\\":{\\\"Connect\\\":{\\\"Unit\\\":\\\"ms\\\",\\\"Avg\\\":58},\\\"FirstByte\\\":{\\\"Unit\\\":\\\"ms\\\",\\\"Avg\\\":216},\\\"Transfer\\\":{\\\"Unit\\\":\\\"ms\\\",\\\"Avg\\\":1},\\\"Total\\\":{\\\"Unit\\\":\\\"ms\\\",\\\"Avg\\\":275}},\\\"h\\\":{\\\"Unit\\\":\\\"%\\\",\\\"Avg\\\":99.979},\\\"u\\\":{\\\"Unit\\\":\\\"%\\\",\\\"Avg\\\":100.0},\\\"s\\\":{\\\"2XX\\\":{\\\"Unit\\\":\\\"\\\",\\\"Avg\\\":null},\\\"3XX\\\":{\\\"Unit\\\":\\\"\\\",\\\"Avg\\\":null},\\\"4XX\\\":{\\\"Unit\\\":\\\"\\\",\\\"Avg\\\":null},\\\"5XX\\\":{\\\"Unit\\\":\\\"\\\",\\\"Avg\\\":null}}}}}\", n=nil, b=nil, ss=nil, ns=nil, bs=nil, m=nil, l=nil, null=nil, bool=nil>}>, delete_request=nil>, #<struct put_request=#<struct item={\"timestamp\"=>#<struct s=nil, n=\"1431407700000.0\", b=nil, ss=nil, ns=nil, bs=nil, m=nil, l=nil, null=nil, bool=nil>, \"_class\"=>#<struct s=\"GroupMetric\", n=nil, b=nil, ss=nil, ns=nil, bs=nil, m=nil, l=nil, null=nil, bool=nil>, \"interval\"=>#<struct s=nil, n=\"900\", b=nil, ss=nil, ns=nil, bs=nil, m=nil, l=nil, null=nil, bool=nil>, \"group_id\"=>#<struct s=nil, n=\"73\", b=nil, ss=nil, ns=nil, bs=nil, m=nil, l=nil, null=nil, bool=nil>, \"group_id_interval\"=>#<struct s=\"73:900\", n=nil, b=nil, ss=nil, ns=nil, bs=nil, m=nil, l=nil, null=nil, bool=nil>, \"organisation_id\"=>#<struct s=nil, n=\"4\", b=nil, ss=nil, ns=nil, bs=nil, m=nil, l=nil, null=nil, bool=nil>, \"config_resource_statistics\"=>#<struct s=\"{\\\"1\\\":{\\\"probe\\\":{\\\"l\\\":{\\\"Connect\\\":{\\\"Unit\\\":\\\"ms\\\",\\\"Avg\\\":61},\\\"FirstByte\\\":{\\\"Unit\\\":\\\"ms\\\",\\\"Avg\\\":224},\\\"Transfer\\\":{\\\"Unit\\\":\\\"ms\\\",\\\"Avg\\\":0},\\\"Total\\\":{\\\"Unit\\\":\\\"ms\\\",\\\"Avg\\\":285}},\\\"h\\\":{\\\"Unit\\\":\\\"%\\\",\\\"Avg\\\":99.983},\\\"u\\\":{\\\"Unit\\\":\\\"%\\\",\\\"Avg\\\":100.0},\\\"s\\\":{\\\"2XX\\\":{\\\"Unit\\\":\\\"\\\",\\\"Avg\\\":null},\\\"3XX\\\":{\\\"Unit\\\":\\\"\\\",\\\"Avg\\\":null},\\\"4XX\\\":{\\\"Unit\\\":\\\"\\\",\\\"Avg\\\":null},\\\"5XX\\\":{\\\"Unit\\\":\\\"\\\",\\\"Avg\\\":null}}}}}\", n=nil, b=nil, ss=nil, ns=nil, bs=nil, m=nil, l=nil, null=nil, bool=nil>}>, delete_request=nil>, #<struct put_request=#<struct item={\"timestamp\"=>#<struct s=nil, n=\"1431408600000.0\", b=nil, ss=nil, ns=nil, bs=nil, m=nil, l=nil, null=nil, bool=nil>, \"_class\"=>#<struct s=\"GroupMetric\", n=nil, b=nil, ss=nil, ns=nil, bs=nil, m=nil, l=nil, null=nil, bool=nil>, \"interval\"=>#<struct s=nil, n=\"900\", b=nil, ss=nil, ns=nil, bs=nil, m=nil, l=nil, null=nil, bool=nil>, \"group_id\"=>#<struct s=nil, n=\"73\", b=nil, ss=nil, ns=nil, bs=nil, m=nil, l=nil, null=nil, bool=nil>, \"group_id_interval\"=>#<struct s=\"73:900\", n=nil, b=nil, ss=nil, ns=nil, bs=nil, m=nil, l=nil, null=nil, bool=nil>, \"organisation_id\"=>#<struct s=nil, n=\"4\", b=nil, ss=nil, ns=nil, bs=nil, m=nil, l=nil, null=nil, bool=nil>, \"config_resource_statistics\"=>#<struct s=\"{\\\"1\\\":{\\\"probe\\\":{\\\"l\\\":{\\\"Connect\\\":{\\\"Unit\\\":\\\"ms\\\",\\\"Avg\\\":56},\\\"FirstByte\\\":{\\\"Unit\\\":\\\"ms\\\",\\\"Avg\\\":228},\\\"Transfer\\\":{\\\"Unit\\\":\\\"ms\\\",\\\"Avg\\\":0},\\\"Total\\\":{\\\"Unit\\\":\\\"ms\\\",\\\"Avg\\\":284}},\\\"h\\\":{\\\"Unit\\\":\\\"%\\\",\\\"Avg\\\":99.98699999999999},\\\"u\\\":{\\\"Unit\\\":\\\"%\\\",\\\"Avg\\\":100.0},\\\"s\\\":{\\\"2XX\\\":{\\\"Unit\\\":\\\"\\\",\\\"Avg\\\":null},\\\"3XX\\\":{\\\"Unit\\\":\\\"\\\",\\\"Avg\\\":null},\\\"4XX\\\":{\\\"Unit\\\":\\\"\\\",\\\"Avg\\\":null},\\\"5XX\\\":{\\\"Unit\\\":\\\"\\\",\\\"Avg\\\":null}}}}}\", n=nil, b=nil, ss=nil, ns=nil, bs=nil, m=nil, l=nil, null=nil, bool=nil>}>, delete_request=nil>]}\n2015-05-15T08:03:49.931Z 23049 TID-ox45wrunc CreateHistoricGroupMetricsWorker JID-25a46a9675cc7fc03ec44445 ERROR: parameter validator found 3 errors:\n  - expected params[:request_items][\"test_cc_group_metrics\"][0] to be a hash\n  - expected params[:request_items][\"test_cc_group_metrics\"][1] to be a hash\n  - expected params[:request_items][\"test_cc_group_metrics\"][2] to be a hash\n2015-05-15T08:03:49.931Z 23049 TID-ox45wrunc CreateHistoricGroupMetricsWorker JID-25a46a9675cc7fc03ec44445 ERROR: parameter validator found 3 errors:\n  - expected params[:request_items][\"test_cc_group_metrics\"][0] to be a hash\n  - expected params[:request_items][\"test_cc_group_metrics\"][1] to be a hash\n  - expected params[:request_items][\"test_cc_group_metrics\"][2] to be a hash\n2015-05-15T08:03:49.931Z 23049 TID-ox45wrunc CreateHistoricGroupMetricsWorker JID-25a46a9675cc7fc03ec44445 ERROR: /geome/releases/93ebf9aea0a4de93b442d41170a42a23877ec91e/vendor/bundle/ruby/2.2.0/gems/aws-sdk-core-2.0.41/lib/seahorse/client/param_validator.rb:24:in `validate!'\n/geome/releases/93ebf9aea0a4de93b442d41170a42a23877ec91e/vendor/bundle/ruby/2.2.0/gems/aws-sdk-core-2.0.41/lib/seahorse/client/param_validator.rb:9:in `validate!'\n/geome/releases/93ebf9aea0a4de93b442d41170a42a23877ec91e/vendor/bundle/ruby/2.2.0/gems/aws-sdk-core-2.0.41/lib/seahorse/client/plugins/param_validation.rb:21:in `call'\n/geome/releases/93ebf9aea0a4de93b442d41170a42a23877ec91e/vendor/bundle/ruby/2.2.0/gems/aws-sdk-core-2.0.41/lib/seahorse/client/plugins/raise_response_errors.rb:14:in `call'\n/geome/releases/93ebf9aea0a4de93b442d41170a42a23877ec91e/vendor/bundle/ruby/2.2.0/gems/aws-sdk-core-2.0.41/lib/aws-sdk-core/plugins/response_paging.rb:10:in `call'\n/geome/releases/93ebf9aea0a4de93b442d41170a42a23877ec91e/vendor/bundle/ruby/2.2.0/gems/aws-sdk-core-2.0.41/lib/seahorse/client/plugins/response_target.rb:18:in `call'\n/geome/releases/93ebf9aea0a4de93b442d41170a42a23877ec91e/vendor/bundle/ruby/2.2.0/gems/aws-sdk-core-2.0.41/lib/seahorse/client/request.rb:70:in `send_request'\n/geome/releases/93ebf9aea0a4de93b442d41170a42a23877ec91e/vendor/bundle/ruby/2.2.0/gems/aws-sdk-core-2.0.41/lib/seahorse/client/base.rb:216:in `block (2 levels) in define_operation_methods'\n/geome/releases/93ebf9aea0a4de93b442d41170a42a23877ec91e/lib/dynamodb/driver.rb:73:in `block (2 levels) in put_items'\n/geome/releases/93ebf9aea0a4de93b442d41170a42a23877ec91e/lib/dynamodb/driver.rb:72:in `loop'\n/geome/releases/93ebf9aea0a4de93b442d41170a42a23877ec91e/lib/dynamodb/driver.rb:72:in `block in put_items'\n/geome/releases/93ebf9aea0a4de93b442d41170a42a23877ec91e/lib/dynamodb/driver.rb:52:in `block in in_batches_of_25'\n. It's in the log output above: you have to scroll to the right a bit. Let me know if that's enough?\n. Ah, my config is:\nconvert_params: false,\n    simple_attributes: false,\n    simple_json: false,\nMostly because that was the easiest way to migrate from the v1 SDK if I remember right.\nI'll try that workaround out: looks way better than my hack, even if it's an interim thing. :-)\n. Fantastic, thanks for that. (I'll take a look at my use of the param conversion a bit later this week, so thanks for the pointer there too.)\n. ",
    "arelenglish": "Sorry, wrong repo.\n. ",
    "tbhi": "Turns out my credentials file was in dos format.\n. ",
    "coveralls": "\nCoverage decreased (-0.04%) to 94.98% when pulling 4e18ac80a44eb7bf53d1180e9f40a41ce12b029b on jimcroft:master into f5f568048eb134f86b4a2552bbb77c28189766b9 on aws:master.\n. \nCoverage increased (+0.01%) to 95.04% when pulling 3406c0fb9309f11b2405c2cc722b7cbca90bbc2e on stan3:credentials_ini into f5f568048eb134f86b4a2552bbb77c28189766b9 on aws:master.\n. \nCoverage decreased (-0.01%) to 95.02% when pulling 01f98e35eb6fe2bd4285cbb5cea52c653f06f859 on awendt:patch-1 into a45e0a996ace3301253c2067b6edb556eeb3f3f3 on aws:master.\n. \nCoverage increased (+0.03%) to 95.14% when pulling bd001d467788d357b83783a5faccfe55535d4c68 on Tim-B:doc_base_attr into 17053fb71dc8673c78b8a6a10bdbabbbedd92943 on aws:master.\n. \nCoverage decreased (-0.02%) to 95.09% when pulling bd001d467788d357b83783a5faccfe55535d4c68 on Tim-B:doc_base_attr into 17053fb71dc8673c78b8a6a10bdbabbbedd92943 on aws:master.\n. \nCoverage decreased (-0.0%) to 95.11% when pulling 90813f4e150dc8fec674488c14806ce72e43943e on kuon:doc into 17053fb71dc8673c78b8a6a10bdbabbbedd92943 on aws:master.\n. \nCoverage decreased (-0.0%) to 95.11% when pulling 90813f4e150dc8fec674488c14806ce72e43943e on kuon:doc into 17053fb71dc8673c78b8a6a10bdbabbbedd92943 on aws:master.\n. \nCoverage decreased (-0.02%) to 95.09% when pulling 90813f4e150dc8fec674488c14806ce72e43943e on kuon:doc into 17053fb71dc8673c78b8a6a10bdbabbbedd92943 on aws:master.\n. \nCoverage remained the same at 95.12% when pulling 9851defaf02b08f3eda94e6684bb54597290558e on omockler:elb-healthy-instance-waiter into 496c5fc6582d6113cdb8334906985b0ba628bbcc on aws:master.\n. \nCoverage increased (+0.01%) to 95.12% when pulling ccef21839f441e025efcd5febaed436a9d6ed145 on omockler:elb-healthy-instance-waiter into 496c5fc6582d6113cdb8334906985b0ba628bbcc on aws:master.\n. \nCoverage decreased (-0.04%) to 95.08% when pulling 63e41f90a9dc45220719d96716dfa1daabc822cf on omockler:elb-healthy-instance-waiter into 496c5fc6582d6113cdb8334906985b0ba628bbcc on aws:master.\n. \nCoverage decreased (-0.04%) to 95.08% when pulling 63e41f90a9dc45220719d96716dfa1daabc822cf on omockler:elb-healthy-instance-waiter into 496c5fc6582d6113cdb8334906985b0ba628bbcc on aws:master.\n. \nCoverage decreased (-0.01%) to 95.11% when pulling 63e41f90a9dc45220719d96716dfa1daabc822cf on omockler:elb-healthy-instance-waiter into 496c5fc6582d6113cdb8334906985b0ba628bbcc on aws:master.\n. \nCoverage increased (+0.05%) to 95.14% when pulling 21a66a078b76a8699d77ee9e2a1466b56ad49705 on matugm:master into be0446cb464d92a55987d41487a7762aff490bf2 on aws:master.\n. \nCoverage increased (+0.02%) to 95.12% when pulling 21a66a078b76a8699d77ee9e2a1466b56ad49705 on matugm:master into be0446cb464d92a55987d41487a7762aff490bf2 on aws:master.\n. \nCoverage increased (+0.07%) to 95.19% when pulling 4844b3ac2224bc83183bb8052e19e037da0fa643 on nitinmohan87:fix_placement_group_and_route_table_resources into 2b7a7e61aae0a74cb6b1ccf33f6e7962658adf09 on aws:master.\n. \nCoverage increased (+0.07%) to 95.19% when pulling 4844b3ac2224bc83183bb8052e19e037da0fa643 on nitinmohan87:fix_placement_group_and_route_table_resources into 2b7a7e61aae0a74cb6b1ccf33f6e7962658adf09 on aws:master.\n. \nCoverage increased (+0.01%) to 95.13% when pulling 4844b3ac2224bc83183bb8052e19e037da0fa643 on nitinmohan87:fix_placement_group_and_route_table_resources into 2b7a7e61aae0a74cb6b1ccf33f6e7962658adf09 on aws:master.\n. \nCoverage increased (+0.09%) to 95.22% when pulling dfca0b2b2eb3c6726a766fc719af38e76919940f on sqs-queue-poller into 035128af435402a1f9949cbb96c2d00283887faa on master.\n. \nCoverage increased (+0.07%) to 95.19% when pulling da255cd316d7ffd912f01b919bf104c2f565d609 on sns-message-verifier into 7bc5dfbd735dcec99378994f808dbf56901a3448 on master.\n. \nCoverage decreased (-0.0%) to 95.13% when pulling 37ef4d2a0c5b62ab350bbb1141a8408d112b02c0 on danielsiwiec:master into 7bc5dfbd735dcec99378994f808dbf56901a3448 on aws:master.\n. \nCoverage increased (+0.21%) to 95.15% when pulling 877e06d9432aee1dc9c4ea25b9053cb0bdae2801 on danielsiwiec:custom-waiter into f9e33b4c8b0a3e26e95c61bb84ae046bd9f47355 on aws:custom-waiter.\n. \nCoverage increased (+0.21%) to 95.15% when pulling fed12aa152fbed33bd42bf86131ea87b99ff12a1 on danielsiwiec:custom-waiter into f9e33b4c8b0a3e26e95c61bb84ae046bd9f47355 on aws:custom-waiter.\n. \nChanges Unknown when pulling 2740d640669d8cf58ca60056008848a2ce550869 on xml-sax-parsing into * on master*.\n. \nChanges Unknown when pulling 9e4cca76af8569759af3521177758e963a694d5c on rlishtaba:master into * on aws:master*.\n. \nChanges Unknown when pulling 8bb10afd3a0bbb6e074192f37bc6c29f958a6ccd on baweaver:patch-1 into * on aws:master*.\n. \nChanges Unknown when pulling 4a3fd4b9e3d6886a5652f142e994958eaa2918c2 on presigned-post into * on master*.\n. \nChanges Unknown when pulling b48216c9991cc412d3906e2fc5386e9fd8c7185f on ajcantu:patch-1 into * on aws:master*.\n. \nChanges Unknown when pulling 6ead08644a5ce7f045101db3521a1b9692284f0b on exists-methods into * on master*.\n. \nCoverage decreased (-0.05%) to 95.63% when pulling 686450f7122b496f41323f6ae8035de811cad71b on morisan:patch-1 into d61484bd732fbba4bb16f0d124bb1e6df618e7fe on aws:master.\n. \nCoverage decreased (-0.05%) to 95.63% when pulling 686450f7122b496f41323f6ae8035de811cad71b on morisan:patch-1 into d61484bd732fbba4bb16f0d124bb1e6df618e7fe on aws:master.\n. \nCoverage decreased (-0.05%) to 95.63% when pulling 686450f7122b496f41323f6ae8035de811cad71b on morisan:patch-1 into d61484bd732fbba4bb16f0d124bb1e6df618e7fe on aws:master.\n. \nCoverage decreased (-0.05%) to 95.63% when pulling 686450f7122b496f41323f6ae8035de811cad71b on morisan:patch-1 into d61484bd732fbba4bb16f0d124bb1e6df618e7fe on aws:master.\n. \nCoverage decreased (-0.05%) to 95.63% when pulling 686450f7122b496f41323f6ae8035de811cad71b on morisan:patch-1 into d61484bd732fbba4bb16f0d124bb1e6df618e7fe on aws:master.\n. \nCoverage decreased (-0.05%) to 95.63% when pulling 686450f7122b496f41323f6ae8035de811cad71b on morisan:patch-1 into d61484bd732fbba4bb16f0d124bb1e6df618e7fe on aws:master.\n. \nCoverage decreased (-0.05%) to 95.63% when pulling 686450f7122b496f41323f6ae8035de811cad71b on morisan:patch-1 into d61484bd732fbba4bb16f0d124bb1e6df618e7fe on aws:master.\n. \nCoverage decreased (-0.05%) to 95.63% when pulling 686450f7122b496f41323f6ae8035de811cad71b on morisan:patch-1 into d61484bd732fbba4bb16f0d124bb1e6df618e7fe on aws:master.\n. \nChanges Unknown when pulling 8cd5394b7886520b06780e3df336c7658eff0f88 on antohaby:patch-1 into * on aws:master*.\n. \nChanges Unknown when pulling 49267d8af8afa27ddea6d29b1e3a1016b46d9cb6 on fidothe:resource-from-basicobject into * on aws:master*.\n. \nChanges Unknown when pulling dbd3b7221927bf9ea89270675893a5a5e9ff5ed6 on wal:symbols_as_strings_attribute_value into * on aws:master*.\n. \nChanges Unknown when pulling 82cc564e3068f3b27a170af4b0e4ebd0c63b0fe3 on sferik:replace-multi_json-with-json into * on aws:master*.\n. ",
    "lwoggardner": "Thanks.  I think the RetryErrors plugin will be my solution, but from a quick test it looks like the retry_limit needs to be set explicitly for the plugin to be used.   If this is supposed to be defaulted to 3 automatically then perhaps that is bug.  I'll do some more testing.\n. Confirmed working as expected. My error setting the retry_limit in the config.\n. Yes, of course. The SDK may not even have the template to know that a given parameter is marked \"NoEcho\".\nThe feature sounds good - would suggest blacklist should be regexes - eg /password|no.*echo/i\nAlso, the filter does need to be CF aware in some respects because the filtering needs to occur deep into the parameters hash. (ie the request parameter is actually \"parameters\", whose value an array of hashes of the form { parameter_key: \"myNoEchoParameter\", parameter_value: \"topsecret\" } \n(BTW I assume the reason it is done that way is to fit the basic API, as it would be much preferable from  rubyist point of view to just pass the parameters as a straight hash)\nI will attempt to create a custom formatter to implement this.\n. ``` ruby\n@param [String] pattern The log format pattern should be a string\nand may contain substitutions.\n\n@option options [Regex] :mask_filter (/password|no.*echo/i)\nWhen summarizing hash values, any keys matching this mask filter\nwill have the value masked with \"*\"\nclass MaskingFormatter < Seahorse::Client::Logging::Formatter\n# regex to match against has key names\n attr_reader :mask_filter\ndef initialize(pattern, options = {})\n    super\n    @mask_filter = options[:mask_filter] || /password|no.*echo/i\n  end\nprivate\ndef summarize_string_hash(hash)\n    hash.map do |key,v|\n      masked_v = (key =~ mask_filter || ( key =~ /value/ && hash[key.gsub('value','key')] =~ mask_filter)) ? '**' : v\n      \"#{key.inspect}=>#{summarize_value(masked_v)}\"\n    end.join(\",\")\n  end\ndef summarize_symbol_hash(hash)\n    hash.map do |key,v|\n      masked_v = (key =~ mask_filter || ( key =~ /value/ && hash[key.to_s.gsub('value','key').to_sym] =~ mask_filter)) ? '*' : v\n      \"#{key}:#{summarize_value(masked_v)}\"\n    end.join(\",\")\n  end\nend\n```\n. Not sure how to \"vote\" for feature requests, so I'll just \"me too\" here.\n. The other point to note is that we are making these calls through a Squid Proxy (via CONNECT, there is no SSL termination on the proxy).\nI have horribly hacked around it with this...\nAws::Plugins::RetryErrors::ErrorInspector::NETWORKING_ERRORS << 'SSLErrorWaitReadable'\nwhich has unblocked me (I think).  May take a little while to generate the reproducible case, but I'll try and do that in the next week or two.\n. I think I've worked this out.\nAdding the error to the retryable list did not stop the errors appearing in my logs.\nHowever, trying to explicitly rescue it didn't work either.\nSo I'm pretty sure that it was being raised and handled by net/http or aws-sdk already, but when I raise my final exception the OpenSSL error is still in $! and is thus associated as the \"cause\" of my exception, and then getting logged when my program finished - leading me down this merry garden path.\nI haven't proved this yet either, but I'm pretty sure it was a non issue to begin with.\n. Workaround hackery?\nruby\nAws::Plugins::RetryErrors::ErrorInspector::NETWORKING_ERRORS << 'Net::HTTPBadResponse'\n. Its a local proxy. Reusing connection s through it is fine except after this S3 HEAD call\n. To be clear it is cntlm forwarding to the commercial corporate proxy forwarding to the real S3.\nIt may well be a proxy issue for reusing a connection in this scenario but I'm still blocked unless I monkey patch the workaround in.\nI'll run the test on a different proxy config.\n. Have decided to try an alternate approach. @cjyclaire  I have a stack_id and I'm trying to get just the name.  My (temporary) workaround is to monkey patch Aws::CloudFormation:;Stack  roughly like\nruby\nclass Aws::CloudFormation::Stack\n  def stack_name\n    name.start_with?('arn:') ? name.split('/')[1] : name\n  end\nend\n. Similar issue exists with Aws::EC2::KeyPair#key_name, it is just an alias to the constructor value #name, so previously where calling this would raise Aws::EC2::Errors::InvalidKeyPairNotFound it now just returns the string.\nruby\nbegin\n    ec2_resource.key_pair(input_value).key_name # in v3 this now returns input_value instead of raising \nrescue Aws::EC2::Errors::InvalidKeyPairNotFound => ec2_error\n   #if input_value key does not exist fall back to master_keyname\n   ec2_resource.key_pair(master_keyname).key_name\nend. Cloudformation's DescribeStacks api accepts either a stack_name or a stack id in the StackName input parameter. The output of that call has two separate return values \"StackName\" and \"StackId\" that produce the respective short and long output regardless of the format of the input.\nIt is not just being lenient - eg to find a deleted stack you must specify the full id arn, and in that case there is no way to resolve the short stack name using the Resource interface. \nThis problem may be unique to Cloudformation::Stack, or there may be other resources that can take a short name or a full arn as the input identifier (SQS SNS?), and in all cases I would expect the respective #xxx_name attribute to return the short value and #arn (or #stack_id in this case) to return the arn, regardless of the format of the input.\n(ARN construction/decomposition in the SDK is a whole other conversation, I suspect any significant app has a helper class trying to unravel the magic inconsistency of ARNs)\nIn addition to the ARN/short name issue, I'm not sure I see that aliasing one attribute to #name is actually simplifying things compared to mapping that attribute to the appropriate value in \"data\" like all the other attributes are anyway, and thus keeping compatibility.  (although I appreciate changing it back now you've gone GA with V3 is tricky too)\nWith the EC2:KeyPair example,  calling key_name after constructing the resource seemed like a logical way to test for existence.  I suppose calling #load would make sense too and be common across all resources.  (or is there a better way general to test existence - should all resources have an #exists? method?)\n(I'd be happy to do a more direct chat, but I believe we are on opposite sides of the planet- ap-southeast-2 is my home)\n. Is it because it is calling it \"options\" but the method is defined as \"*args\" so there is no matching parameter called \"options\" ?\n# @option options [required, Aws::CredentialProvider] :credentials\n...\ndef initialize(*args)\n. I'm still not seeing these docs on the Client initialize methods.\nhttps://docs.aws.amazon.com/sdk-for-ruby/v3/api/Aws/AutoScaling/Client.html#initialize-instance_method. Apologies.  I misinterpreted the error message - it means the value of :key is a Symbol not that the inner tag entry is :key rather than \"key\".\nSo a proper sample would be\n```ruby\n\n\ntags_as_hash = { mytag: 'myvalue' } #=> {:mytag=>\"myvalue\"}\naws_tags = tags_as_hash.map { |(k, v)| {key: k, value: v} } #=> [{:key=>:mytag, :value=>\"myvalue\"}]\nimage.create_tags(tags: aws_tags)\n\n\nArgumentError: expected :key to be a String, got Symbol\nfrom C:/tools/Ruby23-x64/lib/ruby/gems/2.3.0/gems/aws-sdk-ec2-1.13.0/lib/aws-sdk-ec2/tag.rb:264:in `extract_key'\nfrom C:/tools/Ruby23-x64/lib/ruby/gems/2.3.0/gems/aws-sdk-ec2-1.13.0/lib/aws-sdk-ec2/tag.rb:26:in `initialize'\nfrom C:/tools/Ruby23-x64/lib/ruby/gems/2.3.0/gems/aws-sdk-ec2-1.13.0/lib/aws-sdk-ec2/image.rb:372:in `new'\nfrom C:/tools/Ruby23-x64/lib/ruby/gems/2.3.0/gems/aws-sdk-ec2-1.13.0/lib/aws-sdk-ec2/image.rb:372:in `block in create_tags'\nfrom C:/tools/Ruby23-x64/lib/ruby/gems/2.3.0/gems/aws-sdk-ec2-1.13.0/lib/aws-sdk-ec2/image.rb:371:in `each'\nfrom C:/tools/Ruby23-x64/lib/ruby/gems/2.3.0/gems/aws-sdk-ec2-1.13.0/lib/aws-sdk-ec2/image.rb:371:in `create_tags'\n```\nAlthough this does work on V2 I can understand if it was just an accident that V2 accepted symbols here while V3 has stricter validation but it is a backwards compatibility issue nonetheless.\nFor me it is ok, I can just change my utility code (and move on to whatever fails next :-) ). Thanks (BTW  I finished my migration - no further V2 - V3 issues). ",
    "simonmorley": ":+1: \n. ",
    "SpencerBrown": "That is strange. A few days ago, my code broke because it was rescuing\nexceptions on Aws::EC2::Errors::ServiceError, and the SDK raised an\nexception due to a Network Instance not existing, and my rescue didn't\ncatch it. This was repeatable. I changed my code to rescue\nAws::EC2::Errors, and it immediately started working again.\nNow that I try to reproduce it, I'm not seeing that at all.\nThere may have been a fix in the latest SDK which I may have updated to.\nAnyway, thanks for looking into this so quickly.\nBTW which repo should I report issues on, now?  It wasn't clear so I opened\nan issue on both.\nOn Wed, Feb 18, 2015 at 12:52 AM, Trevor Rowe notifications@github.com\nwrote:\n\nI am unable to reproduce this. I sent an invalid request, rescuing the\nraised error. Here are its ancestors:\nerror.class.ancestors\n=> [Aws::EC2::Errors::InvalidInstanceIDMalformed,\n Aws::EC2::Errors::ServiceError,\n Aws::Errors::ServiceError,\n RuntimeError,\n StandardError,\n Exception,\n Object,\n JSON::Ext::Generator::GeneratorMethods::Object,\n PP::ObjectMixin,\n Kernel,\n BasicObject]\nYou should be able to rescue Aws::EC2::Errors::ServiceError or the even\nmore generic Aws::Errors::ServiceError.\n\nReply to this email directly or view it on GitHub\nhttps://github.com/aws/aws-sdk-ruby/issues/710#issuecomment-74821176.\n. \n",
    "bradleyd": "will do.\n. Same issue.  No messages and it keeps growing.  It has been running for 10 minutes.  Starts at 14MB.\nHere is a memory map of the process.  \nThanks for the help!\n``` bash\nAddress           Kbytes     RSS   Dirty Mode  Mapping\n0000000000400000       4       4       0 r-x-- ruby1.9.1\n0000000000600000       4       4       4 r---- ruby1.9.1\n0000000000601000       4       4       4 rw--- ruby1.9.1\n00000000007f8000   44436   44264   44264 rw---   [ anon ]\n00007fbfdc000000     132       4       4 rw---   [ anon ]\n00007fbfdc021000   65404       0       0 -----   [ anon ]\n00007fbfe2c67000      92      12       0 r-x-- CP932.so\n00007fbfe2c7e000    2044       0       0 ----- CP932.so\n00007fbfe2e7d000       4       4       4 r---- CP932.so\n00007fbfe2e7e000       4       4       4 rw--- CP932.so\n00007fbfe2e7f000      16      16       0 r-x-- cparse.so\n00007fbfe2e83000    2044       0       0 ----- cparse.so\n00007fbfe3082000       4       4       4 r---- cparse.so\n00007fbfe3083000       4       4       4 rw--- cparse.so\n00007fbfe3084000    2148     468       0 r-x-- nokogiri.so\n00007fbfe329d000    2044       0       0 ----- nokogiri.so\n00007fbfe349c000      32      32      32 r---- nokogiri.so\n00007fbfe34a4000      12      12      12 rw--- nokogiri.so\n00007fbfe34a7000       4       4       4 rw---   [ anon ]\n00007fbfe34a8000      92      52       0 r-x-- libresolv-2.19.so\n00007fbfe34bf000    2048       0       0 ----- libresolv-2.19.so\n00007fbfe36bf000       4       4       4 r---- libresolv-2.19.so\n00007fbfe36c0000       4       4       4 rw--- libresolv-2.19.so\n00007fbfe36c1000       8       4       4 rw---   [ anon ]\n00007fbfe36c3000      20      12       0 r-x-- libnss_dns-2.19.so\n00007fbfe36c8000    2044       0       0 ----- libnss_dns-2.19.so\n00007fbfe38c7000       4       4       4 r---- libnss_dns-2.19.so\n00007fbfe38c8000       4       4       4 rw--- libnss_dns-2.19.so\n00007fbfe38c9000      44      24       0 r-x-- libnss_files-2.19.so\n00007fbfe38d4000    2044       0       0 ----- libnss_files-2.19.so\n00007fbfe3ad3000       4       4       4 r---- libnss_files-2.19.so\n00007fbfe3ad4000       4       4       4 rw--- libnss_files-2.19.so\n00007fbfe3ad5000    1028      12      12 rw---   [ anon ]\n00007fbfe3bd6000      60      32       0 r-x-- bigdecimal.so\n00007fbfe3be5000    2044       0       0 ----- bigdecimal.so\n00007fbfe3de4000       4       4       4 r---- bigdecimal.so\n00007fbfe3de5000       4       4       4 rw--- bigdecimal.so\n00007fbfe3de6000      28      20       0 r-x-- generator.so\n00007fbfe3ded000    2048       0       0 ----- generator.so\n00007fbfe3fed000       4       4       4 r---- generator.so\n00007fbfe3fee000       4       4       4 rw--- generator.so\n00007fbfe3fef000       4       4       0 r-x-- utf_32le.so\n00007fbfe3ff0000    2044       0       0 ----- utf_32le.so\n00007fbfe41ef000       4       4       4 r---- utf_32le.so\n00007fbfe41f0000       4       4       4 rw--- utf_32le.so\n00007fbfe41f1000       4       4       0 r-x-- utf_32be.so\n00007fbfe41f2000    2044       0       0 ----- utf_32be.so\n00007fbfe43f1000       4       4       4 r---- utf_32be.so\n00007fbfe43f2000       4       4       4 rw--- utf_32be.so\n00007fbfe43f3000       4       4       0 r-x-- utf_16le.so\n00007fbfe43f4000    2048       0       0 ----- utf_16le.so\n00007fbfe45f4000       4       4       4 r---- utf_16le.so\n00007fbfe45f5000       4       4       4 rw--- utf_16le.so\n00007fbfe45f6000       4       4       0 r-x-- utf_16be.so\n00007fbfe45f7000    2048       0       0 ----- utf_16be.so\n00007fbfe47f7000       4       4       4 r---- utf_16be.so\n00007fbfe47f8000       4       4       4 rw--- utf_16be.so\n00007fbfe47f9000      24      20       0 r-x-- parser.so\n00007fbfe47ff000    2044       0       0 ----- parser.so\n00007fbfe49fe000       4       4       4 r---- parser.so\n00007fbfe49ff000       4       4       4 rw--- parser.so\n00007fbfe4a00000     184      36       0 r-x-- date_core.so\n00007fbfe4a2e000    2048       0       0 ----- date_core.so\n00007fbfe4c2e000       4       4       4 r---- date_core.so\n00007fbfe4c2f000       4       4       4 rw--- date_core.so\n00007fbfe4c30000       4       0       0 rw---   [ anon ]\n00007fbfe4c31000      20      16       0 r-x-- strscan.so\n00007fbfe4c36000    2044       0       0 ----- strscan.so\n00007fbfe4e35000       4       4       4 r---- strscan.so\n00007fbfe4e36000       4       4       4 rw--- strscan.so\n00007fbfe4e37000     124      80       0 r-x-- libyaml-0.so.2.0.2\n00007fbfe4e56000    2044       0       0 ----- libyaml-0.so.2.0.2\n00007fbfe5055000       4       4       4 r---- libyaml-0.so.2.0.2\n00007fbfe5056000       4       4       4 rw--- libyaml-0.so.2.0.2\n00007fbfe5057000      24      20       0 r-x-- psych.so\n00007fbfe505d000    2044       0       0 ----- psych.so\n00007fbfe525c000       4       4       4 r---- psych.so\n00007fbfe525d000       4       4       4 rw--- psych.so\n00007fbfe525e000      24      16       0 r-x-- pathname.so\n00007fbfe5264000    2044       0       0 ----- pathname.so\n00007fbfe5463000       4       4       4 r---- pathname.so\n00007fbfe5464000       4       4       4 rw--- pathname.so\n00007fbfe5465000      96       8       0 r-x-- libz.so.1.2.8\n00007fbfe547d000    2044       0       0 ----- libz.so.1.2.8\n00007fbfe567c000       4       4       4 r---- libz.so.1.2.8\n00007fbfe567d000       4       4       4 rw--- libz.so.1.2.8\n00007fbfe567e000      48      24       0 r-x-- zlib.so\n00007fbfe568a000    2044       0       0 ----- zlib.so\n00007fbfe5889000       4       4       4 r---- zlib.so\n00007fbfe588a000       4       4       4 rw--- zlib.so\n00007fbfe588b000     132     100       0 r-x-- socket.so\n00007fbfe58ac000    2048       0       0 ----- socket.so\n00007fbfe5aac000       4       4       4 r---- socket.so\n00007fbfe5aad000       4       4       4 rw--- socket.so\n00007fbfe5aae000       4       4       0 r-x-- fcntl.so\n00007fbfe5aaf000    2044       0       0 ----- fcntl.so\n00007fbfe5cae000       4       4       4 r---- fcntl.so\n00007fbfe5caf000       4       4       4 rw--- fcntl.so\n00007fbfe5cb0000      24      20       0 r-x-- stringio.so\n00007fbfe5cb6000    2044       0       0 ----- stringio.so\n00007fbfe5eb5000       4       4       4 r---- stringio.so\n00007fbfe5eb6000       4       4       4 rw--- stringio.so\n00007fbfe5eb7000      12      12       0 r-x-- digest.so\n00007fbfe5eba000    2044       0       0 ----- digest.so\n00007fbfe60b9000       4       4       4 r---- digest.so\n00007fbfe60ba000       4       4       4 rw--- digest.so\n00007fbfe60bb000    1732     976       0 r-x-- libcrypto.so.1.0.0\n00007fbfe626c000    2044       0       0 ----- libcrypto.so.1.0.0\n00007fbfe646b000     108     108     108 r---- libcrypto.so.1.0.0\n00007fbfe6486000      44      44      44 rw--- libcrypto.so.1.0.0\n00007fbfe6491000      16      16      16 rw---   [ anon ]\n00007fbfe6495000     336     220       0 r-x-- libssl.so.1.0.0\n00007fbfe64e9000    2048       0       0 ----- libssl.so.1.0.0\n00007fbfe66e9000      12      12      12 r---- libssl.so.1.0.0\n00007fbfe66ec000      28      28      28 rw--- libssl.so.1.0.0\n00007fbfe66f3000     292     224       0 r-x-- openssl.so\n00007fbfe673c000    2048       0       0 ----- openssl.so\n00007fbfe693c000       4       4       4 r---- openssl.so\n00007fbfe693d000       8       8       8 rw--- openssl.so\n00007fbfe693f000       4       4       4 rw---   [ anon ]\n00007fbfe6940000       8       8       0 r-x-- transdb.so\n00007fbfe6942000    2048       0       0 ----- transdb.so\n00007fbfe6b42000       4       4       4 r---- transdb.so\n00007fbfe6b43000       4       4       4 rw--- transdb.so\n00007fbfe6b44000       8       8       0 r-x-- encdb.so\n00007fbfe6b46000    2044       0       0 ----- encdb.so\n00007fbfe6d45000       4       4       4 r---- encdb.so\n00007fbfe6d46000       4       4       4 rw--- encdb.so\n00007fbfe6d47000    2852      48       0 r---- locale-archive\n00007fbfe7010000    1044      64       0 r-x-- libm-2.19.so\n00007fbfe7115000    2044       0       0 ----- libm-2.19.so\n00007fbfe7314000       4       4       4 r---- libm-2.19.so\n00007fbfe7315000       4       4       4 rw--- libm-2.19.so\n00007fbfe7316000      36       4       0 r-x-- libcrypt-2.19.so\n00007fbfe731f000    2048       0       0 ----- libcrypt-2.19.so\n00007fbfe751f000       4       4       4 r---- libcrypt-2.19.so\n00007fbfe7520000       4       4       4 rw--- libcrypt-2.19.so\n00007fbfe7521000     184       0       0 rw---   [ anon ]\n00007fbfe754f000      12       8       0 r-x-- libdl-2.19.so\n00007fbfe7552000    2044       0       0 ----- libdl-2.19.so\n00007fbfe7751000       4       4       4 r---- libdl-2.19.so\n00007fbfe7752000       4       4       4 rw--- libdl-2.19.so\n00007fbfe7753000      28      16       0 r-x-- librt-2.19.so\n00007fbfe775a000    2044       0       0 ----- librt-2.19.so\n00007fbfe7959000       4       4       4 r---- librt-2.19.so\n00007fbfe795a000       4       4       4 rw--- librt-2.19.so\n00007fbfe795b000     100      76       0 r-x-- libpthread-2.19.so\n00007fbfe7974000    2044       0       0 ----- libpthread-2.19.so\n00007fbfe7b73000       4       4       4 r---- libpthread-2.19.so\n00007fbfe7b74000       4       4       4 rw--- libpthread-2.19.so\n00007fbfe7b75000      16       4       4 rw---   [ anon ]\n00007fbfe7b79000    1772     656       0 r-x-- libc-2.19.so\n00007fbfe7d34000    2048       0       0 ----- libc-2.19.so\n00007fbfe7f34000      16      16      16 r---- libc-2.19.so\n00007fbfe7f38000       8       8       8 rw--- libc-2.19.so\n00007fbfe7f3a000      20      16      16 rw---   [ anon ]\n00007fbfe7f3f000    2024    1476       0 r-x-- libruby-1.9.1.so.1.9.1\n00007fbfe8139000    2044       0       0 ----- libruby-1.9.1.so.1.9.1\n00007fbfe8338000      20      20      20 r---- libruby-1.9.1.so.1.9.1\n00007fbfe833d000      16      16      16 rw--- libruby-1.9.1.so.1.9.1\n00007fbfe8341000     112     108     108 rw---   [ anon ]\n00007fbfe835d000     140     120       0 r-x-- ld-2.19.so\n00007fbfe83e5000      28      28       0 r--s- gconv-modules.cache\n00007fbfe83ec000       4       0       0 -----   [ anon ]\n00007fbfe83ed000    1560      60      60 rw---   [ anon ]\n00007fbfe8579000       4       0       0 -----   [ anon ]\n00007fbfe857a000      20      16      16 rw---   [ anon ]\n00007fbfe857f000       4       4       4 r---- ld-2.19.so\n00007fbfe8580000       4       4       4 rw--- ld-2.19.so\n00007fbfe8581000       4       4       4 rw---   [ anon ]\n00007fff34143000     136      48      48 rw---   [ stack ]\n00007fff34175000       8       4       0 r-x--   [ anon ]\nffffffffff600000       4       0       0 r-x--   [ anon ]\n\ntotal kB          200976   50076   45128\n```\n. fwiw, I see the same behavior in 1.9 and 2.0.  Though it is not as profound.\n. There appears to be movement on this.\n. gotcha..thanks!\n. ",
    "ph": "I've been trying to reproduce a similar issue in the logstash-input-sqs plugin for logstash 1.4.2.\nFrom what I understand it seems to be a leak in the sqs `#receive_message method.\nIf we let the process run for more than a few hours, all the memory available in the JVM will be consumed and\nthe performance will start to drop because the garbage collector will kick really often to clean the process.\nThe JVM never go OOM in the test, we only see a huge performance drop events processing.\nI am able to consistently reproduce the issue with the 1.35 version of the aws-sdk and the latest 1.63.\nScenario:\nLogstash is running on JRuby 1.7.11\n1. Used a large ec2 instance for the tests to replicate the issue.\n2. Create two SQS queue using the default settings, assume we have an infinite number of evens in the queues.\n3. Create two SQS input with a settings of 20 workers per threads (Most people don't go that far with the workers, this is really an empiric settings.)\n4. After 1h30 the events rate will drop from 400 events per seconds to 80 events per seconds.\nIf I look at some visualvm graph we will see this kind of behavior on the memory and the GC process.\n\n\nI have a also have a thread dump of the JVM.\nAnything else I could provide to help debug this issue?\n. @trevorrowe Thanks!\n. ",
    "pickhardt": "I found this issue because I suspect that the aws gem is leaking memory. Anybody know if upgrading to v2 of the aws gem fixes this?\n. Thanks.\n. ",
    "Tim-B": "No worries.\nFor the moment I've just been doing this which seems to get around the issue:\nresp.each do |log_page|\n  if log_page[:events].length == 0\n    break\n  end\n  # something\nend\n. ",
    "quezacoatl": "Ok, you're probably right about fake_sqs as it's not maintained but everything worked fine until and including version 2.0.0.rc15.\nThe output is:\nreading 221 bytes...\n-> \"<ReceiveMessageResponse>\\n    <ReceiveMessageResult>\\n    </ReceiveMessageResult>\\n    <ResponseMetadata>\\n        <RequestId>19643011-2310-45d0-8611-e8eaaccc550e</RequestId>\\n    </ResponseMetadata>\\n</ReceiveMessageResponse>\\n\"\nread 221 bytes\nConn keep-alive\nE, [2015-02-23T20:58:48.777826 #31500] ERROR -- : Error in EventMachine event loop: undefined method `key?' for \"\\n    \":String\n. I tried to remove the indentation in the FakeSQS::Responder response, and that works perfectly. Ie., Builder::XmlMarkup.new(:indent => 4) to Builder::XmlMarkup.new.\nI don't know if there is any issue with REXML here. The response from AWS is most likely missing indentation.\n. With ox``my fix does not seem to work. The error is``undefined methodkey?' for \"\":String``\n. I reproduced the issue with the gist you sent. I've created a new updated gist here: https://gist.github.com/quezacoatl/7f55bc833b103aa7f378\nYou must make sure that require \"nokogiri\" fails, which is done by require \"bundler/setup\" here. Maybe that is why you could not reproduce?\nIf you still cannot reproduce the issue I'm happy to assist over hangouts.\n. ",
    "omockler": "I was just hoping to wait for one of them to be in the success state, should I change then name? I could also add one for checking if they are all healthy.\n. Yeah, it makes sense, but it doesn't exactly cover the use case I had in mind.\nI'm creating a number of new instances to replace the ones currently attached to an ELB. One instance can handle all of the load coming through by itself, but I am starting additional instances for high availability purposes. I was hoping to start the process of deregistering and terminating the old instances once any number of the new instances become available through the load balancer.\nDo you think that this use case could warrant having a waiter for pathAll and pathAny?\n. Exactly what I was looking for, thanks for the naming suggestion!\n. ",
    "wjordan": "For reference, the AWS CLI (via botocore) allows configuration of EC2 credential retries/timeouts through metadata_service_num_attempts / metadata_service_timeout variables in the global configuration as well as AWS_METADATA_SERVICE_TIMEOUT / AWS_METADATA_SERVICE_NUM_ATTEMPTS environment variables (see source).\nIf this feature is ever implemented in the Ruby SDK it would be nice to re-use these existing names for reuse/consistency across the two SDKs.. Thanks for looking into this! I will update as soon as a new release is cut.\nUnfortunately since the issue is very intermittent (most recently we saw ~2 weeks between occurrences on our production traffic- we logged 692 errors on 11/4, then 139 errors on 11/17), It will be difficult to let you know if it definitively resolves the issue. However I will be sure to update here if the error ever occurs again after updating.\n. We haven't upgraded to V3 yet, but I will follow up here when we do complete an upgrade. This is still affecting our production application- the last burst was 1,232 instances of this error on Apr 29, running version 2.10.79.. ",
    "imlocn": "can the uri take this:  http://username:pass-wordKo@abc!a@proxy.abc.xyz.net:80\n. ",
    "modosc": "previously we were doing something like this:\n@s3_presigned_post ||= S3_BUCKET.presigned_post\n       key: s3_key,\n       success_action_status: 201,\n       acl: 'bucket-owner-full-control'.to_sym,\n       expires: ::S3_PRESIGNED_POST_EXPIRATION,\n       content_length: (0..::S3_MAX_FILE_SIZE),\n       content_type: get_content_type\nto adapt to v2 i changed the code like so:\n@s3_object ||= Aws::S3::Object.new bucket_name: S3_BUCKET, key: s3_key\n    @s3_presigned_post ||= @s3_object.presigned_url :put,\n       success_action_status: 201,\n       acl: 'bucket-owner-full-control'.to_sym,\n       expires: ::S3_PRESIGNED_POST_EXPIRATION,\n       content_length: (0..::S3_MAX_FILE_SIZE),\n       content_type: get_content_type\nwhen i try this i get the errors from #719 and this bug. commenting out the :success_action_status and :content_length lines will make the code work.\n. thanks!\n. ",
    "chenkirk": "This was answered in the old repo\nhttps://github.com/aws/aws-sdk-core-ruby/issues/215#issuecomment-76320218\n. ",
    "Davidslv": "Seems that I ended up doing something in the meantime and that example actually returns something else now\nr.bucket('131')\n=> #<Aws::S3::Bucket name=\"131\">\nAlthough I followed a different path which ended up like this:\n```\n    s3_double        = double\n    bucket_double    = double\n    s3_object_double = double\n    aws_url          = 'amazon.url'\nfile = Tempfile.new 'file'\n\ns3_object_double.stub(:upload_file).with(kind_of(Tempfile), acl: 'public-read')\ns3_object_double.stub(:public_url).and_return(aws_url)\n\nbucket_double.stub(:object).with(kind_of(String)).and_return(s3_object_double)\ns3_double.stub(:bucket).with(BUCKET).and_return(bucket_double)\nAws::S3::Resource.stub(:new).with(region: REGION).and_return(s3_double)\n\n```\n. ",
    "russau": "My mistake, I had a typo in my role access policy.  AssumeRoleWithWebIdentity is working fine now.\nRuss\n. Okay, changing my mind again.  Once I fixed my policy I could see the SDK is now using my .aws/credentials file.  Is anon auth possible via the ruby sdk?\n. ",
    "anazar": "Looks like I was writing the file incorrectly. Sorry about posting in error.\n. ",
    "chr0n1x": "@trevorrowe yeah, I tried removing all of my code that pulled credentials from IMDS out. Still had to set a region, but that worked. It's kind of sad that I have prune it though.\nThanks for the help!\n. ",
    "Sjeanpierre": "In that case what is the best way to pull all items off of the queue using this client?\nSent from my iPhone\nOn Mar 7, 2015, at 12:08 PM, Trevor Rowe notifications@github.com<mailto:notifications@github.com> wrote:\nPaging is only applied on operations that allow you to enumerate a list of resources without side-effects. Receiving messages from a queue has side-effects, removing messages from the queue. The only pageable operation in Aws::SQS::Client is #list_queues: https://github.com/aws/aws-sdk-ruby/blob/master/aws-sdk-core/apis/SQS.paginators.json\nCurrently, every operation is decorated with the pageable response. In theory, this allows customers to code against list operations that aren't pageable yet. Service teams could add paging and customers would start reaping benefits straightway.\nIn practice, I'm not sure this is worth the confusion, especially in this scenario. Receive messages is not a candidate for paging support.\n\nReply to this email directly or view it on GitHubhttps://github.com/aws/aws-sdk-ruby/issues/733#issuecomment-77698573.\n. Thanks for the information Trevor. let me know if I can do anything to help with the polling abstraction.\n. ",
    "DanielRedOak": "What version was this issue addressed in?  The Logstash plugins are still on the v1 branch, latest release I believe they use is 1.61\n. Thanks for clarifying, I just went ahead and re-wrote the Logstash plugin to use v2 of the SDK :smile: \n. Sure, did you want me to post here or somewhere else?\n. Probably, I'll need to sanitize things first.  I'll see about doing that shortly\n. OK, was just about to ask for an email.  Let me know if you need anything else!\n. Just tested again with a very large data set and it seemed to work correctly now.  Thanks for knocking this one out quickly and with a detailed explanation!\n. Absolutely.  Using SES to plop messages into S3 using KMS with the default aws/ses encryption keys.  Here is a little snippet for decrypting and putting to disk.  Pretty simple. kms_key_id in this case is 'aws/ses'\nruby\n    def decrypt(kms_key_id, object_key, bucket, target)\n      s3encrypt = Aws::S3::Encryption::Client.new(kms_key_id: kms_key_id)\n      s3encrypt.get_object(bucket:bucket, key:object_key, response_target: target)\n    end\n. I think we can add support pretty easily by modifying the kms_cipher_provider to include a case check for supported methods and setting GCM as the cipher when 'AES/GCM/NoPadding' is detected\nhttps://github.com/aws/aws-sdk-ruby/blob/master/aws-sdk-resources/lib/aws-sdk-resources/services/s3/encryption/kms_cipher_provider.rb#L16\nhttps://github.com/aws/aws-sdk-ruby/blob/master/aws-sdk-resources/lib/aws-sdk-resources/services/s3/encryption/kms_cipher_provider.rb#L37\n. Yea, I headed down the rabbit hole and ended up with a rather generic Cipher error after all the monkey patching I was working on.  Might re-visit, but atm I have higher priority projects to complete.  Thanks!\n. Worked on it a bit more and from what I can tell things should be working, but I am still hitting that cipher error so something is wrong with the data, key, or iv used.  I wrote up a quick decrypt test to see if I could do the work manually, see below.  I'm thinking that the key is incorrect, but it's hard to say.\n``` ruby\nawskey = decode64()\nawsiv = decode64()\nfile = File.open(, \"rb\")\nawsdata = file.read\ndecipher = OpenSSL::Cipher.new('aes-256-gcm')\ndecipher.decrypt\ndecipher.key = awskey\ndecipher.iv = awsiv\nplain = decipher.update(awsdata) + decipher.final\n```\nMy branch is here: https://github.com/DanielRedOak/aws-sdk-ruby/tree/ses-kms-fix\n. Sounds good.  I do see that things are being decrypted, but there is either padding at the end of the decryption (even with AES/GCM/NoPadding?) or it contains further data like an auth_tag or something.  Using the header 'x-amz-meta-x-amz-unencrypted-content-length' to select the original content works, but .finalize is still broken.  Just thought I'd share my findings.\n. Yep, was the tag hanging out at the end, I just needed to read up a bit more on GCM.  I now have a working implementation and will be able to submit a clean PR yet today.\n. PR https://github.com/aws/aws-sdk-ruby/pull/1043, needs further tests\n. @awood45 need anything from me to get things moving?  We're using SES + encryption in S3 so we're using my branch for decryption atm.\n. Just glancing over things it looks like tests for this change won't be trivial since kms is hardcoded to use AES/CBC/PKCS5Padding but I'll see about matching things up when I get some time\n. Added a test similar to the CBC test to verify decryption and renamed the CBC tests to be more descriptive now that there are multiple ciphers\n. @awood45 anything else we need to get this merged?\n. @awood45 any update?\n. @awood45 just wondering about a timeline on this.  I have a ticket on our end I'd like to close when this gets merged in.  Thanks!\n. My thought was to minimize calls to the API, but that seems reasonable as well as long as we aren't forced into download the full object more than once.  I might be able to peek next week too.  Let me know how things go.\n. ",
    "lexelby": "Ah, glad to hear this is fixed, thanks.\n. ",
    "nhattan": "\ud83d\udc4d \n. ",
    "danielsiwiec": "Hmm, that's a very good question. For my need I hand-rolled the mechanism, but it feels like I'm duplicating logic, that's already there:\n``` ruby\nclass Waiter\n  def initialize(options)\n    @options = options\n  end\ndef wait\n    attempts = 0\n    while @options[:poller].call && attempts < @options[:max_attempts]\n      sleep(@options[:delay])\n      attempts += 1\n    end\n  end\nend\noperation = Proc.new {\n  instance = ec2.instance(instance.instance_id)\n  instance.public_dns_name.empty?\n}\nwaiter = Waiter.new(\n  poller: operation,\n  max_attempts: 20,\n  delay: 1,\n)\nwaiter.wait\n```\nMy naive thinking makes me hope that if we opened up the Waiter and Poller classes and let people create them with a custom operation and pass it in, it would magically work.\nSomeone with a deeper understanding of the Poller would have to testify to that though.\n. Trevor, I took a stab at implementing a custom waiter. I also tried to follow your testing convention. Please look at the commit above and let me know what you think.\nI put extra effort not to recreate the waiting logic and reuse the existing mechanism.\nPros:\n- reused the original waiting logic\n- the interface is very easy to use instance.wait_until(&condition)\nCaveats:\n- implemented only in EC2::Instance class (I'd need some guidance with how to implement the function across all resources, if needed)\n- hardcoded waiter parameters\nLet me know what you think about the approach. I haven't written any docs yet, but if this gets a green light, I'll definitely add some.\nCheers!\n. @trevorrowe I like what you did with the idea. I'm curious why you're counting the attempts in the Poller. Isn't Waiter already doing this?\nI agree with max attempts and delay dilemma. As the custom waiter is more open than wait_until_something, I feel like we're passing more power to the user and one could say that with this power comes a great responsibility to set the values :)\nI also like the resource being the returned value. I wonder why the Client.wait_until isn't doing this too. I struggled to make a good use of PageableResponse\nOn a different note - I don't see reload being documented anywhere and it is really useful!\nAnything I can do to help? Add some tests maybe?\n. Now I see what you mean with the optimization. The way I was doing it, I was reloading the resource before calling the block. That way I didn't have to check the max attempts, but I was doing a possibly redundant API call at first iteration, if the instance is already in the desired state. Performance vs code readability...\nThe problem of updating the internal state of the resource is indeed interesting. On one hand, implicit state changes are a debugging nightmare and, like you mentioned, not thread safe. Also, a wait method name doesn't suggest state change (maybe add wait_until! would?) and could be confusing.\nOn the other hand, a resource is usually a representation of a remote object and it's desired for the state to be current.\nI think whichever approach is fine, given it's consistent with the behavior of other methods. Currently, my impression is the resource instance is really a snapshot of the remote object in a given point in time, unless explicitly reloaded. In that case, this situation is acceptable and understood:\nruby\nstopped_instance.state.name #=> 'stopped'\nrunning_instance.state.name #=> 'running'\nDoes it make sense?\n. Cool! Let me know if you'd like to see a pull request for this. I'd like to contribute to this.\n. Working on it. How do you feel about the interface of the method? \nIn the implementation you suggested, the block is the actual waiting operation. A usage could look like this:\nruby\nresource.wait_until(max_attempts:10) {|instance| instance.state == 'running'}\nWhile I really like the brevity of this approach, it's not very consistent with other waiter methods, like:\nruby\nClient.wait_until(:instance_running) {|waiter| waiter.max_attempts=10}\nThe implementation could be modified, to accept a Proc as an argument and yield the waiter to the block, like this:\nruby\nresource.wait_until(Proc.new {|instance| instance.state == 'running'}) {|waiter| waiter.max_attempts=10}\nThoughts?\n. I modified the implementation, added some specs and did a brief smoke test. All looks good. Still need to add documentation.\n@trevorrowe - for some reason I had the hardest time ever creating a copy of the resource and eventually came up with this:\nruby\nresource_copy = self.class.new(@identifiers.merge(client:@client, data:@data))\nLet me know about the interface dilemma above and I'll start with the docs.\n. #dup does work. Ruby is not really my first language...\nWorking on the docs now and will be ready soon\n. @trevorrowe Pull request ready!\n. @trevorrowe I see this still hasn't been merged in from the custom-waiter branch. Is it just oversight or has it been de-prioritized?\n. Thanks!\n. Doing it right now.\n. Thank you. My pleasure to contribute!\n. ",
    "shaicoleman": "In that case it might make more sense to first create APIs for these methods, and build a wrapper around that when that becomes available\n. ",
    "rromanchuk": ":+1: \n. Actually, this looks like user error\n```\nirb(main):314:0> t[\"createdAt\"].class\n=> BigDecimal\nirb(main):315:0> t[\"createdAt\"].to_i\n=> 1530673621\n```. ",
    "skyhighwings": "Haven't had a chance to look through the code yet, but presuming that it works as it should (and I believe it will), thanks a ton @trevorrowe!\n. ",
    "arunthampi": "sweet, this is great! will you be cutting a new gem release soon which includes this feature @trevorrowe?\n. cool, thanks :+1: \n. I tried using using Aws::S3::PresignedPost in my app (ref: 11ab66582978324b3ed1f636d024eb0ab7c8ff58) with the jquery-file-upload plugin, I get a proper XML response but the file doesn't exist in the bucket. https://gist.github.com/arunthampi/f4e47fa5a1d2ed1b7abc \nFollowed the tutorial here: https://devcenter.heroku.com/articles/direct-to-s3-image-uploads-in-rails, anything I'm missing?\n. Yes that's correct a HEAD or GET on this object returns a 403. The new signature format works for all regions yes?\n. Confirmed that the exact same JS code uploads the file for the v1 version of PresignedPost, but with v2, it returns a valid response but the file itself doesn't get uploaded.\n. Apologies for the confusion, I've update the gist with the v1 PresignedPost:\nhttps://gist.github.com/arunthampi/f4e47fa5a1d2ed1b7abc#file-presigned_post_v1-rb\nand here's v2: \nhttps://gist.github.com/arunthampi/f4e47fa5a1d2ed1b7abc#file-presigned_post_v2-rb\nand here's the the Javascript which does the file upload: \nhttps://gist.github.com/arunthampi/f4e47fa5a1d2ed1b7abc#file-fileupload-js-coffee\nthe relevant lines where I extract the URL of the uploaded file is here: \nhttps://gist.github.com/arunthampi/f4e47fa5a1d2ed1b7abc#file-fileupload-js-coffee-L36-L37\n. doh! that was it, thanks much for your help!\n. ",
    "itskingori": "@trevorrowe Thanks! And yeah I saw that ... it's probably correct. My problem was in some weird behaviour/errors (probably due to my misinterpretation the docs). Lemme provide some context ...\nPlease note that I'm using this in a Rails context and usually I prefer to use the initialiser files to configure gems ... which means that I create a file ... config/initializers/aws.rb ... and it contains this ...\nruby\nAws.config[:credentials] = [ENV['AWS_ACCESS_KEY_ID'], ENV['AWS_SECRET_ACCESS_KEY']]\nSo that didn't work ... and my call kept spewing session token errors so I figured that the gem isn't picking my credentials from the initialiser (at least when setting through the :credentials key) ... and so when I googled around I found someone on SO someone who provided an alternative way of setting the credentials ... which is like so ...\nruby\nAws.config.update(\n  access_key_id: ENV['AWS_ACCESS_KEY_ID'],\n  secret_access_key: ENV['AWS_SECRET_ACCESS_KEY']\n)\nThe README section you referenced mentions accessing from Aws.config[:credentials]. Which I guess is why I tried to set it that way the first time round.\nSo maybe a little clarification on specifically that part I guess. And a big part of the confusion is that I'm coming from a v1 mindset (where I used to assign defaults in an initialiser).\nAnd just for clarity sake, it seems to me that all I had to do is set AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY variables??? :-) ...\n. Ah, @trevorrowe you've nailed it :8ball: It's perfectly clear now! :+1: \n. One last thing I'd like to comment on is the consistency of the environment variables that are in use ... for example on Elastic Beanstalk environment properties config panel ... there's an entry for AWS_ACCESS_KEY_ID and AWS_SECRET_KEY that's there by default. One can't remove them and so I'd bet that most people using EB stick to these names for the key ID and secret key.\n\nThen ... this gem's documentation mentions that it checks for AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY (the latter is different). If someone relied or assumed that there's a consistency to how these variables are named/used by AWS across the different libraries and services you can imagine that they'd be getting errors occasionally. And the difference is a bit subtle to notice.\nIn this repo a quick search for AWS_ACCESS_KEY_ID shows that the gem does check for this throughout. That's good.\nThen search for AWS_SECRET_KEY yields some results ... and search for AWS_SECRET_ACCESS_KEY yields even more results ... yet they are meant to be the same thing. o_O\nThis is why I prefer to explicitly configure credentials than trust the gem :-)\nPs: This doesn't impact the functionality of the gem, just more feedback.\n. @trevorrowe I think your explanation captures it all well. Thanks a lot for your time and attention.\n. @trevorrowe :+1: :+1: :+1: \n. @trevorrowe I see! Forgot about resource interfaces. Thanks. When I got into v2 I didn't quite understand the two methods of access ... I have dabbled with the other side more than I have with resource interfaces. Thanks.\n. Hey @trevorrowe, this isn't a bug or anything but just general feedback.\nWhen I started using V2 all, the, documentation points to (or at least mentions explicitly) the aws-sdk gem (this). Naturally, one would go to the API documentation of the said gem for more information (and assume incorrectly that everything is there).\nI do remember coming across mentions of using Resources (there is after all a simple example on the README) ... I kinda got lost (and gave up looking for it so decided to use the more complex alternative).\nNow months later, after you replied to this issue it finally hits me that the documentation is probably in the API documentation of the other gem. Which I found here ...\nhttp://www.rubydoc.info/gems/aws-sdk-resources/\n. Oh and my point was, I think it would be helpful for others if a link in the README was added to point to the API documentation of the resource interfaces. Explicitly. I started on a pull request but realised that you guys don't use rubydoc.info to host your documentation so my link would be the unofficial link.\n. ",
    "ajcantu": "Ah, yes. It looks like it will just error later at:\nhttps://github.com/aws/aws-sdk-ruby/blob/master/aws-sdk-core/lib/aws-sdk-core/errors.rb#L100\nOk, we'll try to get more information on why this is failing for us. It's only happened once in the last 4 days, so I'm not sure when we'll see it again, but we'll create a new pull request at that time.\nThanks for taking a look!\n. Sweet. Thanks, @trevorrowe!\n. ",
    "ms-ati": "Very cool, thanks for adding it to the list.\n. Sweet, thanks @euank I will try these out when I upgrade.\n. Any progress on this since last year?. Ah -- yes thank you!\nThis is the context I was missing, which did indeed surprise me:\n\nIt\u2019s also important to know that if you specify a major version only, like this:\n```\ngemspec\nspec.add_runtime_dependency 'library', '~> 2'\n```\nIt will only use the latest version from the 2.x series \u2013 so 2.3.0 \u2013 and not 3.0.0. This behaviour may surprise some people, but automatically allowing any major version past version 2 is more surprising behaviour.. \n",
    "euank": "@ms-ati \nGood news, there are some ECS waiters now :smiley:\nI specifically added :services_stable, :services_inactive, :tasks_running, and :tasks_stopped. These are included in the recent v2.1.1 release.\nI did not add a waiter for deregister_container_instance because it's actually a synchronous operation.\nIf you have suggestions for other waiters, or run into issues with these, I'd be happy to look into it.\nCheers,\nEuan\n. ",
    "morisan": ":)\n. ",
    "magegu": "Thanks for the feedback. I'll run the next tests next week and report if there is any improvement!\n. hi @trevorrowe thanks for your help.\nI haven't had this issue since the last update (as far as i saw error reportings at new relic) ... so yes, consider it closed :)\n. ",
    "dtan4": "Thanks for quick resolving :+1: :exclamation: \n. ",
    "jszwedko": "Fixed by 8d3c543e0f42c1864ae629784ee8c01d49d1ebf8\n. No worries!\n. ",
    "acoulton": "@trevorrowe thanks very much for fixing this.\n. ",
    "fidothe": "Le sigh.\nMy guess is https://github.com/capistrano/capistrano/blob/5c9b27e90d7ac81e47c08c9a6c1c88c31fc66fac/lib/capistrano/dsl.rb#L64 is the culprit. Off the top of my head, I can't see why Rake::Task couldn't be extended instead. I'll raise this with them and see where we get to...\n. ...And then I looked at the Rake source code.\nSo, Rake doesn't instance_eval or class_eval or mess with the binding of the task blocks in any way. If you want to add DSL methods to a rake task, you have to extend Object. Sorry - there's not much Capistrano can really do here without Rake changing itself in a way which would almost certainly break half the world's Rakefiles.\nHere's a 4-line change to Aws::Resources::Resource to make it inherit from BasicObject. All the tests pass, and it fixes the problem... Ought to work in 1.9+ (no dice in 1.8)\n. oh sorry, i wasn't paying attention :-(\nAs I learnt on Friday, Object is just BasicObject with Kernel included... \nruby\n(Class.new.instance_methods - Class.new(BasicObject) { include Kernel }.instance_methods).sort\n=> []\nI think your AWS_SDK_SAFE_DEFINE approach is fine, but I also really liked your keep-your-metaprogramming-fingers-out-of-my-sdk-objects approach from before - particularly given how dynamic your object definition approach needs to be for generating stuff the way you do.\nI'd think that subclassing BasicObject and including Kernel would give you the best of both worlds \u2013 all the convenience methods from Object, but the safety of being disconnected from the main Object hierarchy. My #775 PR (which I, under no circumstances, think you should pull in as-is) passed all the tests just fine - without any other code modificiations. (Without including Kernel too, about 90 tests failed).\nBasicObject was pretty much invented for complex metaprogramming needs like yours, after all...\n. Sounds good - my main objection to the solution I proposed in #775 was the issue of edge cases...\n. ",
    "ys": "It was a propagation issue.\nI was scripting everything.\nSorry for this\n. ",
    "mylesmegyesi": "Yep.\n``` ruby\nrequire 'aws-sdk'\nAws.config.update({\n  region: 'us-west-1',\n  access_key_id: 'key-goes-here',\n  secret_access_key: 'key-goes-here'\n})\ncf = Aws::CloudFront::Client.new\nresp = cf.get_distribution(id: 'id-goes-here')\ncf.update_distribution({\n  id: resp.distribution[:id],\n  distribution_config: resp.distribution[:distribution_config].to_h,\n})\n```\nresults in...\nbash\n$ ruby test.rb\n~/.rvm/gems/ruby-2.1.5@default/gems/aws-sdk-core-2.0.38/lib/seahorse/client/plugins/raise_response_errors.rb:15:in `call': Unexpected list element termination (Aws::CloudFront::Errors::MalformedInput)\n        from ~/.rvm/gems/ruby-2.1.5@default/gems/aws-sdk-core-2.0.38/lib/seahorse/client/plugins/param_conversion.rb:22:in `call'\n        from ~/.rvm/gems/ruby-2.1.5@default/gems/aws-sdk-core-2.0.38/lib/aws-sdk-core/plugins/response_paging.rb:10:in `call'\n        from ~/.rvm/gems/ruby-2.1.5@default/gems/aws-sdk-core-2.0.38/lib/seahorse/client/plugins/response_target.rb:18:in `call'\n        from ~/.rvm/gems/ruby-2.1.5@default/gems/aws-sdk-core-2.0.38/lib/seahorse/client/request.rb:70:in `send_request'\n        from ~/.rvm/gems/ruby-2.1.5@default/gems/aws-sdk-core-2.0.38/lib/seahorse/client/base.rb:216:in `block (2 levels) in define_operation_methods'\n        from test.rb:13:in `<main>'\n. I removed the to_h and everything works. That's so much!\n. ",
    "sunao-uehara": "Thanks trevorrowe,\nIt works perfectly fine. I still need to check condition of env to include that code, but I think it is fine for now.\n. ",
    "curtislinden": "This issue was in shoryuken.yml attributes => attribute_names\n. ",
    "StaymanHou": "Sure, here's my code:\n``` ruby\nclass VideosProcessJob < ActiveJob::Base\n  queue_as :default\ndef perform(video_id)\n    @video_id = video_id\n    elastictranscoder = Aws::ElasticTranscoder::Client.new\njob = elastictranscoder.create_job(\n  # required\n  pipeline_id: Settings.elastic_transcoder_job.pipeline_id,\n  # required\n  input: input_settings,\n  outputs: [output_settings_mp4, output_settings_webm]\n)\n\nend\nprivate\ndef input_settings\n    {\n      key: \"videos/#{@video_id}/uploaded_video_file\",\n      frame_rate: 'auto',\n      resolution: 'auto',\n      aspect_ratio: 'auto',\n      interlaced: 'auto',\n      container: 'auto'\n    }\n  end\ndef output_settings_mp4\n    {\n      key: \"videos/#{@video_id}.mp4\",\n      thumbnail_pattern: \"thumbnails/#{@video_id}-{count}\",\n      rotate: 'auto',\n      preset_id: Settings.elastic_transcoder_job.mp4_preset_id\n    }\n  end\ndef output_settings_webm\n    {\n      key: \"videos/#{@video_id}.webm\",\n      rotate: 'auto',\n      preset_id: Settings.elastic_transcoder_job.webm_preset_id\n    }\n  end\nend\n``\n. looking into the sourcepageable_response.rb`:\nruby\n    def each(&block)\n      return enum_for(:each_page) unless block_given?\n      response = self\n      yield(response)\n      until response.last_page?\n        response = response.next_page\n        yield(response)\n      end\n    end\n    alias each_page each\nThe first yield(response) doesn't make sense to me.\n. Never mind. It's related to rspec matcher. The code is working fine. However, when invoked in a rspec expectation, it results in the error.\nThank you very much, @trevorrowe.\n. ",
    "fertobar": "hi guys! do you know if patch is still needed with sdk-3 and Ruby 2.2, or if is need to upgrade Ruby to avoid this issue?\nI was using https://github.com/assembler/aws-sdk-ruby-memory-fix for sdk 2 but is not compatible for sdk 3. sure!\neg\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/DAX.client.run-application-python.03-getitem-test.html\nresult = client.get_item(**params)\n. ok, thanks for the feedback!. ",
    "bilus": "@trevorrowe Have you been able to look into this? We badly need support for presigned urls using CNAMEd buckets. Unlike @jbr, we host the files outside us-standard so +1 to a solution that somehow takes that into account before we close this door and make it difficult to add this functionality in the future. \nIf there's any way I can help move it forward, please let me know.\n. Actually, I've been able to generate the urls using the code similar to this:\nruby\n  bucket_name = \"some-bucket.example.com\"\n  s3 = Aws::S3::Client.new(endpoint: \"http://example.com\", region: \"eu-central-1\")\n  bucket = Aws::S3::Bucket.new(\"some-bucket\", client: s3)\n  object = bucket.object(\"some-key\")\n  object.presigned_url(:get, expires_in: 24 * 3600, secure: false)\nThis probably gives a better idea of what goes where (and should probably work but it's untested):\nruby\ndef generate_presigned_url(region, domain, bucket_basename, key, expire_after)\n  bucket_name = \"#{bucket_basename}.#{domain}\"   \n  s3 = Aws::S3::Client.new(endpoint: \"http://#{domain}\", region: region)   \n  bucket = Aws::S3::Bucket.new(bucket_basename, client: s3)\n  object = bucket.object(key)\n  object.presigned_url(:get, expires_in: expire_after, secure: false)\nend\nAs this based on stepping through the code with a debugger and not docs, I'm leaving stuff that can probably be omitted (e.g. region).\nThe secure: false bit is important because of lack of security certificates for some-bucket.example.com.\n. Thanks!\n. ",
    "wal": "Thanks @trevorrowe \n. ",
    "wazoo": "Also to note, I just tried updating aws-sdk gem to 2.0.39 and i'm getting the same issue.\n. Yep, they are what I expect and match the environment variables I have set.\n. Here you go:\nopening connection to route53.amazonaws.com:443...\nopened\nstarting SSL for route53.amazonaws.com:443...\nSSL established\n<- \"POST /2013-04-01/hostedzone//rrset/ HTTP/1.1\\r\\nContent-Type: \\r\\nAccept-Encoding: \\r\\nUser-Agent: aws-sdk-ruby2/2.0.38 ruby/2.2.1 x86_64-darwin14\\r\\nX-Amz-Date: 20150429T193109Z\\r\\nHost: route53.amazonaws.com\\r\\nX-Amz-Content-Sha256: 0805fb1f8ed18200d26e163285f80334d49327a996a87d71446a5cf97d602180\\r\\nAuthorization: AWS4-HMAC-SHA256 Credential=AKIAIA4GD4IFROR67SNQ/20150429/us-east-1/route53/aws4_request, SignedHeaders=host;user-agent;x-amz-content-sha256;x-amz-date, Signature=154e0287a170ec3daf559c7200d279a2d0dc27453631d270e586b8d9f3f49727\\r\\nContent-Length: 611\\r\\nAccept: */*\\r\\n\\r\\n\"\n-> \"HTTP/1.1 403 Forbidden\\r\\n\"\n-> \"x-amzn-RequestId: 4998cd94-eea6-11e4-a38c-6d0ea567c9a2\\r\\n\"\n-> \"Content-Type: text/xml\\r\\n\"\n-> \"Content-Length: 1018\\r\\n\"\n-> \"Date: Wed, 29 Apr 2015 19:31:08 GMT\\r\\n\"\n-> \"\\r\\n\"\nreading 1018 bytes...\n-> \"<?xml version=\\\"1.0\\\"?>\\n<ErrorResponse xmlns=\\\"https://route53.amazonaws.com/doc/2013-04-01/\\\"><Error><Type>Sender</Type><Code>SignatureDoesNotMatch</Code><Message>The request signature we calculated does not match the signature you provided. Check your AWS Secret Access Key and signing method. Consult the service documentation for details.\\n\\nThe Canonical String for this request should have been\\n'POST\\n/2013-04-01/hostedzone/rrset/\\n\\nhost:route53.amazonaws.com\\nuser-agent:aws-sdk-ruby2/2.0.38 ruby/2.2.1 x86_64-darwin14\\nx-amz-content-sha256:0805fb1f8ed18200d26e163285f80334d49327a996a87d71446a5cf97d602180\\nx-amz-date:20150429T193109Z\\n\\nhost;user-agent;x-amz-content-sha256;x-amz-date\\n0805fb1f8ed18200d26e163285f80334d49327a996a87d71446a5cf97d602180'\\n\\nThe String-to-Sign should have been\\n'AWS4-HMAC-SHA256\\n20150429T193109Z\\n20150429/us-east-1/route53/aws4_request\\nbd0f21fb4d83f5df6cff92c6b156fe55c1948f28764f0a45a7d74341f3587522'\\n</Message></Error><RequestId>4998cd94-eea6-11e4-a38c-6d0ea567c9a2</RequestId></ErrorResponse>\"\nread 1018 bytes\nConn keep-alive\nI have a script that uses this SDK to create a load balancer, this is the only part of the SDK I have seen this error on but I haven't used every section.\n. Damn, sure enough my variable was misspelled that was supplying that.  I wish the error had been more clear, ah well.  Thanks!\n. ",
    "lamroger": "Makes sense. Thanks!\n. Thank you for the quick reply. Makes sense for a Ruby SDK to use Set for sets. Now to differentiate lists from string sets.\nI opened a Ruby Issue for those interested in following up with the Set #to_json. https://bugs.ruby-lang.org/issues/11241\n. ",
    "sferik": "@coveralls This is not helpful at all.\n. @lsegal I was kind of hoping you wouldn\u2019t delete all those spam messages so I could report them for abuse. Now that GitHub supports multiple pull request status checks, @coveralls bot should :skull:.\n. I threaded to report them (on Twitter), so maybe @coveralls did.\n. ",
    "machadolab": "According to http://docs.aws.amazon.com/cli/latest/reference/elb/describe-instance-health.html#output it would be OutOfService, which is what I usually see for a while, and then it stops showing up in the list. So maybe wait on both states.\n. ",
    "mlangenberg": "You are correct. I cannot reproduce the issue with your code example.\nMaybe I was confusing myself with documentation and tutorials using different parameters.\nI will close this issue and only reopen it after I run into this again.\n. One difference between Aws::S3::Client and Aws::S3::Encryption::Client is that get_object does not support passing a block for Aws::S3::Encryption::Client \n``` ruby\nrequire 'openssl'\nkey = OpenSSL::PKey::RSA.new(1024)\ns3 = Aws::S3::Encryption::Client.new(encryption_key: key)\ns3.put_object(bucket:'aws-sdk', key:'secret', body:'handshake')\ns3.client.get_object(bucket:'aws-sdk', key:'secret') do |chunk|\n  p chunk #=> \"ciphertext\"\nend\ns3.get_object(bucket:'aws-sdk', key:'secret') do |chunk|\n  p chunk # never called\nend \n```\n. ",
    "vikas027": "Sure. This is my short script (without my keys) and the error(s) as well on my CentOS 6.6 x86_64 machine.\n``` ruby\n!/usr/local/rvm/rubies/ruby-2.1.5/bin/ruby\nrequire 'aws-sdk'\nrequire 'aws-sdk-core'\nbucket_name = 'vikas-bucket/vikas'\nfile_name = ARGV[0]\ns3 = Aws::S3::Resource.new(\n    credentials: Aws::Credentials.new('MY_ACCESS_KEY', 'MY_SECRET_KEY'),\n    region:'us-west-1'\n)\nputs \"Uploading #{file_name} of #{File.size(file_name)} byte(s) to #{bucket_name} ...\\n\"\nobj = s3.bucket(bucket_name).object(file_name)\nobj.upload_file(file_name)\nvikas@reachvikas:~#\nThis works fine as the size of the file is greater than 0 bytes.\nvikas@reachvikas:~# ls -l abc123/zz.txt\n-rw-r--r-- 1 root root 1024 Apr 29 03:32 abc123/zz.txt\nvikas@reachvikas:~# ./s3_file_upload_V2.rb abc123/zz.txt\nUploading abc123/zz.txt of 1024 byte(s) to vikas-bucket/vikas ...\n```\nBut, I try to upload a file size of zero bytes, I get below error(s).\nbash\nvikas@reachvikas:~# > abc123/zz.txt\nvikas@reachvikas:~# ls -l abc123/zz.txt\n-rw-r--r-- 1 root root 0 Apr 29 03:33 abc123/zz.txt\nvikas@reachvikas:~# ./s3_file_upload_V2.rb abc123/zz.txt\nUploading abc123/zz.txt of 0 byte(s) to vikas-bucket/vikas ...\n/usr/local/rvm/gems/ruby-2.1.5/gems/aws-sdk-core-2.0.41/lib/seahorse/client/plugins/raise_response_errors.rb:15:in `call': The request signature we calculated does not match the signature you provided. Check your key and signing method. (Aws::S3::Errors::SignatureDoesNotMatch)\n    from /usr/local/rvm/gems/ruby-2.1.5/gems/aws-sdk-core-2.0.41/lib/aws-sdk-core/plugins/s3_sse_cpk.rb:18:in `call'\n    from /usr/local/rvm/gems/ruby-2.1.5/gems/aws-sdk-core-2.0.41/lib/seahorse/client/plugins/param_conversion.rb:22:in `call'\n    from /usr/local/rvm/gems/ruby-2.1.5/gems/aws-sdk-core-2.0.41/lib/aws-sdk-core/plugins/response_paging.rb:10:in `call'\n    from /usr/local/rvm/gems/ruby-2.1.5/gems/aws-sdk-core-2.0.41/lib/seahorse/client/plugins/response_target.rb:18:in `call'\n    from /usr/local/rvm/gems/ruby-2.1.5/gems/aws-sdk-core-2.0.41/lib/seahorse/client/request.rb:70:in `send_request'\n    from /usr/local/rvm/gems/ruby-2.1.5/gems/aws-sdk-core-2.0.41/lib/seahorse/client/base.rb:216:in `block (2 levels) in define_operation_methods'\n    from /usr/local/rvm/gems/ruby-2.1.5/gems/aws-sdk-resources-2.0.41/lib/aws-sdk-resources/services/s3/file_uploader.rb:42:in `block in put_object'\n    from /usr/local/rvm/gems/ruby-2.1.5/gems/aws-sdk-resources-2.0.41/lib/aws-sdk-resources/services/s3/file_uploader.rb:49:in `open_file'\n    from /usr/local/rvm/gems/ruby-2.1.5/gems/aws-sdk-resources-2.0.41/lib/aws-sdk-resources/services/s3/file_uploader.rb:41:in `put_object'\n    from /usr/local/rvm/gems/ruby-2.1.5/gems/aws-sdk-resources-2.0.41/lib/aws-sdk-resources/services/s3/file_uploader.rb:34:in `upload'\n    from /usr/local/rvm/gems/ruby-2.1.5/gems/aws-sdk-resources-2.0.41/lib/aws-sdk-resources/services/s3/object.rb:102:in `upload_file'\n    from ./s3_file_upload_V2.rb:16:in `<main>'\nvikas@reachvikas:~#\n. Hi Trevor,\nI am sorry, I do not know how to use rescue in my shell script. Can you please help me to do so ?\nI am following this link. This is the link I am following.\nAdditionally, I have checked on a freshly installed CentOS 6.6 x86_64 machine with latest version of ruby (v 2.2.1) and AWS Ruby SDK (v 2.0.41), the issue still persists.\nRegards,\nVikas\n. Here you are. Please note that I have replaced my access key with text 'MY_ACCESS_KEY'\nbash\nvikas@reachvikas:~# touch abc123/empty\nvikas@reachvikas:~# ./s3_file_upload_V2.rb abc123/empty\nUploading abc123/empty of 0 byte(s) to vikas-bucket/vikas ...\nSignatureDoesNotMatch\nThe request signature we calculated does not match the signature you provided. Check your key and signing method.\n\"<?xml version=\\\"1.0\\\" encoding=\\\"UTF-8\\\"?>\\n<Error><Code>SignatureDoesNotMatch</Code><Message>The request signature we calculated does not match the signature you provided. Check your key and signing method.</Message><AWSAccessKeyId>MY_ACCESS_KEY</AWSAccessKeyId><StringToSign>AWS4-HMAC-SHA256\\n20150429T163001Z\\n20150429/us-west-1/s3/aws4_request\\n20f867df1a645adf595430ea5dcb4f04db0316d38e170460f6aab0d198f6ca2b</StringToSign><SignatureProvided>52d4087f909047eaee0f311f053b466f370748ab2a4c103afc4e1bc967767a82</SignatureProvided><StringToSignBytes>41 57 53 34 2d 48 4d 41 43 2d 53 48 41 32 35 36 0a 32 30 31 35 30 34 32 39 54 31 36 33 30 30 31 5a 0a 32 30 31 35 30 34 32 39 2f 75 73 2d 77 65 73 74 2d 31 2f 73 33 2f 61 77 73 34 5f 72 65 71 75 65 73 74 0a 32 30 66 38 36 37 64 66 31 61 36 34 35 61 64 66 35 39 35 34 33 30 65 61 35 64 63 62 34 66 30 34 64 62 30 33 31 36 64 33 38 65 31 37 30 34 36 30 66 36 61 61 62 30 64 31 39 38 66 36 63 61 32 62</StringToSignBytes><CanonicalRequest>PUT\\n/vikas-bucket/vikas/abc123/empty\\n\\nhost:s3-us-west-1.amazonaws.com\\nuser-agent:aws-sdk-ruby2/2.0.41 ruby/2.1.5 x86_64-linux resources\\nx-amz-content-sha256:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\\nx-amz-date:20150429T163001Z\\n\\nhost;user-agent;x-amz-content-sha256;x-amz-date\\ne3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855</CanonicalRequest><CanonicalRequestBytes>50 55 54 0a 2f 76 69 6b 61 73 2d 62 75 63 6b 65 74 2f 76 69 6b 61 73 2f 61 62 63 31 32 33 2f 65 6d 70 74 79 0a 0a 68 6f 73 74 3a 73 33 2d 75 73 2d 77 65 73 74 2d 31 2e 61 6d 61 7a 6f 6e 61 77 73 2e 63 6f 6d 0a 75 73 65 72 2d 61 67 65 6e 74 3a 61 77 73 2d 73 64 6b 2d 72 75 62 79 32 2f 32 2e 30 2e 34 31 20 72 75 62 79 2f 32 2e 31 2e 35 20 78 38 36 5f 36 34 2d 6c 69 6e 75 78 20 72 65 73 6f 75 72 63 65 73 0a 78 2d 61 6d 7a 2d 63 6f 6e 74 65 6e 74 2d 73 68 61 32 35 36 3a 65 33 62 30 63 34 34 32 39 38 66 63 31 63 31 34 39 61 66 62 66 34 63 38 39 39 36 66 62 39 32 34 32 37 61 65 34 31 65 34 36 34 39 62 39 33 34 63 61 34 39 35 39 39 31 62 37 38 35 32 62 38 35 35 0a 78 2d 61 6d 7a 2d 64 61 74 65 3a 32 30 31 35 30 34 32 39 54 31 36 33 30 30 31 5a 0a 0a 68 6f 73 74 3b 75 73 65 72 2d 61 67 65 6e 74 3b 78 2d 61 6d 7a 2d 63 6f 6e 74 65 6e 74 2d 73 68 61 32 35 36 3b 78 2d 61 6d 7a 2d 64 61 74 65 0a 65 33 62 30 63 34 34 32 39 38 66 63 31 63 31 34 39 61 66 62 66 34 63 38 39 39 36 66 62 39 32 34 32 37 61 65 34 31 65 34 36 34 39 62 39 33 34 63 61 34 39 35 39 39 31 62 37 38 35 32 62 38 35 35</CanonicalRequestBytes><RequestId>5D7E73297A927D2C</RequestId><HostId>xXgSzmXfPk22SeLfqSsF+JxNg3OCpcnF044vUUrDZdnp/9/uktV9H/S5iQj+C4QU0s3Tf0zlooo=</HostId></Error>\"\n{\"user-agent\"=>\"aws-sdk-ruby2/2.0.41 ruby/2.1.5 x86_64-linux resources\", \"x-amz-date\"=>\"20150429T163001Z\", \"host\"=>\"s3-us-west-1.amazonaws.com\", \"x-amz-content-sha256\"=>\"e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\", \"authorization\"=>\"AWS4-HMAC-SHA256 Credential=MY_ACCESS_KEY/20150429/us-west-1/s3/aws4_request, SignedHeaders=host;user-agent;x-amz-content-sha256;x-amz-date, Signature=52d4087f909047eaee0f311f053b466f370748ab2a4c103afc4e1bc967767a82\", \"content-length\"=>\"0\"}\n\"\"\nvikas@reachvikas:~#\n. Thanks Trevor, this works now. But, I am still not too sure why it only affects zero byte files. Anyways, I am glad that I have a workaround. Here is my final script, may be useful for someone who bumps here. I am closing this issue.\nThanks again for your time on this.\n. ",
    "pdrakeweb": "Here is a partial stack trace (the remainder of the stack trace is inside custom code so it wouldn't help to include): https://gist.github.com/pdrakeweb/f548c5b7cb4a83409a74\n. Yes, that is the waiter.. Yes, we are commonly creating, updating and deleting many CloudFormation stacks in many different processes simultaneously.. @awood45 given the use of more than one CloudFormation stack in a single AWS account is likely a common use-case (and is expected to result in Throttling errors), is there a reason why the waiters shouldn't handle Throttling errors as a retry-able response (see https://github.com/aws/aws-sdk-ruby/compare/master...pdrakeweb:retry-throttled-cf-requests).. I understand the concern, however, since throttling is part of the black box of AWS' APIs (and is thus entirely unpredictable) is it really reasonable for someone to expect to reliably fail to poll because of throttling?  I guess there are some use cases for failing instead of succeeding when AWS starts throttling (like sleeping, then re-starting the polling), but I can't think of one which wouldn't be better served by simply succeeding in the first place.. ",
    "glennpratt": "Interesting. Are you planning to create an upstream (Ruby) bug?\n. Nice, thanks for sharing!\n. Thanks.\n. @Doug-AWS thanks for the reply, but that does not appear to be the case in my example (see the types in the commented output) or in the return types in the documentation:\nhttp://docs.aws.amazon.com/sdkforruby/api/Aws/EC2/RouteTable.html#routes-instance_method\nhttp://docs.aws.amazon.com/sdkforruby/api/Aws/EC2/RouteTable.html#create_route-instance_method\n. In my example, route and rt are both Aws::EC2::Route, yet rt.data raises NotImplementedError: #load not defined for Aws::EC2::Route.\n. @Doug-AWS yes, I don't think there is any issue with EC2 functionality, just that Route#create_route seems to return an incomplete object.\n. @awood45 thanks for looking into this, I think I understood that when I posted the issue. I'm confused why returning a broken object is intended behavior.\nIf you can't construct a working object, why return an object of that class from Aws::EC2::RouteTable#create_route? Returning a simpler struct (with appropriate doc changes) would be preferable to a broken object in my opinion.\n. https://bugs.ruby-lang.org/issues/12688\n. ",
    "tdg5": "Hey @trevorrowe, did you ever create an issue with Ruby Core? I couldn't find one so I created this one. Let me know if you created one previously so I can remove the dup. Otherwise, feel free to contribute if there's anything you want to add or disagree with.\nI also made a GitHub PR here: https://github.com/ruby/ruby/pull/1019\nFor now, we've also gone the way of clearing IDEMPOTENT_METHODS_, but this seems like overkill so it'd be nice to get a fix going upstream.\n. Oh duh, yeah I didn't even think to check GitHub. I found https://github.com/ruby/ruby/pull/951, thanks!\n. ",
    "apurvis": "thx; i'll check it out when a release comes out\n. > It looks like the stack trace (relating to your code) is out of date.\nit probably is, but you can recreate by checking out the latest and running specs.\ngit clone git@github.com:lumoslabs/broadside.git\ncd broadside\ngit checkout force_task_definition_update\nbundle install\nbundle exec rspec\nthe spec won't fail because it is pending but you will see the stack trace.\n\nHave you been able to isolate a smaller failing example?\n\ni have not, sorry, though this behavior has been consistent.\n. Thanks!  I think you do validate some response shapes, but I guess not this one, which is confusing.\nWhile i'm here, is it possible to see the past requests made to the stubbed endpoint in anyway?\nLike if a block of code is run, is there a way to see that the calls were like list_task_instances, describe_task_instances(some_arguments) and maybe see what those arguments were?\n. thanks that's very helpful\n. can't you just use Object#put(body: io_object)?  that supports both file and stringIO objects. That's helpful and yeah, the documentation is completely unhelpful on this subject, which is a bit of a problem as users of the gem are encouraged more and  more to use the Resource classes instead of the Client classes.\nMy question then becomes: \"How could I stub a particular set of objects being returned from a particular bucket?\"  e.g. I just want to be able to stub that [s3://bucket/some/path/file1, s3://bucket/some/path/file2] exist in a bucket.  I'm playing with this now but any help is appreciated.. i got it to work; would be awesome if you could update the documentation to include this but i'm good for now.. that part of the developer guide is basically useless for understanding how to stub a Resource; that's why I created this issue.  . ",
    "samjdacanay": "Thanks Trevor. I appreciate the clarification!\n. ",
    "philipmw": "So in lib, there's aws-sdk-v1.rb, but no aws-sdk.rb, which would be necessary to require 'aws-sdk'.\nIn contrast, tag v1.64.0 does have lib/aws-sdk.rb.\n. ok, thanks for clarifying.\n. ",
    "ShahriatHossain": "I am saving presignedUrl into our database and when I browse the url 10 days later or more the url is broken that means expired, can I make this enable for forever use? or has any alternate of presigned url so that I can direct show the file from amazon to our app like the presign work but without expire time or date?. ",
    "praveenv66": "@ShahriatHossain Did you find a solution to generate pre-signed url which doesn't expire or any possible maximum value? I'm using the below code to generate presigned url. client.generate_presigned_url('get_object',Params = {'Bucket': bucketName,'Key': sourceFilePath },ExpiresIn = 7200). ",
    "lssachin": "this can happen when your local system is behind AWS S3's time. this can happen when your local system is behind AWS S3's time. ",
    "jfcsantos": "@lssachin Did you confirm this? \nI'm doing concurrent multipart upload of a large file and trying to renew credentials if they expire....getting this error at some point. \nMy test environment is running locally on probably +1h more than S3.. ",
    "rustyautopsy": "Awesome! Thanks.\n. ",
    "rdark": "opening connection to cloudformation.eu-west-1.amazonaws.com:443...\nopened\nstarting SSL for cloudformation.eu-west-1.amazonaws.com:443...\nSSL established\n<- \"POST / HTTP/1.1\\r\\nAccept: */*\\r\\nAccept-Encoding: \\r\\nAuthorization: AWS4-HMAC-SHA256 Credential=ACCESS_KEY_REDACTED/20150515/eu-west-1/cloudformation/aws4_request, SignedHeaders=content-type;host;user-agent;x-amz-content-sha256;x-amz-date, Signature=437eda00c6b5b55e4cbeeb39c5f4205d3023ca63425f94b14cb162ca4e86b07a\\r\\nContent-Length: 29642\\r\\nContent-Type: application/x-www-form-urlencoded; charset=utf-8\\r\\nHost: cloudformation.eu-west-1.amazonaws.com\\r\\nUser-Agent: aws-sdk-ruby2/2.0.42 ruby/2.0.0 x86_64-linux\\r\\nX-Amz-Content-Sha256: a9722ebd2afe1b7036a120c38c5d2d1f4552d0b45b9e6ff5805701b5fac7f29b\\r\\nX-Amz-Date: 20150515T083825Z\\r\\n\\r\\n\"\n-> \"HTTP/1.1 200 OK\\r\\n\"\n-> \"x-amzn-RequestId: c08d757f-fadd-11e4-aba0-bd7dba9b773b\\r\\n\"\n-> \"Content-Type: text/xml\\r\\n\"\n-> \"Content-Length: 388\\r\\n\"\n-> \"Date: Fri, 15 May 2015 08:38:25 GMT\\r\\n\"\n-> \"\\r\\n\"\nreading 388 bytes...\n-> \"<CreateStackResponse xmlns=\\\"http://cloudformation.amazonaws.com/doc/2010-05-15/\\\">\\n  <CreateStackResult>\\n    <StackId>arn:aws:cloudformation:eu-west-1:ACCOUNT_NUMBER_REDACTED:stack/cont-cros-uact-001/c0a235d0-fadd-11e4-8882-5088487db896</StackId>\\n  </CreateStackResult>\\n  <ResponseMetadata>\\n    <RequestId>c08d757f-fadd-11e4-aba0-bd7dba9b773b</RequestId>\\n  </ResponseMetadata>\\n</CreateStackResponse>\\n\"\nread 388 bytes\nConn keep-alive\n. Hi Trevor,\nThanks for the offer! To save you the effort I'll play around with a few things and maybe knock up a Vagrantfile at some point this weekend to try get a reproducible environment for you.\nCheers,\nRichard\n. So this one was pretty fun to track down - turns out that it was a namespacing issue.\nMy project had a Util module also, and there were three calls to Seahorse::Util methods that were not qualified. The above commit fixes this issue - will open a PR if the above commit is OK by you?\nEDIT: incorrect - I was still rescuing the error in my test code - above commit does not fix the error. sigh\n. Issue turned out to be in a gem that I was using for cloudformation template generation (lono) that was extending/overriding core methods of the Hash object.. \nThanks for your help and sorry for the red herring.. \n. Hi Trevor - not sure if you saw my last message, but the issue turned out not to be in aws-sdk, but in another gem that I was using in my project that was polluting core Hash methods.\n. ",
    "gdanko": "I wonder if it is possible to create a KMS data key and use that to decrypt a file encrypted in Java:\ndek = kms.generate_data_key(\n    key_id: key_id,\n    key_spec: \"AES_256\"\n)\ns3_enc = Aws::S3::Encryption::Client.new(\n    client: s3,\n    encryption_key: dek[\"plaintext\"]\n)\nYou are effectively providing a KMS key as your encryption_key. Maybe?\n. ",
    "keviny22": "@trevorrowe I'm really interested in this feature being added. Anything I can do to help move this feature along?\n. ",
    "andykent": "Hmm, I think this might have been premature. it seems like it's actually maybe needed for ELB support? \nSo maybe it's functionally correct but the docs are just quite unclear here?\n. ",
    "sauravhaloi": "Some of the fields such as URL, instance id etc has been removed from the snippet for privacy reasons\n. ",
    "loto": "Our mobile app works with an API, the app request URL from the server and upload files on S3. So far it worked perfectly.\n. We're not using any form, this is why we choose this approach in the first place. Here's a more detailed example of what we're doing:\n- the app needs to save a log file on S3, it sends a request to the API which returns a presigned url and a unique file name\n- the app renames the file and uploads it on S3\nDoes it help anyhow?\n. ",
    "pamio": "@trevorrowe, I solved the issue by creating a bucket manually on s3 and then it worked. Thanks for the reply. \n. @trevorrowe, thats exactly what happened. Here's what happened. \nI was creating the bucket from my script. \ns3 = AWS::S3.new\nif !s3.buckets['gs-db-backups'].exists?\n   bucket = s3.buckets.create('backups')\nend\nfile_name = \"dump-#{DateTime.now.strftime('%Y-%m-%d-%H-%M')}.zip\"\nfiles = Dir[\"dump/**/*\"].map { |file| file }\n`zip #{file_name} #{files.join(\" \")}`\nfile = File.open(file_name, \"rb\")\nobj = bucket.objects[file_name.split(\".\").first]\nobj.write(file: file)\nNow, the above script always threw the above said error until I created a bucket manually on s3 by logging into s3 console. After I did that It was successfully uploading the file on s3. \n. Sorry - my bad. I had corrected that already!  And posted my reply. BTW the issue is resolved now. After I corrected the name of the bucket, I was still getting the error. As I said, creating a bucket manually solved my problem. \n. ",
    "alkesh26": "@trevorrowe I am very thankful for your response. Can you please help me more in understanding the error. May be with an example or some link. I am unable to identify the root cause. \nIt will be of great help. And thanks a lot once again. Your comment made my day. :) \n. ",
    "ianneub": "Wow, you're fast @trevorrowe ! Thanks for the fix!\n. ",
    "BilalBudhani": "@trevorrowe Thanks for posting a brief explanation on this. I will try this out and update the issue with the conclusion. \nI would like to use DynamoDB local but, it just got launched few months back and lot of libraries / gems are still using older version of aws-sdk which doesn't support DynamoDB local. These constrain makes it really hard to migrate. \n. ",
    "davidcpell": "\nAs far as I know, none of the AWS SDKs, except the CLI have a filesystem to S3 sync function.\n\nHi, just wanted to stop by and add my :( to this. Sync functionality has been available in the PHP SDK since 2013. The Java SDK also has this/something similar.. Thanks @awood45! Really love the built-in stubbing feature by the way.. Thanks @awood45. Can you point me to where I should submit PRs like this in the future?. ",
    "shaunakv1": "Same here.. just stopping by again to add my :( \nThis is a really use full feature to have in sdk, so we don't have to install cli dependency on hosting, and our background jobs can run purely on gem dependency.\nAdding this feature will make it whole lot easier to do PaaS deployments on system like Heroku. \n. ",
    "der-flo": "Thanks for version 2.0.48!\n. ",
    "jbcden": "Failure/Error: s3 = Aws::S3::Client.new(stub_responses: true)\n     WebMock::NetConnectNotAllowedError:\n       Real HTTP connections are disabled. Unregistered request: GET http://169.254.169.254/latest/meta-data/iam/security-credentials/ with headers {'Accept'=>'/', 'Accept-Encoding'=>'gzip;q=1.0,deflate;q=0.6,identity;q=0.3', 'User-Agent'=>'Ruby'}\nYou can stub this request with the following snippet:\nstub_request(:get, \"http://169.254.169.254/latest/meta-data/iam/security-credentials/\").\n         with(:headers => {'Accept'=>'/', 'Accept-Encoding'=>'gzip;q=1.0,deflate;q=0.6,identity;q=0.3', 'User-Agent'=>'Ruby'}).\n         to_return(:status => 200, :body => \"\", :headers => {})\n. @trevorrowe That seems to have fixed most of it. Thanks so much! \n. ",
    "slpsys": "Awesome, thanks! Yeah, that approach seems sensible to me as well, but obviously your call.\n. ",
    "jonaf": "Why have an option to validate parameters if you do not have the context to validate them properly? It would be much more fruitful to either (a) remove the option altogether, thereby implying that such validation should be done (purely) by whomever is using the SDK; or (b) support fully and sensibly the notion of validating parameters. If you must do it the current way, it would make more sense to try to re-cast the value as a String, and only throw a validation error if casting fails.\nUltimately, the CloudFormation API will perform validation on the parameter value (and will do this conversion to a string) anyway, so the context is important for complete validation based on the template, but all parameters will be converted to Strings first... So it would be fine if you validate everything as a string as you currently do, but instead of validate, just convert the type (if possible, and only throw an error if it's not possible). Since CloudFormation will convert all inputs to Strings and then validate them as whatever is the acceptable type (i.e., a \"Number\" type may be an integer or float), I think it would be OK for the SDK to behave identically: coerce the type into a String (if not possible, raise a value error) and then simply pass it along to the CloudFormation API to perform real validation. In that case, the validate_params option is obsolete.\n. ",
    "dirkdk": "you are wrong I am afraid, the OP is linking to the right v2 documentation\n. Sure.\nI upgraded from 2.1.0 to 2.1.9, and now works for me:\ntemp_object = S3_BUCKET.object(temp_key)\ntarget_object = S3_BUCKET.object(target_key)\ntemp_object.copy_to(target_object)\n. ",
    "thebravoman": "Turned out it is an issue with the regions. If the region is in Europe your are close to doomed with \"other things you should do\". Removed the bucket and created it in Oregon.\n. ",
    "babakgh": "I am using version 1.66.0, have same problem only with region ap-northeast-2. unfortunately upgrading to version 2 is not an option right now\n. ",
    "voy": "I have stumbled upon the very same thing using the Java SDK. The api is returning \"available\", while the AttachmentStatus enum only knows attaching/detaching/attached/detached. It is strange, I ended up just using my own \"available\" string constant.. ",
    "reppard": "Should have updated sooner.  Was running a 2.0.x version of the gem.  I blasted my Gemfile.lock and bundle installed and all was well.  Thanks!\n. I did want to add a little feedback in regards to wait_until on the replication groups.  I've got the max retry and delay set to the maximum allowed values and it still hits the limit.  I'm creating just 2 small cluster nodes and calling wait_until before I create my route53 entry to point at the replication group.  Maybe the max retries could be increased for :replication_group_available?\n. I don't mind at all.  Thanks for the help!\n. ",
    "tyler-ball": "Then this seems like an issue with the Amazon EC2 API.  It is returning a message saying that 'yes, the instance does exist' but then fails when trying to update that existence.  Perhaps there are multiple meanings of the word existence from the API?  An Object with an ID has been created, but it doesn't have a state of existence that allows an update to it?\nWhatever the answer is, the current state of this API is still very confusing.\n. As a follow up to this, also found this code block failing:\nruby\nserver = ec2.create_instance(instance_data)\nec2.client.wait_until(\n  :instance_exists,\n  :instance_ids => [server.id]\n)\nThe wait_until is failing with the error Aws::Waiters::Errors::UnexpectedError stopped waiting due to an unexpected error: The instance ID 'i-314ab4c7' does not exist\nBut isn't that why I'm calling the waiter?  I want it to poll until that instance exists?\n. We are using version 2.1.11 (just for more information).  I'll add the block you suggested and see if I can get some more information, but you can see https://github.com/test-kitchen/kitchen-ec2/pull/184 and grep the page for InvalidInstanceIDNotFound to see that its still throwing that error.\nAfter re-reading our two stack traces it looks like we have seen two different exceptions.  First we saw Aws::Waiters::Errors::UnexpectedError with the message stopped waiting due to an unexpected error: The instance ID 'i-314ab4c7' does not exist\nThen I switched from using ec2.client.wait_until to using instance.wait_until_exists - they both call the same AWS API though (describe_instances), correct?\nNow we see the error Aws::EC2::Errors::InvalidInstanceIDNotFound with the message The instance ID 'i-c23dfb34' does not exist\n. I figured out why I was seeing UnexpectedError - the :instance_exists waiter automatically retries InvalidInstanceIDNotFound errors while the  :instance_running waiter does not.  I was switching between :instance_exists and :instance_running trying to get my system working, so I must have seen UnexpectedError wrapping the InvalidInstanceIDNotFound error when using the :instance_running waiter.  This was my incorrect assumption about the waiters - I though that :instance_running would retry the NotFound error the same way the :instance_exists waiter did.  #themoreyouknow\nBut I still see my original issue - the :instance_exists waiter has returned successfully (meaning that the #describe_instances call returned successfully).  However, I then try to tag the instance and I get the InvalidInstanceIDNotFound error.\nI understand that the instance may not be taggable yet, but it still seems very weird that it is returning a NotFound error when the #describe_instances call was just successful.\n. @trevorrowe Thanks so much for all the support!\n. Thanks for the quick turnaround @trevorrowe !\n. Since filing this I haven't reproduced it locally.  I'll close this and re-open with more details if it reappears.  Thanks!\n. We run integration tests in Travis and they fail more than 50% of the time with this error, checking exists? on a Instance Resource.  We have a different project which runs integration tests in Travis and does not see this error.  Test Kitchen runs in a concurrent threading model.\n. Thanks @trevorrowe and @awood45 !\n. Thanks for the research @trevorrowe - I'll update the tests on our side\n. Thanks @awood45 - I looked at other places we were catching the NoSuchEntity error and it was because we were calling .load. Lazy loading FTW\n. Thanks @awood45 !\n. ",
    "DianthuDia": "Yes, documented examples use an authorization header. But please check beginning of the description.\nhttp://docs.aws.amazon.com/cognitoidentity/latest/APIReference/API_GetId.html\n\nThis is a public API. You do not need any credentials to call this API.\n. Thank you for merging and adding test :yellow_heart: \n. \n",
    "dtbartle": "@trevorrowe ping?\n. Thanks!\n. @trevorrowe ping?\n. ",
    "djcp": "Thanks!\n. yep, happens during \":db_instance_available\" too.\n. I think you're right that this was the service. I've not see it occur in quite a while. Thanks for checking in, I'll close.\n. Yeah, we saw it yesterday too for RDS.\n. ",
    "richardboydii": "After doing serveral series of wire traces I'm flummoxed but happy to report that I cannot reproduce. I'm going to close the ticket.\n. ",
    "ktheory": "Thanks, @trevorrowe. You rock!\n. That works like a charm. Thanks for explaining, @trevorrowe!\n. ",
    "sbull": "I believe this regression happened here: https://github.com/aws/aws-sdk-ruby/commit/62766c9a2ce4fae36f414f76bbb88c1758004840 (apparently to fix https://github.com/aws/aws-sdk-ruby/issues/816). But there appears to be some back-and-forth with the headers in general (https://github.com/aws/aws-sdk-ruby/commit/4ee9c952b21c4d011b6763c227d3471cbcd8f3dd, https://github.com/aws/aws-sdk-ruby/commit/72535aaaf4e9108861322b00bcd22f1694c0102f, https://github.com/aws/aws-sdk-ruby/commit/2eae71f285c041f78efd7f38048f4830424964b3, https://github.com/aws/aws-sdk-ruby/issues/543).\n. Curious... is there a reference implementation for the AWS SDKs? Perhaps the Java one? Looks like it's handled here (including SSE headers, ...): https://github.com/aws/aws-sdk-java/blob/948bc1a418d357abf8fd46cc19123ced2e014ce0/aws-java-sdk-s3/src/main/java/com/amazonaws/services/s3/AmazonS3Client.java#L2563\n. When I had tried just removing the headers originally, I was getting a generic authorization error message. I have a bucket policy set up to require that files are encrypted:\n{\n            \"Sid\": \"DenyUnEncryptedObjectUploads\",\n            \"Effect\": \"Deny\",\n            \"Principal\": \"*\",\n            \"Action\": \"s3:PutObject\",\n            \"Resource\": \"arn:aws:s3:::.../*\",\n            \"Condition\": {\n                \"StringNotEquals\": {\n                    \"s3:x-amz-server-side-encryption\": \"aws:kms\"\n                }\n            }\n        }\nI'm guessing that when your object is PUT in your example above, it's not actually being encrypted. But I'll have to try again.\nPolicy reference: http://docs.aws.amazon.com/AmazonS3/latest/dev/UsingKMSEncryption.html\n. Here's what I get using mostly your example code (different bucket, object key, credentials):\nopening connection to <bucket>.s3.amazonaws.com:443...\nopened\nstarting SSL for <bucket>.s3.amazonaws.com:443...\nSSL established\n<- \"PUT /123/456/secret?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=...&X-Amz-Date=20150724T094043Z&X-Amz-Expires=900&X-Amz-SignedHeaders=host&x-amz-server-side-encryption=aws%3Akms&x-amz-server-side-encryption-aws-kms-key-id=...&X-Amz-Signature=c146b9e31433793f7ce1c498e4cf34f7c9e99b361e19bb2637af8e0307a76c78 HTTP/1.1\\r\\nAccept-Encoding: gzip;q=1.0,deflate;q=0.6,identity;q=0.3\\r\\nAccept: */*\\r\\nUser-Agent: Ruby\\r\\nConnection: close\\r\\nHost: <bucket>.s3.amazonaws.com\\r\\nContent-Length: 22\\r\\nContent-Type: application/x-www-form-urlencoded\\r\\n\\r\\n\"\n<- \"secret-body-1437731099\"\n-> \"HTTP/1.1 403 Forbidden\\r\\n\"\n-> \"x-amz-request-id: E6C878ADCC13BDBF\\r\\n\"\n-> \"x-amz-id-2: 3aXhZKqZnNRdh+3nE/DPIlscAn5VDcx7Li1CIvnbL5R6yyyKYYwkbXCSyTzauec3\\r\\n\"\n-> \"Content-Type: application/xml\\r\\n\"\n-> \"Transfer-Encoding: chunked\\r\\n\"\n-> \"Date: Fri, 24 Jul 2015 09:44:54 GMT\\r\\n\"\n-> \"Connection: close\\r\\n\"\n-> \"Server: AmazonS3\\r\\n\"\n-> \"\\r\\n\"\n-> \"e7\\r\\n\"\nreading 231 bytes...\n-> \"<?xml version=\\\"1.0\\\" encoding=\\\"UTF-8\\\"?>\\n<Error><Code>AccessDenied</Code><Message>Access Denied</Message><RequestId>E6C878ADCC13BDBF</RequestId><HostId>3aXhZKqZnNRdh+3nE/DPIlscAn5VDcx7Li1CIvnbL5R6yyyKYYwkbXCSyTzauec3</HostId></Error>\"\nread 231 bytes\nreading 2 bytes...\n-> \"\\r\\n\"\nread 2 bytes\n-> \"0\\r\\n\"\n-> \"\\r\\n\"\nConn close\n=> #<Net::HTTPForbidden 403 Forbidden readbody=true>\nIf I remove the bucket policy restriction described above, it works:\n-> \"HTTP/1.1 200 OK\\r\\n\"\n-> \"x-amz-id-2: y8q3rybqs5+GmhKtOXB2CMLZ3m3AHONdjCEHETXfUr5sJ0qdxYNVopmRCHwo5bFe\\r\\n\"\n-> \"x-amz-request-id: 1D8720EAF0710E9F\\r\\n\"\n-> \"Date: Fri, 24 Jul 2015 09:55:57 GMT\\r\\n\"\n-> \"x-amz-server-side-encryption: aws:kms\\r\\n\"\n-> \"x-amz-server-side-encryption-aws-kms-key-id: ...\\r\\n\"\n-> \"ETag: \\\"359201a47ab0c889dc9126b7ebc282b1\\\"\\r\\n\"\n-> \"Content-Length: 0\\r\\n\"\n-> \"Server: AmazonS3\\r\\n\"\n-> \"Connection: close\\r\\n\"\n-> \"\\r\\n\"\nreading 0 bytes...\n-> \"\"\nread 0 bytes\nConn close\n=> #<Net::HTTPOK 200 OK readbody=true>\nI need to have a bucket policy that restricts to aws:kms encrypted file uploads. So, is there a way to write a bucket policy that checks the URL parameters? If not, I need to use the headers.\n. It sounds like that would work, if I understand correctly. I'd get a URL that looks like it did previously (...&X-Amz-SignedHeaders=host%3Bx-amz-server-side-encryption%3Bx-amz-server-side-encryption-aws-kms-key-id&...), and a hash containing { 'x-amz-server-side-encryption' => 'aws:kms', 'x-amz-server-side-encryption-aws-kms-key-id' => ... }, correct? If so, then that seems like a fine workaround.\nIt would be great if this could be mentioned in the CHANGELOG too.\n. ",
    "tyrauber": "This issue appears to have resurfaced in 2.3.4.  \ns3 = Aws::S3::Resource.new(region:ENV['S3_REGION'])    \nobj = s3.bucket(ENV['S3_BUCKET_NAME']).object(\"#{Time.now.strftime(\"%Y/%m/%d/%H\")}#{SecureRandom.hex}\")\nurl = URI.parse(obj.presigned_url(:put, expires_in: 36000, acl: 'public-read'))\nResults in following request URL:\nhttps://bucketname.s3-us-west-2.amazonaws.com/2016/05/13/110d88197423e3bd5d868528cdf89e2ac5?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AWSKEY/20160513/us-west-2/s3/aws4_request&X-Amz-Date=20160513T173731Z&X-Amz-Expires=36000&X-Amz-SignedHeaders=host&x-amz-acl=public-read&X-Amz-Signature=cb47754a8d22d35c9f696a552772e70793dfd3a0f44c0643cd3d530b2a8f4e1e\nWhich results in the error:\n<Error>\n<Code>AccessDenied</Code>\n<Message>There were headers present in the request which were not signed</Message>\n<HeadersNotSigned>x-amz-acl</HeadersNotSigned>\n<RequestId>F81F5AE6AE7F66C8</RequestId>\n<HostId>eR+lfCsQFb/vGUtc2A6bQI263Q2L9Jip5VU9FmhLWEg73+l0kvrxLfroPa0MfmUUvUI54O5XddE=</HostId>\n</Error>\nDowngrading to 2.0.39 solves the issue.\nI noticed in 2.0.39, the url has a semicolon after the X-Amz-SignedHeaders? Is the lack of the semicolon what is causing the error in 2.3.4?\n&X-Amz-SignedHeaders=host;x-amz-acl&X-Amz-Signature=00f4b33d45d314f63ed7e3a58c4a07bcd847fdd474d2f91f073a200bcd047c93\n. Hi @trevorrowe, is it possible that these issues were fixed in Ruby 2.5.0, or the patch needs to be updated for Ruby 2.5.0?  I am seeing a whole string of failures related to a conflict between the fastly-rails gem and aws-sdk-core, over the shared use of net::http.  https://github.com/aws/aws-sdk-ruby/issues/1752 Any suggestion on how I resolve this conflict?\n. Hi @cjyclaire, I vendored both the aws-sdk-s3 and aws-sdk-core gem and deployed to production in order to test the pull request.  #1756 continues to err out (only through the ruby_1_9_3 module). #1759 addresses this issue by not applying the patch to ruby 2.5.0 or greater.\n. hi @cjyclaire I believe you need RUBY_VERSION <= \u20181.9.3\u2019 < \u20182.0'. Otherwise the ruby_1_9_3 module will be applied to 2.5.0 or greater.  https://github.com/aws/aws-sdk-ruby/pull/1759. ",
    "cjyclaire": "Soft ping here. PR #1477 just opened addressing this feature request : )\nFeel free to chime in and add comments : ). @mcfiredrill Sorry to hear that you have to work around the problem by downgrading. So this is a known limitation from S3 side, we are actively working on a feature that allows flexible customizable presigned behavior in the long run. Meanwhile there is aws-sigv4 available for customized signing and presigned requests.\nSo here are some work-around with aws-sigv4\n`` ruby\nrequireaws-sigv4`\na s3 client at region 'us-west-2'\nsigner = Aws::Sigv4::Signer.new( \n  service: 's3', \n  region: 'us-west-2', \n  credentials_provider: client.config.credentials, \n  uri_escape_path: false, \n)\ncreate presigned url for an object with bucket 'a-fancy-bucket' and key 'hello_world'\nurl = signer.presign_url( \n  http_method: 'PUT', \n  url:'https://a-fancy-bucket.s3-us-west-2.amazonaws.com/hello_world', \n  headers: {                                                                                                                           \n     \"Content-Type\" => \"audio/mpeg\",                                                                                                      \n     \"x-amz-acl\" => \"public-read\"                                                                                     \n  } \n  body_digest: 'UNSIGNED-PAYLOAD', \n) \nmaking request\nbody = ...\nNet::HTTP.start(url.host) do |http|\n  http.send_request(\"PUT\", url.request_uri, body, { \n    \"content-type\" => \"audio/mpeg\", \n    \"x-amz-acl\" => \"public-read\", \n  })\nend\n=> #\n```. This feature is already supported in SDK now, you can decrypt windows instance password by:\n``` ruby\nAfter you have an available running instance\nec2 = Aws::EC2::Resource.new(region: 'us-west-2')\nkey_path = \"/path/to/your/private/key\"\ndecrypted_password = ec2.instance('your-instance-id').decrypt_windows_password(key_path)\n```\nCheers :)\n. Soft ping here. PR #1477 just opened addressing this feature request : )\nFeel free to chime in and add comments : ). Soft ping, the feature PR has been merged, and will be tagged in today's release : ). Fix updated. @trevorrowe \nYet URI.parse() cannot be applied because CloudFront URLs need to handle wildcard characters* while URI module doesn't.\n. Good point, make sense!\n. @phstc could you provide more information for the usage scenario? I'm going to work on this feature request, so it would be nice if you could provide more information. \nFor example, How did you receive this JSON output? Perhaps you are integrating DynamoDB stream with Lambda? Or it comes from GetRecord api of DynamoDB stream?\nCheers!\n. @phstc , good to know, thanks!\n. Agreed on preserving backwards compatibility. \nI'd prefer the 2nd option while treats \"structs-to-vanilla-hash\" as an opt-in feature. In this way, the consistency in response is maintained and vanilla hash enhancement is also available.\n. That's true, in terms of usage, the 1st option does make more sense.\n. Since this feature has been unblocked, PR is updated and ready for another review.\n@trevorrowe @awood45 \n. @awood45 Good suggestions, buffering everything into memory is definitely bad. I made changes and clean up the code, ready for a second review now : ).. @awood45 I updated this PR with multithreading fix, ready for another review : ). Updated, ready for another review ; ). This PR is updated and ready for another review.\n@trevorrowe , @awood45 \n. This PR is updated and ready for another review @awood45 @trevorrowe \n. @awood45 Sounds good, make sense. I'll add that : ). PR #1477 just opened includes this feature, we'll focus on that one : )\nClosing, feel free to chime in and add comments in that PR : ). Apology for the late reply on this, cloud front url signer has been added support :) \nCould you resolve the conflicts first then we can take a look at your PR?. Wow, the history log is blowing out interestingly, could you clean those up a bit?\nOr perhaps, would it be easier for you to open a new PR what includes Cookie Signer only aganist master? : ) Just @ me once it's done, I'll do the review for it. Appreciate that!. No worries, thank you : ). Closing this for now, waiting for your new PR to be reviewed : ). Closing, separate PR #1306 is opened\n. Looks like the fix is already merged :)\n. This PR has been updated and ready for review.\n. @mziwisky Interestingly, when I try to reproduce a cross region #copy_to call, it fails with\nAws::S3::Errors::PermanentRedirect The bucket you are attempting to access must be addressed using the specified endpoint. Please send all future requests to this endpoint.\nIt's expected because I was trying to copy my object to another region.\nI'm curious, could you explain a bit how you find copy_to works cross region for you?. @mziwisky Yeah, #copy_from works without problem. No worries, appreciate the feedback, happy to see if you have some interesting findings, then we could make our documentation better : ). Closing due to ages, yet feel free to reopen anytime once you have further details and interesting findings ; ). Thanks for the PR!\n. @jaredbeck Appreciate your thoughts on this PR! Yet UPGRADING.md is the file used to tracking release note automatically for V2. Instead of having the note here, we already mentioned MIGRATING.md in main README.md. Thank you all the same!\n. @jaredbeck Apology for this, taking a second look, it's CHANGELOG.md is automated actually.\nI'll merged this PR instead. Appreciate your patience on this!\n. Closing, this issue goes away in V3 documentation(which is currently under preview and is backward compatible with v2 doc). Adding some side notes: in V3, service errors are auto generated based on its shape from DynamicError module, and binds to its operation.. This PR has been reviewed offline, merging.. Closing tracking with #1505 instead : ). @pdrakeweb Thanks for the contribution! \nYet this file is automatically generated, I'll make those changes internally from root to address the need. Will be tagged in the following release after it merged. I'll ping there when the fix is done.\nThanks : ). A tagged release containing the fix is out, soft ping.. PR #1477 is opened that supports this feature, tracking this feature request in that PR : )\nClosing, feel free to chime in that PR with comments : ). Tracked in backlog, meanwhile happy to take a PR for review and pull upstream : ). @krishnan-mani  The waiter files in SDK are automatically generated. And this waiter has been added from root, will be included and tagged in release soon! Closing, I'll ping the SIM once it gets out : ). @sharang-d For S3 Virtual Hosting of Buckets, it desgins to address a bucket in a REST API call by using the HTTP Host header. That's why scheme is set to http.\nAppreciate your feed back on this, may I ask is there any difficulty or blockers for you not to use virtual-host for https? (path style and DNS are still available)?. Closing due to inactiveness, yet feel free to reopen with further details or comments : ). @sharang-d No problem at all,  happy to reconsider this as a feature request with more information :).\nSo the reason why I'm curious and need more information is that, for S3 virtual hosting bucket style, the scheme is set to http, it's intentional. I'd like to hear more about why you would want https for virtual host style bucket. If you just want https, you can try :force_path_style.. @sharang-d \ns3obj.public_url(virtual_host: true)\nThis overwrites client config, could you try remove the :virtual_host there?. @sharang-d I understand that you want https with virtual host style, however, as in my first comment, S3 Bucket VirtualHosting has limitations with :https scheme:\n\nWhen using virtual hosted\u2013style buckets with SSL, the SSL wild card certificate only matches buckets that do not contain periods. To work around this, use HTTP or write your own certificate verification logic.\nSpecifying the bucket for the request by using the HTTP Host header is supported for non-SSL requests and when using the REST API\n\nThat's why it's set to http, thus I'm curious about your requirement originally, if you still want ':https' with virtual host, I can make this in the feature backlog as an optional parameter. Does that sound good to you? : ). Taking a look .... @mwalsher Apology for the late reply  and appreciate your patience!\nYour code snippets looks good to me, may I ask the gem version used? Recently, there have been a small fix about multiple params #1386, could you try the latest version to see if that works?\n. @papadeltasierra Thank you for the feedback! So to clarify first, it's true that s3_client.put_object(..) is returning the response for put_object API call, it has nothing to do with resource objects. \nMeanwhile, it might appear easy to make the client accessible from resource object, yet taking a second thought, it's kind of mixing and chaining the OO style for the resource concept. Also, now it's quite easy to have an S3 resource object like Aws::S3::Object.new(bucket_name: 'a-fancy-bucket', key: 'wow') :), could you explain more about why it could be a pain when the conversion is not available? . @papadeltasierra Thank you for the description above, appreciate that!\nThere is a lot going on, so before providing solutions or tag this as feature request, just quick clarify to see if I understand the situation correctly : ) \nYou are using S3 Multipart upload and would like to have an S3::Object as result output to utilize the public_url() method. There is something I'm still a bit confused, since you could already to upload with resources like \nruby\n  obj = Aws::S3::Object.new(..)\n # reference: https://github.com/aws/aws-sdk-ruby/blob/master/aws-sdk-resources/lib/aws-sdk-resources/services/s3/object.rb#L217\n  obj.upload_file( ... ) # This integrates with Multipart with configuration options\nWhy are you suggesting the object is not available? Maybe as you have mentioned, you are managing multipart upload all by yourself with client api calls? (If this is the case, parameters like bucket or key could be accessed from resp.context.params. )\nAlso, a quick note for download, we do have PR #1213 under review, which will introduce the multipart downloader :)\nAppreciate your patience on this :)\n. Looks like questions have been answered, closing. Yet feel free to reopen with further questions. : ). @miensol Interesting, taking a look to see if I could reproduce the issue. Thanks for the information.. @miensol  Adding updates, I couldn't reproduce the issue with \nruby\n{\"namespace\"=>\"aws:elasticbeanstalk:command\",\"option_name\"=>\"DeploymentPolicy\",\"value\"=>\"RollingWithAdditionalBatch\"}\nwith update_environment API, the policy has been updated successfully.\nLooking at the config file provided, I don't see anything that is obviously wrong. So I'm curious what's happening. So may I ask more information? For example, is the update fails with DeployPolicy only? Are there other updates failures? Which version of SDK is used? Is there any error log that you encountered when performing the request?. @miensol Interesting, I'm wondering that might be the problem. \nDid you get any error message when updating VPC alone? May I ask is your environment sits in a VPC already? If not, you might need to do some migration first before updating. If that's the case, you might see an ConfigurationValidationException error.\nIf not, perhaps the environment has the status \"Updating\" when you issued another request? It can only be updated when its status shows \"Ready\".\nHope those helps. @miensol Thanks for the information! So just to clarify on the issue before I dive deeper or contact service team about this behavior, are you suggesting that, for the option_settings parameter, it works when entries included are updated ones (instead of all options) ? Or you find that VPCId is the only one that causes the problem?. @miensol Thanks for the information! I'm having trouble in reproducing the RollingWithAdditionalBatch issue. I've contact service team and let them know about this behavior for verification. And I'll keep you posted, meanwhile, our #GettingHelp section has been updated, feel free to engaged in other channel as well : ). I got information from service team that a support ticket has been opened for this. Happy to know that other resource channel is engaged actively : ) If they need any thing from us, they will let us know. Closing, yet feel free to reopen with further questions or needs  : ). Note that failure is jruby and irrelevant with this change.. Thanks for the contribution! Could you also add some rspec test for the change as well?. @cmarodrigues , That's true, taking a second look, we already have similar checks at dns_compatible. And S3 DNS bucket style do have stricter requirement. I'd suggest using path style (which is recommended) there.\nClosing, same great thanks for your thoughts in contributions : ). This has been tracked in backlog to be prioritized : ). Appreciate your patience! Closing, tracking this feature with PR #1477 instead : ). Closing, tracking this feature with PR #1477 instead : ). PR #1477 has opened including support for this PR, closing, tracking in #1477 instead.\nThe spec test has been included as well.\nFeel free to chime in and add comment in that PR, thanks : ). Thanks for getting back! Could you also elaborate a bit more on\n\nthey started breaking when I upgraded the latest version of the gem\n\nAre you noticing any breaking changes that is not backwards compatible?. @ryanbrainard Thanks for reporting! Looks like ec paginators is missing DescribeNatGateways exactly, that's the cause. This is definitely something we need to fix. Yet introducing pagination behavior for existing APIs might be a breaking change to other customers. We will try to figure out a way to fix this from root.\nMeanwhile, as quick workaround, you could try response.to_a.each .. or  response.to_h.each ... Though in this way, other benefits of paginators will not be enjoyed.. A tagged release containing the fix is out, closing.. @Undo1 Ah interesting, I tried the example and it works for me actually (without using . iso8601).\nSo I'm curious about what's going on, could you also tell me your OS environment, ruby version and SDK version? Also, we just updated our Getting Help section in README.md, free feel to get engaged in other channel as well :).. Some updates, pagination has been enforced from root for future new APIs. Yet for those existing APIs missing paginations, we might still need to do manual works to fix those.\nFree feel to open new issue if any pagination behavior missing causes unpleasant experience for you, we will fix those. Apology for those inconsistencies so far. Closing, feel free to reopen with further comments or questions. Thanks! . I have tested this locally and the cross_link is basically same with what we currently have.. This has been fixed in V3 documentation (which is compatible with v2 docs).\nClosing : ). Note that CI is failing for ruby 1.9.3 only, working on a workaround for that. Thank you for the contribution! I can reproduce the issue and your PR looks good to me.\nI'll merge it, it will be tagged in our next release.. @NomNomCameron Could you provide a bit more information regarding how the error is generated? Is our tests failing on your side, or perhaps you are trying to make real request in spec tests? In mocking request and response for testing, you might be interested in ClientStub.\nAlso, our getting help section has been updated recently, feel free to get engaged in other channel as well : ). If you still find this might be a bug with our repo, it would be great if you could provide more information as I mentioned above : ). @stormsilver Thanks for the info! \nUsing the client API #describe_db_cluster_snapshots API should always unblock you. We are concerned that the PR change at resource model could potentially be a breaking change in the future, see issue #1419. @herebebogans Thanks for the information, could you provide some code snippets that helps reproduce this http_version=nil issue? Also may I ask the SDK version used?\nInterestingly, when I create a distribution with HTTP2 enabled, and get the distribution\nruby\nresp = cloudfront.get_distribution(id: 'MY_ID')\nresp.distribution.distribution_config.http_version # => this gives \"http2\"\nI can get http2 in the response.. @herebebogans Oh you are suggesting list_distributions, I saw your cli command is using get_distribution that's why I tried that.\nI can reproduce this with our SDK, yet I just tested, if you try cli with list_distributions, you will find that they don't have response for \"HttpVersion\" as well. I wonder it might not be a bug specific bounds to our SDK, rather an API behavior, I'll dig more into that and try to verify that with service team, will keep updating. @herebebogans Just quick update, service team is investigating the issue and working on fix : ). @herebebogans Thanks, I'll pass this to service team : ). @herebebogans The fix has been delivered : ) feel free to reopen with further question or comments. Looks good to me, thank you for contributing examples that could be helpful for other developers : )\nI'll merge it.. @hamadata Taking a look right now : ). Looking good, I'll merge it, thank you for the contribution : ) !. @vbichov Quick question, are you looking for this feature with Ec2 instances in particular? Looks like it's true that the code snippet provided in the forum doesn't work. Since our resource is build upon service APIs, I can reach out to service team for verification and perhaps feature request.\nDoes that sounds good to you?. Soft updates,  it's true that they don't support this right now, and a feature request has been created in EC2 backlog. So far they support some regex filters:\nUsing_Filtering\nAPI_DescribeInstances\nThe feature request will be prioritized into implementation queue :)\nAppreciate your patience, closing, feel free to reopen with further questions : ). Update:  Looks like this fix works per gems only, still doesn't work when run yardoc from root, working on the fix.. Merging temple fixing first, will addressing yard tag grouping issue in a separate PR :). @agconstantin Thank you for the information! \nSo, the error message suggests invalid parameter inputs in using our APIs, taking a look at their source code, it looks like a bug in their repo, it's using a string, while our API requires array.\nThis is something that could be fixed easily in 'cucloud' instead of us : ). Closing this right now due to ages, feel free to reopen anytime with further comments or questions.. Merging for testing, aware of the test failure.. Just tested, it works as expected now : ). @vStone Correct, source_profile is the parameter needs to be included part of assume role credentials,  yet source_profile alone won't work, you should also provide role_arn.. Closing due to ages, feel free to reopen with further details or questions : ). @kreynolds  Adding to @awood45 's last comment, perhaps you are suggesting a waiter for #create_load_balancer of elbv2?. @kreynolds Thanks for the clarify! Appreciate the information. So our waiters are automatically generate, I'll add the feature from root and have it in future release once it gets through. : ). This feature will be delivered in today's release, closing : ). Looks the the waiters doesn't get through in this release, yet it's merged from root, I'll fix the problem and have it pulled in at following release.. @kreynolds Adding my guess, are you suggesting when using #authorize_security_group_ingress, having tcp as :ip_protocol and providing :from_port and :to_port information with out :cidr_ip, it doesn't work and no error message?. @kreynolds Thank you for reporting, I can reproduce this. I'll deliver the message to service team internally. Even if error is not excepted to be thrown there, our documentation could do better in this usage. . Appreciate your patience, service team has confirmed that this is the expected behavior, it's a design decision that they perform a no-op rather than throw a client error for a missing cidrip in authroize security group ingress. I have reuqested more public documentation for this behavior in their changelog.\nClosing, feel free to reopen with further questions or comments :). Thanks for the contribution! I'll merge it : ). @141984 Thanks for the information, I cannot reproduce this locally. Since our documentation is reflecting our latest SDK version, and I noticed that it seems like you are using v2.3.4,  the memory is introduced in v2.5.3 could you try upgrade : ) ?. @flyinbutrs Thank you for reporting this! Taking a look .... @flyinbutrs Some quick updates, paginators are auto-generated cross SDKs, I'm fixing this from root and will have that fix in releases after reviewed as soon as possible. Appreciate your patience : ). Fix has been merged from upstream, will be tagged in tomorrow release : ). Closing regarding comments I made at #1446 \nFix at root is under review : ). Taking a look, trying to reproduce .... @DenverJ I couldn't reproduce this error with latest SDK, I could see it responses correctly with either freshly terminated instances or an invalid id.\n\nIt looks like the AWS API returns a different response for a valid instance id that was previously terminated vs an instance id that never existed in the account\n\nThis is an expected behavior for recent terminated instance.\nSo it would be great if you could provide more information, such as, is this causing errors everytime? or this only happens with terminated instances?\nAlso, our #getting help section has been updated recently as well, feel free to get involved in channels : ). I'd be happy to dig deeper in to issue if you feel it might be a potential bug with our SDK.. @DenverJ Thanks for getting back! I'll try that again since the last time when I'm trying to reproduce, it's kind of freshly terminated : ). You can always use client directly with API calls to workaround the issue.\nTracking it as a resource model limitation at backlog, closing.. @mike-bailey Sorry for it causing confusions at first glance, our documentations are automatically generated from service models, so it's not a good practice for adding those manual changes.\nFor the operation section, the \"BucketName\" is a stub value that generally shows the input type there(e.g. here it's a string, as you will also noticed, role: \"Role\"). For the Destination$Bucket, it's under the \"BucketName\", that's why \"BucketName\" is appeared as a sample string. Here is the link for description in our API Docs.\nClosing, I reached out to service team for enhancement in shaping their docs, thanks for reporting.\nFeel free to reopen with further question or comments : ). @MMartyn From your snippet I'm assuming you are trying to stub responses for each instance vpcs and do something with those mocking vpcs?\nFrom the error message, looks like the problem is with #describe_instances, in your code snippet, it should be at .any_instance correct? I don't think SDK have an implementation for this at Aws::EC2::Client, is this something that you have written?\nFrom your current snippet, generally it looks good in the way how you stub data, yet after the data is stubbed, you are trying to get response from resource instead of the same EC2 client, that could be another problem. It will still make requests.\nFeel free to correct me if I misunderstood what you are trying to do : ). @MMartyn Thanks for the clarifications, ah I see, taking a second look. @MMartyn A quick update, I can reproduce this, you could work around this by using the stubbed client making request directly instead of using resource class like:\nruby\nresp = client.describe_vpcs(\n filters: [{ name: 'tag:Name', values: ['Test'] }]\n).to_a.first\nI'm working on diving deeper to see what causes this issue. Thanks!. @MMartyn So, ClientStubs lives in aws-sdk-core and binds with clients instead of resources. And #stub_data is stubbing response data directly instead of an response object. And since SDK build resource object without stubs, when it parses responses, it is assuming a response object, and fetch its #data. That's why it will cause error when using stubs with resources. So, working with clients would be the correct solution there. . Sounds good, your latest snippet looks good to me. Appreciate your patience on this, closing. Yet feel free to reopen with further comments or questions : ). @atreat From your error log, looks like you are using v2.7.3, tags is introduced in v2.7.9, could you try upgrade? : ). @dElogics #modify_snapshot_attribute is expected to return an empty structure in response. When using waiters, you could use waiters like:\n``` ruby\nec2.wait_until(:snapshot_completed, snapshot_ids: ['snap-IDabcd123'])\n # OR\nec2.wait_until(:volume_available, volume_ids: ['volumes_IDabc123'])\nComplete Ec2 waiters list:\nhttps://github.com/aws/aws-sdk-ruby/blob/master/aws-sdk-core/apis/ec2/2016-11-15/waiters-2.json\n```\nFeel free to let me know if you have further questions : ). @AlexMorreale That's definitely possible, you might be interested in ClientStubs, here are some quick example snippets, basically, you could stub responses for #describe_auto_scaling_groups with a mocking \"exist\" response\n``` ruby\nautoscaling = Aws::AutoScaling::Client.new(stub_responses: {\n   describe_auto_scaling_groups: {\n     auto_scaling_groups: [\n        # Mock referencing : http://docs.aws.amazon.com/sdkforruby/api/Aws/AutoScaling/Client.html#describe_auto_scaling_groups-instance_method\n     ]\n   }\n})\nThen use this client whatever you want, even in resources\nThen whenever #describe_auto_scaling_groups is called, it's returning the stub responses\n```\nHope this helps : ). @AlexMorreale Thanks for getting back, so asg.exists is actually using GroupExists waiter, it's making describe_auto_scaling_groups API call and evaluate the response for you. If you want it return false, you could have an empty array in the mocking response.\nWhen you initializing a resource object, could you pass in :client option?\n. Closing due to ages, yet feel free to reopen with further comments or questions : ). @mike-bailey Appreciate your patience and thank you for the feedback! \nSince SDK can only validate params based on their shapes or required or not, and it's difficult to validate correct behavior, such as in this case, some params in rule as descriptions and some are actions etc. And it might take a while to tell the combination for those params inside one rule from current documentation.\nI'll reach out to service team for better documentation or better error handling behavior for the API, and keep updated :) Thanks. Closing due to this is a service API enhancement feature request.\nAnd soft updates, I just had this tracked in their backlog, will keep updated.\nFeel free to reopen with further questions , or if similar experience happens again and frustrates you feel free to cut a ticket for AWS Support for better experience, your feedback will always be highly appreciated : ). @Kaelten Thanks! Adding PR #1505 for tracking :). PR #1505 has been merged and will be tagged in the next release, closing, yet feel free to reopen with further questions or comments : ). @klippx Appreciate your patience and thank you for reporting, do you still see the warnings?\nThis sounds weird, since ExpiredToken should be a 403 error code, and shouldn't cause the #get_region_and_retry that generates the log_warning.\nYet I have one suspicion, may I ask do you also have region information under the credential profile that have the expired token? The first region in the warning is the region of client configuration, so it's fetched from your profile if you didn't provide a region parameter when initializing a client. Then it could be possible that the credentials are refreshed and SDK might still fetching the old region?\nIf that's the case, I'll need to dig deeper to see if there are potentials bugs.. Closing due to inactiveness, feel free to reopen with further details or comments : ). @nate Thanks for the information! The issue is using Response directly, it's passing a BlockIO that doesn't have #rewind method.\nSo a quick one line workaround first:\nruby\n # use body of response instead\n  s3.get_object(bucket: 'a-fancy-bucket', key: 'key_my').body {|c| puts c.inspect}\nPersonally, the fix in Aws::Log::Formatter appears as a hard coded fix to me, since the cause actually lies in response, log formatter just helps uncover it. Yet I agreed that more helpful exception message would be beneficial. I'll open a PR shortly to address this, thanks :). @nate Appreciate the patience, and thanks for the clarification. The reason why I'm raising error instead of letting it go silently is that the usage is actually taking in a wrong .body.\nI'm open to better solutions to make debugging experience better. Yet noted that using the block directly will lead to wrong body, and this is not a good practice. Even if I have warn instead of raise, the error will be uncovered later in the chain any ways.\nI'm a bit curious to know why you are looking for catching the exception instead of fixing/working around the error? I noticed that you mentioned:\n\nSometimes downloads are passed a block and we allow it.\n\nYet I'm assuming that this doesn't mean that you would like wrong input right?\nThoughts?. @nate Apology if my reply has cause the confusion : ) , to clarify first, I'm not suggesting that you shouldn't use block, and as you can see, actually I'm suggesting correct input is important. Silent failure in logging could potential making people ignore the situation and future debugging could be rather painful.\nSo I'm revisiting your situation, using\nruby\nclient.get_object(bucket: bucket, key: key){|_chunk| }\ncauses error yet\nruby\nclient.get_object(bucket: bucket, key: key).body {|_chunk| }\nwould be the correct behavior that will not lead to error.\nMay I ask why that fix won't work in your situation? perhaps I'm missing something obvious?. @nate Ah I see, looks like I misunderstood your initial request somehow, sorry for that, taking a look for proper fix or work-arounds. Now I see why you find Aws::Log::Formatter is causing trouble. Will update shortly!. The fix has been merged in, will be tagged in the next release thanks!. Thanks for the contribution :) merging. @dElogics Thanks for the information, your code snippet looks good, I'm a bit confused about why you say this filter doesn't work, especially it produced correct output for the main routing table.\nSo may I ask more information to see how this troubles?\nThe association.main set to false  helps filter out the main route table in the VPC, if it returns nil, it actually means there is no other route tables except the main table in the VPC specified. Could you try just having myVPC in the filters to see the output (all tables)? If it produces the main routing table only, then no output when false is actually the correct behavior there.. @dElogics I can reproduce this, thanks for reporting, looks like when association.main is set to false, it doesn't work properly, I reached out to service team for this problem, will keep updated.\nMeanwhile, our Getting Help section has been updated recently, free feel to get help from Aws Support as well if this has been blocking for a while : ). @dElogics I digger deeper into this behavior and found that \"association.main, false\" won't work if the route table doesn't have association at all (which is the default behavior if you create a route table under VPC from AWS console). I have asked for better documentation from service team about this.\nClosing. since this doesn't seem to be a SDK bug, rather than an API behavior, if you still struggle with the API, feel free to get involved in other channel or cut a ticket for AWS Support as our latest README suggest :). @kidbrax Thanks for reaching out, may I ask for a bit more information to clear my confusions before digging deeper into the issue?\nAs in the public doc you referred, \n\nIf your domain name contains any of the following characters, you must specify the characters by using escape codes in the format \\three-digit octal code:\n\nAlso, it's true that \n\nYou can create hosted zones that include * in the name. Note the following:\nYou can't include an * in the leftmost label in a domain name. For example, *.example.com is not allowed.\nIf you include * in other positions, DNS treats it as an * character (ASCII 42), not as a wildcard.\n\nMaybe I'm missing something that you are suggesting? Looks like have it not presenting as a wildcard is the correct behavior there? Even if looking at console, just as you have mentioned, it's shows as \\052. May I ask which API are you using that make you feel this is troublesome? Then I might reach out to service team for potential feature request.\nAlso, recently, our README.md's Getting Help section has been updated, feel free to get involved in other channels as well :) If you still find this could potentially be a SDK bug, I'm more than happy to help dig deeper into the behavior :). @kidbrax Thanks for your patience and clarification, now I can reproduce the case and see what you are suggesting : ) Your request sounds reasonable to me, have an optional flag makes sense as well. I'll tag this as a feature request.. This is tracked in our feature backlog, closing.. Closing, will open a new one instead. Soft ping, I'm adding this across SDKs and PR is in progress :). @flyinbutrs Really appreciate that!\nSorry but we don't take manual PR fixes, because those paginator.json files are automatically generated,  I added those pagination behaviors from root (internally) and is under review. \nHowever you could easily test and play with paginators by adding them in the paginator files, in this case, you could try testing with:\n\"ListExports\": {\n      \"input_token\": \"NextToken\",\n      \"output_token\": \"NextToken\",\n      \"result_key\": \"Exports\"\n    },\n    \"ListImports\": {\n      \"input_token\": \"NextToken\",\n      \"output_token\": \"NextToken\",\n      \"result_key\": \"Imports\"\n    }\nNote those are hard coded fixes : ) \nFeel free just report pagination missing for any operations, we have enforces pagination rules for all new operations now, yet it could be possible that some old api might still missing paginations : ). Closing, the changes are delivered in the latest release : ). This smells like a potential SDK bug, since this shape is correctly set as boolean type.\nTaking a look to reproduce .... @dElogics Ah looks like the issue is using snapshot_id and encrypted at same time is not allowed actually. If I use dry_run: true, it will give helpful message (as you have observed): \nAws::EC2::Errors::InvalidParameter Parameter encrypted is invalid. You cannot specify the encrypted flag if specifying a snapshot id in a block device mapping.\nThis is expected since encrypted cannot be used with snapshot_id together. Using nil works because it actually mocks the behavior of mot providing encrypted parameter.. Closing, since this relates to API behavior instead of SDK bug, feel free to reopen with further details if you feel it still feels like a SDK bug :) Thanks. @dElogics I did a quick debugging, looks like the issue is with your .no_device value, API reference here. Could you verify that and try again? SDK can only validate parameter based on their shapes, and this is an error from server side. dry_run  flag would be quite helpful when you are debugging those issues :)\nFeel free to get involved with AWS Support as well if this really frustrates you :) Your feedback will be highly appreciated. If you still finds this might be a SDK bug, I'll be more than happy to dig deeper into the issue with you.\nHope this helps :). Closing, since this relates to API behavior instead of SDK bug, feel free to reopen with further details if you feel it still feels like a SDK bug :) Thanks. Test is failing on 1.9 and 2.1, working on fixing .... Still in progress, yet have finished refractor using client request handler in constructing urls.. Based on the discussion our team had off-line, we will work on a more generalized solution that brings much more benefits and flexibility in the long run (involving client level, request level etc.). And that is a feature has been prioritized in our backlog, and will be shipped through Modularization Ruby SDK which is currently under preview release and will be in GA in the near future. So this PR will not be merged.\nMeanwhile, aws-sigv4 gem is available for general, flexible and customized presign behavior and signer usage, with fully documented source code.\nThis PR will still be kept opened for a while for feedbacks and thoughts! : ). @oyeanuj Sure, you can do it both in V2 and V3, yet I'd recommend using the aws-sigv4 gem with code snippets as below:\n`` ruby\nrequireaws-sigv4`\na s3 client at region 'us-west-2'\nsigner = Aws::Sigv4::Signer.new( \n  service: 's3', \n  region: 'us-west-2', \n  credentials_provider: client.config.credentials, \n  uri_escape_path: false, \n)\ncreate presigned url for an object with bucket 'a-fancy-bucket' and key 'hello_world'\nurl = signer.presign_url( \n  http_method: 'PUT', \n  url:'https://a-fancy-bucket.s3-us-west-2.amazonaws.com/hello_world', \n  headers: {                                                                                                                           \n     \"x-amz-foo\" => \"bar\"                                                                                     \n  } \n  body_digest: 'UNSIGNED-PAYLOAD', \n) \nmaking request\nbody = ...\nNet::HTTP.start(url.host) do |http|\n  http.send_request(\"PUT\", url.request_uri, body, { \n    \"x-amz-foo\" => \"bar\", \n  })\nend\n=> #\n``. @pdrakeweb A quick follow up question to make sure I'm reproducing based on the same waiter, are you suggesting this [waiter](https://github.com/aws/aws-sdk-ruby/blob/master/aws-sdk-core/apis/cloudformation/2010-05-15/waiters-2.json#L29) in particular?. Thanks for the contribution :) I'll merge it. Thanks for the fix, I'll merge it : ). @mhussain Is this still happening frequently for you? I couldn't reproduce this with a sample poller usage from SDK side, you can also cut a support TT to [AWS Support](https://console.aws.amazon.com/support/home) that can help look into the request as well.. Closing due to inactiveness, feel free to reopen with further questions or comments : ). Soft update, changes have been merged, should be out in tomorrow's release  : ). This is tagged in the latest release, closing : ). Appreciate your patience, just a soft update, changes are under review : ). Just a soft update, pagination has been integrated into their next feature updates, which will be landed in SDK soon : ). Pagination has landed inv2.9.17`, closing, thanks again for your patience : ). Thanks for the info : ) while from the example you provided, this PR does make sense. However I do have some concerns for allowing both styles at same time, it could potentials causing some weirdness around parameters values.\nFor example: \nif use one style solo ( as you provided )\nThis works fine \n```ruby\nAws::EC2::Client.new.describe_instances('filters' => [{ name: 'tag-key', values: ['key1'] }])\n<Aws::Structure:Aws::EC2::Types::DescribeInstancesResult\nHowever ruby\ns3.put_object('bucket' => 'a-fancy-bucket', 'key' => 'wow', 'body' => 'hello')\n=> Aws::S3::Errors::InvalidBucketName: The specified bucket is not valid.\nYet\ns3.put_object(bucket: 'a-fancy-bucket', key: 'wow', body: 'hello') \n=> #<struct Aws::S3::Types::PutObjectOutput\nAnd if mixing those 2 styles together ruby\ns3.put_object(bucket: 'a-fancy-bucket', key: 'wow', 'body' => 'hello') \n=> succeed yet the body value is ignored\n``\nI think the behavior there need dig deeper, thoughts?. Thanks for the info. appreciate that, closing this right now, yet please feel free to reopen anytime if this becomes pain point for you or further questions exists : ). Adding to Alex's comments,Aws::Organizationsis alive since [v2.7.16](https://github.com/aws/aws-sdk-ruby/releases/tag/v2.7.16). Adding notes, this failure is caused by [webmock](https://github.com/bblimke/webmock/blob/master/CHANGELOG.md#231), their support for ruby 2.4 landed in webmock 2.3.1. If you trybundle update`, should help fix the problem.\nMeanwhile, other 2.4 refractor work is tracked at #1505, we are working on getting it merged soon : ). PR #1505 has been merged and will be tagged in the next release, feel free to reopen if you still have problem with it : ). Adding info.\nLooks like this is another problem with Ruby 2.4, I can reproduce the same error there. For Ruby 2.3, it works fine though. Definitely an issue that we will follow up with.. @bobziuchkovski Thanks for reporting and appreciate that! Apology for the discrepancy between docs and SDK models. This API Model update is definitely in our radar and will be delivered through all SDKs soon : ) Stay tuned : ). The API update has been included in the latest release, closing : ) Thank you for the patience.. Will merge after CI finished. This has been tracked in our backlog, closing.. @BanzaiMan Thanks for the information, appreciate that. Correct, SDK doesn't do anything special with that operation, I'll reach out to service team and let them know about this situation : ) At least our service API documentation could do better. Meanwhile, feel free to open a ticket to AWS Support to get direct attention as well.. This has been added to service team doc backlog and they are working on it, closing.\nYet feel free to reopen with further comments or requests : )\nThanks again for bringing this up.. @BanzaiMan Exactly : ). @BanzaiMan No worries, I'll follow up with service team with this : ) appreciate your patience!. @BanzaiMan Apologies, I have pinged them again :( will update\n. Tracked in our feature backlog, closing.. Landed in release v2.10.1, closing : ). Taking a look. Np, glad you figure out : ). Looks like 1.9.3 is falling, working on investigating. Update, tests passed for me locally with\nruby 1.9.3p551 (2014-11-13 revision 48407) [x86_64-darwin16.5.0]\nTry to locate the exact version causing the failure.. Change irrelevant error is gone after re-run, will merge once CI finished.. @cunnie Soft update, I can reproduce this, working on investigating. @cunnie Quick update, the problem might has to do with resource models. If using ec2 client directly, it works fine without problem:\n```\nec2.describe_route_tables(route_table_ids:[\"rtb-11111111\"])\n=> both IPv4 and IPv6 are available\n``. @cunnie Soft update, this is caused by our resource model definition doesn't includeDestinationIpv6CidrBlockin the [model](https://github.com/aws/aws-sdk-ruby/blob/code-generation/apis/ec2/2016-11-15/resources-1.json#L1445), it only includesDestinationCidrBlock`.\nI'll open a PR to address the fix shortly. Thanks for your patience. PR #1513 is opened, which is under review.. @johnathandavis Thanks for the info. taking a look.. @johnathandavis I tried with different change set name with numbers or '-', and I couldn't reproduce the problem with any of them. Your code snippet looks fine with me (assuming you intentionally omit template_url or template_body since they are irrelevant to this failure?).\nThe Error received is a service side validation failure, which is not caused by SDK client side validation. So for further debugging, you could enable  http_wire_trace to see the http log when making request like:\nclient = Aws::CloudFormation::Client.new(region: region, http_wire_trace: true)\nRecently our #Getting Help section has been updated, feel free to get involved in other channels as well. Such as Aws Support, they will also help with trouble shooting with your request IDs : )\nIf you still feel this is kind of a SDK bug, feel free to reopen with further details or debugging log etc.. @rhymes Thank you for the interest : )\nThe branch is code-generation\nAs for the changelog, each gem has it's own changelog, such as: S3 Changelog. There is also aws-sdk changelog and aws-sdk-core changelog as well.\nAlso there is UPGRADING.md.. Closing, yet feel free to reopen with further questions : ). I'm hesitate to merge this because now destination_cidr_block is no longer required, thus Routes resource doesn't have an suitable identifier per Route anymore, which even though pass V2 resource structure yet looks like:\n=> #<Aws::Resources::Batch resources=[\n     #<Aws::EC2::Route route_table_id=\"rtb-123\">,\n     #<Aws::EC2::Route route_table_id=\"rtb-123\">\n ]>\nAnd this will cause problem for the code-gen version, and it makes sense since there is a resource without identifier.\nOpen to thoughts, yet I believe perhaps the best option there is using client API call instead of the resource object for this case.. @rhymes Thanks for reporting, appreciate that!\nI suspect that we failed to automatically bump version when core gem introduce changes to all service gems, I'm working on the fix and will update shortly.. @rhymes Soft update, this is a bug with .rc version locking only, I have merged the fix in build and did a new release including generated fixes just now.  After the gem publishing process is done, I'll update and close the issue.\nThanks again for your patience : ). @rhymes Both gems are out: s3 and sns. And I have tested locally, closing, feel free to reopen with further questions or problems : ). @Shockolate Thanks for the info. Taking a look .... @Shockolate Interestingly, I couldn't reproduce it with the code snippet provided:\n``` ruby \nAws> stubs\n=> {:list_aliases=>\n[{:aliases=>[{:function_version=>\"1\", :name=>\"UnitTestingValid\"}, {:function_version=>\"3\", :name=>\"WrongName\"}, {:function_version=>\"3\", :name=>\"DupAlias\"}],\n    :next_marker=>\"alias_marker\"},\n   {:aliases=>[{:function_version=>\"1\", :name=>\"AnotherDuplicate\"}]}]}\nAws> l = Aws::Lambda::Client.new(stub_responses: stubs)\n=> #\nFirst call\nAws> l.list_aliases(function_name: '1')\n[Aws::Lambda::Client 200 0.000509 0 retries] list_aliases(function_name:\"1\")\n=> #<struct Aws::Lambda::Types::ListAliasesResponse\nnext_marker=\"alias_marker\",\n aliases=\n  [#,\n   #,\n   #]>\nSecond call\nAws> l.list_aliases(function_name: '1')\n[Aws::Lambda::Client 200 0.000438 0 retries] list_aliases(function_name:\"1\")\n=> #<struct Aws::Lambda::Types::ListAliasesResponse\nnext_marker=nil,\n aliases=[#]>\nOne thing might be worth checking is that, with your response result, when your client call `#list_aliases`, could it be possible that you are not checking the first call? Because when stubbing 2 responses, after first attempt, no matter how many time it makes the call later on, it will return the second response. And your response exactly matches the second response entry (where `next_marker` is not mocked)\n{aliases: [{ function_version: '1', name: 'AnotherDuplicate' }]}\n``. @Shockolate No problem at all, glad that you figure it out : ). @sorah Thanks for the contribution and background info. Appreciate that! May I ask why the md5 spec tests need to be removed?. @sorah Apologies, that's my misunderstanding from removing the spec test, actually taking closer look at the code, this should be fine.\nJust a bit concerned about that, going to revisit the code a bit ; ). @cconstantine Thanks for the information, looks like from public doc right now, it doesn't has adescribeorlistoperation that contains tag information. I'll reach out to service team for some lights. Also, feel free to cut ticket to [Aws Support`](https://console.aws.amazon.com/support/home) to make your voice heard as well : ). @cconstantine No problem at all, all feedback are highly valued, I'll keep updating : ) Appreciate your patience!. @cconstantine Appreciate your feedback and patience, this feature is tracked in ELB's backlog now : ). :shipit: . @awood45 Added v1 guide, addressed feedback, ready for another review :). I reached out to service team about this documentation confusion, it's in they backlog : )\nAppreciate your reporting, closing. Yet feel free to reopen with further question or comments.. @ohTHATaaronbrown Thanks for reporting! Taking a look to reproduce and test the proposed fix. Meanwhile, happy to take a PR to review as well.. Soft update, unable to reproduce with my local Mac environment with ruby 2.3.3p222, will try to reproduce in a similar environment instead.\nAdded: Unable to reproduce with ruby 2.3.1p112 with latest SDK, have to take a look at windows environment.. Update, I tried with an newly launch EC2 windows instance and still couldn't reproduce. Happy to take a PR with the fix if you can reproduce that in your environment. Just heads up, make sure you have spec test in your PR with this issue : ). Closing due to inactiveness, yet feel free to reopen with further information or questions : ). @stiller-leser Thanks for the information, first, currently no. PresignedUrls are based on single operation currently, such as #get_object or #put_object, for those operations key is a required parameter as you see. I heard your use case, after a quick thought, I couldn't find a single operation tied to it specifically. \nMeanwhile an enhanced and flexible presigned interface feature has already been in our backlog. We are definitely considering possible bucket level usage. Happy to hear more and figure out how we can make it easier for you with that feature request.. Updated with :s3_host_id, will merge once CI test finished. @awood45 Changelog is  fixed, README.md is updated and build task is updated as well : ). Thanks for bring up the idea, appreciate that. I'd happy to dig more to see how do we make it clear as a feature request or enhance the user experience there.\nFor the download, we release multipart downloader in April, when it's making requests by :range or :part, if a single request fails, it ensures cleaning up completed chunks. For those requests, if retry happens, it's actually retrying the same request with :range or :part.\nFrom my understanding of your proposal (feel free to correct me if I'm missing something : )), you are suggesting special handling for \"basic\"(no :range or :part in parameters) :get_object operation when making retries to make it more robustic. The problem is, if :range or :part_number is not specified in the #get_object api call, the response will contains nil value for content_range and parts_count, which will be useless if we want to resume the range.\nPersonally, I feel multipart downloader works better for this case if the file is big, for single #get_object request, I don't see a clear way for making recover less error prone. Thoughts?. @janko-m Thanks for the information, I see.\nWell, as that blog post suggests:\n\nPlease note, when using blocks to downloading objects, the Ruby SDK will NOT retry failed requests after the first chunk of data has been yielded. Doing so could cause file corruption on the client end by starting over mid-stream. For this reason, I recommend using one of the preceding methods for specifying the target file path or IO object.\n\nHowever, if partially retrieve is your main concern, I'm thinking perhaps we could add an extra :recover mode for multipart downloader that writes partial contents to specified file and returns the last part/range bytes information if retry still fails, and accordingly, it should also have a easy usage interface if resume is wanted. \nWill that sounds good to you? If so, I can have that in the feature backlog : ). @janko-m Ah I see, thanks for the clarification,  I agree it's a separate issue, tagging this as a feature request, happy to take a PR for review. I'll do some benchmarking/exploring myself : ). Tracked in backlog to be prioritized : ). @tongueroo From your snippet:\nec2.describe_instances.reservations[0].instances.count\nYou are looking at the instances in the first reservation only, could it be possible that your instances are in different reservations?\nAdded:\nOur resource model is counting all instances across reservations if no other parameters are provided.. @mbaxa That looks weird to me, try to reproduce with that ruby version on my mac right now .... @mbaxa I couldn't reproduce the error with exactly same ruby version. Could you share more error traces by --backtrace option? That might helps identifying the cause?. @mdub Thanks for the information, as you mentioned,\n\nI'm seeing occasional failures\nDoes this happens with a newly launched Ec2 instance? Or a instance just removed and terminated? \n\nI'm trying to reproduce this scenario to see what we can do to make that waiter more robust. Interestingly, the only situation that I can reproduce the error is when I used a wrong instance id, so I'm guessing this is happening when an instance has been terminated for a while?. @mdub Makes sense, thanks for your contribution all the same! I'll pull this PR upstream : ). This will lands in v2.10.2 release, which will be out in an hour, closing.. @janko-m This is cool, looks great, thanks for the contribution!. Thanks for reporting, PR #1541 has opened to address the fix. @toaster I've considered about it, the reason for that is because for SharedCredentials, it allows :profile_name overwrite . Making ENV goes first there will lose the overwrite behavior there.\nEdited, hmm, it can be fixed there as well. I prefer keep it in the CredentialProviderChain to let the chain decide the default behavior : ). @toaster Yes, it's true, and you will still have to have the default profile logic at SharedCredentials as well. That's why I edited my response ; ) We want to keep the default profile logic there.\nAnother thing is, I'd like to CredentialProviderChain deciding those default behavior, making SharedCredentials class as clean as possible : ) . Will merge once tests finished : ). Thanks for contributing, this change has been pulled upstream, will keep the issue open until that patch release :). @tgxworld Thanks for the contribution and benchmarking details! looks good to me, I'll merge this.. Closing due to inactiveness, feel free to reopen with further questions or comments : ). The failure is caused by malformed changelog, fixing changelog right now .... Merging this right now, this only involves documentation changes and I have verified that the generated examples are the same with current v2 examples.. @lwoggardner  Thanks for reporting! Marking this as a resource model update task.. Tracked in backlog, meanwhile happy to take a PR for review and share with other SDKs : ). Feedback addressed, ready for another review : ) @awood45 . @aquexata Thanks for the feedback! Appreciate that.\nI'll do some benchmarking first and then try to make this better, I'll check cli/boto transfer and do some comparison as well. Any further comments and thoughts are welcome.\nA bit clarification for Multipart download helper, it's a higher level abstraction providing customizable usage for S3 multipart APIs. It's true that it works in the way you describe, in terms of I/O required, you could configure the range size per request and concurrent thread in used.. A soft update, talked to the boto team offline, enhancement could be made to make Ruby multipart downloader functionality better. Also, to make IO usage better, max IO parameter could also be added. Tracking this as a backlog item to be prioritized.. Tracked in backlog. @fabiokr Thanks for the information, so you can set :http_wire_trace to true at client to see the http raw wire log, you can also use log formatter to keep track if wire trace is too much for you. \nIf you find something unexpected from SDK side, happy to take a close look if you could provide some reproducing code snippets . Then, if request is behaving weirdly, feel free to contact AWS Support to look into that as well ; ). @Reechee88 The use_bundled_cert! is introduce since v2.1.0, if you update your gem dependency, that will work : ). Appreciate the feedback! So this is the same issue in the refer, long story short, it's a issue in S3 side, S3 should have check query string only, yet it's still checking both signed headers and headers in the request. \nHowever, fixing that will break existing customers. The querystring itself doesn't have a problem, temporary workaround would be using the aws-sigv4 gem as mentioned in the issue (allows custom header signing and headers in requests). Also I'll count your feedback a +1 for the enhancement in S3 presign feature : )\nTracked in at feature backlog, closing.. @Chocksy Just double check, did you verified that request.raw_post is valid? . @Chocksy Apologies for getting back late, from your response, looks like you are using SES notification for SNS? However, the MessageVerifier is used in verifying signature in this format, that's why it throws irrelevant errors.. @Chocksy I don't think so, we won't introduce breaking changes in the response. In the blog post you linked, in the controller test part, it's actually following the correct message format.. @Chocksy I didn't deep dive into the whole blog, yet I think if you revisit your flow that might help uncover something, the MessageVerifier is used for :\n\nverify the authenticity of a notification, subscription confirmation, or unsubscribe confirmation message sent by Amazon SNS\n. Release containing PR is out, closed. Thanks for the contribution! Will pull this upstream to share with all SDKs and CLI and release from there : ). @BRMatt Should be today ; ) I'll ping the thread once release is out. Release contains the change is out, resolving : ). Thanks! Will help pull those upstream to share with other SDKs and CLI and release from there : ). Release containing the change is out, closing. @thegreyfellow Sorry to know that this behavior blocks you for a while. First of all, what you have observed is the correct behavior of SDK. The presigner creates a default s3 client by default if client is not provided to overwrite, that's helpful when :client is not required for creating a presign url : ). Adds to that, it's limited for SDK to know cross-region bucket region correctly for every case, and what you did is correct work around there.. Looking good, thanks for the contribution : ), merging. @jasonperrone SDK has no control over browsers, since Chrome works fine, the url should be correct. If the problem exists with console still, feel free to cut support tickets to AWS for console team or S3.. The fix PR has been merged, will be tagged in the next release ;) (likely tomorrow)\nclosing, will ping the thread once release is out, thanks!. All tests have passed except the oj issue for ruby 1.9.3, will merge once PR #1569 is merged. @tpett I can see that it's a small change which is easy to make, the error message appears misleading, however it's documented as string or in code, could you lead me to the documentation that caused confusion for you? Just want to make sure everything is caught : ). Looks good, thank you for the contribution! Will be tagged in the next release. Hmm it does look like wait is not the proper fix there, I tried same version jruby test locally, it all passed instantly.. Finally I have located the cause ... it has to do with jruby parsing very long strings hanging ...\nClosing this one, will open another one against master. Forward the ping again : ) Appreciate your patience\nAlso, feel free to cut a TT to AWS Support at the same time as well, they can help look into the request IDs.. Thanks for reporting! taking a look .... Just a soft update, I've reached out to SNS team for this issue, will keep posted. Thanks. @stormbeta Quick question, is this happening with message from Lambda only?. @stormbeta Thank you for your patience! the workaround PR fix has been merged will be out in the next release. A quick summary is that this is cause by Lamdba message formatting issue in SNS. I'll ping the thread once the release is out (likely today) : ). The release is out, thanks. This is WIP, not review ready, will update. Now review ready : ). Thanks for the feedback! tracked in our backlog under S3 Multipart enhancement.. @madkin10 Thanks for the info.! Quick question to help me reproduce this, what's your aws-sdk-core version? I just tried to run gem install aws-sdk-resources from scratch and there is no issue for me, the installed aws-sdk-core version is 3.0.0 as well. This might likely has to do with your aws-sdk-core version is still 2.*, aws-sdk-core 3.*doesn't contain executables.. The PR #1586 update is merged, will be out in the next release : ), closing. Thanks for the contribution! Just a soft heads-up, I'll take the work from there and do a bit refractor before merge after we switch to master shortly.. Closing, PR #1586 is under reviewed, tracking with #1586 instead.. The jruby timeout failure is aware, will work on fixing in the separate PR. Merging this right now.. Thank you for your patience, the fix PR was merged, will be out in the next release. I'll ping the thread again once next release is out.. @janko-m Thanks for the ping! taking a look .... @janko-m Just opened another PR for resource object fix, will ping the thread once it got merged and released : ). Sorry for the late ping, the latest version is out. @pchaganti Thanks for the attention! This will be included in next release across all SDKs very soon! I can ping the thread once it's out : ) (likely today). The release is out, closing. @awood45 Ready for another review!\n\nI added spec test for a mock service with operations that's similar to s3, glacier and polly. The mock api.json also cover the the corner cases I met when locating correct streaming shape. I can add another service smoke test if you recommed ; ). @k1LoW Thanks for catching, working on fix this. A side note is that for core version, it's tracking VERSION file instead, which has the correct version, I'll remove the redundant one.. PR has been merged, closing : ). @JaganNarayanan Could you provide more context? Which version of the gem are you on? I tried install the aws-sdk gem from scratch and tried several API calls, I cannot reproduce the error here.. Hmm there is no diff, this is a merge attempt I guess?\nWe have switch from code-generation branch to master already, closing this for now, feel free to reopen with further info. : ). @krystofspl Thanks for the info, taking a look. Curious if this has to do with autoload, could you try our V3 SDK (which gets rid of auto_load) to see if makes any difference? You can even just try the aws-sdk-s3 gem :)\nAnother question is, is this error happens every time or occasionally?. @krystofspl Sorry to hear that, curious to know what problems has v3 caused for you?\nAdding to original issue, could you provide a code snippet that can help me reproduce the error in a jruby environment? Or are you just calling object.upload and it's causing problem for you?. Soft ping checking, does this still happens? Any luck in finding more information for reproducing the issue? : ). For sure, closing this now. Feel free to reopen if you have further info :). Added changelog entry, the only failure is the jruby hanging problem, which has been fixed and merged.\nMerging this right now.. This code snippet looks like JS deployment? Sounds like the error is from Travis?\nI'd suggest open the issue with travis first, and since this is a server side error response, I'd also recommend open ticket with AWS Support if you can, such that they can look into the failed request IDs. \nHappy to help investigate more if you/travis could provide specific ruby scripts that can reproduce this error.. Closing due to ages. Feel free to reopen with further info or a ruby script with SDK usage that can reproduce the error.. @fledman Thanks for noticing! This is true that aws-sigv4 gem make it more flexible and easy to test and use. However, the :time option is mainly used for testing purpose. We intentionally didn't expose all field though S3 interfaces initially.\nMay I ask why you would like to see this from S3 interface? Some use cases explanation would be great! Then I can see if I can provide workaround or tag this as a feature request.. Thanks! Tracking this under feature backlog.. @PetrKaleta Hi, I think it might related to a jruby bug that I thought was travis ci only. I've already merged a PR fix issue here. I'll do a version bump for kms to make this available. I'll ping the thread once that goes out, thanks!. @PetrKaleta The latest kms version is out, could you try update and let me know if it works? (s3 gem has dependency on this) : ). Adding to that, jruby fixed this themselves as well at master. @Rigel89 Hmm, I don't think we do something special for that api call or type, are you using rails that causes this problem? Noticed that you mentioned\n\nI didn't encounter this issue for other EC2 types (doing a similar thing for EC2 Volumes).\n\nCould you provide an example that as_json worked? I tried :create_volume API, as the same with create_snapshot, both returned Seahorse::Client::Response.. Closing due to ages, yet feel free to reopen with further information : ). @pchaganti Q, are you suggesting our aws-sdk-xray gem? Source code here : ). @tarcieri Ah thanks for catching! exactly, happy to review the PR if possible, if you don't have time, let me know and I can take this too.. @tarcieri Taking a second look, this :deep_copy is actually not used else where in the sdk, you can refer the :deep_copy we have at code generation here.. @tarcieri Thanks for the info! I'll work on thw fix and add tests. Soft update, took a while yet PR #1604 is opened to address this issue add add more test for batch operation behavior, which is under review. The PR fix has been merged, will be out in next release.\nAppreciate your patience, will ping the thread again once the release is out : ). Thank you for your patience! The release is out, closing.. Have manually tested EC2 resource batch operations, Autoscaling batch operation and CloudWatch batch operations will merge once CI completed.. The only failure is caused by ruby 1.9.3 kramdown dependency issue, which is addressed at #1605 . Taking a look. @PetrKaleta Hmm, this sounds like a bug, could you help provide some code snippet to help me reproduce the issue?\nI tried to reproduce in the simplest way:\n```ruby\nobj = Aws::S3::Resource.new.bucket('a-fancy-bucket').object('hello')\nobj.last_modified\n=> 2017-08-31 16:37:14 +000\n``\nAnd this works fine for me.. @PetrKaleta Ah sorry about the confusion, I should try with object summary instead. I can reproduce this, looks like this will work if remove.to_hfor the object in:datafield. Double checking v2 behavior now before adding back compatiblity. @PetrKaleta A quick follow up question, may I ask for the documentation source that lead you to use.to_h` in v2? Just trying to make sure I covered all in my fix PR.\nAdding notes here: at V2, passing an object or hash both works.. Soft update, the PR fix has been merged, will ping the thread once release is out. The release is out, closing. Marking this as WIP, trying to simply the diff. Observing some weird git issue locally, will open a new one to address this.. @ajkerr Thanks for reporting, taking a look. Soft update, I can reproduce this, working on fixing the regression, in the meantime, you can use ec2 client waiters as work around like:\nruby\nclient.wait_until(:volume_available, volume_ids:['vol-0b123323123123']). The fix PR has been merged, will ping the thread once release is out : ). The release is out, closing : ). @jeffpereira Thanks for reporting, working on the fix. Soft update, PR #1612 is opened and under review. The fix PR has been merged, will be out in next release, will ping the thread once that is out : ). The release is out, closing : ). Taking a look. @infinityminglei The behavior looks like is expected in SDK (both V2 or V3). The data attribute for the resource object is fetched once.\nIn terms of polling the stack status, we recommend using waiters instead. In your use case, you can try #wait_until_exists for the stack object, or try #wait_until at client.. @infinityminglei It just checks existence of the stack, if you want to check the complete state, you can use :stack_create_complete waiter, scroll down the client #wait_until method, you will see\n\n. Questions have been answered, closing.\nFeel free to reopen with further discussion : ). Ah will follow up with tests. Thought the same. I also thought S3 test might be cleaner, however S3 waiters might not be a good place to test states from block, so I mock the same EC2 issue here. I also make sure the volume is cleaned up. Now ready for another review : ) @awood45 . @makeready Thanks for the contribution, those files are auto generated from shapes defined at .json file here, so, add those files directly won't work. Could you guide me to your original PR that I can see how can I add this to our generator? : ). @eropple Thank you for the feedback, appreciate that! \nHowever, SDK doesn't hand drafting those namings for resource for most scenarios, those are auto-generated by resource model json file, like this, those JSON files are shared across SDKs for all languages and CLI. . Making refreshing at default behavior might be breaking to current customer, could you make an optional flag for this? Also, may I ask for more background information about this feature request? Some use case perhaps? Any pain point there with the no refreshing behavior from file?. @krishnan-mani This sounds like a credential error, how do you provide credentials, especially region parameter? (e.g, default credential chain? config file?) Just tested a similar case locally and cannot reproduce this. Trying to figure out behavior difference there\nAnother question is, for those queue urls, it usually follows pattern: https://sqs.eu-west-1.amazonaws.com/MY-ID/test1, might be a typo, yet double check : ). Closing due to ages, yet feel free to reopen with further info. @immerrr I tried #get_queue_url API, and url response is following pattern\nhttps://sqs.us-west-2.amazonaws.com/123.../foo...., Ruby SDK doesn't have any customization around the response url strings SQS send back.\nAs you mentioned, the CLI pattern is likely a customization on their side or a legacy pattern, I'm not super familiar with boto/CLI code base, I'll need take a deeper look to find out.. >I'm not saying it changes the URL, I'm saying it takes 2nd dot-separated netloc component as region name, and uses it for signing, and naturally fails when that 2nd dot-separated netloc component is queue as in the legacy patterns rather than a region name.\nCorrect, what I'm trying to figure out is whether this is an old pattern that AWS server send back original or it's simply a boto customization. If it's simply a boto only thing, it's not appropriate for Ruby to adding the hack simply for in sync with boto. If it's a server raw response that regressed, we might consider adding a work around for it.. Soft checking, does the issue still happening for you?\nSince this is surfaced by ami_spec, could you try open an issue with them first about this issue? \nI did a quick check in the ami_spec, I didn't see any custom wrapper for assume role/ MFA. This has already been a feature tracked in our feature backlog, can I take your request as a +1 for that?\nIf you still seeing any issues reproducible with our SDK, it would be great if you can share some code snippets for us to reproduce.. Closing due to ages, have added +1 for the feature request in your backlog, feel free to reopen with further info/questions : ). @cotsog Thanks for reporting, it just takes a while :) after our github push. now you can see it.. @SamSaffron  As documentation shows, #put_bucket_lifecycle is deprecated, you can use #put_bucket_lifecycle_configuration instead, see examples here : ). Thanks for the contribution! The CI failure is irrelevant to this change and has been fixed in PR#1626\nMerging now. Hmm apology for late on this. Since it would take a while for me to have a the chef environment to set up, could you let me know what's the test failure error? I believe our gem files are either 644 or 755:\ntotal 24\n-rw-r--r--  1 chejingy  ANT\\Domain Users   1.2K Oct 23 16:07 CHANGELOG.md\n-rw-r--r--  1 chejingy  ANT\\Domain Users     6B Oct 23 16:07 VERSION\n-rw-r--r--  1 chejingy  ANT\\Domain Users   1.1K Nov 29 13:55 aws-sdk-cloudhsm.gemspec\ndrwxr-xr-x  4 chejingy  ANT\\Domain Users   136B Oct 23 16:07 features\ndrwxr-xr-x  4 chejingy  ANT\\Domain Users   136B Oct 23 16:07 lib\ndrwxr-xr-x  3 chejingy  ANT\\Domain Users   102B Oct 23 16:07 spec\nThose only allow write permission to owners, read/exec permission to others.\nAlso, as you might already noticed, aws-sdk-cloudhsmv2 is available as well : ). Looks good, thanks!. @sverchdotgov This might have to do with ec2 resource model outdate, if you try api operation #describe_route_tables directly, destination_cidr_block should be there. And likely, among all routes returned, there are some entries with destination_cidr_block nil. Like entry you mentioned.\n{\n            \"DestinationPrefixListId\": \"pl-63a5400a\",\n            \"GatewayId\": \"vpce-270abd4e\",\n            \"Origin\": \"CreateRoute\",\n            \"State\": \"active\"\n        }\nThis is a resource model limitation that the identifier for route has been changes, the workaround is using api calls directly (which is the cli behavior you see). Closing due to ages, also tracked in backlog under resources. Feel free to reopen with further comments : ). WIP, the error is that I need to create a foo plugin file separately can not use the existing one :( which is easy to fix\nWorking on researching other approaches as well .... @dimitrovmaksim Sorry to hear that, yet I couldn't reproduce the issue with both V2 and V3 SDK, could you try contact AwsSupport here to get help? They could help look into request IDs to see if anything went wrong. They will engage us as well if this has to do with SDK.. PR #1634 is opened to address this issue, thanks for reporting!. Just merged, closing : ). Taking a look, working on reproduce .... @pchaganti Thanks for the info! Sorry to say, yet interestingly I still couldn't reproduce the error with kms key and from assume roles credentials not access keys. I also took a look at .NET fix, and our header is injected correctly and has not problem with that.\nWould you mind contact Aws support to help look into your request IDs? If it's related to SDK, they will find us as well.. @pchaganti Hmmm, quick question to double check, when you using the console, you are using the same assume role that is used with SDK?\nHere is my reproduce steps for reference if you notice any difference with yours:\nI create an assume role, I used this assume role create kms key. I have db cluster in 2 different regions, I create cluster snapshot in source region. I use this role for rds client, used your code snippet and replaced parameter values only and make the request.. Thanks, taking a second look ... I'll take a look at the console one as well. Hmmm still couldn't reproduce, still can see region copy happening as expected. I launched 2 encrypted db instance from scratch in region A, B. Created snapshot in region A, use RDS Client in region B with its kms key id and source region as A etc. Your code snippet does look good to me though.. @MMartyn Sorry for the late reply, yet I couldn't reproduce the error for the API. Feel free to contact Aws Support as well, they can help look into request Ids. Or other channels listed at Geting Help section. @thegreyfellow Sorry for the late reply. What you have observed is the current behavior for current resource design. Instead, you can call #list_objects_v2 API directly. As for top level objects, you can try:\n```ruby\ns3.list_objects_v2(bucket: \"bucket\", delimiter: '/')\nsample response\n=>#,\n   #,\n   #],\n ...\n common_prefixes=\n  [#,\n   #,\n   #],\n ...\n``. Closing : ) feel free to reopen with further comments.. The error is not related to the change and is 1.9.3 only, cannot reproduce in a local rbenv 1.9.3.\nMerging fix changes right now, will work on a separate fix if the failing presists.. Thanks for reporting, have reached out to doc team about this :). This is tracked by doc team, docs updates will be auto include in SDK release once it's fixed.\nClosing, free feel to reopen with further comments : ). Thanks for reporting, have reached out to doc team about this :). This is tracked by doc team, docs updates will be auto include in SDK release once it's fixed.\nClosing, free feel to reopen with further comments : ). @brianknight10 Hmm I'm guessing  \"path ...\" is a mis-paste in your code snippet, yet I cannot reproduce the error with your code snippet:\n![image](https://user-images.githubusercontent.com/10790394/31639961-797084ce-b290-11e7-922c-e50e99b22654.png)\nAs you can see the stubbed value is returned\n. Thanks for reporting, I can reproduce this, reaching out to service team. Soft update, service team is aware of this and this is tracked.. Wow sorry to hear that, sure, there are several channel, you can contact [Aws Support](https://console.aws.amazon.com/support/home) if you want direct conversations with service team, and they will reach out to us if they need our help. You can open an issue with us as well. Also you can also open stackoverflow questions with tags, service team will help out as well. Further more, there is \"Provide feedback\" button sits at bottom for documentation page, like [cloudformation](http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/Welcome.html).. Closing, once service team have this updated, it will flow into our release automatically : ). @lwoggardner Thanks for the info. I can reproduce this locally. A workaround is by callingstack.stack_idinstead, it will return the arn value. This is actually a correct behavior for stack that#stack_namereturns name only and#stack_id` returns the arn.\nTaking a look at V2 about the difference behavior now .... A short summary for this regression here, so in V2, every attribute is fetched from data, in V3, since we are code-generation those resource objects, their identifiers are not loaded from data anymore, those identifiers helps with object initialization instead. \nPersonally, I think using those non-matching input to expect a matching result is not a good practice there. Those identifiers are chosen for resources for a reason usually, for example, as for \"Stack\" object, \"stack_name\" is a require member while \"stack_arn\" is not. We have the promise to keep v2 compatibility there, however I'm not sure whether your usage case is a validate scenario that we would like to keep. Let me know how this usage is beneficial to you, any limitations there?\nOpen to hear your thoughts and happy to have discussion : ). I'm merging this right now, there are other SDKs exposing this already and this will not introducing breaking changes because \"es\" is the only one with empty entry.. Good to know! So resource #wait_until is marked as deprecated mainly to due the fact that the shared resource model is easy to be outdated when compared with client API models, we would not remove it to introduce breaking changes : ) And we have a backlog feature to support custom waiters as well : )\nClosing, feel free to reopen with further comments or questions, I'll also add a  \"+1\" for custom waiters in our feature backlog : ). Hmm they are actually here at client docs, might be a YARD rendering problem, will fix this. Hmmm, tag does check :key, the key value should be a string type instead of symbol type. \nWeird, from your code snippet, it's a string, taking a look to reproduce.. Hmmm I copy paste your snippet and tried with a newly created image and succeed for me. . I see, yeah, it's true that we would prefer stronger validations since they are string types.\nSorry to hear and appreciate that! Feel free to open issues with us with any problems you have in migrating, always happy to take a look : ). @massimocode Hmm weird, I cannot reproduce this in either V3 or V2 SDK with ENV['AWS_SESSION_TOKEN'] set to empty string specifically.\nCan you try not set the token in environment variables if it's not required? Basically, SDK just try fetch \"AWS_SESSION_TOKEN\" key from ENV, since it presents, it will use its value, which is usually used for assume roles.. Closing due to ages, feel free to reopen with further information : ). Thanks for reporting, you are right, this looks like cause by shared resource model definition here, marking this as resource models issue. Meanwhile, you can use the client API directly to workaround this issue.. Tracked in feature backlog as a shared resource model issue, closing.. Sorry for the late response, could you try contact Aws Support for a directly conversation with service team? Since this is an server side response error, they will have more knowledge than us. They can also help look into your  request IDs to see if anything goes wrong.. I contacted service team internally with those info, will update. So to clarify a bit, there is no V1 and V2 bucket concept, you just need to make sure you are using the put_bucket_lifecycle_configuration operation instead of put_bucket_lifecycle for all targeting buckets. If you still see an issue, could you try setting :http_wire_trace to true to give us a full http wire trace for debugging?. @SamSaffron Hmm I cannot reproduce by:\n1. using #put_bucket_lifecycle_configuration(opts) \n2. using #get_bucket_lifecycle_configuration-instance_method\nThe response echo the correct format as expected:\n=> #<struct Aws::S3::Types::GetBucketLifecycleConfigurationOutput\n rules=\n  [#<struct Aws::S3::Types::LifecycleRule\n    expiration=#<struct Aws::S3::Types::LifecycleExpiration date=nil, days=3650, expired_object_delete_marker=nil>,\n    id=\"TestOnly\",\n    prefix=nil,\n    filter=#<struct Aws::S3::Types::LifecycleRuleFilter prefix=\"documents/\", tag=nil, and=nil>,\n    status=\"Enabled\",\n    transitions=[#<struct Aws::S3::Types::Transition date=nil, days=365, storage_class=\"GLACIER\">],\n    noncurrent_version_transitions=[],\n    noncurrent_version_expiration=nil,\n    abort_incomplete_multipart_upload=nil>]>\nCurious and guessing, are you using #put_bucket_lifecycle(the deprecated one) to put configuration originally?. I believe that might be the issue from those API coupling, I'll talk to service team for confirmation and best practice for this. \nFrom SDK side, we just parse the response we received from server side for the API you call and we have no ability to change the logic at server side.. Summary: get_bucket_lifecycle is used with put_bucket_lifecycle, and likewise, get_bucket_lifecycle_configuration is used with put_bucket_lifecycle_configuration. Mixing the usage will throw an error there. \nThus if you want to use get_bucket_lifecycle_configuration for bucket that has lifecycle config with put_bucket_lifecycle, the simplest way is to delete that first with delete_bucket_lifecycle and create a new one with put_bucket_lifecycle_configuration before calling get.\nClosing, yet feel free to reopen with further questions : ) . Hmm, using the same code snippet with same API, I cannot reproduce the error in V3. @fdr Ah I see, sorry for the confusion, this is about the stubbed response instead if a real API call. I can reproduce this, taking a look. Sorry for the late response, so this is a problem with our empty struct for stubs. PR #1655 is opened and is under review.. PR has been merged, will be included in the next release.. The fix is out in the latest release for core, closing, feel free to reopen with further comments or questions. Thank you for your contribution, I have forward those fixes to AWS Doc team : )\nAfter this is fixed from upstream, it can be shared across all SDKs and Tools (including Ruby : ) ). Those have been fixed across SDK and Tools, closing. @patrobinson Thanks for reporting! This has been fixed, RubyGem yelled at us at the first time :) . @mike-bailey I can reproduce the issue, to echo Alex response, I'll also help ping service team on this for updates. A quick question to confirm, is #1460 #1577 all for the same issue? Any preference for a single one to keep? I can help reopen #1460 too if you like as well : ). @mike-bailey Cool, I'll reopen #1460 and close this one, also echo updates in #1577 : )\nSo I'd need the information from service team to see which SDK console is using and how did they provide parameters and got succeed call, they might have provided more parameters than we thought : ), if its a specific Ruby SDK error, we would like to address it definitely. If it's a API usage issue, (e.g. there are fields needed in pairs), S3 can provide better documentation for it.. Thanks for the feedback! Tracking this in feature backlog.\nAt same time, while the release note will be few sentences, you can always find detail intro of each feature release at AWS what's new : ). Thanks for the patience! Ruby Gems yelled at us when we were publishing too many gems :( I'll enhance our release with better retry for Ruby Gems.. Thanks for the info, taking a look to reproduce. I can reproduce this, I reached out to service team about this, will update.. A quick update, 'REGION' is not supported for this API for now(but is supported for some other APIs), the error message is correct.\nSince SDK is generating the hint for the enum shape, it listed all enums available.\nSorry for the confusion right now, we have recommended service team to have different enum shapes for situations like this in the future.. Thanks, appreciate the information, just forwarded the message to S3 service team as well.. @richardkmichael Ah sorry about the confusion! It should be aws-v3.rb for V3 SDK instead, it's available at aws-sdk-resources gem and aws-sdk gem. I'll reach out to doc team to update the page. And I'll create a PR to address this in our README.md as well.. Some quick background there, aws.rb is the name of V2 SDK REPL file : ). Doc only. Thanks for reporting, working on the generator fix.\nAt same time, you can use wait_until method from EC2 client to workaround the issue (recommended).. Another soft update, this is a bug in shared resource model here, and our generator trusted the path value provided, I'll create a PR shortly from our side and fix the shared resource model upstream.. The PR fix has been reviewed and merged in, will be available in the next release. @arianf We use http_wire_trace to set :set_debug_output in Ruby net/http. We don't have much control over there. Curious about what information are you trying to see there?. Separate another user agent plugin, ready for a second look : ). closing, will open a new one with latest suits. Might need to pull in the latest changelog changes here . Amazon S3 supports both virtual-hosted\u2013style and path-style URLs to access a bucket. And Ruby support both, Ruby S3 gem default to virtual-host style, you can set :force_path_style to true at client constructor to get the path style pattern (in your CLI example).. Thanks for the feedback. The reason is that this verifier module doesn't use an SNS client, it doesn't make any client call but making http call directly to get pem, that's why Aws.config is never used for this module. \nTo support this request, SDK might somehow need to extend the message verifier to use the client http interface.. Currently there is no waiter available for SSM, marking this as a feature request for adding waiters for SSM service.. Thanks for the info! Could you share a full trace of the error log? The private_key can accept a string, and from your code snippet, I couldn't find where the response object is introduced?. Adds to that, not a perfect workaround but the source code contains all documentation, it might related to the docs used options but client is using args*. same issue with of #1649, tracking with the older one. Closing. closing due to inactivity, feel free to reopen with further information or questions : ). It's there already? \nClosing, feel free to reopen with further questions.. @NorseGaud this error for #socket_endpoint_error, you can see the endpoint SDK trying to hit by s3_client.endpoint. SDK will make request to the hard code endpoint provided. \n\nYet, I can make an HTTP GET to ...\n\nMay I ask is the request http instead of https? If so, you would need to update the endpoint you provide as well. Also, to help with debugging with wire log, you can set http_wire_trace to true at client.. Glad to know that your figured this out. Should have mentioned earlier with the path style and bucket DNS style for S3. It's true that DNS style is by default for S3 in Ruby SDK, and you have the option to set it to force path style as you mentioned. Since you are providing a customized endpoint,   it's not surprising that using path style makes it work : ) and you are on the right path.\nClosing, feel free to reopen with further questions or comments : ). Thanks for the ping, will take a look shortly. Thanks for the info! I've passed this issue to service team, will update.\nMeanwhile, feel free to contact [AWS Support] directly for this issue as well.. I revisit your issue, below snippet works for me:\nsg = ec2.describe_security_groups(group_ids: ids)\nsg.security_groups do |g|\n  ec2.revoke_security_group_ingress(group_id: g.group_id, ip_permissions: g.ip_permissions)\nend\nCan you tell me which operation cause the dryrun mismatch for you?\nI was working with support, the error he provided to me is different from what you have mentioned earlier, I couldn't reproduce a scenario where SDK doesn't use the parameter provided. I'll work with him to see what issue he ran into.\nAlso, to help with debugging, you can set :http_wire_trace to true from client. taking a look, I finally can reproduce this, will update. Okay, I finally realize what going on there @phene \n\nmissing mandatory parameter: exactly one of remote-security-group, remote-ip-range, remote-ipv6-range, or prefix-list-id must be present\n\nIt means EXACTLY ONE of those ip-range ipv6-range or prefix-list-id is require per one API call. The \"missing mandatory parameter\" is quite misleading.\nIt doesn't mean that you are missing parameters, it's suggesting that you can only provide one per one API call.\nI'll contact service team to update their doc and error message on this, it's quite misleading.. @phene My bad, It's my mistake in my earlier code snippet, I do miss the each at first, so it appear \"succeed\" first and actually nothing is called. After I add the each, I can reproduce same error you got, after I hard code ip_permissions with single required field, request succeed.\n. The point is you might need to do some tweaks instead of passing through the IpPermissions directly. the update can only process one of field per time. . To  follow-up, we were following up with service team to make sure error messages would be enhanced in similar situations : )  Appreciate your patience and efforts!\nClosing the issue as it has been figured out, free feel to re-open with further comments or questions :D. Apologies for the late response.\nI reached out to service team about this, will wait for their comments.\nMeanwhile, feel free to contact AWS support directly since this is an error emit from Server side. And AWS support can help open the conversation with service team directly.. Here is documentation that specifies which resources can be tagged on creation: \nThere is a mismatch between that table and the tags given as examples in the documentation for run-instances. Only instances and volumes can be tagged on a run-instances request; everything else must be tagged after it has been created, including a spot instances request. \nEC2 team will be working on updating documentation to avoid misleading descriptions.\nClosing, feel free to contact AWS Support directly with further issues :). Also ran rake build and no diff is generated for AWS services. Updated test model, without this diff, it would ran into\n```ruby\nFFFF\nFailures:\n1) Client Interface: API Gateway white label SDK populates x-api-key header correctly\n     Failure/Error: Kernel.module_eval(code)\n SyntaxError:\n   (eval):904: class/module name must be CONSTANT\n       class __AnonymousListElement < Struct.new(\n                                     ^\n # ./spec/spec_helper.rb:120:in `module_eval'\n # ./spec/spec_helper.rb:120:in `generate_service'\n # ./spec/interfaces/client/api_gateway_spec.rb:7:in `block (3 levels) in <top (required)>'\n\n``. @vbichov Interestingly, I cannot reproduce this. I tried the script and it worked for me.\nSome quick questions, issome-bucket-namein theus-east-1region for you? Did you provideregionfrom other places? like Aws.Config,AWS_REGIONparameters? For credentials, are you using/.aws/credentialsfile or/.aws/configfile?. @vbichov Does this still happening for you? SDK does support fetching region from'x-amz-bucket-region'` here.\nI also tried with a client in us-east-1 and fetching us-west-1 bucket, it succeed without an issue for me, also, it sounds weird when you mentioned a us-east-1 client failed to fetch us-east-1 bucket with this error. Could you provide more debugging info by setting http_wire_trace to true for the client? This would provide the wire trace, then you can identify which endpoint/region it's trying to hit and whether SDK is functioning properly.. Happy to hear that you have worked around the issue! closing.\nFeel free to reopen if it happens again or with further questions : ). Thanks for the PR! Merging .... Closing, free feel to reopen with further comments or questions : ). Taking a look, the test was succeed for me locally. Referring S3 API Doc for GetObject, the timestamp shape at querystring appears to be .utc.httpdate. Apology for the late reply, I agree that this is an un-intended regression that we introduced. \nWhen we are separating SigV4 gem, stronger restrictions are added as it became a gem that can be used without AWS service gems. We were hoping that helpful error checking can prevent an invalid url in the first place( as string can be anything with un-intended typos).\nFor workarounds, could you try to_i method for ActiveSupport::Duration?\nClosing, feel free to re-open with further comments/issues : )\n. Thanks for the fix, merging. @limitusus From the wire log, there is no ValidationMethod, this means it's not provided from server response, thus it failed to be fetched from client side.\nI've reached out to service team for comments, meanwhile, feel free to reach out to AWS Support directly with the issue.\nWill update! : ). Closing as the question has been answered by AWS Support, I have reached out to service team to improve their documentation on this behavior.. The PR has been merged, closing\nIt'll be included in the next release. Thanks! Could you also add a changelog entry here, such that your changes can be included in the next release of aws-sdk-core gem?. Under \"unreleased changes\" section is fine, just follow the format to add an entry. Our build tools will auto bump the version in the next release : ). Thanks for the contribution! Merged. Thanks for the report, since this API is missing from JSON models SDK consumed, it means it hasn't been released through SDKs and CLI yet. I'm reaching out to service team about this and will update with their comments. Likely it might be released in the near future.. Could you help provide a bit more info. there thus that I can help pass over to service team?\nSDK parse the response server gives back, service team can help look into request ids and provides comments on whether it's a bug or misunderstanding for the API behavior.\nIt would be great if you could provide\n1. a code snippet of usage of the API\n2. where on console/docs you find the mismatched pricing\n3. at wire log of the request (setting http_wire_trace to true at client side). Thanks for the info, appreciate that. I passed this over to service team, will keep update!. Soft update, service team are working on updating the API to reflect update pricing. Thanks, I've passed the message and working on getting an estimate time. It's a bug recognized on their of failing to update the latest pricing.. Soft update, the estimate time is one or two day, and fix is already in progress.. Got update from service team that the fix has been completed, closing, feel free to reopen if you still see an issue! : ). @dineshyadav009 I'll pass the info. \nJust a side note, contacting AWS support will directly pass the info to service team when this is a fix needed from server side. That's why we recommend doing so : ). @dineshyadav009  Could you let me know more information of the diff? Is  i3.xlarge the only one? . Thanks! I've passed this over to cloud watch docs, and they can make the changes upstream and share with all SDKs and CLI.. Sorry for the delays, first of all, I can reproduce the scenario with your code snippet, the code line we take in a file/tempfile is here, nothing special.  \nI think this has to do with the fact that you created a Tempfile, wrote contents but didn't close the file. Thus that data from write still lived in the buffer not flushed in, so when SDK call read on the file, it actually returns nothing. Then first request attempt will failed with error message like Your socket connection to the server was not read from or written to within the timeout period. Idle connections will be closed.. \nSDK has build-in default retry for 3 times, at retry_request, you will see that we rewind request body before resend the request. Thus, the second request succeed this time. That's why a manual seek also fixed the issue : )\nYou can see what I described by setting http_wire_trace to true at the client, then wire log will be visible. In summary, the long hanging time is for the failed first request attempt, not SDK serializing delays.\nClosing, feel free to re-open with further concerns or questions :D\n. Thanks for the contribution, I'll pass this information to service team such that they can update the model upstream and it can be shared across all SDKs and CLI. @fallwith Updates here, this is working as intended, DescribeTasks does not require cluster, but if you do not specify the cluster, the default cluster is assumed, your task is not in the default cluster, that's why MISSING is returned.. Well, for direct API docs and examples, no. However, you can always submit feedback from Github or AWS Documentation page (at the bottom, where is a link for feedback). You are welcomed for contributing developer guide as well, which is open sourced here : ). Thanks for reporting! In the last 36 hours, 2 releases happened, none of them changed aws-sdk directly, but updated its dependency gems with API updates only.\nI tried to reproduce this locally(OSX) with 2.4, but aws-sdk gem installed without any issue. But I do see same warnings from RubyGems of 502 Bad Gateway, though doesn't affect installation. I also tried 2.3 and warnings are the same but still installed successfully.\nI'll try spin up a EC2 ubuntu instance to see if I can get it reproduced there.\nSome quick follow up questions since it feels like rubygems API issue\n1. Is the error log attached contains everything?\n2. Is this starts happening every time in the last 36 hours or is this a transmit issue?\n3. Have you tried open an issue against RubyGems already?. A soft update, I'll be working on checking whether our patching has been fixed by latest Ruby versions.\nI'm kind of full bandwidth right now, apologies for the delay :D\n. PR #1756 has been open to avoid patching for Ruby 2.5, since it might take longer for me to set up to test than your current environment : ) Could you help test the PR branch to see if that fix the issue?\nWant to double check this since although that PR avoid patching for 2.5, it still keeps the logic we need there : ). Fix PR has been merged, will be out in the next release. Release containing the fix is out, closing, feel free to reopen with further questions : ). For build_tools changes, they are all for generated diffs like following pattern (only shows key diffs here, will have a complete code-gen diff example when PR is more finalized)\n\n\nAdd event_stream_handler option\nruby\n...\nadd_plugin(Aws::Plugins::EventStreamConfiguration)\n...\n\n\nFor S3 event stream operation\n``ruby\n  def select_object_content(params = {}, options = {}, &block)\n      params = params.dup\n      event_stream_handler = case handler = params.delete(:event_stream_handler)\n        when EventStreams::SelectObjectContentEventStream then handler\n        when Proc then EventStreams::SelectObjectContentEventStream.new.tap(&handler)\n        when nil then EventStreams::SelectObjectContentEventStream.new\n        else\n          msg = \"expected :event_stream_handler to be a block or \"\\\n            \"instance of Aws::S3::EventStreams::SelectObjectContentEventStream\"\\\n            \", got#{handler.inspect}` instead\"\n          raise ArgumentError, msg\n        end\nyield(event_stream_handler) if block_given?\nreq = build_request(:select_object_content, params)\nreq.context[:event_stream_handler] = event_stream_handler\n  req.handlers.add(Aws::Binary::DecodeHandler, priority: 95)\nreq.send_request(options, &block)\nend\n```\n\n\nFor event shapes\nruby\nclass RecordsEvent < Struct.new(\n  :payload,\n  :event_type)\n  include Aws::Structure\nend\n\n\nFor event stream shapes\n``` ruby\nclass SelectObjectContentEventStream < Enumerator\n\n\ndef event_types\n    [\n    :records,\n    :stats,\n    :progress,\n    :cont,\n    :end\n    ]\n  end\nend\n```\n\n\nAdding default eventstream wrapper for callbacks\n```ruby\nmodule Aws::S3\n  module EventStreams\n    class SelectObjectContentEventStream\ndef initialize\n    @event_emitter = EventEmitter.new\n  end\ndef on_records_event(&block)\n    @event_emitter.on(:records, Proc.new)\n  end\ndef on_stats_event(&block)\n    @event_emitter.on(:stats, Proc.new)\n  end\ndef on_progress_event(&block)\n    @event_emitter.on(:progress, Proc.new)\n  end\ndef on_cont_event(&block)\n    @event_emitter.on(:cont, Proc.new)\n  end\ndef on_end_event(&block)\n    @event_emitter.on(:end, Proc.new)\n  end\ndef on_error_event(&block)\n    @event_emitter.on(:error, Proc.new)\n  end\ndef on_event(&block)\n    on_records_event(&block)\n    on_stats_event(&block)\n    on_progress_event(&block)\n    on_cont_event(&block)\n    on_end_event(&block)\n  end\n# @api private\n  # @return EventEmitter\n  attr_reader :event_emitter\nend\n\n\nend\nend\n``. Pending thoughts foron_initial_response` trigger for fetching headers members values available besides stream data from payload body.\nUpdate\nThey will be sent in body with a separate event type initial-response, working on parser updates. @jclusso Thanks for the ping! Just let you know that this has been actively working on, and it's our top priority, we would like to get this out ASAP.\nCurrently functionality has been completed, and it's in the progress of completing test suite and documentation :D. @jclusso It's because that gem is in a separate PR: https://github.com/aws/aws-sdk-ruby/pull/1754 ;)\nI'm working on get that released soon as well. Attached s3 code gen changes in the last commit.\nReady for a final review @trevorrowe @awood45 . CI is failing on region_spec which shouldn't be relevant, also received warning on initializing enumerator. working on fixing those\n=> Fixed by updating hard code region check line number : )\nThe current CI failing is 1.9.3 specific (1.9.3 doesn't work with const_get from complex string), working on work-arounds\n=> Fix ruby 1.9.3\nAll tests passing, no warnings now :D. The CI failed to find method for Enumerator ... Hmmm working on investigation and fix. addressed 1st round doc feedback locally, will update once get complete 1st review :D. Hmmm, so first I cannot reproduce this at my Mac environment, and I tried few s3 resource usage they both work fine. Since no code update, there is no way that newly release code might break your app. The resource object won't make http calls until fetching attributes associations etc. So crash at initialization step is irrelevant to server side.\nFrom the error trace, it failed at actionview controller_helper originally, could you check those third-party dependencies? Also, is the issue still reproducible if you use aws-sdk-s3 gem solo?. @tyrauber Thanks for the catch!. @ngocpea Thanks for the feedback! The region option is mainly used for endpoint construction and signature v4 signing for API request. You're right for service with global endpoint, it doesn't require region info to be presented at endpoint, and for signing, it will default to us-east-1. \nSDK requires region and credentials to be provided for all AWS services in general, that's why it's required for IAM as well. . I've addressed this in the original PR, closing. @Fryguy Could you update the changelog file with your changes under unreleased changes section? Thus that this change can be included in the next release \ud83d\ude03 \n. @Fryguy Could you pull/rebase latest changes into this PR? \ud83d\ude03\nWe need to use the unreleased changes section to do correct version bummp. Thanks! I'll merge it once CI complete, it should be included in the next release. Thanks for the contribution! this has been updated at patch. Thanks for the contribution! Could you also add a changelog entry here such that your change can be tracked in the next release?\nJust add an entry under Unreleased Changes section same pattern would be fine, don't need to worry about version number or date : ). A nit picking there : ) \nI'd like to see Add sending a User-Agent other than the default User-Agent in Ruby.  Make change to  Adding. Because for regular API request, Ruby SDK request stack does has a plugin for adding user-agent It's special that those instance credentials are making request directly  and avoid the request handler stack we have.. Thanks! merged, will be out in the next release. @Dimcha So for v1, its AWS and for v2 and v3, its Aws, v1 and v3 can co-exist, v1 and v2 can co-exist but v2 and v3 cannot. For v3, we current support Ruby version from 1.93 to 2.5, may I ask what issue do you have when trying to upgrade to V3?  Noted that, if you are using v3, you can use aws-sdk-s3 (modularized gem) instead of aws-sdk(all service included).\nI'm less familiar with paperclip versioning with aws-sdk versioning. I'd need to take a look at paperclip dependency to give advice there, could you let me know is there special gem versioning constraints you are looking for?. Just s soft update, I can reproduce this, looks like an issue when our xml parser is expecting 'Tag' while provided with a 'Key', still investigating. Just figured out what the issue is, so TagSet shape was modeled to be flattened at shape reference instead of shape itself, thus, our parser failed to detect it wants to be flattened.. PR #1766 addressing the fix is opened for review. PR has been merged, will be out in the next release. aws-sdk-core Release 3.20.1 containing the fix is out, closing, feel free to reopen with further questions or issues. @rpbaptist You can provide :endpoint parameter with port number, and that port number will be used for making request. e.g. Aws::SES::Client.new(endpoint: 'https://email.us-west-2.amazonaws.com:1234'). Closing due to inactivity, feel free to reopen if the issue persist and you can provide further info, :D. Thanks for the feedback and sorry for the delays!\nPR #1812 addressing this feature request is opened and under review. The PR has been merged, will be include in the next release :D (likely aws-sdk-s3, v1.16.0) closing. Working on reproducing .... I can reproduce this, and I reached out to service team for comments. At very least, documentation should be updated : )\nMeanwhile,  feel free to contact AWS Support as well, that would create a direct conversation with service team.. Soft updates, for 'tcp', it should be number string '6' instead, service team is working on updating documentation.. EC2 Doc team is actively working on fixing the confusion there and new contents will be pushed soon : )\nResolving since there is nothing more we can do here, once their doc is updated, SDK will auto pickup those changes : ) Feel free to reopen/ let me know if you see no updates for a while. @equivalent By using the first snippet, I couldn't reproduce the issue. I was able to authorize and revoke ingress rule successfully for a security group (though the response is empty). \nCould you provide a wire log for the request that failed to remove the rules? Make sure double check not to include sensitive informations : ) And I'll pass that to service team to take a look the request. Also, if you have an AWS support account, feel free to contact them directly, it would open a direct conversation with service team : ). @jkburges Yes, it's possible : ) Just as the way you assumed in the example. @jkburges Yes, so if you provide :credentials option directly with any credential provider. It will no longer looking for the default credentials chain. \n(You can think it as overwriting default credential chain : ) ) \nOur documentation should do better documentation for instance_profile_credentials_retries option, tracking this as a documentation enhancement in our backlog. We did provide changelog example when we initial launch the feature.\nYou are welcome to submit PR for them as well! Just following the same format (docstring)[https://github.com/aws/aws-sdk-ruby/blob/9ea447b7e9e57e27bb7924aca40a5bf0c3265a94/gems/aws-sdk-core/lib/aws-sdk-core/plugins/credentials_configuration.rb#L24] would be fine :)\nClosing, feel free to reopen with further questions or comments.\n. @koshigoe Fix PR #1792 is under review, I'll work on including this in the next release (likely tomorrow)\nFeel free to try out the patch :) Thank you for your patience!. Latest release v 3.21.2 of aws-sdk-core is out, closing.\nFeel free to reopen with further issue or questions :). You might be interested in EC2 waiters, there is instance_status_ok waiter available.\nBasically, waiters are helping you polling until reach a target state, and polling behavior is configurable.\nrelated Blog Post. Closing as suggestions are provided, feel free to reopen with further comments/issues : ). Thanks for the info! I can reproduce the same SDK pattern you mentioned there, however, I can use the SDK generated presigned url to put an object successfully.\nHere is my code snippet:\n```ruby\nobj = Aws::S3::Object.new(bucket_name: 'bucket', key: 'hello')\nurl = URI.parse( obj.presigned_url(:put, metadata: {\"foo\" => \"bar\"}))\nbody = \"heyhey\"\nNet::HTTP.start(url.host) {|h| h.send_request(\"PUT\", url.request_uri, body, {\"content-type\" => \"\"})}\n=> #\nobj.get.body.read\n=> \"heyhey\"\n```\nTo help further investigate this, could you let me know if/how the generated url failed for you?(Like usage code snippets? Error message?) Also, could you also let me know the version of SDK used? : ). Thanks, apology for forgetting to mention this in my original comment, I can see the metadata is added to the uploaded object:\n```\nresp = client.get_object(bucket:\"bucket\",key:\"hello\")\nresp.metadata\n=> {\"foo\"=>\"bar\"}\nThen, are you suggesting something like this?ruby\nNet::HTTP.start(url.host) {|h| h.send_request(\"PUT\", url.request_uri, body, {\"content-type\" => \"\",  \"x-amz-meta-foo\" => \"bar\"})}\n``\nI don't think this will sign thex-amz-meta-foo` header. Because you are making request with a presigned url, I don't think headers sent when making the request can be signed since url is already presigned.\nThe metadata is an parameter option for put_object instead of a parameter for presigned url for adding custom headers, are you actually looking for signing a custom header in the presigned url?. Have you tried any of the credential configuration at our README? If so, how did you provide credentials?. Happy to hear you figured it out : ) ! closing. Thanks for the contribution, PR has been merged, will be out in the next release!. Ah I see, ruby 1.9.3 hash object doesn't have to_h method at that time. And your test is providing a mixture of Struct and Hash (which is good as caught by testing!)\nCould you also add a changelog entry here such that we can track this PR in the next release? It can start with Issue pattern\n. Thanks for the catch, it's a description typo, we do resolve logic in AWS_REGION AMAZON_REGION AWS_DEFAULT_REGION order. PR #1817 fixing this has been merged : ) closing. If you are using V2 SDK, AppStream is available since 2.6.31, V3 SDK has AppStream since launch (aws-sdk-appstream).\nClosing, feel free to reopen with further issues or questions : ). Could you provide more information about your usage? For non-AWS S3 endpoint, are you suggesting making s3 requests to a testing/mocking endpoint? \nI looked at the linked, you are looking for an option to disable/enable SHA256 sign sigv4 payloads? may I know more about why you would like to have this feature? ( Trying to sure we scope correct if tagging this as a feature request ) . This is now ready for review. @kplimack Quick follow-up question to help me reproduce the issue, does this happen every time every file for you or just occasionally? A rough size of the file? Is this script ran in a multithread environment? . @peggles2 Are you also experiencing this in a multithread env? Or happening when you are using sdk in your APP? What's the APP environment? Does this happens all the time in your case as well?\n@kplimack Soft updates that I'm still working on reproducing the issue, so far failed to reproduce on my mac osx with 5g files with 10 threads in parallel. I'll try with more thread or spin an instance with similar OS and ruby version that you mentioned : ) Let me know if you have any updates on your side that might help me reproduce or invetigate :D. The underlying API call is #describe_db_instances, which doesn't include tag information in response. You can try client API #list_tags_for_resource instead. closing, free feel to reopen with further questions : ). Sounds like you can try API #describe_images ?. Thanks for reporting! this is a typo in the resource model definition here, the glacier resource code you mentioned is auto generated from the typo in definition.\nTo unblock, you can use client API directly https://docs.aws.amazon.com/sdk-for-ruby/v3/api/Aws/Glacier/Client.html. Could you help point me to the python feature for DAX? : )  I'm looking at their Docs for DAX https://boto3.readthedocs.io/en/latest/reference/services/dax.html#client  I don't see #get_item there? \n. @softwaregravy  The code works for me for both v2 and v3, may I ask is the environment having 'S3_BUCKET' value when you are running v3 code?. After approve will port be to v2 as well. PR #1867 is opened addressing this issue. PR has been merged, now you can try:\n```ruby\nclient.api_requests(exclude_presign: true).size.zero?\n=> true\n```\nThis will be out in the next release, closing : ). Also addressed doc issue mentioned in #1868 . @antpaw Thanks for reporting! It's a typo in the doc, you can try\nbucket = s3.bucket('my.bucket.com')\n bucket.url(virtual_host: true)\ninstead :)\nI'll update the doc. PR #1867 covers this doc update.\nClosing, feel free to reopen with further questions : ). Hmmm, interesting, could you provide info on how you install it?\nYou can see it's alive there at RubyGems https://rubygems.org/gems/aws-sdk-cloudwatchlogs. Hmmm, so first of all, for cloudformation client, we didn't have any customization there.\nWe do have customization for Rds client that helps auto populate pre_signed_url if source_region is provided and pre_signed_url is not provided for theses operations. It's likely that server side used tools/mechanism have something similar.\nCould you try not providing the SourceRegion parameter? If that's not working, this might be a good question to be passed to service team. PR #1881 addressing the issue is under review . PR #1881 has been merged, fix will be out in the next release :). will follow up with unit test fixes. Tests fixed, changelog added, ready for final review.. Integ tests added also added dynamo code-gen diff, feedbacks addressed, a final check? :) @awood45 . https://rubygems.org/gems/aws-sdk-core \nfor referencing latest version\nready for a final review before merge :) @awood45 . this PR is now ready for review :). Ready for another round of review \ud83c\udf89 @awood45 . With #1993 tracking instead. This PR is ready for review. Ready for another review  \ud83c\udf89. Yep, I rebuild all services with rake build and no diff is generated. PR #1980 opened under review. release containing the fix is out https://rubygems.org/gems/aws-sdk-core/versions/3.46.2. Update: latest version with fix is released 3.48.0, closing. That's true, I was thinking checking the intersection of  the 2 arrays before getting the result. It seems redundant, I'll remove that.\n. Typo indeed, thanks for catching\n. Could those url parsing part use a method from Signer module? . Maybe separate a helper method  that could be reused in CookieSigner ?. Perhaps separate signed_content.map{ |k, v| \"#{k}=#{v}\" }.join('&').gsub(\"\\n\", '') to a signature variable in another line to make this more readable?. Since Signer module is available, then this #signed_url might doesn't need to duplicate the code?. Perhaps you are testing this as what PHP SDK does : )\nI'd more like to have this to test the actual value, since we already have a mock private key there.\ne.g., similar to how we test url_signer, I'd like to see not only those key matches, but also their value make sense : ). @hamadata  Sounds good, make sense to me : ). Since #signed_url has been removed from this class, could you make changes at documentations as well?. Using hash is fine, I'd prefer having expect key by key, which might make debugging easier in the future.\nPerhaps something like \nruby\nexpect(cookie['CloudFront-Policy']).to eq(' ... ')\nexpect(cookie['CloudFront-Signature']).to eq(' ... ')\nexpect(cookie['CloudFront-Key-Pair-Id']).to eq(' ... '). That's fair, tests added : ). Added.. Make sense, renaming done.. Fixed and refactored in the latest commit.. Sounds good, fixed and refactored in the latest commit. Happy for offline discussion if I still miss something  : ). Good to know, Initially, I use = because for ranges, it will look like bytes=12345-56789, thus I use part_number= prefix to echo that, sounds like it might not be a good option there. Happy to discuss a better naming strategy : )\nP.S. sort_files is introduced because I noticed that original Ruby string compare is having trouble in comparing strings like \"12345-56789\" in a expected numerical way.. That's correct, thanks for catching! Will do that :). Make sense, thanks for the suggestion, taking a look at #copy_stream :). @janko-m Great catch! I did some simple test and you are right about this. I started this PR before the partNumber is released, looks like the actually API differs a bit, anyways, thank you so much for catching this! I'll  fix this and add another cucumber test to make sure that scenario is taken care of properly : ). Make sense, thanks for the advice, will address those : ). Ah that's right, I'll fix that. Make sense, fixed in the latest commit.. Ah that's fair, fixed this in the latest commit.. Fixed, will be EventStreamParserError.new .... Yeah ... though current no real server response is available for testing this, I'll adding this . As talked offline, I'll just let it throw error there for other protocols to make sure we proper test those before release their support. Nice catch, I'll definitely consider more careful check for this one. Looks like :agent_port is not required and doesn't have a default value? \nAnd if it's not provided, at #send_datagram, data will not be sent? . I agree for rescuing this and not hard failure. Maybe some warning message or some info that can be logged?\nThen if customer are having issue with this, there are some debug trace to be found?. Is this WIP or a discussion question or enhancement to be made later?. Lol love to see long list of variable aligned \ud83c\udf89 Is the concern there trying to make the ApiCall/ApiCallAttempt to be explicit as they are private used classed?. So by default client_side_monitoring_publisher enabled but without a port unless client_side_monitoring_port/ENV is specified? Then actually it's not working by pure default? . nit pick, the default variable is not used?. Also feel same? as we are tracking those latency data metrics anyway?. This this comment something to be removed? Or for discussion?\nI image for success requests, at this point, all data need have been collected already?. Nit-Pick\nThis line can be removed. This is fine, but probably smoke_tags can be per file base instead of per test case?. Nit Pick, maybe an extra line between 2 test cases?. how about using to_h of Aws::Structure : ). Ah, you don't need to provide the version stuff here, our release tool will bump the version automatically.\nSorry for the confusion, could you remove these 3 lines and just keep the issue entry with its following new line?\nAfter this is cleaned up, I'll merge the PR. could you add more documentation for #initialize like what kind of process variable it's expecting?. Aws::Credentials?. Could you also add some doc here like an example of how to use this?\ne.g https://docs.aws.amazon.com/sdk-for-ruby/v3/api/Aws/AssumeRoleCredentials.html. Test suites looks good, just checking does these covered all cli test suits for the process credentials as well?. surprise! ;)  you need to update the region line here: https://github.com/aws/aws-sdk-ruby/blob/master/build_tools/spec/region_spec.rb#L12\nThat spec is for testing we don't hard code region names, since the error file has been modified, the line number need to be modified as well :D That' why you are seeing the travis error. nitpick, could you make this multi-line instead of this super long line?. I'd +1 for this, any follow up or discussion on the advice?. If not create a delicate class for it, could you document that each element in the array is actually a hash containing request info? And supported keys for those hashes?. Added more tests in the latest commit, unfortunately, it would be tricker for failed scenarios since  that will happened at Kernel require time later at build times. Yeah, currently no. This event signature v4 signing is currently used for event sending only and received events only requires eventstream decoding.. This doesn't affect test result, but just a typo:\nthe region 'us-east-1' is wrong\nshould be 'us-west-2'?. Addressed in the plugin file doc_strings :). Yep, it's affecting docs and Aws::Svc::EventStreams class generation,\nsamples can be viewed at: https://github.com/aws/aws-sdk-ruby/pull/1945/files#diff-78e820efc317198d8b9bab2f79d7fa47. Yep, sample: https://github.com/aws/aws-sdk-ruby/pull/1945/files#diff-02d74b79db5eac02ca88a5b6f2154efb\n(the diff is hidden by Github :()\nBasic, I avoid to throw build/load errors but errors out immediately when they are trying to initialize an AsyncClient. hmm makes sense, currently \nhttps://github.com/igrigorik/http-2/releases\nmaybe \nspec.add_dependency('http-2', '~> 0.10')\n?. if we are using metadata for identifying dynamo, how about a value that's unlikely to change and unique? I'd prefer service name related values or service id https://github.com/aws/aws-sdk-ruby/blob/master/apis/dynamodb/2012-08-10/api-2.json#L3\nAlthough very rarely but it did happen that endpoint prefix changed for service models. ",
    "mcfiredrill": "I had the same issue as @tyrauber .\nI'm trying to put to a bucket with a presigned url and set the acl to public-read.\n```\n    client = Aws::S3::Client.new(                                                                                                                \n      :region => 'us-east-1',                                                                                                                    \n      :access_key_id => ENV['S3_KEY'],                                                                                                           \n      :secret_access_key => ENV['S3_SECRET'],                                                                                                    \n    )                                                                                                                                            \n    signer = Aws::S3::Presigner.new client: client                                                                                               \n    url = signer.presigned_url(:put_object, bucket: ENV['S3_BUCKET'],                                                                            \n                                            key: \"mykey\",                          \n                                            expires_in: 10.hours.to_i,                                                                           \n                                            acl: \"public-read\")                                                                                    \nI'm sending these headers on the client:\n          ajaxSettings: {                                                                                                                        \n            headers: {                                                                                                                           \n              'Content-Type': 'audio/mpeg',                                                                                                      \n              'x-amz-acl': 'public-read'                                                                                                         \n            }                                                                                                                                    \n```\nThe file starts to upload, but I get a 403 halfway through:\n<Error><Code>AccessDenied</Code><Message>There were headers present in the request which were not signed</Message><HeadersNotSigned>x-amz-acl\nYet it looks like the header is present in the url:\nRequest URL:https://streampusherdev.s3.amazonaws.com/datafruits/02Hiccups.mp3?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAJSOIWA7BITEZINJA%2F20170504%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20170504T032947Z&X-Amz-Expires=36000&X-Amz-SignedHeaders=host&x-amz-acl=public-read&X-Amz-Signature=7127b81a6b9fa7449205fb1b62d43439b1ccf3779054eaed9aa76734be8a681a\nRequest Method:PUT\nStatus Code:403 Forbidden\nI tried removing the headers from my http request on the client side, but it doesn't seem to make a difference.\nI downgraded to 2.0.39 and things were working well (I have to specify the headers on the client side http request). The request url looks a little different:\nRequest URL:https://streampusherdev.s3.amazonaws.com/datafruits/02Hiccups.mp3?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAJSOIWA7BITEZINJA%2F20170504%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20170504T034428Z&X-Amz-Expires=36000&X-Amz-SignedHeaders=host%3Bx-amz-acl&X-Amz-Signature=00e0f0f4f7236b361dc2ae927327b108f16c5040dbdbf86c1420194616cd966a\nx-amz-acl is still present in the X-Amz-SignedHeaders, but the value for it is not.\n. @cjyclaire Thanks I may give that Gem a shot \ud83d\udc4d . ",
    "robert2d": "Thanks for getting back to me. I ended landing on https://github.com/rslifka/elasticity which provides an interface the the modern emr API via Ruby. Cheers\n. bundle update derp\n. ",
    "vbanthia": "Understood how to upload the file.\nWhile creating upload using create_upload api, pre-signed Amazon S3 URL will be returned in response. And then upload apk on that url. (http://docs.aws.amazon.com/AmazonS3/latest/dev/UploadObjectPreSignedURLRubySDK.html)\nSample code:\n``` ruby\ns3_url = \"xxxxxxxxx\" # got from create_upload response\nurl = URI.parse(s3_url)\napk_contents = File.open(\"buggy_app.apk\", \"rb\").read\nNet::HTTP.start(url.host) do |http|\n  http.send_request(\"PUT\", url.request_uri, apk_contents, {\"content-type\" => \"application/octet-stream\"})\nend\n```\n. ",
    "cliffano": "Thanks @awood45 , understood the reasoning.\n. ",
    "syedmusamah": "Thanks!\n. ",
    "funglaub": "The patch is online. I'll provide you with some information once the error occurs again.\n. Nope, I'm sorry. Still waiting for information on this.\n. Finally I can provide some feedback.\nMy current patch looks like that:\nruby\nmodule Aws\n  class InstanceProfileCredentials\n    def refresh\n      retry_count = 0\n      begin\n        credentials = MultiJson.load(get_credentials)\n        @access_key_id = credentials['AccessKeyId']\n        @secret_access_key = credentials['SecretAccessKey']\n        @session_token = credentials['Token']\n        if expires = credentials['Expiration']\n          @expiration = Time.parse(expires)\n        else\n          @expiration = nil\n        end\n      rescue ArgumentError, MultiJson::ParseError => e\n        airbrake_params = { get_credentials: get_credentials }\n        Airbrake.notify(e, parameters: airbrake_params, cgi_data: ENV.to_hash)\n        retry_count += 1\n        retry if retry_count < 3\n      end\n    end\n  end\nend\nIn an error case the notification parameters were (the actual value returned from the Method get_credentials):\n\"get_credentials\" => \n  \"{\\n  \\\"Code\\\" : \\\"Success\\\",\\n  \\\"LastUpdated\\\" : \\\"2015-10-14T07:12:58Z\\\",\\n  \\\"Type\\\" : \\\"AWS-HMAC\\\",\\n  \\\"AccessKeyId\\\" : \\\"ASIAIRJ7CEAJFCFUQJ7A\\\",\\n  \\\"SecretAccessKey\\\" : \\\"xxxxxxxxxxxxx\\\",\\n  \\\"Token\\\" : \\\"xxxxxxxxx\",\\n  \\\"Expiration\\\" : \\\"2015-10-14T13:20:33Z\\\"\\n}\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\"\n. ",
    "JumpyKitten": "That was quick. Thanks.\n. ",
    "vddgil": "Any release date for that feature ?\n. ",
    "verlinden": "+1 \ud83d\udc4d\n. ",
    "albertsj1": "I just did some testing with different versions and my code was working with version 2.0.48, but stopped working with 2.1.0 release.\nChecking the changelog, I can see that 2.1.0 introduced backward breaking changes for paging responses.  Closing this issue since it's an intentional code change that requires me to modify my code.\n. @trevorrowe \nSorry. Saw your comment after I submitted my comment.  I had the window open for an hour before clicking the comment button. oops.\nAnyway, thanks for the detailed explanation.  Let me look deeper at my code and confirm the problem and I'll reply back.\nThanks again.\n. ",
    "MrJoy": "To add a hair more detail, calling Aws::EC2::Resource#vpc_addresses surfaces the issue:\nruby\nec2\n  .vpc_addresses\n  .to_a\n  .map(&:association)\n  .map(&:public_ip) # Chokes hard with the exception mentioned in the issue title.\n. Any word on this?\n. ",
    "malpani": "https://github.com/aws/aws-sdk-ruby/commit/afd3fa214cc28b80d3bdb8e467e3c3f4a1ac1b5d fixes this\n. ",
    "gnitnuj": "@trevorrowe, I actually figured out my problem and just fixed it. My policy expiration was hitting the 1 hour default, so I passed signature_expiration: 5.days.from_now.utc so the policy expiration is far longer than I would ever need it to be. It took a bit of digging through the docs to get the right parameter for the .presigned_post method though since there are multiple other expire options. \n. ",
    "moayman": "Solved. Check stackoverflow question if you had the same issue.\n. ",
    "cmhobbs": "I think you nailed it, thanks!\n. Well, that appears to have been the issue.  I'm glad it was something simple like this.\nIs this documented anywhere or is there any chance of getting better error messages?  This corner of the libarary is a labrynth of metaprogramming.  It was rough trying to track down a potential source.\n. Great, thanks!\n. ",
    "sbwoodside": "+1 I just had this problem with AWS S3 upload in ruby, doing this:\ns3 = Aws::S3::Resource.new\nbucket = s3.bucket(ENV['S3_BUCKET'])\ns3object = bucket.object('hello.png')\ns3object.upload_file('/some/file')\nI had AWS_REGION=us-east-1a in my .env file. Changed it to AWS_REGION=us-east-1 and it's fixed.\nIt would be nice if the error could be more informative.\n. ",
    "mitchless": "Even after updating to 2.1.15, the command sequence results in the same error.\nHere are the credentials returned:\n``` ruby\n\nstubbed_s3_client = Aws::S3::Client.new(stub_responses: true)\n=> #\nstubbed_s3_client.config.credentials\n=> #\n```\n\nIs it possible that aws.yml is interfering?\nEDIT: I have confirmed that removing aws.yml from my config folder allows my stubbed responses to come through.\n. Sorry for the confusion. I meant to come back and update this further but failed to amidst the myriad task swapping of my day.\nThis appears to be a self-caused problem in our Rails application. We pre-fill the credentials of our app from aws.yml and for our development profile those are empty strings. If the file is removed, we do not create the credentials object and update Aws.config.\nIt appears that if a credentials object is specified, Aws will attempt to use it regardless of whether stub_responses: true is present. This probably isn't a big deal, but did catch me by surprise.\n. ",
    "mikelorant": "Looking at the code here:\nhttps://github.com/aws/aws-sdk-ruby/blob/master/aws-sdk-core/lib/aws-sdk-core/shared_credentials.rb\ndef load_from_path\n      profile = load_profile\n      @credentials = Credentials.new(\n        profile['aws_access_key_id'],\n        profile['aws_secret_access_key'],\n        profile['aws_session_token']\n      )\n    end\nIt is clear that this isn't supported as this is just looking for those specific keys. Will need to also handle role_arn and source_profile.\n. ",
    "JonathanSerafini": "We use role_arn and source_profile extensively, with all of our accounts being created in a master authentication account and then access being delegated to child accounts via Assume role.  So I've written a simple pull request to provide this functionality https://github.com/aws/aws-sdk-ruby/pull/998 which I hope meets with your approval.\n. @trevorrowe just out of curiosity, should this have been provided in a different manner ? it's been a month and the only feedback there's been was an accidental close, it makes you feel all warm and fuzzy inside ;)\n. \u00af(\u30c4)/\u00af\n. ",
    "BanzaiMan": "@trevorrowe Great. Thanks for the clarification!\n. https://travis-ci.org/octoblu/gateblu-forever/jobs/79709836 and https://travis-ci.org/octoblu/gateblu-forever/jobs/79896596 indicate that the issue persists. An interesting fact about this case is that the corresponding Linux build (https://travis-ci.org/octoblu/gateblu-forever/jobs/79709835), which had run earlier, was successful.\nThe \"rebar\" case seems a bit different, in that it only runs on Linux (the latest of which is https://travis-ci.org/rebar/rebar3/jobs/79951129#L5691), so the multiple retries do not come into play.\n. @trevorrowe Thanks! There is at least one instance where 2.1.21 was able to upload (https://travis-ci.org/rebar/rebar3/jobs/80520689#L5710-L5731) where previously 2.1.20 threw an error.\nI'm waiting to hear from other users. If I get a few more samples to confirm that it's working, we can close this. :-D\n. Yes, I think so! Thank you for your help.\n. Hello. A user reports a regression (https://github.com/travis-ci/travis-ci/issues/4776#issuecomment-148918938) with 2.1.30. https://travis-ci.org/CentricWebEstate/pieisreal/builds/85906778#L5588\n/home/travis/.rvm/gems/ruby-1.9.3-p551/gems/aws-sdk-core-2.1.30/lib/seahorse/client/plugins/raise_response_errors.rb:15:in `call': The request signature we calculated does not match the signature you provided. Check your key and signing method. (Aws::S3::Errors::SignatureDoesNotMatch)\nShould I reopen this issue, or open a new one?\n. @trevorrowe \n``` yaml\ndeploy:\n  provider: s3\n  \u22ee\n  bucket: pieisreal-sheets\n  local-dir: release\n  upload-dir: \"$TRAVIS_BUILD_NUMBER\"\n  acl: public_read\n  region: ap-southeast-2\n  skip_cleanup: true\n  on:\n    repo: CentricWebEstate/pieisreal\n```\nIt is sending to Sydney. I don't know how big the file is.\n. @awood45 Funny you should mention! I just updated https://github.com/travis-ci/travis-ci/issues/4776.\nUnfortunately, I've had a few reports (from our customers using private repositories, so I don't have direct links) that 2.2.0 is breaking their deployment with S3. I haven't been able to reproduce this myself, but in one case forcing 2.1.36 fixes the problem.\n. @awood45 Thanks for the update. Not sure yet whether 2.2.2 or 2.2.3 is helping yet. I also tested on Python, but couldn't reproduce, either.\n. @awood45 Sorry for the delay in update. I'll ask users to see where we are. I haven't noticed any recent report of failures, but I'd like to be certain.\n. I have not checked all the other dependent gems, so aws-sdk-cloudwatch may not be the only one.. From this, I take it to mean that the documentation would be updated to show the valid characters in version description in the near future.\nThank you very much for the quick turnaround!. @cjyclaire Hello. It's a bit over a month now, but the documentation has not been updated. Perhaps I'm a little impatient, but I'd appreciate some updates. Thank you!. @cjyclaire The documentation is still unclear. Could you check the documentation back log, if you can? Thank you!. Hello there. I am sorry to ping again, but the documentation remains unclear. :-(. ",
    "kholbekj": "Ah, thank you so much! Thought it might be a bug due to the SO timing, but there ya go.\n. ",
    "SubhaPrince": "I am getting this error where the given region is correct\n\nAuthorizationQueryParametersError\n\nError parsing the X-Amz-Credential parameter; the region 'us-east-1' is wrong; expecting 'eu-west-2'\n\neu-west-2\n0A19A991D5C2440C\n\nmqt64amJyBH4Vz/0c68v08GCtMq3OY5k21TGzBuYlxKY9Qt9o1nKpxZnJm9vnYB5xIdg7l1cJMI=\n\n. ",
    "juliantai": "I want to grab all the instance ids once the machines have booted and add them to the load balancers after they are healthy. \n. Yeah that's exactly it\n. That looks like what I was thinking about adding. I'll take a look at waiters if I get some time. Thanks a bunch for helping me out and saving me some effort.\n. ",
    "rob-murray": "This is misleading as there are references to being able to configure the signer with signature_version: 's3' - I cannot see any notice saying this is being removed.\nhttps://github.com/aws/aws-sdk-ruby/releases/tag/v2.2.0\nThis is useful as I need to provide the presigned url to a mobile client I have no control over.\n. ",
    "maclover7": ":+1: \n. ",
    "julik": ":heart: my pleasure\n. Also probably related #965\n. Verified the branch from https://github.com/aws/aws-sdk-ruby/pull/979 and it exhibits the same failure\n. :heart: thanks! I didn't dare to dive that deep into the signing stuff\n. We currently resorted to gsubbing the https to http and that works\n. s3_object.presigned_url_for(:get, ...).to_s.gsub(/^https\\:/, 'http:')\n. I know for sure that this regression was introduced in the transition from 2.1.10 to 2.1.11 (determined by bisection really). Maybe if you initialize the URI object with just the path and then set the scheme the results will be more predictable?\n[1] pry(main)> require 'uri'\n=> true\n[2] pry(main)> u = URI('/some/path')\n=> #<URI::Generic:0x007fb735893a98 URL:/some/path>\n[3] pry(main)> u.scheme = 'https'\n=> \"https\"\n[4] pry(main)> u\n=> #<URI::Generic:0x007fb735893a98 URL:https:/some/path>\n[5] pry(main)> u.to_s\n=> \"https:/some/path\"\n[6] pry(main)> u.scheme = 'http'\n=> \"http\"\n[7] pry(main)> u.to_s\n=> \"http:/some/path\"\n[8] pry(main)>\n. ",
    "dziemid": "Thank you for quick answer.\nIs 1-week limitation enforced across all the regions? We are currently using 60 days in us-east-1.\n. Thank you, make sense.\nIn that case, happy to close this one.\n. ",
    "oncletom": "It seems to be related to the HTTP lib \u2013 not aws-sdk.\nSorry for the confusion!\n. It is mainly due to your OS \u2014 it seems to be related to either a CA certificate missing in the chain or a version of OpenSSL ruby is backed by (I ended up adding my domain CA certificate to the machines I deploy)\n. ",
    "davejlong": "There seems to be a core issue with Ruby's HTTP lib. I've found it in the Mailgun client and AWS SDK client so far. I found a work around for Mailgun by using CURLs certificates, but it doesn't work for AWS. Any known workarounds?\n. ",
    "mattcook": "@trevorrowe Thanks for all your help with debugging this issue. On a whim I tested the issue again today and noticed that the issue has been resolved. :+1: \n. ",
    "HienNguyenAsnet": "@trevorrowe \nAWS_REGION=us-west-2\nLet me know something wrong\n. I find out the reason. Opsworks only support region us-east-1\nChange config AWS_REGION to us-east-1 then it's will work\nAWS_REGION=us-east-1\nThank @trevorrowe \n. ",
    "denodster": "I've tested this same code omitting the virtual_host parameter, and everything works as expected. (Unfortunately the virtual host is essential for my application as otherwise CORS kicks in and breaks everything.)\n. Found the issue. The url being used for creating the signature starts with https:// rather than http://\n. I think the bucket name will always contain dots when it's being used with a CNAME for static web hosting. as the bucket name is require to be the same as the domain name it is serving. I'm not sure if this bit of code is used for other applications.\n. Looks like the regex in the test was broken by the addition of :80 to the end of the domain in the url, also it was kind of too permissive in that it would allow:\nhttp://myabucketacom/ but not http://my.bucket.com:80/\n. ",
    "kcollignon": "+1\n. ",
    "Senjai": "Shouldn't we raise an exception here? In what scenarios would we passing a closed file to aws be okay?\n. @trevorrowe Ah, gotcha. :+1: on that approach.\n. Sorry, this was an implimentation detail.\n. ",
    "dougal": "+1 on this.\n. The exception occurs for both :s3 and :v4 being set as signature_version in config.\n. Custom signed query parameters, as opposed to headers, but yes.\nExample:\n\"https://examplebucket.s3.amazonaws.com/test.txt\"\\\n\"?x-foo=bar\"\\\n\"&X-Amz-Algorithm=AWS4-HMAC-SHA256\"\\\n\"&X-Amz-Credential=AKIAIOSFODNN7EXAMPLE%2F20130524%2F\"\\\n\"us-east-1%2Fs3%2Faws4_request\"\\\n\"&X-Amz-Date=20130524T000000Z&X-Amz-Expires=86400\"\\\n\"&X-Amz-SignedHeaders=host\"\\\n\"&X-Amz-Signature=aeeed9bbccd4d02ee5c0109b86d86835f995330da4c26595\"\\\n\"7d157751f604d404\".\n",
    "ahmad-alkheat": "That didn't work : \n```\ns3 = Aws::S3::Client.new(:stub_responses => true)\nAws.config[:s3] = {\n   stub_responses: {\n     get_object: Aws::S3::Errors::NoSuchKey.new(\"test\", \"test\")  \n   }\n}\ns3.get_object(bucket: \"test\", key: \"test\")\n```\nThis will return an empty response and not an error. \n. @trevorrowe \n. You are the man! much appreciated Trevor. \n. @awood45  Thanks a bunch, that makes sense.\n. ",
    "toaster": "Thanks for the clarification.\n. Yes, you understood me correctly. And yeah, that whitelist would be totally okay for me.\n. Hi there!\nDue to this issue we are still stuck at SDK version 2.2.5. We now want to use the upload acceleration which is not supported by 2.2.5 AFAICS.\nIs there any chance that #1228 will be merged any time soon?\n. Hi again,\nwe worked around our upload acceleration issue with 2.2.5 by simply patching the URL.\nBut now we want to use signed CloudFront URLs. But, alas, the Aws::CloudFront::UrlSigner does not exist in 2.2.5.\nThis problem has now evolved from \u201cannoying\u201d to \u201cblocker\u201d for us.\nWhen will the PR get merged?. Thanks for the fast fix. Out of curiosity I had a look at the PR and wondered why you looked into ENV in the CredentialProviderChain instead of letting the SharedCredentials do its already existing lookup logic.\nDigging even further I found out, that SharedCredentials duplicates (and therefore hides) the logic of SharedConfig which has the same fallback chain but only gets called for the default profile name.\nCould you please explain to me why it has to be this way?. I'm not convinced regarding the necessity of evaluating ENV['AWS_PROFILE'] in CredentialProviderChain. Please see my simplified patch which leaves the specs (rake test) green.. @cjyclaire I understand. But at the moment, you have two (with SharedConfig three) implementations of the \u201cexplicit profile -> AWS_PROFILE -> default\u201d fallback chain.. The refactor should happen in the context that @cjyclaire gave regarding the responsibility for the fallback. I have not enough knowledge to decide this and leave it to you guys :).. This is an occasional error. The request runs fine several hundred times a day with the same input.. The error occurred while doing a simple table.get_item like\ntable = Aws::DynamoDB::Table.new(<our_table_name>)\ntable.get_item(key: {hash_key: \"tenant\", range_key: \"<short_alnum_only_identifier>\"}, consistent_read: true)\nI agree that this is a service issue. But I reported here because I think it would be great if the SDK could handle unexpected service output better, e.g. with an own error including the full body and HTTP status.. And regarding repeating the request: We repeat this exact request many hundred times a day. We do this request with a different range key several thousand times a day. It works most of the time for years now.\nThe last time it didn't work was today (2018-02-19) at 5:35am UTC for two consecutive calls.\nThe table is in the eu-west-1 region.\nIt is accessed by a unicorn process in a docker container on a recent Amazon Linux. The auth stuff is done via IAM instance role.\nOver a private (support) channel I could share details on account, table and the item's keys.. From a client's view I at least would like to have the HTTP response code.\nAs it is now, I cannot see if it is a 5xx or a 4xx.. ",
    "lbrito1": "@trevorrowe I'm wondering whether this behavior applies to copy_to as well:\n\nMetadata, such as content-type, content-encoding, the ACL (access control list), user-supplied metadata, etc are all ignored unless by S3 unless you set a special header, x-amz-metadata-directive: REPLACE.\n\nI have some v1 legacy code which is being updated to v2, and I have a copy_to with a few extra options. In v2, options :content_type and :acl now raise Aws::S3::Errors::InvalidArgument: (blank at the end) errors. Code section looks like this:\n.copy_to(obj.key,\n  cache_control: \"max-age=123456\",\n  expires: some_date.httpdate,\n  acl: :public_read,\n  content_type: obj.get.content_type\n)\nIs this expected? Are these options no longer valid or required?. ",
    "danlherman": "Sorry, it seems to happen on 2.1.28 with some larger files as well. Just tried sending a 48gb file and after a while got this error:\naws-sdk-resources-2.1.28/lib/aws-sdk-resources/services/s3/multipart_file_uploader.rb:72:in abort_upload': multipart upload failed: undefined methodmatch' for nil:NilClass (Aws::S3::MultipartUploadError)\n. This worked for me as well!\nAws::S3::Resource.new(\n  credentials: Aws::Credentials.new('xxxxx', 'xxxxxx'),\n  region: 'us-east-1',\n  signature_version: 'v4'\n)\n. ",
    "RobinClowers": "Looks like I somehow created leaf nodes with the same name as the prefix... Seems like that shouldn't be allowed.\n. ",
    "jmccarty3": "SDK Gem info:\naws-sdk (2.1.29, 2.1.7, 2.0.48)\naws-sdk-core (2.1.29, 2.1.7, 2.0.48)\naws-sdk-resources (2.1.29, 2.1.7, 2.0.48)\nI will check the the RDS forums as well. It could be a bummer if the migrate feature wasn't scriptable through the SDK though.\n. ",
    "trekr5": "Many thanks!\n. ",
    "genslein": "I think you are getting hung up on the error, the issue is you can't reach inside the poller.poll block with the given methodology. It's as you said the credential error went away from a configuration issue that was unrelated. I have tried changing the poller.poll test to do one of each of the following:\nruby\n      messages = [{message_id:'id1', receipt_handle:'rh1', body:json_string}]\n      poller = Aws::SQS::QueuePoller.new(stub_responses: {\n              poll: messages\n              })\n      allow_any_instance_of(Aws::SQS::QueuePoller).to receive(:new).with(any_args).and_return poller\nalternatively\nruby\n      allow_any_instance_of(Aws::SQS::QueuePoller).to receive(:poll).with(any_args).and_return messages\nThe stub_responses method according to what I've dug for is only available on clients, and QueuePoller is a resource that has a client passed in, hence the method not found. Subsequently, passing the parameter to the function also doesn't do anything for the same reason. So the only stacktrace I get is:\nNoMethodError: undefined method `stub_responses' for #<Aws::SQS::QueuePoller:0x007fa9db1ac430>\n./spec/unit/AWSWrapper_spec.rb:52:in `block (3 levels) in <top (required)>'\n/ruby-2.1.5/gems/ruby-debug-ide-0.4.32/lib/ruby-debug-ide.rb:86:in `debug_load'\n/ruby-2.1.5/gems/ruby-debug-ide-0.4.32/lib/ruby-debug-ide.rb:86:in `debug_program'\nI'm really asking for how you enable stubs on resources since it appears it's disallowed in the sdk.\n. In my first post code block, the first bullet point is what I tried having the Aws.config block handle manipulating the SQS client under the hood so poller.poll would operate as if normal parameters were met. I expected it to let me into the code block normally to test my code operating on the messages which it does not. If I missed something on that Aws.config block that would be helpful since I would like to avoid your 2nd bullet point since what the standard RSpec 3 block would look like below doesn't work either\nruby\n    before(:each) do\n      poller = double('poller')\n      msg = double('msg')\n      allow(msg).to receive(:body).and_return json_string\n      allow(poller).to receive(:poll).and_return [msg]\n      allow(Aws::SQS::QueuePoller).to receive(:new).with(any_args).and_return poller\n    end\n. ",
    "rosskevin": "Found related: Placing beanstalk in specific availability zone\n. Thanks, wrote up what I figured out here http://stackoverflow.com/a/33131364/2363935\n. ",
    "geissbock": "Thank you for the explanation! I will forward the workaround to the guys working on the Logstash plugin.\n. ",
    "yutaison": "Thanks @trevorrowe !\n. ",
    "scrossan": "For anybody googling the error message, I had the same problem - @trevorrowe's suggestion fixed it.\nYou'll want to put Aws.use_bundled_cert! where you're requiring the aws-sdk, so for example in my Rakefile:\n``` ruby\nRequires:\nrequire 'pp'\nrequire 'aws-sdk'\nrequire 'pry'\nrequire 'open4'\nrequire 'colorize'\nAws.use_bundled_cert!\nDisable stdout buffer\nSTDOUT.sync = true\nInclude substasks from different namespaces\nDir.glob('rake/*.rake').map { |rake| import rake }\n```\nThanks guys!\n. ",
    "christiangenco": "Here's one of the more probable things I've tried:\n``` ruby\nsource_object = Aws::S3::ObjectVersion.new({\n  bucket_name: \"source_bucket\",\n  object_key: \"test.txt\",\n  id: \"WYIIJB3UpKixITFbqLP6nVbAJsa5Ul5F\",\n  client: Aws::S3::Bucket.new(name: \"source_bucket\").client # there's probably an easier way to get a client object\n})\ndestination_object = Aws::S3::Bucket.new(\"destination_bucket\").object(\"test.txt\")\nsource_object.copy_to(destination_object) \nNoMethodError: undefined method `copy_to' for #\ndestination_object.copy_from(source_object) \nArgumentError: expected source to be an Aws::S3::Object, Hash, or String\nfrom .rbenv/versions/2.1.1/lib/ruby/gems/2.1.0/gems/aws-sdk-resources-2.1.30/lib/aws-sdk-resources/services/s3/object_copier.rb:44:in `copy_source'\n``\n. Looks like thatcopy_fromdidn't work becauseS3::ObjectCopier#copy_sourcedoesn't supportS3::ObjectVersionas a possible argument (and unlike [version 1](https://github.com/aws/aws-sdk-ruby/tree/aws-sdk-v1),S3::ObjectVersionis not a subclass ofS3::Object`).\nI thought the #copy_source method might be monkey patched to fix this:\nruby\nmodule Aws\n  module S3\n    class ObjectCopier\n      def copy_source(source)\n        case source\n        when String then escape(source)\n        when Hash then \"#{source[:bucket]}/#{escape(source[:key])}\"\n        when S3::Object then \"#{source.bucket_name}/#{escape(source.key)}\"\n        when S3::ObjectVersion then \"#{source.bucket_name}/#{escape(source.key)}?versionId=#{escape(source.id)}\"\n        else\n          msg = \"expected source to be an Aws::S3::Object, AWS::S3::ObjectVersion, Hash, or String\"\n          raise ArgumentError, msg\n        end\n      end\n    end\n  end\nend\nbut now destination_object.copy_from(source_object) throws:\nNotImplementedError: #load not defined for Aws::S3::ObjectVersion\nfrom .rbenv/versions/2.1.1/lib/ruby/gems/2.1.0/gems/aws-sdk-resources-2.1.30/lib/aws-sdk-resources/resource.rb:150:in `load'\nI can't find an actual definition for Aws::S3::ObjectVersion (it looks like it's being dynamically created from Aws::Resources::Resource?), so I'm not sure how to implement this mysterious #load method. It might just be assigning a LoadOperation, but I'm in over my head at this point. The only way I see forward at the moment is to define Aws::S3::ObjectVersion identically to how Aws::S3::Object is currently defined and hope the load operation sorts itself out.\nI'll start down the road of submitting a raw REST request, though figuring out how to sign requests outside of this gem looks like it might be just as difficult.\n. Ahh perfect! Thank you so much :smile: \nFor anyone else walking down this path, check out the full documentation on that Aws::S3::Client#copy_object method.\nAlso, it looks like you just added Seahorse::Util#uri_path_escape and it hasn't been published to rubygems yet, so I just patched it in an init script:\nruby\nif !defined? Seahorse::Util.uri_path_escape\n  module Seahorse\n    module Util\n      class << self\n        def uri_path_escape(path)\n          path.gsub(/[^\\/]+/) { |part| uri_escape(part) }\n        end\n      end\n    end\n  end\nend\nThose are some nifty helper methods - URI escaping is always strangely confusing in Ruby. It'd be lovely if you could just do String#escape or String#escape_url, but I digress.\n. ",
    "skippy": "hey @trevorrowe thank you for the prompt response!\ncomments:\n- good point about the CLI and JSON.  It is indeed nice to have standardized output.\n- I understand your comment about ruby syntax, yet this is about stubbing a response, not calling a method on the aws sdk.  You say:\n\nIf you know the name of the operation, it would be possible to provide a context aware translation that does this correctly using the service API model.\n\nIs that documented anywhere?  If not, lets definitely add it as it would be awesome to be able to take the JSON output of the aws CLI and pass that into a stubbed response.\nthanks\n. Yep, the operation is definitely known.  I don't know if overloading :stub_respones , such as\nruby\n      Aws.config[:sqs] = {\n        stub_responses: {\n          receive_message: File.read(\"sqs_receive_msg_response.json\")\n        }\n      }\nor via a different avenue is the right approach, but that is your call of course.  \nI would appreciate tracking this feature request.  Do you do that via github or internally?  Feel free to close this if you track it somewhere else.\n. hi @awood45 \nI hear ya.  I was attempting to use IO pipes to push data from s3 through to the client without writing a tmp file or putting it in memory.  I've used that pattern elsewhere with good effect, but it isn't doable with this library.  \nThe files I'm moving around are in the 10s MB range; so not massive but more than I want to store in memory with a StringIO object.  The app is on a system that doesn't have rw access to disk, but it was easy to modify to give the app a scratch space for this purpose.\nAnyway, not sure it is worth the pain to do this, but that is/was my use-case.\n. ",
    "abhaykumar": "WOW!\nSuper awesome\nthat works great\n. ",
    "abhayathem": "Wrote a post with sample code.\nhttp://www.hackpundit.com/amazon-ses-request-id-sdk-version2/\nHope it help others and save their time.\n. ",
    "eldondevcg": "Here is the relevant backtrace\n- /var/bundles/ruby/1.8/gems/aws-sdk-v1-1.66.0/lib/aws/core/credential_providers.rb:41:in `credentials'\n- /var/bundles/ruby/1.8/gems/aws-sdk-v1-1.66.0/lib/aws/core/credential_providers.rb:404:in `credentials'\n- /var/bundles/ruby/1.8/gems/aws-sdk-v1-1.66.0/lib/aws/core/credential_providers.rb:137:in `credentials'\n- /var/bundles/ruby/1.8/gems/aws-sdk-v1-1.66.0/lib/aws/core/credential_providers.rb:135:in `each'\n- /var/bundles/ruby/1.8/gems/aws-sdk-v1-1.66.0/lib/aws/core/credential_providers.rb:135:in `credentials'\n- /var/bundles/ruby/1.8/gems/aws-sdk-v1-1.66.0/lib/aws/core/credential_providers.rb:62:in `access_key_id'\n- /var/bundles/ruby/1.8/gems/aws-sdk-v1-1.66.0/lib/aws/core/client.rb:549:in `build_request'\n- /var/bundles/ruby/1.8/gems/aws-sdk-v1-1.66.0/lib/aws/core/client.rb:491:in `send'\n- /var/bundles/ruby/1.8/gems/aws-sdk-v1-1.66.0/lib/aws/core/client.rb:491:in `client_request'\n- /var/bundles/ruby/1.8/gems/aws-sdk-v1-1.66.0/lib/aws/core/response.rb:175:in `call'\n- /var/bundles/ruby/1.8/gems/aws-sdk-v1-1.66.0/lib/aws/core/response.rb:175:in `build_request'\n- /var/bundles/ruby/1.8/gems/aws-sdk-v1-1.66.0/lib/aws/core/response.rb:114:in `initialize'\n- /var/bundles/ruby/1.8/gems/aws-sdk-v1-1.66.0/lib/aws/core/client.rb:203:in `new'\n- /var/bundles/ruby/1.8/gems/aws-sdk-v1-1.66.0/lib/aws/core/client.rb:203:in `new_response'\n- /var/bundles/ruby/1.8/gems/aws-sdk-v1-1.66.0/lib/aws/core/client.rb:490:in `client_request'\n- /var/bundles/ruby/1.8/gems/aws-sdk-v1-1.66.0/lib/aws/core/client.rb:391:in `log_client_request'\n- /var/bundles/ruby/1.8/gems/aws-sdk-v1-1.66.0/lib/aws/core/client.rb:477:in `client_request'\n- /var/bundles/ruby/1.8/gems/aws-sdk-v1-1.66.0/lib/aws/core/client.rb:373:in `return_or_raise'\n- /var/bundles/ruby/1.8/gems/aws-sdk-v1-1.66.0/lib/aws/core/client.rb:476:in `client_request'\n- (eval):3:in `list_object_versions'\n- /var/bundles/ruby/1.8/gems/aws-sdk-v1-1.66.0/lib/aws/s3/bucket_version_collection.rb:66:in `list_request'\n- /var/bundles/ruby/1.8/gems/aws-sdk-v1-1.66.0/lib/aws/s3/paginated_collection.rb:29:in `_each_item'\n- /var/bundles/ruby/1.8/gems/aws-sdk-v1-1.66.0/lib/aws/core/collection/with_limit_and_next_token.rb:54:in `_each_batch'\n- /var/bundles/ruby/1.8/gems/aws-sdk-v1-1.66.0/lib/aws/core/collection.rb:80:in `each_batch'\n- /var/bundles/ruby/1.8/gems/aws-sdk-v1-1.66.0/lib/aws/core/collection.rb:47:in `each'\n- /var/bundles/ruby/1.8/gems/aws-sdk-v1-1.66.0/lib/aws/s3/bucket_version_collection.rb:48:in `each'\n- /var/bundles/ruby/1.8/gems/aws-sdk-v1-1.66.0/lib/aws/s3/object_version_collection.rb:79:in `each'\n. ",
    "stopdropandrew": "@awood45 - rebased\n. Test failure seems unrelated, it's happening in another pull as well\n. @awood45 rebased, thanks!\n. @awood45 bump, would love to get this merged\n. ",
    "abhiofdoon": "Here is the wire trace.\nIt tries 12 times. Till the 11th time (165s) it returned progress as 0%, then the 12th time it returned progress as 100%.\nThe AWS console starts saying 100% much earlier.\n2015-10-29 16:44:26 -0700\n<- \"POST / HTTP/1.1\\r\\nContent-Type: application/x-www-form-urlencoded; charset=utf-8\\r\\nAccept-Encoding: \\r\\nUser-Agent: aws-sdk-ruby2/2.1.28 ruby/2.2.1 x86_64-linux\\r\\nX-Amz-Date: 20151029T234426Z\\r\\nHost: ec2.us-west-1.amazonaws.com\\r\\nX-Amz-Content-Sha256: 953ff5dbaf34a2d8d2fb19b38ef5addc03f6e2d0333b4c899e80e76abd212edf\\r\\nAuthorization: AWS4-HMAC-SHA256 Credential=AKIAJCKN6RYFLNR7OCFQ/20151029/us-west-1/ec2/aws4_request, SignedHeaders=content-type;host;user-agent;x-amz-content-sha256;x-amz-date, Signature=383218500c7c3f200b2954637b1cdfee10d4a767030cc074cb2c4e8b022a55d8\\r\\nContent-Length: 70\\r\\nAccept: /\\r\\n\\r\\n\"\n-> \"HTTP/1.1 200 OK\\r\\n\"\n-> \"Vary: Accept-Encoding\\r\\n\"\n-> \"Date: Thu, 29 Oct 2015 23:44:25 GMT\\r\\n\"\n-> \"Server: AmazonEC2\\r\\n\"\n-> \"Transfer-Encoding: chunked\\r\\n\"\n-> \"Content-Type: text/xml;charset=UTF-8\\r\\n\"\n-> \"Via: 1.1 inprtscl03p.corp.emc.com:80 (Cisco-WSA/8.0.6-123)\\r\\n\"\n-> \"Connection: keep-alive\\r\\n\"\n-> \"\\r\\n\"\n-> \"2ec\\r\\n\"\nreading 748 bytes...\n-> \"\"\n-> \"<?xml version=\\\"1.0\\\" encoding=\\\"UTF-8\\\"?>\\n\\n    8fbbb25c-05ca-4e98-bf68-8695e73f52fe\\n    \\n        \\n            snap-303cd112\\n            vol-90838f75\\n            pending\\n            2015-10-29T23:44:25.000Z\\n            0%\\n            746212085856\\n            10\\n            Snapper ffbbe048-eb63-4f31-be9b-984f45ff6f5f Thursday October 29 or Thu 29/10\\n            false\\n        \\n    \\n\"\nread 748 bytes\nreading 2 bytes...\n-> \"\"\n-> \"\\r\\n\"\nread 2 bytes\n-> \"0\\r\\n\"\n-> \"\\r\\n\"\nConn keep-alive\nI, [2015-10-29T16:44:26.233035 #8385]  INFO -- : [Aws::EC2::Client 200 0.216211 0 retries] describe_snapshots(snapshot_ids:[\"snap-303cd112\"])  \nopening connection to ec2.us-west-1.amazonaws.com:443...\nopened\nstarting SSL for ec2.us-west-1.amazonaws.com:443...\nSSL established\n<- \"POST / HTTP/1.1\\r\\nContent-Type: application/x-www-form-urlencoded; charset=utf-8\\r\\nAccept-Encoding: \\r\\nUser-Agent: aws-sdk-ruby2/2.1.28 ruby/2.2.1 x86_64-linux\\r\\nX-Amz-Date: 20151029T234441Z\\r\\nHost: ec2.us-west-1.amazonaws.com\\r\\nX-Amz-Content-Sha256: 953ff5dbaf34a2d8d2fb19b38ef5addc03f6e2d0333b4c899e80e76abd212edf\\r\\nAuthorization: AWS4-HMAC-SHA256 Credential=AKIAJCKN6RYFLNR7OCFQ/20151029/us-west-1/ec2/aws4_request, SignedHeaders=content-type;host;user-agent;x-amz-content-sha256;x-amz-date, Signature=613648ab633e73c851dc2f4e4909d98d7a1ea7593d6c70354d68f0e05d7bd3d4\\r\\nContent-Length: 70\\r\\nAccept: /\\r\\n\\r\\n\"\n-> \"HTTP/1.1 200 OK\\r\\n\"\n-> \"Vary: Accept-Encoding\\r\\n\"\n-> \"Date: Thu, 29 Oct 2015 23:44:40 GMT\\r\\n\"\n-> \"Server: AmazonEC2\\r\\n\"\n-> \"Transfer-Encoding: chunked\\r\\n\"\n-> \"Content-Type: text/xml;charset=UTF-8\\r\\n\"\n-> \"Via: 1.1 inprtscl03p.corp.emc.com:80 (Cisco-WSA/8.0.6-123)\\r\\n\"\n-> \"Connection: keep-alive\\r\\n\"\n-> \"\\r\\n\"\n-> \"2ec\\r\\n\"\nreading 748 bytes...\n-> \"\"\n-> \"<?xml version=\\\"1.0\\\" encoding=\\\"UTF-8\\\"?>\\n\\n    3ebfe498-fc9a-40d0-8ebd-529e172e1193\\n    \\n        \\n            snap-303cd112\\n            vol-90838f75\\n            pending\\n            2015-10-29T23:44:25.000Z\\n            0%\\n            746212085856\\n            10\\n            Snapper ffbbe048-eb63-4f31-be9b-984f45ff6f5f Thursday October 29 or Thu 29/10\\n            false\\n        \\n    \\n\"\nread 748 bytes\nreading 2 bytes...\n-> \"\"\n-> \"\\r\\n\"\nread 2 bytes\n-> \"0\\r\\n\"\n-> \"\\r\\n\"\nConn keep-alive\nI, [2015-10-29T16:44:41.536484 #8385]  INFO -- : [Aws::EC2::Client 200 0.290127 0 retries] describe_snapshots(snapshot_ids:[\"snap-303cd112\"])  \nopening connection to ec2.us-west-1.amazonaws.com:443...\nopened\nstarting SSL for ec2.us-west-1.amazonaws.com:443...\nSSL established\n<- \"POST / HTTP/1.1\\r\\nContent-Type: application/x-www-form-urlencoded; charset=utf-8\\r\\nAccept-Encoding: \\r\\nUser-Agent: aws-sdk-ruby2/2.1.28 ruby/2.2.1 x86_64-linux\\r\\nX-Amz-Date: 20151029T234456Z\\r\\nHost: ec2.us-west-1.amazonaws.com\\r\\nX-Amz-Content-Sha256: 953ff5dbaf34a2d8d2fb19b38ef5addc03f6e2d0333b4c899e80e76abd212edf\\r\\nAuthorization: AWS4-HMAC-SHA256 Credential=AKIAJCKN6RYFLNR7OCFQ/20151029/us-west-1/ec2/aws4_request, SignedHeaders=content-type;host;user-agent;x-amz-content-sha256;x-amz-date, Signature=e506375cc66c39b78d0374086376035ce390411cc3a3b4d6396ab170f13b013a\\r\\nContent-Length: 70\\r\\nAccept: /\\r\\n\\r\\n\"\n-> \"HTTP/1.1 200 OK\\r\\n\"\n-> \"Vary: Accept-Encoding\\r\\n\"\n-> \"Date: Thu, 29 Oct 2015 23:44:56 GMT\\r\\n\"\n-> \"Server: AmazonEC2\\r\\n\"\n-> \"Transfer-Encoding: chunked\\r\\n\"\n-> \"Content-Type: text/xml;charset=UTF-8\\r\\n\"\n-> \"Via: 1.1 inprtscl03p.corp.emc.com:80 (Cisco-WSA/8.0.6-123)\\r\\n\"\n-> \"Connection: keep-alive\\r\\n\"\n-> \"\\r\\n\"\n-> \"2ec\\r\\n\"\nreading 748 bytes...\n-> \"\"\n-> \"<?xml version=\\\"1.0\\\" encoding=\\\"UTF-8\\\"?>\\n\\n    80bbc089-865a-4f40-8924-0131b485f324\\n    \\n        \\n            snap-303cd112\\n            vol-90838f75\\n            pending\\n            2015-10-29T23:44:25.000Z\\n            0%\\n            746212085856\\n            10\\n            Snapper ffbbe048-eb63-4f31-be9b-984f45ff6f5f Thursday October 29 or Thu 29/10\\n            false\\n        \\n    \\n\"\nread 748 bytes\nreading 2 bytes...\n-> \"\"\n-> \"\\r\\n\"\nread 2 bytes\n-> \"0\\r\\n\"\n-> \"\\r\\n\"\nConn keep-alive\nI, [2015-10-29T16:44:56.831954 #8385]  INFO -- : [Aws::EC2::Client 200 0.294305 0 retries] describe_snapshots(snapshot_ids:[\"snap-303cd112\"])  \nopening connection to ec2.us-west-1.amazonaws.com:443...\nopened\nstarting SSL for ec2.us-west-1.amazonaws.com:443...\nSSL established\n<- \"POST / HTTP/1.1\\r\\nContent-Type: application/x-www-form-urlencoded; charset=utf-8\\r\\nAccept-Encoding: \\r\\nUser-Agent: aws-sdk-ruby2/2.1.28 ruby/2.2.1 x86_64-linux\\r\\nX-Amz-Date: 20151029T234511Z\\r\\nHost: ec2.us-west-1.amazonaws.com\\r\\nX-Amz-Content-Sha256: 953ff5dbaf34a2d8d2fb19b38ef5addc03f6e2d0333b4c899e80e76abd212edf\\r\\nAuthorization: AWS4-HMAC-SHA256 Credential=AKIAJCKN6RYFLNR7OCFQ/20151029/us-west-1/ec2/aws4_request, SignedHeaders=content-type;host;user-agent;x-amz-content-sha256;x-amz-date, Signature=7829e17f5c1ae7a27915296a7141cc4eb3fd9280736f8fb6a636aed724e67224\\r\\nContent-Length: 70\\r\\nAccept: /\\r\\n\\r\\n\"\n-> \"HTTP/1.1 200 OK\\r\\n\"\n-> \"Vary: Accept-Encoding\\r\\n\"\n-> \"Date: Thu, 29 Oct 2015 23:45:11 GMT\\r\\n\"\n-> \"Server: AmazonEC2\\r\\n\"\n-> \"Transfer-Encoding: chunked\\r\\n\"\n-> \"Content-Type: text/xml;charset=UTF-8\\r\\n\"\n-> \"Via: 1.1 inprtscl03p.corp.emc.com:80 (Cisco-WSA/8.0.6-123)\\r\\n\"\n-> \"Connection: keep-alive\\r\\n\"\n-> \"\\r\\n\"\n-> \"2ec\\r\\n\"\nreading 748 bytes...\n-> \"\"\n-> \"<?xml version=\\\"1.0\\\" encoding=\\\"UTF-8\\\"?>\\n\\n    b2864a38-3478-4e84-995c-e2e01ff2f101\\n    \\n        \\n            snap-303cd112\\n            vol-90838f75\\n            pending\\n            2015-10-29T23:44:25.000Z\\n            0%\\n            746212085856\\n            10\\n            Snapper ffbbe048-eb63-4f31-be9b-984f45ff6f5f Thursday October 29 or Thu 29/10\\n            false\\n        \\n    \\n\"\nread 748 bytes\nreading 2 bytes...\n-> \"\"\n-> \"\\r\\n\"\nread 2 bytes\n-> \"0\\r\\n\"\n-> \"\\r\\n\"\nConn keep-alive\nI, [2015-10-29T16:45:12.119289 #8385]  INFO -- : [Aws::EC2::Client 200 0.28655 0 retries] describe_snapshots(snapshot_ids:[\"snap-303cd112\"])  \nopening connection to ec2.us-west-1.amazonaws.com:443...\nopened\nstarting SSL for ec2.us-west-1.amazonaws.com:443...\nSSL established\n<- \"POST / HTTP/1.1\\r\\nContent-Type: application/x-www-form-urlencoded; charset=utf-8\\r\\nAccept-Encoding: \\r\\nUser-Agent: aws-sdk-ruby2/2.1.28 ruby/2.2.1 x86_64-linux\\r\\nX-Amz-Date: 20151029T234527Z\\r\\nHost: ec2.us-west-1.amazonaws.com\\r\\nX-Amz-Content-Sha256: 953ff5dbaf34a2d8d2fb19b38ef5addc03f6e2d0333b4c899e80e76abd212edf\\r\\nAuthorization: AWS4-HMAC-SHA256 Credential=AKIAJCKN6RYFLNR7OCFQ/20151029/us-west-1/ec2/aws4_request, SignedHeaders=content-type;host;user-agent;x-amz-content-sha256;x-amz-date, Signature=fe76a693d1b038c56111cda5f07814a47841bd438db20004391290ae9273df50\\r\\nContent-Length: 70\\r\\nAccept: /\\r\\n\\r\\n\"\n-> \"HTTP/1.1 200 OK\\r\\n\"\n-> \"Vary: Accept-Encoding\\r\\n\"\n-> \"Date: Thu, 29 Oct 2015 23:45:26 GMT\\r\\n\"\n-> \"Server: AmazonEC2\\r\\n\"\n-> \"Transfer-Encoding: chunked\\r\\n\"\n-> \"Content-Type: text/xml;charset=UTF-8\\r\\n\"\n-> \"Via: 1.1 inprtscl03p.corp.emc.com:80 (Cisco-WSA/8.0.6-123)\\r\\n\"\n-> \"Connection: keep-alive\\r\\n\"\n-> \"\\r\\n\"\n-> \"2ec\\r\\n\"\nreading 748 bytes...\n-> \"\"\n-> \"<?xml version=\\\"1.0\\\" encoding=\\\"UTF-8\\\"?>\\n\\n    f8c4a3cf-881e-4db2-99c4-65cabef14c8f\\n    \\n        \\n            snap-303cd112\\n            vol-90838f75\\n            pending\\n            2015-10-29T23:44:25.000Z\\n            0%\\n            746212085856\\n            10\\n            Snapper ffbbe048-eb63-4f31-be9b-984f45ff6f5f Thursday October 29 or Thu 29/10\\n            false\\n        \\n    \\n\"\nread 748 bytes\nreading 2 bytes...\n-> \"\"\n-> \"\\r\\n\"\nread 2 bytes\n-> \"0\\r\\n\"\n-> \"\\r\\n\"\nConn keep-alive\nI, [2015-10-29T16:45:27.424034 #8385]  INFO -- : [Aws::EC2::Client 200 0.302368 0 retries] describe_snapshots(snapshot_ids:[\"snap-303cd112\"])  \nopening connection to ec2.us-west-1.amazonaws.com:443...\nopened\nstarting SSL for ec2.us-west-1.amazonaws.com:443...\nSSL established\n<- \"POST / HTTP/1.1\\r\\nContent-Type: application/x-www-form-urlencoded; charset=utf-8\\r\\nAccept-Encoding: \\r\\nUser-Agent: aws-sdk-ruby2/2.1.28 ruby/2.2.1 x86_64-linux\\r\\nX-Amz-Date: 20151029T234542Z\\r\\nHost: ec2.us-west-1.amazonaws.com\\r\\nX-Amz-Content-Sha256: 953ff5dbaf34a2d8d2fb19b38ef5addc03f6e2d0333b4c899e80e76abd212edf\\r\\nAuthorization: AWS4-HMAC-SHA256 Credential=AKIAJCKN6RYFLNR7OCFQ/20151029/us-west-1/ec2/aws4_request, SignedHeaders=content-type;host;user-agent;x-amz-content-sha256;x-amz-date, Signature=b247c96bb2460ff910f76610ea590863bec032531a7c914b9c64c3f3f2e5b8fe\\r\\nContent-Length: 70\\r\\nAccept: /\\r\\n\\r\\n\"\n-> \"HTTP/1.1 200 OK\\r\\n\"\n-> \"Vary: Accept-Encoding\\r\\n\"\n-> \"Date: Thu, 29 Oct 2015 23:45:41 GMT\\r\\n\"\n-> \"Server: AmazonEC2\\r\\n\"\n-> \"Transfer-Encoding: chunked\\r\\n\"\n-> \"Content-Type: text/xml;charset=UTF-8\\r\\n\"\n-> \"Via: 1.1 inprtscl03p.corp.emc.com:80 (Cisco-WSA/8.0.6-123)\\r\\n\"\n-> \"Connection: keep-alive\\r\\n\"\n-> \"\\r\\n\"\n-> \"2ec\\r\\n\"\nreading 748 bytes...\n-> \"\"\n-> \"<?xml version=\\\"1.0\\\" encoding=\\\"UTF-8\\\"?>\\n\\n    ea9716bf-9f51-49f5-9ce9-6ce26cddeaf4\\n    \\n        \\n            snap-303cd112\\n            vol-90838f75\\n            pending\\n            2015-10-29T23:44:25.000Z\\n            0%\\n            746212085856\\n            10\\n            Snapper ffbbe048-eb63-4f31-be9b-984f45ff6f5f Thursday October 29 or Thu 29/10\\n            false\\n        \\n    \\n\"\nread 748 bytes\nreading 2 bytes...\n-> \"\"\n-> \"\\r\\n\"\nread 2 bytes\n-> \"0\\r\\n\"\n-> \"\\r\\n\"\nConn keep-alive\nI, [2015-10-29T16:45:42.747461 #8385]  INFO -- : [Aws::EC2::Client 200 0.321292 0 retries] describe_snapshots(snapshot_ids:[\"snap-303cd112\"])  \nopening connection to ec2.us-west-1.amazonaws.com:443...\nopened\nstarting SSL for ec2.us-west-1.amazonaws.com:443...\nSSL established\n<- \"POST / HTTP/1.1\\r\\nContent-Type: application/x-www-form-urlencoded; charset=utf-8\\r\\nAccept-Encoding: \\r\\nUser-Agent: aws-sdk-ruby2/2.1.28 ruby/2.2.1 x86_64-linux\\r\\nX-Amz-Date: 20151029T234557Z\\r\\nHost: ec2.us-west-1.amazonaws.com\\r\\nX-Amz-Content-Sha256: 953ff5dbaf34a2d8d2fb19b38ef5addc03f6e2d0333b4c899e80e76abd212edf\\r\\nAuthorization: AWS4-HMAC-SHA256 Credential=AKIAJCKN6RYFLNR7OCFQ/20151029/us-west-1/ec2/aws4_request, SignedHeaders=content-type;host;user-agent;x-amz-content-sha256;x-amz-date, Signature=dd8ff5d9d75934e3a33887187075026181c21b540ec24ae1b6e14b83cae57cb2\\r\\nContent-Length: 70\\r\\nAccept: /\\r\\n\\r\\n\"\n-> \"HTTP/1.1 200 OK\\r\\n\"\n-> \"Vary: Accept-Encoding\\r\\n\"\n-> \"Date: Thu, 29 Oct 2015 23:45:57 GMT\\r\\n\"\n-> \"Server: AmazonEC2\\r\\n\"\n-> \"Transfer-Encoding: chunked\\r\\n\"\n-> \"Content-Type: text/xml;charset=UTF-8\\r\\n\"\n-> \"Via: 1.1 inprtscl03p.corp.emc.com:80 (Cisco-WSA/8.0.6-123)\\r\\n\"\n-> \"Connection: keep-alive\\r\\n\"\n-> \"\\r\\n\"\n-> \"2ec\\r\\n\"\nreading 748 bytes...\n-> \"\"\n-> \"<?xml version=\\\"1.0\\\" encoding=\\\"UTF-8\\\"?>\\n\\n    b69ebae3-472f-4216-b19e-437cbcfd2e3a\\n    \\n        \\n            snap-303cd112\\n            vol-90838f75\\n            pending\\n            2015-10-29T23:44:25.000Z\\n            0%\\n            746212085856\\n            10\\n            Snapper ffbbe048-eb63-4f31-be9b-984f45ff6f5f Thursday October 29 or Thu 29/10\\n            false\\n        \\n    \\n\"\nread 748 bytes\nreading 2 bytes...\n-> \"\"\n-> \"\\r\\n\"\nread 2 bytes\n-> \"0\\r\\n\"\n-> \"\\r\\n\"\nConn keep-alive\nI, [2015-10-29T16:45:58.072425 #8385]  INFO -- : [Aws::EC2::Client 200 0.32362 0 retries] describe_snapshots(snapshot_ids:[\"snap-303cd112\"])  \nopening connection to ec2.us-west-1.amazonaws.com:443...\nopened\nstarting SSL for ec2.us-west-1.amazonaws.com:443...\nSSL established\n<- \"POST / HTTP/1.1\\r\\nContent-Type: application/x-www-form-urlencoded; charset=utf-8\\r\\nAccept-Encoding: \\r\\nUser-Agent: aws-sdk-ruby2/2.1.28 ruby/2.2.1 x86_64-linux\\r\\nX-Amz-Date: 20151029T234613Z\\r\\nHost: ec2.us-west-1.amazonaws.com\\r\\nX-Amz-Content-Sha256: 953ff5dbaf34a2d8d2fb19b38ef5addc03f6e2d0333b4c899e80e76abd212edf\\r\\nAuthorization: AWS4-HMAC-SHA256 Credential=AKIAJCKN6RYFLNR7OCFQ/20151029/us-west-1/ec2/aws4_request, SignedHeaders=content-type;host;user-agent;x-amz-content-sha256;x-amz-date, Signature=79adfb684794a9ed09ec6ea66c2254386a28244d00a319459cbfbd2c61797f72\\r\\nContent-Length: 70\\r\\nAccept: /\\r\\n\\r\\n\"\n-> \"HTTP/1.1 200 OK\\r\\n\"\n-> \"Vary: Accept-Encoding\\r\\n\"\n-> \"Date: Thu, 29 Oct 2015 23:46:12 GMT\\r\\n\"\n-> \"Server: AmazonEC2\\r\\n\"\n-> \"Transfer-Encoding: chunked\\r\\n\"\n-> \"Content-Type: text/xml;charset=UTF-8\\r\\n\"\n-> \"Via: 1.1 inprtscl03p.corp.emc.com:80 (Cisco-WSA/8.0.6-123)\\r\\n\"\n-> \"Connection: keep-alive\\r\\n\"\n-> \"\\r\\n\"\n-> \"2ec\\r\\n\"\nreading 748 bytes...\n-> \"\"\n-> \"<?xml version=\\\"1.0\\\" encoding=\\\"UTF-8\\\"?>\\n\\n    e1af5bd4-244e-41ad-9002-2966eb363659\\n    \\n        \\n            snap-303cd112\\n            vol-90838f75\\n            pending\\n            2015-10-29T23:44:25.000Z\\n            0%\\n            746212085856\\n            10\\n            Snapper ffbbe048-eb63-4f31-be9b-984f45ff6f5f Thursday October 29 or Thu 29/10\\n            false\\n        \\n    \\n\"\nread 748 bytes\nreading 2 bytes...\n-> \"\"\n-> \"\\r\\n\"\nread 2 bytes\n-> \"0\\r\\n\"\n-> \"\\r\\n\"\nConn keep-alive\nI, [2015-10-29T16:46:13.365645 #8385]  INFO -- : [Aws::EC2::Client 200 0.29125 0 retries] describe_snapshots(snapshot_ids:[\"snap-303cd112\"])  \nopening connection to ec2.us-west-1.amazonaws.com:443...\nopened\nstarting SSL for ec2.us-west-1.amazonaws.com:443...\nSSL established\n<- \"POST / HTTP/1.1\\r\\nContent-Type: application/x-www-form-urlencoded; charset=utf-8\\r\\nAccept-Encoding: \\r\\nUser-Agent: aws-sdk-ruby2/2.1.28 ruby/2.2.1 x86_64-linux\\r\\nX-Amz-Date: 20151029T234628Z\\r\\nHost: ec2.us-west-1.amazonaws.com\\r\\nX-Amz-Content-Sha256: 953ff5dbaf34a2d8d2fb19b38ef5addc03f6e2d0333b4c899e80e76abd212edf\\r\\nAuthorization: AWS4-HMAC-SHA256 Credential=AKIAJCKN6RYFLNR7OCFQ/20151029/us-west-1/ec2/aws4_request, SignedHeaders=content-type;host;user-agent;x-amz-content-sha256;x-amz-date, Signature=d21364477472984e7c7e129a78b4135d188f180ec191bb288f70f52d26a631ec\\r\\nContent-Length: 70\\r\\nAccept: /\\r\\n\\r\\n\"\n-> \"HTTP/1.1 200 OK\\r\\n\"\n-> \"Vary: Accept-Encoding\\r\\n\"\n-> \"Date: Thu, 29 Oct 2015 23:46:27 GMT\\r\\n\"\n-> \"Server: AmazonEC2\\r\\n\"\n-> \"Transfer-Encoding: chunked\\r\\n\"\n-> \"Content-Type: text/xml;charset=UTF-8\\r\\n\"\n-> \"Via: 1.1 inprtscl03p.corp.emc.com:80 (Cisco-WSA/8.0.6-123)\\r\\n\"\n-> \"Connection: keep-alive\\r\\n\"\n-> \"\\r\\n\"\n-> \"2ec\\r\\n\"\nreading 748 bytes...\n-> \"\"\n-> \"<?xml version=\\\"1.0\\\" encoding=\\\"UTF-8\\\"?>\\n\\n    6f790e0c-5155-427f-9ba6-be7da092dfc5\\n    \\n        \\n            snap-303cd112\\n            vol-90838f75\\n            pending\\n            2015-10-29T23:44:25.000Z\\n            0%\\n            746212085856\\n            10\\n            Snapper ffbbe048-eb63-4f31-be9b-984f45ff6f5f Thursday October 29 or Thu 29/10\\n            false\\n        \\n    \\n\"\nread 748 bytes\nreading 2 bytes...\n-> \"\"\n-> \"\\r\\n\"\nread 2 bytes\n-> \"0\\r\\n\"\n-> \"\\r\\n\"\nConn keep-alive\nI, [2015-10-29T16:46:28.698860 #8385]  INFO -- : [Aws::EC2::Client 200 0.330488 0 retries] describe_snapshots(snapshot_ids:[\"snap-303cd112\"])  \nopening connection to ec2.us-west-1.amazonaws.com:443...\nopened\nstarting SSL for ec2.us-west-1.amazonaws.com:443...\nSSL established\n<- \"POST / HTTP/1.1\\r\\nContent-Type: application/x-www-form-urlencoded; charset=utf-8\\r\\nAccept-Encoding: \\r\\nUser-Agent: aws-sdk-ruby2/2.1.28 ruby/2.2.1 x86_64-linux\\r\\nX-Amz-Date: 20151029T234643Z\\r\\nHost: ec2.us-west-1.amazonaws.com\\r\\nX-Amz-Content-Sha256: 953ff5dbaf34a2d8d2fb19b38ef5addc03f6e2d0333b4c899e80e76abd212edf\\r\\nAuthorization: AWS4-HMAC-SHA256 Credential=AKIAJCKN6RYFLNR7OCFQ/20151029/us-west-1/ec2/aws4_request, SignedHeaders=content-type;host;user-agent;x-amz-content-sha256;x-amz-date, Signature=43142591f88223eaf6f18e6d38651b91f13ef1837ac96fdf45aa886739c3f10a\\r\\nContent-Length: 70\\r\\nAccept: /\\r\\n\\r\\n\"\n-> \"HTTP/1.1 200 OK\\r\\n\"\n-> \"Vary: Accept-Encoding\\r\\n\"\n-> \"Date: Thu, 29 Oct 2015 23:46:43 GMT\\r\\n\"\n-> \"Server: AmazonEC2\\r\\n\"\n-> \"Transfer-Encoding: chunked\\r\\n\"\n-> \"Content-Type: text/xml;charset=UTF-8\\r\\n\"\n-> \"Via: 1.1 inprtscl03p.corp.emc.com:80 (Cisco-WSA/8.0.6-123)\\r\\n\"\n-> \"Connection: keep-alive\\r\\n\"\n-> \"\\r\\n\"\n-> \"2ec\\r\\n\"\nreading 748 bytes...\n-> \"\"\n-> \"<?xml version=\\\"1.0\\\" encoding=\\\"UTF-8\\\"?>\\n\\n    681c263a-8500-4997-bb43-8f28c6f5aac2\\n    \\n        \\n            snap-303cd112\\n            vol-90838f75\\n            pending\\n            2015-10-29T23:44:25.000Z\\n            0%\\n            746212085856\\n            10\\n            Snapper ffbbe048-eb63-4f31-be9b-984f45ff6f5f Thursday October 29 or Thu 29/10\\n            false\\n        \\n    \\n\"\nread 748 bytes\nreading 2 bytes...\n-> \"\"\n-> \"\\r\\n\"\nread 2 bytes\n-> \"0\\r\\n\"\n-> \"\\r\\n\"\nConn keep-alive\nI, [2015-10-29T16:46:43.998668 #8385]  INFO -- : [Aws::EC2::Client 200 0.29743 0 retries] describe_snapshots(snapshot_ids:[\"snap-303cd112\"])  \nopening connection to ec2.us-west-1.amazonaws.com:443...\nopened\nstarting SSL for ec2.us-west-1.amazonaws.com:443...\nSSL established\n<- \"POST / HTTP/1.1\\r\\nContent-Type: application/x-www-form-urlencoded; charset=utf-8\\r\\nAccept-Encoding: \\r\\nUser-Agent: aws-sdk-ruby2/2.1.28 ruby/2.2.1 x86_64-linux\\r\\nX-Amz-Date: 20151029T234659Z\\r\\nHost: ec2.us-west-1.amazonaws.com\\r\\nX-Amz-Content-Sha256: 953ff5dbaf34a2d8d2fb19b38ef5addc03f6e2d0333b4c899e80e76abd212edf\\r\\nAuthorization: AWS4-HMAC-SHA256 Credential=AKIAJCKN6RYFLNR7OCFQ/20151029/us-west-1/ec2/aws4_request, SignedHeaders=content-type;host;user-agent;x-amz-content-sha256;x-amz-date, Signature=338b234937309cd277fb1bf633d0a65d8fd5cad9859fd6dbc6a8f388822458f4\\r\\nContent-Length: 70\\r\\nAccept: /\\r\\n\\r\\n\"\n-> \"HTTP/1.1 200 OK\\r\\n\"\n-> \"Vary: Accept-Encoding\\r\\n\"\n-> \"Date: Thu, 29 Oct 2015 23:46:58 GMT\\r\\n\"\n-> \"Server: AmazonEC2\\r\\n\"\n-> \"Transfer-Encoding: chunked\\r\\n\"\n-> \"Content-Type: text/xml;charset=UTF-8\\r\\n\"\n-> \"Via: 1.1 inprtscl03p.corp.emc.com:80 (Cisco-WSA/8.0.6-123)\\r\\n\"\n-> \"Connection: keep-alive\\r\\n\"\n-> \"\\r\\n\"\n-> \"2ec\\r\\n\"\nreading 748 bytes...\n-> \"\"\n-> \"<?xml version=\\\"1.0\\\" encoding=\\\"UTF-8\\\"?>\\n\\n    45e48b38-9e4a-4c53-8cab-a8983ca09541\\n    \\n        \\n            snap-303cd112\\n            vol-90838f75\\n            pending\\n            2015-10-29T23:44:25.000Z\\n            0%\\n            746212085856\\n            10\\n            Snapper ffbbe048-eb63-4f31-be9b-984f45ff6f5f Thursday October 29 or Thu 29/10\\n            false\\n        \\n    \\n\"\nread 748 bytes\nreading 2 bytes...\n-> \"\"\n-> \"\\r\\n\"\nread 2 bytes\n-> \"0\\r\\n\"\n-> \"\\r\\n\"\nConn keep-alive\nI, [2015-10-29T16:46:59.316753 #8385]  INFO -- : [Aws::EC2::Client 200 0.314751 0 retries] describe_snapshots(snapshot_ids:[\"snap-303cd112\"])  \nopening connection to ec2.us-west-1.amazonaws.com:443...\nopened\nstarting SSL for ec2.us-west-1.amazonaws.com:443...\nSSL established\n<- \"POST / HTTP/1.1\\r\\nContent-Type: application/x-www-form-urlencoded; charset=utf-8\\r\\nAccept-Encoding: \\r\\nUser-Agent: aws-sdk-ruby2/2.1.28 ruby/2.2.1 x86_64-linux\\r\\nX-Amz-Date: 20151029T234714Z\\r\\nHost: ec2.us-west-1.amazonaws.com\\r\\nX-Amz-Content-Sha256: 953ff5dbaf34a2d8d2fb19b38ef5addc03f6e2d0333b4c899e80e76abd212edf\\r\\nAuthorization: AWS4-HMAC-SHA256 Credential=AKIAJCKN6RYFLNR7OCFQ/20151029/us-west-1/ec2/aws4_request, SignedHeaders=content-type;host;user-agent;x-amz-content-sha256;x-amz-date, Signature=67b4c8b077a5da5c608b1f7ef24d43612e288ab9d53bc3eae9cfdfd68a2eb9f7\\r\\nContent-Length: 70\\r\\nAccept: /\\r\\n\\r\\n\"\n-> \"HTTP/1.1 200 OK\\r\\n\"\n-> \"Vary: Accept-Encoding\\r\\n\"\n-> \"Date: Thu, 29 Oct 2015 23:47:13 GMT\\r\\n\"\n-> \"Server: AmazonEC2\\r\\n\"\n-> \"Transfer-Encoding: chunked\\r\\n\"\n-> \"Content-Type: text/xml;charset=UTF-8\\r\\n\"\n-> \"Via: 1.1 inprtscl03p.corp.emc.com:80 (Cisco-WSA/8.0.6-123)\\r\\n\"\n-> \"Connection: keep-alive\\r\\n\"\n-> \"\\r\\n\"\n-> \"2f0\\r\\n\"\nreading 752 bytes...\n-> \"\"\n-> \"<?xml version=\\\"1.0\\\" encoding=\\\"UTF-8\\\"?>\\n\\n    0e194233-1004-4747-ab7b-1f0a7cbc322c\\n    \\n        \\n            snap-303cd112\\n            vol-90838f75\\n            completed\\n            2015-10-29T23:44:25.000Z\\n            100%\\n            746212085856\\n            10\\n            Snapper ffbbe048-eb63-4f31-be9b-984f45ff6f5f Thursday October 29 or Thu 29/10\\n            false\\n        \\n    \\n\"\nread 752 bytes\nreading 2 bytes...\n-> \"\"\n-> \"\\r\\n\"\nread 2 bytes\n-> \"0\\r\\n\"\n-> \"\\r\\n\"\nConn keep-alive\nI, [2015-10-29T16:47:14.663949 #8385]  INFO -- : [Aws::EC2::Client 200 0.343307 0 retries] describe_snapshots(snapshot_ids:[\"snap-303cd112\"])  \n2015-10-29 16:47:14 -0700\n. Thanks for that explanation @awood45 .\nIs that a away to avoid this API call result caching temporarily ?\nIn my code I want to see the real progress of the snapshot and get done as soon as the snapshot is really done.\nAlso, how long are the API call results cached ? Is it same for all the API calls ?\nAny suggestions on how I can do this ?\n. I am trying to automate the process of snapshot and recovery for EBS volumes. \nSo, I started writing some POC like code where my calls are synchronous. This is when I noticed that it takes about 3 minutes for the API to tell us that the snapshot is done.\nI can probably do the snapshot operation asynchronously. However I still do not like the fact that the it takes such a long time for my application (and its clients) to get the correct state from AWS. It just slows down the workflow and creates a confusion about why my application takes a long time to perform a snapshot vs the AWS console.\nPlease do convey this issue to the API team.\n. ",
    "vinhboy": "Thanks!\n. ",
    "tomderham": "Many thanks Alex.\nActually your response gave us the hint we needed to solve this - we had another test VM (forgotten about) that was running another instance of the worker, and so \"stealing\" some of the messages.\nHaving disabled that, everything is working fine.\nIs there any alternative way to deal with this kind of scenario, or we just need to definitively ensure there is only a single consumer worker?\n. ",
    "onyxraven": "Thanks. In the short term, can you ensure the api doesnt ~think~ this is pagination enabled?  I ran into this by assuming it was because each() didn't immediately throw, but then in practice and especially in test/stubbed, thats when it threw the error.\n. Maybe a consistent way to indicate it in the documentation would definitely help.  Let me know if thats something I could open a PR for.\n. ",
    "joaogbcravo": "To have more context about this change:\nI created a machine, then I destroyed it, and waited to not be seen anymore on my dashboard.\nWhen I do describe_instances({instance_ids: [DESTROYED_INSTANCE_ID]}) it returns a good response (200 OK) with an empty reservation list, instead of InvalidInstanceIDNotFound. \nThis  makes the Aws::Resources::Resource exists? method to be a \"liar\" method.\n. Thanks to be so quickly in accepting it :)\n. ",
    "wr0ngway": "If I manually start a new request with the marker returned from the last request, the download proceeds, but eventually fails again in a similar way.  I am thus able to get all the way to the end of the file with some retry logic.  I noticed that the size of the returned data is 1048576 bytes when additional_data_pending=false, but 1048611 when true, so I'm guessing something about the data or how it's processed is causing the abort.\n. Here is a work around that works:\n```\n      open(path, 'wb+') do |f|\n    opts = {\n      db_instance_identifier: db_id,\n      log_file_name: file,\n      marker: \"0\"\n    }\n\n    while true do\n      print \".\"\n      out = rds.download_db_log_file_portion(opts)\n      f.write(out[:log_file_data])\n\n      break if opts[:marker] == out[:marker]\n      opts[:marker] = out[:marker]\n    end\n    puts \".\"\n\n  end\n\n```\n. ",
    "vemv": "Ahhh I just realised that by default (and kinda reasonably), describe_images fetches all kind of images, not just mine. Retrieving 35754 objects certainly takes time!\nAs I said, this slowness was so high that I thought describe_images didn't work at all. Some change ought to be performed - current behavior is not very friendly.\nKeep in mind that describe_images is different from most other methods (e.g. Aws::AutoScaling::Client#describe_launch_configurations), because in most other methods is safe to asume that my objects will be returned.\nThis seems solvable by:\n- Printing a warning if no filters were passed\n- Defaulting to a small request size if no filters were passed (i.e. ask for 100 AMIs instead of 35754)\n- Defaulting to describe my images only.\n. I understand.\nWould changing the default requested pagination size also be considered a breaking change?\n. Thank you @awood45 for an accurate answer!\nYou might want to reflect that in the documentation.\nVictor\n. Confirming it works, feel free to close the issue.\nConsider indicating this in the documentation, the syntax makes sense but it's perfectly possible to not figure it out.\n. ",
    "MarkMurphy": "Got it:\nruby\nAws.config.update({\n  logger: Logger.new($stdout)\n})\n. ",
    "amedeiros": "A api call examples below which are describe or list calls. We have tried to recreate the issues with no luck. It is at random and is also intermittent.\nEC2#describe_security_groups\nRDS#describe_db_instances\nELB#describe_load_balancers\nDirectConnect#describe_virtual_gateways\nS3#list_buckets\nIAM#list_server_certificates\n. They are running on EC2 instances. We use the IAM role attached to the instance then we are assuming role on our customer accounts. Then we make the calls on behalf of the customer via the third party assumed role.\n. We use the role on the EC2 instance that gives us the privileges to do assume role. We then do STS#AssumeRole from the SDK. The returned session is then cached and reused for 50 minutes because a session is good for 60 minutes. So we time out the session 10 minutes prior to what STS does. Then wether or not the credentials come back from the cache or directly from a new STS call we build a Aws::Credentials object then we pass that to the client Aws::EC2::Client.new(credentials: Aws::Credentials, region: public_cloud_region). We do that for all our clients. In the past we were just passing the session credentials as a hash to the clients but were having this same problem with that as well.\n. @awood45 any updates on this currently?\n. ",
    "johnathanludwig": "@awood45 Here is the http request that is logged with the error. Keep in mind this is happening for multiple services and regions.\n```\nAws::ElasticLoadBalancing::Errors::SignatureDoesNotMatch The request signature we calculated does not match the signature you provided. Check your AWS Secret Access Key and signing method. Consult the service documentation for details.\nThe Canonical String for this request should have been\n'POST\n/\ncontent-type:application/x-www-form-urlencoded; charset=utf-8\nhost:elasticloadbalancing.sa-east-1.amazonaws.com\nuser-agent:aws-sdk-ruby2/2.1.36 ruby/2.2.3 x86_64-linux-gnu\nx-amz-content-sha256:234a\nx-amz-date:20151118T182331Z\nx-amz-security-token:AQ==\ncontent-type;host;user-agent;x-amz-content-sha256;x-amz-date;x-amz-security-token\n234a'\nThe String-to-Sign should have been\n'AWS4-HMAC-SHA256\n20151118T182331Z\n20151118/sa-east-1/elasticloadbalancing/aws4_request\nffe5'\nvendor/bundle/ruby/2.2.0/gems/aws-sdk-core-2.1.36/lib/seahorse/client/plugins/raise_response_errors.rb:15 call\nvendor/bundle/ruby/2.2.0/gems/aws-sdk-core-2.1.36/lib/aws-sdk-core/plugins/param_converter.rb:20 call\nvendor/bundle/ruby/2.2.0/gems/aws-sdk-core-2.1.36/lib/aws-sdk-core/plugins/response_paging.rb:26 call\nvendor/bundle/ruby/2.2.0/gems/aws-sdk-core-2.1.36/lib/seahorse/client/plugins/response_target.rb:21 call\nvendor/bundle/ruby/2.2.0/gems/aws-sdk-core-2.1.36/lib/seahorse/client/request.rb:70 send_request\nvendor/bundle/ruby/2.2.0/gems/aws-sdk-core-2.1.36/lib/seahorse/client/base.rb:207 block (2 levels) in define_operation_methods\n\n```\n. Yeah on our end the credentials are always coming from the IAM role attached to the EC2 instance. As @amedeiros said we only use the session from assuming the role and cache it for 50 minutes before getting a new one.\nCould there be something else that causes the session to get expired early?\nAlso is there anyway to know what all the different types of access errors there are? They all seem dynamic and undocumented. We've gotten all of these exceptions that look related and have the same stack trace. Some of them don't give the full output like the example above and instead say\n\nThe security token included in the request is invalid.\n- Aws::Route53::Errors::InvalidClientTokenId\n- Aws::EC2::Errors::AuthFailure\n- Aws::RDS::Errors::InvalidClientTokenId\n- Aws::S3::Errors::InvalidAccessKeyId\n- Aws::ElasticLoadBalancing::Errors::SignatureDoesNotMatch\n- Aws::DirectConnect::Errors::UnrecognizedClientException\n. Ok I was able to catch it while it was happening and test using the cached session and compare with a new session. It did seem to work fine with a new session. So we are going to have to add all these errors to be rescued and get a new session... Still leaves us with those two questions.\n\nThese sessions definitely are intermittently expiring sooner then they should. I was in the middle of testing with one that was working and then it expired with 13 minutes remaining. \nAlso it would be great if there was consistent naming for errors. It seems very likely this issue will resurface under a new exception name.\n. I want to say it was an EC2 AuthFailure, but I unfortunately didn't make a note of which one it was.\nThis is the logic for our caching code. @credentials is what gets passed to the clients we build. You can see that we set a redis ttl based on the expiration that comes back from the assume_role. We don't have any other code that touches the expiration so it shouldn't get changed once set. We unfortunately aren't saving the original expiration, but I have looked at the remaining ttl on redis for keys when they fail and there are often times 10+ minutes remaining. \n``` ruby\nif redis.exists(key)\n  build_credentials(redis.hgetall(key))\nelse\n  role = Aws::STS::Client.new(region: @region).assume_role(@keys)\n  ttl  = ((role.credentials.expiration.to_i - 10.minutes.to_i) - Time.now.to_i).to_i\n  build_credentials(role.credentials)\n  redis.hsetnx(key, :session_token,     credentials.session_token)\n  redis.hsetnx(key, :access_key_id,     credentials.access_key_id)\n  redis.hsetnx(key, :secret_access_key, credentials.secret_access_key)\n  redis.expire(key, ttl)\nend\ndef build_credentials(credentials)\n  @credentials = Aws::Credentials.new(credentials['access_key_id'],\n                                      credentials['secret_access_key'],\n                                      credentials['session_token'])\nend\n```\nI have used the sessions that were failing to manually build different clients and try different api calls. I never got the \"ExpiredToken\" which is what we previously expected. For example, I took one that was throwing Aws::RDS::Errors::SignatureDoesNotMatch and tried to use that session to make an EC2 describe regions call. The result of the describe_regions was:\n\nAws::EC2::Errors::AuthFailure: AWS was not able to validate the provided access credentials\n\nRequesting a new session for the given role and making the same call would work.\n. We have some redis locking code around the code I posted above. I was looking at to see how it works and I believe you are correct. It looks to me like there is a possible race condition where they could both fall into the else at the same time and go through the set calls. \nWe have already put the rescue code in on our end, plus we have another change where the caching happens up front in a single call so the race condition shouldn't happen anymore.\nEither way, we should be able to replace the hsetnx with hmset so they all get set in a single call. That way if they do get overwritten they at least stay lined up correctly.\n@awood45 Thank you for the help!\n. Yeah, sorry. So this change fixes a call like this with DynamoDB for example:\n```ruby\nAws::DynamoDB::Client.new.describe_table('table_name' => 'test')\n=> ArgumentError: missing required parameter params[:table_name]\n```\nHowever optional params like with EC2 DescribeInstances already work:\n```ruby\nAws::EC2::Client.new.describe_instances('filters' => [{ name: 'tag-key', values: ['key1'] }])\n<Aws::Structure:Aws::EC2::Types::DescribeInstancesResult\n```. Interesting. Is S3 just doing something different with the keys then? I haven't dug into all the code, but when I tested with DynamoDB it seemed to search correctly with the string key once I passed the required check that is in this PR.\nUltimate it is not a big deal if we can't accept this. On our end we have added a call to symbolize_keys on the arguments before we make the SDK call. It just seemed appropriate since these are hashes and both styles are common with them.. ",
    "atiro": "Sure, we are trying to get some data from our Amazon Elasticsearch endpoint. We following the python work through to generate the signed request and this worked fine, the only difference with the ruby request was the added Content-Length header which returns the error:\n\"{\\\"message\\\":\\\"The request signature we calculated does not match the signature you provided. Check your AWS Secret Access Key and signing method. Consult the service documentation for details.\\n\\nThe Canonical String for this request should have been\\n'GET\\n\\n\\ncontent-length:\\n[...]\nwhich doesn't match the header the library sends (content-length:0). But we have to send the header because body_stream is set in the seahorse handler.\n. Sure (apologies, new to ruby so terminology is not great!). Am using the example code adapted for ES like follows (replacing with your own ES ENDPOINT):\n```\nrequire 'aws-sdk-core'\nrequire 'uri'\nrequire 'net/http'\nregion = 'eu-west-1'\nprefix= 'es'\nendpoint = URI.parse(ES ENDPOINT)\nsetup required configuration options for the NetHttp::Handler\nconfig = Seahorse::Client::Configuration.new\nSeahorse::Client::NetHttp::ConnectionPool::OPTIONS.each_pair do |name, default|\n  config.add_option(name, default)\nend\nthese are the default options you would get by calling\nthe Configuration.build! method without options\nconfig = config.build!({\n  http_proxy: nil,\n  http_open_timeout: 15,\n  http_read_timeout: 60,\n  http_idle_timeout: 5,\n  http_continue_timeout: 1,\n  http_wire_trace: false,\n  logger: nil,\n  ssl_verify_peer: true,\n  ssl_ca_bundle: nil,\n  ssl_ca_directory: nil,\n  ssl_ca_store: nil,\n})\nbuild the http request\ncontext = Seahorse::Client::RequestContext.new(config: config)\ncontext.http_request.endpoint = endpoint\ncontext.http_request.http_method = \"GET\"\ncontext.http_request.headers[\"x-amz-date\"] = Time.now.utc.strftime(\"%Y%m%dT%H%M%SZ\")\ncontext.http_request.headers[\"host\"] = ES ENDPOINT\nsign the http request\ncredentials = Aws::Credentials.new('','')\nAws::Signers::V4.new(credentials, prefix, region).sign(context.http_request)\nsend the http request\nSeahorse::Client::NetHttp::Handler.new.call(context)\ninspect the http response\np context.http_response.status_code\np context.http_response.headers\np context.http_response.body.read\np context.http_response.error # populated when network error encountered\n```\nThis won't work, failing with the Signature does't match error (because for some reason the content-length header has lost its value when the signature calculated on AWS side, not sure why). Anyway, content-length isn't needed for the GET, so if we stop content-length being sent it works, but we can only do this if  we stop using body_stream. This is hard-coded in build_net_request though.\n. ",
    "agperson": "My use case is very similar and I would love to see movement on this PR!\n. It's really disappointing that this functionality was not merged.\n. ",
    "Freaklin": "Quiet simple :)\nI'm using aws-sdk for ruby and I'm trying to retrieve a password of a windows instance.\nec2 = Aws::EC2::Client.new( region: region_name )\nec2.get_password_data({ instance_id: \"instance_id\" })\nthen it returns me a encrypted password.\nBut using AWS-CLI I'm able to retrieve A already decrypted password, pointing a .pem file.\naws ec2 get-password-data --priv-launch-key file.pem --instance-id \"instance_id\"\nI would like an option like \"--priv-launch-key file.pem\" on aws-sdk for ruby. well I guess not only me once I've found an issue on the aws-sdk-v1 with the same request.\nhttps://github.com/aws/aws-sdk-ruby/issues/459\n. ",
    "dominikgrygiel": "Yes, I have. Please look at the snippet that I've pasted in my first comment. As you can see when I try to retrieve using describe_alarms_for_metric for namespace = 'AWS/DynamoDB' and metric_name = 'ConsumedWriteCapacityUnits' I get no results. However when I retrieve all of available alarms using describe_alarms I get a list, which then revelas to have 10 such alarms.\n. I've tried to debug more and it seems not to be a library related issue. I've tried to do same operations using AWS CLI with same result. Also I've found a thread with somebody having same issue as I am experiencing https://forums.aws.amazon.com/thread.jspa?messageID=372063 I'll post there. Sorry for confusion.\n. ",
    "vancluever": "Basically, I want MFA without having to assume a role. So the idea would be to get the session, and set that as the default credentials using Aws.config. If a role needs to be assumed later, that session can overwrite the credentials set after the fact, and/or the old credentials can be stashed so they can be switched back to easily after one is done with the assumed role's session.\nI am hoping this will allow the ability to assume multiple roles in a single session without having to re-enter MFA. Also, it enables access to stuff that the IAM user has access to without a role but is restricted by MFA in the policy.\nThe workaround I see right now is to set access_key_id, secret_access_key, and session_token directly. The, setting credentials seems to override this, so I can use this when I want to assume a role and just use AssumeRoleCredentials. After I am done with the assumed role session I can just delete that key. I'm not too sure if that is something I can count on though...\n. Thanks Alex!\n. Thanks Trevor,\nI don't necessarily need this right now, the use case I needed it for has changed pretty drastically since I asked about this, but it would be cool to see still!\n. ",
    "bhouse": "An example of a workaround for this: https://gist.github.com/bhouse/f97980adc5df2b5db7fd435a6112c4d8\n. ",
    "franzliedke": "@trevorrowe A bit late to the party, but since I just experienced the same issue, I can tell you about my use-case.\nWe have an in-memory object from rubyzip/rubyzip representing a ZIP file (could be on disk as well). From that, I pull one Zip::InputStream object which references one file in the ZIP archive. Because that class does not implement #size, I cannot upload it directly to S3, having to first call #read on it. Is that a valid use-case? \ud83d\ude00 . ",
    "IBazylchuk": "@awood45 for first i thought about network too, but when i ran shell script to ping dynamo host, all was fine. Seems something with sockets, i set today settings to ubuntu, but still get errors\nnet.core.somaxconn = 65535\nnet.ipv4.ip_local_port_range = 1024 65535\nnet.ipv4.tcp_tw_reuse = 1\nnet.ipv4.tcp_fin_timeout = 10\nnet.ipv4.tcp_tw_recycle = 1\nnet.ipv4.tcp_keepalive_time = 1800\nnet.ipv4.tcp_keepalive_intvl = 15\nnet.ipv4.tcp_keepalive_probes = 5\n. ",
    "hakunin": "I am using shrine and request the url with ajax. Shrine which asks the bucket to pre-sign a post, which results in the bucket url not using the endpoint.\nSince the idea is to use fakes3 not if statements to let the code go completely different path based on the environment, it would be good if setting the endpoint in aws-sdk-ruby would simply return the endpoint path.\nBtw I am using version 2.\n. Thanks alex! \n. Saving them would be awkward, since you have to watch them and re-generate them when old.\n. We don't have much traffic yet and this is the first time I see it. It happened when removing a file from S3 in a background job and it looks like it ended up being successful on an automatic re-run so I don't have data to recreate it.\n. This is from a sidekiq background job.\n. Thanks for sharing @trevorrowe, I think the files could have been uploaded via direct upload (we support both), so I am wondering if that could cause let the issue happen, as it doesn't go through the SDK?\n. ",
    "800a7b32": "Experiencing a potentially related issue with the ruby aws-sdk (v2) whereby we're using a Ceph endpoint rather than S3 itself (ala similar setup to fake-s3) and when we include 'endpoint' in our config (alongside force_path_style), the gem always returns an error:\nAws::S3::Errors::InvalidArgument\n. @awood45 all done for you: #1021 \n. Gemfile.lock for versions:\naws-sdk (2.2.1)\n      aws-sdk-resources (= 2.2.1)\n    aws-sdk-core (2.2.1)\n      jmespath (~> 1.0)\n    aws-sdk-resources (2.2.1)\n      aws-sdk-core (= 2.2.1)\nSpecific task (rake task run nightly): https://gist.github.com/andretanguy/f29a8302450a39e290b0\nFor params, I have the following: AWS_ENDPOINT: 'https://vault.ecloud.co.uk', AWS_REGION: 'default'\nStacktrace:\nUploading to backup servers...\nrake aborted!\nAws::S3::Errors::InvalidArgument: \n/Users/andre/Documents/Web_Applications/VesselHQ/vhq-v3/vendor/bundle/ruby/2.2.0/gems/aws-sdk-core-2.2.1/lib/seahorse/client/plugins/raise_response_errors.rb:15:in `call'\n/Users/andre/Documents/Web_Applications/VesselHQ/vhq-v3/vendor/bundle/ruby/2.2.0/gems/aws-sdk-core-2.2.1/lib/aws-sdk-core/plugins/s3_sse_cpk.rb:18:in `call'\n/Users/andre/Documents/Web_Applications/VesselHQ/vhq-v3/vendor/bundle/ruby/2.2.0/gems/aws-sdk-core-2.2.1/lib/aws-sdk-core/plugins/param_converter.rb:20:in `call'\n/Users/andre/Documents/Web_Applications/VesselHQ/vhq-v3/vendor/bundle/ruby/2.2.0/gems/aws-sdk-core-2.2.1/lib/seahorse/client/plugins/response_target.rb:21:in `call'\n/Users/andre/Documents/Web_Applications/VesselHQ/vhq-v3/vendor/bundle/ruby/2.2.0/gems/aws-sdk-core-2.2.1/lib/seahorse/client/request.rb:70:in `send_request'\n/Users/andre/Documents/Web_Applications/VesselHQ/vhq-v3/vendor/bundle/ruby/2.2.0/gems/aws-sdk-core-2.2.1/lib/seahorse/client/base.rb:207:in `block (2 levels) in define_operation_methods'\n/Users/andre/Documents/Web_Applications/VesselHQ/vhq-v3/vendor/bundle/ruby/2.2.0/gems/aws-sdk-resources-2.2.1/lib/aws-sdk-resources/services/s3/multipart_file_uploader.rb:43:in `initiate_upload'\n/Users/andre/Documents/Web_Applications/VesselHQ/vhq-v3/vendor/bundle/ruby/2.2.0/gems/aws-sdk-resources-2.2.1/lib/aws-sdk-resources/services/s3/multipart_file_uploader.rb:34:in `upload'\n/Users/andre/Documents/Web_Applications/VesselHQ/vhq-v3/vendor/bundle/ruby/2.2.0/gems/aws-sdk-resources-2.2.1/lib/aws-sdk-resources/services/s3/file_uploader.rb:32:in `upload'\n/Users/andre/Documents/Web_Applications/VesselHQ/vhq-v3/vendor/bundle/ruby/2.2.0/gems/aws-sdk-resources-2.2.1/lib/aws-sdk-resources/services/s3/object.rb:231:in `upload_file'\n/Users/andre/Documents/Web_Applications/VesselHQ/vhq-v3/lib/tasks/backup.rake:32:in `send_to_storage'\n/Users/andre/Documents/Web_Applications/VesselHQ/vhq-v3/lib/tasks/backup.rake:23:in `block (2 levels) in <top (required)>'\nTasks: TOP => pg:backup\n(See full trace by running task with --trace)\n. Uploading to backup servers...\nopening connection to vault.ecloud.co.uk:443...\nopened\nstarting SSL for vault.ecloud.co.uk:443...\nSSL established\n<- \"POST dump.sql?uploads HTTP/1.1\\r\\nContent-Type: \\r\\nAccept-Encoding: \\r\\nUser-Agent: aws-sdk-ruby2/2.2.1 ruby/2.2.3 x86_64-darwin14 resources\\r\\nX-Amz-Date: 20151212T203812Z\\r\\nHost: vault.ecloud.co.uk\\r\\nX-Amz-Content-Sha256: xxxx-removed-xxxx\\r\\nAuthorization: AWS4-HMAC-SHA256 Credential=xxxxxx-removed-xxxxxx/default/s3/aws4_request, SignedHeaders=host;user-agent;x-amz-content-sha256;x-amz-date, Signature=xxxx-removed-xxxx\\r\\nContent-Length: 0\\r\\nAccept: */*\\r\\n\\r\\n\"\n-> \"HTTP/1.1 400 Bad Request\\r\\n\"\n-> \"x-amz-request-id: tx0000000000000003a260f-00566c85b6-1bfe76e-default\\r\\n\"\n-> \"Content-Length: 81\\r\\n\"\n-> \"Accept-Ranges: bytes\\r\\n\"\n-> \"Content-type: application/xml\\r\\n\"\n-> \"Date: Sat, 12 Dec 2015 20:38:14 GMT\\r\\n\"\n-> \"\\r\\n\"\nreading 81 bytes...\n-> \"<?xml version=\\\"1.0\\\" encoding=\\\"UTF-8\\\"?><Error><Code>InvalidArgument</Code></Error>\"\nread 81 bytes\nConn keep-alive\nrake aborted!\n. ",
    "traviskroberts": "@awood45 Thanks for the quick reply. Here's the wire trace from the call:\nopening connection to cloudsearch-domain-id.us-east-1.cloudsearch.amazonaws.com:443...\nopened\nstarting SSL for cloudsearch-domain-id.us-east-1.cloudsearch.amazonaws.com:443...\nSSL established\n<- \"GET /2013-01-01/search?format=sdk&pretty=true&expr=%7B%22distance%22%3A%22haversin%2835.8456%2C-86.3903%2Clocation.latitude%2Clocation.longitude%29%22%2C%22myrank%22%3A%22_score%2Flog%28distance%29%22%7D&q=%28or%20%28phrase%20field%3D%27name%27%20boost%3D100%20%27test%27%29%20%28prefix%20field%3D%27name%27%20bo\nost%3D10%20%27test%27%29%20%28phrase%20field%3D%27street_address%27%20boost%3D0.5%20%27test%27%29%20%28phrase%20field%3D%27city%27%20boost%3D1.0%20%27test%27%29%20%28phrase%20field%3D%27state_abbr%27%20boost%3D1.0%20%27test%27%29%20%28phrase%20field%3D%27zip_code%27%20boost%3D1.0%20%27test%27%29%29&q.parser=structu\nred&size=200&sort=myrank%20desc HTTP/1.1\\r\\nContent-Type: \\r\\nAccept-Encoding: \\r\\nUser-Agent: aws-sdk-ruby2/2.2.1 ruby/2.0.0 x86_64-darwin14.0.0\\r\\nX-Amz-Date: 20151123T225907Z\\r\\nHost: cloudsearch-domain-id.us-east-1.cloudsearch.amazonaws.com\\r\\nX-Amz-Content-Sha256: e3b0c44298fc\n1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\\r\\nAuthorization: AWS4-HMAC-SHA256 Credential=AKIAJY7R7EQZLEQWMQTA/20151123/us-east-1/cloudsearch/aws4_request, SignedHeaders=host;user-agent;x-amz-content-sha256;x-amz-date, Signature=70127d7857fd688fb8e04a28fdbf872e5ae49075e4be76b5608add1a7274f6c0\\r\\nContent-Le\nngth: 0\\r\\nAccept: */*\\r\\n\\r\\n\"\n-> \"HTTP/1.1 500 Server Error\\r\\n\"\n-> \"Content-Type: application/json; charset=UTF-8\\r\\n\"\n-> \"ETag: \\\"YzAwMDAwMDAwMDAwMDAwMFNvbHI=\\\"\\r\\n\"\n-> \"Last-Modified: Fri, 20 Nov 2015 16:32:56 GMT\\r\\n\"\n-> \"Content-Length: 196\\r\\n\"\n-> \"Connection: keep-alive\\r\\n\"\n-> \"\\r\\n\"\nreading 196 bytes...\n-> \"{\\\"error\\\":{\\\"rid\\\":\\\"y/LJtJMquiUKmIIQ\\\",\\\"message\\\":\\\"[*Deprecated*: Use the outer message field] Unrecognized method call (log).\\\"},\\\"message\\\":\\\"Unrecognized method call (log).\\\",\\\"__type\\\":\\\"#SearchException\\\"}\"\nread 196 bytes\nConn keep-alive\nopening connection to cloudsearch-domain-id.us-east-1.cloudsearch.amazonaws.com:443...\nopened\nstarting SSL for cloudsearch-domain-id.us-east-1.cloudsearch.amazonaws.com:443...\nSSL established\n<- \"GET /2013-01-01/search?format=sdk&pretty=true&expr=%7B%22distance%22%3A%22haversin%2835.8456%2C-86.3903%2Clocation.latitude%2Clocation.longitude%29%22%2C%22myrank%22%3A%22_score%2Flog%28distance%29%22%7D&q=%28or%20%28phrase%20field%3D%27name%27%20boost%3D100%20%27test%27%29%20%28prefix%20field%3D%27name%27%20b$ost%3D10%20%27test%27%29%20%28phrase%20field%3D%27street_address%27%20boost%3D0.5%20%27test%27%29%20%28phrase%20field%3D%27city%27%20boost%3D1.0%20%27test%27%29%20%28phrase%20field%3D%27state_abbr%27%20boost%3D1.0%20%27test%27%29%20%28phrase%20field%3D%27zip_code%27%20boost%3D1.0%20%27test%27%29%29&q.parser=struct$red&size=200&sort=myrank%20desc HTTP/1.1\\r\\nContent-Type: \\r\\nAccept-Encoding: \\r\\nUser-Agent: aws-sdk-ruby2/2.2.1 ruby/2.0.0 x86_64-darwin14.0.0\\r\\nX-Amz-Date: 20151123T225908Z\\r\\nHost: cloudsearch-domain-id.us-east-1.cloudsearch.amazonaws.com\\r\\nX-Amz-Content-Sha256: e3b0c44298f$1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\\r\\nAuthorization: AWS4-HMAC-SHA256 Credential=AKIAJY7R7EQZLEQWMQTA/20151123/us-east-1/cloudsearch/aws4_request, SignedHeaders=content-length;host;user-agent;x-amz-content-sha256;x-amz-date, Signature=4ed8cf8e7e207419edc888669cdf3cf0727e08b5b8bf10f59706c45631357b$c\\r\\nContent-Length: 0\\r\\nAccept: */*\\r\\n\\r\\n\"\n-> \"HTTP/1.1 403 Forbidden\\r\\n\"\n-> \"Content-Type: application/json\\r\\n\"\n-> \"x-amzn-RequestId: cd6ad2f0-9235-11e5-a368-f51d032365ca\\r\\n\"\n-> \"Content-Length: 3149\\r\\n\"\n-> \"Connection: keep-alive\\r\\n\"\n-> \"\\r\\n\"\nreading 3149 bytes...\n-> \"{\\\"__type\\\":\\\"#SignatureDoesNotMatch\\\",\\\"error\\\":{\\\"message\\\":\\\"[*Deprecated*: Use the outer message field] The request signature we calculated does not match the signature you provided. Check your AWS Secret Access Key and signing method. Consult the service documentation for details.\\\\n\\\\nThe Canonical Strin$ for this request should have been\\\\n'GET\\\\n/2013-01-01/search\\\\nexpr=%7B%22distance%22%3A%22haversin%2835.8456%2C-86.3903%2Clocation.latitude%2Clocation.longitude%29%22%2C%22myrank%22%3A%22_score%2Flog%28distance%29%22%7D&format=sdk&pretty=true&q=%28or%20%28phrase%20field%3D%27name%27%20boost%3D100%20%27test%27%2$%20%28prefix%20field%3D%27name%27%20boost%3D10%20%27test%27%29%20%28phrase%20field%3D%27street_address%27%20boost%3D0.5%20%27test%27%29%20%28phrase%20field%3D%27city%27%20boost%3D1.0%20%27test%27%29%20%28phrase%20field%3D%27state_abbr%27%20boost%3D1.0%20%27test%27%29%20%28phrase%20field%3D%27zip_code%27%20boost%3D$.0%20%27test%27%29%29&q.parser=structured&size=200&sort=myrank%20desc\\\\ncontent-length:\\\\nhost:cloudsearch-domain-id.us-east-1.cloudsearch.amazonaws.com\\\\nuser-agent:aws-sdk-ruby2/2.2.1 ruby/2.0.0 x86_64-darwin14.0.0\\\\nx-amz-content-sha256:e3b0c44298fc1c149afbf4c8996fb92427ae41e46$9b934ca495991b7852b855\\\\nx-amz-date:20151123T225908Z\\\\n\\\\ncontent-length;\"\n-> \"host;user-agent;x-amz-content-sha256;x-amz-date\\\\ne3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855'\\\\n\\\\nThe String-to-Sign should have been\\\\n'AWS4-HMAC-SHA256\\\\n20151123T225908Z\\\\n20151123/us-east-1/cloudsearch/aws4_request\\\\nc9a079251f7b1d5277864df1f2de97fbb331dc78d41998eac2a7761cd724cfad'\\\\$\\\"},\\\"message\\\":\\\"The request signature we calculated does not match the signature you provided. Check your AWS Secret Access Key and signing method. Consult the service documentation for details.\\\\n\\\\nThe Canonical String for this request should have been\\\\n'GET\\\\n/2013-01-01/search\\\\nexpr=%7B%22distance%22%3A%22$aversin%2835.8456%2C-86.3903%2Clocation.latitude%2Clocation.longitude%29%22%2C%22myrank%22%3A%22_score%2Flog%28distance%29%22%7D&format=sdk&pretty=true&q=%28or%20%28phrase%20field%3D%27name%27%20boost%3D100%20%27test%27%29%20%28prefix%20field%3D%27name%27%20boost%3D10%20%27test%27%29%20%28phrase%20field%3D%27stree$_address%27%20boost%3D0.5%20%27test%27%29%20%28phrase%20field%3D%27city%27%20boost%3D1.0%20%27test%27%29%20%28phrase%20field%3D%27state_abbr%27%20boost%3D1.0%20%27test%27%29%20%28phrase%20field%3D%27zip_code%27%20boost%3D1.0%20%27test%27%29%29&q.parser=structured&size=200&sort=myrank%20desc\\\\ncontent-length:\\\\nhos$:cloudsearch-domain-id.us-east-1.cloudsearch.amazonaws.com\\\\nuser-agent:aws-sdk-ruby2/2.2.1 ruby/2.0.0 x86_64-darwin14.0.0\\\\nx-amz-content-sha256:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\\\\nx-amz-date:20151123T225908Z\\\\n\\\\ncontent-length;host;user-agent;x-am$-content-sha256;x-amz-date\\\\ne3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855'\\\\n\\\\nThe String-to-Sign should have been\\\\n'AWS4-HMAC-SHA256\\\\n20151123T225908Z\\\\n20151123/us-east-1/cloudsearch/aws4_request\\\\nc9a079251f7b1d5277864df1f2de97fbb331dc78d41998eac2a7761cd724cfad'\\\\n\\\"}\"\nread 3149 bytes\nConn keep-alive\n. @awood45 Thanks so much for looking into this.\nHere is a gist of my \"search\" class as well as a simplified controller that calls it: https://gist.github.com/traviskroberts/8b99cfdcc6bf441b2769\n. @awood45 Of course!\nHere is the output of each command:\n```\n@client.handlers.entries.each { |h| puts \"#{h.class}: #{h.step} #{h.priority}\" }\nSeahorse::Client::HandlerListEntry: build 90\nSeahorse::Client::HandlerListEntry: send 50\nSeahorse::Client::HandlerListEntry: validate 95\nSeahorse::Client::HandlerListEntry: initialize 90\nSeahorse::Client::HandlerListEntry: sign 0\nSeahorse::Client::HandlerListEntry: initialize 50\nSeahorse::Client::HandlerListEntry: validate 50\nSeahorse::Client::HandlerListEntry: build 50\nSeahorse::Client::HandlerListEntry: sign 99\nSeahorse::Client::HandlerListEntry: sign 50\nSeahorse::Client::HandlerListEntry: initialize 90\nSeahorse::Client::HandlerListEntry: build 50\nSeahorse::Client::HandlerListEntry: sign 50\n```\n```\n@client.handlers.to_a\n[\n  Seahorse::Client::NetHttp::Handler,\n  Seahorse::Client::Plugins::ContentLength::Handler,\n  Aws::Json::ErrorHandler,\n  Aws::Plugins::RequestSigner::Handler,\n  Aws::Plugins::RetryErrors::Handler,\n  Aws::Rest::Handler,\n  Aws::Plugins::UserAgent::Handler,\n  Seahorse::Client::Plugins::Endpoint::Handler,\n  Aws::Plugins::ParamValidator::Handler,\n  Seahorse::Client::Plugins::RaiseResponseErrors::Handler,\n  Aws::Plugins::ParamConverter::Handler,\n  Seahorse::Client::Plugins::ResponseTarget::Handler\n]\n```\n. No problem, here's the updated output:\nSeahorse::Client::Plugins::Endpoint::Handler: build 90\nSeahorse::Client::NetHttp::Handler: send 50\nSeahorse::Client::Plugins::RaiseResponseErrors::Handler: validate 95\nSeahorse::Client::Plugins::ResponseTarget::Handler: initialize 90\nSeahorse::Client::Plugins::ContentLength::Handler: sign 0\nAws::Plugins::ParamConverter::Handler: initialize 50\nAws::Plugins::ParamValidator::Handler: validate 50\nAws::Plugins::UserAgent::Handler: build 50\nAws::Plugins::RetryErrors::Handler: sign 99\nAws::Plugins::RequestSigner::Handler: sign 50\nAws::Plugins::ResponsePaging::Handler: initialize 90\nAws::Rest::Handler: build 50\nAws::Json::ErrorHandler: sign 50\n. @awood45 I can confirm that I get the following signing error with the first query, but the other two work just fine.\n```\nAws::CloudSearchDomain::Errors::SignatureDoesNotMatch: The request signature we calculated does not match the signature you provided. Check your AWS Secret Access Key and signing method. Consult the service documentation for details.\nThe Canonical String for this request should have been\n'GET\n/2013-01-01/search\nexpr=%7B%20%27score%27%3A%20%27log%28_score%29%27%20%7D&format=sdk&pretty=true&q=star&return=name&sort=score%20asc\ncontent-length:\nhost:my-cloudsearch-domain.us-east-1.cloudsearch.amazonaws.com\nuser-agent:aws-sdk-ruby2/2.2.1 ruby/2.0.0 x86_64-darwin14.0.0\nx-amz-content-sha256:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\nx-amz-date:20151125T213051Z\ncontent-length;host;user-agent;x-amz-content-sha256;x-amz-date\ne3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855'\nThe String-to-Sign should have been\n'AWS4-HMAC-SHA256\n20151125T213051Z\n20151125/us-east-1/cloudsearch/aws4_request\nbac396d5a1f08025bc89aaed949fbe87b32d3361d84ae1b377b9f597f2a4fb47'\n``\n. @awood45 Thanks so much for your help in diagnosing what's going on here. I've tried to change my query to uselog10instead oflogand now I'm getting aAws::CloudSearchDomain::Errors::SearchExceptionerror that saysundefined field: \"distance\"in themyrank` expression.\nIt appears that the definition of myrank doesn't have access to the previously defined distance expression. If I sort by distance desc it works just fine.\nAny ideas what might be causing this?\nThanks again for all your help!\n. I was finally able to get around the errors by just combining the two expressions into one.\nInstead of: \nexpr = {\n  \"distance\": \"haversin(35.8456,-86.3903,location.latitude,location.longitude)\",\n  \"myrank\": \"_score/log10(distance)\"\n}\nI changed it to:\nexpr = { \n  \"myrank\": \"_score/log10(haversin(35.8456,-86.3903,location.latitude,location.longitude))\" \n}\n. ",
    "waf-okta": "info for op: https://boto3.readthedocs.org/en/latest/reference/core/session.html#boto3.session.Session for how python does it\n. a=boto3.session.Session(profile_name='aew1d')\nb=a.client('ec2') # for ec2\nc=a.client('s3')\n. ",
    "jkanywhere": "I'm not asking for this to be re-opened.\nJust noting this topic has been considered before: https://github.com/aws/aws-sdk-ruby/pull/703\n. ",
    "AnuragRamdasan": "@awood45 I am actually using an older version but the docs state that this method is available there.\nThe client version in the above code snippet and in the aws docs is V20120601 and the docs say that add_tags is available.\nhttp://docs.aws.amazon.com/AWSRubySDK/latest/AWS/ELB/Client/V20120601.html\n. I ran gem list | grep aws\nand got this response\naws-sdk (2.0.47, 2.0.43, 1.64.0, 1.46.0, 1.39.0)\naws-sdk-core (2.1.35, 2.0.47, 2.0.43)\naws-sdk-resources (2.1.35, 2.0.47, 2.0.43)\naws-sdk-v1 (1.66.0, 1.64.0)\nLooking at it, with 1.66 and 1.64, it should've had add_tags.\n. My gemfile.lock had one version locked on 1.46. I upgraded it to 1.64 but seems like the issue still persists.\nLet me poke around a bit more and then get back to you.\n. ",
    "thoiberg": "@awood45 Stack track is as follows:\nNoMethodError: undefined method `call' for #<Aws::Log::Formatter:0x00000005caa1c0>\nfrom C:/Ruby22-x64/lib/ruby/gems/2.2.0/gems/aws-sdk-core-2.2.3/lib/aws-sdk-core/log/formatter.rb:108:in `method_missing'\n. ",
    "common-nighthawk": "Thanks for the merge, Alex.\nI'm embarrassed to say that I thought { 'hello' => 'world', 'foo' => 'bar', } was invalid Ruby code.  I should have run it in IRB first.\nHowever, I did confirm that this syntax is consistent with accepted best practices--\nhttps://github.com/bbatsov/ruby-style-guide#no-trailing-array-commas\nWhich is all to say... sorry for using 'erroneous' :wink:\n(I recently watched your talk from RailsConf15.  Thanks for everything re: AWS + rb.)\n. ",
    "janko": "Thanks!\nCool, I didn't know I've fixed an open issue. And I'm also the maintainer of Shrine which the opener mentions, it's cool how it's all connected.\n. Updated.\n. I think this is already possible by passing a Proc as a stub value. I discovered this feature when reading tests for aws-sdk itself, and I just made a PR to document this: https://github.com/aws/aws-sdk-ruby/pull/1539.\nTo take your original example, you should be able to do something like this:\nrb\nclient.stub_responses(:delete_certificate, -> (context) {\n  context.params[:certificate_arn] == \"foo\" ? \"YourError\" : {...}\n}). For future reference, there is now a content_disposition gem that correctly formats the value for the Content-Disposition header. Having UTF-8 characters in the Content-Disposition header is not allowed anyway according to the HTTP spec, and content_disposition gem correctly escapes them, adding both filename (ASCII) and filename* (UTF-8) media parameters:\n```rb\nrequire \"content_disposition\"\nContentDisposition.attachment(\"\ud83d\ude2c.pdf\")\n=> \"attachment; filename=\\\"%3F.pdf\\\"; filename*=UTF-8''%F0%9F%98%AC.pdf\"\n``. @awood45 It happens when using the S3 endpoint for deleting multiple objects at once (through the corresponding aws-sdk method).\n. Thanks, but this is executed when app is initializing, so it would be ideal that we don't execute any HTTP requests there. I'm suggesting to make the above method also throw an ArgumentError, just because I think it would improve user experiences for the situation that I mentioned.\n. I agree. Would at least adding a warning thatnilwas passed make sense?\n. @MMartyn That's awesome, I had no idea stubs could accept procs, that way I can actually ensure I'm passing the right arguments. I don't see this documented anywhere, so I will try to come up with a PR for that.. I will close this, as I don't feel too strongly about settingThread#abort_on_exceptiontotrueanymore (I actually like the exception propagating only when we callThread#value`).\nHowever, I think the first commit is nice to have because it brings consistency between MultipartFileUploader and ObjectMultipartCopier implementations. I will open a separate PR for that.. I'm curious, why is the IO object being duplicated? Wouldn't the behaviour be the same if we read the original IO object and rewind it afterwards?. Thanks for a thorough answer!\nI did consider the new multipart downloader, however, my use case is a bit different than normal download. I actually need the to stream the content (i.e. pass a block to #get_object), because I'm allowing users to partially retrieve object's content, without having to fully download it. Here is where I'm using this; that part of code transforms the yielded chunks into an IO-like object, which downloads content (fetches next chunks) as it is being read. For that I cannot use the multipart downloader feature, because it doesn't stream content.. I'm not sure if that will have all the benefits of using #get_object.  For one, we don't know how much the user wants to download, the user chooses that, and I don't think it would be easy to add such an on-demand behaviour to the multipart downloader.\nrb\nio = Down::ChunkedIO.new(chunks: object.enum_for(:get))\nio.read(1*1024*1024) # downloads and returns first ~1MB\nio.close # `#get_object` request is terminated, and nothing more gets downloaded\nSecondly, using the multipart downloader would mean the contents would necessarily need to be downloaded to disk. However, that's not ideal in all cases. For example, Shrine (this gem that I'm maintaining) has a download endpoint, which streams the file directly from S3 to the response body. Because #get_object allows me to retrieve chunks directly as they are downloaded, I can write them to the response body without ever writing to disk, which means you can download many S3 objects through that endpoint at the same time without worrying that you will run out of disk space.\nThirdly, which should be probably discussed in a separate ticket specific to the multipart downloader feature, I haven't managed to achieve the same performance as with #get_object for larger objects (I tested with up to 200MB). It was some time ago, and I meant to raise an issue for that, but I had forgotten the numbers, but I think with multipart downloader it took about twice as slow to download larger objects than using #get_object directly (or maybe less). I tried increasing the chunk size, but I don't remember there was much improvement. IIRC, the most time was spent on IO.copy_stream when merging the files at the end (about 60%). So I would prefer using #get_object also because it's currently faster.. As @sorah has already discovered in https://github.com/aws/aws-sdk-ruby/pull/1516, the FilePart#read method had incorrect behaviour, which doesn't match Ruby's IO#read. This is the relevant bit of IO#read behaviour:\n\nwhen length argument is not given and file is at EOF, return \"\"\nwhen length argument is given and file is at EOF, return nil\n\nFilePart#read respects this behaviour when output buffer is not used, but it doesn't when it is used; in that case it returns \"\" on EOF even if length argument was given. This would cause an endless loop when generating checksums.\nI fixed this now, and the tests are green.. Sure thing.\nBy submitting this pull request, I confirm that my contribution is made under the terms of the Apache 2.0 license.. By submitting this pull request, I confirm that my contribution is made under the terms of the Apache 2.0 license.\nI'm not sure what's making this change fail on JRuby. Maybe we should try upgrading to a newer version.. @cjyclaire I think this is a duplicate of https://github.com/aws/aws-sdk-ruby/issues/1351, which was already added as a feature request.. @cjyclaire I've noticed that https://github.com/aws/aws-sdk-ruby/pull/1591 has brought back streaming support via blocks to Aws::S3::Client#get_object, but not to Aws::S3::Object#get (which I'm using in Shrine). If we check the Aws::S3::Object#get source code, we can see that no block is forwarded to Aws::S3::Client#get_object (and that was the case in v2). https://github.com/aws/aws-sdk-ruby/blob/83ab2ee52b434ef89cad0e0a3caf9c94fcf63e89/gems/aws-sdk-s3/lib/aws-sdk-s3/object.rb#L557-L564. @cjyclaire Awesome, thank you! Shrine tests are passing now \ud83d\udc4c. This is not yet ready for review, there is a complication with TruncatedBodyError.. Ready for review.\nI changed the code to truncate the response target only in case of TruncatedBodyError, which is raised when the number of bytes received doesn't match Content-Length. In that case we don't know which bytes are missing, so if we're streaming we have to raise an error in that case (as is the current behaviour).\nThis PR also makes the assumption that the response target will always respond to #size, so I needed to add it to IODecrypter. This then means that IO objects like Ruby pipes (result of IO.pipe) can't be used as the :response_target anymore, because they don't respond to #size. Let me know if this assumption is not something that we would want, and I can revert to tracking the bytes written in a separate context variable, as I did initially.. Note that this addresses the feature request discussed in https://github.com/aws/aws-sdk-ruby/issues/1535 (that is, it will be fully addressed once Range header is added on retries).. By submitting this pull request, I confirm that my contribution is made under the terms of the Apache 2.0 license.. By submitting this pull request, I confirm that my contribution is made under the terms of the Apache 2.0 license.. Thanks for a quick answer. My use case is pretty standard, the browser makes a request to the server, which generated presign parameters and returns it to the browser, and the browser uses them to upload the file.\nNow, I would like for the browser to be able to include its generated MD5 checksum in the direct upload request, and have S3 verify that checksum after all data is sent. I assumed this checksum needs to be sent already in the \"presign request\" to the server, and that the presign parameters will then include the received checksum along with the correct signature. I thought that if the browser just added the Content-MD5 request header in the upload request, that it will fail because that header wasn't explicitly \"whitelisted\" when the presign was being generated.\nOn the client side I'm using Uppy to upload to S3, so I cannot use the JavaScript SDK.. Ok, here are my findings:\nWhen generating #presigned_url, one needs to include :content_md5 option if they want to later include the Content-MD5 header in the PUT upload request.\nWhen uploading to S3 using #presigned_post, the Content-MD5 header is ignored. This means that, if one wants to use checksums, they need to use #presigned_url. \nSo, I guess I figured out the answer to my original question: #presigned_url(:put) has more features, but #presigned_post exists probably for cases when you need to submit the upload request via an HTML form (which only supports the POST verb).. The test failure on JRuby is unrelated to this change, so unless there are any other objections this can be merged.\nBy submitting this pull request, I confirm that my contribution is made under the terms of the Apache 2.0 license.. It seems that in JRuby, OpenSSL::Cipher#update doesn't accept the buffer argument \u2013 https://github.com/jruby/jruby/issues/5242. I've tested that it's still behaviour in JRuby 9.2.0.0.. Thanks for the quick fix! Ok, the additional option makes sense to me for preserving backwards compatibility.. I noticed that method, but the problem is that it doesn't accept additional parameters. This means that if I'm using server side encryption, I cannot use it because I'm not able to pass the :sse_* parameters to the underlying #head_object call (so the request would fail). That's why I cannot use it.\n```rb\nobject.put(\n  body:                   \"content\",\n  sse_customer_algorithm: \"AES256\",\n  sse_customer_key:       key,\n  sse_customer_key_md5:   Digest::MD5.base64digest(key),\n)\nobject.data # ~> Aws::S3::Errors::BadRequest\n```\nAlso, it's not ideal that this is something that lazily calls #head_object only once. I'd like to use the Object#head call that always calls #head_object and doesn't mutate the underlying Aws::S3::Object. Previously I was relying on the mutation of the Aws::S3::Object, but my assumptions in how it works turned out to be wrong (I thought there weren't any lazy #head_object calls being made), so I would prefer to avoid it now and be explicit about the API calls I'm making to S3.\nWhat I wish is for this:\n```rb\nclass Aws::S3\n  class Object\n# ...\n\ndef load\n  resp = @client.head_object(\n    bucket: @bucket_name,\n    key: @key\n  )\n  @data = resp.data\n  self\nend\n\n# ...\n\nend\nend\n```\nto be refactored into this:\n```rb\nclass Aws::S3\n  class Object\n# ...\n\ndef load\n  @data = head\n  self\nend\n\ndef head(options = {})\n  options = options.merge(\n    bucket: @bucket_name,\n    key: @key\n  )\n  resp = @client.head_object(options)\n  resp.data\nend\n\n# ...\n\nend\nend\n```\nWhat do you think?. It would also be useful if Object#load accepted additional options for #head_object.. @kreynolds aws-sdk-s3 requires \"IO-like\" parameters to respond to #read, #rewind, and #size: https://github.com/aws/aws-sdk-ruby/blob/7e38bd1c0dbbe70da8b088160b0aee81b6532b9f/gems/aws-sdk-core/lib/aws-sdk-core/param_validator.rb#L160-L164\nRuby pipes don't respond to #size, that's why this check fails.\nNote that aws-sdk-core is probably correct to require #size, because otherwise it wouldn't be able to determine the Content-Length request header on upload, without first streaming the pipe content to disk (and AWS S3 doesn't support chunked requests). . I would also love for #delete_object and #head_object  to be added to Aws::S3::Encryption::Client, as it would make it easier to integrate into Shrine \u2013 https://github.com/shrinerb/shrine/issues/348. I think here it would be much better to open both the chunks and the result files in binary mode instead of UTF-8, to copy the bytes exactly as they are, because this looks like it could cause problems with other encodings.\nAlso, currently the chunks are read in memory whole before they are appended (~5MB each), but you can avoid that by reading and writing in smaller chunks, which you can automatize with IO.copy_stream:\nrb\nFile.open(@path, \"wb\") do |result_file|\n  sort_files(fileparts).each do |part_path|\n    # automatically opens the source in binary mode and copies it in chunks\n    IO.copy_stream(part_path, result_file)\n  end\nend. Here you can use the :response_target option with a path, which will automatically download the file in chunks, and also correctly uses the binary format.\nrb\n@client.get_object(\n  :bucket => @bucket,\n  :key => @key,\n  param.to_sym => chunk,\n  :response_target => file\n). What does the :part_number here do exactly? From the API docs:\n\nPart number of the object being read. This is a positive integer between 1 and 10,000. Effectively performs a \\'ranged\\' HEAD request for the part specified. Useful querying about the size of the part and the number of parts in this object.\n\nWouldn't this mean that the content_length in this response would return only length of that one part? I mean, probably not, but just wanted to check \ud83d\ude03 . Good catch on setting Thread#abort_on_exception, I think this ObjectMultipartCopier and MultipartFileUploader should also be updated to set Thread#abort_on_exception (otherwise the thread won't propagate the error, and nothing will be cleaned up).\nBtw, you probably need to set Thread#abort_on_exception before the exception is raised (at the top of the thread definition), otherwise that part of code will never get executed in case an exception is raised. While here, instead of using a thread local variable, you could also use Thread.current to access the current thread:\n```rb\nthreads << Thread.new do\n  Thread.current.abort_on_exception = true\nbegin\n    # execute\n  rescue\n    # clean-up\n  end\nend\n``.Array#push` returns the modified receiver, so this could be just\nrb\nparts = batches.inject([]) {|a, batch| a.push(*batch.values) }\nEven better, you could use Array#flat_map:\nrb\nparts = batches.flat_map(&:values). Only because the MultipartObjectCopier uses  the name PartQueue. I reverted it back to PartList.. Imagine we're downloading an S3 object to a file. Seahorse will append each chunk to the file object as it is being downloaded. However, if a timeout error occurs, currently Seahorse will truncate the incomplete file object, retry the request, and start writing to the file from the beginning. This is not efficient, because there is nothing wrong with what we've already downloaded.\nThis change detects when the request has been retried and remembers how many bytes have already been written, and when it starts downloading again, instead of truncating the file and writing from the beginning, it will simply skip the data that was already written and start sending only new chunks. As a side effect, this allows streaming downloads (where block is provided as a target) to be retried in case of timeouts, where previously they were not retried, as a block is not \"truncatable\".\nIdeally when we retry the request, we should make a Range request to ask S3 only for the remainder of the data that we haven't managed to download. I plan to make that change next, but I thought it's good to have this safety net for when that particular AWS endpoint doesn't support Range requests (since Seahorse is used throughout all the AWS services, if I understood correctly).. ",
    "joraff": "Came across this today trying to use another Ceph installation. Pretty sure this is because Ceph versions < Jewel only supports aws2 signatures. DreamObjects is Hammer at the time of my posting.\n. FYI, Dreamhost just upgraded DreamObjects to the Jewel release. I switched to using this gem over fog on one app and so far so good. I'll report back if I discover any issues.\n. ",
    "atul-shimpi": "Thanks for quick reply.\n. ",
    "bosskovic": "I noticed one thing while I was extracting the relevant code into a blank rails app, the class of the responses is now Seahorse::Client::Response instead of Aws::PageableResponse, and in the code I am checking the class of response in order to determine the success of the request.\nCan you please tell me if the response class has changed for all responses? In other words, can I just substitute the old class with a new wherever I use it (it's not just logging, but also tagging, managing iam and cloudfront distros, etc)?\nHere's a snippet how I am doing it:\n``` ruby\nresponse = client.some_request({ ... })\nsuccess = response.is_a?(Aws::PageableResponse)\nlog/report depending on success\n```\n. ok, after some playing around, it seems that it would be better to do the check like this:\nsuccess = response.error.nil?\nand this works for both versions. I will do some more testing and close the issue if it all turns ok.\n. It's all fine now. Thank you for your assistance :-)\n. ",
    "ErikDahlinghaus": "Yes, I did some grepping of this repo and didn't find that regex pattern anywhere, so I kind of thought that it was the AWS API returning this. But I couldn't be sure. Thanks.\n. http://docs.aws.amazon.com/sdkforruby/api/Aws/EC2/Resource.html#create_instances-instance_method\nPassing an options hash that includes {iam_instance_profile: {name: \"my-instance-profile\"}} or {iam_instance_profile: {arn: \"arn:aws:iam::12194483xxxx:instance-profile/my-instance-profile\"}}\nNeither one works.\n. Alright I'll craft up an example and provide that information here in a bit.\n. Well, I've written a test and it seems to work. It now seems to be a problem with chef-provisioning-aws, and I've taken it back up with them. For posterity, here is my test.\n```\nrequire 'aws-sdk'\nclient = Aws::EC2::Client.new(region: 'us-east-1', http_wire_trace: true)\nresp = client.run_instances({\n  # dry_run: true,\n  image_id: \"ami-61bbf104\", # required\n  min_count: 1, # required\n  max_count: 1, # required\n  key_name: \"my_keypair\",\n  security_group_ids: [\"sg-21d3xxxx\"],\n  instance_type: \"m4.large\",\n  subnet_id: \"subnet-08c8xxxx\",\n  iam_instance_profile: {\n    # arn: \"arn:aws:iam::12194483xxxx:instance-profile/my-instance-profile\",\n    name: \"my-instance-profile\",\n  },\n})\n```\nOutput is happy, creates instance with the proper instance profile. Thanks for your help.\n. https://github.com/chef/chef-provisioning-aws/issues/413\n. Will do. Thanks again for your help.\n. ",
    "gnowxilef": "just to add, i realized i meant to say ~/.aws/config, because i believe that is what the aws cli reads from as well.\n. ",
    "fuzzyami": "just FYI: I got the same error message from AWS last night (on my Sensu client). sorted itself out in a minute, but still weird.\n. ",
    "jtopper": "Ok, I was holding this wrong.  The hash keys under the stub responses are all expected to be symbols, not strings.\n. I hit this bug today too, but can confirm that passing the resulting record set object from #list_resource_record_sets into the deletion call resolves the problem.\n. ",
    "arthousecoop": "This is the line of code its calling to: \nsearch('*', where: where_hash, page: params[:page], per_page: per_page, order: order_order)\n. ",
    "jsgarvin": "Thanks Alex,\nHowever, I'm not seeing in the documentation how to pass in the region and lambda function name, both required fields if I build a lambda integration through the browser UI, so it seems I should need them here, too.\nIf I call...\napi_client.put_integration(\n  rest_api_id: rest_api.id,\n  resource_id: resource.id,\n  http_method: 'GET',\n  type: 'MOCK'\n)\nthat works to create a MOCK integration.  My (probably wrong) interpretation of your response above is to change type: 'MOCK' to type: 'AWS') , however, just doing that results in a Enumeration value for HttpMethod must be non-empty (Aws::APIGateway::Errors::BadRequestException) error, but it looks like I am sending http_method: 'GET', so....  I must be overlooking something.\n. ",
    "abubics": "Hopefully not too stale to help ;)\nI've gotten Lambda integration working today with the AWS type, using this forum post for hints: Does aws-cli support creating methods calling Lambda?\nSample:\n``` ruby\ndef lambda_arn_uri\n  service = 'lambda'\n  acc = '12xxxxxxxxxx'\n  function_name = 'awesome-lambda-function'\n  lambda_arn = \"arn:aws:lambda:#{REGION}:#{acc}:function:#{function_name}\" # can copy this from Lambda console\n  path_or_action = \"path/2015-03-31/functions/#{lambda_arn}\"\n  service_api = 'invocations'\n  \"arn:aws:apigateway:#{REGION}:#{service}:#{path_or_action}/#{service_api}\"\nend\nmethod = 'GET'\napi_gateway.put_integration(\n  rest_api_id: rest_api.id,\n  resource_id: resource.id,\n  http_method: method,\n  type: 'AWS',\n  integration_http_method: method,\n  uri: lambda_arn_uri\n)\n```\nNeeds templates, etc, but should get you started. It'd be cool if the API supported it in a more friendly way, but that isn't a Ruby SDK issue.\n. My current sample is a bit big and noisy. I'll cut it down and send logs through hopefully today or tomorrow :)\n. Sorry for the delay :) \napigateway_test.rb\n``` ruby\nrequire 'aws-sdk'\nclass ApiGatewayDomain\nREGION='ap-northeast-1'\ndef api_gateway\n    @apigateway ||= Aws::APIGateway::Client.new(http_wire_trace: true, region: REGION)\n  end\ndef rest_api_name\n    \"apigateway_test\"\n  end\ndef fetch_candidate_rest_apis\n    api_gateway.get_rest_apis.items.select{ |it| it.name == rest_api_name }\n  end\ndef rest_api\n    @resp_apis ||= fetch_candidate_rest_apis[0]\n  end\ndef gateway_exists?\n    rest_api != nil\n  end\ndef create_gateway\n    api_gateway.create_rest_api name: rest_api_name\n  end\ndef fetch_resources rest_api_id\n    api_gateway.get_resources(rest_api_id: rest_api_id).items\n  end\ndef create_resources\n    puts 'create_resources'\n    base_id = fetch_resources(rest_api.id).select{ |it| it.path == '/' }[0].id\n    api_resources.each do |k, v|\n      puts \"- creating: /#{k}\"\n      api_gateway.create_resource(\n        rest_api_id: rest_api.id,\n        parent_id: base_id,\n        path_part: k\n      )\n    end\n  end\ndef account_id\n    Aws::IAM::Client.new.get_user[:user][:arn].split(':')[4]\n  end\ndef lambda_arn_uri\n    function_name = 'dev-dev-foobar-LambdaFun-1LZOR2U233EO5'\n    lambda_arn = \"arn:aws:lambda:#{REGION}:#{account_id}:function:#{function_name}\"\npath_or_action = \"path/2015-03-31/functions/#{lambda_arn}\"\nservice = 'lambda'\nservice_api = 'invocations'\n\"arn:aws:apigateway:#{REGION}:#{service}:#{path_or_action}/#{service_api}\"\n\nend\ndef create_methods\n    puts 'create_methods'\n    resources = fetch_resources rest_api.id\n    api_resources.each do |k, v|\n      resource = resources.select{ |it| it.path_part == k.to_s }[0]\n      v.each do |method, config|\n        puts \"- #{method} /#{k}\"\n        api_gateway.put_method(\n          rest_api_id: rest_api.id,\n          resource_id: resource.id,\n          http_method: method,\n          authorization_type: 'NONE',\n          api_key_required: true,\n          request_parameters: config[:method_params]\n        )\n        api_gateway.put_method_response(\n          rest_api_id: rest_api.id,\n          resource_id: resource.id,\n          http_method: method,\n          status_code: '200'\n        )\n        api_gateway.put_integration(\n          rest_api_id: rest_api.id,\n          resource_id: resource.id,\n          http_method: method,\n          type: 'AWS',\n          integration_http_method: 'POST',\n          uri: lambda_arn_uri,\n          request_templates: config[:templates]\n        )\n        api_gateway.put_integration_response(\n          rest_api_id: rest_api.id,\n          resource_id: resource.id,\n          http_method: method,\n          status_code: '200',\n          #selection_pattern: '' # XXX Adding this fixes the \"Invalid request input\" error\n        )\n      end\n    end\n  end\ndef api_resources\n    def template action, more_params = nil\n      more = ''\n      if more_params\n        more = \", #{more_params}\"\n      end\n%Q(#set($inputRoot = $input.path('$'))\n{ \"action\": \"#{action}\"#{more} })\n    end\n{\n  foo: { GET: {\n    method_params: {},\n    templates: { 'application/json': template('reviews') }\n  } },\n  bar: { GET: {\n    method_params: { 'method.request.querystring.id': false },\n    templates: { 'application/json':\n      template('articles', %q(\"foobar_id\": \"$input.params('id')\"))\n    }\n  } }\n}\n\nend\nend\napi_gateway = ApiGatewayDomain.new\napi_gateway.create_gateway\napi_gateway.create_resources\napi_gateway.create_methods\n```\nhttp_wire_trace:\nopening connection to apigateway.ap-northeast-1.amazonaws.com:443...\nopened\nstarting SSL for apigateway.ap-northeast-1.amazonaws.com:443...\nSSL established\n<- \"POST /restapis HTTP/1.1\\r\\nContent-Type: \\r\\nAccept-Encoding: \\r\\nUser-Agent: aws-sdk-ruby2/2.1.36 ruby/2.2.2 x86_64-darwin15\\r\\nAccept: application/json\\r\\nX-Amz-Date: 20160202T232542Z\\r\\nHost: apigateway.ap-northeast-1.amazonaws.com\\r\\nX-Amz-Content-Sha256: 53ce6911ff07bd861c21adff11600a010d947a17a43ee2f245aad979fe8eb6ef\\r\\nAuthorization: AWS4-HMAC-SHA256 Credential=____J34A____4EMY____/20160202/ap-northeast-1/apigateway/aws4_request, SignedHeaders=accept;host;user-agent;x-amz-content-sha256;x-amz-date, Signature=619e8a281f9a383e8985073cbd81f51607826d16a04995246a9b9c90424c898c\\r\\nContent-Length: 26\\r\\n\\r\\n\"\n-> \"HTTP/1.1 201 Created\\r\\n\"\n-> \"x-amzn-RequestId: 47f5f7b4-ca04-11e5-8cdf-0bda7028ac65\\r\\n\"\n-> \"Content-Type: application/json\\r\\n\"\n-> \"Content-Length: 70\\r\\n\"\n-> \"Date: Tue, 02 Feb 2016 23:25:44 GMT\\r\\n\"\n-> \"\\r\\n\"\nreading 70 bytes...\n-> \"{\\\"createdDate\\\":1454455544,\\\"id\\\":\\\"hf1lnexvv7\\\",\\\"name\\\":\\\"apigateway_test\\\"}\\n\"\nread 70 bytes\nConn keep-alive\ncreate_resources\nopening connection to apigateway.ap-northeast-1.amazonaws.com:443...\nopened\nstarting SSL for apigateway.ap-northeast-1.amazonaws.com:443...\nSSL established\n<- \"GET /restapis HTTP/1.1\\r\\nContent-Type: \\r\\nAccept-Encoding: \\r\\nUser-Agent: aws-sdk-ruby2/2.1.36 ruby/2.2.2 x86_64-darwin15\\r\\nAccept: application/json\\r\\nX-Amz-Date: 20160202T232544Z\\r\\nHost: apigateway.ap-northeast-1.amazonaws.com\\r\\nX-Amz-Content-Sha256: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\\r\\nAuthorization: AWS4-HMAC-SHA256 Credential=____J34A____4EMY____/20160202/ap-northeast-1/apigateway/aws4_request, SignedHeaders=accept;host;user-agent;x-amz-content-sha256;x-amz-date, Signature=df99ffc91c9d7587820d18561215e63ff79b48e70f1db80bbdcfcfdd74e7a8a4\\r\\nContent-Length: 0\\r\\n\\r\\n\"\n-> \"HTTP/1.1 200 OK\\r\\n\"\n-> \"x-amzn-RequestId: 4883919d-ca04-11e5-927f-973e573b07ad\\r\\n\"\n-> \"Content-Type: application/json\\r\\n\"\n-> \"Content-Length: 223\\r\\n\"\n-> \"Date: Tue, 02 Feb 2016 23:25:44 GMT\\r\\n\"\n-> \"\\r\\n\"\nreading 223 bytes...\n-> \"{\\\"item\\\":[{\\\"createdDate\\\":1449694605,\\\"id\\\":\\\"93vsgqazp1\\\",\\\"name\\\":\\\"api-test\\\"},{\\\"createdDate\\\":1453952132,\\\"id\\\":\\\"aqiqorose6\\\",\\\"name\\\":\\\"dev-__di____\\\"},{\\\"createdDate\\\":1454455544,\\\"id\\\":\\\"hf1lnexvv7\\\",\\\"name\\\":\\\"apigateway_test\\\"}]}\\n\"\nread 223 bytes\nConn keep-alive\nopening connection to apigateway.ap-northeast-1.amazonaws.com:443...\nopened\nstarting SSL for apigateway.ap-northeast-1.amazonaws.com:443...\nSSL established\n<- \"GET /restapis/hf1lnexvv7/resources HTTP/1.1\\r\\nContent-Type: \\r\\nAccept-Encoding: \\r\\nUser-Agent: aws-sdk-ruby2/2.1.36 ruby/2.2.2 x86_64-darwin15\\r\\nAccept: application/json\\r\\nX-Amz-Date: 20160202T232544Z\\r\\nHost: apigateway.ap-northeast-1.amazonaws.com\\r\\nX-Amz-Content-Sha256: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\\r\\nAuthorization: AWS4-HMAC-SHA256 Credential=____J34A____4EMY____/20160202/ap-northeast-1/apigateway/aws4_request, SignedHeaders=accept;host;user-agent;x-amz-content-sha256;x-amz-date, Signature=1e9fc05daa61430c46d770a466fd1a242e09948b6708c214dd0bc7f224853129\\r\\nContent-Length: 0\\r\\n\\r\\n\"\n-> \"HTTP/1.1 200 OK\\r\\n\"\n-> \"x-amzn-RequestId: 49082aee-ca04-11e5-927f-973e573b07ad\\r\\n\"\n-> \"Content-Type: application/json\\r\\n\"\n-> \"Content-Length: 42\\r\\n\"\n-> \"Date: Tue, 02 Feb 2016 23:25:45 GMT\\r\\n\"\n-> \"\\r\\n\"\nreading 42 bytes...\n-> \"{\\\"item\\\":[{\\\"id\\\":\\\"6a64f4pjb3\\\",\\\"path\\\":\\\"/\\\"}]}\\n\"\nread 42 bytes\nConn keep-alive\n- creating: /foo\nopening connection to apigateway.ap-northeast-1.amazonaws.com:443...\nopened\nstarting SSL for apigateway.ap-northeast-1.amazonaws.com:443...\nSSL established\n<- \"POST /restapis/hf1lnexvv7/resources/6a64f4pjb3 HTTP/1.1\\r\\nContent-Type: \\r\\nAccept-Encoding: \\r\\nUser-Agent: aws-sdk-ruby2/2.1.36 ruby/2.2.2 x86_64-darwin15\\r\\nAccept: application/json\\r\\nX-Amz-Date: 20160202T232545Z\\r\\nHost: apigateway.ap-northeast-1.amazonaws.com\\r\\nX-Amz-Content-Sha256: 14892ee034de7d04c1b5b4380a0fc6530f2c316d9ae720822583cf907ba6beb8\\r\\nAuthorization: AWS4-HMAC-SHA256 Credential=____J34A____4EMY____/20160202/ap-northeast-1/apigateway/aws4_request, SignedHeaders=accept;host;user-agent;x-amz-content-sha256;x-amz-date, Signature=e46e07bd9c96bf8d58d6bc6f756e5926a99a7c52ea1c698fbf9967a66b531aa1\\r\\nContent-Length: 18\\r\\n\\r\\n\"\n-> \"HTTP/1.1 201 Created\\r\\n\"\n-> \"x-amzn-RequestId: 499e77c1-ca04-11e5-ba1d-d5bc288ba023\\r\\n\"\n-> \"Content-Type: application/json\\r\\n\"\n-> \"Content-Length: 71\\r\\n\"\n-> \"Date: Tue, 02 Feb 2016 23:25:46 GMT\\r\\n\"\n-> \"\\r\\n\"\nreading 71 bytes...\n-> \"{\\\"id\\\":\\\"6xy359\\\",\\\"parentId\\\":\\\"6a64f4pjb3\\\",\\\"path\\\":\\\"/foo\\\",\\\"pathPart\\\":\\\"foo\\\"}\\n\"\nread 71 bytes\nConn keep-alive\n- creating: /bar\nopening connection to apigateway.ap-northeast-1.amazonaws.com:443...\nopened\nstarting SSL for apigateway.ap-northeast-1.amazonaws.com:443...\nSSL established\n<- \"POST /restapis/hf1lnexvv7/resources/6a64f4pjb3 HTTP/1.1\\r\\nContent-Type: \\r\\nAccept-Encoding: \\r\\nUser-Agent: aws-sdk-ruby2/2.1.36 ruby/2.2.2 x86_64-darwin15\\r\\nAccept: application/json\\r\\nX-Amz-Date: 20160202T232546Z\\r\\nHost: apigateway.ap-northeast-1.amazonaws.com\\r\\nX-Amz-Content-Sha256: 9c53f33a539ba1f4214e3d8fe6b086b87715b95f37de3ab3f9e09f02cceff934\\r\\nAuthorization: AWS4-HMAC-SHA256 Credential=____J34A____4EMY____/20160202/ap-northeast-1/apigateway/aws4_request, SignedHeaders=accept;host;user-agent;x-amz-content-sha256;x-amz-date, Signature=a11c66f340d27acb68a76a1981663b06dfcb5324849a736edbea7e67fb3ab2bd\\r\\nContent-Length: 18\\r\\n\\r\\n\"\n-> \"HTTP/1.1 201 Created\\r\\n\"\n-> \"x-amzn-RequestId: 4a7af4e7-ca04-11e5-8cdf-0bda7028ac65\\r\\n\"\n-> \"Content-Type: application/json\\r\\n\"\n-> \"Content-Length: 71\\r\\n\"\n-> \"Date: Tue, 02 Feb 2016 23:25:47 GMT\\r\\n\"\n-> \"\\r\\n\"\nreading 71 bytes...\n-> \"{\\\"id\\\":\\\"0t241d\\\",\\\"parentId\\\":\\\"6a64f4pjb3\\\",\\\"path\\\":\\\"/bar\\\",\\\"pathPart\\\":\\\"bar\\\"}\\n\"\nread 71 bytes\nConn keep-alive\ncreate_methods\nopening connection to apigateway.ap-northeast-1.amazonaws.com:443...\nopened\nstarting SSL for apigateway.ap-northeast-1.amazonaws.com:443...\nSSL established\n<- \"GET /restapis/hf1lnexvv7/resources HTTP/1.1\\r\\nContent-Type: \\r\\nAccept-Encoding: \\r\\nUser-Agent: aws-sdk-ruby2/2.1.36 ruby/2.2.2 x86_64-darwin15\\r\\nAccept: application/json\\r\\nX-Amz-Date: 20160202T232548Z\\r\\nHost: apigateway.ap-northeast-1.amazonaws.com\\r\\nX-Amz-Content-Sha256: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\\r\\nAuthorization: AWS4-HMAC-SHA256 Credential=____J34A____4EMY____/20160202/ap-northeast-1/apigateway/aws4_request, SignedHeaders=accept;host;user-agent;x-amz-content-sha256;x-amz-date, Signature=5333fa9c94c4d016d9f372a06435f6ec2ca84a762c770f2e88334b4bc2ec1279\\r\\nContent-Length: 0\\r\\n\\r\\n\"\n-> \"HTTP/1.1 200 OK\\r\\n\"\n-> \"x-amzn-RequestId: 4b13d988-ca04-11e5-8cdf-0bda7028ac65\\r\\n\"\n-> \"Content-Type: application/json\\r\\n\"\n-> \"Content-Length: 184\\r\\n\"\n-> \"Date: Tue, 02 Feb 2016 23:25:49 GMT\\r\\n\"\n-> \"\\r\\n\"\nreading 184 bytes...\n-> \"{\\\"item\\\":[{\\\"id\\\":\\\"0t241d\\\",\\\"parentId\\\":\\\"6a64f4pjb3\\\",\\\"path\\\":\\\"/bar\\\",\\\"pathPart\\\":\\\"bar\\\"},{\\\"id\\\":\\\"6a64f4pjb3\\\",\\\"path\\\":\\\"/\\\"},{\\\"id\\\":\\\"6xy359\\\",\\\"parentId\\\":\\\"6a64f4pjb3\\\",\\\"path\\\":\\\"/foo\\\",\\\"pathPart\\\":\\\"foo\\\"}]}\\n\"\nread 184 bytes\nConn keep-alive\n- GET /foo\nopening connection to apigateway.ap-northeast-1.amazonaws.com:443...\nopened\nstarting SSL for apigateway.ap-northeast-1.amazonaws.com:443...\nSSL established\n<- \"PUT /restapis/hf1lnexvv7/resources/6xy359/methods/GET HTTP/1.1\\r\\nContent-Type: \\r\\nAccept-Encoding: \\r\\nUser-Agent: aws-sdk-ruby2/2.1.36 ruby/2.2.2 x86_64-darwin15\\r\\nAccept: application/json\\r\\nX-Amz-Date: 20160202T232549Z\\r\\nHost: apigateway.ap-northeast-1.amazonaws.com\\r\\nX-Amz-Content-Sha256: 259f60ae2a005e13e7a42d211aedd0b2f8a2b644d1bae9ca61b6ee9df43e947b\\r\\nAuthorization: AWS4-HMAC-SHA256 Credential=____J34A____4EMY____/20160202/ap-northeast-1/apigateway/aws4_request, SignedHeaders=accept;host;user-agent;x-amz-content-sha256;x-amz-date, Signature=1e7a38ba785846fd5b3f564943aac1073be2af469d61efc3596e0cd8679455ef\\r\\nContent-Length: 73\\r\\n\\r\\n\"\n-> \"HTTP/1.1 201 Created\\r\\n\"\n-> \"x-amzn-RequestId: 4bc1ccc9-ca04-11e5-8cdf-0bda7028ac65\\r\\n\"\n-> \"Content-Type: application/json\\r\\n\"\n-> \"Content-Length: 93\\r\\n\"\n-> \"Date: Tue, 02 Feb 2016 23:25:50 GMT\\r\\n\"\n-> \"\\r\\n\"\nreading 93 bytes...\n-> \"{\\\"apiKeyRequired\\\":true,\\\"authorizationType\\\":\\\"NONE\\\",\\\"httpMethod\\\":\\\"GET\\\",\\\"requestParameters\\\":{}}\\n\"\nread 93 bytes\nConn keep-alive\nopening connection to apigateway.ap-northeast-1.amazonaws.com:443...\nopened\nstarting SSL for apigateway.ap-northeast-1.amazonaws.com:443...\nSSL established\n<- \"PUT /restapis/hf1lnexvv7/resources/6xy359/methods/GET/responses/200 HTTP/1.1\\r\\nContent-Type: \\r\\nAccept-Encoding: \\r\\nUser-Agent: aws-sdk-ruby2/2.1.36 ruby/2.2.2 x86_64-darwin15\\r\\nAccept: application/json\\r\\nX-Amz-Date: 20160202T232550Z\\r\\nHost: apigateway.ap-northeast-1.amazonaws.com\\r\\nX-Amz-Content-Sha256: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\\r\\nAuthorization: AWS4-HMAC-SHA256 Credential=____J34A____4EMY____/20160202/ap-northeast-1/apigateway/aws4_request, SignedHeaders=accept;host;user-agent;x-amz-content-sha256;x-amz-date, Signature=57f1bf92c1b57f6f95627cb489efabd6d33b977ebc076d883a9378b95d02fab2\\r\\nContent-Length: 0\\r\\n\\r\\n\"\n-> \"HTTP/1.1 201 Created\\r\\n\"\n-> \"x-amzn-RequestId: 4c6c8b9f-ca04-11e5-927f-973e573b07ad\\r\\n\"\n-> \"Content-Type: application/json\\r\\n\"\n-> \"Content-Length: 21\\r\\n\"\n-> \"Date: Tue, 02 Feb 2016 23:25:51 GMT\\r\\n\"\n-> \"\\r\\n\"\nreading 21 bytes...\n-> \"{\\\"statusCode\\\":\\\"200\\\"}\\n\"\nread 21 bytes\nConn keep-alive\nopening connection to apigateway.ap-northeast-1.amazonaws.com:443...\nopened\nstarting SSL for apigateway.ap-northeast-1.amazonaws.com:443...\nSSL established\n<- \"PUT /restapis/hf1lnexvv7/resources/6xy359/methods/GET/integration HTTP/1.1\\r\\nContent-Type: \\r\\nAccept-Encoding: \\r\\nUser-Agent: aws-sdk-ruby2/2.1.36 ruby/2.2.2 x86_64-darwin15\\r\\nAccept: application/json\\r\\nX-Amz-Date: 20160202T232553Z\\r\\nHost: apigateway.ap-northeast-1.amazonaws.com\\r\\nX-Amz-Content-Sha256: 95ee98e4a264df42e22d580b0a7ea2111efbc4b77860bfe27c7496e8a8556b0b\\r\\nAuthorization: AWS4-HMAC-SHA256 Credential=____J34A____4EMY____/20160202/ap-northeast-1/apigateway/aws4_request, SignedHeaders=accept;host;user-agent;x-amz-content-sha256;x-amz-date, Signature=25c82e7c688005901670ba3181900ff2fd254ef423749835fb9ff9762f698be7\\r\\nContent-Length: 335\\r\\n\\r\\n\"\n-> \"HTTP/1.1 201 Created\\r\\n\"\n-> \"x-amzn-RequestId: 4e175571-ca04-11e5-927f-973e573b07ad\\r\\n\"\n-> \"Content-Type: application/json\\r\\n\"\n-> \"Content-Length: 386\\r\\n\"\n-> \"Date: Tue, 02 Feb 2016 23:25:54 GMT\\r\\n\"\n-> \"\\r\\n\"\nreading 386 bytes...\n-> \"{\\\"cacheKeyParameters\\\":[],\\\"cacheNamespace\\\":\\\"6xy359\\\",\\\"httpMethod\\\":\\\"POST\\\",\\\"requestTemplates\\\":{\\\"application/json\\\":\\\"#set($inputRoot = $input.path('$'))\\\\n{ \\\\\\\"action\\\\\\\": \\\\\\\"reviews\\\\\\\" }\\\"},\\\"type\\\":\\\"AWS\\\",\\\"uri\\\":\\\"arn:aws:apigateway:ap-northeast-1:lambda:path/2015-03-31/functions/arn:aws:lambda:ap-northeast-1:____1907____:function:dev-dev-foobar-LambdaFun-1LZOR2U233EO5/invocations\\\"}\\n\"\nread 386 bytes\nConn keep-alive\nopening connection to apigateway.ap-northeast-1.amazonaws.com:443...\nopened\nstarting SSL for apigateway.ap-northeast-1.amazonaws.com:443...\nSSL established\n<- \"PUT /restapis/hf1lnexvv7/resources/6xy359/methods/GET/integration/responses/200 HTTP/1.1\\r\\nContent-Type: \\r\\nAccept-Encoding: \\r\\nUser-Agent: aws-sdk-ruby2/2.1.36 ruby/2.2.2 x86_64-darwin15\\r\\nAccept: application/json\\r\\nX-Amz-Date: 20160202T232554Z\\r\\nHost: apigateway.ap-northeast-1.amazonaws.com\\r\\nX-Amz-Content-Sha256: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\\r\\nAuthorization: AWS4-HMAC-SHA256 Credential=____J34A____4EMY____/20160202/ap-northeast-1/apigateway/aws4_request, SignedHeaders=accept;host;user-agent;x-amz-content-sha256;x-amz-date, Signature=619bf052c7f35745930fa367543d3340eb5c214e8b9c3f2ad7e3179cad125151\\r\\nContent-Length: 0\\r\\n\\r\\n\"\n-> \"HTTP/1.1 400 Bad Request\\r\\n\"\n-> \"x-amzn-RequestId: 4ed415c3-ca04-11e5-927f-973e573b07ad\\r\\n\"\n-> \"x-amzn-ErrorType: BadRequestException\\r\\n\"\n-> \"Content-Type: application/json\\r\\n\"\n-> \"Content-Length: 36\\r\\n\"\n-> \"Date: Tue, 02 Feb 2016 23:25:55 GMT\\r\\n\"\n-> \"Connection: close\\r\\n\"\n-> \"\\r\\n\"\nreading 36 bytes...\n-> \"{\\\"message\\\":\\\"Invalid request input\\\"}\\n\"\nread 36 bytes\nConn close\n/Users/abubics/.rbenv/versions/2.2.2/lib/ruby/gems/2.2.0/gems/aws-sdk-core-2.1.36/lib/seahorse/client/plugins/raise_response_errors.rb:15:in `call': Invalid request input (Aws::APIGateway::Errors::BadRequestException)\n    from /Users/abubics/.rbenv/versions/2.2.2/lib/ruby/gems/2.2.0/gems/aws-sdk-core-2.1.36/lib/aws-sdk-core/plugins/param_converter.rb:20:in `call'\n    from /Users/abubics/.rbenv/versions/2.2.2/lib/ruby/gems/2.2.0/gems/aws-sdk-core-2.1.36/lib/seahorse/client/plugins/response_target.rb:21:in `call'\n    from /Users/abubics/.rbenv/versions/2.2.2/lib/ruby/gems/2.2.0/gems/aws-sdk-core-2.1.36/lib/seahorse/client/request.rb:70:in `send_request'\n    from /Users/abubics/.rbenv/versions/2.2.2/lib/ruby/gems/2.2.0/gems/aws-sdk-core-2.1.36/lib/seahorse/client/base.rb:207:in `block (2 levels) in define_operation_methods'\n    from apigateway_test.rb:92:in `block (2 levels) in create_methods'\n    from apigateway_test.rb:67:in `each'\n    from apigateway_test.rb:67:in `block in create_methods'\n    from apigateway_test.rb:65:in `each'\n    from apigateway_test.rb:65:in `create_methods'\n    from apigateway_test.rb:133:in `<main>'\n. Cool, thanks. I wonder which will come first: updated docs, or bugfix ;D\n. ",
    "natemacinnes": "This is still very unclear in the guides in 2018.\nI managed to get it to work following @abubics example, thanks! \ud83e\udd1b . Still waiting on an update... to the code base or the docs \ud83d\udd50 . ",
    "abe4tawa8": "Thank you for fixing this issue. :smile:\n. ",
    "danp": "@awood45 anything I can do to help this along?\n. Fixed!\n. Thanks for the tip. client.stub_data could help.\nA downside to that approach is you lose validation on the names of methods you've stubbed (with allow) and the provided args. For example, I could use that to make delete_buckets(bucket_names: ...) or delete_bucket(bucket_name: ...) work but neither of those are valid for real use.\nThere are probably ways to use rspec/etc to help prevent that but takes care.\n. ",
    "laetitiamassa": "Thanks for replying.\nI haven't created new code (I was just creating a staging app - the production app works with v1 also).\nI'm used to aws and stayed with the v1 in order to avoid new required elements.\nI've kept using v1 on purpose : \n my gemlock indicates clearly\n\"aws-sdk (1.66.0)\n      aws-sdk-v1 (= 1.66.0)\n    aws-sdk-v1 (1.66.0)\n      json (~> 1.4)\n      nokogiri (>= 1.4.4)\"\nso I don't get why the expected resources (required in v2 but not in v1) come in my way.\nCould you please explain why it would suddenly consider v2 while I've set up the v1 (as I'm used to)?\nThks\n. ",
    "basex": "is there a way to have a waiters for Aws::Route53::Client?\nThe waiter_names list is empty on the documentation: http://docs.aws.amazon.com/sdkforruby/api/Aws/Route53/Client.html#waiter_names-instance_method\n. ",
    "federicolucca": "Hi,\nThis is the config.\nAws.config.update({\n                        region: 'us-west-2',\n                        credentials: Aws::Credentials.new('XXX', 'XXX')\n    })\n    # S3.new will now use the credentials specified in AWS.config\n    s3 = Aws::S3::Resource.new\n    BUCKET = s3.bucket('xxx')\nthe versions are\naws-sdk (2.2.9)\n  aws-sdk-resources (= 2.2.9)\naws-sdk-core (2.2.9)\n  jmespath (~> 1.0)\naws-sdk-resources (2.2.9)\n  aws-sdk-core (= 2.2.9)\nthanks\nFederico\n. Now i try this.\nFile.open(File.expand_path(\"public/\"+url_cover), 'rb') do |file|\n      obj.put(body: file)\nend\nsame error\n. ",
    "kronos": "Thanks!\n. ",
    "jeffchuber": "For those that find this on google - upgrading to ruby 2.2.4 fixed this problem.\n. ",
    "lsilvs": "Tested and approved.\nCheers,\n. Hi @awood45 \nseems that your commit 'Fix SigV4 Signing Region for Aws::IoTDataPlane' solved the problem but the commit 'Simplify Request Signer Region Assignment' brought it back.\nCan you check it out, pls\n. I'm using the region 'eu-west-1' and getting the error:\nAws::IoTDataPlane::Errors::ForbiddenException (Credential should be scoped to a valid region, not 'us-east-1'. )\nSeems that it's only working for ' us-east-1'\n. ",
    "omargallob": "sorry alex, i just saw this email. \ni was getting invalid region 'iot.eu-west-1', after the fix i was getting invalid region 'us-west-1'. i forked the gem and changed the hard coded us-west-1 to fit my region.\nshould of notified before attempting to make a fix.\nSent from my iPhone\n\nOn 12 Jan 2016, at 22:23, Alex Wood notifications@github.com wrote:\nAny further information on this? I'm very curious about your use case because that regex should not be triggered for any endpoints that contain a region, only \"global\" endpoints.\n\u2014\nReply to this email directly or view it on GitHub.\n. \n",
    "mkrisher": "Sounds good. fwiw, a quick ack on the repo didn't return any matches for 'querystring_param' other than the method when it existed.\n. No problem, I am seeing and plan on tracking down the following as well:\n/var/lib/gems/2.0.0/gems/aws-sdk-resources-2.2.9/lib/aws-sdk-resources/services/s3/object.rb:44: warning: method redefined; discarding old copy_from\n/var/lib/gems/2.0.0/gems/aws-sdk-resources-2.2.9/lib/aws-sdk-resources/operation_methods.rb:18: warning: previous definition of copy_from was here\n/var/lib/gems/2.0.0/gems/aws-sdk-resources-2.2.9/lib/aws-sdk-resources/services/s3/object_summary.rb:11: warning: method redefined; discarding old copy_from\n/var/lib/gems/2.0.0/gems/aws-sdk-resources-2.2.9/lib/aws-sdk-resources/operation_methods.rb:18: warning: previous definition of copy_from was here\n. ",
    "dominikdarnel": "It is helpful, but it always returns false, no matter what the key is. How can I make the #exist? depend on the key parameter? What if I want it to return true based on a stubbed existing object?. ",
    "Kapeli": "Note: RubyDoc and Dash generate the docs by unpacking the gem and using yard over the source. You don't need to include the actual HTML docs with the gem, but instead ensure that running yard over the unpacked gem produces the same docs.\n. ",
    "jlmoody": "Thanks guys!\n. ",
    "radhikamalik": "Actually turns out I was using an older release before the method was added!\n. ",
    "matiaskorhonen": "I tried version 2.1.36 and it seems to work, so the regression seems to have been re-introduced somewhere in the 2.2.x series\u2026\n. For version 2.2.12:\nirb(main):001:0> require \"aws-sdk\"\n=> true\nirb(main):002:0> distribution_id = \"XXXXXXXXXXXXX\"\n=> \"XXXXXXXXXXXXX\"\nirb(main):003:0> cloudfront = Aws::CloudFront::Client.new http_wire_trace: true\n=> #<Aws::CloudFront::Client>\nirb(main):004:0>\nirb(main):005:0* resp = cloudfront.get_distribution_config({\nirb(main):006:2*   id: distribution_id,\nirb(main):007:2* })\nopening connection to cloudfront.amazonaws.com:443...\nopened\nstarting SSL for cloudfront.amazonaws.com:443...\nSSL established\n<- \"GET /2016-01-13/distribution/XXXXXXXXXXXXX/config HTTP/1.1\\r\\nContent-Type: \\r\\nAccept-Encoding: \\r\\nUser-Agent: aws-sdk-ruby2/2.2.12 ruby/2.2.4 x86_64-linux-gnu\\r\\nX-Amz-Date: 20160121T183550Z\\r\\nHost: cloudfront.amazonaws.com\\r\\nX-Amz-Content-Sha256: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\\r\\nAuthorization: AWS4-HMAC-SHA256 Credential=AKIAJSD72GXWQ3HPOSKQ/20160121/us-east-1/cloudfront/aws4_request, SignedHeaders=host;x-amz-content-sha256;x-amz-date, Signature=f32d7f75bc13e0e93f91021bae4eecae40da1aed84dfc18b6b1e8302af91f2a0\\r\\nContent-Length: 0\\r\\nAccept: */*\\r\\n\\r\\n\"\n-> \"HTTP/1.1 200 OK\\r\\n\"\n-> \"x-amzn-RequestId: cbacb642-c06d-11e5-a202-b3f5a6876a03\\r\\n\"\n-> \"ETag: E26F7H715C0SRE\\r\\n\"\n-> \"Content-Type: text/xml\\r\\n\"\n-> \"Content-Length: 2448\\r\\n\"\n-> \"Vary: Accept-Encoding\\r\\n\"\n-> \"Date: Thu, 21 Jan 2016 18:35:50 GMT\\r\\n\"\n-> \"\\r\\n\"\nreading 2448 bytes...\n-> \"\"\n-> \"<?xml version=\\\"1.0\\\"?>\\n<DistributionConfig xmlns=\\\"http://cloudfront.amazonaws.com/doc/2016-01-13/\\\"><CallerReference>1452708700690</CallerReference><Aliases><Quantity>1</Quantity><Items><CNAME>cdn.example.net</CNAME></Items></Aliases><DefaultRootObject>404.html</DefaultRootObject><Origins><Quantity>1</Quantity><Items><Origin><Id>assets.example.com</Id><DomainName>assets.example.net</DomainName><OriginPath></OriginPath><CustomHeaders><Quantity>0</Quantity><Items/></CustomHeaders><CustomOriginConfig><HTTPPort>80</HTTPPort><HTTPSPort>443</HTTPSPort><OriginProtocolPolicy>match-viewer</OriginProtocolPolicy><OriginSslProtocols><Quantity>2</Quantity><Items><SslProtocol>TLSv1.1</SslProtocol><SslProtocol>TLSv1.2</SslProtocol></Items></OriginSslProtocols></CustomOriginConfig></Origin></Items></Origins><DefaultCacheBehavior><TargetOriginId>assets.example.com</TargetOriginId><ForwardedValues><QueryString>false</QueryString><Cookies><Forward>none</Forward></Cookies><Headers><Quantity>4</Quantity><Items><Name>Access-Control-Request-Headers</Name><Name>Access-Control-Request-Method</Name><Name>Origin</Name><Name>Referer</Name></Items></Headers></ForwardedValues><TrustedSigners><Enabled>false</Enabled><Quantity>0</Quantity></TrustedSigners><ViewerProtocolPolicy>allow-all</ViewerProtocolPolicy><MinTTL>0</MinTTL><AllowedMethods><Quantity>2</Quantity><Items><Method>HEAD</Method><Method>GET</Method></Items><CachedMethods><Quantity>2</Quantity><Items><Method>HEAD</Method><Method>GET</Method></Items></CachedMethods></AllowedMethods><SmoothStreaming>false</SmoothStreaming><DefaultTTL>86400</DefaultTTL><MaxTTL>31536000</MaxTTL><Compress>true</Compress></DefaultCacheBehavior><CacheBehaviors><Quantity>0</Quantity></CacheBehaviors><CustomErrorResponses><Quantity>0</Quantity></CustomErrorResponses><Comment></Comment><Logging><Enabled>false</Enabled><IncludeCookies>false</IncludeCookies><Bucket></Bucket><Prefix></Prefix></Logging><PriceClass>PriceClass_All</PriceClass><Enabled>true</Enabled><ViewerCertificate><Certificate>YYYYYYYYYYYYYYYYYYYYY</Certificate><CertificateSource>iam</CertificateSource><SSLSupportMethod>sni-only</SSLSupportMethod><MinimumProtocolVersion>TLSv1</MinimumProtocolVersion><IAMCertificateId>YYYYYYYYYYYYYYYYYYYYY</IAMCertificateId></ViewerCertificate><Restrictions><GeoRestriction><RestrictionType>none</RestrictionType><Quantity>0</Quantity></GeoRestriction></Restrictions><WebACLId></WebACLId></DistributionConfig>\"\nread 2448 bytes\nConn keep-alive\n=> #<struct Aws::CloudFront::Types::GetDistributionConfigResult distribution_config=#<struct Aws::CloudFront::Types::DistributionConfig caller_reference=\"1452708700690\", aliases=#<struct Aws::CloudFront::Types::Aliases quantity=1, items=[\"cdn.example.net\"]>, default_root_object=\"404.html\", origins=#<struct Aws::CloudFront::Types::Origins quantity=1, items=[#<struct Aws::CloudFront::Types::Origin id=\"assets.example.com\", domain_name=\"assets.example.net\", origin_path=\"\", custom_headers=#<struct Aws::CloudFront::Types::CustomHeaders quantity=0, items=[]>, s3_origin_config=nil, custom_origin_config=#<struct Aws::CloudFront::Types::CustomOriginConfig http_port=80, https_port=443, origin_protocol_policy=\"match-viewer\", origin_ssl_protocols=#<struct Aws::CloudFront::Types::OriginSslProtocols quantity=2, items=[\"TLSv1.1\", \"TLSv1.2\"]>>>]>, default_cache_behavior=#<struct Aws::CloudFront::Types::DefaultCacheBehavior target_origin_id=\"assets.example.com\", forwarded_values=#<struct Aws::CloudFront::Types::ForwardedValues query_string=false, cookies=#<struct Aws::CloudFront::Types::CookiePreference forward=\"none\", whitelisted_names=nil>, headers=#<struct Aws::CloudFront::Types::Headers quantity=4, items=[\"Access-Control-Request-Headers\", \"Access-Control-Request-Method\", \"Origin\", \"Referer\"]>>, trusted_signers=#<struct Aws::CloudFront::Types::TrustedSigners enabled=false, quantity=0, items=[]>, viewer_protocol_policy=\"allow-all\", min_ttl=0, allowed_methods=#<struct Aws::CloudFront::Types::AllowedMethods quantity=2, items=[\"HEAD\", \"GET\"], cached_methods=#<struct Aws::CloudFront::Types::CachedMethods quantity=2, items=[\"HEAD\", \"GET\"]>>, smooth_streaming=false, default_ttl=86400, max_ttl=31536000, compress=true>, cache_behaviors=#<struct Aws::CloudFront::Types::CacheBehaviors quantity=0, items=[]>, custom_error_responses=#<struct Aws::CloudFront::Types::CustomErrorResponses quantity=0, items=[]>, comment=\"\", logging=#<struct Aws::CloudFront::Types::LoggingConfig enabled=false, include_cookies=false, bucket=\"\", prefix=\"\">, price_class=\"PriceClass_All\", enabled=true, viewer_certificate=#<struct Aws::CloudFront::Types::ViewerCertificate certificate=\"YYYYYYYYYYYYYYYYYYYYY\", certificate_source=\"iam\", ssl_support_method=\"sni-only\", minimum_protocol_version=\"TLSv1\", iam_certificate_id=\"YYYYYYYYYYYYYYYYYYYYY\", cloud_front_default_certificate=nil>, restrictions=#<struct Aws::CloudFront::Types::Restrictions geo_restriction=#<struct Aws::CloudFront::Types::GeoRestriction restriction_type=\"none\", quantity=0, items=[]>>, web_acl_id=\"\">, etag=\"E26F7H715C0SRE\">\nirb(main):008:0> cloudfront.update_distribution({\nirb(main):009:2*   id: distribution_id,\nirb(main):010:2*   distribution_config: resp.distribution_config,\nirb(main):011:2*   if_match: resp.etag\nirb(main):012:2> })\nopening connection to cloudfront.amazonaws.com:443...\nopened\nstarting SSL for cloudfront.amazonaws.com:443...\nSSL established\n<- \"PUT /2016-01-13/distribution/XXXXXXXXXXXXX/config HTTP/1.1\\r\\nContent-Type: \\r\\nAccept-Encoding: \\r\\nUser-Agent: aws-sdk-ruby2/2.2.12 ruby/2.2.4 x86_64-linux-gnu\\r\\nIf-Match: E26F7H715C0SRE\\r\\nX-Amz-Date: 20160121T183614Z\\r\\nHost: cloudfront.amazonaws.com\\r\\nX-Amz-Content-Sha256: 23b80419cc91fcffeb1c9422a48e8350693a0ba0257bd10b925cc36aa8d9eb0e\\r\\nAuthorization: AWS4-HMAC-SHA256 Credential=AKIAJSD72GXWQ3HPOSKQ/20160121/us-east-1/cloudfront/aws4_request, SignedHeaders=host;x-amz-content-sha256;x-amz-date, Signature=864601b4f94fa93d07e555d7a710eb87737de68474e37fb75333070a3bc565da\\r\\nContent-Length: 3147\\r\\nAccept: */*\\r\\n\\r\\n\"\n-> \"HTTP/1.1 400 Bad Request\\r\\n\"\n-> \"x-amzn-RequestId: da3bd2d6-c06d-11e5-aa12-ff9f7a59ea7a\\r\\n\"\n-> \"Content-Type: text/xml\\r\\n\"\n-> \"Content-Length: 283\\r\\n\"\n-> \"Date: Thu, 21 Jan 2016 18:36:14 GMT\\r\\n\"\n-> \"Connection: close\\r\\n\"\n-> \"\\r\\n\"\nreading 283 bytes...\n-> \"\"\n-> \"<?xml version=\\\"1.0\\\"?>\\n<ErrorResponse xmlns=\\\"http://cloudfront.amazonaws.com/doc/2016-01-13/\\\"><Error><Type>Sender</Type><Code>MalformedInput</Code><Message>Unexpected list element termination</Message></Error><RequestId>da3bd2d6-c06d-11e5-aa12-ff9f7a59ea7a</RequestId></ErrorResponse>\"\nread 283 bytes\nConn close\nAws::CloudFront::Errors::MalformedInput: Unexpected list element termination\n    from /var/lib/gems/2.2.0/gems/aws-sdk-core-2.2.12/lib/seahorse/client/plugins/raise_response_errors.rb:15:in `call'\n    from /var/lib/gems/2.2.0/gems/aws-sdk-core-2.2.12/lib/aws-sdk-core/plugins/param_converter.rb:20:in `call'\n    from /var/lib/gems/2.2.0/gems/aws-sdk-core-2.2.12/lib/seahorse/client/plugins/response_target.rb:21:in `call'\n    from /var/lib/gems/2.2.0/gems/aws-sdk-core-2.2.12/lib/seahorse/client/request.rb:70:in `send_request'\n    from /var/lib/gems/2.2.0/gems/aws-sdk-core-2.2.12/lib/seahorse/client/base.rb:207:in `block (2 levels) in define_operation_methods'\n    from (irb):8\n    from /usr/bin/irb:11:in `<main>'\nirb(main):013:0>\n. For version 2.1.36:\n``\nirb(main):001:0> require \"aws-sdk\"\n=> true\nirb(main):002:0> distribution_id = \"XXXXXXXXXXXXX\" # CF Distribution ID here\n=> \"XXXXXXXXXXXXX\"\nirb(main):003:0> cloudfront = Aws::CloudFront::Client.new http_wire_trace: true\n=> #<Aws::CloudFront::Client>\nirb(main):004:0>\nirb(main):005:0* resp = cloudfront.get_distribution_config({\nirb(main):006:2*   id: distribution_id,\nirb(main):007:2* })\nopening connection to cloudfront.amazonaws.com:443...\nopened\nstarting SSL for cloudfront.amazonaws.com:443...\nSSL established\n<- \"GET /2016-01-13/distribution/XXXXXXXXXXXXX/config HTTP/1.1\\r\\nContent-Type: \\r\\nAccept-Encoding: \\r\\nUser-Agent: aws-sdk-ruby2/2.2.12 ruby/2.2.4 x86_64-linux-gnu\\r\\nX-Amz-Date: 20160121T183550Z\\r\\nHost: cloudfront.amazonaws.com\\r\\nX-Amz-Content-Sha256: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\\r\\nAuthorization: AWS4-HMAC-SHA256 Credential=AKIAJSD72GXWQ3HPOSKQ/20160121/us-east-1/cloudfront/aws4_request, SignedHeaders=host;x-amz-content-sha256;x-amz-date, Signature=f32d7f75bc13e0e93f91021bae4eecae40da1aed84dfc18b6b1e8302af91f2a0\\r\\nContent-Length: 0\\r\\nAccept: */*\\r\\n\\r\\n\"\n-> \"HTTP/1.1 200 OK\\r\\n\"\n-> \"x-amzn-RequestId: cbacb642-c06d-11e5-a202-b3f5a6876a03\\r\\n\"\n-> \"ETag: E26F7H715C0SRE\\r\\n\"\n-> \"Content-Type: text/xml\\r\\n\"\n-> \"Content-Length: 2448\\r\\n\"\n-> \"Vary: Accept-Encoding\\r\\n\"\n-> \"Date: Thu, 21 Jan 2016 18:35:50 GMT\\r\\n\"\n-> \"\\r\\n\"\nreading 2448 bytes...\n-> \"\"\n-> \"<?xml version=\\\"1.0\\\"?>\\n<DistributionConfig xmlns=\\\"http://cloudfront.amazonaws.com/doc/2016-01-13/\\\"><CallerReference>1452708700690</CallerReference><Aliases><Quantity>1</Quantity><Items><CNAME>cdn.example.net</CNAME></Items></Aliases><DefaultRootObject>404.html</DefaultRootObject><Origins><Quantity>1</Quantity><Items><Origin><Id>assets.example.com</Id><DomainName>assets.example.net</DomainName><OriginPath></OriginPath><CustomHeaders><Quantity>0</Quantity><Items/></CustomHeaders><CustomOriginConfig><HTTPPort>80</HTTPPort><HTTPSPort>443</HTTPSPort><OriginProtocolPolicy>match-viewer</OriginProtocolPolicy><OriginSslProtocols><Quantity>2</Quantity><Items><SslProtocol>TLSv1.1</SslProtocol><SslProtocol>TLSv1.2</SslProtocol></Items></OriginSslProtocols></CustomOriginConfig></Origin></Items></Origins><DefaultCacheBehavior><TargetOriginId>assets.example.com</TargetOriginId><ForwardedValues><QueryString>false</QueryString><Cookies><Forward>none</Forward></Cookies><Headers><Quantity>4</Quantity><Items><Name>Access-Control-Request-Headers</Name><Name>Access-Control-Request-Method</Name><Name>Origin</Name><Name>Referer</Name></Items></Headers></ForwardedValues><TrustedSigners><Enabled>false</Enabled><Quantity>0</Quantity></TrustedSigners><ViewerProtocolPolicy>allow-all</ViewerProtocolPolicy><MinTTL>0</MinTTL><AllowedMethods><Quantity>2</Quantity><Items><Method>HEAD</Method><Method>GET</Method></Items><CachedMethods><Quantity>2</Quantity><Items><Method>HEAD</Method><Method>GET</Method></Items></CachedMethods></AllowedMethods><SmoothStreaming>false</SmoothStreaming><DefaultTTL>86400</DefaultTTL><MaxTTL>31536000</MaxTTL><Compress>true</Compress></DefaultCacheBehavior><CacheBehaviors><Quantity>0</Quantity></CacheBehaviors><CustomErrorResponses><Quantity>0</Quantity></CustomErrorResponses><Comment></Comment><Logging><Enabled>false</Enabled><IncludeCookies>false</IncludeCookies><Bucket></Bucket><Prefix></Prefix></Logging><PriceClass>PriceClass_All</PriceClass><Enabled>true</Enabled><ViewerCertificate><Certificate>YYYYYYYYYYYYYYYYYYYYY</Certificate><CertificateSource>iam</CertificateSource><SSLSupportMethod>sni-only</SSLSupportMethod><MinimumProtocolVersion>TLSv1</MinimumProtocolVersion><IAMCertificateId>YYYYYYYYYYYYYYYYYYYYY</IAMCertificateId></ViewerCertificate><Restrictions><GeoRestriction><RestrictionType>none</RestrictionType><Quantity>0</Quantity></GeoRestriction></Restrictions><WebACLId></WebACLId></DistributionConfig>\"\nread 2448 bytes\nConn keep-alive\n=> #<struct Aws::CloudFront::Types::GetDistributionConfigResult distribution_config=#<struct Aws::CloudFront::Types::DistributionConfig caller_reference=\"1452708700690\", aliases=#<struct Aws::CloudFront::Types::Aliases quantity=1, items=[\"cdn.example.net\"]>, default_root_object=\"404.html\", origins=#<struct Aws::CloudFront::Types::Origins quantity=1, items=[#<struct Aws::CloudFront::Types::Origin id=\"assets.example.com\", domain_name=\"assets.example.net\", origin_path=\"\", custom_headers=#<struct Aws::CloudFront::Types::CustomHeaders quantity=0, items=[]>, s3_origin_config=nil, custom_origin_config=#<struct Aws::CloudFront::Types::CustomOriginConfig http_port=80, https_port=443, origin_protocol_policy=\"match-viewer\", origin_ssl_protocols=#<struct Aws::CloudFront::Types::OriginSslProtocols quantity=2, items=[\"TLSv1.1\", \"TLSv1.2\"]>>>]>, default_cache_behavior=#<struct Aws::CloudFront::Types::DefaultCacheBehavior target_origin_id=\"assets.example.com\", forwarded_values=#<struct Aws::CloudFront::Types::ForwardedValues query_string=false, cookies=#<struct Aws::CloudFront::Types::CookiePreference forward=\"none\", whitelisted_names=nil>, headers=#<struct Aws::CloudFront::Types::Headers quantity=4, items=[\"Access-Control-Request-Headers\", \"Access-Control-Request-Method\", \"Origin\", \"Referer\"]>>, trusted_signers=#<struct Aws::CloudFront::Types::TrustedSigners enabled=false, quantity=0, items=[]>, viewer_protocol_policy=\"allow-all\", min_ttl=0, allowed_methods=#<struct Aws::CloudFront::Types::AllowedMethods quantity=2, items=[\"HEAD\", \"GET\"], cached_methods=#<struct Aws::CloudFront::Types::CachedMethods quantity=2, items=[\"HEAD\", \"GET\"]>>, smooth_streaming=false, default_ttl=86400, max_ttl=31536000, compress=true>, cache_behaviors=#<struct Aws::CloudFront::Types::CacheBehaviors quantity=0, items=[]>, custom_error_responses=#<struct Aws::CloudFront::Types::CustomErrorResponses quantity=0, items=[]>, comment=\"\", logging=#<struct Aws::CloudFront::Types::LoggingConfig enabled=false, include_cookies=false, bucket=\"\", prefix=\"\">, price_class=\"PriceClass_All\", enabled=true, viewer_certificate=#<struct Aws::CloudFront::Types::ViewerCertificate certificate=\"YYYYYYYYYYYYYYYYYYYYY\", certificate_source=\"iam\", ssl_support_method=\"sni-only\", minimum_protocol_version=\"TLSv1\", iam_certificate_id=\"YYYYYYYYYYYYYYYYYYYYY\", cloud_front_default_certificate=nil>, restrictions=#<struct Aws::CloudFront::Types::Restrictions geo_restriction=#<struct Aws::CloudFront::Types::GeoRestriction restriction_type=\"none\", quantity=0, items=[]>>, web_acl_id=\"\">, etag=\"E26F7H715C0SRE\">\nirb(main):008:0> cloudfront.update_distribution({\nirb(main):009:2*   id: distribution_id,\nirb(main):010:2*   distribution_config: resp.distribution_config,\nirb(main):011:2*   if_match: resp.etag\nirb(main):012:2> })\nopening connection to cloudfront.amazonaws.com:443...\nopened\nstarting SSL for cloudfront.amazonaws.com:443...\nSSL established\n<- \"PUT /2016-01-13/distribution/XXXXXXXXXXXXX/config HTTP/1.1\\r\\nContent-Type: \\r\\nAccept-Encoding: \\r\\nUser-Agent: aws-sdk-ruby2/2.2.12 ruby/2.2.4 x86_64-linux-gnu\\r\\nIf-Match: E26F7H715C0SRE\\r\\nX-Amz-Date: 20160121T183614Z\\r\\nHost: cloudfront.amazonaws.com\\r\\nX-Amz-Content-Sha256: 23b80419cc91fcffeb1c9422a48e8350693a0ba0257bd10b925cc36aa8d9eb0e\\r\\nAuthorization: AWS4-HMAC-SHA256 Credential=AKIAJSD72GXWQ3HPOSKQ/20160121/us-east-1/cloudfront/aws4_request, SignedHeaders=host;x-amz-content-sha256;x-amz-date, Signature=864601b4f94fa93d07e555d7a710eb87737de68474e37fb75333070a3bc565da\\r\\nContent-Length: 3147\\r\\nAccept: */*\\r\\n\\r\\n\"\n-> \"HTTP/1.1 400 Bad Request\\r\\n\"\n-> \"x-amzn-RequestId: da3bd2d6-c06d-11e5-aa12-ff9f7a59ea7a\\r\\n\"\n-> \"Content-Type: text/xml\\r\\n\"\n-> \"Content-Length: 283\\r\\n\"\n-> \"Date: Thu, 21 Jan 2016 18:36:14 GMT\\r\\n\"\n-> \"Connection: close\\r\\n\"\n-> \"\\r\\n\"\nreading 283 bytes...\n-> \"\"\n-> \"<?xml version=\\\"1.0\\\"?>\\n<ErrorResponse xmlns=\\\"http://cloudfront.amazonaws.com/doc/2016-01-13/\\\"><Error><Type>Sender</Type><Code>MalformedInput</Code><Message>Unexpected list element termination</Message></Error><RequestId>da3bd2d6-c06d-11e5-aa12-ff9f7a59ea7a</RequestId></ErrorResponse>\"\nread 283 bytes\nConn close\nAws::CloudFront::Errors::MalformedInput: Unexpected list element termination\n    from /var/lib/gems/2.2.0/gems/aws-sdk-core-2.2.12/lib/seahorse/client/plugins/raise_response_errors.rb:15:incall'\n    from /var/lib/gems/2.2.0/gems/aws-sdk-core-2.2.12/lib/aws-sdk-core/plugins/param_converter.rb:20:in call'\n    from /var/lib/gems/2.2.0/gems/aws-sdk-core-2.2.12/lib/seahorse/client/plugins/response_target.rb:21:incall'\n    from /var/lib/gems/2.2.0/gems/aws-sdk-core-2.2.12/lib/seahorse/client/request.rb:70:in send_request'\n    from /var/lib/gems/2.2.0/gems/aws-sdk-core-2.2.12/lib/seahorse/client/base.rb:207:inblock (2 levels) in define_operation_methods'\n    from (irb):8\n    from /usr/bin/irb:11:in `'\nirb(main):013:0>\nmatt@anoia:~$ sudo gem uninstall aws-sdk\nSelect gem to uninstall:\n 1. aws-sdk-2.1.36\n 2. aws-sdk-2.2.12\n 3. All versions\n\n2\nSuccessfully uninstalled aws-sdk-2.2.12\nmatt@anoia:~$ irb\nirb(main):001:0> require \"aws-sdk\"\n=> true\nirb(main):002:0> distribution_id = \"XXXXXXXXXXXXX\"\n=> \"XXXXXXXXXXXXX\"\nirb(main):003:0> cloudfront = Aws::CloudFront::Client.new http_wire_trace: true\n=> #\nirb(main):004:0> resp = cloudfront.get_distribution_config({\nirb(main):005:2   id: distribution_id,\nirb(main):006:2 })\nopening connection to cloudfront.amazonaws.com:443...\nopened\nstarting SSL for cloudfront.amazonaws.com:443...\nSSL established\n<- \"GET /2015-07-27/distribution/XXXXXXXXXXXXX/config HTTP/1.1\\r\\nContent-Type: \\r\\nAccept-Encoding: \\r\\nUser-Agent: aws-sdk-ruby2/2.1.36 ruby/2.2.4 x86_64-linux-gnu\\r\\nX-Amz-Date: 20160121T184552Z\\r\\nHost: cloudfront.amazonaws.com\\r\\nX-Amz-Content-Sha256: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\\r\\nAuthorization: AWS4-HMAC-SHA256 Credential=AKIAJSD72GXWQ3HPOSKQ/20160121/us-east-1/cloudfront/aws4_request, SignedHeaders=host;user-agent;x-amz-content-sha256;x-amz-date, Signature=65d312951ee7ba17f4ed6d7f3e06f01bd620ab2e713e2834ae559c7b768dd252\\r\\nContent-Length: 0\\r\\nAccept: /\\r\\n\\r\\n\"\n-> \"HTTP/1.1 200 OK\\r\\n\"\n-> \"x-amzn-RequestId: 32b0d295-c06f-11e5-b820-259f80ce4a45\\r\\n\"\n-> \"ETag: E26F7H715C0SRE\\r\\n\"\n-> \"Content-Type: text/xml\\r\\n\"\n-> \"Content-Length: 2126\\r\\n\"\n-> \"Vary: Accept-Encoding\\r\\n\"\n-> \"Date: Thu, 21 Jan 2016 18:45:52 GMT\\r\\n\"\n-> \"\\r\\n\"\nreading 2126 bytes...\n-> \"\"\n-> \"<?xml version=\\\"1.0\\\"?>\\n14527087006901cdn.example.net404.html1assets.example.comassets.example.net80443match-viewerassets.example.comfalsenone4Access-Control-Request-HeadersAccess-Control-Request-MethodOriginRefererfalse0allow-all02HEADGET2HEADGETfalse864003153600000falsefalsePriceClass_AlltrueYYYYYYYYYYYYYYYYYYYYYsni-onlyTLSv1none0\"\nread 2126 bytes\nConn keep-alive\n=> #, default_root_object=\"404.html\", origins=#>]>, default_cache_behavior=#, headers=#>, trusted_signers=#, viewer_protocol_policy=\"allow-all\", min_ttl=0, allowed_methods=#>, smooth_streaming=false, default_ttl=86400, max_ttl=31536000>, cache_behaviors=#, custom_error_responses=#, comment=\"\", logging=#, price_class=\"PriceClass_All\", enabled=true, viewer_certificate=#, restrictions=#>, web_acl_id=\"\">, etag=\"E26F7H715C0SRE\">\nirb(main):007:0> cloudfront.update_distribution({\nirb(main):008:2   id: distribution_id,\nirb(main):009:2   distribution_config: resp.distribution_config,\nirb(main):010:2   if_match: resp.etag\nirb(main):011:2> })\nopening connection to cloudfront.amazonaws.com:443...\nopened\nstarting SSL for cloudfront.amazonaws.com:443...\nSSL established\n<- \"PUT /2015-07-27/distribution/XXXXXXXXXXXXX/config HTTP/1.1\\r\\nContent-Type: \\r\\nAccept-Encoding: \\r\\nUser-Agent: aws-sdk-ruby2/2.1.36 ruby/2.2.4 x86_64-linux-gnu\\r\\nIf-Match: E26F7H715C0SRE\\r\\nX-Amz-Date: 20160121T184605Z\\r\\nHost: cloudfront.amazonaws.com\\r\\nX-Amz-Content-Sha256: 37bea6c8de1f49672e61212b021df12139bf08f4c5fc803166733a34d9ec5989\\r\\nAuthorization: AWS4-HMAC-SHA256 Credential=AKIAJSD72GXWQ3HPOSKQ/20160121/us-east-1/cloudfront/aws4_request, SignedHeaders=host;if-match;user-agent;x-amz-content-sha256;x-amz-date, Signature=eb25eb65c96eea6e4d52d2532d8bffcaea3bc22cd5de71f628ee45bfc48a818f\\r\\nContent-Length: 2661\\r\\nAccept: /*\\r\\n\\r\\n\"\n-> \"HTTP/1.1 200 OK\\r\\n\"\n-> \"x-amzn-RequestId: 3a6023e2-c06f-11e5-a8c7-5957995dbdf4\\r\\n\"\n-> \"ETag: E1CE144GS66ZVJ\\r\\n\"\n-> \"Content-Type: text/xml\\r\\n\"\n-> \"Content-Length: 2474\\r\\n\"\n-> \"Vary: Accept-Encoding\\r\\n\"\n-> \"Date: Thu, 21 Jan 2016 18:46:05 GMT\\r\\n\"\n-> \"\\r\\n\"\nreading 2474 bytes...\n-> \"\"\n-> \"<?xml version=\\\"1.0\\\"?>\\nXXXXXXXXXXXXXInProgress2016-01-21T18:46:05.917Z0d31fm74euk8bn7.cloudfront.netfalse014527087006901cdn.example.net404.html1assets.example.comassets.example.net80443match-viewerassets.example.comfalsenone4Access-Control-Request-HeadersAccess-Control-Request-MethodOriginRefererfalse0allow-all02HEADGET2HEADGETfalse864003153600000falsefalsePriceClass_AlltrueYYYYYYYYYYYYYYYYYYYYYsni-onlyTLSv1none0\"\nread 2474 bytes\nConn keep-alive\n=> #, distribution_config=#, default_root_object=\"404.html\", origins=#>]>, default_cache_behavior=#, headers=#>, trusted_signers=#, viewer_protocol_policy=\"allow-all\", min_ttl=0, allowed_methods=#>, smooth_streaming=false, default_ttl=86400, max_ttl=31536000>, cache_behaviors=#, custom_error_responses=#, comment=\"\", logging=#, price_class=\"PriceClass_All\", enabled=true, viewer_certificate=#, restrictions=#>, web_acl_id=\"\">>, etag=\"E1CE144GS66ZVJ\">\nirb(main):012:0>\n```\n. \n",
    "pbfein": "http://docs.aws.amazon.com/amazondynamodb/latest/developerguide/UsingIAMWithDDB.ResourcePermissions.html\nDescribeReservedCapacity\nDescribeReservedCapacityOfferings\n. ",
    "tilo": "exactly that - pre-sign a url which is posted on an internal web-site, so people can click-through to the S3-content and see the raw file in the browser. The defaults for the MetaData is to have Content-Type 'text/plain' , so this works well in the AWS Console... but when I use the pre-signed URLs for files ending in .log , they always get downloaded , whereas in the AWS Console they show directly as text in the browser (that's what we want).\n. is there any update on this? @cjyclaire do you know who might be the right person to look at this?. ",
    "daneelliotthamilton": "I was able to get it to work by doing  @s3_direct_post.as_json['fields']\n. Is it therefore that the method cannot be directly accessed?  I've seen a few tutorials and questions on stackoverflow where that does not seem to be an issue.\n. I got the error when i tried creating the presigned object in my console as well\n. ",
    "Brotakuu": "Please add this to the docs. Spent way too much time debugging what turned out to be a region mismatch.\n. ",
    "bpourriahi": "I see in the code that it does inject Content-Md5 into the body.\nWould be good to have this in the docs.\n. ",
    "dhunnapotha": "Hey \n- Dint find any specs related to tree_hash. So, I created a file tree_hash_spec and added a test case.\n- Without the fix, the code goes into infinte loop. I can't think of a proper way to have a testcase to test for infinte loop. \nCan you go through the changes and let me know if you have any further inputs?\n. ",
    "simarpreetgujral": "Hi @awood45 ,\nI am facing the similar issue. I am trying to configure aws-rds instance using Chef cookbook. When I wrote\n gem 'aws-sdk' \nin the Gemfile , I am facing error  \" had an error: NameError: uninitialized constant Overclock::Aws::RDS::AWS\"\nWhen I wrote \ngem 'aws-sdk', '~> 1.0'\ngem 'aws-sdk-resources'\nin the Gemfile, it gave error related to gem json. I resolved that error and it again gives the same error :\nERROR: Missing gem 'aws-sdk'. Use the default aws-rds recipe to install it first\nhad an error: NameError: uninitialized constant Overclock::Aws::RDS::AWS\nec2-54-201-144-198.us-west-2.compute.amazonaws.com [2017-02-28T10:44:26+00:00] FATAL: Chef::Exceptions::ChildConvergeError: Chef run process exited unsuccessfully (exit code 1)\nCan you please help.\n . ",
    "nrodriguez": "@awood45  I'm currently looking to automate the cleanup of old application versions. Since we deploy often, I wanted to just delete whatever versions were not deployed to a beanstalk instance\n. @awood45 That solution seemed to work using the application versions and the describe environments methods. I do wish that there was just an endpoint for it but thank you.\n. ",
    "whazzmaster": "I'm checking in on this issue- it's holding up some major code cleanup we have staged. I'm also happy to assist; if there's a root cause identified I can work on PR. I did some debugging on the issue originally but things got confusing once it got down into the ObjectMultipartCopier-- I lost the thread.\n. ## Update\nDebugged through the multipart copy today and found the bug, as well as a workaround.\nBug\nSee aws-sdk-resources/lib/aws-sdk-resources/services/s3/object_multipart_copier.rb:118:\nruby\ndef source_size(options)\n  if options[:content_length]\n    options.delete(:content_length)\n  else\n    bucket, key = options[:copy_source].match(/([^\\/]+?)\\/(.+)/)[1,2]\n    @client.head_object(bucket:bucket, key:key).content_length # Uses target client instead of source client\n  end\nend\nThe source_size method attempts to get the content_length of the source object, but uses the client supplied with the target object. Hence it queries the target region for the content length of the object that does not exist there, and then cannot find it.\nI believe that what needs to happen is that, in ObjectCopier#copy_object needs to pass in the source client explicitly (if it exists) to the options passed into ObjectMultipartCopier#copy. Subsequently, in ObjectMultipartCopier#source_size, if source_client exists in the options then use that to fetch the content length of the source object (in the source region, via the source client). If the client is not in the options, fall back to using the target client as happens today.\nWorkaround\nAs you can see in the above code snippet, if :content_length is present in the options, it does not need to fetch the content length using a client. Hence, per the code snippet in the original issue submission, if you do the copy as such:\nruby\ntarget_object.copy_from(source_object, multipart_copy: true, content_length: source_object.content_length)\n...it short-circuits the fetch of the content_length and the copy proceeds correctly.\n. @trevorrowe Ack- just pushed a PR and didn't see your response (just happened to notice I hadn't started my mail client after reboot this morning :crying_cat_face:)\nThe PR extracts the client from the supplied source object passed to copy_from if it is an S3::Object. If none is found it falls back on the existing behavior. \nConceptually though I like your idea about being more explicit at the external API, and allowing for an explicit source client to be supplied. I think the specific option keys probably depend on whether region-to-region copying should be recognized as a 'non-normal' action that requires extra options to be passed.  Considering the existing ability to pass in a variety of different option types, allowing source_client and source_region depending on user's needs feels like the way to go.\nUltimately I'd say that this PR fixes the bug, but the behavior can be robustified via more/better options to the copy_from call.\n. Hmm, looks like jruby fails one spec. Taking a look now.\n. Soooooo, after debugging this all afternoon, it appears that the rpec mocks are not thread safe and so the multipart upload stubbing of client.upload_part_copy may fail due to an ArrayIndexOutofBoundsException.\nSee aws-sdk-resources/spec/services/s3/object/multipart_copy_spec:189 for the block where 60 mocks get setup. If, to test it out, you comment out that block and replace aws-sdk-resources/lib/aws-sdk-resources/services/s3/object_multipart_copier.rb#copy_part where the etag is retrieved from the stubbed call with some hard-coded string, you will never see the test fail. \nThere seems to be an issue inside the rspec-mocks that if mocks are being 'consumed' in a multi-threaded context, at some point there is an index OOB exception. I just wonder why this only happens in jruby...\n. I think I got everything in there- take another look and let me know if I missed anything.\n. :+1: Thanks!\n. Sounds good- I'll try to collect the feedback and push an update this afternoon.\n. ",
    "bigtiger": ":+1:\n. ",
    "rokka-n": "Hi,\nLooks like a bug. \nThere is a bunch of options missing in the compute_parts method at \nhttps://github.com/aws/aws-sdk-ruby/blob/282f9719cf7c6389bee935ab17360e6dd29956d3/aws-sdk-resources/lib/aws-sdk-resources/services/s3/multipart_file_uploader.rb#L87\nSince \"part\" object is missing some parameters, upload_part fails in cases when key/encryption is required.\nAdding this two, for example fixes problem with sse with customer keys. \nsse_customer_key: options[:sse_customer_key],\n           sse_customer_algorithm: options[:sse_customer_algorithm]\n. Nice, thank you for fixing.\n. ",
    "tawan": "Ok, I'll have a look at the S3 MD5 checker and wrap it in a PR. In case of questions, I'll drop them here.\n. @trevorrowe Thanks for your feedback. I incorporated your suggestion. I amended my changes and rebased, therefore your inline comment seems to be gone. I commented the line in question again for some context.\n. @trevorrowe This is the line you mentioned in the previous commit: I simple added a nil check. Only messages which are which are placed in the successful array are verified.\n. ",
    "chupakabr": "Today I noticed that this is happening when I run the plugin from RSpec like this:\nlet(:output) { LogStash::Outputs::Firehose.new({\"codec\" => \"plain\"}) }\nHowever,  proper exception is being thrown/handled when I install the plugin into Logstash and run Logstash from the command line.\nSo might be a minor one as it is related to tests only or I just don't know how to properly use Ruby and RSpec :smile: \n. Hey Trevor,\nGotcha. I was using JRuby for testing. I'm not JRuby or Ruby expert, so maybe my configuration JRuby is messed up somewhere. Anyway, the code itself works fine for me, cannot say the same about tests though, but not a big deal for me now. :)\nWith best,\nValera\n. ",
    "djpate": "Having the same issue with jRuby 9.0.5.0\n```\nNameError: uninitialized constant Aws::Client::Errors\n                    const_missing at org/jruby/RubyModule.java:3212\n                        const_get at org/jruby/RubyModule.java:3152\n                            error at /home/pate/.rvm/gems/jruby-9.0.5.0@analysis_service/gems/aws-sdk-core-2.2.24/lib/aws-sdk-core/xml/error_handler.rb:25\n```\n. ",
    "drewacl": "I am using aws-sdk (2.2.14).  Here is a list of other GEMS from my machine as well:\nactionmailer (4.2.5.1, 4.2.4)\nactionpack (4.2.5.1, 4.2.4)\nactionview (4.2.5.1, 4.2.4)\nactivejob (4.2.5.1, 4.2.4)\nactivemodel (4.2.5.1, 4.2.4)\nactiverecord (4.2.5.1, 4.2.4)\nactivesupport (4.2.5.1, 4.2.4)\naddressable (2.4.0, 2.3.8)\nansi (1.5.0)\narel (7.0.0, 6.0.3)\nautoprefixer-rails (6.3.1, 6.0.3)\naws-sdk (2.2.14)\naws-sdk-core (2.2.14, 2.1.30, 2.1.23, 2.1.22, 2.1.20)\naws-sdk-resources (2.2.14, 2.1.30, 2.1.23, 2.1.22, 2.1.20)\nbackports (3.6.7, 3.6.6)\nberkshelf (4.1.0, 4.0.1)\nberkshelf-api-client (2.0.0)\nbigdecimal (1.2.6)\nbinding_of_caller (0.7.2)\nbootstrap-sass (3.3.6, 3.3.5.1)\nbuff-config (1.0.1)\nbuff-extensions (1.0.0)\nbuff-ignore (1.1.1)\nbuff-ruby_engine (0.1.0)\nbuff-shell_out (0.2.0)\nbuilder (3.2.2)\nbundler (1.11.2, 1.10.6)\nbusser (0.7.1)\nbyebug (6.0.2)\ncapistrano (3.4.0)\ncapistrano-bundler (1.1.4)\ncapistrano-rails (1.1.6, 1.1.3)\ncelluloid (0.17.3, 0.16.0)\ncelluloid-essentials (0.20.5)\ncelluloid-extras (0.20.5)\ncelluloid-fsm (0.20.5)\ncelluloid-io (0.17.3, 0.16.2)\ncelluloid-pool (0.20.5)\ncelluloid-supervision (0.20.5)\nchef-config (12.6.0)\ncleanroom (1.0.0)\ncoffee-rails (4.1.1, 4.1.0)\ncoffee-script (2.4.1)\ncoffee-script-source (1.10.0, 1.9.1.1)\ncolorize (0.7.7)\ncomposite_primary_keys (8.1.2, 8.1.1)\nconcurrent-ruby (1.0.0)\ncrack (0.4.3, 0.4.2)\ndaemons (1.2.3)\ndebase (0.1.4)\ndebase-ruby_core_source (0.8.4, 0.7.10)\ndebug_inspector (0.0.2)\ndep-selector-libgecode (1.0.2)\ndep_selector (1.0.3)\ndiff-lcs (1.2.5)\ndomain_name (0.5.20160128, 0.5.25, 0.5.24)\nerubis (2.7.0)\neventmachine (1.0.8)\nexecjs (2.6.0)\nfaraday (0.9.2)\nffi (1.9.10 x64-mingw32)\nge_corp_cloud (0.1.0)\nglobalid (0.3.6)\nhashie (3.4.3)\nhitimes (1.2.3)\nhttp-cookie (1.0.2)\nhttpclient (2.7.1, 2.6.0.1)\ni18n (0.7.0)\nio-console (0.4.3)\njbuilder (2.4.0, 2.3.2, 2.3.1)\njmespath (1.1.3, 1.0.2)\njquery-rails (4.1.0, 4.0.5)\njson (1.8.3, 1.8.1)\nloofah (2.0.3)\nmail (2.6.3)\nmime-types (3.0, 2.6.2, 2.6.1)\nmime-types-data (3.2015.1120)\nmini_portile (0.7.0.rc4, 0.6.2)\nmini_portile2 (2.1.0)\nminitar (0.5.4)\nminitest (5.8.4, 5.8.1, 5.8.0)\nminitest-reporters (1.1.7, 1.1.0)\nmixlib-authentication (1.4.0, 1.3.0)\nmixlib-config (2.2.1)\nmixlib-log (1.6.0)\nmixlib-shellout (2.2.6 universal-mingw32, 2.2.5 universal-mingw32)\nmolinillo (0.2.3)\nmulti_json (1.11.2)\nmultipart-post (2.0.0)\nmysql2 (0.4.2 x64-mingw32, 0.3.20 x64-mingw32)\nnet-scp (1.2.1)\nnet-ssh (3.0.2, 3.0.1)\nnetrc (0.11.0, 0.10.3)\nnio4r (1.2.0)\nnokogiri (1.6.8.rc2 x64-mingw32, 1.6.7.rc3 x64-mingw32, 1.6.6.2 x64-mingw32)\noctokit (4.2.0, 3.8.0)\npsych (2.0.8)\npuma (2.14.0)\nrack (1.6.4)\nrack-protection (1.5.3)\nrack-test (0.6.3)\nrails (4.2.5.1, 4.2.4)\nrails-deprecated_sanitizer (1.0.3)\nrails-dom-testing (1.0.7)\nrails-html-sanitizer (1.0.3, 1.0.2)\nrailties (4.2.5.1, 4.2.4)\nrake (10.5.0, 10.4.2)\nrdoc (4.2.1, 4.2.0)\nrest-client (2.0.0.rc2 x64-mingw32, 1.8.0 x64-mingw32)\nretryable (2.0.3)\nridley (4.4.2)\nrspec (3.4.0, 3.3.0)\nrspec-core (3.4.2, 3.3.2)\nrspec-expectations (3.4.0, 3.3.1)\nrspec-mocks (3.4.1, 3.3.2)\nrspec-support (3.4.1, 3.3.0)\nruby-debug-ide (0.4.32)\nruby-progressbar (1.7.5)\nsafe_yaml (1.0.4)\nsass (3.4.21, 3.4.18)\nsass-rails (5.0.4)\nsawyer (0.6.0)\nsdoc (0.4.1)\nsemverse (1.2.1)\nshotgun (0.9.1)\nsinatra (1.4.7, 1.4.6)\nsinatra-contrib (1.4.6)\nsolve (2.0.1, 1.2.1)\nspring (1.6.2, 1.4.0)\nsprockets (3.5.2, 3.3.5, 3.3.4)\nsprockets-rails (3.0.1, 2.3.3)\nsqlite3 (1.3.11 x64-mingw32, 1.3.10 x64-mingw32)\nsshkit (1.8.1, 1.7.1)\nthin (1.6.4)\nthor (0.19.1, 0.19.0)\nthread_safe (0.3.5)\ntilt (2.0.2, 2.0.1)\ntimers (4.1.1, 4.0.4)\nturbolinks (2.5.3)\ntzinfo (1.2.2)\ntzinfo-data (1.2016.1, 1.2015.6)\nuglifier (2.7.2)\nunf (0.1.4)\nunf_ext (0.0.7.2 x64-mingw32, 0.0.7.1 x64-mingw32)\nvaria_model (0.5.0, 0.4.1)\nwarden (1.2.6, 1.2.3)\nweb-console (3.1.1, 2.2.1)\nwin32-process (0.8.3)\nwmi-lite (1.0.0)\nyard (0.8.7.6)\nThanks,\nAndrew\n. I thought our outbound proxy might be the issue, however I duplicated the logic in pyhton with boto3 and was able to successfully upload a file using KMS and sig 4.\nLet me know if you need additional information.\nThanks,\nAndrew\n. Here is the first part:\nCANONICAL_REQUEST: PUT\n/s3%3A%2F%2Fge.corp.cloud-services.us-east.med-corp-preprod/kickstart-med-stage.json\ncontent-md5:CY9rzUYh03PK3k6DJie09g==\nhost:s3.amazonaws.com\nx-amz-content-sha256:9f86d081884c7d659a2feaa0c55ad015a3bf4f1b2b0b822cd15d6c15b0f00a08\nx-amz-date:20160219T204841Z\nx-amz-security-token:AQoDYXdzEN7//////////wEaoAK9Ysst/74W73iHq46GwFCkM/3Pl/lY5au4y1ZHRDDdzoS+N6yGPBcKpdKUBdJX3mz8qR+1bUWBylHuuhM8wDA4fVS8fvJ+gGCmQQvThjL8MRrC9aj+30/T6X7naJUOcXNZG5cANkNZYxZDCATE8hFqkqZyv7DH4CUJVEUGQ/sMX5l2GsfO/qX6XqnodyYLDXTcbaDk+6f0jABh5rkrOuJp1+C+fbdaE3jx7FzGbYl6Srfe/ss71dBt3Wn5NKa5Q8SRxA3RbiSH6FhgS9RQ0p7izbZcovKEHH7Ap8UVJMwb3K2mwcNJS+fwdgewhQfhiv4rlBunK9y+pmBK4GnVNPd6FaJDHah4XA3EDXXprv1zkn+0Fjz+WkIMNOoF33Tld3wgqP+dtgU=\nx-amz-server-side-encryption-aws-kms-key-id:arn:aws:kms:us-east-1:358880727977:key/d0e004da-097a-465d-b1a8-9f45aeeca130\ncontent-md5;host;x-amz-content-sha256;x-amz-date;x-amz-security-token;x-amz-server-side-encryption-aws-kms-key-id\n9f86d081884c7d659a2feaa0c55ad015a3bf4f1b2b0b822cd15d6c15b0f00a08\nSTRING_TO_SIGN: AWS4-HMAC-SHA256\n20160219T204841Z\n20160219/us-east-1/s3/aws4_request\nafb84049a1526630d75908e9de4fa3d14695a8d0c2a57b0680649a0fbde84f0b\n. ################################################################################\nCANONICAL_REQUEST: PUT\n/s3%3A%2F%2Fge.corp.cloud-services.us-east.med-corp-preprod/kickstart-med-stage.json\ncontent-md5:CY9rzUYh03PK3k6DJie09g==\nhost:s3.amazonaws.com\nx-amz-content-sha256:9f86d081884c7d659a2feaa0c55ad015a3bf4f1b2b0b822cd15d6c15b0f00a08\nx-amz-date:20160219T205634Z\nx-amz-security-token:AQoDYXdzEN7//////////wEaoALtOv67S6oc9cf9QWzyWCs8CNY6Hz7DqH5TrBbkqJIGrMSPPF0QVZ+omNvGsZku+BHyV/guvetyqzNJtdzG2cI+lBi1y8nZ/soHiCZoLflB7OFhu+LC6GQVw7F/wCBsmchgMyobTOjSyygowwP9324o4wZR1G/iTss5wGlw9kQiZzMd89y6fGwHDxD3z3vrFJ3bzU6VNIFv03DYqZVOxVVlOC215ZkEPqZ1MSRgg5rUsHI9HELvmWvSiQgUIYmXu4GJu7+ikzAkYgluwUefNkGi7it6B3yw7zr+nDmDis4sOriX6FSzimdiStnUHbldrUOYb7Xrz6OscsT2yFZ+MctFr5RPHr1rwps5G9KpWYENJxP5A/lYnw2iSMSdL3rjlxYggYOetgU=\nx-amz-server-side-encryption-aws-kms-key-id:arn:aws:kms:us-east-1:358880727977:key/d0e004da-097a-465d-b1a8-9f45aeeca130\ncontent-md5;host;x-amz-content-sha256;x-amz-date;x-amz-security-token;x-amz-server-side-encryption-aws-kms-key-id\n9f86d081884c7d659a2feaa0c55ad015a3bf4f1b2b0b822cd15d6c15b0f00a08\n\n\nSTRING_TO_SIGN: AWS4-HMAC-SHA256\n20160219T205634Z\n20160219/us-east-1/s3/aws4_request\n46d533d1da46c8ee48453b8ca058def52d28613245d7d4704b1a5f2fd34b5921\n\n\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\nSignatureDoesNotMatchThe request signature we calculated does not match the signature you provided. Check your key and signing method.ASIAIHX2FYAPF435CJJQAWS4-HMAC-SHA256\n20160219T205634Z\n20160219/us-east-1/s3/aws4_request\n1468870158a1fa5c1c7e7c1f6cdb67c5d6291c2c037036c97af8624a65ae44df766da6bc0bff53d2705a7ba9a04951612b9d165ee2cd7ad6dec487fe9241059c41 57 53 34 2d 48 4d 41 43 2d 53 48 41 32 35 36 0a 32 30 31 36 30 32 31 39 54 32 30 35 36 33 34 5a 0a 32 30 31 36 30 32 31 39 2f 75 73 2d 65 61 73 74 2d 31 2f 73 33 2f 61 77 73 34 5f 72 65 71 75 65 73 74 0a 31 34 36 38 38 37 30 31 35 38 61 31 66 61 35 63 31 63 37 65 37 63 31 66 36 63 64 62 36 37 63 35 64 36 32 39 31 63 32 63 30 33 37 30 33 36 63 39 37 61 66 38 36 32 34 61 36 35 61 65 34 34 64 66PUT\n/s3%3A//ge.corp.cloud-services.us-east.med-corp-preprod/kickstart-med-stage.json\ncontent-md5:CY9rzUYh03PK3k6DJie09g==\nhost:s3.amazonaws.com\nx-amz-content-sha256:9f86d081884c7d659a2feaa0c55ad015a3bf4f1b2b0b822cd15d6c15b0f00a08\nx-amz-date:20160219T205634Z\nx-amz-security-token:AQoDYXdzEN7//////////wEaoALtOv67S6oc9cf9QWzyWCs8CNY6Hz7DqH5TrBbkqJIGrMSPPF0QVZ+omNvGsZku+BHyV/guvetyqzNJtdzG2cI+lBi1y8nZ/soHiCZoLflB7OFhu+LC6GQVw7F/wCBsmchgMyobTOjSyygowwP9324o4wZR1G/iTss5wGlw9kQiZzMd89y6fGwHDxD3z3vrFJ3bzU6VNIFv03DYqZVOxVVlOC215ZkEPqZ1MSRgg5rUsHI9HELvmWvSiQgUIYmXu4GJu7+ikzAkYgluwUefNkGi7it6B3yw7zr+nDmDis4sOriX6FSzimdiStnUHbldrUOYb7Xrz6OscsT2yFZ+MctFr5RPHr1rwps5G9KpWYENJxP5A/lYnw2iSMSdL3rjlxYggYOetgU=\nx-amz-server-side-encryption-aws-kms-key-id:arn:aws:kms:us-east-1:358880727977:key/d0e004da-097a-465d-b1a8-9f45aeeca130\ncontent-md5;host;x-amz-content-sha256;x-amz-date;x-amz-security-token;x-amz-server-side-encryption-aws-kms-key-id\n9f86d081884c7d659a2feaa0c55ad015a3bf4f1b2b0b822cd15d6c15b0f00a0850 55 54 0a 2f 73 33 25 33 41 2f 2f 67 65 2e 63 6f 72 70 2e 63 6c 6f 75 64 2d 73 65 72 76 69 63 65 73 2e 75 73 2d 65 61 73 74 2e 6d 65 64 2d 63 6f 72 70 2d 70 72 65 70 72 6f 64 2f 6b 69 63 6b 73 74 61 72 74 2d 6d 65 64 2d 73 74 61 67 65 2e 6a 73 6f 6e 0a 0a 63 6f 6e 74 65 6e 74 2d 6d 64 35 3a 43 59 39 72 7a 55 59 68 30 33 50 4b 33 6b 36 44 4a 69 65 30 39 67 3d 3d 0a 68 6f 73 74 3a 73 33 2e 61 6d 61 7a 6f 6e 61 77 73 2e 63 6f 6d 0a 78 2d 61 6d 7a 2d 63 6f 6e 74 65 6e 74 2d 73 68 61 32 35 36 3a 39 66 38 36 64 30 38 31 38 38 34 63 37 64 36 35 39 61 32 66 65 61 61 30 63 35 35 61 64 30 31 35 61 33 62 66 34 66 31 62 32 62 30 62 38 32 32 63 64 31 35 64 36 63 31 35 62 30 66 30 30 61 30 38 0a 78 2d 61 6d 7a 2d 64 61 74 65 3a 32 30 31 36 30 32 31 39 54 32 30 35 36 33 34 5a 0a 78 2d 61 6d 7a 2d 73 65 63 75 72 69 74 79 2d 74 6f 6b 65 6e 3a 41 51 6f 44 59 58 64 7a 45 4e 37 2f 2f 2f 2f 2f 2f 2f 2f 2f 2f 77 45 61 6f 41 4c 74 4f 76 36 37 53 36 6f 63 39 63 66 39 51 57 7a 79 57 43 73 38 43 4e 59 36 48 7a 37 44 71 48 35 54 72 42 62 6b 71 4a 49 47 72 4d 53 50 50 46 30 51 56 5a 2b 6f 6d 4e 76 47 73 5a 6b 75 2b 42 48 79 56 2f 67 75 76 65 74 79 71 7a 4e 4a 74 64 7a 47 32 63 49 2b 6c 42 69 31 79 38 6e 5a 2f 73 6f 48 69 43 5a 6f 4c 66 6c 42 37 4f 46 68 75 2b 4c 43 36 47 51 56 77 37 46 2f 77 43 42 73 6d 63 68 67 4d 79 6f 62 54 4f 6a 53 79 79 67 6f 77 77 50 39 33 32 34 6f 34 77 5a 52 31 47 2f 69 54 73 73 35 77 47 6c 77 39 6b 51 69 5a 7a 4d 64 38 39 79 36 66 47 77 48 44 78 44 33 7a 33 76 72 46 4a 33 62 7a 55 36 56 4e 49 46 76 30 33 44 59 71 5a 56 4f 78 56 56 6c 4f 43 32 31 35 5a 6b 45 50 71 5a 31 4d 53 52 67 67 35 72 55 73 48 49 39 48 45 4c 76 6d 57 76 53 69 51 67 55 49 59 6d 58 75 34 47 4a 75 37 2b 69 6b 7a 41 6b 59 67 6c 75 77 55 65 66 4e 6b 47 69 37 69 74 36 42 33 79 77 37 7a 72 2b 6e 44 6d 44 69 73 34 73 4f 72 69 58 36 46 53 7a 69 6d 64 69 53 74 6e 55 48 62 6c 64 72 55 4f 59 62 37 58 72 7a 36 4f 73 63 73 54 32 79 46 5a 2b 4d 63 74 46 72 35 52 50 48 72 31 72 77 70 73 35 47 39 4b 70 57 59 45 4e 4a 78 50 35 41 2f 6c 59 6e 77 32 69 53 4d 53 64 4c 33 72 6a 6c 78 59 67 67 59 4f 65 74 67 55 3d 0a 78 2d 61 6d 7a 2d 73 65 72 76 65 72 2d 73 69 64 65 2d 65 6e 63 72 79 70 74 69 6f 6e 2d 61 77 73 2d 6b 6d 73 2d 6b 65 79 2d 69 64 3a 61 72 6e 3a 61 77 73 3a 6b 6d 73 3a 75 73 2d 65 61 73 74 2d 31 3a 33 35 38 38 38 30 37 32 37 39 37 37 3a 6b 65 79 2f 64 30 65 30 30 34 64 61 2d 30 39 37 61 2d 34 36 35 64 2d 62 31 61 38 2d 39 66 34 35 61 65 65 63 61 31 33 30 0a 0a 63 6f 6e 74 65 6e 74 2d 6d 64 35 3b 68 6f 73 74 3b 78 2d 61 6d 7a 2d 63 6f 6e 74 65 6e 74 2d 73 68 61 32 35 36 3b 78 2d 61 6d 7a 2d 64 61 74 65 3b 78 2d 61 6d 7a 2d 73 65 63 75 72 69 74 79 2d 74 6f 6b 65 6e 3b 78 2d 61 6d 7a 2d 73 65 72 76 65 72 2d 73 69 64 65 2d 65 6e 63 72 79 70 74 69 6f 6e 2d 61 77 73 2d 6b 6d 73 2d 6b 65 79 2d 69 64 0a 39 66 38 36 64 30 38 31 38 38 34 63 37 64 36 35 39 61 32 66 65 61 61 30 63 35 35 61 64 30 31 35 61 33 62 66 34 66 31 62 32 62 30 62 38 32 32 63 64 31 35 64 36 63 31 35 62 30 66 30 30 61 30 382E9BB8D90A2624AFXIFhKQfDX680JJj5dcNdPvUrLr6PYnLh4E73t7j0AhuyaJvgmayWtgjjpLax4RU3WMZfH18+f48=\n\n. The last comment has everything you asked for.  each is separated by '#' * 80\nThanks,\nAndrew\n. I kept debugging and determined this difference:\ncanonical request has:\n/s3%3A%2F%2Fge.corp.cloud-services.us-east.med-corp-preprod/kickstart-med-stage.json\nresult with signature error has:\n/s3%3A//ge.corp.cloud-services.us-east.med-corp-preprod/kickstart-med-stage.json\nSo the canonical request has the %2FA in it instead of /\nThis code works around my issue for now:\n     Aws::Signers::V4.send(:prepend, Module.new do\n```\n    def canonical_request(request, body_digest)\n      result = [\n          request.http_method,\n          path(request.endpoint).gsub(/%2F/, '/'),\n          normalized_querystring(request.endpoint.query || ''),\n          canonical_headers(request) + \"\\n\",\n          signed_headers(request),\n          body_digest\n      ].join(\"\\n\")\nend\n\nend)\n```\nIs there something on my side that is causing the %2FA in the canonical string?  \nThank you very much for the support!\n. Hello,\nLooks like our outbound proxy was modifying the data.\nThank you,\nDrew\nOn Mon, Feb 22, 2016 at 12:45 PM, Trevor Rowe notifications@github.com\nwrote:\n\n@drewacl https://github.com/drewacl I'm glad to see the additional\ndebug output was sufficient to show the cause of the signature error. It\nlooks like you closed this issue, were you able to discover the root cause?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/aws/aws-sdk-ruby/issues/1087#issuecomment-187287201.\n. \n",
    "newellista": "Seems there is a corresponding discussion on the JRuby repo as well.  Adding this link for reference.\nhttps://github.com/jruby/jruby/issues/3645\n. ",
    "ssuprun": "About the failed test... I'm not sure how the following error is related to my changes...\nFailures:\n  1) Aws::S3::Object multipart_copy: true #copy_from aborts the upload on errors\n     Failure/Error:\n       expect {\n         object.copy_from('source-bucket/source/key', multipart_copy: true)\n       }.to raise_error(Aws::S3::Errors::NoSuchKey)\n       expected Aws::S3::Errors::NoSuchKey, got Aws::S3::Errors::BadRequest with backtrace:\n         # ./aws-sdk-core/lib/seahorse/client/plugins/raise_response_errors.rb:15:in `call'\n         # ./aws-sdk-core/lib/aws-sdk-core/plugins/s3_sse_cpk.rb:18:in `call'\n         # ./aws-sdk-core/lib/aws-sdk-core/plugins/param_converter.rb:20:in `call'\n         # ./aws-sdk-core/lib/seahorse/client/plugins/response_target.rb:21:in `call'\n         # ./aws-sdk-core/lib/seahorse/client/request.rb:70:in `send_request'\n         # ./aws-sdk-core/lib/seahorse/client/base.rb:207:in `upload_part_copy'\n         # ./aws-sdk-resources/lib/aws-sdk-resources/services/s3/object_multipart_copier.rb:74:in `copy_part'\n         # ./aws-sdk-resources/lib/aws-sdk-resources/services/s3/object_multipart_copier.rb:63:in `copy_part_thread'\n     # ./aws-sdk-resources/spec/services/s3/object/multipart_copy_spec.rb:219:in `S3'\nFinished in 10.77 seconds (files took 4.57 seconds to load)\n383 examples, 1 failure\n. Guys, do you have any updates?\n. But what happens if we will have the defined one from these parameters in non-source profile? Seems they will be re-written from source one. I believe the usage of method \"merge\" is more correct. But I agree with check the existence of  source profile.\n. Actually you propose the cosmetic change. :) If this one is only blocker I'll do it ;)\n. ",
    "rparkhunovsky": "Any news? This feature is a must.\n. ",
    "durandom": ":heart: for fixing this so fast\n. ",
    "georgepalmer": "+1 for this.  We're seeing big memory spikes since upgrading a load of gems the last week (including AWS to v2) and I've spent most the day tracking down the culprit!\n. We've been playing with this today and have found a small correction required to the above code:\nmodule Aws\n  module Signers\n    class V4\n      def hexdigest(value)\n        if value.is_a? File\n          Digest::SHA256.file(value).hexdigest\n        elsif value.respond_to?(:read)\n          str = value.read\n          value.rewind\n          Digest::SHA256.hexdigest(str)\n        else\n          Digest::SHA256.hexdigest(value)\n        end\n      end\n    end\n  end\nend\n. ",
    "manabusakai": "@trevorrowe Thanks for the quick reply!\nUpdate to v2.2.18, I tried it. I found a another issue.\nGood when attribute is one. :smile:\n``` ruby\nclient.stub_responses(:get_queue_attributes, attributes: {\n  'QueueArn' => 'arn:aws:sqs:ap-northeast-1:123456789123:test',\n})\np client.get_queue_attributes({\n  queue_url: 'https://sqs.ap-northeast-1.amazonaws.com/012345678901/test',\n  attribute_names: ['All'],\n}).attributes\nresult -> {\"QueueArn\"=>\"arn:aws:sqs:ap-northeast-1:123456789123:test\"}\n```\nNot good when attribute is two or more. :cry:\n``` ruby\nclient.stub_responses(:get_queue_attributes, attributes: {\n  'QueueArn'                    => 'arn:aws:sqs:ap-northeast-1:123456789123:test',\n  'ApproximateNumberOfMessages' => '0',\n})\np client.get_queue_attributes({\n  queue_url: 'https://sqs.ap-northeast-1.amazonaws.com/012345678901/test',\n  attribute_names: ['All'],\n}).attributes\nresult -> {\"QueueArnApproximateNumberOfMessages\"=>\"arn:aws:sqs:ap-northeast-1:123456789123:test0\"}\n```\n. @trevorrowe Thank you! :+1:\n. ",
    "thatsanicehat": "I hate dropping a \"me too\", but confirmed this affects my specs as well, and this is not an isolated incident. \n. ",
    "albertogg": "I think that might be the desired behavior. Thanks for the super quick response!\n. Thanks for the quick fix! :clap: \n. ",
    "natetech333": "I verified the aws keys are correct. I was able to insert data from the aws cli.\n. I found the problem. I have to set the region when creating the dynamo client.\n. ",
    "jaypatrickhoward": "Verified that \"storage_class\" isn't being passed on the \"create_multipart_upload\", \"upload_part\" or \"complete_multipart_upload\" calls when those are provoked by a call to \"upload_file\".\n. Happy to oblige.  Let me know if there's anything else I can do to expedite the process of this getting into an official release.  Update the test cases, etc.  (Will require me to figure out how the test cases work, though...)\n. If you're using bundler, does your Gemfile.lock specify version 2.1.2 of aws-sdk? If so, do \"bundle update aws-sdk\" then commit the new Gemfile.lock, which should now specify aws-sdk 2.2.20.\n. ",
    "mirceal": ":+1:\n. ",
    "Optimiza": "Just confirmed is something related to a pointer which is in charge of the head_object row.\nIf I repeat data rows like this:\nruby\n  Aws.config[:s3] = {\n    stub_responses: {\n      list_objects: {\n        contents: [\n          { key: \"#{s3_prefix}template-a/docker-compose.yml\", last_modified: Time.now.utc },\n          { key: \"#{s3_prefix}template-b/docker-compose.yml\", last_modified: Time.now.utc },\n          { key: \"#{s3_prefix}template-c/docker-compose.yml\", last_modified: Time.now.utc },\n          { key: \"#{s3_prefix}not-a-template/docker-compose.xml\", last_modified: Time.now.utc }\n        ]\n      },\n      head_object: [\n        { metadata: { \"instance\" => \"t1.tiny\", \"artifacts\" => \"art1:art2:art3\" } },\n        { metadata: { \"instance\" => \"t2.tiny\", \"artifacts\" => \"art2:art1\" } },\n        { metadata: { \"instance\" => \"t3.tiny\", \"artifacts\" => \"art3:art2\" } },\n        { metadata: { \"instance\" => \"t1.tiny\", \"artifacts\" => \"art1:art2:art3\" } },\n        { metadata: { \"instance\" => \"t2.tiny\", \"artifacts\" => \"art2:art1\" } },\n        { metadata: { \"instance\" => \"t3.tiny\", \"artifacts\" => \"art3:art2\" } },\n        { metadata: { \"instance\" => \"t1.tiny\", \"artifacts\" => \"art1:art2:art3\" } },\n        { metadata: { \"instance\" => \"t2.tiny\", \"artifacts\" => \"art2:art1\" } },\n        { metadata: { \"instance\" => \"t3.tiny\", \"artifacts\" => \"art3:art2\" } },\n        { metadata: { \"instance\" => \"t1.tiny\", \"artifacts\" => \"art1:art2:art3\" } },\n        { metadata: { \"instance\" => \"t2.tiny\", \"artifacts\" => \"art2:art1\" } },\n        { metadata: { \"instance\" => \"t3.tiny\", \"artifacts\" => \"art3:art2\" } }\n      ]\n    }\n  }\nIt returns the right rows till the last one.\n. You are absolutely right, the problem is on my side due I wanted to reuse the Client once it's configured.\nBut this is not a very good idea for testing.\nMany thanks for your help and time.\n+1\n. ",
    "quiver": "@awood45 \n\nI'm already going to be touching Auto Scaling resources to fix another bug, so I will test and push the fix for this - no need for a PR, unless it's already done and tested.\n\nI don't have a work-in-progress PR, so please do.\nThank you for your quick response. I really need this feature!\n. :clap: \n. ",
    "Manju244": "Nice working. My bad.\nThanks for your valuable response.\n. ",
    "nikolai-b": "Solved from https://github.com/clarete/s3sync/issues/30\nWas due to strange chars in the file name, could do with an easier to decode error (or better file name handling...?) \n. The file name had a strange character.  In S3 the file name as url was \nfrui%14t.png but when editing the file name the character didn't appear.  I'm not sure what the character actually was, I'll try find out how that happened but it did brake this gem.\n. Turns out it was a \\u0014, not sure how that crept in there...\n. Sorry I missed the request for a full stack trace, it was very similar to https://github.com/clarete/s3sync/issues/30#issue-32116130 (except they are on an older gem version) but if you'd like me to reproduce it I can try.\n. ",
    "pndurette": "Thanks :+1: \n. Same as #1115. Thanks @awood45.\n. Thanks! :+1: !\n. ",
    "nickchappell": "+1\n. ",
    "jasonschulte": "+1\nThe immediate workaround is to install the kramdown gem.\n. ",
    "darkarnium": "+1\n. @awood45 thanks for the quick fix, I'm just running a test on my end to confirm for ya :)\n. @awood45 I can confirm that from at least EC2 (US-WEST-2) version 2.2.22 is being installed from Ruby gems and is no longer raising this exception.\n```\n$ gem list aws-sdk-core\n LOCAL GEMS \naws-sdk-core (2.2.22)\n$ irb\nirb(main):001:0> require 'aws-sdk-core'\n=> true\nirb(main):002:0> Aws::S3::Client.new(region: 'us-west-2')\n=> #\n```\n. Hey there,\nYep, that release does indeed resolve the issue - as does hacking in a to_json method on the FixNum class, albeit the latter being super dodgy :)\nCheers,\nPeter\n. @trevorrowe Not @musiaht but I'd guess that he may be referring to the pinning between aws-sdk-core and jmespath, as this is the particular pin that seemed to be the cause of this issue.\n. ",
    "warandpeace": "@awood45 Thanks for being so on top of this. :100: \n. ",
    "shortdudey123": "@awood45 thanks for the very quick response!\n. I blew away by bundle dir and reran bundle install.  After that, i can't reproduce:\n[1] pry(main)> require 'aws-sdk'\n=> true\n[2] pry(main)> Aws::VERSION\n=> \"2.7.11\"\n[3] pry(main)> client = Aws::EC2::Client.new\n=> #<Aws::EC2::Client>\n[4] pry(main)> instance_id = 'i-12345678'\n=> \"i-12345678\"\n[5] pry(main)> client.stop_instances(instance_ids: [instance_id])\n=> #<struct Aws::EC2::Types::StopInstancesResult\n stopping_instances=\n  [#<struct Aws::EC2::Types::InstanceStateChange\n    instance_id=\"i-12345678\",\n    current_state=#<struct Aws::EC2::Types::InstanceState code=64, name=\"stopping\">,\n    previous_state=#<struct Aws::EC2::Types::InstanceState code=16, name=\"running\">>]>\n[6] pry(main)> client.stop_instances(instance_ids: [instance_id], force: true)\n=> #<struct Aws::EC2::Types::StopInstancesResult\n stopping_instances=\n  [#<struct Aws::EC2::Types::InstanceStateChange\n    instance_id=\"i-12345678\",\n    current_state=#<struct Aws::EC2::Types::InstanceState code=64, name=\"stopping\">,\n    previous_state=#<struct Aws::EC2::Types::InstanceState code=64, name=\"stopping\">>]>\n[7] pry(main)> client.start_instances(instance_ids: [instance_id])\n=> #<struct Aws::EC2::Types::StartInstancesResult\n starting_instances=\n  [#<struct Aws::EC2::Types::InstanceStateChange\n    instance_id=\"i-12345678\",\n    current_state=#<struct Aws::EC2::Types::InstanceState code=0, name=\"pending\">,\n    previous_state=#<struct Aws::EC2::Types::InstanceState code=80, name=\"stopped\">>]>\n[8] pry(main)>. Based on that, i am going to chock this up to a screwed up gem installation for some reason.. ",
    "thrnio": "sorry, had this up when I bumped the keyboard and github hotkeys managed to create an issue\n. ",
    "robzr": "Thanks Alex, I did that, and am attaching the output from a working change_resource_record_sets call (UPSERT) and the non-working call (DELETE).\nSame data structure being passed as described in my first comment.  Same credentials, the only differences are what was described in the first comment (DELETE vs UPSERT, and the DELETE does not include the resource_record: [ { value: \"rdata\" } ]\nRob\nEDIT - added the hashmaps being passed to #change_resource_record_sets\ntrace_bad_DELETE.txt\ntrace_good_UPSERT.txt\n. If it helps, I used an inline proxy to sniff out the XML.  Here is the request and response.  Again, authentication works, ChangeResourceRecordSetsRequest works for an UPSERT, but fails on a DELETE.  I have verified that the Name/Type/TTL all match what is in R53 and is consistent with the UPSERT requests that do work.\nrequest.txt\nresponse.txt\n. Got it!  Yah guys, I scoured the docs for the API as well as the Ruby API stuff and my reading of it suggests it does not need this, and to get it to work I had to do some rewriting - but you're right, that was it, and it is now working.  \nThanks,\nRob\n. ",
    "alanwds": "Just to know.. i used ChangeBatch based on AWS union.py and work's well:\nroute53.change_resource_record_sets(\n                HostedZoneId=zone_id,\n                ChangeBatch={\n                    \"Comment\": \"Updated by Lambda DDNS\",\n                    \"Changes\": [\n                        {\n                            \"Action\": \"DELETE\",\n                            \"ResourceRecordSet\": {\n                                \"Name\": host_name + hosted_zone_name,\n                                \"Type\": type,\n                                \"TTL\": 60,\n                                \"ResourceRecords\": [\n                                    {\n                                        \"Value\": value\n                                    },\n                                ]\n                            }\n                        },\n                    ]\n                }\n            ). ",
    "hanibash": "@MaxGabriel Did you end up resolving this issue? We are seeing something similar:\nNoMethodError: undefined method `each' for nil:NilClass\nFile \"/app/vendor/bundle/ruby/2.2.0/gems/aws-sdk-core-2.3.0/lib/seahorse/client/plugin_list.rb\" line 18 in initialize\nFile \"/app/vendor/bundle/ruby/2.2.0/gems/aws-sdk-core-2.3.0/lib/seahorse/client/base.rb\" line 223 in new\nFile \"/app/vendor/bundle/ruby/2.2.0/gems/aws-sdk-core-2.3.0/lib/seahorse/client/base.rb\" line 223 in inherited\nFile \"/app/vendor/bundle/ruby/2.2.0/gems/aws-sdk-core-2.3.0/lib/aws-sdk-core/client.rb\" line 40 in initialize\nFile \"/app/vendor/bundle/ruby/2.2.0/gems/aws-sdk-core-2.3.0/lib/aws-sdk-core/client.rb\" line 40 in new\nFile \"/app/vendor/bundle/ruby/2.2.0/gems/aws-sdk-core-2.3.0/lib/aws-sdk-core/client.rb\" line 40 in define\nFile \"/app/vendor/bundle/ruby/2.2.0/gems/aws-sdk-core-2.3.0/lib/aws-sdk-core.rb\" line 440 in block in <module:Aws>\nFile \"/app/vendor/bundle/ruby/2.2.0/gems/aws-sdk-core-2.3.0/lib/aws-sdk-core.rb\" line 430 in call\nFile \"/app/vendor/bundle/ruby/2.2.0/gems/aws-sdk-core-2.3.0/lib/aws-sdk-core.rb\" line 430 in block in add_service\nFile \"/app/vendor/bundle/ruby/2.2.0/gems/aws-sdk-core-2.3.0/lib/aws-sdk-core.rb\" line 429 in each\nFile \"/app/vendor/bundle/ruby/2.2.0/gems/aws-sdk-core-2.3.0/lib/aws-sdk-core.rb\" line 429 in add_service\nFile \"/app/vendor/bundle/ruby/2.2.0/gems/aws-sdk-core-2.3.0/lib/aws-sdk-core/codecommit.rb\" line 1 in <top (required)>\nFile \"/app/vendor/bundle/ruby/2.2.0/gems/aws-sdk-core-2.3.0/lib/aws-sdk-core.rb\" line 389 in const_get\nFile \"/app/vendor/bundle/ruby/2.2.0/gems/aws-sdk-core-2.3.0/lib/aws-sdk-core.rb\" line 389 in block in sub_modules\nFile \"/app/vendor/bundle/ruby/2.2.0/gems/aws-sdk-core-2.3.0/lib/aws-sdk-core.rb\" line 388 in each\nFile \"/app/vendor/bundle/ruby/2.2.0/gems/aws-sdk-core-2.3.0/lib/aws-sdk-core.rb\" line 388 in inject\nFile \"/app/vendor/bundle/ruby/2.2.0/gems/aws-sdk-core-2.3.0/lib/aws-sdk-core.rb\" line 388 in sub_modules\nFile \"/app/vendor/bundle/ruby/2.2.0/gems/aws-sdk-core-2.3.0/lib/aws-sdk-core.rb\" line 376 in eager_autoload!\nFile \"/app/app/classes/firehose.rb\", line 131 in firehose. ",
    "mparramont": "\nSorry about giving you a screenshot instead of a textual stack trace, our error logging tool does not support exporting it :/\nThanks for your time!\n. We're using aws-sdk 2.2.7. Our bundle is only rebuilt on every deploy (we use Heroku), so it's not rebuilt when restarting AFAIK.\n. Will try that, thanks a lot for your support!\n. ",
    "peterkinmond": "Thanks for the quick reply. After more investigation, you're right, it wasn't the dotted bucket name, it was IAM policy that needed updating. I'm now able to call #put_object successfully on a dotted bucket.\n. ",
    "fgouteroux": "It's not a missing functionality from the sdk. We use an s3 compatible storage with Geo-IP, and our load-balancer send a 302 redirect. I'm not sure that this PR is right.\n. ",
    "hguo0303": "@awood45 Could you comment if this PR can be merged or what update is needed to get it merged? Thanks\n. Yeah, I agreed it would be more convenient to have access_key and session_token in the non-source profile. But I made these changes so that it can be uniform with python and java implementations. In botocore, for example, source profile is a mandatory field, and access_key and session_token are only retrieved from source profile. Please refer https://github.com/boto/botocore/blob/develop/botocore/credentials.py#L840-L866 for more information. I believe it is the same idea in java implementation.\n. ",
    "temujin9": "@awood45 FYI, the lack of this is forcing us to write horrid workarounds: https://github.com/bazaarvoice/cloudformation-ruby-dsl/pull/83\nPlease consider accepting this PR as an interim solution, possibly marked with \"alpha code\" warnings, if you are unable to reach an internal solution quickly.\n. Is there an ETA on this getting finished and merged? We have a large environment (including several ruby tools) which is moving to role assumption for our core architecture.\n. @awood45 Thanks for the follow-up. Can I ask you ping here when it does release?\n. Shipment received. Thanks, guys.\n. Aha! We had tried that earlier, but in the storm of problems (this was not the only one) we lost track of trying it again.\nI strongly vote for the removal of the flag. This is the documented way that aws configuration is supposed to work. Inobvious flags needed in addition end up confusing the issue.\nAs we have it patched (via version pinning) where we need it for the moment, I think I'm just going to leave it as is until you've decided. Can you ping here when you do?\n. ",
    "kdstew": "Thanks for the info.\n. ",
    "mwangm": "@bmurtagh @awood45   the code is very simple, It is just @ses_client.send_email payload.  I can't reproduce it in local ,  but it keep happened on prod. \n. ",
    "mtdowling": "Did you consider also utilizing the endpoint specific metadata for influencing the connection? For example, utilizing signaturerVersions, signing names, etc?\n. ",
    "thomasdziedzic": "I was able to fix this issue by upgrading our oj version.\nIt might still be worth fixing this for others though, since there is an implicit dependency on oj here.\n. ",
    "affix": "Edits are the benefits of using 2 computers to submit this bug...\n. @awood45 \nBy doing\nec2.instances.each do | i |\ni is returned as an instance object. So calling wait_for_stopped on the instance resource object stops the instance. I have misunderstood the documentation in that case.\nBut my instance ended up stopping and I don't understand how or why it stopped then.\n. Thanks for the very quick response to this! @awood45 quite impressed\n. ",
    "kayvonghaffari": "Great - thanks @awood45!\n. First time doing an Open Source Project Pull Request. Let me know if I'm doing this correctly!\nAlso, this feature has NOT been tested yet. What's the best way for me to test this in my development environment? Thanks!\n. ",
    "johan-smits": "Thnx for the quick fix. Waiting for the new release to come out.\n. ",
    "SMR39": "Fixed it was my bad reading the yaml file in a wrong way !! creds[:defaults][:aws_access_key_id]\nclosing this issue ... \n. ",
    "Techbrunch": "@awood45 is this still in the request backlog ?. @awood45 Awesome thanks for the quick fix !\n. Hum interesting, I just tried now and it works ? Did you do anything ?\n. Closing it as I cannot reproduce it for now. @awood45 how can I check what request is failing if this happen again in the future ? because there was clearly something going wrong at the time.\n. Go it again:\nopening connection to bucket.s3.amazonaws.com:443...\nopened\nstarting SSL for bucket.s3.amazonaws.com:443...\nSSL established\n<- \"HEAD / HTTP/1.1\\r\\nContent-Type: \\r\\nAccept-Encoding: \\r\\nUser-Agent: aws-sdk-ruby2/2.5.0 ruby/2.2.3 x86_64-linux resources\\r\\nX-Amz-Date: 20160816T193602Z\\r\\nHost: bucket.s3.amazonaws.com\\r\\nX-Amz-Content-Sha256: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\\r\\nAuthorization: AWS4-HMAC-SHA256 Credential=***/20160816/us-east-1/s3/aws4_request, SignedHeaders=host;x-amz-content-sha256;x-amz-date, Signature=72c1f54e43a99e11ae7ec806b083acede862b34b09840bf81b6434fda30515ff\\r\\nContent-Length: 0\\r\\nAccept: */*\\r\\n\\r\\n\"\n-> \"HTTP/1.1 400 Bad Request\\r\\n\"\n-> \"x-amz-request-id: 1BBCA6D05881677F\\r\\n\"\n-> \"x-amz-id-2: ftR5jxUqVEu/sxq8uGJCj1B6jSnRCgWFRZnahhFsGrbbSjv+zgGL9wPlYTlk1K/AdNehp6SP8bE=\\r\\n\"\n-> \"Content-Type: application/xml\\r\\n\"\n-> \"Transfer-Encoding: chunked\\r\\n\"\n-> \"Date: Tue, 16 Aug 2016 19:35:50 GMT\\r\\n\"\n-> \"Connection: close\\r\\n\"\n-> \"Server: AmazonS3\\r\\n\"\n-> \"\\r\\n\"\nConn close\n/home/ubuntu/.rbenv/versions/2.2.3/lib/ruby/gems/2.2.0/gems/aws-sdk-resources-2.5.0/lib/aws-sdk-resources/resource.rb:134:in `rescue in exists?': Aws::S3::Errors::BadRequest\n    from /home/ubuntu/.rbenv/versions/2.2.3/lib/ruby/gems/2.2.0/gems/aws-sdk-resources-2.5.0/lib/aws-sdk-resources/resource.rb:131:in `exists?'\n    from reproduce.rb:6:in `<main>'\n. @awood45 I have no way to check if the bucket was freshly created but this appear to be the issue.  This edge case seems to be handled by aws-shell, maybe you can have a look at how they handle this issue.\n. ",
    "musiaht": "Will it be possible to consider stricter version pinning?\n. @trevorrowe I meant pinning against jmespath; but after reading and  jmespath/jmespath.rb#18  jmespath/jmespath.rb#20 more carefully, I'm not sure if that's going to help.\nThanks for your help\n. ",
    "nybblr": "Correct\u2014for each client request, we generate a new URL, so signing info needs to be submitted through Headers, but there doesn't seem to be an API exposed for that.\nAre you suggesting I could perhaps derive the headers from the query params, and the signature would still be valid? I was thinking from Amazon's article that the entire request (including headers + params) gets signed, so the signature would differ based on using query params vs headers for authorization.\n. Sorry for the slow response. @trevorrowe Ah yes exactly that!\nIn the meantime, I don't suppose it's possible to derive those headers from the query params since the whole Request (with headers + url) is used to generate the signature?\n. @trevorrowe @awood45 any other clarifications needed? Or is this just waiting on the backlog to thin out a bit?\n. ",
    "benwells": "Thanks for the reply @awood45.   These objects have actually already been deleted, which could be the problem.   For each object, there are 2 versions.  generally version 1 is the point at which the file is uploaded, and version 2 is the delete marker.   I can see both versions when viewing the objects in the AWS console, and I can retrieve both versions using the SDK.  Here is a screenshot.\nMy goal here is to detect all objects that were deleted on a certain date and download their most recent, \"non delete marker\" version.    For these objects, I can retrieve the property last_modified on each object, as well as many other object properties, but I need to determine which object version is a delete_marker and which one isn't.  However, I can't seem to be able to call delete_marker without an error. \nThanks again and let me know if there is anymore info I can provide.\n. Thank you for that detail, that definitely sheds some more light on what's going on behind the scenes. \nJust in case someone else out there is battling with this and waiting in suspense for an answer, I wanted to throw out a really bad workaround that at least got me through the task I was working on.  This is horrible for a number of reasons, but:\nLoop through versions. For each version, call version.delete_marker.  If an exception is thrown, you probably found a delete marker! ;)\n``` rb\nversions.each do | version |\n  puts version.object #  #\nbegin\n    puts ver.object.delete_marker  # prints false for all non-delete marker versions\n  rescue\n    puts \"You may have just found a delete marker\"\n  end\nend\n```\n. That's a MUCH better workaround, thank you!  I will give that a shot.\n. ",
    "anthony-battaglia": "Thanks Alex.  I think I understand a little better now. In the case of raise_response_errors: false like the above example, response.error needs to be checked before response.unprocessed_items.  If response.error is not nil, then the whole batch failed.  Otherwise, the request (partially) succeeded and response.unprocessed_items can be inspected for any unwritten records.\n. ",
    "WaldynB": "No it does not have group or groups.  How do I find out what level I of the aws-sdk I am actually running?  It does not seem to support Aws::AutoScaling::AutoScalingGroup  either.\n. The version is 2.0.48.  What version do I need to support AutoScaling::AutoScalingGroup and the group method from Resource?\n. ",
    "marianafranco": "Closing... moving to aws-sdk 2.2.39 did the trick :+1: \n. I think having the version in which the method was included direct in the api doc, something like since: 2.2.x, will do better to help users to debug this type of problem :wink: \n. ",
    "hgani": "@trevorrowe Thanks for the quick response and elaborate explanation\nI'll check further what's causing the increased memory usage and will follow it up here if I find anything that is of relevance to this gem. \n. ",
    "kenjiskywalker": "OK, thanks. \ud83d\ude09 \n. ",
    "reist": "After trying the above and then trying to assume a role, looks like I wasn't careful enough reading every bit of the messages.\nThe errors and get_caller_identity did differ - exactly in the assumed role, with the instance credentials listing the IAM role set for the specific instance.\nReally sorry for the bother.\n. ",
    "ravsom": "I get this error for the region \"ap-south-1\" - Should I raise a new issue?\nS3 client configured for \"ap-south-1\" but the bucket \"dev-embryo-ui\" is in \"ap-south-1\"; Please configure the proper region to avoid multiple unnecessary redirects and signing attempts\n/home/travis/.rvm/gems/ruby-2.2.7/gems/aws-sdk-resources-2.10.38/lib/aws-sdk-resources/resource.rb:134:inrescue in exists?': Aws::S3::Errors::BadRequest\n    from /home/travis/.rvm/gems/ruby-2.2.7/gems/aws-sdk-resources-2.10.38/lib/aws-sdk-resources/resource.rb:131:in exists?'\nI've created the bucket in the same region as I ask Travis to deploy. Despite that, I run into this issue.. Not really. I changed the workflow to use s3 directly for static web\nhosting instead of directing it to eb.\nOn 24 Nov 2017 10:34 am, \"raheel0452\" notifications@github.com wrote:\n\n@ravsom https://github.com/ravsom did you ever get solution to above\nerror ? i'm saving same issue\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/aws/aws-sdk-ruby/issues/1161#issuecomment-346743647,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ADwkltSF_ivsEB0UprDE_1uDLcltT_ABks5s5k5KgaJpZM4IKK5f\n.\n. The deployment is triggered via travis-ci; the script is as below - along with the access keys and secrets which I have currently removed.\n\n```\nlanguage: node_js\nnode_js:\n- '6'\ndeploy:\n  provider: elasticbeanstalk\n  region: \"ap-south-1\"\n  app: opto-embryo-ui\n  bucket_name: dev-embryo-ui\n  env: OptoEmbryoUi-env\n```\n. ",
    "raheel0452": "@ravsom did you ever get solution to above error ? i'm saving same issue. @ravsom @awood45 any update on this ?. ",
    "imouaddine": "Thank you!\nOn Thu, Apr 28, 2016, 1:00 PM Alex Wood notifications@github.com wrote:\n\nClosed #1165 https://github.com/aws/aws-sdk-ruby/issues/1165.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly or view it on GitHub\nhttps://github.com/aws/aws-sdk-ruby/issues/1165#event-644818043\n. \n",
    "four2five": "I sorted it out and it was, indeed, a rookie ruby move. $encrypted_data should have simply been encrypted_data since it was a local variable and not a global.\n. ",
    "timanovsky": "@trevorrowe I apologize, I misunderstood the situation.  I was not aware of how original Net::HTTP code works, and particularly it's built in retry logic. So when I discovered this aws patch in my stack trace and saw retrying logic there I immediately thought it was wrong. Now I see the original Net::HTTP code and that your patch in fact disables this retry logic. Which makes sense. So apologies again. \nAnecdote: in my system transport_request is overridden three times: by aws lib, by newelic lib, by honeybadger lib. \n. ",
    "stickerbush": "We have isolated our problem with large uploads to an issue with jruby 9.0.\nhttps://github.com/jruby/jruby/issues/3435\n. ",
    "steventen": "Yeah, the doc does not mention that.\nAlso if I set virtual_host: true, it seems like I can not use https?\n. @awood45 thanks for the quick reply.\nI found I may misunderstood this virtual host thing. \nSo we are trying move from v1 to v2. With v1, we have code like this:\nruby\nobject.url_for(:read, expires: 7.days.to_i, secure: true, endpoint: '...', force_path_style: true)\nSince the url_for method is removed in v2, I started to use presigned_url, and I didn't see the endpoint option there, so I thought I could use virtual_host to achieve it (by using the hostname as the bucket name).\nNow looks like I can just pass the endpoint in Aws::S3::Resource.new, and then presigned_url could generate the similar thing like url_for without using virtual_host?\nruby\nAws::S3::Resource.new(endpoint: '...').bucket('foo').object('bar').presigned_url(...)\n. So we want to provide customers with download links for the files stored in s3. And these links must have expiration time. We want to use our own domain name for these links.\nWe haven't really enabled the endpoint thing in v1 in production, before switching to v2. It just run successfully in tests (now I understand it is just kind of designed for test.)\nSo looks like in order to acheive what we want, we have to use the domain name as the bucket name, and we can only use http?\n. ",
    "AndrewVos": "@awood45 I have a use case where I want to serve virtual host content behind a cloudfront distribution with https enabled. Know of any workaround for this?\nNot very useful but the code is here: https://github.com/AndrewVos/anmo/blob/master/app/controllers/aws_controller.rb#L23\n. ",
    "sarboc": "@awood45 Thanks for the prompt response! I am indeed using ObjectSummary as a resource equivalent to a #head_object call. You are also correct that I am working with large files (video source files), so loading the object is far from ideal. \n. Thanks for that workaround. It's doing fine on my end. Here's another, which I've been using:\nAws::S3::Object.new(bucket_name: BUCKET, key: KEY)\nI imagine this is working for the same reason you stated above - #get_object isn't being called on #load. I'm definitely not dead in the water here, but wanted to let you know that the method missing error was popping up.\n. Thanks for letting me know about the additional client configurations - changing the signature version worked like a charm. And then, of course, configuring the client with the proper bucket was the permanent solution. \nAlso, thank you for the excellent explanation of what changed in the authentication!\nLooks like I can't close this on my end, but it's resolved as far as I'm concerned.\n. ",
    "felixalias": "LGTM! Thanks for patching!\n. ",
    "joshuasiler": "Is it possible to change the default setting and just use POST across the board? A setting somewhere? \n. Yes this is exactly what is needed.\n. Great to hear. It looks like you are running ruby 2.4, which has known issues with this gem. Not sure if that's the problem but it seems like a candidate. \nSome folks are working on it https://github.com/aws/aws-sdk-ruby/issues/1461. ",
    "ali-l": "It looks like the java sdk implemented POST for CloudSearchDomain search requests. Would a similar implementation work for ruby? https://github.com/aws/aws-sdk-java/blob/fb1d42ee0e00dccf8a3290e04c367989ae5664fc/aws-java-sdk-cloudsearch/src/main/java/com/amazonaws/services/cloudsearchdomain/SwitchToPostHandler.java\n. ",
    "darcy": "Thanks for your response.\nI'm using an ec2/beanstalk role with IAM.\nWhen using a specified key/secret (on a user account, not a role) I didn't run into this issue and the expiration works fine. \nIs there something additional I need to set when using an IAM role? I am not clear how the session token is generated for these requests under the hood. Is there an expiration that needs to be set on that token as well that isn't getting set to match the expiration of the signed url?\n. I think I understand the problem a bit more now - the error I was getting was ExpiredToken, not that the content/url had expired.\nIf I am understanding it right, the session tokens provided to IAM via STS seem to have a maximum duration of 36hours, which means that the token will expire before before the URL, effectively making the maximum 36 hours for IAM users - is that correct?\n. Got it, thanks for the further clarification.\nSo if in normal use an IAM role would not able to set the expiration of the pre-signed URL to over the default session token expiration of 12 hours, then I'm thinking that might be useful to clarify in the documentation. Does the following seem reasonable?\n# @option params [Integer] :expires_in (900) Number of seconds before\n      #   the pre-signed URL expires. This may not exceed one week (604800\n      #   seconds). Note that the pre-signed URL is also only valid as long as the \n      #   credentials used to sign it are. For example, when using IAM roles, \n      #   temporary tokens generated for signing also have a default expiration \n      #   which will affect the effective expiration of the pre-signed URL.\nHappy to create a PR for this if you think that sums it up.\n. ",
    "ravi05cse": "I am using following gems\ngem 'aws-sdk', '1.66.0'\ngem 'aws-sdk-resources', '2.2.12'\n. Yes i am setting :region. Also i want to share you that I am using thread pool here\n. ",
    "hosh": "For now, I have this monkeypatch in my code:\n```\nmodule Aws\n  module Rest\n    module Response\n      class Headers\n    def cast_value(ref, value)\n      case ref.shape\n      when StringShape then value\n      when IntegerShape then value.to_i\n      when FloatShape then value.to_f\n      when BooleanShape then value == 'true'\n      when TimestampShape\n        if value =~ /\\d+(\\.\\d*)/\n          Time.at(value.to_f)\n        else\n          maybe_parse_time(value)\n        end\n      else raise \"unsupported shape #{ref.shape.class}\"\n      end\n    end\n\n    private\n\n    def maybe_parse_time(value)\n      Time.parse(value)\n    rescue ArgumentError => e\n      raise e unless e.message =~ /^no time information in/\n      value\n    end\n  end\nend\n\nend\nend\n```\nI'm not entirely satisfied with it because:\n1. I think there is a reasonable expectation that what comes out of Expires is an http-date (as noted in the RFC), that is enforceable by AWS and not on client side. I don't know if this is because that particular object was passed \"5d\" in the expires parameter when creating the S3 object, or some slipup the team responsible for S3 has. I don't know anyone in the AWS server team, but maybe you do?\n2. I tried to isolate the exception being raised. Not sure if I like that regexp. If S3 consistently sends non-compliant Expires headers, that exception code gets run every single time. (Granted, the network round-trip is probably longer than the time it takes for Ruby to do that extra processing).\n3. This only happens for Expires header, but the patch effects all time-shape. I don't know if that is a good thing or a bad thing. The AWS CLI doesn't do any enforcement. Maybe I'm being a type fanatic for no good purpose?\nIn the code I am working on, there is no existing reliance on expecting that to be a Time. On the other hand, if the S3 is sending back out invalid http-date fields, then this is moot. \nI kinda wonder if there is a way to have something like https://medium.com/learnings-in-and-around-sharetribe/option-pattern-in-ruby-7b0f7c5abdb6#.aremtyrbl but that'd still require code change.\n. @ArthurN using alias or alias_method doesn't work for you?\nBut yeah, I saw the 0 value pop up too for us.\n. ",
    "ArthurN": "I have a related but slight variation of the problem. For us, the Expires values are all \"0\". The code attempts to use Time#at if it's numerical, which would handle it appropriately (i.e. Time.at(0) #=> 1969-12-31 17:00:00 -0700); however, the regular expression in line 38 of that file is a little too specific in that it requires a period. Finding no period, it passes the value to Time#parse instead. Thus, like @hosh, I get a ArgumentError: no time information in \"0\"\n. Here is my monkeypatch. Note the ? I added in the regex. \n(@hosh This monkeypatch approach (taken from this snippet) helps isolate the code changes a little bit better, in that for all other ref.shape types it defers to the original source. You could modify to run your maybe_parse_time)\n``` ruby\nmodule Aws\n  module Rest\n    module Response\n      class Headers\n        # HACKFIX monkeypatch: see https://github.com/aws/aws-sdk-ruby/issues/1184\n        cast_value_method = instance_method(:cast_value)\n    define_method(:cast_value) do |ref, value|\n      if ref.shape.is_a?(TimestampShape)\n        if value =~ /\\d+(\\.?\\d*)/\n          Time.at(value.to_f)\n        else\n          Time.parse(value)\n        end\n      else\n        cast_value_method.bind(self).call(ref, value)\n      end\n    end\n  end\nend\n\nend\nend\n```\nJust be sure to run this after the AWS SDK gem is loaded. \n. @hosh Something like alias_method :old_cast_value, :cast_value, you mean? I'm sure that would work, too.\n. ",
    "psienko": "I didn't notice this error after adding Aws.eager_autoload!. Now, I'm not sure but it seems to me that before I reported this issue I also checked greater versions than 2.3.2 and this problem still existed.\n. ",
    "Maerig": "Hi, I have the exact same problem using either the CLI or the web interface, and as suggested it doesn't come from the IAM policies for the Lambda function, but those of the user updating its configuration. In particular, there is a restriction on the IP addresses:\n{\n    ...\n    \"Statement\": [\n     ...\n        {\n            \"Effect\": \"Deny\",\n            \"Action\": \"*\",\n            \"Resource\": \"*\",\n            \"Condition\": {\n                \"NotIpAddress\": {\n                    \"aws:SourceIp\": [\n                        <IP addresses used by the company>\n                    ]\n                }\n            }\n        }\n    ]\n}\nMaybe you're using a similar policy? In my case, disabling this restriction allows me to update the VPC configuration normally, but I do wish there was a better way.\n. ",
    "EslamElHusseiny": "my bad!\ndidn't notice that the reservation might have more than an instance.\n. ",
    "sjmiller609": "This one got me too. For the next guy: don't forget to iterate through both Reservations and Instances... don't just grab the first Reservation.. ",
    "rejeep": "For anyone else with this issue, I solved it like this:\nruby\nretries = 0\nbegin\n  poller = Aws::SQS::QueuePoller.new(url, client: client)\n  poller.poll do |message|\n    # ...\n  end\nrescue Seahorse::Client::NetworkingError => e\n  if (retries += 1) < 10\n    logger.warn('Unexpected networking error occured')\n    logger.warn(e.message)\n    sleep 10\n    retry\n  end\nend\n. Would that help though? Are you saying that the exception occurs when there are no more retries? My impression was that the exception occurred at any point and aborted the retry chain.\n. ",
    "beanieboi": "hey @trevorrowe \nthanks for the suggestion, i will add a retry logic around the tagging. since tagging is a idempotent operation, this should be save to retry.\nit would be really great if there was a way to provide the tags with the run_instances call. this would avoid a lot of problems. but i guess this would just move the problem from my code to your code. not sure what the best is to solve this :)\nthanks!\nben\n. ",
    "sarge": "The idle timeout as I understand it supports stopping polling if there are no more messages.\nMy situation is that polling or processing has stopped processing. Possibly due to something being hung inside the poll method, which my proposed code I think will fix. \nOr inside the poller internally which I have just realized my code won't fix.\nCould this block indefinitely?\nhttps://github.com/aws/aws-sdk-ruby/blob/ef9f2d3655c4bacb3adff580a6b295df510f78fe/aws-sdk-resources/lib/aws-sdk-resources/services/sqs/queue_poller.rb#L390\n. I understand, the network does throw a lot of non determinism into the equation. \nOn the assumption that poll cannot block. I will test for a possible timeout from the message processing block. If it hangs again I will reopen.\nRevised example\n``` ruby\nsqs = Aws::SQS::Client.new(region: queue[:region])\n      poller = Aws::SQS::QueuePoller.new(queue[:url], { client: sqs })\n  poller.before_request do |stats|\n    logger.info(\"region: #{queue[:region]} requests: #{stats.request_count} - messages: #{stats.received_message_count} - last-timestamp: #{stats.last_message_received_at}\")\n  end\n\n  poller.poll do |msg|\n\n    begin\n      Timeout::timeout(10) {\n        logger.info \"processing...\"\n        # sleep 15\n        process_message(msg, queue)\n      }\n    rescue Timeout::Error => ex\n      logger.error msg.inspect\n      logger.error ex\n      throw :skip_delete\n    rescue Exception => ex\n      logger.error msg.inspect\n      logger.error ex\n      throw :skip_delete\n    end\n\n  end\n\n```\n. ",
    "nickfrandsen": "I can connect to my own S3 account perfectly fine so I'm not saying that the ruby client isn't working but for some reason this particular clients S3 isn't working. I am wondering if it perhaps could be due to them having a forward slash in the bucket? It's allowed on us-east-1 but I can't think of what else it could be. Does anyone know why this particular bucket would fail with a signature error?\n. Thanks guys! \nUsing \nresp = s3.list_objects(bucket: 'bcteam-1b299a00-4ea3-2f92-b048-03f6ca07e7f0', prefix:'15_minute_data')\nFixed my particular issue but probably a good idea with a helpful error message. \nCheers\n. ",
    "thiagofelix": "After some research I found that the EB CLI that I mentioned is the awsebcli. Which I believe might not mirror the AWS API based on this section on EB documentation:\n\nPreviously, Elastic Beanstalk supported a separate CLI that provided direct access to API operations called the Elastic Beanstalk API CLI. This has been replaced with the AWS CLI, which provides the same functionality but for all AWS services' APIs.\n. In practice when you use --single in the EB CLI it doesn't create a load balancer for you environment\n\n\nI think what I am trying to understand is if it is possible to reproduce the same \"Environment Type\" with the aws-cli or the aws-sdk-ruby?\n. Awesome.\nThanks @awood45, fantastic work!\n. ",
    "kushkella": "I did not have any luck yet. I'm going to try to run this example outside of my application and let you know the results. One think to note is that I've both version of SDK installed in my application.\n. I'm not sure if there is a problem with my environment or not but some for reason using IP address 127.0.0.1 instead of domain localhost improves the performance drastically. I imagine the client is trying to resolve the address to an IP address for some reason. Can you confirm if there is anything that forces it to resolve the address in some manner that would slow it down?\n. ",
    "RedaBenh": "you can use this quick and dirty method to convert\n(this manage only the basic types)\n```\ndef convert_dynamodb_stream_to_hash(db_record)\n  records = {}\n  db_record.each do |k, v|\n    val = nil\n     v.each do |key, value|\n      val = case key.to_s\n            when 'S' then value.to_s\n            when 'N' then value.to_i\n            when 'L' then value.map{|i| i.values}.flatten\n            when 'M' then convert_dynamodb_stream_to_hash(value)\n            else value\n          end\n    end\n    records[k] = val\n  end\n  records\nend\n```. ",
    "eredi93": "+1. ",
    "ecnepsnai": "Sorry I probably should've explained my structure before asking this question:\nThe app is a local transit app, and I would use SNS for service notifications.  For each service & severity of alert I have a topic, for example:\n- bus_major\n- bus_minor\n- train_major\n- train_minor\nThese topics are fixed, I.E.: I created then in the AWS control panel and not pragmatically.\nUsers can filter which service and severity they receive notifications for, but what I would like is to be able to query SNS to see which services & severities they are receiving so I don't have to rely on a local copy (which I can keep locally)\nI could list all subscriptions then just map reduce them to the specific endpoint ARN, but that would be quick wasteful both on our side and mine.\nPerhaps there is another way?  Or perhaps there is a better structure than what I am using right now?\n. ",
    "jkiran307": "Hi trevorrowe,\nThis really helped me.\nRegards,\nKiran307. ",
    "ljfranklin": "\nI can forward this issue to the service team\n\nMuch appreciated. Agreed that it's not worth adding a workaround in the SDK, hopefully an easy fix for the server-side team. Closing this out, thanks!\n. ",
    "adityajain21": "I need to use a command which will trigger a code deploy, and use that command from inside the EC2 instance(I am logged into the instance). Is it possible?\n. ",
    "abhi-patel": "@awood45 : Wouldn't it be too much to load on startup by adding Aws.eager_autoload! considering we need s3 and Dynamodb only.\n@trevorrowe \nHave added .eager_autoload! in initializer and have checked that its requiring correct files. \nThere is one more such error\n1) uninitialized constant Aws::Json::Handler::Parser\n  /usr/local/rvm/gems/ruby-2.3.0@abc_gemset/gems/aws-sdk-core-2.3.2/lib/aws-sdk-core/json/handler.rb:44:in `parse_body'\nOne more point I am not able to get is Aws::Plugins::UserAgent::Handler::VERSION should resolve to Aws::VERSION according to rails autoload mechanism, Any Idea why its not being found correctly?\n. Opps. My bad. The issue is solved now. Basically Resque does not run before_eager_load hooks on setup and hence AWS eager load was not working for resque workers.\nThe same code was working fine for web/console requests, but was randomly failing for resque workers.\nHave opened a pull request for the same. resque/resque#1481\n. ",
    "ge1st": "Hi guys, I just wanted to notify you that I saw similar behavior using the latest of the Version 1 line. I was also using multiple threads. Adding eager_autoload! to my initialize looks to have corrected it.\n. ",
    "csuhta": "The RFCs are really vague about this header (surprising absolutely no-one). No mention is made about percent-encoding though.\nMy workaround so far has been to transliterate characters: removing accents and replacing emoji with substitutes, etc.\n. ",
    "erikogan": "It just occurred to me that calling the original_put_object method might be at least partially to blame for this. I am going to try refactoring that out.\n. Strangely, commenting out that line (and replacing it with an empty hash return) causes a similar error in 2.3.14, not the NoMethodError on nil I expected.\nSomething very strange is afoot.\n. @trevorrowe Yup, this is an integration test. \nThat looks like a much cleaner way to assert that the given operation is called with the expected arguments and extract arguments for testing. I\u2019ll try refactoring that way and see what happens.\n. Interesting. I tried to add requests to a RSpec memoized value, and I got the StackOverflowError again. I think there\u2019s a strange interaction going on between aws-sdk & RSpec. I\u2019ll investigate further tomorrow morning.\n. I figured out a workaround for that particular problem so I probably won\u2019t look at it tomorrow morning. But it intrigues me so I may poke at it in the future.\n. I just tried to upgrade to aws-sdk 2.4.0 (and RSpec 3.5.1, but probably not relevant), and I am still getting StackOverflowError errors on the refactored specs. I\u2019ll dig into it a bit and post the updated specs here if I don\u2019t discover something obvious.\n. The (slightly simplified) spec now looks like this: \n``` ruby\nRSpec.describe MyController, type: :controller do\n  let(:s3) { Aws::S3::Resource.new }\n  let(:s3path) { 'a/random/s3/test_file.csv' }\nbefore do\n    Aws.config[:s3] = {\n      stub_responses: {\n        list_objects: { contents: [{ key: s3path }] },\n        get_object: { body: File.read(Rails.root.join('spec/fixtures/signature_data.txt')) },\n      }\n    }\nallow(RedshiftExporter::CONNECTION_POOL).to receive(:with)\n\nallow(Aws::S3::Resource).to receive(:new).and_return(s3)\ns3client = s3.instance_variable_get('@client')\n\n# There is a very strange interaction between aws-sdk & RSpec memoized\n# values that causes a StackOverflowError when putting SDK values into\n# memoized data structures. \nscoped_file_data = {}\n@file_data = scoped_file_data\n\ns3client.handle do |request| # register a handler in the request handler stack\n  if request.operation_name.to_s == 'put_object'\n    scoped_file_data[:csv_file] = StringIO.new(request.params[:body], 'r:UTF-16le')\n  end\n  @handler.call(request)\nend\n\nend\nsubject do\n    @csv_file.read(3) # Remove BOM\n    CSV.new(@csv_file, col_sep: \"\\t\")\n  end\nit 'should have headers' do\n    Sidekiq::Testing.inline! do\n      post :create, params: params, body: body.to_json\n    end\nexpect(subject.shift).to eq headers\n\nend\nend\n```\nI just confirmed that rolling back to 2.3.14 resolves the problem. I am going to see if I can step through and track where the infinite loop happens.\n. Eureka! It is not clear to me why this causes jRuby to overflow its stack, but the problem is that I am passing put_object a body that is an unlinked Tempfile object.\nAws::Plugins::S3Md5s::Handler.call tries to add an MD5 header to the request\nAws::Checksums.md5 correctly identifies it as a Tempfile object, calling\nOpenSSL::Digest::MD5.file which calls File.open(file, 'rb'). \nThis works fine (to my surprise) when the Tempfile still exists in the filesystem, but fails when the file has been unlinked. This appears to be because calling File.open on an existing File object tries to reopen the same file name, instead of dup\u2019ing the underlying fd. I just tested the behavior in MRI 2.0, and acts similarly.\nAttempting to reproduce that error in isolation: \nruby\ntf = Tempfile.new\ntf.unlink\ntf.puts \"hi!\"\nFile.open(tf, \"rb\") {|f| f.read}\nCauses a much more understandable error: TypeError: no implicit conversion of nil into String. (And commenting out the unlink causes it to work.)\nThis leads me to believe that something in the jRuby exception handling (or possibly RSpec\u2019s) is what is really causing the stack to overflow.\nHowever, I do think that aws-sdk should be able to upload data from a file descriptor that has been unlinked in the filesystem without erroring (as it was in 2.3.14).\n. Fortunately it was a pretty easy fix. The code already exists to read from the filehandle and rewind.\n. @trevorrowe See the attached PR. The file can be unlinked in the filesystem, but open file descriptors continue to have access to it until they are closed. The filesystem inodes are only freed once all descriptors have been closed.\nIt is common practice with unix tools to unlink temporary files as soon as they are created, both for security as well as housekeeping reasons.\n. Sorry, I should have been clearer in my description. \nYes, that is exactly as intended, but it won\u2019t fail the next step (at least not on a POSIX-based system (Linux/BSD/OS X, etc)), because the kernel will keep the file available to the open file descriptor wrapped by the Ruby File object. Syscalls wrapped by Ruby File methods will continue to work, so it is still possible to read(2) data from the file object and then rewind(3) the fd back to the beginning for the subsequent read to upload.\n\u2026Until the fd/File object is close(2)\u2019d, at which point you lose access to it (and the filesystem allowed to free it if you were the last fd open).\nThat\u2019s why it is common practice with *nix tools to unlink temporary files as soon as they are created, both for security as well as housekeeping reasons. They know they\u2019ll be the only one with access to the file and it will be reclaimed soon after they are done with it.\n. Incidentally, I tested this fix before posting the PR and it solves the issue I was seeing in #1232. \n. Great! Thanks for adding the test. I should have looked harder for where to add one. I feel better knowing there is one.\n. ",
    "fdr": "Yeah, I'm using Sidekiq in a typical way.\nDoes eager_autoload! also load every part of the SDK? That's a cure worse than the disease, in my case...\n. On Thu, Jul 7, 2016 at 4:33 PM Alex Wood notifications@github.com wrote:\n\nIt does, but you can call it with only a subset of services as well. For\nexample: Aws.eager_autoload!(services: %w(S3 EC2))\nThat could help a great deal: I will do this. Thank you for your help.\n. Well...I can? 100%. Here's the full trace.\n\n``\nArgumentError: missing required parameter params[:is_truncated]\n    from /home/fdr/.rbenv/versions/2.4.2/lib/ruby/gems/2.4.0/gems/aws-sdk-core-3.6.0/lib/aws-sdk-core/param_validator.rb:32:invalidate!'\n    from /home/fdr/.rbenv/versions/2.4.2/lib/ruby/gems/2.4.0/gems/aws-sdk-core-3.6.0/lib/aws-sdk-core/param_validator.rb:13:in validate!'\n    from /home/fdr/.rbenv/versions/2.4.2/lib/ruby/gems/2.4.0/gems/aws-sdk-core-3.6.0/lib/aws-sdk-core/client_stubs.rb:257:indata_to_http_resp'\n    from /home/fdr/.rbenv/versions/2.4.2/lib/ruby/gems/2.4.0/gems/aws-sdk-core-3.6.0/lib/aws-sdk-core/client_stubs.rb:242:in http_response_stub'\n    from /home/fdr/.rbenv/versions/2.4.2/lib/ruby/gems/2.4.0/gems/aws-sdk-core-3.6.0/lib/aws-sdk-core/client_stubs.rb:209:indefault_stub'\n    from /home/fdr/.rbenv/versions/2.4.2/lib/ruby/gems/2.4.0/gems/aws-sdk-core-3.6.0/lib/aws-sdk-core/client_stubs.rb:197:in block in next_stub'\n    from /home/fdr/.rbenv/versions/2.4.2/lib/ruby/gems/2.4.0/gems/aws-sdk-core-3.6.0/lib/aws-sdk-core/client_stubs.rb:194:insynchronize'\n    from /home/fdr/.rbenv/versions/2.4.2/lib/ruby/gems/2.4.0/gems/aws-sdk-core-3.6.0/lib/aws-sdk-core/client_stubs.rb:194:in next_stub'\n    from /home/fdr/.rbenv/versions/2.4.2/lib/ruby/gems/2.4.0/gems/aws-sdk-core-3.6.0/lib/aws-sdk-core/plugins/stub_responses.rb:43:incall'\n    from /home/fdr/.rbenv/versions/2.4.2/lib/ruby/gems/2.4.0/gems/aws-sdk-core-3.6.0/lib/seahorse/client/plugins/content_length.rb:12:in call'\n    from /home/fdr/.rbenv/versions/2.4.2/lib/ruby/gems/2.4.0/gems/aws-sdk-core-3.6.0/lib/aws-sdk-core/xml/error_handler.rb:8:incall'\n    from /home/fdr/.rbenv/versions/2.4.2/lib/ruby/gems/2.4.0/gems/aws-sdk-core-3.6.0/lib/aws-sdk-core/plugins/signature_v4.rb:64:in call'\n    from /home/fdr/.rbenv/versions/2.4.2/lib/ruby/gems/2.4.0/gems/aws-sdk-core-3.6.0/lib/aws-sdk-core/plugins/helpful_socket_errors.rb:10:incall'\n    from /home/fdr/.rbenv/versions/2.4.2/lib/ruby/gems/2.4.0/gems/aws-sdk-core-3.6.0/lib/aws-sdk-core/rest/handler.rb:8:in call'\n    from /home/fdr/.rbenv/versions/2.4.2/lib/ruby/gems/2.4.0/gems/aws-sdk-core-3.6.0/lib/aws-sdk-core/plugins/user_agent.rb:13:incall'\n    from /home/fdr/.rbenv/versions/2.4.2/lib/ruby/gems/2.4.0/gems/aws-sdk-core-3.6.0/lib/seahorse/client/plugins/endpoint.rb:45:in call'\n    from /home/fdr/.rbenv/versions/2.4.2/lib/ruby/gems/2.4.0/gems/aws-sdk-route53-1.3.0/lib/aws-sdk-route53/plugins/id_fix.rb:23:incall'\n    from /home/fdr/.rbenv/versions/2.4.2/lib/ruby/gems/2.4.0/gems/aws-sdk-core-3.6.0/lib/aws-sdk-core/plugins/param_validator.rb:24:in call'\n    from /home/fdr/.rbenv/versions/2.4.2/lib/ruby/gems/2.4.0/gems/aws-sdk-core-3.6.0/lib/seahorse/client/plugins/raise_response_errors.rb:14:incall'\n    from /home/fdr/.rbenv/versions/2.4.2/lib/ruby/gems/2.4.0/gems/aws-sdk-core-3.6.0/lib/aws-sdk-core/plugins/jsonvalue_converter.rb:20:in call'\n    from /home/fdr/.rbenv/versions/2.4.2/lib/ruby/gems/2.4.0/gems/aws-sdk-core-3.6.0/lib/aws-sdk-core/plugins/idempotency_token.rb:17:incall'\n    from /home/fdr/.rbenv/versions/2.4.2/lib/ruby/gems/2.4.0/gems/aws-sdk-core-3.6.0/lib/aws-sdk-core/plugins/param_converter.rb:24:in call'\n    from /home/fdr/.rbenv/versions/2.4.2/lib/ruby/gems/2.4.0/gems/aws-sdk-core-3.6.0/lib/aws-sdk-core/plugins/response_paging.rb:10:incall'\n    from /home/fdr/.rbenv/versions/2.4.2/lib/ruby/gems/2.4.0/gems/aws-sdk-core-3.6.0/lib/seahorse/client/plugins/response_target.rb:23:in call'\n    from /home/fdr/.rbenv/versions/2.4.2/lib/ruby/gems/2.4.0/gems/aws-sdk-core-3.6.0/lib/seahorse/client/request.rb:70:insend_request'\n    from /home/fdr/.rbenv/versions/2.4.2/lib/ruby/gems/2.4.0/gems/aws-sdk-route53-1.3.0/lib/aws-sdk-route53/client.rb:3305:in list_resource_record_sets'\n    from (irb):5\n    from /home/fdr/.rbenv/versions/2.4.2/bin/irb:11:in'\n```. Oh, no problems. Thanks.. ",
    "antoniobeyah": "Same issue here, even with jmespath 1.3.0.  Although the error message is slightly different.  It appears that adding json_pure v2.0.1 causes this to blow up, but using 1.8.3 the error goes away.\nTo reproduce:\nGemfile\n``` ruby\nsource 'https://rubygems.org'\ngem 'aws-sdk', '= 2.3.21'\ngem 'json_pure', '= 2.0.1'\n```\nExcerpt from Gemfile.lock\nspecs:\n    aws-sdk (2.3.21)\n      aws-sdk-resources (= 2.3.21)\n    aws-sdk-core (2.3.21)\n      jmespath (~> 1.0)\n    aws-sdk-resources (2.3.21)\n      aws-sdk-core (= 2.3.21)\n    jmespath (1.3.0)\n    json_pure (2.0.1)\nscript.rb\n``` ruby\n!/usr/bin/env ruby\nrequire 'aws-sdk'\nec2 = ::Aws::EC2::Client.new(region: 'us-west-2')\nthis must point to an instance id that exists to cause the error\nec2.wait_until(:instance_exists, instance_ids: ['i-ebb9547f'])\n```\n-> bundle exec script.rb\nJMESPath::Errors::SyntaxError: unknown token \"0\"\n  /Users/USERNAME/.rvm/gems/ruby-2.2.5@all/gems/jmespath-1.3.0/lib/jmespath/parser.rb:159:in `nud_unknown'\n  /Users/USERNAME/.rvm/gems/ruby-2.2.5@all/gems/jmespath-1.3.0/lib/jmespath/parser.rb:61:in `expr'\n  /Users/USERNAME/.rvm/gems/ruby-2.2.5@all/gems/jmespath-1.3.0/lib/jmespath/parser.rb:165:in `led_comparator'\n  /Users/USERNAME/.rvm/gems/ruby-2.2.5@all/gems/jmespath-1.3.0/lib/jmespath/parser.rb:63:in `expr'\n  /Users/USERNAME/.rvm/gems/ruby-2.2.5@all/gems/jmespath-1.3.0/lib/jmespath/parser.rb:39:in `parse'\n  /Users/USERNAME/.rvm/gems/ruby-2.2.5@all/gems/jmespath-1.3.0/lib/jmespath/caching_parser.rb:25:in `block in cache_expression'\n  /Users/USERNAME/.rvm/gems/ruby-2.2.5@all/gems/jmespath-1.3.0/lib/jmespath/caching_parser.rb:23:in `synchronize'\n  /Users/USERNAME/.rvm/gems/ruby-2.2.5@all/gems/jmespath-1.3.0/lib/jmespath/caching_parser.rb:23:in `cache_expression'\n  /Users/USERNAME/.rvm/gems/ruby-2.2.5@all/gems/jmespath-1.3.0/lib/jmespath/caching_parser.rb:16:in `parse'\n  /Users/USERNAME/.rvm/gems/ruby-2.2.5@all/gems/jmespath-1.3.0/lib/jmespath/runtime.rb:56:in `search'\n  /Users/USERNAME/.rvm/gems/ruby-2.2.5@all/gems/jmespath-1.3.0/lib/jmespath.rb:32:in `search'\n  /Users/USERNAME/.rvm/gems/ruby-2.2.5@all/gems/aws-sdk-core-2.3.21/lib/aws-sdk-core/waiters/poller.rb:70:in `matches_path?'\n  /Users/USERNAME/.rvm/gems/ruby-2.2.5@all/gems/aws-sdk-core-2.3.21/lib/aws-sdk-core/waiters/poller.rb:65:in `acceptor_matches?'\n  /Users/USERNAME/.rvm/gems/ruby-2.2.5@all/gems/aws-sdk-core-2.3.21/lib/aws-sdk-core/waiters/poller.rb:49:in `block in call'\n  /Users/USERNAME/.rvm/gems/ruby-2.2.5@all/gems/aws-sdk-core-2.3.21/lib/aws-sdk-core/waiters/poller.rb:48:in `each'\n  /Users/USERNAME/.rvm/gems/ruby-2.2.5@all/gems/aws-sdk-core-2.3.21/lib/aws-sdk-core/waiters/poller.rb:48:in `call'\n  /Users/USERNAME/.rvm/gems/ruby-2.2.5@all/gems/aws-sdk-core-2.3.21/lib/aws-sdk-core/waiters/waiter.rb:104:in `block in poll'\n  /Users/USERNAME/.rvm/gems/ruby-2.2.5@all/gems/aws-sdk-core-2.3.21/lib/aws-sdk-core/waiters/waiter.rb:101:in `loop'\n  /Users/USERNAME/.rvm/gems/ruby-2.2.5@all/gems/aws-sdk-core-2.3.21/lib/aws-sdk-core/waiters/waiter.rb:101:in `poll'\n  /Users/USERNAME/.rvm/gems/ruby-2.2.5@all/gems/aws-sdk-core-2.3.21/lib/aws-sdk-core/waiters/waiter.rb:91:in `block (2 levels) in wait'\n  /Users/USERNAME/.rvm/gems/ruby-2.2.5@all/gems/aws-sdk-core-2.3.21/lib/aws-sdk-core/waiters/waiter.rb:90:in `catch'\n  /Users/USERNAME/.rvm/gems/ruby-2.2.5@all/gems/aws-sdk-core-2.3.21/lib/aws-sdk-core/waiters/waiter.rb:90:in `block in wait'\n  /Users/USERNAME/.rvm/gems/ruby-2.2.5@all/gems/aws-sdk-core-2.3.21/lib/aws-sdk-core/waiters/waiter.rb:89:in `catch'\n  /Users/USERNAME/.rvm/gems/ruby-2.2.5@all/gems/aws-sdk-core-2.3.21/lib/aws-sdk-core/waiters/waiter.rb:89:in `wait'\n  /Users/USERNAME/.rvm/gems/ruby-2.2.5@all/gems/aws-sdk-core-2.3.21/lib/aws-sdk-core/client_waiters.rb:110:in `wait_until'\n  script.rb:7:in `<top (required)>'\n. ",
    "archangel189": "V2\nOn 8 Jul 2016 2:31 AM, \"Alex Wood\" notifications@github.com wrote:\n\nWhat version of the SDK are you using?\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/aws/aws-sdk-ruby/issues/1236#issuecomment-231133712,\nor mute the thread\nhttps://github.com/notifications/unsubscribe/AAU82WY1JGvRpCrh_0BIjwog52rNbEbVks5qTSnxgaJpZM4JGnEB\n.\n. Right. I was using v2.2.37. I see that this feature was only released in v2.3.2. I guess you can close this issue.\n. \n",
    "Dariusch": "JMESPath version is 1.2.4\nruby version is 2.3.0\n. As far as I can tell the issue is resolved.\nThere are no more errors with the new jmespath version 1.3.1\n. ",
    "fedot": "This doesn't looks related to jmespath (and using 1.3.1 doesn't resolve those issues)\n3 out of 4 posted by OP still happen for us.\n- Ruby 2.3.1\n- aws-sdk-ruby 2.4.0\n- jmespath 1.3.1\n. ",
    "blondowski": "From: Alex Wood notifications@github.com<mailto:notifications@github.com>\nReply-To: aws/aws-sdk-ruby reply@reply.github.com<mailto:reply@reply.github.com>\nDate: Monday, July 11, 2016 at 4:10 PM\nTo: aws/aws-sdk-ruby aws-sdk-ruby@noreply.github.com<mailto:aws-sdk-ruby@noreply.github.com>\nCc: Dan Blondowski dan.blondowski@dhigroupinc.com<mailto:dan.blondowski@dhigroupinc.com>, Author author@noreply.github.com<mailto:author@noreply.github.com>\nSubject: Re: [aws/aws-sdk-ruby] undefined method `match' for nil:NilClass (NoMethodError) when opening encryptionClient (#1240)\nRelated to #1237https://github.com/aws/aws-sdk-ruby/issues/1237\nThis means you haven't configured a region. To configure a region, set the AWS_REGION environment variable, or pass it in as a parameter. For example:\nkey = OpenSSL::PKey::RSA.new(1024)\ns3 = Aws::S3::Encryption::Client.new(encryption_key: key, region: \"us-east-1\") # or the region you are using\n\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHubhttps://github.com/aws/aws-sdk-ruby/issues/1240#issuecomment-231866239, or mute the threadhttps://github.com/notifications/unsubscribe/AGsPJTfOr2XXmtRVqfx5Iq7PYvau7tTlks5qUrFYgaJpZM4JJxUl.\nThis email has been scanned for email related threats and delivered safely by Mimecast.\nFor more information please visit http://www.mimecast.com\n. ",
    "edjames": "DOH! Sorry, I should've tested the latest version. My apologies, and thanks very much for your prompt response.\n. ",
    "dpmanoso": "Hello, thank you for your time. Tried using aws-sdk (1.40.3) and my custom puppet fact now works fine. Though I'm not really sure why does it says syntax in aws-sdk-resources (2.3.22) gem\n. ",
    "johnbarney": "If I do .describe_auto_scaling_groups(auto_scaling_group_names: [\"example-asg\"]) I receive one asg back and it's the correct one. I have verified the console reflects all instances are InService and Healthy.\n```\n\"], health_check_type=\"ELB\", health_check_grace_period=0, instances=[#], termination_policies=[\"Default\"], new_instances_protected_from_scale_in=false>], next_token=nil>\n```\n. +1\nThank you Trevor\n. ",
    "hamadata": "@cjyclaire \nMay I close this issue?. #1255 is the correspondent issue.\n. [memo]\nThis conflict is caused by this commit\nhttps://github.com/aws/aws-sdk-ruby/commit/d3981c8997baa66897ee5c3c02b86ab6b1edc0e1\nIn this PR, most of the lines are moved to singer.rb, which includes the change shown in the link above.\nJSON.dump(json_hash)\n. Sorry, I will create a new pull-request.. #1255 is the correspondent issue.. @cjyclaire\nHi, I have created a new PR.. @cjyclaire \nHow about removing #signed_url from Signer?\nsigned_url is only used in UrlSinger, not in CookieSigner.. I will fix it.. I will fix it.. like this?\ndef scheme_and_uri(url)\n        url_sections = url.split('://')\n        if url_sections.length < 2\n          raise ArgumentError, \"Invaild URL:#{url}\"\n        end\n        scheme = url_sections[0].gsub('*', '')\n        uri = \"#{scheme}://#{url_sections[1]}\"\n        [scheme, uri]\n      end\n. :+1: . :+1: . I removed the comment because this comment was the usage of UrlSigner.\nThe class Signer is a set of methods, not used directly.. ",
    "geekifier": "Thinking some more about this, I believe what I am seeing is that there seems to be no way to pass credentials parsed from the config file using SharedCredentials into AssumeRoleCredentials. This might be simply my lack of understanding of the API, as I am not a Ruby dev. \nBut I would like to essentially be able to construct a SharedCredentials instance, which then can be used by AssumeRoleCredentials to obtain the access keys the token. Basically, assume STS role defined in config profile foo, using credentials defined in profile bar.\nThe use case here is that I am building a wrapper for another product that does not support STS. So I am obtaining the temporary credentials from the target STS role, using whatever local credentials that are specified under a profile in the config file. I am then getting those credentials, and passing them along to a new sub-process using environmental variables.\nI hope this helps to clarify my issue/request.\n. Another way I have tried this:\n``` ruby\ncredentials = Aws::SharedCredentials.new(\"config_profile_name\")\nsts = Aws::STS::Client.new(credentials: credentials)\nsts.assume_role({ role_arn = \"ARN\", role_session_name = \"Session\"})\n```\nThis resulted in the following error:\n/Library/Ruby/Gems/2.0.0/gems/aws-sdk-core-2.4.3/lib/aws-sdk-core/plugins/request_signer.rb:100:inrequire_credentials': unable to sign request without credentials set (Aws::Errors::MissingCredentialsError).`\nThe error only occurs when I specify a CLI profile that sources credentials from another CLI profile. E.g. 'default' profile has actual API keys in the credentials file. Other CLI profiles use the source_profile directive to reference those credentials. They don't themselves have API keys, only the role_arn parameter and region is specified.\nSo the root of my problem, other than my trial-and-error programming approach, seems to be the fact that Aws:SharedCredentials does not seem to load the credentials from source_profile, unlike the CLI.\nMy setup is pretty vanilla, as it follows the example listed in AWS docs:\n```\nIn ~/.aws/credentials:\n[development]\naws_access_key_id=foo\naws_secret_access_key=bar\nIn ~/.aws/config\n[profile crossaccount]\nrole_arn=arn:aws:iam:...\nsource_profile=development\n```\n. Thanks so much for the continued assistance!\nI tried the example you have provided in aws.rb. It does appear to work, as long as the environmental variable is set, and MFA is not required. Could you provide a quick example of how I would make this work with MFA?\nHere's what I tried:\nAws> client = Aws::STS::Client.new(profile: \"CLIENTPROFILE\", region: \"us-west-1\")\n[Aws::STS::Client 403 0.179872 0 retries] assume_role(role_session_name:\"default_session\",role_arn:\"arn:aws:iam::XXXX:role/XXXXX\",external_id:nil,serial_number:\"arn:aws:iam::XXXXXX:mfa/XXXXX\") Aws::STS::Errors::AccessDenied MultiFactorAuthenticaiton failed, must provide both MFA serial number and one time pass code.\nAws::STS::Errors::AccessDenied: MultiFactorAuthenticaiton failed, must provide both MFA serial number and one time pass code.\nfrom /Library/Ruby/Gems/2.0.0/gems/aws-sdk-core-2.4.3/lib/seahorse/client/plugins/raise_response_errors.rb:15:in `call'\nWhen initializing a new STS client, there is nowhere to provide the token_code. \nAgain, my use case here is that I need to export raw access_key_id, secret_key and session_token from assumerole, so that I can pass it onto an app that does not support profiles/STS.\n. OK, I think we finally got to the bottom of my request. Sorry if I haven't done a good job explaining this.\nLooks like in the meantime I will be forced to implement my own parser for the config (or some other way to supply all of the STS parameters) to make this easy for the users.\n. ",
    "markfink": "the suggested AssumeRoleCredentials code works for me. I use this for reading credentials and config:\nhttps://github.com/a2ikm/aws_config\nand STDIN.get.chomp to read in the token_code\n. ",
    "matthewpick": "I am still having the same error in a different environment (Windows 2012 R2), regardless of credential location...\nMight as well close this issue and role it into #455, I have wasted too much time trying to debug this vague error today.\n. Between an error in copy/pasting (added an i with VIM plugin on Atom) and special characters in my credentials, I finally fixed the error. \nLocation of credentials is irrelevant to this issue.\nThis issue helped me fix my problem using the aws cli.\nThere should still be a more specific error to indicate this is a credential-related problem.\n. ",
    "ghostsquad": "@awood45 is there another alternative to Aws.use_bundled_cert! ?. ",
    "malept": "Fixed :smile: \nIt looks like that private fresh() method isn't tested, that probably would have caught that error earlier.\n. Out of curiosity, now that v3 is out, is this now bumped to v4, or could it go in sooner?. That's fine. I should be able to do that within the next week.. I've rebased this PR on master.. ",
    "flyinbutrs": "Is there a way to enable debug logging on the SDK? I'm happy to give you more to go on if it'll help. It's a pretty minor bug, but the right error would have pointed me in the right direction a bit faster.\n. Where is the PR? I want to see how it's done so I can submit a PR myself next time, I seem to have a knack for finding missing paginators.... ",
    "haines": "@awood45 sorry to be \"that guy\" but is there any chance of getting this in soon? It seems to be supported by the majority of the other SDKs, and we are relying on this feature (I've had to monkey-patch it in until this PR lands), so would really appreciate having it in the released gem.\nThanks \u2764\ufe0f . ",
    "soulcutter": "Well, the issue is my test suite doesn't require a bucket name, but the code generates what a URL would look like. Is there a way to ask for a URL via Aws::S3::Object ? Alternately can Aws::S3::Presigner generate an unsigned URL? Or is there some other way to get a public URL?\nOr really, I could require a bucket name even in the test suite. It's not like I don't expect one normally outside of tests\u2026\nThanks for explaining why it does this - the non-subdomain thing hadn't occurred to me in the heat of the moment.\n. Having // in the URL makes sense with empty bucket names given it really represents /<bucket_name>/\n. Ah, that's a nice thought! I came around to the realization that an empty bucket name was just bad anyway. I appreciate your rapid response and taking the time to think about this :100:\n. ",
    "phommata": "I was able to discover that the AWS_ACCESS_KEY_ID and AWS_SECRET_KEY_ID were switched, so I corrected their values and I was able to upload!\n. ",
    "snyff": "No only \ngem 'aws-sdk'\ngem 'aws-ses', '~> 0.4.4', require: 'aws/ses'\n. Yes sure.\n% grep aws Gemfile.lock \n    aws-sdk (2.5.3)\n      aws-sdk-resources (= 2.5.3)\n    aws-sdk-core (2.5.3)\n    aws-sdk-resources (2.5.3)\n      aws-sdk-core (= 2.5.3)\n    aws-ses (0.4.4)\n  aws-sdk\n  aws-ses (~> 0.4.4)\n% cat app/mailers/customer_mailer.rb\n  def ask_for_details(user, cert)\n    mail( to: @user.email,\n          subject: \"[XXX] TEST\" ,\n          bcc: ['address1@domain'])\n  end\nI also tried with  bcc: 'address1@domain') and it didn't work\n. ",
    "stewartl60": "@awood45: aws-sdk (2.5.11)\n. For sure, requested output looks as follows:\n>> Time.now\n2016-09-13 16:18:15 -0700\n>> Time.now.to_s\n\"2016-09-13 16:18:25 -0700\"\n. It would seem, it is not respecting the parameter name itself not the value of that parameter. I can add another parameter: \njunk: 'garbage' \nand it produces the error:\nArgumentError: unexpected value at params[:garbage]\n. @awood45 - flushed my gems and started over, corrected the issue. Please feel free to close this now.\n. ",
    "ibussieres": "Well, as a temporary fix I had switched (back) to a bucket without cloudfront. Now switching back again, the problem does not occur anymore.\nFor what it's worth, it was not when I was fetching, but when I was POSTing with a S3 presigned post. I'll close this for now, as it seems I cannot replicate myself.\n. ",
    "paul": "Looking closer, it appears the stubbed response is a 400, while the actual one is a 404. Do I need to set the status code to 404 myself in the stub? The docs aren't clear on how to stub both a response error object and a status code. I would also expect the \"NotFound\" response string to set the 404 for me.\n. Got it, thanks. Sorry for the dupe, I tried searching the issues, not far enough back I guess. I had also tried just :head_bucket, {status_code: 404} and got an error, didn't realize all the keys were required. Thanks for your help.\n. ",
    "anark": "Oh, Turns out this had to do with passing in a region of 's3-us-west-2' instead of just 'us-west-2'.  Changing this fixed the issue\n. ",
    "rayway30419": "Hi @awood45, thanks for your response.\nI find whats wrong with my app,\nI use this gem, rails_awesome_console, with my development enviroment, and it seems that modify the default output with rails console.\n(https://github.com/ascendbruce/awesome_rails_console)\nHowever, the class Seahorse::Client::Response does not have to_hash method.\nI disable this gem and print the standard result with aws-sdk is correct.\n. Hi @awood45 \nIt happens randomly, \nWhat kind of enviroment setting?\nRuby 2.3.0\nRails 5.0.0.1\naws-sdk 2.6.3\nUbuntu 14.04 Trusty (3.13.0-63-generic)\n. It's on the EC2 AMI.\n```\nregion in configuration file\nclient = Aws::DynamoDB::Client.new(\n  region: Aws.config[:region]\n)\ndo sth.\nclient.put_item params\n``\n. BTW, I found a clue.....it's running on my sidekiq worker, it may happen to mult-ithreading?\n. Another error message when initialize DynamoDB clientNameError: uninitialized constant Aws::CredentialProviderChain::InstanceProfileCredentials`\n. @awood45 \nWith my understand, the sidekiq process will execute the procedures within rails initializer, too.\nI am glad to hear this news.\n. hi @trevorrowe,\nIt still happens, I am looking forward to it!\n. ",
    "benoittgt": "I had unusual latency on my kinesis stream apparently \n\nDon't know if it is related.\nCan this commit can help me to identify the issue ? https://github.com/aws/aws-sdk-ruby/pull/1250\n. Thanks @awood45 for your answer. No higher load, nothing special. With your tips I dig into error logs and saw 87 OpenURI::HTTPError: 500 Internal Server Error in an other part of my app that occured at the same time as Aws::Kinesis::Errors::InternalFailure. I think this is related to some network issue (we have some with Heroku \ud83d\ude1e). \n. For the moment the wire trace show me a nice 200. I need to find a way to rescue  InternalFailure and show the wire trace only in this condition. I think it's the best idea no? \ud83e\udd14\n. Ok thanks @awood45. Thanks for your help. Closing the issue for the moment.\n. ",
    "pmorton": "Hey - We did the work around many months ago by adding Aws.eager_autoload! as a rails initializer. Unfortunately the issue persists. \n. @awood45 look like this is a Jruby bug. :( https://github.com/jruby/jruby/issues/3920\n. ",
    "Migoo": "Hello,\nThanks for your answer. \nin Fact we found the solution, here is the modified json:\n{\n      default: push_notification_text(locale),\n      GCM: {\n        data: {\n          offer_preview: {\n            title: website.name,\n            content: push_notification_text(locale),\n            offer_id: id\n          }\n        }\n      }.to_json,\n      APNS: {\n        aps: {\n          alert: {\n            title: website.name,\n            body: push_notification_text(locale)\n          },\n          payload: {\n            website_identifier: website.identifier,\n            type: 'offer_preview',\n            id: id\n          }\n        }\n      }.to_json\n    }.to_json\nnote that we just move a to_json from aps to APNS object.\nAnd yes it's intentional to have the multiple json. It's inspired by this Php issue on stackoverflow:\nhttp://stackoverflow.com/questions/18845984/how-to-send-extra-parameters-in-payload-via-amazon-sns-push-notification\nIt was really tricky to implement and find a working solution.\nI'm wondering if it's possible to have this json serialization directly in the gem. Like this we just have to pass our hash.\nhere is our ouptut serialized json:\n{:default=>\"Qwine.ch: Port Charlotte Scottish Barley pour 89.\u2013 au lieu de 119.90\", :GCM=>\"{\\\"data\\\":{\\\"offer_preview\\\":{\\\"title\\\":\\\"Qwine.ch\\\",\\\"content\\\":\\\"Qwine.ch: Port Charlotte Scottish Barley pour 89.\u2013 au lieu de 119.90\\\",\\\"offer_id\\\":1000012}}}\", :APNS=>\"{\\\"aps\\\":{\\\"alert\\\":{\\\"title\\\":\\\"Qwine.ch\\\",\\\"body\\\":\\\"Qwine.ch: Port Charlotte Scottish Barley pour 89.\u2013 au lieu de 119.90\\\"},\\\"sound\\\":\\\"default\\\",\\\"payload\\\":{\\\"website_identifier\\\":\\\"qwineqoqach\\\",\\\"type\\\":\\\"offer_preview\\\",\\\"id\\\":1000012}}}\", :APNS_SANDBOX=>\"{\\\"aps\\\":{\\\"alert\\\":{\\\"title\\\":\\\"Qwine.ch\\\",\\\"body\\\":\\\"Qwine.ch: Port Charlotte Scottish Barley pour 89.\u2013 au lieu de 119.90\\\"},\\\"sound\\\":\\\"default\\\",\\\"payload\\\":{\\\"website_identifier\\\":\\\"qwineqoqach\\\",\\\"type\\\":\\\"offer_preview\\\",\\\"id\\\":1000012}}}\"}\nThanks a lot for helping and improving the gem if possible\n. ",
    "jammerful": "@trevorrowe  I'm still seeing this issue on (actually coming from fluentd which uses the SDK):\nRuby               2.3.4\nAWS SDK       2.7.0\nLinux Kernel  4.9.41\nIs this expected? Could this be eventual consistency, instead of code?. ",
    "ttsumibishi": "Tested against 2.6.3 (public) with a completely stripped down version of my code (so I can post here). Removed access/secret keys, bucket name from output.\nAlso, with 2.6.3 I tested with various file sizes (55k, 1M, 10M, 20M, 100M) and multipart thresholds (default, 5M, 1 byte), and could not successfully upload a file with SSE-C.\nNo uploads have been successful with 2.6.3.\nCode\n```\n!/usr/bin/ruby\nrequire 'openssl'\nrequire 'aws-sdk'\nrequire 'logger'\nrequire 'base64'\nrequire 'socket'\nrequire 'fileutils'\nrequire 'pp'\nJust for testing\nsymmetric_key = OpenSSL::Cipher::AES256.new(:CBC).random_key\nclient = Aws::S3::Client.new(\n      region:            'us-west-2',\n      access_key_id:     '[KNOPE]',\n      secret_access_key: '[KNOPE]',\n      log_level:         :debug,\n      logger:            Logger.new($stdout)\n  )\ns3 = Aws::S3::Resource.new(client: client, http_wire_trace: true)\nFile.open(ARGV[0]) do |file|\n    f = File.basename(file)\n    obj = s3.bucket('[KNOPE]').object(f)\n    pp obj\n    obj.upload_file(file, { content_type: 'application/x-xz',\n                            storage_class: 'STANDARD',\n                            server_side_encryption: 'AES256',\n                            sse_customer_algorithm: 'AES256',\n                            sse_customer_key: symmetric_key,\n                            sse_customer_key_md5: Digest::MD5.base64digest(symmetric_key),\n                            multipart_threshold: 1 })\nend\n```\nError\n```\n[ttsumi@freeside testing]$ ./s3.rb test_file_20m.tar.xz\n\nD, [2016-09-24T13:34:40.074772 #23771] DEBUG -- : [Aws::S3::Client 400 0.102537 0 retries] create_multipart_upload(bucket:\"[KNOPE]\",content_type:\"application/x-xz\",key:\"test_file_20m.tar.xz\",server_side_encryption:\"AES256\",storage_class:\"STANDARD\",sse_customer_algorithm:\"AES256\",sse_customer_key:\"[FILTERED]\",sse_customer_key_md5:\"[KNOPE]\") Aws::S3::Errors::InvalidArgument Server Side Encryption with Customer provided key is incompatible with the encryption method specified\n/home/ttsumi/.gem/ruby/gems/aws-sdk-core-2.6.3/lib/seahorse/client/plugins/raise_response_errors.rb:15:in call': Server Side Encryption with Customer provided key is incompatible with the encryption method specified (Aws::S3::Errors::InvalidArgument)\n        from /home/ttsumi/.gem/ruby/gems/aws-sdk-core-2.6.3/lib/aws-sdk-core/plugins/s3_sse_cpk.rb:19:incall'\n        from /home/ttsumi/.gem/ruby/gems/aws-sdk-core-2.6.3/lib/aws-sdk-core/plugins/s3_dualstack.rb:23:in call'\n        from /home/ttsumi/.gem/ruby/gems/aws-sdk-core-2.6.3/lib/aws-sdk-core/plugins/s3_accelerate.rb:33:incall'\n        from /home/ttsumi/.gem/ruby/gems/aws-sdk-core-2.6.3/lib/aws-sdk-core/plugins/param_converter.rb:20:in call'\n        from /home/ttsumi/.gem/ruby/gems/aws-sdk-core-2.6.3/lib/seahorse/client/plugins/response_target.rb:21:incall'\n        from /home/ttsumi/.gem/ruby/gems/aws-sdk-core-2.6.3/lib/seahorse/client/request.rb:70:in send_request'\n        from /home/ttsumi/.gem/ruby/gems/aws-sdk-core-2.6.3/lib/seahorse/client/base.rb:207:inblock (2 levels) in define_operation_methods'\n        from /home/ttsumi/.gem/ruby/gems/aws-sdk-resources-2.6.3/lib/aws-sdk-resources/services/s3/multipart_file_uploader.rb:52:in initiate_upload'\n        from /home/ttsumi/.gem/ruby/gems/aws-sdk-resources-2.6.3/lib/aws-sdk-resources/services/s3/multipart_file_uploader.rb:43:inupload'\n        from /home/ttsumi/.gem/ruby/gems/aws-sdk-resources-2.6.3/lib/aws-sdk-resources/services/s3/file_uploader.rb:32:in upload'\n        from /home/ttsumi/.gem/ruby/gems/aws-sdk-resources-2.6.3/lib/aws-sdk-resources/services/s3/object.rb:251:inupload_file'\n        from ./test.rb:37:in block in <main>'\n        from ./test.rb:33:inopen'\n        from ./test.rb:33:in `'\n```\n. Got it. Digging in a bit (read: lot) more, it was unclear in the public AWS documentation that if I supply the sse_customer_algorithm attribute, it is mutually exclusive (and will throw an error) with the server_side_encryption attribute. The docs only state that if I want to use AMS-managed keys, to set \"server_side_encryption\" attribute to something (like AES256 or KMS), and if I want to supply customer keys, to add the three sse_customer_* attributes. It was my assumption that the server_side_encryption option was just telling the upload that I want to use server side encryption vs client side, not that it was solely to be used for AWS-managed keys.\n\"Server side encryption with customer provided key is incompatible with the encryption method specified.\" is very confusing, as that's what SSE-C is defined as, and since it only supports AES256 (which I thought to be the encryption method), it was very misleading.\nTaking out the server_side_encryption attribute fixes it.\nWhat's kind of funny is with 2.5.1, if I set multipart threshold to 1, it worked regardless of this parameter being in the options hash.\nClosing this for now. I'll find the right channel to fix the public documentation and maybe clarify the message.\n. ",
    "jaredbeck": "It looks like this has been fixed since I opened this issue 23 hours ago. Perhaps one of your colleagues fixed it. I will close. Thanks.\n. > @jaredbeck Appreciate your thoughts on this PR! Yet UPGRADING.md is the file used to tracking release note automatically for V2. Instead of having the note here, we already mentioned MIGRATING.md in main README.md. Thank you all the same!\nWhen I wanted to upgrade from 1 to 2, I looked in the file named \"upgrading\". Seems like a mistake to choose not to help people who think the same way.\n. > @jaredbeck Apology for this, taking a second look, it's CHANGELOG.md is automated actually.\n\nI'll merged this PR instead. Appreciate your patience on this!\n\nCool, thanks for taking a second look. :)\n. ",
    "devopsberlin": "@awood45 thank you very much for your answer.\nPlease kindly see below AWS support answer \nI've reviewed and reproduced the results you're seeing - I also get a long list of unrelated snapshot IDs. Instead I think you should use describe_snapshots and specify owner_ids:\nhttp://docs.aws.amazon.com/AWSRubySDK/latest/AWS/EC2/Client.html#describe_snapshots-instance_method\nbut I got an exception with describe_snapshots\nregions_list = [ \"eu-central-1\" ]\nregions_list.each do  |region|\nAws.config[:credentials] = Aws::Credentials.new(ENV_YML['access_key_id'], ENV_YML['secret_access_key'])\nec2 = Aws::EC2::Client.new(region:\"#{region}\", credentials: Aws.config[:credentials])\n  ec2.describe_snapshots(owner_ids:['123456']).each do |snapshot|\n p snapshot # works but it give all the snapshot details \n p snapshot.snapshot_id # undefined method `snapshot_id'\n  end\nend\nin method_missing': undefined method snapshot_id' for <Seahorse::Client::Response:0x00000002bb9340> (NoMethodError)\nCould you please kindly suggest how to get the snapshot id ?\n. Thank you @awood45, but I still got an error:\n``\n/var/lib/gems/2.3.0/gems/aws-sdk-core-2.6.4/lib/seahorse/client/response.rb:87:inmethod_missing': undefined method `call' for # (NoMethodError)\nDid you mean?  caller\n```\nI will try to search over the internet what is wrong. \nI used :owner_id because that is what AWS support recommend, in order to get only the snapshots that are belong to my account.\nThanks again for your help!\n. @awood45  yay its working !!! just needed to remove the dot\nresp = ec2.describe_snapshots(owner_ids:['123456'])\nresp.snapshots.each do |snapshot|\n  p snapshot.snapshot_id\nend\nthanks!!\n. ",
    "kirhgoff": "We are also hitting maximum rate limit using Ruby gem. I noticed there is an article about RateLimiter class to be used in Java library, does Ruby version has anything like that?\nhttps://aws.amazon.com/blogs/messaging-and-targeting/how-to-handle-a-throttling-maximum-sending-rate-exceeded-error/\nOr only way to get that issue solved is to increase the limit?. ",
    "malekascha": "How would I make sure that the flag is on? Is that something I would configure from the AWS console, or would it be in my Ruby code?\n. Yes, it did. Thanks!\n. ",
    "kalpitad": "The latter. I guess since the retries happen internally, I can't test them or monitor them. Am I missing something?\nAlso, if I wanted to completely handle retries myself (so that I could increment a statsd monitor, for example), how would I turn off the auto retries built into the SDK?\n. @awood45 -- Thanks for passing this along. Did you mean to close the issue?. Sure, this code is in a Rails app with a standard setup:\n\nmyapp\\Gemfile: gem 'aws-sdk', '~> 3'\nmyapp\\config\\application.rb: Bundler.require(*Rails.groups) #standard way to require all gems in the gemfile\nmyapp\\config\\initializers\\aws.rb: contains the code above\n\nDoes this help?. ",
    "peterticketfly": "encountering the same problem, wire trace forthcoming \n. NAME: testdev-frontdoor-NginxAppGroup-BEP369G1WGP6\nopening connection to autoscaling.us-west-2.amazonaws.com:443...\nopened\nstarting SSL for autoscaling.us-west-2.amazonaws.com:443...\nSSL established\n<- \"POST / HTTP/1.1\\r\\nContent-Type: application/x-www-form-urlencoded; charset=utf-8\\r\\nAccept-Encoding: \\r\\nUser-Agent: aws-sdk-ruby2/2.6.14 ruby/2.1.5 x86_64-darwin14.0 resources\\r\\nX-Amz-Date: 20161116T024959Z\\r\\nHost: autoscaling.us-west-2.amazonaws.com\\r\\nX-Amz-Content-Sha256: f3421671055b697e7ee4e3584b1007684a14c98cc7207a710e42ef7f49e8ee89\\r\\nAuthorization: AWS4-HMAC-SHA256 Credential=AKIAJZ3QVVWXD75QOKNA/20161116/us-west-2/autoscaling/aws4_request, SignedHeaders=content-type;host;x-amz-content-sha256;x-amz-date, Signature=33cb771fbaeac5c91a0e6fbd416c0ea82da7049271863148bd3afdd15ce9d62f\\r\\nContent-Length: 102\\r\\nAccept: */*\\r\\n\\r\\n\"\n-> \"HTTP/1.1 200 OK\\r\\n\"\n-> \"x-amzn-RequestId: 5d220f12-aba7-11e6-9bd0-f95fbb8edbd8\\r\\n\"\n-> \"Content-Type: text/xml\\r\\n\"\n-> \"Content-Length: 1928\\r\\n\"\n-> \"Date: Wed, 16 Nov 2016 02:49:58 GMT\\r\\n\"\n-> \"\\r\\n\"\nreading 1928 bytes...\n-> \"\"\n-> \"<DescribeTagsResponse xmlns=\\\"http://autoscaling.amazonaws.com/doc/2011-01-01/\\\">\\n  <DescribeTagsResult>\\n    <Tags>\\n      <member>\\n        <ResourceId>ami-3846e258</ResourceId>\\n        <PropagateAtLaunch>true</PropagateAtLaunch>\\n        <Value>maintenance-03</Value>\\n        <Key>Name</Key>\\n        <ResourceType>auto-scaling-group</ResourceType>\\n      </member>\\n      <member>\\n        <ResourceId>peter-account-AccountAppGroupBlue-Y1F2KWZQA86S</ResourceId>\\n        <PropagateAtLaunch>true</PropagateAtLaunch>\\n        <Value>account-peter</Value>\\n        <Key>Name</Key>\\n        <ResourceType>auto-scaling-group</ResourceType>\\n      </member>\\n      <member>\\n        <ResourceId>peter-account-AccountAppGroupGreen-ALLHO7L6D5IM</ResourceId>\\n        <PropagateAtLaunch>true</PropagateAtLaunch>\\n        <Value>account-peter</Value>\\n        <Key>Name</Key>\\n        <ResourceType>auto-scaling-group</ResourceType>\\n      </member>\\n      <member>\\n        <ResourceId>prod02-frontdoor-NginxAppGroup-1KYYOEX49GMJP</ResourceId>\\n        <PropagateAtLaunch>true</PropagateAtLaunch>\\n        <Value>nginx-prod02</Value>\\n        <Key>Name</Key>\\n        <ResourceType>auto-scaling-group</ResourceType>\\n      </member>\\n      <member>\\n        <ResourceId>prod02-zookeeper-ZookeeperAppGroup-BQNQABK12TIU</ResourceId>\\n        <PropagateAtLaunch>true</PropagateAtLaunch>\\n        <Value>zookeeper-prod02</Value>\\n        <Key>Name</Key>\\n        <ResourceType>auto-scaling-group</ResourceType>\\n      </member>\\n      <member>\\n        <ResourceId>testdev-frontdoor-NginxAppGroup-BEP369G1WGP6</ResourceId>\\n        <PropagateAtLaunch>true</PropagateAtLaunch>\\n        <Value>nginx-testdev</Value>\\n        <Key>Name</Key>\\n        <ResourceType>auto-scaling-group</ResourceType>\\n      </member>\\n    </Tags>\\n  </DescribeTagsResult>\\n  <ResponseMetadata>\\n    <RequestId>5d220f12-aba7-11e6-9bd0-f95fbb8edbd8</RequestId>\\n  </ResponseMetadata>\\n</DescribeTagsResponse>\\n\"\nread 1928 bytes\nConn keep-alive\nName => maintenance-03\nfrom the code:\naccountGroups = asg.groups.each do |g|\n  puts \"NAME: #{g.name}\"\n  g.tags.each { |t| puts \"#{t.key} => #{t.value}\"}\n  puts\nend\ncan't see the request but it looks like the values for the particular key (Name in this case) is returned for all the AutoScaling groups (not just the current one as one would expect) and the wrong one is selected?\n. also, it appears as if an HTTP API invocation happens for every tag returned by tags, which seems curious, would a single call not suffice to retrieve them all? \n. thank you!\n. ",
    "ls-brentsmith": "thank you!   Sorry for not responding more quickly with trace.  Thank you @peterticketfly for grabbing that. My github notifications weren't including comments from this thread.. ",
    "iainelder": "Just now yes \ud83d\ude04 . I've added your observations to awesome_print issue 275.\n. ",
    "gerrywastaken": "\nI would expect that if r.respond_to?(X) == true then I can call r.method(X) to get the associated Method object.\n\n@lsegal thanks for tracking that down. \n@awood45 did you see this?\n. My response is here: https://github.com/awesome-print/awesome_print/pull/285#issuecomment-263048237\nVery similar to what @lsegal said. But either way, looking into this alerted me to a different issue in the awesome_print arity checking code.. ",
    "ags": "Is there any chance of this being looked at?. Thanks!. ",
    "iroller": "Here's what I see in wireshark:\n\nThis is followed by FIN and closed connection.\nIt's only happening when I'm uploading large files (5-10Mb) and never seem to be happening when I'm using smaller files.\nI also confirmed it's happening on the latest v1-1.66.0 aws-sdk:\n```\n\n\ns3.buckets[bucket_name].objects[key].write(:file => file_name)\nOpenSSL::SSL::SSLError: SSL_write:: shutdown while in init\n    from /opt/ree-1.8.7-2012.02/lib/ruby/1.8/openssl/buffering.rb:179:in syswrite'\n    from /opt/ree-1.8.7-2012.02/lib/ruby/1.8/openssl/buffering.rb:179:indo_write'\n    from /opt/ree-1.8.7-2012.02/lib/ruby/1.8/openssl/buffering.rb:193:in write'\n    from /opt/ree-1.8.7-2012.02/lib/ruby/1.8/net/protocol.rb:177:inwrite0'\n    from /opt/ree-1.8.7-2012.02/lib/ruby/1.8/net/protocol.rb:153:in write'\n    from /opt/ree-1.8.7-2012.02/lib/ruby/1.8/net/protocol.rb:168:inwriting'\n    from /opt/ree-1.8.7-2012.02/lib/ruby/1.8/net/protocol.rb:152:in write'\n    from /opt/ree-1.8.7-2012.02/lib/ruby/1.8/net/http.rb:1565:insend_request_with_body_stream'\n    from /opt/ree-1.8.7-2012.02/lib/ruby/1.8/net/http.rb:1535:in exec'\n    from /opt/ree-1.8.7-2012.02/lib/ruby/1.8/net/http.rb:1049:inrequest'\n    from /usr/local/bundle/ruby/1.8/gems/aws-sdk-v1-1.66.0/lib/aws/core/http/connection_pool.rb:356:in request'\n    from /usr/local/bundle/ruby/1.8/gems/aws-sdk-v1-1.66.0/lib/aws/core/http/net_http_handler.rb:64:inhandle'\n    from /usr/local/bundle/ruby/1.8/gems/aws-sdk-v1-1.66.0/lib/aws/core/http/connection_pool.rb:131:in session_for'\n    from /usr/local/bundle/ruby/1.8/gems/aws-sdk-v1-1.66.0/lib/aws/core/http/net_http_handler.rb:56:inhandle'\n    from /usr/local/bundle/ruby/1.8/gems/aws-sdk-v1-1.66.0/lib/aws/core/client.rb:256:in make_sync_request'\n    from /usr/local/bundle/ruby/1.8/gems/aws-sdk-v1-1.66.0/lib/aws/core/client.rb:289:inretry_server_errors'\n    from /usr/local/bundle/ruby/1.8/gems/aws-sdk-v1-1.66.0/lib/aws/s3/region_detection.rb:11:in retry_server_errors'\n    from /usr/local/bundle/ruby/1.8/gems/aws-sdk-v1-1.66.0/lib/aws/core/client.rb:249:inmake_sync_request'\n    from /usr/local/bundle/ruby/1.8/gems/aws-sdk-v1-1.66.0/lib/aws/core/client.rb:511:in client_request'\n    from /usr/local/bundle/ruby/1.8/gems/aws-sdk-v1-1.66.0/lib/aws/core/client.rb:391:inlog_client_request'\n    from /usr/local/bundle/ruby/1.8/gems/aws-sdk-v1-1.66.0/lib/aws/core/client.rb:477:in client_request'\n    from /usr/local/bundle/ruby/1.8/gems/aws-sdk-v1-1.66.0/lib/aws/core/client.rb:373:inreturn_or_raise'\n    from /usr/local/bundle/ruby/1.8/gems/aws-sdk-v1-1.66.0/lib/aws/core/client.rb:476:in client_request'\n    from (eval):3:input_object'\n    from /usr/local/bundle/ruby/1.8/gems/aws-sdk-v1-1.66.0/lib/aws/s3/s3_object.rb:1765:in write_with_put_object'\n    from /usr/local/bundle/ruby/1.8/gems/aws-sdk-v1-1.66.0/lib/aws/s3/s3_object.rb:611:inwrite'\n    from (irb):10>>\n```\n. Hi @awood45, thanks for your response. This is definitely related to openssl version and apparently to how old ruby is using it. Networking shouldn't be an issue, since it's aws ec2 hosts uploading data into aws s3 buckets.\n\n\nGoing forward I've downgraded our containers from Ubuntu Xenial to Ubuntu Trusty and the problem is gone while using all the same ruby 1.8 and aws-sdk 1.42.0.\nHere's how to reproduce it if interested:\n1. Run Ubuntu Xenial with openssl 1.0.2g-1ubuntu4.5\n2. Install ruby 1.8.7\n3. Install aws-sdk v1.42.0 or v1.66.0.\n4. Use the basic commands to upload a file to s3:\n```\n\n\ndd if=/dev/urandom of=/tmp/random.img count=1 bs=10M\nrequire 'rubygems'\nrequire 'aws-sdk'\nbucket_name = 'some-testing-bucket'\nfile_name = '/tmp/random.img'\nkey = File.basename(file_name)\nAWS.config(:access_key_id => '', :secret_access_key => '')\ns3 = AWS::S3.new\n\n\nDo this a few times to see the error\n\n\n10.times { s3.buckets[bucket_name].objects[key].write(:file => file_name) }\nOpenSSL::SSL::SSLError: SSL_write:: shutdown while in init\n    from /opt/ree-1.8.7-2012.02/lib/ruby/1.8/openssl/buffering.rb:179:in syswrite'\n    from /opt/ree-1.8.7-2012.02/lib/ruby/1.8/openssl/buffering.rb:179:indo_write'\n    from /opt/ree-1.8.7-2012.02/lib/ruby/1.8/openssl/buffering.rb:193:in `write'\n```\n. \n\n",
    "bfolkens": "We just got bitten by this one too.  I'm going to look for a workaround and will post if I find something, but otherwise +1 here as well.. ",
    "mujz": "I'm running into this one too :/\nWould appreciate any suggestions or workarounds.. ",
    "daveseff": "Nope. This issue was for restore_db_instance_from_db_snapshot, not db cluster. \n. ",
    "mziwisky": "@cjyclaire and #copy_from works without raising that PermanentRedirect?  I thought I inspected the web request created by each of those method calls and found them to be the same, but I can't be sure anymore.  I wish now I'd have included my test procedure with this issue in the first place.  Alas, I'll have to investigate again.  Will report back after I've done so.. The closing reminded me of this!  So I dug in a bit.  I found that using copy_to to go from us_east_1 to us_west_2 works fine because aws-sdk handles the redirect itself, and then caches the region for the bucket in us-west-2 and uses that for future requests.  So you get a warning the first time you try that, but not on subsequent tries.  However the same can't be said for copy_to from us_west_2 to us_east_1.\nSo the note is true for certain pairs of regions, and false for others.  In that case it's certainly better to keep it around than to delete it, but maybe all regions could gain this auto-redirecting behavior?\nYou can view the test cases and an annotated run output for them at https://gist.github.com/mziwisky/4e9e71c1bdb6c70145acc785057aa545. ",
    "gregk8288": "Thanks Alex, yes that does seem like the most likely cause. \n. It seems really counter intuitive to break something that was working perfectly well for so long. But thanks for raising it none the less. \n. ",
    "islemaster": "Hi there!  I'm a colleague of @wjordan.  This issue has recurred for us using aws-sdk-core 2.9.14.  Here's a recent stacktrace:\nRuntimeError: can't add a new key into hash during iteration\n101             # No error raised? Good, check the session into the pool.\n102             @pool_mutex.synchronize do\n103               @pool[endpoint] = [] unless @pool.key?(endpoint)\n104               @pool[endpoint] << session\n105             end\n[GEM_ROOT]/gems/aws-sdk-core-2.9.14/lib/seahorse/client/net_http/connection_pool.rb:103 :in `block in session_for`\n[GEM_ROOT]/gems/aws-sdk-core-2.9.14/lib/seahorse/client/net_http/connection_pool.rb:102 :in `synchronize`\n[GEM_ROOT]/gems/aws-sdk-core-2.9.14/lib/seahorse/client/net_http/connection_pool.rb:102 :in `session_for`\n[GEM_ROOT]/gems/aws-sdk-core-2.9.14/lib/seahorse/client/net_http/handler.rb:119 :in `session`\n[GEM_ROOT]/gems/aws-sdk-core-2.9.14/lib/seahorse/client/net_http/handler.rb:71 :in `transmit`\n[GEM_ROOT]/gems/aws-sdk-core-2.9.14/lib/seahorse/client/net_http/handler.rb:45 :in `call`\n[GEM_ROOT]/gems/aws-sdk-core-2.9.14/lib/seahorse/client/plugins/content_length.rb:12 :in `call`\n[GEM_ROOT]/gems/aws-sdk-core-2.9.14/lib/aws-sdk-core/plugins/s3_request_signer.rb:88 :in `call`\n[GEM_ROOT]/gems/aws-sdk-core-2.9.14/lib/aws-sdk-core/plugins/s3_request_signer.rb:23 :in `call`\n[GEM_ROOT]/gems/aws-sdk-core-2.9.14/lib/aws-sdk-core/xml/error_handler.rb:8 :in `call`\n[GEM_ROOT]/gems/aws-sdk-core-2.9.14/lib/aws-sdk-core/plugins/helpful_socket_errors.rb:10 :in `call`\n[GEM_ROOT]/gems/aws-sdk-core-2.9.14/lib/aws-sdk-core/plugins/s3_request_signer.rb:65 :in `call`\n[GEM_ROOT]/gems/aws-sdk-core-2.9.14/lib/aws-sdk-core/plugins/s3_redirects.rb:15 :in `call`\n[GEM_ROOT]/gems/aws-sdk-core-2.9.14/lib/aws-sdk-core/plugins/retry_errors.rb:88 :in `call`\n[GEM_ROOT]/gems/aws-sdk-core-2.9.14/lib/aws-sdk-core/plugins/s3_dualstack.rb:32 :in `call`\n[GEM_ROOT]/gems/aws-sdk-core-2.9.14/lib/aws-sdk-core/plugins/s3_accelerate.rb:49 :in `call`\n[GEM_ROOT]/gems/aws-sdk-core-2.9.14/lib/aws-sdk-core/plugins/s3_md5s.rb:31 :in `call`\n[GEM_ROOT]/gems/aws-sdk-core-2.9.14/lib/aws-sdk-core/plugins/s3_expect_100_continue.rb:21 :in `call`\n[GEM_ROOT]/gems/aws-sdk-core-2.9.14/lib/aws-sdk-core/plugins/s3_bucket_name_restrictions.rb:12 :in `call`\n[GEM_ROOT]/gems/aws-sdk-core-2.9.14/lib/aws-sdk-core/plugins/s3_bucket_dns.rb:31 :in `call`\n[GEM_ROOT]/gems/aws-sdk-core-2.9.14/lib/aws-sdk-core/rest/handler.rb:7 :in `call`\n[GEM_ROOT]/gems/aws-sdk-core-2.9.14/lib/aws-sdk-core/plugins/user_agent.rb:12 :in `call`\n[GEM_ROOT]/gems/aws-sdk-core-2.9.14/lib/seahorse/client/plugins/endpoint.rb:41 :in `call`\n[GEM_ROOT]/gems/aws-sdk-core-2.9.14/lib/aws-sdk-core/plugins/param_validator.rb:21 :in `call`\n[GEM_ROOT]/gems/aws-sdk-core-2.9.14/lib/seahorse/client/plugins/raise_response_errors.rb:14 :in `call`\n[GEM_ROOT]/gems/aws-sdk-core-2.9.14/lib/aws-sdk-core/plugins/s3_sse_cpk.rb:19 :in `call`\n[GEM_ROOT]/gems/aws-sdk-core-2.9.14/lib/aws-sdk-core/plugins/s3_dualstack.rb:24 :in `call`\n[GEM_ROOT]/gems/aws-sdk-core-2.9.14/lib/aws-sdk-core/plugins/s3_accelerate.rb:34 :in `call`\n[GEM_ROOT]/gems/aws-sdk-core-2.9.14/lib/aws-sdk-core/plugins/jsonvalue_converter.rb:20 :in `call`\n[GEM_ROOT]/gems/aws-sdk-core-2.9.14/lib/aws-sdk-core/plugins/idempotency_token.rb:18 :in `call`\n[GEM_ROOT]/gems/aws-sdk-core-2.9.14/lib/aws-sdk-core/plugins/param_converter.rb:20 :in `call`\n[GEM_ROOT]/gems/aws-sdk-core-2.9.14/lib/seahorse/client/plugins/response_target.rb:21 :in `call`\n[GEM_ROOT]/gems/aws-sdk-core-2.9.14/lib/seahorse/client/request.rb:70 :in `send_request`\n[GEM_ROOT]/gems/aws-sdk-core-2.9.14/lib/seahorse/client/base.rb:207 :in `block (2 levels) in define_operation_methods`\n[...Application code stacktrace...]. Hi there! This issue just occurred again for us using aws-sdk-core 3.22.1, aws-sdk-firehose 1.4.0. We saw a burst of 600 instances of this error in a 24-hour window this weekend, September 15-16.\nWe're currently investigating available gem upgrades to see if they help:\n\naws-sdk-firehose from 1.4.0 to 1.6.0\naws-sdk-core from 3.22.1 to 3.27.0\n\nHere's a recent backtrace:\n[GEM_ROOT]/gems/aws-sdk-core-3.22.1/lib/seahorse/client/net_http/connection_pool.rb:107 :in `block in session_for`\n[GEM_ROOT]/gems/aws-sdk-core-3.22.1/lib/seahorse/client/net_http/connection_pool.rb:106 :in `synchronize`\n[GEM_ROOT]/gems/aws-sdk-core-3.22.1/lib/seahorse/client/net_http/connection_pool.rb:106 :in `session_for`\n[GEM_ROOT]/gems/aws-sdk-core-3.22.1/lib/seahorse/client/net_http/handler.rb:121 :in `session`\n[GEM_ROOT]/gems/aws-sdk-core-3.22.1/lib/seahorse/client/net_http/handler.rb:73 :in `transmit`\n[GEM_ROOT]/gems/aws-sdk-core-3.22.1/lib/seahorse/client/net_http/handler.rb:47 :in `call`\n[GEM_ROOT]/gems/aws-sdk-core-3.22.1/lib/seahorse/client/plugins/content_length.rb:12 :in `call`\n[GEM_ROOT]/gems/aws-sdk-core-3.22.1/lib/aws-sdk-core/json/error_handler.rb:8 :in `call`\n[GEM_ROOT]/gems/aws-sdk-core-3.22.1/lib/aws-sdk-core/plugins/signature_v4.rb:66 :in `call`\n[GEM_ROOT]/gems/aws-sdk-core-3.22.1/lib/aws-sdk-core/plugins/helpful_socket_errors.rb:10 :in `call`\n[GEM_ROOT]/gems/aws-sdk-core-3.22.1/lib/aws-sdk-core/plugins/retry_errors.rb:138 :in `call`\n[GEM_ROOT]/gems/aws-sdk-core-3.22.1/lib/aws-sdk-core/json/handler.rb:11 :in `call`\n[GEM_ROOT]/gems/aws-sdk-core-3.22.1/lib/aws-sdk-core/plugins/user_agent.rb:13 :in `call`\n[GEM_ROOT]/gems/aws-sdk-core-3.22.1/lib/seahorse/client/plugins/endpoint.rb:45 :in `call`\n[GEM_ROOT]/gems/aws-sdk-core-3.22.1/lib/aws-sdk-core/plugins/param_validator.rb:24 :in `call`\n[GEM_ROOT]/gems/aws-sdk-core-3.22.1/lib/seahorse/client/plugins/raise_response_errors.rb:14 :in `call`\n[GEM_ROOT]/gems/aws-sdk-core-3.22.1/lib/aws-sdk-core/plugins/jsonvalue_converter.rb:20 :in `call`\n[GEM_ROOT]/gems/aws-sdk-core-3.22.1/lib/aws-sdk-core/plugins/idempotency_token.rb:17 :in `call`\n[GEM_ROOT]/gems/aws-sdk-core-3.22.1/lib/aws-sdk-core/plugins/param_converter.rb:24 :in `call`\n[GEM_ROOT]/gems/aws-sdk-core-3.22.1/lib/aws-sdk-core/plugins/response_paging.rb:10 :in `call`\n[GEM_ROOT]/gems/aws-sdk-core-3.22.1/lib/seahorse/client/plugins/response_target.rb:23 :in `call`\n[GEM_ROOT]/gems/aws-sdk-core-3.22.1/lib/seahorse/client/request.rb:70 :in `send_request`\n[GEM_ROOT]/gems/aws-sdk-firehose-1.4.0/lib/aws-sdk-firehose/client.rb:1040 :in `put_record`\n[Application repo]/lib/cdo/firehose.rb:52 :in `put_record`\nFirehose showing up in the backtrace is new, and we're investigating to see if there's something we're doing wrong in our application layer that might cause this, or any infrastructure-level events that might explain it.  In the meantime we figured a backtrace from the v3 sdk might be helpful.  Thank you!. ",
    "joshlory": "Is there any additional information we can provide to help investigate the error?  Thanks!. ",
    "jeremydstone": "It occurs periodically in bursts. For example, we had this happen not at all for the 2 weeks leading up to June 13. Then it happened 252 times on June 13 and 119 times on June 14. Then not at all for a few days, then 511 times on June 19.\nWhen this does happen, it appears to be in a concentrated burst at a rate of about 1-2 faults per minute. They have all been on the same front end server within a burst as far as I can tell, so something about state of the gem or the server, rather than some externality such as network. (This is running on an EC2 instance within AWS VPC, btw.)\nEven with the intermittency, this happens enough that this is presently our top operational issue in terms of faults/week.. ",
    "javawizard": "Aws::S3::Bucket.instance_variable_get(:@operations)[:objects].request.instance_variable_set(:@method_name, 'list_objects_v2')\nworks around it, so I assume it's just hitting the v1 endpoint right now?\n. ",
    "abepetrillo": "@awendt was this ever fixed? I'm hitting the same issue:\naws-sdk-core-3.46.2/lib/aws-sdk-core/param_validator.rb:32:in `validate!': unexpected value at params[:start_after] (ArgumentError). ",
    "keymon": "And appart of adding jitter, what do you think about changing the shape of the function to use a sigmoid one:\nI have been playing with functions here: https://graphsketch.com/?eqn1_color=1&eqn1_eqn=(2%5Ex)0.3&eqn2_color=3&eqn2_eqn=(2%5Ex)0.3%20-%20(1.3%5Ex)0.30.5&eqn3_color=2&eqn3_eqn=(2%5Ex)0.3%20%2B%20(1.3%5Ex)0.30.5&eqn4_color=3&eqn4_eqn=2%5Eatan(x-3)6-2.5&eqn5_color=5&eqn5_eqn=(2%5Eatan(x-3)6-2.5)%20%200.95&eqn6_color=2&eqn6_eqn=(2%5Eatan(x-3)6-2.5)%20%201.05&x_min=-1&x_max=8&y_min=-1&y_max=25&x_tick=1&y_tick=1&x_label_freq=5&y_label_freq=5&do_grid=0&do_grid=1&bold_labeled_lines=0&bold_labeled_lines=1&line_width=2&image_w=850&image_h=525\n\nI propose sigmoid because this exponential backoff becomes useless, as the back off time is too exagerated when we get into 8 or more retries. With a sigmoid function the back off time will converge to a maximum time to backoff.\nUnfortunatelly this is a major change, specially for common retry numbers, that might break tools that use this library. . ",
    "MaxPleaner": "Thanks very much \n. Actually I don't think this is answered. I misspoke in the original issue text, saying I was trying to check whether the bucket exists. I already know it does; what I'm trying to check is whether or not the object exists.\nThe code you've shared:\nrb\n<resource>.bucket(bucket).object(storage_key).exists?\nis the same code that I had originally, and in the case that the object doesn't exist, it will raise the Aws::S3::Errors::Forbidden error like I've shown. . ",
    "mihairadulescu": "```\n     def download_chunk(bucket, file_key)\n        @client.get_object(bucket: bucket, key: file_key) do |chunk|\n          yield chunk\n        end\n      end\n  def download_chunk_of_size(bucket, file_key, size)\n    content = ''\n    download_chunk(bucket, file_key) do |chunk|\n      p 'merging chunks'\n      p size\n      if content.length < size && !chunk.empty?\n        p content.length\n        content += chunk\n      else\n        p 'yielding chunk`s of size`'\n        yield content\n        content = ''\n      end\n    end\n    yield content unless content.empty?\n  end\n\n```. ",
    "rubemz": "@mihairadulescu no, sadly I didn't find a way. . ",
    "XmmmiR": "With proposed changes, options passed to #exists, will be used in Aws::S3::Client#head_object, but it gives same ability to any classes inherited from Aws::Resources::Resource\n. ",
    "baburdick": "+1\n. ",
    "albertosaurus": "No tests?\n. ",
    "penntaylor": "This issue is not present in the 1.x versions of aws-sdk-ruby. We've been seeing this issue's symptoms since switching to aws-sdk-ruby 2.x in February 2016, but hadn't been able to isolate the exact circumstances until now.\n. Gemfile as well as lock are both in the zip attached to the issue. Duplicating here:\nGemfile:\n```\nsource \"https://rubygems.org\"\ngem 'aws-sdk'\n```\nGemfile.lock:\n```\nGEM\n  remote: https://rubygems.org/\n  specs:\n    aws-sdk (2.6.19)\n      aws-sdk-resources (= 2.6.19)\n    aws-sdk-core (2.6.19)\n      jmespath (~> 1.0)\n    aws-sdk-resources (2.6.19)\n      aws-sdk-core (= 2.6.19)\n    jmespath (1.3.1)\nPLATFORMS\n  ruby\nDEPENDENCIES\n  aws-sdk\n``\n. After looking into this more, I figured it out. The difference between the two cases comes down to REXML vs. Nokogiri. Nobundler/setupmeans existing Nokogiri installation (from aws-sdk-ruby v1 !) gets picked up and used for parsing XML. Addingbundler/setup` cuts out Nokogiri and forces use of significantly slower REXML. Adding Nokogiri to Gemfile brings performance of with-bundler/setup case up to same speed as no-bundler/setup case.\nI don't recall seeing this mentioned in the documentation anywhere, and a quick search doesn't turn anything up. Not mentioned in the v1->v2 migration guide either, and migration is how we originally encountered this problem.\n. @trevorrowe, two quick questions on this issue: Is there documentation anywhere of other external dependencies aws-sdk can silently pull in as it does Nokogiri and Ox? Is there a config setting or environment variable that controls this behavior?\n. The underlying ox problem still exists in most recent ox release (2.8.2); however, I discovered today that my patch breaks some functionality in Aws::EC2::Instance. Still working on tracking down the root cause. Will also try to come up with appropriate additions to the tests to catch both defects.. ",
    "thattommyhall": "@awood45 By \"no longer\" I am comparing to docs on the assumption that they are out of date (though on further thought I suppose they may have been auto-generated)\nI was using 2.5.3, now updated to 2.6.28 and it does seem to work now. Sorry, I thought I had only just installed the gem so didnt think to upgrade it. Sorry for wasting your time!\nBests,\nTom\n. ",
    "richseviora": "Howdy!\nI wanted to get the FifoQueue attribute with a non-FIFO queue because I\nneeded to know whether the queue was FIFO, as that changes the message\nattributes required.\nWe ran into this during testing for a PR I submitted\nhttps://github.com/phstc/shoryuken/pull/273 for the Shoryuken gem to\nenable the use of FIFO queues within Rails and its ActiveJob integration.\nOn Fri, 9 Dec 2016 at 14:07 Michael G. Khmelnitsky notifications@github.com\nwrote:\n\nCurrently, the documentation will not be changed. The service team is\nconsidering changing the API behavior right now.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/aws/aws-sdk-ruby/issues/1350#issuecomment-266136404,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AIM_03Y-NbDLb1MAcRoA0KeHA7uIM4U7ks5rGdELgaJpZM4K-LaO\n.\n. \n",
    "makeready": "Current master build is failing CI due to a gem that doesn't compile in ruby < 2, I'll update the pull request once master is fixed.. Essentially the dynamoDB SDK currently prevents us from using client.update_item to perform a PUT action for an attribute when the new value is an empty set. This is because it can't be marshalled - there is a case statement that checks the datatype of the first member of the set and incorrectly raises a type error if the set is empty.\nBTW, note that this fork doesn't actually break any of the tests (I'm testing using rake in the root of the project as per https://aws.amazon.com/blogs/developer/contributing-to-the-aws-sdk-for-ruby/ ), but Travis CI is flagging the build for not being compatible with ruby 1.9 - this incompatibility was forked from master and was introduced to the main build on November 29.. Example of update_item that this PR fixes:\nclient.update_item({\n  table_name: \"myTable\",\n  key: { \n    \"id\" => \"1\"\n  },\n  attribute_updates: {\n    \"mySet\" => {\n      value: Set.new,\n      action: \"PUT\"\n    }\n  }\n}). Here's the original PR: https://github.com/aws/aws-sdk-ruby/pull/1355\nIt occurs to me that there doesn't need to be a new shape created - an empty set could be typed as a BooleanSetAttribute or some other existing set attribute type instead. Depends on whether adding a new shape is more disruptive than allowing a type to exist with a misleading name. \nLet me know which way you'd like to go with it and I can amend the PR, either by adding the new shape to the jsons, or by removing references to empty set and typing it using an existing shape instead.\nThanks!. ",
    "ojiry": "\nAny particular reason we want to change the version here?\n\nBecause we using the latest ruby, we want to verify by Travis CI.\n\nI'm more inclined  to add Ruby 2.4~\n\nI'm sorry. My English was poor and I couldn't understand the meaning of the second half. \ud83d\ude47 . I agree with it. Thanks.. This PR close once because it takes time to fix the tests.. @awood45 Thank you for understanding my poor English.\n\nCould we add a failing test for this?\n\nTest added. An error should occur in the original code.\nThanks.. Thank you for merged :). ",
    "rayson1223": "Sorry, I delete and reinstall again no more issues. . ",
    "mehlah": "Curious to know btw why it's not enough to rely only on SocketError without adding the condition on DNS_ERROR_MESSAGES. ",
    "DavidRagone": "Has there been any consideration to providing the ability to stub on resource w/o having to understand the underlying implementation on the client? \n\nSo, consider the client calls being made by your resource calls. In this case, you would stub the response to :list_objects. It gets complex if you are checking multiple buckets, but if you'll only call it a single way, you could stub the response to :list_objects to similarly return those keys as you did for :list_buckets.\n\nWhat you're saying is that in order to test the API I'm using (Resource), I need to understand how it is implemented in terms of the Client. That feels like an interface that is not fully implemented.. ",
    "papadeltasierra": "Ditto DavidRagone's comments.  I'm using BUCKETS and perhaps I could stub these but I don't want to have to learn the internals of the AWS SDK in order to do this.  I'm guessing that thre is no other simple way to do this though - the example above needs me to know that RESOURCE.instances maps to CLIENT.described_instances[reservations][instances]!  Sigh.. cjyclaire, perhaps the problem I'm trying to solve will explain.  I'm using S3 storage from a Rails application and for upload (user => S3 => Rails) that's easy because the user uploads a file of a known size.  The download is more difficult because normally Rails is creating the file 'on the fly (streaming until I reach the end effectively)' so I can't just create an obect because I don't know the size and I don't want to be storing  of bytes in memory until I have created the entire file.  So I'm using a trick that others seem to use, namely to write a Rails IO-like class that creates a series of 6MB (>5MB) S3 objects, and then uses a multipart upload to merge them (if there are >= 2 of the original objects).  Now I can create the objects from either S3:Bucket or S3:Client APIs but the multipart can only be created using S3:Client API calls.  Having done this, I would then like to return the public_url of the final object to the user, but there is no way to get from S3:Client => new multipart object => public_url because the only  public_url API call I can find is on the S3:Object.\nI don't know the manner in which you've designed the various classes (is there a nice picture somewhere? I like pictures ;-) ) and I really don't care what the final solution is providing I can start from a single S3:Xxxx class and do all the stuff I need from this one point.  As you will see, I'm currently stuck with having to do the multipart stuff from S3:Client which produces an S3:Object, that I can't access,  whose public_url  I can only access via S3:Bucket -> S3:Object.\nI hope that explains my dilemma.. Sorry for the tardy response; I missed some posts earlier.  I'm not having an enduser upload a file rather the Rails application is generating a file on-the-fly ready for the enduser to download it later.  For example, think of a list of clients in a CSV file - the enduser (via a webpage) asks 'I need an address list for all my north American clients', the Rails application makes a filtered query to its database and then creates a CSV file 'line/address by line/address'.  This sort of streamed write ot an S3 object of unknown size is not possible so I'm doing the following to work around this.\n\nRails buffers up to 6MB of addresses into memory\nRails then writes this memory data into a single S3 object of 6MB\nRails repeats this, building up a list of 6MB S3 objects, plus a final (probably smaller) S3 object\nNow Rails uses the S3 multipart upload to create a single 'big object' from the many 6MB+smaller objects; the 6MB S3 objects are the input and the final big object is the output and the enduser is not involved in this step at all.\n\nThis much works fine but at the very end, how do I tell me end user (probably via an e-mail) how to download the address list?  I want to know the public_url of the final large S3 (multipart) object.  And I want all of the processing above to ideally be wrappable into a single S3/Rails class so that my Rails application treats this hoop-jumping as 'just writing to a file' as much as possible.\nIt seems I need to do all the buffer, write, merge to multipart via the S3::Client class but then there is no way to find the public_url of the final object and I need to switch to the S3::Bucket class to find the final S3 object and get it's public_url.\nI appreciate that this is probably not an intended use of the interface but a quick Google implies that there are many people out there making use of this 'loophole' and it would be nice (at least for me :-) ) if the last bit of the process were that tiny bit simpler and integrated.\nI hope that clarifies what I'm doing.\nThanks,\nPapadeltasierra.. ",
    "Erowlin": "Bump. It's still unclear whether you can stub a resource or not. \nIs Aws::EC2::Resource.new(stub_responses: true) gonna work? \nI found it counter intuitive to create a stubbed client and pass it to the resource, since it can false the specs. I currently do that whenever I want to upload something to S3: \nruby\nAws::S3::Resource.new(region: region).bucket(bucket).object(file_name).tap do |object|\n      object.put(body: file_data, **options)\nend. ",
    "nzifnab": "Further bumping this. I'm doing something similar to @erowlin:\n@s3_bucket ||= Aws::S3::Resource.new.bucket(ENV['S3_BUCKET'])\nobject = @s3_bucket.object(key)\n\nAnd then I'm acting on that object (Specifically in this case calling presigned_url), but it's very unclear to me how best to stub out the response from bucket.object(key).\nIn my spec I have:\nresource_stub = Aws::S3::Resource.new(stub_responses: true)\nexpect(Aws::S3::Resource).to receive(:new).and_return(resource_stub)\n\nBut again, I don't know where to go from here. Do I really need to know the underlying implementation of .bucket() and .object() and how they are calling Aws::S3::Client? That seems incredibly counter-intuitive.\nProbably gonna go for stubbing the methods out with rspec, heh. But that doesn't give me a whole lot of confidence because then I can't be 100% sure that the methods I'm calling or the API I'm relying on is giving me the results I'm expecting.. ",
    "yuvmendel": "Have been wondering the exact same thing as @nzifnab. I see this ticket is closed but I wonder if there is a good answer to this, and if not, I would suggest opening a ticket for it.. ",
    "nachokb": "for those who really can't upgrade the AWS SDK gem, you might want to use this:\ngem 'aws-sdk-v1-ruby24',\n  github: 'seielit/aws-sdk-v1-ruby24',\n  ref: 'aws-sdk-v1-ruby24'. ",
    "LimeBlast": "@nachokb You're a life saver. Thank you.. ",
    "jamespdo": "Yes I understand. This is a breaking change to any ruby version that <=2.4.0.\nI have updated the code to handle and rolled them under this same PR. \nAll tests are passed now. Please review again. Thanks!. ",
    "benesch": "I believe this is only failing because of the issue in #1375!. Sounds good. Thanks, @awood45!\nI should note that I never tracked down exactly what caused context.operation_name to switch from a string to a symbol; git bisect only narrowed it down to the fallout from f19e4d8.. Awesome! Thanks so much, @awood45.. ",
    "KateWilkins": "Hi Alex,\nMany thanks for the response, I cant use the managed file uploader as the uploaded contents are dynamically generated in memory, I could flush to disk but it seems a waste and more convoluted.\nI've re-written the code to use the client operations as suggested above and its working fine.\nIt is a little puzzling as to why there aren't any worked examples of how to use the V2 MultipartUpload & MultipartUploadPart objects or some directions in the API docs to say as above and use the client operations.\nI googled \"aws ruby multipart upload\" and got this link\nhttp://docs.aws.amazon.com/AmazonS3/latest/dev/uploadobjusingmpu-ruby-sdk.html\nit pushes developers towards the MPU objects (V1) without any examples then of course I clicked to the V2 version and there I got stuck..... It might be worth updating it with the what you have above as that was all I needed to get it working.\nThanks again.\nKate\n. ",
    "Ladas": "@awood45 it would also help if there would be a call returning just an array of hashes\nI see it in the docs for stacks, cfm.stack_summaries, so something similar? But weirdly enough cfm.stack_summaries method doesn't exist when I try it.. @awood45 thank you :-). ",
    "gotoAndBliss": "That PR request looked like it was for JS. Is there anything for Ruby on this?\nWe create URL's that lead to applets that use the parameter data to seed their responses ( such as the user_id, endpoint, etc. ).\nSince we've upgraded to V3, this breaks with AuthorizationQueryParametersError as we are augmenting the presigner URL after initialization. Is there any way to seed this data into the URL upon creation?. ",
    "dobrite": "I was able to get it to work with the aws cli using aws cloudfront sign. This points to a bug in this library. I'll work on providing a failing spec.. I was able to generate valid signatures using the test case, but not in my development environment. This points to an issue with my development environment (open ssl?), and not this library. I'll reopen this issue if I find something otherwise, but for now I don't want you to waste any (more) time on this.. Ok, I figured it out!\nRails has a default setting that encodes & as \\\\u0026 which explains why the signatures didn't match when an ampersand was involved. It also explains why the test case produced valid signatures and my dev environment didn't (Rails was loaded).\nAs a workaround JSON.dump(json_hash) can be used here. This will fix cloud front url signer for vanilla Rails apps when an ampersand is involved.\nDoes that sound like a reasonable fix?\n```\nRails irb\n[35] pry(main)> JSON.generate(json_hash)\n=> \"{\\\"Statement\\\":[{\\\"Resource\\\":\\\"http://www.google.com?q=blah&a=yo\\\",\\\"Condition\\\":{\\\"DateLessThan\\\":{\\\"AWS:EpochTime\\\":191981981981}}}]}\"\n[36] pry(main)> json_hash.to_json\n=> \"{\\\"Statement\\\":[{\\\"Resource\\\":\\\"http://www.google.com?q=blah\\u0026a=yo\\\",\\\"Condition\\\":{\\\"DateLessThan\\\":{\\\"AWS:EpochTime\\\":191981981981}}}]}\"\n``. I opened PR #1390 to address this issue..expected_url` is incorrect. This spec passes.\nI did re-verified that 1 query param works with aws-sdk-ruby, 2 does not, but 2 does work with the aws cli using same credentials, distribution, etc.. I understand completely. I too am having issues with setup and repro. I was hoping to provide a failing test case, but narrowing it down is providing difficult. In any case, thank you to taking a look!. Thank you for your work!. ",
    "sharang-d": "@cjyclaire Sorry for the late response.\n\nmay I ask is there any difficulty or blockers for you not to use virtual-host for https? (path style and DNS are still available)?\n\nI'm sorry but I don't understand what you mean. Is this is setting on the bucket or something like that which you are referring to?. @cjyclaire\n\nI'd like to hear more about why you would want https for virtual host style bucket\n\nI have configured a Cloudfront Distribution to serve objects from this bucket using a custom domain name over both HTTP and HTTPS.\nI'm using the result of object.public_url(virtual_host: true) to display an image in the browser. When an HTTP URL is used I see the mixed content warning  telling me that \"an insecure content (image URL) is being loaded on a secure page(the app which is on HTTPS)\". This is why I need the API call to return an HTTPS version of the URL to me.\n\nIf you just want https, you can try :force_path_style.\n\nLet me check that and get back to you.. @cjyclaire\n\nIf you just want https, you can try :force_path_style.\n\nThis did not work. I am still getting the http version of the URL.\nruby\ns3_client = Aws::S3::Client.new(region: s3_config[:region], force_path_style: true,\n                                credentials: Aws::Credentials.new(s3_config[:access_key_id], s3_config[:secret_access_key]))\ns3_key = \"SomeKeyWhichExists\"\ns3obj = Aws::S3::Object.new(\"BucketName\", s3_key, client: s3_client)\ns3obj.public_url(virtual_host: true) # Returns the http version. @cjyclaire \n\nThis overwrites client config, could you try remove the\u00a0:virtual_host\u00a0there?\n\nIf I do that the returned URL is not the virtual host URL which I use for hiding the bucket/region details.\n```\ns3obj.public_url(virtual_host: true)\n=> \"http:///d8391719aa34feef8192c0faabc10e93\"\ns3obj.public_url\n=> \"https://.amazonaws.com//d8391719aa34feef8192c0faabc10e93\"\n```\nI want the former to be returned with https.\nI hope you understand my problem/requirement now.. > I'm curious about your requirement originally\nI've answered that above. :)\n\nI'm using the result of object.public_url(virtual_host: true) to display an image in the browser. When an HTTP URL is used I see the mixed content warning  telling me that \"an insecure content (image URL) is being loaded on a secure page(the app which is on HTTPS)\". This is why I need the API call to return an HTTPS version of the URL to me.\n\nThis plus the fact that I have a valid certificate in place to use the HTTPS version makes me want https always.\n\nI can make this in the feature backlog as an optional parameter. Does that sound good to you? : )\n\n\ud83d\udc4d . ",
    "mwalsher": "Hi @cjyclaire, thanks for looking into this. The issue appears to have been fixed!. ",
    "FunkyloverOne": "Guys, I'm not getting it. I simply want to force download, so I'm trying to set Content-Disposition header to \"attachment...\". Here's a simple test code I'm using to sign URLs via CloudFront (together with shrine gem):\n```ruby\nsigner = Aws::CloudFront::UrlSigner.new(\n  key_pair_id:      ENV['cloudfront_key_pair_id'],\n  private_key_path: Rails.root.join('config', 'cloudfront_pk.pem').to_s\n)\ns3_options = {\n  bucket: ENV['s3_bucket_name'],\n  signer: ->(url, options) do\n    p url\n    # => \"https://dwa4d6760oac.cloudfront.net/store/videomessage/5c474c83b1787743eef9abfe/file/original-e331655862bda2b6a17178bb8ab812e9.webm\"\n    p options\n    # => {:expires=>1579712527}\n    p content_disposition = ContentDisposition.attachment('trailer.webm')\n    # => \"attachment; filename=\\\"trailer.webm\\\"; filename*=UTF-8''trailer.webm\"\n    p final_url =\n        \"#{url}?response-content-disposition=#{CGI.escape(content_disposition)}\"\n    # => \"https://dwa4d6760oac.cloudfront.net/store/videomessage/5c474c83b1787743eef9abfe/file/original-e331655862bda2b6a17178bb8ab812e9.webm?response-content-disposition=attachment%3B+filename%3D%22trailer.webm%22%3B+filename%2A%3DUTF-8%27%27trailer.webm\"\n    p signer.signed_url(final_url, options)\n    # => \"https://dwa4d6760oac.cloudfront.net/store/videomessage/5c474c83b1787743eef9abfe/file/original-e331655862bda2b6a17178bb8ab812e9.webm?response-content-disposition=attachment%3B+filename%3D%22trailer.webm%22%3B+filename%2A%3DUTF-8%27%27trailer.webm&Expires=1579712527&Signature=Ml2unXhVZkV0RK8z0U1EXZ6zuL9E4Xgr69Xr-JxGdWnkVLm4p0kgLar2RXivquVKz5qGy~Z8CPL~aYNSSaDUDeC~wDOPqzrjHcbROWlHz5uGAqF5f1hrso49tUrq-vvwtbZGpPT0slWvT~WXakaqNIYtbeA4Xm9jFI6MBLLoePGmqPvt55TxXhMrdVnNNq0uW7tpTg7Oh65AKVnnoO4nlnpJGG56fnvm0hvVkfaL7hXiE2kOrdXu2HRllcl1s-hiQG-qm3JASMfLfVwD8WVunOu7bde~zXePjKpIlsFFD9fGjOhqCX-1cQnv4GM0N8oF8yfgvMSfF7mvxcXMGKd0Ug__&Key-Pair-Id=APKAJTJHIO7PFEOVXHRA\"\n  end\n}\nShrine.storages = {\n  cache: Shrine::Storage::S3.new(prefix: 'cache', s3_options),\n  store: Shrine::Storage::S3.new(prefix: 'store', s3_options)\n}\n```\nSo my final signed URL in this example is: \nhttps://dwa4d6760oac.cloudfront.net/store/videomessage/5c474c83b1787743eef9abfe/file/original-e331655862bda2b6a17178bb8ab812e9.webm?response-content-disposition=attachment%3B+filename%3D%22trailer.webm%22%3B+filename%2A%3DUTF-8%27%27trailer.webm&Expires=1579712527&Signature=Ml2unXhVZkV0RK8z0U1EXZ6zuL9E4Xgr69Xr-JxGdWnkVLm4p0kgLar2RXivquVKz5qGy~Z8CPL~aYNSSaDUDeC~wDOPqzrjHcbROWlHz5uGAqF5f1hrso49tUrq-vvwtbZGpPT0slWvT~WXakaqNIYtbeA4Xm9jFI6MBLLoePGmqPvt55TxXhMrdVnNNq0uW7tpTg7Oh65AKVnnoO4nlnpJGG56fnvm0hvVkfaL7hXiE2kOrdXu2HRllcl1s-hiQG-qm3JASMfLfVwD8WVunOu7bde~zXePjKpIlsFFD9fGjOhqCX-1cQnv4GM0N8oF8yfgvMSfF7mvxcXMGKd0Ug__&Key-Pair-Id=APKAJTJHIO7PFEOVXHRA\nBut it still does not seem to have proper Content-Disposition headers, you can try it, it should work for year since my comment.\nWhat am I doing wrong? Also why doesn't that Aws::CloudFront::UrlSigner#signed_url handle those headers as an input hash like this: \nruby\n{ response_content_disposition: \"my-value-for-this-header-here\" }\nP.S. I guess I'm using the latest CloudFront gem version, like this:\ngem 'aws-sdk-cloudfront', '~> 1.11'. @cjyclaire , @diehlaws ping :) . Hey @igor-alexandrov, thanks for the reply!\nI've just enabled Query String Forwarding and Caching, but it doesn't seem to help, also according to the description, it looks like it shouldn't, here:\n\nIt says S3 don't process query string parameters, and if I understand this functionality correctly - it simply forwards query string to S3 in my case, I guess that's not what I need, right?\nWhat I want to achieve is to make CloudFront add Content-Disposition header to response, I think. I could be wrong though :) \nHere's the new link I've signed with that feature enabled, still has no header :(\nhttps://dwa4d6760oac.cloudfront.net/store/videomessage/5c595de095af974208e37125/file/original-ed07b4550a0a55a28dd73a1f8b3c8f35.webm?response-content-disposition=attachment%3B+filename%3D%22trailer.webm%22%3B+filename%2A%3DUTF-8%27%27trailer.webm&Expires=1551779820&Signature=CMnLz0NannBII3bFyooVLJqpU0JKsRxPnU-b9oT-9nalhy43RSALzGxu69ht2KDwCjHRYbtzDQQN92w006gT7-8AqM30A7~~~aBl7OT1pvktVqJsPa~HBJwxg3FVRn7eQBx1ErbFgiYIz53eHTsPY92x4PcfsmEakc58CtLOxSZFyRtlOJnNWdHR6Eis5tYrMYc78JPw6jwAUGmPRfY0NG79vXNd8MjANC7A3o-dUaUVfxwFJH1IA7LOMWXf1QKA1XTgUlOgJl5iIqJFDl4r3H30Iqjsyi5RkY76k8uKb5l7OluSXKEjVQTFH4ZZbtH5AYepXUcXkjbWeWAdYXD46w__&Key-Pair-Id=APKAJTJHIO7PFEOVXHRA. @janko-m , @awood45 , hey guys!\nCould you please help me figure things out here? I have the following code:\nruby\nAws::S3::Presigner.new.presigned_url(:put_object, bucket: s3_bucket_name, key: key)\nAnd it works good, but when I want to change some headers (cache-control for example) for that object, it doesn't seem like I actually can do it. Documentation for it is here, and it does not include any header options:\nhttps://docs.aws.amazon.com/sdk-for-ruby/v3/api/Aws/S3/Presigner.html#presigned_url-instance_method\nI also tried to still add some header options, but it seems like it didn't work for me. Is there any solution for it now?. Oh I see, I should use #presigned_url from Aws::S3::Object, and I can configure whatever I want for object first, right?. ",
    "igor-alexandrov": "@FunkyloverOne have you enabled Query String Forwarding and Caching in your distribution behavior settings?. ",
    "rhymes": "Thank you for the great explanation. I'm looking forward to the new version then, in the meantime I already switched the code to use a singleton, like you suggested, and it's working fine. Thanks for the discussions pointers also.. @cjyclaire thank you for the swift response and fix! \n. ",
    "miensol": "@cjyclaire  thanks for checking things out.\nHere are the versions that are used:\naws-sdk (2.6.44)\n      aws-sdk-resources (= 2.6.44)\n    aws-sdk-core (2.6.44)\n      aws-sigv4 (~> 1.0)\n      jmespath (~> 1.0)\n    aws-sdk-resources (2.6.44)\n      aws-sdk-core (= 2.6.44)\n    aws-sigv4 (1.0.0)\n    eb_deployer (0.6.6)\n      aws-sdk (~> 2, >= 2.0.0)\n    jmespath (1.3.1)\nI did more testing and it turns out that the issue might not necessarily be related to this library. \nThe following invocation:\nclient.update_environment(\n         environment_id:\"ENV_ID\",\n         version_label:\"backend.1762\",\n         option_settings:[       \n{\n            \"namespace\"=>\"aws:elasticbeanstalk:command\",\n            \"option_name\"=>\"DeploymentPolicy\",\n            \"value\"=>\"RollingWithAdditionalBatch\"\n},\n       {\"namespace\"=>\"aws:ec2:vpc\",\"option_name\"=>\"VPCId\",\"value\"=>\"VPC_ID_ABC\"}\n])\nTriggers Rolling deployment:\n2017-01-25 11:07:33 UTC+0100    INFO    Environment update completed successfully.\n2017-01-25 11:07:33 UTC+0100    INFO    Successfully deployed new configuration to environment.\n2017-01-25 11:07:33 UTC+0100    INFO    New application version was deployed to running EC2 instances.\n2017-01-25 11:06:39 UTC+0100    INFO    Deploying new version to instance(s).\n2017-01-25 11:06:04 UTC+0100    INFO    Updating environment ENV_NAME configuration settings.\n2017-01-25 11:05:47 UTC+0100    INFO    Environment update is starting.\nHowever if we remove the VPC setting Rolling with additional batch is invoked as requested:\nclient.update_environment(\n         environment_id:\"ENV_ID\",\n         version_label:\"backend.1762\",\n         option_settings:[       \n{\n            \"namespace\"=>\"aws:elasticbeanstalk:command\",\n            \"option_name\"=>\"DeploymentPolicy\",\n            \"value\"=>\"RollingWithAdditionalBatch\"\n}\n])\nI find it strange since in both cases the VPC setting effective value did not change. Is that how it's supposed to work?. The environment is already in the target VPC and this VPCId is used in the update_environment call - so it's effective value before and after update is the same. I suspect the questionable behavior is similar with other option_settings elements and not specifically about VPCId. \nThere's also no error message logged - the update environment calls completes successfully in both cases. In our case majority of time all otions_settings are the same in subsequent update_environments calls as typically a deployment does not need to change any environment settings.\nThe environment status is \"Ready\" otherwise we'd get a typical error message/event logged stating that it has to be \"Ready\" to perform an update.. @cjyclaire Not exactly. \nIn my tests between subsequent calls to update_environment the only parameter that has different value is version_label. \nStep 1. We call update_environment with version_label: 1 and certain option_settings. The option_settings include RollingWithAdditionalBatch but also have other settings i.e. VPCId. The environment is updated properly and the RollingWithAdditionalBatch shows up as a configured value in AWS Web Console. \nStep 2,3,4,5,... We call update_environment with version_label: n and the same option_settings as in Step 1. The new version of application is deployed using Rolling strategy which in my view is unexpected.\nIn my tests it turned out that if we perform Step 2,3,5 and in option_settings provide only a single option for RollingWithAdditionalBatch then the new version is deployed correctly using RollingWithAdditionalBatch.\n. @cjyclaire Please let me know if there is anything I can do to help tackling the issue?. Here are some important excerpts from our conversation with aws support so that people having the same problem as we do can find it (emphasis mine):\n\nAs (...) mentioned, this behaviour occurred because the changes you made resulted in a configuration update being triggered. The deployment strategy used by a configuration change is different to that used by an application deployment. Certain modifications will trigger a configuration update, and some of these changes will require that your environment's instances be replaced (though some will not). Importantly, a configuration update can be triggered even if the configuration option has not been changed since the last update_environment call. \n. \n",
    "JanDintel": "@awood45 I'm sorry, didn't catch that it was only for a versioned object. Thanks for your answer!. ",
    "cmarodrigues": "After read more about it, this pull request can make no sense and could be fixed with flag force_path_style = true. Although however, i submit rspec for tests of s3_bucket_dns, if you think this could be helpful, you can merge it.. ",
    "kjwierenga": "This PR is dependent on PR #1402. You can change the scheme of a URI object, but that doesn't change the class of the object. This is what is causing the default port to be appended.\n```ruby\nuri = URI(\"https://foo.bar\") #=> # \nuri.to_s # => \"https://foo.bar\" \nuri.scheme='http'\nuri.port=80\nuri.to_s # => \"http://foo.bar:80\" \nURI.parse(uri.to_s).to_s\nFix like this\nURI(uri.to_s) # => # \n```\nSame goes the other way around:\n```ruby\nuri=URI(\"http://foo.bar\") # => # \nuri.to_s # => \"http://foo.bar\" \nuri.scheme='https'\nuri.port=443\nuri.to_s # => \"https://foo.bar:443\" \nFix like this\nURI(uri.to_s) # => # \n```\nNote how the URI subclass is determined at first parse of the input URL.\nFix is to re-parse the URI after changing the scheme to a URI object with the appropriate subclass gets created.\n. This is a better way of fixing this issue I think. The solution was there all along in sign_but_dont_send which applies exactly the same technique.. ",
    "thomasv314": "Nevermind, it appears ::Aws::EC2::Client.api.version = '2016-09-15' does the trick. \nIs this documented anywhere? I swear I've seen it before, though where exactly currently escapes me. It could have been in the aws-sdk docs for another language.\n. Thanks for the quick reply\nIn terms of use case: I have a number of API interactions that are recorded with VCR and they started breaking when I upgraded the latest version of the gem. \nI wanted to lock the API down as there were cassettes I didn't want to be forced to re-record. \nI was thinking I had seen this API locking functionality in the ruby sdk, but it turns it out was the JS sdk (under 'Locking API Version'). Ah, apologies for not being clearer. \nI've not noticed any breaking API/backwards incompatible changes. \nThe breaking aspect was strictly that the VCR cassettes include the API version in the request body, which meant previously recorded cassettes did not match the requests from the updated gem:\nyaml\n- request:\n    method: post\n    uri: https://ec2.us-east-1.amazonaws.com/\n    body:\n      encoding: UTF-8\n      string: Action=RebootInstances&InstanceId.1=i-3332f203&Version=2016-09-15. ",
    "cameck": "It's the second time the unprocessed items array hits the write_to_dynamo method. \nThe code I'm using is as above, I've stared long and hard at it, but I'm never surprised to find something I overlooked.\nThank you for your time \ud83d\ude03 . Hmm interesting I actually started off with this example by trevorrowe: \n```ruby\nitems = (1..25).map do |n|\n  {\n    put_request: {\n      item: {\n        \"id\" => \"value#{n}\"\n      }\n    }\n  }\nend\nunprocessed_items = nil\n100.times do\n  # the target table I created with 1 write capacity units to ensure I will be throttled on batch write\n  r = dynamodb.batch_write_item(request_items: { \"aws-sdk-slow\": items })\n  if r.unprocessed_items.count > 0\n    unprocessed_items = r.unprocessed_items\n    break\n  end\nend\ndynamodb.batch_write_item(request_items: unprocessed_items)\n```\nSo, I was under the impression I could just throw it back in without disabling simple_atributes, but it should be more like this:\n```ruby\n@dynamodb ||= Aws::DynamoDB::Client.new(simple_attributes: false)\ndef item(batch)\n  { item: {\n    user: { s: batch['user'] },\n    pic: { s: batch['pic'] },\n    bio: { s: batch['bio'] },\n    fav_movie: { s: batch['movie'] },\n    fav_tv_show: { s: batch['fav_tv_shows'] },\n    age: { n: batch['age'] },\n    premium_user: { bool: true },\n    referral_source: { s: batch['source'] }\n  } }\nend\n``. Ok, so I reproduced it again and added some more logging. I was also a bit suspicious of thehandle_unprocessed_items` method being inside the rescue loop but that didn't make a difference either way.\nHere's the updated code with all kinds of logging:\n```ruby\ndef write_to_dynamo(items)\n    begin\n      tries ||= 5\n      resp = @dynamodb.batch_write_item(request_items: items)\n      @logger.info(\"Dynamoe Respones: #{resp}\")\n    rescue StandardError => e\n      @logger.error(\"#{e}\\nError on: #{{ request_items: items }}\")\n      @logger.error(\"Dynamo Response: #{resp}\")\n      sleep(2)\n      retry unless (tries -= 1).zero?\n      @logger.fatal('Fatal Dynamo Errors, Programming Aborting')\n      abort\n    end\n    handle_unprocessed_items(resp.unprocessed_items)\n  end\ndef handle_unprocessed_items(unprocessed_items)\n    if unprocessed_items.count > 0\n      (@tries + 1)\n      log_failure(unprocessed_items[ENV['DYNAMO_TABLE']])\n      @logger.info('Printing unprocessed_items')\n      p unprocessed_items\n      sleep(@tries * 0.75)\n      write_to_dynamo(unprocessed_items)\n    else\n      reset_tries\n      print_sent_message\n    end\n  end\n```\nLogs:\nI, [2017-02-02T22:19:39.682227 #1]  INFO -- DynamoDB: 2 failed to make it resending...\nI, [2017-02-02T22:19:39.682265 #1]  INFO -- DynamoDB: Printing unprocessed_items\n{\"user-table\"=>[#<struct Aws::DynamoDB::Types::WriteRequest put_request=#<struct Aws::DynamoDB::Types::PutRequest item={\"age\"=>#<BigDecimal:5640b85b8868,'0.1486073977E10',18(27)>, \"user\"=>\"jim\", \"pic\"=>\"https://s3.afsldjfl\", \"bio\"=>\"unknown\", \"premium_user\"=>true, \"referral_source\"=>\"google\", \"sign_up_date\"=>#<BigDecimal:5640b85a7068,'0.1536E5',9(18)>, \"fav_movie\"=>\"gossip girl\"}>, delete_request=nil>, #<struct Aws::DynamoDB::Types::WriteRequest put_request=#<struct Aws::DynamoDB::Types::PutRequest item={\"age\"=>#<BigDecimal:5640b85a62f8,'0.1486073977E10',18(27)>, \"user\"=>\"pete\", \"pic\"=>\"https://s3.sdf8yr4wjla\", \"bio\"=>\"unknown\", \"premium_user\"=>true, \"referral_source\"=>\"bing\", \"sign_up_date\"=>#<BigDecimal:5640b85a4db8,'0.42E2',9(18)>, \"fav_movie\"=>\"alien\"}>, delete_request=nil>]}\nI, [2017-02-02T22:19:39.808993 #1]  INFO -- DynamoDB: Dynamoe Respones: #<struct Aws::DynamoDB::Types::BatchWriteItemOutput unprocessed_items={\"user-table\"=>[#<struct Aws::DynamoDB::Types::WriteRequest put_request=#<struct Aws::DynamoDB::Types::PutRequest item={\"age\"=>#<BigDecimal:5640b84b5cb8,'0.1486073977E10',18(27)>, \"user\"=>\"pete\", \"pic\"=>\"https://s3.sdf8yr4wjla\", \"bio\"=>\"unknown\", \"premium_user\"=>true, \"referral_source\"=>\"bing\", \"sign_up_date\"=>#<BigDecimal:5640b84b42f0,'0.42E2',9(18)>, \"fav_movie\"=>\"alien\"}>, delete_request=nil>]}, item_collection_metrics=nil, consumed_capacity=nil>\nI, [2017-02-02T22:19:39.809222 #1]  INFO -- DynamoDB: 1 failed to make it resending...\nI, [2017-02-02T22:19:39.809296 #1]  INFO -- DynamoDB: Printing unprocessed_items\n{\"user-table\"=>[#<struct Aws::DynamoDB::Types::WriteRequest put_request=#<struct Aws::DynamoDB::Types::PutRequest item={\"age\"=>#<BigDecimal:5640b84b5cb8,'0.1486073977E10',18(27)>, \"user\"=>\"pete\", \"pic\"=>\"https://s3.sdf8yr4wjla\", \"bio\"=>\"unknown\", \"premium_user\"=>true, \"referral_source\"=>\"bing\", \"sign_up_date\"=>#<BigDecimal:5640b84b42f0,'0.42E2',9(18)>, \"fav_movie\"=>\"alien\"}>, delete_request=nil>]}\nE, [2017-02-02T22:20:07.597396 #1] ERROR -- DynamoDB: The level of configured provisioned throughput for the table was exceeded. Consider increasing your provisioning level with the UpdateTable API.\nError on: {:request_items=>{\"user-table\"=>[#<struct Aws::DynamoDB::Types::WriteRequest put_request=#<struct Aws::DynamoDB::Types::PutRequest item={\"age\"=>{:n=>\"0.1486073977E10\"}, \"user\"=>{:s=>\"pete\"}, \"pic\"=>{:s=>\"https://s3.sdf8yr4wjla\"}, \"bio\"=>{:s=>\"unknown\"}, \"premium_user\"=>{:bool=>true}, \"referral_source\"=>{:s=>\"bing\"}, \"sign_up_date\"=>{:n=>\"0.42E2\"}, \"fav_movie\"=>{:s=>\"alien\"}}>, delete_request=nil>]}}\nE, [2017-02-02T22:20:07.597621 #1] ERROR -- DynamoDB: Dynamo Response: \nE, [2017-02-02T22:20:09.853818 #1] ERROR -- DynamoDB: The provided key element does not match the schema\nError on: {:request_items=>{\"user-table\"=>[#<struct Aws::DynamoDB::Types::WriteRequest put_request=#<struct Aws::DynamoDB::Types::PutRequest item={\"age\"=>{:m=>{\"n\"=>{:s=>\"0.1486073977E10\"}}}, \"user\"=>{:m=>{\"s\"=>{:s=>\"pete\"}}}, \"pic\"=>{:m=>{\"s\"=>{:s=>\"https://s3.sdf8yr4wjla\"}}}, \"bio\"=>{:m=>{\"s\"=>{:s=>\"unknown\"}}}, \"premium_user\"=>{:m=>{\"bool\"=>{:bool=>true}}}, \"referral_source\"=>{:m=>{\"s\"=>{:s=>\"bing\"}}}, \"sign_up_date\"=>{:m=>{\"n\"=>{:s=>\"0.42E2\"}}}, \"fav_movie\"=>{:m=>{\"s\"=>{:s=>\"alien\"}}}}>, delete_request=nil>]}}\nE, [2017-02-02T22:20:09.854041 #1] ERROR -- DynamoDB: Dynamo Response: \nE, [2017-02-02T22:20:12.166887 #1] ERROR -- DynamoDB: The provided key element does not match the schema\nError on: {:request_items=>{\"user-table\"=>[#<struct Aws::DynamoDB::Types::WriteRequest put_request=#<struct Aws::DynamoDB::Types::PutRequest item={\"age\"=>{:m=>{\"m\"=>{:m=>{\"n\"=>{:m=>{\"s\"=>{:s=>\"0.1486073977E10\"}}}}}}}, \"user\"=>{:m=>{\"m\"=>{:m=>{\"s\"=>{:m=>{\"s\"=>{:s=>\"pete\"}}}}}}}, \"pic\"=>{:m=>{\"m\"=>{:m=>{\"s\"=>{:m=>{\"s\"=>{:s=>\"https://s3.sdf8yr4wjla\"}}}}}}}, \"bio\"=>{:m=>{\"m\"=>{:m=>{\"s\"=>{:m=>{\"s\"=>{:s=>\"unknown\"}}}}}}}, \"premium_user\"=>{:m=>{\"m\"=>{:m=>{\"bool\"=>{:m=>{\"bool\"=>{:bool=>true}}}}}}}, \"referral_source\"=>{:m=>{\"m\"=>{:m=>{\"s\"=>{:m=>{\"s\"=>{:s=>\"bing\"}}}}}}}, \"sign_up_date\"=>{:m=>{\"m\"=>{:m=>{\"n\"=>{:m=>{\"s\"=>{:s=>\"0.42E2\"}}}}}}}, \"fav_movie\"=>{:m=>{\"m\"=>{:m=>{\"s\"=>{:m=>{\"s\"=>{:s=>\"alien\"}}}}}}}}>, delete_request=nil>]}}\nE, [2017-02-02T22:20:12.167116 #1] ERROR -- DynamoDB: Dynamo Response: \nE, [2017-02-02T22:20:14.342812 #1] ERROR -- DynamoDB: The provided key element does not match the schema\nError on: {:request_items=>{\"user-table\"=>[#<struct Aws::DynamoDB::Types::WriteRequest put_request=#<struct Aws::DynamoDB::Types::PutRequest item={\"age\"=>{:m=>{\"m\"=>{:m=>{\"m\"=>{:m=>{\"m\"=>{:m=>{\"n\"=>{:m=>{\"m\"=>{:m=>{\"s\"=>{:m=>{\"s\"=>{:s=>\"0.1486073977E10\"}}}}}}}}}}}}}}}, \"user\"=>{:m=>{\"m\"=>{:m=>{\"m\"=>{:m=>{\"m\"=>{:m=>{\"s\"=>{:m=>{\"m\"=>{:m=>{\"s\"=>{:m=>{\"s\"=>{:s=>\"pete\"}}}}}}}}}}}}}}}, \"pic\"=>{:m=>{\"m\"=>{:m=>{\"m\"=>{:m=>{\"m\"=>{:m=>{\"s\"=>{:m=>{\"m\"=>{:m=>{\"s\"=>{:m=>{\"s\"=>{:s=>\"https://s3.sdf8yr4wjla\"}}}}}}}}}}}}}}}, \"bio\"=>{:m=>{\"m\"=>{:m=>{\"m\"=>{:m=>{\"m\"=>{:m=>{\"s\"=>{:m=>{\"m\"=>{:m=>{\"s\"=>{:m=>{\"s\"=>{:s=>\"unknown\"}}}}}}}}}}}}}}}, \"premium_user\"=>{:m=>{\"m\"=>{:m=>{\"m\"=>{:m=>{\"m\"=>{:m=>{\"bool\"=>{:m=>{\"m\"=>{:m=>{\"bool\"=>{:m=>{\"bool\"=>{:bool=>true}}}}}}}}}}}}}}}, \"referral_source\"=>{:m=>{\"m\"=>{:m=>{\"m\"=>{:m=>{\"m\"=>{:m=>{\"s\"=>{:m=>{\"m\"=>{:m=>{\"s\"=>{:m=>{\"s\"=>{:s=>\"bing\"}}}}}}}}}}}}}}}, \"sign_up_date\"=>{:m=>{\"m\"=>{:m=>{\"m\"=>{:m=>{\"m\"=>{:m=>{\"n\"=>{:m=>{\"m\"=>{:m=>{\"s\"=>{:m=>{\"s\"=>{:s=>\"0.42E2\"}}}}}}}}}}}}}}}, \"fav_movie\"=>{:m=>{\"m\"=>{:m=>{\"m\"=>{:m=>{\"m\"=>{:m=>{\"s\"=>{:m=>{\"m\"=>{:m=>{\"s\"=>{:m=>{\"s\"=>{:s=>\"alien\"}}}}}}}}}}}}}}}}>, delete_request=nil>]}}\nE, [2017-02-02T22:20:14.343118 #1] ERROR -- DynamoDB: Dynamo Response: \nE, [2017-02-02T22:20:16.570434 #1] ERROR -- DynamoDB: The provided key element does not match the schema\nError on: {:request_items=>{\"user-table\"=>[#<struct Aws::DynamoDB::Types::WriteRequest put_request=#<struct Aws::DynamoDB::Types::PutRequest item={\"age\"=>{:m=>{\"m\"=>{:m=>{\"m\"=>{:m=>{\"m\"=>{:m=>{\"m\"=>{:m=>{\"m\"=>{:m=>{\"m\"=>{:m=>{\"m\"=>{:m=>{\"n\"=>{:m=>{\"m\"=>{:m=>{\"m\"=>{:m=>{\"m\"=>{:m=>{\"s\"=>{:m=>{\"m\"=>{:m=>{\"s\"=>{:m=>{\"s\"=>{:s=>\"0.1486073977E10\"}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}, \"user\"=>{:m=>{\"m\"=>{:m=>{\"m\"=>{:m=>{\"m\"=>{:m=>{\"m\"=>{:m=>{\"m\"=>{:m=>{\"m\"=>{:m=>{\"m\"=>{:m=>{\"s\"=>{:m=>{\"m\"=>{:m=>{\"m\"=>{:m=>{\"m\"=>{:m=>{\"s\"=>{:m=>{\"m\"=>{:m=>{\"s\"=>{:m=>{\"s\"=>{:s=>\"pete\"}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}, \"pic\"=>{:m=>{\"m\"=>{:m=>{\"m\"=>{:m=>{\"m\"=>{:m=>{\"m\"=>{:m=>{\"m\"=>{:m=>{\"m\"=>{:m=>{\"m\"=>{:m=>{\"s\"=>{:m=>{\"m\"=>{:m=>{\"m\"=>{:m=>{\"m\"=>{:m=>{\"s\"=>{:m=>{\"m\"=>{:m=>{\"s\"=>{:m=>{\"s\"=>{:s=>\"https://s3.sdf8yr4wjla\"}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}, \"bio\"=>{:m=>{\"m\"=>{:m=>{\"m\"=>{:m=>{\"m\"=>{:m=>{\"m\"=>{:m=>{\"m\"=>{:m=>{\"m\"=>{:m=>{\"m\"=>{:m=>{\"s\"=>{:m=>{\"m\"=>{:m=>{\"m\"=>{:m=>{\"m\"=>{:m=>{\"s\"=>{:m=>{\"m\"=>{:m=>{\"s\"=>{:m=>{\"s\"=>{:s=>\"unknown\"}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}, \"premium_user\"=>{:m=>{\"m\"=>{:m=>{\"m\"=>{:m=>{\"m\"=>{:m=>{\"m\"=>{:m=>{\"m\"=>{:m=>{\"m\"=>{:m=>{\"m\"=>{:m=>{\"bool\"=>{:m=>{\"m\"=>{:m=>{\"m\"=>{:m=>{\"m\"=>{:m=>{\"bool\"=>{:m=>{\"m\"=>{:m=>{\"bool\"=>{:m=>{\"bool\"=>{:bool=>true}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}, \"referral_source\"=>{:m=>{\"m\"=>{:m=>{\"m\"=>{:m=>{\"m\"=>{:m=>{\"m\"=>{:m=>{\"m\"=>{:m=>{\"m\"=>{:m=>{\"m\"=>{:m=>{\"s\"=>{:m=>{\"m\"=>{:m=>{\"m\"=>{:m=>{\"m\"=>{:m=>{\"s\"=>{:m=>{\"m\"=>{:m=>{\"s\"=>{:m=>{\"s\"=>{:s=>\"bing\"}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}, \"sign_up_date\"=>{:m=>{\"m\"=>{:m=>{\"m\"=>{:m=>{\"m\"=>{:m=>{\"m\"=>{:m=>{\"m\"=>{:m=>{\"m\"=>{:m=>{\"m\"=>{:m=>{\"n\"=>{:m=>{\"m\"=>{:m=>{\"m\"=>{:m=>{\"m\"=>{:m=>{\"s\"=>{:m=>{\"m\"=>{:m=>{\"s\"=>{:m=>{\"s\"=>{:s=>\"0.42E2\"}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}, \"fav_movie\"=>{:m=>{\"m\"=>{:m=>{\"m\"=>{:m=>{\"m\"=>{:m=>{\"m\"=>{:m=>{\"m\"=>{:m=>{\"m\"=>{:m=>{\"m\"=>{:m=>{\"s\"=>{:m=>{\"m\"=>{:m=>{\"m\"=>{:m=>{\"m\"=>{:m=>{\"s\"=>{:m=>{\"m\"=>{:m=>{\"s\"=>{:m=>{\"s\"=>{:s=>\"alien\"}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}>, delete_request=nil>]}}\nE, [2017-02-02T22:20:16.570675 #1] ERROR -- DynamoDB: Dynamo Response: \nF, [2017-02-02T22:20:18.576291 #1] FATAL -- DynamoDB: Fatal Dynamo Errors, Programming Aborting\nI hope that helps you as it still seems odd to me \ud83d\ude04 . By the way, I can reproduce this by just setting the writes to one and having two Ruby workers running against the same Dynamo Table.. Ok, so it looks like when it hits the retry logic, it retries the transmuted array rather than the original array. \nAnd apparently that is semi-expected based on what retry is doing.\n\nSource\n. No worries \ud83d\ude03 .\nIf there's anything I can help with, let me know. I'm happy to help.. Hey @awood45 sorry for the delay. Going out of town, will work to reproduce over the weekend. . Okay finally made a separate reproducible example (this is all the code needed to reproduce again):\n```ruby\nrequire 'aws-sdk'\nrequire 'byebug'\nrequire 'logger'\n@dynamo_table = 'test-throttle'\n@dynamodb ||= Aws::DynamoDB::Client.new(profile: 'dev')\n@logger = Logger.new(STDOUT).tap { |log| log.progname = 'DynamoDB' }\n@logger.level = Logger::INFO\n@tries = 0\n@https_tries = 0\n@spot = 0\n@users = *('0'..'50')\ndef build_put_request(data)\n  while data.any?\n    batch = data.slice!(0, 25)\n    items = put_request_items(batch)\n    write_to_dynamo(@dynamo_table => items)\n  end\nend\ndef put_request_items(batch)\n  Array.new(batch.length) { |_i| { put_request: item } }\nend\ndef item\n  @spot < 50 ? @spot += 1 : @spot = 0\n  { item: {\n    user: @users[@spot],\n    pic: 'great pic',\n    bio: 'such words',\n    fav_movies: ['balto', 'the lion king', 'swingers'],\n    fav_tv_shows: ['spongebob', 'dexter', 'stranger things'],\n    age: SecureRandom.uuid,\n    premium_user: true,\n    referral_source: 'Johnson & Jonson'\n  } }\nend\ndef write_to_dynamo(items)\n  byebug if @https_tries > 1\n  resp = @dynamodb.batch_write_item(request_items: items)\n  handle_unprocessed_items(resp.unprocessed_items)\nrescue StandardError => e\n  @logger.error(\"#{e}\\nError on: #{{ request_items: items }}\")\n  @https_tries += 1\n  retry\nend\ndef handle_unprocessed_items(unprocessed_items)\n  if unprocessed_items.count > 0\n    log_failure(unprocessed_items[@dynamo_table]) if (@tries + 1) > 1\n    sleep(@tries * 0.75)\n    write_to_dynamo(unprocessed_items)\n  else\n    @tries = 0\n    @logger.info('Sent 25 items')\n  end\nend\ndef make_dummies\n  build_put_request([*(1..10_000)])\nend\nmake_dummies\n```\nLogs\n``\nI, [2017-02-27T18:11:56.523010 #25539]  INFO -- DynamoDB: Sent 25 items\nI, [2017-02-27T18:12:21.721349 #25539]  INFO -- DynamoDB: Sent 25 items\nI, [2017-02-27T18:12:46.775433 #25539]  INFO -- DynamoDB: Sent 25 items\nI, [2017-02-27T18:13:11.720228 #25539]  INFO -- DynamoDB: Sent 25 items\nE, [2017-02-28T06:13:57.010722 #25539] ERROR -- DynamoDB: Encountered aSocketError` while attempting to connect to:\nhttps://dynamodb.us-east-1.amazonaws.com\nThis is typically the result of an invalid :region option or a\npoorly formatted :endpoint option.\n\n\nAvoid configuring the :endpoint option directly. Endpoints are constructed\n  from the :region. The :endpoint option is reserved for connecting to\n  non-standard test endpoints.\n\n\nNot every service is available in every region.\n\n\nNever suffix region names with availability zones.\n  Use \"us-east-1\", not \"us-east-1a\"\n\n\nKnown AWS regions include (not specific to this service):\nap-northeast-1\nap-northeast-2\nap-south-1\nap-southeast-1\nap-southeast-2\nca-central-1\neu-central-1\neu-west-1\neu-west-2\nsa-east-1\nus-east-1\nus-east-2\nus-west-1\nus-west-2\ncn-north-1\nus-gov-west-1\nError on: {:request_items=>{\"test-throttle\"=>[#{:bool=>true}, \"fav_tv_shows\"=>{:l=>[{:s=>\"spongebob\"}, {:s=>\"dexter\"}, {:s=>\"stranger things\"}]}, \"referral_source\"=>{:s=>\"Johnson & Jonson\"}, \"fav_movies\"=>{:l=>[{:s=>\"balto\"}, {:s=>\"the lion king\"}, {:s=>\"swingers\"}]}, \"bio\"=>{:s=>\"such words\"}, \"pic\"=>{:s=>\"great pic\"}, \"user\"=>{:s=>\"41\"}, \"age\"=>{:s=>\"7e06c5af-6ca2-4cb2-a248-c4521da8ce1a\"}}>, delete_request=nil>, #{:bool=>true}, \"fav_tv_shows\"=>{:l=>[{:s=>\"spongebob\"}, {:s=>\"dexter\"}, {:s=>\"stranger things\"}]}, \"referral_source\"=>{:s=>\"Johnson & Jonson\"}, \"fav_movies\"=>{:l=>[{:s=>\"balto\"}, {:s=>\"the lion king\"}, {:s=>\"swingers\"}]}, \"bio\"=>{:s=>\"such words\"}, \"pic\"=>{:s=>\"great pic\"}, \"user\"=>{:s=>\"42\"}, \"age\"=>{:s=>\"52b6169f-7310-4ae8-bbbd-c51e07828978\"}}>, delete_request=nil>, #{:bool=>true}, \"fav_tv_shows\"=>{:l=>[{:s=>\"spongebob\"}, {:s=>\"dexter\"}, {:s=>\"stranger things\"}]}, \"referral_source\"=>{:s=>\"Johnson & Jonson\"}, \"fav_movies\"=>{:l=>[{:s=>\"balto\"}, {:s=>\"the lion king\"}, {:s=>\"swingers\"}]}, \"bio\"=>{:s=>\"such words\"}, \"pic\"=>{:s=>\"great pic\"}, \"user\"=>{:s=>\"43\"}, \"age\"=>{:s=>\"90482103-ba88-4bde-93d0-dc4f3c01b0d6\"}}>, delete_request=nil>, #{:bool=>true}, \"fav_tv_shows\"=>{:l=>[{:s=>\"spongebob\"}, {:s=>\"dexter\"}, {:s=>\"stranger things\"}]}, \"referral_source\"=>{:s=>\"Johnson & Jonson\"}, \"fav_movies\"=>{:l=>[{:s=>\"balto\"}, {:s=>\"the lion king\"}, {:s=>\"swingers\"}]}, \"bio\"=>{:s=>\"such words\"}, \"pic\"=>{:s=>\"great pic\"}, \"user\"=>{:s=>\"44\"}, \"age\"=>{:s=>\"ea6f2626-f0e5-4d16-a3a9-3e00984ab6e8\"}}>, delete_request=nil>, #{:bool=>true}, \"fav_tv_shows\"=>{:l=>[{:s=>\"spongebob\"}, {:s=>\"dexter\"}, {:s=>\"stranger things\"}]}, \"referral_source\"=>{:s=>\"Johnson & Jonson\"}, \"fav_movies\"=>{:l=>[{:s=>\"balto\"}, {:s=>\"the lion king\"}, {:s=>\"swingers\"}]}, \"bio\"=>{:s=>\"such words\"}, \"pic\"=>{:s=>\"great pic\"}, \"user\"=>{:s=>\"45\"}, \"age\"=>{:s=>\"43c0d7a1-e11c-4214-a2eb-2af9979f65a9\"}}>, delete_request=nil>, #{:bool=>true}, \"fav_tv_shows\"=>{:l=>[{:s=>\"spongebob\"}, {:s=>\"dexter\"}, {:s=>\"stranger things\"}]}, \"referral_source\"=>{:s=>\"Johnson & Jonson\"}, \"fav_movies\"=>{:l=>[{:s=>\"balto\"}, {:s=>\"the lion king\"}, {:s=>\"swingers\"}]}, \"bio\"=>{:s=>\"such words\"}, \"pic\"=>{:s=>\"great pic\"}, \"user\"=>{:s=>\"46\"}, \"age\"=>{:s=>\"95120315-5ad7-46fc-bd8d-3569e3b3ea65\"}}>, delete_request=nil>, #{:bool=>true}, \"fav_tv_shows\"=>{:l=>[{:s=>\"spongebob\"}, {:s=>\"dexter\"}, {:s=>\"stranger things\"}]}, \"referral_source\"=>{:s=>\"Johnson & Jonson\"}, \"fav_movies\"=>{:l=>[{:s=>\"balto\"}, {:s=>\"the lion king\"}, {:s=>\"swingers\"}]}, \"bio\"=>{:s=>\"such words\"}, \"pic\"=>{:s=>\"great pic\"}, \"user\"=>{:s=>\"47\"}, \"age\"=>{:s=>\"ecc48617-d8c8-4332-9bf7-6a540cf0e394\"}}>, delete_request=nil>, #{:bool=>true}, \"fav_tv_shows\"=>{:l=>[{:s=>\"spongebob\"}, {:s=>\"dexter\"}, {:s=>\"stranger things\"}]}, \"referral_source\"=>{:s=>\"Johnson & Jonson\"}, \"fav_movies\"=>{:l=>[{:s=>\"balto\"}, {:s=>\"the lion king\"}, {:s=>\"swingers\"}]}, \"bio\"=>{:s=>\"such words\"}, \"pic\"=>{:s=>\"great pic\"}, \"user\"=>{:s=>\"48\"}, \"age\"=>{:s=>\"fb889f20-c3a0-48e3-80eb-645a7d6e9bc7\"}}>, delete_request=nil>, #{:bool=>true}, \"fav_tv_shows\"=>{:l=>[{:s=>\"spongebob\"}, {:s=>\"dexter\"}, {:s=>\"stranger things\"}]}, \"referral_source\"=>{:s=>\"Johnson & Jonson\"}, \"fav_movies\"=>{:l=>[{:s=>\"balto\"}, {:s=>\"the lion king\"}, {:s=>\"swingers\"}]}, \"bio\"=>{:s=>\"such words\"}, \"pic\"=>{:s=>\"great pic\"}, \"user\"=>{:s=>\"49\"}, \"age\"=>{:s=>\"17af13b7-dbdc-4784-b3fe-9cef7dc31506\"}}>, delete_request=nil>, #{:bool=>true}, \"fav_tv_shows\"=>{:l=>[{:s=>\"spongebob\"}, {:s=>\"dexter\"}, {:s=>\"stranger things\"}]}, \"referral_source\"=>{:s=>\"Johnson & Jonson\"}, \"fav_movies\"=>{:l=>[{:s=>\"balto\"}, {:s=>\"the lion king\"}, {:s=>\"swingers\"}]}, \"bio\"=>{:s=>\"such words\"}, \"pic\"=>{:s=>\"great pic\"}, \"user\"=>{:s=>\"50\"}, \"age\"=>{:s=>\"ed2cfa67-31df-4732-95a4-560aba5af449\"}}>, delete_request=nil>]}}\nE, [2017-02-28T06:13:57.501219 #25539] ERROR -- DynamoDB: Encountered a SocketError while attempting to connect to:\nhttps://dynamodb.us-east-1.amazonaws.com\nThis is typically the result of an invalid :region option or a\npoorly formatted :endpoint option.\n\n\nAvoid configuring the :endpoint option directly. Endpoints are constructed\n  from the :region. The :endpoint option is reserved for connecting to\n  non-standard test endpoints.\n\n\nNot every service is available in every region.\n\n\nNever suffix region names with availability zones.\n  Use \"us-east-1\", not \"us-east-1a\"\n\n\nKnown AWS regions include (not specific to this service):\nap-northeast-1\nap-northeast-2\nap-south-1\nap-southeast-1\nap-southeast-2\nca-central-1\neu-central-1\neu-west-1\neu-west-2\nsa-east-1\nus-east-1\nus-east-2\nus-west-1\nus-west-2\ncn-north-1\nus-gov-west-1\nError on: {:request_items=>{\"test-throttle\"=>[#{:m=>{\"bool\"=>{:bool=>true}}}, \"fav_tv_shows\"=>{:m=>{\"l\"=>{:l=>[{:m=>{\"s\"=>{:s=>\"spongebob\"}}}, {:m=>{\"s\"=>{:s=>\"dexter\"}}}, {:m=>{\"s\"=>{:s=>\"stranger things\"}}}]}}}, \"referral_source\"=>{:m=>{\"s\"=>{:s=>\"Johnson & Jonson\"}}}, \"fav_movies\"=>{:m=>{\"l\"=>{:l=>[{:m=>{\"s\"=>{:s=>\"balto\"}}}, {:m=>{\"s\"=>{:s=>\"the lion king\"}}}, {:m=>{\"s\"=>{:s=>\"swingers\"}}}]}}}, \"bio\"=>{:m=>{\"s\"=>{:s=>\"such words\"}}}, \"pic\"=>{:m=>{\"s\"=>{:s=>\"great pic\"}}}, \"user\"=>{:m=>{\"s\"=>{:s=>\"41\"}}}, \"age\"=>{:m=>{\"s\"=>{:s=>\"7e06c5af-6ca2-4cb2-a248-c4521da8ce1a\"}}}}>, delete_request=nil>, #{:m=>{\"bool\"=>{:bool=>true}}}, \"fav_tv_shows\"=>{:m=>{\"l\"=>{:l=>[{:m=>{\"s\"=>{:s=>\"spongebob\"}}}, {:m=>{\"s\"=>{:s=>\"dexter\"}}}, {:m=>{\"s\"=>{:s=>\"stranger things\"}}}]}}}, \"referral_source\"=>{:m=>{\"s\"=>{:s=>\"Johnson & Jonson\"}}}, \"fav_movies\"=>{:m=>{\"l\"=>{:l=>[{:m=>{\"s\"=>{:s=>\"balto\"}}}, {:m=>{\"s\"=>{:s=>\"the lion king\"}}}, {:m=>{\"s\"=>{:s=>\"swingers\"}}}]}}}, \"bio\"=>{:m=>{\"s\"=>{:s=>\"such words\"}}}, \"pic\"=>{:m=>{\"s\"=>{:s=>\"great pic\"}}}, \"user\"=>{:m=>{\"s\"=>{:s=>\"42\"}}}, \"age\"=>{:m=>{\"s\"=>{:s=>\"52b6169f-7310-4ae8-bbbd-c51e07828978\"}}}}>, delete_request=nil>, #{:m=>{\"bool\"=>{:bool=>true}}}, \"fav_tv_shows\"=>{:m=>{\"l\"=>{:l=>[{:m=>{\"s\"=>{:s=>\"spongebob\"}}}, {:m=>{\"s\"=>{:s=>\"dexter\"}}}, {:m=>{\"s\"=>{:s=>\"stranger things\"}}}]}}}, \"referral_source\"=>{:m=>{\"s\"=>{:s=>\"Johnson & Jonson\"}}}, \"fav_movies\"=>{:m=>{\"l\"=>{:l=>[{:m=>{\"s\"=>{:s=>\"balto\"}}}, {:m=>{\"s\"=>{:s=>\"the lion king\"}}}, {:m=>{\"s\"=>{:s=>\"swingers\"}}}]}}}, \"bio\"=>{:m=>{\"s\"=>{:s=>\"such words\"}}}, \"pic\"=>{:m=>{\"s\"=>{:s=>\"great pic\"}}}, \"user\"=>{:m=>{\"s\"=>{:s=>\"43\"}}}, \"age\"=>{:m=>{\"s\"=>{:s=>\"90482103-ba88-4bde-93d0-dc4f3c01b0d6\"}}}}>, delete_request=nil>, #{:m=>{\"bool\"=>{:bool=>true}}}, \"fav_tv_shows\"=>{:m=>{\"l\"=>{:l=>[{:m=>{\"s\"=>{:s=>\"spongebob\"}}}, {:m=>{\"s\"=>{:s=>\"dexter\"}}}, {:m=>{\"s\"=>{:s=>\"stranger things\"}}}]}}}, \"referral_source\"=>{:m=>{\"s\"=>{:s=>\"Johnson & Jonson\"}}}, \"fav_movies\"=>{:m=>{\"l\"=>{:l=>[{:m=>{\"s\"=>{:s=>\"balto\"}}}, {:m=>{\"s\"=>{:s=>\"the lion king\"}}}, {:m=>{\"s\"=>{:s=>\"swingers\"}}}]}}}, \"bio\"=>{:m=>{\"s\"=>{:s=>\"such words\"}}}, \"pic\"=>{:m=>{\"s\"=>{:s=>\"great pic\"}}}, \"user\"=>{:m=>{\"s\"=>{:s=>\"44\"}}}, \"age\"=>{:m=>{\"s\"=>{:s=>\"ea6f2626-f0e5-4d16-a3a9-3e00984ab6e8\"}}}}>, delete_request=nil>, #{:m=>{\"bool\"=>{:bool=>true}}}, \"fav_tv_shows\"=>{:m=>{\"l\"=>{:l=>[{:m=>{\"s\"=>{:s=>\"spongebob\"}}}, {:m=>{\"s\"=>{:s=>\"dexter\"}}}, {:m=>{\"s\"=>{:s=>\"stranger things\"}}}]}}}, \"referral_source\"=>{:m=>{\"s\"=>{:s=>\"Johnson & Jonson\"}}}, \"fav_movies\"=>{:m=>{\"l\"=>{:l=>[{:m=>{\"s\"=>{:s=>\"balto\"}}}, {:m=>{\"s\"=>{:s=>\"the lion king\"}}}, {:m=>{\"s\"=>{:s=>\"swingers\"}}}]}}}, \"bio\"=>{:m=>{\"s\"=>{:s=>\"such words\"}}}, \"pic\"=>{:m=>{\"s\"=>{:s=>\"great pic\"}}}, \"user\"=>{:m=>{\"s\"=>{:s=>\"45\"}}}, \"age\"=>{:m=>{\"s\"=>{:s=>\"43c0d7a1-e11c-4214-a2eb-2af9979f65a9\"}}}}>, delete_request=nil>, #{:m=>{\"bool\"=>{:bool=>true}}}, \"fav_tv_shows\"=>{:m=>{\"l\"=>{:l=>[{:m=>{\"s\"=>{:s=>\"spongebob\"}}}, {:m=>{\"s\"=>{:s=>\"dexter\"}}}, {:m=>{\"s\"=>{:s=>\"stranger things\"}}}]}}}, \"referral_source\"=>{:m=>{\"s\"=>{:s=>\"Johnson & Jonson\"}}}, \"fav_movies\"=>{:m=>{\"l\"=>{:l=>[{:m=>{\"s\"=>{:s=>\"balto\"}}}, {:m=>{\"s\"=>{:s=>\"the lion king\"}}}, {:m=>{\"s\"=>{:s=>\"swingers\"}}}]}}}, \"bio\"=>{:m=>{\"s\"=>{:s=>\"such words\"}}}, \"pic\"=>{:m=>{\"s\"=>{:s=>\"great pic\"}}}, \"user\"=>{:m=>{\"s\"=>{:s=>\"46\"}}}, \"age\"=>{:m=>{\"s\"=>{:s=>\"95120315-5ad7-46fc-bd8d-3569e3b3ea65\"}}}}>, delete_request=nil>, #{:m=>{\"bool\"=>{:bool=>true}}}, \"fav_tv_shows\"=>{:m=>{\"l\"=>{:l=>[{:m=>{\"s\"=>{:s=>\"spongebob\"}}}, {:m=>{\"s\"=>{:s=>\"dexter\"}}}, {:m=>{\"s\"=>{:s=>\"stranger things\"}}}]}}}, \"referral_source\"=>{:m=>{\"s\"=>{:s=>\"Johnson & Jonson\"}}}, \"fav_movies\"=>{:m=>{\"l\"=>{:l=>[{:m=>{\"s\"=>{:s=>\"balto\"}}}, {:m=>{\"s\"=>{:s=>\"the lion king\"}}}, {:m=>{\"s\"=>{:s=>\"swingers\"}}}]}}}, \"bio\"=>{:m=>{\"s\"=>{:s=>\"such words\"}}}, \"pic\"=>{:m=>{\"s\"=>{:s=>\"great pic\"}}}, \"user\"=>{:m=>{\"s\"=>{:s=>\"47\"}}}, \"age\"=>{:m=>{\"s\"=>{:s=>\"ecc48617-d8c8-4332-9bf7-6a540cf0e394\"}}}}>, delete_request=nil>, #{:m=>{\"bool\"=>{:bool=>true}}}, \"fav_tv_shows\"=>{:m=>{\"l\"=>{:l=>[{:m=>{\"s\"=>{:s=>\"spongebob\"}}}, {:m=>{\"s\"=>{:s=>\"dexter\"}}}, {:m=>{\"s\"=>{:s=>\"stranger things\"}}}]}}}, \"referral_source\"=>{:m=>{\"s\"=>{:s=>\"Johnson & Jonson\"}}}, \"fav_movies\"=>{:m=>{\"l\"=>{:l=>[{:m=>{\"s\"=>{:s=>\"balto\"}}}, {:m=>{\"s\"=>{:s=>\"the lion king\"}}}, {:m=>{\"s\"=>{:s=>\"swingers\"}}}]}}}, \"bio\"=>{:m=>{\"s\"=>{:s=>\"such words\"}}}, \"pic\"=>{:m=>{\"s\"=>{:s=>\"great pic\"}}}, \"user\"=>{:m=>{\"s\"=>{:s=>\"48\"}}}, \"age\"=>{:m=>{\"s\"=>{:s=>\"fb889f20-c3a0-48e3-80eb-645a7d6e9bc7\"}}}}>, delete_request=nil>, #{:m=>{\"bool\"=>{:bool=>true}}}, \"fav_tv_shows\"=>{:m=>{\"l\"=>{:l=>[{:m=>{\"s\"=>{:s=>\"spongebob\"}}}, {:m=>{\"s\"=>{:s=>\"dexter\"}}}, {:m=>{\"s\"=>{:s=>\"stranger things\"}}}]}}}, \"referral_source\"=>{:m=>{\"s\"=>{:s=>\"Johnson & Jonson\"}}}, \"fav_movies\"=>{:m=>{\"l\"=>{:l=>[{:m=>{\"s\"=>{:s=>\"balto\"}}}, {:m=>{\"s\"=>{:s=>\"the lion king\"}}}, {:m=>{\"s\"=>{:s=>\"swingers\"}}}]}}}, \"bio\"=>{:m=>{\"s\"=>{:s=>\"such words\"}}}, \"pic\"=>{:m=>{\"s\"=>{:s=>\"great pic\"}}}, \"user\"=>{:m=>{\"s\"=>{:s=>\"49\"}}}, \"age\"=>{:m=>{\"s\"=>{:s=>\"17af13b7-dbdc-4784-b3fe-9cef7dc31506\"}}}}>, delete_request=nil>, #{:m=>{\"bool\"=>{:bool=>true}}}, \"fav_tv_shows\"=>{:m=>{\"l\"=>{:l=>[{:m=>{\"s\"=>{:s=>\"spongebob\"}}}, {:m=>{\"s\"=>{:s=>\"dexter\"}}}, {:m=>{\"s\"=>{:s=>\"stranger things\"}}}]}}}, \"referral_source\"=>{:m=>{\"s\"=>{:s=>\"Johnson & Jonson\"}}}, \"fav_movies\"=>{:m=>{\"l\"=>{:l=>[{:m=>{\"s\"=>{:s=>\"balto\"}}}, {:m=>{\"s\"=>{:s=>\"the lion king\"}}}, {:m=>{\"s\"=>{:s=>\"swingers\"}}}]}}}, \"bio\"=>{:m=>{\"s\"=>{:s=>\"such words\"}}}, \"pic\"=>{:m=>{\"s\"=>{:s=>\"great pic\"}}}, \"user\"=>{:m=>{\"s\"=>{:s=>\"50\"}}}, \"age\"=>{:m=>{\"s\"=>{:s=>\"ed2cfa67-31df-4732-95a4-560aba5af449\"}}}}>, delete_request=nil>]}}\n[37, 46] in ~/dynamo_debugging_for_aws.rb\n   37:   } }\n   38: end\n   39: \n   40: def write_to_dynamo(items)\n   41:   byebug if @https_tries > 1\n=> 42:   resp = @dynamodb.batch_write_item(request_items: items)\n   43:   handle_unprocessed_items(resp.unprocessed_items)\n   44: rescue StandardError => e\n   45:   @logger.error(\"#{e}\\nError on: #{{ request_items: items }}\")\n   46:   @https_tries += 1\n(byebug) where\n--> #0  Object.write_to_dynamo(items#Hash) at ~/dynamo_debugging_for_aws.rb:42\n    #1  Object.handle_unprocessed_items(unprocessed_items#Hash) at ~/dynamo_debugging_for_aws.rb:54\n    #2  Object.write_to_dynamo(items#Hash) at ~/dynamo_debugging_for_aws.rb:43\n    #3  Object.handle_unprocessed_items(unprocessed_items#Hash) at ~/dynamo_debugging_for_aws.rb:54\n    #4  Object.write_to_dynamo(items#Hash) at ~/dynamo_debugging_for_aws.rb:43\n    #5  Object.handle_unprocessed_items(unprocessed_items#Hash) at ~/dynamo_debugging_for_aws.rb:54\n    #6  Object.write_to_dynamo(items#Hash) at ~/dynamo_debugging_for_aws.rb:43\n    #7  Object.handle_unprocessed_items(unprocessed_items#Hash) at ~/dynamo_debugging_for_aws.rb:54\n    #8  Object.write_to_dynamo(items#Hash) at ~/dynamo_debugging_for_aws.rb:43\n    #9  Object.handle_unprocessed_items(unprocessed_items#Hash) at ~/dynamo_debugging_for_aws.rb:54\n    #10 Object.write_to_dynamo(items#Hash) at ~/dynamo_debugging_for_aws.rb:43\n    #11 Object.handle_unprocessed_items(unprocessed_items#Hash) at ~/dynamo_debugging_for_aws.rb:54\n    #12 Object.write_to_dynamo(items#Hash) at ~/dynamo_debugging_for_aws.rb:43\n    #13 Object.handle_unprocessed_items(unprocessed_items#Hash) at ~/dynamo_debugging_for_aws.rb:54\n    #14 Object.write_to_dynamo(items#Hash) at ~/dynamo_debugging_for_aws.rb:43\n    #15 Object.handle_unprocessed_items(unprocessed_items#Hash) at ~/dynamo_debugging_for_aws.rb:54\n    #16 Object.write_to_dynamo(items#Hash) at ~/dynamo_debugging_for_aws.rb:43\n    #17 Object.handle_unprocessed_items(unprocessed_items#Hash) at ~/dynamo_debugging_for_aws.rb:54\n    #18 Object.write_to_dynamo(items#Hash) at ~/dynamo_debugging_for_aws.rb:43\n    #19 Object.handle_unprocessed_items(unprocessed_items#Hash) at ~/dynamo_debugging_for_aws.rb:54\n    #20 Object.write_to_dynamo(items#Hash) at ~/dynamo_debugging_for_aws.rb:43\n    #21 Object.handle_unprocessed_items(unprocessed_items#Hash) at ~/dynamo_debugging_for_aws.rb:54\n    #22 Object.write_to_dynamo(items#Hash) at ~/dynamo_debugging_for_aws.rb:43\n    #23 Object.handle_unprocessed_items(unprocessed_items#Hash) at ~/dynamo_debugging_for_aws.rb:54\n    #24 Object.write_to_dynamo(items#Hash) at ~/dynamo_debugging_for_aws.rb:43\n    #25 Object.handle_unprocessed_items(unprocessed_items#Hash) at ~/dynamo_debugging_for_aws.rb:54\n    #26 Object.write_to_dynamo(items#Hash) at ~/dynamo_debugging_for_aws.rb:43\n    #27 Object.handle_unprocessed_items(unprocessed_items#Hash) at ~/dynamo_debugging_for_aws.rb:54\n    #28 Object.write_to_dynamo(items#Hash) at ~/dynamo_debugging_for_aws.rb:43\n    #29 Object.build_put_request(data#Array) at ~/dynamo_debugging_for_aws.rb:18\n    #30 Object.make_dummies at ~/dynamo_debugging_for_aws.rb:62\n    #31  at ~/dynamo_debugging_for_aws.rb:65\n(byebug) info locals\n Unknown command 'info locals'. Try 'help info'\n(byebug) locals\n NameError Exception: undefined local variable or method `locals' for main:Object\nnil\n(byebug) info program\nProgram stopped. \nIt stopped after stepping, next'ing or initial start.\n(byebug) list\n[47, 56] in ~/dynamo_debugging_for_aws.rb\n   47:   retry\n   48: end\n   49: \n   50: def handle_unprocessed_items(unprocessed_items)\n   51:   if unprocessed_items.count > 0\n   52:     log_failure(unprocessed_items[@dynamo_table]) if (@tries + 1) > 1\n   53:     sleep(@tries * 0.75)\n   54:     write_to_dynamo(unprocessed_items)\n   55:   else\n   56:     @tries = 0\n(byebug) var all\n$! = nil\n$\" = [\"enumerator.so\", \"thread.rb\", \"rational.so\", \"complex.so\", \"~/.rvm/rubies/ruby-2.3.3/lib/ruby/2.3.0/x86_64-darwin15/enc/encdb.bundle\",...\n$$ = 25539\n$& = nil\n$' = nil\n$* = []\n$+ = nil\n$, = nil\n$-0 = \"\\n\"\n$-F = nil\n$-I = [\"~/.rvm/gems/ruby-2.3.3@global/gems/did_you_mean-1.0.0/lib\", \"~/.rvm/gems/ruby-2.3.3/gems/jmespath-1.3.1/lib\", \"/Use...\n$-W = 1\n$-a = false\n$-d = false\n$-i = nil\n$-l = false\n$-p = false\n$-v = false\n$-w = false\n$. = 57\n$/ = \"\\n\"\n$0 = \"dynamo_debugging_for_aws.rb\"\n$1 = nil\n$2 = nil\n$3 = nil\n$4 = nil\n$5 = nil\n$6 = nil\n$7 = nil\n$8 = nil\n$9 = nil\n$: = [\"~/.rvm/gems/ruby-2.3.3@global/gems/did_you_mean-1.0.0/lib\", \"~/.rvm/gems/ruby-2.3.3/gems/jmespath-1.3.1/lib\", \"/User...\n$; = nil\n$< = ARGF\n$> = #>\n$? = nil\n$@ = nil\n$ARGV = nil\n$CGI_ENV = {\"NVM_RC_VERSION\"=>\"\", \"rvm_bin_path\"=>\"~/.rvm/bin\", \"TERM_PROGRAM\"=>\"Apple_Terminal\", \"NVM_CD_FLAGS\"=>\"\", \"GEM_HOME\"=>\"/Users/ce...\n$DEBUG = false\n$FILENAME = \"-\"\n$LOADED_FEATURES = [\"enumerator.so\", \"thread.rb\", \"rational.so\", \"complex.so\", \"~/.rvm/rubies/ruby-2.3.3/lib/ruby/2.3.0/x86_64-darwin15/enc/...\n$LOAD_PATH = [\"~/.rvm/gems/ruby-2.3.3@global/gems/did_you_mean-1.0.0/lib\", \"~/.rvm/gems/ruby-2.3.3/gems/jmespath-1.3.1/lib\"...\n$PROGRAM_NAME = \"dynamo_debugging_for_aws.rb\"\n$SAFE = 0\n$VERBOSE = false\n$\\ = nil\n$_ = nil\n$= nil\n$fileutils_rb_have_lchmod = nil\n$fileutils_rb_have_lchown = nil\n$stderr = #<IO:<STDERR>>\n$stdin = #<IO:<STDIN>>\n$stdout = #<IO:<STDOUT>>\n$~ = nil\n@dynamo_table = \"test-throttle\"\n@dynamodb = #<Aws::DynamoDB::Client>\n@https_tries = 2\n@logger = #<Logger:0x007fedf503a6e8 @progname=\"DynamoDB\", @level=1, @default_formatter=#<Logger::Formatter:0x007fedf503a6c0 @datetime_format=nil>, @formatter...\n@spot = 50\n@tries = 0\n@users = [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\", \"11\", \"12\", \"13\", \"14\", \"15\", \"16\", \"17\", \"18\", \"19\", \"20\", \"21\", \"22\", \"23\", \"24\", \"25\", \"...\ne = Encountered aSocketError` while attempting to connect to:\nhttps://dynamodb.us-east-1.amazonaws.com\nThis is typically the result of an invalid :reg...\nitems = {\"test-throttle\"=>[#<struct Aws::DynamoDB::Types::WriteRequest put_request=#<struct Aws::DynamoDB::Types::PutRequest item={\"premium_user\"=>{:m...\nresp = nil\n(byebug) var local\ne = Encountered aSocketError` while attempting to connect to:\nhttps://dynamodb.us-east-1.amazonaws.com\nThis is typically the result of an invalid `:reg...\nitems = {\"test-throttle\"=>[#{:m...\nresp = nil\n(byebug) var args\nitems = {\"test-throttle\"=>[#{:m...\np resp[:vhash]\n```\nI did this on a newly created table.\nThe Partition Key was user and the GSI was age.\nThe RCU/WCU for both indexes was set to 1.  \nI did not hit the error I hit before but instead a SocketError, which made the same nesting bug.. no worries \ud83d\ude03 . Very cool, thanks so much \ud83d\udcaf  !. ",
    "ryanbrainard": "Thanks for the quick response. Will the fix for the paginator also fix the \"String\" being returned for next_token in stubs or should I file a separate issue for that?. @awood45 Thanks for looking into this. So it does sound like it's related to #1408. If so, does that mean that there are a lot of resources that need paginators defined?. ",
    "Undo1": "@cjyclaire Bah, was on 1.66.0. Bundler probably snagged an ancient local version without telling me about it (or without me noticing, which is just as bad). \nIt works as described now. Thank you for your time!. ",
    "jaydee864": "Thank you, I'll open a support case with AWS.. ",
    "NomNomCameron": "This issue was caused by not having the AWS id and secret key set in ENV for the CI container. Thank you so much for getting back to me though! ClientStub looks like it will be very useful!. Awesome, thank you @awood45 I think I'll implement that instead of using WebMock!. ",
    "stormsilver": "This would be really great to have. Any reason not to update this and merge it?\nWe are in the process of moving all of our environments from RDS Postgres -> RDS Postgres Aurora, and API parity is quite helpful!. ",
    "herebebogans": "Hi thanks for the quick response.\nVersion info\n$ bundle list | grep -i aws-sdk\n  * aws-sdk (2.5.11)\n  * aws-sdk-core (2.5.11)\n  * aws-sdk-resources (2.5.11)\nget_distribution returns http_version=2\nlist_distribution returns http_version=nil\nQuick Example.\ncf = Aws::CloudFront::Client.new\nresp = cf.get_distribution(id: 'XXXXX')\nputs resp.distribution.distribution_config.http_version\nresp = cf.list_distributions.distribution_list.items.select{ | item | item.id == 'XXXXX'}.first\nputs resp.http_version\nputs resp.http_version.nil?\nOutput\n\nhttp2\n\ntrue\n. Hi - the wire trace output is to verbose to paste but shows the  tag for the get_distibution API call but NOT for the list_distributions API call.\n\nSeems to be the only DistributuonConfig property not present - I think the http_version option was recently added to Cloudfront and possibly the API hasn't been updated??\nSorry - the whole reason this comes up is that the awspec gem uses list_distributions and I'd like to be able to check this property. OK thanks - sorry for the get/list confusion.. Would it be to late to mention the logging property also doesn't come through list_distributions either \ud83d\ude04  Seems to be the only other discrepancy. ",
    "vbichov": "Acctually I stumbled on this when trying to filter volumes that aren't tagged.\nBut yes, it sounds great.\nThank you @cjyclaire . boilerplate workaround:\n    def getBucketACL(bucketName)\n      begin\n        return @s3.get_bucket_acl ( {:bucket => bucketName})\n      rescue Aws::S3::Errors::PermanentRedirect => e\n        Aws.config.update ({region: e.context.http_response.headers['x-amz-bucket-region']})\n        @s3 = Aws::S3::Client.new\n        return getBucketACL(bucketName) \n      end\n    end. aws-sdk-s3 (1.8.2)\nOut of 143 buckets I'm getting this error only on 2.\nThose 1 is located in us-west-1 and the other is in us-east-1\ncredentials are passed as a hash.\nI will try to understand what's different in those buckets\n. @cjyclaire thanks for the follow up.\nIt happen to only 2 buckets that were created in a nonstandard way (no idea how).\nwe deleted and recreated those buckets and we no longer experience the issue. ",
    "cory-p-oncota": "Ruby SDK is at 2.3.22\nJava SDK is at 1.11.3\nThe Java client complains \"Unable to detect encryption information for object\", however, when the header is added to the instruction file manually, the file is successfully decrypted.. It did seem that the header was completely unused by the client, but was absolutely dependent on its presence. We've currently moved our troublesome task into a Scala job, but we're very unsure where Amazon stands on the approach considering that the .NET and Java clients both set the header. Ultimately, it seems unnecessary as all clients require explicit definition of instruction file location.. ",
    "maximebedard": "Got it, thanks for the quick response. I'll investigate toward upgrading the client to v2 \ud83d\udc4d . ",
    "141984": "Thanks, \naws_cf_client.wait_until(:stack_create_complete, stack_name: resp.stack_id) works. updated the gem and works fine, Thanks :). Sorry, no.\nI do not see the update_service response containing the UUID of the tasks created to deploy the updated service.. ",
    "gitleet": "Thanks working now.\nStrange I just put 'gem aws-sdk' and it pulled down such an old version (2.3)\nThanks, I spent the morning trying to figure this out and appreciate your prompt help!. ",
    "ykessler": "Same here: upgraded gem to 2.10 and issue resolved. ",
    "hvdnew": "I am getting the same exception and the libraries I'm using are:\n1. Logstash - 6.6\n2. jruby - 2.3.0\n3. AWS SDK - 2.11.202 \n2019-02-18T13:11:21,904][FATAL][logstash.runner          ] An unexpected error occurred! {:error=>#, :backtrace=>[\"/usr/share/logstash/vendor/bundle/jruby/2.3.0/gems/aws-sdk-core-2.11.202/lib/seahorse/client/plugins/raise_response_errors.rb:15:in call'\", \"/usr/share/logstash/vendor/bundle/jruby/2.3.0/gems/aws-sdk-core-2.11.202/lib/aws-sdk-core/plugins/jsonvalue_converter.rb:20:incall'\", \"/usr/share/logstash/vendor/bundle/jruby/2.3.0/gems/aws-sdk-core-2.11.202/lib/aws-sdk-core/plugins/idempotency_token.rb:18:in call'\", \"/usr/share/logstash/vendor/bundle/jruby/2.3.0/gems/aws-sdk-core-2.11.202/lib/aws-sdk-core/plugins/param_converter.rb:20:incall'\", \"/usr/share/logstash/vendor/bundle/jruby/2.3.0/gems/aws-sdk-core-2.11.202/lib/seahorse/client/plugins/response_target.rb:21:in call'\", \"/usr/share/logstash/vendor/bundle/jruby/2.3.0/gems/aws-sdk-core-2.11.202/lib/seahorse/client/request.rb:70:insend_request'\", \"/usr/share/logstash/vendor/bundle/jruby/2.3.0/gems/aws-sdk-core-2.11.202/lib/seahorse/client/base.rb:207:in block in define_operation_methods'\", \"/usr/share/logstash/vendor/bundle/jruby/2.3.0/gems/logstash-output-sqs-5.1.2/lib/logstash/outputs/sqs.rb:172:insend_message_batch'\", \"/usr/share/logstash/vendor/bundle/jruby/2.3.0/gems/logstash-output-sqs-5.1.2/lib/logstash/outputs/sqs.rb:142:in block in multi_receive_encoded_batch'\", \"org/jruby/RubyArray.java:1734:ineach'\", \"org/jruby/RubyEnumerable.java:1067:in each_with_index'\", \"/usr/share/logstash/vendor/bundle/jruby/2.3.0/gems/logstash-output-sqs-5.1.2/lib/logstash/outputs/sqs.rb:133:inmulti_receive_encoded_batch'\", \"/usr/share/logstash/vendor/bundle/jruby/2.3.0/gems/logstash-output-sqs-5.1.2/lib/logstash/outputs/sqs.rb:120:in multi_receive_encoded'\", \"/usr/share/logstash/logstash-core/lib/logstash/outputs/base.rb:87:inmulti_receive'\", \"org/logstash/config/ir/compiler/OutputStrategyExt.java:114:in multi_receive'\", \"org/logstash/config/ir/compiler/AbstractOutputDelegatorExt.java:97:inmulti_receive'\", \"/usr/share/logstash/logstash-core/lib/logstash/pipeline.rb:373:in block in output_batch'\", \"org/jruby/RubyHash.java:1343:ineach'\", \"/usr/share/logstash/logstash-core/lib/logstash/pipeline.rb:372:in output_batch'\", \"/usr/share/logstash/logstash-core/lib/logstash/pipeline.rb:324:inworker_loop'\", \"/usr/share/logstash/logstash-core/lib/logstash/pipeline.rb:287:in `block in start_workers'\"]}. ",
    "stereobooster": "@awood45 Yes, thanks a lot.  I will try it soon. Getting this error so far. Will try to figure out what happens\n/2.2.6/lib/set.rb:212:in `include?': stack level too deep (SystemStackError)\nfrom /2.2.6/gems/aws-sdk-core-2.7.3/lib/seahorse/client/configuration.rb:213:in `method_missing'\nfrom /2.2.6/gems/aws-sdk-core-2.7.3/lib/aws-sdk-core/plugins/stub_responses.rb:18:in `block in <class:StubResponses>'\nfrom /2.2.6/gems/aws-sdk-core-2.7.3/lib/seahorse/client/configuration.rb:70:in `call'\nfrom /2.2.6/gems/aws-sdk-core-2.7.3/lib/seahorse/client/configuration.rb:70:in `call'\nfrom /2.2.6/gems/aws-sdk-core-2.7.3/lib/seahorse/client/configuration.rb:205:in `block in resolve_defaults'\nfrom /2.2.6/gems/aws-sdk-core-2.7.3/lib/seahorse/client/configuration.rb:57:in `each'\nfrom /2.2.6/gems/aws-sdk-core-2.7.3/lib/seahorse/client/configuration.rb:57:in `each'\nfrom /2.2.6/gems/aws-sdk-core-2.7.3/lib/seahorse/client/configuration.rb:204:in `resolve_defaults'\n ... 8725 levels.... Never mind this is not related to AWS. ",
    "blimmer": "That said, it would be nice if this were an Aws::S3::Errors namespaced exception so authors could catch it and handle it appropriately. \nEDIT: I filed #1435 since it's not directly related to this issue report.. ",
    "iamatypeofwalrus": "I'd like to take a stab at a pull request. Should land sometime this week. . I wasn't quite sure how to test the pool without being too prescriptive of the underlying implementation. The spec file for ConnectionPool didn't have any tests for guidance :-(. Specs passed locally but not on CI. Taking a look.. Updated documentation in latest commit. ",
    "vStone": "Looks like this should be done with the assume role credentials... ",
    "kreynolds": "Yes, apologies for the vagueness. Many of the other operations (elasticache, rds, auto_scale_groups, etc) have populated #waiter_names to watch when they are in_service or not_exists or any number of other things. ElasticLoadBalancingV2 does not appear to even though it takes some time for it to become ready: http://docs.aws.amazon.com/sdkforruby/api/Aws/ElasticLoadBalancingV2/Client.html#waiter_names-instance_method. @cjyclaire has it exactly right, specifically http://docs.aws.amazon.com/sdkforruby/api/Aws/EC2/SecurityGroup.html#authorize_ingress-instance_method\nThis works just fine:\nsecurity_group.authorize_ingress(ip_protocol: 'tcp', to_port: '80', from_port: '80', cidr_ip: '0.0.0.0/0')\n=> #<struct Aws::ACM::Types::EmptyStructure>\nThis does not, but there is no way of knowing that without checking later, no error is raised:\nload_balancer_security_group.authorize_ingress(ip_protocol: 'tcp', to_port: '80', from_port: '80')\n=> #<struct Aws::ACM::Types::EmptyStructure>\n. Its fine, its just inconsistent. If any of the other required items are missing, an error is raised rather than a no-op. AFAICT, this is the only case, perhaps in the entire API, where a required element missing ends up a no-op instead of an error.. excellent!. Of course that will work, you'd be sending it a string. The whole point is that I'm sending it an IO object, which it claims to support (File objects are fine, StringIO objects are fine). I looked through the code and based on the supported Shape configuration it looks like IO objects should be supported, I couldn't immediately tell why it was failing.\n. I'm working on a proxy system that manages certain things about how the objects are stored and retrieved in S3. Like other users, there are cases where data is available in an IO object for streaming but its size is unknown.\nI switched to the multipart implementation of upload_stream and that works ok. I think my issue here really is that IO objects are not supported at all, but the error message and documentation indicates they are. This is probably the last issue you'd see if that were clarified.. I understand what the underlying issue is now, and it makes sense. My recommendation is to change what the error says. Reading it, it looks like nonsense:\nexpected params[:body] to be a String or IO object, got value #<IO:fd 8> (class: IO) instead\nIt should be clear that it needs an object that response to read/rewind/size, not an IO object. This Issue wouldn't exist if the error were clear :). ",
    "DenverJ": "Yes, I would like to report this as a bug running version 2.8.3\nI can reproduce this every time (the key being on an instance that has been terminated but AFTER it disappears from the console and the API).\nAs I was trying to illustrate above, a describeInstances API call filtering on a recently terminated instance returns an object in the array just like filtering for a running instance would. After some time (~1hr) the describeInstances response returns an EMPTY array.\nSo to recreate the issue...\n create an instance\n terminate the instance\n wait at least 1hr (or until it disappears from the console)\n run the example code above\n. ",
    "chef1729": "I have the same issue what was done to fix it ?. ",
    "leggiero": "Thanks for considering the feature.\nI'm using the AWS_PROFILE as alternative.. ",
    "mike-bailey": "What do you mean by stubbed values if you don't mind my asking?\n\nOn Mar 14, 2017 at 12:52 PM,    wrote:\nThose are stubbed values, but I do think this is a good case for an example snippet which would help remove this kind of confusion.\n\u2014\n You are receiving this because you authored the thread.\n Reply to this email directly,   view it on GitHub (https://github.com/aws/aws-sdk-ruby/issues/1453#issuecomment-286485689), or   mute the thread (https://github.com/notifications/unsubscribe-auth/AHQTMY8ALlSYp9yN8wA5pYr2zWLQhzXqks5rlsW4gaJpZM4McB7y).\n\n]]>              \n. (FWIW I did figure out what I was doing wrong with this call :wink:, issue still remains). I'm getting the same error with put_bucket_logging and would appreciate any help.\nI've tried like ten different formats, all similar to the example, including:\nresp = client.put_bucket_logging({\n  bucket: \"oursourcebucket\",\n  bucket_logging_status: {\n    logging_enabled: {\n    target_bucket: \"outdstbucket\",\n    },\n  },\n}). >Closing due to this is a service API enhancement feature request.\nWhen you work through the console, you do not need a prefix. When you do it through the AWS Ruby SDK, you need a prefix. Am I wrong in assuming this means it's probably an SDK issue and not a service API bug?. Can this be reopened? You said feel free to reopen but I can\u2019t :) . Thank you for your help!. No problem, thank you for the clarification!. irb(main):014:0> resp = s3.put_bucket_logging({\nirb(main):015:2*   bucket: \"sourcebucket\",\nirb(main):016:2*   bucket_logging_status: {\nirb(main):017:3*     logging_enabled: {\nirb(main):018:4*     target_bucket: \"targetbucket\",\nirb(main):019:4*     },\nirb(main):020:3*   },\nirb(main):021:2* })\nopening connection to sourcebucketname.s3.amazonaws.com:443...\nopened\nstarting SSL for sourcebucketname.s3.amazonaws.com:443...\nSSL established\n<- \"PUT /?logging HTTP/1.1\\r\\nContent-Type: \\r\\nAccept-Encoding: \\r\\nUser-Agent: aws-sdk-ruby2/2.10.25 ruby/2.4.1 x86_64-darwin16\\r\\nExpect: 100-continue\\r\\nContent-Md5: gTbnihPA+E+nrr7woTgE5Q==\\r\\nX-Amz-Date: 20170824T212836Z\\r\\nHost: sourcebucket.s3.amazonaws.com\\r\\nX-Amz-Content-Sha256: 7906216862bfdea0bc9a4dd1182b4315bbd2dc6ce8884259a1101a46dbdfd05e\\r\\nAuthorization: AWS4-HMAC-SHA256 Credential=redact/20170824/us-east-1/s3/aws4_request, SignedHeaders=content-md5;host;x-amz-content-sha256;x-amz-date, Signature=redact\\r\\nContent-Length: 202\\r\\nAccept: */*\\r\\n\\r\\n\"\n-> \"HTTP/1.1 100 Continue\\r\\n\"\n-> \"\\r\\n\"\n-> \"HTTP/1.1 400 Bad Request\\r\\n\"\n-> \"x-amz-request-id: 8D18B1FA427DF7C5\\r\\n\"\n-> \"x-amz-id-2: redacted+redacted/6AhyISj4Q=\\r\\n\"\n-> \"Content-Type: application/xml\\r\\n\"\n-> \"Transfer-Encoding: chunked\\r\\n\"\n-> \"Date: Thu, 24 Aug 2017 21:28:30 GMT\\r\\n\"\n-> \"Connection: close\\r\\n\"\n-> \"Server: AmazonS3\\r\\n\"\n-> \"\\r\\n\"\n-> \"13f\\r\\n\"\nreading 319 bytes...\n-> \"<?xml version=\\\"1.0\\\" encoding=\\\"UTF-8\\\"?>\\n<Error><Code>MalformedXML</Code><Message>The XML you provided was not well-formed or did not validate against our published schema</Message><RequestId>8D18B1FA427DF7C5</RequestId><HostId>SqGmM48esm+NqvmkVTKvbFRBche3itT0uuuHfsOy8iyx5HVkDx6Qqb6nqs0NcRkkk/6AhyISj4Q=</HostId></Error>\"\nread 319 bytes\nreading 2 bytes...\n-> \"\\r\\n\"\nread 2 bytes\n-> \"0\\r\\n\"\n-> \"\\r\\n\"\nConn close\nAws::S3::Errors::MalformedXML: The XML you provided was not well-formed or did not validate against our published schema\n    from /usr/local/lib/ruby/gems/2.4.0/gems/aws-sdk-core-2.10.25/lib/seahorse/client/plugins/raise_response_errors.rb:15:in `call'\n    from /usr/local/lib/ruby/gems/2.4.0/gems/aws-sdk-core-2.10.25/lib/aws-sdk-core/plugins/s3_sse_cpk.rb:19:in `call'\n    from /usr/local/lib/ruby/gems/2.4.0/gems/aws-sdk-core-2.10.25/lib/aws-sdk-core/plugins/s3_dualstack.rb:24:in `call'\n    from /usr/local/lib/ruby/gems/2.4.0/gems/aws-sdk-core-2.10.25/lib/aws-sdk-core/plugins/s3_accelerate.rb:34:in `call'\n    from /usr/local/lib/ruby/gems/2.4.0/gems/aws-sdk-core-2.10.25/lib/aws-sdk-core/plugins/jsonvalue_converter.rb:20:in `call'\n    from /usr/local/lib/ruby/gems/2.4.0/gems/aws-sdk-core-2.10.25/lib/aws-sdk-core/plugins/idempotency_token.rb:18:in `call'\n    from /usr/local/lib/ruby/gems/2.4.0/gems/aws-sdk-core-2.10.25/lib/aws-sdk-core/plugins/param_converter.rb:20:in `call'\n    from /usr/local/lib/ruby/gems/2.4.0/gems/aws-sdk-core-2.10.25/lib/seahorse/client/plugins/response_target.rb:21:in `call'\n    from /usr/local/lib/ruby/gems/2.4.0/gems/aws-sdk-core-2.10.25/lib/seahorse/client/request.rb:70:in `send_request'\n    from /usr/local/lib/ruby/gems/2.4.0/gems/aws-sdk-core-2.10.25/lib/seahorse/client/base.rb:207:in `block (2 levels) in define_operation_methods'\n    from (irb):14\n    from /usr/local/bin/irb:11:in `<main>'\nirb(main):022:0>\nI removed anything I thought could have been private. If I'm wrong about that please LMK.. Thank you! Great response time. Let me know if there's any way I could help.. Cool, thanks. If it matters at all there's a - in one of the bucket names (which is mandatory per our use case). Let me know if I can help further.. Hello, any updates yet?. Any news? \ud83d\ude04 . Anyone home?. No problem, just making sure it wasn't abandoned.. @cjyclaire Any updates regarding this? What's the expected turnaround if you don't mind my asking?. Unfortunately I can't open a technical ticket as we don't pay for that level of support.. Interesting. Thanks for the update. Neither of these are supposed to be improper calls though, no?\nLifecycle policies should be allowed to apply to buckets and logging as well?. That also helps me because it gives me something to tell our management about how it\u2019s more than likely not the Ruby SDK\u2019s fault (other than @awood45 having just said it it isn\u2019t). Thanks!. >The :target_prefix parameter is required. This is why you're getting an error.\nHowever, in the console, a prefix isn't required? Why is this an API limitation only for these SDKs and not the console? I'd like to be able to apply it to the entire bucket.... Also, any developments on the logging call? (I'm assuming you're referring to the lifecycle bit). Thank you!. Looks like in the actual API:\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/lifecycle-configuration-examples.html\n\nIf you want the lifecycle rule to apply to all objects in the bucket, specify an empty prefix. \n\nMaybe something to add to the docs for the actual SDKs. Thank you!. >My guess is that leaving the field blank is translating to an empty target prefix\nThank you. That makes perfect sense in hindsight.\nStill learning as a measly intern, so I appreciate the patience. Thank you!. Did you do a blank one like @awood45 suggests in this issue?. Maybe post the code if you can?. Sorry, I pasted the wrong sample from #1460. #1460 was about lifecycle policy, which #1577 is not. Ideally yes #1460 should be reopened and this closed, I just opened this for visibility since the comment in #1460 must have gotten lost :) \nHow is this an API issue when it works fine in the console, but not specifically through the SDK?. Seems like it, yes.. Given they're run synchronously I'd expect one to pass after another.. It\u2019s occasional. It\u2019s an app that provisions a distinct stack so the bucket create comes a few calls earlier. . I did capture request ID's so I'll post those.\n\nOptionally, you could retry setting up replication when you encounter this error\n\nWouldn't this require each of my calls having a fallback for the subsequent calls? We make a number of S3 calls that are dependent on the prior one passing, so is the logic that it should go back a step if it fails? We'd risk infinite loops in that case so unfortunately that's a bit of a non-solution.. Thank you for replying.\nTo clarify:\n\nthe evidence indicates that the SDK is executing put_bucket_versioning and put_bucket_replication as expected\n\nI think it's a little strange to say this is as expected, as it returns no errors but didn't actually happen in a timely fashion, so I'm not sure how that's useful to a developer. I filed it here and not with the support forums because if it is expected behavior, it should have some sort of callback or block or something to indicate request status.. Not a callback, but something to be able to determine if a call was actually completed.\n. Thanks. I can get the request IDs in coming hours and I understand I can implement my own waiters, but this isn\u2019t the only AWS service and only S3 interaction we use in our app, so would we be implementing waiters for all of it? We just spotted this issue like a week or two ago and we have been using this codebase for probably 6mo+. >I'm curious if you see that returning \"Disabled\" or not.\nI can\u2019t think of how to reproduce this without polluting our account (we have a lot of sns stuff) since it would take a number of calls to replicate since it only happens on occasion . \nLess redacted for IDs. Is there anywhere I can put them out-of-band? I'm relatively low on the food chain at a forensic company so I have to limit public commentary on particulars like bucket names.\nAs for request ID, none of that is redacted, but I can pull whatever else I have in logs, sure.. We have one planned, but haven't gotten it yet. Have to wait for a budget to hit that's way above my pay grade. \ud83d\ude04 . Granted, this made it easier to justify. I\u2019m still confused as to how this isn\u2019t an SDK issue. So the argument is if there\u2019s a PUT versioning request then a PUT lifecycle request via the SDK the SDK doesn\u2019t garauntee they\u2019ll take in order?. And if the point is \u201cyes but implement a waiter\u201d, shouldn\u2019t there be a waiter available in the SDK given this is a pretty common scenario in s3?. ",
    "pgollucci": "Agreed.  I have the same issue\naws sns add-permission --topic-arn arn:aws:sns:us-east-1:X:name --label StmtLabel --aws-account-id NOTHINGWORKSHEREFORCLOUDTRAIL  --action-name publish\n. ",
    "stefansedich": "Thanks @awood45!. ",
    "MMartyn": "No problem here you go:\n```\nit 'test case' do\n  ::Aws::EC2::Client\n    .any_instance\n    .stubs(:describe_vpcs)\n    .with(filters: [{ name: 'tag:Name', values: ['Test'] }])\n    .returns(\n      ::Aws::EC2::Client.new.stub_data(\n        :describe_vpcs,\n        vpcs: [\n          {\n            vpc_id: 'vpc-12345678',\n            state: 'available',\n            cidr_block: '192.168.0.0/16',\n            dhcp_options_id: 'dopt-a1b2c345',\n            tags: [\n              { key: 'Name', value: 'Test' }\n            ],\n            instance_tenancy: 'default',\n            is_default: false\n          }\n        ]\n      )\n    )\nresource = ::Aws::EC2::Resource.new\n  resource.vpcs(\n    filters: [{ name: 'tag:Name', values: ['Test'] }]\n  ).to_a.first # throws error\nend\n``. @cjyclaire The error for the provided snippet isNoMethodError: undefined method data' for #<Aws::EC2::Types::DescribeVpcsResult but I have also hit this same error stubbing out describe_instances and calling instances on an ec2 resource.\nAs for the any_instance and stubs methods those are from mocha (https://github.com/freerange/mocha) which allow me to stub those calls out and return a stubbed response. I am using that because if I call describe_vpcs with a different filter I want a different stubbed response. . @cjyclaire Thanks for looking into it.. Alright, thanks again for looking into it. I think I see what you mean in the code where it handles stub_response differently from stub_data. While in there I noticed that stub_responses can actually take a Proc, so I am going to make use of that like so:\nstubbed_ec2_client = ::Aws::EC2::Client.new(\n  stub_responses: {\n    describe_instances: proc do |context|\n      ...\n      craft response based on context\n      ...\n    end\n  }\n). Weird. Not sure why you are unable to reproduce. I just tested it again and got the same error:\nAws::EC2::Errors::InvalidAction: The action AssociateIamInstanceProfile is not valid for this web service.. @cjyclaire I am going to go ahead and close. It turned out to be an issue on our side where we were locking down the api version higher up in our code. Thanks for your time.. ",
    "atreat": "That makes perfect sense, after an update these work great.\nThanks!. ",
    "dElogics": "But as per this documentation it must not return a struct -- \nhttp://docs.aws.amazon.com/sdkforruby/api/Aws/EC2/Resource.html#create_snapshot-instance_method\nRequesting documentation update.\nAlso what you've specified is not said in -- \nhttp://docs.aws.amazon.com/sdkforruby/api/Aws/Resources/Resource.html#wait_until-instance_method\nRequesting update here too.. Looks like a verity of issues are being hit (this time it's different) --\nnondefRT = Aws::EC2::Types::Filter.new\nnondefRT.name = \"association.main\"\nnondefRT.values = [\"false\"]\nmyVPC = Aws::EC2::Types::Filter.new\nmyVPC.name = \"vpc-id\"\nmyVPC.values = [\"vpc-8f1886e6\"]\nresource.route_tables( { :filters => [ myVPC, nondefRT ] }).each do\n    |routeTable|\n    puts routeTable.id\nend\nOutputs -- \nrtb-e99af780\nrtb-e89af781\nnondefRT = Aws::EC2::Types::Filter.new\nnondefRT.name = \"association.main\"\nnondefRT.values = [\"false\"]\nmyVPC = Aws::EC2::Types::Filter.new\nmyVPC.name = \"vpc-id\"\nmyVPC.values = [\"vpc-8f1886e6\"]\nresource.route_tables( { :filters => [ myVPC ] }).each do\n    |routeTable|\n    puts routeTable.id\nend\noutputs the same -- \nrtb-e99af780\nrtb-e89af781\nnondefRT = Aws::EC2::Types::Filter.new\nnondefRT.name = \"association.main\"\nnondefRT.values = [\"true\"]\nmyVPC = Aws::EC2::Types::Filter.new\nmyVPC.name = \"vpc-id\"\nmyVPC.values = [\"vpc-8f1886e6\"]\nresource.route_tables( { :filters => [ myVPC, nondefRT ] }).each do\n    |routeTable|\n    puts routeTable.id\nend\nProduces \nrtb-e99af780. When setting dry run to true, I got -- \n/usr/lib64/ruby/gems/2.1.0/gems/aws-sdk-core-2.6.50/lib/seahorse/client/plugins/raise_response_errors.rb:15:in `call': Request would have succeeded, but DryRun flag is set. (Aws::EC2::Errors::DryRunOperation)\n        from /usr/lib64/ruby/gems/2.1.0/gems/aws-sdk-core-2.6.50/lib/aws-sdk-core/plugins/idempotency_token.rb:18:in `call'\n        from /usr/lib64/ruby/gems/2.1.0/gems/aws-sdk-core-2.6.50/lib/aws-sdk-core/plugins/param_converter.rb:20:in `call'\n        from /usr/lib64/ruby/gems/2.1.0/gems/aws-sdk-core-2.6.50/lib/seahorse/client/plugins/response_target.rb:21:in `call'\n        from /usr/lib64/ruby/gems/2.1.0/gems/aws-sdk-core-2.6.50/lib/seahorse/client/request.rb:70:in `send_request'\n        from /usr/lib64/ruby/gems/2.1.0/gems/aws-sdk-core-2.6.50/lib/seahorse/client/base.rb:207:in `block (2 levels) in define_operation_methods'\n        from /usr/lib64/ruby/gems/2.1.0/gems/aws-sdk-resources-2.6.50/lib/aws-sdk-resources/request.rb:24:in `call'\n        from /usr/lib64/ruby/gems/2.1.0/gems/aws-sdk-resources-2.6.50/lib/aws-sdk-resources/operations.rb:41:in `call'\n        from /usr/lib64/ruby/gems/2.1.0/gems/aws-sdk-resources-2.6.50/lib/aws-sdk-resources/operations.rb:87:in `call'\n        from /usr/lib64/ruby/gems/2.1.0/gems/aws-sdk-resources-2.6.50/lib/aws-sdk-resources/operation_methods.rb:19:in `block in add_operation'\n        from ./aws.rb:114:in `<main>'\nSure it's a server side response, but it might be that SDK is not transforming the request well, because at least from the GUI can remove devices.\nAlso from the documentation that you pointed, it's still not clear what will be the value of no_device. Should it be the device node?. ",
    "augustovictor": "I got this error when I passed an empty value to deleteObjects method.\nThat happened because I forgot to handle cases where no objects to be deleted from S3 was provided.. ",
    "ignivaprerna": "Please check the provider path for Uri file. ",
    "klippx": "Thanks for the quick reply!\nIt seems that we are getting this warning when we have an expired token.\nopening connection to eu-non-production-xxx.s3-eu-west-1.amazonaws.com:443...\nopened\nstarting SSL for eu-non-production-xxx.s3-eu-west-1.amazonaws.com:443...\nSSL established\n<- \"GET /?encoding-type=url&prefix=xxx HTTP/1.1\\r\\nContent-Type: \\r\\nAccept-Encoding: \\r\\nUser-Agent: aws-sdk-ruby2/2.8.10 ruby/2.3.3 x86_64-darwin16\\r\\nX-Amz-Date: 20170323T122305Z\\r\\nHost: eu-non-production-xxx.s3-eu-west-1.amazonaws.com\\r\\nX-Amz-Security-Token: xxx\\r\\nX-Amz-Content-Sha256: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\\r\\nAuthorization: AWS4-HMAC-SHA256 Credential=xxx/20170323/eu-west-1/s3/aws4_request, SignedHeaders=host;x-amz-content-sha256;x-amz-date;x-amz-security-token, Signature=xxx\\r\\nContent-Length: 0\\r\\nAccept: */*\\r\\n\\r\\n\"\n-> \"HTTP/1.1 400 Bad Request\\r\\n\"\n-> \"x-amz-bucket-region: eu-west-1\\r\\n\"\n-> \"x-amz-request-id: B0D3F5FB5BA2AA73\\r\\n\"\n-> \"x-amz-id-2: xxx\\r\\n\"\n-> \"Content-Type: application/xml\\r\\n\"\n-> \"Transfer-Encoding: chunked\\r\\n\"\n-> \"Date: Thu, 23 Mar 2017 12:23:05 GMT\\r\\n\"\n-> \"Connection: close\\r\\n\"\n-> \"Server: AmazonS3\\r\\n\"\n-> \"\\r\\n\"\n-> \"2e8\\r\\n\"\nreading 744 bytes...\n-> \"<?xml version=\\\"1.0\\\" encoding=\\\"UTF-8\\\"?>\\n<Error><Code>ExpiredToken</Code><Message>The provided token has expired.</Message><Token-0>xxx</Token-0><RequestId>B0D3F5FB5BA2AA73</RequestId><HostId>xxx</HostId></Error>\"\nread 744 bytes\nreading 2 bytes...\n-> \"\\r\\n\"\nread 2 bytes\n-> \"0\\r\\n\"\n-> \"\\r\\n\"\nConn close\nS3 client configured for \"eu-west-1\" but the bucket \"eu-non-production-xxx\" is in \"eu-west-1\"; Please configure the proper region to avoid multiple unnecessary redirects and signing attempts\nopening connection to eu-non-production-xxx.s3-eu-west-1.amazonaws.com:443...\nAfter subsequent login in the warning disappears.. ",
    "kidbrax": "To clarify, I am talking about a record set which does allow a leftmost wildcard, not a hosted zone.\n\nYou can also create resource record sets that include * in the name. DNS treats the * character either as a wildcard or as the * character (ASCII 42), depending on where it appears in the name. Note the following restrictions on using * as a wildcard in the name of resource record sets:\nThe * must replace the leftmost label in a domain name, for example, .example.com. It can't replace any of the middle labels, for example, marketing..example.com.\nThe * must replace the entire label. For example, you can't specify prod.example.com or prod.example.com.\nYou can't use the * as a wildcard for resource records sets that have a type of NS.\nFor resource record sets, if you include * in any position other than the leftmost label in a domain name, DNS treats it as an * character (ASCII 42), not as a wildcard.\n\nIn my case, the asterisk is in the leftmost position.  However, when i retrieve it through the api, it is shown as the ASCII character.  It seems that Amazon's documentation is misleading and I have a support ticket out to them as well.  I guess what I'm asking for is maybe a flag/switch that will convert ascii characters.  That way, we're covered either way.. ",
    "junedkazi": "@awood45 just wanted to follow up to see if you have any updates ?. Thank you for such a quick turn around.. ",
    "Linuus": "It was actually only the force_path_style option that was required. My bad.. ",
    "oyeanuj": "@cjyclaire I am using v3 and the code below, but wondering how can I add a custom header to my PUT request. Adding it on the client-side throws an error, so it seems like I must do it while creating the pre-signed url?\nrb\nsigner = Aws::S3::Presigner.new(client: s3)\nself.presigned_url = signer.presigned_url(\n  :put_object,\n  bucket:   self.bucket,\n  key:  self.file_key\n)\nSo, is it possible right now in v3? If not, what would be the equivalent code for the code above with the gem mentioned above?\nThank you!. @cjyclaire Thanks! Just curious, is there an advantage of using aws-sigv4 over v3 aws-sdk-s3? FWIW, I need the url just to pass to the client side to make the request.. ",
    "JoeMcGuire": "+1 for fixing, while we wait we have requested an API rate increase.. ",
    "nhinds": "AssumeRoleCredentials doesn't have a way to read the role and source profile from an AWS profile, so it does not help for the case of assuming a role from a profile with MFA. It does work when you know the role, MFA serial, and source credentials in advance though.\nI don't particularly mind whether the default credential provider supports prompting for MFA, as long as it's possible to get the AssumeRoleCredentials somehow. It would be sufficient for my use case to just mark Aws.shared_config.assume_role_credentials_from_config as public API, or provide another mechanism for doing the same thing (assuming a role from a profile with MFA, without needing to know the role ARN, MFA serial, or source profile in advance). ",
    "mhussain": "@awood45 Apologies if the initial issue seemed sparse on details as I was myself not sure what else to put in there because the code is simple enough and no errors/exceptions are being thrown at all.\n\nIn what stage is the poller stalling?\n\nBased on the debug messages, its always at the end of the polling loop. E.g. given the following code \n```ruby\npoller.poll(....) do |m,s|\n  logger.debug 'Starting poll...'\nSOME AMAZING CODE\nlogger.debug 'End of poll'\nend\n``\nIt always pauses right after showingEnd of poll`. \n\nAre you just not seeing processed messages over a period of time?\n\nWhatever it processes, I see immediately! The stall is that the code simply refuses to pull any new messages out of the queue, even though there are 100's.\n\nHow do you know it is stalling instead of just a period of time where no messages are received by the poll call?\n\nThat's a fair point. I guess what I am seeing is that there are 100's of messages in the queue which this poller is polling. It processes a number, then simply stops even though there are more messages in it, and then after a random interval starts again. \nHope this helps and thanks for your help \ud83d\ude04 . @awood45 Thanks for that tip. :) \nI put that option in and I can definitely see that the poller is talking to the queue. However, when the down period happens, nothing comes back, not even the http_wire_trace output. \n. ",
    "toshitapandey": "Thanks @cjyclaire, @awood45, I was using v2.6.9 and that's why this issue.. ",
    "bankair": "Yep, I agree on the usefulness of the exception \ud83d\ude04 . ",
    "bobziuchkovski": "Hmm...it looks like I can make it work by editing aws-sdk-core/apis/cloudformation/2010-05-15/api-2.json and adding the ClientRequestToken shape and member definitions for those operations.  Is there anything else I'd need to update, or could I just submit those changes via a pull request?. @cjyclaire Thank you very much for updating that so quickly!!. Okay, thank you for the clarification.. ",
    "brianolson": "This bug exists in the current documentation for put_item.\nhttp://docs.aws.amazon.com/sdk-for-ruby/v3/api/Aws/DynamoDB/Client.html#put_item-instance_method\nitem => {\"Field\" => {s: \"value\"}} # does not work, doc example is wrong\nitem => {\"Field\" => \"value}} # does work. ",
    "SatkarEE": "@cjyclaire is this issue fixed or still exist?. ",
    "marcelocarlos": "Thanks Alex, \nI'd rather avoid using the first option exactly because of the caveat of the environment variables. I've had quite a few problems with that in the past and I'd rather make everything explicit and predictable. It seems that it is the cause of the \"problems\" I'm having in my tests when using the default credential provider are also related to that. Even when I the profile is explicitly defined (e.g. profile: 'dev') it completely ignores it if environment variables are defined. Is that the correct behaviour even when specifying theprofile` parameter? If so, shouldn't it be a little clearer?\nAnyhow, using AssumeRoleCredentials directly seems a better option. Any idea about how far is that in the roadmap? \nI've implemented an method to perform that in the meantime, which parses both ~/.aws/credentials and ~/.aws/config and uses Aws::STS::Client and Aws::AssumeRoleCredentials if a profile contains role_arn. Also, if it finds source_profile it forces the use of aws_access_key_id and aws_secret_access_key of the source profile. This is working fine for me, but it is more of a temporary measure.. ",
    "sharmaanshul2102": "@marcelocarlos would you mind sharing the code.. ",
    "gbonk": "Nevermind.  My fault.  I'm not iterating correctly...\n          client.list_tags( resource_id_list: trailArns).resource_tag_list.each do |tag|\n\n          end. It is the way my brain works.  I have to ask the question first then the answer comes to me.  I guess I need a better rubber duckie :-).\n",
    "cunnie": "Hi @awood45 ,\nI've posted a gist of the trace here. Line 409 seems to be the point of interest, and it seems to have the proper information.\nI've redacted it to remove our rspec debugging lines and to anonymize our AWS KEY.  please let me know if I have left any creds in the trace that may compromise the security of our AWS account. Thanks.\nThere is a possibility that our IPv6 set is incorrect/incomplete, and that's causing the problem rather than a bug in the SDK. Please let us know if that's the case \u2014 we are barely beginning to make inroads into IPv6. . Thanks @cjyclaire \u2014 that's big help!. @cjyclaire : I'm closing this issue because there's an open PR to fix and, on our end, we've inserted a nil-check in our code to get past this particular error. Thanks again for all your help.. I defer to your wisdom as far as whether to merge the pull request. We've inserted a nil-check to fix our problem, so we're okay on our end.. ",
    "Shockolate": "@cjyclaire my ruby REPL  seems to agree with you. ~~Double~~ Triple checking my code :). @cjyclaire Totally a bug in my code. I thought #list_aliases was called 0 times before the source code I posted. I was wrong: #list_aliases was called once before.\nSorry for wasting your time :/. ",
    "rpradhi": "@awood45 Thanks for you for the detailed explanation. It helped\nBetween from my use case will AWS CloudFront signing URL serves the purpose of Long time expiry? . ",
    "cconstantine": "Oh wow, yeah.  Not having a public API for getting beanstalk tags seems like a decent reason for this gem not having a method for getting those tags.  Thanks for reaching out internally.  \nIt appears that we don't have a high enough support contract to make a support request :(. ",
    "deepeeess": "The example in the official AWS doc I posted double escapes it, I've tried it single escaped, still still bombs out. Ref:\nhttp://docs.aws.amazon.com/sdkforruby/api/Aws/SES/Client.html#send_raw_email-instance_method. I'll verify it again, but I'm almost certain both data: \"From: user@domain.com\\\\nTo: user@domain.com\"  and  data: \"From: user@domain.com\\nTo: user@domain.com\" will fail.  It could be that I need a trailing \\n after the To: field.  Give me a few minutes and I'll test.. Ok. I was wrong.  The double escaped \\\\n as specified in the documentation is what causes the failure.  It will work with or without a single escaped \\n  after the To: field. Thank you \ud83d\udc4d. Yeah I agree the documentation should be updated.  Thank you again for your help.. ",
    "ohTHATaaronbrown": "Hmm... That's odd. I wonder if the ambient default encoding for the Ruby install might have something to do with it. I noticed yesterday that mine was set to IBM437 for some unknown reason.\n```bash\n$ irb\nSwitch to inspect mode.\nEncoding.default_internal\nEncoding.default_internal\nnil\nEncoding.default_external\nEncoding.default_external\n\n```\nI'll investigate further in my environment, and if I can come up with a solid set of reproduction steps that I can codify into a test, I'll look into making up a PR for it.\nThanks for examining!. ",
    "stiller-leser": "First off, thanks for the elaborations. I really appreciate it.\nThe use case here is the following: \nUploading a file every x seconds from the user's browser directly to AWS S3. Yes, we could send a request every x seconds to the rails server to generate a signed url for that specific use case. The goal however was to minimize the interaction with the server as they could not only potentially DDOS the server when many users are doing the same (I exaggerate intentionally), but the upload is also fairly time critical and interaction with the server might take some of this valuable time.\nWe did switch to JavaScript SDK for now - which works fine. However our initial goal was to avoid bringing in another service (Cognito). . ",
    "mbaxa": "ERROR:  While executing gem ... (TypeError)\n    no implicit conversion of nil into String\n    /System/Library/Frameworks/Ruby.framework/Versions/2.0/usr/lib/ruby/2.0.0/rubygems/installer.rb:171:in `check_executable_overwrite'\n    /System/Library/Frameworks/Ruby.framework/Versions/2.0/usr/lib/ruby/2.0.0/rubygems/installer.rb:384:in `block in generate_bin'\n    /System/Library/Frameworks/Ruby.framework/Versions/2.0/usr/lib/ruby/2.0.0/rubygems/installer.rb:371:in `each'\n    /System/Library/Frameworks/Ruby.framework/Versions/2.0/usr/lib/ruby/2.0.0/rubygems/installer.rb:371:in `generate_bin'\n    /System/Library/Frameworks/Ruby.framework/Versions/2.0/usr/lib/ruby/2.0.0/rubygems/installer.rb:231:in `install'\n    /System/Library/Frameworks/Ruby.framework/Versions/2.0/usr/lib/ruby/2.0.0/rubygems/dependency_installer.rb:379:in `block in install'\n    /System/Library/Frameworks/Ruby.framework/Versions/2.0/usr/lib/ruby/2.0.0/rubygems/dependency_installer.rb:339:in `each'\n    /System/Library/Frameworks/Ruby.framework/Versions/2.0/usr/lib/ruby/2.0.0/rubygems/dependency_installer.rb:339:in `each_with_index'\n    /System/Library/Frameworks/Ruby.framework/Versions/2.0/usr/lib/ruby/2.0.0/rubygems/dependency_installer.rb:339:in `install'\n    /System/Library/Frameworks/Ruby.framework/Versions/2.0/usr/lib/ruby/2.0.0/rubygems/commands/install_command.rb:166:in `block in execute'\n    /System/Library/Frameworks/Ruby.framework/Versions/2.0/usr/lib/ruby/2.0.0/rubygems/commands/install_command.rb:158:in `each'\n    /System/Library/Frameworks/Ruby.framework/Versions/2.0/usr/lib/ruby/2.0.0/rubygems/commands/install_command.rb:158:in `execute'\n    /System/Library/Frameworks/Ruby.framework/Versions/2.0/usr/lib/ruby/2.0.0/rubygems/command.rb:305:in `invoke_with_build_args'\n    /System/Library/Frameworks/Ruby.framework/Versions/2.0/usr/lib/ruby/2.0.0/rubygems/command_manager.rb:170:in `process_args'\n    /System/Library/Frameworks/Ruby.framework/Versions/2.0/usr/lib/ruby/2.0.0/rubygems/command_manager.rb:130:in `run'\n    /System/Library/Frameworks/Ruby.framework/Versions/2.0/usr/lib/ruby/2.0.0/rubygems/gem_runner.rb:60:in `run'\n    /usr/bin/gem:21:in `<main>'. Did sudo gem update --system, it worked fine, now I get the following different error when running sudo gem install -V --backtrace aws-sdk\nERROR:  While executing gem ... (Errno::EPERM)\n    Operation not permitted - /usr/bin/aws.rb\n    /Library/Ruby/Site/2.0.0/rubygems/installer.rb:504:in `initialize'\n    /Library/Ruby/Site/2.0.0/rubygems/installer.rb:504:in `open'\n    /Library/Ruby/Site/2.0.0/rubygems/installer.rb:504:in `generate_bin_script'\n    /Library/Ruby/Site/2.0.0/rubygems/installer.rb:484:in `block in generate_bin'\n    /Library/Ruby/Site/2.0.0/rubygems/installer.rb:468:in `each'\n    /Library/Ruby/Site/2.0.0/rubygems/installer.rb:468:in `generate_bin'\n    /Library/Ruby/Site/2.0.0/rubygems/installer.rb:308:in `install'\n    /Library/Ruby/Site/2.0.0/rubygems/resolver/specification.rb:97:in `install'\n    /Library/Ruby/Site/2.0.0/rubygems/request_set.rb:166:in `block in install'\n    /Library/Ruby/Site/2.0.0/rubygems/request_set.rb:156:in `each'\n    /Library/Ruby/Site/2.0.0/rubygems/request_set.rb:156:in `install'\n    /Library/Ruby/Site/2.0.0/rubygems/commands/install_command.rb:205:in `install_gem'\n    /Library/Ruby/Site/2.0.0/rubygems/commands/install_command.rb:255:in `block in install_gems'\n    /Library/Ruby/Site/2.0.0/rubygems/commands/install_command.rb:251:in `each'\n    /Library/Ruby/Site/2.0.0/rubygems/commands/install_command.rb:251:in `install_gems'\n    /Library/Ruby/Site/2.0.0/rubygems/commands/install_command.rb:158:in `execute'\n    /Library/Ruby/Site/2.0.0/rubygems/command.rb:310:in `invoke_with_build_args'\n    /Library/Ruby/Site/2.0.0/rubygems/command_manager.rb:169:in `process_args'\n    /Library/Ruby/Site/2.0.0/rubygems/command_manager.rb:139:in `run'\n    /Library/Ruby/Site/2.0.0/rubygems/gem_runner.rb:55:in `run'\n    /usr/bin/gem:21:in `<main>'\nI've verified that there is no /usr/bin/aws.rb, /usr/bin is owned by root:wheel, and the gem install was run as root, so not sure why it couldn't write the file (if that is the problem). Yeah, looks like system ruby is just a no go for OS X, too many problems getting it to work.. ",
    "pranavshenoy": "This error comes when the name of the project contains the word \"Nil\".\nwhen i removed it, this error is not there. Not sure why its not interpreted as NIl. ",
    "astratto": "@awood45 sure, thanks for looking into this.\nCloudFormation outputs have a Key, Value, Description and optional Export name: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/outputs-section-structure.html\n\nThe Export Name is not extracted, even when present.\nLet me know if you need more info.. Okay, using the SDK I get back a Aws::CloudFormation::Types::DescribeStackOutput with a stacks list of Aws::CloudFormation::Types::Stack that then have parameters of type Aws::CloudFormation::Types::Output.\n```\n\n```\nNothing surprising, so I used the aws cli and the export names are not exposed there neither...\n{ \n   \"Description\": \"Test output\",\n   \"OutputKey\": \"MyOutput\",\n   \"OutputValue\": \"output.value\"\n},\nAnd then found this: https://github.com/aws/aws-cli/issues/2637. ",
    "brogand93": "Any updates on this @awood45? I'd love to see this added to the SDK. ",
    "Chocksy": "@cjyclaire i managed to get the info from the request.raw_post and this is what it looks like:\n{\"notificationType\":\"Bounce\",\"bounce\":{\"bounceType\":\"Permanent\",\"bounceSubType\":\"General\",\"bouncedRecipients\":[{\"emailAddress\":\"bounce@simulator.amazonses.com\",\"action\":\"failed\",\"status\":\"5.1.1\",\"diagnosticCode\":\"smtp; 550 5.1.1 user unknown\"}],\"timestamp\":\"2017-07-27T17:59:38.077Z\",\"feedbackId\":\"0100015d853436fd-0f40b4c1-ff25-45be-ab20-xxxxxxxx-000000\",\"remoteMtaIp\":\"205.251.242.49\",\"reportingMTA\":\"dsn; a8-88.smtp-out.amazonses.com\"},\"mail\":{\"timestamp\":\"2017-07-27T17:59:37.000Z\",\"source\":\"support@epicpxls.com\",\"sourceArn\":\"arn:aws:ses:us-east-1:xxxxxxxxx:identity/support@epicpxls.com\",\"sourceIp\":\"54.147.166.110\",\"sendingAccountId\":\"xxxxxxxxxxxxx\",\"messageId\":\"0100015d85343569-2d17ae37-d6df-4e50-afec-9d8f38d3a3ce-000000\",\"destination\":[\"bounce@simulator.amazonses.com\"]}}\n--\nIt seems fine to me.\nNote: i changed a few of the ids to xxx. Hmm so i should not very the message then? \nI actually took most of the inspiration from this blog post that seems to be doing what i have in my code https://www.downrightlies.net/posts/2015/05/19/setting-up-ses-on-aws-to-send-emails-from-rails.html\nSo i don't understand. Did you guys change the notification?. Ok thanks @cjyclaire for your help. :D . So only for that and not the bounces and complains etc. I guess that would answer it as the initial subscription confirmation works fine but the bounce calls raise the error. I think i can live with it turned off for now.. ",
    "llibicpep": "@awood45 thanks for quick response. Seems that I've found the root cause and it belongs to kitchen-ec2. https://github.com/test-kitchen/kitchen-ec2/blob/v1.3.2/lib/kitchen/driver/aws/client.rb#L77 looks like as the last resort they are trying to pick up InstanceProfileCredentials directly not looking at AWS_CONTAINER_CREDENTIALS_RELATIVE_URI and not using chain resolve. So I am closing this issue.. ",
    "thegreyfellow": "Okay :disappointed:, thanks for the clarifications.. Thanks for the reply, I don't know how I missed this method. . ",
    "jasonperrone": "Now this is interesting. I can't even download a file from S3 using Edge right in the S3 console!. Agreed, thank you for the response.. ",
    "tpett": "Wow, way to be super responsive guys!. Ah yes, I was looking at this documentation and you are correct it does say String. I missed that. I don't think there's any documentation that needs to be updated. It seems like it would be low cost to simply allow either symbol or string here and no compatibility will be broken, but it will be less surprising for discovery.\nNow switching over to just my opinion: I think that symbols would be a more idiomatic Ruby representation of the :mode value in this case. Especially since the keys of the options hash are symbols as well. That being said it's not very important what you do under the hood or in documentation as long as both work and it appears consistent from the outside.\nThanks for all your work on this gem!",
    "tomalok": "r never gets set, because ec2.describe_host_reservations is failing (something with parsing the XML response?)\nFWIW aws-cli finds our reservations just fine...\n~ $ aws ec2 describe-host-reservations --region us-west-2\n{\n    \"HostReservationSet\": [\n        {\n            \"Count\": 1,\n            \"End\": \"2018-02-23T16:47:07Z\",\n            \"HourlyPrice\": \"1.791\",\n            \"InstanceFamily\": \"m4\",\n            \"OfferingId\": \"hro-00000000000000000\",\n            \"PaymentOption\": \"NoUpfront\",\n            \"State\": \"active\",\n            \"HostIdSet\": [\n                \"h-00000000000000000\"\n            ],\n            \"Start\": \"2017-02-23T16:47:07Z\",\n            \"HostReservationId\": \"hr-00000000000000000\",\n            \"UpfrontPrice\": \"0.000\",\n            \"Duration\": 31536000\n        },\n        {\n            \"Count\": 1,\n            \"End\": \"2018-02-23T16:47:24Z\",\n            \"HourlyPrice\": \"1.791\",\n            \"InstanceFamily\": \"m4\",\n            \"OfferingId\": \"hro-00000000000000\",\n            \"PaymentOption\": \"NoUpfront\",\n            \"State\": \"active\",\n            \"HostIdSet\": [\n                \"h-00000000000000000\"\n            ],\n            \"Start\": \"2017-02-23T16:47:24Z\",\n            \"HostReservationId\": \"hr-00000000000000000\",\n            \"UpfrontPrice\": \"0.000\",\n            \"Duration\": 31536000\n        }\n    ]\n}\n. XXX = stripped IDs, etc.\nopening connection to ec2.us-west-2.amazonaws.com:443...\nopened\nstarting SSL for ec2.us-west-2.amazonaws.com:443...\nSSL established\n<- \"POST / HTTP/1.1\\r\\nContent-Type: application/x-www-form-urlencoded; charset=utf-8\\r\\nAccept-Encoding: \\r\\nUser-Agent: aws-sdk-ruby2/2.10.24 ruby/2.4.1 x86_64-linux-musl\\r\\nX-Amz-Date: 20170814T171717Z\\r\\nHost: ec2.us-west-2.amazonaws.com\\r\\nX-Amz-Content-Sha256: XXX\\r\\nAuthorization: AWS4-HMAC-SHA256 Credential=XXX/20170814/us-west-2/ec2/aws4_request, SignedHeaders=content-type;host;x-amz-content-sha256;x-amz-date, Signature=XXX\\r\\nContent-Length: 50\\r\\nAccept: */*\\r\\n\\r\\n\"\n-> \"HTTP/1.1 200 OK\\r\\n\"\n-> \"Content-Type: text/xml;charset=UTF-8\\r\\n\"\n-> \"Transfer-Encoding: chunked\\r\\n\"\n-> \"Vary: Accept-Encoding\\r\\n\"\n-> \"Date: Mon, 14 Aug 2017 17:20:24 GMT\\r\\n\"\n-> \"Server: AmazonEC2\\r\\n\"\n-> \"\\r\\n\"\n-> \"625\\r\\n\"\nreading 1573 bytes...\n-> \"<?xml version=\\\"1.0\\\" encoding=\\\"UTF-8\\\"?>\\n<DescribeHostReservationsResponse xmlns=\\\"http://ec2.amazonaws.com/doc/2016-11-15/\\\">\\n    <requestId>XXX</requestId>\\n    <hostReservationSet>\\n        <item>\\n            <upfrontPrice>0.000</upfrontPrice>\\n            <count>1</count>\\n            <start>2017-02-23T16:47:07Z</start>\\n            <instanceFamily>m4</instanceFamily>\\n            <offeringId>hro-XXX</offeringId>\\n            <duration>31536000</duration>\\n            <paymentOption>NoUpfront</paymentOption>\\n            <end>2018-02-23T16:47:07Z</end>\\n            <state>active</state>\\n            <hostReservationId>hr-XXX</hostReservationId>\\n            <hourlyPrice>1.791</hourlyPrice>\\n            <hostIdSet>\\n                <item>h-XXX</item>\\n            </hostIdSet>\\n        </item>\\n        <item>\\n            <upfrontPrice>0.000</upfrontPrice>\\n            <count>1</count>\\n            <start>2017-02-23T16:47:24Z</start>\\n            <instanceFamily>m4</instanceFamily>\\n            <offeringId>hro-XXX</offeringId>\\n            <duration>31536000</duration>\\n            <paymentOption>NoUpfront</paymentOption>\\n            <end>2018-02-23T16:47:24Z</end>\\n            <state>active</state>\\n            <hostReservationId>hr-XXX</hostReservationId>\\n            <hourlyPrice>1.791</hourlyPrice>\\n            <hostIdSet>\\n                <item>h-XXX</item>\\n            </hostIdSet>\\n        </item>\\n    </hostReservationSet>\\n</DescribeHostReservationsResponse>\"\nread 1573 bytes\nreading 2 bytes...\n-> \"\\r\\n\"\nread 2 bytes\n-> \"0\\r\\n\"\n-> \"\\r\\n\"\nConn keep-alive\nNotImplementedError - NotImplementedError\n(FWIW, describe_host_reservations is currently the only method I'm having trouble with -- describe_hosts, describe_instances, describe_reserved_instances, etc. are all working fine.). emailing you the RequestID, Alex.... ...please confirm receipt of RequestID.. The script at the top of this (discussion) thread is not threaded.  So, threading shouldn't be in play (at the moment).\nPer the backtrace, nokogiri 1.8.0 appears, but here's the whole Gemfile.lock, anyways...\n```\nGEM\n  remote: https://rubygems.org/\n  specs:\n    actionpack (5.1.3)\n      actionview (= 5.1.3)\n      activesupport (= 5.1.3)\n      rack (~> 2.0)\n      rack-test (~> 0.6.3)\n      rails-dom-testing (~> 2.0)\n      rails-html-sanitizer (~> 1.0, >= 1.0.2)\n    actionview (5.1.3)\n      activesupport (= 5.1.3)\n      builder (~> 3.1)\n      erubi (~> 1.4)\n      rails-dom-testing (~> 2.0)\n      rails-html-sanitizer (~> 1.0, >= 1.0.3)\n    activemodel (5.1.3)\n      activesupport (= 5.1.3)\n    activerecord (5.1.3)\n      activemodel (= 5.1.3)\n      activesupport (= 5.1.3)\n      arel (~> 8.0)\n    activesupport (5.1.3)\n      concurrent-ruby (~> 1.0, >= 1.0.2)\n      i18n (~> 0.7)\n      minitest (~> 5.1)\n      tzinfo (~> 1.1)\n    arel (8.0.0)\n    aws-sdk (2.10.28)\n      aws-sdk-resources (= 2.10.28)\n    aws-sdk-core (2.10.28)\n      aws-sigv4 (~> 1.0)\n      jmespath (~> 1.0)\n    aws-sdk-resources (2.10.28)\n      aws-sdk-core (= 2.10.28)\n    aws-sigv4 (1.0.1)\n    bigdecimal (1.3.2)\n    builder (3.2.3)\n    closure_tree (6.6.0)\n      activerecord (>= 4.1.0)\n      with_advisory_lock (>= 3.0.0)\n    concurrent-ruby (1.0.5)\n    erubi (1.6.1)\n    httpclient (2.8.3)\n    i18n (0.8.6)\n    jmespath (1.3.1)\n    json (2.1.0)\n    loofah (2.0.3)\n      nokogiri (>= 1.5.9)\n    method_source (0.8.2)\n    mini_portile2 (2.2.0)\n    minitest (5.10.3)\n    mustermann (1.0.0)\n    mysql2 (0.4.8)\n    nokogiri (1.8.0)\n      mini_portile2 (~> 2.2.0)\n    pg (0.21.0)\n    rack (2.0.3)\n    rack-protection (2.0.0)\n      rack\n    rack-test (0.6.3)\n      rack (>= 1.0)\n    rails-dom-testing (2.0.3)\n      activesupport (>= 4.2.0)\n      nokogiri (>= 1.6)\n    rails-html-sanitizer (1.0.3)\n      loofah (~> 2.0)\n    railties (5.1.3)\n      actionpack (= 5.1.3)\n      activesupport (= 5.1.3)\n      method_source\n      rake (>= 0.8.7)\n      thor (>= 0.18.1, < 2.0)\n    rake (12.0.0)\n    retries (0.0.5)\n    sinatra (2.0.0)\n      mustermann (~> 1.0)\n      rack (~> 2.0)\n      rack-protection (= 2.0.0)\n      tilt (~> 2.0)\n    sinatra-activerecord (2.0.13)\n      activerecord (>= 3.2)\n      sinatra (>= 1.0)\n    standalone_migrations (5.2.3)\n      activerecord (>= 4.2.7, < 5.2.0)\n      railties (>= 4.2.7, < 5.2.0)\n      rake (>= 10.0)\n    thor (0.19.4)\n    thread_safe (0.3.6)\n    tilt (2.0.8)\n    trollop (2.1.2)\n    tzinfo (1.2.3)\n      thread_safe (~> 0.1)\n    vault (0.10.1)\n    with_advisory_lock (3.1.0)\n      activerecord (>= 3.2)\n      thread_safe\nPLATFORMS\n  ruby\nDEPENDENCIES\n  aws-sdk\n  bigdecimal\n  closure_tree\n  httpclient\n  json\n  mysql2 (>= 0.4.0)\n  pg\n  retries\n  sinatra\n  sinatra-activerecord\n  standalone_migrations\n  trollop\n  vault\nBUNDLED WITH\n   1.15.0\n```. This appears to be working fine with v3 of the SDK.. ",
    "route": "Also maybe it's not related to this but sometimes on another side when I receive messages, this issue started to happen more often recently:\n```\nWe encountered an internal error. Please try again.\n/app/vendor/bundle/ruby/2.4.0/gems/aws-sdk-core-2.10.17/lib/seahorse/client/plugins/raise_response_errors.rb:15:in call'\n/app/vendor/bundle/ruby/2.4.0/gems/aws-sdk-core-2.10.17/lib/aws-sdk-core/plugins/jsonvalue_converter.rb:20:incall'\n/app/vendor/bundle/ruby/2.4.0/gems/aws-sdk-core-2.10.17/lib/aws-sdk-core/plugins/idempotency_token.rb:18:in call'\n/app/vendor/bundle/ruby/2.4.0/gems/aws-sdk-core-2.10.17/lib/aws-sdk-core/plugins/param_converter.rb:20:incall'\n/app/vendor/bundle/ruby/2.4.0/gems/aws-sdk-core-2.10.17/lib/seahorse/client/plugins/response_target.rb:21:in call'\n/app/vendor/bundle/ruby/2.4.0/gems/aws-sdk-core-2.10.17/lib/seahorse/client/request.rb:70:insend_request'\n/app/vendor/bundle/ruby/2.4.0/gems/aws-sdk-core-2.10.17/lib/seahorse/client/base.rb:207:in block (2 levels) in define_operation_methods'\n/app/app/lib/crawl/sqs_client.rb:20:inreceive_message'\n/app/app/lib/crawl/queue_listener/base.rb:35:in block in poll'\n/app/app/lib/crawl/queue_listener/base.rb:33:inloop'\n/app/app/lib/crawl/queue_listener/base.rb:33:in poll'\n/app/app/lib/crawl/queue_listener/crawl.rb:18:inblock in start'\n/app/app/lib/crawl/queue_listener/base.rb:62:in block in thread_pool'\n/app/vendor/bundle/ruby/2.4.0/gems/concurrent-ruby-1.0.5/lib/concurrent/executor/ruby_thread_pool_executor.rb:348:inrun_task'\n/app/vendor/bundle/ruby/2.4.0/gems/concurrent-ruby-1.0.5/lib/concurrent/executor/ruby_thread_pool_executor.rb:337:in block (3 levels) in create_worker'\n/app/vendor/bundle/ruby/2.4.0/gems/concurrent-ruby-1.0.5/lib/concurrent/executor/ruby_thread_pool_executor.rb:320:inloop'\n/app/vendor/bundle/ruby/2.4.0/gems/concurrent-ruby-1.0.5/lib/concurrent/executor/ruby_thread_pool_executor.rb:320:in block (2 levels) in create_worker'\n/app/vendor/bundle/ruby/2.4.0/gems/concurrent-ruby-1.0.5/lib/concurrent/executor/ruby_thread_pool_executor.rb:319:incatch'\n/app/vendor/bundle/ruby/2.4.0/gems/concurrent-ruby-1.0.5/lib/concurrent/executor/ruby_thread_pool_executor.rb:319:in `block in create_worker'\n```\nError class remains unknown so far, but I can figure it out later on.\n. The SQS publisher is on DigitalOcean (Seahorse::Client::NetworkingError error), while consumer is on Heroku which is experiencing \"We encountered an internal error. Please try again\" error.. ",
    "TDA": "@awood45 Is there a place where I can track that feature request?. ",
    "ddimmich": "having the same issue from within logstash.  would be great to get an update.. In the context of logstash which uses this library - how would one add the target prefix parameter?  Is this something that needs to be addressed in logstash or in this library?  Thank you. Ok - Thanks.\u00a0 Not sure this is one of the parameters that would get \npassed through - haven't seen a configuration equivalent so it sounds \nlike this would need to be fixed upstream :/\nOn 04/01/2018 11:30, Alex Wood wrote:\n\nThat would be added in the caller - logstash. Unless logstash plumbs \nthrough your own parameters, in which case you would need to add it there.\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub \nhttps://github.com/aws/aws-sdk-ruby/issues/1577#issuecomment-355216092, \nor mute the thread \nhttps://github.com/notifications/unsubscribe-auth/AAD87kPOqqz6SopXP8EoWTjSNDiSwlOvks5tHH4FgaJpZM4PAoW0.\n\n\n. ",
    "admorphit": "This is happening for the nodejs aws-sdk as well in case that helps.. ",
    "ramd123": "I was facing the same issue while setting analytics configuration using aws java sdk, this should have fixed Amazon by now. any solution to skip filter/predicate while creating analytics using java sdk?\nif i add Filter with Predicate it works correct but i dont want add the predicate/prefix to the configuration.. no, blank also not working..  same exception with blank as well.. private void manageAnalytics(String name, AmazonS3 s3) {\n    if (name.equalsIgnoreCase(\"abc-files\")) {\n        SetBucketAnalyticsConfigurationRequest req = new SetBucketAnalyticsConfigurationRequest();\n        req.setBucketName(name);\n        AnalyticsS3BucketDestination s3destination = new AnalyticsS3BucketDestination()\n                .withBucketAccountId(\"12345\").withBucketArn(\"arn:aws:s3:::test-lifecycle\")\n                .withFormat(AnalyticsS3ExportFileFormat.CSV);\n        AnalyticsExportDestination destination = new AnalyticsExportDestination()\n                .withS3BucketDestination(s3destination);\n        StorageClassAnalysisDataExport dataExport = new StorageClassAnalysisDataExport()\n                .withDestination(destination).withOutputSchemaVersion(\"V_1\");\n        AnalyticsConfiguration analyticsConfiguration = new AnalyticsConfiguration()\n                .withId(name + \"-analytics-conf-2\")\n                .withStorageClassAnalysis(new StorageClassAnalysis().withDataExport(dataExport))\n                .withFilter(new AnalyticsFilter().withPredicate(new AnalyticsPrefixPredicate(\"123456\")));\n        req.setAnalyticsConfiguration(analyticsConfiguration);\n\n        s3.setBucketAnalyticsConfiguration(req);\n\n    }\n}. Thanks, I have created the Issue - https://github.com/aws/aws-sdk-java/issues/1613.\n",
    "eric-swann-q2": "Thanks be to this thread!  This issue also cropped up for me when setting up bucket replication.  The lack of a prefix causes this exception as well, which is super unhelpful.  Thanks!. ",
    "pcchannz": "@eric-swann-q2 I have the same the problem for setting bucket replication too but via c# sdk. The workaround i have is to add empty filter, priority and DeleteMarkerReplication.  . ",
    "stormbeta": "@cjyclaire This is coming through a lambda, yes. The messages I was looking at before were sent via email-json subscriptions, if that makes a difference.. ",
    "kstratis": "Does #1711 offer any options for progress callbacks..?. ",
    "madkin10": "@cjyclaire Your theory was correct. However, This is still undesired behavior that you cannot install two separate version of a gem without them conflicting. Why are they trying to write an executable to the same location?. ",
    "autarchprinceps": "This cannot possibly be the desired behaviour. Not only do certain rubygems depend on e.g. 'aws-sdk ~> 2.3' which will keep installing aws-sdk v2, while others may have a >= which would install v3, conflict and crash, it isn't even possible to do a gem update.\nI don't know why aws-sdk v2 and v3 cannot coexist in your mind, but in rubygems in general you should and can install many different versions of the same gem. This is very much a desired behaviour. You cannot possibly expect all ruby gems to update in the exact same moment as you release a major update, not that even that would work, since as I mentioned it requires a clean install, a update is broken as well.. ",
    "alvises": "I think that's could be related to this. I've started to have chef scripts failing. This scripts are used in aws opsworks.\nExpected process to exit with [0], but received '1'\n---- Begin output of /opt/chef/embedded/bin/gem install aws-sdk -q --no-rdoc --no-ri -v \"~> 2.2\" ----\nSTDOUT: \nSTDERR: ERROR:  Error installing aws-sdk:\n\"aws.rb\" from aws-sdk-core conflicts with installed executable from aws-sdk-resources\n---- End output of /opt/chef/embedded/bin/gem install aws-sdk -q --no-rdoc --no-ri -v \"~> 2.2\" ----\nRan /opt/chef/embedded/bin/gem install aws-sdk -q --no-rdoc --no-ri -v \"~> 2.2\" returned 1. @awood45 they are all ec2 machines with ubuntu 14.04 under aws opsworks. Using aws-sdk-ruby library with Chef 12.13.37 . @awood45 thanks! . ",
    "jamtur01": "No release tag either.. ",
    "greysteil": "Thanks for the update. Would be awesome if you could add the above details to the changelog, or a GitHub release - at the moment anyone doing a bundle update without a pinned version will be getting V3 but won't be able to find details of what's changed / currently happening easily.\n(My particular interest is that I run Dependabot which automatically creates PRs to bump dependencies - we've had a few people ask if there's been an error on our side because they haven't been able to find any details of the new version in this repo!). Interesting. Release notes aren't going to work for you with the above setup, but you should be able to get Rubygems to point to the right changelogs, which will help a lot - I'll create a PR for that now.. Got a solution for you on that one: https://github.com/aws/aws-sdk-ruby/pull/1585. Nice, thanks!. @awood45 - sensible compromise?. @awood45 - wasn't sure how to run your code generation tasks, so I'll leave that bit to you. More details on the direction of travel for gemspecs is here and here, and the current state is here. Basically, you can add metadata (like I do here) to tell Rubygems where changelogs live, which is perfect for this setup.. \ud83d\udc4c . ",
    "maschwenk": "Never mind, I see it is under core. ",
    "Resisty": "Thanks for the info on that! For downstream projects (like the definitely currently maintained fluentd plugin) that end-users have no control over, the upgrading guide doesn't seem like an obvious document to read. Could we get that CHANGELOG visible, maybe?. ",
    "JaganNarayanan": "Sorry, newbie here. I get this error 'uninitialized constant Aws::VERSION' when loading my page after using the gem 'aws-sdk'. How do I remove the version file? Will it fix this? Thank you in advance!. Gemfile \ngem 'aws-sdk'\nconfig/environments/development.rb \nconfig.paperclip_defaults = {\n      storage: :s3,\n      s3_region: ENV[\"aws_bucket_region\"],\n      s3_credentials: {\n        # s3_host_name: ENV[\"aws_bucket_name\"],\n        bucket: ENV[\"aws_bucket_name\"],\n        access_key_id: ENV[\"amazon_key\"],\n        secret_access_key: ENV[\"amazon_secret_access_key\"]\n        }\n      }\nconfig/environment/production.rb\nconfig.paperclip_defaults = {\n  :storage => :s3,\n  :preserve_files => true,\n  :bucket => ENV['aws_bucket_name']\n}\nThat was my code. I also tried using older versions of the gem, 1 & 2 but I get the same version error. Any help would be much appreciated, thanks! . ",
    "easybills-admin": "Possible side effect: https://github.com/thoughtbot/paperclip/issues/2484. Possible side effect: https://github.com/thoughtbot/paperclip/issues/2484. ",
    "krystofspl": "Unfortunately v3 causes other problems for us and we cannot debug it at the moment. We'll try to look into the v3 compatibility in the future.\nIt seems to happen every time with mentioned JRuby and multi-threaded use.. I'm very sorry about the delay, couldn't find the time to get into it.\nV3 hangs indefinitely when connecting, but again that might be something with our app, it's quite complex so it's time consuming to debug. Upgrade to v3 is planned in the future for sure, but we need to make it work with v2 for the time being.\nBasically we are just calling object.upload_file(...) with :server_side_encryption => 'AES256' with peach (jruby concurrency gem) and >1 threads... Not helpful a lot :-(. I wasn't able to reproduce the issue since. You can probably close this for now. If something happens, I will report :-) Thanks!. ",
    "ashu106": "I am facing the same issue with aws-sdk-3. It happens occasionally. So, having a really hard time debugging what exactly is the issue. ",
    "fledman": "@cjyclaire \nThe signing api operates on a seconds until expiration basis, as opposed to an explicit expiration timestamp. In most cases, emulating the later by subtracting the current timestamp is good enough.\nHowever, signed urls produced in this way are not stable; the signature for a  pair depends on the time of signing.\nUnfortunately, my use case requires stable signatures (and I cannot use some sort of caching scheme). Fortunately, this can be achieved by explicitly setting the signing timestamp.\nAs an example, say I want to produce stable urls that are valid for a particular day. I can set the signing timestamp to midnight, expires_in to 86400, and produce the same signed url for a given input deterministically.\n. additionally:\nper #1167 the original purpose of the method patch was to fix:\n1. streaming idempotent retry bug\n2. yielding idempotent retry bug\n3. original 100-continue bug\nhowever, this line was later added:\nhttps://github.com/aws/aws-sdk-ruby/blob/e706839d56503436822246b804f5ec49715d9458/gems/aws-sdk-core/lib/seahorse/client/net_http/patches.rb#L15\nclearing the list of idempotent methods directly fixes 1 & 2 ; the method patch isn't doing anything because the rewind code is never reached.\nfurthermore, the fix for 3 was merged into core ruby >= 2.3: https://github.com/ruby/ruby/commit/f0002bd5a21bf091dfd0482fb7b0147b7d3703fb\nso 2.5 > rubies >= 2.3 just need to clear IDEMPOTENT_METHODS_, they don't need the method patch. ```diff\ndiff --git a/lib/seahorse/client/net_http/patches.rb b/lib/seahorse/client/net_http/patches.rb\nindex ca68e55..eccfcba 100644\n--- a/lib/seahorse/client/net_http/patches.rb\n+++ b/lib/seahorse/client/net_http/patches.rb\n@@ -9,13 +9,20 @@ module Seahorse\n       module Patches\n     def self.apply!\n\n\nreturn unless RUBY_VERSION < '2.5'\nif RUBY_VERSION >= '2.0'\nNet::HTTP.send(:include, Ruby_2)\nif RUBY_VERSION >= '2.5'\n\ndo nothing\n\nelsif RUBY_VERSION >= '2.3'\n             Net::HTTP::IDEMPOTENT_METHODS_.clear\nelsif RUBY_VERSION >= '2.0'\nNet::HTTP::IDEMPOTENT_METHODS_.clear\npatch_transport_request!(Ruby_2)\n           elsif RUBY_VERSION >= '1.9.3'\nNet::HTTP.send(:include, Ruby_1_9_3)\npatch_transport_request!(Ruby_1_9_3)\n           end\nend\n+\ndef self.patch_transport_request!(patch)\nNet::HTTP.send(:include, patch)\n           Net::HTTP.send(:alias_method, :old_transport_request, :transport_request)\n           Net::HTTP.send(:alias_method, :transport_request, :new_transport_request)\n         end\n@@ -30,7 +37,7 @@ module Seahorse\n                 begin\n                   res = Net::HTTPResponse.read_new(@socket)\n                   res.decode_content = req.decode_content\nend while res.kind_of?(Net::HTTPContinue)\nend while res.kind_of?(Net::HTTPInformation)         res.uri = req.uri\n\n\n\n```. @awood45 \nThere are two separate points:\n1) Ruby 2.3 & 2.4 do not need the patch, but they DO need the IDEMPOTENT_METHODS_.clear line\n2) the patch (when applied) introduces back a bug that was fixed in mainland ruby >= 2.2. great!. ",
    "PetrKaleta": "Yes, works like a charm now! Thank you!. This code was working without any issues until V3 was introduced\n``` ruby\n      def structure_at(bucket:, path: nil)\n        bucket.client.list_objects_v2 prefix: path,\n                                      bucket: bucket.name,\n                                      delimiter: '/'\n      end\n  def files_at(bucket:, path: nil)\n    structure = structure_at bucket: bucket, path: path\n\n    structure\n      .contents\n      .map do |object|\n        next if object.key.end_with? '/'\n\n        Aws::S3::ObjectSummary.new bucket_name: bucket.name,\n                                   key: object.key,\n                                   data: object.to_h\n      end\n      .compact\n  end\n\n```\nSo it looks like the weak point is data attribute in ObjectSummary constructor... . @cjyclaire Hmmmmm, I think I found it directly in the source code..... ",
    "LeKristapino": "I am having a similar issue without JRuby\n! Unable to load application: Bundler::GemRequireError: There was an error while trying to load the gem 'aws-sdk-sqs'.\nGem Load Error is: undefined method `stop' for nil:NilClass\nBacktrace for gem load error is:\n/bundle/ruby/2.4.0/gems/puma-3.12.0/lib/puma/single.rb:37:in `stop_blocked'\n/bundle/ruby/2.4.0/gems/puma-3.12.0/lib/puma/launcher.rb:292:in `graceful_stop'\n/bundle/ruby/2.4.0/gems/puma-3.12.0/lib/puma/launcher.rb:396:in `block in setup_signals'\n/bundle/ruby/2.4.0/gems/bootsnap-1.3.1/lib/bootsnap/compile_cache/iseq.rb:12:in `compile_file'\n/bundle/ruby/2.4.0/gems/bootsnap-1.3.1/lib/bootsnap/compile_cache/iseq.rb:12:in `input_to_storage'\n/bundle/ruby/2.4.0/gems/bootsnap-1.3.1/lib/bootsnap/compile_cache/iseq.rb:37:in `fetch'\n/bundle/ruby/2.4.0/gems/bootsnap-1.3.1/lib/bootsnap/compile_cache/iseq.rb:37:in `load_iseq'\n/bundle/ruby/2.4.0/gems/bootsnap-1.3.1/lib/bootsnap/load_path_cache/core_ext/kernel_require.rb:21:in `require'\n/bundle/ruby/2.4.0/gems/bootsnap-1.3.1/lib/bootsnap/load_path_cache/core_ext/kernel_require.rb:21:in `block in require_with_bootsnap_lfi'\n/bundle/ruby/2.4.0/gems/bootsnap-1.3.1/lib/bootsnap/load_path_cache/loaded_features_index.rb:65:in `register'\n/bundle/ruby/2.4.0/gems/bootsnap-1.3.1/lib/bootsnap/load_path_cache/core_ext/kernel_require.rb:20:in `require_with_bootsnap_lfi'\n/bundle/ruby/2.4.0/gems/bootsnap-1.3.1/lib/bootsnap/load_path_cache/core_ext/kernel_require.rb:29:in `require'\n/bundle/ruby/2.4.0/gems/activesupport-5.2.0/lib/active_support/dependencies.rb:283:in `block in require'\n/bundle/ruby/2.4.0/gems/activesupport-5.2.0/lib/active_support/dependencies.rb:249:in `load_dependency'\n/bundle/ruby/2.4.0/gems/activesupport-5.2.0/lib/active_support/dependencies.rb:283:in `require'\n/bundle/ruby/2.4.0/gems/bootsnap-1.3.1/lib/bootsnap/load_path_cache/core_ext/kernel_require.rb:44:in `require_relative'\n/bundle/ruby/2.4.0/gems/aws-sdk-core-3.22.1/lib/aws-sdk-core/binary.rb:1:in `<main>'\n/bundle/ruby/2.4.0/gems/aws-sdk-core-3.22.1/lib/aws-sdk-core/binary.rb:1:in `<main>'\n/bundle/ruby/2.4.0/gems/bootsnap-1.3.1/lib/bootsnap/load_path_cache/core_ext/kernel_require.rb:21:in `require'\n/bundle/ruby/2.4.0/gems/bootsnap-1.3.1/lib/bootsnap/load_path_cache/core_ext/kernel_require.rb:21:in `block in require_with_bootsnap_lfi'\n/bundle/ruby/2.4.0/gems/bootsnap-1.3.1/lib/bootsnap/load_path_cache/loaded_features_index.rb:65:in `register'\n/bundle/ruby/2.4.0/gems/bootsnap-1.3.1/lib/bootsnap/load_path_cache/core_ext/kernel_require.rb:20:in `require_with_bootsnap_lfi'\n/bundle/ruby/2.4.0/gems/bootsnap-1.3.1/lib/bootsnap/load_path_cache/core_ext/kernel_require.rb:29:in `require'\n/bundle/ruby/2.4.0/gems/activesupport-5.2.0/lib/active_support/dependencies.rb:283:in `block in require'\n/bundle/ruby/2.4.0/gems/activesupport-5.2.0/lib/active_support/dependencies.rb:249:in `load_dependency'\n/bundle/ruby/2.4.0/gems/activesupport-5.2.0/lib/active_support/dependencies.rb:283:in `require'\n/bundle/ruby/2.4.0/gems/bootsnap-1.3.1/lib/bootsnap/load_path_cache/core_ext/kernel_require.rb:44:in `require_relative'\n/bundle/ruby/2.4.0/gems/aws-sdk-core-3.22.1/lib/aws-sdk-core.rb:66:in `<main>'\n/bundle/ruby/2.4.0/gems/bootsnap-1.3.1/lib/bootsnap/load_path_cache/core_ext/kernel_require.rb:21:in `require'\n/bundle/ruby/2.4.0/gems/bootsnap-1.3.1/lib/bootsnap/load_path_cache/core_ext/kernel_require.rb:21:in `block in require_with_bootsnap_lfi'\n/bundle/ruby/2.4.0/gems/bootsnap-1.3.1/lib/bootsnap/load_path_cache/loaded_features_index.rb:65:in `register'\n/bundle/ruby/2.4.0/gems/bootsnap-1.3.1/lib/bootsnap/load_path_cache/core_ext/kernel_require.rb:20:in `require_with_bootsnap_lfi'\n/bundle/ruby/2.4.0/gems/bootsnap-1.3.1/lib/bootsnap/load_path_cache/core_ext/kernel_require.rb:29:in `require'\n/bundle/ruby/2.4.0/gems/activesupport-5.2.0/lib/active_support/dependencies.rb:283:in `block in require'\n/bundle/ruby/2.4.0/gems/activesupport-5.2.0/lib/active_support/dependencies.rb:249:in `load_dependency'\n/bundle/ruby/2.4.0/gems/activesupport-5.2.0/lib/active_support/dependencies.rb:283:in `require'\n/bundle/ruby/2.4.0/gems/aws-sdk-sqs-1.4.0/lib/aws-sdk-sqs.rb:8:in `<main>'\n/bundle/ruby/2.4.0/gems/bootsnap-1.3.1/lib/bootsnap/load_path_cache/core_ext/kernel_require.rb:21:in `require'\n/bundle/ruby/2.4.0/gems/bootsnap-1.3.1/lib/bootsnap/load_path_cache/core_ext/kernel_require.rb:21:in `block in require_with_bootsnap_lfi'\n/bundle/ruby/2.4.0/gems/bootsnap-1.3.1/lib/bootsnap/load_path_cache/loaded_features_index.rb:65:in `register'\n/bundle/ruby/2.4.0/gems/bootsnap-1.3.1/lib/bootsnap/load_path_cache/core_ext/kernel_require.rb:20:in `require_with_bootsnap_lfi'\n/bundle/ruby/2.4.0/gems/bootsnap-1.3.1/lib/bootsnap/load_path_cache/core_ext/kernel_require.rb:29:in `require'\n/usr/local/lib/ruby/gems/2.4.0/gems/bundler-1.16.3/lib/bundler/runtime.rb:81:in `block (2 levels) in require'\n/usr/local/lib/ruby/gems/2.4.0/gems/bundler-1.16.3/lib/bundler/runtime.rb:76:in `each'\n/usr/local/lib/ruby/gems/2.4.0/gems/bundler-1.16.3/lib/bundler/runtime.rb:76:in `block in require'\n/usr/local/lib/ruby/gems/2.4.0/gems/bundler-1.16.3/lib/bundler/runtime.rb:65:in `each'\n/usr/local/lib/ruby/gems/2.4.0/gems/bundler-1.16.3/lib/bundler/runtime.rb:65:in `require'\n/usr/local/lib/ruby/gems/2.4.0/gems/bundler-1.16.3/lib/bundler.rb:114:in `require'\n/app/config/application.rb:18:in `<top (required)>'\n/app/config/environment.rb:2:in `require_relative'\n/app/config/environment.rb:2:in `<top (required)>'\nconfig.ru:3:in `require_relative'\nconfig.ru:3:in `block in <main>'\n/bundle/ruby/2.4.0/gems/rack-2.0.5/lib/rack/builder.rb:55:in `instance_eval'\n/bundle/ruby/2.4.0/gems/rack-2.0.5/lib/rack/builder.rb:55:in `initialize'\nconfig.ru:in `new'\nconfig.ru:in `<main>'\n/bundle/ruby/2.4.0/gems/rack-2.0.5/lib/rack/builder.rb:49:in `eval'\n/bundle/ruby/2.4.0/gems/rack-2.0.5/lib/rack/builder.rb:49:in `new_from_string'\n/bundle/ruby/2.4.0/gems/rack-2.0.5/lib/rack/builder.rb:40:in `parse_file'\n/bundle/ruby/2.4.0/gems/puma-3.12.0/lib/puma/configuration.rb:318:in `load_rackup'\n/bundle/ruby/2.4.0/gems/puma-3.12.0/lib/puma/configuration.rb:243:in `app'\n/bundle/ruby/2.4.0/gems/puma-3.12.0/lib/puma/runner.rb:145:in `load_and_bind'\n/bundle/ruby/2.4.0/gems/puma-3.12.0/lib/puma/single.rb:96:in `run'\n/bundle/ruby/2.4.0/gems/puma-3.12.0/lib/puma/launcher.rb:184:in `run'\n/bundle/ruby/2.4.0/gems/puma-3.12.0/lib/puma/cli.rb:78:in `run'\n/bundle/ruby/2.4.0/gems/puma-3.12.0/bin/puma:10:in `<top (required)>'\n/bundle/ruby/2.4.0/bin/puma:23:in `load'\n/bundle/ruby/2.4.0/bin/puma:23:in `<top (required)>'\n/usr/local/lib/ruby/gems/2.4.0/gems/bundler-1.16.3/lib/bundler/cli/exec.rb:74:in `load'\n/usr/local/lib/ruby/gems/2.4.0/gems/bundler-1.16.3/lib/bundler/cli/exec.rb:74:in `kernel_load'\n/usr/local/lib/ruby/gems/2.4.0/gems/bundler-1.16.3/lib/bundler/cli/exec.rb:28:in `run'\n/usr/local/lib/ruby/gems/2.4.0/gems/bundler-1.16.3/lib/bundler/cli.rb:424:in `exec'\n/usr/local/lib/ruby/gems/2.4.0/gems/bundler-1.16.3/lib/bundler/vendor/thor/lib/thor/command.rb:27:in `run'\n/usr/local/lib/ruby/gems/2.4.0/gems/bundler-1.16.3/lib/bundler/vendor/thor/lib/thor/invocation.rb:126:in `invoke_command'\n/usr/local/lib/ruby/gems/2.4.0/gems/bundler-1.16.3/lib/bundler/vendor/thor/lib/thor.rb:387:in `dispatch'\n/usr/local/lib/ruby/gems/2.4.0/gems/bundler-1.16.3/lib/bundler/cli.rb:27:in `dispatch'\n/usr/local/lib/ruby/gems/2.4.0/gems/bundler-1.16.3/lib/bundler/vendor/thor/lib/thor/base.rb:466:in `start'\n/usr/local/lib/ruby/gems/2.4.0/gems/bundler-1.16.3/lib/bundler/cli.rb:18:in `start'\n/usr/local/lib/ruby/gems/2.4.0/gems/bundler-1.16.3/exe/bundle:30:in `block in <top (required)>'\n/usr/local/lib/ruby/gems/2.4.0/gems/bundler-1.16.3/lib/bundler/friendly_errors.rb:124:in `with_friendly_errors'\n/usr/local/lib/ruby/gems/2.4.0/gems/bundler-1.16.3/exe/bundle:22:in `<top (required)>'\n/usr/local/bin/bundle:23:in `load'\n/usr/local/bin/bundle:23:in `<main>'\nBundler Error Backtrace:\nbundler: failed to load command: puma (/bundle/ruby/2.4.0/bin/puma)\nBundler::GemRequireError: There was an error while trying to load the gem 'aws-sdk-sqs'.\nGem Load Error is: undefined method `stop' for nil:NilClass\nBacktrace for gem load error is:\n/bundle/ruby/2.4.0/gems/puma-3.12.0/lib/puma/single.rb:37:in `stop_blocked'\n/bundle/ruby/2.4.0/gems/puma-3.12.0/lib/puma/launcher.rb:292:in `graceful_stop'\n/bundle/ruby/2.4.0/gems/puma-3.12.0/lib/puma/launcher.rb:396:in `block in setup_signals'\n/bundle/ruby/2.4.0/gems/bootsnap-1.3.1/lib/bootsnap/compile_cache/iseq.rb:12:in `compile_file'\n- Gracefully stopping, waiting for requests to finish\nPuma starting in single mode...\n* Version 3.12.0 (ruby 2.4.4-p296), codename: Llamas in Pajamas\n* Min threads: 5, max threads: 5\n* Environment: demo\n```\nruby '2.4.4'\ngem 'rails', '~> 5.2.0'\n...\ngem 'shoryuken'\ngem 'aws-sdk-sqs'\ngem 'aws-sdk', '~> 3'\n```. ",
    "pethron": "Yes i'm using the API calls in Rails. I'll leave an example as soon as possible.. ",
    "tarcieri": "If I were to do a PR, I guess my first question would be what is the correct behavior here re: dup/clone?. I encountered this error attempting to invoke batch_create_tags on an Aws::EC2::Instance:\nNameError: undefined local variable or method `v' for Aws::Util:Module\n  /home/tony/.gem/ruby/2.4.0/gems/aws-sdk-core-3.0.0/lib/aws-sdk-core/util.rb:34:in `deep_copy'\n  /home/tony/.gem/ruby/2.4.0/gems/aws-sdk-core-3.0.0/lib/aws-sdk-core/util.rb:28:in `block in deep_copy'\n  /home/tony/.gem/ruby/2.4.0/gems/aws-sdk-core-3.0.0/lib/aws-sdk-core/util.rb:27:in `each'\n  /home/tony/.gem/ruby/2.4.0/gems/aws-sdk-core-3.0.0/lib/aws-sdk-core/util.rb:27:in `inject'\n  /home/tony/.gem/ruby/2.4.0/gems/aws-sdk-core-3.0.0/lib/aws-sdk-core/util.rb:27:in `deep_copy'\n  /home/tony/.gem/ruby/2.4.0/gems/aws-sdk-core-3.0.0/lib/aws-sdk-core/util.rb:32:in `block in deep_copy'\n  /home/tony/.gem/ruby/2.4.0/gems/aws-sdk-core-3.0.0/lib/aws-sdk-core/util.rb:32:in `map'\n  /home/tony/.gem/ruby/2.4.0/gems/aws-sdk-core-3.0.0/lib/aws-sdk-core/util.rb:32:in `deep_copy'\n  /home/tony/.gem/ruby/2.4.0/gems/aws-sdk-core-3.0.0/lib/aws-sdk-core/util.rb:28:in `block in deep_copy'\n  /home/tony/.gem/ruby/2.4.0/gems/aws-sdk-core-3.0.0/lib/aws-sdk-core/util.rb:27:in `each'\n  /home/tony/.gem/ruby/2.4.0/gems/aws-sdk-core-3.0.0/lib/aws-sdk-core/util.rb:27:in `inject'\n  /home/tony/.gem/ruby/2.4.0/gems/aws-sdk-core-3.0.0/lib/aws-sdk-core/util.rb:27:in `deep_copy'\n  /home/tony/.gem/ruby/2.4.0/gems/aws-sdk-core-3.0.0/lib/aws-sdk-core/util.rb:18:in `copy_hash'\n  /home/tony/.gem/ruby/2.4.0/gems/aws-sdk-ec2-1.0.0/lib/aws-sdk-ec2/instance.rb:1399:in `block in batch_create_tags'\n  /home/tony/.gem/ruby/2.4.0/gems/aws-sdk-core-3.0.0/lib/aws-sdk-core/resources/collection.rb:100:in `yield'\n  /home/tony/.gem/ruby/2.4.0/gems/aws-sdk-core-3.0.0/lib/aws-sdk-core/resources/collection.rb:100:in `block (2 levels) in non_empty_batches'\n  /home/tony/.gem/ruby/2.4.0/gems/aws-sdk-core-3.0.0/lib/aws-sdk-core/resources/collection.rb:99:in `each'\n  /home/tony/.gem/ruby/2.4.0/gems/aws-sdk-core-3.0.0/lib/aws-sdk-core/resources/collection.rb:99:in `block in non_empty_batches'\n  /home/tony/.gem/ruby/2.4.0/gems/aws-sdk-ec2-1.0.0/lib/aws-sdk-ec2/instance.rb:1398:in `each'\n  /home/tony/.gem/ruby/2.4.0/gems/aws-sdk-ec2-1.0.0/lib/aws-sdk-ec2/instance.rb:1398:in `each'\n  /home/tony/.gem/ruby/2.4.0/gems/aws-sdk-ec2-1.0.0/lib/aws-sdk-ec2/instance.rb:1398:in `batch_create_tags'. ",
    "jeffpereira": "thanks for the super fast fix and update.  . ",
    "codershunshun": "@cjyclaire the description for wait_until_exists method is not very clear. Does it mean it will wait until the stack CREATE_COMPLETE?. ",
    "sam0x17": "thanks -- this helps!. ",
    "eropple": "I understand that it's generated. The generator for Ruby could/probably-should generate aliases to idiomatically present Ruby-conjugated predicate names, is what I'm saying.. ",
    "Nomane": "Issue is located in SQS endpoint returned by AWS API.\nIn my case I use aws cli by this way\n\naws sqs get-queue-url --queue-name ${context.queueName}\n\nwhich return me something like this:\n\nhttps://eu-central-1.queue.amazonaws.com/XXXXXXXX/myqueuename\n\nIt's not a valid queue URL, you have to use something like this:\n\nhttps://sqs.eu-central-1.amazonaws.com/XXXXXXXX/myqueuename\n\nTo fix this I force the endpoint like that:\n\naws sqs get-queue-url --queue-name ${context.queueName} --endpoint-url https://sqs.eu-central-1.amazonaws.com/**\n\nHope that could be useful\n. ",
    "immerrr": "When you're saying \"not a valid queue URL\", you must have some grounds to believe that. I have no problem accessing queues with https://eu-west-1.queue.amazonaws.com endpoint via boto3 library, hence I believe the problem is in ruby sdk.\nThe error message says that \"queue\" is not a valid region for url that looks like\nhttps://eu-west-1.queue.amazonaws.com\nbut works fine for URLs that look like\nhttps://sqs.eu-west-1.amazonaws.com\nI'd suggest that queue region is extracted from the second dot-separated component of endpoint netloc, which is an incorrect assumption in case of {REGION}.queue.* URLs. And it took me 3mins from that point to finding a place that looks suspiciously close to what I'm describing (disclaimer: i have no experience with ruby, so I might be wrong here).\nPinging @cjyclaire.. Ok, according to https://docs.aws.amazon.com/general/latest/gr/rande.html#sqs_region, sqs.{REGION} URLs are the new and correct ones, and {REGION}.queue ones are legacy that are somehow permitted \"If you use the AWS CLI or SDK for Python\". Still, we've faced this problem today connecting Ruby & Python code, so I strongly believe this is worth fixing.. @cjyclaire API returns you a queue URL based on the endpoint you use:\n```\nIn [10]:  boto3.client('sqs', endpoint_url='https://eu-west-1.queue.amazonaws.com').get_queue_url(QueueName='hello')['QueueUrl']\nOut[10]: 'https://eu-west-1.queue.amazonaws.com/{ACCOUNT_ID}/hello'\nIn [11]:  boto3.client('sqs', endpoint_url='https://sqs.eu-west-1.amazonaws.com').get_queue_url(QueueName='hello')['QueueUrl']\nOut[11]: 'https://sqs.eu-west-1.amazonaws.com/{ACCOUNT_ID}/hello'\n```. > For the code link you mentioned, it's helping for signning regions, doesn't change url provided :)\nI'm not saying it changes the URL, I'm saying it takes 2nd dot-separated netloc component as region name. 2nd dot-separated netloc component is indeed a region name for new patterns, but for old patterns region names go first, and 2nd component is a word queue, which is not a valid region name, and apparently which is what the error message tries to tell.. > unless copying the queue URL from the Python/CLI\nThis is exactly the way I've encountered the problem: when copying a queue URL from Python part of the infra to Ruby. Also, FTR, it was not a choice to use the legacy endpoint, it is the default at least as of versions\n```\nIn [19]: boto3.version\nOut[19]: '1.4.4'\nIn [21]: botocore.version\nOut[21]: '1.5.95'\n```\nThe former was released in Jan 2017, the latter -- in Aug 2017, so the libraries themselves are not exactly legacy.. Apparently, that is still the default endpoint: https://github.com/boto/botocore/blob/develop/botocore/data/endpoints.json#L2232. As per the developers, the old endpoints are used because Py2.6 is unable to work with \"subjectAltName\" extension which is used to add \"new\" URL schemes to certificates, so it will not be fixed until 2.6 is dropped, which should be soon, but unclear yet.. ",
    "rhyas": "I looked at the code for this, and it seems as if the documentation is just flat wrong. It implies it supports the config options with role_arn and source_profile, but the actual code for shared_credentials does not actually do anything related to that process. It defaults to only using the credentials file and keys therein. Seems like a pretty major issue if the documentation says it should work, but there's no code/functionality to support it. Also, this issue applies to both V2 and V3, they have essentially the same module code.. So this is basically using the Assume Role functionality in the SDK via configs. http://docs.aws.amazon.com/cli/latest/userguide/cli-roles.html. The SDK supports this, but what I have come to understand, and this is where the documentation is wrong, it only works if you don't feed include a Credentials object in the config, rather only feed it profile arg, and let the client object discover credentials. None of the Credential objects correctly does the Assume Role from configs. The documentation actually implies that you must feed a credentials object, but it's wrong. You exclude credentials, and it will do the searching.. I discovered this trying to use the kitchen-ec2 package with assumed roles. The code in question is here: https://github.com/test-kitchen/kitchen-ec2/blob/master/lib/kitchen/driver/aws/client.rb and to make things work, I replaced line 50 to be :profile = profile_name. You can say it's \"hand rolled\", but it's following the documentation, or attempting to. The documentation specifically states that you have to provide credentials, and there are only 4 classes, none of which allow you to use the config option of assumed role. I suspect that this ticket, even if it's not using kitchen-ec2, was trying to follow the documentation and running into the same problem. So the issue is not specifically a bug in the SDK, it's a bug in the documentation. (:. ",
    "matt9949": "Hey guys, thanks for looking at this. @awood45 i'm not using kitchen-ec2, i'm using a library called ami_spec to spin up a temporary EC2 instance with an AMI I have built/made changes to for acceptance testing. I believe the library uses the ruby aws-sdk for spinning up EC2 instances.. ",
    "cotsog": "@cjyclaire: Thanks so much for handling this quickly! \ud83d\udc4d . ",
    "SamSaffron": "Thanks @cjyclaire ! . Hi @cjyclaire we started using the new APIs and are getting: \n\"Filter element can only be used in Lifecycle V2.\"\nIs there some documentation anywhere about how we should go about updating various s3 buckets out there? . I see, well we do have an actual bug here I think in the aws gem: \nget_bucket_lifecycle_configuration is returning\n[#<struct Aws::S3::Types::LifecycleRule expiration=#<struct Aws::S3::Types::LifecycleExpiration date=nil, days=30, expired_object_delete_marker=nil>, id=\"purge-tombstone\", prefix=\"tombstone/\", filter=nil, status=\"Enabled\", transitions=[], noncurrent_version_transitions=[], noncurrent_version_expiration=nil, abort_incomplete_multipart_upload=nil>]\nDoing a to_h on this thing returns:\n{:expiration=>{:days=>30}, :id=>\"purge-tombstone\", :prefix=>\"tombstone/\", :status=>\"Enabled\"}\nWhen it should really be: \n{:expiration=>{:days=>30}, :id=>\"purge-tombstone\", :filter => {:prefix=>\"tombstone/\" }, :status=>\"Enabled\"}\nSo this ends up triggering the v2 error. . https://github.com/discourse/discourse/commit/4f28c71b5082d8194129f10084738775b36b8ed3\nhope this helps narrow down the issue. . > Curious and guessing, are you using #put_bucket_lifecycle(the deprecated one) to put configuration originally? \nYes, absolutely, the issue only existed on legacy systems that were previously configured with put_bucket_lifecycle. \nThe repro, in theory, should be to put_bucket_lifecycle then get_bucket_lifecycle_configuration and put_bucket_lifecycle_configuration. . thanks heaps @cjyclaire, nothing urgent on our side cause we have a\nworkaround in place already.\nOn Tue, Nov 14, 2017 at 8:09 AM, cjyclaire notifications@github.com wrote:\n\nI believe that might be the issue from those API coupling, I'll talk to\nservice team for confirmation and best practice for this.\nFrom SDK side, we just parse the response we received from server side for\nthe API you call and we have no ability to change the logic at server side.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/aws/aws-sdk-ruby/issues/1653#issuecomment-344058790,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAAUXQDvTFd84V3v4E8s34lb31c1VLufks5s2LAigaJpZM4QL2Uw\n.\n. \n",
    "amalagaura": "I see there is a spec which expects the tempfile to not be deleted in order to enable caching of retries. \nIs the spec correct? I see a close operation here:\nhttps://github.com/aws/aws-sdk-ruby/blob/9ea447b7e9e57e27bb7924aca40a5bf0c3265a94/gems/aws-sdk-s3/lib/aws-sdk-s3/encryption/encrypt_handler.rb#L42-L44\nDoesn't the close happen on a succesful response? Is the IOEncryption handler instance really closed and then reused or is there a new IOEncryption instance on a retry? . @cjyclaire I removed my PR. Until this is done I am using the following code after an upload:\nresponse = s3_client.put_object bucket: bucket, key: key, body: File.open(file_path)\nencrypted = response.context.http_request.body.instance_variable_get(:@encrypted)\nencrypted.close! if encrypted.respond_to?(:close!). I see there are specs which want the tempfile to not be deleted for caching purposes.. ",
    "jubrad": "It would appear, and quite reasonably so, that there is no destination cidr block when the gateway is a service endpoint. Please re-open with this new info. It seems like there's no requirement for there to be null check for the destination_cidr_block.\nhttps://github.com/aws/aws-sdk-ruby/blob/master/gems/aws-sdk-ec2/lib/aws-sdk-ec2/route.rb#L360\nIf this is true I can make the fix or you can make the fix, but do not just kick this off to some untracked readme and close without investigating. \n```ruby\n[89] pry(main)> awsc.describe_route_tables.route_tables\n                             .map(&:routes).flatten\n                             .select{|r| r.gateway_id =~ /vpce/}.map(&:destination_cidr_block)\n=> [nil,\n nil,\n nil,\n nil,\n nil,\n nil,\n nil,\n nil,\n nil,\n nil,\n nil,\n nil,\n...\n[89] pry(main)> awsc.describe_route_tables.route_tables\n                             .map(&:routes).flatten\n                             .select{|r| not r.gateway_id =~ /vpce/}.map(&:destination_cidr_block)\n                             .include?(nil)\n=> false\n``\n. Slight correction: if the null check is not there replacing or deleting routes will fail if the destination_cidr_block is not set. This seems like an issue as you would be unable to delete/replace any routes for endpoints. The larger issue seems to be that routes don't have a true primary identifier. This has caused the route resource object to attempt to usedestination_cidr_blockandroute_table_id` as the identifier. These are sufficient from a networking standpoint, but fail to take into account vpc endpoints which do not have a set destination cidr. Endpoint routes will either need to be handled separately or the identifier should be adjusted to also use gateway. . ",
    "dimitrovmaksim": "@cjyclaire thanks for the reply. Rewrote the code to be 100% sure and it works now. It seems it was something In our code, that I could not locate earlier. Sorry for the inconvenience!. ",
    "Throne3d": "Thanks!. ",
    "brianknight10": "I sincerely apologize. I had a threading issue happening and that was the cause of my problem. Everything looks good!. ",
    "supersam654": "Thanks for the update. I found another 6 API calls that I think should have a parameter marked as required (just like this one). What's the best way for me to report those (separate issues, just comment here, something else)?. Thanks. I contacted Support and they're handling them.. ",
    "kgalli": "Will this be tackled anytime soon. I can probably find it in the code as already mentioned in this thread. But that destroys the whole idea of easy to read documentation .... ",
    "rboy1": "@cjyclaire what's the solution to this? Is there a way to reach Amazon support and report the issue - they don't have support for regular accounts?. ",
    "eagletmt": "Thanks!. ",
    "ook": "Sure. Let me show you a complete session:\nroot@35afc611394b:/app/user# aws --endpoint-url=http://localstack:4572 s3 mb s3://mybucket\nmake_bucket: mybucket\nroot@35afc611394b:/app/user# ping -c 2 localstack\nPING localstack (172.18.0.2): 56 data bytes\n64 bytes from 172.18.0.2: icmp_seq=0 ttl=64 time=6.260 ms\n64 bytes from 172.18.0.2: icmp_seq=1 ttl=64 time=7.359 ms\n--- localstack ping statistics ---\n2 packets transmitted, 2 packets received, 0% packet loss\nround-trip min/avg/max/stddev = 6.260/6.809/7.359/0.550 ms\nroot@35afc611394b:/app/user# irb\nirb(main):001:0> require 'aws-sdk'\n=> true\nirb(main):002:0> Aws.config.update(endpoint: \"http://localstack:4572/\", credentials: Aws::Credentials.new('credentials', 'areIgnored'))\n=> {:endpoint=>\"http://localstack:4572/\", :credentials=>#<Aws::Credentials access_key_id=\"credentials\">}\nirb(main):003:0> Aws::S3::Resource.new.bucket('mybucket').exists?\nSeahorse::Client::NetworkingError: unable to connect to `mybucket.localstack`; SocketError: getaddrinfo: Name or service not known\n    from /usr/local/lib/ruby/2.2.0/net/http.rb:879:in `initialize'\n    from /usr/local/lib/ruby/2.2.0/net/http.rb:879:in `open'\n    from /usr/local/lib/ruby/2.2.0/net/http.rb:879:in `block in connect'\n    from /usr/local/lib/ruby/2.2.0/timeout.rb:88:in `block in timeout'\n    from /usr/local/lib/ruby/2.2.0/timeout.rb:98:in `call'\n    from /usr/local/lib/ruby/2.2.0/timeout.rb:98:in `timeout'\n    from /usr/local/lib/ruby/2.2.0/net/http.rb:878:in `connect'\n    from /usr/local/lib/ruby/2.2.0/net/http.rb:863:in `do_start'\n    from /usr/local/lib/ruby/2.2.0/net/http.rb:858:in `start'\n    from /usr/local/bundle/gems/aws-sdk-core-3.7.0/lib/seahorse/client/net_http/connection_pool.rb:288:in `start_session'\n    from /usr/local/bundle/gems/aws-sdk-core-3.7.0/lib/seahorse/client/net_http/connection_pool.rb:95:in `session_for'\n    from /usr/local/bundle/gems/aws-sdk-core-3.7.0/lib/seahorse/client/net_http/handler.rb:121:in `session'\n    from /usr/local/bundle/gems/aws-sdk-core-3.7.0/lib/seahorse/client/net_http/handler.rb:73:in `transmit'\n    from /usr/local/bundle/gems/aws-sdk-core-3.7.0/lib/seahorse/client/net_http/handler.rb:47:in `call'\n    from /usr/local/bundle/gems/aws-sdk-core-3.7.0/lib/seahorse/client/plugins/content_length.rb:12:in `call'\n    from /usr/local/bundle/gems/aws-sdk-s3-1.6.0/lib/aws-sdk-s3/plugins/s3_signer.rb:109:in `call'\n... 34 levels...\n    from /usr/local/bundle/gems/aws-sdk-core-3.7.0/lib/seahorse/client/request.rb:70:in `send_request'\n    from /usr/local/bundle/gems/aws-sdk-core-3.7.0/lib/aws-sdk-core/waiters/poller.rb:63:in `send_request'\n    from /usr/local/bundle/gems/aws-sdk-core-3.7.0/lib/aws-sdk-core/waiters/poller.rb:49:in `call'\n    from /usr/local/bundle/gems/aws-sdk-core-3.7.0/lib/aws-sdk-core/waiters/waiter.rb:105:in `block in poll'\n    from /usr/local/bundle/gems/aws-sdk-core-3.7.0/lib/aws-sdk-core/waiters/waiter.rb:102:in `loop'\n    from /usr/local/bundle/gems/aws-sdk-core-3.7.0/lib/aws-sdk-core/waiters/waiter.rb:102:in `poll'\n    from /usr/local/bundle/gems/aws-sdk-core-3.7.0/lib/aws-sdk-core/waiters/waiter.rb:92:in `block (2 levels) in wait'\n    from /usr/local/bundle/gems/aws-sdk-core-3.7.0/lib/aws-sdk-core/waiters/waiter.rb:91:in `catch'\n    from /usr/local/bundle/gems/aws-sdk-core-3.7.0/lib/aws-sdk-core/waiters/waiter.rb:91:in `block in wait'\n    from /usr/local/bundle/gems/aws-sdk-core-3.7.0/lib/aws-sdk-core/waiters/waiter.rb:90:in `catch'\n    from /usr/local/bundle/gems/aws-sdk-core-3.7.0/lib/aws-sdk-core/waiters/waiter.rb:90:in `wait'\n    from /usr/local/bundle/gems/aws-sdk-s3-1.6.0/lib/aws-sdk-s3/waiters.rb:57:in `wait'\n    from /usr/local/bundle/gems/aws-sdk-s3-1.6.0/lib/aws-sdk-s3/bucket.rb:93:in `wait_until_exists'\n    from /usr/local/bundle/gems/aws-sdk-s3-1.6.0/lib/aws-sdk-s3/bucket.rb:74:in `exists?'\n    from (irb):3\n    from /usr/local/bin/irb:11:in `<main>'\nirb(main):004:0> require 'net/http'\n=> false\nirb(main):005:0> Net::HTTP.get(URI('http://localstack:4572/'))\n=> \"<ListAllMyBucketsResult xmlns=\\\"http://s3.amazonaws.com/doc/2006-03-01\\\"><Owner><ID>bcaf1ffd86f41161ca5fb16fd081034f</ID><DisplayName>webfile</DisplayName></Owner><Buckets><Bucket><Name>test1</Name><CreationDate>2006-02-03T16:45:09.000Z</CreationDate></Bucket><Bucket><Name>mybucket</Name><CreationDate>2006-02-03T16:45:09.000Z</CreationDate></Bucket></Buckets></ListAllMyBucketsResult>\"\nSo, it shows the link is well configured and reachable into the container, awscli can access it without a problem. In a ruby console, the stdlib net/http can reach the endpoint, only aws-sdk have problem with it.\nAm I misusing Aws.config.update method?. As followup, someone on SO found a workaroud:  adding force_path_style: true to Aws.config make the sdk behave as expected.\nShouldn't this option been set by default to be consistent with awscli ?. ",
    "gaurish": "By submitting this pull request, I confirm that my contribution is made under the terms of the Apache 2.0 license. ",
    "aquexata": "Hi @awood45 . Thank you for your quick response. Sorry, I haven't contacted AWS Support. If you think this is not related to the SDK I'll close this issue.\nThank you again for looking into this. . ",
    "cheptsov": "@aquexata Did you find out why it's empty when you use lambda? I have the same problem.. ",
    "chevinbrown": "@cjyclaire, Thanks for your work on this gem. I'm in agreement with @oyeanuj on this one. Maintaining OSS is difficult, but updating dependencies with vague descriptions is too. \ud83d\ude38 \nHaving a simple explanation would save everyone that uses this gem some time and make the upgrade process smoother. \n\u2764\ufe0f . ",
    "alexbecker": "It's been published, and deploys on Travis are working again. Thanks.. ",
    "andreydeineko": "I should have just updated the gem. shame. ",
    "richardkmichael": "@cjyclaire Great, I'm using it now.  Thank you!. ",
    "LaithAzer": "That resolved it, thank you!. ",
    "comjf": "@awood45 Thanks for your response, but I'm a little confused... Shouldn't you be able to convert between the --cli-input-json referenced all over the place in AWS documentation and \nthe gem input?\nTo be clear, all I'm doing to reproduce is:\n- Taking a valid configuration in json format (I know it's valid as I can upload via: aws ecs register-task-definition --cli-input-json)\n- My transformation is simply changing the hash keys to be the correct format and ignoring the \"options\" key (as those contain dashes)\n- I expect to be able to upload this via the aws-sdk gem, but the gem validation gives me an error for just the log_driver field. \nI know it's a problem with the gem and not AWS since:\n1. The CLI accepts my input\n2. The gem accepts my input as long as I initialize the client with validate_params: false\n. @awood45 Thanks mate! When you have a test candidate please let me know (even if it requires me building from source).\nI did get around my issue with validate_params: false but I'd love to turn that on the moment this bug is fixed and have a backlog issue tracking it. . ",
    "pizzaops": "I'm also experiencing this bug (exposed via the puppetlabs-aws puppet module).. Also not much help to folks like me who are using a piece of software that depends on the gem, not working with it directly. I guess we could carry a custom patch to that software since it's Ruby, but yeah.\nWe have the power to upgrade the gem and test, but not the power to easily change how it's being used.. Do subkeys need to be symbolized? \nFor example:\n{\n    'log_driver' => 'syslog',\n    'options' => {\n        'tag' =>  '{{.ImageName}}\\{{.Name}}\\{{.ID}}'\n    }\n  }\nDoes tag need to be symbolized here? Or just options and log_driver?. I'm leaving validate_params on, and carrying a patch to puppetlabs-aws that symbolizes the keys for logging configuration (the rest seems to work so I'm leaving it alone).\nTo answer your earlier question about what kind of patch I was thinking about: https://github.com/OtoAnalytics/puppetlabs-aws/commit/2f948e83d8e812d5fc958630fa24d5724e64f7d5\n(This isn't confirmed working yet, just trying things). ",
    "kjenney": "My workaround now is to use the AWS CLI, dump the output of ALL objects into a file and then run Regex matches on the lines of the file:\nbasefolder = File.basename folder\n    system(\"aws s3 ls s3://#{@bucket}/#{folder} --recursive | awk '{print $4}' > /tmp/#{basefolder} 2>&1\")\n    File.readlines(\"/tmp/#{basefolder}\").each do |line|\n      @searches.each do |search|\n        puts line if line.match(search)\n      end\n    end\n    system(\"rm /tmp/#{basefolder}\")\nI would really like a more elegant (and correct) way of accomplishing this :) Thanks!. Here's the code that I'm working off of right now:\n```\nclass S3Find\n  def initialize(bucket, searches, prefix)\n    @s3 = Aws::S3::Client.new\n    @bucket = bucket\n    @searches = searches\n    @prefix = prefix\n  end\n# Returns an array of S3 bucket folders\n  def sub_folders\n    array = []\n    resp = @s3.list_objects_v2(bucket: @bucket, prefix: @prefix, delimiter: '/')\n    resp.common_prefixes.each do |folder|\n      array.push(folder.prefix)\n    end\n    array\n  end\n# Returns a list of files from an S3 folder\n  # PATCH: For now we don't know how to search past 1000 objects, so just\n  # use the local AWS CLI to pull in ALL of the results and filter from there\n  # TODO: Get this working\n  def list_matches_in_folder(folder)\n    #resp = @s3.list_objects_v2(bucket: @bucket, prefix: folder)  ## TODO\n    basefolder = File.basename folder\n    system(\"aws s3 ls s3://#{@bucket}/#{folder} --recursive | awk '{print $4}' > /tmp/#{basefolder} 2>&1\")\n    File.readlines(\"/tmp/#{basefolder}\").each do |line|\n      @searches.each do |search|\n        puts line if line.match(search)\n      end\n    end\n    #system(\"rm /tmp/#{basefolder}\")\n    #resp.contents.each do |object|\n    #  puts object.key\n      #@searches.each do |search|\n      #  puts object.key if object.key.match(search)\n      #end\n    #end\n  end\nend\ns3 = S3Find.new(bucket, searches, prefix)\nsubs = s3.sub_folders\nsubs.each do |folder|\n    s3.list_matches_in_folder(folder)\n  end\nend\n```\nbucket is the bucket that you want to query\nsearches is an array of Regular Expressions\nprefix is the prefix within the bucket that you want to query under\nlist_matches_in_folder is where i'm getting stuck. I'm only getting back 1000 results from a query where there are more than 1000 objects. So I never get to the prefixes that I actually want to search (i.e. 2017-01-01).\n. I did a little searching and found some documentation on pagination here:\nhttps://docs.aws.amazon.com/sdk-for-ruby/v3/developer-guide/paging-responses.html\nI've adjusted the code in my class and it works perfectly:\n```\nclass S3Find\n  def initialize(bucket, searches, prefix)\n    @s3 = Aws::S3::Client.new\n    @bucket = bucket\n    @searches = searches\n    @prefix = prefix\n  end\n# Returns an array of S3 bucket folders\n  def sub_folders\n    array = []\n    resp = @s3.list_objects_v2(bucket: @bucket, prefix: @prefix, delimiter: '/')\n    resp.common_prefixes.each do |folder|\n      array.push(folder.prefix)\n    end\n    array\n  end\n# Returns a list of files from an S3 folder matching one regex search\n  # from @searches array\n  def list_matches_in_folder(folder)\n    @s3.list_objects_v2(bucket: @bucket, prefix: folder).each do |resp|\n      s3objpaths = resp.contents.map(&:key)\n      s3objpaths.each do |str|\n        puts str if search_string(str)\n      end\n    end\n  end\n# Search a string using the search array\n  def search_string(str)\n    @searches.each do |search|\n      return true if str.match(search)\n    end\n    false\n  end\nend\ns3 = S3Find.new(bucket, searches, prefix)\nsubs = s3.sub_folders\nsubs.each do |folder|\n    s3.list_matches_in_folder(folder)\nend\n```\nThanks for the hint!. ",
    "utilum": "Thank you for reviewing, @awood45 . I've removed it, and would be happy to improve the generator as well if you would provide some guidance, not sure where to start.. Resolved by other commits.. ",
    "mksm": "I understand it has to do some extra work but sometimes it takes much longer: no keys set:           0.150000   0.150000   0.300000 (  5.403111).\nI'll add dummy credentials for now. Thanks for looking into this.. ",
    "himberjack": "I cannot reproduce the exact senario, but if the code is run in multi thread enviroment, sometimes, for the same input, different languages are returned. So it could be either a global variable here, or there is an issue with Comprehend itself - however, the latter is eliminated as I made it mutex-ed, and it is working fine. So I bet on the former cause.. ",
    "adolski": "Sorry, this was my mistake. I didn't know that get_object() with no :response_target option downloaded an object into memory. I thought additional steps were required to do that (e.g. reading the body of the GetObjectOutput). What I needed was head_object().. ",
    "nicolasleger": "Tests should have pass cause no change made to code for jruby-9.1.5.0.\nIt seems undeterministic.. ",
    "NorseGaud": "Thanks for the follow up. Really appreciate the help.\nFor the HTTP GET, it's using \nurl = URI('https://vodoriginstorentlka01.g.XXXX.net')\n      https = Net::HTTP.new(url.host,url.port)\n      https.use_ssl = true\n      https.verify_mode = OpenSSL::SSL::VERIFY_NONE # SSL not signed? Don't care!\nRemember though, the HTTPS GET works. I'm more trying to use the Aws::S3 features instead of HTTP to accomplish what I want.\nIs there a way to modify s3_client = Aws::S3::Client.new(endpoint: 'https://vodoriginstorentlka01.g.XXXX.net'), or even s3_client so I can prevent it from using https://mpegstills01.vodoriginstorentlka01.g.XXXX.net and instead use https://vodoriginstorentlka01.g.XXXX.net/mpegstills01 ?\n. Found a spec that seems to indicate that region.endpoint is the format it's definitely setup to create: https://github.com/aws/aws-sdk-ruby/blob/9ea447b7e9e57e27bb7924aca40a5bf0c3265a94/gems/aws-sdk-s3/spec/object/presigned_post_spec.rb\nI'll try and play with it and add a feature for what I want. If anyone has any advice for an existing way to achieve this, please let me know! Thanks\n. More review seems to indicate force_path_style: true is a solution, and it is.\nI had originally used this in my configuration and it didn't change the endpoint/request url. Attempting it again is working. Here is the working code:\n```\nAws.config.update({\n  region: 'vodoriginstorentlka01.g.XXXX.net',\n  force_path_style: true,\n  credentials: Aws::Credentials.new(ENV['ONEORIGIN_AWS_ACCESS_KEY_ID'], ENV['ONEORIGIN_AWS_SECRET_ACCESS_KEY'])\n})\ns3_client = Aws::S3::Client.new(endpoint: 'http://vodoriginstorentlka01.g.XXXX.net')\nAws::S3::Resource.new(client: s3_client).bucket('mpegstills01').objects({prefix: 'CSNC7402979000090001'})\nAws::S3::Resource.new(client: s3_client).bucket('mpegstills01').objects.limit(50).each do |item|\n  puts \"Name:  #{item.key}\"\nend```. ",
    "Fonsan": "I am playing with the thought of implementing a #upload_stream that would yield a io pipe enabling the use of IO.copy_stream and friends which offers built in back pressure. . The documentation should talk about memory usage, the default strategy uses 10 threads and if they are saturated they each are uploading 5MB of data which brings memory consumption to 50MB in #upload_stream. I am guessing that lowering the default number of threads will result performance losses in 90% of use cases where 50MB is readily available.\nWe could offer a strategy that has the same memory characteristics as #upload_file (a few KB by my estimates) by writing a tempfile for each part. But for most use-cases writing and reading via disk seems like a detour. tempfile feature is now implemented as a optional boolean which forces minimal memory usage by never keeping any full part content in memory.\npart_size option has been added to customize the size of parts. I do not have any insight into what would be a good default so for now it is 5MB. The failing jruby spec fails due to a bug in jruby 9.1.5.0 it is however fixed from 9.1.15.0. \nhttps://github.com/jruby/jruby/commit/cd40c31fb8291c7570fcd27015a64790de912d86. @tmedford Funnily enough I had the same problem and went down the rabbit hole that is how bundler searches for gemspecs. Unfortunately I found no way of using the :git directive since it bundler does not traverse the folder structure which would be needed to find gems/aws-sdk-s3/aws-sdk-s3.gemspec. If you check out the project locally and do gem build aws-sdk-s3.gemspec and then gem install path-to-produced.gem inside your projects gemset you might be able to get it to work.. @tmedford \n\nSo if I absolutely needed the stream and tempfile, memory, I would have to deploy my own gem version and source from that until this is merged into repo and another release is pushed?\n\nThere might be a better way but that is the suggestion I could come up with.\n\nCurrently I am seeing a 200mb file take reach up to 600mb when using upload_file (multipart). Messed around with the threads to no avail. \n\nI would not have guessed that #upload_file would have produce a large memory footprint looking at the implementation. Given that the method has been around a few years it is unlikely that there is a correlation between the file size and memory foot print. It would be very interesting if you could come up with a clean example case that used excessive amounts of memory.\n\nAlso do you have a file example for the steam I could validate against.\n\nA example usage of #upload_stream can be found in the step definitions here: https://github.com/aws/aws-sdk-ruby/pull/1711/files#diff-08088f982fdbffd39e194fe7bb233d09\n. @cjyclaire Let me know if you have any questions :). @awood45 great! . @awood45  What is the next step?. @tmedford cool, is there any feedback on the design or implementation?. @tmedford Did you read my earlier comment https://github.com/aws/aws-sdk-ruby/pull/1711#issuecomment-364792290\nIf keeping support for older patch versions of JRuby is critical then the equivalent without the #copy_stream call from the example below would be\nbytes_copied = IO.copy_stream(read_pipe, temp_io, @part_size)\ntranslates into \nruby\nbuffer = read_pipe.read(@part_size)\nbytes_copied = buffer.bytesize\ntemp_io.write(buffer). 9.1.6.0 is 18 months old and there have been more than 10 patch releases for 9.1 since \nhttp://jruby.org/2016/11/09/jruby-9-1-6-0.html\nWhere can I read up on the policy you have for supporting older patch releases of rubies?. @awood45 pinged the wrong guy above. While I do agree on keeping compatibility or for old versions of any Ruby, the limit for me really goes at the minor version https://semver.org/ since any patch release may contain a bug for any special case writing code in a third party library that handles all historic patch versions of all minor versions would be near impossible.\nI would urge you to reconsider the policy of keeping compatibility with anything than the latest patch version. One could conceive a scenario in which a bug in one patch version is followed up by another patch version with a different bug which yields no way conform to both versions without some obscure handling . @awood45 I did some digging when posting this pr and https://github.com/aws/aws-sdk-ruby/pull/1711#issuecomment-364792290 is the commit that fixes the issue. My bad, I missed the case for reading when there was no data left to read\nruby\nbuffer = read_pipe.read(@part_size)\nbytes_copied = if buffer\n  temp_io.write(buffer)\n  buffer.bytesize\nelse\n  0\nend. Perhaps we should as an exercise run the current stable release tests against all historic patch versions of jruby.\nMy money is on multiple releases failing to pass the tests. I see your point and it is a noble effort to avoid forcing users to upgrade. The backwards compatible snippet above might introduce using more memory (twice if I think about it) for all versions and all Rubies. There is a memory usage increase. It will use twice the memory for when the stream is in memory mode and it will act as if in memory mode for tempfile which defeats the purpose of the tempfile approach. There might be a convoluted way to make it backwards compatible by reading and writing small chunks but I think going down that road would prove some of the points I layed out in #1711 . @awood45 Would you consider adding a explicit exception that would be raised if this functionality was called from a incompatible version of jruby allowing for a better experience for the developer?\n. I looked into reimplementing the same semantics as IO.copy_stream and realised that rubinius must do the same thing. \nhttps://github.com/rubinius/rubinius/blob/15b12dbf61f91f44851282af2956587eed9bc128/core/io.rb#L1041\nhttps://github.com/rubinius/rubinius/blob/15b12dbf61f91f44851282af2956587eed9bc128/core/io.rb#L1123\nBut I am not even sure that they get it right as they call seek on the IO object and not all IO objects support seek. Nevermind the bit about seek, MRI makes the same assumption for the fourth parameter https://ruby-doc.org/core-2.3.1/IO.html#method-c-copy_stream\n. ",
    "tmedford": "Could we get this merged in? . @Fonsan How would I reference your version in my local gemfile?. @Fonsan So if I absolutely needed the stream and tempfile, memory, I would have to deploy my own gem version and source from that until this is merged into repo and another release is pushed? Currently I am seeing a 200mb file take reach up to 600mb when using upload_file (multipart). Messed around with the threads to no avail. Also do you have a file example for the steam I could validate against. . @cjyclaire can we get an update. ",
    "pfleeter": "I saw these errors in my logs at 19:23:07.334 +00:00 and checked the AWS status page, but everything is reported as nominal.  I'm using the .NET SDK and the call stack is below:\n\nSystem.AggregateException: One or more errors occurred. (Error unmarshalling response back from AWS. Response Body: Http/1.1 Service Unavailable ) ---> Amazon.Runtime.AmazonUnmarshallingException: Error unmarshalling response back from AWS. Response Body: Http/1.1 Service Unavailable  ---> System.Xml.XmlException: Root element is missing.\nat System.Xml.XmlTextReaderImpl.Throw(Exception e)\nat System.Xml.XmlTextReaderImpl.ParseDocumentContent()\nat System.Xml.XmlTextReaderImpl.Read()\nat Amazon.Runtime.Internal.Transform.XmlUnmarshallerContext.Read()\nat Amazon.Runtime.Internal.Transform.ErrorResponseUnmarshaller.Unmarshall(XmlUnmarshallerContext context)\nat Amazon.Runtime.Internal.Transform.JsonErrorResponseUnmarshaller.Unmarshall(JsonUnmarshallerContext context)\nat Amazon.DynamoDBv2.Model.Internal.MarshallTransformations.GetItemResponseUnmarshaller.UnmarshallException(JsonUnmarshallerContext context, Exception innerException, HttpStatusCode statusCode)\nat Amazon.Runtime.Internal.Transform.JsonResponseUnmarshaller.UnmarshallException(UnmarshallerContext input, Exception innerException, HttpStatusCode statusCode)\nat Amazon.Runtime.Internal.HttpErrorResponseExceptionHandler.HandleException(IExecutionContext executionContext, HttpErrorResponseException exception)\n--- End of inner exception stack trace ---\nat Amazon.Runtime.Internal.HttpErrorResponseExceptionHandler.HandleException(IExecutionContext executionContext, HttpErrorResponseException exception)\nat Amazon.Runtime.Internal.ErrorHandler.ProcessException(IExecutionContext executionContext, Exception exception)\nat Amazon.Runtime.Internal.ErrorHandler.d__51.MoveNext()\n--- End of stack trace from previous location where exception was thrown ---\nat System.Runtime.ExceptionServices.ExceptionDispatchInfo.Throw()\nat System.Runtime.CompilerServices.TaskAwaiter.HandleNonSuccessAndDebuggerNotification(Task task)\nat Amazon.Runtime.Internal.CallbackHandler.<InvokeAsync>d__91.MoveNext()\n--- End of stack trace from previous location where exception was thrown ---\nat System.Runtime.ExceptionServices.ExceptionDispatchInfo.Throw()\nat System.Runtime.CompilerServices.TaskAwaiter.HandleNonSuccessAndDebuggerNotification(Task task)\nat Amazon.Runtime.Internal.CredentialsRetriever.d__71.MoveNext()\n--- End of stack trace from previous location where exception was thrown ---\nat System.Runtime.ExceptionServices.ExceptionDispatchInfo.Throw()\nat System.Runtime.CompilerServices.TaskAwaiter.HandleNonSuccessAndDebuggerNotification(Task task)\nat Amazon.Runtime.Internal.RetryHandler.<InvokeAsync>d__101.MoveNext()\n--- End of stack trace from previous location where exception was thrown ---\nat System.Runtime.ExceptionServices.ExceptionDispatchInfo.Throw()\nat Amazon.Runtime.Internal.RetryHandler.d__101.MoveNext()\n--- End of stack trace from previous location where exception was thrown ---\nat System.Runtime.ExceptionServices.ExceptionDispatchInfo.Throw()\nat System.Runtime.CompilerServices.TaskAwaiter.HandleNonSuccessAndDebuggerNotification(Task task)\nat Amazon.Runtime.Internal.CallbackHandler.<InvokeAsync>d__91.MoveNext()\n--- End of stack trace from previous location where exception was thrown ---\nat System.Runtime.ExceptionServices.ExceptionDispatchInfo.Throw()\nat System.Runtime.CompilerServices.TaskAwaiter.HandleNonSuccessAndDebuggerNotification(Task task)\nat Amazon.Runtime.Internal.CallbackHandler.d__91.MoveNext()\n--- End of stack trace from previous location where exception was thrown ---\nat System.Runtime.ExceptionServices.ExceptionDispatchInfo.Throw()\nat System.Runtime.CompilerServices.TaskAwaiter.HandleNonSuccessAndDebuggerNotification(Task task)\nat Amazon.Runtime.Internal.ErrorCallbackHandler.<InvokeAsync>d__51.MoveNext()\n--- End of stack trace from previous location where exception was thrown ---\nat System.Runtime.ExceptionServices.ExceptionDispatchInfo.Throw()\nat System.Runtime.CompilerServices.TaskAwaiter.HandleNonSuccessAndDebuggerNotification(Task task)\nat Amazon.Runtime.Internal.MetricsHandler.d__1`1.MoveNext()\n--- End of inner exception stack trace ---\nat System.Threading.Tasks.Task.ThrowIfExceptional(Boolean includeTaskCanceledExceptions)\nat System.Threading.Tasks.Task.Wait(Int32 millisecondsTimeout, CancellationToken cancellationToken)\nat System.Threading.Tasks.Task.Wait(). \n",
    "phene": "AWS Support thinks it's the fault of the API library.\n```\nThanks for contacting AWS Support. My name is Gurdeep and I will assist you with this case.\nFrom your case correspondence, I understand while using the AWS Ruby SDK for making the API call \u201crevoke_security_group_ingress\u201d you are getting the following error: \n\u201cAws::EC2::Errors::InvalidParameterValue: missing mandatory parameter: exactly one of remote-security-group, remote-ip-range, remote-ipv6-range, or prefix-list-id must be present\u201d.\nYou have passed the security group id and the IP permissions in the request. But the AWS server is returning the error that required parameters were not passed in the query. Please, correct me if I\u2019m wrong.\nI did deep dive and replicated the scenario you want to achieve with the provided code. I got the same error during the replication. For the further analysis that what is causing the issue, I have taken the packet capture and in the capture, it seems that the API request \u201crevoke_security_group_ingress\u201d is not sending the required parameters to the AWS Server. That's why AWS Server is returning the \u201cInvalidParameterValue\u201d error.\nPacket Capture showing the API call without required parameters:\n08:13:23.721861 IP ip-172-31-6-219.ap-northeast-1.compute.internal.37336 > 54.239.96.159.http: Flags [P.], seq 2638:2971, ack 1727, win 486, length 333 \n0x0000: 4500 0175 5162 4000 ff06 de97 ac1f 06db E..uQb@......... \n0x0010: 36ef 609f 91d8 0050 9154 1fd4 4bdf bcdf 6.`....P.T..K... \n0x0020: 5018 01e6 4bf0 0000 4163 7469 6f6e 3d52 P...K...Action=R \n0x0030: 6576 6f6b 6553 6563 7572 6974 7947 726f evokeSecurityGro \n0x0040: 7570 496e 6772 6573 7326 4772 6f75 7049 upIngress&GroupI \n0x0050: 643d 7367 2d39 3865 3161 3065 3126 4970 d=sg-98e1a0e1&Ip \n0x0060: 5065 726d 6973 7369 6f6e 732e 312e 4672 Permissions.1.Fr \n0x0070: 6f6d 506f 7274 3d31 3233 2649 7050 6572 omPort=123&IpPer \n0x0080: 6d69 7373 696f 6e73 2e31 2e47 726f 7570 missions.1.Group \n0x0090: 733d 2649 7050 6572 6d69 7373 696f 6e73 s=&IpPermissions \n0x00a0: 2e31 2e49 7050 726f 746f 636f 6c3d 7463 .1.IpProtocol=tc \n0x00b0: 7026 4970 5065 726d 6973 7369 6f6e 732e p&IpPermissions. \n0x00c0: 312e 4970 5261 6e67 6573 2e31 2e43 6964 1.IpRanges.1.Cid \n0x00d0: 7249 703d 342e 342e 342e 3425 3246 3332 rIp=4.4.4.4%2F32 \n0x00e0: 2649 7050 6572 6d69 7373 696f 6e73 2e31 &IpPermissions.1 \n0x00f0: 2e49 7052 616e 6765 732e 312e 4465 7363 .IpRanges.1.Desc \n0x0100: 7269 7074 696f 6e3d 7465 7374 2649 7050 ription=test&IpP \n0x0110: 6572 6d69 7373 696f 6e73 2e31 2e49 7076 ermissions.1.Ipv \n0x0120: 3652 616e 6765 733d 2649 7050 6572 6d69 6Ranges=&IpPermi \n0x0130: 7373 696f 6e73 2e31 2e50 7265 6669 784c ssions.1.PrefixL \n0x0140: 6973 7449 6473 3d26 4970 5065 726d 6973 istIds=&IpPermis \n0x0150: 7369 6f6e 732e 312e 546f 506f 7274 3d31 sions.1.ToPort=1 \n0x0160: 3233 2656 6572 7369 6f6e 3d32 3031 362d 23&Version=2016- \n0x0170: 3131 2d31 35 11-15 \nI have raised the issue with the internal SDK team. The internal team is looking into the matter and will fix it. I will update you with the finding of the internal team. Issue is also get acknowledged by the AWS on the Github.\nFor the time being if possible please use other SDKs available on the AWS. For checking more available SDKs please refer to the below given link:\nhttps://aws.amazon.com/tools/\nHope this information is useful for you. Meanwhile, If you have a further query feel free to write us back. We will be more than happy to help you.\nBest regards,\nGURDEEP S.\nAmazon Web Services\n``. When I throw some debugging into the aws client, I see that my request params properly includes theGroups.1.GroupId`, etc that AWS Support does not include.\n\"Action=RevokeSecurityGroupIngress\n&GroupId=sg-bd0f69c2\n&IpPermissions.1.FromPort=0\n&IpPermissions.1.Groups.1.GroupId=sg-af0264d0\n&IpPermissions.1.Groups.1.UserId=elided\n&IpPermissions.1.Groups.2.GroupId=sg-bd0f69c2\n&IpPermissions.1.Groups.2.UserId=elided\n&IpPermissions.1.IpProtocol=tcp\n&IpPermissions.1.IpRanges=\n&IpPermissions.1.Ipv6Ranges=\n&IpPermissions.1.PrefixListIds=\n&IpPermissions.1.ToPort=65535\n&IpPermissions.2.FromPort=0\n&IpPermissions.2.Groups.1.GroupId=sg-af0264d0\n&IpPermissions.2.Groups.1.UserId=elided\n&IpPermissions.2.Groups.2.GroupId=sg-bd0f69c2\n&IpPermissions.2.Groups.2.UserId=elided\n&IpPermissions.2.IpProtocol=udp\n&IpPermissions.2.IpRanges=\n&IpPermissions.2.Ipv6Ranges=\n&IpPermissions.2.PrefixListIds=\n&IpPermissions.2.ToPort=65535\n&IpPermissions.3.FromPort=-1\n&IpPermissions.3.Groups.1.GroupId=sg-af0264d0\n&IpPermissions.3.Groups.1.UserId=elided\n&IpPermissions.3.Groups.2.GroupId=sg-bd0f69c2\n&IpPermissions.3.Groups.2.UserId=elided\n&IpPermissions.3.IpProtocol=icmp\n&IpPermissions.3.IpRanges=\n&IpPermissions.3.Ipv6Ranges=\n&IpPermissions.3.PrefixListIds=\n&IpPermissions.3.ToPort=-1\n&Version=2016-11-15\". Are you testing with security group rules that refer to other security groups? I'm performing this on the security groups that EMR generates ingress rules onto.\n2018-02-22 22:38:33 UTC 492 test.rb: info: Clearing security group ingress rules for geofftest-Pipeline-MAU-Hourly-EMRSlaveSecurityGroup-1HAQF7I7DYHQQ\nopening connection to ec2.us-west-2.amazonaws.com:443...\nopened\nstarting SSL for ec2.us-west-2.amazonaws.com:443...\nSSL established\n<- \"POST / HTTP/1.1\\r\\nContent-Type: application/x-www-form-urlencoded; charset=utf-8\\r\\nAccept-Encoding: \\r\\nUser-Agent: aws-sdk-ruby3/3.16.0 ruby/2.4.2 x86_64-darwin16 aws-sdk-ec2/1.27.0\\r\\nHost: ec2.us-west-2.amazonaws.com\\r\\nX-Amz-Date: 20180222T223833Z\\r\\nX-Amz-Security-Token: [elided]\\r\\nX-Amz-Content-Sha256: 2a023cace68c591c82bbc6e763856e26b24d9be4ff7e80ab7e6b64f2f0edcc55\\r\\nAuthorization: AWS4-HMAC-SHA256 Credential=ASIAJXJFIVK4U7KZNVBA/20180222/us-west-2/ec2/aws4_request, SignedHeaders=content-type;host;x-amz-content-sha256;x-amz-date;x-amz-security-token, Signature=265decb5522a4fc03b347351de0d972566caa20f90a0a00518380b1363880f64\\r\\nContent-Length: 1127\\r\\nAccept: */*\\r\\n\\r\\n\"\n-> \"HTTP/1.1 400 Bad Request\\r\\n\"\n-> \"Transfer-Encoding: chunked\\r\\n\"\n-> \"Date: Thu, 22 Feb 2018 22:38:33 GMT\\r\\n\"\n-> \"Connection: close\\r\\n\"\n-> \"Server: AmazonEC2\\r\\n\"\n-> \"\\r\\n\"\n-> \"154\\r\\n\"\nreading 340 bytes...\n-> \"<?xml version=\\\"1.0\\\" encoding=\\\"UTF-8\\\"?>\\n<Response><Errors><Error><Code>InvalidParameterValue</Code><Message>missing mandatory parameter: exactly one of remote-security-group, remote-ip-range, remote-ipv6-range, or prefix-list-id must be present</Message></Error></Errors><RequestID>632215ad-8a0d-4b6a-a345-d7132e914ec5</RequestID></Response>\"\nread 340 bytes\nreading 2 bytes...\n-> \"\\r\\n\"\nread 2 bytes\n-> \"0\\r\\n\"\n-> \"\\r\\n\"\nConn close\n/Users/geoff/.rvm/gems/ruby-2.4.2@analytics-cluster/gems/aws-sdk-core-3.16.0/lib/seahorse/client/plugins/raise_response_errors.rb:15:in `call': missing mandatory parameter: exactly one of remote-security-group, remote-ip-range, remote-ipv6-range, or prefix-list-id must be present (Aws::EC2::Errors::InvalidParameterValue)\n    from /Users/geoff/.rvm/gems/ruby-2.4.2@analytics-cluster/gems/aws-sdk-core-3.16.0/lib/aws-sdk-core/plugins/jsonvalue_converter.rb:20:in `call'\n    from /Users/geoff/.rvm/gems/ruby-2.4.2@analytics-cluster/gems/aws-sdk-core-3.16.0/lib/aws-sdk-core/plugins/idempotency_token.rb:17:in `call'\n    from /Users/geoff/.rvm/gems/ruby-2.4.2@analytics-cluster/gems/aws-sdk-core-3.16.0/lib/aws-sdk-core/plugins/param_converter.rb:24:in `call'\n    from /Users/geoff/.rvm/gems/ruby-2.4.2@analytics-cluster/gems/aws-sdk-core-3.16.0/lib/aws-sdk-core/plugins/response_paging.rb:10:in `call'\n    from /Users/geoff/.rvm/gems/ruby-2.4.2@analytics-cluster/gems/aws-sdk-core-3.16.0/lib/seahorse/client/plugins/response_target.rb:23:in `call'\n    from /Users/geoff/.rvm/gems/ruby-2.4.2@analytics-cluster/gems/aws-sdk-core-3.16.0/lib/seahorse/client/request.rb:70:in `send_request'\n    from /Users/geoff/.rvm/gems/ruby-2.4.2@analytics-cluster/gems/aws-sdk-ec2-1.27.0/lib/aws-sdk-ec2/client.rb:22760:in `revoke_security_group_ingress'. That doesn't make sense -- doesn't your earlier test pass both ip-range and ipv6-range given they are properties of IpPermission?  You also left out remote-security-group from your list of mandatory parameters, which is the key here.  How would you modify my code such that it succeeds? My suspicion is that the IpPermissions.1.Groups.1.GroupId parameter is not be interpreted as the remote-security-group.. So is the conclusion that revoking via IpPermissions is not supported? This goes against SDK docs.. OK, then the SDK docs and parameter validation should be updated to reflect that. It's rather inconvenient to do this since an IpPermission object can actually reflect multiple rules since there can be multiple user_id_group_pairs entries. This also seems like a major naming kludge and should probably be rectified.. Not yet... I'm not even sure what to do with this error message or the VPC is mentions. There's no VPC ID parameter in the revoke_security_group_ingress method.\n\n```ruby\n      def clear_security_group_rules\n        security_groups.each do |security_group|\n          Helpers.info \"Clearing security group ingress rules for #{security_group.group_name}\"\n          puts security_group.to_hash\n          security_group.ip_permissions.each do |ip_permission|\n            revocations(ip_permission).each do |revocation|\n              revocation.merge!(group_id: security_group.group_id)\n              puts revocation\n              Helpers.call_aws(\n                ec2_client, :revoke_security_group_ingress,\n                revocation\n              )\n            end\n          end\n        end\n      end\n  def revocations(ip_permission)\n    [].tap do |rules|\n      if ip_permission.user_id_group_pairs.any?\n        ip_permission.user_id_group_pairs.each do |user_id_group_pair|\n          rules << {\n            ip_protocol: ip_permission.ip_protocol,\n            to_port: ip_permission.to_port,\n            source_security_group_name: user_id_group_pair.group_id,\n            source_security_group_owner_id: user_id_group_pair.user_id,\n          }\n        end\n      end\n    end\n  end\n\n```\n$ ruby test.rb\n2018-02-23 01:48:39 UTC 18898 test.rb: info: Clearing security group ingress rules for geofftest-Pipeline-MAU-Hourly-EMRSlaveSecurityGroup-1HAQF7I7DYHQQ\n{:description=>\"geofftest-Pipeline-MAU-Hourly EMR Slave\", :group_name=>\"geofftest-Pipeline-MAU-Hourly-EMRSlaveSecurityGroup-1HAQF7I7DYHQQ\", :ip_permissions=>[{:from_port=>0, :ip_protocol=>\"tcp\", :ip_ranges=>[], :ipv_6_ranges=>[], :prefix_list_ids=>[], :to_port=>65535, :user_id_group_pairs=>[{:group_id=>\"sg-af0264d0\", :user_id=>\"585460489129\"}, {:group_id=>\"sg-bd0f69c2\", :user_id=>\"585460489129\"}]}, {:from_port=>0, :ip_protocol=>\"udp\", :ip_ranges=>[], :ipv_6_ranges=>[], :prefix_list_ids=>[], :to_port=>65535, :user_id_group_pairs=>[{:group_id=>\"sg-af0264d0\", :user_id=>\"585460489129\"}, {:group_id=>\"sg-bd0f69c2\", :user_id=>\"585460489129\"}]}, {:from_port=>-1, :ip_protocol=>\"icmp\", :ip_ranges=>[], :ipv_6_ranges=>[], :prefix_list_ids=>[], :to_port=>-1, :user_id_group_pairs=>[{:group_id=>\"sg-af0264d0\", :user_id=>\"585460489129\"}, {:group_id=>\"sg-bd0f69c2\", :user_id=>\"585460489129\"}]}], :owner_id=>\"585460489129\", :group_id=>\"sg-bd0f69c2\", :ip_permissions_egress=>[{:ip_protocol=>\"-1\", :ip_ranges=>[{:cidr_ip=>\"0.0.0.0/0\"}], :ipv_6_ranges=>[], :prefix_list_ids=>[], :user_id_group_pairs=>[]}], :tags=>[{:key=>\"region\", :value=>\"us-west-2\"}, {:key=>\"input_file\", :value=>\"config/parameters/stack-input-files/data-pipeline/default.yaml\"}, {:key=>\"system_name\", :value=>\"geofftest\"}, {:key=>\"Name\", :value=>\"geofftest-Pipeline-MAU-Hourly-EMR-Slave\"}, {:key=>\"aws:cloudformation:logical-id\", :value=>\"EMRSlaveSecurityGroup\"}, {:key=>\"aws:cloudformation:stack-name\", :value=>\"geofftest-Pipeline-MAU-Hourly\"}, {:key=>\"branch\", :value=>\"task/INTEL-4552-emr-managed-security-groups\"}, {:key=>\"user_id\", :value=>\"Jenkins\"}, {:key=>\"aws:cloudformation:stack-id\", :value=>\"arn:aws:cloudformation:us-west-2:585460489129:stack/geofftest-Pipeline-MAU-Hourly/d6b03ee0-168b-11e8-926b-503aca41a099\"}, {:key=>\"main_stack\", :value=>\"geofftest-Pipeline-MAU-Hourly\"}, {:key=>\"build_number\", :value=>\"latest\"}, {:key=>\"job_name\", :value=>\"MAUHourlyPipeline\"}], :vpc_id=>\"vpc-16f0646f\"}\n{:ip_protocol=>\"tcp\", :to_port=>65535, :source_security_group_name=>\"sg-af0264d0\", :source_security_group_owner_id=>\"585460489129\", :group_id=>\"sg-bd0f69c2\"}\n/Users/geoff/.rvm/gems/ruby-2.4.2@analytics-cluster/gems/aws-sdk-core-3.16.0/lib/seahorse/client/plugins/raise_response_errors.rb:15:in `call': The security group 'sg-af0264d0' does not exist in default VPC 'vpc-ba8abbde' (Aws::EC2::Errors::InvalidGroupNotFound)\n    from /Users/geoff/.rvm/gems/ruby-2.4.2@analytics-cluster/gems/aws-sdk-core-3.16.0/lib/aws-sdk-core/plugins/jsonvalue_converter.rb:20:in `call'\n    from /Users/geoff/.rvm/gems/ruby-2.4.2@analytics-cluster/gems/aws-sdk-core-3.16.0/lib/aws-sdk-core/plugins/idempotency_token.rb:17:in `call'\n    from /Users/geoff/.rvm/gems/ruby-2.4.2@analytics-cluster/gems/aws-sdk-core-3.16.0/lib/aws-sdk-core/plugins/param_converter.rb:24:in `call'\n    from /Users/geoff/.rvm/gems/ruby-2.4.2@analytics-cluster/gems/aws-sdk-core-3.16.0/lib/aws-sdk-core/plugins/response_paging.rb:10:in `call'\n    from /Users/geoff/.rvm/gems/ruby-2.4.2@analytics-cluster/gems/aws-sdk-core-3.16.0/lib/seahorse/client/plugins/response_target.rb:23:in `call'\n    from /Users/geoff/.rvm/gems/ruby-2.4.2@analytics-cluster/gems/aws-sdk-core-3.16.0/lib/seahorse/client/request.rb:70:in `send_request'\n    from /Users/geoff/.rvm/gems/ruby-2.4.2@analytics-cluster/gems/aws-sdk-ec2-1.27.0/lib/aws-sdk-ec2/client.rb:22760:in `revoke_security_group_ingress'\n    from /Users/geoff/projects/ops.aws.analytics-cluster/lib/configurator/helpers.rb:255:in `call_aws'\n    from /Users/geoff/projects/ops.aws.analytics-cluster/lib/configurator/stack/data_pipeline.rb:25:in `block (3 levels) in clear_security_group_rules'. I see that source_security_group_name requires it belongs to the default VPC. How can I remove a rule for a source security group that does not exist on the default VPC?. Somehow, this works via the CLI:\naws --profile [elided] --region us-west-2 ec2 revoke-security-group-ingress --group-id sg-c56410ba --source-group sg-577f0b28 --protocol tcp --port 0-65535. Sending one ip_permission entry at a time (like the aws cli) yields the same original error:\n```ruby\n      def clear_security_group_rules\n        security_groups.each do |security_group|\n          Helpers.info \"Clearing security group ingress rules for #{security_group.group_name}\"\n          security_group.ip_permissions.each do |ip_permission|\n            next unless ip_permission.user_id_group_pairs.any?\n            ip_permission.user_id_group_pairs.each do |user_id_group_pair|\n              ip_perm = ip_permission.to_hash\n              ip_perm[:user_id_group_pairs] = [user_id_group_pair.to_hash]\n          ec2_client.revoke_security_group_ingress(\n            group_id: security_group.group_id,\n            ip_permissions: [ip_perm]\n          )\n         end\n      end\n    end\n  end\n\n``. It appears that removing the \"empty\" key/value pairs fixes the issue. Reconstructing a new hash (rather than using to_hash, which includes:ip_ranges=>[], :ipv_6_ranges=>[], :prefix_list_ids=>[],`) succeeds. The error \"missing parameter\" is doubly infuriating since I included too many of them.. To summarize this issue. I cannot send:\nruby\n{\n  group_id: [elided],\n  ip_permissions: [{\n    :from_port => 0, \n    :ip_protocol => \"tcp\", \n    :ip_ranges => [], \n    :ipv_6_ranges => [], \n    :prefix_list_ids => [], \n    :to_port => 65535, \n    :user_id_group_pairs => [\n      {:group_id => \"[elided1]\", :user_id => \"[elided1]\"}, \n      {:group_id => \"[elided2]\", :user_id => \"[elided2]\"}\n    ]\n  },\n  { \n    :from_port => -1,\n    :ip_protocol => \"icmp\", \n    :ip_ranges => [], \n    :ipv_6_ranges => [], \n    :prefix_list_ids => [], \n    :to_port => -1, \n    :user_id_group_pairs => [\n      {:group_id => \"[elided1]\", :user_id => \"[elided1]\"}, \n      {:group_id => \"[elided2]\", :user_id => \"[elided2]\"}\n    ]\n  },\n  ]\n}\nBut I can send:\nruby\n{\n  group_id: [elided],\n  ip_permissions: [{\n    :from_port => 0, \n    :ip_protocol => \"tcp\", \n    :to_port => 65535, \n    :user_id_group_pairs => [\n      {:group_id => \"[elided1]\", :user_id => \"[elided1]\"}\n    ]\n  }]\n}\nand\nruby\n{\n  group_id: [elided],\n  ip_permissions: [{\n    :from_port => 0, \n    :ip_protocol => \"tcp\", \n    :to_port => 65535, \n    :user_id_group_pairs => [\n      {:group_id => \"[elided2]\", :user_id => \"[elided2]\"}\n    ]\n  }]\n}\nand \nruby\n{\n  group_id: [elided],\n  ip_permissions: [{\n    :from_port => -1, \n    :ip_protocol => \"icmp\", \n    :to_port => -1, \n    :user_id_group_pairs => [\n      {:group_id => \"[elided1]\", :user_id => \"[elided1]\"}\n    ]\n  }]\n}\n, etc.... ",
    "rcrews": "The TravisCI test failure is on JRuby 9.1.5.0 only. My changes pass on all other Rubies. I am not able to reproduce the error on JRuby 9.1.16.0. I wonder if the problem is with JRuby 9.1.5.0 (and has since been fixed in 9.1.16.0)? The error relates to calling [] on a nil object, but the spec says @location should default to nil, which is why I used nil for the initialization. The TravisCI problem is at aws-sdk-s3/lib/aws-sdk-s3/object_multipart_copier.rb#L56. The changes in commits 572052a and 9d0343a are not failing.\nRobert. I reverted the commit that initialized the location field. I'll work on solving the warning in another way.. ",
    "jcamejo": "Hi sorry for the delayed answer.\nI was trying to set up my bucket for my S3 integration, i've managed it to accomplish it, but i was thinking that maybe it would be nice to set up the default bucket name in the initial global object. But seeing a little bit more on how it works, maybe is not the best idea. Either way, i appreciate the response.. ",
    "kchsieh": "Updated Mar-3-2018\nThis worked:\nbash\nretry_backoff:lambda{|c| sleep 60}\nBut, not this:\nbash\nretry_backoff:lambda{|c| 60}\nOld comment\nI added the parameter: retry_backoff: lambda { |c| 60 } but it didn't help, all retries happened within 2 seconds.\nFyi, I am using aws-sdk-apigateway v1.9.0 and ruby-2.5.0. ",
    "Gusarov2k": "\nI tried to install AWS CLI, then ran aws configure and set the attributes:\nAWS Acccess Key ID: key\nAWS Secret Access Key: secret\nRegion: None\nFormat: None\n\nIn the file ~ / My_progect / config / aws-sample.yml replaced the data on\ndevelopment:\n  access_key_id: key\n  secret_access_key: secret\ntest:\n  access_key_id: key\n  secret_access_key: secret\nproduction:\n  access_key_id: key\n  secret_access_key: secret\n\nIn the file, aws-sdk-1.25.0 / lib / aws / core / configuration.rb undocumented\n\nAWS.config (: access_key_id => 'key',: secret_access_key => 'secret')\nbut instead received a new error:\nuninitialized constant AWS :: Core :: Configuration\n\nCreated variables\nACCESS_KEY_ID = \"key\"\nSECRET_ACCESS_KEY = \"secret\" in the .bashrc file. It's old project ((. Thank you so much for your great work! I will try to update.\nTell me please this question if I download the database from the production server and install it on the local machine, will my application work or will it still produce an aws error?. \n",
    "juanignaciosl": "@abarrak how did you fix this? \ud83d\ude05 . ",
    "abarrak": "My case was in the test context.\nI was stubbing Time in other test cases without returning it back to current.\n@juanignaciosl Ensure you're not using time stubbing helpers.\nIf you didn't encounter this tests, then:\n\nThis is in most cases  a system, clock, issue.\nSome SDKs have extra options to handle it (e.g: correctClockSkew), but not ruby unfortunately.\nDelayed Request in S3 queue that caused request time to increase.. \n",
    "povils": "Sorry for the not complete question. Yes, I am referring to Auto Scaling :). Thank for response. Yes it's pretty much what I'm doing now. As I see aws-cli also doesnt have this feature, thus maybe it doesnt make sense to expect it in this SDK.. ",
    "hiboma": "Thank you. I appreciate it !. ",
    "beaglebets": "Thank you for your prompt response.. ",
    "oniofchaos": "If someone wants to point me to the right place to update the code generators in order to get frozen_string_literal: true in newly generated files, I'd be happy to look into making that update as well.. I didn't change the line that is failing in Ruby 1.9.3, looks like it has to deal with an encoding issue. Any thoughts?. ",
    "limitusus": "Here is wire trace log (masked)\nThere looks no ValidationMethod in it, as documented in API\n<- \"POST / HTTP/1.1\\r\\nContent-Type: application/x-amz-json-1.1\\r\\nAccept-Encoding: \\r\\nUser-Agent: aws-sdk-ruby3/3.17.1 ruby/2.4.2 x86_64-linux aws-sdk-acm/1.3.0\\r\\nX-Amz-Target: CertificateManager.DescribeCertificate\\r\\nHost: acm.us-east-1.amazonaws.com\\r\\nX-Amz-Date: 20180326T174616Z\\r\\nX-Amz-Content-Sha256: aec613797c700a96b89b1c276d91cd2dede0dbb9b8ba5bb44d3d92ab519a478a\\r\\nAuthorization: AWS4-HMAC-SHA256 Credential=AKIAAAAAAAAAAAAAAAAA/20180326/us-east-1/acm/aws4_request, SignedHeaders=content-type;host;x-amz-content-sha256;x-amz-date;x-amz-target, Signature=b0794a2c376b8ef22e7ffb033d637f464aefd6c5e6491c098a3d947a0394aaef\\r\\nContent-Length: 104\\r\\nAccept: */*\\r\\n\\r\\n\"\n-> \"HTTP/1.1 200 OK\\r\\n\"\n-> \"x-amzn-RequestId: 95c6db68-311d-11e8-890f-6b32867b8be1\\r\\n\"\n-> \"Content-Type: application/x-amz-json-1.1\\r\\n\"\n-> \"Content-Length: 1530\\r\\n\"\n-> \"Date: Mon, 26 Mar 2018 17:46:17 GMT\\r\\n\"\n-> \"\\r\\n\"\nreading 1530 bytes...\n-> \"\"\n-> \"{\\\"Certificate\\\":{\\\"CertificateArn\\\":\\\"arn:aws:acm:us-east-1:XXXXXXXXXXXX:certificate/fbe3b5a4-8206-4a5d-8791-143e30980f95\\\",\\\"CreatedAt\\\":1.50875886E9,\\\"DomainName\\\":\\\"xxx.com\\\",\\\"DomainValidationOptions\\\":[{\\\"DomainName\\\":\\\"xxx.com\\\",\\\"ValidationDomain\\\":\\\"xxx.com\\\",\\\"ValidationEmails\\\":[\\\"admin@xxx.com\\\",\\\"webmaster@xxx.com\\\",\\\"administrator@xxx.com\\\",\\\"hostmaster@xxx.com\\\",\\\"postmaster@xxx.com\\\"],\\\"ValidationStatus\\\":\\\"SUCCESS\\\"},{\\\"DomainName\\\":\\\"*.xxx.com\\\",\\\"ValidationDomain\\\":\\\"xxx.com\\\",\\\"ValidationEmails\\\":[\\\"admin@xxx.com\\\",\\\"webmaster@xxx.com\\\",\\\"administrator@xxx.com\\\",\\\"hostmaster@xxx.com\\\",\\\"postmaster@xxx.com\\\"],\\\"ValidationStatus\\\":\\\"SUCCESS\\\"}],\\\"ExtendedKeyUsages\\\":[{\\\"Name\\\":\\\"TLS_WEB_SERVER_AUTHENTICATION\\\",\\\"OID\\\":\\\"1.3.6.1.5.5.7.3.1\\\"},{\\\"Name\\\":\\\"TLS_WEB_CLIENT_AUTHENTICATION\\\",\\\"OID\\\":\\\"1.3.6.1.5.5.7.3.2\\\"}],\\\"InUseBy\\\":[\\\"arn:aws:cloudfront::XXXXXXXXXXXX:distribution/EEEEEEEEEEEE\\\"],\\\"IssuedAt\\\":1.50875931E9,\\\"Issuer\\\":\\\"Amazon\\\",\\\"KeyAlgorithm\\\":\\\"RSA-2048\\\",\\\"KeyUsages\\\":[{\\\"Name\\\":\\\"DIGITAL_SIGNATURE\\\"},{\\\"Name\\\":\\\"KEY_ENCIPHERMENT\\\"}],\\\"NotAfter\\\":1.5429744E9,\\\"NotBefore\\\":1.5087168E9,\\\"Options\\\":{\\\"CertificateTransparencyLoggingPreference\\\":\\\"ENABLED\\\"},\\\"Serial\\\":\\\"09:20:ef:8e:e3:98:c9:ea:43:52:17:c0:b2:ba:3f:ae\\\",\\\"SignatureAlgorithm\\\":\\\"SHA256WITHRSA\\\",\\\"Status\\\":\\\"ISSUED\\\",\\\"Subject\\\":\\\"CN=xxx.com\\\",\\\"SubjectAlternativeNames\\\":[\\\"xxx.com\\\",\\\"*.xxx.com\\\"],\\\"Type\\\":\\\"AMAZON_ISSUED\\\"}}\". Forgot important info: the previous comment was for Email-validation certs. Wire trace log for DNS-validated ones include ValidationMethod in JSON response.\n\\\"ValidationMethod\\\":\\\"DNS\\\". I contacted AWS and they know this issue. We need to distinguish the validation method only by the key's existence.... ",
    "david-a": "Just want to update, I've found the cause for the problematic (empty) definition of Enumerable::Enumerator in my environment - for me it was the gem CFPropertyList-rails (v1.0.2) which uses an old version of CFPropertyList (v2.2.8).. Sure, I'll look into it later today. \nThanks for your quick response!. @awood45 done :)\nI've looked deeper into it, and this issue occurs only in environments which have another Enumerable module defined, and this module has an Enumerator class defined without each and next methods. \nSomehow my environment has this module, and I know it may be an edge case, but it won't hurt to add those :: as a precaution.. Just want to update, I've found the cause for the problematic (empty) definition of Enumerable::Enumerator in my environment - for me it was the gem CFPropertyList-rails (v1.0.2) which uses an old version of CFPropertyList (v2.2.8).. @cjyclaire sure, under what version number?. @cjyclaire done, I hope it's ok now. ",
    "corbanmailloux": "I'd like to follow up on why this was closed. Is there a different place I should report the issue for the API to be included in the SDKs?. Thanks for following up.. ",
    "dineshyadav009": "\nCode Snippet\nrequire 'aws-sdk-pricing'\nclient = Aws::Pricing::Client.new(:http_wire_trace => true)\nrequest_body = {\n      service_code: 'AmazonEC2',\n      filters: [\n          {field: 'ServiceCode', type: 'TERM_MATCH', value: 'AmazonEC2'},\n          {field: 'operatingSystem', type: 'TERM_MATCH', value: 'Linux'},\n          {field: 'tenancy', type: 'TERM_MATCH', value: 'Shared'},\n          {field: 'instanceType', type: 'TERM_MATCH', value: 'm5.large'},\n          {field: 'location', type: 'TERM_MATCH', value: 'Asia Pacific (Mumbai)'}\n      ]\n  }\nresp = client.get_products(request_body)\nDocs\nhttps://aws.amazon.com/ec2/pricing/reserved-instances/pricing/\nAlso, when I log into the aws console and navigate to EC2, then to purchase reserved instances and there also price shown matches with the doc I shared above.\nWire log\n\n```\nopening connection to api.pricing.us-east-1.amazonaws.com:443...\nopened\nstarting SSL for api.pricing.us-east-1.amazonaws.com:443...\nSSL established\n<- \"POST / HTTP/1.1\\r\\nContent-Type: application/x-amz-json-1.1\\r\\nAccept-Encoding: \\r\\nUser-Agent: aws-sdk-ruby3/3.7.0 ruby/2.1.7 x86_64-linux aws-sdk-pricing/1.0.0\\r\\nX-Amz-Target: AWSPriceListService.GetProducts\\r\\nHost: api.pricing.us-east-1.amazonaws.com\\r\\nX-Amz-Date: 20180331T102453Z\\r\\nX-Amz-Content-Sha256: cece3daf8ff420bedfb9f4fa95397d6c5afe7009acfdd49cfc40ba8982ecd33a\\r\\nAuthorization: AWS4-HMAC-SHA256 Credential=AKIAJMHDMDCO6VYKVBDA/20180331/us-east-1/pricing/aws4_request, SignedHeaders=content-type;host;x-amz-content-sha256;x-amz-date;x-amz-target, Signature=e52ec830f3f67ac28b75f8345d0af7d04766b9b7c679212d9ad80cb2e6301a18\\r\\nContent-Length: 361\\r\\nAccept: /\\r\\n\\r\\n\"\n-> \"HTTP/1.1 200 OK\\r\\n\"\n-> \"x-amzn-RequestId: c1f774bd-34cd-11e8-9672-65d8e79fcbb8\\r\\n\"\n-> \"Content-Type: application/x-amz-json-1.1\\r\\n\"\n-> \"Content-Length: 10324\\r\\n\"\n-> \"Date: Sat, 31 Mar 2018 10:24:56 GMT\\r\\n\"\n-> \"\\r\\n\"\nreading 10324 bytes...\n-> \"\"\n-> \"{\\\"FormatVersion\\\":\\\"aws_v1\\\",\\\"PriceList\\\":[\\\"{\\\\\"product\\\\\":{\\\\\"productFamily\\\\\":\\\\\"Compute Instance\\\\\",\\\\\"attributes\\\\\":{\\\\\"enhancedNetworkingSupported\\\\\":\\\\\"Yes\\\\\",\\\\\"memory\\\\\":\\\\\"8 GiB\\\\\",\\\\\"dedicatedEbsThroughput\\\\\":\\\\\"Upto 2120 Mbps\\\\\",\\\\\"vcpu\\\\\":\\\\\"2\\\\\",\\\\\"capacitystatus\\\\\":\\\\\"Used\\\\\",\\\\\"locationType\\\\\":\\\\\"AWS Region\\\\\",\\\\\"storage\\\\\":\\\\\"EBS only\\\\\",\\\\\"instanceFamily\\\\\":\\\\\"General purpose\\\\\",\\\\\"operatingSystem\\\\\":\\\\\"Linux\\\\\",\\\\\"physicalProcessor\\\\\":\\\\\"Intel Xeon Platinum 8175\\\\\",\\\\\"clockSpeed\\\\\":\\\\\"2.5 GHz\\\\\",\\\\\"ecu\\\\\":\\\\\"10\\\\\",\\\\\"networkPerformance\\\\\":\\\\\"Up to 10 Gigabit\\\\\",\\\\\"servicename\\\\\":\\\\\"Amazon Elastic Compute Cloud\\\\\",\\\\\"instanceType\\\\\":\\\\\"m5.large\\\\\",\\\\\"tenancy\\\\\":\\\\\"Shared\\\\\",\\\\\"usagetype\\\\\":\\\\\"APS3-BoxUsage:m5.large\\\\\",\\\\\"normalizationSizeFactor\\\\\":\\\\\"4\\\\\",\\\\\"processorFeatures\\\\\":\\\\\"Intel AVX, Intel AVX2, Intel AVX512, Intel Turbo\\\\\",\\\\\"servicecode\\\\\":\\\\\"AmazonEC2\\\\\",\\\\\"licenseModel\\\\\":\\\\\"No License required\\\\\",\\\\\"currentGeneration\\\\\":\\\\\"Yes\\\\\",\\\\\"preInstalledSw\\\\\":\\\\\"NA\\\\\",\\\\\"location\\\\\":\\\\\"Asia Pacific (Mumbai)\\\\\",\\\\\"processorArchitecture\\\\\":\\\\\"64-bit\\\\\",\\\\\"operation\\\\\":\\\\\"RunInstances\\\\\"},\\\\\"sku\\\\\":\\\\\"ZHUPGV53EVQF5VC7\\\\\"},\\\\\"serviceCode\\\\\":\\\\\"AmazonEC2\\\\\",\\\\\"terms\\\\\":{\\\\\"OnDemand\\\\\":{\\\\\"ZHUPGV53EVQF5VC7.JRTCKXETXF\\\\\":{\\\\\"priceDimensions\\\\\":{\\\\\"ZHUPGV53EVQF5VC7.JRTCKXETXF.6YS6EN2CT7\\\\\":{\\\\\"unit\\\\\":\\\\\"Hrs\\\\\",\\\\\"endRange\\\\\":\\\\\"Inf\\\\\",\\\\\"description\\\\\":\\\\\"$0.101 per On Demand Linux m5.large Instance Hour\\\\\",\\\\\"appliesTo\\\\\":[],\\\\\"rateCode\\\\\":\\\\\"ZHUPGV53EVQF5VC7.JRTCKXETXF.6YS6EN2CT7\\\\\",\\\\\"beginRange\\\\\":\\\\\"0\\\\\",\\\\\"pricePerUnit\\\\\":{\\\\\"USD\\\\\":\\\\\"0.1010000000\\\\\"}}},\\\\\"sku\\\\\":\\\\\"ZHUPGV53EVQF5VC7\\\\\",\\\\\"effectiveDate\\\\\":\\\\\"2018-03-01T00:00:00Z\\\\\",\\\\\"offerTermCode\\\\\":\\\\\"JRTCKXETXF\\\\\",\\\\\"termAttributes\\\\\":{}}},\\\\\"Reserved\\\\\":{\\\\\"ZHUPGV53EVQF5VC7.7NE97W5U4E\\\\\":{\\\\\"priceDimensions\\\\\":{\\\\\"ZHUPGV53EVQF5VC7.7NE97W5U4E.6YS6EN2CT7\\\\\":{\\\\\"unit\\\\\":\\\\\"Hrs\\\\\",\\\\\"endRange\\\\\":\\\\\"Inf\\\\\",\\\\\"description\\\\\":\\\\\"Linux/UNIX (Amazon VPC), m5.large reserved instance applied\\\\\",\\\\\"appliesTo\\\\\":[],\\\\\"rateCode\\\\\":\\\\\"ZHUPGV53EVQF5VC7.7NE97W5U4E.6YS6EN2CT7\\\\\",\\\\\"beginRange\\\\\":\\\\\"0\\\\\",\\\\\"pricePerUnit\\\\\":{\\\\\"USD\\\\\":\\\\\"0.0870000000\\\\\"}}},\\\\\"sku\\\\\":\\\\\"ZHUPGV53EVQF5VC7\\\\\",\\\\\"effectiveDate\\\\\":\\\\\"2018-02-28T23:59:59Z\\\\\",\\\\\"offerTermCode\\\\\":\\\\\"7NE97W5U4E\\\\\",\\\\\"termAttributes\\\\\":{\\\\\"LeaseContractLength\\\\\":\\\\\"1yr\\\\\",\\\\\"OfferingClass\\\\\":\\\\\"convertible\\\\\",\\\\\"PurchaseOption\\\\\":\\\\\"No Upfront\\\\\"}},\\\\\"ZHUPGV53EVQF5VC7.4NA7Y494T4\\\\\":{\\\\\"priceDimensions\\\\\":{\\\\\"ZHUPGV53EVQF5VC7.4NA7Y494T4.6YS6EN2CT7\\\\\":{\\\\\"unit\\\\\":\\\\\"Hrs\\\\\",\\\\\"endRange\\\\\":\\\\\"Inf\\\\\",\\\\\"description\\\\\":\\\\\"Linux/UNIX (Amazon VPC), m5.large reserved instance applied\\\\\",\\\\\"appliesTo\\\\\":[],\\\\\"rateCode\\\\\":\\\\\"ZHUPGV53EVQF5VC7.4NA7Y494T4.6YS6EN2CT7\\\\\",\\\\\"beginRange\\\\\":\\\\\"0\\\\\",\\\\\"pricePerUnit\\\\\":{\\\\\"USD\\\\\":\\\\\"0.0650000000\\\\\"}}},\\\\\"sku\\\\\":\\\\\"ZHUPGV53EVQF5VC7\\\\\",\\\\\"effectiveDate\\\\\":\\\\\"2018-02-28T23:59:59Z\\\\\",\\\\\"offerTermCode\\\\\":\\\\\"4NA7Y494T4\\\\\",\\\\\"termAttributes\\\\\":{\\\\\"LeaseContractLength\\\\\":\\\\\"1yr\\\\\",\\\\\"OfferingClass\\\\\":\\\\\"standard\\\\\",\\\\\"PurchaseOption\\\\\":\\\\\"No Upfront\\\\\"}},\\\\\"ZHUPGV53EVQF5VC7.CUZHX8X6JH\\\\\":{\\\\\"priceDimensions\\\\\":{\\\\\"ZHUPGV53EVQF5VC7.CUZHX8X6JH.2TG2D8R56U\\\\\":{\\\\\"unit\\\\\":\\\\\"Quantity\\\\\",\\\\\"description\\\\\":\\\\\"Upfront Fee\\\\\",\\\\\"appliesTo\\\\\":[],\\\\\"rateCode\\\\\":\\\\\"ZHUPGV53EVQF5VC7.CUZHX8X6JH.2TG2D8R56U\\\\\",\\\\\"pricePerUnit\\\\\":{\\\\\"USD\\\\\":\\\\\"311\\\\\"}},\\\\\"ZHUPGV53EVQF5VC7.CUZHX8X6JH.6YS6EN2CT7\\\\\":{\\\\\"unit\\\\\":\\\\\"Hrs\\\\\",\\\\\"endRange\\\\\":\\\\\"Inf\\\\\",\\\\\"description\\\\\":\\\\\"Linux/UNIX (Amazon VPC), m5.large reserved instance applied\\\\\",\\\\\"appliesTo\\\\\":[],\\\\\"rateCode\\\\\":\\\\\"ZHUPGV53EVQF5VC7.CUZHX8X6JH.6YS6EN2CT7\\\\\",\\\\\"beginRange\\\\\":\\\\\"0\\\\\",\\\\\"pricePerUnit\\\\\":{\\\\\"USD\\\\\":\\\\\"0.0360000000\\\\\"}}},\\\\\"sku\\\\\":\\\\\"ZHUPGV53EVQF5VC7\\\\\",\\\\\"effectiveDate\\\\\":\\\\\"2018-02-28T23:59:59Z\\\\\",\\\\\"offerTermCode\\\\\":\\\\\"CUZHX8X6JH\\\\\",\\\\\"termAttributes\\\\\":{\\\\\"LeaseContractLength\\\\\":\\\\\"1yr\\\\\",\\\\\"OfferingClass\\\\\":\\\\\"convertible\\\\\",\\\\\"PurchaseOption\\\\\":\\\\\"Partial Upfront\\\\\"}},\\\\\"ZHUPGV53EVQF5VC7.HU7G6KETJZ\\\\\":{\\\\\"priceDimensions\\\\\":{\\\\\"ZHUPGV53EVQF5VC7.HU7G6KETJZ.2TG2D8R56U\\\\\":{\\\\\"unit\\\\\":\\\\\"Quantity\\\\\",\\\\\"description\\\\\":\\\\\"Upfront Fee\\\\\",\\\\\"appliesTo\\\\\":[],\\\\\"rateCode\\\\\":\\\\\"ZHUPGV53EVQF5VC7.HU7G6KETJZ.2TG2D8R56U\\\\\",\\\\\"pricePerUnit\\\\\":{\\\\\"USD\\\\\":\\\\\"271\\\\\"}},\\\\\"ZHUPGV53EVQF5VC7.HU7G6KETJZ.6YS6EN2CT7\\\\\":{\\\\\"unit\\\\\":\\\\\"Hrs\\\\\",\\\\\"endRange\\\\\":\\\\\"Inf\\\\\",\\\\\"description\\\\\":\\\\\"Linux/UNIX (Amazon VPC), m5.large reserved instance applied\\\\\",\\\\\"appliesTo\\\\\":[],\\\\\"rateCode\\\\\":\\\\\"ZHUPGV53EVQF5VC7.HU7G6KETJZ.6YS6EN2CT7\\\\\",\\\\\"beginRange\\\\\":\\\\\"0\\\\\",\\\\\"pricePerUnit\\\\\":{\\\\\"USD\\\\\":\\\\\"0.0310000000\\\\\"}}},\\\\\"sku\\\\\":\\\\\"ZHUPGV53EVQF5VC7\\\\\",\\\\\"effectiveDate\\\\\":\\\\\"2018-02-28T23:59:59Z\\\\\",\\\\\"offerTermCode\\\\\":\\\\\"HU7G6KETJZ\\\\\",\\\\\"termAttributes\\\\\":{\\\\\"LeaseContractLength\\\\\":\\\\\"1yr\\\\\",\\\\\"OfferingClass\\\\\":\\\\\"standard\\\\\",\\\\\"PurchaseOption\\\\\":\\\\\"Partial Upfront\\\\\"}},\\\\\"ZHUPGV53EVQF5VC7.VJWZNREJX2\\\\\":{\\\\\"priceDimensions\\\\\":{\\\\\"ZHUPGV53EVQF5VC7.VJWZNREJX2.6YS6EN2CT7\\\\\":{\\\\\"unit\\\\\":\\\\\"Hrs\\\\\",\\\\\"endRange\\\\\":\\\\\"Inf\\\\\",\\\\\"description\\\\\":\\\\\"Linux/UNIX (Amazon VPC), m5.large reserved instance applied\\\\\",\\\\\"appliesTo\\\\\":[],\\\\\"rateCode\\\\\":\\\\\"ZHUPGV53EVQF5VC7.VJWZNREJX2.6YS6EN2CT7\\\\\",\\\\\"beginRange\\\\\":\\\\\"0\\\\\",\\\\\"pricePerUnit\\\\\":{\\\\\"USD\\\\\":\\\\\"0.0000000000\\\\\"}},\\\\\"ZHUPGV53EVQF5VC7.VJWZNREJX2.2TG2D8R56U\\\\\":{\\\\\"unit\\\\\":\\\\\"Quantity\\\\\",\\\\\"description\\\\\":\\\\\"Upfront Fee\\\\\",\\\\\"appliesTo\\\\\":[],\\\\\"rateCode\\\\\":\\\\\"ZHUPGV53EVQF5VC7.VJWZNREJX2.2TG2D8R56U\\\\\",\\\\\"pricePerUnit\\\\\":{\\\\\"USD\\\\\":\\\\\"610\\\\\"}}},\\\\\"sku\\\\\":\\\\\"ZHUPGV53EVQF5VC7\\\\\",\\\\\"effectiveDate\\\\\":\\\\\"2018-02-28T23:59:59Z\\\\\",\\\\\"offerTermCode\\\\\":\\\\\"VJWZNREJX2\\\\\",\\\\\"termAttributes\\\\\":{\\\\\"LeaseContractLength\\\\\":\\\\\"1yr\\\\\",\\\\\"OfferingClass\\\\\":\\\\\"convertible\\\\\",\\\\\"PurchaseOption\\\\\":\\\\\"All Upfront\\\\\"}},\\\\\"ZHUPGV53EVQF5VC7.BPH4J8HBKS\\\\\":{\\\\\"priceDimensions\\\\\":{\\\\\"ZHUPGV53EVQF5VC7.BPH4J8HBKS.6YS6EN2CT7\\\\\":{\\\\\"unit\\\\\":\\\\\"Hrs\\\\\",\\\\\"endRange\\\\\":\\\\\"Inf\\\\\",\\\\\"description\\\\\":\\\\\"Linux/UNIX (Amazon VPC), m5.large reserved instance applied\\\\\",\\\\\"appliesTo\\\\\":[],\\\\\"rateCode\\\\\":\\\\\"ZHUPGV53EVQF5VC7.BPH4J8HBKS.6YS6EN2CT7\\\\\",\\\\\"beginRange\\\\\":\\\\\"0\\\\\",\\\\\"pricePerUnit\\\\\":{\\\\\"USD\\\\\":\\\\\"0.0440000000\\\\\"}}},\\\\\"sku\\\\\":\\\\\"ZHUPGV53EVQF5VC7\\\\\",\\\\\"effectiveDate\\\\\":\\\\\"2018-02-28T23:59:59Z\\\\\",\\\\\"offerTermCode\\\\\":\\\\\"BPH4J8HBKS\\\\\",\\\\\"termAttributes\\\\\":{\\\\\"LeaseContractLength\\\\\":\\\\\"3yr\\\\\",\\\\\"OfferingClass\\\\\":\\\\\"standard\\\\\",\\\\\"PurchaseOption\\\\\":\\\\\"No Upfront\\\\\"}},\\\\\"ZHUPGV53EVQF5VC7.MZU6U2429S\\\\\":{\\\\\"priceDimensions\\\\\":{\\\\\"ZHUPGV53EVQF5VC7.MZU6U2429S.2TG2D8R56U\\\\\":{\\\\\"unit\\\\\":\\\\\"Quantity\\\\\",\\\\\"description\\\\\":\\\\\"Upfront Fee\\\\\",\\\\\"appliesTo\\\\\":[],\\\\\"rateCode\\\\\":\\\\\"ZHUPGV53EVQF5VC7.MZU6U2429S.2TG2D8R56U\\\\\",\\\\\"pricePerUnit\\\\\":{\\\\\"USD\\\\\":\\\\\"1210\\\\\"}},\\\\\"ZHUPGV53EVQF5VC7.MZU6U2429S.6YS6EN2CT7\\\\\":{\\\\\"unit\\\\\":\\\\\"Hrs\\\\\",\\\\\"endRange\\\\\":\\\\\"Inf\\\\\",\\\\\"description\\\\\":\\\\\"Linux/UNIX (Amazon VPC), m5.large reserved instance applied\\\\\",\\\\\"appliesTo\\\\\":[],\\\\\"rateCode\\\\\":\\\\\"ZHUPGV53EVQF5VC7.MZU6U2429S.6YS6EN2CT7\\\\\",\\\\\"beginRange\\\\\":\\\\\"0\\\\\",\\\\\"pricePerUnit\\\\\":{\\\\\"USD\\\\\":\\\\\"0.0000000000\\\\\"}}},\\\\\"sku\\\\\":\\\\\"ZHUPGV53EVQF5VC7\\\\\",\\\\\"effectiveDate\\\\\":\\\\\"2018-02-28T23:59:59Z\\\\\",\\\\\"offerTermCode\\\\\":\\\\\"MZU6U2429S\\\\\",\\\\\"termAttributes\\\\\":{\\\\\"LeaseContractLength\\\\\":\\\\\"3yr\\\\\",\\\\\"OfferingClass\\\\\":\\\\\"convertible\\\\\",\\\\\"PurchaseOption\\\\\":\\\\\"All Upfront\\\\\"}},\\\\\"ZHUPGV53EVQF5VC7.Z2E3P23VKM\\\\\":{\\\\\"priceDimensions\\\\\":{\\\\\"ZHUPGV53EVQF5VC7.Z2E3P23VKM.6YS6EN2CT7\\\\\":{\\\\\"unit\\\\\":\\\\\"Hrs\\\\\",\\\\\"endRange\\\\\":\\\\\"Inf\\\\\",\\\\\"description\\\\\":\\\\\"Linux/UNIX (Amazon VPC), m5.large reserved instance applied\\\\\",\\\\\"appliesTo\\\\\":[],\\\\\"rateCode\\\\\":\\\\\"ZHUPGV53EVQF5VC7.Z2E3P23VKM.6YS6EN2CT7\\\\\",\\\\\"beginRange\\\\\":\\\\\"0\\\\\",\\\\\"pricePerUnit\\\\\":{\\\\\"USD\\\\\":\\\\\"0.0510000000\\\\\"}}},\\\\\"sku\\\\\":\\\\\"ZHUPGV53EVQF5VC7\\\\\",\\\\\"effectiveDate\\\\\":\\\\\"2018-02-28T23:59:59Z\\\\\",\\\\\"offerTermCode\\\\\":\\\\\"Z2E3P23VKM\\\\\",\\\\\"termAttributes\\\\\":{\\\\\"LeaseContractLength\\\\\":\\\\\"3yr\\\\\",\\\\\"OfferingClass\\\\\":\\\\\"convertible\\\\\",\\\\\"PurchaseOption\\\\\":\\\\\"No Upfront\\\\\"}},\\\\\"ZHUPGV53EVQF5VC7.38NPMPTW36\\\\\":{\\\\\"priceDimensions\\\\\":{\\\\\"ZHUPGV53EVQF5VC7.38NPMPTW36.2TG2D8R56U\\\\\":{\\\\\"unit\\\\\":\\\\\"Quantity\\\\\",\\\\\"description\\\\\":\\\\\"Upfront Fee\\\\\",\\\\\"appliesTo\\\\\":[],\\\\\"rateCode\\\\\":\\\\\"ZHUPGV53EVQF5VC7.38NPMPTW36.2TG2D8R56U\\\\\",\\\\\"pricePerUnit\\\\\":{\\\\\"USD\\\\\":\\\\\"539\\\\\"}},\\\\\"ZHUPGV53EVQF5VC7.38NPMPTW36.6YS6EN2CT7\\\\\":{\\\\\"unit\\\\\":\\\\\"Hrs\\\\\",\\\\\"endRange\\\\\":\\\\\"Inf\\\\\",\\\\\"description\\\\\":\\\\\"Linux/UNIX (Amazon VPC), m5.large reserved instance applied\\\\\",\\\\\"appliesTo\\\\\":[],\\\\\"rateCode\\\\\":\\\\\"ZHUPGV53EVQF5VC7.38NPMPTW36.6YS6EN2CT7\\\\\",\\\\\"beginRange\\\\\":\\\\\"0\\\\\",\\\\\"pricePerUnit\\\\\":{\\\\\"USD\\\\\":\\\\\"0.0240000000\\\\\"}}},\\\\\"sku\\\\\":\\\\\"ZHUPGV53EVQF5VC7\\\\\",\\\\\"effectiveDate\\\\\":\\\\\"2018-02-28T23:59:59Z\\\\\",\\\\\"offerTermCode\\\\\":\\\\\"38NPMPTW36\\\\\",\\\\\"termAttributes\\\\\":{\\\\\"LeaseContractLength\\\\\":\\\\\"3yr\\\\\",\\\\\"OfferingClass\\\\\":\\\\\"standard\\\\\",\\\\\"PurchaseOption\\\\\":\\\\\"Partial Upfront\\\\\"}},\\\\\"ZHUPGV53EVQF5VC7.R5XV2EPZQZ\\\\\":{\\\\\"priceDimensions\\\\\":{\\\\\"ZHUPGV53EVQF5VC7.R5XV2EPZQZ.6YS6EN2CT7\\\\\":{\\\\\"unit\\\\\":\\\\\"Hrs\\\\\",\\\\\"endRange\\\\\":\\\\\"Inf\\\\\",\\\\\"description\\\\\":\\\\\"Linux/UNIX (Amazon VPC), m5.large reserved instance applied\\\\\",\\\\\"appliesTo\\\\\":[],\\\\\"rateCode\\\\\":\\\\\"ZHUPGV53EVQF5VC7.R5XV2EPZQZ.6YS6EN2CT7\\\\\",\\\\\"beginRange\\\\\":\\\\\"0\\\\\",\\\\\"pricePerUnit\\\\\":{\\\\\"USD\\\\\":\\\\\"0.0240000000\\\\\"}},\\\\\"ZHUPGV53EVQF5VC7.R5XV2EPZQZ.2TG2D8R56U\\\\\":{\\\\\"unit\\\\\":\\\\\"Quantity\\\\\",\\\\\"description\\\\\":\\\\\"Upfront Fee\\\\\",\\\\\"appliesTo\\\\\":[],\\\\\"rateCode\\\\\":\\\\\"ZHUPGV53EVQF5VC7.R5XV2EPZQZ.2TG2D8R56U\\\\\",\\\\\"pricePerUnit\\\\\":{\\\\\"USD\\\\\":\\\\\"618\\\\\"}}},\\\\\"sku\\\\\":\\\\\"ZHUPGV53EVQF5VC7\\\\\",\\\\\"effectiveDate\\\\\":\\\\\"2018-02-28T23:59:59Z\\\\\",\\\\\"offerTermCode\\\\\":\\\\\"R5XV2EPZQZ\\\\\",\\\\\"termAttributes\\\\\":{\\\\\"LeaseContractLength\\\\\":\\\\\"3yr\\\\\",\\\\\"OfferingClass\\\\\":\\\\\"convertible\\\\\",\\\\\"PurchaseOption\\\\\":\\\\\"Partial Upfront\\\\\"}},\\\\\"ZHUPGV53EVQF5VC7.NQ3QZPMQV9\\\\\":{\\\\\"priceDimensions\\\\\":{\\\\\"ZHUPGV53EVQF5VC7.NQ3QZPMQV9.2TG2D8R56U\\\\\":{\\\\\"unit\\\\\":\\\\\"Quantity\\\\\",\\\\\"description\\\\\":\\\\\"Upfront Fee\\\\\",\\\\\"appliesTo\\\\\":[],\\\\\"rateCode\\\\\":\\\\\"ZHUPGV53EVQF5VC7.NQ3QZPMQV9.2TG2D8R56U\\\\\",\\\\\"pricePerUnit\\\\\":{\\\\\"USD\\\\\":\\\\\"1161\\\\\"}},\\\\\"ZHUPGV53EVQF5VC7.NQ3QZPMQV9.6YS6EN2CT7\\\\\":{\\\\\"unit\\\\\":\\\\\"Hrs\\\\\",\\\\\"endRange\\\\\":\\\\\"Inf\\\\\",\\\\\"description\\\\\":\\\\\"Linux/UNIX (Amazon VPC), m5.large reserved instance applied\\\\\",\\\\\"appliesTo\\\\\":[],\\\\\"rateCode\\\\\":\\\\\"ZHUPGV53EVQF5VC7.NQ3QZPMQV9.6YS6EN2CT7\\\\\",\\\\\"beginRange\\\\\":\\\\\"0\\\\\",\\\\\"pricePerUnit\\\\\":{\\\\\"USD\\\\\":\\\\\"0.0000000000\\\\\"}}},\\\\\"sku\\\\\":\\\\\"ZHUPGV53EVQF5VC7\\\\\",\\\\\"effectiveDate\\\\\":\\\\\"2018-01-31T23:59:59Z\\\\\",\\\\\"offerTermCode\\\\\":\\\\\"NQ3QZPMQV9\\\\\",\\\\\"termAttributes\\\\\":{\\\\\"LeaseContractLength\\\\\":\\\\\"3yr\\\\\",\\\\\"OfferingClass\\\\\":\\\\\"standard\\\\\",\\\\\"PurchaseOption\\\\\":\\\\\"All Upfront\\\\\"}},\\\\\"ZHUPGV53EVQF5VC7.6QCMYABX3D\\\\\":{\\\\\"priceDimensions\\\\\":{\\\\\"ZHUPGV53EVQF5VC7.6QCMYABX3D.2TG2D8R56U\\\\\":{\\\\\"unit\\\\\":\\\\\"Quantity\\\\\",\\\\\"description\\\\\":\\\\\"Upfront Fee\\\\\",\\\\\"appliesTo\\\\\":[],\\\\\"rateCode\\\\\":\\\\\"ZHUPGV53EVQF5VC7.6QCMYABX3D.2TG2D8R56U\\\\\",\\\\\"pricePerUnit\\\\\":{\\\\\"USD\\\\\":\\\\\"620\\\\\"}},\\\\\"ZHUPGV53EVQF5VC7.6QCMYABX3D.6YS6EN2CT7\\\\\":{\\\\\"unit\\\\\":\\\\\"Hrs\\\\\",\\\\\"endRange\\\\\":\\\\\"Inf\\\\\",\\\\\"description\\\\\":\\\\\"Linux/UNIX (Amazon VPC), m5.large reserved instance applied\\\\\",\\\\\"appliesTo\\\\\":[],\\\\\"rateCode\\\\\":\\\\\"ZHUPGV53EVQF5VC7.6QCMYABX3D.6YS6EN2CT7\\\\\",\\\\\"beginRange\\\\\":\\\\\"0\\\\\",\\\\\"pricePerUnit\\\\\":{\\\\\"USD\\\\\":\\\\\"0.0000000000\\\\\"}}},\\\\\"sku\\\\\":\\\\\"ZHUPGV53EVQF5VC7\\\\\",\\\\\"effectiveDate\\\\\":\\\\\"2018-01-31T23:59:59Z\\\\\",\\\\\"offerTermCode\\\\\":\\\\\"6QCMYABX3D\\\\\",\\\\\"termAttributes\\\\\":{\\\\\"LeaseContractLength\\\\\":\\\\\"1yr\\\\\",\\\\\"OfferingClass\\\\\":\\\\\"standard\\\\\",\\\\\"PurchaseOption\\\\\":\\\\\"All Upfront\\\\\"}}}},\\\\\"version\\\\\":\\\\\"20180328223156\\\\\",\\\\\"publicationDate\\\\\":\\\\\"2018-03-28T22:31:56Z\\\\\"}\\\"]}\"\nread 10324 bytes\nConn keep-alive\n```\nYou can search for 620 in the log I shared and you will be able to find that this price belongs to All Upfront 1 year Standard for Asia Pacific Mumbai region. And that this price value is not the same as compared to value in docs/console.\nAlso, if you are able to confirm the bug, please give me an approximate time limit when this bug will get fixed. I will consider using other ways to fetch pricing if the bug resolution time is high.\nThere is a lot of mismatch. I would say ~50% of the prices are inaccurate. For the m5.large, price for 3 year standard all upfront too doesn't match.. Thanks @cjyclaire\nAwaiting updates.. Thanks @cjyclaire for the update. A few quick questions:\n1. It seems from your comment they have considered the bug as legit. Yes?\n2. A rough idea on the resolution time. Week or two, month(s)?\nI am blocked on the development due to this bug.. @cjyclaire Thanks for the update.. @cjyclaire A lot of prices have been fixed but there are still differences.\nFor eg. On demand prices for i3.xlarge for North Virginia region is wrong. The sdk returns $0.38 while actual price is $0.312. @awood45 We don't have a premium support plan. So, it would be good if you can escalate the same to the support team.. Thanks @awood45 \n@cjyclaire Waiting for the fix :). @cjyclaire Can you please provide an update on the fix?. @cjyclaire To get all the diff, I will have to code and compare prices using the code and figure out the discrepancies, or I will have to manually check the prices. So, I guess this part if done at your service team end will be more fruitful as I do not know how you fetch the prices returned by the sdk. Do you hit the APIs, if yes there is some error in parsing those prices, if not the source where sdk takes the prices from may be buggy.. @cjyclaire @awood45  Any updates?. @cjyclaire @awood45 Any updates?. Sure, will wait.. @awood45 You have stated that there is a fix needed from the server side. Can you please elaborate what server side actually means? Is it some API endpoint?\nHas it anything to do with this endpoint - https://pricing.us-east-1.amazonaws.com/offers/v1.0/aws/AmazonEC2/current/index.json. So, as per my understanding from your comment, SDK internally interacts with the API endpoint I mentioned in the above comment(which I doubt, we will come to it later). If this is true, then it is the SDK that is buggy. \nLet me elaborate a bit. I used to hit the API endpoint directly to download the pricing data, but the complexity is that pricing data is huge, so it takes a lot of memory to parse and process the pricing data. In order to avoid that, I tried using the SDK since there is pretty nice support for pagination here. So, I can confirm that the price that is wrongly returned by the SDK is correctly returned by the API endpoint (for example, on demand charges for i3.xlarge as mentioned in above comments).\nAgain, the API endpoint too returns incorrect prices for few cases and SDK returns correct prices for those cases. So there may be lot of such cases, but I stumbled upon the pricing data for c4.8xlarge class and it was incorrect when directly fetched using the API endpoint.\nregion -                                      US East (N. Virginia) \ntenancy -                                   shared\nplatform -                                   linux\ninstance type -                           c4.8xlarge  \non demand -                              1.591\none year no upfront(hourly) -     5.479 \none year all upfront cost -          46085\none year partial upfront cost -    23126  \none year partial recurring cost - 2.64\nCorrect data returned by SDK:\nregion: \"US East (N. Virginia)\", \ntenancy: \"shared\", \nplatform: \"linux\", \ninstance_type: \"c4.8xlarge\", \nSKU: \"5KQGU4DT589HHWZ6\", \nondemand_cost: 15.1(this particular price is incorrect), \none_yr_no_upfront_cost: 1.008, \none_yr_all_upfront_cost: 8293.0, \none_yr_partial_upfront_cost: 4231.0, \none_yr_partial_recurring_cost: 0.483\nCode using sdk for above case of c4.8xlarge\nrequire 'aws-sdk-pricing'\nclient = Aws::Pricing::Client.new(:http_wire_trace => true)\nrequest_body = {\n      service_code: 'AmazonEC2',\n      filters: [\n          {field: 'ServiceCode', type: 'TERM_MATCH', value: 'AmazonEC2'},\n          {field: 'operatingSystem', type: 'TERM_MATCH', value: 'Linux'},\n          {field: 'tenancy', type: 'TERM_MATCH', value: 'Shared'},\n          {field: 'instanceType', type: 'TERM_MATCH', value: 'c4.8xlarge'},\n          {field: 'location', type: 'TERM_MATCH', value: 'US East (N. Virginia)'},\n          {field: 'offeringClass', type: 'TERM_MATCH', value: 'standard'},\n          {field: 'leaseContractLength', type: 'TERM_MATCH', value: '1yr'}\n      ]\n  }\nresp = client.get_products(request_body)\nWire log:\nopening connection to api.pricing.us-east-1.amazonaws.com:443...\nopened\nstarting SSL for api.pricing.us-east-1.amazonaws.com:443...\nSSL established\n<- \"POST / HTTP/1.1\\r\\nContent-Type: application/x-amz-json-1.1\\r\\nAccept-Encoding: \\r\\nUser-Agent: aws-sdk-ruby3/3.7.0 ruby/2.1.7 x86_64-linux aws-sdk-pricing/1.0.0\\r\\nX-Amz-Target: AWSPriceListService.GetProducts\\r\\nHost: api.pricing.us-east-1.amazonaws.com\\r\\nX-Amz-Date: 20180503T210310Z\\r\\nX-Amz-Content-Sha256: 720df743f9a49a257b53c8d2d4325ef082c12567f4326095d4a791f128c22a18\\r\\nAuthorization: AWS4-HMAC-SHA256 Credential=AKIAJMHDMDCO6VYKVBDA/20180503/us-east-1/pricing/aws4_request, SignedHeaders=content-type;host;x-amz-content-sha256;x-amz-date;x-amz-target, Signature=49bd9dbf93f5ef186835e68342e55c4ec9d65749201253d32628a16075fed6ff\\r\\nContent-Length: 494\\r\\nAccept: */*\\r\\n\\r\\n\"\n-> \"HTTP/1.1 200 OK\\r\\n\"\n-> \"x-amzn-RequestId: 63bc6aca-4f15-11e8-90de-1bfb99f1f30e\\r\\n\"\n-> \"Content-Type: application/x-amz-json-1.1\\r\\n\"\n-> \"Content-Length: 42055\\r\\n\"\n-> \"Date: Thu, 03 May 2018 21:03:12 GMT\\r\\n\"\n-> \"\\r\\n\"\nreading 42055 bytes...\n-> \"\"\n-> \"{\\\"FormatVersion\\\":\\\"aws_v1\\\",\\\"PriceList\\\":[\\\"{\\\\\\\"product\\\\\\\":{\\\\\\\"productFamily\\\\\\\":\\\\\\\"Compute Instance\\\\\\\",\\\\\\\"attributes\\\\\\\":{\\\\\\\"enhancedNetworkingSupported\\\\\\\":\\\\\\\"Yes\\\\\\\",\\\\\\\"memory\\\\\\\":\\\\\\\"60 GiB\\\\\\\",\\\\\\\"dedicatedEbsThroughput\\\\\\\":\\\\\\\"4000 Mbps\\\\\\\",\\\\\\\"vcpu\\\\\\\":\\\\\\\"36\\\\\\\",\\\\\\\"capacitystatus\\\\\\\":\\\\\\\"Used\\\\\\\",\\\\\\\"locationType\\\\\\\":\\\\\\\"AWS Region\\\\\\\",\\\\\\\"storage\\\\\\\":\\\\\\\"EBS only\\\\\\\",\\\\\\\"instanceFamily\\\\\\\":\\\\\\\"Compute optimized\\\\\\\",\\\\\\\"operatingSystem\\\\\\\":\\\\\\\"Linux\\\\\\\",\\\\\\\"physicalProcessor\\\\\\\":\\\\\\\"Intel Xeon E5-2666 v3 (Haswell)\\\\\\\",\\\\\\\"clockSpeed\\\\\\\":\\\\\\\"2.9 GHz\\\\\\\",\\\\\\\"ecu\\\\\\\":\\\\\\\"132\\\\\\\",\\\\\\\"networkPerformance\\\\\\\":\\\\\\\"10 Gigabit\\\\\\\",\\\\\\\"servicename\\\\\\\":\\\\\\\"Amazon Elastic Compute Cloud\\\\\\\",\\\\\\\"instanceType\\\\\\\":\\\\\\\"c4.8xlarge\\\\\\\",\\\\\\\"tenancy\\\\\\\":\\\\\\\"Shared\\\\\\\",\\\\\\\"usagetype\\\\\\\":\\\\\\\"BoxUsage:c4.8xlarge\\\\\\\",\\\\\\\"normalizationSizeFactor\\\\\\\":\\\\\\\"64\\\\\\\",\\\\\\\"processorFeatures\\\\\\\":\\\\\\\"Intel AVX; Intel AVX2; Intel Turbo\\\\\\\",\\\\\\\"servicecode\\\\\\\":\\\\\\\"AmazonEC2\\\\\\\",\\\\\\\"licenseModel\\\\\\\":\\\\\\\"No License required\\\\\\\",\\\\\\\"currentGeneration\\\\\\\":\\\\\\\"Yes\\\\\\\",\\\\\\\"preInstalledSw\\\\\\\":\\\\\\\"SQL Web\\\\\\\",\\\\\\\"location\\\\\\\":\\\\\\\"US East (N. Virginia)\\\\\\\",\\\\\\\"processorArchitecture\\\\\\\":\\\\\\\"64-bit\\\\\\\",\\\\\\\"operation\\\\\\\":\\\\\\\"RunInstances:0200\\\\\\\"},\\\\\\\"sku\\\\\\\":\\\\\\\"5KQGU4DT589HHWZ6\\\\\\\"},\\\\\\\"serviceCode\\\\\\\":\\\\\\\"AmazonEC2\\\\\\\",\\\\\\\"terms\\\\\\\":{\\\\\\\"OnDemand\\\\\\\":{\\\\\\\"5KQGU4DT589HHWZ6.JRTCKXETXF\\\\\\\":{\\\\\\\"priceDimensions\\\\\\\":{\\\\\\\"5KQGU4DT589HHWZ6.JRTCKXETXF.6YS6EN2CT7\\\\\\\":{\\\\\\\"unit\\\\\\\":\\\\\\\"Hrs\\\\\\\",\\\\\\\"endRange\\\\\\\":\\\\\\\"Inf\\\\\\\",\\\\\\\"description\\\\\\\":\\\\\\\"$2.208 per On Demand Linux with SQL Web c4.8xlarge Instance Hour\\\\\\\",\\\\\\\"appliesTo\\\\\\\":[],\\\\\\\"rateCode\\\\\\\":\\\\\\\"5KQGU4DT589HHWZ6.JRTCKXETXF.6YS6EN2CT7\\\\\\\",\\\\\\\"beginRange\\\\\\\":\\\\\\\"0\\\\\\\",\\\\\\\"pricePerUnit\\\\\\\":{\\\\\\\"USD\\\\\\\":\\\\\\\"2.2080000000\\\\\\\"}}},\\\\\\\"sku\\\\\\\":\\\\\\\"5KQGU4DT589HHWZ6\\\\\\\",\\\\\\\"effectiveDate\\\\\\\":\\\\\\\"2018-04-01T00:00:00Z\\\\\\\",\\\\\\\"offerTermCode\\\\\\\":\\\\\\\"JRTCKXETXF\\\\\\\",\\\\\\\"termAttributes\\\\\\\":{}}},\\\\\\\"Reserved\\\\\\\":{\\\\\\\"5KQGU4DT589HHWZ6.VJWZNREJX2\\\\\\\":{\\\\\\\"priceDimensions\\\\\\\":{\\\\\\\"5KQGU4DT589HHWZ6.VJWZNREJX2.2TG2D8R56U\\\\\\\":{\\\\\\\"unit\\\\\\\":\\\\\\\"Quantity\\\\\\\",\\\\\\\"description\\\\\\\":\\\\\\\"Upfront Fee\\\\\\\",\\\\\\\"appliesTo\\\\\\\":[],\\\\\\\"rateCode\\\\\\\":\\\\\\\"5KQGU4DT589HHWZ6.VJWZNREJX2.2TG2D8R56U\\\\\\\",\\\\\\\"pricePerUnit\\\\\\\":{\\\\\\\"USD\\\\\\\":\\\\\\\"14807\\\\\\\"}},\\\\\\\"5KQGU4DT589HHWZ6.VJWZNREJX2.6YS6EN2CT7\\\\\\\":{\\\\\\\"unit\\\\\\\":\\\\\\\"Hrs\\\\\\\",\\\\\\\"endRange\\\\\\\":\\\\\\\"Inf\\\\\\\",\\\\\\\"description\\\\\\\":\\\\\\\"Linux with SQL Server Web (Amazon VPC), c4.8xlarge reserved instance applied\\\\\\\",\\\\\\\"appliesTo\\\\\\\":[],\\\\\\\"rateCode\\\\\\\":\\\\\\\"5KQGU4DT589HHWZ6.VJWZNREJX2.6YS6EN2CT7\\\\\\\",\\\\\\\"beginRange\\\\\\\":\\\\\\\"0\\\\\\\",\\\\\\\"pricePerUnit\\\\\\\":{\\\\\\\"USD\\\\\\\":\\\\\\\"0.0000000000\\\\\\\"}}},\\\\\\\"sku\\\\\\\":\\\\\\\"5KQGU4DT589HHWZ6\\\\\\\",\\\\\\\"effectiveDate\\\\\\\":\\\\\\\"2018-03-31T23:59:59Z\\\\\\\",\\\\\\\"offerTermCode\\\\\\\":\\\\\\\"VJWZNREJX2\\\\\\\",\\\\\\\"termAttributes\\\\\\\":{\\\\\\\"LeaseContractLength\\\\\\\":\\\\\\\"1yr\\\\\\\",\\\\\\\"OfferingClass\\\\\\\":\\\\\\\"convertible\\\\\\\",\\\\\\\"PurchaseOption\\\\\\\":\\\\\\\"All Upfront\\\\\\\"}},\\\\\\\"5KQGU4DT589HHWZ6.Z2E3P23VKM\\\\\\\":{\\\\\\\"priceDimensions\\\\\\\":{\\\\\\\"5KQGU4DT589HHWZ6.Z2E3P23VKM.6YS6EN2CT7\\\\\\\":{\\\\\\\"unit\\\\\\\":\\\\\\\"Hrs\\\\\\\",\\\\\\\"endRange\\\\\\\":\\\\\\\"Inf\\\\\\\",\\\\\\\"description\\\\\\\":\\\\\\\"Linux with SQL Server Web (Amazon VPC), c4.8xlarge reserved instance applied\\\\\\\",\\\\\\\"appliesTo\\\\\\\":[],\\\\\\\"rateCode\\\\\\\":\\\\\\\"5KQGU4DT589HHWZ6.Z2E3P23VKM.6YS6EN2CT7\\\\\\\",\\\\\\\"beginRange\\\\\\\":\\\\\\\"0\\\\\\\",\\\\\\\"pricePerUnit\\\\\\\":{\\\\\\\"USD\\\\\\\":\\\\\\\"1.4230000000\\\\\\\"}}},\\\\\\\"sku\\\\\\\":\\\\\\\"5KQGU4DT589HHWZ6\\\\\\\",\\\\\\\"effectiveDate\\\\\\\":\\\\\\\"2018-03-31T23:59:59Z\\\\\\\",\\\\\\\"offerTermCode\\\\\\\":\\\\\\\"Z2E3P23VKM\\\\\\\",\\\\\\\"termAttributes\\\\\\\":{\\\\\\\"LeaseContractLength\\\\\\\":\\\\\\\"3yr\\\\\\\",\\\\\\\"OfferingClass\\\\\\\":\\\\\\\"convertible\\\\\\\",\\\\\\\"PurchaseOption\\\\\\\":\\\\\\\"No Upfront\\\\\\\"}},\\\\\\\"5KQGU4DT589HHWZ6.R5XV2EPZQZ\\\\\\\":{\\\\\\\"priceDimensions\\\\\\\":{\\\\\\\"5KQGU4DT589HHWZ6.R5XV2EPZQZ.6YS6EN2CT7\\\\\\\":{\\\\\\\"unit\\\\\\\":\\\\\\\"Hrs\\\\\\\",\\\\\\\"endRange\\\\\\\":\\\\\\\"Inf\\\\\\\",\\\\\\\"description\\\\\\\":\\\\\\\"Linux with SQL Server Web (Amazon VPC), c4.8xlarge reserved instance applied\\\\\\\",\\\\\\\"appliesTo\\\\\\\":[],\\\\\\\"rateCode\\\\\\\":\\\\\\\"5KQGU4DT589HHWZ6.R5XV2EPZQZ.6YS6EN2CT7\\\\\\\",\\\\\\\"beginRange\\\\\\\":\\\\\\\"0\\\\\\\",\\\\\\\"pricePerUnit\\\\\\\":{\\\\\\\"USD\\\\\\\":\\\\\\\"0.6810000000\\\\\\\"}},\\\\\\\"5KQGU4DT589HHWZ6.R5XV2EPZQZ.2TG2D8R56U\\\\\\\":{\\\\\\\"unit\\\\\\\":\\\\\\\"Quantity\\\\\\\",\\\\\\\"description\\\\\\\":\\\\\\\"Upfront Fee\\\\\\\",\\\\\\\"appliesTo\\\\\\\":[],\\\\\\\"rateCode\\\\\\\":\\\\\\\"5KQGU4DT589HHWZ6.R5XV2EPZQZ.2TG2D8R56U\\\\\\\",\\\\\\\"pricePerUnit\\\\\\\":{\\\\\\\"USD\\\\\\\":\\\\\\\"17907\\\\\\\"}}},\\\\\\\"sku\\\\\\\":\\\\\\\"5KQGU4DT589HHWZ6\\\\\\\",\\\\\\\"effectiveDate\\\\\\\":\\\\\\\"2018-03-31T23:59:59Z\\\\\\\",\\\\\\\"offerTermCode\\\\\\\":\\\\\\\"R5XV2EPZQZ\\\\\\\",\\\\\\\"termAttributes\\\\\\\":{\\\\\\\"LeaseContractLength\\\\\\\":\\\\\\\"3yr\\\\\\\",\\\\\\\"OfferingClass\\\\\\\":\\\\\\\"convertible\\\\\\\",\\\\\\\"PurchaseOption\\\\\\\":\\\\\\\"Partial Upfront\\\\\\\"}},\\\\\\\"5KQGU4DT589HHWZ6.38NPMPTW36\\\\\\\":{\\\\\\\"priceDimensions\\\\\\\":{\\\\\\\"5KQGU4DT589HHWZ6.38NPMPTW36.6YS6EN2CT7\\\\\\\":{\\\\\\\"unit\\\\\\\":\\\\\\\"Hrs\\\\\\\",\\\\\\\"endRange\\\\\\\":\\\\\\\"Inf\\\\\\\",\\\\\\\"description\\\\\\\":\\\\\\\"Linux with SQL Server Web (Amazon VPC), c4.8xlarge reserved instance applied\\\\\\\",\\\\\\\"appliesTo\\\\\\\":[],\\\\\\\"rateCode\\\\\\\":\\\\\\\"5KQGU4DT589HHWZ6.38NPMPTW36.6YS6EN2CT7\\\\\\\",\\\\\\\"beginRange\\\\\\\":\\\\\\\"0\\\\\\\",\\\\\\\"pricePerUnit\\\\\\\":{\\\\\\\"USD\\\\\\\":\\\\\\\"0.6320000000\\\\\\\"}},\\\\\\\"5KQGU4DT589HHWZ6.38NPMPTW36.2TG2D8R56U\\\\\\\":{\\\\\\\"unit\\\\\\\":\\\\\\\"Quantity\\\\\\\",\\\\\\\"description\\\\\\\":\\\\\\\"Upfront Fee\\\\\\\",\\\\\\\"appliesTo\\\\\\\":[],\\\\\\\"rateCode\\\\\\\":\\\\\\\"5KQGU4DT589HHWZ6.38NPMPTW36.2TG2D8R56U\\\\\\\",\\\\\\\"pricePerUnit\\\\\\\":{\\\\\\\"USD\\\\\\\":\\\\\\\"16614\\\\\\\"}}},\\\\\\\"sku\\\\\\\":\\\\\\\"5KQGU4DT589HHWZ6\\\\\\\",\\\\\\\"effectiveDate\\\\\\\":\\\\\\\"2018-03-31T23:59:59Z\\\\\\\",\\\\\\\"offerTermCode\\\\\\\":\\\\\\\"38NPMPTW36\\\\\\\",\\\\\\\"termAttributes\\\\\\\":{\\\\\\\"LeaseContractLength\\\\\\\":\\\\\\\"3yr\\\\\\\",\\\\\\\"OfferingClass\\\\\\\":\\\\\\\"standard\\\\\\\",\\\\\\\"PurchaseOption\\\\\\\":\\\\\\\"Partial Upfront\\\\\\\"}},\\\\\\\"5KQGU4DT589HHWZ6.NQ3QZPMQV9\\\\\\\":{\\\\\\\"priceDimensions\\\\\\\":{\\\\\\\"5KQGU4DT589HHWZ6.NQ3QZPMQV9.6YS6EN2CT7\\\\\\\":{\\\\\\\"unit\\\\\\\":\\\\\\\"Hrs\\\\\\\",\\\\\\\"endRange\\\\\\\":\\\\\\\"Inf\\\\\\\",\\\\\\\"description\\\\\\\":\\\\\\\"Linux with SQL Server Web (Amazon VPC), c4.8xlarge reserved instance applied\\\\\\\",\\\\\\\"appliesTo\\\\\\\":[],\\\\\\\"rateCode\\\\\\\":\\\\\\\"5KQGU4DT589HHWZ6.NQ3QZPMQV9.6YS6EN2CT7\\\\\\\",\\\\\\\"beginRange\\\\\\\":\\\\\\\"0\\\\\\\",\\\\\\\"pricePerUnit\\\\\\\":{\\\\\\\"USD\\\\\\\":\\\\\\\"0.0000000000\\\\\\\"}},\\\\\\\"5KQGU4DT589HHWZ6.NQ3QZPMQV9.2TG2D8R56U\\\\\\\":{\\\\\\\"unit\\\\\\\":\\\\\\\"Quantity\\\\\\\",\\\\\\\"description\\\\\\\":\\\\\\\"Upfront Fee\\\\\\\",\\\\\\\"appliesTo\\\\\\\":[],\\\\\\\"rateCode\\\\\\\":\\\\\\\"5KQGU4DT589HHWZ6.NQ3QZPMQV9.2TG2D8R56U\\\\\\\",\\\\\\\"pricePerUnit\\\\\\\":{\\\\\\\"USD\\\\\\\":\\\\\\\"32194\\\\\\\"}}},\\\\\\\"sku\\\\\\\":\\\\\\\"5KQGU4DT589HHWZ6\\\\\\\",\\\\\\\"effectiveDate\\\\\\\":\\\\\\\"2018-03-31T23:59:59Z\\\\\\\",\\\\\\\"offerTermCode\\\\\\\":\\\\\\\"NQ3QZPMQV9\\\\\\\",\\\\\\\"termAttributes\\\\\\\":{\\\\\\\"LeaseContractLength\\\\\\\":\\\\\\\"3yr\\\\\\\",\\\\\\\"OfferingClass\\\\\\\":\\\\\\\"standard\\\\\\\",\\\\\\\"PurchaseOption\\\\\\\":\\\\\\\"All Upfront\\\\\\\"}},\\\\\\\"5KQGU4DT589HHWZ6.6QCMYABX3D\\\\\\\":{\\\\\\\"priceDimensions\\\\\\\":{\\\\\\\"5KQGU4DT589HHWZ6.6QCMYABX3D.2TG2D8R56U\\\\\\\":{\\\\\\\"unit\\\\\\\":\\\\\\\"Quantity\\\\\\\",\\\\\\\"description\\\\\\\":\\\\\\\"Upfront Fee\\\\\\\",\\\\\\\"appliesTo\\\\\\\":[],\\\\\\\"rateCode\\\\\\\":\\\\\\\"5KQGU4DT589HHWZ6.6QCMYABX3D.2TG2D8R56U\\\\\\\",\\\\\\\"pricePerUnit\\\\\\\":{\\\\\\\"USD\\\\\\\":\\\\\\\"13571\\\\\\\"}},\\\\\\\"5KQGU4DT589HHWZ6.6QCMYABX3D.6YS6EN2CT7\\\\\\\":{\\\\\\\"unit\\\\\\\":\\\\\\\"Hrs\\\\\\\",\\\\\\\"endRange\\\\\\\":\\\\\\\"Inf\\\\\\\",\\\\\\\"description\\\\\\\":\\\\\\\"Linux with SQL Server Web (Amazon VPC), c4.8xlarge reserved instance applied\\\\\\\",\\\\\\\"appliesTo\\\\\\\":[],\\\\\\\"rateCode\\\\\\\":\\\\\\\"5KQGU4DT589HHWZ6.6QCMYABX3D.6YS6EN2CT7\\\\\\\",\\\\\\\"beginRange\\\\\\\":\\\\\\\"0\\\\\\\",\\\\\\\"pricePerUnit\\\\\\\":{\\\\\\\"USD\\\\\\\":\\\\\\\"0.0000000000\\\\\\\"}}},\\\\\\\"sku\\\\\\\":\\\\\\\"5KQGU4DT589HHWZ6\\\\\\\",\\\\\\\"effectiveDate\\\\\\\":\\\\\\\"2018-03-31T23:59:59Z\\\\\\\",\\\\\\\"offerTermCode\\\\\\\":\\\\\\\"6QCMYABX3D\\\\\\\",\\\\\\\"termAttributes\\\\\\\":{\\\\\\\"LeaseContractLength\\\\\\\":\\\\\\\"1yr\\\\\\\",\\\\\\\"OfferingClass\\\\\\\":\\\\\\\"standard\\\\\\\",\\\\\\\"PurchaseOption\\\\\\\":\\\\\\\"All Upfront\\\\\\\"}},\\\\\\\"5KQGU4DT589HHWZ6.7NE97W5U4E\\\\\\\":{\\\\\\\"priceDimensions\\\\\\\":{\\\\\\\"5KQGU4DT589HHWZ6.7NE97W5U4E.6YS6EN2CT7\\\\\\\":{\\\\\\\"unit\\\\\\\":\\\\\\\"Hrs\\\\\\\",\\\\\\\"endRange\\\\\\\":\\\\\\\"Inf\\\\\\\",\\\\\\\"description\\\\\\\":\\\\\\\"Linux with SQL Server Web (Amazon VPC), c4.8xlarge reserved instance applied\\\\\\\",\\\\\\\"appliesTo\\\\\\\":[],\\\\\\\"rateCode\\\\\\\":\\\\\\\"5KQGU4DT589HHWZ6.7NE97W5U4E.6YS6EN2CT7\\\\\\\",\\\\\\\"beginRange\\\\\\\":\\\\\\\"0\\\\\\\",\\\\\\\"pricePerUnit\\\\\\\":{\\\\\\\"USD\\\\\\\":\\\\\\\"1.7680000000\\\\\\\"}}},\\\\\\\"sku\\\\\\\":\\\\\\\"5KQGU4DT589HHWZ6\\\\\\\",\\\\\\\"effectiveDate\\\\\\\":\\\\\\\"2018-03-31T23:59:59Z\\\\\\\",\\\\\\\"offerTermCode\\\\\\\":\\\\\\\"7NE97W5U4E\\\\\\\",\\\\\\\"termAttributes\\\\\\\":{\\\\\\\"LeaseContractLength\\\\\\\":\\\\\\\"1yr\\\\\\\",\\\\\\\"OfferingClass\\\\\\\":\\\\\\\"convertible\\\\\\\",\\\\\\\"PurchaseOption\\\\\\\":\\\\\\\"No Upfront\\\\\\\"}},\\\\\\\"5KQGU4DT589HHWZ6.CUZHX8X6JH\\\\\\\":{\\\\\\\"priceDimensions\\\\\\\":{\\\\\\\"5KQGU4DT589HHWZ6.CUZHX8X6JH.2TG2D8R56U\\\\\\\":{\\\\\\\"unit\\\\\\\":\\\\\\\"Quantity\\\\\\\",\\\\\\\"description\\\\\\\":\\\\\\\"Upfront Fee\\\\\\\",\\\\\\\"appliesTo\\\\\\\":[],\\\\\\\"rateCode\\\\\\\":\\\\\\\"5KQGU4DT589HHWZ6.CUZHX8X6JH.2TG2D8R56U\\\\\\\",\\\\\\\"pricePerUnit\\\\\\\":{\\\\\\\"USD\\\\\\\":\\\\\\\"7500\\\\\\\"}},\\\\\\\"5KQGU4DT589HHWZ6.CUZHX8X6JH.6YS6EN2CT7\\\\\\\":{\\\\\\\"unit\\\\\\\":\\\\\\\"Hrs\\\\\\\",\\\\\\\"endRange\\\\\\\":\\\\\\\"Inf\\\\\\\",\\\\\\\"description\\\\\\\":\\\\\\\"Linux with SQL Server Web (Amazon VPC), c4.8xlarge reserved instance applied\\\\\\\",\\\\\\\"appliesTo\\\\\\\":[],\\\\\\\"rateCode\\\\\\\":\\\\\\\"5KQGU4DT589HHWZ6.CUZHX8X6JH.6YS6EN2CT7\\\\\\\",\\\\\\\"beginRange\\\\\\\":\\\\\\\"0\\\\\\\",\\\\\\\"pricePerUnit\\\\\\\":{\\\\\\\"USD\\\\\\\":\\\\\\\"0.8560000000\\\\\\\"}}},\\\\\\\"sku\\\\\\\":\\\\\\\"5KQGU4DT589HHWZ6\\\\\\\",\\\\\\\"effectiveDate\\\\\\\":\\\\\\\"2018-03-31T23:59:59Z\\\\\\\",\\\\\\\"offerTermCode\\\\\\\":\\\\\\\"CUZHX8X6JH\\\\\\\",\\\\\\\"termAttributes\\\\\\\":{\\\\\\\"LeaseContractLength\\\\\\\":\\\\\\\"1yr\\\\\\\",\\\\\\\"OfferingClass\\\\\\\":\\\\\\\"convertible\\\\\\\",\\\\\\\"PurchaseOption\\\\\\\":\\\\\\\"Partial Upfront\\\\\\\"}},\\\\\\\"5KQGU4DT589HHWZ6.4NA7Y494T4\\\\\\\":{\\\\\\\"priceDimensions\\\\\\\":{\\\\\\\"5KQGU4DT589HHWZ6.4NA7Y494T4.6YS6EN2CT7\\\\\\\":{\\\\\\\"unit\\\\\\\":\\\\\\\"Hrs\\\\\\\",\\\\\\\"endRange\\\\\\\":\\\\\\\"Inf\\\\\\\",\\\\\\\"description\\\\\\\":\\\\\\\"Linux with SQL Server Web (Amazon VPC), c4.8xlarge reserved instance applied\\\\\\\",\\\\\\\"appliesTo\\\\\\\":[],\\\\\\\"rateCode\\\\\\\":\\\\\\\"5KQGU4DT589HHWZ6.4NA7Y494T4.6YS6EN2CT7\\\\\\\",\\\\\\\"beginRange\\\\\\\":\\\\\\\"0\\\\\\\",\\\\\\\"pricePerUnit\\\\\\\":{\\\\\\\"USD\\\\\\\":\\\\\\\"1.6160000000\\\\\\\"}}},\\\\\\\"sku\\\\\\\":\\\\\\\"5KQGU4DT589HHWZ6\\\\\\\",\\\\\\\"effectiveDate\\\\\\\":\\\\\\\"2018-03-31T23:59:59Z\\\\\\\",\\\\\\\"offerTermCode\\\\\\\":\\\\\\\"4NA7Y494T4\\\\\\\",\\\\\\\"termAttributes\\\\\\\":{\\\\\\\"LeaseContractLength\\\\\\\":\\\\\\\"1yr\\\\\\\",\\\\\\\"OfferingClass\\\\\\\":\\\\\\\"standard\\\\\\\",\\\\\\\"PurchaseOption\\\\\\\":\\\\\\\"No Upfront\\\\\\\"}},\\\\\\\"5KQGU4DT589HHWZ6.HU7G6KETJZ\\\\\\\":{\\\\\\\"priceDimensions\\\\\\\":{\\\\\\\"5KQGU4DT589HHWZ6.HU7G6KETJZ.2TG2D8R56U\\\\\\\":{\\\\\\\"unit\\\\\\\":\\\\\\\"Quantity\\\\\\\",\\\\\\\"description\\\\\\\":\\\\\\\"Upfront Fee\\\\\\\",\\\\\\\"appliesTo\\\\\\\":[],\\\\\\\"rateCode\\\\\\\":\\\\\\\"5KQGU4DT589HHWZ6.HU7G6KETJZ.2TG2D8R56U\\\\\\\",\\\\\\\"pricePerUnit\\\\\\\":{\\\\\\\"USD\\\\\\\":\\\\\\\"6870\\\\\\\"}},\\\\\\\"5KQGU4DT589HHWZ6.HU7G6KETJZ.6YS6EN2CT7\\\\\\\":{\\\\\\\"unit\\\\\\\":\\\\\\\"Hrs\\\\\\\",\\\\\\\"endRange\\\\\\\":\\\\\\\"Inf\\\\\\\",\\\\\\\"description\\\\\\\":\\\\\\\"Linux with SQL Server Web (Amazon VPC), c4.8xlarge reserved instance applied\\\\\\\",\\\\\\\"appliesTo\\\\\\\":[],\\\\\\\"rateCode\\\\\\\":\\\\\\\"5KQGU4DT589HHWZ6.HU7G6KETJZ.6YS6EN2CT7\\\\\\\",\\\\\\\"beginRange\\\\\\\":\\\\\\\"0\\\\\\\",\\\\\\\"pricePerUnit\\\\\\\":{\\\\\\\"USD\\\\\\\":\\\\\\\"0.7840000000\\\\\\\"}}},\\\\\\\"sku\\\\\\\":\\\\\\\"5KQGU4DT589HHWZ6\\\\\\\",\\\\\\\"effectiveDate\\\\\\\":\\\\\\\"2018-03-31T23:59:59Z\\\\\\\",\\\\\\\"offerTermCode\\\\\\\":\\\\\\\"HU7G6KETJZ\\\\\\\",\\\\\\\"termAttributes\\\\\\\":{\\\\\\\"LeaseContractLength\\\\\\\":\\\\\\\"1yr\\\\\\\",\\\\\\\"OfferingClass\\\\\\\":\\\\\\\"standard\\\\\\\",\\\\\\\"PurchaseOption\\\\\\\":\\\\\\\"Partial Upfront\\\\\\\"}},\\\\\\\"5KQGU4DT589HHWZ6.BPH4J8HBKS\\\\\\\":{\\\\\\\"priceDimensions\\\\\\\":{\\\\\\\"5KQGU4DT589HHWZ6.BPH4J8HBKS.6YS6EN2CT7\\\\\\\":{\\\\\\\"unit\\\\\\\":\\\\\\\"Hrs\\\\\\\",\\\\\\\"endRange\\\\\\\":\\\\\\\"Inf\\\\\\\",\\\\\\\"description\\\\\\\":\\\\\\\"Linux with SQL Server Web (Amazon VPC), c4.8xlarge reserved instance applied\\\\\\\",\\\\\\\"appliesTo\\\\\\\":[],\\\\\\\"rateCode\\\\\\\":\\\\\\\"5KQGU4DT589HHWZ6.BPH4J8HBKS.6YS6EN2CT7\\\\\\\",\\\\\\\"beginRange\\\\\\\":\\\\\\\"0\\\\\\\",\\\\\\\"pricePerUnit\\\\\\\":{\\\\\\\"USD\\\\\\\":\\\\\\\"1.3170000000\\\\\\\"}}},\\\\\\\"sku\\\\\\\":\\\\\\\"5KQGU4DT589HHWZ6\\\\\\\",\\\\\\\"effectiveDate\\\\\\\":\\\\\\\"2018-03-31T23:59:59Z\\\\\\\",\\\\\\\"offerTermCode\\\\\\\":\\\\\\\"BPH4J8HBKS\\\\\\\",\\\\\\\"termAttributes\\\\\\\":{\\\\\\\"LeaseContractLength\\\\\\\":\\\\\\\"3yr\\\\\\\",\\\\\\\"OfferingClass\\\\\\\":\\\\\\\"standard\\\\\\\",\\\\\\\"PurchaseOption\\\\\\\":\\\\\\\"No Upfront\\\\\\\"}},\\\\\\\"5KQGU4DT589HHWZ6.MZU6U2429S\\\\\\\":{\\\\\\\"priceDimensions\\\\\\\":{\\\\\\\"5KQGU4DT589HHWZ6.MZU6U2429S.2TG2D8R56U\\\\\\\":{\\\\\\\"unit\\\\\\\":\\\\\\\"Quantity\\\\\\\",\\\\\\\"description\\\\\\\":\\\\\\\"Upfront Fee\\\\\\\",\\\\\\\"appliesTo\\\\\\\":[],\\\\\\\"rateCode\\\\\\\":\\\\\\\"5KQGU4DT589HHWZ6.MZU6U2429S.2TG2D8R56U\\\\\\\",\\\\\\\"pricePerUnit\\\\\\\":{\\\\\\\"USD\\\\\\\":\\\\\\\"35418\\\\\\\"}},\\\\\\\"5KQGU4DT589HHWZ6.MZU6U2429S.6YS6EN2CT7\\\\\\\":{\\\\\\\"unit\\\\\\\":\\\\\\\"Hrs\\\\\\\",\\\\\\\"endRange\\\\\\\":\\\\\\\"Inf\\\\\\\",\\\\\\\"description\\\\\\\":\\\\\\\"Linux with SQL Server Web (Amazon VPC), c4.8xlarge reserved instance applied\\\\\\\",\\\\\\\"appliesTo\\\\\\\":[],\\\\\\\"rateCode\\\\\\\":\\\\\\\"5KQGU4DT589HHWZ6.MZU6U2429S.6YS6EN2CT7\\\\\\\",\\\\\\\"beginRange\\\\\\\":\\\\\\\"0\\\\\\\",\\\\\\\"pricePerUnit\\\\\\\":{\\\\\\\"USD\\\\\\\":\\\\\\\"0.0000000000\\\\\\\"}}},\\\\\\\"sku\\\\\\\":\\\\\\\"5KQGU4DT589HHWZ6\\\\\\\",\\\\\\\"effectiveDate\\\\\\\":\\\\\\\"2018-03-31T23:59:59Z\\\\\\\",\\\\\\\"offerTermCode\\\\\\\":\\\\\\\"MZU6U2429S\\\\\\\",\\\\\\\"termAttributes\\\\\\\":{\\\\\\\"LeaseContractLength\\\\\\\":\\\\\\\"3yr\\\\\\\",\\\\\\\"OfferingClass\\\\\\\":\\\\\\\"convertible\\\\\\\",\\\\\\\"PurchaseOption\\\\\\\":\\\\\\\"All Upfront\\\\\\\"}}}},\\\\\\\"version\\\\\\\":\\\\\\\"20180502184240\\\\\\\",\\\\\\\"publicationDate\\\\\\\":\\\\\\\"2018-05-02T18:42:40Z\\\\\\\"}\\\",\\\"{\\\\\\\"product\\\\\\\":{\\\\\\\"productFamily\\\\\\\":\\\\\\\"Compute Instance\\\\\\\",\\\\\\\"attributes\\\\\\\":{\\\\\\\"enhancedNetworkingSupported\\\\\\\":\\\\\\\"Yes\\\\\\\",\\\\\\\"memory\\\\\\\":\\\\\\\"60 GiB\\\\\\\",\\\\\\\"dedicatedEbsThroughput\\\\\\\":\\\\\\\"4000 Mbps\\\\\\\",\\\\\\\"vcpu\\\\\\\":\\\\\\\"36\\\\\\\",\\\\\\\"capacitystatus\\\\\\\":\\\\\\\"Used\\\\\\\",\\\\\\\"locationType\\\\\\\":\\\\\\\"AWS Region\\\\\\\",\\\\\\\"storage\\\\\\\":\\\\\\\"EBS only\\\\\\\",\\\\\\\"instanceFamily\\\\\\\":\\\\\\\"Compute optimized\\\\\\\",\\\\\\\"operatingSystem\\\\\\\":\\\\\\\"Linux\\\\\\\",\\\\\\\"physicalProcessor\\\\\\\":\\\\\\\"Intel Xeon E5-2666 v3 (Haswell)\\\\\\\",\\\\\\\"clockSpeed\\\\\\\":\\\\\\\"2.9 GHz\\\\\\\",\\\\\\\"ecu\\\\\\\":\\\\\\\"132\\\\\\\",\\\\\\\"networkPerformance\\\\\\\":\\\\\\\"10 Gigabit\\\\\\\",\\\\\\\"servicename\\\\\\\":\\\\\\\"Amazon Elastic Compute Cloud\\\\\\\",\\\\\\\"instanceType\\\\\\\":\\\\\\\"c4.8xlarge\\\\\\\",\\\\\\\"tenancy\\\\\\\":\\\\\\\"Shared\\\\\\\",\\\\\\\"usagetype\\\\\\\":\\\\\\\"BoxUsage:c4.8xlarge\\\\\\\",\\\\\\\"normalizationSizeFactor\\\\\\\":\\\\\\\"64\\\\\\\",\\\\\\\"processorFeatures\\\\\\\":\\\\\\\"Intel AVX; Intel AVX2; Intel Turbo\\\\\\\",\\\\\\\"servicecode\\\\\\\":\\\\\\\"AmazonEC2\\\\\\\",\\\\\\\"licenseModel\\\\\\\":\\\\\\\"No License required\\\\\\\",\\\\\\\"currentGeneration\\\\\\\":\\\\\\\"Yes\\\\\\\",\\\\\\\"preInstalledSw\\\\\\\":\\\\\\\"NA\\\\\\\",\\\\\\\"location\\\\\\\":\\\\\\\"US East (N. Virginia)\\\\\\\",\\\\\\\"processorArchitecture\\\\\\\":\\\\\\\"64-bit\\\\\\\",\\\\\\\"operation\\\\\\\":\\\\\\\"RunInstances\\\\\\\"},\\\\\\\"sku\\\\\\\":\\\\\\\"9G23QA9CK3NU3BRY\\\\\\\"},\\\\\\\"serviceCode\\\\\\\":\\\\\\\"AmazonEC2\\\\\\\",\\\\\\\"terms\\\\\\\":{\\\\\\\"OnDemand\\\\\\\":{\\\\\\\"9G23QA9CK3NU3BRY.JRTCKXETXF\\\\\\\":{\\\\\\\"priceDimensions\\\\\\\":{\\\\\\\"9G23QA9CK3NU3BRY.JRTCKXETXF.6YS6EN2CT7\\\\\\\":{\\\\\\\"unit\\\\\\\":\\\\\\\"Hrs\\\\\\\",\\\\\\\"endRange\\\\\\\":\\\\\\\"Inf\\\\\\\",\\\\\\\"description\\\\\\\":\\\\\\\"$1.591 per On Demand Linux c4.8xlarge Instance Hour\\\\\\\",\\\\\\\"appliesTo\\\\\\\":[],\\\\\\\"rateCode\\\\\\\":\\\\\\\"9G23QA9CK3NU3BRY.JRTCKXETXF.6YS6EN2CT7\\\\\\\",\\\\\\\"beginRange\\\\\\\":\\\\\\\"0\\\\\\\",\\\\\\\"pricePerUnit\\\\\\\":{\\\\\\\"USD\\\\\\\":\\\\\\\"1.5910000000\\\\\\\"}}},\\\\\\\"sku\\\\\\\":\\\\\\\"9G23QA9CK3NU3BRY\\\\\\\",\\\\\\\"effectiveDate\\\\\\\":\\\\\\\"2018-04-01T00:00:00Z\\\\\\\",\\\\\\\"offerTermCode\\\\\\\":\\\\\\\"JRTCKXETXF\\\\\\\",\\\\\\\"termAttributes\\\\\\\":{}}},\\\\\\\"Reserved\\\\\\\":{\\\\\\\"9G23QA9CK3NU3BRY.BPH4J8HBKS\\\\\\\":{\\\\\\\"priceDimensions\\\\\\\":{\\\\\\\"9G23QA9CK3NU3BRY.BPH4J8HBKS.6YS6EN2CT7\\\\\\\":{\\\\\\\"unit\\\\\\\":\\\\\\\"Hrs\\\\\\\",\\\\\\\"endRange\\\\\\\":\\\\\\\"Inf\\\\\\\",\\\\\\\"description\\\\\\\":\\\\\\\"Linux/UNIX (Amazon VPC), c4.8xlarge reserved instance applied\\\\\\\",\\\\\\\"appliesTo\\\\\\\":[],\\\\\\\"rateCode\\\\\\\":\\\\\\\"9G23QA9CK3NU3BRY.BPH4J8HBKS.6YS6EN2CT7\\\\\\\",\\\\\\\"beginRange\\\\\\\":\\\\\\\"0\\\\\\\",\\\\\\\"pricePerUnit\\\\\\\":{\\\\\\\"USD\\\\\\\":\\\\\\\"0.7080000000\\\\\\\"}}},\\\\\\\"sku\\\\\\\":\\\\\\\"9G23QA9CK3NU3BRY\\\\\\\",\\\\\\\"effectiveDate\\\\\\\":\\\\\\\"2017-04-30T23:59:59Z\\\\\\\",\\\\\\\"offerTermCode\\\\\\\":\\\\\\\"BPH4J8HBKS\\\\\\\",\\\\\\\"termAttributes\\\\\\\":{\\\\\\\"LeaseContractLength\\\\\\\":\\\\\\\"3yr\\\\\\\",\\\\\\\"OfferingClass\\\\\\\":\\\\\\\"standard\\\\\\\",\\\\\\\"PurchaseOption\\\\\\\":\\\\\\\"No Upfront\\\\\\\"}},\\\\\\\"9G23QA9CK3NU3BRY.Z2E3P23VKM\\\\\\\":{\\\\\\\"priceDimensions\\\\\\\":{\\\\\\\"9G23QA9CK3NU3BRY.Z2E3P23VKM.6YS6EN2CT7\\\\\\\":{\\\\\\\"unit\\\\\\\":\\\\\\\"Hrs\\\\\\\",\\\\\\\"endRange\\\\\\\":\\\\\\\"Inf\\\\\\\",\\\\\\\"description\\\\\\\":\\\\\\\"Linux/UNIX (Amazon VPC), c4.8xlarge reserved instance applied\\\\\\\",\\\\\\\"appliesTo\\\\\\\":[],\\\\\\\"rateCode\\\\\\\":\\\\\\\"9G23QA9CK3NU3BRY.Z2E3P23VKM.6YS6EN2CT7\\\\\\\",\\\\\\\"beginRange\\\\\\\":\\\\\\\"0\\\\\\\",\\\\\\\"pricePerUnit\\\\\\\":{\\\\\\\"USD\\\\\\\":\\\\\\\"0.8150000000\\\\\\\"}}},\\\\\\\"sku\\\\\\\":\\\\\\\"9G23QA9CK3NU3BRY\\\\\\\",\\\\\\\"effectiveDate\\\\\\\":\\\\\\\"2017-04-30T23:59:59Z\\\\\\\",\\\\\\\"offerTermCode\\\\\\\":\\\\\\\"Z2E3P23VKM\\\\\\\",\\\\\\\"termAttributes\\\\\\\":{\\\\\\\"LeaseContractLength\\\\\\\":\\\\\\\"3yr\\\\\\\",\\\\\\\"OfferingClass\\\\\\\":\\\\\\\"convertible\\\\\\\",\\\\\\\"PurchaseOption\\\\\\\":\\\\\\\"No Upfront\\\\\\\"}},\\\\\\\"9G23QA9CK3NU3BRY.R5XV2EPZQZ\\\\\\\":{\\\\\\\"priceDimensions\\\\\\\":{\\\\\\\"9G23QA9CK3NU3BRY.R5XV2EPZQZ.6YS6EN2CT7\\\\\\\":{\\\\\\\"unit\\\\\\\":\\\\\\\"Hrs\\\\\\\",\\\\\\\"endRange\\\\\\\":\\\\\\\"Inf\\\\\\\",\\\\\\\"description\\\\\\\":\\\\\\\"Linux/UNIX (Amazon VPC), c4.8xlarge reserved instance applied\\\\\\\",\\\\\\\"appliesTo\\\\\\\":[],\\\\\\\"rateCode\\\\\\\":\\\\\\\"9G23QA9CK3NU3BRY.R5XV2EPZQZ.6YS6EN2CT7\\\\\\\",\\\\\\\"beginRange\\\\\\\":\\\\\\\"0\\\\\\\",\\\\\\\"pricePerUnit\\\\\\\":{\\\\\\\"USD\\\\\\\":\\\\\\\"0.3770000000\\\\\\\"}},\\\\\\\"9G23QA9CK3NU3BRY.R5XV2EPZQZ.2TG2D8R56U\\\\\\\":{\\\\\\\"unit\\\\\\\":\\\\\\\"Quantity\\\\\\\",\\\\\\\"description\\\\\\\":\\\\\\\"Upfront Fee\\\\\\\",\\\\\\\"appliesTo\\\\\\\":[],\\\\\\\"rateCode\\\\\\\":\\\\\\\"9G23QA9CK3NU3BRY.R5XV2EPZQZ.2TG2D8R56U\\\\\\\",\\\\\\\"pricePerUnit\\\\\\\":{\\\\\\\"USD\\\\\\\":\\\\\\\"9913\\\\\\\"}}},\\\\\\\"sku\\\\\\\":\\\\\\\"9G23QA9CK3NU3BRY\\\\\\\",\\\\\\\"effectiveDate\\\\\\\":\\\\\\\"2017-04-30T23:59:59Z\\\\\\\",\\\\\\\"offerTermCode\\\\\\\":\\\\\\\"R5XV2EPZQZ\\\\\\\",\\\\\\\"termAttributes\\\\\\\":{\\\\\\\"LeaseContractLength\\\\\\\":\\\\\\\"3yr\\\\\\\",\\\\\\\"OfferingClass\\\\\\\":\\\\\\\"convertible\\\\\\\",\\\\\\\"PurchaseOption\\\\\\\":\\\\\\\"Partial Upfront\\\\\\\"}},\\\\\\\"9G23QA9CK3NU3BRY.NQ3QZPMQV9\\\\\\\":{\\\\\\\"priceDimensions\\\\\\\":{\\\\\\\"9G23QA9CK3NU3BRY.NQ3QZPMQV9.2TG2D8R56U\\\\\\\":{\\\\\\\"unit\\\\\\\":\\\\\\\"Quantity\\\\\\\",\\\\\\\"description\\\\\\\":\\\\\\\"Upfront Fee\\\\\\\",\\\\\\\"appliesTo\\\\\\\":[],\\\\\\\"rateCode\\\\\\\":\\\\\\\"9G23QA9CK3NU3BRY.NQ3QZPMQV9.2TG2D8R56U\\\\\\\",\\\\\\\"pricePerUnit\\\\\\\":{\\\\\\\"USD\\\\\\\":\\\\\\\"16329\\\\\\\"}},\\\\\\\"9G23QA9CK3NU3BRY.NQ3QZPMQV9.6YS6EN2CT7\\\\\\\":{\\\\\\\"unit\\\\\\\":\\\\\\\"Hrs\\\\\\\",\\\\\\\"endRange\\\\\\\":\\\\\\\"Inf\\\\\\\",\\\\\\\"description\\\\\\\":\\\\\\\"Linux/UNIX (Amazon VPC), c4.8xlarge reserved instance applied\\\\\\\",\\\\\\\"appliesTo\\\\\\\":[],\\\\\\\"rateCode\\\\\\\":\\\\\\\"9G23QA9CK3NU3BRY.NQ3QZPMQV9.6YS6EN2CT7\\\\\\\",\\\\\\\"beginRange\\\\\\\":\\\\\\\"0\\\\\\\",\\\\\\\"pricePerUnit\\\\\\\":{\\\\\\\"USD\\\\\\\":\\\\\\\"0.0000000000\\\\\\\"}}},\\\\\\\"sku\\\\\\\":\\\\\\\"9G23QA9CK3NU3BRY\\\\\\\",\\\\\\\"effectiveDate\\\\\\\":\\\\\\\"2016-11-30T23:59:59Z\\\\\\\",\\\\\\\"offerTermCode\\\\\\\":\\\\\\\"NQ3QZPMQV9\\\\\\\",\\\\\\\"termAttributes\\\\\\\":{\\\\\\\"LeaseContractLength\\\\\\\":\\\\\\\"3yr\\\\\\\",\\\\\\\"OfferingClass\\\\\\\":\\\\\\\"standard\\\\\\\",\\\\\\\"PurchaseOption\\\\\\\":\\\\\\\"All Upfront\\\\\\\"}},\\\\\\\"9G23QA9CK3NU3BRY.MZU6U2429S\\\\\\\":{\\\\\\\"priceDimensions\\\\\\\":{\\\\\\\"9G23QA9CK3NU3BRY.MZU6U2429S.2TG2D8R56U\\\\\\\":{\\\\\\\"unit\\\\\\\":\\\\\\\"Quantity\\\\\\\",\\\\\\\"description\\\\\\\":\\\\\\\"Upfront Fee\\\\\\\",\\\\\\\"appliesTo\\\\\\\":[],\\\\\\\"rateCode\\\\\\\":\\\\\\\"9G23QA9CK3NU3BRY.MZU6U2429S.2TG2D8R56U\\\\\\\",\\\\\\\"pricePerUnit\\\\\\\":{\\\\\\\"USD\\\\\\\":\\\\\\\"19429\\\\\\\"}},\\\\\\\"9G23QA9CK3NU3BRY.MZU6U2429S.6YS6EN2CT7\\\\\\\":{\\\\\\\"unit\\\\\\\":\\\\\\\"Hrs\\\\\\\",\\\\\\\"endRange\\\\\\\":\\\\\\\"Inf\\\\\\\",\\\\\\\"description\\\\\\\":\\\\\\\"Linux/UNIX (Amazon VPC), c4.8xlarge reserved instance applied\\\\\\\",\\\\\\\"appliesTo\\\\\\\":[],\\\\\\\"rateCode\\\\\\\":\\\\\\\"9G23QA9CK3NU3BRY.MZU6U2429S.6YS6EN2CT7\\\\\\\",\\\\\\\"beginRange\\\\\\\":\\\\\\\"0\\\\\\\",\\\\\\\"pricePerUnit\\\\\\\":{\\\\\\\"USD\\\\\\\":\\\\\\\"0.0000000000\\\\\\\"}}},\\\\\\\"sku\\\\\\\":\\\\\\\"9G23QA9CK3NU3BRY\\\\\\\",\\\\\\\"effectiveDate\\\\\\\":\\\\\\\"2017-04-30T23:59:59Z\\\\\\\",\\\\\\\"offerTermCode\\\\\\\":\\\\\\\"MZU6U2429S\\\\\\\",\\\\\\\"termAttributes\\\\\\\":{\\\\\\\"LeaseContractLength\\\\\\\":\\\\\\\"3yr\\\\\\\",\\\\\\\"OfferingClass\\\\\\\":\\\\\\\"convertible\\\\\\\",\\\\\\\"PurchaseOption\\\\\\\":\\\\\\\"All Upfront\\\\\\\"}},\\\\\\\"9G23QA9CK3NU3BRY.VJWZNREJX2\\\\\\\":{\\\\\\\"priceDimensions\\\\\\\":{\\\\\\\"9G23QA9CK3NU3BRY.VJWZNREJX2.2TG2D8R56U\\\\\\\":{\\\\\\\"unit\\\\\\\":\\\\\\\"Quantity\\\\\\\",\\\\\\\"description\\\\\\\":\\\\\\\"Upfront Fee\\\\\\\",\\\\\\\"appliesTo\\\\\\\":[],\\\\\\\"rateCode\\\\\\\":\\\\\\\"9G23QA9CK3NU3BRY.VJWZNREJX2.2TG2D8R56U\\\\\\\",\\\\\\\"pricePerUnit\\\\\\\":{\\\\\\\"USD\\\\\\\":\\\\\\\"9478\\\\\\\"}},\\\\\\\"9G23QA9CK3NU3BRY.VJWZNREJX2.6YS6EN2CT7\\\\\\\":{\\\\\\\"unit\\\\\\\":\\\\\\\"Hrs\\\\\\\",\\\\\\\"endRange\\\\\\\":\\\\\\\"Inf\\\\\\\",\\\\\\\"description\\\\\\\":\\\\\\\"Linux/UNIX (Amazon VPC), c4.8xlarge reserved instance applied\\\\\\\",\\\\\\\"appliesTo\\\\\\\":[],\\\\\\\"rateCode\\\\\\\":\\\\\\\"9G23QA9CK3NU3BRY.VJWZNREJX2.6YS6EN2CT7\\\\\\\",\\\\\\\"beginRange\\\\\\\":\\\\\\\"0\\\\\\\",\\\\\\\"pricePerUnit\\\\\\\":{\\\\\\\"USD\\\\\\\":\\\\\\\"0.0000000000\\\\\\\"}}},\\\\\\\"sku\\\\\\\":\\\\\\\"9G23QA9CK3NU3BRY\\\\\\\",\\\\\\\"effectiveDate\\\\\\\":\\\\\\\"2017-10-31T23:59:59Z\\\\\\\",\\\\\\\"offerTermCode\\\\\\\":\\\\\\\"VJWZNREJX2\\\\\\\",\\\\\\\"termAttributes\\\\\\\":{\\\\\\\"LeaseContractLength\\\\\\\":\\\\\\\"1yr\\\\\\\",\\\\\\\"OfferingClass\\\\\\\":\\\\\\\"conve\"\n-> \"rtible\\\\\\\",\\\\\\\"PurchaseOption\\\\\\\":\\\\\\\"All Upfront\\\\\\\"}},\\\\\\\"9G23QA9CK3NU3BRY.HU7G6KETJZ\\\\\\\":{\\\\\\\"priceDimensions\\\\\\\":{\\\\\\\"9G23QA9CK3NU3BRY.HU7G6KETJZ.2TG2D8R56U\\\\\\\":{\\\\\\\"unit\\\\\\\":\\\\\\\"Quantity\\\\\\\",\\\\\\\"description\\\\\\\":\\\\\\\"Upfront Fee\\\\\\\",\\\\\\\"appliesTo\\\\\\\":[],\\\\\\\"rateCode\\\\\\\":\\\\\\\"9G23QA9CK3NU3BRY.HU7G6KETJZ.2TG2D8R56U\\\\\\\",\\\\\\\"pricePerUnit\\\\\\\":{\\\\\\\"USD\\\\\\\":\\\\\\\"4231\\\\\\\"}},\\\\\\\"9G23QA9CK3NU3BRY.HU7G6KETJZ.6YS6EN2CT7\\\\\\\":{\\\\\\\"unit\\\\\\\":\\\\\\\"Hrs\\\\\\\",\\\\\\\"endRange\\\\\\\":\\\\\\\"Inf\\\\\\\",\\\\\\\"description\\\\\\\":\\\\\\\"Linux/UNIX (Amazon VPC), c4.8xlarge reserved instance applied\\\\\\\",\\\\\\\"appliesTo\\\\\\\":[],\\\\\\\"rateCode\\\\\\\":\\\\\\\"9G23QA9CK3NU3BRY.HU7G6KETJZ.6YS6EN2CT7\\\\\\\",\\\\\\\"beginRange\\\\\\\":\\\\\\\"0\\\\\\\",\\\\\\\"pricePerUnit\\\\\\\":{\\\\\\\"USD\\\\\\\":\\\\\\\"0.4830000000\\\\\\\"}}},\\\\\\\"sku\\\\\\\":\\\\\\\"9G23QA9CK3NU3BRY\\\\\\\",\\\\\\\"effectiveDate\\\\\\\":\\\\\\\"2016-11-30T23:59:59Z\\\\\\\",\\\\\\\"offerTermCode\\\\\\\":\\\\\\\"HU7G6KETJZ\\\\\\\",\\\\\\\"termAttributes\\\\\\\":{\\\\\\\"LeaseContractLength\\\\\\\":\\\\\\\"1yr\\\\\\\",\\\\\\\"OfferingClass\\\\\\\":\\\\\\\"standard\\\\\\\",\\\\\\\"PurchaseOption\\\\\\\":\\\\\\\"Partial Upfront\\\\\\\"}},\\\\\\\"9G23\"\n-> \"QA9CK3NU3BRY.4NA7Y494T4\\\\\\\":{\\\\\\\"priceDimensions\\\\\\\":{\\\\\\\"9G23QA9CK3NU3BRY.4NA7Y494T4.6YS6EN2CT7\\\\\\\":{\\\\\\\"unit\\\\\\\":\\\\\\\"Hrs\\\\\\\",\\\\\\\"endRange\\\\\\\":\\\\\\\"Inf\\\\\\\",\\\\\\\"description\\\\\\\":\\\\\\\"Linux/UNIX (Amazon VPC), c4.8xlarge reserved instance applied\\\\\\\",\\\\\\\"appliesTo\\\\\\\":[],\\\\\\\"rateCode\\\\\\\":\\\\\\\"9G23QA9CK3NU3BRY.4NA7Y494T4.6YS6EN2CT7\\\\\\\",\\\\\\\"beginRange\\\\\\\":\\\\\\\"0\\\\\\\",\\\\\\\"pricePerUnit\\\\\\\":{\\\\\\\"USD\\\\\\\":\\\\\\\"1.0080000000\\\\\\\"}}},\\\\\\\"sku\\\\\\\":\\\\\\\"9G23QA9CK3NU3BRY\\\\\\\",\\\\\\\"effectiveDate\\\\\\\":\\\\\\\"2017-04-30T23:59:59Z\\\\\\\",\\\\\\\"offerTermCode\\\\\\\":\\\\\\\"4NA7Y494T4\\\\\\\",\\\\\\\"termAttributes\\\\\\\":{\\\\\\\"LeaseContractLength\\\\\\\":\\\\\\\"1yr\\\\\\\",\\\\\\\"OfferingClass\\\\\\\":\\\\\\\"standard\\\\\\\",\\\\\\\"PurchaseOption\\\\\\\":\\\\\\\"No Upfront\\\\\\\"}},\\\\\\\"9G23QA9CK3NU3BRY.7NE97W5U4E\\\\\\\":{\\\\\\\"priceDimensions\\\\\\\":{\\\\\\\"9G23QA9CK3NU3BRY.7NE97W5U4E.6YS6EN2CT7\\\\\\\":{\\\\\\\"unit\\\\\\\":\\\\\\\"Hrs\\\\\\\",\\\\\\\"endRange\\\\\\\":\\\\\\\"Inf\\\\\\\",\\\\\\\"description\\\\\\\":\\\\\\\"Linux/UNIX (Amazon VPC), c4.8xlarge reserved instance applied\\\\\\\",\\\\\\\"appliesTo\\\\\\\":[],\\\\\\\"rateCode\\\\\\\":\\\\\\\"9G23QA9CK3NU3BRY.7NE97W5U4E.6YS6EN2CT7\\\\\\\",\\\\\\\"beginRange\\\\\\\":\\\\\\\"0\\\\\\\",\\\\\\\"pricePerUnit\\\\\\\":{\\\\\\\"USD\\\\\\\":\\\\\\\"1.1590000000\\\\\\\"}}},\\\\\\\"sku\\\\\\\":\\\\\\\"9G23QA9CK3NU3BRY\\\\\\\",\\\\\\\"effectiveDate\\\\\\\":\\\\\\\"2017-10-31T23:59:59Z\\\\\\\",\\\\\\\"offerTermCode\\\\\\\":\\\\\\\"7NE97W5U4E\\\\\\\",\\\\\\\"termAttributes\\\\\\\":{\\\\\\\"LeaseContractLength\\\\\\\":\\\\\\\"1yr\\\\\\\",\\\\\\\"OfferingClass\\\\\\\":\\\\\\\"convertible\\\\\\\",\\\\\\\"PurchaseOption\\\\\\\":\\\\\\\"No Upfront\\\\\\\"}},\\\\\\\"9G23QA9CK3NU3BRY.CUZHX8X6JH\\\\\\\":{\\\\\\\"priceDimensions\\\\\\\":{\\\\\\\"9G23QA9CK3NU3BRY.CUZHX8X6JH.2TG2D8R56U\\\\\\\":{\\\\\\\"unit\\\\\\\":\\\\\\\"Quantity\\\\\\\",\\\\\\\"description\\\\\\\":\\\\\\\"Upfront Fee\\\\\\\",\\\\\\\"appliesTo\\\\\\\":[],\\\\\\\"rateCode\\\\\\\":\\\\\\\"9G23QA9CK3NU3BRY.CUZHX8X6JH.2TG2D8R56U\\\\\\\",\\\\\\\"pricePerUnit\\\\\\\":{\\\\\\\"USD\\\\\\\":\\\\\\\"4836\\\\\\\"}},\\\\\\\"9G23QA9CK3NU3BRY.CUZHX8X6JH.6YS6EN2CT7\\\\\\\":{\\\\\\\"unit\\\\\\\":\\\\\\\"Hrs\\\\\\\",\\\\\\\"endRange\\\\\\\":\\\\\\\"Inf\\\\\\\",\\\\\\\"description\\\\\\\":\\\\\\\"Linux/UNIX (Amazon VPC), c4.8xlarge reserved instance applied\\\\\\\",\\\\\\\"appliesTo\\\\\\\":[],\\\\\\\"rateCode\\\\\\\":\\\\\\\"9G23QA9CK3NU3BRY.CUZHX8X6JH.6YS6EN2CT7\\\\\\\",\\\\\\\"beginRange\\\\\\\":\\\\\\\"0\\\\\\\",\\\\\\\"pricePerUnit\\\\\\\":{\\\\\\\"USD\\\\\\\":\\\\\\\"0.5520000000\\\\\\\"}}},\\\\\\\"sku\\\\\\\":\\\\\\\"9G23QA9CK3NU3BRY\\\\\\\",\\\\\\\"effectiveDate\\\\\\\":\\\\\\\"2017-10-31T23:59:59Z\\\\\\\",\\\\\\\"offerTermCode\\\\\\\":\\\\\\\"CUZHX8X6JH\\\\\\\",\\\\\\\"termAttributes\\\\\\\":{\\\\\\\"LeaseContractLength\\\\\\\":\\\\\\\"1yr\\\\\\\",\\\\\\\"OfferingClass\\\\\\\":\\\\\\\"convertible\\\\\\\",\\\\\\\"PurchaseOption\\\\\\\":\\\\\\\"Partial Upfront\\\\\\\"}},\\\\\\\"9G23QA9CK3NU3BRY.6QCMYABX3D\\\\\\\":{\\\\\\\"priceDimensions\\\\\\\":{\\\\\\\"9G23QA9CK3NU3BRY.6QCMYABX3D.2TG2D8R56U\\\\\\\":{\\\\\\\"unit\\\\\\\":\\\\\\\"Quantity\\\\\\\",\\\\\\\"description\\\\\\\":\\\\\\\"Upfront Fee\\\\\\\",\\\\\\\"appliesTo\\\\\\\":[],\\\\\\\"rateCode\\\\\\\":\\\\\\\"9G23QA9CK3NU3BRY.6QCMYABX3D.2TG2D8R56U\\\\\\\",\\\\\\\"pricePerUnit\\\\\\\":{\\\\\\\"USD\\\\\\\":\\\\\\\"8293\\\\\\\"}},\\\\\\\"9G23QA9CK3NU3BRY.6QCMYABX3D.6YS6EN2CT7\\\\\\\":{\\\\\\\"unit\\\\\\\":\\\\\\\"Hrs\\\\\\\",\\\\\\\"endRange\\\\\\\":\\\\\\\"Inf\\\\\\\",\\\\\\\"description\\\\\\\":\\\\\\\"Linux/UNIX (Amazon VPC), c4.8xlarge reserved instance applied\\\\\\\",\\\\\\\"appliesTo\\\\\\\":[],\\\\\\\"rateCode\\\\\\\":\\\\\\\"9G23QA9CK3NU3BRY.6QCMYABX3D.6YS6EN2CT7\\\\\\\",\\\\\\\"beginRange\\\\\\\":\\\\\\\"0\\\\\\\",\\\\\\\"pricePerUnit\\\\\\\":{\\\\\\\"USD\\\\\\\":\\\\\\\"0.0000000000\\\\\\\"}}},\\\\\\\"sku\\\\\\\":\\\\\\\"9G23QA9CK3NU3BRY\\\\\\\",\\\\\\\"effectiveDate\\\\\\\":\\\\\\\"2016-11-30T23:59:59Z\\\\\\\",\\\\\\\"offerTermCode\\\\\\\":\\\\\\\"6QCMYABX3D\\\\\\\",\\\\\\\"termAttributes\\\\\\\":{\\\\\\\"LeaseContractLength\\\\\\\":\\\\\\\"1yr\\\\\\\",\\\\\\\"OfferingClass\\\\\\\":\\\\\\\"standard\\\\\\\",\\\\\\\"PurchaseOption\\\\\\\":\\\\\\\"All Upfront\\\\\\\"}},\\\\\\\"9G23QA9CK3NU3BRY.38NPMPTW36\\\\\\\":{\\\\\\\"priceDimensions\\\\\\\":{\\\\\\\"9G23QA9CK3NU3BRY.38NPMPTW36.6YS6EN2CT7\\\\\\\":{\\\\\\\"unit\\\\\\\":\\\\\\\"Hrs\\\\\\\",\\\\\\\"endRange\\\\\\\":\\\\\\\"Inf\\\\\\\",\\\\\\\"description\\\\\\\":\\\\\\\"Linux/UNIX (Amazon VPC), c4.8xlarge reserved instance applied\\\\\\\",\\\\\\\"appliesTo\\\\\\\":[],\\\\\\\"rateCode\\\\\\\":\\\\\\\"9G23QA9CK3NU3BRY.38NPMPTW36.6YS6EN2CT7\\\\\\\",\\\\\\\"beginRange\\\\\\\":\\\\\\\"0\\\\\\\",\\\\\\\"pricePerUnit\\\\\\\":{\\\\\\\"USD\\\\\\\":\\\\\\\"0.3300000000\\\\\\\"}},\\\\\\\"9G23QA9CK3NU3BRY.38NPMPTW36.2TG2D8R56U\\\\\\\":{\\\\\\\"unit\\\\\\\":\\\\\\\"Quantity\\\\\\\",\\\\\\\"description\\\\\\\":\\\\\\\"Upfront Fee\\\\\\\",\\\\\\\"appliesTo\\\\\\\":[],\\\\\\\"rateCode\\\\\\\":\\\\\\\"9G23QA9CK3NU3BRY.38NPMPTW36.2TG2D8R56U\\\\\\\",\\\\\\\"pricePerUnit\\\\\\\":{\\\\\\\"USD\\\\\\\":\\\\\\\"8686\\\\\\\"}}},\\\\\\\"sku\\\\\\\":\\\\\\\"9G23QA9CK3NU3BRY\\\\\\\",\\\\\\\"effectiveDate\\\\\\\":\\\\\\\"2016-11-30T23:59:59Z\\\\\\\",\\\\\\\"offerTermCode\\\\\\\":\\\\\\\"38NPMPTW36\\\\\\\",\\\\\\\"termAttributes\\\\\\\":{\\\\\\\"LeaseContractLength\\\\\\\":\\\\\\\"3yr\\\\\\\",\\\\\\\"OfferingClass\\\\\\\":\\\\\\\"standard\\\\\\\",\\\\\\\"PurchaseOption\\\\\\\":\\\\\\\"Partial Upfront\\\\\\\"}}}},\\\\\\\"version\\\\\\\":\\\\\\\"20180502184240\\\\\\\",\\\\\\\"publicationDate\\\\\\\":\\\\\\\"2018-05-02T18:42:40Z\\\\\\\"}\\\",\\\"{\\\\\\\"product\\\\\\\":{\\\\\\\"productFamily\\\\\\\":\\\\\\\"Compute Instance\\\\\\\",\\\\\\\"attributes\\\\\\\":{\\\\\\\"enhancedNetworkingSupported\\\\\\\":\\\\\\\"Yes\\\\\\\",\\\\\\\"memory\\\\\\\":\\\\\\\"60 GiB\\\\\\\",\\\\\\\"dedicatedEbsThroughput\\\\\\\":\\\\\\\"4000 Mbps\\\\\\\",\\\\\\\"vcpu\\\\\\\":\\\\\\\"36\\\\\\\",\\\\\\\"capacitystatus\\\\\\\":\\\\\\\"Used\\\\\\\",\\\\\\\"locationType\\\\\\\":\\\\\\\"AWS Region\\\\\\\",\\\\\\\"storage\\\\\\\":\\\\\\\"EBS only\\\\\\\",\\\\\\\"instanceFamily\\\\\\\":\\\\\\\"Compute optimized\\\\\\\",\\\\\\\"operatingSystem\\\\\\\":\\\\\\\"Linux\\\\\\\",\\\\\\\"physicalProcessor\\\\\\\":\\\\\\\"Intel Xeon E5-2666 v3 (Haswell)\\\\\\\",\\\\\\\"clockSpeed\\\\\\\":\\\\\\\"2.9 GHz\\\\\\\",\\\\\\\"ecu\\\\\\\":\\\\\\\"132\\\\\\\",\\\\\\\"networkPerformance\\\\\\\":\\\\\\\"10 Gigabit\\\\\\\",\\\\\\\"servicename\\\\\\\":\\\\\\\"Amazon Elastic Compute Cloud\\\\\\\",\\\\\\\"instanceType\\\\\\\":\\\\\\\"c4.8xlarge\\\\\\\",\\\\\\\"tenancy\\\\\\\":\\\\\\\"Shared\\\\\\\",\\\\\\\"usagetype\\\\\\\":\\\\\\\"BoxUsage:c4.8xlarge\\\\\\\",\\\\\\\"normalizationSizeFactor\\\\\\\":\\\\\\\"64\\\\\\\",\\\\\\\"processorFeatures\\\\\\\":\\\\\\\"Intel AVX; Intel AVX2; Intel Turbo\\\\\\\",\\\\\\\"servicecode\\\\\\\":\\\\\\\"AmazonEC2\\\\\\\",\\\\\\\"licenseModel\\\\\\\":\\\\\\\"No License required\\\\\\\",\\\\\\\"currentGeneration\\\\\\\":\\\\\\\"Yes\\\\\\\",\\\\\\\"preInstalledSw\\\\\\\":\\\\\\\"SQL Std\\\\\\\",\\\\\\\"location\\\\\\\":\\\\\\\"US East (N. Virginia)\\\\\\\",\\\\\\\"processorArchitecture\\\\\\\":\\\\\\\"64-bit\\\\\\\",\\\\\\\"operation\\\\\\\":\\\\\\\"RunInstances:0004\\\\\\\"},\\\\\\\"sku\\\\\\\":\\\\\\\"9Z46AP6JSVWUSQRV\\\\\\\"},\\\\\\\"serviceCode\\\\\\\":\\\\\\\"AmazonEC2\\\\\\\",\\\\\\\"terms\\\\\\\":{\\\\\\\"OnDemand\\\\\\\":{\\\\\\\"9Z46AP6JSVWUSQRV.JRTCKXETXF\\\\\\\":{\\\\\\\"priceDimensions\\\\\\\":{\\\\\\\"9Z46AP6JSVWUSQRV.JRTCKXETXF.6YS6EN2CT7\\\\\\\":{\\\\\\\"unit\\\\\\\":\\\\\\\"Hrs\\\\\\\",\\\\\\\"endRange\\\\\\\":\\\\\\\"Inf\\\\\\\",\\\\\\\"description\\\\\\\":\\\\\\\"$5.92 per On Demand Linux with SQL Std c4.8xlarge Instance Hour\\\\\\\",\\\\\\\"appliesTo\\\\\\\":[],\\\\\\\"rateCode\\\\\\\":\\\\\\\"9Z46AP6JSVWUSQRV.JRTCKXETXF.6YS6EN2CT7\\\\\\\",\\\\\\\"beginRange\\\\\\\":\\\\\\\"0\\\\\\\",\\\\\\\"pricePerUnit\\\\\\\":{\\\\\\\"USD\\\\\\\":\\\\\\\"5.9200000000\\\\\\\"}}},\\\\\\\"sku\\\\\\\":\\\\\\\"9Z46AP6JSVWUSQRV\\\\\\\",\\\\\\\"effectiveDate\\\\\\\":\\\\\\\"2018-04-01T00:00:00Z\\\\\\\",\\\\\\\"offerTermCode\\\\\\\":\\\\\\\"JRTCKXETXF\\\\\\\",\\\\\\\"termAttributes\\\\\\\":{}}},\\\\\\\"Reserved\\\\\\\":{\\\\\\\"9Z46AP6JSVWUSQRV.4NA7Y494T4\\\\\\\":{\\\\\\\"priceDimensions\\\\\\\":{\\\\\\\"9Z46AP6JSVWUSQRV.4NA7Y494T4.6YS6EN2CT7\\\\\\\":{\\\\\\\"unit\\\\\\\":\\\\\\\"Hrs\\\\\\\",\\\\\\\"endRange\\\\\\\":\\\\\\\"Inf\\\\\\\",\\\\\\\"description\\\\\\\":\\\\\\\"Linux with SQL Server Standard (Amazon VPC), c4.8xlarge reserved instance applied\\\\\\\",\\\\\\\"appliesTo\\\\\\\":[],\\\\\\\"rateCode\\\\\\\":\\\\\\\"9Z46AP6JSVWUSQRV.4NA7Y494T4.6YS6EN2CT7\\\\\\\",\\\\\\\"beginRange\\\\\\\":\\\\\\\"0\\\\\\\",\\\\\\\"pricePerUnit\\\\\\\":{\\\\\\\"USD\\\\\\\":\\\\\\\"5.3280000000\\\\\\\"}}},\\\\\\\"sku\\\\\\\":\\\\\\\"9Z46AP6JSVWUSQRV\\\\\\\",\\\\\\\"effectiveDate\\\\\\\":\\\\\\\"2018-03-31T23:59:59Z\\\\\\\",\\\\\\\"offerTermCode\\\\\\\":\\\\\\\"4NA7Y494T4\\\\\\\",\\\\\\\"termAttributes\\\\\\\":{\\\\\\\"LeaseContractLength\\\\\\\":\\\\\\\"1yr\\\\\\\",\\\\\\\"OfferingClass\\\\\\\":\\\\\\\"standard\\\\\\\",\\\\\\\"PurchaseOption\\\\\\\":\\\\\\\"No Upfront\\\\\\\"}},\\\\\\\"9Z46AP6JSVWUSQRV.CUZHX8X6JH\\\\\\\":{\\\\\\\"priceDimensions\\\\\\\":{\\\\\\\"9Z46AP6JSVWUSQRV.CUZHX8X6JH.2TG2D8R56U\\\\\\\":{\\\\\\\"unit\\\\\\\":\\\\\\\"Quantity\\\\\\\",\\\\\\\"description\\\\\\\":\\\\\\\"Upfront Fee\\\\\\\",\\\\\\\"appliesTo\\\\\\\":[],\\\\\\\"rateCode\\\\\\\":\\\\\\\"9Z46AP6JSVWUSQRV.CUZHX8X6JH.2TG2D8R56U\\\\\\\",\\\\\\\"pricePerUnit\\\\\\\":{\\\\\\\"USD\\\\\\\":\\\\\\\"23757\\\\\\\"}},\\\\\\\"9Z46AP6JSVWUSQRV.CUZHX8X6JH.6YS6EN2CT7\\\\\\\":{\\\\\\\"unit\\\\\\\":\\\\\\\"Hrs\\\\\\\",\\\\\\\"endRange\\\\\\\":\\\\\\\"Inf\\\\\\\",\\\\\\\"description\\\\\\\":\\\\\\\"Linux with SQL Server Standard (Amazon VPC), c4.8xlarge reserved instance applied\\\\\\\",\\\\\\\"appliesTo\\\\\\\":[],\\\\\\\"rateCode\\\\\\\":\\\\\\\"9Z46AP6JSVWUSQRV.CUZHX8X6JH.6YS6EN2CT7\\\\\\\",\\\\\\\"beginRange\\\\\\\":\\\\\\\"0\\\\\\\",\\\\\\\"pricePerUnit\\\\\\\":{\\\\\\\"USD\\\\\\\":\\\\\\\"2.7120000000\\\\\\\"}}},\\\\\\\"sku\\\\\\\":\\\\\\\"9Z46AP6JSVWUSQRV\\\\\\\",\\\\\\\"effectiveDate\\\\\\\":\\\\\\\"2018-03-31T23:59:59Z\\\\\\\",\\\\\\\"offerTermCode\\\\\\\":\\\\\\\"CUZHX8X6JH\\\\\\\",\\\\\\\"termAttributes\\\\\\\":{\\\\\\\"LeaseContractLength\\\\\\\":\\\\\\\"1yr\\\\\\\",\\\\\\\"OfferingClass\\\\\\\":\\\\\\\"convertible\\\\\\\",\\\\\\\"PurchaseOption\\\\\\\":\\\\\\\"Partial Upfront\\\\\\\"}},\\\\\\\"9Z46AP6JSVWUSQRV.MZU6U2429S\\\\\\\":{\\\\\\\"priceDimensions\\\\\\\":{\\\\\\\"9Z46AP6JSVWUSQRV.MZU6U2429S.2TG2D8R56U\\\\\\\":{\\\\\\\"unit\\\\\\\":\\\\\\\"Quantity\\\\\\\",\\\\\\\"description\\\\\\\":\\\\\\\"Upfront Fee\\\\\\\",\\\\\\\"appliesTo\\\\\\\":[],\\\\\\\"rateCode\\\\\\\":\\\\\\\"9Z46AP6JSVWUSQRV.MZU6U2429S.2TG2D8R56U\\\\\\\",\\\\\\\"pricePerUnit\\\\\\\":{\\\\\\\"USD\\\\\\\":\\\\\\\"132959\\\\\\\"}},\\\\\\\"9Z46AP6JSVWUSQRV.MZU6U2429S.6YS6EN2CT7\\\\\\\":{\\\\\\\"unit\\\\\\\":\\\\\\\"Hrs\\\\\\\",\\\\\\\"endRange\\\\\\\":\\\\\\\"Inf\\\\\\\",\\\\\\\"description\\\\\\\":\\\\\\\"Linux with SQL Server Standard (Amazon VPC), c4.8xlarge reserved instance applied\\\\\\\",\\\\\\\"appliesTo\\\\\\\":[],\\\\\\\"rateCode\\\\\\\":\\\\\\\"9Z46AP6JSVWUSQRV.MZU6U2429S.6YS6EN2CT7\\\\\\\",\\\\\\\"beginRange\\\\\\\":\\\\\\\"0\\\\\\\",\\\\\\\"pricePerUnit\\\\\\\":{\\\\\\\"USD\\\\\\\":\\\\\\\"0.0000000000\\\\\\\"}}},\\\\\\\"sku\\\\\\\":\\\\\\\"9Z46AP6JSVWUSQRV\\\\\\\",\\\\\\\"effectiveDate\\\\\\\":\\\\\\\"2018-03-31T23:59:59Z\\\\\\\",\\\\\\\"offerTermCode\\\\\\\":\\\\\\\"MZU6U2429S\\\\\\\",\\\\\\\"termAttributes\\\\\\\":{\\\\\\\"LeaseContractLength\\\\\\\":\\\\\\\"3yr\\\\\\\",\\\\\\\"OfferingClass\\\\\\\":\\\\\\\"convertible\\\\\\\",\\\\\\\"PurchaseOption\\\\\\\":\\\\\\\"All Upfront\\\\\\\"}},\\\\\\\"9Z46AP6JSVWUSQRV.Z2E3P23VKM\\\\\\\":{\\\\\\\"priceDimensions\\\\\\\":{\\\\\\\"9Z46AP6JSVWUSQRV.Z2E3P23VKM.6YS6EN2CT7\\\\\\\":{\\\\\\\"unit\\\\\\\":\\\\\\\"Hrs\\\\\\\",\\\\\\\"endRange\\\\\\\":\\\\\\\"Inf\\\\\\\",\\\\\\\"description\\\\\\\":\\\\\\\"Linux with SQL Server Standard (Amazon VPC), c4.8xlarge reserved instance applied\\\\\\\",\\\\\\\"appliesTo\\\\\\\":[],\\\\\\\"rateCode\\\\\\\":\\\\\\\"9Z46AP6JSVWUSQRV.Z2E3P23VKM.6YS6EN2CT7\\\\\\\",\\\\\\\"beginRange\\\\\\\":\\\\\\\"0\\\\\\\",\\\\\\\"pricePerUnit\\\\\\\":{\\\\\\\"USD\\\\\\\":\\\\\\\"5.1350000000\\\\\\\"}}},\\\\\\\"sku\\\\\\\":\\\\\\\"9Z46AP6JSVWUSQRV\\\\\\\",\\\\\\\"effectiveDate\\\\\\\":\\\\\\\"2018-03-31T23:59:59Z\\\\\\\",\\\\\\\"offerTermCode\\\\\\\":\\\\\\\"Z2E3P23VKM\\\\\\\",\\\\\\\"termAttributes\\\\\\\":{\\\\\\\"LeaseContractLength\\\\\\\":\\\\\\\"3yr\\\\\\\",\\\\\\\"OfferingClass\\\\\\\":\\\\\\\"convertible\\\\\\\",\\\\\\\"PurchaseOption\\\\\\\":\\\\\\\"No Upfront\\\\\\\"}},\\\\\\\"9Z46AP6JSVWUSQRV.HU7G6KETJZ\\\\\\\":{\\\\\\\"priceDimensions\\\\\\\":{\\\\\\\"9Z46AP6JSVWUSQRV.HU7G6KETJZ.2TG2D8R56U\\\\\\\":{\\\\\\\"unit\\\\\\\":\\\\\\\"Quantity\\\\\\\",\\\\\\\"description\\\\\\\":\\\\\\\"Upfront Fee\\\\\\\",\\\\\\\"appliesTo\\\\\\\":[],\\\\\\\"rateCode\\\\\\\":\\\\\\\"9Z46AP6JSVWUSQRV.HU7G6KETJZ.2TG2D8R56U\\\\\\\",\\\\\\\"pricePerUnit\\\\\\\":{\\\\\\\"USD\\\\\\\":\\\\\\\"23126\\\\\\\"}},\\\\\\\"9Z46AP6JSVWUSQRV.HU7G6KETJZ.6YS6EN2CT7\\\\\\\":{\\\\\\\"unit\\\\\\\":\\\\\\\"Hrs\\\\\\\",\\\\\\\"endRange\\\\\\\":\\\\\\\"Inf\\\\\\\",\\\\\\\"description\\\\\\\":\\\\\\\"Linux with SQL Server Standard (Amazon VPC), c4.8xlarge reserved instance applied\\\\\\\",\\\\\\\"appliesTo\\\\\\\":[],\\\\\\\"rateCode\\\\\\\":\\\\\\\"9Z46AP6JSVWUSQRV.HU7G6KETJZ.6YS6EN2CT7\\\\\\\",\\\\\\\"beginRange\\\\\\\":\\\\\\\"0\\\\\\\",\\\\\\\"pricePerUnit\\\\\\\":{\\\\\\\"USD\\\\\\\":\\\\\\\"2.6400000000\\\\\\\"}}},\\\\\\\"sku\\\\\\\":\\\\\\\"9Z46AP6JSVWUSQRV\\\\\\\",\\\\\\\"effectiveDate\\\\\\\":\\\\\\\"2018-03-31T23:59:59Z\\\\\\\",\\\\\\\"offerTermCode\\\\\\\":\\\\\\\"HU7G6KETJZ\\\\\\\",\\\\\\\"termAttributes\\\\\\\":{\\\\\\\"LeaseContractLength\\\\\\\":\\\\\\\"1yr\\\\\\\",\\\\\\\"OfferingClass\\\\\\\":\\\\\\\"standard\\\\\\\",\\\\\\\"PurchaseOption\\\\\\\":\\\\\\\"Partial Upfront\\\\\\\"}},\\\\\\\"9Z46AP6JSVWUSQRV.VJWZNREJX2\\\\\\\":{\\\\\\\"priceDimensions\\\\\\\":{\\\\\\\"9Z46AP6JSVWUSQRV.VJWZNREJX2.6YS6EN2CT7\\\\\\\":{\\\\\\\"unit\\\\\\\":\\\\\\\"Hrs\\\\\\\",\\\\\\\"endRange\\\\\\\":\\\\\\\"Inf\\\\\\\",\\\\\\\"description\\\\\\\":\\\\\\\"Linux with SQL Server Standard (Amazon VPC), c4.8xlarge reserved instance applied\\\\\\\",\\\\\\\"appliesTo\\\\\\\":[],\\\\\\\"rateCode\\\\\\\":\\\\\\\"9Z46AP6JSVWUSQRV.VJWZNREJX2.6YS6EN2CT7\\\\\\\",\\\\\\\"beginRange\\\\\\\":\\\\\\\"0\\\\\\\",\\\\\\\"pricePerUnit\\\\\\\":{\\\\\\\"USD\\\\\\\":\\\\\\\"0.0000000000\\\\\\\"}},\\\\\\\"9Z46AP6JSVWUSQRV.VJWZNREJX2.2TG2D8R56U\\\\\\\":{\\\\\\\"unit\\\\\\\":\\\\\\\"Quantity\\\\\\\",\\\\\\\"description\\\\\\\":\\\\\\\"Upfront Fee\\\\\\\",\\\\\\\"appliesTo\\\\\\\":[],\\\\\\\"rateCode\\\\\\\":\\\\\\\"9Z46AP6JSVWUSQRV.VJWZNREJX2.2TG2D8R56U\\\\\\\",\\\\\\\"pricePerUnit\\\\\\\":{\\\\\\\"USD\\\\\\\":\\\\\\\"47321\\\\\\\"}}},\\\\\\\"sku\\\\\\\":\\\\\\\"9Z46AP6JSVWUSQRV\\\\\\\",\\\\\\\"effectiveDate\\\\\\\":\\\\\\\"2018-03-31T23:59:59Z\\\\\\\",\\\\\\\"offerTermCode\\\\\\\":\\\\\\\"VJWZNREJX2\\\\\\\",\\\\\\\"termAttributes\\\\\\\":{\\\\\\\"LeaseContractLength\\\\\\\":\\\\\\\"1yr\\\\\\\",\\\\\\\"OfferingClass\\\\\\\":\\\\\\\"convertible\\\\\\\",\\\\\\\"PurchaseOption\\\\\\\":\\\\\\\"All Upfront\\\\\\\"}},\\\\\\\"9Z46AP6JSVWUSQRV.NQ3QZPMQV9\\\\\\\":{\\\\\\\"priceDimensions\\\\\\\":{\\\\\\\"9Z46AP6JSVWUSQRV.NQ3QZPMQV9.2TG2D8R56U\\\\\\\":{\\\\\\\"unit\\\\\\\":\\\\\\\"Quantity\\\\\\\",\\\\\\\"description\\\\\\\":\\\\\\\"Upfront Fee\\\\\\\",\\\\\\\"appliesTo\\\\\\\":[],\\\\\\\"rateCode\\\\\\\":\\\\\\\"9Z46AP6JSVWUSQRV.NQ3QZPMQV9.2TG2D8R56U\\\\\\\",\\\\\\\"pricePerUnit\\\\\\\":{\\\\\\\"USD\\\\\\\":\\\\\\\"129735\\\\\\\"}},\\\\\\\"9Z46AP6JSVWUSQRV.NQ3QZPMQV9.6YS6EN2CT7\\\\\\\":{\\\\\\\"unit\\\\\\\":\\\\\\\"Hrs\\\\\\\",\\\\\\\"endRange\\\\\\\":\\\\\\\"Inf\\\\\\\",\\\\\\\"description\\\\\\\":\\\\\\\"Linux with SQL Server Standard (Amazon VPC), c4.8xlarge reserved instance applied\\\\\\\",\\\\\\\"appliesTo\\\\\\\":[],\\\\\\\"rateCode\\\\\\\":\\\\\\\"9Z46AP6JSVWUSQRV.NQ3QZPMQV9.6YS6EN2CT7\\\\\\\",\\\\\\\"beginRange\\\\\\\":\\\\\\\"0\\\\\\\",\\\\\\\"pricePerUnit\\\\\\\":{\\\\\\\"USD\\\\\\\":\\\\\\\"0.0000000000\\\\\\\"}}},\\\\\\\"sku\\\\\\\":\\\\\\\"9Z46AP6JSVWUSQRV\\\\\\\",\\\\\\\"effectiveDate\\\\\\\":\\\\\\\"2018-03-31T23:59:59Z\\\\\\\",\\\\\\\"offerTermCode\\\\\\\":\\\\\\\"NQ3QZPMQV9\\\\\\\",\\\\\\\"termAttributes\\\\\\\":{\\\\\\\"LeaseContractLength\\\\\\\":\\\\\\\"3yr\\\\\\\",\\\\\\\"OfferingClass\\\\\\\":\\\\\\\"standard\\\\\\\",\\\\\\\"PurchaseOption\\\\\\\":\\\\\\\"All Upfront\\\\\\\"}},\\\\\\\"9Z46AP6JSVWUSQRV.6QCMYABX3D\\\\\\\":{\\\\\\\"priceDimensions\\\\\\\":{\\\\\\\"9Z46AP6JSVWUSQRV.6QCMYABX3D.2TG2D8R56U\\\\\\\":{\\\\\\\"unit\\\\\\\":\\\\\\\"Quantity\\\\\\\",\\\\\\\"description\\\\\\\":\\\\\\\"Upfront Fee\\\\\\\",\\\\\\\"appliesTo\\\\\\\":[],\\\\\\\"rateCode\\\\\\\":\\\\\\\"9Z46AP6JSVWUSQRV.6QCMYABX3D.2TG2D8R56U\\\\\\\",\\\\\\\"pricePerUnit\\\\\\\":{\\\\\\\"USD\\\\\\\":\\\\\\\"46085\\\\\\\"}},\\\\\\\"9Z46AP6JSVWUSQRV.6QCMYABX3D.6YS6EN2CT7\\\\\\\":{\\\\\\\"unit\\\\\\\":\\\\\\\"Hrs\\\\\\\",\\\\\\\"endRange\\\\\\\":\\\\\\\"Inf\\\\\\\",\\\\\\\"description\\\\\\\":\\\\\\\"Linux with SQL Server Standard (Amazon VPC), c4.8xlarge reserved instance applied\\\\\\\",\\\\\\\"appliesTo\\\\\\\":[],\\\\\\\"rateCode\\\\\\\":\\\\\\\"9Z46AP6JSVWUSQRV.6QCMYABX3D.6YS6EN2CT7\\\\\\\",\\\\\\\"beginRange\\\\\\\":\\\\\\\"0\\\\\\\",\\\\\\\"pricePerUnit\\\\\\\":{\\\\\\\"USD\\\\\\\":\\\\\\\"0.0000000000\\\\\\\"}}},\\\\\\\"sku\\\\\\\":\\\\\\\"9Z46AP6JSVWUSQRV\\\\\\\",\\\\\\\"effectiveDate\\\\\\\":\\\\\\\"2018-03-31T23:59:59Z\\\\\\\",\\\\\\\"offerTermCode\\\\\\\":\\\\\\\"6QCMYABX3D\\\\\\\",\\\\\\\"termAttributes\\\\\\\":{\\\\\\\"LeaseContractLength\\\\\\\":\\\\\\\"1yr\\\\\\\",\\\\\\\"OfferingClass\\\\\\\":\\\\\\\"standard\\\\\\\",\\\\\\\"PurchaseOption\\\\\\\":\\\\\\\"All Upfront\\\\\\\"}},\\\\\\\"9Z46AP6JSVWUSQRV.BPH4J8HBKS\\\\\\\":{\\\\\\\"priceDimensions\\\\\\\":{\\\\\\\"9Z46AP6JSVWUSQRV.BPH4J8HBKS.6YS6EN2CT7\\\\\\\":{\\\\\\\"unit\\\\\\\":\\\\\\\"Hrs\\\\\\\",\\\\\\\"endRange\\\\\\\":\\\\\\\"Inf\\\\\\\",\\\\\\\"description\\\\\\\":\\\\\\\"Linux with SQL Server Standard (Amazon VPC), c4.8xlarge reserved instance applied\\\\\\\",\\\\\\\"appliesTo\\\\\\\":[],\\\\\\\"rateCode\\\\\\\":\\\\\\\"9Z46AP6JSVWUSQRV.BPH4J8HBKS.6YS6EN2CT7\\\\\\\",\\\\\\\"beginRange\\\\\\\":\\\\\\\"0\\\\\\\",\\\\\\\"pricePerUnit\\\\\\\":{\\\\\\\"USD\\\\\\\":\\\\\\\"5.0280000000\\\\\\\"}}},\\\\\\\"sku\\\\\\\":\\\\\\\"9Z46AP6JSVWUSQRV\\\\\\\",\\\\\\\"effectiveDate\\\\\\\":\\\\\\\"2018-03-31T23:59:59Z\\\\\\\",\\\\\\\"offerTermCode\\\\\\\":\\\\\\\"BPH4J8HBKS\\\\\\\",\\\\\\\"termAttributes\\\\\\\":{\\\\\\\"LeaseContractLength\\\\\\\":\\\\\\\"3yr\\\\\\\",\\\\\\\"OfferingClass\\\\\\\":\\\\\\\"standard\\\\\\\",\\\\\\\"PurchaseOption\\\\\\\":\\\\\\\"No Upfront\\\\\\\"}},\\\\\\\"9Z46AP6JSVWUSQRV.R5XV2EPZQZ\\\\\\\":{\\\\\\\"priceDimensions\\\\\\\":{\\\\\\\"9Z46AP6JSVWUSQRV.R5XV2EPZQZ.2TG2D8R56U\\\\\\\":{\\\\\\\"unit\\\\\\\":\\\\\\\"Quantity\\\\\\\",\\\\\\\"description\\\\\\\":\\\\\\\"Upfront Fee\\\\\\\",\\\\\\\"appliesTo\\\\\\\":[],\\\\\\\"rateCode\\\\\\\":\\\\\\\"9Z46AP6JSVWUSQRV.R5XV2EPZQZ.2TG2D8R56U\\\\\\\",\\\\\\\"pricePerUnit\\\\\\\":{\\\\\\\"USD\\\\\\\":\\\\\\\"66678\\\\\\\"}},\\\\\\\"9Z46AP6JSVWUSQRV.R5XV2EPZQZ.6YS6EN2CT7\\\\\\\":{\\\\\\\"unit\\\\\\\":\\\\\\\"Hrs\\\\\\\",\\\\\\\"endRange\\\\\\\":\\\\\\\"Inf\\\\\\\",\\\\\\\"description\\\\\\\":\\\\\\\"Linux with SQL Server Standard (Amazon VPC), c4.8xlarge reserved instance applied\\\\\\\",\\\\\\\"appliesTo\\\\\\\":[],\\\\\\\"rateCode\\\\\\\":\\\\\\\"9Z46AP6JSVWUSQRV.R5XV2EPZQZ.6YS6EN2CT7\\\\\\\",\\\\\\\"beginRange\\\\\\\":\\\\\\\"0\\\\\\\",\\\\\\\"pricePerUnit\\\\\\\":{\\\\\\\"USD\\\\\\\":\\\\\\\"2.5370000000\\\\\\\"}}},\\\\\\\"sku\\\\\\\":\\\\\\\"9Z46AP6JSVWUSQRV\\\\\\\",\\\\\\\"effectiveDate\\\\\\\":\\\\\\\"2018-03-31T23:59:59Z\\\\\\\",\\\\\\\"offerTermCode\\\\\\\":\\\\\\\"R5XV2EPZQZ\\\\\\\",\\\\\\\"termAttributes\\\\\\\":{\\\\\\\"LeaseContractLength\\\\\\\":\\\\\\\"3yr\\\\\\\",\\\\\\\"OfferingClass\\\\\\\":\\\\\\\"convertible\\\\\\\",\\\\\\\"PurchaseOption\\\\\\\":\\\\\\\"Partial Upfront\\\\\\\"}},\\\\\\\"9Z46AP6JSVWUSQRV.38NPMPTW36\\\\\\\":{\\\\\\\"priceDimensions\\\\\\\":{\\\\\\\"9Z46AP6JSVWUSQRV.38NPMPTW36.2TG2D8R56U\\\\\\\":{\\\\\\\"unit\\\\\\\":\\\\\\\"Quantity\\\\\\\",\\\\\\\"description\\\\\\\":\\\\\\\"Upfront Fee\\\\\\\",\\\\\\\"appliesTo\\\\\\\":[],\\\\\\\"rateCode\\\\\\\":\\\\\\\"9Z46AP6JSVWUSQRV.38NPMPTW36.2TG2D8R56U\\\\\\\",\\\\\\\"pricePerUnit\\\\\\\":{\\\\\\\"USD\\\\\\\":\\\\\\\"65385\\\\\\\"}},\\\\\\\"9Z46AP6JSVWUSQRV.38NPMPTW36.6YS6EN2CT7\\\\\\\":{\\\\\\\"unit\\\\\\\":\\\\\\\"Hrs\\\\\\\",\\\\\\\"endRange\\\\\\\":\\\\\\\"Inf\\\\\\\",\\\\\\\"description\\\\\\\":\\\\\\\"Linux with SQL Server Standard (Amazon VPC), c4.8xlarge reserved instance applied\\\\\\\",\\\\\\\"appliesTo\\\\\\\":[],\\\\\\\"rateCode\\\\\\\":\\\\\\\"9Z46AP6JSVWUSQRV.38NPMPTW36.6YS6EN2CT7\\\\\\\",\\\\\\\"beginRange\\\\\\\":\\\\\\\"0\\\\\\\",\\\\\\\"pricePerUnit\\\\\\\":{\\\\\\\"USD\\\\\\\":\\\\\\\"2.4880000000\\\\\\\"}}},\\\\\\\"sku\\\\\\\":\\\\\\\"9Z46AP6JSVWUSQRV\\\\\\\",\\\\\\\"effectiveDate\\\\\\\":\\\\\\\"2018-03-31T23:59:59Z\\\\\\\",\\\\\\\"offerTermCode\\\\\\\":\\\\\\\"38NPMPTW36\\\\\\\",\\\\\\\"termAttributes\\\\\\\":{\\\\\\\"LeaseContractLength\\\\\\\":\\\\\\\"3yr\\\\\\\",\\\\\\\"OfferingClass\\\\\\\":\\\\\\\"standard\\\\\\\",\\\\\\\"PurchaseOption\\\\\\\":\\\\\\\"Partial Upfront\\\\\\\"}},\\\\\\\"9Z46AP6JSVWUSQRV.7NE97W5U4E\\\\\\\":{\\\\\\\"priceDimensions\\\\\\\":{\\\\\\\"9Z46AP6JSVWUSQRV.7NE97W5U4E.6YS6EN2CT7\\\\\\\":{\\\\\\\"unit\\\\\\\":\\\\\\\"Hrs\\\\\\\",\\\\\\\"endRange\\\\\\\":\\\\\\\"Inf\\\\\\\",\\\\\\\"description\\\\\\\":\\\\\\\"Linux with SQL Server Standard (Amazon VPC), c4.8xlarge reserved instance applied\\\\\\\",\\\\\\\"appliesTo\\\\\\\":[],\\\\\\\"rateCode\\\\\\\":\\\\\\\"9Z46AP6JSVWUSQRV.7NE97W5U4E.6YS6EN2CT7\\\\\\\",\\\\\\\"beginRange\\\\\\\":\\\\\\\"0\\\\\\\",\\\\\\\"pricePerUnit\\\\\\\":{\\\\\\\"USD\\\\\\\":\\\\\\\"5.4790000000\\\\\\\"}}},\\\\\\\"sku\\\\\\\":\\\\\\\"9Z46AP6JSVWUSQRV\\\\\\\",\\\\\\\"effectiveDate\\\\\\\":\\\\\\\"2018-03-31T23:59:59Z\\\\\\\",\\\\\\\"offerTermCode\\\\\\\":\\\\\\\"7NE97W5U4E\\\\\\\",\\\\\\\"termAttributes\\\\\\\":{\\\\\\\"LeaseContractLength\\\\\\\":\\\\\\\"1yr\\\\\\\",\\\\\\\"OfferingClass\\\\\\\":\\\\\\\"convertible\\\\\\\",\\\\\\\"PurchaseOption\\\\\\\":\\\\\\\"No Upfront\\\\\\\"}}}},\\\\\\\"version\\\\\\\":\\\\\\\"20180502184240\\\\\\\",\\\\\\\"publicationDate\\\\\\\":\\\\\\\"2018-05-02T18:42:40Z\\\\\\\"}\\\",\\\"{\\\\\\\"product\\\\\\\":{\\\\\\\"productFamily\\\\\\\":\\\\\\\"Compute Instance\\\\\\\",\\\\\\\"attributes\\\\\\\":{\\\\\\\"enhancedNetworkingSupported\\\\\\\":\\\\\\\"Yes\\\\\\\",\\\\\\\"memory\\\\\\\":\\\\\\\"60 GiB\\\\\\\",\\\\\\\"dedicatedEbsThroughput\\\\\\\":\\\\\\\"4000 Mbps\\\\\\\",\\\\\\\"vcpu\\\\\\\":\\\\\\\"36\\\\\\\",\\\\\\\"capacitystatus\\\\\\\":\\\\\\\"Used\\\\\\\",\\\\\\\"locationType\\\\\\\":\\\\\\\"AWS Region\\\\\\\",\\\\\\\"storage\\\\\\\":\\\\\\\"EBS only\\\\\\\",\\\\\\\"instanceFamily\\\\\\\":\\\\\\\"Compute optimized\\\\\\\",\\\\\\\"operatingSystem\\\\\\\":\\\\\\\"Linux\\\\\\\",\\\\\\\"physicalProcessor\\\\\\\":\\\\\\\"Intel Xeon E5-2666 v3 (Haswell)\\\\\\\",\\\\\\\"clockSpeed\\\\\\\":\\\\\\\"2.9 GHz\\\\\\\",\\\\\\\"ecu\\\\\\\":\\\\\\\"132\\\\\\\",\\\\\\\"networkPerformance\\\\\\\":\\\\\\\"10 Gigabit\\\\\\\",\\\\\\\"servicename\\\\\\\":\\\\\\\"Amazon Elastic Compute Cloud\\\\\\\",\\\\\\\"instanceType\\\\\\\":\\\\\\\"c4.8xlarge\\\\\\\",\\\\\\\"tenancy\\\\\\\":\\\\\\\"Shared\\\\\\\",\\\\\\\"usagetype\\\\\\\":\\\\\\\"BoxUsage:c4.8xlarge\\\\\\\",\\\\\\\"normalizationSizeFactor\\\\\\\":\\\\\\\"64\\\\\\\",\\\\\\\"processorFeatures\\\\\\\":\\\\\\\"Intel AVX; Intel AVX2; Intel Turbo\\\\\\\",\\\\\\\"servicecode\\\\\\\":\\\\\\\"AmazonEC2\\\\\\\",\\\\\\\"licenseModel\\\\\\\":\\\\\\\"No License required\\\\\\\",\\\\\\\"currentGeneration\\\\\\\":\\\\\\\"Yes\\\\\\\",\\\\\\\"preInstalledSw\\\\\\\":\\\\\\\"SQL Ent\\\\\\\",\\\\\\\"location\\\\\\\":\\\\\\\"US East (N. Virginia)\\\\\\\",\\\\\\\"processorArchitecture\\\\\\\":\\\\\\\"64-bit\\\\\\\",\\\\\\\"operation\\\\\\\":\\\\\\\"RunInstances:0100\\\\\\\"},\\\\\\\"sku\\\\\\\":\\\\\\\"GYYSCZ5UCVAC2A8Y\\\\\\\"},\\\\\\\"serviceCode\\\\\\\":\\\\\\\"AmazonEC2\\\\\\\",\\\\\\\"terms\\\\\\\":{\\\\\\\"OnDemand\\\\\\\":{\\\\\\\"GYYSCZ5UCVAC2A8Y.JRTCKXETXF\\\\\\\":{\\\\\\\"priceDimensions\\\\\\\":{\\\\\\\"GYYSCZ5UCVAC2A8Y.JRTCKXETXF.6YS6EN2CT7\\\\\\\":{\\\\\\\"unit\\\\\\\":\\\\\\\"Hrs\\\\\\\",\\\\\\\"endRange\\\\\\\":\\\\\\\"Inf\\\\\\\",\\\\\\\"description\\\\\\\":\\\\\\\"$15.10 per On Demand Linux with SQL Server Enterprise c4.8xlarge Instance Hour\\\\\\\",\\\\\\\"appliesTo\\\\\\\":[],\\\\\\\"rateCode\\\\\\\":\\\\\\\"GYYSCZ5UCVAC2A8Y.JRTCKXETXF.6YS6EN2CT7\\\\\\\",\\\\\\\"beginRange\\\\\\\":\\\\\\\"0\\\\\\\",\\\\\\\"pricePerUnit\\\\\\\":{\\\\\\\"USD\\\\\\\":\\\\\\\"15.1000000000\\\\\\\"}}},\\\\\\\"sku\\\\\\\":\\\\\\\"GYYSCZ5UCVAC2A8Y\\\\\\\",\\\\\\\"effectiveDate\\\\\\\":\\\\\\\"2018-04-01T00:00:00Z\\\\\\\",\\\\\\\"offerTermCode\\\\\\\":\\\\\\\"JRTCKXETXF\\\\\\\",\\\\\\\"termAttributes\\\\\\\":{}}},\\\\\\\"Reserved\\\\\\\":{\\\\\\\"GYYSCZ5UCVAC2A8Y.HU7G6KETJZ\\\\\\\":{\\\\\\\"priceDimensions\\\\\\\":{\\\\\\\"GYYSCZ5UCVAC2A8Y.HU7G6KETJZ.2TG2D8R56U\\\\\\\":{\\\\\\\"unit\\\\\\\":\\\\\\\"Quantity\\\\\\\",\\\\\\\"description\\\\\\\":\\\\\\\"Upfront Fee\\\\\\\",\\\\\\\"appliesTo\\\\\\\":[],\\\\\\\"rateCode\\\\\\\":\\\\\\\"GYYSCZ5UCVAC2A8Y.HU7G6KETJZ.2TG2D8R56U\\\\\\\",\\\\\\\"pricePerUnit\\\\\\\":{\\\\\\\"USD\\\\\\\":\\\\\\\"63335\\\\\\\"}},\\\\\\\"GYYSCZ5UCVAC2A8Y.HU7G6KETJZ.6YS6EN2CT7\\\\\\\":{\\\\\\\"unit\\\\\\\":\\\\\\\"Hrs\\\\\\\",\\\\\\\"endRange\\\\\\\":\\\\\\\"Inf\\\\\\\",\\\\\\\"description\\\\\\\":\\\\\\\"Linux with SQL Server Enterprise (Amazon VPC), c4.8xlarge reserved instance applied\\\\\\\",\\\\\\\"appliesTo\\\\\\\":[],\\\\\\\"rateCode\\\\\\\":\\\\\\\"GYYSCZ5UCVAC2A8Y.HU7G6KETJZ.6YS6EN2CT7\\\\\\\",\\\\\\\"beginRange\\\\\\\":\\\\\\\"0\\\\\\\",\\\\\\\"pricePerUnit\\\\\\\":{\\\\\\\"USD\\\\\\\":\\\\\\\"7.2300000000\\\\\\\"}}},\\\\\\\"sku\\\\\\\":\\\\\\\"GYYSCZ5UCVAC2A\"\n-> \"8Y\\\\\\\",\\\\\\\"effectiveDate\\\\\\\":\\\\\\\"2018-03-31T23:59:59Z\\\\\\\",\\\\\\\"offerTermCode\\\\\\\":\\\\\\\"HU7G6KETJZ\\\\\\\",\\\\\\\"termAttributes\\\\\\\":{\\\\\\\"LeaseContractLength\\\\\\\":\\\\\\\"1yr\\\\\\\",\\\\\\\"OfferingClass\\\\\\\":\\\\\\\"standard\\\\\\\",\\\\\\\"PurchaseOption\\\\\\\":\\\\\\\"Partial Upfront\\\\\\\"}},\\\\\\\"GYYSCZ5UCVAC2A8Y.6QCMYABX3D\\\\\\\":{\\\\\\\"priceDimensions\\\\\\\":{\\\\\\\"GYYSCZ5UCVAC2A8Y.6QCMYABX3D.2TG2D8R56U\\\\\\\":{\\\\\\\"unit\\\\\\\":\\\\\\\"Quantity\\\\\\\",\\\\\\\"description\\\\\\\":\\\\\\\"Upfront Fee\\\\\\\",\\\\\\\"appliesTo\\\\\\\":[],\\\\\\\"rateCode\\\\\\\":\\\\\\\"GYYSCZ5UCVAC2A8Y.6QCMYABX3D.2TG2D8R56U\\\\\\\",\\\\\\\"pricePerUnit\\\\\\\":{\\\\\\\"USD\\\\\\\":\\\\\\\"126501\\\\\\\"}},\\\\\\\"GYYSCZ5UCVAC2A8Y.6QCMYABX3D.6YS6EN2CT7\\\\\\\":{\\\\\\\"unit\\\\\\\":\\\\\\\"Hrs\\\\\\\",\\\\\\\"endRange\\\\\\\":\\\\\\\"Inf\\\\\\\",\\\\\\\"description\\\\\\\":\\\\\\\"Linux with SQL Server Enterprise (Amazon VPC), c4.8xlarge reserved instance applied\\\\\\\",\\\\\\\"appliesTo\\\\\\\":[],\\\\\\\"rateCode\\\\\\\":\\\\\\\"GYYSCZ5UCVAC2A8Y.6QCMYABX3D.6YS6EN2CT7\\\\\\\",\\\\\\\"beginRange\\\\\\\":\\\\\\\"0\\\\\\\",\\\\\\\"pricePerUnit\\\\\\\":{\\\\\\\"USD\\\\\\\":\\\\\\\"0.0000000000\\\\\\\"}}},\\\\\\\"sku\\\\\\\":\\\\\\\"GYYSCZ5UCVAC2A8Y\\\\\\\",\\\\\\\"effectiveDate\\\\\\\":\\\\\\\"2018-03-31T23:59:59Z\\\\\\\",\\\\\\\"offerTermCode\\\\\\\":\\\\\\\"6QCMYABX3D\\\\\\\",\\\\\\\"termAttributes\\\\\\\":{\\\\\\\"LeaseContractLength\\\\\\\":\\\\\\\"1yr\\\\\\\",\\\\\\\"OfferingClass\\\\\\\":\\\\\\\"standard\\\\\\\",\\\\\\\"PurchaseOption\\\\\\\":\\\\\\\"All Upfront\\\\\\\"}},\\\\\\\"GY\"\n-> \"YSCZ5UCVAC2A8Y.CUZHX8X6JH\\\\\\\":{\\\\\\\"priceDimensions\\\\\\\":{\\\\\\\"GYYSCZ5UCVAC2A8Y.CUZHX8X6JH.2TG2D8R56U\\\\\\\":{\\\\\\\"unit\\\\\\\":\\\\\\\"Quantity\\\\\\\",\\\\\\\"description\\\\\\\":\\\\\\\"Upfront Fee\\\\\\\",\\\\\\\"appliesTo\\\\\\\":[],\\\\\\\"rateCode\\\\\\\":\\\\\\\"GYYSCZ5UCVAC2A8Y.CUZHX8X6JH.2TG2D8R56U\\\\\\\",\\\\\\\"pricePerUnit\\\\\\\":{\\\\\\\"USD\\\\\\\":\\\\\\\"63966\\\\\\\"}},\\\\\\\"GYYSCZ5UCVAC2A8Y.CUZHX8X6JH.6YS6EN2CT7\\\\\\\":{\\\\\\\"unit\\\\\\\":\\\\\\\"Hrs\\\\\\\",\\\\\\\"endRange\\\\\\\":\\\\\\\"Inf\\\\\\\",\\\\\\\"description\\\\\\\":\\\\\\\"Linux with SQL Server Enterprise (Amazon VPC), c4.8xlarge reserved instance applied\\\\\\\",\\\\\\\"appliesTo\\\\\\\":[],\\\\\\\"rateCode\\\\\\\":\\\\\\\"GYYSCZ5UCVAC2A8Y.CUZHX8X6JH.6YS6EN2CT7\\\\\\\",\\\\\\\"beginRange\\\\\\\":\\\\\\\"0\\\\\\\",\\\\\\\"pricePerUnit\\\\\\\":{\\\\\\\"USD\\\\\\\":\\\\\\\"7.3020000000\\\\\\\"}}},\\\\\\\"sku\\\\\\\":\\\\\\\"GYYSCZ5UCVAC2A8Y\\\\\\\",\\\\\\\"effectiveDate\\\\\\\":\\\\\\\"2018-03-31T23:59:59Z\\\\\\\",\\\\\\\"offerTermCode\\\\\\\":\\\\\\\"CUZHX8X6JH\\\\\\\",\\\\\\\"termAttributes\\\\\\\":{\\\\\\\"LeaseContractLength\\\\\\\":\\\\\\\"1yr\\\\\\\",\\\\\\\"OfferingClass\\\\\\\":\\\\\\\"convertible\\\\\\\",\\\\\\\"PurchaseOption\\\\\\\":\\\\\\\"Partial Upfront\\\\\\\"}},\\\\\\\"GYYSCZ5UCVAC2A8Y.7NE97W5U4E\\\\\\\":{\\\\\\\"priceDimensions\\\\\\\":{\\\\\\\"GYYSCZ5UCVAC2A8Y.7NE97W5U4E.6YS6EN2CT7\\\\\\\":{\\\\\\\"unit\\\\\\\":\\\\\\\"Hrs\\\\\\\",\\\\\\\"endRange\\\\\\\":\\\\\\\"Inf\\\\\\\",\\\\\\\"description\\\\\\\":\\\\\\\"Linux with SQL Server Enterprise (Amazon VPC), c4.8xlarge reserved instance applied\\\\\\\",\\\\\\\"appliesTo\\\\\\\":[],\\\\\\\"rateCode\\\\\\\":\\\\\\\"GYYSCZ5UCVAC2A8Y.7NE97W5U4E.6YS6EN2CT7\\\\\\\",\\\\\\\"beginRange\\\\\\\":\\\\\\\"0\\\\\\\",\\\\\\\"pricePerUnit\\\\\\\":{\\\\\\\"USD\\\\\\\":\\\\\\\"14.6590000000\\\\\\\"}}},\\\\\\\"sku\\\\\\\":\\\\\\\"GYYSCZ5UCVAC2A8Y\\\\\\\",\\\\\\\"effectiveDate\\\\\\\":\\\\\\\"2018-03-31T23:59:59Z\\\\\\\",\\\\\\\"offerTermCode\\\\\\\":\\\\\\\"7NE97W5U4E\\\\\\\",\\\\\\\"termAttributes\\\\\\\":{\\\\\\\"LeaseContractLength\\\\\\\":\\\\\\\"1yr\\\\\\\",\\\\\\\"OfferingClass\\\\\\\":\\\\\\\"convertible\\\\\\\",\\\\\\\"PurchaseOption\\\\\\\":\\\\\\\"No Upfront\\\\\\\"}},\\\\\\\"GYYSCZ5UCVAC2A8Y.MZU6U2429S\\\\\\\":{\\\\\\\"priceDimensions\\\\\\\":{\\\\\\\"GYYSCZ5UCVAC2A8Y.MZU6U2429S.2TG2D8R56U\\\\\\\":{\\\\\\\"unit\\\\\\\":\\\\\\\"Quantity\\\\\\\",\\\\\\\"description\\\\\\\":\\\\\\\"Upfront Fee\\\\\\\",\\\\\\\"appliesTo\\\\\\\":[],\\\\\\\"rateCode\\\\\\\":\\\\\\\"GYYSCZ5UCVAC2A8Y.MZU6U2429S.2TG2D8R56U\\\\\\\",\\\\\\\"pricePerUnit\\\\\\\":{\\\\\\\"USD\\\\\\\":\\\\\\\"374209\\\\\\\"}},\\\\\\\"GYYSCZ5UCVAC2A8Y.MZU6U2429S.6YS6EN2CT7\\\\\\\":{\\\\\\\"unit\\\\\\\":\\\\\\\"Hrs\\\\\\\",\\\\\\\"endRange\\\\\\\":\\\\\\\"Inf\\\\\\\",\\\\\\\"description\\\\\\\":\\\\\\\"Linux with SQL Server Enterprise (Amazon VPC), c4.8xlarge reserved instance applied\\\\\\\",\\\\\\\"appliesTo\\\\\\\":[],\\\\\\\"rateCode\\\\\\\":\\\\\\\"GYYSCZ5UCVAC2A8Y.MZU6U2429S.6YS6EN2CT7\\\\\\\",\\\\\\\"beginRange\\\\\\\":\\\\\\\"0\\\\\\\",\\\\\\\"pricePerUnit\\\\\\\":{\\\\\\\"USD\\\\\\\":\\\\\\\"0.0000000000\\\\\\\"}}},\\\\\\\"sku\\\\\\\":\\\\\\\"GYYSCZ5UCVAC2A8Y\\\\\\\",\\\\\\\"effectiveDate\\\\\\\":\\\\\\\"2018-03-31T23:59:59Z\\\\\\\",\\\\\\\"offerTermCode\\\\\\\":\\\\\\\"MZU6U2429S\\\\\\\",\\\\\\\"termAttributes\\\\\\\":{\\\\\\\"LeaseContractLength\\\\\\\":\\\\\\\"3yr\\\\\\\",\\\\\\\"OfferingClass\\\\\\\":\\\\\\\"convertible\\\\\\\",\\\\\\\"PurchaseOption\\\\\\\":\\\\\\\"All Upfront\\\\\\\"}},\\\\\\\"GYYSCZ5UCVAC2A8Y.38NPMPTW36\\\\\\\":{\\\\\\\"priceDimensions\\\\\\\":{\\\\\\\"GYYSCZ5UCVAC2A8Y.38NPMPTW36.2TG2D8R56U\\\\\\\":{\\\\\\\"unit\\\\\\\":\\\\\\\"Quantity\\\\\\\",\\\\\\\"description\\\\\\\":\\\\\\\"Upfront Fee\\\\\\\",\\\\\\\"appliesTo\\\\\\\":[],\\\\\\\"rateCode\\\\\\\":\\\\\\\"GYYSCZ5UCVAC2A8Y.38NPMPTW36.2TG2D8R56U\\\\\\\",\\\\\\\"pricePerUnit\\\\\\\":{\\\\\\\"USD\\\\\\\":\\\\\\\"186010\\\\\\\"}},\\\\\\\"GYYSCZ5UCVAC2A8Y.38NPMPTW36.6YS6EN2CT7\\\\\\\":{\\\\\\\"unit\\\\\\\":\\\\\\\"Hrs\\\\\\\",\\\\\\\"endRange\\\\\\\":\\\\\\\"Inf\\\\\\\",\\\\\\\"description\\\\\\\":\\\\\\\"Linux with SQL Server Enterprise (Amazon VPC), c4.8xlarge reserved instance applied\\\\\\\",\\\\\\\"appliesTo\\\\\\\":[],\\\\\\\"rateCode\\\\\\\":\\\\\\\"GYYSCZ5UCVAC2A8Y.38NPMPTW36.6YS6EN2CT7\\\\\\\",\\\\\\\"beginRange\\\\\\\":\\\\\\\"0\\\\\\\",\\\\\\\"pricePerUnit\\\\\\\":{\\\\\\\"USD\\\\\\\":\\\\\\\"7.0780000000\\\\\\\"}}},\\\\\\\"sku\\\\\\\":\\\\\\\"GYYSCZ5UCVAC2A8Y\\\\\\\",\\\\\\\"effectiveDate\\\\\\\":\\\\\\\"2018-03-31T23:59:59Z\\\\\\\",\\\\\\\"offerTermCode\\\\\\\":\\\\\\\"38NPMPTW36\\\\\\\",\\\\\\\"termAttributes\\\\\\\":{\\\\\\\"LeaseContractLength\\\\\\\":\\\\\\\"3yr\\\\\\\",\\\\\\\"OfferingClass\\\\\\\":\\\\\\\"standard\\\\\\\",\\\\\\\"PurchaseOption\\\\\\\":\\\\\\\"Partial Upfront\\\\\\\"}},\\\\\\\"GYYSCZ5UCVAC2A8Y.R5XV2EPZQZ\\\\\\\":{\\\\\\\"priceDimensions\\\\\\\":{\\\\\\\"GYYSCZ5UCVAC2A8Y.R5XV2EPZQZ.6YS6EN2CT7\\\\\\\":{\\\\\\\"unit\\\\\\\":\\\\\\\"Hrs\\\\\\\",\\\\\\\"endRange\\\\\\\":\\\\\\\"Inf\\\\\\\",\\\\\\\"description\\\\\\\":\\\\\\\"Linux with SQL Server Enterprise (Amazon VPC), c4.8xlarge reserved instance applied\\\\\\\",\\\\\\\"appliesTo\\\\\\\":[],\\\\\\\"rateCode\\\\\\\":\\\\\\\"GYYSCZ5UCVAC2A8Y.R5XV2EPZQZ.6YS6EN2CT7\\\\\\\",\\\\\\\"beginRange\\\\\\\":\\\\\\\"0\\\\\\\",\\\\\\\"pricePerUnit\\\\\\\":{\\\\\\\"USD\\\\\\\":\\\\\\\"7.1270000000\\\\\\\"}},\\\\\\\"GYYSCZ5UCVAC2A8Y.R5XV2EPZQZ.2TG2D8R56U\\\\\\\":{\\\\\\\"unit\\\\\\\":\\\\\\\"Quantity\\\\\\\",\\\\\\\"description\\\\\\\":\\\\\\\"Upfront Fee\\\\\\\",\\\\\\\"appliesTo\\\\\\\":[],\\\\\\\"rateCode\\\\\\\":\\\\\\\"GYYSCZ5UCVAC2A8Y.R5XV2EPZQZ.2TG2D8R56U\\\\\\\",\\\\\\\"pricePerUnit\\\\\\\":{\\\\\\\"USD\\\\\\\":\\\\\\\"187303\\\\\\\"}}},\\\\\\\"sku\\\\\\\":\\\\\\\"GYYSCZ5UCVAC2A8Y\\\\\\\",\\\\\\\"effectiveDate\\\\\\\":\\\\\\\"2018-03-31T23:59:59Z\\\\\\\",\\\\\\\"offerTermCode\\\\\\\":\\\\\\\"R5XV2EPZQZ\\\\\\\",\\\\\\\"termAttributes\\\\\\\":{\\\\\\\"LeaseContractLength\\\\\\\":\\\\\\\"3yr\\\\\\\",\\\\\\\"OfferingClass\\\\\\\":\\\\\\\"convertible\\\\\\\",\\\\\\\"PurchaseOption\\\\\\\":\\\\\\\"Partial Upfront\\\\\\\"}},\\\\\\\"GYYSCZ5UCVAC2A8Y.NQ3QZPMQV9\\\\\\\":{\\\\\\\"priceDimensions\\\\\\\":{\\\\\\\"GYYSCZ5UCVAC2A8Y.NQ3QZPMQV9.2TG2D8R56U\\\\\\\":{\\\\\\\"unit\\\\\\\":\\\\\\\"Quantity\\\\\\\",\\\\\\\"description\\\\\\\":\\\\\\\"Upfront Fee\\\\\\\",\\\\\\\"appliesTo\\\\\\\":[],\\\\\\\"rateCode\\\\\\\":\\\\\\\"GYYSCZ5UCVAC2A8Y.NQ3QZPMQV9.2TG2D8R56U\\\\\\\",\\\\\\\"pricePerUnit\\\\\\\":{\\\\\\\"USD\\\\\\\":\\\\\\\"370985\\\\\\\"}},\\\\\\\"GYYSCZ5UCVAC2A8Y.NQ3QZPMQV9.6YS6EN2CT7\\\\\\\":{\\\\\\\"unit\\\\\\\":\\\\\\\"Hrs\\\\\\\",\\\\\\\"endRange\\\\\\\":\\\\\\\"Inf\\\\\\\",\\\\\\\"description\\\\\\\":\\\\\\\"Linux with SQL Server Enterprise (Amazon VPC), c4.8xlarge reserved instance applied\\\\\\\",\\\\\\\"appliesTo\\\\\\\":[],\\\\\\\"rateCode\\\\\\\":\\\\\\\"GYYSCZ5UCVAC2A8Y.NQ3QZPMQV9.6YS6EN2CT7\\\\\\\",\\\\\\\"beginRange\\\\\\\":\\\\\\\"0\\\\\\\",\\\\\\\"pricePerUnit\\\\\\\":{\\\\\\\"USD\\\\\\\":\\\\\\\"0.0000000000\\\\\\\"}}},\\\\\\\"sku\\\\\\\":\\\\\\\"GYYSCZ5UCVAC2A8Y\\\\\\\",\\\\\\\"effectiveDate\\\\\\\":\\\\\\\"2018-03-31T23:59:59Z\\\\\\\",\\\\\\\"offerTermCode\\\\\\\":\\\\\\\"NQ3QZPMQV9\\\\\\\",\\\\\\\"termAttributes\\\\\\\":{\\\\\\\"LeaseContractLength\\\\\\\":\\\\\\\"3yr\\\\\\\",\\\\\\\"OfferingClass\\\\\\\":\\\\\\\"standard\\\\\\\",\\\\\\\"PurchaseOption\\\\\\\":\\\\\\\"All Upfront\\\\\\\"}},\\\\\\\"GYYSCZ5UCVAC2A8Y.BPH4J8HBKS\\\\\\\":{\\\\\\\"priceDimensions\\\\\\\":{\\\\\\\"GYYSCZ5UCVAC2A8Y.BPH4J8HBKS.6YS6EN2CT7\\\\\\\":{\\\\\\\"unit\\\\\\\":\\\\\\\"Hrs\\\\\\\",\\\\\\\"endRange\\\\\\\":\\\\\\\"Inf\\\\\\\",\\\\\\\"description\\\\\\\":\\\\\\\"Linux with SQL Server Enterprise (Amazon VPC), c4.8xlarge reserved instance applied\\\\\\\",\\\\\\\"appliesTo\\\\\\\":[],\\\\\\\"rateCode\\\\\\\":\\\\\\\"GYYSCZ5UCVAC2A8Y.BPH4J8HBKS.6YS6EN2CT7\\\\\\\",\\\\\\\"beginRange\\\\\\\":\\\\\\\"0\\\\\\\",\\\\\\\"pricePerUnit\\\\\\\":{\\\\\\\"USD\\\\\\\":\\\\\\\"14.2080000000\\\\\\\"}}},\\\\\\\"sku\\\\\\\":\\\\\\\"GYYSCZ5UCVAC2A8Y\\\\\\\",\\\\\\\"effectiveDate\\\\\\\":\\\\\\\"2018-03-31T23:59:59Z\\\\\\\",\\\\\\\"offerTermCode\\\\\\\":\\\\\\\"BPH4J8HBKS\\\\\\\",\\\\\\\"termAttributes\\\\\\\":{\\\\\\\"LeaseContractLength\\\\\\\":\\\\\\\"3yr\\\\\\\",\\\\\\\"OfferingClass\\\\\\\":\\\\\\\"standard\\\\\\\",\\\\\\\"PurchaseOption\\\\\\\":\\\\\\\"No Upfront\\\\\\\"}},\\\\\\\"GYYSCZ5UCVAC2A8Y.VJWZNREJX2\\\\\\\":{\\\\\\\"priceDimensions\\\\\\\":{\\\\\\\"GYYSCZ5UCVAC2A8Y.VJWZNREJX2.6YS6EN2CT7\\\\\\\":{\\\\\\\"unit\\\\\\\":\\\\\\\"Hrs\\\\\\\",\\\\\\\"endRange\\\\\\\":\\\\\\\"Inf\\\\\\\",\\\\\\\"description\\\\\\\":\\\\\\\"Linux with SQL Server Enterprise (Amazon VPC), c4.8xlarge reserved instance applied\\\\\\\",\\\\\\\"appliesTo\\\\\\\":[],\\\\\\\"rateCode\\\\\\\":\\\\\\\"GYYSCZ5UCVAC2A8Y.VJWZNREJX2.6YS6EN2CT7\\\\\\\",\\\\\\\"beginRange\\\\\\\":\\\\\\\"0\\\\\\\",\\\\\\\"pricePerUnit\\\\\\\":{\\\\\\\"USD\\\\\\\":\\\\\\\"0.0000000000\\\\\\\"}},\\\\\\\"GYYSCZ5UCVAC2A8Y.VJWZNREJX2.2TG2D8R56U\\\\\\\":{\\\\\\\"unit\\\\\\\":\\\\\\\"Quantity\\\\\\\",\\\\\\\"description\\\\\\\":\\\\\\\"Upfront Fee\\\\\\\",\\\\\\\"appliesTo\\\\\\\":[],\\\\\\\"rateCode\\\\\\\":\\\\\\\"GYYSCZ5UCVAC2A8Y.VJWZNREJX2.2TG2D8R56U\\\\\\\",\\\\\\\"pricePerUnit\\\\\\\":{\\\\\\\"USD\\\\\\\":\\\\\\\"127738\\\\\\\"}}},\\\\\\\"sku\\\\\\\":\\\\\\\"GYYSCZ5UCVAC2A8Y\\\\\\\",\\\\\\\"effectiveDate\\\\\\\":\\\\\\\"2018-03-31T23:59:59Z\\\\\\\",\\\\\\\"offerTermCode\\\\\\\":\\\\\\\"VJWZNREJX2\\\\\\\",\\\\\\\"termAttributes\\\\\\\":{\\\\\\\"LeaseContractLength\\\\\\\":\\\\\\\"1yr\\\\\\\",\\\\\\\"OfferingClass\\\\\\\":\\\\\\\"convertible\\\\\\\",\\\\\\\"PurchaseOption\\\\\\\":\\\\\\\"All Upfront\\\\\\\"}},\\\\\\\"GYYSCZ5UCVAC2A8Y.4NA7Y494T4\\\\\\\":{\\\\\\\"priceDimensions\\\\\\\":{\\\\\\\"GYYSCZ5UCVAC2A8Y.4NA7Y494T4.6YS6EN2CT7\\\\\\\":{\\\\\\\"unit\\\\\\\":\\\\\\\"Hrs\\\\\\\",\\\\\\\"endRange\\\\\\\":\\\\\\\"Inf\\\\\\\",\\\\\\\"description\\\\\\\":\\\\\\\"Linux with SQL Server Enterprise (Amazon VPC), c4.8xlarge reserved instance applied\\\\\\\",\\\\\\\"appliesTo\\\\\\\":[],\\\\\\\"rateCode\\\\\\\":\\\\\\\"GYYSCZ5UCVAC2A8Y.4NA7Y494T4.6YS6EN2CT7\\\\\\\",\\\\\\\"beginRange\\\\\\\":\\\\\\\"0\\\\\\\",\\\\\\\"pricePerUnit\\\\\\\":{\\\\\\\"USD\\\\\\\":\\\\\\\"14.5080000000\\\\\\\"}}},\\\\\\\"sku\\\\\\\":\\\\\\\"GYYSCZ5UCVAC2A8Y\\\\\\\",\\\\\\\"effectiveDate\\\\\\\":\\\\\\\"2018-03-31T23:59:59Z\\\\\\\",\\\\\\\"offerTermCode\\\\\\\":\\\\\\\"4NA7Y494T4\\\\\\\",\\\\\\\"termAttributes\\\\\\\":{\\\\\\\"LeaseContractLength\\\\\\\":\\\\\\\"1yr\\\\\\\",\\\\\\\"OfferingClass\\\\\\\":\\\\\\\"standard\\\\\\\",\\\\\\\"PurchaseOption\\\\\\\":\\\\\\\"No Upfront\\\\\\\"}},\\\\\\\"GYYSCZ5UCVAC2A8Y.Z2E3P23VKM\\\\\\\":{\\\\\\\"priceDimensions\\\\\\\":{\\\\\\\"GYYSCZ5UCVAC2A8Y.Z2E3P23VKM.6YS6EN2CT7\\\\\\\":{\\\\\\\"unit\\\\\\\":\\\\\\\"Hrs\\\\\\\",\\\\\\\"endRange\\\\\\\":\\\\\\\"Inf\\\\\\\",\\\\\\\"description\\\\\\\":\\\\\\\"Linux with SQL Server Enterprise (Amazon VPC), c4.8xlarge reserved instance applied\\\\\\\",\\\\\\\"appliesTo\\\\\\\":[],\\\\\\\"rateCode\\\\\\\":\\\\\\\"GYYSCZ5UCVAC2A8Y.Z2E3P23VKM.6YS6EN2CT7\\\\\\\",\\\\\\\"beginRange\\\\\\\":\\\\\\\"0\\\\\\\",\\\\\\\"pricePerUnit\\\\\\\":{\\\\\\\"USD\\\\\\\":\\\\\\\"14.3150000000\\\\\\\"}}},\\\\\\\"sku\\\\\\\":\\\\\\\"GYYSCZ5UCVAC2A8Y\\\\\\\",\\\\\\\"effectiveDate\\\\\\\":\\\\\\\"2018-03-31T23:59:59Z\\\\\\\",\\\\\\\"offerTermCode\\\\\\\":\\\\\\\"Z2E3P23VKM\\\\\\\",\\\\\\\"termAttributes\\\\\\\":{\\\\\\\"LeaseContractLength\\\\\\\":\\\\\\\"3yr\\\\\\\",\\\\\\\"OfferingClass\\\\\\\":\\\\\\\"convertible\\\\\\\",\\\\\\\"PurchaseOption\\\\\\\":\\\\\\\"No Upfront\\\\\\\"}}}},\\\\\\\"version\\\\\\\":\\\\\\\"20180502184240\\\\\\\",\\\\\\\"publicationDate\\\\\\\":\\\\\\\"2018-05-02T18:42:40Z\\\\\\\"}\\\"]}\"\nread 42055 bytes\nConn keep-alive\nNote: Those incorrect prices returned by API for 1yr contract are present in SDK response under 3yr contract.\nAttaching two screenshots of the csv downloaded by hitting https://pricing.us-east-1.amazonaws.com/offers/v1.0/aws/AmazonEC2/current/us-east-1/index.json\n\n\nI do not know which part is buggy, but it is causing a lot of issues on our production servers and impacting the business value. I request you to escalate this issue. \nThanks. I believed that both of them are same. Now if they are different, then both the sources(one starting with https and another one with api) allowing programmatic access to the prices are buggy and SDK is fine. \nCan you tell me where can I file a bug issue for https://pricing.us-east-1.amazonaws.com/offers/v1.0/aws/AmazonEC2/current/index.json endpoint if at all it is possible?. @awood45 Exactly. And I have reserved instances with different instance types, I can see problem among those with i3 class only. Seems like a bug.. ",
    "jnewbigin": "rescue Aws::Errors::MissingCredentialsError\n    signal_error \"no credentials provided\". ",
    "neilsimon": "Output from failing/looping command\nbad.txt\n. I've entered a problem with rubygems:\nhttp://help.rubygems.org/discussions/problems/31028-gem-install-loops-on-aws-sdk-301\nI had to kill the install as it does appear to be looping, but as far as I can tell, even after a long time, it doesn't complete, so the above log is essentially everything with that run. Here is output with DEBUG_RESOLVER turned on.\ndebug.txt\nIt happens every time. Some more information:\n1. aws-sdk-resources 3.14.0 doesn't install (it just loops forever)\n1. aws-sdk-resources 3.13.0 installs fine\n1. modified aws-sdk gem file to require a specific version of aws-sdk-resources of 3.13.0 will install\nIt seems that the problem is related to 63e7238a69509e3afe46ae2caa947545f6a76ce6 though I cannot see why it is an issue. The three added dependencies (aws-sdk-secretsmanager, aws-sdk-fms, aws-sdk-acmpca) all install properly.\nComparing the two gemspec.rz files for aws-sdk-resources (3.13.0 and 3.14.0) shows no obvious errors.\nI do suspect that the issue is with ruby, but the fact that without changing which version of aws-sdk I was using resulted in a massive change to through modifications to the dependencies is concerning as it makes it rather hard to lock down to a known good and working version of aws-sdk, but rather we have to be concerned about updates to dependent resources sneaking in through the back door.. I have verified that this issue doesn't happen on RHEL 7.3 when using the same version of ruby (2.4.3) which fails on Ubuntu.. ",
    "paustin01": "+1 seeing this exact behavior all of a sudden provisioning an ubuntu 16.04 instance in EC2 via chef-client v3.8.5 (embedded ruby version 2.4.3p205) when trying to do a chef_gem install of aws-sdk at latest (3.0.1). This worked last week with all the same specs.. ",
    "Amiller-philips": "This is not just happening on Linux - we are trying to provision Windows instances on from our Linux Chef server and encounter the same problem. If i RDP into a freshly bootstrapped instance and install the sdk by hand, the install of the sdk works. Interestingly our cookbook installs chef-sugar without any problems and then ALWAYS fails to install the SDK. \nIf I time the installation that I do manually, it takes between 120 and 150 seconds. AFAIK the timeouts configured in knife winrm are  longer than that.\nThere is no difference WRT Win2012 or Win2016 Server, or if a mini cookbook that just installs the sdk is tested in the kitchen. We ALWAYS fail.... ",
    "shashankshandilya": "GET https://www.rubygems.org/api/v1/dependencies?gems=aws-sdk-resources\n200 OK\nGET https://api.rubygems.org/api/v1/dependencies?gems=aws-sdk-acm,aws-sdk-acmpca,aws-sdk-alexaforbusiness,aws-sdk-apigateway,aws-sdk-applicationautoscaling,aws-sdk-applicationdiscoveryservice,aws-sdk-appstream,aws-sdk-appsync,aws-sdk-athena,aws-sdk-autoscaling,aws-sdk-autoscalingplans,aws-sdk-batch,aws-sdk-budgets,aws-sdk-cloud9,aws-sdk-clouddirectory,aws-sdk-cloudformation,aws-sdk-cloudfront,aws-sdk-cloudhsm,aws-sdk-cloudhsmv2,aws-sdk-cloudsearch,aws-sdk-cloudsearchdomain,aws-sdk-cloudtrail,aws-sdk-cloudwatch,aws-sdk-cloudwatchevents,aws-sdk-cloudwatchlogs,aws-sdk-codebuild,aws-sdk-codecommit,aws-sdk-codedeploy,aws-sdk-codepipeline,aws-sdk-codestar,aws-sdk-cognitoidentity,aws-sdk-cognitoidentityprovider,aws-sdk-cognitosync,aws-sdk-comprehend,aws-sdk-configservice,aws-sdk-connect,aws-sdk-costandusagereportservice,aws-sdk-costexplorer,aws-sdk-databasemigrationservice,aws-sdk-datapipeline,aws-sdk-dax,aws-sdk-devicefarm,aws-sdk-directconnect,aws-sdk-directoryservice,aws-sdk-dynamodb,aws-sdk-dynamodbstreams,aws-sdk-ec2,aws-sdk-ecr,aws-sdk-ecs,aws-sdk-efs,aws-sdk-elasticache,aws-sdk-elasticbeanstalk,aws-sdk-elasticloadbalancing,aws-sdk-elasticloadbalancingv2,aws-sdk-elasticsearchservice,aws-sdk-elastictranscoder,aws-sdk-emr,aws-sdk-firehose,aws-sdk-fms,aws-sdk-gamelift,aws-sdk-glacier,aws-sdk-glue,aws-sdk-greengrass,aws-sdk-guardduty,aws-sdk-health,aws-sdk-iam,aws-sdk-importexport,aws-sdk-inspector,aws-sdk-iot,aws-sdk-iotdataplane,aws-sdk-iotjobsdataplane,aws-sdk-kinesis,aws-sdk-kinesisanalytics,aws-sdk-kinesisvideo,aws-sdk-kinesisvideoarchivedmedia,aws-sdk-kinesisvideomedia,aws-sdk-kms,aws-sdk-lambda,aws-sdk-lambdapreview,aws-sdk-lex,aws-sdk-lexmodelbuildingservice,aws-sdk-lightsail,aws-sdk-machinelearning,aws-sdk-marketplacecommerceanalytics,aws-sdk-marketplaceentitlementservice,aws-sdk-marketplacemetering,aws-sdk-mediaconvert,aws-sdk-medialive,aws-sdk-mediapackage,aws-sdk-mediastore,aws-sdk-mediastoredata,aws-sdk-migrationhub,aws-sdk-mobile,aws-sdk-mq,aws-sdk-mturk,aws-sdk-opsworks,aws-sdk-opsworkscm,aws-sdk-organizations,aws-sdk-pinpoint,aws-sdk-polly,aws-sdk-pricing,aws-sdk-rds,aws-sdk-redshift,aws-sdk-rekognition,aws-sdk-resourcegroups,aws-sdk-resourcegroupstaggingapi,aws-sdk-route53,aws-sdk-route53domains,aws-sdk-s3,aws-sdk-sagemaker,aws-sdk-sagemakerruntime,aws-sdk-secretsmanager,aws-sdk-serverlessapplicationrepository,aws-sdk-servicecatalog,aws-sdk-servicediscovery,aws-sdk-ses,aws-sdk-shield,aws-sdk-simpledb,aws-sdk-sms,aws-sdk-snowball,aws-sdk-sns,aws-sdk-sqs,aws-sdk-ssm,aws-sdk-states,aws-sdk-storagegateway,aws-sdk-support,aws-sdk-swf,aws-sdk-transcribeservice,aws-sdk-translate,aws-sdk-waf,aws-sdk-wafregional,aws-sdk-workdocs,aws-sdk-workmail,aws-sdk-workspaces,aws-sdk-xray\n502 Bad Gateway. Not able to install aws-sdk.\nruby:: ruby 2.5.1p57 (2018-03-29 revision 63029) [x86_64-darwin17]\ngem:: 2.7.6. ",
    "panicstevenson": "To further @shashankshandilya's point:\nhttps://api.rubygems.org/api/v1/dependencies?gems=aws-sdk-acm,aws-sdk-acmpca,aws-sdk-alexaforbusiness,aws-sdk-apigateway,aws-sdk-applicationautoscaling,aws-sdk-applicationdiscoveryservice,aws-sdk-appstream,aws-sdk-appsync,aws-sdk-athena,aws-sdk-autoscaling,aws-sdk-autoscalingplans,aws-sdk-batch,aws-sdk-budgets,aws-sdk-cloud9,aws-sdk-clouddirectory,aws-sdk-cloudformation,aws-sdk-cloudfront,aws-sdk-cloudhsm,aws-sdk-cloudhsmv2,aws-sdk-cloudsearch,aws-sdk-cloudsearchdomain,aws-sdk-cloudtrail,aws-sdk-cloudwatch,aws-sdk-cloudwatchevents,aws-sdk-cloudwatchlogs,aws-sdk-codebuild,aws-sdk-codecommit,aws-sdk-codedeploy,aws-sdk-codepipeline,aws-sdk-codestar,aws-sdk-cognitoidentity,aws-sdk-cognitoidentityprovider,aws-sdk-cognitosync,aws-sdk-comprehend,aws-sdk-configservice,aws-sdk-connect,aws-sdk-costandusagereportservice,aws-sdk-costexplorer,aws-sdk-databasemigrationservice,aws-sdk-datapipeline,aws-sdk-dax,aws-sdk-devicefarm,aws-sdk-directconnect,aws-sdk-directoryservice,aws-sdk-dynamodb,aws-sdk-dynamodbstreams,aws-sdk-ec2,aws-sdk-ecr,aws-sdk-ecs,aws-sdk-efs,aws-sdk-elasticache,aws-sdk-elasticbeanstalk,aws-sdk-elasticloadbalancing,aws-sdk-elasticloadbalancingv2,aws-sdk-elasticsearchservice,aws-sdk-elastictranscoder,aws-sdk-emr,aws-sdk-firehose,aws-sdk-fms,aws-sdk-gamelift,aws-sdk-glacier,aws-sdk-glue,aws-sdk-greengrass,aws-sdk-guardduty,aws-sdk-health,aws-sdk-iam,aws-sdk-importexport,aws-sdk-inspector,aws-sdk-iot,aws-sdk-iotdataplane,aws-sdk-iotjobsdataplane,aws-sdk-kinesis,aws-sdk-kinesisanalytics,aws-sdk-kinesisvideo,aws-sdk-kinesisvideoarchivedmedia,aws-sdk-kinesisvideomedia,aws-sdk-kms,aws-sdk-lambda,aws-sdk-lambdapreview,aws-sdk-lex,aws-sdk-lexmodelbuildingservice,aws-sdk-lightsail,aws-sdk-machinelearning,aws-sdk-marketplacecommerceanalytics,aws-sdk-marketplaceentitlementservice,aws-sdk-marketplacemetering,aws-sdk-mediaconvert,aws-sdk-medialive,aws-sdk-mediapackage,aws-sdk-mediastore,aws-sdk-mediastoredata,aws-sdk-migrationhub,aws-sdk-mobile,aws-sdk-mq,aws-sdk-mturk,aws-sdk-opsworks,aws-sdk-opsworkscm,aws-sdk-organizations,aws-sdk-pinpoint,aws-sdk-polly,aws-sdk-pricing,aws-sdk-rds,aws-sdk-redshift,aws-sdk-rekognition,aws-sdk-resourcegroups,aws-sdk-resourcegroupstaggingapi,aws-sdk-route53,aws-sdk-route53domains,aws-sdk-s3,aws-sdk-sagemaker,aws-sdk-sagemakerruntime,aws-sdk-secretsmanager,aws-sdk-serverlessapplicationrepository,aws-sdk-servicecatalog,aws-sdk-servicediscovery,aws-sdk-ses,aws-sdk-shield,aws-sdk-simpledb,aws-sdk-sms,aws-sdk-snowball,aws-sdk-sns,aws-sdk-sqs,aws-sdk-ssm,aws-sdk-states,aws-sdk-storagegateway,aws-sdk-support,aws-sdk-swf,aws-sdk-transcribeservice,aws-sdk-translate,aws-sdk-waf,aws-sdk-wafregional,aws-sdk-workdocs,aws-sdk-workmail,aws-sdk-workspaces,aws-sdk-xray\nBad\nhttps://api.rubygems.org/api/v1/dependencies?gems=aws-sdk-acm,aws-sdk-acmpca,aws-sdk-alexaforbusiness,aws-sdk-apigateway,aws-sdk-applicationautoscaling,aws-sdk-applicationdiscoveryservice,aws-sdk-appstream,aws-sdk-appsync,aws-sdk-athena,aws-sdk-autoscaling,aws-sdk-autoscalingplans,aws-sdk-batch,aws-sdk-budgets,aws-sdk-cloud9,aws-sdk-clouddirectory,aws-sdk-cloudformation,aws-sdk-cloudfront,aws-sdk-cloudhsm,aws-sdk-cloudhsmv2,aws-sdk-cloudsearch,aws-sdk-cloudsearchdomain,aws-sdk-cloudtrail,aws-sdk-cloudwatch,aws-sdk-cloudwatchevents,aws-sdk-cloudwatchlogs,aws-sdk-codebuild,aws-sdk-codecommit,aws-sdk-codedeploy,aws-sdk-codepipeline,aws-sdk-codestar,aws-sdk-cognitoidentity,aws-sdk-cognitoidentityprovider,aws-sdk-cognitosync,aws-sdk-comprehend,aws-sdk-configservice,aws-sdk-connect,aws-sdk-costandusagereportservice,aws-sdk-costexplorer,aws-sdk-databasemigrationservice,aws-sdk-datapipeline,aws-sdk-dax,aws-sdk-devicefarm,aws-sdk-directconnect,aws-sdk-directoryservice,aws-sdk-dynamodb,aws-sdk-dynamodbstreams,aws-sdk-ec2,aws-sdk-ecr,aws-sdk-ecs,aws-sdk-efs,aws-sdk-elasticache,aws-sdk-elasticbeanstalk,aws-sdk-elasticloadbalancing,aws-sdk-elasticloadbalancingv2,aws-sdk-elasticsearchservice,aws-sdk-elastictranscoder,aws-sdk-emr,aws-sdk-firehose,aws-sdk-fms,aws-sdk-gamelift,aws-sdk-glacier,aws-sdk-glue,aws-sdk-greengrass,aws-sdk-guardduty,aws-sdk-health,aws-sdk-iam,aws-sdk-importexport,aws-sdk-inspector,aws-sdk-iot,aws-sdk-iotdataplane,aws-sdk-iotjobsdataplane,aws-sdk-kinesis,aws-sdk-kinesisanalytics,aws-sdk-kinesisvideo,aws-sdk-kinesisvideoarchivedmedia,aws-sdk-kinesisvideomedia,aws-sdk-kms,aws-sdk-lambda,aws-sdk-lambdapreview,aws-sdk-lex,aws-sdk-lexmodelbuildingservice,aws-sdk-lightsail,aws-sdk-machinelearning,aws-sdk-marketplacecommerceanalytics,aws-sdk-marketplaceentitlementservice,aws-sdk-marketplacemetering,aws-sdk-mediaconvert,aws-sdk-medialive,aws-sdk-mediapackage,aws-sdk-mediastore,aws-sdk-mediastoredata,aws-sdk-migrationhub,aws-sdk-mobile,aws-sdk-mq,aws-sdk-mturk,aws-sdk-opsworks,aws-sdk-opsworkscm,aws-sdk-organizations,aws-sdk-pinpoint,aws-sdk-redshift,aws-sdk-rekognition,aws-sdk-resourcegroups,aws-sdk-resourcegroupstaggingapi,aws-sdk-route53,aws-sdk-route53domains,aws-sdk-s3,aws-sdk-sagemaker,aws-sdk-sagemakerruntime,aws-sdk-secretsmanager,aws-sdk-serverlessapplicationrepository,aws-sdk-servicecatalog,aws-sdk-servicediscovery,aws-sdk-ses,aws-sdk-shield,aws-sdk-simpledb,aws-sdk-sms,aws-sdk-snowball,aws-sdk-sns,aws-sdk-sqs,aws-sdk-ssm,aws-sdk-states,aws-sdk-storagegateway,aws-sdk-support,aws-sdk-swf,aws-sdk-transcribeservice,aws-sdk-translate,aws-sdk-waf,aws-sdk-wafregional,aws-sdk-workdocs,aws-sdk-workmail,aws-sdk-workspaces,aws-sdk-xray\nWorks\nIt looks like the \"bad\" dependency list is exactly 40 characters too long; the issue is irrelevant to the number of gems, but rather the length of the query params.. ",
    "jdblack": "@awood45   The  service gem alone does not seem to be sufficient:\njblack@ip-10-4-5-205:~$ gem list | grep ec2\naws-sdk-ec2 (1.35.0)\njblack@ip-10-4-5-205:~$ ruby -v\nruby 2.5.1p57 (2018-03-29 revision 63029) [x86_64-linux-gnu]\njblack@ip-10-4-5-205:~$ cat test.rb\nrequire 'aws-sdk-ec2'\nREGION='us-west-1'\nec2 = Aws::EC2.Client(region: REGION)\njblack@ip-10-4-5-205:~$ ruby test.rb\nTraceback (most recent call last):\ntest.rb:5:in <main>': undefined methodClient' for Aws::EC2:Module (NoMethodError). ",
    "mdunc": "Ran in to this today.  Installed Ruby 2.5 via RVM on a fresh Ubuntu 14.04 instance and ran gem install aws-sdk and it gets stuck in an endless loop of downloading gemspec.rz files.  . ",
    "multani": "I reported the issue on the RubyGems website.. ",
    "chrisbrooksca": "We ran into this as well. Paired it down to just install what we needed 'aws-sdk-s3' and we're back in business.. ",
    "kingpalethe": "This is now happening for me on a fresh install of Ubuntu 18.04 with Ruby 2.5.1. ",
    "soultech67": "I'm having this issue on osx Mojave with Ruby 2.5.3\n. ",
    "halostatue": "I was briefly looking at the JSON, and it looks like all that\u2019s needed on that is a change to add \"auth_type\": \"none\" to the \"InitiateAuth\" operation in the JSON service description. Except maybe the tests. But I\u2018ll leave that to people who know how the code is generated.. ",
    "jclusso": "Really excited to be able to use this feature with S3! Wondering how far along this is?. @cjyclaire oh that's great! I'm going to pull it in and see if I can get it working if that's the case!. Update: ended up getting it to build but realized it'll be almost impossible to try and include in a project with all the resources that are referenced. \nGuess I'll just have to wait until this gets officially merged. :(\n@cjyclaire not sure what I'm doing wrong. Originally didn't realize I had to build the gem but now after merging the pull requests for flow and binary-encode-decode i get \"LoadError: cannot load such file -- aws-eventstream\" when trying to build. Am I missing something?. ",
    "nunofmendes": "@cjyclaire  Thank you for your reply.\nI have ran some more tests and this is the result:\nSo, If I run in a separated ruby file everything checkouts ok\nIf I call the function with that code as HTTP POST everything checkouts ok\nIf I call the function (which is a task in the scheduler) with rake, it gives that crash.\n. Ok. Fixed. Was related with double call requiring ActionView.. ",
    "Fryguy": "@trevorrowe Please review?. Gah...I missed this comment...I'll add it right away.. @cjyclaire Oh, it seems I've already done that... https://github.com/aws/aws-sdk-ruby/pull/1760/files#diff-4fe8ce5547b24bcbfe5ebd2206060363R5. @trevorrowe @cjyclaire bump?. @cjyclaire Done!. ",
    "Nitrodist": "From my local code that shows the keys available:\n[1] pry(main)> signature.headers\n=> {\"host\"=>\"s3.amazonaws.com\",\n \"x-amz-date\"=>\"20180417T172056Z\",\n \"x-amz-content-sha256\"=>\"foobarbaz\",\n \"authorization\"=>\n  \"AWS4-HMAC-SHA256 Credential=foobarbaz/20180417/us-east-1/s3/aws4_request, SignedHeaders=host;x-amz-content-sha256;x-amz-date, Signature=foobarbaz\"}. ",
    "willbengtson": "Thanks, added this to the CHANGELOG.md.  Let me know if you need the text changed.. Ok, made the wording change.  Let me know what you think now.. ",
    "inokappa": "@awood45 \nThank you for your response.\n\nCan you share your Gemfile.lock? Just want to check something on that.\n\nPlease confirm following.\n```ruby\nGEM\n  remote: https://rubygems.org/\n  specs:\n    aws-partitions (1.80.0)\n    aws-sdk-core (3.19.0)\n      aws-partitions (~> 1.0)\n      aws-sigv4 (~> 1.0)\n      jmespath (~> 1.0)\n    aws-sdk-kms (1.5.0)\n      aws-sdk-core (~> 3)\n      aws-sigv4 (~> 1.0)\n    aws-sdk-s3 (1.9.1)\n      aws-sdk-core (~> 3)\n      aws-sdk-kms (~> 1)\n      aws-sigv4 (~> 1.0)\n    aws-sigv4 (1.0.2)\n    coderay (1.1.2)\n    jmespath (1.4.0)\n    method_source (0.9.0)\n    pry (0.11.3)\n      coderay (~> 1.1.0)\n      method_source (~> 0.9.0)\nPLATFORMS\n  ruby\nDEPENDENCIES\n  aws-sdk-s3 (~> 1)\n  pry\nBUNDLED WITH\n   1.16.1\n```\nRegards,\nYohei. ",
    "rpbaptist": "Alright, thanks very much!. ",
    "jtstrohl": "Figured out the problem.  WinRM has it's own memory cap.  I realized that we had hard-coded the WinRM MaxMemoryPerShellMB parameter to 1024MB (apparently too restrictive) in the powershell script we used to set up WinRM for Packer remote execution (See example here: http://blog.petegoo.com/2016/05/10/packer-aws-windows/).  Once we increased from 1024 to 4096 MB in the WinRM bootstrap script, it started working.. ",
    "equivalent": "Ruby 2.5.0. Thank you for response\n:thinking: \nInteresting, I've refactor all my code from:\nruby\nsg_client = Aws::EC2::SecurityGroup.new(security_group_id, client)....)\nsg_client.re voke_ingress(...)\nto just\nruby \nec2_client =  Aws::EC2::Client.new(region: '.....', credentials: credentials)\nec2_client.revoke_security_group_ingress(....)\nand everything works now\n\nplus instead group_id (sg-xxxxxx) I now usepoh  group_name \n\nand everything works now. Don't know why. It seems there is some WooDoo magic goving on with Aws::EC2::SecurityGroup instances \n\nI didn't change any permissions around those credential tokens at all just refactor the code\n\nWorks form me, I hope it will help some googling devs. I'm ok with that, will change this in next commit.  ...please let me know if you want me to squash commits\n. ",
    "ioquatix": "For example, if you DID intend to redefine a method, use undef. As an example for why this is a problem, I've seen situations where methods were clobbered by accident.. @awood45 I don't believe this should be targeted at a major version, since no actual API changes would need to occur, this is purely \"patch level\" changes. I've made some PRs which incrementally improve the situation.\nI will supply some more PRs to improve the situation. You should be able to merge them with no concern as to functionality/changes.. I made a PR to fix warnings in https://github.com/mustache/mustache/pull/241\nThis will require an on-going commitment, but I hope I've sowed the first seeds.. @awood45 Thanks so much for spending time on this issue. Let me know if there is anything I can do to help. Once you merge the existing PRs, I will be happy to find more. I'll also try to apply some pressure to mustache to get those warnings fixed too.. @awood45 Any update to this?. What was the outcome? Did we manage to get everything merged in the end? Can I help by making other PRs to reduce warnings?. By the way, due to some of the warnings generated, some specs actually fail. That's a good indication that some areas of the code require more scrutiny.. See also https://github.com/aws/aws-sdk-ruby/issues/1778. The warnings from mustache should be fixable, there is an open issue here: https://github.com/mustache/mustache/issues/224. Bump :). Bump :). Hello, do you think you can either merge this PR or tell me why it can't be merged? Thanks.. I am making a single PR.. ",
    "filiptepper": "Thanks for the update @awood45. Is there an issue / PR I could follow?. @awood45 We have rolled out this PR as a monkey patch in our code. We're going to remove it next time we try to upgrade aws-sdk. Thanks!. ",
    "msuzoagu": "So credentials are needed for this tutorial. I am closing this issue but think that docs need to be updated. . ",
    "diachini": "It looks like at least a part of this was maybe undone here:\nhttps://github.com/aws/aws-sdk-ruby/commit/f4b3251658ee1d95403d7a851ace8495ea4115af#diff-eba7bf5f9cb1f1ef94f03eeba1444966L24 \nand here: https://github.com/aws/aws-sdk-ruby/commit/aca0d7cb31d1715b60711cbb907623fab498b595\n(I was updating the aws-sdk-s3 gem in one of my teams' projects, and was just hoping to get an understanding of why this was added and then removed?). ",
    "aryeh-looker": "@awood45 \u2014\u00a0to some degree, yes. There are maybe 3-5 different threads (sometimes more, but typically 3-5) that could be using the Aws::S3 library at any given time. Occasionally, more than one thread might be using the library at the same time (for example multiple threads could be pushing a file to an S3 bucket \u2014 our main use case \u2014 at the same time).. Little more background \u2014\u00a0in our case, bucket/region pairs are user-provided (and we have many users; not a simple case of server misconfiguration, etc.). It will likely be possible for us to work around this bug by using the #get_bucket_location API before making calls to upload files to S3. . Looks like this is due to a bug in JRuby that is slated to be fixed in the 9.1.18.0 release https://github.com/jruby/jruby/issues/5203#issuecomment-408656779.. ",
    "domcleal": "Thanks for the fast response @awood45 and @cjyclaire! Confirmed with a few tests that memory usage is back to normal on 3.21.1.. ",
    "alexagranov": "If accepted, I can make a similar PR against version2 branch.. Updated spec to also pass on 1.9.3 (was using OpenStruct but no to_h on 1.9.3 \ud83d\ude22). Hi, thanks for getting back to this.\nI do see the merit in requiring clients of an SDK to explicitly provide proper objects to marshal - otherwise raise an exception.  \nBut then couldn't one make the similar argument for not automatically calling to_s on provided objects?\nI think to_h on Ruby objects has reached sufficient ubiquity that automatically marshaling with it if found could be reasonable.  Up to you of course but made sense to me to offer.\nCurrent state of PR is backward compatible to 1.9.3.. Sounds good, let me know what you decide.  Happy to tackle specs from a different perspective/direction if you have something in mind.. ",
    "vincentdasari": "Yep. I'm on 2.9.5. I just updated to 3.0.1 and that seems to have done the trick. Thank you!. ",
    "duncaan": "Sure - here's my output:\n```\n<- \"POST / HTTP/1.1\\r\\nContent-Type: application/x-www-form-urlencoded; charset=utf-8\\r\\nAccept-Encoding: \\r\\nUser-Agent: aws-sdk-ruby2/2.11.50 ruby/2.5.1 x86_64-darwin15\\r\\nX-Amz-Date: 20180608T205641Z\\r\\nHost: ec2.us-east-1.amazonaws.com\\r\\nX-Amz-Content-Sha256: blah\\r\\nAuthorization: AWS4-HMAC-SHA256 Credential=blah/20180608/us-east-1/ec2/aws4_request, SignedHeaders=content-type;host;x-amz-content-sha256;x-amz-date, Signature=blah\\r\\nContent-Length: 63\\r\\nAccept: /\\r\\n\\r\\n\"\n-> \"HTTP/1.1 200 OK\\r\\n\"\n-> \"Content-Type: text/xml;charset=UTF-8\\r\\n\"\n-> \"Content-Length: 1229\\r\\n\"\n-> \"Date: Fri, 08 Jun 2018 20:56:40 GMT\\r\\n\"\n-> \"Server: AmazonEC2\\r\\n\"\n-> \"\\r\\n\"\nreading 1229 bytes...\n-> \"\"\n-> \"<?xml version=\\\"1.0\\\" encoding=\\\"UTF-8\\\"?>\\n\\n    867f9668-a70b-420a-99c8-73fba8a520b0\\n    \\n        \\n            ami-c5cf55ba\\n            ubuntu-images-us-east-1-release/xenial/20180522/hvm/instance-store/image.img.manifest.xml\\n            available\\n            099720109477\\n            2018-05-22T19:53:03.000Z\\n            true\\n            x86_64\\n            machine\\n            simple\\n            ubuntu/images/hvm-instance/ubuntu-xenial-16.04-amd64-server-20180522\\n            Canonical, Ubuntu, 16.04 LTS, amd64 xenial image build on 2018-05-22\\n            instance-store\\n            \\n            hvm\\n            xen\\n            true\\n        \\n    \\n\"\n<- \"POST / HTTP/1.1\\r\\nContent-Type: application/x-www-form-urlencoded; charset=utf-8\\r\\nAccept-Encoding: \\r\\nUser-Agent: aws-sdk-ruby2/2.11.50 ruby/2.5.1 x86_64-darwin15\\r\\nX-Amz-Date: 20180608T205642Z\\r\\nHost: ec2.us-east-1.amazonaws.com\\r\\nX-Amz-Content-Sha256: blah\\r\\nAuthorization: AWS4-HMAC-SHA256 Credential=blah/20180608/us-east-1/ec2/aws4_request, SignedHeaders=content-type;host;x-amz-content-sha256;x-amz-date, Signature=blah\\r\\nContent-Length: 1497\\r\\nAccept: /\\r\\n\\r\\n\"\n-> \"HTTP/1.1 400 Bad Request\\r\\n\"\n-> \"Transfer-Encoding: chunked\\r\\n\"\n-> \"Date: Fri, 08 Jun 2018 20:56:43 GMT\\r\\n\"\n-> \"Connection: close\\r\\n\"\n-> \"Server: AmazonEC2\\r\\n\"\n-> \"\\r\\n\"\n-> \"134\\r\\n\"\nreading 308 bytes...\n-> \"<?xml version=\\\"1.0\\\" encoding=\\\"UTF-8\\\"?>\\nInvalidParameterCombinationThe terminate InstanceInterruptionBehavior is not supported when requestType is set to persistent.d596d9e9-6d3a-4fbf-a292-b7773d0a2897\"\nread 308 bytes\nreading 2 bytes...\n-> \"\\r\\n\"\nread 2 bytes\n-> \"0\\r\\n\"\n-> \"\\r\\n\"\nConn close\n. The spot data structure should look something like this, very simple:\n{\n  max_price: \".50\",\n  spot_instance_type: \"persistent\"\n}\n. Here is what we're sending to this method: https://docs.aws.amazon.com/sdkforruby/api/Aws/EC2/Client.html#run_instances-instance_method\n{:min_count=>1, :max_count=>1, :key_name=>\"key_name\", :instance_type=>\"c3.large\", :placement=>{:availability_zone=>\"us-east-1d\"}, :security_groups=>[\"default\", \"sg\"], :image_id=>\"ami-c5cf55ba\", :user_data=>\"some_user_data\", :tag_specifications=>[{:resource_type=>\"instance\", :tags=>[{:key=>\"LaunchedBy\", :value=>\"me\"}, {:key=>\"EnvironmentName\", :value=>\"env\"}, {:key=>\"Role\", :value=>\"role\"}]}], :instance_market_options=>{:market_type=>\"spot\", :spot_options=>{:max_price=>\"0.35\", :spot_instance_type=>\"persistent\"}}}\n. @awood45 - I have attempted to set the termination attribute, this works with the 'one-time' attribute, but with the 'persistent' attribute, the request is still not functioning.  Here's my request and output from the API:\n{:min_count=>1, :max_count=>1, :key_name=>\"key\", :instance_type=>\"c3.large\", :placement=>{:availability_zone=>\"us-east-1d\"}, :security_groups=>[\"default\", \"sg\"], :image_id=>\"ami-c5cf55ba\", :user_data=>\"user_data\", :tag_specifications=>[{:resource_type=>\"instance\", :tags=>[{:key=>\"LaunchedBy\", :value=>\"me\"}, {:key=>\"EnvironmentName\", :value=>\"env\"}, {:key=>\"Role\", :value=>\"some_role\"}]}], :instance_market_options=>{:market_type=>\"spot\", :spot_options=>{:max_price=>\"0.35\", :spot_instance_type=>\"persistent\", :instance_interruption_behavior=>\"terminate\"}}}\nERROR: Aws::EC2::Errors::InvalidParameterCombination: The terminate InstanceInterruptionBehavior is not supported when requestType is set to persistent.\n```. The post you are referring to references EBS-backed instances.  If we provision an instance-store type, there is no option that works appropriately.  I've opened a support case to address this issue.. ",
    "delucas": "This is indeed the answer. ",
    "maletor": "Hi @cjyclaire, \nYes, of course. This is the error. I apologize for not originally including it.\nxml\n<Error><Code>AccessDenied</Code><Message>There were headers present in the request which were not signed</Message><HeadersNotSigned>x-amz-meta-foo</HeadersNotSigned><RequestId>CBA5E63921D90B41</RequestId><HostId>+ZvE/GlIZAb09kAx+LUbFEDe472/gQ3glorIHfeSuNXJd3/NZ5jK0KEkI37QCy12teeAV2tVzBA=</HostId></Error>\nIn order to successfully add the user specific metadata, you must send the request with the desired metadata. And in order to have the request authenticate with a presigned URL, the headers must match. So in your test code snippet, add the headers: x-amz-meta-foo: bar and see that you will get the above error message back. And if you omit the headers, you will see that x-amz-meta-foo: bar is not added to the object.. Hey, you're right. The client/uploader does not need to pass the headers back to S3 when uploading. In fact, doing so results in the XML error message. This requires the server to know the key and value of each of attribute it wants to set.\nWhat if the payload is variable? Say for something like x-amz-meta-x-amz-unencrypted-content-length?. @awood45 it's not a new header but a different value for user metadata. We can live without x-amz-meta-x-amz-unencrypted-content-length but it'd be nice to offer the client (iOS) be able to set a dynamic value for it.. @awood45, our client could pass arguments to our API specifying what it would like for those dynamic values, such as x-amz-unencrypted-content-length or Content-MD5. I don't think there would be any security issues with that.. ",
    "ilrock": "Thanks for the answer @cjyclaire. I had the ENV vars set in my .env (used in the previous version of the gem). I've realized that the names of my variables were a tad different from what was expected. Sorry to have bothered.\nCheers!. ",
    "shibz": "Getting conflicting information.  You're suggesting here to use individual gems rather than \"aws-sdk\" however the official documentation seems to suggest the opposite:\nhttps://aws.amazon.com/sdk-for-ruby/\n```\nInstall the gem: \ngem install aws-sdk\nor Add to Gemfile: \ngem 'aws-sdk', '~> 3'\n```. ",
    "AngelCantugr": "@cjyclaire - CHANGELOG.md got updated. Thanks for the suggestion \ud83d\ude04 . I've updated the PR accordingly.. I had to rollback the change since it broke the build using Ruby 1.9.3 https://travis-ci.org/aws/aws-sdk-ruby/builds/398037219.\nAfter doing some investigation in the Aws::Structure , I found out that the Aws::Structure#to_h has an alias https://github.com/aws/aws-sdk-ruby/blob/master/gems/aws-sdk-core/lib/aws-sdk-core/structure.rb#L46 . So both #to_hash and #to_h point to the same method in the Aws::Structure class.. ",
    "akostadinov": "hmm, I just now notice it is some rvm ruby install. Seems like something broken with the installation.. ",
    "fribeiro1": "Sure, the AWS S3 API is currently implemented by several non-AWS services (e.g: Oracle, DigitalOcean), some of which require payload signing at all times unlike AWS S3 itself.\nLet me know if you need more information before adding the feature request.. Thanks!. ",
    "yvancastilloux": "Hi, we've seen this issue on our side too. We upload a file that has 100MB+ and the memory use is around 100MB. This fix is definitely needed. What's preventing this small fix to be merged? I see that the tests are not passing.... Awesome! Can't wait to try this out!. ",
    "raymondhoagland": "Okay I believe I've made it through that error by adding omit_default_port => true to the excon connection.  \nNow I'm seeing an error that the Transfer-Encoding header is unimplemented, which seems to rule out a simple chunked request through excon.  The reason I had been looking at chunked requests is at one point I saw an error that said that  the Content-Length header wasn't set and Transfer-Encoding wasn't set to chunked (I was incorrectly trying to upload the file as multipart form-data).  If it's not supported that seems a bit misleading.\nI see in the AWS docs for S3 there is a Content-Encoding header which can be set to aws-chunked to provide chunking functionality - is that supported for Device Farm/the aws-sdk-ruby gem?  If so, is there an example/way of doing so in Ruby that someone could point out?  Or am I going in the wrong direction trying to get a chunked request go work?. ",
    "adammw": "@awood45 any update on this? saw an increase of them around 2018-08-06 13:06:48 UTC which triggered a massive exponential backoff in our fluentd instance.. ",
    "kplimack": "@cjyclaire this just started yesterday. We accidentally changed the IAM policy attached to this user which caused access-denied errors.  Once we fixed the IAM issue, this error cropped up.  I've tried a few different files and they all have the same issue. The files range in size from 5-5.6G. and Yes, its a multithreaded environment. ruby\n[3] pry(#<Aws::S3::MultipartFileUploader>)> threads\n=> [#<Thread:0x0000000806dbe8@/opt/chef/embedded/lib/ruby/gems/2.3.0/gems/aws-sdk-s3-1.12.0/lib/aws-sdk-s3/multipart_file_uploader.rb:129 sleep>,\n #<Thread:0x0000000818da50@/opt/chef/embedded/lib/ruby/gems/2.3.0/gems/aws-sdk-s3-1.12.0/lib/aws-sdk-s3/multipart_file_uploader.rb:129 sleep>,\n #<Thread:0x000000082a6248@/opt/chef/embedded/lib/ruby/gems/2.3.0/gems/aws-sdk-s3-1.12.0/lib/aws-sdk-s3/multipart_file_uploader.rb:129 sleep>,\n #<Thread:0x000000083d5538@/opt/chef/embedded/lib/ruby/gems/2.3.0/gems/aws-sdk-s3-1.12.0/lib/aws-sdk-s3/multipart_file_uploader.rb:129 sleep>,\n #<Thread:0x0000000854e1f8@/opt/chef/embedded/lib/ruby/gems/2.3.0/gems/aws-sdk-s3-1.12.0/lib/aws-sdk-s3/multipart_file_uploader.rb:129 sleep>,\n #<Thread:0x00000007ff52d8@/opt/chef/embedded/lib/ruby/gems/2.3.0/gems/aws-sdk-s3-1.12.0/lib/aws-sdk-s3/multipart_file_uploader.rb:129 sleep>,\n #<Thread:0x00000006c5e918@/opt/chef/embedded/lib/ruby/gems/2.3.0/gems/aws-sdk-s3-1.12.0/lib/aws-sdk-s3/multipart_file_uploader.rb:129 sleep>,\n #<Thread:0x00000005d130d0@/opt/chef/embedded/lib/ruby/gems/2.3.0/gems/aws-sdk-s3-1.12.0/lib/aws-sdk-s3/multipart_file_uploader.rb:129 sleep>]\n[4] pry(#<Aws::S3::MultipartFileUploader>)> threads[0]\n=> #<Thread:0x0000000806dbe8@/opt/chef/embedded/lib/ruby/gems/2.3.0/gems/aws-sdk-s3-1.12.0/lib/aws-sdk-s3/multipart_file_uploader.rb:129 sleep>\n[5] pry(#<Aws::S3::MultipartFileUploader>)> threads[0].value\nfatal: No live threads left. Deadlock?\nfrom (pry):20:in `value'\n[11] pry(#<Aws::S3::MultipartFileUploader>)> threads[0].status\n=> \"sleep\"\n[12] pry(#<Aws::S3::MultipartFileUploader>)> threads[0].alive?\n=> true. @cjyclaire I just made a 10Mb file with dd and it uploaded right away, this might be related to the filesize.  . @cjyclaire i also tried using the aws-cli and using s3-cp, it worked, too.  something with the ruby libs seems to be the issue . \ud83d\udc4d thank you! . ",
    "peggles2": "I am having the same problem. When it gets the Aws::S3::MultipartFileUploader and creates the threads the whole thing just hangs. . Hi, not sure.  How can I check if it is or not multithread env?  Its running with thin.. ",
    "designstuhl": "Thank you @cjyclaire !\nThat totally did it. calling the describe_images() api method on the instance.client allows me to find the list of AMI's so long as I enter in some type of filtering mechanism. \nIn case anyone else is looking for this, calling .describe_images() on your instance.client will not yield anything, but if you know the owner ID of the AMI's you can filter by owners: [\"string\"] and return all of your AMI's that way. \nAlso here is the link again that @cjyclaire provided:\nhttps://docs.aws.amazon.com/sdk-for-ruby/v3/api/Aws/EC2/Client.html#describe_images-instance_method. ",
    "philsmy": "To be honest, no. We stuck with drewblas for mailing. . ",
    "gettalong": "Why would you use the --enable-frozen-string-literalRuby option instead of the the # frozen_string_literal: true comment? What's the benefit\nThe latter is completely under your control whereas the former is not. So the option can only meaningfully be used if everything is under your control. If you use gems, you are out of luck.\nAs for kramdown: I will change it to use the frozen string literal comment some time in the future (also see https://github.com/gettalong/kramdown/pull/512).. ",
    "plentz": "@gettalong thanks for the quick response! the option was more of a desired thing than a required thing. I just think that it's better to have everything moving in the \"right\" direction (maybe put that as an allowed-to-fail build step). . I think that the branch is ready now. I've put the # frozen_string_literal: true just on the files that it caused a problem to appear, to make it easier to review - but we can add it to all files - and I think it's the best option.. yep, it is related to freezing strings. but tests only break here if we add frozen_string_literal to:\nrspec ./build_tools/aws-sdk-code-generator/spec/interfaces/client/event_stream_spec.rb\nrspec ./build_tools/aws-sdk-code-generator/spec/protocols_spec.rb\nrspec ./gems/aws-eventstream/spec/bytes_buffer_spec.rb\nrspec ./gems/aws-eventstream/spec/decoder_spec.rb\nrspec ./gems/aws-eventstream/spec/encoder_spec.rb\nbut I added in this PR just what needs to be changed to fix the problems I've found. I think that we can run\n```\npragmater --add --comments \"# frozen_string_literal: true\" --includes \"Gemfile\" \"Guardfile\" \"Rakefile\" \".gemspec\" \"config.ru\" \"bin//*\" \"/.rake\" \"/.rb\"\n````\nafter merging this to ensure that everything stays compatible ;) [and that's how I found this kind of change]. yes, this one is directly related to the change we're doing to the string in the line above. ",
    "tommybonderenka": "I was thinking it was probably an issue with the service as well but wasn't sure. I don't have a wire trace of any of the problematic calls as of yet, though I will look into adding that to the calls to try and capture some more information. . I do not have a way to reproduce it predictably at this point, it seems very random. The records that it affects have almost nothing in common and the timestamps vary greatly, it just kinda \"happens\" sometimes. . ",
    "smcavallo": "@awood45 - thank you!  This has now been refactored as a Presigner class.\nI used the presigner classes from S3 and Polly as examples.\nI definitely would like to expose the presigner functionality for STS.  The new EKS services all use auth tokens which are based on the get_caller_identity presigned_url.  It would be nicer to have a helper method in the sdk to retrieve a token, but creating a presigner for get_caller_identity will at least get part of the way there.  . @awood45 - hoping the formatting looks good now - changed it back to how it was before. ",
    "arianf": "@srchase @awood45 I've created a pull request here: #1889 . Sorry for the delay, had to verify it was okay for me to make this statement.\nBy submitting this pull request, I confirm that my contribution is made under the terms of the Apache 2.0 license.. Oh, didn't realize there is a different changelog file, I've updated the PR to use the correct one.. ",
    "allcentury": "@awood45 yes we've seen the errors in active job and in web requests (via unicorn).  @srchase thank you, i've added the info.. ",
    "softwaregravy": "huh. Everything is the same. I can cause the exception and remove it simply by changing my gemfile. \nI'll try and make a one-off example that breaks to rule out anything else in my project interfering.. I have created a dummy project that can recreate the issue\nhttps://github.com/softwaregravy/aws_sdk_test. it was still breaking in my reproduction app, but the problems suddenly topped. I can't explain. I guess we'll just assume user error for now. Sorry for not responding sooner. I'll re-open when I'm ready to re-investigate.. ",
    "vbalazs": "\nWhat version of Shoryuken are you using?\n\n@srchase sorry, I forgot to share that detail, we are using the latest one, 3.2.3\n\nDo you ever get this error where Shoryuken is not used?\n\nso far no, only experiencing the issue within the worker, looks like this:\n```ruby\nclass EventsWorker\n  include Shoryuken::Worker\nshoryuken_options queue: \"production_events\", body_parser: :json\nDEFAULT_SHORT_DELAY = 1.second\ndef self.queue_up(data)\n    data[:data] = Oj.dump(data[:data]) if data[:data].is_a?(Hash)\n    perform_in(DEFAULT_SHORT_DELAY, data.merge(action: :process))\n  end\ndef perform(sqs_msg, data)\n    case data.delete(\"action\")\n    # when \"..\" ....\n    when \"process\"\n      _process(data)\n    end\n    sqs_msg.delete\n  end\ndef _process(event_data)\n    # ... heavy db operations, 3rd party service connection, etc\n  end\nend\n```\nWe use the following shoryuken configuration in production:\nconcurrency: 25\ndelay: 10\nLet me know if you need more information.. @srchase it happened 8 days ago with aws-sdk-core=3.27.0, aws-sdk-sqs=1.6.0 and shoryuken=3.2.3.\nI spent some time going through the code and investigating it (since it cannot be reliably reproduced, i tried with a minimal example and with a few thousands messages).\nSomehow when a HandlerListEntry gets copied, inserted is missing from the options \ud83e\udd14 \nIt is also independent from the operation because I saw it happening on get_queue_attributes as well as delete_message requests.\nNot sure what's going on. If noone else is experiencing this issue and we cannot reproduce it, I am okay with closing it since we cannot make any progress on it.\nOnce someone else comes around maybe we will start seeing some pattern.. ",
    "xyr115": "Sorry, I figured it out...I was mucking around and found that I wasn't accessing the structs properly.\nThis was version 3\nAnd I ran:   print volume.attachments[0].instance_id\nWhile a good learning experience, and because v3 departs from v2 syntax it would be helpful if the documentation provided real-world, non-foo based examples.\nE.g. to get the data, you'd run volume.data giving us the nested struct, etc.\nOr at the very least, the output could be much cleaner in the documentation:\n=begin\n<Aws::EC2::Volume:blah\nvolume.id gives us....\n\n@id=\"blah\",\nvolume.data gives us....\n\n@data=#<struct Aws::EC2::Types::Volume\n  # volume.attachments gives us....\n\nattachments=\n  [\n    #\n  ],\netc.....\n=end\n. ",
    "jemmyw": "I put a trace in Aws::Query::Handler#parse_xml and the xml it is passing on to get parsed does indeed look corrupted:\n<?xml version=\"1.0\"?>\n<ReceiveMessageResponse xmlns=\"http://queue.amazonaws.com/doc/2012-11-05/\">\n  <ReceiveMessageResult/>\n  <ResponseMetadata>\n  <RequestId>6dc5f341-60f4-5633-a086-a744f1bfd211</RequestId>\n  </ResponseMetadata>\n  </ReceiveMessageResponse>\nR3mlSuIll7xSXzbbuYkLsmFSa/x2RyKv+m3w5dytEXHbbZ4gbIf+WIaz6yskq2R2IkQFIzSSp4yl3bQsOjvVNuL6N7lZxE56ksDiPgKB8VlY1EmwiejoyztkHYhFq4qKD65gwJSpxLMjXWywIY4wQ4n3c4E6DIWGD2Wh9+wlMXzmh+Cb3WkmdvhlKzcQcz4pABg8LVOFGkEcDz/998Px4KufcqN/dHLuD6j1ushB4kpqFkPVJGFu78+JDBVvNLwtRdDCCuyVO2MTPUviit+ijOQunX4BO5+NLcpliMH1a37WYQ2RXF1yQA7XqucHrOxQ/wpomUztqexRpWopyEo++lCEnF4L1+VktFRK4HwYoNttqOhGN1JDnoOAnCt935+TmDEdhY7Y=</ReceiptHandle>\n<MD5OfBody>6b6914ef16eb16c397db1b2395ba4836</MD5OfBody>\n<MD5OfMessageAttributes>98375091b2eb5cb729b45eb0ff7104e4</MD5OfMessageAttributes>\n<Body>{&quot;response_id&quot;:428455}</Body>\n<Attribute><Name>SenderId</Name><Value>AROAIFG2WFJF2EOK2CAN4:e9555382-abb4-40f5-aea5-ecd393ca9151</Value></Attribute>\n<Attribute><Name>ApproximateFirstReceiveTimestamp</Name><Value>1535872479743</Value></Attribute>\n<Attribute><Name>ApproximateReceiveCount</Name><Value>2</Value></Attribute>\n<Attribute><Name>SentTimestamp</Name><Value>1535872479736</Value></Attribute>\n<MessageAttribute><Name>shoryuken_class</Name><Value><StringValue>LoadResponseCampaignData</StringValue><DataType>String</DataType></Value></MessageAttribute>\n<MessageAttribute><Name>trace_id</Name><Value><StringValue>1-5b8b8ddf-32759796d9ceb2905b210b98</StringValue><DataType>String</DataType></Value></MessageAttribute>\n</Message>\n</ReceiveMessageResult>\n<ResponseMetadata>\n<RequestId>dd6d4b2c-e3d8-56b5-8004-1f2972e53d26</RequestId>\n</ResponseMetadata>\n</ReceiveMessageResponse>. Hi, it looks like it. Here is a second example that I collected:\n<?xml version=\"1.0\"?>\n<ReceiveMessageResponse xmlns=\"http://queue.amazonaws.com/doc/2012-11-05/\">\n<ReceiveMessageResult/>\n<ResponseMetadata>\n<RequestId>\n9f08d265-0b58-5486-a2de-866f9d529f77</RequestId>\n</ResponseMetadata>\n</ReceiveMessageResponse>\n8lZbowRVFBN96GWFllu9iJ8odbck23rd1X5P6+PFOBtLIuNvBFhif8k4heYk0cKor8FLIf1GDPIMK2JrOWtZT1gNoo/uqma1rTEoFdlGeyjTNEcZiVTzdVxt+vWfvrtvh7oEIEC6JVpm9XNists7kpoiJUD6kzVXIlTK0AjIOREP2ojreyDXFJ2kn7dWf47ndKXgecAR0ZOhoFix3uZ/G1Z7JV8MIdHoIf2HKVKpVWOHcfcgnSv76CjrjjMOk/LbwS4p+MSKIw3mYsz2YWr+drfwLHrLXdpsGo5su3LHZs4LiMjzND/UEtDeL/WKhDT4FRhO+mqSf7EiqGJQVnQSw5sRwd0v369qMSr0M+E10hzu0mpxudg==</ReceiptHandle>\n<MD5OfBody>\nf0fcfdae9a6585cff532263197b72fdb</MD5OfBody>\n<MD5OfMessageAttributes>\na992083f4729a2c0164ac19f7cebfc35</MD5OfMessageAttributes>\n<Body>\n{&quot;record_import_id&quot;:626}</Body>\n<Attribute>\n<Name>\nSenderId</Name>\n<Value>\nAROAIFG2WFJF2EOK2CAN4:fcabd2d6-7ad3-48e6-a4ac-9c1011fd1959</Value>\n</Attribute>\n<Attribute>\n<Name>\nApproximateFirstReceiveTimestamp</Name>\n<Value>\n1536201674087</Value>\n</Attribute>\n<Attribute>\n<Name>\nApproximateReceiveCount</Name>\n<Value>\n1</Value>\n</Attribute>\n<Attribute>\n<Name>\nSentTimestamp</Name>\n<Value>\n1536201674067</Value>\n</Attribute>\n<MessageAttribute>\n<Name>\nshoryuken_class</Name>\n<Value>\n<StringValue>\nImportRecords</StringValue>\n<DataType>\nString</DataType>\n</Value>\n</MessageAttribute>\n<MessageAttribute>\n<Name>\ntrace_id</Name>\n<Value>\n<StringValue>\n1-5b9093c9-a1ab8e6c1e1d65a6df82ec8a</StringValue>\n<DataType>\nString</DataType>\n</Value>\n</MessageAttribute>\n</Message>\n</ReceiveMessageResult>\n<ResponseMetadata>\n<RequestId>\n7990238a-7876-5d61-8593-7780ed8de711</RequestId>\n</ResponseMetadata>\n</ReceiveMessageResponse>\nCompare with the next pickup which worked correctly:\n<?xml version=\"1.0\"?>\n<ReceiveMessageResponse xmlns=\"http://queue.amazonaws.com/doc/2012-11-05/\">\n<ReceiveMessageResult>\n<Message>\n<MessageId>\n3de7b1c6-e723-45c8-add9-c4e2b536d407</MessageId>\n<ReceiptHandle>\nAQEBmYmL29LOB70QNszrAV+GaiU9O14dFapg/duWW5L63aBk/1YLLHzLSJ5NKdbXErAPc9T29UycBsOiREKluXI84KJixVzxzS8Ev9azGqdKp9vFc53OE0xMvB7LLkZ72CJYzWPfnh62lwXQ/IP1o9DJ2c0QJq4QlBx1wn1iIA1C6+qLXG8hTtqmvOmjCM/gp3KDZRglokcF8dm4XDpDHZfay9szYcUBQjdoH1fXdyzpV/YxQWihupiLvraWUPPI2W/ijGKNEVWlnUUDaHd1seo/UPDXYL8P+MKW94AaRw7FOuEe0M25aIM3bHzL7KvevBhLgJv0gUOnCY9GX44lsU5IuytZfGFAznsNMIiu7ltJZKCXFSYinl7Y3iqa19FLeyKsfhpJw0LWzLNkro2fBH/3FxbtMRdKweEefAQSty8vABo=</ReceiptHandle>\n<MD5OfBody>\nf0fcfdae9a6585cff532263197b72fdb</MD5OfBody>\n<MD5OfMessageAttributes>\na992083f4729a2c0164ac19f7cebfc35</MD5OfMessageAttributes>\n<Body>\n{&quot;record_import_id&quot;:626}</Body>\n<Attribute>\n<Name>\nSenderId</Name>\n<Value>\nAROAIFG2WFJF2EOK2CAN4:fcabd2d6-7ad3-48e6-a4ac-9c1011fd1959</Value>\n</Attribute>\n<Attribute>\n<Name>\nApproximateFirstReceiveTimestamp</Name>\n<Value>\n1536201674087</Value>\n</Attribute>\n<Attribute>\n<Name>\nApproximateReceiveCount</Name>\n<Value>\n2</Value>\n</Attribute>\n<Attribute>\n<Name>\nSentTimestamp</Name>\n<Value>\n1536201674067</Value>\n</Attribute>\n<MessageAttribute>\n<Name>\nshoryuken_class</Name>\n<Value>\n<StringValue>\nImportRecords</StringValue>\n<DataType>\nString</DataType>\n</Value>\n</MessageAttribute>\n<MessageAttribute>\n<Name>\ntrace_id</Name>\n<Value>\n<StringValue>\n1-5b9093c9-a1ab8e6c1e1d65a6df82ec8a</StringValue>\n<DataType>\nString</DataType>\n</Value>\n</MessageAttribute>\n</Message>\n</ReceiveMessageResult>\n<ResponseMetadata>\n<RequestId>\nd4078f92-498d-5673-a91a-db98debe3924</RequestId>\n</ResponseMetadata>\n</ReceiveMessageResponse>. Hi,\nI put a trace on Seahorse::Client::Http::Response#signal_data and here I start seeing something more interesting. The data going through that method looks like this:\n<?xml version=\\\"1.0\\\"?><ReceiveMessageResponse xmlns=\\\"http://queue.amazonaws.com/doc/2012-11-05/\\\"><ReceiveMessageResult/><ResponseMetadata><RequestId>144e51a4-20dc-5bda-8c5f-05a841ed728e</RequestId></ResponseMetadata></ReceiveMessageResponse>\nAnd then parse_xml gets this:\n<?xml version=\\\"1.0\\\"?><ReceiveMessageResponse xmlns=\\\"http://queue.amazonaws.com/doc/2012-11-05/\\\"><ReceiveMessageResult/><ResponseMetadata><RequestId>144e51a4-20dc-5bda-8c5f-05a841ed728e</RequestId></ResponseMetadata></ReceiveMessageResponse>UFU/X5mM5/WCw0y5AQ/bDPxp6moq/gBd1rurIvNKOXYz/BKsUk7vBa82CjVgg6TfCZmuqHWQD7aXpXAQOvkMk8BUWEHr5+YLo6T80MHdiClno38mVr01jjEQZniuwtHfQEkLFJUZMewMZ2yPmkp2V6283D6QKA0tFJ6lBTJsjWL+JHBOYoKqocLTiY6b4nuqslqm6fAEnw+Uv0tpSTxklZYY7uK1S69XOT7Trh6YP6d3qyTjcl7YZ0zgkWfZtH1Ei3UBnDfvrH77sqpAeFxeqIze/x9e0YM6NCKmxVIHqIUOlMHFY7IB/uS/yXTuhRfKvepZpL8wniawvS7KbwxvsjBye0clSjTs7GmJcAgEoMRwgfS7PoA==</ReceiptHandle><MD5OfBody>3ef3a34c1e1bae7c6e67bf4473ecc4ce</MD5OfBody><MD5OfMessageAttributes>717f4cd2c0d7a6ad97c7bc924b8763e4</MD5OfMessageAttributes><Body>{&quot;record_import_id&quot;:659}</Body><Attribute><Name>SenderId</Name><Value>AROAIFG2WFJF2EOK2CAN4:d8373589-c19d-48a0-b34b-dc10466a6be5</Value></Attribute><Attribute><Name>ApproximateFirstReceiveTimestamp</Name><Value>1536319217194</Value></Attribute><Attribute><Name>ApproximateReceiveCount</Name><Value>1</Value></Attribute><Attribute><Name>SentTimestamp</Name><Value>1536319217183</Value></Attribute><MessageAttribute><Name>shoryuken_class</Name><Value><StringValue>ImportRecords</StringValue><DataType>String</DataType></Value></MessageAttribute><MessageAttribute><Name>trace_id</Name><Value><StringValue>1-5b925ef1-d2e540d0cf3d28547236384a</StringValue><DataType>String</DataType></Value></MessageAttribute></Message></ReceiveMessageResult><ResponseMetadata><RequestId>db3dc3ed-b57e-5352-8f01-97e1d451ca06</RequestId></ResponseMetadata></ReceiveMessageResponse>\nNote they have the same RequestId at the beginning, and then another RequestId in parse_xml. \nI can then trace that other RequestId and find signal_data was called with the correct XML another time.\nSo it seems like 2 requests are stomping on each other. This is running under shoyuken which is multithreaded, but with concurrency set to 1 so there should only be 1 thread performing these calls (and in any case this is the message fetch which as far as I can see in shoryuken is always executed in 1 thread anyway).. ",
    "BTheunissen": "I am having the same issue, I had to roll back to an older version of the gem \ud83d\ude1e . ",
    "valscion": "Yeah having the same issue as well. Can't upgrade to latest versions because of this.. Thanks for the quick reply. I also think this might unfortunately impact more gems. Also happening with aws-sdk-rds v1.38.0: https://github.com/aws/aws-sdk-ruby/issues/1918. Seems to be a duplicate of #1917. ",
    "benjamin-cribb": "This, or something similar, also seems to be effecting aws-sdk-ssm. 1.24.0, yes, same client error.  Rolling back to aws-sdk-ssm 1.22.0 fixed the problem. I had to update a bunch of other aws gems to use 1.25, but it also does work. Try a later version of sns. ",
    "makeitgo": "Thanks, I was able to update my bundle.  I had to upgrade my s3 which updated my core ....  bundle update aws-sdk-s3 We had another dependency that was locked to an earlier version.  The error is now gone.  Much happier. ",
    "mark100net": "@srchase That's perfect thanks a bunch. \nFor anyone copying the code above from @srchase note that source variable needs a '/' before 'mypath'.. ",
    "franciscopinheir": "Solved. I had two different versions of Ruby installed. Sorry and thank you for the reply ;). ",
    "maryshirl": "Thank you for your response @srchase.\nThe device wasn't previously confirmed. How do I confirm a device?\nI checked the device in the AWS console, and it's not listed. How do I add the device?\nMy understanding is that every time a user uses a new device, when the user signs in, the api returns a device key and I could use that device key to confirm the device. \n. Okay, thank you! Is there a time frame for this? I'm asking so that I know when to check this out again. Thanks again.. Thank you for the update @srchase!. ",
    "DenisUA": "Hello @srchase \nYes, I've checked that list. All countries are supported. \nWhen I'm trying to execute the second line parameters errors appearing:\nAws::SNS::Errors::InvalidParameter (Invalid parameter: )\nDo I miss something?\nThank you for your help. . @srchase \n\nNot all countries and regions support Sender IDs.\n\nFor sure, I meant all countries where my application in use supports this functionality.\nI've found an issue in a documentation above.\n... including at least one letter and no spaces.\nThank you!\nBut the problem is now that sender ID still the same. Instead of getting sms from \"Custom\" I'm receiving them from different Amazon numbers. \n. @srchase \nThank you for your time.. ",
    "jec": "I added some logging output so I could see the request body:\ncfg\nAction=CreateStack\nCapabilities.member.1=CAPABILITY_NAMED_IAM\nOnFailure=ROLLBACK\nParameters.member.1.ParameterKey=DbInstanceClass\nParameters.member.1.ParameterValue=db.t2.micro\nParameters.member.2.ParameterKey=DbStorageEncrypted\nParameters.member.2.ParameterValue=false\nParameters.member.3.ParameterKey=EnvType\nParameters.member.3.ParameterValue=nonphi\nParameters.member.4.ParameterKey=SourceDBInstanceIdentifier\nParameters.member.4.ParameterValue=arn:aws:rds:us-east-1:...\nParameters.member.5.ParameterKey=SourceRegion\nParameters.member.5.ParameterValue=us-east-1\nStackName=nonphi-dev-...\nTags.member.1.Key=name\nTags.member.1.Value=nonphi-dev-...\nTemplateBody=Description...\nVersion=2010-05-15\nThere is no preSignedUrl, so I'm not sure where it's coming from.. I tried adding PreSignedUrl: '' and preSignedUrl: '' to my template but those gave \"unsupported property\" errors.\nThis is starting to sound like it's possibly not related to the Ruby gem, but an underlying bug in the RDS replication.. In this specific case, this is a cross-region replication. As a result, the SourceRegion is required since the source is in a different region.. This appears to be an issue on the RDS backend. I have a support ticket open with AWS, and they are investigating.\nI was able to remove the SourceRegion parameter from my template, and it did successfully create a cross-region replica. I assumed that it was required because it's cross-region, but since the SourceDBInstanceIdentifier contains an ARN, and the ARN contains the source's region, SourceRegion is redundant.\nI'm closing this issue. Thanks for the help.. ",
    "jksnetwork": "@jec I was experiencing this exact same thing.  I was trying to create a cross-region replica but I did not need to use the SourceRegion property for some reason.  When I included it I received the same preSignedUrl error that you were seeing. Without it,  the replica was created successfully.  Definitely not how the documentation says it should work but it worked.. ",
    "jeyraof": "@srchase Thank you. ",
    "andrew-newell": "This looks like it could be related to https://github.com/aws/aws-sdk-ruby/issues/823 and https://github.com/aws/aws-sdk-core-ruby/issues/197 . > Stubbing for the current version does not support writing the response_target to a file.\n@srchase Can you point me to the documentation for this behavior please?. > @andrew-newell\n\nCan you give this a try instead?\n@s3_client = Aws::S3::Client.new(stub_responses: true)\n@s3_client.stub_responses(:get_object, body: 'this is a body')\n@s3_client.get_object(response_target: 'output.txt', bucket: 'myBucket', key: 'myKey')\n\nYep, that works. Thanks for your help.. ",
    "RUN-CMD": "\nCan you try this instead:\nqueue.receive_messages({message_attribute_names: [\"All\"]})\n\nThis worked for me, thank you !\nThis resolves my issue.. ",
    "philippevezina": "Yes, that's exactly what I was looking for! I couldn't find it in the source code. Thank!. ",
    "bituinb": "Hi,\nThanks for your reply, it's fixed now.\nChanging the configuration for kubernetes deployment for our app-frontend fixes the pods that host our staging environment.\nThank you\n. ",
    "CESteinmetz": "Thank you so much for your response, Alex. \nI am using these stubs to write rspec tests for a wrapper method which modifies an RDS instance using the modify_db_instance method call, and then calls the waiter to ascertain that the db instance is available for use before returning.\nThe wrapper method looks like this:\n```\ndef wrapper_method(client, params)\n ...\n      client.modify_db_instance(\n         ...\n      )\n  client.wait_until(:db_instance_available, client.describe_db_instances(db_instance_identifier: 'my-db-name']).db_instances[0].to_h)\n\nend\n``\nIf I remove thedescribe_db_instances` parameter from the waiter call, then the rspec test passes, as per your example.\nIn your example, because you stubbed describe_db_instances, does the rds client calling the waiter automatically \"bind\" the :db_instance_available to the rds instance defined in describe_db_instances? Would that hold true for a legitimate method call as well? Under what circumstances is it necessary to explicitly call the describe_db_instances method as a parameter to the waiter?. ",
    "lizdenhup": "Hi, thanks for your quick response!\nHere are the logs after setting http_wire_trace: true on the instance of the Aws::Lambda:Client class. \nstarting SSL for lambda.us-east-1.amazonaws.com:443...\nSSL established\n<- \"POST /2015-03-31/functions/excel-to-html-converter/invocations HTTP/1.1\\r\\nContent-Type: \\r\\nAccept-Encoding: \\r\\nUser-Agent: aws-sdk-ruby3/3.32.0 ruby/2.4.4 x86_64-linux aws-sdk-lambda/1.11.0\\r\\nHost: lambda.us-east-1.amazonaws.com\\r\\nX-Amz-Date: 20181022T221332Z\\r\\nX-Amz-Content-Sha256: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\\r\\nAuthorization: AWS4-HMAC-SHA256 Credential=<<redacted>>, SignedHeaders=host;x-amz-content-sha256;x-amz-date, Signature=4df02e4c6d7b659a9a632878590e71687b60edbe2353a4b1a4d6e9610a585110\\r\\nContent-Length: 0\\r\\nAccept: */*\\r\\n\\r\\n\"\n-> \"HTTP/1.1 200 OK\\r\\n\"\n-> \"Date: Mon, 22 Oct 2018 22:13:27 GMT\\r\\n\"\n-> \"Content-Type: application/json\\r\\n\"\n-> \"Content-Length: 53\\r\\n\"\n-> \"Connection: keep-alive\\r\\n\"\n-> \"x-amzn-RequestId: b379188c-d647-11e8-8c8e-65114965fb2f\\r\\n\"\n-> \"x-amzn-Remapped-Content-Length: 0\\r\\n\"\n-> \"X-Amz-Executed-Version: $LATEST\\r\\n\"\n-> \"X-Amzn-Trace-Id: root=1-5bce4b87-9954bf6b36d397b4c73ba36a;sampled=0\\r\\n\"\n-> \"\\r\\n\"\nreading 53 bytes...\n-> \"\\\"<table><tr class=\\\\\\\"strong\\\\\\\"><td>2</td></tr></table>\\\"\"\nread 53 bytes\nConn keep-alive. ",
    "wjaspers": "thats the code i meant to post, but was on my way out the door from work. the issue has been updated.. kms\nThis only seems to have recently broken as our codebase is only a few months old.. it may have. ill have to test changes to our lockfile and update this issue accordingly.. removing the STSClient averted the argument error, but instead returned an S3 Error with no details \nruby\nAws::S3::Errors::BadRequest ():\nfrom /lib/ruby/gems/2.3.0/gems/aws-sdk-core-3.30.0/lib/seahorse/client/plugins/raise_response_errors.rb:15:in `call'\n    from /lib/ruby/gems/2.3.0/gems/aws-sdk-s3-1.21.0/lib/aws-sdk-s3/plugins/sse_cpk.rb:22:in `call'\n    from /lib/ruby/gems/2.3.0/gems/aws-sdk-s3-1.21.0/lib/aws-sdk-s3/plugins/dualstack.rb:26:in `call'\n    from /lib/ruby/gems/2.3.0/gems/aws-sdk-s3-1.21.0/lib/aws-sdk-s3/plugins/accelerate.rb:35:in `call'\n    from /lib/ruby/gems/2.3.0/gems/aws-sdk-core-3.30.0/lib/aws-sdk-core/plugins/jsonvalue_converter.rb:20:in `call'\n    from /lib/ruby/gems/2.3.0/gems/aws-sdk-core-3.30.0/lib/aws-sdk-core/plugins/idempotency_token.rb:17:in `call'\n    from /lib/ruby/gems/2.3.0/gems/aws-sdk-core-3.30.0/lib/aws-sdk-core/plugins/param_converter.rb:24:in `call'\n    from /lib/ruby/gems/2.3.0/gems/aws-sdk-core-3.30.0/lib/aws-sdk-core/plugins/response_paging.rb:10:in `call'\n    from /lib/ruby/gems/2.3.0/gems/aws-sdk-core-3.30.0/lib/seahorse/client/plugins/response_target.rb:23:in `call'\n    from /lib/ruby/gems/2.3.0/gems/aws-sdk-core-3.30.0/lib/seahorse/client/request.rb:70:in `send_request'\n    from /lib/ruby/gems/2.3.0/gems/aws-sdk-s3-1.21.0/lib/aws-sdk-s3/client.rb:3163:in `head_object'\n    from (irb):12. I get the same behavior between aws-sdk v3.30 and v3.35\nand with Aws::Credentials, and the Aws::InstanceProfileCredentials. It looks like i was inadvertently passing the ARN of the bucket through, which was causing the bad request.\nI am still seeing that any attempt to use the InstanceProfileCredentials as an S3 credentials provider is broken.. precisely.\noddly enough, omitting the credentials from the s3 client magically works from the instance\n```ruby\nrequire 'aws-sdk-s3'\nclient = Aws::S3::Client.new(region: 'us-west-2')\nresource = Aws::S3::Resource.new(client: client)\nbucket = resource.bucket('myBucket')\nobject = bucket.object('path-to-file')\nputs object.exists?\n```. This issue can probably be closed as a result.\nI would encourage that the SDK documentation describe that clients should not provide a credentials object when running in an EC2 instance with an IAM role.. ",
    "alinavancea": "@srchase thank you so much for your fast reply! \nThe policy we have set up is not set up on a bucket, therefor we don't have access to list the buckets.\nThe policy is set up for a specific path only s3://path/to/files\nHere is what I'm trying to do\nrole_credentials =  Aws::AssumeRoleCredentials.new(external_id: external_id, role_arn: role_arn, role_session_name: role_session_name) \nclient=Aws::S3::Client.new(credentials: role_credentials) \nclient.list_buckets                                                                                                                                                                            \nAws::S3::Errors::AccessDenied: Access Denied. Thank you I will check that. I'm back with an update on how I manage to to the operations while having access to a specific path only.\nFrom the path we received we figure out that we have the bucket name. Even though we don't have any permission to list the objects inside it, we can access the content for the given path only using the prefixoption\ns3://bucket_name/our_folder\nI was stuck because I was keep trying to list the bucket content itself, which was obviously not working since we did not hat the permission. It did not crossed my mind that there can be a bucket object bucket = s3.bucket(bucket_name) even though no permission and later use it for getting the objects inside with prefix option.\nrole_credentials = Aws::AssumeRoleCredentials.new(external_id: external_id, role_arn: role_arn, role_session_name: role_session_name)\nclient = Aws::S3::Client.new(credentials: role_credentials, region: region)\ns3 = Aws::S3::Resource.new(client: client)\nbucket = s3.bucket(bucket_name)\nbucket.objects(prefix: \"our_folder/\").each do |aws_object|\n   object_key = aws_object.key\n   file = File.open(\"local_file.csv\")\n   client.get_object({ bucket: bucket_name, key: object_key }, target: file)\nend\n. Everything is working ok :)\n@srchase thank you for your time and help!\nNo other questions from me\nBest,\nAlina\n. ",
    "cgledezma1101": "@srchase Unfortunately the package is an internal tool managed by another team at my company. I do not have access to the wire logs but can try finding it out.\nFrom what I've been using the tool for, I would say the issue is somewhere where managing of creation of CloudFormation templates happen. ",
    "itmart": "@srchase \nSorry for the late response, I recently moved on to something else and just now revisit this issue. I didn't realize the contents was for defining a single response. Your suggestion seems to correctly made the code run twice, but when it runs the second time, I'm getting this error:\nArgumentError Exception: unexpected value at params[:key]\nwhen it's defining s3 resource:\nAws::S3::Resource.new\nDo you have any ideas on how to resolve this issue?. @srchase \nThe issue is in my download_file method listed in my description of this issue. The first line is defining the s3 resource s3 = Aws::S3::Resource.new. I'm not sure I get your suggestion on reusing it since I have to define it first, right?. I tried taking out the key: part and just have this:\nAws.config[:s3] = {\n  stub_responses: {\n    list_objects: [\n      Errno::ECONNREFUSED,\n      \"key/aws_file.pdf\"\n    ],\n    get_object: [Errno::ECONNREFUSED, file_fixture('aws_file.pdf').read]\n  }\n}\nBut I'm getting Errno::ECONNREFUSED on both tries when trying to download the file.. @srchase Ahhhhhh it never occurred to me that I was doing that. Let me refactor some stuff around and see what happens.. @srchase \nOkay, so this is my new setup:\n```\nStub\nAws.config[:s3] = {\n  stub_responses: {\n    list_objects: [\n      Errno::ECONNREFUSED,\n      \"key/aws_file.pdf\"\n    ],\n    get_object: [Errno::ECONNREFUSED, file_fixture('aws_file.pdf').read]\n  }\n}\ndef download_file(bucket_name, object_key, s3)\n  bucket = s3.bucket(bucket_name)\n  bucket_objects = bucket.objects.select do |item|\n    item.key.start_with? object_key\n  end\nlatest_object = bucket_objects.last\n  return nil if latest_object.blank?\n  latest_object.presigned_url(:get)\nend\nTest\ntest 'expects to retry to get AWS S3 file when service is temporarily down' do\n  s3 = Aws::S3::Resource.new\n  Retriable.retriable do\n    download_file(\"bucket\", \"key/aws_file.pdf\", s3)\n  end\nend\n```\nThe retriable block is running twice and I got a connection error on the first try as expected. However, on the second try, bucket.objects.select threw this error: Aws::S3::Errors::Keyawsfilepdf Exception: stubbed-response-error-message. \nI tried using this stub instead:\nAws.config[:s3] = {\n  stub_responses: {\n    list_objects: [\n      Errno::ECONNREFUSED,\n      key: \"key/aws_file.pdf\" # using a hash here instead of just a string\n    ],\n    get_object: [Errno::ECONNREFUSED, file_fixture('aws_file.pdf').read]\n  }\n}\nBut that resulted in this error: ArgumentError: unexpected value at params[:key] when it's running s3 = Aws::S3::Resource.new. \nI feel like I'm very close to figuring this out but I can't figure out the right syntax. Can you help me look into what is it that I'm missing? Thank you so much for taking your time and helping me out with this!. Thank you! I was able to make it pass. Thanks for all the help!. ",
    "waldyr": "Really grateful for your reply @srchase. Thank you very much.\nIt's kind of crazy that #head_object and friends don't need it but #copy_object does. As your reply solves my issue I'm satisfied but (I saw you adding and removing the bug tag) if you want to normalize the behavior across methods I could make a PR for that.. ",
    "davidtaylorhq": "For what it's worth, I just hit exactly the same problem with aws-sdk-iam, so this definitely affects more than just aws-sdk-ec2. . ",
    "bjonord": "\ud83d\udc4d . ",
    "coreyaus": "Hey mate,\nI originally created that secret using the \"Credentials for RDS database\" option, and entering the master username and password. As a test, I just tried creating a new secret with the \"Credentials for other database\" option and manually entering the DB details - however, I didn't have any success when running client.execute_sql with that new secret either. Let me know if there are any other tests you'd like me to try to help isolate the issue \ud83d\udc4d \nThanks for your help!. Thanks mate,\nMy RDS instance is running Postgres. I see there was also some confusion about this from others in the issue you referenced above: https://github.com/aws/aws-sdk-js/issues/2376. It could be worth updating the docs to make it clear which endpoints are limited by DB type. For example, there is no mention of any limitations in the SDK docs I was following here: https://docs.aws.amazon.com/sdk-for-ruby/v3/api/Aws/RDSDataService.html. Anyway, just food for thought as the team works through the beta \ud83d\udc4dIs the general plan to support all DB types in the long-run?\nCheers!. ",
    "markwinap": "Still no support for Postgres :(. ",
    "kamaljoshi": "Yes, Something like KPL would be the best option. Considering that KPL's core lib is in C++, one option is to extend it using something like FFI and use that to expose Ruby interfaces. \nRight now, I think the two options to do here are to do a futures like pattern with a lib like concurrent-ruby or the separate thread approach as mentioned.\nIf you guys don't have bandwidth, I can take this up if we can agree on the implementation.. ",
    "conrosebraugh": "Is this something that is under active development? My organization has similar needs and could benefit from this feature.. ",
    "peterwake": "@srchase\nHi Chase,\nThanks for the quick response, and it's great to know that the move_to operation doesn't download data to our server. The http_wire_trace: true option isn't something I knew about.\nDo you have any advice on a sensible number of concurrent threads to run to do the moves? And is the multipart_copy: true bit important?\nI will create a gist at some point with the code we end up with.\nBest regards,\nPeter. Hi @srchase no problem. For anyone else hitting this issue I created a gist at https://gist.github.com/peterwake/49a829d2062ad3c5962435341692a680. 25 concurrent threads seemed to be the point where adding more threads didn't seem to make a tangible performance increase.. ",
    "divergentdave": "I am having this issue as well. I'm using the faraday_middleware-aws-sigv4 gem, which in turn uses aws-sigv4. Here's how I set up that middleware:\nf.request :aws_sigv4,\n          credentials: Aws::InstanceProfileCredentials.new,\n          service: 'es',\n          region: Environment.config['aws']['region']\nHere's where the middleware instatiates the signer: https://github.com/winebarrel/faraday_middleware-aws-sigv4/blob/v0.2.4/lib/faraday_middleware/request/aws_sigv4.rb#L9\nHere's one of the messages I'm getting.\nDEPRECATION WARNING: called deprecated method `access_key_id' of an Aws::CredentialProvider, use #credentials instead\n/home/ubuntu/.rvm/gems/ruby-2.4.5/gems/aws-sigv4-1.0.3/lib/aws-sigv4/signer.rb:575:in `credentials_set?'\n/home/ubuntu/.rvm/gems/ruby-2.4.5/gems/aws-sigv4-1.0.3/lib/aws-sigv4/signer.rb:566:in `get_credentials'\n/home/ubuntu/.rvm/gems/ruby-2.4.5/gems/aws-sigv4-1.0.3/lib/aws-sigv4/signer.rb:204:in `sign_request'\n/home/ubuntu/.rvm/gems/ruby-2.4.5/gems/faraday_middleware-aws-sigv4-0.2.4/lib/faraday_middleware/request/aws_sigv4.rb:22:in `sign!'\n/home/ubuntu/.rvm/gems/ruby-2.4.5/gems/faraday_middleware-aws-sigv4-0.2.4/lib/faraday_middleware/request/aws_sigv4.rb:14:in `call'\n/home/ubuntu/.rvm/gems/ruby-2.4.5/gems/faraday-0.15.4/lib/faraday/rack_builder.rb:143:in `build_response'\n/home/ubuntu/.rvm/gems/ruby-2.4.5/gems/faraday-0.15.4/lib/faraday/connection.rb:387:in `run_request'\n/home/ubuntu/.rvm/gems/ruby-2.4.5/gems/elasticsearch-transport-6.1.0/lib/elasticsearch/transport/transport/http/faraday.rb:23:in `block in perform_request'\n/home/ubuntu/.rvm/gems/ruby-2.4.5/gems/elasticsearch-transport-6.1.0/lib/elasticsearch/transport/transport/base.rb:266:in `perform_request'\n/home/ubuntu/.rvm/gems/ruby-2.4.5/gems/elasticsearch-transport-6.1.0/lib/elasticsearch/transport/transport/http/faraday.rb:20:in `perform_request'\n/home/ubuntu/.rvm/gems/ruby-2.4.5/gems/elasticsearch-transport-6.1.0/lib/elasticsearch/transport/client.rb:131:in `perform_request'\n/home/ubuntu/.rvm/gems/ruby-2.4.5/gems/elasticsearch-api-6.1.0/lib/elasticsearch/api/namespace/common.rb:21:in `perform_request'\n/home/ubuntu/.rvm/gems/ruby-2.4.5/gems/elasticsearch-api-6.1.0/lib/elasticsearch/api/actions/indices/put_settings.rb:67:in `put_settings'\ntasks/elasticsearch.rake:148:in `block (2 levels) in <top (required)>'. ",
    "gondalez": "Hi @srchase any chance of reopening this one? \nI too am seeing the issue and assumed I was the cause :) \nAfter further investigation I believe the following line is the cause as it is calling #access_key_id which is deprecated as mentioned here in the docs.\nhttps://github.com/aws/aws-sdk-ruby/blob/1a89dec5678091178f31ad2638486f18dc14c0a9/gems/aws-sigv4/lib/aws-sigv4/signer.rb#L575. ",
    "dudo": "@srchase I'm doing nothing of the sort. I only initialized the client with stub_responses: true with the expectation that any \"real\" calls would be stubbed. Is that not the case?\nLooks like I need to explicitly give a response to the call?. If it helps, I think I'm moving in the right direction.\nAws::SQS::Client.new(**Rails.application.config.sqs.symbolize_keys)\n\n2 things, I didn't expect:\n- Giving the constructor a hash did not behave as expected - the double splat took care of that\n- The constructor needs symbolized keys. The prior expectation explains why, honestly - you're using keyword arguments. An options hash could make this a bit more flexible.\nMaybe you can hold my hand a bit here, though (please)\n```\nfrozen_string_literal: true\nmodule Sqs\n  class Base\n    attr_reader :client\nQUEUE_NAME = 'undefined'\n\ndef initialize\n  Aws.config[:credentials] = Aws::Credentials.new('', '') unless Rails.env.production?\n  @client = Aws::SQS::Client.new(**Rails.application.config.sqs.symbolize_keys)\nend\n\ndef queue_url\n  @queue_url ||= client.get_queue_url(queue_name: self.class::QUEUE_NAME).queue_url\nend\n\ndef messages\n  client.receive_message(\n    queue_url: queue_url,\n    message_attribute_names: ['All'], # Receive all custom attributes\n    max_number_of_messages: 10, # 10 is the maximum number allowed by SQS\n    wait_time_seconds: 0 # Do not wait to check for the message\n  ).messages\nend\n\ndef process_message(message)\n  # message.body\n  # message.message_attributes\nend\n\ndef delete_message(message)\n  client.delete_message(\n    queue_url: queue_url,\n    receipt_handle: message.receipt_handle\n  )\nend\n\nend\nend\n```\nWith that very lightweight wrapper, I'm getting closer to my expectations:\n> sqs = Sqs::Base.new\n=> #<Sqs::Base:0x00007f9413d89858 @client=#<Aws::SQS::Client>>\n> sqs.queue_url\n=> \"String\"\n> sqs.messages\nArgumentError: invalid endpoint, expected URI::HTTP, URI::HTTPS, or nil, got #<URI::Generic String>\n\nAny help to get me over the hump of using this for basic calls is appreciated =). That helps. Thanks!. ",
    "totem3": "Thank you for the review.\nYes, I confirmed and agreed to the lisence.. ",
    "ankane": "The test failure for JRuby is for an unrelated issue.. ",
    "brandoncc": "@srchase The actual error is being raised here, although you are right that we are using carrierwave-aws which is where the aws-sdk require is coming from.\nCarrierwave was not changed during the rails upgrade, but I have since tried upgrading aws-sdk, carrierwave, and carrierwave-aws. It didn't help.\nAre you saying that the problem can't be in the aws-sdk-ruby gem even though the error is being raised from it?. Thank you for the help @awood45, I will look into whether we can use aws v3 or not. If we can't, I'll try your solution. Thanks again!. ",
    "umanoda": "@srchase\nThanks for the helpful blog post!\nIs the default stub setting deprecated? I'd like to prepare a stub for feature testing.. @srchase \nI referenced https://docs.aws.amazon.com/sdk-for-ruby/v3/api/Aws/ClientStubs.html (see Basic usage)\n\nLastly, default stubs can be configured via Aws.config:.... Immediately after setting Aws.config stubbing to config/rails_helper.rb, we responded by calling a dummy client.\nThis will prevent errors when the S3 client is called before the SQS client during the test.\n\n```\nconfig/rails_helper\nAws.config[:sqs] = {\n  stub_responses: {\n    get_queue_url: {\n      queue_url: \"http://example.com\"\n    }\n  }\n}\nTo activate the stub\nAws::SQS::Resource.new\n```. ",
    "hrjaan": "A similar reference to the problem is located at - https://github.com/fluent/fluent-plugin-s3/issues/260. ",
    "nikriek": "Yes, aws-sdk-sqs 1.10.0 with aws-sdk-core 3.44.1.. I used this https://github.com/phstc/shoryuken/wiki/Using-a-local-mock-SQS-server as a reference. The endpoint is passed using an ENV variable.. I also wonder why it happened so randomly from one day to another. All dependency versions are locked.. It is actually an issue with the docker dependency: https://github.com/p4tin/goaws/issues/187. I downgraded to an older version and it works now. Calling the mock server still works, but the list_queues endpoint was returning wrong urls.. ",
    "pablogmorales": "Hi\nI will try the shared credentials option, did not use it because I did not understand it, I guess that in my_path there should be a credentials files with the keys the same as in .aws folder. and reading from the json file is not working either as I said on the previous post.\nThanks\nRegards. Im, getting the same error:\n`shared_creds = Aws::SharedCredentials.new(profile_name: 'default', path: '/etc/sensu/plugins/credentials')\nAws.config.update(credentials: shared_creds)\n{:credentials=>#}\n/usr/local/share/gems/gems/aws-sdk-core-3.44.1/lib/aws-sdk-core/plugins/signature_v4.rb:72:in sign_request': unable to sign request without credentials set (Aws::Errors::MissingCredentialsError)\n    from /usr/local/share/gems/gems/aws-sdk-core-3.44.1/lib/aws-sdk-core/plugins/signature_v4.rb:112:inapply_signature'\n    from /usr/local/share/gems/gems/aws-sdk-core-3.44.1/lib/aws-sdk-core/plugins/signature_v4.rb:65:in call'\n    from /usr/local/share/gems/gems/aws-sdk-core-3.44.1/lib/aws-sdk-core/plugins/helpful_socket_errors.rb:10:incall'\n    from /usr/local/share/gems/gems/aws-sdk-core-3.44.1/lib/aws-sdk-core/plugins/retry_errors.rb:171:in call'\n    from /usr/local/share/gems/gems/aws-sdk-core-3.44.1/lib/aws-sdk-core/query/handler.rb:28:incall'\n    from /usr/local/share/gems/gems/aws-sdk-core-3.44.1/lib/aws-sdk-core/plugins/user_agent.rb:13:in call'\n    from /usr/local/share/gems/gems/aws-sdk-core-3.44.1/lib/aws-sdk-core/plugins/endpoint_pattern.rb:28:incall'\n    from /usr/local/share/gems/gems/aws-sdk-core-3.44.1/lib/aws-sdk-core/plugins/endpoint_discovery.rb:78:in call'\n    from /usr/local/share/gems/gems/aws-sdk-core-3.44.1/lib/seahorse/client/plugins/endpoint.rb:45:incall'\n    from /usr/local/share/gems/gems/aws-sdk-core-3.44.1/lib/aws-sdk-core/plugins/param_validator.rb:24:in call'\n    from /usr/local/share/gems/gems/aws-sdk-core-3.44.1/lib/seahorse/client/plugins/raise_response_errors.rb:14:incall'\n    from /usr/local/share/gems/gems/aws-sdk-core-3.44.1/lib/aws-sdk-core/plugins/jsonvalue_converter.rb:20:in call'\n    from /usr/local/share/gems/gems/aws-sdk-core-3.44.1/lib/aws-sdk-core/plugins/idempotency_token.rb:17:incall'\n    from /usr/local/share/gems/gems/aws-sdk-core-3.44.1/lib/aws-sdk-core/plugins/param_converter.rb:24:in call'\n    from /usr/local/share/gems/gems/aws-sdk-core-3.44.1/lib/aws-sdk-core/plugins/response_paging.rb:10:incall'\n    from /usr/local/share/gems/gems/aws-sdk-core-3.44.1/lib/seahorse/client/plugins/response_target.rb:23:in call'\n    from /usr/local/share/gems/gems/aws-sdk-core-3.44.1/lib/seahorse/client/request.rb:70:insend_request'\n    from /usr/local/share/gems/gems/aws-sdk-sns-1.9.0/lib/aws-sdk-sns/client.rb:1257:in publish'\n    from /usr/local/share/gems/gems/aws-sdk-sns-1.9.0/lib/aws-sdk-sns/topic.rb:288:inpublish'\n    from send-sms.rb:68:in <main>'\nWhat I do not understand is why if I use this way does not work either, the only way to make it works if exporting the variables from bash.\nAws.config.update({\n   credentials: Aws::Credentials.new('your_access_key_id', 'your_secret_access_key')\n})\nAny idea?\nThanks\nregards\n. I  noticed that it works from the standard path, /root/.aws/credentials.. Yes, I just created .aws under root and copied the credentials file, so, the format is fine, and the permission on the alternate location are fine as well, the process is executed by sensu, so, root should be able to read it as well, and checked the execution permissions as well.\nSo by now, It's working like that, using /root/.aws/credentials which seems to be default.\nBut would like to understand why if I hardcode the keys does not work either, anyway, I do not want to hardcode credentials inside the scripts, but during the test I tried almost everything, so Im curious why it does not work.\nThanks\nRegards. ",
    "SirRawlins": "@awood45 sure thing Alex. Here's the http_wire_trace generated from the code example.\n\nopening connection to cloudfront.amazonaws.com:443...\nopened\nstarting SSL for cloudfront.amazonaws.com:443...\nSSL established\n<- \"PUT /2018-11-05/distribution/XXX/config HTTP/1.1\\r\\nContent-Type: \\r\\nAccept-Encoding: \\r\\nUser-Agent: aws-sdk-ruby3/3.44.2 ruby/2.5.1 x86_64-darwin15 aws-sdk-cloudfront/1.11.0\\r\\nIf-Match: E25PI0SI26B0B0\\r\\nHost: cloudfront.amazonaws.com\\r\\nX-Amz-Date: 20190110T105559Z\\r\\nX-Amz-Content-Sha256: 759e108068b7c2c3eb552f3149b7ff1a7d87248e3339449dfacf3d88a5f586b4\\r\\nAuthorization: AWS4-HMAC-SHA256 Credential=XXX SignedHeaders=host;if-match;x-amz-content-sha256;x-amz-date, Signature=c6db1d97c2e9c97d1223bd58045f54ca8fd137a7e522bef6fc867ec8cc0d7a9d\\r\\nContent-Length: 3224\\r\\nAccept: /\\r\\n\\r\\n\"\n-> \"HTTP/1.1 400 Bad Request\\r\\n\"\n-> \"x-amzn-RequestId: 506c5e55-14c6-11e9-89dc-75337560d9a2\\r\\n\"\n-> \"Content-Type: text/xml\\r\\n\"\n-> \"Content-Length: 283\\r\\n\"\n-> \"Date: Thu, 10 Jan 2019 10:55:59 GMT\\r\\n\"\n-> \"Connection: close\\r\\n\"\n-> \"\\r\\n\"\nreading 283 bytes...\n-> \"\"\n-> \"<?xml version=\\\"1.0\\\"?>\\nSenderMalformedInputUnexpected list element termination506c5e55-14c6-11e9-89dc-75337560d9a2\"\nread 283 bytes\nConn close\nTraceback (most recent call last):\n       16: from /Users/robertrawlins/.rvm/gems/ruby-2.5.1/gems/bootsnap-1.3.2/lib/bootsnap/load_path_cache/core_ext/kernel_require.rb:21:in block in require_with_bootsnap_lfi'\n       15: from /Users/robertrawlins/.rvm/gems/ruby-2.5.1/gems/bootsnap-1.3.2/lib/bootsnap/load_path_cache/core_ext/kernel_require.rb:21:inrequire'\n       14: from /Users/robertrawlins/.rvm/gems/ruby-2.5.1/gems/railties-4.2.11/lib/rails/commands.rb:17:in <main>'\n       13: from /Users/robertrawlins/.rvm/gems/ruby-2.5.1/gems/railties-4.2.11/lib/rails/commands/commands_tasks.rb:39:inrun_command!'\n       12: from /Users/robertrawlins/.rvm/gems/ruby-2.5.1/gems/railties-4.2.11/lib/rails/commands/commands_tasks.rb:68:in console'\n       11: from /Users/robertrawlins/.rvm/gems/ruby-2.5.1/gems/railties-4.2.11/lib/rails/commands/console.rb:9:instart'\n       10: from /Users/robertrawlins/.rvm/gems/ruby-2.5.1/gems/railties-4.2.11/lib/rails/commands/console.rb:110:in start'\n        9: from (irb):7\n        8: from /Users/robertrawlins/.rvm/gems/ruby-2.5.1/gems/aws-sdk-cloudfront-1.11.0/lib/aws-sdk-cloudfront/client.rb:3712:inupdate_distribution'\n        7: from /Users/robertrawlins/.rvm/gems/ruby-2.5.1/gems/aws-sdk-core-3.44.2/lib/seahorse/client/request.rb:70:in send_request'\n        6: from /Users/robertrawlins/.rvm/gems/ruby-2.5.1/gems/aws-sdk-core-3.44.2/lib/seahorse/client/plugins/response_target.rb:23:incall'\n        5: from /Users/robertrawlins/.rvm/gems/ruby-2.5.1/gems/aws-sdk-core-3.44.2/lib/aws-sdk-core/plugins/response_paging.rb:10:in call'\n        4: from /Users/robertrawlins/.rvm/gems/ruby-2.5.1/gems/aws-sdk-core-3.44.2/lib/aws-sdk-core/plugins/param_converter.rb:24:incall'\n        3: from /Users/robertrawlins/.rvm/gems/ruby-2.5.1/gems/aws-sdk-core-3.44.2/lib/aws-sdk-core/plugins/idempotency_token.rb:17:in call'\n        2: from /Users/robertrawlins/.rvm/gems/ruby-2.5.1/gems/aws-sdk-core-3.44.2/lib/aws-sdk-core/plugins/jsonvalue_converter.rb:20:incall'\n        1: from /Users/robertrawlins/.rvm/gems/ruby-2.5.1/gems/aws-sdk-core-3.44.2/lib/seahorse/client/plugins/raise_response_errors.rb:15:in `call'\nAws::CloudFront::Errors::MalformedInput (Unexpected list element termination)\n\nLet me know if there's anything else I can do to help narrow down what's going on.\n. Excellent stuff. Thanks so much. \ud83d\udc4d\nOn Mon, 14 Jan 2019, 17:38 Chase Coalwell <notifications@github.com wrote:\n\n@SirRawlins https://github.com/SirRawlins\nThis is caused by the CloudFront API returning a self-closed tag in the\nget_distribution_config response:\n\n0\n\n\nThat output is an invalid input for the the update_distribution_config\noperation. I've opened a request for the CloudFront Service Team to\ninvestigate.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/aws/aws-sdk-ruby/issues/1954#issuecomment-454093350,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAtiKxrfTT_4ymE0x_wwlrie7nmaegxYks5vDMCOgaJpZM4Z1JJp\n.\n. \n",
    "josvazg": "Sorry but I still think this qualifies as a bug. The response we get is not directly the EmptyStructure class but a PageableResponse, that wraps the actual reponse as data, in this case an  EmptyStructure. Such response implements to_h this way:\nhttps://github.com/aws/aws-sdk-ruby/blob/97b28ccf18558fc908fd56f52741cf3329de9869/gems/aws-sdk-core/lib/aws-sdk-core/pageable_response.rb#L142-L144\n^ Which means that data must implement to_h. An EmptyStructure.new (instance) does, but the class itself doesn't.\n. ",
    "invader-move": "Update : it's working with 2.6 (i make a little test). Thank you !. ",
    "ice799": "@srchase \nIf that is the desired use case, may I suggest option 1 from my list above to modify the comments/documentation for upload_file?\nThe documentation here: https://docs.aws.amazon.com/sdk-for-ruby/v3/api/Aws/S3/Object.html#upload_file-instance_method generated from here https://github.com/aws/aws-sdk-ruby/blob/master/gems/aws-sdk-s3/lib/aws-sdk-s3/file_uploader.rb#L26 indicates that upload_file can accept a Tempfile.\nIf upload_file should only be given and/or only expects a path, I would suggest updating the documentation to clear that up for users.. @srchase \nThe same comment appears in a few other places, all of which would need to be updated if all of these intend to only take a path:\n\nhttps://github.com/aws/aws-sdk-ruby/blob/master/gems/aws-sdk-s3/lib/aws-sdk-s3/multipart_file_uploader.rb#L35-L48\nhttps://github.com/aws/aws-sdk-ruby/blob/master/gems/aws-sdk-s3/lib/aws-sdk-s3/file_part.rb#L9\nhttps://github.com/aws/aws-sdk-ruby/blob/master/gems/aws-sdk-s3/lib/aws-sdk-s3/file_uploader.rb#L26\n\nPossibly others, as well.. @srchase Up to you; I don't find the existing documentation to be sufficiently clear. It sounds like you might agree since we were both confused as to what this method expects (just a path vs a path or a File or Tempfile object that must not be closed) and what state the expected objects should be in, additional documentation can be added to make this more clear for all affected methods.\nI would propose two paths forward here:\n\nIf upload_file intends to take only a path as you mentioned here the documentation should be updated throughout to make this clear, and the method should raise an exception if something that isn't a String is passed in. This will break the API though and is probably not the best choice.\nOr, if upload_file intends to take a path, File, or Tempfile (so long as neither the File or Tempfile objects are closed) the documentation should be updated to make this explicitly clear for all methods and an exception should be raised if the object is not in the correct state when passed in.. \n",
    "satyashanmuka": "@srchase Thanks for the quick response. \nSo, if I add a user-defined tag for an EC2 instance like instance_id:<instanceId value which I assume is unique>, the get_cost_and_usage call with the tag filter set will return cost only for the specific instance.\nIs my understanding correct?\n. Able to add user-defined tags, but for them to show up in cost explorer dashboard, need to activate them as mentioned at https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/activating-tags.html.\nI don't have permission to do so for the aws account, will get this done and test the scenario.\n. ",
    "postmodern": "@awood45 if the solution is as simple as removing all {s: ...} examples from the documentation, I could submit a PR?. :+1: . ",
    "mbarrin": "I don't think this was worth of an update to the changelog as it was just fixing a typo in the docs.\nHappy to make any changes you see fit though.. Spec failure seems unrelated to my change, but I can't trigger a rebuild.. ",
    "thapakazi": "Yeah, apologies I was feeling itchy to open this concern here. \n\nHave you reached out to the Transcribe Service with this request?  \n\nHow do I do that ??  Where though ??. ",
    "nurse": "Oops, my environment actually used aws-sdk-ec2 1.9.0, in which DescribeSecurityGroupsResult doesn't have next_token, on running the script.\nSorry for the noise and thank you for comment!. ",
    "andyvenus": "Yeah we just hit this. The version check in the Gemfile doesn't seem to work.\nhttps://github.com/aws/aws-sdk-ruby/blob/9257098cfd8a386e5424f998fcb5cd2d11222a1a/Gemfile#L62. ",
    "dgem": "I can confirm this, just hit it today with our servers... \n```\ngem install aws-sdk\nERROR:  Error installing aws-sdk:\n    http-2 requires Ruby version >= 2.1.0.\ninstall the previous version works, as a temporary workaround:\ngem install aws-sdk -v 2.11.240\nFetching: aws-sdk-core-2.11.240.gem (100%)\nFetching: aws-sdk-resources-2.11.240.gem (100%)\nFetching: aws-sdk-2.11.240.gem (100%)\nSuccessfully installed aws-sdk-core-2.11.240\nSuccessfully installed aws-sdk-resources-2.11.240\nSuccessfully installed aws-sdk-2.11.240\n3 gems installed\nInstalling ri documentation for aws-sdk-core-2.11.240...\nInstalling ri documentation for aws-sdk-resources-2.11.240...\nInstalling ri documentation for aws-sdk-2.11.240...\nInstalling RDoc documentation for aws-sdk-core-2.11.240...\nInstalling RDoc documentation for aws-sdk-resources-2.11.240...\nInstalling RDoc documentation for aws-sdk-2.11.240...\n```\nGiven there are 1.2K commits since this release ! (https://github.com/aws/aws-sdk-ruby/releases/tag/v2.11.240) I guess it might take a little unpicking. Thanks @awood45 , I was basing my decision on the Releases page, then realized my mistake.\nCan you confirm how to achieve this through the command line, I installed the aws-sdk-core @ 3.46.0\n```\ngem list\n LOCAL GEMS \naws-eventstream (1.0.2)\naws-partitions (1.144.0)\naws-sdk-core (3.46.0)\naws-sigv4 (1.1.0)\njmespath (1.4.0)\nBut still get the problem when installing the sdk:\ngem install aws-sdk\nERROR:  Error installing aws-sdk:\n    http-2 requires Ruby version >= 2.1.0.\n``\nIn my case, these gems are installed as part of a boostrap process to allow the instances to be provisioned rather than as part of an app via a Gemfile.\nThanks for any advice you might be able to offer @awood45 or @michaelwittig . Fantastic news, thanks for fixing so rapidly.\nFYI, as per my earlier comments, we couldn't pin through the cmd line due togem install aws-sdkresulting in the same error after installingaws-sdk-coreat 3.46.0.\nIn the end I found working machine and usedgem lockto generate a Gemfile which I could thenbundle install` with.... mentioning for prosperity.\nLooking forward to removing \u261d\ufe0f that workaround :) . ",
    "michaelwittig": "One question: I pinned the versions of the following gems:\naws-sdk-autoscaling: 1.13.0\naws-sdk-sqs: 1.10.0\nAnd still, I'm running into this new dependency problem. My expectation is that if I pin a gem all the dependent gems are also pinned? Is this not true for the aws-sdk-core?\nUpdate: Just seeing this https://github.com/aws/aws-sdk-ruby/blob/master/gems/aws-sdk-autoscaling/aws-sdk-autoscaling.gemspec#L26\nWhy is the version not pinned to a specific version?. ",
    "MWers": "Thanks for the suggestion @awood45 - This fixed our problem!. ",
    "dividedmind": "How about putting the whole entry in details instead? It'd be put there by reference anyway, so no copying involved. Then [:receipt_handle] would be redundant, but I guess it can be left there for backwards compatibility (marked as deprecated?).\n. ",
    "thenovices": "cleaner would be\nruby\noptions[:client_token] ||= SecureRandom.uuid\n. ",
    "kitchen": "The default options parameter on this method would have raised a NoMethodError and it's a private method, so I got rid of the default.\n. ",
    "mayanks": "Will this not write the chunks out of order?. ",
    "YashoSharma": "The stub_responses method raises a generic RuntimeError, should I leave the error handling the way it is for consistency or use a typed exception?. There was some brief follow up in the outdated comments after Alex cross checked against aws-record. I added documentation for everything inside of the log hash, but since the params hash can be variable based on whatever operation the client made I don't think enumerating the supported keys for it is feasible . "
}
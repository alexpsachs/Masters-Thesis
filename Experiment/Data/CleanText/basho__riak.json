{
    "dizzyd": "Closing pull request -- obsoleted by issues 743 - 745.\n. OJ, thanks for the feedback. We're going to be doing some work on the make system as we expose the mechanisms for building debs/rpms. I'll take a look at the specifics of your patch next week (Nov 8th) -- I'm traveling out to SFO right now.\nThanks!\n. This was fixed in commit 63885dc84c99a54dd2ffeb9adb9eca182d20622a.\n. +1 on push to develop.\n. +1 to merge to mainline.\n. Merged into riak_kv-0.14.\n. Reworking patch to fix this poor history. Closing.\n. Merged to master and 1.0.\n. +1.\n. +1\n. This is overlapping with some work I've been doing w/ node_package and our private repos. Let's touch base in the morning and discuss.\n. +1. I'll look into incorporating these ideas into node_package as well.\n. +1\n. +1\n. Thanks, Scott. Will merge once I've worked out some compilation issues in leveldb.\n. +1\n. Looks good; +1.\n. Erik, can you please rebase this patch against master or the 1.0 branch?\n. +1\n. +1, if it passes b_e\n. +1, if we have a reference to the wiki somewhere in the docs shipped with source...\n. +1\n. +1\n. Hm...is that going to do the right thing when you pass other arguments to the escript? I.e. \nriak escript myescript.erl arg1 arg2\n. Got it, thanks. +1 to merge, but we also need to add this to the EDS version of script.\n. Fixed in https://github.com/basho/erlang_js/pull/30\n. Merged.\n. ",
    "slfritchie": "John, I noticed that your branch is based on code from 19 Aug 2010.  There were some packaging changes for the 0.13 release that might've addressed this problem in a different way.  Would you mind re-checking with a newer package and, if it's still broken, then submit a new pull request?  (We really do want to fix bugs like this, if they still exist in the latest release.) We've had too many problems with Debian vs. Red Hat vs. Fedora vs. Solaris packaging bits to try patching older code without a Master's thesis-worth of code study to prepare for it.....\n-Scott\n. Jesper, I've been informed by informed sources that that config knob will be disappearing in the 0.14 release, due out in a few weeks.  Thanks for the docs (I was about to add your text to https://wiki.basho.com/display/RIAK/Configuration+Files also), but it looks like this batch won't be necessary.\n-Scott\n. Tuncer, I'm a big fan of removing bashisms, thanks.  I'll duplicate these in our other major packages also.\n-Scott\n. Sean: I haven't been able to figure out what & when does the\n{{whatever_variable}} text substitution for Solaris & OpenSolaris\npackages ... but the same thing should be done for them.  If\nyou had a solution for *Solaris, I'd merge it pronto.  :-)\nA version of the 0f199e4 patch for the Debian and *Solaris would\nalso be useful: it'd be nice to avoid contributing to platform-specific\ndrift.\nOne last thing ... a static file name runs the risk of the following\nsequence of events (which has bitten me in the past):\n1. Bad crash happens, file \"foodump\" created.\n2. Customer doesn't react fast enough, another unrelated crash\n    happens and writes a new \"foodump\".\n3. Customer calls for support.\n4. \"foodump\" doesn't contain anything related to step #1's crash.\n. Yup, but we don't have all platforms using the same working dir\neither.  Bah.  I'd love to change things to that the VM's working\ndir was /path/to/riak/data dir, but I haven't the foggiest notion if\nthat'd break an existing installation or not.\n. No, not that I'm aware of.  We could change it, but we also don't have\na config knob that says where that dir is.  IIRC the best that's currently\nthere is to take bitcask's dir and move up to its parent.  Except for\nthose configs that don't use bitcask.  So we'd have to add it.  Which I\nthink would be good, but hasn't been done yet.  \n. Well ... from the OTP team's point of view, the cwd isn't a sharable\nthing.  If you have two OTP apps that want different cwds, one of them\n(at least) is going to be sad.\n. +1, merge at your convenience\n. I dunno why this didn't auto-close, but the patch is in \"master\", closing.\n. basho_expect tests pass, eunit tests aren't broken.  Merge it.\n. After changing etc/app.config for rel/riak and dev/dev? etc dirs,\nall of the changes work, for make stage and 'make stagedevreland\nthen running a single node (former) and 4 nodes joined together into\na single cluster (latter), then storing 10K keys, fetching them, and then\nlooking in the appropriate.../data/leveldb` subdirectory to verify that\nleveldb's files are actually there.  +1 to merge, thanks!\n. Comments from IM:\n- Go whole hog with Apache httpd-style 'chkconfig'\n- Do the same for RiakEE's packaging.\n. > Are we ready to have pipe servicing MapReduce requests by default?\nYes, I think we should flip the switch ... but I think if you could wait until\nmerging the branch until Wednesday late afternoon or so ... there are\nsome packaging details and basho_expect testing that I'd like to do with\nbasho_expect first, before this config change.\nOtherwise, ja, let's do it.  Could you also make this change to the RiakEE\nrepo?\n. +1 to merge.  Please don't forget to do the same thing to RiakEE's packaging, thanks!\n. Hrm, looks like I took care of applying this patch to riak_ee \"master\"\nwhile on a packaging-unification rampage back in July.\n+1 to merge, thanks.\nBug 997 also mentions the 0.14 branches of Riak and RiakEE.  Separate\npull requests for each of those branches isn't required, but please apply\nthe patches to those branches and note the commits in the bug ticket.\n0.14 may live a bit longer, who knows, so might as well apply it as Dizzy\nrequests.\n. Hrm, yes, it could be nice ... but then:\n- If someone's window isn't wide enough, what should we do?\n- Not all ssh clients seem to pass window size through\n. Sean, I think we've already got a PR somewhere in the pipeline to remove bash'isms altogether ... so it isn't clear yet if we'll be using this patch.  Regardless of our decision, thanks for making the patches & submitting the pull request.\n. +1, thanks!\n. Ping ... is this PR needed for Riak 2.0?\n. Ping ... is this PR needed for Riak 2.0?\n. No new failing EUnit failures, no significant changes to Dialyzer warnings.  +1 to merge all 5 related PRs, many thanks!\n. Hrm, I thought that I saw this same app config change with the Luke app removal review that I did recently?  If I'm thinking of another config item that @beerriot removed, then great.\nThis same packaging change should also be applied to the riak_ee repo?\nI don't see anything in https://github.com/basho/riak_kv/pull/436 that looks objectionable, +1 to merge.\n. @rzezeski I know you've done some work on merge index data corruption issues ... what are the odds that your recent work applies to this issue?\n. Closing: 327 is done\n. Ping ... is this PR still relevant?\n. Ping ... is this PR still relevant?\n.  I've updated both repos of \"jem-net-ticktime\" with new words.  If you find them inoffensive, +1 to merge.  If offensive, edit them yourself and then don't wait for more review: ship it.\n. Ping ... is this PR needed for Riak 2.0?\n. Ping ... is this PR needed for Riak 2.0?\n. Ping ... is this PR needed for Riak 2.0?  The prerequisite PR is still open and might be close without action because of other 2.0 features.  Thoughts?\n. Thinking of @Vagabond's recent RFC & work, would this be information that would be exposed before or after authentication?\n. This all worked nicely before I added the riak_core_capability negotiation in a later PR.  :-(\nI've reopened these PRs and added bugfix commits:\n- https://github.com/basho/riak_core/pull/354\n- https://github.com/basho/riak_kv/pull/626\n. Correct\n. Ping ... is this PR needed for Riak 2.0?  Looks like it's lacking rebasing & final review?\n. Howdy.  I don't believe that this kind of feature will be added to the 1.4 branch, and as Jared said, the Cuttlefish-based configuration scheme for Riak 2.0 changes your problem.  (Hopefully in a good way!)\n. Notes:\n1. See also: Basho internal dev discussion\n2. This issue also affect's the riak_ee repo's packaging.\n. Ping ... the prereq (?) issue, basho/riak_core#469, has been merged.  Also, Jared's comment about EE also applies.  I'm working on the 25th, so if y'all don't mind, I can do both repos tomorrow.\n. I wouldn't recommend removing that utility, if it's still possible to specify an app.config and vm.args old-style, bypassing Cuttlefish-style?\n. Comments @bsparrow435 or @joecaswell or @jmshoffs0812 or other CSEs?\n. I'm not involved in a lot of support work lately, but when I am, it's annoying to have to go back and ask for diagnostic info batch #2.  If riak debug is supposed to be the be-all, do-all, one-stop shop, then why two stops?\nKota's point, however, I'll concede.  We recently had a customer who ran riak debug on all nodes in the same cluster ... if that was done via Ansible or multi-session tmux, for example, then it could very well be all running simultaneously.  I'll add a local only flag to the cluster info invocation, then merge.  Thanks!\n. Hi, Rune.  I understand the 2nd fix but not the first.  We try very hard to avoid Bash'isms in our shell script code and to use only Bourne shell-compatible syntax, so I do not understand the reason behind suggesting Bash.\n. @rzezeski @coderoshi Any thoughts on a milestone assignment?\n. No, sorry, please use R16.\n. Please see https://github.com/basho/riak/issues/534, there is no schedule for this work inside of Basho.\n. Basho engineering has not been able to settle on a \"storage zone\" design that isn't horrible in at least one common scenario.\nWorkaround: use ZFS or a RAID-style logical volume manager.\n. If a person were to accept the idea that Basho's developers have a very thorough knowledge of Riak's internals, and if no Basho developer has been able to make a per-disk \"storage zone\"-like proposal that can satisfy the rest of Basho internally, then it's quite likely that there is no easy way to implement per-disk \"storage zone\"-like things within Riak today. And every proposal for a future Riak has had serious negative consequences that Basho (internally) has rejected.\nIf you have a PR that demonstrates that we've overlooked a good idea, we'd be happy to take a look at it.  ^_^\nMeanwhile, use RAID today.\n. The number of MFAs that exceed etop's default number of columns is .... too small for a lot of Riak module+function names.  The patch is there to make the output wider, which is a pain of a different kind, admittedly. Being able to see the entire mod+fun name is the end goal, IMHO.\n. ",
    "OJ": "Hey Dizzy,\nThanks for the response mate. I had a feeling that you would look to use something more complete down the track, this was really an attempt at helping to fill the gap in the mean time until you guys had something more solid in place. Certainly no pressure to accept it. Only if you find it useful! (at least it gets rid of the errors you see when making, not that it was hard to do that :)).\nHave a safe trip mate.\nCheers\nOJ\n. Hey Dizzy, did you get a chance to review this? Is it something that can at least help in the interim?\nCheers dude.\n. Hey Ryan,\nThanks for the feedback mate. I totally understand your change. I speculated a lot on what you would have wanted and would have been surprised if I had managed to produce something that made it to the mainline without modification. I am happy if I've managed to help in any way.\nThanks for the response, all the best!\nOJ\n. ",
    "rtilder": "Hey, OJ.  I wound up doing a partial pull of your changes limited to the Makefile targets.  I changed the manner in which the RIAK_TAG is defined in order to work with versions of git prior to v1.6.2(when the git tags --contains flags appeared).  Many thanks.\n--Ryan\n. +1\nI bite my thumb at Scott's FreeBSD leanings with respect to $(call ...) semantics but  his second comment is something we should bug or later correction.\n. ",
    "jonmeredith": "+1 for the script change - have a pending comment on the changes in riak_kv\n(just reviewed change - did not run)\n. Should the stats dep actually be against riak_core instead?  That way we wouldn't need to adjust deps for riak, riak_ee and riak_search\n. Still need to remove the comments above - don't forget ee & search :)\n. +1 - need to do for ee/search too.\n. Thanks for the patch.  We're refactoring some of the riak scripts at the moment and a new version that returns non-zero for lots of failures should be available soon.\n. +1 to merge.\nChecked start / stop / restart / reboot / attach all work.\nRan up another node on the same handoff ports and start fails now with non-zero exit rather than the node crashing silently.\n. abort, abort.\nWait until the packaging refactor is complete.\n. +1 merge.\n. +1 merge.  There may be a clash with the default backend on merge - please make sure bitcask is the default backend for now.\n. +1 merge\n. +1 merge.\n. Merged to 1.0 for riak and riak_ee\n(I had to export a patch, import it on a branch and merge 230d130b5c3c97f5447b9fd4bef8dc276af878a5)\n. +1 merge\n. +1 merge - thank you.\n. +1 merge - agree the certs should probably be auto-generated, we can file it as an additional task to convert.  It doesn't need to hold up the merge.\n. +1 for package/distribution stuff.\nTake a look at the changes we discussed to use '|| echo FAIL' on the eunit test case to make sure all tests are run.\n. One last minor tweak then +1 to merge.\n. +1 merge, don't forget the EEs\n. Fixed.\n. Workaround by recreating the directory on startup.  Append this snippet to /etc/rc.local\nmkdir /var/run/riak && chown -R riak /var/run/riak\n. +1 merge, starts under r14b04 and r15b01 now.\n. I've also noticed ownership problems with /opt/local/etc/riak/vm.args - I have to chown root:riak to make things work.\n. Although I didn't have any problems with the start_erl.data - were you on EE or OSS?\nroot@somemachine:/var/log/riak# ls -l /opt/local/lib/riak/releases/start_erl.data\n-rw-r--r-- 1 riak riak 12 Aug  3 20:16 /opt/local/lib/riak/releases/start_erl.data\n. +1 merge\nChecked \n  ubuntu 10.04\n  freebsd 9 \n  centos 5\n  fedora 15\n  solaris 10u9 (no changes, just made sure patches was right)\nUnchecked\n  smartos - could not find the package on s3 after buildbot.\n. Merged on EE as b2a57ff51f4dcaec64e12ba6b2faf04353150d4a (had to fix the patch for riak-ee.spec)\n. +1 merge\n. Erlangisms like to creep in everywhere\n```\n(jdm3@jons-macpro.jonmeredith.com)2>  1 =< 2.\ntrue\n(jdm3@jons-macpro.jonmeredith.com)3>  1 <= 2.\n* 1: syntax error before: '<='\n```\n. I'd be interested what'd in the error logs for the Linux 1.2.1 failure.\nOn OSX I suspect you've hit an issue with HiPE mmapping to 512Mb for native code (currently used by bear).  You could either try building the erlang VM with HiPE disabled, or you could modify deps/bear/src/bear.erl and comment out the -compile([native]) line.\n. +1\n. Worth noting that you can now set maximum sizes though.\n. I believe Swift does something similar to the cluster test.  Agreed we should do something like this.  Question is what to do about exposing the keys to list buckets / list keys, although I suppose the current test code writes a visibile key/value.\n. @evanmcc @jaredmorrow I think the time window has closed on this, unless we're planning to rerun backend/cluster testing with this enabled.\n. l vote for after 2.0. I'll reset this milestone to 2.1\n. Yokozuna vs Search2 'branding'.\n. I regularly see dropped messages on startup on my year old mbp.  Normal startup shouldn't trip the threshold.\nIs it feasible to have lager have a higher message/sec threshold during the initial application startup, then perhaps dropped back once all applications started?\nThe other way to attack it is to look and see if any of the startup messages should be lowered in priority.\n. @matthewvon @evanmcc If we wouldn't recommend ever running with ulimit lower than 64k, we should merge this, given that it's only advisory.\n. Is merging #609 sufficient to close this?\n. @jaredmorrow were there any fixes outside basho/node_package#146 needed to close this?\n. Assigned to 2.0.1 as a placeholder, will be triaged into 2.0.x series at later date.\n. I'd be tempted to keep it as time based and keep the current so that if we\njust have the body of HTTP/JSON requests we'll be able to work out when\nreadings were from.\nIf we already have it in a different field, I'm fine with zero.\nNot sure if there's any value in a 'cached: true/false' type field.\nJon\nOn Tue, Aug 12, 2014 at 11:26 AM, John Burwell notifications@github.com\nwrote:\n\n@russelldb https://github.com/russelldb as I understand Exometer, it\ncan be configured to cache. @uwiger https://github.com/uwiger, if my\nunderstanding is correct, would it be correct to update these values when\ncaching is enabled and report \"0\" when it is disabled?\n/cc @jonmeredith https://github.com/jonmeredith\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/basho/riak/pull/576#issuecomment-51947211.\n\n\nJon Meredith\nVP, Engineering\nBasho Technologies, Inc.\njmeredith@basho.com\n. erlang:process_flag(sensitive, true) will help with the crash.log, at the price of not being able to debug problems. Controlling that being set may be an option. Logging in general is tougher and requires a full audit.\nPerhaps providing the notion of never-user-data, permit-keys, all-data or something like that as a configuration setting for either Riak or lager, and adding some extended modifiers to mark sensitive data in the format string.\n. @binarytemple nope, sensitive processes would need to check on startup - could potentially do it as a bucket type property.\n. ... for extra clarity.\nThe process itself needs to set it once it is created.\n. Thanks for reporting, we're aware of the issue and have work scheduled against the 2.0.X series to improve.\n(Tracked against RIAK-1182 in our internal work tracking system).\n. Unfortunately not - it has required refactoring the inter-cluster connection manager and at this time we view the changes as too significant to backport.\nYou can trigger a rebalance by stop/starting the realtime subsystem (pending writes will be queued as long as they are enabled). Additionally our support team have some workarounds to do more targeted rebalancing that require running some code in the erlang shell. Please open a ticket on ZenDesk at help.basho.com if you need assistance with that.\n. When the issue is happening, could you attach to the riak console (riak-admin attach) on one of the nodes and run\nrp(riak_core_node_watcher:nodes(riak_kv)).\n(Dot at the end important, ^G q to exit), And check if all of your nodes are in the list.\n. I believe the setting you need is rtq_max_bytes under riak_repl.\nYou can also 'do it live' with riak_repl_rtq:set_max_bytes(1073741824), but that won't survive a restart (or process crash).\nIn future, we should probably make JIRA file RiakEE issues against https://github.com/basho/riak_ee-issues.\n. Apologies, got the wrong module... the price of rolling upgrade compatibility. riak_repl2_rtq:set_max_bytes(1073741824)\n. Thx for the speedy update @k-bx - I was a bit puzzled by the crash dump :)\n. The truly paranoid test with Solaris /bin/sh ;)\nOn Tue, Jun 23, 2015 at 8:37 AM Jason Voegele notifications@github.com\nwrote:\n\nMerged #743 (RIAK-1862) https://github.com/basho/riak/pull/743.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/basho/riak/pull/743#event-337893050.\n. Hey @jenagraham I took a look at both and it looks like it scooped up some extra changes.  Would you mind submitting with the single riak-debug change please?\n. -1 I think this scoops up a lot of other changes and we just want 09ed9f3e8ec5c2ea447894ba55ef022898b46313 reapplied to 2.0\n. -1 similar issue, think we just want 09ed9f3e8ec5c2ea447894ba55ef022898b46313\n. Thanks @jenagraham will get to it tomorrow while cutting 2.0.6rc1\n. +1 122af1c\n. L31/L30 should be swapped, with || echo \"Eunit: root FAILED\" >> $(TEST_LOG_FILE);)  after the rebar eunit\n. Otherwise a top level failure will not be spotted and the output is weird\n\n```\n==> rel (eunit)\n==> riak (eunit)\nEunit: basho_stats FAILED\nEunit: riak_kv FAILED\nEunit: riak_pipe FAILED\n./rebar eunit skip_deps=true\n==> lucene_parser (eunit)\n  All 91 tests passed.\nCover analysis: /Users/jmeredith/basho/work/1.1/riak/deps/riak_search/apps/lucene_parser/.eunit/index.html\n==> rel (eunit)\n==> riak (eunit)\n  There were no tests to run.\nCover analysis: /Users/jmeredith/basho/work/1.1/riak/.eunit/index.html\nmake: *** [test] Error 3\n```\n. I think [[ is a bashism - how about if \ntest \"$2\" == \"-f\"\ninstead?\n. ",
    "seancribbs": "The alternative to what you mention is to have it be dumped in the directory where the emulator is running, which is what caused lost crash dumps in first place (/usr/lib/riak is not writeable).  From the erts manual (http://erlang.org/doc/apps/erts/crash_dump.html):\n\"The system will write the crash dump in the current directory of the emulator or in the file pointed out by the environment variable (whatever that means on the current operating system) ERL_CRASH_DUMP. For a crash dump to be written, there has to be a writable file system mounted.\"\n. Scott: is there a way to tell the emulator from vm.args or app.config to chdir after starting?\n. Surely we're not the first to run into this. Maybe a patch to OTP is in order as well?\n. This has already been applied elsewhere.\n. Documentation? SSL is always persnickety about what it allows, so knowing the purpose and format of each certificate would be helpful (also how to generate them if necessary).\n. @russelldb When PB refactor lands, this will be obselete. OTOH I think basho/riak_kv#81 has already landed?\n. Cheesy demo video: http://vimeo.com/26782465\n. Dare I ask for SHA256 hashes as well? The Chef cookbook uses them.\n. Hi @wardbekker, this is really stale and we have support for Travis now on our branch/leaf repositories, so I'm closing this PR.\n. Closing in favor of doing a more general approach with writing a pidfile.\n. Closing since the rel templates include a default certificate and key.\n. @jaredmorrow The memory_use crash has already been fixed on Solaris, but anecdotally it also has problems on Ubuntu.  I've opened an issue for the net_kernel/net_sup crash https://github.com/basho/riaknostic/issues/17.\n. @jaredmorrow Also opened documentation issue for the Fedora directory: https://github.com/basho/riaknostic/issues/18\n. @jaredmorrow Merging your PRs as well.\n. This will be obsolete now that riak_api is merged.\n. :+1: merging\n. @mirell PR? Should be a simple fix and make it before feature freeze.\n. Duplicate of basho/riak_kv#326\n. Why not just put the ^ at the front of the original regexp, which accomplishes the same thing?\n. Also note that [s]*name is wrong and should be s?name. Erlang will ignore a switch like -sssssssssname. I vote for grep \"^[[:space:]]*-s\\?name\"\n. :+1:\n. :+1:\n. This is already being addressed by basho/riak_api#6\n. @bsparrow435 The new riak-admin cluster replace might be more appropriate, since reip was always broken and required the node to be down.\n. :+1: merging\n. Assigning to 2.1\n. @evanmcc Disagree. In scenarios where disterl is unresponsive, remsh might refuse to connect but attach always has the pipes.\n. @evanmcc I know, I was more commenting on your suggestion to remove it entirely. Aside from the inability for Erlang to daemonize itself sensibly, having the run_erl/to_erl pair can be extremely helpful for support.\n. @evanmcc :beers: \n. I might also mention that running q(). from the remsh still quits the Riak node. At least you won't hang it by hitting Ctrl-C, however.\n. @doubleyou See my comment above. If disterl is unresponsive (busy_dist_port, e.g.), connecting to the fifos will still work.\n. See basho/riak_test.\n. Here is the corrected URL: http://downloads.basho.com.s3-website-us-east-1.amazonaws.com/riak/1.2/1.2.1/ubuntu/natty/riak_1.2.1-1_amd64.deb\n. @tomascharvat In the release notes we mentioned that IPv6 is not fully supported for the Erlang distribution protocol, but only that handoff and the HTTP and Protocol Buffers APIs supported it.  Until disterl/epmd and the name-resolution code fully support it, you will have to use IPv4 addresses for node names. See also http://erlang.org/pipermail/erlang-patches/2013-February/003528.html\n. Yes, the -name vm.args configuration parameter is different than the handoff_ip setting. Once connected over disterl, the nodes will query each other over that channel for the handoff IP before establishing handoff connections.\nCurrently, only HTTP is dual-stack, specified via multiple listeners. PB only supports one listener at the moment, so it will only allow one stack, the same is the case for handoff. I plan to look into multiple PB listeners for Riak 1.4.\n. See basho/webmachine#128\n. @jaredmorrow Roger, wilco\n. I don't think we'll be adding stats to the PBC API because I understand we have a plan for having a separate \"admin\" interface that is single-protocol. Also, with how quickly the stats structure changes in each release, I doubt we could provide a message type that would make sense.\nThat said, we are making a strong effort to provide parity for all other features.\n. Suggested reviewers: @jaredmorrow @slfritchie @Vagabond \n. @jaredmorrow I'll look into porting it to node_package.\n. @slfritchie Nope!\n. We already have a fork of LevelDB as a backend, and it is heavily optimized for Riak's use-case. @matthewvon (our lead engineer on LevelDB) has already said that he will look into integrating HyperDex's changes when time permits.\n. @Vagabond I have a patch incoming. :smile: \n. Riak values are not constrained to any specific data/content-type, I don't see how this would even apply.\n. According to some of @cmeiklejohn's investigation today, the crypto:hash(Algo, Term) form emerged in R15B02. Can we really claim compatibility with releases earlier than that in rebar.config?\n. Disregard, you've added a macro definition to the build process.\n. @kuenishi I might mention that for the first pre-release we have decided not to allow maps-in-maps via the public API (although they are technically possible).\n. Yes.\nSean Cribbs\n\nOn Nov 29, 2013, at 8:40 AM, Max notifications@github.com wrote:\nHi, are you people aware of this work? http://run.unl.pt/bitstream/10362/7802/1/Sousa_2012.pdf\n\u2014\nReply to this email directly or view it on GitHub.\n. I think we've addressed the RFC bits of this for 2.0, closing. Reopen with comments if you disagree.\n. @cdahlqvist From my reading of the proposal, all operations will be authorized and audited.\n. @slfritchie I think at least it should be filterable by what hosts patterns the user can connect from. This needs more thought!\n. @bakins What client(s) are you currently using?\n. @bakins Not sure we can manage writing our own proxy, but with the way this is designed, it should be very straightforward to write a daemon that reconfigures existing proxy software. Alternatively, a Chef/Puppet/etc run could be used to rewrite the configuration, which is what most people do nowadays. It is my intention that admins will be able to turn off the feature, and client participation is totally optional, even if the feature is enabled.\n. I'd love to, but there's no uint16 type in protobuff.\n\nOn Tue, Aug 27, 2013 at 3:26 AM, UENISHI Kota notifications@github.comwrote:\n\nA small nitpick:\nmessage RpbClientInterface {\n    enum RpbProtocol {\n        PB = 1;\n        HTTP = 2;\n        HTTPS = 3;\n    }\n    required RpbProtocol protocol = 1;\n    // This can be a FQDN or a text representation of an IP address.\n    required bytes host = 2;\n    required uint32 port = 3;    }\nTo prevent from wrong value, isn't it good to set the type of port as\nuint16 ?\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/basho/riak/issues/356#issuecomment-23320266\n.\n\n\nSean Cribbs sean@basho.com\nSoftware Engineer\nBasho Technologies, Inc.\nhttp://basho.com/\n. Unfortunately due to time constraints and other big features, this will not be in 2.0.\n. Closing for the rebase.\n. Actually, let's just use #379 for this.\n. @uberbrady I think it's more constructive to think of them outside the context of the HTTP API. It clouds the issue.\n. :+1:\n. The generators for each individual backend in multi-backend are fine, but the multi_backend_default comes out as a string, not a binary.\nsysctl\nstorage_backend = multi\nmulti_backend.default = bitcask_mult\nmulti_backend.bitcask_mult.storage_backend = bitcask\nmulti_backend.bitcask_mult.bitcask.data_root = data/multi/bitcask\nmulti_backend.leveldb_mult.storage_backend = leveldb\nmulti_backend.leveldb_mult.bitcask.data_root = data/multi/leveldb\nmulti_backend.memory_mult.storage_backend = memory\nmulti_backend.memory_mult.memory_backend.ttl = 1d\nbecomes\nerlang\n {riak_kv,\n     [{multi_backend_default,\"bitcask_mult\"},\n      {multi_backend,\n          [{<<\"bitcask_mult\">>,riak_kv_bitcask_backend,\n            [{data_root,\"data/multi/bitcask\"},\n             {open_timeout,4},\n             {max_file_size,2147483648},\n             {frag_merge_trigger,60},\n             {dead_bytes_merge_trigger,536870912},\n             {frag_threshold,40},\n             {dead_bytes_threshold,134217728},\n             {small_file_threshold,10485760},\n             {max_fold_age,-1},\n             {max_fold_puts,0},\n             {expiry_secs,-1},\n             {require_hint_crc,true},\n             {expiry_grace_time,0},\n             {io_mode,erlang},\n             {sync_strategy,none},\n             {merge_window,always}]},\n           {<<\"leveldb_mult\">>,riak_kv_eleveldb_backend,\n            [{data_root,\"./data/multleveldb\"},\n             {max_open_files,30},\n             {cache_size,8388608},\n             {sync,false},\n             {write_buffer_size_min,15728640},\n             {write_buffer_size_max,31457280},\n             {sst_block_size,4096},\n             {block_restart_interval,16},\n             {verify_checksums,true},\n             {verify_compaction,true},\n             {use_bloomfilter,true}]},\n           {<<\"memory_mult\">>,riak_kv_memory_backend,\n            [{ttl,86400},{max_memory,4096}]}]},\nAlso, I did not include max_memory for the memory backend in the original config, but it is emitted in the generated config.\n. :+1: This looks good. We need to have more validators! :smiley_cat: \n. :+1: on the lager changes only. Either manually merge/cherry-pick or open a new PR.\n. In order for the second commit to work, we need:\n- basho/riak_core#381\n- basho/riak_api#34\n- basho/riak_test#375\nThe related basho/riak_kv#629 was already merged.\n/cc @Vagabond \n. @joedevivo All of the webmachine related pull-requests are merged, and @Vagabond have run riak_tests against this. :+1: to merge\n. :+1:\n. :+1: \n. :+1: Thanks for the fix!\n. So, I get the reasoning for the config, but why remove the plugin?\n. Ok then, :+1: \n. :+1:\n. :+1: \n. We have tests that demonstrate that listing buckets within known bucket-types works. Whether bucket-types themselves should be able to be listed from a client API is a separate issue, I believe (caveats around security/disclosure, etc).\n. This seems evidence that we also need to document and proclaim the purpose of advanced.config very loudly. This would not be an issue if they just copied them into advanced.config.\n. This seems stale. We've already addressed the latency issue via smaller contexts and t2b compression, and there are existing issues/tasks for benchmarking. Closing.\n. No, we explicitly omitted that feature for this release, to be revisited later. Note that on activated types, some properties may also be immutable.\n. Did you run riak-admin bucket-type activate mytype after creating it?\n. Are all of the nodes in your cluster up and reachable? A bucket-type can't be activated until all nodes have seen it.\n. Is this just a difference in shell-quoting? Is the shell you're using\nignoring the single-quotes?\nOn Wed, Feb 12, 2014 at 1:53 PM, Oleksiy Krivoshey <notifications@github.com\n\nwrote:\nThe problem still exists with pre14:\nvagrant@precise64:~$ curl -sO http://s3.amazonaws.com/builds.basho.com/riak/develop/2.0.0pre14/ubuntu/precise/riak_2.0.0pre14-1_amd64.deb\nvagrant@precise64:~$ sudo dpkg -i riak_2.0.0pre14-1_amd64.deb\nSelecting previously unselected package riak.\n(Reading database ... 51127 files and directories currently installed.)\nUnpacking riak (from riak_2.0.0pre14-1_amd64.deb) ...\nSetting up riak (2.0.0pre14-1) ...\nAdding group riak' (GID 111) ...\nDone.\nAdding system userriak' (UID 106) ...\nAdding new user riak' (UID 106) with groupriak' ...\nNot creating home directory `/var/lib/riak'.\nProcessing triggers for ureadahead ...\nureadahead will be reprofiled on next reboot\nProcessing triggers for man-db ...\nvagrant@precise64:~$ sudo service riak start\n!!!!\n!!!! WARNING: ulimit -n is 1024; 4096 is the recommended minimum.\n!!!!\nvagrant@precise64:~$ riak-admin bucket-type create dt-test-set '{\"props\": {\"datatype\": \"set\", \"allow_mult\": \"true\"}}'\nUsage: riak-admin bucket-type create  '{\"props\": { ... }}'\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/basho/riak/issues/487#issuecomment-34909701\n.\n\n\nSean Cribbs sean@basho.com\nSoftware Engineer\nBasho Technologies, Inc.\nhttp://basho.com/\n. @superstructor We'll be making the release candidate publicly available as soon as it's ready.\n. Eng +1\n. Note that previously, this was slurped in from a separate file. Unfortunately you cannot pass init arguments from the command line into an escript; they have to be specified in the comment line after the shebang. This means that any disterl-related arguments have to be in a file that is referred to in the comment line, which is what was done in nodetool in the past. Fixing this issue would require changes to OTP.\n. You already have a riak node running with that name -- this is indicated by\nthe duplicate_name error. Run ps aux | grep riak to see if there is an\nexisting Riak process running.\nOn Mon, Mar 17, 2014 at 10:50 AM, buriwoy notifications@github.com wrote:\n\nI installed riak 2.0 from source (make rel). Also, installed basho/otp\n(patched erlang R16).\nps aux | grep riak shows no other riak instances. Rebooted my whole\nserver. But I'm getting this:\n./riak start\nriak failed to start within 15 seconds,\nsee the output of 'riak console' for more information.\nIf you want to wait longer, set the environment variable\nWAIT_FOR_ERLANG to the number of seconds to wait.\nriak console is outputting this:\n15:47:53.167 [warning] cuttlefish_util:filter_by_variable_starts_with/2 has been deprecated. use cuttlefish_variable:filter_by_prefix/2\nconfig is OK\n-config /home/ld/apps/riak-2.0.0pre11/rel/riak/data/generated.configs/app.2014.03.17.15.47.52.config -args_file /home/ld/apps/riak-2.0.0pre11/rel/riak/data/generated.configs/vm.2014.03.17.15.47.52.args -vm_args /home/ld/apps/riak-2.0.0pre11/rel/riak/data/generated.configs/vm.2014.03.17.15.47.52.args\nExec:  /home/ld/apps/riak-2.0.0pre11/rel/riak/bin/../erts-5.9.2/bin/erlexec -boot /home/ld/apps/riak-2.0.0pre11/rel/riak/bin/../releases/2.0.0pre11/riak               -config /home/ld/apps/riak-2.0.0pre11/rel/riak/data/generated.configs/app.2014.03.17.15.47.52.config -args_file /home/ld/apps/riak-2.0.0pre11/rel/riak/data/generated.configs/vm.2014.03.17.15.47.52.args -vm_args /home/ld/apps/riak-2.0.0pre11/rel/riak/data/generated.configs/vm.2014.03.17.15.47.52.args              -pa /home/ld/apps/riak-2.0.0pre11/rel/riak/bin/../lib/basho-patches -- console\nRoot: /home/ld/apps/riak-2.0.0pre11/rel/riak/bin/..{error_logger,{{2014,3,17},{15,47,53}},\"Protocol: ~p: register error: ~p~n\",[\"inet_tcp\",{{badmatch,{error,duplicate_name}},[{inet_tcp_dist,listen,1,[{file,\"inet_tcp_dist.erl\"},{line,70}]},{net_kernel,start_protos,4,[{file,\"net_kernel.erl\"},{line,1314}]},{net_kernel,start_protos,3,[{file,\"net_kernel.erl\"},{line,1307}]},{net_kernel,init_node,2,[{file,\"net_kernel.erl\"},{line,1197}]},{net_kernel,init,1,[{file,\"net_kernel.erl\"},{line,357}]},{gen_server,init_it,6,[{file,\"gen_server.erl\"},{line,304}]},{proc_lib,init_p_do_apply,3,[{file,\"proc_lib.erl\"},{line,227}]}]}]}{error_logger,{{2014,3,17},{15,47,53}},crash_report,[[{initial_call,{net_kernel,init,['Argument__1']}},{pid,<0.20.0>},{registered_name,[]},{error_info,{exit,{error,badarg},[{gen_server,init_it,6,[{file,\"gen_server.erl\"},{line,320}]},{proc_lib,init_p_do_apply,3,[{file,\"proc_lib.erl\"},{line,227}]}]}},{ancestors,[net_sup,kernel_sup,<0.10.0>]},{messages,[]},{links,[#Port<0.224>,<0.17.0>]},{dictionary,[{longnames,true}]},{trap_exit,true},{status,running},{heap_size,610},{stack_size,24},{reductions,510}],[]]}{error_logger,{{2014,3,17},{15,47,53}},supervisor_report,[{supervisor,{local,net_sup}},{errorContext,start_error},{reason,{'EXIT',nodistribution}},{offender,[{pid,undefined},{name,net_kernel},{mfargs,{net_kernel,start_link,[['riak@127.0.0.1',longnames]]}},{restart_type,permanent},{shutdown,2000},{child_t\n ype,worker}]}]}{error_logger,{{2014,3,17},{15,47,53}},supervisor_report,[{supervisor,{local,kernel_sup}},{errorContext,start_error},{reason,shutdown},{offender,[{pid,undefined},{name,net_sup},{mfargs,{erl_distribution,start_link,[]}},{restart_type,permanent},{shutdown,infinity},{child_type,supervisor}]}]}{error_logger,{{2014,3,17},{15,47,53}},std_info,[{application,kernel},{exited,{shutdown,{kernel,start,[normal,[]]}}},{type,permanent}]}{\"Kernel pid terminated\",application_controller,\"{application_start_failure,kernel,{shutdown,{kernel,start,[normal,[]]}}}\"}\nCrash dump was written to: ./log/erl_crash.dump\nKernel pid terminated (application_controller) ({application_start_failure,kernel,{shutdown,{kernel,start,[normal,[]]}}})\nWhat am I doing wrong?\nThanks!\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/basho/riak/issues/512\n.\n\n\nSean Cribbs sean@basho.com\nSoftware Engineer\nBasho Technologies, Inc.\nhttp://basho.com/\n. Just because it doesn't appear to have started according to the launcher/control script, doesn't it mean that it didn't start. Use ps aux and tail -f on the console log file to be sure.\n. What does this have to do with riak_core_node_watcher?\n. o man o man o man\n. Content-Type header from the client specifies the request media-type, not the acceptable ones. You also cannot update via GET.\nIf you use -H \"Accept: application/xml\" you should get the appropriate 406 response.\n. np, been off my game all week\n. Also, the properties are not w_val and r_val, but w and r respectively.\n. :+1: Easy-peasy\n. We do not support Erlang/OTP R16 on the 1.4.x version of Riak. It's better to use a packaged version of Riak for deployment or your local testing.\n. > %% We left riak_kv.add_paths out on purpose.\nhttps://github.com/basho/riak_kv/blob/develop/priv/riak_kv.schema#L276\n. :+1:\n. @russelldb I think there's a subtle difference between \"wrong\" and \"surprising\", but from the user's perspective both could be \"wrong\". Let's discuss more in detail Monday as a group.\n. Closing. To summarize:\nThe proper semantics don't really exist, we are figuring them out. Once we have determined them, the change can be made. Making this a Known Issue implies we know or at least have vague idea of what needs fixing. The reset-remove semantic can indeed cause things to \"go back in time\", but we know that LWW is lossy. Caveat implementor!\n. - [x] CSE +1 (not @angrycub)\n- [x] Eng +1 (not @seancribbs)\n. This PR's pair was merged, merging.\n. Our bitcask experts tell me that bitcask.expiry.grace_time is only settable globally. What happens if you just comment the final line?\n. @pma Sorry about this bug. I will be writing a test and the fix should land by 2.0.1. In the meantime, you can configure multi_backend in advanced.config.\n. @angrycub I think we've found the source of this. Thanks for the backtrace.\n. Iterating over Mapping objects yields keys, not values. Try this instead:\npython\nfor s in sample_map.sets:\n    print s\n    # Or if you want the value:\n    print sample_map.sets[s]\n. @przemien Ah yes, that was fixed early after release, but we have not issued a patch release of the client. My apologies.\n. @Basho-JIRA ping\n. This is an incorrect payload. The correct way is:\njavascript\n{\"update\": {\n          \"creat1_map\": {\n           \"update\" : {\n               \"country1_map\" : {\n                  \"remove\" : [\"impressions_counter\", \"clicks_counter\",\"downloads_counter\"]\n               }\n           }\n          }\n      }\n    }\nAll of our official libraries create this correctly. It's also important to note that repeated keys of the same name in a JSON object has undefined semantics.\n. This resolved by basho/node_package#170 and will be fixed in 2.0.3.\n. Configuration of buckets is insufficient to get \"perfect consistency\" (which seems to mean \"recency\" in your case). What you want is Strong Consistency.\n. No, this functionality is not in 1.x. \nYour other option is to refetch if the expected value is not received, or turn on the \"return body\" option that replies to writes with the updated value. There is a probabilistic bound on the staleness of any item, so simply refetching the item may work. However, there is no guarantee that you will read your own writes and sessions are not sticky.\nIf you cannot tolerate eventual consistency in its essential behavior, maybe Riak is not a good fit. Alternately, you can upgrade to 2.0 for those items that specifically cannot tolerate staleness. For the rest of your data that doesn't need strong recency, regular eventually-consistent Riak might still work for you.\n. @Basho-JIRA ping\n. :+1:\n. @Basho-JIRA ping\n. If you started the node before changing the ring_size, it will be already persisted to disk. Stop all the nodes, remove /var/lib/riak/ring (assuming a Linux install), ensure the ring_size is set correctly on both nodes, and then restart and rejoin.\n. That should not be possible if you are following the protobufs spec, the bucket field is marked required.\n. We have binary packages for CentOS 6. Please use those to install Riak.\nOn Thu, Jan 29, 2015 at 10:21 PM, molele2 notifications@github.com wrote:\n\nERlang 16B centos6.5 64 riak-develop .2.0.3,2.0.4,2.0.2 no a way\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/basho/riak/issues/674#issuecomment-72159433.\n\n\nSean Cribbs sean@basho.com\nSr. Software Engineer\nBasho Technologies, Inc.\nhttp://basho.com/\n. So, my main reservation here is that this means a new lock file will be generated every time a release is generated, even in development. Is there an alternative?\n. @Basho-JIRA ping\n. It is something we have discussed internally in the past (we called them \"storage zones\"), however isn't this something generally better handled by your volume manager? There are existing solutions to the problem.\n. Use @gglanzani's suggestion, but this is really a question better for the mailing list than for the issue tracker.\n. :+1: This looks good.\n. Note to future self: this looks like it could be fixed in Cuttlefish by allowing 0 to be unit-less.\n. :+1: Identical to EE\n. Put it on a separate line so it separates the config option from the documentation for it.\n. Aha, thanks, fixing.\n. Typo: async_threads\n. This is \"erlang\" in cuttlefish, but we want to keep the customization in Riak.\n. Why? \n\n. Couldn't you just read riak.conf directly?\n. Thanks.\n. ",
    "russelldb": "yeah, is old, and dead, closing now, thanks.\n. Looks good, works fine. Created a patch for riak_ee and opened PR https://github.com/basho/riak_ee/pull/60, you should merge that when you merge this.\n. I think this issue is the same as this issue https://github.com/basho/riak_kv/issues/411\n. Fixed with https://github.com/basho/riak_kv/pull/415, please confirm and I'll close this.\nPR is merged to master, so a build from masterwith r15b02 is all you need, thanks.\n. Erm, woah. Can we talk, 'cos I'm worried about the merges for the stats work I've done with the fsms?\n. Warning didn't appear for an entry of\n{pb, []}\nComment above fixes that. Apart from that +1.\n. Yup, +1.\n. Tested on some legacy indexes, worked as advertised. +1\n. Copying initial comments from private repo:\nFrom @metadave \n\n@seancribbs and I are talking about a general query language for Riak, to be implemented in Erlang, that includes CRDT's:\nhttps://gist.github.com/metadave/6024103\n\nFrom @lenary \n\nI also have ideas for a CRDT Query/Update language: https://gist.github.com/lenary/da1b98a97b43a06ecf3e\nHowever they need some work, and after discussion with @seancribbs it seems I might take them down a completely different track. :)\n. There are many ways to make a CRDT map it seems. None of them are really satisfying to me (yet). But I need to pick one and move on. If you have the time, please help.\n\nAll the maps use something like an OR-Set for keys. All Maps map keys of {name(), crdt_type()} -> crdt(crdt_type()).\nOriginal (Carlos Map): OR-Set for keys, dict of Key->Value\nUpside: pretty small. Only keyset has tombstones.\nDownside: A(update >> remove) | B(update) loses A's update on merge. This is because the remove simply drops the value at A, so B keeps all of A's updates up-to when they diverged and re-introduces them to the value on merge. Add wins, but something is missing.\nRemove Wins: dict of {name(), crdt_type()} -> {vclock(), active(), crdt(crdt_type(), tombstone() | undefined}\nUpside: Semantic is pretty clear: when you remove an element we write a tombstone. On merge any updates concurrent with the remove are dropped. The field can be re-added. If it is re-removed update the tombstone clock.\nDownside: Long partition behaviour is confusing (imagine MDC write - write clusters) If a remove of a field happens on A that is concurrent with updates on B all B's updates are dropped on merge.\nAdd Wins: same as above except a list of tombstones, each a {clock(), crdt()} pair\nUpside: no write is ever lost, ever, ever.\nDownside: fiddly, complex impl, lots of space overhead for tombstones, surprising behaviour in long partitioned replicas (i.e. if A(update >> remove >> update >> remove) | B(update >> remove >> update >> remove) field is present and merged value of all updates)\nReset-Remove: dict of {name(), crdt_type()} -> {vclock(), active(), crdt(crdt_type())} mappings\nUpside: simple to understand semantic, appears most correct most often during partitioned state\nDownside: doesn't support every CRDT, (i.e. G-Counter, G-Set, One Way Flags are unsupported)\nWorks thusly: A remove first resets the crdt value (for a counter this means issue a decrement / increment that sets the counter to zero, for a set remove all elements, for a map reset/remove all fields), then it removes the key. The value is kept as a tombstone.\nIf the user wants to re-add the field, it is as though it were an empty field, but any concurrent updates at other replicas maintain their writes, and the value they had for the removed replica are dominated by the reset operation.\nPick one!\n. @kuenishi Unless I misunderstand your question, this composition is what  the Map is for. A Map is like a JSON document (for example.) Each field in the Map is a CRDT. So there may be Maps in Maps (in Maps).\nYour model would be expressed as (assuming the Key in Riak is User):\n{ {name, riak_dt_lwwreg} -> lwwreg,\n   {email, riak_dt_lwwreg} -> lwwreg,\n   {total_count_liked, riak_dt_pncounter} -> pncounter,\n   {friends, riak_dt_orset} -> orset\n}\nand so on. The Map is used to compose CRDTs. We can only store CRDTs in the Map, the Map's field names include the Type of the thing stored, so Ints are pncounters, strings are Last Write Wins Registers, Lists of Friends are OR-Sets, and so on.\nDoes that answer your question?\n. @radeex the tombstoneless set is based on this work (http://arxiv.org/pdf/1210.3368.pdf ([4] above)), in fact we've been very lucky to have Carlos Baquero work with us on the Map and Set implementation.\nThe confusion about the OR-Set you mention is probably, as @lenary points out, the difference between a state and op based set. The paper at [1] says:\n\"We leave the corresponding state-based speci\ufb01cation as an exercise for the reader. Since\n  every add is e\ufb00ectively unique, a state-based implementation could be based on U-Set.\"\nThe easiest way to think of this is two U-Sets, one for adds, one for removes. And a naive implementation would do just that. In practice you can keep an unique ID per actor, and increment that per add (starting to look a lot like a version vector per element), rather than store all IDs, and use a single bit to denote if an element is present or not\u2026and keep optimising until you end up with an ORSWOT.\n. Hi @maxsz yes! I have read parts of Valter's dissertation (though not all yet). I've been corresponding with Valter, and  Professor Nuno Pregui\u00e7a (his supervisor), along with Carlos Baqueror and others from the CRDT research team.\nWe're actually involved in SyncFree, a new 3 year research project, and hope to bring useful discoveries into Riak in the future. If you want stay informed on that project there is a website here https://syncfree.lip6.fr/\nIn fact, Valter has been looking at CRDT invariants, and I believe he is using the pre5 riak release (with CRDTs) for his research at the moment. He's been on the Riak mailing list lately.\n. I had the same problem and this fixes it for me +1\n. Yes, I can confirm that is fine for make rel / make devrel, but I still get an error from riak_test\nhttps://gist.github.com/russelldb/356d2735b52fffa1c2e7\n. ./riak_test -c rtdev -b bitcask -t verify_counter_converge\nI re-made devrel and re-ran rtdev-current. Don't worry about it\u2026I have a pretty messed up set of branches, going to start from scratch tomorrow.\n. OK\u2026\nhttps://gist.github.com/russelldb/1781bde57b750e1659bd fixes it for me.\n. @roncemer is this any use to you https://github.com/basho/riak_crdt_cookbook/blob/master/counters/README.md?\n. @roncemer We should talk about the incorrectness you're seeing in counters, if you could provide anything that helps reproduce the bug I'd appreciate it. We tested counters pretty extensively, and apart from their non-idempotence in the face of partial failures, we're confident that they're accurate under heavy concurrent use. We never lose an increment we say has been written. We only ever possibly duplicate an increment if we say we failed, but the failure was partial and client retried. If you can demonstrate this to be untrue, I'd be very grateful.\nThe MR JS issues is because we chose an efficient binary format for counters that JS cannot work with. I chose to sacrifce JS MR support in favour of a better binary format. Computering is trade-offs, and I thought that one acceptable.\n. @TJC is it OK to close this issue then? Did you open an issue against the Java Client?\n@roncemer Please can you open an issue against riak_kv with details of the counter issue you've experienced? Please provide steps to reproduce as running multiple concurrent workers against riak counters is something we've done many times in testing and we do not see the behaviour you describe.\n. @TJC ah, OK. Did you raise an issue against the Java Client that I can cross-ref here, then?\n. @roncemer I'm loathe to continue this discussion on this ticket, but I re-ran all our counter tests this morning, including writing a simple reproduction as you described above (see https://gist.github.com/russelldb/a83f5cd5e430c20cc33a) and couldn't provoke either under or overcounting.\n. @TJC Narp, no need if it is fixed. Thanks!\n. We'll look into it. We're not finished making 2.0, and we're only feature complete on CRDTs (not yet optimised at all.)\nAs for size, well, anything you wouldn't store in riak_object you can't store in a Set, since it is just stored in a riak_object. There is also some metadata overhead. If you want to add many values to a set, why not use the add_all operation instead? I think you'll find it much, much, faster.\n. Please post your numbers of doing this in a tight loop with riak_object (fetch, update, put) for the same numbers.\n. Yeah, I'm just pushing a different version of this. riak_dt_orswot:to_binary/1 is the biggest culprit. If you try dropping https://github.com/basho/riak_dt/tree/rdb/orswot-opt branch of riak_dt in as a replacement and test again. We're actually starting on the perf stuff next week, we've known that there are issues (same thing with maps.) We'll keep working on it. Thanks for testing the data types.\n. Cool. Bear with us and we'll keep improving this.\n. Did the value of the object change between when you did the 2i query and when you read the object with the get?\n. I wonder if it is stats? Could you turn up the stat calc TTL to something really long?\nI can't find the setting in cuttlefish, so I guess it'll need to go in your advanced.config\n[{riak_core, [{stat_cache_ttl, $SOME_NUMBER_OF_SECONDS}]}].\nMake $SOME_NUMBER_OF_SECONDS big to make infrequent the background calculation of stats, and let me know what difference that makes, please?\nAlso, until we have a known issue, the mailing list[1] is the best place for this kind of thing, then the community at large benefits, there is general visibility. Assuming we then find an issue as a result, we can open it here. Please?\n[1] riak users mailing list - http://lists.basho.com/mailman/listinfo/riak-users_lists.basho.com\n. Is the spike frequency and size the same whatever TTL setting you use? For example, change that TTL to 120, and is the behaviour the same?\n. So that setting solves your problem? This suggests that there is an issue with stat calculation taking up too much resource.\nWhat is a safe setting for the frequency of stat calculation depends on how often you consume them. A 30 second window might be too big for most users. Once a second is probably too frequent for most users too.\nTune it based on what is acceptable to you as a trade-off CPU usage vs. freshness of stats.\nI think we should keep this issue open, though, one second frequency of stats calc shouldn't really cause constant CPU load, imo.\n. Hmm. Ok. Well, let's leave this open and hopefully we can look at it before 2.0RC.\nThanks.\n. Interesting. I'll look into it\u2026though if you figure it out and send a patch/PR, I'll buy you a beverage of your choice.\n. nope\n. @oleksiyk can you let me know what OS you are using, please?\n. @oleksiyk I'm not seeing this on OS X. I'm running the latest beta. Would you mind trying the current beta, and reporting back, please?\n. @oleksiyk so I can close this issue?\n. Thank you\n. What version of Riak, please?\n. I ask because I'm pretty sure there is validation against negative bucket props in riak from 1.4 onwards (though I need to double check now.)\n. Dammit, I checked, 2.0 has full bucket property validation, 1.4 only validates booleans :(\n. Sean is of course correct, and 2.0 does indeed disallow negative r,w,n values. Going to close this out.\n. Actually I was too hasty. Please see https://github.com/basho/riak_kv/issues/927 which I've opened because of this report. The bug here is that custom bucket properties (in your case r_val, w_val) are not settable via HTTP. I have to ask though: how did you initially create them?\n. Hi Prepor,\nPlease use our excellent mailing list for questions like this, issues are for issues.\nhttp://lists.basho.com/mailman/listinfo/riak-users_lists.basho.com Is this best place to discuss this sort of thing. I'm happy to elaborate there. Short answer is no to 2i (and it is more than an API thing, or we'd have done it) and yes to riak-search 2.0, it was just an extractor.\nI'm going to close this issue now. Thanks.\n. The stat_ts stats are timestamps the say when the cached stat was calculated.\u00a0\n-------- Original message --------\nFrom: John Burwell notifications@github.com \nDate:12/08/2014  17:08  (GMT+00:00) \nTo: basho/riak riak@noreply.github.com \nCc: Russell Brown russell.brown@mac.com \nSubject: Re: [riak] use exometer metrics (#576) \nPerformed functional testing with the Bitcask, LevelDB, and memory backends to verify the completeness of the metrics reports.\nFor all three (3) backends, the following stats were reported by Folsom in Riak 2.0.0RC1 but not by Exometer branch:\nconsistent_get_objsize_100\nconsistent_get_objsize_95\nconsistent_get_objsize_99\nconsistent_get_objsize_mean\nconsistent_get_objsize_median\nconsistent_get_time_100\nconsistent_get_time_95\nconsistent_get_time_99\nconsistent_get_time_mean\nconsistent_get_time_median\nconsistent_gets\nconsistent_gets_total\nconsistent_put_objsize_100\nconsistent_put_objsize_95\nconsistent_put_objsize_99\nconsistent_put_objsize_mean\nconsistent_put_objsize_median\nconsistent_put_time_100\nconsistent_put_time_95\nconsistent_put_time_99\nconsistent_put_time_mean\nconsistent_put_time_median\nconsistent_puts\nconsistent_puts_total\nleveldb_read_block_error\nread_repairs_primary_notfound_count\nread_repairs_primary_notfound_one\nread_repairs_primary_outofdate_count\nread_repairs_primary_outofdate_one\nriak_core_stat_ts\nriak_kv_stat_ts\nriak_pipe_stat_ts\n/cc @uwiger\n\u2014\nReply to this email directly or view it on GitHub.\n. Disagree -1\n. If the LWW Reg was a key in Riak, does \"time go backwards\" if you write K={a, 2}, delete K, K={b, 0} ?\n. ok, thanks.\n. I started looking into this a bit, and found that using these two scripts https://gist.github.com/russelldb/90db1a31a4aaeb30d6cb there is a non-deterministic result from at least 1.4.8 up (will try earlier versions too)\nI think I'd rather look at fixing that first, and see if it isn't the cause of the above too.\n. Yeah, for the record, 1.4.2 does not show the issue from my scripts, but 1.4.4 does. I can't find a 1.4.3 release. So I'll start looking at commits next.\n. I think we have it. Looks like an off by one somewhere between the fold and the FSM. I have a hack fix that passes my script test. Should have updated the issue last night, sorry.\nOn 6 Nov 2014 01:28, Shunichi Shinohara notifications@github.com wrote:I'm digging the same symptom (but originated from riak_cs) with riak 1.4 branch.\nNot yet find the root cause, just a interim memo.\nWith single node cluster with ring size 16.\nAdded the following print debug to riak_core_coverage_fsm:\ndiff --git a/src/riak_core_coverage_fsm.erl b/src/riak_core_coverage_fsm.erl\nindex cd648bb..24dea91 100644\n--- a/src/riak_core_coverage_fsm.erl\n+++ b/src/riak_core_coverage_fsm.erl\n@@ -243,6 +243,12 @@ waiting_results({{ReqId, VNode}, Results},\n                                  req_id=ReqId,\n                                  timeout=Timeout,\n                                  process_fun = ProcessFun}) ->\n+case VNode of\n-    {1004782375664995756265033322492444576013453623296, 'dev1@127.0.0.1'} ->\n-        lager:log(warning, self(), \"**** waiting_results: ~p~n\", [Results]),\n-        ok;\n-    _ -> ok\n  +end,\n   case ProcessFun(VNode, Results, ModState) of\n       {ok, UpdModState} ->\n           UpdStateData = StateData#state{mod_state=UpdModState},\nThen, not always, the log reports duplicated key:\n10:19:07.565 [warning] ** waiting_results: {{<0.857.0>,#Ref<0.0.0.28237>},<<\"riak-cs-gc\">>,[<<\"1415092979_102\">>,<<\"1415092978_118\">>]}\n10:19:07.566 [warning] ** waiting_results: {{<0.857.0>,#Ref<0.0.0.28238>},<<\"riak-cs-gc\">>,[<<\"1415092978_118\">>]}\nThis seems to break the assumption by riak_kv_index_fsm that keys should be orderd.\nSo riak_kv_index_fsm and sms does not work correctly.\n\u2014Reply to this email directly or view it on GitHub.\n. is it only batch remove of maps in maps? Have you tried batch remove of keys from the top level map? Simplest case first helps.\nAre there any errors in the riak logs?\n. First checking riak_dt_map shows that the datastructure works as expected\n```\n\nriak_dt_map:value(M).\n[{{a,riak_dt_map},\n  [{{i2,riak_dt_emcntr},1},{{i1,riak_dt_emcntr},1}]}]\n(riak@127.0.0.1)8> {ok, M2} = riak_dt_map:update({update, [{update, {a, riak_dt_map}, \n                                            {update, [{remove, {i1, riak_dt_emcntr}}, \n                                                           {remove, {i2, riak_dt_emcntr}}]}}]}, bob, M).\n\nok,{[{bob,2}],\n [{{a,riak_dt_map},\n   {[{{bob,2},{[{bob,2}],[],[]}}],{[],[],[]}}}],\n []}}\n(riak@127.0.0.1)9> riak_dt_map:value(M2).\n[{{a,riak_dt_map},[]}]\n```\nAnd a quick trace shows that running your example only one {remove, Field()} tuple is in the operation list.\n. Yup. Tracing the parser:\n(<0.19066.0>) call riak_kv_crdt_json:update_request_from_json(map,{struct,\n[{<<\"update\">>,\n  {struct,\n      [{<<\"creat1_map\">>,\n        {struct,\n            [{<<\"update\">>,\n              {struct,\n                  [{<<\"country1_map\">>,\n                    {struct,\n                        [{<<\"remove\">>,<<\"impressions_counter\">>},\n                         {<<\"remove\">>,<<\"clicks_counter\">>},\n                         {<<\"remove\">>,\n                          <<\"downloads_counter\">>}]}}]}}]}}]}}]},[{map,riak_dt_map},\n {set,riak_dt_orswot},\n {counter,riak_dt_emcntr},\n {flag,riak_dt_od_flag},\n{register,riak_dt_lwwreg}])\n(<0.19066.0>) returned from riak_kv_crdt_json:update_request_from_json/3 -> {map,\n                                                                         {update,\n                                                                          [{update,\n                                                                            {<<\"creat1\">>,\n                                                                             riak_dt_map},\n                                                                            {update,\n                                                                             [{update,\n                                                                               {<<\"country1\">>,\n                                                                                riak_dt_map},\n                                                                               {update,\n                                                                                [{remove,\n                                                                                  {<<\"impressions\">>,\n                                                                                   riak_dt_emcntr}}]}}]}}]},\n                                                                         undefined}\nThree removes enter, one remove leaves. It's like the thunderdrome all over again.\n. Luckily, it also fails in the simple case of a top-level map with multiple fields.\n. Are you setting the timeout high enough on the request?\n. Fixed in riak-2.0.5\n. Hi, do you have pam-dev dependancy installed, as the previous issue stated?\n. This is not an issue, riak builds. I think that the riak-users mailing list is the best source of help for you. Closing this now.\nPlease post to the mailing list in future.  http://lists.basho.com/mailman/listinfo/riak-users_lists.basho.com\n. Hi Mark,\nYou didn't respond to my question in IRC: can you get the bucket properties for the bucket(s) in question? I suspect that is where the binary is coming from.\nRussell\n. I ask because * {badarg,[{lists,split,[<<\"4\">>, suggests that you've updated the default somewhere, which may well be the cause of the issue. Is it in the config file, or some client call that updates bucket props?\n. But the crash shows a bucket of nval <<\"4\">>. How do you know you have 3 buckets configured? Is this in a non-prod env where running list-buckets is feasible?\n. Hmmm, OK. I will dig into it some more, but if the binary isn't the n_val of a bucket, what the hell is it?\n. Curious that it is also a binary of an integer that looks very n_val-esque\n. Testing that now @kesslerm, thanks!\n. Can confirm that adding\n{default_bucket_props, [{allow_mult, false}]},\nto app.config is a workaround and does not impact typed buckets defaults.\n. This is \"expected\" behaviour. Please see https://github.com/basho/cuttlefish/wiki/Cuttlefish-for-Application-Users specifically Cuttlefish will see your app.config sitting where it's supposed to, and use that instead.\nLooping in @seemaj for opinions. I think the right thing is add the workaround to the procedure for upgrading. The best thing would be a script that turned the legacy app.config into a valid riak.conf.\n. nope. HI,\nRiak does not yet support OTP 17.5. Please use the basho erlang 16. The error you are seeing is to do with vnode-status files and the default encoding change in OTP 17.\nThis patch https://github.com/basho/riak_kv/pull/1115 might get you running, but is not the proper solution for the problem you are seeing.\nWe do have a branch of core (https://github.com/basho/riak_core/pull/733) that is awaiting review and merge, and we have an otp 17 development branch of riak. At some point soon riak will support more recent erlang versions. Until then, if you're not confident running your own fork, it is better to stick to the supported versions.\nRegards\nRussell\n. Nope, and that is probably you\ufffdre problem, leveldb failing to get a lock, and backing off, and trying again.\nOn 10 Nov 2015, at 11:28, Zerebokep notifications@github.com wrote:\n\nAnother thing, in /var/lib/riak/leveldb there are some files/folders owned by root instead of riak, is this normal?\nThe files are:\n-rw-r--r-- 1 root root 0 Nov 10 10:09 000002.log\n-rw-r--r-- 1 root root 16 Nov 10 10:09 CURRENT\n-rw-r--r-- 1 root root 0 Nov 10 10:09 LOCK\n-rw-r--r-- 1 root root 2.4K Nov 10 10:09 LOG\n-rw-r--r-- 1 root root 148 Nov 10 10:09 LOG.old\n-rw-r--r-- 1 root root 50 Nov 10 10:09 MANIFEST-000001\ndrwxr-xr-x 2 root root 4.0K Nov 10 10:09 sst_0\ndrwxr-xr-x 2 root root 4.0K Nov 10 10:09 sst_1\ndrwxr-xr-x 2 root root 4.0K Nov 10 10:09 sst_2\ndrwxr-xr-x 2 root root 4.0K Nov 10 10:09 sst_3\ndrwxr-xr-x 2 root root 4.0K Nov 10 10:09 sst_4\ndrwxr-xr-x 2 root root 4.0K Nov 10 10:09 sst_5\ndrwxr-xr-x 2 root root 4.0K Nov 10 10:09 sst_6\n\ufffd\nReply to this email directly or view it on GitHub.\n. Tiot.jp are hosting mirrors of downloads and docs. The debs you look for are here https://files.tiot.jp/riak/kv/2.2/2.2.3/ubuntu/. The docs are here https://www.tiot.jp/riak-docs/. You still need basho's OTP16 for riak. OTP20+ support is coming in 2019. Yup, that's the issue alright. Riak only supports erlang r16b0something. . yes, use erlang-r16. Ideally the basho patched erlang-r16b04-basho10. Does this mean we're dropping SNMP and JMX?. Also, why not just take open the riak_ee repo and make that the default Riak?. There's some missing vars from vars.config and dev_vars.config for the cluster manager port and IP. That aside it looks ok, but I haven't taken the time to build and test it yet.\n\nNeed to make a decision about jmx and snmp I guess. My feeling is that the next release should contain them, but deprecated, and then they can be removed after that.. Saying that, I added repl, jmx, and snmp to a OS Riak in the NHS fork and now Riak needs Java/Maven. Maybe you're right to add only repl.. +1 to that @bryanhuntesl. And then at leisure migrate fsms to statem. I think that way is safer for sure.. Sorry, just saw this now. Do you know if this fixes https://github.com/basho/riak/issues/940 ?\nIf so, can you re-point the PR at develop-2.2?. +1, that's a lovely idea. Thank you!. Looks good to me, thanks @bryanhuntesl . Appreciated, thanks @BillSmargiassi !. Added the spaces, 'cos yeah, weird.. Pushed to address all current comments on this issue\u2026. Sorry, thanks for the review @bryanhuntesl, but @martinsumner has gone re-format crazy so I have some major changes in coming. I've added you review/content changes too.. orelse PB == {ok, []} works for me.\n. indices or indexes? I prefer the former, but a long time ago I think Basho made a choice to stick with the latter.\n. Well, they at basho rebranded it RiakEE, RiakKV, RIakCS, but I'll change it back, gladly. Hahaha. Is this what we're sticking with WRT naming? It always confused me, as riak_kv is an actual repo/project. I prefer my \"\u2026and have done\u2026\" over \". They did a\". I also prefer my \"less positive\" opening. I don't get how this is better.. My opinion: that's Wray/McCrory era stuff that is associated with the decline of Riak, and I contend that we want to be associated with the Sheehy era.. ",
    "beerriot": "+1 merge\n. +1: fixed the 'unable to open nodetool' error I was seeing\n. You got it.  I'll do them both either last thing on Wednesday or first thing on Tuesday.\n. Good point, @dizzyd.  I've added a link in README.org.  I also propose removing README, since GitHub now renders .org, and README is lagging anyway.\n. +1 works for me, thanks\n. Each command should print a message about what the user should do next.  For example, cluster-join should print something to the effect of, \"Run riak-admin cluster plan to view staged changes, or riak-admin cluster clear to unstage.\"  \nWithout a hint like this, it's a bit confusing to see the same output as with the old riak-admin join <node>, yet not see the same behavior.  \"Okay, now what?\"\n. It looks like the zero-argument cluster-leave function is broken:\n$ dev/dev2/bin/riak-admin cluster leave\nFailed: '' is not a member of the cluster.\n. Is there any way to detect and explain more about why a commit may have failed?  The generic \"Unable to commit cluster changes. Plan may have changed, please verify the plan and try to commit again\" left me guessing.  The particular problem I was having was that not all of my member nodes were up, so I needed to mark the down ones using riak-admin down <node> before continuing.\n. I think I've found a corner case in which cluster plan lies.\nStart three fresh nodes:\nshell\n$ for i in {1..3}; do dev/dev$i/bin/riak start; done\nTell dev2 to join dev1, and plan the migration.\n``` shell\n$ dev/dev2/bin/riak-admin cluster join dev1@127.0.0.1\nSent join request to dev1@127.0.0.1\n$ dev/dev1/bin/riak-admin cluster plan\n=============================== Staged Changes ================================\nAction         Nodes(s)\n\njoin           'dev2@127.0.0.1'\nNOTE: Applying these changes will result in 1 cluster transition\n\n                     After cluster transition 1/1\n\n\n================================= Membership ==================================\nStatus     Ring    Pending    Node\n\nvalid     100.0%     50.0%    'dev1@127.0.0.1'\nvalid       0.0%     50.0%    'dev2@127.0.0.1'\n\nValid:2 / Leaving:0 / Exiting:0 / Joining:0 / Down:0\nWARNING: Not all replicas will be on distinct nodes\nTransfers resulting from cluster changes: 32\n  32 transfers from 'dev1@127.0.0.1' to 'dev2@127.0.0.1'\n```\nNow kill dev2 (who knows why, it could happen), and then commit the migration plan:\nshell\n$ dev/dev2/bin/riak stop\nok\n$ dev/dev1/bin/riak-admin cluster commit\nCluster changes committed\nEverything will just sit in Pending as long as we leave dev2 down.\n``` shell\n$ dev/dev1/bin/riak-admin member_status\n================================= Membership ==================================\nStatus     Ring    Pending    Node\n\nvalid     100.0%     50.0%    'dev1@127.0.0.1'\nvalid       0.0%     50.0%    'dev2@127.0.0.1'\n\nValid:2 / Leaving:0 / Exiting:0 / Joining:0 / Down:0\n```\nBut let's add dev3 anyway\u2026\n``` shell\n$ dev/dev3/bin/riak-admin cluster join dev1@127.0.0.1\nSent join request to dev1@127.0.0.1\n$ dev/dev1/bin/riak-admin cluster plan\n=============================== Staged Changes ================================\nAction         Nodes(s)\n\njoin           'dev3@127.0.0.1'\nNOTE: Applying these changes will result in 2 cluster transitions\n\n                     After cluster transition 1/2\n\n\n================================= Membership ==================================\nStatus     Ring    Pending    Node\n\nvalid     100.0%     50.0%    'dev1@127.0.0.1'\nvalid       0.0%     50.0%    'dev2@127.0.0.1'\nvalid       0.0%      0.0%    'dev3@127.0.0.1'\n\nValid:3 / Leaving:0 / Exiting:0 / Joining:0 / Down:0\nWARNING: Not all replicas will be on distinct nodes\nTransfers resulting from cluster changes: 32\n  32 transfers from 'dev1@127.0.0.1' to 'dev2@127.0.0.1'\n\n                     After cluster transition 2/2\n\n\n================================= Membership ==================================\nStatus     Ring    Pending    Node\n\nvalid      50.0%     34.4%    'dev1@127.0.0.1'\nvalid      50.0%     32.8%    'dev2@127.0.0.1'\nvalid       0.0%     32.8%    'dev3@127.0.0.1'\n\nValid:3 / Leaving:0 / Exiting:0 / Joining:0 / Down:0\nWARNING: Not all replicas will be on distinct nodes\nTransfers resulting from cluster changes: 21\n  10 transfers from 'dev1@127.0.0.1' to 'dev3@127.0.0.1'\n  11 transfers from 'dev2@127.0.0.1' to 'dev3@127.0.0.1'\n$ dev/dev1/bin/riak-admin cluster commit\nUnable to commit cluster changes. Plan may have changed, please verify the plan and try to commit again\n```\nSo we see that the earlier migration is still pending, and now we have a new plan staged, but we can't commit it because dev2 is still down.  But, if we mark dev2 as down, we can commit the migration\u2026\nshell\n$ dev/dev1/bin/riak-admin down dev2@127.0.0.1\nSuccess: \"dev2@127.0.0.1\" marked as down\n\u2026but let's check the plan again first:\n``` shell\n$ dev/dev1/bin/riak-admin cluster plan\n=============================== Staged Changes ================================\nAction         Nodes(s)\n\njoin           'dev3@127.0.0.1'\nNOTE: Applying these changes will result in 0 cluster transitions\n```\nI think this plan is a lie, because I can now commit it, and transfers will, indeed, happen:\n``` shell\n$ dev/dev1/bin/riak-admin cluster commit\nCluster changes committed\n$ dev/dev1/bin/riak-admin member_status\n================================= Membership ==================================\nStatus     Ring    Pending    Node\n\ndown        0.0%      --      'dev2@127.0.0.1'\nvalid      67.2%      --      'dev1@127.0.0.1'\nvalid      32.8%      --      'dev3@127.0.0.1'\n\nValid:2 / Leaving:0 / Exiting:0 / Joining:0 / Down:1\n```\nOr, at least member_status thinks they happened.  This could be the lie instead.\nIf we bring dev2 back up, the first transfer completes as well.\n``` shell\n$ dev/dev2/bin/riak start\n$ dev/dev1/bin/riak-admin member_status\n================================= Membership ==================================\nStatus     Ring    Pending    Node\n\nvalid      34.4%      --      'dev1@127.0.0.1'\nvalid      32.8%      --      'dev2@127.0.0.1'\nvalid      32.8%      --      'dev3@127.0.0.1'\n\nValid:3 / Leaving:0 / Exiting:0 / Joining:0 / Down:0\n```\nSo, I don't know that there's anything broken, but there seems to be something confused, which I thought I'd point out, in case it's indicative of some other issue.\n. This is a duplicate of this riak_kv issue: https://github.com/basho/riak_kv/issues/358\nClosing this one to track progress over there.\n. +1 this made \"make test\" run tests again for me, instead of saying there were no tests to run\n. The choice of whether or not to use backpressure in riak_kv#399 is controlled by core capability negotiation, not by application environment. It will enable itself automatically as soon as every node in the cluster supports it. I see no reason to provide the option to disable it, so I think the change in this PR is unecessary.\n. it works! +1, thanks\n. Indeed, you've found two different problems: basho/riak_search#134 and basho/riak_kv#443. Sorry about that. This is partially left over from the days before Search was included in the regular Riak distribution, when it was impossible to disable it.\nI assure you that there are many people worked up about improving our error-reporting.\n. Hi, @roncemer. I apologize for the delay in responding here. I attempted to write a more general response about JS improvement in Riak, but it got a bit unwieldy. So, instead I'm going to respond point by point for now, and possibly return with that broader-scope response later.\n\nJavaScript is a ubiquitous language. Nearly everyone knows it. Erlang is an obscure language with strange constructs and syntax. Almost no one uses it or cares to.\n\nThis is true only in the relative situation here. In the wider world, C, Java, Objective-C, C++, C#, PHP, Basic, and Python are all more widely known and used than Javascript. Erlang, it's true, is even less popular, but \"obscure\" and \"strange\" are still quite relative. Any functional programmer will recognize lists and tuples. We agree, though, that it would be very nice for a user to not need to know Erlang to exploit Riak's distributive power to analyze the data it stores.\n\nAny K/V store which is to be taken seriously in the market MUST have proper support for a popular language when it comes to their M/R implementation.\n\nI think there may be a false assumption embedded here. I don't think any KV store actually needs a MapReduce implementation to be taken seriously. I think KV stores mostly trade on their correctness, management, and performance (not necessarily in that order).\nFurthermore, I think most users would actually prefer nearly any other wide-query system over MapReduce. As proof, look at all of the projects that try to hide Hadoop's MR behind languages that are very much not focused on those particular processing steps.\nUltimately, when a datastore does choose to expose a MapReduce interface, I think you're right that one of the concerns is what language it consumes. MapReduce is only simple to understand at a very high level; the nitty gritty about how data is processed efficiently, and what happens when bugs are encountered, is anything but trivial. A familiar language can make dealing with the rest of these hard problems less daunting, though care should be taken not to force complex concepts into an environment that is not friendly to them.\n\nThe hard-coded 5-second timeout on JavaScript script execution in erlang_js needs to go away, or at least be made configurable.\n\nYes, it should be configurable. If you wanted to get involved with the project, I think this would make a great first issue to sink your teeth into. That timeout is there to keep Riak from starving for VMs when some too-long-running (possibly malicious) function is executed on all of them.\nOne tip: if your Javascript function is taking longer than five seconds to execute, there might be something else wrong. One typical problem is in JS reduce phases - the re-encoding/re-decoding produces more and more garbage, and takes longer and longer, as the function is run over and over when new inputs arrive. Fixing this can be as easy as increasing the batch size, or even setting the phase to wait for all inputs before running at all.\nAnother tip: once you have made that timeout configurable, you'll likely also want to make the backoff/timeout in the VM pool's reservation code riak_kv_js_manager configurable as well, or you'll end up with the same sort of hard-coded timeout problem when you're trying to run more MR queries than you have JS VMs.\n\nIn every case where there is event the remotest possibility of a bad_utf8_character_code error occurring, that code needs to be fixed. The interface between erlang and JavaScript needs to be made binary-safe and bullet-proof.\n\nI think there may be two parts to this issue. The first are bugs that we have missed. Riak does attempt to catch some native Erlang types that would cause encoding errors (see riak_kv_object_json). Finding holes in that module would also be another great way to get involved with the project.\nI fear that it may not be possible to solve the other part of the issue without significant restructuring. Data is passed between Riak and the JS VM as JSON data. Riak supports any binary blob as a key's value, but JSON has no \"binary\" type. So, it comes down to the choice between strings (which must be some valid Unicode encoding, so we're talking \\u encoding, base64, or the like), and an array of integers (one per byte).\nThe restructuring would entail using something other than JSON for communication between Riak and the JS VM. We have occasionally discussed this option as a way to improve the performance as well, because JSON parsing is often a significant amount of the execution time. Something like BERT or Protocol Buffers might be interesting.\n\nThis idea of writing complex map and reduce functions in an obscure language like Erlang, then being forced to compile them and load them on every node in the cluster...that's for chumps. We need to get the JavaScript M/R working properly, with first-rate support.\n\nFor anyone doing serious MR, the same problem exists for Javascript. The best way to use it is to write the code in a file and deploy that file on the cluster. Then, it's possible to just name the functions in phase definitions instead of sending the entire source with every request. This is not only more efficient from the client to the Riak node, but also between the Riak node and the Javascript VM.\nDeveloping a system to install code across the cluster would be a fun project for anyone who wanted to learn more about riak_core, and the new cluster metadata facilities coming in Riak 2.0. Spreading information via the plumtree/gossip protocols embedded there could improve many aspects of deploying either Javascript or Erlang code for MapReduce.\n\nThe documentation gives plenty of examples of JavaScript M/R implementations, but doesn't explain that JavaScript M/R really does blow chunks in Riak.\nLet's light a fire under this, and get it fixed.\n\nImprovements depend on individuals making them happen. Javascript MapReduce is currently not the primary focus of any Basho developer's time. We do continuously discuss its outstanding issues and possible futures, but as yet, we have found other improvements more pressing and more promising. If you are passionate about improving JS MR, though, please submit bug reports and pull requests, and try to find other people in the community to help you. We commiters are much more responsive to small bits of obvious work than we are to vague calls for improvement.\n\nAlso, why does Riak use the Spidermonkey JS engine when the V8 engine is newer and faster? Or is that old news? Has Spidermonkey really surpassed V8?\n\nBriefly: history.\nHistory 1: legacy support. For a long time we needed to be able to deploy Riak on Solaris running on SPARC. We also insisted that Riak support the exact same feature set on all platforms. The version of Spidermonkey we use was the only one that could give us this at that time.\nHistory 2: old V8 architecture. When V8 was first available, its garbage collection design was not compatible with the way Riak wanted to use VMs. I believe this is no longer the case.\nUnfortunately, we were trapped on our version of Spidermonkey for so long that we finally just stopped looking. A few people have since offered to update the version that we use, at least, but as yet none has come through. Again, it comes down to individuals being the only force for improvement, and this improvement hasn't climbed to the top of anyone's focus yet.\nThe only warning I'll add is that Javascript is made available in Riak in more contexts than MapReduce (e.g. commit hooks). Watch out for those gotchas if interfaces need to change.\n\nBut it could be much better if these issues were resolve instead of kicked down the road\n\nIt is hard to define \"solved\" in a changing, scaling system. What is solved for today's use case is not necessarily solved for tomorrow's. Believe it or not, even this Javascript MapReduce system was good enough for some problems at one time. It needs improvement for the future, but iterative improvement is the name of the game, or we'll all be out of jobs shortly.\n\nSo, that's the brief version. There's a ripe field of issues to tackle. Hopefully I've given enough leads for you and others reading along to being to tear in. Thanks for the feedback.\n. ",
    "Vagabond": "+1 to merge\n. +1 to merge\n. I hosed up this pull request, reopening\n. Trying to remove all these merge commits, will open new pull request.\n. I did this last week too, except I needed 6 nodes. Anything over N+1 is definitely handy.\n. Gah, commented in the wrong place. This looks fine. +1 for merge.\n. https://github.com/basho/riak/pull/41\n. +1 on this.\n. This was merged but not closed on GH.\n. +1 to merge. Make sure you also make this change for EE.\n. +1 to merge\n. The bash dependency is pointless and should/will be removed, I just haven't had the time to reverify that the change doesn't break anything.\n. Gah, wrong branch.\n. +1 to merge\n. It'd be really nice if you could do something like stty size | awk '{ print $1 - 9}' to auto-size the etop output to the terminal (9 is the number of lines of overhead that are printed, plus the final line).\n. You have my +1, sir.\n. +1\n. As the links you include say, this is caused by 2 nodes trying to use the same name (and the second one failing). empd -names will show you all the locally registered node names.\nIs it possible you're inadvertantly trying to start riak twice?\n. I just ran into this. It needs fixing.\n. I found out why this happens, RUNNER_LIB_DIR is wrong when we build a package (for local builds it is ./lib, which is correct).\nProof:\nriak      2881 45.8 31.9 20680564 10537320 pts/2 Ssl+ Aug07 2137:20 /usr/lib/riak/erts-5.9.1/bin/beam.smp\n -K true -A 64 -W w -- -root /usr/lib/riak -progname riak -- -home /var/lib/riak -- -boot \n/usr/lib/riak/releases/1.2.0/riak -embedded -config /etc/riak/app.config -name riak@10.0.27.22 -setcookie\nriak4_2itesting -pa /usr/lib/riak/basho-patches -- console\nSo you can see that -pa /usr/lib/riak/basho-patches is there\nbut:\n```\nathompson@r1s12:/usr/lib/riak/lib/basho-patches$ ls /usr/lib/riak/basho-patches/\nls: cannot access /usr/lib/riak/basho-patches/: No such file or directory\n```\nAnd in the riak script:\nRUNNER_LIB_DIR={{platform_lib_dir}}\n...\n -pa $RUNNER_LIB_DIR/basho-patches \\\nThe path should be /usr/lib/riak/lib/basho-patches/\nRUNNER_LIB_DIR is used nowhere else in the file, so it should be safe to fix.\nThis happened with the ubuntu package, have not tested the others, but it also looks broken for the other platforms.\n. I ran into the same things as Bryan, so there's certainly something here that needs to be addressed.\n. Oh, and making the error messages for when a commit fails useful, like we talked about, would be great.\n. I'm +1 on this part of the code now.\n. This looks fine, but I'll defer to @jaredmorrow on merging it,\n. +1, as this option is having no effect.\nI still wonder if we shouldn't try to fix embedded mode, but we can fight about that later.\n. Looks legit at first glance.\n. lager 2.0 changed the syntax, but it still supports the old form. We should probably migrate to the new one, though.\n. @bkerley  Yes, CRL support is something I forgot to cover, will update the text.\n@aphyr Yes, I plan to support both 1 and 2 (but not 3). If you want to use the certificate authentication mode (with no additional password), we require you to handshake via method 1. This is already sort of implemented for the PBC protocol and the riak_test shows its use:\nhttps://github.com/basho/riak_test/blob/adt-security/tests/pb_security.erl#L94\nhttps://github.com/basho/riak_test/blob/adt-security/tests/pb_security.erl#L133\n. @aggress MDC doesn't deal with client authentication at all, it already has support for TLS based authentication, where both sides verify the other's certificate.\n@cdahlqvist Yes, EVERY operation (or almost all of them) will need a permission associated, I just don't know what some of them will look like yet. The only exception to the rule may be the stats and ping endpoints, you may be able to hit them simply by being authenticated, not sure yet.\n. @aggress Yes, that is a good question, will add it to the open questions section. My initial feeling is to NOT replicate that information, but we'll see.\n. So, after a bunch of reading and internal discussion, I think we're going to stick with ACLs, for the following reasons:\n- Users are more familiar with them, other databases are primarily ACL based (when they provide security at all).\n- Clients for Riak don't need to pass credentials around.\n- Credentials are harder to audit, unless you tie them to a per-user certificate you can revoke or something.\nHowever, I think I will add postgres style roles, as a way to implement 'groups'.\n. I am watching that talk, but I'm struggling to extract much of relevance from it. It is sort of like a tech-related street performance that occasionally touches on ACLs en-route to bagels, smoked meat, corporate greed, the incompetence of MBAs, etc.\nThe three key points he seems to make are:\n- ACLs are not turing complete\n- Does not have repetition\n- Does not have externally accessible storage\nMaybe I'm dense, but I don't understand why those are even a problem. I understand his point about ridiculous business rule requirements about time and situation dependent ACLs, but Riak does not really have that problem.\nRight now my takeaway is this: ACLs for people != ACLs for applications. Applications rarely need time-dependent or situation dependent access to data, they have their data and they want to access it whenever they need to, and these access rules change rarely. Riak is not a document management solution, it is a database. It is used by applications, not people.\nI'm happy to have a discussion about this, but providing references to things like the 'authz problem' that is not a 1:10 stream of consciousness rant about all sorts of unrelated things would help your case a lot more. It is fairly telling that none of the questions at the end were even about ACLs at all, beyond one question about making what Zed did into a product.\n. Maybe we can narrow the conversation here. When would the confused deputy problem occur for Riak, along the lines of the compiler example here:\nhttp://waterken.sourceforge.net/aclsdont/current.pdf\n. I guess my biggest sources of mystification are the following:\n- What part of the above proposal has the capacity to act as a confused deputy\n- What does issuing a capability to a user look like, and how would it work\n. Right, I see that is nice to separate authentication from capabilities from an accounting standpoint and arguably from a flexibility one. However, the overhead seems extremely expensive; a public/private key pair per bucket or key, for something like Riak, would be massive overhead.\nThere's also the problem that buckets in Riak are 'created' in response to a key being written into them, if they don't have any custom properties, they only exist by virtue of their contents. If you did something like create a cryptographic token the first time you see a bucket without one, you'd still be vulnerable to race conditions, because the cluster may be partitioned, or 2 users may simply be 'creating' that bucket simultaneously. Bucket types may help with that somewhat, but they won't provide everything we need at least in 2.0.\nAnother concern I have, Riak usually has N of the same thing working with its data, you have N clients for 'Application 1' and then another N for 'Application 2', it isn't like a local filesystem where my user has a bunch of shared resources it could use as a capability store, these are clients running on discrete machines that are using Riak as the 'shared state', adding more requirements so they can share capabilities outside of Riak seems onerous.\nAlso, I'm still concerned about the client management of these capabilities, what happens if the client's HDD crashes and it loses all the credentials? Does the server retain a copy of the private key? If so, how can you be sure you're giving it to the right users?\nBasicially, my problem is this: I remain unconvinced that \"ACLs are bad for user access\" == \"ACLs are bad for application access\" and I feel that the complexity we'd incur implementing a crypto capabilities system would not be worth it for the vast majority of Riak's users. Another factor is time, I have a fixed window to get something implemented for 2.0, and I just don't think, all my above concerns aside, that it is doable in that kind of timeframe.\n. Right, but you can say what buckets are accessible without caring if the bucket exists right now or not, since the bucket itself doesn't have to track anything.\nThe, somewhat modified plan from the above RFC, plan is to use 'bucket types' as part of the ACL, bucket types MUST be explicitly created, unlike buckets, and we can do things like grant a user to write to any under bucket under a particular bucket type, or again define a list of buckets that they can access. Effectively we don't consider bucket creation a thing, simply get/put/delete and read/write bucket properties are the only permissions that matter for a bucket (well, there's also mapreduce/2i and stuff, but that is not relevant).\nAnother advantage of this approach is you can reject a request cheaply compared to to the amount of work needed to check a capability, and you can do it early in the request without pulling anything off the disk. With a capability per-bucket or per-key, you'd need to actually read something from somewhere to compare the capability the user provided against.\n. See #434 and related PRs, not all of the security code has landed yet.\n. +1\n. The json on the command line makes me not too happy, seems very error prone.\n. I don't have a good alternative. I guess we can try JSON for now, the hard part is the issue of datatypes, so a simple key=value syntax won't work, without specifiying the datatype of each possible bucket prop. +1 as the best we can do for now.\n. I think you fucked up the rebase, bro.\n. I'm +1 on this.\n. I argued against bundling these SSL certs in the first place, so I'd be happy to see them go.\n. On Wed, Oct 02, 2013 at 10:27:44AM -0700, Jared Morrow wrote:\n\n+1 for them to be gone.  The original request from Jeff was to make proper docs or a platform-independent script to generate the certificates easily so users won't have any barrier to testing.\n\nI was actually playing with the one bundled with OTP, that OTP uses for\ntesting SSL to use in riak_test for testing revoked certificates. If\nthat works out maybe we can adapt it.\nAndrew\n. Also +1\n. Description of bug and patch for OTP available here:\nhttp://erlang.org/pipermail/erlang-bugs/2013-October/003782.html\n. It was fixed in R16B03 by OTP-11370.\n. +1\n. Yes. HTTP BASIC is not really ideal,. especially since the credentials are re-sent every request. However, one of the design constraints of Riak's security system was to allow pluggable authentication backends. Except in the case of the 'password' backend, Riak does not store any password data at all, it is passed through to the backend for validation. Since we don't control how the backend accepts passwords, and they don't all use the same hashing scheme (when they even support hashed passwords) is it not feasable to have the client send hashed passwords over the wire. Some future work is to investigate alternative approaches to this problem. Also, as of 2.0 we are moving all of the official Riak clients to only support the PB interface. HTTP is still supported, but anyone using an official Basho client will not be using it as the protocol for communicating with Riak.\nAs regards TLS, I completely agree. A saner cipher list and knobs for tuning the minium allowed TLS version are 2 of the things on my TODO list for 2.0. I did a writeup of the RICON talk I gave the other week on this, which you can read here:\nhttp://vagabond.github.io/2013/11/06/ricon-west-2013-talk-writeup/\nI'm not sure we can force TLS 1.2 by default, as a lot of languages have only just gained support this year for it (and TLS 1.0) but we can certainly provide the knobs for it so people with cabable clients can force it.\n. I've implemented the cipher work over in basho/riak_core#469 You can now set the ciphers you want to use, and even turn on 'honor cipher order' style behaviour, if you're using a Basho package (I've patched the erlang SSL application to support that option). The default cipher list is the one taken from here:\nhttps://wiki.mozilla.org/Security/Server_Side_TLS\n. options syntax changed.\n. See also basho/riak_core#469\n. If I do:\n{{#devrel}}\n{mapping, \"leveldb.limited_developer_mem\", \"eleveldb.limited_developer_mem\", [\n  {default, true},\n  {level, basic},\n  merge\n]}.\n{{/devrel}}\nI get\n{mapping, \"leveldb.limited_developer_mem\", \"eleveldb.limited_developer_mem\", [\n  {default, true},\n  {level, basic},\n  merge\n]}.\nWhich causes:\nERROR: generate failed while processing /home/andrew/riak/rel: {'EXIT',\n    {badarg,\n        [{io_lib,format,\n             [\"Default: ~s\",\n              [{error,\n                   \"Tried to convert true, an invalid datatype string to_string.\"}]],\n             [{file,\"io_lib.erl\"},{line,155}]},\n         {cuttlefish_conf,generate_comments,1,\n             [{file,\"src/cuttlefish_conf.erl\"},{line,125}]},\n         {cuttlefish_conf,generate_element,1,\n             [{file,\"src/cuttlefish_conf.erl\"},{line,108}]},\n         {cuttlefish_conf,'-generate/1-fun-0-',2,\n             [{file,\"src/cuttlefish_conf.erl\"},{line,71}]},\n         {lists,foldl,3,[{file,\"lists.erl\"},{line,1248}]},\n         {cuttlefish_conf,generate_file,2,\n. We can certainly add a permission to allow you to list bucket types, along the lines of the permission that allows bucket listing.\n. Make this a basic option, and then +1 from me.\n. I don't think sources should be inherited, authentication is seperate from authorization.\n. The stacktrace should be fixed, however.\n. I guess my concern is that it becomes a lot less clear what source a user is using to log in if we allow sources to be inherited. It also opens up the issue of figuring out which source should be used, in the case where multiple ones are found that match the user.\n. By the way, what version of Riak threw that stacktrace, was it something recent?\n. Fix for the stack trace is linked to this issue.\n. Good point. I would be OK with implementing the + syntax. My main concern was it being implicit.\n. But it will not make 2.0.\n. +1\n. +1\n. Oh, we should probably do that (or just copy the damn riak-admin script over).\n. Their leveldb != our leveldb. Unlke the other leveldb forks out there (hyperdex's hyperleveldb and Facebook's RocksDB) we have not renamed our fork, although we probably should.\n. I agree.\n. +1\n. +1\n. +1\n. We should instead raise the limit after startup if we care about all the startup noise. The limit is there for a very good reason:\nhttp://hijacked.us/~andrew/lager.png\nhttp://hijacked.us/~andrew/error_logger.png\nIf the limit is too high, the node gets overwhelmed before the dropping can kick in.\n. Err, lower the limit, I mean.\n. The problem is that errror_logger is seriously flawed in its implementation. It provides 0 backpressure so that its mailbox can get loaded up with millions of messages.\nOriginally we had some code to drop duplicate messages, but it turned out to be slow enough that it didn't really help when an error_logger storm kicked in. The tradeoff is lose some error_logger messages or crash the node. In what cases are you seeing that many error logger messages?\n. 'Safe' is going to be very system dependant, unfortunately. I tried to pick a reasonable limit that should be safe on all but the slowest machines.\n. Remember, this only affects messages coming from error_logger. Bitcask uses error_logger because we didn't want to tie it to lager with a dependancy.\nWe don't use phased startup, so when is riak 'up'? There's API for changing the error_logger high water mark:\nhttps://github.com/basho/lager/blob/master/src/error_logger_lager_h.erl#L73\nNow, what evan saw where it only reports the dropped message, is weird. I've never seen it do that. It should only drop the log messages that exceed the threshhold for that second, which usually gives you a decent sampling what is storming the error logger without actually killing it. This might be a bug, so I'd be interested if it was possible to reproduce.\nA year ago I did a lot of testing to find a reasonable default for this, I won't stop you guys changing the defaults, but when error_logger starts OOMing nodes again (and the logs get lost anyway), don't be surprised.\n. There is not always a lock-deps file, 1.4 branch does not have one:\nhttps://github.com/basho/riak/tree/1.4\n. Or maybe have 'make snapshot'\n. So, for 2.0.0:\n$ rebar list-deps | grep BRANCH\nrebar_lock_deps_plugin BRANCH master git://github.com/seth/rebar_lock_deps_plugin.git\nFor 2.0.1:\n$ rebar list-deps | grep BRANCH\ncuttlefish BRANCH 2.0 git://github.com/basho/cuttlefish.git\ncuttlefish BRANCH 2.0 git://github.com/basho/cuttlefish.git\ncuttlefish BRANCH 2.0 git://github.com/basho/cuttlefish.git\neleveldb BRANCH 2.0 git://github.com/basho/eleveldb.git\ncuttlefish BRANCH 2.0 git://github.com/basho/cuttlefish.git\nriak_sysmon BRANCH 2.0 git://github.com/basho/riak_sysmon.git\nriak_ensemble BRANCH 2.0 git://github.com/basho/riak_ensemble\neleveldb BRANCH 2.0 git://github.com/basho/eleveldb.git\nriak_core BRANCH 2.0 git://github.com/basho/riak_core.git\nriak_core BRANCH 2.0 git://github.com/basho/riak_core.git\nsidejob BRANCH 2.0 git://github.com/basho/sidejob.git\nbitcask BRANCH 1.7 git://github.com/basho/bitcask.git\nriak_pipe BRANCH 2.0 git://github.com/basho/riak_pipe.git\nriak_api BRANCH 2.0 git://github.com/basho/riak_api.git\nriak_dt BRANCH 2.0 git://github.com/basho/riak_dt.git\nriak_kv BRANCH 2.0 git://github.com/basho/riak_kv.git\nmerge_index BRANCH 2.0 git://github.com/basho/merge_index.git\nriak_core BRANCH 2.0 git://github.com/basho/riak_core.git\nriak_kv BRANCH 2.0 git://github.com/basho/riak_kv.git\ncanola BRANCH 2.0 git://github.com/basho/canola.git\nnode_package BRANCH 2.0 git://github.com/basho/node_package.git\ncluster_info BRANCH 2.0 git://github.com/basho/cluster_info.git\nriak_kv BRANCH 2.0 git://github.com/basho/riak_kv.git\nriak_search BRANCH 2.0 git://github.com/basho/riak_search.git\nriak_control BRANCH 2.0 git://github.com/basho/riak_control.git\nyokozuna BRANCH 2.0 git://github.com/basho/yokozuna.git\nriak_auth_mods BRANCH 2.0 git://github.com/basho/riak_auth_mods.git\nrebar_lock_deps_plugin BRANCH master git://github.com/seth/rebar_lock_deps_plugin.git\n. Why is 'make' doing the wrong thing then? I guess we have run over this before, but if I check out a tag and call 'make', I'd expect it to make that tag, not some weird mismash of tags and branches.\n. Yeah, this was my bad, although as an open source user, it is very confusing. I just don't have a great suggestion for how to make it better for users and developers.\n. Shouldn't the analyzer port be different across the 3 different dev VM configs so they don't conflict?\n. Why aren't all these test files bundled with riak_search?\n. I think you removed a little too much from the comment here. The note saying that after the rolling upgrade is complete you should change this to false is useful information.\n. Committing certificates like this is probably not a good idea. At the very least they should be generated, no?\n. You can automate creating certs by using a script that feeds data into the interactive command. I've done it before. I bet an escript that spawns a port would work fine. There even seems to be an example of doing this shipped with erlang.\nSee also:\nhttp://www.erlang.org/documentation/doc-5.7.5/lib/ssl-3.10.8/doc/html/create_certs.html\nhttps://github.com/erlang/otp/blob/master/lib/ssl/examples/certs/src/make_certs.erl\nhttps://github.com/erlang/otp/blob/master/lib/ssl/test/erl_make_certs.erl\n. Indentation is wrong here.\n. ",
    "rzezeski": "It's been a long time since I did much work with releases but AFICT this PR isn't needed.\n1. Jared woud yell if the build was failing in this way\n2. It seems reltool automatically pulls in applications that listed apps depend on, and since riak_search is listed it will pull in merge_index\nAny objects on closing Dan?\n. Merged manually with --squash\n. Hrm, worked for me just now, ran it 5 times.  I wonder if it's a timing thing?  Could you try putting an echo or two between the delete and exists check and see if that makes a difference.\n. +1 to this\n. I wonder about the app.config changes.  From my understanding when a user upgrades the pkg installers will keep the old app.config.  This means that riak_api will revert to defaults if user doesn't migrate PB config from riak_core to riak_api.  This poses a problem if said user has modified their PB settings (especially IP/Port).\n. I agree.  I'm going to fix riak_api to do just that.  I feel in the future we'll need a better story than \"make sure you go into all your app configs and change this stuff or things will break in the future.\"\n. Hopefully this fixes the upgrade path, need to test still.\nbasho/riak_api#1\n. The recent riak_api additions fix the upgrade path and allow PB to function with old configs.\n+1 to merge.\n. 1. What is the value for -name for both vm.args?\n2. Don't copy them here, but is the value of -cookie in both vm.args the same?\n3. Is port 4369 open on both nodes?\n4. Are all ports open?  By default distributed Erlang binds to a random port.\nSee http://docs.basho.com/riak/1.2.0/cookbooks/Network-Security-and-Firewall-Configurations/\n. +1 to merge\n. I vote we make attach use remsh by default, here are my reasons:\n1. If lager console is enabled and node is busy then it can be impossible to do anything because logging msgs clobber all your input/output.\n2. If you mess with any shared binaries or make other big structures the memory will continue to be used even after you detach.\n3. I recently watched a customer bring down multiple nodes by using Ctrl-C, and being fairly upset about it.\n. +1 to merge\n. @charl \nThe stacktrace you pasted is telling you that your merge index buffers are corrupted.  Merge index is the inverted index used by Riak Search.  If you take the node offline you can use some Erlang code I wrote to detect the bad files and remove them.\nhttps://gist.github.com/rzezeski/3250870\nYou'll want to run that script for every partition in your merge index directory.  Any files it reports as bad you'll either want to move to a different directory or just delete them.  Then restart the node.  You'll have missing indexes but since only one node seems to have corrupted data you can use the repair functionality to fix it.\nhttp://docs.basho.com/riak/latest/cookbooks/Repairing-Search-Indexes/\nFor example, if you remove files from partitions A, B, and C you'll want to attach to Riak via riak attach and run the following.\n``` erlang\n\nriak_search_vnode:repair(A).\nriak_search_vnode:repair(B).\n...\n```\n\n-Z\n. There are no plans to investigate this with the deprecation of legacy riak_search and the introduction of new search (Yokozuna).\n. The 1st option will never be added to legacy riak_search because it has been deprecated. I think considering a native multi-get feature for new search (Yokozuna) is an interesting idea but it need to be discussed by Basho and would likely not be done as suggested above since it a) assumes the Solr endpoint for querying (I'd like to see more general query + multi-get), b) assumes an XML content-type for return body and c) doesn't use a CDATA section.\n. @ksauzz Yes, the enabled option should be in the Yokozuna README.  This last week has been a bit hectic for me getting ready for RICON.\n@jj1bdx Don't worry about yz-merge-master for now.  Once 1.4 drops all these side branches will go away as proper PRs are made to merge Yokozuna into Riak/KV master.\n. Addresses basho/yokozuna#155\n. +1 to merge.\n. AHA! See @joedevivo! I told you there was a problem. :)\n. IIRC once I did this security got in a bad state and I couldn't fix it without a fresh cluster. I could be remembering wrong but would be good to test before closing this issue.\n. Okay so this issue should be reopened then if the default is still 1024.\n. +1, don't forget to update EE\n. Hi, so it looks like you still have an index associated with one of your buckets. I can't tell which bucket from this truncated log entry but if you can link me to a tar.gz of the full logs I'm sure I could figure it out. The 30-90% at idle is Yokozuna AAE trying to fix the divergence between KV and Solr but it looks like you removed the index so (because of 404 in 2nd log entry) the AAE repair keeps failing to write the error document and thus retries over and over indefinitely.\nSo my question is, do you have search_index set on any of your buckets?\n. Try this at the riak console:\n[ proplists:get_value(name, BP) || BP <- riak_core_bucket:get_buckets(yz_misc:get_ring(transformed)), proplists:is_defined(search_index, BP)].\n. Yep, so you need to remove disassociate the index with all those buckets. Try this:\n[ begin Name = proplists:get_value(name, BP), riak_core_bucket:set_bucket(list_to_binary(atom_to_list(Name)), [{search_index, <<\"_dont_index_\">>}]) end || BP <- riak_core_bucket:get_buckets(yz_misc:get_ring(transformed)), proplists:is_defined(search_index, BP)].\nThe other option is to just turn off Yokozuna if you are not using it.\n. Well it sounds like you've been experimenting with this cluster and you certainly have a lot of buckets with indexes so I'm trying to understand what state your cluster is in.\nWhat do you see if you run this:\nredbug:start(\"yz_exchange_fsm:repair -> return\").\n. Sorry, I should have said you have to wait 15 seconds and see if it prints out anything. That command is adding some dynamic tracing to see if that function is ever called and if so it will print the arguments it is called with as well as its return value.\n. If you go the full 15 seconds without any calls to that function you'll just see qutting: timeout.\n. Okay good so it's not AAE repair getting executed. The question is what is eating cpu. Could you try opening another terminal window and running riak-admin top -interval 2. Let it print for a few times then copy output here.\n. Ah so yz_cover seems to be dominating. That is responsible for building coverage plans. Currently it has linear runtime based on number of indexes created. Run this curl for the list of indexes:\ncurl 'http://localhost:<riak_port>/search/index'\n. There it is, the smoking gun. I assumed it was AAE before but I was wrong. It is yz_cover eating up your CPU. If you run curl -X DELETE 'http://localhost:<port>/search/index/<index-name>' for each of those indexes you should see CPU% drop.\n. Yes this is a background process that executes every 2 seconds. It runs regardless if there are incoming queries. I have several ideas to greatly reduce its runtime but at the moment it is linear to the number of indexes. It uses mochiglobal which is one of the best ways to build a global cache in Erlang because in literally generates Erlang code on the fly and loads it but the overhead of generating the AST gets costly quick. \n. I created an issue on the Yokozuna repo to address this problem. I will close this issue but feel free to continue commenting on either issue.\nThank you very much for taking the time to share this and diagnose the issue with me.\n. > Also for yokozuna pbc interface, it only supports q, which didn't support fq,facet or any solr query arguments, so will it ever going to support that? or prefer to use a solr client instead?\nThe PBC interface is limited to the operations supported by legacy\nRiak Search. If you need to use features like faceting then you'll\nhave to use HTTP for now. The fq param is supported but it is called\nfilter in the current PBC interface.\n. > When working with localhost:8098/stats, I get the version of riak_kv_version of 1.4.2-529-g2ea327b, which I can get correct version info from pbc with\n\n[RiakPBC,client] 2.0.0pre18.\n\nThe riak_kv_version is based on the riak.app file which has its\nversion generated based on the last tag (git describe --always --tags).\nIn the case of 2.0.0pre18 the last tag on riak_kv was 1.4.2\nand there have been 529 commits since. We haven't yet tagged the\nriak_kv repo with 2.x because we are still in pre-release stage.\nThe version coming back from PBC is the overall Riak version which is\ncurrently different from the riak_kv version.\n. @jaredmorrow is that a +1 for all three?\n. Okay. FWIW KV is simply an export of 3 functions and Yokozuna is a copy/paste of what KV AAE does.\n. +1 to > 100. I see log msgs getting dropped during riak startup.\n. Applied to riak_ee as commit 66ba173d72c60186b864b87e35a97e437e7319da\n. All commands should be updated/added but this PR is only for search.\n. Merged in ee in commit: 640fe7c2ddc48edca3471ea3e7bec4c096dcd052\n. How'd your Accept game come up?\n. > However, If I run the same query within Solr's web console e.g. name_s:Lion* I get this response:\nYou will not get correct results from Solr's HTTP port or its web console.  Yokozuna handles the distributed bits and you need to query via Yokozuna in order for the appropriate distributed query to be run.  All use of Solr's HTTP port should be avoided.\n. Are you trying to use the new search in Riak 2.0 now?  If so the riak_search_kv_hook has nothing to do with new search.\n. > No, I'm still in 1.4.8 in production. Does 2.0 is ready to use in\n\nproduction?\n\n2.0 is still under development.  There is a beta version available and we are pushing hard to get final out but is probably another month or so away.  But all efforts have been focused on the new search for some time now as it is replacing legacy search in 2.0.\nThat said, you shouldn't be having the issues you are.  Can you share the exact steps you did for the bucket-based indexing (you should definitely avoid the solr update interface in legacy search) so that I can try running on my side.  That is, if you could show me the exact shell commands and code you are using that would be very helpful.\n. I haven't run the code but noticed you are using allow_mult:\nhttps://github.com/mogadanez/riak-search-index-test/blob/master/app.js#L82\nRiak Search in 1.4.8 does not support siblings.  The new search in 2.0 does.  If you keep allow_mult set to false then it should work but you will then rely on wall clock time for resolution.  I noticed you set last_write_wins: true but that doesn't do what you probably think it does; it is poorly named.  That setting means that a write will clobber its previous version.  The \"last write wins\" behavior you want is setting allow_mult: false.\n. @mogadanez I'm going to try running your test program today but did you check the logs on all your machines?  Deleting requires reading a special object that contains the postings list and then sending an update to each node to delete its relevant postings and finally deleting the special postings list object.  Perhaps the second step is failing to send the deletes but the third step is still proceeding; causing postings to exist for objects that have since been deleted.  If this is the case perhaps there are entries in the log about failed updates.\n. @mogadanez I was not able to reproduce with your program.  However,\nlooking more closely at your comments I noticed the issue you see is\nthat numFound does not equal the length of the docs section\nUNLESS you set fl=id.  This immediately made the issue obvious\nto me, let me explain.\nRiak Search (pre 2.0) uses a separate backend for the inverted index\n(called merge_index).  This is where the postings for an object are\nstored and the data structure which is used to answer queries.\nHowever, RS also stores a special object of its own in the KV backend\nthat contains 1) the verbatim value for each field and 2) the postings\nfor the field after analysis.  That is, 3 things are written when you\nindex a KV object:\n1. the KV object itself,\n2. the special RS object with field values and postings,\n3. the postings in merge_index (MI)\nA query with no field list (fl) specified will first query MI to get\nthe identifier for each matching object; then it will use each ID to\nfetch the special object to return the stored fields in the results.\nHowever, there is an optimization.  If fl=id then only the ID needs\nto be returned and thus the second step, retrieving the special\nobjects, can be avoided entirely.  In one case the special objects are\nretrieved in the other they aren't.  This is the reason behind the\nnumFound != docs.length issue.  The numFound comes from the number\nof results by MI, but the docs list will depend on the fl.  When\nthese numbers do not match it indicates that the postings exist in\nmerge index but the special object does not.  This brings us to\ndelete.\nThe reason the special object has a copy of the postings is so that RS\nknows what to delete from MI when an object is deleted.  When an\nobject is deleted it fetches the special object, extracts the\npostings, and sends requests to delete them from the various MI\nservers (note that this is internode communication, not just local)\n[1].  If those request fail but the the special object is still\ndeleted then the inconsistency you witnessed will occur.  After\nlooking at the code I've determined that these requests are not\nchecked for success and the special object is deleted unconditionally.\nThis is a bug in RS.\nAs I said, I was not able to reproduce on a 5-node stagedevrel\ncluster.  My guess is your AWS/EBS deployment is making it easier for\nsome of these requests to fail.  In any case, this is a nasty bug.\nThe only way to fix inconsistency like this in Riak Search is to\nre-index all your data for that bucket.  This is one of the many\nreasons I recommend new users of Riak to avoid the existing Riak\nSearch.  The new version in 2.0 does not have these issues.  I realize\nthat 2.0 is not an option for you since it is not final yet (I don't\nblame you, smart thinking) but I also would not recommend using the\nRiak Search in\n1.4.8.\n[1]: You can see the gory details here: https://github.com/basho/riak_search/blob/develop/src/riak_search_kv_hook.erl#L185\n. > OK - So I sorted this problem. I had a duplicate field within my custom schema. This was visible in the Solr Web UI. Are there any plans to validate the schema at this stage?\nThere is some validation performed but only so much can be done by Riak itself.  Solr provides no mechanism for validating a schema before creating an index.  It's a long discussion and not as straightforward as it seems.  In the future I'd like to see Riak have the ability to \"prepare\" or \"test\" an index before \"committing\" it.  This would allow you to more easily determine if a schema is broken before final creation.  But that is not something that can be done for 2.0.0 as the feature cutoff has long passed.\n\nor is there a command to check the status for the Solr core that gets created via the above command. Since #545 (\"All use of Solr's HTTP port should be avoided\") suggests keeping well a way from Solr's HTTP interface there needs to be a way to check the status for the schema.\n\nFor HTTP you can perform a GET /search/index/Index to determine if an index exists and is up (there is also a PB msg which the clients should support).  I tested this myself by uploading a schema with a duplicate field.  Here is what I saw:\nUpload a schema with duplicate field:\n```\ncurl -i -X PUT -H 'content-type: application/xml' 'http://localhost:8098/search/schema/dup' --data-binary @/root/\nduplicate-field.xml\nHTTP/1.1 100 Continue\nHTTP/1.1 204 No Content\nServer: MochiWeb/1.1 WebMachine/1.10.5 (jokes are better explained)\nDate: Thu, 29 May 2014 16:16:16 GMT\nContent-Type: application/xml\nContent-Length: 0\n```\nCreate an index using dup field:\n```\ncurl -i -X PUT -H 'content-type: application/json' 'http://localhost:8098/search/index/dup' -d '{\"schema\":\"dup\"}'\nHTTP/1.1 204 No Content\nServer: MochiWeb/1.1 WebMachine/1.10.5 (jokes are better explained)\nDate: Thu, 29 May 2014 16:16:56 GMT\nContent-Type: application/json\nContent-Length: 0\n```\nPerform a GET against the index:\n```\n\n[root@basho_dev ~/work/riak]# curl -i 'http://localhost:8098/search/index/dup'\nHTTP/1.1 404 Object Not Found\nServer: MochiWeb/1.1 WebMachine/1.10.5 (jokes are better explained)\nDate: Thu, 29 May 2014 16:17:24 GMT\nContent-Type: text/plain\nContent-Length: 10\nnot found\n```\nContents of the console.log:\n2014-05-29 16:16:56.842 [error] <0.550.0>@yz_index:local_create:193 Couldn't create index dup: {ok,\"400\",[{\"Content-Type\",\"application/xml; charset=UTF-8\"},{\"Transfer-Encoding\",\"chunked\"}],<<\"<?xml version=\\\"1.0\\\" encoding=\\\"UTF-8\\\"?>\\n<response>\\n<lst name=\\\"responseHeader\\\"><int name=\\\"status\\\">400</int><int name=\\\"QTime\\\">645</int></lst><lst name=\\\"error\\\"><str name=\\\"msg\\\">Error CREATEing SolrCore 'dup': Unable to create core: dup Caused by: [schema.xml] Duplicate field definition for 'text' [[[text{type=text_general,properties=indexed,tokenized,multiValued}]]] and [[[text{type=text_general,properties=indexed,tokenized,multiValued}]]]</str><int name=\\\"code\\\">400</int></lst>\\n</response>\\n\">>}\nContents of the solr.log:\n2014-05-29 16:16:56,836 [ERROR] <qtp751799423-20>@CoreContainer.java:988 Unable to create core: dup\norg.apache.solr.common.SolrException: [schema.xml] Duplicate field definition for 'text' [[[text{type=text_general,properties=indexed,tokenized,multiValued}]]] and [[[text{type=text_general,properties=indexed,tokenized,multiValued}]]]. Schema file is /root/work/riak/rel/riak/data/yz/dup/./dup.xml\n    at org.apache.solr.schema.IndexSchema.readSchema(IndexSchema.java:614)\n    at org.apache.solr.schema.IndexSchema.<init>(IndexSchema.java:166)\n    at org.apache.solr.schema.IndexSchemaFactory.create(IndexSchemaFactory.java:55)\n    at org.apache.solr.schema.IndexSchemaFactory.buildIndexSchema(IndexSchemaFactory.java\n...\nUntil the index and schema administration can be made more fluid it may be best to vet a schema in development with the method above before uploading to production.\n. Re-reading your original issue I decided to try GET /search/index with the failed index creation and sure enough it returns dup in the list even though it failed to create.\n```\ncurl -i 'http://localhost:8098/search/index'\nHTTP/1.1 200 OK\nServer: MochiWeb/1.1 WebMachine/1.10.5 (jokes are better explained)\nDate: Thu, 29 May 2014 16:30:52 GMT\nContent-Type: application/json\nContent-Length: 41\n[{\"name\":\"dup\",\"n_val\":3,\"schema\":\"dup\"}]\n```\nThat is definitely confusing.  I'll create an issue.\n. I'm closing this in favor of basho/yokozuna#395.\n. They should, but considering this goes away w/ qilr I'll just remove it in the Qilr patch\n. Because the only way to run them is with an instance of Search running, which is now part of Riak.\n. ",
    "dreverri": "I am no longer able to reproduce this issue.\n. OSS\n. ",
    "jtuple": "+1\n. The first exists check in \"Verify proxy object is deleted for local docs\" fails for me. All the other exists checks work fine.\n...\n(PASS) {echo,   \"Verify proxy object is deleted for local docs\"},\n(PASS) {index,  \"../_files/docs\"},\n(-FAIL-) {exists, <<\"_rsid_test\">>, <<\"doc1\">>, true},\n(PASS) {delete, \"../_files/docs\"},\n(PASS) {exists, <<\"_rsid_test\">>, <<\"doc1\">>, false},\n...\n. All good, merge away.\n. Looks reasonable, +1\n. Change is straightforward. Went ahead and built a 64-bit Ubuntu package off this branch and tested installing it clean versus installing over existing Riak install (and keeping existing config).\nOn fresh install:\n(riak@127.0.0.1)2> application:get_env(riak_core, legacy_vnode_routing).\n{ok,false}\nOn install-over-existing:\n(riak@127.0.0.1)2> application:get_env(riak_core, legacy_vnode_routing).\nundefined\nWhen undefined, Riak defaults to legacy mode. All looks good to me, short of running the basho_expect rolling upgrade test, which seems like overkill here.\n+1 Merge\n. I've pushed out new code to both basho/riak#148 and basho/riak_core#181 that should address raised concerns plus fix some bugs that I ran into when testing further. It would be great if you could both could re-test things. The plan should never lie. Also, you can no longer plan/commit while transfers are on-going. To solve this, I've added riak-admin transfer_limit. Users can set the transfer limit to 0, wait for the ring to stabilize, plan/commit new changes, then increase the transfer limit. A little annoying, but makes things entirely safe.\nOne thing to note is that any node that is ever downed with riak-admin down will always immediately re-join the cluster and claim partitions whenever it comes online. Even if the node was staged for joining, but downed before actually committing the join. There is no easy way around this without major changes to the claimant code. I'm willing to live with this for now.\nAlso, the 'down rejoining the cluster' behavior never shows up in any plan.  It doesn't make sense. If 4 different nodes are in the down state, how each would claim would be a property of which nodes came back online and at what time. It's entirely non-deterministic and based on outside behavior (nodes coming back online, etc). In short, there's a reason it's still riak-admin down and not riak-admin cluster down: down lives outside of the staged clustering system.\nWith that said, the reported down bug in this review has been fixed. Downing nodes should never make the plan lie about what will happen with any staged changes to non-down nodes.\n. It would still be a good idea to double-check if there were any remaining deprecated targets. However, now that basho/erlang_js#29 has been merged, top-level Riak can no longer compile. I've tested that this pull-request fixes that problem. We should go ahead and merge this now to get Riak compiling again, and fix any other deprecated usage down the road if they're discovered.\n+1 merge\n. +1 merge\n. +1 merge. See basho/riak_core#301 for more detailed review.\n. Health checks are forced disabled in 1.3.2 unless people explicitly enable them by uncommenting the line Engel provides here in app.config. This config option is entirely new in 1.3.2. There are other health check flags that exist in earlier releases, but without this new option provided, KV health checks won't be enabled. See this commit for details.\nEDIT: Rather, the riak_kv option Engel mentions in his app.config comment is the new option in 1.3.2. Without also adding this option, KV health checks are disabled regardless of the riak_core setting.\n. Merging as part of basho/riak_kv#710 and basho/riak_core#441\n. All 2.0 RC takes complete. Moving to 2.0.1 to track items that were punted\n. ",
    "jaredmorrow": "+1\n. +1\n. Yeah, we can reopen if we decide to do this.\n. cherry-picked master's rebar so I can test the builds\n. +1, confirmed the package contents and ownership\n. Oh, I can look into that.   I just checked on solaris and digest can support sha256.   If no one minds, I'll just switch md5->sha256 so we aren't creating too many files.   Opinions?\n. Changed to sha256 and changed deb/makefile to be more like rpm/makefile\n. +1\n. +1, git.vsn files now populated, merging for 1.0 rc1\n. +1000, best PR\n. Manually merged since it could not be merged automatically in this PR.\n. +1, thanks\n. +1, tested that everything still works on Solaris, Fedora, Ubuntu\n. +1\n. WIll merge and fix spacing in a follow on commit.\n. Needs to be added to the help line for riak-admin\n. I can see some people having trouble parsing this:\nroot@build-osol-0906-64:~# riak-admin diag\nAttempting to restart script through sudo -u riak\n20:16:23.704 [error] CRASH REPORT Process <0.44.0> with 0 neighbours crashed with reason: {error,badarg}\n20:16:23.704 [error] Supervisor net_sup had child net_kernel started with net_kernel:start_link(['riak_diag627@127.0.0.1',longnames]) at undefined exit with reason {'EXIT',nodistribution} in context start_error\nescript: exception error: no match of right hand side value \n                 {error,\n                     {shutdown,\n                         {child,undefined,net_sup_dynamic,\n                             {erl_distribution,start_link,\n                                 [['riak_diag627@127.0.0.1',longnames]]},\n                             permanent,1000,supervisor,\n                             [erl_distribution]}}}\n  in function  riaknostic_node:start_net/0\n  in call from riaknostic_node:try_connect/0\n  in call from riaknostic_check:check/1\n  in call from riaknostic:'-run/1-fun-1-'/2\n  in call from lists:foldl/3\n  in call from riaknostic:run/1\nWhen the simple error is that riak is not running.   Don't know if that is really needed for the first release, but difficult to read stack traces is one of the things that people complain about and this is supposed to take some of that difficulty out.\n. riak-admin diag with no arguments returned:\n```\nroot@build-osol-0906-64:~# riak-admin diag\nAttempting to restart script through sudo -u riak\nescript: exception error: bad argument\n  in function  list_to_float/1\n     called as list_to_float(\"format:\")\n  in call from riaknostic_util:binary_to_float/1\n  in call from riaknostic_check_memory_use:check/0\n  in call from riaknostic_check:check/1\n  in call from riaknostic:'-run/1-fun-1-'/2\n  in call from lists:foldl/3\n  in call from riaknostic:run/1\n```\nThe riaknostic page claims you can run it without arguments.\n. riak-admin diag memory_use returned:\n```\nroot@build-osol-0906-64:~# riak-admin diag memory_use\nAttempting to restart script through sudo -u riak\nescript: exception error: bad argument\n  in function  list_to_float/1\n     called as list_to_float(\"format:\")\n  in call from riaknostic_util:binary_to_float/1\n  in call from riaknostic_check_memory_use:check/0\n  in call from riaknostic_check:check/1\n  in call from riaknostic:'-run/1-fun-1-'/2\n  in call from lists:foldl/3\n  in call from riaknostic:run/1\n```\n. Since the above are issues with riaknostic, and not the integration of the diag command with riak and riak_ee, I'll continue to test the basic command on multiple platforms rather than trying more specific commands.\n. This a documentation issue (the riaknostic.basho.com page), but going to add it here to keep everything in one place.\n64-bit fedora packages install the lib directory to \"/usr/lib64/riak/lib\"   \nI know, I know, picky.\n. With the additions of the github issues, and the riak-admin help text, I'm +1 to merge.   Just tested Ubuntu (my last remaining platform).\nSo +1 to merge.\n. Added removals from the package vars.config as well.   Merge away when this gets merged.\n. PR used for test of packaging, this PR was mostly whitespace and naming changes that were tested by the buildbot packaging.\n. Nice catch, thanks.\n. This bug was fixed, the proper dependencies are now listed, so the package installation will fail if the proper libssl library is not installed.\n. That is interesting.  What Riak package are you using?  If you uninstall Riak and try installing the package with Yum does that help the issue?  \nThe original bug was us not listing the dependency properly in the RPM package, but that has since been fixed, so I am really curious what is going on with your install.\n. Bogdan,\nOh, great.  Did you happen to notice if the yum command installed anything else along side the riak package?\nIf you have any other troubles, feel free to email me jared at basho dot com.\n. Fixed here https://github.com/basho/riak/pull/175\n. This part was removed in #268 so I will close this issue now.\n. @metadave can you do a PR for your change and I'll +1 it?   I found this while looking through the backlog.\n. Track this issue here: basho/riak_control#14\n. There has been no activity on this for 2 years, so I will close it.  If someone wants to dig hard into this, feel free to reopen.\n. 2 years with not much progress, going to close this as an issue.\n. Fixed here https://github.com/basho/riak/pull/175\n. @postwait if you could give a quick review, I'd appreciate it.\n. +1\n. +1'd from @Vagabond and @jtuple in chat\n. This needs to be rebased to pick up the underscore/hyphen change that went in this week.\n. +1, thanks (woohoo, 5 months later)\n. +1\n. Merging with known problems surrounding pkg_delete and the package.   Future commits will change directory structure and hopefully quiet down pkg_delete.\n. Is this still a requirement by that customer?  I question whether outputting something new over stdout on start would mess up deployment scripts used in the wild.  It might be a more impactful change than we expect.\n. Closing for now, will reopen if this comes up again as a customer request.\n. Tested on EDS, so going to merge so we can have working packages again\n. /cc @russelldb as he would know if there is something in the pipeline or not.\n. Stats are being re-inestigated in 2.1 with the Exometer package, will set the milestone for 2.1.\nSee: https://github.com/basho/riak/pull/448\n. Agreed, this needs to be fixed.  I will set the milestone to post 1.4 to look at redoing this.\n. Removed reboot completely.  Restart will be reworked to continue working with our longer shutdown and startup times.\n. @dcarley thanks for fixing this oversight.   I built a package and tested against our forthcoming 1.2 release.   I will add you to the THANKS file.\n. Try checking the packages directory, you were trying to do a pkg_info command on the source tarball.\n``` shell\n$ pkg_info riak-1.2.0pre1-FreeBSD-amd64.tbz \nInformation for riak-1.2.0pre1-FreeBSD-amd64.tbz:\nComment:\nRiak highly scalable, fault-tolerant distributed database\nDescription:\nRiak is an open source, highly scalable, fault-tolerant distributed database.\nWWW: http://basho.com/products/riak-overview/\n```\n\nWas this package tested at all? It's completely unusable.\n\nThanks for being levelheaded and polite about it.\n. Documentation will be written for 1.2 (when FreeBSD will be supported) when 1.2 is released.  The output of the make package RELEASE=1 command shows where the package is copied so for people building from master it will at least give a hint.\n. Attempt to delete openssl with Riak installed\nshell\njameson% sudo pkg_delete openssl-1.0.0_7  \npkg_delete: package 'openssl-1.0.0_7' is required by these other packages\nand may not be deinstalled:\nriak-1.2.0pre3-075c35a6\n. @sc68cal no problem, let me know if you have any other issues/requests.  I don't have a lot of go-to FreeBSD users, so if you use it please give me feedback.   The obvious missing piece is some sort of startup/init.d script, but that won't make it for 1.2.\n. This does not work on Solaris 10, I will investigate how to change it to work properly.\n. Closing this as there has not been any activity on this and it hasn't come up in the field recently.  We can reopen if it is deemed an issue in riak-debug.\n. Please add a pull request to riak_ee and link the two together with the basho/repo#num syntax so we can make sure both get updated.\n. +1\n. Added https://github.com/basho/riak/commit/eced9827cf350fd11bedf9a8e203edc258c6337e\n. http://basho.com/resources/downloads/ is the new downloads site and downloads.basho.com has been phased out.  The release notes are current for the 1.2 release.\n. Merging based on riak_ee review.\n. Closing the issue in favor of using riak-admin cluster replace in reip's place.\n. The rebar we have is from October, 27th 2011 which is the correct version for Riak 1.1.  Calling make will try rebar get-deps but that's not what you want to do if you download the source tarball because the dependencies are already included.  Follow the installation page about building from source and see if your issues go away.\n. Ah yes, that was a bug found in the build system for 1.1, I'll post a work around in a moment.\n. The tarball for leveldb should be fetched during the build process when I generate the source tarball, but for Riak 1.1 eleveldb wasn't doing that.  To work around that, fetch via zip the proper commit as specified in the build_deps.sh file.\nshell\ntest-1% cd deps/eleveldb/c_src/\ntest-1% wget --no-check-certificate https://github.com/basho/leveldb/zipball/14478f170bbe3d13bc0119d41b70e112b3925453\ntest-1% mv 14478f170bbe3d13bc0119d41b70e112b3925453 leveldb.zip\ntest-1% unzip leveldb.zip\ntest-1% mv basho-leveldb-14478f1 leveldb\ntest-1$ cd ../../../\ntest-1$ make rel\n. Closing now, will add to a recap for people who might not have git available.\n. No it was not fixed in 1.2.0 as our build system depends on clean git checkouts and we didn't come up with a good way to pull in the leveldb C source code in with our erlang build tools in a clean / repeatable way.   For 1.2, use the above instructions, but pull down the following zip file.\nhttps://github.com/basho/leveldb/zipball/2aebdd9173a7840f9307e30146ac95f49fbe8e64\n. Fixed in 1.3 and master (1.4)\n. +1, I will merge and bring it into 1.3.\nFixes #192 \n. Fixed in the about PR's.\n. @wfarr I'm making new builders as we transition datacenters, I'll make a Squeeze builder while I'm at it since there has been a lot of demand for it.\n. This was fixed post Riak 1.2\n. I appreciate the comment, though I don't know if it is something we will add.  'make rel' makes a installation of riak on its own and the other commands in each package script moves those folders around where they should be on that distro.  Since each OS seems to do it slightly different I don't know how much work it will save to move that logic into the toplevel makefile.  In addition, as much as I can, I have tried to keep the packaging out of the top level makefile to keep it as small as possible (and it is already quite large).  Maybe I'm missing something obvious, which is very likely the case, so I hate to just shoot down your idea.  If a 'make install' target will make something easier for your use case, please let me know.  Also, gists / PR's for what you are intending are also very welcome, sometimes I can't quite see the light until someone shines it brightly into my eye!\n. WARNING: deprecated app option used\nOption 'app' has been deprecated\nin favor of 'apps'.\n'app' will be removed soon.\nSome targets used in the makefile use outdated commands.  Running through the various Makefile targets to see what is deprecated or not working would be a good thing to do.\n. This breaks in 32 bit ubuntu  https://gist.github.com/2d99c838fc4fa9b4b42c\n. +1, nice work.\n. Slated for Riak 1.2.1 release.\n. Thanks @stoddn \n. +1\n. This removal would be with the assumption that riaknostic will be included by default, correct?\n. @evanmcc I'm going to assume this is going to be post-1.4 considering none of the other PR's are closed yet?\n. +1, tested on Ubuntu 12.04, thanks!  Please submit a PR for riak_ee repo as well.\n. Thanks for the contribution.\n. Thanks for the contribution.\n. +1\n. We found these issues as well and were fixed for the Riak 1.2 release.  Thanks for the PR!\n. ## To Test\nInitial test\n\nFirst do a make rel to get a working riak install\nTest that rel/riak/bin/riak [start, chkconfig, console] all work with a stock config\n\napp.config errors to induce\nDo each of the following to a stock app.config\n1. Add a comma / period somewhere to break parsing\n   - 'riak chkconfig' should fail\n2. Set pb_port, handoff_port, http_port, https_port in any combination to have identical ports\n   - 'riak chkconfig' should fail\n3. Remove pb_port setting\n   - 'riak chkconfig' should put out a warning and pass\n   - 'riak start' should warn on stderr and start normally\n4. Remove handoff_port setting\n   - riak chkconfig should fail\n. Closed with change to riak_kv.\n. @whitenode after discussing this a bit, we decided right now we don't have the bandwidth to maintain and support another external dependency to Riak.  \nHowever, this is exactly the thing we encourage our community to do and we have an entire page dedicated to community contributions.  We appreciate any and all community contributions, so please consider adding your project to our community page.  You can put in a PR here https://github.com/basho/basho_docs to modify that page if that is something you are interested in doing.  \nAlso, I think people on the Riak Users mailing list would be interested as well, so please be sure to post something there.  Thanks once again for the contribution.\n. We haven't yet tested fully on 15B02, but it should build.  I will get 15B02 installed and take a look.  Thanks for the bug filing.\n. Thanks for the update @mkuznetsov I will close this issue for now, but will still start testing 15B02 as well.\n. +1\n. I like the idea, but might not get a chance to look at it in detail for a little bit.  Sorry to push it off, just want to make sure you know someone saw this!  Thanks for the PR!\n. Since @jtuple did the work on the new cluster subcommand I'll let him decide if he wants this functionality moved.\n. Merge based on basho/riak_ee#96 code review.\n. Requesting updated status on this on the linked riak_kv PR.\n. A big rewrite of clients happened for 2.0, but I don't know if this particular issue was touched.  /cc @broach \n. Shouldn't the dev*_vars_config files be gone in this PR?  Also, SNMP is only a EE feature so make sure that port is left out to avoid confusion.\n. +1, making note that you intended to bump up the node count by default from 4 to 6\nI'm fine with either\n. @mshonle @lordnull has this been merged in to master for 1.3 yet?\n. I'm looking at it now, but for some reason 'make' is not being smart about not running 'deps' for each dev/dev* on a devrel so 'make devrel' is taking a long time.  I will work on it and will commit to the branch.\n. Tested on Snow Leopard & Mountain Lion, as well as the buildbot which saw the problem in the first place.\n. Are you piping standard out, or standard error or all?\n. This appears to be fixed in 2.0.\n. This was addressed in the Riak 1.3.0 release.\n. This is a good idea and would add more flexibility to the backup process.  I know some other folks at Basho are rethinking backup/restore, but I do not know when that will land.\n. Backup/Restore is due for a complete rewrite in 2.1 as far as I know.\n. Test added to basho/riak_test here basho/riak_test#116\n. Yes, this was a bug in 1.2.1 when 'status' was added.  It has been fixed in this issue basho/riak#232 and will be available in the next release.\nI will close this issue based on the existence of issue #232. \n. So if you look at the resulting file, there is two of everything.  Take a look at your branch and re-push to this PR.\n. Also, this might need to wait until after we branch 1.3 unless there is a bug associated that is a must-need.\n. So, if it is a nice-to-have, I'd be more inclined to +1 if the new functionality was something like attach-nice with attach staying the same for 1.3.  Also, I'd hold off on any +1 for a test addition to the basic_command_line test in riak_test.\n. @evanmcc I'd like to see a test added to https://github.com/basho/riak_test/blob/master/tests/basic_command_line.erl if new behavior and old behavior (moving run_erl to a different command) is tested and working, I'm +1.  @reiddraper if it makes it in, I'll move it into node_package and add it to cs/stanchion/etc.\n. I'll pick this up for testing.\n. Wrote partial riak_test, but having trouble with some riak_test functionality.  Tested a lot by hand, and added details to release notes.  @angrycub if you can handle a PR to docs.basho.com to update any mention of riak attach I'll finish the riak_test.  In the meantime, this can be merged.\n. The source for erlang is a separate download and installation.  Please see this page on our docs for full instructions on how to install erlang.\n. Agreed, that was very misleading.  It was referring to Riak source, not erlang source, but even then it was no longer in the App folder.  I have since removed that line.  We'll update the README in the coming months to get it more in line with docs.basho.com.\n. Please look at this page for the most up-to-date and accurate docs for installing both Erlang and Riak.\nIf you need more help, consider joining our mailing-list or look us up on IRC, in #riak on Freenode.\n. I think a commented out lager_syslog_backend example would be a nice addition to the app.config\n. +1, tests pass and I see no issues with the lager_syslog code.\nNeed to patch riak_ee as well.\n. :+1:  goodbye legacy M/R\n. We had a tool written in Python, but since Basho is primarily an Erlang shop we didn't contribute to it as much as we hoped as a team.  We wrote riak_test in erlang so that the primary test writers (the developers) would be more comfortable and able to write good tests quickly. \n. +1 from a riak-admin aae-status and app.config standpoint.  Status command worked as expected.\n. Thanks for the PR @doubleyou, we've talked about doing something similar actually with the remsh because of our problems with attach you mentioned.\nWe are in a release cycle right now, so I'll have to come back to this PR after 1.3 branches for RC status.\nThanks again.\n. This is related to #254 \n. @doubleyou is the change made in basho/riak#254 sufficient for your needs?  If so I will go ahead and close this PR.\n. Thanks @doubleyou \n. Your problem is indicated at the top of your output\n16:24:52.073 [error] gen_server memsup terminated with reason: maximum number of file descriptors exhausted, check ulimit -n\nIncrease your maximum open files and that should fix it.  On OSX that's not the easiest task, so look here for instructions.\n. Did you increase it in the launchctl like the docs I linked to said?  Or did you do it right from the shell using 'ulimit'.  If the latter, that's not enough because the subshells that the riak command uses will not continue to get that ulimit setting.  I ask because these errors still seem to indicate you are trying to use file descriptors outside of your allowed range.\ndriver_select(0x0000000000000967, 1026, ERL_DRV_USE, 0) by tcp_inet driver #Port<0.6503> failed: fd=1026 is larger than the largest allowed fd=1023\n. The issue is a limitation in epmd, see this gist to fix the issue: https://gist.github.com/vinoski/4689170\n. Fixes:\nbasho/riak#306\nbasho/riak#313\nbasho/riak#123\n. Fixes:\nbasho/node_package#40\n. @Vagabond I'm +1 on this now, don't know if you want a final review as well?  I've filed some issues on node_package that shouldn't hold up this initial (and huge) change.\n. +1, thanks for the help!\n. +1\nTwo notes, this now sets the minimum build erlang to 14b04 from 14b03, but that's reasonable.  Also, with PR's from now on, can you branch from the basho organization @jj1bdx rather than your personal account.  It makes it a little easier for us to all collaborate on PR's.\nTested compile on osx, linux, solaris.\n. Feel free to merge\nOn Feb 10, 2013, at 4:10 PM, Kenji Rikitake notifications@github.com wrote:\n\nWill do on branching from The basho organization with my @jj1bdx access rights.\nI've tested locally on FreeBSD 9.1R.\n\u2014\nReply to this email directly or view it on GitHub..\n. The most helpful information would be the contents of the /var/log/riak directory after you do a 'riak start'.  The console.log / error.log should show some information.  If not, you can change the logging settings in your /etc/riak/app.config from:\n\n{handlers, [                                                                                                               \n                           {lager_file_backend, [                                                                                      \n                               {\"/var/log/riak/error.log\", error, 10485760, \"$D0\", 5},                                                         \n                               {\"/var/log/riak/console.log\", info, 10485760, \"$D0\", 5}                                                         \n                           ]}                                                                                                          \n                       ] },\nto:\n{handlers, [                                                                                                               \n                           {lager_console_backend, info},                                                                              \n                           {lager_file_backend, [                                                                                      \n                               {\"/var/log/riak/error.log\", error, 10485760, \"$D0\", 5},                                                         \n                               {\"/var/log/riak/console.log\", info, 10485760, \"$D0\", 5}                                                         \n                           ]}                                                                                                          \n                       ] },\nThis will log plenty of information to the console.log if you are doing riak start or will print it to screen if you do a riak console to start Riak.\nIf you get those startup logs, just added them to this issue and I'll take a look.\n. Also, we've had one other report about permissions problems on the /tmp/riak directory causing startup issues.  We have added a check and warning to RC2 here #192  Either replicate that commit in your riak script, or try RC2 and let me know if that helps any more.\n. Think we figured it out, someone else had this same problem.  If you are on a single CPU machine, it will fail.  We fixed it https://github.com/basho/riak/issues/274 just now.\n. In your vm.args if you add the line -smp enable it should work.\nRegarding out-of-memory, that is an issue, when erlang runs out, it will shutdown.  The graceful solution is to have more ram and a sufficient node count unfortunately.\n. No problem, sorry for the troubles and thanks for testing out a RC!\n. +1 by proxy  attached to the bug itself\n. +1\n. The easiest option is to install the Ubuntu 12.04 package.  It will create a user for you, add a /etc/init.d script and make sure things are installed in the right spot.  \nYou can get the package and install it via:\nwget http://downloads.basho.com.s3-website-us-east-1.amazonaws.com/riak/1.2/1.2.1/ubuntu/precise/riak_1.2.1-1_amd64.deb\nsudo dpkg -i riak_1.2.1-1_amd64.deb\nFor the future, I will update the documentation for building from source to add some notes in there about creating a Riak user.  Sorry for the confusion.\n. @evanmcc also a great idea\n. Still no riak_kv reviewer??  @evanmcc crack the whip!\n. Awesome, thanks @evanmcc \n. This commit is fine with me, +1.\n. Thanks for reporting this, looks like it is an ancient holdover from Erlang R13 days.\n. Fixed in the above PR's, thanks for reporting it!\n. In our packages, we require sudo as a dependency.  It might not be the perfect solution, but sudo'is what most people are familiar with so that is the choice we made.  If compiling from source, obviously you have the choice to change it to use whatever tool works best in your environment.\nWe support Solaris, FreeBSD, SmartOS in the BSD/Unix domain and all of them have sudo either by default or in packages.\n. For now we will not be replacing sudo with sesudo as most users are willing to install sudo to control their ACL policy for Riak.  Thanks for reporting it, we will keep it in mind for releases down the road.\n. @seancribbs this needs a patch / PR to basho/riak_ee as well.\n. Friendly reminder to add a patch to riak_ee as well.   'git format-patch' and 'git am' is how I usually accomplish it easily so you don't need to do another redundant PR.\n. +1\n. Looks good to me, thanks @DeadZen +1\n. I just disconnected my machine from the network and built from the 1.3.0 tarball, I hit the leveldb issue as described in the \"building without internet connectivity\", in the 'make' step.  Once I put the leveldb directory in place everything built fine without any network connectivity.\nI see where you hit your problem though.  In the 'make devrel' step it did try to go out to the network to re-clone all of the dependencies.  To fix this, simply change this line in the Makefile: https://github.com/basho/riak/blob/riak-1.3.0/Makefile#L79 from\ndev% : all riaknostic\nto \ndev% : riaknostic\nRemove the 'dev' directory to make sure everything is clean and try to run 'make devrel' again.  \nLet me know how that goes.\n. @kientzlecatch thanks for sending the update.\n. I haven't been able to reproduce on any OSX machine I've tried.  I have installed the xcode command line tools on every machine as a base requirement.  For now I'm going to close this because it doesn't seem to be an issue in Riak.\n. Right now this would be incredibly customer-dependent as some networks would work fine with 500kb object sizes and some would fall on their face.  Closing due to lack of progress on this.\n. Yes, we don't support R16 yet.  When we do, we will change all of the rebar.config regex's.\n. /cc @seancribbs because he will give you a way better answer than me.\n. There is nothing wrong with riak, we don't support Erlang 16b because our 1.3 branch of releases was developed when only Erlang 15b was available.  If you install Erlang 15B01, things will work as expected.\n. Since replies have been made on the mailing-list, I'm going to go ahead and close this as an issue for now.\n. Not a big deal, but the output \n\u279c  riak git:(eas-2i-reformat-batch-size) ./bin/riak-admin  reformat-indexes 2 200                           \nindex reformat started with options: [{concurrency,2}]\ncheck console.log for status information\nShows an output message talking about the first param, but not the second.  More of a stylistic / consistency comment.\n. Also, you don't exit on getting the wrong parameters, it just prints the message and still runs the script.\n\u279c  riak git:(eas-2i-reformat-batch-size) ./bin/riak-admin  reformat-indexes 2 200 blah blah blah            \nUsage: riak-admin reformat-indexes\n       riak-admin reformat-indexes --downgrade\n       riak-admin reformat-indexes <concurrency> [--downgrade]\n       riak-admin reformat-indexes <concurrency> <batch size> [--downgrade]\nindex reformat started with options: [{concurrency,2}]\ncheck console.log for status information\n. +1\n. Patched into basho/riak_ee as well.\n. @jrwest @engelsanchez added the change to basho/riak_ee.  The change didn't patch in cleanly, so it was a manual commit, but I referenced this PR.\n. Thanks @engelsanchez \n. There was a lot of work done in 1.4.x regarding 2i, and I suspect this is fixed. \nI'm closing for now, unless @engelsanchez indeed thinks this is still an issue.\n. This is indeed a bug, I will fix it as soon as possible.  Thank you for reporting it.\n. This is fixed by PR #268 \n. +100\n. Setting milestone to 1.4++ unless you want to tell me something different @bsparrow435 \n. It is my understanding there are plans in the works for this, /cc @gburd @michellep \nSetting milestone to 2.1, unless Michelle or Greg tell me otherwise.\n. I don't know why github doesn't detect a merge conflict here, but this is based on a riak-admin pre-node_package convert.  Let me update this PR before you merge @jrwest \n. Updated with new function call.  +1 from me.\n. +1, thanks for getting to this before I could\n. Yes, thank you very much for fixing this +1\n. This has been fixed in node_package, so #268 fixes this issue in Riak.\n. I need to look at how to do this right in node_package because the Riak script and nodetool are about to disappear. \nI don't think it will be a problem. \nOn Apr 17, 2013, at 5:07 PM, Andrew Thompson notifications@github.com wrote:\n\nLooks legit at first glance.\n\u2014\nReply to this email directly or view it on GitHub.\n. After looking it over, I don't see anything wrong with it in regards to being generic enough to go into node_package.\n. This is a really stale bug that didn't appear to get looked at.  Sorry @rspeer, have you looked at Yokozuna in Riak 2.0?  \n\n/cc @rzezeski on handling of this issue (close, keep open, etc.).\n. Need to update to a more recent master to pick up the node_package dep (hence the conflict).  Otherwise I'm +1.\n. Beat me to it :)\n+1, deps fetched fine.\n. Fixed in PR to change riaknostic from an escript to an app.\n. Can we close this now @joedevivo \n. The problem this solved (keeping the tests lined up with Riak) has been solved by simply keeping branches and tags lined up in riak_test.  I'm closing this now.\n. This is not a bug, we haven't tested on 16B from 15B01.  The next step will be 15B03, then sometime down the road will be 16B.  Some people run Riak to the edge of what erlang can handle, so performance testing a new release is a difficult and time consuming task. For now, you can fork Riak and change the regex for your source builds.\n. We have separate Debian 6 packages available, that I build on a Debian 6 machine.\nIt has this libc installed\nii  libc-dev-bin                         2.11.3-4        \nii  libc6                                   2.11.3-4               \nii  libc6-dev                             2.11.3-4\nAnd here is the machine's version information:\nbuildbot@build-debian-6-64:~$ cat /etc/debian_version\n6.0.7\nbuildbot@build-debian-6-64:~$ uname -rv\n2.6.32-5-amd64 #1 SMP Mon Feb 25 00:26:11 UTC 2013\nAre you using the Debian packages found here?\n. Oh, to answer your deprecation question, we have no plans to deprecate Debian support.  The only deprecation plans we have coming up is dropping 32bit support in Riak 1.4.\n. This is what I have\n$ dpkg -l | grep libssl0.9.8\nii  libssl0.9.8                          0.9.8o-4squeeze14            SSL shared libraries\nSo either you don't have libssl0.9.8 installed or it is older version that should be updated.  Try simply udo apt-get install libssl0.9.8 to see if it is installed.\n. FYI you can have libssl0.9.8 and libssl1.0.0 installed at the same time since they are two separate packages and not two versions of the same package. \nOn May 4, 2013, at 9:46 AM, Jude Young notifications@github.com wrote:\n\nAh, I see... I've updated my system partially to unstable.\nThis is the libssl I have: libssl1.0.0:amd64 1.0.1e-2\nOk, I'm retarded. I can fix that. Thanks for the help!\n\u2014\nReply to this email directly or view it on GitHub.\n. /cc @rzezeski he would be the most knowledgeable search expert to answer your question\n. Setting milestone to 1.4.9, for lack of a better resolution as we are moving towards Yokozuna in Riak 2.0\n. Well since this is related to deprecated search, I'm closing this issue for now.\n. +1 on visual inspection of code and discussions about your operations on the pesky OS's\n. +1\n. Yes, this is a bug, app_epath.sh needs to be installed in the rel/reltool.config file.  /cc @fprimex have time to fix this  before 1.4 freeze?  Thanks for catching that @jj1bdx \n. +1\n. @evanmcc we will merge 1.3-> master to pick it up\n. Things I checked:\n- Devrel build & riak-admin diag testing\n- dist build & source tarball build while offline\n\nTwo things need to get done if possible, but not before this PR gets in.\n- The riak_test riaknostic test needs to be changed for the new behavior\n- The readme for riaknostic needs to be updated for the new behavior\n@bsparrow435 I removed extra riaknostic calls from the Makefile that weren't needed anymore.\n+1\n. Attempting to fix by adding [kernel,stdlib] to bear in this commit https://github.com/basho/bear/commit/a2ef0336ec9de6e24aa77bf0a04ceb051505151d\nClosing for now, will reopen if we see this problem again in testing\n. @worldhack this is indeed a bug in our documentation, we have the section you were looking for, but it was behind a version regex filter that didn't include 1.3.1.  I'm really sorry you had to take so much time to figure it out on your own.  The below linked PR will fix this issue moving forward.\n. I like the wording.  If someone upgrades from 1.3.1 -> 1.3.2, and they have health checks turned on, what happens?\n. With that said +1 from me @engelsanchez \nPlease 'git format-patch' and 'git am' into riak_ee.  If you would like just ping me and I can merge 1.3->master as well.  I've already done it once and my system should remember the conflict resolutions.\n. These are just suggested things you can do to find out more about Riak.  \nWhen you click on \"Riak\" from the main docs.basho.com page, you are presented with a Getting Started page that lists the Fast Track first. \n\nYou instead started at the install page, so it suggested other places to explore.\n. +1, lets do this\n. I don't see the problem, what is it exactly?\n. +1\n. We don't test against R16B01 yet that's why you got an error.  If you\nreally want to build against R16 instead of R15 just change the regex in\nRiak's rebar.config.\nOn Friday, June 21, 2013, Bryce Verdier wrote:\n\nI'm on ArchLinux using erlang 16B01.\nWhen trying to use the fast track documentation, trying to do the step\n\"make all\" I get this error:\n./rebar compile\n==> Entering directory /home/bryce/Downloads/riak-1.3.2/deps/lager_syslog'\n==> Entering directory/home/bryce/Downloads/riak-1.3.2/deps/lager'\n==> lager (compile)\nERROR: OTP release R16B01 does not match required regex R14B0[234]|R15\nmake: *** [compile] Error 1\nOddly enough I was able to \"make\" within both the deps/lager and\nlager_syslog directories. So I'm not sure where in the build process this\nerror is occurring.\nHappy to help test in anyway.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/basho/riak/issues/340\n.\n. Developers at Basho already run R16B01 on a daily basis so I don't think there would be a problem functionality wise.  The reason we are slow to move Erlang VM's is due to our use of NIF's for our backends and the slight nuances we've found between even minor releases of Erlang.  The testing needed for 16B01 will probably be complex performance / scheduling testing to qualify it.  \n\nI'd say if you are comfortable with the performance I don't see another reason why you couldn't run Riak on R16.  FWIW we did patch R15B01 with R16B01's scheduler changes, so that has an upgrade path going forward for us.\n. I will go ahead and close this as not a bug, but please feel free to continue the conversation after it closes.\n. The effort to remove incompatibilities happened in the 1.4 development cycle, so it is no surprise you found some issues in 1.3.2.  Post 1.4, our plan is to move to R16.\n. Building Erlang from source is quite easy\nWe have a simple guide for linux here: http://docs.basho.com/riak/latest/tutorials/installation/Installing-Erlang/#Installing-on-GNU-Linux\n* Build Erlang R15B01 from source \n* add its `bin` directory to your path\n* build Riak as you normally would.\n. Please remove the rebar.config file from the PR as it would change the rebar.config to point to a specific branch of riak_kv.\n. 2i Index repairs were added in https://github.com/basho/riak/pull/459, so I'm going to close this considering the riak_kv PR is also not merged.  We can resurrect if the riak_kv PR ends up being adjusted and put in.\n. Yes, sorry too many \"index\"s in the code\n. Patched to riak_ee here: https://github.com/basho/riak_ee/commit/85c75d9b66555a920edec9b7ad97de8631b80ee7\nMerged to riak and riak_ee master branches.\n. This is a known issue with the claim_v2 algorithm.  It is also why we suggest running with 5 nodes as a minimum.  There are already plans to fix this issue in the next version of the claim algorithm that is being written.\n. Sorry for the trouble.  We had some issues with packages and the repos\nneeded to be repopulated and this has obviously caused some issues.  We\nwill investigate and fix as soon as we can.\nOn Wednesday, July 10, 2013, Matthew Dawson wrote:\n\nThe APT repository for Ubuntu Precise appears to have incorrect metadata\nin it, preventing Riak 1.4.0 from installing. You can see the effect here:\nhttps://drone.io/github.com/MJDSys/goriakpbc/12 .\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/basho/riak/issues/349\n.\n. We have rebuilt the repos, please let us know if this has not fixed your issue @MJDSys \n\nThanks for reporting it.\n. I'm +1 on this going where you put it. \nI also would be +1 if you put it in the top-level right next to the other RELEASE-NOTES file.  LIke other releases I can then move it to the releasenotes directory when a new set of RELEASE-NOTES comes out.  I don't think you need to hide the Japanese translation away in another directory.  I will let you make the call.\nThanks for doing the work!\n. That RC1 link was actually just broken and the documentation was not updated yet.  We no longer support 32bit builds on any platform.  Currently there is no plan to go back to 32bit in the future.\n. That link will not always be for 1.4.\n1.4 release notes are here https://github.com/basho/riak/blob/1.4/RELEASE-NOTES.md\n. For future reference, documentation issues should be filed against the docs repo here https://github.com/basho/basho_docs\nI don't think this is still an issue, but in case I'll /cc @lucperkins and @coderoshi   Please close if this is no longer relevant.\n. Closing this as most of what was described here landed in 2.0 pre builds.\n. Bumped milestone to 2.1\n. We are shipping R16B02, so closing this issue.\n. This is not related to 32bit Erlang.  We do not support 32bit, but you can still build Riak on 32bit just fine.  This is a general problem people ran into finding libraries installed by XCode.\n. So I don't think this is a issue with Riak, but an issue with how OSX treats library paths.  I'm all for making errors more obvious, but in this case I don't know what we'd change in Riak. \n. Reminder a PR for EE will be needed as well.\n. Unable to build devrel for testing due to cuttlefish build error:\n==> cuttlefish (compile)\nCompiled src/conf_parse.peg\nCompiled src/cuttlefish_vmargs.erl\n/Volumes/Data/Development/basho/config/riak/deps/cuttlefish/src/lager_stderr_backend.erl:34: can't find include file \"deps/lager/include/lager.hrl\"\n/Volumes/Data/Development/basho/config/riak/deps/cuttlefish/src/lager_stderr_backend.erl:61: undefined macro 'INT_LOG/3'\n/Volumes/Data/Development/basho/config/riak/deps/cuttlefish/src/lager_stderr_backend.erl:24: function init/1 undefined\n/Volumes/Data/Development/basho/config/riak/deps/cuttlefish/src/lager_stderr_backend.erl:115: function eol/0 is unused\n/Volumes/Data/Development/basho/config/riak/deps/cuttlefish/src/lager_stderr_backend.erl:127: function is_new_style_console_available/0 is unused\nERROR: compile failed while processing /Volumes/Data/Development/basho/config/riak/deps/cuttlefish: rebar_abort\n. On a fresh devrel build, no vm.args file is made so riak will not start:\n\u279c  dev1 git:(jd-cuttlefish) ls etc\ncert.pem  key.pem   riak.conf\n\u279c  dev1 git:(jd-cuttlefish) ./bin/riak console\negrep: /Users/jared/Data/Development/basho/config/riak/dev/dev1/bin/../etc/vm.args: No such file or directory\nvm.args needs to have either -name or -sname parameter.\n. Tested devrel, rel & package testing on CentOS 5.  +1 on this working with node_package's jd-cuttlefish branch.  I have not combed through the riak.schema, but I'm +1 from a release/packaging standpoint for Pre1.\nBefore this gets a final merge, I'd like to see it squashed, but I bet that was already planned.\n. There is no Riak package built for Ubuntu 32bit, that is why you are getting the errors about not finding Riak.  We only support 64bit platforms.\n. This looks to be fixed / closed in riak_kv.  Closing unless @joedevivo disagrees. \n. Closing in favor of basho/basho_docs#946\n. +1\n. Tested on OSX 'make rel' and the only empty directories I still see are:\n./data/ring\n./lib/yokozuna-v0.9.0-17-gff0cacb/priv/solr/solr-webapp\n./log\nWhich are needed.\n+1\n. +1\n. Marking this as 2.1, unless this is already fixed @evanmcc \n. Changing milestone to 2.0-RC, so we can come back and track this.\n. I don't have a strong opinion on this one, though I wonder if the things mentioned in your first comment have been addressed?\n\nIn order to make it a default setting for vm.args, though, we need to verify a few things:\n- performance isn't negatively impacted on other platforms (single cpu, platforms that don't support sbt, virtualized hardware)\n- performance isn't negatively impacted on other backends (leveldb, memory, most of the testing has been on bitcask and memory so far, but signs for leveldb are good).\n. +1\n\nMake sure to patch riak_ee\n. +1\n. I'll patch to riak_ee\n. I rebased against develop to make this merge-able.  Sorry for the ugly commit.\n+1 from me\n. Yeah, I really did.  Jordan is going to just format-patch it in.  I did everything by the rebase book and obviously screwed up.\n. This also needs to be patched into riak-ee, you can attempt 'git format-patch' on this commit and then 'git am' in the riak_ee repo.\n. +1 for them to be gone.  The original request from Jeff was to make proper docs or a platform-independent script to generate the certificates easily so users won't have any barrier to testing.\n. Associated PR here https://github.com/basho/riak/pull/401\n. Filed this as a riaknostic issue, thanks for the bug report @lukebakken \nSee: basho/riaknostic#72\n. This appears fixed in 2.0.0pre20\n2014-03-24 11:55:53.723 [info] <0.308.0>@riak_kv_app:prep_stop:218 Stopping application riak_kv - marked service down.\n2014-03-24 11:55:53.724 [info] <0.308.0>@riak_kv_app:prep_stop:222 Unregistered pb services\n2014-03-24 11:55:53.724 [info] <0.308.0>@riak_kv_app:prep_stop:227 unregistered webmachine routes\n2014-03-24 11:55:53.724 [info] <0.308.0>@riak_kv_app:prep_stop:229 all active put FSMs completed\n2014-03-24 11:55:53.728 [info] <0.308.0>@riak_kv_app:stop:240 Stopped  application riak_kv.\n2014-03-24 11:55:53.748 [info] <0.149.0>@riak_core_app:stop:110 Stopped  application riak_core.\n. This appears fixed in the above referenced PR's.  Correct me if I'm wrong @slfritchie \n. This is fixed in our shipped Erlang with Riak (R16b02-basho4) as well as Erlang 17 to be released soon.\n. Yes, since this https://github.com/basho/cuttlefish/pull/38 hasn't merged yet, you will have that problem. /cc @joedevivo \n. Okay, I merged basho/cuttlefish#38, confirm that you no longer need this PR.  @argv0 and @russelldb \n. Which test is that Russell?  I'll check it out today.  Dumb question, but\ndid you rebuild your devrel that you ran riak_test against?\nOn Saturday, October 5, 2013, Russell Brown wrote:\n\nYes, I can confirm that is fine for make rel / make devrel, but I still\nget an error from riak_test\nhttps://gist.github.com/russelldb/356d2735b52fffa1c2e7\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/basho/riak/pull/409#issuecomment-25752179\n.\n. Closing now that this is fixed in basho/cuttlefish#38\n. Emailing the riak-users mailing list will probably help you and others easier as people will walk you through your issues.\n\nAt first glance the resource temporarily unavailable suggests that you ran out of file descriptors.  What is your ulimit -n set to?  If it is low, you might want to bump it up to ulimit -n 16384`.  There is more documention on our docs site about open files limit and setting it based on your OS.\n. Try posting something to riak-users, I'm not the AAE expert at Basho and\nyou'll find more people who have possibly seen similar issues.  Sorry to\npawn this off, I just think it'll be a better way for you to get good\nsupport.\nOn Tue, Oct 8, 2013 at 8:28 AM, Ram\u016bnas Dronga notifications@github.comwrote:\n\n@jaredmorrow https://github.com/jaredmorrow I have tried much higher\nulimit, but without any luck. I think problem occurs when some\nfiles(anti_entropy in this case) is corrupted and riak loops infinitely on\ntrying to achieve lock for these files.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/basho/riak/issues/410#issuecomment-25894512\n.\n. This is still an issue, we should fix this for 2.1.\n. Closing this, instead we should remove reip entirely since it is deprecated.\n. reip needs to stay, so this needs fixing.\n. It no longer references a hard coded path, closing.\n. This branch looks like it was branched from 'master', but then is trying to merge into 'develop'.  Can you fix and resubmit?  /cc'ing @slfritchie will be good since he was the original author.\n. Actually it might have been based off of 1.4, because it is trying to pull in tagged rebar.config settings as well.\n. @slfritchie I don't use top a lot, so I wouldn't know how useful this would be for people.  It probably won't make 2.0 considering it needs a rebase and review.\n. Closing due to lack of activity / response.\n. @TJC is this now fixed in the Java client and can be closed here?  @seancribbs opinions on it also being a bug in the HTTP client?\n. We are in the middle of our 2.0 development / test cycle where our config is going to change dramatically from having an app.config to having a generated config based around a simple configuration file.  Because of this, we probably won't merge this PR.  \n\nIn addition, it will probably be best to use your branch as we have changed our development branch from 'master' to 'develop', so we aren't merging PR's into 'master' for now.  \nI'll /cc @joedevivo (who is the primary person reworking config) to make sure he knows to look out for this in 2.0.\nThanks for the PR, sorry it is not going to work with timing for us at the moment.\n. We are possibly releasing another rev on 1.4, but sadly 2.0 is keeping me too busy to give this a good look at the moment.\n. I don't think the presence of cuttlefish affects this issue at all.  An https port still needs to be set as far as I know.  Handing this off to @joedevivo so he can make a call.\n. +1\n. See: https://github.com/basho/riak/issues/455  \nThough this issue is older, I'm closing it since more tracking has been done in #455.\n. +1 whenever riak_core PR merges and brings it in\n. @jrwest setting to 2.1 unless you tell me otherwise.\n. @jrwest is this a 2.1 targeted change?\n. This is still an issue in pre20, here is the result with running the above (after adding the zman user).\n./bin/riak-admin security print-sources                                                                                                                                                                                                                       \n+--------------------+------------+----------+----------+\n|       users        |    cidr    |  source  | options  |\n+--------------------+------------+----------+----------+\n|        zman        |127.0.0.1/32|   zman   |    []    |\n+--------------------+------------+----------+----------+\nThoughts @Vagabond @macintux.\nI'll put this as 2.0.1 for now.\n. /cc @jgnewman @jburwell for riak_control work.  If interested in this, it should be tracked under basho/riak_control rather than here.  If this is something that won't be supported down the road, please close.\n. So, I haven't looked at homebrew and how it does things, but we have a notion of platform_* directories that get used both for installation and for modules to know where to look for things.  The .schema files do act as library files to us because they are typically not changed once installed and they are used as a template to generate the new configuration files.  The generated configuration files, since they are generated, go into the platform_data_dir for each platform, which makes sense.  The *.schema files go into the platform_lib_dir on each platform.  I personally feel this is a good place, because they don't feel like a /etc/riak/ type thing that is expected to be edited, but I'd also like to help you get this working.  For the moment I'm going to /cc @joedevivo so he can think about making a subdirectory in the platform_lib_dir and I can think about how that might affect packaging on various platforms.\n. +1, worksforme\n. @michellep Is this still an active request?  Also, should this be tracked in our PM or platform_task repos?\n. @wb14123 can you run\n./rel/riak/bin/riak chkconfig\nAnd paste the ouptut here.  I'm curious what app.config / vm.args your build is picking up.  My guess is the config is not being generated correctly for something reason on your build.\nI don't have an arch linux setup I can use, so I'll try to walk through this with you if you don't mind running some commands here and there.\n. The 'develop' branch is a work in progress and you shouldn't use it for production.  If you want a stable version, download the source package of 1.4.2 from the bottom of this page http://docs.basho.com/riak/latest/downloads/\nAlso, browse around the docs.basho.com site for some good walkthroughs and getting started pages.\nInstalling from source instructions can be found here http://docs.basho.com/riak/latest/ops/building/installing/from-source/\nFor 1.4.2 the currently support erlang is R15B01, when 2.0 comes out it will use R16B02.\n. Building erlang from source is quite easy and we also have documentation on\nthat page about how to do it if you would rather not wait.\nWe also have release candidates for 2.0 coming in December.\nOn Friday, November 15, 2013, Bin Wang wrote:\n\nBecause of my OS only has official Erlang package with version R16B02. I'd\nlike to install riak 2.0. Waiting for the release...\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/basho/riak/issues/436#issuecomment-28615864\n.\n. Going to close this as no further investigation was done at the time.  We will reinvestigate if we see this in 2.0.\n. Already done https://github.com/basho/riak_ee/pull/191\n. Fixed in above PR's.\n. Merging based on PR +1 in riak_ee https://github.com/basho/riak_ee/pull/193#issuecomment-28531959\n. This is fixed in cuttlefish https://github.com/basho/cuttlefish/blob/develop/priv/erlang_vm.schema#L178\n. But why then is in commented at 32mb?  That is so confusing, if I see a comment that says 32mb, I'm going to assume that's the default unless I uncomment and change it.\n. Okay, going to do just make a branch and get this done, PR... incoming..\n. @evanmcc 32mb reasonable to you, that was our previous suggestion?\n. I have a branch with the fix, but there is currently a bug in cuttlefish.  Leaving this as a placeholder so no one else starts it.\n. +1, don't forget riak_ee\n. Modifying the spec is somewhat difficult, but can be done.  The spec is generated from a template and can actually be found here in node_package.\n\nI'm more concerned that you cannot build from source from our source tarball. \nI just tried to do it fresh\n$ tar -zxf riak-1.4.2.tar.gz\n$ cd riak-1.4.2\n$ make rel\n.... <snip> ...\nCompiled src/riaknostic_check_dumps.erl\nCompiled src/riaknostic_check_ring_membership.erl\nCompiled src/riaknostic_check_disk.erl\nCompiled src/riaknostic.erl\n==> rel (compile)\n==> riak-1.4.2 (compile)\nCompiled src/etop_txt.erl\n./rebar generate\n==> rel (generate)\n$ echo $?\n0\nI think a problem might be in you using the erlang that ships with your OS.  Typically they are configured badly and do not have everything you need.  We highly recommend building erlang from source.   You can find instructions on our docs page about installing erlang  We ship Riak 1.4.2 with erlang 15B01, so we recommend using that when building from source.\n. Feel free to push to EE\n. /cc @joedevivo haha, think you want to tackle one-more-thing before 2.0 freeze?\n. Ping.\n. On OSX with 1.4 app.config and vm.args it fails with:\n./bin/riak-debug\n........E......EE..............usage: grep [-abcDEFGHhIiJLlmnOoPqRSsUVvwxZ] [-A num] [-B num] [-C[num]]\n    [-e pattern] [-f file] [--binary-files=value] [--color=when]\n    [--context[=num]] [--directories=action] [--label] [--line-buffered]\n    [--null] [pattern] [file ...]\nusage: grep [-abcDEFGHhIiJLlmnOoPqRSsUVvwxZ] [-A num] [-B num] [-C[num]]\n    [-e pattern] [-f file] [--binary-files=value] [--color=when]\n    [--context[=num]] [--directories=action] [--label] [--line-buffered]\n    [--null] [pattern] [file ...]\n. /Users/jared/Data/Development/basho/riak/rel/riak/riak@127.0.0.1-riak-debug.tar.gz\n. +1, nice job @joedevivo \n. The EE PR, since I don't see it auto-linked is https://github.com/basho/riak_ee/pull/204\n. What is the status on this PR?  Is it fully reviewed and ready to go in before beta?\n. Closing this in favor of #455 \n. Cuttlefish (the new configuration system introduced in 2.0) fixes this issue.  Sorry for the troubles in 1.3.\n. Don't forget to apply this to riak_ee as well.\n. Any status on this since the last planned meeting? (3 months ago)\n. @ksauzz make sure to add this to basho/riak_ee as well.  You can open a new PR and merge it or do a 'git format-patch' on this commit and 'git am' it to riak-ee.\n. /cc @rzezeski since he is the expert on search index corruption.  I was going to link to his guide on repairing, but it looks like you already tried that.\n. I just tested against pre20 and this is fixed now.\n. @Vagabond milestone added for 2.0.1\n. There has been work from @macintux on the CLI UX for security b/w pre18 and pre20, any idea if these issues still exist?\n. Sorry, accidental \"close & comment\" button push.\n. Thank you, this has been at the back of my mind as a TODO for a while.  Be sure to format-patch/am into riak_ee as well.\n+1\n. Pre8 was not packaged and released into the wild.  I don't know why the URL\nis pointing you there in error.  If you try the same URL but with \"pre5\"\nyou should be all set.  I'll forward this along to the docs team.\nOn Wednesday, January 15, 2014, Conrad Taylor wrote:\n\nHi, I'm not sure if this is the right place but it appears that the\nfollowing link is broken:\nhttp://docs.basho.com/riak/2.0.0pre8/downloads/\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/basho/riak/issues/474\n.\n. We only build packages for LTS releases so you will have to add the precise\nrepo to get the packages.\n\nOn Wednesday, January 15, 2014, Conrad Taylor notifications@github.com\nwrote:\n\nHi, I'm in the process of installing Riak on Ubuntu 13.10. Thus, I have\nperformed the following steps:\n$ sudo bash -c \"echo deb http://apt.basho.com $(lsb_release -sc) main > /etc/apt/sources.list.d/basho.list\"\n$ sudo apt-get update\nHere's a brief summary of the output:\nErr http://apt.basho.com saucy/main amd64 Packages\n  403  Forbidden\nErr http://apt.basho.com saucy/main i386 Packages\n  403  Forbidden\nW: Failed to fetch http://apt.basho.com/dists/saucy/main/binary-amd64/Packages  403  Forbidden\nW: Failed to fetch http://apt.basho.com/dists/saucy/main/binary-i386/Packages  403  Forbidden\nE: Some index files failed to download. They have been ignored, or old ones used instead.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/basho/riak/issues/475\n.\n. We do not currently support ARM as an architecture.  We would most likely have to change our erlang_js to use a javascript VM that supported ARM.  It has been discussed in the past, but it is not currently on our roadmap as far as I know. \n\n@gburd might know more regarding any possibility of supporting it in the future.  For now I will close this.\n. Out of curiosity are you using ARM based servers or is this on an embedded\ndevice?   Greg and I would love to hear more as this topic has come up\nquite a bit.\nOn Saturday, February 1, 2014, Vincent Bernardoff notifications@github.com\nwrote:\n\nOn 31/01/2014 22:39, Gregory Burd wrote:\n\n@vbmithr https://github.com/vbmithr we don't currently develop or test\non ARM architecture CPUs, we may in the future. If this is a critical\narchitecture for you please let us know. Pull requests are always\nwelcome!\n\nStrangly enough, I managed to make it work. It was an issue on erlang_js\nwhen compiled in a submodule of riak. I then checked out erlang_js in a\ndifferent directory and managed to get it working.\nSo far riak (HEAD) works on my ARM test machines.\n\nReply to this email directly or view it on GitHubhttps://github.com/basho/riak/issues/480#issuecomment-33867711\n.\n. echoerr can be used from the env.sh https://github.com/basho/node_package/blob/develop/priv/base/env.sh#L50 so as to print on stderr and not affect runner scripts.\n\nAlso, placeholder to remember to format-patch / am to riak_ee after this is +1'd.\n. This has been reported here https://github.com/basho/node_package/issues/72 and is where the bug would be fixed if we decided to do that.  Changing from sudo to su has weird edge cases passing the environment through the calls, but I haven't looked at it again in a long time.\n. #473 has been merged now.\n. I have a fix for this with 'su' that will be out in the next build.  Sorry\nfor the trouble.\nOn Friday, April 18, 2014, Oleksiy Krivoshey notifications@github.com\nwrote:\n\nAnd the breaking change is in /usr/lib/riak/lib/env.sh:\nthis won't work:\nexec su - $RUNNER_USER -c \"$RUNNER_SCRIPT_DIR/$RUNNER_SCRIPT $ESCAPED_ARGS\"\nthis will work:\nexec sudo -H -u $RUNNER_USER -i $RUNNER_SCRIPT_DIR/$RUNNER_SCRIPT \"$@\n\nReply to this email directly or view it on GitHubhttps://github.com/basho/riak/issues/487#issuecomment-40853558\n.\n. I know this is already closed, but down the road if we look at this PR it has no explanation at all on what it does.  The \"why\" is completely missing.  I'd prefer the PR get edited, so we can at least get some context.\n. Magic!  Thanks.\n. Is there a complementary one for basho/riak_ee?\n. /cc @jtuple regarding the AAE issue, as he is the main expert there.\n. /cc @macintux, he just spent some time debugging UTF8 issues, but I think that was in regards to security.  I'll leave this here for now and put it on the 2.1 milestone.\n. Sorry for the early notice, I was trying out the GitHub releases feature.\n The plan is to send out the proper release announcement today sometime.\n\nOn Thursday, February 20, 2014, George Psarakis notifications@github.com\nwrote:\n\nDoes anyone know when the Ubuntu repositories will be updated for the new\nrelease https://github.com/basho/riak/releases/tag/riak-1.4.8?\nThanks!\n\nReply to this email directly or view it on GitHubhttps://github.com/basho/riak/issues/495\n.\n. For 2.0 we might require using gcc instead of clang, both @Vagabond and I ran into the same problem.  I'll set the target of addressing clang for 2.0.1 milestone for now.\n. Yes, if you build with the GCC flags then the world is happy, if you try to build with clang, the world is sad.  I'm set on FreeBSD10, but if you want people to be able to build leveldb out of the box on FreeBSD with the default compiler, then no there is still an issue.\n. This looks like a split brain scenario where you actually appear to have two distinct clusters.  Are you still having the problem, or has it been resolved?  Are you on the riak-users mailing list?  If not, we will be able to more easily help you there.\n. Also /cc'ing @engelsanchez and setting milestone for 2.0.1.  Since I don't know if this was already fixed in the 1.4.x series.\n. /cc @engelsanchez \n\nWhat version of Riak are you seeing this on @peczenyj?\n. /cc @rzezeski since the codes are in relation to Yokozuna.  I'm setting the milestone to be 2.1 for now since we are too late in the 2.0 cycle to change the API.\n. I'm not the expert on strong consistency, but I know someone is looking and working on the CPU utilization at the moment.\nOne comment on why you are seeing unbalanced results is the use of a 3-node cluster.  With N=3 and 3 physical nodes, you are not guaranteed an even spread of data amongst those three machines in the cluster.  That is why we recommend 5 as the minimum node count if you are using the default replication factor (N) of 3.\n. Thanks for the report, you indeed are correct.  I'll get this fixed a.s.a.p.\n. Fixed, will be in our beta build.\n. This (I believe) https://github.com/basho/riak/blob/1.4/RELEASE-NOTES.md#sorting-in-non-paginated-queries explains the difference you are seeing.  \nClosing this issue, since you have it filed against the ruby client as well.\n. +1, works for me when I used the yz and kv PR's.\nPlease patch riak_ee with the same change.\n. My intention was a +1 for just this PR.  It printed correctly, so if no one else gives you a kv +1 soon, I'll check that code tomorrow and upgrade my +1 to be all-inclusive.\n. I will format-patch, git am this into basho/riak_ee when it is approved.\n. /cc @gburd @michellep \n. Pushed to riak_ee -> https://github.com/basho/riak_ee/commit/815abc93433abc687c8106835dc975be526d9786\n. Besides what Sean already mentioned this being titled wrong, I actually don't think we should increase this so late in the 2.0 cycle.  If we do increase it, it should possibly be less drastic, moving from 4k to 16k or something before going to 64k which might affect users who can't increase their hard capped system limit.\n. Did this go into riak_ee too?\nOn Monday, May 12, 2014, Eric Redmond notifications@github.com wrote:\n\nMerged #519 https://github.com/basho/riak/pull/519.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/basho/riak/pull/519#event-120240046\n.\n. Reading email from a gas station, don't mind me :)\n\nOn Monday, May 12, 2014, Eric Redmond notifications@github.com wrote:\n\nhold your horses @jaredmorrow https://github.com/jaredmorrow!\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/basho/riak/pull/519#issuecomment-42867893\n.\n. @binarytemple you probably just need to update your /etc/launchd.conf\n\n```\n\u279c  riak git:(jem-source-default-scripts) ./bin/riak start\n\u279c  riak git:(jem-source-default-scripts) ulimit -n\n65536\n\u279c  riak git:(jem-source-default-scripts) launchctl limit\n    cpu         unlimited      unlimited\n    filesize    unlimited      unlimited\n    data        unlimited      unlimited\n    stack       8388608        67104768\n    core        0              unlimited\n    rss         unlimited      unlimited\n    memlock     unlimited      unlimited\n    maxproc     709            1064\n    maxfiles    65536          65536\n\u279c  riak git:(jem-source-default-scripts) cat /etc/launchd.conf\nlimit maxfiles 65536 65536\n```\nAfter you update the file you have to restart before the settings take effect.\n. Friendly reminder to patch this over to riak_ee after merge.\n. This has been reported before and we have plans to fix this in our packaging scripts post-2.0.\nhttps://github.com/basho/node_package/issues/125\n. @siddhuwarrier the plan to push this past 2.0 is canceled since we found some time to fix it for 2.0 beta :)\n. +1 works as expected\n. Thanks, +1\n. We are shipping Riak 2.0 with R16 + our patches for crypto.  It will probably be some time before we support R17 officially.\n. Ubuntu 14.04 was not supported by Riak until 2.0, so there will be no\npackages in the apt repo until 2.0 is at final release.  For now you can\ninstall the beta using the deb and dpkg -i.\nOn Monday, May 5, 2014, James Harrison Fisher notifications@github.com\nwrote:\n\nFollowing the installation instructions herehttp://docs.basho.com/riak/latest/ops/building/installing/debian-ubuntu/fail when installing on Ubuntu 14.04.\nTo reproduce, use the ubuntu/trusty64 image on Vagrant.\nHere's my provisioning script:\ncurl http://apt.basho.com/gpg/basho.apt.key | apt-key add -\nbash -c \"echo deb http://apt.basho.com $(lsb_release -sc) main > /etc/apt/sources.list.d/basho.list\"\napt-get update\napt-get install riak\nTo be explicit about it, here's what's happening:\nvagrant@riak:~$ cat /etc/apt/sources.list.d/basho.list\ndeb http://apt.basho.com trusty main\nvagrant@riak:~$ sudo apt-get update\nIgn http://security.ubuntu.com trusty-security InRelease\nHit http://security.ubuntu.com trusty-security Release.gpg\nHit http://security.ubuntu.com trusty-security Release\nHit http://security.ubuntu.com trusty-security/main Sources\nIgn http://archive.ubuntu.com trusty InRelease\nHit http://security.ubuntu.com trusty-security/universe Sources\nIgn http://apt.basho.com trusty InRelease\nHit http://security.ubuntu.com trusty-security/main amd64 Packages\nHit http://security.ubuntu.com trusty-security/universe amd64 Packages\nIgn http://archive.ubuntu.com trusty-updates InRelease\nHit http://security.ubuntu.com trusty-security/main Translation-en\nIgn http://apt.basho.com trusty Release.gpg\nHit http://security.ubuntu.com trusty-security/universe Translation-en\nHit http://archive.ubuntu.com trusty Release.gpg\nIgn http://apt.basho.com trusty Release\nHit http://archive.ubuntu.com trusty-updates Release.gpg\nHit http://archive.ubuntu.com trusty Release\nIgn http://security.ubuntu.com trusty-security/main Translation-en_US\nIgn http://security.ubuntu.com trusty-security/universe Translation-en_US\nHit http://archive.ubuntu.com trusty-updates Release\nHit http://archive.ubuntu.com trusty/main Sources\nHit http://archive.ubuntu.com trusty/universe Sources\nHit http://archive.ubuntu.com trusty/main amd64 Packages\nHit http://archive.ubuntu.com trusty/universe amd64 Packages\nHit http://archive.ubuntu.com trusty/main Translation-en\nHit http://archive.ubuntu.com trusty/universe Translation-en\nHit http://archive.ubuntu.com trusty-updates/main Sources\nHit http://archive.ubuntu.com trusty-updates/universe Sources\nHit http://archive.ubuntu.com trusty-updates/main amd64 Packages\nErr http://apt.basho.com trusty/main amd64 Packages\n  403  Forbidden\nHit http://archive.ubuntu.com trusty-updates/universe amd64 Packages\nIgn http://apt.basho.com trusty/main Translation-en_US\nIgn http://apt.basho.com trusty/main Translation-en\nHit http://archive.ubuntu.com trusty-updates/main Translation-en\nHit http://archive.ubuntu.com trusty-updates/universe Translation-en\nIgn http://archive.ubuntu.com trusty/main Translation-en_US\nIgn http://archive.ubuntu.com trusty/universe Translation-en_US\nIgn http://archive.ubuntu.com trusty-updates/main Translation-en_US\nIgn http://archive.ubuntu.com trusty-updates/universe Translation-en_US\nW: Failed to fetch http://apt.basho.com/dists/trusty/main/binary-amd64/Packages  403  Forbidden\nE: Some index files failed to download. They have been ignored, or old ones used instead.\nvagrant@riak:~$\nThe important lines being:\nvagrant@riak:~$ cat /etc/apt/sources.list.d/basho.list\ndeb http://apt.basho.com trusty main\nvagrant@riak:~$ sudo apt-get update\nW: Failed to fetch http://apt.basho.com/dists/trusty/main/binary-amd64/Packages  403  Forbidden\nThe 403 error looks real:\n$ curl -v -X HEAD http://apt.basho.com/dists/trusty/main/binary-amd64/Packages\n- About to connect() to apt.basho.com port 80 (#0)\n-   Trying 176.32.101.132... connected\n\nHEAD /dists/trusty/main/binary-amd64/Packages HTTP/1.1\nUser-Agent: curl/7.22.0 (x86_64-pc-linux-gnu) libcurl/7.22.0 OpenSSL/1.0.1 zlib/1.2.3.4 libidn/1.23 librtmp/2.3\nHost: apt.basho.com\nAccept: /\n< HTTP/1.1 403 Forbidden\n  < x-amz-error-code: AccessDenied\n  < x-amz-error-message: Access Denied\n  < x-amz-request-id: 8309EFA1741EE63B\n  < x-amz-id-2: QqNaCHX248WXIJTzKNqR/O5WfHXvPG6clC2PHQtkTNcMGOIiRDmSY1DeIx7GHWeI\n  < Transfer-Encoding: chunked\n  < Date: Mon, 05 May 2014 12:21:42 GMT\n  < Server: AmazonS3\n  <\n\nThe corresponding request for Ubuntu 12.04 returns a 200 which suggests to\nme that it's use of 14.04 which is breaking things:\n$ curl -v -X HEAD http://apt.basho.com/dists/precise/main/binary-amd64/Packages\n- About to connect() to apt.basho.com port 80 (#0)\n-   Trying 176.32.99.45... connected\n\nHEAD /dists/precise/main/binary-amd64/Packages HTTP/1.1\nUser-Agent: curl/7.22.0 (x86_64-pc-linux-gnu) libcurl/7.22.0 OpenSSL/1.0.1 zlib/1.2.3.4 libidn/1.23 librtmp/2.3\nHost: apt.basho.com\nAccept: /\n< HTTP/1.1 200 OK\n  < x-amz-id-2: bI3HreeS34YFRKT0m/ei9i79UtRGf/VXwYtcxnyWTexrKxjhCt9hyNuZmu68KvMd\n  < x-amz-request-id: C0658882B8FED04A\n  < Date: Mon, 05 May 2014 12:22:54 GMT\n  < x-amz-meta-s3cmd-attrs: uid:1000/gname:aptrepo/uname:aptrepo/gid:1000/mode:33188/mtime:1393964106/atime:1393964106/ctime:1393964106\n  < Last-Modified: Tue, 04 Mar 2014 20:15:35 GMT\n  < ETag: \"e3897fb07600a409f17cf1e78f583b9b\"\n  < Content-Type: binary/octet-stream\n  < Content-Length: 17949\n  < Server: AmazonS3\n  <\n\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/basho/riak/issues/539\n.\n. The download for 14.04 is on the downloads page http://docs.basho.com/riak/latest/downloads/   Just download the file and use dpkg -i to install it as with the other versions.  The install docs just didn't get updated for the new version that's all.\n. @see0 sorry, there is a PR out right now to update the docs.  We moved from our own apt/yum repos to use packagecloud.io: https://packagecloud.io/basho/riak/\n\nFor ubuntu 14.04, you can look here: https://packagecloud.io/basho/riak/riak_2.0.0-1_amd64.deb?distro=trusty for the install script\nOr you can use one of the automated scripts found here: https://packagecloud.io/basho/riak/install\nSorry for the issue, the docs will be finished this week.\n. We will not change any of our scripts to /bin/bash that we ship with the product because that adds an unnecessary dependency on installing bash on those platforms that do not ship with it by default.  These include Solaris, FreeBSD, OpenBSD, and SmartOS.\nWe will fix the issue with the actual line as there should be no bashisms in the script.\n. The following is the issue I get on riak-debug:\n$ checkbashisms riak-debug   \npossible bashism in riak-debug line 115 (<<< here string):\nread riak_app_config riak_vm_args <<< `\"$riak_bin_dir\"/riak config generate | cut -d' ' -f 3,5`\nThis use of read as a separator should be replaced with two calls to cut -d' ' so continue to work on in sh\n. If you are trying to build develop, right now we are shipping a R16 with patches from R17 included.  You can build erlang from the following tarball http://s3.amazonaws.com/downloads.basho.com/erlang/otp_src_R16B02-basho5.tar.gz\n as you normally would.\nEvery one of the patches have already been accepted by the OTP team.  So there will be forward compatibility when we move to R17 in the future.\nSorry for your issues.\n. +1, thanks @richierichrawr \n. I have tested this on a Solaris devrel and it produces an useable tarball.  Keep in mind some functions are still broken as mentioned in the PR.\nbuild-solaris-10u9-64.(~/dev/riak/rel/riak) ls -lah riak\\@127.0.0.1-riak-debug.tar.gz\n-rw-r--r--   1 buildbot buildbot     17K Jun 10 17:11 riak@127.0.0.1-riak-debug.tar.gz\nbuild-solaris-10u9-64.(~/dev/riak/rel/riak) uname -a\nSunOS build-solaris-10u9-64 5.10 Generic_142910-17 i86pc i386 i86pc\n. Merged to riak_ee as well.\n. So the only problem with this is there will always be a lock-deps file.  So even if we don't make a new one and commit it, it will manage to find one.  Maybe we can solve it with just passing in a flag like concrete does it.\n. Okay, yes, going forward there will always be one in the repo.  But maybe\nthat makes sense to always build packages from the last lock-deps.\nOn Tuesday, June 17, 2014, Andrew Thompson notifications@github.com wrote:\n\nThere is not always a lock-deps file, 1.4 branch does not have one:\nhttps://github.com/basho/riak/tree/1.4\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/basho/riak/issues/560#issuecomment-46381073.\n. I put 2.0.1 because it is something reasonable to think about and do to make it easier for people.\n. Honestly the easy solution is just removing that ping before riak start.  We don't want to add the bootstrapd function call to ping then you wouldn't be able to ping from a non-root or non-riak user.\n. @hectcastro I think removing that ping and just trying a start if it is already started is a reasonable method if we just plan to catch it.  Thoughts?\n. See https://github.com/basho/node_package/pull/145\n. Fixed in https://github.com/basho/node_package/pull/146\n. Why was this changed here? We already have the notion of devnodes\noverridden in our riak_test scripts.\n\nOn Friday, June 20, 2014, Andrew J. Stone notifications@github.com wrote:\n\nSome SC tests require 8 nodes. Therefore raise the default to prevent\nspurious errors.\nYou can merge this Pull Request by running\ngit pull https://github.com/basho/riak feature/eight-devnodes\nOr view, comment on, or merge it at:\nhttps://github.com/basho/riak/pull/562\nCommit Summary\n- Change DEVNODES=5 to DEVNODES=8 in Makefile\nFile Changes\n- M Makefile https://github.com/basho/riak/pull/562/files#diff-0\n  (2)\nPatch Links:\n- https://github.com/basho/riak/pull/562.patch\n- https://github.com/basho/riak/pull/562.diff\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/basho/riak/pull/562.\n. That's fine, I was just curious.  The scripts in riak_test do call 'make\ndevrel' if you are running them to bootstrap, but you are correct that the\ncurrent devrel stuff does not.\n\nOn Friday, June 20, 2014, Andrew J. Stone notifications@github.com wrote:\n\n@jaredmorrow https://github.com/jaredmorrow AFAIK the riak_test scripts\ndon't build the devrels required though, they just copy them. The error\nthat occurs with less nodes is also cryptic. Can't write app.config\nessentially. This was actually a suggestion from @kellymclaughlin\nhttps://github.com/kellymclaughlin after I wasted some time on this\ntoday. I can revert it, but I'd like to prevent others from running into\nthis issue another way if we do that.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/basho/riak/pull/562#issuecomment-46703965.\n. This leaves us in a bit of a pickle, using 'service' strips all env, and Solaris doesn't support su -m.  We might be too deep to fix this for RC.  I'll try to think of the least risky fix for this, since at this point we can't just punt on Solaris.\n. That's what I was initially thinking.  We source it from the init.d script,\nbut clearly that is no longer effective.\n\nOn Mon, Jun 23, 2014 at 2:17 PM, Jon Glick notifications@github.com wrote:\n\nalternately /usr/sbin/riak could source /etc/default/riak. Feels like a\nbit of a hack though, not sure that's the behavior we want.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/basho/riak/issues/563#issuecomment-46896151.\n. Something needs to still happen, I just needed to come back to it.  I'll take a look again at it tomorrow.\n. Will close issue when it is confirmed fixed in the RC packages.\n. +1, can you port this to basho/riak_ee as well?\n. +1, please can you cp this over to riak_ee / 2.0 after this merges?  The files should be identical.\n. +1, please merge to riak_ee at your leisure\n. Only thing I can think of is something getting left out of the toplevel reltool.config and modules aren't getting added to the rel as expected, but if core is doing the right thing, that shouldn't be a problem.\n. FYI, so this won't affect 2.0 right now, since that node_package PR wasn't included.\n. +1\n. If this was built from source, that setting doesn't get set.  This only gets set when packaged with 'make package'.\n\nAll builds from source will report an empty riak version string.\n. If you are doing make package, both Riak and RiakCS will get their own\ncopy of the VM and have their own libraries.  So they will not share the\nsame VM or runner scripts.\nOn Mon, Sep 22, 2014 at 12:04 PM, Shannon Coen notifications@github.com\nwrote:\n\nAlso, even if make package is used, how could one environment variable\nAPP_VERSION provide the version of both Riak and Riak CS? Latest\ncompatible versions are Riak 1.4.10 and Riak CS 1.5.0. Even if APP_VERSION\nwere set, it would be incorrect for one of these (Riak and Riak CS run on\nthe same VMs).\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/basho/riak/issues/599#issuecomment-56413910.\n. All erlang apps built with reltool build an erlang 'rel' which includes a small copy of the VM for the app to run on.  Making a proper self-contained package for each will ensure you can have different versions of libraries for each app, as well as possibly different Erlang VM versions.  This is a very common thing to do in the Erlang world http://www.erlang.org/doc/design_principles/release_structure.html\n\nAs far as how to get the Riak version if you don't have APP_VERSION set, the easiest way I can think of is to just use riak-admin status and parse the riak_kv_version.  I quickly did this and it worked fine from source \n./bin/riak-admin status | grep riak_kv_version | sed -e 's/.*<<\"\\(.*\\)\">>/\\1/'\n2.0.0\n. I'm closing this as this is part of the design and not at this time considered a bug.\n. Well when you build from source you can always just add the 'git describe --tags' version to the runner script as part of your build.\n. This is actually not unusual for Major.0.X releases.  At some point along the way we decided that it might not be worth constantly going from tags in Major.0.0 to branches in to continue development and then back to tags for Major.0.X.  In this case, 2.0.1 -> 2.0.4 used the rebar.config.lock file to move dependencies.\nIf you:\ngit checkout riak-2.0.1\nmake locked-deps\nThen do a rebar list-deps while passing in the lock config, you'll get:\n```\n$ ./rebar list-deps -C rebar.config.lock | grep REV | sort | uniq | awk '{printf(\"%s %s %s\\n\", $1, $3, $4)}'\nbasho_stats 19c532af235ae675439d491b329c55c2f9b02deb git://github.com/basho/basho_stats.git\nbear da820a13c607c3f816ee8b83c587266da5389761 git://github.com/basho/bear.git\nbitcask 2be3c41429fc04e34a57dc052d4fcc57b00e550a git://github.com/basho/bitcask.git\ncanola 9bdfee88fce20b3a01b7003696b53eb21913d6fb git://github.com/basho/canola.git\ncluster_info 1fae829ea639c8fcfeeb080aad02da01ff49b218 git://github.com/basho/cluster_info.git\ncuttlefish b2ce0b15687f97259031f9f42e25ae52cdee3bab git://github.com/basho/cuttlefish.git\nedown 4cb4ad67382590d92228ea6c56225376bd9f5685 git://github.com/esl/edown.git\neleveldb 6e0ff7292bf1d1f2ab20a4e9dacb100ba8c0290b git://github.com/basho/eleveldb.git\neper 7222ecaebceb5422e74a9c1503043bbc6036f6b7 git://github.com/basho/eper.git\nerlang_js 07467d899ab90a2b719ad19ab0be0048c1c8d873 git://github.com/basho/erlang_js.git\nerlydtl d20b53f04837a1053ed18987f645cb60eae82453 git://github.com/evanmiller/erlydtl.git\neunit_formatters dab4291d191f0ca40c911179fd195c3942261667 git://github.com/seancribbs/eunit_formatters\nfolsom e80bd013c8ea906c4f14cba38b92e95116415e23 git://github.com/basho/folsom.git\ngetopt 659a28f4145bc9843598972854299dc4ea77e4cb git://github.com/jcomellas/getopt.git\ngoldrush 71e63212f12c25827e0c1b4198d37d5d018a7fec git://github.com/DeadZen/goldrush.git\nibrowse 9b8b042fb7d37b90994ef6b6615c92dd2769328e git://github.com/cmullaparthi/ibrowse.git\nkvc 75c4e76468cb2bd03067a9c349989ee914a1b581 git://github.com/etrepum/kvc.git\nlager b6b6cebcb27ccff8acc59ae775acebc2f52e4926 git://github.com/basho/lager.git\nlager_syslog fa2e7e3daee0d0a59dadb820fd3381eac4a65770 git://github.com/basho/lager_syslog.git\nmeck 2b25a30a8688f94106d07f23a9f1fb523ac00f08 git://github.com/basho/meck.git\nmerge_index b701dde5c28956c3b629411e5ff7e50cbb5cb4b3 git://github.com/basho/merge_index.git\nmochiweb 5e6e3e688dc9987a97e261431256e29671555c0b git://github.com/basho/mochiweb.git\nneotoma 760928ec8870da02eb11bccb501e2700925d06c6 git://github.com/seancribbs/neotoma.git\nnode_package a56fe9b021e3543a24fdfc98f1c16704b41e62c6 git://github.com/basho/node_package.git\npbkdf2 7076584f5377e98600a7e2cb81980b2992fb2f71 git://github.com/basho/erlang-pbkdf2.git\npoolboy 84d836ab49da618d1e3fb3947207bf9b7fcea335 git://github.com/basho/poolboy.git\nprotobuffs 5257dfe4e000b58487af89b849c7336ffa9bae82 git://github.com/basho/erlang_protobuffs.git\nrebar_lock_deps_plugin 9711549b8a84b065eb2edc22f8eb6ff85e3c94e8 git://github.com/seth/rebar_lock_deps_plugin.git\nriak_api 7c2b5eec21487044ca71c79e983b5262a110f48b git://github.com/basho/riak_api.git\nriak_auth_mods 9ae39fe562fd61a14f3406247a5fda1b2a806576 git://github.com/basho/riak_auth_mods.git\nriak_control adc50dcd5aed26ecb0a19b3c5c3bfc9d28c663b7 git://github.com/basho/riak_control.git\nriak_core 55d26761ffc8725296bbbc5c4fbc71e9143ff6c8 git://github.com/basho/riak_core.git\nriak_dt c06820d95d2ee88dfc4db6c459d2f3739c204ed2 git://github.com/basho/riak_dt.git\nriak_ensemble d66a102bb2bfc17a2697190e92dc9173475848ef git://github.com/basho/riak_ensemble\nriak_kv 547f0e01c2f41240287f83ac74dc4148bc7f780b git://github.com/basho/riak_kv.git\nriak_pb fc18a9b37170431f659e08d66ed78e3f6bbc47ca git://github.com/basho/riak_pb.git\nriak_pipe e838b600a4db934361fcefca5025aa4cd75071fa git://github.com/basho/riak_pipe.git\nriak_search 2d69b2b78a472f6996be39543e33a57d97060b33 git://github.com/basho/riak_search.git\nriak_sysmon 75b00aeded9f4e432137fe9575b0eea3baa2202a git://github.com/basho/riak_sysmon.git\nriaknostic 4e7daa5c27cc6bd8cfb4a1c5f19a6e8ac2a8e344 git://github.com/basho/riaknostic.git\nsext f53f1bd37c29e45dc788a5d8b4512c66511d4cb5 git://github.com/basho/sext.git\nsidejob c5aabba2d7daa80c340e110902bbcfcb552ccdcf git://github.com/basho/sidejob.git\nsyslog 918c9b453e0811b24f2c99b35b712b0ef9f29c7e git://github.com/Vagabond/erlang-syslog.git\nwebmachine e5f82336bb7a4552dd33e02bf8f66e67547f13be git://github.com/basho/webmachine.git\nyokozuna 6ac3a637c429a0a5be6a009b2532d3179cf4515f git://github.com/basho/yokozuna.git\n```\nWhich is correct.\n. Good point.  The fix in this case might be to just add the 'locked-all' target as the default target in major release branches.\n. This won't work for some packages because they callback into the makefile when they build.  You can fix it pretty easily, but will need to test it on all the platforms.\nSo checking in node_package to do due diligence, RPM/Deb/OSX are just fine, they call back into 'make rel' only (but you'll need to make sure that doesn't force a new deps requirement).\nThe following need to be changed to call only make rel:\nhttps://github.com/basho/node_package/blob/develop/priv/templates/fbsd/Makefile#L156\nhttps://github.com/basho/node_package/blob/develop/priv/templates/fbsdng/Makefile#L91\nhttps://github.com/basho/node_package/blob/develop/priv/templates/solaris/Makefile#L29\nhttps://github.com/basho/node_package/blob/develop/priv/templates/smartos/Makefile#L246\nI'll leave testing my theory as an exercise to the reader ;)\n. No problem, that would've been a really hard bug to track down.\nOn Tuesday, February 24, 2015, Greg Cymbalski notifications@github.com\nwrote:\n\nVery cool, I was wondering if node_package would need tweaks. Thanks for\nthe heads-up :)\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/basho/riak/pull/693#issuecomment-75882020.\n. That stuff is already at the top, looks like a bad merge/diff to me.\n. Might want to check /etc/security/limits.d and grab everything in it if it exists.\n. It would be nice to have a comment block above this section.  It is not clear what the goal is for all this mayhem outside the context of the PR.\n. As both a company who is trying to make money and as good stewards to the OSS community I think keeping as much compatibility as possible is a good goal to have.  If R14 is broken with new code and it is hard for us to maintain going forward, then I'm fine to remove it.  If it is just removing it for the sake of cleanup, then I don't feel that's a great reason.  If someone is hitting this line of code, that means they are building from source and already outside of what we claim to support in a production sense, so I don't think that is an issue.\n. FYI: I was concerned about this line on non-posix shells, so I tested in sh and ksh in Solaris and sh in SmartOS and it worked fine.\n. \n",
    "PharkMillups": "Closable? \n. Thanks, Morten! We'll check it out when we have a moment. \nMark \n. I'm not sure why, but it looks like I closed this issue. I don't recall doing it. Regardless, it was done in error. Reopening so that someone more capable than I can close it for the right reasons. \n. We're working on better out-of-the-box support for R15B01. Thanks for using the work-around for now. \n. @gordonad It looks like the problem is Clang. Riak needs GCC. Any chance you can take another pass at the install with that?\n. @gordonad Any resolution on this? \n. I'm going to close this for now. If it rears its head again on 1.2, let us know. \n. Thanks for catching that. I'm not sure how we overlooked it for so long. I'll take a stab at rewriting the copy tomorrow and shoot the commit your way for approval. \n\n... which has lead to at least one fairly important (but apparently not that important since there is no data loss) bug being \nunattended since November 2011\n\nWhich bug are you talking about?\n. I just updated the bug filing info on the wiki. Also, I migrated the JS Built-In issue over to GitHub. It's still in need of resolution, but it'll get some more love on the GitHub repo. \n. +1. Thanks for the fix. \n. Keep em' coming. :) \n. +1. Thanks for the fix \n. @DaMorpheuS Is this still an issue?  Or can we go ahead and close this?\n. Hello there, \nA quick glance at this makes me think it's related to R15B03 and/or your version of OS X (both of which I don't think we've tested against extensively yet). \nAt any rate, this isn't a bug (yet), so it'll be easier to get it resolved on the Riak Mailing List. When you have five minutes, subscribe and post it over there. I'm going to close this for now. Free to reopen if you think it's warranted. \n. Even Jordan had off days. \n. ",
    "kellymclaughlin": "+1 to merge.\n. +1 to merge\n. Changed the backend_status command to the more general vnode-status. vnode-status retrieves the local vnodes instead of using coverage to get the status of the cluster vnodes.\n. +1 to merge.\n. +1 to merge\n. These changes look good to me,  +1.\n. The other problem is that if they simply use the old version of the config then riak fails to start at all with ugly errors. I made a comment about more helpful error messages in that regard on the riak_kv pr. In the interest of backward compatibility maybe riak_api should also check for the existence of the properties in the riak_core section and use those in the absence of riak_api properties in addition to some friendly lager reminders to upgrade the code. Then a couple of versions from now we can take out the riak_core check.\n. riak: https://github.com/basho/riak/pull/196\nriak_ee: https://github.com/basho/riak_ee/pull/80\n. Related riak_kv pr is here.\n. I think it was a similar, but different item that was in the luke removal. I've added a pr here to mirror this change in riak_ee. \n. Eleveldb write thread pool\n. In the section describing the config option on the riak page there is a misspelling, configration, at the end of the description of the effective sub-option.\n. Also in the config section in the description of the describe sub-option: descrption\n. On the riak-admin page under the security sub-commands section, the description of the ciphers option uses ciphers as the plural and cypher as the singular. Why not just cipher? Not a big deal, just noticed the difference and was curious.\n. Do you think we should we add some sort of disclaimer to the backup and restore command descriptions saying \"Do not use! These do not work! DANGER, DANGER!\"?\n. On the riak page the description of attach uses existed instead of exited: ...which can be existed more safely\n. :+1: \n. PR for riak_ee: https://github.com/basho/riak_ee/pull/250\n. :+1: \n. I have reproduced the problem and it can be attributed to the merging of https://github.com/basho/node_package/pull/155. There are several calls in riak_core_console that do not need to accept any input parameters, but accept just the empty list due to the exact limitation described in the node_package PR. I'll go through and convert all those console functions appropriately.\n. After looking more into this and thinking about what it would take to make this work in mixed cluster scenarios up until the point where people are no longer upgrading to some future version from 2.0 (i.e. a few years down the road) I am not sure it is worth the effort. I think I will just open a PR to revert my node_package change and handle the no parameter rpc case for the specific scenrio in riak_cs I was working on the same way it is handled in riak-admin. \n. https://github.com/basho/node_package/pull/156\n. I thought maybe I was being too verbose. I'll add it back.\n. Just a reminder to make sure this get set back to master before merge.\n. May want to rebase before merging to avoid a conflict since this has been updated on master to R14B0[234]|R15.\n. ",
    "ncb000gt": "I built this on a different machine without my change and noticed riak_pipe already being pulled in by the appropriate sub dependencies. I guess my other machine was out of sync or something else was going on. Either way I don't think this is needed.\n. ",
    "jkirkell": "thanks\n. ",
    "msiebuhr": "BTW: This should fix bug 1130.\nRelated: I've been looking for Riak-Search's ditto file (it also misses the sudo-dependency), but I can't seem to find it.\n. Hi Mark & al,\nJust a quick reminder on this (fairly modest) patch, which I guess has been lost on the recent 1.0-work.\nJust for good measure; the patch applies cleanly against the riak (currently 16e0f31a203491ef1b8084bffbece92fa724a7d7).\n. ",
    "eriksoe": "Better now?\n. Hm, apparently not. Thought I had pushed a fix, but in that case it has disappeared.\nTry again now from 'bz1197_riak_attach_loses_stdin_data-II', please.\n. ",
    "reiddraper": "+1\n. make rel and make stage fail right now with\n{\"init terminating in do_boot\",\"Release riak uses non existing application skerl\"}\nI think it's because skerl is still listed in reltool.config, but it's no longer available (it was listed as a dep in luwak). Removing them seems to let Riak compile correctly.\n. +1\n. @benmmurphy do you have a proof-of-concept of the attack? I tried to create one myself, but had trouble since Riak doesn't support CORS, ie. Access-Control-Allow-Origin: *. To be clear too, you're talking about a vulnerability for users (developers) who are running Riak locally, right?\n. @benmmurphy great, thanks\n. If we go with this, let's also update Riak Enterprise, Riak CS, Stanchion and any other projects which have an attach feature.\n. Just use bucket nothing-to-see-here-move-along-this-could-not-possibly-collide-with-a-user-bucket\n. Please file this issue against basho/riak-php-client. Thanks.\n. Can you work some magic to give the commit a description too ;)?\n. cc @engel. Wondering if this is because we changed 2i to not be sorted except when using pagination. @wcamarao what version of Riak are you using?\n. +1, these are great\n. I feel like a celebrity!!!!\n. @coderoshi I dig the 'Also merge into EE' reminder. We should start adopting that until we've rolled them into the same repo.\n. @portfelioorg keep in mind too that this time includes the SSL handshake, which can be quite time-consuming and expensive. Do you have any results/tests that show multiple requests with a keep-alive socket?\n. +1\n. @jaredmorrow any thoughts on a milestone for this issue?\n. ",
    "rustyio": "+1 merge\n. ",
    "zaa": "Works ok for us on \nuname -mrs\nFreeBSD 8.1-RELEASE i386\n. ",
    "jj1bdx": "At least about the shebang script, it looks OK to me.\n(I changed it to /usr/local/bin/bash when I tested, but this is not recommended because for FreeBSD bash is only a Port and not with the OS distribution.) \n. For FreeBSD and other BSD-derived OSes, bash is not installed as a part of the distributions. So shebang of #!/usr/bin/env bash does not work anyway.\nIf there were anything critical in the script needed something only bash could do, the usage of bash would be justified, but I think that's not the case for this pull-request. \n. I think adding a bash dependency to the FreeBSD Port Makefile does not solve the fundamental issues of bash dependencies,  though it is surely a workaround for the time being. The question is whether we have to require Riak to install bash or not. And not all people want to install Riak from the source code.\nFYI, on FreeBSD Port for Erlang (lang/erlang), bash is not a requirement. And I think it shouldn't be. \n. Will do on branching from The basho organization with my @jj1bdx access rights.\nI've tested locally on FreeBSD 9.1R amd64 and Ubuntu 12.10 x86_64.\n. Note: I removed the original fork at jj1bdx/riak because all necessary commits are copied here.\n. After the rollback on https://github.com/basho/riak_core/commit/161816de8cb690a9a958ae7b661aeb9152b2b9bf I confirmed now Riak started normally by riak start.\n. Duplicate to https://github.com/basho/riak/issues/280\n. @ksauzz Yokozuna will NOT be enable WITHOUT this app.config modification, right?\nTo enable yokozuna, you need to set {enabled, true}. Correct me if I'm wrong.\n. @ksauzz So this modification adds the flag for enabling the yokozuna functionality, but the flag itself is set to false, to prevent yokozuna to be accidentally enabled. Correct me if I'm wrong.\n. @ksauzz: +1. This patch is also applicable to the branch yz-merge-master (with 1 fuzz). A brief test here on OSX 10.8.3 with R15B03-1, the enabled flag worked as intended when changing the settings between true and false.\n(BTW comment on wording: reveal -> disclose)\n@rzezeski - Could you take a look at this?\n. Canola (a PAM authenticator module) at https://github.com/basho/canola should be included in the Work In Progress list.\n. Please do note FreeBSD Port has its own leveldb (/usr/ports/databases/leveldb) 1.15.0.  Also, the default Port gcc is now 4.7.3 (port devel/gcc47).\n. I suggest Riak's leveldb to be renamed to avoid confusion.\n. I hope clang issue will be solved on the next Riak release after 2.0.\n. Something like this, especially for OS X, should be suggested (Worked on OS X Mavericks 10.9.2)\n```\n/etc/sysctl.conf\nkern.maxfiles=131072\n/etc/launchd.conf\nlimit maxfiles 65536 131072\n```\nSee also http://apple.stackexchange.com/questions/32235/how-to-properly-increase-ulimit-n-on-lion\n. Yeah the issue subject is misleading.\n. So far I only see \"indexes\" too, though I personally prefer the former. No objection on sticking to \"indexes\" either.\n. ",
    "sc68cal": "What about using env to locate bash - that is typically how to ensure portability.\n. True, but the Makefile for the port can just add\nRUN_DEPENDS=    bash:${PORTSDIR}/shells/bash\n. :beer:\n. I'll give this a shot on my -CURRENT box and report back.\nI have a little bit of experience writing startup scripts for rc.conf(5) - so I may be able to help with that.\n. ",
    "koobs": "Confirming a successful build and test using the following port Makefile and patch: https://gist.github.com/ee3563208933cd8e97a6 \nNote the post-build: target, which blanket replaces /bin/bash references with /bin/sh refs with no other modifications.\n./riak console output: http://codepad.org/Co9W7spF\nAll ./riak and ./riak-admin commands seem to work.\nThe patch in the gist above is for code already committed but didn't make it into 1.1.2 so shouldn't be required in the port for long\nPort is based on draco2002 original work at https://github.com/draco2003/freebsd-database-riak\nOnly work left is to add a pkg_plist, install /rel/* bits to appropriate system locations and get an RC script in.\n. ",
    "drnic": "Do we need to update the copy of rebar for this to work?\nThe one that is in riak-1.1.4 is from Oct 2010.\n```\n./rebar -V\nrebar version: 2 date: 20111027_031525 vcs: git 9197e70\ngit show 9197e70\ncommit 9197e70bd7f442576eb76fab4011fa36b8739960\nAuthor: Tuncer Ayaz tuncer.ayaz@gmail.com\nDate:   Wed Oct 26 23:23:07 2011 +0200\n```\nriak-1.1.4 tar.gz is filled with vsn.git files in the deps, but I think ./rebar get-deps is still trying to run git commands?\nmake: git: Command not found\n./rebar get-deps\nerlexec: HOME must be set\nmake: *** [deps] Error 1\n. The instructions say \"make rel\" which is what I'm doing.\nI'll try compiling a new rebar and see if it helps, and report back.\n. With rebar 3 I get this error during make rel:\nmake[3]: Entering directory `/var/vcap/data/compile/riak/riak-1.1.4/deps/erlang_js/c_src/nsprpub'\ncd config; make -j1 export\nmake[4]: Entering directory `/var/vcap/data/compile/riak/riak-1.1.4/deps/erlang_js/c_src/nsprpub/config'\ncc -m32 -o now.o -c      -Wall -O2 -fPIC  -UDEBUG  -DNDEBUG=1 -DHAVE_VISIBILITY_HIDDEN_ATTRIBUTE=1 -DHAVE_VISIBILITY_PRAGMA=1 -DXP_UNIX=1 -D_GNU_SOURCE=1 -DHAVE_FCNTL_FILE_LOCKING=1 -DLINUX=1 -Di386=1 -D_REENTRANT=1  -DFORCE_PR_LOG -D_PR_PTHREADS -UHAVE_CVAR_BUILT_ON_SEM   now.c\nIn file included from /usr/include/features.h:378,\n                 from /usr/include/stdio.h:28,\n                 from now.c:38:\n/usr/include/gnu/stubs.h:7:27: error: gnu/stubs-32.h: No such file or directory\n. @jaredmorrow I don't understand what you mean \"Calling make [is bad and don't do it]. Follow the installation page...\" which includes make:\ncurl -O http://downloads.basho.com/riak/CURRENT/riak-1.1.4.tar.gz\ntar zxvf riak-1.1.4.tar.gz\ncd riak-1.1.4\nmake rel\n. Going to try just make compile\n. Bah.\n+ set -e\n+ set -u\n+ set +x\nmake: git: Command not found\n./rebar compile\nerlexec: HOME must be set\n. Ok, when I add a missing export HOME=/home/vcap installation of make compile goes smoothly until:\nc_src/build_deps.sh: line 40: git: command not found\nERROR: Command [compile] failed!\nmake: *** [compile] Error 1\nLast part of the output is at https://gist.github.com/7dffae4d7185f2ba99f3\n. This error comes from deps/eleveldb/c_src/build_deps.sh:\nbash\n        if [ ! -d leveldb ]; then\n            git clone git://github.com/basho/leveldb\n            (cd leveldb && git checkout $LEVELDB_VSN)\n        fi\n. Link into eleveldb source https://github.com/basho/eleveldb/blob/master/c_src/build_deps.sh#L51-54\n. Do we need to bundle leveldb into riak's tarball? or into snappy's taball?\n. Yay! Thanks @jaredmorrow you are a champ. I'm back to using make rel now. Awesome.\nFor any BOSH users, the packaging script I am using looks like:\n``` bash\n!/usr/bin/env bash\nThis creates 5 copies of riak/riak-admin etc\n- /var/vcap/packages/riak/rel/bin/riak\n- /var/vcap/packages/riak/dev1/bin/riak\n- /var/vcap/packages/riak/dev2/bin/riak\n- /var/vcap/packages/riak/dev3/bin/riak\n- /var/vcap/packages/riak/dev4/bin/riak\nset -e # exit immediately if a simple command exits with a non-zero status\nset -u # report the usage of uninitialized variables\nset +x\nRIAK_VERSION=1.1.4\nexport HOME=/var/vcap\nPATH=/var/vcap/packages/erlang/bin:$PATH\ntar xzf riak/riak-${RIAK_VERSION}.tar.gz\ncd riak-${RIAK_VERSION}\nriak-1.1.4 failed to contain leveldb\nhttps://github.com/basho/riak/issues/191\ncd deps/eleveldb/c_src/\nunzip ${BOSH_COMPILE_TARGET}/leveldb/leveldb-14478f17.zip\nmv basho-leveldb-14478f1 leveldb\ncd ${BOSH_COMPILE_TARGET}/riak-${RIAK_VERSION}\nmake rel/riak\nmake rel\noutput is in rel/riak\ncp -prv rel/riak ${BOSH_INSTALL_TARGET}/rel\nhttps://wiki.basho.com/Building-a-Development-Environment.html\nmake 4 copies of riak:\n- dev/dev1\n- dev/dev2\n- dev/dev3\n- dev/dev4\nmake devrel\noutput is in dev1/riak, dev2/riak, dev3/riak, dev4/riak\ncp -prv dev/ ${BOSH_INSTALL_TARGET}\n. ah, sorry, didn't mean to close\n. Was this fixed in 1.2.0?\n.\n==> eleveldb (get-deps)\nc_src/build_deps.sh: 81: git: not found\nERROR: Command ['get-deps'] failed!\nmake: ** [deps] Error 1\n```\nWhat leveldb is required for riak 1.2.0? I tried it with one above but got an error during installation.\n. Yep, that's the SHA I used I think. Thanks!\n. ",
    "bflueras": "Hi,\nI've got the same pb. Any hints other than manually installing?\nOS: Linux version 2.6.32-279.el6.x86_64 (mockbuild@sl6.fnal.gov) (gcc version 4.4.6 20110731 (Red Hat 4.4.6-3) (GCC) ) #1 SMP Thu Jun 21 07:08:44 CDT 2012\nuname -a\nLinux machine 2.6.32-279.el6.x86_64 #1 SMP Thu Jun 21 07:08:44 CDT 2012 x86_64 x86_64 x86_64 GNU/Linux\nriak version\nriak (1.2.1 2012-10-16) RedHat x86_64\nopenssl version #latest for my OS\nOpenSSL 1.0.0-fips 29 Mar 2010\ncat /var/log/\n2013-02-06 12:36:45.527 [error] <0.1602.0> Unable to load crypto library. Failed with error:\n\"load_failed, Failed to load NIF library /usr/lib64/riak/lib/crypto-2.1/priv/lib/crypto: 'libcrypto.so.6: cannot open shared object file: No such file or directory'\"\nOpenSSL might not be installed on this system.\n. Hi Jared,\nI reinstalled riak with yum and the problem was solved.\nThanks, \n. ",
    "RobertLowe": "@Vagabond \nSorry for the wait, yup, that was the issue. \nI'll wrap it with some kind of lock.\nThanks a bunch!\n. ",
    "krestenkrab": "It seems that everyone starting out with Erlang has this issue at some point.  Why not provide a more descriptive error message somehow?\n. @uwiger, where can I find an example of how to simply dump all stats to a graphite?\n. @jburwell thanks! /cc @rsltrifork @lixen\n. ",
    "loxs": "I can confirm that\n. ",
    "kamidev": "These four files contain regexes that assume R14 is the highest OTP version.\nriak/rebar.config\nriak/deps/cluster_info/rebar.config\nriak/deps/riak_kv/rebar.config\nriak/deps/riak_search/rebar.config\nFixing this makes everything compile under OS X Lion. The README sample code and basic tutorial stuff appear to work, too. \n. ",
    "spaquet": "Perfect.\nI confirm that make all, make rel and mak devrel are working after this patch.\nThis would be great if included in next src or git release of risk.\n. Great\n. Found.\nThe issue is OS X not Riak.\nThe point was: my user directory was affected to 501:staff and not to username:staff. After proper chown the issue just got solved.\nMy guess is that erlang needs some hidden directories at the root of the home folder and since 501 was not my user it was not possible to have them created, leading erlang to crash without notice.\n. ",
    "bsparrow435": "Tested on Ubuntu and OSX. +1 to merge.\n. PR's welcome.\n. I believe after 2.0 the plan is to deprecate this as well as riak restart but the method for doing so has not yet been decided. Your proposed solution sounds great, if you have time to implement that would be great. Otherwise this will wait until a point release after 2.0 at the earliest i'd assume.\n. https://github.com/basho/riak/pull/194\nLooking for comments.\n. Merging, opened issues against riaknostic for README and riak_test for riaknostic_rt.erl changes. Merging.\n. @slfritchie no, it does not. I chatted with Evan about it last night and we are going to punt on this and slate it for a point release after 2.0. The customer who requested this feature is currently running this functionality as a patch and has no current plans to upgrade to 2.0.\n. Hi @westhand this is a known issue. It has been resolved by removing sysctl checks from Riaknostic but can be resolved in your environment by adding /sbin to PATH.\n. The easiest change would probably be sudo ln -s /sbin/sysctl /bin/. Anytime!\n. Yep. I'll have a look later today.\n. Currently riak-debug uses the eper utility to gather all the configuration items it needs for work. This requires an erlang app.config. I'd like to remove this utility and instead use the return of riak config effective and /awk '/SETTING_NAME/ {print $3}' to return whatever values we need. Will work on that tomorrow.\n. Yep, that i was PR457 is doing. I added the platform_data_dir to the script and am then going to that dir/generated.configs and sorting for the newest file which is an app.config. I can then feed this into eper like normal.  I need to test the change across other vm's/packages but if this is OK then this issue should be fixed.\n. I'm a bit wary of cluster-info being run by default as it's pretty heavy handed.. but I have not seen it take down a node before so i'm fine with this change. :+1: \n. Good point @kuenishi, I agree. Otherwise, collecting a riak-debug from each node in the cluster will generate a lot of unnecessary load.\n. #455 \n. @conradwt This setting will provide a performance boost regardless of backend as Riak does a lot of reading/writing of files on disk and noatime makes these operations more efficient.\nThe warning is coming from the riaknostic package which periodically warns against common configuration settings that can effect performance and data integrity. I highly recommend using this setting if you are doing any sort of performance or load testing. If you are simply developing against riak and don't currently care about performance, then it does not matter.\n. :+1: \n. I just confirmed this locally against develop. The fix is valid but we really should have a riak_test to validate this.\n. +1 CSE\n. Adding more enhancements:\n- riak-cs logs\n- stanchion logs\n- flag for riak-cs access logs\n. Closing, covered in #522 \n. Doesnt really matter but this should be filed against https://github.com/basho/riak_ee-issues\n. +1\n. :+1: \n. :+1: \n. Squash down to one commit and I'll merge.\n. :+1:\n. Squash down to one commit and I'll merge.\n. Going to need to fix this without changing nodetool to use aliases as this will break scripts used by other projects who also use nodetool.. :+1: ec37626f88ef3a5bd43e22d1a81abd1da3c943ac\n. Must also check to make sure node is down.\n. nice catch, this would have been really annoying to fix later. +1\n. @JeetKunDoug . +1. +1. Yes, the reason i did not was to make the awk script as simple as possible. The .conf can have(or not have) spaces before and after the \"=\". Calling riak config effective guarantees a format where the 3rd element on the line will always be only the value.\n. ",
    "ghost": "That's why I call shift to globber the first argument, and pass in $@ for the rest of the arguments.\n```\n./bin/riak escript ~/src/basho/basho-escripts/bitcask_dump_keys.escript \\\ndata/bitcask/1118962191081472546749696200048404186924073353216 \\\n/tmp/\nOpening bitcask: \"data/bitcask/1118962191081472546749696200048404186924073353216\"\nOpening output file: \"/tmp/1118962191081472546749696200048404186924073353216.keys\"\ncat /tmp/1118962191081472546749696200048404186924073353216.keys \nriak_client_test,key1\n```\n. It's kind of a shame it's been two years, and 'riak reboot' still does the same thing: https://github.com/basho/node_package/blob/develop/priv/base/runner#L270\nI guess the documentation was updated at least to note the heartbeat. ...apparently not: http://docs.basho.com/riak/latest/ops/running/tools/riak/#reboot\nMaybe it's documented in 'vm.args' ... http://docs.basho.com/riak/2.0.0pre5/ops/advanced/configs/configuration-files/#vm-args ...no.\nLe Sigh.\n. Considering it seems no one uses heartbeat, the optimal solution appears to\nbe to deprecate this function, note it in the man page, remove it from the\ncommand list help output, add a sanity check to see if vm.args is\nconfigured for heartbeat, and if not, refuse to run the command, and add\nhelp text that they probably meant 'restart'.\nThat way, if anyone actually does use it as intended, it's not broken.\nOn Tuesday, December 24, 2013, Brian Sparrow wrote:\n\nPR's welcome.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/basho/riak/issues/162#issuecomment-31172056\n.\n\n\n===================| Tryn Mirell     || tryn@mirell.org tryn@mirell.org\n|| +1.512.394.3620 |===================\"Even imperfection itself may have\nits ideal or perfect state.\" Thomas de Quincey \n. The initial reason for this was to locate the script directory PATH from a running Riak instance.\nNow, it seems that the main tool that would use this, riak-debug, cludges itself with RIAK_BIN_DIR to get the runner script directory: https://github.com/basho/riak/commit/e175f0203d2e4163baa3b0167e675c9862ab64d4#diff-c6d6322b62b458c5abc1012ad9b412f7R214 via this pull request: https://github.com/basho/riak/pull/322\nSo who knows, @jonmeredith may have some thoughts.\n. Thank you for this\n. Oh yeah, that reminds me: Erlang mapreduce is basically a wide-open door for arbitrary code execution, including, I suppose, modifying the ACLs themselves, so it should only be accessible to the highest privilege levels.\n. Right now this is currently just a test setup to try out 2.0, so that's why there are only three nodes.\nI'm not really concerned with an uneven cpu utilization, I'm only concerned that when the system is idle (no requests being sent at all to the riak cluster), there's a significant amount of cpu utilization showing on the system 'top'.\n. Just for education purposes, is there an open issue tracking this main issue?\n. http://docs.basho.com/riak/latest/ops/running/recovery/errors/ I just found this, I couldn't find the error in that stack of shit until a second ago. I'll see if I can find the solution in here and close this if I can \n. nope, useless:\nmulti_backend_config_unset      No configuration for Multi backend  Configure at least one backend under multi_backend in app.config\nRIP x__x\n. I took a guess, only because I'm brave enough to try to figure stuff like this out, reading a little further into this: http://docs.basho.com/riak/latest/ops/advanced/backends/multi/ which I did not read much into before because it says at the top \"storage_backend = multi\" and that seemed like all I should care about but reading a little further in (actually all the way down at the bottom) \n```\nstorage_backend = multi\nstorage_backend = multi\nmulti_backend.bitcask_mult.storage_backend = bitcask\nmulti_backend.bitcask_mult.bitcask.data_root = ../data/riak/bitcask_mult\nmulti_backend.leveldb_mult.storage_backend = leveldb\nmulti_backend.leveldb_mult.leveldb.data_root = ../data/riak/leveldb_mult\nmulti_backend.default = bitcask_mult\n```\nand that fixed it, had to get lost in the science of configuration to get this working... seems not right now to take another guess and see if I can figure out how to get memory backend working since thats not described here at all (science of guessing)\n. well, also adding: \nmulti_backend.memory_mult.storage_backend = memory\nseems to be fine. Tested: \nlaptop riak # bin/riak-admin bucket-type create Sessions '{\"props\":{\"backend\":\"memory_mult\"}}'\nSessions created\nWARNING: After activating Sessions, nodes in this cluster\ncan no longer be downgraded to a version of Riak prior to 2.0\nlaptop riak #\nI guess thats it. \n. Works as expected apparently: https://github.com/basho/basho_docs/pull/1506\n. ",
    "brianshumate": "Sorry about that. I did do a search for \"riak_client:stream_list_keys\" prior to submitting, but nothing came up.\n. ",
    "shairontoledo": "No reason to verbose mode, consider using ^ in the beginning of the line.\n. ",
    "joecaswell": "This should have been opened under riak_kv - moving it there\nhttps://github.com/basho/riak_kv/issues/345\n. The 2I results seem to be returning incorrectly when small numbers are used for max_results.  Testing this with curl https://gist.github.com/joecaswell/ed2d095a90cece5751fa, 7 seems to be the magic number for this instance where the results become stable.  Smaller result sets intermittently miss some terms. \n. Absolutely correct, that should have been mapped to lager.handlers.  The way it was, if it were the only setting specified, it may be been ignored.  The tests passed because the translation functions pull the settings by the riak.conf name, not their mapped name.. ",
    "benmmurphy": "or in situations where a developer can connect to a production machine and the attacker knows the ip address/hostname of the production machine. though, this is not a very secure setup since riak trusts the network. i will send you proof of concept to your private email.\n. ",
    "dcarley": "Thanks! \n. ",
    "khill": "In your cloned directory, you should have the following file:\nriak/deps/eleveldb/c_src/system/lib/libsnappy.a\nIt sounds like it's not being built.  You could try running \"./rebar compile\" and see what errors you see.\n. ",
    "runesl": "On my machine (OpenSuse 64bit) i got the same error: no such file or directory: 'c_src/system/lib/libsnappy.a\nBecause lib was actually called lib64, this fixed it for me:\ncd deps/eleveldb/c_src/system\nln -s lib64 lib\nAfter this I can build Riak 1.2.0.\n. I get this error when running with sh instead of bash: \n./riak-debug: syntax error at line 179: `(' unexpected\nThis seems to be the offending line:\nif [ 0 -eq $(( $get_cfgs + $get_logs + $get_riakcmds + $get_syscmds + $get_extracmds )) ]; then\nI'm no shell script guru, so I'll leave the fix to you.\n. ",
    "marcelloceschia": "same issue with riak 1.2.1, I need to symlink lib64 -> lib on openSuSE 64bit\n. ",
    "crodas": "I had the same issue with OpenSuse 64 bits and I fixed with issue with \n```\nln -s $(pwd)/deps/eleveldb/c_src/system/lib64/ $(pwd)/deps/eleveldb/c_src/system/lib\n```\n. ",
    "masochist": "That's because the package is broken in several ways. I'm surprised you got it to install at all.\n. On Jun 19, 2012, at 18:31, Jared Morrow wrote:\n\nTry checking the packages directory, you were trying to do a pkg_info command on the source tarball.\nshell\n$ pkg_info riak-1.2.0pre1-FreeBSD-amd64.tbz \nInformation for riak-1.2.0pre1-FreeBSD-amd64.tbz:\n\nI don't see any documentation in the source tree (aside from the Makefiles) pointing me the package/packages directory. Where would it be appropriate to document this, and would you accept a pull request for such documentation?\n. ",
    "n00k": "shouldn't the regex be '^-[s]*name', so it will match \n-name riak@node.host\nbut not a commented name like\n-name test-riak@node.hode\nor a comment about the switch like\nuse the -name switch to specify the name of the node\nor another switch that happens to contain that like\n-administrator-name admin\n. ",
    "metadave": "I tested all cases mentioned above on OSX and the regex grep \"^[[:space:]]*-s\\?name\" works. We'll need to test on other platforms as well.\n. Tested on OSX, Ubuntu 12.04 x64, FreeBSD 9 x64\n. My apologies, I recreated this fix in branch dip_gh169 as I was unsure about my rebase/squash.\nPlease see https://github.com/basho/riak/pull/175\n. ``` text\nfreebsd9_64_vm1# pkg_add riak-1.2.0pre3-075c35a6-FreeBSD-amd64.tbz\npkg_add: could not find package openssl-1.0.0_7 !\nfreebsd9_64_vm1# pkg_add -r openssl\nFetching ftp://ftp.freebsd.org/pub/FreeBSD/ports/amd64/packages-9.0-release/Latest/openssl.tbz... Done.\n...\nfreebsd9_64_vm1# pkg_add riak-1.2.0pre3-075c35a6-FreeBSD-amd64.tbz\nriak::1002:\nriak::1002:1002::0:0:Riak Server:/usr/local/riak:/bin/sh\nThank you for installing Riak.\n...\nRiak has been installed in /usr/local/riak owned by user:group riak:riak\n```\n@jaredmorrow +1\n. Verified permissions on riak, riak-admin, search-cmd, man pages. Ran basho_bench against the instance, and the data dir was populated. Verified with Jared that config files in /usr/local/etc/riak should be root:wheel.\nLog files show up in the right place. \n+1\n. +1\n. please see https://github.com/basho/riak_test/pull/506 if you will be adding this to riak_ee\n. how do we know this doesn't break anything?\n. if I have time, I'll port a test like https://github.com/basho/riak_test/pull/506 to the riak command.\n. this (new) test will help to catch any shell issues: https://github.com/basho/riak_test/blob/master/tests/riak_admin_console_tests.erl\n. +1, we'll need a riak_ee PR as well\n. +1, commands/riak_aae_status appears in the generated .tgz when riak-debug is run with this branch\n. yeah, https://github.com/basho/riak_ee/pull/219\n. if man sees a slash in the name, it will read in the .gz file:\nman ./riak-admin.1.gz\nsee man man for more info\n. ",
    "Amorelandra": "https://issues.basho.com/show_bug.cgi?id=1269\nMy bad... October 2011 *\n. \\o/\n. ",
    "lexual": "apologies, wrong project\n. ",
    "evanmcc": "@mirell tracked down this issue in some detail here: https://gist.github.com/2986216\n. Kind of.  The idea was that cluster info would join riaknostic in our package of optional diagnostic tools, but since then we've come to the conclusion that it would go into riaknostic, since too many customer have complained about not being able to use riaknostic, since they can't install additional stuff in their security-restricted environments.\n. Discussion is fitfully ongoing, but I suspect that due to done\nenvironmental restrictions this will be closed and discarded soon.\nOn Apr 12, 2013 11:41 AM, \"Jared Morrow\" notifications@github.com wrote:\n\n@evanmcc https://github.com/evanmcc I'm going to assume this is going\nto be post-1.4 considering none of the other PR's are closed yet?\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/basho/riak/pull/212#issuecomment-16303389\n.\n. Thanks for the effort, but we're going to go with a check that fits into our model a bit better.  PR to come soon.\n. I think that those pushes take care of it.\n. Good catch, will revert.\n. +1\n. Just stdout.\nOn Nov 18, 2012 1:27 PM, \"Jared Morrow\" notifications@github.com wrote:\nAre you piping standard out, or standard error or all?\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/basho/riak/issues/246#issuecomment-10491871.\n. That looks a lot better.\n\nI think that it can wait till after 1.3, although it's a pretty strong nice-to-have.\n. My feeling is that attach should go away, and I'm willing to wait for 1.4\nif that's what it takes to get it to happen.\nI'll work with charlie to get an update to riak_test.\nOn Thu, Dec 6, 2012 at 12:57 PM, Jared Morrow notifications@github.comwrote:\n\nSo, if it is a nice-to-have, I'd be more inclined to +1 if the new\nfunctionality was something like attach-nice with attach staying the same\nfor 1.3. Also, I'd hold off on any +1 for a test addition to the\nbasic_command_line test in riak_test.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/basho/riak/pull/254#issuecomment-11103433.\n. @seancribbs this retains attach as attach-direct with all of its\nfunctionality intact.\n\nOn Thu, Dec 6, 2012 at 1:13 PM, Charlie Voiselle\nnotifications@github.comwrote:\n\nI'd just not like it to be the \"go to\" version of how we connect. I like\nthe idea of keeping the piped version in, just as a different name. Working\non some riak-tests now to test the behavior.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/basho/riak/pull/254#issuecomment-11104408.\n. I totally agree, Sean.  By go away, I meant as the default.  Sorry if there\nwas any confusion.\n\nOn Thu, Dec 6, 2012 at 1:40 PM, Sean Cribbs notifications@github.comwrote:\n\n@evanmcc https://github.com/evanmcc I know, I was more commenting on\nyour suggestion to remove it entirely. Aside from the inability for Erlang\nto daemonize itself sensibly, having the run_erl/to_erl pair can be\nextremely helpful for support.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/basho/riak/pull/254#issuecomment-11106045.\n. Looking into what it would take to adding a restricted shell.  It doesn't\nseem particularly hard, more of a build problem than a coding issue.\n\nOn Sun, Dec 9, 2012 at 11:27 AM, Sean Cribbs notifications@github.comwrote:\n\nI might also mention that running q(). from the remsh still quits the\nRiak node. At least you won't hang it by hitting Ctrl-C, however.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/basho/riak/pull/254#issuecomment-11174699.\n. Any objections to merging this before the upcoming freeze?  I note that it's still hanging out here, lacking consensus.  I'd like to re-review the code, but my opinion is that it should go in.\n. If we're considering this, we should also consider moving the check into\nriak_kv and logging from there. Maybe a start-up check and log of all the\nsystem's arbitrary limits?\n. These two commits should take care of the issue.  Any opinions?\n. jwest is on the case\n. both PRs merged, closing\n. oof, fail, closing\n. Although this should likely be reverted or altered if we don't go with the other commit on #278 \n. +1, I have a nearly identical PR that got sidetracked last week by customer issues.\n. Is there also a master PR for this?\n. This needs to be rebased to develop, it looks like there have been some conflicting changes.\n. The 1.3 and 1.4 lines are unlikely to ever support R16.\n. :+1:, looks good.\n. Patch applies cleanly.\nmake dist builds with no problem.\nThe tarball produced builds without trying to fetch anything from the network.\n.git dirs are removed and vsn.git files are left in their place.\n\n+1\n. Updated, for when you have a second to look at it.   +1, although we may want someone else to glance at it now that we've both got commits here.\n. Also I guess that this should be moved to develop, rather than merged to master.\n. Retargeted PR has merged, closing.\n. As for 1. It does seem to be real, and something that will hit us on bitcask.\nFor example, a disk bound ramp up of bitcask (identical cluster and test load): \nR16B01, +A 64:\n234.44444444444443 57.422222222222224\n358.5944444444445 90.27222222222221 \n363.52777777777777 89.95 \n356.73333333333335 89.57777777777778\n376.6166666666667 94.89444444444445 \n369.9277777777778 92.26666666666667 \n385.9222222222222 96.40555555555555 \n373.38888888888886 92.82777777777778 \n395.53333333333336 97.81666666666666 \n393.05 97.12777777777778 \nR16B01, +A 67 (workaround is prime-size worker pool):\n549.0944444444444 135.24444444444444\n717.7277777777778 177.95 \n706.9888888888889 178.66666666666666\n743.9555555555556 185.29444444444442\n779.2388888888889 192.4111111111111 \n779.6 193.7888888888889 \n868.7 214.37222222222223 \n911.75 227.2111111111111 \n966.3888888888889 240.82777777777775\n1124.3333333333333 282.69444444444446 \nR15B01: \n606.3277777777778 150.9888888888889\n979.7055555555556 243.48333333333332\n985.3055555555555 244.20555555555558\n1021.6277777777777 257.35 \n1043.788888888889 261.55 \n1085.7611111111112 272.67777777777775\n1225.4111111111113 306.5777777777778 \n1353.5388888888888 335.69444444444446 \n1574.8666666666666 387.13888888888886 \n1961.9833333333333 490.81666666666666 \nFirst column is vnode_gets, second is vnode_puts.\nTwo takeaways from this: \n- R16B01 is much faster with a prime-sized async pool, which suggests that 1 is true.\n- It's still slower than R15B01 even with the worker pool sized correctly.\nIf we end up shipping with this workaround rather than cherry-picking whatever patch the erlang team does for this, we need to note it prominently in the release notes.\n. It looks like R16B02 does fix this issue.  Leaving open for now as it isn't 100% clear to me if we can go with it for other reasons.\n. I applaud the intention here, but I wonder if this isn't best addressed in core itself, if the node-watcher shouldn't bail if it doesn't know about the node.  One thing that could be addressed on this side would be to make if so that if TARGETNODE is blank to just use the local node, which I think would address most instances of this problem more directly.\n. IMO, we should just do it , or do +sbt db for 2.0-RC, but I have never had\nthe time to verify my intuition that it doesn't hurt machines with a single\nnuma domain.\n. I regularly recommend this setting to users with dual-core machines, and\nthus run most of my scalability tests with this setting enabled.  That\nsaid, I don't typically run these tests on single core machines.  Even\nKostis recommends it!  But I am OK with being super-conservative for one\nmore release, I guess.\nOn Mon, Apr 21, 2014 at 8:36 AM, Jon Meredith notifications@github.comwrote:\n\n@evanmcc https://github.com/evanmcc @jaredmorrowhttps://github.com/jaredmorrowI think the time window has closed on this, unless we're planning to rerun\nbackend/cluster testing with this enabled.\n\nReply to this email directly or view it on GitHubhttps://github.com/basho/riak/issues/391#issuecomment-40944687\n.\n. my testing suggest that it's better for leveldb in most if not all\nsituations.  Haven't done much on #1.  Maybe I'll just commit the change\nfor develop right after 2.0 so we have the whole cycle to notice/work on\npotential regressions.\n\nOn Mon, Apr 21, 2014 at 9:25 AM, Jared Morrow notifications@github.comwrote:\n\nI don't have a strong opinion on this one, though I wonder if the things\nmentioned in your first comment have been addressed?\nIn order to make it a default setting for vm.args, though, we need to\nverify a few things:\n- performance isn't negatively impacted on other platforms (single\n  cpu, platforms that don't support sbt, virtualized hardware)\n- performance isn't negatively impacted on other backends (leveldb,\n  memory, most of the testing has been on bitcask and memory so far, but\n  signs for leveldb are good).\n## \nReply to this email directly or view it on GitHubhttps://github.com/basho/riak/issues/391#issuecomment-40948967\n  .\n. I think that these sizes are OK, but fwiw mostly the problematic sibling counts were much lower than this when I was a CSE.  warning at 25 and stopping at 100 sounds more reasonable to me, but I might be over-conservative.\n. K. Drop them to 25/100 and :+1:, we can tune more with CSE input post-ricon.\n. Reviewed Jordan's commit, it works as expected and the change shows up when running.\n. maybe change [option] to [<option>=<value>]?\n\notherwise this is ready when: https://github.com/basho/riak_core/pull/451 goes in, +1.\n. looks great, ready to go when kv PR lands\n. the strongest possible :+1: from me, this absolutely needs to be done.\n. 32MB sounds good to me.\n. Also the warning should be suppressable for people who're using search-cmd in scripts.\n. This is more or less expected with the current build.  There are major\nchanges coming down the pike in the near future.  For the moment, don't\nworry about it.\nOn Wed, Mar 5, 2014 at 9:06 AM, hmai- notifications@github.com wrote:\n\nRight now this is currently just a test setup to try out 2.0, so that's\nwhy there are only three nodes.\nI'm not really concerned with an uneven cpu utilization, I'm only\nconcerned that when the system is idle (no requests being sent at all to\nthe riak cluster), there's a significant amount of cpu utilization showing\non the system 'top'.\n\nReply to this email directly or view it on GitHubhttps://github.com/basho/riak/issues/502#issuecomment-36766464\n.\n. Is it possible to quantify what's safe?  the 100/s limit has made some log\nsets I've looked at entirely useless because there are periods where\nthousands of messages were dropped.  How hard would to be to add\nsyslog-like repeat message folding?  I understand why the limit is there,\nI'm just arguing that it's far too strict by default.\n. At a customer with a very large cluster of very heavily populated bitcask\npartitions, upgrading from 1.3 -> 1.4.  The nodes got sad and kv wouldn't\nstart, but we couldn't figure out what was going on.\n\n2014-03-20 16:53:22.442 [warning] <0.6.0> lager_error_logger_h dropped 46\nmessages in the last second that exceeded the limit of 100 messages/sec\n2014-03-20 16:53:23.015 [warning] <0.6.0> lager_error_logger_h dropped 216\nmessages in the last second that exceeded the limit of 100 messages/sec\n2014-03-20 16:57:34.000 [warning] <0.6.0> lager_error_logger_h dropped 1271\nmessages in the last second that exceeded the limit of 100 messages/sec\n2014-03-20 16:57:35.000 [warning] <0.6.0> lager_error_logger_h dropped 2412\nmessages in the last second that exceeded the limit of 100 messages/sec\n2014-03-20 16:57:36.000 [warning] <0.6.0> lager_error_logger_h dropped 11\nmessages in the last second that exceeded the limit of 100 messages/sec\n2014-03-20 16:58:38.000 [warning] <0.6.0> lager_error_logger_h dropped 5\nmessages in the last second that exceeded the limit of 100 messages/sec\n2014-03-20 16:58:39.000 [warning] <0.6.0> lager_error_logger_h dropped 18\nmessages in the last second that exceeded the limit of 100 messages/sec\n2014-03-20 16:58:44.610 [warning] <0.6.0> lager_error_logger_h dropped 5\nmessages in the last second that exceeded the limit of 100 messages/sec\n2014-03-20 16:58:47.230 [warning] <0.6.0> lager_error_logger_h dropped 5\nmessages in the last second that exceeded the limit of 100 messages/sec\n2014-03-20 16:58:48.000 [warning] <0.6.0> lager_error_logger_h dropped 3\nmessages in the last second that exceeded the limit of 100 messages/sec\n2014-03-20 18:00:20.131 [warning] <0.6.0> lager_error_logger_h dropped 8\nmessages in the last second that exceeded the limit of 100 messages/sec\n. would it be reasonable to move bitcask/eleveldb to a macro-based or\nenvironment toggled log selector?  so in riak, we use lager, whatever else,\nwe use error_logger?\nOn Fri, Mar 28, 2014 at 6:59 AM, Andrew Thompson\nnotifications@github.comwrote:\n\nRemember, this only affects messages coming from error_logger. Bitcask\nuses error_logger because we didn't want to tie it to lager with a\ndependancy.\nWe don't use phased startup, so when is riak 'up'? There's API for\nchanging the error_logger high water mark:\nhttps://github.com/basho/lager/blob/master/src/error_logger_lager_h.erl#L73\nNow, what evan saw where it only reports the dropped message, is weird.\nI've never seen it do that. It should only drop the log messages that\nexceed the threshhold for that second, which usually gives you a decent\nsampling what is storming the error logger without actually killing it.\nThis might be a bug, so I'd be interested if it was possible to reproduce.\nA year ago I did a lot of testing to find a reasonable default for this, I\nwon't stop you guys changing the defaults, but when error_logger starts\nOOMing nodes again (and the logs get lost anyway), don't be surprised.\n\nReply to this email directly or view it on GitHubhttps://github.com/basho/riak/issues/517#issuecomment-38921318\n.\n. As a note, lager is already a dependency of bitcask now that cuttlefish is\nincluded.  Maybe we should just move it over?\n. Yet another inconclusive discussion.  I am just going to close it and move the potential work over to bitcask.\n. merge it\n. +1 55e96f8\n. If we want to do this stuff, it belongs at the top, not at line 27.\n. actually this whole area of the code might be duplication?\n. why is there so much new code here?  it seems like console should not be changed.\n. This option is not documented in generic usage for the script at the top.\n. this isn't catching the too many arguments case.  this may or may not be an issue, but the error message coming from riak isn't terribly specific.\n. I'd really like a message here letting the user know that a background reformat has been launched and to check the logs for completion status. \n. Not knowing whether or not it works on R14, I didn't see any reason to\nremove it, given its state of benign neglect over the course of 1.2, 1.3,\nand 1.4.  That said, I don't mind removing it, as a signal that we're only\ntargeting the latest releases of erlang.\n. My feeling is that we should have all of the versions in there that we're at least willing to take bug reports from in the open source community.  So I think that we should be liberal here and broadly allow R15 and R16, deprecating R15 when we add R17 support sometime next year.\n. I'd like the actual perf stuff to be a separate target, make runperf or something.  This will allow easier tweaking when you don't need to rebuild then entire perfdir.\n. Also it'd be awesome here if instead of the rel path we altered the stagedevrel path, then someone could make a code change, run make compile and then make runperf without having to redo any tweaks to the devrel (after it is removed and rebuilt).\n. I addressed this in my commits.\n. this line needs to be removed if we're removing exometer/sidejob stuff.\n. \n",
    "cmeiklejohn": "For basho/riak_ee#73, basho/riak_control#28. @jaredmorrow\n. It's something specifically related to nspr (Netscape Portable Runtime) in erlang_js; some sort of environment change is happening when building with 64-bit clang, causing these files to no longer be available in the search path.\n. Confirmed this error does not occur when building erlang_js outside of Riak.\n. Resolved in basho/erlang_js#39.\n. Tracking in basho/erlang_js#39.\n. See basho/erlang_js#40.  We might need to make an additional change for Mavericks.\n. Let me know what I can do here; I'd love to have this make it for pre3.\n. Addressed by #402.\n. Cool; I'm going to open a PR to have them removed.\n. Please let me know when this is merged, so I can open a PR against enterprise.\n. :beers:\n. Please let me know when this is merged, so I can open a PR against enterprise.\n. Was this also fixed for riak_ee?\n. Tagging @Vagabond.\n. Tracking a OTP fix for this here: http://erlang.org/pipermail/erlang-bugs/2013-October/003782.html\nThanks @Vagabond \n. Yeah; I'm working on this right now.\n. I believe this is resolved by #188, but would love confirmation.\n. Yup; I'm tracking this along with the failed riaknostic_rt test.\n. :+1: \n. release build; riak started.  :+1: \n. Do you have the pam-devel package installed?  This is what provides that missing header file.\n. Merging to meet the pre13 deadline.\n. @metadave Do we need to also apply this back-ported fix to riak_ee as well?\n. sigh, it's Friday, I'm off my game. :)\n. Poorly.\n. While Riak should compile on Erlang 17 now, we're not officially supporting it until we release a Erlang 17-based Riak, which won't be our next immediate release, but should follow shortly after.\n. @lucperkins Can we get this added to the MapReduce documentation?  It just came up on the mailing list again, and appears to be a straightforward fix.\n. Historically, we've only ever advanced the version of the supported runtime at major revisions of Riak, because it requires a large amount of testing to ensure that there are no unforeseen performance penalties.  That said, Riak 2.0 (released September 2, 2014) was the first release to ship with R16 and there are no current plans for moving to 17 in the short term.  We will reevaluate when identifying features targeted for the next major release.\n. :+1:, make locked-deps has been the only way to reproduce the tag build.\n. :+1: 3ccdc69\n. Now that everything's been merged up and the dependencies resolved, this is no longer an issue.\n. This bug should be filed against riak_ee.\n. :+1: looks good; built with the specified locked deps.\n. We're in the progress of addressing these issues, one component at a time.  The next major release of Riak will be targeted at Erlang 17, but right now, only R16B02 is officially supported.\n. Opened #697 to address.\n. Addressed by #699 (and a slew of other PRs across several repos.)\n. :+1: this ensures the release is build with locked dependencies and does not alter or regenerate the file\n. Need to resolve this:\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n. :+1:\n. :+1: 49ce677\n. :+1: 09ed9f3, please open backports for 2.0 and 2.1 if necessary.\n. :+1: 09ed9f3, please open backports for 2.0 and 2.1 if necessary.\n. :+1: to being explicit and remove it.\n. This repository isn't open source yet, thought?  Link is 404.. ",
    "yeysus": "On Sun, Aug 26, 2012 at 6:57 AM, Jared Morrow notifications@github.comwrote:\n\nhttp://basho.com/resources/downloads/ is the new downloads site and\ndownloads.basho.com has been phased out. The release notes are current\nfor the 1.2 release.\nHello Jared,\n\nThe page http://basho.com/resources/downloads/ goes to a page that loads\njavascript and you have to click to move down to Riak / CURRENT, it is not\nan easy scriptable url to determine if you have a newer version available.\nIs the number of the current version available somewhere else?\nThanks,\nJesus\n. ",
    "bahamas10": "awesome, excited for this.\n. ",
    "ybiruk": "I have the same issue\n. ",
    "drhip": "unfortunately I'm using version 1.1.2 and it doesn't support riak-admin cluster command\n. ",
    "calston": "Actually I can sort-of deal with this by wrapping the mapreduce query with a try-catch, in which case the \"busy being deleted\" nodes are stepped over. \nWhy are these provided to the map function in the first place?\n. Obviously the above error was because search was disabled in Riak but enabled in the bucket metadata. Obviously. I mean you can all see it, right? It didn't waste hours of my time for a really simple config mismatch for nothing, right??\n. ",
    "mpermar": "Just found the same issue. Launching a JS function after a delete gives on the client an error like: \nCaused by: java.io.IOException: {\"phase\":0,\"error\":\"[{<<\\\"lineno\\\">>,466},{<<\\\"message\\\">>,<<\\\"SyntaxError: syntax error\\\">>},{<<\\\"source\\\">>,<<\\\"()\\\">>}]\",\"input\":\"{ok,{r_object,<<\\\"nodes\\\">>,<<\\\"localhost\\\">>,[{r_content,{dict,4,16,16,8,80,48,{[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]},{{[],[],[],[],[],[],[],[],[],[],[[<<\\\"X-Riak-VTag\\\">>,54,74,77,72,72,53,84,115,110,115,52,99,103,55,119,88,115,101,106,68,83,85]],[[<<\\\"index\\\">>]],[[<<\\\"X-Riak-Deleted\\\">>,116,114,117,101]],[[<<\\\"X-Riak-Last-Modified\\\">>|{1343,657396,301806}]],[],[]}}},<<>>}],[{<<197,82,177,11,80,18,85,204>>,{2,63510876596}}],{dict,1,16,16,8,80,48,{[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],...},...},...},...}\"}\n    at com.basho.riak.client.raw.pbc.PBClientAdapter.mapReduce(PBClientAdapter.java:416)\n    at com.basho.riak.client.query.MapReduce.execute(MapReduce.java:78)\n    ... 24 more\n. ",
    "joedevivo": "+1\n. +1\n. Looks great to me +1\n. Not only do I like it, but it totally works via riak_test and manual devrel testing. +1\n. you broke my :heart: @angrycub, YOU BROKE MY :heart:!\n. I'll forward port this to cuttlefish\n. Nope, it's covered in #379 \n. I'll port this to cuttlefish\n. @jaredmorrow there will be CSE review of the entire schema in the coming weeks.\n. going to close this, see #371 \n. Merged, on @jaredmorrow's +1 from #365 \n. The tests pass for the same change in the cuttlefish project. there's no test in this project, but a riak_test is forthcoming.\n. The following is from the 1.4.2.documentation\ndefault_bucket_props These are properties used for buckets that have not been explicitly defined (as outlined in the HTTP API). They are useful for setting default bucket behavior such as:\nerlang\n{default_bucket_props, [\n    {n_val,3},\n    {allow_mult,false},\n    {last_write_wins,false},\n    {precommit, []},\n    {postcommit, []},\n    {chash_keyfun, {riak_core_util, chash_std_keyfun}},\n    {linkfun, {modfun, riak_kv_wm_link_walker, mapreduce_linkfun}}\n    ]}\n- n_val - the number of replicas stored. Note: See CAP Controls for further discussion.\n- Read, Write and Delete quorum values. Valid options include numeric values (e.g. {r, 2}), and the following symbolic values:  quorum (a majority of the replicas must respond, equivalent to n_val / 2 + 1)\n  all (all N replicas must respond)\n  - r - Read quorum value (the number of Riak nodes which must return results for a GET request before it is considered successful). Default: quorum.\n  - pr - Primary read quorum (the number of primary, non-fallback nodes that must return results for a successful GET request). Default: 0. Note: See Eventual Consistency for an explanation of primary nodes.\n  - w - Write quorum value (the number of Riak nodes which must accept a PUT request). Default: quorum.\n  - dw - Durable write quorum (the number of Riak nodes which have received an acknowledgment of the write from the storage backend). Default: quorum.\n  - pw - Primary write quorum (the number of primary, non-fallback nodes that must accept a PUT request). Default: 0.\n  - rw - Delete quorum. Default: quorum.\n- allow_mult - whether or not siblings are allowed. Note: See Vector Clocks for a discussion of sibling resolution.\n- precommit - global pre-commit hook functions, either in Javascript or Erlang.\n- postcommit - global post-commit hook functions. Erlang only.\n. Track work here: https://github.com/basho/riak_kv/pull/695\n. +1 to closing\n. @seancribbs I'll have j4 and Jimmy on validators next week, this was just to prove it worked.\n. Do not merge until https://github.com/basho/yokozuna/pull/170 is also merged\n. Please close #338 and #361 when this is merged\n. #361 was not ready, need to roll back commit\n. o man o man o man\n. OMG @rzezeski !\n. +1 to merge\n. THIS IS PERFECT! WHOEVER YOU COPIED THIS CODE FROM IS A GENIUS!\n+1\n. +1\n. +1\n. +1\n. zdbbl is hitting some weird corner case in cuttlefish that I don't have time to get to the bottom of today. For now I've removed the datatype from zdbbl so it's useable. Will dig into this for the next \"pre\" or whatever.\n. The thing about 15 seconds is most likely a node_package thing. It's been that way from the start, so I don't think a blocker, but it is worth fixing. I'll get to it next week.\n. @bsparrow435 should be involved here\n. riak-debug currently pulls from the app.config. That's where it gets interesting because of the conditional logic in cuttlefish that needs to be smart about which files exist. It goes like this:\n- If there's no app.config and no vm.args in the etc dir, it generates them with cuttlefish from the riak.conf\n- If there's only one of those in etc, it generates both, but uses the existing one from etc, and pulls in the other from cuttlefish\n- If both exist, cuttlefish does nothing.\nIf riak has never been started or there's an error in the riak.conf, the app.config and vm.args will never be generated. For the first time, we're in a situation where we can't count on app.config existing. Fortunately for us, if this is the case, the only two pieces of information riak-debug pulls from the app.config will be empty anyway.\nWhat's more confusing is that the only place that we can be sure the config is the one we're using is the generated app.config. I don't love the idea of replicating cuttlefish logic in riak-debug.\nNow that I've written down where I think the problems are, let me float a solution that has popped into my head while writing this.\ncuttlefish does it's thing through the cuttlefish_escript. That includes that conditional logic above. the cuttlefish escript is already included in the riak build. We could add a command line flag that queries the config, the way epath does for app.config. I actually like that alot, but probably can't get it done for tech preview. \nUntil this issue is closed, riak-debug will work, it just won't include the riak logs or ring state.\n. tested +1\n. unnecessary. only the plugin in rel/rebar.config was ever being productive.\n. Yes @wb14123, please check your riak.conf for nodename and let me know what the line says\n. @wb14123 the problem is that your cuttlefish repo is too new for your riak repo.\nIf you have riak checked out from github, go head and pull develop. That'll get you the up to date version you need.\n. If this change to the default configuration is made, it should be done as an override in riak.schema, not in the erlang_vm.schema. have a nice day.\n. +1\n. What's one plus?\n. @bsparrow435 said he was working on.\n. retagging for RC. won't hold up beta for it.\n. retagging as RC, won't hold up beta for it, but will still try to have it merged for beta\n. There's an easier way to do this without duplicating code:\nerlang\n{mapping, \"leveldb.limited_developer_mem\", \"eleveldb.limited_developer_mem\", [\n  {default, true},\n  merge\n]},\nWhat you're telling the fish is \"I've already defined limited_developer_mem, but I just want this one part to override.\nThis way you don't have to copy the @doc and make sure it stays in sync with the leveldb schema.\n. +1, I'll make another PR to make this less repeaty in the future.\n. We do have these, they're in erlang_vm.schema\n. https://github.com/basho/cuttlefish/blob/develop/priv/erlang_vm.schema#L229-L240\n. @jaredmorrow what are you talking about? This PR tooooooootally has a description.\n. basho/cuttlefish#130 has resolved this issue. You can try building from develop to use your multi_backend config.\n. This needs a CSE, PM and ENG +1,\nI've already tested it while fixing the underlying cuttlefissue. +1 Eng.\n. He tasks me. He tasks me, and I shall have him. I'll chase him round the Moons of Nibia and round the Antares Maelstrom and round Perdition's flames before I give him up!\n. Is there a reason to keep R14 compatibility?\n. Do we want to take it as far as saying \"A version of Riak is targeted at a particular OTP version\" and explicitly require that version? In this case, R16B01.\n. nope! We decided not to include search in the riak.conf file. If you want search, you'll need to use advanced.config. Use yokozuna instead :)\n. yes to riak.control. I could have sworn that was in here.\n. ",
    "wfarr": "/cc @argv0 @tnm\n. Awesome! :metal:\n- Will\nOn Aug 26, 2012, at 0:48, Jared Morrow notifications@github.com wrote:\n\n@wfarr I'm making new builders as we transition datacenters, I'll make a Squeeze builder while I'm at it since there has been a lot of demand for it.\n\u2014\nReply to this email directly or view it on GitHub.\n. \n",
    "jsmartin": "Should be good now.  Found the proper way to determine the architecture dpkg uses to construct filenames.\n. ",
    "staffan-einarsson": "I can repeat this as well in my tests. It seems from dmesg that using the ejsLog function in a mapreduce phase causes a memory protection fault in the Erlang JavaScript driver. I got the following line:\nEDIT:\nIt happens if I use ejsLog() from either the map phase or reduce phase. So it's not isolated to reduce.\nHere are two phase functions that I use that can reproduce the problem. Uncommenting either of the two ejsLog calls will crash the Erlang VM.\nMy Map function is:\nfunction(object, keydata, arg)\n{\n    //ejsLog('mapreduce.log', 'Hello!');\n    return [JSON.parse(object.values[0].data)];\n}\nMy Reduce function is:\nfunction(valueList, keydata, arg)\n{\n    //ejsLog('mapreduce.log', 'Hello!');\n    valueList.sort();\n    var result = valueList;\n    return result;\n}\nIn this case the MapReduce was sent to operate on a whole bucket.\nEDIT 2:\nI'm running Riak 1.2.0 on Ubuntu Server 12.04.\n. ",
    "shuveb": "Yes, I can confirm the same on MacOS X Lion. The \"beam.smp\" process simply crashes. Logs tell me it is: EXC_BAD_ACCESS (SIGSEGV)\nIf I comment the ejsLog() function, things seem to be just fine.\nRiak version I'm running is 1.2.0\n. ",
    "stoddn": "Verified fix in 1.2.1rc2 on CentOS 5.6 with LevelDB.\n. ",
    "scalp42": "Riak SmartOS package can't start (even with those 2 patches) on 2012Q2 repo :\n[root@prod-riak5 /var/svc/log]# riak start\n  Attempting to restart script through sudo -H -u riak\n\nWarning: please use 'svcadm enable riak' instead\n\nld.so.1: beam.smp: fatal: libncurses.so.5: open failed: No such file or directory\n  /opt/local/sbin/riak: line 97: 21662: Killed\n  ld.so.1: beam.smp: fatal: libncurses.so.5: open failed: No such file or directory\n  /opt/local/sbin/riak: line 130: 21669: Killed\n  Error reading /opt/local/etc/riak/app.config\n. Thanks @No9 will look into it\n. Sweet!\n. ",
    "No9": "Has a similar problem on openindiana erlang was expecting libncurses.so.5 in /usr/lib \n```\nln -s /usr/gnu/lib/libncurses.so.5 /usr/lib/libncurses.so.5\n```\nN.B. YMMV\n. ",
    "mkuznetsov": "After reinstall of erlang and clean clone of git repository (previous was updated with pull). Sources compiled fine and release generated without errors. So far it works.\nSincerely yours,\nMikhail Kuznetsov\nWhen best practices meet everyday life and lead to perfection...\nOct 15, 2012, \u00e2 18:06 , Jared Morrow \u00ed\u00e0\u00ef\u00e8\u00f1\u00e0\u00eb(\u00e0):\n\nWe haven't yet tested fully on 15B02, but it should build. I will get 15B02 installed and take a look. Thanks for the bug filing.\n\u2014\nReply to this email directly or view it on GitHub.\n. installed libpam0g-dev and finish my compilation. Thanks a lot\n. \n",
    "jrwest": "It is not needed. At a minimum it needs to be re-opened to point at develop instead of master and I imagine requires a merge/rebase given the changes to riak-admin this time around. Closing for now with the expectation that @lenary will re-open if he wants.\n. riak-admin backup in its current incarnation has been deprecated. Given this is a feature request for that specific, deprecated, implementation I believe this should be closed as wontfix.\n. backup was deprected in https://github.com/basho/riak_kv/pull/850\n. +1\n. +1\n. ugh should be targetted at 1.3. closing and reopening. \n. changed indices -> indexes\n. related PR (for 1.4) https://github.com/basho/riak_kv/pull/836. the changes also exist today on develop\n. @jaredmorrow thanks! \n. imo we should close as \"wontfix\". Riak Search has been deprecated in favor of Riak Search 2 in Riak 2.0 and 1.4.x releases will only contain bug fixes not new features. Multi-get for k/v data has been implemented at the client layer in most (if not all?) official clients. Perhaps @rzezeski @gburd @michellep have a different opinion though? \n. +1\n. Just to clarify, the repair mentioned here is the existing repair mechanism [1] before the work done by @engelsanchez for 1.4.4+ and 2.0. Since we have a new mechanism and we never decided if indeed we wanted to expose the original method used here, I think closing is fine.\n[1] http://docs.basho.com/riak/latest/ops/running/recovery/repairing-indexes/\n. re: replication between clusters, that isn't something we planned to support in cluster metadata (where this info will be stored) -- at least initially. \nEither way, I agree w/ @Vagabond's initial feeling. Even if the cluster is accessed by the same logical users I would assume they are typically accessed by different hardware (external LB at the least, probably different application servers). \n. @gideondk @peschkaj great! glad to hear you guys won't find it too be too much of a pain.\n@brunogirin There is no technical reason why it cannot be added. We may do so in a future release depending on demand but for the upcoming release it probably won't make it in. For existing link walking users nothing will break, which I think is the most important thing. As usual an early community contribution is always welcome and I'll be happy to squeeze in a review where I can.\n. > These don't really feel like types to me - more like \"bucket groups\".\n\nSeparately, there seem to be some optional settings you can put in for all\nbuckets that are in one group.\nWe considered the \"group\" as well and ultimately the decision was between\nthat and \"type\". In the end we chose \"type\" because more of us in the room\nfelt it was a better name. One fear was that from \"group\" one may or may\nnot infer that data may be grouped (either on disk or within nodes of the\ncluster) which isn't the case. Ultimately, I think \"type\" does a good job\nof conveying that these buckets have a set of settings common between. We\nalso considered \"family\" (which has too much resemblance to a similar name\nin another data model) and \"class\" (which has too many parallels to\nobject-oriented programming that we decided against it).\nI don't know if I'm just bikeshedding here - but I feel like you could\njust allow for some \"/\" characters in a bucket - a sort of simple bucket\nhierarchy - and get most of what you need.\nThis is the problem Bucket Types set out to solve. Since there are no\nrestrictions on bucket names if we decide to bless \"prefix/...\" and an\napplication already uses it we have a problem. The proposed security\nfeature's wildcard support is another area where this clashes. It would be\npossible to provide a way to audit and migrate these buckets but that would\nprobably be slow and not ops-friendly.\nBuckets that aren't in any group - (normal-looking Riak URL's) would maybe\nstay the same; and grouped buckets would just have another slash in the\nbucket part of the URL.\ne.g.\nGET /riak/bucket/key\nwould be a GET for an object in a classic 'default' bucket\nGET /riak/mygroup/bucket/key\nwould be a GET for an object in a grouped one.\nThis is what we plan to do exactly with bucket types.\nI'm just looking at this from the point of someone who doesn't know the\ninternals of Riak at all - so there could be all kinds of terrible things\nwrong with my counterproposal.\nAnd ultimately I don't think it matters too much. But I do like the fact\nthat the URL's I'm fetching things from stay looking pretty much the same,\neven if I use the new features.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/basho/riak/issues/362#issuecomment-22579681\n.\n. @uberbrady one thing I didn't realize upon first read is your version used the already \"old\" API and the RFC uses the \"newer\" APIs. The APIs you are referring to are not yet deprecated within Riak but they are also not commonly extended (for example). The effect will be the same and the API you refer to will continue to work for at-least buckets in the default type (it may be extended as well depending on time, further discussion).\n. Awesome to see you taking things for a spin! My comments are below.\nI'v been doing some minor testing and have some questions:\n- Currently there is no way to delete a bucket type, is the only way\n  around this to delete the datadir?\nThere is not currently a way to delete a bucket type similar to buckets in\n  Riak. This is not something we plan to address for Riak 2.0 (but we may in\n  the future).\n- Calling riak_core_bucket_type:reset/1 will only set the default\n  values for some keys, for instance yz_index is not altered. Should\n  reset be equivalent of calling\n\nAs mentioned elsewhere, reset is not ready for use and the example you\nfound is one reason why. This will be addressed or reset will be removed\nbefore Riak 2.0.\n\n-\n- When using riak-admin, should some bucket properties be read only?\n  (like active should not be changeable through bucket-type update).\n  It's already there for datatype.\n\nGood catch. We are aware not all validation is completely implemented,\nhowever there was no issue tracking these specific cases so I opened one:\nhttps://github.com/basho/riak_core/issues/442\n\n-\n- Is there a way to store custom custom attributes in the bucket\n  properties? i.e. I would like to store information about the structure of\n  the keys in this bucket.\n\nYou should be able to store your own properties. The validation does not\ntake into account custom properties and should not reject them. However,\nyou may have problems using the same property names Riak uses (e.g.\ndatatype or n_val).\n. @lafka ah right. There is nothing stopping you from using internal APIs but the command-line parsing code will prevent it. Although we may not stay w/ JSON [1]  for the riak-admin bucket-type commands I'm not sure if this restriction will be lifted. I haven't run into this issue myself as a user of Riak but I imagine there are existing workarounds (loading a module w/ your atoms?) since buckets have had the same restriction when setting them via the API.\n[1] https://github.com/basho/riak/issues/424\n. @Vagabond i thought the json was what we discussed? I agree its not ideal and am open to alternatives but this was the quickest way\n. merged manually in https://github.com/basho/riak/commit/b80235e891f0eb695848b436ff1e8df4f0f00ede\n. I uncommented erlang.zdbbl = 32MB in the generated config and cuttlefish failed with:\n$ dev/dev1/bin/riak console \n00:49:12.774 [info] Application lager started on node nonode@nohost\n00:49:12.774 [info] Checking /home/jordan/sandbox/basho/review/jd-cuttlefish-riak/riak/dev/dev1/bin/../etc/app.config exists... false\n00:49:12.774 [info] Checking /home/jordan/sandbox/basho/review/jd-cuttlefish-riak/riak/dev/dev1/bin/../etc/vm.args exists... false\n00:49:12.774 [info] No app.config or vm.args detected in /home/jordan/sandbox/basho/review/jd-cuttlefish-riak/riak/dev/dev1/bin/../etc, activating cuttlefish\n00:49:12.858 [info] Adding Defaults\n00:49:12.863 [info] Applying Datatypes\n00:49:12.864 [info] Validation\n00:49:12.865 [info] Applied 1:1 Mappings\n00:49:12.865 [info] Applied Translations\nError generating config with cuttlefish, there should be logs\nThis is a fresh checkout of Riak and jd-cuttlefish but I rebased on develop (and resolved conflicts in rebar.config in favor of develop). I tried un-commenting a few other options including erlang.swfi but this is the only one that gave me problems.\nAlso, I noticed if cuttlefish fails, riak start errors with riak failed to start within 15 seconds. Is this something we plan to fix? Its unclear that you need to riak console to figure out the cuttlefish error and why wait 15 seconds when we know the error immediately?\n. now, when I un-comment erlang.zddbl Erlang says: Invalid dbbl limit: 33554432 along w/ a bunch of help output. Pretty sure the problem is the value is in bytes not kilobytes...\nalso from the output:\n-zdbbl size set the distribution buffer busy limit in kilobytes\n            valid range is [1-2097151]\nErlang:\nErlang R16B02 (erts-5.10.3) [source] [64-bit] [smp:8:8] [async-threads:10] [kernel-poll:false]\nUpdate: changed value to 32768 in riak.conf and Riak now boots fine.\n. Verified control config is generated w/ recent changes. After fixing the zdbbl default +1 to merge. \n. Pushed a fix for zdbbl and @evanmcc double checked it. Merging (as @joedevivo requested).\n. not 100% but this may be related re: HTTP and UTF8 https://github.com/basho/riak-erlang-http-client/pull/44 cc/ @macintux \n. moved to 2.0-RC. I was hoping leaving it would anger people enough I'd get asked to remove the JSON, but it didn't work. The \"no props\" version should work at a minimum.\n. unfortunately, yes\n. @lafka reset was purposefully removed from riak-admin temporarily [1] because it is not ready to be exposed to users and has ambiguous results. It will be fixed for the final release of 2.0.\n[1] https://github.com/basho/riak/pull/421\n. another vote for raising it? https://github.com/basho/riak/issues/508\n. We have discussed a list bucket types API previously as part of the larger bucket type API discussion. We are aware that several features are missing from that API and do not plan to expose any of it this release (the only way to work w/ bucket types should be the command line w/ Riak 2.0). I believe we plan to revist and fully implement the API in the future (caveats re: security).\nAdditionally, we need to make a distinction between \"listing bucket types\" and \"listing buckets under a type\". The current list bucket operation maps to \"listing buckets under the default type\". I believe we have extended that to including \"listing buckets under any specific type\" however, it sounds like there is a feature request for \"listing buckets across multiple types at once\"?\nSent from my iPhone\n\nOn Jan 3, 2014, at 11:40 AM, Andrew Thompson notifications@github.com wrote:\nWe can certainly add a permission to allow you to list bucket types, along the lines of the permission that allows bucket listing.\n\u2014\nReply to this email directly or view it on GitHub.\n. ok. just wanted to clarify. considering that would be an entirely new feature I was confused by the report that customers are currently affected...Even with the current state of non-existence of the API one could continue to operate web admin tools that use the \"listing buckets under/within a type\"  as @seancribbs mentioned. Riak 2.0 operates just as it used to under the default bucket type and web-admin tools and other applications should not be affected just because they exist.\n\nIn addition, bucket types, from a data modeling perspective, at least at this time, should not be created dynamically (something that would obviously be difficult anyways since there is no write operations via API currently). This should make it easy to enumerate them all even w/o an API to do so. Tools may continue to operate but a w/ degraded user experience but they are not broken. Once the API is available they can of course be improved, and that is why we would like to provide the API in a future release. \nI would be curious to know more about the customers that may potentially have issues w/ this to see if we can find a workaround, in the meantime, as well as to ensure we build the API they want, in the future. Would be happy to discuss that elsewhere.\nRegardless, I think it would be should to use this issue to discuss the future work on the bucket types API, so we have a central place. It has been scattered across several issues which makes the discussion harder to follow.\nFor completeness, here is the current state of bucket types as has been shown to users (the only mention of an API is there isn't one): https://gist.github.com/jrwest/3652ecd1db609e25cdaa\n[1] https://github.com/basho/riak_test/blob/master/tests/bucket_types.erl#L125\n. Sounds good. I agree there is functionality missing. Unfortunately, it was a casualty of the release cycle and the lack of time necessary to really map out how things played w/ security on/off, etc. Customers also probably won't love that we are using JSON on the command line either, but we were unable to fix that in time as well...\n. Unfortunately I probably won't be able to make that mumble -- not that I am required for the conversation but fyi. I will be out of town. I may be able to set something up for the evening (pacific) while I'm away otherwise I will be back the following Tuesday.\n. The tl;dr is there is no user-facing API for bucket types (code exists for some of it, but it is not documented or supported in Riak 2.0). We should use this issue to track work for 2.1 to add it. \n. moved to 2.1 release. @michellep @gburd happy to discuss again if necessary.\n. are there any errors in your logs @oleksiyk? also, could you attach to one of your riak nodes and run the following at the erlang shell and paste the result back here?\nriak_core_claimant:get_bucket_type(<<\"mytype\">>, undefined, false).\n. Ok thanks for the update. if recent attempts on newer builds are working and it can no longer be reproduced I'm going to go ahead and close this out. If you run into this again please re-open and let us know\n. @oleksiyk could you try pre14? This issue has previously been reported and I believe addressed [1]. Although, I thought the fix was included in pre11 I'd like to make sure it doesn't affect the most recent builds before proceeding. \n[1] https://github.com/basho/riak/issues/471\n. sorry its not linked anywhere yet but it can be downloaded from: http://s3.amazonaws.com/builds.basho.com/riak/develop/2.0.0pre14/ubuntu/precise/riak_2.0.0pre14-1_amd64.deb\n. my apologies, it seems this issue does have a fix [1], but it has not yet been merged. thanks for the report. I'm going to close this out as a duplicate for now.\n[1] https://github.com/basho/riak/pull/473\n. I believe its a known issue that Riak's erlang_js does not support unicode cc/ @cmeiklejohn \n. when you perform a delete operation w/o a vector clock (as your code seems to be) you are in fact performing a read [1]. Since you have 2 nodes and 3 replicas,  each read will fill up one of the nodes buffer w/ 512KB and the other w/ 256KB. A 1MB buffer is then filled by any two (or three) concurrent delete operations. As the buffer is shared by all partitions on a node, this effectively halts concurrent work on the node. Its feasible, then, that 60 reads and 60 deletes (3 replicas, 20 operations) plus the coordination overhead involved takes 60 seconds. \nRaising +zdbbl is common for production Riak clusters, especially for values in the 100+ KB range. In addition you should consider increasing the number of nodes or reducing the number of replicas so that one node isn't doing double the work it normally would. \n[1] https://github.com/basho/riak_kv/blob/develop/src/riak_kv_delete.erl#L56-L66\n. A head request only controls which information is returned to the client [1]. The internal erlang buffers are still filled with the objects. So there isn't very much, if any, difference between the two (reading first or letting riak do it, that is).\n[1] https://github.com/basho/riak_kv/blob/develop/src/riak_kv_pb_object.erl#L133-L160\n. the vector clock is necessary for the delete to supersede previously written values and to establish if any operations occur concurrently with the delete. At this time, internally, reading object metadata and data is one operation.\n. If you are concerned with performance of deletes because of this, you may consider using something like Riak 2.0 sets (where you do not need to read before write) and inserting a key to be deleted by a garbage collection process out-of-band\n. are you working with bucket types as well? we recently reverted the default for allow_mult to false for buckets under the default type [1]. Defaults are stored locally on each node and are not gossiped so there should be no delay when using them. If you are setting custom properties (defaults or otherwise) there may be some delay (usually microseconds in a devrel) as the update propagates around the cluster.\n[1] https://github.com/basho/riak_kv/pull/825\n. For the default bucket type (which you are working with) allow_mult=false will continue to be the default. This is in order to minimize potential surprises on upgrade (data you expected to never have siblings all of a sudden does). However, we believe allow_mult=true is the correct default, so bucket types, by default, use allow_mult=true. So yes, the scenario you describe is possible.\n. I've marked this for milestone 2.0-RC, it will be investigated at the latest by that release. However, I'm sure @joedevivo can shed some light on this a bit sooner.\n. things build, config works. +1 merge\n. should the same be done for new security and bucket-type commands?\n. bah this branch will need a rebase and some changes before merging\n. closing in favor of #532 \n. the reset command will not be supported in Riak 2.0 and was previously removed: https://github.com/basho/riak/pull/421\n. This is a duplicate of https://github.com/basho/riak_kv/issues/930. The changes in the associated pull request will be included in the next beta or release candidate release. thanks!\n. with make locked-deps ran before make devrel the build works fine. @jaredmorrow any off-the-top-of-your-head ideas before I dig a bit more?\n. agreed. on the riak-2.0.0rc1 tag I have confirmed that everything builds fine. \n. we do not support building with R17 at this time -- the error seems very related to the erlang version. closing.\n. @ashwinak08 the riak mailing list is a better place for your inquiry. There you can also attach the files in Riak's log directory, which will aid in debugging. You may want to try addressing the file limit (ulimit) warning and see if that resolves your issue first. \nI'm going to close this issue. Please email the mailing list if the issue persists. \n. For as long as I can remember re: 2.0 you have had to \"make locked-deps\" followed by \"make all\" for repeatable tag builds.\nNot saying it's the best way but it's been that way for a while and was documented (at least internally)\nJordan\nSent from my iPhone\n\nOn Jan 1, 2015, at 1:15 PM, Jared Morrow notifications@github.com wrote:\nGood point. The fix in this case might be to just add the 'locked-all' target as the default target in major release branches.\n\u2014\nReply to this email directly or view it on GitHub.\n. i lean towards the former as well mostly because of riak_core_ring (e.g. https://github.com/basho/riak_core/blob/master/src/riak_core_ring.erl#L42)\n. but if we decided on \"indexes\" a while ago will change. realizing riak_core_ring is totally different kind of indices.\n. added in https://github.com/basho/riak/commit/e6f4b3f98550eefc2eb01831ee8192c2b4c3c07c\n. handled in riak_kv\n. handled in riak_kv\n. these changes are already on develop [1] and some, like lager, seem more recent. Should this be removed before merge?\n\n[1] https://github.com/basho/riak/blob/develop/rebar.config#L16\n. I see you have a jd-cuttlefish branch on riak_search. does this PR depend on that branch (I don't see a PR for it)?\n. this list needs riak_control.schema as well i think...and search when that is ready.\n. thanks!!!!!\n. \"released\" -> \"release\". ",
    "glickbot": "Also, did you use these instructions to install Riak on Azure: http://docs.basho.com/riak/latest/tutorials/installation/Installing-on-Windows-Azure/\n( you don't have to obviously, it would just help debug the issue )\n. alternately /usr/sbin/riak could source /etc/default/riak. Feels like a bit of a hack though, not sure that's the behavior we want.\n. Looks good, visual +1\n. ",
    "lordnull": "@russelldb Sure, ping me when you've got a minute.\n. Alas, not yet, there was an issue preventing it from being ready (yet).\n. Issue exists, but is a display issue, not a code error. For each user in a source, one source row is added; even is the user added is 'all'. The user lists is combined for display. Then, if a source has 'all' in the user list, an additional row is added to the display. Because the source in the example has no users explicitly added, the row before the 'all' row has an empty list of users.\nPossible solutions: \n- If there are no explicit users, don't show the explicit user row.\n- Add documentation that a row with no users means that if the 'all' entry for the source is removed, no all users from that source will be effected.\nNote that in both cases, if the documentation doesn't already cover this case, it should:\n+--------------------+------------+----------+----------+\n|       users        |    cidr    |  source  | options  |\n+--------------------+------------+----------+----------+\n| jackson, c, b, a,  |127.0.0.1/32|  trust   |    []    |\n|       jones        |            |          |          |\n|        all         |127.0.0.1/32|  trust   |    []    |\n+--------------------+------------+----------+----------+\nIf the 'all' source is removed, users jackson, c, b, a, and jones will still be trusted.\n. Works as advertised. :+1:\n. ",
    "angrycub": "I'd just not like it to be the \"go to\" version of how we connect.  I like the idea of keeping the piped version in, just as a different name.  Working on some riak-tests now to test the behavior.\n. +1\n. CSE +1\n. -pa and -pz can also be set in advanced.config like this:\nerlang\n[{vm_args, [\n    % These have to be unique in the first element,\n    % or they will be collapsed LWW fashion. Since\n    % there can be multiple `-pa` options we \n    % just keep going until we accomplish a unique value\n    % for the purpose of the config.  In some cases, the \n    % `-pa` key is the complete path.  \n    {\"-pa /usr/lib64/erlang/lib/lwes-2.3.1.0/ebin\",\"\"},\n    {\"-pa /usr/lib64/erlang/lib/mondemand-4.1.1.0/ebin\",\"\"},\n    {\"-pa /usr/lib64/erlang/lib/mondriak-0.4.0/ebin\",\"\"},\n    ]},\n...\n]\nThis enables you to set Erlang VM flags for non-schema items, like -s, -pa, -pz, -scl\n. Of note, it's one of the settings that has a translation.  For more understanding, I created a similar setting that didn't use the translation and it worked as expected.  On further inspection looks to be related to the fact that it's a duration.\n. Intentionally broke that catch statement so that we could see the actual error.  Looks like the cascading unit requirement is causing sad.\nin riak.conf \nmulti_backend.bitcask_multi.bitcask.expiry = 36000s\n...\n09:04:13.416 [debug] Running translation for riak_kv.multi_backend\n09:04:13.418 [debug] Adding Defaults\n09:04:13.419 [debug] Right Hand Side Substitutions\n09:04:13.419 [debug] Applying Datatypes\nescript: exception error: bad argument\n  in function  list_to_atom/1\n     called as list_to_atom(36000)\n  in call from cuttlefish_datatypes:from_string/2 (src/cuttlefish_datatypes.erl, line 198)\n  in call from cuttlefish_generator:transform_supported_type/4 (src/cuttlefish_generator.erl, line 519)\n  in call from cuttlefish_generator:transform_extended_type/4 (src/cuttlefish_generator.erl, line 538)\n  in call from cuttlefish_generator:'-transform_datatypes/2-fun-3-'/3 (src/cuttlefish_generator.erl, line 405)\n  in call from lists:foldl/3 (lists.erl, line 1248)\n  in call from cuttlefish_generator:map_transform_datatypes/2 (src/cuttlefish_generator.erl, line 77)\n. Duplicated in #767; closing this issue in favor of the other one because there was more activity there.. Sorry about the radio silence on this ticket.  I can confirm that Riak KV 2.2.0 will be released with Debian 8 packages via packagecloud.  Stay tuned! \n. Closing; luwak is a end-of-life feature and should not be in the rebar.config. We are now on node_package 4.0. Not sure how/why this PR got missed, but closing it to clean up.. We are now on node_package 4.0. Not sure how/why this PR got missed, but closing it to clean up.. @rooms, Does #857 close this issue?. Opted instead to use the set command to set the cuttlefish setting via riak-admin.. The broken dylib error has been resolved with https://github.com/basho/eleveldb/pull/237.  We are working on modifying the builders to prevent linking against the system OpenSSL and will post an update here once this is resolved.. The OSX build machine is configured to statically link in the brew openssl to the embedded erlang.  We anticipate having the 2.2.1 packages able to cleanly pass the brew audit.. @lucafavatella, thank you for this very thorough feature request.  We are currently working on retooling our dependency management and Erlang VM compatibility and anticipate a move to OTP 19 or greater in an upcoming feature release.  The next release will be an LTS candidate, so we will not introduce such a large scale change there; however, the pace of work suggests that it will be ready for the feature release following that.\nYou have pointed out myriad reasons why, based on the current state of Riak and Docker, that we do not suggest running Riak in Docker containers at this point without significant patching work by the end-user. However, we are also currently working on incorporating a much more recent version of OTP and switching to a rebar3/relx workflow.  . +1. +1 again\n. I think it's copypasta. \n. Let me start with, I love fallback dates.  Questions: Is a week enough to smooth out all cases?  If we miss the first date, do we establish a new fallback once our fallback becomes the target?. should this be mapped to crash_log_count?. ",
    "doubleyou": "My 5 cents: https://github.com/basho/riak/pull/265#issuecomment-12005507\n. I'll repeat myself here, but why use run_erl in the first place? Does it have any benefits except logs rotation?\n. Frankly, run_erl way sucks in many ways.\n- Too easy to accidentally close Erlang, by pressing usual Ctrl+C instead of expected Ctrl+D.\n- Attached console is extremely inconvenient, compared to the one from \"erl -remsh\"\n- All attached console IO goes into logs as well, might be bad for logs processing software\n- Its logs rotation ways is inconvenient\nHave you ever considered getting rid of the run_erl completely? Lager can do logs rotation much better. And multilog from daemontools does this even more awesome, IMO, (I'm even thinking of making another logs rotation method for lager, completely similar to multilog's way).\nBesides logs rotation, I don't see any reasons for keeping run_erl, at least so far I see its purpose.\n. Yeah, I think we're good. We'll just set ERL_FLAGS for -noshell and -noinput. Thanks!\n. ",
    "asomov": "The problem is that the WIKI is very misleading (because it says that the source is under the 'app' folder)\n. I understand the problem now.  The '. ~/erlang/r15b01/activate' command is valid for a session only. It is so easy to forget it. \nProbably this page can be improved: http://docs.basho.com/riak/latest/tutorials/installation/Installing-Erlang/\n. This is not a solution. It is far too heavy to install. It also requires Erlang knowledge to contribute. If the test suite is written in JavaScript then many more users may contribute their use cases.\n. The idea is to be able to run tests with ZERO installation. You just point your browser to the URL and press a button.\nThis proposal does not mean that developers do not write Erlang tests. It just means that the tests can be written and run by anyone. If the project provides a basic framework to launch tests, the community can help to deliver more functional tests.\n. ",
    "kuenishi": "Do you have erlang properly installed? Try running commands like erlc or\nescript from the same console you tried make.\nOn Thursday, December 13, 2012, Andrey Somov wrote:\n\nI try to build from source and I get this error:\n~/projects/riak$ make rel\n./rebar get-deps\n/usr/bin/env: escript: No such file or directory\nmake: *** [deps] Error 127\nWhat has to be installed ? (I am afraid the wiki page is out of date...)\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/basho/riak/issues/256.\n\n\nSent from Gmail Mobile, iPhone\n. OH, I tried again today and failed to reproduce at Linux with 1.2.1.\nat MacOS as you said I just hit on the HiPE issue. and, 1.2.1 release on MacOS was caused this problem by using R15B03 (not supported) :\nhttps://github.com/basho/riak_kv/issues/411\n. Moved to toplevel. If there's more suitable filename, fix it please.\n. Just got curious: How can we construct a combined CRDTs like {crdt_map(), crdt_map(), crdt_counter()} - which I expect each element of the tuple each CRDT data? Theoretically it looks like possible if CRDTs are not nested, like crdt_map(string(), crdt_counter()) .\nThis is because, from the viewpoint of application development ease, we rarely store single-valued object with only single counter included, but usually user model where multiple member included like this:\nerlang\n-record(user, {\n               name :: string(),\n               email :: string(),\n               total_count_liked :: non_neg_integer(),\n               friends :: [ string() ], %% usually friends' user names\n               ... }).\nIn this case possible CRDT is applicable at total_count_liked and friends .  Or is this much advanced problem to solve in future?\n. Thank you, now I get it! That's what we have been wanted.\n. A small nitpick:\nprotobuf\nmessage RpbClientInterface {\n    enum RpbProtocol {\n        PB = 1;\n        HTTP = 2;\n        HTTPS = 3;\n    }\n    required RpbProtocol protocol = 1;\n    // This can be a FQDN or a text representation of an IP address.\n    required bytes host = 2;\n    required uint32 port = 3;    \n}\nTo prevent from wrong value, isn't it good to set the type of port as uint16 ?\nAfter all I totally like this idea because several users are suffering network bottleneck at the load balancer or automatic client-side failover of connection.\n. A short Google search made me misunderstand that it has uint16. I should have refered here: https://developers.google.com/protocol-buffers/docs/proto#scalar\nAnyway, thanks!\n. Listing non-valid-utf8-named buckets or non-valid-utf8-named keys always fails because there is no way to represent them in JSON String. It's possible to encode the stuff in some way (say, base64 or whatever) but it's not right. This is why HTTP requests always fail. So I don't think this is a bug.\nRJC PB interface should support that use case non-valid utf8 Strings - in RJC 2.0\nbut RJC 1.x seems to have only String keys ? Oh, Strings in Java are also for valid unicode only.\nc.f. https://github.com/basho/riak_kv/issues/468\n. My understanding is, cluster-info is cluster-wide by default while existing riak-debug is per-node. So I think it'd be better by making it local like:\ndump cluster-info riak-admin cluster-info $CI local\n. +1\n. ? +1 oh, you didn't attached the diff? zenhub or something broken confuses me.\n. +1\n. +1\n. +1\n. How about just mentioning these procedures on README? There is a paragraph describing how to build. Although I sometimes don't trust documents, just in case we all forgot this thread.\n. @andrewjstone Maybe you just missed https://github.com/basho/riak/issues/659 discussion? Also, +1 to not using 2.0 but just develop, until we start working on 2.1 or even 3.0.\n. Riak will not support CORS and may be this is about CS S3 API.\n. fixed after all\n. We don't need \u300cRiak\u3067\u306f\u300d\n. ",
    "chardan": "+1\n. ",
    "chopachom": "Yeah, I know that, I've included output of ulimit -n in console output, but forgot to mention that I've already increased it from 2048 to 524288 and it doesn't help. Should I increase it even more?\n. Yes, I increased it using launchctl:\n32 \u279c cat /etc/launchd.conf\nlimit maxfiles 524288 1048576\n33 \u279c launchctl limit\n    cpu         unlimited      unlimited      \n    filesize    unlimited      unlimited      \n    data        unlimited      unlimited      \n    stack       8388608        67104768       \n    core        0              unlimited      \n    rss         unlimited      unlimited      \n    memlock     unlimited      unlimited      \n    maxproc     709            1064           \n    maxfiles    524288         1048576\n. Don't know, but looks like an OS X related issue - http://erlang.org/pipermail/erlang-questions/2011-December/063119.html\n. ",
    "benjaminbarbe": "It's resolved with latest version of erlang, according to:\nhttps://github.com/mxcl/homebrew/issues/6143\n@dch: FWIW this has been addressed in R16B01 anyway now -- search for erlang FD_SETSIZE if you want details.\nbrew upgrade erlang\n. ",
    "alx": "Solved this issue by removing tasm man files:\ncd /usr/lib/erlang/man/man1/\nsudo rm tasm*\n. ",
    "SnehaSahu05": "thanks for the solution.\nI tried updating ElevelDB, editing 'rabar.config', but only this simple solution solved my issue\n\n==> rel (generate)\nERROR: Unable to generate spec: read file info /deps/riak_control/test/javascripts/admin/css/css/css... failed\nmake: *** [generate] Error 1\n\nsolution\n$ cd deps/riak_control/test/javascripts\n$ sudo rm admin*\n$ make rel\n. ",
    "charl": "I am running RC2 and it exhibits the same issue:\n```\ndpkg -l 'riak'\nDesired=Unknown/Install/Remove/Purge/Hold\n| Status=Not/Inst/Conf-files/Unpacked/halF-conf/Half-inst/trig-aWait/Trig-pend\n|/ Err?=(none)/Reinst-required (Status,Err: uppercase=bad)\n||/ Name                                        Version                                     Description\n+++-===========================================-===========================================-======================================================================================================\nii  riak                                        1.3.0rc2~precise1                           Riak is a highly scalable, fault-tolerant distributed database\n```\nMy /tmp/riak permissions look fine:\n```\nls -lad /tmp/riak/\ndrwxr-xr-x 2 riak riak 4096 Feb 14 18:11 /tmp/riak/\nls -la /tmp/riak/\ntotal 8\ndrwxr-xr-x 2 riak riak 4096 Feb 14 18:11 .\ndrwxrwxrwt 7 root root 4096 Feb 14 18:17 ..\n```\nI made the changes to the logging config and still nothing appears in the logs when I run 'service riak start' or just 'riak start'. If I however run 'riak console' I get the following:\n```\nriak console\nAttempting to restart script through sudo -H -u riak\nExec: /usr/lib/riak/erts-5.9.1/bin/erlexec -boot /usr/lib/riak/releases/1.3.0rc2/riak             -embedded -config /etc/riak/app.config             -pa /usr/lib/riak/lib/basho-patches             -args_file /etc/riak/vm.args -- console\nRoot: /usr/lib/riak\nErlang R15B01 (erts-5.9.1) [source] [64-bit] [async-threads:64] [kernel-poll:true]\n18:16:12.015 [info] Application lager started on node 'riak@127.0.0.1'\n18:16:12.035 [info] Application sasl started on node 'riak@127.0.0.1'\n18:16:12.038 [info] Application crypto started on node 'riak@127.0.0.1'\n18:16:12.038 [info] Application public_key started on node 'riak@127.0.0.1'\n18:16:12.045 [info] Application ssl started on node 'riak@127.0.0.1'\n18:16:12.050 [info] Application riak_sysmon started on node 'riak@127.0.0.1'\n18:16:12.071 [info] Application os_mon started on node 'riak@127.0.0.1'\n18:16:12.118 [info] Application runtime_tools started on node 'riak@127.0.0.1'\n18:16:12.125 [info] Application erlang_js started on node 'riak@127.0.0.1'\n18:16:12.146 [info] Application inets started on node 'riak@127.0.0.1'\n18:16:12.149 [info] Application mochiweb started on node 'riak@127.0.0.1'\n18:16:12.155 [info] Application webmachine started on node 'riak@127.0.0.1'\n18:16:12.155 [info] Application basho_stats started on node 'riak@127.0.0.1'\n18:16:12.162 [info] Application bitcask started on node 'riak@127.0.0.1'\n18:16:12.302 [info] New capability: {riak_core,vnode_routing} = proxy\n18:16:12.310 [info] New capability: {riak_core,staged_joins} = true\n18:16:12.318 [info] Application riak_core started on node 'riak@127.0.0.1'\n18:16:12.324 [info] New capability: {riak_pipe,trace_format} = ordsets\n18:16:12.357 [info] Waiting for application riak_pipe to start (0 seconds).\n18:16:12.358 [info] Application riak_pipe started on node 'riak@127.0.0.1'\n18:16:12.385 [info] Application riak_api started on node 'riak@127.0.0.1'\n18:16:12.410 [info] Waiting for service riak_kv to start (0 seconds)\n18:16:12.452 [info] Spidermonkey VM (thread stack: 16MB, max heap: 8MB, pool: riak_kv_js_map) host starting (<0.308.0>)\n18:16:12.456 [info] Spidermonkey VM (thread stack: 16MB, max heap: 8MB, pool: riak_kv_js_map) host starting (<0.309.0>)\n18:16:12.460 [info] Wait complete for application riak_pipe (0 seconds)\n18:16:12.461 [info] Spidermonkey VM (thread stack: 16MB, max heap: 8MB, pool: riak_kv_js_map) host starting (<0.310.0>)\n18:16:12.472 [info] Spidermonkey VM (thread stack: 16MB, max heap: 8MB, pool: riak_kv_js_map) host starting (<0.311.0>)\n18:16:12.479 [info] Spidermonkey VM (thread stack: 16MB, max heap: 8MB, pool: riak_kv_js_map) host starting (<0.312.0>)\n18:16:12.483 [info] Spidermonkey VM (thread stack: 16MB, max heap: 8MB, pool: riak_kv_js_map) host starting (<0.317.0>)\n18:16:12.487 [info] Spidermonkey VM (thread stack: 16MB, max heap: 8MB, pool: riak_kv_js_map) host starting (<0.324.0>)\n18:16:12.491 [info] Spidermonkey VM (thread stack: 16MB, max heap: 8MB, pool: riak_kv_js_map) host starting (<0.331.0>)\n18:16:12.495 [info] Spidermonkey VM (thread stack: 16MB, max heap: 8MB, pool: riak_kv_js_reduce) host starting (<0.341.0>)\n18:16:12.499 [info] Spidermonkey VM (thread stack: 16MB, max heap: 8MB, pool: riak_kv_js_reduce) host starting (<0.348.0>)\n18:16:12.504 [info] Spidermonkey VM (thread stack: 16MB, max heap: 8MB, pool: riak_kv_js_reduce) host starting (<0.355.0>)\n18:16:12.507 [info] Spidermonkey VM (thread stack: 16MB, max heap: 8MB, pool: riak_kv_js_reduce) host starting (<0.362.0>)\n18:16:12.511 [info] Spidermonkey VM (thread stack: 16MB, max heap: 8MB, pool: riak_kv_js_reduce) host starting (<0.369.0>)\n18:16:12.516 [info] Spidermonkey VM (thread stack: 16MB, max heap: 8MB, pool: riak_kv_js_reduce) host starting (<0.376.0>)\n18:16:12.519 [info] Spidermonkey VM (thread stack: 16MB, max heap: 8MB, pool: riak_kv_js_hook) host starting (<0.384.0>)\n18:16:12.523 [info] Spidermonkey VM (thread stack: 16MB, max heap: 8MB, pool: riak_kv_js_hook) host starting (<0.391.0>)\n18:16:12.524 [info] New capability: {riak_kv,vnode_vclocks} = true\n18:16:12.534 [info] New capability: {riak_kv,legacy_keylisting} = false\n18:16:12.543 [info] New capability: {riak_kv,listkeys_backpressure} = true\n18:16:12.552 [info] New capability: {riak_kv,index_backpressure} = true\n18:16:12.561 [info] New capability: {riak_kv,mapred_system} = pipe\n18:16:12.578 [info] New capability: {riak_kv,mapred_2i_pipe} = true\n18:16:12.588 [info] New capability: {riak_kv,anti_entropy} = enabled_v1\n18:16:12.618 [info] Waiting for application riak_kv to start (0 seconds).\n18:16:12.625 [info] Application riak_kv started on node 'riak@127.0.0.1'\n18:16:12.632 [info] Application merge_index started on node 'riak@127.0.0.1'\n18:16:12.634 [info] Waiting for service riak_kv to start (0 seconds)\n18:16:12.721 [info] Wait complete for application riak_kv (0 seconds)\nenif_send: env==NULL on non-SMP VM/usr/lib/riak/lib/os_mon-2.2.9/priv/bin/memsup: Erlang has closed. \nErlang has closed\n```\n. Just did an install of riak_1.3.0rc2-1_amd64.deb on a clean install of Ubuntu 12.04.2 LTS and I get the same issue.\n. I think I've tracked the issue down to a memory consumption issue. The staging box only has 1GB memory and the same goes for the newly installed Ubuntu box I referred to above.\nRiak just will not run on these machines.\nI resized the IGB machine to 2GB and it now seems to be working fine.\nIs there some better way to cope with out of memory issues with riak (or more generally with erlang)?\n. Both these machines are single CPU, dual core. In my case more RAM fixed this issue. I'll go have a look at #274 none the less.\n. I can confirm that the vm.args change gets things running on my 1GB box. Thanks for all the help!\n. @slfritchie Is there at least a process I can follow to reinitialize my failed node to get it back up and functioning as a cluster node again?\nhttp://docs.basho.com/riak/1.2.0/cookbooks/Recovering-a-Failed-Node/ seems to suggest I can simply start my bitcask backed node normally to recover but it does of course not start properly so this is not really applicable to my situation.\nIs it safe for me to simply remove all in /var/lib/riak and start up as a member of the cluster it used to be part of?\n. @rzezeski Thanks, I'll give it a whirl.\n. detect-bad-files from  https://gist.github.com/rzezeski/3250870 worked for me.\n. This issue no longer exists in riak_1.3.0 so I am closing the ticket.\n. ",
    "tylerhannan": "Reproduced with 1.3rc3 both with and without the flag in the vm.args.\nWithout - won't start on single core\nWith - starts happily.\n+1\n. ",
    "binarykitchen": "Thanks, will do!\n. ",
    "savar": "yep.. would be great..\n. +1\n. ",
    "tomascharvat": "According to erlang R16A release notes, there shall be CPPFLAGS=-DEPMD6 to enable IPv6 support in epmd.\nCompiled both erlang and riak with DEPMD6. EPMD finaly listen at tcp6 and its possible to connect via telnet. However riak cant start.\n sudo -u riak riak console\nExec: /usr/lib/riak/erts-5.9.1/bin/erlexec -boot /usr/lib/riak/releases/1.3.0/riak             -embedded -config /etc/riak/app.config             -pa /usr/lib/riak/basho-patches             -args_file /etc/riak/vm.args -- console\nRoot: /usr/lib/riak\n{error_logger,{{2013,2,24},{14,2,23}},\"Protocol: ~p: register error: ~p~n\",[\"inet6_tcp\",{{badmatch,{error,epmd_close}},[{inet6_tcp_dist,listen,1,[{file,\"inet6_tcp_dist.erl\"},{line,74}]},{net_kernel,start_protos,4,[{file,\"net_kernel.erl\"},{line,1314}]},{net_kernel,start_protos,3,[{file,\"net_kernel.erl\"},{line,1307}]},{net_kernel,init_node,2,[{file,\"net_kernel.erl\"},{line,1197}]},{net_kernel,init,1,[{file,\"net_kernel.erl\"},{line,357}]},{gen_server,init_it,6,[{file,\"gen_server.erl\"},{line,304}]},{proc_lib,init_p_do_apply,3,[{file,\"proc_lib.erl\"},{line,227}]}]}]}\n{error_logger,{{2013,2,24},{14,2,23}},crash_report,[[{initial_call,{net_kernel,init,['Argument__1']}},{pid,<0.21.0>},{registered_name,[]},{error_info,{exit,{error,badarg},[{gen_server,init_it,6,[{file,\"gen_server.erl\"},{line,320}]},{proc_lib,init_p_do_apply,3,[{file,\"proc_lib.erl\"},{line,227}]}]}},{ancestors,[net_sup,kernel_sup,<0.10.0>]},{messages,[]},{links,[#Port<0.195>,<0.18.0>]},{dictionary,[{longnames,true}]},{trap_exit,true},{status,running},{heap_size,987},{stack_size,24},{reductions,443}],[]]}\n{error_logger,{{2013,2,24},{14,2,23}},supervisor_report,[{supervisor,{local,net_sup}},{errorContext,start_error},{reason,{'EXIT',nodistribution}},{offender,[{pid,undefined},{name,net_kernel},{mfargs,{net_kernel,start_link,[['riak@localhost',longnames]]}},{restart_type,permanent},{shutdown,2000},{child_type,worker}]}]}\n{error_logger,{{2013,2,24},{14,2,23}},supervisor_report,[{supervisor,{local,kernel_sup}},{errorContext,start_error},{reason,shutdown},{offender,[{pid,undefined},{name,net_sup},{mfargs,{erl_distribution,start_link,[]}},{restart_type,permanent},{shutdown,infinity},{child_type,supervisor}]}]}\n{error_logger,{{2013,2,24},{14,2,23}},std_info,[{application,kernel},{exited,{shutdown,{kernel,start,[normal,[]]}}},{type,permanent}]}\n{\"Kernel pid terminated\",application_controller,\"{application_start_failure,kernel,{shutdown,{kernel,start,[normal,[]]}}}\"}\n. Thanks for link, appreciated. Do you refer to -name in vm.args by node name ?\nSo vm.args shall have -name riak@1.2.3.4, and then handoff may bind to ::0\nDo members of the ring get handoff IPv6 trough gossip that use riak@1.2.3.4 ?\nIs handoff, http, PB dual stack or IPv[4or6]only\n. ",
    "kulasama": "i got the same problem\n. ",
    "alexmoore": "+1.  Docs/tutorials will have to be updated.\n. @ArpitSuthar You'd have to implement Riak's Backend API.  Basho does this through eLevelDB, which is the Erlang interface to Level, and our branch of LevelDB.\n. ",
    "engelsanchez": "This can not be merged until basho/riak_kv#497 has been merged in first\n. Dropping for now as there are many other places doing the same (mentioning riak-admin) in core. We need a better solution.\n. Got it. Fixes on their way\n. Oops, thanks for taking care of that!\n. This is now a simple changed and I have reviewed and tested :+1: \n. Wow, I got all sorts of confused when I got this today. We really have mega-overload problems on the word index :)\n. +1 Without this, my riak tests were failing, now pass. Ship it! :dancer: :boat: \n. The sizes were pulled out of a hat. I would feel more comfortable with anything recommended from CSE experience.\n. @evanmcc I've changed the limits based on your CSE experience recommendation and I've opened a PR to apply a copy of this to Riak EE here https://github.com/basho/riak_ee/pull/182\n. :+1: More notes in the main PR https://github.com/basho/riak_core/pull/437\n. :+1:  Ship it!\n. +1 712d1f1\n. Yes. We tested mixed configurations and the crash on manager down was the one issue found. Let @jtuple or @andrewjstone re-open if any undocumented issued was found.\n. ",
    "theslava": "If the /usr/sbin/riak script the only part of Riak that requires sudo simply because it tries to become the riak user, I do not agree that sudo should be a requirement for Riak.\n\nSlava\n. ",
    "kientzlecatch": "For the record:  I have full Internet connectivity on my dev machine.  The \"building without internet connectivity\" instructions do not describe the problems I'm having.\nNow I see where I went wrong.  The full messages I got before were:\n$ make rel\nfatal: Not a git repository (or any of the parent directories): .git\n./rebar get-deps\nenv: escript: No such file or directory\nThe git \"fatal\" error confused me.  The real problem was not having the freshly-installed Erlang in my path.  (Since I couldn't use MacPorts because of the Erlang version problem, I had to use 'kerl' to install Erlang and kerl does not set up your path for you you.)\nAfter fixing that, the instructions work better.  There's still the snappy build failure on Mac to work around, but I've already puzzled that one out.\n. ",
    "lusis": "If anyone finds this with google, snappy doesn't use CPPFLAGS, it uses CXXFLAGS. Set that properly before make rel and things should build nicely\n. ",
    "subnetmarco": "The cluster crashed again, this time the I've been able to log the following error:\n** Generic server disksup terminating \n** Last message in was timeout\n** When Server state == {state,80,1800000,{unix,linux},[],#Port<0.448653>}\n** Reason for termination == \n** {badarg,[{erlang,port_close,[#Port<0.448653>],[]},{disksup,terminate,2,[{file,\"disksup.erl\"},{line,164}]},{gen_server,terminate,6,[{file,\"gen_server.erl\"},{line,722}]},{proc_lib,init_p_do_apply,3,[{file,\"proc_lib.erl\"},{line,227}]}]}\n2013-04-02 12:12:39 =CRASH REPORT====\n  crasher:\n    initial call: disksup:init/1\n    pid: <0.7375.222>\n    registered_name: disksup\n    exception exit: {{badarg,[{erlang,port_close,[#Port<0.448653>],[]},{disksup,terminate,2,[{file,\"disksup.erl\"},{line,164}]},{gen_server,terminate,6,[{file,\"gen_server.erl\"},{line,722}]},{proc_lib,init_p_do_apply,3,[{file,\"proc_lib.erl\"},{line,227}]}]},[{gen_server,terminate,6,[{file,\"gen_server.erl\"},{line,725}]},{proc_lib,init_p_do_apply,3,[{file,\"proc_lib.erl\"},{line,227}]}]}\n    ancestors: [os_mon_sup,<0.81.0>]\n    messages: []\n    links: [<0.82.0>]\n    dictionary: []\n    trap_exit: true\n    status: running\n    heap_size: 610\n    stack_size: 24\n    reductions: 312\n  neighbours:\n2013-04-02 12:12:39 =SUPERVISOR REPORT====\n     Supervisor: {local,os_mon_sup}\n     Context:    child_terminated\n     Reason:     {badarg,[{erlang,port_close,[#Port<0.448653>],[]},{disksup,terminate,2,[{file,\"disksup.erl\"},{line,164}]},{gen_server,terminate,6,[{file,\"gen_server.erl\"},{line,722}]},{proc_lib,init_p_do_apply,3,[{file,\"proc_lib.erl\"},{line,227}]}]}\n     Offender:   [{pid,<0.7375.222>},{name,disksup},{mfargs,{disksup,start_link,[]}},{restart_type,permanent},{shutdown,2000},{child_type,worker}]\n. After updating to 1.3.1 the whole cluster crashed again. Now I've been able to log the following error, which suggests a lack of memory on the server. I wonder how can I then limit memory consumption.\n```\n2013-04-08 11:34:28 =SUPERVISOR REPORT====\n     Supervisor: {local,riak_pipe_builder_sup}\n     Context:    child_terminated\n     Reason:     {error,insufficient_vnodes_available}\n     Offender:   [{pid,<0.10710.548>},{name,undefined},{mfargs,{riak_pipe_builder,start_link,undefined}},{restart_type,temporary},{shutdown,brutal_kill},{child_type,worker}]\n2013-04-08 13:20:54 =ERROR REPORT====\n Generic server disksup terminating \n Last message in was {'EXIT',#Port<0.117611876>,normal}\n When Server state == [{data,[{\"OS\",{unix,linux}},{\"Timeout\",1800000},{\"Threshold\",80},{\"DiskData\",[{\"/\",206424760,29},{\"/dev/shm\",3559472,0}]}]}]\n Reason for termination == \n {port_died,normal}\n2013-04-08 13:20:54 =ERROR REPORT====\n Generic server memsup terminating \n Last message in was {'EXIT',<0.85.0>,{port_died,normal}}\n When Server state == [{data,[{\"Timeout\",60000}]},{items,{\"Memory Usage\",[{\"Allocated\",7256293376},{\"Total\",7289798656}]}},{items,{\"Worst Memory User\",[{\"Pid\",<0.141.0>},{\"Memory\",4114672}]}}]\n Reason for termination == \n {port_died,normal}\n2013-04-08 13:20:54 =CRASH REPORT====\n  crasher:\n    initial call: memsup:init/1\n    pid: <0.84.0>\n    registered_name: memsup\n    exception exit: {{port_died,normal},[{gen_server,terminate,6,[{file,\"gen_server.erl\"},{line,747}]},{proc_lib,init_p_do_apply,3,[{file,\"proc_lib.erl\"},{line,227}]}]}\n    ancestors: [os_mon_sup,<0.81.0>]\n    messages: []\n    links: [<0.82.0>]\n    dictionary: [{system_memory_high_watermark,set}]\n    trap_exit: true\n    status: running\n    heap_size: 377\n    stack_size: 24\n    reductions: 2613311929\n  neighbours:\n```\n. @mmajere http://riak-users.197444.n3.nabble.com/Unexpected-Riak-1-3-crash-td4027359.html\n. +1\n. ",
    "performantpro": "Jaredmorrow,\nI am having the exact same issue and I am on the mailing list as well.  Can you refer me to the thread that gave you some insight as to the answer. \nthx.\n. ",
    "ivenhov": "Here's the thread where I reported the issue\nhttp://comments.gmane.org/gmane.comp.db.riak.user/10468\nEasily reproducible with 3k keys, 3 nodes and 512 vnodes. With default 64 vnodes I could not reproduce it.\nThe only workaround to reliably list keys via 2i was to wait until transfer completes\nDaniel\n. I just wanted to check if there's any progress on that considering 1.4 is round the corner.\n. ",
    "skorgu": "+1\n. ",
    "michellep-basho": "It is safe to tag it with 2.1. thanks\n. Yes, this has come up again recently with 2 other customers and 1 open source user. It doesn't necessarily have to be through riak-admin, but more visibility is needed. thanks\n. Same goes for the list key operations.\n. ...however, it sounds like there is a feature request for \"listing buckets across multiple types at once\"?\nyes\n. I will ask Randy to weigh in more on the existing customer use cases and security considerations, so we have more context. I understand that there are other things we need to consider like security, but I can also see how from a customer perspective it might seem like basic functionality is missing in this initial implementation. thanks\n. Randy is available for next Tue's core/kv mumble.  We can discuss more.\n. Let's plan on discussing this the following week when Jordan is back. thanks\n. +1 from PM\n. Hi Russell,\nDo you have any updates on this?\nthanks,\nMichelle\n. ",
    "alexanderkiel": "Is this issue still considered to be open?\n. ",
    "rspeer": "We have looked at Yokozuna, but we're not migrating to an unreleased version of Riak for it.\nCurrently we use secondary indices for text search, as a temporary measure. We are in the process of moving our searchable text off of Riak altogether. As this bug report shows, our use case hasn't been well supported by Riak.\n. ",
    "quantumpotato": "Where do I look up what to change it to?\n. ",
    "1100110": "Sorry! I confused my attempts with the Ubuntu Packages.  The Ubuntu gives the (expected) libc error, the Debian package gives a \"Error: Dependency is not satisfiable: libssl0.9.8 (>=0.9.8m-1)\"\n. Ah, I see...  I've updated my system partially to unstable.\nThis is the libssl I have: libssl1.0.0:amd64 1.0.1e-2\nOk, I'm retarded. I can fix that. Thanks for the help!\n. ",
    "abolibibelot": "Actually this is not only search-related, a multi-GET for the plain kv part would be nice too.\n. ",
    "remialvado": "We did not create a pull request for this one last year but we did implement the first option and it works quite well. cc @awetzel\n. ",
    "ksauzz": "@jj1bdx Yes. Changing the flag to true is required to run yokozuna. This modification is to just show riak user the existence of the option. Unfortunately, there is no method to know how to run yokozuna except reading source code now. So I thinks the option should be revealed on the configuration file like riak_control. Also we should update 'Getting Started' on README of yokozuna after the merging.\n. Thanks. ok, I'll try to update that.\n. Thanks. I'll also apply the changes to riak-ee.\n. Sorry for the confusion. I tweaked some descriptions after your +1. thanks.\n. Thanks! I made PR on riak-ee and merged it.\n. Looks nice!  :+1: \n. ",
    "DeadZen": "I believe @matthewvon mentioned this was heavily optimized for Riak's 1.0 use-case. We are well beyond that now (two years now with 3.0 on the roadmap), has there been anything more substantial than a cursory glance of the contrasts in the implementations? Maybe some Basho benchmarks?; I think the appropriate response here is quantitative because I fail to see how multiple writers per partition, not favoring reads over writes and easements on compaction are use cases that are not optimized for Riak... Understandably, multiple leveldb databases are used for partitions, but is that actually a comparably efficient strategy compared to the solutions provided here?; If so, then what is that evidenced by? Another aspect was write throttling, as I'm aware put fsms are throttled at an application level with sidejob so it seems the logic is already being re/placed outside of leveldb to provide selective back pressure to readers/writers (possibly even ironically linked to overload situations cited by some of the solutions implemented in @rescrv hyperleveldb repo and mentioned on http://hyperdex.org/performance/leveldb/). \nSo I'm really curious, How's this story looking today? \n. ugh --datatype counter. Did you do a eleveldb:repair to resolve this?\n. ",
    "btv": "Thank you Jared.\nCan I assist at all by running tests? Is there a test suite to run?\n. So I came across some compile errors with 1.3.2 and R16B01. Should I post them here or would you like new bug submitted (after checking if they are already submitted of course ;) )?\n. Thank you Jared for the information. Arch (and fedora 19) seemed both to have moved to R16. I'll have to find another way to play with Riak while I patiently wait for R16 compatibility.\nI wouldn't mind helping out, but my Erlang & C skills are pretty weak, not sure where I could be of help.\n. Thank you for the quick response and the correct answer.\n. ",
    "cdahlqvist": "Rolled back my change to rebar.config to original 1.3.2 value.\n. It would be useful to also have authorization for list_keys, list_buckets and secondary index queries as well as for the ability to run mapreduce queries.\n. busy_dist_port messages indicates the buffers for communication between nodes are filling up, disrupting communication. This is usually due to either the +zdbbl parameter being set too low in the vm.args file or that there are large objects in the cluster causing problems.\nVersion 1.4.8 added logging of large objects, so if you are using this version any large objects should be visible in the logs. It is generally recommended to not store objects larger than 2MB in Riak. If you do not see any messages about large objects, I would consider increasing the +zdbbl parameter and see if the timeout issues persist once the busy-dist_port messages have been eliminated.\n. ",
    "UtahDave": "Thanks for the quick fix!\n. ",
    "MJDSys": "Everything is work great now!  Thanks for fixing it.\n. ",
    "xpe": "1.4.0 release notes are here: https://github.com/basho/riak/blob/master/RELEASE-NOTES.md (for reference)\n. +1 I haven't dug into the technical detail above, but this part resonates with my infrastructure needs:\n\nSince any Riak node can handle any request, we should be able to advertise the existence of other nodes to clients so that they can reconfigure automatically, spreading load more evenly among the Riak nodes. This can reduce operational burden, because applications will not need to be restarted to take advantage of a grown cluster. \n. \n",
    "radix": "Hi, I'm just a curious observer learning about CRDTs, but I'm confused about something.\nYou mention that the literature describes OR-Set as two sets of pairs of (e, unique), but the INRIA paper actually describes it as only one set of pairs of (e, unique) (see Specification 15).\nIs there another definition of OR-Set that I'm missing?\nPardon my annoying question...\n. Okay, I was wondering if it was about the op-based vs state-based implementations. Your implementation sounds pretty clever. Thanks for the explanation!\n. ",
    "lenary": "@radeex our two sets are essentially 1) a set of (elem, token) for additions, and 2) a set of (elem, token) for removals (copies of the ones in the addition set if something is removed). We get the exact same semantics, without having to track when things were added and removed if we do it this way, and just unioning the set on a merge. This is the simplest implementation. \nI'd also point out that Spec 15 isn't state-based OR-Set, which is what we implemented, it's an op-based OR-Set. We've avoided op-based CRDTs for various reasons, essentially feeling that state-based give a simpler model for us to program with underneath.\nHowever, It turns out, @russelldb is a genius, and has worked out how to implement a set crdt with OR-Set semantics, that can track the causality of when something was added or removed enough that we only have a single tracking \"set\", and merges can work out how to do the right thing. It's called riak_dt_orswot, and is in the develop branch of the basho/riak_dt repo. \n. ",
    "maxsz": "Hi, are you people aware of this work? http://run.unl.pt/bitstream/10362/7802/1/Sousa_2012.pdf\n. ",
    "coderoshi": "Concerning user groups, I'd vote yes. If a group of permissions could be bundled (aka roles), then users could be assigned to a group/role rather than granted/revoked permissions individually. This could prove helpful, not only in implemented RBAC, but also reduce the complexity of defining multiple users with similar complex roles.\n. @Glagnar This is a different sort of security altogether. If a box itself is compromised, the user can simply give themselves any permissions they want via riak-admin.\n. No this is not possible. All Riak nodes are equivalent.\nOn Nov 12, 2013 3:31 AM, \"Glagnar\" notifications@github.com wrote:\n\nIs it possible to perhaps setup RIAK in a unidirectional replication\nmanor. I.e. A is master, and B & C are slaves. This means that it does not\nmatter if B or C are compromised. Then 3 clusters could be set up, one\nwhere in turn A, B or C is master. A client would then be able to detect if\none master had been compromised, by looking at the difference between the\nthree clusters.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/basho/riak/issues/355#issuecomment-28286317\n.\n. Should note that this first requires https://github.com/basho/yokozuna/pull/166 because of yokozuna.schema\n. +1\n. After painful deliberation, it looks like we're back to the \"Riak Search\" name.\n. @rzezeski https://github.com/basho/riak_ee/pull/220\n. @seancribbs ugh, bad clipboard pasting. Fixed issue name.\n\n@jj1bdx we do have in documentation how to change the ulimit (http://docs.basho.com/riak/latest/ops/tuning/open-files-limit/)\n@jaredmorrow this is really just a warning, it doesn't affect the startup of the app. imo we should change it to a value we actually suggest in production, rather than making increments. That said, any other value is fine, just as long as it's a realistic minimum, which 4k isn't.\n. hold your horses @jaredmorrow!\n. +1 67fb35b\n- [x] Manual test\nIt functions, but one nice-to-have would be a simple message, even just a printout like \"Switching interfaces to new search\".\n. +1 95bd660\nI'm :thumbsup: on this as-is, but there are other undocumented commands. @jrwest mentioned a couple, but there are missing in total:\n- aae-status\n- bucket-type\n- diag\n- downgrade-objects\n- reformat-indexes\n- repair-2i\n- security\n- transfer-limit\n. ",
    "glassresistor": "+1\n. ",
    "bkerley": "\nCertificate - The client provides a certificate, signed by the same CA as the server's certificate. If this certificate is validated and the common-name matches the requested username, the user is authenticated.\n\nThis should include a configurable Certificate Revocation List; otherwise untrusted clients can't be removed without basically starting the CA from scratch.\n. Looks okay, do the tests pass?\n. > Also, as of 2.0 we are moving all of the official Riak clients to only support the PB interface. HTTP is still supported, but anyone using an official Basho client will not be using it as the protocol for communicating with Riak.\nThe Python client (/cc @seancribbs ) will still support HTTP for the time being, as some Python client users have already built considerable security (authentication, auditing) infrastructure around HTTP.\n. ",
    "peschkaj": "+1 \nGroups are important, especially in the LDAP/ActiveDirectory world.\n. I don't see any large problems in client implementation, either. I'm going to guess that the .proto implementation will include an optional field for type which means sane protocol buffers clients shouldn't have to make any changes or do any version detection dance.\nBarring a sudden introduction of new PBC methods, this looks good to me.\n. ",
    "aphyr": "There are fundamentally three modes for TLS' authentication model:\n1. Assume you create a separate client key and certificate for each user. Both client and server verify each other's certificate. These channels are secure against MITM attacks, even where the client's CA chain is tampered with. Completing the TLS handshake proves the client's identity to Riak. A username and password is not required.\n2. Assume clients generate their own certs, and keep the server's CA chain on hand to verify the server's certificate during channel negotiation. These channels are secure against MITM attacks so long as an attacker cannot manipulate the client's CA store. A username and password is still required to authenticate the channel, since the client is anonymous.\n3. Assume totally anonymous mode: neither the client nor the server are authenticated. This mode is trivially vulnerable to MITM attacks. Since usernames and passwords are required in this case, attackers can readily capture credentials and impersonate any user.\nI recommend either 1.) fully authenticated or 2.) server-authenticated TLS channels. This mandates the use of a certificate store on each client. While you're going to that trouble, it might make sense to also generate and store client certificates as well--and indeed, the proposal requires the secure storage of client keys and certificates; which means you'll need a key distribution scheme for clients.\nGiven the presence of client secure storage for client keys and certs, you might as well encode the user credentials in the certificate directly. This removes the need for passwords and their secure storage on the server, which reduces the attack profile. It also removes the need for a separate username/password auth channel in the Riak protocol. That'd make it simpler for client maintainers to add auth support to their clients, since they can rely on the TLS protocol to do the work for them. Clients only have to store/configure [key, cert], instead of [key, cert, username, password]. User access can be cancelled via the usual CRL techniques.\n. @Glagnar: I doubt you'll satisfy that property in any major distributed database without end-to-end cryptographic verification of writes by both all servers and all clients. As an example, take a look at what's required to build http://www.pmg.csail.mit.edu/bft/castro99correctness-abstract.html\n. @Glagnar @tarcieri Note that Tahoe-LAFS does not provide robustness to a single compromised gateway or client node; only storage nodes.\n. Definitely use the Jepsen new branch; it'll make your life waaaay easier as far as setting up the DB goes. There's a riak installer in there already IIRC.\n. ",
    "randysecrist": "Will the ACL style permissions preclude / (make it difficult) to use a more capability driven security model (oauth scopes) in a future phase?  Has oauth been discussed?\n. @tarcieri I like your work on keyspace, and it pairs with what I was going for when I asked about a capability based model a bit earlier.  +1 for this.\n. I think that is the default value which is quorum.  (n/2 + 1). ",
    "aggress": "How would this work with MDC?\n. @Vagabond I was thinking more along the lines of will users/roles created in cluster a) be replicated over to cluster b) or will they need to be set up individually and how might things work with such things like cascading writes?\n. How might commit hooks be handled? stopping user a from using a commit hook that updates a bucket only user b has access to\n. ",
    "tarcieri": "Something Riak might consider is a capability-based security model for granting access to buckets. I think capability-based security could fit extremely well with Riak's key/value storage model and have done a bit of work in this space.\nUnder this model, authentication could be handled using whatever mechanism is desired (e.g. mutual TLS), but to authorize access to a particular bucket, the client would need to present a bucket-specific token, which could actually be a combination of cryptographic keys (known as a crypto-capability model).\nI've implemented a generic key/value store encryption system which works with Riak among other key/value stores here, if you're interested in seeing a real-world example of what I'm describing. My scheme encrypts both keys and values, allows data to be accessed using only encrypted keys, and allows clients to decrypt the key names if so desired:\nhttps://github.com/cryptosphere/keyspace\nThe best part of this approach is that it has minimal impact on Riak. In fact, encryption is orthogonal, and something only clients would have to support. The only thing that would have to be added to Riak itself is a digital signature check (along with a timestamp check to prevent replay attacks) to ensure values being written are authentic.\n. Sad to hear that :(\nI buy the familiarity argument, but that's really the only thing ACLs have going for them over capabilities. Since capabilities solve the AuthZ problem, you can still use MTLS to solve AuthN and revoke access that way. Audit logging can be used to spot abuse of capabilities.\nRelevant: Zed Shaw - The ACL Is Dead\n. Haha, sorry about that. But I hope it drives home that ACLs are in an uncanny valley between a capability based system and Turing-complete code for providing AuthZ.\nWaterken Web describes some of the tradeoffs of capabilities vs ACLs:\nhttp://waterken.sourceforge.net/\nYou might also take a look at how Tahoe-LAFS implements \"writecaps\" and \"readcaps\" for its mutable files. You wouldn't need anything so elaborate, just a digital signature:\nhttp://eprint.iacr.org/2012/524.pdf\nTahoe ends up providing something that looks an awful lot like an encrypted version of Riak, sans many of the features that make Riak compelling as a database (read repair, vector clocks, 2I, etc)\n. @Vagabond that's not a question I can answer until you have defined a threat model. Only then can you enumerate potential attacks and choose defenses.\nI can perhaps enumerate why ACLs don't work in practice with an example threat model:\nThreat: We want to give Alice, but not Mallory, AuthZ to X even though both Alice and Mallory can both AuthN to the service providing X and Alice and Bob are conspirators\nCapability attack scenario: Alice gives Mallory the capability to access X. Mallory can then access X. Our audit logs reflect Mallory accessing X\nACL attack scenario: Alice downloads X and gives it to Mallory. Mallory now has X. Our audit logs reflect Alice accessing X, not Mallory. Now we have the problem that Alice is authorized to access X and thus this may appear to be normal behavior, combined with the fact that Mallory gaining access to the content is not reflected in the audit logs.\nIn the end the result is the same, with some caveats: In the capability scenario, we see Mallory accessing the resource illicitly, but don't learn that Alice is a conspirator. In the ACL scenario, we don't learn about Mallory's involvement at all, as it appears that Alice accessed the resource. In the ACL scenario, Alice's behavior in the audit logs looks \"normal\", because Alice is authorized to access X. In the capability scenario, we can cross check the audit logs with our records of who should be able to access what, and determine that Mallory accessed X illicitly.\nThus, while capabilities are shareable, it's probably in Mallory's best interest to act as if they weren't and obtain X through a conspirator, lest his actions show up in the audit logs. In other words, while the fact capabilities are shareable appears to be disadvantageous, it's actually in the attacker's best interest not to take advantage of this fact, lest their actions appear in the audit logs. A sophisticated attacker will want to piggyback their attack on normal looking behavior as this will make it harder to detect.\n\nWhat does issuing a capability to a user look like, and how would it work\n\nThis is a fairly open-ended question as there are many ways that capabilities can be implemented. I can roughly detail what you could do with the sort of crypto-capabilities model implemented by Tahoe (although in this case I'm only describing how you'd ensure authenticity of data, not confidentiality. Tahoe provides both)\nIn general capability tokens are considered necessary and sufficient in and of themselves for accessing a particular resource. This doesn't preclude adding an additional mutual TLS layer or what have you to AuthN to the service.\nIdeally every part of the system has an associated set of capabilities. All data is individually, uniquely, and securely identifiable. So for starters: every bucket would have separate write/authenticate capabilities, if not every key.\nSo, at the time you create a bucket, a public and private digital signature key would be generated. The server would store the public key and use it to authenticate writes. The private key would allow new data to be written. The server would mandate that all writes be digitally signed (hopefully with a timestamp to prevent replay attacks)\nRequests to write would include some type of request parameter containing a digital signature produced client side by the holder of a private key for a particular bucket or bucket:key combination. The server would authenticate digital signatures before accepting the write.\n. @Vagabond\n\nThere's also the problem that buckets in Riak are 'created' in response to a key being written into them, if they don't have any custom properties, they only exist by virtue of their contents\n\nHow is this going to work in an ACL scenario? Won't you still have to declare what buckets are accessible to a given user?\nI guess more generally: do you plan to restrict the capability to create/modify buckets in any way as part of your security model? What security guarantees to do you intend to provide if you don't do this?\n. Here's another paper on capabilities you might consider reading:\nhttp://www.links.org/files/capabilities.pdf\n. @Glagnar if you really want a \"trust no one\" system where the compromise of a single node has zero impact on the rest of the grid, you might look at Tahoe-LAFS. It satisfies those properties (namely end-to-end cryptographic confidentiality and integrity of all content as @aphyr described): http://tahoe-lafs.org\n. @aphyr well yes, but ideally you separate the Tahoe nodes which provide storage service from the clients which are accessing the content, in which case only the clients see the capabilities/secrets, and the storage nodes are otherwise completely oblivious and see only ciphertexts. In such a deployment, the servers could be compromised without worry\n. ",
    "camshaft": "+1 capabilities. It's generally easier to understand and more secure. Managing ACLs becomes cumbersome very quickly from my experience.\n. ",
    "webhat": "@Vagabond rather than using a public/private key for each bucket you would use these only for en-/decrypting request for a synchronous key this key is used to en-/decrypt, in the same way TLS does it. Some information and references can be found in Key Management.\nIn the ACLs vs. capabilities discussion I've - perhaps wrongly - viewed them as a question of who has control: Does the user control what can be done with an object? Or can an object control what is done to it by the user?\n. ",
    "danostrowski": "+1 and thanks!\n. ",
    "Glagnar": "I am looking into Riak for project requiring a secure distributed. I need to make sure that if one node is compromised, i.e. server has been taken over, it will not be possible to break the entire cluster. For example, by prevention against altering permissions, or changing commit hooks. \nWill either be possible with Riak 2.0 ?\n. @coderoshi Thanks, I know. That was my exact source of worry. In a situation where the server is compromised, could an 'admin password' not solve this issue ? I.e. without password authentication, it should not be be allowed to change for example permissions within the cluster of nodes ?\n@aphyr I am not sure my issue has this requirement, as it is not initially the 'data' writes I am worried about.\n. Is it possible to perhaps setup RIAK in a unidirectional replication manor. I.e. A is master, and B & C are slaves. This means that it does not matter if B or C are compromised. Then 3 clusters could be set up, one where in turn A, B or C is master. A client would then be able to detect if one master had been compromised, by looking at the difference between the three clusters. \n. ",
    "sogabe": "I tried Security extensions with user/CIDR authentication. It seems to work fine. But I can't find how to remove Sources. Could anyone tell me when I should try most of functions?\n. Thanks, @Vagabond\n. ",
    "bakins": "I think having something akin to what couchbase has with moxi would be helpful. A lightweight \"proxy\" that understands the autoconfig -- and possibly authentication, as well. Could be used in place of haproxy in most of my use cases and I can continue to use \"dumb\" clients.\n. @seancribbs ruby, go, and Lua\n. ",
    "shino": "+1\n. :+1:\n. Oops, I mistakenly commented on the commit, no on the PR...\nPlease see comments for 1.3.2 change at https://github.com/basho/riak/commit/c4b226fa8430adad593c35e6680c2caf1ca46e69\n. +1 to merge after fixing one nitpick above. Nice work :exclamation: \n. :+1: \n. :+1: \n. :+1: \n. I'm digging the same symptom (but originated from riak_cs) with riak 1.4 branch.\nNot yet find the root cause, just a interim memo.\nWith single node cluster with ring size 16.\nAdded the following print debug to riak_core_coverage_fsm:\ndiff --git a/src/riak_core_coverage_fsm.erl b/src/riak_core_coverage_fsm.erl\nindex cd648bb..24dea91 100644\n--- a/src/riak_core_coverage_fsm.erl\n+++ b/src/riak_core_coverage_fsm.erl\n@@ -243,6 +243,12 @@ waiting_results({{ReqId, VNode}, Results},\n                                  req_id=ReqId,\n                                  timeout=Timeout,\n                                  process_fun = ProcessFun}) ->\n+case VNode of\n+    {1004782375664995756265033322492444576013453623296, 'dev1@127.0.0.1'} ->\n+        lager:log(warning, self(), \"********** waiting_results: ~p~n\", [Results]),\n+        ok;\n+    _ -> ok\n+end,\n     case ProcessFun(VNode, Results, ModState) of\n         {ok, UpdModState} ->\n             UpdStateData = StateData#state{mod_state=UpdModState},\nThen, not always, the log reports duplicated key (<<\"1415092978_118\">> for this case)\n by executing a streaming 2i request:\n10:19:07.565 [warning] ********** waiting_results: {{<0.857.0>,#Ref<0.0.0.28237>},<<\"riak-cs-gc\">>,[<<\"1415092979_102\">>,<<\"1415092978_118\">>]}\n10:19:07.566 [warning] ********** waiting_results: {{<0.857.0>,#Ref<0.0.0.28238>},<<\"riak-cs-gc\">>,[<<\"1415092978_118\">>]}\nThis seems to break the assumption by riak_kv_index_fsm that keys should be orderd.\nSo riak_kv_index_fsm and sms does not work correctly.\n. Thank you for update. It may be same or may be not, please let me add\nmy memo of digging around.\nMy case:\n- index is $key\n- paginated 2i with max_results=2\nIt seems for me that the point is accumulator (buffer) treatment in\nstoppable_fold of riak_kv_eleveldb_backend [1].  By calling\nFun(...), the inside buffer (riak_kv_fold_buffer) is updated, but\nthe exeption (throw(stop_fold)) can be thrown and the updated buffer\nis lost in catch clause. Then previous accumulator (buffer) is sent by\nFinishFun.\ncall / message sequence:\n- Acc is a buffer used in stoppable_fold\n- only one vnode worker is written for simplicity\n| index fsm                | Acc = buffer (size=2) | vnode worker        |\n|--------------------------+-----------------------+---------------------|\n|                          |                       | push k1 to buffer   |\n|                          | [k1]                  |                     |\n|                          |                       | push k2 to buffer   |\n|                          | Fun(k2, [k1])         |                     |\n|                          | <- ! [k1, k2]         |                     |\n| process_results([k1,k2]) |                       |                     |\n| [the vnode suffices      |                       |                     |\n|  max_results]            |                       |                     |\n| -> ! stop_fold           |                       |                     |\n|                          | throw(stop_fold)      |                     |\n|                          | (in riak_kv_vnode:    |                     |\n|                          |  result_fun_ack/2)    |                     |\n|                          |                       | FinishFun([k1])     |\n|                          |                       | (flush buffer [k1]) |\n| process_results([k1])    |                       |                     |\n[1] https://github.com/basho/riak_kv/blob/1.4/src/riak_kv_eleveldb_backend.erl#L798\n. I think this is fixed by https://github.com/basho/riak_kv/pull/1155\n. +1\n. +1\n. It seems that [1] fixed this bug. The commits are included in riak_core's\n2.0 and 2.1 branches, but not yet included in riak releases 2.0.6 or 2.1.1.\n\nAt the last commit hash of the PR\n```\n% git checkout 9b205cf\n% ERL_LIBS=deps erl -pz ebin\nErlang R16B02_basho9 (erts-5.10.3) [source] [64-bit] [smp:8:8] [async-threads:10] [hipe] [kernel-poll:false]\nEshell V5.10.3  (abort with ^G)\n1> riak_core_bucket:append_bucket_defaults(\n      riak_core_bucket_type:defaults(default_type)). %% Line 67 of riak_core_app.erl\nok\n2> proplists:get_value(dvv_enabled, element(2,\n      application:get_env(riak_core, default_bucket_props))).\nfalse\n```\nAt the parent commit hash of the first commit of the PR\n```\n% git show --parents baa1c0c | egrep commit\ncommit baa1c0c399446ddf8d33ecf9be3577dcd892a601 cbca6176806370984cff71a8e156c2529dc10dae\n% git checkout cbca6176806370984cff71a8e156c2529dc10dae\nPrevious HEAD position was 9b205cf... Restructure default proplist generation to be less ugly, update comments\nHEAD is now at cbca617... Bumped exometer_core to pick up meck 0.8.2\n% ./rebar compile skip_deps=true\n% ERL_LIBS=deps erl -pz ebin\nErlang R16B02_basho9 (erts-5.10.3) [source] [64-bit] [smp:8:8] [async-threads:10] [hipe] [kernel-poll:false]\nEshell V5.10.3  (abort with ^G)\n1> riak_core_bucket:append_bucket_defaults(\n       riak_core_bucket_type:defaults()).    %% Line 67 of riak_core_app.erl\nok\n2> proplists:get_value(dvv_enabled, element(2, application:get_env(riak_core, default_bucket_props))).\ntrue\n```\n[1] https://github.com/basho/riak_core/pull/765\n. +1\n. :+1: \n. s/\u7570\u5e38/\u4ee5\u4e0a/\n. s/\u6307\u5b9a\u3055\u308c\u305f\u6570\u306e\u30d7\u30e9\u30a4\u30de\u30ea\u3060\u3051\u304c\u30aa\u30f3\u30e9\u30a4\u30f3\u304b\u3069\u3046\u304b\u3092/\u6307\u5b9a\u3055\u308c\u305f\u6570\u306e\u30d7\u30e9\u30a4\u30de\u30ea\u304c\u30aa\u30f3\u30e9\u30a4\u30f3\u304b\u3069\u3046\u304b\u3060\u3051\u3092/\n. s/vnode \u304c\u672c\u5f53\u306b\u751f\u304d\u3066\u3044\u308b\u304b\u3069\u3046\u304b\u307e\u3067\u306f/\u30d7\u30e9\u30a4\u30de\u30ea\u306e vnode \u304c\u672c\u5f53\u306b\u5fdc\u7b54\u3057\u305f\u304b\u3069\u3046\u304b\u307e\u3067\u306f/\n. \u3082\u3057PW=2\u3060\u3063\u305f\u5834\u5408\u3001\u3072\u3068\u3064\u306e\u30d7\u30e9\u30a4\u30de\u30ea\u3068\u3072\u3068\u3064\u306e\u30d5\u30a9\u30fc\u30eb\u30d0\u30c3\u30af\u306b\u66f8\u304d\u8fbc\u307f\u6210\u529f\u3059\u308c\u3070\u3001\n\u3082\u3046\u3072\u3068\u3064\u306e\u30d7\u30e9\u30a4\u30de\u30ea\u306b\u66f8\u304d\u8fbc\u307f\u5931\u6557\u3067\u3082\u6210\u529f\u306e\u5fdc\u7b54\u3092\u8fd4\u3057\u3066\u3044\u307e\u3057\u305f\u3002\n. s/\uff08\u4ed6\u306ewrite\u3092\u59a8\u3052\u3066\u30ec\u30d7\u30ea\u30ab\u3092\u6d88\u3057\u305f\u3053\u3068\u306b\u306a\u308b\uff09/\uff08\u305d\u306e\u9593\u306b\u4ed6\u306e\u66f8\u304d\u8fbc\u307f\u3084\u3001\u4fee\u5fa9\u4e0d\u80fd\u306a\u30ec\u30d7\u30ea\u30ab\u969c\u5bb3\u304c\u306a\u3044\u9650\u308a\uff09/\n\u5f8c\u8005\u304c\u3088\u304f\u308f\u304b\u3089\u306a\u3044\u3051\u3069\u3001\u30d7\u30e9\u30a4\u30de\u30ea\u30ce\u30fc\u30c9\u306e\u30c7\u30a3\u30b9\u30af\u304c\u6b7b\u3093\u3067\u307e\u3063\u3055\u3089\u3067\u8d77\u52d5\u3057\u306a\u304a\u3057\u305f\u306e\u3092\u60f3\u5b9a\u3057\u3066\u3044\u308b\u6c17\u304c\u3059\u308b\u3002\n. s/\u3082\u3067/\u3082/\n. ",
    "gideondk": "Very good addition IMHO. Next to easier bucket configuration, a extra dimension really helps in structuring equally named buckets for different scenario's (instead of prefixing it yourself). \nDon't see any large problems in client implementation though... :-)\n. ",
    "brunogirin": "Is there any particular reason why link walking will be restricted to the default bucket type?\n. @jrwest Thanks for the answer. If there is no technical blocker, that's brilliant and it makes complete sense to me that you would want to limit the scope of a first delivery.\nFrom a point of view of what it would solve for me, it would make the multi backend concept more usable by being able to configure one backend for one bucket type and a different backend for another bucket type. I also like @mrallen1 's use case.\n. ",
    "uberbrady": "These don't really feel like types to me - more like \"bucket groups\". Separately, there seem to be some optional settings you can put in for all buckets that are in one group.\nI don't know if I'm just bikeshedding here - but I feel like you could just allow for some \"/\" characters in a bucket - a sort of simple bucket hierarchy - and get most of what you need.\nBuckets that aren't in any group - (normal-looking Riak URL's) would maybe stay the same; and grouped buckets would just have another slash in the bucket part of the URL.\ne.g.\nGET /riak/bucket/key\nwould be a GET for an object in a classic 'default' bucket\nGET /riak/mygroup/bucket/key\nwould be a GET for an object in a grouped one.\nI'm just looking at this from the point of someone who doesn't know the internals of Riak at all - so there could be all kinds of terrible things wrong with my counterproposal.\nAnd ultimately I don't think it matters too much. But I do like the fact that the URL's I'm fetching things from stay looking pretty much the same, even if I use the new features.\n. ",
    "mrallen1": "We have hit one of the problems described with the current design which is: we have some data where \"last write wins\" is fine and other data where \"allow_multi: true\" is the right way to handle things. So I support this proposal because it sounds like we can easily share the riak infrastructure even though we might want mixed behaviors about concurrent writes within the same cluster.\n. On my system, /usr/include/security/pam_appl.h is part of the pam-devel package. Looks like on 12.04 Ubuntu that's libpam0g-dev.\n. See https://github.com/basho/riak_kv/pull/1141\n. This sounds like a case where it would be simplest to send a message using a timer with an explicit message using erlang:send_after/3 and then maybe manipulating the FSM state from the info message properly.\n. Or, I suppose another option would be to piggy-back on the management tick message.\n. This is the relevant bit of the crash report here: {error,{file_error,\"./data/cluster_meta/manifest.dets\",eacces}}\nIt's a permissions issue of some kind - did you build the whole system as the root user?\n. According to the error, the user account that Riak is started under does not have permission to read or write the ./data/cluster_meta/manifest.dets file.  Is there some kind of mismatch between the user account that's starting the riak application and the file system location where it's being run out of?\n. This is the relevant part that's failing. Could you try running dets:open_file(test, [{file,\"./data/cluster_meta/manifest.dets\"}]). from the erlang shell?  Just type erl from your terminal.  You should see a 1> prompt.  Enter the dets command at 1>.\nIf that fails, try dets:open_file(test, [{file,\"/tmp/foo.dets\"}]). and see what happens.\n. OK, does the ./data/cluster_meta/manifest.dets file exist on disk? If so, what does ls -l ./data/cluster_meta/manifest.dets show?\n. Hopefully there's no data you need to keep around?  If so, you can delete the directories and try to start Riak again.\n. Yes data/*\n. ",
    "lafka": "I'v been doing some minor testing and have some questions:\n- Currently there is no way to delete a bucket type, is the only way around this to  delete the datadir?\n- Calling riak_core_bucket_type:reset/1 will only set the default values for some keys, for instance yz_index is not altered. Should reset be equivalent of calling \n- When using riak-admin, should some bucket properties be read only? (like active should not be changeable through bucket-type update). It's already there for datatype.\n- Is there a way to store custom custom attributes in the bucket properties? i.e. I would like to store information about the structure of the keys in this bucket. \n. @jrwest the custom attribute is limited by list_to_existing_atom/1 in riak_kv_wm_utils:erlify_bucket_prop. The same conversion is used in HTTP API, so might not be feasible to use list_to_atom/1.\n. ",
    "elight": "While I realize what the error indicated does not appear to correlate with\n32bit Erlang, per @cmeik's advice, I rebuilt my Erlang to 64bit and the\ncompilation error vanished.\nWhy this should be makes no sense to me either\u2026\nThe missing header files were present on my machine; however, they were\nunder significantly different paths than any supplied in the include paths\nto GCC.\nOn Sun, Aug 18, 2013 at 9:34 AM, Jared Morrow notifications@github.comwrote:\n\nThis is not related to 32bit Erlang. We do not support 32bit, but you can\nstill build Riak on 32bit just fine. This is a general problem people ran\ninto finding libraries installed by XCode.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/basho/riak/issues/364#issuecomment-22830545\n.\n\n\nEvan Light\nhttp://evan.tiggerpalace.com\n@elight\n. ",
    "ramunasd": "Riak does not supports binary data in javascript map reduce functions.\nYou can stream/get all bucket keys using special functions. I.e. python api - http://basho.github.io/riak-python-client/bucket.html#listing-keys\n. @jaredmorrow I have tried much higher ulimit, but without any luck. I think problem occurs when some files(anti_entropy in this case) is corrupted and riak loops infinitely on trying to achieve lock for these files.\n. More info on that:\nthis node shows only itself in riak-admin member-status while other nodes shows all nodes(including problematic one).\nriak-admin ringready from another node gives error [...]list different partition owners.\nAfter riak-admin cluster join... problematic node really starts hinted handoffs. No more errors Handoff receiver for partition undefined exited after processing 0 objects.\nHuge traffic between nodes was due to AAE.\n. Except info about Handoff receiver for partition undefined exited after processing 0 objects i'm not getting any error.\n. ",
    "westhand": "I forgot:\nI have several ideas for a workaround, but which one is recommended?\nSymlinking sysctl to a path?\nEditing /etc/profile?\nAdding a home directory to the riak user and fix $PATH in .bashrc?\nAliasing?\nRunning as root? cough\n. Hi Brian\nsorry, you posted while I was writing my Annex.\nThanks for the answer.\n. ",
    "vinoski": "This is caused by trying to build Riak with a 32-bit Erlang rather than a 64-bit Erlang. Try rebuilding with a 64-bit Erlang and you'll be successful.\n. I've built and run Riak on Mavericks and only eleveldb changes are needed. See eleveldb/pull/76 and leveldb/pull/104.\n. ",
    "jgnewman": "My bad, didn't realize I had to change it in 2 places.  Doing it now.\nriak_ee PR #179\n. ",
    "roncemer": "This same problem also occurs if you use the new Riak counters functionality.  Set up a bucket with allow_mult=true.  Increment some counters in that bucket.  Then try to do a map/reduce query on them.  You get this error.\nIf you eliminate the JavaScript map and reduce functions, you can get the results, but the results are just bucket,key pairs without the actual counter values.  Useless.\nThis bug needs to be fixed, like NOW, and a new release issued.  It's a major show-stopper for counters.\nNo storage engine should be non-binary safe for both keys and values.  Riak is no exception.\nPeople are getting sick of Riak's second-rate JavaScript support.  JavaScript should be the PRIMARY language for map/reduce, and should be easier to use and more efficient than erlang.  Erlang is an obscure language which nobody wants to learn.\nLet's get this fixed right away, so people can use counters in Riak.\n. Actually, it turns out that Riak counters seem to suffer from the same problems as other vector clock-based counter implementations.  Simultaneous increments through different nodes on the same counter result in under-counts or over-counts (under-counts in the case that I've tested).\nStart up a 6-node Riak cluster, set allow_mult=true on a bucket.  Pick a new counter key which doesn't exist yet, and write a multi-threaded or multi-process CLI app to connect to a random node in the cluster and increment the counter for that key 100 times on each node.  Do it in, say, 50 threads or 50 processes.  So each thread or process connects to a random Riak node and increments the same key 100 times.  100 x 50 = 5,000.  All of the threads/processes start at the same time and run until they've each done 100 increments on the key.  After they finish, wait a while for the dust to settle while the eventual consistency to kicks in.  The final count will not be 5,000.\nActually, the way counters are implemented is unreliable by nature.  Seems like a better approach would be for each node to keep track of how many times a given key was incremented ON THAT NODE ONLY.  Then, the total count would be the sum of the counts for all nodes in the cluster.  Regularly, the nodes would share with each other what their per-node counts are for each recently incremented counter.  Once a node's per-node count has been broadcast to other nodes, it can zero that count back out and start accumulating it again.  Whichever node is primarily responsible for the vnode on which the counter lives, would be the responsible for regularly fetching and summing the per-node counts.  You'd never lose or duplicate an increment that way, unless a node went down before that increment got copied to the other replicas for that key.\nBut I digress.\nThe issue which appears to be causing the bad_utf8_character_code error must have something to do with either Erlang's inability to manage binary strings, or perhaps the Riak devs are accidentally treating binary data such as that which is stored for counters, as UTF8 strings.  That would be a big mistake.  Since Riak does not dictate that the data for a key must be in a particular format, it doesn't make sense for Riak's mapreduce to throw errors when it encounters a key which contains binary data.\nIn reality, it Riak would have been a much better product had they avoided Erlang altogether, given first-rate support for JavaScript mapreduce and queries the way Mongo does, and stored buckets in separate directories in order to make it quick and easy to drop a bucket.  Nearly everything about Riak's design makes it much more painful to use than it should be.  The big benefits with Riak are the ring topology, no single point of failure, and automatic/tuneable replication.\n. I should add that I've been using Riak for a big-data project for several months now, and I like it.  But it could be much better if these issues were resolve instead of kicked down the road -- or making excuses as to why we don't want to solve them.  Let's just do this.\n. ",
    "Micka33": "I have enabled Riak Search Config, after some fail using the search function, I wanted to list all my buckets to be sure I was requesting the good one.\nThis is what I got\nirb(main):004:0> client = Riak::Client.new :solr => \"/solr\"\n=> #<Riak::Client [#<Node 127.0.0.1:8098:8087>]>\nirb(main):005:0> client.search \"user\" \"email:micka3@email.com\"\n=> {\"num_found\"=>0, \"max_score\"=>0.0, \"docs\"=>[]}\nirb(main):006:0> client.search \"user\" \"username:micka3\"\n=> {\"num_found\"=>0, \"max_score\"=>0.0, \"docs\"=>[]}\n....\nirb(main):010:0> client.buckets\nRiak::Client#buckets is an expensive operation that should not be used in production.\n    (irb):10:in `irb_binding'\n    /usr/local/Cellar/ruby193/1.9.3-p448/lib/ruby/1.9.1/irb/workspace.rb:80:in `eval'\n    /usr/local/Cellar/ruby193/1.9.3-p448/lib/ruby/1.9.1/irb/workspace.rb:80:in `evaluate'\n    /usr/local/Cellar/ruby193/1.9.3-p448/lib/ruby/1.9.1/irb/context.rb:254:in `evaluate'\n    /usr/local/Cellar/ruby193/1.9.3-p448/lib/ruby/1.9.1/irb.rb:159:in `block (2 levels) in eval_input'\n    /usr/local/Cellar/ruby193/1.9.3-p448/lib/ruby/1.9.1/irb.rb:273:in `signal_status'\n    /usr/local/Cellar/ruby193/1.9.3-p448/lib/ruby/1.9.1/irb.rb:156:in `block in eval_input'\n    /usr/local/Cellar/ruby193/1.9.3-p448/lib/ruby/1.9.1/irb/ruby-lex.rb:243:in `block (2 levels) in each_top_level_statement'\n    /usr/local/Cellar/ruby193/1.9.3-p448/lib/ruby/1.9.1/irb/ruby-lex.rb:229:in `loop'\n    /usr/local/Cellar/ruby193/1.9.3-p448/lib/ruby/1.9.1/irb/ruby-lex.rb:229:in `block in each_top_level_statement'\n    /usr/local/Cellar/ruby193/1.9.3-p448/lib/ruby/1.9.1/irb/ruby-lex.rb:228:in `catch'\n    /usr/local/Cellar/ruby193/1.9.3-p448/lib/ruby/1.9.1/irb/ruby-lex.rb:228:in `each_top_level_statement'\n    /usr/local/Cellar/ruby193/1.9.3-p448/lib/ruby/1.9.1/irb.rb:155:in `eval_input'\n    /usr/local/Cellar/ruby193/1.9.3-p448/lib/ruby/1.9.1/irb.rb:70:in `block in start'\n    /usr/local/Cellar/ruby193/1.9.3-p448/lib/ruby/1.9.1/irb.rb:69:in `catch'\n    /usr/local/Cellar/ruby193/1.9.3-p448/lib/ruby/1.9.1/irb.rb:69:in `start'\n    /usr/local/Cellar/ruby193/1.9.3-p448/lib/ruby/gems/1.9.1/gems/railties-3.2.14/lib/rails/commands/console.rb:47:in `start'\n    /usr/local/Cellar/ruby193/1.9.3-p448/lib/ruby/gems/1.9.1/gems/railties-3.2.14/lib/rails/commands/console.rb:8:in `start'\n    /usr/local/Cellar/ruby193/1.9.3-p448/lib/ruby/gems/1.9.1/gems/railties-3.2.14/lib/rails/commands.rb:41:in `<top (required)>'\n    script/rails:6:in `require'\n    script/rails:6:in `<main>'\nRiak::HTTPFailedRequest: Expected 200 from Riak but received 500. <html><head><title>500 Internal Server Error</title></head><body><h1>Internal Server Error</h1>The server encountered an error while processing this request:<br><pre>{error,{exit,{ucs,{bad_utf8_character_code}},\n             [{xmerl_ucs,from_utf8,1,[{file,\"xmerl_ucs.erl\"},{line,185}]},\n              {mochijson2,json_encode_string,2,\n                          [{file,\"src/mochijson2.erl\"},{line,186}]},\n              {mochijson2,'-json_encode_array/2-fun-0-',3,\n                          [{file,\"src/mochijson2.erl\"},{line,157}]},\n              {lists,foldl,3,[{file,\"lists.erl\"},{line,1197}]},\n              {mochijson2,json_encode_array,2,\n                          [{file,\"src/mochijson2.erl\"},{line,159}]},\n              {mochijson2,'-json_encode_proplist/2-fun-0-',3,\n                          [{file,\"src/mochijson2.erl\"},{line,167}]},\n              {lists,foldl,3,[{file,\"lists.erl\"},{line,1197}]},\n              {mochijson2,json_encode_proplist,2,\n                          [{file,\"src/mochijson2.erl\"},{line,170}]}]}}</pre><P><HR><ADDRESS>mochiweb+webmachine web server</ADDRESS></body></html>\n    from /usr/local/Cellar/ruby193/1.9.3-p448/lib/ruby/gems/1.9.1/gems/riak-client-1.4.2/lib/riak/client/net_http_backend.rb:58:in `block (2 levels) in perform'\n    from /usr/local/Cellar/ruby193/1.9.3-p448/lib/ruby/1.9.1/net/http.rb:1323:in `block (2 levels) in transport_request'\n    from /usr/local/Cellar/ruby193/1.9.3-p448/lib/ruby/1.9.1/net/http.rb:2672:in `reading_body'\n    from /usr/local/Cellar/ruby193/1.9.3-p448/lib/ruby/1.9.1/net/http.rb:1322:in `block in transport_request'\n    from /usr/local/Cellar/ruby193/1.9.3-p448/lib/ruby/1.9.1/net/http.rb:1317:in `catch'\n    from /usr/local/Cellar/ruby193/1.9.3-p448/lib/ruby/1.9.1/net/http.rb:1317:in `transport_request'\n    from /usr/local/Cellar/ruby193/1.9.3-p448/lib/ruby/1.9.1/net/http.rb:1294:in `request'\n    from /usr/local/Cellar/ruby193/1.9.3-p448/lib/ruby/gems/1.9.1/gems/riak-client-1.4.2/lib/riak/client/net_http_backend.rb:56:in `block in perform'\n    from /usr/local/Cellar/ruby193/1.9.3-p448/lib/ruby/gems/1.9.1/gems/riak-client-1.4.2/lib/riak/client/net_http_backend.rb:54:in `tap'\n    from /usr/local/Cellar/ruby193/1.9.3-p448/lib/ruby/gems/1.9.1/gems/riak-client-1.4.2/lib/riak/client/net_http_backend.rb:54:in `perform'\n    from /usr/local/Cellar/ruby193/1.9.3-p448/lib/ruby/gems/1.9.1/gems/riak-client-1.4.2/lib/riak/client/http_backend/transport_methods.rb:44:in `get'\n    from /usr/local/Cellar/ruby193/1.9.3-p448/lib/ruby/gems/1.9.1/gems/riak-client-1.4.2/lib/riak/client/http_backend.rb:213:in `list_buckets'\n    from /usr/local/Cellar/ruby193/1.9.3-p448/lib/ruby/gems/1.9.1/gems/riak-client-1.4.2/lib/riak/client.rb:179:in `block in buckets'\n    from /usr/local/Cellar/ruby193/1.9.3-p448/lib/ruby/gems/1.9.1/gems/riak-client-1.4.2/lib/riak/client.rb:470:in `block in recover_from'\n    from /usr/local/Cellar/ruby193/1.9.3-p448/lib/ruby/gems/1.9.1/gems/innertube-1.0.2/lib/innertube.rb:127:in `take'\n    from /usr/local/Cellar/ruby193/1.9.3-p448/lib/ruby/gems/1.9.1/gems/riak-client-1.4.2/lib/riak/client.rb:468:in `recover_from'\n    from /usr/local/Cellar/ruby193/1.9.3-p448/lib/ruby/gems/1.9.1/gems/riak-client-1.4.2/lib/riak/client.rb:321:in `http'\n    from /usr/local/Cellar/ruby193/1.9.3-p448/lib/ruby/gems/1.9.1/gems/riak-client-1.4.2/lib/riak/client.rb:138:in `backend'\n    from /usr/local/Cellar/ruby193/1.9.3-p448/lib/ruby/gems/1.9.1/gems/riak-client-1.4.2/lib/riak/client.rb:178:in `buckets'\n    from (irb):10\n    from /usr/local/Cellar/ruby193/1.9.3-p448/lib/ruby/gems/1.9.1/gems/railties-3.2.14/lib/rails/commands/console.rb:47:in `start'\n    from /usr/local/Cellar/ruby193/1.9.3-p448/lib/ruby/gems/1.9.1/gems/railties-3.2.14/lib/rails/commands/console.rb:8:in `start'\n    from /usr/local/Cellar/ruby193/1.9.3-p448/lib/ruby/gems/1.9.1/gems/railties-3.2.14/lib/rails/commands.rb:41:in `<top (required)>'\n    from script/rails:6:in `require'\n    from script/rails:6:in `<main>'irb(main):011:0>\n. ",
    "TJC": "In the case of my original error -- the cause was Erlang code creating buckets and keys with binary characters in them, that can not be converted to UTF8 in any valid way.\nI later discovered it's also a problem with the PBC interface in Java, as well as the HTTP interface in any language.\n. @russelldb  --- no, it is not OK to close this issue, because the issue is present on BOTH the java client AND the HTTP interface. They're two separate bugs.\nThe HTTP interface throws a 500 Error regardless of client for this bug.\n. @russelldb - There was some discussion on the mailing list around the java client issue, and apparently the issue is already fixed in the 2.0 branch, so I didn't create an issue on github for it. I am happy to do so if that helps though?\n. Re bug in HTTP API -- being unable to list your buckets surely counts as a bug?\n(Even if being unable to list or interact with keys isn't)\n. Jared - I haven't tested binary handling with the RJC 2.0 client yet, but the use of \"BinaryValue\" types to represent buckets and keys looks promising. I'll try and get time to setup a cluster in VMs and try it out later today.\n. @binarytemple, you wrote \"So the solution, in the presence of non-valid-utf8-named buckets or non-valid-utf8-named keys is to use the protocol buffers interface - as they can't be represented in valid JSON. This sounds reasonable to me.\"\nIn the two years since I created this ticket, I started using the PBC interface via the RJC via Scala, and can confirm that it does work with the non-unicode-buckets.\nIf you have multiple applications at play, some using PBC and some using HTTP, then a mistake in the PBC code (inserting non-unicode keys or buckets) will then cause the HTTP-based apps to suddenly completely fail. I see that sort of surprise failure as very undesirable, as it's the sort that could reasonably make it through testing and into production, and then bring your whole system down. (It didn't happen to us, but I could see how it could)\nI think you have a choice between which of these bugs you consider a bug:\na) HTTP client cannot access non-unicode-safe keys and buckets\nb) PBC client is allowed to create non-unicode-safe keys and buckets\nc) Riak server allows non-unicode-safe keys and buckets to be created\n. ",
    "binarytemple": "So the solution, in the presence of non-valid-utf8-named buckets or  non-valid-utf8-named keys is to use the protocol buffers interface - as they can't be represented in valid JSON. This sounds reasonable to me. Javascript Map/Reduce is also deprecated. Can this issue be closed now? \n. Interestingly, the python client doesn't seem able to list them either. \nc=riak.RiakClient(protocol='pbc',nodes=[{'host':'127.0.0.1', 'http_port':'10018','pb_port':10017}])\nc.get_buckets()\n```\nIn [29]: b=c.get_buckets()\n\nTypeError                                 Traceback (most recent call last)\n in ()\n----> 1 b=c.get_buckets()\n/usr/local/lib/python2.7/site-packages/riak/client/transport.pyc in wrapper(self, *args, kwargs)\n    194             return fn(self, transport, *args, kwargs)\n    195\n--> 196         return self._with_retries(pool, thunk)\n    197\n    198     wrapper.doc = fn.doc\n/usr/local/lib/python2.7/site-packages/riak/client/transport.pyc in _with_retries(self, pool, fn)\n    136                 with pool.transaction(_filter=_skip_bad_nodes) as transport:\n    137                     try:\n---> 63             bucketfn = lambda name: self.bucket(name)                                                      [0/1648]\n     64\n     65         return [bucketfn(bytes_to_str(name)) for name in\n/usr/local/lib/python2.7/site-packages/riak/client/init.pyc in bucket(self, name, bucket_type)\n    269\n    270         return self._buckets.setdefault((bucket_type, name),\n--> 271                                         RiakBucket(self, name, bucket_type))\n    272\n    273     def bucket_type(self, name):\n/usr/local/lib/python2.7/site-packages/riak/bucket.pyc in init(self, client, name, bucket_type)\n     59                     raise TypeError('Bucket name must be a string')\n     60             except UnicodeError:\n---> 61                 raise TypeError('Unicode bucket names are not supported.')\n     62\n     63         if not isinstance(bucket_type, BucketType):\nTypeError: Unicode bucket names are not supported.\nIn [30]: c=riak.RiakClient(protocol='pbc',nodes=[{'host':'127.0.0.1', 'http_port':'10018','pb_port':10017}])\nKeyboardInterrupt\n```\nIt might be worth raising an issue against the Python client if unicode bucket names are indeed supported. Thoughts? \n. +1 It would be very useful to identify specific MR jobs. I see instances where one person is running a bunch of MR without informing their teammates.\n. OSX .... \n[/basho/riak/dev%]sudo launchctl limit  maxfiles 65536 65536\n[/basho/riak/dev%]sudo ulimit -n 65500\n[/basho/riak/dev%]sudo ulimit -n 65535\n[/basho/riak/dev%]sudo ulimit -n 65536\n[/basho/riak/dev%]./dev1/bin/riak start\n!!!!\n!!!! WARNING: ulimit -n is 65535; 65536 is the recommended minimum.\n!!!!\nOn OSX Mavericks ulimit cannot be set to 65536, 65535 is the maximum the system will allow. \n. My PR from 24th Oct 2014 - https://github.com/basho/riak/pull/614 check ulimits are set in /etc/default/riak save someone a day sometime.\n. Until this gets added to a cuttlefish schema, the current way to add_paths is the following:\n\nThe new syslog style config system introduced with Riak 2.0 will also make use of an advanced.config if available to provide and/or override some settings.\nTo include Erlang paths:\n1.) Stop Riak with riak stop\n2.) Create a file called /etc/riak/advanced.config with the following contents where /tmp is replaced with your intended path\n[{riak_kv, [{add_paths, [ \"/tmp\" ]}]}].\n3.) Run riak chkconfig to confirm the syntax is valid\n4.) Start Riak with riak start\n. @cipy didn't you have a suggestion in the chat room, about controlling the size of the dumps?\n\n@jonmeredith would it require a patch to set erlang:process_flag(sensitive, true) before riak_core started up? \n. Is this going anywhere or should we just close out?\n. @jpease does this sound like a good idea? Originated in ticket 9129 - wasted two CSE days.\n. > tcp6       0      0 :::8093                 :::*                    LISTEN\nIt is bound and listening on the every interface, and shows up under IPV6.\nDepending on your OS/network config that may mean it is also available over\nIPV4. In any event it is not intended that you interact with that port\ndirectly, rather you should query Riak, and let it handle the marshalling,\nredirection, etc.\nBryan\nOn Sat, Dec 13, 2014 at 1:02 PM, Paige Thompson notifications@github.com\nwrote:\n\nlaptop etc # ps -aux | grep solr\nroot      9370  0.0  0.0   6844   860 pts/5    S+   11:55   0:00 grep --colour=auto solr\nroot      9933  0.2  5.3 3372060 429248 ?      Ssl  Dec11   6:25 /usr/lib/jvm//oracle-jdk-bin-1.7/bin/java -Djava.awt.headless=true -Djetty.home=/home/erratic/riak/rel/riak/bin/../lib/yokozuna-2.0.0-1-g6ac3a63/priv/solr -Djetty.port=8093 -Dsolr.solr.home=/home/erratic/riak/rel/riak/data/yz -DhostContext=/internal_solr -cp /home/erratic/riak/rel/riak/bin/../lib/yokozuna-2.0.0-1-g6ac3a63/priv/solr/start.jar -Dlog4j.configuration=file:///home/erratic/riak/rel/riak/etc/solr-log4j.properties -Dyz.lib.dir=/home/erratic/riak/rel/riak/bin/../lib/yokozuna-2.0.0-1-g6ac3a63/priv/java_lib -d64 -Xms1g -Xmx1g -XX:+UseStringCache -XX:+UseCompressedOops -Dcom.sun.management.jmxremote.port=8985 -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false org.eclipse.jetty.start.Main\nlaptop etc # netstat -an | grep LISTEN\ntcp        0      0 127.0.0.1:37806         0.0.0.0:*               LISTEN\ntcp        0      0 0.0.0.0:111             0.0.0.0:*               LISTEN\ntcp        0      0 127.0.0.1:9040          0.0.0.0:*               LISTEN\ntcp        0      0 0.0.0.0:8080            0.0.0.0:*               LISTEN\ntcp        0      0 0.0.0.0:4369            0.0.0.0:*               LISTEN\ntcp        0      0 0.0.0.0:113             0.0.0.0:*               LISTEN\ntcp        0      0 192.168.122.1:53        0.0.0.0:*               LISTEN\ntcp        0      0 127.0.0.1:46806         0.0.0.0:*               LISTEN\ntcp        0      0 0.0.0.0:22              0.0.0.0:*               LISTEN\ntcp        0      0 127.0.0.1:38327         0.0.0.0:*               LISTEN\ntcp        0      0 127.0.0.1:8087          0.0.0.0:*               LISTEN\ntcp        0      0 127.0.0.1:631           0.0.0.0:*               LISTEN\ntcp        0      0 127.0.0.1:9050          0.0.0.0:*               LISTEN\ntcp        0      0 127.0.0.1:34651         0.0.0.0:*               LISTEN\ntcp        0      0 127.0.0.1:60092         0.0.0.0:*               LISTEN\ntcp        0      0 127.0.0.1:36476         0.0.0.0:*               LISTEN\ntcp        0      0 127.0.0.1:50238         0.0.0.0:*               LISTEN\ntcp        0      0 127.0.0.1:8098          0.0.0.0:*               LISTEN\ntcp        0      0 0.0.0.0:8099            0.0.0.0:*               LISTEN\ntcp        0      0 127.0.0.1:33829         0.0.0.0:*               LISTEN\ntcp        0      0 0.0.0.0:51429           0.0.0.0:*               LISTEN\ntcp        0      0 0.0.0.0:39494           0.0.0.0:*               LISTEN\ntcp        0      0 127.0.0.1:3306          0.0.0.0:*               LISTEN\ntcp6       0      0 :::39405                :::*                    LISTEN\ntcp6       0      0 :::111                  :::*                    LISTEN\ntcp6       0      0 :::80                   :::*                    LISTEN\ntcp6       0      0 :::113                  :::*                    LISTEN\ntcp6       0      0 :::59633                :::*                    LISTEN\ntcp6       0      0 :::22                   :::*                    LISTEN\ntcp6       0      0 ::1:631                 :::*                    LISTEN\ntcp6       0      0 :::8985                 :::*                    LISTEN\ntcp6       0      0 :::443                  :::*                    LISTEN\ntcp6       0      0 :::8093                 :::*                    LISTEN\ntcp6       0      0 :::59040                :::*                    LISTEN\nunix  2      [ ACC ]     STREAM     LISTENING     9132     /tmp/runtime-erratic/kdeinit5__0\nunix  2      [ ACC ]     STREAM     LISTENING     10812    /tmp/runtime-erratic/klauncherXM4699.1.slave-socket\nunix  2      [ ACC ]     STREAM     LISTENING     10847    /tmp/.ICE-unix/4708\nunix  2      [ ACC ]     STREAM     LISTENING     10897    /tmp/pulse-QB9I7FKmGvxm/native\nunix  2      [ ACC ]     STREAM     LISTENING     10846    @/tmp/.ICE-unix/4708\nunix  2      [ ACC ]     STREAM     LISTENING     6940     /run/cups/cups.sock\nunix  2      [ ACC ]     STREAM     LISTENING     217541   @/tmp/dbus-c6NtrntCVi\nunix  2      [ ACC ]     STREAM     LISTENING     8732     @/tmp/.X11-unix/X0\nunix  2      [ ACC ]     STREAM     LISTENING     231120   /tmp/tmp9ff4b91.tmp\nunix  2      [ ACC ]     STREAM     LISTENING     819      /var/run/clamav/clamd.sock\nunix  2      [ ACC ]     STREAM     LISTENING     7955     @/tmp/dbus-tTfgqxRjYA\nunix  2      [ ACC ]     STREAM     LISTENING     859      /var/run/rpcbind.sock\nunix  2      [ ACC ]     STREAM     LISTENING     2920     /var/run/mysqld/mysqld.sock\nunix  2      [ ACC ]     STREAM     LISTENING     878      /var/run/libvirt/libvirt-sock\nunix  2      [ ACC ]     STREAM     LISTENING     880      /var/run/libvirt/libvirt-sock-ro\nunix  2      [ ACC ]     STREAM     LISTENING     928210   /tmp/ksocket-erratic/kdeinit4__0\nunix  2      [ ACC ]     STREAM     LISTENING     927554   /tmp/ksocket-erratic/klauncherM32400.slave-socket\nunix  2      [ ACC ]     STREAM     LISTENING     3999     /dev/gpmctl\nunix  2      [ ACC ]     STREAM     LISTENING     3047     /tmp/.dguardianipc\nunix  2      [ ACC ]     STREAM     LISTENING     3048     /tmp/.dguardianurlipc\nunix  2      [ ACC ]     STREAM     LISTENING     981647   /tmp/tmp29f0a955.tmp\nunix  2      [ ACC ]     SEQPACKET  LISTENING     2480     /run/udev/control\nunix  2      [ ACC ]     STREAM     LISTENING     200898   /var/run/cgisock.10205\nunix  2      [ ACC ]     STREAM     LISTENING     7829     @/tmp/dbus-jVZIw1Iio4\nunix  2      [ ACC ]     STREAM     LISTENING     3789     /var/run/dbus/system_bus_socket\nunix  2      [ ACC ]     STREAM     LISTENING     9772     @/tmp/dbus-WNaQOGvqGX\nunix  2      [ ACC ]     STREAM     LISTENING     8733     /tmp/.X11-unix/X0\nlaptop etc # netstat -an | grep LISTEN\n\nAcceptable values:\n- on or off\nsearch = on\nHow long Riak will wait for Solr to start. The start sequence\nwill be tried twice. If both attempts timeout, then the Riak node\nwill be shutdown. This may need to be increased as more data is\nindexed and Solr takes longer to start. Values lower than 1s will\nbe rounded up to the minimum 1s.\n\nDefault: 30s\n\nAcceptable values:\n- a time duration with units, e.g. '10s' for 10 seconds\nsearch.solr.start_timeout = 30s\nThe port number which Solr binds to.\nNOTE: Binds on every interface.\n\nDefault: 8093\n\nAcceptable values:\n- an integer\nsearch.solr.port = 8093\nThe port number which Solr JMX binds to.\nNOTE: Binds on every interface.\n\nDefault: 8985\n\nAcceptable values:\n- an integer\nsearch.solr.jmx_port = 8985\nThe options to pass to the Solr JVM.  Non-standard options,\ni.e. -XX, may not be portable across JVM implementations.\nE.g. -XX:+UseCompressedStrings\n\nDefault: -d64 -Xms1g -Xmx1g -XX:+UseStringCache -XX:+UseCompressedOops\n\nAcceptable values:\n- text\nsearch.solr.jvm_options = -d64 -Xms1g -Xmx1g -XX:+UseStringCache -XX:+UseCompressedOops\n~\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/basho/riak/issues/653.\n. This is using the HTTP API\n. Confirmed from our side and @joecaswell also had some comments: \n\nThe inactive message only gets sent to the vnode manager when the vnode\nFSM receives the atom timeout while in the active state.\nThe active state accepts and ignores any message it does not expect:\nThis means that if the vnode receives any message (of any type from any sender) at least once per minute, the FSM timeout will never occur so the inactive message will never get sent.\nI think resolving this in a resilient manner will involve adding a now-type last action timestamp to the state that is updated when action requests (get,put,fold,index, etc.) are handled.\nAnd having the continue/1 function use something like:   \n{next_state, active, State, max(1,State#state.inactivity_timeout -\n   abs(timer:now_diff(os:timestamp(),State#state.last_action_timestamp)/1000))\n}.\nInstead of\n{next_state, active, State, State#state.inactivity_timeout}.\n. Confirmed, and thanks for your help. Perhaps this could be a future rebar enhancement. B\n. Never good practice anyway to have too many bucket-types. In fact, if that causes problems the user is probably doing it wrong anyway. \n. FYI - just dug up this old gist ... very amateur ha ha  but enough info to write an Erlang script to dump out the bucket-type info. \n. @haraldmosh - it doesn't seem a priority at the moment - been a long time in the queue - recommend you contact Basho sales if you need it as a supported feature.. One kludge could be to run a cron job which would execute something like the following ( originally from @engelsanchez ) on one of your Riak nodes:\nTo dynamically disable AAE from the Riak console, you can run this command:\n\n\n riak_core_util:rpc_every_member_ann(riak_kv_entropy_manager, disable, [], 60000).\n\n\n\nand enable with the similar:\n\n\n  riak_core_util:rpc_every_member_ann(riak_kv_entropy_manager, enable, [], 60000).\n\n\n\nThat last number is just a timeout for the RPC operation.  I hope this saves you some extra load on your clusters.\n\n\n. Yes. This sucks. The build grabs a patched version of Apache Solr from the basho s3 amazon account. Basho is no more - so no more s3 account. It's a bit unfortunate it wasn't uploaded somewhere like the Apache Maven repository where it could have sat forever, but that's life.\nIf you look in build-solr.sh you will see an example of how to build the patched Solr. \nYou could then upload it somewhere and set the environment variable ARTIFACT_URL_PREFIX to grab it from your chosen location.\nI believe @Bob-The-Marauder has already done this and uploaded a copy to a tokyo.ti amazon account - perhaps he can comment. \n. Thanks @zowers - someone needs to close this ticket now ;-). Yep. The env var override ARTIFACT_URL_PREFIX only works (untested) on develop branch. I'm going to release some docker stuff over the next couple of days, and ESL are going to start adding downloads on their website. For my builds I've actually swapped out /usr/bin/curl with a monkey patched script that calls the original /usr/bin/curl.bak with a substituted URL - ugly, but works, actually not that ugly because I'm building using docker so it's not like I'm messing up a system for everyone else.. ",
    "mengzyou": "How about this issue? The HTTP API list buckets and list keys still response 500 error to version 2.2.3:  \n<html><head><title>500 Internal Server Error</title></head><body><h1>Internal Server Error</h1>The server encountered an error while processing this request:<br><pre>{error,{exit,{ucs,{bad_utf8_character_code}},\n             [{xmerl_ucs,from_utf8,1,[{file,\"xmerl_ucs.erl\"},{line,185}]},\n              {mochijson2,json_encode_string,2,\n                          [{file,\"src/mochijson2.erl\"},{line,200}]},\n              {mochijson2,'-json_encode_array/2-fun-0-',3,\n                          [{file,\"src/mochijson2.erl\"},{line,171}]},\n              {lists,foldl,3,[{file,\"lists.erl\"},{line,1248}]},\n              {mochijson2,json_encode_array,2,\n                          [{file,\"src/mochijson2.erl\"},{line,173}]},\n              {mochijson2,'-json_encode_proplist/2-fun-0-',3,\n                          [{file,\"src/mochijson2.erl\"},{line,181}]},\n              {lists,foldl,3,[{file,\"lists.erl\"},{line,1248}]},\n              {mochijson2,json_encode_proplist,2,\n                          [{file,\"src/mochijson2.erl\"},{line,184}]}]}}</pre><P><HR><ADDRESS>mochiweb+webmachine web server</ADDRESS></body></html>. ",
    "andrelaszlo": "@jaredmorrow No problem, just thought I should share :) I noticed that you changed the whole config system in 2.0 but I didn't know if you were still adding stuff to the 1.4 branch. Looking forward to see the final result!\n. Cuttlefish looks really promising, looking forward to seeing it in action. Thank you both for having a look!\n. :bell: ping :bell:\n. ",
    "macintux": "I don't know that this is a bug. @Vagabond's design allows for pluggable authentication mechanisms. \nWe certainly could reject unknown sources, or issue a warning that the source isn't recognized but allow it.\n. Touching this; I've had it on my list for a very long time to fix this, so hope to take a look soon.\n. Trivial patch forthcoming\n. PR submitted, also submitted docs issue basho/basho_docs#1111\n. Closing, PR is being merged\n. @evanmcc What are next steps here?\n. Stealing from @jaredmorrow, tackling this right now.\n. Fixed via linked PRs, closing.\n. Should be identical but aren't. Will do.\n. Looks good. :+1:\n. @lucperkins Once you've sorted through any merge conflict with my earlier changes today, please copy the resulting release notes to riak_ee and create a PR there.\n. Thanks @matthewvon, fixed my oops.\n. Thanks for raising the issue.\nI'm not sure .pem is sufficient; there are other common certificate extensions to watch out for (http://blogs.msdn.com/b/kaushal/archive/2010/11/05/ssl-certificates.aspx via a quick search looks useful).\n. Thanks for this report, and the detail. I know we've been a bit wary about HIPE; when I saw it mentioned I fully expected the failing node to be using it.\nhttps://github.com/basho/otp/pull/3 is the only change between basho5 and basho6.\n. One of our engineers attempted to reproduce and failed, but he wanted to try again more rigorously.\nUnfortunately he\u2019s been extremely busy. I hoped he\u2019d have time this week, but he\u2019s been out sick.\n-John\n. :+1:\n. :+1:\n. Yes, riak-admin timezone with no additional argument will display the current setting.\n. Should not be merged until basho/riak_core#847 (RIAK-2639) merges\n. https://github.com/basho/riak_ee/pull/403\n. ",
    "lukebakken": "FWIW, I ran into this today by mis-typing certificate. The only way to know it had happened was to turn on debug logging in Riak to see why my client certificate authentication was failing.\n. Additional steps taken to try and grant this permission to the lbakken user, results in a stacktrace:\n```\nriak-admin security grant riak_core.get_bucket_type ON ANY TO lbakken\ncurl -k -u 'lbakken:Pass@word1' 'https://localhost:8443/types/default/props'\n401 UnauthorizedUnauthorizedUnauthorizedmochiweb+webmachine web server\nriak-admin security del-user admin\ncurl -k -u 'lbakken:Pass@word1' 'https://localhost:8443/types/default/props'\n500 Internal Server ErrorInternal Server ErrorThe server encountered an error while processing this request:{error,function_clause,\n       [{riak_core_security,'-authenticate/3-fun-0-',\n                            [{{<<\"admin\">>,{{127,0,0,1},32}},['$deleted']},[]],\n                            [{file,\"src/riak_core_security.erl\"},{line,231}]},\n        {riak_core_metadata,fold_it,3,\n                            [{file,\"src/riak_core_metadata.erl\"},{line,130}]},\n        {riak_core_security,authenticate,3,\n                            [{file,\"src/riak_core_security.erl\"},{line,231}]},\n        {riak_api_web_security,is_authorized,1,\n                               [{file,\"src/riak_api_web_security.erl\"},\n                                {line,21}]},\n        {riak_kv_wm_bucket_type,is_authorized,2,\n                                [{file,\"src/riak_kv_wm_bucket_type.erl\"},\n                                 {line,108}]},\n        {webmachine_resource,resource_call,3,\n                             [{file,\"src/webmachine_resource.erl\"},\n                              {line,186}]},\n        {webmachine_resource,do,3,\n                             [{file,\"src/webmachine_resource.erl\"},\n                              {line,142}]},\n        {webmachine_decision_core,resource_call,1,\n                                  [{file,\"src/webmachine_decision_core.erl\"},\n                                   {line,48}]}]}mochiweb+webmachine web server\n```\n. OK, it appears that sources are not inherited, as this command fixes the initial issue:\nriak-admin security add-source lbakken 127.0.0.1/32 password\nI'm not sure if roles should also cause sources to be inherited.\n. Doesn't the current behavior require many add-source commands to be run to set up sources for a set of users that are all in the same role? You could use add-source all but that may be too permissive.\n. OK great we'll get this documented ... paging @lucperkins \nI can test the stacktrace fix whenever its available.\nThanks!\n. 2.0.0pre10\n. Authentication records in pg_hba.conf for postgres do allow specifying users or roles. I only mention this here because PG authentication/authorization was used as a model for Riak's.\nhttp://www.postgresql.org/docs/9.3/static/auth-pg-hba-conf.html\n\nSpecifies which database user name(s) this record matches. The value all specifies that it matches all users. Otherwise, this is either the name of a specific database user, or a group name preceded by +. (Recall that there is no real distinction between users and groups in PostgreSQL; a + mark really means \"match any of the roles that are directly or indirectly members of this role\", while a name without a + mark matches only that specific role.)\n. @metadave - good question.\n\nI'll re-visit all arguments to scripts that use env.sh as well as riak-admin. If any of the arguments could have been separated by whitespace, I'll investigate further. However, I don't believe any arguments to these scripts allow  whitespace ... except for the JSON argument to riak-admin bucket-type create\nObviously, a test would be ideal :smile:\n. riak-debug should list the contents of the basho-patches directory\n. @laurenrother something here about docs and where to get Ubuntu packages? :fish_cake: \n. Unfortunately, the C# protobuf library doesn't throw exceptions when required data is missing (https://code.google.com/p/protobuf-net/issues/detail?id=262).\n. Related to #734 \n. Fine by me!\n. Would it be simpler to just add unset CDPATH at the top of the scripts?\nhttps://github.com/sstephenson/rbenv/pull/326\nor, unset CDPATH and this, if necessary:\nhttps://github.com/sstephenson/rbenv/commit/e3f72ebae20768079ca4b4425a364900f3f16fc6\n. I vote for fixing this \"once and for all\" in the top of all of our scripts. It may be more work now, but it would prevent this issue permanently going forward.\n. unset CDPATH is sufficient, I shouldn't have linked to the other bit of code.\nIt should just be a matter of adding unset CDPATH here, unless I'm missing something:\nhttps://github.com/basho/node_package/blob/develop/priv/base/env.sh#L17\n. Well that's unfortunate. I'd like to suggest that 1>/dev/null be used instead of 1>&- for this reason as well as the 1>/dev/null idiom being more familiar.\n. Example of the latter:\nbash\nlbakken@brahms ~\n$ export CDPATH=\"$HOME/Projects\"\nlbakken@brahms ~\n$ cd basho\n/home/lbakken/Projects/basho\nlbakken@brahms ~/Projects/basho\n$ cd\nlbakken@brahms ~\n$ cd basho 1>&-; echo $?\n-bash: cd: write error: Bad file descriptor\n1\n. If that's not portable we have a bigger problem than CDPATH :smile:\nSeriously, though, the /dev/null device is part of the POSIX standard.\n. Also, using 1>&- won't work:\nbash\nlbakken@brahms ~\n$ export CDPATH=\"$HOME/Projects\"\nlbakken@brahms ~\n$ cd basho 1>&- && /bin/pwd\n-bash: cd: write error: Bad file descriptor\n. Wow, such bizarre behavior:\nbash\nlbakken@BRAHMS-WIN8PRO ~\n$ export CDPATH=\"$HOME/Projects/basho\"\nlbakken@BRAHMS-WIN8PRO ~\n$ cd riak-nodejs-client 1>&- && /bin/pwd\n-bash: cd: write error: Bad file descriptor\nlbakken@BRAHMS-WIN8PRO ~/Projects/basho/riak-nodejs-client (issues/lrb/gh-69 *=)\n$\nThe working directory changed even though cd failed to write to stdout. Not what I would expect. As expected, /bin/pwd did not run which would cause issues since we're trying to capture its output.\n. To be pedantic, we can't use 1>&- - see my latest example.\n. Is this a stock Ubuntu 14 machine or have you modified it in any way, especially with regard to the following environment variables: LANGUAGE, LC_ALL and LANG?\n. That's not really what I asked :smile: \n- What version of Ubuntu 14 are you using - desktop or server?\n- Is your Ubuntu 14.04 machine freshly installed or have there been modifications made?\n- The presence of /tmp/mozilla_mozillaUser0/riak_2.1.3-1_amd64.deb in your log file is interesting. How are you attempting to install from the .deb file?\n. Hello - have you had issues joining the mailing list? That is the way to avoid moderation.\nhttp://lists.basho.com/mailman/listinfo/riak-users_lists.basho.com\nThere is an extensive list of community resources here: http://docs.basho.com/riak/latest/community/\nI will bring up your concerns internally, thanks.\n. > official forum or answers page for riak\nQuestions from internal team members:\n- What would you suggest as a replacement for a mailing list?\n- How does the mailing list or other community resources not meet your needs?\n. Thank you for your suggestions. We do moderate the riak-users mailing list on a daily basis. If you find that an email to the list awaits moderation, joining the list will bypass that step.\n. Please read the documentation for building Erlang on CentOS. Most likely, you need to use CFLAGS=\"-DOPENSSL_NO_EC=1\".\nIf this does not resolve your issue, please re-open this issue.\n. @ooshlablu \n. @javajolt we definitely need the same version as Riak EE: 2.1.1\n. What is the output of the following commands when run on both nodes in this cluster?\nriak-admin ringready\nriak-admin member-status\nriak-admin ring-status\n. Looks good to me. Anyone else?\n. Don't forget riak-debug in the riak_ee repo!\n. Don't forget riak_ee ... :smile: \n. Please see this document:\nhttp://docs.basho.com/riak/kv/2.1.4/developing/api/http/set-bucket-props/\n. Hello -\nI have opened this issue for tracking, please follow up here: basho/riak-java-client#681\n. How about changing git:// to https:// while you're at it? \ud83d\ude3a \n. As you can see, development has been occurring on the develop-2.2 branch. develop will become the default branch at some point.\nIn the meantime, I recommend cloning the riak repository and checking out a tag corresponding to the release you'd like to build from source:\ngit clone https://github.com/basho/riak.git\ncd riak\ngit checkout riak-2.2.0\nmake locked-deps rel\nIf you have any questions or run into any issues, please re-open this issue. Thanks!. @sharpe5 thank you for taking the time to report this. Please see this issue: basho-labs/riak-docker#4\nWe'll leave this issue open in case the root cause is not Riak Explorer.. @w-p - would you mind opening this issue in the basho_docs repository?\nhttps://github.com/basho/basho_docs\nThanks.. Thank you so much \ud83d\ude03 . reload() is specific to the Python client. Could you please open an issue here with code to reproduce your issue? Thanks.. How did you install Riak?\nWhat happens when you try to start Solr on its own? I got this command out of console.log.txt:\n/usr/bin/java -Djava.awt.headless=true -Djetty.home=/usr/local/Cellar/riak/2.2.0/libexec/lib/yokozuna-2.1.7-0-g6cf80ad/priv/solr -Djetty.temp=/usr/local/var/lib/riak/yz_temp -Djetty.port=8093 -Dsolr.solr.home=/usr/local/var/lib/riak/yz -DhostContext=/internal_solr -cp /usr/local/Cellar/riak/2.2.0/libexec/lib/yokozuna-2.1.7-0-g6cf80ad/priv/solr/start.jar -Dlog4j.configuration=file:///usr/local/Cellar/riak/2.2.0/libexec/etc/solr-log4j.properties -Dyz.lib.dir=/usr/local/Cellar/riak/2.2.0/libexec/lib/yokozuna-2.1.7-0-g6cf80ad/priv/java_lib -d64 -Xms1g -Xmx2g -XX:+UseStringCache -XX:+UseCompressedOops -Dcom.sun.management.jmxremote.port=8985 -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false org.eclipse.jetty.start.Main. \ud83d\udc4d don't forget EE \ud83d\ude04 . @mankyKitty please see basho/riak_kv#1645. You must use R16B02 to build Riak, preferably Basho's version from this tag. Please open an issue or submit a PR in this repository. Thanks.. Your server needs more memory. In addition, single-node Riak servers should not be used in production. If you wish to discuss this further, please use the riak-users mailing list, and provide more detail.. This could be $(realpath $(CURDIR)) to avoid a shell.\n. Could be built-in dir function instead of shell dirname.\n. ",
    "adamv": "Alternatively, what is the recommended way of installing riak in a \"non-conflicting\" way on a system?\n. ",
    "asparagui": "We reworked how we're packaging risk slightly, so no need to touch anything on your end.  Sorry about the bother and good luck w/ the 2.0 release.\n. ",
    "mortman": "@Vagabond  Thanks for the prompt and great answers. It is as usual far more complex a problem that it seemed at first blush. Would it be worth adding some documentation about why HTTP Basic isn't ideal and why users might want to use other options? I'm happy to contribute to the cause.\nAlso thanks for the link to the writeup of your RICON talk. Will there be video posted as well?\n\"I'm not sure we can force TLS 1.2 by default, as a lot of languages have only just gained support this year for it (and TLS 1.0) but we can certainly provide the knobs for it so people with cabable clients can force it.\"'\nYay knobs! Choice is great. That would be great for ciphers/hashes generally the SSL options in general.  What about setting a default of 1.2 and letting folks uncomment things or lower security though rather then making them opt-in to the better security options?\n. ",
    "wb14123": "Thanks for the help.\nI  rebuild it with make clean; make rel and no error is outputed.\nWhen I run ./rel/riak/bin/riak chkconfig, the output is the same:\nvm.args needs to have a -name parameter.\n  -sname is not supported.\nShould I check some config files?\n. I only find riak.conf in deps/cuttlefish/test/riak.conf and there is no nodename. The content of it:\nring_size = 32\nanti_entropy = debug\nlog.error.file = /var/log/error.log\nlog.console.file = /var/log/console.log\nlog.syslog = on\nlistener.http.internal = 127.0.0.1:8098\nlistener.http.external = 10.0.0.1:80\nAnd I found nodename in rel/riak/etc/cuttlefish.conf:\nnodename = riak@127.0.0.1\nThe files in some of my directories which I think may include the config files are:\n```\nwangbin@bin-macair ~/open_source/riak (3bd9938) > ls rel\nfiles  gen_dev  rebar.config  reltool.config  riak  vars  vars.config\nwangbin@bin-macair ~/open_source/riak (3bd9938) > ls rel/vars\ndev1_vars.config  dev2_vars.config  dev3_vars.config  dev4_vars.config  dev5_vars.config  dev_vars.config.src  perf_vars.config.src\nwangbin@bin-macair ~/open_source/riak (3bd9938) > ls rel/riak/etc \ncuttlefish.conf  solr-log4j.properties\n```\n. What if I want to use a stable version? Should the deps in rebar.config specify the git tag?\n. Because of my OS only has official Erlang package with version R16B02. I'd like to install riak 2.0. Waiting for the release...\n. ",
    "nisaacson": ":+1: just ran into this as well. Thanks for the fix @cmeiklejohn \n. ",
    "vimalk78": "for ubuntu:\n```\n$ sudo apt-cache search pam-dev\nlibpam0g-dev - Development files for PAM\n$ sudo apt-get install libpam0g-dev\n```\n. ",
    "jburwell": "Superseded by https://github.com/basho/riak/pull/621\n. Debugging/investigation revealed that the Ubuntu/Debian init.d script performs a ping before starting Riak.  In contrast, the Red Hat init.d script does not perform a ping on start.  The runner:ping function spins up a Erlang VM without dropping privileges -- causing the .erlang.cookie to be created with root:root ownership.  Adding a call to boostrapd $@ to the runner:ping function fixes the issue.\nI am now adding a Vagantfile to the riak project builds packages for a specified platform in order to test  the fix, and regression test affected functionality.\n. @jared I like the removal of the ping approach least invasive for 2.0 and consistent with the Red Hat behavior.  \nI think it would be prudent to add a env.sh:invoke_erl function post-2.0 to properly drop permissions/manage execution across all of the scripts. If you agree, I will create a ticket for such a refactoring.\nFinally, we also need to stop swallowing stderr/stdout output and redirect to a log.  Such a log would have literally saved me hours of debugging, and I imagine it would be same for our users and support.\n. @jaredmorrow @hectcastro The solution we are considering will fix riak start on Ubuntu/Debian systems, but it doesn't protect against a misplaced ping wedging the permissions on the .erlang.cookie file.  Deleting the /var/lib/riak/.erlang.cookie file when the ping fails prevents the permissions from getting wedged while maintaining the non-root users to ping.  My thought is to add the following to the end of the env.sh:ping_node function:\nshell\n     if [ \"$?\" -ne 0 ]; then\n         rm -f $RUNNER_BASE_DIR/.erlang.cookie\n    fi\nThe current user would have caused the file to be created, and thus would have the ability to delete it.  If the node is running, then we leave the .erlang.cookie file in place ensuring that we don't disturb the environment of the running node.\nOne issue: the rm call would override $? value relied upon by callers.  Therefore, this approach would need to be enhanced to preserve the return when the .erlang.cookie file is deleted.\n. I think the PR submitted by @hectcastro to address this issue is better overall approach than the delete method I proposed above.  My suggestion  creates the potential of race condition if/when root is pinging a node while operations as the riak user need to be performed.  While the window for this race is very small, @hectcastro's PR avoids it entirely.  The only concern that I have is the removal of the other $HOME directory logic throughout the rest of the script.  I also wonder if it might not be better to check if the user executing the ping is root, and drop privileges in this circumstance in order to simplify/consolidate the execution logic.\n/cc @jaredmorrow \n. @hectcastro's PR needs to be closed since @jaredmorrow's PR has been selected to address this defect.\n. Initial implementation will use the Jepsen old branch because the Riak driver has not been ported to the new branch.  Once I have verified operation, I will port the driver to the 2.0 branch.\n. Incorrect defect.  Closing.\n. The verify_riak_stats test fails against this branch with the following error:\n``\n17:02:50.542 [info] verify_riak_stats prereqs: []\n17:02:50.542 [notice] Running Test verify_riak_stats\n17:02:51.545 [info] Test Runneruname -a` : Darwin jburwell-basho.local 13.3.0 Darwin Kernel Version 13.3.0: Tue Jun  3 21:27:35 PDT 2014; root:xnu-2422.110.17~1/RELEASE_X86_64 x86_64\n17:02:51.545 [info] Riak path: \"/Users/jburwell/rt/riak-exometer-2.0\"\n17:02:51.579 [info] Running: /Users/jburwell/rt/riak-exometer-2.0/dev/dev1/bin/riak start\n17:02:54.478 [info] Wait until 'dev1@127.0.0.1' is pingable\n17:02:54.745 [info] Wait until 'dev1@127.0.0.1' is pingable\n17:02:54.745 [info] Wait until riak_core_ring_manager is up on 'dev1@127.0.0.1'\n17:02:54.746 [info] Check 'dev1@127.0.0.1' is a singleton\n17:02:54.825 [info] Deployed nodes: ['dev1@127.0.0.1']\n17:02:54.825 [info] Waiting for services [riak_kv] to start on ['dev1@127.0.0.1'].\n17:03:04.838 [info] Wait until nodes are ready : ['dev1@127.0.0.1']\n17:03:14.976 [error] Error in process <0.127.0> on node 'riak_test@127.0.0.1' with exit value: {{assertNotEqual_failed,[{module,verify_riak_stats},{line,122},{expression,\"0\"},{value,0}]},[{verify_riak_stats,'-verify_nz/2-fun-0-',1,[{file,\"tests/verify_riak_stats.erl\"},{line,122}]},{verify_riak_stats,'-verify_nz/2-lc$^0/1-0-'...\n17:03:14.976 [warning] verify_riak_stats failed: {{assertNotEqual_failed,[{module,verify_riak_stats},{line,122},{expression,\"0\"},{value,0}]},[{verify_riak_stats,'-verify_nz/2-fun-0-',1,[{file,\"tests/verify_riak_stats.erl\"},{line,122}]},{verify_riak_stats,'-verify_nz/2-lc$^0/1-0-',2,[{file,\"tests/verify_riak_stats.erl\"},{line,122}]},{verify_riak_stats,'-verify_nz/2-lc$^0/1-0-',2,[{file,\"tests/verify_riak_stats.erl\"},{line,122}]},{verify_riak_stats,confirm,0,[{file,\"tests/verify_riak_stats.erl\"},{line,32}]},{riak_test_runner,return_to_exit,3,[{file,\"src/riak_test_runner.erl\"},{line,159}]}]}\n17:03:14.976 [error]\n================ verify_riak_stats failure stack trace =====================\n{{assertNotEqual_failed,[{module,verify_riak_stats},\n                         {line,122},\n                         {expression,\"0\"},\n                         {value,0}]},\n [{verify_riak_stats,'-verify_nz/2-fun-0-',1,\n                     [{file,\"tests/verify_riak_stats.erl\"},{line,122}]},\n  {verify_riak_stats,'-verify_nz/2-lc$^0/1-0-',2,\n                     [{file,\"tests/verify_riak_stats.erl\"},{line,122}]},\n  {verify_riak_stats,'-verify_nz/2-lc$^0/1-0-',2,\n                     [{file,\"tests/verify_riak_stats.erl\"},{line,122}]},\n  {verify_riak_stats,confirm,0,\n                     [{file,\"tests/verify_riak_stats.erl\"},{line,32}]},\n  {riak_test_runner,return_to_exit,3,\n                    [{file,\"src/riak_test_runner.erl\"},{line,159}]}]}\n============================================================================\n17:03:14.976 [notice] verify_riak_stats Test Run Complete\n17:03:14.979 [info] Stopping cover\nTest Results:\nverify_riak_stats-bitcask: fail\n\n1 Tests Failed\n0 Tests Passed\nThat's 0.0% for those keeping score\n```\nAdditionally, the following log entries are occurring repeatedly during the test run:\n2014-07-31 17:17:23.712 [warning] <0.156.0>@exometer_report_collectd:exometer_info:194 Could not reconnect: {error,enoent}\n2014-07-31 17:17:53.713 [info] <0.156.0>@exometer_report:reporter_loop:1062 Custom invocation: exometer_report_collectd({exometer_callback,reconnect})\n2014-07-31 17:17:53.713 [info] <0.156.0>@exometer_report_collectd:exometer_info:189 Reconnecting: {st,auto,\"/var/run/collectd-unixsock\",exometer,auto,10000,\n[{[riak,riak_kv,node,puts,one],\"counter\"}],5000,5000,30000,undefined}\nIt seems to me that the collectd reporter should be disabled by default because this yields a tremendous amount of log spam.\n. Working with @russelldb, I have put together a performance test plan to verify that Exometer's measurement overhead is less than or equal to Folsom's overhead.\n. Performed functional testing with the Bitcask, LevelDB, and memory backends to verify the completeness of the metrics reports.\nFor all three (3) backends, the following stats were reported by Folsom in Riak 2.0.0RC1 but not by Exometer branch:\n- consistent_get_objsize_100\n- consistent_get_objsize_95\n- consistent_get_objsize_99\n- consistent_get_objsize_mean\n- consistent_get_objsize_median\n- consistent_get_time_100\n- consistent_get_time_95\n- consistent_get_time_99\n- consistent_get_time_mean\n- consistent_get_time_median\n- consistent_gets\n- consistent_gets_total\n- consistent_put_objsize_100\n- consistent_put_objsize_95\n- consistent_put_objsize_99\n- consistent_put_objsize_mean\n- consistent_put_objsize_median\n- consistent_put_time_100\n- consistent_put_time_95\n- consistent_put_time_99\n- consistent_put_time_mean\n- consistent_put_time_median\n- consistent_puts\n- consistent_puts_total\n- leveldb_read_block_error\n- read_repairs_primary_notfound_count\n- read_repairs_primary_notfound_one\n- read_repairs_primary_outofdate_count\n- read_repairs_primary_outofdate_one\n- riak_core_stat_ts\n- riak_kv_stat_ts\n- riak_pipe_stat_ts\n/cc @uwiger\n. @russelldb as I understand Exometer, it can be configured to cache.  @uwiger, if my understanding is correct, would it be correct to update these values when caching is enabled and report \"0\" when it is disabled?  \n/cc @jonmeredith \n. @jonmeredith My suggestion was in the spirit of maintaining compatibility with 2.0.0 stats output.  However, based on @uwiger's comments, it seems as though these metrics is obsolete in the context of the Exometer integration. \n@jonmeredith To confirm, it is acceptable for Exometer to drop the riak_core_stat_ts, riak_kv_stat_ts, and riak_pipe_stat_ts stats.\n. The following are the tasks required to complete the review of the PR:\n- Coverage Testing: Compare the output of stats between these branches and Riak 2.0 RC1 to ensure all stats are present.\n  - [ ] bitcask Backend\n  - [ ] leveldb Backend\n  - [ ] memory Backend\n  - [ ] Strong Consistency Enabled\n  - [ ] Yokozuna Enabled\n  - [ ] Legacy Search Enabled\n  - [ ] Using map datatype\n  - [ ] Using set datatype\n  - [ ] Using counter datatype\n  - [ ] Using register datatype\n  - [ ] MDC Enabled\n- Comparison Tests: All metrics values are within +/- 5% with the exception of version strings (which should be equal) and handoffs related metrics\n  - [ ] bitcask Backend\n  - [ ] leveldb Backend\n  - [ ] memory Backend\n  - [ ] Strong Consistency Enabled\n  - [ ] Yokozuna Enabled\n  - [ ] Legacy Search Enabled\n  - [ ] Using map datatype\n  - [ ] Using set datatype\n  - [ ] Using counter datatype\n  - [ ] Using register datatype\n  - [ ] MDC Enabled\n- - [ ] Overhead Verification: The impact of Exometer and Folsom on throughput, get, and update latencies should be within +/- 5% for 1, 2, 5, 10, and 50 stats clients\n- riak-admin stats functionality: Add r_t test cases to verify the operation of the following riak-admin stats commands added for this enhancement:\n  - [ ] Implement show command test case \n  - [ ] Pass show command test case\n  - [ ] Implement show-all command test case\n  - [ ] Pass show-all command test case\n  - [ ] Implement enable command test case\n  - [ ] Pass enable command test case \n  - [ ] Implement disable command test case\n  - [ ] Pass disable command test case\n  - [ ] Implement reset command test case\n  - [ ] Pass reset command test case\n  - [ ] Implement info command test case\n  - [ ] Pass info command test case\n- [ ] Pass the verify_stats test\n- [ ] Pass the replication_stats test\n- [ ] Implement/pass handoff stats tests\n- [ ] CSE/Product Management review of the stats cuttlefish schema\n. The latest coverage tests found the following keys present in the Folsom-based stats, but not Exometer for the bitcask, leveldb, and memory backends:\n- consistent_get_objsize_100\n- consistent_get_time_100\nThe Exometer stas contained the following stats that were not present in the Folsom stats:\n- afunix_version\n- bear_version\n- consistent_get_objsize_max\n- consistent_get_time_max\n- exometer_version\n- list_fsm_create_error_total\n- list_fsm_create_total\n- list_fsm_error\nComparison of the stats values are capturing this Google Docs spreadsheet (https://docs.google.com/a/basho.com/spreadsheets/d/1lqTnmmDwZ6uOdtO8raI9TDcJ77M8IxyRSysTF7wWPdU/edit#gid=1013106721). \n/cc @uwiger \n. Executed another round of coverage tests on 24 Sept 2014 for bitcask, leveldb, memory, and Yokozuna search.  For bitcask, leveldb, and memory, delete operations were added to run to trigger a wider range of metrics.  I have encountered a error with Basho Bench's mapreduce operation that I will resolve for the next round of testing.  For this run, the following metrics were present in the Folsom stats but not Exometer:\n- consistent_get_objsize_100\n- consistent_get_time_100\n- search_index_fail_count\n- search_index_fail_one\n- search_index_latency_95\n- search_index_latency_99\n- search_index_latency_999\n- search_index_latency_max\n- search_index_latency_median\n- search_index_latency_min\n- search_index_throughput_count\n- search_index_throughtput_one\n- search_query_fail_count\n- search_query_fail_one\n- search_query_latency_95\n- search_query_latency_99\n- search_query_latency_999\n- search_query_latency_max\n- search_query_latency_median\n- search_query_latency_min\n- search_query_throughput_count\n- search_query_throughput_one\nThe following metrics were present in the Exometer stats but not Folsom:\n- afunix_version\n- bear_version\n- consistent_get_objsize_max\n- consistent_get_time_max\n- exometer_version\n- list_fsm_create_error_total\n- list_fsm_create_total\n- list_fsm_error\n- read_repairs_primary_outofdate_count\n- read_repairs_primary_outofdate_one\nA comparison of the stats values are captured this spreadsheet.\nN.B. The search stats appear inconsistent.  The Basho Bench job issues a series of queries that should fail (i.e. querying for an integer value out of range).  However, the stats run reflects no query failures.  Also, the search_index_latency and search_index_throughput_one histograms have a zero value yet the other histograms reflect search operation activity.  Therefore, a zero values for these metrics seem inconsistent.  I will investigate these results before the next test run to ensure that my assumption/expectations are correct, the Basho Bench job is defined properly, and/or YZ metrics collection is correct.\n/cc @uwiger @jonmeredith @coderoshi \n. Executed a third round of coverage/comparison tests on 2 October 2014.  For this run, the following metrics were present in the Folsom stats but not Exometer:\n- search_index_fail_count\n- search_index_fail_one\n- search_query_fail_count\nThe following metrics were present in the Exometer stats but not Folsom:\n- afunix_version\n- bear_version\n- exometer_version\n- list_fsm_create_error_total\n- list_fsm_create_total\n- list_fsm_error\n- read_repairs_primary_outofdate_count\n- read_repairs_primary_outofdate_one\n- search_index_latency_95\n- search_index_latency_99\n- search_index_latency_999\n- search_index_latency_max\n- search_index_latency_median\n- search_index_latency_min\n- search_index_throughput_count\n- search_index_throughtput_one\n- search_query_fail_one\n- search_query_latency_95\n- search_query_latency_99\n- search_query_latency_999\n- search_query_latency_max\n- search_query_latency_median\n- search_query_latency_min\n- search_query_throughput_count\n- search_query_throughput_one\n- search_search_query_fail_count\nA comparison of the stats values are captured this spreadsheet.\n/cc @uwiger @jonmeredith \n. Re-executed the coverage/comparison test suite to test @uwiger's latest changes.  The following metrics were present in Riak 2.0.0, but not the feuerlabs-exometer2 branch:\n- converge_delay_last\n- converge_delay_max\n- converge_delay_min\n- search_index_fail_count\n- search_index_fail_one\n- search_query_fail_count\n- read_repairs_primary_notfound_count\n- read_repairs_primary_notfound_one\nThe following metrics were present in the feuerlabs-exometer2 branch, but not Folsom:\n- afunix_version\n- bear_version\n- exometer_version\n- list_fsm_create_error_total\n- list_fsm_create_total\n- list_fsm_error\n- late_put_fsm_coordinator_ack\n- node_get_fsm_in_rate\n- node_get_fsm_out_rate\n- node_put_fsm_active\n- node_put_fsm_in_rate\n- node_put_fsm_out_rate\n- rings_reconciled\n- search_index_latency_95\n- search_index_latency_99\n- search_index_latency_999\n- search_index_latency_max\n- search_index_latency_median\n- search_index_latency_min\n- search_index_throughput_count\n- search_index_throughtput_one\n- search_query_fail_one\n- search_query_latency_95\n- search_query_latency_99\n- search_query_latency_999\n- search_query_latency_max\n- search_query_latency_median\n- search_query_latency_min\n- search_query_throughput_count\n- search_query_throughput_one\n- search_search_query_fail_count\n- read_repairs_primary_outofdate_count\n- read_repairs_primary_outofdate_one\nA comparison of the stats values are captured in this spreadsheet.\n/cc @uwiger @jonmeredith \n. @krestenkrab The best way is to use collectd to pull off the HTTP endpoint and push over to Graphite.  See this gist for a set of collectd configuration templates that will pull both Riak stats and the relevant system stats.  The collectd.conf.template should be rendered into /etc/collectd and the other templates into etc/collect.d. \nN.B. The Riak stats being pulled in the gist are a subset of the available stats.   Therefore, you may need to change the list to match your needs.\n. Superseded by https://github.com/basho/riak/pull/621\n. :+1: 3429e2b\n. :+1:\n. :+1: Performed a local make distclean deps compile, and the afunix dependency was not pulled and all modules compiled successfully.\n. @ulf Does riak_ee need a similar change?\n. @ulf Can you create a riak_ee PR that removes afunix from the Makefile and reltool?\n. :+1: Verified the failure to build stagedevrel locally on 2.0, and that this branch corrects the missing directory error.\n. @ulf I apologize for the mixup.  @uwiger is @ulf in Hipchat so muscle memory kicks in.  I apologize again.\n. :+1: \n. ",
    "yaderbh": "I found a spec file example:\nhttps://gist.github.com/hiroakis/5272527\n. Looks like compiling erlang from source fixes those issues. Thank you.\n. ",
    "cheeseplus": "@jolyon2000 is this a 64bit Ubuntu 12.04 install? If not you'll get an error as we do not provide 32bit packages.\n. Is there an epmd process running?\n. It's a a supervisory Erlang process that doesn't necessarily die when you stop Riak. You should be able to grep for epmd in a process list and kill it.\n. This is a classic Riak error and this ML post sums it up well: http://lists.basho.com/pipermail/riak-users_lists.basho.com/2012-March/007806.html\n. ",
    "jmshoffs0812": "plus equal very yes\n. ",
    "uwiger": "This functionality is now implemented (albeit slightly differently) in the Exometer stats version, #448 and corresponding riak_kv PR #748. Even histograms can be reset, in fact. Whether that's a good thing, I guess depends on your needs.\n. Note, the old PR was #448 \n. Seems to be the ring_creation_size metric that's missing (although the test output doesn't say). I'll look into it.\nI was thinking about setting the reporter to off by default, and providing a cuttlefish toggle. Another possibility would be to limit the number of retries if it can't find the collectd socket. It makes little sense to retry endlessly.\n. I can certainly put them back in, and have them report 0 unless I can find some other sensible interpretation for them.\nCaching in exometer would, in any event, not be exactly like the riak_core_stat_cache, which cached everything. Exometer would only cache the metrics that cost a lot to sample. One metric type that fits that description is the CPU stats, but it actually handles it internally. One day, perhaps I'll rewrite it to use the exometer cache (eat one's own dog food ...), but it doesn't seem a high priority atm.\n. @jburwell riak_ee still has the afunix dependency, both in Makefile and reltool.config\n. Apologies. I had done that, but not pushed it. Pushed now.\n2013/12/6 Evan Vigil-McClanahan notifications@github.com\n\nIn rel/reltool.config:\n\n{template, \"../deps/riak_kv/priv/multi_backend.schema\", \"lib/20-multi_backend.schema\"},\n        {template, \"../deps/eleveldb/priv/eleveldb.schema\", \"lib/21-leveldb.schema\"},\n-           {template, \"../deps/sidejob/priv/sidejob.schema\", \"lib/22-sidejob.schema\"},\n\nthis line needs to be removed if we're removing exometer/sidejob stuff.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/basho/riak/pull/448/files#r8168872\n.\n. \n",
    "dgrnbrg": "I don't think cuttlefish fixes this, unless there's a tool to automatically\nconvert a legacy config to the cuttlefish config. We have a pretty large\nand complex config, and porting it isn't cost-effective, but this type of\nvalidation is really valuable to guard against typos.\nOn Monday, March 24, 2014, Jared Morrow notifications@github.com wrote:\n\nCuttlefish (the new configuration system introduced in 2.0) fixes this\nissue. Sorry for the troubles in 1.3.\n\nReply to this email directly or view it on GitHubhttps://github.com/basho/riak/issues/458#issuecomment-38471262\n.\n. \n",
    "nishannorunobi": "same problem here when i use \"sudo apt-get update\"\n. ",
    "rastkojokic": "I am experiencing similar issue when I try to install Riak.\nI am using: Ubuntu 14.04.2 LTS\nHere is the sequence of commands that I have executed:\ncurl http://apt.basho.com/gpg/basho.apt.key | sudo apt-key add -\nsudo bash -c \"echo deb http://apt.basho.com $(lsb_release -sc) main > /etc/apt/sources.list.d/basho.list\"\nsudo apt-get update\nUpdate gives me the following error:\nFailed to fetch http://apt.basho.com/dists/trusty/main/binary-amd64/Packages  403  Forbidden\nThank in advance!\n. ",
    "gburd": "+1\n. @vbmithr we don't currently develop or test on ARM architecture CPUs, we may in the future.  If this is a critical architecture for you please let us know.  Pull requests are always welcome!\n. PM +1\n. PM +1\n. ",
    "allinora": "oops. sorry about that. \n. ",
    "vbmithr": "On 31/01/2014 22:39, Gregory Burd wrote:\n\n@vbmithr https://github.com/vbmithr we don't currently develop or test\non ARM architecture CPUs, we may in the future. If this is a critical\narchitecture for you please let us know. Pull requests are always welcome!\n\nStrangly enough, I managed to make it work. It was an issue on erlang_js\nwhen compiled in a submodule of riak. I then checked out erlang_js in a\ndifferent directory and managed to get it working.\nSo far riak (HEAD) works on my ARM test machines.\n. On 01/02/2014 16:37, Jared Morrow wrote:\n\nOut of curiosity are you using ARM based servers or is this on an embedded\ndevice? Greg and I would love to hear more as this topic has come up\nquite a bit.\n\nIt is embedded devices indeed: Beaglebone-style boards, like the\ncubieboard2, the odroid XU, \u2026\nIn my case, I'm working on an Internet of Things project that requires\nthe nodes to share a DB, with a strong need for fault tolerance, and\nconsistency. I'm trying to use Riak for that. It seems to me that it is\nthe most suitable DB for this use, even though I don't care much about\nthe distribution of data, more the replication.\n. ",
    "oleksiyk": "@russelldb in my use case I'm gradually adding values into the set but not all at once. The problem is that a single insert (DtUpdateReq request) is getting really much slower for each subsequent insert. \nWith Riak 1.4 I used a JSON array: for each insert I would first fetch the object, resolve possible siblings by doing set union (allow_mult=true), then add new value to array and put the object back. With this scenario it is also getting slower while array grows but not so slow:\n1000: 2392ms // first thousand inserted in 2,392ms\n2000: 3065ms // second thousand inserted in 3,065ms\n3000: 3868ms // third thousand inserted in 3,868ms\n4000: 4642ms // ... \n5000: 5326ms\n6000: 5694ms\n7000: 6380ms\nUsing 2.0.0-pre11\n. Yes, it makes a difference!\n1000: 1554ms\n2000: 2425ms\n3000: 3733ms\n4000: 4774ms\n5000: 6215ms\n6000: 6764ms\n7000: 7900ms\n. Ok, thanks.\n. Yes:\n$ riak-admin bucket-type create mytype '{\"props\": {\"datatype\": \"set\", \"allow_mult\": \"true\"}}'\nmytype created\n$ riak-admin bucket-type activate mytype\nmytype has been created but cannot be activated yet\n. A snapshot from console:\n$ riak-admin bucket-type activate mytype\nmytype has been created but cannot be activated yet\n$ riak-admin bucket-type list\nmytype (not active)\n$ riak-admin bucket-type activate mytype\nmytype has been created but cannot be activated yet\n. Yes, that was the output of ring_status:\n```\n$ riak-admin ring_status\n================================== Claimant ===================================\nClaimant:  'oleksiy@192.168.1.99'\nStatus:     up\nRing Ready: true\n============================== Ownership Handoff ==============================\nNo pending changes.\n============================== Unreachable Nodes ==============================\nAll nodes are up and reachable\n``\n. Sorry, that was a testing environment so its already gone :( I've tried to replicate the problem later but no luck.\n. Ispre14available as package somewhere? Or only as source?\n. The problem still exists withpre14`:\nvagrant@precise64:~$ curl -sO http://s3.amazonaws.com/builds.basho.com/riak/develop/2.0.0pre14/ubuntu/precise/riak_2.0.0pre14-1_amd64.deb\nvagrant@precise64:~$ sudo dpkg -i riak_2.0.0pre14-1_amd64.deb\nSelecting previously unselected package riak.\n(Reading database ... 51127 files and directories currently installed.)\nUnpacking riak (from riak_2.0.0pre14-1_amd64.deb) ...\nSetting up riak (2.0.0pre14-1) ...\nAdding group `riak' (GID 111) ...\nDone.\nAdding system user `riak' (UID 106) ...\nAdding new user `riak' (UID 106) with group `riak' ...\nNot creating home directory `/var/lib/riak'.\nProcessing triggers for ureadahead ...\nureadahead will be reprofiled on next reboot\nProcessing triggers for man-db ...\nvagrant@precise64:~$ sudo service riak start\n!!!!\n!!!! WARNING: ulimit -n is 1024; 4096 is the recommended minimum.\n!!!!\nvagrant@precise64:~$ riak-admin bucket-type create dt-test-set '{\"props\": {\"datatype\": \"set\", \"allow_mult\": \"true\"}}'\nUsage: riak-admin bucket-type create <type> '{\"props\": { ... }}'\n. The error is in riak-admin script on line 9: https://github.com/basho/riak/blob/develop/rel/files/riak-admin#L9\nThat line should read: check_user \"$@\" (quotes added)\n. Great, thanks!\n. I'm afraid the problem has come back with beta1:\nbash\n$ sudo riak-admin bucket-type create dt-test-set '{\"props\":{\"datatype\":\"set\",\"allow_mult\":\"true\"}}'\nCannot create bucket type dt-test-set: invalid json\nDefault Ubuntu 12.04 installation.\nshell: /bin/bash 4.2.24\n. And the breaking change is in /usr/lib/riak/lib/env.sh:\nthis won't work:\nexec su - $RUNNER_USER -c \"$RUNNER_SCRIPT_DIR/$RUNNER_SCRIPT $ESCAPED_ARGS\"\nthis will work:\nexec sudo -H -u $RUNNER_USER -i $RUNNER_SCRIPT_DIR/$RUNNER_SCRIPT \"$@\n. I can run riak attach now with any commands you need\n. Yes I have and probably a lot of them. Yesterday I was working on yokozuna support in node.js PB client: https://github.com/nlf/riakpbc/pull/51 and have created a lot of indexes, schemas and buckets during tests. Where do I send the tar.gz with logs?\n. Logs: https://www.dropbox.com/s/cyk0mo58lryq5os/log.tar.gz\n. ```\n(riak@127.0.0.1)1> [ proplists:get_value(name, BP) || BP <- riak_core_bucket:get_buckets(yz_misc:get_ring(transformed)), proplists:is_defined(search_index, BP)].\n[<<\"test-search-bucket-1392227619643\">>,\n <<\"test-search-bucket-\">>,\n <<\"test-search-bucket-1392217261937\">>,\n <<\"test-search-bucket-1392218155115\">>,\n <<\"test-search-bucket-1392212877282\">>,\n <<\"test-search-bucket-1392217216435\">>,\n <<\"test-search-bucket-1392217688533\">>,\n <<\"test-search-bucket-1392218919117\">>,\n <<\"test-search-bucket-1392218701506\">>,\n <<\"test-search-bucket-1392217332535\">>,\n <<\"test-search-bucket-1392211710806\">>,\n <<\"test-search-bucket-1392216201765\">>,\n <<\"test-search-bucket-1392216921609\">>,\n <<\"test-search-bucket-1392216147530\">>,\n <<\"test-search-bucket-1392218725916\">>,\n <<\"test-search-bucket-1392216206678\">>,\n <<\"test-search-bucket-1392217101488\">>,\n <<\"test-search-bucket-1392218166680\">>,\n <<\"test-search-bucket-1392218091644\">>,\n <<\"test-search-bucket-1392212922806\">>,\n <<\"test-search-bucket-1392216002780\">>,\n <<\"test-search-bucket-139221868\"...>>,\n <<\"test-search-bucket-13922\"...>>,\n <<\"test-search-bucket-1\"...>>,<<\"test-search-buck\"...>>,\n <<\"test-search-\"...>>,<<\"test-sea\"...>>]\n(riak@127.0.0.1)2> length([ proplists:get_value(name, BP) || BP <- riak_core_bucket:get_buckets(yz_misc:get_ring(transformed)), proplists:is_defined(search_index, BP)]).\n27\n```\n. I slightly modified your command:\nriak_core_bucket:set_bucket(list_to_binary(atom_to_list(Name))... to riak_core_bucket:set_bucket(Name...\n[ begin Name = proplists:get_value(name, BP), riak_core_bucket:set_bucket(Name, [{search_index, <<\"_dont_index_\">>}]) end || BP <- riak_core_bucket:get_buckets(yz_misc:get_ring(transformed)), proplists:is_defined(search_index, BP)].\n[ok,ok,ok,ok,ok,ok,ok,ok,ok,ok,ok,ok,ok,ok,ok,ok,ok,ok,ok,\n ok,ok,ok,ok,ok,ok,ok,ok]\nHowever the CPU is still overloaded\n. I mean I understand that I can turn off yokozuna but just wondering what to do in such situations if it happens and you can't turn Yk off.\n. (riak@127.0.0.1)9> redbug:start(\"yz_exchange_fsm:repair -> return\").\nok\n. Oh, sorry. It has timed out:\nquitting: timeout\n. ```\n===============================================================================================================================\n 'riak@127.0.0.1'                                                          21:57:36\n Load:  cpu         0               Memory:  total      145541    binary      38776\n        procs    1960                        processes   73465    code        16044\n        runq        0                        atom          647    ets         10109\nPid                 Name or Initial Func         Time       Reds     Memory       MsgQ Current Function\n<6310.545.0>        yz_cover                      '-'30780043421      21616          0 compile:do_compile/2\n<6310.25.0>         code_server                   '-'11815477291     372232          0 code_server:loop/1\n<6310.542.0>        yz_events                     '-' 4953850175     973184          0 gen_server:loop/6\n<6310.24.0>         file_server_2                 '-'  876616482       3904          0 gen_server:loop/6\n<6310.182.0>        riak_core_vnode_manager       '-'   67122657     381448          0 gen_server:loop/6\n<6310.24185.36>     erlang:apply/2                '-'   45646518       2576          0 cpu_sup:measurement_server_loop/1\n<6310.544.0>        yz_entropy_mgr                '-'   45287997      98168          0 gen_server:loop/6\n<6310.7.0>          application_controller        '-'   30244748      56072          0 gen_server:loop/6\n<6310.165.0>        riak_core_ring_manager        '-'   27620551    4119560          0 gen_server:loop/6\n<6310.360.0>        riak_kv_entropy_manager       '-'   20700407      93272          0 gen_server:loop/6\n===============================================================================================================================\n 'riak@127.0.0.1'                                                          21:57:38\n Load:  cpu         1               Memory:  total      145403    binary      38766\n        procs    1960                        processes   73339    code        16044\n        runq        0                        atom          647    ets         10109\nPid                 Name or Initial Func         Time       Reds     Memory       MsgQ Current Function\n<6310.545.0>        yz_cover                      '-'     131164      21544          0 gen_server:loop/6\n<6310.25.0>         code_server                   '-'      67259     230376          0 code_server:loop/1\n<6310.91.0>         riak_sysmon_filter            '-'       2009       3904          0 gen_server:loop/6\n<6310.24185.36>     erlang:apply/2                '-'       1461       2576          0 cpu_sup:measurement_server_loop/1\n<6310.7.0>          application_controller        '-'        917      89560          0 gen_server:loop/6\n<6310.192.0>        riak_core_stat_cache          '-'        606      34336          0 gen_server:loop/6\n<6310.24.0>         file_server_2                 '-'        592       3904          0 gen_server:loop/6\n<6310.193.0>        riak_core_stat_calc_sup       '-'        555      14160          0 gen_server:loop/6\n<6310.333.0>        riak_kv_get_fsm_sj_stats      '-'        309       5736          0 gen_server:loop/6\n<6310.345.0>        riak_kv_stat_sj_stats         '-'        276       5736          0 gen_server:loop/6\n===============================================================================================================================\n 'riak@127.0.0.1'                                                          21:57:39\n Load:  cpu         5               Memory:  total      145627    binary      38771\n        procs    1960                        processes   73556    code        16044\n        runq        0                        atom          647    ets         10109\nPid                 Name or Initial Func         Time       Reds     Memory       MsgQ Current Function\n<6310.545.0>        yz_cover                      '-'    1422084      21544          0 gen_server:loop/6\n<6310.25.0>         code_server                   '-'     549064     372232          0 code_server:loop/1\n<6310.182.0>        riak_core_vnode_manager       '-'      11639     381448          0 gen_server:loop/6\n<6310.24.0>         file_server_2                 '-'       4890       3904          0 gen_server:loop/6\n<6310.183.0>        riak_core_capability          '-'       2266      21544          0 gen_server:loop/6\n<6310.91.0>         riak_sysmon_filter            '-'       2208       5776          0 gen_server:loop/6\n<6310.24185.36>     erlang:apply/2                '-'       1461       2576          0 cpu_sup:measurement_server_loop/1\n<6310.12.0>         rex                           '-'        765       2720          0 gen_server:loop/6\n<6310.7.0>          application_controller        '-'        649     143744          0 gen_server:loop/6\n<6310.345.0>        riak_kv_stat_sj_stats         '-'        542       8752          0 gen_server:loop/6\n===============================================================================================================================\n 'riak@127.0.0.1'                                                          21:57:41\n Load:  cpu         0               Memory:  total      145595    binary      38771\n        procs    1960                        processes   73525    code        16044\n        runq        0                        atom          647    ets         10109\nPid                 Name or Initial Func         Time       Reds     Memory       MsgQ Current Function\n<6310.24185.36>     erlang:apply/2                '-'       2922       2576          0 cpu_sup:measurement_server_loop/1\n<6310.192.0>        riak_core_stat_cache          '-'        689      21544          0 gen_server:loop/6\n<6310.193.0>        riak_core_stat_calc_sup       '-'        651      14160          0 gen_server:loop/6\n<6310.7.0>          application_controller        '-'        406     143744          0 gen_server:loop/6\n<6310.321.0>        riak_kv_put_fsm_sj_stats      '-'        310       5736          0 gen_server:loop/6\n<6310.333.0>        riak_kv_get_fsm_sj_stats      '-'        287       5736          0 gen_server:loop/6\n<6310.286.0>        riak_api_pb_sup               '-'        278       4464          0 gen_server:loop/6\n<6310.102.0>        os_cmd_port_creator           '-'        270       2576          0 os:start_port_srv_loop/0\n<6310.345.0>        riak_kv_stat_sj_stats         '-'        251       5736          0 gen_server:loop/6\n<6310.557.0>        yz_stat_sj_stats              '-'        247       5736          0 gen_server:loop/6\n^C\n.\n[{\"name\":\"test-search-index-\",\"schema\":\"test-search-schema-\"},\n{\"name\":\"test-search-index-1392209941310\",\"schema\":\"test-search-schema-1392209941310\"},\n{\"name\":\"test-search-index-1392210117317\",\"schema\":\"test-search-schema-1392210117317\"},\n{\"name\":\"test-search-index-1392210486553\",\"schema\":\"test-search-schema-1392210486553\"},\n{\"name\":\"test-search-index-1392210573525\",\"schema\":\"test-search-schema-1392210573525\"},\n{\"name\":\"test-search-index-1392210583834\",\"schema\":\"test-search-schema-1392210583834\"},\n{\"name\":\"test-search-index-1392210647068\",\"schema\":\"test-search-schema-1392210647068\"},\n{\"name\":\"test-search-index-1392210937771\",\"schema\":\"test-search-schema-1392210937771\"},\n{\"name\":\"test-search-index-1392211710806\",\"schema\":\"test-search-schema-1392211710806\"},\n{\"name\":\"test-search-index-1392212749116\",\"schema\":\"test-search-schema-1392212749116\"},\n{\"name\":\"test-search-index-1392212768091\",\"schema\":\"test-search-schema-1392212768091\"},\n{\"name\":\"test-search-index-1392212792960\",\"schema\":\"test-search-schema-1392212792960\"},\n{\"name\":\"test-search-index-1392212877282\",\"schema\":\"test-search-schema-1392212877282\"},\n{\"name\":\"test-search-index-1392212922806\",\"schema\":\"test-search-schema-1392212922806\"},\n{\"name\":\"test-search-index-1392216002780\",\"schema\":\"test-search-schema-1392216002780\"},\n{\"name\":\"test-search-index-1392216921609\",\"schema\":\"test-search-schema-1392216921609\"},\n{\"name\":\"test-search-index-1392217010851\",\"schema\":\"test-search-schema-1392217010851\"},\n{\"name\":\"test-search-index-1392217101488\",\"schema\":\"test-search-schema-1392217101488\"},\n{\"name\":\"test-search-index-1392217216435\",\"schema\":\"test-search-schema-1392217216435\"},\n{\"name\":\"test-search-index-1392217261937\",\"schema\":\"test-search-schema-1392217261937\"},\n{\"name\":\"test-search-index-1392217332535\",\"schema\":\"test-search-schema-1392217332535\"},\n{\"name\":\"test-search-index-1392217421360\",\"schema\":\"test-search-schema-1392217421360\"},\n{\"name\":\"test-search-index-1392217688533\",\"schema\":\"test-search-schema-1392217688533\"},\n{\"name\":\"test-search-index-1392217951215\",\"schema\":\"test-search-schema-1392217951215\"},\n{\"name\":\"test-search-index-1392218091644\",\"schema\":\"test-search-schema-1392218091644\"},\n{\"name\":\"test-search-index-1392218155116\",\"schema\":\"test-search-schema-1392218155116\"},\n{\"name\":\"test-search-index-1392218166680\",\"schema\":\"test-search-schema-1392218166680\"},\n{\"name\":\"test-search-index-1392218644162\",\"schema\":\"test-search-schema-1392218644162\"},\n{\"name\":\"test-search-index-1392218683929\",\"schema\":\"test-search-schema-1392218683929\"},\n{\"name\":\"test-search-index-1392218701507\",\"schema\":\"test-search-schema-1392218701507\"},\n{\"name\":\"test-search-index-1392218725916\",\"schema\":\"test-search-schema-1392218725916\"},\n{\"name\":\"test-search-index-1392218919117\",\"schema\":\"test-search-schema-1392218919117\"},\n{\"name\":\"test-search-index-1392227619643\",\"schema\":\"test-search-schema-1392227619643\"}]\n```\n. Yep, it solved it. Thanks! \nBut what can be the actual reason? It was almost 24 hours since I finished experimenting with Yokozuna. And then CPU load went up with no yokozuna related queries. I'm almost sure that most if not all of those indexes were successfully created and tested (e.g. they worked).\n. The firewall is not a problem. I can easily replicate the problem with hanging delete operations running 'devrel' cluster (commit f0d02eb9) on localhost with just two nodes (dev1 and dev2). Adding third node makes the problem much less likely to appear. And I was not able to replicate the problem when running all 5 nodes. But still the logs contain the following messages:\n2014-03-16 10:10:28.715 [info] <0.100.0>@riak_core_sysmon_handler:handle_event:92 monitor busy_dist_port <0.26745.1> [{initial_call,{riak_kv_put_fsm,init,1}},{almost_current_function,{gen_fsm,loop,7}},{message_queue_len,0}] {#Port<0.8989>,'dev2@127.0.0.1'}\n2014-03-16 10:10:40.025 [info] <0.100.0>@riak_core_sysmon_handler:handle_event:92 monitor busy_dist_port <0.31426.1> [{initial_call,{riak_kv_put_fsm,init,1}},{almost_current_function,{gen_fsm,loop,7}},{message_queue_len,0}] {#Port<0.8989>,'dev2@127.0.0.1'}\n2014-03-16 10:10:40.618 [info] <0.100.0>@riak_core_sysmon_handler:handle_event:97 Monitor got {suppressed,port_events,1}\n2014-03-16 10:10:42.762 [info] <0.100.0>@riak_core_sysmon_handler:handle_event:92 monitor busy_dist_port <0.32592.1> [{initial_call,{riak_kv_put_fsm,init,1}},{almost_current_function,{erlang,bif_return_trap,1}},{message_queue_len,0}] {#Port<0.22998>,'dev3@127.0.0.1'}\n2014-03-16 10:11:32.651 [info] <0.100.0>@riak_core_sysmon_handler:handle_event:92 monitor busy_dist_port <0.18244.2> [{initial_call,{riak_kv_put_fsm,init,1}},{almost_current_function,{erlang,bif_return_trap,1}},{message_queue_len,0}] {#Port<0.8989>,'dev2@127.0.0.1'}\n2014-03-16 10:11:33.618 [info] <0.100.0>@riak_core_sysmon_handler:handle_event:97 Monitor got {suppressed,port_events,1}\n2014-03-16 10:11:39.292 [info] <0.100.0>@riak_core_sysmon_handler:handle_event:92 monitor busy_dist_port <0.20618.2> [{initial_call,{riak_kv_put_fsm,init,1}},{almost_current_function,{gen_fsm,loop,7}},{message_queue_len,0}] {#Port<0.22998>,'dev3@127.0.0.1'}\n2014-03-16 10:11:42.139 [info] <0.100.0>@riak_core_sysmon_handler:handle_event:92 monitor busy_dist_port <0.21780.2> [{initial_call,{riak_kv_put_fsm,init,1}},{almost_current_function,{erlang,bif_return_trap,1}},{message_queue_len,1}] {#Port<0.8989>,'dev2@127.0.0.1'}\nI also tried increasing protobuf.backlog up to 512 to no avail (i'm using connection pooling with a maximum of 10 connections). \n. The same problems happens with Riak 1.4.8, commit cb5f1c2 running devrel build on localhost. And it doesn't matter if it is a leveldb or bitcask. To summarise common circumstances: \n- more than 1 node (happens more often when running just two nodes)\n- client uses connection pooling (10 connections max)\n- once in a while RpbDelReq will hang on a valid key until dropped by Riak with timeout error.\n- any other riak options are default\nI've created a simple application (node.js) which replicates the problem: \nhttps://www.dropbox.com/s/ryd4zw2kbkx97lo/riak-508.tar.gz\nUsually running it 50 times will eventually trigger the problem and 'del' operation will timeout:\nbash\nfor i in `seq 0 50`; do node test || break; done\n. Thanks, I'll try +zdbbl but i'm not storing objects larger than 2MB. If you can try the test code I've posted above you'll see that the objects are 256kb. And the actual problem is that the 'del' operation hangs. I experience 'del' lockouts with no load on Riak at all. \n. I've increased +zdbbl to 32MB and the messages have gone away. And I can't replicate the del timeout. However I'm really worried that under no load at all I can get a del timeout and in order to decrease the chance of this situation I should somehow guess the +zdbbl limit. A del operation doesn't transfer much traffic (even 20 operations) so why would I need 32MB network buffer? If I correctly understand http://www.erlang.org/doc/man/erl.html#%2bzdbbl the sending processes should suspend until the buffer is cleared. So it means that 1MB buffer was not enough to delete 20 objects 256kb each in 60 seconds (timeout time) on localhost. It doesn't sound right... Can you please provide more information on this topic and suggest some confident solutions? Thanks!\n. Can someone please provide more information on this issue? If 1MB buffer caused a delete operation to timeout with no load on Riak at all then how much is 32MB? Why would delete operation require so huge network buffer? Its not write, its delete and even writes didn't timeout.\n. Well, the code I posted really doesn't use vclocks, but my actual code does use them: https://github.com/oleksiyk/riakfs/blob/master/lib/index.js#L103 \nThat's the exact line where del operation is hanging with my real code. I do the read first with head=true so it shouldn't transfer object?\n. I see. Why its necessary to read the object data in order to delete it?\n. Thanks!\n. I've already tried new sets and they are cool! But unfortunately their performance is a bit worse compared to plain operations: https://github.com/basho/riak/issues/483\nEspecially when set gets bigger. \nI'm really looking forward to using them though.\n. My order of operations is (its a test actually):\n1. set bucket properties (via RpbSetBucketReq): allow_mult=true\n2. insert several values without vclocks\nI don't use bucket types. \nIf there is no delay between 1. and 2. then I don't get siblings, if I wait a bit between 1. and 2. then I get siblings as expected.\nSo if allow_mult=false is default again as it was in 1.4 then it all makes sense. Would it be default in all future 2.x?\n. Thanks!\n. Disabling riak_control=off helped a bit, but still the load is constantly 3-20%:\n```\n'riak@10.0.1.1'                                                           08:35:45\n Load:  cpu         0               Memory:  total       47623    binary       4864\n        procs    1093                        processes   13267    code        12629\n        runq        0                        atom          541    ets          6226\nPid                 Name or Initial Func         Time       Reds     Memory       MsgQ Current Function\n<6315.104.0>        erlang:apply/2                '-'    7170561       2600          0 cpu_sup:measurement_server_loop/1\n<6315.191.0>        riak_core_vnode_manager       '-'    4730615     234104          0 gen_server:loop/6\n<6315.93.0>         riak_sysmon_filter            '-'    3282618       5760          0 gen_server:loop/6\n<6315.3.0>          erl_prim_loader               '-'    2781141     142624          0 erl_prim_loader:loop/3\n<6315.194.0>        riak_core_capability          '-'    2139023      55016          0 gen_server:loop/6\n<6315.165.0>        riak_core_ring_manager        '-'    1666166     230360          0 gen_server:loop/6\n<6315.7.0>          application_controller        '-'    1626588      89544          0 gen_server:loop/6\n<6315.485.0>        riak_kv_stat_sj_stats         '-'    1153005       8736          0 gen_server:loop/6\n<6315.453.0>        riak_kv_put_fsm_sj_stats      '-'    1142398       5720          0 gen_server:loop/6\n<6315.469.0>        riak_kv_get_fsm_sj_stats      '-'    1139338       5720          0 gen_server:loop/6\n===============================================================================================================================\n 'riak@10.0.1.1'                                                           08:35:46\n Load:  cpu         0               Memory:  total       47668    binary       4864\n        procs    1093                        processes   13089    code        12629\n        runq        0                        atom          541    ets          6226\nPid                 Name or Initial Func         Time       Reds     Memory       MsgQ Current Function\n<6315.104.0>        erlang:apply/2                '-'       2351       2600          0 cpu_sup:measurement_server_loop/1\n<6315.93.0>         riak_sysmon_filter            '-'       2284       5760          0 gen_server:loop/6\n<6315.7.0>          application_controller        '-'        662     143728          0 gen_server:loop/6\n<6315.485.0>        riak_kv_stat_sj_stats         '-'        385       8736          0 gen_server:loop/6\n<6315.453.0>        riak_kv_put_fsm_sj_stats      '-'        382       8736          0 gen_server:loop/6\n<6315.469.0>        riak_kv_get_fsm_sj_stats      '-'        361       5720          0 gen_server:loop/6\n<6315.210.0>        riak_core_stat_cache          '-'        189      55016          0 gen_server:loop/6\n<6315.211.0>        riak_core_stat_calc_sup       '-'        153       5960          0 gen_server:loop/6\n<6315.390.0>        riak_api_pb_sup               '-'         46       2704          0 gen_server:loop/6\n<6315.94.0>         timer_server                  '-'         44       2784          0 gen_server:loop/6\n===============================================================================================================================\n 'riak@10.0.1.1'                                                           08:35:48\n Load:  cpu         0               Memory:  total       47477    binary       4864\n        procs    1093                        processes   12897    code        12629\n        runq        0                        atom          541    ets          6226\nPid                 Name or Initial Func         Time       Reds     Memory       MsgQ Current Function\n<6315.191.0>        riak_core_vnode_manager       '-'      13515     146432          0 gen_server:loop/6\n<6315.194.0>        riak_core_capability          '-'       6106      55016          0 gen_server:loop/6\n<6315.104.0>        erlang:apply/2                '-'       4702       2600          0 cpu_sup:measurement_server_loop/1\n<6315.7.0>          application_controller        '-'        871      89544          0 gen_server:loop/6\n<6315.195.0>        riak_core_gossip              '-'        827      88504          0 gen_server:loop/6\n<6315.469.0>        riak_kv_get_fsm_sj_stats      '-'        775       8736          0 gen_server:loop/6\n<6315.210.0>        riak_core_stat_cache          '-'        750      55016          0 gen_server:loop/6\n<6315.453.0>        riak_kv_put_fsm_sj_stats      '-'        740       5720          0 gen_server:loop/6\n<6315.485.0>        riak_kv_stat_sj_stats         '-'        729       8736          0 gen_server:loop/6\n<6315.211.0>        riak_core_stat_calc_sup       '-'        332       5960          0 gen_server:loop/6\n===============================================================================================================================\n 'riak@10.0.1.1'                                                           08:35:49\n Load:  cpu         0               Memory:  total       47523    binary       4865\n        procs    1093                        processes   12916    code        12629\n        runq        0                        atom          541    ets          6226\nPid                 Name or Initial Func         Time       Reds     Memory       MsgQ Current Function\n<6315.104.0>        erlang:apply/2                '-'       2351       2600          0 cpu_sup:measurement_server_loop/1\n<6315.93.0>         riak_sysmon_filter            '-'       2294       5760          0 gen_server:loop/6\n<6315.7.0>          application_controller        '-'        672     143728          0 gen_server:loop/6\n<6315.485.0>        riak_kv_stat_sj_stats         '-'        394       8736          0 gen_server:loop/6\n<6315.469.0>        riak_kv_get_fsm_sj_stats      '-'        363       5720          0 gen_server:loop/6\n<6315.453.0>        riak_kv_put_fsm_sj_stats      '-'        358       8736          0 gen_server:loop/6\n<6315.210.0>        riak_core_stat_cache          '-'        189      55016          0 gen_server:loop/6\n<6315.211.0>        riak_core_stat_calc_sup       '-'        179       5960          0 gen_server:loop/6\n<6315.94.0>         timer_server                  '-'         63       2784          0 gen_server:loop/6\n<6315.390.0>        riak_api_pb_sup               '-'         46       2704          0 gen_server:loop/6\n===============================================================================================================================\n 'riak@10.0.1.1'                                                           08:35:51\n Load:  cpu         0               Memory:  total       47476    binary       4864\n        procs    1093                        processes   12896    code        12629\n        runq        0                        atom          541    ets          6226\nPid                 Name or Initial Func         Time       Reds     Memory       MsgQ Current Function\n<6315.104.0>        erlang:apply/2                '-'       4702       2600          0 cpu_sup:measurement_server_loop/1\n<6315.7.0>          application_controller        '-'        871      89544          0 gen_server:loop/6\n<6315.453.0>        riak_kv_put_fsm_sj_stats      '-'        761       8736          0 gen_server:loop/6\n<6315.485.0>        riak_kv_stat_sj_stats         '-'        752       5720          0 gen_server:loop/6\n<6315.210.0>        riak_core_stat_cache          '-'        750      55016          0 gen_server:loop/6\n<6315.469.0>        riak_kv_get_fsm_sj_stats      '-'        748       8736          0 gen_server:loop/6\n<6315.211.0>        riak_core_stat_calc_sup       '-'        332       5960          0 gen_server:loop/6\n<6315.93.0>         riak_sysmon_filter            '-'        285       5760          0 gen_server:loop/6\n<6315.390.0>        riak_api_pb_sup               '-'         96       2704          0 gen_server:loop/6\n<6315.103.0>        cpu_sup                       '-'         67       2704          0 gen_server:loop/6\n^C\n```\n. Sorry about using github instead of mailing list, will use it next time!\nI've added the following to advanced.config:\nerlang\n[\n    {riak_core, [\n    {stat_cache_ttl, 30}\n    ]}\n].\nAnd it helped a bit, however I still see the CPU load spikes at about 20% each 5-10 seconds (not 30 as configured for stats ttl).\n. It seems it has settled by itself. The load by beam.smp now is constant 1% with spikes not over 5%\n. Is it safe to set stat_cache_ttl to 30 secs in production?\n. Yes, once I set stat_cache_ttle to 1 second the CPU load by beam.smp is back to minimum 4-7% with spikes to 20-30%\n. Yes, and riak-admin top shows cpu_sup:measurement_server_loop/1 at the top of each iteration\n. @russelldb OS is Ubuntu 14.04\nI'm running riak_2.0.0beta1-1\nstat_cache_ttl is set to 1 second\nCPU load by beam SMP is 4-7% (idle). I think that's totally acceptable.\n. Well I haven't noticed any slow down when putting real load on Riak so probably you can close this issue.\n. ",
    "superstructor": "Do you have an ETA on the fix being rolled into another beta ? This is still an issue with the latest beta1  Ubuntu package on the Basho downloads page. Thanks.\n. ",
    "sallespro": "missing packages : \nerlang-eunit and erlang-dev\n. ",
    "georgepsarakis": "@jaredmorrow thanks for taking the time to answer!\n. ",
    "matthewvon": "Is this issue resolved?\n. 64K is needed in 1.4 for nodes approaching 1.5T, and 2.0 nodes approaching 4Tbytes.\n. +1 from me and Dave\n. +1\n. Why?  Have you personally tested the configuration?\nSee third party results:  https://influxdb.com/blog/2014/06/20/leveldb_vs_rocksdb_vs_hyperleveldb_vs_lmdb_performance.html\n. RocksDB comparison is still on the \"we want to do this list\".  But honestly, it is not going to happen soon.\nNot discounting your results.  Not ignoring your results.  Just saying it will be a while before we have time to duplicate and evaluate your results.  It is a revenue priorities issue, not a technical issue.\n. fyi:  I am being forced to work with RocksDB on a different project.  It looks really great on raw numbers.  But the detail graphs look horrid ... it stalls all over the place ... and its performance really slumps once it starts actually reading data for compactions from disk.  See this posting on RocksDB developer support page (especially the graph toward the end):  https://www.facebook.com/groups/rocksdb.dev/permalink/1369946463103864/. Segfault during stop fixed by change in util/env_posix.cc function Shutdown().  Said fix moved to develop branch with merge of mv-timed-grooming.  Awaiting release.  Segfault \"during normal operation\" has not be documented / researched.\n. s/release of/released/   seems better to me\n. ",
    "peczenyj": "I don't know if it is easy to reproduce. Lets try to give one idea.\nWe create 14 Gb per day of data stored in my buckets. And we create some 2i indexes ( using elevelDB ) for cleanup and searching. For example, we have one index for last_modified_time to search itens changed in the last 24 hours. When I try to search itens changed between 20 and 21 days ago I find itens changed, in fact, yesterday. But it is not easy to reproduce, I need search and search until find. I note it can happens when we use range queries and pagination and, sometimes, I can receive different results for the same query when I run in sequence. \n. No, the value of the object does not change between the 2i query and when I read the object. In this particular case, if it is true I expect some value +120 days from now. 1395013128 is the epoch for '2014-03-16T23:38:48'\n. One more interesting fact: IF I perform the same query over and over, I can find different (wrong) results. But when I inspect one entry, If I perform the same query again and again I can find this result. It is like: when we GET the data, it change what is returning the value with a wrong secondary index value ( like clean some cache ). It make sense?\n. ",
    "RyanGordon": "Thanks!\n. ",
    "wcamarao": "@reiddraper 1.4.1 on OSX and 1.4.6 on Linux\nJust replied https://github.com/basho/riak-ruby-client/issues/153#issuecomment-37608478 with riak-admin status on both machines\n. ",
    "613038475": "That patch is not useable, it will make erl boot from \"riak start\" or \"riak console\" duplicate the ssl argument.\n. ",
    "tombriden": "This has recently been an issue for me, the attached patch doesn't appear to work with 2.0.5 but a slight variation on it works and there are no duplicate SSL variables when using \"riak console\" or \"riak start\"\n``\nSSL_ARGS=grep -e '^-ssl_dist_opt' -e '^-proto_dist' $RUNNER_ETC_DIR/vm.args | tr '\\n' ' '`\nSetup command to control the node\nNODETOOL=\"eval ERL_FLAGS=\\\"$ERL_FLAGS $SSL_ARGS\\\" $ERTS_PATH/escript $ERTS_PATH/nodetool $NET_TICKTIME_ARG $NAME_ARG $COOKIE_ARG\"\n```\nthere's also a separate issue with some extra output from nodetool when using SSL making get_pid fail, fixable with\nPID=`$NODETOOL getpid < /dev/null|head -1`\n. The mod to env.sh isn't a complete fix as some commands still fail, such as riak-admin bucket-type create\nWhat actually works is to use an alias for nodetool rather than store the command in a variable. So, changing env.sh to\nif [ -f $RUNNER_ETC_DIR/vm.args ]; then\n    SSL_ARGS=$(grep -e '^\\-ssl_dist_opt' -e '^\\-proto_dist' $RUNNER_ETC_DIR/vm.args | tr '\\n' ' ')\nfi\nalias NODETOOL=\"ERL_FLAGS=\\\"$ERL_FLAGS $SSL_ARGS\\\" $ERTS_PATH/escript $ERTS_PATH/nodetool $NET_TICKTIME_ARG $NAME_ARG $COOKIE_ARG\"\nand then updating env.sh and riak-admin so all $NODETOOL calls become just NODETOOL.\n. ",
    "ryanbamford": "have modded this additionally to include a check for the file \nif [ -f $RUNNER_ETC_DIR/vm.args ]; then\n   SSL_ARGS=grep -e '^\\-ssl_dist_opt' -e '^\\-proto_dist' $RUNNER_ETC_DIR/vm.args | tr '\\n' ' '\n   NODETOOL=\"eval ERL_FLAGS=\\\"$ERL_FLAGS $SSL_ARGS\\\" $ERTS_PATH/escript $ERTS_PATH/nodetool $NET_TICKTIME_ARG $NAME_ARG $COOKIE_ARG\"\nelse\n   NODETOOL=\"$ERTS_PATH/escript $ERTS_PATH/nodetool $NET_TICKTIME_ARG $NAME_ARG $COOKIE_ARG\"\nfi\nso it can run with and without the vm.args file without errors \nnot a full fix as we are using 2.0.5 which uses the generated vm.args and we are having to override them to get the tls settings in \nso we are reading the settings from the override location \n. bug -  code removed post R15\nhttp://erlang.org/pipermail/erlang-questions/2013-April/073241.html\n. ",
    "Basho-JIRA": "Fixed by changing node_package to use an alias for nodetool instead of an environment variable. \n[posted via JIRA by Brian Sparrow]. https://github.com/basho/basho_docs/pull/2430 \n[posted via JIRA by Lauren Rother]. KV 2.2.0 - https://github.com/basho/basho_docs/pull/2430\nWe're waiting on a PR to 2.0.8: https://github.com/basho/basho_docs/tree/riak_2.0.8/content/riak/kv/2.0.8 \n[posted via JIRA by Lauren Rother]. https://github.com/basho/basho_docs/pull/2440 PR into 2.0.8, Travis failed though \n[posted via JIRA by Eric Johnson]. Fix version and PR removed since not related per Doug. \n_[posted via JIRA by Patricia Brewer]_\n. Something was fixed in the .net client, not here /Riak \n_[posted via JIRA by Patricia Brewer]_\n. See comments here for additional detail - https://github.com/basho/riak/pull/669\nThese issues should just be closed without merging and we should just use the current release strategy (for now).\nClosing the bug as we will not include the updates. \n_[posted via JIRA by Derek Somogyi]_\n. This was fixed this weekend. \n_[posted via JIRA by Chris Meiklejohn]_\n. This was assigned to me, but I just pulled the latest version of Riak EE and it appears to be working just fine? \n_[posted via JIRA by Chris Meiklejohn]_\n. Doug researched and the PR is the wrong one, so removed the Fix Version. \n_[posted via JIRA by Patricia Brewer]_\n. An ESL customer has been experiencing a similar issue. Attaching riak-debug. \n_[posted via JIRA by Bryan Hunt]_\n. https://github.com/basho/riak/issues/685 \n_[posted via JIRA by Deborah Rakow]_\n. Jon M confirmed code merged to 2.0.6 on 6/25/15.  Dev is done. Now just waiting to go into additional releases.  \n_[posted via JIRA by Patricia Brewer]_\n. Can the fix or commit be linked on this ticket? \n_[posted via JIRA by Dan Brown]_\n. This is linked to GH Riak/685.  [~dbrown] are you looking for something different?  \n_[posted via JIRA by Patricia Brewer]_\n. This is linked to GH Riak/685.  [~dbrown] are you looking for something different?  This fix didn't go into 2.0.6.  Needs to go in a future release.   \n_[posted via JIRA by Patricia Brewer]_\n. Thanks Patricia. I would like to know which version of Riak this will be available in and would like a link to the commit (not the GH issue) of the code change that fixes this. \n_[posted via JIRA by Dan Brown]_\n. It's been long enough that Jon M and others do not remember the specific context of any fix which may or may not have been merged into riak_kv/leveldb/eleveldb. There doesn't seem to be any commits in the time range of this ticket which would make sense as being a fix for this issue.\nThe attached riak_debug shows a riak node that is out of disk space failing to stop at all (much less gracefully.) It's difficult to say from the debug whether the \"out of disk space\" is the root cause of the failure or if there's some deeper defect for which the disk space problem is coincidental.\nFor now we should close this ticket and open a backlog item about the out of disk space issue (which requires investigation and a debug session) and if this reoccurs in the future, we ought to open a fresh ticket. \n_[posted via JIRA by Mark Allen]_\n. This seems only appropriate for HTTP interface (PB already has strict types on properties), but should already be caught by bucket validators. The exception is potentially that a bad default property could be set and not caught by the validators if the user does not explicitly create buckets/bucket-types. \n_[posted via JIRA by Sean Cribbs]_\n. I have found an alternative to Sean's approach that works; needs more review to make sure it's as otherwise harmless as I think it is.\nbugfix/jrd/727 \n_[posted via JIRA by John Daily]_\n. I have found an alternative to Sean's approach that works; needs more review to make sure it's as otherwise harmless as I think it is.\nhttps://github.com/basho/riak_core/tree/bugfix/jrd/727 \n_[posted via JIRA by John Daily]_\n. Filed https://github.com/basho/riak_core/pull/765 \n_[posted via JIRA by John Daily]_\n. [~jdaily] Does that mean we should abandon [https://github.com/basho/riak_kv/pull/1109]? \n_[posted via JIRA by Brett Hazen]_\n. Closing this based on the comment above stating it was resolved with PR 734 in release 2.1.1 \n_[posted via JIRA by Patricia Brewer]_\n. Checked - rel has limited_developer_mem=false \n_[posted via JIRA by Jon Meredith]_\n. Jon M confirmed code merged to 2.0.6 on 6/25/15.  Dev is done. Now just waiting to go into additional releases.  \n_[posted via JIRA by Patricia Brewer]_\n. Confirmed for Riak 2.2 \n_[posted via JIRA by Patricia Brewer]_\n. So's it's been recorded, this week I started the process of updating riak-debug in a few ways. Gathering meaningful bucket (type) property data is high on the TODO list. I had CLISERV-32 assigned to myself, so I'd be happy to take this task if that would be appropriate. It may take me a while to produce any meaningful changes, though, as I'll be working on this between normal CSE activities. \n[posted via JIRA by Drew Pirrone-Brusse]. Need to consider what effect this will have on Riak running on non-Basho OTP, and whether that should be a concern. \n_[posted via JIRA by Ted Burghart]_\n. It will only change the etop output so it would be nice to know how much that is actually being used.\nAnd if the information is relevant - might be that we should change the information.\nOr do we have other things that will show the same information? \n_[posted via JIRA by Torben Hoffmann]_\n. Re-assigning this to Andy, since it's really a change in the erlang parser code that needs to happen, and I suspect he's a lot more familiar with it than I am.  Happy to work on it too, but I'll wait for Andy to have a first crack at it. \n_[posted via JIRA by Erik Leitch]_\n. Working on this currently. \n_[posted via JIRA by Andy Till]_\n. And as you noted, this is:\npartly the unbounded range query problem\nie, we don't support time > 100, therefore can't support time > 100 AND time != 400\nwhich we've decided is definitely not part of TS1.0, in which case we just need to document what's allowed and not allowed for TS1.0\npartly error handling\nwe should throw an error then if we get passed a query like time > 100 AND time != 400\nwhich should definitely be part of TS1.0, and \npartly fixable\ntime > 100 AND time < 400 AND time != 300 could be supported since it's bounded, but we have to pass the last clause to the filter\nwhich could be part of TS1.0\nThis last bit is tricky, though.  What if someone passes time > 100 AND time < 1000 AND time < 300 AND time > 200?  You'd have to sort through all the time clauses to figure out 1) if you can construct a bounded query, and if so 2) what's the most liberal bound you can construct (ie, time > 100 AND time < 1000 in the above example), and then pass all the rest as part of the filter.  I don't know how much work that is in practice, so do we pursue this now or leave it for a later TS release?\nMaybe the best path for now is not to allow these sorts of queries, and make sure we throw meaningful errors if we get an unbounded query?  \n[~atill] you should comment on this when you've had a chance to scope out the work (or you may already know the answer). \n_[posted via JIRA by Erik Leitch]_\n. And as you noted, this is:\npartly the unbounded range query problem\nie, we don't support time > 100, therefore can't support time > 100 AND time != 400\nwhich we've decided is definitely not part of TS1.0, in which case we just need to document what's allowed and not allowed for TS1.0\npartly error handling\nwe should throw an error then if we get passed a query like time > 100 AND time != 400\nwhich should definitely be part of TS1.0, and \npartly fixable\ntime > 100 AND time < 400 AND time != 300 could be supported since it's bounded, but we have to pass the last clause to the filter\nwhich could be part of TS1.0\nThis last bit is tricky, though.  What if someone passes time > 100 AND time < 1000 AND time < 300 AND time > 200?  You'd have to sort through all the time clauses to figure out 1) if you can construct a bounded query, and if so 2) what's the most liberal bound you can construct (ie, time > 100 AND time < 1000 in the above example), and then pass all the rest as part of the filter.  I don't know how much work that is in practice, so do we pursue this now or leave it for a later TS release?\nMaybe the best path for now is not to allow these sorts of queries, and just make sure we throw meaningful errors if we get an unbounded query?  \n[~atill] you should comment on this when you've had a chance to scope out the work (or you may already know the answer). \n_[posted via JIRA by Erik Leitch]_\n. And as you noted, this is:\npartly the unbounded range query problem\nie, we don't support time > 100, therefore can't support time > 100 AND time != 400\nwhich we've decided is definitely not part of TS1.0, in which case we just need to document what's allowed and not allowed for TS1.0\npartly error handling\nwe should throw an error then if we get passed a query like time > 100 AND time != 400\nwhich should definitely be part of TS1.0, and \npartly fixable\ntime > 100 AND time < 400 AND time != 300 could be supported since it's bounded, but we have to pass the last clause to the filter\nwhich could be part of TS1.0\nThis last bit is tricky, though.  What if someone passes time > 100 AND time < 1000 AND time < 300 AND time > 200?  You'd have to sort through all the time clauses to figure out 1) if you can construct a bounded query, and if so 2) what's the most liberal bound you can construct (ie, time > 100 AND time < 1000 in the above example), and then pass all the rest as part of the filter.  I don't know how much work that is in practice, so do we pursue this now or leave it for a later TS release?\nMaybe the best path for now is not to allow these sorts of queries, and just make sure we throw meaningful errors if we get an unbounded query?  \n[~atill] you should comment on this when you've had a chance to scope out the work (or you may already know the answer). \n_[posted via JIRA by Erik Leitch]_\n. PR https://github.com/basho/riak_kv/pull/1271 \n_[posted via JIRA by Andy Till]_\n. Needs to forward merge \n_[posted via JIRA by Patricia Brewer]_\n. https://github.com/basho/riak/pull/798 for riak\nhttps://github.com/basho/riak_ee/pull/365 for riak_ee\nBoth are in the 2.0 branches of the respective projects - just need to be forward-merged to 2.1 (and make sure we don't pick up anything else unless it's relevant). \n_[posted via JIRA by Douglas Rohrer]_\n. https://github.com/basho/riak/pull/808 for riak - riak_ee was the first commit on the develop-2.2 branch w/o PR. \n_[posted via JIRA by Douglas Rohrer]_\n. Confirmed fix is on riak and riak_ee, both for 2.0 and develop-2.2 branches \n_[posted via JIRA by Douglas Rohrer]_\n. [~Mark Allen] please review the attached PR - it's really simple (increase a number, and fix the test to expect that number). \n_[posted via JIRA by Douglas Rohrer]_\n. Validated tests and +1'd \n_[posted via JIRA by Mark Allen]_\n. [~lbakken] - In previous releases we set this to the TS version, e.g., 1.2.0.  Are you sure we need 2.1.1? I know for client capabilities this would be useful to be 2.1+ \n_[posted via JIRA by Brett Hazen]_\n. [8:18 AM] Luke Bakken: It is fixed in the 1.3.1 tag: https://github.com/basho/riak/blob/riak_ts-1.3.1/rel/reltool.config\n[8:18 AM] Luke Bakken: Let me make sure it's fixed in the right branch too\n[8:19 AM] Luke Bakken: Yep, looks to be fixed https://github.com/basho/riak/commit/3f1a80bab659e844a12ddb21f25d9084e0af0940#diff-ee46c4f44a5bbed6a... \n_[posted via JIRA by Derek Somogyi]_\n. [~lbakken] Sheeeeit.  OK. Then I just broke it with recent merges because I thought previously we had been using the TS numbering system, but Github tells me I'm delusional.  I'll fix.\n_[posted via JIRA by Brett Hazen]_\n. Charlie commenting. \n_[posted via JIRA by Charlie Voiselle]_\n. -- This notification was sent from JIRA RIAK-2639 to all linked Zendesk tickets by Derek Somogyi.--\nTest for \"Notify Zendesk\" comments...\n_[posted via JIRA by Derek Somogyi]_\n. Test \"Jira Comments\" field \n_[posted via JIRA by Derek Somogyi]_\n. Comment - \"Ready for Review\" \n_[posted via JIRA by Derek Somogyi]_\n. Consider for 2.3.  Needs some design and thought.  2.2 is on a faster path.   \n_[posted via JIRA by Patricia Brewer]_\n. Testing how comments in Jira (when connected to a Zendesk ticket) are passed to Zendesk...if at all. \n_[posted via JIRA by Derek Somogyi]_\n. Reviewed and merged per Brian.   \n_[posted via JIRA by Patricia Brewer]_\n. For historical purposes, this is completed in PR https://github.com/basho/riak/pull/876. It adds the required object sync to warn users of \"unrepairable\" objects that suffer from a pre-2.1 bug with version vectors being equal even when the object are not equal. \n_[posted via JIRA by Douglas Rohrer]_\n. PR:\nhttps://github.com/basho/riak_ee/pull/411 \n[posted via JIRA by Luke Bakken]. ",
    "chapani": "I did that before and it didn't help. Here is the fresh one:\nbash\nps aux | grep riak\nld        4134  0.0  0.0   7404   596 ?        S    15:31   0:00 /home/ld/apps/riak-2.0.0pre11/rel/riak/bin/../erts-5.9.2/bin/epmd -daemon\nld        4142  0.0  0.0  14824   808 ?        S    15:31   0:00 /home/ld/apps/riak-2.0.0pre11/rel/riak/bin/../erts-5.9.2/bin/run_erl -daemon /tmp//home/ld/apps/riak-2.0.0pre11/rel/riak/bin/..// /home/ld/apps/riak-2.0.0pre11/rel/riak/bin/../log exec /home/ld/apps/riak-2.0.0pre11/rel/riak/bin/riak console\nld        4145  4.6  6.9 7738420 70168 pts/6   Ssl+ 15:31   3:19 /home/ld/apps/riak-2.0.0pre11/rel/riak/bin/../erts-5.9.2/bin/beam.smp -P 256000 -e -Q -A 64 -K true -W w -- -root /home/ld/apps/riak-2.0.0pre11/rel/riak/bin/.. -progname riak -- -home /home/ld -- -boot /home/ld/apps/riak-2.0.0pre11/rel/riak/bin/../releases/2.0.0pre11/riak -config /home/ld/apps/riak-2.0.0pre11/rel/riak/data/generated.configs/app.2014.03.17.15.31.47.config -setcookie riak 256000 65536 -name riak@127.0.0.1 -smp enable -vm_args /home/ld/apps/riak-2.0.0pre11/rel/riak/data/generated.configs/vm.2014.03.17.15.31.47.args -pa /home/ld/apps/riak-2.0.0pre11/rel/riak/bin/../lib/basho-patches -- console\nld        4395  0.0  0.0   4288   608 ?        Ss   15:31   0:00 /home/ld/apps/riak-2.0.0pre11/rel/riak/bin/../lib/os_mon-2.2.10/priv/bin/memsup\nld        4396  0.0  0.0   4288   352 ?        Ss   15:31   0:00 /home/ld/apps/riak-2.0.0pre11/rel/riak/bin/../lib/os_mon-2.2.10/priv/bin/cpu_sup\n. bash\nld$ ps aux | grep riak\nld        7080  0.0  0.0   9380   936 pts/4    S+   16:49   0:00 grep --color=auto riak\nld$ ./riak start\nriak failed to start within 15 seconds,\nsee the output of 'riak console' for more information.\nIf you want to wait longer, set the environment variable\nWAIT_FOR_ERLANG to the number of seconds to wait.\nld$\n. did you mean this:\nbash\nld$ ps aux | tail -f\nld        2935  0.0  1.0  29468 10820 pts/0    Ss   15:30   0:00 -bash\nld        3121  0.0  0.1  17792  1164 pts/0    S+   15:30   0:00 tmux new -s processes\nld        3123  0.1  0.1  26540  1896 ?        Ss   15:30   0:05 tmux new -s processes\nld        3124  0.0  1.0  29520 10840 pts/1    Ss+  15:30   0:01 -bash\nld        3310  0.0  1.0  29520 10848 pts/2    Ss+  15:31   0:01 -bash\nld        3460  0.0  1.0  29520 10844 pts/3    Ss+  15:31   0:01 -bash\nld        5219  0.0  1.0  29552 10948 pts/4    Ss   15:42   0:01 -bash\nroot      7756  0.0  0.0      0     0 ?        S    16:49   0:00 [kworker/u16:2]\nld        8003  0.0  0.1  18156  1272 pts/4    R+   17:05   0:00 ps aux\nld        8004  0.0  0.0   7192   680 pts/4    S+   17:05   0:00 tail -f\nld$ ps aux | grep riak\nld        8006  0.0  0.0   9380   936 pts/4    S+   17:05   0:00 grep --color=auto riak\nld$ ./riak start\nriak failed to start within 15 seconds,\nsee the output of 'riak console' for more information.\nIf you want to wait longer, set the environment variable\nWAIT_FOR_ERLANG to the number of seconds to wait.\n. thanks!\nI don't know what epmd is. How can I check it?\n. I killed both epmd and beam.smp, but it didn't help. I rebooted, checked ports and they were free. Still having the \"WAIT_FOR_ERLANG\" error.\nBtw, I was able to start my old Riak. So, it must be something only 2.0-related.\n. the last build was successfully started: riak-2.0.0pre15\nthanks guys,\nBuriwoy\n. Finally figured out the reason. It was /home/user/.erlang.cookie. After deleting it, riak started successfully. Hope it will help someone :)\n. If you mean riak installation by \"the whole system\", no, I used neither root nor sudo. Except for dependencies:\nbash\nsudo apt-get install build-essential libc6-dev-i386 git\nBut I don't think it has anything to do with the issue.\n. No, everything is the same, in the user's home dir. That same user installed basho's erlang and Riak. \nI also tried to change user:group recursively under that user (my guess is even before that everything belonged to the user). It didn't help.\n. Here is what I got:\n``` bash\nErlang R16B02_basho8 (erts-5.10.3) [source] [64-bit] [smp:2:2] [async-threads:10] [hipe] [kernel-poll:false]\nEshell V5.10.3  (abort with ^G)\n1> dets:open_file(test, [{file,\"./data/cluster_meta/manifest.dets\"}]).\n{error,{file_error,\"./data/cluster_meta/manifest.dets\",\n                   eacces}}\n2> dets:open_file(test, [{file,\"/tmp/foo.dets\"}]).\n{ok,test}\n```\n. yes, it exists. Surprisingly, it belongs to root:\nbash\nls -l data/cluster_meta/manifest.dets \n-rw-r--r-- 1 root root 5464 Sep 17 05:53 data/cluster_meta/manifest.dets\n. what directories you mean? data/* ?\n. I got a new error:\nbash\nbin/riak start\nmkdir: cannot create directory \u2018/tmp//home/user/apps/new_riak-2.1.1\u2019: Permission denied\nriak failed to start within 15 seconds,\nsee the output of 'riak console' for more information.\nIf you want to wait longer, set the environment variable\nWAIT_FOR_ERLANG to the number of seconds to wait.\n. Thanks, @mrallen1 for your efforts!\nI figured out the issue. It was /tmp//home/user/apps/new_riak-2.1.1. After deleting /tmp/home, it worked.\nthanks again, have a good weekend,\n-- buriwoy\n. ",
    "jonasrichard": "About the basho-patches it would be good to do a\nm(riak_patched_module).\nfor all files in the lib/basho-pacthes in order to see when it was compiled, what kind of export it has, etc. It would be good to see some kind of commit hash.\n. According to this conversation, it has been introduced in util-linux 2.20.1 and yes, some older OSes don't support it. Is there any way to check if dmesg -T is supported?\nhttp://stackoverflow.com/questions/13890789/convert-dmesg-timestamp-to-custom-date-format\n. Is it possible to apply that fix to 1.4.x product line?\n. ",
    "andrewjstone": "A wild @PharkMillups appears\n. Yep, that's all I know of. Valid close :)\n. @jaredmorrow AFAIK the riak_test scripts don't build the devrels required though, they just copy them. The error that occurs with less nodes is also cryptic. Can't write app.config essentially. This was actually a suggestion from @kellymclaughlin after I wasted some time on this today. I can revert it, but I'd like to prevent others from running into this issue another way if we do that.\n. :+1: 6fdf4e4\n. :+1: 29a3893\n. :+1:  be34633\n. This never got merged. I cherry-picked be34633 instead. closing.\n. :+1:  8263bd3\n. :+1: \n. :+1: \n. After thinking about this PR for a bit, I think we should wait until the next release cycle to perform this versioning. This PR should probably just be closed along with it's ee brethren. I think there was probably a misunderstanding in what I was asking for. We need to version every repo properly and ensure we can have repeatable builds.  I'm in the process of writing a document to describe how we should do things for the next build cycle.\n. @kuenishi I did forget about that issue. The document I'm working on addresses the same concerns in a systematic manner. I don't think we should wait too long to address the failures in our release process. I just don't think we should try to get it in by Friday for 2.0.5. IMO we shouldn't release anything after 2.0.5 until the build process is documented, standardized internally and repeatable.\n. :+1:  a8c3202\n. :+1: \n. :+1:  a7661f6\n. +1 7bb76dd\n. node_package tag 2.0.0 doesn't actually contain the tip of the 2.0 branch that includes many new changes. We need to to create a new tag for the node_package repo on the 2.0 branch head. Then of course we'll have to ensure that changes get merged back to develop eventually. We also need to do the same thing for  any other repo that has commits later than the 2.0.0 tag. \nWe then seriously need to consider our branching strategy on these repos. They don't have anywhere near the frequency of changes, or the formal release structure of riak/riak_ee/riak_core/riak_kv.\nUsually jumping major and minor versions means significant change, but in our case it mainly means \"matches riak/ee versioning.\" Since these versions change infrequently I suggest not developing on long lived versioned branches as much as we do now. For most changes we should probably just create a feature or bugfix branch directly off develop. Then we can simply merge that in and tag develop as part of the regular development process, updating rebar.config of parent projects as necessary. This will prevent the need to 'stabilize' a release and figure out \"what is in it\" for all repos. It also means we don't have to re-branch/re-tag each subrepo for each major riak release. \nOf course for repos with major changes and large, long running and diverging features we can create versioned branches as necessary.\n. ",
    "sendtopms": "Thats nice. It will be great if it applies based on detected erl. Thanks.\n. ",
    "sumerman": "@jaredmorrow Please, allow others to compile riak & riak_core with r17 without fixing all the rebar.configs?\n. ",
    "pavelsmolka": "Yes, I use Riak 1.4.8:\nbash\n$ riak-admin status | grep riak_kv_version\nriak_kv_version : <<\"1.4.8-0-g7545390\">>\n. Thanks a lot, that was simpler than I expected. What confused me is \"n_val\" in the example below, but \"r\" and \"w\" without val (not in the example):\nhttp://docs.basho.com/riak/latest/dev/references/http/set-bucket-props/#Example\nIf I do this, it works:\n``` bash\n$ curl -XPUT -i -H \"Content-Type: application/json\" -d '{\"props\":{\"n_val\":5,\"w\":2,\"r\":3}}' http://localhost:83/buckets/test/props\n$ curl -XGET -i http://localhost:83/buckets/test/props\nHTTP/1.1 200 OK\nVary: Accept-Encoding\nServer: MochiWeb/1.1 WebMachine/1.10.0 (never breaks eye contact)\nDate: Thu, 24 Apr 2014 17:06:58 GMT\nContent-Type: application/json\nContent-Length: 429\n{\"props\":{\"allow_mult\":false,\"basic_quorum\":false,\"big_vclock\":50,\"chash_keyfun\":{\"mod\":\"riak_core_util\",\"fun\":\"chash_std_keyfun\"},\"dw\":\"quorum\",\"last_write_wins\":false,\"linkfun\":{\"mod\":\"riak_kv_wm_link_walker\",\"fun\":\"mapreduce_linkfun\"},\"n\":5,\"n_val\":5,\"name\":\"test\",\"notfound_ok\":true,\"old_vclock\":86400,\"postcommit\":[],\"pr\":0,\"precommit\":[],\"pw\":0,\"r\":3,\"rw\":\"quorum\",\"search\":false,\"small_vclock\":50,\"w\":2,\"young_vclock\":20}}\n```\n. ",
    "jameshfisher": "Okay, thanks. I'll leave it to you to decide on closing the ticket.\n. ",
    "areski": "@jaredmorrow the final release is out for some time but the problem still remain, as 14.04 is the next LTS it might be good to find a solution:\nhttp://docs.basho.com/riak/latest/ops/building/installing/debian-ubuntu/\n. ",
    "see0": "It seems like the official repository is not yet updated for Ubuntu 14.04 and CentOS 6. Should I just switch to installing from binaries distributed directly instead of using official repositories?\n. ",
    "brett-w-thompson": "I ran into the same issue- perhaps the fixing-the-docs PR was never merged?\n```\ncurl http://apt.basho.com/gpg/basho.apt.key | sudo apt-key add -\nsudo bash -c \"echo deb http://apt.basho.com $(lsb_release -sc) main > /etc/apt/sources.list.d/basho.list\"\nsudo apt-get update\n```\n...snip...\n```\n Hit http://security.ubuntu.com trusty-security/universe Translation-en\n W: Failed to fetch http://apt.basho.com/dists/trusty/main/binary-amd64/Packages  403  Forbidden\nE: Some index files failed to download. They have been ignored, or old ones used instead.\n```\n. ",
    "roooms": "create jira issue\n. Full output of riak-admin bucket-type status default when using riak.conf and app.config as above:\n- with riak.conf\nallow_mult: false\n  basic_quorum: false\n  big_vclock: 50\n  chash_keyfun: {riak_core_util,chash_std_keyfun}\n  dvv_enabled: false\n  dw: quorum\n  last_write_wins: false\n  linkfun: {modfun,riak_kv_wm_link_walker,mapreduce_linkfun}\n  n_val: 3\n  notfound_ok: true\n  old_vclock: 86400\n  postcommit: []\n  pr: 0\n  precommit: []\n  pw: 0\n  r: quorum\n  rw: quorum\n  small_vclock: 50\n  w: quorum\n  young_vclock: 20\n- with app.config\nallow_mult: true\n  basic_quorum: false\n  big_vclock: 50\n  chash_keyfun: {riak_core_util,chash_std_keyfun}\n  dvv_enabled: true\n  dw: quorum\n  last_write_wins: false\n  linkfun: {modfun,riak_kv_wm_link_walker,mapreduce_linkfun}\n  n_val: 3\n  notfound_ok: true\n  old_vclock: 86400\n  postcommit: []\n  pr: 0\n  precommit: []\n  pw: 0\n  r: quorum\n  rw: quorum\n  small_vclock: 50\n  w: quorum\n  young_vclock: 20\nDifferences:\n- allow_mult\n- dvv_enabled\n. Resolved in https://github.com/basho/riak/pull/857. Forgot to add I tested this by running riak-debug pre- and post-change and compared output. Only cluster-info and cluster-info.html are missing from post-change riak-debug, as desired. \n. ",
    "carlskii": "Thanks this makes sense now. Might be worth highlighting this useful info in the docs.\n. OK - So I sorted this problem. I had a duplicate field within my custom schema. This was visible in the Solr Web UI. Are there any plans to validate the schema at this stage? \ncurl -XPUT \"$RIAK_HOST/search/index/ved\" -H 'content-type:application/json' -d '{\"schema\":\"custom\"}\nor is there a command to check the status for the Solr core that gets created via the above command. Since #545 (\"All use of Solr's HTTP port should be avoided\") suggests keeping well a way from Solr's HTTP interface there needs to be a way to check the status for the schema. \n. ",
    "mogadanez": "Looks like actually remove index leave item in that state\nSequence ( probably have more steps that need for reproduce ), operations not overlap, doing one by one.\n23:38:59.925 - Add index\n<add>\n  <doc>\n    <field name=\"id\">8599212460240-e85d22fd888746f9862a0dde04d66f4f</field>\n    <field name=\"originalID\">e85d22fd888746f9862a0dde04d66f4f</field>\n    ...\n  </doc>\n</add>\n23:38:59.942 - Got response\n23:39:00.866 - Add index ( same data inserted over ) \n<add>\n  <doc>\n    <field name=\"id\">8599212460240-e85d22fd888746f9862a0dde04d66f4f</field>\n    <field name=\"originalID\">e85d22fd888746f9862a0dde04d66f4f</field>\n    ...\n  </doc>\n</add>\n23:39:00.866 - Got response\n23:39:01.514 - Remove index\n<delete>\n  <id>8599212460240-e85d22fd888746f9862a0dde04d66f4f</id>\n </delete>\n23:39:01.529 - Got response\n23:39:01.547 - Add index \n<add>\n  <doc>\n    <field name=\"id\">8599212458595-e85d22fd888746f9862a0dde04d66f4f</field>\n    <field name=\"originalID\">e85d22fd888746f9862a0dde04d66f4f</field>\n    ...\n  </doc>\n</add>\n23:39:01.547- Got response\nAfter that:\nq=originalID:e85d22fd888746f9862a0dde04d66f4f&fl=id returns:\n<result name=\"response\" numFound=\"2\" start=\"0\" maxScore=\"0.0\">\n<doc>\n<str name=\"id\">8599212458595-e85d22fd888746f9862a0dde04d66f4f</str>\n</doc>\n<doc>\n<str name=\"id\">8599212460240-e85d22fd888746f9862a0dde04d66f4f</str>\n</doc>\n</result>\nq=originalID:e85d22fd888746f9862a0dde04d66f4f returns:\n<result name=\"response\" numFound=\"2\" start=\"0\" maxScore=\"0.353553\">\n<doc>\n<str name=\"id\">8599212458595-e85d22fd888746f9862a0dde04d66f4f</str>\n<str name=\"originalID\">e85d22fd888746f9862a0dde04d66f4f</str>\n</doc>\n</result>\nI cannot reproduce with same sequence( update index over existing ) via curl running it one by one, so i believe main reason is in concurrency\nriak 1.4.8, cluster of 5 nodes, aws\n. Reproduced also without index over with same Id( delete every time )\nhttp://joxi.ru/Q25-UxjKTJAlTvTn20A\nq=originalID:2196661f49c640378952200f8150734e&fl=id\n<result name=\"response\" numFound=\"2\" start=\"0\" maxScore=\"0.0\">\n<doc>\n<str name=\"id\">8599205980559-2196661f49c640378952200f8150734e</str>\n</doc>\n<doc>\n<str name=\"id\">8599205982553-2196661f49c640378952200f8150734e</str>\n</doc>\n</result>\nq=originalID:2196661f49c640378952200f8150734e\n<result name=\"response\" numFound=\"2\" start=\"0\" maxScore=\"0.353553\">\n<doc>\n<str name=\"id\">8599205980559-2196661f49c640378952200f8150734e</str>\n<str name=\"originalID\">2196661f49c640378952200f8150734e</str>\n</doc>\n</result>\n. Since  Riak 2.0 will stop Solr update interfaces ( https://github.com/basho/riak_search/issues/141#issuecomment-35469036 )\nI remake it for now to new paradigm\nWrite it like Riak. Query it like Solr.\n. Reopened, because it is not helps.\nNow I use another bucket to store re-structured data.\nbucket options:\n{\nprops: {\nallow_mult: true,\nbasic_quorum: false,\nbig_vclock: 50,\nchash_keyfun: {\nmod: \"riak_core_util\",\nfun: \"chash_std_keyfun\"\n},\ndw: \"quorum\",\nlast_write_wins: false,\nlinkfun: {\nmod: \"riak_kv_wm_link_walker\",\nfun: \"mapreduce_linkfun\"\n},\nn_val: 3,\nname: \"ordered-proofs-v3\",\nnotfound_ok: true,\nold_vclock: 86400,\npostcommit: [ ],\npr: 0,\nprecommit: [\n{\nmod: \"riak_search_kv_hook\",\nfun: \"precommit\"\n}\n],\npw: 0,\nr: \"quorum\",\nrw: \"quorum\",\nsearch: true,\nsmall_vclock: 50,\nw: \"quorum\",\nyoung_vclock: 20\n}\n}\nSo, instead post to solr index interface I  save and remove items from this bucket, and riak_search_kv_hook should handle it and maintain  search index. \nBut  problem still here. With riak search i get count  more data that exists in bucket\n. No, I'm still in 1.4.8 in production. Does 2.0 is ready to use in\nproduction?\n\u043f\u044f\u0442\u043d\u0438\u0446\u0430, 23 \u043c\u0430\u044f 2014 \u0433. \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044c Ryan Zezeski \u043d\u0430\u043f\u0438\u0441\u0430\u043b:\n\nAre you trying to use the new search in Riak 2.0 now? If so the\nriak_search_kv_hook has nothing to do with new search.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/basho/riak/issues/547#issuecomment-44034554\n.\n. Reproduce:\nhttps://github.com/mogadanez/riak-search-index-test\n\nIt fails not every time, sometimes at first attempt, sometimes from 20th.\nIt fails only on cluster setup( 5 nodes with haproxy in front, AWS, c3.large, Debian, Riak 1.4.8 )\nUPDATE:\n~~Cannot reproduce on  5 nodes c3.xlarge cluster with EBS optimized , so i belive some micro-racing conditions appears.~~\nUPDATE2:\nIt still reproduced with EBS optimized, but need more attempts,\n- I also try:  use last_write_wins:true - still reproduce.\n- Add delay ( 1sec ) between Save and remove -   still reproduce.\n. I able to reproduce it  with allow_mult: false and last_write_wins: false\nAlso, simplest reproduce sequence is Save, Remove, without any siblings or concurrent writes.\nI add test with allow_mult: false ( in separate new bucket, so it never have  allow_mult: true  )\n. I remove 2 nodes from cluster,  and it now harder to reproduce. sometimes 20000 retries will not reproduce issue.\nAnd it completely stop reproduced when I back to single node setup( runs tests several hours )\n. ",
    "jwmtllc": "I understand this is closed but I ran into something similar. I had a Solr index that I was unable to delete. My error was that I had accidently associated two buckets with the same bucket-type, the bucket-type associated with my index. This may be okay in some cases. In my case the identical lookup key was created twice (along with additional data I did not want indexed!). I was able to find this error by looking at the yz_rb field from a curl invocation on the index. The remedy was to run a curl -XDELETE on both buckets (with the same key). I believe I have the latest version of Riak KV.\n. ",
    "taurusser": "issue still be there.\nubuntu 14.04 trusty\nerlang : otp_src_R16B02-basho5\neleveldb : rolling tag 1.2.0\nmake rel\n==> riak_ensemble (compile)\n==> pbkdf2 (compile)\n==> riak_core (compile)\n==> riak_pipe (compile)\n==> protobuffs (compile)\n==> riak_pb (compile)\n==> mochiweb (compile)\n==> ibrowse (compile)\n==> webmachine (compile)\n==> riak_api (compile)\n==> riak_dt (compile)\n==> eunit_formatters (compile)\n==> riak_kv (compile)\n==> merge_index (compile)\n==> lucene_parser (compile)\n==> riak_search (compile)\n==> erlydtl (compile)\n==> riak_control (compile)\nCompiled handlebars asset priv/admin/js/generated/templates.js\n==> riaknostic (compile)\n==> kvc (compile)\n==> yokozuna (compile)\n==> canola (compile)\n==> riak_auth_mods (compile)\n==> rebar_lock_deps_plugin (compile)\n==> rel (compile)\n==> riak (compile)\n==> riak (compile)\n./rebar generate \n==> rel (generate)\nERROR: Unexpected error: {'EXIT',{{badmatch,{error,enoent}},\n                                  [{rebar_reltool,execute_overlay,4,[]},\n                                   {rebar_reltool,generate,2,[]},\n                                   {rebar_core,run_modules,4,[]},\n                                   {rebar_core,execute,5,[]},\n                                   {rebar_core,process_dir1,6,[]},\n                                   {rebar_core,process_each,5,[]},\n                                   {rebar_core,process_dir1,6,[]},\n                                   {rebar_core,process_commands,2,[]}]}}\nERROR: generate failed while processing /home/pandora/download/riak/rel: rebar_abort\nmake: *** [generate] \u9519\u8bef 1\n. ",
    "NoKriK": "Hi,\nI just hit the same issue while trying to compile Riak on Scaleway.\n./rebar generate\n==> rel (generate)\nERROR: Unexpected error: {'EXIT',\n                             {{badmatch,{error,enoent}},\n                              [{rebar_reltool,execute_overlay,4,\n                                   [{file,\"src/rebar_reltool.erl\"},\n                                    {line,388}]},\n                               {rebar_reltool,generate,2,\n                                   [{file,\"src/rebar_reltool.erl\"},{line,60}]},\n                               {rebar_core,run_modules,4,\n                                   [{file,\"src/rebar_core.erl\"},{line,491}]},\n                               {rebar_core,execute,6,\n                                   [{file,\"src/rebar_core.erl\"},{line,416}]},\n                               {rebar_core,maybe_execute,8,\n                                   [{file,\"src/rebar_core.erl\"},{line,300}]},\n                               {rebar_core,process_dir1,7,\n                                   [{file,\"src/rebar_core.erl\"},{line,259}]},\n                               {rebar_core,process_each,5,\n                                   [{file,\"src/rebar_core.erl\"},{line,349}]},\n                               {rebar_core,process_dir1,7,\n                                   [{file,\"src/rebar_core.erl\"},\n                                    {line,251}]}]}}\nERROR: generate failed while processing /root/riak/rel: rebar_abort\nMakefile:34: recipe for target 'generate' failed\nmake: *** [generate] Error 1\nHere is the environment in which I'm trying to compile Riak:\nErlang version: R16B02-basho8 built using kerl\nOS: Ubuntu 15.04 (Vivid)\nRiak tag: 2.1.1\nElevelDB: 1.1.1\nI just found that version 1.1.1 of ElevelDB didn't have the two configuration template files which rebar is awaiting.\nThis solves the issue:\ncurl https://raw.githubusercontent.com/basho/eleveldb/2.1.1/priv/eleveldb.schema > deps/eleveldb/priv/eleveldb.schema\ncurl https://raw.githubusercontent.com/basho/eleveldb/2.1.1/priv/eleveldb_multi.schema > deps/eleveldb/priv/eleveldb_multi.schema\nThe complete build now succeed but Riak is giving a nice Segmentation Fault.\n. ",
    "kesslerm": "Addressed in #797. Closing.\n. Checking for distribution specific, undocumented strings is usually a bad idea. I'd just leave out the second line altogether. If the config file exists, collect it, otherwise not. Also, apart from Ubuntu, there are other distributions that make use of /etc/default, and the equivalent /etc/sysconfig exists on RedHat and SuSE.\n. Duplicate of #671\n. Hi molele2,\nI can assure you that the riak project is very much alive. Riak has been created by basho (http://www.basho.com) and is actively maintained and developed further by this company.\nRegarding your issue, I note that the very first error relates to a missing header file. Please make sure that you install the development packages for your system. In particular you will want to have the pam-dev package.\nHope this helps.\nRegards,\nMagnus Kessler\nBasho Client Services Engineer\n. Hi molele2,\nAs already advised on #671 and #672, you have to have the development packages available for your system. You don't seem to have the necessary header files for 'pam' installed. According to http://www.rpmseek.com/rpm-pl/pam-devel.html?hl=com&cx=0:: for CentOS 6 you need pam-devel-1.1.1. I don't have that version of CentOS available to me to verify this exact version.\nUnless you make sure you have the necessary header files in place you will never be able to compile Riak from source.\nBut as Sean advised, you can also download Basho's pre-compiled binary packages.\nFor a successful installation of Riak on CentOS, please follow the instructions on http://docs.basho.com/riak/latest/ops/building/installing/rhel-centos/\nRegards,\nMagnus\nCustomer Services Engineer @ Basho\n. Merged as part of PR #676. Closing.\n. Hi @molele2,\nhave you followed the instructions in http://docs.basho.com/riak/latest/dev/using/data-types/#Setting-Up-Buckets-to-Use-Riak-Data-Types? You need to set up your CRDT buckets separately from the command line:\nriak-admin bucket-type create maps '{\"props\":{\"datatype\":\"map\"}}'\nriak-admin bucket-type create sets '{\"props\":{\"datatype\":\"set\"}}'\nriak-admin bucket-type create counters '{\"props\":{\"datatype\":\"counter\"}}'\nMay I also suggest to use the public riak-users mailing list (http://lists.basho.com/mailman/listinfo/riak-users_lists.basho.com) for support questions. The issue tracker is for genuine bugs only.\nHope this helps.\nRegards,\nMagnus\n. Dear @molele2,\nrunning your latest message through google translator suggest that you were severely inebriated writing this. I will close this bug report. Please come back once you are able to formulate your thoughts in a clearer manner.\n. Hi @molele2,\nThe github issue tracker is not a support forum. Please subscribe to our public mailing list. Instructions to do this are available at http://lists.basho.com/mailman/listinfo/riak-users_lists.basho.com\nThat said, you should use version 2.0 of the riak-java-client with Riak-2.0.x.\nI hope to see your further questions on the mailing list and will be happy to answer them there.\nRegards,\nMagnus\n. Using cuttlefish (riak.conf), the allow_mult = true default is set for the default bucket type at https://github.com/basho/riak_kv/blob/develop/priv/riak_kv.schema#L512\nWithout riak.conf no further overrides of the bucket defaults for all buckets (https://github.com/basho/riak_core/blob/develop/src/riak_core_bucket_type.erl#L135) seem to happen. The defaults for the default bucket are assembled through app_helper:getenv in https://github.com/basho/riak_core/blob/develop/src/riak_core_bucket_props.erl#L108.\nA default app.config from version 1.4 won't have the riak_core.default_bucket_props settings, and the code will therefore fall back to the hardcoded defaults. We should test if adding a section for riak_core.default_bucket_props to app.config can be used as a workaround and document it.\n. The following patch against riak-debug from 2.1.2 seems to work on stock Solaris 10\n``\n--- /opt/riak/sbin/riak-debug.orig  Wed Dec  2 00:25:23 2015\n+++ /opt/riak/sbin/riak-debug   Thu Dec  3 17:51:49 2015\n@@ -30,7 +30,8 @@\n     POSIX_SHELL=\"true\"\n     export POSIX_SHELL\n     # To support 'whoami' add /usr/ucb to path\n-    PATH=/usr/ucb:$PATH\n+    # To use 'nawk' as 'awk', add /usr/xpg4/bin to path\n+    PATH=/usr/xpg4/bin:/usr/ucb:$PATH\n     export PATH\n     exec /usr/bin/ksh $0 \"$@\"\n fi\n@@ -491,7 +492,7 @@\n     riak_epaths=make_app_epaths \"${riak_app_config}\"`\n # Get one http listener (epath might output a newline-separated list).\n\n\nriak_api_http=\"epath 'riak_api http' \"$riak_epaths\" | sed -e 's/^\"//' -e 's/\" /:/' | head -n1\"\n\nriak_api_http=\"epath 'riak_api http' \"$riak_epaths\" | sed -e 's/^\\\"//' -e 's/\\\" /:/' | head -n1\"\n# Dump the output of the /stats HTTP endpoint.\n dump riak_http_stats curl -s \"http://$riak_api_http/stats\"\n@@ -500,7 +501,7 @@\n mkdir_or_die \"${start_dir}\"/\"${debug_dir}\"/ring/.info\n cd \"${start_dir}\"/\"${debug_dir}\"/ring\n\n\nring_dir=\"epath 'riak_core ring_state_dir' \"$riak_epaths\" | sed -e 's/^\"//' -e 's/\".*$//'\"\n\n\nring_dir=\"epath 'riak_core ring_state_dir' \"$riak_epaths\" | sed -e 's/^\\\"//' -e 's/\\\".*$//'\"\n     if [ '/' != echo \"$ring_dir\" | cut -c1 ]; then\n         # relative path. prepend base dir\n         ring_dir=\"$riak_base_dir\"/\"$ring_dir\"\n@@ -533,13 +534,13 @@\n     # if not already made, make a flat, searchable version of the app.config\n     [ -z \"$riak_epaths\" ] && riak_epaths=make_app_epaths \"${riak_app_config}\"\n\n\nyz_dir=\"epath 'yokozuna root_dir' \"$riak_epaths\" | sed -e 's/^\"//' -e 's/\".*$//'\"\n\n\nyz_dir=\"epath 'yokozuna root_dir' \"$riak_epaths\" | sed -e 's/^\\\"//' -e 's/\\\".*$//'\"\n     if [ '/' != echo \"$yz_dir\" | cut -c1 ]; then\n         # relative path. prepend base dir\n         yz_dir=\"$riak_base_dir\"/\"$yz_dir\"\n     fi\n\n\nyz_aae_dir=\"epath 'yokozuna anti_entropy_data_dir' \"$riak_epaths\" | sed -e 's/^\"//' -e 's/\".*$//'\"\n\n\nyz_aae_dir=\"epath 'yokozuna anti_entropy_data_dir' \"$riak_epaths\" | sed -e 's/^\\\"//' -e 's/\\\".*$//'\"\n     if [ '/' != echo \"$yz_aae_dir\" | cut -c1 ]; then\n         # relative path. prepend base dir\n         yz_aae_dir=\"$riak_base_dir\"/\"$yz_aae_dir\"\n@@ -607,7 +608,7 @@\n     # if not already made, make a flat, searchable version of the app.config\n     [ -z \"$riak_epaths\" ] && riak_epaths=make_app_epaths \"${riak_app_config}\"\n\n\nlog_dir=\"epath 'riak_core platform_log_dir' \"$riak_epaths\" | sed -e 's/^\"//' -e 's/\".*$//'\"\n\n\nlog_dir=\"epath 'riak_core platform_log_dir' \"$riak_epaths\" | sed -e 's/^\\\"//' -e 's/\\\".*$//'\"\n     if [ '/' != echo \"$log_dir\" | cut -c1 ]; then\n         # relative path. prepend base dir\n         log_dir=\"$riak_base_dir\"/\"$log_dir\"\n@@ -633,9 +634,9 @@\n     fi\n# Lager info and error files\n-    new_format_lager_files=\"epath 'lager handlers lager_file_backend file' \"$riak_epaths\" | sed -e 's/^\"//' -e 's/\".*$//'\"\n+    new_format_lager_files=\"epath 'lager handlers lager_file_backend file' \"$riak_epaths\" | sed -e 's/^\\\"//' -e 's/\\\".*$//'\"\n if [ -z \"$new_format_lager_files\" ]; then\n-        lager_files=\"epath 'lager handlers lager_file_backend' \"$riak_epaths\" | cut -d' ' -f 1 | sed -e 's/^\"//' -e 's/\".*$//'\"\n+        lager_files=\"epath 'lager handlers lager_file_backend' \"$riak_epaths\" | cut -d' ' -f 1 | sed -e 's/^\\\"//' -e 's/\\\".*$//'\"\n else\n     lager_files=$new_format_lager_files\n fi\n@@ -662,7 +663,7 @@\n # Gather backend logs, listing, sizing information, etc..\n backend=epath 'riak_kv storage_backend' \"$riak_epaths\"\n if [ 'riak_kv_eleveldb_backend' = \"$backend\" ]; then\n-        leveldb_dir=\"epath 'eleveldb data_root' \"$riak_epaths\" | sed -e 's/^\"//' -e 's/\".*$//'\"\n+        leveldb_dir=\"epath 'eleveldb data_root' \"$riak_epaths\" | sed -e 's/^\\\"//' -e 's/\\\".*$//'\"\n if [ '/' != `echo \"$leveldb_dir\" | cut -c1` ]; then\n     # relative path. prepend base dir\n\n@@ -693,7 +694,7 @@\n         ' \"${start_dir}\"/\"${debug_dir}\"/logs/leveldb {} \\;\nelif [ 'riak_kv_bitcask_backend' = \"$backend\" ]; then\n-        bitcask_dir=\"epath 'bitcask data_root' \"$riak_epaths\" | sed -e 's/^\"//' -e 's/\".*$//'\"\n+        bitcask_dir=\"epath 'bitcask data_root' \"$riak_epaths\" | sed -e 's/^\\\"//' -e 's/\\\".*$//'\"\n if [ '/' != `echo \"$bitcask_dir\" | cut -c1` ]; then\n     # relative path. prepend base dir\n\n@@ -723,7 +724,7 @@\n         backend=\"epath \"riak_kv multi_backend $b\" \"$riak_epaths\" | cut -d ' ' -f 1 | uniq\"\n         if [ 'riak_kv_eleveldb_backend' = \"$backend\" ]; then\n             dr=\"epath \"riak_kv multi_backend $b riak_kv_eleveldb_backend data_root\" \"$riak_epaths\" |\n-                    sed -e 's/^\"//' -e 's/\".*$//'\"\n+                    sed -e 's/^\\\"//' -e 's/\\\".*$//'`\"\n         if [ '/' != `echo \"$dr\" | cut -c1` ]; then\n             # relative path. prepend base dir\n\n@@ -755,7 +756,7 @@\n     elif [ 'riak_kv_bitcask_backend' = \"$backend\" ]; then\n         dr=\"`epath \"riak_kv multi_backend $b riak_kv_bitcask_backend data_root\" \"$riak_epaths\" |\n\n\nsed -e 's/^\"//' -e 's/\".*$//'`\"\nsed -e 's/^\\\"//' -e 's/\\\".*$//'`\"     if [ '/' != `echo \"$dr\" | cut -c1` ]; then\n         # relative path. prepend base dir\n\n@@ -786,7 +787,7 @@\n     # an empty variable regardless of the value set in the configuration\n     # settings. We're going to grab the anti_entropy_data_dir instead, and\n     # assume it's presence on disk indicates activity.\n-    anti_entropy_dir=\"epath 'riak_kv anti_entropy_data_dir' \"$riak_epaths\" | sed -e 's/^\"//' -e 's/\".*$//'\"\n+    anti_entropy_dir=\"epath 'riak_kv anti_entropy_data_dir' \"$riak_epaths\" | sed -e 's/^\\\"//' -e 's/\\\".*$//'\"\n\n\nif [ '/' != echo \"$anti_entropy_dir\" | cut -c1 ]; then\n     # relative path. prepend base dir\n@@ -899,9 +900,9 @@\n\n\nif [ '-' = \"$outfile\" ]; then\n     # So we don't get a file literally named -\n-    tar zcf - \"${debug_dir}\"\n+    tar cf - \"${debug_dir}\" | gzip\n else\n-    tar zcf \"$outfile\" \"${debug_dir}\"\n+    tar cf - \"${debug_dir}\" | gzip > \"$outfile\"\n # provide some indication of the output filename\n printf \" $outfile\" 1>&2\n\n```\n. Has been merged by separate PRs to 2.0, 2.1 and develop 2.2 branches. Still not merged back to develop, but nobody cares(?).\n. @JeetKunDoug was this ever merged forward into the 2.1 branches of Riak and Riak-EE?\n. @alisawangJJ @sharphellowang \nYes, see if restarting the affected node resolves the issue. I'm going to mark this issue as resolved, as the github issue tracker is not a support forum.\nYou may want to ask on the riak-users mailing list, instead.\n. ",
    "darkchanter": "sorry I'm new to Github. Can't find the information of the merged bug or version tag to download. Where do I start?\n. Actually I got my version from that page. I picked the Source since openSuSE isn't available there. Looks like 2.0.0 pre15 compiles, but I couldn't start dev1 yet\n. ",
    "dcy": "I was wrong, sorry!\n. ",
    "pyrrho": "It should be noted that -T is a non-standard extension to dmesg. If anything, we should add dmesg -T to the command list rather than replacing dmesg. Even then, we would be polluting some debug archives with spurious failures.\n. Two pull requests that relate to this, #676 will fix this bug in the 2.0 branch and #678 will fix it in the 1.4 brach.\n. As the above PRs have already been merged, I'm going to go ahead and close this issue. Feel free to re-open if there's more to discuss.\n. Two pull requests that relate to this, #676 will add this functionality to the 2.0 branch and #678 will add it to the 1.4 brach.\nCurrently, the extensions that will (by default) be ignored are .pfx, .p12, .csr, .sst, .sto, .stl, .pem, and .key.\n. As the above PRs have been merged, I'm going to go ahead and close this issue. Feel free to reopen if there's more to discuss.\n. Two pull requests that relate to this, #676 will add this functionality to the 2.0 branch and #678 will add it to the 1.4 brach.\n. As the above PRs have been merged, I'm going to go ahead and close this issue. Feel free to re-open if there's more to discuss.\n. No reason not to do this.\nBecause /etc/default/riak is an Ubuntu-specific thing, I'd say changing the second line to\n[ `uname -a | grep -i \"ubuntu\" -c` -gt 0 ] && [ ! -f /etc/default/riak ] && dump etc_default_riak echo \"error: /etc/default/riak missing\"\n. I was under the impression that only Ubuntu or Ubuntu-based systems made use of /etc/default. As that seems to be an incorrect assumption, I agree that the second line should be left.\n. Added to the (work in progress?) #676 PR.\n. Thanks for raising these concerns, guys!\nAs this issue is specifically calling out documentation, it can't really be solved in the base Riak repository. I've gone ahead and opened up an identical issue in our Basho Docs repository, and we should be able to get these improvements made there. As such, I'm going to close out this issue.\nThanks again!\n. This PR (and the related three) is comically out of date. Closing it to get it off my dashboard.. This PR (and the related three) is comically out of date. Closing it to get it off my dashboard.. This PR (and the related three) is comically out of date. Closing it to get it off my dashboard.. This PR (and the related three) is comically out of date. Closing it to get it off my dashboard.. I'd rename releasenotes/riak-2.0.5.md to releasenotes/riak-2.0.md to match the format that we're apparently using in that directory.\n. Oy... I correctly wrapped every other if [ -n...\n:cake: \n. This has been, at least partially, incorporated into #907. That PR adds a section that loops over Bucket Types listed with riak-admin bucket-types list and prints the configuration set for each Type.. This PR has been incorporated in (copy/pasted into) #907. I recommend closing this PR in favor of the newer.. ",
    "5HT": "Just replace here\nhttps://github.com/basho/riak_kv/blob/develop/src/riak_kv_vnode.erl#L1983\n\"~p\" with \"~tp\"\nand also update vnode status on any error not just enoent.\nThat will fix R17 vnode status updates.\n. you can you our version of riak: https://github.com/riak-synrc\nthis is production ready riak for erlang > 17.0\n. ",
    "enewhuis": "@5HT it looks like the code in the development branch has changed since your suggestion.  I am having the same problem you did back then now also trying to hack together an R17 release.\n@jrwest I realize this isn't supported but is there any info that might help me?\n. That is quite righteous of you, thanks!\n. ",
    "masudjbd": "I'm having the following crash.log\n2016-06-27 14:25:27 =ERROR REPORT====\n* Generic server yz_solr_proc terminating \n* Last message in was {check_solr,0}\n* When Server state == {state,\"./data/yz\",#Port<0.9350>,8093,8985}\n* Reason for termination == \n** \"solr didn't start in alloted time\"\n2016-06-27 14:25:27 =CRASH REPORT====\n  crasher:\n    initial call: yz_solr_proc:init/1\n    pid: <0.581.0>\n    registered_name: yz_solr_proc\n    exception exit: {\"solr didn't start in alloted time\",[{gen_server,terminate,6,[{file,\"gen_server.erl\"},{line,744}]},{proc_lib,init_p_do_apply,3,[{file,\"proc_lib.erl\"},{line,239}]}]}\n    ancestors: [yz_solr_sup,yz_sup,<0.578.0>]\n    messages: [{'EXIT',#Port<0.9350>,normal}]\n    links: [<0.580.0>]\n    dictionary: []\n    trap_exit: true\n    status: running\n    heap_size: 376\n    stack_size: 27\n    reductions: 16181\n  neighbours:\n2016-06-27 14:25:27 =SUPERVISOR REPORT====\n     Supervisor: {local,yz_solr_sup}\n     Context:    child_terminated\n     Reason:     \"solr didn't start in alloted time\"\n     Offender:   [{pid,<0.581.0>},{name,yz_solr_proc},{mfargs,{yz_solr_proc,start_link,[\"./data/yz\",\"./data/yz_temp\",8093,8985]}},{restart_type,permanent},{shutdown,5000},{child_type,worker}]\n2016-06-27 14:25:28 =ERROR REPORT====\n* Generic server yz_solr_proc terminating \n* Last message in was {#Port<0.12213>,{exit_status,1}}\n* When Server state == {state,\"./data/yz\",#Port<0.12213>,8093,8985}\n* Reason for termination == \n** {\"solr OS process exited\",1}\n2016-06-27 14:25:28 =CRASH REPORT====\n  crasher:\n    initial call: yz_solr_proc:init/1\n    pid: <0.2339.0>\n    registered_name: yz_solr_proc\n    exception exit: {{\"solr OS process exited\",1},[{gen_server,terminate,6,[{file,\"gen_server.erl\"},{line,744}]},{proc_lib,init_p_do_apply,3,[{file,\"proc_lib.erl\"},{line,239}]}]}\n    ancestors: [yz_solr_sup,yz_sup,<0.578.0>]\n    messages: [{'EXIT',#Port<0.12213>,normal}]\n    links: [<0.580.0>]\n    dictionary: []\n    trap_exit: true\n    status: running\n    heap_size: 376\n    stack_size: 27\n    reductions: 9207\n  neighbours:\n2016-06-27 14:25:28 =SUPERVISOR REPORT====\n     Supervisor: {local,yz_solr_sup}\n     Context:    child_terminated\n     Reason:     {\"solr OS process exited\",1}\n     Offender:   [{pid,<0.2339.0>},{name,yz_solr_proc},{mfargs,{yz_solr_proc,start_link,[\"./data/yz\",\"./data/yz_temp\",8093,8985]}},{restart_type,permanent},{shutdown,5000},{child_type,worker}]\n2016-06-27 14:25:28 =SUPERVISOR REPORT====\n     Supervisor: {local,yz_solr_sup}\n     Context:    shutdown\n     Reason:     reached_max_restart_intensity\n     Offender:   [{pid,<0.2339.0>},{name,yz_solr_proc},{mfargs,{yz_solr_proc,start_link,[\"./data/yz\",\"./data/yz_temp\",8093,8985]}},{restart_type,permanent},{shutdown,5000},{child_type,worker}]\n2016-06-27 14:25:28 =SUPERVISOR REPORT====\n     Supervisor: {local,yz_sup}\n     Context:    child_terminated\n     Reason:     shutdown\n     Offender:   [{pid,<0.580.0>},{name,yz_solr_sup},{mfargs,{yz_solr_sup,start_link,[]}},{restart_type,permanent},{shutdown,5000},{child_type,supervisor}]\n2016-06-27 14:25:28 =SUPERVISOR REPORT====\n     Supervisor: {local,yz_sup}\n     Context:    shutdown\n     Reason:     reached_max_restart_intensity\n     Offender:   [{pid,<0.580.0>},{name,yz_solr_sup},{mfargs,{yz_solr_sup,start_link,[]}},{restart_type,permanent},{shutdown,5000},{child_type,supervisor}]\n. Here's the log:\n+ [[ -x /usr/sbin/riak ]]\n+ export RIAK=/usr/sbin/riak\n+ RIAK=/usr/sbin/riak\n+ export RIAK_CONF=/etc/riak/riak.conf\n+ RIAK_CONF=/etc/riak/riak.conf\n+ export USER_CONF=/etc/riak/user.conf\n+ USER_CONF=/etc/riak/user.conf\n+ export RIAK_ADVANCED_CONF=/etc/riak/advanced.config\n+ RIAK_ADVANCED_CONF=/etc/riak/advanced.config\n+ [[ -x /usr/sbin/riak-admin ]]\n+ export RIAK_ADMIN=/usr/sbin/riak-admin\n+ RIAK_ADMIN=/usr/sbin/riak-admin\n+ export SCHEMAS_DIR=/etc/riak/schemas/\n+ SCHEMAS_DIR=/etc/riak/schemas/\n+ export PB_PORT=8087\n+ PB_PORT=8087\n+ export HTTP_PORT=8098\n+ HTTP_PORT=8098\n++ ping -c1 e1336787118d\n++ awk '/^PING/ {print $3}'\n++ sed 's/[()]//g'\n+ export HOST=172.17.0.2\n+ HOST=172.17.0.2\n+ export CLUSTER_NAME=riak\n+ CLUSTER_NAME=riak\n+ export COORDINATOR_NODE=e1336787118d\n+ COORDINATOR_NODE=e1336787118d\n++ ping -c1 e1336787118d\n++ awk '/^PING/ {print $3}'\n++ sed 's/[()]//g'\n+ export COORDINATOR_NODE_HOST=172.17.0.2\n+ COORDINATOR_NODE_HOST=172.17.0.2\n++ find /etc/riak/prestart.d -name '*.sh' -print\n++ sort\n+ PRESTART='/etc/riak/prestart.d/00-update-riak-conf.sh\n/etc/riak/prestart.d/01-maybe-update-cluster-convergence.sh'\n+ for s in '$PRESTART'\n+ . /etc/riak/prestart.d/00-update-riak-conf.sh\n++ cat\n++ '[' -s /etc/riak/user.conf ']'\n+ for s in '$PRESTART'\n+ . /etc/riak/prestart.d/01-maybe-update-cluster-convergence.sh\n+++ awk -F= '/ring_size/{print $2}' /etc/riak/riak.conf\n+++ sed 's/[ ]//'\n++ RING_SIZE=64\n++ CLUSTER_CONVERGENCE=standard\n++ [[ fast == \\s\\t\\a\\n\\d\\a\\r\\d ]]\n+ /usr/sbin/riak start\n+ /usr/sbin/riak-admin wait-for-service riak_kv\nriak_kv is not up: []\nriak_kv is not up: []\nriak_kv is not up: []\nriak_kv is up\n++ find /etc/riak/poststart.d -name '*.sh' -print\n++ sort\n+ POSTSTART='/etc/riak/poststart.d/01-bootstrap-schemas.sh\n/etc/riak/poststart.d/02-bootstrap-datatypes.sh\n/etc/riak/poststart.d/99-join-cluster.sh'\n+ for s in '$POSTSTART'\n+ . /etc/riak/poststart.d/01-bootstrap-schemas.sh\n++ '[' KV == TS ']'\n+ for s in '$POSTSTART'\n+ . /etc/riak/poststart.d/02-bootstrap-datatypes.sh\n++ echo 'Looking for datatypes in /etc/riak/schemas/...'\nLooking for datatypes in /etc/riak/schemas/...\n+++ find /etc/riak/schemas/ -name '*.dt' -print\n+ for s in '$POSTSTART'\n+ . /etc/riak/poststart.d/99-join-cluster.sh\n+++ /usr/sbin/riak-admin cluster status\n+++ egrep 172.17.0.2\n++ [[ -z | (C) riak@172.17.0.2 |valid |  up   |100.0|  --   | ]]\n+ PID=1027\n+ trap '/usr/sbin/riak stop; kill 1027' SIGTERM SIGINT\n+ tail -n 1024 -f /var/log/riak/console.log\n+ wait 1027\n2017-11-09 18:15:32.354 [info] <0.7.0> Application lager started on node 'riak@172.17.0.2'\n2017-11-09 18:15:32.370 [info] <0.7.0> Application sasl started on node 'riak@172.17.0.2'\n2017-11-09 18:15:32.371 [info] <0.7.0> Application asn1 started on node 'riak@172.17.0.2'\n2017-11-09 18:15:32.375 [info] <0.7.0> Application crypto started on node 'riak@172.17.0.2'\n2017-11-09 18:15:32.375 [info] <0.7.0> Application public_key started on node 'riak@172.17.0.2'\n2017-11-09 18:15:32.384 [info] <0.7.0> Application ssl started on node 'riak@172.17.0.2'\n2017-11-09 18:15:32.387 [info] <0.7.0> Application riak_sysmon started on node 'riak@172.17.0.2'\n2017-11-09 18:15:32.404 [info] <0.7.0> Application os_mon started on node 'riak@172.17.0.2'\n2017-11-09 18:15:32.421 [info] <0.7.0> Application runtime_tools started on node 'riak@172.17.0.2'\n2017-11-09 18:15:32.431 [info] <0.7.0> Application erlang_js started on node 'riak@172.17.0.2'\n2017-11-09 18:15:32.431 [info] <0.7.0> Application xmerl started on node 'riak@172.17.0.2'\n2017-11-09 18:15:32.458 [info] <0.7.0> Application inets started on node 'riak@172.17.0.2'\n2017-11-09 18:15:32.458 [info] <0.7.0> Application mochiweb started on node 'riak@172.17.0.2'\n2017-11-09 18:15:32.469 [info] <0.7.0> Application webmachine started on node 'riak@172.17.0.2'\n2017-11-09 18:15:32.469 [info] <0.7.0> Application basho_stats started on node 'riak@172.17.0.2'\n2017-11-09 18:15:32.479 [info] <0.7.0> Application bitcask started on node 'riak@172.17.0.2'\n2017-11-09 18:15:32.500 [info] <0.7.0> Application clique started on node 'riak@172.17.0.2'\n2017-11-09 18:15:32.501 [info] <0.7.0> Application eleveldb started on node 'riak@172.17.0.2'\n2017-11-09 18:15:32.501 [info] <0.7.0> Application pbkdf2 started on node 'riak@172.17.0.2'\n2017-11-09 18:15:32.501 [info] <0.7.0> Application poolboy started on node 'riak@172.17.0.2'\n2017-11-09 18:15:32.556 [info] <0.161.0>@exometer_report:do_start_reporters:613 Starting reporters with []\n2017-11-09 18:15:32.557 [info] <0.7.0> Application exometer_core started on node 'riak@172.17.0.2'\n2017-11-09 18:15:32.713 [warning] <0.181.0>@riak_core_ring_manager:reload_ring:359 No ring file available.\n2017-11-09 18:15:32.996 [info] <0.197.0>@riak_core_capability:process_capability_changes:555 New capability: {riak_core,vnode_routing} = proxy\n2017-11-09 18:15:33.005 [info] <0.197.0>@riak_core_capability:process_capability_changes:555 New capability: {riak_core,staged_joins} = true\n2017-11-09 18:15:33.019 [info] <0.197.0>@riak_core_capability:process_capability_changes:555 New capability: {riak_core,resizable_ring} = true\n2017-11-09 18:15:33.028 [info] <0.197.0>@riak_core_capability:process_capability_changes:555 New capability: {riak_core,fold_req_version} = v2\n2017-11-09 18:15:33.037 [info] <0.197.0>@riak_core_capability:process_capability_changes:555 New capability: {riak_core,security} = true\n2017-11-09 18:15:33.054 [info] <0.197.0>@riak_core_capability:process_capability_changes:555 New capability: {riak_core,bucket_types} = true\n2017-11-09 18:15:33.064 [info] <0.197.0>@riak_core_capability:process_capability_changes:555 New capability: {riak_core,net_ticktime} = true\n2017-11-09 18:15:33.227 [info] <0.7.0> Application riak_core started on node 'riak@172.17.0.2'\n2017-11-09 18:15:33.241 [info] <0.197.0>@riak_core_capability:process_capability_changes:555 New capability: {riak_pipe,trace_format} = ordsets\n2017-11-09 18:15:33.307 [info] <0.7.0> Application riak_pipe started on node 'riak@172.17.0.2'\n2017-11-09 18:15:33.313 [info] <0.7.0> Application sidejob started on node 'riak@172.17.0.2'\n2017-11-09 18:15:33.313 [info] <0.7.0> Application riak_dt started on node 'riak@172.17.0.2'\n2017-11-09 18:15:33.313 [info] <0.7.0> Application protobuffs started on node 'riak@172.17.0.2'\n2017-11-09 18:15:33.313 [info] <0.7.0> Application riak_pb started on node 'riak@172.17.0.2'\n2017-11-09 18:15:33.378 [info] <0.7.0> Application riak_api started on node 'riak@172.17.0.2'\n2017-11-09 18:15:33.467 [info] <0.329.0>@riak_kv_env:doc_env:46 Environment and OS variables:\n2017-11-09 18:15:33.868 [info] <0.197.0>@riak_core_capability:process_capability_changes:555 New capability: {riak_kv,object_hash_version} = 0\n2017-11-09 18:15:33.884 [info] <0.197.0>@riak_core_capability:process_capability_changes:555 New capability: {riak_kv,anti_entropy} = enabled_v1\n2017-11-09 18:15:33.893 [info] <0.360.0>@riak_core_throttle:enable_throttle:119 Enabling throttle for riak_kv/aae_throttle.\n2017-11-09 18:15:33.955 [info] <0.370.0>@riak_core:wait_for_service:504 Waiting for service riak_kv to start (0 seconds)\n2017-11-09 18:15:34.006 [info] <0.387.0>@riak_kv_js_vm:init:73 Spidermonkey VM (thread stack: 16MB, max heap: 8MB, pool: riak_kv_js_map) host starting (<0.387.0>)\n2017-11-09 18:15:34.007 [info] <0.388.0>@riak_kv_js_vm:init:73 Spidermonkey VM (thread stack: 16MB, max heap: 8MB, pool: riak_kv_js_map) host starting (<0.388.0>)\n2017-11-09 18:15:34.009 [info] <0.389.0>@riak_kv_js_vm:init:73 Spidermonkey VM (thread stack: 16MB, max heap: 8MB, pool: riak_kv_js_map) host starting (<0.389.0>)\n2017-11-09 18:15:34.010 [info] <0.390.0>@riak_kv_js_vm:init:73 Spidermonkey VM (thread stack: 16MB, max heap: 8MB, pool: riak_kv_js_map) host starting (<0.390.0>)\n2017-11-09 18:15:34.013 [info] <0.391.0>@riak_kv_js_vm:init:73 Spidermonkey VM (thread stack: 16MB, max heap: 8MB, pool: riak_kv_js_map) host starting (<0.391.0>)\n2017-11-09 18:15:34.015 [info] <0.392.0>@riak_kv_js_vm:init:73 Spidermonkey VM (thread stack: 16MB, max heap: 8MB, pool: riak_kv_js_map) host starting (<0.392.0>)\n2017-11-09 18:15:34.016 [info] <0.393.0>@riak_kv_js_vm:init:73 Spidermonkey VM (thread stack: 16MB, max heap: 8MB, pool: riak_kv_js_map) host starting (<0.393.0>)\n2017-11-09 18:15:34.018 [info] <0.394.0>@riak_kv_js_vm:init:73 Spidermonkey VM (thread stack: 16MB, max heap: 8MB, pool: riak_kv_js_map) host starting (<0.394.0>)\n2017-11-09 18:15:34.020 [info] <0.396.0>@riak_kv_js_vm:init:73 Spidermonkey VM (thread stack: 16MB, max heap: 8MB, pool: riak_kv_js_reduce) host starting (<0.396.0>)\n2017-11-09 18:15:34.021 [info] <0.397.0>@riak_kv_js_vm:init:73 Spidermonkey VM (thread stack: 16MB, max heap: 8MB, pool: riak_kv_js_reduce) host starting (<0.397.0>)\n2017-11-09 18:15:34.023 [info] <0.398.0>@riak_kv_js_vm:init:73 Spidermonkey VM (thread stack: 16MB, max heap: 8MB, pool: riak_kv_js_reduce) host starting (<0.398.0>)\n2017-11-09 18:15:34.026 [info] <0.399.0>@riak_kv_js_vm:init:73 Spidermonkey VM (thread stack: 16MB, max heap: 8MB, pool: riak_kv_js_reduce) host starting (<0.399.0>)\n2017-11-09 18:15:34.027 [info] <0.400.0>@riak_kv_js_vm:init:73 Spidermonkey VM (thread stack: 16MB, max heap: 8MB, pool: riak_kv_js_reduce) host starting (<0.400.0>)\n2017-11-09 18:15:34.029 [info] <0.401.0>@riak_kv_js_vm:init:73 Spidermonkey VM (thread stack: 16MB, max heap: 8MB, pool: riak_kv_js_reduce) host starting (<0.401.0>)\n2017-11-09 18:15:34.030 [info] <0.403.0>@riak_kv_js_vm:init:73 Spidermonkey VM (thread stack: 16MB, max heap: 8MB, pool: riak_kv_js_hook) host starting (<0.403.0>)\n2017-11-09 18:15:34.031 [info] <0.404.0>@riak_kv_js_vm:init:73 Spidermonkey VM (thread stack: 16MB, max heap: 8MB, pool: riak_kv_js_hook) host starting (<0.404.0>)\n2017-11-09 18:15:34.036 [info] <0.197.0>@riak_core_capability:process_capability_changes:555 New capability: {riak_kv,vnode_vclocks} = true\n2017-11-09 18:15:34.052 [info] <0.197.0>@riak_core_capability:process_capability_changes:555 New capability: {riak_kv,legacy_keylisting} = false\n2017-11-09 18:15:34.062 [info] <0.197.0>@riak_core_capability:process_capability_changes:555 New capability: {riak_kv,listkeys_backpressure} = true\n2017-11-09 18:15:34.071 [info] <0.197.0>@riak_core_capability:process_capability_changes:555 New capability: {riak_kv,index_backpressure} = true\n2017-11-09 18:15:34.081 [info] <0.197.0>@riak_core_capability:process_capability_changes:555 New capability: {riak_kv,mapred_system} = pipe\n2017-11-09 18:15:34.091 [info] <0.197.0>@riak_core_capability:process_capability_changes:555 New capability: {riak_kv,mapred_2i_pipe} = true\n2017-11-09 18:15:34.100 [info] <0.197.0>@riak_core_capability:process_capability_changes:555 New capability: {riak_kv,handoff_data_encoding} = encode_raw\n2017-11-09 18:15:34.110 [info] <0.197.0>@riak_core_capability:process_capability_changes:555 New capability: {riak_kv,object_format} = v1\n2017-11-09 18:15:34.119 [info] <0.197.0>@riak_core_capability:process_capability_changes:555 New capability: {riak_kv,secondary_index_version} = v3\n2017-11-09 18:15:34.132 [info] <0.197.0>@riak_core_capability:process_capability_changes:555 New capability: {riak_kv,vclock_data_encoding} = encode_zlib\n2017-11-09 18:15:34.152 [info] <0.197.0>@riak_core_capability:process_capability_changes:555 New capability: {riak_kv,crdt} = [pncounter,riak_dt_pncounter,riak_dt_orswot,riak_dt_map,riak_kv_hll]\n2017-11-09 18:15:34.165 [info] <0.197.0>@riak_core_capability:process_capability_changes:555 New capability: {riak_kv,crdt_epoch_versions} = [{riak_dt_map,2},{riak_dt_orswot,2},{riak_dt_pncounter,2},{riak_kv_hll,1}]\n2017-11-09 18:15:34.177 [info] <0.197.0>@riak_core_capability:process_capability_changes:555 New capability: {riak_kv,put_fsm_ack_execute} = enabled\n2017-11-09 18:15:34.218 [info] <0.7.0> Application riak_kv started on node 'riak@172.17.0.2'\n2017-11-09 18:15:34.229 [info] <0.7.0> Application merge_index started on node 'riak@172.17.0.2'\n2017-11-09 18:15:34.233 [info] <0.7.0> Application riak_search started on node 'riak@172.17.0.2'\n2017-11-09 18:15:34.244 [info] <0.7.0> Application ibrowse started on node 'riak@172.17.0.2'\n2017-11-09 18:15:34.268 [info] <0.7.0> Application fuse started on node 'riak@172.17.0.2'\n2017-11-09 18:15:34.280 [info] <0.583.0>@riak_core:wait_for_service:504 Waiting for service riak_kv to start (0 seconds)\n2017-11-09 18:15:44.474 [info] <0.370.0>@riak_core:wait_for_service:498 Wait complete for service riak_kv (10 seconds)\n2017-11-09 18:15:44.520 [info] <0.583.0>@riak_core:wait_for_service:498 Wait complete for service riak_kv (10 seconds)\n2017-11-09 18:15:44.539 [info] <0.7.0> Application yokozuna started on node 'riak@172.17.0.2'\n2017-11-09 18:15:44.546 [info] <0.7.0> Application cluster_info started on node 'riak@172.17.0.2'\n2017-11-09 18:15:44.570 [info] <0.197.0>@riak_core_capability:process_capability_changes:555 New capability: {riak_control,member_info_version} = v1\n2017-11-09 18:15:44.587 [info] <0.7.0> Application riak_control started on node 'riak@172.17.0.2'\n2017-11-09 18:15:44.587 [info] <0.7.0> Application erlydtl started on node 'riak@172.17.0.2'\n2017-11-09 18:15:44.613 [info] <0.7.0> Application riak_auth_mods started on node 'riak@172.17.0.2'\n2017-11-09 18:15:48.894 [info] <0.360.0>@riak_core_throttle:maybe_log_throttle_change:372 Changing throttle for riak_kv/aae_throttle from undefined to 0 based on load factor 0. ",
    "fanchangyong": "OK,thank you.\nBut I want to know when will riak support R17?\n. I see ,thank you.\n. ",
    "pma": "Commenting the grace_time line produces the same error:\n```\nmulti_backend.bitcask_expiry_1d.bitcask.io_mode = erlang\nmulti_backend.bitcask_expiry_1d.bitcask.data_root = $(platform_data_dir)/bitcask_expiry_1d\nmulti_backend.bitcask_expiry_1d.bitcask.expiry = 1d\nmulti_backend.bitcask_expiry_1d.bitcask.expiry.grace_time = 1d\n```\n$ riak config generate -l debug\n...\n21:55:37.008 [error] Error generating configuration in phase apply_translations\n21:55:37.008 [error] Error running translation for riak_kv.multi_backend, [error, badarg].\n. ",
    "eminarcissus": "Failed to create schema, fixed after using a correct schema\n. ",
    "shalako": "Looks like rel/bin/riak version simply calls echo $APP_VERSION and rel/lib/env.sh is supposed to set APP_VERSION= but isn't.\n\n. Thank you, Jared. We are building from source. We'll look into whether make package is an option.\nCould you please tell me how to discover the running version of riak, riak cs, and stanchion in this case? Seems like there ought to be a more direct way to query the version than relying on an environment variable.\n. Also, even if make package is used, how could one environment variable APP_VERSION provide the version of both Riak and Riak CS? Latest compatible versions are Riak 1.4.10 and Riak CS 1.5.0. Even if APP_VERSION were set, it would be incorrect for one of these (Riak and Riak CS run on the same VMs). \n. Not sure I understand; will have to look into make package more. We expect Riak and Riak CS to run on the same VM. Doubling the number of VMs in a cluster seems unnecessary.\nRegardless, could you please tell me how to discover the running version of Riak, Riak CS, and Stanchion when APP_VERSION is not set?\n. Thank you, Jared. However riak-admin status only works for riak. There doesn't appear to be a similar solution for Riak CS or Stanchion. \n. ",
    "nickelization": "basho/webmachine#183 is now fixed, so I'm closing this issue as well.\n. Oh, one more note to add though, to avoid any potential confusion....\nThe version of webmachine used in the develop branch of riak is currently 1.10.5, which doesn't yet include the necessary fix. Once a newer version of webmachine is added to riak, this will actually be fixed.\n. +1 d3a474c\n. I don't know of any existing way to automatically convert gen_fsm modules to gen_statem, and my gut says it would be tricky to develop such a thing since you would need some way to programmatically distinguish whether a given function is a state handler or not. But, it can be done by hand in a relatively straightforward, mechanical fashion. There's a good example of this right in the gen_fsm documentation: http://erlang.org/doc/man/gen_fsm.html\nDoing it by hand would not be risk-free (nor particularly fun) but I don't think it would be hugely risky or take that much effort. It would also just be nice to have all of Riak updated to use gen_statem, since it is generally more powerful and flexible. (This is a bit of a tangent, and this wouldn't be a necessary change for a first pass, but I'd really like to be able to strip out the late message protection code in riak_ensemble. gen_statem has this sort of functionality built into its call mechanism already so we wouldn't need to manually implement it anymore.)\nOn the other hand, I would also say let's not let the perfect be the enemy of the good. I'd much rather see OTP 20 support at all, even if it has some deprecated module warnings when you build it. Whether we convert to gen_statem or accept some build warnings, I think it's still better to go straight to OTP 20 than to try and target 19 now and then just have to upgrade again later.. ",
    "przemien": "Thank you for your answer Sean. But, I am observing other behaviour.\nI got Map datatype that I am storing in RIAK. It's Map that contains few Maps that contain counters.\nThe expected output of such program would be: \n{\n    \"sample_map_key\" : {\n        \"maps\" : {\n            \"Q1\" : {\n                \"counters\" : {\n                    \"a1\" : 10,\n                    \"a2\" : 13\n                }\n            },\n            Q2 : {\n                \"counters\" : {\n                    \"a1\":31,\n                    \"a2\":21,\n                    \"a3\":10,\n                }\n            }\n        }\n    }\n}\nBut instead I am getting  exception: \"TypeError: unhashable type: 'Map'\".\nThanks for your help. Here are the sample codes: \n``` python\nfrom riak.datatypes import Map\nimport riak\ndef main():\n    store_values()\n    read_values()\ndef store_values():\n    riak_client = riak.RiakClient(host=\"localhost\", http_port=\"8087\", protocol=\"pbc\")\n    mapBucketType = riak.BucketType(riak_client, \"maps\")\n    maps_bucket = riak.RiakBucket(riak_client, \"sample_maps_bucket\", mapBucketType)\n    sample_map = Map(maps_bucket, \"sample_map_key\")\n    sample_map.maps[\"Q1\"].counters[\"a1\"].increment(10)\n    sample_map.maps[\"Q1\"].counters[\"a2\"].increment(13)\n    sample_map.maps[\"Q2\"].counters[\"a1\"].increment(31)\n    sample_map.maps[\"Q2\"].counters[\"a2\"].increment(21)\n    sample_map.maps[\"Q2\"].counters[\"a3\"].increment(10)\n    sample_map.store()\ndef read_values():\n    riak_client = riak.RiakClient(host=\"localhost\", http_port=\"8087\", protocol=\"pbc\")\n    mapBucketType = riak.BucketType(riak_client, \"maps\")\n    maps_bucket = riak.RiakBucket(riak_client, \"sample_maps_bucket\", mapBucketType)\n    sample_map = Map(maps_bucket, \"sample_map_key\")\n    sample_map.reload()\n    results = {\"sample_map_key\": to_json(sample_map)}\n    print results\ndef to_json(map):\n        obj = {}\n        for m in map.maps:\n            if \"maps\" not in obj:\n                obj[\"maps\"] = {}\n            obj[\"maps\"][m] = to_json(map.maps[m])\n        for c in map.counters:\n            if \"counters\" not in obj:\n                obj[\"counters\"] = {}\n            obj[\"counters\"][c] = map.counters[c]\n        return obj\nif name == 'main':\n    main()\n```\n. Ok. I got it working. It seems that there is an inconsistency between the RIAK library and description. Iterating over map.maps is not yielding keys but values. But, if you iterate over map itself you will get the key/type touple. Here are the working codes:\n``` python\n    from riak.datatypes import Map\n    import riak\n    def main():\n        store_values()\n        read_values()\ndef store_values():\n    riak_client = riak.RiakClient(host=\"localhost\", http_port=\"8087\", protocol=\"pbc\")\n    mapBucketType = riak.BucketType(riak_client, \"maps\")\n    maps_bucket = riak.RiakBucket(riak_client, \"sample_maps_bucket\", mapBucketType)\n    sample_map = Map(maps_bucket, \"sample_map_key\")\n    sample_map.maps[\"Q1\"].counters[\"a1\"].increment(10)\n    sample_map.maps[\"Q1\"].counters[\"a2\"].increment(13)\n    sample_map.maps[\"Q2\"].counters[\"a1\"].increment(31)\n    sample_map.maps[\"Q2\"].counters[\"a2\"].increment(21)\n    sample_map.maps[\"Q2\"].counters[\"a3\"].increment(10)\n    sample_map.store()\n\ndef read_values():\n    riak_client = riak.RiakClient(host=\"localhost\", http_port=\"8087\", protocol=\"pbc\")\n    mapBucketType = riak.BucketType(riak_client, \"maps\")\n    maps_bucket = riak.RiakBucket(riak_client, \"sample_maps_bucket\", mapBucketType)\n    sample_map = Map(maps_bucket, \"sample_map_key\")\n    sample_map.reload()\n    results = {\"sample_map_key\": to_json(sample_map)}\n    print results\n\ndef to_json(map):\n        obj = {}\n        for m, type in map:\n            if (type == \"map\"):\n                if \"maps\" not in obj:\n                    obj[\"maps\"] = {}\n                obj[\"maps\"][m] = to_json(map.maps[m])\n\n            if (type == \"counter\"):\n                if \"counters\" not in obj:\n                    obj[\"counters\"] = {}\n                obj[\"counters\"][m] = map.counters[m].value\n        return obj\n\nif __name__ == '__main__':\n    main()\n\n```\n. Ok, great. When the official patch can be expected? \n. ",
    "pigmej": "So @joecaswell, I assume that the bug is confirmed ?\nThanks for poiting about magic 7 number. I can resolve some testing issues with client side fix for it.\n. As we talked, I can confirm that your script is doing similar thing, it's the same problem. 1.4.2 is fine. \n. Russell, I can test your patch about 9am CET.\n-----Original Message-----\nFrom: Russell Brown notifications@github.com\nDate: Wed, 05 Nov 2014 23:20:08 \nTo: basho/riakriak@noreply.github.com\nReply-To: basho/riak reply@reply.github.com\nCc: J\u0119drzej Nowakpigmej@gmail.com\nSubject: Re: [riak] binary 2i ~broken on 2.0.1 (~swapping/floating results)\n BUG (#608)\nI think we have it. Looks like an off by one somewhere between the fold and the FSM. I have a hack fix that passes my script test. Should have updated the issue last night, sorry.\nOn 6 Nov 2014 01:28, Shunichi Shinohara notifications@github.com wrote:I'm digging the same symptom (but originated from riak_cs) with riak 1.4 branch.\nNot yet find the root cause, just a interim memo.\nWith single node cluster with ring size 16.\nAdded the following print debug to riak_core_coverage_fsm:\ndiff --git a/src/riak_core_coverage_fsm.erl b/src/riak_core_coverage_fsm.erl\nindex cd648bb..24dea91 100644\n--- a/src/riak_core_coverage_fsm.erl\n+++ b/src/riak_core_coverage_fsm.erl\n@@ -243,6 +243,12 @@ waiting_results({{ReqId, VNode}, Results},\n                                  req_id=ReqId,\n                                  timeout=Timeout,\n                                  process_fun = ProcessFun}) ->\n+case VNode of\n-    {1004782375664995756265033322492444576013453623296, 'dev1@127.0.0.1'} ->\n-        lager:log(warning, self(), \"**** waiting_results: ~p~n\", [Results]),\n-        ok;\n-    _ -> ok\n  +end,\n   case ProcessFun(VNode, Results, ModState) of\n       {ok, UpdModState} ->\n           UpdStateData = StateData#state{mod_state=UpdModState},\nThen, not always, the log reports duplicated key:\n10:19:07.565 [warning] ** waiting_results: {{<0.857.0>,#Ref<0.0.0.28237>},<<\"riak-cs-gc\">>,[<<\"1415092979_102\">>,<<\"1415092978_118\">>]}\n10:19:07.566 [warning] ** waiting_results: {{<0.857.0>,#Ref<0.0.0.28238>},<<\"riak-cs-gc\">>,[<<\"1415092978_118\">>]}\nThis seems to break the assumption by riak_kv_index_fsm that keys should be orderd.\nSo riak_kv_index_fsm and sms does not work correctly.\n\u2014Reply to this email directly or view it on GitHub.\n\nReply to this email directly or view it on GitHub:\nhttps://github.com/basho/riak/issues/608#issuecomment-61936388\n. @russelldb a check that you gave to me (this is not a fix, just for testing):\ndiff --git src/riak_kv_vnode.erl src/riak_kv_vnode.erl\nindex ec97036..9403cdb 100644\n--- src/riak_kv_vnode.erl\n+++ src/riak_kv_vnode.erl\n@@ -830,9 +830,9 @@ prepare_index_query(Q) ->\n %% @doc Batch size for results is set to 2i max_results if that is less\n %% than the default size. Without this the vnode may send back to the FSM\n %% more items than could ever be sent back to the client.\n-buffer_size_for_index_query(#riak_kv_index_v3{max_results=N}, DefaultSize)\n-  when is_integer(N), N < DefaultSize ->\n-    N;\n+%% buffer_size_for_index_query(#riak_kv_index_v3{max_results=N}, DefaultSize)\n+%%   when is_integer(N), N < DefaultSize ->\n+%%     N;\n buffer_size_for_index_query(_Q, DefaultSize) ->\n     DefaultSize.\nsolves this on 2.0.1.\n@shino you can probably try to apply the same for 1.4.4+ series to confirm that it's the same problem. \nAbove patch is NOT a fix\n. I have also run this script on 5 node cluster on centos7 and the same happens. (all requests were made to the same riak node)\n. Really it's closed ? From what I see it shouldn't be closed...\n. ",
    "jpmitche11": "I agree this feature is not well documented.\nI was able to get a mapReduce query to work by setting input as follows:\n{\n    inputs: [bucketType, bucketName],\n    query: ...\n}\n. ",
    "joaonrb": "@jpmitche11 are you sure that you define bucket type like that? My query return 200 like that but do not return me the expected values. And according to documentation Erlang should verify a list with 2 strings to be \"bucketName\" and \"key\".\n. ",
    "mitchellwrosen": "@joaonrb I can confirm that the two-element list is interpreted as [bucket_type, bucket_name] rather than [bucket_name, key] (even though the documentation says otherwise).\n. ",
    "Quentin01": "This link is for Riak 2.0, but we use Riak 1.X :(\nIs there a similar option / configuration in Riak 1.X?\n. Thank you for your reply, I will use this informations.\n. ",
    "pseudorandumb": "I have the same issue and looking at the logs it looks related to a solr memory error.\njava.lang.OutOfMemoryError: GC overhead limit exceeded\n. ",
    "gglanzani": "Adding a +1 here. I filed a bug in the riak-python-client, but I think it's an upstream (i.e. Riak) issue.\nhttps://github.com/basho/riak-python-client/issues/393\n. I'm consistently getting partial set, then {timeout}. The workaround (I'm using the Python client) for our use case (delete keys), is to wrap the stream in a try/except block wrapped in a while block, and let the client reconnect.\n. The little riak book? https://github.com/coderoshi/little_riak_book/commits/master\n. ",
    "zeeshanlakhani": "@pigmej sorry... twas by accident. We will keep this open and test. Just got lost in some Jira shuffling.\n. :+1:  f41544a\n. :+1: 330fdb2\n. :+1: 7f4a6c3\n. Reopened at https://github.com/basho/yokozuna/issues/480. \n. :+1: ed316a4a655ba3556589190c963b745e69803866\n. :+1: c12ec72a4c81d19436be7f3eaa431d0387160eae\n. ",
    "DSomogyi": "Fixed by Riak-1127.\n. ",
    "ulf": "Hi John,\nwould you mind using Ulfs correct handle?\nEverytime you use \"@ulf\" I get some notifications...\nThanks\nUlf\nOn Wed, Nov 26, 2014 at 5:02 PM, John Burwell notifications@github.com\nwrote:\n\n@ulf https://github.com/ulf Does riak_ee need a similar change?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/basho/riak/pull/647#issuecomment-64667513.\n. No problem, thank you :)\n\nOn Wed, Nov 26, 2014 at 5:42 PM, John Burwell notifications@github.com\nwrote:\n\n@ulf https://github.com/ulf I apologize for the mixup. @uwiger\nhttps://github.com/uwiger is @ulf https://github.com/ulf in Hipchat\nso muscle memory kicks in. I apologize again.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/basho/riak/pull/647#issuecomment-64674174.\n. \n",
    "sandeepbisht": "Thank you it works.\nBut is it needed to remove /var/lib/riak/ring if some changes made in riak.conf related to ring_size ?\nBecause till now changes on riak.conf automatically reflected.It not needed manual deletion.Whenever i have to modify riak.conf.I modify it  on all nodes and restart all nodes.Change reflected.\n. ",
    "cipy": "Can be seen in clusters where the data on disk greatly (i.e. at least 10 times) exceeds the available RAM dedicated to Riak.\n. we found the following versions to be affected: 1.4.12+ and 2.0.5+\n. ",
    "molele2": "no  boy  ansow my problem?The  project is dead ???????\n. Hi kesslerm ,I try use this project to my new case,But not intall from network,So,  I assumed it was dead,I feel so  sorry, But I hope the project was strong ,Becase I plan to develop new case  for my boss,Thanks for your  support\n. ERlang 16B   centos6.5  64 riak-develop .2.0.3,2.0.4,2.0.2 no a way\n. How to set bucket-type ??????\nset? map?  and other?\n. import java.net.UnknownHostException;\nimport java.util.LinkedList;\nimport java.util.List;\nimport java.util.concurrent.ExecutionException;\nimport com.basho.riak.client.core.RiakCluster;\nimport com.basho.riak.client.core.RiakNode;\nimport com.basho.riak.client.core.operations.SearchOperation;\nimport com.basho.riak.client.core.util.BinaryValue;\npublic class RiakClusterClientTester {\n```\n/*\n * @param args\n /\npublic static void main(String[] args) {\n    // TODO Auto-generated method stub\ntry {\n    RiakNode.Builder builder = new RiakNode.Builder();\n    builder.withMinConnections(10);\n    builder.withMaxConnections(50);\n\n    List<String> addresses = new LinkedList<String>();\n    addresses.add(\"120.24.55.208\");\n\n```\n//          addresses.add(\"192.168.1.2\");\n//          addresses.add(\"192.168.1.3\");\n            List nodes = RiakNode.Builder.buildNodes(builder, addresses);\n            RiakCluster cluster = new RiakCluster.Builder(nodes).build();\n            cluster.start();\n```\n        String index = \"scores\";\n        String query = \"counter:[20 TO *]\";\n        SearchOperation searchOp = new SearchOperation.Builder(BinaryValue.create(index),query)\n            .build();\n    cluster.execute(searchOp);\n    SearchOperation.Response results = searchOp.get();\n    System.out.println(\"counters::::\" + results.getAllResults());\n    cluster.shutdown();\n} catch (UnknownHostException | InterruptedException | ExecutionException e) {\n    e.printStackTrace();\n}\n\n}\n```\n}\njava.util.concurrent.ExecutionException: com.basho.riak.client.core.netty.RiakResponseException: Unknown message code: 27\n    at com.basho.riak.client.core.FutureOperation.get(FutureOperation.java:260)\n    at com.warunion.shop.riak.RiakClusterClientTester.main(RiakClusterClientTester.java:41)\nCaused by: com.basho.riak.client.core.netty.RiakResponseException: Unknown message code: 27\n    at com.basho.riak.client.core.netty.RiakResponseHandler.channelRead(RiakResponseHandler.java:53)\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)\n    at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)\n    at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:182)\n    at io.netty.handler.codec.ByteToMessageCodec.channelRead(ByteToMessageCodec.java:103)\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)\n    at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)\n    at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:846)\n    at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:130)\n    at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:511)\n    at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)\n    at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)\n    at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)\n    at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:116)\n    at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:137)\n    at java.lang.Thread.run(Unknown Source)\n. riak-admin bucket-type update sets '{\"props\":{\"search_index\":\"hobbies\"}}'\nimport java.net.UnknownHostException;\nimport java.util.concurrent.ExecutionException;\nimport com.basho.riak.client.api.RiakClient;\nimport com.basho.riak.client.api.commands.search.StoreIndex;\nimport com.basho.riak.client.core.query.Namespace;\nimport com.basho.riak.client.core.query.search.YokozunaIndex;\npublic class RiakQuerySets {\n```\n/*\n * @param args\n /\npublic static void main(String[] args) {\n    // TODO Auto-generated method stub\nString query = \"set:football\";\n\n\ntry {\n    RiakClient client = RiakClient.newClient(\"120.24.55.208\");\n\n```\n//          Namespace peopleBucket = new Namespace(\"sets\", \"people\");\n            YokozunaIndex hobbiesIndex = new YokozunaIndex(\"hobbies\");\n            StoreIndex storeIndex =\n              new StoreIndex.Builder(hobbiesIndex).build();\n            client.execute(storeIndex);\n```\n        client.shutdown();\n} catch (UnknownHostException e) {\ne.printStackTrace();\n} catch (ExecutionException e) {\n// TODO Auto-generated catch block\ne.printStackTrace();\n} catch (InterruptedException e) {\n// TODO Auto-generated catch block\ne.printStackTrace();\n}\n\n}\n```\n}\nCaused by: com.basho.riak.client.core.netty.RiakResponseException: Unknown message code: 56\n    at com.basho.riak.client.core.netty.RiakResponseHandler.channelRead(RiakResponseHandler.java:53)\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)\n    at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)\n    at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:182)\n    at io.netty.handler.codec.ByteToMessageCodec.channelRead(ByteToMessageCodec.java:103)\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)\n    at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)\n    at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:846)\n    at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:130)\n    at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:511)\n    at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)\n    at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)\n    at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)\n    at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:116)\n    at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:137)\n    at java.lang.Thread.run(Unknown Source)\n. import com.basho.riak.client.FetchMeta;\nimport com.basho.riak.client.StoreMeta;\nimport com.basho.riak.client.convert.Converter;\nimport com.basho.riak.client.convert.PassThroughConverter;\nimport com.basho.riak.client.core.RiakCluster;\nimport com.basho.riak.client.core.RiakNode;\nimport com.basho.riak.client.core.operations.DeleteOperation;\nimport com.basho.riak.client.core.operations.FetchOperation;\nimport com.basho.riak.client.core.operations.StoreOperation;\nimport com.basho.riak.client.query.RiakObject;\nimport com.basho.riak.client.util.ByteArrayWrapper;\nimport org.junit.AfterClass;\nimport org.junit.Assert;\nimport org.junit.BeforeClass;\nimport org.junit.Test;\nimport java.net.UnknownHostException;\nimport java.util.concurrent.ExecutionException;\npublic class StoreFetchDeleteTest\n{\n```\nprivate final Converter domainObjectConverter = new PassThroughConverter();\nprivate static RiakCluster cluster;\nprivate ByteArrayWrapper bucket = ByteArrayWrapper.create(\"bucket\");\n@BeforeClass\npublic static void setup() throws UnknownHostException\n{\n    RiakNode node = new RiakNode.Builder()\n        .withRemoteAddress(\"localhost\")\n        .withRemotePort(8087)\n        .build();\ncluster = RiakCluster.builder(node).build();\ncluster.start();\n\n}\n@AfterClass\npublic static void teardown()\n{\n    cluster.stop();\n}\n@Test\npublic void testStoreFetchDelete() throws ExecutionException, InterruptedException\n{\nRiakObject o = RiakObject.create(bucket.unsafeGetValue()).setValue(\"test value\");\nStoreMeta storeMeta = new StoreMeta.Builder().returnBody(true).build();\n\nStoreOperation<RiakObject> store =\n    new StoreOperation<RiakObject>(bucket, o)\n        .withConverter(domainObjectConverter)\n        .withStoreMeta(storeMeta);\n\ncluster.execute(store);\n\nRiakObject storeReturn = store.get();\n\nByteArrayWrapper returnedKey = ByteArrayWrapper.create(storeReturn.getBucketAsBytes());\nFetchOperation<RiakObject> fetch =\n    new FetchOperation<RiakObject>(bucket, returnedKey)\n    .withConverter(domainObjectConverter);\n\ncluster.execute(fetch);\n\nRiakObject fetchReturn = fetch.get();\n\nDeleteOperation delete = new DeleteOperation(bucket, returnedKey);\n\ncluster.execute(delete);\n\ndelete.get();\n\nFetchOperation<RiakObject> tombstoneFetch =\n    new FetchOperation<RiakObject>(bucket, returnedKey)\n    .withConverter(domainObjectConverter);\n\ncluster.execute(tombstoneFetch);\n\nRiakObject tombstone = tombstoneFetch.get();\n\nAssert.assertTrue(tombstone.isNotFound());\n\n}\n@Test\npublic void testSiblings() throws ExecutionException, InterruptedException\n{\nRiakObject o = RiakObject.create(bucket.unsafeGetValue()).setValue(\"test value\");\nStoreMeta storeMeta = new StoreMeta.Builder().returnBody(true).build();\n\nStoreOperation<RiakObject> store1 =\n    new StoreOperation<RiakObject>(bucket, o)\n        .withConverter(domainObjectConverter)\n        .withStoreMeta(storeMeta);\n\ncluster.execute(store1);\n\nRiakObject storeReturn1 = store1.get();\n\nByteArrayWrapper key = ByteArrayWrapper.create(storeReturn1.getKeyAsBytes());\nStoreOperation<RiakObject> store2 =\n    new StoreOperation<RiakObject>(bucket, key, o)\n        .withConverter(domainObjectConverter)\n        .withStoreMeta(storeMeta);\n\ncluster.execute(store2);\n\nRiakObject storeReturn2 = store2.get();\n\n}\n```\n}\nWhy version 2.0 code test programer use lib  1.4.4 api?????????????\nIt's joke!!!!!\n. so All version is eable to used!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n. the url http://docs.basho.com/riak/latest/dev/using/search/#Why-Riak-Search is fine for use success!\nThe riak is very joke!!!!!!!!!!!!!!\n. \u7ecf\u8fc7\u6211\u4e00\u4e2a\u6708\u5de6\u53f3\u65f6\u95f4\u7684\u5c1d\u8bd5,\u6211\u51b3\u5b9a\u653e\u5f03riak\u4e86,\u7ed9\u51fa\u7684\u6587\u6863\u6587\u7ae0\u4e00\u7247\u4e71,\u4e0a\u9762\u6807\u6ce8\u77402.0.5\u7684\u7248\u672c,\u4f46\u662f\u4f8b\u5b50\u4e2d\u9519\u8bef\u767e\u51fa,\u751a\u81f3\u4ecegithub\u4e0a\u4e0b\u8f7d\u76842.0\u7684java\u5e93,\u4e2d\u5e26\u77401.4.4\u7684\u4ee3\u7801,\u4e4b\u540e\u6574\u4e2atest\u5de5\u7a0b\u4e0b\u5168\u662f\u4e00\u5806\u6ca1\u7528\u7684\u4ee3\u7801,\u5f62\u5982:\nimport junit.framework.Test;\nimport junit.framework.TestCase;\nimport junit.framework.TestSuite;\n/\n- Unit test for simple App.\n  /\n  public class AppTest \n  extends TestCase\n  {\n  /\n  - Create the test case\n    \n  - @param testName name of the test case\n    */\n    public AppTest( String testName )\n    {\n    super( testName );\n    }\n/*\n  - @return the suite of tests being tested\n    /\n    public static Test suite()\n    {\n    return new TestSuite( AppTest.class );\n    }\n/*\n  - Rigourous Test :-)\n    /\n    public void testApp()\n    {\n    assertTrue( true );\n    }\n    }\n\u4e0d\u77e5\u9053\u5f62\u5982\u8fd9\u6837\u7684\u7ee7\u627f\u7c7b\u7528\u6765\u5e72\u561b?\n\u4ee3\u7801\u4e2d\u6ca1\u6709\u4e00\u4e2a\u770b\u8d77\u6765\u80fd\u591f\u57282.0.5\u4e0b\u9762\u6b63\u786e\u4f7f\u7528\u7684lib,\u6211\u89c9\u5f97\u767d\u767d\u6d6a\u8d39\u4e86\u4e00\u4e2a\u6708\u65f6\u95f4\u5728riak\u4e0a,\u8fd8\u4e0d\u77e5\u9053\u8fd9\u4e2a\u4e07\u4ebf\u80fd\u5e72\u5417\n\u8bf4\u5b9e\u8bdd\u73b0\u5728\u5230\u5904\u90fd\u662fkv,\u5b9e\u5728\u8ba9\u4eba\u4e0d\u77e5\u9053kv\u5e72\u561b\u7528,\u800c\u4e14leveldb\u4e5f\u662fkv,riak\u4e5f\u662fkv,mongodb\u662fkv,reids\u662fkv,memcached\u662fkv,kv\u6709\u51e0\u4f55,\u90c1\u95f7\u4e86,\u6211\u9189\u4e86\n. kv\u52a0\u6cb9\n. \u7ecf\u8fc7\u4e00\u756a\u6323\u624e\u4e4b\u540e\u6211\u51b3\u5b9a\u653e\u5f03riak\u8fd9\u4e2a\u4e1c\u897f\u592a\u4e0d\u9760\u666e\u4e86\n. bye riak!!!!!!!!\n. key-value \u5b58\u50a8\u7cfb\u7edf RocksDB\n\u8f7b\u91cf\u7ea7Key-value\u5b58\u50a8 iPage\nkey-value \u5b58\u50a8\u7cfb\u7edf Keyspace\nkey-value \u6570\u636e\u5b58\u50a8 OpenKeyval\nKey-Value\u6570\u636e\u5f15\u64ce quickdb\n\u5206\u5e03\u5f0f key-value \u5b58\u50a8\u7cfb\u7edf Scalaris\nkey-value\u5b58\u50a8\u7cfb\u7edf kvs\nkey-value \u5b58\u50a8\u670d\u52a1\u5668 QDBM\n\u52a0\u5bc6\u7684 key-value \u5b58\u50a8\u7cfb\u7edf Trousseau\nKey-Value \u6570\u636e\u5e93 GT.M\nPython\u5b9e\u73b0\u7684Key-Value\u6570\u636e\u5e93 spugdb\n\u5206\u5e03\u5f0f\u7684Key-Value\u5b58\u50a8\u7cfb\u7edf Kai\n\u5206\u5e03\u5f0f\u7684Key-Value\u5b58\u50a8\u7cfb\u7edf Ringo\nKey-Value\u6570\u636e\u5e93 Flare\n\u5206\u5e03\u5f0f\u7684Key-Value\u5b58\u50a8\u7cfb\u7edf Dynomite\nkey-value \u5b58\u50a8\u670d\u52a1\u5668 LightCloud\nkey-value \u5b58\u50a8\u670d\u52a1\u5668 Riak\n\u5206\u5e03\u5f0f\u7684Key-Value\u5b58\u50a8\u7cfb\u7edf ThruDB\n\u57fa\u4e8e\u5185\u5b58\u7684\u6570\u636e\u5e93\u7cfb\u7edf VoltDB\nK/V\u5b58\u50a8\u6570\u636e\u5e93 TreapDB\nK/V\u5b58\u50a8\u65b9\u6848 TomP2P\n. so many kv\nkv\nkv\nkv ........................\noh,my god,\u6211\u9189\u4e86\uff0c\u602a\u4e0d\u5f97\u6211\u5199\u4e86n\u591a\u5e74\u6280\u672f\u4e4b\u540e\uff0c\u8d8a\u6765\u8d8a\u4e0d\u559c\u6b22\uff0c\u8fd9\u4e9b\u957f\u7740\u6280\u672f\u6837\uff0c\u4e2a\u4e2a\u90fd\u8bf4\u81ea\u5df1\u662fkv\u7684\u5bb6\u4f19\uff0c\u4e00\u70b9\u8425\u517b\u4ef7\u503c\u90fd\u6ca1\u6709\uff0c\u800c\u4e14\u4e2a\u4e2a\u90fd\u8bf4\u81ea\u5df1\u5f88\u597d\u5f88\u725b\u76ae\uff0c\u6211\u4ee5\u4e3a\u662f\u4ece\u706b\u661f\u6765\u7684\u6280\u672f\uff0ckv\nkv\uff0c\u6211\u53c8\u9189\u4e86\n. Which 2.0?Which 1.4 ?\n. ",
    "kayabendroth-epages": "@khauser Can you update the summary to something like: Starting Riak on Debian 8 fails?\n. ",
    "khauser": "fixed in 2.1.0\n. ",
    "marksteele": "I have three buckets configured:\n{\"props\":{\"allow_mult\":true,\"backend\":\"bitcask_mult\",\"basic_quorum\":false,\"big_vclock\":50,\"chash_keyfun\":{\"mod\":\"riak_core_util\",\"fun\":\"chash_std_keyfun\"},\"dw\":\"quorum\",\"last_write_wins\":false,\"linkfun\":{\"mod\":\"riak_kv_wm_link_walker\",\"fun\":\"mapreduce_linkfun\"},\"n_val\":2,\"name\":\"midway_counter\",\"notfound_ok\":true,\"old_vclock\":86400,\"postcommit\":[],\"pr\":0,\"precommit\":[],\"pw\":0,\"r\":\"quorum\",\"rw\":\"quorum\",\"small_vclock\":50,\"w\":\"quorum\",\"young_vclock\":20}}\n{\"props\":{\"name\":\"promo_code_banned\",\"allow_mult\":false,\"basic_quorum\":false,\"big_vclock\":50,\"chash_keyfun\":{\"mod\":\"riak_core_util\",\"fun\":\"chash_std_keyfun\"},\"dw\":\"quorum\",\"last_write_wins\":false,\"linkfun\":{\"mod\":\"riak_kv_wm_link_walker\",\"fun\":\"mapreduce_linkfun\"},\"n_val\":3,\"notfound_ok\":true,\"old_vclock\":86400,\"postcommit\":[],\"pr\":0,\"precommit\":[],\"pw\":0,\"r\":\"quorum\",\"rw\":\"quorum\",\"small_vclock\":50,\"w\":\"quorum\",\"young_vclock\":20}}\n{\"props\":{\"name\":\"sessions\",\"allow_mult\":false,\"basic_quorum\":false,\"big_vclock\":50,\"chash_keyfun\":{\"mod\":\"riak_core_util\",\"fun\":\"chash_std_keyfun\"},\"dw\":\"quorum\",\"last_write_wins\":false,\"linkfun\":{\"mod\":\"riak_kv_wm_link_walker\",\"fun\":\"mapreduce_linkfun\"},\"n_val\":3,\"notfound_ok\":true,\"old_vclock\":86400,\"postcommit\":[],\"pr\":0,\"precommit\":[],\"pw\":0,\"r\":\"quorum\",\"rw\":\"quorum\",\"small_vclock\":50,\"w\":\"quorum\",\"young_vclock\":20}}\nI suspect the first one is the one involved in this bug, it is the one that is configured via a client call.\n. That's how I know how many buckets I have :)\n. Might somehow be counter related, as we've recently starting using the CRDT counters and the bucket in question is used for storing counters.\n. The php code that sets bucket properties is using the php pecl extension (https://github.com/php-riak/php_riak#bucket-properties-riak-14-and-above). \n. ",
    "weaktyper": "+1 agree.\n. ",
    "jaconstantine": "+1\n. ",
    "ssylvester87": "I have observed this behavior when all nodes in a multi-cluster realtime-replication configuration are issued a stop command simultaneously (via an automation tool such as Ansible). Failed repl leadership negotiation seems to cause a race condition when the operation times out and riak stop never finishes.\n. ",
    "gcymbalski": "Very cool, I was wondering if node_package would need tweaks. Thanks for the heads-up :)\n. Oh, this just uses the locked deps instead of remaking it (lockdeps vs. locked-deps- clear, no?). So it shouldn't cause that, but I will definitely double-check that tomorrow.\n. They never have been.\nIs this a new requirement?\n\nOn Sep 8, 2015, at 08:13, Basho JIRA bot! notifications@github.com wrote:\nAssigned #714 to @gcymbalski.\n\u2014\nReply to this email directly or view it on GitHub.\n. \n",
    "aeden": "Oh, ok, thanks. Maybe update the README indicating that it can only be built with version up to R16B02.\n. Thanks, I've confirmed the build succeeds now on my machine.\n. ",
    "nbari": "Cleaned and updated the patch: https://bz-attachments.freebsd.org/attachment.cgi?id=153931\nNew patch to avoid the BASH dependecy :   https://github.com/basho/yokozuna/issues/461\n==> yokozuna (compile)\nenv: bash: No such file or directory\nERROR: Command [compile] failed!\nhttps://bz-attachments.freebsd.org/attachment.cgi?id=153981\nTo install it, update the FreeBDS ports:\n```\nportsnap fetch extract\nportsnap fetch update\n```\nPatch current riak port:\n```\ncd /usr/ports/databases/riak\npatch -E -p1 < /path/to/riak.diff\nfind . -name \"*.orig\" -exec rm -f {} \\;\n```\nInstall, create package:\n```\nmake install\nmake package\n```\n. New patch passing poudriere testport:  https://bz-attachments.freebsd.org/attachment.cgi?id=154128\nIs there a way to avoid solr / yokozuna when compiling ? \nIt would be nice to have yokozuna as an extension rather than a dependency while compiling \n. New patch removing yokozuna: https://bugs.freebsd.org/bugzilla/attachment.cgi?id=154161\nA second port could be created to include yokozuna, mainly because of the solr dependency \n. Maybe a good idea to have a riak2 port so that both 1.4.x and 2.0.x are available. \nFor riak 2 (2.0.5) I created this shar file: https://bz-attachments.freebsd.org/attachment.cgi?id=154212 https://bugs.freebsd.org/bugzilla/show_bug.cgi?id=198517 \nTo use (in FreeBSD):\n```\nfetch \"https://bz-attachments.freebsd.org/attachment.cgi?id=154212\" -o /tmp/riak2.shar\ncd /usr/ports/databases\nsh /tmp/riak2.shar\ncd /usr/ports/databases/riak2\nmake install\n```\n. I created a port: lang/riak-erlang to compile riak using Bashio's custom erlang port:\nTo use:\n```\nfetch \"https://bz-attachments.freebsd.org/attachment.cgi?id=154452\" -o /tmp/riak-erlang.sh\nfetch \"https://bz-attachments.freebsd.org/attachment.cgi?id=154455\" -o /tmp/riak2.sh\ncd /usr/ports/lang\nsh /tmp/riak-erlang.sh\ncd /usr/ports/databases\nsh /tmp/riak2.sh\ncd /usr/ports/databases/riak2\nmake install\n```\n. Hi, I am currently testing port, software and so far working as expected, I am just waiting feedback from FreeBSD guys to see if they can accept the port. \n. Hi, new port depending on lang/riak-erlang includes yokozuna, in one patch I disable it just to try to pass the ports rules, but I agree that disabling it removes all search stuff, therefore I put them back, basically my last port should compile as bashio recomends. \n. I tried my best, but I think the port wont be accepted by the FreeBSD committers, more details here: https://bugs.freebsd.org/bugzilla/show_bug.cgi?id=198517#c15 https://bugs.freebsd.org/bugzilla/show_bug.cgi?id=198656#c18\nAnyway the port compiles and work nice, just in case someone interested or want to use with tools like poudriere.\nMaybe the only way is to create another port than just install basho's binaries as suggested, but well problem here is that you don\u00b4t know if it will crash or if it will perfectly fit your architecture, since is more difficult to trust talking in term of stability, compiling, build, etc etc. \n. http://www.freshports.org/databases/riak2/\nMany thanks \n. ",
    "sdebnath": "@nbari removing yokozuna disables all of search 2.0 which is critical in riak 2.0+\n. ",
    "william-gr": "I have committed databases/riak2 and lang/erlang-riak yesterday to FreeBSD ports tree, I'll be adding stanchion and riak-cs today.\n. ",
    "eejoin": "volume manager  may have a lower performance than riak manage disks itself?\n@slfritchie As more and more nodes, communication between nodes will become the system bottleneck, so we have to reduce nodes by \"storage zone\" or other way like volume manager? can you tell me why riak not been able to settle on a \"storage zone\"  design, It is difficult or just think don't need at present?\n. ",
    "hazen": ":+1:  a5561dc\n. Good catch, @GabrielNicolasAvellaneda!\n. :+1: 3e0a13a\n. I've already got a directive to include riak-shell: https://github.com/basho/riak_ee/blob/riak_ts-rc-1.2.0/rel/reltool.config#L149\nThe script needs to be run through the preprocessor to do some constant replacements, so it must be a template.\n. Thanks, however 1.2 is EE only which has this change.  We'll need to same one on riak for 1.3, however.\n. We need more changes than that, I'm afraid: https://github.com/basho/riak_ee/pull/370\n. Also need this change: https://github.com/basho/riak_ee/pull/372\n. :+1: \n. @macintux Do we have the capability to view the current TZ setting?\n. :+1: 237609b\n. :+1:. +1. Why Wednesday? Sounds like we are wanting to standardize the release days, etc, which I think is a great idea.. I was also going to say, it would be nice to have the release URL already packaged with the RC, even if there is no content yet behind it. Want to add a note about the basho_bench setting?. Want to mention that were are two different tags and repos: one for OSS and the other for EE?. Is there a document on triaging Jenkins and or Ansible Tower? Not sure this is the right place for it, but a pointer might be nice since it's part of the release cycle.. We should just merge these two.... It's a yuge team!. ",
    "mbbroberg": "Hey @gcymbalski, this request is a valid new feature for us. It's certainly a standard in our industry. Thanks @danieldreier for opening it up. Our infrastructure team is building some pretty big projects of late so this may take a little while to address just so you know. Cheers! :bow: \n. Hey @danieldreier - I guess I need to follow up on this elsewhere. \n. ",
    "danieldreier": "thanks @mjbrender \n. thanks @mjbrender - it's probably worth noting that packagecloud (which basho currently uses) can sign packages for you. I don't know what your build pipeline looks like but it should be relatively straightforward to enable that step.\n. ",
    "JeetKunDoug": "@xiaoliuliu2050 did you activate your bucket type after creating it? In order for riak to make sure the bucket type is available across the cluster, you need to first create the bucket type, and then activate. See http://docs.basho.com/riak/latest/dev/advanced/bucket-types/#Managing-Bucket-Types-Through-the-Command-Line for more information\n. This is handled by https://github.com/basho/riak/pull/734, which was released in 2.1.1. Can we close this and the related PR?\n. So two issues - We're not sure if unset CDPATH is sufficient, and the other proposal, export -n CDPATH isn't actually POSIX/portable from what I can tell. We do, however, know that this fix (redirecting cd's stdout) will always work, no matter what stupid thing some shell decides to add to cd to output additional things.\n-EDIT- Also, editing the scripts means this fix also affects node_package, not just riak\n. also needs to be included in https://github.com/basho/node_package/blob/develop/priv/base/runner#L6 if we go that way.\n. More scripts which would need unsets:\n- https://github.com/basho/riak/blob/89e4df90026966838e3156df51688c1b0dbcf3c5/rel/files/search-cmd#L6\n- https://github.com/basho/riak/blob/89e4df90026966838e3156df51688c1b0dbcf3c5/rel/files/riak-admin#L5\n- https://github.com/basho/riak/blob/89e4df90026966838e3156df51688c1b0dbcf3c5/rel/files/riak-debug#L137\n- https://github.com/basho/riak_ee/blob/8ef6406f354f323ec627dae0859ff46da55d9251/rel/files/search-cmd#L5\n- https://github.com/basho/riak_ee/blob/8ef6406f354f323ec627dae0859ff46da55d9251/rel/files/riak-repl#L6\n- https://github.com/basho/riak_ee/blob/8ef6406f354f323ec627dae0859ff46da55d9251/rel/files/riak-admin#L6\n- https://github.com/basho/riak_ee/blob/8ef6406f354f323ec627dae0859ff46da55d9251/rel/files/riak-debug#L137\n. Yea - it seems as if cd doesn't actually care if it can write to stdout or not - it writes the failure to stderr but still succeeds. That said, > /dev/null is probably better (although I somehow still like 1>&- better :) )\n. Ah yea - I think it only works because we happened to test it from the \"right\" parent dir to make it work even though the command returned no output. Not so good. /dev/null it is.\n. Tested on Linux (Ubuntu) and Mac OS, for paranoia's sake. :+1: \n. I've reproduced this by first building the develop branch, then switching to the 2.0 branch and rebuilding without first cleaning up deps/dev directories - but it doesn't reproduce after a clean (as the problem has to do with a change made in the 2.1 series cuttlefish schema for handoff.ip). make distclean will do this for you, but make clean doesn't do enough (as it doesn't actually remove your deps, which is required to make sure you've got the correct dependencies when changing branches.\nCan you please confirm that removing deps (either using make distclean or rm -rf deps) resolves this issue for you as well?\n. So make clean means \"please remove any compile-derived files from my filesystem\", not necessarily \"remove everything other than source from my filesystem\", and cleaning the deps directory seems to fall into the latter case.\n. :+1: \n. Can you try a few things for me:\n1. See if you have anything in crash.log or error.log, which may help us understand what's going on.\n2. Run riak as riak console, which will start riak but keep all its output on the console, and see if you get any further information.\nThanks,\nDoug\n. @zeeshanlakhani here's the PR we discussed.\n. The three connected PRs are ready for review - need to merge basho/riak_core#794 first, as the other two can't pass xref until the core PR is merged.\n. With the three PRs merged, this issue is now resolved.\n. :+1: \n. :+1: tested on Centos (bash + ksh), Mac OS X (bash), and Solaris 10 (bash and /sbin/sh, the default shell, where this failed before the fixes)\n. Not yet, but will be before 2.2 (was too late for 2.1.2/3 release).\n. @kesslerm has +1ed in Hipchat - going to merge.\n. :+1: 4f11213\n. :+1: f4338e7 - No BORS, so merging manually\n. :+1: Thanks for putting this in - may help people not build broken things.\n. @lukebakken can you please recreate this targeting the 2.0 branch for merge? Will get it forward-merged from there.\n. :+1: Thank you for retargeting this - will merge.\n. :+1:\n. Superseded by release notes in #827 \n. :+1: Thanks... looks good\n. \ud83d\udc4d \n. +1 99d59bd (not that there's a BORS here)\n. +1 d630e1c\n. +1. +1. +1. +1. Please document why this is np_etop (it's mimicking node_package to avoid doubling the atom table usage).. ",
    "spicydonuts": "Status?  Seeing the same thing but with 3 nodes.  It worked last week.\n. ",
    "boosty": "@jonmeredith Thanks! I was the one asking @cipy for help in this case :wink: \nJust wanted to let you know that setting it live via riak attach does not seem to work with 1.4.12:\nriak_repl_rtq:set_max_bytes(1073741824).\n** exception error: undefined function riak_repl_rtq:set_max_bytes/1\nHowever, I am happy that setting rtq_max_bytes works via app.config.\n. ",
    "sramakr": "I understand this thread is very old, do anyone know how to do this in riak 2.2.\nFigured it out, had to use riak attach instead of direct erlang.\nFixed with, riak attach $\nriak_repl2_rtq:set_max_bytes(2073741824).\nok. ",
    "k-bx": "Sorry, my bad. I modified a bit haskell's riak driver (I am implementing lookup-by-secondary-index functionality) and I didn't put msg types into the right place, so driver started sending wrong message codes.\n. Sorry, my bad. I modified a bit haskell's riak driver (I am implementing lookup-by-secondary-index functionality) and I didn't put msg types into the right place, so driver started sending wrong message codes.\n. ",
    "jvoegele": "That might indeed be simpler, but it cannot be isolated in the same way that the current approach can. In other words, to unset the CDPATH would require doing so in each script that uses the runner_script_dir variable, whereas if we fix the problem in the runner_script_dir itself it applies to any and all scripts that use it.\nWhat do you think?\n. @lukebakken I'm happy to change it to 1>/dev/null as long as that is portable to all of our supported systems.\n. Ha, fair enough. \n. I have made the change to redirect to /dev/null in a0453c3.\n. Also: https://github.com/basho/riak_ee/pull/338\n. +1 5474b5369645b609a27589e946f5e1a32f4f4bea\n. ",
    "jenafermiller": "PR for fix:  https://github.com/basho/riak/pull/749 \n. @cmeiklejohn backports :) thanks!\n2.0:  https://github.com/basho/riak/pull/750\n2.1: https://github.com/basho/riak/pull/751\n. @jonmeredith Thanks for the catch!  :)  I've added two new pull requests for each back port against the specific branch.  Let me know if that works out better.  \n2.0:  https://github.com/basho/riak/pull/752\n2.1:  https://github.com/basho/riak/pull/753\n. ",
    "lucasbru": "I have the same problem (Fedora 23)\n. Cryptic error message, easy solution.\nTry a sudo apt-get install erlang-parsetools\n. ",
    "ArpitSuthar": "From the third party results we can that RocksDB is performing better in terms of Read/Write/Delete queries and DB compaction. Also it has performed average in other areas that are tested.\n@matthewvon  I would love to personally test the configuration of RockDB as storage with comparison of existing LevelDB. Though I need some guide to how to switch from RocksDB and test it.\n. @matthewvon instead of completely switching to rocksdb , is it possible to have options for storage method like currently we have for bitcask and eleveldb ?\n. ",
    "urbanserj": "I would like to share my experience with rocksdb as a replacement for leveldb in our riak_kv-based database. I use leofs' erocksdb nif library [1]. It takes two working days to transit existing custom leveldb-backend to new rocksdb-backend.\nPros:\n1. Storage efficiency increases up to 2 times\n2. Disk utilization slumps [2]\n3. Get, put, and fold operations speed up 3 times\n4. Memory usage reduces to one-third\n5. Cpu/schedulers utilization slightly diminishes\nCons:\n1. Rocksdb has too many options\nAlso, rocksdb made it possible to implement new features. For example, now we have an efficient online snapshot for backup using checkpoints, automatic repair after a hard restart, and so on.\n[1] https://github.com/leo-project/erocksdb\n[2] https://monosnap.com/file/1jS0v2UUiGqI4KVhHq0vuAHOyFKwfS.png\n. ",
    "schrepfler": "Wondering if anyone picked this up?. ",
    "yangchengjian": "I meet the problem too!\n. ",
    "cclam0827": "riak 2.1.1 not yet support otp17\n. ",
    "laurenrother": ":+1: \n. \ud83d\udc4d \n. Beautiful! \ud83d\udc4d \ud83c\udf89 \n. Can we say \"built, reviewed, and are ready to ship\"? I think having Product/Docs review the packages has been really helpful in catching hiccups early.. Can we clarify what \"expected delivery\" means? I read this a couple times before realizing it was not \"expected delivery of the release to the public\".. What is an External Release doc?. Weren't we initially talking about trying to do two releases at once to see how it went? Has that become obviously a bad idea? If so, why? #curious . Are we defining who makes up the Release Management team?. I don't know if it makes sense here, but I would love it if we noted somewhere official that Release Notes in packages = link to the URL of the release notes on the docs site.. Yeah, and that is 100% easy to do since it will always be the same URL just incremented up a number. . Can we have a \"predocs submitted to Tech Pubs team\" added, please? . ",
    "kri5": ":+1: \n. ",
    "jagguli": "+1 \n. I tried to build on stretch got an erlang version error, riak requires OTP 17 and current is 19. ",
    "Tatikoma": "+1\n. ",
    "nerophon": "Seconded. However we must consider the case where there a lot of bucket-types. How many is it practical to list (including configuration)? i.e. what's the overhead, does it bloat the size of debug-output?\n. Seems sensible to me.\n. All sounds good but why remove riak-admin vnode-status? Wasn't that missing for ages then we deliberately added it?\nupdate Drew notes that riak-admin vnode-status can badly bloat output; therefore it should be used only when specifically needed, and directly as per the notes above.. ",
    "GabrielNicolasAvellaneda": "@javajolt :+1: \n. ",
    "ph07": "Hello,\nFor now, Redis add-on will be only available in Enterprise Edition. We do plan to release some of functionality as open source in future releases, but there is no hard date for it.\nThanks,\nPavel\n. ",
    "ronalfei": "thank you for reply, expecting open source!\n. ",
    "ZhaoX": "@ph07  I'm very interested in Redis add-on too. While it is only available in Enterprise Edition, are there more documents about it's architecture? \nThanks.\n. @cuyler Thank you for your reply.\n. ",
    "cuyler": "@ZhaoX You can take a look at that official documentation here: http://docs.basho.com/dataplatform/latest/\nThat'll give at least a bit of an idea on how it works.\n. ",
    "ooshlablu": "Looks good, ship it.\n. +1\n. +1 \n. Looks good.\n. +1\n. Creating new PR.\n. :+1: \n. :+1: \n. :+1: \n. :+1: \n. @laurenrother yes, I'll fix it in the next iteration.\n@javajolt Wednesday would be delivery from Eng, which would give us time to have the go/no-go the next day after we sit on it, and have a day to stage, etc. But yes, standardization, similar to what the TS team is doing. The review that I think is most helpful in that regard is when they are actually staged in the download locations, which I have as a different step. I want the go/no-go to happen before staging so I don't have to double up efforts if we back out at this point. I can explicitly call that out in the staging step.. something like this: https://github.com/basho/internal_wiki/wiki/riak-2.0.8-External-Release\nI don't know really what to call it, since it's for internal use only, but contains the links for the external packages. I'm open to suggestions.. We have to try it first and see, and if it works, we can change the policy. For now, I want to set expectations that we won't be doing it until we have done that experiment so it doesn't surprise us.... because you know it will :-). @laurenrother I will change the wording so that becomes clear\n@javajolt My concern is that the release notes are the only guaranteed way, at this point, to move the sha's forward so we can tag it and have jenkins pick it up. Everything except for 2.0 series relies on tagging the deps all the way down, which is probably the right way to do it, but since 2.0 just uses locked deps, we can't rely on it everywhere. Maybe we should just change that for 2.0 tho, and start tagging everything like we do for everything else. :man_shrugging: . Yeah, it's an actual team: @javajolt and @korry8911 \nAt some point soon, I'm gonna change this line anyway, since the smoke testing will be automated in jenkins as part of the standardized build process.. I was thinking that it would be evident in the doc I'm gonna drop in EE as well, but it probably makes more sense to call out each product in both places so there's no change of forgetting. Should be a separate set of docs, but they don't exist yet. it would definitely be a good idea to link them in when they get put together.. yeah, coming up shortly. Yeah, I think we should establish a new one. I can add that in.. ",
    "andytill": "+1 82b29e8\n. ",
    "tburghart": "Empirically, HiPE _seems_ to be ok on 64-bit Linux, but I'm not going to go out on a limb and recommend it - the problem is that if it breaks, it won't generally do it until the system is pretty well loaded, so the results can be catastrophic.\nOn careful review of the above there are version differences in several key components, so they can't both be from the 2.0.4 tag. Are these Basho release packages?\n. Hi @cread \nI continue to believe what you're seeing is not directly related to HiPE, but I'm wondering if you're seeing the same behavior across versions with and without a HiPE-enabled VM?  In particular, HiPE (when it works) does optimize some operations, so could this be related to being CPU-bound?\nAre you seeing your CPUs maxed out (or nearly so) without HiPE, but with lower loads when HiPE's enabled?\n. @cread, can you confirm whether your kernel is tuned in accordance with the \"Virtual Memory\" and \"Schedulers\" sections of our System Performance Tuning docs?\nIf it's not, can you try those settings?  Given the symptoms you describe (which we're working to reproduce), it's clearly not the direct problem, but I'd like to eliminate (or identify) it as a contributor that might be driving the node into the state you're seeing.\n. Thanks, just trying to cover all the bases\n. With OpenSSL deprecated in OS X, the interface to the version included in the OS is undefined.\nBy statically linking to OpenSSL, the OTP crypto app is guaranteed to use runtime code that matches the interface it was built against.\nThat assurance is no longer available with dynamic linking.. ",
    "cread": "The packages that are giving me problems are the Basho released .deb packages available from http://docs.basho.com/riak/latest/downloads/\nWe've been running the HiPE enabled build in production since late January this year, which I built from the 2.0.4 tarball downloaded from the above URL. I built them following the Installing from Source instructions for 2.0.0.\n. I've just done a test with my own build of 2.1.1 from source. The only difference between this build and the .deb packages is that my build has HiPE enabled. We see the same behavior.\nJust in case it matters, here are the properties for the bucket we're writing to:\njson\n{\n  \"props\": {\n    \"young_vclock\": 20,\n    \"write_once\": false,\n    \"w\": \"quorum\",\n    \"small_vclock\": 50,\n    \"rw\": \"quorum\",\n    \"r\": 1,\n    \"last_write_wins\": false,\n    \"dw\": \"quorum\",\n    \"dvv_enabled\": false,\n    \"chash_keyfun\": {\n      \"fun\": \"chash_std_keyfun\",\n      \"mod\": \"riak_core_util\"\n    },\n    \"big_vclock\": 50,\n    \"basic_quorum\": false,\n    \"allow_mult\": false,\n    \"name\": \"r\",\n    \"linkfun\": {\n      \"fun\": \"mapreduce_linkfun\",\n      \"mod\": \"riak_kv_wm_link_walker\"\n    },\n    \"n_val\": 3,\n    \"notfound_ok\": false,\n    \"old_vclock\": 86400,\n    \"postcommit\": [],\n    \"pr\": 0,\n    \"precommit\": [],\n    \"pw\": 0\n  }\n}\nI'm going to see if I can reproduce this on a single node now.\n. I've been able to reproduce the problem using basho_bench on our test cluster. Here's the basho_bench configuration:\nerlang\n{mode, max}.\n{duration, 1440}.\n{report_interval, 1}.\n{concurrent, 192}.\n{driver, basho_bench_driver_riakc_pb}.\n{key_generator, {int_to_bin_bigendian, {partitioned_sequential_int, 4294967296}}}.\n{value_generator, {uniform_bin, 2048, 1048576}}.\n{riakc_pb_ips, [\n        {10,1,1,1},\n        {10,1,1,2},\n        {10,1,1,3},\n        {10,1,1,4},\n        {10,1,1,5}\n]}.\n{riakc_pb_replies, 1}.\n{operations, [{put, 1}]}.\n{pb_connect_options, [{auto_reconnect, true}]}.\n{pb_timeout_general, 30000}.\n{pb_timeout_read, 5000}.\n{pb_timeout_write, 5000}.\n{pb_timeout_listkeys, 50000}.\nWithin 2 hours or so you'll see node_put_fsm_active start creeping up. After it passes 1000 or so you can stop the load and that value does not drop.\nHere's the put_fsm stats from one of the nodes once the load has been removed:\nlate_put_fsm_coordinator_ack : 941\nnode_put_fsm_active : 626\nnode_put_fsm_active_60s : 0\nnode_put_fsm_counter_time_100 : 0\nnode_put_fsm_counter_time_95 : 0\nnode_put_fsm_counter_time_99 : 0\nnode_put_fsm_counter_time_mean : 0\nnode_put_fsm_counter_time_median : 0\nnode_put_fsm_in_rate : 0\nnode_put_fsm_map_time_100 : 0\nnode_put_fsm_map_time_95 : 0\nnode_put_fsm_map_time_99 : 0\nnode_put_fsm_map_time_mean : 0\nnode_put_fsm_map_time_median : 0\nnode_put_fsm_out_rate : 0\nnode_put_fsm_rejected : 0\nnode_put_fsm_rejected_60s : 0\nnode_put_fsm_rejected_total : 0\nnode_put_fsm_set_time_100 : 0\nnode_put_fsm_set_time_95 : 0\nnode_put_fsm_set_time_99 : 0\nnode_put_fsm_set_time_mean : 0\nnode_put_fsm_set_time_median : 0\nnode_put_fsm_time_100 : 0\nnode_put_fsm_time_95 : 0\nnode_put_fsm_time_99 : 0\nnode_put_fsm_time_mean : 0\nnode_put_fsm_time_median : 0\nvnode_put_fsm_time_100 : 0\nvnode_put_fsm_time_95 : 0\nvnode_put_fsm_time_99 : 0\nvnode_put_fsm_time_mean : 0\nvnode_put_fsm_time_median : 0\nI'm now running this configuration against a single machine to see if I can reproduce it there.\n. Ran the same script on a single node for 8 hours without being able to reproduce the lock up which I guess makes this a cluster problem.\n. Bump. \n. Correct, this is not HiPE related, as I pointed out in my first comment on 11/11. I get the same bad behavior using Riak 2.1.1 using the .deb package for Ubuntu 14.04 as built by Basho (no HiPE) and using a build I did from the source tarball with HiPE. The CPU's do not max out with either build.\nHave you been able to reproduce the lockup using the basho_bench config I provided earlier? \n. @tburghart I can confirm we're tuned as per the doc. \n. @tburghart have you guys been able to reproduce this yet?\n. Ping! Anyone there? Anyone looking at this?\n. Thanks John! Let me know if you need any more help or info from me getting to the bottom of this...\n. ",
    "Zerebokep": "\nThe last few errors in the error log are:\n\n2015-11-08 23:20:15.760 [error] <0.229.0> Supervisor riak_pipe_fitting_sup had child undefined started with riak_pipe_fitting:start_link() at <0.5108.3055> exit with reason noproc in context shutdown_error\n2015-11-08 23:21:01.390 [error] <0.229.0> Supervisor riak_pipe_fitting_sup had child undefined started with riak_pipe_fitting:start_link() at <0.8717.3055> exit with reason noproc in context shutdown_error\n2015-11-08 23:25:46.886 [error] <0.229.0> Supervisor riak_pipe_fitting_sup had child undefined started with riak_pipe_fitting:start_link() at <0.27331.3055> exit with reason noproc in context shutdown_error\n2015-11-09 15:57:56.647 [error] <0.10544.7>@riak_kv_console:status:185 Status failed exit:{shutdown,{gen_server,call,[riak_core_vnode_manager,{all_vnodes,riak_kv_vnode},infinity]}}\np.s. The riak_pipe_fitting_sup error exists since we use riak and should be unrelated to this issue.\ncrash log:\n2015-11-08 23:17:35 =SUPERVISOR REPORT====\n     Supervisor: {local,riak_pipe_fitting_sup}\n     Context:    shutdown_error\n     Reason:     noproc\n     Offender:   [{pid,<0.23412.3054>},{name,undefined},{mfargs,{riak_pipe_fitting,start_link,[]}},{restart_type,temporary},{shutdown,2000},{child_type,worker}]\n2015-11-08 23:20:15 =SUPERVISOR REPORT====\n     Supervisor: {local,riak_pipe_fitting_sup}\n     Context:    shutdown_error\n     Reason:     noproc\n     Offender:   [{pid,<0.5108.3055>},{name,undefined},{mfargs,{riak_pipe_fitting,start_link,[]}},{restart_type,temporary},{shutdown,2000},{child_type,worker}]\n2015-11-08 23:21:01 =SUPERVISOR REPORT====\n     Supervisor: {local,riak_pipe_fitting_sup}\n     Context:    shutdown_error\n     Reason:     noproc\n     Offender:   [{pid,<0.8717.3055>},{name,undefined},{mfargs,{riak_pipe_fitting,start_link,[]}},{restart_type,temporary},{shutdown,2000},{child_type,worker}]\n2015-11-08 23:25:46 =SUPERVISOR REPORT====\n     Supervisor: {local,riak_pipe_fitting_sup}\n     Context:    shutdown_error\n     Reason:     noproc\n     Offender:   [{pid,<0.27331.3055>},{name,undefined},{mfargs,{riak_pipe_fitting,start_link,[]}},{restart_type,temporary},{shutdown,2000},{child_type,worker}]\n2.\nriak console\nconfig is OK\n-config /etc/riak/app.config -args_file /etc/riak/vm.args -vm_args /etc/riak/vm.args\nExec:  /usr/lib/riak/erts-5.10.3/bin/erlexec -boot /usr/lib/riak/releases/2.0.6/riak               -config /etc/riak/app.config -args_file /etc/riak/vm.args -vm_args /etc/riak/vm.args              -pa /usr/lib/riak/lib/basho-patches -- console\nRoot: /usr/lib/riak\nErlang R16B02_basho8 (erts-5.10.3) [source] [64-bit] [smp:8:8] [async-threads:16] [kernel-poll:true] [frame-pointer]\nEshell V5.10.3  (abort with ^G)\n(riak@127.0.0.1)1> \n\n\nI've also found this one in our kern.log:\n\nbeam.smp[3150]: segfault at 0 ip 00007feea1acb8ed sp 00007fee972cfbb0 error 4 in eleveldb.so[7feea1a75000+85000]\nIt's a debian server, running with riak 2.0.6.\n. I'm not sure if this helps, but here is some output of riak top:\n\n'riak@127.0.0.1'                                                          09:21:44\n Load:  cpu         0               Memory:  total       79619    binary        208\n        procs    1391                        processes   41132    code        13421\n        runq        0                        atom          549    ets          6949\nPid                 Name or Initial Func         Time       Reds     Memory       MsgQ Current Function\n<6410.92.0>         riak_sysmon_filter            '-'       2813       2744          0 gen_server:loop/6\n<6410.103.0>        erlang:apply/2                '-'       2351       2600          0 cpu_sup:measurement_server_loop/1\n<6410.341.0>        riak_kv_put_fsm_sj_stats      '-'       1112       5720          0 gen_server:loop/6\n<6410.365.0>        riak_kv_stat_sj_stats         '-'       1090       5720          0 gen_server:loop/6\n<6410.353.0>        riak_kv_get_fsm_sj_stats      '-'       1064       5720          0 gen_server:loop/6\n<6410.308.0>        http://127.0.0.1:8098_moc     '-'        936       4528          0 gen_server:loop/6\n<6410.349.0>        riak_kv_get_fsm_sj_5          '-'        714       3848          0 gen_server:loop/6\n<6410.135.0>        webmachine_log_event          '-'        374      21528          0 gen_event:fetch_msg/5\n<6410.372.0>        riak_kv_vnode_master          '-'        264       2704          0 timer:sleep/1\n<6410.2460.0>       mochiweb_acceptor:init/3      '-'        109       2776          0 prim_inet:accept0/2                     \n\n'riak@127.0.0.1'                                                          09:21:49\n Load:  cpu         0               Memory:  total       79644    binary        233\n        procs    1391                        processes   41134    code        13421\n        runq        0                        atom          549    ets          6949\nPid                 Name or Initial Func         Time       Reds     Memory       MsgQ Current Function\n<6410.92.0>         riak_sysmon_filter            '-'       2607       3888          0 gen_server:loop/6\n<6410.308.0>        http://127.0.0.1:8098_moc     '-'       2571       6400          0 gen_server:loop/6\n<6410.349.0>        riak_kv_get_fsm_sj_5          '-'       1641       3848          0 gen_server:loop/6\n<6410.341.0>        riak_kv_put_fsm_sj_stats      '-'       1379       5720          0 gen_server:loop/6\n<6410.365.0>        riak_kv_stat_sj_stats         '-'       1360       8736          0 gen_server:loop/6\n<6410.353.0>        riak_kv_get_fsm_sj_stats      '-'       1321       5720          0 gen_server:loop/6\n<6410.135.0>        webmachine_log_event          '-'       1122      21528          0 gen_event:fetch_msg/5\n<6410.348.0>        riak_kv_get_fsm_sj_4          '-'        435       3848          0 gen_server:loop/6\n<6410.372.0>        riak_kv_vnode_master          '-'        272       2704          0 timer:sleep/1\n<6410.20.0>         net_kernel                    '-'        156       4048          0 gen_server:loop/6                       \n\n'riak@127.0.0.1'                                                          09:21:53\n Load:  cpu         0               Memory:  total       79632    binary        232\n        procs    1391                        processes   41102    code        13421\n        runq        0                        atom          549    ets          6949\nPid                 Name or Initial Func         Time       Reds     Memory       MsgQ Current Function\n<6410.3.0>          erl_prim_loader               '-'      15777     142624          0 erl_prim_loader:loop/3\n<6410.92.0>         riak_sysmon_filter            '-'       2813       3888          0 gen_server:loop/6\n<6410.103.0>        erlang:apply/2                '-'       2351       2600          0 cpu_sup:measurement_server_loop/1\n<6410.195.0>        riak_core_capability          '-'       2312      21528          0 gen_server:loop/6\n<6410.197.0>        riak_core_claimant            '-'       2111      88504          0 gen_server:loop/6\n<6410.353.0>        riak_kv_get_fsm_sj_stats      '-'       1117       5720          0 gen_server:loop/6\n<6410.341.0>        riak_kv_put_fsm_sj_stats      '-'       1112       5720          0 gen_server:loop/6\n<6410.365.0>        riak_kv_stat_sj_stats         '-'       1090       8736          0 gen_server:loop/6\n<6410.308.0>        http://127.0.0.1:8098_moc     '-'       1079       6400          0 gen_server:loop/6\n<6410.349.0>        riak_kv_get_fsm_sj_5          '-'        844       3848          0 gen_server:loop/6                       \n\n'riak@127.0.0.1'                                                          09:21:58\n Load:  cpu         0               Memory:  total       79630    binary        232\n        procs    1391                        processes   41120    code        13421\n        runq        0                        atom          549    ets          6949\nPid                 Name or Initial Func         Time       Reds     Memory       MsgQ Current Function\n<6410.308.0>        http://127.0.0.1:8098_moc     '-'       5045       6400          0 gen_server:loop/6\n<6410.349.0>        riak_kv_get_fsm_sj_5          '-'       3400       2704          0 gen_server:loop/6\n<6410.103.0>        erlang:apply/2                '-'       2351       2600          0 cpu_sup:measurement_server_loop/1\n<6410.135.0>        webmachine_log_event          '-'       2244      13624          0 gen_event:fetch_msg/5\n<6410.341.0>        riak_kv_put_fsm_sj_stats      '-'       1403       5720          0 gen_server:loop/6\n<6410.365.0>        riak_kv_stat_sj_stats         '-'       1367       5720          0 gen_server:loop/6\n<6410.353.0>        riak_kv_get_fsm_sj_stats      '-'       1346       5720          0 gen_server:loop/6\n<6410.92.0>         riak_sysmon_filter            '-'        806       2744          0 gen_server:loop/6\n<6410.348.0>        riak_kv_get_fsm_sj_4          '-'        536       2704          0 gen_server:loop/6\n<6410.20.0>         net_kernel                    '-'        305       4048          0 gen_server:loop/6                       \n\n'riak@127.0.0.1'                                                          09:22:02\n Load:  cpu         0               Memory:  total       79634    binary        231\n        procs    1391                        processes   41124    code        13421\n        runq        0                        atom          549    ets          6949\nPid                 Name or Initial Func         Time       Reds     Memory       MsgQ Current Function\n<6410.3.0>          erl_prim_loader               '-'      15207     142624          0 erl_prim_loader:loop/3\n<6410.92.0>         riak_sysmon_filter            '-'       2813       2744          0 gen_server:loop/6\n<6410.195.0>        riak_core_capability          '-'       2494      21528          0 gen_server:loop/6\n<6410.103.0>        erlang:apply/2                '-'       2351       2600          0 cpu_sup:measurement_server_loop/1\n<6410.197.0>        riak_core_claimant            '-'       2111      88504          0 gen_server:loop/6\n<6410.353.0>        riak_kv_get_fsm_sj_stats      '-'       1117       5720          0 gen_server:loop/6\n<6410.341.0>        riak_kv_put_fsm_sj_stats      '-'       1112       5720          0 gen_server:loop/6\n<6410.365.0>        riak_kv_stat_sj_stats         '-'       1090       5720          0 gen_server:loop/6\n<6410.308.0>        http://127.0.0.1:8098_moc     '-'        430       6400          0 gen_server:loop/6\n<6410.349.0>        riak_kv_get_fsm_sj_5          '-'        426       2704          0 gen_server:loop/6\n. I've changed the permissions, but it doesn't seem to solve the problem.\n. That's weird, even the status commands are telling me that everything is fine.\nriak-admin member-status\n================================= Membership ==================================\nStatus     Ring    Pending    Node\nvalid     100.0%      --      'riak@127.0.0.1'\nValid:1 / Leaving:0 / Exiting:0 / Joining:0 / Down:0\nriak-admin ring-status\n================================== Claimant ===================================\nClaimant:  'riak@127.0.0.1'\nStatus:     up\nRing Ready: true\n============================== Ownership Handoff ==============================\nNo pending changes.\n============================== Unreachable Nodes ==============================\nAll nodes are up and reachable\n. Okay guys, problem is solved. I've repaired all leveldb partitions and riak started normally. It seems there is some reporting issue with riak when something is wrong with an leveldb partition.\n. I think a bulletin board software would be perfect (e.g. invision power board, vBulletin, etc.). Or stackexchange like Unity Answers (http://answers.unity3d.com).\nImo mailing lists have a horrible usability, they are slow (if they are moderated), they have no search (you have to use google site: to find your info), you always need to publish your email, it's an really old medium, no one wants anymore. \n. ",
    "hmmr": "To me Segfault at 0 suggests it's a NULL pointer dereference, somewhere in eleveldb C++ code. @mvm?\n. Thanks for the patch. Can you open a PR in basho/eleveldb, against branch 2.1? cc @erikleitch.\n. @javajolt you are correct. Fixed and force-pushed.\n. +1 a9b8c23\n. The pedant in me suggests <{+|-}HHMM>.\n. ",
    "yuriscom": "it was supposed to be for basho-riak-client. sorry guys.\n. ",
    "muke5hy": "No I did not install any language pack. \n. ",
    "bashopatricia": "create jira issue\n. create jira issue\n. create jira issue. create jira issue. ",
    "seanjensengrey": "Hey @zhaohanweng do you have search enabled for your bucket? \nI haven't used solr geospatial but you might need a custom extractor\nI'll check back tomorrow, it is late here.\n. Great. I am working on a proof of concept around the format you described. I'll update this issue when it is published.\n. @fgonzalezaguirre The most expedient thing would be to build a release from src.\nhttp://docs.basho.com/riak/kv/2.1.4/setup/installing/source/erlang/#installing-with-kerl\nRiak requires a Basho Erlang\n./kerl build git git://github.com/basho/otp.git OTP_R16B02_basho8 R16B02-basho8\n. Hey Humberto,\nCould you try with 1.3.1 and report back?\nhttp://docs.basho.com/riak/ts/1.3.1/downloads/\nwhat is the contents of \n/etc/hosts\n/etc/resolv.conf\nAre your running any other Erlang based services? ps aux | grep beam\n. Hey RuyiOne,\nIt sounds like you are trying to setup a multi-node cluster on your local machine. This is totally doable.\nI have a local 3 node cluster. The main thing to worry about is not having any collisions in the ports, below is a diff between riak-a and riak-b two nodes in my cluster.\nUnpack the archive for an install, rename it and modify etc/riak.conf for each one so that ports are unique.\nStart each node with the same cookie and then join them together.\nIt is late here. Will check back in the morning.\n```\n(x.env) ip-192-168-0-7:test-riak-3-node basho$ diff riak-a/etc/riak.conf riak-b/etc/riak.conf\n1,3c1,7\n< listener.protobuf.internal = 127.0.0.1:8087\n< listener.http.internal = 127.0.0.1:8098\n< handoff.port = 8099\n\n\nlistener.protobuf.internal = 127.0.0.1:8187\nlistener.http.internal = 127.0.0.1:8198\nhandoff.port = 8199 \nsearch.solr.port = 8193\nsearch.solr.jmx_port = 9985\n5,6d8\n< search.solr.port = 8093\n< search.solr.jmx_port = 8985\n113c115\n< nodename = riaka@127.0.0.1\n\n\n\nnodename = riakb@127.0.0.1\n```\n. \n",
    "zhaohanweng": "ya. the search is enabled. Now I have also stored the data in 'lat, lon' format, so do not need a custom extractor.\n. ",
    "ubeatha": "Updated report to include ringready output.  Output for ringready, ring-status, and member-status identical on both nodes. \n. ",
    "Linicks": "A package for Ubuntu 16.04 would be appreciated.  I would be happy to do any testing, etc.\n. ",
    "manifest": "I also look forward to it.\n. ",
    "fgonzalezaguirre": "Can you guys provide an ETA? I just want to see if I should just install from source or wait for the package to be available (which I'd much prefer as I wouldn't have to change my ansible scripts on a per version basis)\n. ",
    "rhumbertgz": "Hi, thanks for the advice. I downloaded 1.3.1 and it works perfectly :)\nBest regards \nHumberto\n\nOn 20 Jul 2016, at 04:19, Sean Jensen-Grey notifications@github.com wrote:\nHey Humberto,\nCould you try with 1.3.1 and report back?\nhttp://docs.basho.com/riak/ts/1.3.1/downloads/ http://docs.basho.com/riak/ts/1.3.1/downloads/\nwhat is the contents of\n/etc/hosts\n/etc/resolv.conf\nAre your running any other Erlang based services? ps aux | grep beam\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub https://github.com/basho/riak/issues/851#issuecomment-233820896, or mute the thread https://github.com/notifications/unsubscribe-auth/AJ6WnTLvKRQ0l5P0WlAW4kc4jZS0F6KBks5qXYWUgaJpZM4JC7LN.\n. \n",
    "fadushin": "+1 3dc625d\n. +1 c1e33da\n. +1 e6ded3e\n. +1 06d11ba\n. +1 6f1a80b\n. +1. +1. +1. +1. 2.3.0?. We need to make sure we remove this when we PR the removal of riak_search from the riak repo.. Definitely thumbs up to this. ",
    "haraldmosh": "Anything happening with this?\nWe're seeing it in production. Under heavy load it seems to cause large queues which eventually cause riak to complain it's overloaded.. ",
    "binarytemple-external": "This going into the next release ? \n. Nice one @roooms @bsparrow435 \n. ",
    "jbrisbin": "I don't remember for certain but the ones I created the first time I did this used a similar technique with the dot and it creates slight weirdness when unzipping. I switched to a splat instead and that worked better.\n. @ooshlablu +1 /cc @mdigan @sanmiguel \n. ",
    "bashoshaun": "Done.\n. ",
    "RuyiOne": "Thanks. It was the naming convention.\nnodename = riaka@127.0.0.1\nnodename = riakb@127.0.0.1\nI think documentation was a little fuzzy. Thanks again @seanjensengrey !!!\n. ",
    "wozniakjan": "I would like to take the liberty and provide a makeshift answer, because I faced the issue few weeks ago as well. The error message means you may have misconfigured the ssh and git on your system. I found two possible solutions:\n1) take a look at ssh and git configuration and set it up\n2) since the repositories are all hosted on github and protocols are for your case interchangeable, you may take a look at rebar.config and rebar.config.lock and change the lines from 'git@github.com:jlouis/fuse.git' to 'git://github.com/jlouis/fuse.git' and the others when needed. This will bypass ssh and use git protocol for the network communication (also you can use https://)\nmore on git here: https://git-scm.com/book/en/v2/Git-on-the-Server-The-Protocols\n. My apologies, I should have specified in which  dependency rebar config was attempting to use ssh. The last dependency that failed to compile is yokozuna according to the log you provided. Try checking a file in 'deps/yokozuna/rebar.config'. Also you can recursively grep through the directory where you tried building riak for hints - 'grep . --include=*.config -nrie \"git@github.com\" '\n. ",
    "topeomot2": "Hi Wozniakjan, I checked both files and I can't find any line with  'git@github.com:jlouis/fuse.git'.\n. It worked. Thanks.\n. ",
    "korry8911": "+1\n. ",
    "paulhenrich": "\ud83d\udc4d . ",
    "thumbot": "|  |  | |  |\n| ------------ | -------------|------------ | ------------- |\nfd-deps-cleanup a647b0f | :arrow_right: | develop eea8277 | :white_check_mark: completed\nLooks good!  :+1: \n\n:white_check_mark: MERGE \n\n\n> Started at: 2017-01-11 11:29\n> Duration: 1 seconds.\n> Result:  OK\n> Message: Merge Success: fd-deps-cleanup a647b0f6716477d282b719fde981a28d81c42849 onto target branch: develop eea82771170f47bc2d8a2cafbff909b5caed681d\n> Exit Code:  OK\n\n> :page_facing_up:\n\n\n```\n\n\n\n\n\n\n  Merge made by the 'recursive' strategy.\n .thumbs.yml  |  8 ++++++++\n rebar.config | 22 +++++++++++-----------\n 2 files changed, 19 insertions(+), 11 deletions(-)\n create mode 100644 .thumbs.yml\n\n\n\n```\n\n--------------------------------------------------\n\n\n\n:white_check_mark: MAKE_DEPS \n\n\n> Started at: 2017-01-11 11:29\n> Duration: 44 seconds.\n> Result:  OK\n> Message: OK\n> Exit Code:  0\n\n> :page_facing_up:\n\n\n```\n\ncd /tmp/thumbs/develop.eea8277.fd-deps-cleanup.a647b0f; make deps\n\n\n\n\n\n\n... Snipped 47 lines ...\n\"https://github.com/basho/erlang-syslog\",\n                         {tag,\"1.0.3\"}}\nCloning into 'syslog'...\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> lager (get-deps)\nPulling goldrush from {git,\"https://github.com/basho/goldrush.git\",\n                           {tag,\"0.1.9\"}}\nCloning into 'goldrush'...\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> goldrush (get-deps)\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> syslog (get-deps)\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> cluster_info (get-deps)\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> riak_kv (get-deps)\nPulling sidejob from {git,\"https://github.com/basho/sidejob.git\",\n                          {tag,\"2.0.1\"}}\nCloning into 'sidejob'...\nPulling erlang_js from {git,\"https://github.com/basho/erlang_js.git\",\n                            {tag,\"1.3.0\"}}\nCloning into 'erlang_js'...\nPulling bitcask from {git,\"https://github.com/basho/bitcask.git\",\n                          {tag,\"2.0.6\"}}\nCloning into 'bitcask'...\nPulling eper from {git,\"https://github.com/basho/eper.git\",{tag,\"0.78\"}}\nCloning into 'eper'...\nPulling sext from {git,\"https://github.com/basho/sext.git\",{tag,\"1.1p3\"}}\nCloning into 'sext'...\nPulling riak_pipe from {git,\"https://github.com/basho/riak_pipe.git\",\n                            {branch,\"develop\"}}\nCloning into 'riak_pipe'...\nPulling riak_dt from {git,\"https://github.com/basho/riak_dt.git\",\n                          {branch,\"develop\"}}\nCloning into 'riak_dt'...\nPulling eunit_formatters from {git,\"https://github.com/basho/eunit_formatters\",\n                                   {tag,\"0.1.2\"}}\nCloning into 'eunit_formatters'...\nPulling riak_api from {git,\"https://github.com/basho/riak_api.git\",\n                           {branch,\"sweeper-2.3-integration\"}}\nCloning into 'riak_api'...\nPulling hyper from {git,\"https://github.com/basho/hyper\",{tag,\"1.0.0\"}}\nCloning into 'hyper'...\nPulling clique from {git,\"https://github.com/basho/clique.git\",{tag,\"0.3.7\"}}\nCloning into 'clique'...\nPulling chronos from {git,\"https://github.com/lehoff/chronos.git\",\n                          {tag,\"0.1.4\"}}\nCloning into 'chronos'...\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> sidejob (get-deps)\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> erlang_js (get-deps)\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> meck (get-deps)\n==> bitcask (get-deps)\nPulling cuttlefish from {git,\"https://github.com/basho/cuttlefish.git\",\n                             {tag,\"2.0.10\"}}\nCloning into 'cuttlefish'...\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> cuttlefish (get-deps)\nPulling getopt from {git,\"https://github.com/basho/getopt.git\",{tag,\"v0.8.2\"}}\nCloning into 'getopt'...\nPulling neotoma from {git,\"https://github.com/basho/neotoma.git\",\n                          {tag,\"1.7.3\"}}\nCloning into 'neotoma'...\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> getopt (get-deps)\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> neotoma (get-deps)\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> eper (get-deps)\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> sext (get-deps)\nPulling edown from {git,\"git://github.com/uwiger/edown.git\",{tag,\"0.5\"}}\nCloning into 'edown'...\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> edown (get-deps)\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> riak_pipe (get-deps)\nPulling riak_core from {git,\"git://github.com/basho/riak_core.git\",\n                            {branch,\"develop\"}}\nCloning into 'riak_core'...\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> clique (get-deps)\n==> riak_core (get-deps)\nPulling poolboy from {git,\"git://github.com/basho/poolboy.git\",\n                          {tag,\"0.8.1p3\"}}\nCloning into 'poolboy'...\nPulling basho_stats from {git,\"git://github.com/basho/basho_stats.git\",\n                              {tag,\"1.0.3\"}}\nCloning into 'basho_stats'...\nPulling riak_sysmon from {git,\"git://github.com/basho/riak_sysmon.git\",\n                              {tag,\"2.1.4\"}}\nCloning into 'riak_sysmon'...\nPulling eleveldb from {git,\"git://github.com/basho/eleveldb.git\",\n                           {tag,\"2.0.32\"}}\nCloning into 'eleveldb'...\nPulling riak_ensemble from {git,\"git://github.com/basho/riak_ensemble\",\n                                {tag,\"2.1.6\"}}\nCloning into 'riak_ensemble'...\nPulling pbkdf2 from {git,\"git://github.com/basho/erlang-pbkdf2.git\",\n                         {tag,\"2.0.0\"}}\nCloning into 'pbkdf2'...\nPulling exometer_core from {git,\"git://github.com/basho/exometer_core.git\",\n                                {tag,\"1.0.0-basho9\"}}\nCloning into 'exometer_core'...\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> poolboy (get-deps)\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> basho_stats (get-deps)\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> riak_sysmon (get-deps)\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> eleveldb (get-deps)\nCloning into 'leveldb'...\nNote: checking out '2.0.31'.\n\nYou are in 'detached HEAD' state. You can look around, make experimental\nchanges and commit them, and you can discard any commits you make in this\nstate without impacting any branches by performing another checkout.\n\nIf you want to create a new branch to retain commits you create, you may\ndo so (now or later) by using -b with the checkout command again. Example:\n\n  git checkout -b new_branch_name\n\nHEAD is now at e6a2939... add (int) cast to fix compile errors on 10 of 16 platforms ... argh.\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> riak_ensemble (get-deps)\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> pbkdf2 (get-deps)\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> src (get-deps)\n==> exometer_core (get-deps)\nPulling parse_trans from {git,\"git://github.com/basho/parse_trans.git\",\n                              {tag,\"2.9.2p1\"}}\nCloning into 'parse_trans'...\nPulling folsom from {git,\"git://github.com/basho/folsom.git\",{tag,\"0.7.4p5\"}}\nCloning into 'folsom'...\nPulling setup from {git,\"git://github.com/basho/setup.git\",{tag,\"1.4\"}}\nCloning into 'setup'...\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> parse_trans (get-deps)\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> folsom (get-deps)\nPulling bear from {git,\"git://github.com/basho/bear.git\",{tag,\"0.1.3p1\"}}\nCloning into 'bear'...\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> bear (get-deps)\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> setup (get-deps)\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> riak_dt (get-deps)\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> eunit_formatters (get-deps)\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> riak_api (get-deps)\nPulling riak_pb from {git,\"git://github.com/basho/riak_pb.git\",\n                          {branch,\"sweeper-develop-merge\"}}\nCloning into 'riak_pb'...\nPulling webmachine from {git,\"git://github.com/basho/webmachine.git\",\n                             {tag,\"1.10.8-basho1\"}}\nCloning into 'webmachine'...\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> riak_pb (get-deps)\nPulling hamcrest from {git,\"https://github.com/basho/hamcrest-erlang.git\",\n                           {tag,\"0.3.0-basho\"}}\nCloning into 'hamcrest'...\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> hamcrest (get-deps)\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> webmachine (get-deps)\nPulling mochiweb from {git,\"git://github.com/basho/mochiweb.git\",\n                           {tag,\"v2.9.0p2\"}}\nCloning into 'mochiweb'...\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> mochiweb (get-deps)\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> hyper (get-deps)\nPulling proper from {git,\"https://github.com/basho/proper.git\",{tag,\"v1.2p1\"}}\nCloning into 'proper'...\nPulling stdlib2 from {git,\"https://github.com/basho/stdlib2.git\",\n                          {tag,\"0.0.1\"}}\nCloning into 'stdlib2'...\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> proper (get-deps)\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> stdlib2 (get-deps)\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> chronos (get-deps)\nPulling gproc from {git,\"https://github.com/uwiger/gproc\",\"master\"}\nCloning into 'gproc'...\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> gproc (get-deps)\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> lucene_parser (get-deps)\n==> riak_search (get-deps)\nPulling merge_index from {git,\"git://github.com/basho/merge_index.git\",\n                              {tag,\"2.0.4\"}}\nCloning into 'merge_index'...\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> merge_index (get-deps)\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> riak_control (get-deps)\nPulling erlydtl from {git,\"git://github.com/basho/erlydtl.git\",\"d20b53f0\"}\nCloning into 'erlydtl'...\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> erlydtl (get-deps)\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> riaknostic (get-deps)\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> yokozuna (get-deps)\nPulling kvc from {git,\"https://github.com/basho/kvc.git\",{tag,\"v1.5.0\"}}\nCloning into 'kvc'...\nPulling ibrowse from {git,\"https://github.com/basho/ibrowse.git\",{tag,\"v4.3\"}}\nCloning into 'ibrowse'...\nPulling fuse from {git,\"https://github.com/basho/fuse.git\",{tag,\"v2.1.0\"}}\nCloning into 'fuse'...\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> kvc (get-deps)\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> ibrowse (get-deps)\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> fuse (get-deps)\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> riak_auth_mods (get-deps)\nPulling canola from {git,\"git://github.com/basho/canola.git\",{tag,\"2.0.0\"}}\nCloning into 'canola'...\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> canola (get-deps)\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> rebar_lock_deps_plugin (get-deps)\n\n\n\n\n```\n\n--------------------------------------------------\n\n\n\n:white_check_mark: MAKE \n\n\n> Started at: 2017-01-11 11:29\n> Duration: 92 seconds.\n> Result:  OK\n> Message: OK\n> Exit Code:  0\n\n> :page_facing_up:\n\n\n```\n\ncd /tmp/thumbs/develop.eea8277.fd-deps-cleanup.a647b0f; make\n\n\n\n\n\n\n... Snipped 2228 lines ...\n\nCompiled priv/tracers/tracer_accumulating_time.erl\nCompiled src/riak_kv_wm_keylist.erl\nCompiled priv/tracers/tracer_gc_latency.erl\nCompiled priv/tracers/tracer_large4.erl\nCompiled priv/tracers/tracer_backend_latency.erl\nCompiled priv/tracers/tracer_fsm_init.erl\nCompiled priv/tracers/tracer_timeit.erl\nCompiled priv/tracers/tracer_eleveldb_put_size.erl\nCompiled priv/tracers/tracer_latency_histogram.erl\nCompiled src/riak_kv_vnode.erl\nCompiled priv/tracers/tracer_read_bin_trace_file.erl\nCompiled priv/tracers/tracer_merge_and_and_handoff.erl\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> merge_index (compile)\nCompiled src/mi_buffer_converter.erl\nCompiled src/mi_bloom.erl\nCompiled src/mi_utils.erl\nCompiled src/mi_sup.erl\nCompiled src/mi_buffer.erl\nCompiled src/mi_app.erl\nCompiled src/mi_locks.erl\nCompiled src/merge_index.erl\nCompiled src/mi_buffer_converter_sup.erl\nCompiled src/mi_segment.erl\nCompiled src/mi_scheduler.erl\nCompiled src/mi_segment_writer.erl\nCompiled src/mi_server.erl\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> lucene_parser (compile)\nCompiled src/lucene_scan.xrl\nCompiled src/lucene_parse.yrl\nCompiled src/lucene_parse.erl\nCompiled src/lucene_scan.erl\nCompiled src/lucene_parser.erl\nCompiled src/lucene_scan_test.erl\nCompiled src/lucene_parse_test.erl\n==> riak_search (compile)\nCompiled src/riak_search_backend.erl\nCompiled src/riak_search_utils.erl\nCompiled src/riak_search_app.erl\nCompiled src/riak_search_stat.erl\nCompiled src/riak_search_kv_json_extractor.erl\nCompiled src/basho_bench_driver_riaksearch.erl\nCompiled src/riak_search_op_term.erl\nCompiled src/riak_search_op_range_sized.erl\nCompiled src/riak_solr_xml.erl\nCompiled src/riak_search_kv_raw_extractor.erl\nCompiled src/riak_search_op_proximity.erl\nCompiled src/riak_search_op_negation.erl\nCompiled src/riak_search_op.erl\nCompiled src/riak_search_ets_backend.erl\nCompiled src/riak_solr_sort.erl\nCompiled src/riak_search_kv_hook.erl\nCompiled src/riak_search_vnode.erl\nCompiled src/riak_search_basic_qc.erl\nCompiled src/riak_solr_xml_xform.erl\nCompiled src/riak_search_schema.erl\nCompiled src/riak_search_kv_erlang_extractor.erl\nCompiled src/riak_search_op_group.erl\nCompiled src/riak_search_operators_qc.erl\nCompiled src/riak_search_kv_xml_extractor.erl\nCompiled src/riak_search_config.erl\nCompiled src/riak_search_kv_erlang_binary_extractor.erl\nCompiled src/riak_solr_qc.erl\nCompiled src/riak_search_ring_utils.erl\nCompiled src/riak_search_op_union.erl\nCompiled src/riak_search_cmd.erl\nCompiled src/riak_search_op_intersection.erl\nCompiled src/riak_solr_indexer_wm.erl\nCompiled src/merge_index_backend.erl\nCompiled src/riak_search_kv_extractor.erl\nCompiled src/riak_search_dir_indexer.erl\nCompiled src/solr_search.erl\nCompiled src/riak_search_client.erl\nCompiled src/search.erl\nCompiled src/riak_search_pb_query.erl\nCompiled src/riak_search_schema_parser.erl\nCompiled src/riak_search_op_scope.erl\nCompiled src/riak_search_worker.erl\nCompiled src/riak_search_op_node.erl\nCompiled src/riak_search.erl\nCompiled src/riak_search_inlines.erl\nCompiled src/riak_search_op_range.erl\nCompiled src/riak_search_op_mockterm.erl\nCompiled src/riak_solr_output.erl\nCompiled src/riak_search_test.erl\nCompiled src/riak_solr_searcher_wm.erl\nCompiled src/riak_search_vnode_sup.erl\nCompiled src/riak_search_op_string.erl\nCompiled src/riak_search_cinfo.erl\nCompiled src/text_analyzers.erl\nCompiled src/riak_solr_search_client.erl\nCompiled src/riak_search_op_utils.erl\nCompiled src/riak_search_sup.erl\nCompiled src/riak_search_op_range_worker.erl\nCompiled src/riak_indexed_doc.erl\nCompiled src/riak_search_repl_helper.erl\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> erlydtl (compile)\nCompiled src/erlydtl_parser.yrl\nCompiled src/erlydtl_parser.erl\nCompiled src/erlydtl_i18n.erl\nCompiled src/erlydtl_contrib_humanize.erl\nCompiled src/i18n/po_generator.erl\nCompiled src/i18n/sources_parser.erl\nCompiled src/i18n/blocktrans_extractor.erl\nCompiled src/erlydtl_runtime.erl\nCompiled src/i18n/blocktrans_parser.erl\nCompiled src/i18n/i18n_manager.erl\nCompiled src/i18n/blocktrans_scanner.erl\nCompiled src/i18n/po_scanner.erl\nCompiled src/erlydtl_deps.erl\nCompiled src/erlydtl.erl\nCompiled src/filter_lib/erlydtl_dateformat.erl\nCompiled src/erlydtl_unparser.erl\nCompiled src/filter_lib/erlydtl_slice.erl\nCompiled src/erlydtl_filters.erl\nCompiled src/erlydtl_scanner.erl\nCompiled src/erlydtl_compiler.erl\n==> riak_control (compile)\nCompiled src/riak_control_app.erl\nCompiled src/riak_control_security.erl\nCompiled src/riak_control_wm_gui.erl\nCompiled src/riak_control_sup.erl\nCompiled src/riak_control_wm_cluster.erl\nCompiled src/riak_control_ring.erl\nCompiled src/riak_control.erl\nCompiled src/riak_control_wm_nodes.erl\nCompiled src/riak_control_session.erl\nCompiled src/riak_control_routes.erl\nCompiled src/riak_control_wm_partitions.erl\n:0: Warning: function render_tag/3 is unused\n:0: Warning: variable 'TagName' is unused\nCompiled templates/index.dtl\nCompiled handlebars asset priv/admin/js/generated/templates.js\nBuilt asset priv/admin/js/generated/vendor.js\nWARN:  Bypassing stylesheet processing of priv/admin/css: stylus missing.\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> riaknostic (compile)\nCompiled src/riaknostic_check.erl\nCompiled src/riaknostic.erl\nCompiled src/riaknostic_check_ring_preflists.erl\nCompiled src/riaknostic_check_strong_consistency.erl\nCompiled src/riaknostic_util.erl\nCompiled src/riaknostic_node.erl\nCompiled src/riaknostic_check_monitors.erl\nCompiled src/riaknostic_check_memory_use.erl\nCompiled src/riaknostic_check_ring_membership.erl\nCompiled src/riaknostic_check_search.erl\nCompiled src/riaknostic_check_nodes_connected.erl\nCompiled src/riaknostic_check_ring_size.erl\nCompiled src/riaknostic_check_dumps.erl\nCompiled src/riaknostic_check_disk.erl\nCompiled src/riaknostic_config.erl\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> kvc (compile)\nCompiled src/kvc.erl\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> ibrowse (compile)\nCompiled src/ibrowse_lib.erl\nCompiled src/ibrowse_app.erl\nCompiled src/ibrowse_socks5.erl\nCompiled src/ibrowse_lb.erl\nCompiled src/ibrowse_sup.erl\nCompiled src/ibrowse.erl\nCompiled src/ibrowse_http_client.erl\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> fuse (compile)\nCompiled src/fuse_stats_plugin.erl\nCompiled src/fuse_app.erl\nCompiled src/fuse_stats_folsom.erl\nCompiled src/fuse_event.erl\nCompiled src/fuse_stats_exometer.erl\nCompiled src/fuse_time.erl\nCompiled src/fuse_stats_ets.erl\nCompiled src/fuse_sup.erl\nCompiled src/fuse.erl\nCompiled src/fuse_monitor.erl\nCompiled src/fuse_server.erl\n==> yokozuna (compile)\nCreate dir ../build\nUsing cached copy of Solr /var/tmp/yokozuna/solr-4.10.4-yz-2.tgz\nCreating Solr dir ../priv/solr\nSolr dir created successfully\nDownloading yokozuna-3.jar\n--2017-01-11 11:31:20--  http://s3.amazonaws.com/files.basho.com/yokozuna/yokozuna-3.jar\nResolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.32.123\nConnecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.32.123|:80... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 11261 (11K) [application/java-archive]\nSaving to: \u2018yokozuna-3.jar\u2019\n\n     0K                                                      100% 29.6M=0s\n\n2017-01-11 11:31:20 (29.6 MB/s) - \u2018yokozuna-3.jar\u2019 saved [11261/11261]\n\nDownloading yz_monitor-1.jar\n--2017-01-11 11:31:20--  http://s3.amazonaws.com/files.basho.com/yokozuna/yz_monitor-1.jar\nResolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.32.123\nConnecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.32.123|:80... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 2661 (2.6K) [application/java-archive]\nSaving to: \u2018yz_monitor-1.jar\u2019\n\n     0K                                                      100% 13.5M=0s\n\n2017-01-11 11:31:20 (13.5 MB/s) - \u2018yz_monitor-1.jar\u2019 saved [2661/2661]\n\nCompiled src/yz_extractor.erl\nCompiled src/yz_pb_search.erl\nCompiled src/yz_stat.erl\nCompiled src/yz_wm_search.erl\nCompiled src/yz_misc.erl\nCompiled src/yz_wm_extract.erl\nCompiled src/yz_kv.erl\nCompiled src/yz_stat_worker.erl\nCompiled src/yz_solrq_sup.erl\nCompiled src/yz_solrq_drain_mgr.erl\nCompiled src/yz_wm_schema.erl\nCompiled src/yz_sup.erl\nCompiled src/yz_doc.erl\nCompiled src/yz_xml_extractor.erl\nCompiled src/yz_general_sup.erl\nCompiled src/yz_solr_sup.erl\nCompiled src/yz_bucket_validator.erl\nCompiled src/rt_intercept_pt.erl\nCompiled src/yz_entropy_mgr.erl\nCompiled src/yz_entropy.erl\nCompiled src/yz_console.erl\nCompiled src/yz_app.erl\nCompiled src/yz_solrq.erl\nCompiled src/yz_wm_index.erl\nCompiled src/yokozuna.erl\nCompiled src/yz_fuse_stats_sidejob.erl\nCompiled src/yz_rs_migration.erl\nCompiled src/yz_noop_extractor.erl\nCompiled src/yz_solr_proc.erl\nCompiled src/yz_schema.erl\nCompiled src/yz_index.erl\nCompiled src/yz_exchange_fsm.erl\nCompiled src/yz_fuse.erl\nCompiled src/yz_events.erl\nCompiled src/yz_solrq_helper.erl\nCompiled src/yz_json_extractor.erl\nCompiled src/yz_pb_admin.erl\nCompiled src/yz_index_hashtree.erl\nCompiled src/yz_solrq_queue_pair_sup.erl\nCompiled src/yz_solrq_worker.erl\nCompiled src/yz_diag.erl\nCompiled src/yz_text_extractor.erl\nCompiled src/yz_solr.erl\nCompiled src/yz_cover.erl\nCompiled src/yz_dt_extractor.erl\nCompiled src/yz_index_hashtree_sup.erl\nCompiled src/yz_solrq_drain_fsm.erl\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> canola (compile)\nCompiled src/canola.erl\nCompiling c_src/canola-port.c\n==> riak_auth_mods (compile)\nCompiled src/riak_auth_mods_pam.erl\nCompiled src/riak_auth_mods_app.erl\nCompiled src/riak_auth_mods_sup.erl\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> rebar_lock_deps_plugin (compile)\nCompiled src/rldp_util.erl\nCompiled src/rldp_change_log.erl\nCompiled src/rebar_lock_deps_plugin.erl\n==> rel (compile)\n==> riak (compile)\nCompiled src/etop_txt.erl\n==> develop.eea8277.fd-deps-cleanup.a647b0f (compile)\n\n\n\n\n```\n\n--------------------------------------------------\n\n\n\n:white_check_mark: MAKE_REL \n\n\n> Started at: 2017-01-11 11:31\n> Duration: 32 seconds.\n> Result:  OK\n> Message: OK\n> Exit Code:  0\n\n> :page_facing_up:\n\n\n```\n\ncd /tmp/thumbs/develop.eea8277.fd-deps-cleanup.a647b0f; make rel\n\n\n\n\n  ./rebar get-deps\n==> node_package (get-deps)\n==> goldrush (get-deps)\n==> lager (get-deps)\n==> syslog (get-deps)\n==> lager_syslog (get-deps)\n==> cluster_info (get-deps)\n==> sidejob (get-deps)\n==> erlang_js (get-deps)\n==> meck (get-deps)\n==> getopt (get-deps)\n==> neotoma (get-deps)\n==> cuttlefish (get-deps)\n==> bitcask (get-deps)\n==> eper (get-deps)\n==> edown (get-deps)\n==> sext (get-deps)\n==> poolboy (get-deps)\n==> basho_stats (get-deps)\n==> riak_sysmon (get-deps)\n==> eleveldb (get-deps)\n==> riak_ensemble (get-deps)\n==> pbkdf2 (get-deps)\n==> parse_trans (get-deps)\n==> bear (get-deps)\n==> folsom (get-deps)\n==> setup (get-deps)\n==> src (get-deps)\n==> exometer_core (get-deps)\n==> clique (get-deps)\n==> riak_core (get-deps)\n==> riak_pipe (get-deps)\n==> riak_dt (get-deps)\n==> eunit_formatters (get-deps)\n==> hamcrest (get-deps)\n==> riak_pb (get-deps)\n==> mochiweb (get-deps)\n==> webmachine (get-deps)\n==> riak_api (get-deps)\n==> proper (get-deps)\n==> stdlib2 (get-deps)\n==> hyper (get-deps)\n==> gproc (get-deps)\n==> chronos (get-deps)\n==> riak_kv (get-deps)\n==> merge_index (get-deps)\n==> lucene_parser (get-deps)\n==> riak_search (get-deps)\n==> erlydtl (get-deps)\n==> riak_control (get-deps)\n==> riaknostic (get-deps)\n==> kvc (get-deps)\n==> ibrowse (get-deps)\n==> fuse (get-deps)\n==> yokozuna (get-deps)\n==> canola (get-deps)\n==> riak_auth_mods (get-deps)\n==> rebar_lock_deps_plugin (get-deps)\n==> rel (get-deps)\n==> riak (get-deps)\n==> develop.eea8277.fd-deps-cleanup.a647b0f (get-deps)\n./rebar compile\n==> node_package (compile)\n==> goldrush (compile)\n==> lager (compile)\n==> syslog (compile)\n==> lager_syslog (compile)\n==> cluster_info (compile)\n==> sidejob (compile)\n==> erlang_js (compile)\nmake[1]: Entering directory `/tmp/thumbs/develop.eea8277.fd-deps-cleanup.a647b0f/deps/erlang_js'\ncd c_src; make\nmake[2]: Entering directory `/tmp/thumbs/develop.eea8277.fd-deps-cleanup.a647b0f/deps/erlang_js/c_src'\nmake[2]: Nothing to be done for `js'.\nmake[2]: Leaving directory `/tmp/thumbs/develop.eea8277.fd-deps-cleanup.a647b0f/deps/erlang_js/c_src'\nmake[1]: Leaving directory `/tmp/thumbs/develop.eea8277.fd-deps-cleanup.a647b0f/deps/erlang_js'\nCompiling c_src/spidermonkey.c\nCompiling c_src/spidermonkey_drv.c\nc_src/spidermonkey_drv.c: In function \u2018send_immediate_ok_response\u2019:\nc_src/spidermonkey_drv.c:82:3: warning: \u2018driver_output_term\u2019 is deprecated (declared at /opt/basho/installs/erlang/R16B02-basho10DEV/erts-5.10.3/include/erl_driver.h:622) [-Wdeprecated-declarations]\n   driver_output_term(dd->port, terms, sizeof(terms) / sizeof(terms[0]));\n   ^\nc_src/spidermonkey_drv.c: In function \u2018ready_async\u2019:\nc_src/spidermonkey_drv.c:256:3: warning: \u2018driver_output_term\u2019 is deprecated (declared at /opt/basho/installs/erlang/R16B02-basho10DEV/erts-5.10.3/include/erl_driver.h:622) [-Wdeprecated-declarations]\n   driver_output_term(dd->port,\n   ^\n==> meck (compile)\n==> getopt (compile)\n==> neotoma (compile)\n==> cuttlefish (compile)\n==> getopt (escriptize)\n==> goldrush (escriptize)\n==> lager (escriptize)\n==> neotoma (escriptize)\n==> cuttlefish (escriptize)\n==> bitcask (compile)\n==> eper (compile)\n==> edown (compile)\n==> sext (compile)\n==> poolboy (compile)\n==> basho_stats (compile)\n==> riak_sysmon (compile)\n==> eleveldb (compile)\nmake[1]: Entering directory `/tmp/thumbs/develop.eea8277.fd-deps-cleanup.a647b0f/deps/eleveldb/c_src/leveldb'\nmake[1]: Nothing to be done for `all'.\nmake[1]: Leaving directory `/tmp/thumbs/develop.eea8277.fd-deps-cleanup.a647b0f/deps/eleveldb/c_src/leveldb'\nmake[1]: Entering directory `/tmp/thumbs/develop.eea8277.fd-deps-cleanup.a647b0f/deps/eleveldb/c_src/leveldb'\nmake[1]: Nothing to be done for `tools'.\nmake[1]: Leaving directory `/tmp/thumbs/develop.eea8277.fd-deps-cleanup.a647b0f/deps/eleveldb/c_src/leveldb'\n==> riak_ensemble (compile)\n==> pbkdf2 (compile)\n==> parse_trans (compile)\n==> bear (compile)\n==> folsom (compile)\n==> setup (compile)\n==> edown (escriptize)\n==> setup (escriptize)\n==> src (compile)\n==> exometer_core (compile)\n==> clique (compile)\n==> riak_core (compile)\n==> riak_pipe (compile)\n==> riak_dt (compile)\n==> eunit_formatters (compile)\n==> hamcrest (compile)\n==> hamcrest (post_compile)\n==> riak_pb (compile)\n==> mochiweb (compile)\n==> webmachine (compile)\n==> riak_api (compile)\n==> proper (compile)\nmake[1]: Entering directory `/tmp/thumbs/develop.eea8277.fd-deps-cleanup.a647b0f/deps/proper'\nmake[1]: `include/compile_flags.hrl' is up to date.\nmake[1]: Leaving directory `/tmp/thumbs/develop.eea8277.fd-deps-cleanup.a647b0f/deps/proper'\n==> stdlib2 (compile)\n==> hyper (compile)\n==> gproc (compile)\n==> chronos (compile)\n==> riak_kv (compile)\n==> merge_index (compile)\n==> lucene_parser (compile)\n==> riak_search (compile)\n==> erlydtl (compile)\n==> riak_control (compile)\nCompiled handlebars asset priv/admin/js/generated/templates.js\nWARN:  Bypassing stylesheet processing of priv/admin/css: stylus missing.\n==> riaknostic (compile)\n==> kvc (compile)\n==> ibrowse (compile)\n==> fuse (compile)\n==> yokozuna (compile)\n==> canola (compile)\n==> riak_auth_mods (compile)\n==> rebar_lock_deps_plugin (compile)\n==> rel (compile)\n==> riak (compile)\n==> develop.eea8277.fd-deps-cleanup.a647b0f (compile)\n./rebar generate \n==> rel (generate)\nSchema: [\"/tmp/thumbs/develop.eea8277.fd-deps-cleanup.a647b0f/rel/riak/lib/10-riak.schema\",\n         \"/tmp/thumbs/develop.eea8277.fd-deps-cleanup.a647b0f/rel/riak/lib/11-erlang_vm.schema\",\n         \"/tmp/thumbs/develop.eea8277.fd-deps-cleanup.a647b0f/rel/riak/lib/12-riak_core.schema\",\n         \"/tmp/thumbs/develop.eea8277.fd-deps-cleanup.a647b0f/rel/riak/lib/13-riak_api.schema\",\n         \"/tmp/thumbs/develop.eea8277.fd-deps-cleanup.a647b0f/rel/riak/lib/14-riak_kv.schema\",\n         \"/tmp/thumbs/develop.eea8277.fd-deps-cleanup.a647b0f/rel/riak/lib/15-riak_sysmon.schema\",\n         \"/tmp/thumbs/develop.eea8277.fd-deps-cleanup.a647b0f/rel/riak/lib/16-bitcask.schema\",\n         \"/tmp/thumbs/develop.eea8277.fd-deps-cleanup.a647b0f/rel/riak/lib/17-bitcask_multi.schema\",\n         \"/tmp/thumbs/develop.eea8277.fd-deps-cleanup.a647b0f/rel/riak/lib/18-riak_control.schema\",\n         \"/tmp/thumbs/develop.eea8277.fd-deps-cleanup.a647b0f/rel/riak/lib/20-multi_backend.schema\",\n         \"/tmp/thumbs/develop.eea8277.fd-deps-cleanup.a647b0f/rel/riak/lib/21-leveldb.schema\",\n         \"/tmp/thumbs/develop.eea8277.fd-deps-cleanup.a647b0f/rel/riak/lib/22-leveldb_multi.schema\",\n         \"/tmp/thumbs/develop.eea8277.fd-deps-cleanup.a647b0f/rel/riak/lib/30-yokozuna.schema\"]\nWARN:  'generate' command does not apply to directory /tmp/thumbs/develop.eea8277.fd-deps-cleanup.a647b0f\n\n\n\n\n```\n\n--------------------------------------------------\n\n\n\n:white_large_square: 0 of 1 Code reviews from organization basho\n\n\n. |  |  | |  |\n| ------------ | -------------|------------ | ------------- |\nfd-deps-cleanup a647b0f | :arrow_right: | develop fbc4c25 | :white_check_mark: completed\nLooks good!  :+1: \n\n:white_check_mark: MERGE \n\n\n> Started at: 2017-01-27 10:31\n> Duration: 2 seconds.\n> Result:  OK\n> Message: Merge Success: fd-deps-cleanup a647b0f6716477d282b719fde981a28d81c42849 onto target branch: develop fbc4c256773daa9b963d0d8151cec0d18c78e0a4\n> Exit Code:  OK\n\n> :page_facing_up:\n\n\n```\n\n\n\n\n\n\n  Merge made by the 'recursive' strategy.\n .thumbs.yml  |  8 ++++++++\n rebar.config | 22 +++++++++++-----------\n 2 files changed, 19 insertions(+), 11 deletions(-)\n create mode 100644 .thumbs.yml\n\n\n\n```\n\n--------------------------------------------------\n\n\n\n:white_check_mark: MAKE_DEPS \n\n\n> Started at: 2017-01-27 10:31\n> Duration: 42 seconds.\n> Result:  OK\n> Message: OK\n> Exit Code:  0\n\n> :page_facing_up:\n\n\n```\n\ncd /tmp/thumbs/develop.fbc4c25.fd-deps-cleanup.a647b0f; make deps\n\n\n\n\n\n\n... Snipped 47 lines ...\nsyslog from {git,\"https://github.com/basho/erlang-syslog\",\n                         {tag,\"1.0.3\"}}\nCloning into 'syslog'...\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> lager (get-deps)\nPulling goldrush from {git,\"https://github.com/basho/goldrush.git\",\n                           {tag,\"0.1.9\"}}\nCloning into 'goldrush'...\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> goldrush (get-deps)\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> syslog (get-deps)\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> cluster_info (get-deps)\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> riak_kv (get-deps)\nPulling sidejob from {git,\"https://github.com/basho/sidejob.git\",\n                          {tag,\"2.0.1\"}}\nCloning into 'sidejob'...\nPulling erlang_js from {git,\"https://github.com/basho/erlang_js.git\",\n                            {tag,\"1.3.0\"}}\nCloning into 'erlang_js'...\nPulling bitcask from {git,\"https://github.com/basho/bitcask.git\",\n                          {tag,\"2.0.6\"}}\nCloning into 'bitcask'...\nPulling eper from {git,\"https://github.com/basho/eper.git\",{tag,\"0.78\"}}\nCloning into 'eper'...\nPulling sext from {git,\"https://github.com/basho/sext.git\",{tag,\"1.1p3\"}}\nCloning into 'sext'...\nPulling riak_pipe from {git,\"https://github.com/basho/riak_pipe.git\",\n                            {branch,\"develop\"}}\nCloning into 'riak_pipe'...\nPulling riak_dt from {git,\"https://github.com/basho/riak_dt.git\",\n                          {branch,\"develop\"}}\nCloning into 'riak_dt'...\nPulling eunit_formatters from {git,\"https://github.com/basho/eunit_formatters\",\n                                   {tag,\"0.1.2\"}}\nCloning into 'eunit_formatters'...\nPulling riak_api from {git,\"https://github.com/basho/riak_api.git\",\n                           {branch,\"develop\"}}\nCloning into 'riak_api'...\nPulling hyper from {git,\"https://github.com/basho/hyper\",{tag,\"1.0.0\"}}\nCloning into 'hyper'...\nPulling clique from {git,\"https://github.com/basho/clique.git\",{tag,\"0.3.7\"}}\nCloning into 'clique'...\nPulling chronos from {git,\"https://github.com/basho/chronos.git\",\n                          {tag,\"0.1.4-basho1\"}}\nCloning into 'chronos'...\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> sidejob (get-deps)\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> erlang_js (get-deps)\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> meck (get-deps)\n==> bitcask (get-deps)\nPulling cuttlefish from {git,\"https://github.com/basho/cuttlefish.git\",\n                             {tag,\"2.0.10\"}}\nCloning into 'cuttlefish'...\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> cuttlefish (get-deps)\nPulling getopt from {git,\"https://github.com/basho/getopt.git\",{tag,\"v0.8.2\"}}\nCloning into 'getopt'...\nPulling neotoma from {git,\"https://github.com/basho/neotoma.git\",\n                          {tag,\"1.7.3\"}}\nCloning into 'neotoma'...\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> getopt (get-deps)\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> neotoma (get-deps)\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> eper (get-deps)\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> sext (get-deps)\nPulling edown from {git,\"git://github.com/uwiger/edown.git\",{tag,\"0.5\"}}\nCloning into 'edown'...\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> edown (get-deps)\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> riak_pipe (get-deps)\nPulling riak_core from {git,\"git://github.com/basho/riak_core.git\",\n                            {branch,\"develop\"}}\nCloning into 'riak_core'...\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> clique (get-deps)\n==> riak_core (get-deps)\nPulling poolboy from {git,\"git://github.com/basho/poolboy.git\",\n                          {tag,\"0.8.1p3\"}}\nCloning into 'poolboy'...\nPulling basho_stats from {git,\"git://github.com/basho/basho_stats.git\",\n                              {tag,\"1.0.3\"}}\nCloning into 'basho_stats'...\nPulling riak_sysmon from {git,\"git://github.com/basho/riak_sysmon.git\",\n                              {tag,\"2.1.4\"}}\nCloning into 'riak_sysmon'...\nPulling eleveldb from {git,\"git://github.com/basho/eleveldb.git\",\n                           {tag,\"2.0.32\"}}\nCloning into 'eleveldb'...\nPulling riak_ensemble from {git,\"git://github.com/basho/riak_ensemble\",\n                                {tag,\"2.1.6\"}}\nCloning into 'riak_ensemble'...\nPulling pbkdf2 from {git,\"git://github.com/basho/erlang-pbkdf2.git\",\n                         {tag,\"2.0.0\"}}\nCloning into 'pbkdf2'...\nPulling exometer_core from {git,\"git://github.com/basho/exometer_core.git\",\n                                {tag,\"1.0.0-basho9\"}}\nCloning into 'exometer_core'...\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> poolboy (get-deps)\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> basho_stats (get-deps)\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> riak_sysmon (get-deps)\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> eleveldb (get-deps)\nCloning into 'leveldb'...\nNote: checking out '2.0.31'.\n\nYou are in 'detached HEAD' state. You can look around, make experimental\nchanges and commit them, and you can discard any commits you make in this\nstate without impacting any branches by performing another checkout.\n\nIf you want to create a new branch to retain commits you create, you may\ndo so (now or later) by using -b with the checkout command again. Example:\n\n  git checkout -b new_branch_name\n\nHEAD is now at e6a2939... add (int) cast to fix compile errors on 10 of 16 platforms ... argh.\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> riak_ensemble (get-deps)\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> pbkdf2 (get-deps)\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> src (get-deps)\n==> exometer_core (get-deps)\nPulling parse_trans from {git,\"git://github.com/basho/parse_trans.git\",\n                              {tag,\"2.9.2p1\"}}\nCloning into 'parse_trans'...\nPulling folsom from {git,\"git://github.com/basho/folsom.git\",{tag,\"0.7.4p5\"}}\nCloning into 'folsom'...\nPulling setup from {git,\"git://github.com/basho/setup.git\",{tag,\"1.4\"}}\nCloning into 'setup'...\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> parse_trans (get-deps)\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> folsom (get-deps)\nPulling bear from {git,\"git://github.com/basho/bear.git\",{tag,\"0.1.3p1\"}}\nCloning into 'bear'...\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> bear (get-deps)\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> setup (get-deps)\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> riak_dt (get-deps)\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> eunit_formatters (get-deps)\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> riak_api (get-deps)\nPulling riak_pb from {git,\"https://github.com/basho/riak_pb.git\",\n                          {branch,\"develop\"}}\nCloning into 'riak_pb'...\nPulling webmachine from {git,\"https://github.com/basho/webmachine.git\",\n                             {tag,\"1.10.8-basho1\"}}\nCloning into 'webmachine'...\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> riak_pb (get-deps)\nPulling hamcrest from {git,\"https://github.com/basho/hamcrest-erlang.git\",\n                           {tag,\"0.3.0-basho\"}}\nCloning into 'hamcrest'...\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> hamcrest (get-deps)\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> webmachine (get-deps)\nPulling mochiweb from {git,\"git://github.com/basho/mochiweb.git\",\n                           {tag,\"v2.9.0p2\"}}\nCloning into 'mochiweb'...\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> mochiweb (get-deps)\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> hyper (get-deps)\nPulling proper from {git,\"https://github.com/basho/proper.git\",{tag,\"v1.2p1\"}}\nCloning into 'proper'...\nPulling stdlib2 from {git,\"https://github.com/basho/stdlib2.git\",\n                          {tag,\"0.0.1\"}}\nCloning into 'stdlib2'...\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> proper (get-deps)\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> stdlib2 (get-deps)\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> chronos (get-deps)\nPulling gproc from {git,\"https://github.com/basho/gproc.git\",\"master\"}\nCloning into 'gproc'...\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> gproc (get-deps)\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> lucene_parser (get-deps)\n==> riak_search (get-deps)\nPulling merge_index from {git,\"git://github.com/basho/merge_index.git\",\n                              {tag,\"2.0.4\"}}\nCloning into 'merge_index'...\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> merge_index (get-deps)\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> riak_control (get-deps)\nPulling erlydtl from {git,\"git://github.com/basho/erlydtl.git\",\"d20b53f0\"}\nCloning into 'erlydtl'...\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> erlydtl (get-deps)\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> riaknostic (get-deps)\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> yokozuna (get-deps)\nPulling kvc from {git,\"https://github.com/basho/kvc.git\",{tag,\"v1.5.0\"}}\nCloning into 'kvc'...\nPulling ibrowse from {git,\"https://github.com/basho/ibrowse.git\",{tag,\"v4.3\"}}\nCloning into 'ibrowse'...\nPulling fuse from {git,\"https://github.com/basho/fuse.git\",{tag,\"v2.1.0\"}}\nCloning into 'fuse'...\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> kvc (get-deps)\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> ibrowse (get-deps)\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> fuse (get-deps)\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> riak_auth_mods (get-deps)\nPulling canola from {git,\"git://github.com/basho/canola.git\",{tag,\"2.0.0\"}}\nCloning into 'canola'...\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> canola (get-deps)\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> rebar_lock_deps_plugin (get-deps)\n\n\n\n\n```\n\n--------------------------------------------------\n\n\n\n:white_check_mark: MAKE \n\n\n> Started at: 2017-01-27 10:32\n> Duration: 92 seconds.\n> Result:  OK\n> Message: OK\n> Exit Code:  0\n\n> :page_facing_up:\n\n\n```\n\ncd /tmp/thumbs/develop.fbc4c25.fd-deps-cleanup.a647b0f; make\n\n\n\n\n\n\n... Snipped 2228 lines ...\n\nCompiled src/riak_kv_wm_keylist.erl\nCompiled priv/tracers/tracer_accumulating_time.erl\nCompiled priv/tracers/tracer_gc_latency.erl\nCompiled priv/tracers/tracer_backend_latency.erl\nCompiled priv/tracers/tracer_large4.erl\nCompiled src/riak_kv_vnode.erl\nCompiled priv/tracers/tracer_fsm_init.erl\nCompiled priv/tracers/tracer_latency_histogram.erl\nCompiled priv/tracers/tracer_timeit.erl\nCompiled priv/tracers/tracer_eleveldb_put_size.erl\nCompiled priv/tracers/tracer_read_bin_trace_file.erl\nCompiled priv/tracers/tracer_merge_and_and_handoff.erl\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> merge_index (compile)\nCompiled src/mi_buffer_converter.erl\nCompiled src/mi_bloom.erl\nCompiled src/mi_utils.erl\nCompiled src/mi_sup.erl\nCompiled src/mi_buffer.erl\nCompiled src/mi_app.erl\nCompiled src/mi_locks.erl\nCompiled src/merge_index.erl\nCompiled src/mi_buffer_converter_sup.erl\nCompiled src/mi_scheduler.erl\nCompiled src/mi_segment.erl\nCompiled src/mi_server.erl\nCompiled src/mi_segment_writer.erl\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> lucene_parser (compile)\nCompiled src/lucene_scan.xrl\nCompiled src/lucene_parse.yrl\nCompiled src/lucene_parse.erl\nCompiled src/lucene_scan.erl\nCompiled src/lucene_parser.erl\nCompiled src/lucene_scan_test.erl\nCompiled src/lucene_parse_test.erl\n==> riak_search (compile)\nCompiled src/riak_search_backend.erl\nCompiled src/riak_search_utils.erl\nCompiled src/riak_search_app.erl\nCompiled src/riak_search_kv_json_extractor.erl\nCompiled src/riak_search_stat.erl\nCompiled src/riak_search_op_term.erl\nCompiled src/basho_bench_driver_riaksearch.erl\nCompiled src/riak_search_op_range_sized.erl\nCompiled src/riak_solr_xml.erl\nCompiled src/riak_search_kv_raw_extractor.erl\nCompiled src/riak_search_op_proximity.erl\nCompiled src/riak_search_op_negation.erl\nCompiled src/riak_search_op.erl\nCompiled src/riak_search_ets_backend.erl\nCompiled src/riak_solr_sort.erl\nCompiled src/riak_search_kv_hook.erl\nCompiled src/riak_search_vnode.erl\nCompiled src/riak_search_basic_qc.erl\nCompiled src/riak_solr_xml_xform.erl\nCompiled src/riak_search_schema.erl\nCompiled src/riak_search_kv_erlang_extractor.erl\nCompiled src/riak_search_kv_xml_extractor.erl\nCompiled src/riak_search_op_group.erl\nCompiled src/riak_search_operators_qc.erl\nCompiled src/riak_search_config.erl\nCompiled src/riak_search_kv_erlang_binary_extractor.erl\nCompiled src/riak_solr_qc.erl\nCompiled src/riak_search_ring_utils.erl\nCompiled src/riak_search_op_union.erl\nCompiled src/riak_search_cmd.erl\nCompiled src/riak_search_op_intersection.erl\nCompiled src/riak_solr_indexer_wm.erl\nCompiled src/merge_index_backend.erl\nCompiled src/riak_search_dir_indexer.erl\nCompiled src/solr_search.erl\nCompiled src/riak_search_kv_extractor.erl\nCompiled src/riak_search_client.erl\nCompiled src/riak_search_pb_query.erl\nCompiled src/search.erl\nCompiled src/riak_search_op_scope.erl\nCompiled src/riak_search_schema_parser.erl\nCompiled src/riak_search_worker.erl\nCompiled src/riak_search_op_node.erl\nCompiled src/riak_search_inlines.erl\nCompiled src/riak_search.erl\nCompiled src/riak_search_op_range.erl\nCompiled src/riak_search_op_mockterm.erl\nCompiled src/riak_solr_output.erl\nCompiled src/riak_solr_searcher_wm.erl\nCompiled src/riak_search_test.erl\nCompiled src/riak_search_vnode_sup.erl\nCompiled src/riak_search_op_string.erl\nCompiled src/riak_search_cinfo.erl\nCompiled src/text_analyzers.erl\nCompiled src/riak_solr_search_client.erl\nCompiled src/riak_search_op_utils.erl\nCompiled src/riak_search_op_range_worker.erl\nCompiled src/riak_search_sup.erl\nCompiled src/riak_indexed_doc.erl\nCompiled src/riak_search_repl_helper.erl\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> erlydtl (compile)\nCompiled src/erlydtl_parser.yrl\nCompiled src/erlydtl_parser.erl\nCompiled src/erlydtl_i18n.erl\nCompiled src/erlydtl_contrib_humanize.erl\nCompiled src/i18n/po_generator.erl\nCompiled src/i18n/sources_parser.erl\nCompiled src/i18n/blocktrans_extractor.erl\nCompiled src/erlydtl_runtime.erl\nCompiled src/i18n/i18n_manager.erl\nCompiled src/i18n/blocktrans_parser.erl\nCompiled src/i18n/blocktrans_scanner.erl\nCompiled src/i18n/po_scanner.erl\nCompiled src/erlydtl_deps.erl\nCompiled src/erlydtl.erl\nCompiled src/erlydtl_unparser.erl\nCompiled src/filter_lib/erlydtl_dateformat.erl\nCompiled src/filter_lib/erlydtl_slice.erl\nCompiled src/erlydtl_filters.erl\nCompiled src/erlydtl_scanner.erl\nCompiled src/erlydtl_compiler.erl\n==> riak_control (compile)\nCompiled src/riak_control_app.erl\nCompiled src/riak_control_security.erl\nCompiled src/riak_control_wm_gui.erl\nCompiled src/riak_control_sup.erl\nCompiled src/riak_control_wm_cluster.erl\nCompiled src/riak_control_ring.erl\nCompiled src/riak_control_wm_nodes.erl\nCompiled src/riak_control_session.erl\nCompiled src/riak_control.erl\nCompiled src/riak_control_routes.erl\nCompiled src/riak_control_wm_partitions.erl\n:0: Warning: function render_tag/3 is unused\n:0: Warning: variable 'TagName' is unused\nCompiled templates/index.dtl\nCompiled handlebars asset priv/admin/js/generated/templates.js\nBuilt asset priv/admin/js/generated/vendor.js\nWARN:  Bypassing stylesheet processing of priv/admin/css: stylus missing.\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> riaknostic (compile)\nCompiled src/riaknostic_check.erl\nCompiled src/riaknostic.erl\nCompiled src/riaknostic_check_ring_preflists.erl\nCompiled src/riaknostic_check_strong_consistency.erl\nCompiled src/riaknostic_node.erl\nCompiled src/riaknostic_util.erl\nCompiled src/riaknostic_check_monitors.erl\nCompiled src/riaknostic_check_ring_membership.erl\nCompiled src/riaknostic_check_memory_use.erl\nCompiled src/riaknostic_check_search.erl\nCompiled src/riaknostic_check_nodes_connected.erl\nCompiled src/riaknostic_check_dumps.erl\nCompiled src/riaknostic_check_ring_size.erl\nCompiled src/riaknostic_check_disk.erl\nCompiled src/riaknostic_config.erl\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> kvc (compile)\nCompiled src/kvc.erl\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> ibrowse (compile)\nCompiled src/ibrowse_lib.erl\nCompiled src/ibrowse_app.erl\nCompiled src/ibrowse_socks5.erl\nCompiled src/ibrowse_lb.erl\nCompiled src/ibrowse_sup.erl\nCompiled src/ibrowse.erl\nCompiled src/ibrowse_http_client.erl\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> fuse (compile)\nCompiled src/fuse_stats_plugin.erl\nCompiled src/fuse_app.erl\nCompiled src/fuse_stats_folsom.erl\nCompiled src/fuse_event.erl\nCompiled src/fuse_stats_exometer.erl\nCompiled src/fuse_time.erl\nCompiled src/fuse_stats_ets.erl\nCompiled src/fuse_sup.erl\nCompiled src/fuse_monitor.erl\nCompiled src/fuse.erl\nCompiled src/fuse_server.erl\n==> yokozuna (compile)\nCreate dir ../build\nUsing cached copy of Solr /var/tmp/yokozuna/solr-4.10.4-yz-2.tgz\nCreating Solr dir ../priv/solr\nSolr dir created successfully\nDownloading yokozuna-3.jar\n--2017-01-27 10:33:52--  http://s3.amazonaws.com/files.basho.com/yokozuna/yokozuna-3.jar\nResolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.16.227\nConnecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.16.227|:80... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 11261 (11K) [application/java-archive]\nSaving to: \u2018yokozuna-3.jar\u2019\n\n     0K                                                      100% 33.4M=0s\n\n2017-01-27 10:33:52 (33.4 MB/s) - \u2018yokozuna-3.jar\u2019 saved [11261/11261]\n\nDownloading yz_monitor-1.jar\n--2017-01-27 10:33:52--  http://s3.amazonaws.com/files.basho.com/yokozuna/yz_monitor-1.jar\nResolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.16.227\nConnecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.16.227|:80... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 2661 (2.6K) [application/java-archive]\nSaving to: \u2018yz_monitor-1.jar\u2019\n\n     0K                                                      100% 13.5M=0s\n\n2017-01-27 10:33:52 (13.5 MB/s) - \u2018yz_monitor-1.jar\u2019 saved [2661/2661]\n\nCompiled src/yz_extractor.erl\nCompiled src/yz_pb_search.erl\nCompiled src/yz_stat.erl\nCompiled src/yz_wm_search.erl\nCompiled src/yz_misc.erl\nCompiled src/yz_wm_extract.erl\nCompiled src/yz_kv.erl\nCompiled src/yz_stat_worker.erl\nCompiled src/yz_solrq_sup.erl\nCompiled src/yz_solrq_drain_mgr.erl\nCompiled src/yz_wm_schema.erl\nCompiled src/yz_sup.erl\nCompiled src/yz_doc.erl\nCompiled src/yz_general_sup.erl\nCompiled src/yz_xml_extractor.erl\nCompiled src/yz_solr_sup.erl\nCompiled src/yz_bucket_validator.erl\nCompiled src/rt_intercept_pt.erl\nCompiled src/yz_entropy_mgr.erl\nCompiled src/yz_entropy.erl\nCompiled src/yz_console.erl\nCompiled src/yz_app.erl\nCompiled src/yz_solrq.erl\nCompiled src/yz_wm_index.erl\nCompiled src/yokozuna.erl\nCompiled src/yz_fuse_stats_sidejob.erl\nCompiled src/yz_rs_migration.erl\nCompiled src/yz_noop_extractor.erl\nCompiled src/yz_solr_proc.erl\nCompiled src/yz_schema.erl\nCompiled src/yz_index.erl\nCompiled src/yz_fuse.erl\nCompiled src/yz_exchange_fsm.erl\nCompiled src/yz_events.erl\nCompiled src/yz_solrq_helper.erl\nCompiled src/yz_json_extractor.erl\nCompiled src/yz_pb_admin.erl\nCompiled src/yz_index_hashtree.erl\nCompiled src/yz_solrq_queue_pair_sup.erl\nCompiled src/yz_solrq_worker.erl\nCompiled src/yz_diag.erl\nCompiled src/yz_text_extractor.erl\nCompiled src/yz_solr.erl\nCompiled src/yz_cover.erl\nCompiled src/yz_dt_extractor.erl\nCompiled src/yz_solrq_drain_fsm.erl\nCompiled src/yz_index_hashtree_sup.erl\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> canola (compile)\nCompiled src/canola.erl\nCompiling c_src/canola-port.c\n==> riak_auth_mods (compile)\nCompiled src/riak_auth_mods_pam.erl\nCompiled src/riak_auth_mods_app.erl\nCompiled src/riak_auth_mods_sup.erl\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> rebar_lock_deps_plugin (compile)\nCompiled src/rldp_util.erl\nCompiled src/rldp_change_log.erl\nCompiled src/rebar_lock_deps_plugin.erl\n==> rel (compile)\n==> riak (compile)\nCompiled src/etop_txt.erl\n==> develop.fbc4c25.fd-deps-cleanup.a647b0f (compile)\n\n\n\n\n```\n\n--------------------------------------------------\n\n\n\n:white_check_mark: MAKE_REL \n\n\n> Started at: 2017-01-27 10:33\n> Duration: 28 seconds.\n> Result:  OK\n> Message: OK\n> Exit Code:  0\n\n> :page_facing_up:\n\n\n```\n\ncd /tmp/thumbs/develop.fbc4c25.fd-deps-cleanup.a647b0f; make rel\n\n\n\n\n  ./rebar get-deps\n==> node_package (get-deps)\n==> goldrush (get-deps)\n==> lager (get-deps)\n==> syslog (get-deps)\n==> lager_syslog (get-deps)\n==> cluster_info (get-deps)\n==> sidejob (get-deps)\n==> erlang_js (get-deps)\n==> meck (get-deps)\n==> getopt (get-deps)\n==> neotoma (get-deps)\n==> cuttlefish (get-deps)\n==> bitcask (get-deps)\n==> eper (get-deps)\n==> edown (get-deps)\n==> sext (get-deps)\n==> poolboy (get-deps)\n==> basho_stats (get-deps)\n==> riak_sysmon (get-deps)\n==> eleveldb (get-deps)\n==> riak_ensemble (get-deps)\n==> pbkdf2 (get-deps)\n==> parse_trans (get-deps)\n==> bear (get-deps)\n==> folsom (get-deps)\n==> setup (get-deps)\n==> src (get-deps)\n==> exometer_core (get-deps)\n==> clique (get-deps)\n==> riak_core (get-deps)\n==> riak_pipe (get-deps)\n==> riak_dt (get-deps)\n==> eunit_formatters (get-deps)\n==> hamcrest (get-deps)\n==> riak_pb (get-deps)\n==> mochiweb (get-deps)\n==> webmachine (get-deps)\n==> riak_api (get-deps)\n==> proper (get-deps)\n==> stdlib2 (get-deps)\n==> hyper (get-deps)\n==> gproc (get-deps)\n==> chronos (get-deps)\n==> riak_kv (get-deps)\n==> merge_index (get-deps)\n==> lucene_parser (get-deps)\n==> riak_search (get-deps)\n==> erlydtl (get-deps)\n==> riak_control (get-deps)\n==> riaknostic (get-deps)\n==> kvc (get-deps)\n==> ibrowse (get-deps)\n==> fuse (get-deps)\n==> yokozuna (get-deps)\n==> canola (get-deps)\n==> riak_auth_mods (get-deps)\n==> rebar_lock_deps_plugin (get-deps)\n==> rel (get-deps)\n==> riak (get-deps)\n==> develop.fbc4c25.fd-deps-cleanup.a647b0f (get-deps)\n./rebar compile\n==> node_package (compile)\n==> goldrush (compile)\n==> lager (compile)\n==> syslog (compile)\n==> lager_syslog (compile)\n==> cluster_info (compile)\n==> sidejob (compile)\n==> erlang_js (compile)\nmake[1]: Entering directory `/tmp/thumbs/develop.fbc4c25.fd-deps-cleanup.a647b0f/deps/erlang_js'\ncd c_src; make\nmake[2]: Entering directory `/tmp/thumbs/develop.fbc4c25.fd-deps-cleanup.a647b0f/deps/erlang_js/c_src'\nmake[2]: Nothing to be done for `js'.\nmake[2]: Leaving directory `/tmp/thumbs/develop.fbc4c25.fd-deps-cleanup.a647b0f/deps/erlang_js/c_src'\nmake[1]: Leaving directory `/tmp/thumbs/develop.fbc4c25.fd-deps-cleanup.a647b0f/deps/erlang_js'\n==> meck (compile)\n==> getopt (compile)\n==> neotoma (compile)\n==> cuttlefish (compile)\n==> getopt (escriptize)\n==> goldrush (escriptize)\n==> lager (escriptize)\n==> neotoma (escriptize)\n==> cuttlefish (escriptize)\n==> bitcask (compile)\n==> eper (compile)\n==> edown (compile)\n==> sext (compile)\n==> poolboy (compile)\n==> basho_stats (compile)\n==> riak_sysmon (compile)\n==> eleveldb (compile)\nmake[1]: Entering directory `/tmp/thumbs/develop.fbc4c25.fd-deps-cleanup.a647b0f/deps/eleveldb/c_src/leveldb'\nmake[1]: Nothing to be done for `all'.\nmake[1]: Leaving directory `/tmp/thumbs/develop.fbc4c25.fd-deps-cleanup.a647b0f/deps/eleveldb/c_src/leveldb'\nmake[1]: Entering directory `/tmp/thumbs/develop.fbc4c25.fd-deps-cleanup.a647b0f/deps/eleveldb/c_src/leveldb'\nmake[1]: Nothing to be done for `tools'.\nmake[1]: Leaving directory `/tmp/thumbs/develop.fbc4c25.fd-deps-cleanup.a647b0f/deps/eleveldb/c_src/leveldb'\n==> riak_ensemble (compile)\n==> pbkdf2 (compile)\n==> parse_trans (compile)\n==> bear (compile)\n==> folsom (compile)\n==> setup (compile)\n==> edown (escriptize)\n==> setup (escriptize)\n==> src (compile)\n==> exometer_core (compile)\n==> clique (compile)\n==> riak_core (compile)\n==> riak_pipe (compile)\n==> riak_dt (compile)\n==> eunit_formatters (compile)\n==> hamcrest (compile)\n==> hamcrest (post_compile)\n==> riak_pb (compile)\n==> mochiweb (compile)\n==> webmachine (compile)\n==> riak_api (compile)\n==> proper (compile)\nmake[1]: Entering directory `/tmp/thumbs/develop.fbc4c25.fd-deps-cleanup.a647b0f/deps/proper'\nmake[1]: `include/compile_flags.hrl' is up to date.\nmake[1]: Leaving directory `/tmp/thumbs/develop.fbc4c25.fd-deps-cleanup.a647b0f/deps/proper'\n==> stdlib2 (compile)\n==> hyper (compile)\n==> gproc (compile)\n==> chronos (compile)\n==> riak_kv (compile)\n==> merge_index (compile)\n==> lucene_parser (compile)\n==> riak_search (compile)\n==> erlydtl (compile)\n==> riak_control (compile)\nCompiled handlebars asset priv/admin/js/generated/templates.js\nWARN:  Bypassing stylesheet processing of priv/admin/css: stylus missing.\n==> riaknostic (compile)\n==> kvc (compile)\n==> ibrowse (compile)\n==> fuse (compile)\n==> yokozuna (compile)\n==> canola (compile)\n==> riak_auth_mods (compile)\n==> rebar_lock_deps_plugin (compile)\n==> rel (compile)\n==> riak (compile)\n==> develop.fbc4c25.fd-deps-cleanup.a647b0f (compile)\n./rebar generate \n==> rel (generate)\nSchema: [\"/tmp/thumbs/develop.fbc4c25.fd-deps-cleanup.a647b0f/rel/riak/lib/10-riak.schema\",\n         \"/tmp/thumbs/develop.fbc4c25.fd-deps-cleanup.a647b0f/rel/riak/lib/11-erlang_vm.schema\",\n         \"/tmp/thumbs/develop.fbc4c25.fd-deps-cleanup.a647b0f/rel/riak/lib/12-riak_core.schema\",\n         \"/tmp/thumbs/develop.fbc4c25.fd-deps-cleanup.a647b0f/rel/riak/lib/13-riak_api.schema\",\n         \"/tmp/thumbs/develop.fbc4c25.fd-deps-cleanup.a647b0f/rel/riak/lib/14-riak_kv.schema\",\n         \"/tmp/thumbs/develop.fbc4c25.fd-deps-cleanup.a647b0f/rel/riak/lib/15-riak_sysmon.schema\",\n         \"/tmp/thumbs/develop.fbc4c25.fd-deps-cleanup.a647b0f/rel/riak/lib/16-bitcask.schema\",\n         \"/tmp/thumbs/develop.fbc4c25.fd-deps-cleanup.a647b0f/rel/riak/lib/17-bitcask_multi.schema\",\n         \"/tmp/thumbs/develop.fbc4c25.fd-deps-cleanup.a647b0f/rel/riak/lib/18-riak_control.schema\",\n         \"/tmp/thumbs/develop.fbc4c25.fd-deps-cleanup.a647b0f/rel/riak/lib/20-multi_backend.schema\",\n         \"/tmp/thumbs/develop.fbc4c25.fd-deps-cleanup.a647b0f/rel/riak/lib/21-leveldb.schema\",\n         \"/tmp/thumbs/develop.fbc4c25.fd-deps-cleanup.a647b0f/rel/riak/lib/22-leveldb_multi.schema\",\n         \"/tmp/thumbs/develop.fbc4c25.fd-deps-cleanup.a647b0f/rel/riak/lib/30-yokozuna.schema\"]\nWARN:  'generate' command does not apply to directory /tmp/thumbs/develop.fbc4c25.fd-deps-cleanup.a647b0f\n\n\n\n\n```\n\n--------------------------------------------------\n\n\n\n:white_large_square: 0 of 1 Code reviews from organization basho\n\n\n. |  |  | |  |\n| ------------ | -------------|------------ | ------------- |\nfd-deps-cleanup b82ed12 | :arrow_right: | develop 2217ad5 | :no_entry: completed\nThere seems to be an issue with build step **merge,make_deps,make,make_rel** !  :cloud: \n\n:no_entry: MERGE \n\n\n> Started at: 2017-03-10 08:44\n> Duration:  seconds.\n> Result:  ERROR\n> Message: Merge Failed: fd-deps-cleanup b82ed12f6f7fab4b0045adc443684df8e9cc3fb0 onto target branch: develop 2217ad5b326d2af584536c1058718b7346727538\n> Exit Code:  ERROR\n\n> :page_facing_up:\n\n\n```\n\n\n\n\n\n\n  #&1:error: Your local changes to the following files would be overwritten by merge:\n    rebar.config\nPlease, commit your changes or stash them before you can merge.\nerror: The following untracked working tree files would be overwritten by merge:\n    .thumbs.yml\nPlease move or remove them before you can merge.\nAborting>\n\n\n\n```\n\n--------------------------------------------------\n\n\n\n:no_entry: MAKE_DEPS \n\n\n> Started at: 2017-03-10 08:44\n> Duration: 0 seconds.\n> Result:  ERROR\n> Message: Step make_deps Failed!\n> Exit Code:  2\n\n> :page_facing_up:\n\n\n```\n\ncd /tmp/thumbs/develop.2217ad5.fd-deps-cleanup.b82ed12; make deps\n\n\n\n\n  ./rebar get-deps\nERROR: Failed to load /tmp/thumbs/develop.2217ad5.fd-deps-cleanup.b82ed12/rebar.config: {error,\n                                                                                         {16,\n                                                                                          erl_parse,\n                                                                                          [\"syntax error before: \",\n                                                                                           \"'<'\"]}}\nmake: *** [deps] Error 1\n\n\n\n\n```\n\n--------------------------------------------------\n\n\n\n:no_entry: MAKE \n\n\n> Started at: 2017-03-10 08:44\n> Duration: 0 seconds.\n> Result:  ERROR\n> Message: Step make Failed!\n> Exit Code:  2\n\n> :page_facing_up:\n\n\n```\n\ncd /tmp/thumbs/develop.2217ad5.fd-deps-cleanup.b82ed12; make\n\n\n\n\n  ./rebar get-deps\nERROR: Failed to load /tmp/thumbs/develop.2217ad5.fd-deps-cleanup.b82ed12/rebar.config: {error,\n                                                                                         {16,\n                                                                                          erl_parse,\n                                                                                          [\"syntax error before: \",\n                                                                                           \"'<'\"]}}\nmake: *** [deps] Error 1\n\n\n\n\n```\n\n--------------------------------------------------\n\n\n\n:no_entry: MAKE_REL \n\n\n> Started at: 2017-03-10 08:44\n> Duration: 39 seconds.\n> Result:  ERROR\n> Message: Step make_rel Failed!\n> Exit Code:  2\n\n> :page_facing_up:\n\n\n```\n\ncd /tmp/thumbs/develop.2217ad5.fd-deps-cleanup.b82ed12; make rel\n\n\n\n\n\n\n... Snipped 93 lines ...\n into 'exometer_core'...\nPulling clique from {git,\"https://github.com/basho/clique.git\",\n                         \"5e2d0925fe43ab4c9a1f4e998a435ea751bc41e3\"}\nCloning into 'clique'...\nPulling riak_core from {git,\"git://github.com/basho/riak_core.git\",\n                            \"f79eb14ea55893c3e413c129e2cf2da58953e5ca\"}\nCloning into 'riak_core'...\nPulling riak_pipe from {git,\"https://github.com/basho/riak_pipe.git\",\n                            \"ef32cc11ebe52c2a4889a19d14686f751becf12f\"}\nCloning into 'riak_pipe'...\nPulling riak_dt from {git,\"https://github.com/basho/riak_dt.git\",\n                          \"f96f943ca66902e12263a445cf9099d7f635d5ab\"}\nCloning into 'riak_dt'...\nPulling eunit_formatters from {git,\"https://github.com/basho/eunit_formatters\",\n                                   \"96b6ced4d45ba641cbf2c8a8ae9b350dd300bc10\"}\nCloning into 'eunit_formatters'...\nPulling hamcrest from {git,\"https://github.com/basho/hamcrest-erlang.git\",\n                           \"98bc7aa19ea081478c816824aa05fc5a48acae66\"}\nCloning into 'hamcrest'...\nPulling riak_pb from {git,\"https://github.com/basho/riak_pb.git\",\n                          \"6c67dc58251e1a1c29b2e0c444b1e9c9aa2d4a24\"}\nCloning into 'riak_pb'...\nPulling mochiweb from {git,\"git://github.com/basho/mochiweb.git\",\n                           \"4d3882181d0e0e507a05115782a2b091a1db2be4\"}\nCloning into 'mochiweb'...\nPulling webmachine from {git,\"https://github.com/basho/webmachine.git\",\n                             \"494d14fa951816051732324bd324019b99dbc2e0\"}\nCloning into 'webmachine'...\nPulling riak_api from {git,\"https://github.com/basho/riak_api.git\",\n                           \"7ae8d115444e2360d26fd7d006830a289c3a0c1c\"}\nCloning into 'riak_api'...\nPulling proper from {git,\"https://github.com/basho/proper.git\",\n                         \"f5589897de7d87fedf71e8a8298cdfdebb665832\"}\nCloning into 'proper'...\nPulling stdlib2 from {git,\"https://github.com/basho/stdlib2.git\",\n                          \"0c334200fd9c7ddd79f6dcc3a63c0aa5de5d3a33\"}\nCloning into 'stdlib2'...\nPulling hyper from {git,\"https://github.com/basho/hyper\",\n                        \"f6ed834cd8799623ec00faaedc9ef2a55876d5d8\"}\nCloning into 'hyper'...\nPulling gproc from {git,\"https://github.com/basho/gproc.git\",\n                        \"01c8fbfdd5e4701e8e4b57b0c8279872f9574b0b\"}\nCloning into 'gproc'...\nPulling chronos from {git,\"https://github.com/basho/chronos.git\",\n                          \"a92b6e3554e32b1d71b1f414b1b712b68df65d70\"}\nCloning into 'chronos'...\nPulling riak_kv from {git,\"git://github.com/basho/riak_kv.git\",\n                          \"46fb94ce94f7a4f7d8f8ab4996e9213d50849639\"}\nCloning into 'riak_kv'...\nPulling merge_index from {git,\"git://github.com/basho/merge_index.git\",\n                              \"c5efac6d3ccf6ee976978e21697016bfbec2265c\"}\nCloning into 'merge_index'...\nPulling riak_search from {git,\"git://github.com/basho/riak_search.git\",\n                              \"f2d9592184ef9a1e627fa25970ae340334d438d4\"}\nCloning into 'riak_search'...\nPulling erlydtl from {git,\"git://github.com/basho/erlydtl.git\",\n                          \"d20b53f04837a1053ed18987f645cb60eae82453\"}\nCloning into 'erlydtl'...\nPulling riak_control from {git,\"git://github.com/basho/riak_control.git\",\n                               \"f7d74fbdf55b8c2b7296b68f6c0e0835ca2c4d9d\"}\nCloning into 'riak_control'...\nPulling riaknostic from {git,\"git://github.com/basho/riaknostic.git\",\n                             \"08fdf30e680861f47c37e2afc0b479ae02d9011b\"}\nCloning into 'riaknostic'...\nPulling kvc from {git,\"https://github.com/basho/kvc.git\",\n                      \"5565fe51857747662410cc3c06362ebcf48a2f04\"}\nCloning into 'kvc'...\nPulling ibrowse from {git,\"https://github.com/basho/ibrowse.git\",\n                          \"b28542d1e326ba44bcfaf7fd6d3c7f8761d20f08\"}\nCloning into 'ibrowse'...\nPulling fuse from {git,\"https://github.com/basho/fuse.git\",\n                       \"21c6e52ced3af294f2fe636039106068da12eeeb\"}\nCloning into 'fuse'...\nPulling riakc from {git,\"git://github.com/basho/riak-erlang-client\",\n                        \"f3e64014d90c261880f5b06113bb604a6d5df495\"}\nCloning into 'riakc'...\nPulling yokozuna from {git,\"git://github.com/basho/yokozuna.git\",\n                           \"7112b2ea5c0a724c6481cf4172155a0eda2223ff\"}\nCloning into 'yokozuna'...\nPulling canola from {git,\"git://github.com/basho/canola.git\",\n                         \"9bdfee88fce20b3a01b7003696b53eb21913d6fb\"}\nCloning into 'canola'...\nPulling riak_auth_mods from {git,\"git://github.com/basho/riak_auth_mods.git\",\n                                 \"31b8b30e6c215418522eaa615264ae9769a87410\"}\nCloning into 'riak_auth_mods'...\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> rebar_lock_deps_plugin (get-deps)\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> node_package (get-deps)\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> goldrush (get-deps)\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> lager (get-deps)\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> syslog (get-deps)\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> lager_syslog (get-deps)\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> cluster_info (get-deps)\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> sidejob (get-deps)\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> erlang_js (get-deps)\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> meck (get-deps)\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> getopt (get-deps)\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> neotoma (get-deps)\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> cuttlefish (get-deps)\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> bitcask (get-deps)\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> eper (get-deps)\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> edown (get-deps)\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> sext (get-deps)\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> poolboy (get-deps)\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> basho_stats (get-deps)\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> riak_sysmon (get-deps)\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> eleveldb (get-deps)\nCloning into 'leveldb'...\nNote: checking out '2.0.31'.\n\nYou are in 'detached HEAD' state. You can look around, make experimental\nchanges and commit them, and you can discard any commits you make in this\nstate without impacting any branches by performing another checkout.\n\nIf you want to create a new branch to retain commits you create, you may\ndo so (now or later) by using -b with the checkout command again. Example:\n\n  git checkout -b new_branch_name\n\nHEAD is now at e6a2939... add (int) cast to fix compile errors on 10 of 16 platforms ... argh.\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> riak_ensemble (get-deps)\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> pbkdf2 (get-deps)\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> parse_trans (get-deps)\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> bear (get-deps)\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> folsom (get-deps)\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> setup (get-deps)\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> src (get-deps)\n==> exometer_core (get-deps)\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> clique (get-deps)\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> riak_core (get-deps)\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> riak_pipe (get-deps)\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> riak_dt (get-deps)\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> eunit_formatters (get-deps)\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> hamcrest (get-deps)\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> riak_pb (get-deps)\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> mochiweb (get-deps)\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> webmachine (get-deps)\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> riak_api (get-deps)\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> proper (get-deps)\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> stdlib2 (get-deps)\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> hyper (get-deps)\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> gproc (get-deps)\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> chronos (get-deps)\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> riak_kv (get-deps)\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> merge_index (get-deps)\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> lucene_parser (get-deps)\n==> riak_search (get-deps)\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> erlydtl (get-deps)\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> riak_control (get-deps)\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> riaknostic (get-deps)\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> kvc (get-deps)\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> ibrowse (get-deps)\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> fuse (get-deps)\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> riakc (get-deps)\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> yokozuna (get-deps)\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> canola (get-deps)\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> riak_auth_mods (get-deps)\n./rebar compile\nERROR: Failed to load /tmp/thumbs/develop.2217ad5.fd-deps-cleanup.b82ed12/rebar.config: {error,\n                                                                                         {16,\n                                                                                          erl_parse,\n                                                                                          [\"syntax error before: \",\n                                                                                           \"'<'\"]}}\nmake: *** [compile] Error 1\n\n\n\n\n```\n\n--------------------------------------------------\n\n\n\n:white_large_square: 0 of 1 Code reviews from organization basho\n\n\n. |  |  | |  |\n| ------------ | -------------|------------ | ------------- |\nfd-deps-cleanup 5c1ee63 | :arrow_right: | develop 2217ad5 | :white_check_mark: completed\nLooks good!  :+1: \n\n:white_check_mark: MERGE \n\n\n> Started at: 2017-03-10 08:46\n> Duration: 1 seconds.\n> Result:  OK\n> Message: Merge Success: fd-deps-cleanup 5c1ee6320928910aa1a3d30c82d1a61e943a31fe onto target branch: develop 2217ad5b326d2af584536c1058718b7346727538\n> Exit Code:  OK\n\n> :page_facing_up:\n\n\n```\n\n\n\n\n\n\n  Updating 2217ad5..5c1ee63\nFast-forward (no commit created; -m option ignored)\n .thumbs.yml  |  8 ++++++++\n rebar.config | 21 +++++++++++----------\n 2 files changed, 19 insertions(+), 10 deletions(-)\n create mode 100644 .thumbs.yml\n\n\n\n```\n\n--------------------------------------------------\n\n\n\n:white_check_mark: MAKE_DEPS \n\n\n> Started at: 2017-03-10 08:46\n> Duration: 42 seconds.\n> Result:  OK\n> Message: OK\n> Exit Code:  0\n\n> :page_facing_up:\n\n\n```\n\ncd /tmp/thumbs/develop.2217ad5.fd-deps-cleanup.5c1ee63; make deps\n\n\n\n\n\n\n... Snipped 52 lines ...\nng goldrush from {git,\"https://github.com/basho/goldrush.git\",\n                           {tag,\"0.1.9\"}}\nCloning into 'goldrush'...\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> goldrush (get-deps)\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> syslog (get-deps)\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> cluster_info (get-deps)\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> riak_kv (get-deps)\nPulling sidejob from {git,\"https://github.com/basho/sidejob.git\",\n                          {tag,\"2.0.1\"}}\nCloning into 'sidejob'...\nPulling erlang_js from {git,\"https://github.com/basho/erlang_js.git\",\n                            {tag,\"1.3.0\"}}\nCloning into 'erlang_js'...\nPulling bitcask from {git,\"https://github.com/basho/bitcask.git\",\n                          {tag,\"2.0.6\"}}\nCloning into 'bitcask'...\nPulling eper from {git,\"https://github.com/basho/eper.git\",{tag,\"0.78\"}}\nCloning into 'eper'...\nPulling sext from {git,\"https://github.com/basho/sext.git\",{tag,\"1.1p3\"}}\nCloning into 'sext'...\nPulling riak_pipe from {git,\"https://github.com/basho/riak_pipe.git\",\n                            {branch,\"develop\"}}\nCloning into 'riak_pipe'...\nPulling riak_dt from {git,\"https://github.com/basho/riak_dt.git\",\n                          {branch,\"develop\"}}\nCloning into 'riak_dt'...\nPulling eunit_formatters from {git,\"https://github.com/basho/eunit_formatters\",\n                                   {tag,\"0.1.2\"}}\nCloning into 'eunit_formatters'...\nPulling riak_api from {git,\"https://github.com/basho/riak_api.git\",\n                           {branch,\"develop\"}}\nCloning into 'riak_api'...\nPulling hyper from {git,\"https://github.com/basho/hyper\",{tag,\"1.0.0\"}}\nCloning into 'hyper'...\nPulling clique from {git,\"https://github.com/basho/clique.git\",{tag,\"0.3.7\"}}\nCloning into 'clique'...\nPulling chronos from {git,\"https://github.com/basho/chronos.git\",\n                          {tag,\"0.1.4-basho1\"}}\nCloning into 'chronos'...\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> sidejob (get-deps)\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> erlang_js (get-deps)\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> meck (get-deps)\n==> bitcask (get-deps)\nPulling cuttlefish from {git,\"https://github.com/basho/cuttlefish.git\",\n                             {tag,\"2.0.10\"}}\nCloning into 'cuttlefish'...\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> cuttlefish (get-deps)\nPulling getopt from {git,\"https://github.com/basho/getopt.git\",{tag,\"v0.8.2\"}}\nCloning into 'getopt'...\nPulling neotoma from {git,\"https://github.com/basho/neotoma.git\",\n                          {tag,\"1.7.3\"}}\nCloning into 'neotoma'...\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> getopt (get-deps)\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> neotoma (get-deps)\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> eper (get-deps)\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> sext (get-deps)\nPulling edown from {git,\"git://github.com/uwiger/edown.git\",{tag,\"0.5\"}}\nCloning into 'edown'...\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> edown (get-deps)\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> riak_pipe (get-deps)\nPulling riak_core from {git,\"git://github.com/basho/riak_core.git\",\n                            {branch,\"develop\"}}\nCloning into 'riak_core'...\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> clique (get-deps)\n==> riak_core (get-deps)\nPulling poolboy from {git,\"git://github.com/basho/poolboy.git\",\n                          {tag,\"0.8.1p3\"}}\nCloning into 'poolboy'...\nPulling basho_stats from {git,\"git://github.com/basho/basho_stats.git\",\n                              {tag,\"1.0.3\"}}\nCloning into 'basho_stats'...\nPulling riak_sysmon from {git,\"git://github.com/basho/riak_sysmon.git\",\n                              {tag,\"2.1.4\"}}\nCloning into 'riak_sysmon'...\nPulling eleveldb from {git,\"git://github.com/basho/eleveldb.git\",\n                           {tag,\"2.0.32\"}}\nCloning into 'eleveldb'...\nPulling riak_ensemble from {git,\"git://github.com/basho/riak_ensemble\",\n                                {tag,\"2.1.6\"}}\nCloning into 'riak_ensemble'...\nPulling pbkdf2 from {git,\"git://github.com/basho/erlang-pbkdf2.git\",\n                         {tag,\"2.0.0\"}}\nCloning into 'pbkdf2'...\nPulling exometer_core from {git,\"git://github.com/basho/exometer_core.git\",\n                                {tag,\"1.0.0-basho9\"}}\nCloning into 'exometer_core'...\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> poolboy (get-deps)\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> basho_stats (get-deps)\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> riak_sysmon (get-deps)\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> eleveldb (get-deps)\nCloning into 'leveldb'...\nNote: checking out '2.0.31'.\n\nYou are in 'detached HEAD' state. You can look around, make experimental\nchanges and commit them, and you can discard any commits you make in this\nstate without impacting any branches by performing another checkout.\n\nIf you want to create a new branch to retain commits you create, you may\ndo so (now or later) by using -b with the checkout command again. Example:\n\n  git checkout -b new_branch_name\n\nHEAD is now at e6a2939... add (int) cast to fix compile errors on 10 of 16 platforms ... argh.\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> riak_ensemble (get-deps)\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> pbkdf2 (get-deps)\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> src (get-deps)\n==> exometer_core (get-deps)\nPulling parse_trans from {git,\"git://github.com/basho/parse_trans.git\",\n                              {tag,\"2.9.2p1\"}}\nCloning into 'parse_trans'...\nPulling folsom from {git,\"git://github.com/basho/folsom.git\",{tag,\"0.7.4p5\"}}\nCloning into 'folsom'...\nPulling setup from {git,\"git://github.com/basho/setup.git\",{tag,\"1.4\"}}\nCloning into 'setup'...\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> parse_trans (get-deps)\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> folsom (get-deps)\nPulling bear from {git,\"git://github.com/basho/bear.git\",{tag,\"0.1.3p1\"}}\nCloning into 'bear'...\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> bear (get-deps)\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> setup (get-deps)\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> riak_dt (get-deps)\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> eunit_formatters (get-deps)\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> riak_api (get-deps)\nPulling riak_pb from {git,\"https://github.com/basho/riak_pb.git\",\n                          {tag,\"2.3.0.0\"}}\nCloning into 'riak_pb'...\nPulling webmachine from {git,\"https://github.com/basho/webmachine.git\",\n                             {tag,\"1.10.8-basho1\"}}\nCloning into 'webmachine'...\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> riak_pb (get-deps)\nPulling hamcrest from {git,\"https://github.com/basho/hamcrest-erlang.git\",\n                           {tag,\"0.3.0-basho\"}}\nCloning into 'hamcrest'...\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> hamcrest (get-deps)\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> webmachine (get-deps)\nPulling mochiweb from {git,\"git://github.com/basho/mochiweb.git\",\n                           {tag,\"v2.9.0p2\"}}\nCloning into 'mochiweb'...\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> mochiweb (get-deps)\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> hyper (get-deps)\nPulling proper from {git,\"https://github.com/basho/proper.git\",{tag,\"v1.2p1\"}}\nCloning into 'proper'...\nPulling stdlib2 from {git,\"https://github.com/basho/stdlib2.git\",\n                          {tag,\"0.0.1\"}}\nCloning into 'stdlib2'...\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> proper (get-deps)\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> stdlib2 (get-deps)\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> chronos (get-deps)\nPulling gproc from {git,\"https://github.com/basho/gproc.git\",\"master\"}\nCloning into 'gproc'...\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> gproc (get-deps)\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> lucene_parser (get-deps)\n==> riak_search (get-deps)\nPulling merge_index from {git,\"git://github.com/basho/merge_index.git\",\n                              {tag,\"2.0.4\"}}\nCloning into 'merge_index'...\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> merge_index (get-deps)\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> riak_control (get-deps)\nPulling erlydtl from {git,\"git://github.com/basho/erlydtl.git\",\"d20b53f0\"}\nCloning into 'erlydtl'...\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> erlydtl (get-deps)\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> riaknostic (get-deps)\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> yokozuna (get-deps)\nPulling kvc from {git,\"https://github.com/basho/kvc.git\",{tag,\"v1.5.0\"}}\nCloning into 'kvc'...\nPulling ibrowse from {git,\"https://github.com/basho/ibrowse.git\",{tag,\"v4.3\"}}\nCloning into 'ibrowse'...\nPulling fuse from {git,\"https://github.com/basho/fuse.git\",{tag,\"v2.1.0\"}}\nCloning into 'fuse'...\nPulling riakc from {git,\"git://github.com/basho/riak-erlang-client\",\n                        {tag,\"2.5.2\"}}\nCloning into 'riakc'...\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> kvc (get-deps)\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> ibrowse (get-deps)\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> fuse (get-deps)\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> riakc (get-deps)\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> riak_auth_mods (get-deps)\nPulling canola from {git,\"git://github.com/basho/canola.git\",{tag,\"2.0.0\"}}\nCloning into 'canola'...\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> canola (get-deps)\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> rebar_lock_deps_plugin (get-deps)\n\n\n\n\n```\n\n--------------------------------------------------\n\n\n\n:white_check_mark: MAKE \n\n\n> Started at: 2017-03-10 08:47\n> Duration: 92 seconds.\n> Result:  OK\n> Message: OK\n> Exit Code:  0\n\n> :page_facing_up:\n\n\n```\n\ncd /tmp/thumbs/develop.2217ad5.fd-deps-cleanup.5c1ee63; make\n\n\n\n\n\n\n... Snipped 2244 lines ...\ndoff.erl\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> merge_index (compile)\nCompiled src/mi_buffer_converter.erl\nCompiled src/mi_bloom.erl\nCompiled src/mi_utils.erl\nCompiled src/mi_buffer.erl\nCompiled src/mi_sup.erl\nCompiled src/mi_app.erl\nCompiled src/mi_locks.erl\nCompiled src/merge_index.erl\nCompiled src/mi_buffer_converter_sup.erl\nCompiled src/mi_segment.erl\nCompiled src/mi_scheduler.erl\nCompiled src/mi_segment_writer.erl\nCompiled src/mi_server.erl\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> lucene_parser (compile)\nCompiled src/lucene_scan.xrl\nCompiled src/lucene_parse.yrl\nCompiled src/lucene_parse.erl\nCompiled src/lucene_scan.erl\nCompiled src/lucene_parser.erl\nCompiled src/lucene_scan_test.erl\nCompiled src/lucene_parse_test.erl\n==> riak_search (compile)\nCompiled src/riak_search_backend.erl\nCompiled src/riak_search_utils.erl\nCompiled src/riak_search_app.erl\nCompiled src/riak_search_stat.erl\nCompiled src/riak_search_kv_json_extractor.erl\nCompiled src/basho_bench_driver_riaksearch.erl\nCompiled src/riak_search_op_term.erl\nCompiled src/riak_search_op_range_sized.erl\nCompiled src/riak_solr_xml.erl\nCompiled src/riak_search_kv_raw_extractor.erl\nCompiled src/riak_search_op_proximity.erl\nCompiled src/riak_search_op_negation.erl\nCompiled src/riak_search_op.erl\nCompiled src/riak_search_ets_backend.erl\nCompiled src/riak_solr_sort.erl\nCompiled src/riak_search_kv_hook.erl\nCompiled src/riak_search_vnode.erl\nCompiled src/riak_search_basic_qc.erl\nCompiled src/riak_solr_xml_xform.erl\nCompiled src/riak_search_schema.erl\nCompiled src/riak_search_kv_erlang_extractor.erl\nCompiled src/riak_search_op_group.erl\nCompiled src/riak_search_operators_qc.erl\nCompiled src/riak_search_config.erl\nCompiled src/riak_search_kv_xml_extractor.erl\nCompiled src/riak_search_kv_erlang_binary_extractor.erl\nCompiled src/riak_solr_qc.erl\nCompiled src/riak_search_ring_utils.erl\nCompiled src/riak_search_op_union.erl\nCompiled src/riak_search_cmd.erl\nCompiled src/riak_search_op_intersection.erl\nCompiled src/riak_solr_indexer_wm.erl\nCompiled src/riak_search_dir_indexer.erl\nCompiled src/merge_index_backend.erl\nCompiled src/riak_search_kv_extractor.erl\nCompiled src/solr_search.erl\nCompiled src/riak_search_client.erl\nCompiled src/riak_search_pb_query.erl\nCompiled src/search.erl\nCompiled src/riak_search_schema_parser.erl\nCompiled src/riak_search_op_scope.erl\nCompiled src/riak_search_op_node.erl\nCompiled src/riak_search_worker.erl\nCompiled src/riak_search_inlines.erl\nCompiled src/riak_search.erl\nCompiled src/riak_search_op_range.erl\nCompiled src/riak_search_op_mockterm.erl\nCompiled src/riak_solr_output.erl\nCompiled src/riak_solr_searcher_wm.erl\nCompiled src/riak_search_vnode_sup.erl\nCompiled src/riak_search_test.erl\nCompiled src/riak_search_op_string.erl\nCompiled src/riak_search_cinfo.erl\nCompiled src/riak_solr_search_client.erl\nCompiled src/text_analyzers.erl\nCompiled src/riak_search_op_utils.erl\nCompiled src/riak_search_sup.erl\nCompiled src/riak_search_op_range_worker.erl\nCompiled src/riak_indexed_doc.erl\nCompiled src/riak_search_repl_helper.erl\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> erlydtl (compile)\nCompiled src/erlydtl_parser.yrl\nCompiled src/erlydtl_parser.erl\nCompiled src/erlydtl_i18n.erl\nCompiled src/erlydtl_contrib_humanize.erl\nCompiled src/i18n/po_generator.erl\nCompiled src/i18n/sources_parser.erl\nCompiled src/i18n/blocktrans_extractor.erl\nCompiled src/erlydtl_runtime.erl\nCompiled src/i18n/i18n_manager.erl\nCompiled src/i18n/blocktrans_parser.erl\nCompiled src/i18n/blocktrans_scanner.erl\nCompiled src/erlydtl_deps.erl\nCompiled src/i18n/po_scanner.erl\nCompiled src/erlydtl.erl\nCompiled src/erlydtl_unparser.erl\nCompiled src/filter_lib/erlydtl_dateformat.erl\nCompiled src/filter_lib/erlydtl_slice.erl\nCompiled src/erlydtl_filters.erl\nCompiled src/erlydtl_scanner.erl\nCompiled src/erlydtl_compiler.erl\n==> riak_control (compile)\nCompiled src/riak_control_app.erl\nCompiled src/riak_control_security.erl\nCompiled src/riak_control_wm_gui.erl\nCompiled src/riak_control_sup.erl\nCompiled src/riak_control_wm_cluster.erl\nCompiled src/riak_control_ring.erl\nCompiled src/riak_control.erl\nCompiled src/riak_control_wm_nodes.erl\nCompiled src/riak_control_session.erl\nCompiled src/riak_control_routes.erl\nCompiled src/riak_control_wm_partitions.erl\n:0: Warning: function render_tag/3 is unused\n:0: Warning: variable 'TagName' is unused\nCompiled templates/index.dtl\nCompiled handlebars asset priv/admin/js/generated/templates.js\nBuilt asset priv/admin/js/generated/vendor.js\nWARN:  Bypassing stylesheet processing of priv/admin/css: stylus missing.\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> riaknostic (compile)\nCompiled src/riaknostic_check.erl\nCompiled src/riaknostic.erl\nCompiled src/riaknostic_check_ring_preflists.erl\nCompiled src/riaknostic_check_strong_consistency.erl\nCompiled src/riaknostic_node.erl\nCompiled src/riaknostic_util.erl\nCompiled src/riaknostic_check_monitors.erl\nCompiled src/riaknostic_check_ring_membership.erl\nCompiled src/riaknostic_check_memory_use.erl\nCompiled src/riaknostic_check_search.erl\nCompiled src/riaknostic_check_nodes_connected.erl\nCompiled src/riaknostic_check_ring_size.erl\nCompiled src/riaknostic_check_dumps.erl\nCompiled src/riaknostic_check_disk.erl\nCompiled src/riaknostic_config.erl\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> kvc (compile)\nCompiled src/kvc.erl\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> ibrowse (compile)\nCompiled src/ibrowse_lib.erl\nCompiled src/ibrowse_app.erl\nCompiled src/ibrowse_socks5.erl\nCompiled src/ibrowse_lb.erl\nCompiled src/ibrowse_sup.erl\nCompiled src/ibrowse_http_client.erl\nCompiled src/ibrowse.erl\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> fuse (compile)\nCompiled src/fuse_stats_plugin.erl\nCompiled src/fuse_app.erl\nCompiled src/fuse_stats_folsom.erl\nCompiled src/fuse_event.erl\nCompiled src/fuse_stats_exometer.erl\nCompiled src/fuse_time.erl\nCompiled src/fuse_stats_ets.erl\nCompiled src/fuse_sup.erl\nCompiled src/fuse.erl\nCompiled src/fuse_monitor.erl\nCompiled src/fuse_server.erl\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> riakc (compile)\nCompiled src/riakc_datatype.erl\nCompiled src/riakc_ts_query_operator.erl\nCompiled src/riakc_set.erl\nCompiled src/riakc_ts_put_operator.erl\nCompiled src/riakc_ts.erl\nCompiled src/riakc_hll.erl\nCompiled src/riakc_register.erl\nCompiled src/riakc_ts_get_operator.erl\nCompiled src/riakc_flag.erl\nCompiled src/riakc_map.erl\nCompiled src/riakc_utils.erl\nCompiled src/riakc_gset.erl\nCompiled src/riakc_counter.erl\nCompiled src/riakc_obj.erl\nCompiled src/riakc_pb_socket.erl\n==> yokozuna (compile)\nCreate dir ../build\nUsing cached copy of Solr /var/tmp/yokozuna/solr-4.10.4-yz-2.tgz\nCreating Solr dir ../priv/solr\nSolr dir created successfully\nDownloading yokozuna-3.jar\n--2017-03-10 08:48:38--  http://s3.amazonaws.com/files.basho.com/yokozuna/yokozuna-3.jar\nResolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.32.139\nConnecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.32.139|:80... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 11261 (11K) [application/java-archive]\nSaving to: \u2018yokozuna-3.jar\u2019\n\n     0K                                                      100% 25.4M=0s\n\n2017-03-10 08:48:38 (25.4 MB/s) - \u2018yokozuna-3.jar\u2019 saved [11261/11261]\n\nDownloading yz_monitor-1.jar\n--2017-03-10 08:48:38--  http://s3.amazonaws.com/files.basho.com/yokozuna/yz_monitor-1.jar\nResolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.32.139\nConnecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.32.139|:80... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 2661 (2.6K) [application/java-archive]\nSaving to: \u2018yz_monitor-1.jar\u2019\n\n     0K                                                      100% 23.1M=0s\n\n2017-03-10 08:48:38 (23.1 MB/s) - \u2018yz_monitor-1.jar\u2019 saved [2661/2661]\n\nCompiled src/yz_extractor.erl\nCompiled src/yz_stat.erl\nCompiled src/yz_pb_search.erl\nCompiled src/yz_wm_search.erl\nCompiled src/yz_misc.erl\nCompiled src/yz_kv.erl\nCompiled src/yz_wm_extract.erl\nCompiled src/yz_stat_worker.erl\nCompiled src/yz_solrq_sup.erl\nCompiled src/yz_wm_schema.erl\nCompiled src/yz_solrq_drain_mgr.erl\nCompiled src/yz_sup.erl\nCompiled src/yz_doc.erl\nCompiled src/yz_xml_extractor.erl\nCompiled src/yz_general_sup.erl\nCompiled src/yz_solr_sup.erl\nCompiled src/yz_bucket_validator.erl\nCompiled src/rt_intercept_pt.erl\nCompiled src/yz_entropy_mgr.erl\nCompiled src/yz_entropy.erl\nCompiled src/yz_console.erl\nCompiled src/yz_app.erl\nCompiled src/yz_solrq.erl\nCompiled src/yz_wm_index.erl\nCompiled src/yokozuna.erl\nCompiled src/yz_fuse_stats_sidejob.erl\nCompiled src/yz_rs_migration.erl\nCompiled src/yz_noop_extractor.erl\nCompiled src/yz_solr_proc.erl\nCompiled src/yz_schema.erl\nCompiled src/yz_index.erl\nCompiled src/yz_fuse.erl\nCompiled src/yz_exchange_fsm.erl\nCompiled src/yz_events.erl\nCompiled src/yz_json_extractor.erl\nCompiled src/yz_solrq_helper.erl\nCompiled src/yz_pb_admin.erl\nCompiled src/yz_index_hashtree.erl\nCompiled src/yz_solrq_queue_pair_sup.erl\nCompiled src/yz_diag.erl\nCompiled src/yz_solrq_worker.erl\nCompiled src/yz_text_extractor.erl\nCompiled src/yz_solr.erl\nCompiled src/yz_cover.erl\nCompiled src/yz_dt_extractor.erl\nCompiled src/yz_index_hashtree_sup.erl\nCompiled src/yz_solrq_drain_fsm.erl\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> canola (compile)\nCompiled src/canola.erl\nCompiling c_src/canola-port.c\n==> riak_auth_mods (compile)\nCompiled src/riak_auth_mods_pam.erl\nCompiled src/riak_auth_mods_app.erl\nCompiled src/riak_auth_mods_sup.erl\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> rebar_lock_deps_plugin (compile)\nCompiled src/rldp_util.erl\nCompiled src/rldp_change_log.erl\nCompiled src/rebar_lock_deps_plugin.erl\n==> rel (compile)\n==> riak (compile)\nCompiled src/etop_txt.erl\n==> develop.2217ad5.fd-deps-cleanup.5c1ee63 (compile)\n\n\n\n\n```\n\n--------------------------------------------------\n\n\n\n:white_check_mark: MAKE_REL \n\n\n> Started at: 2017-03-10 08:48\n> Duration: 28 seconds.\n> Result:  OK\n> Message: OK\n> Exit Code:  0\n\n> :page_facing_up:\n\n\n```\n\ncd /tmp/thumbs/develop.2217ad5.fd-deps-cleanup.5c1ee63; make rel\n\n\n\n\n  Using rebar.config.lock file to fetch dependencies\n./rebar -C rebar.config.lock get-deps\n==> rebar_lock_deps_plugin (get-deps)\n==> node_package (get-deps)\n==> goldrush (get-deps)\n==> lager (get-deps)\n==> syslog (get-deps)\n==> lager_syslog (get-deps)\n==> cluster_info (get-deps)\n==> sidejob (get-deps)\n==> erlang_js (get-deps)\n==> meck (get-deps)\n==> getopt (get-deps)\n==> neotoma (get-deps)\n==> cuttlefish (get-deps)\n==> bitcask (get-deps)\n==> eper (get-deps)\n==> edown (get-deps)\n==> sext (get-deps)\n==> poolboy (get-deps)\n==> basho_stats (get-deps)\n==> riak_sysmon (get-deps)\n==> eleveldb (get-deps)\n==> riak_ensemble (get-deps)\n==> pbkdf2 (get-deps)\n==> parse_trans (get-deps)\n==> bear (get-deps)\n==> folsom (get-deps)\n==> setup (get-deps)\n==> src (get-deps)\n==> exometer_core (get-deps)\n==> clique (get-deps)\n==> riak_core (get-deps)\n==> riak_pipe (get-deps)\n==> riak_dt (get-deps)\n==> eunit_formatters (get-deps)\n==> hamcrest (get-deps)\n==> riak_pb (get-deps)\n==> mochiweb (get-deps)\n==> webmachine (get-deps)\n==> riak_api (get-deps)\n==> proper (get-deps)\n==> stdlib2 (get-deps)\n==> hyper (get-deps)\n==> gproc (get-deps)\n==> chronos (get-deps)\n==> riak_kv (get-deps)\n==> merge_index (get-deps)\n==> lucene_parser (get-deps)\n==> riak_search (get-deps)\n==> erlydtl (get-deps)\n==> riak_control (get-deps)\n==> riaknostic (get-deps)\n==> kvc (get-deps)\n==> ibrowse (get-deps)\n==> fuse (get-deps)\n==> riakc (get-deps)\n==> yokozuna (get-deps)\n==> canola (get-deps)\n==> riak_auth_mods (get-deps)\n==> rel (get-deps)\n==> riak (get-deps)\n==> develop.2217ad5.fd-deps-cleanup.5c1ee63 (get-deps)\n./rebar compile\n==> node_package (compile)\n==> goldrush (compile)\n==> lager (compile)\n==> syslog (compile)\n==> lager_syslog (compile)\n==> cluster_info (compile)\n==> sidejob (compile)\n==> erlang_js (compile)\nmake[1]: Entering directory `/tmp/thumbs/develop.2217ad5.fd-deps-cleanup.5c1ee63/deps/erlang_js'\ncd c_src; make\nmake[2]: Entering directory `/tmp/thumbs/develop.2217ad5.fd-deps-cleanup.5c1ee63/deps/erlang_js/c_src'\nmake[2]: Nothing to be done for `js'.\nmake[2]: Leaving directory `/tmp/thumbs/develop.2217ad5.fd-deps-cleanup.5c1ee63/deps/erlang_js/c_src'\nmake[1]: Leaving directory `/tmp/thumbs/develop.2217ad5.fd-deps-cleanup.5c1ee63/deps/erlang_js'\nCompiling c_src/spidermonkey.c\nCompiling c_src/spidermonkey_drv.c\nc_src/spidermonkey_drv.c: In function \u2018send_immediate_ok_response\u2019:\nc_src/spidermonkey_drv.c:82:3: warning: \u2018driver_output_term\u2019 is deprecated (declared at /opt/basho/installs/erlang/R16B02-basho10DEV/erts-5.10.3/include/erl_driver.h:622) [-Wdeprecated-declarations]\n   driver_output_term(dd->port, terms, sizeof(terms) / sizeof(terms[0]));\n   ^\nc_src/spidermonkey_drv.c: In function \u2018ready_async\u2019:\nc_src/spidermonkey_drv.c:256:3: warning: \u2018driver_output_term\u2019 is deprecated (declared at /opt/basho/installs/erlang/R16B02-basho10DEV/erts-5.10.3/include/erl_driver.h:622) [-Wdeprecated-declarations]\n   driver_output_term(dd->port,\n   ^\n==> meck (compile)\n==> getopt (compile)\n==> neotoma (compile)\n==> cuttlefish (compile)\n==> getopt (escriptize)\n==> goldrush (escriptize)\n==> lager (escriptize)\n==> neotoma (escriptize)\n==> cuttlefish (escriptize)\n==> bitcask (compile)\n==> eper (compile)\n==> edown (compile)\n==> sext (compile)\n==> poolboy (compile)\n==> basho_stats (compile)\n==> riak_sysmon (compile)\n==> eleveldb (compile)\nmake[1]: Entering directory `/tmp/thumbs/develop.2217ad5.fd-deps-cleanup.5c1ee63/deps/eleveldb/c_src/leveldb'\nmake[1]: Nothing to be done for `all'.\nmake[1]: Leaving directory `/tmp/thumbs/develop.2217ad5.fd-deps-cleanup.5c1ee63/deps/eleveldb/c_src/leveldb'\nmake[1]: Entering directory `/tmp/thumbs/develop.2217ad5.fd-deps-cleanup.5c1ee63/deps/eleveldb/c_src/leveldb'\nmake[1]: Nothing to be done for `tools'.\nmake[1]: Leaving directory `/tmp/thumbs/develop.2217ad5.fd-deps-cleanup.5c1ee63/deps/eleveldb/c_src/leveldb'\n==> riak_ensemble (compile)\n==> pbkdf2 (compile)\n==> parse_trans (compile)\n==> bear (compile)\n==> folsom (compile)\n==> setup (compile)\n==> edown (escriptize)\n==> setup (escriptize)\n==> src (compile)\n==> exometer_core (compile)\n==> clique (compile)\n==> riak_core (compile)\n==> riak_pipe (compile)\n==> riak_dt (compile)\n==> eunit_formatters (compile)\n==> hamcrest (compile)\n==> hamcrest (post_compile)\n==> riak_pb (compile)\n==> mochiweb (compile)\n==> webmachine (compile)\n==> riak_api (compile)\n==> proper (compile)\nmake[1]: Entering directory `/tmp/thumbs/develop.2217ad5.fd-deps-cleanup.5c1ee63/deps/proper'\nmake[1]: `include/compile_flags.hrl' is up to date.\nmake[1]: Leaving directory `/tmp/thumbs/develop.2217ad5.fd-deps-cleanup.5c1ee63/deps/proper'\n==> stdlib2 (compile)\n==> hyper (compile)\n==> gproc (compile)\n==> chronos (compile)\n==> riak_kv (compile)\n==> merge_index (compile)\n==> lucene_parser (compile)\n==> riak_search (compile)\n==> erlydtl (compile)\n==> riak_control (compile)\nCompiled handlebars asset priv/admin/js/generated/templates.js\nWARN:  Bypassing stylesheet processing of priv/admin/css: stylus missing.\n==> riaknostic (compile)\n==> kvc (compile)\n==> ibrowse (compile)\n==> fuse (compile)\n==> riakc (compile)\n==> yokozuna (compile)\n==> canola (compile)\n==> riak_auth_mods (compile)\n==> rebar_lock_deps_plugin (compile)\n==> rel (compile)\n==> riak (compile)\n==> develop.2217ad5.fd-deps-cleanup.5c1ee63 (compile)\n./rebar generate \n==> rel (generate)\nSchema: [\"/tmp/thumbs/develop.2217ad5.fd-deps-cleanup.5c1ee63/rel/riak/lib/10-riak.schema\",\n         \"/tmp/thumbs/develop.2217ad5.fd-deps-cleanup.5c1ee63/rel/riak/lib/11-erlang_vm.schema\",\n         \"/tmp/thumbs/develop.2217ad5.fd-deps-cleanup.5c1ee63/rel/riak/lib/12-riak_core.schema\",\n         \"/tmp/thumbs/develop.2217ad5.fd-deps-cleanup.5c1ee63/rel/riak/lib/13-riak_api.schema\",\n         \"/tmp/thumbs/develop.2217ad5.fd-deps-cleanup.5c1ee63/rel/riak/lib/14-riak_kv.schema\",\n         \"/tmp/thumbs/develop.2217ad5.fd-deps-cleanup.5c1ee63/rel/riak/lib/15-riak_sysmon.schema\",\n         \"/tmp/thumbs/develop.2217ad5.fd-deps-cleanup.5c1ee63/rel/riak/lib/16-bitcask.schema\",\n         \"/tmp/thumbs/develop.2217ad5.fd-deps-cleanup.5c1ee63/rel/riak/lib/17-bitcask_multi.schema\",\n         \"/tmp/thumbs/develop.2217ad5.fd-deps-cleanup.5c1ee63/rel/riak/lib/18-riak_control.schema\",\n         \"/tmp/thumbs/develop.2217ad5.fd-deps-cleanup.5c1ee63/rel/riak/lib/20-multi_backend.schema\",\n         \"/tmp/thumbs/develop.2217ad5.fd-deps-cleanup.5c1ee63/rel/riak/lib/21-leveldb.schema\",\n         \"/tmp/thumbs/develop.2217ad5.fd-deps-cleanup.5c1ee63/rel/riak/lib/22-leveldb_multi.schema\",\n         \"/tmp/thumbs/develop.2217ad5.fd-deps-cleanup.5c1ee63/rel/riak/lib/29-riak_search.schema\",\n         \"/tmp/thumbs/develop.2217ad5.fd-deps-cleanup.5c1ee63/rel/riak/lib/30-yokozuna.schema\"]\nWARN:  'generate' command does not apply to directory /tmp/thumbs/develop.2217ad5.fd-deps-cleanup.5c1ee63\n\n\n\n\n```\n\n--------------------------------------------------\n\n\n\n:white_large_square: 0 of 1 Code reviews from organization basho\n\n\n. |  |  | |  |\n| ------------ | -------------|------------ | ------------- |\nbjs/bugfix/restrict_atom_usage ee43c4c | :arrow_right: | 2.0 e41753a | :white_check_mark: completed\nLooks good!  :+1: \n\n:white_check_mark: MERGE \n\n\n> Started at: 2017-01-09 12:16\n> Duration: 0 seconds.\n> Result:  OK\n> Message: Merge Success: bjs/bugfix/restrict_atom_usage ee43c4c47d82c78b1c5e6ce6808266a6043bfc38 onto target branch: 2.0 e41753a259c62853bd2bd5a0e615c9805679a88e\n> Exit Code:  OK\n\n> :page_facing_up:\n\n\n```\n\n\n\n\n\n\n  Updating e41753a..ee43c4c\nFast-forward (no commit created; -m option ignored)\n rel/files/riak-admin | 4 ++--\n 1 file changed, 2 insertions(+), 2 deletions(-)\n\n\n\n```\n\n--------------------------------------------------\n\n\n\n. |  |  | |  |\n| ------------ | -------------|------------ | ------------- |\nbjs/bugfix/restrict_atom_usage 88177ea | :arrow_right: | 2.0 e41753a | :white_check_mark: completed\nLooks good!  :+1: \n\n:white_check_mark: MERGE \n\n\n> Started at: 2017-01-09 12:41\n> Duration: 0 seconds.\n> Result:  OK\n> Message: Merge Success: bjs/bugfix/restrict_atom_usage 88177eab457bb80d76e9595e961e25b1b2cb35c8 onto target branch: 2.0 e41753a259c62853bd2bd5a0e615c9805679a88e\n> Exit Code:  OK\n\n> :page_facing_up:\n\n\n```\n\n\n\n\n\n\n  Updating e41753a..88177ea\nFast-forward (no commit created; -m option ignored)\n rel/files/riak-admin | 5 +++--\n 1 file changed, 3 insertions(+), 2 deletions(-)\n\n\n\n```\n\n--------------------------------------------------\n\n\n\n. |  |  | |  |\n| ------------ | -------------|------------ | ------------- |\nrelease_doc b2c778c | :arrow_right: | 2.0 f75e11e | :white_check_mark: completed\nLooks good!  :+1: \n\n:white_check_mark: MERGE \n\n\n> Started at: 2017-03-01 09:50\n> Duration: 2 seconds.\n> Result:  OK\n> Message: Merge Success: release_doc b2c778c9e3b0c381d547c6b81814cc4c30fddf77 onto target branch: 2.0 f75e11effaaa0ae83c435d9cb9d47bc97bd6eaaa\n> Exit Code:  OK\n\n> :page_facing_up:\n\n\n```\n\n\n\n\n\n\n  Updating f75e11e..b2c778c\nFast-forward (no commit created; -m option ignored)\n How-To-Release-Riak.md | 145 +++++++++++++++++++++++++++++++++++++++++++++++++\n 1 file changed, 145 insertions(+)\n create mode 100644 How-To-Release-Riak.md\n\n\n\n```\n\n--------------------------------------------------\n\n\n\n. |  |  | |  |\n| ------------ | -------------|------------ | ------------- |\nrelease_doc e622935 | :arrow_right: | 2.0 f75e11e | :white_check_mark: completed\nLooks good!  :+1: \n\n:white_check_mark: MERGE \n\n\n> Started at: 2017-03-02 11:54\n> Duration: 1 seconds.\n> Result:  OK\n> Message: Merge Success: release_doc e622935b9b05afb798ca7dbdff1a63bd982df79b onto target branch: 2.0 f75e11effaaa0ae83c435d9cb9d47bc97bd6eaaa\n> Exit Code:  OK\n\n> :page_facing_up:\n\n\n```\n\n\n\n\n\n\n  Updating f75e11e..e622935\nFast-forward (no commit created; -m option ignored)\n How-To-Release-Riak.md | 153 +++++++++++++++++++++++++++++++++++++++++++++++++\n 1 file changed, 153 insertions(+)\n create mode 100644 How-To-Release-Riak.md\n\n\n\n```\n\n--------------------------------------------------\n\n\n\n. |  |  | |  |\n| ------------ | -------------|------------ | ------------- |\nrelease_doc 5a9344e | :arrow_right: | 2.0 f75e11e | :white_check_mark: completed\nLooks good!  :+1: \n\n:white_check_mark: MERGE \n\n\n> Started at: 2017-03-06 12:56\n> Duration: 1 seconds.\n> Result:  OK\n> Message: Merge Success: release_doc 5a9344e5a9e57caafd715f5d4cc3439fa0bc47d6 onto target branch: 2.0 f75e11effaaa0ae83c435d9cb9d47bc97bd6eaaa\n> Exit Code:  OK\n\n> :page_facing_up:\n\n\n```\n\n\n\n\n\n\n  Updating f75e11e..5a9344e\nFast-forward (no commit created; -m option ignored)\n How-To-Release-Riak.md | 158 +++++++++++++++++++++++++++++++++++++++++++++++++\n 1 file changed, 158 insertions(+)\n create mode 100644 How-To-Release-Riak.md\n\n\n\n```\n\n--------------------------------------------------\n\n\n\n. |  |  | |  |\n| ------------ | -------------|------------ | ------------- |\nrelease_doc 1efe508 | :arrow_right: | 2.0 f75e11e | :white_check_mark: completed\nLooks good!  :+1: \n\n:white_check_mark: MERGE \n\n\n> Started at: 2017-03-06 12:57\n> Duration: 1 seconds.\n> Result:  OK\n> Message: Merge Success: release_doc 1efe5084abbd1bb475089b6e5743952d941cab84 onto target branch: 2.0 f75e11effaaa0ae83c435d9cb9d47bc97bd6eaaa\n> Exit Code:  OK\n\n> :page_facing_up:\n\n\n```\n\n\n\n\n\n\n  Updating f75e11e..1efe508\nFast-forward (no commit created; -m option ignored)\n How-To-Release-Riak.md | 158 +++++++++++++++++++++++++++++++++++++++++++++++++\n 1 file changed, 158 insertions(+)\n create mode 100644 How-To-Release-Riak.md\n\n\n\n```\n\n--------------------------------------------------\n\n\n\n. |  |  | |  |\n| ------------ | -------------|------------ | ------------- |\nrelease_doc 660002b | :arrow_right: | 2.0 f75e11e | :white_check_mark: completed\nLooks good!  :+1: \n\n:white_check_mark: MERGE \n\n\n> Started at: 2017-03-06 13:01\n> Duration: 1 seconds.\n> Result:  OK\n> Message: Merge Success: release_doc 660002bb9a33d8f92fd128dc864b64bc763754c9 onto target branch: 2.0 f75e11effaaa0ae83c435d9cb9d47bc97bd6eaaa\n> Exit Code:  OK\n\n> :page_facing_up:\n\n\n```\n\n\n\n\n\n\n  Updating f75e11e..660002b\nFast-forward (no commit created; -m option ignored)\n How-To-Release-Riak.md | 158 +++++++++++++++++++++++++++++++++++++++++++++++++\n 1 file changed, 158 insertions(+)\n create mode 100644 How-To-Release-Riak.md\n\n\n\n```\n\n--------------------------------------------------\n\n\n\n. |  |  | |  |\n| ------------ | -------------|------------ | ------------- |\nrelease_doc 2db947c | :arrow_right: | 2.0 f75e11e | :white_check_mark: completed\nLooks good!  :+1: \n\n:white_check_mark: MERGE \n\n\n> Started at: 2017-03-06 13:02\n> Duration: 1 seconds.\n> Result:  OK\n> Message: Merge Success: release_doc 2db947ce4ec128b0c79ec04fdab1325340652cfa onto target branch: 2.0 f75e11effaaa0ae83c435d9cb9d47bc97bd6eaaa\n> Exit Code:  OK\n\n> :page_facing_up:\n\n\n```\n\n\n\n\n\n\n  Updating f75e11e..2db947c\nFast-forward (no commit created; -m option ignored)\n How-To-Release-Riak.md | 158 +++++++++++++++++++++++++++++++++++++++++++++++++\n 1 file changed, 158 insertions(+)\n create mode 100644 How-To-Release-Riak.md\n\n\n\n```\n\n--------------------------------------------------\n\n\n\n. |  |  | |  |\n| ------------ | -------------|------------ | ------------- |\nrelease_doc ab080f3 | :arrow_right: | 2.0 16ede07 | :white_check_mark: completed\nLooks good!  :+1: \n\n:white_check_mark: MERGE \n\n\n> Started at: 2017-03-15 10:04\n> Duration: 0 seconds.\n> Result:  OK\n> Message: Merge Success: release_doc ab080f30786ac8cd72a2231558626211265e12d4 onto target branch: 2.0 16ede0763dee5f9e86b5501b279c04aab50883ac\n> Exit Code:  OK\n\n> :page_facing_up:\n\n\n```\n\n\n\n\n\n\n  Merge made by the 'recursive' strategy.\n How-To-Release-Riak.md | 160 +++++++++++++++++++++++++++++++++++++++++++++++++\n 1 file changed, 160 insertions(+)\n create mode 100644 How-To-Release-Riak.md\n\n\n\n```\n\n--------------------------------------------------\n\n\n\n. |  |  | |  |\n| ------------ | -------------|------------ | ------------- |\njhc-schema-lager-rotation bb8a581 | :arrow_right: | develop 109e618 | :white_check_mark: completed\nLooks good!  :+1: \n\n:white_check_mark: MERGE \n\n\n> Started at: 2017-04-02 23:20\n> Duration: 1 seconds.\n> Result:  OK\n> Message: Merge Success: jhc-schema-lager-rotation bb8a581c11e5a40705402504bc152aecd7929a3b onto target branch: develop 109e61812e8cd1014e5439cc33b72c904144fe39\n> Exit Code:  OK\n\n> :page_facing_up:\n\n\n```\n\n\n\n\n\n\n  Merge made by the 'recursive' strategy.\n apps/riak/test/riak_schema_test.erl | 18 +++++---\n rel/files/riak.schema               | 84 +++++++++++++++++++++++++++++++++----\n 2 files changed, 87 insertions(+), 15 deletions(-)\n\n\n\n```\n\n--------------------------------------------------\n\n\n\n. |  |  | |  |\n| ------------ | -------------|------------ | ------------- |\nbugfix-remove-execs 9ca3a93 | :arrow_right: | 2.0 f75e11e | :white_check_mark: completed\nLooks good!  :+1: \n\n:white_check_mark: MERGE \n\n\n> Started at: 2017-03-13 13:14\n> Duration: 1 seconds.\n> Result:  OK\n> Message: Merge Success: bugfix-remove-execs 9ca3a931c3286703b554e4847d5545bf6e4e2755 onto target branch: 2.0 f75e11effaaa0ae83c435d9cb9d47bc97bd6eaaa\n> Exit Code:  OK\n\n> :page_facing_up:\n\n\n```\n\n\n\n\n\n\n  Updating f75e11e..9ca3a93\nFast-forward (no commit created; -m option ignored)\n rel/files/riak-admin | 8 ++++----\n rel/reltool.config   | 2 +-\n 2 files changed, 5 insertions(+), 5 deletions(-)\n\n\n\n```\n\n--------------------------------------------------\n\n\n\n. Thanks @JeetKunDoug!\nSettings\n```yaml\nminimum_reviewers: 1\nbuild_steps:\n- make\nmerge: false\norg_mode: true\ntimeout: 1790\n ```\n\n. |  |  | |  |\n| ------------ | -------------|------------ | ------------- |\ndr-add-thumbs 42b6673 | :arrow_right: | develop 6ca76f7 | :white_check_mark: completed\nLooks good!  :+1: \n\n:white_check_mark: MERGE \n\n\n> Started at: 2017-03-14 15:40\n> Duration: 5 seconds.\n> Result:  OK\n> Message: Merge Success: dr-add-thumbs 42b6673e27424889805c88afbe769e1c5ea9eb04 onto target branch: develop 6ca76f7abb4e896d222ffb273a08cf21f172aa7e\n> Exit Code:  OK\n\n> :page_facing_up:\n\n\n```\n\n\n\n\n\n\n  Updating 6ca76f7..42b6673\nFast-forward (no commit created; -m option ignored)\n .thumbs.yml | 6 ++++++\n 1 file changed, 6 insertions(+)\n create mode 100644 .thumbs.yml\n\n\n\n```\n\n--------------------------------------------------\n\n\n\n:white_check_mark: MAKE \n\n\n> Started at: 2017-03-14 15:40\n> Duration: 192 seconds.\n> Result:  OK\n> Message: OK\n> Exit Code:  0\n\n> :page_facing_up:\n\n\n```\n\ncd /tmp/thumbs/develop.6ca76f7.dr-add-thumbs.42b6673; make\n\n\n\n\n\n\n... Snipped 2346 lines ...\ns_fsm.erl\nCompiled src/riak_kv_sweeper_fold.erl\nCompiled src/riak_kv_get_fsm.erl\nCompiled src/riak_kv_mrc_map.erl\nCompiled src/riak_kv_put_fsm.erl\nCompiled src/riak_kv_wm_utils.erl\nCompiled src/riak_kv_delete_sup.erl\nCompiled src/riak_kv_crdt_json.erl\nCompiled src/riak_kv_wm_keylist.erl\nCompiled priv/tracers/tracer_func_args.erl\nCompiled priv/tracers/tracer_accumulating_time.erl\nCompiled priv/tracers/tracer_gc_latency.erl\nCompiled priv/tracers/tracer_backend_latency.erl\nCompiled priv/tracers/tracer_large4.erl\nCompiled priv/tracers/tracer_fsm_init.erl\nCompiled priv/tracers/tracer_eleveldb_put_size.erl\nCompiled priv/tracers/tracer_timeit.erl\nCompiled priv/tracers/tracer_latency_histogram.erl\nCompiled priv/tracers/tracer_read_bin_trace_file.erl\nCompiled priv/tracers/tracer_merge_and_and_handoff.erl\nCompiled src/riak_kv_vnode.erl\n==> merge_index (compile)\nCompiled src/mi_buffer_converter.erl\nCompiled src/mi_bloom.erl\nCompiled src/mi_utils.erl\nCompiled src/mi_sup.erl\nCompiled src/mi_buffer.erl\nCompiled src/mi_app.erl\nCompiled src/mi_locks.erl\nCompiled src/merge_index.erl\nCompiled src/mi_buffer_converter_sup.erl\nCompiled src/mi_segment.erl\nCompiled src/mi_scheduler.erl\nCompiled src/mi_segment_writer.erl\nCompiled src/mi_server.erl\n==> lucene_parser (compile)\nCompiled src/lucene_scan.xrl\nCompiled src/lucene_parse.yrl\nCompiled src/lucene_parse.erl\nCompiled src/lucene_scan.erl\nCompiled src/lucene_parser.erl\nCompiled src/lucene_scan_test.erl\nCompiled src/lucene_parse_test.erl\n==> riak_search (compile)\nCompiled src/riak_search_backend.erl\nCompiled src/riak_search_utils.erl\nCompiled src/riak_search_app.erl\nCompiled src/riak_search_stat.erl\nCompiled src/riak_search_kv_json_extractor.erl\nCompiled src/riak_search_op_term.erl\nCompiled src/basho_bench_driver_riaksearch.erl\nCompiled src/riak_search_op_range_sized.erl\nCompiled src/riak_solr_xml.erl\nCompiled src/riak_search_kv_raw_extractor.erl\nCompiled src/riak_search_op_proximity.erl\nCompiled src/riak_search_op_negation.erl\nCompiled src/riak_search_op.erl\nCompiled src/riak_search_ets_backend.erl\nCompiled src/riak_solr_sort.erl\nCompiled src/riak_search_vnode.erl\nCompiled src/riak_search_kv_hook.erl\nCompiled src/riak_search_basic_qc.erl\nCompiled src/riak_solr_xml_xform.erl\nCompiled src/riak_search_schema.erl\nCompiled src/riak_search_kv_erlang_extractor.erl\nCompiled src/riak_search_op_group.erl\nCompiled src/riak_search_config.erl\nCompiled src/riak_search_kv_xml_extractor.erl\nCompiled src/riak_search_operators_qc.erl\nCompiled src/riak_search_kv_erlang_binary_extractor.erl\nCompiled src/riak_solr_qc.erl\nCompiled src/riak_search_ring_utils.erl\nCompiled src/riak_search_cmd.erl\nCompiled src/riak_search_op_union.erl\nCompiled src/riak_search_op_intersection.erl\nCompiled src/riak_solr_indexer_wm.erl\nCompiled src/riak_search_dir_indexer.erl\nCompiled src/merge_index_backend.erl\nCompiled src/riak_search_kv_extractor.erl\nCompiled src/solr_search.erl\nCompiled src/riak_search_client.erl\nCompiled src/riak_search_pb_query.erl\nCompiled src/search.erl\nCompiled src/riak_search_schema_parser.erl\nCompiled src/riak_search_op_scope.erl\nCompiled src/riak_search_worker.erl\nCompiled src/riak_search_op_node.erl\nCompiled src/riak_search_inlines.erl\nCompiled src/riak_search.erl\nCompiled src/riak_search_op_range.erl\nCompiled src/riak_search_op_mockterm.erl\nCompiled src/riak_solr_searcher_wm.erl\nCompiled src/riak_solr_output.erl\nCompiled src/riak_search_vnode_sup.erl\nCompiled src/riak_search_test.erl\nCompiled src/riak_search_op_string.erl\nCompiled src/riak_solr_search_client.erl\nCompiled src/riak_search_cinfo.erl\nCompiled src/text_analyzers.erl\nCompiled src/riak_search_op_utils.erl\nCompiled src/riak_search_op_range_worker.erl\nCompiled src/riak_search_sup.erl\nCompiled src/riak_indexed_doc.erl\nCompiled src/riak_search_repl_helper.erl\n==> erlydtl (compile)\nCompiled src/erlydtl_parser.yrl\nCompiled src/erlydtl_parser.erl\nCompiled src/erlydtl_i18n.erl\nCompiled src/erlydtl_contrib_humanize.erl\nCompiled src/i18n/po_generator.erl\nCompiled src/i18n/sources_parser.erl\nCompiled src/i18n/blocktrans_extractor.erl\nCompiled src/i18n/i18n_manager.erl\nCompiled src/erlydtl_runtime.erl\nCompiled src/i18n/blocktrans_parser.erl\nCompiled src/i18n/blocktrans_scanner.erl\nCompiled src/erlydtl_deps.erl\nCompiled src/i18n/po_scanner.erl\nCompiled src/erlydtl.erl\nCompiled src/erlydtl_unparser.erl\nCompiled src/filter_lib/erlydtl_dateformat.erl\nCompiled src/filter_lib/erlydtl_slice.erl\nCompiled src/erlydtl_filters.erl\nCompiled src/erlydtl_scanner.erl\nCompiled src/erlydtl_compiler.erl\n==> riak_control (compile)\nCompiled src/riak_control_app.erl\nCompiled src/riak_control_security.erl\nCompiled src/riak_control_wm_gui.erl\nCompiled src/riak_control_wm_cluster.erl\nCompiled src/riak_control_sup.erl\nCompiled src/riak_control_ring.erl\nCompiled src/riak_control.erl\nCompiled src/riak_control_session.erl\nCompiled src/riak_control_wm_nodes.erl\nCompiled src/riak_control_routes.erl\nCompiled src/riak_control_wm_partitions.erl\n:0: Warning: function render_tag/3 is unused\n:0: Warning: variable 'TagName' is unused\nCompiled templates/index.dtl\nCompiled handlebars asset priv/admin/js/generated/templates.js\nBuilt asset priv/admin/js/generated/vendor.js\nWARN:  Bypassing stylesheet processing of priv/admin/css: stylus missing.\n==> riaknostic (compile)\nCompiled src/riaknostic_check.erl\nCompiled src/riaknostic.erl\nCompiled src/riaknostic_check_ring_preflists.erl\nCompiled src/riaknostic_check_strong_consistency.erl\nCompiled src/riaknostic_util.erl\nCompiled src/riaknostic_node.erl\nCompiled src/riaknostic_check_memory_use.erl\nCompiled src/riaknostic_check_monitors.erl\nCompiled src/riaknostic_check_ring_membership.erl\nCompiled src/riaknostic_check_search.erl\nCompiled src/riaknostic_check_nodes_connected.erl\nCompiled src/riaknostic_check_ring_size.erl\nCompiled src/riaknostic_check_dumps.erl\nCompiled src/riaknostic_check_disk.erl\nCompiled src/riaknostic_config.erl\n==> kvc (compile)\nCompiled src/kvc.erl\n==> ibrowse (compile)\nCompiled src/ibrowse_lib.erl\nCompiled src/ibrowse_app.erl\nCompiled src/ibrowse_socks5.erl\nCompiled src/ibrowse_lb.erl\nCompiled src/ibrowse_sup.erl\nCompiled src/ibrowse.erl\nCompiled src/ibrowse_http_client.erl\n==> fuse (compile)\nCompiled src/fuse_stats_plugin.erl\nCompiled src/fuse_app.erl\nCompiled src/fuse_stats_folsom.erl\nCompiled src/fuse_event.erl\nCompiled src/fuse_stats_exometer.erl\nCompiled src/fuse_time.erl\nCompiled src/fuse_stats_ets.erl\nCompiled src/fuse_sup.erl\nCompiled src/fuse.erl\nCompiled src/fuse_monitor.erl\nCompiled src/fuse_server.erl\n==> riakc (compile)\nCompiled src/riakc_datatype.erl\nCompiled src/riakc_ts_query_operator.erl\nCompiled src/riakc_set.erl\nCompiled src/riakc_ts_put_operator.erl\nCompiled src/riakc_ts.erl\nCompiled src/riakc_hll.erl\nCompiled src/riakc_register.erl\nCompiled src/riakc_ts_get_operator.erl\nCompiled src/riakc_flag.erl\nCompiled src/riakc_map.erl\nCompiled src/riakc_utils.erl\nCompiled src/riakc_gset.erl\nCompiled src/riakc_counter.erl\nCompiled src/riakc_obj.erl\nCompiled src/riakc_pb_socket.erl\n==> yokozuna (compile)\nCreate dir ../build\nUsing cached copy of Solr /var/tmp/yokozuna/solr-4.10.4-yz-2.tgz\nCreating Solr dir ../priv/solr\nSolr dir created successfully\nDownloading yokozuna-3.jar\n--2017-03-14 15:43:51--  http://s3.amazonaws.com/files.basho.com/yokozuna/yokozuna-3.jar\nResolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.17.67\nConnecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.17.67|:80... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 11261 (11K) [application/java-archive]\nSaving to: \u2018yokozuna-3.jar\u2019\n\n     0K                                                      100% 25.0M=0s\n\n2017-03-14 15:43:51 (25.0 MB/s) - \u2018yokozuna-3.jar\u2019 saved [11261/11261]\n\nDownloading yz_monitor-1.jar\n--2017-03-14 15:43:51--  http://s3.amazonaws.com/files.basho.com/yokozuna/yz_monitor-1.jar\nResolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.17.67\nConnecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.17.67|:80... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 2661 (2.6K) [application/java-archive]\nSaving to: \u2018yz_monitor-1.jar\u2019\n\n     0K                                                      100% 14.6M=0s\n\n2017-03-14 15:43:51 (14.6 MB/s) - \u2018yz_monitor-1.jar\u2019 saved [2661/2661]\n\nCompiled src/yz_extractor.erl\nCompiled src/yz_pb_search.erl\nCompiled src/yz_stat.erl\nCompiled src/yz_wm_search.erl\nCompiled src/yz_kv.erl\nCompiled src/yz_misc.erl\nCompiled src/yz_wm_extract.erl\nCompiled src/yz_stat_worker.erl\nCompiled src/yz_solrq_sup.erl\nCompiled src/yz_solrq_drain_mgr.erl\nCompiled src/yz_wm_schema.erl\nCompiled src/yz_sup.erl\nCompiled src/yz_doc.erl\nCompiled src/yz_xml_extractor.erl\nCompiled src/yz_general_sup.erl\nCompiled src/yz_solr_sup.erl\nCompiled src/yz_bucket_validator.erl\nCompiled src/rt_intercept_pt.erl\nCompiled src/yz_entropy_mgr.erl\nCompiled src/yz_entropy.erl\nCompiled src/yz_console.erl\nCompiled src/yz_app.erl\nCompiled src/yz_solrq.erl\nCompiled src/yz_wm_index.erl\nCompiled src/yokozuna.erl\nCompiled src/yz_fuse_stats_sidejob.erl\nCompiled src/yz_rs_migration.erl\nCompiled src/yz_noop_extractor.erl\nCompiled src/yz_solr_proc.erl\nCompiled src/yz_schema.erl\nCompiled src/yz_index.erl\nCompiled src/yz_exchange_fsm.erl\nCompiled src/yz_fuse.erl\nCompiled src/yz_events.erl\nCompiled src/yz_solrq_helper.erl\nCompiled src/yz_json_extractor.erl\nCompiled src/yz_pb_admin.erl\nCompiled src/yz_solrq_queue_pair_sup.erl\nCompiled src/yz_index_hashtree.erl\nCompiled src/yz_solrq_worker.erl\nCompiled src/yz_diag.erl\nCompiled src/yz_text_extractor.erl\nCompiled src/yz_solr.erl\nCompiled src/yz_dt_extractor.erl\nCompiled src/yz_cover.erl\nCompiled src/yz_index_hashtree_sup.erl\nCompiled src/yz_solrq_drain_fsm.erl\n==> canola (compile)\nCompiled src/canola.erl\nCompiling c_src/canola-port.c\n==> riak_auth_mods (compile)\nCompiled src/riak_auth_mods_sup.erl\nCompiled src/riak_auth_mods_pam.erl\nCompiled src/riak_auth_mods_app.erl\n==> rel (compile)\n==> riak (compile)\nCompiled src/etop_txt.erl\n==> develop.6ca76f7.dr-add-thumbs.42b6673 (compile)\n\n\n\n\n```\n\n--------------------------------------------------\n\n\n\n:white_large_square: 0 of 1 Code reviews from organization basho\n\n\n. |  |  | |  |\n| ------------ | -------------|------------ | ------------- |\ndr-add-thumbs 42b6673 | :arrow_right: | develop 109e618 | :no_entry: completed\nThere seems to be an issue with build step **make** !  :cloud: \n\n:white_check_mark: MERGE \n\n\n> Started at: 2017-03-20 08:45\n> Duration: 1 seconds.\n> Result:  OK\n> Message: Merge Success: dr-add-thumbs 42b6673e27424889805c88afbe769e1c5ea9eb04 onto target branch: develop 109e61812e8cd1014e5439cc33b72c904144fe39\n> Exit Code:  OK\n\n> :page_facing_up:\n\n\n```\n\n\n\n\n\n\n  Merge made by the 'recursive' strategy.\n .thumbs.yml | 6 ++++++\n 1 file changed, 6 insertions(+)\n create mode 100644 .thumbs.yml\n\n\n\n```\n\n--------------------------------------------------\n\n\n\n:no_entry: MAKE \n\n\n> Started at: 2017-03-20 08:45\n> Duration: 123 seconds.\n> Result:  ERROR\n> Message: Step make Failed!\n> Exit Code:  2\n\n> :page_facing_up:\n\n\n```\n\ncd /tmp/thumbs/develop.109e618.dr-add-thumbs.42b6673; make\n\n\n\n\n\n\n... Snipped 2195 lines ...\npile)\nCompiled src/hyper_register.erl\nCompiled src/hyper_gb.erl\nCompiled src/hyper.erl\nCompiled src/hyper_array.erl\nCompiled src/hyper_binary.erl\nCompiled src/hyper_const.erl\nCompiled src/hyper_binary_rle.erl\n==> chronos (compile)\nCompiled src/chronos_command.erl\nCompiled src/chronos.erl\n==> riak_kv (compile)\nCompiled src/riak_kv_backend.erl\nCompiled src/riak_kv_sweeper.erl\nCompiled src/riak_kv_update_hook.erl\nCompiled src/riak_kv_test_util.erl\nCompiled src/riak_kv_ensemble_backend.erl\nCompiled src/raw_link_walker.erl\nCompiled src/riak_kv_mrc_sink.erl\nCompiled src/riak_kv_w1c_sup.erl\nCompiled src/riak_kv_pipe_get.erl\nCompiled src/riak_kv_counter.erl\nCompiled src/riak_kv_pb_index.erl\nCompiled src/riak_object.erl\nCompiled src/riak_kv_pb_bucket.erl\nCompiled src/riak_kv_sweeper_state.erl\nCompiled src/riak_object_json.erl\nCompiled src/riak_kv_requests.erl\nCompiled src/riak_kv_env.erl\nCompiled src/riak_kv_cinfo.erl\nCompiled src/riak_kv_pipe_index.erl\nCompiled src/riak_kv_exometer_sidejob.erl\nCompiled src/riak_kv_exchange_fsm.erl\nCompiled src/riak_kv_fold_buffer.erl\nCompiled src/riak_kv_mapred_json.erl\nCompiled src/riak_kv_hooks.erl\nCompiled src/riak_kv_js_sup.erl\nCompiled src/riak_kv_wm_preflist.erl\nCompiled src/riak_kv_memory_backend.erl\nCompiled src/riak_kv_wm_ping.erl\nCompiled src/riak_kv_hll.erl\nCompiled src/riak_kv_mrc_pipe.erl\nCompiled src/riak_kv_multi_backend.erl\nCompiled src/riak_kv_bucket.erl\nCompiled src/riak_kv_pb_object.erl\nCompiled src/riak_kv_crdt.erl\nCompiled src/riak_kv_mrc_sink_sup.erl\nCompiled src/riak_kv_console.erl\nCompiled src/riak_kv_sup.erl\nCompiled src/riak_kv_get_core.erl\nCompiled src/riak_kv_w1c_worker.erl\nCompiled src/riak_kv_vnode_status_mgr.erl\nCompiled src/riak_kv_index_fsm_sup.erl\nCompiled src/riak_kv_stat.erl\nCompiled src/riak_kv_pncounter.erl\nCompiled src/riak_kv_js_vm.erl\nCompiled src/riak_kv_entropy_info.erl\nCompiled src/riak_kv_js_manager.erl\nCompiled src/riak_kv_ensemble_console.erl\nCompiled src/riak_kv_wm_buckets.erl\nCompiled src/riak_core_pb.erl\nCompiled src/riak_kv_wm_bucket_type.erl\nCompiled src/riak_kv_fsm_timing.erl\nCompiled src/riak_client.erl\nCompiled src/riak_kv_entropy_manager.erl\nCompiled src/riak_kv_buckets_fsm_sup.erl\nCompiled src/riak_kv_gcounter.erl\nCompiled src/riak_kv_wm_stats.erl\nCompiled src/riak_kv_wm_object.erl\nCompiled src/riak_kv_worker.erl\nCompiled src/riak_kv_mapreduce.erl\nCompiled src/riak_kv_noop_update_hook.erl\nCompiled src/riak_kv_wm_mapred.erl\nCompiled src/riak_kv_yessir_backend.erl\nCompiled src/riak_kv_http_cache.erl\nCompiled src/riak_kv_pb_bucket_key_apl.erl\nCompiled src/riak_kv_mapred_filters.erl\nCompiled src/riak_kv_multi_prefix_backend.erl\nCompiled src/riak_kv_w_reduce.erl\nCompiled src/riak_kv_stat_worker.erl\nCompiled src/riak_kv_put_core.erl\nCompiled src/riak_kv_wm_index.erl\nCompiled src/sms.erl\nCompiled src/riak_kv_delete.erl\nCompiled src/riak_kv_encoding_migrate.erl\nCompiled src/riak_kv_mapred_term.erl\nCompiled src/riak_kv_reformat.erl\nCompiled src/riak_kv_util.erl\nCompiled src/riak_kv_eleveldb_backend.erl\nCompiled src/riak_kv_pb_csbucket.erl\nCompiled src/riak_kv_legacy_vnode.erl\nCompiled src/riak_kv_pb_counter.erl\nCompiled src/riak_kv_cli_registry.erl\nCompiled src/riak_kv_pb_mapred.erl\nCompiled src/json_pp.erl\nCompiled src/riak_kv_wm_crdt.erl\nCompiled src/riak_kv_wm_counter.erl\nCompiled src/riak_kv_pb_crdt.erl\nCompiled src/riak_kv_buckets_fsm.erl\nCompiled src/riak_kv_2i_aae.erl\nCompiled src/riak_kv_keys_fsm_sup.erl\nCompiled src/riak_kv_object_ttl.erl\nCompiled src/riak_kv_index_fsm.erl\nCompiled src/riak_kv_app.erl\nCompiled src/riak_kv_bitcask_backend.erl\nCompiled src/riak_index.erl\nCompiled src/riak_kv_ensembles.erl\nCompiled src/riak_kv_wm_props.erl\nCompiled src/riak_kv_wm_link_walker.erl\nCompiled src/riak_kv_coverage_filter.erl\nCompiled src/riak_kv_index_hashtree.erl\nCompiled src/riak_kv_stat_bc.erl\nCompiled src/riak_kv_pipe_listkeys.erl\nCompiled src/riak_kv_sweeper_cli.erl\nCompiled src/riak.erl\nCompiled src/riak_kv_status.erl\nCompiled src/riak_kv_web.erl\nCompiled src/riak_kv_backup.erl\nCompiled src/riak_kv_keys_fsm.erl\nCompiled src/riak_kv_sweeper_fold.erl\nCompiled src/riak_kv_get_fsm.erl\nCompiled src/riak_kv_put_fsm.erl\nCompiled src/riak_kv_mrc_map.erl\nCompiled src/riak_kv_wm_utils.erl\nCompiled src/riak_kv_crdt_json.erl\nCompiled src/riak_kv_delete_sup.erl\nCompiled priv/tracers/tracer_func_args.erl\nCompiled priv/tracers/tracer_accumulating_time.erl\nCompiled src/riak_kv_wm_keylist.erl\nCompiled priv/tracers/tracer_gc_latency.erl\nCompiled priv/tracers/tracer_large4.erl\nCompiled priv/tracers/tracer_backend_latency.erl\nCompiled priv/tracers/tracer_fsm_init.erl\nCompiled src/riak_kv_vnode.erl\nCompiled priv/tracers/tracer_eleveldb_put_size.erl\nCompiled priv/tracers/tracer_timeit.erl\nCompiled priv/tracers/tracer_latency_histogram.erl\nCompiled priv/tracers/tracer_read_bin_trace_file.erl\nCompiled priv/tracers/tracer_merge_and_and_handoff.erl\n==> merge_index (compile)\nCompiled src/mi_buffer_converter.erl\nCompiled src/mi_bloom.erl\nCompiled src/mi_utils.erl\nCompiled src/mi_sup.erl\nCompiled src/mi_buffer.erl\nCompiled src/mi_app.erl\nCompiled src/mi_locks.erl\nCompiled src/merge_index.erl\nCompiled src/mi_buffer_converter_sup.erl\nCompiled src/mi_segment.erl\nCompiled src/mi_scheduler.erl\nCompiled src/mi_segment_writer.erl\nCompiled src/mi_server.erl\n==> lucene_parser (compile)\nCompiled src/lucene_scan.xrl\nCompiled src/lucene_parse.yrl\nCompiled src/lucene_parse.erl\nCompiled src/lucene_scan.erl\nCompiled src/lucene_parser.erl\nCompiled src/lucene_scan_test.erl\nCompiled src/lucene_parse_test.erl\n==> riak_search (compile)\nCompiled src/riak_search_backend.erl\nCompiled src/riak_search_utils.erl\nCompiled src/riak_search_app.erl\nCompiled src/riak_search_stat.erl\nCompiled src/riak_search_kv_json_extractor.erl\nCompiled src/basho_bench_driver_riaksearch.erl\nCompiled src/riak_search_op_term.erl\nCompiled src/riak_search_op_range_sized.erl\nCompiled src/riak_solr_xml.erl\nCompiled src/riak_search_kv_raw_extractor.erl\nCompiled src/riak_search_op_proximity.erl\nCompiled src/riak_search_op_negation.erl\nCompiled src/riak_search_op.erl\nCompiled src/riak_search_ets_backend.erl\nCompiled src/riak_solr_sort.erl\nCompiled src/riak_search_kv_hook.erl\nCompiled src/riak_search_vnode.erl\nCompiled src/riak_search_basic_qc.erl\nCompiled src/riak_solr_xml_xform.erl\nCompiled src/riak_search_kv_erlang_extractor.erl\nCompiled src/riak_search_schema.erl\nCompiled src/riak_search_op_group.erl\nCompiled src/riak_search_kv_xml_extractor.erl\nCompiled src/riak_search_operators_qc.erl\nCompiled src/riak_search_config.erl\nCompiled src/riak_search_kv_erlang_binary_extractor.erl\nCompiled src/riak_solr_qc.erl\nCompiled src/riak_search_ring_utils.erl\nCompiled src/riak_search_op_union.erl\nCompiled src/riak_search_cmd.erl\nCompiled src/riak_search_op_intersection.erl\nCompiled src/riak_solr_indexer_wm.erl\nCompiled src/merge_index_backend.erl\nCompiled src/riak_search_dir_indexer.erl\nCompiled src/solr_search.erl\nCompiled src/riak_search_kv_extractor.erl\nCompiled src/riak_search_client.erl\nCompiled src/riak_search_pb_query.erl\nCompiled src/search.erl\nCompiled src/riak_search_op_scope.erl\nCompiled src/riak_search_schema_parser.erl\nCompiled src/riak_search_worker.erl\nCompiled src/riak_search_op_node.erl\nCompiled src/riak_search_inlines.erl\nCompiled src/riak_search.erl\nCompiled src/riak_search_op_range.erl\nCompiled src/riak_search_op_mockterm.erl\nCompiled src/riak_solr_output.erl\nCompiled src/riak_search_test.erl\nCompiled src/riak_solr_searcher_wm.erl\nCompiled src/riak_search_vnode_sup.erl\nCompiled src/riak_search_op_string.erl\nCompiled src/riak_solr_search_client.erl\nCompiled src/riak_search_cinfo.erl\nCompiled src/text_analyzers.erl\nCompiled src/riak_search_op_range_worker.erl\nCompiled src/riak_search_op_utils.erl\nCompiled src/riak_search_sup.erl\nCompiled src/riak_indexed_doc.erl\nCompiled src/riak_search_repl_helper.erl\n==> erlydtl (compile)\nCompiled src/erlydtl_parser.yrl\nCompiled src/erlydtl_parser.erl\nCompiled src/erlydtl_i18n.erl\nCompiled src/erlydtl_contrib_humanize.erl\nCompiled src/i18n/po_generator.erl\nCompiled src/i18n/sources_parser.erl\nCompiled src/i18n/blocktrans_extractor.erl\nCompiled src/erlydtl_runtime.erl\nCompiled src/i18n/i18n_manager.erl\nCompiled src/i18n/blocktrans_parser.erl\nCompiled src/i18n/blocktrans_scanner.erl\nCompiled src/i18n/po_scanner.erl\nCompiled src/erlydtl_deps.erl\nCompiled src/erlydtl.erl\nCompiled src/erlydtl_unparser.erl\nCompiled src/filter_lib/erlydtl_dateformat.erl\nCompiled src/filter_lib/erlydtl_slice.erl\nCompiled src/erlydtl_filters.erl\nCompiled src/erlydtl_scanner.erl\nCompiled src/erlydtl_compiler.erl\n==> riak_control (compile)\nCompiled src/riak_control_app.erl\nCompiled src/riak_control_security.erl\nCompiled src/riak_control_wm_gui.erl\nCompiled src/riak_control_wm_cluster.erl\nCompiled src/riak_control_sup.erl\nCompiled src/riak_control_ring.erl\nCompiled src/riak_control.erl\nCompiled src/riak_control_session.erl\nCompiled src/riak_control_wm_nodes.erl\nCompiled src/riak_control_routes.erl\nCompiled src/riak_control_wm_partitions.erl\n:0: Warning: function render_tag/3 is unused\n:0: Warning: variable 'TagName' is unused\nCompiled templates/index.dtl\nCompiled handlebars asset priv/admin/js/generated/templates.js\nBuilt asset priv/admin/js/generated/vendor.js\nWARN:  Bypassing stylesheet processing of priv/admin/css: stylus missing.\n==> riaknostic (compile)\nCompiled src/riaknostic_check.erl\nCompiling /tmp/thumbs/develop.109e618.dr-add-thumbs.42b6673/deps/riaknostic/src/riaknostic.erl failed:\n/tmp/thumbs/develop.109e618.dr-add-thumbs.42b6673/deps/riaknostic/src/riaknostic.erl:120: undefined macro 'DEFAULT_SINK'\n/tmp/thumbs/develop.109e618.dr-add-thumbs.42b6673/deps/riaknostic/src/riaknostic.erl:76: function run/1 undefined\n/tmp/thumbs/develop.109e618.dr-add-thumbs.42b6673/deps/riaknostic/src/riaknostic.erl:135: Warning: function validate_checks/2 is unused\nERROR: compile failed while processing /tmp/thumbs/develop.109e618.dr-add-thumbs.42b6673/deps/riaknostic: rebar_abort\nmake: *** [compile] Error 1\n\n\n\n\n```\n\n--------------------------------------------------\n\n\n\n:white_large_square: 0 of 1 Code reviews from organization basho\n\n\n. ",
    "sharpe5": "You're welcome, Riak-TS sounds amazing, and I'm happy to contribute. . ## ODBC driver for your favourite database GUI front end\nI think that currently, the easiest way to get a quick win with Riak is to use an ODBC or JDBC driver to interact with it.\nThis means that you can fire up your favourite SQL editor, such as pgAdmin or Sql Server Management Studio (SSMS) and insert/query/explore/import data. \nPerhaps the interactive web front end could suggest this? This trick is obvious in hindsight, but not obvious if one is not aware of the connectivity options available from Riak.\nExamples\n\nPostgres offers a Foreign Data Wrapper (FDW) for Riak: https://wiki.postgresql.org/wiki/Foreign_data_wrappers#NoSQL_Database_Wrappers\nMSSQL, Oracle, MySQL or Postgres can interact to Riak by using an ODBC or JDBC driver: https://github.com/basho-labs/Riak-TS-JDBC-Driver\nUsing Postgres, import a CSV file into Postgres, then export it into Riak using the JDBC connector. . \n",
    "ilovezfs": "Any update here?. neat. out of interest: why static. ",
    "w-p": "Done - https://github.com/basho/basho_docs/issues/2386. ",
    "AliakseiMat": "I have installed Riak with homebrew:\nsh\nbrew install riak\nLet me try to run Solr in standalone mode.. In standalone mode Solr works good. \nWhat doesn't mean in log : \"Yokozuna has exited - shutting down Solr\". Where I can find the reason of \"Yokozuna has exited\"\nThanks!. So, I have figure out the problem. Looks like the root cause of this issue is this java bug:https://bugs.openjdk.java.net/browse/JDK-8044306\nThe solution is add the following string into /ets/hosts\nsh\n127.0.0.1       localhost     <hostname>.local\nThe \\<hostname>.local can be got with this terminal command:\nsh\n...$ hostname\nIt also will fix this IntelliJ idea bug :) https://youtrack.jetbrains.com/issue/IDEA-157303. ",
    "lucafavatella": "Notes on possible solutions\nA solution for this would need to:\n Bump OTP in Riak to 19.3 - or any version with the SIGTERM improvement in https://github.com/erlang/otp/commit/4a8e2aeee7 - potentially backported;\n Review usage or run_erl - not sure how it behaves with SIGTERM.\nBackground\nRiak deployment on Debian-like OSes\nI understand that as of Riak KV 2.2.0 on Debian-like OSes:\n Riak startup relies on OS init, specifying an init script that eventually calls run_erl;\n After started, Riak Unix process tree looks like \"run_erl -> beam.smp -> solr\";\n* Riak shutdown relies on the OS init to call the init script with stop, that calls another script that in turn calls nodetool that, using rpc, feeds a nice log message to Riak error_logger and calls Riak init:stop().\nPlease notice that run_erl appears not to handle SIGTERM.\nRiak KV 2.2.0 appears to include Basho\u2019s OTP R16B02 https://github.com/basho/otp/tree/OTP_R16B02_basho10/\nDeployment of OTP releases with alternative OS init\nWhen using alternative OS inits e.g. runit - often for containerized processes e.g. Docker - the best practice is stopping the UNIX process by plain SIGTERM OS signal. Using recent rebar3 and relx with extended_start_script, the foreground option can be used that calls erlexec (that in turn execs BEAM VM).\nOTP and SIGTERM\nRecent OTP improved handling of SIGTERM:\n SIGTERM (from OTP 19.3) will halt Erlang via init:stop/0 (see https://github.com/erlang/otp/commit/4a8e2aeee7)\n OTP master (20?) has erl_signal_server. This request appears to be common from people attempting to run Riak in Docker (in non-production environments I understand). From a recent thread in riak-users list:\n\nIf you're running it in a Docker container, you need to figure out a way to capture the incoming SIGTERM and then use that to shutdown Riak cleanly.\n\nOther related links:\nDocker for testing Riak, and performance\nDocker and EPMD and riak core membership management\nRiak ticket on lock files - lots of tickets linked.\nRiak shutting down not cleanly on Docker\n. ",
    "bidiu": "I don't know Erlang actually, but I think maybe this is because a library is deprecated regarding my Erlang version?. Appreciate it!. ",
    "southerncloudz": "Hi lukebakken,\nWe have only 2013 keys and the system has 16 GB of RAM and 50 GB of Hard Drive.\nI think the hardware is not too low for 2013 keys. \nIn another machine we have tested with 15000+ similar keys and did not face any problem.\nP.S\nI have sent an email to the riak-users mailing list also.. ",
    "sharphellowang": "{error, <<\"locked\">>}, this error occur very high to me.  I use riak-2.2.0-1.el7.centos.x86_64.rpm, and  riak-cs-2.1.1-1.el7.centos.x86_64.rpm, 5 riak node and now only use one cs node, when store data, log show \" gen_server <0.32401.0> terminated with reason: no match of right hand value {error,<<\"{error,locked}\">>} in riak_cs_block_server:handle_cast/2 line 197\", I find it cause by riakc_pb_socket:put(). try server times may be success. The all error log is:\n2017-05-23 23:14:15.298 [error] <0.32401.0>@riak_cs_block_server:handle_cast:193 Put <<\"bl-im-file\">> <<\"ssk.jpg\">> UUID <<208,222,132,114,224,216,77,125,167,0,189,131,44,79,119,236>> block 0 failed: {error,<<\"{error,locked}\">>}\n2017-05-23 23:14:15.298 [error] <0.32401.0> gen_server <0.32401.0> terminated with reason: no match of right hand value {error,<<\"{error,locked}\">>} in riak_cs_block_server:handle_cast/2 line 197\n2017-05-23 23:14:15.298 [error] <0.32401.0> CRASH REPORT Process <0.32401.0> with 1 neighbours exited with reason: no match of right hand value {error,<<\"{error,locked}\">>} in riak_cs_block_server:handle_cast/2 line 197 in gen_server:terminate/6 line 744\n. restart riak can ok?  see:https://stackoverflow.com/questions/36452658/riak-returning-locked-status-when-storing-key. ",
    "Bob-The-Marauder": "@zowers I am afraid that with Basho ceasing most business activities on 31st May 2017 and later going into receivership, you may be a while waiting on that. The Open Source community is already rallying around building new versions and there is talk of a Riak Consortium. Unfortunately neither of these will happen overnight. Given this, I am afraid that you will have to make do with either Jessie or Wheezy if you want official Basho Debian packages - https://files.tiot.jp/riak/kv/2.2/2.2.3/debian/ or, alternately, try compiling from source - https://files.tiot.jp/riak/kv/2.2/2.2.3/riak-2.2.3.tar.gz\n@jagguli That is correct, Riak requires older versions of Erlang. Please see https://www.tiot.jp/riak-docs/riak/kv/2.2.3/setup/installing/source/erlang/#installing-with-kerl for how to install Erlang via Kerl.\nThat said, there is an issue with Yokozuna in that the Yokozuna jar files and Solr files required to build Riak were hosted on the Basho AWS service which is no longer available. A workaround has been released for develop, develop-2.0 and develop-2.2 that points Yokozuna to copies of the necessary files that are hosted by TI Tokyo.\nOnce you have the correct version of Erlang, this is an example of how you could build:\ngit clone riak\ncd riak\ncheckout develop\nmake deps\nexport ARTIFACT_URL_PREFIX=\"https://files.tiot.jp/riak\"\nmake\nSubstitute checkout develop for either checkout develop-2.0 or checkout develop-2.2 depending on which version you want to build. The ARTIFACT_URL_PREFIX points the script to the variable you set here i.e. TI Tokyo's file store instead of the default Basho AWS service as that is currently unavailable. Should Basho's AWS service ever become available again, this work around would no longer be needed.. The cause you have is here ERROR: OTP release 20 does not match required regex R16|17\nRiak only works with older versions of Erlang i.e. 16 and 17 according to this error. As brew no longer supports these older versions of Erlang, you will have to install via Kerl. See https://www.tiot.jp/riak-docs/riak/kv/2.1.4/setup/installing/source/erlang/#installing-with-kerl\nThat said, there is an issue with Yokozuna in that the Yokozuna jar files and Solr files required to build were hosted on the Basho AWS service which is no longer available. A patch has been released for develop, develop-2.0 and develop-2.2 that points Yokozuna to copies of the necessary files that are hosted by TI Tokyo. I am not sure if it works with 2.1.4 as I haven't tested.\nOnce you have the correct version of Erlang, this would be how you build:\ngit clone riak\ncd riak\ncheckout develop\nmake deps\nexport ARTIFACT_URL_PREFIX=\"https://files.tiot.jp/riak\"\nmake\nSubstitute checkout develop for either checkout develop-2.0 or checkout develop-2.2 depending on which version you want to build.\nIf the build fails but your heart is set on 2.1.4, you can download the Mac OSX version from https://files.tiot.jp/riak/kv/2.1/2.1.4/osx/10.8/riak-2.1.4-OSX-x86_64.tar.gz and the sha file is https://files.tiot.jp/riak/kv/2.1/2.1.4/osx/10.8/riak-2.1.4-OSX-x86_64.tar.gz.sha. Based on the title of your post, you'll be wanting Erlang 16 which you'll need Kerl for. Please see https://www.tiot.jp/riak-docs/riak/kv/2.2.3/setup/installing/source/erlang/#kerl-prerequisites for details.\nThis is also referenced in https://github.com/basho/riak/issues/920. Can you post the content of the console.log file? I think the Homebrew path would be /usr/local/Cellar/riak/2.2.3/logs/console.log but I don't have Riak on a Mac handy to test. It should tell you why it failed to start in there or somebody here should be able to notice why.. I think the point has been missed here. The idea of salting the encryption key with the bucket name and key name is to render the encryption key alone useless.\nFrom Riak's point of view, it doesn't care whether your bucket is called \"myBucket\" or \"as789AW!233fh84325qhfdsuy9876\", so encrypting the bucket name with the encryption key salted with the bucket name is a perfectly valid solution. This way, the attacker cannot retrieve the bucket name from the file system without significant brute force work or some other non-Riak related method to get the list of bucket and key names a different way.\nThis method does have the caveat that you cannot ask Riak for a list of buckets and keys but even Basho said that this should not be done in a production environment:\nhttps://www.tiot.jp/riak-docs/riak/kv/2.2.3/developing/api/http/list-buckets/\nhttps://www.tiot.jp/riak-docs/riak/kv/2.2.3/developing/api/http/list-keys/\nI do not argue that having filesystem-level encryption would be beneficial. However, security is all about layers and if the attack is against a live server where the attacker successfully gains riak or root then filesystem-level encryption will be worthless. Thinking about it, with the bucket and key names encrypted, the attacker would not be able to run a bucket list or key list to retrieve the buckets or keys as the returned lists would be the encrypted names. Of course, the attacker could brute force them but still, this would further slow them down and that is the entire idea of layered security. Nothing is totally secure but every step to hinder the attacker is worthwhile.\nAt the end of the day, we need to look at this with a regard to the future suitability of Riak for corporate customers which means we need to address the problem listed at the start of the post:\n\nMany competing systems are offering encryption of data at rest i.e. marketing.\nIndustries are starting to make encryption of data at rest a requirement i.e. usage viability.\nHackers are becoming better at accessing file systems where non-encrypted data is at risk i.e. security.\n\nRiak already has a surprisingly wide array of security options https://www.tiot.jp/riak-docs/riak/kv/2.2.3/using/security/basics/ which have been nicely summarised in this blog post https://gist.github.com/lucperkins/8920660 and, in the corporate world, where ticking boxes is often as far as senior management looks towards IT security, having this feature available would fill that requirement nicely. \nIt may be a lot of work that many people do not see as a priority right now but as regulations start rolling out, I am not sure whether Riak will be able to survive outside of obscurity for long if its only remaining users are hobbyists and one or two die hard corporate fans.. +1. Merely trying to avoid confusion with CS, TS and the Riak family as a whole. We covered this once before in https://github.com/TI-Tokyo/riak-community-admin/blob/master/proposal/Riak%20Community%20Proposal.md#naming-conventions. ",
    "martincox": "I'm guessing the reason for wanting EE packages is to get replication? That's now been ported to OS Riak. There's an internal release 2.2.4 which is 2.2.3 with repl included - there's no packages for it though, so you would build from source and package yourself. There is an upcoming 2.2.5 release, however, which should land in Jan.\nIf you really need EE packages regardless, you might get some help in Slack.. No, this was just putting repl into open-source Riak. If there's an immediate need for it, let's add in the other components too. Any idea how widely used they are, if at all?\nOn the repo point, seemed less effort to add repl to to OSS Riak than to open up the private repo. It made more sense to keep it around - more issues, more activity, more forks, plain Riak over EE. There's not much in it really.\n. I blame it on old eyes - that covers us both :laughing: . Just starting to browse through all of these PRs - a really minor point; do we need to store the images in here (looks like they amount to a few MB)?\nAlso, I know you've raised the other PRs for deps, but they are still pointing at your GH in this rebar.config.. Also, interested to see the impact from the soft vnode limits in relation to the issues that we described earlier in the week around net_kernel timeouts. I'll try and get some comparative tests run early next week to see if it alleviates that problem.. @cmeiklejohn It's about to be!. @cmeiklejohn open :). ",
    "kimgust": "Seems their S3 bucket(s) are down:\n$ curl -I https://docs.basho.com\nHTTP/1.1 403 Forbidden\nConnection: keep-alive\nx-amz-error-code: AllAccessDisabled\nx-amz-error-message: All access to this object has been disabled\nDate: Fri, 18 Aug 2017 22:45:44 GMT\nServer: AmazonS3\nX-Cache: Error from cloudfront\nVia: 1.1 6a9f10f41b76b2383b9a1f1260dd91cd.cloudfront.net (CloudFront)\nX-Amz-Cf-Id: Xmky3-E0uA_W0V3QOH_3pIL7nW1CW7DFSvZfMlTdKU-7EwY5RstukQ==\n$ curl -I http://downloads.basho.com.s3-website-us-east-1.amazonaws.com/riak/CURRENT/rhel/6/riak-1.2.0-1.el6.x86_64.rpm\nHTTP/1.1 403 Forbidden\nx-amz-error-code: AllAccessDisabled\nx-amz-error-message: All access to this object has been disabled\nx-amz-request-id: E3F9B560B76A8AE2\nx-amz-id-2: mZeW4TJUT0LHukIcC0NcsKJpJ5ulHBhFxnHDLcZIKGvVOMFPV7Fd/jR83OQ1ErReMoVhY7tVtKU=\nTransfer-Encoding: chunked\nDate: Fri, 18 Aug 2017 19:38:26 GMT\nServer: AmazonS3. ",
    "srikanthkakumanu": "Basho is in bankruptcy state and no longer providing the access to their package downloads, i have been trying to access it since many days but no resolution. Hence you need to rely on open source Riak Git repos.. ",
    "niranjan92": "@srikanthkakumanu thanks for the info. seems like now the docs aren't working either. . @russelldb thanks, closing this issue now. ",
    "dotsiadia": "Referring to @Bob-The-Marauder solution, when installing riak-2.2.3 from the source, I just edited the  file build-solr.sh found in the tools directory of yokozuna directory and replaced http://s3.amazonaws.com/files.basho.com  with https://files.tiot.jp/riak where ever it appears in the file. I was following the procedure of installing from the source as stated here https://www.tiot.jp/riak-docs/riak/kv/2.2.3/setup/installing/source/#installing-from-source-package.. Referring to @binarytemple  solution, when installing riak-2.2.3 from the source, I just edited the  file build-solr.sh found in the tools directory of yokozuna directory and replaced http://s3.amazonaws.com/files.basho.com  with https://files.tiot.jp/riak where ever it appears in the file. I was following the procedure of installing from the source as stated here https://www.tiot.jp/riak-docs/riak/kv/2.2.3/setup/installing/source/#installing-from-source-package.. ",
    "zowers": "see https://github.com/basho/riak/issues/919#issuecomment-323918548\n\nBasho is in bankruptcy state and no longer providing the access to their package downloads, i have been trying to access it since many days but no resolution. Hence you need to rely on open source Riak Git repos.\nTiot.jp are hosting mirrors of downloads and docs. The debs you look for are here https://files.tiot.jp/riak/kv/2.2/2.2.3/ubuntu/. The docs are here https://www.tiot.jp/riak-docs/. see https://github.com/basho/riak/issues/920#issuecomment-325121500. \n",
    "joshzinny": "Hi guy, I am having this same problem. Is there any solution to this? Thanks in advance\njoshzinny:riak joshzinny$ make devrel\n./rebar get-deps\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> rebar_lock_deps_plugin (get-deps)\n==> riak_pb (get-deps)\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> mochiweb (get-deps)\n==> webmachine (get-deps)\n==> riak_api (get-deps)\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> proper (get-deps)\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> stdlib2 (get-deps)\n==> hyper (get-deps)\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> chronos (get-deps)\n==> riak_kv (get-deps)\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> rel (get-deps)\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> riak (get-deps)\n==> riak (get-deps)\n./rebar compile\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\nWARN:  Missing plugins: [rebar_lock_deps_plugin]\n==> rebar_lock_deps_plugin (compile)\nERROR: OTP release 21 does not match required regex R16|17\nERROR: compile failed while processing /Users/joshzinny/riak/deps/rebar_lock_deps_plugin: rebar_abort\nmake: *** [compile] Error 1. I am using Erlang/OTP 21 [erts-10.0] [source] [64-bit] [smp:4:4] [ds:4:4:10] [async-threads:1]\nEshell V10.0  (abort with ^G). Is there any work around solution now?\n. Okay thanks.. ",
    "bryanhuntesl": "Suggest we close this out - nothing useful in that log file - stuff needs to get chased or followed up or the trail goes cold. . My 2\u00a2 - better to encrypt the data at the middleware layer - if it were confidential I'd never even send it to the storage system in the first place. . We don't have to convert gen_fsm to gen_statem - we can copy the gen_fsm source into riak_core or a project dependency, remove the deprecation warning and just move on - the erlang code is Apache 2 licensed so no conflict with riak code. . I wonder if we could go even further than this and make the advanced.config completely empty by default - the lager configuration section is, as far as I can tell, superfluous.  \nIt was added by @bsparrow435 back in 2016 but I can't see any reference to it anywhere in the code.\nEveryone seems to be copy-n-pasting that block, but I'm smelling a potential cargo-cult.\n. Nice fix +1 . verified it works for top level riak project. @martincox - left a dead function clause in the first attempt. . @jeanparpaillon - can you pipe the compilation output to a file and attach to the ticket please. . Got it from Slack conversation - pasting here for posterity. \nCompiling /home/jeanp/riak/deps/hyper/src/hyper_gb.erl failed:\n/home/jeanp/riak/deps/hyper/src/hyper_gb.erl:70: type gb_tree/0 is deprecated and will be removed in OTP 18.0; use use gb_trees:tree/0 or preferably gb_trees:tree/2\nERROR: compile failed while processing /home/jeanp/riak/deps/hyper: rebar_abort\nmake: *** [compile] Error 1. I don\u2019t think it should be a blocker for 2.2.5 release, just add it to the release notes as a known \u2018feature\u2019 and promise to improve user experience in a later release.. That's in the riak_repl cuttlefish schema : \n%% @doc Path (relative or absolute) to the working directory for the\n%% replication process\n{mapping, \"mdc.data_root\", \"riak_repl.data_root\", [\n    {default, \"{{repl_data_root}}\"}\n]}.\nIt relies upon the variable repl_data_root being set.\nI wonder if that variable is not being set, perhaps it got lost in the move from riak_ee repository?\n. So I wonder if it is not being set by default, or that it is being set to an empty value. . @dams - is this issue when running riak 2.2.5 RC2 from packages or via make rel or make devrel ? . Thanks @dams - so to summarize, if a user is upgrading to Riak 2.2.5 and enabling riak_repl for the first time - they will need to create the file /etc/riak/advanced.config - the recommended contents of that file will be : \n```\n[\n {riak_core,\n  [\n   {cluster_mgr, {\"0.0.0.0\", 9080 } }\n  ]},\n {riak_repl,\n  [\n   {data_root, \"/var/lib/riak/riak_repl/\"},\n   {max_fssource_cluster, 5},\n   {max_fssource_node, 1},\n   {max_fssink_node, 1},\n   {fullsync_on_connect, true},\n   {fullsync_interval, 30},\n   {rtq_max_bytes, 104857600},\n   {proxy_get, disabled},\n   {rt_heartbeat_interval, 15},\n   {rt_heartbeat_timeout, 15},\n   {fullsync_use_background_manager, true}\n  ]},\n{lager,\n   [\n      {extra_sinks,\n           [\n            {object_lager_event,\n             [{handlers,\n               [{lager_file_backend,\n                 [{file, \"/var/log/riak/object.log\"},\n                  {level, info},\n                  {formatter_config, [date, \" \", time,\" [\",severity,\"] \",message, \"\\n\"]}\n                 ]\n                }]\n              },\n              {async_threshold, 500},\n              {async_threshold_window, 50}]\n            }\n            ]\n      }\n    ]\n}\n].\n``\n. More details - on a fresh install, if you subsequently delete the file/etc/riak/advanced.config- or if you upgrade and fail to provide an appropriate/etc/riak/advanced.config` file, then :  \n\nRiak will fail to start\nRiak and not generate any console messages\nA subsequent check of the file /var/log/riak/crash.log will reveal the following entry: \n\n2018-04-24 14:19:27 =CRASH REPORT====\n  crasher:\n    initial call: application_master:init/4\n    pid: <0.633.0>\n    registered_name: []\n    exception exit: {{bad_return,{{riak_repl_app,start,[normal,[]]},{'EXIT',{{badmatch,{error,enoent}},[{riak_repl_app,start,2,[{file,\"src/riak_repl_app.erl\"},{line,37}]},{application_master,start_it_old,4,[{file,\"application_master.erl\"},{line,269}]}]}}}},[{application_master,init,4,[{file,\"application_master.erl\"},{line,133}]},{proc_lib,init_p_do_apply,3,[{file,\"proc_lib.erl\"},{line,239}]}]}\n    ancestors: [<0.632.0>]\n    messages: [{'EXIT',<0.634.0>,normal}]\n    links: [<0.632.0>,<0.7.0>]\n    dictionary: []\n    trap_exit: true\n    status: running\n    heap_size: 987\n    stack_size: 27\n    reductions: 181\n  neighbours:\n. So in conversation with the team - we've decided this is not going to be a blocker for the  2.2.5 release, will include it as a 'feature' in the release notes. \nOnly affects users who upgrade from a version which lacks riak_repl. \nAlso from user feedback, only occurs when performing upgrades with using RPM packages : \n\n\nbecause the deb package installs the new config, and gets rid of the old one. RPM does it the other way round\n\n\nSolution is to integrate and test the existing riak_repl cuttlefish configuration.\n. +1 \n\n. Once Riak has your AAE it's going to be a lot easier in any event (YZ). Nice work. . Well spotted - let me remove that .  #### This released is dedicated to the memory of Andy Gross. Thank you and RIP. . @russelldb - you've already provided the useful config directly, no point in directing users to the comment again, also let's make it consistent with the format you've used for others.\nMore details about the issue can be found in riak_repl/782 [2.2.5 - [enoent] - riak_repl couldn't create log dir \"data/riak_repl/logs\" ](https://github.com/basho/riak/issues/940). Riak EE. All code is now Open Source.. Developer Improvements\n. Ha ha, yeah Basho marketing. I tried to explain the various branding to a colleague from the marketing department yesterday afternoon. Her, \"so, they renamed Riak CS (cloud storage) to Riak S2, because it supported the Amazon S3 API, but wasn't quite as good?\". Me, \"I can't think of any other explanation\". \ud83d\ude06 . ",
    "Licenser": "To be blunt this seems like a lot of work for no result. If the encryption key lives on a text file on the host, copying the filesystem means copying the key and all needed information as:\n\nThe key is on the same node - an attacker has it\nThe bucket names need to be stored unencrypted for bucket listing/bucket metadata (or encrypted with the key only but as we have the key it's the same thing) - an attacker has them\nThe key names have to be stored unencrypted (or encrypted with key + bucket name but since we got those on the last two steps it's the same thing)\n\nThe only possible solution for this I see is having a user provide the encryption key on request, it still lives in memory for a while so a crafty attacker would be able to obtain it but at least it'll be a bit safer. This could be somewhat mitigated by using asymetric encyption, i.e. have a public key assigned to a bucket (that lives on the nodes w/o issue) that would mean writes do not need an additional key, and either return encrypted data (which would mean reworking how repair & CRDTs work) or providing the secret key on reads (which introduces another security risk).. When it comes to just encryption at rest, filesystem-level encryption will probably do a better job then what we can come up with, those systems are often build by experts and audited for security.. ",
    "nickkeers": "test comment . @binarytemple what's this function case for? Am i being dense? Looks like the above case will always match. ",
    "jamesruan": "Sorry for the unstable network caused repeated issue.. Sorry for the unstable network caused repeated issue.. Sorry to reply late.\nI'm no longer having access to the server. So I can not provide more info for this case. As far as I remember the key is time+type, indexed further by source subtype.\nIf in case that the delete has not cover the entire key, why would the select be successful?. ",
    "gordonguthrie": "What is your key defintion? I would expect it to be 'time' and 'source' at least.\nYour delete statement needs to cover the entire key\nWhat is the CREATE TABLE you used to create the table\nhttp://docs.basho.com/riak/ts/1.5.2/using/querying/delete/ . ",
    "dams": "maybe it ought to be sorted by packaging, but riak didn't want to start until I had this bits in advanced.conf:\n{riak_repl,\n  [\n   {data_root, \"/var/lib/riak/riak_repl/\"}\n  ]\n  }\nFor easier user experience, I think riak_repl data_root should default to riak base_dir / riak_repl or something similar.. it was when upgrading via packages (rpm in this case). The effect was that an old advanced.conf file was kept, in which there were no riak_repl section at all.\nTo reproduce the bug, make sure you have an advanced.conf file that is not empty, but that doesn't contain a riak_repl section. at startup, the riak_repl data_root directory will be data/riak_repl/ which is not what you'd expect. It's not a sane default because it's relative to where riak is started, not under riak_root.. basically, yes. Few things:\n\nI think that even without riak_repl enabled, the issue occurs.\nUsers will need to either create or advanced.config as you mentioned, or update an existing one the had before, making sure the riak_repl is as you indicates\n\nhope that helps. ",
    "martinsumner": "+1. Something up with Markdown on headers in the earlier release notes section.  Missing space?. On the specific issue of providing enough attention in the release notes + 1. I think the release contents need a separate header - so they don't appear to be under known issues. +1. @martincox - top level repo change (again just a merge into develop-2.9 branch). Yes, good point on the pointing.  I will re-point it all after the big drops are in.\nWith hindsight I should have started a separate repo for all the images and release stuff.  I can take them out now, but I think with git their footprint may never go away.  I'll have a look into what can be done.. @martincox I've removed all the images from the PR now.  I have a separate repo for the testing notes now.. rebar.config updated now.\nPR depends on associated develop-2.9 PRs to:\nriak_kv\nriak_core\nriak_repl. @martincox will go ahead with merge into develop-2.9 this evening, unless I hear otherwise.  Hope that's OK.  Ta. Moves develop-2.9 to RC3 candidate. The timeouts seen in the iclerk looks like they were related to memory exhaustion.  As part of RC2 the way a level 0 SST file would fetch its data changes, meaning the data had to be kept on the loop state in the starting state.  In the reader state - this data was removed from the loop state.\nL0 files are normally short-lived.  However in handoff, they are more likely to be switched rather than merged to clear L0.  In this case, the L0 sst file can become long-lived.  The file occupied about 50M memory, more than 95% of which was uncollected garbage (the now deleted data from the loop state).\nThis could lead to a rapid accumulation of memory in the beam. \nTo avoid this, on switching a level zero file, the file now calls garbage_collect(self()).  This leads to a more smoother growth in riak memory - and means pauses related to GC in the future (and hence timeouts on function calls) are less likely.. I think the three main places where there are NIFs are:\nerlang_js (JS map reduce)\nleveldb\nbitcask\nBut there is also:\nebloom (not sure it is used outside of riak_repl, and there are pure erlang alternatives)\nriak_ensemble (has a monotonic clock NIF - could drop ensemble, or with OTP 20 work use native erlang version?)\nsyslog (used by lager syslog backend.  Just drop this? Does anyone use it?)\ncanola (PAM authentication used by riak_auth_mods.  Just drop this?  Does anyone use it?)\nSo it would need to be a slimmed down riak.. Also updates riak_control to develop-2.9 branch.  riak_control was previously pulling incorrect versions, and causing possible conflicts when running make deps. Holding off on this pull for now.  There may be further issues with the eper branch change. The eper branch will now be cleaned up following a test.  Once riak_test suites have been run this PR is good to go.. ",
    "llelf": "@martinsumner update the version in reltool.config. ",
    "ramensen": "+1 I'm happy with this. ",
    "massung": "Nope, can't create them in the Makefile since the user has to interact with the program to actually generate them. Dizzy and Jon both seemed fine w/ checking in a self-signed certificate. I'm conflicted. On one hand you want this to be dead simple for someone to launch and get running. On the other hand, you have all the fun security stuff. \nOne idea I had - but am unsure if it would work - was to have the makefile create soft links to the OS's default certs, or perhaps to some created by mochiweb? But I'm not sure if that's a bad idea or not.\n. ",
    "dhull": "The existing indentation in this file is a hodgepodge of spaces and tabs (sometimes with both on the same line). Previously this line had eight spaces. I replaced them with a tab. It lines up properly if a tab == eight spaces, which is the equivalence that the rest of the file assumes.\n. "
}